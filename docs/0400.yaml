- en: 'Ethics in AI: Potential Root Causes for Biased Algorithms'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/ethics-in-ai-potential-root-causes-for-biased-algorithms-890091915aa3?source=collection_archive---------1-----------------------#2023-01-27](https://towardsdatascience.com/ethics-in-ai-potential-root-causes-for-biased-algorithms-890091915aa3?source=collection_archive---------1-----------------------#2023-01-27)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: An alternative approach to understanding bias in data
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@jonas_dieckmann?source=post_page-----890091915aa3--------------------------------)[![Jonas
    Dieckmann](../Images/e1f2d236e6bda6ec1e14fd5eaa9d205e.png)](https://medium.com/@jonas_dieckmann?source=post_page-----890091915aa3--------------------------------)[](https://towardsdatascience.com/?source=post_page-----890091915aa3--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----890091915aa3--------------------------------)
    [Jonas Dieckmann](https://medium.com/@jonas_dieckmann?source=post_page-----890091915aa3--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F1c8d1cf684f2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fethics-in-ai-potential-root-causes-for-biased-algorithms-890091915aa3&user=Jonas+Dieckmann&userId=1c8d1cf684f2&source=post_page-1c8d1cf684f2----890091915aa3---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----890091915aa3--------------------------------)
    ·12 min read·Jan 27, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F890091915aa3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fethics-in-ai-potential-root-causes-for-biased-algorithms-890091915aa3&user=Jonas+Dieckmann&userId=1c8d1cf684f2&source=-----890091915aa3---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F890091915aa3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fethics-in-ai-potential-root-causes-for-biased-algorithms-890091915aa3&source=-----890091915aa3---------------------bookmark_footer-----------)![](../Images/e5c4b13259f189e635f5142221eb191e.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: 'Image Credits: [Pixabay](https://pixabay.com/vectors/lady-justice-allegory-justice-7313900/)'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: A s the number of data science applications increases, so does the potential
    for abuse. It is easy to condemn developers or analytics teams for the results
    of their algorithms, but are they the main culprits? The following article tries
    to discuss the problem from a different angle and **concludes that ethical abuse
    might be the real problem with data in our society**.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: A better world won’t come about simply because we use data; data has its dark
    underside. ¹
  id: totrans-10
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Biased algorithms
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Today, discussions of data science are often associated with significant opportunities
    in business and industry to deliver more accurate predictive and classification
    solutions and improve people’s lives in the health and environmental sectors.²
    ³ Perhaps as important as the benefits are the data ethics challenges that should
    be considered when evaluating new solutions and approaches in data analytics.
    As a specific branch of ethics, data ethics addresses moral issues related to
    data, algorithms, and the respective practices to formulate and support morally
    good solutions.⁴ Overall, there seems to be a fine line between the use and misuse
    of data. It is common for companies to collect data not only to increase their
    profits but also to provide a tailored and targeted experience to their customers.⁵
    However, when companies start to externally exploit their data for purposes other
    than those for which it was collected, ethical questions arise. In addition to
    these privacy-related issues, another challenge is data analysis bias, which can
    arise in several ways. Examples include the creation of survey questions by people
    with a particular intent or framing, the selective collection of data from groups
    with a particular background, or the underlying bias of those from whom the data
    is drawn.⁶
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 如今，数据科学的讨论常常与商业和工业中的重大机遇相关联，以提供更准确的预测和分类解决方案，并改善人们在健康和环境领域的生活。² ³ 也许和这些好处一样重要的是，在评估数据分析中的新解决方案和方法时，应该考虑数据伦理挑战。作为伦理学的一个特定分支，数据伦理处理与数据、算法及相关实践有关的道德问题，以制定和支持道德良好的解决方案。⁴
    总体而言，数据的使用与滥用之间似乎存在一条微妙的界限。公司不仅收集数据以增加利润，还为了向客户提供量身定制的体验。⁵ 然而，当公司开始将其数据用于其他收集目的之外的外部利用时，就会出现伦理问题。除了这些隐私相关的问题外，另一个挑战是数据分析偏差，它可能以多种方式出现。例如，由有特定意图或框架的人设计的调查问题、从具有特定背景的群体中选择性收集数据，或数据来源本身的偏差。⁶
- en: '![](../Images/8cf06feccb2e3d60dd59b0cee91c5553.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8cf06feccb2e3d60dd59b0cee91c5553.png)'
- en: 'Image Credits: [Pixabay](https://pixabay.com/photos/play-figures-green-blue-play-wood-4541731/)'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：[Pixabay](https://pixabay.com/photos/play-figures-green-blue-play-wood-4541731/)
- en: Popular examples of algorithmic discrimination
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 算法歧视的流行例子
- en: 'The issues are not at all theoretical — they lead to real concerns and cause
    serious discussions as described in several case studies:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 这些问题绝非理论上的——它们引发了实际关注，并在多个案例研究中引发了严重的讨论：
- en: '**Racial discrimination in crime prediction**'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**犯罪预测中的种族歧视**'
- en: A problematic and well-known example is the prediction of crime. A 2016 machine
    learning paper by Shanghai Jiao Tong University investigated whether crime can
    be detected in people based on facial feature analysis.⁷ The result was that members
    of the population who look “different” from the average are more likely to be
    suspected of committing crimes. ⁸ Another example of biased data science approaches
    is the risk assessment algorithm used in the United States to predict the likelihood
    of re-offending (repeated criminal behavior) by arrested individuals. It turned
    out that the underlying algorithm tended to predict the likelihood of recidivism
    of black defendants twice as high as that of white defendants, while white defendants
    tended to be incorrectly classified as low-risk when in fact they were recidivists.⁹
  id: totrans-18
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 一个有问题且广为人知的例子是犯罪预测。2016年，上海交通大学的一篇机器学习论文研究了是否可以通过面部特征分析来检测犯罪行为。⁷ 结果显示，那些外貌与平均水平“不同”的人更可能被怀疑犯有罪行。⁸
    另一个例子是美国用于预测被逮捕个体再犯可能性的风险评估算法。结果表明，底层算法倾向于将黑人被告的再犯可能性预测为白人被告的两倍，而白人被告则常被错误分类为低风险，即使他们实际上是再犯者。⁹
- en: '**Gender discrimination in recruitment**'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**招聘中的性别歧视**'
- en: Another area that is affected by biased algorithms is recruitment. In 2014,
    Amazon used machine learning-based software to evaluate and rank the CVs of potential
    candidates to find new top talent. In 2015, the company discovered that its new
    system was not evaluating applicants in a gender-neutral way, especially for software
    development jobs. Based on hiring over the last ten years, the system penalized
    all applications that included the word “women” in their CVs.¹⁰
  id: totrans-20
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 另一个受偏见算法影响的领域是招聘。2014年，亚马逊使用基于机器学习的软件来评估和排名潜在候选人的简历，以寻找新的顶级人才。2015年，该公司发现其新系统在评估申请者时并未性别中立，特别是在软件开发职位上。根据过去十年的招聘情况，该系统惩罚了所有在简历中包含“女性”一词的申请。¹⁰
- en: '**Stereotypes in the social media**'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**社交媒体中的刻板印象**'
- en: Gender and race bias can also be found in Facebook’s advertising. In their 2019
    empirical study, Ali et al. discovered that there may be biased rules for delivering
    ads to specific users, based on the predicted relevance of those ads to different
    audiences. This relevance is often based on male/female stereotypes, classified
    and correlated through image analysis software that analyses digital advertising
    media.¹¹
  id: totrans-22
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 性别和种族偏见也可以在Facebook的广告中找到。在他们2019年的实证研究中，Ali等人发现，根据广告与不同受众的相关性，可能会对特定用户投放广告时存在偏见规则。这种相关性通常基于男性/女性刻板印象，通过分析数字广告媒体的图像分析软件进行分类和关联。¹¹
- en: Principle for big data ethics
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 大数据伦理原则
- en: 'How can this be prevented? These cases are just a few of many instances where
    algorithms are used in a biased way in today’s world. Many experts discuss and
    evaluate ways to avoid bias in data analysis, which is expressed in one of the
    “5 Principles for Big Data Ethics” published by Herdiana: “Big Data should not
    institutionalize unfair biases such as racism or sexism.” ¹² While this principle
    is considered a general rule within data ethics, the examples given “such as racism
    or sexism” seem to be the most common occurrences. However, a more general description
    might be:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 如何防止这种情况的发生？这些案例只是现代世界中算法被偏见性使用的众多实例之一。许多专家讨论并评估避免数据分析偏见的方法，这在Herdiana发布的《大数据伦理5原则》之一中有所体现：“大数据不应固化诸如种族主义或性别歧视等不公平偏见。”¹²
    虽然这一原则被认为是数据伦理中的通用规则，但给出的示例“如种族主义或性别歧视”似乎是最常见的情况。然而，更通用的描述可能是：
- en: “Big data should not institutionalize unfair biases that cause discrimination”.
  id: totrans-25
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “大数据不应固化导致歧视的不公平偏见。”
- en: The Canadian Human Rights Commission (CHRC) describes discrimination as an act
    or decision that treats a person or group poorly for reasons called grounds. These
    grounds are protected by the Canadian Human Rights Act and include *race, national
    or ethnic origin, color, religion, age, sex, sexual orientation, gender identity
    or expression, marital status, family status, disability, genetic characteristics,
    and a conviction for which a pardon has been granted or registration suspended*.
    ¹³ In sociology, discrimination describes the belittling of others based on individual
    or group characteristics, practiced systemically and therefore constituting a
    gross violation of human rights.¹⁴ The European Convention on Human Rights (ECHR)
    also prohibits discrimination in all of the above areas in Article 14, adding
    to the CHRC list *language, political opinion,* and *property* as well.¹⁵
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 加拿大人权委员会（CHRC）将歧视描述为因称为“理由”的原因而对某人或某群体的不公平对待。这些理由受到《加拿大人权法案》的保护，包括*种族、民族或族裔来源、肤色、宗教、年龄、性别、性取向、性别认同或表现、婚姻状况、家庭状况、残疾、遗传特征以及已获赦免或注册被暂停的罪行*。¹³
    在社会学中，歧视描述了基于个人或群体特征的贬低行为，这种行为是系统性地进行的，因此构成了严重的人权侵犯。¹⁴ 欧洲人权公约（ECHR）也在第14条中禁止在上述所有领域的歧视，并将*语言、政治观点*和*财产*等添加到CHRC的名单中。¹⁵
- en: 'The European Economic and Social Committee (EESC) study entitled “The ethics
    of Big Data: Balancing economic benefits and ethical questions of Big Data in
    the EU policy context” describes algorithm bias as one of the critical ethical
    problems of Big Data. They define trust in algorithms as an ethical problem because
    most people think that machines are neutral by definition. In reality, this is
    not the case, and therefore the risk can be very high.¹⁶ But what are possible
    reasons for this lack of neutrality? It is hard to imagine that the best data
    scientists in the world have developed their algorithms with this goal in mind.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 欧洲经济和社会委员会（EESC）的一项研究题为“**大数据的伦理：在欧盟政策背景下平衡经济利益和大数据的伦理问题**”将算法偏见描述为大数据的关键伦理问题之一。他们将对算法的信任定义为一个伦理问题，因为大多数人认为机器天生是中立的。实际上情况并非如此，因此风险可能非常高。¹⁶
    但这种缺乏中立性的可能原因是什么？很难想象世界上最优秀的数据科学家在开发算法时有这样的目标。
- en: Root causes analysis
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 根本原因分析
- en: 'Algorithms are not biased. Nevertheless, there exist many popular examples
    of skewed outcomes from algorithms while deeper assessments of the causes and
    drivers of the current outcome are often superficial. Moreover, the possible alternatives
    and outcomes in a hypothetical world without algorithms are missing. Therefore,
    the question of biased algorithms should not focus exclusively on the neutrality
    of algorithms, but also highlight the (missing) causes. The following analysis,
    therefore, aims to further explore issues related to bias and how today’s ethical
    pressure on developers could be understood as a collective responsibility. Nevertheless,
    it is crucial to underline the importance of eliminating discrimination in both
    technical and non-technical world, as the two are directly linked and influence
    each other:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 算法本身并没有偏见。然而，许多算法产生的结果是偏颇的，而对这些结果的原因和驱动因素的深入评估往往是肤浅的。此外，缺乏一个假设世界中没有算法的可能替代方案和结果。因此，对算法偏见的问题不应仅仅集中在算法的中立性上，还应强调（缺失的）原因。因此，以下分析旨在进一步探讨与偏见相关的问题，以及今天对开发者的伦理压力如何理解为一种集体责任。然而，必须强调消除技术和非技术世界中的歧视的重要性，因为这两者直接相关并相互影响。
- en: '**A biased society leads to biased data** By design, **discrimination takes
    place before the data is even collected, as datasets are historic collections
    that have been influenced by society itself**. To think about the technical aspects
    of this issue, one needs to examine the possible causes of biased systems. In
    most papers and studies, the authors state that the algorithms themselves are
    biased and cause discrimination.¹⁷ ¹⁸ However, it is difficult to ascribe the
    very cause of discrimination to algorithms themselves, since algorithms perform
    decision rules based on a given input and the interactions of given parameters
    in an automated way. ¹⁹ The rationale behind machine learning algorithms is usually
    an optimization problem where rules are applied to parameters based on their past
    effect on the outcome.²⁰ The presence of bias in the interaction between a given
    variable (e.g. gender) and the target (e.g. suitability for a vacancy) is therefore
    not caused by the algorithm itself, but by the underlying data set. Looking at
    the Amazon recruiting example, the biased outcome (i.e., favoring male applications
    vs. female) can be attributed to the training dataset initially fed to the algorithm.
    Assuming that the majority of former employees in technical positions were male,
    the algorithm anticipated this would make a success factor. So discrimination
    does not begin with the algorithm or its developer as an individual, but rather
    with the very hiring behavior of the company, which has been perpetrated for decades.'
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**一个有偏见的社会会导致有偏见的数据** 从设计上讲，**歧视在数据收集之前就已经发生，因为数据集是历史的集合，受到了社会本身的影响**。要深入思考这个问题的技术方面，需要检查可能导致偏见系统的原因。在大多数论文和研究中，作者指出算法本身存在偏见，导致了歧视。¹⁷
    ¹⁸ 然而，很难将歧视的根本原因归咎于算法本身，因为算法根据给定的输入和参数的交互以自动化方式执行决策规则。¹⁹ 机器学习算法背后的原理通常是一个优化问题，其中规则根据过去对结果的影响应用于参数。²⁰
    因此，在给定变量（例如性别）与目标（例如职位适合性）之间的交互中出现的偏见，并不是由算法本身造成的，而是由基础数据集造成的。以亚马逊招聘的例子来看，偏见的结果（即，男性申请者比女性申请者更受青睐）可以归因于最初输入到算法中的训练数据集。假设技术职位的前员工大多数是男性，算法就预期这会成为一个成功因素。因此，歧视并不是从算法或其开发者个人开始的，而是从公司长期以来的招聘行为开始的。'
- en: '**Hidden bias between variables** Provocatively, **one could argue that every
    dataset is biased to some extent since it originated in a likely biased environment**.
    An algorithm can only factor in what is present in the (biased) data, and data
    usually only contains what is present in the (biased) non-technical world. A reasonable
    and common-sense approach would be to remove such discriminatory variables from
    the dataset before beginning to model. However, no guarantee removing a variable
    will remove the presence or influence of that variable on the rest of the dataset.
    Let us imagine a fictitious dataset containing the variables *gender* and *sport*.
    Both variables could have an interaction, such as a “correlation between men and
    football and women and riding”, although this blunt example may not fully represent
    our society, since there may be women who prefer football to riding and vice versa.
    However, due to an unbalanced (e.g., male-heavy) dataset, it is likely that the
    algorithm indirectly favors candidates with a football preference over others,
    as the training process is based on biased information. This example shows that
    removing critical variables can reduce the risk of discrimination, but is not
    sufficient to prevent it. Moreover, it is of great importance how balanced the
    data sets represent the different expressions of a variable. A sensitive (discriminatory)
    item may be less problematic if the expressions are evenly distributed. If a subset
    of entries is underrepresented, the algorithm might factor this in negatively.
    Therefore, it is difficult to establish whether a dataset is truly unbiased.²¹'
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**变量之间的隐藏偏见** 挑衅地说，**有人可能会认为每个数据集都在某种程度上存在偏见，因为它源于一个可能存在偏见的环境**。算法只能考虑（偏见的）数据中存在的内容，而数据通常仅包含（偏见的）非技术世界中的内容。一种合理且符合常识的方法是，在开始建模之前，将这些歧视性变量从数据集中移除。然而，移除一个变量并不能保证消除该变量对数据集其余部分的存在或影响。我们可以设想一个虚拟的数据集，其中包含变量*性别*和*运动*。这两个变量可能存在互动，例如“男性与足球、女性与骑马之间的相关性”，尽管这个直白的例子可能不能完全代表我们的社会，因为可能有女性更喜欢足球而非骑马，反之亦然。然而，由于数据集的不平衡（例如男性偏多），算法可能会间接地偏向于有足球偏好的人，因为训练过程是基于偏见信息的。这个例子表明，移除关键变量可以降低歧视的风险，但不足以完全防止歧视。此外，数据集如何平衡地代表变量的不同表现也非常重要。如果表达均匀分布，敏感（歧视性）项目可能不那么问题严重。如果某一部分条目被低估，算法可能会在负面上考虑这一点。因此，很难确定一个数据集是否真正无偏。²¹'
- en: '**Technical decisions vs. human decisions**'
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**技术决策与人工决策**'
- en: '**There is no guarantee that humans make less biased decisions than algorithms**.
    In addition to the idea that any dataset can be biased and lead to biased results
    in an algorithmic process, another key indicator of unfair predictions/classifications
    is the lack of information, in the form of incomplete data.²² It is difficult
    to guarantee that a given dataset possesses full visibility to explain a target
    behavior. Common reasons for the lack of data may be limited access, active exclusion
    of information, or lack of knowledge about the marginal impact of additional information.
    Another risk for discrimination does not only result from the technical implementation
    but can also be rooted in the existing structures of our society. The advantage
    of Big Data in combination with machine learning algorithms is the possibility
    to apply human rules and decisions to a large amount of information. If algorithms
    are considered biased, one might wonder how humans would have processed the analysis
    of given data. Looking back to social media advertising, personalized advertising
    aims to make individual suggestions based on one’s preferences. This implies that
    algorithms support individualism. However, algorithms have the opposite effect,
    as they process automated and standardized decision rules based on general rather
    than individual patterns. Ultimately, it can be assumed that **algorithms support
    collectivism** rather than individualism.²³'
  id: totrans-33
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '**没有保证人类做出的决策比算法更少偏见**。除了任何数据集都可能存在偏见并导致算法过程中的偏见结果这一观点外，另一个关键的指标是不完整数据形式的信息缺乏。²²
    很难保证给定的数据集具有完整的可视性来解释目标行为。数据缺乏的常见原因可能是有限的访问权限、主动排除信息或对额外信息边际影响的缺乏了解。歧视的另一风险不仅源于技术实现，还可能根植于我们社会的现有结构。大数据与机器学习算法的结合优势在于能够将人类规则和决策应用于大量信息。如果算法被认为是有偏见的，人们可能会怀疑人类如何处理给定数据的分析。回顾社交媒体广告，个性化广告旨在根据个人的偏好做出个体化建议。这意味着算法支持个体主义。然而，算法却有相反的效果，因为它们基于一般模式而非个体模式处理自动化和标准化的决策规则。最终，可以认为**算法支持集体主义**而非个体主义。²³'
- en: Conclusion
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: Our analysis has shown that the bias in algorithms is mainly a result of bias
    in today’s society. The latter supports the collection of biased datasets, which
    may contain (hidden) relationships, ultimately leading to biased results. Furthermore,
    we question whether human decision-making is less biased compared to algorithmic
    rules. Lastly, we have concluded that more complex algorithms are often too non-transparent.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的分析表明，算法中的偏见主要是当今社会偏见的结果。后者支持收集带有偏见的数据集，这些数据集可能包含（隐藏的）关系，最终导致偏见结果。此外，我们质疑人类决策是否比算法规则更少偏见。最后，我们得出结论，更复杂的算法往往过于不透明。
- en: '![](../Images/14d1030419c35000674f8e36d30d092c.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/14d1030419c35000674f8e36d30d092c.png)'
- en: 'Image Credits: Pixabay'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：Pixabay
- en: '**Mitigation strategy**'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '**缓解策略**'
- en: In discussions about the responsibility for biased results of machine learning
    algorithms, the developer or the data science team is often blamed for discriminatory
    behavior. On one hand, this argument is overly simplistic, and the apportionment
    of the blame needs to be further differentiated. On the other, any developer working
    with machine learning algorithms should be aware of the sensitivity of their task
    and their respective responsibilities. Since algorithms support collectivism,
    it is important to investigate the fairness of these collective algorithms that
    make decisions or suggestions for individuals. Literature suggests other techniques
    to reduce bias, such as a system design that randomizes a small sample of queries
    or the use of propensity weighting techniques.²⁴ For bias in human-based content,
    there are several ideas for quantifying discrimination and assessing the fairness
    of algorithms.²⁵
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在关于机器学习算法偏见结果的责任讨论中，开发者或数据科学团队常常被指责为歧视性行为的源头。一方面，这种说法过于简单化，责任分配需要进一步区分。另一方面，任何使用机器学习算法的开发者都应意识到他们任务的敏感性及其相应的责任。由于算法支持集体主义，调查这些集体算法是否对个人做出决策或建议的公平性是重要的。文献建议了其他减少偏见的技术，例如设计一个对小样本查询进行随机化的系统，或使用倾向加权技术。²⁴
    对于基于人类内容的偏见，有几种量化歧视和评估算法公平性的思路。²⁵
- en: '**Fairness as a shared responsibility: both in the technical and real world**
    It is the responsibility of data teams to identify and minimize biased behavior
    in their data models. However, looking at the discriminatory factors examined
    in this article, it is a global responsibility to ensure that no one experiences
    such injustices, either through technical systems or human actions:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '**公平作为共同责任：在技术和现实世界中** 数据团队有责任识别和最小化其数据模型中的偏见行为。然而，鉴于本文所考察的歧视性因素，确保没有人通过技术系统或人为行为经历这种不公正是一项全球责任。'
- en: Humans must treat humans equally and without discrimination
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 人类必须平等对待他人，不带有歧视
- en: Algorithms must treat humans equally and without discrimination
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 算法必须平等对待人类，不带有歧视
- en: We should not hide behind algorithms to justify biased and discriminatory human
    behavior
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们不应借助算法来为偏见和歧视性的人类行为辩解。
- en: '**Outlook: now what?**'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '**展望：接下来怎么办？**'
- en: It seems important to evaluate decisions or predictions not only based on the
    outcome itself but also based on the ethical considerations involved in working
    out the solution. We also need to rethink systems that involve biased behavior.
    However, there is no evidence that the alternative (human-based decision-making)
    leads to more equality in general. Consequently, we should not only focus on the
    problems with bias *within* technology but rather broaden the discussion to include
    bias *outside* technology. Nevertheless, *precisely because of this*, **unbiased
    systems should help improve our current social weaknesses** and possibly lead
    to **more equality in the world to come.** People could use AI to more easily
    detect discrimination in the non-technological world. Because **wherever we find
    bias in the data, there is also bias in society**.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 评估决策或预测时，不仅应考虑结果本身，还需考虑在解决方案中涉及的伦理问题。我们还需要重新思考涉及偏见行为的系统。然而，没有证据表明替代方案（以人为基础的决策）会导致总体上的更多平等。因此，我们不应仅关注*技术内部*的偏见问题，而应将讨论扩大到*技术外部*的偏见问题。然而，*正因为如此*，**无偏见的系统应有助于改善我们当前的社会弱点**，并可能带来**未来世界中的更多平等**。人们可以利用AI更容易地检测非技术世界中的歧视。因为**只要我们在数据中发现偏见，社会中也存在偏见**。
- en: '[](https://medium.com/@jonas_dieckmann?source=post_page-----890091915aa3--------------------------------)
    [## Jonas Dieckmann - Medium'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '[Jonas Dieckmann - Medium](https://medium.com/@jonas_dieckmann?source=post_page-----890091915aa3--------------------------------) '
- en: Read writing from Jonas Dieckmann on Medium. analytics manager & product owner
    @ philips | passionate and writing about…
  id: totrans-47
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 阅读Jonas Dieckmann在Medium上的文章。分析经理兼产品负责人 @ Philips | 对…充满热情并撰写关于…
- en: medium.com](https://medium.com/@jonas_dieckmann?source=post_page-----890091915aa3--------------------------------)
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '[Jonas Dieckmann - Medium](https://medium.com/@jonas_dieckmann?source=post_page-----890091915aa3--------------------------------)'
- en: I hope you find it interesting. Let me know your thoughts and feel free to connect
    on LinkedIn [https://www.linkedin.com/in/jonas-dieckmann](https://www.linkedin.com/in/jonas-dieckmann/)
    and/or to follow me here on medium.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 希望你觉得这些内容有趣。请告诉我你的想法，并随时在LinkedIn [https://www.linkedin.com/in/jonas-dieckmann](https://www.linkedin.com/in/jonas-dieckmann/)
    上联系我，或者在medium上关注我。
- en: 'See also some of my other articles:'
  id: totrans-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 也可以查看我的其他文章：
- en: '[](/how-to-set-started-with-tensorflow-using-keras-api-and-google-colab-5421e5e4ef56?source=post_page-----890091915aa3--------------------------------)
    [## How to get started with TensorFlow using Keras API and Google Colab'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '[如何使用Keras API和Google Colab开始使用TensorFlow](https://towardsdatascience.com/how-to-set-started-with-tensorflow-using-keras-api-and-google-colab-5421e5e4ef56?source=post_page-----890091915aa3--------------------------------) '
- en: Step-by-step tutorial to analyze human activity with neuronal networks
  id: totrans-52
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用神经网络分析人类活动的逐步教程
- en: 'towardsdatascience.com](/how-to-set-started-with-tensorflow-using-keras-api-and-google-colab-5421e5e4ef56?source=post_page-----890091915aa3--------------------------------)
    [](/case-study-applying-a-data-science-process-model-to-a-real-world-scenario-93ae57b682bf?source=post_page-----890091915aa3--------------------------------)
    [## Case Study: Applying a Data Science Process Model to a Real-World Scenario'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '[案例研究：将数据科学过程模型应用于现实世界场景](https://towardsdatascience.com/how-to-set-started-with-tensorflow-using-keras-api-and-google-colab-5421e5e4ef56?source=post_page-----890091915aa3--------------------------------) '
- en: Development of a machine learning model for materials planning in the supply
    chain
  id: totrans-54
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 供应链中材料规划的机器学习模型开发
- en: towardsdatascience.com](/case-study-applying-a-data-science-process-model-to-a-real-world-scenario-93ae57b682bf?source=post_page-----890091915aa3--------------------------------)
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: References
  id: totrans-56
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[1]: Loukides, M., Mason, H., & Patil, D. (2018). Ethics and Data Science.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: '[2]: Floridi, L., & Taddeo, M. (2016). What is data ethics? Philosophical Transactions
    Of The Royal Society A: Mathematical, Physical, And Engineering Sciences, 374(2083),
    20160360\. doi: 10.1098/rsta.2016.0360'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: '[3]: Google (2021). AI FOR SOCIAL GOOD, Applying AI to some of the world’s
    biggest challenges Retrieved 10 March 2021, from [https://ai.google/social-good](https://ai.google/social-good)'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: '[4] Mittelstadt, B., & Floridi, L. (2015). The Ethics of Big Data: Current
    and Foreseeable Issues in Biomedical Contexts. Science And Engineering Ethics,
    22(2), 303–341\. DOI: 10.1007/s11948–015- 9652–2'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: '[5] Hand, D. (2018). Aspects of Data Ethics in a Changing World: Where Are
    We Now? Big Data, 6(3), 176–190\. DOI: 10.1089/big.2018.0083'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: '[6] Broad, E., Smith, A., & Wells, P. (2017). Helping organizations navigate
    ethical concerns in their data practices. Open Data Inst.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: '[7] Wu, Xiaolin & Zhang, Xi. (2016). Automated Inference on Criminality using
    Face Images.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: '[8] Sullivan, B. (2016). A New Program Judges If You’re a Criminal From Your
    Facial Features. Retrieved 10 March 2021, from [https://www.vice.com/en/article/d7ykmw/new-programdecides-criminality-from-facial-features](https://www.vice.com/en/article/d7ykmw/new-programdecides-criminality-from-facial-features)'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: '[9] Angwin, J., Larson, J., Mattu, S., & Kirchner, L. (2016). Machine Bias.
    Retrieved 28 February 2021, from [https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing](https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing)'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: '[10] Dastin, J. (2018). Amazon scraps secret AI recruiting tool that showed
    bias against women. Retrieved 28 February 2021, from [https://www.reuters.com/article/us-amazon-com-jobsautomation-insight-idUSKCN1MK08G](https://www.reuters.com/article/us-amazon-com-jobsautomation-insight-idUSKCN1MK08G)'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
- en: '[11] Ali, M., Sapiezynski, P., Bogen, M., Korolova, A., Mislove, A., & Rieke,
    A. (2019). Discrimination through Optimization. Proceedings Of The ACM On Human-Computer
    Interaction, 3(CSCW), 1–30\. DOI: 10.1145/3359301'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: '[12] Herdiana (2013), “5 Principles for Big Data Ethics,” Journal of Chemical
    Information and Modeling. Retrieved 10 March 2021, from: [https://aiforceos.com/2018/06/16/big-data-ethics](https://aiforceos.com/2018/06/16/big-data-ethics)'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: '[13] Canadian Human Rights Commission. What is Discrimination. Retrieved 28
    February 2021, from [https://www.chrc-ccdp.gc.ca/eng/content/what-discrimination](https://www.chrc-ccdp.gc.ca/eng/content/what-discrimination)'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: '[14] Hartfiel, G., & Hillmann, K. (1994). Dictionary of Sociology. Stuttgart:
    Kröner'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: '[15] Council of Europe (1950). European Convention for the Protection of Human
    Rights and Fundamental Freedoms. Retrieved 10 March 2021, from [www.refworld.org/docid/3ae6b3b04](http://www.refworld.org/docid/3ae6b3b04)'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '[15] 欧洲委员会（1950）。《保护人权和基本自由的欧洲公约》。检索日期：2021年3月10日，来自[www.refworld.org/docid/3ae6b3b04](http://www.refworld.org/docid/3ae6b3b04)'
- en: '[16] European Economic and Social Committee. (2017). The ethics of Big Data.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '[16] 欧洲经济和社会委员会。（2017）。大数据的伦理。'
- en: '[17] Eslami, M., Vaccaro, K., Karahalios, K. and Hamilton, K. (2017) ‘Be Careful;
    Things Can Be Worse than They Appear’: Understanding Biased Algorithms and Users’
    Behavior Around Them in Rating Platforms. Proceedings of the International AAAI
    Conference on Web and Social Media, 11(1).'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '[17] Eslami, M., Vaccaro, K., Karahalios, K. 和 Hamilton, K.（2017）‘小心；事情可能比看起来更糟’：理解评级平台中的偏见算法和用户行为。《国际AAAI网络与社会媒体会议论文集》，11(1)。'
- en: '[18] Kim, P. (2017) Auditing Algorithms for Discrimination, 166 U. Pa. L. Rev.
    Retrieved 10 March 2021, from scholarship.law.upenn.edu/penn_law_review_online/vol166/iss1/10'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '[18] Kim, P.（2017）审计算法以防歧视，166 U. Pa. L. Rev. 检索日期：2021年3月10日，来自scholarship.law.upenn.edu/penn_law_review_online/vol166/iss1/10'
- en: '[19] Mittelstadt, B., Allo, P., Taddeo, M., Wachter, S., & Floridi, L. (2016).
    The ethics of algorithms: Mapping the debate. Big Data & Society, 3(2), 205395171667967\.
    DOI: 10.1177/2053951716679679'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '[19] Mittelstadt, B., Allo, P., Taddeo, M., Wachter, S., & Floridi, L.（2016）。算法的伦理：辩论的映射。《大数据与社会》，3(2)，205395171667967。DOI:
    10.1177/2053951716679679'
- en: '[20] Lehr, D., Ohm, P. (2017). Playing with the Data: What Legal Scholars Should
    Learn About Machine Learning, UCDL Review 51: 653, 671.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '[20] Lehr, D., Ohm, P.（2017）。数据的游戏：法律学者应当了解的机器学习，《UCDL评论》51：653，671。'
- en: '[21] Calders, T., & Žliobaitė, I. (2013). Why Unbiased Computational Processes
    Can Lead to Discriminative Decision Procedures. Studies In Applied Philosophy,
    Epistemology, And Rational Ethics, 43–57\. DOI: 10.1007/978–3–642–30487–3_3'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '[21] Calders, T., & Žliobaitė, I.（2013）。为何无偏计算过程可能导致歧视性决策程序。《应用哲学、认识论与理性伦理研究》，43–57。DOI:
    10.1007/978–3–642–30487–3_3'
- en: '[22] Rosen, R. (2019). Missing Data and Imputation. Retrieved 10 March 2021,
    from [https://towardsdatascience.com/missing-data-and-imputation-89e9889268c8](/missing-data-and-imputation-89e9889268c8)'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '[22] Rosen, R.（2019）。缺失数据和插补。检索日期：2021年3月10日，来自[https://towardsdatascience.com/missing-data-and-imputation-89e9889268c8](/missing-data-and-imputation-89e9889268c8)'
- en: '[23] Saake. I., Nassehi, A. (2004). Die Kulturalisierung der Ethik. Eine zeitdiagnostische
    Anwendung des Luhmannschen Kulturbegriffs. Frankfurt/M: 102, 135'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '[23] Saake. I., Nassehi, A.（2004）。伦理的文化化。卢曼文化概念的时代诊断应用。法兰克福/美因：102，135'
- en: '[24] Ridgeway, G., Kovalchik, S., Griffin, B., & Kabeto, M. (2015). Propensity
    Score Analysis with Survey Weighted Data. Journal Of Causal Inference, 3(2), 237–249\.
    doi: 10.1515/jci-2014–0039'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '[24] Ridgeway, G., Kovalchik, S., Griffin, B., & Kabeto, M.（2015）。带权重数据的倾向得分分析。《因果推断期刊》，3(2)，237–249。doi:
    10.1515/jci-2014–0039'
- en: '[25] Floridi, L. (2016). Faultless responsibility: on the nature and allocation
    of moral responsibility for distributed moral actions. Philosophical Transactions
    Of The Royal Society A: Mathematical, Physical, And Engineering Sciences, 374(2083),
    20160112\. doi: 10.1098/rsta.2016.0112'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '[25] Floridi, L.（2016）。无过错责任：关于分布式道德行为的性质和道德责任的分配。《皇家学会A类：数学、物理和工程科学哲学交易》，374(2083)，20160112。doi:
    10.1098/rsta.2016.0112'
