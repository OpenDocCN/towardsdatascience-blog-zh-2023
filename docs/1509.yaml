- en: See What You Segment with SAM
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/see-what-you-sam-4eea9ad9a5de?source=collection_archive---------1-----------------------#2023-05-03](https://towardsdatascience.com/see-what-you-sam-4eea9ad9a5de?source=collection_archive---------1-----------------------#2023-05-03)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: How to generate and visualize Segment Anything Model predictions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@jacob_marks?source=post_page-----4eea9ad9a5de--------------------------------)[![Jacob
    Marks, Ph.D.](../Images/94d9832b8706d1044e3195386613bfab.png)](https://medium.com/@jacob_marks?source=post_page-----4eea9ad9a5de--------------------------------)[](https://towardsdatascience.com/?source=post_page-----4eea9ad9a5de--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----4eea9ad9a5de--------------------------------)
    [Jacob Marks, Ph.D.](https://medium.com/@jacob_marks?source=post_page-----4eea9ad9a5de--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ff7dc0c0eae92&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsee-what-you-sam-4eea9ad9a5de&user=Jacob+Marks%2C+Ph.D.&userId=f7dc0c0eae92&source=post_page-f7dc0c0eae92----4eea9ad9a5de---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----4eea9ad9a5de--------------------------------)
    ·10 min read·May 3, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F4eea9ad9a5de&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsee-what-you-sam-4eea9ad9a5de&user=Jacob+Marks%2C+Ph.D.&userId=f7dc0c0eae92&source=-----4eea9ad9a5de---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4eea9ad9a5de&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsee-what-you-sam-4eea9ad9a5de&source=-----4eea9ad9a5de---------------------bookmark_footer-----------)![](../Images/69d09f042372384691aca27cf4873995.png)'
  prefs: []
  type: TYPE_NORMAL
- en: '*Segmentation of Open Images V7 (*[*license*](https://github.com/openimages/dataset/blob/main/LICENSE)*)
    samples with Meta AI’s Segment Anything Model (*[*license*](https://github.com/facebookresearch/segment-anything/blob/main/LICENSE)*).
    Image courtesy of the author.*'
  prefs: []
  type: TYPE_NORMAL
- en: Over the past few weeks, Meta AI Research’s general purpose image segmentation
    model has attracted a lot of attention. The model, aptly named [Segment Anything
    Model (SAM)](https://github.com/facebookresearch/segment-anything) ([Apache license
    2.0](https://github.com/facebookresearch/segment-anything/blob/main/LICENSE)),
    was trained on a dataset consisting of 11 million images and more than a billion
    segmentation masks.
  prefs: []
  type: TYPE_NORMAL
- en: SAM is remarkably powerful. But as always, before deploying a model in production,
    you need to understand how the model performs on your dataset. In the context
    of computer vision, a crucial element in this equation is visualizing model predictions.
  prefs: []
  type: TYPE_NORMAL
- en: 'This blog post is designed to help you get up and running with SAM: we’ll walk
    you through how to use SAM to add segmentation masks to your dataset, and how
    to systematically visualize these segmentation masks across the entire dataset.
    By visualizing (and evaluating) these predictions, we can better understand how
    SAM fares on our dataset, its limitations, and the potential downstream impacts
    of integrating the model into our pipelines.'
  prefs: []
  type: TYPE_NORMAL
- en: 'SAM provides multiple avenues for generating segmentation masks:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Automatic: it just *works*, without any prompts or hints'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'From bounding box: given a bounding box, SAM segments the bounded object'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'From points: given point labels, which can be positive or negative SAM infers
    the area to be segmented'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'From points and boxes: you can provide both points *and* bounding boxes to
    improve performance'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Below, we will explicitly go through the first three. The post will be structured
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Setup](#dc9a)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Auto-segmentation with SAM](#58c1)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Semantic segmentation with SAM](#23ee)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Instance segmentation with SAM](#37d5)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Setup
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Installation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This tutorial requires `python≥3.8`, `pytorch≥1.7` and `torchvision≥0.8`. If
    you don’t have Torch or Torchvision installed, run:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Additionally, we will be using the open source computer vision library [FiftyOne](https://github.com/voxel51/fiftyone),
    for loading datasets and visualizing predictions. If you don’t have FiftyOne installed,
    you can run:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'In order to use SAM, you can install the [Segment Anything library](https://github.com/facebookresearch/segment-anything)
    from source, with:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: You will then be able to import the library as `segment_anything`.
  prefs: []
  type: TYPE_NORMAL
- en: After that, download a [model checkpoint](https://github.com/facebookresearch/segment-anything#model-checkpoints).
    For this walkthrough, we will be using the default [ViT-H SAM model](https://huggingface.co/facebook/sam-vit-huge),
    i.e. the “huge” vision transformer Segment Anything Model. If you’d prefer, you
    can instead use the large (ViT-L SAM) or base (ViT-B SAM) model.
  prefs: []
  type: TYPE_NORMAL
- en: Importing modules
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Here is the header code we will need to import all of the modules we will use:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Defining constants
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We can also define some elements that will not change across all of our segmentation
    applications:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Loading the dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For this tutorial, we will be using images from Google’s [Open Images V7](https://storage.googleapis.com/openimages/web/index.html)
    ([Apache license 2.0](https://github.com/openimages/dataset/blob/main/LICENSE))
    dataset. The dataset already has instance segmentation masks for many images,
    but for the sake of illustration, we will only load in point labels and object
    detection bounding boxes. For a comprehensive tutorial on how to work with point
    labels in Open Images V7, check out [this Medium post](https://medium.com/voxel51/exploring-googles-open-images-v7-a6218d0098cb).
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s load in 100 random images from the validation split:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'We will name the dataset and make it persistent. Additionally, we will store
    the image width and height in pixels by running `compute_metadata()`, so that
    we can use this information to convert between absolute and relative coordinates:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'This is what the dataset looks like, before we start adding in SAM predictions:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d906fa04c0f8dac283c99ab15cc42fdc.png)'
  prefs: []
  type: TYPE_IMG
- en: '*Images from Open Images V7 visualized in the FiftyOne App. Image courtesy
    of the author.*'
  prefs: []
  type: TYPE_NORMAL
- en: Auto-segmentation with SAM
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you don’t have any existing keypoints or bounding boxes with which to guide
    the Segment Anything Model, you can use “auto-segmentation” functionality to generate
    segmentation masks for any [things and stuff](https://arxiv.org/abs/1612.03716)
    in an image. This is done via the `SamAutomaticMaskGenerator` class. Note that
    this is not panoptic segmentation, as the masks are not labeled.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can instantiate a `SamAutomaticMaskGenerator` object, setting the intersection
    over union (IoU) threshold, minimum area of a returned mask, and other parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: For a complete list of allowable parameters, see [this SAM notebook](https://github.com/facebookresearch/segment-anything/blob/main/notebooks/automatic_mask_generator_example.ipynb).
  prefs: []
  type: TYPE_NORMAL
- en: 'Given a sample (image located at `sample.filepath`), we can generate masks
    by reading the image with Pillow and calling the `generate()` method of our `SamAutomaticMaskGenerator`
    object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: These masks contain 2D “segmentation” arrays, but no labels. If we wanted labels
    as well, we could use a library like [Semantic Segment Anything](https://github.com/fudan-zvg/Semantic-Segment-Anything).
    For the sake of simplicity, we will just show you how to combine all of these
    into a full image mask, assigning a different color to each of the individual
    masks returned by our mask generator.
  prefs: []
  type: TYPE_NORMAL
- en: To add an “automatic” segmentation mask to a single sample, we can pass the
    image associated with that sample to our mask generator. Then for each of the
    masks that is returned, we can add that mask to our full image mask, multiplying
    by a unique number so that the display color is unique to that sub-mask. We can
    then store this complete image mask as a `Segmentation` label object on our dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is encompassed in the function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: The addition step is valid here as long as you set `crop_n_layers=1` when defining
    the mask generator. This code will work for up to 256 unique sub-masks.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will loop through the samples in our dataset, saving each sample as we go:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'When we visualize the results in the FiftyOne App, this is what we see:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/69d09f042372384691aca27cf4873995.png)'
  prefs: []
  type: TYPE_IMG
- en: '*Automatic segmentation of Open Images V7 samples with Meta AI’s Segment Anything
    Model. Image courtesy of the author.*'
  prefs: []
  type: TYPE_NORMAL
- en: Looking at these automatically generated masks, we can see that there are quite
    a few tiny blobs that are not particularly meaningful to us. When we defined our
    mask generator, we set the minimum mask region area to 400 pixels. If we were
    going to use this method as part of a larger pipeline, we may need to consider
    increasing this minimum requirement, or using different minima for certain images,
    depending on the number of pixels in the image.
  prefs: []
  type: TYPE_NORMAL
- en: Semantic segmentation with SAM
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you have point labels (keypoints) on the images in your dataset, then you
    can use these point labels to prompt the SAM model. This is true for both positive
    and negative point labels! This section will show you how to do just that.
  prefs: []
  type: TYPE_NORMAL
- en: In FiftyOne, point labels are represented as `Keypoint` objects. In Open Images
    V7, each individual point displayed on an image is stored in its own `Keypoint`
    object within the “points” field, because it carries along additional information.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can access the contents of these point labels for a given sample via the
    `keypoints` attribute. For instance, to get the first point label for the first
    sample in our dataset, we can use:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: This point is a negative label (`estimated_yes_no` field) for the class `Rope`,
    a result determined by the number of individual `yes` and `no` votes. Throughout
    the Open Images V7 dataset, point labels have `estimated_yes_no` in `(“yes”, “no",
    “unsure”)`. We will ignore the `unsure` points (which represent a very small fraction
    of total point labels) and focus on the high certainty points.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s instantiate a SAM predictor model, which we will use for both semantic
    and instance segmentation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: To prime the predictor, we are going to pass in information about the point
    labels in the image via the `point_coords` and `point_labels` arguments.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `SamPredictor` expects `point_coords` in absolute coordinates, whereas
    FiftyOne stores points in relative coordinates. Additionally, `point_labels` accepts
    arrays of `0`’s and `1`’s, so we will convert from `[yes, no]`. The following
    function takes in the list of point labels for a given image, and a label class,
    plus image width and height, and returns the `point_coords` and `point_labels`
    for all relevant points:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'For a single sample, we can add a SAM semantic segmentation mask with the function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Here, `n2i` is a dictionary mapping class name to integer value with which to
    populate the segmentation mask. It is also worth noting that with `multimask_output=True`,
    the predictor returns multiple guesses at segmentation masks for each input. We
    select the highest confidence prediction (maximum `score`).
  prefs: []
  type: TYPE_NORMAL
- en: 'Looping over the samples in our dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'We can generate segmentation masks for our dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3f2d7f7a665af436ad38e6fdc04ed93a.png)'
  prefs: []
  type: TYPE_IMG
- en: '*Semantic segmentation of Open Images V7 samples with Meta AI’s Segment Anything
    Model. Image courtesy of the author.*'
  prefs: []
  type: TYPE_NORMAL
- en: Of course, not *everything* is semantically segmented here, as the images contain
    somewhat sparse point labels. Adding more points to the initial data would result
    in denser semantic segmentation masks for the images in the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: We can also see that while SAM does a pretty good job across the entire dataset,
    it struggles to appropriately segment the motorcycle’s wheels.
  prefs: []
  type: TYPE_NORMAL
- en: Instance segmentation with SAM
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'If you already have bounding boxes for objects in your dataset, you can prompt
    the SAM model with these bounding boxes and generate segmentation masks for these
    objects! Here’s how:'
  prefs: []
  type: TYPE_NORMAL
- en: 'As with point labels, we will need to convert bounding boxes from relative
    to absolute coordinates. In FiftyOne, bounding boxes are stored in `[<top-left-x>,
    <top-left-y>, <width>, <height>]` format, with coordinates in `[0,1]`. On the
    other hand, SAM bounding boxes are `[<top-left-x>, <top-left-y>, <top-right-x>,
    <top-right-y>]`, absolute coordinates. The following function will perform the
    conversion for us:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Once we have generated an instance segmentation mask with SAM for a given object
    detection, we can add the mask to the detection object with:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'To add instance segmentation masks to an image, we loop through all object
    detections, using the `SamPredictor` object with each detection’s bounding box,
    and adding the resulting mask to the FiftyOne `Detection` object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'For instance segmentation, extending this to the entire dataset is trivial:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Coloring by label, we get something that looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ec63cc352ddba57be6d72b4cdba891c2.png)'
  prefs: []
  type: TYPE_IMG
- en: '*Instance segmentation of Open Images V7 samples with Meta AI’s Segment Anything
    Model. Image courtesy of the author.*'
  prefs: []
  type: TYPE_NORMAL
- en: '*Note: for better efficiency, you can also batch these predictions!*'
  prefs: []
  type: TYPE_NORMAL
- en: Panoptic segmentation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'If you wanted to use SAM to panoptically segment your dataset, you could combine
    keypoint and bounding box approaches in the following way:'
  prefs: []
  type: TYPE_NORMAL
- en: '**For each bounded object, or *thing*, such as a car or a table:**'
  prefs: []
  type: TYPE_NORMAL
- en: Generate a bounding box around the object, either via traditional annotation,
    or [Grounding DINO](https://github.com/IDEA-Research/GroundingDINO), or some other
    method.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select the center of the bounding box as the default keypoint for that object.
    If this turns out to not be inside the object, adjust accordingly.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use these keypoints and bounding boxes to compute instance segmentation masks
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**For each contiguous region of *stuff* (for instance, sky or grass):**'
  prefs: []
  type: TYPE_NORMAL
- en: Add one or multiple labeled keypoints.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute semantic segmentation masks using these keypoints
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Fill in the gaps:**'
  prefs: []
  type: TYPE_NORMAL
- en: Given all of the instance and semantic segmentation masks, identify regions
    of overlapping coverage and regions without any mask.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Address these regions with whatever strategy suits your application best.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Meta AI’s Segment Anything Model is incredibly powerful and versatile. That
    being said, SAM is just one of many exciting advances in the areas of segmentation
    and prompted/guided computer vision. The field is moving incredibly fast! If you
    are interested in learning more, I’d encourage you to check out the following
    related projects:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Track Anything](https://github.com/gaomingqi/track-anything): video segmentation
    — built on top of SAM'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[MedSAM](https://github.com/bowang-lab/medsam): Segment Anything for medical
    images'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Inpaint Anything](https://github.com/geekyutao/inpaint-anything): Segment
    Anything + Inpainting'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Semantic Segment Anything](https://github.com/fudan-zvg/Semantic-Segment-Anything):
    SAM + semantic labeling engine'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Segment Everything Everywhere All at Once](https://github.com/ux-decoder/segment-everything-everywhere-all-at-once):
    similar to SAM, and supports multi-modal prompts'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sources
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[Segment Anything Model](https://github.com/facebookresearch/segment-anything/blob/main/LICENSE)
    ([license](https://github.com/facebookresearch/segment-anything/blob/main/LICENSE))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Open Images V7](https://github.com/openimages/dataset/blob/main/LICENSE) ([license](https://github.com/openimages/dataset/blob/main/LICENSE))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
