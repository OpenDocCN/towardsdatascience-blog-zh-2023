- en: See What You Segment with SAM
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 查看你使用 SAM 的分割效果
- en: 原文：[https://towardsdatascience.com/see-what-you-sam-4eea9ad9a5de?source=collection_archive---------1-----------------------#2023-05-03](https://towardsdatascience.com/see-what-you-sam-4eea9ad9a5de?source=collection_archive---------1-----------------------#2023-05-03)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/see-what-you-sam-4eea9ad9a5de?source=collection_archive---------1-----------------------#2023-05-03](https://towardsdatascience.com/see-what-you-sam-4eea9ad9a5de?source=collection_archive---------1-----------------------#2023-05-03)
- en: How to generate and visualize Segment Anything Model predictions
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何生成和可视化 Segment Anything Model 的预测结果
- en: '[](https://medium.com/@jacob_marks?source=post_page-----4eea9ad9a5de--------------------------------)[![Jacob
    Marks, Ph.D.](../Images/94d9832b8706d1044e3195386613bfab.png)](https://medium.com/@jacob_marks?source=post_page-----4eea9ad9a5de--------------------------------)[](https://towardsdatascience.com/?source=post_page-----4eea9ad9a5de--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----4eea9ad9a5de--------------------------------)
    [Jacob Marks, Ph.D.](https://medium.com/@jacob_marks?source=post_page-----4eea9ad9a5de--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@jacob_marks?source=post_page-----4eea9ad9a5de--------------------------------)[![Jacob
    Marks, Ph.D.](../Images/94d9832b8706d1044e3195386613bfab.png)](https://medium.com/@jacob_marks?source=post_page-----4eea9ad9a5de--------------------------------)[](https://towardsdatascience.com/?source=post_page-----4eea9ad9a5de--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----4eea9ad9a5de--------------------------------)
    [Jacob Marks, Ph.D.](https://medium.com/@jacob_marks?source=post_page-----4eea9ad9a5de--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ff7dc0c0eae92&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsee-what-you-sam-4eea9ad9a5de&user=Jacob+Marks%2C+Ph.D.&userId=f7dc0c0eae92&source=post_page-f7dc0c0eae92----4eea9ad9a5de---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----4eea9ad9a5de--------------------------------)
    ·10 min read·May 3, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F4eea9ad9a5de&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsee-what-you-sam-4eea9ad9a5de&user=Jacob+Marks%2C+Ph.D.&userId=f7dc0c0eae92&source=-----4eea9ad9a5de---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ff7dc0c0eae92&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsee-what-you-sam-4eea9ad9a5de&user=Jacob+Marks%2C+Ph.D.&userId=f7dc0c0eae92&source=post_page-f7dc0c0eae92----4eea9ad9a5de---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----4eea9ad9a5de--------------------------------)
    ·10 分钟阅读·2023年5月3日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F4eea9ad9a5de&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsee-what-you-sam-4eea9ad9a5de&user=Jacob+Marks%2C+Ph.D.&userId=f7dc0c0eae92&source=-----4eea9ad9a5de---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4eea9ad9a5de&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsee-what-you-sam-4eea9ad9a5de&source=-----4eea9ad9a5de---------------------bookmark_footer-----------)![](../Images/69d09f042372384691aca27cf4873995.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4eea9ad9a5de&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsee-what-you-sam-4eea9ad9a5de&source=-----4eea9ad9a5de---------------------bookmark_footer-----------)![](../Images/69d09f042372384691aca27cf4873995.png)'
- en: '*Segmentation of Open Images V7 (*[*license*](https://github.com/openimages/dataset/blob/main/LICENSE)*)
    samples with Meta AI’s Segment Anything Model (*[*license*](https://github.com/facebookresearch/segment-anything/blob/main/LICENSE)*).
    Image courtesy of the author.*'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '*使用 Meta AI 的 Segment Anything Model (*[*许可证*](https://github.com/openimages/dataset/blob/main/LICENSE)*)
    对 Open Images V7 的样本进行分割 (*[*许可证*](https://github.com/facebookresearch/segment-anything/blob/main/LICENSE)*)。图片由作者提供。*'
- en: Over the past few weeks, Meta AI Research’s general purpose image segmentation
    model has attracted a lot of attention. The model, aptly named [Segment Anything
    Model (SAM)](https://github.com/facebookresearch/segment-anything) ([Apache license
    2.0](https://github.com/facebookresearch/segment-anything/blob/main/LICENSE)),
    was trained on a dataset consisting of 11 million images and more than a billion
    segmentation masks.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去的几周里，Meta AI Research 的通用图像分割模型吸引了大量关注。这个模型，名为 [Segment Anything Model (SAM)](https://github.com/facebookresearch/segment-anything)
    ([Apache 许可证 2.0](https://github.com/facebookresearch/segment-anything/blob/main/LICENSE))，是在一个包含
    1100 万张图像和超过十亿个分割掩码的数据集上进行训练的。
- en: SAM is remarkably powerful. But as always, before deploying a model in production,
    you need to understand how the model performs on your dataset. In the context
    of computer vision, a crucial element in this equation is visualizing model predictions.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: SAM功能强大。但像往常一样，在将模型投入生产之前，你需要了解模型在你的数据集上的表现。在计算机视觉的背景下，关键因素之一是可视化模型预测。
- en: 'This blog post is designed to help you get up and running with SAM: we’ll walk
    you through how to use SAM to add segmentation masks to your dataset, and how
    to systematically visualize these segmentation masks across the entire dataset.
    By visualizing (and evaluating) these predictions, we can better understand how
    SAM fares on our dataset, its limitations, and the potential downstream impacts
    of integrating the model into our pipelines.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 本博文旨在帮助你快速上手SAM：我们将引导你如何使用SAM将分割掩码添加到你的数据集中，以及如何系统地可视化整个数据集中的这些分割掩码。通过可视化（和评估）这些预测，我们可以更好地了解SAM在我们数据集上的表现、其局限性以及将模型集成到我们管道中的潜在影响。
- en: 'SAM provides multiple avenues for generating segmentation masks:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: SAM提供了多种生成分割掩码的途径：
- en: 'Automatic: it just *works*, without any prompts or hints'
  id: totrans-13
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 自动：它*自动工作*，无需任何提示或提示
- en: 'From bounding box: given a bounding box, SAM segments the bounded object'
  id: totrans-14
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从边界框：给定一个边界框，SAM对边界内的对象进行分割
- en: 'From points: given point labels, which can be positive or negative SAM infers
    the area to be segmented'
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从点：给定点标签，可能是正数或负数，SAM推断需要分割的区域。
- en: 'From points and boxes: you can provide both points *and* bounding boxes to
    improve performance'
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从点和框：你可以提供点*和*边界框来提高性能
- en: 'Below, we will explicitly go through the first three. The post will be structured
    as follows:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将明确介绍前三个。本文将按以下结构组织：
- en: '[Setup](#dc9a)'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[设置](#dc9a)'
- en: '[Auto-segmentation with SAM](#58c1)'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用SAM进行自动分割](#58c1)'
- en: '[Semantic segmentation with SAM](#23ee)'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用SAM进行语义分割](#23ee)'
- en: '[Instance segmentation with SAM](#37d5)'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用SAM进行实例分割](#37d5)'
- en: Setup
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设置
- en: Installation
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 安装
- en: 'This tutorial requires `python≥3.8`, `pytorch≥1.7` and `torchvision≥0.8`. If
    you don’t have Torch or Torchvision installed, run:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 本教程要求`python≥3.8`，`pytorch≥1.7`和`torchvision≥0.8`。如果你没有安装Torch或Torchvision，请运行：
- en: '[PRE0]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Additionally, we will be using the open source computer vision library [FiftyOne](https://github.com/voxel51/fiftyone),
    for loading datasets and visualizing predictions. If you don’t have FiftyOne installed,
    you can run:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们将使用开源计算机视觉库[FiftyOne](https://github.com/voxel51/fiftyone)来加载数据集和可视化预测。如果你没有安装FiftyOne，可以运行：
- en: '[PRE1]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'In order to use SAM, you can install the [Segment Anything library](https://github.com/facebookresearch/segment-anything)
    from source, with:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使用SAM，你可以从源代码安装[Segment Anything库](https://github.com/facebookresearch/segment-anything)，使用：
- en: '[PRE2]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: You will then be able to import the library as `segment_anything`.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，你将能够将库导入为`segment_anything`。
- en: After that, download a [model checkpoint](https://github.com/facebookresearch/segment-anything#model-checkpoints).
    For this walkthrough, we will be using the default [ViT-H SAM model](https://huggingface.co/facebook/sam-vit-huge),
    i.e. the “huge” vision transformer Segment Anything Model. If you’d prefer, you
    can instead use the large (ViT-L SAM) or base (ViT-B SAM) model.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，下载一个[模型检查点](https://github.com/facebookresearch/segment-anything#model-checkpoints)。在这次演练中，我们将使用默认的[ViT-H
    SAM模型](https://huggingface.co/facebook/sam-vit-huge)，即“huge”视觉变换器分割模型。如果你愿意，你也可以使用大型（ViT-L
    SAM）或基础（ViT-B SAM）模型。
- en: Importing modules
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 导入模块
- en: 'Here is the header code we will need to import all of the modules we will use:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们需要导入所有模块的头部代码：
- en: '[PRE3]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Defining constants
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 定义常量
- en: 'We can also define some elements that will not change across all of our segmentation
    applications:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以定义一些在所有分割应用中不会改变的元素：
- en: '[PRE4]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Loading the dataset
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 加载数据集
- en: For this tutorial, we will be using images from Google’s [Open Images V7](https://storage.googleapis.com/openimages/web/index.html)
    ([Apache license 2.0](https://github.com/openimages/dataset/blob/main/LICENSE))
    dataset. The dataset already has instance segmentation masks for many images,
    but for the sake of illustration, we will only load in point labels and object
    detection bounding boxes. For a comprehensive tutorial on how to work with point
    labels in Open Images V7, check out [this Medium post](https://medium.com/voxel51/exploring-googles-open-images-v7-a6218d0098cb).
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 对于本教程，我们将使用来自Google的[Open Images V7](https://storage.googleapis.com/openimages/web/index.html)（[Apache许可证2.0](https://github.com/openimages/dataset/blob/main/LICENSE)）数据集的图像。数据集已经为许多图像准备了实例分割掩码，但为了说明，我们只会加载点标签和目标检测边界框。关于如何处理Open
    Images V7中的点标签的全面教程，请查看[此Medium文章](https://medium.com/voxel51/exploring-googles-open-images-v7-a6218d0098cb)。
- en: 'Let’s load in 100 random images from the validation split:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从验证集中加载100个随机图像：
- en: '[PRE5]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'We will name the dataset and make it persistent. Additionally, we will store
    the image width and height in pixels by running `compute_metadata()`, so that
    we can use this information to convert between absolute and relative coordinates:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将命名数据集并使其持久化。此外，我们还将通过运行`compute_metadata()`将图像的宽度和高度存储为像素，以便我们可以使用这些信息在绝对坐标和相对坐标之间进行转换：
- en: '[PRE6]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'This is what the dataset looks like, before we start adding in SAM predictions:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始添加SAM预测之前，数据集的外观如下所示：
- en: '![](../Images/d906fa04c0f8dac283c99ab15cc42fdc.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d906fa04c0f8dac283c99ab15cc42fdc.png)'
- en: '*Images from Open Images V7 visualized in the FiftyOne App. Image courtesy
    of the author.*'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '*Open Images V7中的图像在FiftyOne App中可视化。图像由作者提供。*'
- en: Auto-segmentation with SAM
  id: totrans-47
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用SAM进行自动分割
- en: If you don’t have any existing keypoints or bounding boxes with which to guide
    the Segment Anything Model, you can use “auto-segmentation” functionality to generate
    segmentation masks for any [things and stuff](https://arxiv.org/abs/1612.03716)
    in an image. This is done via the `SamAutomaticMaskGenerator` class. Note that
    this is not panoptic segmentation, as the masks are not labeled.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您没有任何现有的关键点或边界框来指导Segment Anything模型，您可以使用“自动分割”功能为图像中的任何[对象和物品](https://arxiv.org/abs/1612.03716)生成分割掩码。这是通过`SamAutomaticMaskGenerator`类完成的。请注意，这不是全景分割，因为掩码没有标注。
- en: 'You can instantiate a `SamAutomaticMaskGenerator` object, setting the intersection
    over union (IoU) threshold, minimum area of a returned mask, and other parameters:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以实例化一个`SamAutomaticMaskGenerator`对象，并设置交并比（IoU）阈值、返回的掩码的最小面积和其他参数：
- en: '[PRE7]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: For a complete list of allowable parameters, see [this SAM notebook](https://github.com/facebookresearch/segment-anything/blob/main/notebooks/automatic_mask_generator_example.ipynb).
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 有关可接受参数的完整列表，请参阅[此SAM笔记本](https://github.com/facebookresearch/segment-anything/blob/main/notebooks/automatic_mask_generator_example.ipynb)。
- en: 'Given a sample (image located at `sample.filepath`), we can generate masks
    by reading the image with Pillow and calling the `generate()` method of our `SamAutomaticMaskGenerator`
    object:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个样本（位于`sample.filepath`的图像），我们可以使用Pillow读取图像，并调用我们的`SamAutomaticMaskGenerator`对象的`generate()`方法生成掩码：
- en: '[PRE8]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: These masks contain 2D “segmentation” arrays, but no labels. If we wanted labels
    as well, we could use a library like [Semantic Segment Anything](https://github.com/fudan-zvg/Semantic-Segment-Anything).
    For the sake of simplicity, we will just show you how to combine all of these
    into a full image mask, assigning a different color to each of the individual
    masks returned by our mask generator.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 这些掩码包含2D的“分割”数组，但没有标签。如果我们还想要标签，我们可以使用类似[语义分割工具](https://github.com/fudan-zvg/Semantic-Segment-Anything)的库。为简单起见，我们将向您展示如何将所有这些组合成一个完整的图像掩码，并为我们的掩码生成器返回的每个单独的掩码分配一个不同的颜色。
- en: To add an “automatic” segmentation mask to a single sample, we can pass the
    image associated with that sample to our mask generator. Then for each of the
    masks that is returned, we can add that mask to our full image mask, multiplying
    by a unique number so that the display color is unique to that sub-mask. We can
    then store this complete image mask as a `Segmentation` label object on our dataset.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 要为单个样本添加“自动”分割掩码，我们可以将与该样本关联的图像传递给我们的掩码生成器。然后对于返回的每个掩码，我们可以将该掩码添加到完整的图像掩码中，乘以一个唯一的数字，使得显示颜色对应于该子掩码。然后，我们可以将这个完整的图像掩码作为`Segmentation`标签对象存储在我们的数据集中。
- en: 'This is encompassed in the function:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 这包含在以下函数中：
- en: '[PRE9]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The addition step is valid here as long as you set `crop_n_layers=1` when defining
    the mask generator. This code will work for up to 256 unique sub-masks.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 只要在定义掩码生成器时设置`crop_n_layers=1`，添加步骤就是有效的。此代码可以处理最多256个唯一子掩码。
- en: 'We will loop through the samples in our dataset, saving each sample as we go:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将遍历数据集中的样本，并在此过程中保存每个样本：
- en: '[PRE10]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'When we visualize the results in the FiftyOne App, this is what we see:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们在FiftyOne App中可视化结果时，我们看到的是：
- en: '![](../Images/69d09f042372384691aca27cf4873995.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/69d09f042372384691aca27cf4873995.png)'
- en: '*Automatic segmentation of Open Images V7 samples with Meta AI’s Segment Anything
    Model. Image courtesy of the author.*'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '*Meta AI的Segment Anything Model对Open Images V7样本的自动分割。图片由作者提供。*'
- en: Looking at these automatically generated masks, we can see that there are quite
    a few tiny blobs that are not particularly meaningful to us. When we defined our
    mask generator, we set the minimum mask region area to 400 pixels. If we were
    going to use this method as part of a larger pipeline, we may need to consider
    increasing this minimum requirement, or using different minima for certain images,
    depending on the number of pixels in the image.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 观察这些自动生成的掩码，我们可以看到有一些很小的斑点对我们并没有特别的意义。当我们定义掩码生成器时，我们将最小掩码区域面积设置为400像素。如果我们将此方法用作更大管道的一部分，我们可能需要考虑增加这一最小要求，或者根据图像中的像素数量为某些图像使用不同的最小值。
- en: Semantic segmentation with SAM
  id: totrans-65
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用SAM进行语义分割
- en: If you have point labels (keypoints) on the images in your dataset, then you
    can use these point labels to prompt the SAM model. This is true for both positive
    and negative point labels! This section will show you how to do just that.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你的数据集中图像上有点标签（关键点），你可以使用这些点标签来提示SAM模型。这对于正负点标签都是适用的！本节将向你展示如何做到这一点。
- en: In FiftyOne, point labels are represented as `Keypoint` objects. In Open Images
    V7, each individual point displayed on an image is stored in its own `Keypoint`
    object within the “points” field, because it carries along additional information.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在FiftyOne中，点标签表示为`Keypoint`对象。在Open Images V7中，图像上显示的每个单独点都存储在“points”字段中的自己的`Keypoint`对象中，因为它携带了额外的信息。
- en: 'We can access the contents of these point labels for a given sample via the
    `keypoints` attribute. For instance, to get the first point label for the first
    sample in our dataset, we can use:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过`keypoints`属性访问给定样本的点标签内容。例如，要获取数据集中第一个样本的第一个点标签，我们可以使用：
- en: '[PRE11]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '[PRE12]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: This point is a negative label (`estimated_yes_no` field) for the class `Rope`,
    a result determined by the number of individual `yes` and `no` votes. Throughout
    the Open Images V7 dataset, point labels have `estimated_yes_no` in `(“yes”, “no",
    “unsure”)`. We will ignore the `unsure` points (which represent a very small fraction
    of total point labels) and focus on the high certainty points.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 这个点是类别`Rope`的负标签（`estimated_yes_no`字段），其结果由单个`yes`和`no`投票数决定。在Open Images V7数据集中，点标签有`estimated_yes_no`值`("yes",
    "no", "unsure")`。我们将忽略`unsure`点（这仅占总点标签的一小部分），并关注高确定性的点。
- en: 'Let’s instantiate a SAM predictor model, which we will use for both semantic
    and instance segmentation:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们实例化一个SAM预测器模型，用于语义和实例分割：
- en: '[PRE13]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: To prime the predictor, we are going to pass in information about the point
    labels in the image via the `point_coords` and `point_labels` arguments.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 为了初始化预测器，我们将通过`point_coords`和`point_labels`参数传递图像中的点标签信息。
- en: 'The `SamPredictor` expects `point_coords` in absolute coordinates, whereas
    FiftyOne stores points in relative coordinates. Additionally, `point_labels` accepts
    arrays of `0`’s and `1`’s, so we will convert from `[yes, no]`. The following
    function takes in the list of point labels for a given image, and a label class,
    plus image width and height, and returns the `point_coords` and `point_labels`
    for all relevant points:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '`SamPredictor`期望`point_coords`使用绝对坐标，而FiftyOne存储相对坐标。此外，`point_labels`接受`0`和`1`的数组，所以我们将从`[yes,
    no]`转换过来。以下函数接受给定图像的点标签列表、标签类别、图像宽度和高度，并返回所有相关点的`point_coords`和`point_labels`：'
- en: '[PRE14]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'For a single sample, we can add a SAM semantic segmentation mask with the function:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 对于单个样本，我们可以使用以下函数添加SAM语义分割掩码：
- en: '[PRE15]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Here, `n2i` is a dictionary mapping class name to integer value with which to
    populate the segmentation mask. It is also worth noting that with `multimask_output=True`,
    the predictor returns multiple guesses at segmentation masks for each input. We
    select the highest confidence prediction (maximum `score`).
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，`n2i` 是一个字典，将类名映射到整数值，用于填充分割掩码。值得注意的是，当`multimask_output=True`时，预测器会为每个输入返回多个分割掩码的猜测。我们选择置信度最高的预测（最大
    `score`）。
- en: 'Looping over the samples in our dataset:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 遍历数据集中的样本：
- en: '[PRE16]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'We can generate segmentation masks for our dataset:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以为数据集生成分割掩码：
- en: '![](../Images/3f2d7f7a665af436ad38e6fdc04ed93a.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3f2d7f7a665af436ad38e6fdc04ed93a.png)'
- en: '*Semantic segmentation of Open Images V7 samples with Meta AI’s Segment Anything
    Model. Image courtesy of the author.*'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '*Meta AI 的 Segment Anything 模型对 Open Images V7 样本的语义分割。图片由作者提供。*'
- en: Of course, not *everything* is semantically segmented here, as the images contain
    somewhat sparse point labels. Adding more points to the initial data would result
    in denser semantic segmentation masks for the images in the dataset.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，并不是*所有*的内容都进行了语义分割，因为图像中包含一些稀疏的点标签。向初始数据中添加更多点将导致数据集中图像的语义分割掩码更加密集。
- en: We can also see that while SAM does a pretty good job across the entire dataset,
    it struggles to appropriately segment the motorcycle’s wheels.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以看到，虽然SAM在整个数据集上的表现相当不错，但在适当地分割摩托车轮子方面表现得比较困难。
- en: Instance segmentation with SAM
  id: totrans-87
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用SAM进行实例分割
- en: 'If you already have bounding boxes for objects in your dataset, you can prompt
    the SAM model with these bounding boxes and generate segmentation masks for these
    objects! Here’s how:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你已经有了数据集中对象的边界框，你可以用这些边界框来提示SAM模型，并生成这些对象的分割掩码！方法如下：
- en: 'As with point labels, we will need to convert bounding boxes from relative
    to absolute coordinates. In FiftyOne, bounding boxes are stored in `[<top-left-x>,
    <top-left-y>, <width>, <height>]` format, with coordinates in `[0,1]`. On the
    other hand, SAM bounding boxes are `[<top-left-x>, <top-left-y>, <top-right-x>,
    <top-right-y>]`, absolute coordinates. The following function will perform the
    conversion for us:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 与点标签一样，我们需要将边界框从相对坐标转换为绝对坐标。在 FiftyOne 中，边界框的存储格式为 `[<top-left-x>, <top-left-y>,
    <width>, <height>]`，坐标在 `[0,1]` 范围内。而 SAM 边界框的格式为 `[<top-left-x>, <top-left-y>,
    <top-right-x>, <top-right-y>]`，使用绝对坐标。以下函数将为我们执行转换：
- en: '[PRE17]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Once we have generated an instance segmentation mask with SAM for a given object
    detection, we can add the mask to the detection object with:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们为给定的对象检测生成了实例分割掩码，我们可以使用以下方式将掩码添加到检测对象中：
- en: '[PRE18]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'To add instance segmentation masks to an image, we loop through all object
    detections, using the `SamPredictor` object with each detection’s bounding box,
    and adding the resulting mask to the FiftyOne `Detection` object:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 要将实例分割掩码添加到图像中，我们需要遍历所有对象检测，使用 `SamPredictor` 对象与每个检测的边界框，并将生成的掩码添加到 FiftyOne
    `Detection` 对象中：
- en: '[PRE19]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'For instance segmentation, extending this to the entire dataset is trivial:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 对于实例分割，将其扩展到整个数据集是很简单的：
- en: '[PRE20]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Coloring by label, we get something that looks like this:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 按标签着色，我们得到的效果如下：
- en: '![](../Images/ec63cc352ddba57be6d72b4cdba891c2.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ec63cc352ddba57be6d72b4cdba891c2.png)'
- en: '*Instance segmentation of Open Images V7 samples with Meta AI’s Segment Anything
    Model. Image courtesy of the author.*'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '*Meta AI 的 Segment Anything 模型对 Open Images V7 样本的实例分割。图片由作者提供。*'
- en: '*Note: for better efficiency, you can also batch these predictions!*'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '*注意：为了提高效率，你还可以批量处理这些预测！*'
- en: Panoptic segmentation
  id: totrans-101
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 全景分割
- en: 'If you wanted to use SAM to panoptically segment your dataset, you could combine
    keypoint and bounding box approaches in the following way:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你希望使用SAM对数据集进行全景分割，你可以通过以下方式结合关键点和边界框的方法：
- en: '**For each bounded object, or *thing*, such as a car or a table:**'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '**对于每一个边界对象，或者*事物*，如汽车或桌子：**'
- en: Generate a bounding box around the object, either via traditional annotation,
    or [Grounding DINO](https://github.com/IDEA-Research/GroundingDINO), or some other
    method.
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 生成对象周围的边界框，可以通过传统注释，或使用[Grounding DINO](https://github.com/IDEA-Research/GroundingDINO)，或者其他方法。
- en: Select the center of the bounding box as the default keypoint for that object.
    If this turns out to not be inside the object, adjust accordingly.
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择边界框的中心点作为该对象的默认关键点。如果发现这个点不在对象内部，请相应调整。
- en: Use these keypoints and bounding boxes to compute instance segmentation masks
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用这些关键点和边界框来计算实例分割掩码
- en: '**For each contiguous region of *stuff* (for instance, sky or grass):**'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '**对于每个连续的*物体*区域（例如天空或草地）：**'
- en: Add one or multiple labeled keypoints.
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 添加一个或多个标记的关键点。
- en: Compute semantic segmentation masks using these keypoints
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用这些关键点计算语义分割掩码
- en: '**Fill in the gaps:**'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '**填补空白：**'
- en: Given all of the instance and semantic segmentation masks, identify regions
    of overlapping coverage and regions without any mask.
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在所有实例和语义分割掩码的基础上，识别重叠区域和没有任何掩码的区域。
- en: Address these regions with whatever strategy suits your application best.
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用适合你应用的策略处理这些区域。
- en: Conclusion
  id: totrans-113
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: 'Meta AI’s Segment Anything Model is incredibly powerful and versatile. That
    being said, SAM is just one of many exciting advances in the areas of segmentation
    and prompted/guided computer vision. The field is moving incredibly fast! If you
    are interested in learning more, I’d encourage you to check out the following
    related projects:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: Meta AI的Segment Anything Model功能强大且多才多艺。尽管如此，SAM只是分割和提示/引导计算机视觉领域众多令人兴奋的进展中的一个。这个领域发展非常迅速！如果你对了解更多感兴趣，我建议你查看以下相关项目：
- en: '[Track Anything](https://github.com/gaomingqi/track-anything): video segmentation
    — built on top of SAM'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Track Anything](https://github.com/gaomingqi/track-anything)：视频分割——基于SAM构建'
- en: '[MedSAM](https://github.com/bowang-lab/medsam): Segment Anything for medical
    images'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[MedSAM](https://github.com/bowang-lab/medsam)：用于医学图像的全方位分割'
- en: '[Inpaint Anything](https://github.com/geekyutao/inpaint-anything): Segment
    Anything + Inpainting'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Inpaint Anything](https://github.com/geekyutao/inpaint-anything)：Segment Anything
    + 填充'
- en: '[Semantic Segment Anything](https://github.com/fudan-zvg/Semantic-Segment-Anything):
    SAM + semantic labeling engine'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Semantic Segment Anything](https://github.com/fudan-zvg/Semantic-Segment-Anything)：SAM
    + 语义标注引擎'
- en: '[Segment Everything Everywhere All at Once](https://github.com/ux-decoder/segment-everything-everywhere-all-at-once):
    similar to SAM, and supports multi-modal prompts'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Segment Everything Everywhere All at Once](https://github.com/ux-decoder/segment-everything-everywhere-all-at-once)：类似于SAM，并支持多模态提示'
- en: Sources
  id: totrans-120
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 来源
- en: '[Segment Anything Model](https://github.com/facebookresearch/segment-anything/blob/main/LICENSE)
    ([license](https://github.com/facebookresearch/segment-anything/blob/main/LICENSE))'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Segment Anything Model](https://github.com/facebookresearch/segment-anything/blob/main/LICENSE)（[许可证](https://github.com/facebookresearch/segment-anything/blob/main/LICENSE)）'
- en: '[Open Images V7](https://github.com/openimages/dataset/blob/main/LICENSE) ([license](https://github.com/openimages/dataset/blob/main/LICENSE))'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Open Images V7](https://github.com/openimages/dataset/blob/main/LICENSE)（[许可证](https://github.com/openimages/dataset/blob/main/LICENSE)）'
