["```py\nclustering_llm\n├─ data\n│  ├─ data.rar\n├─ img\n├─ embedding.ipynb\n├─ embedding_creation.py\n├─ kmeans.ipynb\n├─ kprototypes.ipynb\n├─ README.md\n└─ requirements.txt\n```", "```py\nimport pandas as pd # dataframe manipulation\nimport numpy as np # linear algebra\n\n# data visualization\nimport matplotlib.pyplot as plt\nimport matplotlib.cm as cm\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport seaborn as sns\nimport shap\n\n# sklearn \nfrom sklearn.cluster import KMeans\nfrom sklearn.preprocessing import PowerTransformer, OrdinalEncoder\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.manifold import TSNE\nfrom sklearn.metrics import silhouette_score, silhouette_samples, accuracy_score, classification_report\n\nfrom pyod.models.ecod import ECOD\nfrom yellowbrick.cluster import KElbowVisualizer\n\nimport lightgbm as lgb\nimport prince\n\n# Read file\ndf = pd.read_csv(\"train.csv\", sep = \";\")\ndf = df.iloc[:, 0:8]\n\n# Preprocessing part\ncategorical_transformer_onehot = Pipeline(\n    steps=[\n        (\"encoder\", OneHotEncoder(handle_unknown=\"ignore\", drop=\"first\", sparse=False))\n    ])\n\ncategorical_transformer_ordinal = Pipeline(\n    steps=[\n        (\"encoder\", OrdinalEncoder())\n    ])\n\nnum = Pipeline(\n    steps=[\n        (\"encoder\", PowerTransformer())\n    ])\n\npreprocessor  = ColumnTransformer(transformers = [\n                ('cat_onehot', categorical_transformer_onehot, [\"default\", \"housing\", \"loan\", \"job\", \"marital\"]),\n                ('cat_ordinal', categorical_transformer_ordinal, [\"education\"]),\n                ('num', num, [\"age\", \"balance\"])\n                ])\n\npipeline = Pipeline(\n    steps=[(\"preprocessor\", preprocessor)]\n    )\npipe_fit = pipeline.fit(df)\n\ndata = pd.DataFrame(pipe_fit.transform(df), columns = pipe_fit.get_feature_names_out().tolist())\n\nprint(data.columns.tolist())\n\n# OUTPUT\n['cat_onehot__default_yes',\n 'cat_onehot__housing_yes',\n 'cat_onehot__loan_yes',\n 'cat_onehot__job_blue-collar',\n 'cat_onehot__job_entrepreneur',\n 'cat_onehot__job_housemaid',\n 'cat_onehot__job_management',\n 'cat_onehot__job_retired',\n 'cat_onehot__job_self-employed',\n 'cat_onehot__job_services',\n 'cat_onehot__job_student',\n 'cat_onehot__job_technician',\n 'cat_onehot__job_unemployed',\n 'cat_onehot__job_unknown',\n 'cat_onehot__marital_married',\n 'cat_onehot__marital_single',\n 'cat_ordinal__education',\n 'num__age',\n 'num__balance']\n```", "```py\nfrom pyod.models.ecod import ECOD\n\nclf = ECOD()\nclf.fit(data)\noutliers = clf.predict(data) \n\ndata[\"outliers\"] = outliers\n\n# Data without outliers\ndata_no_outliers = data[data[\"outliers\"] == 0]\ndata_no_outliers = data_no_outliers.drop([\"outliers\"], axis = 1)\n\n# Data with Outliers\ndata_with_outliers = data.copy()\ndata_with_outliers = data_with_outliers.drop([\"outliers\"], axis = 1)\n\nprint(data_no_outliers.shape) -> (40690, 19)\nprint(data_with_outliers.shape) -> (45211, 19) \n```", "```py\nfrom yellowbrick.cluster import KElbowVisualizer\n\n# Instantiate the clustering model and visualizer\nkm = KMeans(init=\"k-means++\", random_state=0, n_init=\"auto\")\nvisualizer = KElbowVisualizer(km, k=(2,10))\n\nvisualizer.fit(data_no_outliers)        # Fit the data to the visualizer\nvisualizer.show() \n```", "```py\nfrom sklearn.metrics import davies_bouldin_score, silhouette_score, silhouette_samples\nimport matplotlib.cm as cm\n\ndef make_Silhouette_plot(X, n_clusters):\n    plt.xlim([-0.1, 1])\n    plt.ylim([0, len(X) + (n_clusters + 1) * 10])\n    clusterer = KMeans(n_clusters=n_clusters, max_iter = 1000, n_init = 10, init = 'k-means++', random_state=10)\n    cluster_labels = clusterer.fit_predict(X)\n    silhouette_avg = silhouette_score(X, cluster_labels)\n    print(\n        \"For n_clusters =\", n_clusters,\n        \"The average silhouette_score is :\", silhouette_avg,\n    )\n# Compute the silhouette scores for each sample\n    sample_silhouette_values = silhouette_samples(X, cluster_labels)\n    y_lower = 10\n    for i in range(n_clusters):\n        ith_cluster_silhouette_values = sample_silhouette_values[cluster_labels == i]\n        ith_cluster_silhouette_values.sort()\n        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n        y_upper = y_lower + size_cluster_i\n        color = cm.nipy_spectral(float(i) / n_clusters)\n        plt.fill_betweenx(\n            np.arange(y_lower, y_upper),\n            0,\n            ith_cluster_silhouette_values,\n            facecolor=color,\n            edgecolor=color,\n            alpha=0.7,\n        )\n        plt.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n        y_lower = y_upper + 10\n        plt.title(f\"The Silhouette Plot for n_cluster = {n_clusters}\", fontsize=26)\n        plt.xlabel(\"The silhouette coefficient values\", fontsize=24)\n        plt.ylabel(\"Cluster label\", fontsize=24)\n        plt.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n        plt.yticks([])  \n        plt.xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n\nrange_n_clusters = list(range(2,10))\n\nfor n_clusters in range_n_clusters:\n    print(f\"N cluster: {n_clusters}\")\n    make_Silhouette_plot(data_no_outliers, n_clusters)   \n    plt.savefig('Silhouette_plot_{}.png'.format(n_clusters))\n    plt.close()\n\nOUTPUT:\n\n\"\"\"\nN cluster: 2\nFor n_clusters = 2 The average silhouette_score is : 0.18111287366156115\nN cluster: 3\nFor n_clusters = 3 The average silhouette_score is : 0.16787543108034586\nN cluster: 4\nFor n_clusters = 4 The average silhouette_score is : 0.1583411958880734\nN cluster: 5\nFor n_clusters = 5 The average silhouette_score is : 0.1672987260052535\nN cluster: 6\nFor n_clusters = 6 The average silhouette_score is : 0.15485098506258177\nN cluster: 7\nFor n_clusters = 7 The average silhouette_score is : 0.1495307642182009\nN cluster: 8\nFor n_clusters = 8 The average silhouette_score is : 0.15098396457075294\nN cluster: 9\nFor n_clusters = 9 The average silhouette_score is : 0.14842917303536465\n\"\"\"\n```", "```py\nkm = KMeans(n_clusters=5,\n            init='k-means++', \n            n_init=10,\n            max_iter=100, \n            random_state=42)\n\nclusters_predict = km.fit_predict(data_no_outliers)\n\n\"\"\"\nclusters_predict -> array([4, 2, 0, ..., 3, 4, 3])\nnp.unique(clusters_predict) -> array([0, 1, 2, 3, 4])\n\"\"\"\n```", "```py\nfrom sklearn.metrics import silhouette_score\nfrom sklearn.metrics import calinski_harabasz_score\nfrom sklearn.metrics import davies_bouldin_score\n\n\"\"\"\nThe Davies Bouldin index is defined as the average similarity measure \nof each cluster with its most similar cluster, where similarity \nis the ratio of within-cluster distances to between-cluster distances.\n\nThe minimum value of the DB Index is 0, whereas a smaller \nvalue (closer to 0) represents a better model that produces better clusters.\n\"\"\"\nprint(f\"Davies bouldin score: {davies_bouldin_score(data_no_outliers,clusters_predict)}\")\n\n\"\"\"\nCalinski Harabaz Index -> Variance Ratio Criterion.\n\nCalinski Harabaz Index is defined as the ratio of the \nsum of between-cluster dispersion and of within-cluster dispersion.\n\nThe higher the index the more separable the clusters.\n\"\"\"\nprint(f\"Calinski Score: {calinski_harabasz_score(data_no_outliers,clusters_predict)}\")\n\n\"\"\"\nThe silhouette score is a metric used to calculate the goodness of \nfit of a clustering algorithm, but can also be used as \na method for determining an optimal value of k (see here for more).\n\nIts value ranges from -1 to 1.\nA value of 0 indicates clusters are overlapping and either\nthe data or the value of k is incorrect.\n\n1 is the ideal value and indicates that clusters are very \ndense and nicely separated.\n\"\"\"\nprint(f\"Silhouette Score: {silhouette_score(data_no_outliers,clusters_predict)}\")\n\nOUTPUT:\n\n\"\"\"Davies bouldin score: 1.676769775662788\nCalinski Score: 6914.705500337112\nSilhouette Score: 0.16729335453305272\n\"\"\"\n```", "```py\nimport prince\nimport plotly.express as px\n\ndef get_pca_2d(df, predict):\n\n    pca_2d_object = prince.PCA(\n    n_components=2,\n    n_iter=3,\n    rescale_with_mean=True,\n    rescale_with_std=True,\n    copy=True,\n    check_input=True,\n    engine='sklearn',\n    random_state=42\n    )\n\n    pca_2d_object.fit(df)\n\n    df_pca_2d = pca_2d_object.transform(df)\n    df_pca_2d.columns = [\"comp1\", \"comp2\"]\n    df_pca_2d[\"cluster\"] = predict\n\n    return pca_2d_object, df_pca_2d\n\ndef get_pca_3d(df, predict):\n\n    pca_3d_object = prince.PCA(\n    n_components=3,\n    n_iter=3,\n    rescale_with_mean=True,\n    rescale_with_std=True,\n    copy=True,\n    check_input=True,\n    engine='sklearn',\n    random_state=42\n    )\n\n    pca_3d_object.fit(df)\n\n    df_pca_3d = pca_3d_object.transform(df)\n    df_pca_3d.columns = [\"comp1\", \"comp2\", \"comp3\"]\n    df_pca_3d[\"cluster\"] = predict\n\n    return pca_3d_object, df_pca_3d\n\ndef plot_pca_3d(df, title = \"PCA Space\", opacity=0.8, width_line = 0.1):\n\n    df = df.astype({\"cluster\": \"object\"})\n    df = df.sort_values(\"cluster\")\n\n    fig = px.scatter_3d(\n          df, \n          x='comp1', \n          y='comp2', \n          z='comp3',\n          color='cluster',\n          template=\"plotly\",\n\n          # symbol = \"cluster\",\n\n          color_discrete_sequence=px.colors.qualitative.Vivid,\n          title=title).update_traces(\n              # mode = 'markers',\n              marker={\n                  \"size\": 4,\n                  \"opacity\": opacity,\n                  # \"symbol\" : \"diamond\",\n                  \"line\": {\n                      \"width\": width_line,\n                      \"color\": \"black\",\n                  }\n              }\n          ).update_layout(\n                  width = 800, \n                  height = 800, \n                  autosize = True, \n                  showlegend = True,\n                  legend=dict(title_font_family=\"Times New Roman\",\n                              font=dict(size= 20)),\n                  scene = dict(xaxis=dict(title = 'comp1', titlefont_color = 'black'),\n                              yaxis=dict(title = 'comp2', titlefont_color = 'black'),\n                              zaxis=dict(title = 'comp3', titlefont_color = 'black')),\n                  font = dict(family = \"Gilroy\", color  = 'black', size = 15))\n\n    fig.show()\n```", "```py\npca_3d_object, df_pca_3d = pca_plot_3d(data_no_outliers, clusters_predict)\nplot_pca_3d(df_pca_3d, title = \"PCA Space\", opacity=1, width_line = 0.1)\nprint(\"The variability is :\", pca_3d_object.eigenvalues_summary)\n```", "```py\nfrom sklearn.manifold import TSNE\n\nsampling_data = data_no_outliers.sample(frac=0.5, replace=True, random_state=1)\nsampling_clusters = pd.DataFrame(clusters_predict).sample(frac=0.5, replace=True, random_state=1)[0].values\n\ndf_tsne_3d = TSNE(\n                  n_components=3, \n                  learning_rate=500, \n                  init='random', \n                  perplexity=200, \n                  n_iter = 5000).fit_transform(sampling_data)\n\ndf_tsne_3d = pd.DataFrame(df_tsne_3d, columns=[\"comp1\", \"comp2\",'comp3'])\ndf_tsne_3d[\"cluster\"] = sampling_clusters\nplot_pca_3d(df_tsne_3d, title = \"PCA Space\", opacity=1, width_line = 0.1)\n```", "```py\nimport lightgbm as lgb\nimport shap\n\n# We create the LGBMClassifier model and train it\nclf_km = lgb.LGBMClassifier(colsample_by_tree=0.8)\nclf_km.fit(X=data_no_outliers, y=clusters_predict)\n\n#SHAP values\nexplainer_km = shap.TreeExplainer(clf_km)\nshap_values_km = explainer_km.shap_values(data_no_outliers)\nshap.summary_plot(shap_values_km, data_no_outliers, plot_type=\"bar\", plot_size=(15, 10))\n```", "```py\ndf_no_outliers = df[df.outliers == 0]\ndf_no_outliers[\"cluster\"] = clusters_predict\n\ndf_no_outliers.groupby('cluster').agg(\n    {\n        'job': lambda x: x.value_counts().index[0],\n        'marital': lambda x: x.value_counts().index[0],\n        'education': lambda x: x.value_counts().index[0],\n        'housing': lambda x: x.value_counts().index[0],\n        'loan': lambda x: x.value_counts().index[0],\n        'contact': lambda x: x.value_counts().index[0],\n        'age':'mean',\n        'balance': 'mean',\n        'default': lambda x: x.value_counts().index[0],\n\n    }\n).reset_index()\n```", "```py\npipe = Pipeline([('scaler', PowerTransformer())])\n\ndf_aux = pd.DataFrame(pipe_fit.fit_transform(df_no_outliers[[\"age\", \"balance\"]] ), columns = [\"age\", \"balance\"])\ndf_no_outliers_norm = df_no_outliers.copy()\n# Replace age and balance columns by preprocessed values\ndf_no_outliers_norm = df_no_outliers_norm.drop([\"age\", \"balance\"], axis = 1)\ndf_no_outliers_norm[\"age\"] = df_aux[\"age\"].values\ndf_no_outliers_norm[\"balance\"] = df_aux[\"balance\"].values\ndf_no_outliers_norm\n```", "```py\n# Choose optimal K using Elbow method\nfrom kmodes.kprototypes import KPrototypes\nfrom plotnine import *\nimport plotnine\ncost = []\nrange_ = range(2, 15)\nfor cluster in range_:\n\n        kprototype = KPrototypes(n_jobs = -1, n_clusters = cluster, init = 'Huang', random_state = 0)\n        kprototype.fit_predict(df_no_outliers, categorical = categorical_columns_index)\n        cost.append(kprototype.cost_)\n        print('Cluster initiation: {}'.format(cluster))\n\n# Converting the results into a dataframe and plotting them\ndf_cost = pd.DataFrame({'Cluster':range_, 'Cost':cost})\n# Data viz\nplotnine.options.figure_size = (8, 4.8)\n(\n    ggplot(data = df_cost)+\n    geom_line(aes(x = 'Cluster',\n                  y = 'Cost'))+\n    geom_point(aes(x = 'Cluster',\n                   y = 'Cost'))+\n    geom_label(aes(x = 'Cluster',\n                   y = 'Cost',\n                   label = 'Cluster'),\n               size = 10,\n               nudge_y = 1000) +\n    labs(title = 'Optimal number of cluster with Elbow Method')+\n    xlab('Number of Clusters k')+\n    ylab('Cost')+\n    theme_minimal()\n)\n```", "```py\n# We get the index of categorical columns\nnumerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\ncategorical_columns = df_no_outliers_norm.select_dtypes(exclude=numerics).columns\nprint(categorical_columns)\ncategorical_columns_index = [df_no_outliers_norm.columns.get_loc(col) for col in categorical_columns]\n\n# Create the model\ncluster_num = 5\nkprototype = KPrototypes(n_jobs = -1, n_clusters = cluster_num, init = 'Huang', random_state = 0)\nkprototype.fit(df_no_outliers_norm, categorical = categorical_columns_index)\nclusters = kprototype.predict(df_no_outliers , categorical = categorical_columns_index)\n\nprint(clusters) \" -> array([3, 1, 1, ..., 1, 1, 2], dtype=uint16)\"\n```", "```py\nfrom prince import MCA\n\ndef get_MCA_3d(df, predict):\n    mca = MCA(n_components =3, n_iter = 100, random_state = 101)\n    mca_3d_df = mca.fit_transform(df)\n    mca_3d_df.columns = [\"comp1\", \"comp2\", \"comp3\"]\n    mca_3d_df[\"cluster\"] = predict\n    return mca, mca_3d_df\n\ndef get_MCA_2d(df, predict):\n    mca = MCA(n_components =2, n_iter = 100, random_state = 101)\n    mca_2d_df = mca.fit_transform(df)\n    mca_2d_df.columns = [\"comp1\", \"comp2\"]\n    mca_2d_df[\"cluster\"] = predict\n    return mca, mca_2d_df\n\"-------------------------------------------------------------------\"\nmca_3d, mca_3d_df = get_MCA_3d(df_no_outliers_norm, clusters)\n```", "```py\nmca_3d.eigenvalues_summary\n```", "```py\nimport pandas as pd # dataframe manipulation\nimport numpy as np # linear algebra\nfrom sentence_transformers import SentenceTransformer\n\ndf = pd.read_csv(\"data/train.csv\", sep = \";\")\n\n# -------------------- First Step --------------------\ndef compile_text(x):\n\n    text =  f\"\"\"Age: {x['age']},  \n                housing load: {x['housing']}, \n                Job: {x['job']}, \n                Marital: {x['marital']}, \n                Education: {x['education']}, \n                Default: {x['default']}, \n                Balance: {x['balance']}, \n                Personal loan: {x['loan']}, \n                contact: {x['contact']}\n            \"\"\"\n\n    return text\n\nsentences = df.apply(lambda x: compile_text(x), axis=1).tolist()\n\n# -------------------- Second Step --------------------\n\nmodel = SentenceTransformer(r\"sentence-transformers/paraphrase-MiniLM-L6-v2\")\noutput = model.encode(sentences=sentences,\n         show_progress_bar=True,\n         normalize_embeddings=True)\n\ndf_embedding = pd.DataFrame(output)\ndf_embedding\n```", "```py\n# Normal Dataset\ndf = pd.read_csv(\"data/train.csv\", sep = \";\")\ndf = df.iloc[:, 0:8]\n\n# Embedding Dataset\ndf_embedding = pd.read_csv(\"data/embedding_train.csv\", sep = \",\")\n```", "```py\ndf_embedding_no_out.shape  -> (40690, 384)\ndf_embedding_with_out.shape -> (45211, 384)\n```", "```py\nn_clusters = 5\nclusters = KMeans(n_clusters=n_clusters, init = \"k-means++\").fit(df_embedding_no_out)\nprint(clusters.inertia_)\nclusters_predict = clusters.predict(df_embedding_no_out)\n```", "```py\nDavies bouldin score: 1.8095386826791042\nCalinski Score: 6419.447089002081\nSilhouette Score: 0.20360442824114108\n```", "```py\nplot_pca_3d(df_pca_3d, title = \"PCA Space\", opacity=0.2, width_line = 0.1)\n```"]