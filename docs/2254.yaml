- en: Building a maintainable and modular LLM application stack with Hamilton in 13
    minutes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/building-a-maintainable-and-modular-llm-application-stack-with-hamilton-baa68e55e8cd?source=collection_archive---------10-----------------------#2023-07-13](https://towardsdatascience.com/building-a-maintainable-and-modular-llm-application-stack-with-hamilton-baa68e55e8cd?source=collection_archive---------10-----------------------#2023-07-13)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: LLM Applications are dataflows, use a tool specifically designed to express
    them
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@stefan.krawczyk?source=post_page-----baa68e55e8cd--------------------------------)[![Stefan
    Krawczyk](../Images/150405abaad9590e1dc2589168ed2fa3.png)](https://medium.com/@stefan.krawczyk?source=post_page-----baa68e55e8cd--------------------------------)[](https://towardsdatascience.com/?source=post_page-----baa68e55e8cd--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----baa68e55e8cd--------------------------------)
    [Stefan Krawczyk](https://medium.com/@stefan.krawczyk?source=post_page-----baa68e55e8cd--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F193628e26f00&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-a-maintainable-and-modular-llm-application-stack-with-hamilton-baa68e55e8cd&user=Stefan+Krawczyk&userId=193628e26f00&source=post_page-193628e26f00----baa68e55e8cd---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----baa68e55e8cd--------------------------------)
    ·13 min read·Jul 13, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fbaa68e55e8cd&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-a-maintainable-and-modular-llm-application-stack-with-hamilton-baa68e55e8cd&user=Stefan+Krawczyk&userId=193628e26f00&source=-----baa68e55e8cd---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fbaa68e55e8cd&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-a-maintainable-and-modular-llm-application-stack-with-hamilton-baa68e55e8cd&source=-----baa68e55e8cd---------------------bookmark_footer-----------)![](../Images/84bcb5f1c47c2c34ce7c753f621da5a8.png)'
  prefs: []
  type: TYPE_NORMAL
- en: LLM stacks. Using the right tool, like Hamilton, can sure your stack doesn’t
    become a pain to maintain and manage. Image from [pixabay](https://pixabay.com/photos/books-stack-book-store-1163695/).
  prefs: []
  type: TYPE_NORMAL
- en: '*This post is written in collaboration with* [*Thierry Jean*](https://medium.com/u/cf12dc7f8440)
    *and originally appeared* [*here*](https://blog.dagworks.io/p/building-a-maintainable-and-modular)*.*'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this post, we’re going to share how [Hamilton](https://github.com/dagWorks-Inc/hamilton),
    an open source framework, can help you write modular and maintainable code for
    your large language model (LLM) application stack. Hamilton is great for describing
    any type of [dataflow](https://en.wikipedia.org/wiki/Dataflow), which is exactly
    what you’re doing when building an LLM powered application. With Hamilton you
    get strong [software maintenance ergonomics](https://ceur-ws.org/Vol-3306/paper5.pdf),
    with the added benefit of being able to easily swap and evaluate different providers/implementations
    for components of your application. Disclaimer: I’m one of the authors of the
    Hamilton package.'
  prefs: []
  type: TYPE_NORMAL
- en: The example we’ll walk you through will mirror a typical LLM application workflow
    you’d run to populate a vector database with some text knowledge. Specifically,
    we’ll cover pulling data from the web, creating text embeddings (vectors) and
    pushing them to a vector store.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3af6ce8a4bc5b6f967b1e28e6085bcc0.png)'
  prefs: []
  type: TYPE_IMG
- en: Stack overview. Image by authors.
  prefs: []
  type: TYPE_NORMAL
- en: The LLM application dataflow
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To start, let’s describe what a typical LLM dataflow consists of. The application
    will receive a small data input (e.g., a text, a command) and act within a larger
    context (e.g., chat history, documents, state). This data will move through different
    services (LLM, vector database, document store, etc.) to perform operations, generate
    new data artifacts, and return final results. Most use cases repeat this flow
    multiple times while iterating over different inputs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Some common operations include:'
  prefs: []
  type: TYPE_NORMAL
- en: Convert text to embeddings
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Store / search / retrieve embeddings
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Find nearest neighbors to an embedding
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Retrieve text for an embedding
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Determine context required to pass into a prompt
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Prompt models with context from relevant text
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Send results to another service (API, database, etc.)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: …
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: and chaining them together!
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now, let’s think about the above in a production context, and imagine a user
    is unsatisfied with the outputs of your application and you want to find the root
    cause of the issue. Your application logged both the prompt and the results. Your
    code allows you to figure out the sequence of operations. Yet, you have no clue
    where things went wrong and the system produced undesirable output… To mitigate
    this, we’d argue it’s critical then to have lineage of data artifacts and the
    code that produces them, so you can debug situations such as these quickly.
  prefs: []
  type: TYPE_NORMAL
- en: To add to the complexity of your LLM application dataflow, many operations are
    non-deterministic, meaning you can’t rerun or reverse engineer the operation to
    reproduce intermediate results. For example, an API call to generate a text or
    image response will likely be non-reproducible even if you have access to the
    same input and configuration (you *can* *mitigate some of this* with options such
    as [temperature](https://platform.openai.com/docs/api-reference/chat/create#chat/create-temperature)).
    This also extends to certain vector database operations like “find nearest” where
    the result depends on the objects currently stored in the database. In production
    settings, it is prohibitive to near impossible to snapshot DB states to make calls
    reproducible.
  prefs: []
  type: TYPE_NORMAL
- en: 'For these reasons, it is important to adopt flexible tooling to create robust
    dataflows that allow you to:'
  prefs: []
  type: TYPE_NORMAL
- en: plugin in various components easily.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: see how components connect to each other.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: add and customize common production needs like caching, validation, and observability.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: adjust the flow structure to your needs without requiring a strong engineering
    skill set
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: plug into the traditional data processing and machine learning ecosystem.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In this post we’ll give an overview of how Hamilton fulfills points 1, 2, &
    4\. We refer you to our [documentation](https://hamilton.dagworks.io/en/latest/)
    for points 3 & 5.
  prefs: []
  type: TYPE_NORMAL
- en: Current LLM application development tooling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The LLM space is still in its infancy, and the usage patterns and tooling are
    rapidly evolving. While LLM frameworks can get you started, current options are
    not production tested; to our knowledge, no established tech companies are using
    the current popular LLM frameworks in production.
  prefs: []
  type: TYPE_NORMAL
- en: 'Don’t get us wrong, some of the tooling out there is great for getting a quick
    proof of concept up and running! However, we feel they fall short in two specific
    areas:'
  prefs: []
  type: TYPE_NORMAL
- en: '**1\. How to model the LLM application’s dataflow.** We strongly believe that
    the dataflow of “actions” is better modeled as functions, rather than through
    object oriented classes and lifecycles. Functions are much simpler to reason about,
    test, and change. Object oriented classes can become quite opaque and impose more
    mental burden.'
  prefs: []
  type: TYPE_NORMAL
- en: When something errors, object-oriented frameworks require you to drill into
    the objects’ source code to understand it. Whereas with Hamilton functions, a
    clear dependency lineage tells you where to look and helps you reason about what
    happened (more on this below)!
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**2\. Customization/extensions.** Unfortunately you need a strong software
    engineering skill set to modify the current frameworks once you step outside the
    bounds of what they make “easy” to do. If that’s not an option, this means you
    can end up stepping outside the framework for a particular piece of custom business
    logic, which can inadvertently lead you to maintaining more code surface area
    than if you didn’t use the framework in the first place.'
  prefs: []
  type: TYPE_NORMAL
- en: For more on these two points we point you to threads such as these two ([hacker
    news](https://news.ycombinator.com/item?id=36645575#36647985), [reddit](https://old.reddit.com/r/LangChain/comments/13fcw36/langchain_is_pointless/))
    that have users speak in more detail.
  prefs: []
  type: TYPE_NORMAL
- en: While Hamilton is not a complete replacement for current LLM frameworks (e.g.
    there is no “agent” component), it *does* have all the building blocks to meet
    your LLM application needs and both can work in conjunction. If you want a clean,
    clear, and customizable way to write production code, integrate several LLM stack
    components, and gain observability over your app, then let’s move onto the next
    few sections!
  prefs: []
  type: TYPE_NORMAL
- en: Building with Hamilton
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Hamilton is a declarative micro-framework to describe [dataflows](https://en.wikipedia.org/wiki/Dataflow)
    in Python. It’s not a new framework (3.5+ years old), and has been used for years
    in production modeling data & machine learning dataflows. Its strength is expressing
    the flow of data & computation in a way that is straightforward to create and
    maintain (much like DBT does for SQL), which lends itself very well to support
    modeling the data & computational needs of LLM applications.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ac83c0914a4533f8f7429b752d511120.png)'
  prefs: []
  type: TYPE_IMG
- en: The Hamilton paradigm in a picture. Instead of procedural assignment, you instead
    model it as a function. The function name is an “output” you can get, while the
    function input arguments declare dependencies on what’s required to be computed.
    Image by author.
  prefs: []
  type: TYPE_NORMAL
- en: 'The basics of Hamilton are simple, and it can be extended in quite a few ways;
    you don''t have to know Hamilton to get value out of this post, but if you''re
    interested, check out:'
  prefs: []
  type: TYPE_NORMAL
- en: '[tryhamilton.dev](https://www.tryhamilton.dev/) – an interactive tutorial in
    your browser!'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Pandas data transformations in Hamilton in 5 minutes](/how-to-use-hamilton-with-pandas-in-5-minutes-89f63e5af8f5)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Lineage + Hamilton in 10 minutes](https://blog.dagworks.io/p/lineage-hamilton-in-10-minutes-c2b8a944e2e6)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Hamilton + Airflow for production](https://blog.dagworks.io/publish/post/130538397)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Onto our example
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To help set some mental context, picture this. You’re a small data team that
    is tasked with creating an LLM application to “chat” with your organization’s
    document. You believe it’s important to evaluate candidate architectures in terms
    of features, performance profile, licenses, infrastructure requirements, and costs.
    Ultimately, you know your organization’s primary concern is providing the most
    relevant results and a great user experience. The best way to assess this is to
    build a prototype, test different stacks and compare their characteristics and
    outputs. Then when you transition to production, you’ll want confidence that the
    system can be maintained and introspected easily, to consistently provide a great
    user experience.
  prefs: []
  type: TYPE_NORMAL
- en: 'With that in mind, in this example, we will implement part of an LLM application,
    specifically the data ingestion step to index a knowledge base, where we convert
    text to embeddings and store them in a vector database. We implement this in a
    modular with a few different services/technologies. The broad steps are:'
  prefs: []
  type: TYPE_NORMAL
- en: Load the [SQuAD dataset](https://huggingface.co/datasets/squad) from the HuggingFace
    Hub. You would swap this out for your corpus of preprocessed documents.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Embed text entries using the [Cohere API](https://docs.cohere.com/reference/embed),
    the [OpenAI API](https://platform.openai.com/docs/api-reference/embeddings), or
    the [SentenceTransformer library](https://www.sbert.net/examples/applications/computing-embeddings/README.html).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Store embeddings in a vector database, either [LanceDB](https://lancedb.github.io/lancedb/),
    [Pinecone](https://docs.pinecone.io/docs/overview), or [Weaviate](https://weaviate.io/developers/weaviate).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'If you need to know more about embeddings & search, we direct readers to the
    following links:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Text embeddings explained - Weaviate](https://weaviate.io/blog/vector-embeddings-explained)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[How-to conduct semantic search with Pinecone](https://docs.pinecone.io/docs/semantic-text-search)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'As we’re walking through this example, it would be useful for you to think
    about/keep in mind the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Compare what we show you with what you’re doing now.** See how Hamilton enables
    you to curate and structure a project without needing an explicit LLM-centric
    framework.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Project and application structure.** Understand how Hamilton enforces a structure
    that enables you to build and maintain a modular stack.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Confidence in iteration & project longevity.** Combining the above two points,
    Hamilton enables you to more easily maintain an LLM application in production,
    no matter who authored it.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let’s start with a visualization to give you an overview of what we’re talking
    about:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/149c01e23dbcb8ef11df1a74517ca8f3.png)'
  prefs: []
  type: TYPE_IMG
- en: Hamilton DAG visualization of Pinecone + Sentence transformer stack. Image by
    author.
  prefs: []
  type: TYPE_NORMAL
- en: Here’s what the LLM Application dataflow would look like when using pinecone
    with sentence transformers. With Hamilton to understand how things connect is
    just as simple as `display_all_functions()` call on the [Hamilton driver object](https://hamilton.dagworks.io/en/latest/reference/drivers/Driver/#hamilton.driver.Driver.display_all_functions).
  prefs: []
  type: TYPE_NORMAL
- en: Modular Code
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let’s explain the two main ways to implement modular code with Hamilton using
    our example for context.
  prefs: []
  type: TYPE_NORMAL
- en: '@config.when'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Hamilton’s focus is on readability. Without explaining what `@config.when` does,
    you can probably tell that this is a conditional statement, and only included
    *when* the predicate is satisfied. Below you will find the implementation for
    converting text to embeddings with the OpenAI and the Cohere API.
  prefs: []
  type: TYPE_NORMAL
- en: Hamilton will recognize two functions as alternative implementations because
    of the `@config.when` decorator and the same function name `embeddings` preceding
    the double underscore (`__cohere`, `__openai`). Their function signatures need
    not be entirely the same, which means it's easy and clear to adopt different implementations.
  prefs: []
  type: TYPE_NORMAL
- en: embedding_module.py
  prefs: []
  type: TYPE_NORMAL
- en: For this project, it made sense to have all embedding services implemented in
    the same file with the `@config.when` decorator since there are only 3 functions
    per service. However, as the project grows in complexity, functions could be moved
    to separate modules too, and the next section’s modularity pattern employed instead.
    Another point to note is that each of these functions is independently unit-testable.
    Should you have specific needs, it’s straightforward to encapsulate it in the
    function and test it.
  prefs: []
  type: TYPE_NORMAL
- en: Switching out python modules
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Below you will find the implementation of vector database operations for Pinecone
    and Weaviate. Note that the snippets are from `pinecone_module.py` and `weaviate_module.py`
    and notice how function signatures resemble and differ.
  prefs: []
  type: TYPE_NORMAL
- en: pinecone_module.py and weaviate_module.py
  prefs: []
  type: TYPE_NORMAL
- en: With Hamilton, the dataflow is stitched together using function names and function
    input arguments. Therefore by sharing function names for similar operations, the
    two modules are easily interchangeable. Since the LanceDB, Pinecone, and Weaviate
    implementations reside in separate modules, it reduces the number of dependencies
    per file and makes them shorter, improving both readability and maintainability.
    The logic for each implementation is clearly encapsulated in these named functions,
    so unit testing is straightforward to implement for each respective module. The
    separate modules reinforce the idea that they shouldn’t be loaded simultaneously.
    The Hamilton driver will actually throw an error when multiple functions with
    the same name are found that helps enforce this concept.
  prefs: []
  type: TYPE_NORMAL
- en: Driver implications
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The key part for running Hamilton code is the `Driver` object found in `run.py`.
    Excluding the code for the CLI and some argument parsing, we get:'
  prefs: []
  type: TYPE_NORMAL
- en: Snippet of run.py
  prefs: []
  type: TYPE_NORMAL
- en: 'The Hamilton Driver, which orchestrates execution and is what you manipulate
    your dataflow through, allows modularity through three mechanisms as seen in the
    above code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Driver configuration.** this is a dictionary the driver receives at instantiation
    containing information that should remain constant, such as which API to use,
    or the embedding service API key. This integrates well with a command plane that
    can pass JSON or strings (e.g., a Docker container, [Airflow](https://blog.dagworks.io/publish/posts/detail/130538397),
    [Metaflow](https://outerbounds.com/blog/developing-scalable-feature-engineering-dags/),
    etc.). Concretely this is where we’d specify swapping out what embedding API to
    use.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Driver modules.** the driver can receive an arbitrary number of independent
    Python modules to build the dataflow from. Here, the `vector_db_module` can be
    swapped in for the desired vector database implementation we’re connecting to.
    One can also import modules dynamically through [importlib](https://docs.python.org/3/library/importlib.html#importlib.import_module),
    which can be useful for development vs production contexts, and also enable a
    configuration driven way to changing the dataflow implementation.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Driver execution.** The `final_vars` parameter determines what output should
    be returned. You do not need to restructure your code to change what output you
    want to get. Let’s take the case of wanting to debug something within our dataflow,
    it is possible to request the output of any function by adding its name to `final_vars`.
    For example, if you have some intermediate output to debug, it’s easy to request
    it, or stop execution at that spot entirely. Note, the driver can receive `inputs`
    and `overrides` values when calling `execute()`; in the code above, the `class_name`
    is an execution time `input` that indicates the embedding object we want to create
    and where to store it in our vector database.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Modularity Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In Hamilton, the key to enable swappable components is to:'
  prefs: []
  type: TYPE_NORMAL
- en: define functions with effectively the same name and then,
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: annotate them with `@config.when` and choose which one to use via configuration
    passed to the driver, or,
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: put them in separate python modules and pass in the desired module to the driver.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: So we’ve just shown how you can plugin, swap, and call various LLM components
    with Hamilton. We didn’t need to explain what an object oriented hierarchy is,
    nor require you to have extensive software engineering experience to follow (we
    hope!). To accomplish this, we just had to match function names, and their output
    types. We think this way of writing and modularizing code is therefore more accessible
    than current LLM frameworks permit.
  prefs: []
  type: TYPE_NORMAL
- en: Hamilton code in practice
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To add to our claims, here a few practical implications of writing Hamilton
    code for LLM workflows that we’ve observed:'
  prefs: []
  type: TYPE_NORMAL
- en: CI/CD
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This ability to swap out modules/`@config.when` also means that integration
    testing in a CI system is straightforward to think about, since you have the flexibility
    and freedom to swap/isolate parts of the dataflow as desired.
  prefs: []
  type: TYPE_NORMAL
- en: Collaboration
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The modularity Hamilton enables can allow one to mirror cross team boundaries
    easily. The function names & their output types become a contract, which ensures
    one can make surgical changes and be confident in the change, as well as have
    the visibility into downstream dependencies with Hamilton’s [visualization and
    lineage features](https://blog.dagworks.io/p/lineage-hamilton-in-10-minutes-c2b8a944e2e6)
    (like the initial visualization we saw). For example, it’s clear how to interact
    and consume from the vector database.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Code changes are simpler to review, because the flow is defined by declarative
    functions. The changes are self-contained; because there is no object oriented
    hierarchy to learn, just a function to modify. Anything “custom” is de facto supported
    by Hamilton.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Debugging
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When there is an error with Hamilton, it’s clear as to what the code it maps
    to is, and because of how the function is defined, one knows where to place it
    within the dataflow.
  prefs: []
  type: TYPE_NORMAL
- en: Take the simple example of the embeddings function using cohere. If there was
    a time out, or error in parsing the response it would be clear that it maps to
    this code, and from the function definition you’d know where in the flow it fits.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/ed4bd30c315330f5fcec9bd4984202c4.png)'
  prefs: []
  type: TYPE_IMG
- en: Visualization showing where `embeddings` fits in the dataflow. Image by author.
  prefs: []
  type: TYPE_NORMAL
- en: Tips for creating a modular LLM stack
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before we finish, here are some ideas to guide you through building your application.
    Some decisions might not have an obvious best choice, but having the right approach
    to modularity will allow you to efficiently iterate as requirements evolve.
  prefs: []
  type: TYPE_NORMAL
- en: Before writing any code, draw a DAG of the logical steps of your workflow. This
    sets the basis for defining common steps and interfaces that are not service-specific.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Identify steps that could be swapped. By being purposeful with configuration
    points, you will reduce risks of [speculative generality](https://refactoring.guru/smells/speculative-generality).
    Concretely, this would result in functions with less arguments, default values,
    and grouped into thematic modules.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Chunk parts of your dataflow into modules with few dependencies, if relevant.
    This will lead to shorter Python files with fewer package dependencies, improved
    readability and maintainability. Hamilton is indifferent and can build its DAG
    from multiple modules.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: To close & future directions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Thanks for getting this far. We believe that Hamilton has a part to play in
    helping everyone express their dataflows, and LLM applications are just one use
    case! To summarize our messaging in this post can be boiled down to:'
  prefs: []
  type: TYPE_NORMAL
- en: It is useful to conceive of LLM applications as dataflows, and are therefore
    a great fit for using Hamilton.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Object-centric LLM frameworks can be opaque and hard to extend and maintain
    for your production needs. Instead, one should write their own integrations with
    Hamilton’s straightforward declarative style. Doing so will improve your code’s
    transparency and maintainability, with clear testable functions, clear mapping
    of runtime errors to functions, and built-in visualization of your dataflow.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The modularity prescribed by using Hamilton will make collaboration more efficient
    and provide you with the requisite flexibility to modify and change your LLM workflows
    at the speed at which the field is moving.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We now invite you to play around with, try, and modify the full example for
    yourselves [here](https://github.com/DAGWorks-Inc/hamilton/tree/main/examples/LLM_Workflows/modular_llm_stack).
    There is a `README` that will explain the commands to run and get started. Otherwise,
    we are working on making the Hamilton + LLM Application experience even better
    by thinking about the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Agents.** Can we provide the same level of visibility to agents that we have
    for regular Hamilton dataflows?'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Parallelization.** How can we make it simpler to express running a dataflow
    over a list of documents for example. See this [work in progress PR](https://github.com/DAGWorks-Inc/hamilton/pull/216)
    for what we mean.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Plugins for caching and observability.** One can already implement a custom
    caching and observability solution on top of Hamilton. We’re working on providing
    more standard options out of the box for common components, e.g. redis.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**A user contributed dataflows section.** We see the possibility to standardize
    on common names for specific LLM application use cases. In which case we can start
    to aggregate Hamilton dataflows, and allow people to pull them down for their
    needs.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We want to hear from you!
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'If you’re excited by any of this, or have strong opinions, drop by our Slack
    channel / or leave some comments here! Some resources to get you help:'
  prefs: []
  type: TYPE_NORMAL
- en: 📣 join our community on [Slack](https://hamilton-opensource.slack.com/join/shared_invite/zt-1bjs72asx-wcUTgH7q7QX1igiQ5bbdcg#/shared-invite/email)
    — we’re more than happy to help answer questions you might have or get you started.
  prefs: []
  type: TYPE_NORMAL
- en: ⭐️ us on [GitHub](https://github.com/DAGWorks-Inc/hamilton)
  prefs: []
  type: TYPE_NORMAL
- en: 📝 leave us an [issue](https://github.com/DAGWorks-Inc/hamilton/issues) if you
    find something
  prefs: []
  type: TYPE_NORMAL
- en: 'Other Hamilton posts you might be interested in:'
  prefs: []
  type: TYPE_NORMAL
- en: '[tryhamilton.dev](https://www.tryhamilton.dev/) – an interactive tutorial in
    your browser!'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Pandas data transformations in Hamilton in 5 minutes](https://blog.dagworks.io/p/how-to-use-hamilton-with-pandas-in-5-minutes-89f63e5af8f5)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Lineage + Hamilton in 10 minutes](https://blog.dagworks.io/p/lineage-hamilton-in-10-minutes-c2b8a944e2e6)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Hamilton + Airflow for production](https://blog.dagworks.io/publish/post/130538397)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
