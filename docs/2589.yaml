- en: 'Graph Convolutional Networks: Introduction to GNNs'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ÂéüÊñáÔºö[https://towardsdatascience.com/graph-convolutional-networks-introduction-to-gnns-24b3f60d6c95?source=collection_archive---------0-----------------------#2023-08-14](https://towardsdatascience.com/graph-convolutional-networks-introduction-to-gnns-24b3f60d6c95?source=collection_archive---------0-----------------------#2023-08-14)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: A step-by-step guide using PyTorch Geometric
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@mlabonne?source=post_page-----24b3f60d6c95--------------------------------)[![Maxime
    Labonne](../Images/a7efdd305e3cc77d5509bbb1076d57d8.png)](https://medium.com/@mlabonne?source=post_page-----24b3f60d6c95--------------------------------)[](https://towardsdatascience.com/?source=post_page-----24b3f60d6c95--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----24b3f60d6c95--------------------------------)
    [Maxime Labonne](https://medium.com/@mlabonne?source=post_page-----24b3f60d6c95--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ¬∑
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fdc89da634938&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgraph-convolutional-networks-introduction-to-gnns-24b3f60d6c95&user=Maxime+Labonne&userId=dc89da634938&source=post_page-dc89da634938----24b3f60d6c95---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----24b3f60d6c95--------------------------------)
    ¬∑16 min read¬∑Aug 14, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F24b3f60d6c95&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgraph-convolutional-networks-introduction-to-gnns-24b3f60d6c95&user=Maxime+Labonne&userId=dc89da634938&source=-----24b3f60d6c95---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F24b3f60d6c95&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgraph-convolutional-networks-introduction-to-gnns-24b3f60d6c95&source=-----24b3f60d6c95---------------------bookmark_footer-----------)![](../Images/53a44290154e9eb7c20b5a32cd4d5642.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: '**Graph Neural Networks** (GNNs) represent one of the most captivating and
    rapidly evolving architectures within the deep learning landscape. As deep learning
    models designed to process data structured as graphs, GNNs bring remarkable versatility
    and powerful learning capabilities.'
  prefs: []
  type: TYPE_NORMAL
- en: Among the various types of GNNs, the **Graph Convolutional Networks** (GCNs)
    have emerged as the most [prevalent and broadly applied model](https://paperswithcode.com/methods/category/graph-models).
    GCNs are innovative due to their ability to leverage both the features of a node
    and its locality to make predictions, providing an effective way to handle graph-structured
    data.
  prefs: []
  type: TYPE_NORMAL
- en: In this article, we will delve into the mechanics of the GCN layer and explain
    its inner workings. Furthermore, we will explore its practical application for
    node classification tasks, using [PyTorch Geometric](https://pytorch-geometric.readthedocs.io/en/latest/index.html)
    as our tool of choice.
  prefs: []
  type: TYPE_NORMAL
- en: PyTorch Geometric is a specialized extension of PyTorch that has been created
    specifically for the development and implementation of GNNs. It is an advanced,
    yet user-friendly library that provides a comprehensive suite of tools to facilitate
    graph-based machine learning. To commence our journey, the PyTorch Geometric installation
    will be required. If you are using Google Colab, [PyTorch](https://pytorch.org/get-started/locally/)
    should already be in place, so all we need to do is execute a few additional commands.
  prefs: []
  type: TYPE_NORMAL
- en: All the code is available on [Google Colab](https://colab.research.google.com/drive/1ZugveUjRrbSNwUbryeKJN2wyhGFRCw0q?usp=sharing)
    and [GitHub](https://github.com/mlabonne/graph-neural-network-course).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Now that PyTorch Geometric is installed, let‚Äôs explore the dataset we will use
    in this tutorial.
  prefs: []
  type: TYPE_NORMAL
- en: üåê I. Graph data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[Graphs](https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)) are an
    essential structure for representing relationships between objects. You can encounter
    graph data in a multitude of real-world scenarios, such as social and computer
    networks, chemical structures of molecules, natural language processing, and image
    recognition, to name a few.'
  prefs: []
  type: TYPE_NORMAL
- en: In this article, we will study the infamous and much-used [Zachary‚Äôs karate
    club](https://en.wikipedia.org/wiki/Zachary%27s_karate_club) dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/00365b1de2d9e1641523993393467f7c.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: The Zachary‚Äôs karate club dataset embodies the relationships formed within a
    karate club as observed by Wayne W. Zachary during the 1970s. It is a kind of
    social network, where each node represents a club member, and edges between nodes
    represent interactions that occurred outside the club environment.
  prefs: []
  type: TYPE_NORMAL
- en: In this particular scenario, the members of the club are split into four distinct
    groups. Our task is to **assign the correct group to each member** (node classification),
    based on the pattern of their interactions.
  prefs: []
  type: TYPE_NORMAL
- en: Let‚Äôs import the dataset with PyG‚Äôs built-in function and try to understand
    the `Datasets` object it uses.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: This dataset only has 1 graph, where each node has a feature vector of 34 dimensions
    and is part of one out of four classes (our four groups). Actually, the `Datasets`
    object can be seen as a collection of `Data` (graph) objects.
  prefs: []
  type: TYPE_NORMAL
- en: We can further inspect our unique graph to know more about it.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The `[Data](https://pytorch-geometric.readthedocs.io/en/latest/modules/data.html)`
    object is particularly interesting. Printing it offers a good summary of the graph
    we''re studying:'
  prefs: []
  type: TYPE_NORMAL
- en: '`x=[34, 34]` is the **node feature matrix** with shape (number of nodes, number
    of features). In our case, it means that we have 34 nodes (our 34 members), each
    node being associated to a 34-dim feature vector.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`edge_index=[2, 156]` represents the **graph connectivity** (how the nodes
    are connected) with shape (2, number of directed edges).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`y=[34]` is the **node ground-truth labels**. In this problem, every node is
    assigned to one class (group), so we have one value for each node.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`train_mask=[34]` is an optional attribute that tells which nodes should be
    used for training with a list of `True` or `False` statements.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let‚Äôs print each of these tensors to understand what they store. Let‚Äôs start
    with the node features.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, the node feature matrix `x` is an identity matrix: it **doesn''t contain
    any relevant information** about the nodes. It could contain information like
    age, skill level, etc. but this is not the case in this dataset. It means we''ll
    have to classify our nodes just by looking at their connections.'
  prefs: []
  type: TYPE_NORMAL
- en: Now, let‚Äôs print the edge index.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: In graph theory and network analysis, connectivity between nodes is stored using
    a variety of data structures. The `edge_index` is one such data structure, where
    the graph's connections are stored in **two lists** (156 directed edges, which
    equate to 78 bidirectional edges). The reason for these two lists is that one
    list stores the source nodes, while the second one identifies the destination
    nodes.
  prefs: []
  type: TYPE_NORMAL
- en: This method is known as a **coordinate list** (COO) format, which is essentially
    a means to efficiently store a [sparse matrix](https://en.wikipedia.org/wiki/Sparse_matrix#Storing_a_sparse_matrix).
    Sparse matrices are data structures that efficiently store matrices with a majority
    of zero elements. In the COO format, only non-zero elements are stored, saving
    memory and computational resources.
  prefs: []
  type: TYPE_NORMAL
- en: Contrarily, a more intuitive and straightforward way to represent graph connectivity
    is through an **adjacency matrix** *A*. This is a square matrix where each element
    *A*·µ¢‚±º *s*pecifies the presence or absence of an edge from node *i* to node *j*
    in the graph. In other words, a non-zero element *A*·µ¢‚±º implies a connection from
    node *i* to node *j*, and a zero indicates no direct connection.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7a85c8e90254bb8ed5f76a90cbc92443.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: An adjacency matrix, however, is not as space-efficient as the COO format for
    sparse matrices or graphs with fewer edges. However, for clarity and easy interpretation,
    the adjacency matrix remains a popular choice for representing graph connectivity.
  prefs: []
  type: TYPE_NORMAL
- en: The adjacency matrix can be inferred from the `edge_index` with a utility function
    `to_dense_adj()`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: With graph data, it is relatively uncommon for nodes to be densely interconnected.
    As you can see, our adjacency matrix *A* is **sparse** (filled with zeros).
  prefs: []
  type: TYPE_NORMAL
- en: In many real-world graphs, most nodes are connected to only a few other nodes,
    resulting in a large number of zeros in the adjacency matrix. Storing so many
    zeros is not efficient at all, which is why the COO format is adopted by PyG.
  prefs: []
  type: TYPE_NORMAL
- en: On the contrary, ground-truth labels are easy to understand.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Our node ground-truth labels stored in `y` simply encode the group number (0,
    1, 2, 3) for each node, which is why we have 34 values.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, let‚Äôs print the train mask.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: The train mask shows which nodes are supposed to be used for training with `True`
    statements. These nodes represent the training set, while the others can be considered
    as the test set. This division helps in model evaluation by providing unseen data
    for testing.
  prefs: []
  type: TYPE_NORMAL
- en: 'But we‚Äôre not done yet! The `[Data](https://pytorch-geometric.readthedocs.io/en/latest/modules/data.html)`
    object has a lot more to offer. It provides various utility functions that enable
    the investigation of several properties of the graph. For instance:'
  prefs: []
  type: TYPE_NORMAL
- en: '`is_directed()` tells you if the graph is **directed**. A directed graph signifies
    that the adjacency matrix is not symmetric, i.e., the direction of edges matters
    in the connections between nodes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`isolated_nodes()` checks if some nodes are **not connected** to the rest of
    the graph. These nodes are likely to pose challenges in tasks like classification
    due to their lack of connections.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`has_self_loops()` indicates if at least one node is **connected to itself**.
    This is distinct from the concept of [loops](https://en.wikipedia.org/wiki/Loop_(graph_theory)):
    a loop implies a path that starts and ends at the same node, traversing other
    nodes in between.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the context of the Zachary‚Äôs karate club dataset, all these properties return
    `False`. This implies that the graph is not directed, does not have any isolated
    nodes, and none of its nodes are connected to themselves.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Finally, we can convert a graph from PyTorch Geometric to the popular graph
    library [NetworkX](https://networkx.org/) using `[to_networkx](https://pytorch-geometric.readthedocs.io/en/latest/modules/utils.html?highlight=to_networkx#torch_geometric.utils.to_networkx)`.
    This is particularly useful to visualize a small graph with `networkx` and `matplotlib`.
  prefs: []
  type: TYPE_NORMAL
- en: Let‚Äôs plot our dataset with a different color for each group.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/571cb9949785200ba0307b6172e0f3fb.png)'
  prefs: []
  type: TYPE_IMG
- en: This plot of Zachary‚Äôs karate club displays our 34 nodes, 78 (bidirectional)
    edges, and 4 labels with 4 different colors. Now that we‚Äôve seen the essentials
    of loading and handling a dataset with PyTorch Geometric, we can introduce the
    **Graph Convolutional Network** architecture.
  prefs: []
  type: TYPE_NORMAL
- en: ‚úâÔ∏è II. Graph Convolutional Network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This section aims to introduce and build the graph convolutional layer from
    the ground up.
  prefs: []
  type: TYPE_NORMAL
- en: 'In traditional neural networks, linear layers apply a **linear transformation**
    to the incoming data. This transformation converts input features *x* into hidden
    vectors *h* through the use of a weight matrix ùêñ. Ignoring biases for the time
    being, this can be expressed as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d99b824e5c2dca4657e4b2eb18b4d9e7.png)'
  prefs: []
  type: TYPE_IMG
- en: With graph data, an additional layer of complexity is added through the **connections
    between nodes**. These connections matter because, typically, in networks, it‚Äôs
    assumed that similar nodes are more likely to be linked to each other than dissimilar
    ones, a phenomenon known as [network homophily](https://en.wikipedia.org/wiki/Network_homophily).
  prefs: []
  type: TYPE_NORMAL
- en: We can enrich our **node representation** by merging its features with those
    of its neighbors. This operation is called convolution, or neighborhood aggregation.
    Let‚Äôs represent the neighborhood of node *i* including itself as *√ë*.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c368d96059e7977d7ff141a4b92b8bd6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Unlike filters in Convolutional Neural Networks (CNNs), our weight matrix ùêñ
    is unique and shared among every node. But there is another issue: nodes do not
    have a **fixed number of neighbors** like pixels do.'
  prefs: []
  type: TYPE_NORMAL
- en: How do we address cases where one node has only one neighbor, and another has
    500? If we simply sum the feature vectors, the resulting embedding *h* would be
    much larger for the node with 500 neighbors. To ensure a **similar range** of
    values for all nodes and comparability between them, we can normalize the result
    based on the **degree** of nodes, where degree refers to the number of connections
    a node has.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/99665492296bf842d6b89bd2c19fb899.png)'
  prefs: []
  type: TYPE_IMG
- en: We‚Äôre almost there! Introduced by Kipf et al. (2016), the [graph convolutional
    layer](https://arxiv.org/abs/1609.02907) has one final improvement.
  prefs: []
  type: TYPE_NORMAL
- en: 'The authors observed that features from nodes with numerous neighbors propagate
    much more easily than those from more isolated nodes. To offset this effect, they
    suggested assigning **bigger weights** to features from nodes with fewer neighbors,
    thus balancing the influence across all nodes. This operation is written as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/04becb820335cbd89cebb547724d12ea.png)'
  prefs: []
  type: TYPE_IMG
- en: Note that when *i* and *j* have the same number of neighbors, it is equivalent
    to our own layer. Now, let‚Äôs see how to implement it in Python with PyTorch Geometric.
  prefs: []
  type: TYPE_NORMAL
- en: üß† III. Implementing a GCN
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: PyTorch Geometric provides the `GCNConv` function, which directly implements
    the graph convolutional layer.
  prefs: []
  type: TYPE_NORMAL
- en: In this example, we‚Äôll create a basic Graph Convolutional Network with a single
    GCN layer, a ReLU activation function, and a linear output layer. This output
    layer will yield **four values** corresponding to our four categories, with the
    highest value determining the class of each node.
  prefs: []
  type: TYPE_NORMAL
- en: In the following code block, we define the GCN layer with a 3-dimensional hidden
    layer.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: If we added a second GCN layer, our model would not only aggregate feature vectors
    from the neighbors of each node, but also from the neighbors of these neighbors.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can **stack several graph layers** to aggregate more and more distant values,
    but there‚Äôs a catch: if we add too many layers, the aggregation becomes so intense
    that all the embeddings end up looking the same. This phenomenon is called **over-smoothing**
    and can be a real problem when you have too many layers.'
  prefs: []
  type: TYPE_NORMAL
- en: Now that we‚Äôve defined our GNN, let‚Äôs write a simple training loop with PyTorch.
    I chose a regular cross-entropy loss since it‚Äôs a multi-class classification task,
    with Adam as optimizer. In this article, we won‚Äôt implement a train/test split
    to keep things simple and focus on how GNNs learn instead.
  prefs: []
  type: TYPE_NORMAL
- en: 'The training loop is standard: we try to predict the correct labels, and we
    compare the GCN‚Äôs results to the values stored in `data.y`. The error is calculated
    by the cross-entropy loss and backpropagated with Adam to fine-tune our GNN''s
    weights and biases. Finally, we print metrics every 10 epochs.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: Great! Without much surprise, we reach 100% accuracy on the training set (full
    dataset). It means that our model learned to correctly assign every member of
    the karate club to its correct group.
  prefs: []
  type: TYPE_NORMAL
- en: We can produce a neat visualization by animating the graph and see the evolution
    of the GNN‚Äôs predictions during the training process.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/04e6b675ab152228d57739474387a402.png)'
  prefs: []
  type: TYPE_IMG
- en: The first predictions are random, but the GCN perfectly labels every node after
    a while. Indeed, the final graph is the same as the one we plotted at the end
    of the first section. But what does the GCN really learn?
  prefs: []
  type: TYPE_NORMAL
- en: By aggregating features from neighboring nodes, the GNN learns a vector representation
    (or **embedding**) of every node in the network. In our model, the final layer
    just learns how to use these representations to produce the best classifications.
    However, embeddings are the real products of GNNs.
  prefs: []
  type: TYPE_NORMAL
- en: Let‚Äôs print the embeddings learned by our model.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, embeddings do not need to have the same dimensions as feature
    vectors. Here, I chose to reduce the number of dimensions from 34 (`dataset.num_features`)
    to three to get a nice visualization in 3D.
  prefs: []
  type: TYPE_NORMAL
- en: Let‚Äôs plot these embeddings before any training happens, at epoch 0.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/a961a759d54dbe8911d59fb3ab66f824.png)'
  prefs: []
  type: TYPE_IMG
- en: We see every node from Zachary‚Äôs karate club with their true labels (and not
    the model‚Äôs predictions). For now, they‚Äôre all over the place since the GNN is
    not trained yet. But if we plot these embeddings at each step of the training
    loop, we‚Äôd be able to visualize what the GNN truly learns.
  prefs: []
  type: TYPE_NORMAL
- en: Let‚Äôs see how they evolve over time, as the GCN gets better and better at classifying
    nodes.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/b4af8ca00284bd6d1418674e7584cebe.png)'
  prefs: []
  type: TYPE_IMG
- en: Our Graph Convolutional Network (GCN) has effectively learned embeddings that
    group similar nodes into **distinct clusters**. This enables the final linear
    layer to distinguish them into separate classes with ease.
  prefs: []
  type: TYPE_NORMAL
- en: 'Embeddings are not unique to GNNs: they can be found everywhere in deep learning.
    They don‚Äôt have to be 3D either: actually, they rarely are. For instance, language
    models like [BERT](https://arxiv.org/abs/1810.04805) produce embeddings with 768
    or even 1024 dimensions.'
  prefs: []
  type: TYPE_NORMAL
- en: Additional dimensions store more information about nodes, text, images, etc.
    but they also create bigger models that are more difficult to train. This is why
    keeping low-dimensional embeddings as long as possible is advantageous.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Graph Convolutional Networks are an incredibly versatile architecture that can
    be applied in **many contexts**. In this article, we familiarized ourselves with
    the PyTorch Geometric library and objects like `Datasets` and `Data`. Then, we
    successfully reconstructed a graph convolutional layer from the ground up. Next,
    we put theory into practice by implementing a GCN, which gave us an understanding
    of practical aspects and how individual components interact. Finally, we visualized
    the training process and obtained a clear perspective of what it involves for
    such a network.
  prefs: []
  type: TYPE_NORMAL
- en: 'Zachary‚Äôs karate club is a simplistic dataset, but it is good enough to understand
    the most important concepts in graph data and GNNs. Although we only talked about
    node classification in this article, there are other tasks GNNs can accomplish:
    **link prediction** (e.g., to recommend a friend), **graph classification** (e.g.,
    to label molecules), **graph generation** (e.g., to create new molecules), and
    so on.'
  prefs: []
  type: TYPE_NORMAL
- en: Beyond GCN, numerous GNN layers and architectures have been proposed by researchers.
    In the next article, we‚Äôll introduce the [Graph Attention Network](https://mlabonne.github.io/blog/gat/)
    (GAT) architecture, which dynamically computes the GCN‚Äôs normalization factor
    and the importance of each connection with an attention mechanism.
  prefs: []
  type: TYPE_NORMAL
- en: If you want to know more about graph neural networks, dive deeper into the world
    of GNNs with my book, [Hands-On Graph Neural Networks](https://mlabonne.github.io/blog/book.html).
  prefs: []
  type: TYPE_NORMAL
- en: Next article
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[](/graph-attention-networks-in-python-975736ac5c0c?source=post_page-----24b3f60d6c95--------------------------------)
    [## Chapter 2: Graph Attention Networks: Self-Attention Explained'
  prefs: []
  type: TYPE_NORMAL
- en: A guide to GNNs with self-attention using PyTorch Geometric
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/graph-attention-networks-in-python-975736ac5c0c?source=post_page-----24b3f60d6c95--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: '*Learn more about machine learning and support my work with one click ‚Äî become
    a Medium member here:*'
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/@mlabonne/membership?source=post_page-----24b3f60d6c95--------------------------------)
    [## Join Medium with my referral link ‚Äî Maxime Labonne'
  prefs: []
  type: TYPE_NORMAL
- en: As a Medium member, a portion of your membership fee goes to writers you read,
    and you get full access to every story‚Ä¶
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: medium.com](https://medium.com/@mlabonne/membership?source=post_page-----24b3f60d6c95--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: '*If you‚Äôre already a member, you can* [*follow me on Medium*](https://medium.com/@mlabonne)*.*'
  prefs: []
  type: TYPE_NORMAL
