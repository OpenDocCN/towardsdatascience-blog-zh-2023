- en: 'Step by Step Basics: Text Classifier'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/step-by-step-basics-text-classifier-e666c6bac52b?source=collection_archive---------2-----------------------#2023-02-17](https://towardsdatascience.com/step-by-step-basics-text-classifier-e666c6bac52b?source=collection_archive---------2-----------------------#2023-02-17)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: An Instructional Guide and Flow Diagram for Building a Supervised Machine Learning
    Text Classifier in Python
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@lucydickinson?source=post_page-----e666c6bac52b--------------------------------)[![Lucy
    Dickinson](../Images/5a075bb38f9133678d55a26b2683729f.png)](https://medium.com/@lucydickinson?source=post_page-----e666c6bac52b--------------------------------)[](https://towardsdatascience.com/?source=post_page-----e666c6bac52b--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----e666c6bac52b--------------------------------)
    [Lucy Dickinson](https://medium.com/@lucydickinson?source=post_page-----e666c6bac52b--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F243c7ff13cc2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fstep-by-step-basics-text-classifier-e666c6bac52b&user=Lucy+Dickinson&userId=243c7ff13cc2&source=post_page-243c7ff13cc2----e666c6bac52b---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----e666c6bac52b--------------------------------)
    ·10 min read·Feb 17, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fe666c6bac52b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fstep-by-step-basics-text-classifier-e666c6bac52b&user=Lucy+Dickinson&userId=243c7ff13cc2&source=-----e666c6bac52b---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fe666c6bac52b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fstep-by-step-basics-text-classifier-e666c6bac52b&source=-----e666c6bac52b---------------------bookmark_footer-----------)![](../Images/7616f5c71c4500433b491ed8fdd0db92.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Photo by [Patrick Tomasso](https://unsplash.com/@impatrickt?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: Let’s cut to the chase. There are a lot of steps involved in building a **text
    classifier** and understanding the world of **Natural Language Processing (NLP)**.
    These steps have to be implemented in a specific order. There are even more steps
    required if the target class in the data is imbalanced. Learning this all from
    scratch can be a bit of a minefield. There are plenty of learning resources online,
    yet finding a holistic guide that covers everything in a high level proved tricky.
    So, I am writing this article to hopefully provide some transparency on this process
    with a 10 easy step guide.
  prefs: []
  type: TYPE_NORMAL
- en: I’m going to start with providing a flow diagram that I’ve compiled with all
    the necessary steps and key points to understand, all the way from clarifying
    the task to deploying a trained text classifier.
  prefs: []
  type: TYPE_NORMAL
- en: First of all, what is a text classifier?
  prefs: []
  type: TYPE_NORMAL
- en: A text classifier is an algorithm that learns the presence or pattern of words
    to predict some kind of target or outcome, usually a category such as whether
    an email is spam or not.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: It is important to mention here that I will be focussing on building a text
    classifier using Supervised Machine Learning methods. An alternative approach
    is to use Deep Learning methods such as Neural Networks.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s take a peek at that flow diagram.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/cf105d0f0e197d6701c455062b55f7d4.png)'
  prefs: []
  type: TYPE_IMG
- en: Diagram by author
  prefs: []
  type: TYPE_NORMAL
- en: There’s a lot to digest there. Let’s break it up into bitesize chunks and walk
    through each section.
  prefs: []
  type: TYPE_NORMAL
- en: 1\. Clarify the task
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This is one of the most important steps of any data science project. Ensure
    that you have fully grasped the question that is being asked. Do you have the
    relevant data available to answer the question? Does your methodology align with
    what the stakeholder is expecting? If you need stakeholder buy in, don’t go building
    some super complex model that will be hard to interpret. Start simple, bring everyone
    along on that journey with you.
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Data quality checks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Another essential step to any project. Your model will only be as good as the
    data that goes in, so make sure duplicates are removed and missing values are
    dealt with accordingly.
  prefs: []
  type: TYPE_NORMAL
- en: 3\. Exploratory Data Analysis (EDA)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now we can move onto some text data specific analysis. EDA is all about understanding
    the data and getting a feel for what you can derive from it. One of the key points
    for this step is to understand the **target class distribution**. You can use
    either the pandas .value_counts() method or plot a bar chart to visualise the
    distribution of each class within the dataset. You’ll be able to see which are
    the **majority** and **minority classes.**
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1a7238087383fb8dd9dd6e49f45175eb.png)'
  prefs: []
  type: TYPE_IMG
- en: Imbalanced class distribution of a binary labelled dataset
  prefs: []
  type: TYPE_NORMAL
- en: Models do not perform well with imbalanced data. The model will often ignore
    the minority class(es) as there simply is not enough data to train the model to
    detect them. Alas, it’s not the end of the world if you find yourself with an
    imbalanced dataset with a heavy skew towards one of your target classes. That’s
    in fact quite normal. It’s just important to know this ahead of your model building
    process so you can adjust for this later on.
  prefs: []
  type: TYPE_NORMAL
- en: The presence of an imbalanced dataset should also get you thinking about which
    metrics you should use to assess model performance. In this instance, ‘accuracy’
    (proportion of correct predictions) really **isn’t** your friend. Let’s say you
    have a dataset with a binary target class where 80% of data is labelled ‘red’
    and 20% is labelled ‘blue’. Your model could simply predict ‘red’ for the entire
    test set and still be 80% accurate. Hence, the accuracy of a model may be misleading,
    given that your model could simply predict the majority class.
  prefs: []
  type: TYPE_NORMAL
- en: Some better metrics to use are **recall** (proportion of true positives predicted
    correctly), **precision** (proportion of positive predictions predicted correctly),
    or the mean of the two, the **F1 score**. Pay close attention to these scores
    for your minority classes once you’re in the model building stage. It’ll be these
    scores that you’ll want to improve.
  prefs: []
  type: TYPE_NORMAL
- en: 4\. Text pre-processing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now on to some fun stuff! Text data can contain a whole load of stuff that just
    really isn’t useful to any machine learning model (depending on the nature of
    the task). This process is really about removing the ‘noise’ within your dataset,
    homogenising words and stripping it back to the bare bones so that only the useful
    words and ultimately, features, remain.
  prefs: []
  type: TYPE_NORMAL
- en: 'Generally, you’ll want to remove punctuation, special characters, stop-words
    (words like ‘this’, ‘the’, ‘and’) and reduce each word down to its lemma or stem.
    You can play around with making your own functions to get an idea of what’s in
    your data before cleansing it. Take the function below for example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Then when you’ve got a better idea of what needs to be removed from your data,
    have a go at writing a function that does it all for you in one go:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: You can then run the first function again on the cleansed data to check that
    the everything that you wanted to be removed has indeed been removed.
  prefs: []
  type: TYPE_NORMAL
- en: For those who noticed the functions above don’t remove any stop-words, well
    spotted. You can remove stop-words during the vectorisation process in a few steps
    time.
  prefs: []
  type: TYPE_NORMAL
- en: 5\. Train-test split
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This is getting its own sub heading because it is so important to do this step
    BEFORE your start fiddling with the features. Split your data using sklearn’s
    train_test_split() function and then **leave the test data alone** so there’s
    no risk of data leakage.
  prefs: []
  type: TYPE_NORMAL
- en: If your data are imbalanced, there are a few optional arguments (‘shuffle’ and
    ‘stratify’) that you can specify within the test-train split to ensure an even
    split across your target classes. This ensures that your minority classes don’t
    end up all in your training or test set exclusively.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 6\. Text vectorisation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Models cannot interpret words. Instead, the words have to be converted into
    numbers using a process known as vectorisation. There are two methods for vectorisation;
    Bag of Words and Word Embeddings. Bag of Words methods look for exact matches
    of words between texts, whereas Word Embedding methods take into account word
    context, and so can look for similar words between texts. An interesting article
    comparing the two methods can be found [here](https://medium.com/swlh/word-embeddings-versus-bag-of-words-the-curious-case-of-recommender-systems-6ac1604d4424).
  prefs: []
  type: TYPE_NORMAL
- en: For the Bag of Words method, sentences are tokenised and then each unique word
    becomes a feature. Each unique word in the dataset will correspond to a feature,
    where each feature will have an integer associated depending on how many times
    that word appears in the text (a Word Count Vector — sklearn’s CountVectorizer())
    or a weighted integer that indicates the importance of the word in the text (a
    TF-IDF Vector — sklearn’s TfidVectorizer()). A useful article explaining TF-IDF
    vectorisation can be found [here](/tf-idf-simplified-aba19d5f5530#:~:text=Term%20frequency%2Dinverse%20document%20frequency,specific%20term%20in%20a%20document.).
  prefs: []
  type: TYPE_NORMAL
- en: Be sure to train the vectoriser object on the training data and then use this
    to transform the test data.
  prefs: []
  type: TYPE_NORMAL
- en: 7\. Model selection
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: It’s a good idea to try out a few classification models to see which performs
    best with your data. You can then use performance metrics to select the most appropriate
    model to optimise. I did this by running a for loop which iterated over each model
    using the cross_validate() function.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 8\. Baseline model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Before you get carried away with tweaking your chosen model’s hyperparameters
    in a bid to get those performance metrics up, STOP. Make a note of your model’s
    performance before you start optimising it. You’ll only be able to know (and prove)
    that your model improved by comparing it to the baseline scores. It helps you
    with stakeholder buy in and storytelling if you’re in a position where you’ve
    been asked to walk through your methodology.
  prefs: []
  type: TYPE_NORMAL
- en: Create an empty DataFrame, then after each model iteration, append your metric(s)
    of choice along with the number or name of the iteration so you can clearly see
    how your model progressed through your optimisation attempts.
  prefs: []
  type: TYPE_NORMAL
- en: 9\. Model tuning — rectifying imbalanced data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Generally, fine tuning your model might involve tweaking its hyperparameters
    and feature engineering with the aim of improving the model’s predictive capability.
    For this section however, I’m going to focus on the techniques that can be used
    to reduce the effect of class imbalance.
  prefs: []
  type: TYPE_NORMAL
- en: Short of collecting more data for the minority classes, there are 5 methods
    (that I know of) that you can use to address class imbalance. The majority are
    a form of feature engineering, with the aim of either oversampling the minority
    class(es) or undersampling the majority class(es) to even out the overall class
    distribution.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s take a quick look at each method:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Adding a minority class penalty**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Classification algorithms have a parameter, mostly known as ‘class_weight’ that
    you can specify when training the model. This is essentially a penalty function,
    where a higher penalty will be given if a minority class is misclassified in order
    to deter against misclassification. You can either elect for an automated argument,
    or you may be able to manually assign the penalty based on the class. Be sure
    to read the documentation for the algorithm you’re using.
  prefs: []
  type: TYPE_NORMAL
- en: '**2.** **Oversample minority class**'
  prefs: []
  type: TYPE_NORMAL
- en: Random oversampling involves randomly duplicating examples from the minority
    class(es) and adding them to the training dataset to create a uniform class distribution.
    This method can lead to overfitting as no new data points are being generated,
    so be sure to check for this.
  prefs: []
  type: TYPE_NORMAL
- en: The python library [imblearn](https://imbalanced-learn.org/stable/) contains
    functions for oversampling and undersampling data. It is important to know that
    any oversampling or undersampling techniques are only applied to the training
    data.
  prefs: []
  type: TYPE_NORMAL
- en: If you are using a cross-validation method to fit the data to a model, you will
    need to use a pipeline to ensure that only the training folds are being oversampled.
    The Pipeline() function can be imported from the imblearn library.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '**3\. Undersample majority class**'
  prefs: []
  type: TYPE_NORMAL
- en: An alternative method to the above is to instead undersample the majority class,
    rather than oversample the majority class. Some might argue it’s never worth removing
    data if you have it, but this could be an option worth trying for yourself. Again,
    the imblearn library has oversampling functions to use.
  prefs: []
  type: TYPE_NORMAL
- en: '**4\. Synthesise new instances of minority class**'
  prefs: []
  type: TYPE_NORMAL
- en: New instances of the minority classes can be generated using a process called
    SMOTE (Synthetic Minority Oversampling Technique), which again can be implemented
    using the imblearn library. There is a great article [here](https://machinelearningmastery.com/smote-oversampling-for-imbalanced-classification/)
    that provides some examples of implementing SMOTE.
  prefs: []
  type: TYPE_NORMAL
- en: '**5\. Text augmentation**'
  prefs: []
  type: TYPE_NORMAL
- en: New data can be generated using synonyms of existing data to increase the number
    of data points of minority classes. Methods involve synonym replacement and back
    translation (translating into one language and back to the original language).
    The [nlpaug library](https://nlpaug.readthedocs.io/en/latest/) is a handy library
    for exploring these options.
  prefs: []
  type: TYPE_NORMAL
- en: Running through each of these balancing processing steps iteratively and comparing
    the scores with your baseline score will then allow you to see which method works
    best with your data.
  prefs: []
  type: TYPE_NORMAL
- en: 10\. Deploy trained classifier
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This is the time to push the trained classifier into a production environment
    and let it work its magic on unseen and unlabelled data, provided it has been
    tested. The method of deployment depends on the platforms that your company uses,
    [here](https://neptune.ai/blog/deploy-nlp-models-in-production) is an article
    that walks through some of the options.
  prefs: []
  type: TYPE_NORMAL
- en: 'There we have it! 10 easy steps to building a text classifier in python using
    a supervised machine learning approach. In summary, we learnt:'
  prefs: []
  type: TYPE_NORMAL
- en: The sequence of steps required to build a text classifier
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The significance of inspecting class distribution and understand how this could
    impact model performance metrics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Text pre-processing steps
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to select an appropriate model and record baseline model performance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Methods to resolve class imbalance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: I hope this has been useful. Please leave any thoughts, comments or suggestions
    :)
  prefs: []
  type: TYPE_NORMAL
