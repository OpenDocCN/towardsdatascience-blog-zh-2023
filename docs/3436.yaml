- en: 'The History of Open-Source LLMs: Better Base Models (Part Two)'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 开源 LLM 的历史：更好的基础模型（第二部分）
- en: 原文：[https://towardsdatascience.com/the-history-of-open-source-llms-better-base-models-part-two-6ca51ae74ebe?source=collection_archive---------7-----------------------#2023-11-18](https://towardsdatascience.com/the-history-of-open-source-llms-better-base-models-part-two-6ca51ae74ebe?source=collection_archive---------7-----------------------#2023-11-18)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/the-history-of-open-source-llms-better-base-models-part-two-6ca51ae74ebe?source=collection_archive---------7-----------------------#2023-11-18](https://towardsdatascience.com/the-history-of-open-source-llms-better-base-models-part-two-6ca51ae74ebe?source=collection_archive---------7-----------------------#2023-11-18)
- en: How LLaMA, MPT, Falcon, and LLaMA-2 put open-source LLMs on the map…
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: LLaMA、MPT、Falcon 和 LLaMA-2 如何将开源 LLM 推向舞台……
- en: '[](https://wolfecameron.medium.com/?source=post_page-----6ca51ae74ebe--------------------------------)[![Cameron
    R. Wolfe, Ph.D.](../Images/52bb88d7cf1105501be2fae5ccbe7a03.png)](https://wolfecameron.medium.com/?source=post_page-----6ca51ae74ebe--------------------------------)[](https://towardsdatascience.com/?source=post_page-----6ca51ae74ebe--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----6ca51ae74ebe--------------------------------)
    [Cameron R. Wolfe, Ph.D.](https://wolfecameron.medium.com/?source=post_page-----6ca51ae74ebe--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://wolfecameron.medium.com/?source=post_page-----6ca51ae74ebe--------------------------------)[![Cameron
    R. Wolfe, Ph.D.](../Images/52bb88d7cf1105501be2fae5ccbe7a03.png)](https://wolfecameron.medium.com/?source=post_page-----6ca51ae74ebe--------------------------------)[](https://towardsdatascience.com/?source=post_page-----6ca51ae74ebe--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----6ca51ae74ebe--------------------------------)
    [Cameron R. Wolfe, Ph.D.](https://wolfecameron.medium.com/?source=post_page-----6ca51ae74ebe--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F28aa6026c553&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-history-of-open-source-llms-better-base-models-part-two-6ca51ae74ebe&user=Cameron+R.+Wolfe%2C+Ph.D.&userId=28aa6026c553&source=post_page-28aa6026c553----6ca51ae74ebe---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----6ca51ae74ebe--------------------------------)
    ·16 min read·Nov 18, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F6ca51ae74ebe&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-history-of-open-source-llms-better-base-models-part-two-6ca51ae74ebe&user=Cameron+R.+Wolfe%2C+Ph.D.&userId=28aa6026c553&source=-----6ca51ae74ebe---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F28aa6026c553&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-history-of-open-source-llms-better-base-models-part-two-6ca51ae74ebe&user=Cameron+R.+Wolfe%2C+Ph.D.&userId=28aa6026c553&source=post_page-28aa6026c553----6ca51ae74ebe---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----6ca51ae74ebe--------------------------------)
    ·16分钟阅读·2023年11月18日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F6ca51ae74ebe&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-history-of-open-source-llms-better-base-models-part-two-6ca51ae74ebe&user=Cameron+R.+Wolfe%2C+Ph.D.&userId=28aa6026c553&source=-----6ca51ae74ebe---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F6ca51ae74ebe&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-history-of-open-source-llms-better-base-models-part-two-6ca51ae74ebe&source=-----6ca51ae74ebe---------------------bookmark_footer-----------)![](../Images/1cad7310b9d455e5c1b8cf3ad38f5a03.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F6ca51ae74ebe&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-history-of-open-source-llms-better-base-models-part-two-6ca51ae74ebe&source=-----6ca51ae74ebe---------------------bookmark_footer-----------)![](../Images/1cad7310b9d455e5c1b8cf3ad38f5a03.png)'
- en: (Photo by [Iñaki del Olmo](https://unsplash.com/@inakihxz?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)
    on [Unsplash](https://unsplash.com/photos/assorted-title-of-books-piled-in-the-shelves-NIJuEQw0RKg?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash))
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: (照片由 [Iñaki del Olmo](https://unsplash.com/@inakihxz?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)
    提供，来源于 [Unsplash](https://unsplash.com/photos/assorted-title-of-books-piled-in-the-shelves-NIJuEQw0RKg?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash))
- en: Open-source research on large language models (LLMs) is incredibly valuable,
    as it aims to democratize a powerful and influential technology. Although open-source
    LLMs are now commonly used and widely studied, this area of research saw some
    initial struggles that were difficult to overcome. Namely, open-source LLMs performed
    poorly at first and were heavily criticized. Within this overview, we will study
    a line of research that changed this narrative by making high-performing pre-trained
    LLMs available to everyone. Given that pre-training a language model is so expensive,
    the models we will study here are especially impactful. After these high-performing
    base models were created and released, many people could conduct research using
    these models at marginal added cost.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 开源大语言模型（LLMs）研究极具价值，因为它旨在使这一强大而有影响力的技术实现民主化。虽然开源LLMs现在已经被广泛使用和研究，但这个研究领域最初经历了一些难以克服的困难。即，开源LLMs最初表现不佳，并受到严重批评。在本概述中，我们将研究一项改变这种叙事的研究线，该研究通过向所有人提供高性能的预训练LLMs来改变局面。由于预训练语言模型成本高昂，我们在这里研究的模型特别具有影响力。在这些高性能基础模型被创建和发布后，许多人可以以边际附加成本使用这些模型进行研究。
- en: “The capabilities of LLMs are remarkable considering the seemingly straightforward
    nature of the training methodology.” *— from [14]*
  id: totrans-10
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “考虑到训练方法看似简单，LLMs的能力是令人惊叹的。” *— 引自[14]*
- en: '**The current series.** This overview is part two of a three part series on
    the history of open-source LLMs. The [first part](https://medium.com/towards-data-science/the-history-of-open-source-llms-early-days-part-one-d782bcd8f7e8)
    in the series overviewed initial attempts at creating open-source LLMs. Here,
    we will study the most popular open-source base models (i.e., language models
    that…'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '**当前系列。** 本概述是关于开源大语言模型（LLMs）历史的三部分系列中的第二部分。系列的[第一部分](https://medium.com/towards-data-science/the-history-of-open-source-llms-early-days-part-one-d782bcd8f7e8)概述了创建开源LLMs的初步尝试。在这里，我们将研究最流行的开源基础模型（即语言模型）……'
