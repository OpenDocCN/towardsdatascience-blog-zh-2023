- en: 'The History of Open-Source LLMs: Better Base Models (Part Two)'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/the-history-of-open-source-llms-better-base-models-part-two-6ca51ae74ebe?source=collection_archive---------7-----------------------#2023-11-18](https://towardsdatascience.com/the-history-of-open-source-llms-better-base-models-part-two-6ca51ae74ebe?source=collection_archive---------7-----------------------#2023-11-18)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: How LLaMA, MPT, Falcon, and LLaMA-2 put open-source LLMs on the map…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://wolfecameron.medium.com/?source=post_page-----6ca51ae74ebe--------------------------------)[![Cameron
    R. Wolfe, Ph.D.](../Images/52bb88d7cf1105501be2fae5ccbe7a03.png)](https://wolfecameron.medium.com/?source=post_page-----6ca51ae74ebe--------------------------------)[](https://towardsdatascience.com/?source=post_page-----6ca51ae74ebe--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----6ca51ae74ebe--------------------------------)
    [Cameron R. Wolfe, Ph.D.](https://wolfecameron.medium.com/?source=post_page-----6ca51ae74ebe--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F28aa6026c553&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-history-of-open-source-llms-better-base-models-part-two-6ca51ae74ebe&user=Cameron+R.+Wolfe%2C+Ph.D.&userId=28aa6026c553&source=post_page-28aa6026c553----6ca51ae74ebe---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----6ca51ae74ebe--------------------------------)
    ·16 min read·Nov 18, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F6ca51ae74ebe&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-history-of-open-source-llms-better-base-models-part-two-6ca51ae74ebe&user=Cameron+R.+Wolfe%2C+Ph.D.&userId=28aa6026c553&source=-----6ca51ae74ebe---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F6ca51ae74ebe&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-history-of-open-source-llms-better-base-models-part-two-6ca51ae74ebe&source=-----6ca51ae74ebe---------------------bookmark_footer-----------)![](../Images/1cad7310b9d455e5c1b8cf3ad38f5a03.png)'
  prefs: []
  type: TYPE_NORMAL
- en: (Photo by [Iñaki del Olmo](https://unsplash.com/@inakihxz?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)
    on [Unsplash](https://unsplash.com/photos/assorted-title-of-books-piled-in-the-shelves-NIJuEQw0RKg?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash))
  prefs: []
  type: TYPE_NORMAL
- en: Open-source research on large language models (LLMs) is incredibly valuable,
    as it aims to democratize a powerful and influential technology. Although open-source
    LLMs are now commonly used and widely studied, this area of research saw some
    initial struggles that were difficult to overcome. Namely, open-source LLMs performed
    poorly at first and were heavily criticized. Within this overview, we will study
    a line of research that changed this narrative by making high-performing pre-trained
    LLMs available to everyone. Given that pre-training a language model is so expensive,
    the models we will study here are especially impactful. After these high-performing
    base models were created and released, many people could conduct research using
    these models at marginal added cost.
  prefs: []
  type: TYPE_NORMAL
- en: “The capabilities of LLMs are remarkable considering the seemingly straightforward
    nature of the training methodology.” *— from [14]*
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**The current series.** This overview is part two of a three part series on
    the history of open-source LLMs. The [first part](https://medium.com/towards-data-science/the-history-of-open-source-llms-early-days-part-one-d782bcd8f7e8)
    in the series overviewed initial attempts at creating open-source LLMs. Here,
    we will study the most popular open-source base models (i.e., language models
    that…'
  prefs: []
  type: TYPE_NORMAL
