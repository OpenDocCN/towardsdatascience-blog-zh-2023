["```py\ngit clone https://github.com/tomasonjo/langchain2neo4j\n```", "```py\nsh seed_db.sh\n```", "```py\nif model_name in ['gpt-3.5-turbo', 'gpt-4']:\n    llm = ChatOpenAI(temperature=0, model_name=model_name)\nelse:\n    raise Exception(f\"Model {model_name} is currently not supported\")\n```", "```py\nmemory = ConversationBufferMemory(\n    memory_key=\"chat_history\", return_messages=True)\n```", "```py\nAssistant can ask the user to use tools to look up information \nthat may be helpful in answering the users original question.\nThe tools the human can use are:\n{{tools}}\n{format_instructions}\nUSER'S INPUT - - - - - - - - - - \nHere is the user's input \n(remember to respond with a markdown code snippet of a \njson blob with a single action, and NOTHING else):\n{{{{input}}}}\n```", "```py\ntools = [\n    Tool(\n        name=\"Cypher search\",\n        func=cypher_tool.run,\n        description=\"\"\"\n        Utilize this tool to search within a movie database, \n        specifically designed to answer movie-related questions.\n        This specialized tool offers streamlined search capabilities\n        to help you find the movie information you need with ease.\n        Input should be full question.\"\"\",\n    ),\n    Tool(\n        name=\"Keyword search\",\n        func=fulltext_tool.run,\n        description=\"Utilize this tool when explicitly told to use \n        keyword search.Input should be a list of relevant movies \n        inferred from the question.\",\n    ),\n    Tool(\n        name=\"Vector search\",\n        func=vector_tool.run,\n        description=\"Utilize this tool when explicity told to use \n        vector search.Input should be full question.Do not include \n        agent instructions.\",\n    ),\n\n]\n```", "```py\ndef _call(self, inputs: Dict[str, str]) -> Dict[str, str]:\n    chat_prompt = ChatPromptTemplate.from_messages(\n        [self.system_prompt] + inputs['chat_history'] + [self.human_prompt])\n    cypher_executor = LLMChain(\n        prompt=chat_prompt, llm=self.llm, callback_manager=self.callback_manager\n    )\n    cypher_statement = cypher_executor.predict(\n        question=inputs[self.input_key], stop=[\"Output:\"])\n    # If Cypher statement was not generated due to lack of context\n    if not \"MATCH\" in cypher_statement:\n        return {'answer': 'Missing context to create a Cypher statement'}\n    context = self.graph.query(cypher_statement)\n\n    return {'answer': context}\n```", "```py\nSYSTEM_TEMPLATE = \"\"\"\nYou are an assistant with an ability to generate Cypher queries based off \nexample Cypher queries. Example Cypher queries are:\\n\"\"\" + examples + \"\"\"\\n\nDo not response with any explanation or any other information except the \nCypher query. You do not ever apologize and strictly generate cypher statements\nbased of the provided Cypher examples. Do not provide any Cypher statements \nthat can't be inferred from Cypher examples. Inform the user when you can't \ninfer the cypher statement due to the lack of context of the conversation \nand state what is the missing context.\n\"\"\"\n```", "```py\ndef _call(self, inputs: Dict[str, str]) -> Dict[str, Any]:\n    \"\"\"Extract entities, look up info and answer question.\"\"\"\n    question = inputs[self.input_key]\n    params = generate_params(question)\n    context = self.graph.query(\n        fulltext_search, {'query': params})\n    return {self.output_key: context}\n```", "```py\nCALL db.index.fulltext.queryNodes(\"movie\", $query) \nYIELD node, score\nWITH node, score LIMIT 5\nCALL {\n  WITH node\n  MATCH (node)-[r:!RATED]->(target)\n  RETURN coalesce(node.name, node.title) + \" \" + type(r) + \" \" + coalesce(target.name, target.title) AS result\n  UNION\n  WITH node\n  MATCH (node)<-[r:!RATED]-(target)\n  RETURN coalesce(target.name, target.title) + \" \" + type(r) + \" \" + coalesce(node.name, node.title) AS result\n}\nRETURN result LIMIT 100\n```", "```py\ndef _call(self, inputs: Dict[str, str]) -> Dict[str, Any]:\n    \"\"\"Embed a question and do vector search.\"\"\"\n    question = inputs[self.input_key]\n    embedding = self.embeddings.embed_query(question)\n    context = self.graph.query(\n        vector_search, {'embedding': embedding})\n    return {self.output_key: context}\n```", "```py\nWITH $embedding AS e\nCALL db.index.vector.queryNodes('movies',5, e) yield node as m, score\nCALL {\n  WITH m\n  MATCH (m)-[r:!RATED]->(target)\n  RETURN coalesce(m.name, m.title) + \" \" + type(r) + \" \" + coalesce(target.name, target.title) AS result\n  UNION\n  WITH m\n  MATCH (m)<-[r:!RATED]-(target)\n  RETURN coalesce(target.name, target.title) + \" \" + type(r) + \" \" + coalesce(m.name, m.title) AS result\n}\nRETURN result LIMIT 100\n```"]