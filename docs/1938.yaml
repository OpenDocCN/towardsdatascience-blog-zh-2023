- en: Deep Reinforcement Learning improved sorting algorithms
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度强化学习改进了排序算法
- en: 原文：[https://towardsdatascience.com/deep-reinforcement-learning-improved-sorting-algorithms-2f6a1969e3af?source=collection_archive---------13-----------------------#2023-06-13](https://towardsdatascience.com/deep-reinforcement-learning-improved-sorting-algorithms-2f6a1969e3af?source=collection_archive---------13-----------------------#2023-06-13)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/deep-reinforcement-learning-improved-sorting-algorithms-2f6a1969e3af?source=collection_archive---------13-----------------------#2023-06-13](https://towardsdatascience.com/deep-reinforcement-learning-improved-sorting-algorithms-2f6a1969e3af?source=collection_archive---------13-----------------------#2023-06-13)
- en: How Google DeepMind created a more efficient sorting algorithm
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 谷歌DeepMind如何创建出更高效的排序算法
- en: '[](https://medium.com/@jonathanbogerd?source=post_page-----2f6a1969e3af--------------------------------)[![Jonathan
    Bogerd](../Images/e844961c6ea9766476d3d520dd993ae2.png)](https://medium.com/@jonathanbogerd?source=post_page-----2f6a1969e3af--------------------------------)[](https://towardsdatascience.com/?source=post_page-----2f6a1969e3af--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----2f6a1969e3af--------------------------------)
    [Jonathan Bogerd](https://medium.com/@jonathanbogerd?source=post_page-----2f6a1969e3af--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@jonathanbogerd?source=post_page-----2f6a1969e3af--------------------------------)[![Jonathan
    Bogerd](../Images/e844961c6ea9766476d3d520dd993ae2.png)](https://medium.com/@jonathanbogerd?source=post_page-----2f6a1969e3af--------------------------------)[](https://towardsdatascience.com/?source=post_page-----2f6a1969e3af--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----2f6a1969e3af--------------------------------)
    [Jonathan Bogerd](https://medium.com/@jonathanbogerd?source=post_page-----2f6a1969e3af--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F3863776b2716&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-reinforcement-learning-improved-sorting-algorithms-2f6a1969e3af&user=Jonathan+Bogerd&userId=3863776b2716&source=post_page-3863776b2716----2f6a1969e3af---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----2f6a1969e3af--------------------------------)
    ·8 min read·Jun 13, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F2f6a1969e3af&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-reinforcement-learning-improved-sorting-algorithms-2f6a1969e3af&user=Jonathan+Bogerd&userId=3863776b2716&source=-----2f6a1969e3af---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F3863776b2716&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-reinforcement-learning-improved-sorting-algorithms-2f6a1969e3af&user=Jonathan+Bogerd&userId=3863776b2716&source=post_page-3863776b2716----2f6a1969e3af---------------------post_header-----------)
    发表在 [数据科学前沿](https://towardsdatascience.com/?source=post_page-----2f6a1969e3af--------------------------------)
    · 8分钟阅读 · 2023年6月13日 [](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F2f6a1969e3af&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-reinforcement-learning-improved-sorting-algorithms-2f6a1969e3af&user=Jonathan+Bogerd&userId=3863776b2716&source=-----2f6a1969e3af---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F2f6a1969e3af&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-reinforcement-learning-improved-sorting-algorithms-2f6a1969e3af&source=-----2f6a1969e3af---------------------bookmark_footer-----------)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F2f6a1969e3af&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-reinforcement-learning-improved-sorting-algorithms-2f6a1969e3af&source=-----2f6a1969e3af---------------------bookmark_footer-----------)'
- en: 'Last week, Google DeepMind published a paper in the journal Nature in which
    they claimed to have found a more efficient sorting algorithm by using Deep Reinforcement
    Learning (DLR). DeepMind is known for pushing the boundaries in Reinforcement
    Learning (RL). A few years ago, they beat the best player in the game of Go with
    their agent AlphaGo, after using a similar program to crush existing chess engines.
    In 2022 DeepMind introduced AlphaTensor, a program that found a more efficient
    matrix multiplication algorithm with DLR. The techniques used are similar to DeepMind’s
    newest achievement: improving a standard sorting algorithm. In this article, I
    will discuss how they achieved this by introducing Reinforcement Learning, Monte
    Carlo Tree Search, and the details of DeepMind’s approach and agent AlphaDev.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 上周，Google DeepMind 在《自然》杂志上发表了一篇论文，声称通过使用深度强化学习（DLR）发现了一种更高效的排序算法。DeepMind 以推动强化学习（RL）的边界而闻名。几年前，他们利用类似程序击败了围棋游戏中的最佳选手
    AlphaGo，之后又用相似的程序战胜了现有的国际象棋引擎。在 2022 年，DeepMind 推出了 AlphaTensor，这是一种利用 DLR 找到更高效矩阵乘法算法的程序。所使用的技术类似于
    DeepMind 最新的成就：改进标准排序算法。在本文中，我将讨论他们如何通过引入强化学习、蒙特卡洛树搜索以及 DeepMind 的方法和代理 AlphaDev
    的细节来实现这一点。
- en: '![](../Images/3cc34c7da927bcebedd0a2ae41751b25.png)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3cc34c7da927bcebedd0a2ae41751b25.png)'
- en: Photo by [AltumCode](https://unsplash.com/@altumcode?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由 [AltumCode](https://unsplash.com/@altumcode?utm_source=medium&utm_medium=referral)
    提供，来源于 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: '**Sorting**'
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**排序**'
- en: 'Writing a sorting algorithm is simple: compare all values one by one to find
    the lowest (or highest) value, save this value at the first element of your return
    list, and continue to the next value until no more value is left. Although this
    algorithm will work, it is far from efficient. Because sorting is very common
    in computer programs, efficient sorting algorithms are subject to intensive research.
    DeepMind focused on two variants of sorting: Fixed sorting and variable sorting.
    Fixed sorting involves sorting a sequence of values with a fixed, predefined length.
    In variable sorting, the size of the list of values is not defined in advance.
    Only the maximum size of the list is provided. Both variants of sorting are used
    extensively in state-of-the-art sorting algorithms. Often, a large list is sorted
    by repeatedly sorting small sections of the list.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 编写排序算法很简单：逐个比较所有值以找到最低（或最高）值，将此值保存在返回列表的第一个元素中，然后继续处理下一个值，直到没有更多值为止。虽然这个算法有效，但远非高效。由于排序在计算机程序中非常常见，因此高效的排序算法受到深入研究。DeepMind
    专注于两种排序变体：固定排序和变量排序。固定排序涉及对具有固定预定义长度的值序列进行排序。在变量排序中，值列表的大小没有预先定义。只提供了列表的最大大小。这两种排序变体在最先进的排序算法中广泛使用。通常，通过反复排序列表的小部分来对大列表进行排序。
- en: Current state-of-the-art algorithms use so-called sorting networks in both fixed
    and variable sorting. In this article, I will not discuss the details of sorting
    networks. You can find more information [here](https://dl.acm.org/doi/pdf/10.1145/1468075.1468121).
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，最先进的算法在固定排序和变量排序中都使用了所谓的排序网络。在本文中，我不会讨论排序网络的详细信息。你可以在[这里](https://dl.acm.org/doi/pdf/10.1145/1468075.1468121)找到更多信息。
- en: '**Reinforcement Learning**'
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**强化学习**'
- en: In this section, I will introduce the main concepts of Reinforcement Learning.
    Reinforcement Learning is a branch of Machine Learning, in which an agent is tasked
    with finding the best actions in an environment, given the current state. The
    state of an environment contains all relevant aspects of an environment that can
    or should influence the action to be taken. The best action is defined as the
    action that maximizes the (discounted) cumulative reward. Actions are taken sequentially,
    and after each action, the obtained reward and the new state are recorded. Usually,
    an environment has some termination criteria after which the next episode starts.
    Early versions of RL used tables to keep track of the value of certain states
    and actions with the idea of always taking the action with the highest value.
    Deep neural networks often replace these tables because they can generalize better
    and enumerating all states is often impossible. When deep neural networks are
    used for Reinforcement Learning, the term Deep Reinforcement Learning is used.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我将介绍强化学习的主要概念。强化学习是机器学习的一个分支，其中一个智能体被任务赋予在环境中找到最佳行动，基于当前状态。环境的状态包含了所有可能影响所采取行动的相关方面。最佳行动定义为最大化（折扣）累积奖励的行动。行动是依次进行的，在每次行动后，获得的奖励和新状态都会被记录。通常，环境有一些终止标准，在这些标准之后，下一个回合开始。早期版本的
    RL 使用表格来跟踪某些状态和行动的值，目的是总是采取具有最高值的行动。深度神经网络常常替代这些表格，因为它们能够更好地泛化，而列举所有状态通常是不可能的。当深度神经网络用于强化学习时，称之为深度强化学习。
- en: '**Monte Carlo Tree Search**'
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**蒙特卡洛树搜索**'
- en: AlphaDev is trained using Deep Reinforcement Learning and Monte Carlo Tree Search,
    which is a search algorithm to find the best action given some initial state.
    It is often combined with DRL, for instance in AlphaZero and AlphaGo. It deserves
    its own article, but a summary is provided here.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: AlphaDev 使用深度强化学习和蒙特卡洛树搜索进行训练，这是一种在给定初始状态下寻找最佳行动的搜索算法。它通常与 DRL 结合使用，例如在 AlphaZero
    和 AlphaGo 中。它值得拥有自己的一篇文章，但这里提供了一个摘要。
- en: Monte Carlo Tree Search (MCTS) builds a tree of possible outcome states where
    the current state is the root node. For a given number of simulations, this tree
    will be explored to find the consequences of taking certain actions. In each simulation,
    a rollout (sometimes called playout) is used to expand one node into child nodes
    if the game is not terminated at that point. Which action to take is based on
    selection criteria. In our case, the probability of selecting an action is given
    by the policy network which will be discussed below. The value of a node is a
    measure of how good the state of that node is. It is determined by the value network,
    which is also discussed in a future section. If a terminal state is reached, the
    value is replaced by the true reward value of the environment.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 蒙特卡洛树搜索（MCTS）构建了一个可能结果状态的树，其中当前状态是根节点。对于给定数量的模拟，将探索这棵树以找出采取某些行动的后果。在每次模拟中，使用回放（有时称为游戏过程）来扩展一个节点为子节点，如果游戏在此时没有结束。采取哪种行动基于选择标准。在我们的案例中，选择某个行动的概率由策略网络提供，下面将讨论这一点。节点的值是衡量该节点状态好坏的指标。它由值网络确定，这也将在未来的部分中讨论。如果达到终止状态，则该值会被环境的真实奖励值所替代。
- en: The values are propagated up the search tree. In this way, the value of the
    current state depends on the value of the states that can be reached from the
    current state.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 值在搜索树中向上传播。通过这种方式，当前状态的值依赖于从当前状态可以到达的状态的值。
- en: '**DeepMind’s Approach**'
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**DeepMind 的方法**'
- en: The DRL agent of DeepMind that is trained to improve the sorting algorithm implementation
    is called AlphaDev. The task of AlphaDev is to write a more efficient sorting
    algorithm in Assembly. Assembly is the bridge between high-level languages such
    as C++, Java, and Python and machine code. If you use sort in C++, the compiler
    compiles your program into assembly code. The assembler then converts this code
    to machine code. AlphaDev is tasked with selecting a series of assembly instructions
    to create a sorting algorithm that is both correct and fast. This is a difficult
    task because adding one instruction can make the program completely wrong, even
    if it was correct before.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 由 DeepMind 训练的 DRL 代理，旨在改进排序算法实现，称为 AlphaDev。AlphaDev 的任务是编写一个更高效的汇编排序算法。汇编语言是高层语言（如
    C++、Java 和 Python）与机器代码之间的桥梁。如果你在 C++ 中使用 sort，编译器会将你的程序编译成汇编代码。汇编器随后将这些代码转换为机器代码。AlphaDev
    的任务是选择一系列汇编指令，以创建一个既正确又快速的排序算法。这是一个困难的任务，因为添加一条指令可能会使程序完全错误，即使之前是正确的。
- en: AlphaDev finds an efficient sorting algorithm by creating and solving the AssemblyGame.
    In each turn, it has to select one action corresponding to a CPU instruction.
    By formulating this problem as a game, it can easily fit the standard Reinforcement
    Learning framework.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: AlphaDev 通过创建并解决 AssemblyGame 来寻找高效的排序算法。在每一轮中，它必须选择一个与 CPU 指令对应的动作。通过将这个问题表述为一个游戏，它可以轻松适应标准的强化学习框架。
- en: '**Reinforcement Learning Formulation**'
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**强化学习公式**'
- en: A standard RL formulation contains states, actions, rewards, and a termination
    criterium.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 标准的 RL 公式包含状态、动作、奖励和终止标准。
- en: '**State**'
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**状态**'
- en: The state of the AssemblyGame consists of two parts. First, a representation
    of the current program. AlphaDev uses the output of a Transformer, a neural network
    architecture, to represent the current algorithm. Transformers have had much success
    with large language models recently and are a suitable choice for encoding the
    current algorithm because it is text-based. The second part of the state is a
    representation of the state of the memory and registers after running the current
    algorithm. This is done by a conventional deep neural network.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: AssemblyGame 的状态由两部分组成。首先是当前程序的表示。AlphaDev 使用 Transformer 的输出，Transformer 是一种神经网络架构，用于表示当前算法。最近
    Transformer 在大规模语言模型中取得了很大成功，并且由于算法是基于文本的，Transformer 也非常适合对当前算法进行编码。状态的第二部分是运行当前算法后内存和寄存器状态的表示。这通过传统的深度神经网络来完成。
- en: '**Action**'
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**动作**'
- en: 'The actions of the AssemblyGame are appending a legal assembly instruction
    to the program. AlphaDev can choose to append any instruction as long as it is
    legal. DeepMind created the following six rules for legal actions:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: AssemblyGame 的动作是将合法的汇编指令附加到程序中。AlphaDev 可以选择添加任何合法的指令。DeepMind 为合法动作创建了以下六条规则：
- en: Memory locations are always read in incremental order
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 内存位置总是按递增顺序读取
- en: Registers are allocated in incremental order
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 寄存器按递增顺序分配
- en: Read and write to each memory location once
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 每个内存位置读取和写入一次
- en: No consecutive compare instructions
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 不允许连续比较指令
- en: No comparison or conditional moves to memory
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 不允许将比较或条件移动到内存中
- en: Non-initialized registers cannot be used
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 未初始化的寄存器不能使用
- en: The last two actions are illegal from a programming standpoint, the others are
    enforced because they will not change the program. By removing these actions the
    search space is limited without affecting the algorithm.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 从编程角度来看，最后两个动作是非法的，其他的则是被强制执行的，因为它们不会改变程序。通过删除这些动作，搜索空间被限制而不影响算法。
- en: '**Reward**'
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**奖励**'
- en: The reward of the AssemblyGame is a combination of two parts. The first element
    of the reward is a correctness score. Based on a series of test sequences, the
    provided algorithm is evaluated. The closer the algorithm gets to properly sorting
    the test sequence, the higher the reward for correctness. The second element of
    the reward is the speed or latency of the program. This is measured by the number
    of lines of the program, if there are no conditional branches. In practice, this
    means that the length of the program is used for fixed sorting because no conditions
    are required for that program. For variable sorting, the algorithm has to condition
    on the actual length of the sequence. Because not all parts of the algorithm are
    used when sorting, the actual latency of the program is measured instead of the
    number of lines.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: AssemblyGame的奖励由两部分组成。奖励的第一个元素是正确性分数。根据一系列测试序列，提供的算法会被评估。算法越接近正确排序测试序列，正确性的奖励就越高。奖励的第二个元素是程序的速度或延迟。这通过程序的行数来衡量，如果没有条件分支的话。在实践中，这意味着程序的长度用于固定排序，因为该程序不需要条件。对于可变排序，算法必须根据序列的实际长度进行条件判断。由于排序时并非所有部分的算法都会被使用，因此测量的是程序的实际延迟，而不是行数。
- en: '**Termination and Goal**'
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**终止和目标**'
- en: According to the paper released by DeepMind, they terminate the AssemblyGame
    after “a limited number of steps”. What this exactly means is unclear, but I assume
    they limit the number of steps based on the current human benchmark. The goal
    of the game is to find a correct and fast algorithm. If either an incorrect or
    a slow algorithm is provided by AlphaDev, the game is lost.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 根据DeepMind发布的论文，他们在“有限步骤”后终止AssemblyGame。这具体意味着什么不清楚，但我猜他们是根据当前的人工基准限制步骤数量。游戏的目标是找到一个正确且快速的算法。如果AlphaDev提供了一个错误或慢速的算法，游戏就会失败。
- en: '**Policy and Value Networks**'
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**策略和价值网络**'
- en: To use Monte Carlo Tree Search a policy and a value network are required. The
    policy network sets the probability for each action and the value network is trained
    to evaluate a state. The policy network is updated based on the number of visits
    of a certain state. This creates an iterative procedure, where the policy is used
    to perform MCTS and the policy network is updated with the number of visits of
    a state in MCTS.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 使用蒙特卡罗树搜索（Monte Carlo Tree Search）时，需要一个策略网络和一个价值网络。策略网络设置每个动作的概率，而价值网络则训练用于评估状态。策略网络根据特定状态的访问次数进行更新。这创建了一个迭代过程，其中策略用于执行MCTS，策略网络则根据MCTS中状态的访问次数进行更新。
- en: The value network outputs two values, one for the correctness of the algorithm
    and one for the latency of the algorithm. According to DeepMind, this yields better
    results than combining these values in one score by the network. The value network
    is updated based on the obtained rewards.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 价值网络输出两个值，一个是算法的正确性，一个是算法的延迟。根据DeepMind的说法，这比通过网络将这些值组合成一个分数的结果更好。价值网络根据获得的奖励进行更新。
- en: '**Results**'
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**结果**'
- en: After training AlphaDev, it found shorter algorithms for fixed sorting with
    sequence lengths 3 and 5\. For fixed sorting with length 4, it found the current
    implementation, so no improvement was made there. AlphaDev achieved these results
    for fixed sorting by applying two new ideas. These new ideas, called the swap
    and the copy move, result in fewer comparisons between values and therefore a
    faster algorithm. For fixed sorting with length 3, DeepMind proved that no shorter
    program exists by brute forcing through all options with a shorter program length.
    For longer programs, this approach is infeasible, as the search space grows exponentially.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练AlphaDev后，它找到了一些长度为3和5的固定排序的更短算法。对于长度为4的固定排序，它找到了当前的实现，因此没有改进。在固定排序中，AlphaDev通过应用两个新想法实现了这些结果。这些新想法被称为交换（swap）和复制移动（copy
    move），它们减少了值之间的比较次数，从而使算法更快。对于长度为3的固定排序，DeepMind通过穷举所有较短程序长度的选项证明了没有更短的程序存在。对于较长的程序，这种方法不可行，因为搜索空间呈指数级增长。
- en: For variable sorting, AlphaDev came up with a new design of the algorithm. For
    instance for variable sorting a sequence with a length at most 4, AlphaDev suggested
    to first sort 3 values and then perform a simplified version of sort for the last
    remaining element. The figure below shows the algorithm design for Varsort(4)
    given by AlphaDev.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 对于可变排序，AlphaDev提出了算法的新设计。例如，对于长度最多为4的可变排序序列，AlphaDev建议先对3个值进行排序，然后对最后剩余的元素执行简化版本的排序。下图展示了AlphaDev提供的Varsort(4)的算法设计。
- en: '![](../Images/520282b71fa09dc24538b67782345e34.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/520282b71fa09dc24538b67782345e34.png)'
- en: Image by D. Mankowitz et al. , Faster sorting algorithms discovered using deep
    reinforcement learning (2023), Nature
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由 D. Mankowitz 等人提供，《使用深度强化学习发现更快的排序算法》（2023），《自然》
- en: Overall, faster algorithms were found for fixed and variable sorting, proving
    that DRL can be used to implement efficient algorithms. The sorting implementation
    in C++ has been updated to use these new algorithms. Details on the performance
    gains achieved by AlphaDev can be found in the table below.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '-   总体而言，为固定和可变排序发现了更快的算法，证明了深度强化学习可以用于实现高效的算法。C++中的排序实现已经更新为使用这些新算法。有关 AlphaDev
    实现的性能提升的详细信息，请参见下表。'
- en: '![](../Images/d9479b578df767dabbd6b0cc9cfe887d.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d9479b578df767dabbd6b0cc9cfe887d.png)'
- en: Table by D. Mankowitz et al. , Faster sorting algorithms discovered using deep
    reinforcement learning (2023), Nature
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 表格由 D. Mankowitz 等人提供，《使用深度强化学习发现更快的排序算法》（2023），《自然》
- en: To test if this approach generalizes to other programming implementations, DeepMind
    also tested AlphaDev on a competitive coding challenge and a protocol buffer used
    by Google. In both cases, AlphaDev was able to come up with more efficient implementations,
    proving that DLR with Monte Carlo Tree Search is a valid approach to finding efficient
    implementations for common algorithms.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '-   为了测试这种方法是否可以推广到其他编程实现，DeepMind 还在一个竞争性编码挑战和 Google 使用的协议缓冲区上测试了 AlphaDev。在这两个案例中，AlphaDev
    能够提出更高效的实现，证明了使用蒙特卡洛树搜索的深度强化学习是一种找到常见算法高效实现的有效方法。'
- en: '**Conclusion**'
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**结论**'
- en: Deep Reinforcement Learning has shown to be successful in many different environments,
    from games like Go and Chess to algorithm implementation as seen here. Although
    the domain is challenging, often dependent on low-level implementation details,
    and hyperparameter choices, these successes show the results that can be achieved
    by this powerful concept.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '-   深度强化学习在许多不同的环境中取得了成功，从围棋和国际象棋等游戏到算法实现（如本文所示）。尽管该领域具有挑战性，通常取决于低级实现细节和超参数选择，但这些成功展示了这一强大概念能够取得的成果。'
- en: I hope this article gave you an insight into the latest achievement in RL. If
    you want a more detailed explanation of Monte Carlo Tree Search or other algorithms
    that I discussed, please let me know!
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '-   我希望这篇文章让你对强化学习的最新成就有了一些了解。如果你想要更详细的蒙特卡洛树搜索或我讨论的其他算法的解释，请告诉我！'
- en: '**Sources**'
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**来源**'
- en: '[1] K. Batcher, Sorting networks and their applications (1968), Proceedings
    of the April 30 — May 2, 1968, spring joint computer conference (pp. 307–314)'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] K. Batcher，《排序网络及其应用》（1968），《1968年4月30日至5月2日春季联合计算机会议论文集》（第307–314页）'
- en: '[2] D. Mankowitz et al., Faster sorting algorithms discovered using deep reinforcement
    learning (2023), Nature 618.7964: 257–263'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] D. Mankowitz 等人，《使用深度强化学习发现更快的排序算法》（2023），《自然》618.7964: 257–263'
- en: '[3] D. Silver et al., Mastering chess and shogi by self-play with a general
    reinforcement learning algorithm (2017), arXiv preprint arXiv:1712.01815'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] D. Silver 等人，《通过自我对弈掌握国际象棋和将棋的通用强化学习算法》（2017），arXiv 预印本 arXiv:1712.01815'
- en: '[4] D. Silver et al., Mastering the game of Go with deep neural networks and
    tree search (2016), Nature 529.7587: 484–489'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] D. Silver 等人，《通过深度神经网络和树搜索掌握围棋》（2016），《自然》529.7587: 484–489'
- en: '[5] A. Fawzi et al., Discovering faster matrix multiplication algorithms with
    reinforcement learning (2022), Nature 610.7930: 47–53'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] A. Fawzi 等人，《通过强化学习发现更快的矩阵乘法算法》（2022），《自然》610.7930: 47–53'
- en: '[6] C. Browne et al., A survey of monte carlo tree search methods (2012), IEEE
    Transactions on Computational Intelligence and AI in games 4.1: 1–43.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '[6] C. Browne 等人，《蒙特卡洛树搜索方法概述》（2012），《IEEE计算智能与游戏中的人工智能》4.1: 1–43。'
