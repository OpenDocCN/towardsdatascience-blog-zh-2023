- en: Analyze Your Website with NLP and Knowledge Graphs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/analyze-your-website-with-nlp-and-knowledge-graphs-88e291f6cbf4?source=collection_archive---------1-----------------------#2023-01-05](https://towardsdatascience.com/analyze-your-website-with-nlp-and-knowledge-graphs-88e291f6cbf4?source=collection_archive---------1-----------------------#2023-01-05)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Combine various NLP techniques to construct a knowledge graph representing your
    website
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://bratanic-tomaz.medium.com/?source=post_page-----88e291f6cbf4--------------------------------)[![Tomaz
    Bratanic](../Images/d5821aa70918fcb3fc1ff0013497b3d5.png)](https://bratanic-tomaz.medium.com/?source=post_page-----88e291f6cbf4--------------------------------)[](https://towardsdatascience.com/?source=post_page-----88e291f6cbf4--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----88e291f6cbf4--------------------------------)
    [Tomaz Bratanic](https://bratanic-tomaz.medium.com/?source=post_page-----88e291f6cbf4--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F57f13c0ea39a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fanalyze-your-website-with-nlp-and-knowledge-graphs-88e291f6cbf4&user=Tomaz+Bratanic&userId=57f13c0ea39a&source=post_page-57f13c0ea39a----88e291f6cbf4---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----88e291f6cbf4--------------------------------)
    ·14 min read·Jan 5, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F88e291f6cbf4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fanalyze-your-website-with-nlp-and-knowledge-graphs-88e291f6cbf4&user=Tomaz+Bratanic&userId=57f13c0ea39a&source=-----88e291f6cbf4---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F88e291f6cbf4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fanalyze-your-website-with-nlp-and-knowledge-graphs-88e291f6cbf4&source=-----88e291f6cbf4---------------------bookmark_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: A website is a reflection of the company. For the most part, it is used to inform
    users about various products and services and drive sales. However, the website
    grows and changes over time and many minor and major changes are introduced. As
    a result, it is not uncommon to end up with a disorganized website that fails
    to accomplish its original mission. Therefore, it makes sense to regularly evaluate
    the structure and content of the website to make it as optimized as possible.
    Optimizing websites is a huge business, and consequently, there are multiple commercial
    tools to help you with SEO and other suggestions. However, I will show you how
    you can create a comprehensive and detailed representation of the content on your
    website with a little bit of coding knowledge, which will allow you to analyze
    and improve it.
  prefs: []
  type: TYPE_NORMAL
- en: You can extract the structure of the website using any of the available web
    scrapers. Additionally, it makes sense to not only evaluate the structure but
    also the content of the website by utilizing various natural language processing
    techniques. Since most websites are copyrighted, I have decided to use the Neo4j
    documentation website as an example in this tutorial. The content of the documentation
    website is available under the [CC 4.0 license](https://neo4j.com/docs/license/).
    However, you can apply a similar workflow to any web page you desire.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6ad00f4f0b3e3a40467335675a7bddb2.png)'
  prefs: []
  type: TYPE_IMG
- en: Extracting information from documentation to construct a knowledge graph. Image
    by the author.
  prefs: []
  type: TYPE_NORMAL
- en: It might seem a bit magical (if you ignore my arrows) how you might construct
    a knowledge graph using information from your website. Throughout this post, I
    aim to bring more clarity to information extraction and provide you with tools
    you can use on your own. I have used similar approaches with [medical documents](/construct-a-biomedical-knowledge-graph-with-nlp-1f25eddc54a0),
    [news](https://medium.com/neo4j/making-sense-of-news-the-knowledge-graph-way-d33810ce5005),
    or even [crypto reports](https://medium.com/neo4j/monitoring-the-cryptocurrency-space-with-nlp-and-knowledge-graphs-92a1cfaebd1a),
    and now we’ll analyze a website with the help of NLP and knowledge graphs.
  prefs: []
  type: TYPE_NORMAL
- en: Data collection and modeling workflow
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/8ae215b99d5174684f778dd69e16f5e8.png)'
  prefs: []
  type: TYPE_IMG
- en: Data collection and modeling workflow. Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: The data collection and preprocessing consist of three parts.
  prefs: []
  type: TYPE_NORMAL
- en: 'Web scraper: A Python script that walks through the documentation web page
    and collects links and text'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'NLP pipeline: Extracts keywords from text and calculate text embeddings to
    detect similar/duplicate content'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Knowledge graph: Store results as a knowledge graph for further analysis'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The code for the data collection and preprocessing is available [on GitHub as
    a Jupyter notebook](https://github.com/tomasonjo/blogs/blob/master/neo4jdocs/Preprocessing.ipynb).
  prefs: []
  type: TYPE_NORMAL
- en: '*You don’t have to run the data collection and processing yourself since it
    takes a couple of hours. I have prepared a* [*Neo4j dump*](https://drive.google.com/file/d/1vpQe2gfoPiUVXQpUq-J6Fsjxq5BamyXM/view?usp=sharing)
    *that you can use if you want to follow along with the analysis later in the post.*'
  prefs: []
  type: TYPE_NORMAL
- en: '**Web scraper**'
  prefs: []
  type: TYPE_NORMAL
- en: I usually use [Python Selenium](https://selenium-python.readthedocs.io/) for
    web scraping, but you can use any of other libraries or languages you want to
    extract relevant information from websites. I won’t go into too many details about
    the code, as the goal of this post is not to teach you how to scrape websites.
    However, you can examine the [Jupyter notebook](https://github.com/tomasonjo/blogs/blob/master/neo4jdocs/Preprocessing.ipynb)
    that handles the web scraping.
  prefs: []
  type: TYPE_NORMAL
- en: Specifically for the Neo4j documentation website, I avoided scraping the links
    from the left and top navigation bars as that would introduce much noise in the
    graph since most of the pages have the same navigation bars present.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/bf41ce13ef482afed63dfc518a87ae2f.png)'
  prefs: []
  type: TYPE_IMG
- en: Links from navigation bars are ignored during scraping. Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: With the Neo4j documentation website, I wanted to capture how a user could traverse
    the documentation without using the navigation bar. Otherwise, we would introduce
    noise in the knowledge graph as all the pages would be linking to the same pages
    in the navigation bars. Additionally, I have focused on extracting text and links
    from only documentation web pages, so some product or marketing pages were not
    scraped for their content.
  prefs: []
  type: TYPE_NORMAL
- en: '**Natural language processing**'
  prefs: []
  type: TYPE_NORMAL
- en: The natural language processing step involves extracting keywords and calculating
    text embeddings to detect similar and duplicate content. Before even considering
    training your own NLP model, it is always a good idea to check the [HuggingFace
    model repository](https://huggingface.co/models) and see if any publically available
    models are a good fit for your use case.
  prefs: []
  type: TYPE_NORMAL
- en: After a bit of research, I have found a [keyword extraction model](https://huggingface.co/yanekyuk/bert-uncased-keyword-extractor)
    made available by **Yankı Ekin Yüksel** that we will use. I really love how simple
    it is to load and run a model using transformers and HuggingFace.
  prefs: []
  type: TYPE_NORMAL
- en: The following code loads the keyword extraction model and prepares a NLP pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: You don’t have to download models or worry about file paths. Instead, you can
    simply define the model name as the argument of the tokenizer and the model, and
    the transformers library does all the work for you.
  prefs: []
  type: TYPE_NORMAL
- en: The pipeline returns tokens, which are not necessarily a word. Therefore, we
    need to construct the words back from tokens after the NLP pipeline finishes.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The example shows that the model extracted **cloud computing**, **vmware**,
    and **broadcom** from the given text. The results seem fitting for our use case
    as we are analyzing the Neo4j documentation, which should contain many technological
    keywords.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we also need to calculate text embeddings that will help us identify similar
    and duplicate content. Again, I’ve search the HuggingFace model repository a bit
    and came across a [sentence transformers model](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2)
    that can be used to identify similar sentences or paragraphs. Again, the model
    can be loaded and used with as little as three lines of codes.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: We need to convert the result to a float of lists as Neo4j Driver doesn’t support
    NumPy arrays.
  prefs: []
  type: TYPE_NORMAL
- en: '**Knowledge graph construction**'
  prefs: []
  type: TYPE_NORMAL
- en: After the web scraper and natural language processing steps are done, we can
    go ahead and construct a knowledge graph. You might have already guessed that
    we are going to be using Neo4j to store our knowledge graph. You can use a [free
    cloud instance](https://neo4j.com/cloud/platform/aura-graph-database/) or setup
    a [local environment](https://neo4j.com/download/).
  prefs: []
  type: TYPE_NORMAL
- en: The graph schema after the initial import is defined as the following.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/65579d768a46deaf64679cbfb3fb339e.png)'
  prefs: []
  type: TYPE_IMG
- en: Initial graph schema. Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: In the center of our graph are web pages. We know their URL address, text embedding
    value, and whether or not the web scraper extracted text from the page. Pages
    can also link or redirect to other pages, which is represented with according
    relationship. As a part of the NLP workflow, we have also detected keywords on
    the site, which we will store as separate nodes.
  prefs: []
  type: TYPE_NORMAL
- en: Check out the [preprocessing notebook](https://github.com/tomasonjo/blogs/blob/master/neo4jdocs/Preprocessing.ipynb)
    if you are interested in the code implementation of the data import. Otherwise,
    we will jump straight to the network analysis part.
  prefs: []
  type: TYPE_NORMAL
- en: '**Network Analysis**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*I have prepared a* [*Neo4j database dump*](https://drive.google.com/file/d/1vpQe2gfoPiUVXQpUq-J6Fsjxq5BamyXM/view?usp=sharing)
    *if you don’t want to scrape the Neo4j documentation but would still like to follow
    the network analysis examples.*'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/26d47caa8d619cfe56d2ce2b2c1d69f7.png)'
  prefs: []
  type: TYPE_IMG
- en: Sample subgraph of the Neo4j documentation website knowledge graph. Image by
    the author.
  prefs: []
  type: TYPE_NORMAL
- en: I will walk you through some website network analysis examples that I found
    interesting. We will be using the [Graph Data Science Python](https://neo4j.com/docs/graph-data-science/current/python-client/)
    client, which is ideal tool to perform network analysis with Neo4j from Python.
  prefs: []
  type: TYPE_NORMAL
- en: The Jupyter Notebook with all the relevant code for the network analysis is
    [available on GitHub](https://github.com/tomasonjo/blogs/blob/master/neo4jdocs/Analysis.ipynb).
  prefs: []
  type: TYPE_NORMAL
- en: '**Overall statistics**'
  prefs: []
  type: TYPE_NORMAL
- en: First of, we will begin by evaluating the size of our dataset by counting the
    number of nodes and relationships with the `apoc.meta.stats` procedure.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Our knowledge graph has 15370 pages and 4199 keywords, along with 62365 links
    and 723 redirects. I was not expecting this many pages. However, considering that
    the documentation covers multiple products across multiple versions, it makes
    sense that the number of pages on a website can explode. Additionally, many links
    point to pages outside the Neo4j website.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will evaluate from how many pages we successfuly retrieved content
    information.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: We have successfully retrieved the content and calculated text embeddings from
    9688 web pages. The web scraper focused on recovering content from the documentation
    website while mostly ignoring the structure and text of the product and similar
    pages. Therefore, there are 2972 on the Neo4j website for which we haven’t retrieved
    content. Finally, the Neo4j website links to 2710 outside its primary domain.
    Pages outside of the Neo4j documentation website were explicitly ignored during
    web scraping.
  prefs: []
  type: TYPE_NORMAL
- en: Out of curiosity, we can list ten random outside web pages that Neo4j links
    to the most.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '*Results*'
  prefs: []
  type: TYPE_NORMAL
- en: The most linked web page is actually a localhost URL, which is the default address
    of the Neo4j Browser. Following are some links to APOC releases on GitHub. Finally,
    it seems that Neo4j has some products or services that support integrations with
    Microsoft NLP and AWS cloud APIs, as otherwise, they probably wouldn’t link them
    in their documentation.
  prefs: []
  type: TYPE_NORMAL
- en: '**Identify dead links**'
  prefs: []
  type: TYPE_NORMAL
- en: We will proceed by identifying dead or broken links. A broken link is a link
    that points to a non-existing web page. Like most websites, Neo4j documentation
    has a designated 404 web page. The web scraper assigns a “404” text value to any
    URL that responds with a 404 page.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: There are 241 broken links in the dataset. The broken link number sounds small,
    given that there is a total of 62 thousand links in the database. However, if
    you performed this analysis on your website, you could always forward the results
    to the appropriate teams to get the links fixed.
  prefs: []
  type: TYPE_NORMAL
- en: '**Finding shortest paths**'
  prefs: []
  type: TYPE_NORMAL
- en: Most websites are designed to gently push the users along the journey to the
    end goal. For example, if you are running an e-commerce website, the goal is probably
    a purchase event. With Neo4j, you can analyze all the paths a user might follow
    to reach the desired destination. Since we are dealing with a documentation website,
    we can’t explore how a user might end up completing a purchase on the website.
    However, we can apply the same techniques and evaluate the shortest paths between
    various parts of the website.
  prefs: []
  type: TYPE_NORMAL
- en: The following code finds all the shortest paths between the two given web pages.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The results show that a user must traverse the following web pages in order
    to reach the Aura console page from the documentation home:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://neo4j.com/docs](https://neo4j.com/docs,)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://neo4j.com/docs/aura/auradb](https://neo4j.com/docs/aura/auradb,)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://neo4j.com/docs/aura/auradb/getting-started/create-database](https://neo4j.com/docs/aura/auradb/getting-started/create-database,)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://console.neo4j.io](https://console.neo4j.io)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Representing your website as a knowledge graph can significantly improve the
    understanding of designed flows throughout your web page, which in turn can help
    you optimize them.
  prefs: []
  type: TYPE_NORMAL
- en: '**Link analysis**'
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will use centrality algorithms to rank the importance of the web pages.
    For example, let’s say we simply define the rank of web pages as the number of
    incoming links. In that case, we can utilize the Degree centrality algorithm to
    rank the importance of the web pages.
  prefs: []
  type: TYPE_NORMAL
- en: To execute any graph algorithms from the [Graph Data Science library](https://neo4j.com/product/graph-data-science/),
    we have first to project an in-memory graph.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: With the Graph Data Science library projection, you have the option to choose
    a specific subgraph of your knowledge graph you want to evaluate with graph algorithms.
    In this example, we selected the **Page** nodes and **LINKS_TO** and **REDIRECTS**
    relationships. For simplicity’s sake, we will treat the links and redirects as
    identical. However, for more in-depth network analysis, we could define some weights
    and perhaps treat redirects as more important than links.
  prefs: []
  type: TYPE_NORMAL
- en: The following code will calculate the incoming degree centrality, which is simply
    the number of incoming links or redirects a web page has.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'One thing to note here is that we are using the Graph Data Science Python Client
    to interact with the database, so the syntax might be slightly different if you
    are used to Cypher procedure calls. The results are the following:'
  prefs: []
  type: TYPE_NORMAL
- en: The developer knowledge base page has 598 incoming links. Many links also point
    to specific tags of the developer blog and graph gists. I would assume that tons
    of documentation sites point to specific examples which can be found in blogs
    of graph gists. If we were to understand the intended flow more, we could drill
    down where the links are coming from and so on.
  prefs: []
  type: TYPE_NORMAL
- en: Sometimes the number of incoming links is not a sufficient ranking metric. The
    founders of Google were aware of this issue as they derived the most famous graph
    algorithm, PageRank, which takes into account the number of incoming links and
    where they are coming from. For example, there is a difference if the web page
    has a direct link from the home page or some periphery documentation page that
    only some people visit.
  prefs: []
  type: TYPE_NORMAL
- en: The following code will calculate the PageRank score and merge it with the degree
    dataframe.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Now we can examine the top five most important web pages judging by the PageRank
    score.
  prefs: []
  type: TYPE_NORMAL
- en: The score column represent the number of incoming links and redirects, while
    the pagerank columns hold the PageRank score.
  prefs: []
  type: TYPE_NORMAL
- en: Interestingly, only the developer knowledge base page retains its position when
    using PageRank instead of Degree centrality. It seems that GraphConnect was vital
    as it is still the second most important web page. As a web UX designer, you might
    take this information and try to change the structure of the website so that perhaps
    the latest GraphConnect would be more important. Remember, we are only scratching
    the surface with this network analysis. However, you could find interesting patterns
    and then drill down to understand the web page flow and optimize it.
  prefs: []
  type: TYPE_NORMAL
- en: '**Keyword analysis and co-occurrence topic clustering**'
  prefs: []
  type: TYPE_NORMAL
- en: In the last part of this analysis, we will take a look at the keyword analysis.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d05668d1485566abd2d6977f4cc1a65d.png)'
  prefs: []
  type: TYPE_IMG
- en: Graph representation of web pages and their keywords. Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: Having the right keywords on the right web pages is one of the critical aspects
    of search engine optimization. We can get a high-level overview of the page by
    examining its most frequent keywords.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '*Results*'
  prefs: []
  type: TYPE_NORMAL
- en: The results look exactly what we might expect. The web page talks about nodes,
    neo4j, graphs, and Java. Not sure why the clipboard is there. Perhaps there are
    a lot of “Copy to clipboard” sections throughout the documentation.
  prefs: []
  type: TYPE_NORMAL
- en: We can drill down a bit and look at the most frequent keywords for web pages
    where the “graph-data-science” is present in the URL address. This way, we filter
    primarily for Neo4j Graph Data Science library documentation.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '*Results*'
  prefs: []
  type: TYPE_NORMAL
- en: It looks very similar to the overall keyword presence, except the keyword “algorithm”
    shows up more frequently here. Now, we could go ahead and drill down keywords
    by other sections or by individual pages. A knowledge graph is a fantastic tool
    for either drill-down analysis or to analyze the distribution of keywords and
    content through designated user flows. Additionally, if you used an NLP model
    that is able to detect both short- and long-tail keywords, it would greatly help
    with any SEO analysis and optimization.
  prefs: []
  type: TYPE_NORMAL
- en: Lastly, we can also perform a keyword co-occurrence clustering with only a couple
    of lines of code. A keyword co-occurrence clustering can be understood as a task
    of identifying topics, where the topics consist of multiple keywords that frequently
    co-occur in the text corpus.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d729bfd6b956be3df688b531b7b0cae0.png)'
  prefs: []
  type: TYPE_IMG
- en: Diagram of the co-occurrence topic clustering output. Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: 'The workflow of the keyword co-occurrence clustering or topic clustering in
    Neo4j is the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Project an in-memory graph with relevant information.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a CO_OCCUR relationship between keyword that commonly appear together
    in text.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Run a community detection algorithm like the Louvain method to identify communities
    or clusters of keywords.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We will begin by projecting an in-memory graph with all the relevant information.
    We need to project both the Page and the Keyword nodes along with the connecting
    HAS_KEYWORD relationship. We need to reverse the relationship orientation in the
    graph projection as we want to examine clusters of co-occurring keywords and not
    groups of similar web pages.
  prefs: []
  type: TYPE_NORMAL
- en: '*P.s. If you leave the natural orientation and follow the examples you will
    identify clusters of similar web pages based on the keywords they mention*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Next, we need to create a **CO_OCCUR** relationship between keywords frequently
    appearing together on web pages. To solve this task, we will use the Node Similarity
    algorithm. The Node Similarity uses the [Jaccard similarity coefficient](https://en.wikipedia.org/wiki/Jaccard_index)
    by default to calculate the similarity between two nodes.
  prefs: []
  type: TYPE_NORMAL
- en: In this example, each keyword has a set of web pages it is mentioned in. If
    the Jaccard coefficient between a pair of keywords based on the web pages they
    appear in is greater than 0.40, then a new **CO_OCCUR** relationship is created
    between them. We use the mutate mode to store the algorithm’s results back to
    the in-memory projected graph.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Finally, we will use the Louvain method algorithm, a community detection algorithm,
    to identify clusters of keywords. The algorithm outputs each node and its community
    id. Therefore, we need to group the results by their community id to create a
    list of keywords that form a topic or a cluster.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '*Results*'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f7fcca075382867620c5b7a9c8eb250a.png)'
  prefs: []
  type: TYPE_IMG
- en: Since the topic clustering workflow we followed is an unsupervised technique,
    we need to manually assign overall topic names. For example, we can observe that
    the first and largest topic contains keywords like chewbacca, jedi, christmas
    day, independence day, and so on. It is a an interesting mix of holidays and Star
    Wars. We could explore why both holidays and Star Wars are mixed together. Additionally,
    the second largest topic seems to talk about various panama and paradise papers
    along with the companies and people involved.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In my opinion, knowledge graphs and natural language processing techniques are
    a match made in heaven. As mentioned, I have seen similar approaches to analyzing
    [medical documents](/construct-a-biomedical-knowledge-graph-with-nlp-1f25eddc54a0),
    [news](https://medium.com/neo4j/making-sense-of-news-the-knowledge-graph-way-d33810ce5005),
    or even [crypto reports](https://medium.com/neo4j/monitoring-the-cryptocurrency-space-with-nlp-and-knowledge-graphs-92a1cfaebd1a).
    The idea is to use NLP and other tools to extract valuable information from unstructured
    data, which is then used to construct a knowledge graph. A knowledge graph offers
    a friendly and flexible structure of the extracted information that can be used
    to support various analytical workflows.
  prefs: []
  type: TYPE_NORMAL
- en: All the code of this blog post is [available on GitHub](https://github.com/tomasonjo/blogs/tree/master/neo4jdocs).
  prefs: []
  type: TYPE_NORMAL
