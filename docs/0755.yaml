- en: Data Preprocessing for Machine Translation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/data-preprocessing-for-machine-translation-fcbedef0e26a?source=collection_archive---------5-----------------------#2023-02-25](https://towardsdatascience.com/data-preprocessing-for-machine-translation-fcbedef0e26a?source=collection_archive---------5-----------------------#2023-02-25)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Clean, normalize, and tokenize
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@bnjmn_marie?source=post_page-----fcbedef0e26a--------------------------------)[![Benjamin
    Marie](../Images/3ea1ad230cb1e67610418a8e36a5e5dd.png)](https://medium.com/@bnjmn_marie?source=post_page-----fcbedef0e26a--------------------------------)[](https://towardsdatascience.com/?source=post_page-----fcbedef0e26a--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----fcbedef0e26a--------------------------------)
    [Benjamin Marie](https://medium.com/@bnjmn_marie?source=post_page-----fcbedef0e26a--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fad2a414578b3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdata-preprocessing-for-machine-translation-fcbedef0e26a&user=Benjamin+Marie&userId=ad2a414578b3&source=post_page-ad2a414578b3----fcbedef0e26a---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----fcbedef0e26a--------------------------------)
    ·14 min read·Feb 25, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Ffcbedef0e26a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdata-preprocessing-for-machine-translation-fcbedef0e26a&user=Benjamin+Marie&userId=ad2a414578b3&source=-----fcbedef0e26a---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ffcbedef0e26a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdata-preprocessing-for-machine-translation-fcbedef0e26a&source=-----fcbedef0e26a---------------------bookmark_footer-----------)![](../Images/c636f6f625036e3463e36d30b90eb824.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Image from [Pixabay](https://pixabay.com/photos/coffee-pot-cup-of-coffee-filter-2139481/).
  prefs: []
  type: TYPE_NORMAL
- en: Data preprocessing is a critical step for any machine learning tasks. The data
    must be correct, clean, and in the expected format.
  prefs: []
  type: TYPE_NORMAL
- en: In this blog article, I explain all the steps that are required to preprocess
    the data used to train, validate, and evaluate machine translation systems.
  prefs: []
  type: TYPE_NORMAL
- en: I explain each preprocessing step with examples and code snippets to reproduce
    them by yourself.
  prefs: []
  type: TYPE_NORMAL
- en: 'For the preprocessing examples in this article, I use the first 100,000 segments
    from the Spanish-English (Es→En) [ParaCrawl v9](https://paracrawl.eu/) corpus
    (CC0). I directly provide [this dataset here](https://benjaminmarie.com/data/paracrawl100k.en-es.zip)
    (size: 9Mb).'
  prefs: []
  type: TYPE_NORMAL
- en: 'If you want to make this corpus by yourself, follow these steps (be patient,
    the original dataset is zipped but still weights 24Gb):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'In my previous article, I presented all the main characteristics of the machine
    translation datasets used for training, validation, and evaluation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[](/datasets-to-train-validate-and-evaluate-machine-translation-d61905d126aa?source=post_page-----fcbedef0e26a--------------------------------)
    [## Datasets to Train, Validate, and Evaluate Machine Translation'
  prefs: []
  type: TYPE_NORMAL
- en: Select, check, and split
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/datasets-to-train-validate-and-evaluate-machine-translation-d61905d126aa?source=post_page-----fcbedef0e26a--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: 'Data formats: TXT, TSV, and TMX'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When looking for datasets for machine translation, you will often find them
    in different formats that try to best deal with their multilinguality nature.
  prefs: []
  type: TYPE_NORMAL
- en: No matter what is the original format, most frameworks for training machine
    translation systems only take as input data in a raw text format.
  prefs: []
  type: TYPE_NORMAL
- en: So you may have to convert the datasets that you got if they are not already
    text files.
  prefs: []
  type: TYPE_NORMAL
- en: 'The most common formats that you may find are:'
  prefs: []
  type: TYPE_NORMAL
- en: 'parallel text (.txt): This is ideal. We don’t have to do any conversion. The
    source segments are in one text file and the target segments in another text file.
    Most of the following preprocessing steps will be applied to these two files in
    parallel. In the introduction, we downloaded ParaCrawl in this format.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'tab-separated values (.tsv): This is a single file with each pair of source
    and target segments on the same line separated by a tabulation. Converting it
    into text files is straightforward with the command “cut”:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Translation Memory eXchange (.tmx): This is an XML format often used by professional
    translators. It is a very verbose format. That’s why it is rarely used for large
    corpus. Dealing with TMX is slightly more difficult. We can start by stripping
    the XML tags. To do this, I use [a script](https://github.com/moses-smt/mosesdecoder/blob/master/scripts/generic/strip-xml.perl)
    from the Moses project (LGPL license):'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Don’t modify the target side of evaluation datasets
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Before going further, there is a very important rule to follow when preprocessing
    datasets for machine translation:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Never preprocessed the target side of the evaluation data!**'
  prefs: []
  type: TYPE_NORMAL
- en: These are so-called “reference translations.” Since they are “references,” we
    shouldn’t touch them.
  prefs: []
  type: TYPE_NORMAL
- en: There are several reasons for that. The main one is that **the target side of
    the evaluation data should look like the data you want your system to generate.**
  prefs: []
  type: TYPE_NORMAL
- en: For instance, in some of the preprocessing steps, we will remove empty lines
    and tokenize the segments.
  prefs: []
  type: TYPE_NORMAL
- en: You may want your system to return empty lines, for instance when translating
    empty text, and certainly you don’t want to return tokenized texts as a final
    output of your machine translation systems.
  prefs: []
  type: TYPE_NORMAL
- en: If you remove empty lines from the references, you won’t be able to directly
    evaluate the ability of your system in generating empty lines when needed. While
    if you tokenize the references, you will only know how good your system is at
    generating tokenized text. As we will see, tokenized texts are not what you want
    your system to generate.
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, reference translations are used to compute automatic metric scores
    to evaluate the machine translation quality. If we modify these translations,
    we modify the scores. Then, the scores would not be comparable anymore with the
    other published scores for the same reference translations, since we modified
    these references.
  prefs: []
  type: TYPE_NORMAL
- en: So keeping the original reference translation is critical to enable the **reproducibility
    and comparability** of an evaluation.
  prefs: []
  type: TYPE_NORMAL
- en: If at some point during the preprocessing the target side of the evaluation
    data is not the same as the original one, it means that something went wrong.
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 1: Cleaning and Filtering'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Step to be applied to:** source and target sides of the training and validation
    data.'
  prefs: []
  type: TYPE_NORMAL
- en: For various reasons, publicly available parallel data may require some cleaning.
  prefs: []
  type: TYPE_NORMAL
- en: This is especially true if the data has been automatically created from text
    crawled on the Web.
  prefs: []
  type: TYPE_NORMAL
- en: 'Cleaning often implies removing the following segments (or sentences) from
    the parallel data:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Empty** or mostly containing **non-printable characters**.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With **invalid UTF8**, i.e., not properly encoded.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Containing **extremely long tokens** (or “words”) since they are often non-translatable,
    for instance, DNA sequences, digit sequences, nonsense, etc.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: And, sometimes, **duplicates**, i.e., if a segment, or a pair of segments, appear
    more than once in the parallel data, we keep only one instance of it.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'It is not necessary, but I usually remove duplicates of segment pairs in the
    training parallel data, for several reasons:'
  prefs: []
  type: TYPE_NORMAL
- en: They are rarely useful for training.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: They give more weight inside the training data to a particular translation without
    good reasons.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: They are often the unwanted product of a defective process used to acquire the
    data (e.g., crawling), in other words, these duplicates should never have been
    there in the first place.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All these filtering rules are applied to keep only what is useful to train a
    neural model. It also removes segments that may trigger some errors in the following
    steps of the preprocessing.
  prefs: []
  type: TYPE_NORMAL
- en: They also slightly reduce the size of the parallel data.
  prefs: []
  type: TYPE_NORMAL
- en: Keep in mind that we are cleaning/filtering **parallel data**. Each filtering
    rule should be applied to both sides of the data, simultaneously. For instance,
    if a source segment is empty and should be removed, the target segment should
    also be removed to preserve the parallelism of the data.
  prefs: []
  type: TYPE_NORMAL
- en: 'In addition to the rules I mentioned above, we should filter out the segment
    pairs with:'
  prefs: []
  type: TYPE_NORMAL
- en: A **very long** segment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A **very short** segment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A **high fertility**, i.e., when a segment appear disproportionately longer,
    or shorter, than its counterpart
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These rules are inherited from the statistical machine translation era during
    which these segments were dramatically increasing the computational cost while
    not being useful to train a translation model.
  prefs: []
  type: TYPE_NORMAL
- en: With the neural algorithms used for training today, these rules are not necessary
    anymore. Nonetheless, these segments are still mostly useless to train a translation
    model and can be safely removed from the training data to further reduce its size.
  prefs: []
  type: TYPE_NORMAL
- en: There are many tools to perform this cleaning that are publicly available.
  prefs: []
  type: TYPE_NORMAL
- en: '[*preprocess*](https://github.com/kpu/preprocess) ([LGPL](https://github.com/kpu/preprocess/blob/master/LICENSE)
    license) is an efficient framework that can do many filtering operations. It is
    used by the [workshop on machine translation](http://www2.statmt.org/wmt23/) to
    prepare the data for the main international machine translation competitions.'
  prefs: []
  type: TYPE_NORMAL
- en: I usually complement it with homemade scripts and additional frameworks such
    as [Moses scripts](https://github.com/moses-smt/mosesdecoder/tree/master/scripts)
    ([LGPL](https://github.com/moses-smt/mosesdecoder/blob/master/COPYING) license).
  prefs: []
  type: TYPE_NORMAL
- en: In the following paragraphs, I describe step by step the entire cleaning and
    filtering process that I usually apply to raw parallel data.
  prefs: []
  type: TYPE_NORMAL
- en: Practice
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We want to clean our Paracrawl Spanish-English parallel data (provided in the
    introduction of this article).
  prefs: []
  type: TYPE_NORMAL
- en: One of the most costly steps, in terms of memory, is the removal of duplicates
    (so-called “deduplication”). Before deduplication, we should remove as many segment
    pairs as possible.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can start by applying the [clean-n-corpus.perl](https://github.com/moses-smt/mosesdecoder/blob/master/scripts/training/clean-corpus-n.perl)
    (this script doesn’t require installing Moses), as follows: *Note: This step assumes
    the existence of spaces in both the source and target languages. If one of the
    languages (mostly) doesn’t use spaces, such as Japanese or Chinese, you must first
    tokenize the source and target text files. If it applies to your use case, go
    directly to “Step 2 and 3” and then come back here once it is done.*'
  prefs: []
  type: TYPE_NORMAL
- en: 'To know how to use clean-n-corpus.perl, call the script without any arguments.
    It should return:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The arguments are:'
  prefs: []
  type: TYPE_NORMAL
- en: '*ratio*: This is the fertility. By default, it is set to 9\. We usually don’t
    need to modify it, and thus don’t use this argument.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*corpus*: This one is the path to the dataset to clean, without the extension.
    The script assumes that you named both source and target file the same, using
    the language ISO codes as extensions, for instance, train.es and train.en in our
    case. If you adopt the same file name convention I used for ParaCrawl, you simply
    have to put there: “train” (assuming that you are in the directory containing
    your data).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*l1*: the extension of one of the parallel files, e.g., “es”.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*l2*: the extension of the other parallel file, e.g., “en”.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*clean-corpus*: The name of the files after cleaning. For instance, if you
    enter “train.clean”, the script will save the filtered parallel data into “train.clean.es”
    and “train.clean.en”'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*min*: The minimum number of tokens under which a segment should be discarded.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*max*: The maximum number of tokens above which a segment should be discarded.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*max-word-length* (not shown here): The maximum number of characters in one
    token. If a segment pair contains a token longer than max-word-length, it is removed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To clean our ParaCrawl corpus, the full command to run is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'This command removes from train.es and train.en segment pairs with:'
  prefs: []
  type: TYPE_NORMAL
- en: An empty segment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A segment longer than 150 words (or tokens)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A high fertility
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Segments with a word (or token) containing more than 50 characters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: And save the results into “train.clean.es” and “train.clean.en”.
  prefs: []
  type: TYPE_NORMAL
- en: 'The script displays the number of segments removed. If you did the same as
    I did, it should remains 99976 segments in the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that for each segment removed, its parallel segment is also removed. train.clean.es
    and train.clean.en should have the same number of lines. You can check it with:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Next, we remove segments with *preprocess*.
  prefs: []
  type: TYPE_NORMAL
- en: 'We need to compile it first (cmake is required):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we can use *preprocess* to remove lines with:'
  prefs: []
  type: TYPE_NORMAL
- en: Invalid UTF-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Control characters (except tab and newline)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Too many Common and Inherited Unicode script characters (like numbers)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Too much or too little punctuation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Too little in the expected script (to remove for instance Chinese sentences
    in English data)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We use the “simple_cleaning” binary for this. It handles parallel data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: The filtered data are saved in two new files that I named “train.clean.pp.es”
    and “train.clean.pp.en”.
  prefs: []
  type: TYPE_NORMAL
- en: 'And it should print:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'And finally we can remove duplicates with “dedupe”:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'And it should print:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: We finished cleaning the data.
  prefs: []
  type: TYPE_NORMAL
- en: We nearly removed 15% of the segments. It means that eachtraining epoch of neural
    machine translation will be 15% (approximately) faster.
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 2: Normalization'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Step to be applied to**: the source sides of all the datasets, and potentially
    to the target side of the training and validation datasets.'
  prefs: []
  type: TYPE_NORMAL
- en: The objective of the normalization is to make sure that the same symbols, such
    as punctuation marks, numbers, and spaces, with the same UTF8 codes, are used
    in all the datasets.
  prefs: []
  type: TYPE_NORMAL
- en: In practice, this step can also **reduce the size of the vocabulary** (the number
    of different token types) by mapping symbols with a similar role or meaning to
    the same symbol.
  prefs: []
  type: TYPE_NORMAL
- en: 'For instance, this step can normalize these different quote marks to the same
    one, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: ‘ **→** “
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**« → “**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 《 **→ “**
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This step can also make sure that your system won’t generate translations with
    different styles of punctuation marks, if you apply it to the target sides of
    your training data.
  prefs: []
  type: TYPE_NORMAL
- en: Of course, if we also normalize the target side of the training data, we have
    to make sure that we map to the desired characters.
  prefs: []
  type: TYPE_NORMAL
- en: 'For instance, if you prefer “《” because your target language uses this type
    of quote mark, then you should do a different mapping, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: ‘ **→**《
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**« →**《'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**“ →**《'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Since this step is commonly performed when preparing data for machine translation,
    there are various tools to do it.
  prefs: []
  type: TYPE_NORMAL
- en: I use [sacremoses](https://github.com/alvations/sacremoses) (MIT license). It
    is an implementation, in Python, of the [normalize-punctuation.perl](https://github.com/moses-smt/mosesdecoder/blob/master/scripts/tokenizer/normalize-punctuation.perl)
    from the Moses project.
  prefs: []
  type: TYPE_NORMAL
- en: Practice
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: sacremoses [normalizer](https://github.com/alvations/sacremoses/blob/master/sacremoses/normalize.py)
    maps dozens of symbols from many languages. The rules can easily be edited to
    better match your expectations.
  prefs: []
  type: TYPE_NORMAL
- en: 'sacremoses can be installed with pip (Python 3 is required):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Then you can normalize your data with the CLI:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Examples of differences (obtained with the command “diff”): *Note: I took a
    screenshot instead of copy/paste the sentences in this article because the blog
    editor automatically applies its own normalization rules.*'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d04f41c9cb7179b7b4f21c8ec3c34e32.png)'
  prefs: []
  type: TYPE_IMG
- en: Sentences from ParaCrawl V9 (CC0) before and after normalization. Screenshot
    by the author.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can pass to this command several options, for instance, if you want to
    normalize numbers, add the option “-n”. To see all the options run:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Step 3: Tokenization'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Step to be applied to**: the source sides of all the datasets, and to the
    target side of the training and validation datasets.'
  prefs: []
  type: TYPE_NORMAL
- en: Traditionally, datasets for machine translation were tokenized with rule-based
    tokenizers. They often simply use spaces to delimitate tokens with the addition
    of rules to handle special cases.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s take an example with the following English sentences:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'After tokenization with sacremoses tokenizer, we obtain:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: The differences before and after tokenization are difficult to spot. If you
    don’t see them, watch for the spaces near punctuation marks.
  prefs: []
  type: TYPE_NORMAL
- en: For several reasons, these **rule-based tokenizers are not practical for neural
    machine translation**. For instance, they generate far **too many rare tokens**
    that cannot be modeled properly by the neural model.
  prefs: []
  type: TYPE_NORMAL
- en: The data must be “sub-tokenized”. For instance, tokens generated by a traditional
    tokenizer are split into smaller tokens. This is what the [byte-pair encoding
    approach (BPE)](https://aclanthology.org/P16-1162.pdf) does.
  prefs: []
  type: TYPE_NORMAL
- en: Even more simple, the [SentencePiece approach](https://aclanthology.org/D18-2012.pdf)
    doesn’t even require a traditional tokenization. Consequently, we have one less
    tool (the traditional tokenizer) to apply and thus one less source of potential
    errors/mistakes in our preprocessing.
  prefs: []
  type: TYPE_NORMAL
- en: SentencePiece can be applied directly to **any sequences of characters**. This
    is especially practical for languages for which spaces are rare such as Japanese,
    Chinese, and Thai.
  prefs: []
  type: TYPE_NORMAL
- en: Actually, SentencePiece is currently one of the most used tokenization algorithms
    for large language models such as the ones from the [T5](https://arxiv.org/abs/1910.10683)
    or [FLAN](https://arxiv.org/pdf/2109.01652.pdf) families.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s see how it works by taking the same English sentences. We will obtain:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: The sentences are much more difficult to read by humans and the tokenization
    isn’t intuitive. Yet, with neural models it works very well.
  prefs: []
  type: TYPE_NORMAL
- en: To obtain this result, you first need to train a SentencePiece model. The model
    is then used to tokenize the data, as well as all new inputs that will be sent
    to our machine translation system.
  prefs: []
  type: TYPE_NORMAL
- en: Practice
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To train this model, we first need to install [SentencePiece](https://github.com/google/sentencepiece)
    with:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Then, concatenate the source and target sides of your parallel data in one single
    text file. This will allow us to train **a bilingual tokenization model**, rather
    than training different models for the source and target languages.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Before moving on to the training, we must decide on a vocabulary size.
  prefs: []
  type: TYPE_NORMAL
- en: 'This choice is a difficult but important decision. We usually use a rule of
    thumb: **A value between 8,000 and 16,000 works well for most use cases**. You
    may choose a higher value if you have a very large parallel data, or a lower value
    if you have much smaller training data, for instance less than 100,000 segment
    pairs.'
  prefs: []
  type: TYPE_NORMAL
- en: The rationale is that if you set the vocabulary size too high, your vocabulary
    will contain rarer tokens. If your training parallel data doesn’t contain enough
    instances of these tokens, their embeddings would be poorly estimated.
  prefs: []
  type: TYPE_NORMAL
- en: In contrast, if you set the value too low, the neural model will have to deal
    with smaller tokens which would have to carry more information in their embeddings.
    The model may struggle to compose good translations in this situation.
  prefs: []
  type: TYPE_NORMAL
- en: Since our parallel data is quite small, I arbitrarily chose 8,000 for the vocabulary
    size.
  prefs: []
  type: TYPE_NORMAL
- en: 'To start the training:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: This should be fast (less than 2 minutes).
  prefs: []
  type: TYPE_NORMAL
- en: 'The arguments are the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '*input*: The data used to train the SentencePiece model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*model_prefix*: The name of the SentencePiece model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*vocab_size*: The vocabulary size.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Then you have to apply the model to **all the data, except the target side
    of the test set** (remember: We never touch this one).'
  prefs: []
  type: TYPE_NORMAL
- en: 'For our ParaCrawl corpus, we do:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: And that’s it! Our datasets are all preprocessed. We can now start to train
    a machine translation system.
  prefs: []
  type: TYPE_NORMAL
- en: '**Optional steps: truecasing and shuffling**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There are two more steps that you may find in some preprocessing pipeline for
    machine translation: truecasing and shuffling.'
  prefs: []
  type: TYPE_NORMAL
- en: Truecasing is getting deprecated but may yield slightly better translation quality.
    This preprocessing step lowercases characters that are uppercased only due to
    their position in a sentence.
  prefs: []
  type: TYPE_NORMAL
- en: 'For instance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'is truecased as:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: The “h” in “He” is lowercased since it was uppercased only due to its position
    in the sentence. This is the only difference.
  prefs: []
  type: TYPE_NORMAL
- en: This step slightly reduces the vocabulary size.
  prefs: []
  type: TYPE_NORMAL
- en: sacremoses implements truecasing.
  prefs: []
  type: TYPE_NORMAL
- en: As for shuffling segment pairs, it is probably already integrated in the framework
    you’ll use to train your machine translation system.
  prefs: []
  type: TYPE_NORMAL
- en: The training data is often automatically reshuffled for each training epoch.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Filtering and normalization are steps that can significantly decrease the computational
    cost of training neural machine translation.
  prefs: []
  type: TYPE_NORMAL
- en: These steps may also improve the translation quality, especially if the training
    data was very noisy.
  prefs: []
  type: TYPE_NORMAL
- en: The rules I suggest in this article for filtering and normalization are not
    suitable for all use cases. They will work well for most language pairs, but you
    may have to adapt them depending on your languages, e.g., when dealing with Asian
    languages such as Japanese you may want to change most of the rules for normalization
    to avoid generating English punctuation marks inside Japanese texts.
  prefs: []
  type: TYPE_NORMAL
- en: Tokenization is even more critical. Fortunately, this is also the step that
    is most straightforward. Most preprocessing pipelines for machine translation
    do the same for tokenization, albeit with different hyperparameters.
  prefs: []
  type: TYPE_NORMAL
- en: 'In a next article, I’ll explain all you need to know to train a machine translation
    system using the data you just preprocessed: frameworks, neural architectures,
    and hyperparameters.'
  prefs: []
  type: TYPE_NORMAL
- en: All my articles are published in The Kaitchup, my newsletter. Subscribe to receive
    weekly news, tips, and tutorials to run large language models and machine translation
    systems on your computer.
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://kaitchup.substack.com/?source=post_page-----fcbedef0e26a--------------------------------)
    [## The Kaitchup - AI on a Budget | Benjamin Marie, PhD | Substack'
  prefs: []
  type: TYPE_NORMAL
- en: Subscribe for weekly AI news, tips, and tutorials on fine-tuning, running, and
    serving large language models on your…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: kaitchup.substack.com](https://kaitchup.substack.com/?source=post_page-----fcbedef0e26a--------------------------------)
  prefs: []
  type: TYPE_NORMAL
