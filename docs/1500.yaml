- en: 'TranSPormer: A Transformer Network for the Travelling Salesman Problem'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/transpormer-a-transformer-network-for-the-travelling-salesman-problem-154bd33c37b0?source=collection_archive---------8-----------------------#2023-05-02](https://towardsdatascience.com/transpormer-a-transformer-network-for-the-travelling-salesman-problem-154bd33c37b0?source=collection_archive---------8-----------------------#2023-05-02)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: An original approach to tackle TSP leveraging a Transformer neural network
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@davidecaffagni98?source=post_page-----154bd33c37b0--------------------------------)[![Davide
    Caffagni](../Images/a7ef4b6d21ea0564d43180a50a84280a.png)](https://medium.com/@davidecaffagni98?source=post_page-----154bd33c37b0--------------------------------)[](https://towardsdatascience.com/?source=post_page-----154bd33c37b0--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----154bd33c37b0--------------------------------)
    [Davide Caffagni](https://medium.com/@davidecaffagni98?source=post_page-----154bd33c37b0--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fb628be49d740&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftranspormer-a-transformer-network-for-the-travelling-salesman-problem-154bd33c37b0&user=Davide+Caffagni&userId=b628be49d740&source=post_page-b628be49d740----154bd33c37b0---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----154bd33c37b0--------------------------------)
    ·11 min read·May 2, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F154bd33c37b0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftranspormer-a-transformer-network-for-the-travelling-salesman-problem-154bd33c37b0&user=Davide+Caffagni&userId=b628be49d740&source=-----154bd33c37b0---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F154bd33c37b0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftranspormer-a-transformer-network-for-the-travelling-salesman-problem-154bd33c37b0&source=-----154bd33c37b0---------------------bookmark_footer-----------)![](../Images/19ab14273299f2d8a77fd057abc70be5.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Photo by [Alina Grubnyak](https://unsplash.com/@alinnnaaaa?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: The Travelling Salesman Problem (TSP) is one of the classic challenges of combinatorial
    optimization. Although it’s been studied for a long, currently no exact method
    is known to guarantee the optimal solution. With the advancements in the field
    of artificial intelligence, new proposals to tackle the TSP have been born. In
    this article, we leverage a transformer neural network to figure out good solutions
    for this problem.
  prefs: []
  type: TYPE_NORMAL
- en: 'Code repository: [GitHub](https://github.com/dcaffo98/transpormer)'
  prefs: []
  type: TYPE_NORMAL
- en: The aforementioned codebase is a refactoring of a [previous project](https://github.com/bizza251/adm-project)
    I accomplished to pass the Automated Decision Making course exam during my MCS.
    I want to thank my friend and colleague Alessandro, who collaborated on that.
  prefs: []
  type: TYPE_NORMAL
- en: Background — TSP
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Given a list of cities and the distances between each pair of cities, what
    is the shortest possible route that visits each city exactly once and returns
    to the origin city? (source: [Wikipedia](https://en.wikipedia.org/wiki/Travelling_salesman_problem))'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The above question defines the Travelling Salesman Problem (TSP), one of the
    *NP-hard* dilemmas, meaning that no currently known method leads to the optimal
    solution in a polynomial time. A little more formally, given a graph ***G****(****V****,*
    ***E****)*, where ***V*** is the set of nodes (the cities) and ***E*** is the
    set of edges connecting any pair of nodes in ***V***, we are asked to find the
    shortest possible Hamiltonian tour (i.e. a *“route that visits each city exactly
    once and returns to the origin city”*) in ***G***.
  prefs: []
  type: TYPE_NORMAL
- en: In this project, we are going to consider a node as a simple pair of features,
    namely the x and y positions in a 2d vector space. Consequently, there exists
    an edge between any pair of nodes *(u, v)*, whose weight is given by the Euclidean
    distance between *u* and *v*.
  prefs: []
  type: TYPE_NORMAL
- en: Background — Transformer for TSP
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We take inspiration from the work of Bresson *et al.* [1], who leverage a transformer
    neural network to address the TSP, obtaining very promising results. Briefly,
    their model is split into two components. First, the transformer **encoder** takes
    as input a set of *tokens*, i.e. vectors representing the nodes in an input graph
    ***G***, and transforms them into a new latent space thanks to the self-attention
    layers. We’ll refer to the output of the encoder as **graph embeddings**. Subsequently,
    the transformer **decoder** is fed with a fictitious token ***z*,** which signals
    the model to start building the tour, plus the graph embeddings from the encoder,
    and generates a probability distribution modeling the likelihood for each node
    in the graph to be selected as the next candidate in the tour. We draw the index
    *t* corresponding to the chosen node from that distribution. Then, we concatenate
    ***z*** with the *t-th* graph embedding and we again feed the decoder with the
    resulting sequence, that represents the partial tour. This process goes on until
    all the available nodes have been selected. Once finished, we remove ***z*** from
    the obtained sequence and append to it the first selected node, thus closing the
    tour. Such a decoder is called **auto-regressive**, since at each selection step
    it works on its own output from the previous step. The model is trained with the
    REINFORCE [2] algorithm to minimize the average length of the tours generated
    from random graphs with 50 nodes each.
  prefs: []
  type: TYPE_NORMAL
- en: Proposed method
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/998284862fd6de205dcd2100cc1e346f.png)'
  prefs: []
  type: TYPE_IMG
- en: Overview of the architecture proposed by [Bresson et al.](https://doi.org/10.48550/arXiv.2103.03012)
    (right) and by us (left). Note that our network has no auto-regressive components,
    i.e. there’s no backward link between the output and the input. Photo by the author
  prefs: []
  type: TYPE_NORMAL
- en: In this project, we too employ a transformer architecture, since we aim to exploit
    attention to learn valuable global dependencies among the nodes in a graph. At
    the same time, we want to design a model **avoiding any auto-regressive component**,
    i.e. a neural network that directly outputs a feasible TSP solution, without the
    need for a single forward pass for each node.
  prefs: []
  type: TYPE_NORMAL
- en: We start by considering that the TSP can be framed as a set-to-sequence problem.
    We have a bunch of nodes for which an **order** is not defined, and we want to
    figure out how to sort them so that the resulting tour is the shortest possible.
    Representing the concept of order in a neural network is usually done with some
    sort of **positional encoding** [3]. That is, we sum each token in an input set
    with a particular vector, whose components are unique for a specific position.
    So, if we change the order of tokens, the positional vector will be different,
    resulting in a diverse token as well. Naturally, there are tons of possible implementations
    for positional encoding, but to keep things simple, we stick with the original
    proposal.
  prefs: []
  type: TYPE_NORMAL
- en: The core of our method is the attention operator, specifically the cross-attention,
    so here we provide a brief recap about it.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d6dcec872a4d403138a7d2d3602c4d37.png)'
  prefs: []
  type: TYPE_IMG
- en: 3-step explanation of the attention. Photo by the author
  prefs: []
  type: TYPE_NORMAL
- en: The critical step is the second. ***A*** is a *[n,n]* matrix, whose *(i,j)*
    item is a real number proportional to the **cosine similarity** of the *i-th*
    token w.r.t. *j-th* token. To understand why, recall from step-2 that *(i,j)*
    is the outcome of the dot-product between query *i* and key *j*, both being two
    vectors of shape *(d_k).* But, the dot-product is nothing more than the cosine
    of the angle amid two vectors, apart from a scaling factor represented by the
    product of the norm of the very same vectors.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/93edee1b52df6300a62f77f27deb81f0.png)'
  prefs: []
  type: TYPE_IMG
- en: The formula of the cosine between two vectors. Photo by the author
  prefs: []
  type: TYPE_NORMAL
- en: After applying the softmax (step-3) to ***A*** (actually, to a scaled version
    of ***A***, but that’s not the important stuff), we obtain a matrix whose *i-th*
    row is a probability distribution, modeling the likelihood of the query *i* to
    be similar to the key *j*=1,…,*n*. Finally, ***A*** applies a linear transformation
    to ***V***, weighting its *d=1,…k* features by the similarity among queries and
    keys. For instance, let’s say that ***x_0***, ***x_1***, and ***x_2*** are tokens
    that stand for the cities of New York (NY), Washington DC (WDC), and Los Angeles
    (LA) respectively. Their features are related to the geographical location of
    the 3 metropolises. We extract the 3 matrices of ***Q***, ***K,*** and ***V***
    from them and we compute the attention matrix ***A***. Since the distance NY-WDC
    is way shorter than NY-LA, the first row of ***A*** will have a very high value
    in its first column (because the distance NY-NY is naturally the shortest), a
    mid-high value in the second column, and a low value in the third one. NY is also
    represented by the first row in the matrix ***V***, i.e. *v_0*. Well, after the
    ***AV*** matrix multiplication, *v_0* will have its features weighted by the first
    row (NY) of A mostly, by the second row (WDC) reasonably, and by the third row
    (LA) poorly. The same process holds for the other two cities. Our original set
    of tokens has been transformed into a new set of vectors, where their features
    are weighted by (spatial, in this case) similarity.
  prefs: []
  type: TYPE_NORMAL
- en: This type of attention is called **self-attention** since ***Q***, ***K***,
    and ***V*** all come from the same input. On the other hand, in a transformer
    decoder, there is also a second type of attention, **cross-attention**. This time,
    the query ***Q*** comes from the decoder itself. For instance, in the model of
    Bresson *et al.*, ***Q*** represents the partial tour. ***K*** and ***V*** are
    conversely linear projections of the output from the encoder.
  prefs: []
  type: TYPE_NORMAL
- en: Well, in this work, we proposed to leverage cross-attention differently. Indeed,
    we use the positional encoding as a query, while keys and values are extracted
    from a previous self-attention layer, which is free to operate over all the nodes
    of ***G***. This way when we compute the attention matrix, we end up with a matrix
    where each row is a probability distribution of the likelihood of a given position
    to be similar to a given node. The following picture may help in understanding
    this claim.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/43d727b6d4f7cce04326875a10b95064.png)'
  prefs: []
  type: TYPE_IMG
- en: Visualizing the cross-attention in our model. Photo by the author
  prefs: []
  type: TYPE_NORMAL
- en: 'As per the above attention matrix, node #1 will likely be the first in the
    proposed tour. #46 is probably the second. Nodes #39 and #36 will be placed in
    the third and fourth positions respectively. Node #23 is a good candidate for
    both the fifth and sixth positions, and so on… In practice, the element *(i,j)*
    of such a matrix tells the likelihood for node *j* to be inserted in position
    *i* in the proposed tour. Our model is essentially a stack of blocks, each one
    consisting of a self-attention layer, the presented cross-attention, and finally
    a feed-forward network, as in any transformer. The attention matrix from the last
    cross-attention layer will be used to generate the tour. A full attention matrix
    produced by our model is something like this one:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/16a0a21694a50f985f103c5a07f07805.png)'
  prefs: []
  type: TYPE_IMG
- en: Attention matrix without training, with weights randomly initialized. Photo
    by the author
  prefs: []
  type: TYPE_NORMAL
- en: Which is apparently quite chaotic. After optimizing the network with the REINFORCE
    algorithm to minimize the average tour length, we end up with matrices like the
    following.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e188dee466ca32bccc81ac0552ee34c3.png)'
  prefs: []
  type: TYPE_IMG
- en: Attention matrix after training. Photo by the author
  prefs: []
  type: TYPE_NORMAL
- en: Starting from the above matrix, we can draw a node for each row, and the resulting
    sequence will be our tour (after appending the first node to *close* it). For
    the sake of simplicity, in all our experiments we go with greedy decoding to select
    nodes, i.e. we always take the node with the highest value for the current row.
    We leave to future developments the exploration of other strategies such as the
    beam search algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: Feasible solutions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You’ve probably already figured out the main problem of our method. To build
    a feasible tour, we need to choose each node **only once**. Conversely, if you
    carefully observe the previous picture, you’ll notice that there are some repetitions.
    Namely, node #47 is the most likely one for two consecutive positions, as well
    as node #32\. As it is, our network is not able to provide feasible TSP solutions.
    What we would like to obtain from the model is a **permutation matrix**. i.e.
    a matrix with a single entry of value 1 for each row and column. The issue is
    that neural networks are not good at predicting such sparse objects.'
  prefs: []
  type: TYPE_NORMAL
- en: To overcome this hitch, we can formulate the TSP as a linear [sum assignment](https://en.wikipedia.org/wiki/Hungarian_algorithm)
    (LSA) problem, whose (inverse) cost matrix is the attention matrix computed by
    our transformer. In this work, we leverage the implementation from [SciPy](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.linear_sum_assignment.html)
    to find the minimum cost matching between the set of nodes (the *workers* in the
    typical LSA formulation) and the set of positions (the *jobs*) in the tour.
  prefs: []
  type: TYPE_NORMAL
- en: Sinkhorn operator
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Naturally, the solution of LSA with costs given by a permutation matrix is
    straightforward: you just have to select the *1s* for each row of the matrix.
    The objective of our training is thus to make the final attention matrix as close
    as possible to a permutation one, such that the LSA will be easy and will lead
    to a matching representing a tour with a short total length.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In our project, we leverage the Sinkhorn operator [4] to turn the dense attention
    matrices of our models into soft-permutation ones: they don’t truly represent
    a permutation, but they get very close to it. In the following, I show the course
    of a typical training setup. The average tour length of the solutions proposed
    by our model is expected to decrease while training. This doesn’t happen if we
    stop applying Sinkhorn before solving the LSA problem.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b1f606e40d074bdc8f835d5356c01123.png)'
  prefs: []
  type: TYPE_IMG
- en: Impact of the Sinkhorn operator in our method. Photo by the author
  prefs: []
  type: TYPE_NORMAL
- en: Results and comparisons
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We compare our proposal against the algorithm of Christofides [5], a popular
    heuristic for the TSP, as well as the auto-regressive transformer from Bresson
    et al., which we take as a reference also concerning the training setup. The evaluation
    is carried out over 10000 graphs with 50 nodes each, whose coordinates are randomly
    generated.
  prefs: []
  type: TYPE_NORMAL
- en: Here we provide a box plot depicting the length of the solutions proposed by
    the three contenders. Undoubtedly, our transformer is the worst of the context.
    Producing good TSP solutions leveraging a neural network without exploiting auto-regression
    is a hard task.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9c110a610494482c04a362ade19e5f33.png)'
  prefs: []
  type: TYPE_IMG
- en: Length of the TSP solutions from the three compared methods. Photo by the author
  prefs: []
  type: TYPE_NORMAL
- en: 'However, solving optimization problems is not just about performance. Computational
    time plays a fundamental role as well. That is the main feature of our proposal:
    since our transformer requires a single forward pass to provide a solution, we
    are faster than the method of Bresson *et al.*, as they have to loop as many times
    as there are nodes in the graph to get a complete tour. The next plot shows the
    computational time, measured with the Python [profiler](https://docs.python.org/3.9/library/profile.html#module-cProfile).
    For Christofides, we use the implementation from [NetworkX](https://networkx.org/documentation/stable/reference/algorithms/generated/networkx.algorithms.approximation.traveling_salesman.traveling_salesman_problem.html).
    Note that, for a fair comparison, the two neural networks run on CPU since Christofides
    is not thought to run on GPU.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7984ba907e8ccdea861eb87bf3e8359b.png)'
  prefs: []
  type: TYPE_IMG
- en: CPU computational time to solve TSP instances. Photo by the author
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we show some qualitative results of the best tours produced by our
    network. In these cases, our model was able to improve over the solutions generated
    by Christofides.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ba5ce202ad4bc5af7fcd41698179e0cb.png)![](../Images/41422af2cb9c27982f181794a11e603a.png)![](../Images/727c7e2d4550634e6579bd64dab419ed.png)![](../Images/ec2279e1a4032b521e0a5f4c8cda5f0e.png)![](../Images/498f36854b3e2da4a3f09e8b5f23818e.png)![](../Images/b5f7ed25eece0a78f185d3f01a4feb0e.png)![](../Images/5f0d9208e09945634eeb58de28efaf6a.png)![](../Images/91ff78fca37d4c28bbc1ca56f86aa914.png)![](../Images/1ec91cbba98f7b137b902880ce13a86f.png)![](../Images/9632311a76e3b586a5b9de04316bbe5a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Qualitative results: TSP solutions from the [NetworkX](https://networkx.org/documentation/stable/reference/algorithms/generated/networkx.algorithms.approximation.traveling_salesman.traveling_salesman_problem.html)
    implementation of Christofides (left) and from our transformer (right). Photo
    by the author'
  prefs: []
  type: TYPE_NORMAL
- en: Conclusions and future work
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this work, we’ve presented an original neural network to tackle the TSP.
    While we manage to design a suitable architecture that avoids auto-regression,
    the quality of the proposed tours is questionable. The main drawback of this approach
    is that it is not an end-to-end neural method, since we need to solve an LSA instance
    to ensure a feasible solution. We tried to force the model to avoid node repetitions,
    by adding some penalty terms to the loss function, but we didn’t get any good
    results. To learn a permutation from a set is a hard task on its own.
  prefs: []
  type: TYPE_NORMAL
- en: Probably, just switching from mere greedy decoding to a beam search strategy
    is sufficient to slightly improve results. Furthermore, there exist more advanced
    reinforcement learning algorithms to explore, such as [PPO](https://arxiv.org/abs/1707.06347).
    We hope that others may be inspired by this project and will continue to explore
    this pathway.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] Bresson, Xavier, and Thomas Laurent. “[The transformer network for the
    traveling salesman problem.](https://doi.org/10.48550/arXiv.2103.03012)” *arXiv
    preprint arXiv:2103.03012* (2021)'
  prefs: []
  type: TYPE_NORMAL
- en: '[2] Williams, Ronald J. “[Simple statistical gradient-following algorithms
    for connectionist reinforcement learning.](https://link.springer.com/content/pdf/10.1023/A:1022672621406.pdf)”
    *Reinforcement learning* (1992): 5–32'
  prefs: []
  type: TYPE_NORMAL
- en: '[3] Vaswani, Ashish, et al. “[Attention is all you need.](https://doi.org/10.48550/arXiv.1706.03762)”
    *Advances in neural information processing systems* 30 (2017)'
  prefs: []
  type: TYPE_NORMAL
- en: '[4] Adams, Ryan Prescott, and Richard S. Zemel. “[Ranking via sinkhorn propagation.](https://doi.org/10.48550/arXiv.1106.1925)”
    *arXiv preprint arXiv:1106.1925* (2011)'
  prefs: []
  type: TYPE_NORMAL
- en: '[5] Christofides, Nicos. [*Worst-case analysis of a new heuristic for the travelling
    salesman problem*.](http://dx.doi.org/10.1007/s43069-021-00101-z) Carnegie-Mellon
    Univ Pittsburgh Pa Management Sciences Research Group, 1976'
  prefs: []
  type: TYPE_NORMAL
