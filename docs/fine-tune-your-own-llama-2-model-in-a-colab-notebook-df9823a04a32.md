# åœ¨ Colab ç¬”è®°æœ¬ä¸­å¾®è°ƒä½ è‡ªå·±çš„ Llama 2 æ¨¡å‹

> åŸæ–‡ï¼š[`towardsdatascience.com/fine-tune-your-own-llama-2-model-in-a-colab-notebook-df9823a04a32`](https://towardsdatascience.com/fine-tune-your-own-llama-2-model-in-a-colab-notebook-df9823a04a32)

## å®ç”¨çš„ LLM å¾®è°ƒä»‹ç»

[](https://medium.com/@mlabonne?source=post_page-----df9823a04a32--------------------------------)![Maxime Labonne](https://medium.com/@mlabonne?source=post_page-----df9823a04a32--------------------------------)[](https://towardsdatascience.com/?source=post_page-----df9823a04a32--------------------------------)![Towards Data Science](https://towardsdatascience.com/?source=post_page-----df9823a04a32--------------------------------) [Maxime Labonne](https://medium.com/@mlabonne?source=post_page-----df9823a04a32--------------------------------)

Â·å‘è¡¨äº [Towards Data Science](https://towardsdatascience.com/?source=post_page-----df9823a04a32--------------------------------) Â·12 åˆ†é’Ÿé˜…è¯»Â·2023 å¹´ 7 æœˆ 25 æ—¥

--

![](img/deab49c0869889d83573906da9f2c40b.png)

ä½œè€…æä¾›çš„å›¾ç‰‡

éšç€ LLaMA v1 çš„å‘å¸ƒï¼Œæˆ‘ä»¬è§è¯äº†å¾®è°ƒæ¨¡å‹çš„å¯’æ­¦çºªå¤§çˆ†å‘ï¼ŒåŒ…æ‹¬[Alpaca](https://github.com/tatsu-lab/stanford_alpaca)ã€[Vicuna](https://huggingface.co/lmsys/vicuna-13b-v1.3)å’Œ[WizardLM](https://huggingface.co/WizardLM/WizardLM-13B-V1.1)ç­‰ã€‚è¿™ä¸€è¶‹åŠ¿ä¿ƒä½¿ä¸åŒçš„å…¬å¸æ¨å‡ºäº†é€‚åˆå•†ä¸šä½¿ç”¨çš„åŸºç¡€æ¨¡å‹è®¸å¯è¯ï¼Œå¦‚[OpenLLaMA](https://github.com/openlm-research/open_llama)ã€[Falcon](https://falconllm.tii.ae/)å’Œ[XGen](https://github.com/salesforce/xgen)ç­‰ã€‚Llama 2 çš„å‘å¸ƒç°å°†ä¸¤è€…çš„æœ€ä½³å…ƒç´ ç»“åˆèµ·æ¥ï¼šæä¾›äº†**é«˜æ•ˆçš„åŸºç¡€æ¨¡å‹å’Œæ›´å®½æ¾çš„è®¸å¯è¯**ã€‚

åœ¨ 2023 å¹´ä¸ŠåŠå¹´ï¼Œè½¯ä»¶é¢†åŸŸå—åˆ°äº†**API çš„å¹¿æ³›ä½¿ç”¨**ï¼ˆå¦‚ OpenAI APIï¼‰çš„é‡å¤§å½±å“ï¼Œåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ„å»ºåŸºç¡€è®¾æ–½ã€‚[LangChain](https://python.langchain.com/docs/get_started/introduction.html)å’Œ[LlamaIndex](https://www.llamaindex.ai/)ç­‰åº“åœ¨è¿™ä¸€è¶‹åŠ¿ä¸­å‘æŒ¥äº†å…³é”®ä½œç”¨ã€‚è¿›å…¥ä¸‹åŠå¹´ï¼Œ**å¾®è°ƒï¼ˆæˆ–æŒ‡ä»¤è°ƒä¼˜ï¼‰è¿™äº›æ¨¡å‹å°†æˆä¸º LLMOps å·¥ä½œæµä¸­çš„æ ‡å‡†æµç¨‹**ã€‚è¿™ä¸€è¶‹åŠ¿å—åˆ°å¤šç§å› ç´ æ¨åŠ¨ï¼šèŠ‚çœæˆæœ¬çš„æ½œåŠ›ã€å¤„ç†æœºå¯†æ•°æ®çš„èƒ½åŠ›ï¼Œç”šè‡³æ˜¯å¼€å‘è¶…è¶Š ChatGPT å’Œ GPT-4 ç­‰æ˜¾è‘—æ¨¡å‹åœ¨ç‰¹å®šä»»åŠ¡ä¸­çš„è¡¨ç°çš„æ¨¡å‹çš„æ½œåŠ›ã€‚

åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å°†æ¢è®¨ä¸ºä»€ä¹ˆæŒ‡ä»¤è°ƒä¼˜æœ‰æ•ˆï¼Œä»¥åŠå¦‚ä½•åœ¨ Google Colab ç¬”è®°æœ¬ä¸­å®ç°å®ƒï¼Œä»¥åˆ›å»ºä½ è‡ªå·±çš„ Llama 2 æ¨¡å‹ã€‚ä¸å¾€å¸¸ä¸€æ ·ï¼Œä»£ç å¯åœ¨[Colab](https://colab.research.google.com/drive/1PEQyJO1-f6j0S_XJ8DV50NkpzasXkrzd?usp=sharing)å’Œ[GitHub](https://github.com/mlabonne/llm-course)ä¸Šæ‰¾åˆ°ã€‚

# **ğŸ”§** å¾®è°ƒ LLMs çš„èƒŒæ™¯

![](img/a0c0ab9d87266afc92a4a41531b2f70f.png)

å›¾ç‰‡ç”±ä½œè€…æä¾›

LLMs åœ¨å¤§é‡æ–‡æœ¬è¯­æ–™åº“ä¸Šè¿›è¡Œé¢„è®­ç»ƒã€‚ä»¥[Llama 2](https://arxiv.org/abs/2307.09288)ä¸ºä¾‹ï¼Œæˆ‘ä»¬å¯¹è®­ç»ƒé›†çš„ç»„æˆçŸ¥ä¹‹ç”šå°‘ï¼Œé™¤äº†å®ƒæœ‰ 2 ä¸‡äº¿ä¸ªæ ‡è®°ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œ[BERT](https://arxiv.org/abs/1810.04805)ï¼ˆ2018 å¹´ï¼‰â€œä»…â€åœ¨ BookCorpusï¼ˆ8 äº¿è¯ï¼‰å’Œè‹±è¯­ç»´åŸºç™¾ç§‘ï¼ˆ25 äº¿è¯ï¼‰ä¸Šè¿›è¡Œè¿‡è®­ç»ƒã€‚ä»ç»éªŒæ¥çœ‹ï¼Œè¿™**æ˜¯ä¸€ä¸ªéå¸¸æ˜‚è´µä¸”æ¼«é•¿çš„è¿‡ç¨‹**ï¼Œå¹¶ä¸”æœ‰å¾ˆå¤šç¡¬ä»¶é—®é¢˜ã€‚å¦‚æœä½ æƒ³äº†è§£æ›´å¤šï¼Œæˆ‘æ¨èé˜…è¯»[Meta çš„æ—¥å¿—](https://github.com/facebookresearch/metaseq/blob/main/projects/OPT/chronicles/OPT175B_Logbook.pdf)ï¼Œäº†è§£ OPT-175B æ¨¡å‹çš„é¢„è®­ç»ƒæƒ…å†µã€‚

å½“é¢„è®­ç»ƒå®Œæˆåï¼Œåƒ Llama 2 è¿™æ ·çš„è‡ªå›å½’æ¨¡å‹å¯ä»¥**é¢„æµ‹åºåˆ—ä¸­çš„ä¸‹ä¸€ä¸ªæ ‡è®°**ã€‚ç„¶è€Œï¼Œè¿™å¹¶æ²¡æœ‰ä½¿å®ƒä»¬æˆä¸ºç‰¹åˆ«æœ‰ç”¨çš„åŠ©æ‰‹ï¼Œå› ä¸ºå®ƒä»¬ä¸ä¼šå¯¹æŒ‡ä»¤ä½œå‡ºå›åº”ã€‚è¿™å°±æ˜¯æˆ‘ä»¬é‡‡ç”¨æŒ‡ä»¤è°ƒä¼˜æ¥ä½¿å®ƒä»¬çš„å›ç­”ä¸äººç±»æœŸæœ›å¯¹é½çš„åŸå› ã€‚ä¸»è¦æœ‰ä¸¤ç§å¾®è°ƒæŠ€æœ¯ï¼š

+   **ç›‘ç£å¾®è°ƒ**ï¼ˆSFTï¼‰ï¼šæ¨¡å‹åœ¨ä¸€ç»„æŒ‡ä»¤å’Œå“åº”çš„æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒã€‚å®ƒè°ƒæ•´ LLM ä¸­çš„æƒé‡ï¼Œä»¥æœ€å°åŒ–ç”Ÿæˆçš„ç­”æ¡ˆä¸çœŸå®å“åº”ä¹‹é—´çš„å·®å¼‚ï¼Œä½œä¸ºæ ‡ç­¾ã€‚

+   **æ¥è‡ªäººç±»åé¦ˆçš„å¼ºåŒ–å­¦ä¹ **ï¼ˆRLHFï¼‰ï¼šæ¨¡å‹é€šè¿‡ä¸ç¯å¢ƒäº’åŠ¨å¹¶æ¥æ”¶åé¦ˆæ¥å­¦ä¹ ã€‚å®ƒä»¬è¢«è®­ç»ƒä»¥æœ€å¤§åŒ–å¥–åŠ±ä¿¡å·ï¼ˆä½¿ç”¨[PPO](https://arxiv.org/abs/1707.06347)ï¼‰ï¼Œè¿™ä¸ªä¿¡å·é€šå¸¸æ¥æºäºäººç±»å¯¹æ¨¡å‹è¾“å‡ºçš„è¯„ä¼°ã€‚

ä¸€èˆ¬æ¥è¯´ï¼ŒRLHF è¢«è¯æ˜èƒ½å¤Ÿæ•æ‰åˆ°**æ›´å¤æ‚å’Œç»†è‡´çš„**äººç±»åå¥½ï¼Œä½†ä¹Ÿæ›´éš¾æœ‰æ•ˆå®æ–½ã€‚ç¡®å®ï¼Œå®ƒéœ€è¦ç²¾å¿ƒè®¾è®¡å¥–åŠ±ç³»ç»Ÿï¼Œå¹¶ä¸”å¯¹äººç±»åé¦ˆçš„è´¨é‡å’Œä¸€è‡´æ€§è¾ƒä¸ºæ•æ„Ÿã€‚æœªæ¥çš„ä¸€ä¸ªå¯èƒ½æ›¿ä»£æ–¹æ¡ˆæ˜¯[ç›´æ¥åå¥½ä¼˜åŒ–](https://arxiv.org/abs/2305.18290)ï¼ˆDPOï¼‰ç®—æ³•ï¼Œå®ƒç›´æ¥åœ¨ SFT æ¨¡å‹ä¸Šè¿è¡Œåå¥½å­¦ä¹ ã€‚

åœ¨æˆ‘ä»¬çš„æ¡ˆä¾‹ä¸­ï¼Œæˆ‘ä»¬å°†æ‰§è¡Œ SFTï¼Œä½†è¿™æå‡ºäº†ä¸€ä¸ªé—®é¢˜ï¼šä¸ºä»€ä¹ˆå¾®è°ƒåœ¨ç¬¬ä¸€æ—¶é—´ä¼šæœ‰æ•ˆï¼Ÿæ­£å¦‚[Orca è®ºæ–‡](https://mlabonne.github.io/blog/notes/Large%20Language%20Models/orca.html)ä¸­æ‰€å¼ºè°ƒçš„ï¼Œæˆ‘ä»¬çš„ç†è§£æ˜¯å¾®è°ƒ**åˆ©ç”¨äº†åœ¨é¢„è®­ç»ƒè¿‡ç¨‹ä¸­è·å¾—çš„çŸ¥è¯†**ã€‚æ¢å¥è¯è¯´ï¼Œå¦‚æœæ¨¡å‹ä»æœªè§è¿‡ä½ æ„Ÿå…´è¶£çš„æ•°æ®ç±»å‹ï¼Œå¾®è°ƒå°†æ— æµäºäº‹ã€‚ç„¶è€Œï¼Œå¦‚æœæ˜¯è¿™ç§æƒ…å†µï¼ŒSFT å¯ä»¥è¡¨ç°å¾—æå…¶ä¼˜ç§€ã€‚

ä¾‹å¦‚ï¼Œ[LIMA è®ºæ–‡](https://mlabonne.github.io/blog/notes/Large%20Language%20Models/lima.html)å±•ç¤ºäº†å¦‚ä½•é€šè¿‡åœ¨ä»… 1,000 ä¸ªé«˜è´¨é‡æ ·æœ¬ä¸Šå¾®è°ƒä¸€ä¸ªå…·æœ‰ 65 äº¿å‚æ•°çš„ LLaMAï¼ˆv1ï¼‰æ¨¡å‹æ¥è¶…è¶Š GPT-3ï¼ˆDaVinci003ï¼‰ã€‚**æŒ‡ä»¤æ•°æ®é›†çš„è´¨é‡è‡³å…³é‡è¦**ï¼Œä»¥è¾¾åˆ°è¿™ç§æ€§èƒ½æ°´å¹³ï¼Œè¿™å°±æ˜¯ä¸ºä»€ä¹ˆè®¸å¤šå·¥ä½œé›†ä¸­åœ¨è¿™ä¸ªé—®é¢˜ä¸Šï¼ˆå¦‚[evol-instruct](https://arxiv.org/abs/2304.12244)ã€Orca æˆ–[phi-1](https://mlabonne.github.io/blog/notes/Large%20Language%20Models/phi1.html)ï¼‰ã€‚è¯·æ³¨æ„ï¼ŒLLM çš„å¤§å°ï¼ˆ65bï¼Œè€Œä¸æ˜¯ 13b æˆ– 7bï¼‰å¯¹æœ‰æ•ˆåˆ©ç”¨å·²æœ‰çŸ¥è¯†ä¹Ÿæ˜¯è‡³å…³é‡è¦çš„ã€‚

ä¸æ•°æ®è´¨é‡ç›¸å…³çš„å¦ä¸€ä¸ªé‡è¦ç‚¹æ˜¯**æç¤ºæ¨¡æ¿**ã€‚æç¤ºç”±ç±»ä¼¼çš„å…ƒç´ ç»„æˆï¼šç”¨äºæŒ‡å¯¼æ¨¡å‹çš„ç³»ç»Ÿæç¤ºï¼ˆå¯é€‰ï¼‰ï¼Œç”¨äºæä¾›æŒ‡ä»¤çš„ç”¨æˆ·æç¤ºï¼ˆå¿…éœ€ï¼‰ï¼Œéœ€è¦è€ƒè™‘çš„é¢å¤–è¾“å…¥ï¼ˆå¯é€‰ï¼‰ï¼Œä»¥åŠæ¨¡å‹çš„å›ç­”ï¼ˆå¿…éœ€ï¼‰ã€‚åœ¨ Llama 2 çš„æƒ…å†µä¸‹ï¼Œä½œè€…ä½¿ç”¨äº†ä»¥ä¸‹**èŠå¤©æ¨¡å‹**çš„æ¨¡æ¿ï¼š

```py
<s>[INST] <<SYS>>
System prompt
<</SYS>>

User prompt [/INST] Model answer </s>
```

è¿˜æœ‰å…¶ä»–æ¨¡æ¿ï¼Œæ¯”å¦‚æ¥è‡ª Alpaca å’Œ Vicuna çš„æ¨¡æ¿ï¼Œå®ƒä»¬çš„å½±å“è¿˜ä¸å¤ªæ˜ç¡®ã€‚åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œæˆ‘ä»¬å°†é‡æ–°æ ¼å¼åŒ–æˆ‘ä»¬çš„æŒ‡ä»¤æ•°æ®é›†ä»¥ç¬¦åˆ Llama 2 çš„æ¨¡æ¿ã€‚ä¸ºäº†æœ¬æ•™ç¨‹çš„ç›®çš„ï¼Œæˆ‘å·²ç»ä½¿ç”¨äº†ä¼˜ç§€çš„`[timdettmers/openassistant-guanaco](https://huggingface.co/datasets/timdettmers/openassistant-guanaco)`æ•°æ®é›†ã€‚ä½ å¯ä»¥åœ¨ Hugging Face ä¸Šæ‰¾åˆ°å®ƒï¼Œåç§°ä¸º`[mlabonne/guanaco-llama2-1k](https://huggingface.co/datasets/mlabonne/guanaco-llama2-1k)`ã€‚è¯·æ³¨æ„ï¼Œå¦‚æœä½ ä½¿ç”¨çš„æ˜¯åŸºç¡€ç‰ˆ Llama 2 æ¨¡å‹è€Œä¸æ˜¯èŠå¤©ç‰ˆæœ¬ï¼Œä½ ä¸éœ€è¦éµå¾ªç‰¹å®šçš„æç¤ºæ¨¡æ¿ã€‚

# ğŸ¦™ å¦‚ä½•å¾®è°ƒ Llama 2

åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ Google Colabï¼ˆ2.21 ä¿¡ç”¨/å°æ—¶ï¼‰åœ¨é…å¤‡é«˜ RAM çš„ T4 GPU ä¸Šå¾®è°ƒä¸€ä¸ªå…·æœ‰ 70 äº¿å‚æ•°çš„ Llama 2 æ¨¡å‹ã€‚è¯·æ³¨æ„ï¼ŒT4 çš„ VRAM ä»…ä¸º 16 GBï¼Œè¿™å‹‰å¼ºè¶³å¤Ÿ**å­˜å‚¨ Llama 2â€“7b çš„æƒé‡**ï¼ˆ7b Ã— 2 å­—èŠ‚ = 14 GB çš„ FP16ï¼‰ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜éœ€è¦è€ƒè™‘ä¼˜åŒ–å™¨çŠ¶æ€ã€æ¢¯åº¦å’Œå‰å‘æ¿€æ´»çš„å¼€é”€ï¼ˆæœ‰å…³æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚è§[è¿™ç¯‡ä¼˜ç§€çš„æ–‡ç« ](https://huggingface.co/docs/transformers/perf_train_gpu_one#anatomy-of-models-memory)ï¼‰ã€‚è¿™æ„å‘³ç€åœ¨è¿™é‡Œè¿›è¡Œå…¨é¢å¾®è°ƒæ˜¯ä¸å¯èƒ½çš„ï¼šæˆ‘ä»¬éœ€è¦åƒ[LoRA](https://arxiv.org/abs/2106.09685)æˆ–[QLoRA](https://arxiv.org/abs/2305.14314)è¿™æ ·çš„å‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆPEFTï¼‰æŠ€æœ¯ã€‚

ä¸ºäº†å¤§å¹…å‡å°‘ VRAM ä½¿ç”¨é‡ï¼Œæˆ‘ä»¬å¿…é¡»**ç”¨ 4 ä½ç²¾åº¦å¾®è°ƒæ¨¡å‹**ï¼Œè¿™å°±æ˜¯æˆ‘ä»¬åœ¨è¿™é‡Œä½¿ç”¨ QLoRA çš„åŸå› ã€‚å¥½æ¶ˆæ¯æ˜¯æˆ‘ä»¬å¯ä»¥åˆ©ç”¨ Hugging Face ç”Ÿæ€ç³»ç»Ÿä¸­çš„`transformers`ã€`accelerate`ã€`peft`ã€`trl`å’Œ`bitsandbytes`åº“ã€‚æˆ‘ä»¬å°†åœ¨ä»¥ä¸‹ä»£ç ä¸­è¿›è¡Œæ“ä½œï¼ŒåŸºäº Younes Belkada çš„[GitHub Gist](https://gist.github.com/younesbelkada/9f7f75c94bdc1981c8ca5cc937d4a4da)ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬å®‰è£…å¹¶åŠ è½½è¿™äº›åº“ã€‚

```py
!pip install -q accelerate==0.21.0 peft==0.4.0 bitsandbytes==0.40.2 transformers==4.31.0 trl==0.4.7
```

```py
import os
import torch
from datasets import load_dataset
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    BitsAndBytesConfig,
    HfArgumentParser,
    TrainingArguments,
    pipeline,
    logging,
)
from peft import LoraConfig, PeftModel
from trl import SFTTrainer
```

è®©æˆ‘ä»¬ç¨å¾®è®¨è®ºä¸€ä¸‹å¯ä»¥åœ¨è¿™é‡Œè°ƒæ•´çš„å‚æ•°ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬å¸Œæœ›åŠ è½½ä¸€ä¸ª`llama-2-7b-chat-hf`æ¨¡å‹ï¼ˆ**chat** æ¨¡å‹ï¼‰ï¼Œå¹¶åœ¨`mlabonne/guanaco-llama2-1k`ï¼ˆ1,000 ä¸ªæ ·æœ¬ï¼‰ä¸Šè®­ç»ƒå®ƒï¼Œè¿™å°†ç”Ÿæˆæˆ‘ä»¬å¾®è°ƒåçš„æ¨¡å‹`llama-2-7b-miniguanaco`ã€‚å¦‚æœä½ å¯¹å¦‚ä½•åˆ›å»ºè¿™ä¸ªæ•°æ®é›†æ„Ÿå…´è¶£ï¼Œå¯ä»¥æŸ¥çœ‹[è¿™ä¸ªç¬”è®°æœ¬](https://colab.research.google.com/drive/1Ad7a9zMmkxuXTOh1Z7-rNSICA4dybpM2?usp=sharing)ã€‚å¯ä»¥éšæ„æ›´æ”¹ï¼šåœ¨[Hugging Face Hub](https://huggingface.co/datasets)ä¸Šæœ‰å¾ˆå¤šå¥½çš„æ•°æ®é›†ï¼Œæ¯”å¦‚`[databricks/databricks-dolly-15k](https://huggingface.co/datasets/databricks/databricks-dolly-15k)`ã€‚

QLoRA å°†ä½¿ç”¨ 64 çš„ç§©å’Œ 16 çš„ç¼©æ”¾å‚æ•°ï¼ˆæœ‰å…³ LoRA å‚æ•°çš„æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚è§[è¿™ç¯‡æ–‡ç« ](https://rentry.org/llm-training#low-rank-adaptation-lora_1)ï¼‰ã€‚æˆ‘ä»¬å°†ä½¿ç”¨ NF4 ç±»å‹ä»¥ 4 ä½ç²¾åº¦ç›´æ¥åŠ è½½ Llama 2 æ¨¡å‹ï¼Œå¹¶è®­ç»ƒä¸€ä¸ªå‘¨æœŸã€‚æœ‰å…³å…¶ä»–å‚æ•°çš„æ›´å¤šä¿¡æ¯ï¼Œè¯·æŸ¥çœ‹[TrainingArguments](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments)ã€[PeftModel](https://huggingface.co/docs/peft/package_reference/peft_model)å’Œ[SFTTrainer](https://huggingface.co/docs/trl/main/en/sft_trainer) æ–‡æ¡£ã€‚

```py
# The model that you want to train from the Hugging Face hub
model_name = "NousResearch/Llama-2-7b-chat-hf"

# The instruction dataset to use
dataset_name = "mlabonne/guanaco-llama2-1k"

# Fine-tuned model name
new_model = "llama-2-7b-miniguanaco"

################################################################################
# QLoRA parameters
################################################################################

# LoRA attention dimension
lora_r = 64

# Alpha parameter for LoRA scaling
lora_alpha = 16

# Dropout probability for LoRA layers
lora_dropout = 0.1

################################################################################
# bitsandbytes parameters
################################################################################

# Activate 4-bit precision base model loading
use_4bit = True

# Compute dtype for 4-bit base models
bnb_4bit_compute_dtype = "float16"

# Quantization type (fp4 or nf4)
bnb_4bit_quant_type = "nf4"

# Activate nested quantization for 4-bit base models (double quantization)
use_nested_quant = False

################################################################################
# TrainingArguments parameters
################################################################################

# Output directory where the model predictions and checkpoints will be stored
output_dir = "./results"

# Number of training epochs
num_train_epochs = 1

# Enable fp16/bf16 training (set bf16 to True with an A100)
fp16 = False
bf16 = False

# Batch size per GPU for training
per_device_train_batch_size = 4

# Batch size per GPU for evaluation
per_device_eval_batch_size = 4

# Number of update steps to accumulate the gradients for
gradient_accumulation_steps = 1

# Enable gradient checkpointing
gradient_checkpointing = True

# Maximum gradient normal (gradient clipping)
max_grad_norm = 0.3

# Initial learning rate (AdamW optimizer)
learning_rate = 2e-4

# Weight decay to apply to all layers except bias/LayerNorm weights
weight_decay = 0.001

# Optimizer to use
optim = "paged_adamw_32bit"

# Learning rate schedule (constant a bit better than cosine)
lr_scheduler_type = "constant"

# Number of training steps (overrides num_train_epochs)
max_steps = -1

# Ratio of steps for a linear warmup (from 0 to learning rate)
warmup_ratio = 0.03

# Group sequences into batches with same length
# Saves memory and speeds up training considerably
group_by_length = True

# Save checkpoint every X updates steps
save_steps = 25

# Log every X updates steps
logging_steps = 25

################################################################################
# SFT parameters
################################################################################

# Maximum sequence length to use
max_seq_length = None

# Pack multiple short examples in the same input sequence to increase efficiency
packing = False

# Load the entire model on the GPU 0
device_map = {"": 0}
```

ç°åœ¨æˆ‘ä»¬å¯ä»¥åŠ è½½æ‰€æœ‰å†…å®¹å¹¶å¼€å§‹å¾®è°ƒè¿‡ç¨‹ã€‚æˆ‘ä»¬ä¾èµ–å¤šä¸ªåŒ…è£…å™¨ï¼Œè¯·è€å¿ƒç­‰å¾…ã€‚

+   é¦–å…ˆï¼Œæˆ‘ä»¬è¦**åŠ è½½æˆ‘ä»¬å®šä¹‰çš„æ•°æ®é›†**ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬çš„æ•°æ®é›†å·²ç»è¿‡é¢„å¤„ç†ï¼Œä½†é€šå¸¸ï¼Œè¿™æ—¶ä½ ä¼šé‡æ–°æ ¼å¼åŒ–æç¤ºã€ç­›é€‰æ‰ä¸è‰¯æ–‡æœ¬ã€åˆå¹¶å¤šä¸ªæ•°æ®é›†ç­‰ã€‚

+   ç„¶åï¼Œæˆ‘ä»¬æ­£åœ¨é…ç½® `bitsandbytes` ä»¥è¿›è¡Œ 4 ä½é‡åŒ–ã€‚

+   æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬æ­£åœ¨ä½¿ç”¨ç›¸åº”çš„åˆ†è¯å™¨å°† Llama 2 æ¨¡å‹ä»¥ 4 ä½ç²¾åº¦åŠ è½½åˆ° GPU ä¸Šã€‚

+   æœ€åï¼Œæˆ‘ä»¬æ­£åœ¨åŠ è½½ QLoRA é…ç½®ã€å¸¸è§„è®­ç»ƒå‚æ•°ï¼Œå¹¶å°†æ‰€æœ‰å†…å®¹ä¼ é€’ç»™`SFTTrainer`ã€‚è®­ç»ƒå¯ä»¥æœ€ç»ˆå¼€å§‹ï¼

```py
# Load dataset (you can process it here)
dataset = load_dataset(dataset_name, split="train")

# Load tokenizer and model with QLoRA configuration
compute_dtype = getattr(torch, bnb_4bit_compute_dtype)

bnb_config = BitsAndBytesConfig(
    load_in_4bit=use_4bit,
    bnb_4bit_quant_type=bnb_4bit_quant_type,
    bnb_4bit_compute_dtype=compute_dtype,
    bnb_4bit_use_double_quant=use_nested_quant,
)

# Check GPU compatibility with bfloat16
if compute_dtype == torch.float16 and use_4bit:
    major, _ = torch.cuda.get_device_capability()
    if major >= 8:
        print("=" * 80)
        print("Your GPU supports bfloat16: accelerate training with bf16=True")
        print("=" * 80)

# Load base model
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    quantization_config=bnb_config,
    device_map=device_map
)
model.config.use_cache = False
model.config.pretraining_tp = 1

# Load LLaMA tokenizer
tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = "right"

# Load LoRA configuration
peft_config = LoraConfig(
    lora_alpha=lora_alpha,
    lora_dropout=lora_dropout,
    r=lora_r,
    bias="none",
    task_type="CAUSAL_LM",
)

# Set training parameters
training_arguments = TrainingArguments(
    output_dir=output_dir,
    num_train_epochs=num_train_epochs,
    per_device_train_batch_size=per_device_train_batch_size,
    gradient_accumulation_steps=gradient_accumulation_steps,
    optim=optim,
    save_steps=save_steps,
    logging_steps=logging_steps,
    learning_rate=learning_rate,
    weight_decay=weight_decay,
    fp16=fp16,
    bf16=bf16,
    max_grad_norm=max_grad_norm,
    max_steps=max_steps,
    warmup_ratio=warmup_ratio,
    group_by_length=group_by_length,
    lr_scheduler_type=lr_scheduler_type,
    report_to="tensorboard"
)

# Set supervised fine-tuning parameters
trainer = SFTTrainer(
    model=model,
    train_dataset=dataset,
    peft_config=peft_config,
    dataset_text_field="text",
    max_seq_length=max_seq_length,
    tokenizer=tokenizer,
    args=training_arguments,
    packing=packing,
)

# Train model
trainer.train()

# Save trained model
trainer.model.save_pretrained(new_model)
```

![](img/7a4459b066e68e7cd3002cd3113c8876.png)

ä½œè€…æä¾›çš„å›¾ç‰‡

è®­ç»ƒå¯èƒ½ä¼šå¾ˆé•¿æ—¶é—´ï¼Œè¿™å–å†³äºæ•°æ®é›†çš„å¤§å°ã€‚åœ¨è¿™é‡Œï¼ŒT4 GPU ä¸ŠèŠ±äº†ä¸åˆ°ä¸€ä¸ªå°æ—¶ã€‚æˆ‘ä»¬å¯ä»¥åœ¨ tensorboard ä¸Šæ£€æŸ¥å›¾è¡¨ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š

```py
%load_ext tensorboard
%tensorboard --logdir results/runs
```

![](img/0961e701ded34f0fb82b7c66fcdad85f.png)

ä½œè€…æä¾›çš„å›¾ç‰‡

è®©æˆ‘ä»¬ç¡®ä¿æ¨¡å‹è¡Œä¸ºæ­£ç¡®ã€‚è¿™éœ€è¦æ›´å…¨é¢çš„è¯„ä¼°ï¼Œä½†æˆ‘ä»¬å¯ä»¥ä½¿ç”¨**æ–‡æœ¬ç”Ÿæˆç®¡é“**æ¥æå‡ºè¯¸å¦‚â€œä»€ä¹ˆæ˜¯å¤§è¯­è¨€æ¨¡å‹ï¼Ÿâ€è¿™æ ·çš„é—®é¢˜ã€‚è¯·æ³¨æ„ï¼Œæˆ‘æ­£åœ¨æ ¼å¼åŒ–è¾“å…¥ä»¥åŒ¹é… Llama 2 çš„æç¤ºæ¨¡æ¿ã€‚

```py
# Ignore warnings
logging.set_verbosity(logging.CRITICAL)

# Run text generation pipeline with our next model
prompt = "What is a large language model?"
pipe = pipeline(task="text-generation", model=model, tokenizer=tokenizer, max_length=200)
result = pipe(f"<s>[INST] {prompt} [/INST]")
print(result[0]['generated_text'])
```

æ¨¡å‹è¾“å‡ºäº†ä»¥ä¸‹å“åº”ï¼š

```py
A large language model is a type of artificial intelligence that is trained on a large dataset of text to generate human-like language. It is typically trained on a dataset of text that is much larger than the dataset used for smaller language models. The large dataset allows the model to learn more complex patterns in language, which can result in more accurate and natural-sounding language generation.Large language models are often used for tasks such as text summarization, language translation, and chatbots. They are also used for more complex tasks such as writing articles, generating creative content, and even composing music.Large language models are trained using a technique called deep learning, which involves using many layers of artificial neural networks to learn complex patterns in the data. The model is trained on a large dataset of text, and the neural networks are trained to predict the next word in a sequence of text given
```

æ ¹æ®ç»éªŒï¼Œå¯¹äºåªæœ‰ 70 äº¿å‚æ•°çš„æ¨¡å‹æ¥è¯´ï¼Œå®ƒ**éå¸¸ä¸€è‡´**ã€‚ä½ å¯ä»¥å°è¯•ä½¿ç”¨è¯„ä¼°æ•°æ®é›†ä¸­çš„æ›´éš¾é—®é¢˜ï¼Œå¦‚[BigBench-Hard](https://github.com/suzgunmirac/BIG-Bench-Hard)ã€‚Guanaco æ˜¯ä¸€ä¸ªä¼˜ç§€çš„æ•°æ®é›†ï¼Œä»¥å‰ç”Ÿäº§è¿‡é«˜è´¨é‡çš„æ¨¡å‹ã€‚ä½ å¯ä»¥ä½¿ç”¨`[mlabonne/guanaco-llama2](https://huggingface.co/datasets/mlabonne/guanaco-llama2)`åœ¨æ•´ä¸ªæ•°æ®é›†ä¸Šè®­ç»ƒ Llama 2 æ¨¡å‹ã€‚

ç°åœ¨æˆ‘ä»¬å¦‚ä½•å­˜å‚¨æˆ‘ä»¬æ–°çš„ `llama-2-7b-miniguanaco` æ¨¡å‹ï¼Ÿæˆ‘ä»¬éœ€è¦å°† LoRA çš„æƒé‡ä¸åŸºç¡€æ¨¡å‹åˆå¹¶ã€‚ä¸å¹¸çš„æ˜¯ï¼Œæ®æˆ‘æ‰€çŸ¥ï¼Œæ²¡æœ‰ç®€å•çš„æ–¹æ³•æ¥åšåˆ°è¿™ä¸€ç‚¹ï¼šæˆ‘ä»¬éœ€è¦ä»¥ FP16 ç²¾åº¦é‡æ–°åŠ è½½åŸºç¡€æ¨¡å‹ï¼Œå¹¶ä½¿ç”¨ `peft` åº“æ¥åˆå¹¶æ‰€æœ‰å†…å®¹ã€‚å¯æƒœçš„æ˜¯ï¼Œè¿™ä¹Ÿä¼šå¼•å‘ VRAM çš„é—®é¢˜ï¼ˆå°½ç®¡å·²ç»æ¸…ç©ºï¼‰ï¼Œæ‰€ä»¥æˆ‘å»ºè®®**é‡å¯ç¬”è®°æœ¬**ï¼Œé‡æ–°æ‰§è¡Œå‰ä¸‰ä¸ªå•å…ƒæ ¼ï¼Œç„¶åæ‰§è¡Œä¸‹ä¸€ä¸ªå•å…ƒæ ¼ã€‚å¦‚æœä½ çŸ¥é“è§£å†³åŠæ³•ï¼Œè¯·è”ç³»æˆ‘ï¼

```py
# Reload model in FP16 and merge it with LoRA weights
base_model = AutoModelForCausalLM.from_pretrained(
    model_name,
    low_cpu_mem_usage=True,
    return_dict=True,
    torch_dtype=torch.float16,
    device_map=device_map,
)
model = PeftModel.from_pretrained(base_model, new_model)
model = model.merge_and_unload()

# Reload tokenizer to save it
tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
tokenizer.add_special_tokens({'pad_token': '[PAD]'})
tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = "right"
```

æˆ‘ä»¬çš„æƒé‡å·²åˆå¹¶å¹¶é‡æ–°åŠ è½½äº†åˆ†è¯å™¨ã€‚ç°åœ¨æˆ‘ä»¬å¯ä»¥å°†æ‰€æœ‰å†…å®¹æ¨é€åˆ° Hugging Face Hub ä»¥ä¿å­˜æˆ‘ä»¬çš„æ¨¡å‹ã€‚

```py
!huggingface-cli login

model.push_to_hub(new_model, use_temp_dir=False)
tokenizer.push_to_hub(new_model, use_temp_dir=False)
```

ç°åœ¨ï¼Œä½ å¯ä»¥åƒåŠ è½½ Hub ä¸Šçš„å…¶ä»– Llama 2 æ¨¡å‹ä¸€æ ·ä½¿ç”¨è¿™ä¸ªæ¨¡å‹è¿›è¡Œæ¨ç†ã€‚ä¹Ÿå¯ä»¥é‡æ–°åŠ è½½å®ƒä»¥è¿›è¡Œæ›´å¤šçš„å¾®è°ƒâ€”â€”ä¹Ÿè®¸ç”¨å¦ä¸€ä¸ªæ•°æ®é›†ï¼Ÿ

å¦‚æœä½ è®¤çœŸè€ƒè™‘å¾®è°ƒæ¨¡å‹ï¼Œå»ºè®®**ä½¿ç”¨è„šæœ¬**è€Œä¸æ˜¯ç¬”è®°æœ¬ã€‚ä½ å¯ä»¥åœ¨ Lambda Labsã€Runpodã€Vast.ai ä¸Šä»¥ä½äº 0.3$/å°æ—¶çš„ä»·æ ¼è½»æ¾ç§Ÿç”¨ GPUã€‚ä¸€æ—¦è¿æ¥ï¼Œä½ å¯ä»¥å®‰è£…åº“ã€å¯¼å…¥è„šæœ¬ã€ç™»å½• Hugging Face å’Œå…¶ä»–å·¥å…·ï¼ˆå¦‚ Weights & Biases ç”¨äºè®°å½•å®éªŒï¼‰ï¼Œç„¶åå¼€å§‹å¾®è°ƒã€‚

`trl` è„šæœ¬ç›®å‰éå¸¸æœ‰é™ï¼Œå› æ­¤æˆ‘åŸºäºä¹‹å‰çš„ç¬”è®°æœ¬åˆ¶ä½œäº†è‡ªå·±çš„ç‰ˆæœ¬ã€‚ä½ å¯ä»¥åœ¨[**GitHub Gist ä¸Šæ‰¾åˆ°å®ƒ**](https://gist.github.com/mlabonne/8eb9ad60c6340cb48a17385c68e3b1a5)ã€‚å¦‚æœä½ åœ¨å¯»æ‰¾å…¨é¢çš„è§£å†³æ–¹æ¡ˆï¼Œå¯ä»¥æŸ¥çœ‹æ¥è‡ª OpenAccess AI Collective çš„ [Axolotl](https://github.com/OpenAccess-AI-Collective/axolotl)ï¼Œå®ƒä¹ŸåŸç”Ÿæ”¯æŒå¤šä¸ªæ•°æ®é›†ã€Deepspeedã€Flash Attention ç­‰ã€‚

# ç»“è®º

åœ¨è¿™ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬å±•ç¤ºäº†å¦‚ä½•ä½¿ç”¨ Colab ç¬”è®°æœ¬å¯¹ Llama 2 7b æ¨¡å‹è¿›è¡Œå¾®è°ƒã€‚æˆ‘ä»¬ä»‹ç»äº†ä¸€äº› LLM è®­ç»ƒå’Œå¾®è°ƒçš„å¿…è¦èƒŒæ™¯ï¼Œä»¥åŠä¸æŒ‡ä»¤æ•°æ®é›†ç›¸å…³çš„é‡è¦è€ƒè™‘å› ç´ ã€‚åœ¨ç¬¬äºŒéƒ¨åˆ†ï¼Œæˆ‘ä»¬**æˆåŠŸåœ°å¾®è°ƒäº† Llama 2 æ¨¡å‹**ï¼Œä½¿ç”¨äº†å…¶åŸç”Ÿæç¤ºæ¨¡æ¿å’Œè‡ªå®šä¹‰å‚æ•°ã€‚

è¿™äº›å¾®è°ƒåçš„æ¨¡å‹å¯ä»¥é›†æˆåˆ° LangChain å’Œå…¶ä»–æ¶æ„ä¸­ï¼Œä½œä¸º OpenAI API çš„æœ‰åˆ©æ›¿ä»£æ–¹æ¡ˆã€‚è®°ä½ï¼Œåœ¨è¿™ä¸ªæ–°èŒƒå¼ä¸­ï¼ŒæŒ‡ä»¤æ•°æ®é›†æ˜¯æ–°çš„é»„é‡‘ï¼Œä½ çš„æ¨¡å‹çš„è´¨é‡åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šå–å†³äºå…¶å¾®è°ƒçš„æ•°æ®ã€‚å› æ­¤ï¼Œç¥ä½ æ„å»ºé«˜è´¨é‡æ•°æ®é›†å¥½è¿ï¼

å¦‚æœä½ å¯¹æ›´å¤šå…³äº LLM çš„å†…å®¹æ„Ÿå…´è¶£ï¼Œå¯ä»¥åœ¨ Twitter ä¸Šå…³æ³¨æˆ‘ [@maximelabonne](https://twitter.com/maximelabonne)ã€‚

# å‚è€ƒèµ„æ–™

+   Hugo Touvron, Thomas Scialom ç­‰äººï¼ˆ2023 å¹´ï¼‰ã€‚[Llama 2: Open Foundation and Fine-Tuned Chat Models](https://arxiv.org/abs/2307.09288)ã€‚

+   Philipp Schmid, Omar Sanseviero, Pedro Cuenca, & Lewis Tunstall. Llama 2 å·²ä¸Šçº¿â€”â€”åœ¨ Hugging Face ä¸Šè·å–å®ƒã€‚ [`huggingface.co/blog/llama2`](https://huggingface.co/blog/llama2)

+   ç½—æ±‰Â·å¡”å¥¥é‡Œã€ä¼Šèæ©Â·å¤å°”æ‹‰è´¾å°¼ã€å¼ å¤©è‰ºã€æ‰¬Â·æœå¸ƒç“¦ã€æé›ªè¾°ã€å¡æ´›æ–¯Â·ç›–æ–¯ç‰¹æ—ã€ç€è¥¿Â·æ¢åŠè¾°é‡åšã€‚ (2023). [æ–¯å¦ç¦é˜¿å°”å¸•å¡ï¼šä¸€ç§éµå¾ªæŒ‡ä»¤çš„ LLaMA æ¨¡å‹](https://crfm.stanford.edu/2023/03/13/alpaca.html)ã€‚

+   é›…å„å¸ƒÂ·å¾·å¤«æ—ã€æ˜ä¼ŸÂ·å¼ ã€è‚¯é¡¿Â·æåŠå…‹é‡Œæ–¯è’‚å¨œÂ·æ‰˜ç‰¹è¯ºå¨ƒã€‚ (2019). [BERTï¼šç”¨äºè¯­è¨€ç†è§£çš„æ·±åº¦åŒå‘å˜æ¢å™¨çš„é¢„è®­ç»ƒ](https://arxiv.org/abs/1810.04805)ã€‚

+   æå§†Â·å¾·ç‰¹é»˜æ–¯ã€é˜¿å°”è’‚å¤šç½—Â·å¸•å°¼å¥¥å°¼ã€é˜¿é‡ŒÂ·éœå°”èŒ¨æ›¼åŠå¢å…‹Â·æ³½ç‰¹å°”æ‘©è€¶ã€‚ (2023). [QLoRAï¼šé«˜æ•ˆå¾®è°ƒé‡åŒ– LLM](https://arxiv.org/abs/2305.14314)ã€‚

# ç›¸å…³æ–‡ç« 

[](/4-bit-quantization-with-gptq-36b0f4f02c34?source=post_page-----df9823a04a32--------------------------------) ## ä½¿ç”¨ GPTQ è¿›è¡Œ 4 ä½é‡åŒ–

### ä½¿ç”¨ AutoGPTQ å¯¹è‡ªå·±çš„ LLM è¿›è¡Œé‡åŒ–

towardsdatascience.com [](/decoding-strategies-in-large-language-models-9733a8f70539?source=post_page-----df9823a04a32--------------------------------) ## å¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„è§£ç ç­–ç•¥

### ä»æŸæœç´¢åˆ°æ ¸é‡‡æ ·çš„æ–‡æœ¬ç”ŸæˆæŒ‡å—

towardsdatascience.com

*äº†è§£æ›´å¤šæœºå™¨å­¦ä¹ çŸ¥è¯†ï¼Œå¹¶é€šè¿‡ç‚¹å‡»æ”¯æŒæˆ‘çš„å·¥ä½œâ€”â€”æˆä¸º Medium ä¼šå‘˜ï¼š*

[](https://medium.com/@mlabonne/membership?source=post_page-----df9823a04a32--------------------------------) [## é€šè¿‡æˆ‘çš„æ¨èé“¾æ¥åŠ å…¥ Medium - Maxime Labonne

### ä½œä¸º Medium ä¼šå‘˜ï¼Œä½ çš„ä¼šå‘˜è´¹çš„ä¸€éƒ¨åˆ†ä¼šæ”¯æŒä½ é˜…è¯»çš„ä½œè€…ï¼ŒåŒæ—¶ä½ å¯ä»¥å…¨é¢è®¿é—®æ¯ä¸€ä¸ªæ•…äº‹â€¦â€¦

medium.com](https://medium.com/@mlabonne/membership?source=post_page-----df9823a04a32--------------------------------)

*å¦‚æœä½ å·²ç»æ˜¯ä¼šå‘˜ï¼Œä½ å¯ä»¥* [*åœ¨ Medium ä¸Šå…³æ³¨æˆ‘*](https://medium.com/@mlabonne)*ã€‚*
