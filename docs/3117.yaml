- en: 'QA-LoRA: Fine-Tune a Quantized Large Language Model on Your GPU'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/qa-lora-fine-tune-a-quantized-large-language-model-on-your-gpu-c7291866706c?source=collection_archive---------0-----------------------#2023-10-14](https://towardsdatascience.com/qa-lora-fine-tune-a-quantized-large-language-model-on-your-gpu-c7291866706c?source=collection_archive---------0-----------------------#2023-10-14)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Quantization-aware fine-tuning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@bnjmn_marie?source=post_page-----c7291866706c--------------------------------)[![Benjamin
    Marie](../Images/3ea1ad230cb1e67610418a8e36a5e5dd.png)](https://medium.com/@bnjmn_marie?source=post_page-----c7291866706c--------------------------------)[](https://towardsdatascience.com/?source=post_page-----c7291866706c--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----c7291866706c--------------------------------)
    [Benjamin Marie](https://medium.com/@bnjmn_marie?source=post_page-----c7291866706c--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fad2a414578b3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fqa-lora-fine-tune-a-quantized-large-language-model-on-your-gpu-c7291866706c&user=Benjamin+Marie&userId=ad2a414578b3&source=post_page-ad2a414578b3----c7291866706c---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----c7291866706c--------------------------------)
    ·10 min read·Oct 14, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fc7291866706c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fqa-lora-fine-tune-a-quantized-large-language-model-on-your-gpu-c7291866706c&user=Benjamin+Marie&userId=ad2a414578b3&source=-----c7291866706c---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc7291866706c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fqa-lora-fine-tune-a-quantized-large-language-model-on-your-gpu-c7291866706c&source=-----c7291866706c---------------------bookmark_footer-----------)![](../Images/eb5cbf8008d012e797a4d1e66f9ca0c6.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Illustration by the author — Made with images from Pixabay ([1](https://pixabay.com/vectors/llama-alpaca-animal-mammal-zoo-297668/),[2](https://pixabay.com/vectors/skateboard-skating-skate-silhouette-7192315/))
  prefs: []
  type: TYPE_NORMAL
- en: State-of-the-art large language models (LLMs) are pre-trained with billions
    of parameters. While pre-trained LLMs can perform many tasks, they can become
    much better once fine-tuned.
  prefs: []
  type: TYPE_NORMAL
- en: Thanks to LoRA, fine-tuning costs can be dramatically reduced. LoRA adds low-rank
    tensors, i.e., a small number of parameters (millions), on top of the frozen original
    parameters. Only the parameters in the added tensors are trained during fine-tuning.
  prefs: []
  type: TYPE_NORMAL
- en: LoRA still requires the model to be loaded in memory. To reduce the memory cost
    and speed-up fine-tuning, a new approach proposes quantization-aware LoRA (QA-LoRA)
    fine-tuning.
  prefs: []
  type: TYPE_NORMAL
- en: In this article, I explain QA-LoRA and review its performance compared with
    previous work (especially QLoRA). I also show how to use QA-LoRA to fine-tune
    your own quantization-aware LoRA for Llama 2.
  prefs: []
  type: TYPE_NORMAL
- en: What’s Wrong with QLoRA?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Fine-tuning LoRA on top of a quantized LLM is something that can already be
    done with QLoRA. In my previous articles, I used it many times to fine-tune LLMs,
    for instance, Llama 2 and GPT-NeoX, on my desktop computer or using the free instance
    of Google Colab.
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://kaitchup.substack.com/p/fine-tune-llama-2-on-your-computer?source=post_page-----c7291866706c--------------------------------)
    [## Fine-tune Llama 2 on Your Computer with QLoRa and TRL'
  prefs: []
  type: TYPE_NORMAL
- en: On Guanaco and with the correct padding
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: kaitchup.substack.com](https://kaitchup.substack.com/p/fine-tune-llama-2-on-your-computer?source=post_page-----c7291866706c--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: Before delving into QA-LoRA, it is interesting to understand what are the current
    limits of QLoRA.
  prefs: []
  type: TYPE_NORMAL
- en: The NormalFloat4 (NF4) Quantization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: LLM quantization algorithms usually quantize parameters to a 4-bit precision
    using the INT4 data type. Computation with this data type is more and more optimized
    with recent GPUs.
  prefs: []
  type: TYPE_NORMAL
- en: QLoRA doesn’t use INT4 by default but another data type called NormalFloat4
    (NF4). You can see it as a compressed float number. According to the authors of
    [QLoRA](https://arxiv.org/abs/2305.14314), NF4 is superior to INT4\. LLMs quantized
    with NF4 achieve a lower perplexity.
  prefs: []
  type: TYPE_NORMAL
- en: However, NF4 computation is not optimal for fast inference. This is one of the
    reasons why models quantized with GPTQ are faster than models quantized with bitsandbytes
    NF4\. In previous articles, I confirmed that GPTQ models are indeed faster.
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://kaitchup.substack.com/p/gptq-or-bitsandbytes-which-quantization?source=post_page-----c7291866706c--------------------------------)
    [## GPTQ or bitsandbytes: Which Quantization Method to Use for LLMs - Examples
    with Llama 2'
  prefs: []
  type: TYPE_NORMAL
- en: Large language model quantization for affordable fine-tuning and inference on
    your computer
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: kaitchup.substack.com](https://kaitchup.substack.com/p/gptq-or-bitsandbytes-which-quantization?source=post_page-----c7291866706c--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: NF4 is also one of the weaknesses pointed out by the authors of [QA-LoRA](https://arxiv.org/abs/2309.14717).
  prefs: []
  type: TYPE_NORMAL
- en: NF4 Base Model, But FP16 LoRA
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: While the base model is quantized with NF4, the trained LoRA’s parameters remain
    at a higher precision which is usually FP16, as illustrated in the figure below.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c43d096218c9bbe4f6315cd7d0b00846.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure by the author
  prefs: []
  type: TYPE_NORMAL
- en: This is key in the QLoRA performance as naively training quantized parameters
    would lead to poor performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'Consequently, for inference, we have two different ways to use the LoRA adapters
    trained with QLoRA:'
  prefs: []
  type: TYPE_NORMAL
- en: Loading them on top of the base LLMs as we do during QLoRA fine-tuning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Merging them with the base LLMs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Loading them is optimal to preserve the performance. We keep LoRA’s parameters
    at 16-bit precision but, since they are only a few million, they don’t consume
    much VRAM relative to the quantized base LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: The other alternative is to merge the LoRA’s parameters with the base model.
    I explored [several merging recipes in a previous article](https://kaitchup.substack.com/p/lora-adapters-when-a-naive-merge).
  prefs: []
  type: TYPE_NORMAL
- en: Ideally, we have to dequantize the base model to the same precision used by
    LoRA’s parameters, and then merge LoRA’s parameters with the dequantized base
    model.
  prefs: []
  type: TYPE_NORMAL
- en: But then, as a result, the merged model is not quantized anymore (FP16). It’s
    a big dequantized model. We could quantize the entire merged model but quantization
    always loses information. We would obtain a model performing below the performance
    we originally obtained at the end of the QLoRA fine-tuning.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are the results I obtained [for different configurations](https://kaitchup.substack.com/p/lora-adapters-when-a-naive-merge):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d7606d20e92226d15ea81fa36fb68d5b.png)'
  prefs: []
  type: TYPE_IMG
- en: Table and results by the author
  prefs: []
  type: TYPE_NORMAL
- en: '*Note: Lower perplexity is better.*'
  prefs: []
  type: TYPE_NORMAL
- en: We can see that quantizing the merged model leads to a significantly higher
    perplexity. We can’t merge the QLoRA adapters, while preserving the quantization,
    without a significant performance drop. QLoRA adapters are not “quantization-aware”.
  prefs: []
  type: TYPE_NORMAL
- en: Quantization-Aware Fine-tuning with QA-LoRA
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'QA-LoRA is presented in this arXiv paper:'
  prefs: []
  type: TYPE_NORMAL
- en: '[QA-LoRA: Quantization-Aware Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2309.14717)
    (Xu et al., 2023)'
  prefs: []
  type: TYPE_NORMAL
- en: This isn’t an easy paper to read. QA-LoRA is well-motivated and most of the
    results/experiments are convincing. However, understanding why it works requires
    being familiar with the mechanics behind quantization.
  prefs: []
  type: TYPE_NORMAL
- en: I won’t go deep into the mathematical theory and demonstrations here. I think
    the easiest way to understand QA-LoRA is to see it as the **process of jointly
    quantizing and fine-tuning LoRA’s parameters**. The adapter’s parameters and quantization
    parameters are both learned, and applied, during the fine-tuning process.
  prefs: []
  type: TYPE_NORMAL
- en: 'To highlight the difference with QLoRA, we can refer to this paragraph from
    the paper:'
  prefs: []
  type: TYPE_NORMAL
- en: '*We introduce group-wise operations, increasing the number of parameters of
    quantization from Dout to L×Dout, meanwhile decreasing that of adaptation from
    Din×Dint+Dint×Dout to L × Dint + Dint × Dout. As we shall see in experiments,
    a moderate L can achieve satisfying accuracy of language understanding meanwhile
    preserving computational efficiency.*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: In addition, QA-LoRA uses the standard INT4 data type while QLoRA uses NF4.
  prefs: []
  type: TYPE_NORMAL
- en: QA-LoRA Performance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let’s have a look at the performance reported by the authors of QA-LoRA. They
    report on many experiments but I think the following table is the one that gives
    the best overview of QA-LoRA performance, compared to QLoRA, and for various quantization
    precisions:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/21d796605ac260448e91c2c6283f2339.png)'
  prefs: []
  type: TYPE_IMG
- en: Table by [Xu et al. (2023)](https://arxiv.org/abs/2309.14717)
  prefs: []
  type: TYPE_NORMAL
- en: 'In this table, we have the performance of the original LLaMA 7B (16-bit) compared
    to:'
  prefs: []
  type: TYPE_NORMAL
- en: Standard QLoRA with NF4 quantized base LLM and FP16 LoRA (denoted QLoRA”)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LLaMA 7B quantized with GPTQ to INT4 (denoted “LLaMA-7B w/ GPTQ”)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Merged QLoRA adapter quantized with GTPQ (denoted “QLoRA w/ GPTQ”)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: QA-LoRA
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The standard QLoRA performs the best. This is expected since it uses a very
    good data type for quantization (NF4) while LoRA’s parameters remain FP16.
  prefs: []
  type: TYPE_NORMAL
- en: We can see that when we want to merge QLoRA adapters and then quantize the merged
    models (QLoRA w/ GPTQ), the performance significantly drops. Again, as we discussed
    in the previous section of this article, this is expected.
  prefs: []
  type: TYPE_NORMAL
- en: QA-LoRA on the other hand performs almost as well as the standard QLoRA while
    the LLM is entirely quantized with INT4\. In other words, QA-LoRA works.
  prefs: []
  type: TYPE_NORMAL
- en: QA-LoRA is also more flexible than QLoRA by allowing fine-tuning with LLMs quantized
    to the lower precisions. QA-LoRA with 3-bit precision is superior to QLoRA merged
    and quantized to 4-bit (60.1% accuracy for QA-LoRA 3-bit against 59.8% for QLoRA
    w/ GPTQ 4-bit).
  prefs: []
  type: TYPE_NORMAL
- en: Overall, QA-LoRA results look very impressive.
  prefs: []
  type: TYPE_NORMAL
- en: Overview of the Implementation of QA-LoRA
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The authors of [QA-LoRA released their implementation on GitHub](https://github.com/yuhuixu1993/qa-lora/tree/main)
    (MIT license). [Note: The original implementation is no longer available. I made
    a fork that you can find here.](https://github.com/benjamin-marie/qa-lora)'
  prefs: []
  type: TYPE_NORMAL
- en: The QA-LoRA implementation heavily relies on [AutoGPTQ](https://github.com/PanQiWei/AutoGPTQ)
    (MIT license). It exploits a specific branch of AutoGPTQ and replaces several
    functions.
  prefs: []
  type: TYPE_NORMAL
- en: The LLM that you wish to fine-tune must be already quantized with this specific
    branch of AutoGPTQ. You may try with quantized LLMs from the Hugging Face Hub
    but since AutoGPTQ often changes, these quantized LLMs might not be all compatible
    with QA-LoRA. For instance, [the Llama 2 7B that I quantized with AutoGPTQ](https://huggingface.co/kaitchup/llama-2-7b-4bit-autogptq)
    for a previous article has a quantization configuration that is not supported
    by this branch of AutoGPTQ.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you wish to see what QA-LoRA changes in AutoGPTQ, have a closer look at
    the file “[peft_utils.py](https://github.com/yuhuixu1993/qa-lora/blob/main/peft_utils.py)”
    where there is a class “GPTQLoraLinear”. The main innovation behind QA-LoRA holds
    in two lines of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Adaptation of the Code for Llama 2 Support
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: I had to patch the implementation of QA-LoRA to make it work for Llama 2.
  prefs: []
  type: TYPE_NORMAL
- en: 'If QA-LoRA still doesn’t run Llama 2 when you read this article, replace the
    file “qalora.py” with this one:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://about.benjaminmarie.com/data/py/qalora/qalora.py](https://about.benjaminmarie.com/data/py/qalora/qalora.py)'
  prefs: []
  type: TYPE_NORMAL
- en: 'I only made two modifications:'
  prefs: []
  type: TYPE_NORMAL
- en: Replace “model.config.torch_dtype=(torch.float32 if args.fp16 else (torch.bfloat16
    if args.bf16 else torch.float32))” with “model.config.torch_dtype=torch.float16”
    (line 300 of the current version)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Replace “module = module.to(torch.float32)” with “module = module.to(torch.float16)”
    (line 340 of the current version)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The current implementation only works for models using a pad token. Llama 2
    doesn’t use one. I had to manually modify the config.json of the quantized Llama
    2 to add this line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: It simply specifies the “unk_token”, whose id is 0, for padding.
  prefs: []
  type: TYPE_NORMAL
- en: Requirements for Fine-tuning Llama 2 with QA-LoRA
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'I implemented all the following sections in a notebook that you can find here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Get the notebook (#21)](https://kaitchup.substack.com/p/notebooks)'
  prefs: []
  type: TYPE_NORMAL
- en: QA-LoRA Dependencies
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: I recommend creating a sandbox, for instance using conda, before setting up
    QA-LoRA. The current implementation uses outdated versions of several packages.
  prefs: []
  type: TYPE_NORMAL
- en: AutoGPTQ must be compiled from source since we have to replace a source file
    in AutoGPTQ to add QA-LoRA support.
  prefs: []
  type: TYPE_NORMAL
- en: 'Because of this replacement, we must first clone both repositories, AutoGPTQ
    and QA-LoRA, and then replace the file in AutoGPTQ:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Apply my patch (if necessary):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'AutoGPTQ is now ready to be installed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: This can take up to 10 minutes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Install QA-LoRA dependencies:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: I also installed bitsandbytes from source, following the recommendations of
    QA-LoRA’s documentation, but I don’t think it’s necessary. You may try a “pip
    install bitsandbytes” instead (it’s much faster).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Pre-Quantized LLM
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For now, QA-LoRA only fine-tunes LLMs already quantized with AutoGPTQ (INT4,
    INT3, and INT2 are all supported). If you want to fine-tune an fp16/32 LLM, you
    would have to quantize it before with the version of AutoGPTQ that we have installed.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can also look at the end of the notebook where I wrote the code for quantization.
    For this article, I quantized Llama 2 7B and uploaded it on the Hugging Face Hub.
    You can use it to run this tutorial:'
  prefs: []
  type: TYPE_NORMAL
- en: '[kaitchup/Llama-2-7b-4bit-32g-autogptq](https://huggingface.co/kaitchup/Llama-2-7b-4bit-32g-autogptq)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Note: safetensors format is not supported yet. QA-LoRA expects to find a file
    pytorch_model.bin in the repository.*'
  prefs: []
  type: TYPE_NORMAL
- en: The group size for quantization was set to 32 (32g). I chose this group size
    because it’s hardcoded to 32 in QA-LoRA.
  prefs: []
  type: TYPE_NORMAL
- en: Hardware Requirements
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The following sections can run on the free instance of Google Colab or a GPU
    with at least 10 GB of VRAM.
  prefs: []
  type: TYPE_NORMAL
- en: QA-LoRA Fine-tuning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: I fine-tuned with the default dataset, Alpaca, for only 100 steps. With a batch
    size of 1 and gradient_accumulation_steps at 16\. It consumes around 7 GB of VRAM.
  prefs: []
  type: TYPE_NORMAL
- en: If your fine-tuning appears unstable, changing the learning rate and/or LoRA
    alpha/rank may also improve the stability.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: With the T4 GPU of Google Colab, this fine-tuning took 28 minutes.
  prefs: []
  type: TYPE_NORMAL
- en: 'I uploaded the final checkpoint on the Hugging Face Hub:'
  prefs: []
  type: TYPE_NORMAL
- en: '[kaitchup/Llama-2-7b-4bit-32g-autogptq-QALoRA](https://huggingface.co/kaitchup/Llama-2-7b-4bit-32g-autogptq-QALoRA/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Merge QA-LoRA Adapters
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In contrast with QLoRA, QA-LoRA adapters can be merged without performance loss
    into the quantized base LLM.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the code for merging:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: This code runs on the CPU so you only need enough CPU RAM to load the model
    to be merged. Note that the base LLM and the QA-LoRA adapter that we fine-tuned
    must be accessible locally.
  prefs: []
  type: TYPE_NORMAL
- en: Once merged, the model is ready for inference. It’s a standard GPTQ model.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: QA-LoRA works. We fine-tuned a quantization-aware LoRA for Llama 2.
  prefs: []
  type: TYPE_NORMAL
- en: 'We saw that quantization-aware fine-tuning has 2 significant advantages over
    QLoRA:'
  prefs: []
  type: TYPE_NORMAL
- en: It’s faster
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It fine-tunes an adapter that can be perfectly merged with the base LLM
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The current implementation is not very flexible. This is still a very young
    project. The main issue is that it relies on an older version of AutoGPTQ. The
    authors of QA-LoRA plan to support the most recent version of AutoGPTQ later.
  prefs: []
  type: TYPE_NORMAL
- en: Note that in my experiment I used a 4-bit quantization. QA-LoRA already supports
    2-bit and 3-bit quantization. You may try these lower precisions to further reduce
    memory consumption.
  prefs: []
  type: TYPE_NORMAL
- en: 'To support my work, consider subscribing to my newsletter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://kaitchup.substack.com/?source=post_page-----c7291866706c--------------------------------)
    [## The Kaitchup - AI on a Budget | Benjamin Marie, PhD | Substack'
  prefs: []
  type: TYPE_NORMAL
- en: Weekly news, tips, and tutorials on fine-tuning, running, and serving large
    language models on your computer. Each…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: kaitchup.substack.com](https://kaitchup.substack.com/?source=post_page-----c7291866706c--------------------------------)
  prefs: []
  type: TYPE_NORMAL
