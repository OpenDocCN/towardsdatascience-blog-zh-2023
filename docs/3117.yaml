- en: 'QA-LoRA: Fine-Tune a Quantized Large Language Model on Your GPU'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: QA-LoRA：在你的GPU上微调量化的大型语言模型
- en: 原文：[https://towardsdatascience.com/qa-lora-fine-tune-a-quantized-large-language-model-on-your-gpu-c7291866706c?source=collection_archive---------0-----------------------#2023-10-14](https://towardsdatascience.com/qa-lora-fine-tune-a-quantized-large-language-model-on-your-gpu-c7291866706c?source=collection_archive---------0-----------------------#2023-10-14)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/qa-lora-fine-tune-a-quantized-large-language-model-on-your-gpu-c7291866706c?source=collection_archive---------0-----------------------#2023-10-14](https://towardsdatascience.com/qa-lora-fine-tune-a-quantized-large-language-model-on-your-gpu-c7291866706c?source=collection_archive---------0-----------------------#2023-10-14)
- en: Quantization-aware fine-tuning
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 量化感知微调
- en: '[](https://medium.com/@bnjmn_marie?source=post_page-----c7291866706c--------------------------------)[![Benjamin
    Marie](../Images/3ea1ad230cb1e67610418a8e36a5e5dd.png)](https://medium.com/@bnjmn_marie?source=post_page-----c7291866706c--------------------------------)[](https://towardsdatascience.com/?source=post_page-----c7291866706c--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----c7291866706c--------------------------------)
    [Benjamin Marie](https://medium.com/@bnjmn_marie?source=post_page-----c7291866706c--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@bnjmn_marie?source=post_page-----c7291866706c--------------------------------)[![Benjamin
    Marie](../Images/3ea1ad230cb1e67610418a8e36a5e5dd.png)](https://medium.com/@bnjmn_marie?source=post_page-----c7291866706c--------------------------------)[](https://towardsdatascience.com/?source=post_page-----c7291866706c--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----c7291866706c--------------------------------)
    [Benjamin Marie](https://medium.com/@bnjmn_marie?source=post_page-----c7291866706c--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fad2a414578b3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fqa-lora-fine-tune-a-quantized-large-language-model-on-your-gpu-c7291866706c&user=Benjamin+Marie&userId=ad2a414578b3&source=post_page-ad2a414578b3----c7291866706c---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----c7291866706c--------------------------------)
    ·10 min read·Oct 14, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fc7291866706c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fqa-lora-fine-tune-a-quantized-large-language-model-on-your-gpu-c7291866706c&user=Benjamin+Marie&userId=ad2a414578b3&source=-----c7291866706c---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fad2a414578b3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fqa-lora-fine-tune-a-quantized-large-language-model-on-your-gpu-c7291866706c&user=Benjamin+Marie&userId=ad2a414578b3&source=post_page-ad2a414578b3----c7291866706c---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----c7291866706c--------------------------------)
    · 10分钟阅读 · 2023年10月14日 [](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fc7291866706c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fqa-lora-fine-tune-a-quantized-large-language-model-on-your-gpu-c7291866706c&user=Benjamin+Marie&userId=ad2a414578b3&source=-----c7291866706c---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc7291866706c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fqa-lora-fine-tune-a-quantized-large-language-model-on-your-gpu-c7291866706c&source=-----c7291866706c---------------------bookmark_footer-----------)![](../Images/eb5cbf8008d012e797a4d1e66f9ca0c6.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc7291866706c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fqa-lora-fine-tune-a-quantized-large-language-model-on-your-gpu-c7291866706c&source=-----c7291866706c---------------------bookmark_footer-----------)![](../Images/eb5cbf8008d012e797a4d1e66f9ca0c6.png)'
- en: Illustration by the author — Made with images from Pixabay ([1](https://pixabay.com/vectors/llama-alpaca-animal-mammal-zoo-297668/),[2](https://pixabay.com/vectors/skateboard-skating-skate-silhouette-7192315/))
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 作者插图 — 使用了来自Pixabay的图像 ([1](https://pixabay.com/vectors/llama-alpaca-animal-mammal-zoo-297668/),[2](https://pixabay.com/vectors/skateboard-skating-skate-silhouette-7192315/))
- en: State-of-the-art large language models (LLMs) are pre-trained with billions
    of parameters. While pre-trained LLMs can perform many tasks, they can become
    much better once fine-tuned.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 最先进的大型语言模型（LLMs）经过数十亿参数的预训练。虽然预训练的LLMs可以执行许多任务，但经过微调后，它们的性能会大幅提升。
- en: Thanks to LoRA, fine-tuning costs can be dramatically reduced. LoRA adds low-rank
    tensors, i.e., a small number of parameters (millions), on top of the frozen original
    parameters. Only the parameters in the added tensors are trained during fine-tuning.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: LoRA still requires the model to be loaded in memory. To reduce the memory cost
    and speed-up fine-tuning, a new approach proposes quantization-aware LoRA (QA-LoRA)
    fine-tuning.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: In this article, I explain QA-LoRA and review its performance compared with
    previous work (especially QLoRA). I also show how to use QA-LoRA to fine-tune
    your own quantization-aware LoRA for Llama 2.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: What’s Wrong with QLoRA?
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Fine-tuning LoRA on top of a quantized LLM is something that can already be
    done with QLoRA. In my previous articles, I used it many times to fine-tune LLMs,
    for instance, Llama 2 and GPT-NeoX, on my desktop computer or using the free instance
    of Google Colab.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://kaitchup.substack.com/p/fine-tune-llama-2-on-your-computer?source=post_page-----c7291866706c--------------------------------)
    [## Fine-tune Llama 2 on Your Computer with QLoRa and TRL'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: On Guanaco and with the correct padding
  id: totrans-16
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: kaitchup.substack.com](https://kaitchup.substack.com/p/fine-tune-llama-2-on-your-computer?source=post_page-----c7291866706c--------------------------------)
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: Before delving into QA-LoRA, it is interesting to understand what are the current
    limits of QLoRA.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: The NormalFloat4 (NF4) Quantization
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: LLM quantization algorithms usually quantize parameters to a 4-bit precision
    using the INT4 data type. Computation with this data type is more and more optimized
    with recent GPUs.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: QLoRA doesn’t use INT4 by default but another data type called NormalFloat4
    (NF4). You can see it as a compressed float number. According to the authors of
    [QLoRA](https://arxiv.org/abs/2305.14314), NF4 is superior to INT4\. LLMs quantized
    with NF4 achieve a lower perplexity.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: However, NF4 computation is not optimal for fast inference. This is one of the
    reasons why models quantized with GPTQ are faster than models quantized with bitsandbytes
    NF4\. In previous articles, I confirmed that GPTQ models are indeed faster.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://kaitchup.substack.com/p/gptq-or-bitsandbytes-which-quantization?source=post_page-----c7291866706c--------------------------------)
    [## GPTQ or bitsandbytes: Which Quantization Method to Use for LLMs - Examples
    with Llama 2'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: Large language model quantization for affordable fine-tuning and inference on
    your computer
  id: totrans-24
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: kaitchup.substack.com](https://kaitchup.substack.com/p/gptq-or-bitsandbytes-which-quantization?source=post_page-----c7291866706c--------------------------------)
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: NF4 is also one of the weaknesses pointed out by the authors of [QA-LoRA](https://arxiv.org/abs/2309.14717).
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: NF4 Base Model, But FP16 LoRA
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: While the base model is quantized with NF4, the trained LoRA’s parameters remain
    at a higher precision which is usually FP16, as illustrated in the figure below.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c43d096218c9bbe4f6315cd7d0b00846.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
- en: Figure by the author
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 图由作者提供
- en: This is key in the QLoRA performance as naively training quantized parameters
    would lead to poor performance.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 这对于 QLoRA 性能至关重要，因为天真地训练量化参数会导致性能较差。
- en: 'Consequently, for inference, we have two different ways to use the LoRA adapters
    trained with QLoRA:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在推理时，我们有两种不同的方法来使用用 QLoRA 训练的 LoRA 适配器：
- en: Loading them on top of the base LLMs as we do during QLoRA fine-tuning
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将它们加载到基础 LLM 之上，就像我们在 QLoRA 微调期间所做的那样
- en: Merging them with the base LLMs
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将它们与基础 LLM 合并
- en: Loading them is optimal to preserve the performance. We keep LoRA’s parameters
    at 16-bit precision but, since they are only a few million, they don’t consume
    much VRAM relative to the quantized base LLMs.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 加载它们是保持性能的最佳方式。我们将 LoRA 的参数保持在 16 位精度，但由于它们仅有几百万个参数，相对于量化的基础 LLM，它们并不会消耗太多 VRAM。
- en: The other alternative is to merge the LoRA’s parameters with the base model.
    I explored [several merging recipes in a previous article](https://kaitchup.substack.com/p/lora-adapters-when-a-naive-merge).
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种选择是将 LoRA 参数与基础模型合并。我在[上一篇文章中探索了几种合并方案](https://kaitchup.substack.com/p/lora-adapters-when-a-naive-merge)。
- en: Ideally, we have to dequantize the base model to the same precision used by
    LoRA’s parameters, and then merge LoRA’s parameters with the dequantized base
    model.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 理想情况下，我们需要将基础模型解量化到 LoRA 参数所使用的相同精度，然后将 LoRA 参数与解量化的基础模型合并。
- en: But then, as a result, the merged model is not quantized anymore (FP16). It’s
    a big dequantized model. We could quantize the entire merged model but quantization
    always loses information. We would obtain a model performing below the performance
    we originally obtained at the end of the QLoRA fine-tuning.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 但结果是，合并模型不再被量化（FP16）。它是一个大规模解量化模型。我们可以对整个合并模型进行量化，但量化总是会丢失信息。我们将得到一个性能低于原始 QLoRA
    微调结束时的性能的模型。
- en: 'Here are the results I obtained [for different configurations](https://kaitchup.substack.com/p/lora-adapters-when-a-naive-merge):'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我获得的[不同配置的结果](https://kaitchup.substack.com/p/lora-adapters-when-a-naive-merge)：
- en: '![](../Images/d7606d20e92226d15ea81fa36fb68d5b.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d7606d20e92226d15ea81fa36fb68d5b.png)'
- en: Table and results by the author
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 表格和作者提供的结果
- en: '*Note: Lower perplexity is better.*'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '*注意：较低的困惑度更好。*'
- en: We can see that quantizing the merged model leads to a significantly higher
    perplexity. We can’t merge the QLoRA adapters, while preserving the quantization,
    without a significant performance drop. QLoRA adapters are not “quantization-aware”.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，量化合并模型会导致显著更高的困惑度。我们不能在不显著降低性能的情况下合并 QLoRA 适配器，同时保持量化。QLoRA 适配器不是“量化感知”的。
- en: Quantization-Aware Fine-tuning with QA-LoRA
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 QA-LoRA 的量化感知微调
- en: 'QA-LoRA is presented in this arXiv paper:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: QA-LoRA 在这篇 arXiv 论文中提出：
- en: '[QA-LoRA: Quantization-Aware Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2309.14717)
    (Xu et al., 2023)'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '[QA-LoRA: 量化感知的大型语言模型低秩适应](https://arxiv.org/abs/2309.14717)（Xu 等人，2023）'
- en: This isn’t an easy paper to read. QA-LoRA is well-motivated and most of the
    results/experiments are convincing. However, understanding why it works requires
    being familiar with the mechanics behind quantization.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 这篇论文不容易阅读。QA-LoRA 动机充分，大部分结果/实验令人信服。然而，理解其工作原理需要对量化背后的机制有所了解。
- en: I won’t go deep into the mathematical theory and demonstrations here. I think
    the easiest way to understand QA-LoRA is to see it as the **process of jointly
    quantizing and fine-tuning LoRA’s parameters**. The adapter’s parameters and quantization
    parameters are both learned, and applied, during the fine-tuning process.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 我不会深入探讨数学理论和证明。我认为理解 QA-LoRA 最简单的方法是将其视为**联合量化和微调 LoRA 参数的过程**。适配器的参数和量化参数都是在微调过程中学习和应用的。
- en: 'To highlight the difference with QLoRA, we can refer to this paragraph from
    the paper:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 为了突出与 QLoRA 的区别，我们可以参考论文中的这一段：
- en: '*We introduce group-wise operations, increasing the number of parameters of
    quantization from Dout to L×Dout, meanwhile decreasing that of adaptation from
    Din×Dint+Dint×Dout to L × Dint + Dint × Dout. As we shall see in experiments,
    a moderate L can achieve satisfying accuracy of language understanding meanwhile
    preserving computational efficiency.*'
  id: totrans-50
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*我们引入了分组操作，将量化的参数数量从 Dout 增加到 L×Dout，同时将适应的参数数量从 Din×Dint+Dint×Dout 减少到 L ×
    Dint + Dint × Dout。正如我们在实验中将看到的，适度的 L 可以在保持计算效率的同时实现令人满意的语言理解准确性。*'
- en: In addition, QA-LoRA uses the standard INT4 data type while QLoRA uses NF4.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，QA-LoRA 使用标准的 INT4 数据类型，而 QLoRA 使用 NF4。
- en: QA-LoRA Performance
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: QA-LoRA 性能
- en: 'Let’s have a look at the performance reported by the authors of QA-LoRA. They
    report on many experiments but I think the following table is the one that gives
    the best overview of QA-LoRA performance, compared to QLoRA, and for various quantization
    precisions:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/21d796605ac260448e91c2c6283f2339.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
- en: Table by [Xu et al. (2023)](https://arxiv.org/abs/2309.14717)
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: 'In this table, we have the performance of the original LLaMA 7B (16-bit) compared
    to:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: Standard QLoRA with NF4 quantized base LLM and FP16 LoRA (denoted QLoRA”)
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LLaMA 7B quantized with GPTQ to INT4 (denoted “LLaMA-7B w/ GPTQ”)
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Merged QLoRA adapter quantized with GTPQ (denoted “QLoRA w/ GPTQ”)
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: QA-LoRA
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The standard QLoRA performs the best. This is expected since it uses a very
    good data type for quantization (NF4) while LoRA’s parameters remain FP16.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: We can see that when we want to merge QLoRA adapters and then quantize the merged
    models (QLoRA w/ GPTQ), the performance significantly drops. Again, as we discussed
    in the previous section of this article, this is expected.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: QA-LoRA on the other hand performs almost as well as the standard QLoRA while
    the LLM is entirely quantized with INT4\. In other words, QA-LoRA works.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: QA-LoRA is also more flexible than QLoRA by allowing fine-tuning with LLMs quantized
    to the lower precisions. QA-LoRA with 3-bit precision is superior to QLoRA merged
    and quantized to 4-bit (60.1% accuracy for QA-LoRA 3-bit against 59.8% for QLoRA
    w/ GPTQ 4-bit).
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: Overall, QA-LoRA results look very impressive.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: Overview of the Implementation of QA-LoRA
  id: totrans-66
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The authors of [QA-LoRA released their implementation on GitHub](https://github.com/yuhuixu1993/qa-lora/tree/main)
    (MIT license). [Note: The original implementation is no longer available. I made
    a fork that you can find here.](https://github.com/benjamin-marie/qa-lora)'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: The QA-LoRA implementation heavily relies on [AutoGPTQ](https://github.com/PanQiWei/AutoGPTQ)
    (MIT license). It exploits a specific branch of AutoGPTQ and replaces several
    functions.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: The LLM that you wish to fine-tune must be already quantized with this specific
    branch of AutoGPTQ. You may try with quantized LLMs from the Hugging Face Hub
    but since AutoGPTQ often changes, these quantized LLMs might not be all compatible
    with QA-LoRA. For instance, [the Llama 2 7B that I quantized with AutoGPTQ](https://huggingface.co/kaitchup/llama-2-7b-4bit-autogptq)
    for a previous article has a quantization configuration that is not supported
    by this branch of AutoGPTQ.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: 'If you wish to see what QA-LoRA changes in AutoGPTQ, have a closer look at
    the file “[peft_utils.py](https://github.com/yuhuixu1993/qa-lora/blob/main/peft_utils.py)”
    where there is a class “GPTQLoraLinear”. The main innovation behind QA-LoRA holds
    in two lines of code:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Adaptation of the Code for Llama 2 Support
  id: totrans-72
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: I had to patch the implementation of QA-LoRA to make it work for Llama 2.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
- en: 'If QA-LoRA still doesn’t run Llama 2 when you read this article, replace the
    file “qalora.py” with this one:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: '[https://about.benjaminmarie.com/data/py/qalora/qalora.py](https://about.benjaminmarie.com/data/py/qalora/qalora.py)'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: 'I only made two modifications:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: Replace “model.config.torch_dtype=(torch.float32 if args.fp16 else (torch.bfloat16
    if args.bf16 else torch.float32))” with “model.config.torch_dtype=torch.float16”
    (line 300 of the current version)
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Replace “module = module.to(torch.float32)” with “module = module.to(torch.float16)”
    (line 340 of the current version)
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The current implementation only works for models using a pad token. Llama 2
    doesn’t use one. I had to manually modify the config.json of the quantized Llama
    2 to add this line:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: It simply specifies the “unk_token”, whose id is 0, for padding.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: Requirements for Fine-tuning Llama 2 with QA-LoRA
  id: totrans-82
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'I implemented all the following sections in a notebook that you can find here:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: '[Get the notebook (#21)](https://kaitchup.substack.com/p/notebooks)'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: QA-LoRA Dependencies
  id: totrans-85
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: I recommend creating a sandbox, for instance using conda, before setting up
    QA-LoRA. The current implementation uses outdated versions of several packages.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
- en: AutoGPTQ must be compiled from source since we have to replace a source file
    in AutoGPTQ to add QA-LoRA support.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
- en: 'Because of this replacement, we must first clone both repositories, AutoGPTQ
    and QA-LoRA, and then replace the file in AutoGPTQ:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Apply my patch (if necessary):'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'AutoGPTQ is now ready to be installed:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: This can take up to 10 minutes.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: 'Install QA-LoRA dependencies:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: I also installed bitsandbytes from source, following the recommendations of
    QA-LoRA’s documentation, but I don’t think it’s necessary. You may try a “pip
    install bitsandbytes” instead (it’s much faster).
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Pre-Quantized LLM
  id: totrans-99
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For now, QA-LoRA only fine-tunes LLMs already quantized with AutoGPTQ (INT4,
    INT3, and INT2 are all supported). If you want to fine-tune an fp16/32 LLM, you
    would have to quantize it before with the version of AutoGPTQ that we have installed.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: 'You can also look at the end of the notebook where I wrote the code for quantization.
    For this article, I quantized Llama 2 7B and uploaded it on the Hugging Face Hub.
    You can use it to run this tutorial:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: '[kaitchup/Llama-2-7b-4bit-32g-autogptq](https://huggingface.co/kaitchup/Llama-2-7b-4bit-32g-autogptq)'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Note: safetensors format is not supported yet. QA-LoRA expects to find a file
    pytorch_model.bin in the repository.*'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: The group size for quantization was set to 32 (32g). I chose this group size
    because it’s hardcoded to 32 in QA-LoRA.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: Hardware Requirements
  id: totrans-105
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The following sections can run on the free instance of Google Colab or a GPU
    with at least 10 GB of VRAM.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: QA-LoRA Fine-tuning
  id: totrans-107
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: I fine-tuned with the default dataset, Alpaca, for only 100 steps. With a batch
    size of 1 and gradient_accumulation_steps at 16\. It consumes around 7 GB of VRAM.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: If your fine-tuning appears unstable, changing the learning rate and/or LoRA
    alpha/rank may also improve the stability.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: With the T4 GPU of Google Colab, this fine-tuning took 28 minutes.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: 'I uploaded the final checkpoint on the Hugging Face Hub:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 我在Hugging Face Hub上上传了最终的检查点：
- en: '[kaitchup/Llama-2-7b-4bit-32g-autogptq-QALoRA](https://huggingface.co/kaitchup/Llama-2-7b-4bit-32g-autogptq-QALoRA/)'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[kaitchup/Llama-2-7b-4bit-32g-autogptq-QALoRA](https://huggingface.co/kaitchup/Llama-2-7b-4bit-32g-autogptq-QALoRA/)'
- en: Merge QA-LoRA Adapters
  id: totrans-114
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 合并QA-LoRA适配器
- en: In contrast with QLoRA, QA-LoRA adapters can be merged without performance loss
    into the quantized base LLM.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 与QLoRA相比，QA-LoRA适配器可以在不损失性能的情况下与量化基础LLM合并。
- en: 'Here is the code for merging:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是合并代码：
- en: '[PRE8]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: This code runs on the CPU so you only need enough CPU RAM to load the model
    to be merged. Note that the base LLM and the QA-LoRA adapter that we fine-tuned
    must be accessible locally.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 该代码在CPU上运行，因此你只需要足够的CPU RAM来加载要合并的模型。注意基础LLM和我们微调的QA-LoRA适配器必须在本地可访问。
- en: Once merged, the model is ready for inference. It’s a standard GPTQ model.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦合并，模型就准备好进行推理。它是一个标准的GPTQ模型。
- en: Conclusion
  id: totrans-120
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: QA-LoRA works. We fine-tuned a quantization-aware LoRA for Llama 2.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: QA-LoRA有效。我们对Llama 2进行了量化感知的LoRA微调。
- en: 'We saw that quantization-aware fine-tuning has 2 significant advantages over
    QLoRA:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 我们发现量化感知微调相较于QLoRA有两个显著的优势：
- en: It’s faster
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这更快
- en: It fine-tunes an adapter that can be perfectly merged with the base LLM
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它微调了一个可以与基础LLM完美合并的适配器。
- en: The current implementation is not very flexible. This is still a very young
    project. The main issue is that it relies on an older version of AutoGPTQ. The
    authors of QA-LoRA plan to support the most recent version of AutoGPTQ later.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 当前的实现还不够灵活。这仍然是一个非常年轻的项目。主要问题在于它依赖于旧版本的AutoGPTQ。QA-LoRA的作者计划稍后支持最新版本的AutoGPTQ。
- en: Note that in my experiment I used a 4-bit quantization. QA-LoRA already supports
    2-bit and 3-bit quantization. You may try these lower precisions to further reduce
    memory consumption.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 注意在我的实验中使用了4-bit量化。QA-LoRA已支持2-bit和3-bit量化。你可以尝试这些更低的精度以进一步减少内存消耗。
- en: 'To support my work, consider subscribing to my newsletter:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 为了支持我的工作，请考虑订阅我的通讯：
- en: '[](https://kaitchup.substack.com/?source=post_page-----c7291866706c--------------------------------)
    [## The Kaitchup - AI on a Budget | Benjamin Marie, PhD | Substack'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://kaitchup.substack.com/?source=post_page-----c7291866706c--------------------------------)
    [## Kaitchup - 预算内的AI | 本杰明·玛丽，博士 | Substack]'
- en: Weekly news, tips, and tutorials on fine-tuning, running, and serving large
    language models on your computer. Each…
  id: totrans-129
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 每周的新闻、提示和有关在你的计算机上微调、运行和服务大型语言模型的教程。每个...
- en: kaitchup.substack.com](https://kaitchup.substack.com/?source=post_page-----c7291866706c--------------------------------)
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '[kaitchup.substack.com](https://kaitchup.substack.com/?source=post_page-----c7291866706c--------------------------------)'
