["```py\n# Convert dataframe to a list of dict for Pinecone data upsert\ndata = df_item.to_dict('records')\n```", "```py\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\n\ntext_splitter = RecursiveCharacterTextSplitter(\n    chunk_size=400,\n    chunk_overlap=20,\n    length_function=tiktoken_len,\n    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n)\n```", "```py\nimport pinecone\nfrom langchain.embeddings.openai import OpenAIEmbeddings\n\n# 0\\. Initialize Pinecone Client\nwith open('./credentials.yml', 'r') as file:\n    cre = yaml.safe_load(file)\n    # pinecone API\n    pinecone_api_key = cre['pinecone']['apikey']\n\npinecone.init(api_key=pinecone_api_key, environment=\"us-west1-gcp\")\n\n# 1\\. Create a new index\nindex_name = 'outside-chatgpt'\n\n# 2\\. Use OpenAI's ada-002 as embedding model\nmodel_name = 'text-embedding-ada-002'\nembed = OpenAIEmbeddings(\n    document_model_name=model_name,\n    query_model_name=model_name,\n    openai_api_key=OPENAI_API_KEY\n)\nembed_dimension = 1536\n\n# 3\\. check if index already exists (it shouldn't if this is first time)\nif index_name not in pinecone.list_indexes():\n    # if does not exist, create index\n    pinecone.create_index(\n        name=index_name,\n        metric='cosine',\n        dimension=embed_dimension\n    )\n\n# 3\\. Connect to index\nindex = pinecone.Index(index_name)\n```", "```py\n# If using terminal\nfrom tqdm.auto import tqdm\n\n# If using in Jupyter notebook\nfrom tqdm.autonotebook import tqdm\n\nfrom uuid import uuid4\n\nbatch_limit = 100\n\ntexts = []\nmetadatas = []\n\nfor i, record in enumerate(tqdm(data)):\n    # 1\\. Get metadata fields for this record\n    metadata = {\n        'item_uuid': str(record['id']),\n        'source': record['url'],\n        'title': record['title']\n    }\n    # 2\\. Create chunks from the record text\n    record_texts = text_splitter.split_text(record['content'])\n\n    # 3\\. Create individual metadata dicts for each chunk\n    record_metadatas = [{\n        \"chunk\": j, \"text\": text, **metadata\n    } for j, text in enumerate(record_texts)]\n\n    # 4\\. Append these to current batches\n    texts.extend(record_texts)\n    metadatas.extend(record_metadatas)\n\n    # 5\\. Special case: if we have reached the batch_limit we can add texts\n    if len(texts) >= batch_limit:\n        ids = [str(uuid4()) for _ in range(len(texts))]\n        embeds = embed.embed_documents(texts)\n        index.upsert(vectors=zip(ids, embeds, metadatas))\n        texts = []\n        metadatas = []\n```", "```py\nfrom langchain.vectorstores import Pinecone\nfrom langchain.chains import VectorDBQAWithSourcesChain\nfrom langchain.embeddings.openai import OpenAIEmbeddings\n\n# 1\\. Specify Pinecone as Vectorstore\n# =======================================\n# 1.1 get pinecone index name\nindex = pinecone.Index(index_name) #'outside-chatgpt'\n\n# 1.2 specify embedding model\nmodel_name = 'text-embedding-ada-002'\nembed = OpenAIEmbeddings(\n    document_model_name=model_name,\n    query_model_name=model_name,\n    openai_api_key=OPENAI_API_KEY\n)\n\n# 1.3 provides text_field\ntext_field = \"text\"\n\nvectorstore = Pinecone(\n    index, embed.embed_query, text_field\n)\n\n# 2\\. Wrap the chain as a function\nqa_with_sources = VectorDBQAWithSourcesChain.from_chain_type(\n    llm=llm,\n    chain_type=\"stuff\",\n    vectorstore=vectorstore\n)\n```", "```py\nimport pinecone\nimport streamlit as st\nfrom langchain.chains import VectorDBQAWithSourcesChain\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.vectorstores import Pinecone\nfrom langchain.embeddings.openai import OpenAIEmbeddings\n\n# ------OpenAI: LLM---------------\nOPENAI_API_KEY = st.secrets[\"OPENAI_KEY\"]\nllm = ChatOpenAI(\n    openai_api_key=OPENAI_API_KEY,\n    model_name='gpt-3.5-turbo',\n    temperature=0.0\n)\n\n# ------OpenAI: Embed model-------------\nmodel_name = 'text-embedding-ada-002'\nembed = OpenAIEmbeddings(\n    document_model_name=model_name,\n    query_model_name=model_name,\n    openai_api_key=OPENAI_API_KEY\n)\n\n# --- Pinecone ------\npinecone_api_key = st.secrets[\"PINECONE_API_KEY\"]\npinecone.init(api_key=pinecone_api_key, environment=\"us-west1-gcp\")\nindex_name = \"outside-chatgpt\"\nindex = pinecone.Index(index_name)\ntext_field = \"text\"\nvectorstore = Pinecone(index, embed.embed_query, text_field)\n\n#  ======= Langchain ChatDBQA with source chain =======\ndef qa_with_sources(query):\n    qa = VectorDBQAWithSourcesChain.from_chain_type(\n        llm=llm,\n        chain_type=\"stuff\",\n        vectorstore=vectorstore\n    )\n\n    response = qa(query)\n    return response \n```", "```py\n import os\nimport openai\nfrom PIL import Image\nfrom streamlit_chat import message\nfrom utils import *\n\nopenai.api_key = st.secrets[\"OPENAI_KEY\"]\n# For Langchain\nos.environ[\"OPENAI_API_KEY\"] = openai.api_key\n\n# ==== Section 1: Streamlit Settings ======\nwith st.sidebar:\n    st.markdown(\"# Welcome to chatOutside 🙌\")\n    st.markdown(\n        \"**chatOutside** allows you to talk to version of **chatGPT** \\n\"\n        \"that has access to latest Outside content!  \\n\"\n        )\n    st.markdown(\n        \"Unlike chatGPT, chatOutside can't make stuff up\\n\"\n        \"and will answer from Outside knowledge base. \\n\"\n    )\n    st.markdown(\"👩‍🏫 Developer: Wen Yang\")\n    st.markdown(\"---\")\n    st.markdown(\"# Under The Hood 🎩 🐇\")\n    st.markdown(\"How to Prevent Large Language Model (LLM) hallucination?\")\n    st.markdown(\"- **Pinecone**: vector database for Outside knowledge\")\n    st.markdown(\"- **Langchain**: to remember the context of the conversation\")\n\n# Homepage title\nst.title(\"chatOutside: Outside + ChatGPT\")\n# Hero Image\nimage = Image.open('VideoBkg_08.jpg')\nst.image(image, caption='Get Outside!')\n\nst.header(\"chatGPT 🤖\")\n\n# ====== Section 2: ChatGPT only ======\ndef chatgpt(prompt):\n    res = openai.ChatCompletion.create(\n        model='gpt-3.5-turbo',\n        messages=[\n            {\"role\": \"system\",\n             \"content\": \"You are a friendly and helpful assistant. \"\n                        \"Answer the question as truthfully as possible. \"\n                        \"If unsure, say you don't know.\"},\n            {\"role\": \"user\", \"content\": prompt},\n        ],\n        temperature=0,\n    )[\"choices\"][0][\"message\"][\"content\"]\n\n    return res\n\ninput_gpt = st.text_input(label='Chat here! 💬')\noutput_gpt = st.text_area(label=\"Answered by chatGPT:\",\n                          value=chatgpt(input_gpt), height=200)\n# ========= End of Section 2 ===========\n\n# ========== Section 3: chatOutside ============================\nst.header(\"chatOutside 🏕️\")\n\ndef chatoutside(query):\n    # start chat with chatOutside\n    try:\n        response = qa_with_sources(query)\n        answer = response['answer']\n        source = response['sources']\n\n    except Exception as e:\n        print(\"I'm afraid your question failed! This is the error: \")\n        print(e)\n        return None\n\n    if len(answer) > 0:\n        return answer, source\n\n    else:\n        return None\n# ============================================================\n\n# ========== Section 4\\. Display ChatOutside in chatbot style ===========\nif 'generated' not in st.session_state:\n    st.session_state['generated'] = []\n\nif 'past' not in st.session_state:\n    st.session_state['past'] = []\n\nif 'source' not in st.session_state:\n    st.session_state['source'] = []\n\ndef clear_text():\n    st.session_state[\"input\"] = \"\"\n\n# We will get the user's input by calling the get_text function\ndef get_text():\n    input_text = st.text_input('Chat here! 💬', key=\"input\")\n    return input_text\n\nuser_input = get_text()\n\nif user_input:\n    # source contain urls from Outside\n    output, source = chatoutside(user_input)\n\n    # store the output\n    st.session_state.past.append(user_input)\n    st.session_state.generated.append(output)\n    st.session_state.source.append(source)\n\n    # Display source urls\n    st.write(source)\n\nif st.session_state['generated']:\n    for i in range(len(st.session_state['generated'])-1, -1, -1):\n        message(st.session_state[\"generated\"][i],  key=str(i))\n        message(st.session_state['past'][i], is_user=True,\n                avatar_style=\"big-ears\",  key=str(i) + '_user')\n```"]