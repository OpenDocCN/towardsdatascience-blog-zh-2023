["```py\nmodel_path = \"your/model/path/\"\n\n# instantiate your model\nmodel = XRayClassifier() \n\n# load your model. Here we're loading on CPU since we're not going to do \n# large amounts of inference\nmodel.load_state_dict(torch.load(model_path, map_location=torch.device('cpu'))) \n\n# put it in evaluation mode for inference\nmodel.eval()\n```", "```py\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n# hyperparameters\nnc = 3 # number of channels\nnf = 64 # number of features to begin with\ndropout = 0.2\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# setup a resnet block and its forward function\nclass ResNetBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, stride=1):\n        super(ResNetBlock, self).__init__()\n        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(out_channels)\n        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(out_channels)\n\n        self.shortcut = nn.Sequential()\n        if stride != 1 or in_channels != out_channels:\n            self.shortcut = nn.Sequential(\n                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(out_channels)\n            )\n\n    def forward(self, x):\n        out = F.relu(self.bn1(self.conv1(x)))\n        out = self.bn2(self.conv2(out))\n        out += self.shortcut(x)\n        out = F.relu(out)\n        return out\n\n# setup the final model structure\nclass XRayClassifier(nn.Module):\n    def __init__(self, nc=nc, nf=nf, dropout=dropout):\n        super(XRayClassifier, self).__init__()\n\n        self.resnet_blocks = nn.Sequential(\n            ResNetBlock(nc,   nf,    stride=2), # (B, C, H, W) -> (B, NF, H/2, W/2), i.e., (64,64,128,128)\n            ResNetBlock(nf,   nf*2,  stride=2), # (64,128,64,64)\n            ResNetBlock(nf*2, nf*4,  stride=2), # (64,256,32,32)\n            ResNetBlock(nf*4, nf*8,  stride=2), # (64,512,16,16)\n            ResNetBlock(nf*8, nf*16, stride=2), # (64,1024,8,8)\n        )\n\n        self.classifier = nn.Sequential(\n            nn.Conv2d(nf*16, 1, 8, 1, 0, bias=False),\n            nn.Dropout(p=dropout),\n            nn.Sigmoid(),\n        )\n\n    def forward(self, input):\n        output = self.resnet_blocks(input.to(device))\n        output = self.classifier(output)\n        return output\n```", "```py\nXRayClassifier(\n  (resnet_blocks): Sequential(\n    (0): ResNetBlock(\n      (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (shortcut): Sequential(\n        (0): Conv2d(3, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): ResNetBlock(\n      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (shortcut): Sequential(\n        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (2): ResNetBlock(\n      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (shortcut): Sequential(\n        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (3): ResNetBlock(\n      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (shortcut): Sequential(\n        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    **(4): ResNetBlock(\n      (conv1): Conv2d(512, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (shortcut): Sequential(\n        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )**\n  )\n  (classifier): Sequential(\n    (0): Conv2d(1024, 1, kernel_size=(8, 8), stride=(1, 1), bias=False)\n    (1): Dropout(p=0.2, inplace=False)\n    (2): Sigmoid()\n  )\n)\n```", "```py\nmodel.resnet_blocks[-1]\n#ResNetBlock(\n#  (conv1): Conv2d(512, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n#  (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n#  (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n#  (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n#  (shortcut): Sequential(\n#    (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n#    (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n#  )\n#)\n```", "```py\nhook(module, grad_input, grad_output) -> tuple(Tensor) or None\n```", "```py\nhook(module, args, output) -> None or modified output\n```", "```py\n# defines two global scope variables to store our gradients and activations\ngradients = None\nactivations = None\n\ndef backward_hook(module, grad_input, grad_output):\n  global gradients # refers to the variable in the global scope\n  print('Backward hook running...')\n  gradients = grad_output\n  # In this case, we expect it to be torch.Size([batch size, 1024, 8, 8])\n  print(f'Gradients size: {gradients[0].size()}') \n  # We need the 0 index because the tensor containing the gradients comes\n  # inside a one element tuple.\n\ndef forward_hook(module, args, output):\n  global activations # refers to the variable in the global scope\n  print('Forward hook running...')\n  activations = output\n  # In this case, we expect it to be torch.Size([batch size, 1024, 8, 8])\n  print(f'Activations size: {activations.size()}')\n```", "```py\nbackward_hook = model.resnet_blocks[-1].register_full_backward_hook(backward_hook, prepend=False)\nforward_hook = model.resnet_blocks[-1].register_forward_hook(forward_hook, prepend=False)\n```", "```py\nfrom PIL import Image\n\nimg_path = \"/your/image/path/\"\nimage = Image.open(img_path).convert('RGB')\n```", "```py\nfrom torchvision import transforms\nfrom torchvision.transforms import ToTensor\n\nimage_size = 256\ntransform = transforms.Compose([\n                               transforms.Resize(image_size, antialias=True),\n                               transforms.CenterCrop(image_size),\n                               transforms.ToTensor(),\n                               transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n                           ])\n\nimg_tensor = transform(image) # stores the tensor that represents the image\n```", "```py\n# since we're feeding only one image, it is a 3d tensor (3, 256, 256). \n# we need to unsqueeze to it has 4 dimensions (1, 3, 256, 256) as \n# the model expects it to.\nmodel(img_tensor.unsqueeze(0)).backward()\n# here we did the forward and the backward pass in one line.\n```", "```py\nForward hook running...\nActivations size: torch.Size([1, 1024, 8, 8])\nBackward hook running...\nGradients size: torch.Size([1, 1024, 8, 8])\n```", "```py\n# pool the gradients across the channels\npooled_gradients = torch.mean(gradients[0], dim=[0, 2, 3])\n```", "```py\nimport torch.nn.functional as F\nimport matplotlib.pyplot as plt\n\n# weight the channels by corresponding gradients\nfor i in range(activations.size()[1]):\n    activations[:, i, :, :] *= pooled_gradients[i]\n\n# average the channels of the activations\nheatmap = torch.mean(activations, dim=1).squeeze()\n\n# relu on top of the heatmap\nheatmap = F.relu(heatmap)\n\n# normalize the heatmap\nheatmap /= torch.max(heatmap)\n\n# draw the heatmap\nplt.matshow(heatmap.detach())\n```", "```py\nfrom torchvision.transforms.functional import to_pil_image\nfrom matplotlib import colormaps\nimport numpy as np\nimport PIL\n\n# Create a figure and plot the first image\nfig, ax = plt.subplots()\nax.axis('off') # removes the axis markers\n\n# First plot the original image\nax.imshow(to_pil_image(img_tensor, mode='RGB'))\n\n# Resize the heatmap to the same size as the input image and defines\n# a resample algorithm for increasing image resolution\n# we need heatmap.detach() because it can't be converted to numpy array while\n# requiring gradients\noverlay = to_pil_image(heatmap.detach(), mode='F')\n                      .resize((256,256), resample=PIL.Image.BICUBIC)\n\n# Apply any colormap you want\ncmap = colormaps['jet']\noverlay = (255 * cmap(np.asarray(overlay) ** 2)[:, :, :3]).astype(np.uint8)\n\n# Plot the heatmap on the same axes, \n# but with alpha < 1 (this defines the transparency of the heatmap)\nax.imshow(overlay, alpha=0.4, interpolation='nearest', extent=extent)\n\n# Show the plot\nplt.show()\n```", "```py\nbackward_hook.remove()\nforward_hook.remove()\n```"]