- en: Using Server-less Functions to Govern and Monitor Cloud-Based Training Experiments
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/using-server-less-functions-to-govern-and-monitor-cloud-based-training-experiments-755c43fba26b?source=collection_archive---------4-----------------------#2023-12-17](https://towardsdatascience.com/using-server-less-functions-to-govern-and-monitor-cloud-based-training-experiments-755c43fba26b?source=collection_archive---------4-----------------------#2023-12-17)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: A simple routine that can save you loads of money
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://chaimrand.medium.com/?source=post_page-----755c43fba26b--------------------------------)[![Chaim
    Rand](../Images/c52659c389f167ad5d6dc139940e7955.png)](https://chaimrand.medium.com/?source=post_page-----755c43fba26b--------------------------------)[](https://towardsdatascience.com/?source=post_page-----755c43fba26b--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----755c43fba26b--------------------------------)
    [Chaim Rand](https://chaimrand.medium.com/?source=post_page-----755c43fba26b--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F9440b37e27fe&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fusing-server-less-functions-to-govern-and-monitor-cloud-based-training-experiments-755c43fba26b&user=Chaim+Rand&userId=9440b37e27fe&source=post_page-9440b37e27fe----755c43fba26b---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----755c43fba26b--------------------------------)
    ·11 min read·Dec 17, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F755c43fba26b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fusing-server-less-functions-to-govern-and-monitor-cloud-based-training-experiments-755c43fba26b&user=Chaim+Rand&userId=9440b37e27fe&source=-----755c43fba26b---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F755c43fba26b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fusing-server-less-functions-to-govern-and-monitor-cloud-based-training-experiments-755c43fba26b&source=-----755c43fba26b---------------------bookmark_footer-----------)![](../Images/da83f9c1fe760e7679aaa950ae6c5b1c.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Photo by [Ziyou Zhang](https://unsplash.com/@teadrinker42?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: This blog post was co-authored with my colleague [Shay Margalit](https://www.linkedin.com/in/shay-margalit-40581668/?originalSubdomain=il).
    It summarizes his research into how [AWS Lambda](https://aws.amazon.com/lambda/)
    functions can be used to increase the control over the usage and **costs** of
    the [Amazon SageMaker](https://aws.amazon.com/sagemaker/) training service. Interested?
    Please read on :).
  prefs: []
  type: TYPE_NORMAL
- en: We are fortunate (or very unfortunate — [depending on who you ask](https://www.bbc.co.uk/news/uk-65746524))
    to be sharing a front row seat to an AI revolution that is expected by many to
    change the world as we know it. Powered by advances in hardware development and
    access to enormous amounts of data, this revolution is likely to impact many aspects
    of our daily lives — although precisely how, no one can say for sure. To support
    the growing appetite for artificial intelligence, the sizes of the underlying
    machine learning models are increasing rapidly as are the resources that are required
    to train them. The bottom line is that staying relevant in the AI development
    playing field requires a sizable investment into heavy, and expensive, machinery.
  prefs: []
  type: TYPE_NORMAL
- en: Cloud-based managed training services, such as Amazon SageMaker, Google Vertex
    AI, and Microsoft Azure ML, have lowered the entry barrier to AI development by
    enabling developers to train on machines that they could otherwise not afford.
    Although such services reduce the upfront costs of AI and enable you to pay **only**
    for the time you spend training, the potential for the variable costs to add up
    warrants careful planning of how the training services will be used and how they
    will contribute to your overall training expense. However, inevitably, things
    don’t always go according to plan. To paraphrase an old Yiddish proverb “developers
    plan and the programming gods laugh”. When the stakes are high, as when training
    AI models — where an errant experiment can result in hundreds or thousands of
    dollars worth of wasted compute time, it is wise to institute multiple lines of
    defense.
  prefs: []
  type: TYPE_NORMAL
- en: First Line of Defense — Encourage Healthy Development Habits
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The first line of defense should address the development practices of the ML
    algorithm engineers. Here are examples of some [guiding principles](/cloud-ml-performance-checklist-caa51e798002)
    you might consider:'
  prefs: []
  type: TYPE_NORMAL
- en: Encourage appropriate and cost-optimal use of the hardware resources used for
    training (e.g., see [here](/instance-selection-for-deep-learning-7463d774cff0)).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Identify and terminate failing experiments early.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Increase price performance by regularly analyzing and optimizing runtime performance
    (e.g., see [here](https://medium.com/towards-data-science/pytorch-model-performance-analysis-and-optimization-10c3c5822869)).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: While formulating and adapting AI development principles such as the ones above
    are likely to increase your productivity and reduce waste, they do not offer full
    protection against all possible failures. For example, a dedicated failure detection
    runtime process may not help address a situation in which a training experiment
    stalls (e.g., due to a deadlock in the training application’s processes) but the
    training job remains active until it is actively stopped or times out.
  prefs: []
  type: TYPE_NORMAL
- en: Second Line of Defense — Deploy Cross-project Guardrails
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this post we propose instituting a second line of defense that monitors all
    of the training activities in the project (or organization), verifies their compliance
    with a predetermined set of rules, and takes appropriate action in the case that
    errant training experiments are identified. One way to do this is to use dedicated
    **server-less functions** that are triggered at different stages of a training
    job and programmed to evaluate the job’s state and optionally stop or restart
    it (possibly with changes to the job settings), accordingly. In the next sections
    we will demonstrate a few examples of how to use AWS Lambda as a second line of
    defense against errant Amazon SageMaker training experiments.
  prefs: []
  type: TYPE_NORMAL
- en: Disclaimers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Although we have chosen Amazon SageMaker and AWS Lambda for our demonstrations,
    the contents of the post are just as relevant to other services and similar functionality
    can be implemented for them. Please do not interpret our choice of these services
    as an endorsement of their use over their alternatives. There are multiple options
    available for cloud-based training each with their own advantages and disadvantages.
    The best choice for you will greatly depend on the details of your project.
  prefs: []
  type: TYPE_NORMAL
- en: While we will share a few Python examples of server-less code, we will not go
    into the details of how to create and deploy them as AWS Lambda functions. There
    are many ways of interacting with AWS Lambda. We refer the reader to the [official
    AWS documentation](https://docs.aws.amazon.com/lambda/latest/dg/python-tracing.html)
    to learn more about them.
  prefs: []
  type: TYPE_NORMAL
- en: The examples below were created for demonstrative purposes. They will likely
    require modification to suit the specific needs of your project. Be sure to fully
    understand all of the details of the code and the associated service costs before
    adapting the type of solution we propose. Importantly, the code we will share
    has not undergone rigorous testing. Any solution that includes creation and invocation
    of multiple Lambda functions and [Amazon CloudWatch alarms](https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/AlarmThatSendsEmail.html)
    (as described here) requires appropriate validation to prevent the accumulation
    of redundant/orphan artifacts.
  prefs: []
  type: TYPE_NORMAL
- en: We highly advise that you verify the details of this post against the most up-to-date
    AWS Lambda documentation and most up-to-date versions of the supporting libraries.
  prefs: []
  type: TYPE_NORMAL
- en: Enforcing Developer Compliance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'While [cloud governance](https://www.redhat.com/en/topics/automation/what-is-cloud-governance)
    is often vital for successful and efficient use of cloud services, its enforcement
    can sometimes be challenging. For example: Amazon SageMaker includes an API for
    appending [tags](https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_Tag.html)
    to training jobs. These can be used to include metadata associated with the SageMaker
    job such as the name of the training project, the stage of development, the goal
    of the current trial, the name of the development group or user running the job,
    etc. This metadata can be used to collect statistics such as the cost of development
    per project or group. In the code block below, we demonstrate the application
    of several tags to a SageMaker training job:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Naturally, these tags are only helpful if we can enforce their application.
    This is where [AWS Lambda](https://aws.amazon.com/lambda/) comes to the rescue.
    Using [Amazon EventBridge](https://docs.aws.amazon.com/sagemaker/latest/dg/automating-sagemaker-with-eventbridge.html)
    we can monitor changes in the status of a SageMaker training jobs and register
    a function that will be triggered on every change. In the code block below, we
    propose a Python routine that will verify the presence of specific SageMaker tags
    every time a job is started. In case a tag is missing the job is automatically
    terminated. The structure of the event is documented [here](https://docs.aws.amazon.com/sagemaker/latest/dg/automating-sagemaker-with-eventbridge.html#eventbridge-training).
    Note the use of (the more detailed) [*SecondaryStatus*](https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_DescribeTrainingJob.html)field
    to poll the status of the training job (rather than *TrainingJobStatus*).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: AWS offers multiple ways for creating a Lambda function. Please see the [AWS
    Lambda](https://docs.aws.amazon.com/lambda/latest/dg/API_CreateFunction.html)
    documentation for details. Once created, make sure to set the function as the
    target of the [EventBridge rule](https://docs.aws.amazon.com/sagemaker/latest/dg/automating-sagemaker-with-eventbridge.html#eventbridge-model).
  prefs: []
  type: TYPE_NORMAL
- en: 'The same function can be used to enforce additional development rules that
    are aimed at controlling cost such as: the types of instances that can be used,
    the maximum number of instances per job, the maximum runtime of a job, and more.'
  prefs: []
  type: TYPE_NORMAL
- en: Stopping Stalled Experiments
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Imagine the following scenario: You have planned a large cloud-based training
    job that will run on eight $30-an-hour ML compute instances for a period of three
    days. For the purpose of this task, you have secured a budget of $17,280 (8 instances
    x $30 an hour x 24 hours x 3 days). You start up the training job just before
    heading out for a three-day holiday weekend. When you return from your holiday
    weekend, you discover that an hour into the job, the training process stalled
    causing the expensive machinery to essentially remain completely idle for three
    long days. Not only have you wasted $17,280 (good luck explaining that to your
    boss) but your development has now been pushed back by three days!!'
  prefs: []
  type: TYPE_NORMAL
- en: One way to protect yourself against this type of occurrence, is to monitor the
    utilization of the underlying training job resources. For example, if the GPU
    utilization your training instances remains below a certain threshold for an extended
    period of time, this is likely to be a sign that something has gone wrong and
    that the training job should be stopped immediately.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will do this by defining an [Amazon CloudWatch alarm](https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/AlarmThatSendsEmail.html)
    that monitors the GPU utilization of one of the training instances of each SageMaker
    job and invokes an AWS Lambda function that terminates the job if the alarm is
    triggered. Setting this up requires three components: an [Amazon CloudWatch alarm](https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/AlarmThatSendsEmail.html)
    (one per training job), an [AWS Lambda](https://aws.amazon.com/lambda/) function,
    and an [Amazon Simple Notification Service (SNS) topic](https://docs.aws.amazon.com/sns/)
    that is used to link the Lambda function to the CloudWatch alarms.'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we create an SNS topic. This can be done via the Amazon SNS Console
    or in Python, as shown below:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Next, we extend the *sagemaker_event_handler* function we defined above to create
    a unique alarm each time a training job is started. We program the alarm to measure
    the average GPU utilization over five-minute periods and to alert our SNS topic
    when there are three consecutive measurements below 1%. The alarm is deleted when
    the job is completed.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Last, we define a second Python AWS Lambda function that [parses messages received
    from the SNS topic](https://docs.aws.amazon.com/lambda/latest/dg/with-sns-create-package.html#with-sns-example-deployment-pkg-python)
    and terminates the training job associated with the alarm.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: AWS offers multiple mechanisms for subscribing a Lambda function to an SNS topic
    including the [AWS Console](https://docs.aws.amazon.com/sns/latest/dg/lambda-console.html),
    [AWS CLI](https://docs.aws.amazon.com/lambda/latest/dg/with-sns-example.html),
    and [the AWS Serverless Application Model (AWS SAM)](https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/sam-resource-function.html).
  prefs: []
  type: TYPE_NORMAL
- en: 'The solution we described is summarized in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/129dedc3d99a0d37b85e084f33a18beb.png)'
  prefs: []
  type: TYPE_IMG
- en: AWS Architecture Diagram (by Author)
  prefs: []
  type: TYPE_NORMAL
- en: Note that the same architecture can be used to enforce a minimum level of GPU
    utilization of your ML training projects. GPUs are typically the most expensive
    resource in your training infrastructure and your goal should be to maximize the
    utilization of all of your training workloads. By dictating a minimum level of
    utilization (e.g. 80%) you can ensure that all developers [optimize their workloads
    appropriately](/cloud-ml-performance-checklist-caa51e798002).
  prefs: []
  type: TYPE_NORMAL
- en: Ensuring Continuity of Development
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In our previous example, we demonstrated how to identify and stop a stalled
    experiment. In the large training job scenario that we described, this helped
    save a lot of money, but it did not address the three day delay to development.
    Obviously, if the source of the stall is in your code, it makes sense to postpone
    resuming training until the problem is fixed. However, we often encounter training
    interruptions that are not caused by our code but rather by sporadic failures
    in the service environment. In such scenarios, your priority may be to ensure
    training continuity rather than having to wait for someone to manually resume
    the training job (using the most recent [training checkpoint](https://pytorch.org/tutorials/recipes/recipes/saving_and_loading_a_general_checkpoint.html)).
    In the code block below, we use the [boto3](https://boto3.amazonaws.com/v1/documentation/api/latest/index.html)
    [*create_training_job*](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/sagemaker/client/create_training_job.html)API
    to extend our *sagemaker_event_handler* function to (naively) resume any training
    job that has failed after running for at least two hours.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: The function above automatically resumes any job that fails after two hours.
    A more practical solution might attempt to diagnose the type of error to determine
    whether resuming the job would be appropriate. One way to do this is to parse
    the failure description message and/or the CloudWatch logs associated with the
    failing job.
  prefs: []
  type: TYPE_NORMAL
- en: Advanced Spot-instance Utilization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the compelling features of Amazon SageMaker is its support for [managed
    spot training](https://docs.aws.amazon.com/sagemaker/latest/dg/model-managed-spot-training.html).
    [Amazon EC2 Spot Instances](https://aws.amazon.com/ec2/spot/) allow you to take
    advantage of unused EC2 capacity at discounted prices. The catch is that these
    instances can be taken away (“interrupted”) in the middle of their use. Thus,
    Spot instances should be used only for fault-tolerant workloads. SageMaker makes
    it easy to take advantage of Spot instances by identifying Spot interruptions
    on your behalf and automatically restarting jobs when new Spot instances become
    available. While [managed spot](https://docs.aws.amazon.com/sagemaker/latest/dg/model-managed-spot-training.html)
    instances can be used to reduce cost of training, sometimes this strategy can
    backfire. For example, when there is low spot capacity your training jobs might
    time out before starting. Alternatively, the job might experience frequent interruptions
    that prevent it from making any meaningful progress. Both occurrences can interfere
    with development and reduce productivity. These types of situations can be monitored
    and addressed using AWS Lambda. In the code block below, we extend our *sagemaker_event_handler*
    function to identify a training job that has been interrupted more than three
    times and replace it with a cloned job in which the managed spot training is disabled.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: The implementation above determined the spot usage strategy based solely on
    the number of interruptions of the training job in question. A more elaborate
    solution might take into account other jobs (that use the same instance types),
    the duration of time across which the interruptions occurred, the amount of active
    training time, and/or the number of recent jobs that timed out due to low Spot
    instance capacity.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Effective AI model development requires the definition of a creative and detailed
    training infrastructure architecture in order to minimize cost and maximize productivity.
    In this post we have demonstrated how serverless AWS Lambda functions can be used
    to augment Amazon SageMaker’s managed training service in order to address some
    common issues that can occur during training. Naturally, the precise manner in
    which you might apply these kinds of techniques will depend greatly on the specifics
    of your project.
  prefs: []
  type: TYPE_NORMAL
- en: Please feel free to reach out with questions, comments, and corrections. Be
    sure to check out our [other posts](https://chaimrand.medium.com/) on the topic
    of DL training optimization.
  prefs: []
  type: TYPE_NORMAL
