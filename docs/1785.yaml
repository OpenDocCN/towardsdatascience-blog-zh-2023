- en: 'QLoRa: Fine-Tune a Large Language Model on Your GPU'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/qlora-fine-tune-a-large-language-model-on-your-gpu-27bed5a03e2b?source=collection_archive---------1-----------------------#2023-05-30](https://towardsdatascience.com/qlora-fine-tune-a-large-language-model-on-your-gpu-27bed5a03e2b?source=collection_archive---------1-----------------------#2023-05-30)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Fine-tuning models with billions of parameters is now possible on consumer hardware
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@bnjmn_marie?source=post_page-----27bed5a03e2b--------------------------------)[![Benjamin
    Marie](../Images/3ea1ad230cb1e67610418a8e36a5e5dd.png)](https://medium.com/@bnjmn_marie?source=post_page-----27bed5a03e2b--------------------------------)[](https://towardsdatascience.com/?source=post_page-----27bed5a03e2b--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----27bed5a03e2b--------------------------------)
    [Benjamin Marie](https://medium.com/@bnjmn_marie?source=post_page-----27bed5a03e2b--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fad2a414578b3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fqlora-fine-tune-a-large-language-model-on-your-gpu-27bed5a03e2b&user=Benjamin+Marie&userId=ad2a414578b3&source=post_page-ad2a414578b3----27bed5a03e2b---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----27bed5a03e2b--------------------------------)
    ·6 min read·May 30, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F27bed5a03e2b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fqlora-fine-tune-a-large-language-model-on-your-gpu-27bed5a03e2b&user=Benjamin+Marie&userId=ad2a414578b3&source=-----27bed5a03e2b---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F27bed5a03e2b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fqlora-fine-tune-a-large-language-model-on-your-gpu-27bed5a03e2b&source=-----27bed5a03e2b---------------------bookmark_footer-----------)![](../Images/2bf17a5fcbca8e98c743ab818939b6ab.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Illustration by the author.
  prefs: []
  type: TYPE_NORMAL
- en: Most large language models (LLM) are too big to be fine-tuned on consumer hardware.
    For instance, to fine-tune a 65 billion parameters model we need more than 780
    Gb of GPU memory. This is equivalent to ten A100 80 Gb GPUs. In other words, you
    would need cloud computing to fine-tune your models.
  prefs: []
  type: TYPE_NORMAL
- en: Now, with QLoRa ([Dettmers et al., 2023](https://arxiv.org/pdf/2305.14314.pdf)),
    you could do it with only one A100.
  prefs: []
  type: TYPE_NORMAL
- en: In this blog post, I will introduce QLoRa. I will briefly describe how it works
    and we will see how to use it to fine-tune a GPT model with 20 billion parameters,
    on your GPU.
  prefs: []
  type: TYPE_NORMAL
- en: '*Note: I used my own nVidia RTX 3060 12 Gb to run all the commands in this
    post. You can also use a free instance of Google Colab to achieve the same results.
    If you want to use a GPU with a smaller memory, you would have to use a smaller
    LLM.*'
  prefs: []
  type: TYPE_NORMAL
- en: 'I provide all the necessary code to run QLoRa for fine-tuning in this article.
    If you don’t want to code it by yourself, [I also created a Google Colab notebook
    on The Kaitchup (my substack newsletter). This is notebook #2.](https://newsletter.kaitchup.com/p/notebooks)'
  prefs: []
  type: TYPE_NORMAL
- en: 'QLoRa: Quantized LLMs with Low-Rank Adapters'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In June 2021, [Hu et al. (2021)](https://arxiv.org/abs/2106.09685) introduced
    low-rank adapters (LoRa) for LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: LoRa adds a tiny amount of trainable parameters, i.e., adapters, for each layer
    of the LLM and freezes all the original parameters. For fine-tuning, we only have
    to update the adapter weights which significantly reduces the memory footprint.
  prefs: []
  type: TYPE_NORMAL
- en: 'QLoRa goes three steps further by introducing: 4-bit quantization, double quantization,
    and the exploitation of nVidia unified memory for paging.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In a few words, each one of these steps works as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**4-bit NormalFloat quantization**: This is a method that improves upon quantile
    quantization. It ensures an equal number of values in each quantization bin. This
    avoids computational issues and errors for outlier values.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Double quantization**: The authors of QLoRa define it as follows: “*the process
    of quantizing the quantization constants for additional memory savings.*”'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Paging with unified memory**: It relies on the NVIDIA Unified Memory feature
    and automatically handles page-to-page transfers between the CPU and GPU. It ensures
    error-free GPU processing, especially in situations where the GPU may run out
    of memory.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All of these steps drastically reduce the memory requirements for fine-tuning,
    while performing almost on par with standard fine-tuning.
  prefs: []
  type: TYPE_NORMAL
- en: Fine-tuning a GPT model with QLoRa
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Hardware requirements for QLoRa:'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**GPU**: The following demo works on a GPU with 12 Gb of VRAM, for a model
    with less than 20 billion parameters, e.g., GPT-J. For instance, I ran it with
    my RTX 3060 12 Gb. If you have a bigger card with 24 Gb of VRAM, you can do it
    with a 20 billion parameter model, e.g., GPT-NeoX-20b.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**RAM**: I recommend a minimum of 6 Gb. Most recent computers have enough RAM.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Hard drive**: GPT-J and GPT-NeoX-20b are both very big models. I recommend
    at least 80 Gb of free space.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If your machine doesn’t meet these requirements, the free instance of Google
    Colab would be enough instead.
  prefs: []
  type: TYPE_NORMAL
- en: 'Software requirements for QLoRa:'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We need CUDA. Make sure it is installed on your machine.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will also need to install all the dependencies:'
  prefs: []
  type: TYPE_NORMAL
- en: '**bitsandbytes**: A library that contains all we need to quantize an LLM.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Hugging Face Transformers and Accelerate**: These are standard libraries
    that are used to efficiently train models from Hugging Face Hub.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**PEFT**: A library that provides the implementations for various methods to
    only fine-tune a small number of (extra) model parameters. We need it for LoRa.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Datasets**: This one is not a requirement. We will only use it to get a dataset
    for fine-tuning. Of course, you can provide instead your own dataset.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We can get all of them with PIP:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Next, we can start writing the Python script.
  prefs: []
  type: TYPE_NORMAL
- en: Loading and Quantization of a GPT model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We need the following imports to load and quantize an LLM.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'For this demo, we will fine-tune the [GPT NeoX](https://huggingface.co/EleutherAI/gpt-neox-20b)
    model pre-trained by [EleutherAI](https://www.eleuther.ai/). This is a model with
    20 billion parameters. *Note: GPT NeoX has a permissive license (Apache 2.0) that
    allows commercial use.*'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can get this model and the associated tokenizer from Hugging Face Hub:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we need to detail the configuration of the quantizer, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'load_in_4bit: The model will be loaded in the memory with 4-bit precision.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'bnb_4bit_use_double_quant: We will do the double quantization proposed by QLoRa.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'bnb_4bit_quant_type: This is the type of quantization. “nf4” stands for 4-bit
    NormalFloat.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'bnb_4bit_compute_dtype: While we load and store the model in 4-bit, we will
    partially dequantize it when needed and do all the computations with a 16-bit
    precision (bfloat16).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'So now we can load the model in 4-bit:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we enable gradient checkpointing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Preprocessing the GPT model for LoRa
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This is where we use PEFT. We prepare the model for LoRa, adding trainable adapters
    for each layer.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: In LoraConfig, you can play with r, alpha, and dropout to obtain better results
    on your task. You can find more options and details in the [PEFT repository](https://github.com/huggingface/peft/tree/main).
  prefs: []
  type: TYPE_NORMAL
- en: With LoRa, we add only 8 million parameters. We will only train these parameters
    and freeze everything else. Fine-tuning should be fast.
  prefs: []
  type: TYPE_NORMAL
- en: Get your dataset ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For this demo, I use the “english_quotes” dataset. [This is a dataset made of
    famous quotes](https://huggingface.co/datasets/Abirate/english_quotes) distributed
    under a [CC BY 4.0 license](https://creativecommons.org/licenses/by/4.0/).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Fine-tuning GPT-NeoX-20B with QLoRa
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Finally, the fine-tuning with Hugging Face Transformers is very standard.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Don’t forget optim=”paged_adamw_8bit”. It activates the paging for better memory
    management. Without it, we get out-of-memory errors.
  prefs: []
  type: TYPE_NORMAL
- en: Running this fine-tuning should only take 5 minutes on Google Colab.
  prefs: []
  type: TYPE_NORMAL
- en: The VRAM consumption should peak at 15 Gb.
  prefs: []
  type: TYPE_NORMAL
- en: And that’s it, we fine-tuned an LLM for free!
  prefs: []
  type: TYPE_NORMAL
- en: Does it work? Let’s try inference.
  prefs: []
  type: TYPE_NORMAL
- en: GPT Inference with QLoRa
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The QLoRa model we fine-tuned can be directly used with the standard Hugging
    Face Transformers’ inference, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'You should get this quote as output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: We got the expected quote. Not bad for 5 minutes of fine-tuning!
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Large language models got bigger but, at the same time, we finally got the tools
    to do fine-tuning and inference on consumer hardware.
  prefs: []
  type: TYPE_NORMAL
- en: Thanks to LoRa, and now QLoRa, we can fine-tune models with billion parameters
    without relying on cloud computing and without a significant drop in performance
    according to the [QLoRa paper](https://arxiv.org/abs/2106.09685).
  prefs: []
  type: TYPE_NORMAL
- en: If you have any problem running the code, please drop a comment, and I’ll try
    to help. You can also find more information about [QLoRa implementation in the
    official GitHub repository](https://github.com/artidoro/qlora).
  prefs: []
  type: TYPE_NORMAL
- en: 'If you want to deploy an LLM, have a look at my tutorial using nVidia Triton
    Inference Server:'
  prefs: []
  type: TYPE_NORMAL
- en: '[](/deploy-your-local-gpt-server-with-triton-a825d528aa5d?source=post_page-----27bed5a03e2b--------------------------------)
    [## Deploy Your Local GPT Server With Triton'
  prefs: []
  type: TYPE_NORMAL
- en: How to run large language models on your local server
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/deploy-your-local-gpt-server-with-triton-a825d528aa5d?source=post_page-----27bed5a03e2b--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: '*If you like this article and would be interested in reading the next ones,
    the best way to support my work is to subscribe to The Kaitchup:*'
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://newsletter.kaitchup.com/?source=post_page-----27bed5a03e2b--------------------------------)
    [## The Kaitchup - AI on a Budget | Benjamin Marie | Substack'
  prefs: []
  type: TYPE_NORMAL
- en: Weekly tutorials, tips, and news on fine-tuning, running, and serving large
    language models on your computer. The…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: newsletter.kaitchup.com](https://newsletter.kaitchup.com/?source=post_page-----27bed5a03e2b--------------------------------)
  prefs: []
  type: TYPE_NORMAL
