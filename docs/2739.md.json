["```py\ndef singletons_and_pairs(lst):\n    singletons = [(x,) for x in lst]\n    pairs = list(combinations(lst, 2))\n    return singletons + pairs\n\ningredients = [\"o\", \"a\", \"b\",\"l\", \"m\"]\nmodel = singletons_and_pairs(ingredients)\nw = np.random.normal(0, 1, size = (len(model),))\np = np.random.randint(0, 2, size = (len(model),))\nw = w * p\n```", "```py\ndef vectorize_smoothie(smoothie):\n    arr = np.zeros(len(model))\n    print(list(smoothie))\n    for i in range(arr.shape[0]):\n        if all(j in list(smoothie) for j in model[i]):\n            arr[i] = 1\n    return arr\n```", "```py\nvectorize_smoothie(\"oa\") @ w\n# Return w_a + w_o + w_oa\n```", "```py\ndef sample_dataset(n):\n    ingredients = [\"o\", \"a\", \"b\",\"l\", \"m\"]\n    model = singletons_and_pairs(ingredients)\n    X = []\n    y = []\n    w = sample_w(model)\n    subsets = set()\n    while len(subsets) != n:\n        s = random_subset(ingredients)\n        subsets.add(s)\n    subsets = list(subsets)\n    for i in range(len(subsets)-1):\n        x_i = vectorize_smoothie(subsets[i])\n        for j in range(i+1, len(subsets)):\n            x_j = vectorize_smoothie(subsets[j])\n            x1 = x_i - x_j\n            x2 = x_j - x_i\n            if f(subsets[i], w) == f(subsets[j], w):\n                continue\n            if f(subsets[i], w) > f(subsets[j], w):\n                X.append(x1)\n                X.append(x2)\n                y.append(1)\n                y.append(0)\n                continue\n            if f(subsets[i], w) < f(subsets[j], w):\n                X.append(x1)\n                X.append(x2)\n                y.append(0)\n                y.append(1)\n                continue\n    X = np.array(X)\n    y = np.array(y)\n    return X,y,w,model\n```", "```py\nX,y,w,model = sample_dataset(30)\n\nX_tensor = torch.FloatTensor(X)\ny_tensor = torch.FloatTensor(y)\n\ndataset = TensorDataset(X_tensor, y_tensor)\ntrain_size = int(0.3 * len(dataset))\ntest_size = len(dataset) - train_size\ntrain_dataset, test_dataset = random_split(dataset, [train_size, test_size])\ntrain_loader = DataLoader(dataset=train_dataset, batch_size=32, shuffle=True)\ntest_loader = DataLoader(dataset=test_dataset, batch_size=1, shuffle=False)\n```", "```py\nclass BinaryClassifier(nn.Module):\n    def __init__(self, input_dim):\n        super(BinaryClassifier, self).__init__()\n        self.fc1 = nn.Linear(input_dim, 1)\n\n    def forward(self, x):\n        x = torch.sigmoid(self.fc1(x))\n        return x\n```", "```py\ninput_dim = X.shape[1]\nmodel = BinaryClassifier(input_dim)\n\n# Loss and optimizer\ncriterion = nn.BCELoss()\noptimizer = optim.Adam(model.parameters(), lr=0.01)\n\nlosses = []\n\n# Train the model\nepochs = 200\nfor epoch in range(epochs):\n    for batch_idx, (data, target) in enumerate(train_loader):\n        optimizer.zero_grad()\n        output = model(data).squeeze()\n        loss = criterion(output, target)\n        loss.backward()\n        optimizer.step()\n```", "```py\nmodel.eval()\nwith torch.no_grad():\n  correct = 0\n  total = 0\n  for data, target in test_loader:\n    output = model(data).squeeze()\n    predicted = (output > 0.5).float()\n    total += target.size(0)\n    correct += (predicted == target).sum().item()\n  acc = correct / total\n  accuracy.append(acc)\n\nif (epoch+1) % 50 == 0:\n  print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}')\n  print(f'Test Accuracy: {100 * correct / total:.2f}%')\n```", "```py\nimport pymc3 as pm\n\nwith pm.Model() as probit_model:\n\n    # Priors for weights and bias\n    weights = pm.Normal('weights', mu=0, sd=4, shape=X.shape[1])\n    bias = pm.Normal('bias', mu=0, sd=4)\n\n    # Probit link function\n    mu = pm.math.dot(X, weights) + bias\n    phi = pm.math.invprobit(mu)  # Inverse probit link function\n\n    # Likelihood\n    y_obs = pm.Bernoulli('y_obs', p=phi, observed=y)\n\n    # Sample from the posterior\n    trace = pm.sample(5000, tune=1000, chains=5, target_accept = 0.90)\n```"]