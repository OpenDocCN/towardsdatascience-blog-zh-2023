["```py\npip install truss\n```", "```py\ntruss init falcon_7b_truss\n```", "```py\n├── falcon_7b_truss\n│   ├── config.yaml\n│   ├── data\n│   ├── examples.yaml\n│   ├── model\n│   │   ├── __init__.py\n│   │   └── model.py\n│   └── packages\n└── main.py\n```", "```py\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\nfrom typing import Dict\n\nMODEL_NAME = \"tiiuae/falcon-7b-instruct\"\nDEFAULT_MAX_LENGTH = 128\n\nclass Model:\n    def __init__(self, data_dir: str, config: Dict, **kwargs) -> None:\n        self._data_dir = data_dir\n        self._config = config\n        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n        print(\"THE DEVICE INFERENCE IS RUNNING ON IS: \", self.device)\n        self.tokenizer = None\n        self.pipeline = None\n\n    def load(self):\n        self.tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n        model_8bit = AutoModelForCausalLM.from_pretrained(\n            MODEL_NAME,\n            device_map=\"auto\",\n            load_in_8bit=True,\n            trust_remote_code=True)\n\n        self.pipeline = pipeline(\n            \"text-generation\",\n            model=model_8bit,\n            tokenizer=self.tokenizer,\n            torch_dtype=torch.bfloat16,\n            trust_remote_code=True,\n            device_map=\"auto\",\n        )\n\n    def predict(self, request: Dict) -> Dict:\n        with torch.no_grad():\n            try:\n                prompt = request.pop(\"prompt\")\n                data = self.pipeline(\n                    prompt,\n                    eos_token_id=self.tokenizer.eos_token_id,\n                    max_length=DEFAULT_MAX_LENGTH,\n                    **request\n                )[0]\n                return {\"data\": data}\n\n            except Exception as exc:\n                return {\"status\": \"error\", \"data\": None, \"message\": str(exc)}\n```", "```py\npip install transformers\npip install torch\npip install peft\npip install bitsandbytes\npip install einops\npip install scipy \n```", "```py\nimport truss\nfrom pathlib import Path\nimport requests\n\ntr = truss.load(\"./falcon_7b_truss\")\noutput = tr.predict({\"prompt\": \"Hi there how are you?\"})\nprint(output)\n```", "```py\n{'data': {'generated_text': \"Hi there how are you?\\nI'm doing well. I'm in the middle of a move, so I'm a bit tired. I'm also a bit overwhelmed. I'm not sure how to get started. I'm not sure what I'm doing. I'm not sure if I'm doing it right. I'm not sure if I'm doing it wrong. I'm not sure if I'm doing it at all.\\nI'm not sure if I'm doing it right. I'm not sure if I'm doing it wrong. I\"}}\n```", "```py\napply_library_patches: true\nbundled_packages_dir: packages\ndata_dir: data\ndescription: null\nenvironment_variables: {}\nexamples_filename: examples.yaml\nexternal_package_dirs: []\ninput_type: Any\nlive_reload: false\nmodel_class_filename: model.py\nmodel_class_name: Model\nmodel_framework: custom\nmodel_metadata: {}\nmodel_module_dir: model\nmodel_name: Falcon-7B\nmodel_type: custom\npython_version: py39\nrequirements:\n- torch\n- peft\n- sentencepiece\n- accelerate\n- bitsandbytes\n- einops\n- scipy\n- git+https://github.com/huggingface/transformers.git\nresources:\n  use_gpu: true\n  cpu: \"3\"\n  memory: 14Gi\nsecrets: {}\nspec_version: '2.0'\nsystem_packages: []\n```", "```py\nimport truss\nfrom pathlib import Path\nimport requests\n\ntr = truss.load(\"./falcon_7b_truss\")\ncommand = tr.docker_build_setup(build_dir=Path(\"./falcon_7b_truss\"))\nprint(command)\n```", "```py\ndocker tag falcon-7b-model <docker_user_id>/falcon-7b-model\n```", "```py\ndocker push <docker_user_id>/falcon-7b-model\n```", "```py\ndistribution=$(. /etc/os-release;echo $ID$VERSION_ID) && curl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | sudo apt-key add - && curl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.list | sudo tee /etc/apt/sources.list.d/nvidia-docker.list\n```", "```py\napt-get update\napt-get install -y nvidia-docker2\n```", "```py\nsudo systemctl restart docker\n```", "```py\ndocker run --gpus all -d -p 8080:8080 falcon-7b-model\n```", "```py\nimport requests\n\ndata = {\"prompt\": \"Hi there, how's it going?\"}\nres = requests.post(\"http://127.0.0.1:8080/v1/models/model:predict\", json=data)\nprint(res.json())\n```", "```py\ngcloud config set compute/zone us-central1-c\n```", "```py\ngcloud container clusters get-credentials gpu-cluster-1\n```", "```py\nkubectl get nodes\n```", "```py\nkubectl apply -f https://raw.githubusercontent.com/GoogleCloudPlatform/container-engine-accelerators/master/nvidia-driver-installer/cos/daemonset-preloaded.yaml\n```", "```py\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: truss-falcon-7b\n  namespace: default\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      component: truss-falcon-7b-layer\n  template:\n    metadata:\n      labels:\n        component: truss-falcon-7b-layer\n    spec:\n      containers:\n      - name: truss-falcon-7b-container\n        image: <your_docker_id>/falcon-7b-model:latest\n        ports:\n          - containerPort: 8080\n        resources:\n          limits:\n            nvidia.com/gpu: 1\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: truss-falcon-7b-service\n  namespace: default\nspec:\n  type: ClusterIP\n  selector:\n    component: truss-falcon-7b-layer\n  ports:\n  - port: 8080\n    protocol: TCP\n    targetPort: 8080\n```", "```py\nkubectl create -f truss-falcon-deployment.yaml\n```", "```py\nkubectl get deployments\n```", "```py\nNAME              READY   UP-TO-DATE   AVAILABLE   AGE\ntruss-falcon-7b   0/1     1            0           8s\n```", "```py\nkubectl get pods\n```", "```py\nkubectl logs truss-falcon-7b-8fbb476f4-bggts\n```", "```py\nDownloading (…)model.bin.index.json: 100%|██████████| 16.9k/16.9k [00:00<00:00, 1.92MB/s]\nDownloading (…)l-00001-of-00002.bin: 100%|██████████| 9.95G/9.95G [02:37<00:00, 63.1MB/s]\nDownloading (…)l-00002-of-00002.bin: 100%|██████████| 4.48G/4.48G [01:04<00:00, 69.2MB/s]\nDownloading shards: 100%|██████████| 2/2 [03:42<00:00, 111.31s/it][01:04<00:00, 71.3MB/s]\n```", "```py\n{\"asctime\": \"2023-06-29 21:40:40,646\", \"levelname\": \"INFO\", \"message\": \"Completed model.load() execution in 330588 ms\"}\n```", "```py\nkubectl get svc\n```", "```py\nNAME                      TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)    AGE\nkubernetes                ClusterIP   10.80.0.1    <none>        443/TCP    46m\ntruss-falcon-7b-service   ClusterIP   10.80.1.96   <none>        8080/TCP   6m19s\n```", "```py\nkubectl port-forward svc/truss-falcon-7b-service 8080\n```", "```py\nForwarding from 127.0.0.1:8080 -> 8080\nForwarding from [::1]:8080 -> 8080\n```", "```py\nimport requests\n\ndata = {\"prompt\": \"Whats the most interesting thing about a falcon?\"}\nres = requests.post(\"http://127.0.0.1:8080/v1/models/model:predict\", json=data)\nprint(res.json())\n```", "```py\n {'data': {'generated_text': 'Whats the most interesting thing about a falcon?\\nFalcons are known for their incredible speed and agility in the air, as well as their impressive hunting skills. They are also known for their distinctive feathering, which can vary greatly depending on the species.'}}\n```", "```py\nkubectl delete -f truss-falcon-deployment.yaml\n```"]