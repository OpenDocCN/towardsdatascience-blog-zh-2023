- en: 'GPT vs BERT: Which is Better?'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/gpt-vs-bert-which-is-better-2f1cf92af21a?source=collection_archive---------0-----------------------#2023-06-23](https://towardsdatascience.com/gpt-vs-bert-which-is-better-2f1cf92af21a?source=collection_archive---------0-----------------------#2023-06-23)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Comparing two large-language models: Approach and example'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://pranay-dave9.medium.com/?source=post_page-----2f1cf92af21a--------------------------------)[![Pranay
    Dave](../Images/accecc418ea23d26862761bf470fcf04.png)](https://pranay-dave9.medium.com/?source=post_page-----2f1cf92af21a--------------------------------)[](https://towardsdatascience.com/?source=post_page-----2f1cf92af21a--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----2f1cf92af21a--------------------------------)
    [Pranay Dave](https://pranay-dave9.medium.com/?source=post_page-----2f1cf92af21a--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F89d4bb7ead78&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgpt-vs-bert-which-is-better-2f1cf92af21a&user=Pranay+Dave&userId=89d4bb7ead78&source=post_page-89d4bb7ead78----2f1cf92af21a---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----2f1cf92af21a--------------------------------)
    ·6 min read·Jun 23, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F2f1cf92af21a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgpt-vs-bert-which-is-better-2f1cf92af21a&user=Pranay+Dave&userId=89d4bb7ead78&source=-----2f1cf92af21a---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F2f1cf92af21a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgpt-vs-bert-which-is-better-2f1cf92af21a&source=-----2f1cf92af21a---------------------bookmark_footer-----------)![](../Images/948d3d929fc0378773816d8614d45deb.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Image created by DALLE and PPT by author ([https://labs.openai.com/s/qQgpAQbLi0srYlZHOHQjGKWh](https://labs.openai.com/s/qQgpAQbLi0srYlZHOHQjGKWh))
  prefs: []
  type: TYPE_NORMAL
- en: 'The rise in popularity of generative AI has also led to an increase in the
    number of large language models. In this story, I will make a comparison between
    two of them: GPT and BERT. GPT (Generative Pre-trained Transformer) is developed
    by OpenAI and is based on decoder-only architecture. On the other hand, BERT (Bidirectional
    Encoder Representations from Transformers) is developed by Google and is an encoder-only
    pre-trained model'
  prefs: []
  type: TYPE_NORMAL
- en: Both are technically different, but, they have a similar objective — to perform
    natural language processing tasks. Many articles compare the two from a technical
    point of view. However, in this story, I would compare them based on the quality
    of their objective, which is natural language processing.
  prefs: []
  type: TYPE_NORMAL
- en: Comparison approach
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: How to compare two completely different technical architectures? GPT is decoder-only
    architecture and BERT is encoder-only architecture. So a technical comparison
    of a decoder-only vs encoder-only architecture is like comparing Ferrari vs Lamborgini
    — both are great but with completely different technology under the chassis.
  prefs: []
  type: TYPE_NORMAL
- en: However, we can make a comparison based on the quality of a common natural language
    task that both can do — which is…
  prefs: []
  type: TYPE_NORMAL
