["```py\nimport sympy as sm\n\nx, y, λ, ρ = sm.symbols('x y λ ρ')\n\n# f(x): 100*(y-x**2)**2 + (1-x)**2 \n# h(x): x**2 - y = 2\n# g_1(x): x <= 0\n# g_2(x) y >= 3\n\nobjective = 100*(y-x**2)**2 + (1-x)**2 + λ*(x**2-y-2) - ρ*sm.log((y-3)*(-x))\nsymbols = [x,y,λ,ρ] # Function requires last symbol to be ρ (barrier parameter)\nx0 = {x:-15,y:15,λ:0,ρ:10}\n\nconstrained_newton_method(objective,symbols,x0)\n```", "```py\nimport pandas as pd\n\ndf = pd.DataFrame()\n\n## Digital Advertising - ln(δ) \ndf['log_digital_advertising'] = np.log(np.random.normal(loc=50000,scale=15000,size=120).round())\n\n## Television Advertising - ln(τ)\ndf['log_television_advertising'] = np.log(np.random.normal(loc=50000,scale=15000,size=120).round())\n\n## Matrix X of covariates\n\n# Lag Digital Advertising\ndf['log_digital_advertising_lag1'] = df['log_digital_advertising'].shift(1)\ndf['log_digital_advertising_lag2'] = df['log_digital_advertising'].shift(2)\n\n# Lag Television Advertising\ndf['log_television_advertising_lag1'] = df['log_television_advertising'].shift(1)\ndf['log_television_advertising_lag2'] = df['log_television_advertising'].shift(2)\n\n# Price\ndf['price'] = np.random.normal(loc=180,scale=15,size=120).round()\ndf['price_lag1'] = df['price'].shift(1)\ndf['price_lag2'] = df['price'].shift(2)\n\n# Competitor Price\ndf['comp_price'] = np.random.normal(loc=120,scale=15,size=120).round()\ndf['comp_price_lag1'] = df['comp_price'].shift(1)\ndf['comp_price_lag2'] = df['comp_price'].shift(2)\n\n# Seasonality \nfrom itertools import cycle\n\nmonths = cycle(['Jan','Feb','Mar','Apr','May','June','July','Aug','Sep','Oct','Nov','Dec'])\ndf['months'] = [next(months) for m in range(len(df))]\n\none_hot = pd.get_dummies(df['months'], dtype=int)\none_hot = one_hot[['Jan','Feb','Mar','Apr','May','June','July','Aug','Sep','Oct','Nov','Dec']]\ndf = df.join(one_hot).drop('months',axis=1)\n\n## Constant\ndf['constant'] = 1\n\n# Drop NaN (Two lags)\ndf = df.dropna()\n```", "```py\nparams = np.array(\n    [10_000, # β\n    5_000, # γ\n    2_000, # Ω\n    1_000, # Ω\n    3_000, # Ω\n    1_000, # Ω\n   -1_000, # Ω\n   -500,   # Ω\n   -100,   # Ω\n    500,   # Ω\n    300,   # Ω\n    100,   # Ω\n    25_000, # S\n    15_000, # S \n    15_000, # S\n    10_000, # S\n    10_000, # S\n    10_000, # S\n    15_000, # S\n    15_000, # S\n    25_000, # S\n    35_000, # S\n    35_000, # S\n    40_000, # S\n    50_000  # α\n    ])\n```", "```py\ndef quantity_ar2_process(T, ϕ1, ϕ2, q0, q_1, ϵ, df, params):\n\n    Φ = np.identity(T)  # The T x T identity matrix\n\n    for i in range(T):\n\n        if i-1 >= 0:\n            Φ[i, i-1] = -ϕ1\n\n        if i-2 >= 0:\n            Φ[i, i-2] = -ϕ2\n\n    B = np.array(df) @ params + ϵ\n\n    B[0] = B[0] + ϕ1 * q0 + ϕ2 * q_1\n    B[1] = B[1] + ϕ2 * q0\n\n    return np.linalg.inv(Φ) @ B\n\n## Quantity Demand AR(2) component process\n\n# Parameters\nT = 118 # Time periods less two lags \nϕ1 = 0.3 # Lag 1 coefficient (ϕ1)\nϕ2 = 0.05 # Lag 2 coefficient (ϕ2)\nq_1 = 250_000 # Initial Condition q_-1\nq0 = 300_000 # Initial Condition q_0\nϵ = np.random.normal(0, 5000, size=T) # Random Error (ϵ)\n\nquantity_demanded_ar = quantity_ar2_process(T,ϕ1,ϕ2,q0,q_1,ϵ,df,params)\n\n# Quantity_demanded target variable\ndf['quantity_demanded'] = quantity_demanded_ar\n\n# Additional covariates of lagged quantity demanded\ndf['quantity_demanded_lag1'] = df['quantity_demanded'].shift(1)\ndf['quantity_demanded_lag2'] = df['quantity_demanded'].shift(2)\n```", "```py\nimport statsmodels.api as stats\n\n## Fit Econometric model using OLS\n\ndf = df[2:] # Drop first two lagged values\n\ny = df['quantity_demanded']\nX = df.drop(['quantity_demanded','July'],axis=1)\n\nmod = stats.OLS(y,X)\nresults = mod.fit()\n\nprint(results.summary())\n```", "```py\n# Build Symbolic Functions with all variables in function\nδ, τ, λ, ρ  = sm.symbols('δ τ λ ρ')\n\n## Values of current variables\nprice = 180\ncomp_price = 120\nJan = 1\n\n## Obtain Lagged Values\nlog_digital_advertising_lag1 = df['log_digital_advertising_lag1'].iloc[-1]\nlog_digital_advertising_lag2 = df['log_digital_advertising_lag2'].iloc[-2]\nlog_television_advertising_lag1 = df['log_television_advertising_lag1'].iloc[-1]\nlog_television_advertising_lag2 = df['log_television_advertising_lag2'].iloc[-2]\nprice_lag1 = df['price_lag1'].iloc[-1]\nprice_lag2 = df['price_lag2'].iloc[-2]\ncomp_price_lag1 = df['comp_price_lag1'].iloc[-1]\ncomp_price_lag2 = df['comp_price_lag2'].iloc[-2]\nquantity_demanded_lag1 = df['quantity_demanded_lag1'].iloc[-1]\nquantity_demanded_lag2 = df['quantity_demanded_lag2'].iloc[-2]\n\nvariables = [sm.log(δ),\n            sm.log(τ),\n            log_digital_advertising_lag1,\n            log_digital_advertising_lag2,\n            log_television_advertising_lag1,\n            log_television_advertising_lag2,\n            price,\n            price_lag1,\n            price_lag2,\n            comp_price,\n            comp_price_lag1,\n            comp_price_lag2,\n            Jan,0,0,0,0,0,0,0,0,0,0, # All Months less July \n            1, # Constant\n            quantity_demanded_lag1,\n            quantity_demanded_lag2\n            ]\n\n# Quantity Demanded\nquantity_demanded = np.array([variables]) @ np.array(results.params) # params from ols model\nquantity_demanded = quantity_demanded[0]\n\nprint(quantity_demanded)\n```", "```py\nRevenue = price * quantity_demanded\nCost = quantity_demanded * (140 - 0.0001*quantity_demanded) + τ + δ\nprofit = Revenue - Cost\n\nprint(profit)\n```", "```py\n## Optimization Problem\n\nobjective = -profit + λ*(τ + δ - 100_000) - ρ*sm.log((τ-20_000)*(δ-10_000))\n\nsymbols = [δ, τ, λ, ρ]\nx0 = {δ:20_000, τ:80_000, λ:0, ρ:100000}\n\nresults = constrained_newton_method(objective,symbols,x0,iterations=1000)\n```", "```py\ndigital_ad = results[δ]\ntelevision_ad = results[τ]\n\nquantity = quantity_demanded.evalf(subs={δ:digital_ad,τ:television_ad})\nrevenue = Revenue.evalf(subs={δ:digital_ad,τ:television_ad})\ncost = Cost.evalf(subs={δ:digital_ad,τ:television_ad})\nprofit = revenue - cost\n\nprint(f\"Quantity: {int(quantity):,}\")\nprint(f\"Total Revenue: ${round(revenue,2):,}\")\nprint(f\"Total Cost: ${round(cost,2):,}\")\nprint(f\"Profit: ${round(profit,2):,}\")\n```", "```py\n# Pull all variables in X and create them as SymPy symbols\nvariablez = list(df.drop(['quantity_demanded','July'],axis=1).columns)\nsymbols = []\nfor i in variablez:\n    i  = sm.symbols(f'{i}')\n    symbols.append(i)\n\n# Create vectors and matrices of outcome (y), covariates (X), and parameters(β) \ny = np.array(df['quantity_demanded'])\nX = np.array(df.drop(['quantity_demanded','July'],axis=1))\nβ = np.array(symbols)\n\n# Specify objective function and starting values \nobjective = (y - X@β).T @ (y - X@β) # Residual Sum of Squares\nβ_0 = dict(zip(symbols,[0]*len(symbols))) # Initial guess (0 for all)\n\nβ_numerical = newton_method(objective,symbols,β_0)\n```", "```py\n# OLS Analytical Solution\nβ_analytical = np.linalg.inv(X.T @ X) @ X.T @ y \n\n# Compute standard errors\ndf_residuals = len(X)-len(β_analytical)\nσ2 = 1/df_residuals*((y-X@β_analytical).T @ (y-X@β_analytical)) # MSE \nΣ =  σ2 * np.linalg.inv(X.T @ X)\nstandard_errors = np.sqrt(np.diag(Σ))\n```", "```py\nols_results = pd.DataFrame()\n\nols_results['variable'] = variablez\nols_results['β_numerical'] = list(β_numerical.values())\nols_results['β_analytical'] = β_analytical\nols_results['std_err_analytical'] = standard_errors\nols_results['β_statsmodels'] = list(results.params) # from statsmodels code above\nols_results['std_err_statsmodels'] = list(results.bse) # from statsmodels code above\nols_results = ols_results.set_index('variable')\n\nols_results\n```", "```py\nimport sympy as sm\nimport numpy as np\n\ndef get_gradient(\n    function: sm.core.expr.Expr,\n    symbols: list[sm.core.symbol.Symbol],\n    x0: dict[sm.core.symbol.Symbol, float],\n) -> np.ndarray:\n    \"\"\"\n    Calculate the gradient of a function at a given point.\n\n    Args:\n        function (sm.core.expr.Expr): The function to calculate the gradient of.\n        symbols (list[sm.core.symbol.Symbol]): The symbols representing the variables in the function.\n        x0 (dict[sm.core.symbol.Symbol, float]): The point at which to calculate the gradient.\n\n    Returns:\n        numpy.ndarray: The gradient of the function at the given point.\n    \"\"\"\n    d1 = {}\n    gradient = np.array([])\n\n    for i in symbols:\n        d1[i] = sm.diff(function, i, 1).evalf(subs=x0)\n        gradient = np.append(gradient, d1[i])\n\n    return gradient.astype(np.float64)\n\ndef get_hessian(\n    function: sm.core.expr.Expr,\n    symbols: list[sm.core.symbol.Symbol],\n    x0: dict[sm.core.symbol.Symbol, float],\n) -> np.ndarray:\n    \"\"\"\n    Calculate the Hessian matrix of a function at a given point.\n\n    Args:\n    function (sm.core.expr.Expr): The function for which the Hessian matrix is calculated.\n    symbols (list[sm.core.symbol.Symbol]): The list of symbols used in the function.\n    x0 (dict[sm.core.symbol.Symbol, float]): The point at which the Hessian matrix is evaluated.\n\n    Returns:\n    numpy.ndarray: The Hessian matrix of the function at the given point.\n    \"\"\"\n    d2 = {}\n    hessian = np.array([])\n\n    for i in symbols:\n        for j in symbols:\n            d2[f\"{i}{j}\"] = sm.diff(function, i, j).evalf(subs=x0)\n            hessian = np.append(hessian, d2[f\"{i}{j}\"])\n\n    hessian = np.array(np.array_split(hessian, len(symbols)))\n\n    return hessian.astype(np.float64)\n\ndef newton_method(\n    function: sm.core.expr.Expr,\n    symbols: list[sm.core.symbol.Symbol],\n    x0: dict[sm.core.symbol.Symbol, float],\n    iterations: int = 100,\n) -> dict[sm.core.symbol.Symbol, float] or None:\n    \"\"\"\n    Perform Newton's method to find the solution to the optimization problem.\n\n    Args:\n        function (sm.core.expr.Expr): The objective function to be optimized.\n        symbols (list[sm.core.symbol.Symbol]): The symbols used in the objective function.\n        x0 (dict[sm.core.symbol.Symbol, float]): The initial values for the symbols.\n        iterations (int, optional): The maximum number of iterations. Defaults to 100.\n\n    Returns:\n        dict[sm.core.symbol.Symbol, float] or None: The solution to the optimization problem, or None if no solution is found.\n    \"\"\"\n\n    x_star = {}\n    x_star[0] = np.array(list(x0.values()))\n\n    # x = [] ## Return x for visual!\n\n    print(f\"Starting Values: {x_star[0]}\")\n\n    for i in range(iterations):\n        # x.append(dict(zip(x0.keys(),x_star[i]))) ## Return x for visual!\n\n        gradient = get_gradient(function, symbols, dict(zip(x0.keys(), x_star[i])))\n        hessian = get_hessian(function, symbols, dict(zip(x0.keys(), x_star[i])))\n\n        x_star[i + 1] = x_star[i].T - np.linalg.inv(hessian) @ gradient.T\n\n        if np.linalg.norm(x_star[i + 1] - x_star[i]) < 10e-5:\n            solution = dict(zip(x0.keys(), x_star[i + 1]))\n            print(f\"\\nConvergence Achieved ({i+1} iterations): Solution = {solution}\")\n            break\n        else:\n            solution = None\n\n        print(f\"Step {i+1}: {x_star[i+1]}\")\n\n    return solution\n\ndef constrained_newton_method(\n    function: sm.core.expr.Expr,\n    symbols: list[sm.core.symbol.Symbol],\n    x0: dict[sm.core.symbol.Symbol, float],\n    iterations: int = 100,\n) -> dict[sm.core.symbol.Symbol, float] or None:\n    \"\"\"\n    Performs constrained Newton's method to find the optimal solution of a function subject to constraints.\n\n    Parameters:\n        function (sm.core.expr.Expr): The function to optimize.\n        symbols (list[sm.core.symbol.Symbol]): The symbols used in the function.\n        x0 (dict[sm.core.symbol.Symbol, float]): The initial values for the symbols.\n        iterations (int, optional): The maximum number of iterations. Defaults to 100.\n\n    Returns:\n        dict[sm.core.symbol.Symbol, float] or None: The optimal solution if convergence is achieved, otherwise None.\n    \"\"\"\n    x_star = {}\n    x_star[0] = np.array(list(x0.values())[:-1])\n\n    optimal_solutions = []\n    optimal_solutions.append(dict(zip(list(x0.keys())[:-1], x_star[0])))\n\n    for step in range(iterations):\n        # Evaluate function at rho value\n        if step == 0:  # starting rho\n            rho_sub = list(x0.values())[-1]\n\n        rho_sub_values = {list(x0.keys())[-1]: rho_sub}\n        function_eval = function.evalf(subs=rho_sub_values)\n\n        print(f\"Step {step} w/ {rho_sub_values}\")  # Barrier method step\n        print(f\"Starting Values: {x_star[0]}\")\n\n        # Newton's Method\n        for i in range(iterations):\n            gradient = get_gradient(\n                function_eval, symbols[:-1], dict(zip(list(x0.keys())[:-1], x_star[i]))\n            )\n            hessian = get_hessian(\n                function_eval, symbols[:-1], dict(zip(list(x0.keys())[:-1], x_star[i]))\n            )\n\n            x_star[i + 1] = x_star[i].T - np.linalg.inv(hessian) @ gradient.T\n\n            if np.linalg.norm(x_star[i + 1] - x_star[i]) < 10e-5:\n                solution = dict(zip(list(x0.keys())[:-1], x_star[i + 1]))\n                print(\n                    f\"Convergence Achieved ({i+1} iterations): Solution = {solution}\\n\"\n                )\n                break\n\n        # Record optimal solution & previous optimal solution for each barrier method iteration\n        optimal_solution = x_star[i + 1]\n        previous_optimal_solution = list(optimal_solutions[step - 1].values())\n        optimal_solutions.append(dict(zip(list(x0.keys())[:-1], optimal_solution)))\n\n        # Check for overall convergence\n        if np.linalg.norm(optimal_solution - previous_optimal_solution) < 10e-5:\n            print(\n                f\"\\n Overall Convergence Achieved ({step} steps): Solution = {optimal_solutions[step]}\\n\"\n            )\n            overall_solution = optimal_solutions[step]\n            break\n        else:\n            overall_solution = None\n\n        # Set new starting point\n        x_star = {}\n        x_star[0] = optimal_solution\n\n        # Update rho\n        rho_sub = 0.9 * rho_sub\n\n    return overall_solution\n```"]