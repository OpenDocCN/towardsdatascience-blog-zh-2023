- en: 'Optimization, Newton’s Method, & Profit Maximization: Part 1 — Basic Optimization
    Theory'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/optimization-newtons-method-profit-maximization-part-1-basic-optimization-theory-ff7c5f966565?source=collection_archive---------10-----------------------#2023-01-10](https://towardsdatascience.com/optimization-newtons-method-profit-maximization-part-1-basic-optimization-theory-ff7c5f966565?source=collection_archive---------10-----------------------#2023-01-10)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/e6f766d329c212d0721d49d9bd28830e.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
- en: All Images by Author
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: Learn how to solve and utilize Newton’s Method for multi-dimensional optimization
    problems
  id: totrans-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@jakepenzak?source=post_page-----ff7c5f966565--------------------------------)[![Jacob
    Pieniazek](../Images/2d9c6295d39fcaaec4e62f11c359cb29.png)](https://medium.com/@jakepenzak?source=post_page-----ff7c5f966565--------------------------------)[](https://towardsdatascience.com/?source=post_page-----ff7c5f966565--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----ff7c5f966565--------------------------------)
    [Jacob Pieniazek](https://medium.com/@jakepenzak?source=post_page-----ff7c5f966565--------------------------------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: ·
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F6f0948d99b1c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Foptimization-newtons-method-profit-maximization-part-1-basic-optimization-theory-ff7c5f966565&user=Jacob+Pieniazek&userId=6f0948d99b1c&source=post_page-6f0948d99b1c----ff7c5f966565---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----ff7c5f966565--------------------------------)
    ·14 min read·Jan 10, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fff7c5f966565&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Foptimization-newtons-method-profit-maximization-part-1-basic-optimization-theory-ff7c5f966565&user=Jacob+Pieniazek&userId=6f0948d99b1c&source=-----ff7c5f966565---------------------clap_footer-----------)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: --
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fff7c5f966565&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Foptimization-newtons-method-profit-maximization-part-1-basic-optimization-theory-ff7c5f966565&source=-----ff7c5f966565---------------------bookmark_footer-----------)'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: This article is the **1st** in a 3 part series. In the 1st part, we will be
    studying basic optimization theory. Then, in [pt. 2](/optimization-newtons-method-profit-maximization-part-2-constrained-optimization-theory-dc18613c5770),
    we will be extending this theory to constrained optimization problems. Lastly,
    in pt. 3, we will apply the optimization theory covered, as well as econometric
    and economic theory, to solve a profit maximization problem.
  id: totrans-10
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Mathematical optimization is an extremely powerful field of mathematics the
    underpins much of what we, as data scientists, implicitly, or explicitly, utilize
    on a regular basis — in fact, nearly all machine learning algorithms make use
    of optimization theory to obtain model convergence. Take, for example, a classification
    problem, we seek to *minimize* log-loss by choosing the *optimal* parameters or
    weights of the model. *In general, mathematical optimization can be thought of
    as the primary theoretical mechanism by which machines learn.* A robust understanding
    of mathematical optimization is an extremely beneficial skillset to have in the
    data scientists toolbox — it enables the data scientist to have a deeper understanding
    of many of the algorithms used today and, furthermore, to solve a *vast array
    of unique optimization problems*.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: Many of the readers may be familiar with gradient descent, or related optimization
    algorithms such as stochastic gradient descent. However, this post will discuss
    in more depth the classical Newton method for optimization, sometimes referred
    to as the Newton-Raphson method. We will, nevertheless, develop the mathematics
    behind optimization theory from the basics to gradient descent and then dive more
    into Newton’s method with implementations in python. This will serve as the necessary
    preliminaries for our excursion into constrained optimization in [**part 2**](/optimization-newtons-method-profit-maximization-part-2-constrained-optimization-theory-dc18613c5770)
    and an econometric profit-maximization problem in **part 3** of this series.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: Optimization Basics — A Simple Quadratic Function
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Mathematical optimization can be defined “as the science of determining the
    best solutions to mathematically defined problems.”[1] This may be conceptualized
    in some real-world examples as: choosing the parameters to minimize a loss function
    for a machine learning algorithm, choosing price and advertising to maximize profit,
    choosing stocks to maximize risk-adjusted financial return, etc. Formally, any
    mathematical optimization problem can be formulated abstractly as such:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/91d0602859b3778af388eb855d77cd2a.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
- en: (1)
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: 'This can be read as follows: Choose real values of the vector **x** that minimize
    the *objective function* *f*(**x**) (or maximize -*f*(**x**)) subject to the *inequality
    constraints* *g*(**x**) and *equality constraints* *h*(**x**).We will be addressing
    how to solve for constrained optimization problems in [part 2](/optimization-newtons-method-profit-maximization-part-2-constrained-optimization-theory-dc18613c5770)
    of this series — as they can make the optimization problems particularly non-trivial.
    For now, let’s look at an unconstrained single variable example — consider the
    following optimization problem:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/776e66032cdf1167135d13f03ac59c6c.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
- en: (2)
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: In this case, we want to choose the value of x that minimizes the above quadratic
    function. There are multiple ways we can go about this — first, a naïve approach
    would be to do a grid search iterating over a large range of x values and choose
    x where *f*(x) has the lowest functional value. However, this approach can quickly
    lose computational tractability as the search space increases, the function becomes
    more complex, or the dimensions increase.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: 'Alternatively, we can solve directly using calculus if a closed-form solution
    exists. That is, we can solve analytically for the value of x via calculus. By
    taking the derivative (or, as covered later, the gradient in higher dimensions)
    and setting it equal to 0 — *the first order necessary condition for a relative
    minimum —* we can solve for the relative extrema of the function. We can then
    take the second derivate (or, as covered later, the Hessian in higher dimensions)
    to determine whether this extrema is a maximum or minimum. A second derivative
    greater than 0 (or a positive definite Hessian) — *the second order necessary
    condition for a relative minimum —* implies a minimum and vice-versa. Observe:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b356d5910e63261dd7e34294b0bcc246.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
- en: (3)
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: 'We can verify this graphically for (2) above:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6b090480c2e5931b739e8fb4f3c0d907.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
- en: Notethat when multiple extrema of a function exist (i.e., multiple minimums
    or maximums), care must be taken to determine which is the global extrema — we
    will briefly discuss this issue further in this article.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: The analytical approach demonstrated above can be extended into higher dimensions
    utilizing gradients and Hessians — however, we will not be solving the closed-form
    solutions in higher dimensions — the intuition, however, remains the same. We
    will, nevertheless, be solving higher dimensional problems utilizing *iterative
    schemes.* What do I mean by *iterative schemes*? In general, a closed form (or
    analytical) solution may not exist, and certainly need not exist for a maximum
    or minimum to exist. Thus, we require a methodology to numerically solve the optimization
    problem. This leads us to the more generalized *iterative schemes* including gradient
    descent and the Newton methods.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: Iterative Optimization Schemes
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In general, there are three main categories of iterative optimization schemes.
    Namely, *zero-order*, *first-order*, and *second-order*, which make use of local
    information about the function from no derivatives, first derivatives, or second
    derivatives, respectively.[1] In order to use each iterative scheme, the function
    f(**x**) must be a continuously differentiable function to the respective degree.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，迭代优化方案主要分为三类，即 *零阶*、*一阶* 和 *二阶*，它们分别利用函数的零阶、一阶或二阶导数的局部信息。[1] 要使用每种迭代方案，函数
    f(**x**) 必须是相应阶数上连续可微的函数。
- en: '**Zero-order iterative schemes**'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '**零阶迭代方案**'
- en: '*Zero-order iterative schemes* are closely aligned with the grid-search as
    mentioned above — simply, you search over a certain range possible values of the
    value **x** to obtain the minimum functional value. As you likely suspect, these
    methods tend to be much more computationally expensive than methods that utilize
    higher orders. Needless to say, they can be reliable and easy to program. There
    are methodologies out there that improve upon the simple grid-search, see [1]
    for more information; however, we will be focusing more-so on the higher-order
    schemes.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '*零阶迭代方案* 与上述的网格搜索紧密相关——简单来说，你在一定范围内搜索可能的 **x** 值以获得最小的函数值。正如你可能猜到的，这些方法往往比使用高阶方法的计算开销大得多。不用说，它们可以是可靠的并且容易编程。市场上有一些方法可以改进简单的网格搜索，参见[1]了解更多信息；然而，我们将更多关注高阶方案。'
- en: '**First-order iterative schemes**'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '**一阶迭代方案**'
- en: '*First-order iterative schemes* are iterative schemes that utilize local information
    of the first derivatives of the objective function. Most notably, gradient descent
    methods fall under this category. For a single variable function as above, the
    gradient is just the first derivative. Generalizing this to *n* dimensions, for
    a function *f*(**x**), the gradient is the vector of first order partial derivatives:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '*一阶迭代方案* 是利用目标函数一阶导数的局部信息的迭代方案。最显著的例子是梯度下降方法。对于如上所述的单变量函数，梯度就是一阶导数。将此推广到 *n*
    维度，对于一个函数 *f*(**x**)，梯度是一阶偏导数的向量：'
- en: '![](../Images/d43e3d17e6b500055468db4db576bd0b.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d43e3d17e6b500055468db4db576bd0b.png)'
- en: (4) Gradient of a continuously differentiable function *f*(**x**)
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: (4) 连续可微函数 *f*(**x**)
- en: 'Gradient descent begins by choosing a random starting point and iteratively
    taking steps in the direction of the negative gradient of *f*(**x**) — *the steepest
    direction of the function*. Each iterative step can be represented as follows:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度下降从选择一个随机的起点开始，并在 *f*(**x**) 的负梯度方向上迭代进行——*函数的最陡方向*。每次迭代步骤可以表示如下：
- en: '![](../Images/d0c20a695835779d9d20b310cf778c7d.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d0c20a695835779d9d20b310cf778c7d.png)'
- en: (5) Gradient Descent Iterative Scheme
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: (5) 梯度下降迭代方案
- en: 'where *γ* is the respective learning rate, which controls how fast or slow
    the gradient descent algorithm “learns” at each iteration. Too large and our iterations
    can diverge uncontrollably. Too small and the iterations can take forever to converge.
    This scheme is conducted iteratively until any one or more convergence criteria
    is achieved, such as:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *γ* 是相应的学习率，控制梯度下降算法在每次迭代中“学习”的快慢。学习率过大会导致我们的迭代不受控制地发散。学习率过小则迭代可能需要很长时间才能收敛。此方案会迭代进行，直到达到一个或多个收敛准则，如：
- en: '![](../Images/855991698a9b3c03d179ffcba7c2ef8c.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/855991698a9b3c03d179ffcba7c2ef8c.png)'
- en: (6) Convergence Criteria for Iterative Optimization Schemes
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: (6) 迭代优化方案的收敛准则
- en: 'for some small epsilon threshold. Referring back to our quadratic example,
    setting our initial guess to x = 3 and the learning rate to 0.1, the steps would
    look as follows:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 对于某个小的 epsilon 阈值。回到我们的二次例子，将初始猜测设置为 x = 3 和学习率设置为 0.1，步骤如下：
- en: '![](../Images/a1ec26982129d237011ea7c103ff6bb3.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a1ec26982129d237011ea7c103ff6bb3.png)'
- en: (7)
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: (7)
- en: 'And visually:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 视觉化如下：
- en: '![](../Images/eb6bbeaa05e96ebae221a06c3e6933e0.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/eb6bbeaa05e96ebae221a06c3e6933e0.png)'
- en: Gradient descent and first-order iterative schemes are notably reliable in their
    performance. In fact, gradient descent algorithms are primarily utilized for optimization
    of loss functions in Neural Networks and ML models, and many developments have
    improved the efficacy of these algorithms. Nevertheless, they are still using
    limited local information about the function (only the first derivative). Thus,
    in higher dimension and depending on the nature of the objective function & the
    learning rate, these schemes 1) can have a slow convergence rate as they maintain
    a linear convergence rate and 2) may fail entirely to converge. Because of this,
    it is beneficial for the data scientist to expand their optimization arsenal!
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: '**Second-order iterative schemes**'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: 'As you have likely now pieced together, *Second-order iterative schemes* are
    iterative schemes that utilize local information of the first derivatives ***and***
    thesecond derivatives of the objective function. Most notably, we have the Newton
    method (NM), which makes use of the Hessian of the objective function. For a single
    variable function, the Hessian is simply the second derivative. Similar to the
    gradient, generalizing this to *n* dimensions, the Hessian is an *n* x *n symmetrical*
    matrix of the second order partial derivatives of a twice continuously differentiable
    function *f*(**x**):'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/02d6b85d3d647d76e0d3a4440e2afba7.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
- en: (8) Hessian of a twice continuously differentiable function *f*(**x**)
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: 'Now moving on to derive the NM, first recall the first order necessary condition
    for a minimum:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9716e1afa505ae541eed48d13afd769a.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
- en: (9) First-Order Necessary Condition for Relative Minimum at x*
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: 'We can approximate ***x**** using a Taylor Series expansion:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f671020420ba3b2e88ede14734e5cb88.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
- en: (10)
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: 'Each iterative addition of delta, **Δ,** is an expected better approximation
    of **x***. Thus, each iterative step using the NM can be represented as follows:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/aeea601b1cde8c66b5fd09c5d76f1710.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
- en: (11) Newton Method Iterative Scheme
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: 'Referring back to our quadratic example, setting our initial guess to x = 3,
    the steps would look as follows:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a0c7a8dfd976ea5a79692fc8ba4b574c.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
- en: (12)
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: And we, elegantly, converge to the optimal solution on our first iteration.
    Note, the convergence criteria is the same regardless of scheme.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: Note that all of the optimization schemes suffer from the possibility of getting
    caught in a relative extremum, rather than the global extremum (i.e., think a
    higher order polynomial with multiple extrema (min’s and/or max’s)— we could get
    stuck in one relative extrema when, in reality, another extrema may be globally
    more optimal for our problem). There are methods developed, and always being developed,
    for dealing with global optimization, which we will not dive too deep into. You
    can use prior knowledge of the functional form to set expectations of what results
    you anticipate (i.e., If a strictly convex function has a critical point, then
    it must be a global minimum). **Nevertheless, as a general rule of thumb, it is
    always wise to iterate optimization schemes over different possible starting values
    of x and then study the stability of results, usually picking the results with
    the most optimal functional values for the problem at hand.**
  id: totrans-65
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 请注意，所有优化方案都可能陷入局部极值，而不是全局极值（即，考虑具有多个极值（最小值和/或最大值）的高阶多项式——我们可能会陷入一个局部极值，而实际上，另一个极值可能在全球范围内对我们的实际问题更为优化）。已有的方法，并且仍在不断开发，用于处理全局优化，我们将不会深入探讨。你可以利用函数形式的先验知识来设置对结果的预期（即，如果一个严格凸函数有一个临界点，则它必须是全局最小值）。**然而，作为一般经验法则，通常明智的做法是对不同的初始值x迭代优化方案，然后研究结果的稳定性，通常选择具有最优函数值的结果。**
- en: Multi-Dimensional Example — Rosenbrock’s Parabolic Valley
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 多维示例——罗森布罗克的抛物线谷
- en: 'Let’s now consider the following optimization problem of two variables:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们考虑以下两个变量的优化问题：
- en: '![](../Images/932d986307b137d667c91cffb7f002e0.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/932d986307b137d667c91cffb7f002e0.png)'
- en: (13) Rosenbrock’s Parabolic Valley
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: (13) 罗森布罗克的抛物线谷
- en: '![](../Images/74f4e8929ed07a5c8fb08b9506e49b66.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/74f4e8929ed07a5c8fb08b9506e49b66.png)'
- en: We will first solve the above optimization problem first by hand and then in
    python, both utilizing Newton’s Method.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先通过手动求解上述优化问题，然后在 Python 中进行求解，均使用牛顿法。
- en: '**Solving via Hand**'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '**通过手动求解**'
- en: 'To solve by hand, we will need to solve for the gradient, solve for the Hessian,
    choose our initial guess **Γ** = [x,y], and then iterate plugging this information
    into the NM algorithm until convergence is achieved. First, solving for the gradient,
    we have:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 要手动求解，我们需要求解梯度，求解 Hessian，选择我们的初始猜测 **Γ** = [x,y]，然后迭代将这些信息输入到 NM 算法中，直到收敛为止。首先，求解梯度，我们得到：
- en: '![](../Images/cedfd948a0bb2401ef6ccb4831579ccf.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cedfd948a0bb2401ef6ccb4831579ccf.png)'
- en: (14)
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: (14)
- en: 'Solving for the Hessian, we have:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 求解 Hessian，我们得到：
- en: '![](../Images/420358b7e714700813e65ad39ff22643.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/420358b7e714700813e65ad39ff22643.png)'
- en: (15)
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: (15)
- en: 'Setting our initial guess to **Γ** = [-1.2,1], we have:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 将我们的初始猜测设置为 **Γ** = [-1.2,1]，我们得到：
- en: '![](../Images/c34850f961e3220718d01cfeb43d05b9.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c34850f961e3220718d01cfeb43d05b9.png)'
- en: (16)
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: (16)
- en: Thus, we successfully solve for the optimal minimum of our objective function
    at **Γ*** = [1,1].
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们成功地求解了目标函数的最优最小值为 **Γ*** = [1,1]。
- en: '**Solving via Python**'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '**通过 Python 求解**'
- en: 'We will now turn to solving this problem, and generalizing it to any function,
    in python using S[ymPy](https://www.sympy.org/en/index.html) — a python library
    for symbolic mathematics. First, let’s walk through defining Rosenbrock’s parabolic
    valley and calculating the gradient & Hessian of the function:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将转向用 Python 求解这个问题，并将其推广到任何函数，使用 S[ymPy](https://www.sympy.org/en/index.html)
    —— 一个用于符号数学的 Python 库。首先，让我们定义罗森布罗克的抛物线谷，并计算该函数的梯度和 Hessian：
- en: '[PRE0]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'SymPy allows us to investigate the symbolic representation of our equations.
    For example, if we call `objective` , we will see the corresponding output:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: SymPy 允许我们调查方程的符号表示。例如，如果我们调用 `objective` ，我们将看到相应的输出：
- en: '![](../Images/9df35cc55695fea8ebbfb88b4c9ee3c8.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9df35cc55695fea8ebbfb88b4c9ee3c8.png)'
- en: Symbolic representation of function by SymPy
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: SymPy 的函数符号表示
- en: 'Additionally, SymPy allows us take the derivatives of the respective function
    utilizing the `sm.diff()` command. If we run our defined functions to obtain the
    gradient `get_gradient(objective,Gamma)` , we obtain a numpy array representing
    the gradient:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，SymPy 允许我们利用 `sm.diff()` 命令对相关函数进行求导。如果我们运行定义的函数以获得梯度 `get_gradient(objective,Gamma)`
    ，我们得到一个表示梯度的 numpy 数组：
- en: '![](../Images/21ab38bf204a0bc7b70ba2ed3655f69c.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/21ab38bf204a0bc7b70ba2ed3655f69c.png)'
- en: Gradient as solved by SymPy
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: SymPy 求解的梯度
- en: 'Accessing a specific element, we can see the symbolic representation `get_gradient(objective,
    Gamma)[0]` :'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 访问特定元素时，我们可以看到符号表示 `get_gradient(objective, Gamma)[0]`：
- en: '![](../Images/6f149886b0c1fe9de8b3c3b61e564d05.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
- en: df(**Γ**)/dx as solved by SymPy
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: 'Similarly, for the Hessian we can call `get_hessian(objective, Gamma)` :'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b8ce56d342c7cbe4e30c8e69748f863d.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
- en: Hessian as solved by SymPy
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: Accessing a specific element `get_hessian(objective,Gamma)[0][1]`
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a4359f5c2a62c74d4e04d877482c8e8e.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
- en: df(**Γ**)/dxdy as solved by SymPy
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: 'One can easily verify that the gradient and Hessian are identical to the ones
    we solved out by hand. SymPy allows for the evaluation of any function given specified
    values for the symbols. For example, we can evaluate the gradient at our initial
    guess by tweaking the function as follows:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'We can now compute our gradient given our starting point by calling `get_gradient(objective,
    Gamma, {x:-1.2,y:1.0})` :'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b7bf3a05cba745cf7b866255c08fe451.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
- en: Gradient evaluated at initial point
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: 'Similarly, for the Hessian `get_hessian(objective, Gamma, {x:-1.2,y:1.0})`
    :'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f285e3836dc3ecf69ad2b37843093ca7.png)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
- en: Hessian evaluated at initial point
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: Again, we can verify that these values are correct from our work by hand above.
    *Now we have all the ingredients necessary to code Newton’s method* (the code
    for gradient descent is given at the end of this article as well)*:*
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'We can now run the code via `newton_method(objective,Gamma,{x:-1.2,y:1})` :'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e02ab685850059438bb587571f23aa35.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
- en: Conclusion
  id: totrans-113
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There you have it! If you have made it up to this point, you now have a robust
    understanding of how to think about and abstractly formulate unconstrained mathematical
    optimization problems, along with the basic analytical approach and the more complex
    iterative methods for solving such problems. It is clear that the more information
    that we can incorporate about the function in the iterative schemes (i.e., higher
    order derivatives), the more efficient the convergence rate. ***Note that we are
    just brushing the surface of the complex world that is mathematical optimization.***
    Nevertheless, the tools we have discussed today can absolutely be utilized in
    practice and extended to higher dimensional optimization problems.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: Stay tuned for [**Part 2**](/optimization-newtons-method-profit-maximization-part-2-constrained-optimization-theory-dc18613c5770)
    **of this series** where we will extend what we have learned here to solving constrained
    optimization problems — which is an extremely practical extension on unconstrained
    optimization. In fact, most real world optimization problems will have some form
    of constraints on the choice variables. Then we will shift to **Part 3 of this
    series** where we will apply the optimization theory learned and additional econometric
    & economic theory to solve a simple example of profit maximization problem. I
    hope you have enjoyed reading this as much as I have enjoyed writing it!
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: Bonus— The Pitfalls of Newton’s Method
  id: totrans-116
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Despite the attractiveness of Newton’s method, it is not without its own pitfalls.
    Notably, two main pitfalls exists — 1) NM is not always convergent even when choosing
    starting points near the solution & 2) NM requires the computation of the Hessian
    matrix at each step which can be computationally very expensive in higher dimensions.
    For pitfall #1), a respective solution is the Modified Newton method (MNM), which
    can be loosely thought of as gradient descent where the search direction is given
    by the Newton step, **Δ**. For pitfall #2), quasi-Newton methods, such as DFP
    or BFGS, have been proposed that approximate the inverse-Hessian used at each
    step to improve computational burden. For more information see, [1].'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: Gradient Descent Function
  id: totrans-118
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Resources
  id: totrans-120
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] Snyman, J. A., & Wilke, D. N. (2019). *Practical mathematical optimization:
    Basic optimization theory and gradient-based algorithms* (2nd ed.). Springer.'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: '[2] [https://en.wikipedia.org/wiki/Gradient_descent](https://en.wikipedia.org/wiki/Gradient_descent)'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: '[3] [https://en.wikipedia.org/wiki/Newton%27s_method#:~:text=In%20numerical%20analysis%2C%20Newton%27s%20method%2C%20also%20known%20as,roots%20%28or%20zeroes%29%20of%20a%20real%20-valued%20function](https://en.wikipedia.org/wiki/Newton%27s_method#:~:text=In%20numerical%20analysis%2C%20Newton%27s%20method%2C%20also%20known%20as,roots%20%28or%20zeroes%29%20of%20a%20real%20-valued%20function).'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: '*Access all the code via this GitHub Repo:* [https://github.com/jakepenzak/Blog-Posts](https://github.com/jakepenzak/Blog-Posts)'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: '*I appreciate you reading my post! My posts on Medium seek to explore real-world
    and theoretical applications utilizing* ***econometric*** *and* ***statistical/machine
    learning*** *techniques. Additionally, I seek to provide posts on the theoretical
    underpinnings of certain methodologies via theory and simulations. Most importantly,
    I write to learn! I hope to make complex topics slightly more accessible to all.
    If you enjoyed this post, please consider* [***following me on Medium***](https://medium.com/@jakepenzak)*!*'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
