- en: 'Optimization, Newton’s Method, & Profit Maximization: Part 1 — Basic Optimization
    Theory'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/optimization-newtons-method-profit-maximization-part-1-basic-optimization-theory-ff7c5f966565?source=collection_archive---------10-----------------------#2023-01-10](https://towardsdatascience.com/optimization-newtons-method-profit-maximization-part-1-basic-optimization-theory-ff7c5f966565?source=collection_archive---------10-----------------------#2023-01-10)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/e6f766d329c212d0721d49d9bd28830e.png)'
  prefs: []
  type: TYPE_IMG
- en: All Images by Author
  prefs: []
  type: TYPE_NORMAL
- en: Learn how to solve and utilize Newton’s Method for multi-dimensional optimization
    problems
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@jakepenzak?source=post_page-----ff7c5f966565--------------------------------)[![Jacob
    Pieniazek](../Images/2d9c6295d39fcaaec4e62f11c359cb29.png)](https://medium.com/@jakepenzak?source=post_page-----ff7c5f966565--------------------------------)[](https://towardsdatascience.com/?source=post_page-----ff7c5f966565--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----ff7c5f966565--------------------------------)
    [Jacob Pieniazek](https://medium.com/@jakepenzak?source=post_page-----ff7c5f966565--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F6f0948d99b1c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Foptimization-newtons-method-profit-maximization-part-1-basic-optimization-theory-ff7c5f966565&user=Jacob+Pieniazek&userId=6f0948d99b1c&source=post_page-6f0948d99b1c----ff7c5f966565---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----ff7c5f966565--------------------------------)
    ·14 min read·Jan 10, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fff7c5f966565&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Foptimization-newtons-method-profit-maximization-part-1-basic-optimization-theory-ff7c5f966565&user=Jacob+Pieniazek&userId=6f0948d99b1c&source=-----ff7c5f966565---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fff7c5f966565&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Foptimization-newtons-method-profit-maximization-part-1-basic-optimization-theory-ff7c5f966565&source=-----ff7c5f966565---------------------bookmark_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: This article is the **1st** in a 3 part series. In the 1st part, we will be
    studying basic optimization theory. Then, in [pt. 2](/optimization-newtons-method-profit-maximization-part-2-constrained-optimization-theory-dc18613c5770),
    we will be extending this theory to constrained optimization problems. Lastly,
    in pt. 3, we will apply the optimization theory covered, as well as econometric
    and economic theory, to solve a profit maximization problem.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Mathematical optimization is an extremely powerful field of mathematics the
    underpins much of what we, as data scientists, implicitly, or explicitly, utilize
    on a regular basis — in fact, nearly all machine learning algorithms make use
    of optimization theory to obtain model convergence. Take, for example, a classification
    problem, we seek to *minimize* log-loss by choosing the *optimal* parameters or
    weights of the model. *In general, mathematical optimization can be thought of
    as the primary theoretical mechanism by which machines learn.* A robust understanding
    of mathematical optimization is an extremely beneficial skillset to have in the
    data scientists toolbox — it enables the data scientist to have a deeper understanding
    of many of the algorithms used today and, furthermore, to solve a *vast array
    of unique optimization problems*.
  prefs: []
  type: TYPE_NORMAL
- en: Many of the readers may be familiar with gradient descent, or related optimization
    algorithms such as stochastic gradient descent. However, this post will discuss
    in more depth the classical Newton method for optimization, sometimes referred
    to as the Newton-Raphson method. We will, nevertheless, develop the mathematics
    behind optimization theory from the basics to gradient descent and then dive more
    into Newton’s method with implementations in python. This will serve as the necessary
    preliminaries for our excursion into constrained optimization in [**part 2**](/optimization-newtons-method-profit-maximization-part-2-constrained-optimization-theory-dc18613c5770)
    and an econometric profit-maximization problem in **part 3** of this series.
  prefs: []
  type: TYPE_NORMAL
- en: Optimization Basics — A Simple Quadratic Function
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Mathematical optimization can be defined “as the science of determining the
    best solutions to mathematically defined problems.”[1] This may be conceptualized
    in some real-world examples as: choosing the parameters to minimize a loss function
    for a machine learning algorithm, choosing price and advertising to maximize profit,
    choosing stocks to maximize risk-adjusted financial return, etc. Formally, any
    mathematical optimization problem can be formulated abstractly as such:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/91d0602859b3778af388eb855d77cd2a.png)'
  prefs: []
  type: TYPE_IMG
- en: (1)
  prefs: []
  type: TYPE_NORMAL
- en: 'This can be read as follows: Choose real values of the vector **x** that minimize
    the *objective function* *f*(**x**) (or maximize -*f*(**x**)) subject to the *inequality
    constraints* *g*(**x**) and *equality constraints* *h*(**x**).We will be addressing
    how to solve for constrained optimization problems in [part 2](/optimization-newtons-method-profit-maximization-part-2-constrained-optimization-theory-dc18613c5770)
    of this series — as they can make the optimization problems particularly non-trivial.
    For now, let’s look at an unconstrained single variable example — consider the
    following optimization problem:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/776e66032cdf1167135d13f03ac59c6c.png)'
  prefs: []
  type: TYPE_IMG
- en: (2)
  prefs: []
  type: TYPE_NORMAL
- en: In this case, we want to choose the value of x that minimizes the above quadratic
    function. There are multiple ways we can go about this — first, a naïve approach
    would be to do a grid search iterating over a large range of x values and choose
    x where *f*(x) has the lowest functional value. However, this approach can quickly
    lose computational tractability as the search space increases, the function becomes
    more complex, or the dimensions increase.
  prefs: []
  type: TYPE_NORMAL
- en: 'Alternatively, we can solve directly using calculus if a closed-form solution
    exists. That is, we can solve analytically for the value of x via calculus. By
    taking the derivative (or, as covered later, the gradient in higher dimensions)
    and setting it equal to 0 — *the first order necessary condition for a relative
    minimum —* we can solve for the relative extrema of the function. We can then
    take the second derivate (or, as covered later, the Hessian in higher dimensions)
    to determine whether this extrema is a maximum or minimum. A second derivative
    greater than 0 (or a positive definite Hessian) — *the second order necessary
    condition for a relative minimum —* implies a minimum and vice-versa. Observe:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b356d5910e63261dd7e34294b0bcc246.png)'
  prefs: []
  type: TYPE_IMG
- en: (3)
  prefs: []
  type: TYPE_NORMAL
- en: 'We can verify this graphically for (2) above:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6b090480c2e5931b739e8fb4f3c0d907.png)'
  prefs: []
  type: TYPE_IMG
- en: Notethat when multiple extrema of a function exist (i.e., multiple minimums
    or maximums), care must be taken to determine which is the global extrema — we
    will briefly discuss this issue further in this article.
  prefs: []
  type: TYPE_NORMAL
- en: The analytical approach demonstrated above can be extended into higher dimensions
    utilizing gradients and Hessians — however, we will not be solving the closed-form
    solutions in higher dimensions — the intuition, however, remains the same. We
    will, nevertheless, be solving higher dimensional problems utilizing *iterative
    schemes.* What do I mean by *iterative schemes*? In general, a closed form (or
    analytical) solution may not exist, and certainly need not exist for a maximum
    or minimum to exist. Thus, we require a methodology to numerically solve the optimization
    problem. This leads us to the more generalized *iterative schemes* including gradient
    descent and the Newton methods.
  prefs: []
  type: TYPE_NORMAL
- en: Iterative Optimization Schemes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In general, there are three main categories of iterative optimization schemes.
    Namely, *zero-order*, *first-order*, and *second-order*, which make use of local
    information about the function from no derivatives, first derivatives, or second
    derivatives, respectively.[1] In order to use each iterative scheme, the function
    f(**x**) must be a continuously differentiable function to the respective degree.
  prefs: []
  type: TYPE_NORMAL
- en: '**Zero-order iterative schemes**'
  prefs: []
  type: TYPE_NORMAL
- en: '*Zero-order iterative schemes* are closely aligned with the grid-search as
    mentioned above — simply, you search over a certain range possible values of the
    value **x** to obtain the minimum functional value. As you likely suspect, these
    methods tend to be much more computationally expensive than methods that utilize
    higher orders. Needless to say, they can be reliable and easy to program. There
    are methodologies out there that improve upon the simple grid-search, see [1]
    for more information; however, we will be focusing more-so on the higher-order
    schemes.'
  prefs: []
  type: TYPE_NORMAL
- en: '**First-order iterative schemes**'
  prefs: []
  type: TYPE_NORMAL
- en: '*First-order iterative schemes* are iterative schemes that utilize local information
    of the first derivatives of the objective function. Most notably, gradient descent
    methods fall under this category. For a single variable function as above, the
    gradient is just the first derivative. Generalizing this to *n* dimensions, for
    a function *f*(**x**), the gradient is the vector of first order partial derivatives:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d43e3d17e6b500055468db4db576bd0b.png)'
  prefs: []
  type: TYPE_IMG
- en: (4) Gradient of a continuously differentiable function *f*(**x**)
  prefs: []
  type: TYPE_NORMAL
- en: 'Gradient descent begins by choosing a random starting point and iteratively
    taking steps in the direction of the negative gradient of *f*(**x**) — *the steepest
    direction of the function*. Each iterative step can be represented as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d0c20a695835779d9d20b310cf778c7d.png)'
  prefs: []
  type: TYPE_IMG
- en: (5) Gradient Descent Iterative Scheme
  prefs: []
  type: TYPE_NORMAL
- en: 'where *γ* is the respective learning rate, which controls how fast or slow
    the gradient descent algorithm “learns” at each iteration. Too large and our iterations
    can diverge uncontrollably. Too small and the iterations can take forever to converge.
    This scheme is conducted iteratively until any one or more convergence criteria
    is achieved, such as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/855991698a9b3c03d179ffcba7c2ef8c.png)'
  prefs: []
  type: TYPE_IMG
- en: (6) Convergence Criteria for Iterative Optimization Schemes
  prefs: []
  type: TYPE_NORMAL
- en: 'for some small epsilon threshold. Referring back to our quadratic example,
    setting our initial guess to x = 3 and the learning rate to 0.1, the steps would
    look as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a1ec26982129d237011ea7c103ff6bb3.png)'
  prefs: []
  type: TYPE_IMG
- en: (7)
  prefs: []
  type: TYPE_NORMAL
- en: 'And visually:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/eb6bbeaa05e96ebae221a06c3e6933e0.png)'
  prefs: []
  type: TYPE_IMG
- en: Gradient descent and first-order iterative schemes are notably reliable in their
    performance. In fact, gradient descent algorithms are primarily utilized for optimization
    of loss functions in Neural Networks and ML models, and many developments have
    improved the efficacy of these algorithms. Nevertheless, they are still using
    limited local information about the function (only the first derivative). Thus,
    in higher dimension and depending on the nature of the objective function & the
    learning rate, these schemes 1) can have a slow convergence rate as they maintain
    a linear convergence rate and 2) may fail entirely to converge. Because of this,
    it is beneficial for the data scientist to expand their optimization arsenal!
  prefs: []
  type: TYPE_NORMAL
- en: '**Second-order iterative schemes**'
  prefs: []
  type: TYPE_NORMAL
- en: 'As you have likely now pieced together, *Second-order iterative schemes* are
    iterative schemes that utilize local information of the first derivatives ***and***
    thesecond derivatives of the objective function. Most notably, we have the Newton
    method (NM), which makes use of the Hessian of the objective function. For a single
    variable function, the Hessian is simply the second derivative. Similar to the
    gradient, generalizing this to *n* dimensions, the Hessian is an *n* x *n symmetrical*
    matrix of the second order partial derivatives of a twice continuously differentiable
    function *f*(**x**):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/02d6b85d3d647d76e0d3a4440e2afba7.png)'
  prefs: []
  type: TYPE_IMG
- en: (8) Hessian of a twice continuously differentiable function *f*(**x**)
  prefs: []
  type: TYPE_NORMAL
- en: 'Now moving on to derive the NM, first recall the first order necessary condition
    for a minimum:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9716e1afa505ae541eed48d13afd769a.png)'
  prefs: []
  type: TYPE_IMG
- en: (9) First-Order Necessary Condition for Relative Minimum at x*
  prefs: []
  type: TYPE_NORMAL
- en: 'We can approximate ***x**** using a Taylor Series expansion:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f671020420ba3b2e88ede14734e5cb88.png)'
  prefs: []
  type: TYPE_IMG
- en: (10)
  prefs: []
  type: TYPE_NORMAL
- en: 'Each iterative addition of delta, **Δ,** is an expected better approximation
    of **x***. Thus, each iterative step using the NM can be represented as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/aeea601b1cde8c66b5fd09c5d76f1710.png)'
  prefs: []
  type: TYPE_IMG
- en: (11) Newton Method Iterative Scheme
  prefs: []
  type: TYPE_NORMAL
- en: 'Referring back to our quadratic example, setting our initial guess to x = 3,
    the steps would look as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a0c7a8dfd976ea5a79692fc8ba4b574c.png)'
  prefs: []
  type: TYPE_IMG
- en: (12)
  prefs: []
  type: TYPE_NORMAL
- en: And we, elegantly, converge to the optimal solution on our first iteration.
    Note, the convergence criteria is the same regardless of scheme.
  prefs: []
  type: TYPE_NORMAL
- en: Note that all of the optimization schemes suffer from the possibility of getting
    caught in a relative extremum, rather than the global extremum (i.e., think a
    higher order polynomial with multiple extrema (min’s and/or max’s)— we could get
    stuck in one relative extrema when, in reality, another extrema may be globally
    more optimal for our problem). There are methods developed, and always being developed,
    for dealing with global optimization, which we will not dive too deep into. You
    can use prior knowledge of the functional form to set expectations of what results
    you anticipate (i.e., If a strictly convex function has a critical point, then
    it must be a global minimum). **Nevertheless, as a general rule of thumb, it is
    always wise to iterate optimization schemes over different possible starting values
    of x and then study the stability of results, usually picking the results with
    the most optimal functional values for the problem at hand.**
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Multi-Dimensional Example — Rosenbrock’s Parabolic Valley
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s now consider the following optimization problem of two variables:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/932d986307b137d667c91cffb7f002e0.png)'
  prefs: []
  type: TYPE_IMG
- en: (13) Rosenbrock’s Parabolic Valley
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/74f4e8929ed07a5c8fb08b9506e49b66.png)'
  prefs: []
  type: TYPE_IMG
- en: We will first solve the above optimization problem first by hand and then in
    python, both utilizing Newton’s Method.
  prefs: []
  type: TYPE_NORMAL
- en: '**Solving via Hand**'
  prefs: []
  type: TYPE_NORMAL
- en: 'To solve by hand, we will need to solve for the gradient, solve for the Hessian,
    choose our initial guess **Γ** = [x,y], and then iterate plugging this information
    into the NM algorithm until convergence is achieved. First, solving for the gradient,
    we have:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/cedfd948a0bb2401ef6ccb4831579ccf.png)'
  prefs: []
  type: TYPE_IMG
- en: (14)
  prefs: []
  type: TYPE_NORMAL
- en: 'Solving for the Hessian, we have:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/420358b7e714700813e65ad39ff22643.png)'
  prefs: []
  type: TYPE_IMG
- en: (15)
  prefs: []
  type: TYPE_NORMAL
- en: 'Setting our initial guess to **Γ** = [-1.2,1], we have:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c34850f961e3220718d01cfeb43d05b9.png)'
  prefs: []
  type: TYPE_IMG
- en: (16)
  prefs: []
  type: TYPE_NORMAL
- en: Thus, we successfully solve for the optimal minimum of our objective function
    at **Γ*** = [1,1].
  prefs: []
  type: TYPE_NORMAL
- en: '**Solving via Python**'
  prefs: []
  type: TYPE_NORMAL
- en: 'We will now turn to solving this problem, and generalizing it to any function,
    in python using S[ymPy](https://www.sympy.org/en/index.html) — a python library
    for symbolic mathematics. First, let’s walk through defining Rosenbrock’s parabolic
    valley and calculating the gradient & Hessian of the function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'SymPy allows us to investigate the symbolic representation of our equations.
    For example, if we call `objective` , we will see the corresponding output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9df35cc55695fea8ebbfb88b4c9ee3c8.png)'
  prefs: []
  type: TYPE_IMG
- en: Symbolic representation of function by SymPy
  prefs: []
  type: TYPE_NORMAL
- en: 'Additionally, SymPy allows us take the derivatives of the respective function
    utilizing the `sm.diff()` command. If we run our defined functions to obtain the
    gradient `get_gradient(objective,Gamma)` , we obtain a numpy array representing
    the gradient:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/21ab38bf204a0bc7b70ba2ed3655f69c.png)'
  prefs: []
  type: TYPE_IMG
- en: Gradient as solved by SymPy
  prefs: []
  type: TYPE_NORMAL
- en: 'Accessing a specific element, we can see the symbolic representation `get_gradient(objective,
    Gamma)[0]` :'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6f149886b0c1fe9de8b3c3b61e564d05.png)'
  prefs: []
  type: TYPE_IMG
- en: df(**Γ**)/dx as solved by SymPy
  prefs: []
  type: TYPE_NORMAL
- en: 'Similarly, for the Hessian we can call `get_hessian(objective, Gamma)` :'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b8ce56d342c7cbe4e30c8e69748f863d.png)'
  prefs: []
  type: TYPE_IMG
- en: Hessian as solved by SymPy
  prefs: []
  type: TYPE_NORMAL
- en: Accessing a specific element `get_hessian(objective,Gamma)[0][1]`
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a4359f5c2a62c74d4e04d877482c8e8e.png)'
  prefs: []
  type: TYPE_IMG
- en: df(**Γ**)/dxdy as solved by SymPy
  prefs: []
  type: TYPE_NORMAL
- en: 'One can easily verify that the gradient and Hessian are identical to the ones
    we solved out by hand. SymPy allows for the evaluation of any function given specified
    values for the symbols. For example, we can evaluate the gradient at our initial
    guess by tweaking the function as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now compute our gradient given our starting point by calling `get_gradient(objective,
    Gamma, {x:-1.2,y:1.0})` :'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b7bf3a05cba745cf7b866255c08fe451.png)'
  prefs: []
  type: TYPE_IMG
- en: Gradient evaluated at initial point
  prefs: []
  type: TYPE_NORMAL
- en: 'Similarly, for the Hessian `get_hessian(objective, Gamma, {x:-1.2,y:1.0})`
    :'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f285e3836dc3ecf69ad2b37843093ca7.png)'
  prefs: []
  type: TYPE_IMG
- en: Hessian evaluated at initial point
  prefs: []
  type: TYPE_NORMAL
- en: Again, we can verify that these values are correct from our work by hand above.
    *Now we have all the ingredients necessary to code Newton’s method* (the code
    for gradient descent is given at the end of this article as well)*:*
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now run the code via `newton_method(objective,Gamma,{x:-1.2,y:1})` :'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e02ab685850059438bb587571f23aa35.png)'
  prefs: []
  type: TYPE_IMG
- en: Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There you have it! If you have made it up to this point, you now have a robust
    understanding of how to think about and abstractly formulate unconstrained mathematical
    optimization problems, along with the basic analytical approach and the more complex
    iterative methods for solving such problems. It is clear that the more information
    that we can incorporate about the function in the iterative schemes (i.e., higher
    order derivatives), the more efficient the convergence rate. ***Note that we are
    just brushing the surface of the complex world that is mathematical optimization.***
    Nevertheless, the tools we have discussed today can absolutely be utilized in
    practice and extended to higher dimensional optimization problems.
  prefs: []
  type: TYPE_NORMAL
- en: Stay tuned for [**Part 2**](/optimization-newtons-method-profit-maximization-part-2-constrained-optimization-theory-dc18613c5770)
    **of this series** where we will extend what we have learned here to solving constrained
    optimization problems — which is an extremely practical extension on unconstrained
    optimization. In fact, most real world optimization problems will have some form
    of constraints on the choice variables. Then we will shift to **Part 3 of this
    series** where we will apply the optimization theory learned and additional econometric
    & economic theory to solve a simple example of profit maximization problem. I
    hope you have enjoyed reading this as much as I have enjoyed writing it!
  prefs: []
  type: TYPE_NORMAL
- en: Bonus— The Pitfalls of Newton’s Method
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Despite the attractiveness of Newton’s method, it is not without its own pitfalls.
    Notably, two main pitfalls exists — 1) NM is not always convergent even when choosing
    starting points near the solution & 2) NM requires the computation of the Hessian
    matrix at each step which can be computationally very expensive in higher dimensions.
    For pitfall #1), a respective solution is the Modified Newton method (MNM), which
    can be loosely thought of as gradient descent where the search direction is given
    by the Newton step, **Δ**. For pitfall #2), quasi-Newton methods, such as DFP
    or BFGS, have been proposed that approximate the inverse-Hessian used at each
    step to improve computational burden. For more information see, [1].'
  prefs: []
  type: TYPE_NORMAL
- en: Gradient Descent Function
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Resources
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] Snyman, J. A., & Wilke, D. N. (2019). *Practical mathematical optimization:
    Basic optimization theory and gradient-based algorithms* (2nd ed.). Springer.'
  prefs: []
  type: TYPE_NORMAL
- en: '[2] [https://en.wikipedia.org/wiki/Gradient_descent](https://en.wikipedia.org/wiki/Gradient_descent)'
  prefs: []
  type: TYPE_NORMAL
- en: '[3] [https://en.wikipedia.org/wiki/Newton%27s_method#:~:text=In%20numerical%20analysis%2C%20Newton%27s%20method%2C%20also%20known%20as,roots%20%28or%20zeroes%29%20of%20a%20real%20-valued%20function](https://en.wikipedia.org/wiki/Newton%27s_method#:~:text=In%20numerical%20analysis%2C%20Newton%27s%20method%2C%20also%20known%20as,roots%20%28or%20zeroes%29%20of%20a%20real%20-valued%20function).'
  prefs: []
  type: TYPE_NORMAL
- en: '*Access all the code via this GitHub Repo:* [https://github.com/jakepenzak/Blog-Posts](https://github.com/jakepenzak/Blog-Posts)'
  prefs: []
  type: TYPE_NORMAL
- en: '*I appreciate you reading my post! My posts on Medium seek to explore real-world
    and theoretical applications utilizing* ***econometric*** *and* ***statistical/machine
    learning*** *techniques. Additionally, I seek to provide posts on the theoretical
    underpinnings of certain methodologies via theory and simulations. Most importantly,
    I write to learn! I hope to make complex topics slightly more accessible to all.
    If you enjoyed this post, please consider* [***following me on Medium***](https://medium.com/@jakepenzak)*!*'
  prefs: []
  type: TYPE_NORMAL
