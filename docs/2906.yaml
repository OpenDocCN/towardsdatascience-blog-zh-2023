- en: Balancing Innovation With Safety & Privacy in the Era of Large Language Models
    (LLM)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/balancing-innovation-with-safety-privacy-in-the-era-of-large-language-models-llm-a63570e4a24a?source=collection_archive---------2-----------------------#2023-09-20](https://towardsdatascience.com/balancing-innovation-with-safety-privacy-in-the-era-of-large-language-models-llm-a63570e4a24a?source=collection_archive---------2-----------------------#2023-09-20)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: A Guide to Implement Safety, and Privacy Mechanisms for your Generative AI applications
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@anjanava.biswas?source=post_page-----a63570e4a24a--------------------------------)[![Anjan
    Biswas](../Images/d8ba1231ad138fdc44a95ce12adeb1e2.png)](https://medium.com/@anjanava.biswas?source=post_page-----a63570e4a24a--------------------------------)[](https://towardsdatascience.com/?source=post_page-----a63570e4a24a--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----a63570e4a24a--------------------------------)
    [Anjan Biswas](https://medium.com/@anjanava.biswas?source=post_page-----a63570e4a24a--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fdbbc0b48552b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbalancing-innovation-with-safety-privacy-in-the-era-of-large-language-models-llm-a63570e4a24a&user=Anjan+Biswas&userId=dbbc0b48552b&source=post_page-dbbc0b48552b----a63570e4a24a---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----a63570e4a24a--------------------------------)
    ·12 min read·Sep 20, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fa63570e4a24a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbalancing-innovation-with-safety-privacy-in-the-era-of-large-language-models-llm-a63570e4a24a&user=Anjan+Biswas&userId=dbbc0b48552b&source=-----a63570e4a24a---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fa63570e4a24a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbalancing-innovation-with-safety-privacy-in-the-era-of-large-language-models-llm-a63570e4a24a&source=-----a63570e4a24a---------------------bookmark_footer-----------)![](../Images/a667a614c9ee7df2114edacede095dca.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Photo by [Jason Dent](https://unsplash.com/@jdent?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: The AI era has ushered in Large Language Models (aka LLMs) to the technological
    forefront, which has been much of the talk in 2023, and is likely to remain as
    such for many years to come. LLMs are the AI models that are the power house behind
    things like [ChatGPT](https://chat.openai.com/). These AI models, fueled by vast
    amounts of data and computational prowess, have unlocked remarkable capabilities,
    from human-like text generating to assisting with natural language understanding
    (NLU) tasks. They have quickly become the foundation upon which countless applications
    and software services are being built, or at least being augmented with.
  prefs: []
  type: TYPE_NORMAL
- en: However, as with any groundbreaking innovations, the rise of LLMs brings forth
    a critical question — *“How do we balance this pursuit of technological advancement
    with the imperative of safety and privacy?”.* This is not a mere philosophical
    question, but a challenge that requires proactive and thoughtful action.
  prefs: []
  type: TYPE_NORMAL
- en: Safety and privacy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To prioritize safety and privacy in our LLM powered applications, we’ll be honing
    in on key areas, including controlling the spread of personal data ([personally
    identifiable information](https://www.dol.gov/general/ppii), a.k.a. PII) and harmful
    or toxic content. This is essential whether you’re fine-tuning an LLM with your
    own dataset or simply using an LLM for text generation tasks. *Why does this matter?*
    There are a few reasons as to why it could be important.
  prefs: []
  type: TYPE_NORMAL
- en: Compliance with government regulations that mandates protection of user personal
    information (such as [GDPR](https://gdpr-info.eu/), [CCPA](https://oag.ca.gov/privacy/ccpa),
    [HIPAA Privacy Rule](https://www.hhs.gov/hipaa/for-professionals/privacy/special-topics/de-identification/index.html)
    etc.)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Compliance with LLM provider End-User License Agreement (EULA) or Acceptable
    Use Policy (AUP)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Comply with InfoSec policies set within organizations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mitigate possibility of bias and skew in your model; post fine tuning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ensure the ethical use of LLMs and preserve brand reputation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Be prepared for any [AI regulation](https://www.barrons.com/articles/washington-ai-regulation-meetings-senate-33e8c1a5)
    that may be in the horizon
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Considerations for fine tuning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When preparing for fine-tuning an LLM, the first step is data preparation. Outside
    of research, education, or personal projects, it is likely that you will run into
    situations where your training data may contain PII information. The first step
    here is to identify the existence of these PII entities in the data, and the second
    step is to scrub the data to ensure that these PII entities are anonymized properly.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6e37eaa775478d72974668177f2337e8.png)'
  prefs: []
  type: TYPE_IMG
- en: LLM fine tuning
  prefs: []
  type: TYPE_NORMAL
- en: Considerations for text generation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For text generation using LLMs, there are a couple if things to keep in mind.
    First, we ensure that any prompt containing toxic content are restricted from
    propagating to the LLM, and second we ensure that our prompt is free of any PII
    entities. Consecutively, in some cases, it may be appropriate to run these validations
    **on the text generated by the LLM**, or on the “*machine generated text”.* This
    gives a dual layer of protection in ensuring our ethos of safety and privacy.
    A third aspect is of determining the *intent* of the prompt itself which may,
    to some extent, curtail things like [*prompt injection*](https://arxiv.org/pdf/2306.05499.pdf)attacks*.*
    However, I will primarily focus on PII and toxicity in this article and discuss
    *intent* classification and it’s effect on LLMs in a separate discussion.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f5438261e7c25eff870f22c2a90bc311.png)'
  prefs: []
  type: TYPE_IMG
- en: Text generation with LLM
  prefs: []
  type: TYPE_NORMAL
- en: Implementation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will take a two step approach in order to implement a solution for this.
    First, we make use of a **name entity recognition (NER)** model that can identify
    PII entities in the text and allows us to anonymize the entities. PII entities
    usually includes things like person name, location or address, phone number, credit
    card number, SSN and so on. Second, we use a **text classification** model to
    classify if a text is `toxic` or `neutral`. Examples of toxic text typically are
    text that contain abuse, obscenities, harassment, bullying and so forth.
  prefs: []
  type: TYPE_NORMAL
- en: For the PII NER model, a most common choice would be a [BERT Base model](https://huggingface.co/bert-base-uncased)
    that can be fine tuned to detect specific PII entities. You can also fine tune
    pre-trained [transformer](https://huggingface.co/docs/transformers/index) models
    such as the [Robust DeID](https://huggingface.co/obi/deid_roberta_i2b2) (de-identification)
    pre-trained model which is a [RoBERTa](https://arxiv.org/pdf/1907.11692.pdf) model
    fine-tuned for de-identification of medical notes and mostly focuses on personal
    health information (a.k.a PHI). A much simpler option to begin experimenting would
    be using [spaCy ER (EntityRecognizer)](https://spacy.io/api/entityrecognizer).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: which gives us
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d79f386de85671b87d776398390aa039.png)'
  prefs: []
  type: TYPE_IMG
- en: Annotation of PII entities detected by spaCy
  prefs: []
  type: TYPE_NORMAL
- en: spaCy `EntityRecognizer` was able to identify three entities — `PERSON` (People,
    including fictional characters), `FAC` (Location or address), and `CARDINAL` (Numerals
    that do not fall under another type). spaCy also gives us the start and end offset
    (character position in the text) of the detected entity which we can use to perform
    anonymization.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: which gives us
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: But there are a few obvious issues here. spaCy ER’s default entity list is not
    exhaustive to cover for all types of PII entities. For example, in our case we
    would like to detect `555-123-1290` as a `PHONE_NUMBER` as opposed to just part
    of the text as `CARDINAL` leading to incomplete entity detection. Of-course, just
    like the transformer based NER models, spaCy can also be trained with your own
    dataset of custom name entities in order to make it more robust. However, we will
    use **open-source** [**Presidio**](https://microsoft.github.io/presidio/) **SDK**
    which is a more purpose-built toolkit for *data protection* and *de-identification*.
  prefs: []
  type: TYPE_NORMAL
- en: PII detection and anonymization with Presidio
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The Presidio SDK provides a full set of PII detection capabilities, with a long
    list of [supported PII entities](https://microsoft.github.io/presidio/supported_entities/).
    Presidio primarily uses pattern matching, along with ML capabilities of spaCy
    and [Stanza](https://stanfordnlp.github.io/stanza/). However, Presidio is customizable
    and can be plugged-in to use your transformer based PII entity recognition model,
    or with even cloud based PII capabilities such as [Azure Text Analytics PII detection](https://learn.microsoft.com/en-us/azure/ai-services/language-service/personally-identifiable-information/how-to-call),
    or [Amazon Comprehend PII detection](https://docs.aws.amazon.com/comprehend/latest/dg/how-pii.html).
    It also comes with a built-in customizable anonymizer that can help scrub and
    redact PII entities from text.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: which gives us
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: and
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f00a219f85855b7d41a62eb8927cf806.png)'
  prefs: []
  type: TYPE_IMG
- en: Annotation of PII entities detected by Presidio
  prefs: []
  type: TYPE_NORMAL
- en: As we’ve seen before, it’s a rather trivial task to anonymize the text since
    we have the beginning and end offsets of each of the entities within the text.
    However, we are going to make use of Presidio’s built-in `AnonymizerEngine` to
    help us with this.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: which gives us
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: This so far is great, but what if we want the anonymization to be just plain
    masking. In that case we can pass in custom configuration to the `AnonymizerEngine`
    which can perform simple masking of the PII entities. For example, we mask the
    entities with the asterisk (`*`) characters only.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: gives us
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Considerations for anonymization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There are a few things to keep in mind when you decide to anonymize PII entities
    in the text.
  prefs: []
  type: TYPE_NORMAL
- en: Presidio’s default `AnonymizerEngine` uses a pattern `<ENTITY_LABEL>` to mask
    the PII entities (like `<PHONE_NUMBER>` ). This can potentially cause issues especially
    with LLM fine-tuning. Replacing PII with entity type labels can introduce words
    that carry semantic meaning, potentially affecting the behavior of language models.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[***Pseudonymization***](https://en.wikipedia.org/wiki/Pseudonymization)is
    a useful tool for data protection, however you should exercise caution performing
    pseudonymization on your training data. For example, replacing all `NAME` entities
    with the pseudonym `John Doe` , or replacing all `DATE` entities with `01-JAN-2000`
    in your fine-tuning data may lead to extreme bias in your fine-tuned model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Be aware of how your LLM reacts to certain characters or patterns in your prompt.
    Some LLMs may need a very specific way of templating prompts to get the most out
    of the model, for example Anthropic recommends using [*prompt tags*](https://docs.anthropic.com/claude/docs/constructing-a-prompt#mark-different-parts-of-the-prompt)*.*
    Being aware of this will help decide how you may want to perform anonymization.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There could be other general side effects of anonymized data on model fine-tuning
    such as *loss of context*, *semantic drift*, *model* *hallucinations* and so on.
    It is important to iterate and experiment to see what level of anonymization is
    appropriate for your needs, while minimizing it’s negative effects on the model’s
    performance.
  prefs: []
  type: TYPE_NORMAL
- en: Toxicity detection with text classification
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In order to identify whether a text contains toxic content or not, we will use
    a binary classification approach — `0` if the text is neutral, `1` if the text
    is toxic. I decided to train a [DistilBERT base model (uncased)](https://arxiv.org/abs/1910.01108)
    which is a distilled version of a [BERT base model](https://arxiv.org/abs/1810.04805).
    For training data, I used the [Jigsaw dataset](https://www.kaggle.com/c/jigsaw-multilingual-toxic-comment-classification).
  prefs: []
  type: TYPE_NORMAL
- en: I won’t go into the details of how the model was trained and model metrics etc.
    however you can refer to this article on [training a DistilBERT base model](https://medium.com/geekculture/hugging-face-distilbert-tensorflow-for-custom-text-classification-1ad4a49e26a7)
    for text-classification tasks. You can see the model training script I wrote [here](https://github.com/annjawn/llm-safety-privacy/blob/main/toxicity.ipynb).
    The model is available in HuggingFace Hub as `[tensor-trek/distilbert-toxicity-classifier](https://huggingface.co/tensor-trek/distilbert-toxicity-classifier)`.
    Let’s run a few sample pieces of text through inference to check what the model
    tells us.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: which gives us —
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: The model is correctly classifying the text as `NEUTRAL` or `TOXIC` with pretty
    high confidence. This text classification model, in conjunction to our previously
    discussed PII entity classification can now be used to create a mechanism that
    can enforce privacy and safety within our LLM powered applications or services.
  prefs: []
  type: TYPE_NORMAL
- en: Putting things together
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We’ve tackled *privacy* through a PII entity recognition mechanism, and we tackled
    the *safety* part with a text toxicity classifier. You can think of other mechanisms
    that may be relevant to your organization’s definition of safety & privacy. For
    example, healthcare organizations may be more concerned about PHI instead of PII
    and so forth. Ultimately, the overall implementation approach for this remains
    the same no matter what controls you want to introduce.
  prefs: []
  type: TYPE_NORMAL
- en: With that in mind, it is now time to put everything together into action. We
    want to be able use both the privacy and safety mechanisms in conjunction with
    an LLM for an application where we want to introduce generative AI capabilities.
    I am going to use the popular [LangChain](https://www.langchain.com/) framework’s
    [Python flavor](https://python.langchain.com/docs/get_started/introduction) (also
    available in [JavaScript/TS](https://js.langchain.com/docs/get_started/introduction/))
    to build a generative AI application which will include the two mechanisms. Here’s
    how our overall architecture looks like.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0184314e1d57b457e6e7fa7e1113517e.png)'
  prefs: []
  type: TYPE_IMG
- en: Privacy and safety flow with LangChain
  prefs: []
  type: TYPE_NORMAL
- en: In the above architecture, the first thing I check is if the text contains toxic
    content with at least more than 80% of model accuracy. If so, the execution of
    the whole LangChain application stops at that point, and the user is shown an
    appropriate message. If the text is classified largely as *neutral,* then I pass
    it onto the next step of identifying PII entities. I then perform anonymization
    on those entities in the text if the confidence score of each of these entity
    detection is more than 50%. Once the text is fully anonymized, it is passed as
    a prompt to the LLM for further text generation by the model. Note that the accuracy
    thresholds (80% and 50%) are arbitrary, you would want to test the accuracy of
    both the detectors (PII & toxicity) on your data and decide on a threshold that
    works best for your use-case. *The lower the threshold, the stricter the system
    becomes, the higher the threshold the weaker the enforcement of these checks*.
  prefs: []
  type: TYPE_NORMAL
- en: Another more conservative approach would be to stop the execution if any PII
    entities are detected. This could be useful for applications that are not certified
    to handle PII data at all and you want to ensure, no matter how, text containing
    PII doesn’t end up being fed into the application as an input.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b387590c1db20c3bec017720c12e683e.png)'
  prefs: []
  type: TYPE_IMG
- en: Privacy and safety flow with LangChain — ***alternate flow***
  prefs: []
  type: TYPE_NORMAL
- en: LangChain implementation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In order to make it work with LangChain, I created a [custom *chain*](https://python.langchain.com/docs/modules/chains/how_to/custom_chain)called
    `PrivacyAndSafetyChain` . This can be chained with any [LangChain supported LLMs](https://python.langchain.com/docs/modules/model_io/models/llms/)
    to implement a privacy and safety mechanism. This is how it looks like —
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: By default, `PrivacyAndSafetyChain` performs toxicity detection first. If it
    detects any toxic content then it will error out essentially stopping the chain
    as we discussed earlier. If not, then it passess the entered text to the PII entity
    recognizer and based on what masking character to use, the chain will perform
    anonymization of the text with the detected PII entities. The output of the preceding
    code is as shown below. Since there is no toxic content the chain didn’t stop,
    and it detected `PHONE_NUMBER` and `SSN` and correctly anonymized it.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The biggest takeaway in this post is that as we continue to innovate with Large
    Language Models, it becomes imperative to balance the scales of innovation with
    safety and privacy. The enthusiasm surrounding LLMs, and our ever-growing desire
    to integrate them with a whole world of possible use-cases is un-deniable. However,
    the potential pitfalls — like data privacy breaches, unintended biases, or misuse
    — are equally real and warrant our immediate attention. I covered how you can
    establish a mechanism of detecting PII and toxic content going into your LLMs
    and discussed an implementation with LangChain.
  prefs: []
  type: TYPE_NORMAL
- en: There’s still much research and development that remains to be done— perhaps
    a better architecture, more reliable and seamless way to ensure data privacy and
    safety. The code in this post is trimmed down for brevity, but I encourage you
    to checkout my [GitHub repository](https://github.com/annjawn/llm-safety-privacy/tree/main)
    where I have collated detailed Notebooks on each step along with full source code
    of the custom LangChain that we discussed. Use it, fork it, improve it, go forth
    and innovate!
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] Jacob Devlin, Ming-Wei Chang et. al. [BERT: Pre-training of Deep Bidirectional
    Transformers for Language Understanding](https://arxiv.org/abs/1810.04805)'
  prefs: []
  type: TYPE_NORMAL
- en: '[2] Victor Sanh, Lysandre Debut et. al [DistilBERT, a distilled version of
    BERT: smaller, faster, cheaper and lighter](https://arxiv.org/abs/1910.01108)'
  prefs: []
  type: TYPE_NORMAL
- en: '[3] Dataset — [Jigsaw Multilingual Toxic Comment Classification 2020](https://www.kaggle.com/c/jigsaw-multilingual-toxic-comment-classification)'
  prefs: []
  type: TYPE_NORMAL
- en: '*Unless otherwise noted, all images are by the author*'
  prefs: []
  type: TYPE_NORMAL
