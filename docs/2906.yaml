- en: Balancing Innovation With Safety & Privacy in the Era of Large Language Models
    (LLM)
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在大语言模型（LLM）时代平衡创新与安全性和隐私
- en: 原文：[https://towardsdatascience.com/balancing-innovation-with-safety-privacy-in-the-era-of-large-language-models-llm-a63570e4a24a?source=collection_archive---------2-----------------------#2023-09-20](https://towardsdatascience.com/balancing-innovation-with-safety-privacy-in-the-era-of-large-language-models-llm-a63570e4a24a?source=collection_archive---------2-----------------------#2023-09-20)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/balancing-innovation-with-safety-privacy-in-the-era-of-large-language-models-llm-a63570e4a24a?source=collection_archive---------2-----------------------#2023-09-20](https://towardsdatascience.com/balancing-innovation-with-safety-privacy-in-the-era-of-large-language-models-llm-a63570e4a24a?source=collection_archive---------2-----------------------#2023-09-20)
- en: A Guide to Implement Safety, and Privacy Mechanisms for your Generative AI applications
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 关于为你的生成式 AI 应用实施安全性和隐私机制的指南
- en: '[](https://medium.com/@anjanava.biswas?source=post_page-----a63570e4a24a--------------------------------)[![Anjan
    Biswas](../Images/d8ba1231ad138fdc44a95ce12adeb1e2.png)](https://medium.com/@anjanava.biswas?source=post_page-----a63570e4a24a--------------------------------)[](https://towardsdatascience.com/?source=post_page-----a63570e4a24a--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----a63570e4a24a--------------------------------)
    [Anjan Biswas](https://medium.com/@anjanava.biswas?source=post_page-----a63570e4a24a--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@anjanava.biswas?source=post_page-----a63570e4a24a--------------------------------)[![Anjan
    Biswas](../Images/d8ba1231ad138fdc44a95ce12adeb1e2.png)](https://medium.com/@anjanava.biswas?source=post_page-----a63570e4a24a--------------------------------)[](https://towardsdatascience.com/?source=post_page-----a63570e4a24a--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----a63570e4a24a--------------------------------)
    [Anjan Biswas](https://medium.com/@anjanava.biswas?source=post_page-----a63570e4a24a--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fdbbc0b48552b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbalancing-innovation-with-safety-privacy-in-the-era-of-large-language-models-llm-a63570e4a24a&user=Anjan+Biswas&userId=dbbc0b48552b&source=post_page-dbbc0b48552b----a63570e4a24a---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----a63570e4a24a--------------------------------)
    ·12 min read·Sep 20, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fa63570e4a24a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbalancing-innovation-with-safety-privacy-in-the-era-of-large-language-models-llm-a63570e4a24a&user=Anjan+Biswas&userId=dbbc0b48552b&source=-----a63570e4a24a---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fdbbc0b48552b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbalancing-innovation-with-safety-privacy-in-the-era-of-large-language-models-llm-a63570e4a24a&user=Anjan+Biswas&userId=dbbc0b48552b&source=post_page-dbbc0b48552b----a63570e4a24a---------------------post_header-----------)
    发表在[Towards Data Science](https://towardsdatascience.com/?source=post_page-----a63570e4a24a--------------------------------)
    ·12分钟阅读·2023年9月20日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fa63570e4a24a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbalancing-innovation-with-safety-privacy-in-the-era-of-large-language-models-llm-a63570e4a24a&user=Anjan+Biswas&userId=dbbc0b48552b&source=-----a63570e4a24a---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fa63570e4a24a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbalancing-innovation-with-safety-privacy-in-the-era-of-large-language-models-llm-a63570e4a24a&source=-----a63570e4a24a---------------------bookmark_footer-----------)![](../Images/a667a614c9ee7df2114edacede095dca.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fa63570e4a24a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbalancing-innovation-with-safety-privacy-in-the-era-of-large-language-models-llm-a63570e4a24a&source=-----a63570e4a24a---------------------bookmark_footer-----------)![](../Images/a667a614c9ee7df2114edacede095dca.png)'
- en: Photo by [Jason Dent](https://unsplash.com/@jdent?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 照片由[Jason Dent](https://unsplash.com/@jdent?utm_source=medium&utm_medium=referral)提供，拍摄于[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: The AI era has ushered in Large Language Models (aka LLMs) to the technological
    forefront, which has been much of the talk in 2023, and is likely to remain as
    such for many years to come. LLMs are the AI models that are the power house behind
    things like [ChatGPT](https://chat.openai.com/). These AI models, fueled by vast
    amounts of data and computational prowess, have unlocked remarkable capabilities,
    from human-like text generating to assisting with natural language understanding
    (NLU) tasks. They have quickly become the foundation upon which countless applications
    and software services are being built, or at least being augmented with.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: However, as with any groundbreaking innovations, the rise of LLMs brings forth
    a critical question — *“How do we balance this pursuit of technological advancement
    with the imperative of safety and privacy?”.* This is not a mere philosophical
    question, but a challenge that requires proactive and thoughtful action.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: Safety and privacy
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To prioritize safety and privacy in our LLM powered applications, we’ll be honing
    in on key areas, including controlling the spread of personal data ([personally
    identifiable information](https://www.dol.gov/general/ppii), a.k.a. PII) and harmful
    or toxic content. This is essential whether you’re fine-tuning an LLM with your
    own dataset or simply using an LLM for text generation tasks. *Why does this matter?*
    There are a few reasons as to why it could be important.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: Compliance with government regulations that mandates protection of user personal
    information (such as [GDPR](https://gdpr-info.eu/), [CCPA](https://oag.ca.gov/privacy/ccpa),
    [HIPAA Privacy Rule](https://www.hhs.gov/hipaa/for-professionals/privacy/special-topics/de-identification/index.html)
    etc.)
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Compliance with LLM provider End-User License Agreement (EULA) or Acceptable
    Use Policy (AUP)
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Comply with InfoSec policies set within organizations
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mitigate possibility of bias and skew in your model; post fine tuning
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ensure the ethical use of LLMs and preserve brand reputation
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Be prepared for any [AI regulation](https://www.barrons.com/articles/washington-ai-regulation-meetings-senate-33e8c1a5)
    that may be in the horizon
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Considerations for fine tuning
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When preparing for fine-tuning an LLM, the first step is data preparation. Outside
    of research, education, or personal projects, it is likely that you will run into
    situations where your training data may contain PII information. The first step
    here is to identify the existence of these PII entities in the data, and the second
    step is to scrub the data to ensure that these PII entities are anonymized properly.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6e37eaa775478d72974668177f2337e8.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
- en: LLM fine tuning
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: Considerations for text generation
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For text generation using LLMs, there are a couple if things to keep in mind.
    First, we ensure that any prompt containing toxic content are restricted from
    propagating to the LLM, and second we ensure that our prompt is free of any PII
    entities. Consecutively, in some cases, it may be appropriate to run these validations
    **on the text generated by the LLM**, or on the “*machine generated text”.* This
    gives a dual layer of protection in ensuring our ethos of safety and privacy.
    A third aspect is of determining the *intent* of the prompt itself which may,
    to some extent, curtail things like [*prompt injection*](https://arxiv.org/pdf/2306.05499.pdf)attacks*.*
    However, I will primarily focus on PII and toxicity in this article and discuss
    *intent* classification and it’s effect on LLMs in a separate discussion.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f5438261e7c25eff870f22c2a90bc311.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
- en: Text generation with LLM
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: Implementation
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will take a two step approach in order to implement a solution for this.
    First, we make use of a **name entity recognition (NER)** model that can identify
    PII entities in the text and allows us to anonymize the entities. PII entities
    usually includes things like person name, location or address, phone number, credit
    card number, SSN and so on. Second, we use a **text classification** model to
    classify if a text is `toxic` or `neutral`. Examples of toxic text typically are
    text that contain abuse, obscenities, harassment, bullying and so forth.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: For the PII NER model, a most common choice would be a [BERT Base model](https://huggingface.co/bert-base-uncased)
    that can be fine tuned to detect specific PII entities. You can also fine tune
    pre-trained [transformer](https://huggingface.co/docs/transformers/index) models
    such as the [Robust DeID](https://huggingface.co/obi/deid_roberta_i2b2) (de-identification)
    pre-trained model which is a [RoBERTa](https://arxiv.org/pdf/1907.11692.pdf) model
    fine-tuned for de-identification of medical notes and mostly focuses on personal
    health information (a.k.a PHI). A much simpler option to begin experimenting would
    be using [spaCy ER (EntityRecognizer)](https://spacy.io/api/entityrecognizer).
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: which gives us
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d79f386de85671b87d776398390aa039.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
- en: Annotation of PII entities detected by spaCy
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: spaCy `EntityRecognizer` was able to identify three entities — `PERSON` (People,
    including fictional characters), `FAC` (Location or address), and `CARDINAL` (Numerals
    that do not fall under another type). spaCy also gives us the start and end offset
    (character position in the text) of the detected entity which we can use to perform
    anonymization.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: which gives us
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: But there are a few obvious issues here. spaCy ER’s default entity list is not
    exhaustive to cover for all types of PII entities. For example, in our case we
    would like to detect `555-123-1290` as a `PHONE_NUMBER` as opposed to just part
    of the text as `CARDINAL` leading to incomplete entity detection. Of-course, just
    like the transformer based NER models, spaCy can also be trained with your own
    dataset of custom name entities in order to make it more robust. However, we will
    use **open-source** [**Presidio**](https://microsoft.github.io/presidio/) **SDK**
    which is a more purpose-built toolkit for *data protection* and *de-identification*.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: PII detection and anonymization with Presidio
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The Presidio SDK provides a full set of PII detection capabilities, with a long
    list of [supported PII entities](https://microsoft.github.io/presidio/supported_entities/).
    Presidio primarily uses pattern matching, along with ML capabilities of spaCy
    and [Stanza](https://stanfordnlp.github.io/stanza/). However, Presidio is customizable
    and can be plugged-in to use your transformer based PII entity recognition model,
    or with even cloud based PII capabilities such as [Azure Text Analytics PII detection](https://learn.microsoft.com/en-us/azure/ai-services/language-service/personally-identifiable-information/how-to-call),
    or [Amazon Comprehend PII detection](https://docs.aws.amazon.com/comprehend/latest/dg/how-pii.html).
    It also comes with a built-in customizable anonymizer that can help scrub and
    redact PII entities from text.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: which gives us
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: and
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f00a219f85855b7d41a62eb8927cf806.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
- en: Annotation of PII entities detected by Presidio
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: As we’ve seen before, it’s a rather trivial task to anonymize the text since
    we have the beginning and end offsets of each of the entities within the text.
    However, we are going to make use of Presidio’s built-in `AnonymizerEngine` to
    help us with this.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: which gives us
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: This so far is great, but what if we want the anonymization to be just plain
    masking. In that case we can pass in custom configuration to the `AnonymizerEngine`
    which can perform simple masking of the PII entities. For example, we mask the
    entities with the asterisk (`*`) characters only.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: gives us
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Considerations for anonymization
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There are a few things to keep in mind when you decide to anonymize PII entities
    in the text.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: Presidio’s default `AnonymizerEngine` uses a pattern `<ENTITY_LABEL>` to mask
    the PII entities (like `<PHONE_NUMBER>` ). This can potentially cause issues especially
    with LLM fine-tuning. Replacing PII with entity type labels can introduce words
    that carry semantic meaning, potentially affecting the behavior of language models.
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[***Pseudonymization***](https://en.wikipedia.org/wiki/Pseudonymization)is
    a useful tool for data protection, however you should exercise caution performing
    pseudonymization on your training data. For example, replacing all `NAME` entities
    with the pseudonym `John Doe` , or replacing all `DATE` entities with `01-JAN-2000`
    in your fine-tuning data may lead to extreme bias in your fine-tuned model.'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Be aware of how your LLM reacts to certain characters or patterns in your prompt.
    Some LLMs may need a very specific way of templating prompts to get the most out
    of the model, for example Anthropic recommends using [*prompt tags*](https://docs.anthropic.com/claude/docs/constructing-a-prompt#mark-different-parts-of-the-prompt)*.*
    Being aware of this will help decide how you may want to perform anonymization.
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There could be other general side effects of anonymized data on model fine-tuning
    such as *loss of context*, *semantic drift*, *model* *hallucinations* and so on.
    It is important to iterate and experiment to see what level of anonymization is
    appropriate for your needs, while minimizing it’s negative effects on the model’s
    performance.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: Toxicity detection with text classification
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In order to identify whether a text contains toxic content or not, we will use
    a binary classification approach — `0` if the text is neutral, `1` if the text
    is toxic. I decided to train a [DistilBERT base model (uncased)](https://arxiv.org/abs/1910.01108)
    which is a distilled version of a [BERT base model](https://arxiv.org/abs/1810.04805).
    For training data, I used the [Jigsaw dataset](https://www.kaggle.com/c/jigsaw-multilingual-toxic-comment-classification).
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: I won’t go into the details of how the model was trained and model metrics etc.
    however you can refer to this article on [training a DistilBERT base model](https://medium.com/geekculture/hugging-face-distilbert-tensorflow-for-custom-text-classification-1ad4a49e26a7)
    for text-classification tasks. You can see the model training script I wrote [here](https://github.com/annjawn/llm-safety-privacy/blob/main/toxicity.ipynb).
    The model is available in HuggingFace Hub as `[tensor-trek/distilbert-toxicity-classifier](https://huggingface.co/tensor-trek/distilbert-toxicity-classifier)`.
    Let’s run a few sample pieces of text through inference to check what the model
    tells us.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: which gives us —
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The model is correctly classifying the text as `NEUTRAL` or `TOXIC` with pretty
    high confidence. This text classification model, in conjunction to our previously
    discussed PII entity classification can now be used to create a mechanism that
    can enforce privacy and safety within our LLM powered applications or services.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: Putting things together
  id: totrans-68
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We’ve tackled *privacy* through a PII entity recognition mechanism, and we tackled
    the *safety* part with a text toxicity classifier. You can think of other mechanisms
    that may be relevant to your organization’s definition of safety & privacy. For
    example, healthcare organizations may be more concerned about PHI instead of PII
    and so forth. Ultimately, the overall implementation approach for this remains
    the same no matter what controls you want to introduce.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过 PII 实体识别机制解决了 *隐私* 问题，通过文本有毒内容分类器解决了 *安全* 部分。你可以考虑其他可能与贵组织的安全和隐私定义相关的机制。例如，医疗组织可能更关心
    PHI 而不是 PII，等等。最终，无论你想引入什么控制措施，整体实施方法保持不变。
- en: With that in mind, it is now time to put everything together into action. We
    want to be able use both the privacy and safety mechanisms in conjunction with
    an LLM for an application where we want to introduce generative AI capabilities.
    I am going to use the popular [LangChain](https://www.langchain.com/) framework’s
    [Python flavor](https://python.langchain.com/docs/get_started/introduction) (also
    available in [JavaScript/TS](https://js.langchain.com/docs/get_started/introduction/))
    to build a generative AI application which will include the two mechanisms. Here’s
    how our overall architecture looks like.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到这一点，现在是将一切付诸实践的时候了。我们希望能够将隐私和安全机制与 LLM 结合使用，以便在我们希望引入生成式 AI 功能的应用中使用。我将使用流行的
    [LangChain](https://www.langchain.com/) 框架的 [Python 版本](https://python.langchain.com/docs/get_started/introduction)（也可以在
    [JavaScript/TS](https://js.langchain.com/docs/get_started/introduction/) 中使用）来构建一个包含这两种机制的生成式
    AI 应用。这就是我们的总体架构。
- en: '![](../Images/0184314e1d57b457e6e7fa7e1113517e.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0184314e1d57b457e6e7fa7e1113517e.png)'
- en: Privacy and safety flow with LangChain
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: LangChain 的隐私和安全流程
- en: In the above architecture, the first thing I check is if the text contains toxic
    content with at least more than 80% of model accuracy. If so, the execution of
    the whole LangChain application stops at that point, and the user is shown an
    appropriate message. If the text is classified largely as *neutral,* then I pass
    it onto the next step of identifying PII entities. I then perform anonymization
    on those entities in the text if the confidence score of each of these entity
    detection is more than 50%. Once the text is fully anonymized, it is passed as
    a prompt to the LLM for further text generation by the model. Note that the accuracy
    thresholds (80% and 50%) are arbitrary, you would want to test the accuracy of
    both the detectors (PII & toxicity) on your data and decide on a threshold that
    works best for your use-case. *The lower the threshold, the stricter the system
    becomes, the higher the threshold the weaker the enforcement of these checks*.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述架构中，我首先检查文本是否包含有毒内容，模型准确率至少需达到 80% 以上。如果是，整个 LangChain 应用的执行会在此时停止，并向用户显示适当的消息。如果文本被大致分类为
    *中性*，则我会将其传递到下一步以识别 PII 实体。如果这些实体的检测置信度得分超过 50%，我将对文本中的这些实体进行匿名化。一旦文本完全匿名化，它将作为提示传递给
    LLM 进行模型的进一步文本生成。请注意，准确率阈值（80% 和 50%）是任意的，你需要在你的数据上测试两个检测器（PII 和有毒内容）的准确性，并决定一个适合你用例的阈值。*阈值越低，系统变得越严格；阈值越高，检查的执行力度越弱*。
- en: Another more conservative approach would be to stop the execution if any PII
    entities are detected. This could be useful for applications that are not certified
    to handle PII data at all and you want to ensure, no matter how, text containing
    PII doesn’t end up being fed into the application as an input.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种更保守的方法是如果检测到任何 PII 实体，则停止执行。这对那些完全未获得处理 PII 数据认证的应用可能很有用，你希望确保无论如何，包含 PII
    的文本不会作为输入被喂入应用。
- en: '![](../Images/b387590c1db20c3bec017720c12e683e.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b387590c1db20c3bec017720c12e683e.png)'
- en: Privacy and safety flow with LangChain — ***alternate flow***
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: LangChain 的隐私和安全流程——***备用流程***
- en: LangChain implementation
  id: totrans-77
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: LangChain 实现
- en: In order to make it work with LangChain, I created a [custom *chain*](https://python.langchain.com/docs/modules/chains/how_to/custom_chain)called
    `PrivacyAndSafetyChain` . This can be chained with any [LangChain supported LLMs](https://python.langchain.com/docs/modules/model_io/models/llms/)
    to implement a privacy and safety mechanism. This is how it looks like —
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使其与 LangChain 配合使用，我创建了一个 [自定义 *链*](https://python.langchain.com/docs/modules/chains/how_to/custom_chain)
    叫做 `PrivacyAndSafetyChain`。这个链可以与任何 [LangChain 支持的 LLMs](https://python.langchain.com/docs/modules/model_io/models/llms/)
    连接，以实现隐私和安全机制。这就是它的样子——
- en: '[PRE11]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: By default, `PrivacyAndSafetyChain` performs toxicity detection first. If it
    detects any toxic content then it will error out essentially stopping the chain
    as we discussed earlier. If not, then it passess the entered text to the PII entity
    recognizer and based on what masking character to use, the chain will perform
    anonymization of the text with the detected PII entities. The output of the preceding
    code is as shown below. Since there is no toxic content the chain didn’t stop,
    and it detected `PHONE_NUMBER` and `SSN` and correctly anonymized it.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Conclusion
  id: totrans-82
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The biggest takeaway in this post is that as we continue to innovate with Large
    Language Models, it becomes imperative to balance the scales of innovation with
    safety and privacy. The enthusiasm surrounding LLMs, and our ever-growing desire
    to integrate them with a whole world of possible use-cases is un-deniable. However,
    the potential pitfalls — like data privacy breaches, unintended biases, or misuse
    — are equally real and warrant our immediate attention. I covered how you can
    establish a mechanism of detecting PII and toxic content going into your LLMs
    and discussed an implementation with LangChain.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: There’s still much research and development that remains to be done— perhaps
    a better architecture, more reliable and seamless way to ensure data privacy and
    safety. The code in this post is trimmed down for brevity, but I encourage you
    to checkout my [GitHub repository](https://github.com/annjawn/llm-safety-privacy/tree/main)
    where I have collated detailed Notebooks on each step along with full source code
    of the custom LangChain that we discussed. Use it, fork it, improve it, go forth
    and innovate!
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: References
  id: totrans-85
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] Jacob Devlin, Ming-Wei Chang et. al. [BERT: Pre-training of Deep Bidirectional
    Transformers for Language Understanding](https://arxiv.org/abs/1810.04805)'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
- en: '[2] Victor Sanh, Lysandre Debut et. al [DistilBERT, a distilled version of
    BERT: smaller, faster, cheaper and lighter](https://arxiv.org/abs/1910.01108)'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
- en: '[3] Dataset — [Jigsaw Multilingual Toxic Comment Classification 2020](https://www.kaggle.com/c/jigsaw-multilingual-toxic-comment-classification)'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: '*Unless otherwise noted, all images are by the author*'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
