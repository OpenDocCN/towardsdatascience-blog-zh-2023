- en: 'FastSpeech: Paper Overview & Implementation'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: FastSpeech：论文概述与实现
- en: 原文：[https://towardsdatascience.com/fastspeech-paper-overview-implementation-e2b3808648f1?source=collection_archive---------7-----------------------#2023-11-09](https://towardsdatascience.com/fastspeech-paper-overview-implementation-e2b3808648f1?source=collection_archive---------7-----------------------#2023-11-09)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/fastspeech-paper-overview-implementation-e2b3808648f1?source=collection_archive---------7-----------------------#2023-11-09](https://towardsdatascience.com/fastspeech-paper-overview-implementation-e2b3808648f1?source=collection_archive---------7-----------------------#2023-11-09)
- en: Learn about text-to-speech and how it’s realized by transformers
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 了解文本到语音的实现方式以及其如何通过变压器（transformers）来实现
- en: '[](https://essamwissam.medium.com/?source=post_page-----e2b3808648f1--------------------------------)[![Essam
    Wisam](../Images/6320ce88ba2e5d56d70ce3e0f97ceb1d.png)](https://essamwissam.medium.com/?source=post_page-----e2b3808648f1--------------------------------)[](https://towardsdatascience.com/?source=post_page-----e2b3808648f1--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----e2b3808648f1--------------------------------)
    [Essam Wisam](https://essamwissam.medium.com/?source=post_page-----e2b3808648f1--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://essamwissam.medium.com/?source=post_page-----e2b3808648f1--------------------------------)[![Essam
    Wisam](../Images/6320ce88ba2e5d56d70ce3e0f97ceb1d.png)](https://essamwissam.medium.com/?source=post_page-----e2b3808648f1--------------------------------)[](https://towardsdatascience.com/?source=post_page-----e2b3808648f1--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----e2b3808648f1--------------------------------)
    [Essam Wisam](https://essamwissam.medium.com/?source=post_page-----e2b3808648f1--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fccb82b9f3b87&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffastspeech-paper-overview-implementation-e2b3808648f1&user=Essam+Wisam&userId=ccb82b9f3b87&source=post_page-ccb82b9f3b87----e2b3808648f1---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----e2b3808648f1--------------------------------)
    ·9 min read·Nov 9, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fe2b3808648f1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffastspeech-paper-overview-implementation-e2b3808648f1&user=Essam+Wisam&userId=ccb82b9f3b87&source=-----e2b3808648f1---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fccb82b9f3b87&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffastspeech-paper-overview-implementation-e2b3808648f1&user=Essam+Wisam&userId=ccb82b9f3b87&source=post_page-ccb82b9f3b87----e2b3808648f1---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----e2b3808648f1--------------------------------)
    · 9分钟阅读 · 2023年11月9日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fe2b3808648f1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffastspeech-paper-overview-implementation-e2b3808648f1&user=Essam+Wisam&userId=ccb82b9f3b87&source=-----e2b3808648f1---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fe2b3808648f1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffastspeech-paper-overview-implementation-e2b3808648f1&source=-----e2b3808648f1---------------------bookmark_footer-----------)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fe2b3808648f1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffastspeech-paper-overview-implementation-e2b3808648f1&source=-----e2b3808648f1---------------------bookmark_footer-----------)'
- en: In 2019, FastSpeech has pushed the frontier of neural text-to-speech by offering
    significant improvement in inference speed while maintaining robustness to prevent
    word repetition or omission. It also allowed for controllability of the output
    speech in terms of speech and prosody.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 2019年，FastSpeech 在神经文本到语音（neural text-to-speech）领域推动了前沿技术，通过显著提高推理速度，同时保持鲁棒性以防止单词重复或遗漏。它还允许在语音和韵律方面对输出语音进行控制。
- en: In this story, we aim to familiarize you with how transformers are employed
    for text-to-speech, provide you with a concise overview of the FastSpeech paper
    and point you to how you can implement it from scratch. In this, we will assume
    that you are familiar with transformers and their different components. If not,
    we highly recommend reviewing the [preceding article](https://medium.com/@essamwissam/a-systematic-explanation-of-transformers-db82e039b913)
    that delves into this topic.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个故事中，我们旨在让你熟悉变压器（transformers）如何用于文本到语音的转换，提供 FastSpeech 论文的简明概述，并指导你如何从零实现它。在此过程中，我们假设你对变压器及其不同组件已经有一定了解。如果没有，我们强烈推荐你先阅读[前一篇文章](https://medium.com/@essamwissam/a-systematic-explanation-of-transformers-db82e039b913)，该文深入探讨了这一主题。
- en: '![](../Images/81cd05f50413ff3ee41978bb64af16b7.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/81cd05f50413ff3ee41978bb64af16b7.png)'
- en: Van Gogh-style Painting Featuring a Transformer Speaking into a Microphone at
    a Podium — Generated by Author using Canva
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 梵高风格的画作，描绘了一个变压器在讲台上对着麦克风讲话——由作者使用Canva生成
- en: Table of Contents
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 目录
- en: · [Background](#d164)
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: · [背景](#d164)
- en: ∘ [Introduction](#13ce)
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: ∘ [介绍](#13ce)
- en: ∘ [Mel Spectrogram](#b824)
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: ∘ [梅尔频谱图](#b824)
- en: · [Paper Overview](#524a)
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: · [论文概述](#524a)
- en: ∘ [Introduction](#8563)
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: ∘ [介绍](#8563)
- en: ∘ [Experiments and Results](#9a34)
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: ∘ [实验和结果](#9a34)
- en: ∘ [Architecture](#06b7)
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: ∘ [架构](#06b7)
- en: ∘ [Encoder](#e77c)
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: ∘ [编码器](#e77c)
- en: ∘ [Length Regulator](#853b)
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: ∘ [长度调节器](#853b)
- en: ∘ [Decoder](#d70e)
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: ∘ [解码器](#d70e)
- en: · [Implementation](#5cd8)
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: · [实现](#5cd8)
- en: ∘ [Strategy](#4469)
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: ∘ [策略](#4469)
- en: ∘ [Full Implementatio](#af70)n
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: ∘ [完整实现](#af70)
- en: Background
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 背景
- en: Introduction
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 介绍
- en: Traditional text-to-speech (TTS) models relied on concatenative and statistical
    techniques. Concatenative techniques relied on synthesizing speech by concatenating
    sounds from a database of phoneme sounds (distinct units of sound in the language).
    Statistical techniques (e.g., HMMs) attempted to model basic properties of speech
    that are sufficient to generate a waveform. Both approaches often had issues with
    producing natural sounds or expressing emotion. In other words, they tend to produce
    unnatural or robotic speech for the given text.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 传统的文本到语音（TTS）模型依赖于拼接和统计技术。拼接技术通过将来自语音音素数据库（语言中的独立声音单元）的声音拼接起来合成语音。统计技术（例如 HMM）试图建模足够生成波形的语音基本属性。这两种方法通常在生成自然声音或表达情感时存在问题。换句话说，它们往往为给定文本生成不自然或机械的语音。
- en: 'The quality of speech has been significantly improved by using deep learning
    (neural networks) for TTS. Such methods usually consist of two main models: the
    first takes in text and outputs a corresponding Mel Spectrogram and the second
    takes in the Mel Spectrogram and synthesizes speech (called a vocoder).'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 使用深度学习（神经网络）来进行 TTS 显著提升了语音质量。这些方法通常由两个主要模型组成：第一个模型接收文本并输出相应的梅尔频谱图，第二个模型接收梅尔频谱图并合成语音（称为声码器）。
- en: Mel Spectrogram
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 梅尔频谱图
- en: '![](../Images/0350dcdeb16302849a92fa3ebc67b404.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0350dcdeb16302849a92fa3ebc67b404.png)'
- en: Spectrogram by The Official CTBTO Photostream on [Flickr](https://www.flickr.com/photos/ctbto/13465848765)
    CC BY-SA 2.0.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 频谱图由 [Flickr](https://www.flickr.com/photos/ctbto/13465848765) 上的官方 CTBTO 照片流提供
    CC BY-SA 2.0。
- en: In its most basic form, a speech waveform is just a sequence of amplitudes that
    represent the variations in air pressure over time. We can transform any waveform
    into a corresponding Mel Spectrogram (which is a matrix indicating the magnitude
    of different frequencies at different time windows of the original waveform) using
    the short-time Fourier transform (STFT). It’s easy to map a piece of audio to
    its Mel Spectrogram using the short-time Fourier transform; however, doing the
    inverse is quite harder and the best systematic methods (e.g., Griffin Lim) can
    yield coarse results. A preferred approach is to train a model for this task.
    Existing models trained for this task include WaveGlow and WaveNet.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在最基本的形式下，语音波形只是代表空气压力随时间变化的幅度序列。我们可以使用短时傅里叶变换（STFT）将任何波形转换为相应的梅尔频谱图（这是一个矩阵，表示原始波形不同时间窗口中不同频率的幅度）。使用短时傅里叶变换将一段音频映射到其梅尔频谱图很容易；然而，反向操作相当困难，最佳系统方法（例如，Griffin
    Lim）可能会产生粗糙的结果。一个更好的方法是为这个任务训练一个模型。现有的训练此任务的模型包括 WaveGlow 和 WaveNet。
- en: Thus, to reiterate, deep learning methods often approach text-to-speech by training
    the model to predict the MelSpectrogram of speech corresponding to many instances
    of text. It then relies on another model (called vocoder) to map the predicted
    spectrogram to audio. FastSpeech uses the [WaveGlow](https://github.com/NVIDIA/waveglow)
    model by Nvidia.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，重申一下，深度学习方法通常通过训练模型来预测与许多文本实例对应的语音梅尔频谱图来处理文本到语音。然后，它依赖另一个模型（称为声码器）将预测的频谱图映射到音频。FastSpeech
    使用了 Nvidia 的 [WaveGlow](https://github.com/NVIDIA/waveglow) 模型。
- en: '![](../Images/200b26ad6342ecb7effa2ed4fc8a43cc.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/200b26ad6342ecb7effa2ed4fc8a43cc.png)'
- en: A Happy Transformer Writing a Research Paper, Painted in Van Gogh Style. — Generated
    by Author using Canva
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 一个快乐的变压器在写研究论文，画作呈现梵高风格。——由作者使用Canva生成
- en: Paper Overview
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 论文概述
- en: Introduction
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 介绍
- en: 'Although recent transformer-based TTS methods have drastically improved speech
    quality over traditional methods, there still remains three main issues with these
    models:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管基于变压器的最新 TTS 方法在语音质量上相比传统方法有了显著提升，但这些模型仍然存在三个主要问题：
- en: They suffer from slow inference speech because the transformer’s decoder is
    autoregressive. That is, they generate chunks of the Mel Spectrogram sequentially
    relying on previously generated chunks. This also holds for older deep learning
    models based on RNNs and CNNs.
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于变换器的解码器是自回归的，因此它们在推理语音时速度较慢。也就是说，它们依赖于先前生成的块，逐步生成 Mel 频谱图。这同样适用于基于 RNN 和 CNN
    的旧深度学习模型。
- en: They are not robust; word skipping, or repetition may occur due to small errors
    in attention scores (aka alignments) that propagate during sequential generation.
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们不够鲁棒；由于注意力分数（即对齐）的微小错误在顺序生成过程中传播，可能会发生词语跳过或重复现象。
- en: They lack an easy way to control features of the generated speech such as speed
    or prosody (e.g., intonation).
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 他们缺乏一种简单的方法来控制生成语音的特性，例如速度或韵律（如语调）。
- en: 'FastSpeech attempts to solve all three issues. The two key differences from
    other transformer architectures are that:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: FastSpeech 尝试解决所有三个问题。与其他变换器架构相比，它的两个关键差异是：
- en: The decoder is non-autoregressive; it’s perfectly parallelizable; hence, solving
    the speed issue.
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解码器是非自回归的；它是完全可以并行处理的，因此解决了速度问题。
- en: It uses a length regulator component just before the decoder that attempts to
    ensure ideal alignment between phonemes and the Mel spectrogram and drops the
    cross-attention component.
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它在解码器之前使用了一个长度调节组件，试图确保音素和 Mel 频谱图之间的理想对齐，并移除了交叉注意力组件。
- en: The way the length regulator operates allows easy control of speech speed via
    a hyperparameter. Minor properties of prosody such as pause durations can be also
    controlled in a similar fashion.
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 长度调节器的操作方式允许通过超参数轻松控制语音速度。韵律的微小属性，如暂停时长，也可以以类似的方式进行控制。
- en: In return, for purposes of the length regular, it uses sequence-level knowledge
    distillation during training. In other words, it relies on another already trained
    text-to-speech model for training (Transformer TTS model).
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 作为回报，出于长度调节的目的，它在训练过程中使用了序列级知识蒸馏。换句话说，它依赖于另一个已经训练好的文本到语音模型进行训练（Transformer TTS
    模型）。
- en: Experiments and Results
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实验与结果
- en: The authors used the LJSpeech dataset which includes audio length of about 24
    hours scattered through 13100 audio clips (Each comes with its corresponding input
    text). The training task is to input the text and have the model predict the corresponding
    spectrogram. About 95.6% of the data was used for training and the rest was split
    to be used for validation and testing.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 作者使用了 LJSpeech 数据集，该数据集包括约 24 小时的音频，分布在 13100 个音频片段中（每个片段都有相应的输入文本）。训练任务是输入文本并让模型预测相应的频谱图。约
    95.6% 的数据用于训练，其余部分被拆分用于验证和测试。
- en: '**Inference Speed Up** It increases the inference speed by 38x (or 270x without
    including the vocoder) compared to autoregressive transformer TTS models; hence,
    the name FastSpeech.'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**推理速度提升** 相比于自回归变换器 TTS 模型，它将推理速度提高了 38 倍（不包括声码器则为 270 倍）；因此，得名 FastSpeech。'
- en: '**Audio Quality** Using the mean opinion score of 20 native English speakers,
    the authors have shown that FastSpeech closely match the quality of the Transformer
    TTS model and Tacotron 2 (state-of-the-art at the time).'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**音频质量** 利用 20 位母语为英语的评分者的平均意见分数，作者展示了 FastSpeech 的质量与 Transformer TTS 模型和
    Tacotron 2（当时的最先进技术）非常接近。'
- en: '**Robustness** FastSpeech outperformed Transformer TTS and Tacotron 2 with
    a zero-error rate (in terms of skips and repetitions) on 50 challenging text-to-speech
    examples, compared to 24% and 34% for Transformer TTS and Tacotron 2 respectively.'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**鲁棒性** FastSpeech 在 50 个具有挑战性的文本到语音示例中，表现优于 Transformer TTS 和 Tacotron 2，具有零错误率（以跳过和重复为标准），相比之下
    Transformer TTS 和 Tacotron 2 的错误率分别为 24% 和 34%。'
- en: '**Controllability** The authors presented examples to demonstrate that speed
    and pause duration control work.'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可控性** 作者展示了速度和暂停时长控制的有效性。'
- en: '**Ablation** The authors confirm the effectiveness of decisions like integrating
    1D convolutions in the transformer and employing sequence-level knowledge distillation.
    They reveal performance degradation (in terms of the mean opinion score) in the
    absence of each decision.'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**消融实验** 作者确认了集成 1D 卷积到变换器中和采用序列级知识蒸馏等决策的有效性。他们揭示了在缺少每项决策时，性能（以平均意见分数为标准）的下降。'
- en: Architecture
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 架构
- en: '![](../Images/b56ebe8523f72efe19fa047a9ed1a4bb.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b56ebe8523f72efe19fa047a9ed1a4bb.png)'
- en: FastSpeech Architecture Figure from the [FastSpeech](https://arxiv.org/abs/1905.09263)
    paper
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: FastSpeech 架构图来自于 [FastSpeech](https://arxiv.org/abs/1905.09263) 论文
- en: 'The first figure portrays the whole architecture which consists of an encoder,
    length regulator, and decoder:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 第一图描绘了整个架构，包括编码器、长度调节器和解码器：
- en: '![](../Images/0188bc91aa23424ea80fe7909d8fa416.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0188bc91aa23424ea80fe7909d8fa416.png)'
- en: The Feedforward Transformer (FFT) block is used in both the encoder and the
    decoder. It is similar to the encoder layer in the transformer but swaps out the
    position-wise FFN for 1D convolution. A hyperparameter *N* represents the number
    of FFT blocks (connected sequentially) in the encoder and decoder. N is set as
    6 in the paper.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: Feedforward Transformer (FFT)块在编码器和解码器中都使用。它类似于变换器中的编码器层，但将位置依赖的FFN替换为1D卷积。一个超参数*N*代表编码器和解码器中连接顺序的FFT块的数量。论文中将N设为6。
- en: The length regulator adjusts the sequence lengths of its inputs based on the
    duration predictor (third figure). The duration predictor is a simple network
    shown in the fourth figure.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 长度调节器根据持续时间预测器（第三图）调整输入序列的长度。持续时间预测器是一个简单的网络，如第四图所示。
- en: 'You should be able to intuit that the data flow then takes the following form:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该能够直观地感受到数据流的形式如下：
- en: '![](../Images/db3e8cf743a1033a5bdfddc2970c78fc.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/db3e8cf743a1033a5bdfddc2970c78fc.png)'
- en: Encoder
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 编码器
- en: The encoder takes a sequence of integers corresponding to characters given in
    the text. A grapheme-to-phoneme converter can be used to convert the text into
    a sequence of phonetic characters as mentioned in the paper; however, we will
    simply use letters as the character unit and assume that the model can learn any
    phonetic representation it needs during training. Thus, for an input “Say hello!”,
    the encoder takes a sequence 10 integers corresponding to`[“S”,”a”,”y”,…,”!”]`.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 编码器接受对应于文本中的字符的整数序列。可以使用字形到音素的转换器将文本转换为音素字符序列，如论文中提到的；但我们将简单地使用字母作为字符单元，并假设模型在训练期间可以学习任何需要的音素表示。因此，对于输入“Say
    hello!”，编码器接受一个序列10个整数，对应于`[“S”,”a”,”y”,…,”!”]`。
- en: Similar to the transformer encoder, the purpose of the encoder is to assign
    each character a rich vector representation that takes into account the phonetic
    character itself, its order, and its relationship with the other ones in the given
    text. Similar to the transformer, it maintains the dimensionality of the assigned
    vectors in the encoder for Add & Norm purposes.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 与变换器编码器类似，编码器的目的是为每个字符分配一个丰富的向量表示，该表示考虑了字符本身、其顺序以及与给定文本中其他字符的关系。与变换器类似，它在编码器中保持分配向量的维度用于Add
    & Norm目的。
- en: For an input sequence with *n* characters, the encoder outputs *[h₁,h₂,…,hₙ]*
    where each representation has dimensionality `emb_dim`.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 对于具有*n*个字符的输入序列，编码器输出*[h₁,h₂,…,hₙ]*，其中每个表示的维度为`emb_dim`。
- en: Length Regulator
  id: totrans-68
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 长度调节器
- en: The purpose of the length regulator is simply to repeat the encoder representation
    given to each character. The idea is that the pronunciation of each character
    in the text generally corresponds to multiple (or zero) Mel-spectrogram units
    (to be generated by the decoder); it’s not just one unit of sound. By a Mel-spectrogram
    unit, we mean one column in the Mel Spectrogram, which assigns a frequency distribution
    of sound to the time window corresponding to that column and corresponds to actual
    sound in the waveform.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 长度调节器的目的只是重复给定给每个字符的编码器表示。其理念是文本中每个字符的发音通常对应于多个（或零个）Mel谱图单元（由解码器生成）；而不仅仅是一个声音单元。Mel谱图单元指的是Mel谱图中的一列，为该列对应的时间窗口分配声音的频率分布，并对应于波形中的实际声音。
- en: 'The length regulator operates as follows:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 长度调节器的操作如下：
- en: Predict the number of Mel Spectrogram units of each character.
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 预测每个字符的Mel谱图单元的数量。
- en: Repeat the encoder representation according to that number.
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据该数量重复编码器的表示。
- en: 'For instance, given the encoder representations *[h₁, h₂, h₃, h₄, h*₅*]* of
    input characters corresponding to *“knight”.* The following happens in inference
    time:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，给定输入字符的编码器表示*[h₁, h₂, h₃, h₄, h*₅*]* 对应于*“knight”*。推理时会发生以下情况：
- en: The length regulator passes each representation to the duration predictor which
    uses the representation (which involves the relationships with all other characters
    in the text) to predict a single integer that represents the number of Mel Spectrograms
    for the corresponding character.
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 长度调节器将每个表示传递给持续时间预测器，后者利用表示（涉及与文本中所有其他字符的关系）来预测一个整数，该整数表示对应字符的Mel谱图数量。
- en: Suppose the duration predictor returns [ 1, 2, 3, 2, 1] then the length regulator
    repeats each hidden state according to the predicted duration which yields *[h₁,
    h₂, h₂, h₃, h₃, h₃, h₄, h₄, h*₅*].* Now we know the length of the sequence (10)
    is the length of the Mel Spectrogram.
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 假设持续时间预测器返回[ 1, 2, 3, 2, 1]，那么长度调节器根据预测的持续时间重复每个隐藏状态，得到*[h₁, h₂, h₂, h₃, h₃,
    h₃, h₄, h₄, h₅]*。现在我们知道序列的长度（10）就是Mel频谱图的长度。
- en: It passes this new sequence to the decoder.
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它将这个新序列传递给解码器。
- en: Note that in a real setting, passing `knight` to FastSpeech and inspecting the
    output of the duration predictor yielded `[ 1, 8, 15, 3, 0, 17]`. Notice that
    the letters `k` , `g` , `h` contribute negligibly to the Mel Spectrogram compared
    to other letters. Indeed, what’s really pronounced when that word is spoken is
    mostly the `n` , `i` , `t`.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在真实环境中，将`knight`传递给FastSpeech并检查持续时间预测器的输出得到了`[ 1, 8, 15, 3, 0, 17]`。注意，字母`k`，`g`，`h`相比其他字母对Mel频谱图的贡献微不足道。事实上，当发音时真正发音的主要是`n`，`i`，`t`。
- en: '**Controllability** It’s easy to control the speed by scaling the predicted
    durations. For example, if `[ 1, 8, 15, 3, 0, 17]` is doubled, it will take twice
    the time to say the word `knight` *(0.5x* speed up*)* and if it’s multiplied by
    half (then rounded) then it will take half the time to speak the word (*2x* speed
    up). It’s also possible to change only the duration corresponding to specific
    characters (e.g., spaces) to control the duration of their pronunciation (e.g.,
    pause duration).'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '**可控性** 通过缩放预测的持续时间可以轻松控制速度。例如，如果`[ 1, 8, 15, 3, 0, 17]`加倍，则说单词`knight`将需要两倍的时间（*0.5x*加速），如果将其乘以一半（然后四舍五入），则说单词将需要一半的时间（*2x*加速）。还可以仅更改特定字符（例如，空格）对应的持续时间，以控制其发音的持续时间（例如，停顿持续时间）。'
- en: '**Training**'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '**训练**'
- en: In training, FastSpeech doesn’t predict durations using the duration predictor
    (it’s not trained) and rather predicts the duration using the attention matrices
    of a trained TTS Transformer.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练中，FastSpeech不使用持续时间预测器来预测持续时间（它没有被训练），而是使用训练过的TTS变压器的注意力矩阵来预测持续时间。
- en: Cross-attention in that transformer associates each character and Mel Spectrogram
    unit with an attention score via an attention matrix.
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在那个变压器中的交叉注意力将每个字符和Mel频谱图单元与一个注意力分数关联起来，通过注意力矩阵。
- en: Thus, to predict the number of Mel Spectrogram units (duration) of a character
    *c* during the training of FastSpeech, it counts the number of Mel Spectrogram
    units that had maximum attention towards that character using the cross-attention
    matrix in the TTS Transformer.
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 因此，在FastSpeech的训练中，为了预测字符*c*的Mel频谱图单元（持续时间），它使用TTS变压器中的交叉注意力矩阵计算对该字符的最大关注度的Mel频谱图单元数量。
- en: Because cross-attention involves multiple attention matrices (one for each head),
    it does this operation on the attention matrix that is most “diagonal”. It could
    be that this ensures realistic alignment between the characters and Mel Spectrogram
    units.
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 因为交叉注意力涉及多个注意力矩阵（每个头一个），它在最“对角线”的注意力矩阵上执行此操作。这可能确保字符和Mel频谱图单元之间的现实对齐。
- en: It uses this duration to train the duration predictor as well (simple regression
    task). This way we don’t need this teacher model during inference.
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它使用这个持续时间来训练持续时间预测器（简单的回归任务）。这样在推理期间就不需要这个教师模型了。
- en: Decoder
  id: totrans-85
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解码器
- en: The decoder receives this new representation and aims to predict the frequency
    content (vector) of each Mel Spectrogram unit. This is tantamount to predicting
    the entire spectrogram corresponding to the text which can be transformed to audio
    using a vocoder.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 解码器接收这个新表示，并且旨在预测每个Mel频谱图单元的频率内容（向量）。这相当于预测文本对应的整个频谱图，可以使用声码器转换为音频。
- en: The decoder follows a similar architecture to the encoder. It simply replaces
    the first block (embedding layer) with a linear layer as the last block. This
    layer is what produces the frequency vectors for each Mel Spectrogram unit using
    complex feature representations that earlier FFT blocks in the decoder have formed
    for the Mel Spectrogram units.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 解码器与编码器具有类似的架构。它只是用线性层替换第一个块（嵌入层）作为最后一个块。这一层使用早期解码器中形成的复杂特征表示为每个Mel频谱图单元生成频率向量。
- en: The number of frequencies `n_mels` is a hyperparameter of this layer. Set as
    `80` in the paper.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 频率`n_mels`是该层的一个超参数。在论文中设置为`80`。
- en: '![](../Images/bb2a25865f9c5746e1fe16877d6e8a90.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bb2a25865f9c5746e1fe16877d6e8a90.png)'
- en: A Modern Futuristic Transformer Programming a Computer, Painted in Van Gogh
    Style — — Generated by Author using Canva
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 一位现代未来感的变压器正在编程计算机，画风为梵高风格 —— 作者使用Canva生成
- en: Implementation
  id: totrans-91
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现
- en: '![](../Images/b56ebe8523f72efe19fa047a9ed1a4bb.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b56ebe8523f72efe19fa047a9ed1a4bb.png)'
- en: FastSpeech Architecture Figure from the [FastSpeech](https://arxiv.org/abs/1905.09263)
    paper
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: FastSpeech架构图来自[FastSpeech](https://arxiv.org/abs/1905.09263)论文
- en: Strategy
  id: totrans-94
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 策略
- en: The FastSpeech architecture corresponds to
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: FastSpeech架构对应于
- en: '![](../Images/0188bc91aa23424ea80fe7909d8fa416.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0188bc91aa23424ea80fe7909d8fa416.png)'
- en: 'We will start with implementing:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从实现开始：
- en: '![](../Images/f932d12d8926e4731764d133a64ec346.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f932d12d8926e4731764d133a64ec346.png)'
- en: and
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 和
- en: '![](../Images/192fc8feb4a4142b4ce9a4f061bce2dc.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/192fc8feb4a4142b4ce9a4f061bce2dc.png)'
- en: then we can implement the encoder and decoder as their composition is
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们可以实现编码器和解码器，因为它们的组合是
- en: '![](../Images/547d2fb6479f43b7f03db4a7e81674f1.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/547d2fb6479f43b7f03db4a7e81674f1.png)'
- en: Now all we need is the length regulator
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们需要的是长度调节器
- en: '![](../Images/37f7076dd375cf2cd7756f5ab1b8e35b.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/37f7076dd375cf2cd7756f5ab1b8e35b.png)'
- en: because once done the last step is
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 因为一旦完成最后一步就是
- en: '![](../Images/b31602c91b87f24e712702143896cb7c.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b31602c91b87f24e712702143896cb7c.png)'
- en: Full Implementation
  id: totrans-107
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 完整实现
- en: To avoid spamming this article with a lot of code, I had earlier prepared an
    annotated notebook with an organized, code-optimized and learning-friendly version
    of an [original implementation](https://github.com/xcmyz/FastSpeech), for inference
    purposes. You can find it on [Github](https://github.com/TheBotiverse/Botiverse/blob/main/botiverse/models/FastSpeech1/FastSpeech.ipynb)
    or [Google Colab](https://colab.research.google.com/github/TheBotiverse/Botiverse/blob/main/botiverse/models/FastSpeech1/FastSpeech.ipynb#scrollTo=mbfHmqAlLc8W).
    It is highly recommended that you understand the different components found in
    the [transformer architecture](https://medium.com/@essamwissam/a-systematic-explanation-of-transformers-db82e039b913)
    before jumping into the implementation.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 为了避免在本文中插入大量代码，我之前准备了一个带注释的笔记本，包含了一个经过组织、优化和适合学习的[原始实现](https://github.com/xcmyz/FastSpeech)版本，供推断使用。你可以在[Github](https://github.com/TheBotiverse/Botiverse/blob/main/botiverse/models/FastSpeech1/FastSpeech.ipynb)或者[Google
    Colab](https://colab.research.google.com/github/TheBotiverse/Botiverse/blob/main/botiverse/models/FastSpeech1/FastSpeech.ipynb#scrollTo=mbfHmqAlLc8W)找到它。强烈建议你在开始实现之前了解[Transformer架构](https://medium.com/@essamwissam/a-systematic-explanation-of-transformers-db82e039b913)中的不同组件。
- en: '![](../Images/54e0d43a5538a0bd84cde3254f4b59ab.png)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/54e0d43a5538a0bd84cde3254f4b59ab.png)'
- en: A Modern Futuristic Jet Flying Towards the Stars Painted in Van Gogh style —
    Generated by Author using Canva
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 一架现代未来感喷气式飞机飞向星空，画风为梵高风格 —— 作者使用Canva生成
- en: I hope the explanation provided in this story has been helpful in enhancing
    your understanding of FastSpeech and its architecture, while guiding you on how
    you can implement it from scratch. Till next time, au revoir.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 我希望这篇文章中的解释能帮助你更好地理解FastSpeech及其架构，并指导你如何从零开始实现它。下次见，再见。
