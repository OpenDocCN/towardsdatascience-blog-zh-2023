- en: 'FastSpeech: Paper Overview & Implementation'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/fastspeech-paper-overview-implementation-e2b3808648f1?source=collection_archive---------7-----------------------#2023-11-09](https://towardsdatascience.com/fastspeech-paper-overview-implementation-e2b3808648f1?source=collection_archive---------7-----------------------#2023-11-09)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Learn about text-to-speech and how it’s realized by transformers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://essamwissam.medium.com/?source=post_page-----e2b3808648f1--------------------------------)[![Essam
    Wisam](../Images/6320ce88ba2e5d56d70ce3e0f97ceb1d.png)](https://essamwissam.medium.com/?source=post_page-----e2b3808648f1--------------------------------)[](https://towardsdatascience.com/?source=post_page-----e2b3808648f1--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----e2b3808648f1--------------------------------)
    [Essam Wisam](https://essamwissam.medium.com/?source=post_page-----e2b3808648f1--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fccb82b9f3b87&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffastspeech-paper-overview-implementation-e2b3808648f1&user=Essam+Wisam&userId=ccb82b9f3b87&source=post_page-ccb82b9f3b87----e2b3808648f1---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----e2b3808648f1--------------------------------)
    ·9 min read·Nov 9, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fe2b3808648f1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffastspeech-paper-overview-implementation-e2b3808648f1&user=Essam+Wisam&userId=ccb82b9f3b87&source=-----e2b3808648f1---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fe2b3808648f1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffastspeech-paper-overview-implementation-e2b3808648f1&source=-----e2b3808648f1---------------------bookmark_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: In 2019, FastSpeech has pushed the frontier of neural text-to-speech by offering
    significant improvement in inference speed while maintaining robustness to prevent
    word repetition or omission. It also allowed for controllability of the output
    speech in terms of speech and prosody.
  prefs: []
  type: TYPE_NORMAL
- en: In this story, we aim to familiarize you with how transformers are employed
    for text-to-speech, provide you with a concise overview of the FastSpeech paper
    and point you to how you can implement it from scratch. In this, we will assume
    that you are familiar with transformers and their different components. If not,
    we highly recommend reviewing the [preceding article](https://medium.com/@essamwissam/a-systematic-explanation-of-transformers-db82e039b913)
    that delves into this topic.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/81cd05f50413ff3ee41978bb64af16b7.png)'
  prefs: []
  type: TYPE_IMG
- en: Van Gogh-style Painting Featuring a Transformer Speaking into a Microphone at
    a Podium — Generated by Author using Canva
  prefs: []
  type: TYPE_NORMAL
- en: Table of Contents
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: · [Background](#d164)
  prefs: []
  type: TYPE_NORMAL
- en: ∘ [Introduction](#13ce)
  prefs: []
  type: TYPE_NORMAL
- en: ∘ [Mel Spectrogram](#b824)
  prefs: []
  type: TYPE_NORMAL
- en: · [Paper Overview](#524a)
  prefs: []
  type: TYPE_NORMAL
- en: ∘ [Introduction](#8563)
  prefs: []
  type: TYPE_NORMAL
- en: ∘ [Experiments and Results](#9a34)
  prefs: []
  type: TYPE_NORMAL
- en: ∘ [Architecture](#06b7)
  prefs: []
  type: TYPE_NORMAL
- en: ∘ [Encoder](#e77c)
  prefs: []
  type: TYPE_NORMAL
- en: ∘ [Length Regulator](#853b)
  prefs: []
  type: TYPE_NORMAL
- en: ∘ [Decoder](#d70e)
  prefs: []
  type: TYPE_NORMAL
- en: · [Implementation](#5cd8)
  prefs: []
  type: TYPE_NORMAL
- en: ∘ [Strategy](#4469)
  prefs: []
  type: TYPE_NORMAL
- en: ∘ [Full Implementatio](#af70)n
  prefs: []
  type: TYPE_NORMAL
- en: Background
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Traditional text-to-speech (TTS) models relied on concatenative and statistical
    techniques. Concatenative techniques relied on synthesizing speech by concatenating
    sounds from a database of phoneme sounds (distinct units of sound in the language).
    Statistical techniques (e.g., HMMs) attempted to model basic properties of speech
    that are sufficient to generate a waveform. Both approaches often had issues with
    producing natural sounds or expressing emotion. In other words, they tend to produce
    unnatural or robotic speech for the given text.
  prefs: []
  type: TYPE_NORMAL
- en: 'The quality of speech has been significantly improved by using deep learning
    (neural networks) for TTS. Such methods usually consist of two main models: the
    first takes in text and outputs a corresponding Mel Spectrogram and the second
    takes in the Mel Spectrogram and synthesizes speech (called a vocoder).'
  prefs: []
  type: TYPE_NORMAL
- en: Mel Spectrogram
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0350dcdeb16302849a92fa3ebc67b404.png)'
  prefs: []
  type: TYPE_IMG
- en: Spectrogram by The Official CTBTO Photostream on [Flickr](https://www.flickr.com/photos/ctbto/13465848765)
    CC BY-SA 2.0.
  prefs: []
  type: TYPE_NORMAL
- en: In its most basic form, a speech waveform is just a sequence of amplitudes that
    represent the variations in air pressure over time. We can transform any waveform
    into a corresponding Mel Spectrogram (which is a matrix indicating the magnitude
    of different frequencies at different time windows of the original waveform) using
    the short-time Fourier transform (STFT). It’s easy to map a piece of audio to
    its Mel Spectrogram using the short-time Fourier transform; however, doing the
    inverse is quite harder and the best systematic methods (e.g., Griffin Lim) can
    yield coarse results. A preferred approach is to train a model for this task.
    Existing models trained for this task include WaveGlow and WaveNet.
  prefs: []
  type: TYPE_NORMAL
- en: Thus, to reiterate, deep learning methods often approach text-to-speech by training
    the model to predict the MelSpectrogram of speech corresponding to many instances
    of text. It then relies on another model (called vocoder) to map the predicted
    spectrogram to audio. FastSpeech uses the [WaveGlow](https://github.com/NVIDIA/waveglow)
    model by Nvidia.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/200b26ad6342ecb7effa2ed4fc8a43cc.png)'
  prefs: []
  type: TYPE_IMG
- en: A Happy Transformer Writing a Research Paper, Painted in Van Gogh Style. — Generated
    by Author using Canva
  prefs: []
  type: TYPE_NORMAL
- en: Paper Overview
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Although recent transformer-based TTS methods have drastically improved speech
    quality over traditional methods, there still remains three main issues with these
    models:'
  prefs: []
  type: TYPE_NORMAL
- en: They suffer from slow inference speech because the transformer’s decoder is
    autoregressive. That is, they generate chunks of the Mel Spectrogram sequentially
    relying on previously generated chunks. This also holds for older deep learning
    models based on RNNs and CNNs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: They are not robust; word skipping, or repetition may occur due to small errors
    in attention scores (aka alignments) that propagate during sequential generation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: They lack an easy way to control features of the generated speech such as speed
    or prosody (e.g., intonation).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'FastSpeech attempts to solve all three issues. The two key differences from
    other transformer architectures are that:'
  prefs: []
  type: TYPE_NORMAL
- en: The decoder is non-autoregressive; it’s perfectly parallelizable; hence, solving
    the speed issue.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It uses a length regulator component just before the decoder that attempts to
    ensure ideal alignment between phonemes and the Mel spectrogram and drops the
    cross-attention component.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The way the length regulator operates allows easy control of speech speed via
    a hyperparameter. Minor properties of prosody such as pause durations can be also
    controlled in a similar fashion.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In return, for purposes of the length regular, it uses sequence-level knowledge
    distillation during training. In other words, it relies on another already trained
    text-to-speech model for training (Transformer TTS model).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Experiments and Results
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The authors used the LJSpeech dataset which includes audio length of about 24
    hours scattered through 13100 audio clips (Each comes with its corresponding input
    text). The training task is to input the text and have the model predict the corresponding
    spectrogram. About 95.6% of the data was used for training and the rest was split
    to be used for validation and testing.
  prefs: []
  type: TYPE_NORMAL
- en: '**Inference Speed Up** It increases the inference speed by 38x (or 270x without
    including the vocoder) compared to autoregressive transformer TTS models; hence,
    the name FastSpeech.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Audio Quality** Using the mean opinion score of 20 native English speakers,
    the authors have shown that FastSpeech closely match the quality of the Transformer
    TTS model and Tacotron 2 (state-of-the-art at the time).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Robustness** FastSpeech outperformed Transformer TTS and Tacotron 2 with
    a zero-error rate (in terms of skips and repetitions) on 50 challenging text-to-speech
    examples, compared to 24% and 34% for Transformer TTS and Tacotron 2 respectively.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Controllability** The authors presented examples to demonstrate that speed
    and pause duration control work.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Ablation** The authors confirm the effectiveness of decisions like integrating
    1D convolutions in the transformer and employing sequence-level knowledge distillation.
    They reveal performance degradation (in terms of the mean opinion score) in the
    absence of each decision.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Architecture
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/b56ebe8523f72efe19fa047a9ed1a4bb.png)'
  prefs: []
  type: TYPE_IMG
- en: FastSpeech Architecture Figure from the [FastSpeech](https://arxiv.org/abs/1905.09263)
    paper
  prefs: []
  type: TYPE_NORMAL
- en: 'The first figure portrays the whole architecture which consists of an encoder,
    length regulator, and decoder:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0188bc91aa23424ea80fe7909d8fa416.png)'
  prefs: []
  type: TYPE_IMG
- en: The Feedforward Transformer (FFT) block is used in both the encoder and the
    decoder. It is similar to the encoder layer in the transformer but swaps out the
    position-wise FFN for 1D convolution. A hyperparameter *N* represents the number
    of FFT blocks (connected sequentially) in the encoder and decoder. N is set as
    6 in the paper.
  prefs: []
  type: TYPE_NORMAL
- en: The length regulator adjusts the sequence lengths of its inputs based on the
    duration predictor (third figure). The duration predictor is a simple network
    shown in the fourth figure.
  prefs: []
  type: TYPE_NORMAL
- en: 'You should be able to intuit that the data flow then takes the following form:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/db3e8cf743a1033a5bdfddc2970c78fc.png)'
  prefs: []
  type: TYPE_IMG
- en: Encoder
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The encoder takes a sequence of integers corresponding to characters given in
    the text. A grapheme-to-phoneme converter can be used to convert the text into
    a sequence of phonetic characters as mentioned in the paper; however, we will
    simply use letters as the character unit and assume that the model can learn any
    phonetic representation it needs during training. Thus, for an input “Say hello!”,
    the encoder takes a sequence 10 integers corresponding to`[“S”,”a”,”y”,…,”!”]`.
  prefs: []
  type: TYPE_NORMAL
- en: Similar to the transformer encoder, the purpose of the encoder is to assign
    each character a rich vector representation that takes into account the phonetic
    character itself, its order, and its relationship with the other ones in the given
    text. Similar to the transformer, it maintains the dimensionality of the assigned
    vectors in the encoder for Add & Norm purposes.
  prefs: []
  type: TYPE_NORMAL
- en: For an input sequence with *n* characters, the encoder outputs *[h₁,h₂,…,hₙ]*
    where each representation has dimensionality `emb_dim`.
  prefs: []
  type: TYPE_NORMAL
- en: Length Regulator
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The purpose of the length regulator is simply to repeat the encoder representation
    given to each character. The idea is that the pronunciation of each character
    in the text generally corresponds to multiple (or zero) Mel-spectrogram units
    (to be generated by the decoder); it’s not just one unit of sound. By a Mel-spectrogram
    unit, we mean one column in the Mel Spectrogram, which assigns a frequency distribution
    of sound to the time window corresponding to that column and corresponds to actual
    sound in the waveform.
  prefs: []
  type: TYPE_NORMAL
- en: 'The length regulator operates as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Predict the number of Mel Spectrogram units of each character.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeat the encoder representation according to that number.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For instance, given the encoder representations *[h₁, h₂, h₃, h₄, h*₅*]* of
    input characters corresponding to *“knight”.* The following happens in inference
    time:'
  prefs: []
  type: TYPE_NORMAL
- en: The length regulator passes each representation to the duration predictor which
    uses the representation (which involves the relationships with all other characters
    in the text) to predict a single integer that represents the number of Mel Spectrograms
    for the corresponding character.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Suppose the duration predictor returns [ 1, 2, 3, 2, 1] then the length regulator
    repeats each hidden state according to the predicted duration which yields *[h₁,
    h₂, h₂, h₃, h₃, h₃, h₄, h₄, h*₅*].* Now we know the length of the sequence (10)
    is the length of the Mel Spectrogram.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It passes this new sequence to the decoder.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note that in a real setting, passing `knight` to FastSpeech and inspecting the
    output of the duration predictor yielded `[ 1, 8, 15, 3, 0, 17]`. Notice that
    the letters `k` , `g` , `h` contribute negligibly to the Mel Spectrogram compared
    to other letters. Indeed, what’s really pronounced when that word is spoken is
    mostly the `n` , `i` , `t`.
  prefs: []
  type: TYPE_NORMAL
- en: '**Controllability** It’s easy to control the speed by scaling the predicted
    durations. For example, if `[ 1, 8, 15, 3, 0, 17]` is doubled, it will take twice
    the time to say the word `knight` *(0.5x* speed up*)* and if it’s multiplied by
    half (then rounded) then it will take half the time to speak the word (*2x* speed
    up). It’s also possible to change only the duration corresponding to specific
    characters (e.g., spaces) to control the duration of their pronunciation (e.g.,
    pause duration).'
  prefs: []
  type: TYPE_NORMAL
- en: '**Training**'
  prefs: []
  type: TYPE_NORMAL
- en: In training, FastSpeech doesn’t predict durations using the duration predictor
    (it’s not trained) and rather predicts the duration using the attention matrices
    of a trained TTS Transformer.
  prefs: []
  type: TYPE_NORMAL
- en: Cross-attention in that transformer associates each character and Mel Spectrogram
    unit with an attention score via an attention matrix.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Thus, to predict the number of Mel Spectrogram units (duration) of a character
    *c* during the training of FastSpeech, it counts the number of Mel Spectrogram
    units that had maximum attention towards that character using the cross-attention
    matrix in the TTS Transformer.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Because cross-attention involves multiple attention matrices (one for each head),
    it does this operation on the attention matrix that is most “diagonal”. It could
    be that this ensures realistic alignment between the characters and Mel Spectrogram
    units.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It uses this duration to train the duration predictor as well (simple regression
    task). This way we don’t need this teacher model during inference.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Decoder
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The decoder receives this new representation and aims to predict the frequency
    content (vector) of each Mel Spectrogram unit. This is tantamount to predicting
    the entire spectrogram corresponding to the text which can be transformed to audio
    using a vocoder.
  prefs: []
  type: TYPE_NORMAL
- en: The decoder follows a similar architecture to the encoder. It simply replaces
    the first block (embedding layer) with a linear layer as the last block. This
    layer is what produces the frequency vectors for each Mel Spectrogram unit using
    complex feature representations that earlier FFT blocks in the decoder have formed
    for the Mel Spectrogram units.
  prefs: []
  type: TYPE_NORMAL
- en: The number of frequencies `n_mels` is a hyperparameter of this layer. Set as
    `80` in the paper.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/bb2a25865f9c5746e1fe16877d6e8a90.png)'
  prefs: []
  type: TYPE_IMG
- en: A Modern Futuristic Transformer Programming a Computer, Painted in Van Gogh
    Style — — Generated by Author using Canva
  prefs: []
  type: TYPE_NORMAL
- en: Implementation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/b56ebe8523f72efe19fa047a9ed1a4bb.png)'
  prefs: []
  type: TYPE_IMG
- en: FastSpeech Architecture Figure from the [FastSpeech](https://arxiv.org/abs/1905.09263)
    paper
  prefs: []
  type: TYPE_NORMAL
- en: Strategy
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The FastSpeech architecture corresponds to
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0188bc91aa23424ea80fe7909d8fa416.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We will start with implementing:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f932d12d8926e4731764d133a64ec346.png)'
  prefs: []
  type: TYPE_IMG
- en: and
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/192fc8feb4a4142b4ce9a4f061bce2dc.png)'
  prefs: []
  type: TYPE_IMG
- en: then we can implement the encoder and decoder as their composition is
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/547d2fb6479f43b7f03db4a7e81674f1.png)'
  prefs: []
  type: TYPE_IMG
- en: Now all we need is the length regulator
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/37f7076dd375cf2cd7756f5ab1b8e35b.png)'
  prefs: []
  type: TYPE_IMG
- en: because once done the last step is
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b31602c91b87f24e712702143896cb7c.png)'
  prefs: []
  type: TYPE_IMG
- en: Full Implementation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To avoid spamming this article with a lot of code, I had earlier prepared an
    annotated notebook with an organized, code-optimized and learning-friendly version
    of an [original implementation](https://github.com/xcmyz/FastSpeech), for inference
    purposes. You can find it on [Github](https://github.com/TheBotiverse/Botiverse/blob/main/botiverse/models/FastSpeech1/FastSpeech.ipynb)
    or [Google Colab](https://colab.research.google.com/github/TheBotiverse/Botiverse/blob/main/botiverse/models/FastSpeech1/FastSpeech.ipynb#scrollTo=mbfHmqAlLc8W).
    It is highly recommended that you understand the different components found in
    the [transformer architecture](https://medium.com/@essamwissam/a-systematic-explanation-of-transformers-db82e039b913)
    before jumping into the implementation.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/54e0d43a5538a0bd84cde3254f4b59ab.png)'
  prefs: []
  type: TYPE_IMG
- en: A Modern Futuristic Jet Flying Towards the Stars Painted in Van Gogh style —
    Generated by Author using Canva
  prefs: []
  type: TYPE_NORMAL
- en: I hope the explanation provided in this story has been helpful in enhancing
    your understanding of FastSpeech and its architecture, while guiding you on how
    you can implement it from scratch. Till next time, au revoir.
  prefs: []
  type: TYPE_NORMAL
