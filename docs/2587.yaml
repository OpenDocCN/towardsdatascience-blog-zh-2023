- en: Spoken language recognition on Mozilla Common Voice — Audio Transformations.
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/spoken-language-recognition-on-mozilla-common-voice-audio-transformations-24d5ceaa832b?source=collection_archive---------1-----------------------#2023-08-13](https://towardsdatascience.com/spoken-language-recognition-on-mozilla-common-voice-audio-transformations-24d5ceaa832b?source=collection_archive---------1-----------------------#2023-08-13)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[](https://medium.com/@sergeyvilov?source=post_page-----24d5ceaa832b--------------------------------)[![Sergey
    Vilov](../Images/42efe223e2aa575250e050cf3660cf20.png)](https://medium.com/@sergeyvilov?source=post_page-----24d5ceaa832b--------------------------------)[](https://towardsdatascience.com/?source=post_page-----24d5ceaa832b--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----24d5ceaa832b--------------------------------)
    [Sergey Vilov](https://medium.com/@sergeyvilov?source=post_page-----24d5ceaa832b--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F33297faf768d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fspoken-language-recognition-on-mozilla-common-voice-audio-transformations-24d5ceaa832b&user=Sergey+Vilov&userId=33297faf768d&source=post_page-33297faf768d----24d5ceaa832b---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----24d5ceaa832b--------------------------------)
    ·5 min read·Aug 13, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F24d5ceaa832b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fspoken-language-recognition-on-mozilla-common-voice-audio-transformations-24d5ceaa832b&user=Sergey+Vilov&userId=33297faf768d&source=-----24d5ceaa832b---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F24d5ceaa832b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fspoken-language-recognition-on-mozilla-common-voice-audio-transformations-24d5ceaa832b&source=-----24d5ceaa832b---------------------bookmark_footer-----------)![](../Images/a05c695d6e175040ecd08602ecb19ba9.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Photo by [Kelly Sikkema](https://unsplash.com/@kellysikkema?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: This is the third article on spoken language recognition based on the [Mozilla
    Common Voice](https://commonvoice.mozilla.org/en) dataset. In [Part I](/spoken-language-recognition-on-mozilla-common-voice-part-i-3f5400bbbcd8),
    we discussed data selection and data preprocessing and in [Part II](/spoken-language-recognition-on-mozilla-common-voice-part-ii-models-b32780ea1ee4)
    we analysed performance of several neural network classifiers.
  prefs: []
  type: TYPE_NORMAL
- en: The final model achieved 92% accuracy and 97% pairwise accuracy. Since this
    model suffers from somewhat high variance, the accuracy could potentially be improved
    by adding more data. One very common way to get extra data is to synthesize it
    by performing various transformations on the available dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this article, we will consider 5 popular transformations for audio data
    augmentation: adding noise, changing speed, changing pitch, time masking, and
    cut & splice.'
  prefs: []
  type: TYPE_NORMAL
- en: '**The tutorial notebook can be found** [**here**](https://github.com/sergeyvilov/ML-tutorials/blob/main/audio_transforms/audio_transforms.ipynb)**.**'
  prefs: []
  type: TYPE_NORMAL
- en: For illustration purposes, will use the sample *common_voice_en_100040* from
    the [Mozilla Common Voice](https://commonvoice.mozilla.org/) (MCV) dataset. This
    is the sentence *The burning fire had been extinguished*.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Original sample *common_voice_en_100040 from MCV.*
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5ae08da5e063d63b7a63fc9bd9a7ac5a.png)'
  prefs: []
  type: TYPE_IMG
- en: Original signal waveform (image by the author)
  prefs: []
  type: TYPE_NORMAL
- en: Adding noise
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Adding noise is the simplest audio augmentation. The amount of noise is characterised
    by the signal-to-noise ratio (SNR) — the ratio between maximal signal amplitude
    and standard deviation of noise. We will generate several noise levels, defined
    with SNR, and see how they change the signal.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Signals obtained by superimposing noise with SNR=5 and SNR=1000 on the original
    MCV sample common_voice_en_100040(generated by the author).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c4d4745c92215c63a55fa7f8bea5f45c.png)'
  prefs: []
  type: TYPE_IMG
- en: Signal waveform for several noise levels (image by the author)
  prefs: []
  type: TYPE_NORMAL
- en: So, SNR=1000 sounds almost like the unperturbed audio, while at SNR=5 one can
    only distinguish the strongest parts of the signal. In practice, the SNR level
    is hyperparameter that depends on the dataset and the chosen classifier.
  prefs: []
  type: TYPE_NORMAL
- en: Changing speed
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The simplest way to change the speed is just to pretend that the signal has
    a different sample rate. However, this will also change the pitch (how low/high
    in frequency the audio sounds). Increasing the sampling rate will make the voice
    sound higher. To illustrate this we shall “increase” the sampling rate for our
    example by 1.5:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Signal obtained by using a false sampling rate for the original MCV sample common_voice_en_100040(generated
    by the author).
  prefs: []
  type: TYPE_NORMAL
- en: Changing the speed without affecting the pitch is more challenging. One needs
    to use the [Phase Vocoder](https://en.wikipedia.org/wiki/Phase_vocoder)(PV) algorithm.
    In brief, the input signal is first split into overlapping frames. Then, the spectrum
    within each frame is computed by applying Fast Fourier Transformation (FFT). The
    playing speed is then modifyed by resynthetizing frames at a different rate. Since
    the frequency content of each frame is not affected, the pitch remains the same.
    The PV interpolates between the frames and uses the phase information to achieve
    smoothness.
  prefs: []
  type: TYPE_NORMAL
- en: For our experiments, we will use the *stretch_wo_loop* time stretching function
    from [this](https://github.com/gaganbahga/time_stretch) PV implementation.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Signal obtained by varying the speed of the original MCV sample common_voice_en_100040(generated
    by the author).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/de5b95ce491cb4b12c56d1e58edd6779.png)'
  prefs: []
  type: TYPE_IMG
- en: Signal waveform after speed increase (image by the author)
  prefs: []
  type: TYPE_NORMAL
- en: So, the duration of the signal decreased since we increased the speed. However,
    one can hear that the pitch has not changed. Note that when the stretching factor
    is substantial, the phase interpolation between frames might not work well. As
    a result, echo artefacts may appear in the transformed audio.
  prefs: []
  type: TYPE_NORMAL
- en: Changing pitch
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To alter the pitch without affecting the speed, we can use the same PV time
    stretch but pretend that the signal has a different sampling rate such that the
    total duration of the signal stays the same:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Signal obtained by varying pitch of the original MCV sample common_voice_en_100040(generated
    by the author).
  prefs: []
  type: TYPE_NORMAL
- en: Why do we ever bother with this PV while [*librosa*](https://librosa.org/) already
    has *time_stretch* and *pitch_shift* functions? Well, these functions transform
    the signal back to the time domain. When you need to compute embeddings afterwards,
    you will lose time on redundant Fourier transforms. On the other hand, it is easy
    to modify the *stretch_wo_loop* functionsuch that it yields Fourier output without
    taking the inverse transform. One could probably also try to dig into *librosa*
    codes to achieve similar results.
  prefs: []
  type: TYPE_NORMAL
- en: Time masking and cut & splice
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: These two transformation were initially proposed in the *frequency* domain ([Park
    et al. 2019](https://www.isca-speech.org/archive_v0/Interspeech_2019/pdfs/2680.pdf)).
    The idea was to save time on FFT by using precomputed spectra for audio augmentations.
    For simplicity, we will demonstrate how these transformations work in the *time*
    domain. The listed operations can be easily transferred to the frequency domain
    by replacing the time axis with frame indices.
  prefs: []
  type: TYPE_NORMAL
- en: Time masking
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The idea of time masking is to cover up a random region in the signal. The neural
    network has then less chances to learn signal-specific temporal variations that
    are not generalizable.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Signal obtained by applying time mask transformation on the original MCV sample
    common_voice_en_100040(generated by the author).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/93e8ac68243f0db355fe067d836a0c65.png)'
  prefs: []
  type: TYPE_IMG
- en: Signal waveform after time masking (the masked region is indicated with orange)
    (image by the author)
  prefs: []
  type: TYPE_NORMAL
- en: Cut & splice
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The idea is to replace a randomly selected region of the signal with a random
    fragment from another signal having the same label. The implementation is almost
    the same as for time masking, except that a piece of another signal is placed
    instead of the mask.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Synthetic signal obtained by applying cut&splice transformation on the original
    MCV sample common_voice_en_100040 (generated by the author).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e3c3f47ff55952f337762f862acfcead.png)'
  prefs: []
  type: TYPE_IMG
- en: Signal waveform after cut&splice transformation (the inserted fragment from
    the other signal is indicated with orange) (image by the author)
  prefs: []
  type: TYPE_NORMAL
- en: 'The table below shows the accuracy of the [AttNN model](/spoken-language-recognition-on-mozilla-common-voice-part-ii-models-b32780ea1ee4)
    on the validation set for each of these transformations with typical values of
    parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/70d59432a66adb1d9d311230e919c991.png)'
  prefs: []
  type: TYPE_IMG
- en: AttNN accuracy on Mozilla Common Voice dataset for each transformation with
    typical parameters (image by the author).
  prefs: []
  type: TYPE_NORMAL
- en: As can be seen, none of the transformations changed accuracy of our MCV-based
    spoken recognition setup significantly. It is, however, well possible that these
    transformations could boost performance on some other datasets. Lastly, when looking
    for optimal hyperparameters, it maskes sense to try these transformations one
    by one instead of random/grid search. After that, the effective transformations
    may be combined together.
  prefs: []
  type: TYPE_NORMAL
