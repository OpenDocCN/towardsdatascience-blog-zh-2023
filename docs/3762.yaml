- en: 'Newton’s Laws of Motion: The Original Gradient Descent'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/newtons-laws-of-motion-the-original-gradient-descent-a2860037c76f?source=collection_archive---------4-----------------------#2023-12-27](https://towardsdatascience.com/newtons-laws-of-motion-the-original-gradient-descent-a2860037c76f?source=collection_archive---------4-----------------------#2023-12-27)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Exploring the shared language of gradient descent and Newton's motion equations
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@rodrigopesilva?source=post_page-----a2860037c76f--------------------------------)[![Rodrigo
    Silva](../Images/d260f05ed9887c5072e0590db1481be2.png)](https://medium.com/@rodrigopesilva?source=post_page-----a2860037c76f--------------------------------)[](https://towardsdatascience.com/?source=post_page-----a2860037c76f--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----a2860037c76f--------------------------------)
    [Rodrigo Silva](https://medium.com/@rodrigopesilva?source=post_page-----a2860037c76f--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F222e82adf972&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnewtons-laws-of-motion-the-original-gradient-descent-a2860037c76f&user=Rodrigo+Silva&userId=222e82adf972&source=post_page-222e82adf972----a2860037c76f---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----a2860037c76f--------------------------------)
    ·7 min read·Dec 27, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fa2860037c76f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnewtons-laws-of-motion-the-original-gradient-descent-a2860037c76f&user=Rodrigo+Silva&userId=222e82adf972&source=-----a2860037c76f---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fa2860037c76f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnewtons-laws-of-motion-the-original-gradient-descent-a2860037c76f&source=-----a2860037c76f---------------------bookmark_footer-----------)![](../Images/0510803a4ad720b3e2ff6ee725c64c85.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: Photo by [Luddmyla .](https://unsplash.com/@luddmyla?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)
    on [Unsplash](https://unsplash.com/photos/man-playing-skateboard-while-making-tricks-pKSLMEwRpqI?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: 'I remember the first course on Machine Learning I took during undergrad as
    a physics student in the school of engineering. In other words, I was an outsider.
    While the professor explained the backpropagation algorithm via gradient descent,
    I had this somewhat vague question in my head: "Is gradient descent a random algorithm?"
    Before raising my hand to ask the professor, the non-familiar environment made
    me think twice; I shrunk a little bit. Suddenly, the answer struck me.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: Here's what I thought.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: Gradient Descent
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To say what gradient descent is, first we need to define the problem of training
    a neural network, and we can do this with an overview of how machines learn.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: Overview of a neural network training
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In all supervised neural network tasks, we have a prediction and the true value.
    The larger the difference between the prediction and the true value, the worse
    our neural network is when it comes to predicting the values. Hence, we create
    a function called the *loss function,* usually denoted as *L*, that quantifies
    how much difference there is between the actual value and the predicted value.
    The task of training the neural network is to update the weights and biases (for
    short, the parameters) to minimize the loss function. That's the big picture of
    training a neural network, and "learning" is simply updating the parameters to
    fit actual data best, i.e., minimizing the loss function.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: Optimizing via gradient descent
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Gradient descent is one of the optimization techniques used to calculate these
    new parameters. Since our task is to choose the parameters to minimize the loss
    function, we need a criterion for such a choice. The loss function that we are
    trying to minimize is a function of the neural network output, so mathematically
    we express it as *L = L*(*y_nn, y*). But the neural network output *y_nn* also
    depends on its parameters, so *y_nn* = *y_nn*(**θ**), where **θ** is a vector
    containing all the parameters of our neural network. In other words, the loss
    function itself is a function of the neural networks' parameters.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: Borrowing some concepts from vector calculus, we know that to minimize a function,
    you need to go against its *gradient*, since the gradient points in the direction
    of the fastest increase of the function. To gain some intuition, let's take a
    look at what L(**θ**) might look like in Fig. 1.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1139deaa99d1bfa730554f63b8062840.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Surface showing *L(w1,w2) as a function of w1 and w2\. Image by author.*'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, we have a clear intuition of what is desirable and what is not when training
    a neural network: we want the values of the loss function to be smaller, so if
    we start with parameters w1 and w2 that result in a loss function in the yellow/orange
    region, we want to slide down to the surface in the direction of the purple region.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: This "sliding down" motion is achieved through the gradient descent method.
    If we are placed at the brightest region on the surface, the gradient will continue
    to point up, since it's the direction of maximally fast increase. Then, going
    in the opposite direction (hence, gradient *descent*) creates a motion onto the
    region of maximally fast decrease.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: 'To see this, we can plot the gradient descent vector, as displayed in Fig.
    2\. In this figure, we have a contour plot that shows the same region and the
    same function displayed in Fig. 1, but the values of the loss function are now
    encoded into the color: the brighter, the larger.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3fea759395a7eec55f659093c188b1dd.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: Contour plot showing the vector pointing in the direction of gradient
    descent. Image by author.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: We can see that if we pick a point in the yellow/orange region, the gradient
    descent vector points in the direction that arrives the fastest in the purple
    region.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: A nice disclaimer is that usually a neural network may contain an arbitrarily
    large number of parameters (GPT 3 has over 100 billion parameters!), which means
    that these nice visualizations are completely unpractical in real-life applications,
    and parameter optimization in neural networks is usually a very high-dimensional
    problem.
  id: totrans-26
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Mathematically, the gradient descent algorithm is then given by
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b58f4f03cc362ecb6bedab4bc1bc52ce.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
- en: Here, **θ**_(n+1) are the updated parameters (the result of sliding down the
    surface of Fig. 1); **θ**_(n) are the parameters that we started with; ρ is called
    the learning rate (how big is the step towards the direction where the gradient
    descent is pointing); and ∇L is the gradient of the loss function calculated at
    the initial point **θ**_(n). What gives the name *descent* here is the minus sign
    in front of it.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: Mathematics is key here because we'll see that Newton's Second Law of Motion
    has the same mathematical formulation as the gradient descent equation.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: Newton's Second Law
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Newton''s second law of motion is probably one of the most important concepts
    in classical mechanics since it tells how force, mass, and acceleration are tied
    together. Everybody knows the high school formulation of Newton''s second law:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c327ee2b0739daab2e55cac453b94ff7.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
- en: 'where F is the force, m is the mass, and a the acceleration. However, Newton''s
    original formulation was in terms of a deeper quantity: *momentum*. Momentum is
    the product between the mass and the velocity of a body:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/86c23dd0fcfa9576402ad0d87154229e.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
- en: and can be interpreted as the *quantity of movement* of a body. The idea behind
    Newton's second law is that to change the momentum of a body, you need to disturb
    it somehow, and this disturbance is called a *force*. Hence, a neat formulation
    of Newton's second law is
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f9989f6a13514f256555efe79f7a782f.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
- en: This formulation works for every force you can think of, but we would like a
    little more structure in our discussion and, to gain structure, we need to constrain
    our domain of possibilities. Let's talk about conservative forces and potentials.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: Conservative forces and potentials
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A conservative force is a force that does not dissipate energy. It means that,
    when we are in a system with only conservative forces involved, the total energy
    is a constant. This must sound very restrictive, but in reality, the most fundamental
    forces in nature are conservative, such as gravity and electric force.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: For each conservative force, we associate something called a *potential*. This
    potential is related to the force via the equation
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/be34c31af4e79f240bf4dd5ea17c8609.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
- en: 'in one dimension. If we take a closer look at the last two equations presented,
    we arrive at the second law of motion for conservative fields:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/75d544970a15fd52099e4d73e83d767a.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
- en: 'Since derivatives are kind of complicated to deal with, and in computer sciences
    we approximate derivatives as finite differences anyway, let''s replace d''s with
    Δ''s:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c567d46cecf9295ec5d559c1857105e6.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
- en: We know that Δ means "take the updated value and subtract by the current value".
    Hence, we can re-write the formula above as
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/31faf4864f2ad24e1dcab43280c27134.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
- en: 'This already looks pretty similar to the gradient descent equation shown in
    some lines above. To make it even more similar, we just have to look at it in
    three dimensions, where the gradient arises naturally:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/061d25617148bb0cda8e33dc704ee48e.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
- en: We see a clear correspondence between the gradient descent and the formulation
    shown above, which completely derives from Newtonian physics. The momentum of
    a body (and you can read this as velocity if you prefer) will always point toward
    the direction where the potential decreases the fastest, with a step size given
    by Δt.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: Final words and takeaways
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So we can relate the potential, within the Newtonian formulation, to the loss
    function in machine learning. The momentum vector is similar to the parameter
    vector, which we are trying to optimize, and the time step constant is the learning
    rate, i.e., how fast we are moving towards the minimum of the loss function. Hence,
    the similar mathematical formulation shows that these concepts are tied together
    and present a nice, unified way of looking at them.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: 'If you are wondering, the answer to my question at the beginning is "no". There
    is no randomness in the gradient descent algorithm since it replicates what nature
    does every day: the physical trajectory of a particle always tries to find a way
    to rest in the lowest possible potential around it. If you let a ball fall from
    a certain hight, it will always have the same trajectory, no randomness. When
    you see someone on a skateboard sliding down a steep ramp, remember: that''s literally
    nature applying the gradient descent algorithm.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: The way we see a problem may influence its solution. In this article, I have
    not shown you anything new in terms of computer science or physics (indeed, the
    physics presented here is ~400 years old), but shifting the perspective and tying
    (apparently) non-related concepts together may create new links and intuitions
    about a topic.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看待问题的方式可能会影响其解决方案。在这篇文章中，我没有展示任何关于计算机科学或物理的新内容（实际上，这里的物理知识已有约400年历史），但改变视角和将（表面上）不相关的概念结合在一起，可能会创造出新的联系和对某一主题的直觉。
- en: References
  id: totrans-56
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] Robert Kwiatkowski, [Gradient Descent Algorithm — a deep dive](/gradient-descent-algorithm-a-deep-dive-cf04e8115f21),
    2021.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] Robert Kwiatkowski, [梯度下降算法——深度探讨](/gradient-descent-algorithm-a-deep-dive-cf04e8115f21)，2021年。'
- en: '[2] Nivaldo A. Lemos, Analytical Mechanics, Cambridge University Press, 2018.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] Nivaldo A. Lemos, 《解析力学》，剑桥大学出版社，2018年。'
