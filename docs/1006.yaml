- en: Mixture Models, Latent Variables and the Expectation Maximization Algorithm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/mixture-models-latent-variables-and-the-expectation-maximization-algorithm-e5b18e15faa?source=collection_archive---------1-----------------------#2023-03-19](https://towardsdatascience.com/mixture-models-latent-variables-and-the-expectation-maximization-algorithm-e5b18e15faa?source=collection_archive---------1-----------------------#2023-03-19)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[](https://reoneo.medium.com/?source=post_page-----e5b18e15faa--------------------------------)[![Reo
    Neo](../Images/a3c192dafc1222b06b2e7fcf4d35cb27.png)](https://reoneo.medium.com/?source=post_page-----e5b18e15faa--------------------------------)[](https://towardsdatascience.com/?source=post_page-----e5b18e15faa--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----e5b18e15faa--------------------------------)
    [Reo Neo](https://reoneo.medium.com/?source=post_page-----e5b18e15faa--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F9fb220b09dcf&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmixture-models-latent-variables-and-the-expectation-maximization-algorithm-e5b18e15faa&user=Reo+Neo&userId=9fb220b09dcf&source=post_page-9fb220b09dcf----e5b18e15faa---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----e5b18e15faa--------------------------------)
    ·8 min read·Mar 19, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fe5b18e15faa&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmixture-models-latent-variables-and-the-expectation-maximization-algorithm-e5b18e15faa&user=Reo+Neo&userId=9fb220b09dcf&source=-----e5b18e15faa---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fe5b18e15faa&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmixture-models-latent-variables-and-the-expectation-maximization-algorithm-e5b18e15faa&source=-----e5b18e15faa---------------------bookmark_footer-----------)![](../Images/eae2fea5e6268b616a5acfcaca0178b9.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Photo by [Sung Shin](https://unsplash.com/ko/@ironstagram?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: Unsupervised learning has always been fascinating to me. It is a way to learn
    about data without manual labeling effort and allows for the identification of
    patterns within the dataset. Out of the various unsupervised learning techniques,
    the simplest of which is clustering. In essence, a clustering algorithm aims to
    find data points that are similar to one another. By clustering data points together
    we can derive valuable insights about the dataset and what the various clusters
    represent.
  prefs: []
  type: TYPE_NORMAL
- en: This article aims to provide an in-depth look at the Gaussian Mixture Model
    clustering algorithm, how it models the data, and more importantly how Expectation-Maximization
    can be used to fit the model on a new dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '**What is a mixture model?**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Essentially, a mixture model (or mixture distribution) is a **combination of
    multiple probability distributions into a single one**.
  prefs: []
  type: TYPE_NORMAL
- en: To combine these distributions together, we assign a **weight** to each component
    distribution such that the total probability under the distribution sums to 1\.
    A simple example would be a mixture distribution that consists of 2 gaussians.
    We can have 2 distributions of different means and variance and combine the 2
    using different weights.
  prefs: []
  type: TYPE_NORMAL
