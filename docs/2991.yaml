- en: The Multi-Task Optimization Controversy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/the-multi-task-optimization-controversy-793cbb431d98?source=collection_archive---------12-----------------------#2023-09-29](https://towardsdatascience.com/the-multi-task-optimization-controversy-793cbb431d98?source=collection_archive---------12-----------------------#2023-09-29)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Do we need special algorithms to train models on multiple tasks at the same
    time?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@samuel.flender?source=post_page-----793cbb431d98--------------------------------)[![Samuel
    Flender](../Images/390d82a673de8a8bb11cef66978269b5.png)](https://medium.com/@samuel.flender?source=post_page-----793cbb431d98--------------------------------)[](https://towardsdatascience.com/?source=post_page-----793cbb431d98--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----793cbb431d98--------------------------------)
    [Samuel Flender](https://medium.com/@samuel.flender?source=post_page-----793cbb431d98--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fce56d9dcd568&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-multi-task-optimization-controversy-793cbb431d98&user=Samuel+Flender&userId=ce56d9dcd568&source=post_page-ce56d9dcd568----793cbb431d98---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----793cbb431d98--------------------------------)
    ·6 min read·Sep 29, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F793cbb431d98&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-multi-task-optimization-controversy-793cbb431d98&user=Samuel+Flender&userId=ce56d9dcd568&source=-----793cbb431d98---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F793cbb431d98&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-multi-task-optimization-controversy-793cbb431d98&source=-----793cbb431d98---------------------bookmark_footer-----------)![](../Images/d49f4ff793f5ebba7bb708389b741416.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Photo by [Javier Allegue Barros](https://unsplash.com/@soymeraki?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: The [multi-task learning paradigm](/multi-task-learning-in-recommender-systems-a-primer-508e661a2029#:~:text=Multi%2Dtask%20learning%20matters%20because,each%20other%20%E2%80%94%20creating%20negative%20transfer.)
    — that is, the ability to train models on multiple tasks at the same time — has
    been a blessing as much as a curse.
  prefs: []
  type: TYPE_NORMAL
- en: 'A blessing because it allows us to build a single model where previously we
    would have needed multiple. That makes live simpler: fewer models that need to
    be maintained, re-trained, tuned, and monitored.'
  prefs: []
  type: TYPE_NORMAL
- en: 'A curse because it opens up an entirely new pandora’s box of questions: which
    tasks should be learned together? Which tasks do we really need? What happens
    if tasks are competing with each other? How can we make make the model prioritize
    certain tasks over others? How can we avoid ‘task rot’, that is, the accumulation
    of task heads over time that eventually lead to degradation of model performance?'
  prefs: []
  type: TYPE_NORMAL
- en: It is questions like these that spawned a new subdomain of Machine Learning
    known as *multi-task optimization*, that is, the science of how to optimize a
    model on multiple, sometimes competing, tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Scalarization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Scalarization is Mathematic’s answer to the multi-task optimization problem.
    In a multi-task model we are trying to learn K tasks, such as predicting…
  prefs: []
  type: TYPE_NORMAL
