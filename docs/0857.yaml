- en: Teaching CLIP Some Fashion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/teaching-clip-some-fashion-3005ac3fdcc3?source=collection_archive---------3-----------------------#2023-03-07](https://towardsdatascience.com/teaching-clip-some-fashion-3005ac3fdcc3?source=collection_archive---------3-----------------------#2023-03-07)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Training FashionCLIP, a domain-specific CLIP model for Fashion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://fede-bianchi.medium.com/?source=post_page-----3005ac3fdcc3--------------------------------)[![Federico
    Bianchi](../Images/fa38ff2051af04df7803af7d84c5cd4d.png)](https://fede-bianchi.medium.com/?source=post_page-----3005ac3fdcc3--------------------------------)[](https://towardsdatascience.com/?source=post_page-----3005ac3fdcc3--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----3005ac3fdcc3--------------------------------)
    [Federico Bianchi](https://fede-bianchi.medium.com/?source=post_page-----3005ac3fdcc3--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F2aff872fe60e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fteaching-clip-some-fashion-3005ac3fdcc3&user=Federico+Bianchi&userId=2aff872fe60e&source=post_page-2aff872fe60e----3005ac3fdcc3---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----3005ac3fdcc3--------------------------------)
    ·10 min read·Mar 7, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F3005ac3fdcc3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fteaching-clip-some-fashion-3005ac3fdcc3&user=Federico+Bianchi&userId=2aff872fe60e&source=-----3005ac3fdcc3---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F3005ac3fdcc3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fteaching-clip-some-fashion-3005ac3fdcc3&source=-----3005ac3fdcc3---------------------bookmark_footer-----------)![](../Images/34cea25c9ba0b5d71769a94173d2ba36.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Photo by [Domenico Loia](https://unsplash.com/@domenicoloia?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    on [Unsplash](https://unsplash.com/photos/hGV2TfOh0ns).
  prefs: []
  type: TYPE_NORMAL
- en: This is a short blog post describing [FashionCLIP](https://doi.org/10.1038/s41598-022-23052-9).
    If you are a data scientist you probably have to deal with both images and text.
    However, your data will be very specific to your domain, and standard models might not work well.
    This post explains how **domain-specific** vision and language models can be used
    in a **domain-specific** setting and why using them can be a promising way to
    create a search engine or a (zero-shot) classifier.
  prefs: []
  type: TYPE_NORMAL
- en: 'FashionCLIP is a new vision and language model for the fashion industry and
    supports practitioners in solving two tasks:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Categorization**: zero-shot classification of product images;'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Search**: efficient retrieval of products given a query.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: While FashionCLIP is the result of many people working hard, this blog post
    is mainly my summary and my personal view of the amazing experience I had while
    building this, and does not necessarily represent the view of all other authors
    and their organizations.
  prefs: []
  type: TYPE_NORMAL
- en: Models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We currently release the model in two different formats:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Our Internal Wrapper](https://github.com/patrickjohncyh/fashion-clip)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[HuggingFace Weights](https://huggingface.co/patrickjohncyh/fashion-clip)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We also have a [colab tutorial](https://colab.research.google.com/drive/1Z1hAxBnWjF76bEi9KQ6CMBBEmI_FVDrW?usp=sharing)
    that goes over most of the things you can do with FashionCLIP.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Fashion is one of those industries that can benefit the most from AI products.
    Indeed, due to the nature of the domain, the existence of different catalogs,
    and client-specific datasets it is often difficult to build solutions that can
    be applied seamlessly to different problems.
  prefs: []
  type: TYPE_NORMAL
- en: 'Imagine two data scientists at a major fashion company: Mary, and Luis. The
    two have to deal with an ever-changing system, and its operations require constant
    care:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Mary is building a ***product classifier***to help with categorization at scale:
    her model takes a product and selects one among a list of categories (shoes, dress,
    etc.);'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Luis is working on ***product matching*** to improve the search experience:
    his model takes a query in one of the supported languages (e.g., “a red dress”),
    and gives back a list of products matching the query.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'As every practitioner knows, any new model in production brings to life a complex
    life cycle and somehow brittle dependencies:'
  prefs: []
  type: TYPE_NORMAL
- en: Mary’s model needs to be constantly re-trained as inventory grows and categories
    shift;
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Luis’ model depends on the quality of product meta-data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Same company, different use-cases, different models.**'
  prefs: []
  type: TYPE_NORMAL
- en: '**What if there was another way?**'
  prefs: []
  type: TYPE_NORMAL
- en: Today we try to take a step forward, showing how we can build a general model
    for Fashion data. We describe FashionCLIP a fine-tuned version of the famous [CLIP](https://openai.com/blog/clip/)
    model, tailored to treat Fashion data. Our recent paper on [FashionCLIP](https://doi.org/10.1038/s41598-022-23052-9)
    has been published in Nature Scientific Reports.
  prefs: []
  type: TYPE_NORMAL
- en: Chia, P.J., Attanasio, G., Bianchi, F. *et al.* **Contrastive language and vision
    learning of general fashion concepts**. *Sci Rep* **12**, 18958 (2022). [https://doi.org/10.1038/s41598-022-23052-9](https://doi.org/10.1038/s41598-022-23052-9)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: FashionCLIP came to life through a collaboration with [*Farfetch*](https://www.farfetch.com/),
    a giant (and real) luxury e-commerce traded on the NYSE. FashionCLIP is a joint
    work with people from both industry (Coveo, Farfetch) and academia (Stanford,
    Bocconi, Bicocca). Model weights are available online in [HuggingFace format](https://huggingface.co/patrickjohncyh/fashion-clip).
    An example of usage can be found on [Patrick’s repo](https://github.com/patrickjohncyh/fashion-clip/blob/master/fashion_clip_api_demo.ipynb).
  prefs: []
  type: TYPE_NORMAL
- en: We will first go over the use case and explain some more in-depth details of
    the model. Finally, we will share the code we have been using to train the model
    and how to access the weights.
  prefs: []
  type: TYPE_NORMAL
- en: 'FashionCLIP: The Story'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'FashionCLIP is a general model to embed images of fashion products and their
    description in the same vector space: each image and each product will be represented
    by a single dense vector.'
  prefs: []
  type: TYPE_NORMAL
- en: Why are we putting them in the same vector space? **So that they can be compared.**
    This principle is the key to the success of a model like CLIP.
  prefs: []
  type: TYPE_NORMAL
- en: 'FashionCLIP is derived from the original CLIP. The idea is pretty straightforward.
    If you take:'
  prefs: []
  type: TYPE_NORMAL
- en: A ton of images with captions;
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An image encoder (this could be a CNN or ViT);
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A text encoder (this could be a transformers-based language model).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can train a model (with a contrastive loss) to put the embedding of an image
    close to its caption embedding and far from irrelevant captions. In the GIF you
    show an example in 2 dimensions. The concept generalizes to N dimensions.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d12f8e4672fad8eedc508ddc8bf324ab.png)'
  prefs: []
  type: TYPE_IMG
- en: FashionCLIP embeds descriptions and images in the same vector space. This is
    useful for zero-shot classification and image retrieval. *Image by the author
    using Farfetch catalog.*
  prefs: []
  type: TYPE_NORMAL
- en: 'The end result is a **multi-modal space**, allowing you to move between visual
    and textual interactions using novel images and novel text descriptions: if you
    have some text, you can retrieve *corresponding images* (as in product search);
    if you have some images, you can *rank* captions based on semantic similarity
    (as in classification).'
  prefs: []
  type: TYPE_NORMAL
- en: To fine-tune CLIP, you need a good dataset. We jointly worked with Farfetch
    to train CLIP with high-quality images and captions. The dataset (soon to be openly
    released) comprises more than 800K samples.
  prefs: []
  type: TYPE_NORMAL
- en: We train the model for a couple of epochs and check the performance on several
    benchmarks encompassing zero-shot classification, probing, and retrieval. Before
    seeing the results, let’s take a deeper look at what we can do now that we have
    a trained FashionCLIP.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will not delve deeper into CLIP itself. If you want to know more about CLIP,
    I have a dedicated blog post here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[](/how-to-train-your-clip-45a451dcd303?source=post_page-----3005ac3fdcc3--------------------------------)
    [## How to Train your CLIP'
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to CLIP and to how we fine-tuned it for the Italian Language during
    the HuggingFace Community Week.
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/how-to-train-your-clip-45a451dcd303?source=post_page-----3005ac3fdcc3--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: 'The two key tasks that FashionCLIP can tackle are:'
  prefs: []
  type: TYPE_NORMAL
- en: Image Retrieval
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zero-shot Classification
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Retrieval: From Text to Image'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We first move from text to image: we encode a search query (“A red dress”)
    with FashionCLIP text encoder and retrieve the closest image vectors through a
    simple dot product. **The greater the value of the dot product, the more similar
    the text and the image are**. In the GIF below, the search is done on 4 product
    images as an example.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6c62314d50e09c9bb8aad4a8e7602a44.png)'
  prefs: []
  type: TYPE_IMG
- en: '*For retrieval, we can pre-compute image embeddings on the target catalog.
    At runtime, we encode the query and rank images through a simple dot product.
    Image by the author using Farfetch catalog.*'
  prefs: []
  type: TYPE_NORMAL
- en: 'While “red dress” is a simple query for which the [search engine](https://www.farfetch.com/shopping/women/search/items.aspx?q=red+dress&skipsl=1)
    may not need additional input, things get quickly get interesting with slightly
    more ambiguous queries, such as “light red dress” vs “dark red dress”, in which
    “light” and “dark” are modifiers of the same color:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/08c90f64ddd3663a05b31ff1a44d1b7b.png)'
  prefs: []
  type: TYPE_IMG
- en: '*FashionCLIP helps disambiguate geometric features. Image by the author using
    Farfetch catalog.*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Even more interesting is FashionCLIP''s ability to capture items *represented
    within clothes*. Product descriptions often fail to explicitly mention figurative
    patterns, FashionCLIP instead is able to recognize printed items, even in a cartoonish-like
    shape, like the *cat* hanging on a bag in the t-shirt below:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/489ca1c4a29687e99febe59046b17e22.png)'
  prefs: []
  type: TYPE_IMG
- en: '*FashionCLIP recognizes figurative items printed on t-shirts. Image by the
    author using Farfetch catalog.*'
  prefs: []
  type: TYPE_NORMAL
- en: While we have not evaluated this capability in detail, we believe this might
    come from the “knowledge” possessed by the original CLIP, which is partially kept
    during fine-tuning.
  prefs: []
  type: TYPE_NORMAL
- en: Of course, information is better encoded in descriptions (e.g., brands are often
    mentioned in the description) than in any semantic nuances FashionCLIP may capture.
    However, its capabilities in augmenting standard learn-to-rank signals without
    behavioral data may greatly improve the search experience, especially for cold-start
    scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: 'Classification: From Image to Text'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We now go from image to text for classification: we encode the image of a fashion
    item we want to classify with FashionCLIP’s image encoder and retrieve the closest
    label vectors through a dot product:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/335ba8d48b19e52d28ffb351596aa5c1.png)'
  prefs: []
  type: TYPE_IMG
- en: '*For zero-shot classification, we compute the image embeddings of the query
    item and the text embedding of the target labels. Image by the author using Farfetch
    catalog.*'
  prefs: []
  type: TYPE_NORMAL
- en: The trick of CLIP-like models is treating labels not as categorical variables,
    but as semantically meaningful labels.
  prefs: []
  type: TYPE_NORMAL
- en: In other words, when “classifying”, we are asking the question “which of these
    texts is the best caption for this image?”.
  prefs: []
  type: TYPE_NORMAL
- en: 'Thanks to CLIP pre-training and the infinite possibilities of natural language,
    we now have a classifier that is not confined to *any specific set of labels,
    categories, or attributes*: while, of course, the first application could be using
    this classifier on new products in the Farfetch catalog, we can re-use the same
    model on other datasets with different labels or purposes, e.g.:'
  prefs: []
  type: TYPE_NORMAL
- en: if a supplier doesn’t categorize shoes as “high-heel shoes” vs “flat shoes”,
    we can add that attribute;
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If merchandisers are creating new views on the catalog — for example, matching
    items to *styles* — we can classify existing products according to new dimensions
    (“elegant”, “streetwear”, etc.).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The generalization abilities of CLIP come of course at the expense of *some
    precision*: that is, if we train a new classifier in a supervised fashion to solve
    the use cases above, they all will be a bit better than FashionCLIP. As usual,
    there is no one-size fits all with real-world ML, and the trade-off between one
    model or many can be assessed in different ways depending on the importance of
    the use case, training time, labeling costs, etc.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Performance**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We compare FashionCLIP to CLIP on two different tasks on various datasets. More
    details about the setup are found in the paper, the scope of this section is just
    to show that there is a boost in performance when using FashionCLIP in place of
    CLIP for fashion-related tasks.
  prefs: []
  type: TYPE_NORMAL
- en: For Zero-Shot Classification we use three different datasets (KAGL, DEEP, and
    FMNIST) that should serve as out-of-distribution datasets (we know from other
    experiments that we work much better than CLIP on in-domain data, but this is
    expected).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0e48f853336913b2ae8c0da04d530ac8.png)'
  prefs: []
  type: TYPE_IMG
- en: Weighted Macro F1 score on different datasets (out-of-domain data). FashionCLIP
    shows a significant improvement over CLIP on these datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Zero-shot results confirm that our model works as expected!
  prefs: []
  type: TYPE_NORMAL
- en: 'For Image Retrieval, we use a portion of the original dataset that we left
    out during training. Note that this obviously gives us an advantage with respect
    to CLIP as this dataset is going to be in-domain for us. However, it is still
    an interesting experiment. The following results confirm that our model is best:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4f374e654dfa0c33699501fff4429f67.png)'
  prefs: []
  type: TYPE_IMG
- en: Precision at 5 and at 10 on our internal test set (in-domain data). FashionCLIP
    has a much better retrieval performance.
  prefs: []
  type: TYPE_NORMAL
- en: Torch Implementation and HuggingFace weights
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Thanks to Patrick’s work, FashionCLIP is very easy to use. You can simply load
    the model and run zero-shot classification with a simple method, all with python!
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: And you can also do image retrieval!
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Farewell
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Conclusion of a Long Journey
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Building FashionCLIP has been a long and fun adventure with old and new friends
    from some of the coolest places on earth. The results always taste better when
    you get them with your friends. Also, some of us have been working for years together
    and actually never met in real life!
  prefs: []
  type: TYPE_NORMAL
- en: 'On a more pragmatic note, we hope that FashionCLIP can open up unprecedented
    opportunities for companies quickly iterating in internal and external fashion
    use cases: for example, while you may end up building a devoted style classifier,
    using FashionCLIP for your proof of concept will go a long way in proving the
    value of the feature *without investing upfront in a new model life-cycle support*.'
  prefs: []
  type: TYPE_NORMAL
- en: 'When we consider the growing number of SaaS players for intelligent APIs in
    retail — Coveo, Algolia, Bloomreach — the importance of vertical models cannot
    be underestimated: since B2B companies grow with accounts, robustness, and re-usability
    matter more than pure precision. We envision a near future in which FashionCLIP
    — *and DIYCLIP, ElectronicsCLIP, etc*. — will be a standard component of B2B Machine
    Learning players, enabling quick iteration, data standardization, and economies
    of scale on a completely different level than what has been possible so far.'
  prefs: []
  type: TYPE_NORMAL
- en: 'I also gave a talk last year at Pinecone about FashionCLIP:'
  prefs: []
  type: TYPE_NORMAL
- en: The talk I gave at Pinecone about building models like FashionCLIP.
  prefs: []
  type: TYPE_NORMAL
- en: An Additional Demo
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'What’s the power of Open Source? [Pablo](https://www.linkedin.com/in/pablomendes/)
    saw the model and reached out with a UI to help us test the difference between
    the standard HuggingFace CLIP vs the FashionCLIP we just released — I then used
    [Objective Search](https://www.objective.inc) to test the search using FashionCLIP
    with a couple of queries (see it for yourself [here](https://www.objective.inc/demos/fashion-clip)):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/14f32da68e1f2c2d972fc31f78f7409b.png)'
  prefs: []
  type: TYPE_IMG
- en: Using FashionCLIP for search. GIF by author, images from the H&M dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Cool, isn’t it?
  prefs: []
  type: TYPE_NORMAL
- en: Limitations, Bias, and Fairness
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We acknowledge certain limitations of FashionCLIP and expect that it inherits
    certain limitations and biases present in the original CLIP model. We do not expect
    our fine-tuning to significantly augment these limitations: we acknowledge that
    the fashion data we use makes explicit assumptions about the notion of gender
    as in “blue shoes for a woman” that inevitably associate aspects of clothing with
    specific people.'
  prefs: []
  type: TYPE_NORMAL
- en: Our investigations also suggest that the data used introduces certain limitations
    in FashionCLIP. From the textual modality, given that most captions derived from
    the Farfetch dataset are long, we observe that FashionCLIP may be more performant
    in longer queries than shorter ones.
  prefs: []
  type: TYPE_NORMAL
- en: From the image modality, FashionCLIP is also biased towards standard product
    images (centered, white background). This means that the model might underperform
    on images that do not have the same structure.
  prefs: []
  type: TYPE_NORMAL
- en: More Things We Did
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: FashionCLIP has been a long journey, but there are a couple of things we did
    while we waited for the official release.
  prefs: []
  type: TYPE_NORMAL
- en: GradedRecs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We built on top of our work in FashionCLIP to explore recommendations by traversing
    the latent space. Check out our [paper](https://aclanthology.org/2022.ecnlp-1.22/)
    if you’re interested!
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/da83f43146bfb0a834a9359af421fe9a.png)'
  prefs: []
  type: TYPE_IMG
- en: GradedRec. Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: Fairness in Recommender System Evaluation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If you are interested in related industry tasks, such as recommendations, we
    ran a challenge last year on a well-rounded evaluation of recommender systems.
  prefs: []
  type: TYPE_NORMAL
- en: The challenge was meant at understanding how we can build evaluations that are
    not focused only on point-wise metrics (e.g., accuracy). You can find some details
    and an introductory blog post here
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://fede-bianchi.medium.com/a-rounded-evaluation-of-recommender-systems-b9fa101ef79a?source=post_page-----3005ac3fdcc3--------------------------------)
    [## A Rounded Evaluation of Recommender Systems'
  prefs: []
  type: TYPE_NORMAL
- en: 'EvalRS: Evaluating Recommender Systems on Many Tests'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: fede-bianchi.medium.com](https://fede-bianchi.medium.com/a-rounded-evaluation-of-recommender-systems-b9fa101ef79a?source=post_page-----3005ac3fdcc3--------------------------------)
  prefs: []
  type: TYPE_NORMAL
