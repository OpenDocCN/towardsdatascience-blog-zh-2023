- en: Large Language Models, StructBERT — Incorporating Language Structures into Pretraining
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/large-language-models-structbert-incorporating-language-structures-into-pretraining-be3058ab23b3?source=collection_archive---------3-----------------------#2023-11-22](https://towardsdatascience.com/large-language-models-structbert-incorporating-language-structures-into-pretraining-be3058ab23b3?source=collection_archive---------3-----------------------#2023-11-22)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Making models smarter by incorporating better learning objectives
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@slavahead?source=post_page-----be3058ab23b3--------------------------------)[![Vyacheslav
    Efimov](../Images/db4b02e75d257063e8e9d3f1f75d9d6d.png)](https://medium.com/@slavahead?source=post_page-----be3058ab23b3--------------------------------)[](https://towardsdatascience.com/?source=post_page-----be3058ab23b3--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----be3058ab23b3--------------------------------)
    [Vyacheslav Efimov](https://medium.com/@slavahead?source=post_page-----be3058ab23b3--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fc8a0ca9d85d8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flarge-language-models-structbert-incorporating-language-structures-into-pretraining-be3058ab23b3&user=Vyacheslav+Efimov&userId=c8a0ca9d85d8&source=post_page-c8a0ca9d85d8----be3058ab23b3---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----be3058ab23b3--------------------------------)
    ·5 min read·Nov 22, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fbe3058ab23b3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flarge-language-models-structbert-incorporating-language-structures-into-pretraining-be3058ab23b3&user=Vyacheslav+Efimov&userId=c8a0ca9d85d8&source=-----be3058ab23b3---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fbe3058ab23b3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flarge-language-models-structbert-incorporating-language-structures-into-pretraining-be3058ab23b3&source=-----be3058ab23b3---------------------bookmark_footer-----------)![](../Images/9950addc5beece240f634d42ca38618b.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: After its first appearance, BERT has shown phenomenal results in a variety of
    NLP tasks including sentiment analysis, text similarity, question answering, etc.
    Since then, researchers notoriously tried to make BERT even more performant by
    either modifying its architecture, augmenting training data, increasing vocabulary
    size or changing the hidden size of layers.
  prefs: []
  type: TYPE_NORMAL
- en: '[](/bert-3d1bf880386a?source=post_page-----be3058ab23b3--------------------------------)
    [## Large Language Models: BERT — Bidirectional Encoder Representations from Transformer'
  prefs: []
  type: TYPE_NORMAL
- en: Understand how BERT constructs state-of-the-art embeddings
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/bert-3d1bf880386a?source=post_page-----be3058ab23b3--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: Despite the creation of other powerful BERT-based models like RoBERTa, researchers
    found another efficient way to boost BERT’s performance which is going to be discussed
    in this article. This led to the development of a new model called **StructBERT**
    which confidently surpasses BERT on top benchmarks.
  prefs: []
  type: TYPE_NORMAL
- en: The StructBERT idea is relatively simple and focuses on slightly modifying BERT’s
    pretraining objective.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: In this article, we will go through the main details of the StructBERT paper
    and understand the originally modified objectives.
  prefs: []
  type: TYPE_NORMAL
- en: Pretraining
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For the most part, StructBERT has the same architectural principles as BERT.
    Nevertheless, StructBERT presents two new pretraining objectives to expand linguistic
    knowledge of BERT. The model is trained on this objective alongside masked language
    modeling. Let us look at these two objectives below.
  prefs: []
  type: TYPE_NORMAL
- en: 1\. Word sentence objective
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Experiments showed that masked language modeling (MSM) task plays a crucial
    role in the BERT setting to help it obtain vast linguistic knowledge. After pretraining,
    BERT can correctly guess masked words with high accuracy. Nevertheless, it is
    not capable of correctly reconstructing a sentence whose words are shuffled. To
    achieve this goal, the StructBERT developers modified the MSM objective by partially
    shuffling input tokens.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: As in the original BERT, an input sequence is tokenised, masked and then mapped
    to token, positional and segment embeddings. All of these embeddings are then
    summed up to produce combined embeddings which are fed to BERT.
  prefs: []
  type: TYPE_NORMAL
- en: During masking, 15% of randomly chosen tokens are masked and then used for language
    modeling, as in BERT. But right after masking, StructBERT randomly selects 5%
    of K consecutive unmasked tokens and shuffles them within each subsequence. By
    default, StructBERT operates on trigrams (K = 3).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ae1dfe76bcdacacb333330b5f75ccc0c.png)'
  prefs: []
  type: TYPE_IMG
- en: Example of a trigram shuffling
  prefs: []
  type: TYPE_NORMAL
- en: When the last hidden layer is computed, output embeddings of masked and shuffled
    tokens are then used to predict original tokens taking into account their initial
    positions.
  prefs: []
  type: TYPE_NORMAL
- en: Ultimately, word sentence objective is combined with MLM objective with equal
    weights.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 2\. Sentence structural objective
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Next sentence prediction which is another BERT pretraining task is considered
    relatively simple. Mastering it does not lead to a significant boost to BERT performance
    on most downstream tasks. That is why StructBERT researchers increased the difficulty
    of this objective by making BERT predict the sentence order.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'By taking a pair of sequential sentences S₁ and S₂ in a document, StructBERT
    uses them to construct a training example in one of three possible ways. Each
    of these ways occurs with an equal probability of 1 / 3:'
  prefs: []
  type: TYPE_NORMAL
- en: S₂ is followed by S₁ *(label 1);*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: S₁ is followed by S₂ *(label 2);*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Another sentence S₃ from a random document is sampled and is followed by S₁
    *(label 0).*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each of these three procedures results in a ordered pair of sentences which
    are then concatenated. The token [CLS] is added before the beginning of the first
    sentence and [SEP] tokens are used to mark the end of each sentence. BERT takes
    this sequence as input and outputs a set of embeddings on the last hidden layer.
  prefs: []
  type: TYPE_NORMAL
- en: The output of the [CLS] embedding which was originally used in BERT for next
    sentence prediction task, is now used in StructBERT to correctly identify one
    of three possible labels corresponding to the original way the input sequence
    was built.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/53c9a4372539ab5637e1fef508e2bc7d.png)'
  prefs: []
  type: TYPE_IMG
- en: Composition of training samples
  prefs: []
  type: TYPE_NORMAL
- en: Final objective
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The final objective consists of a linear combination of word and sentence structural
    objectives.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4d5d17bd852192d75c360c85a07f28c6.png)'
  prefs: []
  type: TYPE_IMG
- en: BERT pretraining including word and sentence structural objectives
  prefs: []
  type: TYPE_NORMAL
- en: StructBERT settings
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'All of the main pretraining details are the same in BERT and StructBERT:'
  prefs: []
  type: TYPE_NORMAL
- en: 'StructBERT uses the same pretraining corpus as BERT: English Wikipedia (2500M
    words) and BookCorpus (800M words). Tokenization is done by WordPiece tokenizer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Optimisator: Adam (learning rate *l* = 1e-4, weight decay L₂ = 0.01, β₁ = 0.9,
    β₂ = 0.999).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Learning rate warmup is performed over the first 10% of total steps and then
    reduced linearly.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dropout (α = 0.1) layer is used on all layers.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Activation function: GELU.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The pretraining procedure is run for 40 epochs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: StructBERT versions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Like the original BERT, StructBERT comes up with base and large versions. All
    the main settings like the number of layers, attention heads, hidden size and
    the number parameters correspond exactly to base and large versions of BERT respectively.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/554d81d8503b1480c90ba7c32d3130c4.png)'
  prefs: []
  type: TYPE_IMG
- en: Comparison of StructBERT base and StructBERT large
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: By introducing a new pair of training objectives, StructBERT reaches new limits
    in NLP consistently outperforming BERT on various downstream tasks. It was demonstrated
    that both of the objectives play an indispensable role in the StructBERT setting.
    While the word structural objective mostly enhances the model’s performance on
    single-sentence problems making StructBERT able to reconstruct word order, the
    sentence structural objective improves the ability to understand inter-sentence
    relations which is particularly important for sentence-pair tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Resources
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[StructBERT: Incorporating Language Structures Into Pre-training for Deep Language
    Understanding](https://arxiv.org/pdf/1908.04577.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*All images unless otherwise noted are by the author*'
  prefs: []
  type: TYPE_NORMAL
