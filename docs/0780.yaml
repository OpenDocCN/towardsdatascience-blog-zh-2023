- en: 'Gradient Descent vs. Gradient Boosting: A Side-by-Side Comparison'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/gradient-descent-vs-gradient-boosting-a-side-by-side-comparison-7067bb3c5712?source=collection_archive---------7-----------------------#2023-02-28](https://towardsdatascience.com/gradient-descent-vs-gradient-boosting-a-side-by-side-comparison-7067bb3c5712?source=collection_archive---------7-----------------------#2023-02-28)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: From Initialization to Convergence in simple English
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@angela.shi?source=post_page-----7067bb3c5712--------------------------------)[![Angela
    and Kezhan Shi](../Images/a89d678f2f3887c0c2ff3928f9d767b4.png)](https://medium.com/@angela.shi?source=post_page-----7067bb3c5712--------------------------------)[](https://towardsdatascience.com/?source=post_page-----7067bb3c5712--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----7067bb3c5712--------------------------------)
    [Angela and Kezhan Shi](https://medium.com/@angela.shi?source=post_page-----7067bb3c5712--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F2bf03e38122e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgradient-descent-vs-gradient-boosting-a-side-by-side-comparison-7067bb3c5712&user=Angela+and+Kezhan+Shi&userId=2bf03e38122e&source=post_page-2bf03e38122e----7067bb3c5712---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----7067bb3c5712--------------------------------)
    ·5 min read·Feb 28, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F7067bb3c5712&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgradient-descent-vs-gradient-boosting-a-side-by-side-comparison-7067bb3c5712&user=Angela+and+Kezhan+Shi&userId=2bf03e38122e&source=-----7067bb3c5712---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F7067bb3c5712&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgradient-descent-vs-gradient-boosting-a-side-by-side-comparison-7067bb3c5712&source=-----7067bb3c5712---------------------bookmark_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Gradient descent and gradient boosting are two popular machine learning algorithms.
    Despite their different approaches and applications, both gradient descent and
    gradient boosting algorithms are founded on gradient calculations and share several
    common steps. The main aim of this article is to provide a detailed comparison
    of these two algorithms to help readers gain a better understanding of their similarities
    and differences.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8c776904768c4cbd3516306acf0a29c3.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Gregoire Jeanneau](https://unsplash.com/es/@gregjeanneau?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: Gradient Descent
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Gradient descent is a common optimization algorithm used in machine learning
    to minimize a cost function. The goal is to find the best set of parameters that
    minimize the error between the predicted and actual values. The process starts
    by randomly initializing the weights or coefficients of the model. Then, it iteratively
    updates the weights in the direction of the steepest descent of the cost function
    by calculating the gradient of the cost function with respect to each parameter.
  prefs: []
  type: TYPE_NORMAL
- en: Gradient Boosting
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Gradient boosting is an ensemble method that combines multiple weak models to
    create a stronger predictive model. It works by iteratively…
  prefs: []
  type: TYPE_NORMAL
