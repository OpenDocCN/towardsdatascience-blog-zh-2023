- en: 'Gradient Descent vs. Gradient Boosting: A Side-by-Side Comparison'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 梯度下降与梯度提升：逐一比较
- en: 原文：[https://towardsdatascience.com/gradient-descent-vs-gradient-boosting-a-side-by-side-comparison-7067bb3c5712?source=collection_archive---------7-----------------------#2023-02-28](https://towardsdatascience.com/gradient-descent-vs-gradient-boosting-a-side-by-side-comparison-7067bb3c5712?source=collection_archive---------7-----------------------#2023-02-28)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/gradient-descent-vs-gradient-boosting-a-side-by-side-comparison-7067bb3c5712?source=collection_archive---------7-----------------------#2023-02-28](https://towardsdatascience.com/gradient-descent-vs-gradient-boosting-a-side-by-side-comparison-7067bb3c5712?source=collection_archive---------7-----------------------#2023-02-28)
- en: From Initialization to Convergence in simple English
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从初始化到收敛的简单英文解释
- en: '[](https://medium.com/@angela.shi?source=post_page-----7067bb3c5712--------------------------------)[![Angela
    and Kezhan Shi](../Images/a89d678f2f3887c0c2ff3928f9d767b4.png)](https://medium.com/@angela.shi?source=post_page-----7067bb3c5712--------------------------------)[](https://towardsdatascience.com/?source=post_page-----7067bb3c5712--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----7067bb3c5712--------------------------------)
    [Angela and Kezhan Shi](https://medium.com/@angela.shi?source=post_page-----7067bb3c5712--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@angela.shi?source=post_page-----7067bb3c5712--------------------------------)[![Angela
    和 Kezhan Shi](../Images/a89d678f2f3887c0c2ff3928f9d767b4.png)](https://medium.com/@angela.shi?source=post_page-----7067bb3c5712--------------------------------)[](https://towardsdatascience.com/?source=post_page-----7067bb3c5712--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----7067bb3c5712--------------------------------)
    [Angela 和 Kezhan Shi](https://medium.com/@angela.shi?source=post_page-----7067bb3c5712--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F2bf03e38122e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgradient-descent-vs-gradient-boosting-a-side-by-side-comparison-7067bb3c5712&user=Angela+and+Kezhan+Shi&userId=2bf03e38122e&source=post_page-2bf03e38122e----7067bb3c5712---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----7067bb3c5712--------------------------------)
    ·5 min read·Feb 28, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F7067bb3c5712&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgradient-descent-vs-gradient-boosting-a-side-by-side-comparison-7067bb3c5712&user=Angela+and+Kezhan+Shi&userId=2bf03e38122e&source=-----7067bb3c5712---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F2bf03e38122e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgradient-descent-vs-gradient-boosting-a-side-by-side-comparison-7067bb3c5712&user=Angela+and+Kezhan+Shi&userId=2bf03e38122e&source=post_page-2bf03e38122e----7067bb3c5712---------------------post_header-----------)
    发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----7067bb3c5712--------------------------------)
    ·5分钟阅读·2023年2月28日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F7067bb3c5712&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgradient-descent-vs-gradient-boosting-a-side-by-side-comparison-7067bb3c5712&user=Angela+and+Kezhan+Shi&userId=2bf03e38122e&source=-----7067bb3c5712---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F7067bb3c5712&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgradient-descent-vs-gradient-boosting-a-side-by-side-comparison-7067bb3c5712&source=-----7067bb3c5712---------------------bookmark_footer-----------)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F7067bb3c5712&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgradient-descent-vs-gradient-boosting-a-side-by-side-comparison-7067bb3c5712&source=-----7067bb3c5712---------------------bookmark_footer-----------)'
- en: Introduction
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: Gradient descent and gradient boosting are two popular machine learning algorithms.
    Despite their different approaches and applications, both gradient descent and
    gradient boosting algorithms are founded on gradient calculations and share several
    common steps. The main aim of this article is to provide a detailed comparison
    of these two algorithms to help readers gain a better understanding of their similarities
    and differences.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度下降和梯度提升是两种流行的机器学习算法。尽管它们在方法和应用上有所不同，但这两种算法都基于梯度计算，并且具有若干相同的步骤。本文的主要目的是提供这两种算法的详细比较，帮助读者更好地理解它们的相似性和差异。
- en: '![](../Images/8c776904768c4cbd3516306acf0a29c3.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8c776904768c4cbd3516306acf0a29c3.png)'
- en: Photo by [Gregoire Jeanneau](https://unsplash.com/es/@gregjeanneau?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由[格雷戈瓦·让诺](https://unsplash.com/es/@gregjeanneau?utm_source=medium&utm_medium=referral)提供，来自[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: Gradient Descent
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 梯度下降
- en: Gradient descent is a common optimization algorithm used in machine learning
    to minimize a cost function. The goal is to find the best set of parameters that
    minimize the error between the predicted and actual values. The process starts
    by randomly initializing the weights or coefficients of the model. Then, it iteratively
    updates the weights in the direction of the steepest descent of the cost function
    by calculating the gradient of the cost function with respect to each parameter.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度下降是一种常见的优化算法，用于机器学习中以最小化成本函数。目标是找到一组最佳参数，以最小化预测值与实际值之间的误差。该过程开始时随机初始化模型的权重或系数。然后，通过计算成本函数相对于每个参数的梯度，迭代地更新权重，沿着成本函数的最大下降方向前进。
- en: Gradient Boosting
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 梯度提升
- en: Gradient boosting is an ensemble method that combines multiple weak models to
    create a stronger predictive model. It works by iteratively…
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '梯度提升是一种集成方法，通过将多个弱模型结合起来创建一个更强的预测模型。它通过迭代的方式工作…… '
