- en: 'Entity Resolution: Identifying Real-World Entities in Noisy Data'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实体解析：识别嘈杂数据中的真实世界实体
- en: 原文：[https://towardsdatascience.com/entity-resolution-identifying-real-world-entities-in-noisy-data-3e8c59f4f41c?source=collection_archive---------3-----------------------#2023-09-21](https://towardsdatascience.com/entity-resolution-identifying-real-world-entities-in-noisy-data-3e8c59f4f41c?source=collection_archive---------3-----------------------#2023-09-21)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/entity-resolution-identifying-real-world-entities-in-noisy-data-3e8c59f4f41c?source=collection_archive---------3-----------------------#2023-09-21](https://towardsdatascience.com/entity-resolution-identifying-real-world-entities-in-noisy-data-3e8c59f4f41c?source=collection_archive---------3-----------------------#2023-09-21)
- en: Fundamental theories and Python implementations
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基本理论和Python实现
- en: '[](https://medium.com/@tnmasui?source=post_page-----3e8c59f4f41c--------------------------------)[![Tomonori
    Masui](../Images/e3c6ffae4b4f748394e743a349ab7e59.png)](https://medium.com/@tnmasui?source=post_page-----3e8c59f4f41c--------------------------------)[](https://towardsdatascience.com/?source=post_page-----3e8c59f4f41c--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----3e8c59f4f41c--------------------------------)
    [Tomonori Masui](https://medium.com/@tnmasui?source=post_page-----3e8c59f4f41c--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@tnmasui?source=post_page-----3e8c59f4f41c--------------------------------)[![Tomonori
    Masui](../Images/e3c6ffae4b4f748394e743a349ab7e59.png)](https://medium.com/@tnmasui?source=post_page-----3e8c59f4f41c--------------------------------)[](https://towardsdatascience.com/?source=post_page-----3e8c59f4f41c--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----3e8c59f4f41c--------------------------------)
    [Tomonori Masui](https://medium.com/@tnmasui?source=post_page-----3e8c59f4f41c--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F703ffb2ff12d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fentity-resolution-identifying-real-world-entities-in-noisy-data-3e8c59f4f41c&user=Tomonori+Masui&userId=703ffb2ff12d&source=post_page-703ffb2ff12d----3e8c59f4f41c---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----3e8c59f4f41c--------------------------------)
    ·19 min read·Sep 21, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F3e8c59f4f41c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fentity-resolution-identifying-real-world-entities-in-noisy-data-3e8c59f4f41c&user=Tomonori+Masui&userId=703ffb2ff12d&source=-----3e8c59f4f41c---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F703ffb2ff12d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fentity-resolution-identifying-real-world-entities-in-noisy-data-3e8c59f4f41c&user=Tomonori+Masui&userId=703ffb2ff12d&source=post_page-703ffb2ff12d----3e8c59f4f41c---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----3e8c59f4f41c--------------------------------)
    ·19分钟阅读·2023年9月21日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F3e8c59f4f41c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fentity-resolution-identifying-real-world-entities-in-noisy-data-3e8c59f4f41c&user=Tomonori+Masui&userId=703ffb2ff12d&source=-----3e8c59f4f41c---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F3e8c59f4f41c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fentity-resolution-identifying-real-world-entities-in-noisy-data-3e8c59f4f41c&source=-----3e8c59f4f41c---------------------bookmark_footer-----------)![](../Images/1d691114d0c38478a3a450438730f0fc.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F3e8c59f4f41c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fentity-resolution-identifying-real-world-entities-in-noisy-data-3e8c59f4f41c&source=-----3e8c59f4f41c---------------------bookmark_footer-----------)![](../Images/1d691114d0c38478a3a450438730f0fc.png)'
- en: Image by author using Midjourney
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者使用Midjourney生成
- en: In today’s data-driven world, organizations often face challenges with diverse
    and inconsistent data sources. Entity resolution, also called record linkage or
    deduplication, helps identify and merge duplicate or related records that do not
    share any unique identifiers within or across datasets. Accurate entity resolution
    improves data quality, enhances decision-making, and provides valuable insights.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在当今数据驱动的世界中，组织常常面临多样且不一致的数据来源的挑战。实体解析，也称为记录链接或去重，帮助识别和合并在数据集内或跨数据集没有共享唯一标识符的重复或相关记录。准确的实体解析提高了数据质量，增强了决策制定，并提供了有价值的见解。
- en: '![](../Images/7366b7a19c73f8c0febc9bad93abd0bf.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7366b7a19c73f8c0febc9bad93abd0bf.png)'
- en: Entity resolution identifies the same real-world entity within or across inconsistent
    data sources (Image by author)
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 实体解析在不一致的数据源中识别相同的现实世界实体（图像由作者提供）
- en: Entity resolution applies to various industries. For example, CRM systems consolidate
    customer information, improve service, and enable targeted marketing by resolving
    duplicate customer records. E-commerce platforms use entity resolution to merge
    product listings, enhancing search functionality, recommendations, and the customer
    experience.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 实体解析适用于各种行业。例如，CRM 系统通过解析重复的客户记录来整合客户信息、改善服务和实现精准营销。电子商务平台使用实体解析来合并产品列表，提升搜索功能、推荐系统和客户体验。
- en: In this post, we will explore the technical details of fundamental entity resolution
    approaches using a benchmark dataset.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们将探讨使用基准数据集的基本实体解析方法的技术细节。
- en: Table of Contents
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 目录
- en: '[Overview of Entity Resolution](#41b0)'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[实体解析概述](#41b0)'
- en: '[Benchmark Dataset](#e9d0)'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[基准数据集](#e9d0)'
- en: '[Blocking](#b64f)'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[阻断](#b64f)'
- en: '[Block Processing](#bad6)'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[块处理](#bad6)'
- en: '[Entity Matching](#095a)'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[实体匹配](#095a)'
- en: '[Clustering](#12bb)'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[聚类](#12bb)'
- en: '[Cluster Evaluation](#9478)'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[集群评估](#9478)'
- en: Overview of Entity Resolution
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实体解析概述
- en: 'The standard entity resolution (ER) framework consists of several steps: blocking,
    block processing, entity matching, and clustering.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 标准的实体解析（ER）框架包括几个步骤：阻断、块处理、实体匹配和聚类。
- en: '**1\. Blocking**: This is the first step in entity resolution and aims to reduce
    the search space to identify the same entity by dividing the dataset into smaller,
    manageable blocks. These blocks contain records that share similar attributes,
    making the subsequent comparison more efficient.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '**1\. 阻断**：这是实体解析的第一步，旨在通过将数据集划分为较小、可管理的块来减少搜索空间，从而识别相同的实体。这些块包含共享类似属性的记录，使后续的比较更加高效。'
- en: '**2\. Block Processing**: This step refines the blocks to minimize the number
    of comparisons by discarding two types of unnecessary comparisons: the redundant
    ones, which are repeated across multiple blocks, and the superfluous ones, which
    involve records unlikely to match.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '**2\. 块处理**：此步骤通过丢弃两种不必要的比较来精化块，从而最小化比较的数量：冗余比较，即在多个块中重复出现的比较，以及无用比较，即涉及不太可能匹配的记录的比较。'
- en: '**3\. Entity Matching**: This focuses on comparing records within blocks to
    find matches based on the similarity of the records. Various similarity metrics
    and matching algorithms can be employed to classify pairs of records as matches
    or non-matches.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '**3\. 实体匹配**：这一阶段专注于比较块中的记录，以根据记录的相似性寻找匹配。可以使用各种相似性度量和匹配算法来将记录对分类为匹配或不匹配。'
- en: '**4\. Clustering**: Clustering involves grouping the matched records into clusters
    based on their similarity. The created clusters can be used to get a consolidated
    view of entities.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '**4\. 聚类**：聚类涉及根据记录的相似性将匹配的记录分组到集群中。创建的集群可以用于获取实体的综合视图。'
- en: '![](../Images/94784cd78c3e29f33f244772d94ad1e3.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/94784cd78c3e29f33f244772d94ad1e3.png)'
- en: Entity Resolution workflow (Image by author)
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 实体解析工作流程（图像由作者提供）
- en: Benchmark Dataset
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基准数据集
- en: In the following sections, we will dive into more details of each step in the
    entity resolution process, along with Python implementation using a benchmark
    dataset.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将深入探讨实体解析过程中的每个步骤的更多细节，并使用基准数据集进行 Python 实现。
- en: The dataset, sourced from [the database group at the University of Leipzig](https://dbs.uni-leipzig.de/research/projects/object_matching/benchmark_datasets_for_entity_resolution)
    and licensed with [Creative Commons](https://creativecommons.org/licenses/by/4.0/),
    is derived from actual records concerning songs from the [MusicBrainz](https://musicbrainz.org/)
    database but has been deliberately altered using [the **DAPO** data pollution
    tool](https://vsis-www.informatik.uni-hamburg.de/getDoc.php/publications/568/Panse-TBD2021-Preprint.pdf).
    This tool injects both duplicates and errors into the dataset, resulting in a
    situation where it contains duplicates for 50% of the original records in two
    to five sources. These duplicates have been generated with a high degree of corruption,
    serving as a rigorous test to evaluate the effectiveness of ER and clustering
    approaches.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 该数据集来源于[莱比锡大学数据库组](https://dbs.uni-leipzig.de/research/projects/object_matching/benchmark_datasets_for_entity_resolution)，并获得了[创作共用](https://creativecommons.org/licenses/by/4.0/)许可证，来源于实际的[MusicBrainz](https://musicbrainz.org/)数据库中有关歌曲的记录，但经过[**DAPO**
    数据污染工具](https://vsis-www.informatik.uni-hamburg.de/getDoc.php/publications/568/Panse-TBD2021-Preprint.pdf)故意进行了修改。该工具向数据集中注入了重复项和错误，导致数据集中包含了50%原始记录的重复项，覆盖了两个到五个来源。这些重复项具有较高的损坏程度，作为评估ER和聚类方法有效性的严格测试。
- en: We can load the data with the following code.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用以下代码加载数据。
- en: '[PRE0]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Some example records look like something below.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 一些示例记录如下所示。
- en: '![](../Images/81dd39eb0071ef0cacb8620dbedf8aba.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/81dd39eb0071ef0cacb8620dbedf8aba.png)'
- en: Each record represents a song having attributes such as artist, title, album,
    year, etc (You can find field descriptions in [this link](https://dbs.uni-leipzig.de/files/datasets/saeedi/musicBrainz_readme.txt)).
    `CID` is cluster ID and the records having the same `CID` are duplicates (in the
    example above all three records represent the same song). Our goal is to identify
    those duplicates in this noisy dataset.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 每条记录代表一首歌曲，具有诸如艺术家、标题、专辑、年份等属性（你可以在[这个链接](https://dbs.uni-leipzig.de/files/datasets/saeedi/musicBrainz_readme.txt)中找到字段描述）。`CID`是集群ID，具有相同`CID`的记录是重复的（在上面的示例中，所有三条记录代表同一首歌曲）。我们的目标是在这个嘈杂的数据集中识别这些重复项。
- en: To simplify our work, we are focusing only on English songs. The code below
    identifies records with cluster IDs that have English songs.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简化工作，我们只关注英文歌曲。下面的代码识别出具有英文歌曲的集群ID的记录。
- en: '[PRE1]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: We are also preprocessing some of the string fields to get standardized values.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还对一些字符串字段进行预处理，以获得标准化的值。
- en: '[PRE2]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Please note that this benchmark dataset is a single dataset, and if you have
    multiple data sources for which you want to resolve entities, you need to standardize
    their data schemas and consolidate these multiple data sources into a unified
    dataset before proceeding with the subsequent steps.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，这个基准数据集是一个单一的数据集，如果你有多个数据来源需要解决实体问题，你需要标准化它们的数据模式，并将这些多个数据源整合成一个统一的数据集，然后再继续后续步骤。
- en: Blocking
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 阻断
- en: Blocking is the first step in entity resolution that groups similar records
    together based on certain attributes. By doing so, the process narrows its search
    to only consider comparisons within each block, rather than examining all possible
    record pairs in the dataset. This significantly reduces the number of comparisons
    and accelerates the ER process. As it skips many comparisons, it possibly leads
    to missed true matches. Therefore, Blocking should achieve a good balance between
    efficiency and accuracy. In this section, we will explore three different blocking
    approaches (standard blocking, token blocking, and sorted neighborhood) to find
    the best balance on that trade-off.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 阻断是实体解析的第一步，它根据某些属性将相似记录分组。通过这样做，过程将搜索范围缩小到仅考虑每个块内的比较，而不是检查数据集中的所有可能记录对。这显著减少了比较的数量，加快了ER过程。由于跳过了许多比较，这可能会导致错过真实匹配。因此，阻断应该在效率和准确性之间取得良好的平衡。在本节中，我们将探索三种不同的阻断方法（标准阻断、标记阻断和排序邻域），以找到最佳的平衡点。
- en: Standard Blocking
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 标准阻断
- en: The most straightforward blocking technique involves partitioning the dataset
    into blocks based on a specific attribute. For example, in our dataset, one might
    create blocks based on `Artist` or `Title` field. This approach is intuitive and
    easy to implement, but its effectiveness is very sensitive to noise, as the slightest
    difference in the blocking keys of duplicates places them in different blocks.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 最直接的块处理技术是根据特定属性将数据集划分为块。例如，在我们的数据集中，可以根据 `Artist` 或 `Title` 字段创建块。这种方法直观且易于实现，但其有效性对噪声非常敏感，因为重复项的阻塞键有一点点不同就会把它们放在不同的块中。
- en: '![](../Images/0b30929d7fce27fe4213ffac684fca7a.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0b30929d7fce27fe4213ffac684fca7a.png)'
- en: Example of Standard Blocking on Artist field (Image by author)
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在艺术家字段上的标准块示例（图像由作者提供）
- en: We can get standard blocks with the function below. The dictionary `blocks`will
    store blocking keys (`key`) and their corresponding indices (`idx`) of blocked
    records.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用以下函数获得标准块。字典 `blocks` 将存储阻塞键（`key`）及其对应的已阻塞记录的索引（`idx`）。
- en: '[PRE3]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: In the following code, we are creating three independent standard blocks using
    the fields of `title`, `artist`, and `album`.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下代码中，我们使用 `title`、`artist` 和 `album` 字段创建三个独立的标准块。
- en: '[PRE4]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Token Blocking
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 令牌块
- en: 'Token blocking focuses on breaking down (i.e. tokenizing) the values of attributes
    into smaller units, called tokens, and then using these tokens to create blocks
    for comparison. Tokens are typically single words or small n-grams (substrings
    of length `n`) extracted from the text. Token blocking creates a block for every
    distinct token value, regardless of the associated attributes: two records will
    be in the same block if they share a token in any of their attributes. This yields
    high recall, due to redundancy (i.e. a single record can belong to multiple blocks),
    at the cost of low precision.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 令牌块处理的重点是将属性值分解（即令牌化）为更小的单位，称为令牌，然后使用这些令牌创建用于比较的块。令牌通常是从文本中提取的单个单词或小的 n-gram（长度为`n`的子字符串）。令牌块为每个不同的令牌值创建一个块，而不考虑相关属性：如果两个记录在其任何属性中共享一个令牌，它们将位于同一个块中。这产生了高召回率，因为冗余（即单个记录可以属于多个块），代价是低精确度。
- en: '![](../Images/8e81d019b8e4f0d7428eb0e7d3ddd14f.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8e81d019b8e4f0d7428eb0e7d3ddd14f.png)'
- en: Example of Token Blocking (Image by author)
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 令牌块的示例（图像由作者提供）
- en: The function below generates token blocks based on word tokens. Please note
    we are excluding stop words (e.g. “a”, “the”, “is”, etc) from the tokens.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 以下函数基于单词令牌生成令牌块。请注意，我们排除了停用词（例如“a”、“the”、“is”等）中的令牌。
- en: '[PRE5]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'As we know which fields are relevant to create blocks, we only use specific
    fields (`title`, `artist`, and `album`) to perform token blocking:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们知道哪些字段与创建块相关，我们仅使用特定字段（`title`、`artist` 和 `album`）来执行令牌块处理：
- en: '[PRE6]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Sorted Neighborhood
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 排序邻域
- en: Sorted Neighborhood sorts records by specific fields’ values in alphabetical
    order. A fixed-size window slides over the sorted records and all the possible
    pairs within the window are identified as candidate pairs for comparison. Please
    note that it directly produces a list of pairs instead of blocks. While this method
    effectively handles noise in blocking fields, opting for a smaller window sacrifices
    recall in favor of precision, whereas a larger window has higher recall with lower
    precision.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 排序邻域按特定字段的值进行字母顺序排序。一个固定大小的窗口在排序后的记录上滑动，窗口内的所有可能对被标识为比较的候选对。请注意，它直接生成一对对的列表，而不是块。虽然这种方法有效处理了阻塞字段中的噪声，但选择较小的窗口会牺牲召回率以提高精度，而较大的窗口具有更高的召回率但精度较低。
- en: '![](../Images/358bc5d1ef9c4662d5383dd2c6ff244a.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/358bc5d1ef9c4662d5383dd2c6ff244a.png)'
- en: Example of Sorted Neighborhood with window size 3 (Image by author)
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 带有窗口大小为3的排序邻域示例（图像由作者提供）
- en: The code below performs Sorted Neighborhood with window size 3, using the fields
    of `title`, `artist`, and `album` as the sorting keys.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码执行窗口大小为3的排序邻域，使用 `title`、`artist` 和 `album` 字段作为排序键。
- en: '[PRE7]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: We will compare the performance of the three approaches discussed in this section
    after performing block processing and entity matching in the next two sections.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在接下来的两个部分中进行块处理和实体匹配后，比较这三种方法的性能。
- en: Block Processing
  id: totrans-68
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 块处理
- en: This step aims to improve the precision of blocks while maintaining a comparable
    level of recall. The relevant techniques involve reducing unnecessary and redundant
    comparisons within the input set of blocks `B`, resulting in generation of a new
    set of blocks `B′` with improved precision. We will explore some of the major
    block-processing techniques in this section.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 这一步旨在提高块的精度，同时保持可比较的召回率水平。相关技术包括在输入块集合`B`内减少不必要和冗余的比较，从而生成一个具有改进精度的新块集合`B'`。我们将在本节探讨一些主要的块处理技术。
- en: Block Purging
  id: totrans-70
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 块清理
- en: Block Purging sets an upper limit on the block size and purges blocks if their
    sizes go over the limit. It assumes that excessively large blocks are dominated
    by redundant comparisons, meaning that duplicates contained in those blocks are
    more likely to appear in other smaller blocks.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 块清理设置块大小的上限，并清除大小超过限制的块。它假设过大的块由冗余比较主导，这意味着这些块中包含的重复项更可能出现在其他较小的块中。
- en: The code below purges blocks by a predetermined limit (set as 1000 records here).
    It also filters out blocks with just one record as they do not create pairs to
    compare. We are performing this `purge_blocks` function on the three standard
    blocks and the token blocks from the previous section.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的代码按预定的限制值（此处设为1000条记录）清除块。它还过滤掉只有一条记录的块，因为这些块不生成可比较的对。我们在前一节的三个标准块和标记块上执行此`purge_blocks`函数。
- en: '[PRE8]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Meta-Blocking
  id: totrans-74
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 元块
- en: 'Meta-blocking transforms the input block collection into a graph (or adjacency
    matrix), where each node corresponds to a record, and edges link every pair of
    records that co-occur in a block. An edge weight represents the frequency of pair
    occurrences across blocks: higher weights indicate a greater likelihood of a match.
    Edges with low weights are pruned, as they likely represent superfluous comparisons.
    Consequently, for each retained edge, a new block is generated, resulting in a
    refined block collection (or a list of pairs as each of the refined blocks only
    has a single pair of records).'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 元块转换输入块集合为图（或邻接矩阵），其中每个节点对应一条记录，边连接每对在块中共同出现的记录。边权重表示跨块对出现频率：权重越高，匹配可能性越大。低权重的边被剪枝，因为它们可能代表多余的比较。因此，对于每个保留的边，生成一个新块，导致精细化的块集合（或作为每个精细化块仅有一对记录的对列表）。
- en: '![](../Images/07a052fd6c6e4d3cb0adfdaf29330ca5.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/07a052fd6c6e4d3cb0adfdaf29330ca5.png)'
- en: Example of Meta Blocking (Image by author)
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 元块示例（作者提供的图片）
- en: We are performing meta-blocking only on token blocks as they have many overlaps
    across the blocks. The following code creates a list of pairs from the token blocks
    first and then converts it into an adjacency matrix.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 我们仅对标记块执行元块，因为它们在块之间有很多重叠。下面的代码首先从标记块中创建一对对的列表，然后将其转换为邻接矩阵。
- en: '[PRE9]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Next, we are pruning edges in the adjacency matrix based on the edge weight.
    Here we are pruning all edges with edge weight 1, meaning pairs that only appear
    in a single block are trimmed.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们根据边权重在邻接矩阵中剪枝。在这里，我们剪枝所有边权重为1的边，即仅在单个块中出现的对被修剪。
- en: '[PRE10]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Then, we get pairs from the pruned adjacency matrix.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 接着，我们从经过剪枝的邻接矩阵中获取对。
- en: '[PRE11]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Union of Blocks
  id: totrans-84
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 块的并集
- en: In the case of the standard blocks, we obtain a union of the three independent
    blocks. First, we convert the blocks into a list of adjacency matrices.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 对于标准块，我们获取三个独立块的并集。首先，我们将块转换为邻接矩阵列表。
- en: '[PRE12]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Then, we get a union of the matrices and candidate pairs from it.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 接着，我们从矩阵的并集和其中的候选对中获取结果。
- en: '[PRE13]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: The table below summarizes the final number of candidate pairs from the three
    different blocking approaches.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 下表总结了三种不同块处理方法生成的最终候选对数量。
- en: '![](../Images/38dec5fe9bbd7727e0eee52531dadc56.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/38dec5fe9bbd7727e0eee52531dadc56.png)'
- en: We will determine which one is the best for our data by looking at a matching
    result in the next section.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将通过查看下一节中的匹配结果来确定哪个最适合我们的数据。
- en: Entity Matching
  id: totrans-92
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实体匹配
- en: 'In this step, we identify matched pairs from the candidate pairs generated
    in the previous step. While there are various methods to find matches, one straightforward
    approach can be outlined as follows:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一步骤中，我们从上一步生成的候选对中识别匹配对。虽然有多种方法可以找到匹配，但一种简单直接的方法可以如下概述：
- en: '**Measure similarity on each attribute**'
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**在每个属性上测量相似度**'
- en: You can use any similarity metrics such as cosine similarity, Jaccard similarity,
    or Levenshtein distance similarity, based on suitability for your data or specific
    requirements. Text fields may want to be tokenized before computing similarity
    for some of the metrics.
  id: totrans-95
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你可以使用任何相似性度量，如余弦相似度、杰卡德相似度或莱文斯坦距离相似度，根据你的数据或具体要求的适用性。在计算某些度量的相似性之前，文本字段可能需要进行分词。
- en: '**Compute overall similarity**'
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**计算整体相似性**'
- en: This step combines the per-attribute similarities into an overall similarity
    score either by applying manually defined rules or utilizing a machine learning
    model trained on labeled data if available.
  id: totrans-97
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 此步骤将每个属性的相似性组合成一个整体的相似性分数，可以通过应用手动定义的规则或利用在标记数据上训练的机器学习模型来实现。
- en: '**Determine matches**'
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**确定匹配**'
- en: A similarity threshold is applied to the overall similarity score to find matches
  id: totrans-99
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对整体相似性分数应用相似性阈值以找到匹配
- en: '![](../Images/accddcd7befa2c43c5d341ff7ee8887f.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/accddcd7befa2c43c5d341ff7ee8887f.png)'
- en: Example of Entity Matching (Image by author)
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 实体匹配示例（图片由作者提供）
- en: The function `get_field_similarity_scores` below takes care of the step 1 above.
    If `sim_type` is set to `“fuzzy”`, it calculates [cosine similarity](https://en.wikipedia.org/wiki/Cosine_similarity);
    otherwise, it performs an exact match. The cosine similarity is calculated on
    character level 3-grams which are vectorized from input strings using the `[CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)`
    module from scikit-learn. We compute the cosine similarity for the fields of `title`,
    `artist`, and `album`, while performing an exact match on `number` field.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 以下函数 `get_field_similarity_scores` 处理上述第1步。如果 `sim_type` 设置为 `“fuzzy”`，则计算 [余弦相似度](https://en.wikipedia.org/wiki/Cosine_similarity)；否则，进行精确匹配。余弦相似度是在字符级
    3-grams 上计算的，这些 3-grams 通过使用来自 scikit-learn 的 `[CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)`
    模块从输入字符串中向量化。我们计算 `title`、`artist` 和 `album` 字段的余弦相似度，同时对 `number` 字段进行精确匹配。
- en: '[PRE14]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Rule-based matching
  id: totrans-104
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基于规则的匹配
- en: '![](../Images/203cc7d2d670653a4fde2a549743cbea.png)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/203cc7d2d670653a4fde2a549743cbea.png)'
- en: Rule-based matching (Image by author)
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 基于规则的匹配（图片由作者提供）
- en: 'After computing field-specific similarity scores, we want to combine them into
    an overall similarity score, as outlined in step 2 above. We perform a very simple
    approach here: we just calculate the average of the attributes’ scores, and subsequently,
    we apply a score threshold to identify matches (step 3). The threshold value below
    is already tuned here, but you may want to tune it by looking at some examples
    of matched/unmatched pairs when you work on your own dataset.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在计算特定领域的相似性分数后，我们希望将它们组合成一个整体的相似性分数，如上述第2步所述。我们在这里采用了一个非常简单的方法：仅计算属性分数的平均值，随后应用一个分数阈值来识别匹配（第3步）。下面的阈值值已在这里调整过，但你可能需要通过查看一些匹配/不匹配的示例来调整它，当你处理自己的数据集时。
- en: '[PRE15]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: The code above performs matching on the pairs from the standard block. Additionally,
    we extend this matching process to the pairs from the token block and the sorted
    neighborhood, allowing us to compare their performances. The code below summarizes
    the comparison in a table.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的代码对标准块中的对进行了匹配。此外，我们将此匹配过程扩展到来自 token block 和 sorted neighborhood 的对，允许我们比较它们的表现。下面的代码在表格中总结了比较结果。
- en: '[PRE16]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Below is the output.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是输出结果。
- en: '![](../Images/712486e6f28de580c3123493b8ea74f0.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/712486e6f28de580c3123493b8ea74f0.png)'
- en: As you can see in the table, Token Blocking yields the highest number of matches,
    while Sorted Neighborhood has the highest matching rate. As Token Blocking likely
    has the fewest missed matches, we will proceed with the outcome from this approach.
    It is worth noting that our small dataset does not present scalability concerns.
    However, for larger datasets where Token Blocking may not be feasible, you may
    want to consider the other more scalable approaches.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 如表中所示，Token Blocking 产生了最多的匹配数，而 Sorted Neighborhood 的匹配率最高。由于 Token Blocking
    可能错过的匹配最少，我们将继续使用这种方法的结果。值得注意的是，我们的小数据集并未显示出可扩展性问题。然而，对于较大的数据集，其中 Token Blocking
    可能不可行，你可能需要考虑其他更具可扩展性的方法。
- en: Machine-learning matching
  id: totrans-114
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 机器学习匹配
- en: '![](../Images/f6eb09c7745036d3fe672ac729d7f969.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f6eb09c7745036d3fe672ac729d7f969.png)'
- en: Machine-learning matching (Image by author)
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习匹配（图片由作者提供）
- en: If you have labeled data or have manually labeled sample pairs as matches or
    non-matches, you can train a machine-learning model to predict matched pairs.
    As our data has cluster labels `CID`, we will convert these into matching labels
    (match/unmatch) for pairs and train a machine-learning model, subsequently comparing
    its performance with the rule-based approach performed in the previous section.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您有标记数据或手动标记的样本对（作为匹配或非匹配），您可以训练一个机器学习模型来预测匹配的对。由于我们的数据具有集群标签`CID`，我们将这些转换为对的匹配标签（匹配/非匹配），并训练一个机器学习模型，随后与前一节中执行的基于规则的方法进行性能比较。
- en: The following code generates the model input `X` and the corresponding target
    variable `y`. Pairs within the same cluster `CID` are designated as matches (`y
    = 1`), while pairs outside the same cluster are labeled as non-matches (`y = 0`).
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码生成模型输入`X`和相应的目标变量`y`。同一集群`CID`内的对被标记为匹配（`y = 1`），而不同集群内的对被标记为非匹配（`y = 0`）。
- en: '[PRE17]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Next, we split them into training and testing sets, followed by training a logistic
    regression model.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将它们分为训练集和测试集，然后训练逻辑回归模型。
- en: '[PRE18]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: The code below compares its performance (`[f1_score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html)`)
    with the rule-based approach.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的代码将其性能与基于规则的方法进行比较（`[f1_score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html)`）。
- en: '[PRE19]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Below is the output:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是输出：
- en: '![](../Images/969acd2480dfd8b7aa2f7713fd3bbe0d.png)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![图片链接](../Images/969acd2480dfd8b7aa2f7713fd3bbe0d.png)'
- en: While the model’s performance is better, the performance of the rule-based approach
    may still be reasonably good.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管模型的性能更好，基于规则的方法的性能仍然可能相当不错。
- en: For the following steps, we will use matches identified through the rule-based
    approach, considering that in many real-world cases, manual data labeling might
    not be practical due to resource constraints. The code below extracts matched
    pairs and their similarity scores from the candidate pairs and their scores on
    token blocking.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 对于接下来的步骤，我们将使用通过基于规则的方法识别出的匹配项，考虑到在许多实际情况下，由于资源限制，手动数据标记可能并不实际。下面的代码从候选对中提取匹配对及其相似性分数及其在标记阻塞上的分数。
- en: '[PRE20]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Clustering
  id: totrans-129
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 聚类
- en: In this step, we are creating entity clusters based on the matched pairs from
    the previous step. Each cluster includes all records corresponding to a distinct
    real-world entity.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一步中，我们基于上一步中的匹配对创建实体集群。每个集群包括所有对应于一个不同现实世界实体的记录。
- en: 'Clustering for entity resolution has several requirements:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 实体解析的聚类有几个要求：
- en: '**Unconstrained algorithm**'
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**无约束算法**'
- en: The algorithms should not require any domain-specific parameters as input, such
    as the number of clusters or the diameter of the clusters.
  id: totrans-133
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 算法不应需要任何领域特定的参数作为输入，例如集群数或集群直径。
- en: '**Capability of handling an incomplete similarity matrix**'
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**处理不完整相似性矩阵的能力**'
- en: As the entity resolution process does not compute similarity on every possible
    pair (which could be described as an N by N matrix), the algorithms must be able
    to handle an incomplete similarity matrix (or a list of matched pairs).
  id: totrans-135
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 由于实体解析过程不会在每个可能的对上计算相似性（可以描述为N乘以N矩阵），因此算法必须能够处理不完整的相似性矩阵（或匹配对列表）。
- en: '**Scalability**'
  id: totrans-136
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**可扩展性**'
- en: Entity resolution often handles sizable datasets, making it important that algorithms
    are capable of handling such data. In cases of large data, algorithms having high
    complexity like hierarchical clustering might not be practical.
  id: totrans-137
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 实体解析通常处理大型数据集，因此算法能够处理此类数据非常重要。在大数据情况下，像层次聚类这样的高复杂度算法可能不实际。
- en: 'For the clustering, we will examine three major single-pass clustering algorithms:
    Partitioning (i.e. connected components), Center Clustering, and Merge-Center
    Clustering, all of which satisfy the requirements. These algorithms are highly
    efficient as they create clusters by a single scan (or O(n) time complexity) of
    the list of candidate pairs, although some of them require the list to be sorted
    by similarity score.'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 对于聚类，我们将研究三种主要的单遍聚类算法：分区（即连接组件）、中心聚类和合并中心聚类，它们都满足要求。这些算法非常高效，因为它们通过一次扫描（或O(n)时间复杂度）候选对列表来创建集群，尽管其中一些算法要求列表按相似性分数排序。
- en: '![](../Images/07ea1680d7018fe8a75c87b34d9a83a4.png)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
  zh: '![图片链接](../Images/07ea1680d7018fe8a75c87b34d9a83a4.png)'
- en: 'Single-pass clustering algorithms (source: [http://www.vldb.org/pvldb/vol2/vldb09-1025.pdf](http://www.vldb.org/pvldb/vol2/vldb09-1025.pdf))'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 单遍聚类算法（来源：[http://www.vldb.org/pvldb/vol2/vldb09-1025.pdf](http://www.vldb.org/pvldb/vol2/vldb09-1025.pdf)）
- en: Partitioning/Connected Components
  id: totrans-141
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 划分/连通组件
- en: This algorithm initiates clustering by initially assigning each node to its
    individual cluster. Then, it conducts a single scan of the list of matched pairs.
    If it identifies connected nodes that do not belong to the same cluster, it merges
    their clusters. In short, it forms a cluster by grouping all the connected nodes
    via edges (i.e. matched records via pairs). The algorithm may create clusters
    that connect dissimilar records via long paths.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 该算法通过最初将每个节点分配到其单独的集群来启动聚类。然后，它对匹配对的列表进行单次扫描。如果发现不属于同一集群的连接节点，它将合并它们的集群。简而言之，它通过将所有连接节点通过边（即通过配对的匹配记录）分组形成一个集群。该算法可能会创建通过长路径连接不相似记录的集群。
- en: Connected components clustering can be performed using the Scipy module as you
    can see in the code below. Before performing it, you want to convert the list
    of pairs into an adjacency matrix.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 连通组件聚类可以使用Scipy模块执行，如下面的代码所示。在执行之前，你需要将配对列表转换为邻接矩阵。
- en: '[PRE21]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Center Clustering
  id: totrans-145
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 中心聚类
- en: This algorithm [[5]](#299e) performs clustering where each cluster has a center
    and all records in each cluster are similar to the center of the cluster. It requires
    the list of similar pairs to be sorted by descending order of similarity scores.
    The algorithm then performs clustering by a single scan of the sorted list. When
    a node `u` is encountered for the first time in the scan, it’s designated as the
    cluster center. Any subsequent nodes `v` that are similar to `u` (i.e., appear
    in a pair `(u, v)` in the list) are assigned to the cluster of `u` and are not
    considered again during the process.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 该算法[[5]](#299e)执行聚类，其中每个集群都有一个中心，并且每个集群中的所有记录都与该集群的中心相似。它要求相似对的列表按相似度分数的降序排序。然后，算法通过对排序列表的单次扫描来执行聚类。当第一次在扫描中遇到节点`u`时，它被指定为集群中心。任何后续与`u`相似的节点`v`（即，出现在列表中的对`(u,
    v)`中）都被分配到`u`的集群中，并且在处理过程中不再考虑。
- en: '![](../Images/30e7bb903d184ed17162526ea0dd62e7.png)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/30e7bb903d184ed17162526ea0dd62e7.png)'
- en: Example of Center Clustering (Image by author)
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 中心聚类示例（图像由作者提供）
- en: Merge-Center Clustering
  id: totrans-149
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 合并中心聚类
- en: This algorithm [[6]](#d172) performs similarly to Center clustering, but merges
    two clusters `cᵢ` and `cⱼ` whenever a record that is similar to the center of
    the cluster `cᵢ` is similar to the center of `cⱼ`. Note that when two clusters
    are merged, it does not choose a single center node, which means that merged clusters
    can have multiple center nodes. This algorithm can be performed similarly by a
    single scan of the list of similar pairs, but by keeping track of the records
    that are connected through a merged cluster.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 该算法[[6]](#d172)的功能类似于中心聚类，但每当一个与集群`cᵢ`的中心相似的记录也与`cⱼ`的中心相似时，就会合并两个集群`cᵢ`和`cⱼ`。请注意，当两个集群合并时，它不会选择一个单一的中心节点，这意味着合并的集群可以有多个中心节点。该算法可以通过类似的方式进行，即通过对相似对的列表进行单次扫描，同时跟踪通过合并集群连接的记录。
- en: '![](../Images/d7ffac0d99cbdc94df6091902cde9669.png)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d7ffac0d99cbdc94df6091902cde9669.png)'
- en: Example of Merge-Center Clustering (Image by author)
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 合并中心聚类示例（图像由作者提供）
- en: To perform Center/Merge-Center clustering, we first need to sort the list of
    pairs by descending order of the similarity scores.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 要执行中心/合并中心聚类，我们首先需要按相似度分数的降序对配对列表进行排序。
- en: '[PRE22]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Next, the code below yields two sets of pairs: center-child pairs, denoted
    as `center_cluster_pairs`, and merged node pairs, referred to as `merge_cluster_pairs`.
    We can then generate Center clusters and Merge-Center clusters by applying connected
    components to these lists of pairs.'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，下面的代码生成两个配对集合：中心-子节点配对，表示为`center_cluster_pairs`，和合并节点配对，称为`merge_cluster_pairs`。然后，我们可以通过将连通组件应用于这些配对列表来生成中心集群和合并中心集群。
- en: '[PRE23]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Cluster Evaluation
  id: totrans-157
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 集群评估
- en: 'As we have the cluster labels, we can evaluate the quality of the clusters
    using [Rand Index](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.rand_score.html)
    or [adjusted Rand Index](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.adjusted_rand_score.html).
    Rand Index is a cluster evaluation metric that represents the proportion of pairs
    that are correctly clustered together or apart. It is defined as follows:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 有了集群标签后，我们可以使用[Rand Index](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.rand_score.html)或[调整后的
    Rand Index](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.adjusted_rand_score.html)来评估集群的质量。Rand
    Index 是一种集群评估指标，表示正确聚类在一起或分开的配对比例。其定义如下：
- en: '*TP = Number of pairs that are clustered* ***together*** *in both predicted
    and true clusters.*'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '*TP = 在预测簇和真实簇中* ***同时*** *被聚类的对数。*'
- en: '*TN = Number of pairs that are clustered* ***apart*** *in both predicted and
    true clusters.*'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '*TN = 在预测簇和真实簇中* ***分开*** *被聚类的对数。*'
- en: '***Rand Index*** *= (TP + TN) / Total number of possible pairs*'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '***Rand Index*** *= (TP + TN) / 所有可能对数的总数*'
- en: '![](../Images/64c13a26cc139266e495ea3778ee4fb8.png)'
  id: totrans-162
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/64c13a26cc139266e495ea3778ee4fb8.png)'
- en: Example of Rand Index Calculation (Image by author)
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: Rand Index 计算示例（图片由作者提供）
- en: Adjusted Rand Index is a modified version of Rand Index that is corrected for
    chance. The adjustment accounts for the possibility of random agreement from clustering
    results that were randomly assigned.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 调整后的 Rand Index 是 Rand Index 的一种修改版本，已为偶然性进行修正。该调整考虑了随机分配的聚类结果可能产生的随机一致性。
- en: '![](../Images/495cfc03deac0d5898fdc893bd4a7a1c.png)'
  id: totrans-165
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/495cfc03deac0d5898fdc893bd4a7a1c.png)'
- en: Equation of adjusted Rand Index
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 调整后的 Rand Index 的方程
- en: We won’t delve into how each termin the above equation is calculated here, but
    anyone who is interested in this topic can refer to [the paper from KY Yeung](https://faculty.washington.edu/kayee/pca/supp.pdf)
    which explains the metric with some examples.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不会深入探讨上述方程中每个术语的计算方式，但任何对这个主题感兴趣的人可以参考 [KY Yeung 的论文](https://faculty.washington.edu/kayee/pca/supp.pdf)，该论文解释了这个指标并提供了一些示例。
- en: The code below gives us a comparison of the clusters using those metrics along
    with some additional basic statistics.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的代码提供了使用这些指标的簇比较以及一些额外的基本统计信息。
- en: '[PRE24]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Below is the output.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是输出结果。
- en: '![](../Images/ac1808afd199a87e6b00c6a31850c4e1.png)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ac1808afd199a87e6b00c6a31850c4e1.png)'
- en: As you can see in the table, connected components produce the larger clusters
    with the fewest cluster count, while the gap between connected components and
    the Merge-Center clusters is minimal. Conversely, the Center clusters yield the
    smaller clusters with the highest count. Please note that all three clusters have
    a perfect Rand Index, as they have a large numbe of clusters making inter-cluster
    pairs dominant (i.e. even random clustering yields a respectable Rand Index).
    However, if you look at the Adjusted Rand Index, Merge-Center clustering is the
    best, while its difference from connected components is marginal.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 从表中可以看出，连接组件生成的簇较大且簇数最少，而连接组件与 Merge-Center 簇之间的差距最小。相反，Center 簇生成的簇较小且数量最多。请注意，所有三个簇的
    Rand Index 都是完美的，因为它们有大量的簇，使得簇间对形成主导地位（即即使是随机簇也会得到相当的 Rand Index）。然而，如果你查看调整后的
    Rand Index，Merge-Center 聚类表现最佳，其与连接组件的差异很小。
- en: That concludes our exploration of the entity resolution framework. How you proceed
    with the created clusters depends on your specific business requirements or use
    case. If you aim to establish a canonical representation for each cluster, you
    can achieve this by extracting the most representative value (such as the most
    frequent value) for each field within each cluster.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是我们对实体解析框架的探索。你如何处理创建的簇取决于你的具体业务需求或使用案例。如果你的目标是为每个簇建立规范的表示，你可以通过提取每个簇内每个字段的最具代表性的值（如最频繁的值）来实现。
- en: If you are interested, the whole code is available in the Google Colab and the
    GitHub repo below.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你感兴趣，完整的代码可以在下面的 Google Colab 和 GitHub 仓库中找到。
- en: '[](https://colab.research.google.com/drive/1Wkw-uaNX9Im9PNnw8CpUgWQHxxDxeiDr?usp=sharing&source=post_page-----3e8c59f4f41c--------------------------------)
    [## Google Colaboratory'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: '[## Google Colaboratory](https://colab.research.google.com/drive/1Wkw-uaNX9Im9PNnw8CpUgWQHxxDxeiDr?usp=sharing&source=post_page-----3e8c59f4f41c--------------------------------)'
- en: Entity Resolution
  id: totrans-176
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 实体解析
- en: colab.research.google.com](https://colab.research.google.com/drive/1Wkw-uaNX9Im9PNnw8CpUgWQHxxDxeiDr?usp=sharing&source=post_page-----3e8c59f4f41c--------------------------------)
    [](https://github.com/tomonori-masui/entity-resolution/blob/main/entity_resolution_implementations.ipynb?source=post_page-----3e8c59f4f41c--------------------------------)
    [## entity-resolution/entity_resolution_implementations.ipynb
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: '[colab.research.google.com](https://colab.research.google.com/drive/1Wkw-uaNX9Im9PNnw8CpUgWQHxxDxeiDr?usp=sharing&source=post_page-----3e8c59f4f41c--------------------------------)
    [## entity-resolution/entity_resolution_implementations.ipynb](https://github.com/tomonori-masui/entity-resolution/blob/main/entity_resolution_implementations.ipynb?source=post_page-----3e8c59f4f41c--------------------------------)'
- en: Entity Resolution
  id: totrans-178
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 实体解析
- en: github.com](https://github.com/tomonori-masui/entity-resolution/blob/main/entity_resolution_implementations.ipynb?source=post_page-----3e8c59f4f41c--------------------------------)
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: '[github.com](https://github.com/tomonori-masui/entity-resolution/blob/main/entity_resolution_implementations.ipynb?source=post_page-----3e8c59f4f41c--------------------------------)'
- en: References
  id: totrans-180
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] Christophides et al., [End-to-End Entity Resolution for Big Data: A Survey](https://arxiv.org/abs/1905.06397)
    (2019)'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] Christophides 等人，[大数据端到端实体解析：综述](https://arxiv.org/abs/1905.06397)（2019）'
- en: '[2] Papadakis et al., [Comparative analysis of approximate blocking techniques
    for entity resolution](http://www.vldb.org/pvldb/vol9/p684-papadakis.pdf) (2016)'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] Papadakis 等人，[实体解析的近似阻塞技术比较分析](http://www.vldb.org/pvldb/vol9/p684-papadakis.pdf)（2016）'
- en: '[3] Papadakis et al., [A survey of blocking and filtering techniques for entity
    resolution](https://arxiv.org/abs/1905.06167) (2020)'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] Papadakis 等人，[实体解析的阻塞和过滤技术综述](https://arxiv.org/abs/1905.06167)（2020）'
- en: '[4] Hassanzadeh et al., [Framework for evaluating clustering algorithms in
    duplicate detection](http://www.vldb.org/pvldb/vol2/vldb09-1025.pdf) (2009)'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] Hassanzadeh 等人，[用于重复检测的聚类算法评估框架](http://www.vldb.org/pvldb/vol2/vldb09-1025.pdf)（2009）'
- en: '[5] Haveliwala et al., [Scalable techniques for clustering the web](https://www.researchgate.net/publication/221035516_Scalable_Techniques_for_Clustering_the_Web)
    (2000)'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] Haveliwala 等人，[用于网页聚类的可扩展技术](https://www.researchgate.net/publication/221035516_Scalable_Techniques_for_Clustering_the_Web)（2000）'
- en: '[6] Hassanzadeh & Miller, [Creating probabilistic databases from duplicated
    data](https://www.researchgate.net/publication/220473509_Creating_probabilistic_databases_from_duplicated_data)
    (2009)'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: '[6] Hassanzadeh & Miller，[从重复数据中创建概率数据库](https://www.researchgate.net/publication/220473509_Creating_probabilistic_databases_from_duplicated_data)（2009）'
