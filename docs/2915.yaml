- en: 'Entity Resolution: Identifying Real-World Entities in Noisy Data'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/entity-resolution-identifying-real-world-entities-in-noisy-data-3e8c59f4f41c?source=collection_archive---------3-----------------------#2023-09-21](https://towardsdatascience.com/entity-resolution-identifying-real-world-entities-in-noisy-data-3e8c59f4f41c?source=collection_archive---------3-----------------------#2023-09-21)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Fundamental theories and Python implementations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@tnmasui?source=post_page-----3e8c59f4f41c--------------------------------)[![Tomonori
    Masui](../Images/e3c6ffae4b4f748394e743a349ab7e59.png)](https://medium.com/@tnmasui?source=post_page-----3e8c59f4f41c--------------------------------)[](https://towardsdatascience.com/?source=post_page-----3e8c59f4f41c--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----3e8c59f4f41c--------------------------------)
    [Tomonori Masui](https://medium.com/@tnmasui?source=post_page-----3e8c59f4f41c--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F703ffb2ff12d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fentity-resolution-identifying-real-world-entities-in-noisy-data-3e8c59f4f41c&user=Tomonori+Masui&userId=703ffb2ff12d&source=post_page-703ffb2ff12d----3e8c59f4f41c---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----3e8c59f4f41c--------------------------------)
    ·19 min read·Sep 21, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F3e8c59f4f41c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fentity-resolution-identifying-real-world-entities-in-noisy-data-3e8c59f4f41c&user=Tomonori+Masui&userId=703ffb2ff12d&source=-----3e8c59f4f41c---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F3e8c59f4f41c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fentity-resolution-identifying-real-world-entities-in-noisy-data-3e8c59f4f41c&source=-----3e8c59f4f41c---------------------bookmark_footer-----------)![](../Images/1d691114d0c38478a3a450438730f0fc.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Image by author using Midjourney
  prefs: []
  type: TYPE_NORMAL
- en: In today’s data-driven world, organizations often face challenges with diverse
    and inconsistent data sources. Entity resolution, also called record linkage or
    deduplication, helps identify and merge duplicate or related records that do not
    share any unique identifiers within or across datasets. Accurate entity resolution
    improves data quality, enhances decision-making, and provides valuable insights.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7366b7a19c73f8c0febc9bad93abd0bf.png)'
  prefs: []
  type: TYPE_IMG
- en: Entity resolution identifies the same real-world entity within or across inconsistent
    data sources (Image by author)
  prefs: []
  type: TYPE_NORMAL
- en: Entity resolution applies to various industries. For example, CRM systems consolidate
    customer information, improve service, and enable targeted marketing by resolving
    duplicate customer records. E-commerce platforms use entity resolution to merge
    product listings, enhancing search functionality, recommendations, and the customer
    experience.
  prefs: []
  type: TYPE_NORMAL
- en: In this post, we will explore the technical details of fundamental entity resolution
    approaches using a benchmark dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Table of Contents
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Overview of Entity Resolution](#41b0)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Benchmark Dataset](#e9d0)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Blocking](#b64f)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Block Processing](#bad6)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Entity Matching](#095a)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Clustering](#12bb)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Cluster Evaluation](#9478)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Overview of Entity Resolution
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The standard entity resolution (ER) framework consists of several steps: blocking,
    block processing, entity matching, and clustering.'
  prefs: []
  type: TYPE_NORMAL
- en: '**1\. Blocking**: This is the first step in entity resolution and aims to reduce
    the search space to identify the same entity by dividing the dataset into smaller,
    manageable blocks. These blocks contain records that share similar attributes,
    making the subsequent comparison more efficient.'
  prefs: []
  type: TYPE_NORMAL
- en: '**2\. Block Processing**: This step refines the blocks to minimize the number
    of comparisons by discarding two types of unnecessary comparisons: the redundant
    ones, which are repeated across multiple blocks, and the superfluous ones, which
    involve records unlikely to match.'
  prefs: []
  type: TYPE_NORMAL
- en: '**3\. Entity Matching**: This focuses on comparing records within blocks to
    find matches based on the similarity of the records. Various similarity metrics
    and matching algorithms can be employed to classify pairs of records as matches
    or non-matches.'
  prefs: []
  type: TYPE_NORMAL
- en: '**4\. Clustering**: Clustering involves grouping the matched records into clusters
    based on their similarity. The created clusters can be used to get a consolidated
    view of entities.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/94784cd78c3e29f33f244772d94ad1e3.png)'
  prefs: []
  type: TYPE_IMG
- en: Entity Resolution workflow (Image by author)
  prefs: []
  type: TYPE_NORMAL
- en: Benchmark Dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the following sections, we will dive into more details of each step in the
    entity resolution process, along with Python implementation using a benchmark
    dataset.
  prefs: []
  type: TYPE_NORMAL
- en: The dataset, sourced from [the database group at the University of Leipzig](https://dbs.uni-leipzig.de/research/projects/object_matching/benchmark_datasets_for_entity_resolution)
    and licensed with [Creative Commons](https://creativecommons.org/licenses/by/4.0/),
    is derived from actual records concerning songs from the [MusicBrainz](https://musicbrainz.org/)
    database but has been deliberately altered using [the **DAPO** data pollution
    tool](https://vsis-www.informatik.uni-hamburg.de/getDoc.php/publications/568/Panse-TBD2021-Preprint.pdf).
    This tool injects both duplicates and errors into the dataset, resulting in a
    situation where it contains duplicates for 50% of the original records in two
    to five sources. These duplicates have been generated with a high degree of corruption,
    serving as a rigorous test to evaluate the effectiveness of ER and clustering
    approaches.
  prefs: []
  type: TYPE_NORMAL
- en: We can load the data with the following code.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Some example records look like something below.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/81dd39eb0071ef0cacb8620dbedf8aba.png)'
  prefs: []
  type: TYPE_IMG
- en: Each record represents a song having attributes such as artist, title, album,
    year, etc (You can find field descriptions in [this link](https://dbs.uni-leipzig.de/files/datasets/saeedi/musicBrainz_readme.txt)).
    `CID` is cluster ID and the records having the same `CID` are duplicates (in the
    example above all three records represent the same song). Our goal is to identify
    those duplicates in this noisy dataset.
  prefs: []
  type: TYPE_NORMAL
- en: To simplify our work, we are focusing only on English songs. The code below
    identifies records with cluster IDs that have English songs.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: We are also preprocessing some of the string fields to get standardized values.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Please note that this benchmark dataset is a single dataset, and if you have
    multiple data sources for which you want to resolve entities, you need to standardize
    their data schemas and consolidate these multiple data sources into a unified
    dataset before proceeding with the subsequent steps.
  prefs: []
  type: TYPE_NORMAL
- en: Blocking
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Blocking is the first step in entity resolution that groups similar records
    together based on certain attributes. By doing so, the process narrows its search
    to only consider comparisons within each block, rather than examining all possible
    record pairs in the dataset. This significantly reduces the number of comparisons
    and accelerates the ER process. As it skips many comparisons, it possibly leads
    to missed true matches. Therefore, Blocking should achieve a good balance between
    efficiency and accuracy. In this section, we will explore three different blocking
    approaches (standard blocking, token blocking, and sorted neighborhood) to find
    the best balance on that trade-off.
  prefs: []
  type: TYPE_NORMAL
- en: Standard Blocking
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The most straightforward blocking technique involves partitioning the dataset
    into blocks based on a specific attribute. For example, in our dataset, one might
    create blocks based on `Artist` or `Title` field. This approach is intuitive and
    easy to implement, but its effectiveness is very sensitive to noise, as the slightest
    difference in the blocking keys of duplicates places them in different blocks.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0b30929d7fce27fe4213ffac684fca7a.png)'
  prefs: []
  type: TYPE_IMG
- en: Example of Standard Blocking on Artist field (Image by author)
  prefs: []
  type: TYPE_NORMAL
- en: We can get standard blocks with the function below. The dictionary `blocks`will
    store blocking keys (`key`) and their corresponding indices (`idx`) of blocked
    records.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: In the following code, we are creating three independent standard blocks using
    the fields of `title`, `artist`, and `album`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Token Blocking
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Token blocking focuses on breaking down (i.e. tokenizing) the values of attributes
    into smaller units, called tokens, and then using these tokens to create blocks
    for comparison. Tokens are typically single words or small n-grams (substrings
    of length `n`) extracted from the text. Token blocking creates a block for every
    distinct token value, regardless of the associated attributes: two records will
    be in the same block if they share a token in any of their attributes. This yields
    high recall, due to redundancy (i.e. a single record can belong to multiple blocks),
    at the cost of low precision.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8e81d019b8e4f0d7428eb0e7d3ddd14f.png)'
  prefs: []
  type: TYPE_IMG
- en: Example of Token Blocking (Image by author)
  prefs: []
  type: TYPE_NORMAL
- en: The function below generates token blocks based on word tokens. Please note
    we are excluding stop words (e.g. “a”, “the”, “is”, etc) from the tokens.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'As we know which fields are relevant to create blocks, we only use specific
    fields (`title`, `artist`, and `album`) to perform token blocking:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Sorted Neighborhood
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Sorted Neighborhood sorts records by specific fields’ values in alphabetical
    order. A fixed-size window slides over the sorted records and all the possible
    pairs within the window are identified as candidate pairs for comparison. Please
    note that it directly produces a list of pairs instead of blocks. While this method
    effectively handles noise in blocking fields, opting for a smaller window sacrifices
    recall in favor of precision, whereas a larger window has higher recall with lower
    precision.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/358bc5d1ef9c4662d5383dd2c6ff244a.png)'
  prefs: []
  type: TYPE_IMG
- en: Example of Sorted Neighborhood with window size 3 (Image by author)
  prefs: []
  type: TYPE_NORMAL
- en: The code below performs Sorted Neighborhood with window size 3, using the fields
    of `title`, `artist`, and `album` as the sorting keys.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: We will compare the performance of the three approaches discussed in this section
    after performing block processing and entity matching in the next two sections.
  prefs: []
  type: TYPE_NORMAL
- en: Block Processing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This step aims to improve the precision of blocks while maintaining a comparable
    level of recall. The relevant techniques involve reducing unnecessary and redundant
    comparisons within the input set of blocks `B`, resulting in generation of a new
    set of blocks `B′` with improved precision. We will explore some of the major
    block-processing techniques in this section.
  prefs: []
  type: TYPE_NORMAL
- en: Block Purging
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Block Purging sets an upper limit on the block size and purges blocks if their
    sizes go over the limit. It assumes that excessively large blocks are dominated
    by redundant comparisons, meaning that duplicates contained in those blocks are
    more likely to appear in other smaller blocks.
  prefs: []
  type: TYPE_NORMAL
- en: The code below purges blocks by a predetermined limit (set as 1000 records here).
    It also filters out blocks with just one record as they do not create pairs to
    compare. We are performing this `purge_blocks` function on the three standard
    blocks and the token blocks from the previous section.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Meta-Blocking
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Meta-blocking transforms the input block collection into a graph (or adjacency
    matrix), where each node corresponds to a record, and edges link every pair of
    records that co-occur in a block. An edge weight represents the frequency of pair
    occurrences across blocks: higher weights indicate a greater likelihood of a match.
    Edges with low weights are pruned, as they likely represent superfluous comparisons.
    Consequently, for each retained edge, a new block is generated, resulting in a
    refined block collection (or a list of pairs as each of the refined blocks only
    has a single pair of records).'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/07a052fd6c6e4d3cb0adfdaf29330ca5.png)'
  prefs: []
  type: TYPE_IMG
- en: Example of Meta Blocking (Image by author)
  prefs: []
  type: TYPE_NORMAL
- en: We are performing meta-blocking only on token blocks as they have many overlaps
    across the blocks. The following code creates a list of pairs from the token blocks
    first and then converts it into an adjacency matrix.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Next, we are pruning edges in the adjacency matrix based on the edge weight.
    Here we are pruning all edges with edge weight 1, meaning pairs that only appear
    in a single block are trimmed.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Then, we get pairs from the pruned adjacency matrix.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Union of Blocks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the case of the standard blocks, we obtain a union of the three independent
    blocks. First, we convert the blocks into a list of adjacency matrices.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Then, we get a union of the matrices and candidate pairs from it.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: The table below summarizes the final number of candidate pairs from the three
    different blocking approaches.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/38dec5fe9bbd7727e0eee52531dadc56.png)'
  prefs: []
  type: TYPE_IMG
- en: We will determine which one is the best for our data by looking at a matching
    result in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Entity Matching
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this step, we identify matched pairs from the candidate pairs generated
    in the previous step. While there are various methods to find matches, one straightforward
    approach can be outlined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Measure similarity on each attribute**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You can use any similarity metrics such as cosine similarity, Jaccard similarity,
    or Levenshtein distance similarity, based on suitability for your data or specific
    requirements. Text fields may want to be tokenized before computing similarity
    for some of the metrics.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Compute overall similarity**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This step combines the per-attribute similarities into an overall similarity
    score either by applying manually defined rules or utilizing a machine learning
    model trained on labeled data if available.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Determine matches**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A similarity threshold is applied to the overall similarity score to find matches
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![](../Images/accddcd7befa2c43c5d341ff7ee8887f.png)'
  prefs: []
  type: TYPE_IMG
- en: Example of Entity Matching (Image by author)
  prefs: []
  type: TYPE_NORMAL
- en: The function `get_field_similarity_scores` below takes care of the step 1 above.
    If `sim_type` is set to `“fuzzy”`, it calculates [cosine similarity](https://en.wikipedia.org/wiki/Cosine_similarity);
    otherwise, it performs an exact match. The cosine similarity is calculated on
    character level 3-grams which are vectorized from input strings using the `[CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)`
    module from scikit-learn. We compute the cosine similarity for the fields of `title`,
    `artist`, and `album`, while performing an exact match on `number` field.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Rule-based matching
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/203cc7d2d670653a4fde2a549743cbea.png)'
  prefs: []
  type: TYPE_IMG
- en: Rule-based matching (Image by author)
  prefs: []
  type: TYPE_NORMAL
- en: 'After computing field-specific similarity scores, we want to combine them into
    an overall similarity score, as outlined in step 2 above. We perform a very simple
    approach here: we just calculate the average of the attributes’ scores, and subsequently,
    we apply a score threshold to identify matches (step 3). The threshold value below
    is already tuned here, but you may want to tune it by looking at some examples
    of matched/unmatched pairs when you work on your own dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: The code above performs matching on the pairs from the standard block. Additionally,
    we extend this matching process to the pairs from the token block and the sorted
    neighborhood, allowing us to compare their performances. The code below summarizes
    the comparison in a table.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Below is the output.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/712486e6f28de580c3123493b8ea74f0.png)'
  prefs: []
  type: TYPE_IMG
- en: As you can see in the table, Token Blocking yields the highest number of matches,
    while Sorted Neighborhood has the highest matching rate. As Token Blocking likely
    has the fewest missed matches, we will proceed with the outcome from this approach.
    It is worth noting that our small dataset does not present scalability concerns.
    However, for larger datasets where Token Blocking may not be feasible, you may
    want to consider the other more scalable approaches.
  prefs: []
  type: TYPE_NORMAL
- en: Machine-learning matching
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/f6eb09c7745036d3fe672ac729d7f969.png)'
  prefs: []
  type: TYPE_IMG
- en: Machine-learning matching (Image by author)
  prefs: []
  type: TYPE_NORMAL
- en: If you have labeled data or have manually labeled sample pairs as matches or
    non-matches, you can train a machine-learning model to predict matched pairs.
    As our data has cluster labels `CID`, we will convert these into matching labels
    (match/unmatch) for pairs and train a machine-learning model, subsequently comparing
    its performance with the rule-based approach performed in the previous section.
  prefs: []
  type: TYPE_NORMAL
- en: The following code generates the model input `X` and the corresponding target
    variable `y`. Pairs within the same cluster `CID` are designated as matches (`y
    = 1`), while pairs outside the same cluster are labeled as non-matches (`y = 0`).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Next, we split them into training and testing sets, followed by training a logistic
    regression model.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: The code below compares its performance (`[f1_score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html)`)
    with the rule-based approach.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Below is the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/969acd2480dfd8b7aa2f7713fd3bbe0d.png)'
  prefs: []
  type: TYPE_IMG
- en: While the model’s performance is better, the performance of the rule-based approach
    may still be reasonably good.
  prefs: []
  type: TYPE_NORMAL
- en: For the following steps, we will use matches identified through the rule-based
    approach, considering that in many real-world cases, manual data labeling might
    not be practical due to resource constraints. The code below extracts matched
    pairs and their similarity scores from the candidate pairs and their scores on
    token blocking.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Clustering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this step, we are creating entity clusters based on the matched pairs from
    the previous step. Each cluster includes all records corresponding to a distinct
    real-world entity.
  prefs: []
  type: TYPE_NORMAL
- en: 'Clustering for entity resolution has several requirements:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Unconstrained algorithm**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The algorithms should not require any domain-specific parameters as input, such
    as the number of clusters or the diameter of the clusters.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Capability of handling an incomplete similarity matrix**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: As the entity resolution process does not compute similarity on every possible
    pair (which could be described as an N by N matrix), the algorithms must be able
    to handle an incomplete similarity matrix (or a list of matched pairs).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Scalability**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Entity resolution often handles sizable datasets, making it important that algorithms
    are capable of handling such data. In cases of large data, algorithms having high
    complexity like hierarchical clustering might not be practical.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'For the clustering, we will examine three major single-pass clustering algorithms:
    Partitioning (i.e. connected components), Center Clustering, and Merge-Center
    Clustering, all of which satisfy the requirements. These algorithms are highly
    efficient as they create clusters by a single scan (or O(n) time complexity) of
    the list of candidate pairs, although some of them require the list to be sorted
    by similarity score.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/07ea1680d7018fe8a75c87b34d9a83a4.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Single-pass clustering algorithms (source: [http://www.vldb.org/pvldb/vol2/vldb09-1025.pdf](http://www.vldb.org/pvldb/vol2/vldb09-1025.pdf))'
  prefs: []
  type: TYPE_NORMAL
- en: Partitioning/Connected Components
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This algorithm initiates clustering by initially assigning each node to its
    individual cluster. Then, it conducts a single scan of the list of matched pairs.
    If it identifies connected nodes that do not belong to the same cluster, it merges
    their clusters. In short, it forms a cluster by grouping all the connected nodes
    via edges (i.e. matched records via pairs). The algorithm may create clusters
    that connect dissimilar records via long paths.
  prefs: []
  type: TYPE_NORMAL
- en: Connected components clustering can be performed using the Scipy module as you
    can see in the code below. Before performing it, you want to convert the list
    of pairs into an adjacency matrix.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Center Clustering
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This algorithm [[5]](#299e) performs clustering where each cluster has a center
    and all records in each cluster are similar to the center of the cluster. It requires
    the list of similar pairs to be sorted by descending order of similarity scores.
    The algorithm then performs clustering by a single scan of the sorted list. When
    a node `u` is encountered for the first time in the scan, it’s designated as the
    cluster center. Any subsequent nodes `v` that are similar to `u` (i.e., appear
    in a pair `(u, v)` in the list) are assigned to the cluster of `u` and are not
    considered again during the process.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/30e7bb903d184ed17162526ea0dd62e7.png)'
  prefs: []
  type: TYPE_IMG
- en: Example of Center Clustering (Image by author)
  prefs: []
  type: TYPE_NORMAL
- en: Merge-Center Clustering
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This algorithm [[6]](#d172) performs similarly to Center clustering, but merges
    two clusters `cᵢ` and `cⱼ` whenever a record that is similar to the center of
    the cluster `cᵢ` is similar to the center of `cⱼ`. Note that when two clusters
    are merged, it does not choose a single center node, which means that merged clusters
    can have multiple center nodes. This algorithm can be performed similarly by a
    single scan of the list of similar pairs, but by keeping track of the records
    that are connected through a merged cluster.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d7ffac0d99cbdc94df6091902cde9669.png)'
  prefs: []
  type: TYPE_IMG
- en: Example of Merge-Center Clustering (Image by author)
  prefs: []
  type: TYPE_NORMAL
- en: To perform Center/Merge-Center clustering, we first need to sort the list of
    pairs by descending order of the similarity scores.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, the code below yields two sets of pairs: center-child pairs, denoted
    as `center_cluster_pairs`, and merged node pairs, referred to as `merge_cluster_pairs`.
    We can then generate Center clusters and Merge-Center clusters by applying connected
    components to these lists of pairs.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Cluster Evaluation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As we have the cluster labels, we can evaluate the quality of the clusters
    using [Rand Index](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.rand_score.html)
    or [adjusted Rand Index](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.adjusted_rand_score.html).
    Rand Index is a cluster evaluation metric that represents the proportion of pairs
    that are correctly clustered together or apart. It is defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '*TP = Number of pairs that are clustered* ***together*** *in both predicted
    and true clusters.*'
  prefs: []
  type: TYPE_NORMAL
- en: '*TN = Number of pairs that are clustered* ***apart*** *in both predicted and
    true clusters.*'
  prefs: []
  type: TYPE_NORMAL
- en: '***Rand Index*** *= (TP + TN) / Total number of possible pairs*'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/64c13a26cc139266e495ea3778ee4fb8.png)'
  prefs: []
  type: TYPE_IMG
- en: Example of Rand Index Calculation (Image by author)
  prefs: []
  type: TYPE_NORMAL
- en: Adjusted Rand Index is a modified version of Rand Index that is corrected for
    chance. The adjustment accounts for the possibility of random agreement from clustering
    results that were randomly assigned.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/495cfc03deac0d5898fdc893bd4a7a1c.png)'
  prefs: []
  type: TYPE_IMG
- en: Equation of adjusted Rand Index
  prefs: []
  type: TYPE_NORMAL
- en: We won’t delve into how each termin the above equation is calculated here, but
    anyone who is interested in this topic can refer to [the paper from KY Yeung](https://faculty.washington.edu/kayee/pca/supp.pdf)
    which explains the metric with some examples.
  prefs: []
  type: TYPE_NORMAL
- en: The code below gives us a comparison of the clusters using those metrics along
    with some additional basic statistics.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Below is the output.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ac1808afd199a87e6b00c6a31850c4e1.png)'
  prefs: []
  type: TYPE_IMG
- en: As you can see in the table, connected components produce the larger clusters
    with the fewest cluster count, while the gap between connected components and
    the Merge-Center clusters is minimal. Conversely, the Center clusters yield the
    smaller clusters with the highest count. Please note that all three clusters have
    a perfect Rand Index, as they have a large numbe of clusters making inter-cluster
    pairs dominant (i.e. even random clustering yields a respectable Rand Index).
    However, if you look at the Adjusted Rand Index, Merge-Center clustering is the
    best, while its difference from connected components is marginal.
  prefs: []
  type: TYPE_NORMAL
- en: That concludes our exploration of the entity resolution framework. How you proceed
    with the created clusters depends on your specific business requirements or use
    case. If you aim to establish a canonical representation for each cluster, you
    can achieve this by extracting the most representative value (such as the most
    frequent value) for each field within each cluster.
  prefs: []
  type: TYPE_NORMAL
- en: If you are interested, the whole code is available in the Google Colab and the
    GitHub repo below.
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://colab.research.google.com/drive/1Wkw-uaNX9Im9PNnw8CpUgWQHxxDxeiDr?usp=sharing&source=post_page-----3e8c59f4f41c--------------------------------)
    [## Google Colaboratory'
  prefs: []
  type: TYPE_NORMAL
- en: Entity Resolution
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: colab.research.google.com](https://colab.research.google.com/drive/1Wkw-uaNX9Im9PNnw8CpUgWQHxxDxeiDr?usp=sharing&source=post_page-----3e8c59f4f41c--------------------------------)
    [](https://github.com/tomonori-masui/entity-resolution/blob/main/entity_resolution_implementations.ipynb?source=post_page-----3e8c59f4f41c--------------------------------)
    [## entity-resolution/entity_resolution_implementations.ipynb
  prefs: []
  type: TYPE_NORMAL
- en: Entity Resolution
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: github.com](https://github.com/tomonori-masui/entity-resolution/blob/main/entity_resolution_implementations.ipynb?source=post_page-----3e8c59f4f41c--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[1] Christophides et al., [End-to-End Entity Resolution for Big Data: A Survey](https://arxiv.org/abs/1905.06397)
    (2019)'
  prefs: []
  type: TYPE_NORMAL
- en: '[2] Papadakis et al., [Comparative analysis of approximate blocking techniques
    for entity resolution](http://www.vldb.org/pvldb/vol9/p684-papadakis.pdf) (2016)'
  prefs: []
  type: TYPE_NORMAL
- en: '[3] Papadakis et al., [A survey of blocking and filtering techniques for entity
    resolution](https://arxiv.org/abs/1905.06167) (2020)'
  prefs: []
  type: TYPE_NORMAL
- en: '[4] Hassanzadeh et al., [Framework for evaluating clustering algorithms in
    duplicate detection](http://www.vldb.org/pvldb/vol2/vldb09-1025.pdf) (2009)'
  prefs: []
  type: TYPE_NORMAL
- en: '[5] Haveliwala et al., [Scalable techniques for clustering the web](https://www.researchgate.net/publication/221035516_Scalable_Techniques_for_Clustering_the_Web)
    (2000)'
  prefs: []
  type: TYPE_NORMAL
- en: '[6] Hassanzadeh & Miller, [Creating probabilistic databases from duplicated
    data](https://www.researchgate.net/publication/220473509_Creating_probabilistic_databases_from_duplicated_data)
    (2009)'
  prefs: []
  type: TYPE_NORMAL
