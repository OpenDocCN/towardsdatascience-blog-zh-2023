- en: 'Inside GPT — II : The core mechanics of prompt engineering'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: GPT内部 — II：提示工程的核心机制
- en: 原文：[https://towardsdatascience.com/inside-gpt-ii-why-exactly-does-your-prompt-matter-1aea1aef35da?source=collection_archive---------8-----------------------#2023-12-21](https://towardsdatascience.com/inside-gpt-ii-why-exactly-does-your-prompt-matter-1aea1aef35da?source=collection_archive---------8-----------------------#2023-12-21)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/inside-gpt-ii-why-exactly-does-your-prompt-matter-1aea1aef35da?source=collection_archive---------8-----------------------#2023-12-21](https://towardsdatascience.com/inside-gpt-ii-why-exactly-does-your-prompt-matter-1aea1aef35da?source=collection_archive---------8-----------------------#2023-12-21)
- en: The simple reasoning behind prompt engineering
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 提示工程背后的简单推理
- en: '[](https://medium.com/@fatih-demirci?source=post_page-----1aea1aef35da--------------------------------)[![Fatih
    Demirci](../Images/f60108429c4fac601a511f38954982bf.png)](https://medium.com/@fatih-demirci?source=post_page-----1aea1aef35da--------------------------------)[](https://towardsdatascience.com/?source=post_page-----1aea1aef35da--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----1aea1aef35da--------------------------------)
    [Fatih Demirci](https://medium.com/@fatih-demirci?source=post_page-----1aea1aef35da--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@fatih-demirci?source=post_page-----1aea1aef35da--------------------------------)[![Fatih
    Demirci](../Images/f60108429c4fac601a511f38954982bf.png)](https://medium.com/@fatih-demirci?source=post_page-----1aea1aef35da--------------------------------)[](https://towardsdatascience.com/?source=post_page-----1aea1aef35da--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----1aea1aef35da--------------------------------)
    [Fatih Demirci](https://medium.com/@fatih-demirci?source=post_page-----1aea1aef35da--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fe4aaee0b8cc3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Finside-gpt-ii-why-exactly-does-your-prompt-matter-1aea1aef35da&user=Fatih+Demirci&userId=e4aaee0b8cc3&source=post_page-e4aaee0b8cc3----1aea1aef35da---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----1aea1aef35da--------------------------------)
    ·8 min read·Dec 21, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F1aea1aef35da&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Finside-gpt-ii-why-exactly-does-your-prompt-matter-1aea1aef35da&user=Fatih+Demirci&userId=e4aaee0b8cc3&source=-----1aea1aef35da---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fe4aaee0b8cc3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Finside-gpt-ii-why-exactly-does-your-prompt-matter-1aea1aef35da&user=Fatih+Demirci&userId=e4aaee0b8cc3&source=post_page-e4aaee0b8cc3----1aea1aef35da---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----1aea1aef35da--------------------------------)
    ·8分钟阅读·2023年12月21日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F1aea1aef35da&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Finside-gpt-ii-why-exactly-does-your-prompt-matter-1aea1aef35da&user=Fatih+Demirci&userId=e4aaee0b8cc3&source=-----1aea1aef35da---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F1aea1aef35da&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Finside-gpt-ii-why-exactly-does-your-prompt-matter-1aea1aef35da&source=-----1aea1aef35da---------------------bookmark_footer-----------)![](../Images/c8268cc53ce4b66c6f6365fc2b262ae8.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F1aea1aef35da&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Finside-gpt-ii-why-exactly-does-your-prompt-matter-1aea1aef35da&source=-----1aea1aef35da---------------------bookmark_footer-----------)![](../Images/c8268cc53ce4b66c6f6365fc2b262ae8.png)'
- en: Decrypting & Depicting Enigma Machine (created through midjourney)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 解密与描绘恩igma机（通过midjourney创建）
- en: Large language models are the compressions of the world through the lens of
    human text*. Having de-fractured the model architecture of GPT in [Part I](/inside-gpt-i-1e8840ca8093),
    let’s explore the ways we can decode this compression and manipulate the output
    of an already trained model.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型是通过人类文本的视角对世界的压缩*。在[第一部分](/inside-gpt-i-1e8840ca8093)中，我们已经解构了GPT的模型架构，接下来，让我们探讨如何解码这种压缩并操控已经训练好的模型的输出。
- en: During model training, our model learns about the world through human text projection.
    After training, each time we prompt the model and generate text(inference), it
    forms a probability distribution involving every token in its vocabulary through
    the parameters it learned during the training.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在模型训练过程中，我们的模型通过人类文本的投射了解世界。在训练之后，每次我们提示模型并生成文本（推理）时，它通过训练过程中学到的参数形成一个涉及词汇表中每个词元的概率分布。
- en: Predicted token at a time becomes a last token of the input. Appended input
    with last predicted token is then used to predict the next token. So the text
    generation simply put, is a probability expression of the next token prediction
    given the tokens appear in the input(prompt).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 预测的标记一次成为输入的最后一个标记。将最后预测的标记附加到输入中，然后用来预测下一个标记。因此，文本生成简单来说，就是在给定输入（提示）中标记出现的情况下，对下一个标记预测的概率表达。
- en: What forms this probability? The model training. The text data that the model
    had seen during training process.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 这个概率是由什么形成的？模型训练。模型在训练过程中看到的文本数据。
- en: 'Let’s examine an example prompt sentence and make our explanation above less
    ambiguous:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们查看一个示例提示句，并使我们上面的解释更清晰：
- en: “Germany is known for its”
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '“德国以其” '
- en: How can you complete this sentence? You have learned what “Germany” is and you
    hold a conception based on what you have seen/heard/read (training data) throughout
    your life. Similar to a model when it is in its training phase. We collect large
    text data from internet/books. Then we filter and remove the harmful content.
    Through the model training eventually our model understands the world through
    the lens of human text.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 你如何完成这个句子？你已经了解了“德国”是什么，并且你基于你一生中所见/所闻/所读的内容（训练数据）形成了一个概念。这类似于模型在训练阶段的情况。我们从互联网/书籍中收集大量文本数据。然后我们筛选并删除有害内容。通过模型训练，最终我们的模型通过人类文本的视角理解世界。
- en: So what is the next token that has the highest probability to complete our sentence?
    Lets look at GPT-2 to get the real probabilities by the model.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，完成我们句子的下一个概率最高的标记是什么？让我们看一下GPT-2模型，以获取模型的实际概率。
- en: '[PRE0]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '*(Note: examples in this article are generated using GPT-2 model as it is a
    public model and small enough to illustrate the concepts through real examples.)*'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '*(注：本文中的示例使用了GPT-2模型生成，因为它是一个公开模型，且足够小，可以通过实际示例来说明这些概念。)*'
- en: Above lays the probability distribution of the model given the prompt. For every
    token in the corpus (the text dataset that the model is trained on) we have a
    corresponding calculated probability. The total number of vocabulary of the model(which
    is 50,257 in GPT-2), is also the size of the probability distribution. The techniques
    behind the calculation of these probabilities were explained in detail, in the
    first article of this blog post series. ([link](/inside-gpt-i-1e8840ca8093))
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 上面展示了模型在给定提示下的概率分布。对于语料库中的每个标记（模型训练时使用的文本数据集），我们都有一个相应的计算概率。模型的词汇总数（在GPT-2中为50,257）也是概率分布的大小。这些概率计算背后的技术在本博客文章系列的第一篇文章中进行了详细解释。
    ([link](/inside-gpt-i-1e8840ca8093))
- en: The output you can generate from the pre-trained language model can be controlled
    through several decoding strategies. So whether we are trying to extract factual
    information or generate creative stories, we can impact the output and tune its
    factuality and creativity.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 从预训练语言模型生成的输出可以通过几种解码策略进行控制。因此，无论我们是尝试提取事实信息还是生成创意故事，我们都可以影响输出，并调整其真实性和创造性。
- en: The simplest way to decode the predicted probabilities of the generated text
    is to simply get the token that has the highest probability at each prediction
    step. This strategy is also known as the “greedy” decoding.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 解码生成文本的预测概率最简单的方法是直接在每一步预测时选择具有最高概率的标记。这种策略也被称为“贪婪”解码。
- en: Let’s extract the first five tokens with the highest predicted probabilities
    given our initial input sentence(prompt). Each predicted token becomes the last
    token of the input sentence until we reach the maximum token limit which is a
    parameter we define. In this example, let’s generate the next 10 tokens.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们提取给定初始输入句子（提示）的预测概率最高的前五个标记。每个预测标记成为输入句子的最后一个标记，直到我们达到最大标记限制，这是我们定义的一个参数。在这个例子中，让我们生成接下来的10个标记。
- en: '![](../Images/086d2983380c2697a8aef591efe5fc28.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/086d2983380c2697a8aef591efe5fc28.png)'
- en: First 5 choice by probability (image by the author)
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 前5个概率选择（图片由作者提供）
- en: As you can see above with greedy strategy, we append the token with the highest
    probability to the input sequence and predict the next token.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你在上面所见，通过贪婪策略，我们将具有最高概率的标记附加到输入序列中，并预测下一个标记。
- en: '![](../Images/f4da8be65cd5f68aecbb911ccda86659.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f4da8be65cd5f68aecbb911ccda86659.png)'
- en: greedy search decision path (image by the author)
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 贪婪搜索决策路径（图片由作者提供）
- en: Using this strategy let’s generate a longer text with 128 next tokens using
    greedy-search decoding.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这种策略，让我们用贪婪搜索解码生成128个后续标记的较长文本。
- en: '[PRE1]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: As you we can see from the text above, although it is the simplest logic, the
    drawback of this approach is the generated repetitive sequences. As it fails to
    capture the probabilities of sequences, meaning, the overall probability of a
    several words coming one after another is overlooked. Greedy search predicts and
    considers only the probability one step at a time.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你从上面的文本中看到的，尽管这是最简单的逻辑，但这种方法的缺点是生成了重复的序列。由于它未能捕捉到序列的概率，也就是说，多个单词一个接一个的整体概率被忽视。贪婪搜索一次只预测并考虑一步的概率。
- en: Repetitive text is a problem. We would desire our generated output to be concise,
    how can we achieve it?
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 重复的文本是一个问题。我们希望生成的输出简洁，我们该如何实现呢？
- en: Instead of choosing the token that has highest probability at each step, we
    consider future x-steps and calculate the joint probability(simply multiplication
    of consecutive probabilities) and choose the *next token sequence* that is most
    probable. While x refers to number of beams, it is the depth of the future sequence
    we look into the future steps. This strategy is known as the *beam search.*
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不是在每一步选择概率最高的标记，而是考虑未来的 x 步，计算联合概率（简单地是连续概率的乘积），并选择*下一个标记序列*，即最可能的序列。这里的 x
    指的是 beams 的数量，也就是我们查看未来步骤的序列深度。这种策略被称为*beam 搜索*。
- en: Let’s go back to our example from GPT-2 and explore beam vs greedy search scenarios.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回到 GPT-2 的示例，探索 beam 搜索与贪婪搜索的情境。
- en: Given the prompt, looking at the two tokens with highest probability and their
    continuation(4 beams) in a tree diagram
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 根据提示，查看概率最高的两个标记及其在树状图中的延续（4 个 beams）。
- en: '![](../Images/f1935a0dbf35b7155cd4669c9b0511e3.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f1935a0dbf35b7155cd4669c9b0511e3.png)'
- en: greedy search vs beam search(image by the author)
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 贪婪搜索 vs beam 搜索（图像由作者提供）
- en: Lets calculate the join probabilities of the green sequences above.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们计算上述绿色序列的联合概率。
- en: Germany is known for its -> high-quality beer…
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 德国以其 -> 高质量的啤酒而闻名……
- en: with the joint probability 3.30%*24.24%*31.26%*6.54% = 0.0016353
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 具有联合概率 3.30%*24.24%*31.26%*6.54% = 0.0016353
- en: whereas the lower path with the sequence;
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 而下方路径的序列；
- en: Germany is known for its -> strong tradition of life…
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 德国以其 -> 强大的生活传统而闻名……
- en: 2.28%*2.54%*87.02%*38.26% = 0.0019281.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 2.28%*2.54%*87.02%*38.26% = 0.0019281。
- en: The bottom sequence overall resulted with the higher joint probability, although
    the first next token prediction step in the top sequence has higher probability.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管顶部序列中的第一个下一个标记预测步骤具有更高的概率，但底部序列总体上具有更高的联合概率。
- en: While greedy search priorities the absolute maximum probability at each prediction
    step, it neglects the token probabilities in sequences. Beam search decoding enables
    us to go in depth of sequences and help us decode text in more extensive fashion.
    So is beam-search the ultimate solution?
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管贪婪搜索在每个预测步骤中优先考虑绝对最大概率，但它忽略了序列中标记的概率。Beam 搜索解码使我们能够深入序列，并帮助我们以更广泛的方式解码文本。那么，beam
    搜索是最终解决方案吗？
- en: Lets explore further and decode the next 128 tokens with the depth of 5 beams.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们进一步探索，并以 5 个 beam 的深度解码接下来的 128 个标记。
- en: '[PRE2]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Although comparatively lesser than the greedy-search, beam-search suffers from
    repetitive output too. However, with beam search decoding, we can solve this problem
    by penalising the repeated pairs of word sequences. In other words, the probability
    of token sequences is assigned zero, if the sequence has already been decoded
    before. This penalisation of a repeated tokens sequence is also know as n-gram
    penalty.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管与贪婪搜索相比较少，beam 搜索也会遭遇重复输出的问题。然而，通过 beam 搜索解码，我们可以通过惩罚重复的词序列对来解决这个问题。换句话说，如果序列之前已经被解码过，那么标记序列的概率将被赋值为零。重复标记序列的这种惩罚也被称为
    n-gram 惩罚。
- en: While “n” signifies the length of the sequence, “gram” is a term that refers
    to “unit” in computational linguistic often corresponds to the term token in our
    case.
  id: totrans-48
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 当“n”表示序列的长度时，“gram”是一个在计算语言学中指“单位”的术语，通常对应于我们案例中的“标记”。
- en: The reasoning behind is to discourage the generation of sequences that contain
    consecutive repeating n-grams. The decoding algorithm will penalise generated
    sequences that contain repeating pairs of words in the output.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 其背后的原因是为了避免生成包含连续重复 n-gram 的序列。解码算法将惩罚包含重复词对的生成序列。
- en: Knowing this, let’s apply n-gram penalty of n = 2.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 知道这一点后，让我们应用 n-gram 惩罚，其中 n = 2。
- en: '[PRE3]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: This is the best completion of the input prompt we extracted from the model
    so far in terms of coherence and compactness. Through n-gram penalisation the
    output decoded with beam-search became more human-like.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们从模型中提取的关于输入提示的最佳完成文本，体现了连贯性和紧凑性。通过 n-gram 惩罚，使用 beam-search 解码的输出变得更加类似人类。
- en: When should we use beam-search and when greedy-search? Where the factualness
    is paramount, like solving a math problem, key information extraction, summarisation
    or translation, greedy-search should be preferred. However, when we want to achieve
    creative output and factuality is not our priority (like it can be in the case
    of story generation) beam-search is often the better suited approach.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 什么时候应该使用 beam-search，什么时候使用 greedy-search？当事实准确性至关重要时，比如解决数学问题、关键信息提取、总结或翻译时，应该优先选择
    greedy-search。然而，当我们希望实现创意输出而事实准确性不是优先考虑的问题时（比如故事生成），beam-search 通常是更合适的方法。
- en: Why exactly does your prompt matter? Because every word you choose to use, the
    sentence structure, the layout of your instructions will activate different series
    of parameters in the deep layers of large language model and the probabilities
    will be formed differently for each different prompt. In the essence of the matter,
    the text generation is a probability expression conditional on your prompt.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么你的提示内容如此重要？因为你选择的每个词、句子结构、指令布局都会在大型语言模型的深层激活不同的参数序列，每个不同的提示都会形成不同的概率。问题的本质在于，文本生成是对你的提示的概率表达。
- en: There are also alternative methods to prevent repetitions and influence the
    factuality/creativity of the generated text, such as truncating the distribution
    of vocabulary or sampling methods. If you are interested in a higher-level in-depth
    exploration of the subject, I’d highly recommend the [article](https://huggingface.co/blog/how-to-generate)
    from Patrick von Platen in HuggingFace blog.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 还有其他替代方法可以防止重复并影响生成文本的事实准确性/创造性，例如截断词汇分布或采样方法。如果你对深入探讨这一主题感兴趣，我强烈推荐 Patrick
    von Platen 在 HuggingFace 博客中的 [文章](https://huggingface.co/blog/how-to-generate)。
- en: Next and the last article of this series will explore fine-tuning and reinforcement
    learning through human feedback which played an important role on why pre-trained
    models succeeded to surpass SOTA models in several benchmarks. I hope in this
    blog post, I was able help you understand the reasoning of prompt engineering
    better. Many thanks for the read. Until next time.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 本系列的下一篇也是最后一篇文章将探讨通过人类反馈的微调和强化学习，这在为什么预训练模型能够在多个基准测试中超越 SOTA 模型中起到了重要作用。希望在这篇博客文章中，我能够帮助你更好地理解提示工程的推理。感谢阅读。下次再见。
- en: '**References:**'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '**参考文献：**'
- en: '* — Ilya Sutskever, at No Priors Ep. 39 | With OpenAI Co-Founder & Chief Scientist
    Ilya Sutskever link: [https://www.youtube.com/watch?v=Ft0gTO2K85A](https://www.youtube.com/watch?v=Ft0gTO2K85A)'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*— 伊利亚·苏茨克维，No Priors 第39期 | 与 OpenAI 联合创始人兼首席科学家伊利亚·苏茨克维对话 [https://www.youtube.com/watch?v=Ft0gTO2K85A](https://www.youtube.com/watch?v=Ft0gTO2K85A)*'
- en: 'L. Tunstall, L. von Werra, and T. Wolf, “Natural Language Processing with Transformers,
    Revised Edition,” O’Reilly Media, Inc., Released May 2022, ISBN: 9781098136796.'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'L. Tunstall, L. von Werra, 和 T. Wolf，《使用变换器的自然语言处理（修订版）》，O’Reilly Media, Inc.，2022年5月发布，ISBN:
    9781098136796。'
- en: '**Relevant links:**'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**相关链接：**'
- en: 'Inside GPT — I : Understanding the text generation [https://towardsdatascience.com/inside-gpt-i-1e8840ca8093](/inside-gpt-i-1e8840ca8093)'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Inside GPT — I : 理解文本生成 [https://towardsdatascience.com/inside-gpt-i-1e8840ca8093](/inside-gpt-i-1e8840ca8093)'
- en: 'How to generate text: using different decoding methods for language generation
    with Transformers'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何生成文本：使用不同的解码方法生成变换器语言文本
- en: '[https://huggingface.co/blog/how-to-generate](https://huggingface.co/blog/how-to-generate)'
  id: totrans-63
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[https://huggingface.co/blog/how-to-generate](https://huggingface.co/blog/how-to-generate)'
