- en: 'Inside GPT — II : The core mechanics of prompt engineering'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/inside-gpt-ii-why-exactly-does-your-prompt-matter-1aea1aef35da?source=collection_archive---------8-----------------------#2023-12-21](https://towardsdatascience.com/inside-gpt-ii-why-exactly-does-your-prompt-matter-1aea1aef35da?source=collection_archive---------8-----------------------#2023-12-21)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The simple reasoning behind prompt engineering
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@fatih-demirci?source=post_page-----1aea1aef35da--------------------------------)[![Fatih
    Demirci](../Images/f60108429c4fac601a511f38954982bf.png)](https://medium.com/@fatih-demirci?source=post_page-----1aea1aef35da--------------------------------)[](https://towardsdatascience.com/?source=post_page-----1aea1aef35da--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----1aea1aef35da--------------------------------)
    [Fatih Demirci](https://medium.com/@fatih-demirci?source=post_page-----1aea1aef35da--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fe4aaee0b8cc3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Finside-gpt-ii-why-exactly-does-your-prompt-matter-1aea1aef35da&user=Fatih+Demirci&userId=e4aaee0b8cc3&source=post_page-e4aaee0b8cc3----1aea1aef35da---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----1aea1aef35da--------------------------------)
    ·8 min read·Dec 21, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F1aea1aef35da&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Finside-gpt-ii-why-exactly-does-your-prompt-matter-1aea1aef35da&user=Fatih+Demirci&userId=e4aaee0b8cc3&source=-----1aea1aef35da---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F1aea1aef35da&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Finside-gpt-ii-why-exactly-does-your-prompt-matter-1aea1aef35da&source=-----1aea1aef35da---------------------bookmark_footer-----------)![](../Images/c8268cc53ce4b66c6f6365fc2b262ae8.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Decrypting & Depicting Enigma Machine (created through midjourney)
  prefs: []
  type: TYPE_NORMAL
- en: Large language models are the compressions of the world through the lens of
    human text*. Having de-fractured the model architecture of GPT in [Part I](/inside-gpt-i-1e8840ca8093),
    let’s explore the ways we can decode this compression and manipulate the output
    of an already trained model.
  prefs: []
  type: TYPE_NORMAL
- en: During model training, our model learns about the world through human text projection.
    After training, each time we prompt the model and generate text(inference), it
    forms a probability distribution involving every token in its vocabulary through
    the parameters it learned during the training.
  prefs: []
  type: TYPE_NORMAL
- en: Predicted token at a time becomes a last token of the input. Appended input
    with last predicted token is then used to predict the next token. So the text
    generation simply put, is a probability expression of the next token prediction
    given the tokens appear in the input(prompt).
  prefs: []
  type: TYPE_NORMAL
- en: What forms this probability? The model training. The text data that the model
    had seen during training process.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s examine an example prompt sentence and make our explanation above less
    ambiguous:'
  prefs: []
  type: TYPE_NORMAL
- en: “Germany is known for its”
  prefs: []
  type: TYPE_NORMAL
- en: How can you complete this sentence? You have learned what “Germany” is and you
    hold a conception based on what you have seen/heard/read (training data) throughout
    your life. Similar to a model when it is in its training phase. We collect large
    text data from internet/books. Then we filter and remove the harmful content.
    Through the model training eventually our model understands the world through
    the lens of human text.
  prefs: []
  type: TYPE_NORMAL
- en: So what is the next token that has the highest probability to complete our sentence?
    Lets look at GPT-2 to get the real probabilities by the model.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '*(Note: examples in this article are generated using GPT-2 model as it is a
    public model and small enough to illustrate the concepts through real examples.)*'
  prefs: []
  type: TYPE_NORMAL
- en: Above lays the probability distribution of the model given the prompt. For every
    token in the corpus (the text dataset that the model is trained on) we have a
    corresponding calculated probability. The total number of vocabulary of the model(which
    is 50,257 in GPT-2), is also the size of the probability distribution. The techniques
    behind the calculation of these probabilities were explained in detail, in the
    first article of this blog post series. ([link](/inside-gpt-i-1e8840ca8093))
  prefs: []
  type: TYPE_NORMAL
- en: The output you can generate from the pre-trained language model can be controlled
    through several decoding strategies. So whether we are trying to extract factual
    information or generate creative stories, we can impact the output and tune its
    factuality and creativity.
  prefs: []
  type: TYPE_NORMAL
- en: The simplest way to decode the predicted probabilities of the generated text
    is to simply get the token that has the highest probability at each prediction
    step. This strategy is also known as the “greedy” decoding.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s extract the first five tokens with the highest predicted probabilities
    given our initial input sentence(prompt). Each predicted token becomes the last
    token of the input sentence until we reach the maximum token limit which is a
    parameter we define. In this example, let’s generate the next 10 tokens.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/086d2983380c2697a8aef591efe5fc28.png)'
  prefs: []
  type: TYPE_IMG
- en: First 5 choice by probability (image by the author)
  prefs: []
  type: TYPE_NORMAL
- en: As you can see above with greedy strategy, we append the token with the highest
    probability to the input sequence and predict the next token.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f4da8be65cd5f68aecbb911ccda86659.png)'
  prefs: []
  type: TYPE_IMG
- en: greedy search decision path (image by the author)
  prefs: []
  type: TYPE_NORMAL
- en: Using this strategy let’s generate a longer text with 128 next tokens using
    greedy-search decoding.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: As you we can see from the text above, although it is the simplest logic, the
    drawback of this approach is the generated repetitive sequences. As it fails to
    capture the probabilities of sequences, meaning, the overall probability of a
    several words coming one after another is overlooked. Greedy search predicts and
    considers only the probability one step at a time.
  prefs: []
  type: TYPE_NORMAL
- en: Repetitive text is a problem. We would desire our generated output to be concise,
    how can we achieve it?
  prefs: []
  type: TYPE_NORMAL
- en: Instead of choosing the token that has highest probability at each step, we
    consider future x-steps and calculate the joint probability(simply multiplication
    of consecutive probabilities) and choose the *next token sequence* that is most
    probable. While x refers to number of beams, it is the depth of the future sequence
    we look into the future steps. This strategy is known as the *beam search.*
  prefs: []
  type: TYPE_NORMAL
- en: Let’s go back to our example from GPT-2 and explore beam vs greedy search scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: Given the prompt, looking at the two tokens with highest probability and their
    continuation(4 beams) in a tree diagram
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f1935a0dbf35b7155cd4669c9b0511e3.png)'
  prefs: []
  type: TYPE_IMG
- en: greedy search vs beam search(image by the author)
  prefs: []
  type: TYPE_NORMAL
- en: Lets calculate the join probabilities of the green sequences above.
  prefs: []
  type: TYPE_NORMAL
- en: Germany is known for its -> high-quality beer…
  prefs: []
  type: TYPE_NORMAL
- en: with the joint probability 3.30%*24.24%*31.26%*6.54% = 0.0016353
  prefs: []
  type: TYPE_NORMAL
- en: whereas the lower path with the sequence;
  prefs: []
  type: TYPE_NORMAL
- en: Germany is known for its -> strong tradition of life…
  prefs: []
  type: TYPE_NORMAL
- en: 2.28%*2.54%*87.02%*38.26% = 0.0019281.
  prefs: []
  type: TYPE_NORMAL
- en: The bottom sequence overall resulted with the higher joint probability, although
    the first next token prediction step in the top sequence has higher probability.
  prefs: []
  type: TYPE_NORMAL
- en: While greedy search priorities the absolute maximum probability at each prediction
    step, it neglects the token probabilities in sequences. Beam search decoding enables
    us to go in depth of sequences and help us decode text in more extensive fashion.
    So is beam-search the ultimate solution?
  prefs: []
  type: TYPE_NORMAL
- en: Lets explore further and decode the next 128 tokens with the depth of 5 beams.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Although comparatively lesser than the greedy-search, beam-search suffers from
    repetitive output too. However, with beam search decoding, we can solve this problem
    by penalising the repeated pairs of word sequences. In other words, the probability
    of token sequences is assigned zero, if the sequence has already been decoded
    before. This penalisation of a repeated tokens sequence is also know as n-gram
    penalty.
  prefs: []
  type: TYPE_NORMAL
- en: While “n” signifies the length of the sequence, “gram” is a term that refers
    to “unit” in computational linguistic often corresponds to the term token in our
    case.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The reasoning behind is to discourage the generation of sequences that contain
    consecutive repeating n-grams. The decoding algorithm will penalise generated
    sequences that contain repeating pairs of words in the output.
  prefs: []
  type: TYPE_NORMAL
- en: Knowing this, let’s apply n-gram penalty of n = 2.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: This is the best completion of the input prompt we extracted from the model
    so far in terms of coherence and compactness. Through n-gram penalisation the
    output decoded with beam-search became more human-like.
  prefs: []
  type: TYPE_NORMAL
- en: When should we use beam-search and when greedy-search? Where the factualness
    is paramount, like solving a math problem, key information extraction, summarisation
    or translation, greedy-search should be preferred. However, when we want to achieve
    creative output and factuality is not our priority (like it can be in the case
    of story generation) beam-search is often the better suited approach.
  prefs: []
  type: TYPE_NORMAL
- en: Why exactly does your prompt matter? Because every word you choose to use, the
    sentence structure, the layout of your instructions will activate different series
    of parameters in the deep layers of large language model and the probabilities
    will be formed differently for each different prompt. In the essence of the matter,
    the text generation is a probability expression conditional on your prompt.
  prefs: []
  type: TYPE_NORMAL
- en: There are also alternative methods to prevent repetitions and influence the
    factuality/creativity of the generated text, such as truncating the distribution
    of vocabulary or sampling methods. If you are interested in a higher-level in-depth
    exploration of the subject, I’d highly recommend the [article](https://huggingface.co/blog/how-to-generate)
    from Patrick von Platen in HuggingFace blog.
  prefs: []
  type: TYPE_NORMAL
- en: Next and the last article of this series will explore fine-tuning and reinforcement
    learning through human feedback which played an important role on why pre-trained
    models succeeded to surpass SOTA models in several benchmarks. I hope in this
    blog post, I was able help you understand the reasoning of prompt engineering
    better. Many thanks for the read. Until next time.
  prefs: []
  type: TYPE_NORMAL
- en: '**References:**'
  prefs: []
  type: TYPE_NORMAL
- en: '* — Ilya Sutskever, at No Priors Ep. 39 | With OpenAI Co-Founder & Chief Scientist
    Ilya Sutskever link: [https://www.youtube.com/watch?v=Ft0gTO2K85A](https://www.youtube.com/watch?v=Ft0gTO2K85A)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'L. Tunstall, L. von Werra, and T. Wolf, “Natural Language Processing with Transformers,
    Revised Edition,” O’Reilly Media, Inc., Released May 2022, ISBN: 9781098136796.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Relevant links:**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Inside GPT — I : Understanding the text generation [https://towardsdatascience.com/inside-gpt-i-1e8840ca8093](/inside-gpt-i-1e8840ca8093)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'How to generate text: using different decoding methods for language generation
    with Transformers'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://huggingface.co/blog/how-to-generate](https://huggingface.co/blog/how-to-generate)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
