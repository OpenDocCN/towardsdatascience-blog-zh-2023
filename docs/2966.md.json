["```py\ntoken_perplexity = (probs.log() * -1.0).exp()\nprint(f\"Token Perplexity: {token_perplexity}\")\n```", "```py\n# The perplexity is e^(average NLL).\nsentence_perplexity = (probs.log() * -1.0).mean().exp().item()\nprint(f\"Sentence Perplexity: {sentence_perplexity:.2f}\")\n```", "```py\ndef get_pointwise_loss(self, inputs: List[str], tok):\n    self.model.eval()\n    all_probs = []\n    with torch.inference_mode():\n        for input in inputs:\n            ids_list: List[int] = tok.encode(input).ids\n            # ids has shape (1, len(ids_list))\n            ids: Torch.Tensor = torch.tensor(ids_list, device=self.device).unsqueeze(0)\n\n            # probs below is the probability that the token at that location\n            # completes the sentence (in ids) so far.\n            y = self.model(ids)\n            criterion = nn.CrossEntropyLoss(reduction='none', ignore_index=0)\n\n            # Compute the loss starting from the 2nd token in the model's output.\n            loss = criterion(y[:,:,:-1], ids[:,1:])\n\n            # To compute the probability of each token, we need to compute the\n            # negative of the log loss and exponentiate it.\n            loss = loss * -1.0\n\n            # Set the probabilities that we are not interested in to -inf.\n            # This is done to make the softmax set these values to 0.0\n            loss[loss == 0.0] = float(\"-inf\")\n\n            # probs holds the probability of each token's prediction\n            # starting from the 2nd token since we don't want to include\n            # the probability of the model predicting the beginning of\n            # a sentence given no existing sentence context.\n            #\n            # To compute perplexity, we should probably ignore the first\n            # handful of predictions of the model since there's insufficient\n            # context. We donâ€™t do that here, though.\n            probs = loss.exp()\n            all_probs.append(probs)\n        #\n    #\n    return all_probs\n#\n```", "```py\ndef get_html_for_token_perplexity(tok, sentence, tok_ppx, model_ppx):\n    tokens = tok.encode(sentence).tokens\n    ids = tok.encode(sentence).ids\n    cleaned_tokens = []\n    for word in tokens:\n        m = list(map(ord, word))\n        m = list(map(lambda x: x if x != 288 else ord(' '), m))\n        m = list(map(chr, m))\n        m = ''.join(m)\n        cleaned_tokens.append(m)\n    #\n    html = [\n        f\"<span>{cleaned_tokens[0]}</span>\",\n    ]\n    for ct, ppx in zip(cleaned_tokens[1:], tok_ppx):\n        color = \"black\"\n        if ppx.item() >= 0:\n            if ppx.item() <= model_ppx * 1.1:\n                color = \"red\"\n            else:\n                color = \"green\"\n            #\n        #\n        html.append(f\"<span style='color:{color};'>{ct}</span>\")\n    #\n    return \"\".join(html)\n#\n```"]