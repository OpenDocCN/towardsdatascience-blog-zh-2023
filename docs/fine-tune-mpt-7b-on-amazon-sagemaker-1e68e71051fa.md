# åœ¨ Amazon SageMaker ä¸Šå¾®è°ƒ MPT-7B

> åŸæ–‡ï¼š[`towardsdatascience.com/fine-tune-mpt-7b-on-amazon-sagemaker-1e68e71051fa`](https://towardsdatascience.com/fine-tune-mpt-7b-on-amazon-sagemaker-1e68e71051fa)

![](img/2f5691f2830e17d1af6c907a144f4567.png)

ç…§ç‰‡ç”± [Jeffery Ho](https://unsplash.com/@jefferyho?utm_source=medium&utm_medium=referral) æä¾›ï¼Œæ¥æºäº [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)ã€‚

## å­¦ä¹ å¦‚ä½•å‡†å¤‡æ•°æ®é›†å¹¶åˆ›å»ºä¸€ä¸ªè®­ç»ƒä»»åŠ¡ï¼Œä»¥ä¾¿åœ¨ Amazon SageMaker ä¸Šå¾®è°ƒ MPT-7Bã€‚

[](https://medium.com/@joao.pereira.abt?source=post_page-----1e68e71051fa--------------------------------)![JoÃ£o Pereira](https://medium.com/@joao.pereira.abt?source=post_page-----1e68e71051fa--------------------------------)[](https://towardsdatascience.com/?source=post_page-----1e68e71051fa--------------------------------)![Towards Data Science](https://towardsdatascience.com/?source=post_page-----1e68e71051fa--------------------------------) [JoÃ£o Pereira](https://medium.com/@joao.pereira.abt?source=post_page-----1e68e71051fa--------------------------------)

Â·å‘è¡¨äº [Towards Data Science](https://towardsdatascience.com/?source=post_page-----1e68e71051fa--------------------------------) Â·é˜…è¯»æ—¶é—´ 9 åˆ†é’ŸÂ·2023 å¹´ 6 æœˆ 20 æ—¥

--

æ¯å‘¨éƒ½ä¼šæœ‰æ–°çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è¢«å®£å¸ƒï¼Œæ¯ä¸ªæ¨¡å‹éƒ½è¯•å›¾è¶…è¶Šå…¶å‰ä»»å¹¶å æ®è¯„ä¼°æ’è¡Œæ¦œçš„é¦–ä½ã€‚å…¶ä¸­æœ€æ–°çš„æ¨¡å‹ä¹‹ä¸€æ˜¯ [MPT-7B](https://www.mosaicml.com/blog/mpt-7b)ï¼Œç”± MosaicML å‘å¸ƒã€‚ä¸åŒç±»å…¶ä»–æ¨¡å‹ä¸åŒï¼Œè¿™æ¬¾ 70 äº¿å‚æ•°çš„æ¨¡å‹æ˜¯å¼€æºçš„ï¼Œå¹¶ä¸”è·å¾—äº†å•†ä¸šä½¿ç”¨çš„è®¸å¯è¯ ([Apache 2.0 è®¸å¯è¯](https://github.com/mosaicml/llm-foundry/blob/main/LICENSE)) ğŸš€ã€‚

åƒ MPT-7B è¿™æ ·çš„åŸºç¡€æ¨¡å‹æ˜¯åœ¨å…·æœ‰ä¸‡äº¿ä¸ªæ ‡è®°ï¼ˆ100 ä¸ªæ ‡è®°çº¦ç­‰äº 75 ä¸ªå•è¯ï¼‰çš„æ•°æ®é›†ä¸Šè¿›è¡Œé¢„è®­ç»ƒçš„ï¼Œå¹¶ä¸”å½“æç¤ºå¾—å½“æ—¶ï¼Œå®ƒä»¬èƒ½å¤Ÿç”Ÿæˆä»¤äººå°è±¡æ·±åˆ»çš„è¾“å‡ºã€‚ç„¶è€Œï¼Œè¦çœŸæ­£é‡Šæ”¾å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å®é™…åº”ç”¨ä¸­çš„ä»·å€¼ï¼Œä»…ä»…æ™ºèƒ½çš„æç¤ºå·¥ç¨‹å¯èƒ½ä¸è¶³ä»¥ä½¿å…¶é€‚ç”¨äºä½ çš„ç”¨ä¾‹ï¼Œå› æ­¤ï¼Œéœ€è¦åœ¨é¢†åŸŸç‰¹å®šçš„æ•°æ®é›†ä¸Šå¯¹åŸºç¡€æ¨¡å‹è¿›è¡Œå¾®è°ƒã€‚

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å…·æœ‰æ•°åäº¿ä¸ªå‚æ•°ï¼Œå› æ­¤å¾®è°ƒå¦‚æ­¤åºå¤§çš„æ¨¡å‹æ˜¯å…·æœ‰æŒ‘æˆ˜æ€§çš„ã€‚å¥½æ¶ˆæ¯æ˜¯ï¼Œç›¸æ¯”äºé¢„è®­ç»ƒåŸºç¡€æ¨¡å‹ï¼Œå¾®è°ƒçš„æˆæœ¬æ›´ä½ã€é€Ÿåº¦æ›´å¿«ï¼Œå› ä¸º 1) é¢†åŸŸç‰¹å®šçš„æ•°æ®é›†æ˜¯â€œè¾ƒå°â€çš„ï¼Œ2) å¾®è°ƒåªéœ€å¯¹è®­ç»ƒæ•°æ®è¿›è¡Œå°‘é‡éå†ã€‚

***åœ¨æœ¬æ–‡ä¸­æˆ‘ä»¬å°†å­¦ä¹ ï¼š***

+   å¦‚ä½•åˆ›å»ºå’Œæ„å»ºç”¨äºå¾®è°ƒå¤§å‹è¯­è¨€æ¨¡å‹çš„æ•°æ®é›†ã€‚

+   ä»€ä¹ˆæ˜¯å®Œå…¨åˆ†ç‰‡æ•°æ®å¹¶è¡Œçš„åˆ†å¸ƒå¼è®­ç»ƒä½œä¸šä»¥åŠå¦‚ä½•é…ç½®å®ƒã€‚

+   å¦‚ä½•å®šä¹‰ä¸€ä¸ª ğŸ˜Š HuggingFace ä¼°ç®—å™¨ã€‚

+   å¦‚ä½•åœ¨ Amazon SageMaker ä¸­å¯åŠ¨ä¸€ä¸ªå¾®è°ƒ MPT-7B çš„è®­ç»ƒä½œä¸šã€‚

# 1\. å®‰è£…ä¾èµ–é¡¹å¹¶è®¾ç½® S3 è·¯å¾„

è®©æˆ‘ä»¬é¦–å…ˆå®‰è£… [SageMaker Python SDK](https://github.com/aws/sagemaker-python-sdk) å’Œå…¶ä»–ä¸€äº›åŒ…ã€‚è¿™ä¸ª SDK ä½¿å¾—åœ¨ AWS ä¸Šè®­ç»ƒå’Œéƒ¨ç½²æœºå™¨å­¦ä¹ æ¨¡å‹å˜å¾—å¯èƒ½ï¼Œåªéœ€å‡ è¡Œ Python ä»£ç ã€‚ä¸‹é¢çš„ä»£ç å¯ä»¥åœ¨ Github çš„ `[sagemaker_finetuning.ipynb](https://github.com/jpcpereira/sagemaker-fine-tune-mpt-7b/blob/main/sagemaker_finetuning.ipynb)` notebook ä¸­æ‰¾åˆ°ã€‚åœ¨ SageMaker Studioã€SageMaker notebook å®ä¾‹æˆ–åœ¨ç¬”è®°æœ¬ç”µè„‘ä¸Šè¿è¡Œ notebookï¼Œå‰ææ˜¯ [è®¤è¯åˆ° AWS è´¦æˆ·](https://docs.aws.amazon.com/signin/latest/userguide/command-line-sign-in.html)ã€‚

```py
!pip install "sagemaker==2.162.0" s3path boto3 --quiet

from sagemaker.huggingface import HuggingFace
from sagemaker.inputs import TrainingInput
from sagemaker import s3_utils
import sagemaker
import boto3
import json
```

ä¸‹ä¸€æ­¥æ˜¯å®šä¹‰æ•°æ®å°†ä¿å­˜åˆ° S3 çš„è·¯å¾„ï¼Œå¹¶åˆ›å»ºä¸€ä¸ª SageMaker ä¼šè¯ã€‚

```py
# Define S3 paths
bucket             = "<YOUR-S3-BUCKET>"
training_data_path = f"s3://{bucket}/toy_data/train/data.jsonl"
test_data_path     = f"s3://{bucket}/toy_data/test/data.jsonl"
output_path        = f"s3://{bucket}/outputs"
code_location      = f"s3://{bucket}/code"

# Create SageMaker session
sagemaker_session  = sagemaker.Session()
region             = sagemaker_session.boto_region_name
role               = sagemaker.get_execution_role()
```

# 2\. æ„å»ºå¾®è°ƒæ•°æ®é›†

æˆ‘ä»¬å°†åˆ›å»ºä¸€ä¸ªè™šæ‹Ÿæ•°æ®é›†æ¥æ¼”ç¤ºå¦‚ä½•å¾®è°ƒ MPT-7Bã€‚ç”±äºåœ¨å®Œæ•´æ•°æ®é›†ä¸Šè®­ç»ƒæ­¤å¤§å°çš„æ¨¡å‹éœ€è¦è¾ƒé•¿æ—¶é—´ä¸”æˆæœ¬è¾ƒé«˜ï¼Œå› æ­¤é¦–å…ˆåœ¨å°æ•°æ®é›†ä¸Šæµ‹è¯•å’Œè°ƒè¯•è®­ç»ƒä»»åŠ¡æ˜¯ä¸€ä¸ªå¥½ä¸»æ„ï¼Œå…¶æ¬¡å°†è®­ç»ƒè§„æ¨¡æ‰©å¤§åˆ°å®Œæ•´æ•°æ®é›†ã€‚

+   **å°†æ•°æ®é›†æ ¼å¼åŒ–ä¸ºå­—å…¸åˆ—è¡¨** â€” æ•°æ®é›†åº”æ ¼å¼åŒ–ä¸ºå­—å…¸åˆ—è¡¨ï¼Œæ¯ä¸ªç¤ºä¾‹å…·æœ‰é”®å€¼ç»“æ„ï¼Œ*ä¾‹å¦‚*ï¼Œ

```py
{
    "prompt": "What is a Pastel de Nata?",
    "response": "A Pastel de Nata is a Portuguese egg custard tart pastry, optionally dusted with cinnamon."
}
```

`prompt` æ˜¯ç»™æ¨¡å‹çš„è¾“å…¥ï¼ˆ*ä¾‹å¦‚*ï¼Œä¸€ä¸ªé—®é¢˜ï¼‰ã€‚`response` æ˜¯æ¨¡å‹è®­ç»ƒé¢„æµ‹çš„è¾“å‡ºï¼ˆ*ä¾‹å¦‚*ï¼Œ`prompt` ä¸­é—®é¢˜çš„ç­”æ¡ˆï¼‰ã€‚åŸå§‹æç¤ºé€šå¸¸ä¼šç»è¿‡é¢„å¤„ç†ï¼Œä»¥é€‚åº”æç¤ºæ¨¡æ¿ï¼Œè¿™æœ‰åŠ©äºæ¨¡å‹ç”Ÿæˆæ›´å¥½çš„è¾“å‡ºã€‚è¯·æ³¨æ„ï¼Œæ¨¡å‹æ˜¯ä¸ºå› æœè¯­è¨€å»ºæ¨¡è®­ç»ƒçš„ï¼Œå› æ­¤å¯ä»¥æŠŠå®ƒçœ‹ä½œæ˜¯ä¸€ä¸ªâ€œæ–‡æ¡£å®Œæˆå™¨â€ã€‚è®¾è®¡æç¤ºæ¨¡æ¿æ—¶ï¼Œæœ€å¥½è®©æ¨¡å‹è§‰å¾—å®ƒæ­£åœ¨å®Œæˆä¸€ä¸ªæ–‡æ¡£ã€‚Andrej Karpathy åœ¨ä»–çš„æ¼”è®² [*State of GPT*](https://youtu.be/bZQun8Y4L2A) ä¸­å¾ˆå¥½åœ°è§£é‡Šäº†è¿™ä¸€æœºåˆ¶ã€‚

```py
prompt_template = """Write a response that appropriately answers the question below.
### Question:
{question}

### Response:
"""

dataset = [
    {"prompt": "What is a Pastel de Nata?",
     "response": "A Pastel de Nata is a Portuguese egg custard tart pastry, optionally dusted with cinnamon."},
    {"prompt": "Which museums are famous in Amsterdam?",
     "response": "Amsterdam is home to various world-famous museums, and no trip to the city is complete without stopping by the Rijksmuseum, Van Gogh Museum, or Stedelijk Museum."},
    {"prompt": "Where is the European Parliament?",
     "response": "Strasbourg is the official seat of the European Parliament."},
    {"prompt": "How is the weather in The Netherlands?",
     "response": "The Netherlands is a country that boasts a typical maritime climate with mild summers and cold winters."},
    {"prompt": "What are Poffertjes?",
     "response": "Poffertjes are a traditional Dutch batter treat. Resembling small, fluffy pancakes, they are made with yeast and buckwheat flour."},
]

# Format prompt based on template
for example in dataset:
    example["prompt"] = prompt_template.format(question=example["prompt"])

training_data, test_data = dataset[0:4], dataset[4:]

print(f"Size of training data: {len(training_data)}\nSize of test data: {len(test_data)}")
```

+   **å°†è®­ç»ƒå’Œæµ‹è¯•æ•°æ®ä¸Šä¼ åˆ° S3** â€” ä¸€æ—¦è®­ç»ƒå’Œæµ‹è¯•é›†å‡†å¤‡å¥½å¹¶æ ¼å¼åŒ–ä¸ºå­—å…¸åˆ—è¡¨ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ä¸‹é¢çš„å·¥å…·å‡½æ•°å°†å®ƒä»¬ä½œä¸º JSON è¡Œä¸Šä¼ åˆ° S3ï¼š

```py
def write_jsonlines_to_s3(data, s3_path):
    """Writes list of dictionaries as a JSON lines file to S3"""

    json_string = ""
    for d in data:
        json_string += json.dumps(d) + "\n"

    s3_client   = boto3.client("s3")

    bucket, key = s3_utils.parse_s3_url(s3_path)
    s3_client.put_object(
         Body   = json_string,
         Bucket = bucket,
         Key    = key,
    )

write_jsonlines_to_s3(training_data, training_data_path)
write_jsonlines_to_s3(test_data, test_data_path)
```

# 3\. SageMaker è®­ç»ƒä»»åŠ¡

ä½¿ç”¨ S3 ä¸­å¯ç”¨çš„æ•°æ®é›†ï¼Œæˆ‘ä»¬ç°åœ¨å°†åœ¨ Amazon SageMaker ä¸­åˆ›å»ºä¸€ä¸ªè®­ç»ƒä»»åŠ¡ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬éœ€è¦åˆ›å»ºä¸€ä¸ªå…¥å£ç‚¹è„šæœ¬ï¼Œä¿®æ”¹é…ç½®æ–‡ä»¶ä»¥æŒ‡å®šè®­ç»ƒè®¾ç½®ï¼Œå¹¶å®šä¹‰ä¸€ä¸ª HuggingFace ä¼°ç®—å™¨ã€‚æˆ‘ä»¬å°†ï¼ˆé‡æ–°ï¼‰ä½¿ç”¨æ¥è‡ª [LLM Foundry](https://github.com/mosaicml/llm-foundry) å’Œ [Composer](https://github.com/mosaicml/composer) åº“çš„ CLI å¯åŠ¨å™¨ï¼Œè¿™äº›å¯åŠ¨å™¨è®¾ç½®äº†åˆ†å¸ƒå¼è®­ç»ƒç¯å¢ƒã€‚è¿™ä¸¤ä¸ªåŒ…å‡ç”± [MosaicML](https://www.mosaicml.com/) ç»´æŠ¤ï¼Œè¯¥å…¬å¸æ˜¯ MPT-7B çš„èƒŒåå…¬å¸ã€‚å·¥ä½œæ–‡ä»¶å¤¹åº”æŒ‰å¦‚ä¸‹ç»“æ„ç»„ç»‡ï¼š

```py
â””â”€â”€ **fine-tune-mpt-7b-sagemaker**/
    â”œâ”€â”€ training_script_launcher.sh
    â”œâ”€â”€ fine_tuning_config.yaml
    â”œâ”€â”€ sagemaker_finetuning.ipynb
```

æˆ‘ä»¬ç°åœ¨å°†æ·±å…¥äº†è§£è¿™äº›æ–‡ä»¶ä¸­çš„æ¯ä¸€ä¸ªã€‚

+   **åˆ›å»ºä¸€ä¸ªé…ç½®æ–‡ä»¶** `[**finetuning_config.yaml**](https://github.com/jpcpereira/sagemaker-fine-tune-mpt-7b/blob/main/finetuning_config.yaml)`â€” [LLM Foundry](https://github.com/mosaicml/llm-foundry) ä»“åº“ä¸­æä¾›çš„æ¨¡æ¿æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„èµ·ç‚¹ï¼Œç‰¹åˆ«æ˜¯ `[mpt-7b-dolly-sft.yaml](https://github.com/mosaicml/llm-foundry/blob/main/scripts/train/yamls/finetune/mpt-7b_dolly_sft.yaml)` æ–‡ä»¶ã€‚ç„¶è€Œï¼Œæ ¹æ®æ•°æ®é›†å¤§å°å’Œè®­ç»ƒå®ä¾‹ï¼Œæ‚¨å¯èƒ½éœ€è¦è°ƒæ•´ä¸€äº›é…ç½®ï¼Œä¾‹å¦‚æ‰¹é‡å¤§å°ã€‚æˆ‘å·²ç»ä¿®æ”¹äº†è¯¥æ–‡ä»¶ä»¥åœ¨ SageMaker ä¸­å¾®è°ƒæ¨¡å‹ï¼ˆè¯·æŸ¥çœ‹ `[finetuning_config.yaml](https://github.com/jpcpereira/sagemaker-fine-tune-mpt-7b/blob/main/finetuning_config.yaml)`ï¼‰ã€‚æ‚¨éœ€è¦å…³æ³¨çš„å‚æ•°å¦‚ä¸‹ï¼š

```py
max_seq_len: 512
global_seed: 17

...
# Dataloaders
train_loader:
  name: finetuning
  dataset:
    hf_name: json
    hf_kwargs:
        data_dir: /opt/ml/input/data/train/
...

eval_loader:
  name: finetuning
  dataset:
    hf_name: json
    hf_kwargs:
        data_dir: /opt/ml/input/data/test/

...
max_duration: 3ep
eval_interval: 1ep
...
global_train_batch_size: 128

...
# FSDP
fsdp_config:
  sharding_strategy: FULL_SHARD
  mixed_precision: PURE
  activation_checkpointing: true
  activation_checkpointing_reentrant: false
  activation_cpu_offload: false
  limit_all_gathers: true
  verbose: false

# Checkpoint to local filesystem or remote object store
save_folder: /tmp/checkpoints
dist_timeout: 2000
```

`max_seq_length` æŒ‡ç¤ºè¾“å…¥çš„æœ€å¤§ä»¤ç‰Œæ•°ï¼ˆè®°ä½ 100 ä¸ªä»¤ç‰Œçº¦ç­‰äº 75 ä¸ªå•è¯ï¼‰ã€‚è®­ç»ƒå’Œæµ‹è¯•æ•°æ®å°†ä½¿ç”¨ ğŸ˜Š [Datasets](https://huggingface.co/docs/datasets/index) åº“ä»å®¹å™¨å†…ä¸è®­ç»ƒä»»åŠ¡å…³è”çš„ `/opt/ml/input/data/{train, test}` ç›®å½•åŠ è½½ã€‚æŸ¥çœ‹ [SageMaker Training Storage Folders](https://docs.aws.amazon.com/sagemaker/latest/dg/model-train-storage.html) æ–‡æ¡£ï¼Œä»¥äº†è§£å®¹å™¨ç›®å½•çš„ç»“æ„ã€‚`max_duration` æŒ‡å®šå¾®è°ƒçš„è½®æ•°ã€‚ä¸¤åˆ°ä¸‰è½®é€šå¸¸æ˜¯ä¸€ä¸ªä¸é”™çš„é€‰æ‹©ã€‚`eval_interval` æŒ‡ç¤ºæ¨¡å‹åœ¨æµ‹è¯•é›†ä¸Šçš„è¯„ä¼°é¢‘ç‡ã€‚

åˆ†å¸ƒå¼è®­ç»ƒç­–ç•¥æ˜¯ Fully Sharded Data Parallel (FSDP)ï¼Œè¿™ä½¿å¾—åƒ MPT-7B è¿™æ ·çš„å·¨å¤§æ¨¡å‹çš„è®­ç»ƒå˜å¾—é«˜æ•ˆã€‚ä¸ä¼ ç»Ÿçš„æ•°æ®å¹¶è¡Œç­–ç•¥ä¸åŒï¼Œåè€…åœ¨æ¯ä¸ª GPU ä¸­ä¿ç•™æ¨¡å‹å‰¯æœ¬ï¼ŒFSDP å°†æ¨¡å‹å‚æ•°ã€ä¼˜åŒ–å™¨çŠ¶æ€å’Œæ¢¯åº¦åœ¨æ•°æ®å¹¶è¡Œå·¥ä½œè€…ä¹‹é—´è¿›è¡Œåˆ†ç‰‡ã€‚å¦‚æœæ‚¨æƒ³æ·±å…¥äº†è§£ FSDPï¼Œå¯ä»¥æŸ¥çœ‹è¿™ç¯‡æœ‰è§åœ°çš„ [PyTorch ä»‹ç»æ–‡ç« ](https://pytorch.org/blog/introducing-pytorch-fully-sharded-data-parallel-api/)ã€‚FSDP å·²é›†æˆåœ¨ [Composer](https://github.com/mosaicml/composer) ä¸­ï¼Œè¿™æ˜¯ [LLM Foundry](https://github.com/mosaicml/llm-foundry) ä½¿ç”¨çš„åˆ†å¸ƒå¼è®­ç»ƒåº“ã€‚

`save_folder` ç¡®å®šæ¨¡å‹æ£€æŸ¥ç‚¹ï¼ˆ`.pt` æ–‡ä»¶ï¼‰ä¿å­˜çš„ä½ç½®ã€‚æˆ‘ä»¬å°†å…¶è®¾ç½®ä¸ºä¸´æ—¶æ–‡ä»¶å¤¹ `/tmp/checkpoints`ã€‚

+   **åˆ›å»ºå…¥å£è„šæœ¬** `[**launcher.sh**](https://github.com/jpcpereira/sagemaker-fine-tune-mpt-7b/blob/main/launcher.sh)`â€” ä¸€ä¸ª bash è„šæœ¬ä½œä¸ºå…¥å£ç‚¹ã€‚è¿™ä¸ª bash è„šæœ¬å…‹éš†äº† LLM Foundry ä»“åº“ï¼Œå®‰è£…äº†æ‰€éœ€çš„ä¾èµ–ï¼Œå¹¶ä¸”æ›´é‡è¦çš„æ˜¯ï¼Œä½¿ç”¨ Composer åº“çš„åˆ†å¸ƒå¼å¯åŠ¨å™¨è¿è¡Œè®­ç»ƒè„šæœ¬ã€‚è¯·æ³¨æ„ï¼Œé€šå¸¸åœ¨ SageMaker ä¸­ï¼Œè®­ç»ƒä½œä¸šæ˜¯é€šè¿‡åƒ `python train.py` è¿™æ ·çš„å‘½ä»¤è¿è¡Œè®­ç»ƒè„šæœ¬ã€‚ç„¶è€Œï¼Œåœ¨æˆ‘ä»¬çš„åœºæ™¯ä¸­ï¼Œå¯ä»¥å°† bash è„šæœ¬ä½œä¸ºå…¥å£ç‚¹ï¼Œè¿™æä¾›äº†æ›´å¤šçš„çµæ´»æ€§ã€‚æœ€åï¼Œæˆ‘ä»¬å°†ä¿å­˜åˆ° `/tmp/checkpoints` çš„æ¨¡å‹æ£€æŸ¥ç‚¹è½¬æ¢ä¸º HuggingFace æ¨¡å‹æ ¼å¼ï¼Œå¹¶å°†æœ€ç»ˆçš„å·¥ä»¶ä¿å­˜åˆ° `/opt/ml/model/`ã€‚SageMaker ä¼šå‹ç¼©æ­¤ç›®å½•ä¸­çš„æ‰€æœ‰æ–‡ä»¶ï¼Œåˆ›å»ºä¸€ä¸ª tarball `model.tar.gz`ï¼Œå¹¶å°†å…¶ä¸Šä¼ åˆ° S3ã€‚è¿™ä¸ª tarball å¯¹äºæ¨ç†éå¸¸æœ‰ç”¨ã€‚

```py
# Clone llm-foundry package from MosaicML
# This is where the training script is hosted
git clone https://github.com/mosaicml/llm-foundry.git
cd llm-foundry

# Install required packages
pip install -e ".[gpu]"
pip install git+https://github.com/mosaicml/composer.git@dev

# Run training script with fine-tuning configuration
composer scripts/train/train.py /opt/ml/code/finetuning_config.yaml

# Convert Composer checkpoint to HuggingFace model format
python scripts/inference/convert_composer_to_hf.py \
    --composer_path /tmp/checkpoints/latest-rank0.pt \
    --hf_output_path /opt/ml/model/hf_fine_tuned_model \
    --output_precision bf16

# Print content of the model artifact directory
ls /opt/ml/model/
```

+   **å®šä¹‰ ğŸ˜Š HuggingFace ä¼°ç®—å™¨** â€” ä¼°ç®—å™¨è®¾ç½®äº†ç”¨äºè¿è¡Œè®­ç»ƒä½œä¸šçš„ Docker å®¹å™¨ã€‚æˆ‘ä»¬å°†ä½¿ç”¨ä¸€ä¸ªå¸¦æœ‰ PyTorch 2.0.0 å’Œ Python 3.10 çš„é•œåƒã€‚bash è„šæœ¬å’Œé…ç½®æ–‡ä»¶ä¼šè‡ªåŠ¨ä¸Šä¼ åˆ° S3ï¼Œå¹¶åœ¨å®¹å™¨å†…å¯ç”¨ï¼ˆç”± SageMaker Python SDK å¤„ç†ï¼‰ã€‚æˆ‘ä»¬å°†è®­ç»ƒå®ä¾‹è®¾ç½®ä¸º`[g5.48xlarge](https://aws.amazon.com/ec2/instance-types/g5/)`ï¼Œå®ƒé…å¤‡äº† 8x NVIDIA A10G GPUã€‚`[p4d.24xlarge](https://aws.amazon.com/ec2/instance-types/p4/)` ä¹Ÿæ˜¯ä¸€ä¸ªä¸é”™çš„é€‰æ‹©ã€‚è™½ç„¶å®ƒæ›´è´µï¼Œä½†é…å¤‡äº† 8x NVIDIA A100 GPUã€‚æˆ‘ä»¬è¿˜æŒ‡æ˜äº†è¦åœ¨è®­ç»ƒé›†å’Œæµ‹è¯•é›†ä¸­è·Ÿè¸ªçš„æŒ‡æ ‡ï¼ˆäº¤å‰ç†µå’Œå›°æƒ‘åº¦ï¼‰ã€‚è¿™äº›æŒ‡æ ‡çš„å€¼é€šè¿‡æ­£åˆ™è¡¨è¾¾å¼æ•è·å¹¶å‘é€åˆ° Amazon CloudWatchã€‚

```py
# Define container image for the training job
training_image_uri = f"763104351884.dkr.ecr.{region}.amazonaws.com/huggingface-pytorch-training:2.0.0-transformers4.28.1-gpu-py310-cu118-ubuntu20.04-v1.1"

# Define metrics to send to CloudWatch
metrics = [
    # On training set
    {"Name": "train:LanguageCrossEntropy",
     "Regex": "Train metrics\/train\/LanguageCrossEntropy: ([+-]?((\d+\.?\d*)|(\.\d+)))"},
    {"Name": "train:LanguagePerplexity",
     "Regex": "Train metrics\/train\/LanguagePerplexity: ([+-]?((\d+\.?\d*)|(\.\d+)))"},
    # On test set
    {"Name": "test:LanguageCrossEntropy",
     "Regex": "Eval metrics\/eval\/LanguageCrossEntropy: ([+-]?((\d+\.?\d*)|(\.\d+)))"},
    {"Name": "test:LanguagePerplexity",
     "Regex": "Eval metrics\/eval\/LanguagePerplexity: ([+-]?((\d+\.?\d*)|(\.\d+)))"},
]

estimator_args = {
    "image_uri": training_image_uri,     # Training container image
    "entry_point": "launcher.sh",        # Launcher bash script
    "source_dir": ".",                   # Directory with launcher script and configuration file
    "instance_type": "ml.g5.48xlarge",   # Instance type
    "instance_count": 1,                 # Number of training instances
    "base_job_name": "fine-tune-mpt-7b", # Prefix of the training job name
    "role": role,                        # IAM role
    "volume_size": 300,                  # Size of the EBS volume attached to the instance (GB)
    "py_version": "py310",               # Python version
    "metric_definitions": metrics,       # Metrics to track
    "output_path": output_path,          # S3 location where the model artifact will be uploaded
    "code_location": code_location,      # S3 location where the source code will be saved
    "disable_profiler": True,            # Do not create profiler instance
    "keep_alive_period_in_seconds": 240, # Enable Warm Pools while experimenting
}

huggingface_estimator = HuggingFace(**estimator_args)
```

> âš ï¸ ç¡®ä¿è¯·æ±‚ SageMaker è®­ç»ƒçš„ç›¸åº”é…é¢ï¼Œå¹¶åœ¨ä½¿ç”¨æ­¤é…·ç‚«åŠŸèƒ½æ—¶è¯·æ±‚ [çƒ­æ± ](https://docs.aws.amazon.com/sagemaker/latest/dg/train-warm-pools.html) çš„é…é¢ã€‚å¦‚æœä½ è®¡åˆ’åœ¨ SageMaker ä¸­è¿è¡Œå¤šä¸ªä½œä¸šï¼Œå¯ä»¥æŸ¥çœ‹ [SageMaker èŠ‚çœè®¡åˆ’](https://aws.amazon.com/savingsplans/ml-pricing/)ã€‚

+   **å¯åŠ¨è®­ç»ƒä½œä¸š ğŸš€** â€” æˆ‘ä»¬å·²å‡†å¤‡å¥½åœ¨ Amazon SageMaker ä¸Šå¼€å§‹è®­ç»ƒä½œä¸šï¼š

```py
huggingface_estimator.fit({
    "train": TrainingInput(
        s3_data=training_data_path,
        content_type="application/jsonlines"),
    "test": TrainingInput(
        s3_data=test_data_path,
        content_type="application/jsonlines"),
}, wait=True)
```

è®­ç»ƒæ—¶é—´å°†å–å†³äºæ•°æ®é›†çš„å¤§å°ã€‚ä½¿ç”¨æˆ‘ä»¬çš„è™šæ‹Ÿæ•°æ®é›†ï¼Œè®­ç»ƒå¤§çº¦éœ€è¦ **20min** å®Œæˆã€‚ä¸€æ—¦æ¨¡å‹è®­ç»ƒå®Œæˆå¹¶è½¬æ¢ä¸º ğŸ˜Š HuggingFace æ ¼å¼ï¼ŒSageMaker å°†æŠŠæ¨¡å‹ tarball (`model.tar.gz`) ä¸Šä¼ åˆ° S3 `output_path`ã€‚æˆ‘å‘ç°å®é™…ä¸Šä¸Šä¼ æ­¥éª¤èŠ±è´¹çš„æ—¶é—´è¾ƒé•¿ï¼ˆ>1hï¼‰ï¼Œè¿™å¯èƒ½æ˜¯ç”±äºæ¨¡å‹å·¥ä»¶å‹ç¼©çš„å¤§å°ï¼ˆ~25GBï¼‰ã€‚

# 4\. æ€»ç»“

åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘å±•ç¤ºäº†å¦‚ä½•å‡†å¤‡æ•°æ®é›†å¹¶åœ¨ SageMaker ä¸­åˆ›å»ºè®­ç»ƒä»»åŠ¡ï¼Œä»¥å¾®è°ƒ MPT-7B ä»¥é€‚åº”ä½ çš„ä½¿ç”¨æ¡ˆä¾‹ã€‚è¯¥å®ç°åˆ©ç”¨äº†æ¥è‡ª [LLM Foundry](https://github.com/mosaicml/llm-foundry) çš„è®­ç»ƒè„šæœ¬ï¼Œå¹¶ä½¿ç”¨äº† [Composer](https://github.com/mosaicml/composer) åº“çš„åˆ†å¸ƒå¼è®­ç»ƒå¯åŠ¨å™¨ã€‚ä¸€æ—¦ä½ å¾®è°ƒäº†ä½ çš„æ¨¡å‹å¹¶æƒ³è¦éƒ¨ç½²å®ƒï¼Œæˆ‘å»ºè®®æŸ¥çœ‹ [Philipp Schmid çš„åšå®¢æ–‡ç« ](https://www.philschmid.de/)ï¼›é‚£é‡Œæœ‰å¾ˆå¤šå…³äºå¦‚ä½•åœ¨ SageMaker ä¸­éƒ¨ç½² LLM çš„ç¤ºä¾‹ã€‚ç¥ä½ ç©å¾—æ„‰å¿«ï¼Œäº«å—ä½ çš„å¾®è°ƒ MPT-7B æ¨¡å‹ï¼ğŸ‰

æœ¬æ–‡ä¸­ä½¿ç”¨çš„æ‰€æœ‰ä»£ç éƒ½å¯ä»¥åœ¨ Github ä¸Šæ‰¾åˆ°ï¼š

[## GitHub - jpcpereira/sagemaker-fine-tune-mpt-7b](https://github.com/jpcpereira/sagemaker-fine-tune-mpt-7b?source=post_page-----1e68e71051fa--------------------------------)

[github.com](https://github.com/jpcpereira/sagemaker-fine-tune-mpt-7b?source=post_page-----1e68e71051fa--------------------------------)

â€” ä¹”æ˜‚Â·ä½©é›·æ‹‰

*æ„Ÿè°¢é˜…è¯»ã€‚å¸Œæœ›æœ¬æ–‡èƒ½å¸®åŠ©ä½ å…¥é—¨åœ¨ Amazon SageMaker ä¸­å¾®è°ƒå¤§å‹è¯­è¨€æ¨¡å‹å¦‚ MPT-7Bã€‚å¦‚æœä½ æƒ³é˜…è¯»æˆ‘æœªæ¥çš„æ–‡ç« ï¼Œè¯·* [*å…³æ³¨æˆ‘*](https://medium.com/@joao.pereira.abt/subscribe)*ã€‚éå¸¸æ„Ÿè°¢åé¦ˆï¼å¦‚æœä½ æœ‰ä»»ä½•é—®é¢˜ï¼Œè¯·åœ¨ä¸‹é¢ç•™è¨€ï¼Œæˆ–ç›´æ¥é€šè¿‡* ***ç”µå­é‚®ä»¶*** *æˆ–åœ¨* [***LinkedIn***](https://www.linkedin.com/in/jpcpereira/)*è”ç³»æˆ‘ã€‚*
