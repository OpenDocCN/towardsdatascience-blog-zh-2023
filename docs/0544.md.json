["```py\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(df.drop('target', axis=1), df['target'], test_size=0.2, random_state=0)\n\n# Train and score Random Forest \nsimple_rf_model = RandomForestClassifier(n_estimators=100, random_state=0)\nsimple_rf_model.fit(X_train, y_train)\nprint(f\"accuracy: {simple_rf_model.score(X_test, y_test)}\")\n\n# accuracy: 0.93\n```", "```py\nsimple_rf_model = RandomForestClassifier(n_estimators=100, random_state=0)\n```", "```py\n# Create decision trees\ntree1 = DecisionTreeClassifier().fit(X_train, y_train)\ntree2 = DecisionTreeClassifier().fit(X_train, y_train)\ntree3 = DecisionTreeClassifier().fit(X_train, y_train)\n\n# predict each decision tree on X_test\npredictions_1 = tree1.predict(X_test)\npredictions_2 = tree2.predict(X_test)\npredictions_3 = tree3.predict(X_test)\nprint(predictions_1, predictions_2, predictions_3)\n\n# take the majority rules\nfinal_prediction = np.array([np.round((predictions_1[i] + predictions_2[i] + predictions_3[i])/3) for i in range(len(predictions_1))])\nprint(final_prediction)\n```", "```py\nimport numpy as np\nimport pandas as pd\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import train_test_split\n\n# Take 3 samples with replacement from X_train for each tree\ndf_sample1 = df.sample(frac=.67, replace=True)\ndf_sample2 = df.sample(frac=.67, replace=True)\ndf_sample3 = df.sample(frac=.67, replace=True)\n\nX_train_sample1, X_test_sample1, y_train_sample1, y_test_sample1 = train_test_split(df_sample1.drop('target', axis=1), df_sample1['target'], test_size=0.2)\nX_train_sample2, X_test_sample2, y_train_sample2, y_test_sample2 = train_test_split(df_sample2.drop('target', axis=1), df_sample2['target'], test_size=0.2)\nX_train_sample3, X_test_sample3, y_train_sample3, y_test_sample3 = train_test_split(df_sample3.drop('target', axis=1), df_sample3['target'], test_size=0.2)\n\n# Create the decision trees\ntree1 = DecisionTreeClassifier().fit(X_train_sample1, y_train_sample1)\ntree2 = DecisionTreeClassifier().fit(X_train_sample2, y_train_sample2)\ntree3 = DecisionTreeClassifier().fit(X_train_sample3, y_train_sample3)\n\n# predict each decision tree on X_test\npredictions_1 = tree1.predict(X_test)\npredictions_2 = tree2.predict(X_test)\npredictions_3 = tree3.predict(X_test)\ndf = pd.DataFrame([predictions_1, predictions_2, predictions_3]).T\ndf.columns = [\"tree1\", \"tree2\", \"tree3\"]\n\n# take the majority rules \nfinal_prediction = np.array([np.round((predictions_1[i] + predictions_2[i] + predictions_3[i])/3) for i in range(len(predictions_1))])\npreds = pd.DataFrame([predictions_1, predictions_2, predictions_3, final_prediction, y_test]).T.head(20)\npreds.columns = [\"tree1\", \"tree2\", \"tree3\", \"final\", \"label\"]\npreds\n```", "```py\nimport numpy as np\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import BaggingClassifier\n\n# Number of trees to use in the ensemble\nn_estimators = 3\n\n# Initialize the bagging classifier\nbag_clf = BaggingClassifier(\n    DecisionTreeClassifier(), n_estimators=n_estimators, bootstrap=True)\n\n# Fit the bagging classifier on the training data\nbag_clf.fit(X_train, y_train)\n\n# Make predictions on the test data\ny_pred = bag_clf.predict(X_test)\npd.DataFrame([y_pred, y_test]).T\n```", "```py\nimport numpy as np\nimport pandas as pd\nimport math\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import train_test_split\n\n# take 3 samples from X_train for each tree\ndf_sample1 = df.sample(frac=.67, replace=True)\ndf_sample2 = df.sample(frac=.67, replace=True)\ndf_sample3 = df.sample(frac=.67, replace=True)\n\n# split off train set\nX_train_sample1, y_train_sample1 = df_sample1.drop('target', axis=1), df_sample1['target']\nX_train_sample2, y_train_sample2 = df_sample2.drop('target', axis=1), df_sample2['target']\nX_train_sample3, y_train_sample3 = df_sample3.drop('target', axis=1), df_sample3['target']\n\n# get sampled features for train and test using sqrt, note how replace=False now\nnum_features = len(X_train.columns)\nX_train_sample1 = X_train_sample1.sample(n=int(math.sqrt(num_features)), replace=False, axis = 1)\nX_train_sample2 = X_train_sample2.sample(n=int(math.sqrt(num_features)), replace=False, axis = 1)\nX_train_sample3 = X_train_sample3.sample(n=int(math.sqrt(num_features)), replace=False, axis = 1)\n\n# create the decision trees, this time we are sampling columns\ntree1 = DecisionTreeClassifier().fit(X_train_sample1, y_train_sample1)\ntree2 = DecisionTreeClassifier().fit(X_train_sample2, y_train_sample2)\ntree3 = DecisionTreeClassifier().fit(X_train_sample3, y_train_sample3)\n\n# predict each decision tree on X_test\npredictions_1 = tree1.predict(X_test[X_train_sample1.columns])\npredictions_2 = tree2.predict(X_test[X_train_sample2.columns])\npredictions_3 = tree3.predict(X_test[X_train_sample3.columns])\npreds = pd.DataFrame([predictions_1, predictions_2, predictions_3]).T\npreds.columns = [\"tree1\", \"tree2\", \"tree3\"]\n\n# take the majority rules \nfinal_prediction = np.array([np.round((predictions_1[i] + predictions_2[i] + predictions_3[i])/3) for i in range(len(predictions_1))])\npreds = pd.DataFrame([predictions_1, predictions_2, predictions_3, final_prediction, y_test]).T.head(20)\npreds.columns = [\"tree1\", \"tree2\", \"tree3\", \"final\", \"label\"]\n```", "```py\nfrom collections import Counter\nfrom math import log2\n\n# my predictor classes are 0 or 1\\. 0 is a blue marble, 1 is a green marble.\ndata = [0, 0, 0, 1, 1, 1, 1, 0, 1, 0]\n# get length of labels\nlen_labels = len(data)\ndef calculate_entropy(data, len_labels):\n    # we do a count of each class\n    counts = Counter(labels)\n    # we calculate the fractions, the output should be [.5, .5] for this example\n    probs = [count / num_labels for count in counts.values()]\n    # the actual entropy calculation \n    return - sum(p * log2(p) for p in probs)\n\ncalculate_entropy(labels, num_labels)\n```", "```py\ndef information_gain(left_labels, right_labels, parent_entropy):\n    \"\"\"Calculate the information gain of a split\"\"\"\n    # calculate the weight of the left node\n    proportion_left_node = float(len(left_labels)) / (len(left_labels) + len(right_labels))\n    #calculate the weight of the right node\n    proportion_right_node = 1 - proportion_left_node\n    # compute the weighted average of the child node\n    weighted_average_of_child_nodes = ((proportion_left_node * entropy(left_labels)) + (proportion_right_node * entropy(right_labels)))\n    # return the parent node entropy - the weighted entropy of child nodes\n    return parent_entropy - weighted_average_of_child_nodes\n```", "```py\nimport pandas as pd\nimport numpy as np\nfrom math import log2\n\ndef entropy(data, target_col):\n    # calculate the entropy of the entire dataset\n    values, counts = np.unique(data[target_col], return_counts=True)\n    entropy = np.sum([-count/len(data) * log2(count/len(data)) for count in counts])\n    return entropy\n\ndef compute_information_gain(data, feature, target_col):\n    parent_entropy = entropy(data, target_col)\n    # calculate the information gain for splitting on a given feature\n    split_values = np.unique(data[feature])\n    # initialize at 0\n    weighted_child_entropy = 0\n    # compute the weighted entropy, remember that this is related to the number of points in the new node\n    for value in split_values:\n        sub_data = data[data[feature] == value]\n        node_weight = len(sub_data)/len(data)\n        weighted_child_entropy += node_weight * entropy(sub_data, target_col)\n    # same calculation as before, we just subtract the weighted entropy from the parent node entropy \n    return parent_entropy - weighted_child_entropy\n\ndef grow_tree(data, features, target_col, depth=0, max_depth=3):\n    # we set a max depth of 3 to \"pre-prune\" or limit the tree complexity\n    if depth >= max_depth or len(np.unique(data[target_col])) == 1:\n        # stop growing the tree if maximum depth is reached or all labels are the same. All labels being the same means the entropy is 0\n        return np.unique(data[target_col])[0]\n    # we compute the best feature (or best question to ask) based on information gain\n    node = {}\n    gains = [compute_information_gain(data, feature, target_col) for feature in features]\n    best_feature = features[np.argmax(gains)]\n\n    for value in np.unique(data[best_feature]):\n        sub_data = data[data[best_feature] == value]\n        node[value] = grow_tree(sub_data, features, target_col, depth+1, max_depth)\n\n    return node\n\n# simulate some data and make a dataframe, note how we have a target\ndata = {\n    'A': [1, 2, 1, 2, 1, 2, 1, 2],\n    'B': [3, 3, 4, 4, 3, 3, 4, 4],\n    'C': [5, 5, 5, 5, 6, 6, 6, 6],\n    'target': [0, 0, 0, 1, 1, 1, 1, 0]\n}\ndf = pd.DataFrame(data)\n\n# define our features and label\nfeatures = [\"A\", \"B\", \"C\"]\ntarget_col = \"target\"\n\n# grow the tree\ntree = grow_tree(df, features, target_col, max_depth=3)\nprint(tree)\n```", "```py\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(df.drop('target', axis=1), df['target'], test_size=0.2, random_state=0)\n\n# Train and score Random Forest \nsimple_rf_model = RandomForestClassifier(n_estimators=100, random_state=0)\nsimple_rf_model.fit(X_train, y_train)\nprint(f\"accuracy: {simple_rf_model.score(X_test, y_test)}\")\n\n# accuracy: 0.93\n```"]