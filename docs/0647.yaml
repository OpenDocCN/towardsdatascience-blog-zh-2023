- en: PCA vs Autoencoders for a Small Dataset in Dimensionality Reduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/pca-vs-autoencoders-for-a-small-dataset-in-dimensionality-reduction-67b15318dea0?source=collection_archive---------7-----------------------#2023-02-16](https://towardsdatascience.com/pca-vs-autoencoders-for-a-small-dataset-in-dimensionality-reduction-67b15318dea0?source=collection_archive---------7-----------------------#2023-02-16)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Neural Networks and Deep Learning Course: Part 45'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://rukshanpramoditha.medium.com/?source=post_page-----67b15318dea0--------------------------------)[![Rukshan
    Pramoditha](../Images/b80426aff64ff186cb915795644590b1.png)](https://rukshanpramoditha.medium.com/?source=post_page-----67b15318dea0--------------------------------)[](https://towardsdatascience.com/?source=post_page-----67b15318dea0--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----67b15318dea0--------------------------------)
    [Rukshan Pramoditha](https://rukshanpramoditha.medium.com/?source=post_page-----67b15318dea0--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ff90a3bb1d400&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpca-vs-autoencoders-for-a-small-dataset-in-dimensionality-reduction-67b15318dea0&user=Rukshan+Pramoditha&userId=f90a3bb1d400&source=post_page-f90a3bb1d400----67b15318dea0---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----67b15318dea0--------------------------------)
    ·8 min read·Feb 16, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F67b15318dea0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpca-vs-autoencoders-for-a-small-dataset-in-dimensionality-reduction-67b15318dea0&user=Rukshan+Pramoditha&userId=f90a3bb1d400&source=-----67b15318dea0---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F67b15318dea0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpca-vs-autoencoders-for-a-small-dataset-in-dimensionality-reduction-67b15318dea0&source=-----67b15318dea0---------------------bookmark_footer-----------)![](../Images/40244380e27388a744474a8be2aa482a.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Photo by [Robert Katzki](https://unsplash.com/@ro_ka?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    on [Unsplash](https://unsplash.com/photos/jbtfM0XBeRc?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
  prefs: []
  type: TYPE_NORMAL
- en: '***Can general machine learning algorithms outperform neural networks with
    small datasets?***'
  prefs: []
  type: TYPE_NORMAL
- en: In general, deep learning algorithms such as neural networks require a massive
    amount of data to achieve reasonable performance. So, neural networks like autoencoders
    can benefit from very large datasets that we use to train the models.
  prefs: []
  type: TYPE_NORMAL
- en: Sometimes, general machine learning algorithms can outperform neural network
    algorithms when they are trained with very small datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Autoencoders can also be used in dimensionality reduction applications, even
    though they are widely used in other popular applications such as image denoising,
    image generation, image colorization, image compression, image super-resolution,
    etc.
  prefs: []
  type: TYPE_NORMAL
- en: Earlier, we compared the performance of autoencoders in dimensionality reduction
    against PCA by training the models on the *very large* MNIST dataset. There, the
    autoencoder model easily outperformed the PCA model [ref¹] because the MNIST data
    is large and non-linear.
  prefs: []
  type: TYPE_NORMAL
- en: 'ref¹: [*How Autoencoders Outperform PCA in Dimensionality Reduction*](/how-autoencoders-outperform-pca-in-dimensionality-reduction-1ae44c68b42f)'
  prefs: []
  type: TYPE_NORMAL
- en: Autoencoders work well with large and non-linear…
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
