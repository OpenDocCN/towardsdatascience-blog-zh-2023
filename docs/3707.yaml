- en: 'Beyond English: Implementing a multilingual RAG solution'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: è¶…è¶Šè‹±è¯­ï¼šå®ç°å¤šè¯­è¨€RAGè§£å†³æ–¹æ¡ˆ
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/beyond-english-implementing-a-multilingual-rag-solution-12ccba0428b6?source=collection_archive---------0-----------------------#2023-12-20](https://towardsdatascience.com/beyond-english-implementing-a-multilingual-rag-solution-12ccba0428b6?source=collection_archive---------0-----------------------#2023-12-20)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://towardsdatascience.com/beyond-english-implementing-a-multilingual-rag-solution-12ccba0428b6?source=collection_archive---------0-----------------------#2023-12-20](https://towardsdatascience.com/beyond-english-implementing-a-multilingual-rag-solution-12ccba0428b6?source=collection_archive---------0-----------------------#2023-12-20)
- en: An introduction to the doâ€™s and donâ€™ts when implementing a non-english Retrieval
    Augmented Generation (RAG) system
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å®æ–½éè‹±è¯­æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ç³»ç»Ÿæ—¶çš„æ³¨æ„äº‹é¡¹
- en: '[](https://medium.com/@jalkestrup?source=post_page-----12ccba0428b6--------------------------------)[![Jesper
    Alkestrup](../Images/199803c75758a9b943e72746105f3de5.png)](https://medium.com/@jalkestrup?source=post_page-----12ccba0428b6--------------------------------)[](https://towardsdatascience.com/?source=post_page-----12ccba0428b6--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----12ccba0428b6--------------------------------)
    [Jesper Alkestrup](https://medium.com/@jalkestrup?source=post_page-----12ccba0428b6--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@jalkestrup?source=post_page-----12ccba0428b6--------------------------------)[![Jesper
    Alkestrup](../Images/199803c75758a9b943e72746105f3de5.png)](https://medium.com/@jalkestrup?source=post_page-----12ccba0428b6--------------------------------)[](https://towardsdatascience.com/?source=post_page-----12ccba0428b6--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----12ccba0428b6--------------------------------)
    [Jesper Alkestrup](https://medium.com/@jalkestrup?source=post_page-----12ccba0428b6--------------------------------)'
- en: Â·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Â·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F84e00f9ebe19&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbeyond-english-implementing-a-multilingual-rag-solution-12ccba0428b6&user=Jesper+Alkestrup&userId=84e00f9ebe19&source=post_page-84e00f9ebe19----12ccba0428b6---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----12ccba0428b6--------------------------------)
    Â·18 min readÂ·Dec 20, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F12ccba0428b6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbeyond-english-implementing-a-multilingual-rag-solution-12ccba0428b6&user=Jesper+Alkestrup&userId=84e00f9ebe19&source=-----12ccba0428b6---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[é˜…è¯»](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F84e00f9ebe19&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbeyond-english-implementing-a-multilingual-rag-solution-12ccba0428b6&user=Jesper+Alkestrup&userId=84e00f9ebe19&source=post_page-84e00f9ebe19----12ccba0428b6---------------------post_header-----------)
    åœ¨ [Towards Data Science](https://towardsdatascience.com/?source=post_page-----12ccba0428b6--------------------------------)
    å‘å¸ƒ Â· 18 åˆ†é’Ÿé˜…è¯» Â· 2023å¹´12æœˆ20æ—¥'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F12ccba0428b6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbeyond-english-implementing-a-multilingual-rag-solution-12ccba0428b6&source=-----12ccba0428b6---------------------bookmark_footer-----------)![](../Images/4b0d38957430dc14c526618b898f1182.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F12ccba0428b6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbeyond-english-implementing-a-multilingual-rag-solution-12ccba0428b6&source=-----12ccba0428b6---------------------bookmark_footer-----------)![](../Images/4b0d38957430dc14c526618b898f1182.png)'
- en: RAG, an all knowing colleague, available 24/7 (Image generated by author w.
    Dall-E 3)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: RAGï¼Œä¸€ä¸ªæ— æ‰€ä¸çŸ¥çš„åŒäº‹ï¼Œå…¨å¤©å€™æä¾›æœåŠ¡ï¼ˆå›¾ç‰‡ç”±ä½œè€…ä½¿ç”¨ Dall-E 3 ç”Ÿæˆï¼‰
- en: TLDR
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: TLDR
- en: 'This article provides an introduction to the considerations one should take
    into account when developing non-English RAG systems, complete with specific examples
    and techniques. Some of the key points include:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ç¯‡æ–‡ç« ä»‹ç»äº†åœ¨å¼€å‘éè‹±è¯­RAGç³»ç»Ÿæ—¶åº”è€ƒè™‘çš„å› ç´ ï¼Œå¹¶æä¾›äº†å…·ä½“çš„ç¤ºä¾‹å’ŒæŠ€æœ¯ã€‚å…³é”®ç‚¹åŒ…æ‹¬ï¼š
- en: Prioritize maintaining syntactic structure during data loading, as it is crucial
    for meaningful text segmentation.
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åœ¨æ•°æ®åŠ è½½è¿‡ç¨‹ä¸­ä¼˜å…ˆä¿æŒå¥æ³•ç»“æ„ï¼Œå› ä¸ºè¿™å¯¹æœ‰æ„ä¹‰çš„æ–‡æœ¬åˆ†å‰²è‡³å…³é‡è¦ã€‚
- en: Format documents using simple delimiters like \n\n to facilitate efficient text
    splitting.
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ç®€å•åˆ†éš”ç¬¦å¦‚\n\næ¥æ ¼å¼åŒ–æ–‡æ¡£ï¼Œä»¥ä¿ƒè¿›é«˜æ•ˆçš„æ–‡æœ¬æ‹†åˆ†ã€‚
- en: Opt for rule-based text splitters, given the computational intensity and subpar
    performance of ML-based semantic splitters in multilingual contexts.
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: é€‰æ‹©åŸºäºè§„åˆ™çš„æ–‡æœ¬åˆ†å‰²å™¨ï¼Œå› ä¸ºåœ¨å¤šè¯­è¨€ç¯å¢ƒä¸­ï¼ŒåŸºäº ML çš„è¯­ä¹‰åˆ†å‰²å™¨è®¡ç®—å¼ºåº¦å¤§ä¸”æ€§èƒ½è¾ƒå·®ã€‚
- en: In selecting an embedding model, consider both its multilingual capabilities
    and asymmetric retrieval performance.
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åœ¨é€‰æ‹©åµŒå…¥æ¨¡å‹æ—¶ï¼Œè€ƒè™‘å…¶å¤šè¯­è¨€èƒ½åŠ›å’Œä¸å¯¹ç§°æ£€ç´¢æ€§èƒ½ã€‚
- en: For multilingual projects, fine-tuning an embedding model with a Large Language
    Model (LLM) can enhance performance, and may be needed to achieve sufficient accuracy.
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¯¹äºå¤šè¯­è¨€é¡¹ç›®ï¼Œé€šè¿‡å¤§è¯­è¨€æ¨¡å‹ (LLM) å¾®è°ƒåµŒå…¥æ¨¡å‹å¯ä»¥æé«˜æ€§èƒ½ï¼Œå¯èƒ½éœ€è¦ä»¥å®ç°è¶³å¤Ÿçš„å‡†ç¡®æ€§ã€‚
- en: Implementing an LLM-based retrieval evaluation benchmark is strongly recommended
    to fine-tune the hyperparameters of your RAG system effectively, and can be done
    easily with existing frameworks.
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¼ºçƒˆæ¨èå®æ–½åŸºäº LLM çš„æ£€ç´¢è¯„ä¼°åŸºå‡†ï¼Œä»¥æœ‰æ•ˆå¾®è°ƒ RAG ç³»ç»Ÿçš„è¶…å‚æ•°ï¼Œå¹¶ä¸”å¯ä»¥åˆ©ç”¨ç°æœ‰æ¡†æ¶è½»æ¾å®Œæˆã€‚
- en: It is no wonder that RAG has become the trendiest term within search technology
    in 2023\. Retrieval Augmented Generation (RAG) is transforming how organizations
    utilize their vast quantity of existing data to power intelligent ChatBots. These
    bots, capable of conversations in natural language, can draw on an organizationâ€™s
    collective knowledge to function as an always-available, in-house expert to deliver
    relevant answers, grounded in verified data. While a considerable number of resources
    are available on building RAG systems, most are geared toward the English language,
    leaving a gap for smaller languages.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: RAG æˆä¸º 2023 å¹´æœç´¢æŠ€æœ¯ä¸­æœ€æµè¡Œçš„æœ¯è¯­ä¹Ÿå°±ä¸è¶³ä¸ºå¥‡äº†ã€‚æ£€ç´¢å¢å¼ºç”Ÿæˆ (RAG) æ­£åœ¨æ”¹å˜ç»„ç»‡åˆ©ç”¨å…¶å¤§é‡ç°æœ‰æ•°æ®æ¥æ¨åŠ¨æ™ºèƒ½èŠå¤©æœºå™¨äººçš„æ–¹å¼ã€‚è¿™äº›èƒ½å¤Ÿè¿›è¡Œè‡ªç„¶è¯­è¨€å¯¹è¯çš„æœºå™¨äººï¼Œå¯ä»¥åˆ©ç”¨ç»„ç»‡çš„é›†ä½“çŸ¥è¯†ï¼Œå……å½“ä¸€ä¸ªå§‹ç»ˆå¯ç”¨çš„å†…éƒ¨ä¸“å®¶ï¼Œæä¾›åŸºäºç»éªŒè¯æ•°æ®çš„ç›¸å…³ç­”æ¡ˆã€‚è™½ç„¶æœ‰å¤§é‡èµ„æºå¯ç”¨äºæ„å»º
    RAG ç³»ç»Ÿï¼Œä½†å¤§å¤šæ•°èµ„æºé’ˆå¯¹çš„æ˜¯è‹±è¯­ï¼Œè¾ƒå°è¯­è¨€çš„èµ„æºä»æœ‰ç¼ºå£ã€‚
- en: This 6-step easy-to-follow guide will walk you through the doâ€™s and donâ€™ts when
    creating RAG systems for non-English languages.
  id: totrans-18
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: è¿™ä»½æ˜“äºéµå¾ªçš„ 6 æ­¥æŒ‡å—å°†å¼•å¯¼ä½ äº†è§£åœ¨ä¸ºéè‹±è¯­è¯­è¨€åˆ›å»º RAG ç³»ç»Ÿæ—¶çš„æ³¨æ„äº‹é¡¹ã€‚
- en: RAG structure, a brief recap
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: RAG ç»“æ„ï¼Œç®€è¦å›é¡¾
- en: 'This article presumes familiarity with concepts like embeddings, vectors, and
    tokens. For those needing a brief refresher on the architecture of RAG systems,
    they essentially consist of two core components:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬æ–‡å‡è®¾è¯»è€…å¯¹åµŒå…¥ã€å‘é‡å’Œæ ‡è®°ç­‰æ¦‚å¿µæœ‰ä¸€å®šäº†è§£ã€‚å¯¹äºéœ€è¦ç®€è¦å›é¡¾ RAG ç³»ç»Ÿæ¶æ„çš„äººæ¥è¯´ï¼Œå®ƒä»¬ä¸»è¦ç”±ä¸¤ä¸ªæ ¸å¿ƒç»„ä»¶ç»„æˆï¼š
- en: 'Indexing phase (the focus of this article): This initial stage involves processing
    the input data. The data is first loaded, appropriately formatted, then split.
    Later, it undergoes vectorization through embedding techniques, culminating in
    its storage within a knowledge base for future retrieval.'
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ç´¢å¼•é˜¶æ®µï¼ˆæœ¬æ–‡çš„é‡ç‚¹ï¼‰ï¼šè¿™ä¸€åˆå§‹é˜¶æ®µæ¶‰åŠå¤„ç†è¾“å…¥æ•°æ®ã€‚æ•°æ®é¦–å…ˆè¢«åŠ è½½ã€é€‚å½“æ ¼å¼åŒ–ï¼Œç„¶åè¿›è¡Œæ‹†åˆ†ã€‚ä¹‹åï¼Œæ•°æ®é€šè¿‡åµŒå…¥æŠ€æœ¯è¿›è¡Œå‘é‡åŒ–ï¼Œæœ€ç»ˆå­˜å‚¨åœ¨çŸ¥è¯†åº“ä¸­ä»¥ä¾¿å°†æ¥æ£€ç´¢ã€‚
- en: 'Generative phase: In this phase, a userâ€™s query is input to the retrieval system.
    This system then extracts relevant information snippets from the knowledge base.
    Leveraging a Large Language Model (LLM), the system interprets this data to formulate
    a coherent, natural language response, effectively addressing the userâ€™s inquiry.'
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ç”Ÿæˆé˜¶æ®µï¼šåœ¨æ­¤é˜¶æ®µï¼Œç”¨æˆ·çš„æŸ¥è¯¢è¢«è¾“å…¥åˆ°æ£€ç´¢ç³»ç»Ÿä¸­ã€‚è¯¥ç³»ç»Ÿéšåä»çŸ¥è¯†åº“ä¸­æå–ç›¸å…³ä¿¡æ¯ç‰‡æ®µã€‚åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹ (LLM)ï¼Œç³»ç»Ÿè§£é‡Šè¿™äº›æ•°æ®ä»¥åˆ¶å®šè¿è´¯çš„è‡ªç„¶è¯­è¨€å“åº”ï¼Œæœ‰æ•ˆåœ°è§£ç­”ç”¨æˆ·çš„è¯¢é—®ã€‚
- en: '![](../Images/0e5538579e680f53cdcb9062d811cc75.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0e5538579e680f53cdcb9062d811cc75.png)'
- en: Now letâ€™s get started!
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨è®©æˆ‘ä»¬å¼€å§‹å§ï¼
- en: '*Disclaimer:*'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '*å…è´£å£°æ˜ï¼š*'
- en: '*This guide doesnâ€™t aim to be an exhaustive manual on using any particular
    tool. Instead, its purpose is to shed light on the overarching decisions that
    should guide your tool selection. In practice, I strongly recommend leveraging
    an established framework for constructing your systemâ€™s foundation. For building
    RAG systems, I would personally recommend LlamaIndex as they provide detailed
    guides and features focused strictly on indexing and retrieval optimization.*'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '*æœ¬æŒ‡å—å¹¶ä¸æ—¨åœ¨æˆä¸ºä½¿ç”¨ä»»ä½•ç‰¹å®šå·¥å…·çš„è¯¦å°½æ‰‹å†Œã€‚ç›¸åï¼Œå…¶ç›®çš„æ˜¯é˜æ˜åº”æŒ‡å¯¼å·¥å…·é€‰æ‹©çš„æ€»ä½“å†³ç­–ã€‚å®é™…ä¸Šï¼Œæˆ‘å¼ºçƒˆå»ºè®®åˆ©ç”¨å·²å»ºç«‹çš„æ¡†æ¶æ¥æ„å»ºç³»ç»ŸåŸºç¡€ã€‚å¯¹äºæ„å»º
    RAG ç³»ç»Ÿï¼Œæˆ‘ä¸ªäººæ¨è LlamaIndexï¼Œå› ä¸ºå®ƒä»¬æä¾›äº†è¯¦ç»†çš„æŒ‡å—å’Œä¸“æ³¨äºç´¢å¼•å’Œæ£€ç´¢ä¼˜åŒ–çš„åŠŸèƒ½ã€‚*'
- en: '*Additionally, this guide is written with the assumption that weâ€™re dealing
    with languages that use the latin script and read from left to right. This includes
    languages like German, French, Spanish, Czech, , Turkish, Vietnamese, Norwegian,
    Polish, and quite a few others. Languages outside of this group may have different
    needs and considerations.*'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '*æ­¤å¤–ï¼Œæœ¬æŒ‡å—å‡è®¾æˆ‘ä»¬å¤„ç†çš„æ˜¯ä½¿ç”¨æ‹‰ä¸å­—æ¯å¹¶ä»å·¦å‘å³é˜…è¯»çš„è¯­è¨€ã€‚è¿™åŒ…æ‹¬å¾·è¯­ã€æ³•è¯­ã€è¥¿ç­ç‰™è¯­ã€æ·å…‹è¯­ã€åœŸè€³å…¶è¯­ã€è¶Šå—è¯­ã€æŒªå¨è¯­ã€æ³¢å…°è¯­ä»¥åŠå…¶ä»–ä¸€äº›è¯­è¨€ã€‚å…¶ä»–è¯­è¨€å¯èƒ½æœ‰ä¸åŒçš„éœ€æ±‚å’Œè€ƒè™‘å› ç´ ã€‚*'
- en: '1\. Data loader: The devilâ€™s in the details'
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 1\. æ•°æ®åŠ è½½å™¨ï¼šå…³é”®åœ¨äºç»†èŠ‚
- en: '![](../Images/ff11a5cb9d6053f2e24509540b6b2c14.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ff11a5cb9d6053f2e24509540b6b2c14.png)'
- en: A cool looking multi-modal dataloader (Image generated by author w. Dall-E 3)
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªå¤–è§‚é…·ç‚«çš„å¤šæ¨¡æ€æ•°æ®åŠ è½½å™¨ï¼ˆå›¾åƒç”±ä½œè€…ä½¿ç”¨ Dall-E 3 ç”Ÿæˆï¼‰
- en: The first step in a RAG system involves using a dataloader to handle diverse
    formats, from text documents to multimedia, extracting all relevant content for
    further processing. For text-based formats, dataloaders typically perform consistently
    across languages, as they donâ€™t involve language-specific processing. With the
    advent of multi-modal RAG systems, it is however crucial to be aware of the reduced
    performance of speech to text models compared to their English counterparts. Models
    like [Whisper v3](https://github.com/openai/whisper) demonstrate impressive multilingual
    capabilities, but itâ€™s wise to check out their performance on benchmarks like
    [Mozilla Common Voice](https://commonvoice.mozilla.org/en/datasets) or the [Fleurs](https://huggingface.co/datasets/google/fleurs)
    dataset, and ideally evaluate those on your own benchmark.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: RAG ç³»ç»Ÿçš„ç¬¬ä¸€æ­¥æ˜¯ä½¿ç”¨æ•°æ®åŠ è½½å™¨å¤„ç†å„ç§æ ¼å¼ï¼Œä»æ–‡æœ¬æ–‡ä»¶åˆ°å¤šåª’ä½“ï¼Œæå–æ‰€æœ‰ç›¸å…³å†…å®¹ä»¥ä¾›è¿›ä¸€æ­¥å¤„ç†ã€‚å¯¹äºåŸºäºæ–‡æœ¬çš„æ ¼å¼ï¼Œæ•°æ®åŠ è½½å™¨é€šå¸¸åœ¨ä¸åŒè¯­è¨€é—´è¡¨ç°ä¸€è‡´ï¼Œå› ä¸ºå®ƒä»¬ä¸æ¶‰åŠç‰¹å®šè¯­è¨€çš„å¤„ç†ã€‚ç„¶è€Œï¼Œéšç€å¤šæ¨¡æ€
    RAG ç³»ç»Ÿçš„å‡ºç°ï¼Œäº†è§£è¯­éŸ³è½¬æ–‡æœ¬æ¨¡å‹åœ¨ä¸å…¶è‹±è¯­å¯¹åº”æ¨¡å‹ç›¸æ¯”æ€§èƒ½é™ä½çš„æƒ…å†µéå¸¸é‡è¦ã€‚åƒ[Whisper v3](https://github.com/openai/whisper)è¿™æ ·çš„æ¨¡å‹å±•ç¤ºäº†ä»¤äººå°è±¡æ·±åˆ»çš„å¤šè¯­è¨€èƒ½åŠ›ï¼Œä½†æœ€å¥½æŸ¥çœ‹å®ƒä»¬åœ¨[Mozilla
    Common Voice](https://commonvoice.mozilla.org/en/datasets)æˆ–[Fleurs](https://huggingface.co/datasets/google/fleurs)æ•°æ®é›†ä¸Šçš„è¡¨ç°ï¼Œå¹¶ä¸”æœ€å¥½åœ¨è‡ªå·±çš„åŸºå‡†ä¸Šè¿›è¡Œè¯„ä¼°ã€‚
- en: For the remainder of this article, weâ€™ll however concentrate on text-based inputs.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬æ–‡å…¶ä½™éƒ¨åˆ†å°†é›†ä¸­è®¨è®ºåŸºäºæ–‡æœ¬çš„è¾“å…¥ã€‚
- en: Why retaining syntactic structure is important
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä¿ç•™å¥æ³•ç»“æ„ä¸ºä½•é‡è¦
- en: A key aspect of data loading is to preserve the original dataâ€™s syntactic integrity.
    The loss of elements such as headers or paragraph structures can impact the accuracy
    of subsequent information retrieval. This concern is heightened for non-English
    languages due to the limited availability of machine learning-based segmentation
    tools.
  id: totrans-34
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: æ•°æ®åŠ è½½çš„ä¸€ä¸ªå…³é”®æ–¹é¢æ˜¯ä¿æŒåŸå§‹æ•°æ®çš„å¥æ³•å®Œæ•´æ€§ã€‚ä¸¢å¤±è¯¸å¦‚æ ‡é¢˜æˆ–æ®µè½ç»“æ„çš„å…ƒç´ å¯èƒ½ä¼šå½±å“åç»­ä¿¡æ¯æ£€ç´¢çš„å‡†ç¡®æ€§ã€‚å¯¹äºéè‹±è¯­è¯­è¨€ï¼Œè¿™ç§æ‹…å¿§å°¤ä¸ºçªå‡ºï¼Œå› ä¸ºåŸºäºæœºå™¨å­¦ä¹ çš„åˆ†æ®µå·¥å…·çš„å¯ç”¨æ€§æœ‰é™ã€‚
- en: Syntactic information plays a crucial role because the effectiveness of RAG
    systems in delivering meaningful answers depends partly on their ability to split
    data into semantically accurate subsections.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: å¥æ³•ä¿¡æ¯å‘æŒ¥ç€è‡³å…³é‡è¦çš„ä½œç”¨ï¼Œå› ä¸º RAG ç³»ç»Ÿåœ¨æä¾›æœ‰æ„ä¹‰ç­”æ¡ˆçš„æ•ˆæœéƒ¨åˆ†å–å†³äºå®ƒä»¬å°†æ•°æ®æ‹†åˆ†ä¸ºè¯­ä¹‰å‡†ç¡®çš„å­éƒ¨åˆ†çš„èƒ½åŠ›ã€‚
- en: To highlight the differences between a data loading approach that retains the
    structure and one that does not, letâ€™s take the example of using a basic HTML
    dataloader versus a PDF loader on a [medium article](https://blog.llamaindex.ai/boosting-rag-picking-the-best-embedding-reranker-models-42d079022e83).
    Libraries such as [LangChain](https://python.langchain.com/docs/modules/data_connection/document_loaders/html)
    and [LlamaIndex](https://llamahub.ai/l/web-beautiful_soup_web) both rely on the
    exact same libraries, but just wrap the functions in their own document classes
    (Requests+BS4 for web, PyPDF2 for PDFs).
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†çªå‡ºä¿ç•™ç»“æ„çš„æ•°æ®åŠ è½½æ–¹æ³•ä¸ä¸ä¿ç•™ç»“æ„çš„æ–¹æ³•ä¹‹é—´çš„åŒºåˆ«ï¼Œä¸¾ä¸€ä¸ªä½¿ç”¨åŸºç¡€ HTML æ•°æ®åŠ è½½å™¨ä¸ PDF åŠ è½½å™¨å¯¹[medium article](https://blog.llamaindex.ai/boosting-rag-picking-the-best-embedding-reranker-models-42d079022e83)çš„ä¾‹å­ã€‚åƒ[LangChain](https://python.langchain.com/docs/modules/data_connection/document_loaders/html)å’Œ[LlamaIndex](https://llamahub.ai/l/web-beautiful_soup_web)è¿™æ ·çš„åº“éƒ½ä¾èµ–äºå®Œå…¨ç›¸åŒçš„åº“ï¼Œä½†åªæ˜¯å°†å‡½æ•°å°è£…åœ¨å„è‡ªçš„æ–‡æ¡£ç±»ä¸­ï¼ˆWeb
    ç”¨ Requests+BS4ï¼ŒPDF ç”¨ PyPDF2ï¼‰ã€‚
- en: '**HTML Dataloader: This method retains the syntactic structure of the content.**'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '**HTML æ•°æ®åŠ è½½å™¨ï¼šæ­¤æ–¹æ³•ä¿ç•™äº†å†…å®¹çš„å¥æ³•ç»“æ„ã€‚**'
- en: '[PRE0]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '[PRE1]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '**PDF data loader, example in which syntactic information is lost** (saved
    article as PDF, then re-loaded)'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '**PDF æ•°æ®åŠ è½½å™¨ï¼Œå¥æ³•ä¿¡æ¯ä¸¢å¤±çš„ç¤ºä¾‹**ï¼ˆå°†æ–‡ç« ä¿å­˜ä¸º PDF åé‡æ–°åŠ è½½ï¼‰'
- en: '[PRE2]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[PRE3]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Upon initial review, the PDF dataloaderâ€™s output appears more readable, but
    closer inspection reveals a loss of structural information â€” how would one tell
    what is a header, and where a section ends? In contrast, the HTML file retains
    all the relevant structure.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: åˆæ­¥æ£€æŸ¥æ˜¾ç¤ºï¼ŒPDF æ•°æ®åŠ è½½å™¨çš„è¾“å‡ºçœ‹èµ·æ¥æ›´å¯è¯»ï¼Œä½†ä»”ç»†æ£€æŸ¥åå‘ç°ä¸¢å¤±äº†ç»“æ„ä¿¡æ¯â€”â€”å¦‚ä½•åŒºåˆ†æ ‡é¢˜å’ŒèŠ‚çš„ç»“æŸï¼Ÿç›¸æ¯”ä¹‹ä¸‹ï¼ŒHTML æ–‡ä»¶ä¿ç•™äº†æ‰€æœ‰ç›¸å…³çš„ç»“æ„ã€‚
- en: Ideally, you want to retain all original formatting in the data loader, and
    only decide on filtering and reformatting in the next step. However, that might
    involve building custom data loaders for your use case, and in some cases be impossible.
    I recommend to simply start with a standard data loader, but spend a few minutes
    to inspect examples of the loaded data carefully and understand what structure
    has been lost.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: ç†æƒ³æƒ…å†µä¸‹ï¼Œä½ å¸Œæœ›åœ¨æ•°æ®åŠ è½½å™¨ä¸­ä¿ç•™æ‰€æœ‰åŸå§‹æ ¼å¼ï¼Œå¹¶ä¸”ä»…åœ¨ä¸‹ä¸€æ­¥å†³å®šè¿‡æ»¤å’Œé‡æ–°æ ¼å¼åŒ–ã€‚ç„¶è€Œï¼Œè¿™å¯èƒ½æ¶‰åŠä¸ºä½ çš„ä½¿ç”¨æ¡ˆä¾‹æ„å»ºè‡ªå®šä¹‰æ•°æ®åŠ è½½å™¨ï¼Œå¹¶ä¸”åœ¨æŸäº›æƒ…å†µä¸‹å¯èƒ½æ˜¯ä¸å¯èƒ½çš„ã€‚æˆ‘å»ºè®®ä½ ä»æ ‡å‡†æ•°æ®åŠ è½½å™¨å¼€å§‹ï¼Œä½†èŠ±å‡ åˆ†é’Ÿä»”ç»†æ£€æŸ¥åŠ è½½çš„æ•°æ®ç¤ºä¾‹ï¼Œå¹¶äº†è§£ä¸¢å¤±äº†å“ªäº›ç»“æ„ã€‚
- en: Understanding what syntactic that is lost is crucial, as it guides potential
    improvements if the systemâ€™s downstream retrieval performance needs enhancement,
    allowing for targeted refinements.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: äº†è§£ä¸¢å¤±çš„è¯­æ³•ç»“æ„æ˜¯è‡³å…³é‡è¦çš„ï¼Œå› ä¸ºå®ƒæŒ‡å¯¼äº†ç³»ç»Ÿä¸‹æ¸¸æ£€ç´¢æ€§èƒ½éœ€è¦æ”¹è¿›çš„æ½œåœ¨æ–¹å‘ï¼Œå…è®¸è¿›è¡Œæœ‰é’ˆå¯¹æ€§çš„ä¼˜åŒ–ã€‚
- en: '2\. Data formatting: Boringâ€¦ but important'
  id: totrans-46
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 2. æ•°æ®æ ¼å¼åŒ–ï¼šæ— èŠâ€¦â€¦ä½†é‡è¦
- en: '![](../Images/82a92d20f1ab6a15b7c2408f5cb9668f.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/82a92d20f1ab6a15b7c2408f5cb9668f.png)'
- en: Document chunking (Image generated by author w. Dall-E 3)
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: æ–‡æ¡£åˆ†å—ï¼ˆå›¾åƒç”±ä½œè€…ä½¿ç”¨ Dall-E 3 ç”Ÿæˆï¼‰
- en: The second step, formatting, serves the primary purpose of uniforming the data
    from your data loaders in a way that prepares the data for the next step of text
    splitting. As the following section explains, dividing the input text into a myriad
    of smaller chunks will be necessary. A successful formatting sets up the text
    in a way that provides the best possible conditions for dividing the content into
    semantically meaningful chunks. Simply put, your goal is to transform the potentially
    complex syntactic structure retrieved from a html or a markdown file, into a plain
    text file with basic delimiters such as /n (line change) and /n/n (end of section)
    to guide the text splitter.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: ç¬¬äºŒæ­¥ï¼Œæ ¼å¼åŒ–ï¼Œå…¶ä¸»è¦ç›®çš„æ˜¯ä»¥ç»Ÿä¸€çš„æ–¹å¼æ•´ç†æ¥è‡ªæ•°æ®åŠ è½½å™¨çš„æ•°æ®ï¼Œä»¥ä¾¿ä¸ºä¸‹ä¸€æ­¥çš„æ–‡æœ¬æ‹†åˆ†åšå‡†å¤‡ã€‚å¦‚ä»¥ä¸‹ç« èŠ‚æ‰€è¿°ï¼Œå°†è¾“å…¥æ–‡æœ¬åˆ’åˆ†ä¸ºæ— æ•°è¾ƒå°çš„å—æ˜¯å¿…è¦çš„ã€‚æˆåŠŸçš„æ ¼å¼åŒ–å°†æ–‡æœ¬è®¾ç½®æˆæä¾›æœ€ä½³æ¡ä»¶ä»¥å°†å†…å®¹åˆ’åˆ†ä¸ºè¯­ä¹‰ä¸Šæœ‰æ„ä¹‰çš„å—ã€‚ç®€å•æ¥è¯´ï¼Œä½ çš„ç›®æ ‡æ˜¯å°†ä»
    html æˆ– markdown æ–‡ä»¶ä¸­æ£€ç´¢åˆ°çš„æ½œåœ¨å¤æ‚è¯­æ³•ç»“æ„è½¬æ¢ä¸ºå¸¦æœ‰åŸºæœ¬åˆ†éš”ç¬¦çš„çº¯æ–‡æœ¬æ–‡ä»¶ï¼Œå¦‚ /nï¼ˆæ¢è¡Œï¼‰å’Œ /n/nï¼ˆèŠ‚ç»“æŸï¼‰ï¼Œä»¥æŒ‡å¯¼æ–‡æœ¬æ‹†åˆ†å™¨ã€‚
- en: 'A simple function to format the BS4 HTML object into a dictionary with title
    and text could look like the below:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªç®€å•çš„å‡½æ•°å°† BS4 HTML å¯¹è±¡æ ¼å¼åŒ–ä¸ºåŒ…å«æ ‡é¢˜å’Œæ–‡æœ¬çš„å­—å…¸ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š
- en: '[PRE4]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[PRE5]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: For complex RAG systems where there might be multiple correct answers relative
    to the context, storing additional information like document titles or headers
    as metadata along the text chunks is beneficial. This metadata can be used later
    for filtering, and if available, formatting elements like headers should influence
    your chunking strategy. A library like LlamaIndex natively work with the concept
    of metadata and text wrapped together in Node objects, and I highly recommend
    using this or a similar framework
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºå¤æ‚çš„ RAG ç³»ç»Ÿï¼Œå…¶ä¸­ç›¸å¯¹äºä¸Šä¸‹æ–‡å¯èƒ½æœ‰å¤šä¸ªæ­£ç¡®ç­”æ¡ˆï¼Œå°†æ–‡æ¡£æ ‡é¢˜æˆ–æ ‡é¢˜ç­‰é™„åŠ ä¿¡æ¯å­˜å‚¨ä¸ºæ–‡æœ¬å—çš„å…ƒæ•°æ®æ˜¯æœ‰ç›Šçš„ã€‚è¿™äº›å…ƒæ•°æ®å¯ä»¥åœ¨ä¹‹åç”¨äºè¿‡æ»¤ï¼Œå¦‚æœå¯ç”¨ï¼Œæ ¼å¼åŒ–å…ƒç´ å¦‚æ ‡é¢˜åº”å½±å“ä½ çš„åˆ†å—ç­–ç•¥ã€‚åƒ
    LlamaIndex è¿™æ ·çš„åº“æœ¬åœ°å¤„ç†ä¸å…ƒæ•°æ®å’Œæ–‡æœ¬ä¸€èµ·å°è£…åœ¨ Node å¯¹è±¡ä¸­çš„æ¦‚å¿µï¼Œæˆ‘å¼ºçƒˆæ¨èä½¿ç”¨è¿™ä¸ªæˆ–ç±»ä¼¼çš„æ¡†æ¶ã€‚
- en: Now that weâ€™ve done our formatting correctly, letâ€™s dive into the key aspects
    of text splitting!
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬å·²ç»æ­£ç¡®åœ°å®Œæˆäº†æ ¼å¼åŒ–ï¼Œè®©æˆ‘ä»¬æ·±å…¥æ¢è®¨æ–‡æœ¬æ‹†åˆ†çš„å…³é”®æ–¹é¢å§ï¼
- en: '3: Text splitting: Size matters'
  id: totrans-55
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '3: æ–‡æœ¬æ‹†åˆ†ï¼šå¤§å°é‡è¦'
- en: '![](../Images/a4f4db130ed93d7637ff014bd449b7a6.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a4f4db130ed93d7637ff014bd449b7a6.png)'
- en: Splitting text, the simple way (Image generated by author w. Dall-E 3)
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: æ‹†åˆ†æ–‡æœ¬ï¼Œç®€å•çš„æ–¹æ³•ï¼ˆå›¾åƒç”±ä½œè€…ä½¿ç”¨ Dall-E 3 ç”Ÿæˆï¼‰
- en: When preparing data for embedding and retrieval in a RAG system, splitting the
    text into appropriately sized chunks is crucial. This process is guided by two
    main factors, Model Constraints and Retrieval Effectiveness.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä¸º RAG ç³»ç»Ÿå‡†å¤‡æ•°æ®ä»¥è¿›è¡ŒåµŒå…¥å’Œæ£€ç´¢æ—¶ï¼Œå°†æ–‡æœ¬æ‹†åˆ†ä¸ºé€‚å½“å¤§å°çš„å—æ˜¯è‡³å…³é‡è¦çš„ã€‚è¿™ä¸ªè¿‡ç¨‹å—ä¸¤ä¸ªä¸»è¦å› ç´ çš„æŒ‡å¯¼ï¼šæ¨¡å‹çº¦æŸå’Œæ£€ç´¢æœ‰æ•ˆæ€§ã€‚
- en: '**Model Constraints**'
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**æ¨¡å‹çº¦æŸ**'
- en: Embedding models have a maximum token length for input; anything beyond this
    limit gets truncated. Be aware of your chosen modelâ€™s limitations and ensure that
    each data chunk doesnâ€™t exceed this max token length.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: åµŒå…¥æ¨¡å‹å¯¹è¾“å…¥çš„æœ€å¤§ token é•¿åº¦æœ‰ä¸€ä¸ªé™åˆ¶ï¼›è¶…å‡ºæ­¤é™åˆ¶çš„å†…å®¹ä¼šè¢«æˆªæ–­ã€‚äº†è§£æ‰€é€‰æ‹©æ¨¡å‹çš„é™åˆ¶ï¼Œå¹¶ç¡®ä¿æ¯ä¸ªæ•°æ®å—ä¸è¶…è¿‡æ­¤æœ€å¤§ token é•¿åº¦ã€‚
- en: Multilingual models, in particular, often have shorter sequence limits compared
    to their English counterparts. For instance, the widely used Paraphrase multilingual
    MiniLM-L12 v2 model has a maximum context window of just 128 tokens.
  id: totrans-61
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: å¤šè¯­è¨€æ¨¡å‹ï¼Œç‰¹åˆ«æ˜¯ï¼Œä¸å…¶è‹±æ–‡å¯¹åº”æ¨¡å‹ç›¸æ¯”ï¼Œé€šå¸¸å…·æœ‰è¾ƒçŸ­çš„åºåˆ—é™åˆ¶ã€‚ä¾‹å¦‚ï¼Œå¹¿æ³›ä½¿ç”¨çš„ Paraphrase multilingual MiniLM-L12
    v2 æ¨¡å‹çš„æœ€å¤§ä¸Šä¸‹æ–‡çª—å£ä»…ä¸º 128 ä¸ª tokenã€‚
- en: Also, consider the text length the model was trained on â€” some models might
    technically accept longer inputs but were trained on shorter chunks, which could
    affect performance on longer texts. One such is example, is the [Multi QA base
    from SBERT](https://huggingface.co/sentence-transformers/multi-qa-mpnet-base-dot-v1)
    as seen below,
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤å¤–ï¼Œè¿˜è¦è€ƒè™‘æ¨¡å‹çš„è®­ç»ƒæ–‡æœ¬é•¿åº¦â€”â€”ä¸€äº›æ¨¡å‹è™½ç„¶åœ¨æŠ€æœ¯ä¸Šå¯ä»¥æ¥å—æ›´é•¿çš„è¾“å…¥ï¼Œä½†å…¶è®­ç»ƒæ•°æ®å´è¾ƒçŸ­ï¼Œè¿™å¯èƒ½ä¼šå½±å“å¯¹è¾ƒé•¿æ–‡æœ¬çš„æ€§èƒ½ã€‚ä¾‹å¦‚ï¼Œ[SBERT çš„ Multi
    QA åŸºç¡€æ¨¡å‹](https://huggingface.co/sentence-transformers/multi-qa-mpnet-base-dot-v1)
    å¦‚ä¸‹æ‰€ç¤ºï¼Œ
- en: '![](../Images/e0d8262a6f1b3b939051aba7f8901fbb.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e0d8262a6f1b3b939051aba7f8901fbb.png)'
- en: '**Retrieval effectiveness**'
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**æ£€ç´¢æ•ˆæœ**'
- en: While chunking data to the modelâ€™s maximum length seems logical, it might not
    always lead to the best retrieval outcomes. Larger chunks offer more context for
    the LLM but can obscure key details, making it harder to retrieve precise matches.
    Conversely, smaller chunks can enhance match accuracy but might lack the context
    needed for complete answers. Hybrid approaches use smaller chunks for search but
    include surrounding context at query time for balance.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: è™½ç„¶å°†æ•°æ®æ‹†åˆ†åˆ°æ¨¡å‹çš„æœ€å¤§é•¿åº¦ä¼¼ä¹æ˜¯åˆç†çš„ï¼Œä½†è¿™å¯èƒ½å¹¶ä¸æ€»æ˜¯èƒ½å¸¦æ¥æœ€ä½³çš„æ£€ç´¢ç»“æœã€‚è¾ƒå¤§çš„å—ä¸º LLM æä¾›äº†æ›´å¤šçš„ä¸Šä¸‹æ–‡ï¼Œä½†å¯èƒ½ä¼šæ©ç›–å…³é”®ç»†èŠ‚ï¼Œä½¿å¾—ç²¾ç¡®åŒ¹é…æ›´åŠ å›°éš¾ã€‚ç›¸åï¼Œè¾ƒå°çš„å—å¯ä»¥æé«˜åŒ¹é…å‡†ç¡®æ€§ï¼Œä½†å¯èƒ½ç¼ºä¹è·å–å®Œæ•´ç­”æ¡ˆæ‰€éœ€çš„ä¸Šä¸‹æ–‡ã€‚æ··åˆæ–¹æ³•ä½¿ç”¨è¾ƒå°çš„å—è¿›è¡Œæœç´¢ï¼Œä½†åœ¨æŸ¥è¯¢æ—¶åŒ…æ‹¬å‘¨å›´çš„ä¸Šä¸‹æ–‡ä»¥ä¿æŒå¹³è¡¡ã€‚
- en: While there isnâ€™t a definitive answer regarding chunk size, the considerations
    for chunk size remain consistent whether youâ€™re working on multilingual or English
    projects. I would recommend reading further on the topic from resources such as
    [Evaluating the Ideal Chunk Size for RAG System using Llamaindex](https://blog.llamaindex.ai/evaluating-the-ideal-chunk-size-for-a-rag-system-using-llamaindex-6207e5d3fec5)
    or [Building RAG-based LLM Applications for Production](https://www.anyscale.com/blog/a-comprehensive-guide-for-building-rag-based-llm-applications-part-1?utm_source=gradientflow&utm_medium=newsletter#chunk-data).
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: å°½ç®¡å…³äºå—å¤§å°æ²¡æœ‰ç¡®åˆ‡çš„ç­”æ¡ˆï¼Œä½†å—å¤§å°çš„è€ƒè™‘åœ¨å¤šè¯­è¨€é¡¹ç›®å’Œè‹±è¯­é¡¹ç›®ä¸­æ˜¯ä¸€è‡´çš„ã€‚æˆ‘å»ºè®®è¿›ä¸€æ­¥é˜…è¯»ç›¸å…³èµ„æºï¼Œå¦‚ [ä½¿ç”¨ Llamaindex è¯„ä¼° RAG
    ç³»ç»Ÿçš„ç†æƒ³å—å¤§å°](https://blog.llamaindex.ai/evaluating-the-ideal-chunk-size-for-a-rag-system-using-llamaindex-6207e5d3fec5)
    æˆ– [ä¸ºç”Ÿäº§ç¯å¢ƒæ„å»ºåŸºäº RAG çš„ LLM åº”ç”¨ç¨‹åº](https://www.anyscale.com/blog/a-comprehensive-guide-for-building-rag-based-llm-applications-part-1?utm_source=gradientflow&utm_medium=newsletter#chunk-data)ã€‚
- en: '**Text splitting: Methods for splitting text**'
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**æ–‡æœ¬æ‹†åˆ†ï¼šæ–‡æœ¬æ‹†åˆ†çš„æ–¹æ³•**'
- en: '[Text can be split using various methods](/how-to-chunk-text-data-a-comparative-analysis-3858c4a0997a),
    mainly falling into two categories: rule-based (focusing on character analysis)
    and machine learning-based models. ML approaches, from simple NLTK & Spacy tokenizers
    to advanced transformer models, often depend on language-specific training, primarily
    in English. Although simple models like NLTK & Spacy support multiple languages,
    they mainly address sentence splitting, not semantic sectioning.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '[æ–‡æœ¬å¯ä»¥é€šè¿‡å„ç§æ–¹æ³•è¿›è¡Œæ‹†åˆ†](/how-to-chunk-text-data-a-comparative-analysis-3858c4a0997a)ï¼Œä¸»è¦åˆ†ä¸ºä¸¤ç±»ï¼šåŸºäºè§„åˆ™çš„ï¼ˆæ³¨é‡å­—ç¬¦åˆ†æï¼‰å’ŒåŸºäºæœºå™¨å­¦ä¹ çš„æ¨¡å‹ã€‚æœºå™¨å­¦ä¹ æ–¹æ³•ï¼Œä»ç®€å•çš„
    NLTK å’Œ Spacy åˆ†è¯å™¨åˆ°å…ˆè¿›çš„ transformer æ¨¡å‹ï¼Œé€šå¸¸ä¾èµ–äºè¯­è¨€ç‰¹å®šçš„è®­ç»ƒï¼Œä¸»è¦æ˜¯è‹±è¯­ã€‚å°½ç®¡åƒ NLTK å’Œ Spacy è¿™æ ·çš„ç®€å•æ¨¡å‹æ”¯æŒå¤šç§è¯­è¨€ï¼Œä½†å®ƒä»¬ä¸»è¦å¤„ç†å¥å­æ‹†åˆ†ï¼Œè€Œéè¯­ä¹‰åˆ’åˆ†ã€‚'
- en: '*Since ML based sentence splitters currently work poorly for most non-English
    languages, and are compute intensive, I recommend starting with a simple rule-based
    splitter. If youâ€™ve preserved relevant syntactic structure from the original data,
    and formatted the data correctly, the result will be of good quality.*'
  id: totrans-69
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*ç”±äºåŸºäºæœºå™¨å­¦ä¹ çš„å¥å­æ‹†åˆ†å™¨ç›®å‰åœ¨å¤§å¤šæ•°éè‹±è¯­è¯­è¨€ä¸­æ•ˆæœä¸ä½³ä¸”è®¡ç®—å¯†é›†ï¼Œæˆ‘å»ºè®®ä»ç®€å•çš„åŸºäºè§„åˆ™çš„æ‹†åˆ†å™¨å¼€å§‹ã€‚å¦‚æœä½ ä¿ç•™äº†åŸå§‹æ•°æ®çš„ç›¸å…³å¥æ³•ç»“æ„ï¼Œå¹¶æ­£ç¡®åœ°æ ¼å¼åŒ–äº†æ•°æ®ï¼Œç»“æœå°†ä¼šè´¨é‡è‰¯å¥½ã€‚*'
- en: A common and effective method is a recursive character text splitter, like those
    used in [LangChain](https://python.langchain.com/docs/modules/data_connection/document_transformers/text_splitters/recursive_text_splitter)
    or LlamaIndex, which shortens sections by finding the nearest split character
    in a prioritized sequence (e.g., \n\n, \n, ., ?, !).
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ç§å¸¸è§è€Œæœ‰æ•ˆçš„æ–¹æ³•æ˜¯é€’å½’å­—ç¬¦æ–‡æœ¬åˆ†å‰²å™¨ï¼Œä¾‹å¦‚åœ¨ [LangChain](https://python.langchain.com/docs/modules/data_connection/document_transformers/text_splitters/recursive_text_splitter)
    æˆ– LlamaIndex ä¸­ä½¿ç”¨çš„ï¼Œå®ƒé€šè¿‡åœ¨ä¼˜å…ˆåºåˆ—ä¸­æ‰¾åˆ°æœ€è¿‘çš„åˆ†éš”å­—ç¬¦ï¼ˆä¾‹å¦‚ \n\n, \n, ., ?, !ï¼‰æ¥ç¼©çŸ­æ®µè½ã€‚
- en: 'Taking the formatted text from the previous section, an example of using LangChains
    recursive character splitter would look like:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨å‰ä¸€éƒ¨åˆ†æ ¼å¼åŒ–æ–‡æœ¬çš„ç¤ºä¾‹ï¼Œä½¿ç”¨ LangChain çš„é€’å½’å­—ç¬¦åˆ†å‰²å™¨å¦‚ä¸‹æ‰€ç¤ºï¼š
- en: '[PRE6]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Here itâ€™s important to note that one should define the tokenizer as the embedding
    model intended to use, since different models â€˜countâ€™ the words differently. The
    function will now, in a prioritized order, split any text longer than 128 tokens
    first by the \n\n we introduced at end of sections, and if that is not possible,
    then by end of paragraphs delimited by \n and so forth. The first 3 chunks will
    be:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™é‡Œï¼Œéœ€è¦æ³¨æ„çš„æ˜¯ï¼Œåº”è¯¥å°†åˆ†è¯å™¨å®šä¹‰ä¸ºæ‹Ÿä½¿ç”¨çš„åµŒå…¥æ¨¡å‹ï¼Œå› ä¸ºä¸åŒæ¨¡å‹å¯¹è¯æ±‡çš„è®¡æ•°æ–¹å¼ä¸åŒã€‚å‡½æ•°ç°åœ¨å°†æŒ‰ç…§ä¼˜å…ˆé¡ºåºï¼Œé¦–å…ˆé€šè¿‡æˆ‘ä»¬åœ¨æ®µè½æœ«å°¾å¼•å…¥çš„ \n\n
    æ‹†åˆ†ä»»ä½•è¶…è¿‡128ä¸ªæ ‡è®°çš„æ–‡æœ¬ï¼Œå¦‚æœä¸å¯èƒ½ï¼Œåˆ™é€šè¿‡ \n åˆ†éš”çš„æ®µè½æœ«å°¾ï¼Œä¾æ­¤ç±»æ¨ã€‚å‰ä¸‰ä¸ªå—å°†æ˜¯ï¼š
- en: '[PRE7]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Now that we have successfully split the text in a semantically meaningful way,
    we can move onto the final part of embedding these chunks for storage.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬å·²ç»æˆåŠŸåœ°ä»¥è¯­ä¹‰ä¸Šæœ‰æ„ä¹‰çš„æ–¹å¼æ‹†åˆ†äº†æ–‡æœ¬ï¼Œå¯ä»¥è¿›å…¥æœ€ç»ˆé˜¶æ®µï¼Œå³å°†è¿™äº›å—åµŒå…¥ä»¥ä¾¿å­˜å‚¨ã€‚
- en: '**4\. Embedding Models: Navigating the jungle**'
  id: totrans-76
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**4\. åµŒå…¥æ¨¡å‹ï¼šåœ¨ä¸›æ—ä¸­å¯¼èˆª**'
- en: '![](../Images/15ebcd61fd4e03d1657cceb920ad2483.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/15ebcd61fd4e03d1657cceb920ad2483.png)'
- en: Embedding models convert text to vectors (Image generated by author w. Dall-E
    3)
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: åµŒå…¥æ¨¡å‹å°†æ–‡æœ¬è½¬æ¢ä¸ºå‘é‡ï¼ˆå›¾ç‰‡ç”±ä½œè€…ä½¿ç”¨ Dall-E 3 ç”Ÿæˆï¼‰
- en: Choosing the right embedding model is critical for the success of a Retrieval
    Augmented Generation (RAG) system, and something that is less straight forward
    than for the English language. A comprehensive resource for comparing models is
    the [Massive Text Embedding Benchmark (MTEB)](https://huggingface.co/spaces/mteb/leaderboard),
    which includes benchmarks for over 100 languages.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: é€‰æ‹©æ­£ç¡®çš„åµŒå…¥æ¨¡å‹å¯¹äºæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ç³»ç»Ÿçš„æˆåŠŸè‡³å…³é‡è¦ï¼Œè¿™æ¯”è‹±è¯­è¯­è¨€çš„æƒ…å†µå¤æ‚å¾—å¤šã€‚æ¯”è¾ƒæ¨¡å‹çš„ä¸€ä¸ªå…¨é¢èµ„æºæ˜¯ [Massive Text Embedding
    Benchmark (MTEB)](https://huggingface.co/spaces/mteb/leaderboard)ï¼Œå…¶ä¸­åŒ…å«è¶…è¿‡100ç§è¯­è¨€çš„åŸºå‡†ã€‚
- en: The model of your choice must either be multilingual or specifically tailored
    to the language youâ€™re working with (monolingual). Remember, the latest high-performing
    models are often English-centric and may not work well with other languages.
  id: totrans-80
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ä½ é€‰æ‹©çš„æ¨¡å‹å¿…é¡»æ˜¯å¤šè¯­è¨€çš„ï¼Œæˆ–ä¸“é—¨é’ˆå¯¹ä½ æ­£åœ¨ä½¿ç”¨çš„è¯­è¨€ï¼ˆå•è¯­è¨€ï¼‰å®šåˆ¶çš„ã€‚è¯·è®°ä½ï¼Œæœ€æ–°çš„é«˜æ€§èƒ½æ¨¡å‹é€šå¸¸ä»¥è‹±è¯­ä¸ºä¸­å¿ƒï¼Œå¯èƒ½ä¸é€‚ç”¨äºå…¶ä»–è¯­è¨€ã€‚
- en: '![](../Images/35f837806d09da41b4fdced9b6bc0438.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/35f837806d09da41b4fdced9b6bc0438.png)'
- en: If available, refer to language-specific benchmarks relevant to your task. For
    instance, in classification tasks, there are over 50 language-specific benchmarks,
    aiding in selecting the most efficient model for languages ranging from Danish
    to Spanish. However, itâ€™s important to note that these benchmarks may not directly
    indicate a modelâ€™s efficiency in retrieving relevant information for a RAG system,
    because retrieval is different from classification, clustering or another task.
    The task is to find models trained for asymmetric search, as those not trained
    for this specific task might inaccurately prioritize shorter passages over longer,
    more relevant ones.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæœ‰ç›¸å…³èµ„æºï¼Œè¯·å‚è€ƒä¸ä½ çš„ä»»åŠ¡ç›¸å…³çš„è¯­è¨€ç‰¹å®šåŸºå‡†ã€‚ä¾‹å¦‚ï¼Œåœ¨åˆ†ç±»ä»»åŠ¡ä¸­ï¼Œæœ‰è¶…è¿‡50ä¸ªè¯­è¨€ç‰¹å®šçš„åŸºå‡†ï¼Œå¸®åŠ©é€‰æ‹©æœ€æœ‰æ•ˆçš„æ¨¡å‹ï¼Œé€‚ç”¨äºä»ä¸¹éº¦è¯­åˆ°è¥¿ç­ç‰™è¯­çš„è¯­è¨€ã€‚ç„¶è€Œï¼Œé‡è¦çš„æ˜¯è¦æ³¨æ„ï¼Œè¿™äº›åŸºå‡†å¯èƒ½ä¸ä¼šç›´æ¥æŒ‡ç¤ºæ¨¡å‹åœ¨RAGç³»ç»Ÿä¸­æ£€ç´¢ç›¸å…³ä¿¡æ¯çš„æ•ˆç‡ï¼Œå› ä¸ºæ£€ç´¢ä¸åˆ†ç±»ã€èšç±»æˆ–å…¶ä»–ä»»åŠ¡ä¸åŒã€‚ä»»åŠ¡æ˜¯æ‰¾åˆ°è®­ç»ƒç”¨äºä¸å¯¹ç§°æœç´¢çš„æ¨¡å‹ï¼Œå› ä¸ºé‚£äº›æ²¡æœ‰é’ˆå¯¹è¿™ä¸€ç‰¹å®šä»»åŠ¡è®­ç»ƒçš„æ¨¡å‹å¯èƒ½ä¼šä¸å‡†ç¡®åœ°ä¼˜å…ˆè€ƒè™‘è¾ƒçŸ­çš„æ®µè½è€Œéè¾ƒé•¿ä¸”æ›´ç›¸å…³çš„æ®µè½ã€‚
- en: The model should excel in [asymmetric retrieval](https://www.sbert.net/examples/applications/semantic-search/README.html),
    matching short queries to longer text chunks. The reason why is that, in a RAG
    system, you often match a brief query to more extensive passages to extract meaningful
    answers. The MTEB benchmarks related to asymmetric search are listed under the
    Retrieval. A challenge is that as of November 2023, MTEBâ€™s Retrieval benchmark
    includes only English, Chinese, and Polish.
  id: totrans-83
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: è¯¥æ¨¡å‹åº”åœ¨[éå¯¹ç§°æ£€ç´¢](https://www.sbert.net/examples/applications/semantic-search/README.html)ä¸­è¡¨ç°å‡ºè‰²ï¼Œå°†çŸ­æŸ¥è¯¢åŒ¹é…åˆ°è¾ƒé•¿çš„æ–‡æœ¬å—ã€‚åŸå› åœ¨äºï¼Œåœ¨RAGç³»ç»Ÿä¸­ï¼Œä½ é€šå¸¸éœ€è¦å°†ç®€çŸ­çš„æŸ¥è¯¢åŒ¹é…åˆ°æ›´é•¿çš„æ®µè½ä¸­ä»¥æå–æœ‰æ„ä¹‰çš„ç­”æ¡ˆã€‚ä¸éå¯¹ç§°æ£€ç´¢ç›¸å…³çš„MTEBåŸºå‡†åˆ—åœ¨æ£€ç´¢éƒ¨åˆ†ã€‚ä¸€ä¸ªæŒ‘æˆ˜æ˜¯æˆªè‡³2023å¹´11æœˆï¼ŒMTEBçš„æ£€ç´¢åŸºå‡†ä»…åŒ…æ‹¬è‹±è¯­ã€ä¸­æ–‡å’Œæ³¢å…°è¯­ã€‚
- en: When dealing with languages like Norwegian, where there may not be specific
    retrieval benchmarks, you might wonder whether to choose the best-performing model
    from classification benchmarks or a general multilingual model proficient in English
    retrieval?
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å¤„ç†åƒæŒªå¨è¯­è¿™æ ·çš„è¯­è¨€æ—¶ï¼Œå¯èƒ½æ²¡æœ‰ç‰¹å®šçš„æ£€ç´¢åŸºå‡†ï¼Œä½ å¯èƒ½ä¼šæƒ³çŸ¥é“æ˜¯å¦åº”è¯¥é€‰æ‹©åˆ†ç±»åŸºå‡†ä¸­è¡¨ç°æœ€å¥½çš„æ¨¡å‹ï¼Œè¿˜æ˜¯é€‰æ‹©ä¸€ä¸ªåœ¨è‹±è¯­æ£€ç´¢æ–¹é¢è¡¨ç°å‡ºè‰²çš„é€šç”¨å¤šè¯­è¨€æ¨¡å‹ï¼Ÿ
- en: As for practical advice, a simple rule of thumb is to opt for the top-performing
    multilingual model in the MTEB Retrieval benchmark. Beware that the retrieval
    score itself, is however still based on English, so benchmarking on your own language
    is needed to qualify the performance (step 6). As of December 2023, the E5-multilingual
    family is a strong choice for an open source model. The model is fine-tuned for
    asymmetric search, and by tagging texts as â€˜queryâ€™ or â€˜passageâ€™ before embedding,
    it optimizes the retrieval process by considering the nature of the input. This
    approach ensures a more effective match between queries and relevant information
    in your knowledge base, enhancing the overall performance of your RAG system.
    As seen on the benchmark, the cohere-embed-multilingual-v3.0 likely has better
    performance, but has to be paid for.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºå®é™…å»ºè®®ï¼Œç®€å•çš„ç»éªŒæ³•åˆ™æ˜¯é€‰æ‹©MTEBæ£€ç´¢åŸºå‡†ä¸­è¡¨ç°æœ€å¥½çš„å¤šè¯­è¨€æ¨¡å‹ã€‚æ³¨æ„ï¼Œæ£€ç´¢è¯„åˆ†æœ¬èº«ä»ç„¶åŸºäºè‹±è¯­ï¼Œå› æ­¤éœ€è¦åœ¨ä½ è‡ªå·±çš„è¯­è¨€ä¸Šè¿›è¡ŒåŸºå‡†æµ‹è¯•ä»¥éªŒè¯æ€§èƒ½ï¼ˆç¬¬6æ­¥ï¼‰ã€‚æˆªè‡³2023å¹´12æœˆï¼ŒE5-å¤šè¯­è¨€ç³»åˆ—æ˜¯å¼€æºæ¨¡å‹çš„ä¸€ä¸ªå¼ºæœ‰åŠ›çš„é€‰æ‹©ã€‚è¯¥æ¨¡å‹ç»è¿‡é’ˆå¯¹éå¯¹ç§°æ£€ç´¢çš„å¾®è°ƒï¼Œé€šè¿‡åœ¨åµŒå…¥å‰å°†æ–‡æœ¬æ ‡è®°ä¸ºâ€œæŸ¥è¯¢â€æˆ–â€œæ®µè½â€ï¼Œå®ƒé€šè¿‡è€ƒè™‘è¾“å…¥çš„æ€§è´¨ä¼˜åŒ–äº†æ£€ç´¢è¿‡ç¨‹ã€‚è¿™ç§æ–¹æ³•ç¡®ä¿äº†æŸ¥è¯¢ä¸çŸ¥è¯†åº“ä¸­ç›¸å…³ä¿¡æ¯ä¹‹é—´çš„æ›´æœ‰æ•ˆåŒ¹é…ï¼Œä»è€Œæå‡äº†RAGç³»ç»Ÿçš„æ•´ä½“æ€§èƒ½ã€‚æ ¹æ®åŸºå‡†æµ‹è¯•ï¼Œcohere-embed-multilingual-v3.0å¯èƒ½è¡¨ç°æ›´ä½³ï¼Œä½†éœ€ä»˜è´¹ã€‚
- en: The step of embedding is often done as part of storing the documents in a vector
    DB, but a simple example of embedding all the split sentences using the E5 family
    can be done as below using the Sentence Transformer library.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: åµŒå…¥æ­¥éª¤é€šå¸¸ä½œä¸ºå­˜å‚¨æ–‡æ¡£åˆ°å‘é‡æ•°æ®åº“çš„ä¸€éƒ¨åˆ†å®Œæˆï¼Œä½†ä½¿ç”¨E5ç³»åˆ—å¯¹æ‰€æœ‰åˆ†å‰²å¥å­è¿›è¡ŒåµŒå…¥çš„ç®€å•ç¤ºä¾‹å¦‚ä¸‹ï¼Œä½¿ç”¨äº†Sentence Transformeråº“ã€‚
- en: '[PRE8]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '[PRE9]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: If off the shelf embeddings turn out not to provide sufficient performance for
    your specific retrieval domain, fear not. With the advent of LLMs it has now become
    feasible to auto-generate training-data from your existing corpus, and increase
    the performance of up to 5â€“10% by fine-tuning an existing embedding on your own
    data, [LlamaIndex provides a guide here](https://blog.llamaindex.ai/fine-tuning-embeddings-for-rag-with-synthetic-data-e534409a3971)
    or [SBERTs GenQ approach](https://www.sbert.net/examples/unsupervised_learning/query_generation/README.html#bi-encoder-training)
    where mainly the Bi-Encoder training part is relevant.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœç°æˆçš„åµŒå…¥åœ¨ä½ çš„ç‰¹å®šæ£€ç´¢é¢†åŸŸä¸­è¡¨ç°ä¸å¤Ÿç†æƒ³ï¼Œä¸ç”¨æ‹…å¿ƒã€‚éšç€LLMçš„å‡ºç°ï¼Œç°åœ¨å¯ä»¥ä»ç°æœ‰è¯­æ–™åº“ä¸­è‡ªåŠ¨ç”Ÿæˆè®­ç»ƒæ•°æ®ï¼Œå¹¶é€šè¿‡åœ¨ä½ è‡ªå·±çš„æ•°æ®ä¸Šå¾®è°ƒç°æœ‰åµŒå…¥æé«˜æ€§èƒ½ï¼Œæå‡å¹…åº¦å¯è¾¾5â€“10%ã€‚[LlamaIndexåœ¨è¿™é‡Œæä¾›äº†ä¸€ä¸ªæŒ‡å—](https://blog.llamaindex.ai/fine-tuning-embeddings-for-rag-with-synthetic-data-e534409a3971)
    æˆ– [SBERTs GenQæ–¹æ³•](https://www.sbert.net/examples/unsupervised_learning/query_generation/README.html#bi-encoder-training)ï¼Œå…¶ä¸­ä¸»è¦æ˜¯Bi-Encoderè®­ç»ƒéƒ¨åˆ†ç›¸å…³ã€‚
- en: '**5\. Vector databases: The home of embeddings**'
  id: totrans-90
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**5\. å‘é‡æ•°æ®åº“ï¼šåµŒå…¥çš„å®¶å›­**'
- en: '![](../Images/ed7609bab1bea443496512fadd371f33.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ed7609bab1bea443496512fadd371f33.png)'
- en: Embeddings are stored in a database for retrieval (Image generated by author
    w. Dall-E 3)
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: åµŒå…¥å­˜å‚¨åœ¨æ•°æ®åº“ä¸­ä»¥ä¾›æ£€ç´¢ï¼ˆå›¾åƒç”±ä½œè€…é€šè¿‡Dall-E 3ç”Ÿæˆï¼‰
- en: 'After loading, formatting, splitting your data, and selecting an embedding
    model, the next step in your RAG system setup is to embed the data and store these
    vector embeddings for retrieval. Most platforms, including LangChain and LlamaIndex,
    provide integrated local storage solutions, using vector databases like Qdrant,
    Milvus, Chroma DB or offer direct integration with cloud-based storage options
    such as Pinecone or ActiveLoop. The choice of vector storage is generally unaffected
    by whether your data is in English or another language. For a comprehensive understanding
    of storage and search options, including vector databases, I recommend exploring
    existing resources, such as this detailed introduction: [All You Need to Know
    About Vector Databases and How to Use Them to Augment Your LLM Apps](/all-you-need-to-know-about-vector-databases-and-how-to-use-them-to-augment-your-llm-apps-596f39adfedb#f02b).
    This guide will provide you with the necessary insights to effectively manage
    the storage aspect of your RAG system.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨åŠ è½½ã€æ ¼å¼åŒ–ã€æ‹†åˆ†æ•°æ®å¹¶é€‰æ‹©åµŒå…¥æ¨¡å‹ä¹‹åï¼ŒRAG ç³»ç»Ÿè®¾ç½®çš„ä¸‹ä¸€æ­¥æ˜¯åµŒå…¥æ•°æ®å¹¶å­˜å‚¨è¿™äº›å‘é‡åµŒå…¥ä»¥ä¾›æ£€ç´¢ã€‚å¤§å¤šæ•°å¹³å°ï¼ŒåŒ…æ‹¬ LangChain å’Œ LlamaIndexï¼Œéƒ½æä¾›äº†é›†æˆçš„æœ¬åœ°å­˜å‚¨è§£å†³æ–¹æ¡ˆï¼Œä½¿ç”¨åƒ
    Qdrantã€Milvusã€Chroma DB è¿™æ ·çš„å‘é‡æ•°æ®åº“ï¼Œæˆ–è€…ç›´æ¥ä¸åŸºäºäº‘çš„å­˜å‚¨é€‰é¡¹å¦‚ Pinecone æˆ– ActiveLoop é›†æˆã€‚å‘é‡å­˜å‚¨çš„é€‰æ‹©é€šå¸¸ä¸å—æ•°æ®è¯­è¨€ï¼ˆè‹±è¯­æˆ–å…¶ä»–è¯­è¨€ï¼‰çš„å½±å“ã€‚ä¸ºäº†å…¨é¢äº†è§£å­˜å‚¨å’Œæœç´¢é€‰é¡¹ï¼ŒåŒ…æ‹¬å‘é‡æ•°æ®åº“ï¼Œæˆ‘æ¨èä½ æ¢ç´¢ç°æœ‰èµ„æºï¼Œä¾‹å¦‚è¿™ä¸ªè¯¦ç»†ä»‹ç»ï¼š[å…³äºå‘é‡æ•°æ®åº“åŠå…¶å¦‚ä½•å¢å¼ºä½ çš„
    LLM åº”ç”¨ç¨‹åºçš„å…¨éƒ¨çŸ¥è¯†](/all-you-need-to-know-about-vector-databases-and-how-to-use-them-to-augment-your-llm-apps-596f39adfedb#f02b)ã€‚è¿™ä¸ªæŒ‡å—å°†ä¸ºä½ æä¾›æœ‰æ•ˆç®¡ç†
    RAG ç³»ç»Ÿå­˜å‚¨æ–¹é¢çš„å¿…è¦è§è§£ã€‚
- en: At this point, you have successfully created the knowledge base that will serve
    as the brain of the retrieval system.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ°ç›®å‰ä¸ºæ­¢ï¼Œä½ å·²ç»æˆåŠŸåˆ›å»ºäº†ä½œä¸ºæ£€ç´¢ç³»ç»Ÿâ€œå¤§è„‘â€çš„çŸ¥è¯†åº“ã€‚
- en: '![](../Images/34c0e985a66083b497a9e454d51b7d0a.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/34c0e985a66083b497a9e454d51b7d0a.png)'
- en: Generating responses (Image generated by author w. Dall-E 3)
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: ç”Ÿæˆå“åº”ï¼ˆå›¾åƒç”±ä½œè€… w. Dall-E 3 ç”Ÿæˆï¼‰
- en: '6\. The generative phase: Go read elsewhere ğŸ˜‰'
  id: totrans-97
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 6. ç”Ÿæˆé˜¶æ®µï¼šå»å…¶ä»–åœ°æ–¹é˜…è¯» ğŸ˜‰
- en: The second half of the RAG system, the generative phase, is equally important
    in ensuring a successful solution. Strictly speaking, itâ€™s a search optimization
    problem with a sprinkle of LLM on top, where the considerations are less language-dependent.
    This means that guides for English retrieval optimization are generally applicable
    to other languages as well, hence it is not included here.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: RAG ç³»ç»Ÿçš„ç¬¬äºŒéƒ¨åˆ†ï¼Œç”Ÿæˆé˜¶æ®µï¼Œåœ¨ç¡®ä¿è§£å†³æ–¹æ¡ˆæˆåŠŸæ–¹é¢åŒæ ·é‡è¦ã€‚ä¸¥æ ¼æ¥è¯´ï¼Œè¿™æ˜¯ä¸€ä¸ªæœç´¢ä¼˜åŒ–é—®é¢˜ï¼Œä¸Šé¢åŠ ä¸Šäº†ä¸€äº› LLMï¼Œè€ƒè™‘å› ç´ è¾ƒå°‘ä¾èµ–è¯­è¨€ã€‚è¿™æ„å‘³ç€é’ˆå¯¹è‹±è¯­çš„æ£€ç´¢ä¼˜åŒ–æŒ‡å—é€šå¸¸ä¹Ÿé€‚ç”¨äºå…¶ä»–è¯­è¨€ï¼Œå› æ­¤åœ¨æ­¤æœªåŒ…å«ã€‚
- en: 'In its simplest form, the generative phase involves a straightforward process:
    taking a userâ€™s question, embedding it using the selected embedding model from
    step 4, performing a vector similarity search in the newly created database, and
    finally feeding the relevant text chunks to the LLM. This allows the system to
    respond to the query in natural language. However, to achieve a high-performing
    RAG system, several adjustments on the retrieval side are necessary such as re-ranking,
    filtering and much more. For further insights, I recommend exploring articles
    such as [10 ways to improve the performance of retrieval augmented generation
    systems](/10-ways-to-improve-the-performance-of-retrieval-augmented-generation-systems-5fa2cee7cd5c)
    or [Improving Retrieval performance in RAG pipelines with Hybrid Search](/improving-retrieval-performance-in-rag-pipelines-with-hybrid-search-c75203c2f2f5)'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ€ç®€å•çš„å½¢å¼ä¸­ï¼Œç”Ÿæˆé˜¶æ®µæ¶‰åŠä¸€ä¸ªç›´æ¥çš„è¿‡ç¨‹ï¼šè·å–ç”¨æˆ·çš„é—®é¢˜ï¼Œä½¿ç”¨ç¬¬ 4 æ­¥ä¸­é€‰æ‹©çš„åµŒå…¥æ¨¡å‹è¿›è¡ŒåµŒå…¥ï¼Œåœ¨æ–°åˆ›å»ºçš„æ•°æ®åº“ä¸­æ‰§è¡Œå‘é‡ç›¸ä¼¼åº¦æœç´¢ï¼Œç„¶åå°†ç›¸å…³çš„æ–‡æœ¬å—æä¾›ç»™
    LLMã€‚è¿™ä½¿å¾—ç³»ç»Ÿèƒ½å¤Ÿç”¨è‡ªç„¶è¯­è¨€å›åº”æŸ¥è¯¢ã€‚ç„¶è€Œï¼Œè¦å®ç°é«˜æ€§èƒ½çš„ RAG ç³»ç»Ÿï¼Œéœ€è¦åœ¨æ£€ç´¢æ–¹é¢è¿›è¡Œè‹¥å¹²è°ƒæ•´ï¼Œå¦‚é‡æ–°æ’åºã€è¿‡æ»¤ç­‰ã€‚æœ‰å…³æ›´å¤šè§è§£ï¼Œæˆ‘å»ºè®®ä½ æ¢ç´¢ä¸€äº›æ–‡ç« ï¼Œä¾‹å¦‚
    [æå‡æ£€ç´¢å¢å¼ºç”Ÿæˆç³»ç»Ÿæ€§èƒ½çš„ 10 ç§æ–¹æ³•](/10-ways-to-improve-the-performance-of-retrieval-augmented-generation-systems-5fa2cee7cd5c)
    æˆ– [é€šè¿‡æ··åˆæœç´¢æ”¹è¿› RAG ç®¡é“ä¸­çš„æ£€ç´¢æ€§èƒ½](/improving-retrieval-performance-in-rag-pipelines-with-hybrid-search-c75203c2f2f5)ã€‚
- en: '**Outro: Evaluating your RAG system**'
  id: totrans-100
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**ç»“è¯­ï¼šè¯„ä¼°ä½ çš„ RAG ç³»ç»Ÿ**'
- en: '![](../Images/0943409b8b61f1ab7294a1bf17066d1c.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0943409b8b61f1ab7294a1bf17066d1c.png)'
- en: What are the right choices? (Image generated by author w. Dall-E 3)
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: æ­£ç¡®çš„é€‰æ‹©æ˜¯ä»€ä¹ˆï¼Ÿï¼ˆå›¾åƒç”±ä½œè€… w. Dall-E 3 ç”Ÿæˆï¼‰
- en: So what do you do from here, what is the right configuration for your exact
    problem, and language?
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: é‚£ä¹ˆä½ æ¥ä¸‹æ¥è¯¥åšä»€ä¹ˆï¼Ÿé’ˆå¯¹ä½ çš„å…·ä½“é—®é¢˜å’Œè¯­è¨€ï¼Œæ­£ç¡®çš„é…ç½®æ˜¯ä»€ä¹ˆï¼Ÿ
- en: As it might be clear at this point, deciding on the optimal settings for your
    RAG system can be a complex task due to the numerous variables involved. A custom
    query & context benchmark is essential to evaluate different configurations, especially
    since a pre-existing benchmark for your specific multilingual dataset and use
    case is very unlikely to exist.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨å¯èƒ½å·²ç»å¾ˆæ¸…æ¥šï¼Œå†³å®šRAGç³»ç»Ÿçš„æœ€ä½³è®¾ç½®å¯èƒ½æ˜¯ä¸€é¡¹å¤æ‚çš„ä»»åŠ¡ï¼Œå› ä¸ºæ¶‰åŠçš„å˜é‡ä¼—å¤šã€‚å®šåˆ¶çš„æŸ¥è¯¢å’Œä¸Šä¸‹æ–‡åŸºå‡†å¯¹äºè¯„ä¼°ä¸åŒé…ç½®è‡³å…³é‡è¦ï¼Œç‰¹åˆ«æ˜¯å› ä¸ºé’ˆå¯¹ä½ ç‰¹å®šçš„å¤šè¯­è¨€æ•°æ®é›†å’Œç”¨ä¾‹çš„ç°æœ‰åŸºå‡†éå¸¸ä¸å¯èƒ½å­˜åœ¨ã€‚
- en: Thankfully, with Large Language Models (LLMs), creating a tailored benchmark
    dataset has become feasible. A benchmark for retrieval systems typically comprises
    search queries and their corresponding context (the text chunks we split in step
    4). If you have the raw data, LLMs can automate the generation of fictional queries
    related to your dataset. [Tools like LlamaIndex provide built-in functions for
    this purpose](https://docs.llamaindex.ai/en/stable/examples/evaluation/retrieval/retriever_eval.html).
    By generating custom queries, you can systematically test how adjustments in the
    embedding model, chunk size, or data formatting impact the retrieval performance
    for your specific scenario.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: å¹¸è¿çš„æ˜¯ï¼Œå‡­å€Ÿå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ï¼Œåˆ›å»ºå®šåˆ¶çš„åŸºå‡†æ•°æ®é›†å·²å˜å¾—å¯è¡Œã€‚æ£€ç´¢ç³»ç»Ÿçš„åŸºå‡†é€šå¸¸åŒ…æ‹¬æœç´¢æŸ¥è¯¢åŠå…¶å¯¹åº”çš„ä¸Šä¸‹æ–‡ï¼ˆæˆ‘ä»¬åœ¨ç¬¬4æ­¥ä¸­æ‹†åˆ†çš„æ–‡æœ¬å—ï¼‰ã€‚å¦‚æœä½ æ‹¥æœ‰åŸå§‹æ•°æ®ï¼ŒLLMså¯ä»¥è‡ªåŠ¨ç”Ÿæˆä¸æ•°æ®é›†ç›¸å…³çš„è™šæ„æŸ¥è¯¢ã€‚[åƒLlamaIndexè¿™æ ·çš„å·¥å…·æä¾›äº†å†…ç½®åŠŸèƒ½æ¥å®ç°è¿™ä¸€ç›®çš„](https://docs.llamaindex.ai/en/stable/examples/evaluation/retrieval/retriever_eval.html)ã€‚é€šè¿‡ç”Ÿæˆè‡ªå®šä¹‰æŸ¥è¯¢ï¼Œä½ å¯ä»¥ç³»ç»Ÿåœ°æµ‹è¯•åµŒå…¥æ¨¡å‹ã€å—å¤§å°æˆ–æ•°æ®æ ¼å¼çš„è°ƒæ•´å¯¹ä½ ç‰¹å®šåœºæ™¯ä¸‹æ£€ç´¢æ€§èƒ½çš„å½±å“ã€‚
- en: Creating a representative evaluation benchmark has a fair amount of doâ€™s and
    dontâ€™s involved, and in early 2024 I will follow up with a separate post on how
    to create a well performing retrieval benchmark â€” stay tuned!
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ›å»ºä¸€ä¸ªå…·æœ‰ä»£è¡¨æ€§çš„è¯„ä¼°åŸºå‡†æ¶‰åŠè®¸å¤šæ³¨æ„äº‹é¡¹ï¼Œ2024å¹´åˆæˆ‘å°†è·Ÿè¿›ä¸€ç¯‡å…³äºå¦‚ä½•åˆ›å»ºä¸€ä¸ªè¡¨ç°è‰¯å¥½çš„æ£€ç´¢åŸºå‡†çš„å•ç‹¬æ–‡ç« â€”â€”æ•¬è¯·æœŸå¾…ï¼
- en: Thanks for taking the time to read this post, I hope you have found the article
    useful.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: æ„Ÿè°¢ä½ æŠ½å‡ºæ—¶é—´é˜…è¯»è¿™ç¯‡æ–‡ç« ï¼Œå¸Œæœ›ä½ è§‰å¾—è¿™ç¯‡æ–‡ç« å¯¹ä½ æœ‰æ‰€å¸®åŠ©ã€‚
- en: '**Remember to throw some ğŸ‘ğŸ‘ğŸ‘ if the content was of help, and feel free to reach
    out if you have questions or comments to the post.**'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '**å¦‚æœå†…å®¹å¯¹ä½ æœ‰å¸®åŠ©ï¼Œè¯·è®°å¾—ç‚¹èµğŸ‘ğŸ‘ğŸ‘ï¼Œå¦‚æœ‰é—®é¢˜æˆ–è¯„è®ºï¼Œè¯·éšæ—¶ä¸æˆ‘è”ç³»ã€‚**'
- en: '**References:**'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '**å‚è€ƒæ–‡çŒ®ï¼š**'
- en: '[Evaluating the Ideal Chunk Size for RAG System using Llamaindex](https://blog.llamaindex.ai/evaluating-the-ideal-chunk-size-for-a-rag-system-using-llamaindex-6207e5d3fec5)'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[ä½¿ç”¨Llamaindexè¯„ä¼°RAGç³»ç»Ÿçš„ç†æƒ³å—å¤§å°](https://blog.llamaindex.ai/evaluating-the-ideal-chunk-size-for-a-rag-system-using-llamaindex-6207e5d3fec5)'
- en: '[Building RAG-based LLM Applications for Production](https://www.anyscale.com/blog/a-comprehensive-guide-for-building-rag-based-llm-applications-part-1?utm_source=gradientflow&utm_medium=newsletter#chunk-data)'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[æ„å»ºåŸºäºRAGçš„LLMåº”ç”¨ç¨‹åºä»¥æŠ•å…¥ç”Ÿäº§](https://www.anyscale.com/blog/a-comprehensive-guide-for-building-rag-based-llm-applications-part-1?utm_source=gradientflow&utm_medium=newsletter#chunk-data)'
- en: '[How to chunk text a comparative analysis](/how-to-chunk-text-data-a-comparative-analysis-3858c4a0997a)'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[å¦‚ä½•åˆ†å—æ–‡æœ¬æ•°æ®çš„æ¯”è¾ƒåˆ†æ](/how-to-chunk-text-data-a-comparative-analysis-3858c4a0997a)'
- en: '[Massive Text Embedding Benchmark (MTEB)](https://huggingface.co/spaces/mteb/leaderboard)'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[å¤§è§„æ¨¡æ–‡æœ¬åµŒå…¥åŸºå‡† (MTEB)](https://huggingface.co/spaces/mteb/leaderboard)'
- en: '[SBERT on Asymmetric retrieval](https://www.sbert.net/examples/applications/semantic-search/README.html)'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[SBERTåœ¨éå¯¹ç§°æ£€ç´¢ä¸­çš„åº”ç”¨](https://www.sbert.net/examples/applications/semantic-search/README.html)'
- en: '[Finetuning embeddings using LlamaIndex](https://blog.llamaindex.ai/fine-tuning-embeddings-for-rag-with-synthetic-data-e534409a3971)'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[ä½¿ç”¨LlamaIndexå¾®è°ƒåµŒå…¥](https://blog.llamaindex.ai/fine-tuning-embeddings-for-rag-with-synthetic-data-e534409a3971)'
- en: '[Finetuning embeddings using SBERTs GenQ approach](https://www.sbert.net/examples/unsupervised_learning/query_generation/README.html#bi-encoder-training)'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[ä½¿ç”¨SBERTçš„GenQæ–¹æ³•å¾®è°ƒåµŒå…¥](https://www.sbert.net/examples/unsupervised_learning/query_generation/README.html#bi-encoder-training)'
- en: '[All You Need to Know About Vector Databases and How to Use Them to Augment
    Your LLM Apps](/all-you-need-to-know-about-vector-databases-and-how-to-use-them-to-augment-your-llm-apps-596f39adfedb#f02b)'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[å…³äºå‘é‡æ•°æ®åº“åŠå…¶å¦‚ä½•å¢å¼ºä½ çš„LLMåº”ç”¨ç¨‹åºçš„æ‰€æœ‰ä¿¡æ¯](/all-you-need-to-know-about-vector-databases-and-how-to-use-them-to-augment-your-llm-apps-596f39adfedb#f02b)'
- en: '[10 ways to improve the performance of retrieval augmented generation systems](/10-ways-to-improve-the-performance-of-retrieval-augmented-generation-systems-5fa2cee7cd5c)'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[æé«˜æ£€ç´¢å¢å¼ºç”Ÿæˆç³»ç»Ÿæ€§èƒ½çš„10ç§æ–¹æ³•](/10-ways-to-improve-the-performance-of-retrieval-augmented-generation-systems-5fa2cee7cd5c)'
- en: '[Improving Retrieval performance in RAG pipelines with Hybrid Search](/improving-retrieval-performance-in-rag-pipelines-with-hybrid-search-c75203c2f2f5)'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[é€šè¿‡æ··åˆæœç´¢æé«˜RAGç®¡é“ä¸­çš„æ£€ç´¢æ€§èƒ½](/improving-retrieval-performance-in-rag-pipelines-with-hybrid-search-c75203c2f2f5)'
- en: '[Evaluating retrieval performance of RAG systems using LlamaIndex](https://docs.llamaindex.ai/en/stable/examples/evaluation/retrieval/retriever_eval.html)'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[ä½¿ç”¨LlamaIndexè¯„ä¼°RAGç³»ç»Ÿçš„æ£€ç´¢æ€§èƒ½](https://docs.llamaindex.ai/en/stable/examples/evaluation/retrieval/retriever_eval.html)'
