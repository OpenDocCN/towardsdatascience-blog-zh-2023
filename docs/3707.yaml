- en: 'Beyond English: Implementing a multilingual RAG solution'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 超越英语：实现多语言RAG解决方案
- en: 原文：[https://towardsdatascience.com/beyond-english-implementing-a-multilingual-rag-solution-12ccba0428b6?source=collection_archive---------0-----------------------#2023-12-20](https://towardsdatascience.com/beyond-english-implementing-a-multilingual-rag-solution-12ccba0428b6?source=collection_archive---------0-----------------------#2023-12-20)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/beyond-english-implementing-a-multilingual-rag-solution-12ccba0428b6?source=collection_archive---------0-----------------------#2023-12-20](https://towardsdatascience.com/beyond-english-implementing-a-multilingual-rag-solution-12ccba0428b6?source=collection_archive---------0-----------------------#2023-12-20)
- en: An introduction to the do’s and don’ts when implementing a non-english Retrieval
    Augmented Generation (RAG) system
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实施非英语检索增强生成（RAG）系统时的注意事项
- en: '[](https://medium.com/@jalkestrup?source=post_page-----12ccba0428b6--------------------------------)[![Jesper
    Alkestrup](../Images/199803c75758a9b943e72746105f3de5.png)](https://medium.com/@jalkestrup?source=post_page-----12ccba0428b6--------------------------------)[](https://towardsdatascience.com/?source=post_page-----12ccba0428b6--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----12ccba0428b6--------------------------------)
    [Jesper Alkestrup](https://medium.com/@jalkestrup?source=post_page-----12ccba0428b6--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@jalkestrup?source=post_page-----12ccba0428b6--------------------------------)[![Jesper
    Alkestrup](../Images/199803c75758a9b943e72746105f3de5.png)](https://medium.com/@jalkestrup?source=post_page-----12ccba0428b6--------------------------------)[](https://towardsdatascience.com/?source=post_page-----12ccba0428b6--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----12ccba0428b6--------------------------------)
    [Jesper Alkestrup](https://medium.com/@jalkestrup?source=post_page-----12ccba0428b6--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F84e00f9ebe19&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbeyond-english-implementing-a-multilingual-rag-solution-12ccba0428b6&user=Jesper+Alkestrup&userId=84e00f9ebe19&source=post_page-84e00f9ebe19----12ccba0428b6---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----12ccba0428b6--------------------------------)
    ·18 min read·Dec 20, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F12ccba0428b6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbeyond-english-implementing-a-multilingual-rag-solution-12ccba0428b6&user=Jesper+Alkestrup&userId=84e00f9ebe19&source=-----12ccba0428b6---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[阅读](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F84e00f9ebe19&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbeyond-english-implementing-a-multilingual-rag-solution-12ccba0428b6&user=Jesper+Alkestrup&userId=84e00f9ebe19&source=post_page-84e00f9ebe19----12ccba0428b6---------------------post_header-----------)
    在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----12ccba0428b6--------------------------------)
    发布 · 18 分钟阅读 · 2023年12月20日'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F12ccba0428b6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbeyond-english-implementing-a-multilingual-rag-solution-12ccba0428b6&source=-----12ccba0428b6---------------------bookmark_footer-----------)![](../Images/4b0d38957430dc14c526618b898f1182.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F12ccba0428b6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbeyond-english-implementing-a-multilingual-rag-solution-12ccba0428b6&source=-----12ccba0428b6---------------------bookmark_footer-----------)![](../Images/4b0d38957430dc14c526618b898f1182.png)'
- en: RAG, an all knowing colleague, available 24/7 (Image generated by author w.
    Dall-E 3)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: RAG，一个无所不知的同事，全天候提供服务（图片由作者使用 Dall-E 3 生成）
- en: TLDR
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: TLDR
- en: 'This article provides an introduction to the considerations one should take
    into account when developing non-English RAG systems, complete with specific examples
    and techniques. Some of the key points include:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 这篇文章介绍了在开发非英语RAG系统时应考虑的因素，并提供了具体的示例和技术。关键点包括：
- en: Prioritize maintaining syntactic structure during data loading, as it is crucial
    for meaningful text segmentation.
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在数据加载过程中优先保持句法结构，因为这对有意义的文本分割至关重要。
- en: Format documents using simple delimiters like \n\n to facilitate efficient text
    splitting.
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用简单分隔符如\n\n来格式化文档，以促进高效的文本拆分。
- en: Opt for rule-based text splitters, given the computational intensity and subpar
    performance of ML-based semantic splitters in multilingual contexts.
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择基于规则的文本分割器，因为在多语言环境中，基于 ML 的语义分割器计算强度大且性能较差。
- en: In selecting an embedding model, consider both its multilingual capabilities
    and asymmetric retrieval performance.
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在选择嵌入模型时，考虑其多语言能力和不对称检索性能。
- en: For multilingual projects, fine-tuning an embedding model with a Large Language
    Model (LLM) can enhance performance, and may be needed to achieve sufficient accuracy.
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于多语言项目，通过大语言模型 (LLM) 微调嵌入模型可以提高性能，可能需要以实现足够的准确性。
- en: Implementing an LLM-based retrieval evaluation benchmark is strongly recommended
    to fine-tune the hyperparameters of your RAG system effectively, and can be done
    easily with existing frameworks.
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 强烈推荐实施基于 LLM 的检索评估基准，以有效微调 RAG 系统的超参数，并且可以利用现有框架轻松完成。
- en: It is no wonder that RAG has become the trendiest term within search technology
    in 2023\. Retrieval Augmented Generation (RAG) is transforming how organizations
    utilize their vast quantity of existing data to power intelligent ChatBots. These
    bots, capable of conversations in natural language, can draw on an organization’s
    collective knowledge to function as an always-available, in-house expert to deliver
    relevant answers, grounded in verified data. While a considerable number of resources
    are available on building RAG systems, most are geared toward the English language,
    leaving a gap for smaller languages.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: RAG 成为 2023 年搜索技术中最流行的术语也就不足为奇了。检索增强生成 (RAG) 正在改变组织利用其大量现有数据来推动智能聊天机器人的方式。这些能够进行自然语言对话的机器人，可以利用组织的集体知识，充当一个始终可用的内部专家，提供基于经验证数据的相关答案。虽然有大量资源可用于构建
    RAG 系统，但大多数资源针对的是英语，较小语言的资源仍有缺口。
- en: This 6-step easy-to-follow guide will walk you through the do’s and don’ts when
    creating RAG systems for non-English languages.
  id: totrans-18
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 这份易于遵循的 6 步指南将引导你了解在为非英语语言创建 RAG 系统时的注意事项。
- en: RAG structure, a brief recap
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: RAG 结构，简要回顾
- en: 'This article presumes familiarity with concepts like embeddings, vectors, and
    tokens. For those needing a brief refresher on the architecture of RAG systems,
    they essentially consist of two core components:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 本文假设读者对嵌入、向量和标记等概念有一定了解。对于需要简要回顾 RAG 系统架构的人来说，它们主要由两个核心组件组成：
- en: 'Indexing phase (the focus of this article): This initial stage involves processing
    the input data. The data is first loaded, appropriately formatted, then split.
    Later, it undergoes vectorization through embedding techniques, culminating in
    its storage within a knowledge base for future retrieval.'
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 索引阶段（本文的重点）：这一初始阶段涉及处理输入数据。数据首先被加载、适当格式化，然后进行拆分。之后，数据通过嵌入技术进行向量化，最终存储在知识库中以便将来检索。
- en: 'Generative phase: In this phase, a user’s query is input to the retrieval system.
    This system then extracts relevant information snippets from the knowledge base.
    Leveraging a Large Language Model (LLM), the system interprets this data to formulate
    a coherent, natural language response, effectively addressing the user’s inquiry.'
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 生成阶段：在此阶段，用户的查询被输入到检索系统中。该系统随后从知识库中提取相关信息片段。利用大语言模型 (LLM)，系统解释这些数据以制定连贯的自然语言响应，有效地解答用户的询问。
- en: '![](../Images/0e5538579e680f53cdcb9062d811cc75.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0e5538579e680f53cdcb9062d811cc75.png)'
- en: Now let’s get started!
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们开始吧！
- en: '*Disclaimer:*'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '*免责声明：*'
- en: '*This guide doesn’t aim to be an exhaustive manual on using any particular
    tool. Instead, its purpose is to shed light on the overarching decisions that
    should guide your tool selection. In practice, I strongly recommend leveraging
    an established framework for constructing your system’s foundation. For building
    RAG systems, I would personally recommend LlamaIndex as they provide detailed
    guides and features focused strictly on indexing and retrieval optimization.*'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '*本指南并不旨在成为使用任何特定工具的详尽手册。相反，其目的是阐明应指导工具选择的总体决策。实际上，我强烈建议利用已建立的框架来构建系统基础。对于构建
    RAG 系统，我个人推荐 LlamaIndex，因为它们提供了详细的指南和专注于索引和检索优化的功能。*'
- en: '*Additionally, this guide is written with the assumption that we’re dealing
    with languages that use the latin script and read from left to right. This includes
    languages like German, French, Spanish, Czech, , Turkish, Vietnamese, Norwegian,
    Polish, and quite a few others. Languages outside of this group may have different
    needs and considerations.*'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '*此外，本指南假设我们处理的是使用拉丁字母并从左向右阅读的语言。这包括德语、法语、西班牙语、捷克语、土耳其语、越南语、挪威语、波兰语以及其他一些语言。其他语言可能有不同的需求和考虑因素。*'
- en: '1\. Data loader: The devil’s in the details'
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 1\. 数据加载器：关键在于细节
- en: '![](../Images/ff11a5cb9d6053f2e24509540b6b2c14.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ff11a5cb9d6053f2e24509540b6b2c14.png)'
- en: A cool looking multi-modal dataloader (Image generated by author w. Dall-E 3)
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 一个外观酷炫的多模态数据加载器（图像由作者使用 Dall-E 3 生成）
- en: The first step in a RAG system involves using a dataloader to handle diverse
    formats, from text documents to multimedia, extracting all relevant content for
    further processing. For text-based formats, dataloaders typically perform consistently
    across languages, as they don’t involve language-specific processing. With the
    advent of multi-modal RAG systems, it is however crucial to be aware of the reduced
    performance of speech to text models compared to their English counterparts. Models
    like [Whisper v3](https://github.com/openai/whisper) demonstrate impressive multilingual
    capabilities, but it’s wise to check out their performance on benchmarks like
    [Mozilla Common Voice](https://commonvoice.mozilla.org/en/datasets) or the [Fleurs](https://huggingface.co/datasets/google/fleurs)
    dataset, and ideally evaluate those on your own benchmark.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: RAG 系统的第一步是使用数据加载器处理各种格式，从文本文件到多媒体，提取所有相关内容以供进一步处理。对于基于文本的格式，数据加载器通常在不同语言间表现一致，因为它们不涉及特定语言的处理。然而，随着多模态
    RAG 系统的出现，了解语音转文本模型在与其英语对应模型相比性能降低的情况非常重要。像[Whisper v3](https://github.com/openai/whisper)这样的模型展示了令人印象深刻的多语言能力，但最好查看它们在[Mozilla
    Common Voice](https://commonvoice.mozilla.org/en/datasets)或[Fleurs](https://huggingface.co/datasets/google/fleurs)数据集上的表现，并且最好在自己的基准上进行评估。
- en: For the remainder of this article, we’ll however concentrate on text-based inputs.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 本文其余部分将集中讨论基于文本的输入。
- en: Why retaining syntactic structure is important
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 保留句法结构为何重要
- en: A key aspect of data loading is to preserve the original data’s syntactic integrity.
    The loss of elements such as headers or paragraph structures can impact the accuracy
    of subsequent information retrieval. This concern is heightened for non-English
    languages due to the limited availability of machine learning-based segmentation
    tools.
  id: totrans-34
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 数据加载的一个关键方面是保持原始数据的句法完整性。丢失诸如标题或段落结构的元素可能会影响后续信息检索的准确性。对于非英语语言，这种担忧尤为突出，因为基于机器学习的分段工具的可用性有限。
- en: Syntactic information plays a crucial role because the effectiveness of RAG
    systems in delivering meaningful answers depends partly on their ability to split
    data into semantically accurate subsections.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 句法信息发挥着至关重要的作用，因为 RAG 系统在提供有意义答案的效果部分取决于它们将数据拆分为语义准确的子部分的能力。
- en: To highlight the differences between a data loading approach that retains the
    structure and one that does not, let’s take the example of using a basic HTML
    dataloader versus a PDF loader on a [medium article](https://blog.llamaindex.ai/boosting-rag-picking-the-best-embedding-reranker-models-42d079022e83).
    Libraries such as [LangChain](https://python.langchain.com/docs/modules/data_connection/document_loaders/html)
    and [LlamaIndex](https://llamahub.ai/l/web-beautiful_soup_web) both rely on the
    exact same libraries, but just wrap the functions in their own document classes
    (Requests+BS4 for web, PyPDF2 for PDFs).
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 为了突出保留结构的数据加载方法与不保留结构的方法之间的区别，举一个使用基础 HTML 数据加载器与 PDF 加载器对[medium article](https://blog.llamaindex.ai/boosting-rag-picking-the-best-embedding-reranker-models-42d079022e83)的例子。像[LangChain](https://python.langchain.com/docs/modules/data_connection/document_loaders/html)和[LlamaIndex](https://llamahub.ai/l/web-beautiful_soup_web)这样的库都依赖于完全相同的库，但只是将函数封装在各自的文档类中（Web
    用 Requests+BS4，PDF 用 PyPDF2）。
- en: '**HTML Dataloader: This method retains the syntactic structure of the content.**'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '**HTML 数据加载器：此方法保留了内容的句法结构。**'
- en: '[PRE0]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '[PRE1]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '**PDF data loader, example in which syntactic information is lost** (saved
    article as PDF, then re-loaded)'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '**PDF 数据加载器，句法信息丢失的示例**（将文章保存为 PDF 后重新加载）'
- en: '[PRE2]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[PRE3]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Upon initial review, the PDF dataloader’s output appears more readable, but
    closer inspection reveals a loss of structural information — how would one tell
    what is a header, and where a section ends? In contrast, the HTML file retains
    all the relevant structure.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 初步检查显示，PDF 数据加载器的输出看起来更可读，但仔细检查后发现丢失了结构信息——如何区分标题和节的结束？相比之下，HTML 文件保留了所有相关的结构。
- en: Ideally, you want to retain all original formatting in the data loader, and
    only decide on filtering and reformatting in the next step. However, that might
    involve building custom data loaders for your use case, and in some cases be impossible.
    I recommend to simply start with a standard data loader, but spend a few minutes
    to inspect examples of the loaded data carefully and understand what structure
    has been lost.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 理想情况下，你希望在数据加载器中保留所有原始格式，并且仅在下一步决定过滤和重新格式化。然而，这可能涉及为你的使用案例构建自定义数据加载器，并且在某些情况下可能是不可能的。我建议你从标准数据加载器开始，但花几分钟仔细检查加载的数据示例，并了解丢失了哪些结构。
- en: Understanding what syntactic that is lost is crucial, as it guides potential
    improvements if the system’s downstream retrieval performance needs enhancement,
    allowing for targeted refinements.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 了解丢失的语法结构是至关重要的，因为它指导了系统下游检索性能需要改进的潜在方向，允许进行有针对性的优化。
- en: '2\. Data formatting: Boring… but important'
  id: totrans-46
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 2. 数据格式化：无聊……但重要
- en: '![](../Images/82a92d20f1ab6a15b7c2408f5cb9668f.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/82a92d20f1ab6a15b7c2408f5cb9668f.png)'
- en: Document chunking (Image generated by author w. Dall-E 3)
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 文档分块（图像由作者使用 Dall-E 3 生成）
- en: The second step, formatting, serves the primary purpose of uniforming the data
    from your data loaders in a way that prepares the data for the next step of text
    splitting. As the following section explains, dividing the input text into a myriad
    of smaller chunks will be necessary. A successful formatting sets up the text
    in a way that provides the best possible conditions for dividing the content into
    semantically meaningful chunks. Simply put, your goal is to transform the potentially
    complex syntactic structure retrieved from a html or a markdown file, into a plain
    text file with basic delimiters such as /n (line change) and /n/n (end of section)
    to guide the text splitter.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 第二步，格式化，其主要目的是以统一的方式整理来自数据加载器的数据，以便为下一步的文本拆分做准备。如以下章节所述，将输入文本划分为无数较小的块是必要的。成功的格式化将文本设置成提供最佳条件以将内容划分为语义上有意义的块。简单来说，你的目标是将从
    html 或 markdown 文件中检索到的潜在复杂语法结构转换为带有基本分隔符的纯文本文件，如 /n（换行）和 /n/n（节结束），以指导文本拆分器。
- en: 'A simple function to format the BS4 HTML object into a dictionary with title
    and text could look like the below:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 一个简单的函数将 BS4 HTML 对象格式化为包含标题和文本的字典，如下所示：
- en: '[PRE4]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[PRE5]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: For complex RAG systems where there might be multiple correct answers relative
    to the context, storing additional information like document titles or headers
    as metadata along the text chunks is beneficial. This metadata can be used later
    for filtering, and if available, formatting elements like headers should influence
    your chunking strategy. A library like LlamaIndex natively work with the concept
    of metadata and text wrapped together in Node objects, and I highly recommend
    using this or a similar framework
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 对于复杂的 RAG 系统，其中相对于上下文可能有多个正确答案，将文档标题或标题等附加信息存储为文本块的元数据是有益的。这些元数据可以在之后用于过滤，如果可用，格式化元素如标题应影响你的分块策略。像
    LlamaIndex 这样的库本地处理与元数据和文本一起封装在 Node 对象中的概念，我强烈推荐使用这个或类似的框架。
- en: Now that we’ve done our formatting correctly, let’s dive into the key aspects
    of text splitting!
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经正确地完成了格式化，让我们深入探讨文本拆分的关键方面吧！
- en: '3: Text splitting: Size matters'
  id: totrans-55
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '3: 文本拆分：大小重要'
- en: '![](../Images/a4f4db130ed93d7637ff014bd449b7a6.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a4f4db130ed93d7637ff014bd449b7a6.png)'
- en: Splitting text, the simple way (Image generated by author w. Dall-E 3)
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 拆分文本，简单的方法（图像由作者使用 Dall-E 3 生成）
- en: When preparing data for embedding and retrieval in a RAG system, splitting the
    text into appropriately sized chunks is crucial. This process is guided by two
    main factors, Model Constraints and Retrieval Effectiveness.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在为 RAG 系统准备数据以进行嵌入和检索时，将文本拆分为适当大小的块是至关重要的。这个过程受两个主要因素的指导：模型约束和检索有效性。
- en: '**Model Constraints**'
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**模型约束**'
- en: Embedding models have a maximum token length for input; anything beyond this
    limit gets truncated. Be aware of your chosen model’s limitations and ensure that
    each data chunk doesn’t exceed this max token length.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入模型对输入的最大 token 长度有一个限制；超出此限制的内容会被截断。了解所选择模型的限制，并确保每个数据块不超过此最大 token 长度。
- en: Multilingual models, in particular, often have shorter sequence limits compared
    to their English counterparts. For instance, the widely used Paraphrase multilingual
    MiniLM-L12 v2 model has a maximum context window of just 128 tokens.
  id: totrans-61
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 多语言模型，特别是，与其英文对应模型相比，通常具有较短的序列限制。例如，广泛使用的 Paraphrase multilingual MiniLM-L12
    v2 模型的最大上下文窗口仅为 128 个 token。
- en: Also, consider the text length the model was trained on — some models might
    technically accept longer inputs but were trained on shorter chunks, which could
    affect performance on longer texts. One such is example, is the [Multi QA base
    from SBERT](https://huggingface.co/sentence-transformers/multi-qa-mpnet-base-dot-v1)
    as seen below,
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，还要考虑模型的训练文本长度——一些模型虽然在技术上可以接受更长的输入，但其训练数据却较短，这可能会影响对较长文本的性能。例如，[SBERT 的 Multi
    QA 基础模型](https://huggingface.co/sentence-transformers/multi-qa-mpnet-base-dot-v1)
    如下所示，
- en: '![](../Images/e0d8262a6f1b3b939051aba7f8901fbb.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e0d8262a6f1b3b939051aba7f8901fbb.png)'
- en: '**Retrieval effectiveness**'
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**检索效果**'
- en: While chunking data to the model’s maximum length seems logical, it might not
    always lead to the best retrieval outcomes. Larger chunks offer more context for
    the LLM but can obscure key details, making it harder to retrieve precise matches.
    Conversely, smaller chunks can enhance match accuracy but might lack the context
    needed for complete answers. Hybrid approaches use smaller chunks for search but
    include surrounding context at query time for balance.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然将数据拆分到模型的最大长度似乎是合理的，但这可能并不总是能带来最佳的检索结果。较大的块为 LLM 提供了更多的上下文，但可能会掩盖关键细节，使得精确匹配更加困难。相反，较小的块可以提高匹配准确性，但可能缺乏获取完整答案所需的上下文。混合方法使用较小的块进行搜索，但在查询时包括周围的上下文以保持平衡。
- en: While there isn’t a definitive answer regarding chunk size, the considerations
    for chunk size remain consistent whether you’re working on multilingual or English
    projects. I would recommend reading further on the topic from resources such as
    [Evaluating the Ideal Chunk Size for RAG System using Llamaindex](https://blog.llamaindex.ai/evaluating-the-ideal-chunk-size-for-a-rag-system-using-llamaindex-6207e5d3fec5)
    or [Building RAG-based LLM Applications for Production](https://www.anyscale.com/blog/a-comprehensive-guide-for-building-rag-based-llm-applications-part-1?utm_source=gradientflow&utm_medium=newsletter#chunk-data).
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管关于块大小没有确切的答案，但块大小的考虑在多语言项目和英语项目中是一致的。我建议进一步阅读相关资源，如 [使用 Llamaindex 评估 RAG
    系统的理想块大小](https://blog.llamaindex.ai/evaluating-the-ideal-chunk-size-for-a-rag-system-using-llamaindex-6207e5d3fec5)
    或 [为生产环境构建基于 RAG 的 LLM 应用程序](https://www.anyscale.com/blog/a-comprehensive-guide-for-building-rag-based-llm-applications-part-1?utm_source=gradientflow&utm_medium=newsletter#chunk-data)。
- en: '**Text splitting: Methods for splitting text**'
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**文本拆分：文本拆分的方法**'
- en: '[Text can be split using various methods](/how-to-chunk-text-data-a-comparative-analysis-3858c4a0997a),
    mainly falling into two categories: rule-based (focusing on character analysis)
    and machine learning-based models. ML approaches, from simple NLTK & Spacy tokenizers
    to advanced transformer models, often depend on language-specific training, primarily
    in English. Although simple models like NLTK & Spacy support multiple languages,
    they mainly address sentence splitting, not semantic sectioning.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '[文本可以通过各种方法进行拆分](/how-to-chunk-text-data-a-comparative-analysis-3858c4a0997a)，主要分为两类：基于规则的（注重字符分析）和基于机器学习的模型。机器学习方法，从简单的
    NLTK 和 Spacy 分词器到先进的 transformer 模型，通常依赖于语言特定的训练，主要是英语。尽管像 NLTK 和 Spacy 这样的简单模型支持多种语言，但它们主要处理句子拆分，而非语义划分。'
- en: '*Since ML based sentence splitters currently work poorly for most non-English
    languages, and are compute intensive, I recommend starting with a simple rule-based
    splitter. If you’ve preserved relevant syntactic structure from the original data,
    and formatted the data correctly, the result will be of good quality.*'
  id: totrans-69
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*由于基于机器学习的句子拆分器目前在大多数非英语语言中效果不佳且计算密集，我建议从简单的基于规则的拆分器开始。如果你保留了原始数据的相关句法结构，并正确地格式化了数据，结果将会质量良好。*'
- en: A common and effective method is a recursive character text splitter, like those
    used in [LangChain](https://python.langchain.com/docs/modules/data_connection/document_transformers/text_splitters/recursive_text_splitter)
    or LlamaIndex, which shortens sections by finding the nearest split character
    in a prioritized sequence (e.g., \n\n, \n, ., ?, !).
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 一种常见而有效的方法是递归字符文本分割器，例如在 [LangChain](https://python.langchain.com/docs/modules/data_connection/document_transformers/text_splitters/recursive_text_splitter)
    或 LlamaIndex 中使用的，它通过在优先序列中找到最近的分隔字符（例如 \n\n, \n, ., ?, !）来缩短段落。
- en: 'Taking the formatted text from the previous section, an example of using LangChains
    recursive character splitter would look like:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 使用前一部分格式化文本的示例，使用 LangChain 的递归字符分割器如下所示：
- en: '[PRE6]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Here it’s important to note that one should define the tokenizer as the embedding
    model intended to use, since different models ‘count’ the words differently. The
    function will now, in a prioritized order, split any text longer than 128 tokens
    first by the \n\n we introduced at end of sections, and if that is not possible,
    then by end of paragraphs delimited by \n and so forth. The first 3 chunks will
    be:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，需要注意的是，应该将分词器定义为拟使用的嵌入模型，因为不同模型对词汇的计数方式不同。函数现在将按照优先顺序，首先通过我们在段落末尾引入的 \n\n
    拆分任何超过128个标记的文本，如果不可能，则通过 \n 分隔的段落末尾，依此类推。前三个块将是：
- en: '[PRE7]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Now that we have successfully split the text in a semantically meaningful way,
    we can move onto the final part of embedding these chunks for storage.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经成功地以语义上有意义的方式拆分了文本，可以进入最终阶段，即将这些块嵌入以便存储。
- en: '**4\. Embedding Models: Navigating the jungle**'
  id: totrans-76
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**4\. 嵌入模型：在丛林中导航**'
- en: '![](../Images/15ebcd61fd4e03d1657cceb920ad2483.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/15ebcd61fd4e03d1657cceb920ad2483.png)'
- en: Embedding models convert text to vectors (Image generated by author w. Dall-E
    3)
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入模型将文本转换为向量（图片由作者使用 Dall-E 3 生成）
- en: Choosing the right embedding model is critical for the success of a Retrieval
    Augmented Generation (RAG) system, and something that is less straight forward
    than for the English language. A comprehensive resource for comparing models is
    the [Massive Text Embedding Benchmark (MTEB)](https://huggingface.co/spaces/mteb/leaderboard),
    which includes benchmarks for over 100 languages.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 选择正确的嵌入模型对于检索增强生成（RAG）系统的成功至关重要，这比英语语言的情况复杂得多。比较模型的一个全面资源是 [Massive Text Embedding
    Benchmark (MTEB)](https://huggingface.co/spaces/mteb/leaderboard)，其中包含超过100种语言的基准。
- en: The model of your choice must either be multilingual or specifically tailored
    to the language you’re working with (monolingual). Remember, the latest high-performing
    models are often English-centric and may not work well with other languages.
  id: totrans-80
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 你选择的模型必须是多语言的，或专门针对你正在使用的语言（单语言）定制的。请记住，最新的高性能模型通常以英语为中心，可能不适用于其他语言。
- en: '![](../Images/35f837806d09da41b4fdced9b6bc0438.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/35f837806d09da41b4fdced9b6bc0438.png)'
- en: If available, refer to language-specific benchmarks relevant to your task. For
    instance, in classification tasks, there are over 50 language-specific benchmarks,
    aiding in selecting the most efficient model for languages ranging from Danish
    to Spanish. However, it’s important to note that these benchmarks may not directly
    indicate a model’s efficiency in retrieving relevant information for a RAG system,
    because retrieval is different from classification, clustering or another task.
    The task is to find models trained for asymmetric search, as those not trained
    for this specific task might inaccurately prioritize shorter passages over longer,
    more relevant ones.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 如果有相关资源，请参考与你的任务相关的语言特定基准。例如，在分类任务中，有超过50个语言特定的基准，帮助选择最有效的模型，适用于从丹麦语到西班牙语的语言。然而，重要的是要注意，这些基准可能不会直接指示模型在RAG系统中检索相关信息的效率，因为检索与分类、聚类或其他任务不同。任务是找到训练用于不对称搜索的模型，因为那些没有针对这一特定任务训练的模型可能会不准确地优先考虑较短的段落而非较长且更相关的段落。
- en: The model should excel in [asymmetric retrieval](https://www.sbert.net/examples/applications/semantic-search/README.html),
    matching short queries to longer text chunks. The reason why is that, in a RAG
    system, you often match a brief query to more extensive passages to extract meaningful
    answers. The MTEB benchmarks related to asymmetric search are listed under the
    Retrieval. A challenge is that as of November 2023, MTEB’s Retrieval benchmark
    includes only English, Chinese, and Polish.
  id: totrans-83
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 该模型应在[非对称检索](https://www.sbert.net/examples/applications/semantic-search/README.html)中表现出色，将短查询匹配到较长的文本块。原因在于，在RAG系统中，你通常需要将简短的查询匹配到更长的段落中以提取有意义的答案。与非对称检索相关的MTEB基准列在检索部分。一个挑战是截至2023年11月，MTEB的检索基准仅包括英语、中文和波兰语。
- en: When dealing with languages like Norwegian, where there may not be specific
    retrieval benchmarks, you might wonder whether to choose the best-performing model
    from classification benchmarks or a general multilingual model proficient in English
    retrieval?
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理像挪威语这样的语言时，可能没有特定的检索基准，你可能会想知道是否应该选择分类基准中表现最好的模型，还是选择一个在英语检索方面表现出色的通用多语言模型？
- en: As for practical advice, a simple rule of thumb is to opt for the top-performing
    multilingual model in the MTEB Retrieval benchmark. Beware that the retrieval
    score itself, is however still based on English, so benchmarking on your own language
    is needed to qualify the performance (step 6). As of December 2023, the E5-multilingual
    family is a strong choice for an open source model. The model is fine-tuned for
    asymmetric search, and by tagging texts as ‘query’ or ‘passage’ before embedding,
    it optimizes the retrieval process by considering the nature of the input. This
    approach ensures a more effective match between queries and relevant information
    in your knowledge base, enhancing the overall performance of your RAG system.
    As seen on the benchmark, the cohere-embed-multilingual-v3.0 likely has better
    performance, but has to be paid for.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 对于实际建议，简单的经验法则是选择MTEB检索基准中表现最好的多语言模型。注意，检索评分本身仍然基于英语，因此需要在你自己的语言上进行基准测试以验证性能（第6步）。截至2023年12月，E5-多语言系列是开源模型的一个强有力的选择。该模型经过针对非对称检索的微调，通过在嵌入前将文本标记为“查询”或“段落”，它通过考虑输入的性质优化了检索过程。这种方法确保了查询与知识库中相关信息之间的更有效匹配，从而提升了RAG系统的整体性能。根据基准测试，cohere-embed-multilingual-v3.0可能表现更佳，但需付费。
- en: The step of embedding is often done as part of storing the documents in a vector
    DB, but a simple example of embedding all the split sentences using the E5 family
    can be done as below using the Sentence Transformer library.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入步骤通常作为存储文档到向量数据库的一部分完成，但使用E5系列对所有分割句子进行嵌入的简单示例如下，使用了Sentence Transformer库。
- en: '[PRE8]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '[PRE9]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: If off the shelf embeddings turn out not to provide sufficient performance for
    your specific retrieval domain, fear not. With the advent of LLMs it has now become
    feasible to auto-generate training-data from your existing corpus, and increase
    the performance of up to 5–10% by fine-tuning an existing embedding on your own
    data, [LlamaIndex provides a guide here](https://blog.llamaindex.ai/fine-tuning-embeddings-for-rag-with-synthetic-data-e534409a3971)
    or [SBERTs GenQ approach](https://www.sbert.net/examples/unsupervised_learning/query_generation/README.html#bi-encoder-training)
    where mainly the Bi-Encoder training part is relevant.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 如果现成的嵌入在你的特定检索领域中表现不够理想，不用担心。随着LLM的出现，现在可以从现有语料库中自动生成训练数据，并通过在你自己的数据上微调现有嵌入提高性能，提升幅度可达5–10%。[LlamaIndex在这里提供了一个指南](https://blog.llamaindex.ai/fine-tuning-embeddings-for-rag-with-synthetic-data-e534409a3971)
    或 [SBERTs GenQ方法](https://www.sbert.net/examples/unsupervised_learning/query_generation/README.html#bi-encoder-training)，其中主要是Bi-Encoder训练部分相关。
- en: '**5\. Vector databases: The home of embeddings**'
  id: totrans-90
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**5\. 向量数据库：嵌入的家园**'
- en: '![](../Images/ed7609bab1bea443496512fadd371f33.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ed7609bab1bea443496512fadd371f33.png)'
- en: Embeddings are stored in a database for retrieval (Image generated by author
    w. Dall-E 3)
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入存储在数据库中以供检索（图像由作者通过Dall-E 3生成）
- en: 'After loading, formatting, splitting your data, and selecting an embedding
    model, the next step in your RAG system setup is to embed the data and store these
    vector embeddings for retrieval. Most platforms, including LangChain and LlamaIndex,
    provide integrated local storage solutions, using vector databases like Qdrant,
    Milvus, Chroma DB or offer direct integration with cloud-based storage options
    such as Pinecone or ActiveLoop. The choice of vector storage is generally unaffected
    by whether your data is in English or another language. For a comprehensive understanding
    of storage and search options, including vector databases, I recommend exploring
    existing resources, such as this detailed introduction: [All You Need to Know
    About Vector Databases and How to Use Them to Augment Your LLM Apps](/all-you-need-to-know-about-vector-databases-and-how-to-use-them-to-augment-your-llm-apps-596f39adfedb#f02b).
    This guide will provide you with the necessary insights to effectively manage
    the storage aspect of your RAG system.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在加载、格式化、拆分数据并选择嵌入模型之后，RAG 系统设置的下一步是嵌入数据并存储这些向量嵌入以供检索。大多数平台，包括 LangChain 和 LlamaIndex，都提供了集成的本地存储解决方案，使用像
    Qdrant、Milvus、Chroma DB 这样的向量数据库，或者直接与基于云的存储选项如 Pinecone 或 ActiveLoop 集成。向量存储的选择通常不受数据语言（英语或其他语言）的影响。为了全面了解存储和搜索选项，包括向量数据库，我推荐你探索现有资源，例如这个详细介绍：[关于向量数据库及其如何增强你的
    LLM 应用程序的全部知识](/all-you-need-to-know-about-vector-databases-and-how-to-use-them-to-augment-your-llm-apps-596f39adfedb#f02b)。这个指南将为你提供有效管理
    RAG 系统存储方面的必要见解。
- en: At this point, you have successfully created the knowledge base that will serve
    as the brain of the retrieval system.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，你已经成功创建了作为检索系统“大脑”的知识库。
- en: '![](../Images/34c0e985a66083b497a9e454d51b7d0a.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/34c0e985a66083b497a9e454d51b7d0a.png)'
- en: Generating responses (Image generated by author w. Dall-E 3)
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 生成响应（图像由作者 w. Dall-E 3 生成）
- en: '6\. The generative phase: Go read elsewhere 😉'
  id: totrans-97
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 6. 生成阶段：去其他地方阅读 😉
- en: The second half of the RAG system, the generative phase, is equally important
    in ensuring a successful solution. Strictly speaking, it’s a search optimization
    problem with a sprinkle of LLM on top, where the considerations are less language-dependent.
    This means that guides for English retrieval optimization are generally applicable
    to other languages as well, hence it is not included here.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: RAG 系统的第二部分，生成阶段，在确保解决方案成功方面同样重要。严格来说，这是一个搜索优化问题，上面加上了一些 LLM，考虑因素较少依赖语言。这意味着针对英语的检索优化指南通常也适用于其他语言，因此在此未包含。
- en: 'In its simplest form, the generative phase involves a straightforward process:
    taking a user’s question, embedding it using the selected embedding model from
    step 4, performing a vector similarity search in the newly created database, and
    finally feeding the relevant text chunks to the LLM. This allows the system to
    respond to the query in natural language. However, to achieve a high-performing
    RAG system, several adjustments on the retrieval side are necessary such as re-ranking,
    filtering and much more. For further insights, I recommend exploring articles
    such as [10 ways to improve the performance of retrieval augmented generation
    systems](/10-ways-to-improve-the-performance-of-retrieval-augmented-generation-systems-5fa2cee7cd5c)
    or [Improving Retrieval performance in RAG pipelines with Hybrid Search](/improving-retrieval-performance-in-rag-pipelines-with-hybrid-search-c75203c2f2f5)'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在最简单的形式中，生成阶段涉及一个直接的过程：获取用户的问题，使用第 4 步中选择的嵌入模型进行嵌入，在新创建的数据库中执行向量相似度搜索，然后将相关的文本块提供给
    LLM。这使得系统能够用自然语言回应查询。然而，要实现高性能的 RAG 系统，需要在检索方面进行若干调整，如重新排序、过滤等。有关更多见解，我建议你探索一些文章，例如
    [提升检索增强生成系统性能的 10 种方法](/10-ways-to-improve-the-performance-of-retrieval-augmented-generation-systems-5fa2cee7cd5c)
    或 [通过混合搜索改进 RAG 管道中的检索性能](/improving-retrieval-performance-in-rag-pipelines-with-hybrid-search-c75203c2f2f5)。
- en: '**Outro: Evaluating your RAG system**'
  id: totrans-100
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**结语：评估你的 RAG 系统**'
- en: '![](../Images/0943409b8b61f1ab7294a1bf17066d1c.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0943409b8b61f1ab7294a1bf17066d1c.png)'
- en: What are the right choices? (Image generated by author w. Dall-E 3)
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 正确的选择是什么？（图像由作者 w. Dall-E 3 生成）
- en: So what do you do from here, what is the right configuration for your exact
    problem, and language?
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 那么你接下来该做什么？针对你的具体问题和语言，正确的配置是什么？
- en: As it might be clear at this point, deciding on the optimal settings for your
    RAG system can be a complex task due to the numerous variables involved. A custom
    query & context benchmark is essential to evaluate different configurations, especially
    since a pre-existing benchmark for your specific multilingual dataset and use
    case is very unlikely to exist.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 现在可能已经很清楚，决定RAG系统的最佳设置可能是一项复杂的任务，因为涉及的变量众多。定制的查询和上下文基准对于评估不同配置至关重要，特别是因为针对你特定的多语言数据集和用例的现有基准非常不可能存在。
- en: Thankfully, with Large Language Models (LLMs), creating a tailored benchmark
    dataset has become feasible. A benchmark for retrieval systems typically comprises
    search queries and their corresponding context (the text chunks we split in step
    4). If you have the raw data, LLMs can automate the generation of fictional queries
    related to your dataset. [Tools like LlamaIndex provide built-in functions for
    this purpose](https://docs.llamaindex.ai/en/stable/examples/evaluation/retrieval/retriever_eval.html).
    By generating custom queries, you can systematically test how adjustments in the
    embedding model, chunk size, or data formatting impact the retrieval performance
    for your specific scenario.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，凭借大型语言模型（LLMs），创建定制的基准数据集已变得可行。检索系统的基准通常包括搜索查询及其对应的上下文（我们在第4步中拆分的文本块）。如果你拥有原始数据，LLMs可以自动生成与数据集相关的虚构查询。[像LlamaIndex这样的工具提供了内置功能来实现这一目的](https://docs.llamaindex.ai/en/stable/examples/evaluation/retrieval/retriever_eval.html)。通过生成自定义查询，你可以系统地测试嵌入模型、块大小或数据格式的调整对你特定场景下检索性能的影响。
- en: Creating a representative evaluation benchmark has a fair amount of do’s and
    dont’s involved, and in early 2024 I will follow up with a separate post on how
    to create a well performing retrieval benchmark — stay tuned!
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个具有代表性的评估基准涉及许多注意事项，2024年初我将跟进一篇关于如何创建一个表现良好的检索基准的单独文章——敬请期待！
- en: Thanks for taking the time to read this post, I hope you have found the article
    useful.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 感谢你抽出时间阅读这篇文章，希望你觉得这篇文章对你有所帮助。
- en: '**Remember to throw some 👏👏👏 if the content was of help, and feel free to reach
    out if you have questions or comments to the post.**'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '**如果内容对你有帮助，请记得点赞👏👏👏，如有问题或评论，请随时与我联系。**'
- en: '**References:**'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '**参考文献：**'
- en: '[Evaluating the Ideal Chunk Size for RAG System using Llamaindex](https://blog.llamaindex.ai/evaluating-the-ideal-chunk-size-for-a-rag-system-using-llamaindex-6207e5d3fec5)'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用Llamaindex评估RAG系统的理想块大小](https://blog.llamaindex.ai/evaluating-the-ideal-chunk-size-for-a-rag-system-using-llamaindex-6207e5d3fec5)'
- en: '[Building RAG-based LLM Applications for Production](https://www.anyscale.com/blog/a-comprehensive-guide-for-building-rag-based-llm-applications-part-1?utm_source=gradientflow&utm_medium=newsletter#chunk-data)'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[构建基于RAG的LLM应用程序以投入生产](https://www.anyscale.com/blog/a-comprehensive-guide-for-building-rag-based-llm-applications-part-1?utm_source=gradientflow&utm_medium=newsletter#chunk-data)'
- en: '[How to chunk text a comparative analysis](/how-to-chunk-text-data-a-comparative-analysis-3858c4a0997a)'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[如何分块文本数据的比较分析](/how-to-chunk-text-data-a-comparative-analysis-3858c4a0997a)'
- en: '[Massive Text Embedding Benchmark (MTEB)](https://huggingface.co/spaces/mteb/leaderboard)'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[大规模文本嵌入基准 (MTEB)](https://huggingface.co/spaces/mteb/leaderboard)'
- en: '[SBERT on Asymmetric retrieval](https://www.sbert.net/examples/applications/semantic-search/README.html)'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[SBERT在非对称检索中的应用](https://www.sbert.net/examples/applications/semantic-search/README.html)'
- en: '[Finetuning embeddings using LlamaIndex](https://blog.llamaindex.ai/fine-tuning-embeddings-for-rag-with-synthetic-data-e534409a3971)'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用LlamaIndex微调嵌入](https://blog.llamaindex.ai/fine-tuning-embeddings-for-rag-with-synthetic-data-e534409a3971)'
- en: '[Finetuning embeddings using SBERTs GenQ approach](https://www.sbert.net/examples/unsupervised_learning/query_generation/README.html#bi-encoder-training)'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用SBERT的GenQ方法微调嵌入](https://www.sbert.net/examples/unsupervised_learning/query_generation/README.html#bi-encoder-training)'
- en: '[All You Need to Know About Vector Databases and How to Use Them to Augment
    Your LLM Apps](/all-you-need-to-know-about-vector-databases-and-how-to-use-them-to-augment-your-llm-apps-596f39adfedb#f02b)'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[关于向量数据库及其如何增强你的LLM应用程序的所有信息](/all-you-need-to-know-about-vector-databases-and-how-to-use-them-to-augment-your-llm-apps-596f39adfedb#f02b)'
- en: '[10 ways to improve the performance of retrieval augmented generation systems](/10-ways-to-improve-the-performance-of-retrieval-augmented-generation-systems-5fa2cee7cd5c)'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[提高检索增强生成系统性能的10种方法](/10-ways-to-improve-the-performance-of-retrieval-augmented-generation-systems-5fa2cee7cd5c)'
- en: '[Improving Retrieval performance in RAG pipelines with Hybrid Search](/improving-retrieval-performance-in-rag-pipelines-with-hybrid-search-c75203c2f2f5)'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[通过混合搜索提高RAG管道中的检索性能](/improving-retrieval-performance-in-rag-pipelines-with-hybrid-search-c75203c2f2f5)'
- en: '[Evaluating retrieval performance of RAG systems using LlamaIndex](https://docs.llamaindex.ai/en/stable/examples/evaluation/retrieval/retriever_eval.html)'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用LlamaIndex评估RAG系统的检索性能](https://docs.llamaindex.ai/en/stable/examples/evaluation/retrieval/retriever_eval.html)'
