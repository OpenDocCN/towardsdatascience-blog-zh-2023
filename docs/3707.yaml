- en: 'Beyond English: Implementing a multilingual RAG solution'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/beyond-english-implementing-a-multilingual-rag-solution-12ccba0428b6?source=collection_archive---------0-----------------------#2023-12-20](https://towardsdatascience.com/beyond-english-implementing-a-multilingual-rag-solution-12ccba0428b6?source=collection_archive---------0-----------------------#2023-12-20)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: An introduction to the do’s and don’ts when implementing a non-english Retrieval
    Augmented Generation (RAG) system
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@jalkestrup?source=post_page-----12ccba0428b6--------------------------------)[![Jesper
    Alkestrup](../Images/199803c75758a9b943e72746105f3de5.png)](https://medium.com/@jalkestrup?source=post_page-----12ccba0428b6--------------------------------)[](https://towardsdatascience.com/?source=post_page-----12ccba0428b6--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----12ccba0428b6--------------------------------)
    [Jesper Alkestrup](https://medium.com/@jalkestrup?source=post_page-----12ccba0428b6--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F84e00f9ebe19&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbeyond-english-implementing-a-multilingual-rag-solution-12ccba0428b6&user=Jesper+Alkestrup&userId=84e00f9ebe19&source=post_page-84e00f9ebe19----12ccba0428b6---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----12ccba0428b6--------------------------------)
    ·18 min read·Dec 20, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F12ccba0428b6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbeyond-english-implementing-a-multilingual-rag-solution-12ccba0428b6&user=Jesper+Alkestrup&userId=84e00f9ebe19&source=-----12ccba0428b6---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F12ccba0428b6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbeyond-english-implementing-a-multilingual-rag-solution-12ccba0428b6&source=-----12ccba0428b6---------------------bookmark_footer-----------)![](../Images/4b0d38957430dc14c526618b898f1182.png)'
  prefs: []
  type: TYPE_NORMAL
- en: RAG, an all knowing colleague, available 24/7 (Image generated by author w.
    Dall-E 3)
  prefs: []
  type: TYPE_NORMAL
- en: TLDR
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This article provides an introduction to the considerations one should take
    into account when developing non-English RAG systems, complete with specific examples
    and techniques. Some of the key points include:'
  prefs: []
  type: TYPE_NORMAL
- en: Prioritize maintaining syntactic structure during data loading, as it is crucial
    for meaningful text segmentation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Format documents using simple delimiters like \n\n to facilitate efficient text
    splitting.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Opt for rule-based text splitters, given the computational intensity and subpar
    performance of ML-based semantic splitters in multilingual contexts.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In selecting an embedding model, consider both its multilingual capabilities
    and asymmetric retrieval performance.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For multilingual projects, fine-tuning an embedding model with a Large Language
    Model (LLM) can enhance performance, and may be needed to achieve sufficient accuracy.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing an LLM-based retrieval evaluation benchmark is strongly recommended
    to fine-tune the hyperparameters of your RAG system effectively, and can be done
    easily with existing frameworks.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is no wonder that RAG has become the trendiest term within search technology
    in 2023\. Retrieval Augmented Generation (RAG) is transforming how organizations
    utilize their vast quantity of existing data to power intelligent ChatBots. These
    bots, capable of conversations in natural language, can draw on an organization’s
    collective knowledge to function as an always-available, in-house expert to deliver
    relevant answers, grounded in verified data. While a considerable number of resources
    are available on building RAG systems, most are geared toward the English language,
    leaving a gap for smaller languages.
  prefs: []
  type: TYPE_NORMAL
- en: This 6-step easy-to-follow guide will walk you through the do’s and don’ts when
    creating RAG systems for non-English languages.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: RAG structure, a brief recap
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This article presumes familiarity with concepts like embeddings, vectors, and
    tokens. For those needing a brief refresher on the architecture of RAG systems,
    they essentially consist of two core components:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Indexing phase (the focus of this article): This initial stage involves processing
    the input data. The data is first loaded, appropriately formatted, then split.
    Later, it undergoes vectorization through embedding techniques, culminating in
    its storage within a knowledge base for future retrieval.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Generative phase: In this phase, a user’s query is input to the retrieval system.
    This system then extracts relevant information snippets from the knowledge base.
    Leveraging a Large Language Model (LLM), the system interprets this data to formulate
    a coherent, natural language response, effectively addressing the user’s inquiry.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](../Images/0e5538579e680f53cdcb9062d811cc75.png)'
  prefs: []
  type: TYPE_IMG
- en: Now let’s get started!
  prefs: []
  type: TYPE_NORMAL
- en: '*Disclaimer:*'
  prefs: []
  type: TYPE_NORMAL
- en: '*This guide doesn’t aim to be an exhaustive manual on using any particular
    tool. Instead, its purpose is to shed light on the overarching decisions that
    should guide your tool selection. In practice, I strongly recommend leveraging
    an established framework for constructing your system’s foundation. For building
    RAG systems, I would personally recommend LlamaIndex as they provide detailed
    guides and features focused strictly on indexing and retrieval optimization.*'
  prefs: []
  type: TYPE_NORMAL
- en: '*Additionally, this guide is written with the assumption that we’re dealing
    with languages that use the latin script and read from left to right. This includes
    languages like German, French, Spanish, Czech, , Turkish, Vietnamese, Norwegian,
    Polish, and quite a few others. Languages outside of this group may have different
    needs and considerations.*'
  prefs: []
  type: TYPE_NORMAL
- en: '1\. Data loader: The devil’s in the details'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/ff11a5cb9d6053f2e24509540b6b2c14.png)'
  prefs: []
  type: TYPE_IMG
- en: A cool looking multi-modal dataloader (Image generated by author w. Dall-E 3)
  prefs: []
  type: TYPE_NORMAL
- en: The first step in a RAG system involves using a dataloader to handle diverse
    formats, from text documents to multimedia, extracting all relevant content for
    further processing. For text-based formats, dataloaders typically perform consistently
    across languages, as they don’t involve language-specific processing. With the
    advent of multi-modal RAG systems, it is however crucial to be aware of the reduced
    performance of speech to text models compared to their English counterparts. Models
    like [Whisper v3](https://github.com/openai/whisper) demonstrate impressive multilingual
    capabilities, but it’s wise to check out their performance on benchmarks like
    [Mozilla Common Voice](https://commonvoice.mozilla.org/en/datasets) or the [Fleurs](https://huggingface.co/datasets/google/fleurs)
    dataset, and ideally evaluate those on your own benchmark.
  prefs: []
  type: TYPE_NORMAL
- en: For the remainder of this article, we’ll however concentrate on text-based inputs.
  prefs: []
  type: TYPE_NORMAL
- en: Why retaining syntactic structure is important
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A key aspect of data loading is to preserve the original data’s syntactic integrity.
    The loss of elements such as headers or paragraph structures can impact the accuracy
    of subsequent information retrieval. This concern is heightened for non-English
    languages due to the limited availability of machine learning-based segmentation
    tools.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Syntactic information plays a crucial role because the effectiveness of RAG
    systems in delivering meaningful answers depends partly on their ability to split
    data into semantically accurate subsections.
  prefs: []
  type: TYPE_NORMAL
- en: To highlight the differences between a data loading approach that retains the
    structure and one that does not, let’s take the example of using a basic HTML
    dataloader versus a PDF loader on a [medium article](https://blog.llamaindex.ai/boosting-rag-picking-the-best-embedding-reranker-models-42d079022e83).
    Libraries such as [LangChain](https://python.langchain.com/docs/modules/data_connection/document_loaders/html)
    and [LlamaIndex](https://llamahub.ai/l/web-beautiful_soup_web) both rely on the
    exact same libraries, but just wrap the functions in their own document classes
    (Requests+BS4 for web, PyPDF2 for PDFs).
  prefs: []
  type: TYPE_NORMAL
- en: '**HTML Dataloader: This method retains the syntactic structure of the content.**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '**PDF data loader, example in which syntactic information is lost** (saved
    article as PDF, then re-loaded)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Upon initial review, the PDF dataloader’s output appears more readable, but
    closer inspection reveals a loss of structural information — how would one tell
    what is a header, and where a section ends? In contrast, the HTML file retains
    all the relevant structure.
  prefs: []
  type: TYPE_NORMAL
- en: Ideally, you want to retain all original formatting in the data loader, and
    only decide on filtering and reformatting in the next step. However, that might
    involve building custom data loaders for your use case, and in some cases be impossible.
    I recommend to simply start with a standard data loader, but spend a few minutes
    to inspect examples of the loaded data carefully and understand what structure
    has been lost.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding what syntactic that is lost is crucial, as it guides potential
    improvements if the system’s downstream retrieval performance needs enhancement,
    allowing for targeted refinements.
  prefs: []
  type: TYPE_NORMAL
- en: '2\. Data formatting: Boring… but important'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/82a92d20f1ab6a15b7c2408f5cb9668f.png)'
  prefs: []
  type: TYPE_IMG
- en: Document chunking (Image generated by author w. Dall-E 3)
  prefs: []
  type: TYPE_NORMAL
- en: The second step, formatting, serves the primary purpose of uniforming the data
    from your data loaders in a way that prepares the data for the next step of text
    splitting. As the following section explains, dividing the input text into a myriad
    of smaller chunks will be necessary. A successful formatting sets up the text
    in a way that provides the best possible conditions for dividing the content into
    semantically meaningful chunks. Simply put, your goal is to transform the potentially
    complex syntactic structure retrieved from a html or a markdown file, into a plain
    text file with basic delimiters such as /n (line change) and /n/n (end of section)
    to guide the text splitter.
  prefs: []
  type: TYPE_NORMAL
- en: 'A simple function to format the BS4 HTML object into a dictionary with title
    and text could look like the below:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: For complex RAG systems where there might be multiple correct answers relative
    to the context, storing additional information like document titles or headers
    as metadata along the text chunks is beneficial. This metadata can be used later
    for filtering, and if available, formatting elements like headers should influence
    your chunking strategy. A library like LlamaIndex natively work with the concept
    of metadata and text wrapped together in Node objects, and I highly recommend
    using this or a similar framework
  prefs: []
  type: TYPE_NORMAL
- en: Now that we’ve done our formatting correctly, let’s dive into the key aspects
    of text splitting!
  prefs: []
  type: TYPE_NORMAL
- en: '3: Text splitting: Size matters'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/a4f4db130ed93d7637ff014bd449b7a6.png)'
  prefs: []
  type: TYPE_IMG
- en: Splitting text, the simple way (Image generated by author w. Dall-E 3)
  prefs: []
  type: TYPE_NORMAL
- en: When preparing data for embedding and retrieval in a RAG system, splitting the
    text into appropriately sized chunks is crucial. This process is guided by two
    main factors, Model Constraints and Retrieval Effectiveness.
  prefs: []
  type: TYPE_NORMAL
- en: '**Model Constraints**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Embedding models have a maximum token length for input; anything beyond this
    limit gets truncated. Be aware of your chosen model’s limitations and ensure that
    each data chunk doesn’t exceed this max token length.
  prefs: []
  type: TYPE_NORMAL
- en: Multilingual models, in particular, often have shorter sequence limits compared
    to their English counterparts. For instance, the widely used Paraphrase multilingual
    MiniLM-L12 v2 model has a maximum context window of just 128 tokens.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Also, consider the text length the model was trained on — some models might
    technically accept longer inputs but were trained on shorter chunks, which could
    affect performance on longer texts. One such is example, is the [Multi QA base
    from SBERT](https://huggingface.co/sentence-transformers/multi-qa-mpnet-base-dot-v1)
    as seen below,
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e0d8262a6f1b3b939051aba7f8901fbb.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Retrieval effectiveness**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: While chunking data to the model’s maximum length seems logical, it might not
    always lead to the best retrieval outcomes. Larger chunks offer more context for
    the LLM but can obscure key details, making it harder to retrieve precise matches.
    Conversely, smaller chunks can enhance match accuracy but might lack the context
    needed for complete answers. Hybrid approaches use smaller chunks for search but
    include surrounding context at query time for balance.
  prefs: []
  type: TYPE_NORMAL
- en: While there isn’t a definitive answer regarding chunk size, the considerations
    for chunk size remain consistent whether you’re working on multilingual or English
    projects. I would recommend reading further on the topic from resources such as
    [Evaluating the Ideal Chunk Size for RAG System using Llamaindex](https://blog.llamaindex.ai/evaluating-the-ideal-chunk-size-for-a-rag-system-using-llamaindex-6207e5d3fec5)
    or [Building RAG-based LLM Applications for Production](https://www.anyscale.com/blog/a-comprehensive-guide-for-building-rag-based-llm-applications-part-1?utm_source=gradientflow&utm_medium=newsletter#chunk-data).
  prefs: []
  type: TYPE_NORMAL
- en: '**Text splitting: Methods for splitting text**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Text can be split using various methods](/how-to-chunk-text-data-a-comparative-analysis-3858c4a0997a),
    mainly falling into two categories: rule-based (focusing on character analysis)
    and machine learning-based models. ML approaches, from simple NLTK & Spacy tokenizers
    to advanced transformer models, often depend on language-specific training, primarily
    in English. Although simple models like NLTK & Spacy support multiple languages,
    they mainly address sentence splitting, not semantic sectioning.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Since ML based sentence splitters currently work poorly for most non-English
    languages, and are compute intensive, I recommend starting with a simple rule-based
    splitter. If you’ve preserved relevant syntactic structure from the original data,
    and formatted the data correctly, the result will be of good quality.*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: A common and effective method is a recursive character text splitter, like those
    used in [LangChain](https://python.langchain.com/docs/modules/data_connection/document_transformers/text_splitters/recursive_text_splitter)
    or LlamaIndex, which shortens sections by finding the nearest split character
    in a prioritized sequence (e.g., \n\n, \n, ., ?, !).
  prefs: []
  type: TYPE_NORMAL
- en: 'Taking the formatted text from the previous section, an example of using LangChains
    recursive character splitter would look like:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Here it’s important to note that one should define the tokenizer as the embedding
    model intended to use, since different models ‘count’ the words differently. The
    function will now, in a prioritized order, split any text longer than 128 tokens
    first by the \n\n we introduced at end of sections, and if that is not possible,
    then by end of paragraphs delimited by \n and so forth. The first 3 chunks will
    be:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Now that we have successfully split the text in a semantically meaningful way,
    we can move onto the final part of embedding these chunks for storage.
  prefs: []
  type: TYPE_NORMAL
- en: '**4\. Embedding Models: Navigating the jungle**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/15ebcd61fd4e03d1657cceb920ad2483.png)'
  prefs: []
  type: TYPE_IMG
- en: Embedding models convert text to vectors (Image generated by author w. Dall-E
    3)
  prefs: []
  type: TYPE_NORMAL
- en: Choosing the right embedding model is critical for the success of a Retrieval
    Augmented Generation (RAG) system, and something that is less straight forward
    than for the English language. A comprehensive resource for comparing models is
    the [Massive Text Embedding Benchmark (MTEB)](https://huggingface.co/spaces/mteb/leaderboard),
    which includes benchmarks for over 100 languages.
  prefs: []
  type: TYPE_NORMAL
- en: The model of your choice must either be multilingual or specifically tailored
    to the language you’re working with (monolingual). Remember, the latest high-performing
    models are often English-centric and may not work well with other languages.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/35f837806d09da41b4fdced9b6bc0438.png)'
  prefs: []
  type: TYPE_IMG
- en: If available, refer to language-specific benchmarks relevant to your task. For
    instance, in classification tasks, there are over 50 language-specific benchmarks,
    aiding in selecting the most efficient model for languages ranging from Danish
    to Spanish. However, it’s important to note that these benchmarks may not directly
    indicate a model’s efficiency in retrieving relevant information for a RAG system,
    because retrieval is different from classification, clustering or another task.
    The task is to find models trained for asymmetric search, as those not trained
    for this specific task might inaccurately prioritize shorter passages over longer,
    more relevant ones.
  prefs: []
  type: TYPE_NORMAL
- en: The model should excel in [asymmetric retrieval](https://www.sbert.net/examples/applications/semantic-search/README.html),
    matching short queries to longer text chunks. The reason why is that, in a RAG
    system, you often match a brief query to more extensive passages to extract meaningful
    answers. The MTEB benchmarks related to asymmetric search are listed under the
    Retrieval. A challenge is that as of November 2023, MTEB’s Retrieval benchmark
    includes only English, Chinese, and Polish.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: When dealing with languages like Norwegian, where there may not be specific
    retrieval benchmarks, you might wonder whether to choose the best-performing model
    from classification benchmarks or a general multilingual model proficient in English
    retrieval?
  prefs: []
  type: TYPE_NORMAL
- en: As for practical advice, a simple rule of thumb is to opt for the top-performing
    multilingual model in the MTEB Retrieval benchmark. Beware that the retrieval
    score itself, is however still based on English, so benchmarking on your own language
    is needed to qualify the performance (step 6). As of December 2023, the E5-multilingual
    family is a strong choice for an open source model. The model is fine-tuned for
    asymmetric search, and by tagging texts as ‘query’ or ‘passage’ before embedding,
    it optimizes the retrieval process by considering the nature of the input. This
    approach ensures a more effective match between queries and relevant information
    in your knowledge base, enhancing the overall performance of your RAG system.
    As seen on the benchmark, the cohere-embed-multilingual-v3.0 likely has better
    performance, but has to be paid for.
  prefs: []
  type: TYPE_NORMAL
- en: The step of embedding is often done as part of storing the documents in a vector
    DB, but a simple example of embedding all the split sentences using the E5 family
    can be done as below using the Sentence Transformer library.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: If off the shelf embeddings turn out not to provide sufficient performance for
    your specific retrieval domain, fear not. With the advent of LLMs it has now become
    feasible to auto-generate training-data from your existing corpus, and increase
    the performance of up to 5–10% by fine-tuning an existing embedding on your own
    data, [LlamaIndex provides a guide here](https://blog.llamaindex.ai/fine-tuning-embeddings-for-rag-with-synthetic-data-e534409a3971)
    or [SBERTs GenQ approach](https://www.sbert.net/examples/unsupervised_learning/query_generation/README.html#bi-encoder-training)
    where mainly the Bi-Encoder training part is relevant.
  prefs: []
  type: TYPE_NORMAL
- en: '**5\. Vector databases: The home of embeddings**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/ed7609bab1bea443496512fadd371f33.png)'
  prefs: []
  type: TYPE_IMG
- en: Embeddings are stored in a database for retrieval (Image generated by author
    w. Dall-E 3)
  prefs: []
  type: TYPE_NORMAL
- en: 'After loading, formatting, splitting your data, and selecting an embedding
    model, the next step in your RAG system setup is to embed the data and store these
    vector embeddings for retrieval. Most platforms, including LangChain and LlamaIndex,
    provide integrated local storage solutions, using vector databases like Qdrant,
    Milvus, Chroma DB or offer direct integration with cloud-based storage options
    such as Pinecone or ActiveLoop. The choice of vector storage is generally unaffected
    by whether your data is in English or another language. For a comprehensive understanding
    of storage and search options, including vector databases, I recommend exploring
    existing resources, such as this detailed introduction: [All You Need to Know
    About Vector Databases and How to Use Them to Augment Your LLM Apps](/all-you-need-to-know-about-vector-databases-and-how-to-use-them-to-augment-your-llm-apps-596f39adfedb#f02b).
    This guide will provide you with the necessary insights to effectively manage
    the storage aspect of your RAG system.'
  prefs: []
  type: TYPE_NORMAL
- en: At this point, you have successfully created the knowledge base that will serve
    as the brain of the retrieval system.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/34c0e985a66083b497a9e454d51b7d0a.png)'
  prefs: []
  type: TYPE_IMG
- en: Generating responses (Image generated by author w. Dall-E 3)
  prefs: []
  type: TYPE_NORMAL
- en: '6\. The generative phase: Go read elsewhere 😉'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The second half of the RAG system, the generative phase, is equally important
    in ensuring a successful solution. Strictly speaking, it’s a search optimization
    problem with a sprinkle of LLM on top, where the considerations are less language-dependent.
    This means that guides for English retrieval optimization are generally applicable
    to other languages as well, hence it is not included here.
  prefs: []
  type: TYPE_NORMAL
- en: 'In its simplest form, the generative phase involves a straightforward process:
    taking a user’s question, embedding it using the selected embedding model from
    step 4, performing a vector similarity search in the newly created database, and
    finally feeding the relevant text chunks to the LLM. This allows the system to
    respond to the query in natural language. However, to achieve a high-performing
    RAG system, several adjustments on the retrieval side are necessary such as re-ranking,
    filtering and much more. For further insights, I recommend exploring articles
    such as [10 ways to improve the performance of retrieval augmented generation
    systems](/10-ways-to-improve-the-performance-of-retrieval-augmented-generation-systems-5fa2cee7cd5c)
    or [Improving Retrieval performance in RAG pipelines with Hybrid Search](/improving-retrieval-performance-in-rag-pipelines-with-hybrid-search-c75203c2f2f5)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Outro: Evaluating your RAG system**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/0943409b8b61f1ab7294a1bf17066d1c.png)'
  prefs: []
  type: TYPE_IMG
- en: What are the right choices? (Image generated by author w. Dall-E 3)
  prefs: []
  type: TYPE_NORMAL
- en: So what do you do from here, what is the right configuration for your exact
    problem, and language?
  prefs: []
  type: TYPE_NORMAL
- en: As it might be clear at this point, deciding on the optimal settings for your
    RAG system can be a complex task due to the numerous variables involved. A custom
    query & context benchmark is essential to evaluate different configurations, especially
    since a pre-existing benchmark for your specific multilingual dataset and use
    case is very unlikely to exist.
  prefs: []
  type: TYPE_NORMAL
- en: Thankfully, with Large Language Models (LLMs), creating a tailored benchmark
    dataset has become feasible. A benchmark for retrieval systems typically comprises
    search queries and their corresponding context (the text chunks we split in step
    4). If you have the raw data, LLMs can automate the generation of fictional queries
    related to your dataset. [Tools like LlamaIndex provide built-in functions for
    this purpose](https://docs.llamaindex.ai/en/stable/examples/evaluation/retrieval/retriever_eval.html).
    By generating custom queries, you can systematically test how adjustments in the
    embedding model, chunk size, or data formatting impact the retrieval performance
    for your specific scenario.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a representative evaluation benchmark has a fair amount of do’s and
    dont’s involved, and in early 2024 I will follow up with a separate post on how
    to create a well performing retrieval benchmark — stay tuned!
  prefs: []
  type: TYPE_NORMAL
- en: Thanks for taking the time to read this post, I hope you have found the article
    useful.
  prefs: []
  type: TYPE_NORMAL
- en: '**Remember to throw some 👏👏👏 if the content was of help, and feel free to reach
    out if you have questions or comments to the post.**'
  prefs: []
  type: TYPE_NORMAL
- en: '**References:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[Evaluating the Ideal Chunk Size for RAG System using Llamaindex](https://blog.llamaindex.ai/evaluating-the-ideal-chunk-size-for-a-rag-system-using-llamaindex-6207e5d3fec5)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Building RAG-based LLM Applications for Production](https://www.anyscale.com/blog/a-comprehensive-guide-for-building-rag-based-llm-applications-part-1?utm_source=gradientflow&utm_medium=newsletter#chunk-data)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[How to chunk text a comparative analysis](/how-to-chunk-text-data-a-comparative-analysis-3858c4a0997a)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Massive Text Embedding Benchmark (MTEB)](https://huggingface.co/spaces/mteb/leaderboard)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[SBERT on Asymmetric retrieval](https://www.sbert.net/examples/applications/semantic-search/README.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Finetuning embeddings using LlamaIndex](https://blog.llamaindex.ai/fine-tuning-embeddings-for-rag-with-synthetic-data-e534409a3971)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Finetuning embeddings using SBERTs GenQ approach](https://www.sbert.net/examples/unsupervised_learning/query_generation/README.html#bi-encoder-training)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[All You Need to Know About Vector Databases and How to Use Them to Augment
    Your LLM Apps](/all-you-need-to-know-about-vector-databases-and-how-to-use-them-to-augment-your-llm-apps-596f39adfedb#f02b)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[10 ways to improve the performance of retrieval augmented generation systems](/10-ways-to-improve-the-performance-of-retrieval-augmented-generation-systems-5fa2cee7cd5c)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Improving Retrieval performance in RAG pipelines with Hybrid Search](/improving-retrieval-performance-in-rag-pipelines-with-hybrid-search-c75203c2f2f5)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Evaluating retrieval performance of RAG systems using LlamaIndex](https://docs.llamaindex.ai/en/stable/examples/evaluation/retrieval/retriever_eval.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
