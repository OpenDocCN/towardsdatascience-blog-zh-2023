- en: Host Hundreds of NLP Models Utilizing SageMaker Multi-Model Endpoints Backed
    By GPU Instances
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 托管数百个 NLP 模型，利用 SageMaker 多模型端点和 GPU 实例
- en: 原文：[https://towardsdatascience.com/host-hundreds-of-nlp-models-utilizing-sagemaker-multi-model-endpoints-backed-by-gpu-instances-1ec215886248?source=collection_archive---------8-----------------------#2023-09-22](https://towardsdatascience.com/host-hundreds-of-nlp-models-utilizing-sagemaker-multi-model-endpoints-backed-by-gpu-instances-1ec215886248?source=collection_archive---------8-----------------------#2023-09-22)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/host-hundreds-of-nlp-models-utilizing-sagemaker-multi-model-endpoints-backed-by-gpu-instances-1ec215886248?source=collection_archive---------8-----------------------#2023-09-22](https://towardsdatascience.com/host-hundreds-of-nlp-models-utilizing-sagemaker-multi-model-endpoints-backed-by-gpu-instances-1ec215886248?source=collection_archive---------8-----------------------#2023-09-22)
- en: Integrate Triton Inference Server With Amazon SageMaker
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将 Triton 推理服务器与 Amazon SageMaker 集成
- en: '[](https://ram-vegiraju.medium.com/?source=post_page-----1ec215886248--------------------------------)[![Ram
    Vegiraju](../Images/07d9334e905f710d9f3c6187cf69a1a5.png)](https://ram-vegiraju.medium.com/?source=post_page-----1ec215886248--------------------------------)[](https://towardsdatascience.com/?source=post_page-----1ec215886248--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----1ec215886248--------------------------------)
    [Ram Vegiraju](https://ram-vegiraju.medium.com/?source=post_page-----1ec215886248--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://ram-vegiraju.medium.com/?source=post_page-----1ec215886248--------------------------------)[![Ram
    Vegiraju](../Images/07d9334e905f710d9f3c6187cf69a1a5.png)](https://ram-vegiraju.medium.com/?source=post_page-----1ec215886248--------------------------------)[](https://towardsdatascience.com/?source=post_page-----1ec215886248--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----1ec215886248--------------------------------)
    [Ram Vegiraju](https://ram-vegiraju.medium.com/?source=post_page-----1ec215886248--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F6e49569edd2b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhost-hundreds-of-nlp-models-utilizing-sagemaker-multi-model-endpoints-backed-by-gpu-instances-1ec215886248&user=Ram+Vegiraju&userId=6e49569edd2b&source=post_page-6e49569edd2b----1ec215886248---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----1ec215886248--------------------------------)
    ·7 min read·Sep 22, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F1ec215886248&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhost-hundreds-of-nlp-models-utilizing-sagemaker-multi-model-endpoints-backed-by-gpu-instances-1ec215886248&user=Ram+Vegiraju&userId=6e49569edd2b&source=-----1ec215886248---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F6e49569edd2b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhost-hundreds-of-nlp-models-utilizing-sagemaker-multi-model-endpoints-backed-by-gpu-instances-1ec215886248&user=Ram+Vegiraju&userId=6e49569edd2b&source=post_page-6e49569edd2b----1ec215886248---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----1ec215886248--------------------------------)
    · 7 分钟阅读 · 2023 年 9 月 22 日'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F1ec215886248&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhost-hundreds-of-nlp-models-utilizing-sagemaker-multi-model-endpoints-backed-by-gpu-instances-1ec215886248&source=-----1ec215886248---------------------bookmark_footer-----------)![](../Images/a0b73ef0eb92e36d0a4cc5bd7159c302.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F1ec215886248&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhost-hundreds-of-nlp-models-utilizing-sagemaker-multi-model-endpoints-backed-by-gpu-instances-1ec215886248&source=-----1ec215886248---------------------bookmark_footer-----------)![](../Images/a0b73ef0eb92e36d0a4cc5bd7159c302.png)'
- en: Image from [Unsplash](https://unsplash.com/photos/6b5uqlWabB0)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来自 [Unsplash](https://unsplash.com/photos/6b5uqlWabB0)
- en: In the past we’ve explored [SageMaker Multi-Model Endpoints (MME)](/deploy-multiple-tensorflow-models-to-one-endpoint-65bea81c3f2f)
    as a cost effective option to host multiple models behind a singular endpoint.
    While hosting smaller models is possible on MME with CPU based instances, as these
    models get larger and more complex in nature sometimes GPU compute may be necessary.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 过去，我们探索了 [SageMaker 多模型端点 (MME)](/deploy-multiple-tensorflow-models-to-one-endpoint-65bea81c3f2f)
    作为在单一端点后面托管多个模型的经济有效选项。虽然可以在 CPU 基础实例上托管较小的模型，但随着这些模型变得更大更复杂，有时可能需要 GPU 计算。
- en: '[MME backed by GPU](https://aws.amazon.com/about-aws/whats-new/2022/10/amazon-sagemaker-cost-effectively-host-1000s-gpu-multi-model-endpoint/)
    based instances is a specific SageMaker Inference feature that we will harness
    in this article to showcase how we can host hundreds of NLP models efficiently
    on a single endpoint. Note that at the time of this article, MME GPU on SageMaker
    currently supports the following single GPU based instance families: p2, p3, g4dn,
    and g5.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '[由 GPU 支持的 MME](https://aws.amazon.com/about-aws/whats-new/2022/10/amazon-sagemaker-cost-effectively-host-1000s-gpu-multi-model-endpoint/)
    实例是一个特定的 SageMaker 推理功能，我们将在本文中利用它展示如何在单一端点上高效地托管数百个 NLP 模型。请注意，在本文撰写时，SageMaker
    上的 MME GPU 当前支持以下单 GPU 基础实例系列：p2、p3、g4dn 和 g5。'
- en: 'MME GPU is currently also powered by two model serving stacks:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: MME GPU 当前还由两个模型服务堆栈提供支持：
- en: '[Nvidia Triton Inference Server](https://aws.amazon.com/blogs/machine-learning/run-multiple-deep-learning-models-on-gpu-with-amazon-sagemaker-multi-model-endpoints/)'
  id: totrans-12
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[Nvidia Triton 推理服务器](https://aws.amazon.com/blogs/machine-learning/run-multiple-deep-learning-models-on-gpu-with-amazon-sagemaker-multi-model-endpoints/)'
- en: '[TorchServe](https://aws.amazon.com/blogs/machine-learning/run-multiple-generative-ai-models-on-gpu-using-amazon-sagemaker-multi-model-endpoints-with-torchserve-and-save-up-to-75-in-inference-costs/)'
  id: totrans-13
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[TorchServe](https://aws.amazon.com/blogs/machine-learning/run-multiple-generative-ai-models-on-gpu-using-amazon-sagemaker-multi-model-endpoints-with-torchserve-and-save-up-to-75-in-inference-costs/)'
- en: For the purpose of this article we will be utilizing Triton Inference Server
    with a PyTorch backend to host BERT based models on our GPU instance. If you are
    new to Triton, we will have a slight primer, but I would recommend referencing
    my starter article [here](/deploying-pytorch-models-with-nvidia-triton-inference-server-bb139066a387).
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 为了本文的目的，我们将利用 Triton 推理服务器与 PyTorch 后端在我们的 GPU 实例上托管基于 BERT 的模型。如果你对 Triton
    不熟悉，我们会进行简单的介绍，但我建议你参考我的入门文章 [这里](/deploying-pytorch-models-with-nvidia-triton-inference-server-bb139066a387)。
