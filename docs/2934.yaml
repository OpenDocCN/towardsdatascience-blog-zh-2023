- en: Host Hundreds of NLP Models Utilizing SageMaker Multi-Model Endpoints Backed
    By GPU Instances
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/host-hundreds-of-nlp-models-utilizing-sagemaker-multi-model-endpoints-backed-by-gpu-instances-1ec215886248?source=collection_archive---------8-----------------------#2023-09-22](https://towardsdatascience.com/host-hundreds-of-nlp-models-utilizing-sagemaker-multi-model-endpoints-backed-by-gpu-instances-1ec215886248?source=collection_archive---------8-----------------------#2023-09-22)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Integrate Triton Inference Server With Amazon SageMaker
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://ram-vegiraju.medium.com/?source=post_page-----1ec215886248--------------------------------)[![Ram
    Vegiraju](../Images/07d9334e905f710d9f3c6187cf69a1a5.png)](https://ram-vegiraju.medium.com/?source=post_page-----1ec215886248--------------------------------)[](https://towardsdatascience.com/?source=post_page-----1ec215886248--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----1ec215886248--------------------------------)
    [Ram Vegiraju](https://ram-vegiraju.medium.com/?source=post_page-----1ec215886248--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F6e49569edd2b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhost-hundreds-of-nlp-models-utilizing-sagemaker-multi-model-endpoints-backed-by-gpu-instances-1ec215886248&user=Ram+Vegiraju&userId=6e49569edd2b&source=post_page-6e49569edd2b----1ec215886248---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----1ec215886248--------------------------------)
    ·7 min read·Sep 22, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F1ec215886248&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhost-hundreds-of-nlp-models-utilizing-sagemaker-multi-model-endpoints-backed-by-gpu-instances-1ec215886248&user=Ram+Vegiraju&userId=6e49569edd2b&source=-----1ec215886248---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F1ec215886248&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhost-hundreds-of-nlp-models-utilizing-sagemaker-multi-model-endpoints-backed-by-gpu-instances-1ec215886248&source=-----1ec215886248---------------------bookmark_footer-----------)![](../Images/a0b73ef0eb92e36d0a4cc5bd7159c302.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Image from [Unsplash](https://unsplash.com/photos/6b5uqlWabB0)
  prefs: []
  type: TYPE_NORMAL
- en: In the past we’ve explored [SageMaker Multi-Model Endpoints (MME)](/deploy-multiple-tensorflow-models-to-one-endpoint-65bea81c3f2f)
    as a cost effective option to host multiple models behind a singular endpoint.
    While hosting smaller models is possible on MME with CPU based instances, as these
    models get larger and more complex in nature sometimes GPU compute may be necessary.
  prefs: []
  type: TYPE_NORMAL
- en: '[MME backed by GPU](https://aws.amazon.com/about-aws/whats-new/2022/10/amazon-sagemaker-cost-effectively-host-1000s-gpu-multi-model-endpoint/)
    based instances is a specific SageMaker Inference feature that we will harness
    in this article to showcase how we can host hundreds of NLP models efficiently
    on a single endpoint. Note that at the time of this article, MME GPU on SageMaker
    currently supports the following single GPU based instance families: p2, p3, g4dn,
    and g5.'
  prefs: []
  type: TYPE_NORMAL
- en: 'MME GPU is currently also powered by two model serving stacks:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Nvidia Triton Inference Server](https://aws.amazon.com/blogs/machine-learning/run-multiple-deep-learning-models-on-gpu-with-amazon-sagemaker-multi-model-endpoints/)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[TorchServe](https://aws.amazon.com/blogs/machine-learning/run-multiple-generative-ai-models-on-gpu-using-amazon-sagemaker-multi-model-endpoints-with-torchserve-and-save-up-to-75-in-inference-costs/)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For the purpose of this article we will be utilizing Triton Inference Server
    with a PyTorch backend to host BERT based models on our GPU instance. If you are
    new to Triton, we will have a slight primer, but I would recommend referencing
    my starter article [here](/deploying-pytorch-models-with-nvidia-triton-inference-server-bb139066a387).
  prefs: []
  type: TYPE_NORMAL
