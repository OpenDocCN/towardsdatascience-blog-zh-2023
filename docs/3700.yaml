- en: 'Gaussian Head Avatars: A Summary'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/gaussian-head-avatars-a-summary-2bd17bd48500?source=collection_archive---------0-----------------------#2023-12-19](https://towardsdatascience.com/gaussian-head-avatars-a-summary-2bd17bd48500?source=collection_archive---------0-----------------------#2023-12-19)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: There has been a recent explosion of Gaussian Splatting papers and the avatar
    space is no exception. How do they work and are they going to revolutionise the
    field?
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@jacksaunders909?source=post_page-----2bd17bd48500--------------------------------)[![Jack
    Saunders](../Images/00c752fe1c5bc03f52943238cb034ee4.png)](https://medium.com/@jacksaunders909?source=post_page-----2bd17bd48500--------------------------------)[](https://towardsdatascience.com/?source=post_page-----2bd17bd48500--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----2bd17bd48500--------------------------------)
    [Jack Saunders](https://medium.com/@jacksaunders909?source=post_page-----2bd17bd48500--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F24c6b2ceeccc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgaussian-head-avatars-a-summary-2bd17bd48500&user=Jack+Saunders&userId=24c6b2ceeccc&source=post_page-24c6b2ceeccc----2bd17bd48500---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----2bd17bd48500--------------------------------)
    ·15 min read·Dec 19, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F2bd17bd48500&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgaussian-head-avatars-a-summary-2bd17bd48500&user=Jack+Saunders&userId=24c6b2ceeccc&source=-----2bd17bd48500---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F2bd17bd48500&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgaussian-head-avatars-a-summary-2bd17bd48500&source=-----2bd17bd48500---------------------bookmark_footer-----------)![](../Images/5f471a64d0ba8b95c526decafc9fd6d3.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: Gaussian Splatting at an early stage of training. Image by the author.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: 'And breathe… If you’re interested in the research into digital humans and have
    any form of social media, you’ve almost certainly been bombarded by dozens of
    papers applying Gaussian splatting to the field. [As the great Jia-Bin Huang](https://twitter.com/jbhuang0604/status/1714280734016540860)
    said: 2023 is indeed the year of replacing all NeRFs with Gaussian Splatting.
    [GaussianAvatars](https://shenhanqian.github.io/gaussian-avatars), [FlashAvatar](https://ustc3dv.github.io/FlashAvatar/),
    [Relightable Gaussian Codec Avatars](https://shunsukesaito.github.io/rgca/), [MonoGaussianAvatar](https://yufan1012.github.io/MonoGaussianAvatar):
    these papers represent just a subsection of the papers covering the face alone!'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 放松一下……如果你对数字人类的研究感兴趣，并且有任何形式的社交媒体，你几乎肯定已经被大量将高斯喷溅应用于该领域的论文轰炸过了。[正如伟大的贾宾·黄](https://twitter.com/jbhuang0604/status/1714280734016540860)所说：2023年确实是用高斯喷溅取代所有NeRF的年份。[GaussianAvatars](https://shenhanqian.github.io/gaussian-avatars)，[FlashAvatar](https://ustc3dv.github.io/FlashAvatar/)，[可重光照高斯编解码头像](https://shunsukesaito.github.io/rgca/)，[MonoGaussianAvatar](https://yufan1012.github.io/MonoGaussianAvatar)：这些论文仅代表了涵盖面部的论文中的一小部分！
- en: If you’re anything like me, you’re completely overwhelmed by all this incredible
    work. I’m writing this article to try and compare and contrast these papers, and
    to try to boil down the key components that underpin all these works. I will,
    almost certainly, have missed some papers, and by the time I’ve finished this
    article, I expect there will be more that weren’t around when I started! I’ll
    start by giving a quick recap of Gaussian Splatting as a method and then cover
    some of the key papers.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你像我一样，你会被这些令人惊叹的工作完全压倒。我写这篇文章是为了比较和对比这些论文，并试图提炼出支撑这些工作的关键组件。我几乎肯定会遗漏一些论文，并且在我完成这篇文章时，我预计会有更多论文出现！我将首先简要回顾高斯喷溅作为一种方法，然后讨论一些关键论文。
- en: '![](../Images/36ffc18d89ac9eabb337c21048454a02.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/36ffc18d89ac9eabb337c21048454a02.png)'
- en: 'Teaser figures from a collection of Gaussian Avatar papers. Top Left: [MonoGaussianAvatar](https://yufan1012.github.io/MonoGaussianAvatar),
    Bottom Left: [GaussianAvatars](https://shenhanqian.github.io/gaussian-avatars),
    Top Right: [Relightable Gaussian Codec Avatars](https://shunsukesaito.github.io/rgca/),
    Middle Right: [Gaussian Head](https://github.com/chiehwangs/gaussian-head/tree/main),
    Bottom Right: [Gaussian Head Avatars](https://yuelangx.github.io/gaussianheadavatar/)'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 一系列高斯头像论文中的预览图。左上： [MonoGaussianAvatar](https://yufan1012.github.io/MonoGaussianAvatar)，左下：
    [GaussianAvatars](https://shenhanqian.github.io/gaussian-avatars)，右上： [可重光照高斯编解码头像](https://shunsukesaito.github.io/rgca/)，右中：
    [高斯头](https://github.com/chiehwangs/gaussian-head/tree/main)，右下： [高斯头头像](https://yuelangx.github.io/gaussianheadavatar/)。
- en: Gaussian Splatting — The General Idea
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 高斯喷溅——总体思路
- en: Gaussian Splatting is everywhere now. There’s a good chance you already know
    the basics, and if you don’t there are a lot of resources out there that do a
    much better job of explaining them than I can. Here are some examples if you are
    interested ([1](https://huggingface.co/blog/gaussian-splatting), [2](https://aras-p.info/blog/2023/09/05/Gaussian-Splatting-is-pretty-cool/),
    [3](https://www.reshot.ai/3d-gaussian-splatting)). Nonetheless, I’ll do my best
    to give a quick, general overview here.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 高斯喷溅现在无处不在。你很可能已经了解了基础知识，如果不了解，还有很多资源可以比我更好地解释它们。如果你感兴趣，这里有一些例子（[1](https://huggingface.co/blog/gaussian-splatting)，[2](https://aras-p.info/blog/2023/09/05/Gaussian-Splatting-is-pretty-cool/)，[3](https://www.reshot.ai/3d-gaussian-splatting)）。尽管如此，我会尽力给出一个快速、一般的概述。
- en: '![](../Images/2a81c13cdcdd0fa4926486ca0c2365d6.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2a81c13cdcdd0fa4926486ca0c2365d6.png)'
- en: A rasterised triangle (left) vs a splatted Gaussian (right). Inspired by [this
    HuggingFace article.](https://huggingface.co/blog/gaussian-splatting)
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 栅格化三角形（左）与高斯喷溅（右）。灵感来自于[这篇 HuggingFace 文章](https://huggingface.co/blog/gaussian-splatting)。
- en: 'In a nutshell, Gaussian splatting is a form of rasterisation. It takes some
    representation of a scene and converts it to an image on the screen. This is similar
    to the rendering of triangles that form the basis of most graphics engines. Instead
    of drawing a triangle, Gaussian splatting (unsurprisingly) ‘splats’ Gaussians
    onto the screen. Each Gaussian is represented by a set of parameters:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，高斯喷溅是一种栅格化形式。它将场景的某种表示转换为屏幕上的图像。这类似于大多数图形引擎的三角形渲染。与绘制三角形不同，高斯喷溅（毫不奇怪）将高斯分布“喷溅”到屏幕上。每个高斯分布由一组参数表示：
- en: A position in 3D space (in the scene). **μ**
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 3D空间中的一个位置（场景中）。**μ**
- en: A per-axis scaling (the skew of the Gaussian). **s**
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个轴的缩放（高斯的倾斜）。**s**
- en: A colour (can be RGB or more complex to change based on viewpoint). **c**
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一种颜色（可以是 RGB 或更复杂的，取决于视角）。**c**
- en: An opacity (the opposite of transparency). **α**
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一种不透明度（透明度的相反）。**α**
- en: Gaussian splatting itself is not new, it’s been around since the ‘90s at least.
    What is new is the ability to render them in a ***differentiable*** way. This
    allows us to fit them to a scene using a set of images. Combine this with a method
    of creating more Gaussians and deleting useless ons and we get an extremely powerful
    model for representing 3D scenes.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 高斯溅射本身并不新鲜，它至少从 90 年代就存在了。新的在于能够以***可微分***的方式渲染它们。这使我们能够使用一组图像将它们拟合到场景中。结合创建更多高斯和删除无用点的方法，我们得到了一种极其强大的
    3D 场景表示模型。
- en: 'Why do we want to do this? There are a few reasons:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 我们为什么要这样做？有几个原因：
- en: The quality speaks for itself. It is visually stunning in a way even NeRFs are
    not.
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 其质量不言而喻。它在视觉效果上甚至超越了 NeRFs。
- en: A scene rendered with Gaussian splatting can be run at hundreds of fps, and
    we haven’t even begun to get deep into the hardware/software optimisation yet.
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用高斯溅射渲染的场景可以以数百 fps 运行，而且我们甚至还没有深入探讨硬件/软件优化。
- en: They can be easily edited/combined as they are discrete points, unlike neural
    representations.
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于它们是离散点，可以轻松编辑/组合，这与神经表示不同。
- en: Making Gaussians Animatable
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使高斯能够动画化
- en: Gaussian splatting is obviously cool. It’s no surprise that people have been
    looking for a way to apply them to faces. [You may have seen Apple’s personas](https://www.theverge.com/2023/6/5/23750096/apple-vision-pro-headset-persona-facetime)
    which generated a bit of hype. The papers in this article completely blow them
    out of the water. Imagine fully controllable digital avatars that can run natively
    on a consumer-grade VR headset, with 6 degrees of freedom camera movement, running
    at 100+ fps for both eyes. This would make the ‘metaverse’ finally realisable.
    I would personally bet any amount of money that this scenario will be realised
    within the next 2–3 years. Most likely, Gaussian Splatting (or some variant) is
    the tech that is needed to make this work.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 高斯溅射显然很酷。人们寻求将其应用于面孔并不令人惊讶。[你可能见过苹果的角色](https://www.theverge.com/2023/6/5/23750096/apple-vision-pro-headset-persona-facetime)，它们引起了一些关注。本文中的论文完全超越了这些角色。想象一下，完全可控的数字化身可以在消费级
    VR 头显上原生运行，具有 6 自由度的相机运动，并且双眼帧率超过 100 fps。这将使“元宇宙”最终成为现实。我个人敢打赌，这种情况将在未来 2–3 年内实现。高斯溅射（或某种变体）很可能是实现这一目标所需的技术。
- en: Of course, the ability to render a static scene is not enough. We need to be
    able to do with Gaussians what we are currently able to do with triangular meshes
    that we use currently. It didn’t take long to get dynamic Gaussians (e.g. [1](https://dynamic3dgaussians.github.io/),
    [2](https://ingra14m.github.io/Deformable-Gaussians/)). These allow for the capture
    and replay of “Gaussian Videos”.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，渲染静态场景的能力还不够。我们需要能够用高斯实现我们目前在三角网格中能够做到的事情。动态高斯（例如 [1](https://dynamic3dgaussians.github.io/)，[2](https://ingra14m.github.io/Deformable-Gaussians/)）的出现并不久。这些方法允许捕捉和重播“高斯视频”。
- en: '![](../Images/2e547a3ed88ed0c7d12194135173f57a.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2e547a3ed88ed0c7d12194135173f57a.png)'
- en: 'An example of Dynamic Gaussians, best viewed at source: [here](https://dynamic3dgaussians.github.io/).
    Image Credits: JonathonLuiten, [MIT License](https://github.com/JonathonLuiten/Dynamic3DGaussians?tab=License-1-ov-file#readme)'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 动态高斯的示例，最佳查看源头：[这里](https://dynamic3dgaussians.github.io/)。图片来源：JonathonLuiten，[MIT
    许可证](https://github.com/JonathonLuiten/Dynamic3DGaussians?tab=License-1-ov-file#readme)
- en: Again, really cool but not what we’re after. Ideally, we want a representation
    that we can control with motion capture, audio, or other signals. Thankfully,
    we have a huge body of research designed to do exactly this. Enter our old friend
    the 3DMM. [I’ve covered how these work in a previous post](https://medium.com/@jacksaunders909/person-specific-deepfakes-with-3d-morphable-models-1df9618b9f6a).
    But at their core, they are a 3D model that is represented with a small set of
    parameters. These parameters determine the geometry of the face, with facial expressions
    decoupled from the face shape. This allows for control over the facial expression
    by just changing a small number of expression parameters. ***Most of the papers
    that seek to animate using Gaussian Splatting use a 3DMM at their core.***
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: We will now cover some of the recent Gaussian Head Animation papers (in no particular
    order). I’ve added some TLDR summaries at the end of each.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: 'Gaussian Avatars: Photorealistic Head Avatars with Rigged 3D Gaussians'
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/8a5e32731f274b1269eb9aa3010a4a6f.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
- en: Method diagram for Gaussian Avatars. Image reproduced directly from the [arxiv
    paper](https://arxiv.org/abs/2312.02069).
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: 'Gaussian Avatars: Photorealistic Head Avatars with Rigged 3D Gaussians. [Shenhan
    Qian](https://arxiv.org/search/cs?searchtype=author&query=Qian%2C+S), [Tobias
    Kirschstein](https://arxiv.org/search/cs?searchtype=author&query=Kirschstein%2C+T),
    [Liam Schoneveld](https://arxiv.org/search/cs?searchtype=author&query=Schoneveld%2C+L),
    [Davide Davoli](https://arxiv.org/search/cs?searchtype=author&query=Davoli%2C+D),
    [Simon Giebenhain](https://arxiv.org/search/cs?searchtype=author&query=Giebenhain%2C+S),
    [Matthias Nießner](https://arxiv.org/search/cs?searchtype=author&query=Nie%C3%9Fner%2C+M).
    Arxiv preprint, 4 December 2023\. [**Link**](https://arxiv.org/abs/2312.02069)'
  id: totrans-37
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The first of the papers we will look at is an interesting collaboration between
    the Technical University of Munich and Toyota (I’m really curious about Toyota’s
    involvement). This group is using FLAME, a very popular 3DMM. The method aims
    to take multi-view video and use it to get a model that can be controlled by FLAME
    parameters. It consists of a few separate stages.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: FLAME Fitting
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The first stage of the pipeline aims to reconstruct a coarse approximation of
    the facial geometry using the FLAME mesh. This is done using differntiable rendering.
    Specifically, they use NVDiffrast to render the FLAME mesh in a way that allows
    for backpropagation. Again, [I have previously covered how this works](https://medium.com/@jacksaunders909/person-specific-deepfakes-with-3d-morphable-models-1df9618b9f6a).
    The difference between their method and existing trackers is threefold. **1)**
    It is multiview, meaning they optimise over multiple cameras at once, as opposed
    to monocular reconstruction. **2)** They also include an additional constant offset
    to the FLAME vertices allowing for better shape reconstruction. This is possible
    as the depth ambiguity problem is not present in the multi-view setup. **3)**
    They include a Laplacian mesh regulariser that encourages the meshes to be smooth.
    An example of the FLAME tracking can be seen below.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 流水线的第一阶段旨在使用 FLAME 网格重建面部几何的粗略近似。这是通过可微分渲染来完成的。具体来说，他们使用 NVDiffrast 以允许反向传播的方式渲染
    FLAME 网格。再次，[我之前已经介绍过这如何工作](https://medium.com/@jacksaunders909/person-specific-deepfakes-with-3d-morphable-models-1df9618b9f6a)。他们的方法与现有跟踪器的不同之处有三点。**1)**
    它是多视角的，这意味着他们在多个相机上同时进行优化，而不是单目重建。**2)** 他们还在 FLAME 顶点中添加了一个额外的常数偏移，允许更好的形状重建。这是可能的，因为多视角设置中不存在深度模糊问题。**3)**
    他们包括一个拉普拉斯网格正则化器，鼓励网格光滑。下面可以看到 FLAME 跟踪的示例。
- en: '![](../Images/ce9dd83723d2ded7b1d1261a39285db3.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ce9dd83723d2ded7b1d1261a39285db3.png)'
- en: An example FLAME mesh reconstruction for a given frame. Image reproduced directly
    from the [arxiv paper](https://arxiv.org/abs/2312.02069).
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 给定帧的 FLAME 网格重建示例。图像直接转载自 [arxiv 论文](https://arxiv.org/abs/2312.02069)。
- en: Fitting Gaussians
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 拟合高斯
- en: The next goal is to fit the Gaussians in such a way that they are controlled
    by the FLAME mesh. In this paper, the approach is similar to that of [INSTA](https://zielon.github.io/insta/),
    where each point in 3D space is ‘bound’ to a triangle on the FLAME mesh. As the
    mesh moves, the point moves with it. To extend this idea to Gaussians is fairly
    straightforward, simply assign each Gaussian to a triangle. We define the parameters
    for each Gaussian in a local frame defined by the parent triangle and alter its
    parameters according to the transformations defined by the FLAME mesh relative
    to a neutral FLAME mesh.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来的目标是以 FLAME 网格控制的方式拟合高斯。在本文中，该方法类似于 [INSTA](https://zielon.github.io/insta/)，其中
    3D 空间中的每个点都“绑定”到 FLAME 网格上的一个三角形。随着网格的移动，点也随之移动。将这个想法扩展到高斯是相当简单的，只需将每个高斯分配到一个三角形。我们在父三角形定义的局部框架中定义每个高斯的参数，并根据
    FLAME 网格相对于中性 FLAME 网格定义的变换来调整其参数。
- en: '![](../Images/dcc2840d77ae06fb457a166566b2333c.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/dcc2840d77ae06fb457a166566b2333c.png)'
- en: The Gaussians are initialised in a co-ordinate frame local to each triangle.
    Image reproduced directly from the [arxiv paper](https://arxiv.org/abs/2312.02069).
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 高斯在每个三角形的局部坐标框架中初始化。图像直接转载自 [arxiv 论文](https://arxiv.org/abs/2312.02069)。
- en: For example, let's say we open the mouth and a triangle on the chin moves down
    1cm and rotates by 5 degrees, we would apply this same transform to the position
    of any bound Gaussians and rotate the rotation of the Gaussian in the same way.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设我们张嘴，一个位于下巴的三角形向下移动 1cm 并旋转 5 度，我们会对任何绑定的高斯应用相同的变换，并以相同的方式旋转高斯的旋转。
- en: From here the process is fairly similar to the original Gaussian paper, the
    forward pass now takes the Gaussian parameters, transforms them according to the
    tracked mesh, and splats them. The parameters are optimised using backpropagation.
    Additional losses are used to prevent Gaussians from getting too far from the
    parent triangle. Finally, the densification process is changed slightly so that
    any spawned Gaussian is bound to the same triangle as its parents.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 从这里开始，过程与原始高斯论文非常相似，前向传递现在获取高斯参数，根据跟踪的网格对其进行变换，并进行点滴。参数使用反向传播进行优化。使用附加损失来防止高斯离父三角形过远。最后，密集化过程稍有变化，以确保任何生成的高斯都绑定到与其父级相同的三角形上。
- en: '***TLDR: Assign Gaussians to a triangle of FLAME, and transform it with the
    mesh.***'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '***TLDR: 将高斯分配到 FLAME 的三角形上，并通过网格进行变换。***'
- en: 'FlashAvatar: High-Fidelity Digital Avatar Rendering at 300FPS'
  id: totrans-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 'FlashAvatar: 高保真数字头像渲染，300FPS'
- en: '![](../Images/b46d7501ee076e0d0327eef0b928d28f.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b46d7501ee076e0d0327eef0b928d28f.png)'
- en: Method Diagram for FlashAvatars. Image reproduced directly from the [arxiv paper.](https://arxiv.org/abs/2312.02214)
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: FlashAvatars的方法示意图。图像直接来自于[arxiv论文](https://arxiv.org/abs/2312.02214)。
- en: 'FlashAvatar: High-Fidelity Digital Avatar Rendering at 300FPS. [Jun Xiang](https://arxiv.org/search/cs?searchtype=author&query=Xiang%2C+J),
    [Xuan Gao](https://arxiv.org/search/cs?searchtype=author&query=Gao%2C+X), [Yudong
    Guo](https://arxiv.org/search/cs?searchtype=author&query=Guo%2C+Y), [Juyong Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+J).
    Arxiv preprint, 3rd Decemeber 2023\. [**Link**](https://arxiv.org/abs/2312.02214)'
  id: totrans-53
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 'FlashAvatar: 高保真数字化身渲染，帧率达到300FPS。[项俊](https://arxiv.org/search/cs?searchtype=author&query=Xiang%2C+J)，[高轩](https://arxiv.org/search/cs?searchtype=author&query=Gao%2C+X)，[郭宇东](https://arxiv.org/search/cs?searchtype=author&query=Guo%2C+Y)，[张聚勇](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+J)。Arxiv预印本，2023年12月3日。[**链接**](https://arxiv.org/abs/2312.02214)'
- en: In my opinion, this is the easiest paper to follow. This is a monocular (one
    camera) Gaussian Head Avatar paper with a focus on speed. It can run at 300fps
    (!!) and trains in just a few minutes. Again, there are a few stages to this model.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在我看来，这是最容易理解的论文。这是一篇关注速度的单目（单摄像头）高斯头部化身论文。它可以以300fps（!!）运行，并且训练仅需几分钟。再次强调，该模型分为几个阶段。
- en: FLAME Fitting
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: FLAME拟合
- en: As this is a single-camera method, monocular reconstruction is employed. The
    method used to do this is based on differentiable rendering and Pytorch3D. [It
    is available open-source on GitHub, coming from MICA](https://github.com/Zielon/metrical-tracker).
    Again, I have covered how this method works in previous blog posts.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这是一个单摄像头方法，因此采用了单目重建。这个方法基于可微渲染和Pytorch3D。[该方法在GitHub上开源，来自MICA](https://github.com/Zielon/metrical-tracker)。另外，我在之前的博客文章中也介绍了这个方法的工作原理。
- en: Fitting Gaussians
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 高斯分布拟合
- en: This paper models the Gaussians in the uv-space. A predefined uv map is used
    to get a correspondence between a 2D image and the 3D mesh. Each Gaussian is then
    defined by its position in the uv space rather than the 3D space. By sampling
    a pixel in the uv space, the 3D position of the Gaussian is obtained by getting
    the corresponding point on the posed 3D mesh. To capture the mouth interior, this
    paper adds some additional faces in the mouth interior.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 本文在uv空间中建模高斯分布。使用预定义的uv地图来获得2D图像和3D网格之间的对应关系。每个高斯分布由其在uv空间的位置而不是3D空间中的位置来定义。通过在uv空间中采样一个像素，可以通过获取在姿态3D网格上的对应点来获得高斯分布的3D位置。为了捕捉口腔内部，本文在口腔内部添加了一些额外的面片。
- en: '![](../Images/e8b212f8b6e35bd08648d8d6081f9148.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e8b212f8b6e35bd08648d8d6081f9148.png)'
- en: The mouth interior is modelled as a flat surface in this paper. Image reproduced
    directly from the [arxiv paper.](https://arxiv.org/abs/2312.02214)
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 本文将口腔内部建模为一个平面。图像直接来自于[arxiv论文](https://arxiv.org/abs/2312.02214)。
- en: This uv correspondence, however, limits the position of the Gaussians to the
    mesh surface. This is undesirable as the coarse FLAME mesh is not a perfect reconstruction
    of the geometry. To overcome this, a small MLP is learned to offset the Gaussian
    relative to its position on the mesh. The quality of the result is improved by
    conditioning this MLP on the expression parameters of the FLAME model, this can
    be thought of as a form of neural correctives.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这种uv对应关系将高斯分布的位置限制在了网格表面。这是不理想的，因为粗略的FLAME网格并不能完美重建几何形状。为了解决这个问题，学习了一个小型MLP来将高斯分布相对于其在网格上的位置进行偏移。通过将该MLP基于FLAME模型的表情参数进行条件化，结果的质量得到了改善，这可以看作是一种神经校正方法。
- en: The model is trained using reconstruction losses with L1 and LPIPS losses. The
    mouth region is masked higher to increase the fidelity here where the reconstruction
    is more difficult.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型使用L1和LPIPS损失进行重建训练。口腔区域的遮罩被提高，以增加在重建更困难的区域的保真度。
- en: '***TLDR; Model the Gaussians in uv space, and use an MLP conditioned on expression
    to offset relative to the mesh.***'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '***TLDR; 在uv空间中建模高斯分布，并使用基于表情的MLP对其在网格上的相对位置进行偏移。***'
- en: Relightable Gaussian Codec Avatars
  id: totrans-64
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 可重光高斯编解码化身
- en: '![](../Images/6798fbb65337ac1bfc34b35bd8f90c75.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6798fbb65337ac1bfc34b35bd8f90c75.png)'
- en: Method Diagram for Relightable Gaussian Codec Avatars. Image reproduced directly
    from the [arxiv paper](https://arxiv.org/abs/2312.03704).
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 可重光高斯编解码化身的方法示意图。图像直接来自于[arxiv论文](https://arxiv.org/abs/2312.03704)。
- en: Relightable Gaussian Codec Avatars. [Shunsuke Saito](https://arxiv.org/search/cs?searchtype=author&query=Saito%2C+S),
    [Gabriel Schwartz](https://arxiv.org/search/cs?searchtype=author&query=Schwartz%2C+G),
    [Tomas Simon](https://arxiv.org/search/cs?searchtype=author&query=Simon%2C+T),
    [Junxuan Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+J), [Giljoo
    Nam](https://arxiv.org/search/cs?searchtype=author&query=Nam%2C+G). Arxiv preprint,
    6th December 2023\. [**Link**](https://arxiv.org/abs/2312.03704)
  id: totrans-67
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 可重光照高斯编码头像。 [斋藤俊介](https://arxiv.org/search/cs?searchtype=author&query=Saito%2C+S)，[加布里埃尔·施瓦茨](https://arxiv.org/search/cs?searchtype=author&query=Schwartz%2C+G)，[托马斯·西蒙](https://arxiv.org/search/cs?searchtype=author&query=Simon%2C+T)，[李俊轩](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+J)，[南吉珠](https://arxiv.org/search/cs?searchtype=author&query=Nam%2C+G)。
    Arxiv 预印本，2023年12月6日。 [**链接**](https://arxiv.org/abs/2312.03704)
- en: This next paper is probably the one that has generated the most hype. It’s a
    paper coming from Meta’s reality labs. In addition to being animatable, it is
    also possible to change the lighting for these models, making them easier to composite
    into varying scenes. As this is Meta and Meta have taken a big bet on the ‘metaverse’
    I expect this may lead to a product fairly soon. The paper builds upon the already
    popular codec avatars, using Gaussian splitting.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 下一篇论文可能是引起最多关注的那一篇。这篇论文来自 Meta 的现实实验室。除了可以进行动画处理外，还可以更改这些模型的光照，使其更容易融入不同的场景。由于这是
    Meta，而 Meta 在“元宇宙”上进行了重大投资，我预计这可能很快会转化为产品。该论文在已经流行的编码头像基础上进行扩展，使用了高斯点云。
- en: Mesh Fitting
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 网格拟合
- en: Unfortunately, the mesh reconstruction algorithm used by Meta is a bit more
    complex, and it builds upon several previous papers by the company. It is sufficient
    to say, however, that they can reconstruct a tracked mesh with consistent topology
    with temporal consistency over multiple frames. They use a very expensive and
    complex capture rig to do this.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，Meta 使用的网格重建算法稍显复杂，它建立在公司之前几篇论文的基础上。尽管如此，足以说明的是，他们能够在多个帧上重建具有一致拓扑的跟踪网格，并保持时间一致性。他们使用一种非常昂贵且复杂的捕捉装置来完成这一点。
- en: CVAE Training — Before Gaussians
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: CVAE 训练 — 在高斯分布之前
- en: '![](../Images/021af833be33928fa51526d5365f040e.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/021af833be33928fa51526d5365f040e.png)'
- en: The CVAE is based on this version from the paper “Deep relightable appearance
    models for animatable faces”. Image reproduced from the [paper.](https://dl.acm.org/doi/pdf/10.1145/3450626.3459829)
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: CVAE 基于论文“可动画面孔的深度可重光照外观模型”中的版本。图片摘自[论文](https://dl.acm.org/doi/pdf/10.1145/3450626.3459829)。
- en: The previous approach to Meta is based on a CVAE (Conditional Variational Autoencoder).
    This takes in the tracked mesh and the average texture and encodes them into a
    latent vector. This is then decoded (after reparameterization) into the mesh and
    a set of features is used to reproduce the texture. The objective of this current
    paper is to use a similar model but with Gaussians.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: Meta 的先前方法基于 CVAE（条件变分自编码器）。它接收跟踪网格和平均纹理，并将它们编码成一个潜在向量。然后，这个向量在重新参数化后被解码成网格，并使用一组特征来重现纹理。这篇论文的目标是使用类似的模型，但采用高斯分布。
- en: '**CVAE With Gaussians**'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '**CVAE 与高斯分布**'
- en: To extend this model to Gaussian splatting a few changes need to be made. The
    encoder, however, is not. This encoder still takes in the vertices V of the tracked
    mesh and an average texture. The geometry and appearance of the avatar are decoded
    separately. The geometry is represented using a series of Gaussians. One of the
    more interesting parts of the paper is the representation of Gaussians in a uv
    space. Here a uv-texture map is defined for the mesh template, this means that
    each pixel in the texture map (texel) corresponds to a point on the mesh surface.
    In this paper, each texel defines a Gaussian. Instead of an absolute position,
    each texel Gaussian is defined by its displacement from the mesh surface, e.g.
    the shown texel is a Gaussian that is tied to the eyebrow and moves with it. Each
    texel also has a value for rotation, scale and opacity, as well as roughness (σ)
    and SH coefficients for RGB colour and monochrome.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 要将此模型扩展到高斯点云，需要进行一些更改。然而，编码器却没有改变。这个编码器仍然接收跟踪网格的顶点 V 和一个平均纹理。头像的几何形状和外观是分别解码的。几何形状通过一系列高斯分布来表示。论文中较有趣的部分之一是高斯在
    uv 空间中的表示。在这里，为网格模板定义了一个 uv 纹理图，这意味着纹理图中的每个像素（texel）对应于网格表面上的一个点。在这篇论文中，每个 texel
    定义了一个高斯分布。每个 texel 高斯分布不是通过绝对位置定义，而是通过它与网格表面的位移来定义，例如，所示的 texel 是一个与眉毛相关联并随其移动的高斯分布。每个
    texel 还具有旋转、缩放和透明度的值，以及 RGB 颜色和单色的粗糙度（σ）和 SH 系数。
- en: '![](../Images/365960a6362b873392d09c47078d3086.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
- en: The image on the left hand side is represents the geometry on the right. Each
    texel represents a Gaussian. Image reproduced directly from the [arxiv paper](https://arxiv.org/abs/2312.03704).
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: In addition to the Gaussians, the decoder also predicts a surface normal map
    and visibility maps. These are all combined using approximations of the rendering
    equation for lighting. The following is a very rough explanation that is almost
    certainly wrong/lacking as I’m not an expert on lighting.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8f153d022e28d60b22abf68c75ec0857.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
- en: 'From left to right: Normal maps, specular lighting maps, diffuse lightning
    map, albedo and final lit appearance. Image reproduced directly from the [arxiv
    paper](https://arxiv.org/abs/2312.03704).'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: The diffuse component of the light is computed using spherical harmonics. Each
    Gaussian has an albedo (ρ) and SH coefficients (d). Usually, SH coefficients are
    only represented up to the 3rd order, however, this is not enough to represent
    shadows. To balance this with saving space, the authors use 3rd-order RGB coefficients
    but 5th-order monochrome (grayscale) ones. In addition to diffuse lighting, the
    paper also models specularities (e.g. reflection) by assigning a roughness to
    each Gaussian and using the decoded normal maps. If you’re interested in exactly
    how this works, I would recommend reading the paper and supplementary materials.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: Finally, a separate decoder also predicts the vertices of the template mesh.
    All models are trained together using reconstruction losses at both the image
    level and mesh level. A variety of regularisation losses are also used. The result
    is an extremely high-quality avatar with control over the expression and lighting.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: '***TLDR; Represent Gaussians as uv-space images, decompose the lighting and
    model it explicitly, and improve the codec avatars using this Gaussian representation.***'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: 'MonoGaussianAvatar: Monocular Gaussian Point-based Head Avatar'
  id: totrans-85
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/445dce6b913e751e964a92fa3a67de10.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
- en: 'Method Diagram for MonoGaussianAvatar: Image reproduced directly from the [arxiv
    paper](https://arxiv.org/abs/2312.04558).'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
- en: 'MonoGaussianAvatar: Monocular Gaussian Point-based Head Avatar. [Yufan Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+Y),
    [Lizhen Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+L),
    [Qijing Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+Q), [Hongjiang
    Xiao](https://arxiv.org/search/cs?searchtype=author&query=Xiao%2C+H), [Shengping
    Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+S), [Hongxun
    Yao](https://arxiv.org/search/cs?searchtype=author&query=Yao%2C+H), [Yebin Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+Y).
    Arxiv preprint, 7th December 2023\. [**Link**](https://arxiv.org/abs/2312.04558)'
  id: totrans-88
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: This is another paper that looks to work in the monocular case, e.g. with only
    a single camera. Yet again this is a model based around a 3DMM, but this one takes
    a slightly different approach to the others. Building on the ideas outlined in
    [IMAvatar](https://github.com/zhengyuf/IMavatar?tab=readme-ov-file) and PointAvatars,
    it extends the deformations defined by the FLAME model into a continuous deformation
    field. Using this field, Gaussians can then be deformed according to the FLAME
    parameters. This is also a multi-stage process.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 这是另一篇针对单目情况（例如，仅使用一个摄像头）进行研究的论文。这个模型依然围绕3DMM构建，但采用了与其他模型略有不同的方法。基于[IMAvatar](https://github.com/zhengyuf/IMavatar?tab=readme-ov-file)和PointAvatars中概述的想法，它将FLAME模型定义的变形扩展为一个连续的变形场。使用该变形场，高斯点可以根据FLAME参数进行变形。这也是一个多阶段的过程。
- en: FLAME Fitting
  id: totrans-90
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: FLAME拟合
- en: The fitting process here is very similar to that of FlashAvatar. I have already
    covered it in this article, so I will not do so again. Please read that section
    if you’re interested.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的拟合过程与FlashAvatar非常相似。我已经在这篇文章中介绍过，因此不再重复。如果你感兴趣，请阅读那一部分。
- en: Extending FLAME to a Deformation Field
  id: totrans-92
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 扩展FLAME到变形场
- en: Ideally, we would like to transform the Gaussians in the same way as we do the
    vertices of the FLAME mesh. However, the FLAME mesh deformations are defined only
    for the 5023 vertices that it consists of. While most of the other methods have
    attempted to couple the Gaussians to some point on the mesh, this paper looks
    to extend the FLAME deformations to cover all points in a canonical space. What’s
    the canonical space? We’ll cover that in a moment. In FLAME, expression and pose
    correctives are defined by a linear combination of blendshapes defined for 5023
    vertices. In this paper, these are instead represented by MLP networks. Let’s
    say we have 100 expressions, we would define a network that takes a position in
    canonical space and outputs a (100, 3) size matrix representing the expression
    basis at that point. An MLP is also used to represent the pose corrective blendshapes
    and the skinning weights for each joint.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 理想情况下，我们希望以与FLAME网格的顶点相同的方式变形高斯点。然而，FLAME网格的变形仅定义了它所包含的5023个顶点。虽然大多数其他方法试图将高斯点耦合到网格上的某一点，但本文旨在扩展FLAME的变形，以覆盖规范空间中的所有点。什么是规范空间？我们稍后会介绍。在FLAME中，表情和姿势校正是通过对5023个顶点定义的混合形状进行线性组合来定义的。在本文中，这些校正则由MLP网络表示。假设我们有100种表情，我们将定义一个网络，该网络接受规范空间中的位置，并输出一个（100,
    3）大小的矩阵，表示该点的表情基。在每个关节的皮肤加权和姿势校正混合形状也由MLP表示。
- en: '![](../Images/3d410995cebb78464f1f1b51f339528e.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3d410995cebb78464f1f1b51f339528e.png)'
- en: The MLP-based expression, pose and skinning weights from [IMAvatar](https://github.com/zhengyuf/IMavatar?tab=readme-ov-file).
    Image reproduced directly from the [arxiv paper](https://arxiv.org/abs/2312.04558).
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 基于[IMAvatar](https://github.com/zhengyuf/IMavatar?tab=readme-ov-file)的MLP网络表示的表情、姿势和皮肤加权。图片直接来自[arxiv论文](https://arxiv.org/abs/2312.04558)。
- en: These MLPs are trained together with the rest of the optimisation. A regularisation
    loss is defined by taking the nearest point on the FLAME mesh to each Gaussian
    and requiring that the deformation field at the point matches that defined in
    the actual FLAME model.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 这些多层感知机（MLP）与优化过程中的其他部分一起进行训练。通过将每个高斯点与FLAME网格上最近的点对齐，并要求该点的变形场与实际FLAME模型中定义的变形场一致，从而定义一个正则化损失。
- en: Fitting Gaussians — 3 Spaces
  id: totrans-97
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 高斯点拟合 — 3个空间
- en: There are 3 spaces defined in this paper. The Gaussians are deformed through
    each space and finally rendered before being compared to the ground truth images.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 本文中定义了3个空间。高斯点通过每个空间进行变形，最后进行渲染，然后与真实图像进行比较。
- en: Instead of all the usual parameters for Gaussians, in this paper, they are defined
    only by their position in the first space. This is the initialisation space. From
    here MLPs predict all the usual attributes, taking the initialisation space position
    as input and producing a position, scale, rotation, etc in a second space. This
    is referred to as the canonical space. To improve stability, the position in the
    canonical space is given as an offset from the position in the initialization
    space. Finally, each Gaussian is deformed using the deformation MLPs and a final
    set of MLPs also modifies all the other Gaussian parameters based on the position
    in the canonical space.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ccedcdeec1381e5b583f7da1b70ab8fd.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
- en: The three different spaces. Initialisation space is shown on the left, canonical
    space on top and final posed space at the bottom. Image reproduced directly from
    the [arxiv paper](https://arxiv.org/abs/2312.04558).
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: This paper also uses densification to improve the quality of the results. It
    is more similar to the type used in PointAvatar. Any Gaussian that has an opacity
    <0.1 (e.g. close to transparent) is deleted. An additional number of Gaussians
    are sampled every 5 epochs, this is done by selecting a parent Gaussian sampling
    a position near it and copying the other parameters from the parent. Over time,
    the radius of sampling is reduced.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: The model is trained using the original Gaussian losses, plus the above FLAME
    deformation loss, and a perceptual VGG loss. The gradients are backpropagated
    through all three spaces.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: '***TLDR; Replace the discrete FLAME model with continuous deformation fields.
    Fit Gaussians in these fields.***'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: Ethical Concerns
  id: totrans-105
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Gaussian splatting allows for real-time photorealistic rendering of people.
    This will certainly come with a series of ethical concerns. The most immediately
    obvious of these are directly related to deepfakes. For example, they may be used
    to generate misinformation or non-consensual explicit material. Given the ability
    to produce avatars with a new level of realism, the potential harms are significant.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: To make matters worse, it’s likely the existing techniques we have to counter
    image-based deepfakes (e.g. deepfake detection, watermarking, or inoculation)
    will not work with Gaussian-based methods. With this in mind, I argue that it
    is essential that researchers consider developing such methods alongside their
    work. [Some work](https://arxiv.org/pdf/2309.11747v1.pdf) has suggested that watermarking,
    in particular, is possible for NeRFs. It should, therefore, be possible to adapt
    these techniques to Gaussian splatting.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: If I have one criticism of the work presented in this article, it is a lack
    of consideration surrounding the potential implications of the work. **Of the
    papers listed in this work, only two mention ethical implications at all, and
    even then the discussions are short.**
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: While it is currently difficult to actually misuse this prototype technology,
    owing to the challenges in implementing the models and the high data requirements,
    we are likely only a handful of papers away from models that could do real harm.
    As a community of researchers, we have a responsibility to consider the consequences
    of our work.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管目前由于实施模型和数据要求的挑战，这项原型技术实际被滥用的难度很大，但我们可能只差几篇论文，就能得到可能造成实际伤害的模型。作为研究社区，我们有责任考虑我们工作的后果。
- en: In my opinion, it is time we establish a code of best practises around digital
    human research.
  id: totrans-110
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 在我看来，是时候围绕数字人类研究建立最佳实践规范了。
- en: Discussion — Similarities, Differences and Future Directions
  id: totrans-111
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 讨论 — 相似性、差异性和未来方向
- en: That’s a lot of papers! And we’ve barely even covered a fraction of the ones
    out there. While I think it's useful to understand each of these papers individually,
    there’s more value in understanding the general theme of all the papers. Here
    are some of my insights from reading these works, please feel free to debate them
    or add your own.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 这真的有很多论文！我们几乎只是覆盖了其中的一小部分。虽然我认为理解每篇论文的细节很有用，但更有价值的是理解所有论文的总体主题。以下是我从阅读这些文献中得到的一些见解，请随时进行讨论或补充你的观点。
- en: '**FLAME:** Every paper tries to attach Gaussians to an existing mesh model,
    and in all but the Meta paper, this is FLAME. FLAME continues to be incredibly
    popular, but in my opinion, it is still imperfect. A lack of teeth is an obvious
    one addressed by two of the papers, but an inability to model certain lip shapes
    such as “O” is also prevalent. I think there’s space to see a new model come in
    and improve this. Personally, I expect to see something like the [Neural Parametric
    Head Model](https://simongiebenhain.github.io/NPHM/) gain in popularity. With
    correspondences to 2D-uv spaces, it should be possible to apply some of these
    Gaussian methods to the much better geometry these models offer.'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '**FLAME：** 每篇论文都试图将高斯附加到现有的网格模型上，在除了Meta的论文外，这就是FLAME。FLAME仍然非常受欢迎，但在我看来，它仍然不完美。明显的问题是缺乏细节，这是两篇论文所解决的，但像“O”这样的某些唇形的建模能力也很普遍。我认为有空间看到新的模型出现并改进这一点。个人而言，我期待像[神经参数化头部模型](https://simongiebenhain.github.io/NPHM/)这样的模型获得更高的关注。通过与2D-uv空间的对应关系，应该可以将一些高斯方法应用到这些模型提供的更优几何结构上。'
- en: '**Method of attachment:** Two papers attach the Gaussians to the mesh using
    uv-spaces, one attaches them to triangles and one extends FLAME to a continuous
    deformation field. All seem to work very well. I’m most excited about the uv space
    ones myself. I think this could open up the possibility of learning a generative
    model over Gaussian avatars. For example, if one were to train thousands of uv
    space models, a diffusion model /GAN could be trained over these, allowing for
    the sampling of random, photorealistic avatars.'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '**附加方法：** 两篇论文将高斯附加到网格上使用uv空间，一篇将其附加到三角形上，另一篇将FLAME扩展到连续变形场。这些方法似乎都很有效。我个人对使用uv空间的方法感到最兴奋。我认为这可能打开了学习生成模型的可能性，以生成高斯头像。例如，如果训练数千个uv空间模型，可以在这些模型上训练扩散模型/GAN，从而允许采样随机、逼真的头像。'
- en: '**Speed:** All of these methods are very fast, running faster than real-time.
    This will open up a lot of new, previously impossible applications. Expect to
    see prototypes for telecommunications, gaming and entertainment in the near future.'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '**速度：** 所有这些方法都非常快速，运行速度比实时还要快。这将开启许多以前不可能的应用。预计在不久的将来会看到电信、游戏和娱乐的原型。'
- en: In conclusion, Gaussian splatting has well and truly made its way to Head Avatars,
    and looks to be in a good position to open up a lot of exciting applications.
    However, these need to be balanced against the potential harms. Assuming we can
    get this right, the future of digital human research looks bright!
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，高斯散点图已经成功地进入了头部头像领域，并且看起来有很大的潜力来开启许多令人兴奋的应用。然而，这些应用需要平衡潜在的风险。如果我们能够做好这一点，数字人类研究的未来将会非常光明！
