# TSMixer: è°·æ­ŒæŽ¨å‡ºçš„æœ€æ–°é¢„æµ‹æ¨¡åž‹

> åŽŸæ–‡ï¼š[`towardsdatascience.com/tsmixer-the-latest-forecasting-model-by-google-2fd1e29a8ccb`](https://towardsdatascience.com/tsmixer-the-latest-forecasting-model-by-google-2fd1e29a8ccb)

## æŽ¢ç´¢ TSMixer çš„æž¶æž„ï¼Œå¹¶åœ¨ Python ä¸­å®žçŽ°å®ƒï¼Œç”¨äºŽé•¿æœŸå¤šå˜é‡é¢„æµ‹ä»»åŠ¡ã€‚

[](https://medium.com/@marcopeixeiro?source=post_page-----2fd1e29a8ccb--------------------------------)![Marco Peixeiro](https://medium.com/@marcopeixeiro?source=post_page-----2fd1e29a8ccb--------------------------------)[](https://towardsdatascience.com/?source=post_page-----2fd1e29a8ccb--------------------------------)![Towards Data Science](https://towardsdatascience.com/?source=post_page-----2fd1e29a8ccb--------------------------------) [Marco Peixeiro](https://medium.com/@marcopeixeiro?source=post_page-----2fd1e29a8ccb--------------------------------)

Â·å‘è¡¨äºŽ[Towards Data Science](https://towardsdatascience.com/?source=post_page-----2fd1e29a8ccb--------------------------------) Â·é˜…è¯»æ—¶é—´ 12 åˆ†é’ŸÂ·2023 å¹´ 11 æœˆ 14 æ—¥

--

![](img/e73873566af0876cf840a6284a7f2f21.png)

ç”±[ZdenÄ›k MachÃ¡Äek](https://unsplash.com/@zmachacek?utm_source=medium&utm_medium=referral)æ‹æ‘„çš„ç…§ç‰‡ï¼Œæ¥è‡ª[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)

æ—¶é—´åºåˆ—é¢„æµ‹é¢†åŸŸä»åœ¨è“¬å‹ƒå‘å±•ï¼Œæœ€è¿‘æœ‰è®¸å¤šé‡è¦çš„è´¡çŒ®ï¼Œå¦‚ N-HiTSã€PatchTSTã€TimesNet ä»¥åŠå½“ç„¶è¿˜æœ‰ TimeGPTã€‚

ä¸Žæ­¤åŒæ—¶ï¼ŒTransformer æž¶æž„åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰é¢†åŸŸå®žçŽ°äº†å‰æ‰€æœªæœ‰çš„æ€§èƒ½ï¼Œä½†åœ¨æ—¶é—´åºåˆ—é¢„æµ‹ä¸­å¹¶éžå¦‚æ­¤ã€‚

äº‹å®žä¸Šï¼Œè®¸å¤šåŸºäºŽ Transformer çš„æ¨¡åž‹å¦‚ Autoformerã€Informerã€FEDformer ç­‰è¢«æå‡ºã€‚è¿™äº›æ¨¡åž‹é€šå¸¸è®­ç»ƒæ—¶é—´éžå¸¸é•¿ï¼Œè€Œç®€å•çš„çº¿æ€§æ¨¡åž‹åœ¨è®¸å¤šåŸºå‡†æ•°æ®é›†ä¸Šè¡¨çŽ°æ›´å¥½ï¼ˆè§[Zheng et al., 2022](https://arxiv.org/pdf/2205.13504.pdf)ï¼‰ã€‚

äº‹å®žä¸Šï¼Œåœ¨ 2023 å¹´ 9 æœˆï¼Œè°·æ­Œäº‘ AI ç ”ç©¶äººå‘˜æå‡ºäº†**TSMixer**ï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºŽå¤šå±‚æ„ŸçŸ¥æœºï¼ˆMLPï¼‰çš„æ¨¡åž‹ï¼Œä¸“æ³¨äºŽæ··åˆæ—¶é—´å’Œç‰¹å¾ç»´åº¦ï¼Œä»¥æä¾›æ›´å¥½çš„é¢„æµ‹ã€‚

åœ¨ä»–ä»¬çš„è®ºæ–‡[TSMixer: An All-MLP Architecture for Time Series Forecasting](https://arxiv.org/pdf/2303.06053.pdf)ä¸­ï¼Œä½œè€…å±•ç¤ºäº†è¯¥æ¨¡åž‹åœ¨è®¸å¤šåŸºå‡†æ•°æ®é›†ä¸Šå®žçŽ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼ŒåŒæ—¶ä¿æŒäº†å®žçŽ°çš„ç®€å•æ€§ã€‚

åœ¨è¿™ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬é¦–å…ˆæŽ¢ç´¢ TSMixer çš„æž¶æž„ï¼Œä»¥ç†è§£å…¶å†…éƒ¨å·¥ä½œåŽŸç†ã€‚ç„¶åŽï¼Œæˆ‘ä»¬åœ¨ Python ä¸­å®žçŽ°è¯¥æ¨¡åž‹ï¼Œå¹¶è¿è¡Œè‡ªå·±çš„å®žéªŒï¼Œå°†å…¶æ€§èƒ½ä¸Ž N-HiTS è¿›è¡Œæ¯”è¾ƒã€‚

æ¬²äº†è§£æœ‰å…³ TSMixer çš„æ›´å¤šç»†èŠ‚ï¼Œè¯·åŠ¡å¿…é˜…è¯»[åŽŸå§‹è®ºæ–‡](https://arxiv.org/pdf/2303.06053.pdf)ã€‚

> **é€šè¿‡æˆ‘çš„** [**å…è´¹æ—¶é—´åºåˆ—å¤‡å¿˜å•**](https://www.datasciencewithmarco.com/pl/2147608294) **ï¼Œ** **å­¦ä¹ æœ€æ–°çš„æ—¶é—´åºåˆ—åˆ†æžæŠ€æœ¯ï¼èŽ·å–ç»Ÿè®¡å’Œæ·±åº¦å­¦ä¹ æŠ€æœ¯çš„å®žçŽ°ï¼Œå…¨éƒ¨ç”¨ Python å’Œ TensorFlowï¼**

è®©æˆ‘ä»¬å¼€å§‹å§ï¼

# æŽ¢ç´¢ TSMixer

åœ¨é¢„æµ‹æ–¹é¢ï¼Œæˆ‘ä»¬ç›´è§‚åœ°çŸ¥é“ï¼Œä½¿ç”¨äº¤å‰å˜é‡ä¿¡æ¯å¯ä»¥å¸®åŠ©åšå‡ºæ›´å¥½çš„é¢„æµ‹ã€‚

ä¾‹å¦‚ï¼Œå¤©æ°”å’Œé™æ°´é‡å¯èƒ½ä¼šå½±å“æ¸¸ä¹å›­çš„è®¿å®¢æ•°é‡ã€‚åŒæ ·ï¼Œæ˜ŸæœŸå‡ å’ŒèŠ‚å‡æ—¥ä¹Ÿä¼šäº§ç”Ÿå½±å“ã€‚

å› æ­¤ï¼Œæ‹¥æœ‰èƒ½å¤Ÿåˆ©ç”¨åå˜é‡å’Œå…¶ä»–ç‰¹å¾ä¿¡æ¯è¿›è¡Œé¢„æµ‹çš„æ¨¡åž‹æ˜¯æœ‰æ„ä¹‰çš„ã€‚

è¿™æ¿€å‘äº† TSMixer çš„åˆ›å»ºã€‚ç”±äºŽç®€å•çš„å•å˜é‡çº¿æ€§æ¨¡åž‹è¢«è¯æ˜Žä¼˜äºŽæ›´å¤æ‚çš„æž¶æž„ï¼ˆè§ [Zheng et al., 2022](https://arxiv.org/pdf/2205.13504.pdf)ï¼‰ï¼ŒTSMixer çŽ°åœ¨é€šè¿‡æ·»åŠ äº¤å‰å˜é‡å‰é¦ˆå±‚æ¥æ‰©å±•çº¿æ€§æ¨¡åž‹çš„èƒ½åŠ›ã€‚

å› æ­¤ï¼Œæˆ‘ä»¬çŽ°åœ¨å¾—åˆ°ä¸€ä¸ªèƒ½å¤Ÿå¤„ç†å¤šå˜é‡é¢„æµ‹çš„çº¿æ€§æ¨¡åž‹ï¼Œå®ƒå¯ä»¥åˆ©ç”¨åå˜é‡å’Œå…¶ä»–é™æ€ç‰¹å¾çš„ä¿¡æ¯ã€‚

## TSMixer çš„æž¶æž„

ä¸€èˆ¬æž¶æž„å¦‚ä¸‹é¢çš„å›¾æ‰€ç¤ºã€‚

![](img/d91af3dd374db62e24ad58bc3635b890.png)

TSMixer çš„æž¶æž„ã€‚å›¾åƒæ¥æºï¼šS. Chenã€C. Liã€N. Yoderã€S. Arik å’Œ T. Pfisterï¼Œæ¥è‡ª [TSMixer: An All-MLP Architecture for Time Series Forecasting](https://arxiv.org/pdf/2303.06053.pdf)

ç”±äºŽ TSMixer åªæ˜¯æ‰©å±•çº¿æ€§æ¨¡åž‹ï¼Œå®ƒçš„æž¶æž„ç›¸å½“ç®€å•ï¼Œå› ä¸ºå®ƒå®Œå…¨åŸºäºŽ MLPã€‚

ä»Žä¸Šå›¾å¯ä»¥çœ‹å‡ºï¼Œè¯¥æ¨¡åž‹ä¸»è¦åŒ…æ‹¬ä¸¤ä¸ªæ­¥éª¤ï¼šæ··åˆå±‚å’Œæ—¶é—´æŠ•å½±ã€‚

è®©æˆ‘ä»¬æ›´è¯¦ç»†åœ°æŽ¢è®¨æ¯ä¸€æ­¥ã€‚

## æ··åˆå±‚

è¿™æ˜¯æ—¶é—´æ··åˆå’Œç‰¹å¾æ··åˆå‘ç”Ÿçš„åœ°æ–¹ï¼Œå› æ­¤å‘½åä¸º TSMixerã€‚

![](img/b06f726222fad431f6ffb69063c8e599.png)

ä¸“æ³¨äºŽæ··åˆå±‚ã€‚å›¾åƒæ¥æºï¼šS. Chenã€C. Liã€N. Yoderã€S. Arik å’Œ T. Pfisterï¼Œæ¥è‡ª [TSMixer: An All-MLP Architecture for Time Series Forecasting](https://arxiv.org/pdf/2303.06053.pdf)

åœ¨ä¸Šå›¾ä¸­ï¼Œæˆ‘ä»¬çœ‹åˆ°å¯¹äºŽæ—¶é—´æ··åˆï¼ŒMLP åŒ…æ‹¬ä¸€ä¸ªå…¨è¿žæŽ¥å±‚ï¼ŒéšåŽæ˜¯ ReLU æ¿€æ´»å‡½æ•°å’Œä¸€ä¸ª dropout å±‚ã€‚

è¾“å…¥æ•°æ®ä¸­ï¼Œè¡Œä»£è¡¨æ—¶é—´ï¼Œåˆ—ä»£è¡¨ç‰¹å¾ï¼Œæ•°æ®è¢«è½¬ç½®ï¼Œä»¥ä¾¿ MLP åº”ç”¨äºŽæ—¶é—´åŸŸï¼Œå¹¶åœ¨æ‰€æœ‰ç‰¹å¾ä¸­å…±äº«ã€‚è¿™ä¸ªå•å…ƒè´Ÿè´£å­¦ä¹ æ—¶é—´æ¨¡å¼ã€‚

åœ¨ç¦»å¼€æ—¶é—´æ··åˆå•å…ƒä¹‹å‰ï¼ŒçŸ©é˜µä¼šå†æ¬¡è½¬ç½®ï¼Œç„¶åŽå‘é€åˆ°ç‰¹å¾æ··åˆå•å…ƒã€‚

ç‰¹å¾æ··åˆå•å…ƒåŒ…æ‹¬ä¸¤ä¸ª MLPã€‚ç”±äºŽå®ƒåœ¨ç‰¹å¾åŸŸä¸­åº”ç”¨ï¼Œå› æ­¤åœ¨æ‰€æœ‰æ—¶é—´æ­¥é•¿ä¸­å…±äº«ã€‚åœ¨è¿™é‡Œï¼Œæ²¡æœ‰å¿…è¦è½¬ç½®ï¼Œå› ä¸ºç‰¹å¾å·²ç»åœ¨æ°´å¹³è½´ä¸Šã€‚

æ³¨æ„ï¼Œåœ¨ä¸¤ä¸ªæ··åˆå™¨ä¸­ï¼Œæˆ‘ä»¬éƒ½æœ‰å½’ä¸€åŒ–å±‚å’Œæ®‹å·®è¿žæŽ¥ã€‚åŽè€…å¸®åŠ©æ¨¡åž‹å­¦ä¹ æ•°æ®çš„æ›´æ·±å±‚æ¬¡è¡¨ç¤ºï¼ŒåŒæ—¶ä¿æŒè®¡ç®—æˆæœ¬åˆç†ï¼Œè€Œå½’ä¸€åŒ–æ˜¯æé«˜æ·±åº¦å­¦ä¹ æ¨¡åž‹è®­ç»ƒçš„å¸¸ç”¨æŠ€æœ¯ã€‚

ä¸€æ—¦æ··åˆå®Œæˆï¼Œè¾“å‡ºå°†å‘é€åˆ°æ—¶é—´æŠ•å½±æ­¥éª¤ã€‚

## æ—¶é—´æŠ•å½±

æ—¶é—´æŠ•å½±æ­¥éª¤æ˜¯ç”Ÿæˆ TSMixer é¢„æµ‹çš„éƒ¨åˆ†ã€‚

![](img/001d4503e1f393a0b44601e994fe95ac.png)

é‡ç‚¹å…³æ³¨æ—¶é—´æŠ•å½±å±‚ã€‚å›¾åƒæ¥è‡ª**S. Chen**ã€**C. Li**ã€**N. Yoder**ã€**S. Arik**å’Œ**T. Pfister**ï¼Œè§[TSMixer: An All-MLP Architecture for Time Series Forecasting](https://arxiv.org/pdf/2303.06053.pdf)

åœ¨è¿™é‡Œï¼ŒçŸ©é˜µå†æ¬¡è¢«è½¬ç½®ï¼Œå¹¶é€šè¿‡å…¨è¿žæŽ¥å±‚ç”Ÿæˆé¢„æµ‹ã€‚æœ€ç»ˆæ­¥éª¤æ˜¯å†æ¬¡è½¬ç½®è¯¥çŸ©é˜µï¼Œä½¿ç‰¹å¾ä½äºŽæ°´å¹³è½´ä¸Šï¼Œæ—¶é—´æ­¥ä½äºŽåž‚ç›´è½´ä¸Šã€‚

çŽ°åœ¨æˆ‘ä»¬äº†è§£äº† TSMixer çš„å·¥ä½œåŽŸç†ï¼Œè®©æˆ‘ä»¬åœ¨ Python ä¸­å®žçŽ°å®ƒå¹¶è¿›è¡Œæµ‹è¯•ã€‚

# å®žçŽ° TSMixer

æ®æˆ‘äº†è§£ï¼ŒTSMixer åœ¨ Python ä¸­çš„å¸¸ç”¨æ—¶é—´åºåˆ—åº“ä¸­å°šæœªå®žçŽ°ï¼Œå¦‚ Darts æˆ– Neuralforecastã€‚å› æ­¤ï¼Œæˆ‘å°†è°ƒæ•´åŽŸå§‹å®žçŽ°ä»¥é€‚åº”æˆ‘çš„å®žéªŒã€‚

åŽŸå§‹å®žçŽ°å¯åœ¨[Google Research çš„ä»£ç åº“](https://github.com/google-research/google-research/tree/master/tsmixer/tsmixer_basic)ä¸­æ‰¾åˆ°ã€‚

æœ¬å®žéªŒçš„å®Œæ•´ä»£ç å¯åœ¨[GitHub](https://github.com/marcopeix/time-series-analysis/blob/master/TSMixer.ipynb)ä¸ŠèŽ·å–ã€‚

## è¯»å–å’Œæ ¼å¼åŒ–æ•°æ®

åº”ç”¨æ·±åº¦å­¦ä¹ æ¨¡åž‹è¿›è¡Œæ—¶é—´åºåˆ—é¢„æµ‹ä¸­æœ€å›°éš¾çš„éƒ¨åˆ†æ— ç–‘æ˜¯æ ¼å¼åŒ–æ•°æ®é›†ä»¥è¾“å…¥ç¥žç»ç½‘ç»œã€‚

æ‰€ä»¥ï¼Œç¬¬ä¸€æ­¥æ˜¯åˆ›å»ºä¸€ä¸ª`DataLoader`ç±»æ¥å¤„ç†æ•°æ®é›†çš„æ‰€æœ‰è½¬æ¢ã€‚è¿™ä¸ªç±»é€šè¿‡æ‰¹å¤„ç†å¤§å°ã€è¾“å…¥åºåˆ—é•¿åº¦ã€è¾“å‡ºåºåˆ—é•¿åº¦ï¼ˆè§†é‡Žï¼‰ä»¥åŠç›®æ ‡çš„åˆ‡ç‰‡å¯¹è±¡è¿›è¡Œåˆå§‹åŒ–ã€‚

```py
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
from sklearn.preprocessing import StandardScaler

class DataLoader:

  def __init__(self, batch_size, seq_len, pred_len):
    self.batch_size = batch_size
    self.seq_len = seq_len
    self.pred_len = pred_len
    self.target_slice = slice(0, None)

    self._read_data()
```

ç„¶åŽï¼Œæˆ‘ä»¬æ·»åŠ ä¸€ä¸ªæ–¹æ³•æ¥è¯»å–å’Œç¼©æ”¾æ•°æ®ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬ä½¿ç”¨ç”µåŠ›å˜åŽ‹å™¨æ•°æ®é›† Etth1ï¼Œå…¬å¼€[åœ¨ GitHub ä¸ŠèŽ·å–](https://github.com/zhouhaoyi/ETDataset/blob/main/ETT-small/ETTh1.csv)ï¼Œæ ¹æ®åˆ›ä½œå…±ç”¨ç½²åè®¸å¯ã€‚

```py
def _read_data(self):

    filepath = ('data/ETTh1_original.csv')

    df_raw = pd.read_csv(filepath)
    df = df_raw.set_index('date')

    # split train/valid/test
    n = len(df)
    train_end = int(n * 0.7)
    val_end = n - int(n * 0.2)
    test_end = n

    train_df = df[:train_end]
    val_df = df[train_end - self.seq_len : val_end]
    test_df = df[val_end - self.seq_len : test_end]

    # standardize by training set
    self.scaler = StandardScaler()
    self.scaler.fit(train_df.values)

    def scale_df(df, scaler):
      data = scaler.transform(df.values)
      return pd.DataFrame(data, index=df.index, columns=df.columns)

    self.train_df = scale_df(train_df, self.scaler)
    self.val_df = scale_df(val_df, self.scaler)
    self.test_df = scale_df(test_df, self.scaler)
    self.n_feature = self.train_df.shape[-1]
```

åœ¨ä¸Šé¢çš„ä»£ç å—ä¸­ï¼Œè¯·æ³¨æ„ç¼©æ”¾æ•°æ®ä»¥æé«˜æ¨¡åž‹è®­ç»ƒæ—¶é—´æ˜¯è‡³å…³é‡è¦çš„ã€‚è¿˜è¦æ³¨æ„ï¼Œæˆ‘ä»¬åªåœ¨è®­ç»ƒé›†ä¸Šæ‹Ÿåˆç¼©æ”¾å™¨ï¼Œä»¥é¿å…åœ¨éªŒè¯é›†å’Œæµ‹è¯•é›†ä¸­å‡ºçŽ°æ•°æ®æ³„æ¼ã€‚

ç„¶åŽï¼Œæˆ‘ä»¬åˆ›å»ºä¸¤ä¸ªæ–¹æ³•ï¼Œå°†æ•°æ®çª—å£æ‹†åˆ†ä¸ºè¾“å…¥å’Œæ ‡ç­¾ï¼Œç„¶åŽåˆ›å»ºä¸€ä¸ªå¯ä»¥è¾“å…¥åˆ° Keras ç¥žç»ç½‘ç»œçš„æ•°æ®é›†ã€‚

```py
def _split_window(self, data):
    inputs = data[:, : self.seq_len, :]
    labels = data[:, self.seq_len :, self.target_slice]

    inputs.set_shape([None, self.seq_len, None])
    labels.set_shape([None, self.pred_len, None])
    return inputs, labels

  def _make_dataset(self, data, shuffle=True):
    data = np.array(data, dtype=np.float32)
    ds = tf.keras.utils.timeseries_dataset_from_array(
        data=data,
        targets=None,
        sequence_length=(self.seq_len + self.pred_len),
        sequence_stride=1,
        shuffle=shuffle,
        batch_size=self.batch_size,
    )
    ds = ds.map(self._split_window)
    return ds
```

æœ€åŽï¼Œæˆ‘ä»¬å®Œæˆ`DataLoader`ç±»ï¼Œæ·»åŠ æ–¹æ³•ä»¥é€†å‘è½¬æ¢é¢„æµ‹ç»“æžœï¼Œå¹¶ç”Ÿæˆè®­ç»ƒã€éªŒè¯å’Œæµ‹è¯•é›†ã€‚

```py
 def inverse_transform(self, data):
    return self.scaler.inverse_transform(data)

  def get_train(self, shuffle=True):
    return self._make_dataset(self.train_df, shuffle=shuffle)

  def get_val(self):
    return self._make_dataset(self.val_df, shuffle=False)

  def get_test(self):
    return self._make_dataset(self.test_df, shuffle=False)
```

å®Œæ•´çš„`DataLoader`ç±»å¦‚ä¸‹æ‰€ç¤ºï¼š

```py
class DataLoader:

  def __init__(self, batch_size, seq_len, pred_len):
    self.batch_size = batch_size
    self.seq_len = seq_len
    self.pred_len = pred_len
    self.target_slice = slice(0, None)

    self._read_data()

  def _read_data(self):

    filepath = ('data/ETTh1_original.csv')

    df_raw = pd.read_csv(filepath)
    df = df_raw.set_index('date')

    # split train/valid/test
    n = len(df)
    train_end = int(n * 0.7)
    val_end = n - int(n * 0.2)
    test_end = n

    train_df = df[:train_end]
    val_df = df[train_end - self.seq_len : val_end]
    test_df = df[val_end - self.seq_len : test_end]

    # standardize by training set
    self.scaler = StandardScaler()
    self.scaler.fit(train_df.values)

    def scale_df(df, scaler):
      data = scaler.transform(df.values)
      return pd.DataFrame(data, index=df.index, columns=df.columns)

    self.train_df = scale_df(train_df, self.scaler)
    self.val_df = scale_df(val_df, self.scaler)
    self.test_df = scale_df(test_df, self.scaler)
    self.n_feature = self.train_df.shape[-1]

  def _split_window(self, data):
    inputs = data[:, : self.seq_len, :]
    labels = data[:, self.seq_len :, self.target_slice]

    inputs.set_shape([None, self.seq_len, None])
    labels.set_shape([None, self.pred_len, None])
    return inputs, labels

  def _make_dataset(self, data, shuffle=True):
    data = np.array(data, dtype=np.float32)
    ds = tf.keras.utils.timeseries_dataset_from_array(
        data=data,
        targets=None,
        sequence_length=(self.seq_len + self.pred_len),
        sequence_stride=1,
        shuffle=shuffle,
        batch_size=self.batch_size,
    )
    ds = ds.map(self._split_window)
    return ds

  def inverse_transform(self, data):
    return self.scaler.inverse_transform(data)

  def get_train(self, shuffle=True):
    return self._make_dataset(self.train_df, shuffle=shuffle)

  def get_val(self):
    return self._make_dataset(self.val_df, shuffle=False)

  def get_test(self):
    return self._make_dataset(self.test_df, shuffle=False)
```

ç„¶åŽï¼Œæˆ‘ä»¬å¯ä»¥ç®€å•åœ°åˆå§‹åŒ–`DataLoader`ç±»çš„ä¸€ä¸ªå®žä¾‹ï¼Œä»¥è¯»å–æˆ‘ä»¬çš„æ•°æ®é›†å¹¶åˆ›å»ºç›¸å…³çš„æ•°æ®é›†ã€‚

åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬ä½¿ç”¨äº† 96 çš„é¢„æµ‹èŒƒå›´ã€512 çš„è¾“å…¥åºåˆ—é•¿åº¦å’Œ 32 çš„æ‰¹é‡å¤§å°ã€‚

```py
data_loader = DataLoader(batch_size=32, seq_len=512, pred_len=96)

train_data = data_loader.get_train()
val_data = data_loader.get_val()
test_data = data_loader.get_test()
```

çŽ°åœ¨æ•°æ®å·²ç»å‡†å¤‡å¥½ï¼Œæˆ‘ä»¬å¯ä»¥æž„å»º TSMixer æ¨¡åž‹äº†ã€‚

## æž„å»º TSMixer

æž„å»º TSMixer éžå¸¸ç®€å•ï¼Œå› ä¸ºè¯¥æ¨¡åž‹ä»…ç”± MLP ç»„æˆã€‚è®©æˆ‘ä»¬å›žé¡¾ä¸€ä¸‹å…¶æž¶æž„ï¼Œä»¥ä¾¿åœ¨æž„å»ºæ¨¡åž‹æ—¶å‚è€ƒã€‚

![](img/d91af3dd374db62e24ad58bc3635b890.png)

TSMixer çš„æž¶æž„ã€‚å›¾ç‰‡æ¥è‡ª [TSMixer: An All-MLP Architecture for Time Series Forecasting](https://arxiv.org/pdf/2303.06053.pdf) çš„ S. Chenã€C. Liã€N. Yoderã€S. Arik å’Œ T. Pfisterã€‚

é¦–å…ˆï¼Œæˆ‘ä»¬å¿…é¡»å¤„ç† Mixer Layerï¼Œå®ƒåŒ…æ‹¬ï¼š

+   æ‰¹é‡å½’ä¸€åŒ–

+   è½¬ç½®çŸ©é˜µ

+   è¾“å…¥åˆ°å…·æœ‰ ReLu æ¿€æ´»çš„å…¨è¿žæŽ¥å±‚

+   å†æ¬¡è½¬ç½®

+   dropout å±‚

+   å¹¶åœ¨æœ€åŽæ·»åŠ æ®‹å·®

è¿™å¯ä»¥ç¿»è¯‘æˆå¦‚ä¸‹ä»£ç ï¼š

```py
from tensorflow.keras import layers

def res_block(inputs, norm_type, activation, dropout, ff_dim):

  norm = layers.BatchNormalization

  # Time mixing
  x = norm(axis=[-2, -1])(inputs)
  x = tf.transpose(x, perm=[0, 2, 1])  # [Batch, Channel, Input Length]
  x = layers.Dense(x.shape[-1], activation='relu')(x)
  x = tf.transpose(x, perm=[0, 2, 1])  # [Batch, Input Length, Channel]
  x = layers.Dropout(dropout)(x)
  res = x + inputs
```

ç„¶åŽï¼Œæˆ‘ä»¬æ·»åŠ ç‰¹å¾æ··åˆéƒ¨åˆ†ï¼Œå…¶ä¸­åŒ…æ‹¬ï¼š

+   æ‰¹é‡å½’ä¸€åŒ–

+   ä¸€ä¸ª dense å±‚

+   dropout å±‚

+   å¦ä¸€ä¸ª dense å±‚

+   å¦ä¸€ä¸ª dropout å±‚

+   å¹¶æ·»åŠ æ®‹å·®ä»¥å½¢æˆæ®‹å·®è¿žæŽ¥

```py
 # Feature mixing
  x = norm(axis=[-2, -1])(res)
  x = layers.Dense(ff_dim, activation='relu')(x)  # [Batch, Input Length, FF_Dim]
  x = layers.Dropout(0.7)(x)
  x = layers.Dense(inputs.shape[-1])(x)  # [Batch, Input Length, Channel]
  x = layers.Dropout(0.7)(x)
  return x + res
```

å°±è¿™æ ·ï¼Mixer Layer çš„å®Œæ•´å‡½æ•°å¦‚ä¸‹ï¼š

```py
from tensorflow.keras import layers

def res_block(inputs, ff_dim):

  norm = layers.BatchNormalization

  # Time mixing
  x = norm(axis=[-2, -1])(inputs)
  x = tf.transpose(x, perm=[0, 2, 1])  # [Batch, Channel, Input Length]
  x = layers.Dense(x.shape[-1], activation='relu')(x)
  x = tf.transpose(x, perm=[0, 2, 1])  # [Batch, Input Length, Channel]
  x = layers.Dropout(0.7)(x)
  res = x + inputs

  # Feature mixing
  x = norm(axis=[-2, -1])(res)
  x = layers.Dense(ff_dim, activation='relu')(x)  # [Batch, Input Length, FF_Dim]
  x = layers.Dropout(0.7)(x)
  x = layers.Dense(inputs.shape[-1])(x)  # [Batch, Input Length, Channel]
  x = layers.Dropout(0.7)(x)
  return x + res
```

çŽ°åœ¨ï¼Œæˆ‘ä»¬ç®€å•åœ°ç¼–å†™ä¸€ä¸ªå‡½æ•°æ¥æž„å»ºæ¨¡åž‹ã€‚æˆ‘ä»¬åŒ…æ‹¬ä¸€ä¸ª for å¾ªçŽ¯ä»¥åˆ›å»ºæˆ‘ä»¬éœ€è¦çš„ Mixer å±‚ï¼Œå¹¶æ·»åŠ æœ€ç»ˆçš„æ—¶é—´æŠ•å½±æ­¥éª¤ã€‚

ä»Žä¸Šé¢çš„å›¾ä¸­å¯ä»¥çœ‹åˆ°ï¼Œæ—¶é—´æŠ•å½±æ­¥éª¤åªæ˜¯ï¼š

+   ä¸€ä¸ªè½¬ç½®

+   ç»è¿‡ dense å±‚å¤„ç†

+   æœ€åŽçš„è½¬ç½®

```py
def build_model(
    input_shape,
    pred_len,
    n_block,
    ff_dim,
    target_slice,
):

  inputs = tf.keras.Input(shape=input_shape)
  x = inputs  # [Batch, Input Length, Channel]
  for _ in range(n_block):
    x = res_block(x, norm_type, activation, dropout, ff_dim)

  if target_slice:
    x = x[:, :, target_slice]

  # Temporal projection
  x = tf.transpose(x, perm=[0, 2, 1])  # [Batch, Channel, Input Length]
  x = layers.Dense(pred_len)(x)  # [Batch, Channel, Output Length]
  outputs = tf.transpose(x, perm=[0, 2, 1])  # [Batch, Output Length, Channel])

  return tf.keras.Model(inputs, outputs)
```

çŽ°åœ¨ï¼Œæˆ‘ä»¬å¯ä»¥è¿è¡Œå‡½æ•°æ¥æž„å»ºæ¨¡åž‹ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬åœ¨ Mixer Layer ä¸­ä½¿ç”¨äº†å…«ä¸ªå—ã€‚

```py
model = build_model(
    input_shape=(512, data_loader.n_feature),
    pred_len=96,
    n_block=8,
    ff_dim=64,
    target_slice=data_loader.target_slice
)
```

## è®­ç»ƒ TSMixer

æˆ‘ä»¬çŽ°åœ¨å‡†å¤‡å¥½è®­ç»ƒæ¨¡åž‹äº†ã€‚

æˆ‘ä»¬ä½¿ç”¨å­¦ä¹ çŽ‡ä¸º 1e-4 çš„ Adam ä¼˜åŒ–å™¨ã€‚æˆ‘ä»¬è¿˜å®žçŽ°äº†æ£€æŸ¥ç‚¹ä»¥ä¿å­˜æœ€ä½³æ¨¡åž‹ï¼Œå¹¶é€šè¿‡æ—©åœæœºåˆ¶åœ¨è¿žç»­ä¸‰æ¬¡è®­ç»ƒè½®æ¬¡æ²¡æœ‰æ”¹è¿›æ—¶åœæ­¢è®­ç»ƒã€‚

```py
tf.keras.utils.set_random_seed(42)

optimizer = tf.keras.optimizers.Adam(1e-4)

model.compile(optimizer, loss='mse', metrics=['mae'])

checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(
    filepath='tsmixer_checkpoints/',
    vebose=1,
    save_best_only=True,
    save_weights_only=True
)

early_stop_callback = tf.keras.callbacks.EarlyStopping(
    monitor='val_loss',
    patience=3
)

history = model.fit(
    train_data,
    epochs= 30,
    validation_data=val_data,
    callbacks=[checkpoint_callback, early_stop_callback]
)
```

æ³¨æ„ï¼Œä»…ä½¿ç”¨ CPU è®­ç»ƒæ¨¡åž‹èŠ±è´¹äº† 15 åˆ†é’Ÿã€‚

ä¸€æ—¦æ¨¡åž‹è®­ç»ƒå®Œæˆï¼Œæˆ‘ä»¬å¯ä»¥åŠ è½½é€šè¿‡æ£€æŸ¥ç‚¹å›žè°ƒä¿å­˜çš„æœ€ä½³æ¨¡åž‹ã€‚

```py
best_epoch = np.argmin(history.history['val_loss'])

model.load_weights("tsmixer_checkpoints/")
```

ç„¶åŽï¼Œè®©æˆ‘ä»¬è®¿é—®æœ€åŽä¸€ä¸ª 96 æ—¶é—´æ­¥çš„é¢„æµ‹ã€‚æ³¨æ„ï¼Œé¢„æµ‹çŽ°åœ¨æ˜¯ç»è¿‡ç¼©æ”¾çš„ã€‚

```py
predictions = model.predict(test_data)

scaled_preds = predictions[-1,:,:]
```

æœ€åŽï¼Œæˆ‘ä»¬å°†ç¼©æ”¾åŽçš„é¢„æµ‹å’Œé€†å˜æ¢åŽçš„é¢„æµ‹å­˜å‚¨åœ¨ DataFrame ä¸­ï¼Œä»¥ä¾¿åŽç»­è¯„ä¼°æ€§èƒ½å’Œç»˜åˆ¶é¢„æµ‹ç»“æžœã€‚

```py
cols = ['HUFL', 'HULL', 'MUFL', 'MULL', 'LUFL', 'LULL', 'OT']

scaled_preds_df = pd.DataFrame(scaled_preds)
scaled_preds_df.columns = cols

preds = data_loader.inverse_transform(scaled_preds)

preds_df = pd.DataFrame(preds)
preds_df.columns = cols
```

## ä½¿ç”¨ N-HiTS è¿›è¡Œé¢„æµ‹

ä¸ºäº†è¯„ä¼° TSMixer çš„æ€§èƒ½ï¼Œæˆ‘ä»¬ä½¿ç”¨ä¸Ž N-HiTS ç›¸åŒçš„è®­ç»ƒåè®®ï¼Œå› ä¸ºå®ƒä»¬ä¹Ÿæ”¯æŒå¤šå˜é‡é¢„æµ‹ã€‚

æé†’ä¸€ä¸‹ï¼Œä½ å¯ä»¥åœ¨ [GitHub](https://github.com/marcopeix/time-series-analysis/blob/master/TSMixer.ipynb) ä¸Šè®¿é—®æ­¤å®žéªŒçš„å®Œæ•´ä»£ç ã€‚

å¯¹äºŽè¿™ä¸€éƒ¨åˆ†ï¼Œæˆ‘ä»¬ä½¿ç”¨ NeuralForecast åº“ã€‚å› æ­¤ï¼Œè‡ªç„¶çš„ç¬¬ä¸€æ­¥æ˜¯è¯»å–æ•°æ®å¹¶æŒ‰éœ€æ ¼å¼åŒ–ã€‚

```py
from neuralforecast.core import NeuralForecast
from neuralforecast.models import NHITS

df = pd.read_csv('data/ETTh1_original.csv')

columns_to_melt = ['date', 'HUFL', 'HULL', 'MUFL', 'MULL', 'LUFL', 'LULL', 'OT']

melted_df = df.melt(id_vars=['date'], value_vars=columns_to_melt, var_name='unique_id', value_name='y')

melted_df.rename(columns={'date': 'ds'}, inplace=True)

melted_df['ds'] = pd.to_datetime(melted_df['ds'])
```

ç„¶åŽï¼Œæˆ‘ä»¬å¯ä»¥åˆå§‹åŒ– N-HiTS å¹¶åœ¨æ•°æ®ä¸Šè¿›è¡Œæ‹Ÿåˆã€‚

```py
horizon = 96

models = [
    NHITS(h=horizon, input_size=512, max_steps=30)
]

nf = NeuralForecast(models=models, freq='H')

n_preds_df = nf.cross_validation(
  df=melted_df, 
  val_size=int(0.2*len(df)), 
  test_size=int(0.1*len(df)), 
  n_windows=None)
```

ç„¶åŽï¼Œæˆ‘ä»¬ä»…æå–è¿‡åŽ» 96 ä¸ªæ—¶é—´æ­¥çš„é¢„æµ‹ã€‚

```py
df['date'][-96:] = pd.to_datetime(df['date'][-96:])

max_date = df['date'][-96:].max()
min_date = df['date'][-96:].min()

last_n_preds_df = n_preds_df[(n_preds_df['ds'] >= min_date) & (n_preds_df['ds'] <= max_date)]

cols = ['HUFL', 'HULL', 'MUFL', 'MULL', 'LUFL', 'LULL', 'OT']

clean_last_n_preds_df = pd.DataFrame()

for col in cols:
    temp_df = last_n_preds_df[last_n_preds_df['unique_id'] == col].drop_duplicates(subset='ds', keep='first')
    clean_last_n_preds_df = pd.concat([clean_last_n_preds_df, temp_df], ignore_index=True)
```

æ­¤æ—¶ï¼Œæˆ‘ä»¬å·²ç»å¾—åˆ°äº†æ¯ä¸€åˆ—åœ¨è¿‡åŽ» 96 ä¸ªæ—¶é—´æ­¥çš„é¢„æµ‹ï¼Œå¦‚ä¸‹æ‰€ç¤ºã€‚

![](img/993ea50dd61bab4b5aa76d2ec22b0747.png)

N-HiTS å¯¹è¿‡åŽ» 96 ä¸ªæ—¶é—´æ­¥çš„æ¯ä¸ªåºåˆ—çš„é¢„æµ‹ã€‚å›¾ç‰‡ç”±ä½œè€…æä¾›ã€‚

çŽ°åœ¨ï¼Œæˆ‘ä»¬å‡†å¤‡å¥½å¯è§†åŒ–å’Œè¡¡é‡æˆ‘ä»¬æ¨¡åž‹çš„è¡¨çŽ°äº†ã€‚

## è¯„ä¼°

é¦–å…ˆï¼Œè®©æˆ‘ä»¬å¯è§†åŒ–é¢„æµ‹ç»“æžœã€‚

ä¸ºäº†ç®€ä¾¿èµ·è§ï¼Œæˆ‘ä»¬åªç»˜åˆ¶äº†æ•°æ®é›†ä¸­å‰å››ä¸ªåºåˆ—çš„é¢„æµ‹ã€‚

```py
nhits_preds = pd.read_csv('data/nhits_preds_etth1_h96.csv')
tsmixer_preds = pd.read_csv('data/tsmixer_preds_etth1_h96.csv')

cols_to_plot = ['HUFL', 'HULL', 'MUFL', 'MULL']

fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(12,8))

for i, ax in enumerate(axes.flatten()):
    col = cols_to_plot[i]

    nhits_df = nhits_preds[nhits_preds['unique_id'] == col] 

    ax.plot(df['date'][-96:], df[col][-96:])
    ax.plot(df['date'][-96:], tsmixer_preds[col], label='TSMixer', ls='--', color='green')
    ax.plot(df['date'][-96:], nhits_df['NHITS'], label='N-HiTS', ls=':', color='black')

    ax.legend(loc='best')
    ax.set_xlabel('Time steps')
    ax.set_ylabel('Value')
    ax.set_title(col)

plt.tight_layout()
fig.autofmt_xdate()
```

![](img/c3cba66a8a601f65922c0bea1bc9ab92.png)

N-HiTS å’Œ TSMixer å¯¹æ•°æ®é›†ä¸­å‰å››ä¸ªåºåˆ—çš„é¢„æµ‹ã€‚æˆ‘ä»¬çœ‹åˆ° TSMixer åœ¨ MULL å’Œ HULL ä¸Šéš¾ä»¥æ³›åŒ–ï¼Œè€Œ N-HiTS ä¼¼ä¹Žå¾ˆå¥½åœ°è·Ÿéšäº†å®žé™…æ›²çº¿ã€‚å›¾ç‰‡ç”±ä½œè€…æä¾›ã€‚

ä»Žä¸Šå›¾ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ° TSMixer åœ¨é¢„æµ‹ HUFL å’Œ MUFL ä¸Šè¡¨çŽ°å¾—ç›¸å½“ä¸é”™ï¼Œä½†åœ¨ MULL å’Œ HULL ä¸Šè¡¨çŽ°ä¸ä½³ã€‚ç„¶è€Œï¼ŒN-HiTS ä¼¼ä¹Žåœ¨æ‰€æœ‰åºåˆ—ä¸Šçš„è¡¨çŽ°éƒ½å¾ˆä¸é”™ã€‚

ä¸è¿‡ï¼Œè¯„ä¼°è¡¨çŽ°çš„æœ€ä½³æ–¹å¼è¿˜æ˜¯é€šè¿‡æµ‹é‡è¯¯å·®æŒ‡æ ‡ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬è®¡ç®— MAE å’Œ MSEã€‚

```py
from sklearn.metrics import mean_absolute_error, mean_squared_error

y_actual = df.drop('date', axis=1)[-96:]

data = {'N-HiTS': 
            [mean_absolute_error(nhits_preds['y'], nhits_preds['NHITS']), 
             mean_squared_error(nhits_preds['y'], nhits_preds['NHITS'])],
       'TSMixer': 
            [mean_absolute_error(y_actual, tsmixer_preds), 
             mean_squared_error(y_actual, tsmixer_preds)]}

metrics_df = pd.DataFrame(data=data)
metrics_df.index = ['mae', 'mse']

metrics_df.style.highlight_min(color='lightgreen', axis=1)
```

![](img/64fc5b5d4d677632dd5a2459780d4404.png)

N-HiTS å’Œ TSMixer åœ¨ 96 ä¸ªæ—¶é—´æ­¥é•¿çš„å¤šå˜é‡é¢„æµ‹ä»»åŠ¡ä¸­çš„ MAE å’Œ MSEã€‚å›¾ç‰‡ç”±ä½œè€…æä¾›ã€‚

ä»Žä¸Šå›¾ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ° TSMixer åœ¨ 96 ä¸ªæ—¶é—´æ­¥é•¿çš„å¤šå˜é‡é¢„æµ‹ä»»åŠ¡ä¸­ä¼˜äºŽ N-HiTSï¼Œå› ä¸ºå®ƒå®žçŽ°äº†æœ€ä½Žçš„ MAE å’Œ MSEã€‚

è™½ç„¶è¿™ä¸æ˜¯æœ€å…¨é¢çš„å®žéªŒï¼Œä½†çœ‹åˆ°è¿™ç§æ€§èƒ½æ¥è‡ªç›¸å½“ç®€å•çš„æ¨¡åž‹æž¶æž„ä»ç„¶å¾ˆæœ‰è¶£ã€‚

# ç»“è®º

TSMixer æ˜¯ä¸€ä¸ªä¸“é—¨ä¸ºå¤šå˜é‡æ—¶é—´åºåˆ—é¢„æµ‹è®¾è®¡çš„å…¨ MLP æ¨¡åž‹ã€‚

å®ƒé€šè¿‡æ·»åŠ äº¤å‰å˜é‡å‰é¦ˆå±‚æ‰©å±•äº†çº¿æ€§æ¨¡åž‹çš„èƒ½åŠ›ï¼Œä½¿æ¨¡åž‹åœ¨é•¿æ—¶é—´è·¨åº¦çš„å¤šå˜é‡é¢„æµ‹ä»»åŠ¡ä¸­è¾¾åˆ°æœ€å…ˆè¿›çš„è¡¨çŽ°ã€‚

è™½ç„¶ç›®å‰è¿˜æ²¡æœ‰çŽ°æˆçš„å®žçŽ°ï¼Œä½†ä½ çŽ°åœ¨æ‹¥æœ‰äº†è‡ªå·±å®žçŽ°å®ƒçš„çŸ¥è¯†å’ŒæŠ€èƒ½ï¼Œå› ä¸ºå®ƒç®€å•çš„æž¶æž„ä½¿å¾—æˆ‘ä»¬å¯ä»¥è½»æ¾åšåˆ°è¿™ä¸€ç‚¹ã€‚

ä¸€å¦‚æ—¢å¾€ï¼Œæ¯ä¸ªé¢„æµ‹é—®é¢˜éƒ½éœ€è¦ç‹¬ç‰¹çš„æ–¹æ³•å’Œç‰¹å®šçš„æ¨¡åž‹ï¼Œå› æ­¤è¯·ç¡®ä¿æµ‹è¯• TSMixer ä»¥åŠå…¶ä»–æ¨¡åž‹ã€‚

æ„Ÿè°¢é˜…è¯»ï¼å¸Œæœ›ä½ å–œæ¬¢ï¼Œå¹¶ä¸”å­¦åˆ°äº†ä¸€äº›æ–°ä¸œè¥¿ï¼

æƒ³è¦æŽŒæ¡æ—¶é—´åºåˆ—é¢„æµ‹å—ï¼Ÿé‚£å°±çœ‹çœ‹æˆ‘çš„è¯¾ç¨‹ [Applied Time Series Forecasting in Python](https://www.datasciencewithmarco.com/offers/zTAs2hi6/checkout?coupon_code=ATSFP10)ã€‚è¿™æ˜¯å”¯ä¸€ä¸€ä¸ªä½¿ç”¨ Python å®žçŽ°ç»Ÿè®¡ã€æ·±åº¦å­¦ä¹ å’Œæœ€å…ˆè¿›æ¨¡åž‹çš„ 16 ä¸ªå¼•å¯¼æ€§å®žè·µé¡¹ç›®çš„è¯¾ç¨‹ã€‚

å¹²æ¯ ðŸ»

# æ”¯æŒæˆ‘

å–œæ¬¢æˆ‘çš„å·¥ä½œå—ï¼Ÿé€šè¿‡ [Buy me a coffee](http://buymeacoffee.com/dswm) æ”¯æŒæˆ‘ï¼Œè¿™æ˜¯ä¸€ç§ç®€å•çš„æ–¹å¼æ¥é¼“åŠ±æˆ‘ï¼ŒåŒæ—¶æˆ‘å¯ä»¥äº«å—ä¸€æ¯å’–å•¡ï¼å¦‚æžœä½ æ„¿æ„ï¼Œåªéœ€ç‚¹å‡»ä¸‹é¢çš„æŒ‰é’® ðŸ‘‡

![](img/d8286ff73e873a825293e2ae8e2a7da3.png)

# å‚è€ƒæ–‡çŒ®

Si-An Chen, Chun-Liang Li, Nate Yoder, Sercan O. Arik, Tomas Pfister â€” [TSMixer: An All-MLP Architecture for Time Series Forecasting](https://arxiv.org/abs/2303.06053)

Google ç ”ç©¶äººå‘˜çš„ TSMixer åŽŸå§‹å®žçŽ°â€” [GitHub](https://github.com/google-research/google-research/tree/master/tsmixer/tsmixer_basic)
