["```py\nâ”œâ”€â”€ Dockerfile <--- Container\nâ”œâ”€â”€ requirements.txt <--- Libraries and Dependencies\nâ”œâ”€â”€ setup.py <--- Build and distribute microservices as Python packages\nâ””â”€â”€ src\n    â”œâ”€â”€ agents <--- Name of your Microservice\n    â”‚   â”œâ”€â”€ __init__.py\n    â”‚   â”œâ”€â”€ api\n    â”‚   â”‚   â”œâ”€â”€ __init__.py\n    â”‚   â”‚   â”œâ”€â”€ routes.py\n    â”‚   â”‚   â””â”€â”€ schemas.py\n    â”‚   â”œâ”€â”€ crud.py\n    â”‚   â”œâ”€â”€ database.py\n    â”‚   â”œâ”€â”€ main.py\n    â”‚   â”œâ”€â”€ models.py\n    â”‚   â””â”€â”€ processing.py\n    â””â”€â”€ agentsfwrk <--- Name of your Common Framework\n        â”œâ”€â”€ __init__.py\n        â”œâ”€â”€ integrations.py\n        â””â”€â”€ logger.py\n```", "```py\n# Requirements.txt\nfastapi==0.95.2\nipykernel==6.22.0\njupyter-bokeh==2.0.2\njupyterlab==3.6.3\nopenai==0.27.6\npandas==2.0.1\nsqlalchemy-orm==1.2.10\nsqlalchemy==2.0.15\nuvicorn<0.22.0,>=0.21.1\n```", "```py\n# setup.py\nfrom setuptools import find_packages, setup\n\nsetup(\n    name = 'conversational-agents',\n    version = '0.1',\n    description = 'microservices for conversational agents',\n    packages = find_packages('src'),\n    package_dir = {'': 'src'},\n    # This is optional btw\n    author = 'XXX XXXX',\n    author_email = 'XXXX@XXXXX.ai',\n    maintainer = 'XXX XXXX',\n    maintainer_email = 'XXXX@XXXXX.ai',\n)\n```", "```py\nâ””â”€â”€ agentsfwrk <--- Name of your Common Framework\n    â”œâ”€â”€ __init__.py\n    â”œâ”€â”€ integrations.py\n    â””â”€â”€ logger.py\n```", "```py\nimport logging\nimport multiprocessing\nimport sys\n\nAPP_LOGGER_NAME = 'CaiApp'\n\ndef setup_applevel_logger(logger_name = APP_LOGGER_NAME, file_name = None):\n    \"\"\"\n    Setup the logger for the application\n    \"\"\"\n    logger = logging.getLogger(logger_name)\n    logger.setLevel(logging.DEBUG)\n    formatter = logging.Formatter(\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\")\n    sh = logging.StreamHandler(sys.stdout)\n    sh.setFormatter(formatter)\n    logger.handlers.clear()\n    logger.addHandler(sh)\n    if file_name:\n        fh = logging.FileHandler(file_name)\n        fh.setFormatter(formatter)\n        logger.addHandler(fh)\n\n    return logger\n\ndef get_multiprocessing_logger(file_name = None):\n    \"\"\"\n    Setup the logger for the application for multiprocessing\n    \"\"\"\n    logger = multiprocessing.get_logger()\n    logger.setLevel(logging.DEBUG)\n    formatter = logging.Formatter(\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\")\n\n    sh = logging.StreamHandler(sys.stdout)\n    sh.setFormatter(formatter)\n\n    if not len(logger.handlers):\n        logger.addHandler(sh)\n\n    if file_name:\n        fh = logging.FileHandler(file_name)\n        fh.setFormatter(formatter)\n        logger.addHandler(fh)\n\n    return logger\n\ndef get_logger(module_name, logger_name = None):\n    \"\"\"\n    Get the logger for the module\n    \"\"\"\n    return logging.getLogger(logger_name or APP_LOGGER_NAME).getChild(module_name)\n```", "```py\n# integrations.py\n# LLM provider common module\nimport json\nimport os\nimport time\nfrom typing import Union\n\nimport openai\nfrom openai.error import APIConnectionError, APIError, RateLimitError\n\nimport agentsfwrk.logger as logger\n\nlog = logger.get_logger(__name__)\n\nopenai.api_key = os.getenv('OPENAI_API_KEY')\n\nclass OpenAIIntegrationService:\n    def __init__(\n        self,\n        context: Union[str, dict],\n        instruction: Union[str, dict]\n    ) -> None:\n\n        self.context = context\n        self.instructions = instruction\n\n        if isinstance(self.context, dict):\n            self.messages = []\n            self.messages.append(self.context)\n\n        elif isinstance(self.context, str):\n            self.messages = self.instructions + self.context\n\n    def get_models(self):\n        return openai.Model.list()\n\n    def add_chat_history(self, messages: list):\n        \"\"\"\n        Adds chat history to the conversation.\n        \"\"\"\n        self.messages += messages\n\n    def answer_to_prompt(self, model: str, prompt: str, **kwargs):\n        \"\"\"\n        Collects prompts from user, appends to messages from the same conversation\n        and return responses from the gpt models.\n        \"\"\"\n        # Preserve the messages in the conversation\n        self.messages.append(\n            {\n                'role': 'user',\n                'content': prompt\n            }\n        )\n\n        retry_exceptions = (APIError, APIConnectionError, RateLimitError)\n        for _ in range(3):\n            try:\n                response = openai.ChatCompletion.create(\n                    model       = model,\n                    messages    = self.messages,\n                    **kwargs\n                )\n                break\n            except retry_exceptions as e:\n                if _ == 2:\n                    log.error(f\"Last attempt failed, Exception occurred: {e}.\")\n                    return {\n                        \"answer\": \"Sorry, I'm having technical issues.\"\n                    }\n                retry_time = getattr(e, 'retry_after', 3)\n                log.error(f\"Exception occurred: {e}. Retrying in {retry_time} seconds...\")\n                time.sleep(retry_time)\n\n        response_message = response.choices[0].message[\"content\"]\n        response_data = {\"answer\": response_message}\n        self.messages.append(\n            {\n                'role': 'assistant',\n                'content': response_message\n            }\n        )\n\n        return response_data\n\n    def answer_to_simple_prompt(self, model: str, prompt: str, **kwargs) -> dict:\n        \"\"\"\n        Collects context and appends a prompt from a user and return response from\n        the gpt model given an instruction.\n        This method only allows one message exchange.\n        \"\"\"\n\n        messages = self.messages + f\"\\n<Client>: {prompt} \\n\"\n\n        retry_exceptions = (APIError, APIConnectionError, RateLimitError)\n        for _ in range(3):\n            try:\n                response = openai.Completion.create(\n                    model = model,\n                    prompt = messages,\n                    **kwargs\n                )\n                break\n            except retry_exceptions as e:\n                if _ == 2:\n                    log.error(f\"Last attempt failed, Exception occurred: {e}.\")\n                    return {\n                        \"intent\": False,\n                        \"answer\": \"Sorry, I'm having technical issues.\"\n                    }\n                retry_time = getattr(e, 'retry_after', 3)\n                log.error(f\"Exception occurred: {e}. Retrying in {retry_time} seconds...\")\n                time.sleep(retry_time)\n\n        response_message = response.choices[0].text\n\n        try:\n            response_data = json.loads(response_message)\n            answer_text = response_data.get('answer')\n            if answer_text is not None:\n                self.messages = self.messages + f\"\\n<Client>: {prompt} \\n\" + f\"<Agent>: {answer_text} \\n\"\n            else:\n                raise ValueError(\"The response from the model is not valid.\")\n        except ValueError as e:\n            log.error(f\"Error occurred while parsing response: {e}\")\n            log.error(f\"Prompt from the user: {prompt}\")\n            log.error(f\"Response from the model: {response_message}\")\n            log.info(\"Returning a safe response to the user.\")\n            response_data = {\n                \"intent\": False,\n                \"answer\": response_message\n            }\n\n        return response_data\n\n    def verify_end_conversation(self):\n        \"\"\"\n        Verify if the conversation has ended by checking the last message from the user\n        and the last message from the assistant.\n        \"\"\"\n        pass\n\n    def verify_goal_conversation(self, model: str, **kwargs):\n        \"\"\"\n        Verify if the conversation has reached the goal by checking the conversation history.\n        Format the response as specified in the instructions.\n        \"\"\"\n        messages = self.messages.copy()\n        messages.append(self.instructions)\n\n        retry_exceptions = (APIError, APIConnectionError, RateLimitError)\n        for _ in range(3):\n            try:\n                response = openai.ChatCompletion.create(\n                    model       = model,\n                    messages    = messages,\n                    **kwargs\n                )\n                break\n            except retry_exceptions as e:\n                if _ == 2:\n                    log.error(f\"Last attempt failed, Exception occurred: {e}.\")\n                    raise\n                retry_time = getattr(e, 'retry_after', 3)\n                log.error(f\"Exception occurred: {e}. Retrying in {retry_time} seconds...\")\n                time.sleep(retry_time)\n\n        response_message = response.choices[0].message[\"content\"]\n        try:\n            response_data = json.loads(response_message)\n            if response_data.get('summary') is None:\n                raise ValueError(\"The response from the model is not valid. Missing summary.\")\n        except ValueError as e:\n            log.error(f\"Error occurred while parsing response: {e}\")\n            log.error(f\"Response from the model: {response_message}\")\n            log.info(\"Returning a safe response to the user.\")\n            raise\n\n        return response_data\n```", "```py\nfrom fastapi import FastAPI\n\nfrom agentsfwrk.logger import setup_applevel_logger\n\nlog = setup_applevel_logger(file_name = 'agents.log')\n\napp = FastAPI()\n\n@app.get(\"/\")\nasync def root():\n    return {\"message\": \"Hello there conversational ai user!\"}\n```", "```py\nfrom sqlalchemy import create_engine\nfrom sqlalchemy.ext.declarative import declarative_base\nfrom sqlalchemy.orm import sessionmaker\n\nSQLALCHEMY_DATABASE_URL = \"sqlite:///agents.db\"\n\nengine = create_engine(\n    SQLALCHEMY_DATABASE_URL, connect_args = {\"check_same_thread\": False}\n)\nSessionLocal = sessionmaker(autocommit = False, autoflush = False, bind = engine)\n\nBase = declarative_base()\n```", "```py\nfrom typing import List\n\nfrom fastapi import APIRouter, Depends, HTTPException\nfrom sqlalchemy.orm import Session\n\nimport agents.api.schemas\nimport agents.models\nfrom agents.database import SessionLocal, engine\n\nfrom agentsfwrk import integrations, logger\n\nlog = logger.get_logger(__name__)\n\nagents.models.Base.metadata.create_all(bind = engine)\n\n# Router basic information\nrouter = APIRouter(\n    prefix = \"/agents\",\n    tags = [\"Chat\"],\n    responses = {404: {\"description\": \"Not found\"}}\n)\n\n# Dependency: Used to get the database in our endpoints.\ndef get_db():\n    db = SessionLocal()\n    try:\n        yield db\n    finally:\n        db.close()\n\n# Root endpoint for the router.\n@router.get(\"/\")\nasync def agents_root():\n    return {\"message\": \"Hello there conversational ai!\"}\n```", "```py\nfrom datetime import datetime\nfrom typing import List, Optional\nfrom pydantic import BaseModel\n\nclass AgentBase(BaseModel): # <-- Base model\n    context: str # <-- Our agents context\n    first_message: str # <-- Our agents will approach the users with a first message.\n    response_shape: str # <-- The expected shape (for programatic communication) of the response of each agent's interaction with the user\n    instructions: str # <-- Set of instructions that our agent should follow.\n\nclass AgentCreate(AgentBase): # <-- Creation data model\n    pass\n\nclass Agent(AgentBase): # <-- Agent data model\n    id: str\n    timestamp: datetime = datetime.utcnow()\n\n    class Config:\n        orm_mode = True\n```", "```py\nfrom sqlalchemy import Column, ForeignKey, String, DateTime, JSON\nfrom sqlalchemy.orm import relationship\nfrom datetime import datetime\n\nfrom agents.database import Base\n\nclass Agent(Base):\n    __tablename__ = \"agents\"\n\n    id          = Column(String, primary_key = True, index = True)\n    timestamp   = Column(DateTime, default = datetime.utcnow)\n\n    context            = Column(String, nullable = False)\n    first_message      = Column(String, nullable = False)\n    response_shape     = Column(JSON,   nullable = False)\n    instructions       = Column(String, nullable = False)\n```", "```py\n@router.post(\"/create-agent\", response_model = agents.api.schemas.Agent)\nasync def create_agent(campaign: agents.api.schemas.AgentCreate, db: Session = Depends(get_db)):\n    \"\"\"\n    Create an agent\n    \"\"\"\n    log.info(f\"Creating agent\")\n    # db_agent = create_agent(db, agent)\n    log.info(f\"Agent created with id: {db_agent.id}\")\n\n    return db_agent\n```", "```py\n# crud.py\nimport uuid\nfrom sqlalchemy.orm import Session\nfrom agents import models\nfrom agents.api import schemas\n\ndef create_agent(db: Session, agent: schemas.AgentCreate):\n    \"\"\"\n    Create an agent in the database\n    \"\"\"\n    db_agent = models.Agent(\n        id              = str(uuid.uuid4()),\n        context         = agent.context,\n        first_message   = agent.first_message,\n        response_shape  = agent.response_shape,\n        instructions    = agent.instructions\n    )\n    db.add(db_agent)\n    db.commit()\n    db.refresh(db_agent)\n\n    return db_agent\n```", "```py\nimport agents.crud\n\n@router.post(\"/create-agent\", response_model = agents.api.schemas.Agent)\nasync def create_agent(agent: agents.api.schemas.AgentCreate, db: Session = Depends(get_db)):\n    \"\"\"\n    Create an agent endpoint.\n    \"\"\"\n    log.info(f\"Creating agent: {agent.json()}\")\n    db_agent = agents.crud.create_agent(db, agent)\n    log.info(f\"Agent created with id: {db_agent.id}\")\n\n    return db_agent\n```", "```py\n# main.py\nfrom fastapi import FastAPI\n\nfrom agents.api.routes import router as ai_agents # NOTE: <-- new addition\nfrom agentsfwrk.logger import setup_applevel_logger\n\nlog = setup_applevel_logger(file_name = 'agents.log')\n\napp = FastAPI()\napp.include_router(router = ai_agents) # NOTE: <-- new addition\n\n@app.get(\"/\")\nasync def root():\n    return {\"message\": \"Hello there conversational ai user!\"}\n```", "```py\n# Run from the root of the project.\n$ pip install -e .\n# Command to run the app.\n$ uvicorn agents.main:app --host 0.0.0.0 --port 8000 --reload\n```", "```py\nclass ConversationBase(BaseModel): # <-- base of our conversations, they must belong to an agent\n    agent_id: str\n\nclass ConversationCreate(ConversationBase): # <-- conversation creation object\n    pass\n\nclass Conversation(ConversationBase): # <-- The conversation objects\n    id: str\n    timestamp: datetime = datetime.utcnow()\n\n    class Config:\n        orm_mode = True\n\nclass Agent(AgentBase): # <-- Agent data model\n    id: str\n    timestamp: datetime = datetime.utcnow()\n    conversations: List[Conversation] = [] # <-- NOTE: we have added the conversation as a list of Conversations objects.\n\n    class Config:\n        orm_mode = True\n```", "```py\n# models.py\n\nclass Agent(Base):\n    __tablename__ = \"agents\"\n\n    id          = Column(String, primary_key = True, index = True)\n    timestamp   = Column(DateTime, default = datetime.utcnow)\n\n    context            = Column(String, nullable = False)\n    first_message      = Column(String, nullable = False)\n    response_shape     = Column(JSON,   nullable = False)\n    instructions       = Column(String, nullable = False)\n\n    conversations      = relationship(\"Conversation\", back_populates = \"agent\") # <-- NOTE: We add the conversation relationship into the agents table\n\nclass Conversation(Base):\n    __tablename__ = \"conversations\"\n\n    id          = Column(String, primary_key = True, index = True)\n    agent_id    = Column(String, ForeignKey(\"agents.id\"))\n    timestap    = Column(DateTime, default = datetime.utcnow)\n\n    agent       = relationship(\"Agent\", back_populates = \"conversations\") # <-- We add the relationship between the conversation and the agent\n```", "```py\ndef get_agent(db: Session, agent_id: str):\n    \"\"\"\n    Get an agent by its id\n    \"\"\"\n    return db.query(models.Agent).filter(models.Agent.id == agent_id).first()\n\ndef get_conversation(db: Session, conversation_id: str):\n    \"\"\"\n    Get a conversation by its id\n    \"\"\"\n    return db.query(models.Conversation).filter(models.Conversation.id == conversation_id).first()\n\ndef create_conversation(db: Session, conversation: schemas.ConversationCreate):\n    \"\"\"\n    Create a conversation\n    \"\"\"\n    db_conversation = models.Conversation(\n        id          = str(uuid.uuid4()),\n        agent_id    = conversation.agent_id,\n    )\n    db.add(db_conversation)\n    db.commit()\n    db.refresh(db_conversation)\n\n    return db_conversation\n```", "```py\n@router.post(\"/create-conversation\", response_model = agents.api.schemas.Conversation)\nasync def create_conversation(conversation: agents.api.schemas.ConversationCreate, db: Session = Depends(get_db)):\n    \"\"\"\n    Create a conversation linked to an agent\n    \"\"\"\n    log.info(f\"Creating conversation assigned to agent id: {conversation.agent_id}\")\n    db_conversation = agents.crud.create_conversation(db, conversation)\n    log.info(f\"Conversation created with id: {db_conversation.id}\")\n\n    return db_conversation\n```", "```py\n##########################################\n# Internal schemas\n##########################################\nclass MessageBase(BaseModel): # <-- Every message is composed by user/client message and the agent \n    user_message: str\n    agent_message: str\n\nclass MessageCreate(MessageBase):\n    pass\n\nclass Message(MessageBase): # <-- Data model for the Message entity\n    id: str\n    timestamp: datetime = datetime.utcnow()\n    conversation_id: str\n\n    class Config:\n        orm_mode = True\n\n##########################################\n# API schemas\n##########################################\nclass UserMessage(BaseModel):\n    conversation_id: str\n    message: str\n\nclass ChatAgentResponse(BaseModel):\n    conversation_id: str\n    response: str\n```", "```py\n# models.py\n\nclass Conversation(Base):\n    __tablename__ = \"conversations\"\n\n    id          = Column(String, primary_key = True, index = True)\n    agent_id    = Column(String, ForeignKey(\"agents.id\"))\n    timestap    = Column(DateTime, default = datetime.utcnow)\n\n    agent       = relationship(\"Agent\", back_populates = \"conversations\")\n    messages    = relationship(\"Message\", back_populates = \"conversation\") # <-- We define the relationship between the conversation and the multiple messages in them.\n\nclass Message(Base):\n    __tablename__ = \"messages\"\n\n    id          = Column(String, primary_key = True, index = True)\n    timestamp   = Column(DateTime, default = datetime.utcnow)\n\n    user_message    = Column(String)\n    agent_message   = Column(String)\n\n    conversation_id = Column(String, ForeignKey(\"conversations.id\")) # <-- A message belongs to a conversation\n    conversation    = relationship(\"Conversation\", back_populates = \"messages\") # <-- We define the relationship between the messages and the conversation.\n```", "```py\ndef create_conversation_message(db: Session, message: schemas.MessageCreate, conversation_id: str):\n    \"\"\"\n    Create a message for a conversation\n    \"\"\"\n    db_message = models.Message(\n        id              = str(uuid.uuid4()),\n        user_message    = message.user_message,\n        agent_message   = message.agent_message,\n        conversation_id = conversation_id\n    )\n    db.add(db_message)\n    db.commit()\n    db.refresh(db_message)\n\n    return db_message\n```", "```py\n@router.post(\"/chat-agent\", response_model = agents.api.schemas.ChatAgentResponse)\nasync def chat_completion(message: agents.api.schemas.UserMessage, db: Session = Depends(get_db)):\n    \"\"\"\n    Get a response from the GPT model given a message from the client using the chat\n    completion endpoint.\n\n    The response is a json object with the following structure:\n    ```", "```py\n    \"\"\"\n    log.info(f\"User conversation id: {message.conversation_id}\")\n    log.info(f\"User message: {message.message}\")\n\n    conversation = agents.crud.get_conversation(db, message.conversation_id)\n\n    if not conversation:\n        # If there are no conversations, we can choose to create one on the fly OR raise an exception.\n        # Which ever you choose, make sure to uncomment when necessary.\n\n        # Option 1:\n        # conversation = agents.crud.create_conversation(db, message.conversation_id)\n\n        # Option 2:\n        return HTTPException(\n            status_code = 404,\n            detail = \"Conversation not found. Please create conversation first.\"\n        )\n\n    log.info(f\"Conversation id: {conversation.id}\")\n```", "```py\n# processing.py\n\nimport json\n\n########################################\n# Chat Properties\n########################################\ndef craft_agent_chat_context(context: str) -> dict:\n    \"\"\"\n    Craft the context for the agent to use for chat endpoints.\n    \"\"\"\n    agent_chat_context = {\n        \"role\": \"system\",\n        \"content\": context\n    }\n    return agent_chat_context\n\ndef craft_agent_chat_first_message(content: str) -> dict:\n    \"\"\"\n    Craft the first message for the agent to use for chat endpoints.\n    \"\"\"\n    agent_chat_first_message = {\n        \"role\": \"assistant\",\n        \"content\": content\n    }\n    return agent_chat_first_message\n\ndef craft_agent_chat_instructions(instructions: str, response_shape: str) -> dict:\n    \"\"\"\n    Craft the instructions for the agent to use for chat endpoints.\n    \"\"\"\n    agent_instructions = {\n        \"role\": \"user\",\n        \"content\": instructions + f\"\\n\\nFollow a RFC8259 compliant JSON with a shape of: {json.dumps(response_shape)} format without deviation.\"\n    }\n    return agent_instructions\n```", "```py\n# New imports from the processing module.\nfrom agents.processing import (\n  craft_agent_chat_context,\n  craft_agent_chat_first_message,\n  craft_agent_chat_instructions\n)\n\n@router.post(\"/chat-agent\", response_model = agents.api.schemas.ChatAgentResponse)\nasync def chat_completion(message: agents.api.schemas.UserMessage, db: Session = Depends(get_db)):\n    \"\"\"\n    Get a response from the GPT model given a message from the client using the chat\n    completion endpoint.\n\n    The response is a json object with the following structure:\n    ```", "```py\n    \"\"\"\n    log.info(f\"User conversation id: {message.conversation_id}\")\n    log.info(f\"User message: {message.message}\")\n\n    conversation = agents.crud.get_conversation(db, message.conversation_id)\n\n    if not conversation:\n        # If there are no conversations, we can choose to create one on the fly OR raise an exception.\n        # Which ever you choose, make sure to uncomment when necessary.\n\n        # Option 1:\n        # conversation = agents.crud.create_conversation(db, message.conversation_id)\n\n        # Option 2:\n        return HTTPException(\n            status_code = 404,\n            detail = \"Conversation not found. Please create conversation first.\"\n        )\n\n    log.info(f\"Conversation id: {conversation.id}\")\n\n    # NOTE: We are crafting the context first and passing the chat messages in a list\n    # appending the first message (the approach from the agent) to it.\n    context = craft_agent_chat_context(conversation.agent.context)\n    chat_messages = [craft_agent_chat_first_message(conversation.agent.first_message)]\n\n    # NOTE: Append to the conversation all messages until the last interaction from the agent\n    # If there are no messages, then this has no effect.\n    # Otherwise, we append each in order by timestamp (which makes logical sense).\n    hist_messages = conversation.messages\n    hist_messages.sort(key = lambda x: x.timestamp, reverse = False)\n    if len(hist_messages) > 0:\n        for mes in hist_messages:\n            log.info(f\"Conversation history message: {mes.user_message} | {mes.agent_message}\")\n            chat_messages.append(\n                {\n                    \"role\": \"user\",\n                    \"content\": mes.user_message\n                }\n            )\n            chat_messages.append(\n                {\n                    \"role\": \"assistant\",\n                    \"content\": mes.agent_message\n                }\n            )\n    # NOTE: We could control the conversation by simply adding\n    # rules to the length of the history.\n    if len(hist_messages) > 10:\n        # Finish the conversation gracefully.\n        log.info(\"Conversation history is too long, finishing conversation.\")\n        api_response = agents.api.schemas.ChatAgentResponse(\n            conversation_id = message.conversation_id,\n            response        = \"This conversation is over, good bye.\"\n        )\n        return api_response\n\n    # Send the message to the AI agent and get the response\n    service = integrations.OpenAIIntegrationService(\n        context = context,\n        instruction = craft_agent_chat_instructions(\n            conversation.agent.instructions,\n            conversation.agent.response_shape\n        )\n    )\n    service.add_chat_history(messages = chat_messages)\n\n    response = service.answer_to_prompt(\n        # We can test different OpenAI models.\n        model               = \"gpt-3.5-turbo\",\n        prompt              = message.message,\n        # We can test different parameters too.\n        temperature         = 0.5,\n        max_tokens          = 1000,\n        frequency_penalty   = 0.5,\n        presence_penalty    = 0\n    )\n\n    log.info(f\"Agent response: {response}\")\n\n    # Prepare response to the user\n    api_response = agents.api.schemas.ChatAgentResponse(\n        conversation_id = message.conversation_id,\n        response        = response.get('answer')\n    )\n\n    # Save interaction to database\n    db_message = agents.crud.create_conversation_message(\n        db = db,\n        conversation_id = conversation.id,\n        message = agents.api.schemas.MessageCreate(\n            user_message = message.message,\n            agent_message = response.get('answer'),\n        ),\n    )\n    log.info(f\"Conversation message id {db_message.id} saved to database\")\n\n    return api_response\n```", "```py\n{\n    \"context\": \"You are a chef specializing in Mediterranean food that provides receipts with a maximum of simple 10 ingredients. The user can have many food preferences or ingredient preferences, and your job is always to analyze and guide them to use simple ingredients for the recipes you suggest and these should also be Mediterranean. The response should include detailed information on the recipe. The response should also include questions to the user when necessary. If you think your response may be inaccurate or vague, do not write it and answer with the exact text: `I don't have a response.`\",\n    \"first_message\": \"Hello, I am your personal chef and cooking advisor and I am here to help you with your meal preferences and your cooking skills. What can I can do for you today?\",\n    \"response_shape\": \"{'recipes': 'List of strings with the name of the recipes', 'ingredients': 'List of the ingredients used in the recipes', 'summary': 'String, summary of the conversation'}\",\n    \"instructions\": \"Run through the conversation messages and discard any messages that are not relevant for cooking. Focus on extracting the recipes that were mentioned in the conversation and for each of them extract the list of ingredients. Make sure to provide a summary of the conversation when asked.\"\n}\n```", "```py\n{\n    \"agent_id\": \"Replace with the UUID from the agent you just created.\"\n} \n```", "```py\n{\n  \"conversation_id\": \"74485ccb-21e5-45bd-8008-001cbd6d3e37\",\n  \"message\": \"Hey I am interested in cooking dinner for me and my friends tomorrow night, and I need your advice on what to cook for 5 people.\"\n}\n```", "```py\n# Our question\n{\n  \"conversation_id\": \"74485ccb-21e5-45bd-8008-001cbd6d3e37\",\n  \"message\": \"There is someone that is allergic to peanuts and that's it. My only preference is seafood and anything that goes perfectly with white wine.\"\n}\n# Our agents response\n{\n  \"conversation_id\": \"74485ccb-21e5-45bd-8008-001cbd6d3e37\",\n  \"response\": \"Great! We can definitely work around the peanut allergy and incorporate seafood into the menu. How about we start with a refreshing appetizer of Grilled Shrimp Skewers with Lemon and Garlic? It's simple, delicious, and pairs well with white wine.\\n\\nHere's the recipe for Grilled Shrimp Skewers with Lemon and Garlic:\\n\\nIngredients:\\n1\\. 1 pound of large shrimp, peeled and deveined\\n2\\. 2 cloves of garlic, minced\\n3\\. 2 tablespoons of fresh lemon juice\\n4\\. 2 tablespoons of olive oil\\n5\\. Salt and pepper to taste\\n6\\. Wooden skewers, soaked in water for 30 minutes\\n\\nInstructions:\\n1\\. Preheat your grill to medium-high heat.\\n2\\. In a bowl, combine minced garlic, lemon juice, olive oil, salt, and pepper.\\n3\\. Thread the shrimp onto the soaked wooden skewers.\\n4\\. Brush the shrimp skewers with the garlic-lemon marinade on both sides.\\n5\\. Place the skewers on the preheated grill and cook for about 2-3 minutes per side until they turn pink and opaque.\\n6\\. Remove from the grill and serve hot.\\n\\nFor the main course, how about a Mediterranean-style Baked Salmon? It's flavorful, healthy, and pairs wonderfully with white wine.\\n\\nHere's the recipe for Mediterranean Baked Salmon:\\n\\nIngredients:\\n1\\. 1 pound of salmon fillets\\n2\\. 2 tablespoons of olive oil\\n3\\. 2 cloves of garlic, minced\\n4\\. 1 teaspoon of dried oregano\\n5\\. Juice of half a lemon\\n6\\. Salt and pepper to taste\\n\\nInstructions:\\n1\\. Preheat your oven to 375Â°F (190Â°C).\\n2\\. Place the salmon fillets on a baking sheet lined with parchment paper or aluminum foil.\\n3\\. In a small bowl, mix together olive oil, minced garlic, dried oregano, lemon juice, salt, and pepper.\\n4\\. Brush the mixture over the salmon fillets, ensuring they are evenly coated.\\n5\\. Bake the salmon in the preheated oven for about 12-15 minutes or until it flakes easily with a fork.\\n6\\. Remove from the oven and let it rest for a few minutes before serving.\\n\\nTo complement the meal, you can serve a side of Roasted Mediterranean Vegetables. It's a colorful and nutritious addition to your dinner.\\n\\nHere's the recipe for Roasted Mediterranean Vegetables:\\n\\nIngredients:\\n1\\. 1 red bell pepper, sliced\\n2\\. 1 yellow bell pepper, sliced\\n3\\. 1 zucchini, sliced\\n4\\. 1 eggplant, diced\\n5\\. 1 red onion, sliced\\n6\\. 2 tablespoons of olive oil\\n7\\. Salt and pepper to taste\\n8\\. Fresh herbs like basil or parsley (optional)\\n\\nInstructions:\\n1\\. Preheat your oven to 425Â°F (220Â°C).\\n2\\. In a large bowl, toss together all the vegetables with olive oil, salt, and pepper until well coated.\\n3\\. Spread the vegetables in a single layer on a baking sheet.\\n4\\. Roast in the preheated oven for about 20-25 minutes or until they are tender and slightly caramelized.\\n5\\. Remove from the oven and sprinkle with fresh herbs if desired.\\n\\nFor dessert, you can keep it simple yet satisfying with a classic Mediterranean dessert like Fresh Fruit Salad with Honey and Mint.\\n\\nHere's how you can prepare Fresh Fruit Salad with Honey and Mint:\\n\\nIngredients:\\n1\\. Assorted fresh fruits of your choice (such as berries, melons, grapes)\\n2\\. Honey for drizzling\\n3\\. Fresh mint leaves for garnish\\n\\nInstructions:\\n1\\. Wash and cut the fresh fruits into bite-sized pieces.\\n2\\. Combine all the fruits in a bowl.\\n3\\. Drizzle honey over the fruit salad and gently toss to coat.\\n4\\. Garnish with fresh mint leaves.\\n5\\. Serve chilled.\\n\\nI hope you and your friends enjoy this Mediterranean dinner menu! If you have any other questions or need further assistance, feel free to ask.\"\n}\n# Our question - Note that we are not mentioning the allergy, as we expect the agent to remember we are talking about an allergy to peanuts.\n{\n  \"conversation_id\": \"74485ccb-21e5-45bd-8008-001cbd6d3e37\",\n  \"message\": \"Is the recipe ok for my friend who is allergic?\"\n}\n# Our agents response - Note the mention of these two indices.\n{\n  \"conversation_id\": \"74485ccb-21e5-45bd-8008-001cbd6d3e37\",\n  \"response\": \"Yes, the Grilled Shrimp Skewers with Lemon and Garlic recipe should be safe for your friend with a peanut allergy. However, it's always important to double-check the ingredients you use to ensure they are free from any potential allergens or cross-contamination.\"\n}\n```", "```py\n# Dockerfile\nFROM python:3.10-slim-bullseye\n\n# Set the working directory\nWORKDIR /app\n\n# Copy the project files to the container\nCOPY . .\n\n# Install the package using setup.py\nRUN pip install -e .\n\n# Install dependencies\nRUN pip install pip -U && \\\n    pip install --no-cache-dir -r requirements.txt\n\n# Set the environment variable\nARG OPENAI_API_KEY\nENV OPENAI_API_KEY=$OPENAI_API_KEY\n\n# Expose the necessary ports\nEXPOSE 8000\n\n# Run the application\n# CMD [\"uvicorn\", \"agents.main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"]\n```", "```py\n# Build the image\n$ docker build - build-arg OPENAI_API_KEY=<Replace with your OpenAI Key> -t agents-app .\n# Run the container with the command from the agents app (Use -d flag for the detached run).\n$ docker run -p 8000:8000 agents-app uvicorn agents.main:app --host 0.0.0.0 --port 8000\n# Output\nINFO:     Started server process [1]\nINFO:     Waiting for application startup.\nINFO:     Application startup complete.\nINFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\nINFO:     172.17.0.1:41766 - \"GET / HTTP/1.1\" 200 OK\nINFO:     172.17.0.1:41766 - \"GET /favicon.ico HTTP/1.1\" 404 Not Found\nINFO:     172.17.0.1:41770 - \"GET /docs HTTP/1.1\" 200 OK\nINFO:     172.17.0.1:41770 - \"GET /openapi.json HTTP/1.1\" 200 OK\n```", "```py\n# Pin a version if you need\n$ pip install streamlit==1.25.0\n# Our requirements.txt (added streamlit)\n$ cat requirements.txt\nfastapi==0.95.2\nipykernel==6.22.0\njupyter-bokeh==2.0.2\njupyterlab==3.6.3\nopenai==0.27.6\npandas==2.0.1\nsqlalchemy-orm==1.2.10\nsqlalchemy==2.0.15\nstreamlit==1.25.0\nuvicorn<0.22.0,>=0.21.1\n```", "```py\nimport streamlit as st\nimport requests\n\nAPI_URL = \"http://0.0.0.0:8000/agents\"  # We will use our local URL and port defined of our microservice for this example\n\ndef get_agents():\n    \"\"\"\n    Get the list of available agents from the API\n    \"\"\"\n    response = requests.get(API_URL + \"/get-agents\")\n    if response.status_code == 200:\n        agents = response.json()\n        return agents\n\n    return []\n\ndef get_conversations(agent_id: str):\n    \"\"\"\n    Get the list of conversations for the agent with the given ID\n    \"\"\"\n    response = requests.get(API_URL + \"/get-conversations\", params = {\"agent_id\": agent_id})\n    if response.status_code == 200:\n        conversations = response.json()\n        return conversations\n\n    return []\n\ndef get_messages(conversation_id: str):\n    \"\"\"\n    Get the list of messages for the conversation with the given ID\n    \"\"\"\n    response = requests.get(API_URL + \"/get-messages\", params = {\"conversation_id\": conversation_id})\n    if response.status_code == 200:\n        messages = response.json()\n        return messages\n\n    return []\n\ndef send_message(agent_id, message):\n    \"\"\"\n    Send a message to the agent with the given ID\n    \"\"\"\n    payload = {\"conversation_id\": agent_id, \"message\": message}\n    response = requests.post(API_URL + \"/chat-agent\", json = payload)\n    if response.status_code == 200:\n        return response.json()\n\n    return {\"response\": \"Error\"}\n\ndef main():\n    st.set_page_config(page_title = \"ðŸ¤—ðŸ’¬ AIChat\")\n\n    with st.sidebar:\n        st.title(\"Conversational Agent Chat\")\n\n        # Dropdown to select agent\n        agents = get_agents()\n        agent_ids = [agent[\"id\"] for agent in agents]\n        selected_agent = st.selectbox(\"Select an Agent:\", agent_ids)\n\n        for agent in agents:\n            if agent[\"id\"] == selected_agent:\n                selected_agent_context = agent[\"context\"]\n                selected_agent_first_message = agent[\"first_message\"]\n\n        # Dropdown to select conversation\n        conversations = get_conversations(selected_agent)\n        conversation_ids = [conversation[\"id\"] for conversation in conversations]\n        selected_conversation = st.selectbox(\"Select a Conversation:\", conversation_ids)\n\n        if selected_conversation is None:\n            st.write(\"Please select a conversation from the dropdown.\")\n        else:\n            st.write(f\"**Selected Agent**: {selected_agent}\")\n            st.write(f\"**Selected Conversation**: {selected_conversation}\")\n\n    # Display chat messages\n    st.title(\"Chat\")\n    st.write(\"This is a chat interface for the selected agent and conversation. You can send messages to the agent and see its responses.\")\n    st.write(f\"**Agent Context**: {selected_agent_context}\")\n\n    messages = get_messages(selected_conversation)\n    with st.chat_message(\"assistant\"):\n        st.write(selected_agent_first_message)\n\n    for message in messages:\n        with st.chat_message(\"user\"):\n            st.write(message[\"user_message\"])\n        with st.chat_message(\"assistant\"):\n            st.write(message[\"agent_message\"])\n\n    # User-provided prompt\n    if prompt := st.chat_input(\"Send a message:\"):\n        with st.chat_message(\"user\"):\n            st.write(prompt)\n        with st.spinner(\"Thinking...\"):\n            response = send_message(selected_conversation, prompt)\n            with st.chat_message(\"assistant\"):\n                st.write(response[\"response\"])\n\nif __name__ == \"__main__\":\n    main()\n```"]