- en: 'Vectorize and Parallelize RL Environments with JAX: Q-learning at the Speed
    of Light⚡'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/vectorize-and-parallelize-rl-environments-with-jax-q-learning-at-the-speed-of-light-49d07373adf5?source=collection_archive---------1-----------------------#2023-10-15](https://towardsdatascience.com/vectorize-and-parallelize-rl-environments-with-jax-q-learning-at-the-speed-of-light-49d07373adf5?source=collection_archive---------1-----------------------#2023-10-15)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: In this article, we learn to vectorize an RL environment and train 30 Q-learning
    agents in parallel on a CPU, at 1.8 million iterations per second.
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@ryanpegoud?source=post_page-----49d07373adf5--------------------------------)[![Ryan
    Pégoud](../Images/9314b76c2be56bda8b73b4badf9e3e4d.png)](https://medium.com/@ryanpegoud?source=post_page-----49d07373adf5--------------------------------)[](https://towardsdatascience.com/?source=post_page-----49d07373adf5--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----49d07373adf5--------------------------------)
    [Ryan Pégoud](https://medium.com/@ryanpegoud?source=post_page-----49d07373adf5--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F27fba63b402e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fvectorize-and-parallelize-rl-environments-with-jax-q-learning-at-the-speed-of-light-49d07373adf5&user=Ryan+P%C3%A9goud&userId=27fba63b402e&source=post_page-27fba63b402e----49d07373adf5---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----49d07373adf5--------------------------------)
    ·11 min read·Oct 15, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F49d07373adf5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fvectorize-and-parallelize-rl-environments-with-jax-q-learning-at-the-speed-of-light-49d07373adf5&user=Ryan+P%C3%A9goud&userId=27fba63b402e&source=-----49d07373adf5---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F49d07373adf5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fvectorize-and-parallelize-rl-environments-with-jax-q-learning-at-the-speed-of-light-49d07373adf5&source=-----49d07373adf5---------------------bookmark_footer-----------)![](../Images/e3dd3c8e0de078193309dd3d907fe589.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: Image by [Google DeepMind](https://unsplash.com/fr/@googledeepmind) on [Unsplash](https://unsplash.com/fr)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: In the previous story, we introduced **Temporal-Difference Learning,** particularly
    **Q-learning**, in the context of a GridWorld.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: '[](/temporal-difference-learning-and-the-importance-of-exploration-an-illustrated-guide-5f9c3371413a?source=post_page-----49d07373adf5--------------------------------)
    [## Temporal-Difference Learning and the importance of exploration: An illustrated
    guide'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: A comparison of model-free (Q-learning) and model-based (Dyna-Q and Dyna-Q+)
    TD methods on a dynamic grid world.
  id: totrans-11
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/temporal-difference-learning-and-the-importance-of-exploration-an-illustrated-guide-5f9c3371413a?source=post_page-----49d07373adf5--------------------------------)
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: While this implementation served the purpose of demonstrating the differences
    in performances and exploration mechanisms of these algorithms, ***it was painfully
    slow***.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: Indeed, the environment and agents were mainly coded in **Numpy**, which is
    by no means a standard in RL, even though it makes the code easy to understand
    and debug.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: 'In this article, we’ll see how to scale up RL experiments by **vectorizing
    environments** and seamlessly **parallelizing** the training of dozens of agents
    using **JAX**. In particular, this article covers:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: JAX basics and useful features for RL
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vectorized environment and why they are so fast
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementation of an environment, policy, and Q-learning agent in JAX
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Single-agent training
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to parallelize agent training, and how easy it is!
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*All the code featured in this article is available on* [***GitHub***](https://github.com/RPegoud)*:*'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://github.com/RPegoud/jym?source=post_page-----49d07373adf5--------------------------------)
    [## GitHub - RPegoud/jym: JAX implementation of RL algorithms and vectorized environments'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: 'JAX implementation of RL algorithms and vectorized environments - GitHub -
    RPegoud/jym: JAX implementation of RL…'
  id: totrans-23
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: github.com](https://github.com/RPegoud/jym?source=post_page-----49d07373adf5--------------------------------)
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: JAX Basics
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: JAX is *yet another* Python Deep Learning framework developed by Google and
    widely used by companies such as DeepMind.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: “JAX is [Autograd](https://github.com/hips/autograd) (automatic differenciation)
    and [XLA](https://www.tensorflow.org/xla) (Accelerated Linear Algebra, a TensorFlow
    compiler), brought together for high-performance numerical computing.” — [Official
    Documentation](https://jax.readthedocs.io/en/latest/index.html)
  id: totrans-27
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: As opposed to what most Python developers are used to, JAX doesn’t embrace the
    **object-oriented programming** (OOP) paradigm, but rather **functional programming
    (FP)[1]**.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: Put simply, it relies on ***pure functions*** (**deterministic** and **without
    side effects**) and ***immutable data structures* (**instead of changing the data
    in place, **new data structures** are **created with the desired modifications)**
    as primary building blocks. As a result, FP encourages a more functional and mathematical
    approach to programming, making it well-suited for tasks like numerical computing
    and machine learning.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s illustrate the differences between those two paradigms by looking at
    pseudocode for a Q-update function:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过查看 Q 更新函数的伪代码来说明这两种范式之间的差异：
- en: The **object-oriented** approach relies on a ***class instance*** containing
    various ***state variables*** (such as the Q-values). The update function is defined
    as a class method that **updates the *internal state*** of the instance.
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**面向对象** 方法依赖于一个包含各种 ***状态变量***（如 Q 值）的 ***类实例***。更新函数被定义为一个类方法，它 **更新实例的 *内部状态***。'
- en: The **functional programming** approach relies on a ***pure function***. Indeed,
    this Q-update is **deterministic** as the Q-values are passed as an argument.
    Therefore, any call to this function with the **same inputs** will result in the
    **same outputs** whereas a class method’s outputs may depend on the internal state
    of the instance. Also, **data structures** such as arrays are **defined** and
    **modified** in the **global scope**.
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**函数式编程** 方法依赖于 ***纯函数***。实际上，这个 Q 更新是 **确定性的**，因为 Q 值作为参数传递。因此，对这个函数的任何调用只要
    **输入相同** 就会产生 **相同的输出**，而类方法的输出可能依赖于实例的内部状态。此外，**数据结构** 如数组在 **全局范围** 内被 **定义**
    和 **修改**。'
- en: '![](../Images/56e8dbf4f2b5c86ce9ee248c52e7eea1.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/56e8dbf4f2b5c86ce9ee248c52e7eea1.png)'
- en: Implementing a Q-update in **Object-Oriented Programming** and **Functional
    Programming** (made by the author)
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在 **面向对象编程** 和 **函数式编程** 中实现 Q 更新（作者制作）
- en: 'As such, JAX offers a variety of **function decorators** that are particularly
    useful in the context of RL:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，JAX 提供了各种 **函数装饰器**，在 RL 的上下文中尤为有用：
- en: '[**vmap**](https://jax.readthedocs.io/en/latest/_autosummary/jax.vmap.html#jax.vmap)
    **(vectorized map)**: Allows a function acting on a single sample to be applied
    on a **batch**. For instance, if *env.step()* is a function performing a step
    in a single environment, *vmap(env.step)()* is a function performing a step in
    **multiple environments**. In other words, vmap adds a **batch dimension** to
    a function.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**vmap**](https://jax.readthedocs.io/en/latest/_autosummary/jax.vmap.html#jax.vmap)
    **（向量化映射）**：允许作用于单个样本的函数应用于一个 **批次**。例如，如果 *env.step()* 是一个在单个环境中执行一步的函数，那么 *vmap(env.step)()*
    是一个在 **多个环境** 中执行一步的函数。换句话说，vmap 为函数添加了一个 **批次维度**。'
- en: '![](../Images/087c8d85c6e7830b41797d23bccd389e.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/087c8d85c6e7830b41797d23bccd389e.png)'
- en: Illustration of a **step** function vectorized using **vmap** (made by the author)
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 **vmap** 向量化的 **step** 函数示例（作者制作）
- en: '**j**[**it**](https://jax.readthedocs.io/en/latest/_autosummary/jax.jit.html#jax.jit)
    **(just-in-time compilation)**: Allows JAX to perform a “*Just In Time compilation
    of a JAX Python function”* making it **XLA-compatible***.* Essentially, using
    jit allows us to **compile functions** and provides **significant speed improvements**
    (in exchange for some additional overhead when first compiling the function).'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**j**[**it**](https://jax.readthedocs.io/en/latest/_autosummary/jax.jit.html#jax.jit)
    **（即时编译）**：允许 JAX 执行 “*JAX Python 函数的即时编译*” 使其 **兼容 XLA**。本质上，使用 jit 允许我们 **编译函数**
    并提供 **显著的速度提升**（以在首次编译函数时的一些额外开销为代价）。'
- en: '[**pmap**](https://jax.readthedocs.io/en/latest/_autosummary/jax.pmap.html#jax.pmap)
    **(parallel map)**: Similarly to vmap, pmap enables easy parallelization. However,
    instead of adding a batch dimension to a function, it replicates the function
    and executes it on **several XLA devices**. *Note: when applying pmap, jit is
    also applied* ***automatically****.*'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**pmap**](https://jax.readthedocs.io/en/latest/_autosummary/jax.pmap.html#jax.pmap)
    **（并行映射）**：类似于 vmap，pmap 实现了简便的并行化。然而，它不是为函数添加批次维度，而是复制函数并在 **多个 XLA 设备** 上执行它。*注意：应用
    pmap 时，jit 也会被* ***自动*** *应用*。'
- en: '![](../Images/0487f82f161ef56721ef4ffb3c518b2e.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0487f82f161ef56721ef4ffb3c518b2e.png)'
- en: Illustration of a **step** function parallelized using **pmap** (made by the
    author)
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 **pmap** 并行化的 **step** 函数示例（作者制作）
- en: Now that we have laid down the basics of JAX, we’ll see how to obtain massive
    speed-ups by vectorizing environments.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经掌握了 JAX 的基础知识，我们将探讨如何通过向量化环境获得巨大的速度提升。
- en: 'Vectorized Environments:'
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 向量化环境：
- en: First, what is a vectorized environment and what problems does vectorization
    solve?
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，什么是向量化环境，它解决了什么问题？
- en: In most cases, RL experiments are **slowed down** by **CPU-GPU data transfers**.
    Deep Learning RL algorithms such as **Proximal Policy Optimization** (PPO) use
    Neural Networks to approximate the policy.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在大多数情况下，RL 实验由于 **CPU-GPU 数据传输** 而 **变慢**。深度学习 RL 算法如 **近端策略优化**（PPO）使用神经网络来近似策略。
- en: As always in Deep Learning, Neural Networks use **GPUs** at **training** and
    **inference** time. However, in most cases, **environments** run on the **CPU**
    (even in the case of multiple environments being used in parallel).
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 像深度学习中的常规做法一样，神经网络在**训练**和**推理**时使用**GPU**。然而，在大多数情况下，**环境**运行在**CPU**上（即使在使用多个环境并行的情况下也是如此）。
- en: This means that the usual RL loop of selecting actions via the policy (Neural
    Networks) and receiving observations and rewards from the environment requires
    **constant back-and-forths** between the GPU and the CPU, which **hurts performance**.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着，通过策略（神经网络）选择动作并从环境中接收观察和奖励的常规RL循环需要**不断的来回交换**，这**影响了性能**。
- en: In addition, using frameworks such as PyTorch without *“jitting”* might cause
    some overhead, since the GPU might have to wait for Python to send back observations
    and rewards from the CPU.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，使用诸如PyTorch的框架而不进行*“jitting”*可能会导致一些开销，因为GPU可能需要等待Python将观察和奖励从CPU发送回来。
- en: '![](../Images/63569ff4a86de897ea6ec51e9117e770.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/63569ff4a86de897ea6ec51e9117e770.png)'
- en: Usual RL batched training setup in **PyTorch** (made by the author)
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 通常的RL批量训练设置在**PyTorch**中（由作者制作）
- en: On the other hand, JAX enables us to easily run batched environments on the
    GPU, removing the friction caused by GPU-CPU data transfer.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，JAX使我们能够轻松地在GPU上运行批量环境，消除由GPU-CPU数据传输引起的摩擦。
- en: Moreover, as jit compiles our JAX code to XLA, the execution is no longer (or
    at least less) affected by the inefficiency of Python.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，随着jit将我们的JAX代码编译为XLA，执行不再（或至少减少）受到Python低效的影响。
- en: '![](../Images/409f40c6f76e51efdedbb3839402877e.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/409f40c6f76e51efdedbb3839402877e.png)'
- en: RL batched training setup in **JAX** (made by the author)
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: RL批量训练设置在**JAX**中（由作者制作）
- en: For more details and exciting applications to **meta-learning RL research**,
    I highly recommend this blog post by [Chris Lu](https://chrislu.page/blog/meta-disco/).
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 有关**元学习RL研究**的更多细节和令人兴奋的应用，我强烈推荐[Chris Lu](https://chrislu.page/blog/meta-disco/)的这篇博客文章。
- en: 'Environment, Agent, and Policy implementations:'
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 环境、代理和策略实现：
- en: 'Let’s take a look at the implementation of the different parts of our RL experiment.
    Here’s a high-level overview of the basic functions we’ll need:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们查看RL实验中不同部分的实现。以下是我们需要的基本函数的高级概述：
- en: '![](../Images/725493b5fac4cb5c5d9d444a489b8166.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/725493b5fac4cb5c5d9d444a489b8166.png)'
- en: Class methods required for a simple RL setup (made by the author)
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 简单RL设置所需的类方法（由作者制作）
- en: The environment
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 环境
- en: This implementation follows the scheme provided by [Nikolaj Goodger](https://medium.com/@ngoodger_7766)
    in his great article on writing environments in JAX.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 该实现遵循[Nikolaj Goodger](https://medium.com/@ngoodger_7766)在其关于在JAX中编写环境的精彩文章中提供的方案。
- en: '[](https://medium.com/@ngoodger_7766/writing-an-rl-environment-in-jax-9f74338898ba?source=post_page-----49d07373adf5--------------------------------)
    [## Writing an RL Environment in JAX'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@ngoodger_7766/writing-an-rl-environment-in-jax-9f74338898ba?source=post_page-----49d07373adf5--------------------------------)
    [## 在JAX中编写RL环境'
- en: How to run CartPole at 1.25 Billion Step/Sec
  id: totrans-64
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 如何以1.25亿步/秒运行CartPole
- en: medium.com](https://medium.com/@ngoodger_7766/writing-an-rl-environment-in-jax-9f74338898ba?source=post_page-----49d07373adf5--------------------------------)
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: medium.com](https://medium.com/@ngoodger_7766/writing-an-rl-environment-in-jax-9f74338898ba?source=post_page-----49d07373adf5--------------------------------)
- en: 'Let’s start with a **high-level view** of the environment and its methods.
    This is a general plan for implementing an environment in JAX:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从环境及其方法的**高级视图**开始。这是实现JAX环境的一般计划：
- en: 'Let’s take a closer look at the class methods *(as a reminder, functions starting
    with “_” are* ***private*** *and shall not be called outside of the scope of the
    class)*:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更详细地查看类方法（*作为提醒，函数以“_”开头的是* ***私有的*** *，不应在类的作用域之外调用*）：
- en: '**_get_obs**: This method converts the environment state to an observation
    for the agent. In a **partially observable** or **stochastic** environment, the
    processing functions applied to the state would go here.'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**_get_obs**：此方法将环境状态转换为代理的观察。在**部分可观察**或**随机**环境中，应用于状态的处理函数将在这里。'
- en: '**_reset**: As we’ll be running multiple agents in parallel, we need a method
    for individual resets on the completion of an episode.'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**_reset**：由于我们将并行运行多个代理，因此我们需要一个方法来在完成一个回合后进行单独的重置。'
- en: '**_reset_if_done**: This method will be called at each step and trigger _reset
    if the “done” flag is set to True.'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**_reset_if_done**：此方法将在每一步调用，并在“done”标志设置为True时触发_reset。'
- en: '**reset**: This method is called at the beginning of the experiment to get
    the initial state of each agent, as well as the associated random keys'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**step**: Given a state and an action, the environment returns an observation
    (new state), a reward, and the updated “done” flag.'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In practice, a generic implementation of a GridWorld environment would look
    like this:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
- en: Notice that, as mentioned earlier, all class methods follow the **functional
    programming** paradigm. Indeed, we never update the internal state of the class
    instance. Furthermore, the **class attributes** are all **constants** that won’t
    be modified after instantiation.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s take a closer look:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: '**__init__:** In the context of our GridWorld, the available actions are **[**0,
    1, 2, 3**]**. These actions are translated into a 2-dimensional array using *self.movements*
    and added to the state in the step function.'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**_get_obs:** Our environment is **deterministic** and **fully observable**,
    therefore the agent receives the state directly instead of a processed observation.'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**_reset_if_done:** The argument *env_state* corresponds to the (state, key)
    tuple where key is a [*jax.random.PRNGKey*](https://jax.readthedocs.io/en/latest/jax.random.html)*.*
    This function simply returns the initial state if the *done* flag is set to True,
    however, we cannot use conventional Python control flow within JAX jitted functions.
    Using [*jax.lax.cond*](https://jax.readthedocs.io/en/latest/notebooks/Common_Gotchas_in_JAX.html#cond)we
    essentially get an expression equivalent to:'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '**step:** We convert the action to a movement and add it to the current state
    (*jax.numpy.clip* ensures that the agent stays within the grid). We then update
    the *env_state* tuple before checking if the environment needs to be reset. As
    the step function is used frequently throughout training, jitting it allows significant
    performance gains. The *@partial(jit, static_argnums=(0, )* decorator signals
    that the “*self”* argument of the class method should be considered **static**.
    In other words, the **class properties are constant** and won’t change during
    successive calls to the step function.'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Q-Learning Agent
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The Q-learning agent is defined by the **update** function, as well as a static
    **learning rate** and **discount factor**.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: Once again, when jitting the update function, we pass the “self” argument as
    static. Also, notice that the *q_values* matrix is modified in place using *set()*
    and its value is not stored as a class attribute.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: Epsilon-Greedy Policy
  id: totrans-84
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Finally, the policy used in this experiment is the standard **epsilon-greedy
    policy**. One important detail is that it uses **random tie-breaks**, which means
    that if the maximal Q-value is not unique, the action will be **sampled uniformly**
    from the **maximal Q-values** *(using argmax would always return the first action
    with maximal Q-value).* This is especially important if Q-values are initialized
    as a matrix of zeros, as the action 0 (move right) would always be selected.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: 'Otherwise, the policy can be summarized by this snippet:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 否则，策略可以通过这段代码总结：
- en: '[PRE1]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Note that when we use a ***key*** in JAX *(e.g. here we sampled a random float
    and used random.choice)* it is common practice to split the key afterward *(i.e.
    “move on to a new random state”, more details* [*here*](https://jax.readthedocs.io/en/latest/jax-101/05-random-numbers.html#random-numbers-in-jax)*).*
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，当我们在JAX中使用***key***时*（例如这里我们采样了一个随机浮点数并使用了random.choice）*，通常的做法是之后拆分key*（即“转到新的随机状态”，更多细节见[*这里*](https://jax.readthedocs.io/en/latest/jax-101/05-random-numbers.html#random-numbers-in-jax)）。*
- en: 'Single-agent training loop:'
  id: totrans-89
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 单代理训练循环：
- en: Now that we have all the required components, let’s train a single agent.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了所有必要的组件，让我们训练一个单一的代理。
- en: 'Here’s a ***Pythonic*** training loop, as you can see we are essentially selecting
    an action using the policy, performing a step in the environment, and updating
    the Q-values, until the end of an episode. Then we repeat the process for ***N***
    episodes. As we’ll see in a minute, this way of training an agent is quite **inefficient**,
    however, it summarizes the key steps of the algorithm in a readable way:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个***Pythonic***的训练循环，正如你所见，我们基本上是使用策略选择一个动作，在环境中执行一步，并更新Q值，直到一个回合结束。然后我们重复这个过程***N***回合。正如我们稍后会看到的，这种训练代理的方式相当**低效**，但它以一种可读的方式总结了算法的关键步骤：
- en: On a single CPU, we complete 10.000 episodes in 11 seconds, at a rate of 881
    episodes and 21 680 steps per second.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在单个CPU上，我们在11秒内完成了10,000个回合，以每秒881个回合和21,680步的速度。
- en: '[PRE2]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Now, let’s replicate the same training loop using JAX syntax. Here’s a high-level
    description of the **rollout** function:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们使用JAX语法重复相同的训练循环。以下是**rollout**函数的高级描述：
- en: '![](../Images/6e3d6e59e8ee5b273e0f2796595a0ec7.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6e3d6e59e8ee5b273e0f2796595a0ec7.png)'
- en: Training rollout function using **JAX syntax** (made by the author)
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 使用**JAX语法**的训练rollout函数（作者制作）
- en: 'To summarize, the rollout function:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 总结一下，rollout函数：
- en: '**Initializes** the **observations**, **rewards**, and **done** flags as empty
    arrays with a dimension equal to the number of time steps using *jax.numpy.zeros.*
    The **Q-values** are initialized as an empty matrix with shape **[**timesteps**+1**,
    grid_dimension_x, grid_dimension_y, n_actions**]**.'
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**初始化** **观察值**、**奖励**和**完成**标志为空数组，维度等于时间步的数量，使用*jax.numpy.zeros.* **Q值**被初始化为一个形状为**[**timesteps**+1**,
    grid_dimension_x, grid_dimension_y, n_actions**]**的空矩阵。'
- en: Calls the ***env.reset()*** function to get the initial state
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 调用***env.reset()***函数来获取初始状态
- en: Uses the ***jax.lax.fori_loop()*** function to call a ***fori_body()*** function
    ***N*** times, where ***N*** is the ***timestep*** parameter
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用***jax.lax.fori_loop()***函数调用***fori_body()***函数***N***次，其中***N***是***timestep***参数
- en: The ***fori_body()*** function behaves similarly to the previous Python loop.
    After selecting an action, performing a step, and computing the Q-update, we update
    the obs, rewards, done, and q_values arrays in place *(the Q-update targets the
    time step* ***t+1****)*.
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '***fori_body()***函数的行为类似于之前的Python循环。在选择一个动作、执行一步并计算Q更新后，我们在原地更新obs、rewards、done和q_values数组*（Q更新目标是时间步***t+1****）*。'
- en: This additional complexity leads to an **85x speed-up**, we now train our agent
    at roughly **1.83 million steps per second**. Note that here, the training is
    done on a ***single CPU*** as the environment is simplistic.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 这种额外的复杂性导致了**85倍加速**，我们现在以大约**183万步每秒**的速度训练我们的代理。请注意，这里训练是在***单个CPU***上进行的，因为环境较为简单。
- en: However, **end-to-end vectorization scales even better** when applied to **complex
    environments** and **algorithms benefitting from multiple GPUs** ([Chris Lu’s
    article](https://chrislu.page/blog/meta-disco/) reports a whopping **4000x speed-up**
    between a CleanRL PyTorch implementation of PPO and a JAX reproduction).
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，**端到端的向量化应用于** **复杂环境**和**受益于多GPU的算法**时**效果更佳**（[Chris Lu的文章](https://chrislu.page/blog/meta-disco/)报告了CleanRL
    PyTorch PPO实现与JAX复现之间惊人的**4000倍加速**）。
- en: '[PRE3]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: After training our agent, we plot the maximal Q-value for each cell (i.e. *state*)
    of the GridWorld and we observe that it has effectively learned to go from the
    initial state (bottom right corner) to the objective (top left corner).
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练我们的代理后，我们绘制了GridWorld中每个单元格（即*状态*）的最大Q值，并观察到它已经有效地学会了从初始状态（右下角）到目标（左上角）的路径。
- en: Heatmap representation of the maximal Q-value for each cell of the GridWorld
    (made by the author)
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: GridWorld中每个单元格的最大Q值的热图表示（作者制作）
- en: '**Parallel agents training loop:**'
  id: totrans-107
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**并行代理训练循环：**'
- en: As promised, now that we’ve written the functions required to train a **single
    agent**, we have little to no work left to train **multiple agents** in **parallel**
    on batched environments!
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 如承诺的那样，现在我们已经编写了训练 **单个代理** 所需的函数，剩下的工作就是在批处理环境中训练 **多个代理**，几乎没有其他工作！
- en: Thanks to **vmap** we can quickly transform our previous functions to work on
    batches of data. We only have to specify the expected input and output shapes,
    for instance for ***env.step:***
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 **vmap** 的帮助，我们可以快速将之前的函数转换为处理数据批次。我们只需指定预期的输入和输出形状，例如对于 ***env.step:***
- en: '**in_axes** = ((0,0), 0) represents the input shape, which is composed of the
    *env_state* tuple (dimension (0, 0)) and an *observation* (dimension 0).'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**in_axes** = ((0,0), 0) 表示输入形状，由 *env_state* 元组（维度 (0, 0)）和一个 *observation*（维度
    0）组成。'
- en: '**out_axes** = ((0, 0), 0, 0, 0) represents the output shape, with the output
    being ((env_state), obs, reward, done).'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**out_axes** = ((0, 0), 0, 0, 0) 表示输出形状，输出为 ((env_state), obs, reward, done)。'
- en: Now, we can call ***v_step*** on an **array** of *env_states* and *actions*
    and receive an **array** of processed *env_states*, *observations*, *rewards*,
    and *done flags.*
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 现在，我们可以在一个 **array** 的 *env_states* 和 *actions* 上调用 ***v_step***，并接收一个处理后的 **array**，其中包含
    *env_states*、*observations*、*rewards* 和 *done flags*。
- en: Note that we also **jit** all batched functions for performance (arguably, jitting
    *env.reset()* is unnecessary given that it is only called once in our training
    function).
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 注意，我们还对所有批处理函数进行了 **jit** 优化以提高性能（可以说，对 *env.reset()* 进行jit优化是多余的，因为它在我们的训练函数中只调用一次）。
- en: The last adjustment we have to make is to **add a batch dimension** to our arrays
    to account for each agent’s data.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 我们必须做的最后一个调整是 **为我们的数组添加批处理维度**，以考虑每个代理的数据。
- en: 'By doing this, we obtain a function that allows us to train **multiple agents
    in parallel**, with minimal adjustments compared to the single agent function:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这样做，我们获得了一个函数，可以在 **并行** 训练 **多个代理**，与单个代理函数相比，只需最小的调整：
- en: 'We get similar performances with this version of our training function:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这个版本的训练函数，我们得到了类似的性能：
- en: '[PRE4]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: And that’s it! Thanks for reading this far, I hope this article provided a helpful
    introduction to implementing vectorized environments in **JAX**.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 就这些了！感谢你读到这里，希望这篇文章为你提供了有关在 **JAX** 中实现矢量化环境的有用介绍。
- en: If you enjoyed the read, please consider **sharing** this article and **starring**
    my GitHub repository, thanks for your support! 🙏
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你喜欢这篇文章，请考虑 **分享** 这篇文章并 **收藏** 我的GitHub仓库，谢谢你的支持！ 🙏
- en: '[](https://github.com/RPegoud/jym?source=post_page-----49d07373adf5--------------------------------)
    [## GitHub - RPegoud/jym: JAX implementation of RL algorithms and vectorized environments'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://github.com/RPegoud/jym?source=post_page-----49d07373adf5--------------------------------)
    [## GitHub - RPegoud/jym: JAX实现的RL算法和矢量化环境'
- en: 'JAX implementation of RL algorithms and vectorized environments - GitHub -
    RPegoud/jym: JAX implementation of RL…'
  id: totrans-121
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 'JAX实现的RL算法和矢量化环境 - GitHub - RPegoud/jym: JAX实现的RL...'
- en: github.com](https://github.com/RPegoud/jym?source=post_page-----49d07373adf5--------------------------------)
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '[github.com](https://github.com/RPegoud/jym?source=post_page-----49d07373adf5--------------------------------)'
- en: 'Finally, for those interested in digging a little deeper, here’s a list of
    **useful resources** that helped me get started with JAX and redacting this article:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，对于那些希望深入了解的人，这里有一个 **有用的资源** 列表，帮助我入门JAX并撰写这篇文章：
- en: 'A curated list of awesome JAX articles and resources:'
  id: totrans-124
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 精心策划的JAX文章和资源汇总：
- en: '[1] Coderized, (functional programming) [*The purest coding style, where bugs
    are near impossible*](https://www.youtube.com/watch?v=HlgG395PQWw&t=254s), YouTube'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] Coderized, (函数式编程) [*最纯粹的编码风格，几乎不可能出错*](https://www.youtube.com/watch?v=HlgG395PQWw&t=254s),
    YouTube'
- en: '[2] Aleksa Gordić, [*JAX From Zero to Hero YouTube Playlist*](https://www.youtube.com/watch?v=SstuvS-tVc0&list=PLBoQnSflObckOARbMK9Lt98Id0AKcZurq)
    *(2022), The AI Epiphany*'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] Aleksa Gordić, [*从零到英雄的JAX YouTube播放列表*](https://www.youtube.com/watch?v=SstuvS-tVc0&list=PLBoQnSflObckOARbMK9Lt98Id0AKcZurq)
    *(2022), The AI Epiphany*'
- en: '[3] Nikolaj Goodger, [*Writing an RL Environment in JAX*](https://medium.com/@ngoodger_7766/writing-an-rl-environment-in-jax-9f74338898ba)
    *(2021)*'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] Nikolaj Goodger, [*用JAX编写RL环境*](https://medium.com/@ngoodger_7766/writing-an-rl-environment-in-jax-9f74338898ba)
    *(2021)*'
- en: '[4] Chris Lu*,* [*Achieving 4000x Speedups and Meta-Evolving Discoveries with
    PureJaxRL*](https://chrislu.page/blog/meta-disco/) *(2023),* [University of Oxford](https://www.ox.ac.uk/),
    [Foerster Lab for AI Research](https://www.foersterlab.com/)'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] Chris Lu*,* [*通过PureJaxRL实现4000倍加速和元进化发现*](https://chrislu.page/blog/meta-disco/)
    *(2023),* [牛津大学](https://www.ox.ac.uk/), [Foerster人工智能研究实验室](https://www.foersterlab.com/)'
- en: '[5] Nicholas Vadivelu,[*Awesome-JAX*](https://github.com/n2cholas/awesome-jax)
    *(2020)*, a list of JAX libraries, projects, and resources'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] Nicholas Vadivelu，[*Awesome-JAX*](https://github.com/n2cholas/awesome-jax)
    *(2020)*，一个 JAX 库、项目和资源的列表'
- en: '[6] JAX Official Documentation, [*Training a Simple Neural Network, with PyTorch
    Data Loading*](https://jax.readthedocs.io/en/latest/notebooks/Neural_Network_and_Data_Loading.html)'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '[6] JAX 官方文档，[*使用 PyTorch 数据加载训练简单神经网络*](https://jax.readthedocs.io/en/latest/notebooks/Neural_Network_and_Data_Loading.html)'
