- en: 'Vectorize and Parallelize RL Environments with JAX: Q-learning at the Speed
    of Lightâš¡'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/vectorize-and-parallelize-rl-environments-with-jax-q-learning-at-the-speed-of-light-49d07373adf5?source=collection_archive---------1-----------------------#2023-10-15](https://towardsdatascience.com/vectorize-and-parallelize-rl-environments-with-jax-q-learning-at-the-speed-of-light-49d07373adf5?source=collection_archive---------1-----------------------#2023-10-15)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: In this article, we learn to vectorize an RL environment and train 30 Q-learning
    agents in parallel on a CPU, at 1.8 million iterations per second.
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@ryanpegoud?source=post_page-----49d07373adf5--------------------------------)[![Ryan
    PÃ©goud](../Images/9314b76c2be56bda8b73b4badf9e3e4d.png)](https://medium.com/@ryanpegoud?source=post_page-----49d07373adf5--------------------------------)[](https://towardsdatascience.com/?source=post_page-----49d07373adf5--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----49d07373adf5--------------------------------)
    [Ryan PÃ©goud](https://medium.com/@ryanpegoud?source=post_page-----49d07373adf5--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: Â·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F27fba63b402e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fvectorize-and-parallelize-rl-environments-with-jax-q-learning-at-the-speed-of-light-49d07373adf5&user=Ryan+P%C3%A9goud&userId=27fba63b402e&source=post_page-27fba63b402e----49d07373adf5---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----49d07373adf5--------------------------------)
    Â·11 min readÂ·Oct 15, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F49d07373adf5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fvectorize-and-parallelize-rl-environments-with-jax-q-learning-at-the-speed-of-light-49d07373adf5&user=Ryan+P%C3%A9goud&userId=27fba63b402e&source=-----49d07373adf5---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F49d07373adf5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fvectorize-and-parallelize-rl-environments-with-jax-q-learning-at-the-speed-of-light-49d07373adf5&source=-----49d07373adf5---------------------bookmark_footer-----------)![](../Images/e3dd3c8e0de078193309dd3d907fe589.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: Image by [Google DeepMind](https://unsplash.com/fr/@googledeepmind) on [Unsplash](https://unsplash.com/fr)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: In the previous story, we introduced **Temporal-Difference Learning,** particularly
    **Q-learning**, in the context of a GridWorld.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: '[](/temporal-difference-learning-and-the-importance-of-exploration-an-illustrated-guide-5f9c3371413a?source=post_page-----49d07373adf5--------------------------------)
    [## Temporal-Difference Learning and the importance of exploration: An illustrated
    guide'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: A comparison of model-free (Q-learning) and model-based (Dyna-Q and Dyna-Q+)
    TD methods on a dynamic grid world.
  id: totrans-11
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/temporal-difference-learning-and-the-importance-of-exploration-an-illustrated-guide-5f9c3371413a?source=post_page-----49d07373adf5--------------------------------)
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: While this implementation served the purpose of demonstrating the differences
    in performances and exploration mechanisms of these algorithms, ***it was painfully
    slow***.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: Indeed, the environment and agents were mainly coded in **Numpy**, which is
    by no means a standard in RL, even though it makes the code easy to understand
    and debug.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: 'In this article, weâ€™ll see how to scale up RL experiments by **vectorizing
    environments** and seamlessly **parallelizing** the training of dozens of agents
    using **JAX**. In particular, this article covers:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: JAX basics and useful features for RL
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vectorized environment and why they are so fast
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementation of an environment, policy, and Q-learning agent in JAX
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Single-agent training
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to parallelize agent training, and how easy it is!
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*All the code featured in this article is available on* [***GitHub***](https://github.com/RPegoud)*:*'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://github.com/RPegoud/jym?source=post_page-----49d07373adf5--------------------------------)
    [## GitHub - RPegoud/jym: JAX implementation of RL algorithms and vectorized environments'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: 'JAX implementation of RL algorithms and vectorized environments - GitHub -
    RPegoud/jym: JAX implementation of RLâ€¦'
  id: totrans-23
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: github.com](https://github.com/RPegoud/jym?source=post_page-----49d07373adf5--------------------------------)
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: JAX Basics
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: JAX is *yet another* Python Deep Learning framework developed by Google and
    widely used by companies such as DeepMind.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: â€œJAX is [Autograd](https://github.com/hips/autograd) (automatic differenciation)
    and [XLA](https://www.tensorflow.org/xla) (Accelerated Linear Algebra, a TensorFlow
    compiler), brought together for high-performance numerical computing.â€ â€” [Official
    Documentation](https://jax.readthedocs.io/en/latest/index.html)
  id: totrans-27
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: As opposed to what most Python developers are used to, JAX doesnâ€™t embrace the
    **object-oriented programming** (OOP) paradigm, but rather **functional programming
    (FP)[1]**.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: Put simply, it relies on ***pure functions*** (**deterministic** and **without
    side effects**) and ***immutable data structures* (**instead of changing the data
    in place, **new data structures** are **created with the desired modifications)**
    as primary building blocks. As a result, FP encourages a more functional and mathematical
    approach to programming, making it well-suited for tasks like numerical computing
    and machine learning.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: 'Letâ€™s illustrate the differences between those two paradigms by looking at
    pseudocode for a Q-update function:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬é€šè¿‡æŸ¥çœ‹ Q æ›´æ–°å‡½æ•°çš„ä¼ªä»£ç æ¥è¯´æ˜è¿™ä¸¤ç§èŒƒå¼ä¹‹é—´çš„å·®å¼‚ï¼š
- en: The **object-oriented** approach relies on a ***class instance*** containing
    various ***state variables*** (such as the Q-values). The update function is defined
    as a class method that **updates the *internal state*** of the instance.
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**é¢å‘å¯¹è±¡** æ–¹æ³•ä¾èµ–äºä¸€ä¸ªåŒ…å«å„ç§ ***çŠ¶æ€å˜é‡***ï¼ˆå¦‚ Q å€¼ï¼‰çš„ ***ç±»å®ä¾‹***ã€‚æ›´æ–°å‡½æ•°è¢«å®šä¹‰ä¸ºä¸€ä¸ªç±»æ–¹æ³•ï¼Œå®ƒ **æ›´æ–°å®ä¾‹çš„ *å†…éƒ¨çŠ¶æ€***ã€‚'
- en: The **functional programming** approach relies on a ***pure function***. Indeed,
    this Q-update is **deterministic** as the Q-values are passed as an argument.
    Therefore, any call to this function with the **same inputs** will result in the
    **same outputs** whereas a class methodâ€™s outputs may depend on the internal state
    of the instance. Also, **data structures** such as arrays are **defined** and
    **modified** in the **global scope**.
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**å‡½æ•°å¼ç¼–ç¨‹** æ–¹æ³•ä¾èµ–äº ***çº¯å‡½æ•°***ã€‚å®é™…ä¸Šï¼Œè¿™ä¸ª Q æ›´æ–°æ˜¯ **ç¡®å®šæ€§çš„**ï¼Œå› ä¸º Q å€¼ä½œä¸ºå‚æ•°ä¼ é€’ã€‚å› æ­¤ï¼Œå¯¹è¿™ä¸ªå‡½æ•°çš„ä»»ä½•è°ƒç”¨åªè¦
    **è¾“å…¥ç›¸åŒ** å°±ä¼šäº§ç”Ÿ **ç›¸åŒçš„è¾“å‡º**ï¼Œè€Œç±»æ–¹æ³•çš„è¾“å‡ºå¯èƒ½ä¾èµ–äºå®ä¾‹çš„å†…éƒ¨çŠ¶æ€ã€‚æ­¤å¤–ï¼Œ**æ•°æ®ç»“æ„** å¦‚æ•°ç»„åœ¨ **å…¨å±€èŒƒå›´** å†…è¢« **å®šä¹‰**
    å’Œ **ä¿®æ”¹**ã€‚'
- en: '![](../Images/56e8dbf4f2b5c86ce9ee248c52e7eea1.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/56e8dbf4f2b5c86ce9ee248c52e7eea1.png)'
- en: Implementing a Q-update in **Object-Oriented Programming** and **Functional
    Programming** (made by the author)
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ **é¢å‘å¯¹è±¡ç¼–ç¨‹** å’Œ **å‡½æ•°å¼ç¼–ç¨‹** ä¸­å®ç° Q æ›´æ–°ï¼ˆä½œè€…åˆ¶ä½œï¼‰
- en: 'As such, JAX offers a variety of **function decorators** that are particularly
    useful in the context of RL:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼ŒJAX æä¾›äº†å„ç§ **å‡½æ•°è£…é¥°å™¨**ï¼Œåœ¨ RL çš„ä¸Šä¸‹æ–‡ä¸­å°¤ä¸ºæœ‰ç”¨ï¼š
- en: '[**vmap**](https://jax.readthedocs.io/en/latest/_autosummary/jax.vmap.html#jax.vmap)
    **(vectorized map)**: Allows a function acting on a single sample to be applied
    on a **batch**. For instance, if *env.step()* is a function performing a step
    in a single environment, *vmap(env.step)()* is a function performing a step in
    **multiple environments**. In other words, vmap adds a **batch dimension** to
    a function.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**vmap**](https://jax.readthedocs.io/en/latest/_autosummary/jax.vmap.html#jax.vmap)
    **ï¼ˆå‘é‡åŒ–æ˜ å°„ï¼‰**ï¼šå…è®¸ä½œç”¨äºå•ä¸ªæ ·æœ¬çš„å‡½æ•°åº”ç”¨äºä¸€ä¸ª **æ‰¹æ¬¡**ã€‚ä¾‹å¦‚ï¼Œå¦‚æœ *env.step()* æ˜¯ä¸€ä¸ªåœ¨å•ä¸ªç¯å¢ƒä¸­æ‰§è¡Œä¸€æ­¥çš„å‡½æ•°ï¼Œé‚£ä¹ˆ *vmap(env.step)()*
    æ˜¯ä¸€ä¸ªåœ¨ **å¤šä¸ªç¯å¢ƒ** ä¸­æ‰§è¡Œä¸€æ­¥çš„å‡½æ•°ã€‚æ¢å¥è¯è¯´ï¼Œvmap ä¸ºå‡½æ•°æ·»åŠ äº†ä¸€ä¸ª **æ‰¹æ¬¡ç»´åº¦**ã€‚'
- en: '![](../Images/087c8d85c6e7830b41797d23bccd389e.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/087c8d85c6e7830b41797d23bccd389e.png)'
- en: Illustration of a **step** function vectorized using **vmap** (made by the author)
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ **vmap** å‘é‡åŒ–çš„ **step** å‡½æ•°ç¤ºä¾‹ï¼ˆä½œè€…åˆ¶ä½œï¼‰
- en: '**j**[**it**](https://jax.readthedocs.io/en/latest/_autosummary/jax.jit.html#jax.jit)
    **(just-in-time compilation)**: Allows JAX to perform a â€œ*Just In Time compilation
    of a JAX Python functionâ€* making it **XLA-compatible***.* Essentially, using
    jit allows us to **compile functions** and provides **significant speed improvements**
    (in exchange for some additional overhead when first compiling the function).'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**j**[**it**](https://jax.readthedocs.io/en/latest/_autosummary/jax.jit.html#jax.jit)
    **ï¼ˆå³æ—¶ç¼–è¯‘ï¼‰**ï¼šå…è®¸ JAX æ‰§è¡Œ â€œ*JAX Python å‡½æ•°çš„å³æ—¶ç¼–è¯‘*â€ ä½¿å…¶ **å…¼å®¹ XLA**ã€‚æœ¬è´¨ä¸Šï¼Œä½¿ç”¨ jit å…è®¸æˆ‘ä»¬ **ç¼–è¯‘å‡½æ•°**
    å¹¶æä¾› **æ˜¾è‘—çš„é€Ÿåº¦æå‡**ï¼ˆä»¥åœ¨é¦–æ¬¡ç¼–è¯‘å‡½æ•°æ—¶çš„ä¸€äº›é¢å¤–å¼€é”€ä¸ºä»£ä»·ï¼‰ã€‚'
- en: '[**pmap**](https://jax.readthedocs.io/en/latest/_autosummary/jax.pmap.html#jax.pmap)
    **(parallel map)**: Similarly to vmap, pmap enables easy parallelization. However,
    instead of adding a batch dimension to a function, it replicates the function
    and executes it on **several XLA devices**. *Note: when applying pmap, jit is
    also applied* ***automatically****.*'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**pmap**](https://jax.readthedocs.io/en/latest/_autosummary/jax.pmap.html#jax.pmap)
    **ï¼ˆå¹¶è¡Œæ˜ å°„ï¼‰**ï¼šç±»ä¼¼äº vmapï¼Œpmap å®ç°äº†ç®€ä¾¿çš„å¹¶è¡ŒåŒ–ã€‚ç„¶è€Œï¼Œå®ƒä¸æ˜¯ä¸ºå‡½æ•°æ·»åŠ æ‰¹æ¬¡ç»´åº¦ï¼Œè€Œæ˜¯å¤åˆ¶å‡½æ•°å¹¶åœ¨ **å¤šä¸ª XLA è®¾å¤‡** ä¸Šæ‰§è¡Œå®ƒã€‚*æ³¨æ„ï¼šåº”ç”¨
    pmap æ—¶ï¼Œjit ä¹Ÿä¼šè¢«* ***è‡ªåŠ¨*** *åº”ç”¨*ã€‚'
- en: '![](../Images/0487f82f161ef56721ef4ffb3c518b2e.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0487f82f161ef56721ef4ffb3c518b2e.png)'
- en: Illustration of a **step** function parallelized using **pmap** (made by the
    author)
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ **pmap** å¹¶è¡ŒåŒ–çš„ **step** å‡½æ•°ç¤ºä¾‹ï¼ˆä½œè€…åˆ¶ä½œï¼‰
- en: Now that we have laid down the basics of JAX, weâ€™ll see how to obtain massive
    speed-ups by vectorizing environments.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: æ—¢ç„¶æˆ‘ä»¬å·²ç»æŒæ¡äº† JAX çš„åŸºç¡€çŸ¥è¯†ï¼Œæˆ‘ä»¬å°†æ¢è®¨å¦‚ä½•é€šè¿‡å‘é‡åŒ–ç¯å¢ƒè·å¾—å·¨å¤§çš„é€Ÿåº¦æå‡ã€‚
- en: 'Vectorized Environments:'
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å‘é‡åŒ–ç¯å¢ƒï¼š
- en: First, what is a vectorized environment and what problems does vectorization
    solve?
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: é¦–å…ˆï¼Œä»€ä¹ˆæ˜¯å‘é‡åŒ–ç¯å¢ƒï¼Œå®ƒè§£å†³äº†ä»€ä¹ˆé—®é¢˜ï¼Ÿ
- en: In most cases, RL experiments are **slowed down** by **CPU-GPU data transfers**.
    Deep Learning RL algorithms such as **Proximal Policy Optimization** (PPO) use
    Neural Networks to approximate the policy.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å¤§å¤šæ•°æƒ…å†µä¸‹ï¼ŒRL å®éªŒç”±äº **CPU-GPU æ•°æ®ä¼ è¾“** è€Œ **å˜æ…¢**ã€‚æ·±åº¦å­¦ä¹  RL ç®—æ³•å¦‚ **è¿‘ç«¯ç­–ç•¥ä¼˜åŒ–**ï¼ˆPPOï¼‰ä½¿ç”¨ç¥ç»ç½‘ç»œæ¥è¿‘ä¼¼ç­–ç•¥ã€‚
- en: As always in Deep Learning, Neural Networks use **GPUs** at **training** and
    **inference** time. However, in most cases, **environments** run on the **CPU**
    (even in the case of multiple environments being used in parallel).
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: åƒæ·±åº¦å­¦ä¹ ä¸­çš„å¸¸è§„åšæ³•ä¸€æ ·ï¼Œç¥ç»ç½‘ç»œåœ¨**è®­ç»ƒ**å’Œ**æ¨ç†**æ—¶ä½¿ç”¨**GPU**ã€‚ç„¶è€Œï¼Œåœ¨å¤§å¤šæ•°æƒ…å†µä¸‹ï¼Œ**ç¯å¢ƒ**è¿è¡Œåœ¨**CPU**ä¸Šï¼ˆå³ä½¿åœ¨ä½¿ç”¨å¤šä¸ªç¯å¢ƒå¹¶è¡Œçš„æƒ…å†µä¸‹ä¹Ÿæ˜¯å¦‚æ­¤ï¼‰ã€‚
- en: This means that the usual RL loop of selecting actions via the policy (Neural
    Networks) and receiving observations and rewards from the environment requires
    **constant back-and-forths** between the GPU and the CPU, which **hurts performance**.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ„å‘³ç€ï¼Œé€šè¿‡ç­–ç•¥ï¼ˆç¥ç»ç½‘ç»œï¼‰é€‰æ‹©åŠ¨ä½œå¹¶ä»ç¯å¢ƒä¸­æ¥æ”¶è§‚å¯Ÿå’Œå¥–åŠ±çš„å¸¸è§„RLå¾ªç¯éœ€è¦**ä¸æ–­çš„æ¥å›äº¤æ¢**ï¼Œè¿™**å½±å“äº†æ€§èƒ½**ã€‚
- en: In addition, using frameworks such as PyTorch without *â€œjittingâ€* might cause
    some overhead, since the GPU might have to wait for Python to send back observations
    and rewards from the CPU.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤å¤–ï¼Œä½¿ç”¨è¯¸å¦‚PyTorchçš„æ¡†æ¶è€Œä¸è¿›è¡Œ*â€œjittingâ€*å¯èƒ½ä¼šå¯¼è‡´ä¸€äº›å¼€é”€ï¼Œå› ä¸ºGPUå¯èƒ½éœ€è¦ç­‰å¾…Pythonå°†è§‚å¯Ÿå’Œå¥–åŠ±ä»CPUå‘é€å›æ¥ã€‚
- en: '![](../Images/63569ff4a86de897ea6ec51e9117e770.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/63569ff4a86de897ea6ec51e9117e770.png)'
- en: Usual RL batched training setup in **PyTorch** (made by the author)
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: é€šå¸¸çš„RLæ‰¹é‡è®­ç»ƒè®¾ç½®åœ¨**PyTorch**ä¸­ï¼ˆç”±ä½œè€…åˆ¶ä½œï¼‰
- en: On the other hand, JAX enables us to easily run batched environments on the
    GPU, removing the friction caused by GPU-CPU data transfer.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: å¦ä¸€æ–¹é¢ï¼ŒJAXä½¿æˆ‘ä»¬èƒ½å¤Ÿè½»æ¾åœ°åœ¨GPUä¸Šè¿è¡Œæ‰¹é‡ç¯å¢ƒï¼Œæ¶ˆé™¤ç”±GPU-CPUæ•°æ®ä¼ è¾“å¼•èµ·çš„æ‘©æ“¦ã€‚
- en: Moreover, as jit compiles our JAX code to XLA, the execution is no longer (or
    at least less) affected by the inefficiency of Python.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤å¤–ï¼Œéšç€jitå°†æˆ‘ä»¬çš„JAXä»£ç ç¼–è¯‘ä¸ºXLAï¼Œæ‰§è¡Œä¸å†ï¼ˆæˆ–è‡³å°‘å‡å°‘ï¼‰å—åˆ°Pythonä½æ•ˆçš„å½±å“ã€‚
- en: '![](../Images/409f40c6f76e51efdedbb3839402877e.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/409f40c6f76e51efdedbb3839402877e.png)'
- en: RL batched training setup in **JAX** (made by the author)
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: RLæ‰¹é‡è®­ç»ƒè®¾ç½®åœ¨**JAX**ä¸­ï¼ˆç”±ä½œè€…åˆ¶ä½œï¼‰
- en: For more details and exciting applications to **meta-learning RL research**,
    I highly recommend this blog post by [Chris Lu](https://chrislu.page/blog/meta-disco/).
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: æœ‰å…³**å…ƒå­¦ä¹ RLç ”ç©¶**çš„æ›´å¤šç»†èŠ‚å’Œä»¤äººå…´å¥‹çš„åº”ç”¨ï¼Œæˆ‘å¼ºçƒˆæ¨è[Chris Lu](https://chrislu.page/blog/meta-disco/)çš„è¿™ç¯‡åšå®¢æ–‡ç« ã€‚
- en: 'Environment, Agent, and Policy implementations:'
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç¯å¢ƒã€ä»£ç†å’Œç­–ç•¥å®ç°ï¼š
- en: 'Letâ€™s take a look at the implementation of the different parts of our RL experiment.
    Hereâ€™s a high-level overview of the basic functions weâ€™ll need:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬æŸ¥çœ‹RLå®éªŒä¸­ä¸åŒéƒ¨åˆ†çš„å®ç°ã€‚ä»¥ä¸‹æ˜¯æˆ‘ä»¬éœ€è¦çš„åŸºæœ¬å‡½æ•°çš„é«˜çº§æ¦‚è¿°ï¼š
- en: '![](../Images/725493b5fac4cb5c5d9d444a489b8166.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/725493b5fac4cb5c5d9d444a489b8166.png)'
- en: Class methods required for a simple RL setup (made by the author)
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: ç®€å•RLè®¾ç½®æ‰€éœ€çš„ç±»æ–¹æ³•ï¼ˆç”±ä½œè€…åˆ¶ä½œï¼‰
- en: The environment
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç¯å¢ƒ
- en: This implementation follows the scheme provided by [Nikolaj Goodger](https://medium.com/@ngoodger_7766)
    in his great article on writing environments in JAX.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: è¯¥å®ç°éµå¾ª[Nikolaj Goodger](https://medium.com/@ngoodger_7766)åœ¨å…¶å…³äºåœ¨JAXä¸­ç¼–å†™ç¯å¢ƒçš„ç²¾å½©æ–‡ç« ä¸­æä¾›çš„æ–¹æ¡ˆã€‚
- en: '[](https://medium.com/@ngoodger_7766/writing-an-rl-environment-in-jax-9f74338898ba?source=post_page-----49d07373adf5--------------------------------)
    [## Writing an RL Environment in JAX'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@ngoodger_7766/writing-an-rl-environment-in-jax-9f74338898ba?source=post_page-----49d07373adf5--------------------------------)
    [## åœ¨JAXä¸­ç¼–å†™RLç¯å¢ƒ'
- en: How to run CartPole at 1.25 Billion Step/Sec
  id: totrans-64
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: å¦‚ä½•ä»¥1.25äº¿æ­¥/ç§’è¿è¡ŒCartPole
- en: medium.com](https://medium.com/@ngoodger_7766/writing-an-rl-environment-in-jax-9f74338898ba?source=post_page-----49d07373adf5--------------------------------)
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: medium.com](https://medium.com/@ngoodger_7766/writing-an-rl-environment-in-jax-9f74338898ba?source=post_page-----49d07373adf5--------------------------------)
- en: 'Letâ€™s start with a **high-level view** of the environment and its methods.
    This is a general plan for implementing an environment in JAX:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬ä»ç¯å¢ƒåŠå…¶æ–¹æ³•çš„**é«˜çº§è§†å›¾**å¼€å§‹ã€‚è¿™æ˜¯å®ç°JAXç¯å¢ƒçš„ä¸€èˆ¬è®¡åˆ’ï¼š
- en: 'Letâ€™s take a closer look at the class methods *(as a reminder, functions starting
    with â€œ_â€ are* ***private*** *and shall not be called outside of the scope of the
    class)*:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬æ›´è¯¦ç»†åœ°æŸ¥çœ‹ç±»æ–¹æ³•ï¼ˆ*ä½œä¸ºæé†’ï¼Œå‡½æ•°ä»¥â€œ_â€å¼€å¤´çš„æ˜¯* ***ç§æœ‰çš„*** *ï¼Œä¸åº”åœ¨ç±»çš„ä½œç”¨åŸŸä¹‹å¤–è°ƒç”¨*ï¼‰ï¼š
- en: '**_get_obs**: This method converts the environment state to an observation
    for the agent. In a **partially observable** or **stochastic** environment, the
    processing functions applied to the state would go here.'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**_get_obs**ï¼šæ­¤æ–¹æ³•å°†ç¯å¢ƒçŠ¶æ€è½¬æ¢ä¸ºä»£ç†çš„è§‚å¯Ÿã€‚åœ¨**éƒ¨åˆ†å¯è§‚å¯Ÿ**æˆ–**éšæœº**ç¯å¢ƒä¸­ï¼Œåº”ç”¨äºçŠ¶æ€çš„å¤„ç†å‡½æ•°å°†åœ¨è¿™é‡Œã€‚'
- en: '**_reset**: As weâ€™ll be running multiple agents in parallel, we need a method
    for individual resets on the completion of an episode.'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**_reset**ï¼šç”±äºæˆ‘ä»¬å°†å¹¶è¡Œè¿è¡Œå¤šä¸ªä»£ç†ï¼Œå› æ­¤æˆ‘ä»¬éœ€è¦ä¸€ä¸ªæ–¹æ³•æ¥åœ¨å®Œæˆä¸€ä¸ªå›åˆåè¿›è¡Œå•ç‹¬çš„é‡ç½®ã€‚'
- en: '**_reset_if_done**: This method will be called at each step and trigger _reset
    if the â€œdoneâ€ flag is set to True.'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**_reset_if_done**ï¼šæ­¤æ–¹æ³•å°†åœ¨æ¯ä¸€æ­¥è°ƒç”¨ï¼Œå¹¶åœ¨â€œdoneâ€æ ‡å¿—è®¾ç½®ä¸ºTrueæ—¶è§¦å‘_resetã€‚'
- en: '**reset**: This method is called at the beginning of the experiment to get
    the initial state of each agent, as well as the associated random keys'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**step**: Given a state and an action, the environment returns an observation
    (new state), a reward, and the updated â€œdoneâ€ flag.'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In practice, a generic implementation of a GridWorld environment would look
    like this:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
- en: Notice that, as mentioned earlier, all class methods follow the **functional
    programming** paradigm. Indeed, we never update the internal state of the class
    instance. Furthermore, the **class attributes** are all **constants** that wonâ€™t
    be modified after instantiation.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: 'Letâ€™s take a closer look:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: '**__init__:** In the context of our GridWorld, the available actions are **[**0,
    1, 2, 3**]**. These actions are translated into a 2-dimensional array using *self.movements*
    and added to the state in the step function.'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**_get_obs:** Our environment is **deterministic** and **fully observable**,
    therefore the agent receives the state directly instead of a processed observation.'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**_reset_if_done:** The argument *env_state* corresponds to the (state, key)
    tuple where key is a [*jax.random.PRNGKey*](https://jax.readthedocs.io/en/latest/jax.random.html)*.*
    This function simply returns the initial state if the *done* flag is set to True,
    however, we cannot use conventional Python control flow within JAX jitted functions.
    Using [*jax.lax.cond*](https://jax.readthedocs.io/en/latest/notebooks/Common_Gotchas_in_JAX.html#cond)we
    essentially get an expression equivalent to:'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '**step:** We convert the action to a movement and add it to the current state
    (*jax.numpy.clip* ensures that the agent stays within the grid). We then update
    the *env_state* tuple before checking if the environment needs to be reset. As
    the step function is used frequently throughout training, jitting it allows significant
    performance gains. The *@partial(jit, static_argnums=(0, )* decorator signals
    that the â€œ*selfâ€* argument of the class method should be considered **static**.
    In other words, the **class properties are constant** and wonâ€™t change during
    successive calls to the step function.'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Q-Learning Agent
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The Q-learning agent is defined by the **update** function, as well as a static
    **learning rate** and **discount factor**.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: Once again, when jitting the update function, we pass the â€œselfâ€ argument as
    static. Also, notice that the *q_values* matrix is modified in place using *set()*
    and its value is not stored as a class attribute.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: Epsilon-Greedy Policy
  id: totrans-84
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Finally, the policy used in this experiment is the standard **epsilon-greedy
    policy**. One important detail is that it uses **random tie-breaks**, which means
    that if the maximal Q-value is not unique, the action will be **sampled uniformly**
    from the **maximal Q-values** *(using argmax would always return the first action
    with maximal Q-value).* This is especially important if Q-values are initialized
    as a matrix of zeros, as the action 0 (move right) would always be selected.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: 'Otherwise, the policy can be summarized by this snippet:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: å¦åˆ™ï¼Œç­–ç•¥å¯ä»¥é€šè¿‡è¿™æ®µä»£ç æ€»ç»“ï¼š
- en: '[PRE1]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Note that when we use a ***key*** in JAX *(e.g. here we sampled a random float
    and used random.choice)* it is common practice to split the key afterward *(i.e.
    â€œmove on to a new random stateâ€, more details* [*here*](https://jax.readthedocs.io/en/latest/jax-101/05-random-numbers.html#random-numbers-in-jax)*).*
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: æ³¨æ„ï¼Œå½“æˆ‘ä»¬åœ¨JAXä¸­ä½¿ç”¨***key***æ—¶*ï¼ˆä¾‹å¦‚è¿™é‡Œæˆ‘ä»¬é‡‡æ ·äº†ä¸€ä¸ªéšæœºæµ®ç‚¹æ•°å¹¶ä½¿ç”¨äº†random.choiceï¼‰*ï¼Œé€šå¸¸çš„åšæ³•æ˜¯ä¹‹åæ‹†åˆ†key*ï¼ˆå³â€œè½¬åˆ°æ–°çš„éšæœºçŠ¶æ€â€ï¼Œæ›´å¤šç»†èŠ‚è§[*è¿™é‡Œ*](https://jax.readthedocs.io/en/latest/jax-101/05-random-numbers.html#random-numbers-in-jax)ï¼‰ã€‚*
- en: 'Single-agent training loop:'
  id: totrans-89
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å•ä»£ç†è®­ç»ƒå¾ªç¯ï¼š
- en: Now that we have all the required components, letâ€™s train a single agent.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬æœ‰äº†æ‰€æœ‰å¿…è¦çš„ç»„ä»¶ï¼Œè®©æˆ‘ä»¬è®­ç»ƒä¸€ä¸ªå•ä¸€çš„ä»£ç†ã€‚
- en: 'Hereâ€™s a ***Pythonic*** training loop, as you can see we are essentially selecting
    an action using the policy, performing a step in the environment, and updating
    the Q-values, until the end of an episode. Then we repeat the process for ***N***
    episodes. As weâ€™ll see in a minute, this way of training an agent is quite **inefficient**,
    however, it summarizes the key steps of the algorithm in a readable way:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯ä¸€ä¸ª***Pythonic***çš„è®­ç»ƒå¾ªç¯ï¼Œæ­£å¦‚ä½ æ‰€è§ï¼Œæˆ‘ä»¬åŸºæœ¬ä¸Šæ˜¯ä½¿ç”¨ç­–ç•¥é€‰æ‹©ä¸€ä¸ªåŠ¨ä½œï¼Œåœ¨ç¯å¢ƒä¸­æ‰§è¡Œä¸€æ­¥ï¼Œå¹¶æ›´æ–°Qå€¼ï¼Œç›´åˆ°ä¸€ä¸ªå›åˆç»“æŸã€‚ç„¶åæˆ‘ä»¬é‡å¤è¿™ä¸ªè¿‡ç¨‹***N***å›åˆã€‚æ­£å¦‚æˆ‘ä»¬ç¨åä¼šçœ‹åˆ°çš„ï¼Œè¿™ç§è®­ç»ƒä»£ç†çš„æ–¹å¼ç›¸å½“**ä½æ•ˆ**ï¼Œä½†å®ƒä»¥ä¸€ç§å¯è¯»çš„æ–¹å¼æ€»ç»“äº†ç®—æ³•çš„å…³é”®æ­¥éª¤ï¼š
- en: On a single CPU, we complete 10.000 episodes in 11 seconds, at a rate of 881
    episodes and 21 680 steps per second.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å•ä¸ªCPUä¸Šï¼Œæˆ‘ä»¬åœ¨11ç§’å†…å®Œæˆäº†10,000ä¸ªå›åˆï¼Œä»¥æ¯ç§’881ä¸ªå›åˆå’Œ21,680æ­¥çš„é€Ÿåº¦ã€‚
- en: '[PRE2]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Now, letâ€™s replicate the same training loop using JAX syntax. Hereâ€™s a high-level
    description of the **rollout** function:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼Œè®©æˆ‘ä»¬ä½¿ç”¨JAXè¯­æ³•é‡å¤ç›¸åŒçš„è®­ç»ƒå¾ªç¯ã€‚ä»¥ä¸‹æ˜¯**rollout**å‡½æ•°çš„é«˜çº§æè¿°ï¼š
- en: '![](../Images/6e3d6e59e8ee5b273e0f2796595a0ec7.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6e3d6e59e8ee5b273e0f2796595a0ec7.png)'
- en: Training rollout function using **JAX syntax** (made by the author)
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨**JAXè¯­æ³•**çš„è®­ç»ƒrolloutå‡½æ•°ï¼ˆä½œè€…åˆ¶ä½œï¼‰
- en: 'To summarize, the rollout function:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: æ€»ç»“ä¸€ä¸‹ï¼Œrolloutå‡½æ•°ï¼š
- en: '**Initializes** the **observations**, **rewards**, and **done** flags as empty
    arrays with a dimension equal to the number of time steps using *jax.numpy.zeros.*
    The **Q-values** are initialized as an empty matrix with shape **[**timesteps**+1**,
    grid_dimension_x, grid_dimension_y, n_actions**]**.'
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**åˆå§‹åŒ–** **è§‚å¯Ÿå€¼**ã€**å¥–åŠ±**å’Œ**å®Œæˆ**æ ‡å¿—ä¸ºç©ºæ•°ç»„ï¼Œç»´åº¦ç­‰äºæ—¶é—´æ­¥çš„æ•°é‡ï¼Œä½¿ç”¨*jax.numpy.zeros.* **Qå€¼**è¢«åˆå§‹åŒ–ä¸ºä¸€ä¸ªå½¢çŠ¶ä¸º**[**timesteps**+1**,
    grid_dimension_x, grid_dimension_y, n_actions**]**çš„ç©ºçŸ©é˜µã€‚'
- en: Calls the ***env.reset()*** function to get the initial state
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: è°ƒç”¨***env.reset()***å‡½æ•°æ¥è·å–åˆå§‹çŠ¶æ€
- en: Uses the ***jax.lax.fori_loop()*** function to call a ***fori_body()*** function
    ***N*** times, where ***N*** is the ***timestep*** parameter
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ä½¿ç”¨***jax.lax.fori_loop()***å‡½æ•°è°ƒç”¨***fori_body()***å‡½æ•°***N***æ¬¡ï¼Œå…¶ä¸­***N***æ˜¯***timestep***å‚æ•°
- en: The ***fori_body()*** function behaves similarly to the previous Python loop.
    After selecting an action, performing a step, and computing the Q-update, we update
    the obs, rewards, done, and q_values arrays in place *(the Q-update targets the
    time step* ***t+1****)*.
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '***fori_body()***å‡½æ•°çš„è¡Œä¸ºç±»ä¼¼äºä¹‹å‰çš„Pythonå¾ªç¯ã€‚åœ¨é€‰æ‹©ä¸€ä¸ªåŠ¨ä½œã€æ‰§è¡Œä¸€æ­¥å¹¶è®¡ç®—Qæ›´æ–°åï¼Œæˆ‘ä»¬åœ¨åŸåœ°æ›´æ–°obsã€rewardsã€doneå’Œq_valuesæ•°ç»„*ï¼ˆQæ›´æ–°ç›®æ ‡æ˜¯æ—¶é—´æ­¥***t+1****ï¼‰*ã€‚'
- en: This additional complexity leads to an **85x speed-up**, we now train our agent
    at roughly **1.83 million steps per second**. Note that here, the training is
    done on a ***single CPU*** as the environment is simplistic.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ç§é¢å¤–çš„å¤æ‚æ€§å¯¼è‡´äº†**85å€åŠ é€Ÿ**ï¼Œæˆ‘ä»¬ç°åœ¨ä»¥å¤§çº¦**183ä¸‡æ­¥æ¯ç§’**çš„é€Ÿåº¦è®­ç»ƒæˆ‘ä»¬çš„ä»£ç†ã€‚è¯·æ³¨æ„ï¼Œè¿™é‡Œè®­ç»ƒæ˜¯åœ¨***å•ä¸ªCPU***ä¸Šè¿›è¡Œçš„ï¼Œå› ä¸ºç¯å¢ƒè¾ƒä¸ºç®€å•ã€‚
- en: However, **end-to-end vectorization scales even better** when applied to **complex
    environments** and **algorithms benefitting from multiple GPUs** ([Chris Luâ€™s
    article](https://chrislu.page/blog/meta-disco/) reports a whopping **4000x speed-up**
    between a CleanRL PyTorch implementation of PPO and a JAX reproduction).
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶è€Œï¼Œ**ç«¯åˆ°ç«¯çš„å‘é‡åŒ–åº”ç”¨äº** **å¤æ‚ç¯å¢ƒ**å’Œ**å—ç›Šäºå¤šGPUçš„ç®—æ³•**æ—¶**æ•ˆæœæ›´ä½³**ï¼ˆ[Chris Luçš„æ–‡ç« ](https://chrislu.page/blog/meta-disco/)æŠ¥å‘Šäº†CleanRL
    PyTorch PPOå®ç°ä¸JAXå¤ç°ä¹‹é—´æƒŠäººçš„**4000å€åŠ é€Ÿ**ï¼‰ã€‚
- en: '[PRE3]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: After training our agent, we plot the maximal Q-value for each cell (i.e. *state*)
    of the GridWorld and we observe that it has effectively learned to go from the
    initial state (bottom right corner) to the objective (top left corner).
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è®­ç»ƒæˆ‘ä»¬çš„ä»£ç†åï¼Œæˆ‘ä»¬ç»˜åˆ¶äº†GridWorldä¸­æ¯ä¸ªå•å…ƒæ ¼ï¼ˆå³*çŠ¶æ€*ï¼‰çš„æœ€å¤§Qå€¼ï¼Œå¹¶è§‚å¯Ÿåˆ°å®ƒå·²ç»æœ‰æ•ˆåœ°å­¦ä¼šäº†ä»åˆå§‹çŠ¶æ€ï¼ˆå³ä¸‹è§’ï¼‰åˆ°ç›®æ ‡ï¼ˆå·¦ä¸Šè§’ï¼‰çš„è·¯å¾„ã€‚
- en: Heatmap representation of the maximal Q-value for each cell of the GridWorld
    (made by the author)
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: GridWorldä¸­æ¯ä¸ªå•å…ƒæ ¼çš„æœ€å¤§Qå€¼çš„çƒ­å›¾è¡¨ç¤ºï¼ˆä½œè€…åˆ¶ä½œï¼‰
- en: '**Parallel agents training loop:**'
  id: totrans-107
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**å¹¶è¡Œä»£ç†è®­ç»ƒå¾ªç¯ï¼š**'
- en: As promised, now that weâ€™ve written the functions required to train a **single
    agent**, we have little to no work left to train **multiple agents** in **parallel**
    on batched environments!
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æ‰¿è¯ºçš„é‚£æ ·ï¼Œç°åœ¨æˆ‘ä»¬å·²ç»ç¼–å†™äº†è®­ç»ƒ **å•ä¸ªä»£ç†** æ‰€éœ€çš„å‡½æ•°ï¼Œå‰©ä¸‹çš„å·¥ä½œå°±æ˜¯åœ¨æ‰¹å¤„ç†ç¯å¢ƒä¸­è®­ç»ƒ **å¤šä¸ªä»£ç†**ï¼Œå‡ ä¹æ²¡æœ‰å…¶ä»–å·¥ä½œï¼
- en: Thanks to **vmap** we can quickly transform our previous functions to work on
    batches of data. We only have to specify the expected input and output shapes,
    for instance for ***env.step:***
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±äº **vmap** çš„å¸®åŠ©ï¼Œæˆ‘ä»¬å¯ä»¥å¿«é€Ÿå°†ä¹‹å‰çš„å‡½æ•°è½¬æ¢ä¸ºå¤„ç†æ•°æ®æ‰¹æ¬¡ã€‚æˆ‘ä»¬åªéœ€æŒ‡å®šé¢„æœŸçš„è¾“å…¥å’Œè¾“å‡ºå½¢çŠ¶ï¼Œä¾‹å¦‚å¯¹äº ***env.step:***
- en: '**in_axes** = ((0,0), 0) represents the input shape, which is composed of the
    *env_state* tuple (dimension (0, 0)) and an *observation* (dimension 0).'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**in_axes** = ((0,0), 0) è¡¨ç¤ºè¾“å…¥å½¢çŠ¶ï¼Œç”± *env_state* å…ƒç»„ï¼ˆç»´åº¦ (0, 0)ï¼‰å’Œä¸€ä¸ª *observation*ï¼ˆç»´åº¦
    0ï¼‰ç»„æˆã€‚'
- en: '**out_axes** = ((0, 0), 0, 0, 0) represents the output shape, with the output
    being ((env_state), obs, reward, done).'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**out_axes** = ((0, 0), 0, 0, 0) è¡¨ç¤ºè¾“å‡ºå½¢çŠ¶ï¼Œè¾“å‡ºä¸º ((env_state), obs, reward, done)ã€‚'
- en: Now, we can call ***v_step*** on an **array** of *env_states* and *actions*
    and receive an **array** of processed *env_states*, *observations*, *rewards*,
    and *done flags.*
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼Œæˆ‘ä»¬å¯ä»¥åœ¨ä¸€ä¸ª **array** çš„ *env_states* å’Œ *actions* ä¸Šè°ƒç”¨ ***v_step***ï¼Œå¹¶æ¥æ”¶ä¸€ä¸ªå¤„ç†åçš„ **array**ï¼Œå…¶ä¸­åŒ…å«
    *env_states*ã€*observations*ã€*rewards* å’Œ *done flags*ã€‚
- en: Note that we also **jit** all batched functions for performance (arguably, jitting
    *env.reset()* is unnecessary given that it is only called once in our training
    function).
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ³¨æ„ï¼Œæˆ‘ä»¬è¿˜å¯¹æ‰€æœ‰æ‰¹å¤„ç†å‡½æ•°è¿›è¡Œäº† **jit** ä¼˜åŒ–ä»¥æé«˜æ€§èƒ½ï¼ˆå¯ä»¥è¯´ï¼Œå¯¹ *env.reset()* è¿›è¡Œjitä¼˜åŒ–æ˜¯å¤šä½™çš„ï¼Œå› ä¸ºå®ƒåœ¨æˆ‘ä»¬çš„è®­ç»ƒå‡½æ•°ä¸­åªè°ƒç”¨ä¸€æ¬¡ï¼‰ã€‚
- en: The last adjustment we have to make is to **add a batch dimension** to our arrays
    to account for each agentâ€™s data.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¿…é¡»åšçš„æœ€åä¸€ä¸ªè°ƒæ•´æ˜¯ **ä¸ºæˆ‘ä»¬çš„æ•°ç»„æ·»åŠ æ‰¹å¤„ç†ç»´åº¦**ï¼Œä»¥è€ƒè™‘æ¯ä¸ªä»£ç†çš„æ•°æ®ã€‚
- en: 'By doing this, we obtain a function that allows us to train **multiple agents
    in parallel**, with minimal adjustments compared to the single agent function:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: é€šè¿‡è¿™æ ·åšï¼Œæˆ‘ä»¬è·å¾—äº†ä¸€ä¸ªå‡½æ•°ï¼Œå¯ä»¥åœ¨ **å¹¶è¡Œ** è®­ç»ƒ **å¤šä¸ªä»£ç†**ï¼Œä¸å•ä¸ªä»£ç†å‡½æ•°ç›¸æ¯”ï¼Œåªéœ€æœ€å°çš„è°ƒæ•´ï¼š
- en: 'We get similar performances with this version of our training function:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨è¿™ä¸ªç‰ˆæœ¬çš„è®­ç»ƒå‡½æ•°ï¼Œæˆ‘ä»¬å¾—åˆ°äº†ç±»ä¼¼çš„æ€§èƒ½ï¼š
- en: '[PRE4]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: And thatâ€™s it! Thanks for reading this far, I hope this article provided a helpful
    introduction to implementing vectorized environments in **JAX**.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: å°±è¿™äº›äº†ï¼æ„Ÿè°¢ä½ è¯»åˆ°è¿™é‡Œï¼Œå¸Œæœ›è¿™ç¯‡æ–‡ç« ä¸ºä½ æä¾›äº†æœ‰å…³åœ¨ **JAX** ä¸­å®ç°çŸ¢é‡åŒ–ç¯å¢ƒçš„æœ‰ç”¨ä»‹ç»ã€‚
- en: If you enjoyed the read, please consider **sharing** this article and **starring**
    my GitHub repository, thanks for your support! ğŸ™
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä½ å–œæ¬¢è¿™ç¯‡æ–‡ç« ï¼Œè¯·è€ƒè™‘ **åˆ†äº«** è¿™ç¯‡æ–‡ç« å¹¶ **æ”¶è—** æˆ‘çš„GitHubä»“åº“ï¼Œè°¢è°¢ä½ çš„æ”¯æŒï¼ ğŸ™
- en: '[](https://github.com/RPegoud/jym?source=post_page-----49d07373adf5--------------------------------)
    [## GitHub - RPegoud/jym: JAX implementation of RL algorithms and vectorized environments'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://github.com/RPegoud/jym?source=post_page-----49d07373adf5--------------------------------)
    [## GitHub - RPegoud/jym: JAXå®ç°çš„RLç®—æ³•å’ŒçŸ¢é‡åŒ–ç¯å¢ƒ'
- en: 'JAX implementation of RL algorithms and vectorized environments - GitHub -
    RPegoud/jym: JAX implementation of RLâ€¦'
  id: totrans-121
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 'JAXå®ç°çš„RLç®—æ³•å’ŒçŸ¢é‡åŒ–ç¯å¢ƒ - GitHub - RPegoud/jym: JAXå®ç°çš„RL...'
- en: github.com](https://github.com/RPegoud/jym?source=post_page-----49d07373adf5--------------------------------)
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '[github.com](https://github.com/RPegoud/jym?source=post_page-----49d07373adf5--------------------------------)'
- en: 'Finally, for those interested in digging a little deeper, hereâ€™s a list of
    **useful resources** that helped me get started with JAX and redacting this article:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åï¼Œå¯¹äºé‚£äº›å¸Œæœ›æ·±å…¥äº†è§£çš„äººï¼Œè¿™é‡Œæœ‰ä¸€ä¸ª **æœ‰ç”¨çš„èµ„æº** åˆ—è¡¨ï¼Œå¸®åŠ©æˆ‘å…¥é—¨JAXå¹¶æ’°å†™è¿™ç¯‡æ–‡ç« ï¼š
- en: 'A curated list of awesome JAX articles and resources:'
  id: totrans-124
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç²¾å¿ƒç­–åˆ’çš„JAXæ–‡ç« å’Œèµ„æºæ±‡æ€»ï¼š
- en: '[1] Coderized, (functional programming) [*The purest coding style, where bugs
    are near impossible*](https://www.youtube.com/watch?v=HlgG395PQWw&t=254s), YouTube'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] Coderized, (å‡½æ•°å¼ç¼–ç¨‹) [*æœ€çº¯ç²¹çš„ç¼–ç é£æ ¼ï¼Œå‡ ä¹ä¸å¯èƒ½å‡ºé”™*](https://www.youtube.com/watch?v=HlgG395PQWw&t=254s),
    YouTube'
- en: '[2] Aleksa GordiÄ‡, [*JAX From Zero to Hero YouTube Playlist*](https://www.youtube.com/watch?v=SstuvS-tVc0&list=PLBoQnSflObckOARbMK9Lt98Id0AKcZurq)
    *(2022), The AI Epiphany*'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] Aleksa GordiÄ‡, [*ä»é›¶åˆ°è‹±é›„çš„JAX YouTubeæ’­æ”¾åˆ—è¡¨*](https://www.youtube.com/watch?v=SstuvS-tVc0&list=PLBoQnSflObckOARbMK9Lt98Id0AKcZurq)
    *(2022), The AI Epiphany*'
- en: '[3] Nikolaj Goodger, [*Writing an RL Environment in JAX*](https://medium.com/@ngoodger_7766/writing-an-rl-environment-in-jax-9f74338898ba)
    *(2021)*'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] Nikolaj Goodger, [*ç”¨JAXç¼–å†™RLç¯å¢ƒ*](https://medium.com/@ngoodger_7766/writing-an-rl-environment-in-jax-9f74338898ba)
    *(2021)*'
- en: '[4] Chris Lu*,* [*Achieving 4000x Speedups and Meta-Evolving Discoveries with
    PureJaxRL*](https://chrislu.page/blog/meta-disco/) *(2023),* [University of Oxford](https://www.ox.ac.uk/),
    [Foerster Lab for AI Research](https://www.foersterlab.com/)'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] Chris Lu*,* [*é€šè¿‡PureJaxRLå®ç°4000å€åŠ é€Ÿå’Œå…ƒè¿›åŒ–å‘ç°*](https://chrislu.page/blog/meta-disco/)
    *(2023),* [ç‰›æ´¥å¤§å­¦](https://www.ox.ac.uk/), [Foersteräººå·¥æ™ºèƒ½ç ”ç©¶å®éªŒå®¤](https://www.foersterlab.com/)'
- en: '[5] Nicholas Vadivelu,[*Awesome-JAX*](https://github.com/n2cholas/awesome-jax)
    *(2020)*, a list of JAX libraries, projects, and resources'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] Nicholas Vadiveluï¼Œ[*Awesome-JAX*](https://github.com/n2cholas/awesome-jax)
    *(2020)*ï¼Œä¸€ä¸ª JAX åº“ã€é¡¹ç›®å’Œèµ„æºçš„åˆ—è¡¨'
- en: '[6] JAX Official Documentation, [*Training a Simple Neural Network, with PyTorch
    Data Loading*](https://jax.readthedocs.io/en/latest/notebooks/Neural_Network_and_Data_Loading.html)'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '[6] JAX å®˜æ–¹æ–‡æ¡£ï¼Œ[*ä½¿ç”¨ PyTorch æ•°æ®åŠ è½½è®­ç»ƒç®€å•ç¥ç»ç½‘ç»œ*](https://jax.readthedocs.io/en/latest/notebooks/Neural_Network_and_Data_Loading.html)'
