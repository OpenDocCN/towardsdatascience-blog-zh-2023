- en: Zero-Shot vs. Similarity-Based Text Classification
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: é›¶-shot ä¸ç›¸ä¼¼åº¦åŸºäºçš„æ–‡æœ¬åˆ†ç±»
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/zero-shot-vs-similarity-based-text-classification-83115d9879f5?source=collection_archive---------3-----------------------#2023-01-05](https://towardsdatascience.com/zero-shot-vs-similarity-based-text-classification-83115d9879f5?source=collection_archive---------3-----------------------#2023-01-05)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://towardsdatascience.com/zero-shot-vs-similarity-based-text-classification-83115d9879f5?source=collection_archive---------3-----------------------#2023-01-05](https://towardsdatascience.com/zero-shot-vs-similarity-based-text-classification-83115d9879f5?source=collection_archive---------3-----------------------#2023-01-05)
- en: An evaluation of unsupervised text classification approaches
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å¯¹æ— ç›‘ç£æ–‡æœ¬åˆ†ç±»æ–¹æ³•çš„è¯„ä¼°
- en: '[](https://medium.com/@tim.schopf?source=post_page-----83115d9879f5--------------------------------)[![Tim
    Schopf](../Images/7d98a87af243ae6a82f837aa04ac2675.png)](https://medium.com/@tim.schopf?source=post_page-----83115d9879f5--------------------------------)[](https://towardsdatascience.com/?source=post_page-----83115d9879f5--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----83115d9879f5--------------------------------)
    [Tim Schopf](https://medium.com/@tim.schopf?source=post_page-----83115d9879f5--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@tim.schopf?source=post_page-----83115d9879f5--------------------------------)[![Tim
    Schopf](../Images/7d98a87af243ae6a82f837aa04ac2675.png)](https://medium.com/@tim.schopf?source=post_page-----83115d9879f5--------------------------------)[](https://towardsdatascience.com/?source=post_page-----83115d9879f5--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----83115d9879f5--------------------------------)
    [Tim Schopf](https://medium.com/@tim.schopf?source=post_page-----83115d9879f5--------------------------------)'
- en: Â·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Â·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F7fe3665aa3e3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fzero-shot-vs-similarity-based-text-classification-83115d9879f5&user=Tim+Schopf&userId=7fe3665aa3e3&source=post_page-7fe3665aa3e3----83115d9879f5---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----83115d9879f5--------------------------------)
    Â·9 min readÂ·Jan 5, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F83115d9879f5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fzero-shot-vs-similarity-based-text-classification-83115d9879f5&user=Tim+Schopf&userId=7fe3665aa3e3&source=-----83115d9879f5---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[å…³æ³¨](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F7fe3665aa3e3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fzero-shot-vs-similarity-based-text-classification-83115d9879f5&user=Tim+Schopf&userId=7fe3665aa3e3&source=post_page-7fe3665aa3e3----83115d9879f5---------------------post_header-----------)
    å‘è¡¨åœ¨ [Towards Data Science](https://towardsdatascience.com/?source=post_page-----83115d9879f5--------------------------------)
    Â·9åˆ†é’Ÿé˜…è¯»Â·2023å¹´1æœˆ5æ—¥'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F83115d9879f5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fzero-shot-vs-similarity-based-text-classification-83115d9879f5&source=-----83115d9879f5---------------------bookmark_footer-----------)![](../Images/071fc6c7022736231951f65546ea8c1e.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F83115d9879f5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fzero-shot-vs-similarity-based-text-classification-83115d9879f5&source=-----83115d9879f5---------------------bookmark_footer-----------)![](../Images/071fc6c7022736231951f65546ea8c1e.png)'
- en: Image by [GertrÅ«da ValaseviÄiÅ«tÄ—](https://unsplash.com/@skraidantisdrambliukas?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    on [Unsplash](https://unsplash.com/de/fotos/xMObPS6V_gY?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ç‰‡æ¥æºäº [GertrÅ«da ValaseviÄiÅ«tÄ—](https://unsplash.com/@skraidantisdrambliukas?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    åœ¨ [Unsplash](https://unsplash.com/de/fotos/xMObPS6V_gY?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
- en: '*This post is based on our NLPIR 2022 paper* [*â€œEvaluating Unsupervised Text
    Classification: Zero-shot and Similarity-based Approachesâ€*](https://arxiv.org/abs/2211.16285)*.
    You can read more details there.*'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '*è¿™ç¯‡æ–‡ç« åŸºäºæˆ‘ä»¬çš„ NLPIR 2022 è®ºæ–‡* [*â€œè¯„ä¼°æ— ç›‘ç£æ–‡æœ¬åˆ†ç±»ï¼šé›¶-shot ä¸ç›¸ä¼¼åº¦åŸºäºçš„æ–¹æ³•â€*](https://arxiv.org/abs/2211.16285)*ã€‚ä½ å¯ä»¥åœ¨è¿™é‡Œé˜…è¯»æ›´å¤šç»†èŠ‚ã€‚*'
- en: What is unsupervised text classification?
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ä»€ä¹ˆæ˜¯æ— ç›‘ç£æ–‡æœ¬åˆ†ç±»ï¼Ÿ
- en: Unsupervised text classification approaches aim to perform categorization without
    using annotated data during training and therefore offer the potential to reduce
    annotation costsğŸ’°. Generally, unsupervised text classification approaches aim
    to map text to labels based on their textual description, without using annotated
    training data. To accomplish this, there exist mainly two categories of approaches.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: æ— ç›‘ç£æ–‡æœ¬åˆ†ç±»æ–¹æ³•æ—¨åœ¨åœ¨è®­ç»ƒæœŸé—´ä¸ä½¿ç”¨æ ‡æ³¨æ•°æ®çš„æƒ…å†µä¸‹è¿›è¡Œåˆ†ç±»ï¼Œå› æ­¤æœ‰æ½œåŠ›å‡å°‘æ ‡æ³¨æˆæœ¬ğŸ’°ã€‚ä¸€èˆ¬æ¥è¯´ï¼Œæ— ç›‘ç£æ–‡æœ¬åˆ†ç±»æ–¹æ³•æ—¨åœ¨æ ¹æ®æ–‡æœ¬æè¿°å°†æ–‡æœ¬æ˜ å°„åˆ°æ ‡ç­¾ï¼Œè€Œä¸ä½¿ç”¨æ ‡æ³¨è®­ç»ƒæ•°æ®ã€‚ä¸ºæ­¤ï¼Œä¸»è¦å­˜åœ¨ä¸¤ç±»æ–¹æ³•ã€‚
- en: ğŸ‘¯ The first category can be summarized under **similarity-based** **approaches**.
    Thereby, the approaches generate semantic embeddings of both the texts and the
    label descriptions, before attempting to match the texts to the labels using similarity
    measures such as cosine similarity.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ‘¯ ç¬¬ä¸€ç±»æ–¹æ³•å¯ä»¥æ€»ç»“ä¸º**åŸºäºç›¸ä¼¼æ€§**çš„**æ–¹æ³•**ã€‚åœ¨æ­¤è¿‡ç¨‹ä¸­ï¼Œè¿™äº›æ–¹æ³•é¦–å…ˆç”Ÿæˆæ–‡æœ¬å’Œæ ‡ç­¾æè¿°çš„è¯­ä¹‰åµŒå…¥ï¼Œç„¶åå°è¯•ä½¿ç”¨è¯¸å¦‚ä½™å¼¦ç›¸ä¼¼åº¦ä¹‹ç±»çš„ç›¸ä¼¼æ€§åº¦é‡å°†æ–‡æœ¬ä¸æ ‡ç­¾è¿›è¡ŒåŒ¹é…ã€‚
- en: 0ï¸âƒ£ ğŸ”« The second category uses **zero-shot learning** to classify texts of unseen
    classes. Zero-shot learning uses labeled training instances belonging to seen
    classes to learn a classifier that can predict testing instances belonging to
    different, unseen classes. For example, a zero-shot classification model may learn
    to correctly classify texts about soccer âš½ during training, and then use this
    knowledge during testing to classify texts about basketball ğŸ€ without ever having
    seen texts about basketball before. The idea is that the model can transfer the
    knowledge learned about soccer to the very similar class of basketball. Although
    zero-shot learning techniques employ annotated data for training, they do not
    use labels to provide information about the target classes and can use their knowledge
    of the previously seen classes to classify instances of unseen classes. Since
    pretrained zero-shot text classification models do not require fine-tuning on
    labeled data from the target classes, we categorize them as an unsupervised text
    classification strategy.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 0ï¸âƒ£ ğŸ”« ç¬¬äºŒç±»æ–¹æ³•ä½¿ç”¨**é›¶æ ·æœ¬å­¦ä¹ **æ¥åˆ†ç±»æœªè§è¿‡çš„æ–‡æœ¬ã€‚é›¶æ ·æœ¬å­¦ä¹ åˆ©ç”¨å±äºå·²è§ç±»åˆ«çš„æ ‡è®°è®­ç»ƒå®ä¾‹æ¥å­¦ä¹ ä¸€ä¸ªåˆ†ç±»å™¨ï¼Œè¯¥åˆ†ç±»å™¨å¯ä»¥é¢„æµ‹å±äºä¸åŒã€æœªè§è¿‡ç±»åˆ«çš„æµ‹è¯•å®ä¾‹ã€‚ä¾‹å¦‚ï¼Œä¸€ä¸ªé›¶æ ·æœ¬åˆ†ç±»æ¨¡å‹å¯èƒ½åœ¨è®­ç»ƒæœŸé—´å­¦ä¼šæ­£ç¡®åˆ†ç±»å…³äºè¶³çƒâš½çš„æ–‡æœ¬ï¼Œç„¶ååœ¨æµ‹è¯•æ—¶åˆ©ç”¨è¿™ä¸€çŸ¥è¯†æ¥åˆ†ç±»å…³äºç¯®çƒğŸ€çš„æ–‡æœ¬ï¼Œå³ä½¿ä¹‹å‰ä»æœªè§è¿‡å…³äºç¯®çƒçš„æ–‡æœ¬ã€‚å…¶ç†å¿µæ˜¯ï¼Œæ¨¡å‹å¯ä»¥å°†å…³äºè¶³çƒçš„çŸ¥è¯†è½¬ç§»åˆ°éå¸¸ç›¸ä¼¼çš„ç¯®çƒç±»åˆ«ä¸Šã€‚è™½ç„¶é›¶æ ·æœ¬å­¦ä¹ æŠ€æœ¯åœ¨è®­ç»ƒæ—¶ä½¿ç”¨äº†æ ‡æ³¨æ•°æ®ï¼Œä½†å®ƒä»¬ä¸ä½¿ç”¨æ ‡ç­¾æ¥æä¾›ç›®æ ‡ç±»åˆ«çš„ä¿¡æ¯ï¼Œè€Œæ˜¯åˆ©ç”¨å·²è§ç±»åˆ«çš„çŸ¥è¯†æ¥åˆ†ç±»æœªè§ç±»åˆ«çš„å®ä¾‹ã€‚ç”±äºé¢„è®­ç»ƒçš„é›¶æ ·æœ¬æ–‡æœ¬åˆ†ç±»æ¨¡å‹ä¸éœ€è¦åœ¨ç›®æ ‡ç±»åˆ«çš„æ ‡è®°æ•°æ®ä¸Šè¿›è¡Œå¾®è°ƒï¼Œå› æ­¤æˆ‘ä»¬å°†å…¶å½’ç±»ä¸ºæ— ç›‘ç£æ–‡æœ¬åˆ†ç±»ç­–ç•¥ã€‚
- en: '**In this blog post, we summarize the contributions of our paper ğŸ“„** [***â€œEvaluating
    Unsupervised Text Classification: Zero-shot and Similarity-based Approaches (2022)â€***](https://arxiv.org/abs/2211.16285)**as
    follows:**'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '**åœ¨è¿™ç¯‡åšå®¢æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬æ€»ç»“äº†æˆ‘ä»¬è®ºæ–‡ğŸ“„** [***â€œè¯„ä¼°æ— ç›‘ç£æ–‡æœ¬åˆ†ç±»ï¼šé›¶æ ·æœ¬å’ŒåŸºäºç›¸ä¼¼æ€§çš„ approaches (2022)â€***](https://arxiv.org/abs/2211.16285)**çš„è´¡çŒ®å¦‚ä¸‹ï¼š**'
- en: We **evaluate** the **similarity-based** and **zero-shot learning** categories
    for unsupervised text classification of topics. Thereby, we conduct experiments
    with representative approaches of each category on different benchmark datasets.
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬**è¯„ä¼°**äº†æ— ç›‘ç£æ–‡æœ¬åˆ†ç±»ä¸­**åŸºäºç›¸ä¼¼æ€§**å’Œ**é›¶æ ·æœ¬å­¦ä¹ **ç±»åˆ«ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬åœ¨ä¸åŒçš„åŸºå‡†æ•°æ®é›†ä¸Šå¯¹æ¯ä¸ªç±»åˆ«çš„ä»£è¡¨æ€§æ–¹æ³•è¿›è¡Œå®éªŒã€‚
- en: We **propose simple but strong baselines for unsupervised text classification**
    based on [SimCSE](https://aclanthology.org/2021.emnlp-main.552/) and [SBERT](https://aclanthology.org/D19-1410/)
    sentence embedding similarities. Previous work has mostly been evaluated against
    different weak baselines such as [Word2Vec](/introduction-to-word-embedding-and-word2vec-652d0c2060fa)
    similarities which are easy to outperform and tend to overestimate the performance
    of new unsupervised text classification approaches.
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬**æå‡ºäº†åŸºäº [SimCSE](https://aclanthology.org/2021.emnlp-main.552/) å’Œ [SBERT](https://aclanthology.org/D19-1410/)
    å¥å­åµŒå…¥ç›¸ä¼¼æ€§çš„ç®€å•ä½†å¼ºå¤§çš„æ— ç›‘ç£æ–‡æœ¬åˆ†ç±»åŸºçº¿**ã€‚ä¹‹å‰çš„å·¥ä½œå¤§å¤šä¸ä¸åŒçš„å¼±åŸºçº¿å¦‚ [Word2Vec](/introduction-to-word-embedding-and-word2vec-652d0c2060fa)
    ç›¸æ¯”ï¼Œè¿™äº›å¼±åŸºçº¿å®¹æ˜“è¢«è¶…è¶Šï¼Œå¹¶ä¸”å¾€å¾€é«˜ä¼°äº†æ–°çš„æ— ç›‘ç£æ–‡æœ¬åˆ†ç±»æ–¹æ³•çš„æ€§èƒ½ã€‚
- en: Since transformer-based text representations have been widely established as
    state-of-the-art for semantic text similarity in recent years, **we further adapt**
    [**Lbl2Vec**](/unsupervised-text-classification-with-lbl2vec-6c5e040354de), one
    of the most recent and well-performing similarity-based methods for unsupervised
    text classification, to be used with transformer-based language models.
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç”±äºåŸºäºå˜æ¢å™¨çš„æ–‡æœ¬è¡¨ç¤ºè¿‘å¹´æ¥è¢«å¹¿æ³›å»ºç«‹ä¸ºè¯­ä¹‰æ–‡æœ¬ç›¸ä¼¼åº¦çš„æœ€å…ˆè¿›æŠ€æœ¯ï¼Œ**æˆ‘ä»¬è¿›ä¸€æ­¥è°ƒæ•´** [**Lbl2Vec**](/unsupervised-text-classification-with-lbl2vec-6c5e040354de)ï¼Œè¿™æ˜¯æœ€æ–°ä¸”è¡¨ç°ä¼˜è‰¯çš„æ— ç›‘ç£æ–‡æœ¬åˆ†ç±»ç›¸ä¼¼åº¦æ–¹æ³•ä¹‹ä¸€ï¼Œä»¥ä¸åŸºäºå˜æ¢å™¨çš„è¯­è¨€æ¨¡å‹é…åˆä½¿ç”¨ã€‚
- en: Unsupervised text classification approaches
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ— ç›‘ç£æ–‡æœ¬åˆ†ç±»æ–¹æ³•
- en: ğŸ‘¯ Similarity-based text classification with Lbl2Vec
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ğŸ‘¯ ä½¿ç”¨Lbl2Vecçš„åŸºäºç›¸ä¼¼åº¦çš„æ–‡æœ¬åˆ†ç±»
- en: Numerous similarity-based approaches for unsupervised text classification exist.
    However, the recently introduced [**Lbl2Vec**](https://medium.com/towards-data-science/unsupervised-text-classification-with-lbl2vec-6c5e040354de)
    **approach yields improved performance compared with other similarity-based approaches**.
    Therefore, we focused on this approach in this study. [Lbl2Vec](https://medium.com/towards-data-science/unsupervised-text-classification-with-lbl2vec-6c5e040354de)
    works by jointly embedding word, document, and label representations. First, word
    and documented representations are learned with [Doc2Vec](https://radimrehurek.com/gensim/models/doc2vec.html).
    Then, the average of label keyword representations for each class is used to find
    a set of most similar candidate document representations via cosine similarity.
    The average of candidate document representations, in turn, generates the label
    vector for each class. For classification, eventually, the documents are assigned
    to the class where the cosine similarity of the label vector and the document
    vector is the highest. [Here](https://medium.com/towards-data-science/unsupervised-text-classification-with-lbl2vec-6c5e040354de),
    you can find more information about how [Lbl2Vec](https://medium.com/towards-data-science/unsupervised-text-classification-with-lbl2vec-6c5e040354de)
    works.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: å­˜åœ¨è®¸å¤šåŸºäºç›¸ä¼¼åº¦çš„æ— ç›‘ç£æ–‡æœ¬åˆ†ç±»æ–¹æ³•ã€‚ç„¶è€Œï¼Œæœ€è¿‘å¼•å…¥çš„[**Lbl2Vec**](https://medium.com/towards-data-science/unsupervised-text-classification-with-lbl2vec-6c5e040354de)
    **æ–¹æ³•ç›¸æ¯”å…¶ä»–åŸºäºç›¸ä¼¼åº¦çš„æ–¹æ³•è¡¨ç°æ›´ä½³**ã€‚å› æ­¤ï¼Œæˆ‘ä»¬åœ¨æœ¬ç ”ç©¶ä¸­é‡ç‚¹å…³æ³¨äº†è¿™ä¸€æ–¹æ³•ã€‚[Lbl2Vec](https://medium.com/towards-data-science/unsupervised-text-classification-with-lbl2vec-6c5e040354de)é€šè¿‡è”åˆåµŒå…¥è¯ã€æ–‡æ¡£å’Œæ ‡ç­¾è¡¨ç¤ºæ¥å·¥ä½œã€‚é¦–å…ˆï¼Œä½¿ç”¨[Doc2Vec](https://radimrehurek.com/gensim/models/doc2vec.html)å­¦ä¹ è¯å’Œæ–‡æ¡£çš„è¡¨ç¤ºã€‚ç„¶åï¼Œé€šè¿‡ä½™å¼¦ç›¸ä¼¼åº¦åˆ©ç”¨æ¯ä¸ªç±»åˆ«çš„æ ‡ç­¾å…³é”®è¯è¡¨ç¤ºçš„å¹³å‡å€¼æ¥æ‰¾åˆ°ä¸€ç»„æœ€ç›¸ä¼¼çš„å€™é€‰æ–‡æ¡£è¡¨ç¤ºã€‚å€™é€‰æ–‡æ¡£è¡¨ç¤ºçš„å¹³å‡å€¼ï¼Œåè¿‡æ¥ï¼Œä¸ºæ¯ä¸ªç±»åˆ«ç”Ÿæˆæ ‡ç­¾å‘é‡ã€‚æœ€ç»ˆï¼Œå¯¹äºåˆ†ç±»ï¼Œæ–‡æ¡£è¢«åˆ†é…åˆ°æ ‡ç­¾å‘é‡å’Œæ–‡æ¡£å‘é‡ä½™å¼¦ç›¸ä¼¼åº¦æœ€é«˜çš„ç±»åˆ«ã€‚[åœ¨è¿™é‡Œ](https://medium.com/towards-data-science/unsupervised-text-classification-with-lbl2vec-6c5e040354de)ï¼Œä½ å¯ä»¥æ‰¾åˆ°æ›´å¤šå…³äº[Lbl2Vec](https://medium.com/towards-data-science/unsupervised-text-classification-with-lbl2vec-6c5e040354de)å¦‚ä½•å·¥ä½œçš„èµ„æ–™ã€‚
- en: Furthermore, **we adapt the** [**Lbl2Vec**](https://medium.com/towards-data-science/unsupervised-text-classification-with-lbl2vec-6c5e040354de)
    **approach, using transformer-based text representations** instead of [Doc2Vec](https://radimrehurek.com/gensim/models/doc2vec.html)
    to create jointly embedded word, document, and label representations. Since transformer-based
    text representations currently achieve state-of-the-art results in text- similarity
    tasks, we investigate the effect of the different resulting text representations
    on this similarity-based text classification strategy. In this paper, we use [SimCSE](https://aclanthology.org/2021.emnlp-main.552/)
    and [SBERT](https://aclanthology.org/D19-1410/) transformer- models to create
    text representations. In the following, this approach is referred to as **Lbl2TransformerVec**.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤å¤–ï¼Œ**æˆ‘ä»¬è°ƒæ•´äº†** [**Lbl2Vec**](https://medium.com/towards-data-science/unsupervised-text-classification-with-lbl2vec-6c5e040354de)
    **æ–¹æ³•ï¼Œä½¿ç”¨åŸºäºå˜æ¢å™¨çš„æ–‡æœ¬è¡¨ç¤º**ï¼Œè€Œä¸æ˜¯[Doc2Vec](https://radimrehurek.com/gensim/models/doc2vec.html)ï¼Œæ¥åˆ›å»ºè”åˆåµŒå…¥çš„è¯ã€æ–‡æ¡£å’Œæ ‡ç­¾è¡¨ç¤ºã€‚ç”±äºåŸºäºå˜æ¢å™¨çš„æ–‡æœ¬è¡¨ç¤ºç›®å‰åœ¨æ–‡æœ¬ç›¸ä¼¼åº¦ä»»åŠ¡ä¸­å®ç°äº†æœ€å…ˆè¿›çš„ç»“æœï¼Œæˆ‘ä»¬ç ”ç©¶äº†ä¸åŒçš„æ–‡æœ¬è¡¨ç¤ºå¯¹è¿™ç§åŸºäºç›¸ä¼¼åº¦çš„æ–‡æœ¬åˆ†ç±»ç­–ç•¥çš„å½±å“ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨[SimCSE](https://aclanthology.org/2021.emnlp-main.552/)å’Œ[SBERT](https://aclanthology.org/D19-1410/)å˜æ¢å™¨æ¨¡å‹æ¥åˆ›å»ºæ–‡æœ¬è¡¨ç¤ºã€‚æ¥ä¸‹æ¥ï¼Œè¿™ç§æ–¹æ³•è¢«ç§°ä¸º**Lbl2TransformerVec**ã€‚
- en: 0ï¸âƒ£ ğŸ”« Zero-shot text classification using the entailment approach
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 0ï¸âƒ£ ğŸ”« ä½¿ç”¨è•´å«æ–¹æ³•çš„é›¶æ ·æœ¬æ–‡æœ¬åˆ†ç±»
- en: Although newer zero-shot text classification (0SHOT-TC) approaches exist, **the**
    [**zero-shot entailment approach**](https://aclanthology.org/D19-1404/) **still
    produces state-of-the-art 0SHOT-TC results in predicting instances of unseen classes**
    compared to models of similar size. As the name already implies, the zero-shot
    entailment approach deals with 0SHOT-TC as a textual entailment problem. The underlying
    idea is similar to that of similarity-based text classification approaches. Conventional
    0SHOT-TC classifiers fail to understand the actual problem since the label names
    are usually converted into simple indices. Therefore, these classifiers can hardly
    generalize from seen to unseen classes. **Considering 0SHOT-TC as an entailment
    problem provides the classifier with a textual label description and therefore
    enables it to understand the meaning of labels.**
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: å°½ç®¡å­˜åœ¨æ›´æ–°çš„é›¶æ ·æœ¬æ–‡æœ¬åˆ†ç±»ï¼ˆ0SHOT-TCï¼‰æ–¹æ³•ï¼Œ**[**é›¶æ ·æœ¬è•´å«æ–¹æ³•**](https://aclanthology.org/D19-1404/)**ä»ç„¶åœ¨é¢„æµ‹æœªè§ç±»åˆ«çš„å®ä¾‹æ—¶ï¼Œä¸ç±»ä¼¼å¤§å°çš„æ¨¡å‹ç›¸æ¯”ï¼Œèƒ½å¤Ÿäº§ç”Ÿæœ€å…ˆè¿›çš„0SHOT-TCç»“æœã€‚æ­£å¦‚åç§°æ‰€æš—ç¤ºçš„ï¼Œé›¶æ ·æœ¬è•´å«æ–¹æ³•å°†0SHOT-TCè§†ä¸ºä¸€ä¸ªæ–‡æœ¬è•´å«é—®é¢˜ã€‚å…¶åŸºæœ¬æ€æƒ³ç±»ä¼¼äºåŸºäºç›¸ä¼¼æ€§çš„æ–‡æœ¬åˆ†ç±»æ–¹æ³•ã€‚ä¼ ç»Ÿçš„0SHOT-TCåˆ†ç±»å™¨æ— æ³•ç†è§£å®é™…é—®é¢˜ï¼Œå› ä¸ºæ ‡ç­¾åç§°é€šå¸¸è¢«è½¬æ¢ä¸ºç®€å•çš„ç´¢å¼•ã€‚å› æ­¤ï¼Œè¿™äº›åˆ†ç±»å™¨å¾ˆéš¾ä»å·²è§ç±»åˆ«æ¨å¹¿åˆ°æœªè§ç±»åˆ«ã€‚**å°†0SHOT-TCè§†ä¸ºä¸€ä¸ªè•´å«é—®é¢˜ä¸ºåˆ†ç±»å™¨æä¾›äº†æ–‡æœ¬æ ‡ç­¾æè¿°ï¼Œå› æ­¤ä½¿å…¶èƒ½å¤Ÿç†è§£æ ‡ç­¾çš„å«ä¹‰ã€‚**
- en: 0ï¸âƒ£ ğŸ”« Zero-shot text classification using TARS
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 0ï¸âƒ£ ğŸ”« ä½¿ç”¨TARSè¿›è¡Œé›¶æ ·æœ¬æ–‡æœ¬åˆ†ç±»
- en: '[TARS](https://aclanthology.org/2020.coling-main.285/) also uses the textual
    label description to classify text in a zero-shot setting. However, TARS approaches
    the task as a binary classification problem, where a text and a textual label
    description is given to the model, which makes a prediction about whether that
    label is true or not. The TARS authors state that this approach significantly
    outperforms [GPT-2](https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf)
    in 0SHOT-TC'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '[TARS](https://aclanthology.org/2020.coling-main.285/)ä¹Ÿä½¿ç”¨æ–‡æœ¬æ ‡ç­¾æè¿°æ¥åœ¨é›¶æ ·æœ¬è®¾ç½®ä¸­è¿›è¡Œæ–‡æœ¬åˆ†ç±»ã€‚ç„¶è€Œï¼ŒTARSå°†ä»»åŠ¡è§†ä¸ºä¸€ä¸ªäºŒåˆ†ç±»é—®é¢˜ï¼Œå…¶ä¸­ç»™å®šä¸€ä¸ªæ–‡æœ¬å’Œä¸€ä¸ªæ–‡æœ¬æ ‡ç­¾æè¿°ï¼Œæ¨¡å‹å¯¹è¯¥æ ‡ç­¾æ˜¯å¦çœŸå®è¿›è¡Œé¢„æµ‹ã€‚TARSçš„ä½œè€…è¡¨ç¤ºï¼Œè¿™ç§æ–¹æ³•åœ¨0SHOT-TCä¸­æ˜¾è‘—ä¼˜äº[GPT-2](https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf)'
- en: ğŸ“ Baselines
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ğŸ“ åŸºå‡†
- en: We compare the findings of current state-of-the-art unsupervised text classification
    approaches to some basic baselines to evaluate their performance.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†å½“å‰æœ€å…ˆè¿›çš„æ— ç›‘ç£æ–‡æœ¬åˆ†ç±»æ–¹æ³•çš„å‘ç°ä¸ä¸€äº›åŸºæœ¬åŸºå‡†è¿›è¡Œæ¯”è¾ƒï¼Œä»¥è¯„ä¼°å®ƒä»¬çš„æ€§èƒ½ã€‚
- en: '[**LSA:**](https://asistdl.onlinelibrary.wiley.com/doi/abs/10.1002/%28SICI%291097-4571%28199009%2941%3A6%3C391%3A%3AAID-ASI1%3E3.0.CO%3B2-9)For
    each dataset, we apply LSA to learn ğ‘› = number of classes concepts. Afterwards,
    the text documents are classified according to the highest cosine similarity of
    resulting LSA vectors of documents and label keywords.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '[**LSA:**](https://asistdl.onlinelibrary.wiley.com/doi/abs/10.1002/%28SICI%291097-4571%28199009%2941%3A6%3C391%3A%3AAID-ASI1%3E3.0.CO%3B2-9)å¯¹äºæ¯ä¸ªæ•°æ®é›†ï¼Œæˆ‘ä»¬åº”ç”¨LSAæ¥å­¦ä¹ ğ‘›
    = ç±»åˆ«æ•°æ¦‚å¿µã€‚éšåï¼Œæ ¹æ®æ–‡æ¡£å’Œæ ‡ç­¾å…³é”®è¯çš„LSAå‘é‡çš„æœ€é«˜ä½™å¼¦ç›¸ä¼¼åº¦å¯¹æ–‡æœ¬æ–‡ä»¶è¿›è¡Œåˆ†ç±»ã€‚'
- en: '[**Word2Vec:**](https://papers.nips.cc/paper/2013/hash/9aa42b31882ec039965f3c4923ce901b-Abstract.html)This
    produces semantic vector representations of words based on surrounding context
    words. The average of word embeddings is used to represent the text documents
    and label keywords. The text documents are predicted according to the highest
    cosine similarity of the resulting Word2Vec representations of documents and label
    keywords for classification.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '[**Word2Vec:**](https://papers.nips.cc/paper/2013/hash/9aa42b31882ec039965f3c4923ce901b-Abstract.html)è¿™æ ¹æ®å‘¨å›´çš„ä¸Šä¸‹æ–‡è¯ç”Ÿæˆè¯çš„è¯­ä¹‰å‘é‡è¡¨ç¤ºã€‚è¯åµŒå…¥çš„å¹³å‡å€¼ç”¨äºè¡¨ç¤ºæ–‡æœ¬æ–‡ä»¶å’Œæ ‡ç­¾å…³é”®è¯ã€‚æ–‡æœ¬æ–‡ä»¶æ ¹æ®ç”Ÿæˆçš„Word2Vecæ–‡æ¡£å’Œæ ‡ç­¾å…³é”®è¯çš„è¡¨ç¤ºçš„æœ€é«˜ä½™å¼¦ç›¸ä¼¼åº¦è¿›è¡Œé¢„æµ‹å’Œåˆ†ç±»ã€‚'
- en: '[**SimCSE:**](https://aclanthology.org/2021.emnlp-main.552/)This is a contrastive
    learning framework that produces sentence embeddings which acieve state-of-the-art
    results in semantic similarity tasks. We use, SimCSE document embeddings and SimCSE
    label keyword embeddings as class representations. Finally, the text documents
    are classified according to the highest cosine similarity of the resulting SimCSE
    representations of document and label keywords.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '[**SimCSE:**](https://aclanthology.org/2021.emnlp-main.552/)è¿™æ˜¯ä¸€ä¸ªå¯¹æ¯”å­¦ä¹ æ¡†æ¶ï¼Œå®ƒç”Ÿæˆå¥å­åµŒå…¥ï¼Œåœ¨è¯­ä¹‰ç›¸ä¼¼æ€§ä»»åŠ¡ä¸­å–å¾—äº†æœ€å…ˆè¿›çš„ç»“æœã€‚æˆ‘ä»¬ä½¿ç”¨SimCSEæ–‡æ¡£åµŒå…¥å’ŒSimCSEæ ‡ç­¾å…³é”®è¯åµŒå…¥ä½œä¸ºç±»åˆ«è¡¨ç¤ºã€‚æœ€åï¼Œæ–‡æœ¬æ–‡ä»¶æ ¹æ®ç”Ÿæˆçš„SimCSEæ–‡æ¡£å’Œæ ‡ç­¾å…³é”®è¯çš„è¡¨ç¤ºçš„æœ€é«˜ä½™å¼¦ç›¸ä¼¼åº¦è¿›è¡Œåˆ†ç±»ã€‚'
- en: '[**SBERT:**](https://aclanthology.org/D19-1410/)This is a modification of [BERT](https://aclanthology.org/N19-1423/)
    that uses siamese and triplet network structures to derive semantically meaningful
    sentence embeddings. We use the same classification approach as with SimCSE, except
    that we now use SBERT embeddings instead of SimCSE embeddings.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '[**SBERT:**](https://aclanthology.org/D19-1410/)è¿™æ˜¯ä¸€ç§å¯¹[BERT](https://aclanthology.org/N19-1423/)çš„ä¿®æ”¹ï¼Œä½¿ç”¨äº†siameseå’Œtripletç½‘ç»œç»“æ„æ¥æ¨å¯¼å…·æœ‰è¯­ä¹‰æ„ä¹‰çš„å¥å­åµŒå…¥ã€‚æˆ‘ä»¬ä½¿ç”¨ä¸SimCSEç›¸åŒçš„åˆ†ç±»æ–¹æ³•ï¼Œåªä¸è¿‡è¿™æ¬¡ä½¿ç”¨SBERTåµŒå…¥è€Œä¸æ˜¯SimCSEåµŒå…¥ã€‚'
- en: ğŸ”¬ Experiments
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ğŸ”¬ å®éªŒ
- en: For our SimCSE experiments, we use the [sup-simcse-roberta-large](https://huggingface.co/princeton-nlp/sup-simcse-roberta-large)
    model. To create embeddings with SBERT, we use two different pretrained SBERT
    models. We choose the general purpose models [all-mpnet- base-v2](https://huggingface.co/sentence-transformers/all-mpnet-base-v2)
    and [all-MiniLM-L6-v2](https://huggingface.co/sentence-transformers/all-mpnet-base-v2),
    trained on more than one billion training pairs and expected to perform well on
    sentence similarity tasks. The all-mpnet-base-v2 model is larger than the all-MiniLM-
    L6-v2 model and guarantees slightly better quality sentence embeddings. The smaller
    all-MiniLM-L6-v2 model, on the other hand, guarantees a five times faster encoding
    time while still providing sentence embeddings of high quality.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºæˆ‘ä»¬çš„SimCSEå®éªŒï¼Œæˆ‘ä»¬ä½¿ç”¨[**sup-simcse-roberta-large**](https://huggingface.co/princeton-nlp/sup-simcse-roberta-large)æ¨¡å‹ã€‚ä¸ºäº†ä½¿ç”¨SBERTåˆ›å»ºåµŒå…¥ï¼Œæˆ‘ä»¬ä½¿ç”¨äº†ä¸¤ä¸ªä¸åŒçš„é¢„è®­ç»ƒSBERTæ¨¡å‹ã€‚æˆ‘ä»¬é€‰æ‹©äº†é€šç”¨æ¨¡å‹[all-mpnet-base-v2](https://huggingface.co/sentence-transformers/all-mpnet-base-v2)å’Œ[all-MiniLM-L6-v2](https://huggingface.co/sentence-transformers/all-mpnet-base-v2)ï¼Œè¿™äº›æ¨¡å‹åœ¨è¶…è¿‡åäº¿å¯¹è®­ç»ƒæ ·æœ¬ä¸Šè¿›è¡Œäº†è®­ç»ƒï¼Œé¢„è®¡åœ¨å¥å­ç›¸ä¼¼åº¦ä»»åŠ¡ä¸Šè¡¨ç°è‰¯å¥½ã€‚all-mpnet-base-v2æ¨¡å‹æ¯”all-MiniLM-L6-v2æ¨¡å‹å¤§ï¼Œä¿è¯äº†ç¨å¾®æ›´å¥½çš„è´¨é‡å¥å­åµŒå…¥ã€‚è€Œè¾ƒå°çš„all-MiniLM-L6-v2æ¨¡å‹åˆ™ä¿è¯äº†äº”å€çš„ç¼–ç é€Ÿåº¦ï¼ŒåŒæ—¶ä»èƒ½æä¾›é«˜è´¨é‡çš„å¥å­åµŒå…¥ã€‚
- en: 'For evaluation of 0SHOT-TC, we conduct experiments with three different pretrained
    zero-shot entailment models: a [DeBERTa](https://huggingface.co/MoritzLaurer/DeBERTa-v3-base-mnli-fever-docnli-ling-2c)
    model, a large [BART](https://huggingface.co/facebook/bart-large-mnli) model,
    and a smaller [DistilBERT](https://huggingface.co/typeform/distilbert-base-uncased-mnli)
    model. For TARS experiments, we use the BERT-based pretrained [tars-base-v8](https://flair.informatik.hu-berlin.de/resources/models/tars-base/)
    model.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†è¯„ä¼°0SHOT-TCï¼Œæˆ‘ä»¬è¿›è¡Œäº†ä¸‰ç§ä¸åŒé¢„è®­ç»ƒçš„é›¶æ ·æœ¬è•´å«æ¨¡å‹çš„å®éªŒï¼šä¸€ä¸ª[DeBERTa](https://huggingface.co/MoritzLaurer/DeBERTa-v3-base-mnli-fever-docnli-ling-2c)æ¨¡å‹ï¼Œä¸€ä¸ªå¤§å‹[BART](https://huggingface.co/facebook/bart-large-mnli)æ¨¡å‹ï¼Œä»¥åŠä¸€ä¸ªè¾ƒå°çš„[DistilBERT](https://huggingface.co/typeform/distilbert-base-uncased-mnli)æ¨¡å‹ã€‚å¯¹äºTARSå®éªŒï¼Œæˆ‘ä»¬ä½¿ç”¨åŸºäºBERTçš„é¢„è®­ç»ƒ[tars-base-v8](https://flair.informatik.hu-berlin.de/resources/models/tars-base/)æ¨¡å‹ã€‚
- en: ğŸ’¾ Data
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ğŸ’¾ æ•°æ®
- en: Our evaluation is based on the four publicly available text classification datasets,
    [*20Newsgroups*](http://qwone.com/~jason/20Newsgroups), [*AGâ€˜s Corpus*](http://groups.di.unipi.it/~gulli/AG_corpus_of_news_articles)
    , [*Yahoo! Answers*](https://huggingface.co/datasets/yahoo_answers_topics), and
    [*Medical Abstracts*](https://github.com/sebischair/Medical-Abstracts-TC-Corpus)
    from different domains. As we use the semantic meaning of class descriptions for
    unsupervised text classification, we infer label keywords from each class name
    that serves the purpose of textual class descriptions. Thereby, the inference
    step simply consists of using the class names provided by the official documentation
    of the datasets as label keywords. In a few cases, we additionally substituted
    the class names with synonymous or semantically similar keywords, if we considered
    this to be a more appropriate description of a certain class.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬çš„è¯„ä¼°åŸºäºå››ä¸ªå…¬å¼€å¯ç”¨çš„æ–‡æœ¬åˆ†ç±»æ•°æ®é›†ï¼Œ[*20Newsgroups*](http://qwone.com/~jason/20Newsgroups)ï¼Œ[*AGâ€˜s
    Corpus*](http://groups.di.unipi.it/~gulli/AG_corpus_of_news_articles)ï¼Œ[*Yahoo!
    Answers*](https://huggingface.co/datasets/yahoo_answers_topics)ï¼Œä»¥åŠ[*Medical Abstracts*](https://github.com/sebischair/Medical-Abstracts-TC-Corpus)ï¼Œè¿™äº›æ•°æ®é›†æ¥è‡ªä¸åŒé¢†åŸŸã€‚ç”±äºæˆ‘ä»¬ä½¿ç”¨ç±»åˆ«æè¿°çš„è¯­ä¹‰æ„ä¹‰è¿›è¡Œæ— ç›‘ç£æ–‡æœ¬åˆ†ç±»ï¼Œæˆ‘ä»¬ä»æ¯ä¸ªç±»åˆ«åç§°æ¨æ–­æ ‡ç­¾å…³é”®è¯ï¼Œè¿™äº›å…³é”®è¯ä½œä¸ºæ–‡æœ¬ç±»åˆ«æè¿°çš„ç›®çš„ã€‚å› æ­¤ï¼Œæ¨æ–­æ­¥éª¤ä»…ä»…æ˜¯ä½¿ç”¨æ•°æ®é›†å®˜æ–¹æ–‡æ¡£æä¾›çš„ç±»åˆ«åç§°ä½œä¸ºæ ‡ç­¾å…³é”®è¯ã€‚åœ¨å°‘æ•°æƒ…å†µä¸‹ï¼Œå¦‚æœæˆ‘ä»¬è®¤ä¸ºè¿™æ˜¯å¯¹æŸä¸ªç±»åˆ«æ›´åˆé€‚çš„æè¿°ï¼Œæˆ‘ä»¬è¿˜ç”¨åŒä¹‰è¯æˆ–è¯­ä¹‰ç›¸ä¼¼çš„å…³é”®è¯æ›¿æ¢äº†ç±»åˆ«åç§°ã€‚
- en: '![](../Images/ae318fb6f9a50f1d5430f36fef35eda6.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ae318fb6f9a50f1d5430f36fef35eda6.png)'
- en: Overview of the used text classification datasets.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨çš„æ–‡æœ¬åˆ†ç±»æ•°æ®é›†æ¦‚è¿°ã€‚
- en: ğŸ“Š Evaluation Results
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ğŸ“Š è¯„ä¼°ç»“æœ
- en: '![](../Images/f94c4acf04ac7ef43f10e5be515fca65.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f94c4acf04ac7ef43f10e5be515fca65.png)'
- en: F1-scores (micro) of examined text classification approaches on different datasets.
    The best results on the respective dataset are displayed in bold. Since we use
    micro-averaging to calculate our classification metrics, we realize equal F1,
    Precision, and Recall scores respectively.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä¸åŒæ•°æ®é›†ä¸Šæ£€æŸ¥çš„æ–‡æœ¬åˆ†ç±»æ–¹æ³•çš„F1åˆ†æ•°ï¼ˆmicroï¼‰ã€‚å„æ•°æ®é›†ä¸Šæœ€ä½³ç»“æœä»¥**ç²—ä½“**æ˜¾ç¤ºã€‚ç”±äºæˆ‘ä»¬ä½¿ç”¨å¾®å¹³å‡æ³•æ¥è®¡ç®—åˆ†ç±»æŒ‡æ ‡ï¼Œæˆ‘ä»¬å¯ä»¥å®ç°F1ã€Precisionå’ŒRecallçš„ç›¸ç­‰åˆ†æ•°ã€‚
- en: We can observe that none of the baselines achieves the highest F1-scores on
    any dataset based on these data. This indicates that the use of advanced unsupervised
    text classification approaches usually yields better results than simple baseline
    approaches. However, we observe that the **LSA and Word2Vec approaches** generally
    **yield the worst results and** **are easy to outperform** ğŸ‘. In contrast, the
    **SimCSE and SBERT baselines produce strong F1-scores** ğŸ’ª that even some of the
    advanced approaches could not surpass in certain cases. Furthermore, the SimCSE
    and SBERT baseline approaches may produce better results than the Lbl2Vec similarity-based
    approach on three datasets. We nevertheless can deduce that **the use of advanced
    similarity-based approaches generally produces better unsupervised text classification
    results than the use of simple baseline approaches or 0SHOT-TC**. Specifically,
    the **Lbl2TransformerVec approaches using SBERT embeddings appear to be promising,
    as they consistently perform well across all datasets** ğŸ† and outperform the baseline
    results. In contrast, the **0SHOT-TC approaches perform consistently weak** and
    in the majority of cases did not even manage to outperform the baseline results
    ğŸ˜². However, the DeBERTa zero-shot entailment model could classify the domain-specific
    medical abstracts surprisingly well and achieved the best F1-scores of all classifiers
    on this dataset. We observe, that the large DeBERTa zero-shot entailment model
    always significantly outperforms the smaller BART-large and DistilBERT zero-shot
    entailment models. Additionally, the BERT-based TARS model performs slightly better
    than the smaller DistilBERT zero-shot entailment model, except in case of the
    domain-specific Medical Abstracts dataset. Thus, we conclude that **the performance
    of the 0SHOT-TC approaches improves with increasing model size** ğŸ¤”.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥è§‚å¯Ÿåˆ°ï¼Œæ ¹æ®è¿™äº›æ•°æ®ï¼Œæ²¡æœ‰ä»»ä½•åŸºçº¿æ–¹æ³•åœ¨ä»»ä½•æ•°æ®é›†ä¸Šè·å¾—æœ€é«˜çš„F1åˆ†æ•°ã€‚è¿™è¡¨æ˜ï¼Œä½¿ç”¨å…ˆè¿›çš„æ— ç›‘ç£æ–‡æœ¬åˆ†ç±»æ–¹æ³•é€šå¸¸èƒ½è·å¾—æ¯”ç®€å•åŸºçº¿æ–¹æ³•æ›´å¥½çš„ç»“æœã€‚ç„¶è€Œï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°**LSAå’ŒWord2Vecæ–¹æ³•**é€šå¸¸**æ•ˆæœæœ€å·®ï¼Œå¹¶ä¸”**
    **å®¹æ˜“è¢«è¶…è¶Š** ğŸ‘ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œ**SimCSEå’ŒSBERTåŸºçº¿æ–¹æ³•äº§ç”Ÿäº†å¼ºåŠ²çš„F1åˆ†æ•°** ğŸ’ªï¼Œç”šè‡³åœ¨æŸäº›æƒ…å†µä¸‹è¿ä¸€äº›å…ˆè¿›æ–¹æ³•ä¹Ÿæœªèƒ½è¶…è¶Šã€‚æ­¤å¤–ï¼ŒSimCSEå’ŒSBERTåŸºçº¿æ–¹æ³•åœ¨ä¸‰ä¸ªæ•°æ®é›†ä¸Šçš„è¡¨ç°å¯èƒ½ä¼˜äºLbl2VecåŸºäºç›¸ä¼¼æ€§çš„æ–¹æ³•ã€‚æˆ‘ä»¬å¯ä»¥æ¨æ–­å‡ºï¼Œ**ä½¿ç”¨å…ˆè¿›çš„åŸºäºç›¸ä¼¼æ€§çš„æ–¹æ³•é€šå¸¸èƒ½æ¯”ä½¿ç”¨ç®€å•åŸºçº¿æ–¹æ³•æˆ–0SHOT-TCè·å¾—æ›´å¥½çš„æ— ç›‘ç£æ–‡æœ¬åˆ†ç±»ç»“æœ**ã€‚å…·ä½“æ¥è¯´ï¼Œ**ä½¿ç”¨SBERTåµŒå…¥çš„Lbl2TransformerVecæ–¹æ³•è¡¨ç°å‡ºè‰²ï¼Œå› ä¸ºå®ƒä»¬åœ¨æ‰€æœ‰æ•°æ®é›†ä¸Šéƒ½è¡¨ç°è‰¯å¥½**
    ğŸ†ï¼Œå¹¶è¶…è¶Šäº†åŸºçº¿ç»“æœã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œ**0SHOT-TCæ–¹æ³•è¡¨ç°æŒç»­è¾ƒå·®**ï¼Œåœ¨å¤§å¤šæ•°æƒ…å†µä¸‹ç”šè‡³æœªèƒ½è¶…è¶ŠåŸºçº¿ç»“æœ ğŸ˜²ã€‚ç„¶è€Œï¼ŒDeBERTaé›¶-shotè•´æ¶µæ¨¡å‹èƒ½å¤Ÿä»¤äººæƒŠè®¶åœ°å¾ˆå¥½åœ°åˆ†ç±»é¢†åŸŸç‰¹å®šçš„åŒ»å­¦æ‘˜è¦ï¼Œå¹¶åœ¨è¯¥æ•°æ®é›†ä¸Šå–å¾—äº†æ‰€æœ‰åˆ†ç±»å™¨ä¸­çš„æœ€ä½³F1åˆ†æ•°ã€‚æˆ‘ä»¬è§‚å¯Ÿåˆ°ï¼Œå¤§å‹DeBERTaé›¶-shotè•´æ¶µæ¨¡å‹æ€»æ˜¯æ˜¾è‘—ä¼˜äºè¾ƒå°çš„BART-largeå’ŒDistilBERTé›¶-shotè•´æ¶µæ¨¡å‹ã€‚æ­¤å¤–ï¼ŒåŸºäºBERTçš„TARSæ¨¡å‹çš„è¡¨ç°ç•¥ä¼˜äºè¾ƒå°çš„DistilBERTé›¶-shotè•´æ¶µæ¨¡å‹ï¼Œé™¤éåœ¨é¢†åŸŸç‰¹å®šçš„åŒ»å­¦æ‘˜è¦æ•°æ®é›†ä¸Šã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¾—å‡ºç»“è®ºï¼Œ**0SHOT-TCæ–¹æ³•çš„è¡¨ç°éšç€æ¨¡å‹å¤§å°çš„å¢åŠ è€Œæé«˜**
    ğŸ¤”ã€‚
- en: ğŸ’¡Conclusion
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ğŸ’¡ç»“è®º
- en: '**Similarity-based TC approaches generally outperform 0SHOT-TC** approaches
    in a variety of different domains.'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**åŸºäºç›¸ä¼¼æ€§çš„æ–‡æœ¬åˆ†ç±»æ–¹æ³•é€šå¸¸åœ¨å„ç§ä¸åŒé¢†åŸŸä¸­ä¼˜äº0SHOT-TC**æ–¹æ³•ã€‚'
- en: The characteristics of text embeddings enable representations of similar topics
    or classes to be located close to each other in embedding space. This implies
    that text representations which are capable of coherently clustering topics in
    the embedding space perform well when used in unsupervised text classification
    approaches. This characteristic is also evident in our work and can be seen in
    the figure below ğŸ‘‡.
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ–‡æœ¬åµŒå…¥çš„ç‰¹æ€§ä½¿å¾—ç›¸ä¼¼çš„ä¸»é¢˜æˆ–ç±»åˆ«åœ¨åµŒå…¥ç©ºé—´ä¸­é å¾—å¾ˆè¿‘ã€‚è¿™æ„å‘³ç€èƒ½å¤Ÿåœ¨åµŒå…¥ç©ºé—´ä¸­è¿è´¯åœ°èšç±»ä¸»é¢˜çš„æ–‡æœ¬è¡¨ç¤ºåœ¨æ— ç›‘ç£æ–‡æœ¬åˆ†ç±»æ–¹æ³•ä¸­è¡¨ç°è‰¯å¥½ã€‚è¿™ä¸€ç‰¹æ€§åœ¨æˆ‘ä»¬çš„å·¥ä½œä¸­ä¹Ÿå¾ˆæ˜æ˜¾ï¼Œå¯ä»¥åœ¨ä¸‹å›¾ä¸­çœ‹åˆ°
    ğŸ‘‡ã€‚
- en: '![](../Images/2b1581848e8ee0356bd3dbc312e68c7e.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2b1581848e8ee0356bd3dbc312e68c7e.png)'
- en: '[DensMAP](https://www.biorxiv.org/content/10.1101/2020.05.12.077776v1) visualizations
    of the document embeddings for each dataset. The document embeddings were created
    using SBERT (all-mpnet-base-v2).'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '[DensMAP](https://www.biorxiv.org/content/10.1101/2020.05.12.077776v1)çš„æ–‡æ¡£åµŒå…¥å¯è§†åŒ–ã€‚æ–‡æ¡£åµŒå…¥æ˜¯ä½¿ç”¨SBERTï¼ˆall-mpnet-base-v2ï¼‰åˆ›å»ºçš„ã€‚'
- en: Simple approaches such as **LSA or Word2Vec are easy to outperform** and therefore
    are **not recommended to be used as baselines** for text classification of unseen
    classes.
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç®€å•æ–¹æ³•å¦‚**LSAæˆ–Word2Vecå®¹æ˜“è¶…è¶Š**ï¼Œå› æ­¤**ä¸å»ºè®®ç”¨ä½œæœªçŸ¥ç±»æ–‡æœ¬åˆ†ç±»çš„åŸºçº¿**ã€‚
- en: '**SimCSE and SBERT** **baseline approaches** **generate strong unsupervised
    text classification results**, outperforming even some more advanced classifiers.
    Therefore, we propose to **use SimCSE and SBERT baselines for evaluating unsupervised
    text classification** approaches and 0SHOT-TC performance on unseen classes in
    future work.'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**SimCSEå’ŒSBERT**çš„**åŸºçº¿æ–¹æ³•****äº§ç”Ÿå¼ºå¤§çš„æ— ç›‘ç£æ–‡æœ¬åˆ†ç±»ç»“æœ**ï¼Œç”šè‡³è¶…è¿‡äº†ä¸€äº›æ›´é«˜çº§çš„åˆ†ç±»å™¨ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å»ºè®®**ä½¿ç”¨SimCSEå’ŒSBERTåŸºçº¿æ¥è¯„ä¼°æœªç›‘ç£æ–‡æœ¬åˆ†ç±»**æ–¹æ³•å’ŒæœªçŸ¥ç±»0SHOT-TCåœ¨å°†æ¥çš„æ€§èƒ½ã€‚'
- en: '**Lbl2TransformerVec**, our proposed similarity-based text classification approach
    **yields best F1-scores** for almost all datasets.'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Lbl2TransformerVec**ï¼Œæˆ‘ä»¬æå‡ºçš„åŸºäºç›¸ä¼¼æ€§çš„æ–‡æœ¬åˆ†ç±»æ–¹æ³•**è·å¾—äº†æœ€ä½³çš„F1åˆ†æ•°**å‡ ä¹æ‰€æœ‰æ•°æ®é›†ã€‚'
- en: We made our [Lbl2TransformerVec](https://github.com/sebischair/Lbl2Vec) code
    publicly available at [https://github.com/sebischair/Lbl2Vec](https://github.com/sebischair/Lbl2Vec).
    If you want to read more details about our approach, you can read our paper [here](https://arxiv.org/abs/2211.16285).
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†æˆ‘ä»¬çš„[Lbl2TransformerVec](https://github.com/sebischair/Lbl2Vec)ä»£ç å…¬å¼€åœ¨[https://github.com/sebischair/Lbl2Vec](https://github.com/sebischair/Lbl2Vec)ã€‚å¦‚æœæ‚¨æƒ³é˜…è¯»æ›´å¤šå…³äºæˆ‘ä»¬æ–¹æ³•çš„è¯¦ç»†ä¿¡æ¯ï¼Œæ‚¨å¯ä»¥åœ¨è¿™é‡Œé˜…è¯»æˆ‘ä»¬çš„è®ºæ–‡(https://arxiv.org/abs/2211.16285)ã€‚
- en: Sources
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Sources
- en: '[](https://arxiv.org/abs/2211.16285?source=post_page-----83115d9879f5--------------------------------)
    [## Evaluating Unsupervised Text Classification: Zero-shot and Similarity-based
    Approaches'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://arxiv.org/abs/2211.16285?source=post_page-----83115d9879f5--------------------------------)
    [## è¯„ä¼°æ— ç›‘ç£æ–‡æœ¬åˆ†ç±»ï¼šé›¶æ ·æœ¬å’ŒåŸºäºç›¸ä¼¼æ€§çš„æ–¹æ³•'
- en: Text classification of unseen classes is a challenging Natural Language Processing
    task and is mainly attempted usingâ€¦
  id: totrans-54
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: æœªçŸ¥ç±»çš„æ–‡æœ¬åˆ†ç±»æ˜¯ä¸€é¡¹å…·æœ‰æŒ‘æˆ˜æ€§çš„è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ï¼Œä¸»è¦æ˜¯ä½¿ç”¨â€¦
- en: 'arxiv.org](https://arxiv.org/abs/2211.16285?source=post_page-----83115d9879f5--------------------------------)
    [](https://arxiv.org/abs/2210.06023?source=post_page-----83115d9879f5--------------------------------)
    [## Lbl2Vec: An Embedding-Based Approach for Unsupervised Document Retrieval on
    Predefined Topics'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 'arxiv.org](https://arxiv.org/abs/2211.16285?source=post_page-----83115d9879f5--------------------------------)
    [](https://arxiv.org/abs/2210.06023?source=post_page-----83115d9879f5--------------------------------)
    [## Lbl2Vec: An Embedding-Based Approach for Unsupervised Document Retrieval on
    Predefined Topics'
- en: In this paper, we consider the task of retrieving documents with predefined
    topics from an unlabeled document datasetâ€¦
  id: totrans-56
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬è€ƒè™‘äº†ä»æ— æ ‡è®°æ–‡æ¡£æ•°æ®é›†ä¸­æ£€ç´¢å…·æœ‰é¢„å®šä¹‰ä¸»é¢˜çš„æ–‡æ¡£çš„ä»»åŠ¡â€¦
- en: arxiv.org](https://arxiv.org/abs/2210.06023?source=post_page-----83115d9879f5--------------------------------)
    [](/unsupervised-text-classification-with-lbl2vec-6c5e040354de?source=post_page-----83115d9879f5--------------------------------)
    [## Unsupervised Text Classification with Lbl2Vec
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: arxiv.org](https://arxiv.org/abs/2210.06023?source=post_page-----83115d9879f5--------------------------------)
    [](/unsupervised-text-classification-with-lbl2vec-6c5e040354de?source=post_page-----83115d9879f5--------------------------------)
    [## Unsupervised Text Classification with Lbl2Vec
- en: An introduction to embedding-based classification of unlabeled text documents
  id: totrans-58
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: An introduction to embedding-based classification of unlabeled text documents
- en: 'towardsdatascience.com](/unsupervised-text-classification-with-lbl2vec-6c5e040354de?source=post_page-----83115d9879f5--------------------------------)
    [](https://github.com/sebischair/Lbl2Vec?source=post_page-----83115d9879f5--------------------------------)
    [## GitHub - sebischair/Lbl2Vec: Lbl2Vec learns jointly embedded label, document
    and word vectors toâ€¦'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 'towardsdatascience.com](/unsupervised-text-classification-with-lbl2vec-6c5e040354de?source=post_page-----83115d9879f5--------------------------------)
    [](https://github.com/sebischair/Lbl2Vec?source=post_page-----83115d9879f5--------------------------------)
    [## GitHub - sebischair/Lbl2Vec: Lbl2Vec learns jointly embedded label, document
    and word vectors toâ€¦'
- en: Lbl2Vec learns jointly embedded label, document and word vectors to retrieve
    documents with predefined topics from anâ€¦
  id: totrans-60
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Lbl2Vecå­¦ä¹ è”åˆåµŒå…¥æ ‡ç­¾ï¼Œæ–‡æ¡£å’Œå•è¯å‘é‡æ¥æ£€ç´¢å…·æœ‰é¢„å®šä¹‰ä¸»é¢˜çš„æ–‡æ¡£â€¦
- en: 'github.com](https://github.com/sebischair/Lbl2Vec?source=post_page-----83115d9879f5--------------------------------)
    [](https://aclanthology.org/D19-1410/?source=post_page-----83115d9879f5--------------------------------)
    [## Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 'github.com](https://github.com/sebischair/Lbl2Vec?source=post_page-----83115d9879f5--------------------------------)
    [](https://aclanthology.org/D19-1410/?source=post_page-----83115d9879f5--------------------------------)
    [## Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks'
- en: Abstract BERT (Devlin et al., 2018) and RoBERTa (Liu et al., 2019) has set a
    new state-of-the-art performance onâ€¦
  id: totrans-62
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Abstract BERTï¼ˆDevlin et al., 2018ï¼‰å’ŒRoBERTaï¼ˆLiu et al., 2019ï¼‰åœ¨æ–°çš„æœ€å…ˆè¿›è¡¨ç°æ–¹é¢è®¾ç«‹äº†æ–°çš„æ ‡å‡†æ€§èƒ½â€¦
- en: 'aclanthology.org](https://aclanthology.org/D19-1410/?source=post_page-----83115d9879f5--------------------------------)
    [](https://aclanthology.org/2021.emnlp-main.552/?source=post_page-----83115d9879f5--------------------------------)
    [## SimCSE: Simple Contrastive Learning of Sentence Embeddings'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: Tianyu Gao, Xingcheng Yao, Danqi Chen. Proceedings of the 2021 Conference on
    Empirical Methods in Natural Languageâ€¦
  id: totrans-64
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'aclanthology.org](https://aclanthology.org/2021.emnlp-main.552/?source=post_page-----83115d9879f5--------------------------------)
    [](https://aclanthology.org/D19-1404/?source=post_page-----83115d9879f5--------------------------------)
    [## Benchmarking Zero-shot Text Classification: Datasets, Evaluation and Entailment
    Approach'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: Abstract Zero-shot text classification (0Shot-TC) is a challenging NLU problem
    to which little attention has been paidâ€¦
  id: totrans-66
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: aclanthology.org](https://aclanthology.org/D19-1404/?source=post_page-----83115d9879f5--------------------------------)
    [](https://aclanthology.org/2020.coling-main.285/?source=post_page-----83115d9879f5--------------------------------)
    [## Task-Aware Representation of Sentences for Generic Text Classification
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: Kishaloy Halder, Alan Akbik, Josip Krapac, Roland Vollgraf. Proceedings of the
    28th International Conference onâ€¦
  id: totrans-68
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: aclanthology.org](https://aclanthology.org/2020.coling-main.285/?source=post_page-----83115d9879f5--------------------------------)
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
