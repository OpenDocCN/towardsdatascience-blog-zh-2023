- en: Zero-Shot vs. Similarity-Based Text Classification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ÂéüÊñáÔºö[https://towardsdatascience.com/zero-shot-vs-similarity-based-text-classification-83115d9879f5?source=collection_archive---------3-----------------------#2023-01-05](https://towardsdatascience.com/zero-shot-vs-similarity-based-text-classification-83115d9879f5?source=collection_archive---------3-----------------------#2023-01-05)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: An evaluation of unsupervised text classification approaches
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@tim.schopf?source=post_page-----83115d9879f5--------------------------------)[![Tim
    Schopf](../Images/7d98a87af243ae6a82f837aa04ac2675.png)](https://medium.com/@tim.schopf?source=post_page-----83115d9879f5--------------------------------)[](https://towardsdatascience.com/?source=post_page-----83115d9879f5--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----83115d9879f5--------------------------------)
    [Tim Schopf](https://medium.com/@tim.schopf?source=post_page-----83115d9879f5--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ¬∑
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F7fe3665aa3e3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fzero-shot-vs-similarity-based-text-classification-83115d9879f5&user=Tim+Schopf&userId=7fe3665aa3e3&source=post_page-7fe3665aa3e3----83115d9879f5---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----83115d9879f5--------------------------------)
    ¬∑9 min read¬∑Jan 5, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F83115d9879f5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fzero-shot-vs-similarity-based-text-classification-83115d9879f5&user=Tim+Schopf&userId=7fe3665aa3e3&source=-----83115d9879f5---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F83115d9879f5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fzero-shot-vs-similarity-based-text-classification-83115d9879f5&source=-----83115d9879f5---------------------bookmark_footer-----------)![](../Images/071fc6c7022736231951f65546ea8c1e.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Image by [Gertr≈´da Valaseviƒçi≈´tƒó](https://unsplash.com/@skraidantisdrambliukas?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    on [Unsplash](https://unsplash.com/de/fotos/xMObPS6V_gY?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
  prefs: []
  type: TYPE_NORMAL
- en: '*This post is based on our NLPIR 2022 paper* [*‚ÄúEvaluating Unsupervised Text
    Classification: Zero-shot and Similarity-based Approaches‚Äù*](https://arxiv.org/abs/2211.16285)*.
    You can read more details there.*'
  prefs: []
  type: TYPE_NORMAL
- en: What is unsupervised text classification?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Unsupervised text classification approaches aim to perform categorization without
    using annotated data during training and therefore offer the potential to reduce
    annotation costsüí∞. Generally, unsupervised text classification approaches aim
    to map text to labels based on their textual description, without using annotated
    training data. To accomplish this, there exist mainly two categories of approaches.
  prefs: []
  type: TYPE_NORMAL
- en: üëØ The first category can be summarized under **similarity-based** **approaches**.
    Thereby, the approaches generate semantic embeddings of both the texts and the
    label descriptions, before attempting to match the texts to the labels using similarity
    measures such as cosine similarity.
  prefs: []
  type: TYPE_NORMAL
- en: 0Ô∏è‚É£ üî´ The second category uses **zero-shot learning** to classify texts of unseen
    classes. Zero-shot learning uses labeled training instances belonging to seen
    classes to learn a classifier that can predict testing instances belonging to
    different, unseen classes. For example, a zero-shot classification model may learn
    to correctly classify texts about soccer ‚öΩ during training, and then use this
    knowledge during testing to classify texts about basketball üèÄ without ever having
    seen texts about basketball before. The idea is that the model can transfer the
    knowledge learned about soccer to the very similar class of basketball. Although
    zero-shot learning techniques employ annotated data for training, they do not
    use labels to provide information about the target classes and can use their knowledge
    of the previously seen classes to classify instances of unseen classes. Since
    pretrained zero-shot text classification models do not require fine-tuning on
    labeled data from the target classes, we categorize them as an unsupervised text
    classification strategy.
  prefs: []
  type: TYPE_NORMAL
- en: '**In this blog post, we summarize the contributions of our paper üìÑ** [***‚ÄúEvaluating
    Unsupervised Text Classification: Zero-shot and Similarity-based Approaches (2022)‚Äù***](https://arxiv.org/abs/2211.16285)**as
    follows:**'
  prefs: []
  type: TYPE_NORMAL
- en: We **evaluate** the **similarity-based** and **zero-shot learning** categories
    for unsupervised text classification of topics. Thereby, we conduct experiments
    with representative approaches of each category on different benchmark datasets.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We **propose simple but strong baselines for unsupervised text classification**
    based on [SimCSE](https://aclanthology.org/2021.emnlp-main.552/) and [SBERT](https://aclanthology.org/D19-1410/)
    sentence embedding similarities. Previous work has mostly been evaluated against
    different weak baselines such as [Word2Vec](/introduction-to-word-embedding-and-word2vec-652d0c2060fa)
    similarities which are easy to outperform and tend to overestimate the performance
    of new unsupervised text classification approaches.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Since transformer-based text representations have been widely established as
    state-of-the-art for semantic text similarity in recent years, **we further adapt**
    [**Lbl2Vec**](/unsupervised-text-classification-with-lbl2vec-6c5e040354de), one
    of the most recent and well-performing similarity-based methods for unsupervised
    text classification, to be used with transformer-based language models.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Unsupervised text classification approaches
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: üëØ Similarity-based text classification with Lbl2Vec
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Numerous similarity-based approaches for unsupervised text classification exist.
    However, the recently introduced [**Lbl2Vec**](https://medium.com/towards-data-science/unsupervised-text-classification-with-lbl2vec-6c5e040354de)
    **approach yields improved performance compared with other similarity-based approaches**.
    Therefore, we focused on this approach in this study. [Lbl2Vec](https://medium.com/towards-data-science/unsupervised-text-classification-with-lbl2vec-6c5e040354de)
    works by jointly embedding word, document, and label representations. First, word
    and documented representations are learned with [Doc2Vec](https://radimrehurek.com/gensim/models/doc2vec.html).
    Then, the average of label keyword representations for each class is used to find
    a set of most similar candidate document representations via cosine similarity.
    The average of candidate document representations, in turn, generates the label
    vector for each class. For classification, eventually, the documents are assigned
    to the class where the cosine similarity of the label vector and the document
    vector is the highest. [Here](https://medium.com/towards-data-science/unsupervised-text-classification-with-lbl2vec-6c5e040354de),
    you can find more information about how [Lbl2Vec](https://medium.com/towards-data-science/unsupervised-text-classification-with-lbl2vec-6c5e040354de)
    works.
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, **we adapt the** [**Lbl2Vec**](https://medium.com/towards-data-science/unsupervised-text-classification-with-lbl2vec-6c5e040354de)
    **approach, using transformer-based text representations** instead of [Doc2Vec](https://radimrehurek.com/gensim/models/doc2vec.html)
    to create jointly embedded word, document, and label representations. Since transformer-based
    text representations currently achieve state-of-the-art results in text- similarity
    tasks, we investigate the effect of the different resulting text representations
    on this similarity-based text classification strategy. In this paper, we use [SimCSE](https://aclanthology.org/2021.emnlp-main.552/)
    and [SBERT](https://aclanthology.org/D19-1410/) transformer- models to create
    text representations. In the following, this approach is referred to as **Lbl2TransformerVec**.
  prefs: []
  type: TYPE_NORMAL
- en: 0Ô∏è‚É£ üî´ Zero-shot text classification using the entailment approach
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Although newer zero-shot text classification (0SHOT-TC) approaches exist, **the**
    [**zero-shot entailment approach**](https://aclanthology.org/D19-1404/) **still
    produces state-of-the-art 0SHOT-TC results in predicting instances of unseen classes**
    compared to models of similar size. As the name already implies, the zero-shot
    entailment approach deals with 0SHOT-TC as a textual entailment problem. The underlying
    idea is similar to that of similarity-based text classification approaches. Conventional
    0SHOT-TC classifiers fail to understand the actual problem since the label names
    are usually converted into simple indices. Therefore, these classifiers can hardly
    generalize from seen to unseen classes. **Considering 0SHOT-TC as an entailment
    problem provides the classifier with a textual label description and therefore
    enables it to understand the meaning of labels.**
  prefs: []
  type: TYPE_NORMAL
- en: 0Ô∏è‚É£ üî´ Zero-shot text classification using TARS
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[TARS](https://aclanthology.org/2020.coling-main.285/) also uses the textual
    label description to classify text in a zero-shot setting. However, TARS approaches
    the task as a binary classification problem, where a text and a textual label
    description is given to the model, which makes a prediction about whether that
    label is true or not. The TARS authors state that this approach significantly
    outperforms [GPT-2](https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf)
    in 0SHOT-TC'
  prefs: []
  type: TYPE_NORMAL
- en: üìê Baselines
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We compare the findings of current state-of-the-art unsupervised text classification
    approaches to some basic baselines to evaluate their performance.
  prefs: []
  type: TYPE_NORMAL
- en: '[**LSA:**](https://asistdl.onlinelibrary.wiley.com/doi/abs/10.1002/%28SICI%291097-4571%28199009%2941%3A6%3C391%3A%3AAID-ASI1%3E3.0.CO%3B2-9)For
    each dataset, we apply LSA to learn ùëõ = number of classes concepts. Afterwards,
    the text documents are classified according to the highest cosine similarity of
    resulting LSA vectors of documents and label keywords.'
  prefs: []
  type: TYPE_NORMAL
- en: '[**Word2Vec:**](https://papers.nips.cc/paper/2013/hash/9aa42b31882ec039965f3c4923ce901b-Abstract.html)This
    produces semantic vector representations of words based on surrounding context
    words. The average of word embeddings is used to represent the text documents
    and label keywords. The text documents are predicted according to the highest
    cosine similarity of the resulting Word2Vec representations of documents and label
    keywords for classification.'
  prefs: []
  type: TYPE_NORMAL
- en: '[**SimCSE:**](https://aclanthology.org/2021.emnlp-main.552/)This is a contrastive
    learning framework that produces sentence embeddings which acieve state-of-the-art
    results in semantic similarity tasks. We use, SimCSE document embeddings and SimCSE
    label keyword embeddings as class representations. Finally, the text documents
    are classified according to the highest cosine similarity of the resulting SimCSE
    representations of document and label keywords.'
  prefs: []
  type: TYPE_NORMAL
- en: '[**SBERT:**](https://aclanthology.org/D19-1410/)This is a modification of [BERT](https://aclanthology.org/N19-1423/)
    that uses siamese and triplet network structures to derive semantically meaningful
    sentence embeddings. We use the same classification approach as with SimCSE, except
    that we now use SBERT embeddings instead of SimCSE embeddings.'
  prefs: []
  type: TYPE_NORMAL
- en: üî¨ Experiments
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For our SimCSE experiments, we use the [sup-simcse-roberta-large](https://huggingface.co/princeton-nlp/sup-simcse-roberta-large)
    model. To create embeddings with SBERT, we use two different pretrained SBERT
    models. We choose the general purpose models [all-mpnet- base-v2](https://huggingface.co/sentence-transformers/all-mpnet-base-v2)
    and [all-MiniLM-L6-v2](https://huggingface.co/sentence-transformers/all-mpnet-base-v2),
    trained on more than one billion training pairs and expected to perform well on
    sentence similarity tasks. The all-mpnet-base-v2 model is larger than the all-MiniLM-
    L6-v2 model and guarantees slightly better quality sentence embeddings. The smaller
    all-MiniLM-L6-v2 model, on the other hand, guarantees a five times faster encoding
    time while still providing sentence embeddings of high quality.
  prefs: []
  type: TYPE_NORMAL
- en: 'For evaluation of 0SHOT-TC, we conduct experiments with three different pretrained
    zero-shot entailment models: a [DeBERTa](https://huggingface.co/MoritzLaurer/DeBERTa-v3-base-mnli-fever-docnli-ling-2c)
    model, a large [BART](https://huggingface.co/facebook/bart-large-mnli) model,
    and a smaller [DistilBERT](https://huggingface.co/typeform/distilbert-base-uncased-mnli)
    model. For TARS experiments, we use the BERT-based pretrained [tars-base-v8](https://flair.informatik.hu-berlin.de/resources/models/tars-base/)
    model.'
  prefs: []
  type: TYPE_NORMAL
- en: üíæ Data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Our evaluation is based on the four publicly available text classification datasets,
    [*20Newsgroups*](http://qwone.com/~jason/20Newsgroups), [*AG‚Äòs Corpus*](http://groups.di.unipi.it/~gulli/AG_corpus_of_news_articles)
    , [*Yahoo! Answers*](https://huggingface.co/datasets/yahoo_answers_topics), and
    [*Medical Abstracts*](https://github.com/sebischair/Medical-Abstracts-TC-Corpus)
    from different domains. As we use the semantic meaning of class descriptions for
    unsupervised text classification, we infer label keywords from each class name
    that serves the purpose of textual class descriptions. Thereby, the inference
    step simply consists of using the class names provided by the official documentation
    of the datasets as label keywords. In a few cases, we additionally substituted
    the class names with synonymous or semantically similar keywords, if we considered
    this to be a more appropriate description of a certain class.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ae318fb6f9a50f1d5430f36fef35eda6.png)'
  prefs: []
  type: TYPE_IMG
- en: Overview of the used text classification datasets.
  prefs: []
  type: TYPE_NORMAL
- en: üìä Evaluation Results
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/f94c4acf04ac7ef43f10e5be515fca65.png)'
  prefs: []
  type: TYPE_IMG
- en: F1-scores (micro) of examined text classification approaches on different datasets.
    The best results on the respective dataset are displayed in bold. Since we use
    micro-averaging to calculate our classification metrics, we realize equal F1,
    Precision, and Recall scores respectively.
  prefs: []
  type: TYPE_NORMAL
- en: We can observe that none of the baselines achieves the highest F1-scores on
    any dataset based on these data. This indicates that the use of advanced unsupervised
    text classification approaches usually yields better results than simple baseline
    approaches. However, we observe that the **LSA and Word2Vec approaches** generally
    **yield the worst results and** **are easy to outperform** üëé. In contrast, the
    **SimCSE and SBERT baselines produce strong F1-scores** üí™ that even some of the
    advanced approaches could not surpass in certain cases. Furthermore, the SimCSE
    and SBERT baseline approaches may produce better results than the Lbl2Vec similarity-based
    approach on three datasets. We nevertheless can deduce that **the use of advanced
    similarity-based approaches generally produces better unsupervised text classification
    results than the use of simple baseline approaches or 0SHOT-TC**. Specifically,
    the **Lbl2TransformerVec approaches using SBERT embeddings appear to be promising,
    as they consistently perform well across all datasets** üèÜ and outperform the baseline
    results. In contrast, the **0SHOT-TC approaches perform consistently weak** and
    in the majority of cases did not even manage to outperform the baseline results
    üò≤. However, the DeBERTa zero-shot entailment model could classify the domain-specific
    medical abstracts surprisingly well and achieved the best F1-scores of all classifiers
    on this dataset. We observe, that the large DeBERTa zero-shot entailment model
    always significantly outperforms the smaller BART-large and DistilBERT zero-shot
    entailment models. Additionally, the BERT-based TARS model performs slightly better
    than the smaller DistilBERT zero-shot entailment model, except in case of the
    domain-specific Medical Abstracts dataset. Thus, we conclude that **the performance
    of the 0SHOT-TC approaches improves with increasing model size** ü§î.
  prefs: []
  type: TYPE_NORMAL
- en: üí°Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Similarity-based TC approaches generally outperform 0SHOT-TC** approaches
    in a variety of different domains.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The characteristics of text embeddings enable representations of similar topics
    or classes to be located close to each other in embedding space. This implies
    that text representations which are capable of coherently clustering topics in
    the embedding space perform well when used in unsupervised text classification
    approaches. This characteristic is also evident in our work and can be seen in
    the figure below üëá.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/2b1581848e8ee0356bd3dbc312e68c7e.png)'
  prefs: []
  type: TYPE_IMG
- en: '[DensMAP](https://www.biorxiv.org/content/10.1101/2020.05.12.077776v1) visualizations
    of the document embeddings for each dataset. The document embeddings were created
    using SBERT (all-mpnet-base-v2).'
  prefs: []
  type: TYPE_NORMAL
- en: Simple approaches such as **LSA or Word2Vec are easy to outperform** and therefore
    are **not recommended to be used as baselines** for text classification of unseen
    classes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**SimCSE and SBERT** **baseline approaches** **generate strong unsupervised
    text classification results**, outperforming even some more advanced classifiers.
    Therefore, we propose to **use SimCSE and SBERT baselines for evaluating unsupervised
    text classification** approaches and 0SHOT-TC performance on unseen classes in
    future work.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Lbl2TransformerVec**, our proposed similarity-based text classification approach
    **yields best F1-scores** for almost all datasets.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We made our [Lbl2TransformerVec](https://github.com/sebischair/Lbl2Vec) code
    publicly available at [https://github.com/sebischair/Lbl2Vec](https://github.com/sebischair/Lbl2Vec).
    If you want to read more details about our approach, you can read our paper [here](https://arxiv.org/abs/2211.16285).
  prefs: []
  type: TYPE_NORMAL
- en: Sources
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[](https://arxiv.org/abs/2211.16285?source=post_page-----83115d9879f5--------------------------------)
    [## Evaluating Unsupervised Text Classification: Zero-shot and Similarity-based
    Approaches'
  prefs: []
  type: TYPE_NORMAL
- en: Text classification of unseen classes is a challenging Natural Language Processing
    task and is mainly attempted using‚Ä¶
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'arxiv.org](https://arxiv.org/abs/2211.16285?source=post_page-----83115d9879f5--------------------------------)
    [](https://arxiv.org/abs/2210.06023?source=post_page-----83115d9879f5--------------------------------)
    [## Lbl2Vec: An Embedding-Based Approach for Unsupervised Document Retrieval on
    Predefined Topics'
  prefs: []
  type: TYPE_NORMAL
- en: In this paper, we consider the task of retrieving documents with predefined
    topics from an unlabeled document dataset‚Ä¶
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: arxiv.org](https://arxiv.org/abs/2210.06023?source=post_page-----83115d9879f5--------------------------------)
    [](/unsupervised-text-classification-with-lbl2vec-6c5e040354de?source=post_page-----83115d9879f5--------------------------------)
    [## Unsupervised Text Classification with Lbl2Vec
  prefs: []
  type: TYPE_NORMAL
- en: An introduction to embedding-based classification of unlabeled text documents
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'towardsdatascience.com](/unsupervised-text-classification-with-lbl2vec-6c5e040354de?source=post_page-----83115d9879f5--------------------------------)
    [](https://github.com/sebischair/Lbl2Vec?source=post_page-----83115d9879f5--------------------------------)
    [## GitHub - sebischair/Lbl2Vec: Lbl2Vec learns jointly embedded label, document
    and word vectors to‚Ä¶'
  prefs: []
  type: TYPE_NORMAL
- en: Lbl2Vec learns jointly embedded label, document and word vectors to retrieve
    documents with predefined topics from an‚Ä¶
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'github.com](https://github.com/sebischair/Lbl2Vec?source=post_page-----83115d9879f5--------------------------------)
    [](https://aclanthology.org/D19-1410/?source=post_page-----83115d9879f5--------------------------------)
    [## Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks'
  prefs: []
  type: TYPE_NORMAL
- en: Abstract BERT (Devlin et al., 2018) and RoBERTa (Liu et al., 2019) has set a
    new state-of-the-art performance on‚Ä¶
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'aclanthology.org](https://aclanthology.org/D19-1410/?source=post_page-----83115d9879f5--------------------------------)
    [](https://aclanthology.org/2021.emnlp-main.552/?source=post_page-----83115d9879f5--------------------------------)
    [## SimCSE: Simple Contrastive Learning of Sentence Embeddings'
  prefs: []
  type: TYPE_NORMAL
- en: Tianyu Gao, Xingcheng Yao, Danqi Chen. Proceedings of the 2021 Conference on
    Empirical Methods in Natural Language‚Ä¶
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'aclanthology.org](https://aclanthology.org/2021.emnlp-main.552/?source=post_page-----83115d9879f5--------------------------------)
    [](https://aclanthology.org/D19-1404/?source=post_page-----83115d9879f5--------------------------------)
    [## Benchmarking Zero-shot Text Classification: Datasets, Evaluation and Entailment
    Approach'
  prefs: []
  type: TYPE_NORMAL
- en: Abstract Zero-shot text classification (0Shot-TC) is a challenging NLU problem
    to which little attention has been paid‚Ä¶
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: aclanthology.org](https://aclanthology.org/D19-1404/?source=post_page-----83115d9879f5--------------------------------)
    [](https://aclanthology.org/2020.coling-main.285/?source=post_page-----83115d9879f5--------------------------------)
    [## Task-Aware Representation of Sentences for Generic Text Classification
  prefs: []
  type: TYPE_NORMAL
- en: Kishaloy Halder, Alan Akbik, Josip Krapac, Roland Vollgraf. Proceedings of the
    28th International Conference on‚Ä¶
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: aclanthology.org](https://aclanthology.org/2020.coling-main.285/?source=post_page-----83115d9879f5--------------------------------)
  prefs: []
  type: TYPE_NORMAL
