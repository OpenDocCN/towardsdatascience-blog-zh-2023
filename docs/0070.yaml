- en: Zero-Shot vs. Similarity-Based Text Classification
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 零-shot 与相似度基于的文本分类
- en: 原文：[https://towardsdatascience.com/zero-shot-vs-similarity-based-text-classification-83115d9879f5?source=collection_archive---------3-----------------------#2023-01-05](https://towardsdatascience.com/zero-shot-vs-similarity-based-text-classification-83115d9879f5?source=collection_archive---------3-----------------------#2023-01-05)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/zero-shot-vs-similarity-based-text-classification-83115d9879f5?source=collection_archive---------3-----------------------#2023-01-05](https://towardsdatascience.com/zero-shot-vs-similarity-based-text-classification-83115d9879f5?source=collection_archive---------3-----------------------#2023-01-05)
- en: An evaluation of unsupervised text classification approaches
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 对无监督文本分类方法的评估
- en: '[](https://medium.com/@tim.schopf?source=post_page-----83115d9879f5--------------------------------)[![Tim
    Schopf](../Images/7d98a87af243ae6a82f837aa04ac2675.png)](https://medium.com/@tim.schopf?source=post_page-----83115d9879f5--------------------------------)[](https://towardsdatascience.com/?source=post_page-----83115d9879f5--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----83115d9879f5--------------------------------)
    [Tim Schopf](https://medium.com/@tim.schopf?source=post_page-----83115d9879f5--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@tim.schopf?source=post_page-----83115d9879f5--------------------------------)[![Tim
    Schopf](../Images/7d98a87af243ae6a82f837aa04ac2675.png)](https://medium.com/@tim.schopf?source=post_page-----83115d9879f5--------------------------------)[](https://towardsdatascience.com/?source=post_page-----83115d9879f5--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----83115d9879f5--------------------------------)
    [Tim Schopf](https://medium.com/@tim.schopf?source=post_page-----83115d9879f5--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F7fe3665aa3e3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fzero-shot-vs-similarity-based-text-classification-83115d9879f5&user=Tim+Schopf&userId=7fe3665aa3e3&source=post_page-7fe3665aa3e3----83115d9879f5---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----83115d9879f5--------------------------------)
    ·9 min read·Jan 5, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F83115d9879f5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fzero-shot-vs-similarity-based-text-classification-83115d9879f5&user=Tim+Schopf&userId=7fe3665aa3e3&source=-----83115d9879f5---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F7fe3665aa3e3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fzero-shot-vs-similarity-based-text-classification-83115d9879f5&user=Tim+Schopf&userId=7fe3665aa3e3&source=post_page-7fe3665aa3e3----83115d9879f5---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----83115d9879f5--------------------------------)
    ·9分钟阅读·2023年1月5日'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F83115d9879f5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fzero-shot-vs-similarity-based-text-classification-83115d9879f5&source=-----83115d9879f5---------------------bookmark_footer-----------)![](../Images/071fc6c7022736231951f65546ea8c1e.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F83115d9879f5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fzero-shot-vs-similarity-based-text-classification-83115d9879f5&source=-----83115d9879f5---------------------bookmark_footer-----------)![](../Images/071fc6c7022736231951f65546ea8c1e.png)'
- en: Image by [Gertrūda Valasevičiūtė](https://unsplash.com/@skraidantisdrambliukas?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    on [Unsplash](https://unsplash.com/de/fotos/xMObPS6V_gY?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源于 [Gertrūda Valasevičiūtė](https://unsplash.com/@skraidantisdrambliukas?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    在 [Unsplash](https://unsplash.com/de/fotos/xMObPS6V_gY?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
- en: '*This post is based on our NLPIR 2022 paper* [*“Evaluating Unsupervised Text
    Classification: Zero-shot and Similarity-based Approaches”*](https://arxiv.org/abs/2211.16285)*.
    You can read more details there.*'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '*这篇文章基于我们的 NLPIR 2022 论文* [*“评估无监督文本分类：零-shot 与相似度基于的方法”*](https://arxiv.org/abs/2211.16285)*。你可以在这里阅读更多细节。*'
- en: What is unsupervised text classification?
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是无监督文本分类？
- en: Unsupervised text classification approaches aim to perform categorization without
    using annotated data during training and therefore offer the potential to reduce
    annotation costs💰. Generally, unsupervised text classification approaches aim
    to map text to labels based on their textual description, without using annotated
    training data. To accomplish this, there exist mainly two categories of approaches.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 无监督文本分类方法旨在在训练期间不使用标注数据的情况下进行分类，因此有潜力减少标注成本💰。一般来说，无监督文本分类方法旨在根据文本描述将文本映射到标签，而不使用标注训练数据。为此，主要存在两类方法。
- en: 👯 The first category can be summarized under **similarity-based** **approaches**.
    Thereby, the approaches generate semantic embeddings of both the texts and the
    label descriptions, before attempting to match the texts to the labels using similarity
    measures such as cosine similarity.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 👯 第一类方法可以总结为**基于相似性**的**方法**。在此过程中，这些方法首先生成文本和标签描述的语义嵌入，然后尝试使用诸如余弦相似度之类的相似性度量将文本与标签进行匹配。
- en: 0️⃣ 🔫 The second category uses **zero-shot learning** to classify texts of unseen
    classes. Zero-shot learning uses labeled training instances belonging to seen
    classes to learn a classifier that can predict testing instances belonging to
    different, unseen classes. For example, a zero-shot classification model may learn
    to correctly classify texts about soccer ⚽ during training, and then use this
    knowledge during testing to classify texts about basketball 🏀 without ever having
    seen texts about basketball before. The idea is that the model can transfer the
    knowledge learned about soccer to the very similar class of basketball. Although
    zero-shot learning techniques employ annotated data for training, they do not
    use labels to provide information about the target classes and can use their knowledge
    of the previously seen classes to classify instances of unseen classes. Since
    pretrained zero-shot text classification models do not require fine-tuning on
    labeled data from the target classes, we categorize them as an unsupervised text
    classification strategy.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 0️⃣ 🔫 第二类方法使用**零样本学习**来分类未见过的文本。零样本学习利用属于已见类别的标记训练实例来学习一个分类器，该分类器可以预测属于不同、未见过类别的测试实例。例如，一个零样本分类模型可能在训练期间学会正确分类关于足球⚽的文本，然后在测试时利用这一知识来分类关于篮球🏀的文本，即使之前从未见过关于篮球的文本。其理念是，模型可以将关于足球的知识转移到非常相似的篮球类别上。虽然零样本学习技术在训练时使用了标注数据，但它们不使用标签来提供目标类别的信息，而是利用已见类别的知识来分类未见类别的实例。由于预训练的零样本文本分类模型不需要在目标类别的标记数据上进行微调，因此我们将其归类为无监督文本分类策略。
- en: '**In this blog post, we summarize the contributions of our paper 📄** [***“Evaluating
    Unsupervised Text Classification: Zero-shot and Similarity-based Approaches (2022)”***](https://arxiv.org/abs/2211.16285)**as
    follows:**'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '**在这篇博客文章中，我们总结了我们论文📄** [***“评估无监督文本分类：零样本和基于相似性的 approaches (2022)”***](https://arxiv.org/abs/2211.16285)**的贡献如下：**'
- en: We **evaluate** the **similarity-based** and **zero-shot learning** categories
    for unsupervised text classification of topics. Thereby, we conduct experiments
    with representative approaches of each category on different benchmark datasets.
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们**评估**了无监督文本分类中**基于相似性**和**零样本学习**类别。为此，我们在不同的基准数据集上对每个类别的代表性方法进行实验。
- en: We **propose simple but strong baselines for unsupervised text classification**
    based on [SimCSE](https://aclanthology.org/2021.emnlp-main.552/) and [SBERT](https://aclanthology.org/D19-1410/)
    sentence embedding similarities. Previous work has mostly been evaluated against
    different weak baselines such as [Word2Vec](/introduction-to-word-embedding-and-word2vec-652d0c2060fa)
    similarities which are easy to outperform and tend to overestimate the performance
    of new unsupervised text classification approaches.
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们**提出了基于 [SimCSE](https://aclanthology.org/2021.emnlp-main.552/) 和 [SBERT](https://aclanthology.org/D19-1410/)
    句子嵌入相似性的简单但强大的无监督文本分类基线**。之前的工作大多与不同的弱基线如 [Word2Vec](/introduction-to-word-embedding-and-word2vec-652d0c2060fa)
    相比，这些弱基线容易被超越，并且往往高估了新的无监督文本分类方法的性能。
- en: Since transformer-based text representations have been widely established as
    state-of-the-art for semantic text similarity in recent years, **we further adapt**
    [**Lbl2Vec**](/unsupervised-text-classification-with-lbl2vec-6c5e040354de), one
    of the most recent and well-performing similarity-based methods for unsupervised
    text classification, to be used with transformer-based language models.
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于基于变换器的文本表示近年来被广泛建立为语义文本相似度的最先进技术，**我们进一步调整** [**Lbl2Vec**](/unsupervised-text-classification-with-lbl2vec-6c5e040354de)，这是最新且表现优良的无监督文本分类相似度方法之一，以与基于变换器的语言模型配合使用。
- en: Unsupervised text classification approaches
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 无监督文本分类方法
- en: 👯 Similarity-based text classification with Lbl2Vec
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 👯 使用Lbl2Vec的基于相似度的文本分类
- en: Numerous similarity-based approaches for unsupervised text classification exist.
    However, the recently introduced [**Lbl2Vec**](https://medium.com/towards-data-science/unsupervised-text-classification-with-lbl2vec-6c5e040354de)
    **approach yields improved performance compared with other similarity-based approaches**.
    Therefore, we focused on this approach in this study. [Lbl2Vec](https://medium.com/towards-data-science/unsupervised-text-classification-with-lbl2vec-6c5e040354de)
    works by jointly embedding word, document, and label representations. First, word
    and documented representations are learned with [Doc2Vec](https://radimrehurek.com/gensim/models/doc2vec.html).
    Then, the average of label keyword representations for each class is used to find
    a set of most similar candidate document representations via cosine similarity.
    The average of candidate document representations, in turn, generates the label
    vector for each class. For classification, eventually, the documents are assigned
    to the class where the cosine similarity of the label vector and the document
    vector is the highest. [Here](https://medium.com/towards-data-science/unsupervised-text-classification-with-lbl2vec-6c5e040354de),
    you can find more information about how [Lbl2Vec](https://medium.com/towards-data-science/unsupervised-text-classification-with-lbl2vec-6c5e040354de)
    works.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 存在许多基于相似度的无监督文本分类方法。然而，最近引入的[**Lbl2Vec**](https://medium.com/towards-data-science/unsupervised-text-classification-with-lbl2vec-6c5e040354de)
    **方法相比其他基于相似度的方法表现更佳**。因此，我们在本研究中重点关注了这一方法。[Lbl2Vec](https://medium.com/towards-data-science/unsupervised-text-classification-with-lbl2vec-6c5e040354de)通过联合嵌入词、文档和标签表示来工作。首先，使用[Doc2Vec](https://radimrehurek.com/gensim/models/doc2vec.html)学习词和文档的表示。然后，通过余弦相似度利用每个类别的标签关键词表示的平均值来找到一组最相似的候选文档表示。候选文档表示的平均值，反过来，为每个类别生成标签向量。最终，对于分类，文档被分配到标签向量和文档向量余弦相似度最高的类别。[在这里](https://medium.com/towards-data-science/unsupervised-text-classification-with-lbl2vec-6c5e040354de)，你可以找到更多关于[Lbl2Vec](https://medium.com/towards-data-science/unsupervised-text-classification-with-lbl2vec-6c5e040354de)如何工作的资料。
- en: Furthermore, **we adapt the** [**Lbl2Vec**](https://medium.com/towards-data-science/unsupervised-text-classification-with-lbl2vec-6c5e040354de)
    **approach, using transformer-based text representations** instead of [Doc2Vec](https://radimrehurek.com/gensim/models/doc2vec.html)
    to create jointly embedded word, document, and label representations. Since transformer-based
    text representations currently achieve state-of-the-art results in text- similarity
    tasks, we investigate the effect of the different resulting text representations
    on this similarity-based text classification strategy. In this paper, we use [SimCSE](https://aclanthology.org/2021.emnlp-main.552/)
    and [SBERT](https://aclanthology.org/D19-1410/) transformer- models to create
    text representations. In the following, this approach is referred to as **Lbl2TransformerVec**.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，**我们调整了** [**Lbl2Vec**](https://medium.com/towards-data-science/unsupervised-text-classification-with-lbl2vec-6c5e040354de)
    **方法，使用基于变换器的文本表示**，而不是[Doc2Vec](https://radimrehurek.com/gensim/models/doc2vec.html)，来创建联合嵌入的词、文档和标签表示。由于基于变换器的文本表示目前在文本相似度任务中实现了最先进的结果，我们研究了不同的文本表示对这种基于相似度的文本分类策略的影响。在本文中，我们使用[SimCSE](https://aclanthology.org/2021.emnlp-main.552/)和[SBERT](https://aclanthology.org/D19-1410/)变换器模型来创建文本表示。接下来，这种方法被称为**Lbl2TransformerVec**。
- en: 0️⃣ 🔫 Zero-shot text classification using the entailment approach
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 0️⃣ 🔫 使用蕴含方法的零样本文本分类
- en: Although newer zero-shot text classification (0SHOT-TC) approaches exist, **the**
    [**zero-shot entailment approach**](https://aclanthology.org/D19-1404/) **still
    produces state-of-the-art 0SHOT-TC results in predicting instances of unseen classes**
    compared to models of similar size. As the name already implies, the zero-shot
    entailment approach deals with 0SHOT-TC as a textual entailment problem. The underlying
    idea is similar to that of similarity-based text classification approaches. Conventional
    0SHOT-TC classifiers fail to understand the actual problem since the label names
    are usually converted into simple indices. Therefore, these classifiers can hardly
    generalize from seen to unseen classes. **Considering 0SHOT-TC as an entailment
    problem provides the classifier with a textual label description and therefore
    enables it to understand the meaning of labels.**
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管存在更新的零样本文本分类（0SHOT-TC）方法，**[**零样本蕴含方法**](https://aclanthology.org/D19-1404/)**仍然在预测未见类别的实例时，与类似大小的模型相比，能够产生最先进的0SHOT-TC结果。正如名称所暗示的，零样本蕴含方法将0SHOT-TC视为一个文本蕴含问题。其基本思想类似于基于相似性的文本分类方法。传统的0SHOT-TC分类器无法理解实际问题，因为标签名称通常被转换为简单的索引。因此，这些分类器很难从已见类别推广到未见类别。**将0SHOT-TC视为一个蕴含问题为分类器提供了文本标签描述，因此使其能够理解标签的含义。**
- en: 0️⃣ 🔫 Zero-shot text classification using TARS
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 0️⃣ 🔫 使用TARS进行零样本文本分类
- en: '[TARS](https://aclanthology.org/2020.coling-main.285/) also uses the textual
    label description to classify text in a zero-shot setting. However, TARS approaches
    the task as a binary classification problem, where a text and a textual label
    description is given to the model, which makes a prediction about whether that
    label is true or not. The TARS authors state that this approach significantly
    outperforms [GPT-2](https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf)
    in 0SHOT-TC'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '[TARS](https://aclanthology.org/2020.coling-main.285/)也使用文本标签描述来在零样本设置中进行文本分类。然而，TARS将任务视为一个二分类问题，其中给定一个文本和一个文本标签描述，模型对该标签是否真实进行预测。TARS的作者表示，这种方法在0SHOT-TC中显著优于[GPT-2](https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf)'
- en: 📐 Baselines
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 📐 基准
- en: We compare the findings of current state-of-the-art unsupervised text classification
    approaches to some basic baselines to evaluate their performance.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将当前最先进的无监督文本分类方法的发现与一些基本基准进行比较，以评估它们的性能。
- en: '[**LSA:**](https://asistdl.onlinelibrary.wiley.com/doi/abs/10.1002/%28SICI%291097-4571%28199009%2941%3A6%3C391%3A%3AAID-ASI1%3E3.0.CO%3B2-9)For
    each dataset, we apply LSA to learn 𝑛 = number of classes concepts. Afterwards,
    the text documents are classified according to the highest cosine similarity of
    resulting LSA vectors of documents and label keywords.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '[**LSA:**](https://asistdl.onlinelibrary.wiley.com/doi/abs/10.1002/%28SICI%291097-4571%28199009%2941%3A6%3C391%3A%3AAID-ASI1%3E3.0.CO%3B2-9)对于每个数据集，我们应用LSA来学习𝑛
    = 类别数概念。随后，根据文档和标签关键词的LSA向量的最高余弦相似度对文本文件进行分类。'
- en: '[**Word2Vec:**](https://papers.nips.cc/paper/2013/hash/9aa42b31882ec039965f3c4923ce901b-Abstract.html)This
    produces semantic vector representations of words based on surrounding context
    words. The average of word embeddings is used to represent the text documents
    and label keywords. The text documents are predicted according to the highest
    cosine similarity of the resulting Word2Vec representations of documents and label
    keywords for classification.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '[**Word2Vec:**](https://papers.nips.cc/paper/2013/hash/9aa42b31882ec039965f3c4923ce901b-Abstract.html)这根据周围的上下文词生成词的语义向量表示。词嵌入的平均值用于表示文本文件和标签关键词。文本文件根据生成的Word2Vec文档和标签关键词的表示的最高余弦相似度进行预测和分类。'
- en: '[**SimCSE:**](https://aclanthology.org/2021.emnlp-main.552/)This is a contrastive
    learning framework that produces sentence embeddings which acieve state-of-the-art
    results in semantic similarity tasks. We use, SimCSE document embeddings and SimCSE
    label keyword embeddings as class representations. Finally, the text documents
    are classified according to the highest cosine similarity of the resulting SimCSE
    representations of document and label keywords.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '[**SimCSE:**](https://aclanthology.org/2021.emnlp-main.552/)这是一个对比学习框架，它生成句子嵌入，在语义相似性任务中取得了最先进的结果。我们使用SimCSE文档嵌入和SimCSE标签关键词嵌入作为类别表示。最后，文本文件根据生成的SimCSE文档和标签关键词的表示的最高余弦相似度进行分类。'
- en: '[**SBERT:**](https://aclanthology.org/D19-1410/)This is a modification of [BERT](https://aclanthology.org/N19-1423/)
    that uses siamese and triplet network structures to derive semantically meaningful
    sentence embeddings. We use the same classification approach as with SimCSE, except
    that we now use SBERT embeddings instead of SimCSE embeddings.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '[**SBERT:**](https://aclanthology.org/D19-1410/)这是一种对[BERT](https://aclanthology.org/N19-1423/)的修改，使用了siamese和triplet网络结构来推导具有语义意义的句子嵌入。我们使用与SimCSE相同的分类方法，只不过这次使用SBERT嵌入而不是SimCSE嵌入。'
- en: 🔬 Experiments
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 🔬 实验
- en: For our SimCSE experiments, we use the [sup-simcse-roberta-large](https://huggingface.co/princeton-nlp/sup-simcse-roberta-large)
    model. To create embeddings with SBERT, we use two different pretrained SBERT
    models. We choose the general purpose models [all-mpnet- base-v2](https://huggingface.co/sentence-transformers/all-mpnet-base-v2)
    and [all-MiniLM-L6-v2](https://huggingface.co/sentence-transformers/all-mpnet-base-v2),
    trained on more than one billion training pairs and expected to perform well on
    sentence similarity tasks. The all-mpnet-base-v2 model is larger than the all-MiniLM-
    L6-v2 model and guarantees slightly better quality sentence embeddings. The smaller
    all-MiniLM-L6-v2 model, on the other hand, guarantees a five times faster encoding
    time while still providing sentence embeddings of high quality.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的SimCSE实验，我们使用[**sup-simcse-roberta-large**](https://huggingface.co/princeton-nlp/sup-simcse-roberta-large)模型。为了使用SBERT创建嵌入，我们使用了两个不同的预训练SBERT模型。我们选择了通用模型[all-mpnet-base-v2](https://huggingface.co/sentence-transformers/all-mpnet-base-v2)和[all-MiniLM-L6-v2](https://huggingface.co/sentence-transformers/all-mpnet-base-v2)，这些模型在超过十亿对训练样本上进行了训练，预计在句子相似度任务上表现良好。all-mpnet-base-v2模型比all-MiniLM-L6-v2模型大，保证了稍微更好的质量句子嵌入。而较小的all-MiniLM-L6-v2模型则保证了五倍的编码速度，同时仍能提供高质量的句子嵌入。
- en: 'For evaluation of 0SHOT-TC, we conduct experiments with three different pretrained
    zero-shot entailment models: a [DeBERTa](https://huggingface.co/MoritzLaurer/DeBERTa-v3-base-mnli-fever-docnli-ling-2c)
    model, a large [BART](https://huggingface.co/facebook/bart-large-mnli) model,
    and a smaller [DistilBERT](https://huggingface.co/typeform/distilbert-base-uncased-mnli)
    model. For TARS experiments, we use the BERT-based pretrained [tars-base-v8](https://flair.informatik.hu-berlin.de/resources/models/tars-base/)
    model.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估0SHOT-TC，我们进行了三种不同预训练的零样本蕴含模型的实验：一个[DeBERTa](https://huggingface.co/MoritzLaurer/DeBERTa-v3-base-mnli-fever-docnli-ling-2c)模型，一个大型[BART](https://huggingface.co/facebook/bart-large-mnli)模型，以及一个较小的[DistilBERT](https://huggingface.co/typeform/distilbert-base-uncased-mnli)模型。对于TARS实验，我们使用基于BERT的预训练[tars-base-v8](https://flair.informatik.hu-berlin.de/resources/models/tars-base/)模型。
- en: 💾 Data
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 💾 数据
- en: Our evaluation is based on the four publicly available text classification datasets,
    [*20Newsgroups*](http://qwone.com/~jason/20Newsgroups), [*AG‘s Corpus*](http://groups.di.unipi.it/~gulli/AG_corpus_of_news_articles)
    , [*Yahoo! Answers*](https://huggingface.co/datasets/yahoo_answers_topics), and
    [*Medical Abstracts*](https://github.com/sebischair/Medical-Abstracts-TC-Corpus)
    from different domains. As we use the semantic meaning of class descriptions for
    unsupervised text classification, we infer label keywords from each class name
    that serves the purpose of textual class descriptions. Thereby, the inference
    step simply consists of using the class names provided by the official documentation
    of the datasets as label keywords. In a few cases, we additionally substituted
    the class names with synonymous or semantically similar keywords, if we considered
    this to be a more appropriate description of a certain class.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的评估基于四个公开可用的文本分类数据集，[*20Newsgroups*](http://qwone.com/~jason/20Newsgroups)，[*AG‘s
    Corpus*](http://groups.di.unipi.it/~gulli/AG_corpus_of_news_articles)，[*Yahoo!
    Answers*](https://huggingface.co/datasets/yahoo_answers_topics)，以及[*Medical Abstracts*](https://github.com/sebischair/Medical-Abstracts-TC-Corpus)，这些数据集来自不同领域。由于我们使用类别描述的语义意义进行无监督文本分类，我们从每个类别名称推断标签关键词，这些关键词作为文本类别描述的目的。因此，推断步骤仅仅是使用数据集官方文档提供的类别名称作为标签关键词。在少数情况下，如果我们认为这是对某个类别更合适的描述，我们还用同义词或语义相似的关键词替换了类别名称。
- en: '![](../Images/ae318fb6f9a50f1d5430f36fef35eda6.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ae318fb6f9a50f1d5430f36fef35eda6.png)'
- en: Overview of the used text classification datasets.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 使用的文本分类数据集概述。
- en: 📊 Evaluation Results
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 📊 评估结果
- en: '![](../Images/f94c4acf04ac7ef43f10e5be515fca65.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f94c4acf04ac7ef43f10e5be515fca65.png)'
- en: F1-scores (micro) of examined text classification approaches on different datasets.
    The best results on the respective dataset are displayed in bold. Since we use
    micro-averaging to calculate our classification metrics, we realize equal F1,
    Precision, and Recall scores respectively.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在不同数据集上检查的文本分类方法的F1分数（micro）。各数据集上最佳结果以**粗体**显示。由于我们使用微平均法来计算分类指标，我们可以实现F1、Precision和Recall的相等分数。
- en: We can observe that none of the baselines achieves the highest F1-scores on
    any dataset based on these data. This indicates that the use of advanced unsupervised
    text classification approaches usually yields better results than simple baseline
    approaches. However, we observe that the **LSA and Word2Vec approaches** generally
    **yield the worst results and** **are easy to outperform** 👎. In contrast, the
    **SimCSE and SBERT baselines produce strong F1-scores** 💪 that even some of the
    advanced approaches could not surpass in certain cases. Furthermore, the SimCSE
    and SBERT baseline approaches may produce better results than the Lbl2Vec similarity-based
    approach on three datasets. We nevertheless can deduce that **the use of advanced
    similarity-based approaches generally produces better unsupervised text classification
    results than the use of simple baseline approaches or 0SHOT-TC**. Specifically,
    the **Lbl2TransformerVec approaches using SBERT embeddings appear to be promising,
    as they consistently perform well across all datasets** 🏆 and outperform the baseline
    results. In contrast, the **0SHOT-TC approaches perform consistently weak** and
    in the majority of cases did not even manage to outperform the baseline results
    😲. However, the DeBERTa zero-shot entailment model could classify the domain-specific
    medical abstracts surprisingly well and achieved the best F1-scores of all classifiers
    on this dataset. We observe, that the large DeBERTa zero-shot entailment model
    always significantly outperforms the smaller BART-large and DistilBERT zero-shot
    entailment models. Additionally, the BERT-based TARS model performs slightly better
    than the smaller DistilBERT zero-shot entailment model, except in case of the
    domain-specific Medical Abstracts dataset. Thus, we conclude that **the performance
    of the 0SHOT-TC approaches improves with increasing model size** 🤔.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以观察到，根据这些数据，没有任何基线方法在任何数据集上获得最高的F1分数。这表明，使用先进的无监督文本分类方法通常能获得比简单基线方法更好的结果。然而，我们观察到**LSA和Word2Vec方法**通常**效果最差，并且**
    **容易被超越** 👎。相比之下，**SimCSE和SBERT基线方法产生了强劲的F1分数** 💪，甚至在某些情况下连一些先进方法也未能超越。此外，SimCSE和SBERT基线方法在三个数据集上的表现可能优于Lbl2Vec基于相似性的方法。我们可以推断出，**使用先进的基于相似性的方法通常能比使用简单基线方法或0SHOT-TC获得更好的无监督文本分类结果**。具体来说，**使用SBERT嵌入的Lbl2TransformerVec方法表现出色，因为它们在所有数据集上都表现良好**
    🏆，并超越了基线结果。相比之下，**0SHOT-TC方法表现持续较差**，在大多数情况下甚至未能超越基线结果 😲。然而，DeBERTa零-shot蕴涵模型能够令人惊讶地很好地分类领域特定的医学摘要，并在该数据集上取得了所有分类器中的最佳F1分数。我们观察到，大型DeBERTa零-shot蕴涵模型总是显著优于较小的BART-large和DistilBERT零-shot蕴涵模型。此外，基于BERT的TARS模型的表现略优于较小的DistilBERT零-shot蕴涵模型，除非在领域特定的医学摘要数据集上。因此，我们得出结论，**0SHOT-TC方法的表现随着模型大小的增加而提高**
    🤔。
- en: 💡Conclusion
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 💡结论
- en: '**Similarity-based TC approaches generally outperform 0SHOT-TC** approaches
    in a variety of different domains.'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基于相似性的文本分类方法通常在各种不同领域中优于0SHOT-TC**方法。'
- en: The characteristics of text embeddings enable representations of similar topics
    or classes to be located close to each other in embedding space. This implies
    that text representations which are capable of coherently clustering topics in
    the embedding space perform well when used in unsupervised text classification
    approaches. This characteristic is also evident in our work and can be seen in
    the figure below 👇.
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文本嵌入的特性使得相似的主题或类别在嵌入空间中靠得很近。这意味着能够在嵌入空间中连贯地聚类主题的文本表示在无监督文本分类方法中表现良好。这一特性在我们的工作中也很明显，可以在下图中看到
    👇。
- en: '![](../Images/2b1581848e8ee0356bd3dbc312e68c7e.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2b1581848e8ee0356bd3dbc312e68c7e.png)'
- en: '[DensMAP](https://www.biorxiv.org/content/10.1101/2020.05.12.077776v1) visualizations
    of the document embeddings for each dataset. The document embeddings were created
    using SBERT (all-mpnet-base-v2).'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '[DensMAP](https://www.biorxiv.org/content/10.1101/2020.05.12.077776v1)的文档嵌入可视化。文档嵌入是使用SBERT（all-mpnet-base-v2）创建的。'
- en: Simple approaches such as **LSA or Word2Vec are easy to outperform** and therefore
    are **not recommended to be used as baselines** for text classification of unseen
    classes.
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 简单方法如**LSA或Word2Vec容易超越**，因此**不建议用作未知类文本分类的基线**。
- en: '**SimCSE and SBERT** **baseline approaches** **generate strong unsupervised
    text classification results**, outperforming even some more advanced classifiers.
    Therefore, we propose to **use SimCSE and SBERT baselines for evaluating unsupervised
    text classification** approaches and 0SHOT-TC performance on unseen classes in
    future work.'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**SimCSE和SBERT**的**基线方法****产生强大的无监督文本分类结果**，甚至超过了一些更高级的分类器。因此，我们建议**使用SimCSE和SBERT基线来评估未监督文本分类**方法和未知类0SHOT-TC在将来的性能。'
- en: '**Lbl2TransformerVec**, our proposed similarity-based text classification approach
    **yields best F1-scores** for almost all datasets.'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Lbl2TransformerVec**，我们提出的基于相似性的文本分类方法**获得了最佳的F1分数**几乎所有数据集。'
- en: We made our [Lbl2TransformerVec](https://github.com/sebischair/Lbl2Vec) code
    publicly available at [https://github.com/sebischair/Lbl2Vec](https://github.com/sebischair/Lbl2Vec).
    If you want to read more details about our approach, you can read our paper [here](https://arxiv.org/abs/2211.16285).
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将我们的[Lbl2TransformerVec](https://github.com/sebischair/Lbl2Vec)代码公开在[https://github.com/sebischair/Lbl2Vec](https://github.com/sebischair/Lbl2Vec)。如果您想阅读更多关于我们方法的详细信息，您可以在这里阅读我们的论文(https://arxiv.org/abs/2211.16285)。
- en: Sources
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Sources
- en: '[](https://arxiv.org/abs/2211.16285?source=post_page-----83115d9879f5--------------------------------)
    [## Evaluating Unsupervised Text Classification: Zero-shot and Similarity-based
    Approaches'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://arxiv.org/abs/2211.16285?source=post_page-----83115d9879f5--------------------------------)
    [## 评估无监督文本分类：零样本和基于相似性的方法'
- en: Text classification of unseen classes is a challenging Natural Language Processing
    task and is mainly attempted using…
  id: totrans-54
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 未知类的文本分类是一项具有挑战性的自然语言处理任务，主要是使用…
- en: 'arxiv.org](https://arxiv.org/abs/2211.16285?source=post_page-----83115d9879f5--------------------------------)
    [](https://arxiv.org/abs/2210.06023?source=post_page-----83115d9879f5--------------------------------)
    [## Lbl2Vec: An Embedding-Based Approach for Unsupervised Document Retrieval on
    Predefined Topics'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 'arxiv.org](https://arxiv.org/abs/2211.16285?source=post_page-----83115d9879f5--------------------------------)
    [](https://arxiv.org/abs/2210.06023?source=post_page-----83115d9879f5--------------------------------)
    [## Lbl2Vec: An Embedding-Based Approach for Unsupervised Document Retrieval on
    Predefined Topics'
- en: In this paper, we consider the task of retrieving documents with predefined
    topics from an unlabeled document dataset…
  id: totrans-56
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在本文中，我们考虑了从无标记文档数据集中检索具有预定义主题的文档的任务…
- en: arxiv.org](https://arxiv.org/abs/2210.06023?source=post_page-----83115d9879f5--------------------------------)
    [](/unsupervised-text-classification-with-lbl2vec-6c5e040354de?source=post_page-----83115d9879f5--------------------------------)
    [## Unsupervised Text Classification with Lbl2Vec
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: arxiv.org](https://arxiv.org/abs/2210.06023?source=post_page-----83115d9879f5--------------------------------)
    [](/unsupervised-text-classification-with-lbl2vec-6c5e040354de?source=post_page-----83115d9879f5--------------------------------)
    [## Unsupervised Text Classification with Lbl2Vec
- en: An introduction to embedding-based classification of unlabeled text documents
  id: totrans-58
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: An introduction to embedding-based classification of unlabeled text documents
- en: 'towardsdatascience.com](/unsupervised-text-classification-with-lbl2vec-6c5e040354de?source=post_page-----83115d9879f5--------------------------------)
    [](https://github.com/sebischair/Lbl2Vec?source=post_page-----83115d9879f5--------------------------------)
    [## GitHub - sebischair/Lbl2Vec: Lbl2Vec learns jointly embedded label, document
    and word vectors to…'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 'towardsdatascience.com](/unsupervised-text-classification-with-lbl2vec-6c5e040354de?source=post_page-----83115d9879f5--------------------------------)
    [](https://github.com/sebischair/Lbl2Vec?source=post_page-----83115d9879f5--------------------------------)
    [## GitHub - sebischair/Lbl2Vec: Lbl2Vec learns jointly embedded label, document
    and word vectors to…'
- en: Lbl2Vec learns jointly embedded label, document and word vectors to retrieve
    documents with predefined topics from an…
  id: totrans-60
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Lbl2Vec学习联合嵌入标签，文档和单词向量来检索具有预定义主题的文档…
- en: 'github.com](https://github.com/sebischair/Lbl2Vec?source=post_page-----83115d9879f5--------------------------------)
    [](https://aclanthology.org/D19-1410/?source=post_page-----83115d9879f5--------------------------------)
    [## Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 'github.com](https://github.com/sebischair/Lbl2Vec?source=post_page-----83115d9879f5--------------------------------)
    [](https://aclanthology.org/D19-1410/?source=post_page-----83115d9879f5--------------------------------)
    [## Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks'
- en: Abstract BERT (Devlin et al., 2018) and RoBERTa (Liu et al., 2019) has set a
    new state-of-the-art performance on…
  id: totrans-62
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Abstract BERT（Devlin et al., 2018）和RoBERTa（Liu et al., 2019）在新的最先进表现方面设立了新的标准性能…
- en: 'aclanthology.org](https://aclanthology.org/D19-1410/?source=post_page-----83115d9879f5--------------------------------)
    [](https://aclanthology.org/2021.emnlp-main.552/?source=post_page-----83115d9879f5--------------------------------)
    [## SimCSE: Simple Contrastive Learning of Sentence Embeddings'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: Tianyu Gao, Xingcheng Yao, Danqi Chen. Proceedings of the 2021 Conference on
    Empirical Methods in Natural Language…
  id: totrans-64
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'aclanthology.org](https://aclanthology.org/2021.emnlp-main.552/?source=post_page-----83115d9879f5--------------------------------)
    [](https://aclanthology.org/D19-1404/?source=post_page-----83115d9879f5--------------------------------)
    [## Benchmarking Zero-shot Text Classification: Datasets, Evaluation and Entailment
    Approach'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: Abstract Zero-shot text classification (0Shot-TC) is a challenging NLU problem
    to which little attention has been paid…
  id: totrans-66
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: aclanthology.org](https://aclanthology.org/D19-1404/?source=post_page-----83115d9879f5--------------------------------)
    [](https://aclanthology.org/2020.coling-main.285/?source=post_page-----83115d9879f5--------------------------------)
    [## Task-Aware Representation of Sentences for Generic Text Classification
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: Kishaloy Halder, Alan Akbik, Josip Krapac, Roland Vollgraf. Proceedings of the
    28th International Conference on…
  id: totrans-68
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: aclanthology.org](https://aclanthology.org/2020.coling-main.285/?source=post_page-----83115d9879f5--------------------------------)
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
