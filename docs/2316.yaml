- en: Attention from Alignment, Practically Explained
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/attention-from-alignment-practically-explained-548ef6588aa4?source=collection_archive---------2-----------------------#2023-07-19](https://towardsdatascience.com/attention-from-alignment-practically-explained-548ef6588aa4?source=collection_archive---------2-----------------------#2023-07-19)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Learn from what matters, Ignore what doesn’t.
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@danielwarfield1?source=post_page-----548ef6588aa4--------------------------------)[![Daniel
    Warfield](../Images/c1c8b4dd514f6813e08e401401324bca.png)](https://medium.com/@danielwarfield1?source=post_page-----548ef6588aa4--------------------------------)[](https://towardsdatascience.com/?source=post_page-----548ef6588aa4--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----548ef6588aa4--------------------------------)
    [Daniel Warfield](https://medium.com/@danielwarfield1?source=post_page-----548ef6588aa4--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fbdc4072cbfdc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fattention-from-alignment-practically-explained-548ef6588aa4&user=Daniel+Warfield&userId=bdc4072cbfdc&source=post_page-bdc4072cbfdc----548ef6588aa4---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----548ef6588aa4--------------------------------)
    ·11 min read·Jul 19, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F548ef6588aa4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fattention-from-alignment-practically-explained-548ef6588aa4&user=Daniel+Warfield&userId=bdc4072cbfdc&source=-----548ef6588aa4---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F548ef6588aa4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fattention-from-alignment-practically-explained-548ef6588aa4&source=-----548ef6588aa4---------------------bookmark_footer-----------)![](../Images/fddcbc3132d55c17e2ede623de6dbc4b.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Photo by [Armand Khoury](https://unsplash.com/@armand_khoury?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: Attention, as popularized by the landmark paper [Attention Is All You Need](https://arxiv.org/pdf/1706.03762.pdf)
    (2017), is arguably the most important architectural trend in machine learning
    right now. Originally intended for sequence to sequence modeling, attention has
    exploded into virtually every sub-discipline of machine learning.
  prefs: []
  type: TYPE_NORMAL
- en: This post will describe a particular flavor of attention which proceeded the
    transforner style of attention. We’ll discuss how it works, and why it’s useful.
    We’ll also go over some literature and a tutorial implementing this form of attention
    in PyTorch. By reading this post, you will have a more thorough understanding
    of attention as a general concept, which is useful in exploring more cutting edge
    applications.
  prefs: []
  type: TYPE_NORMAL
- en: The Reason For Attention
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The attention mechanism was originally popularized in [Neural Machine Translation
    by Jointly Learning to Align and Translate](https://arxiv.org/pdf/1409.0473v7.pdf)(2014),
    which is the guiding reference for this particular post. This paper employs an
    encoder-decoder architecture for english-to-french translation.
  prefs: []
  type: TYPE_NORMAL
