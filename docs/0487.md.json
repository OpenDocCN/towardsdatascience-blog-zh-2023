["```py\nimport sympy as sm\nimport numpy as np\n\ndef get_gradient(\n    function: sm.core.expr.Expr,\n    symbols: list[sm.core.symbol.Symbol],\n    x0: dict[sm.core.symbol.Symbol, float],\n) -> np.ndarray:\n    \"\"\"\n    Calculate the gradient of a function at a given point.\n\n    Args:\n        function (sm.core.expr.Expr): The function to calculate the gradient of.\n        symbols (list[sm.core.symbol.Symbol]): The symbols representing the variables in the function.\n        x0 (dict[sm.core.symbol.Symbol, float]): The point at which to calculate the gradient.\n\n    Returns:\n        numpy.ndarray: The gradient of the function at the given point.\n    \"\"\"\n    d1 = {}\n    gradient = np.array([])\n\n    for i in symbols:\n        d1[i] = sm.diff(function, i, 1).evalf(subs=x0)\n        gradient = np.append(gradient, d1[i])\n\n    return gradient.astype(np.float64)\n\ndef get_hessian(\n    function: sm.core.expr.Expr,\n    symbols: list[sm.core.symbol.Symbol],\n    x0: dict[sm.core.symbol.Symbol, float],\n) -> np.ndarray:\n    \"\"\"\n    Calculate the Hessian matrix of a function at a given point.\n\n    Args:\n    function (sm.core.expr.Expr): The function for which the Hessian matrix is calculated.\n    symbols (list[sm.core.symbol.Symbol]): The list of symbols used in the function.\n    x0 (dict[sm.core.symbol.Symbol, float]): The point at which the Hessian matrix is evaluated.\n\n    Returns:\n    numpy.ndarray: The Hessian matrix of the function at the given point.\n    \"\"\"\n    d2 = {}\n    hessian = np.array([])\n\n    for i in symbols:\n        for j in symbols:\n            d2[f\"{i}{j}\"] = sm.diff(function, i, j).evalf(subs=x0)\n            hessian = np.append(hessian, d2[f\"{i}{j}\"])\n\n    hessian = np.array(np.array_split(hessian, len(symbols)))\n\n    return hessian.astype(np.float64)\n\ndef newton_method(\n    function: sm.core.expr.Expr,\n    symbols: list[sm.core.symbol.Symbol],\n    x0: dict[sm.core.symbol.Symbol, float],\n    iterations: int = 100,\n) -> dict[sm.core.symbol.Symbol, float] or None:\n    \"\"\"\n    Perform Newton's method to find the solution to the optimization problem.\n\n    Args:\n        function (sm.core.expr.Expr): The objective function to be optimized.\n        symbols (list[sm.core.symbol.Symbol]): The symbols used in the objective function.\n        x0 (dict[sm.core.symbol.Symbol, float]): The initial values for the symbols.\n        iterations (int, optional): The maximum number of iterations. Defaults to 100.\n\n    Returns:\n        dict[sm.core.symbol.Symbol, float] or None: The solution to the optimization problem, or None if no solution is found.\n    \"\"\"\n\n    x_star = {}\n    x_star[0] = np.array(list(x0.values()))\n\n    # x = [] ## Return x for visual!\n\n    print(f\"Starting Values: {x_star[0]}\")\n\n    for i in range(iterations):\n        # x.append(dict(zip(x0.keys(),x_star[i]))) ## Return x for visual!\n\n        gradient = get_gradient(function, symbols, dict(zip(x0.keys(), x_star[i])))\n        hessian = get_hessian(function, symbols, dict(zip(x0.keys(), x_star[i])))\n\n        x_star[i + 1] = x_star[i].T - np.linalg.inv(hessian) @ gradient.T\n\n        if np.linalg.norm(x_star[i + 1] - x_star[i]) < 10e-5:\n            solution = dict(zip(x0.keys(), x_star[i + 1]))\n            print(f\"\\nConvergence Achieved ({i+1} iterations): Solution = {solution}\")\n            break\n        else:\n            solution = None\n\n        print(f\"Step {i+1}: {x_star[i+1]}\")\n\n    return solution\n```", "```py\nimport sympy as sm\n\n# Define Symbols\nx, y = sm.symbols('x y') \nGamma = [x,y] \n\n# Define Objective Function (Rosenbrock's Parabolic Valley)\nobjective = 100*(y-x**2)**2 + (1-x)**2\n\n# Specify starting values\nGamma0 = {x:-1.2,y:1}\n\n# Call function\nnewton_method(objective, Gamma, Gamma0)\n```", "```py\nimport sympy as sm\n\nx, y, λ  = sm.symbols('x y λ')\n\nLangrangian_objective = 100*(y-x**2)**2 + (1-x)**2 + λ*(x**2-y-2)\nGamma = [x,y,λ]\nGamma0 = {x:-1.2,y:1,λ:1}\n\nnewton_method(Langrangian_objective,Gamma,Gamma0)\n```", "```py\nimport sympy as sm\nimport numpy as np\n\ndef constrained_newton_method(\n    function: sm.core.expr.Expr,\n    symbols: list[sm.core.symbol.Symbol],\n    x0: dict[sm.core.symbol.Symbol, float],\n    iterations: int = 100,\n) -> dict[sm.core.symbol.Symbol, float] or None:\n    \"\"\"\n    Performs constrained Newton's method to find the optimal solution of a function subject to constraints.\n\n    Parameters:\n        function (sm.core.expr.Expr): The function to optimize.\n        symbols (list[sm.core.symbol.Symbol]): The symbols used in the function.\n        x0 (dict[sm.core.symbol.Symbol, float]): The initial values for the symbols.\n        iterations (int, optional): The maximum number of iterations. Defaults to 100.\n\n    Returns:\n        dict[sm.core.symbol.Symbol, float] or None: The optimal solution if convergence is achieved, otherwise None.\n    \"\"\"\n    x_star = {}\n    x_star[0] = np.array(list(x0.values())[:-1])\n\n    optimal_solutions = []\n    optimal_solutions.append(dict(zip(list(x0.keys())[:-1], x_star[0])))\n\n    for step in range(iterations):\n        # Evaluate function at rho value\n        if step == 0:  # starting rho\n            rho_sub = list(x0.values())[-1]\n\n        rho_sub_values = {list(x0.keys())[-1]: rho_sub}\n        function_eval = function.evalf(subs=rho_sub_values)\n\n        print(f\"Step {step} w/ {rho_sub_values}\")  # Barrier method step\n        print(f\"Starting Values: {x_star[0]}\")\n\n        # Newton's Method\n        for i in range(iterations):\n            gradient = get_gradient(\n                function_eval, symbols[:-1], dict(zip(list(x0.keys())[:-1], x_star[i]))\n            )\n            hessian = get_hessian(\n                function_eval, symbols[:-1], dict(zip(list(x0.keys())[:-1], x_star[i]))\n            )\n\n            x_star[i + 1] = x_star[i].T - np.linalg.inv(hessian) @ gradient.T\n\n            if np.linalg.norm(x_star[i + 1] - x_star[i]) < 10e-5:\n                solution = dict(zip(list(x0.keys())[:-1], x_star[i + 1]))\n                print(\n                    f\"Convergence Achieved ({i+1} iterations): Solution = {solution}\\n\"\n                )\n                break\n\n        # Record optimal solution & previous optimal solution for each barrier method iteration\n        optimal_solution = x_star[i + 1]\n        previous_optimal_solution = list(optimal_solutions[step - 1].values())\n        optimal_solutions.append(dict(zip(list(x0.keys())[:-1], optimal_solution)))\n\n        # Check for overall convergence\n        if np.linalg.norm(optimal_solution - previous_optimal_solution) < 10e-5:\n            print(\n                f\"\\n Overall Convergence Achieved ({step} steps): Solution = {optimal_solutions[step]}\\n\"\n            )\n            overall_solution = optimal_solutions[step]\n            break\n        else:\n            overall_solution = None\n\n        # Set new starting point\n        x_star = {}\n        x_star[0] = optimal_solution\n\n        # Update rho\n        rho_sub = 0.9 * rho_sub\n\n    return overall_solution\n```", "```py\nimport sympy as sm\n\nx, y, ρ = sm.symbols('x y ρ')\n\nBarrier_objective = 100*(y-x**2)**2 + (1-x)**2 - ρ*sm.log((-x)*(y-3))\nGamma = [x,y,ρ] # Function requires last symbol to be ρ!\nGamma0 = {x:-15,y:15,ρ:10}\n\nconstrained_newton_method(Barrier_objective,Gamma,Gamma0)\n```", "```py\nimport sympy as sm\n\nx, y, λ, ρ = sm.symbols('x y λ ρ')\n\ncombined_objective = 100*(y-x**2)**2 + (1-x)**2 + λ*(x**2-y-2) - ρ*sm.log((-x)*(y-3))\nGamma = [x,y,λ,ρ] # Function requires last symbol to be ρ!\nGamma0 = {x:-15,y:15,λ:0,ρ:10}\n\nconstrained_newton_method(combined_objective,Gamma,Gamma0)\n```"]