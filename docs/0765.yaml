- en: 'Unsupervised data pruning: less data to learn better'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/unsupervised-data-pruning-less-data-to-learn-better-30cd2bfbd855?source=collection_archive---------7-----------------------#2023-02-27](https://towardsdatascience.com/unsupervised-data-pruning-less-data-to-learn-better-30cd2bfbd855?source=collection_archive---------7-----------------------#2023-02-27)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Foundation models | Scaling law | Large models | Data pruning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Not always more data is meaning a more accurate model, but how to choose your
    data?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://salvatore-raieli.medium.com/?source=post_page-----30cd2bfbd855--------------------------------)[![Salvatore
    Raieli](../Images/6bb4520e2df40d20283e7283141b5e06.png)](https://salvatore-raieli.medium.com/?source=post_page-----30cd2bfbd855--------------------------------)[](https://towardsdatascience.com/?source=post_page-----30cd2bfbd855--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----30cd2bfbd855--------------------------------)
    [Salvatore Raieli](https://salvatore-raieli.medium.com/?source=post_page-----30cd2bfbd855--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ff1a08d9452cd&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funsupervised-data-pruning-less-data-to-learn-better-30cd2bfbd855&user=Salvatore+Raieli&userId=f1a08d9452cd&source=post_page-f1a08d9452cd----30cd2bfbd855---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----30cd2bfbd855--------------------------------)
    ·11 min read·Feb 27, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F30cd2bfbd855&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funsupervised-data-pruning-less-data-to-learn-better-30cd2bfbd855&user=Salvatore+Raieli&userId=f1a08d9452cd&source=-----30cd2bfbd855---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F30cd2bfbd855&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funsupervised-data-pruning-less-data-to-learn-better-30cd2bfbd855&source=-----30cd2bfbd855---------------------bookmark_footer-----------)![](../Images/1c41d0d7b79c96998daaf3dda4a30237.png)'
  prefs: []
  type: TYPE_NORMAL
- en: image by the author using [DALL-E](https://openai.com/dall-e-2/)
  prefs: []
  type: TYPE_NORMAL
- en: Scaling law has been observed in different contexts (pictures, text, language,
    speech, and so on). Is increasing the number of parameters really the only recipe
    for a better model? And if not what you can actually do?
  prefs: []
  type: TYPE_NORMAL
- en: What is scaling law and why is it problematic
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In recent years we have seen the number of parameters in models increase by
    leaps and bounds. All the biggest companies have been pushing to create more and
    more capable models. This has led to a reduced error in benchmark datasets and
    the emergence of unanticipated behavior. **But what is scaling law?**
  prefs: []
  type: TYPE_NORMAL
- en: '**In short, the scaling law states that the “test error often falls off as
    a power law with either the amount of training data, model size, or compute.”**
    Put another way, to improve the performance of a model one must increase one of
    these three factors: the number of examples during training, the number of parameters,
    or the duration of training.'
  prefs: []
  type: TYPE_NORMAL
