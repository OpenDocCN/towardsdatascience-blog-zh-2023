["```py\nclass SimpleNN(nn.Module):\n    def __init__(\n        self,\n        num_layers: int = 1,\n        num_neurons: int = 5,\n    ) -> None:\n        \"\"\"Basic neural network architecture with linear layers\n\n        Args:\n            num_layers (int, optional): number of hidden layers\n            num_neurons (int, optional): neurons for each hidden layer\n        \"\"\"\n        super().__init__()\n\n        layers = []\n\n        # input layer\n        layers.append(nn.Linear(1, num_neurons))\n\n        # hidden layers with linear layer and activation\n        for _ in range(num_layers):\n            layers.extend([nn.Linear(num_neurons, num_neurons), nn.Tanh()])\n\n        # output layer\n        layers.append(nn.Linear(num_neurons, 1))\n\n        # build the network\n        self.network = nn.Sequential(*layers)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        return self.network(x.reshape(-1, 1)).squeeze()\n```", "```py\nimport torch\n\nx = torch.randn(10)\nmodel = SimpleNN() # constructed above\noptimizer = torch.optim.SGD(model.parameters())\n\n# modify the state of the model\n# by applying a single optimization step\nout1 = model(x)\nmodel.backward()\noptimizer.step()\n\n# recompute the output with exactly the same input\nout2 = model(x)\nassert not torch.equal(out1, out2)\n```", "```py\nimport torch\nfrom torch.func import functional_call\n\nx = torch.randn(10) # random input data\nmodel = SimpleNN() # constructed above\nparams = dict(model.named_parameters()) # model parameters\n\n# make a functional call to the model above\nout = functional_call(model, params, (x,))\n```", "```py\nfrom torch.func import grad\n\n# the `grad` function returns another function\n# which takes the same inputs as the model forward pass\ngrad_fn = grad(model)\n\n# now this function can be used to compute gradients\n# with respect to the first input\nparams = tuple(model.parameters())\ngrad_values = grad_fn(x[0], params)\n```", "```py\nfrom torch.func import grad, vmap\n\nx = torch.rand(10)\n\n# combine twice `grad` with `vmap` to compute\n# the model second order derivative (Laplacian) with\n# respect to batched input data\nlaplacian_fn = vmap(grad(grad(model)))\nparams = tuple(model.parameters())\nout = laplacian_fn(x, params)\n```", "```py\nimport torch\nfrom torch.func import functional_call, grad\n\nx = torch.randn(1) # random input data point\nmodel = SimpleNN() # constructed above\n\n# forward pass using the functional API\n# to take the parameters as input arguments\ndef make_functional_fwd(_model):\n    def fn(data, parameters):\n        return functional_call(_model, parameters, (data,))\n    return fn\n\nmodel_func = make_functional_fwd(my_model) # functional forward\nparams = tuple(my_model.parameters()) # model parameters\n\n# the `argnums` argument allows to select with\n# respect to which input argument of the functional forward\n# pass defined in the closure\ngrad_params = grad(model_func, argnums=1)(x[0], params)\n\n# as before but for computing the gradient with\n# respect to the input data\ngrad_x = grad(model_func, argnums=0)(x[0], params)\n```", "```py\nimport torch\n\ndef get_data(n_points = 20):\n  x = torch.rand(n_points) * 2.0 * torch.pi\n  y = 2.0 * torch.sin(x + 2.0 * torch.pi)\n  return x, y\n\nx_train, y_train = get_data(n_points=40)\nx_test, y_test = get_data(n_points=10)\n```", "```py\nimport torch\nimport torchopt\n\n# hyperparameters and optimizer choice from `torchopt`\nnum_epochs = 500\nlr = 0.01\noptimizer = torchopt.FuncOptimizer(torchopt.adam(lr=lr))\nloss_fn = torch.nn.MSELoss()\n\nloss_evolution = []  # track the loss evolution per epoch\nparams = tuple(model.parameters())  # initialize the parameters\n\nfor i in range(num_epochs):\n\n  # update the parameters using the functional API\n  y = model_func(x_train, params)\n  loss = loss_fn(y, y_train)\n  params = optimizer.step(loss, params)\n  loss_evolution.append(float(loss))\n\n  if i % 100 == 0:\n      print(f\"Iteration {i} with loss {float(loss)}\")\n\n# accuracy on test set\ny_pred = model_func(x_test, params)\nprint(f\"Loss on the test set: {loss_fn(y_pred, y_test)}\")\n```"]