- en: Deploying Cohere Language Models On Amazon SageMaker
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/deploying-cohere-language-models-on-amazon-sagemaker-23a3f79639b1?source=collection_archive---------10-----------------------#2023-05-18](https://towardsdatascience.com/deploying-cohere-language-models-on-amazon-sagemaker-23a3f79639b1?source=collection_archive---------10-----------------------#2023-05-18)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Scale and Host LLMs on AWS
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://ram-vegiraju.medium.com/?source=post_page-----23a3f79639b1--------------------------------)[![Ram
    Vegiraju](../Images/07d9334e905f710d9f3c6187cf69a1a5.png)](https://ram-vegiraju.medium.com/?source=post_page-----23a3f79639b1--------------------------------)[](https://towardsdatascience.com/?source=post_page-----23a3f79639b1--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----23a3f79639b1--------------------------------)
    [Ram Vegiraju](https://ram-vegiraju.medium.com/?source=post_page-----23a3f79639b1--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F6e49569edd2b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeploying-cohere-language-models-on-amazon-sagemaker-23a3f79639b1&user=Ram+Vegiraju&userId=6e49569edd2b&source=post_page-6e49569edd2b----23a3f79639b1---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----23a3f79639b1--------------------------------)
    ·7 min read·May 18, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F23a3f79639b1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeploying-cohere-language-models-on-amazon-sagemaker-23a3f79639b1&user=Ram+Vegiraju&userId=6e49569edd2b&source=-----23a3f79639b1---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F23a3f79639b1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeploying-cohere-language-models-on-amazon-sagemaker-23a3f79639b1&source=-----23a3f79639b1---------------------bookmark_footer-----------)![](../Images/57c4cb0d9ebd119ae5f3cd5986778694.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Image from [Unsplash](https://unsplash.com/photos/EgwhIBec0Ck) by [Sigmund](https://unsplash.com/@sigmund)
  prefs: []
  type: TYPE_NORMAL
- en: Large Language Models (LLMs) and Generative AI are accelerating Machine Learning
    growth across various industries. With LLMs the scope for Machine Learning has
    increased to incredible heights, but has also been accompanied with a new set
    of challenges.
  prefs: []
  type: TYPE_NORMAL
- en: The size of LLMs lead to difficult problems in both the Training and Hosting
    portions of the ML lifecycle. Specifically for Hosting LLMs there are a myriad
    of challenges to consider. How can we fit a model into a singular GPU for inference?
    How can we apply model compression and partitioning techniques without compromising
    on accuracy? How can we improve inference latency and throughput for these LLMs?
  prefs: []
  type: TYPE_NORMAL
- en: To be able to address many of these questions requires advanced ML Engineering
    where we have to orchestrate model hosting on a platform that can apply compression
    and parallelization techniques at a container and hardware level. There are solutions
    such as [DJL Serving](https://github.com/deepjavalibrary/djl-serving/tree/master)
    that provide [containers](https://aws.amazon.com/blogs/machine-learning/deploy-bloom-176b-and-opt-30b-on-amazon-sagemaker-with-large-model-inference-deep-learning-containers-and-deepspeed/)
    tuned for LLM hosting, but we will not explore them in this article.
  prefs: []
  type: TYPE_NORMAL
- en: In this article, we’ll explore [SageMaker JumpStart Foundational Models](https://aws.amazon.com/sagemaker/jumpstart/getting-started/?sagemaker-jumpstart-cards.sort-by=item.additionalFields.priority&sagemaker-jumpstart-cards.sort-order=asc&awsf.sagemaker-jumpstart-filter-product-type=*all&awsf.sagemaker-jumpstart-filter-text=*all&awsf.sagemaker-jumpstart-filter-vision=*all&awsf.sagemaker-jumpstart-filter-tabular=*all&awsf.sagemaker-jumpstart-filter-audio-tasks=*all&awsf.sagemaker-jumpstart-filter-multimodal=*all&awsf.sagemaker-jumpstart-filter-RL=*all).
    With Foundational Models we don’t worry about containers or model parallelization
    and compression techniques, but focus primarily on directly deploying a…
  prefs: []
  type: TYPE_NORMAL
