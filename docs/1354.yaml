- en: The Map Of Transformers
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å˜æ¢å™¨çš„åœ°å›¾
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/the-map-of-transformers-e14952226398?source=collection_archive---------4-----------------------#2023-04-18](https://towardsdatascience.com/the-map-of-transformers-e14952226398?source=collection_archive---------4-----------------------#2023-04-18)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://towardsdatascience.com/the-map-of-transformers-e14952226398?source=collection_archive---------4-----------------------#2023-04-18](https://towardsdatascience.com/the-map-of-transformers-e14952226398?source=collection_archive---------4-----------------------#2023-04-18)
- en: Transformers
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å˜æ¢å™¨
- en: A broad overview of Transformers research
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å˜æ¢å™¨ç ”ç©¶çš„å¹¿æ³›æ¦‚è¿°
- en: '[](https://soran-ghaderi.medium.com/?source=post_page-----e14952226398--------------------------------)[![Soran
    Ghaderi](../Images/49d2b0022c2962d8ad7af5017383374f.png)](https://soran-ghaderi.medium.com/?source=post_page-----e14952226398--------------------------------)[](https://towardsdatascience.com/?source=post_page-----e14952226398--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----e14952226398--------------------------------)
    [Soran Ghaderi](https://soran-ghaderi.medium.com/?source=post_page-----e14952226398--------------------------------)'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://soran-ghaderi.medium.com/?source=post_page-----e14952226398--------------------------------)[![Soran
    Ghaderi](../Images/49d2b0022c2962d8ad7af5017383374f.png)](https://soran-ghaderi.medium.com/?source=post_page-----e14952226398--------------------------------)[](https://towardsdatascience.com/?source=post_page-----e14952226398--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----e14952226398--------------------------------)
    [Soran Ghaderi](https://soran-ghaderi.medium.com/?source=post_page-----e14952226398--------------------------------)'
- en: Â·
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: Â·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fd2b75b0bb761&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-map-of-transformers-e14952226398&user=Soran+Ghaderi&userId=d2b75b0bb761&source=post_page-d2b75b0bb761----e14952226398---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----e14952226398--------------------------------)
    Â·25 min readÂ·Apr 18, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fe14952226398&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-map-of-transformers-e14952226398&user=Soran+Ghaderi&userId=d2b75b0bb761&source=-----e14952226398---------------------clap_footer-----------)'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '[å…³æ³¨](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fd2b75b0bb761&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-map-of-transformers-e14952226398&user=Soran+Ghaderi&userId=d2b75b0bb761&source=post_page-d2b75b0bb761----e14952226398---------------------post_header-----------)
    åœ¨ [Towards Data Science](https://towardsdatascience.com/?source=post_page-----e14952226398--------------------------------)
    ä¸Šå‘è¡¨ Â· 25 åˆ†é’Ÿé˜…è¯» Â· 2023å¹´4æœˆ18æ—¥ [](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fe14952226398&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-map-of-transformers-e14952226398&user=Soran+Ghaderi&userId=d2b75b0bb761&source=-----e14952226398---------------------clap_footer-----------)'
- en: --
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fe14952226398&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-map-of-transformers-e14952226398&source=-----e14952226398---------------------bookmark_footer-----------)![](../Images/4c821282032da4ddc63dc6024fcab1a0.png)'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fe14952226398&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-map-of-transformers-e14952226398&source=-----e14952226398---------------------bookmark_footer-----------)![](../Images/4c821282032da4ddc63dc6024fcab1a0.png)'
- en: Fig. 1\. Isometric map. Designed by [vectorpocket / Freepik](http://www.freepik.com).
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 1\. ç­‰è·åœ°å›¾ã€‚ç”± [vectorpocket / Freepik](http://www.freepik.com) è®¾è®¡ã€‚
- en: 1\. Introduction
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 1\. ä»‹ç»
- en: 'The pace of research in deep learning has accelerated significantly in recent
    years, making it increasingly difficult to keep abreast of all the latest developments.
    Despite this, there is a particular direction of investigation that has garnered
    significant attention due to its demonstrated success across a diverse range of
    domains, including natural language processing, computer vision, and audio processing.
    This is due in large part to its highly adaptable architecture. The model is called
    Transformer, and it makes use of an array of mechanisms and techniques in the
    field (i.e., attention mechanisms). You can read more about the building blocks
    and their implementation along with multiple illustrations in the following articles:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: è¿‘å¹´æ¥ï¼Œæ·±åº¦å­¦ä¹ çš„ç ”ç©¶è¿›å±•æ˜¾è‘—åŠ å¿«ï¼Œä½¿å¾—è·Ÿè¸ªæ‰€æœ‰æœ€æ–°å‘å±•çš„éš¾åº¦è¶Šæ¥è¶Šå¤§ã€‚å°½ç®¡å¦‚æ­¤ï¼Œæœ‰ä¸€ä¸ªç‰¹å®šçš„ç ”ç©¶æ–¹å‘ç”±äºå…¶åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ã€è®¡ç®—æœºè§†è§‰å’ŒéŸ³é¢‘å¤„ç†ç­‰å¤šä¸ªé¢†åŸŸçš„æ˜¾è‘—æˆåŠŸï¼Œå—åˆ°äº†å¹¿æ³›å…³æ³¨ã€‚è¿™åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šå½’åŠŸäºå…¶é«˜åº¦é€‚åº”çš„æ¶æ„ã€‚è¿™ä¸ªæ¨¡å‹è¢«ç§°ä¸ºTransformerï¼Œå®ƒåˆ©ç”¨äº†è¯¥é¢†åŸŸçš„ä¸€ç³»åˆ—æœºåˆ¶å’ŒæŠ€æœ¯ï¼ˆå³æ³¨æ„åŠ›æœºåˆ¶ï¼‰ã€‚ä½ å¯ä»¥åœ¨ä»¥ä¸‹æ–‡ç« ä¸­æ·±å…¥äº†è§£è¿™äº›æ„å»ºå—åŠå…¶å®ç°ï¼Œå¹¶æŸ¥çœ‹å¤šä¸ªæ’å›¾ï¼š
- en: '[](/transformers-in-action-attention-is-all-you-need-ac10338a023a?source=post_page-----e14952226398--------------------------------)
    [## Transformers in Action: Attention Is All You Need'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/transformers-in-action-attention-is-all-you-need-ac10338a023a?source=post_page-----e14952226398--------------------------------)
    [## Transformerçš„åº”ç”¨ï¼šæ³¨æ„åŠ›å³æ˜¯å…¨éƒ¨'
- en: A brief survey, illustration, and implementation
  id: totrans-13
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ç®€è¦è°ƒæŸ¥ã€æ’å›¾å’Œå®ç°
- en: towardsdatascience.com](/transformers-in-action-attention-is-all-you-need-ac10338a023a?source=post_page-----e14952226398--------------------------------)
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '[towardsdatascience.com](/transformers-in-action-attention-is-all-you-need-ac10338a023a?source=post_page-----e14952226398--------------------------------)'
- en: 'This article provides more details about the attention mechanisms that I will
    be talking about throughout this article:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ç¯‡æ–‡ç« æä¾›äº†æ›´å¤šå…³äºæ³¨æ„åŠ›æœºåˆ¶çš„ç»†èŠ‚ï¼Œæˆ‘å°†åœ¨æœ¬æ–‡ä¸­è®¨è®ºè¿™äº›æœºåˆ¶ï¼š
- en: '[](/rethinking-thinking-how-do-attention-mechanisms-actually-work-a6f67d313f99?source=post_page-----e14952226398--------------------------------)
    [## Rethinking Thinking: How Do Attention Mechanisms Actually Work?'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/rethinking-thinking-how-do-attention-mechanisms-actually-work-a6f67d313f99?source=post_page-----e14952226398--------------------------------)
    [## é‡æ–°æ€è€ƒï¼šæ³¨æ„åŠ›æœºåˆ¶ç©¶ç«Ÿæ˜¯å¦‚ä½•å·¥ä½œçš„ï¼Ÿ'
- en: The brain, the mathematics, and DL â€” research frontiers
  id: totrans-17
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: å¤§è„‘ã€æ•°å­¦ä¸æ·±åº¦å­¦ä¹ â€”â€”ç ”ç©¶å‰æ²¿
- en: towardsdatascience.com](/rethinking-thinking-how-do-attention-mechanisms-actually-work-a6f67d313f99?source=post_page-----e14952226398--------------------------------)
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '[towardsdatascience.com](/rethinking-thinking-how-do-attention-mechanisms-actually-work-a6f67d313f99?source=post_page-----e14952226398--------------------------------)'
- en: 2\. Taxonomy of the Transformers
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 2\. Transformerçš„åˆ†ç±»
- en: 'A comprehensive range of models has been explored based on the vanilla Transformer
    to date, which can broadly be broken down into three categories:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ°ç›®å‰ä¸ºæ­¢ï¼ŒåŸºäºåŸå§‹Transformerå·²ç»æ¢ç´¢äº†å¹¿æ³›çš„æ¨¡å‹ï¼Œè¿™äº›æ¨¡å‹å¤§è‡´å¯ä»¥åˆ†ä¸ºä¸‰ç±»ï¼š
- en: Architectural modifications
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ¶æ„ä¿®æ”¹
- en: Pretraining methods
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: é¢„è®­ç»ƒæ–¹æ³•
- en: Applications
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åº”ç”¨
- en: '![](../Images/7491b188970493b11bc9aac813510f92.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7491b188970493b11bc9aac813510f92.png)'
- en: Fig. 2\. Transformer variants modifications. Photo by [author](https://github.com/soran-ghaderi).
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾2. Transformerå˜ä½“ä¿®æ”¹ã€‚ç…§ç‰‡ç”±[ä½œè€…](https://github.com/soran-ghaderi)æä¾›ã€‚
- en: Each category above contains several other sub-categories, which I will investigate
    thoroughly in the next sections. Fig. 2\. illustrates the categories researchers
    have modified Transformers.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸Šè¿°æ¯ä¸ªç±»åˆ«åŒ…å«å‡ ä¸ªå­ç±»åˆ«ï¼Œæˆ‘å°†åœ¨æ¥ä¸‹æ¥çš„éƒ¨åˆ†ä¸­æ·±å…¥æ¢è®¨ã€‚å›¾2å±•ç¤ºäº†ç ”ç©¶äººå‘˜å¯¹Transformerçš„ä¿®æ”¹ç±»åˆ«ã€‚
- en: 3\. Attention
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 3\. æ³¨æ„åŠ›
- en: Self-attention plays an elemental role in Transformer, although, it suffers
    from two main disadvantages in practice [1].
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: è‡ªæ³¨æ„åŠ›åœ¨Transformerä¸­å‘æŒ¥äº†åŸºæœ¬ä½œç”¨ï¼Œä½†åœ¨å®è·µä¸­å­˜åœ¨ä¸¤ä¸ªä¸»è¦ç¼ºç‚¹[1]ã€‚
- en: '**Complexity**: As for long sequences, this module turns into a bottleneck
    since its computational complexity is O(TÂ²Â·D).'
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**å¤æ‚æ€§**ï¼šå¯¹äºé•¿åºåˆ—ï¼Œè¿™ä¸ªæ¨¡å—å˜æˆäº†ç“¶é¢ˆï¼Œå› ä¸ºå…¶è®¡ç®—å¤æ‚åº¦ä¸ºO(TÂ²Â·D)ã€‚'
- en: '**Structural prior:** It does not tackle the structural bias of the inputs
    and requires additional mechanisms to be injected into the training data which
    later it can learn (i.e. learning the order information of the input sequences).'
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**ç»“æ„å…ˆéªŒ**ï¼šå®ƒæ²¡æœ‰å¤„ç†è¾“å…¥çš„ç»“æ„åå·®ï¼Œéœ€è¦åœ¨è®­ç»ƒæ•°æ®ä¸­æ³¨å…¥é¢å¤–çš„æœºåˆ¶ï¼Œæ¨¡å‹æ‰èƒ½åç»­å­¦ä¹ ï¼ˆä¾‹å¦‚å­¦ä¹ è¾“å…¥åºåˆ—çš„é¡ºåºä¿¡æ¯ï¼‰ã€‚'
- en: '![](../Images/56cdb5c915c769faa741c72e1dd844bc.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/56cdb5c915c769faa741c72e1dd844bc.png)'
- en: Fig. 3\. Categories of attention modifications and example papers. Photo by
    [author](https://www.linkedin.com/in/soran-ghaderi/).
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾3. æ³¨æ„åŠ›ä¿®æ”¹ç±»åˆ«åŠç¤ºä¾‹è®ºæ–‡ã€‚ç…§ç‰‡ç”±[ä½œè€…](https://www.linkedin.com/in/soran-ghaderi/)æä¾›ã€‚
- en: Therefore, researchers have explored various techniques to overcome these drawbacks.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼Œç ”ç©¶äººå‘˜æ¢ç´¢äº†å„ç§æŠ€æœ¯æ¥å…‹æœè¿™äº›ç¼ºé™·ã€‚
- en: '**Sparse attention:** This technique tries to lower the computation time and
    the memory requirements of the attention mechanism by taking a smaller portion
    of the inputs into account instead of the entire input sequence, producing a sparse
    matrix in contrast to a full matrix.'
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**ç¨€ç–æ³¨æ„åŠ›ï¼š** è¯¥æŠ€æœ¯é€šè¿‡åªè€ƒè™‘è¾“å…¥åºåˆ—çš„ä¸€å°éƒ¨åˆ†è€Œä¸æ˜¯æ•´ä¸ªè¾“å…¥åºåˆ—æ¥é™ä½æ³¨æ„åŠ›æœºåˆ¶çš„è®¡ç®—æ—¶é—´å’Œå†…å­˜éœ€æ±‚ï¼Œä»è€Œäº§ç”Ÿç¨€ç–çŸ©é˜µï¼Œç›¸è¾ƒäºå…¨çŸ©é˜µã€‚'
- en: '**Linearized attention:** Disentangling the attention matrix using kernel feature
    maps, this method tries to compute the attention in the reverse order to reduce
    the resource requirements to linear complexity.'
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**çº¿æ€§åŒ–æ³¨æ„åŠ›ï¼š** é€šè¿‡ä½¿ç”¨æ ¸ç‰¹å¾æ˜ å°„è§£æ„æ³¨æ„åŠ›çŸ©é˜µï¼Œè¿™ç§æ–¹æ³•å°è¯•ä»¥åå‘é¡ºåºè®¡ç®—æ³¨æ„åŠ›ï¼Œä»è€Œå°†èµ„æºéœ€æ±‚é™ä½åˆ°çº¿æ€§å¤æ‚åº¦ã€‚'
- en: '**Prototype and memory compression:** This line of modification tries to decrease
    the queries and key-value pairs to achieve a smaller attention matrix which in
    turn reduces the time and computational complexity.'
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**åŸå‹å’Œå†…å­˜å‹ç¼©ï¼š** è¿™ç§ä¿®æ”¹çº¿è¯•å›¾å‡å°‘æŸ¥è¯¢å’Œé”®-å€¼å¯¹ï¼Œä»¥å®ç°è¾ƒå°çš„æ³¨æ„åŠ›çŸ©é˜µï¼Œä»è€Œå‡å°‘æ—¶é—´å’Œè®¡ç®—å¤æ‚åº¦ã€‚'
- en: '**Low-rank self-attention:** By explicitly modeling the low-rank property of
    the self-attention matrix using parameterization or replacing it with a low-rank
    approximation tries to improve the performance of the transformer.'
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**ä½ç§©è‡ªæ³¨æ„åŠ›ï¼š** é€šè¿‡æ˜¾å¼å»ºæ¨¡è‡ªæ³¨æ„åŠ›çŸ©é˜µçš„ä½ç§©ç‰¹æ€§ï¼Œä½¿ç”¨å‚æ•°åŒ–æˆ–ç”¨ä½ç§©è¿‘ä¼¼æ›¿ä»£ï¼Œä»¥æœŸæé«˜å˜æ¢å™¨çš„æ€§èƒ½ã€‚'
- en: '**Attention with prior:** Leveraging the prior attention distribution from
    other sources, this approach, combines other attention distributions with the
    one obtained from the inputs.'
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**å¸¦æœ‰å…ˆéªŒçš„æ³¨æ„åŠ›ï¼š** åˆ©ç”¨æ¥è‡ªå…¶ä»–æ¥æºçš„å…ˆéªŒæ³¨æ„åŠ›åˆ†å¸ƒï¼Œè¿™ç§æ–¹æ³•å°†å…¶ä»–æ³¨æ„åŠ›åˆ†å¸ƒä¸ä»è¾“å…¥ä¸­è·å¾—çš„æ³¨æ„åŠ›åˆ†å¸ƒç»“åˆèµ·æ¥ã€‚'
- en: '**Modified multi-head mechanism:** There are various ways to modify and improve
    the performance of the multi-head mechanism which can be categorized under this
    research direction.'
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**ä¿®æ”¹çš„å¤šå¤´æœºåˆ¶ï¼š** æœ‰å¤šç§æ–¹æ³•å¯ä»¥ä¿®æ”¹å’Œæé«˜å¤šå¤´æœºåˆ¶çš„æ€§èƒ½ï¼Œè¿™äº›æ–¹æ³•å¯ä»¥å½’ç±»äºè¿™ä¸€ç ”ç©¶æ–¹å‘ã€‚'
- en: 3.1\. Sparse attention
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.1. ç¨€ç–æ³¨æ„åŠ›
- en: The standard self-attention mechanism in a transformer requires every token
    to attend to all other tokens. However, it has been observed that in many cases,
    the attention matrix is often very sparse, meaning that only a small number of
    tokens actually attend to each other [2]. This suggests that it is possible to
    reduce the computational complexity of the self-attention mechanism by limiting
    the number of query-key pairs that each query attends to. By only computing the
    similarity scores for pre-defined patterns of query-key pairs, it is possible
    to significantly reduce the amount of computation required without sacrificing
    performance.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: å˜æ¢å™¨ä¸­çš„æ ‡å‡†è‡ªæ³¨æ„åŠ›æœºåˆ¶è¦æ±‚æ¯ä¸ªæ ‡è®°éƒ½å…³æ³¨æ‰€æœ‰å…¶ä»–æ ‡è®°ã€‚ç„¶è€Œï¼Œå·²ç»è§‚å¯Ÿåˆ°åœ¨è®¸å¤šæƒ…å†µä¸‹ï¼Œæ³¨æ„åŠ›çŸ©é˜µé€šå¸¸éå¸¸ç¨€ç–ï¼Œè¿™æ„å‘³ç€åªæœ‰å°‘é‡æ ‡è®°å®é™…ä¸Šå½¼æ­¤å…³æ³¨[2]ã€‚è¿™è¡¨æ˜ï¼Œå¯ä»¥é€šè¿‡é™åˆ¶æ¯ä¸ªæŸ¥è¯¢å…³æ³¨çš„æŸ¥è¯¢-é”®å¯¹çš„æ•°é‡æ¥å‡å°‘è‡ªæ³¨æ„åŠ›æœºåˆ¶çš„è®¡ç®—å¤æ‚åº¦ã€‚é€šè¿‡ä»…è®¡ç®—é¢„å®šä¹‰æ¨¡å¼çš„æŸ¥è¯¢-é”®å¯¹çš„ç›¸ä¼¼æ€§åˆ†æ•°ï¼Œå¯ä»¥åœ¨ä¸ç‰ºç‰²æ€§èƒ½çš„æƒ…å†µä¸‹æ˜¾è‘—å‡å°‘æ‰€éœ€çš„è®¡ç®—é‡ã€‚
- en: '![](../Images/91697d5a6d8416a50cbc58e164a206d9.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/91697d5a6d8416a50cbc58e164a206d9.png)'
- en: Eq. 1
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: Eq. 1
- en: In the un-normalized attention matrix Ã‚, the -âˆ items are not typically stored
    in memory in order to reduce the memory footprint. This is done to decrease the
    amount of memory required to implement the matrix, which can improve the efficiency
    and performance of the system.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨éè§„èŒƒåŒ–çš„æ³¨æ„åŠ›çŸ©é˜µÃ‚ä¸­ï¼Œ-âˆé¡¹é€šå¸¸ä¸ä¼šå­˜å‚¨åœ¨å†…å­˜ä¸­ï¼Œä»¥å‡å°‘å†…å­˜å ç”¨ã€‚è¿™æ˜¯ä¸ºäº†é™ä½å®ç°çŸ©é˜µæ‰€éœ€çš„å†…å­˜é‡ï¼Œä»è€Œæé«˜ç³»ç»Ÿçš„æ•ˆç‡å’Œæ€§èƒ½ã€‚
- en: We can map the attention matrix to a bipartite graph where the standard attention
    mechanism can be thought of as a complete bipartite graph, where each query receives
    information from all of the nodes in the memory and uses this information to update
    its representation. In this way, the attention mechanism allows each query to
    attend to all of the other nodes in the memory and incorporate their information
    into its representation. This allows the model to capture complex relationships
    and dependencies between the nodes in the memory. The sparse attention mechanism,
    on the other hand, can be thought of as a sparse graph. This means that not all
    of the nodes in the graph are connected, which can reduce the computational complexity
    of the system and improve its efficiency and performance. By limiting the number
    of connections between nodes, the sparse attention mechanism can still capture
    important relationships and dependencies, but with less computational overhead.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥å°†æ³¨æ„åŠ›çŸ©é˜µæ˜ å°„åˆ°ä¸€ä¸ªäºŒåˆ†å›¾ï¼Œå…¶ä¸­æ ‡å‡†æ³¨æ„åŠ›æœºåˆ¶å¯ä»¥è¢«è®¤ä¸ºæ˜¯ä¸€ä¸ªå®Œæ•´çš„äºŒåˆ†å›¾ï¼Œæ¯ä¸ªæŸ¥è¯¢ä»è®°å¿†ä¸­çš„æ‰€æœ‰èŠ‚ç‚¹æ¥æ”¶ä¿¡æ¯ï¼Œå¹¶åˆ©ç”¨è¿™äº›ä¿¡æ¯æ›´æ–°å…¶è¡¨ç¤ºã€‚è¿™æ ·ï¼Œæ³¨æ„åŠ›æœºåˆ¶å…è®¸æ¯ä¸ªæŸ¥è¯¢å…³æ³¨è®°å¿†ä¸­çš„æ‰€æœ‰å…¶ä»–èŠ‚ç‚¹ï¼Œå¹¶å°†å®ƒä»¬çš„ä¿¡æ¯çº³å…¥å…¶è¡¨ç¤ºä¸­ã€‚è¿™ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿæ•æ‰è®°å¿†èŠ‚ç‚¹ä¹‹é—´å¤æ‚çš„å…³ç³»å’Œä¾èµ–æ€§ã€‚å¦ä¸€æ–¹é¢ï¼Œç¨€ç–æ³¨æ„åŠ›æœºåˆ¶å¯ä»¥è¢«è®¤ä¸ºæ˜¯ä¸€ä¸ªç¨€ç–å›¾ã€‚è¿™æ„å‘³ç€å›¾ä¸­çš„å¹¶éæ‰€æœ‰èŠ‚ç‚¹éƒ½ç›¸äº’è¿æ¥ï¼Œè¿™å¯ä»¥å‡å°‘ç³»ç»Ÿçš„è®¡ç®—å¤æ‚æ€§ï¼Œæé«˜å…¶æ•ˆç‡å’Œæ€§èƒ½ã€‚é€šè¿‡é™åˆ¶èŠ‚ç‚¹ä¹‹é—´çš„è¿æ¥æ•°é‡ï¼Œç¨€ç–æ³¨æ„åŠ›æœºåˆ¶ä»ç„¶å¯ä»¥æ•æ‰é‡è¦çš„å…³ç³»å’Œä¾èµ–æ€§ï¼Œä½†è®¡ç®—å¼€é”€è¾ƒå°ã€‚
- en: There are two main classes of approaches to sparse attention, based on the metrics
    used to determine the sparse connections between nodes [1]. These are **position-based**
    and **content-based** sparse attention.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: ç¨€ç–æ³¨æ„åŠ›æ–¹æ³•ä¸»è¦æœ‰ä¸¤ç±»ï¼ŒåŸºäºç”¨äºç¡®å®šèŠ‚ç‚¹ä¹‹é—´ç¨€ç–è¿æ¥çš„æŒ‡æ ‡[1]ã€‚è¿™ä¸¤ç±»æ˜¯**åŸºäºä½ç½®**å’Œ**åŸºäºå†…å®¹**çš„ç¨€ç–æ³¨æ„åŠ›ã€‚
- en: 3.1.1\. Position-based sparse attention
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.1.1\. åŸºäºä½ç½®çš„ç¨€ç–æ³¨æ„åŠ›
- en: In this type of attention, the connections in the attention matrix are limited
    according to predetermined patterns. They can be expressed as combinations of
    simpler patterns, which can be useful for understanding and analyzing the behavior
    of the attention mechanism.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ç§ç±»å‹çš„æ³¨æ„åŠ›ä¸­ï¼Œæ³¨æ„åŠ›çŸ©é˜µä¸­çš„è¿æ¥æ˜¯æ ¹æ®é¢„å®šæ¨¡å¼è¿›è¡Œé™åˆ¶çš„ã€‚å®ƒä»¬å¯ä»¥è¡¨ç¤ºä¸ºæ›´ç®€å•æ¨¡å¼çš„ç»„åˆï¼Œè¿™å¯¹ç†è§£å’Œåˆ†ææ³¨æ„åŠ›æœºåˆ¶çš„è¡Œä¸ºéå¸¸æœ‰ç”¨ã€‚
- en: '![](../Images/654f80182028882725b4412ae9ed3394.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/654f80182028882725b4412ae9ed3394.png)'
- en: Fig. 4\. Main atomic sparse attention patterns. The colored squares demonstrate
    correspondent calculated attention scores. Image from [[1]](https://arxiv.org/abs/2106.04554v2).
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 4\. ä¸»è¦çš„åŸå­ç¨€ç–æ³¨æ„åŠ›æ¨¡å¼ã€‚å½©è‰²æ–¹å—å±•ç¤ºäº†ç›¸åº”çš„è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°ã€‚å›¾ç‰‡æ¥æºäº[[1]](https://arxiv.org/abs/2106.04554v2)ã€‚
- en: '**3.1.1.1\. Atomic sparse attention:** There are five basic atomic sparse attention
    patterns that can be used to construct a variety of different sparse attention
    mechanisms that have different trade-offs between computational complexity and
    performance as shown in Fig. 4.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '**3.1.1.1\. åŸå­ç¨€ç–æ³¨æ„åŠ›ï¼š** æœ‰äº”ç§åŸºæœ¬çš„åŸå­ç¨€ç–æ³¨æ„åŠ›æ¨¡å¼å¯ä»¥ç”¨æ¥æ„å»ºå„ç§ä¸åŒçš„ç¨€ç–æ³¨æ„åŠ›æœºåˆ¶ï¼Œè¿™äº›æœºåˆ¶åœ¨è®¡ç®—å¤æ‚æ€§å’Œæ€§èƒ½ä¹‹é—´æœ‰ä¸åŒçš„æƒè¡¡ï¼Œå¦‚å›¾
    4 æ‰€ç¤ºã€‚'
- en: '**Global attention:** Global nodes can be used as an information hub across
    all other nodes that can attend to all other nodes in the sequence and vice versa
    as in Fig. 4 (a).'
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**å…¨å±€æ³¨æ„åŠ›ï¼š** å…¨å±€èŠ‚ç‚¹å¯ä»¥ä½œä¸ºä¿¡æ¯ä¸­å¿ƒï¼Œèƒ½å¤Ÿå…³æ³¨åºåˆ—ä¸­çš„æ‰€æœ‰å…¶ä»–èŠ‚ç‚¹ï¼Œåä¹‹äº¦ç„¶ï¼Œå¦‚å›¾ 4 (a) æ‰€ç¤ºã€‚'
- en: '**Band attention (also sliding window attention or local attention):** The
    relationships and dependencies between different parts of the data are often local
    rather than global. In the band attention, the attention matrix is a band matrix,
    with the queries only attending to a certain number of neighboring nodes on either
    side as shown in Fig. 4 (b).'
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**å¸¦çŠ¶æ³¨æ„åŠ›ï¼ˆä¹Ÿç§°ä¸ºæ»‘åŠ¨çª—å£æ³¨æ„åŠ›æˆ–å±€éƒ¨æ³¨æ„åŠ›ï¼‰ï¼š** æ•°æ®ä¸åŒéƒ¨åˆ†ä¹‹é—´çš„å…³ç³»å’Œä¾èµ–æ€§é€šå¸¸æ˜¯å±€éƒ¨çš„è€Œéå…¨å±€çš„ã€‚åœ¨å¸¦çŠ¶æ³¨æ„åŠ›ä¸­ï¼Œæ³¨æ„åŠ›çŸ©é˜µæ˜¯ä¸€ä¸ªå¸¦çŠ¶çŸ©é˜µï¼ŒæŸ¥è¯¢ä»…å…³æ³¨ä¸¤ä¾§ä¸€å®šæ•°é‡çš„é‚»è¿‘èŠ‚ç‚¹ï¼Œå¦‚å›¾
    4 (b) æ‰€ç¤ºã€‚'
- en: '**Dilated attention:** Similar to how dilated convolutional neural networks
    (CNNs) can increase the receptive field without increasing computational complexity,
    it is possible to do the same with band attention by using a dilated window with
    gaps of dilation *w_d* >= 1, as shown in Fig. 4 (c). Also, it can be extended
    to strided attention where the dilation ğ‘¤ ğ‘‘ is assumed to be a large value.'
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**æ‰©å¼ æ³¨æ„åŠ›ï¼š** ç±»ä¼¼äºæ‰©å¼ å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNsï¼‰å¯ä»¥åœ¨ä¸å¢åŠ è®¡ç®—å¤æ‚æ€§çš„æƒ…å†µä¸‹æ‰©å¤§æ„Ÿå—é‡ï¼Œé€šè¿‡ä½¿ç”¨æ‰©å¼ çª—å£ï¼ˆæ‰©å¼ *ğ‘¤_d* >= 1ï¼‰å¯ä»¥å®ç°å¸¦çŠ¶æ³¨æ„åŠ›çš„ç±»ä¼¼æ•ˆæœï¼Œå¦‚å›¾
    4 (c) æ‰€ç¤ºã€‚æ­¤å¤–ï¼Œå®ƒä¹Ÿå¯ä»¥æ‰©å±•åˆ°æ­¥å¹…æ³¨æ„åŠ›ï¼Œå…¶ä¸­æ‰©å¼ ğ‘¤ ğ‘‘ è¢«è®¤ä¸ºæ˜¯ä¸€ä¸ªè¾ƒå¤§çš„å€¼ã€‚'
- en: '**Random attention:** To improve the ability of the attention mechanism to
    capture non-local interactions, a few edges can be randomly sampled for each query,
    as depicted in Fig. 4 (d).'
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**éšæœºæ³¨æ„åŠ›ï¼š** ä¸ºäº†æé«˜æ³¨æ„åŠ›æœºåˆ¶æ•æ‰éå±€éƒ¨äº¤äº’çš„èƒ½åŠ›ï¼Œå¯ä»¥ä¸ºæ¯ä¸ªæŸ¥è¯¢éšæœºé‡‡æ ·ä¸€äº›è¾¹ï¼Œå¦‚å›¾4(d)æ‰€ç¤ºã€‚'
- en: '**Block local attention:** The input sequence is segmented into several non-intersecting
    query blocks, each of which is associated with a local memory block. The queries
    within each query block only attend to the keys in the corresponding memory block,
    shown in 3(e).'
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**å—çº§å±€éƒ¨æ³¨æ„åŠ›ï¼š** è¾“å…¥åºåˆ—è¢«åˆ†å‰²æˆå‡ ä¸ªäº’ä¸äº¤å‰çš„æŸ¥è¯¢å—ï¼Œæ¯ä¸ªæŸ¥è¯¢å—éƒ½å…³è”ä¸€ä¸ªå±€éƒ¨å†…å­˜å—ã€‚æ¯ä¸ªæŸ¥è¯¢å—ä¸­çš„æŸ¥è¯¢ä»…å…³æ³¨ç›¸åº”å†…å­˜å—ä¸­çš„é”®ï¼Œå¦‚å›¾3(e)æ‰€ç¤ºã€‚'
- en: '**3.1.1.2\. Compound sparse attention:** As illustrated in Fig. 5, many existing
    sparse attention mechanisms are composed of more than one of the atomic patterns
    described above.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '**3.1.1.2\. å¤åˆç¨€ç–æ³¨æ„åŠ›ï¼š** å¦‚å›¾5æ‰€ç¤ºï¼Œè®¸å¤šç°æœ‰çš„ç¨€ç–æ³¨æ„åŠ›æœºåˆ¶ç”±ä¸Šè¿°æè¿°çš„å¤šä¸ªåŸå­æ¨¡å¼ç»„æˆã€‚'
- en: '![](../Images/fc2ed23f12fc57d7c3cda43bcedf436c.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fc2ed23f12fc57d7c3cda43bcedf436c.png)'
- en: Fig. 5\. Four different compound sparse attention patterns. Image from [[1]](https://arxiv.org/abs/2106.04554v2).
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾5\. å››ç§ä¸åŒçš„å¤åˆç¨€ç–æ³¨æ„åŠ›æ¨¡å¼ã€‚å›¾ç‰‡æ¥è‡ª[[1]](https://arxiv.org/abs/2106.04554v2)ã€‚
- en: '**3.1.1.3\. Extended sparse attention:** There are also other types of patterns
    that have been explored for specific data types. By way of example, BP-Transformer
    [3] uses a binary tree to capture a combination of global and local attention
    across the input sequence. Tokens are leaf nodes and the internal nodes are span
    nodes containing multiple tokens. Fig. 6 shows a number of extended sparse attention
    patterns.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '**3.1.1.3\. æ‰©å±•ç¨€ç–æ³¨æ„åŠ›ï¼š** è¿˜æœ‰å…¶ä»–ç±»å‹çš„æ¨¡å¼å·²è¢«æ¢ç´¢ç”¨äºç‰¹å®šæ•°æ®ç±»å‹ã€‚ä¾‹å¦‚ï¼ŒBP-Transformer [3] ä½¿ç”¨äºŒå‰æ ‘æ¥æ•æ‰è¾“å…¥åºåˆ—ä¸­å…¨å±€å’Œå±€éƒ¨æ³¨æ„åŠ›çš„ç»„åˆã€‚ä»¤ç‰Œæ˜¯å¶èŠ‚ç‚¹ï¼Œå†…éƒ¨èŠ‚ç‚¹æ˜¯åŒ…å«å¤šä¸ªä»¤ç‰Œçš„è·¨åº¦èŠ‚ç‚¹ã€‚å›¾6å±•ç¤ºäº†å¤šç§æ‰©å±•ç¨€ç–æ³¨æ„åŠ›æ¨¡å¼ã€‚'
- en: '![](../Images/33205e4eb8177520d005fdfc0cac64b5.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/33205e4eb8177520d005fdfc0cac64b5.png)'
- en: Fig. 6\. Different extended sparse attention patterns. Image from [[1]](https://arxiv.org/abs/2106.04554v2).
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾6\. ä¸åŒçš„æ‰©å±•ç¨€ç–æ³¨æ„åŠ›æ¨¡å¼ã€‚å›¾ç‰‡æ¥è‡ª[[1]](https://arxiv.org/abs/2106.04554v2)ã€‚
- en: 3.1.2\. Content-based sparse attention
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.1.2\. åŸºäºå†…å®¹çš„ç¨€ç–æ³¨æ„åŠ›
- en: In this approach, a sparse graph is constructed where the sparse connections
    are based on the inputs. It selects the keys that have high similarity scores
    with the given query. An efficient way to build this graph is to use Maximum Inner
    Product Search (MIPS) which finds the maximum dot-product between keys and the
    query without calculating all dot-products.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ç§æ–¹æ³•ä¸­ï¼Œæ„å»ºä¸€ä¸ªç¨€ç–å›¾ï¼Œå…¶ä¸­ç¨€ç–è¿æ¥åŸºäºè¾“å…¥ã€‚å®ƒé€‰æ‹©ä¸ç»™å®šæŸ¥è¯¢å…·æœ‰é«˜ç›¸ä¼¼åº¦çš„é”®ã€‚æ„å»ºæ­¤å›¾çš„é«˜æ•ˆæ–¹æ³•æ˜¯ä½¿ç”¨æœ€å¤§å†…ç§¯æœç´¢ï¼ˆMIPSï¼‰ï¼Œè¯¥æ–¹æ³•åœ¨ä¸è®¡ç®—æ‰€æœ‰ç‚¹ç§¯çš„æƒ…å†µä¸‹æ‰¾åˆ°é”®ä¸æŸ¥è¯¢ä¹‹é—´çš„æœ€å¤§ç‚¹ç§¯ã€‚
- en: '![](../Images/ef21f3839a71e0eaac3848b36f39baca.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ef21f3839a71e0eaac3848b36f39baca.png)'
- en: Fig. 7\. 2-D attention schemes for the Routing Transformer compared to local
    attention and strided attention. Image from [[4](https://arxiv.org/abs/2003.05997)]
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾7\. Routing Transformerçš„2-Dæ³¨æ„åŠ›æ–¹æ¡ˆï¼Œä¸å±€éƒ¨æ³¨æ„åŠ›å’Œè·¨æ­¥æ³¨æ„åŠ›ç›¸æ¯”ã€‚å›¾ç‰‡æ¥è‡ª[[4](https://arxiv.org/abs/2003.05997)]
- en: Routing Transformer [4] as shown in Fig. 7, equips the self-attention mechanism
    with a sparse routing module by using online k-means clustering to cluster keys
    and queries on the same centroid vectors. It isolates the queries to only attend
    keys within the same cluster. Reformer [5] uses locality-sensitive hashing (LSH)
    instead of dot-product attention to select keys and values for each query. It
    enables the queries to only attend to tokens within the same bucket which are
    derived from the queries and keys using LSH. Using the LSTM edge predictor, Sparse
    Adaptive Connection (SAC) [6] constructs a graph from the input sequence and achieves
    attention edges to enhance the tasks-specific performance by leveraging an adaptive
    sparse connection.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: Routing Transformer [4] å¦‚å›¾7æ‰€ç¤ºï¼Œé€šè¿‡ä½¿ç”¨åœ¨çº¿k-meansèšç±»å¯¹é”®å’Œå€¼è¿›è¡ŒåŒå¿ƒå¿ƒå‘é‡èšç±»ï¼Œä¸ºè‡ªæ³¨æ„åŠ›æœºåˆ¶é…å¤‡äº†ç¨€ç–è·¯ç”±æ¨¡å—ã€‚å®ƒå°†æŸ¥è¯¢éš”ç¦»ï¼Œåªå…³æ³¨åŒä¸€ç°‡å†…çš„é”®ã€‚Reformer
    [5] ä½¿ç”¨å±€éƒ¨æ•æ„Ÿå“ˆå¸Œï¼ˆLSHï¼‰ä»£æ›¿ç‚¹ç§¯æ³¨æ„åŠ›ï¼Œä¸ºæ¯ä¸ªæŸ¥è¯¢é€‰æ‹©é”®å’Œå€¼ã€‚å®ƒä½¿æŸ¥è¯¢ä»…å…³æ³¨æ¥è‡ªLSHç”Ÿæˆçš„åŒä¸€æ¡¶ä¸­çš„ä»¤ç‰Œã€‚ä½¿ç”¨LSTMè¾¹é¢„æµ‹å™¨ï¼Œç¨€ç–è‡ªé€‚åº”è¿æ¥ï¼ˆSACï¼‰
    [6] ä»è¾“å…¥åºåˆ—ä¸­æ„å»ºå›¾ï¼Œå¹¶é€šè¿‡åˆ©ç”¨è‡ªé€‚åº”ç¨€ç–è¿æ¥æ¥å¢å¼ºä»»åŠ¡ç‰¹å®šçš„æ€§èƒ½ã€‚
- en: 3.2\. Linearized attention
  id: totrans-68
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.2\. çº¿æ€§åŒ–æ³¨æ„åŠ›
- en: The computational complexity of the dot-product attention mechanism (softmax(QK^âŠ¤)V)
    increases quadratically with the spatiotemporal size (length) of the input. Therefore,
    it impedes its usage when exposed to large inputs such as videos, long sequences,
    or high-resolution images. By disentangling softmax(QK^âŠ¤) to Qâ€² Kâ€²^âŠ¤, the (Qâ€²
    Kâ€²^âŠ¤ V) can be computed in reverse order, resulting in a linear complexity O(ğ‘‡
    ).
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: ç‚¹ç§¯æ³¨æ„åŠ›æœºåˆ¶çš„è®¡ç®—å¤æ‚åº¦ï¼ˆsoftmax(QK^âŠ¤)Vï¼‰éšç€è¾“å…¥çš„ç©ºé—´æ—¶é—´å¤§å°ï¼ˆé•¿åº¦ï¼‰çš„å¢åŠ è€Œå‘ˆäºŒæ¬¡å¢åŠ ã€‚å› æ­¤ï¼Œå½“æš´éœ²äºå¤§è¾“å…¥ï¼ˆå¦‚è§†é¢‘ã€é•¿åºåˆ—æˆ–é«˜åˆ†è¾¨ç‡å›¾åƒï¼‰æ—¶ï¼Œå®ƒé˜»ç¢äº†å…¶ä½¿ç”¨ã€‚é€šè¿‡å°†
    softmax(QK^âŠ¤) è§£å¼€æˆ Qâ€² Kâ€²^âŠ¤ï¼Œ(Qâ€² Kâ€²^âŠ¤ V) å¯ä»¥æŒ‰ç›¸åé¡ºåºè®¡ç®—ï¼Œç»“æœæ˜¯çº¿æ€§å¤æ‚åº¦ O(ğ‘‡)ã€‚
- en: '![](../Images/4dc671f6875350317d9d0f7768061ba3.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4dc671f6875350317d9d0f7768061ba3.png)'
- en: Fig. 8\. Standard self-attention and linearized self-attention complexity difference.
    Image from [[1](https://arxiv.org/abs/2106.04554v2)].
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 8\. æ ‡å‡†è‡ªæ³¨æ„åŠ›å’Œçº¿æ€§åŒ–è‡ªæ³¨æ„åŠ›çš„å¤æ‚åº¦å·®å¼‚ã€‚å›¾ç‰‡æ¥è‡ª [[1](https://arxiv.org/abs/2106.04554v2)]ã€‚
- en: Assuming Ã‚ = exp(QK^âŠ¤) denotes an un-normalized attention matrix, where exp(.)
    is applied element-wise, Linearized attention is a technique that approximates
    the un-normalized attention matrix exp(QK^âŠ¤) with ğœ™(Q) ğœ™(K)^âŠ¤ where ğœ™ is a row-wise
    feature map. By applying this technique, we can do ğœ™(Q) (ğœ™(K)^âŠ¤ V) which is a
    linearized computation of an un-normalized attention matrix as illustrated in
    Fig. 8.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: å‡è®¾ Ã‚ = exp(QK^âŠ¤) è¡¨ç¤ºæœªå½’ä¸€åŒ–çš„æ³¨æ„åŠ›çŸ©é˜µï¼Œå…¶ä¸­ exp(.) é€å…ƒç´ åº”ç”¨ï¼Œçº¿æ€§åŒ–æ³¨æ„åŠ›æ˜¯ä¸€ç§è¿‘ä¼¼æœªå½’ä¸€åŒ–æ³¨æ„åŠ›çŸ©é˜µ exp(QK^âŠ¤)
    çš„æŠ€æœ¯ï¼Œå…¶å½¢å¼ä¸º ğœ™(Q) ğœ™(K)^âŠ¤ï¼Œå…¶ä¸­ ğœ™ æ˜¯é€è¡Œç‰¹å¾æ˜ å°„ã€‚é€šè¿‡åº”ç”¨è¿™ç§æŠ€æœ¯ï¼Œæˆ‘ä»¬å¯ä»¥æ‰§è¡Œ ğœ™(Q) (ğœ™(K)^âŠ¤ V)ï¼Œè¿™æ˜¯æœªå½’ä¸€åŒ–æ³¨æ„åŠ›çŸ©é˜µçš„çº¿æ€§åŒ–è®¡ç®—ï¼Œå¦‚å›¾
    8 æ‰€ç¤ºã€‚
- en: To achieve a deeper understanding of linearized attention, I will explore the
    formulation in vector form. I will examine the general form of attention in order
    to gain further insight.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†æ›´æ·±å…¥åœ°ç†è§£çº¿æ€§åŒ–æ³¨æ„åŠ›ï¼Œæˆ‘å°†æ¢ç´¢å‘é‡å½¢å¼çš„å…¬å¼ã€‚æˆ‘å°†æ£€æŸ¥æ³¨æ„åŠ›çš„ä¸€èˆ¬å½¢å¼ï¼Œä»¥è·å¾—è¿›ä¸€æ­¥çš„æ´è§ã€‚
- en: '![](../Images/a652c4b566db4a5e99dd1caa2935f3b5.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a652c4b566db4a5e99dd1caa2935f3b5.png)'
- en: Eq. 2
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: å…¬å¼ 2
- en: In this context, sim(Â·, Â·) is a scoring function that measures the similarity
    between input vectors. In the vanilla Transformer, the scoring function is the
    exponential of the inner product, exp(âŸ¨Â·, Â·âŸ©). A suitable selection for sim(Â·,
    Â·) is a kernel function, K(x, y) = ğœ™(x)ğœ™(y)^âŠ¤ , which leads to further insights
    into the linearized attention.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ä¸ªèƒŒæ™¯ä¸‹ï¼Œsim(Â·, Â·) æ˜¯è¡¡é‡è¾“å…¥å‘é‡ç›¸ä¼¼æ€§çš„è¯„åˆ†å‡½æ•°ã€‚åœ¨æ ‡å‡† Transformer ä¸­ï¼Œè¯„åˆ†å‡½æ•°æ˜¯å†…ç§¯çš„æŒ‡æ•°å½¢å¼ï¼Œexp(âŸ¨Â·, Â·âŸ©)ã€‚ä¸€ä¸ªé€‚åˆçš„é€‰æ‹©æ˜¯æ ¸å‡½æ•°
    sim(Â·, Â·) = K(x, y) = ğœ™(x)ğœ™(y)^âŠ¤ ï¼Œè¿™è¿›ä¸€æ­¥æ­ç¤ºäº†çº¿æ€§åŒ–æ³¨æ„åŠ›çš„æ´è§ã€‚
- en: '![](../Images/c3d9b54c2ac39325feb71330701caa5d.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c3d9b54c2ac39325feb71330701caa5d.png)'
- en: Eq. 3
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: å…¬å¼ 3
- en: in this formulation, the outer product of vectors is denoted by âŠ—. Attention
    can be linearized by first computing the highlighted terms which allow the autoregressive
    models i.e. transformer decoders to run like RNNs.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ä¸ªå…¬å¼ä¸­ï¼Œå‘é‡çš„å¤–ç§¯ç”¨ âŠ— è¡¨ç¤ºã€‚æ³¨æ„åŠ›å¯ä»¥é€šè¿‡é¦–å…ˆè®¡ç®—çªå‡ºæ˜¾ç¤ºçš„æœ¯è¯­æ¥çº¿æ€§åŒ–ï¼Œè¿™å…è®¸è‡ªå›å½’æ¨¡å‹ï¼Œå³ Transformer è§£ç å™¨åƒ RNN ä¸€æ ·è¿è¡Œã€‚
- en: Eq. 2 shows that it keeps a memory matrix by aggregating associations from outer
    products of (feature-mapped) keys and queries. It later retrieves it by multiplying
    the memory matrix with the feature-mapped query with proper normalization.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: å…¬å¼ 2 è¡¨æ˜ï¼Œé€šè¿‡èšåˆï¼ˆç‰¹å¾æ˜ å°„åçš„ï¼‰é”®å’ŒæŸ¥è¯¢çš„å¤–ç§¯ï¼Œå®ƒä¿ç•™äº†ä¸€ä¸ªå†…å­˜çŸ©é˜µã€‚ç¨åé€šè¿‡å°†å†…å­˜çŸ©é˜µä¸ç‰¹å¾æ˜ å°„åçš„æŸ¥è¯¢ä¹˜ä»¥é€‚å½“çš„å½’ä¸€åŒ–æ¥æ£€ç´¢å®ƒã€‚
- en: 'This approach consists of two foundational components:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ç§æ–¹æ³•ç”±ä¸¤ä¸ªåŸºç¡€ç»„ä»¶ç»„æˆï¼š
- en: '**Feature map ğœ™ (Â·):** the kernel feature map for each attention implementation
    (ex. ğœ™ğ‘–(x) = elu(ğ‘¥ ğ‘– )+1 proposed in Linear Transformer'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ç‰¹å¾æ˜ å°„ ğœ™ (Â·)ï¼š** æ¯ç§æ³¨æ„åŠ›å®ç°çš„æ ¸ç‰¹å¾æ˜ å°„ï¼ˆä¾‹å¦‚ï¼ŒLinear Transformer æå‡ºçš„ğœ™ğ‘–(x) = elu(ğ‘¥ ğ‘– )+1ï¼‰ã€‚'
- en: '**Aggregation rule:** aggregating the associations {ğœ™ (k)ğ‘— âŠ— vğ‘—} into the memory
    matrix by simple summation.'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**èšåˆè§„åˆ™ï¼š** é€šè¿‡ç®€å•æ±‚å’Œå°†å…³è” {ğœ™ (k)ğ‘— âŠ— vğ‘—} èšåˆåˆ°å†…å­˜çŸ©é˜µä¸­ã€‚'
- en: 3.3\. Query prototyping and memory compression
  id: totrans-84
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.3\. æŸ¥è¯¢åŸå‹åŒ–å’Œå†…å­˜å‹ç¼©
- en: Aside from employing the utilization of sparse attention or kernel-based linearized
    attention, it is also feasible to mitigate the intricacy of attention through
    a decrease in the quantity of queries or key-value pairs, thereby resulting in
    the initiation of query prototypes and the implementation of memory compression
    techniques, respectively.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: é™¤äº†ä½¿ç”¨ç¨€ç–æ³¨æ„åŠ›æˆ–åŸºäºæ ¸çš„çº¿æ€§åŒ–æ³¨æ„åŠ›ï¼Œè¿˜å¯ä»¥é€šè¿‡å‡å°‘æŸ¥è¯¢æˆ–é”®å€¼å¯¹çš„æ•°é‡æ¥ç¼“è§£æ³¨æ„åŠ›çš„å¤æ‚æ€§ï¼Œä»è€Œå¼•å…¥æŸ¥è¯¢åŸå‹å’Œå†…å­˜å‹ç¼©æŠ€æœ¯ã€‚
- en: '![](../Images/7c0ff26beef060b9c57b31245f7634db.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7c0ff26beef060b9c57b31245f7634db.png)'
- en: Fig. 9\. Qurey prototyping and memory compression. Photo from [[1](https://arxiv.org/abs/2106.04554v2)]
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 9\. æŸ¥è¯¢åŸå‹åŒ–å’Œå†…å­˜å‹ç¼©ã€‚ç…§ç‰‡æ¥è‡ª [[1](https://arxiv.org/abs/2106.04554v2)]
- en: '**3.3.1\. Attention with prototype queries:** The implementation of Attention
    with Prototype Queries involves the utilization of a set of query prototypes as
    the primary basis for computing attention distributions. The model employs two
    distinct methodologies, either by copying the computed distributions to the positions
    occupied by the represented queries, or by filling those positions with discrete
    uniform distributions. The flow of computation in this process is depicted in
    Figure 9(a).'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '**3.3.1\. å¸¦åŸå‹æŸ¥è¯¢çš„æ³¨æ„åŠ›ï¼š** å®æ–½å¸¦åŸå‹æŸ¥è¯¢çš„æ³¨æ„åŠ›æ¶‰åŠä½¿ç”¨ä¸€ç»„æŸ¥è¯¢åŸå‹ä½œä¸ºè®¡ç®—æ³¨æ„åŠ›åˆ†å¸ƒçš„ä¸»è¦ä¾æ®ã€‚æ¨¡å‹é‡‡ç”¨ä¸¤ç§ä¸åŒçš„æ–¹æ³•ï¼Œè¦ä¹ˆå°†è®¡ç®—å¾—åˆ°çš„åˆ†å¸ƒå¤åˆ¶åˆ°ä»£è¡¨æŸ¥è¯¢çš„ä½ç½®ï¼Œè¦ä¹ˆåœ¨è¿™äº›ä½ç½®å¡«å……ç¦»æ•£å‡åŒ€åˆ†å¸ƒã€‚è¯¥è¿‡ç¨‹çš„è®¡ç®—æµç¨‹å¦‚å›¾9(a)æ‰€ç¤ºã€‚'
- en: Clustered Attention, as described in [7], involves the aggregation of queries
    into several clusters, with attention distributions being computed for the centroids
    of these clusters. All queries within a cluster are assigned the attention distribution
    calculated for its corresponding centroid.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: èšç±»æ³¨æ„åŠ›ï¼Œå¦‚[7]æ‰€è¿°ï¼Œæ¶‰åŠå°†æŸ¥è¯¢èšåˆåˆ°å‡ ä¸ªç°‡ä¸­ï¼Œæ³¨æ„åŠ›åˆ†å¸ƒåˆ™æ˜¯é’ˆå¯¹è¿™äº›ç°‡çš„è´¨å¿ƒè®¡ç®—çš„ã€‚ç°‡å†…æ‰€æœ‰æŸ¥è¯¢éƒ½è¢«åˆ†é…ç»™ç›¸åº”è´¨å¿ƒè®¡ç®—å‡ºçš„æ³¨æ„åŠ›åˆ†å¸ƒã€‚
- en: Informer, as outlined in [8], employs a methodology of explicit query sparsity
    measurement, derived from an approximation of the Kullback-Leibler divergence
    between the queryâ€™s attention distribution and the discrete uniform distribution,
    to select query prototypes. Attention distributions are then calculated only for
    the top-ğ‘¢ queries as determined by the query sparsity measurement, with the remaining
    queries being assigned discrete uniform distributions.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: Informerï¼Œå¦‚[8]æ‰€è¿°ï¼Œé‡‡ç”¨äº†æ˜¾å¼æŸ¥è¯¢ç¨€ç–åº¦æµ‹é‡çš„æ–¹æ³•ï¼Œè¯¥æ–¹æ³•æºè‡ªäºå¯¹æŸ¥è¯¢çš„æ³¨æ„åŠ›åˆ†å¸ƒä¸ç¦»æ•£å‡åŒ€åˆ†å¸ƒä¹‹é—´Kullback-Leibleræ•£åº¦çš„è¿‘ä¼¼ï¼Œä»¥é€‰æ‹©æŸ¥è¯¢åŸå‹ã€‚ç„¶åï¼Œä»…å¯¹ç”±æŸ¥è¯¢ç¨€ç–åº¦æµ‹é‡ç¡®å®šä¸ºå‰ğ‘¢ä¸ªçš„æŸ¥è¯¢è®¡ç®—æ³¨æ„åŠ›åˆ†å¸ƒï¼Œè€Œå…¶ä½™æŸ¥è¯¢åˆ™åˆ†é…ç¦»æ•£å‡åŒ€åˆ†å¸ƒã€‚
- en: '**3.3.2\. Attention with compressed key-value memory:** This technique reduces
    the complexity of the attention mechanism in the Transformer by reducing the number
    of key-value pairs before applying attention as shown in Fig. 9(b). This is achieved
    by compressing the key-value memory. The compressed memory is then used to compute
    attention scores. This technique can significantly reduce the computational cost
    of attention while maintaining good performance on various NLP tasks.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '**3.3.2\. å‹ç¼©é”®å€¼å†…å­˜çš„æ³¨æ„åŠ›ï¼š** è¿™ç§æŠ€æœ¯é€šè¿‡åœ¨åº”ç”¨æ³¨æ„åŠ›ä¹‹å‰å‡å°‘é”®-å€¼å¯¹çš„æ•°é‡æ¥é™ä½Transformerä¸­æ³¨æ„åŠ›æœºåˆ¶çš„å¤æ‚æ€§ï¼Œå¦‚å›¾9(b)æ‰€ç¤ºã€‚è¿™é€šè¿‡å‹ç¼©é”®å€¼å†…å­˜æ¥å®ç°ã€‚å‹ç¼©å†…å­˜ç„¶åç”¨äºè®¡ç®—æ³¨æ„åŠ›åˆ†æ•°ã€‚è¿™ç§æŠ€æœ¯å¯ä»¥æ˜¾è‘—é™ä½æ³¨æ„åŠ›çš„è®¡ç®—æˆæœ¬ï¼ŒåŒæ—¶åœ¨å„ç§è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸Šä¿æŒè‰¯å¥½çš„æ€§èƒ½ã€‚'
- en: '*Liu et al. [9]* suggest a technique called *Memory Compressed Attention (MCA)*
    in their paper. *MCA* involves using strided convolution to decrease the number
    of keys and values. *MCA* is utilized alongside local attention, which is also
    proposed in the same paper. By reducing the number of keys and values by a factor
    of the kernel size, *MCA* is able to capture global context and process longer
    sequences than the standard Transformer model with the same computational resources.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '*Liu et al. [9]* åœ¨å…¶è®ºæ–‡ä¸­æå‡ºäº†ä¸€ç§åä¸º *Memory Compressed Attention (MCA)* çš„æŠ€æœ¯ã€‚*MCA*
    ä½¿ç”¨è·¨æ­¥å·ç§¯æ¥å‡å°‘é”®å’Œå€¼çš„æ•°é‡ã€‚*MCA* ä¸æœ¬æ–‡ä¸­ä¹Ÿæå‡ºçš„å±€éƒ¨æ³¨æ„åŠ›ä¸€èµ·ä½¿ç”¨ã€‚é€šè¿‡å°†é”®å’Œå€¼çš„æ•°é‡å‡å°‘åˆ°å·ç§¯æ ¸å¤§å°çš„å› å­ï¼Œ*MCA* èƒ½å¤Ÿæ•è·å…¨å±€ä¸Šä¸‹æ–‡å¹¶å¤„ç†æ¯”æ ‡å‡†Transformeræ¨¡å‹æ›´é•¿çš„åºåˆ—ï¼ŒåŒæ—¶ä¿æŒç›¸åŒçš„è®¡ç®—èµ„æºã€‚'
- en: '*Set Transformer* [10] and *Luna* [11] are two models that utilize external
    trainable global nodes to condense information from inputs. The condensed representations
    then function as a compressed memory that the inputs attend to, effectively reducing
    the quadratic complexity of self-attention to linear complexity concerning the
    length of the input sequence.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '*Set Transformer* [10] å’Œ *Luna* [11] æ˜¯ä¸¤ä¸ªåˆ©ç”¨å¤–éƒ¨å¯è®­ç»ƒçš„å…¨å±€èŠ‚ç‚¹æ¥å‹ç¼©è¾“å…¥ä¿¡æ¯çš„æ¨¡å‹ã€‚å‹ç¼©è¡¨ç¤ºç„¶åä½œä¸ºè¾“å…¥çš„æ³¨æ„åŠ›çš„å‹ç¼©å†…å­˜ï¼Œæœ‰æ•ˆåœ°å°†è‡ªæ³¨æ„åŠ›çš„äºŒæ¬¡å¤æ‚åº¦é™ä½åˆ°ä¸è¾“å…¥åºåˆ—é•¿åº¦çº¿æ€§ç›¸å…³çš„å¤æ‚åº¦ã€‚'
- en: '*Linformer* [12] reduces the computational complexity of self-attention to
    linear by linearly projecting keys and values from the length *n* to a smaller
    length *n_k.* The setback with this approach is the pre-assumed input sequence
    length, making it unsuitable for autoregressive attention.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '*Linformer* [12]å°†è‡ªæ³¨æ„åŠ›çš„è®¡ç®—å¤æ‚åº¦çº¿æ€§é™ä½ï¼Œé€šè¿‡å°†é”®å’Œå€¼ä»é•¿åº¦ä¸º *n* çº¿æ€§æŠ•å½±åˆ°æ›´å°çš„é•¿åº¦ *n_k*ã€‚è¿™ç§æ–¹æ³•çš„ç¼ºç‚¹æ˜¯é¢„è®¾çš„è¾“å…¥åºåˆ—é•¿åº¦ï¼Œå› æ­¤ä¸é€‚åˆè‡ªå›å½’æ³¨æ„åŠ›æ¨¡å‹ã€‚'
- en: '*Poolingformer* [13] employs a two-level attention mechanism that combines
    sliding window attention with compressed memory attention. Compressed memory attention
    helps with enlarging the receptive field. To reduce the number of keys and values,
    several pooling operations are explored, including max pooling and Dynamic Convolution-based
    pooling.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '*Poolingformer* [13] é‡‡ç”¨äº†ä¸€ä¸ªä¸¤çº§æ³¨æ„åŠ›æœºåˆ¶ï¼Œå°†æ»‘åŠ¨çª—å£æ³¨æ„åŠ›ä¸å‹ç¼©å†…å­˜æ³¨æ„åŠ›ç›¸ç»“åˆã€‚å‹ç¼©å†…å­˜æ³¨æ„åŠ›æœ‰åŠ©äºæ‰©å¤§æ„Ÿå—é‡ã€‚ä¸ºäº†å‡å°‘é”®å’Œå€¼çš„æ•°é‡ï¼Œæ¢ç´¢äº†å‡ ç§æ± åŒ–æ“ä½œï¼ŒåŒ…æ‹¬æœ€å¤§æ± åŒ–å’ŒåŸºäºåŠ¨æ€å·ç§¯çš„æ± åŒ–ã€‚'
- en: 3.4\. Low-rank self-attention
  id: totrans-96
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.4\. ä½ç§©è‡ªæ³¨æ„åŠ›
- en: 'According to empirical and theoretical analyses conducted by various researchers
    [14, 12], the self-attention matrix A âˆˆ Rğ‘‡ Ã—ğ‘‡ exhibits low-rank characteristics
    in many cases. This observation offers two implications: Firstly, the low-rank
    nature can be explicitly modeled using parameterization. This could lead to the
    development of new models that leverage this property to improve performance.
    Secondly, instead of using the full self-attention matrix, a low-rank approximation
    could be used in its place. This approach could enable more efficient computations
    and further enhance the scalability of self-attention-based models.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: æ ¹æ®å„ç§ç ”ç©¶è€… [14, 12] è¿›è¡Œçš„å®è¯å’Œç†è®ºåˆ†æï¼Œè‡ªæ³¨æ„åŠ›çŸ©é˜µ A âˆˆ Rğ‘‡ Ã—ğ‘‡ åœ¨è®¸å¤šæƒ…å†µä¸‹è¡¨ç°å‡ºä½ç§©ç‰¹æ€§ã€‚è¿™ä¸€è§‚å¯Ÿæä¾›äº†ä¸¤ä¸ªå«ä¹‰ï¼šé¦–å…ˆï¼Œå¯ä»¥ä½¿ç”¨å‚æ•°åŒ–æ˜¾å¼å»ºæ¨¡ä½ç§©ç‰¹æ€§ã€‚è¿™å¯èƒ½ä¼šå¯¼è‡´å¼€å‘åˆ©ç”¨è¿™ä¸€ç‰¹æ€§ä»¥æé«˜æ€§èƒ½çš„æ–°æ¨¡å‹ã€‚å…¶æ¬¡ï¼Œå¯ä»¥ç”¨ä½ç§©è¿‘ä¼¼ä»£æ›¿å®Œæ•´çš„è‡ªæ³¨æ„åŠ›çŸ©é˜µã€‚è¿™ç§æ–¹æ³•å¯ä»¥å®ç°æ›´é«˜æ•ˆçš„è®¡ç®—ï¼Œå¹¶è¿›ä¸€æ­¥æé«˜åŸºäºè‡ªæ³¨æ„åŠ›çš„æ¨¡å‹çš„å¯æ‰©å±•æ€§ã€‚
- en: '**3.4.1\. Low-rank parameterization:** When the rank of the attention matrix
    is lower than the sequence length, it suggests that over-parameterizing the model
    by setting ğ·ğ‘˜ > ğ‘‡ would lead to overfitting in situations where the input is typically
    short. Therefore, it is sensible to restrict the dimension of ğ·ğ‘˜ and leverage
    the low-rank property as an inductive bias. To this end, Guo et al. [14] propose
    decomposing the self-attention matrix into a low-rank attention module with a
    small ğ·ğ‘˜ that captures long-range non-local interactions, and a band attention
    module that captures local dependencies. This approach can be beneficial in scenarios
    where the input is short and requires effective modeling of both local and non-local
    dependencies.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '**3.4.1\. ä½ç§©å‚æ•°åŒ–ï¼š** å½“æ³¨æ„åŠ›çŸ©é˜µçš„ç§©ä½äºåºåˆ—é•¿åº¦æ—¶ï¼Œè¿™è¡¨æ˜é€šè¿‡è®¾ç½® ğ·ğ‘˜ > ğ‘‡ æ¥è¿‡åº¦å‚æ•°åŒ–æ¨¡å‹ä¼šå¯¼è‡´åœ¨è¾“å…¥é€šå¸¸è¾ƒçŸ­çš„æƒ…å†µä¸‹å‡ºç°è¿‡æ‹Ÿåˆã€‚å› æ­¤ï¼Œé™åˆ¶
    ğ·ğ‘˜ çš„ç»´åº¦å¹¶åˆ©ç”¨ä½ç§©ç‰¹æ€§ä½œä¸ºå½’çº³åå·®æ˜¯æ˜æ™ºçš„ã€‚ä¸ºæ­¤ï¼ŒGuo ç­‰äºº [14] æå‡ºäº†å°†è‡ªæ³¨æ„åŠ›çŸ©é˜µåˆ†è§£ä¸ºä¸€ä¸ªå°çš„ ğ·ğ‘˜ ä½ç§©æ³¨æ„åŠ›æ¨¡å—ï¼Œç”¨äºæ•æ‰é•¿è·ç¦»çš„éå±€éƒ¨äº¤äº’ï¼Œä»¥åŠä¸€ä¸ªå¸¦çŠ¶æ³¨æ„åŠ›æ¨¡å—ï¼Œç”¨äºæ•æ‰å±€éƒ¨ä¾èµ–ã€‚è¿™ç§æ–¹æ³•åœ¨è¾“å…¥è¾ƒçŸ­ä¸”éœ€è¦æœ‰æ•ˆå»ºæ¨¡å±€éƒ¨å’Œéå±€éƒ¨ä¾èµ–çš„åœºæ™¯ä¸­å¯èƒ½ä¼šæœ‰æ‰€å¸®åŠ©ã€‚'
- en: '**3.4.2\. Low-rank approximation:** The low-rank property of the attention
    matrix can also be leveraged to reduce the complexity of self-attention by using
    a low-rank matrix approximation. This methodology is closely related to the low-rank
    approximation of kernel matrices, and some existing works are inspired by kernel
    approximation. For instance, Performer, as discussed in Section 3.2, uses a random
    feature map originally proposed to approximate Gaussian kernels to decompose the
    attention distribution matrix A into Cğ‘„ GCğ¾, where G is a Gaussian kernel matrix
    and the random feature map approximates G.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '**3.4.2\. ä½ç§©è¿‘ä¼¼ï¼š** ä¹Ÿå¯ä»¥åˆ©ç”¨æ³¨æ„åŠ›çŸ©é˜µçš„ä½ç§©ç‰¹æ€§ï¼Œé€šè¿‡ä½¿ç”¨ä½ç§©çŸ©é˜µè¿‘ä¼¼æ¥é™ä½è‡ªæ³¨æ„åŠ›çš„å¤æ‚æ€§ã€‚è¿™ç§æ–¹æ³•ä¸æ ¸çŸ©é˜µçš„ä½ç§©è¿‘ä¼¼å¯†åˆ‡ç›¸å…³ï¼Œä¸€äº›ç°æœ‰å·¥ä½œå—åˆ°æ ¸è¿‘ä¼¼çš„å¯å‘ã€‚ä¾‹å¦‚ï¼ŒPerformerï¼ˆå¦‚ç¬¬
    3.2 èŠ‚æ‰€è®¨è®ºçš„ï¼‰ä½¿ç”¨äº†ä¸€ç§æœ€åˆç”¨äºè¿‘ä¼¼é«˜æ–¯æ ¸çš„éšæœºç‰¹å¾æ˜ å°„ï¼Œå°†æ³¨æ„åŠ›åˆ†å¸ƒçŸ©é˜µ A åˆ†è§£ä¸º Cğ‘„ GCğ¾ï¼Œå…¶ä¸­ G æ˜¯é«˜æ–¯æ ¸çŸ©é˜µï¼Œéšæœºç‰¹å¾æ˜ å°„è¿‘ä¼¼ Gã€‚'
- en: 'An alternative approach to dealing with the low-rank property of attention
    matrices is to use NystrÃ¶m-based methods [15, 16]. In these methods, a subset
    of landmark nodes is selected from the input sequence using down-sampling techniques
    such as strided average pooling. The selected landmarks are then used as queries
    and keys to approximate the attention matrix. Specifically, the attention computation
    involves softmax normalization of the product of the original queries with the
    selected keys, followed by the product of the selected queries with the normalized
    result. This can be expressed as:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: å¤„ç†æ³¨æ„åŠ›çŸ©é˜µä½ç§©ç‰¹æ€§çš„ä¸€ç§æ›¿ä»£æ–¹æ³•æ˜¯ä½¿ç”¨åŸºäº NystrÃ¶m çš„æ–¹æ³• [15, 16]ã€‚åœ¨è¿™äº›æ–¹æ³•ä¸­ï¼Œä»è¾“å…¥åºåˆ—ä¸­é€‰æ‹©ä¸€éƒ¨åˆ†åœ°æ ‡èŠ‚ç‚¹ï¼Œä½¿ç”¨ä¸‹é‡‡æ ·æŠ€æœ¯ï¼Œå¦‚æ­¥é•¿å¹³å‡æ± åŒ–ã€‚é€‰æ‹©çš„åœ°æ ‡èŠ‚ç‚¹è¢«ç”¨ä½œæŸ¥è¯¢å’Œé”®ï¼Œä»¥è¿‘ä¼¼æ³¨æ„åŠ›çŸ©é˜µã€‚å…·ä½“è€Œè¨€ï¼Œæ³¨æ„åŠ›è®¡ç®—åŒ…æ‹¬å¯¹åŸå§‹æŸ¥è¯¢ä¸é€‰æ‹©çš„é”®çš„ä¹˜ç§¯è¿›è¡Œ
    softmax å½’ä¸€åŒ–ï¼Œç„¶åè®¡ç®—é€‰æ‹©çš„æŸ¥è¯¢ä¸å½’ä¸€åŒ–ç»“æœçš„ä¹˜ç§¯ã€‚è¿™å¯ä»¥è¡¨ç¤ºä¸ºï¼š
- en: '![](../Images/262220a7ba22b64149ec700c89c9b3a4.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/262220a7ba22b64149ec700c89c9b3a4.png)'
- en: Eq. 4
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: æ–¹ç¨‹ 4
- en: Note that the inverse of the matrix **M**^-1 = (softmax(QÌƒKÌƒ^T))^-1 may not
    always exist, but this issue can be mitigated in various ways. For example, CSALR
    [15] adds an identity matrix to **M** to ensure the inverse always exists, while
    NystrÃ¶m-former [16] uses the Moore-Penrose pseudoinverse of **M** to handle singular
    cases.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: æ³¨æ„ï¼ŒçŸ©é˜µ **M**^-1 = (softmax(QÌƒKÌƒ^T))^-1 çš„é€†å¯èƒ½å¹¶ä¸æ€»æ˜¯å­˜åœ¨ï¼Œä½†å¯ä»¥é€šè¿‡å¤šç§æ–¹å¼å‡è½»æ­¤é—®é¢˜ã€‚ä¾‹å¦‚ï¼ŒCSALR [15]
    å‘ **M** æ·»åŠ å•ä½çŸ©é˜µä»¥ç¡®ä¿é€†çŸ©é˜µå§‹ç»ˆå­˜åœ¨ï¼Œè€Œ NystrÃ¶m-former [16] ä½¿ç”¨ **M** çš„ Moore-Penrose ä¼ªé€†æ¥å¤„ç†å¥‡å¼‚æƒ…å†µã€‚
- en: 3.5\. Attention with prior
  id: totrans-104
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.5\. å¸¦å…ˆéªŒçš„æ³¨æ„åŠ›
- en: The attention mechanism is a way of focusing on specific parts of an input sequence.
    It does this by generating a weighted sum of the vectors in the sequence, where
    the weights are determined by an attention distribution. The attention distribution
    can be generated from the inputs, or it can come from other sources, such as prior
    knowledge. In most cases, the attention distribution from the inputs and the prior
    attention distribution are combined by computing a weighted sum of their scores
    before applying softmax, thus, allowing the neural network to learn from both
    the inputs and the prior knowledge.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: æ³¨æ„åŠ›æœºåˆ¶æ˜¯ä¸€ç§å…³æ³¨è¾“å…¥åºåˆ—ä¸­ç‰¹å®šéƒ¨åˆ†çš„æ–¹æ³•ã€‚å®ƒé€šè¿‡ç”Ÿæˆåºåˆ—ä¸­å‘é‡çš„åŠ æƒå’Œæ¥å®ç°ï¼Œå…¶ä¸­æƒé‡ç”±æ³¨æ„åŠ›åˆ†å¸ƒå†³å®šã€‚æ³¨æ„åŠ›åˆ†å¸ƒå¯ä»¥ä»è¾“å…¥ä¸­ç”Ÿæˆï¼Œä¹Ÿå¯ä»¥æ¥è‡ªå…¶ä»–æ¥æºï¼Œå¦‚å…ˆéªŒçŸ¥è¯†ã€‚åœ¨å¤§å¤šæ•°æƒ…å†µä¸‹ï¼Œè¾“å…¥çš„æ³¨æ„åŠ›åˆ†å¸ƒå’Œå…ˆéªŒæ³¨æ„åŠ›åˆ†å¸ƒé€šè¿‡è®¡ç®—å®ƒä»¬åˆ†æ•°çš„åŠ æƒå’Œåç»“åˆï¼Œç„¶ååº”ç”¨
    softmaxï¼Œä»è€Œä½¿ç¥ç»ç½‘ç»œèƒ½å¤Ÿä»è¾“å…¥å’Œå…ˆéªŒçŸ¥è¯†ä¸­å­¦ä¹ ã€‚
- en: '![](../Images/f3d9acc3d75dd3afcdc64f246b3f0bdb.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f3d9acc3d75dd3afcdc64f246b3f0bdb.png)'
- en: Fig. 10\. Attention with prior combines generated and prior attention scores
    to compute the final attention scores. Image from [[1]](https://arxiv.org/abs/2106.04554v2).
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 10\. å¸¦å…ˆéªŒçš„æ³¨æ„åŠ›å°†ç”Ÿæˆçš„æ³¨æ„åŠ›åˆ†æ•°å’Œå…ˆéªŒæ³¨æ„åŠ›åˆ†æ•°ç»“åˆèµ·æ¥è®¡ç®—æœ€ç»ˆçš„æ³¨æ„åŠ›åˆ†æ•°ã€‚å›¾ç‰‡æ¥è‡ª [[1]](https://arxiv.org/abs/2106.04554v2)ã€‚
- en: '**3.5.1\. Prior that models locality:** To model the locality of certain types
    of data like text, a Gaussian distribution over positions can be used as prior
    attention. This involves multiplying the generated attention distribution with
    a Gaussian density and renormalizing or adding a bias term G to the generated
    attention scores, where higher G indicates a higher prior probability of attending
    to a specific input.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '**3.5.1\. å»ºæ¨¡å±€éƒ¨æ€§çš„å…ˆéªŒï¼š** ä¸ºäº†å»ºæ¨¡æŸäº›ç±»å‹æ•°æ®çš„å±€éƒ¨æ€§ï¼Œå¦‚æ–‡æœ¬ï¼Œå¯ä»¥ä½¿ç”¨ä½ç½®ä¸Šçš„é«˜æ–¯åˆ†å¸ƒä½œä¸ºå…ˆéªŒæ³¨æ„åŠ›ã€‚è¿™æ¶‰åŠåˆ°å°†ç”Ÿæˆçš„æ³¨æ„åŠ›åˆ†å¸ƒä¸é«˜æ–¯å¯†åº¦ç›¸ä¹˜ï¼Œå¹¶å¯¹ç”Ÿæˆçš„æ³¨æ„åŠ›åˆ†æ•°è¿›è¡Œå½’ä¸€åŒ–æˆ–æ·»åŠ åç½®é¡¹
    Gï¼Œå…¶ä¸­æ›´é«˜çš„ G è¡¨ç¤ºå¯¹ç‰¹å®šè¾“å…¥çš„å…ˆéªŒæ¦‚ç‡æ›´é«˜ã€‚'
- en: 'Yang et al. [17] propose a method of predicting a central position for each
    input and defining the Gaussian bias accordingly:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: Yang ç­‰äºº [17] æå‡ºäº†é¢„æµ‹æ¯ä¸ªè¾“å…¥çš„ä¸­å¿ƒä½ç½®å¹¶ç›¸åº”åœ°å®šä¹‰é«˜æ–¯åç½®çš„æ–¹æ³•ï¼š
- en: '![](../Images/af94364ef136d40e6cb207f656871ecb.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/af94364ef136d40e6cb207f656871ecb.png)'
- en: Eq. 5
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: æ–¹ç¨‹ 5
- en: where ğœ denotes the standard deviation for the Gaussian. The Gaussian bias is
    defined as the negative of the squared distance between the central position and
    the input position, divided by the standard deviation of the Gaussian distribution.
    The standard deviation can be determined as a hyperparameter or predicted from
    the inputs.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­ ğœ è¡¨ç¤ºé«˜æ–¯çš„æ ‡å‡†å·®ã€‚é«˜æ–¯åç½®å®šä¹‰ä¸ºä¸­å¿ƒä½ç½®ä¸è¾“å…¥ä½ç½®ä¹‹é—´çš„å¹³æ–¹è·ç¦»çš„è´Ÿå€¼ï¼Œé™¤ä»¥é«˜æ–¯åˆ†å¸ƒçš„æ ‡å‡†å·®ã€‚æ ‡å‡†å·®å¯ä»¥ä½œä¸ºè¶…å‚æ•°ç¡®å®šï¼Œä¹Ÿå¯ä»¥ä»è¾“å…¥ä¸­é¢„æµ‹ã€‚
- en: '![](../Images/fbbdad900d98f2d1f54606ee25c4d9bc.png)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fbbdad900d98f2d1f54606ee25c4d9bc.png)'
- en: Fig. 11\. The proposed approach in [17] is illustrated, using a window size
    of 2 (D = 2). Photo from [[17](https://aclanthology.org/D18-1475/)].
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 11\. ä½¿ç”¨çª—å£å¤§å°ä¸º 2 (D = 2) çš„[17]æå‡ºçš„æ–¹æ³•è¿›è¡Œè¯´æ˜ã€‚å›¾ç‰‡æ¥è‡ª [[17](https://aclanthology.org/D18-1475/)]ã€‚
- en: The Gaussian Transformer [18] model assumes that the central position for each
    input query ğ‘ğ‘– is ğ‘–, and defines the bias term ğºğ‘– ğ‘— for the generated attention
    scores as
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: é«˜æ–¯ Transformer [18] æ¨¡å‹å‡è®¾æ¯ä¸ªè¾“å…¥æŸ¥è¯¢ ğ‘ğ‘– çš„ä¸­å¿ƒä½ç½®ä¸º ğ‘–ï¼Œå¹¶å°†ç”Ÿæˆçš„æ³¨æ„åŠ›åˆ†æ•°çš„åç½®é¡¹ ğºğ‘– ğ‘— å®šä¹‰ä¸º
- en: '![](../Images/00d462bccc16e5a6fdbe1ea8805bdf2b.png)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/00d462bccc16e5a6fdbe1ea8805bdf2b.png)'
- en: Eq. 6
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: æ–¹ç¨‹ 6
- en: where ğ‘¤ is a non-negative scalar parameter controlling the deviation and ğ‘ is
    a negative scalar parameter reducing the weight for the central position.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­ ğ‘¤ æ˜¯ä¸€ä¸ªéè´Ÿæ ‡é‡å‚æ•°ï¼Œæ§åˆ¶åå·®ï¼Œğ‘ æ˜¯ä¸€ä¸ªè´Ÿæ ‡é‡å‚æ•°ï¼Œå‡å°‘ä¸­å¿ƒä½ç½®çš„æƒé‡ã€‚
- en: '**3.5.2\. Prior from lower modules:** In Transformer architecture, attention
    distributions between adjacent layers are often found to be similar. Therefore,
    it is reasonable to use the attention distribution from a lower layer as a prior
    for computing attention in a higher layer. This can be achieved by combining the
    attention scores from the current layer with a weighted sum of the previous layerâ€™s
    attention scores and a translation function that maps the previous scores to the
    prior to be applied.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '**3.5.2\. æ¥æºäºä½å±‚æ¨¡å—ï¼š** åœ¨ Transformer æ¶æ„ä¸­ï¼Œç›¸é‚»å±‚ä¹‹é—´çš„æ³¨æ„åŠ›åˆ†å¸ƒé€šå¸¸è¢«å‘ç°æ˜¯ç›¸ä¼¼çš„ã€‚å› æ­¤ï¼Œä½¿ç”¨æ¥è‡ªä½å±‚çš„æ³¨æ„åŠ›åˆ†å¸ƒä½œä¸ºè®¡ç®—é«˜å±‚æ³¨æ„åŠ›çš„å…ˆéªŒæ˜¯åˆç†çš„ã€‚è¿™å¯ä»¥é€šè¿‡å°†å½“å‰å±‚çš„æ³¨æ„åŠ›åˆ†æ•°ä¸å‰ä¸€å±‚æ³¨æ„åŠ›åˆ†æ•°çš„åŠ æƒå’Œä»¥åŠå°†å‰ä¸€å±‚åˆ†æ•°æ˜ å°„åˆ°è¦åº”ç”¨çš„å…ˆéªŒçš„è½¬æ¢å‡½æ•°ç›¸ç»“åˆæ¥å®ç°ã€‚'
- en: '![](../Images/97d7e1995b33ec4e42a969251bb4f0eb.png)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/97d7e1995b33ec4e42a969251bb4f0eb.png)'
- en: Eq. 7
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: Eq. 7
- en: 'where A(ğ‘™) represents the *l-*th layer attention scores while *w*1â€‹ and *w*2â€‹
    control the relative importance of the previous attention scores and the current
    attention scores. Also, the function ğ‘”: Rğ‘›Ã—ğ‘› â†’ Rğ‘›Ã—ğ‘› translates the previous attention
    scores into a prior to be applied to the current attention scores.'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 'å…¶ä¸­ A(ğ‘™) ä»£è¡¨ *l-* å±‚æ³¨æ„åŠ›åˆ†æ•°ï¼Œè€Œ *w*1 å’Œ *w*2 æ§åˆ¶ä¹‹å‰çš„æ³¨æ„åŠ›åˆ†æ•°å’Œå½“å‰æ³¨æ„åŠ›åˆ†æ•°çš„ç›¸å¯¹é‡è¦æ€§ã€‚æ­¤å¤–ï¼Œå‡½æ•° ğ‘”: Rğ‘›Ã—ğ‘›
    â†’ Rğ‘›Ã—ğ‘› å°†ä¹‹å‰çš„æ³¨æ„åŠ›åˆ†æ•°è½¬åŒ–ä¸ºåº”ç”¨äºå½“å‰æ³¨æ„åŠ›åˆ†æ•°çš„å…ˆéªŒã€‚'
- en: The *Predictive Attention Transformer* proposed in the paper [19] suggests using
    a 2D-convolutional layer on the previous attention scores to compute the final
    attention scores as a convex combination of the generated attention scores and
    the convolved scores. In other words, the weight parameters for the generated
    and convolved scores are set to ğ›¼ and 1-ğ›¼, respectively, and the function ğ‘”(Â·)
    in Eq. (6) is a convolutional layer. The paper presents experiments showing that
    training the model from scratch and fine-tuning it after adapting a pre-trained
    BERT model both lead to improvements over baseline models.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: è®ºæ–‡ [19] ä¸­æå‡ºçš„ *Predictive Attention Transformer* å»ºè®®åœ¨ä¹‹å‰çš„æ³¨æ„åŠ›åˆ†æ•°ä¸Šä½¿ç”¨ 2D å·ç§¯å±‚æ¥è®¡ç®—æœ€ç»ˆçš„æ³¨æ„åŠ›åˆ†æ•°ï¼Œè¯¥åˆ†æ•°æ˜¯ç”Ÿæˆçš„æ³¨æ„åŠ›åˆ†æ•°å’Œå·ç§¯åˆ†æ•°çš„å‡¸ç»„åˆã€‚æ¢å¥è¯è¯´ï¼Œç”Ÿæˆå’Œå·ç§¯åˆ†æ•°çš„æƒé‡å‚æ•°åˆ†åˆ«è®¾ç½®ä¸º
    ğ›¼ å’Œ 1-ğ›¼ï¼Œè€Œ Eq. (6) ä¸­çš„å‡½æ•° ğ‘”(Â·) æ˜¯ä¸€ä¸ªå·ç§¯å±‚ã€‚è®ºæ–‡ä¸­çš„å®éªŒè¡¨æ˜ï¼Œæ— è®ºæ˜¯ä»å¤´å¼€å§‹è®­ç»ƒæ¨¡å‹è¿˜æ˜¯åœ¨é€‚é…é¢„è®­ç»ƒ BERT æ¨¡å‹åè¿›è¡Œå¾®è°ƒï¼Œéƒ½æ¯”åŸºçº¿æ¨¡å‹æœ‰äº†æ”¹è¿›ã€‚
- en: The *Realformer* model proposed in [20] introduces a residual skip connection
    on attention maps by directly adding the previous attention scores to the newly
    generated ones. This can be seen as setting ğ‘¤ 1 = ğ‘¤ 2 = 1 and ğ‘”(Â·) to be the identity
    map in Eq. (6). The authors conduct pre-training experiments on this model and
    report that it outperforms the baseline BERT model in multiple datasets, even
    with significantly lower pre-training budgets.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: è®ºæ–‡ [20] ä¸­æå‡ºçš„ *Realformer* æ¨¡å‹é€šè¿‡ç›´æ¥å°†ä¹‹å‰çš„æ³¨æ„åŠ›åˆ†æ•°æ·»åŠ åˆ°æ–°ç”Ÿæˆçš„åˆ†æ•°ä¸­ï¼Œå¼•å…¥äº†å¯¹æ³¨æ„åŠ›å›¾çš„æ®‹å·®è·³è·ƒè¿æ¥ã€‚è¿™å¯ä»¥è§†ä¸ºåœ¨ Eq.
    (6) ä¸­å°† ğ‘¤ 1 = ğ‘¤ 2 = 1 å’Œ ğ‘”(Â·) è®¾ç½®ä¸ºæ’ç­‰æ˜ å°„ã€‚ä½œè€…åœ¨è¯¥æ¨¡å‹ä¸Šè¿›è¡Œçš„é¢„è®­ç»ƒå®éªŒæŠ¥å‘Šç§°ï¼Œè¯¥æ¨¡å‹åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šä¼˜äºåŸºçº¿ BERT æ¨¡å‹ï¼Œå³ä½¿åœ¨æ˜¾è‘—é™ä½é¢„è®­ç»ƒé¢„ç®—çš„æƒ…å†µä¸‹ã€‚
- en: '*Lazyformer* [21] proposes an innovative approach where attention maps are
    shared between adjacent layers to reduce computational costs. This is achieved
    by setting ğ‘”(Â·) to identity and alternately switching between the settings of
    ğ‘¤ 1 = 0, ğ‘¤ 2 = 1 and ğ‘¤ 1 = 1, ğ‘¤ 2 = 0\. This method enables the computation of
    attention maps only once and reuses them in succeeding layers. The pre-training
    experiments conducted by Lazyformer show that their model is not only efficient
    but also effective, outperforming the baseline models with significantly lower
    computation budgets.'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '*Lazyformer* [21] æå‡ºäº†ä¸€ä¸ªåˆ›æ–°æ–¹æ³•ï¼Œé€šè¿‡åœ¨ç›¸é‚»å±‚ä¹‹é—´å…±äº«æ³¨æ„åŠ›å›¾æ¥é™ä½è®¡ç®—æˆæœ¬ã€‚è¿™æ˜¯é€šè¿‡å°† ğ‘”(Â·) è®¾ä¸ºæ’ç­‰æ˜ å°„å¹¶äº¤æ›¿åˆ‡æ¢ ğ‘¤
    1 = 0, ğ‘¤ 2 = 1 å’Œ ğ‘¤ 1 = 1, ğ‘¤ 2 = 0 çš„è®¾ç½®æ¥å®ç°çš„ã€‚è¿™ç§æ–¹æ³•ä½¿å¾—åªéœ€è®¡ç®—ä¸€æ¬¡æ³¨æ„åŠ›å›¾ï¼Œå¹¶åœ¨åç»­å±‚ä¸­é‡å¤ä½¿ç”¨ã€‚Lazyformer
    è¿›è¡Œçš„é¢„è®­ç»ƒå®éªŒè¡¨æ˜ï¼Œä»–ä»¬çš„æ¨¡å‹ä¸ä»…é«˜æ•ˆï¼Œè€Œä¸”æœ‰æ•ˆï¼Œè¶…è¶Šäº†åŸºçº¿æ¨¡å‹ï¼Œå¹¶æ˜¾è‘—é™ä½äº†è®¡ç®—é¢„ç®—ã€‚'
- en: '**3.5.3\. Prior as multi-task adapters:** The Prior as Multi-task Adapters
    approach uses trainable attention priors that enable efficient parameter sharing
    across tasks [22]. The Conditionally Adaptive Multi-Task Learning (CAMTL) [23]
    framework is a technique for multi-task learning that enables the efficient sharing
    of pre-trained models between tasks. CAMTL uses trainable attention prior, which
    depends on task encoding, to act as an adapter for multi-task inductive knowledge
    transfer. Specifically, the attention prior is represented as a block diagonal
    matrix that is added to the attention scores of upper layers in pre-trained Transformers:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '**3.5.3\. ä½œä¸ºå¤šä»»åŠ¡é€‚é…å™¨çš„å…ˆéªŒï¼š** ä½œä¸ºå¤šä»»åŠ¡é€‚é…å™¨çš„å…ˆéªŒæ–¹æ³•ä½¿ç”¨å¯è®­ç»ƒçš„æ³¨æ„åŠ›å…ˆéªŒï¼Œè¿™äº›å…ˆéªŒä½¿å¾—è·¨ä»»åŠ¡çš„å‚æ•°å…±äº«æ›´ä¸ºé«˜æ•ˆ [22]ã€‚æ¡ä»¶è‡ªé€‚åº”å¤šä»»åŠ¡å­¦ä¹ ï¼ˆCAMTLï¼‰[23]
    æ¡†æ¶æ˜¯ä¸€ç§å¤šä»»åŠ¡å­¦ä¹ æŠ€æœ¯ï¼Œå®ƒä½¿å¾—åœ¨ä»»åŠ¡ä¹‹é—´é«˜æ•ˆå…±äº«é¢„è®­ç»ƒæ¨¡å‹æˆä¸ºå¯èƒ½ã€‚CAMTL ä½¿ç”¨ä¾èµ–äºä»»åŠ¡ç¼–ç çš„å¯è®­ç»ƒæ³¨æ„åŠ›å…ˆéªŒï¼Œä½œä¸ºå¤šä»»åŠ¡è¯±å¯¼çŸ¥è¯†è½¬ç§»çš„é€‚é…å™¨ã€‚å…·ä½“æ¥è¯´ï¼Œæ³¨æ„åŠ›å…ˆéªŒè¢«è¡¨ç¤ºä¸ºå—å¯¹è§’çŸ©é˜µï¼Œæ·»åŠ åˆ°é¢„è®­ç»ƒ
    Transformer çš„ä¸Šå±‚æ³¨æ„åŠ›åˆ†æ•°ä¸­ï¼š'
- en: '![](../Images/cca478938fd9c353b29199a52fe294f5.png)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cca478938fd9c353b29199a52fe294f5.png)'
- en: Eq. 8
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: Eq. 8
- en: in which, âŠ• represents direct sum, ğ´ğ‘— are trainable parameters with dimensions
    (ğ‘›/ğ‘š)Ã—(ğ‘›/ğ‘š) and ğ›¾ğ‘— and ğ›½ğ‘— are Feature Wise Linear Modulation functions with input
    and output dimensions of Rğ·ğ‘§ and (ğ‘›/ğ‘š)Ã—(ğ‘›/ğ‘š), respectively [24]. The CAMTL framework
    specifies a maximum sequence length ğ‘›ğ‘šğ‘ğ‘¥ in implementation. The attention prior,
    which is a trainable matrix, is added to the attention scores of the upper layers
    in pre-trained Transformers. This addition creates an adapter that allows for
    parameter-efficient multi-task inductive knowledge transfer. The prior is organized
    as a block diagonal matrix for efficient computation.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å…¶ä¸­ï¼ŒâŠ• è¡¨ç¤ºç›´æ¥å’Œï¼Œğ´ğ‘— æ˜¯å…·æœ‰ (ğ‘›/ğ‘š)Ã—(ğ‘›/ğ‘š) ç»´åº¦çš„å¯è®­ç»ƒå‚æ•°ï¼Œğ›¾ğ‘— å’Œ ğ›½ğ‘— æ˜¯å…·æœ‰è¾“å…¥å’Œè¾“å‡ºç»´åº¦ä¸º Rğ·ğ‘§ å’Œ (ğ‘›/ğ‘š)Ã—(ğ‘›/ğ‘š)
    çš„ç‰¹å¾çº¿æ€§è°ƒåˆ¶å‡½æ•° [24]ã€‚CAMTL æ¡†æ¶åœ¨å®ç°ä¸­è§„å®šäº†æœ€å¤§åºåˆ—é•¿åº¦ ğ‘›ğ‘šğ‘ğ‘¥ã€‚æ³¨æ„åŠ›å…ˆéªŒæ˜¯ä¸€ä¸ªå¯è®­ç»ƒçŸ©é˜µï¼Œå®ƒè¢«æ·»åŠ åˆ°é¢„è®­ç»ƒ Transformer çš„ä¸Šå±‚æ³¨æ„åŠ›åˆ†æ•°ä¸­ã€‚è¿™ç§æ·»åŠ åˆ›å»ºäº†ä¸€ä¸ªé€‚é…å™¨ï¼Œä½¿å¾—å¤šä»»åŠ¡è¯±å¯¼çŸ¥è¯†è½¬ç§»åœ¨å‚æ•°ä¸Šæ›´é«˜æ•ˆã€‚å…ˆéªŒè¢«ç»„ç»‡ä¸ºå—å¯¹è§’çŸ©é˜µä»¥æé«˜è®¡ç®—æ•ˆç‡ã€‚
- en: '**3.5.4\. Attention with only prior:** Zhang et al. [25] have developed an
    alternative approach to attention distribution that does not rely on pair-wise
    interaction between inputs. Their method is called the â€œaverage attention network,â€
    and it uses a discrete uniform distribution as the sole source of attention distribution.
    The values are then aggregated as a cumulative average of all values. To enhance
    the networkâ€™s expressiveness, a feed-forward gating layer is added on top of the
    average attention module. The benefit of this approach is that the modified Transformer
    decoder can be trained in a parallel manner, and it can decode like an RNN, avoiding
    the O(ğ‘‡Â²) complexity associated with decoding.'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '**3.5.4\. ä»…ä½¿ç”¨å…ˆéªŒçš„æ³¨æ„åŠ›ï¼š** Zhang ç­‰äºº [25] å¼€å‘äº†ä¸€ç§æ›¿ä»£çš„æ³¨æ„åŠ›åˆ†å¸ƒæ–¹æ³•ï¼Œè¯¥æ–¹æ³•ä¸ä¾èµ–äºè¾“å…¥ä¹‹é—´çš„æˆå¯¹äº¤äº’ã€‚ä»–ä»¬çš„æ–¹æ³•ç§°ä¸ºâ€œå¹³å‡æ³¨æ„åŠ›ç½‘ç»œâ€ï¼Œå®ƒä½¿ç”¨ç¦»æ•£å‡åŒ€åˆ†å¸ƒä½œä¸ºæ³¨æ„åŠ›åˆ†å¸ƒçš„å”¯ä¸€æ¥æºã€‚ç„¶åå°†è¿™äº›å€¼èšåˆä¸ºæ‰€æœ‰å€¼çš„ç´¯ç§¯å¹³å‡å€¼ã€‚ä¸ºäº†å¢å¼ºç½‘ç»œçš„è¡¨è¾¾èƒ½åŠ›ï¼Œåœ¨å¹³å‡æ³¨æ„åŠ›æ¨¡å—ä¸Šæ·»åŠ äº†ä¸€ä¸ªå‰é¦ˆé—¨æ§å±‚ã€‚è¿™ç§æ–¹æ³•çš„å¥½å¤„æ˜¯ï¼Œä¿®æ”¹åçš„
    Transformer è§£ç å™¨å¯ä»¥ä»¥å¹¶è¡Œæ–¹å¼è¿›è¡Œè®­ç»ƒï¼Œå¹¶ä¸”èƒ½å¤Ÿåƒ RNN ä¸€æ ·è§£ç ï¼Œé¿å…äº†ä¸è§£ç ç›¸å…³çš„ O(ğ‘‡Â²) å¤æ‚æ€§ã€‚'
- en: similar to Yang et al. [17] and Guo et al. [18], which use a fixed local window
    for attention distribution, You et al. [26] incorporate a hardcoded Gaussian distribution
    attention for attention calculation. However, They completely ignore the calculated
    attention and solely use the Gaussian distribution for attention computation in
    which, the mean and variance are the hyperparameters. Provided it is implemented
    on self-attention, it can produce results close to the baseline models in machine
    translation tasks.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: ç±»ä¼¼äº Yang ç­‰äºº [17] å’Œ Guo ç­‰äºº [18]ï¼Œå®ƒä»¬ä½¿ç”¨å›ºå®šçš„å±€éƒ¨çª—å£è¿›è¡Œæ³¨æ„åŠ›åˆ†å¸ƒï¼ŒYou ç­‰äºº [26] å°†ç¡¬ç¼–ç çš„é«˜æ–¯åˆ†å¸ƒæ³¨æ„åŠ›ç”¨äºæ³¨æ„åŠ›è®¡ç®—ã€‚ç„¶è€Œï¼Œä»–ä»¬å®Œå…¨å¿½ç•¥äº†è®¡ç®—å¾—åˆ°çš„æ³¨æ„åŠ›ï¼Œåªä½¿ç”¨é«˜æ–¯åˆ†å¸ƒè¿›è¡Œæ³¨æ„åŠ›è®¡ç®—ï¼Œå…¶ä¸­å‡å€¼å’Œæ–¹å·®æ˜¯è¶…å‚æ•°ã€‚åªè¦åœ¨è‡ªæ³¨æ„åŠ›ä¸Šå®ç°ï¼Œå®ƒå°±å¯ä»¥åœ¨æœºå™¨ç¿»è¯‘ä»»åŠ¡ä¸­äº§ç”Ÿæ¥è¿‘åŸºçº¿æ¨¡å‹çš„ç»“æœã€‚
- en: 'Synthesizer [27] has proposed a novel way of generating attention scores in
    Transformers. Instead of using the traditional method of generating attention
    scores, they replace them with two variants: (1) learnable, randomly initialized
    attention scores, and (2) attention scores output by a feed-forward network that
    is only conditioned on the input being queried. The results of their experiments
    on machine translation and language modeling tasks demonstrate that these variants
    perform comparably to the standard Transformer model. However, the reason why
    these variants work is not fully explained, leaving room for further investigation.'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: Synthesizer [27] æå‡ºäº†ä¸€ç§åœ¨ Transformers ä¸­ç”Ÿæˆæ³¨æ„åŠ›åˆ†æ•°çš„æ–°æ–¹æ³•ã€‚ä»–ä»¬ç”¨ä¸¤ç§å˜ä½“æ›¿ä»£ä¼ ç»Ÿçš„ç”Ÿæˆæ³¨æ„åŠ›åˆ†æ•°çš„æ–¹æ³•ï¼š(1)
    å¯å­¦ä¹ çš„ã€éšæœºåˆå§‹åŒ–çš„æ³¨æ„åŠ›åˆ†æ•°ï¼Œä»¥åŠ (2) ç”±ä»…å¯¹è¾“å…¥è¿›è¡Œæ¡ä»¶å¤„ç†çš„å‰é¦ˆç½‘ç»œè¾“å‡ºçš„æ³¨æ„åŠ›åˆ†æ•°ã€‚ä»–ä»¬åœ¨æœºå™¨ç¿»è¯‘å’Œè¯­è¨€å»ºæ¨¡ä»»åŠ¡ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œè¿™äº›å˜ä½“çš„è¡¨ç°ä¸æ ‡å‡†
    Transformer æ¨¡å‹ç›¸å½“ã€‚ç„¶è€Œï¼Œè¿™äº›å˜ä½“ä¸ºä½•æœ‰æ•ˆå°šæœªå®Œå…¨è§£é‡Šï¼Œä»æœ‰è¿›ä¸€æ­¥ç ”ç©¶çš„ç©ºé—´ã€‚
- en: 3.6\. Improved multi-head mechanism
  id: totrans-133
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.6\. æ”¹è¿›çš„å¤šå¤´æœºåˆ¶
- en: Multi-head attention is a powerful technique because it allows a model to attend
    to different parts of the input simultaneously. However, it is not guaranteed
    that each attention head will learn unique and complementary features. As a result,
    some researchers have explored methods to ensure that each attention head captures
    distinct information.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: å¤šå¤´æ³¨æ„åŠ›æ˜¯ä¸€ç§å¼ºå¤§çš„æŠ€æœ¯ï¼Œå› ä¸ºå®ƒå…è®¸æ¨¡å‹åŒæ—¶å…³æ³¨è¾“å…¥çš„ä¸åŒéƒ¨åˆ†ã€‚ç„¶è€Œï¼Œä¸èƒ½ä¿è¯æ¯ä¸ªæ³¨æ„åŠ›å¤´éƒ½ä¼šå­¦ä¹ åˆ°ç‹¬ç‰¹ä¸”äº’è¡¥çš„ç‰¹å¾ã€‚å› æ­¤ï¼Œä¸€äº›ç ”ç©¶äººå‘˜æ¢ç´¢äº†ç¡®ä¿æ¯ä¸ªæ³¨æ„åŠ›å¤´æ•æ‰åˆ°ä¸åŒä¿¡æ¯çš„æ–¹æ³•ã€‚
- en: '**3.6.1\. Head behavior modeling:** Multi-head attention is a useful tool in
    natural language processing models as it enables the simultaneous processing of
    multiple inputs and feature representations [28]. However, the vanilla Transformer
    model lacks a mechanism to ensure that different attention heads capture distinct
    and non-redundant features. Additionally, there is no provision for interaction
    among the heads. To address these limitations, recent research has focused on
    introducing novel mechanisms that guide the behavior of attention heads or enable
    interaction between them.'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '**3.6.1\. å¤´éƒ¨è¡Œä¸ºå»ºæ¨¡ï¼š** å¤šå¤´æ³¨æ„åŠ›æ˜¯è‡ªç„¶è¯­è¨€å¤„ç†æ¨¡å‹ä¸­çš„ä¸€ä¸ªæœ‰ç”¨å·¥å…·ï¼Œå› ä¸ºå®ƒå…è®¸åŒæ—¶å¤„ç†å¤šä¸ªè¾“å…¥å’Œç‰¹å¾è¡¨ç¤º[28]ã€‚ç„¶è€Œï¼Œä¼ ç»Ÿçš„ Transformer
    æ¨¡å‹ç¼ºä¹ç¡®ä¿ä¸åŒæ³¨æ„åŠ›å¤´æ•æ‰åˆ°ä¸åŒä¸”éå†—ä½™ç‰¹å¾çš„æœºåˆ¶ã€‚æ­¤å¤–ï¼Œä¹Ÿæ²¡æœ‰å¤´éƒ¨ä¹‹é—´ç›¸äº’ä½œç”¨çš„è§„å®šã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œè¿‘æœŸçš„ç ”ç©¶é›†ä¸­äºå¼•å…¥æ–°é¢–çš„æœºåˆ¶æ¥æŒ‡å¯¼æ³¨æ„åŠ›å¤´çš„è¡Œä¸ºæˆ–ä½¿å®ƒä»¬ä¹‹é—´èƒ½å¤Ÿè¿›è¡Œäº¤äº’ã€‚'
- en: 'In order to promote diversity among different attention heads, Li et al. [29]
    propose an additional regularization term in the loss function. This regularization
    consists of two parts: the first two aim to maximize the cosine distances between
    input subspaces and output representations, while the latter encourages dispersion
    of the positions attended by multiple heads through element-wise multiplication
    of their corresponding attention matrices. By adding this auxiliary term, the
    model is encouraged to learn a more diverse set of attention patterns across different
    heads, which can improve its performance on various tasks.'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†ä¿ƒè¿›ä¸åŒæ³¨æ„åŠ›å¤´ä¹‹é—´çš„å¤šæ ·æ€§ï¼ŒLi ç­‰äºº [29] åœ¨æŸå¤±å‡½æ•°ä¸­æå‡ºäº†é¢å¤–çš„æ­£åˆ™åŒ–é¡¹ã€‚è¿™ä¸ªæ­£åˆ™åŒ–ç”±ä¸¤éƒ¨åˆ†ç»„æˆï¼šå‰ä¸¤éƒ¨åˆ†æ—¨åœ¨æœ€å¤§åŒ–è¾“å…¥å­ç©ºé—´å’Œè¾“å‡ºè¡¨ç¤ºä¹‹é—´çš„ä½™å¼¦è·ç¦»ï¼Œè€Œåè€…é€šè¿‡å…ƒç´ çº§ä¹˜æ³•é¼“åŠ±å¤šä¸ªå¤´éƒ¨å…³æ³¨çš„ä½ç½®çš„åˆ†æ•£ã€‚é€šè¿‡æ·»åŠ è¿™ä¸€è¾…åŠ©é¡¹ï¼Œæ¨¡å‹è¢«é¼“åŠ±åœ¨ä¸åŒå¤´éƒ¨ä¹‹é—´å­¦ä¹ åˆ°æ›´å¤šæ ·çš„æ³¨æ„åŠ›æ¨¡å¼ï¼Œä»è€Œæé«˜åœ¨å„ç§ä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚
- en: Numerous studies have shown that pre-trained Transformer models exhibit certain
    self-attention patterns that do not align well with natural language processing.
    Kovaleva et al. [30] identify several of these patterns in BERT, including attention
    heads that focus exclusively on the special tokens [CLS] and [SEP]. To improve
    training, Deshpande and Narasimhan [31] suggest using an auxiliary loss function
    that measures the Frobenius norm between the attention distribution maps and predefined
    attention patterns. This approach introduces constraints to encourage more meaningful
    attention patterns.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: è®¸å¤šç ”ç©¶è¡¨æ˜ï¼Œé¢„è®­ç»ƒçš„ Transformer æ¨¡å‹å±•ç¤ºäº†æŸäº›è‡ªæ³¨æ„åŠ›æ¨¡å¼ï¼Œè¿™äº›æ¨¡å¼ä¸è‡ªç„¶è¯­è¨€å¤„ç†å¹¶ä¸å®Œå…¨å¥‘åˆã€‚Kovaleva ç­‰äºº [30] åœ¨ BERT
    ä¸­è¯†åˆ«å‡ºå…¶ä¸­çš„å‡ ç§æ¨¡å¼ï¼ŒåŒ…æ‹¬ä¸“æ³¨äºç‰¹æ®Šæ ‡è®° [CLS] å’Œ [SEP] çš„æ³¨æ„åŠ›å¤´ã€‚ä¸ºäº†æ”¹è¿›è®­ç»ƒï¼ŒDeshpande å’Œ Narasimhan [31] æå‡ºäº†ä½¿ç”¨è¾…åŠ©æŸå¤±å‡½æ•°ï¼Œè¯¥å‡½æ•°æµ‹é‡æ³¨æ„åŠ›åˆ†å¸ƒå›¾ä¸é¢„å®šä¹‰æ³¨æ„åŠ›æ¨¡å¼ä¹‹é—´çš„
    Frobenius èŒƒæ•°ã€‚è¿™ç§æ–¹æ³•å¼•å…¥äº†çº¦æŸï¼Œä»¥é¼“åŠ±æ›´æœ‰æ„ä¹‰çš„æ³¨æ„åŠ›æ¨¡å¼ã€‚
- en: In the paper by Shen et al. [32], a new mechanism called Talking-head Attention
    is introduced, which aims to encourage the model to transfer information between
    different attention heads in a learnable manner. This mechanism involves linearly
    projecting the generated attention scores from the hidden dimension to a new space
    with h_k heads, applying softmax in this space, and then projecting the results
    to another space with h_v heads for value aggregation. This way, the attention
    mechanism can learn to dynamically transfer information between the different
    attention heads, leading to improved performance in various natural language processing
    tasks.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨Shenç­‰äºº[32]çš„è®ºæ–‡ä¸­ï¼Œæå‡ºäº†ä¸€ç§åä¸ºâ€œTalking-head Attentionâ€çš„æ–°æœºåˆ¶ï¼Œè¯¥æœºåˆ¶æ—¨åœ¨é¼“åŠ±æ¨¡å‹ä»¥å¯å­¦ä¹ çš„æ–¹å¼åœ¨ä¸åŒçš„æ³¨æ„åŠ›å¤´ä¹‹é—´ä¼ é€’ä¿¡æ¯ã€‚è¯¥æœºåˆ¶åŒ…æ‹¬å°†ç”Ÿæˆçš„æ³¨æ„åŠ›åˆ†æ•°ä»éšè—ç»´åº¦çº¿æ€§æŠ•å½±åˆ°å…·æœ‰h_kä¸ªå¤´çš„æ–°ç©ºé—´ï¼Œåœ¨è¯¥ç©ºé—´ä¸­åº”ç”¨softmaxï¼Œç„¶åå°†ç»“æœæŠ•å½±åˆ°å¦ä¸€ä¸ªå…·æœ‰h_vä¸ªå¤´çš„ç©ºé—´ä»¥è¿›è¡Œå€¼èšåˆã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œæ³¨æ„åŠ›æœºåˆ¶å¯ä»¥å­¦ä¹ åœ¨ä¸åŒæ³¨æ„åŠ›å¤´ä¹‹é—´åŠ¨æ€ä¼ é€’ä¿¡æ¯ï¼Œä»è€Œæé«˜å„ç§è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡çš„æ€§èƒ½ã€‚
- en: Collaborative Multi-head Attention is a mechanism proposed in [33] that involves
    the use of shared query and key projections, denoted as Wğ‘„ and Wğ¾, respectively,
    along with a mixing vector mğ‘–. This mixing vector is used to filter the projection
    parameters for the ğ‘–-th head. Specifically, the attention computation is adapted
    to reflect this mechanism, resulting in a modified equation (3).
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: åä½œå¤šå¤´æ³¨æ„åŠ›æ˜¯ä¸€ç§åœ¨[33]ä¸­æå‡ºçš„æœºåˆ¶ï¼Œæ¶‰åŠä½¿ç”¨å…±äº«çš„æŸ¥è¯¢å’Œé”®æŠ•å½±ï¼Œåˆ†åˆ«è®°ä½œWğ‘„å’ŒWğ¾ï¼Œä»¥åŠä¸€ä¸ªæ··åˆå‘é‡mğ‘–ã€‚è¯¥æ··åˆå‘é‡ç”¨äºè¿‡æ»¤ğ‘–-thå¤´çš„æŠ•å½±å‚æ•°ã€‚å…·ä½“è€Œè¨€ï¼Œæ³¨æ„åŠ›è®¡ç®—è¢«è°ƒæ•´ä»¥åæ˜ è¿™ä¸€æœºåˆ¶ï¼Œä»è€Œå¾—å‡ºä¿®æ”¹åçš„æ–¹ç¨‹ï¼ˆ3ï¼‰ã€‚
- en: '![](../Images/bc4e7c6e5518925240d2d2681e63b0a7.png)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bc4e7c6e5518925240d2d2681e63b0a7.png)'
- en: Eq. 9
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: Eq. 9
- en: where all heads share W^q and W^k.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­æ‰€æœ‰å¤´å…±äº«W^qå’ŒW^kã€‚
- en: '**3.6.2\. Multi-head with restricted spans:**'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '**3.6.2\. å…·æœ‰é™åˆ¶èŒƒå›´çš„å¤šå¤´æ³¨æ„åŠ›ï¼š**'
- en: 'The vanilla attention mechanism typically assumes full attention spans, allowing
    a query to attend to all key-value pairs. However, it has been observed that some
    attention heads tend to focus more on local contexts, while others attend to broader
    contexts. As a result, it may be advantageous to impose constraints on attention
    spans for specific purposes:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: æ ‡å‡†çš„æ³¨æ„åŠ›æœºåˆ¶é€šå¸¸å‡è®¾å…¨èŒƒå›´æ³¨æ„åŠ›ï¼Œå…è®¸æŸ¥è¯¢å¯¹æ‰€æœ‰é”®å€¼å¯¹è¿›è¡Œæ³¨æ„ã€‚ç„¶è€Œï¼Œå·²ç»è§‚å¯Ÿåˆ°ä¸€äº›æ³¨æ„åŠ›å¤´æ›´å€¾å‘äºå…³æ³¨å±€éƒ¨ä¸Šä¸‹æ–‡ï¼Œè€Œå…¶ä»–æ³¨æ„åŠ›å¤´åˆ™å…³æ³¨æ›´å¹¿æ³›çš„ä¸Šä¸‹æ–‡ã€‚å› æ­¤ï¼Œå¯¹ç‰¹å®šç›®çš„æ–½åŠ æ³¨æ„åŠ›èŒƒå›´çš„çº¦æŸå¯èƒ½æ˜¯æœ‰åˆ©çš„ï¼š
- en: 'Locality: Restricting attention spans can explicitly impose local constraints,
    which can be beneficial in scenarios where locality is an important consideration.'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å±€éƒ¨æ€§ï¼šé™åˆ¶æ³¨æ„åŠ›èŒƒå›´å¯ä»¥æ˜ç¡®æ–½åŠ å±€éƒ¨çº¦æŸï¼Œè¿™åœ¨å±€éƒ¨æ€§æ˜¯é‡è¦è€ƒè™‘å› ç´ çš„æƒ…å†µä¸‹å°¤ä¸ºæœ‰ç›Šã€‚
- en: 'Efficiency: Appropriately implemented, such a model can scale to longer sequences
    without introducing additional memory usage or computational time.'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ•ˆç‡ï¼šå¦‚æœå®æ–½å¾—å½“ï¼Œè¿™ç§æ¨¡å‹å¯ä»¥åœ¨ä¸å¢åŠ é¢å¤–å†…å­˜ä½¿ç”¨æˆ–è®¡ç®—æ—¶é—´çš„æƒ…å†µä¸‹æ‰©å±•åˆ°æ›´é•¿çš„åºåˆ—ã€‚
- en: Restricting attention spans involves multiplying each attention distribution
    value with a mask value, followed by re-normalization. The mask value can be determined
    by a non-increasing function that maps a distance to a value in the range [0,
    1]. In vanilla attention, a mask value of 1 is assigned for all distances, as
    illustrated in Figure 12(a).
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: é™åˆ¶æ³¨æ„åŠ›èŒƒå›´æ¶‰åŠå°†æ¯ä¸ªæ³¨æ„åŠ›åˆ†å¸ƒå€¼ä¸æ©ç å€¼ç›¸ä¹˜ï¼Œç„¶åè¿›è¡Œé‡æ–°å½’ä¸€åŒ–ã€‚æ©ç å€¼å¯ä»¥ç”±ä¸€ä¸ªéé€’å¢å‡½æ•°ç¡®å®šï¼Œè¯¥å‡½æ•°å°†è·ç¦»æ˜ å°„åˆ°[0, 1]èŒƒå›´å†…çš„ä¸€ä¸ªå€¼ã€‚åœ¨æ ‡å‡†æ³¨æ„åŠ›ä¸­ï¼Œå¯¹äºæ‰€æœ‰è·ç¦»åˆ†é…æ©ç å€¼ä¸º1ï¼Œå¦‚å›¾12(a)æ‰€ç¤ºã€‚
- en: '![](../Images/6398180ac8234a6b560c662fad7c4831.png)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6398180ac8234a6b560c662fad7c4831.png)'
- en: Fig. 12 showcases three distinct types of span masking functions denoted as
    ğ‘š(ğ‘¥). The horizontal axis represents the distance ğ‘¥, while the vertical axis represents
    the corresponding mask value. This visual representation offers insights into
    the diverse behaviors and patterns exhibited by these masking functions, providing
    a clear visualization of how the mask values change with respect to the distance
    between key-value pairs. Image from [[1]](https://arxiv.org/abs/2106.04554v2).
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾12å±•ç¤ºäº†ä¸‰ç§ä¸åŒç±»å‹çš„èŒƒå›´æ©ç å‡½æ•°ï¼Œè®°ä½œğ‘š(ğ‘¥)ã€‚æ°´å¹³è½´è¡¨ç¤ºè·ç¦»ğ‘¥ï¼Œè€Œå‚ç›´è½´è¡¨ç¤ºç›¸åº”çš„æ©ç å€¼ã€‚è¿™ä¸€è§†è§‰è¡¨ç¤ºæä¾›äº†è¿™äº›æ©ç å‡½æ•°å±•ç¤ºçš„ä¸åŒè¡Œä¸ºå’Œæ¨¡å¼çš„æ´è§ï¼Œæ¸…æ™°åœ°å±•ç¤ºäº†æ©ç å€¼å¦‚ä½•éšç€é”®å€¼å¯¹ä¹‹é—´çš„è·ç¦»å˜åŒ–ã€‚å›¾ç‰‡æ¥æºäº[[1]](https://arxiv.org/abs/2106.04554v2)ã€‚
- en: In a study by Sukhbaatar et al. [34], a novel approach was proposed, introducing
    a learnable attention span that is depicted in the intriguing Figure 12(b). This
    innovative technique utilizes a mask parameterized by a learnable scalar ğ‘§, combined
    with a hyperparameter ğ‘…, to adaptively modulate the attention span. Remarkably,
    experimental results on character-level language modeling demonstrated that these
    adaptive-span models outperformed the baseline models while requiring significantly
    fewer FLOPS. Notably, an interesting observation was made that lower layers of
    the model tended to exhibit smaller learned spans, whereas higher layers displayed
    larger spans. This intriguing finding suggests that the model can autonomously
    learn a hierarchical composition of features, showcasing its exceptional ability
    to capture complex patterns and structures in the data.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨Sukhbaatarç­‰äºº [34] çš„ç ”ç©¶ä¸­ï¼Œæå‡ºäº†ä¸€ç§æ–°é¢–çš„æ–¹æ³•ï¼Œå¼•å…¥äº†ä¸€ä¸ªå¯å­¦ä¹ çš„æ³¨æ„åŠ›èŒƒå›´ï¼Œå¦‚æœ‰è¶£çš„å›¾12(b)æ‰€ç¤ºã€‚è¿™ç§åˆ›æ–°æŠ€æœ¯åˆ©ç”¨äº†ç”±å¯å­¦ä¹ æ ‡é‡
    ğ‘§ å‚æ•°åŒ–çš„æ©ç ï¼Œå¹¶ç»“åˆè¶…å‚æ•° ğ‘…ï¼Œè‡ªé€‚åº”åœ°è°ƒèŠ‚æ³¨æ„åŠ›èŒƒå›´ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¿™äº›è‡ªé€‚åº”èŒƒå›´æ¨¡å‹åœ¨å­—ç¬¦çº§è¯­è¨€å»ºæ¨¡ä¸Šä¼˜äºåŸºçº¿æ¨¡å‹ï¼ŒåŒæ—¶éœ€è¦æ˜¾è‘—æ›´å°‘çš„FLOPSã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œä¸€ä¸ªæœ‰è¶£çš„è§‚å¯Ÿæ˜¯æ¨¡å‹çš„ä½å±‚é€šå¸¸æ˜¾ç¤ºå‡ºè¾ƒå°çš„å­¦ä¹ èŒƒå›´ï¼Œè€Œé«˜å±‚åˆ™è¡¨ç°å‡ºè¾ƒå¤§çš„èŒƒå›´ã€‚è¿™ä¸€æœ‰è¶£å‘ç°è¡¨æ˜æ¨¡å‹å¯ä»¥è‡ªä¸»å­¦ä¹ ç‰¹å¾çš„å±‚æ¬¡ç»„åˆï¼Œå±•ç¤ºäº†å…¶æ•æ‰æ•°æ®ä¸­å¤æ‚æ¨¡å¼å’Œç»“æ„çš„å“è¶Šèƒ½åŠ›ã€‚
- en: The *Multi-Scale Transformer* [35] presents a novel approach to attention spans
    that challenges the traditional paradigm. Unlike vanilla attention, which assumes
    a uniform attention span across all heads, this innovative model introduces a
    fixed attention span with dynamic scaling in different layers. Illustrated in
    Fig. 12(c), the fixed attention span acts as a window that can be scaled up or
    down, controlled by a scale value denoted as ğ‘¤.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '*å¤šå°ºåº¦å˜æ¢å™¨* [35] æå‡ºäº†ä¸€ä¸ªæ–°çš„æ³¨æ„åŠ›èŒƒå›´æ–¹æ³•ï¼ŒæŒ‘æˆ˜äº†ä¼ ç»ŸèŒƒå¼ã€‚ä¸å‡è®¾æ‰€æœ‰å¤´éƒ¨å…·æœ‰ç»Ÿä¸€æ³¨æ„åŠ›èŒƒå›´çš„æ™®é€šæ³¨æ„åŠ›ä¸åŒï¼Œè¯¥åˆ›æ–°æ¨¡å‹å¼•å…¥äº†ä¸€ä¸ªå…·æœ‰åŠ¨æ€ç¼©æ”¾çš„å›ºå®šæ³¨æ„åŠ›èŒƒå›´ã€‚å¦‚å›¾12(c)æ‰€ç¤ºï¼Œå›ºå®šæ³¨æ„åŠ›èŒƒå›´ä½œä¸ºä¸€ä¸ªå¯ä»¥ç¼©æ”¾çš„çª—å£ï¼Œç”±æ ‡è®°ä¸º
    ğ‘¤ çš„å°ºåº¦å€¼æ§åˆ¶ã€‚'
- en: '![](../Images/6ac644da107d37c3434399808e584639.png)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6ac644da107d37c3434399808e584639.png)'
- en: Fig. 13\. Multi-Scale Multi-Head Self-Attention, where three heads are depicted,
    each representing a different scale. The blue, green, and red boxes illustrate
    the scales of Ï‰ = 1, Ï‰ = 3, and Ï‰ = 5, respectively. Photo by [author](https://www.linkedin.com/in/soran-ghaderi/).
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾13\. å¤šå°ºåº¦å¤šå¤´è‡ªæ³¨æ„åŠ›ï¼Œå…¶ä¸­æç»˜äº†ä¸‰ä¸ªå¤´éƒ¨ï¼Œæ¯ä¸ªå¤´éƒ¨ä»£è¡¨ä¸åŒçš„å°ºåº¦ã€‚è“è‰²ã€ç»¿è‰²å’Œçº¢è‰²æ¡†åˆ†åˆ«è¡¨ç¤ºå°ºåº¦ Ï‰ = 1ã€Ï‰ = 3 å’Œ Ï‰ = 5ã€‚ç…§ç‰‡ç”±
    [ä½œè€…](https://www.linkedin.com/in/soran-ghaderi/) æä¾›ã€‚
- en: The scale values vary, with higher layers favoring larger scales for broader
    contextual dependencies and lower layers opting for smaller scales for more localized
    attention as shown in Figure 13\. The experimental results of the Multi-Scale
    Transformer demonstrate its superior performance over baseline models on various
    tasks, showcasing its potential for more efficient and effective language processing.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: å°ºåº¦å€¼å„å¼‚ï¼Œé«˜å±‚å€¾å‘äºä½¿ç”¨è¾ƒå¤§çš„å°ºåº¦ä»¥è·å–æ›´å¹¿æ³›çš„ä¸Šä¸‹æ–‡ä¾èµ–ï¼Œè€Œä½å±‚åˆ™é€‰æ‹©è¾ƒå°çš„å°ºåº¦ä»¥è·å¾—æ›´å±€éƒ¨çš„å…³æ³¨ï¼Œå¦‚å›¾13æ‰€ç¤ºã€‚å¤šå°ºåº¦å˜æ¢å™¨çš„å®éªŒç»“æœæ˜¾ç¤ºï¼Œå®ƒåœ¨å„ç§ä»»åŠ¡ä¸Šè¡¨ç°å‡ºä¼˜äºåŸºçº¿æ¨¡å‹çš„æ€§èƒ½ï¼Œå±•ç¤ºäº†å…¶åœ¨è¯­è¨€å¤„ç†ä¸Šçš„æ›´é«˜æ•ˆå’Œæœ‰æ•ˆçš„æ½œåŠ›ã€‚
- en: '**3.6.3\. Multi-head with refined aggregation:**'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '**3.6.3\. å¤šå¤´ä¸ç²¾ç»†åŒ–èšåˆï¼š**'
- en: The vanilla multi-head attention mechanism, as proposed by Vaswani et al. [28],
    involves the computation of multiple attention heads that operate in parallel
    to generate individual output representations. These representations are then
    concatenated and subjected to a linear transformation, as defined in Eq. (11),
    to obtain the final output representation. By combining Eqs. (10), (11), and (12),
    it can be observed that this concatenate-and-project formulation is equivalent
    to a summation over re-parameterized attention outputs. This approach allows for
    efficient aggregation of the diverse attention head outputs, enabling the model
    to capture complex dependencies and relationships in the input data.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: æ™®é€šçš„å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶ï¼Œå¦‚Vaswaniç­‰äºº [28] æå‡ºçš„ï¼Œæ¶‰åŠè®¡ç®—å¤šä¸ªå¹¶è¡Œæ“ä½œçš„æ³¨æ„åŠ›å¤´ï¼Œä»¥ç”Ÿæˆå„è‡ªçš„è¾“å‡ºè¡¨ç¤ºã€‚è¿™äº›è¡¨ç¤ºç„¶åè¢«ä¸²è”ï¼Œå¹¶è¿›è¡Œçº¿æ€§å˜æ¢ï¼Œå¦‚Eq.
    (11) å®šä¹‰ï¼Œä»¥è·å¾—æœ€ç»ˆçš„è¾“å‡ºè¡¨ç¤ºã€‚é€šè¿‡ç»“åˆ Eq. (10)ã€(11) å’Œ (12)ï¼Œå¯ä»¥è§‚å¯Ÿåˆ°è¿™ç§ä¸²è”ä¸æŠ•å½±çš„å½¢å¼ç­‰åŒäºå¯¹é‡æ–°å‚æ•°åŒ–çš„æ³¨æ„åŠ›è¾“å‡ºè¿›è¡Œæ±‚å’Œã€‚è¿™ç§æ–¹æ³•å…è®¸é«˜æ•ˆåœ°èšåˆå¤šæ ·çš„æ³¨æ„åŠ›å¤´è¾“å‡ºï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿæ•æ‰è¾“å…¥æ•°æ®ä¸­çš„å¤æ‚ä¾èµ–å…³ç³»å’Œå…³ç³»ã€‚
- en: '![](../Images/07b6abd9b22daba4413bd081f9b09d01.png)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/07b6abd9b22daba4413bd081f9b09d01.png)'
- en: Eq. 10
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: Eq. 10
- en: and
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: å’Œ
- en: '![](../Images/c6fe42c1c28247a1f9ad3f0c572be9dc.png)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c6fe42c1c28247a1f9ad3f0c572be9dc.png)'
- en: Eq. 11
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: Eq. 11
- en: where
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­
- en: '![](../Images/9d6b8fb6740b786dda46a499be5a1038.png)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9d6b8fb6740b786dda46a499be5a1038.png)'
- en: Eq. 12
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: Eq. 12
- en: To facilitate the aggregation process, the weight matrix Wğ‘‚ âˆˆ Rğ·ğ‘š Ã—ğ·ğ‘š used for
    the linear transformation is partitioned into ğ» blocks, where ğ» represents the
    number of attention heads.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†ä¾¿äºèšåˆè¿‡ç¨‹ï¼Œçº¿æ€§å˜æ¢ä½¿ç”¨çš„æƒé‡çŸ©é˜µWğ‘‚ âˆˆ Rğ·ğ‘š Ã—ğ·ğ‘š è¢«åˆ’åˆ†ä¸ºğ»ä¸ªå—ï¼Œå…¶ä¸­ğ»è¡¨ç¤ºæ³¨æ„åŠ›å¤´çš„æ•°é‡ã€‚
- en: '![](../Images/eef0efcb393851c8ccb50c23f836825a.png)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/eef0efcb393851c8ccb50c23f836825a.png)'
- en: Eq. 13
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: Eq. 13
- en: 'The weight matrix Wğ‘‚_ğ‘–, with dimension ğ·ğ‘£ Ã— ğ·ğ‘š, is used for the linear transformation
    in each attention head, allowing for re-parameterized attention outputs through
    the concatenate-and-project formulation, as defined in Eq. (14):'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: æƒé‡çŸ©é˜µWğ‘‚_ğ‘–ï¼Œç»´åº¦ä¸ºğ·ğ‘£ Ã— ğ·ğ‘šï¼Œç”¨äºæ¯ä¸ªæ³¨æ„åŠ›å¤´ä¸­çš„çº¿æ€§å˜æ¢ï¼Œé€šè¿‡è¿æ¥-æŠ•å½±å…¬å¼é‡æ–°å‚æ•°åŒ–æ³¨æ„åŠ›è¾“å‡ºï¼Œå¦‚Eq. (14)æ‰€å®šä¹‰ï¼š
- en: '![](../Images/f2ecb33a4229e43be136ee5f3e0da112.png)'
  id: totrans-169
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f2ecb33a4229e43be136ee5f3e0da112.png)'
- en: Eq. 14
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: Eq. 14
- en: Some researchers may argue that the straightforward aggregate-by-summation approach
    may not fully leverage the expressive power of multi-head attention and that a
    more complex aggregation scheme could be more desirable.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€äº›ç ”ç©¶äººå‘˜å¯èƒ½ä¼šäº‰è¾©è¯´ï¼Œç®€å•çš„åŠ å’Œèšåˆæ–¹æ³•å¯èƒ½æ— æ³•å……åˆ†åˆ©ç”¨å¤šå¤´æ³¨æ„åŠ›çš„è¡¨è¾¾èƒ½åŠ›ï¼Œæ›´å¤æ‚çš„èšåˆæ–¹æ¡ˆå¯èƒ½æ›´ä¸ºç†æƒ³ã€‚
- en: Gu and Feng [36] and Li et al. [37] propose employing routing methods originally
    conceived for capsule networks [38] as a means to further aggregate information
    derived from distinct attention heads. Through a process of transforming the outputs
    of attention heads into input capsules and subsequently undergoing an iterative
    routing procedure, output capsules are obtained. These output capsules are then
    concatenated to serve as the final output of the multi-head attention mechanism.
    Notably, the dynamic routing [38] and EM routing [39] mechanisms employed in these
    works introduce additional parameters and computational overhead. Nevertheless,
    Li et al. [37] empirically demonstrate that selectively applying the routing mechanism
    to the lower layers of the model achieves an optimal balance between translation
    performance and computational efficiency.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: Gu å’Œ Feng [36] ä»¥åŠ Li ç­‰äºº [37] æå‡ºäº†ä½¿ç”¨æœ€åˆä¸ºèƒ¶å›Šç½‘ç»œ [38] è®¾è®¡çš„è·¯ç”±æ–¹æ³•ï¼Œä½œä¸ºè¿›ä¸€æ­¥èšåˆæ¥è‡ªä¸åŒæ³¨æ„åŠ›å¤´ä¿¡æ¯çš„ä¸€ç§æ‰‹æ®µã€‚é€šè¿‡å°†æ³¨æ„åŠ›å¤´çš„è¾“å‡ºè½¬åŒ–ä¸ºè¾“å…¥èƒ¶å›Šå¹¶éšåç»å†è¿­ä»£è·¯ç”±è¿‡ç¨‹ï¼Œè·å¾—è¾“å‡ºèƒ¶å›Šã€‚è¿™äº›è¾“å‡ºèƒ¶å›Šéšåè¢«è¿æ¥ä½œä¸ºå¤šå¤´æ³¨æ„åŠ›æœºåˆ¶çš„æœ€ç»ˆè¾“å‡ºã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œè¿™äº›å·¥ä½œä¸­ä½¿ç”¨çš„åŠ¨æ€è·¯ç”±
    [38] å’Œ EM è·¯ç”± [39] æœºåˆ¶å¼•å…¥äº†é¢å¤–çš„å‚æ•°å’Œè®¡ç®—å¼€é”€ã€‚ç„¶è€Œï¼ŒLi ç­‰äºº [37] å®è¯è¡¨æ˜ï¼Œé€‰æ‹©æ€§åœ°å°†è·¯ç”±æœºåˆ¶åº”ç”¨äºæ¨¡å‹çš„è¾ƒä½å±‚æ¬¡èƒ½åœ¨ç¿»è¯‘æ€§èƒ½å’Œè®¡ç®—æ•ˆç‡ä¹‹é—´å–å¾—æœ€ä½³å¹³è¡¡ã€‚
- en: '**3.6.4\. Other multi-head modifications:**'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '**3.6.4\. å…¶ä»–å¤šå¤´ä¿®æ”¹ï¼š**'
- en: In addition to the aforementioned modifications, several other approaches have
    been proposed to enhance the performance of the multi-head attention mechanism.
    Shazeer [40] introduced the concept of multi-query attention, where key-value
    pairs are shared among all attention heads. This reduces the memory bandwidth
    requirements during decoding and leads to faster decoding, albeit with minor quality
    degradation compared to the baseline. On the other hand, Bhojanapalli et al. [41]
    identified that the size of attention keys could impact their ability to represent
    arbitrary distributions. To address this, they proposed disentangling the head
    size from the number of heads, contrary to the conventional practice of setting
    the head size as ğ·ğ‘š/â„, where ğ·ğ‘š is the model dimension and â„ is the number of
    heads.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: é™¤äº†ä¸Šè¿°ä¿®æ”¹ä¹‹å¤–ï¼Œè¿˜æå‡ºäº†å‡ ç§å…¶ä»–æ–¹æ³•æ¥å¢å¼ºå¤šå¤´æ³¨æ„åŠ›æœºåˆ¶çš„æ€§èƒ½ã€‚Shazeer [40] å¼•å…¥äº†å¤šæŸ¥è¯¢æ³¨æ„åŠ›çš„æ¦‚å¿µï¼Œå…¶ä¸­é”®å€¼å¯¹åœ¨æ‰€æœ‰æ³¨æ„åŠ›å¤´ä¹‹é—´å…±äº«ã€‚è¿™å‡å°‘äº†è§£ç è¿‡ç¨‹ä¸­çš„å†…å­˜å¸¦å®½éœ€æ±‚ï¼Œå¹¶ä½¿è§£ç é€Ÿåº¦æ›´å¿«ï¼Œå°½ç®¡ä¸åŸºçº¿ç›¸æ¯”å­˜åœ¨è½»å¾®çš„è´¨é‡ä¸‹é™ã€‚å¦ä¸€æ–¹é¢ï¼ŒBhojanapalli
    ç­‰äºº [41] å‘ç°æ³¨æ„åŠ›é”®çš„å¤§å°å¯èƒ½ä¼šå½±å“å®ƒä»¬è¡¨ç¤ºä»»æ„åˆ†å¸ƒçš„èƒ½åŠ›ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œä»–ä»¬æå‡ºå°†å¤´çš„å¤§å°ä¸å¤´çš„æ•°é‡åˆ†å¼€ï¼Œè¿™ä¸ä¼ ç»Ÿä¸Šå°†å¤´çš„å¤§å°è®¾å®šä¸ºğ·ğ‘š/â„çš„åšæ³•ç›¸åï¼Œå…¶ä¸­ğ·ğ‘šæ˜¯æ¨¡å‹ç»´åº¦ï¼Œâ„æ˜¯å¤´çš„æ•°é‡ã€‚
- en: 4\. Summary
  id: totrans-175
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 4\. æ€»ç»“
- en: In conclusion, the taxonomy of transformers and the various advancements in
    attention mechanisms have significantly expanded the capabilities and efficiency
    of transformer-based models. Sparse attention techniques, such as position-based
    and content-based sparse attention, along with linearized attention, have addressed
    the computational limitations of traditional dense attention. Query prototyping
    and memory compression methods have introduced innovative ways to improve the
    efficiency of attention mechanisms. Low-rank self-attention has enabled parameterization
    and approximation techniques for more efficient attention computations. Incorporating
    priors, such as locality modeling, lower module priors, and multi-task adapters,
    has shown promising results in improving attention mechanisms. Lastly, modifications
    to the multi-head mechanism, such as head behavior modeling, restricted spans,
    refined aggregation, and other variations, have shown the potential to further
    enhance the performance of transformer-based models.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: æ€»ç»“æ¥è¯´ï¼Œå˜æ¢å™¨çš„åˆ†ç±»åŠæ³¨æ„æœºåˆ¶çš„å„ç§è¿›å±•æ˜¾è‘—æ‰©å±•äº†åŸºäºå˜æ¢å™¨çš„æ¨¡å‹çš„èƒ½åŠ›å’Œæ•ˆç‡ã€‚ç¨€ç–æ³¨æ„æŠ€æœ¯ï¼Œå¦‚åŸºäºä½ç½®å’ŒåŸºäºå†…å®¹çš„ç¨€ç–æ³¨æ„ï¼Œä»¥åŠçº¿æ€§åŒ–æ³¨æ„ï¼Œè§£å†³äº†ä¼ ç»Ÿç¨ å¯†æ³¨æ„çš„è®¡ç®—é™åˆ¶ã€‚æŸ¥è¯¢åŸå‹å’Œå†…å­˜å‹ç¼©æ–¹æ³•å¼•å…¥äº†åˆ›æ–°çš„æ–¹æ³•æ¥æé«˜æ³¨æ„æœºåˆ¶çš„æ•ˆç‡ã€‚ä½ç§©è‡ªæ³¨æ„ä½¿å¾—å‚æ•°åŒ–å’Œè¿‘ä¼¼æŠ€æœ¯ç”¨äºæ›´é«˜æ•ˆçš„æ³¨æ„è®¡ç®—ã€‚å¼•å…¥å…ˆéªŒï¼Œå¦‚å±€éƒ¨å»ºæ¨¡ã€è¾ƒä½æ¨¡å—å…ˆéªŒå’Œå¤šä»»åŠ¡é€‚é…å™¨ï¼Œåœ¨æé«˜æ³¨æ„æœºåˆ¶æ–¹é¢è¡¨ç°å‡ºä»¤äººé¼“èˆçš„ç»“æœã€‚æœ€åï¼Œå¯¹å¤šå¤´æœºåˆ¶çš„ä¿®æ”¹ï¼Œå¦‚å¤´è¡Œä¸ºå»ºæ¨¡ã€å—é™è·¨åº¦ã€ç²¾ç»†èšåˆåŠå…¶ä»–å˜ä½“ï¼Œå±•ç¤ºäº†è¿›ä¸€æ­¥æå‡åŸºäºå˜æ¢å™¨çš„æ¨¡å‹æ€§èƒ½çš„æ½œåŠ›ã€‚
- en: These advancements in attention mechanisms offer exciting prospects for future
    research and applications in various domains, such as natural language processing,
    computer vision, and machine translation. By leveraging these innovative techniques,
    transformer-based models can continue to push the boundaries of performance and
    efficiency, opening up new possibilities for advanced machine-learning applications.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: æ³¨æ„æœºåˆ¶çš„è¿™äº›è¿›å±•ä¸ºæœªæ¥åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ã€è®¡ç®—æœºè§†è§‰å’Œæœºå™¨ç¿»è¯‘ç­‰å¤šä¸ªé¢†åŸŸçš„ç ”ç©¶å’Œåº”ç”¨æä¾›äº†ä»¤äººå…´å¥‹çš„å‰æ™¯ã€‚é€šè¿‡åˆ©ç”¨è¿™äº›åˆ›æ–°æŠ€æœ¯ï¼ŒåŸºäºå˜æ¢å™¨çš„æ¨¡å‹å¯ä»¥ç»§ç»­çªç ´æ€§èƒ½å’Œæ•ˆç‡çš„ç•Œé™ï¼Œä¸ºé«˜çº§æœºå™¨å­¦ä¹ åº”ç”¨æ‰“å¼€æ–°çš„å¯èƒ½æ€§ã€‚
- en: '**Please share your thoughts, questions, and opinions in the comments section
    below.**'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: '**è¯·åœ¨ä¸‹æ–¹è¯„è®ºåŒºåˆ†äº«ä½ çš„æƒ³æ³•ã€é—®é¢˜å’Œæ„è§ã€‚**'
- en: '5\. Upcoming Topics: Unveiling the Next Chapters in the Transformer Journey'
  id: totrans-179
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 5\. å³å°†åˆ°æ¥çš„ä¸»é¢˜ï¼šæ­ç¤ºå˜æ¢å™¨æ—…ç¨‹ä¸­çš„ä¸‹ä¸€ä¸ªç« èŠ‚
- en: 'In future articles, the following topics will be discussed in more detail:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœªæ¥çš„æ–‡ç« ä¸­ï¼Œå°†æ›´è¯¦ç»†åœ°è®¨è®ºä»¥ä¸‹ä¸»é¢˜ï¼š
- en: '**Other Module-Level Modifications:** These cover additional modifications
    at the module level, such as position representations, layer normalization, and
    position-wise feed-forward networks (FFN), which play crucial roles in the performance
    and efficiency of transformer-based models.'
  id: totrans-181
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**å…¶ä»–æ¨¡å—çº§ä¿®æ”¹ï¼š** è¿™äº›æ¶µç›–äº†æ¨¡å—çº§çš„å…¶ä»–ä¿®æ”¹ï¼Œå¦‚ä½ç½®è¡¨ç¤ºã€å±‚å½’ä¸€åŒ–å’Œä½ç½®-wise å‰é¦ˆç½‘ç»œï¼ˆFFNï¼‰ï¼Œå®ƒä»¬åœ¨åŸºäºå˜æ¢å™¨çš„æ¨¡å‹çš„æ€§èƒ½å’Œæ•ˆç‡ä¸­å‘æŒ¥äº†è‡³å…³é‡è¦çš„ä½œç”¨ã€‚'
- en: '**Architecture-Level Variants:** This section will explore various architecture-level
    variants of transformers, including adapting transformers to be lightweight, strengthening
    cross-block connectivity, adaptive computation time, transformers with divide-and-conquer
    strategies, and exploring alternative architecture designs to further improve
    the capabilities and efficiency of transformers.'
  id: totrans-182
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**æ¶æ„çº§å˜ä½“ï¼š** æœ¬èŠ‚å°†æ¢è®¨å˜æ¢å™¨çš„å„ç§æ¶æ„çº§å˜ä½“ï¼ŒåŒ…æ‹¬å°†å˜æ¢å™¨è°ƒæ•´ä¸ºè½»é‡çº§ã€å¢å¼ºè·¨å—è¿æ¥ã€é€‚åº”æ€§è®¡ç®—æ—¶é—´ã€é‡‡ç”¨åˆ†è€Œæ²»ä¹‹ç­–ç•¥çš„å˜æ¢å™¨ï¼Œä»¥åŠæ¢ç´¢æ›¿ä»£æ¶æ„è®¾è®¡ä»¥è¿›ä¸€æ­¥æå‡å˜æ¢å™¨çš„èƒ½åŠ›å’Œæ•ˆç‡ã€‚'
- en: '**Pre-trained Transformers:** delving into pre-trained transformers, which
    have gained significant attention in recent years for their ability to leverage
    large-scale pre-training data for improving the performance of downstream tasks,
    this section will discuss different pre-training techniques, such as BERT, GPT,
    and T5.'
  id: totrans-183
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**é¢„è®­ç»ƒå˜æ¢å™¨ï¼š** æ·±å…¥æ¢è®¨é¢„è®­ç»ƒå˜æ¢å™¨ï¼Œè¿™äº›å˜æ¢å™¨è¿‘å¹´æ¥å› å…¶åˆ©ç”¨å¤§è§„æ¨¡é¢„è®­ç»ƒæ•°æ®æé«˜ä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½çš„èƒ½åŠ›è€Œå—åˆ°å¹¿æ³›å…³æ³¨ï¼Œæœ¬èŠ‚å°†è®¨è®ºä¸åŒçš„é¢„è®­ç»ƒæŠ€æœ¯ï¼Œå¦‚
    BERTã€GPT å’Œ T5ã€‚'
- en: '**Applications of Transformers:** A diverse range of applications will be highlighted
    in this part, where transformers have shown remarkable performance, including
    natural language processing, computer vision, speech recognition, and recommendation
    systems, among others. The potential and versatility of transformers in various
    domains will be discussed.'
  id: totrans-184
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**å˜å‹å™¨çš„åº”ç”¨ï¼š** åœ¨è¿™ä¸€éƒ¨åˆ†ä¸­å°†çªå‡ºæ˜¾ç¤ºå˜å‹å™¨åœ¨å„ç§é¢†åŸŸå±•ç¤ºå‡ºå“è¶Šæ€§èƒ½çš„å¤šæ ·åŒ–åº”ç”¨ï¼ŒåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€è®¡ç®—æœºè§†è§‰ã€è¯­éŸ³è¯†åˆ«å’Œæ¨èç³»ç»Ÿç­‰ã€‚å°†è®¨è®ºå˜å‹å™¨åœ¨å„ä¸ªé¢†åŸŸä¸­çš„æ½œåŠ›å’Œå¤šæ ·æ€§ã€‚'
- en: '**Research Directions:** Providing insights into the future directions of research
    and development in the field of transformers, this component discusses emerging
    trends, challenges, and opportunities for further advancements in transformer-based
    models, offering a glimpse into the exciting possibilities of transformers in
    the years to come.'
  id: totrans-185
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**ç ”ç©¶æ–¹å‘ï¼š** æä¾›å…³äºå˜å‹å™¨ç ”ç©¶å’Œå¼€å‘æœªæ¥æ–¹å‘çš„è§è§£ï¼Œè®¨è®ºå˜å‹å™¨æ¨¡å‹åœ¨æ–°å…´è¶‹åŠ¿ã€æŒ‘æˆ˜å’Œæœºä¼šæ–¹é¢çš„è¿›ä¸€æ­¥å‘å±•ï¼Œå±•ç¤ºå˜å‹å™¨åœ¨æœªæ¥å‡ å¹´é‡Œçš„ä»¤äººå…´å¥‹çš„å¯èƒ½æ€§ã€‚'
- en: By covering these topics, the article aims to provide a comprehensive overview
    of the advancements, modifications, applications, and future directions of transformers,
    shedding light on the potential of these powerful models in driving the next generation
    of machine learning and artificial intelligence applications.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: é€šè¿‡æ¶µç›–è¿™äº›ä¸»é¢˜ï¼Œæœ¬æ–‡æ—¨åœ¨å…¨é¢ä»‹ç»å˜å‹å™¨çš„è¿›å±•ã€ä¿®æ”¹ã€åº”ç”¨å’Œæœªæ¥å‘å±•æ–¹å‘ï¼Œæ­ç¤ºè¿™äº›å¼ºå¤§æ¨¡å‹åœ¨æ¨åŠ¨ä¸‹ä¸€ä»£æœºå™¨å­¦ä¹ å’Œäººå·¥æ™ºèƒ½åº”ç”¨ä¸­çš„æ½œåŠ›ã€‚
- en: 'Contributing to TransformerX: Ways to Get Involved'
  id: totrans-187
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å‚ä¸TransformerXé¡¹ç›®ï¼šå‚ä¸æ–¹å¼
- en: '[](https://github.com/tensorops/TransformerX?source=post_page-----e14952226398--------------------------------)
    [## GitHub - tensorops/TransformerX: Flexible Python library providing building
    blocks (layers) forâ€¦'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://github.com/tensorops/TransformerX?source=post_page-----e14952226398--------------------------------)
    [## GitHub - tensorops/TransformerX: Flexible Python library providing building
    blocks (layers) forâ€¦'
- en: Flexible Python library providing building blocks (layers) for reproducible
    Transformers research (Tensorflow âœ…â€¦
  id: totrans-189
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: çµæ´»çš„Pythonåº“ï¼Œä¸ºå¯é‡ç°çš„Transformerç ”ç©¶æä¾›æ„å»ºå—ï¼ˆå±‚ï¼‰ï¼ˆTensorflow âœ…â€¦
- en: github.com](https://github.com/tensorops/TransformerX?source=post_page-----e14952226398--------------------------------)
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: '[github.com](https://github.com/tensorops/TransformerX?source=post_page-----e14952226398--------------------------------)'
- en: 'I am grateful to those who have been inspiring and encouraging in developing
    the TransformerX library. We always look for your contributions in the following
    forms:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘æ„Ÿè°¢é‚£äº›åœ¨å¼€å‘TransformerXåº“ä¸­ç»™äºˆå¯å‘å’Œé¼“åŠ±çš„äººã€‚æˆ‘ä»¬ä¸€ç›´æœŸå¾…æ‚¨ä»¥ä»¥ä¸‹å½¢å¼åšå‡ºè´¡çŒ®ï¼š
- en: '**Contributing code and developing new layers:** Check out [specific issues](https://github.com/tensorops/TransformerX/issues?q=is%3Aissue+is%3Aopen+label%3A%22issue+list%22)
    labeled `issue_list`for ideas, and start implementing them with the help of our
    guides.'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**è´¡çŒ®ä»£ç å’Œå¼€å‘æ–°å±‚æ¬¡ï¼š** æŸ¥çœ‹[ç‰¹å®šé—®é¢˜](https://github.com/tensorops/TransformerX/issues?q=is%3Aissue+is%3Aopen+label%3A%22issue+list%22)æ ‡è®°ä¸º`issue_list`çš„æƒ³æ³•ï¼Œå¹¶åœ¨æˆ‘ä»¬çš„æŒ‡å—çš„å¸®åŠ©ä¸‹å¼€å§‹å®æ–½å®ƒä»¬ã€‚'
- en: '**Suggesting new features or reporting bugs:** Create issues to share your
    suggestions or report any issues you encounter.'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**å»ºè®®æ–°åŠŸèƒ½æˆ–æŠ¥å‘Šé”™è¯¯ï¼š** åˆ›å»ºé—®é¢˜æ¥åˆ†äº«ä½ çš„å»ºè®®æˆ–æŠ¥å‘Šä½ é‡åˆ°çš„ä»»ä½•é—®é¢˜ã€‚'
- en: '**Writing documentation and tutorial resources:** Help us improve documentation
    for the library.'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ç¼–å†™æ–‡æ¡£å’Œæ•™ç¨‹èµ„æºï¼š** å¸®åŠ©æˆ‘ä»¬æ”¹è¿›åº“çš„æ–‡æ¡£ã€‚'
- en: '**Sharing and writing about TransformerX on social media:** Mention us on Twitter
    for a reshare or on LinkedIn.'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**åœ¨ç¤¾äº¤åª’ä½“ä¸Šåˆ†äº«å’Œæ’°å†™TransformerXç›¸å…³å†…å®¹ï¼š** åœ¨Twitterä¸ŠæåŠæˆ‘ä»¬ï¼Œè¿›è¡Œè½¬å‘ï¼Œæˆ–åœ¨LinkedInä¸Šåˆ†äº«ã€‚'
- en: Donâ€™t worry if you are not sure about coding, We **will help you** through every
    step to make your first contribution.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä½ ä¸ç¡®å®šç¼–ç ï¼Œä¸ç”¨æ‹…å¿ƒï¼Œæˆ‘ä»¬ä¼šåœ¨æ¯ä¸€æ­¥éƒ½**å¸®åŠ©ä½ **è¿›è¡Œç¬¬ä¸€æ¬¡è´¡çŒ®ã€‚
- en: ğŸŒŸ By giving the TransformerX a star on GitHub you show your support for the
    project! Your contribution helps us continue to improve and develop this library.
    Thank you! ğŸš€
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸŒŸ åœ¨GitHubä¸Šä¸ºTransformerXç‚¹æ˜Ÿï¼Œå±•ç¤ºä½ å¯¹é¡¹ç›®çš„æ”¯æŒï¼ä½ çš„è´¡çŒ®å¸®åŠ©æˆ‘ä»¬ä¸æ–­æ”¹è¿›å’Œå‘å±•è¿™ä¸ªåº“ã€‚è°¢è°¢ï¼ğŸš€
- en: Like to see more related content?
  id: totrans-198
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å–œæ¬¢çœ‹æ›´å¤šç›¸å…³å†…å®¹å—ï¼Ÿ
- en: Follow me on [**Twitter**](https://twitter.com/soranghadri)ğŸ¦, [**GitHub**](https://github.com/soran-ghaderi)ğŸš€
    (where Iâ€™m most active), and letâ€™s connect on [**LinkedIn**](https://www.linkedin.com/in/soran-ghaderi/)ğŸ’¼
    and of course, follow my [Medium](http://soran-ghaderi.medium.com) ğŸ“, and [subscribe](https://soran-ghaderi.medium.com/subscribe)
    to my posts.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: å…³æ³¨æˆ‘çš„[**Twitter**](https://twitter.com/soranghadri)ğŸ¦ï¼Œ[**GitHub**](https://github.com/soran-ghaderi)ğŸš€ï¼ˆæˆ‘æœ€æ´»è·ƒçš„å¹³å°ï¼‰ï¼Œè®©æˆ‘ä»¬åœ¨[**LinkedIn**](https://www.linkedin.com/in/soran-ghaderi/)ğŸ’¼ä¸Šè¿æ¥ï¼Œå½“ç„¶ï¼Œå…³æ³¨æˆ‘çš„[Medium](http://soran-ghaderi.medium.com)
    ğŸ“ï¼Œå¹¶[è®¢é˜…](https://soran-ghaderi.medium.com/subscribe)æˆ‘çš„æ–‡ç« ã€‚
- en: Also, here is my [**website**](http://soran-ghaderi.github.io/).
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤å¤–ï¼Œè¿™é‡Œæ˜¯æˆ‘çš„[**ç½‘ç«™**](http://soran-ghaderi.github.io/)ã€‚
- en: References
  id: totrans-201
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å‚è€ƒæ–‡çŒ®
- en: For the list of references, please visit this [gist](https://gist.github.com/soran-ghaderi/41abae1310007bd962c7b7bb5406556a).
    This will provide you with a comprehensive list of sources that were referenced
    in this article, for further reading and in-depth information on the topic. Happy
    exploring!
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: æœ‰å…³å‚è€ƒæ–‡çŒ®çš„åˆ—è¡¨ï¼Œè¯·è®¿é—®è¿™ä¸ª[gist](https://gist.github.com/soran-ghaderi/41abae1310007bd962c7b7bb5406556a)ã€‚è¿™å°†ä¸ºæ‚¨æä¾›ä¸€ä»½å…¨é¢çš„å‚è€ƒèµ„æ–™åˆ—è¡¨ï¼Œä»¥ä¾¿è¿›ä¸€æ­¥é˜…è¯»å’Œæ·±å…¥äº†è§£è¯¥ä¸»é¢˜ã€‚ç¥æ‚¨æ¢ç´¢æ„‰å¿«ï¼
