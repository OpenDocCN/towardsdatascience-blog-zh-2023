- en: The Map Of Transformers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/the-map-of-transformers-e14952226398?source=collection_archive---------4-----------------------#2023-04-18](https://towardsdatascience.com/the-map-of-transformers-e14952226398?source=collection_archive---------4-----------------------#2023-04-18)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Transformers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A broad overview of Transformers research
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://soran-ghaderi.medium.com/?source=post_page-----e14952226398--------------------------------)[![Soran
    Ghaderi](../Images/49d2b0022c2962d8ad7af5017383374f.png)](https://soran-ghaderi.medium.com/?source=post_page-----e14952226398--------------------------------)[](https://towardsdatascience.com/?source=post_page-----e14952226398--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----e14952226398--------------------------------)
    [Soran Ghaderi](https://soran-ghaderi.medium.com/?source=post_page-----e14952226398--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fd2b75b0bb761&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-map-of-transformers-e14952226398&user=Soran+Ghaderi&userId=d2b75b0bb761&source=post_page-d2b75b0bb761----e14952226398---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----e14952226398--------------------------------)
    ·25 min read·Apr 18, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fe14952226398&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-map-of-transformers-e14952226398&user=Soran+Ghaderi&userId=d2b75b0bb761&source=-----e14952226398---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fe14952226398&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-map-of-transformers-e14952226398&source=-----e14952226398---------------------bookmark_footer-----------)![](../Images/4c821282032da4ddc63dc6024fcab1a0.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Fig. 1\. Isometric map. Designed by [vectorpocket / Freepik](http://www.freepik.com).
  prefs: []
  type: TYPE_NORMAL
- en: 1\. Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The pace of research in deep learning has accelerated significantly in recent
    years, making it increasingly difficult to keep abreast of all the latest developments.
    Despite this, there is a particular direction of investigation that has garnered
    significant attention due to its demonstrated success across a diverse range of
    domains, including natural language processing, computer vision, and audio processing.
    This is due in large part to its highly adaptable architecture. The model is called
    Transformer, and it makes use of an array of mechanisms and techniques in the
    field (i.e., attention mechanisms). You can read more about the building blocks
    and their implementation along with multiple illustrations in the following articles:'
  prefs: []
  type: TYPE_NORMAL
- en: '[](/transformers-in-action-attention-is-all-you-need-ac10338a023a?source=post_page-----e14952226398--------------------------------)
    [## Transformers in Action: Attention Is All You Need'
  prefs: []
  type: TYPE_NORMAL
- en: A brief survey, illustration, and implementation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/transformers-in-action-attention-is-all-you-need-ac10338a023a?source=post_page-----e14952226398--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: 'This article provides more details about the attention mechanisms that I will
    be talking about throughout this article:'
  prefs: []
  type: TYPE_NORMAL
- en: '[](/rethinking-thinking-how-do-attention-mechanisms-actually-work-a6f67d313f99?source=post_page-----e14952226398--------------------------------)
    [## Rethinking Thinking: How Do Attention Mechanisms Actually Work?'
  prefs: []
  type: TYPE_NORMAL
- en: The brain, the mathematics, and DL — research frontiers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/rethinking-thinking-how-do-attention-mechanisms-actually-work-a6f67d313f99?source=post_page-----e14952226398--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Taxonomy of the Transformers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A comprehensive range of models has been explored based on the vanilla Transformer
    to date, which can broadly be broken down into three categories:'
  prefs: []
  type: TYPE_NORMAL
- en: Architectural modifications
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pretraining methods
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Applications
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/7491b188970493b11bc9aac813510f92.png)'
  prefs: []
  type: TYPE_IMG
- en: Fig. 2\. Transformer variants modifications. Photo by [author](https://github.com/soran-ghaderi).
  prefs: []
  type: TYPE_NORMAL
- en: Each category above contains several other sub-categories, which I will investigate
    thoroughly in the next sections. Fig. 2\. illustrates the categories researchers
    have modified Transformers.
  prefs: []
  type: TYPE_NORMAL
- en: 3\. Attention
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Self-attention plays an elemental role in Transformer, although, it suffers
    from two main disadvantages in practice [1].
  prefs: []
  type: TYPE_NORMAL
- en: '**Complexity**: As for long sequences, this module turns into a bottleneck
    since its computational complexity is O(T²·D).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Structural prior:** It does not tackle the structural bias of the inputs
    and requires additional mechanisms to be injected into the training data which
    later it can learn (i.e. learning the order information of the input sequences).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](../Images/56cdb5c915c769faa741c72e1dd844bc.png)'
  prefs: []
  type: TYPE_IMG
- en: Fig. 3\. Categories of attention modifications and example papers. Photo by
    [author](https://www.linkedin.com/in/soran-ghaderi/).
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, researchers have explored various techniques to overcome these drawbacks.
  prefs: []
  type: TYPE_NORMAL
- en: '**Sparse attention:** This technique tries to lower the computation time and
    the memory requirements of the attention mechanism by taking a smaller portion
    of the inputs into account instead of the entire input sequence, producing a sparse
    matrix in contrast to a full matrix.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Linearized attention:** Disentangling the attention matrix using kernel feature
    maps, this method tries to compute the attention in the reverse order to reduce
    the resource requirements to linear complexity.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Prototype and memory compression:** This line of modification tries to decrease
    the queries and key-value pairs to achieve a smaller attention matrix which in
    turn reduces the time and computational complexity.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Low-rank self-attention:** By explicitly modeling the low-rank property of
    the self-attention matrix using parameterization or replacing it with a low-rank
    approximation tries to improve the performance of the transformer.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Attention with prior:** Leveraging the prior attention distribution from
    other sources, this approach, combines other attention distributions with the
    one obtained from the inputs.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Modified multi-head mechanism:** There are various ways to modify and improve
    the performance of the multi-head mechanism which can be categorized under this
    research direction.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 3.1\. Sparse attention
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The standard self-attention mechanism in a transformer requires every token
    to attend to all other tokens. However, it has been observed that in many cases,
    the attention matrix is often very sparse, meaning that only a small number of
    tokens actually attend to each other [2]. This suggests that it is possible to
    reduce the computational complexity of the self-attention mechanism by limiting
    the number of query-key pairs that each query attends to. By only computing the
    similarity scores for pre-defined patterns of query-key pairs, it is possible
    to significantly reduce the amount of computation required without sacrificing
    performance.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/91697d5a6d8416a50cbc58e164a206d9.png)'
  prefs: []
  type: TYPE_IMG
- en: Eq. 1
  prefs: []
  type: TYPE_NORMAL
- en: In the un-normalized attention matrix Â, the -∞ items are not typically stored
    in memory in order to reduce the memory footprint. This is done to decrease the
    amount of memory required to implement the matrix, which can improve the efficiency
    and performance of the system.
  prefs: []
  type: TYPE_NORMAL
- en: We can map the attention matrix to a bipartite graph where the standard attention
    mechanism can be thought of as a complete bipartite graph, where each query receives
    information from all of the nodes in the memory and uses this information to update
    its representation. In this way, the attention mechanism allows each query to
    attend to all of the other nodes in the memory and incorporate their information
    into its representation. This allows the model to capture complex relationships
    and dependencies between the nodes in the memory. The sparse attention mechanism,
    on the other hand, can be thought of as a sparse graph. This means that not all
    of the nodes in the graph are connected, which can reduce the computational complexity
    of the system and improve its efficiency and performance. By limiting the number
    of connections between nodes, the sparse attention mechanism can still capture
    important relationships and dependencies, but with less computational overhead.
  prefs: []
  type: TYPE_NORMAL
- en: There are two main classes of approaches to sparse attention, based on the metrics
    used to determine the sparse connections between nodes [1]. These are **position-based**
    and **content-based** sparse attention.
  prefs: []
  type: TYPE_NORMAL
- en: 3.1.1\. Position-based sparse attention
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this type of attention, the connections in the attention matrix are limited
    according to predetermined patterns. They can be expressed as combinations of
    simpler patterns, which can be useful for understanding and analyzing the behavior
    of the attention mechanism.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/654f80182028882725b4412ae9ed3394.png)'
  prefs: []
  type: TYPE_IMG
- en: Fig. 4\. Main atomic sparse attention patterns. The colored squares demonstrate
    correspondent calculated attention scores. Image from [[1]](https://arxiv.org/abs/2106.04554v2).
  prefs: []
  type: TYPE_NORMAL
- en: '**3.1.1.1\. Atomic sparse attention:** There are five basic atomic sparse attention
    patterns that can be used to construct a variety of different sparse attention
    mechanisms that have different trade-offs between computational complexity and
    performance as shown in Fig. 4.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Global attention:** Global nodes can be used as an information hub across
    all other nodes that can attend to all other nodes in the sequence and vice versa
    as in Fig. 4 (a).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Band attention (also sliding window attention or local attention):** The
    relationships and dependencies between different parts of the data are often local
    rather than global. In the band attention, the attention matrix is a band matrix,
    with the queries only attending to a certain number of neighboring nodes on either
    side as shown in Fig. 4 (b).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Dilated attention:** Similar to how dilated convolutional neural networks
    (CNNs) can increase the receptive field without increasing computational complexity,
    it is possible to do the same with band attention by using a dilated window with
    gaps of dilation *w_d* >= 1, as shown in Fig. 4 (c). Also, it can be extended
    to strided attention where the dilation 𝑤 𝑑 is assumed to be a large value.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Random attention:** To improve the ability of the attention mechanism to
    capture non-local interactions, a few edges can be randomly sampled for each query,
    as depicted in Fig. 4 (d).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Block local attention:** The input sequence is segmented into several non-intersecting
    query blocks, each of which is associated with a local memory block. The queries
    within each query block only attend to the keys in the corresponding memory block,
    shown in 3(e).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**3.1.1.2\. Compound sparse attention:** As illustrated in Fig. 5, many existing
    sparse attention mechanisms are composed of more than one of the atomic patterns
    described above.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/fc2ed23f12fc57d7c3cda43bcedf436c.png)'
  prefs: []
  type: TYPE_IMG
- en: Fig. 5\. Four different compound sparse attention patterns. Image from [[1]](https://arxiv.org/abs/2106.04554v2).
  prefs: []
  type: TYPE_NORMAL
- en: '**3.1.1.3\. Extended sparse attention:** There are also other types of patterns
    that have been explored for specific data types. By way of example, BP-Transformer
    [3] uses a binary tree to capture a combination of global and local attention
    across the input sequence. Tokens are leaf nodes and the internal nodes are span
    nodes containing multiple tokens. Fig. 6 shows a number of extended sparse attention
    patterns.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/33205e4eb8177520d005fdfc0cac64b5.png)'
  prefs: []
  type: TYPE_IMG
- en: Fig. 6\. Different extended sparse attention patterns. Image from [[1]](https://arxiv.org/abs/2106.04554v2).
  prefs: []
  type: TYPE_NORMAL
- en: 3.1.2\. Content-based sparse attention
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this approach, a sparse graph is constructed where the sparse connections
    are based on the inputs. It selects the keys that have high similarity scores
    with the given query. An efficient way to build this graph is to use Maximum Inner
    Product Search (MIPS) which finds the maximum dot-product between keys and the
    query without calculating all dot-products.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ef21f3839a71e0eaac3848b36f39baca.png)'
  prefs: []
  type: TYPE_IMG
- en: Fig. 7\. 2-D attention schemes for the Routing Transformer compared to local
    attention and strided attention. Image from [[4](https://arxiv.org/abs/2003.05997)]
  prefs: []
  type: TYPE_NORMAL
- en: Routing Transformer [4] as shown in Fig. 7, equips the self-attention mechanism
    with a sparse routing module by using online k-means clustering to cluster keys
    and queries on the same centroid vectors. It isolates the queries to only attend
    keys within the same cluster. Reformer [5] uses locality-sensitive hashing (LSH)
    instead of dot-product attention to select keys and values for each query. It
    enables the queries to only attend to tokens within the same bucket which are
    derived from the queries and keys using LSH. Using the LSTM edge predictor, Sparse
    Adaptive Connection (SAC) [6] constructs a graph from the input sequence and achieves
    attention edges to enhance the tasks-specific performance by leveraging an adaptive
    sparse connection.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2\. Linearized attention
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The computational complexity of the dot-product attention mechanism (softmax(QK^⊤)V)
    increases quadratically with the spatiotemporal size (length) of the input. Therefore,
    it impedes its usage when exposed to large inputs such as videos, long sequences,
    or high-resolution images. By disentangling softmax(QK^⊤) to Q′ K′^⊤, the (Q′
    K′^⊤ V) can be computed in reverse order, resulting in a linear complexity O(𝑇
    ).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4dc671f6875350317d9d0f7768061ba3.png)'
  prefs: []
  type: TYPE_IMG
- en: Fig. 8\. Standard self-attention and linearized self-attention complexity difference.
    Image from [[1](https://arxiv.org/abs/2106.04554v2)].
  prefs: []
  type: TYPE_NORMAL
- en: Assuming Â = exp(QK^⊤) denotes an un-normalized attention matrix, where exp(.)
    is applied element-wise, Linearized attention is a technique that approximates
    the un-normalized attention matrix exp(QK^⊤) with 𝜙(Q) 𝜙(K)^⊤ where 𝜙 is a row-wise
    feature map. By applying this technique, we can do 𝜙(Q) (𝜙(K)^⊤ V) which is a
    linearized computation of an un-normalized attention matrix as illustrated in
    Fig. 8.
  prefs: []
  type: TYPE_NORMAL
- en: To achieve a deeper understanding of linearized attention, I will explore the
    formulation in vector form. I will examine the general form of attention in order
    to gain further insight.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a652c4b566db4a5e99dd1caa2935f3b5.png)'
  prefs: []
  type: TYPE_IMG
- en: Eq. 2
  prefs: []
  type: TYPE_NORMAL
- en: In this context, sim(·, ·) is a scoring function that measures the similarity
    between input vectors. In the vanilla Transformer, the scoring function is the
    exponential of the inner product, exp(⟨·, ·⟩). A suitable selection for sim(·,
    ·) is a kernel function, K(x, y) = 𝜙(x)𝜙(y)^⊤ , which leads to further insights
    into the linearized attention.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c3d9b54c2ac39325feb71330701caa5d.png)'
  prefs: []
  type: TYPE_IMG
- en: Eq. 3
  prefs: []
  type: TYPE_NORMAL
- en: in this formulation, the outer product of vectors is denoted by ⊗. Attention
    can be linearized by first computing the highlighted terms which allow the autoregressive
    models i.e. transformer decoders to run like RNNs.
  prefs: []
  type: TYPE_NORMAL
- en: Eq. 2 shows that it keeps a memory matrix by aggregating associations from outer
    products of (feature-mapped) keys and queries. It later retrieves it by multiplying
    the memory matrix with the feature-mapped query with proper normalization.
  prefs: []
  type: TYPE_NORMAL
- en: 'This approach consists of two foundational components:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Feature map 𝜙 (·):** the kernel feature map for each attention implementation
    (ex. 𝜙𝑖(x) = elu(𝑥 𝑖 )+1 proposed in Linear Transformer'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Aggregation rule:** aggregating the associations {𝜙 (k)𝑗 ⊗ v𝑗} into the memory
    matrix by simple summation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 3.3\. Query prototyping and memory compression
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Aside from employing the utilization of sparse attention or kernel-based linearized
    attention, it is also feasible to mitigate the intricacy of attention through
    a decrease in the quantity of queries or key-value pairs, thereby resulting in
    the initiation of query prototypes and the implementation of memory compression
    techniques, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7c0ff26beef060b9c57b31245f7634db.png)'
  prefs: []
  type: TYPE_IMG
- en: Fig. 9\. Qurey prototyping and memory compression. Photo from [[1](https://arxiv.org/abs/2106.04554v2)]
  prefs: []
  type: TYPE_NORMAL
- en: '**3.3.1\. Attention with prototype queries:** The implementation of Attention
    with Prototype Queries involves the utilization of a set of query prototypes as
    the primary basis for computing attention distributions. The model employs two
    distinct methodologies, either by copying the computed distributions to the positions
    occupied by the represented queries, or by filling those positions with discrete
    uniform distributions. The flow of computation in this process is depicted in
    Figure 9(a).'
  prefs: []
  type: TYPE_NORMAL
- en: Clustered Attention, as described in [7], involves the aggregation of queries
    into several clusters, with attention distributions being computed for the centroids
    of these clusters. All queries within a cluster are assigned the attention distribution
    calculated for its corresponding centroid.
  prefs: []
  type: TYPE_NORMAL
- en: Informer, as outlined in [8], employs a methodology of explicit query sparsity
    measurement, derived from an approximation of the Kullback-Leibler divergence
    between the query’s attention distribution and the discrete uniform distribution,
    to select query prototypes. Attention distributions are then calculated only for
    the top-𝑢 queries as determined by the query sparsity measurement, with the remaining
    queries being assigned discrete uniform distributions.
  prefs: []
  type: TYPE_NORMAL
- en: '**3.3.2\. Attention with compressed key-value memory:** This technique reduces
    the complexity of the attention mechanism in the Transformer by reducing the number
    of key-value pairs before applying attention as shown in Fig. 9(b). This is achieved
    by compressing the key-value memory. The compressed memory is then used to compute
    attention scores. This technique can significantly reduce the computational cost
    of attention while maintaining good performance on various NLP tasks.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Liu et al. [9]* suggest a technique called *Memory Compressed Attention (MCA)*
    in their paper. *MCA* involves using strided convolution to decrease the number
    of keys and values. *MCA* is utilized alongside local attention, which is also
    proposed in the same paper. By reducing the number of keys and values by a factor
    of the kernel size, *MCA* is able to capture global context and process longer
    sequences than the standard Transformer model with the same computational resources.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Set Transformer* [10] and *Luna* [11] are two models that utilize external
    trainable global nodes to condense information from inputs. The condensed representations
    then function as a compressed memory that the inputs attend to, effectively reducing
    the quadratic complexity of self-attention to linear complexity concerning the
    length of the input sequence.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Linformer* [12] reduces the computational complexity of self-attention to
    linear by linearly projecting keys and values from the length *n* to a smaller
    length *n_k.* The setback with this approach is the pre-assumed input sequence
    length, making it unsuitable for autoregressive attention.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Poolingformer* [13] employs a two-level attention mechanism that combines
    sliding window attention with compressed memory attention. Compressed memory attention
    helps with enlarging the receptive field. To reduce the number of keys and values,
    several pooling operations are explored, including max pooling and Dynamic Convolution-based
    pooling.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.4\. Low-rank self-attention
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'According to empirical and theoretical analyses conducted by various researchers
    [14, 12], the self-attention matrix A ∈ R𝑇 ×𝑇 exhibits low-rank characteristics
    in many cases. This observation offers two implications: Firstly, the low-rank
    nature can be explicitly modeled using parameterization. This could lead to the
    development of new models that leverage this property to improve performance.
    Secondly, instead of using the full self-attention matrix, a low-rank approximation
    could be used in its place. This approach could enable more efficient computations
    and further enhance the scalability of self-attention-based models.'
  prefs: []
  type: TYPE_NORMAL
- en: '**3.4.1\. Low-rank parameterization:** When the rank of the attention matrix
    is lower than the sequence length, it suggests that over-parameterizing the model
    by setting 𝐷𝑘 > 𝑇 would lead to overfitting in situations where the input is typically
    short. Therefore, it is sensible to restrict the dimension of 𝐷𝑘 and leverage
    the low-rank property as an inductive bias. To this end, Guo et al. [14] propose
    decomposing the self-attention matrix into a low-rank attention module with a
    small 𝐷𝑘 that captures long-range non-local interactions, and a band attention
    module that captures local dependencies. This approach can be beneficial in scenarios
    where the input is short and requires effective modeling of both local and non-local
    dependencies.'
  prefs: []
  type: TYPE_NORMAL
- en: '**3.4.2\. Low-rank approximation:** The low-rank property of the attention
    matrix can also be leveraged to reduce the complexity of self-attention by using
    a low-rank matrix approximation. This methodology is closely related to the low-rank
    approximation of kernel matrices, and some existing works are inspired by kernel
    approximation. For instance, Performer, as discussed in Section 3.2, uses a random
    feature map originally proposed to approximate Gaussian kernels to decompose the
    attention distribution matrix A into C𝑄 GC𝐾, where G is a Gaussian kernel matrix
    and the random feature map approximates G.'
  prefs: []
  type: TYPE_NORMAL
- en: 'An alternative approach to dealing with the low-rank property of attention
    matrices is to use Nyström-based methods [15, 16]. In these methods, a subset
    of landmark nodes is selected from the input sequence using down-sampling techniques
    such as strided average pooling. The selected landmarks are then used as queries
    and keys to approximate the attention matrix. Specifically, the attention computation
    involves softmax normalization of the product of the original queries with the
    selected keys, followed by the product of the selected queries with the normalized
    result. This can be expressed as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/262220a7ba22b64149ec700c89c9b3a4.png)'
  prefs: []
  type: TYPE_IMG
- en: Eq. 4
  prefs: []
  type: TYPE_NORMAL
- en: Note that the inverse of the matrix **M**^-1 = (softmax(Q̃K̃^T))^-1 may not
    always exist, but this issue can be mitigated in various ways. For example, CSALR
    [15] adds an identity matrix to **M** to ensure the inverse always exists, while
    Nyström-former [16] uses the Moore-Penrose pseudoinverse of **M** to handle singular
    cases.
  prefs: []
  type: TYPE_NORMAL
- en: 3.5\. Attention with prior
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The attention mechanism is a way of focusing on specific parts of an input sequence.
    It does this by generating a weighted sum of the vectors in the sequence, where
    the weights are determined by an attention distribution. The attention distribution
    can be generated from the inputs, or it can come from other sources, such as prior
    knowledge. In most cases, the attention distribution from the inputs and the prior
    attention distribution are combined by computing a weighted sum of their scores
    before applying softmax, thus, allowing the neural network to learn from both
    the inputs and the prior knowledge.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f3d9acc3d75dd3afcdc64f246b3f0bdb.png)'
  prefs: []
  type: TYPE_IMG
- en: Fig. 10\. Attention with prior combines generated and prior attention scores
    to compute the final attention scores. Image from [[1]](https://arxiv.org/abs/2106.04554v2).
  prefs: []
  type: TYPE_NORMAL
- en: '**3.5.1\. Prior that models locality:** To model the locality of certain types
    of data like text, a Gaussian distribution over positions can be used as prior
    attention. This involves multiplying the generated attention distribution with
    a Gaussian density and renormalizing or adding a bias term G to the generated
    attention scores, where higher G indicates a higher prior probability of attending
    to a specific input.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Yang et al. [17] propose a method of predicting a central position for each
    input and defining the Gaussian bias accordingly:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/af94364ef136d40e6cb207f656871ecb.png)'
  prefs: []
  type: TYPE_IMG
- en: Eq. 5
  prefs: []
  type: TYPE_NORMAL
- en: where 𝜎 denotes the standard deviation for the Gaussian. The Gaussian bias is
    defined as the negative of the squared distance between the central position and
    the input position, divided by the standard deviation of the Gaussian distribution.
    The standard deviation can be determined as a hyperparameter or predicted from
    the inputs.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/fbbdad900d98f2d1f54606ee25c4d9bc.png)'
  prefs: []
  type: TYPE_IMG
- en: Fig. 11\. The proposed approach in [17] is illustrated, using a window size
    of 2 (D = 2). Photo from [[17](https://aclanthology.org/D18-1475/)].
  prefs: []
  type: TYPE_NORMAL
- en: The Gaussian Transformer [18] model assumes that the central position for each
    input query 𝑞𝑖 is 𝑖, and defines the bias term 𝐺𝑖 𝑗 for the generated attention
    scores as
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/00d462bccc16e5a6fdbe1ea8805bdf2b.png)'
  prefs: []
  type: TYPE_IMG
- en: Eq. 6
  prefs: []
  type: TYPE_NORMAL
- en: where 𝑤 is a non-negative scalar parameter controlling the deviation and 𝑏 is
    a negative scalar parameter reducing the weight for the central position.
  prefs: []
  type: TYPE_NORMAL
- en: '**3.5.2\. Prior from lower modules:** In Transformer architecture, attention
    distributions between adjacent layers are often found to be similar. Therefore,
    it is reasonable to use the attention distribution from a lower layer as a prior
    for computing attention in a higher layer. This can be achieved by combining the
    attention scores from the current layer with a weighted sum of the previous layer’s
    attention scores and a translation function that maps the previous scores to the
    prior to be applied.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/97d7e1995b33ec4e42a969251bb4f0eb.png)'
  prefs: []
  type: TYPE_IMG
- en: Eq. 7
  prefs: []
  type: TYPE_NORMAL
- en: 'where A(𝑙) represents the *l-*th layer attention scores while *w*1​ and *w*2​
    control the relative importance of the previous attention scores and the current
    attention scores. Also, the function 𝑔: R𝑛×𝑛 → R𝑛×𝑛 translates the previous attention
    scores into a prior to be applied to the current attention scores.'
  prefs: []
  type: TYPE_NORMAL
- en: The *Predictive Attention Transformer* proposed in the paper [19] suggests using
    a 2D-convolutional layer on the previous attention scores to compute the final
    attention scores as a convex combination of the generated attention scores and
    the convolved scores. In other words, the weight parameters for the generated
    and convolved scores are set to 𝛼 and 1-𝛼, respectively, and the function 𝑔(·)
    in Eq. (6) is a convolutional layer. The paper presents experiments showing that
    training the model from scratch and fine-tuning it after adapting a pre-trained
    BERT model both lead to improvements over baseline models.
  prefs: []
  type: TYPE_NORMAL
- en: The *Realformer* model proposed in [20] introduces a residual skip connection
    on attention maps by directly adding the previous attention scores to the newly
    generated ones. This can be seen as setting 𝑤 1 = 𝑤 2 = 1 and 𝑔(·) to be the identity
    map in Eq. (6). The authors conduct pre-training experiments on this model and
    report that it outperforms the baseline BERT model in multiple datasets, even
    with significantly lower pre-training budgets.
  prefs: []
  type: TYPE_NORMAL
- en: '*Lazyformer* [21] proposes an innovative approach where attention maps are
    shared between adjacent layers to reduce computational costs. This is achieved
    by setting 𝑔(·) to identity and alternately switching between the settings of
    𝑤 1 = 0, 𝑤 2 = 1 and 𝑤 1 = 1, 𝑤 2 = 0\. This method enables the computation of
    attention maps only once and reuses them in succeeding layers. The pre-training
    experiments conducted by Lazyformer show that their model is not only efficient
    but also effective, outperforming the baseline models with significantly lower
    computation budgets.'
  prefs: []
  type: TYPE_NORMAL
- en: '**3.5.3\. Prior as multi-task adapters:** The Prior as Multi-task Adapters
    approach uses trainable attention priors that enable efficient parameter sharing
    across tasks [22]. The Conditionally Adaptive Multi-Task Learning (CAMTL) [23]
    framework is a technique for multi-task learning that enables the efficient sharing
    of pre-trained models between tasks. CAMTL uses trainable attention prior, which
    depends on task encoding, to act as an adapter for multi-task inductive knowledge
    transfer. Specifically, the attention prior is represented as a block diagonal
    matrix that is added to the attention scores of upper layers in pre-trained Transformers:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/cca478938fd9c353b29199a52fe294f5.png)'
  prefs: []
  type: TYPE_IMG
- en: Eq. 8
  prefs: []
  type: TYPE_NORMAL
- en: in which, ⊕ represents direct sum, 𝐴𝑗 are trainable parameters with dimensions
    (𝑛/𝑚)×(𝑛/𝑚) and 𝛾𝑗 and 𝛽𝑗 are Feature Wise Linear Modulation functions with input
    and output dimensions of R𝐷𝑧 and (𝑛/𝑚)×(𝑛/𝑚), respectively [24]. The CAMTL framework
    specifies a maximum sequence length 𝑛𝑚𝑎𝑥 in implementation. The attention prior,
    which is a trainable matrix, is added to the attention scores of the upper layers
    in pre-trained Transformers. This addition creates an adapter that allows for
    parameter-efficient multi-task inductive knowledge transfer. The prior is organized
    as a block diagonal matrix for efficient computation.
  prefs: []
  type: TYPE_NORMAL
- en: '**3.5.4\. Attention with only prior:** Zhang et al. [25] have developed an
    alternative approach to attention distribution that does not rely on pair-wise
    interaction between inputs. Their method is called the “average attention network,”
    and it uses a discrete uniform distribution as the sole source of attention distribution.
    The values are then aggregated as a cumulative average of all values. To enhance
    the network’s expressiveness, a feed-forward gating layer is added on top of the
    average attention module. The benefit of this approach is that the modified Transformer
    decoder can be trained in a parallel manner, and it can decode like an RNN, avoiding
    the O(𝑇²) complexity associated with decoding.'
  prefs: []
  type: TYPE_NORMAL
- en: similar to Yang et al. [17] and Guo et al. [18], which use a fixed local window
    for attention distribution, You et al. [26] incorporate a hardcoded Gaussian distribution
    attention for attention calculation. However, They completely ignore the calculated
    attention and solely use the Gaussian distribution for attention computation in
    which, the mean and variance are the hyperparameters. Provided it is implemented
    on self-attention, it can produce results close to the baseline models in machine
    translation tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 'Synthesizer [27] has proposed a novel way of generating attention scores in
    Transformers. Instead of using the traditional method of generating attention
    scores, they replace them with two variants: (1) learnable, randomly initialized
    attention scores, and (2) attention scores output by a feed-forward network that
    is only conditioned on the input being queried. The results of their experiments
    on machine translation and language modeling tasks demonstrate that these variants
    perform comparably to the standard Transformer model. However, the reason why
    these variants work is not fully explained, leaving room for further investigation.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.6\. Improved multi-head mechanism
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Multi-head attention is a powerful technique because it allows a model to attend
    to different parts of the input simultaneously. However, it is not guaranteed
    that each attention head will learn unique and complementary features. As a result,
    some researchers have explored methods to ensure that each attention head captures
    distinct information.
  prefs: []
  type: TYPE_NORMAL
- en: '**3.6.1\. Head behavior modeling:** Multi-head attention is a useful tool in
    natural language processing models as it enables the simultaneous processing of
    multiple inputs and feature representations [28]. However, the vanilla Transformer
    model lacks a mechanism to ensure that different attention heads capture distinct
    and non-redundant features. Additionally, there is no provision for interaction
    among the heads. To address these limitations, recent research has focused on
    introducing novel mechanisms that guide the behavior of attention heads or enable
    interaction between them.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to promote diversity among different attention heads, Li et al. [29]
    propose an additional regularization term in the loss function. This regularization
    consists of two parts: the first two aim to maximize the cosine distances between
    input subspaces and output representations, while the latter encourages dispersion
    of the positions attended by multiple heads through element-wise multiplication
    of their corresponding attention matrices. By adding this auxiliary term, the
    model is encouraged to learn a more diverse set of attention patterns across different
    heads, which can improve its performance on various tasks.'
  prefs: []
  type: TYPE_NORMAL
- en: Numerous studies have shown that pre-trained Transformer models exhibit certain
    self-attention patterns that do not align well with natural language processing.
    Kovaleva et al. [30] identify several of these patterns in BERT, including attention
    heads that focus exclusively on the special tokens [CLS] and [SEP]. To improve
    training, Deshpande and Narasimhan [31] suggest using an auxiliary loss function
    that measures the Frobenius norm between the attention distribution maps and predefined
    attention patterns. This approach introduces constraints to encourage more meaningful
    attention patterns.
  prefs: []
  type: TYPE_NORMAL
- en: In the paper by Shen et al. [32], a new mechanism called Talking-head Attention
    is introduced, which aims to encourage the model to transfer information between
    different attention heads in a learnable manner. This mechanism involves linearly
    projecting the generated attention scores from the hidden dimension to a new space
    with h_k heads, applying softmax in this space, and then projecting the results
    to another space with h_v heads for value aggregation. This way, the attention
    mechanism can learn to dynamically transfer information between the different
    attention heads, leading to improved performance in various natural language processing
    tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Collaborative Multi-head Attention is a mechanism proposed in [33] that involves
    the use of shared query and key projections, denoted as W𝑄 and W𝐾, respectively,
    along with a mixing vector m𝑖. This mixing vector is used to filter the projection
    parameters for the 𝑖-th head. Specifically, the attention computation is adapted
    to reflect this mechanism, resulting in a modified equation (3).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/bc4e7c6e5518925240d2d2681e63b0a7.png)'
  prefs: []
  type: TYPE_IMG
- en: Eq. 9
  prefs: []
  type: TYPE_NORMAL
- en: where all heads share W^q and W^k.
  prefs: []
  type: TYPE_NORMAL
- en: '**3.6.2\. Multi-head with restricted spans:**'
  prefs: []
  type: TYPE_NORMAL
- en: 'The vanilla attention mechanism typically assumes full attention spans, allowing
    a query to attend to all key-value pairs. However, it has been observed that some
    attention heads tend to focus more on local contexts, while others attend to broader
    contexts. As a result, it may be advantageous to impose constraints on attention
    spans for specific purposes:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Locality: Restricting attention spans can explicitly impose local constraints,
    which can be beneficial in scenarios where locality is an important consideration.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Efficiency: Appropriately implemented, such a model can scale to longer sequences
    without introducing additional memory usage or computational time.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Restricting attention spans involves multiplying each attention distribution
    value with a mask value, followed by re-normalization. The mask value can be determined
    by a non-increasing function that maps a distance to a value in the range [0,
    1]. In vanilla attention, a mask value of 1 is assigned for all distances, as
    illustrated in Figure 12(a).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6398180ac8234a6b560c662fad7c4831.png)'
  prefs: []
  type: TYPE_IMG
- en: Fig. 12 showcases three distinct types of span masking functions denoted as
    𝑚(𝑥). The horizontal axis represents the distance 𝑥, while the vertical axis represents
    the corresponding mask value. This visual representation offers insights into
    the diverse behaviors and patterns exhibited by these masking functions, providing
    a clear visualization of how the mask values change with respect to the distance
    between key-value pairs. Image from [[1]](https://arxiv.org/abs/2106.04554v2).
  prefs: []
  type: TYPE_NORMAL
- en: In a study by Sukhbaatar et al. [34], a novel approach was proposed, introducing
    a learnable attention span that is depicted in the intriguing Figure 12(b). This
    innovative technique utilizes a mask parameterized by a learnable scalar 𝑧, combined
    with a hyperparameter 𝑅, to adaptively modulate the attention span. Remarkably,
    experimental results on character-level language modeling demonstrated that these
    adaptive-span models outperformed the baseline models while requiring significantly
    fewer FLOPS. Notably, an interesting observation was made that lower layers of
    the model tended to exhibit smaller learned spans, whereas higher layers displayed
    larger spans. This intriguing finding suggests that the model can autonomously
    learn a hierarchical composition of features, showcasing its exceptional ability
    to capture complex patterns and structures in the data.
  prefs: []
  type: TYPE_NORMAL
- en: The *Multi-Scale Transformer* [35] presents a novel approach to attention spans
    that challenges the traditional paradigm. Unlike vanilla attention, which assumes
    a uniform attention span across all heads, this innovative model introduces a
    fixed attention span with dynamic scaling in different layers. Illustrated in
    Fig. 12(c), the fixed attention span acts as a window that can be scaled up or
    down, controlled by a scale value denoted as 𝑤.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6ac644da107d37c3434399808e584639.png)'
  prefs: []
  type: TYPE_IMG
- en: Fig. 13\. Multi-Scale Multi-Head Self-Attention, where three heads are depicted,
    each representing a different scale. The blue, green, and red boxes illustrate
    the scales of ω = 1, ω = 3, and ω = 5, respectively. Photo by [author](https://www.linkedin.com/in/soran-ghaderi/).
  prefs: []
  type: TYPE_NORMAL
- en: The scale values vary, with higher layers favoring larger scales for broader
    contextual dependencies and lower layers opting for smaller scales for more localized
    attention as shown in Figure 13\. The experimental results of the Multi-Scale
    Transformer demonstrate its superior performance over baseline models on various
    tasks, showcasing its potential for more efficient and effective language processing.
  prefs: []
  type: TYPE_NORMAL
- en: '**3.6.3\. Multi-head with refined aggregation:**'
  prefs: []
  type: TYPE_NORMAL
- en: The vanilla multi-head attention mechanism, as proposed by Vaswani et al. [28],
    involves the computation of multiple attention heads that operate in parallel
    to generate individual output representations. These representations are then
    concatenated and subjected to a linear transformation, as defined in Eq. (11),
    to obtain the final output representation. By combining Eqs. (10), (11), and (12),
    it can be observed that this concatenate-and-project formulation is equivalent
    to a summation over re-parameterized attention outputs. This approach allows for
    efficient aggregation of the diverse attention head outputs, enabling the model
    to capture complex dependencies and relationships in the input data.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/07b6abd9b22daba4413bd081f9b09d01.png)'
  prefs: []
  type: TYPE_IMG
- en: Eq. 10
  prefs: []
  type: TYPE_NORMAL
- en: and
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c6fe42c1c28247a1f9ad3f0c572be9dc.png)'
  prefs: []
  type: TYPE_IMG
- en: Eq. 11
  prefs: []
  type: TYPE_NORMAL
- en: where
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9d6b8fb6740b786dda46a499be5a1038.png)'
  prefs: []
  type: TYPE_IMG
- en: Eq. 12
  prefs: []
  type: TYPE_NORMAL
- en: To facilitate the aggregation process, the weight matrix W𝑂 ∈ R𝐷𝑚 ×𝐷𝑚 used for
    the linear transformation is partitioned into 𝐻 blocks, where 𝐻 represents the
    number of attention heads.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/eef0efcb393851c8ccb50c23f836825a.png)'
  prefs: []
  type: TYPE_IMG
- en: Eq. 13
  prefs: []
  type: TYPE_NORMAL
- en: 'The weight matrix W𝑂_𝑖, with dimension 𝐷𝑣 × 𝐷𝑚, is used for the linear transformation
    in each attention head, allowing for re-parameterized attention outputs through
    the concatenate-and-project formulation, as defined in Eq. (14):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f2ecb33a4229e43be136ee5f3e0da112.png)'
  prefs: []
  type: TYPE_IMG
- en: Eq. 14
  prefs: []
  type: TYPE_NORMAL
- en: Some researchers may argue that the straightforward aggregate-by-summation approach
    may not fully leverage the expressive power of multi-head attention and that a
    more complex aggregation scheme could be more desirable.
  prefs: []
  type: TYPE_NORMAL
- en: Gu and Feng [36] and Li et al. [37] propose employing routing methods originally
    conceived for capsule networks [38] as a means to further aggregate information
    derived from distinct attention heads. Through a process of transforming the outputs
    of attention heads into input capsules and subsequently undergoing an iterative
    routing procedure, output capsules are obtained. These output capsules are then
    concatenated to serve as the final output of the multi-head attention mechanism.
    Notably, the dynamic routing [38] and EM routing [39] mechanisms employed in these
    works introduce additional parameters and computational overhead. Nevertheless,
    Li et al. [37] empirically demonstrate that selectively applying the routing mechanism
    to the lower layers of the model achieves an optimal balance between translation
    performance and computational efficiency.
  prefs: []
  type: TYPE_NORMAL
- en: '**3.6.4\. Other multi-head modifications:**'
  prefs: []
  type: TYPE_NORMAL
- en: In addition to the aforementioned modifications, several other approaches have
    been proposed to enhance the performance of the multi-head attention mechanism.
    Shazeer [40] introduced the concept of multi-query attention, where key-value
    pairs are shared among all attention heads. This reduces the memory bandwidth
    requirements during decoding and leads to faster decoding, albeit with minor quality
    degradation compared to the baseline. On the other hand, Bhojanapalli et al. [41]
    identified that the size of attention keys could impact their ability to represent
    arbitrary distributions. To address this, they proposed disentangling the head
    size from the number of heads, contrary to the conventional practice of setting
    the head size as 𝐷𝑚/ℎ, where 𝐷𝑚 is the model dimension and ℎ is the number of
    heads.
  prefs: []
  type: TYPE_NORMAL
- en: 4\. Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In conclusion, the taxonomy of transformers and the various advancements in
    attention mechanisms have significantly expanded the capabilities and efficiency
    of transformer-based models. Sparse attention techniques, such as position-based
    and content-based sparse attention, along with linearized attention, have addressed
    the computational limitations of traditional dense attention. Query prototyping
    and memory compression methods have introduced innovative ways to improve the
    efficiency of attention mechanisms. Low-rank self-attention has enabled parameterization
    and approximation techniques for more efficient attention computations. Incorporating
    priors, such as locality modeling, lower module priors, and multi-task adapters,
    has shown promising results in improving attention mechanisms. Lastly, modifications
    to the multi-head mechanism, such as head behavior modeling, restricted spans,
    refined aggregation, and other variations, have shown the potential to further
    enhance the performance of transformer-based models.
  prefs: []
  type: TYPE_NORMAL
- en: These advancements in attention mechanisms offer exciting prospects for future
    research and applications in various domains, such as natural language processing,
    computer vision, and machine translation. By leveraging these innovative techniques,
    transformer-based models can continue to push the boundaries of performance and
    efficiency, opening up new possibilities for advanced machine-learning applications.
  prefs: []
  type: TYPE_NORMAL
- en: '**Please share your thoughts, questions, and opinions in the comments section
    below.**'
  prefs: []
  type: TYPE_NORMAL
- en: '5\. Upcoming Topics: Unveiling the Next Chapters in the Transformer Journey'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In future articles, the following topics will be discussed in more detail:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Other Module-Level Modifications:** These cover additional modifications
    at the module level, such as position representations, layer normalization, and
    position-wise feed-forward networks (FFN), which play crucial roles in the performance
    and efficiency of transformer-based models.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Architecture-Level Variants:** This section will explore various architecture-level
    variants of transformers, including adapting transformers to be lightweight, strengthening
    cross-block connectivity, adaptive computation time, transformers with divide-and-conquer
    strategies, and exploring alternative architecture designs to further improve
    the capabilities and efficiency of transformers.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Pre-trained Transformers:** delving into pre-trained transformers, which
    have gained significant attention in recent years for their ability to leverage
    large-scale pre-training data for improving the performance of downstream tasks,
    this section will discuss different pre-training techniques, such as BERT, GPT,
    and T5.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Applications of Transformers:** A diverse range of applications will be highlighted
    in this part, where transformers have shown remarkable performance, including
    natural language processing, computer vision, speech recognition, and recommendation
    systems, among others. The potential and versatility of transformers in various
    domains will be discussed.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Research Directions:** Providing insights into the future directions of research
    and development in the field of transformers, this component discusses emerging
    trends, challenges, and opportunities for further advancements in transformer-based
    models, offering a glimpse into the exciting possibilities of transformers in
    the years to come.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: By covering these topics, the article aims to provide a comprehensive overview
    of the advancements, modifications, applications, and future directions of transformers,
    shedding light on the potential of these powerful models in driving the next generation
    of machine learning and artificial intelligence applications.
  prefs: []
  type: TYPE_NORMAL
- en: 'Contributing to TransformerX: Ways to Get Involved'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[](https://github.com/tensorops/TransformerX?source=post_page-----e14952226398--------------------------------)
    [## GitHub - tensorops/TransformerX: Flexible Python library providing building
    blocks (layers) for…'
  prefs: []
  type: TYPE_NORMAL
- en: Flexible Python library providing building blocks (layers) for reproducible
    Transformers research (Tensorflow ✅…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: github.com](https://github.com/tensorops/TransformerX?source=post_page-----e14952226398--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: 'I am grateful to those who have been inspiring and encouraging in developing
    the TransformerX library. We always look for your contributions in the following
    forms:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Contributing code and developing new layers:** Check out [specific issues](https://github.com/tensorops/TransformerX/issues?q=is%3Aissue+is%3Aopen+label%3A%22issue+list%22)
    labeled `issue_list`for ideas, and start implementing them with the help of our
    guides.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Suggesting new features or reporting bugs:** Create issues to share your
    suggestions or report any issues you encounter.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Writing documentation and tutorial resources:** Help us improve documentation
    for the library.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Sharing and writing about TransformerX on social media:** Mention us on Twitter
    for a reshare or on LinkedIn.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Don’t worry if you are not sure about coding, We **will help you** through every
    step to make your first contribution.
  prefs: []
  type: TYPE_NORMAL
- en: 🌟 By giving the TransformerX a star on GitHub you show your support for the
    project! Your contribution helps us continue to improve and develop this library.
    Thank you! 🚀
  prefs: []
  type: TYPE_NORMAL
- en: Like to see more related content?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Follow me on [**Twitter**](https://twitter.com/soranghadri)🐦, [**GitHub**](https://github.com/soran-ghaderi)🚀
    (where I’m most active), and let’s connect on [**LinkedIn**](https://www.linkedin.com/in/soran-ghaderi/)💼
    and of course, follow my [Medium](http://soran-ghaderi.medium.com) 📝, and [subscribe](https://soran-ghaderi.medium.com/subscribe)
    to my posts.
  prefs: []
  type: TYPE_NORMAL
- en: Also, here is my [**website**](http://soran-ghaderi.github.io/).
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For the list of references, please visit this [gist](https://gist.github.com/soran-ghaderi/41abae1310007bd962c7b7bb5406556a).
    This will provide you with a comprehensive list of sources that were referenced
    in this article, for further reading and in-depth information on the topic. Happy
    exploring!
  prefs: []
  type: TYPE_NORMAL
