- en: 'Machine Learning, Illustrated: Incremental Learning'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/machine-learning-illustrated-incremental-machine-learning-4d73747dc60c?source=collection_archive---------4-----------------------#2023-09-15](https://towardsdatascience.com/machine-learning-illustrated-incremental-machine-learning-4d73747dc60c?source=collection_archive---------4-----------------------#2023-09-15)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: How models learn new information over time, maintaining and building upon previous
    knowledge
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@shreya.rao?source=post_page-----4d73747dc60c--------------------------------)[![Shreya
    Rao](../Images/03f13be6f5f67783d32f0798f09a4f86.png)](https://medium.com/@shreya.rao?source=post_page-----4d73747dc60c--------------------------------)[](https://towardsdatascience.com/?source=post_page-----4d73747dc60c--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----4d73747dc60c--------------------------------)
    [Shreya Rao](https://medium.com/@shreya.rao?source=post_page-----4d73747dc60c--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F99b63de2f2c3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learning-illustrated-incremental-machine-learning-4d73747dc60c&user=Shreya+Rao&userId=99b63de2f2c3&source=post_page-99b63de2f2c3----4d73747dc60c---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----4d73747dc60c--------------------------------)
    ·7 min read·Sep 15, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F4d73747dc60c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learning-illustrated-incremental-machine-learning-4d73747dc60c&user=Shreya+Rao&userId=99b63de2f2c3&source=-----4d73747dc60c---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4d73747dc60c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learning-illustrated-incremental-machine-learning-4d73747dc60c&source=-----4d73747dc60c---------------------bookmark_footer-----------)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: Welcome back to the Illustrated Machine Learning series. If you read the other
    [articles in the series](https://medium.com/@shreya.rao/list/machine-learning-illustrated-dfb4532491ff),
    you know the drill. We take a (*boring sounding*) machine learning concept and
    make it fun by illustrating it! This article will cover a concept called **Incremental
    Learning**, where machine learning models learn new information over time, maintaining
    and building upon previous knowledge. But before getting into that, let’s first
    talk about what the model building process looks like today.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: We usually follow a process called **static learning** when building models.
    In this process, we train a model using the latest available data. We tweak and
    tune the model in the training process. And once we’re happy with its performance,
    we deploy it. This model is in production for a while. Then we notice that the
    model performance is getting worse over time. That’s when we throw away the existing
    model and build a new one using the latest available data. And we rinse and repeat
    this same process.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在构建模型时，我们通常遵循一种称为**静态学习**的过程。在这个过程中，我们使用最新的数据来训练模型。我们在训练过程中对模型进行调整和优化。一旦对其性能感到满意，就会部署该模型。这个模型会在生产环境中运行一段时间。然后我们注意到模型的性能随着时间的推移而变差。此时，我们就会丢弃现有的模型，使用最新的数据重新构建一个新的模型。然后，我们不断重复这个过程。
- en: '![](../Images/07e8722405e2926bf8c48eebe746363e.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/07e8722405e2926bf8c48eebe746363e.png)'
- en: Let’s illustrate this using a concrete example. Consider this hypothetical scenario.
    We started building a fraud model at the end of January 2023\. This model detects
    whether a…
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过一个具体的例子来说明这个问题。考虑一下这个假设情景。我们在2023年1月底开始构建一个欺诈检测模型。这个模型用来检测是否发生了欺诈行为……
