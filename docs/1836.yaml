- en: Decoding Strategies in Large Language Models
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 大型语言模型中的解码策略
- en: 原文：[https://towardsdatascience.com/decoding-strategies-in-large-language-models-9733a8f70539?source=collection_archive---------0-----------------------#2023-06-04](https://towardsdatascience.com/decoding-strategies-in-large-language-models-9733a8f70539?source=collection_archive---------0-----------------------#2023-06-04)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/decoding-strategies-in-large-language-models-9733a8f70539?source=collection_archive---------0-----------------------#2023-06-04](https://towardsdatascience.com/decoding-strategies-in-large-language-models-9733a8f70539?source=collection_archive---------0-----------------------#2023-06-04)
- en: A Guide to Text Generation From Beam Search to Nucleus Sampling
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从束搜索到核采样的文本生成指南
- en: '[](https://medium.com/@mlabonne?source=post_page-----9733a8f70539--------------------------------)[![Maxime
    Labonne](../Images/a7efdd305e3cc77d5509bbb1076d57d8.png)](https://medium.com/@mlabonne?source=post_page-----9733a8f70539--------------------------------)[](https://towardsdatascience.com/?source=post_page-----9733a8f70539--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----9733a8f70539--------------------------------)
    [Maxime Labonne](https://medium.com/@mlabonne?source=post_page-----9733a8f70539--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@mlabonne?source=post_page-----9733a8f70539--------------------------------)[![Maxime
    Labonne](../Images/a7efdd305e3cc77d5509bbb1076d57d8.png)](https://medium.com/@mlabonne?source=post_page-----9733a8f70539--------------------------------)[](https://towardsdatascience.com/?source=post_page-----9733a8f70539--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----9733a8f70539--------------------------------)
    [Maxime Labonne](https://medium.com/@mlabonne?source=post_page-----9733a8f70539--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fdc89da634938&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdecoding-strategies-in-large-language-models-9733a8f70539&user=Maxime+Labonne&userId=dc89da634938&source=post_page-dc89da634938----9733a8f70539---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----9733a8f70539--------------------------------)
    ·15 min read·Jun 4, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F9733a8f70539&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdecoding-strategies-in-large-language-models-9733a8f70539&user=Maxime+Labonne&userId=dc89da634938&source=-----9733a8f70539---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fdc89da634938&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdecoding-strategies-in-large-language-models-9733a8f70539&user=Maxime+Labonne&userId=dc89da634938&source=post_page-dc89da634938----9733a8f70539---------------------post_header-----------)
    发布在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----9733a8f70539--------------------------------)
    ·15分钟阅读·2023年6月4日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F9733a8f70539&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdecoding-strategies-in-large-language-models-9733a8f70539&user=Maxime+Labonne&userId=dc89da634938&source=-----9733a8f70539---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F9733a8f70539&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdecoding-strategies-in-large-language-models-9733a8f70539&source=-----9733a8f70539---------------------bookmark_footer-----------)![](../Images/8e3084bd4e009d7fb0c6ec5d7aef4aa6.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F9733a8f70539&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdecoding-strategies-in-large-language-models-9733a8f70539&source=-----9733a8f70539---------------------bookmark_footer-----------)![](../Images/8e3084bd4e009d7fb0c6ec5d7aef4aa6.png)'
- en: Image by author.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供。
- en: In the fascinating world of large language models (LLMs), much attention is
    given to model architectures, data processing, and optimization. However, decoding
    strategies like beam search, which play a crucial role in text generation, are
    often overlooked. In this article, we will explore how LLMs generate text by delving
    into the mechanics of greedy search and beam search, as well as sampling techniques
    with top-k and nucleus sampling.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在大型语言模型（LLMs）迷人的世界中，模型架构、数据处理和优化受到广泛关注。然而，像束搜索这样的解码策略在文本生成中扮演着关键角色，但常被忽视。本文将深入探讨LLMs如何生成文本，通过剖析贪婪搜索和束搜索的机制，以及使用top-k和核采样的采样技术。
- en: By the conclusion of this article, you’ll not only understand these decoding
    strategies thoroughly but also be familiar with how to handle important hyperparameters
    like temperature, num_beams, top_k, and top_p.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 到本文结束时，你将不仅彻底理解这些解码策略，还会熟悉如何处理温度、num_beams、top_k和top_p等重要超参数。
- en: The code for this article can be found on [GitHub](https://github.com/mlabonne/llm-course/blob/main/Decoding_Strategies_in_Large_Language%C2%A0Models.ipynb)
    and [Google Colab](https://colab.research.google.com/drive/19CJlOS5lI29g-B3dziNn93Enez1yiHk2?usp=sharing)
    for reference and further exploration.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 本文的代码可以在[GitHub](https://github.com/mlabonne/llm-course/blob/main/Decoding_Strategies_in_Large_Language%C2%A0Models.ipynb)和[Google
    Colab](https://colab.research.google.com/drive/19CJlOS5lI29g-B3dziNn93Enez1yiHk2?usp=sharing)中找到，供参考和进一步探索。
- en: 📚 Background
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 📚 背景
- en: To kick things off, let’s start with an example. We’ll feed the text “I have
    a dream” to a GPT-2 model and ask it to generate the next five tokens (words or
    subwords).
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 为了开始，让我们用一个例子来讲解。我们将把文本“I have a dream”输入到GPT-2模型中，并要求它生成接下来的五个标记（词或子词）。
- en: '[PRE0]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
