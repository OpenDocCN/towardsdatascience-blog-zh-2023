- en: Transformer Models For Custom Text Classification Through Fine-Tuning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/transformer-models-for-custom-text-classification-through-fine-tuning-3b065cc08da1?source=collection_archive---------0-----------------------#2023-01-20](https://towardsdatascience.com/transformer-models-for-custom-text-classification-through-fine-tuning-3b065cc08da1?source=collection_archive---------0-----------------------#2023-01-20)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: A tutorial on how to build a spam classifier (or any other classifier) by fine-tuning
    the DistilBERT model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://skanda-vivek.medium.com/?source=post_page-----3b065cc08da1--------------------------------)[![Skanda
    Vivek](../Images/9d25bee2fb75176ca7f7ea6eff7d7ab5.png)](https://skanda-vivek.medium.com/?source=post_page-----3b065cc08da1--------------------------------)[](https://towardsdatascience.com/?source=post_page-----3b065cc08da1--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----3b065cc08da1--------------------------------)
    [Skanda Vivek](https://skanda-vivek.medium.com/?source=post_page-----3b065cc08da1--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F220d9bbb8014&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransformer-models-for-custom-text-classification-through-fine-tuning-3b065cc08da1&user=Skanda+Vivek&userId=220d9bbb8014&source=post_page-220d9bbb8014----3b065cc08da1---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----3b065cc08da1--------------------------------)
    ·4 min read·Jan 20, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F3b065cc08da1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransformer-models-for-custom-text-classification-through-fine-tuning-3b065cc08da1&user=Skanda+Vivek&userId=220d9bbb8014&source=-----3b065cc08da1---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F3b065cc08da1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransformer-models-for-custom-text-classification-through-fine-tuning-3b065cc08da1&source=-----3b065cc08da1---------------------bookmark_footer-----------)![](../Images/b7152fa9d2fbe16e6c1e4d3080b00560.png)'
  prefs: []
  type: TYPE_NORMAL
- en: '[Fine-Tuned SMS Spam Classifier Model](https://huggingface.co/skandavivek2/spam-classifier)
    Output | Skanda Vivek'
  prefs: []
  type: TYPE_NORMAL
- en: 'The [DistiBERT mode](https://arxiv.org/abs/1910.01108)l was released by the
    folks at Hugging Face, as a cheaper, faster alternative to large transformer models
    like BERT. It was [originally introduced in a blog post.](https://medium.com/huggingface/distilbert-8cf3380435b5)
    The way this model works — is by using a teacher-student training approach, where
    the “student” model is a smaller version of the teacher model. Then, instead of
    training the student on the ultimate target outputs (basically one-hot encodings
    of the label class), the model is trained on the softmax outputs of the original
    “teacher model”. This is a brilliantly simple idea, and the authors show that:'
  prefs: []
  type: TYPE_NORMAL
- en: “it is possible to reduce the size of a BERT model by 40%, while retaining 97%
    of its language understanding capabilities and being 60% faster.”
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Loading and Preprocessing the Data For Classification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this example, I use the [SMS spam collection dataset](https://www.kaggle.com/datasets/uciml/sms-spam-collection-dataset)
    in the UCI Machine Learning Repository and build a classifier that detects SPAM
    vs HAM (not SPAM). The data contains 5,574…
  prefs: []
  type: TYPE_NORMAL
