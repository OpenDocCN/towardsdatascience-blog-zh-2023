- en: Transformer Models For Custom Text Classification Through Fine-Tuning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Transformer 模型用于通过微调进行自定义文本分类
- en: 原文：[https://towardsdatascience.com/transformer-models-for-custom-text-classification-through-fine-tuning-3b065cc08da1?source=collection_archive---------0-----------------------#2023-01-20](https://towardsdatascience.com/transformer-models-for-custom-text-classification-through-fine-tuning-3b065cc08da1?source=collection_archive---------0-----------------------#2023-01-20)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/transformer-models-for-custom-text-classification-through-fine-tuning-3b065cc08da1?source=collection_archive---------0-----------------------#2023-01-20](https://towardsdatascience.com/transformer-models-for-custom-text-classification-through-fine-tuning-3b065cc08da1?source=collection_archive---------0-----------------------#2023-01-20)
- en: A tutorial on how to build a spam classifier (or any other classifier) by fine-tuning
    the DistilBERT model
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 关于如何通过微调 DistilBERT 模型来构建垃圾邮件分类器（或任何其他分类器）的教程
- en: '[](https://skanda-vivek.medium.com/?source=post_page-----3b065cc08da1--------------------------------)[![Skanda
    Vivek](../Images/9d25bee2fb75176ca7f7ea6eff7d7ab5.png)](https://skanda-vivek.medium.com/?source=post_page-----3b065cc08da1--------------------------------)[](https://towardsdatascience.com/?source=post_page-----3b065cc08da1--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----3b065cc08da1--------------------------------)
    [Skanda Vivek](https://skanda-vivek.medium.com/?source=post_page-----3b065cc08da1--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://skanda-vivek.medium.com/?source=post_page-----3b065cc08da1--------------------------------)[![Skanda
    Vivek](../Images/9d25bee2fb75176ca7f7ea6eff7d7ab5.png)](https://skanda-vivek.medium.com/?source=post_page-----3b065cc08da1--------------------------------)[](https://towardsdatascience.com/?source=post_page-----3b065cc08da1--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----3b065cc08da1--------------------------------)
    [Skanda Vivek](https://skanda-vivek.medium.com/?source=post_page-----3b065cc08da1--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F220d9bbb8014&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransformer-models-for-custom-text-classification-through-fine-tuning-3b065cc08da1&user=Skanda+Vivek&userId=220d9bbb8014&source=post_page-220d9bbb8014----3b065cc08da1---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----3b065cc08da1--------------------------------)
    ·4 min read·Jan 20, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F3b065cc08da1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransformer-models-for-custom-text-classification-through-fine-tuning-3b065cc08da1&user=Skanda+Vivek&userId=220d9bbb8014&source=-----3b065cc08da1---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F220d9bbb8014&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransformer-models-for-custom-text-classification-through-fine-tuning-3b065cc08da1&user=Skanda+Vivek&userId=220d9bbb8014&source=post_page-220d9bbb8014----3b065cc08da1---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----3b065cc08da1--------------------------------)
    · 4 分钟阅读 · 2023 年 1 月 20 日'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F3b065cc08da1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransformer-models-for-custom-text-classification-through-fine-tuning-3b065cc08da1&source=-----3b065cc08da1---------------------bookmark_footer-----------)![](../Images/b7152fa9d2fbe16e6c1e4d3080b00560.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F3b065cc08da1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransformer-models-for-custom-text-classification-through-fine-tuning-3b065cc08da1&source=-----3b065cc08da1---------------------bookmark_footer-----------)![](../Images/b7152fa9d2fbe16e6c1e4d3080b00560.png)'
- en: '[Fine-Tuned SMS Spam Classifier Model](https://huggingface.co/skandavivek2/spam-classifier)
    Output | Skanda Vivek'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '[微调的 SMS 垃圾邮件分类器模型](https://huggingface.co/skandavivek2/spam-classifier) 输出
    | Skanda Vivek'
- en: 'The [DistiBERT mode](https://arxiv.org/abs/1910.01108)l was released by the
    folks at Hugging Face, as a cheaper, faster alternative to large transformer models
    like BERT. It was [originally introduced in a blog post.](https://medium.com/huggingface/distilbert-8cf3380435b5)
    The way this model works — is by using a teacher-student training approach, where
    the “student” model is a smaller version of the teacher model. Then, instead of
    training the student on the ultimate target outputs (basically one-hot encodings
    of the label class), the model is trained on the softmax outputs of the original
    “teacher model”. This is a brilliantly simple idea, and the authors show that:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '[DistiBERT 模型](https://arxiv.org/abs/1910.01108) 是 Hugging Face 团队发布的，作为大规模变换器模型（如
    BERT）的便宜且更快的替代方案。它 [最初在一篇博客文章中介绍](https://medium.com/huggingface/distilbert-8cf3380435b5)。该模型的工作方式是使用教师-学生训练方法，其中“学生”模型是教师模型的较小版本。然后，模型不是在最终目标输出（基本上是标签类别的独热编码）上进行训练，而是在原始“教师模型”的
    softmax 输出上进行训练。这是一个极其简单的想法，作者展示了：'
- en: “it is possible to reduce the size of a BERT model by 40%, while retaining 97%
    of its language understanding capabilities and being 60% faster.”
  id: totrans-10
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “可以将 BERT 模型的大小减少 40%，同时保留 97% 的语言理解能力，并且速度提高 60%。”
- en: Loading and Preprocessing the Data For Classification
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加载和预处理数据以进行分类
- en: In this example, I use the [SMS spam collection dataset](https://www.kaggle.com/datasets/uciml/sms-spam-collection-dataset)
    in the UCI Machine Learning Repository and build a classifier that detects SPAM
    vs HAM (not SPAM). The data contains 5,574…
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我使用了 [SMS 垃圾短信数据集](https://www.kaggle.com/datasets/uciml/sms-spam-collection-dataset)
    来自 UCI 机器学习库，并构建了一个分类器来检测 SPAM 与 HAM（非 SPAM）。数据包含 5,574…
