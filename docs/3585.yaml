- en: Retrieval Augmented Generation (RAG) Inference Engines with LangChain on CPUs
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 LangChain 在 CPU 上的检索增强生成（RAG）推理引擎
- en: 原文：[https://towardsdatascience.com/retrieval-augmented-generation-rag-inference-engines-with-langchain-on-cpus-d5d55f398502?source=collection_archive---------1-----------------------#2023-12-05](https://towardsdatascience.com/retrieval-augmented-generation-rag-inference-engines-with-langchain-on-cpus-d5d55f398502?source=collection_archive---------1-----------------------#2023-12-05)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/retrieval-augmented-generation-rag-inference-engines-with-langchain-on-cpus-d5d55f398502?source=collection_archive---------1-----------------------#2023-12-05](https://towardsdatascience.com/retrieval-augmented-generation-rag-inference-engines-with-langchain-on-cpus-d5d55f398502?source=collection_archive---------1-----------------------#2023-12-05)
- en: '![](../Images/1e2477ae7cf2785a1000e1cf8f7cdf54.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1e2477ae7cf2785a1000e1cf8f7cdf54.png)'
- en: Created with Nightcafe — Property of Author
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Nightcafe 创建 — 作者所有
- en: Exploring scale, fidelity, and latency in AI applications with RAG
  id: totrans-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 探索 AI 应用中的规模、保真度和延迟，利用 RAG
- en: '[](https://eduand-alvarez.medium.com/?source=post_page-----d5d55f398502--------------------------------)[![Eduardo
    Alvarez](../Images/afa0ad855c8ec2e977ebbe60dc3e77a4.png)](https://eduand-alvarez.medium.com/?source=post_page-----d5d55f398502--------------------------------)[](https://towardsdatascience.com/?source=post_page-----d5d55f398502--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----d5d55f398502--------------------------------)
    [Eduardo Alvarez](https://eduand-alvarez.medium.com/?source=post_page-----d5d55f398502--------------------------------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://eduand-alvarez.medium.com/?source=post_page-----d5d55f398502--------------------------------)[![Eduardo
    Alvarez](../Images/afa0ad855c8ec2e977ebbe60dc3e77a4.png)](https://eduand-alvarez.medium.com/?source=post_page-----d5d55f398502--------------------------------)[](https://towardsdatascience.com/?source=post_page-----d5d55f398502--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----d5d55f398502--------------------------------)
    [Eduardo Alvarez](https://eduand-alvarez.medium.com/?source=post_page-----d5d55f398502--------------------------------)'
- en: ·
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fe49cc416a8ef&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fretrieval-augmented-generation-rag-inference-engines-with-langchain-on-cpus-d5d55f398502&user=Eduardo+Alvarez&userId=e49cc416a8ef&source=post_page-e49cc416a8ef----d5d55f398502---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----d5d55f398502--------------------------------)
    ·13 min read·Dec 5, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fd5d55f398502&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fretrieval-augmented-generation-rag-inference-engines-with-langchain-on-cpus-d5d55f398502&user=Eduardo+Alvarez&userId=e49cc416a8ef&source=-----d5d55f398502---------------------clap_footer-----------)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fe49cc416a8ef&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fretrieval-augmented-generation-rag-inference-engines-with-langchain-on-cpus-d5d55f398502&user=Eduardo+Alvarez&userId=e49cc416a8ef&source=post_page-e49cc416a8ef----d5d55f398502---------------------post_header-----------)
    发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----d5d55f398502--------------------------------)
    ·13 min read·2023年12月5日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fd5d55f398502&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fretrieval-augmented-generation-rag-inference-engines-with-langchain-on-cpus-d5d55f398502&user=Eduardo+Alvarez&userId=e49cc416a8ef&source=-----d5d55f398502---------------------clap_footer-----------)'
- en: --
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fd5d55f398502&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fretrieval-augmented-generation-rag-inference-engines-with-langchain-on-cpus-d5d55f398502&source=-----d5d55f398502---------------------bookmark_footer-----------)'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fd5d55f398502&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fretrieval-augmented-generation-rag-inference-engines-with-langchain-on-cpus-d5d55f398502&source=-----d5d55f398502---------------------bookmark_footer-----------)'
- en: While Retrieval Augmented Generation (RAG) is extensively covered, particularly
    in its application to chat-based LLMs, in this article we aim to view it from
    a different perspective and analyze its prowess as a powerful operational tool.
    We will also provide a useful hands-on example to get practical experience with
    RAG-based applications. By the end of the article, you’ll develop a unique vantage
    point on RAG — understanding its role and potential in scalable inference within
    production-scale LLM deployments.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: But first, let’s refresh our understanding of inference
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Inference is the process that transforms data into predictions. This component
    of the ML lifecycle is often cradled by data pipelines that manage pre-processing
    and post-processing tasks. Let’s evaluate a practical example, consider a music
    streaming service’s recommendation system, as shown in Figure 1\. When a user
    visits the streaming platform, an intelligently curated list of top 10 songs is
    presented in the application’s interface. The recommendation system responsible
    for this list relies on a trained model and robust data pipelines to ensure a
    high-quality result.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/75837d8a56126276ae978236c03b5275.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
- en: Figure 1\. Simple diagram illustrating a recommendation system supporting “top
    10 recommended song list” functionality — image by author
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: The pre-processing stage, represented by yellow boxes in our diagram, is crucial
    for ensuring that the model’s prediction closely aligns with the user’s unique
    taste. Starting with the last 250 songs played by the user, the pipeline processes
    the data and generates a set of contextual features before passing it to a trained
    model for inference. The inference step predicts what this user might like and
    yields an output that passes to a post-processing stage (illustrated in orange).
    In this final step, the model’s top recommendations are enriched with additional
    metadata — album art, song titles, artist names, and their ranking. This information
    is then displayed on the user’s interface for consumption.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: In the workflow described above, it’s clear how proximal the inference step
    is to the user in the application topology. Unlike other AI lifecycle components
    like data collection and model optimization, which tend to operate in the background,
    the inference engine is in the frontline, interacting closely with the user interface
    (UI) and user experience (UX).
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ebe160f4764522808d7b8d4905ab88e4.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
- en: Figure 2\. An illustration of various components of an AI system and their proximity
    to user experience. — Image by author
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: We can use the diagram above (Figure 2) to illustrate the proximity of various
    components of the AI lifecycle to the user. While many components like data collection
    and annotation sit “behind the scenes,” the inference engine stands as a critical
    bridge between the AI’s internal processes and what end-users are exposed to.
    It’s not just another backend mechanism, it is a core part of the user’s tangible
    experience with the application.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用上述图（图2）来说明AI生命周期的各个组件与用户的接近程度。虽然许多组件如数据收集和标注都在“幕后”，但推理引擎则是AI内部过程与最终用户之间的关键桥梁。它不仅仅是另一个后台机制，它是用户与应用程序之间切实体验的核心部分。
- en: 'Given this critical role in shaping user experience, it is essential that the
    inference engine — encompassing the inference process and its peripheral components
    like pre/post-processing, API management, and compute management — operates flawlessly.
    To establish boundary conditions for inference engine quality, I introduce the
    “Inference Quality (IQ) Triangle,” depicted in Figure 3\. This qualitative figure
    highlights three pivotal aspects to focus on when enhancing the performance of
    an inference workload:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于推理引擎在塑造用户体验中的关键作用，推理引擎 — 包括推理过程及其外围组件，如预处理/后处理、API管理和计算管理 — 必须无缝运行。为了建立推理引擎质量的边界条件，我引入了“推理质量（IQ）三角形”，如图3所示。这个定性图形突出了在提升推理工作负载性能时需要关注的三个关键方面：
- en: '**Latency:** if the inference engine takes less time to yield a response, it
    reduces overhead for the application and leads to a better user experience.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**延迟：** 如果推理引擎响应所需的时间较短，它将减少应用程序的开销，从而改善用户体验。'
- en: '**Fidelity:** inference needs to provide answers that users can trust and feel
    confident in. This includes but is not limited to ensuring high accuracy of responses
    and reducing hallucinations.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**保真度：** 推理需要提供用户可以信任并感到自信的答案。这包括但不限于确保响应的高准确性和减少虚假信息。'
- en: '**Scalability:** As the load on the AI system fluctuates, the ability to scale
    infrastructure is key to optimizing cost and enabling the right-sizing of computing
    resources.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可扩展性：** 随着AI系统负载的波动，扩展基础设施的能力是优化成本和实现计算资源正确配置的关键。'
- en: '![](../Images/c7167e48daf98f92fa5db84d6f8758ac.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c7167e48daf98f92fa5db84d6f8758ac.png)'
- en: Figure 3\. A simplistic model for aligning three key elements of high-quality
    inference engines. A strong focus on operation excellence where AI systems need
    to scale while maintaining low latency and returning high-fidelity insights. —
    Image by author
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 图3\. 一个简单的模型，用于对齐高质量推理引擎的三个关键要素。重点关注操作卓越，AI系统需要在保持低延迟的同时进行扩展，并提供高保真度的见解。 — 作者提供的图像
- en: As we progress through the article, we will reference the IQ Triangle to dive
    deeply into how these three components — latency, fidelity, and scale — align
    well with the RAG workload.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 随着文章的深入，我们将参考IQ三角形深入探讨这三个组件——延迟、保真度和可扩展性——如何与RAG工作负载很好地对齐。
- en: Brief Introduction to Retrieval Augmented Generation
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 检索增强生成简介
- en: Retrieval augmented generation, also known as RAG, is a technique introduced
    initially by Piktus et al. (2021) in *Retrieval-Augmented Generation for Knowledge-Intensive
    NLP Tasks* and has since been adapted in various frameworks and applications.
    RAG falls under the category of in-context learning techniques, which focus on
    providing additional knowledge to pre-trained models in an effort to augment the
    quality of their responses.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 检索增强生成，也称为RAG，是由Piktus等人（2021年）在*检索增强生成用于知识密集型NLP任务*中首次引入的技术，之后在各种框架和应用中得到了适应。RAG属于上下文学习技术的范畴，重点是为预训练模型提供额外的知识，以提高其响应的质量。
- en: The hallmark of RAG is in the intelligent retrieval of additional information
    from relevant data sources, typically vector databases using algorithms like similarity
    search. The retrieved data is combined with the user’s query, enriching the input
    provided to the generative model. A standard RAG workflow is depicted in Figure
    4.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: RAG的特点在于从相关数据源中智能检索额外的信息，通常使用相似性搜索等算法的向量数据库。检索到的数据与用户的查询结合，丰富了提供给生成模型的输入。标准RAG工作流程如图4所示。
- en: '![](../Images/fdf37eb0af1e796fd7cf4098a76f8286.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fdf37eb0af1e796fd7cf4098a76f8286.png)'
- en: Figure 4\. A simple RAG workflow diagram — Image by author
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 图4\. 简单的RAG工作流程图 — 作者提供的图像
- en: 'To grasp RAG’s real value, let’s consider a practical scenario: a financial
    analyst at a major corporation (Figure 5) is grappling with the task of building
    a quarterly earnings report. Executing this task in a traditional fashion would
    be a time-intensive endeavor. LLM-based applications offer significant efficiency
    improvements, but there’s a catch — the need for up-to-date, proprietary information,
    which isn’t accessible at the time of training foundational open-source models.
    This could be partially resolved by fine-tuning but the rapid pace of business
    operations means this process would require continuous fine-tuning to keep models
    current.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解 RAG 的真正价值，让我们考虑一个实际场景：一家大型公司的金融分析师（图 5）正在进行季度收益报告的编制任务。以传统方式执行此任务将是一项耗时的工作。基于
    LLM 的应用程序提供了显著的效率提升，但有一个问题——需要最新的专有信息，而在训练基础开源模型时这些信息并不可用。这可以通过微调部分解决，但商业操作的快速节奏意味着这个过程需要持续的微调以保持模型的最新。
- en: '![](../Images/1ae0610f4167cb96bd81fe0f231940fe.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1ae0610f4167cb96bd81fe0f231940fe.png)'
- en: Figure 5\. Adapted version of Figure 6\. In this adaptation, a scenario involving
    a financial analyst’s use of RAG is represented to illustrate the workflow’s validity
    in building RAG systems based on proprietary data. — Image by author
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5\. 图 6 的改编版本。在这一改编中，涉及金融分析师使用 RAG 的场景被展示出来，以说明基于专有数据构建 RAG 系统的工作流程的有效性。——
    作者提供的图像
- en: RAG tackles these challenges by retrieving relevant, live data, allowing models
    to be dynamically updated with the latest information. This puts pressure on the
    quality of the underlying database, but at least data management is more empirical
    and predictable than LLM hallucinations and neural network extrapolation. As an
    additional bonus, this approach safeguards sensitive data within the organization’s
    data infrastructure.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: RAG 通过检索相关的实时数据来应对这些挑战，使模型能够动态更新最新信息。这对底层数据库的质量提出了压力，但至少数据管理比 LLM 幻觉和神经网络推断更具经验性和可预测性。作为额外的好处，这种方法可以保护组织数据基础设施中的敏感数据。
- en: Many applied AI engineers agree that there should be a shift toward a hybrid
    strategy focusing on periodic fine-tuning and robust RAG pipelines. In practice,
    this strategy experiences improved alignment with domain-specific tasks and increases
    model relevance in applications with fast-evolving data environments.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 许多应用 AI 工程师认为，应该转向一种混合策略，专注于周期性微调和强大的 RAG 管道。在实践中，这种策略与特定领域任务的对齐得到了改善，并增加了在数据环境快速演变的应用中的模型相关性。
- en: '*In the hands-on example, you can toggle RAG on/off to see the impacts of pre-trained
    model response quality with and without the context provided by the intelligent
    retrieval mechanisms. See Figure 10.*'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '*在实践示例中，你可以切换 RAG 的开/关，以查看提供和不提供智能检索机制上下文时，预训练模型响应质量的影响。见图 10。*'
- en: Operational RAG Systems in Support of Quality Inference Engines
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 支持高质量推理引擎的操作性 RAG 系统
- en: Now that we have a foundational understanding of RAG and its role in LLM-based
    applications, we will focus on implementing these systems’ practical and operational
    aspects.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们对 RAG 及其在基于 LLM 的应用中的作用有了基础理解，我们将专注于实施这些系统的实际和操作方面。
- en: As promised, let’s revisit the IQ Triangle (Figure 3) which underscores three
    vital aspects of high-quality operational inference engines. We will analyze the
    opportunity to address all three of these aspects — scalability, latency, and
    fidelity — using the stack illustrated below (Figure 6), which focuses on an inference
    engine composed of RAG pipelines and heavily optimized models running on CPUs.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 如承诺的，让我们重新审视 IQ 三角形（图 3），它突显了高质量操作推理引擎的三个关键方面。我们将使用下图所示的技术栈（图 6）来分析解决这三个方面——可扩展性、延迟和保真度——的机会，该技术栈专注于由
    RAG 管道组成并在 CPU 上进行高度优化的模型的推理引擎。
- en: '![](../Images/449e2b8fab0d5a6410796b6ab8cc9162.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/449e2b8fab0d5a6410796b6ab8cc9162.png)'
- en: Figure 6\. Proposed Inference Engine stack — Image my author
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6\. 提议的推理引擎技术栈 —— 作者提供的图像
- en: RAG’s Architectural Benefits
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: RAG 的架构优势
- en: RAG-based applications bring significant architectural benefits. From a **scalability**
    perspective, all data-centric components of the pipeline converge on a single
    (or few) vector databases (Figure 7), allowing the fresh data benefits of RAG
    to scale well with increasing/decreasing user requests. This unified approach
    can significantly improve the **fidelity** of responses to domain-specific tasks
    while greatly simplifying data governance.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 基于 RAG 的应用带来了显著的架构优势。从**可扩展性**的角度来看，管道中所有以数据为中心的组件都会汇聚到一个（或几个）向量数据库（见图 7），允许
    RAG 的新数据优势随着用户请求的增加/减少而良好地扩展。这种统一的方法可以显著提高对特定领域任务的**准确度**响应，同时大大简化数据治理。
- en: '![](../Images/ea79e05dd5e7c389c2453a539c83a288.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ea79e05dd5e7c389c2453a539c83a288.png)'
- en: Figure 7\. Simple Data Flow diagram showing the flow of data across a RAG-based
    AI System — Image by author
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7\. 简单数据流图，展示了基于 RAG 的 AI 系统中数据的流动 — 作者提供的图像
- en: 'Optimized Models: Efficiency and Performance'
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 优化模型：效率与性能
- en: Models can achieve smaller computational and environmental footprints through
    model compression and parameter-efficient fine-tuning techniques. While fine-tuning
    can help tailor models to specific tasks, enhancing their predictive accuracy
    (**fidelity**), compression methods like quantization can shrink model sizes,
    significantly improving inference **latency**. These lean and tuned models are
    easier to deploy in the data center and enable AI applications at the edge, opening
    the door for various innovative use cases.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 模型可以通过模型压缩和参数高效微调技术实现更小的计算和环境足迹。虽然微调可以帮助将模型调整到特定任务，从而提升其预测准确性（**准确度**），但像量化这样的压缩方法可以缩小模型的大小，显著提高推理**延迟**。这些精简和调整过的模型在数据中心部署更为便捷，并且能够在边缘计算中实现
    AI 应用，开辟了各种创新用例的可能性。
- en: '![](../Images/4062887d1a54b6abaab8b38f88894567.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4062887d1a54b6abaab8b38f88894567.png)'
- en: Figure 8 — Image by Author
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8 — 作者提供的图像
- en: CPUs in Support of RAG
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 支持 RAG 的 CPU
- en: Regarding workflows involving complex logic, like RAG, CPUs stand out for their
    ubiquity and cost-efficiency. This enables improved scale since almost any organization
    can access enterprise-grade CPUs in the cloud, unlike specialized accelerators,
    which are harder to come by.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 对于涉及复杂逻辑的工作流程，例如 RAG，CPU 因其普及性和成本效益而脱颖而出。这提高了规模，因为几乎任何组织都可以在云中访问企业级 CPU，这与更难获得的专用加速器不同。
- en: Modern CPUs also come equipped with low-level optimizations — take, for instance,
    Intel Advanced Matrix Extensions in their 4th Generation Xeon Processors — which
    improve memory management and matrix operations in deep learning training and
    inference phases. Their support for lower precision data types (such as bf16 and
    int8) makes them well-suited for achieving low **latency** during inference.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 现代 CPU 还配备了低级别的优化——例如，Intel 第四代 Xeon 处理器中的 Intel 高级矩阵扩展——这些优化改善了深度学习训练和推理阶段的内存管理和矩阵运算。它们对较低精度数据类型（如
    bf16 和 int8）的支持使其非常适合在推理过程中实现低**延迟**。
- en: Furthermore, the compatibility of CPUs with the multiple components of a RAG
    pipeline (Figure 9), including vector databases and intelligent search (for example,
    similarity search), streamlines infrastructure management, making **scaled** deployment
    more straightforward and efficient.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，CPU 与 RAG 管道（见图 9）中多个组件的兼容性，包括向量数据库和智能搜索（例如，相似度搜索），简化了基础设施管理，使得**规模化**部署更加直接和高效。
- en: '![](../Images/063e1262526782e9e5f47c365f685326.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/063e1262526782e9e5f47c365f685326.png)'
- en: Figure 9\. Adaptation of Figure 7\. Showcases the ability of CPUs to support
    various parts of the RAG system. — Image by author
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9\. 图 7 的适配。展示了 CPU 支持 RAG 系统各部分的能力。 — 作者提供的图像
- en: '*Before moving on, I must disclose my affiliation with Intel and the product
    used below. As a Senior AI Engineer at Intel, the following hands-on sample is
    run on the Intel Developer Cloud (IDC). We will use IDC as* a free and convenient
    way to access Compute to get practical experience *with the concepts described
    in this article.*'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '*在继续之前，我必须披露我与 Intel 的关系以及下面使用的产品。作为 Intel 的高级 AI 工程师，以下实践示例在 Intel 开发者云（IDC）上运行。我们将使用
    IDC 作为* 访问计算的免费且便利的方式，以便获得*与本文所述概念的实际经验。*'
- en: 'Hands-On Example: Implementing RAG with LangChain on the Intel Developer Cloud
    (IDC)'
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实践示例：在 Intel 开发者云（IDC）上实施 RAG 与 LangChain
- en: To follow along with the following hands-on example, create a free account on
    the [Intel Developer Cloud](https://bit.ly/3sTXnHt) and navigate to the “Training
    and Workshops” page. Under the *Gen AI Essentials* section, select **Retrieval
    Augmented Generation (RAG) with LangChain** option. Follow the instructions on
    the webpage to launch a JupyterLab window and automatically load the notebook
    with all of the sample code.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 要跟随以下的动手示例，请在[Intel Developer Cloud](https://bit.ly/3sTXnHt)上创建一个免费账户，并导航到“Training
    and Workshops”页面。在*Gen AI Essentials*部分中，选择**Retrieval Augmented Generation (RAG)
    with LangChain**选项。按照网页上的说明启动JupyterLab窗口，并自动加载包含所有示例代码的笔记本。
- en: The notebook includes detailed docstrings and descriptions of the code. This
    article will discuss the high-level mechanics while providing context for specific
    functions.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 该笔记本包含详细的文档字符串和代码描述。本文将讨论高级机制，并为特定功能提供背景。
- en: Setting up Dependencies
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 设置依赖项
- en: We start by installing all of the required packages into the base environment.
    You’re welcome to create your conda environment, but this is a quick and easy
    way to start.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先将所有必需的软件包安装到基础环境中。你可以创建自己的conda环境，但这是一个快速简便的开始方法。
- en: '[PRE0]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: These commands will install all the necessary packages into your base environment.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 这些命令将把所有必要的软件包安装到你的基础环境中。
- en: The Data and Model
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据和模型
- en: We will be using a quantized version of Falcon 7B (gpt4all-falcon-q4_0) from
    the GPT4All project. You can learn more about this model on the [GPT4ALL page](https://gpt4all.io/index.html)
    in the “Model Explorer” section. The model has been stored on disk to simplify
    the model access process.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用GPT4All项目中的Falcon 7B（gpt4all-falcon-q4_0）的量化版本。你可以在[GPT4ALL页面](https://gpt4all.io/index.html)的“Model
    Explorer”部分了解更多关于此模型的信息。该模型已存储在磁盘上，以简化模型访问过程。
- en: The following logic downloads the available datasets from a Hugging Face project
    called [FunDialogues](https://huggingface.co/FunDialogues). The selected data
    will be passed through an embedding model and placed in our vector database in
    a subsequent step.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 以下逻辑从一个名为[FunDialogues](https://huggingface.co/FunDialogues)的Hugging Face项目下载可用的数据集。选择的数据将通过嵌入模型处理，并在后续步骤中放入我们的向量数据库。
- en: '[PRE1]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'In the code snippet above, you can select from 4 different synthetic datasets:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在上面的代码片段中，你可以从4种不同的合成数据集中进行选择：
- en: '**Robot Maintenance**: conversations between a technician and a customer support
    agent while troubleshooting a robot arm.'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**机器人维护**：技术人员和客户支持代理在排除机器人手臂故障时的对话。'
- en: '**Basketball Coach**: conversations between basketball coaches and players
    during a game.'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**篮球教练**：篮球教练和球员在比赛中的对话。'
- en: '**Physics Professor**: conversations between students and physics professor
    during office hours.'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**物理学教授**：学生和物理学教授在办公时间的对话。'
- en: '**Grocery Cashier**: conversations between a grocery store cashier and customers'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**杂货店收银员**：杂货店收银员和顾客的对话'
- en: Configuring the Model
  id: totrans-74
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 配置模型
- en: 'The GPT4ALL extension in the LangChain API takes care of loading the model
    into memory and establishing a variety of parameters, such as:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: LangChain API中的GPT4ALL扩展负责将模型加载到内存中，并建立各种参数，如：
- en: '**model_path**: This line specifies the file path for a pre-trained model.'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**model_path**：这一行指定了预训练模型的文件路径。'
- en: '**n_threads**: Sets the number of threads to be used, which might influence
    parallel processing or inference speed. This is especially relevant for multi-core
    systems.'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**n_threads**：设置要使用的线程数，这可能会影响并行处理或推理速度。这对于多核系统特别相关。'
- en: '**max_tokens**: Limits the number of tokens (words or subwords) for the input
    or output sequences, ensuring that the data fed into or produced by the model
    does not exceed this length.'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**max_tokens**：限制输入或输出序列中的令牌（单词或子词）数量，确保输入到模型中的数据或模型生成的数据不会超出此长度。'
- en: '**repeat_penalty**: This parameter possibly penalizes repetitive content in
    the model’s output. A value greater than 1.0 prevents the model from generating
    repeated sequences.'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**repeat_penalty**：该参数可能会惩罚模型输出中的重复内容。大于1.0的值可以防止模型生成重复的序列。'
- en: '**n_batch**: Specifies the batch size for processing data. This can help optimize
    processing speed and memory usage.'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**n_batch**：指定处理数据的批量大小。这有助于优化处理速度和内存使用。'
- en: '**top_k**: Defines the “top-k” sampling strategy during the model’s generation.
    When generating text, the model will consider only the top k most probable next
    tokens.'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**top_k**：定义模型生成过程中的“top-k”采样策略。生成文本时，模型将仅考虑最可能的前k个下一个令牌。'
- en: '[PRE2]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Building the Vector Database with ChromaDB
  id: totrans-83
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用ChromaDB构建向量数据库
- en: 'The Chroma vector database is an integral part of our RAG setup, where we store
    and manage our data efficiently. Here’s how we build it:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: Chroma向量数据库是我们RAG设置的核心部分，我们在这里高效地存储和管理数据。以下是我们构建它的方法：
- en: '[PRE3]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Executing the Retrieval Mechanism
  id: totrans-86
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 执行检索机制
- en: Upon receiving a user’s query, we use similarity search to search our vector
    DB for similar data. Once a k number of matching results are found, they are retrieved
    and used to add the context to the user’s query. We use the `PromptTemplate` function
    to build a template and embed the user’s query alongside the retrieved context.
    Once the template has been populated, we move on to the inference component.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在接收到用户查询后，我们使用相似性搜索在向量数据库中查找相似数据。一旦找到k个匹配结果，它们会被检索并用于为用户的查询添加上下文。我们使用`PromptTemplate`函数构建模板，并将用户的查询与检索到的上下文嵌入其中。一旦模板填充完毕，我们继续进行推理组件。
- en: '[PRE4]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The LangChain `LLMChain` utility to execute inference based on the query passed
    by the user and the configured template. The result is returned to the user.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: LangChain `LLMChain`工具用于根据用户传递的查询和配置的模板执行推理。结果将返回给用户。
- en: '[PRE5]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Interactive Experimentation
  id: totrans-91
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 互动实验
- en: To help you get started quickly, the notebook includes integrated ipywidget
    components. You must run all the cells in the notebook to enable these components.
    We encourage you to adjust the parameters and evaluate the impact on the latency
    and fidelity of the system’s response. Remember, this is just a starting point
    and a basic demonstration of RAG’s capabilities.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 为了帮助你快速入门，笔记本包含了集成的ipywidget组件。你必须运行笔记本中的所有单元格以启用这些组件。我们鼓励你调整参数，评估对系统响应的延迟和保真度的影响。记住，这只是一个起点和RAG能力的基本演示。
- en: '![](../Images/6ff90f892d94b23b4608c11f563e7e04.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6ff90f892d94b23b4608c11f563e7e04.png)'
- en: Figure 10\. In this example, we get a quick taste of the power of RAG, clearly
    seeing the benefits of the additional context provided by RAG, which helps yield
    a helpful answer to the user’s question — “My robot is not turning on, Can you
    help me?” The RAG-enabled output provides valid recommendations, while the raw
    model without RAG simply provides a polite inquiry that is not very helpful to
    the user.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 图10。在这个示例中，我们快速体验了RAG的强大，清楚地看到RAG提供的额外上下文的好处，这有助于为用户的问题——“我的机器人没有开启，你能帮我吗？”——提供有用的答案。启用了RAG的输出提供了有效的建议，而没有RAG的原始模型仅仅提供了一个礼貌的询问，对用户帮助不大。
- en: Summary and Discussion
  id: totrans-95
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结与讨论
- en: No one wants to interact with slow, unstable chatbots that respond with bogus
    information. There are a plethora of technical stack combinations to help developers
    avoid building systems that yield terrible user experiences. In this article,
    we have interpreted the importance of inference engine quality to the user experience
    from the perspective of a stack that enables scale, fidelity, and latency benefits.
    The combination of RAG, CPUs, and model optimization techniques checks all corners
    of the IQ Triangle (Figure 3), aligning well with the needs of operational LLM-based
    AI chat applications.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 没有人愿意与响应虚假信息的慢速、不稳定聊天机器人互动。技术栈组合的选择繁多，帮助开发者避免构建产生糟糕用户体验的系统。本文从一个能够提供规模、保真度和延迟效益的技术栈角度，解读了推理引擎质量对用户体验的重要性。RAG、CPU和模型优化技术的组合涵盖了IQ三角形的各个方面（图3），很好地满足了基于LLM的AI聊天应用程序的需求。
- en: '**A few exciting things to try would be:**'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '**一些令人兴奋的尝试包括：**'
- en: Edit the prompt template found in the `retrieval_mechanism` method to engineer
    better prompts in tandem with the retrieved context.
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 编辑`retrieval_mechanism`方法中的提示模板，以便更好地设计提示，与检索到的上下文配合。
- en: Adjust the various model and RAG-specific parameters and evaluate the impact
    on inference latency and response quality.
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 调整各种模型和RAG特定参数，评估对推理延迟和响应质量的影响。
- en: Add new datasets that are meaningful to your domain and test the viability of
    using RAG to build your AI chat-based applications.
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 添加对你的领域有意义的新数据集，并测试使用RAG构建AI聊天应用程序的可行性。
- en: This example’s model (gpt4all-falcon-q4_0) is not optimized for Xeon processors.
    Explore using models that are optimized for CPU platforms and evaluate the inference
    latency benefits.
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这个示例中的模型（gpt4all-falcon-q4_0）未针对Xeon处理器进行优化。探索使用优化了CPU平台的模型，并评估推理延迟的益处。
- en: '***Thank you for reading! Don’t forget to follow*** [***my profile for more
    articles***](https://eduand-alvarez.medium.com/) ***like this!***'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '***感谢你的阅读！别忘了关注*** [***我的个人资料以获取更多类似的文章***](https://eduand-alvarez.medium.com/)
    ***！***'
