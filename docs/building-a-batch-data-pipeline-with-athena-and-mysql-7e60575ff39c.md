# ä½¿ç”¨ Athena å’Œ MySQL æ„å»ºæ‰¹é‡æ•°æ®ç®¡é“

> åŸæ–‡ï¼š[`towardsdatascience.com/building-a-batch-data-pipeline-with-athena-and-mysql-7e60575ff39c`](https://towardsdatascience.com/building-a-batch-data-pipeline-with-athena-and-mysql-7e60575ff39c)

## åˆå­¦è€…çš„ç«¯åˆ°ç«¯æ•™ç¨‹

[](https://mshakhomirov.medium.com/?source=post_page-----7e60575ff39c--------------------------------)![ğŸ’¡Mike Shakhomirov](https://mshakhomirov.medium.com/?source=post_page-----7e60575ff39c--------------------------------)[](https://towardsdatascience.com/?source=post_page-----7e60575ff39c--------------------------------)![Towards Data Science](https://towardsdatascience.com/?source=post_page-----7e60575ff39c--------------------------------) [ğŸ’¡Mike Shakhomirov](https://mshakhomirov.medium.com/?source=post_page-----7e60575ff39c--------------------------------)

Â·å‘å¸ƒäº [Towards Data Science](https://towardsdatascience.com/?source=post_page-----7e60575ff39c--------------------------------) Â·16 min é˜…è¯»Â·2023 å¹´ 10 æœˆ 20 æ—¥

--

![](img/368293b91e4bc0283007a555789b6479.png)

å›¾ç‰‡ç”± [Redd F](https://unsplash.com/@raddfilms?utm_source=medium&utm_medium=referral) æä¾›ï¼Œæ¥è‡ª [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)

åœ¨è¿™ä¸ªæ•…äº‹ä¸­ï¼Œæˆ‘å°†è®²è¿°ä¸€ç§éå¸¸æµè¡Œçš„æ•°æ®è½¬æ¢ä»»åŠ¡æ‰§è¡Œæ–¹å¼â€”â€”æ‰¹é‡æ•°æ®å¤„ç†ã€‚å½“æˆ‘ä»¬éœ€è¦ä»¥å—çŠ¶æ–¹å¼å¤„ç†æ•°æ®æ—¶ï¼Œè¿™ç§æ•°æ®ç®¡é“è®¾è®¡æ¨¡å¼å˜å¾—æå…¶æœ‰ç”¨ï¼Œéå¸¸é€‚åˆéœ€è¦è°ƒåº¦çš„ ETL ä½œä¸šã€‚æˆ‘å°†é€šè¿‡ä½¿ç”¨ MySQL å’Œ Athena æ„å»ºæ•°æ®è½¬æ¢ç®¡é“æ¥å±•ç¤ºå¦‚ä½•å®ç°è¿™ä¸€ç›®æ ‡ã€‚æˆ‘ä»¬å°†ä½¿ç”¨åŸºç¡€è®¾æ–½å³ä»£ç åœ¨äº‘ä¸­éƒ¨ç½²å®ƒã€‚

æƒ³è±¡ä¸€ä¸‹ï¼Œä½ åˆšåˆšä½œä¸ºæ•°æ®å·¥ç¨‹å¸ˆåŠ å…¥äº†ä¸€å®¶å…¬å¸ã€‚ä»–ä»¬çš„æ•°æ®å †æ ˆç°ä»£ã€äº‹ä»¶é©±åŠ¨ã€æˆæœ¬æ•ˆç›Šé«˜ã€çµæ´»ï¼Œå¹¶ä¸”å¯ä»¥è½»æ¾æ‰©å±•ä»¥æ»¡è¶³ä¸æ–­å¢é•¿çš„æ•°æ®èµ„æºã€‚ä½ æ•°æ®å¹³å°ä¸­çš„å¤–éƒ¨æ•°æ®æºå’Œæ•°æ®ç®¡é“ç”±æ•°æ®å·¥ç¨‹å›¢é˜Ÿç®¡ç†ï¼Œä½¿ç”¨å…·æœ‰ CI/CD GitHub é›†æˆçš„çµæ´»ç¯å¢ƒè®¾ç½®ã€‚

ä½œä¸ºæ•°æ®å·¥ç¨‹å¸ˆï¼Œä½ éœ€è¦åˆ›å»ºä¸€ä¸ªä¸šåŠ¡æ™ºèƒ½ä»ªè¡¨æ¿ï¼Œå±•ç¤ºå…¬å¸æ”¶å…¥æ¥æºçš„åœ°ç†åˆ†å¸ƒï¼Œå¦‚ä¸‹æ‰€ç¤ºã€‚åŸå§‹æ”¯ä»˜æ•°æ®å­˜å‚¨åœ¨æœåŠ¡å™¨æ•°æ®åº“ï¼ˆMySQLï¼‰ä¸­ã€‚ä½ æƒ³æ„å»ºä¸€ä¸ªæ‰¹é‡ç®¡é“ï¼Œä»è¯¥æ•°æ®åº“ä¸­æ¯æ—¥æå–æ•°æ®ï¼Œç„¶åä½¿ç”¨ AWS S3 å­˜å‚¨æ•°æ®æ–‡ä»¶ï¼Œå¹¶ä½¿ç”¨ Athena è¿›è¡Œå¤„ç†ã€‚

![](img/7dc86278ad5d6755486da64418c7b7bf.png)

æ”¶å…¥ä»ªè¡¨æ¿ã€‚å›¾åƒç”±ä½œè€…æä¾›ã€‚

## æ‰¹é‡æ•°æ®ç®¡é“

æ•°æ®ç®¡é“å¯ä»¥è¢«è§†ä¸ºä¸€ç³»åˆ—æ•°æ®å¤„ç†æ­¥éª¤ã€‚ç”±äºè¿™äº›é˜¶æ®µä¹‹é—´çš„***é€»è¾‘æ•°æ®æµè¿æ¥***ï¼Œæ¯ä¸ªé˜¶æ®µç”Ÿæˆçš„**è¾“å‡º**ä½œä¸ºä¸‹ä¸€ä¸ªé˜¶æ®µçš„**è¾“å…¥**ã€‚

> åªè¦åœ¨ç‚¹ A å’Œç‚¹ B ä¹‹é—´è¿›è¡Œæ•°æ®å¤„ç†ï¼Œå°±å­˜åœ¨æ•°æ®ç®¡é“ã€‚

æ•°æ®ç®¡é“å¯èƒ½å› å…¶æ¦‚å¿µå’Œé€»è¾‘æ€§è´¨è€Œæœ‰æ‰€ä¸åŒã€‚æˆ‘ä¹‹å‰åœ¨è¿™é‡Œå†™è¿‡ [1]ï¼š

[æ•°æ®ç®¡é“è®¾è®¡æ¨¡å¼](https://towardsdatascience.com/data-pipeline-design-patterns-100afa4b93e3?source=post_page-----7e60575ff39c--------------------------------)

### é€‰æ‹©åˆé€‚çš„æ¶æ„åŠå…¶ç¤ºä¾‹

[æ•°æ®ç®¡é“è®¾è®¡æ¨¡å¼](https://towardsdatascience.com/data-pipeline-design-patterns-100afa4b93e3?source=post_page-----7e60575ff39c--------------------------------)

æˆ‘ä»¬å¸Œæœ›åˆ›å»ºä¸€ä¸ªæ•°æ®ç®¡é“ï¼Œåœ¨ä»¥ä¸‹ **æ­¥éª¤** ä¸­è½¬æ¢æ•°æ®ï¼š

1\. ä½¿ç”¨ Lambda å‡½æ•°å°†æ•°æ®ä» MySQL æ•°æ®åº“è¡¨ `myschema.users` å’Œ `myschema.transactions` æå–åˆ° S3 æ•°æ®æ¹–æ¡¶ä¸­ã€‚

2\. æ·»åŠ ä¸€ä¸ªå…·æœ‰ Athena èµ„æºçš„çŠ¶æ€æœºèŠ‚ç‚¹ä»¥å¯åŠ¨æ‰§è¡Œ (`arn:aws:states:::athena:startQueryExecution.sync`) å¹¶åˆ›å»ºä¸€ä¸ªåä¸º `mydatabase` çš„æ•°æ®åº“

3\. åˆ›å»ºå¦ä¸€ä¸ªæ•°æ®ç®¡é“èŠ‚ç‚¹ä»¥æ˜¾ç¤º Athena æ•°æ®åº“ä¸­çš„ç°æœ‰è¡¨ã€‚ä½¿ç”¨è¯¥èŠ‚ç‚¹çš„è¾“å‡ºæ‰§è¡Œæ‰€éœ€çš„æ•°æ®è½¬æ¢ã€‚

å¦‚æœè¡¨ä¸å­˜åœ¨ï¼Œæˆ‘ä»¬å¸Œæœ›æˆ‘ä»¬çš„ç®¡é“åœ¨ Athena ä¸­æ ¹æ®æ¥è‡ªæ•°æ®æ¹– S3 æ¡¶çš„æ•°æ®åˆ›å»ºå®ƒä»¬ã€‚æˆ‘ä»¬å¸Œæœ›åˆ›å»ºä¸¤ä¸ª **å¤–éƒ¨è¡¨**ï¼Œæ•°æ®æ¥è‡ª MySQLï¼š

+   mydatabase.users (LOCATION â€˜s3://<YOUR_DATALAKE_BUCKET>/data/myschema/users/â€™)

+   mydatabase.transactions (LOCATION â€˜s3://<YOUR_DATALAKE_BUCKET>/data/myschema/transactions/â€™)

ç„¶åæˆ‘ä»¬å¸Œæœ›åˆ›å»ºä¸€ä¸ª **ä¼˜åŒ–çš„ ICEBERG** è¡¨ï¼š

+   mydatabase.user_transactions (â€˜table_typeâ€™=â€™ICEBERGâ€™, â€˜formatâ€™=â€™parquetâ€™) ä½¿ç”¨ä»¥ä¸‹ SQLï¼š

```py
SELECT 
      date(dt) dt
    , user_id
    , sum(total_cost) total_cost_usd
    , registration_date
  FROM mydatabase.transactions 
  LEFT JOIN mydatabase.users
  ON users.id = transactions.user_id
  GROUP BY
    dt
    , user_id
    , registration_date
;
```

+   æˆ‘ä»¬è¿˜å°†ä½¿ç”¨ MERGE æ¥æ›´æ–°æ­¤è¡¨ã€‚

MERGE æ˜¯ä¸€ç§éå¸¸æœ‰ç”¨çš„ SQL æŠ€å·§ï¼Œç”¨äºè¡¨ä¸­çš„å¢é‡æ›´æ–°ã€‚æŸ¥çœ‹æˆ‘ä¹‹å‰çš„æ•…äº‹ [3] ä»¥è·å–æ›´é«˜çº§çš„ç¤ºä¾‹ï¼š

[é«˜çº§ SQL æŠ€å·§](https://towardsdatascience.com/advanced-sql-techniques-for-beginners-211851a28488?source=post_page-----7e60575ff39c--------------------------------)

### ä» 1 åˆ° 10ï¼Œä½ çš„æ•°æ®ä»“åº“æŠ€èƒ½æœ‰å¤šå¥½ï¼Ÿ

[é«˜çº§ SQL æŠ€å·§](https://towardsdatascience.com/advanced-sql-techniques-for-beginners-211851a28488?source=post_page-----7e60575ff39c--------------------------------)

Athena å¯ä»¥é€šè¿‡è¿è¡Œæœ‰å¸å¼•åŠ›çš„å³å¸­ SQL æŸ¥è¯¢æ¥åˆ†æå­˜å‚¨åœ¨ Amazon S3 ä¸­çš„ç»“æ„åŒ–ã€éç»“æ„åŒ–å’ŒåŠç»“æ„åŒ–æ•°æ®ï¼Œæ— éœ€ç®¡ç†åŸºç¡€è®¾æ–½ã€‚

> æˆ‘ä»¬ä¸éœ€è¦åŠ è½½æ•°æ®ï¼Œè¿™ä½¿å¾—å®ƒæˆä¸ºæˆ‘ä»¬ä»»åŠ¡çš„å®Œç¾é€‰æ‹©ã€‚

å®ƒå¯ä»¥è½»æ¾åœ°ä¸ Business Intelligence (BI) è§£å†³æ–¹æ¡ˆå¦‚ QuickSight é›†æˆä»¥ç”ŸæˆæŠ¥å‘Šã€‚

ICEBERG æ˜¯ä¸€ç§éå¸¸æœ‰ç”¨ä¸”é«˜æ•ˆçš„è¡¨æ ¼æ ¼å¼ï¼Œå¤šä¸ªç‹¬ç«‹ç¨‹åºå¯ä»¥åŒæ—¶ä¸”ä¸€è‡´åœ°å¤„ç†ç›¸åŒçš„æ•°æ®é›† [2]ã€‚æˆ‘ä¹‹å‰åœ¨è¿™é‡Œå†™è¿‡ï¼š

[ä»‹ç» Apache Iceberg è¡¨](https://towardsdatascience.com/introduction-to-apache-iceberg-tables-a791f1758009?source=post_page-----7e60575ff39c--------------------------------)

### é€‰æ‹© Apache Iceberg ä½œä¸ºæ•°æ®æ¹–çš„å‡ ä¸ªæœ‰åŠ›ç†ç”±

[ä»‹ç» Apache Iceberg è¡¨](https://towardsdatascience.com/introduction-to-apache-iceberg-tables-a791f1758009?source=post_page-----7e60575ff39c--------------------------------)

## MySQL æ•°æ®è¿æ¥å™¨

è®©æˆ‘ä»¬åˆ›å»ºä¸€ä¸ª AWS Lambda å‡½æ•°ï¼Œå®ƒèƒ½å¤Ÿåœ¨ MySQL æ•°æ®åº“ä¸­æ‰§è¡Œ SQL æŸ¥è¯¢ã€‚

> ä»£ç éå¸¸ç®€å•ä¸”é€šç”¨ã€‚å®ƒå¯ä»¥åœ¨ä»»ä½•æ— æœåŠ¡å™¨åº”ç”¨ç¨‹åºä¸­ä¸ä»»ä½•äº‘æœåŠ¡æä¾›å•†ä¸€èµ·ä½¿ç”¨ã€‚

æˆ‘ä»¬å°†ä½¿ç”¨å®ƒå°†æ”¶å…¥æ•°æ®æå–åˆ°æ•°æ®æ¹–ä¸­ã€‚å»ºè®®çš„ Lambda æ–‡ä»¶å¤¹ç»“æ„å¦‚ä¸‹æ‰€ç¤ºï¼š

```py
.
â””â”€â”€ stack
    â”œâ”€â”€ mysql_connector
    â”‚   â”œâ”€â”€ config       # config folder with environment related settings
    â”‚   â”œâ”€â”€ populate_database.sql  # sql script to create source tables
    â”‚   â”œâ”€â”€ export.sql   # sql script to export data to s3 datalake
    â”‚   â””â”€â”€ app.py       # main application file
    â”œâ”€â”€ package          # required libraries
    â”‚   â”œâ”€â”€ PyMySQL-1.0.2.dist-info
    â”‚   â””â”€â”€ pymysql
    â”œâ”€â”€ requirements.txt # required Python modules
    â””â”€â”€ stack.zip        # Lambda package
```

æˆ‘ä»¬å°†é€šè¿‡ AWS Step Functions å°†è¿™ä¸ªå°æœåŠ¡é›†æˆåˆ°ç®¡é“ä¸­ï¼Œä»¥ä¾¿äº **ç¼–æ’å’Œå¯è§†åŒ–**ã€‚

ä¸ºäº†åˆ›å»ºä¸€ä¸ªèƒ½å¤Ÿä» MySQL æ•°æ®åº“ä¸­æå–æ•°æ®çš„ Lambda å‡½æ•°ï¼Œæˆ‘ä»¬éœ€è¦å…ˆä¸ºæˆ‘ä»¬çš„ Lambda åˆ›å»ºä¸€ä¸ªæ–‡ä»¶å¤¹ã€‚é¦–å…ˆåˆ›å»ºä¸€ä¸ªåä¸º stack` çš„æ–°æ–‡ä»¶å¤¹ï¼Œç„¶ååœ¨å…¶ä¸­åˆ›å»ºä¸€ä¸ªåä¸º `mysql_connector` çš„æ–‡ä»¶å¤¹ï¼š

```py
mkdir stack
cd stack
mkdir mysql_connector
```

ç„¶åæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ä¸‹é¢çš„ä»£ç ï¼ˆå°†æ•°æ®åº“è¿æ¥è®¾ç½®æ›¿æ¢ä¸ºä½ çš„è®¾ç½®ï¼‰æ¥åˆ›å»º `app.py`ï¼š

```py
 import os
import sys
import yaml
import logging
import pymysql

from datetime import datetime
import pytz

ENV = os.environ['ENV']
TESTING = os.environ['TESTING']
LAMBDA_PATH = os.environ['LAMBDA_PATH']
print('ENV: {}, Running locally: {}'.format(ENV, TESTING))

def get_work_dir(testing):
    if (testing == 'true'):
        return LAMBDA_PATH
    else:
        return '/var/task/' + LAMBDA_PATH

def get_settings(env, path):
    if (env == 'staging'):
        with open(path + "config/staging.yaml", "r") as f:
            config = yaml.load(f, Loader=yaml.FullLoader)
    elif (env == 'live'):
        with open(path + "config/production.yaml", "r") as f:
            config = yaml.load(f, Loader=yaml.FullLoader)
    elif (env == 'test'):
        with open(path + "config/test.yaml", "r") as f:
            config = yaml.load(f, Loader=yaml.FullLoader)
    else:
        print('No config found')
    return config

work_dir = get_work_dir(TESTING)
print('LAMBDA_PATH: {}'.format(work_dir))
config=get_settings(ENV, work_dir)
print(config)
DATA_S3 = config.get('S3dataLocation') # i.e. datalake.staging.something. Replace it with your unique bucket name.

logger = logging.getLogger()
logger.setLevel(logging.INFO)

# rds settings
rds_host  = config.get('Mysql')['rds_host'] # i.e. "mymysqldb.12345.eu-west-1.rds.amazonaws.com"
user_name = "root"
password = "AmazingPassword"
db_name = "mysql"

# create the database connection outside of the handler to allow connections to be
# re-used by subsequent function invocations.
try:
    conn = pymysql.connect(host=rds_host, user=user_name, passwd=password, db=db_name, connect_timeout=5)

except pymysql.MySQLError as e:
    logger.error("ERROR: Unexpected error: Could not connect to MySQL instance.")
    logger.error(e)
    sys.exit()

logger.info("SUCCESS: Connection to RDS MySQL instance succeeded")

def lambda_handler(event, context):
    processed = 0
    print("")
    try:
        _populate_db()
        _export_to_s3()
    except Exception as e:
        print(e)
    message = 'Successfully populated the database and created an export job.'
    return {
        'statusCode': 200,
        'body': { 'lambdaResult': message }
    }

# Helpers:

def _now():
    return datetime.utcnow().replace(tzinfo=pytz.utc).strftime('%Y-%m-%dT%H:%M:%S.%f')

def _populate_db():
    try:
        # Generate data and populate database:
        fd = open(work_dir + '/populate_database.sql', 'r')
        sqlFile = fd.read()
        fd.close()
        sqlCommands = sqlFile.split(';')
        # Execute every command from the input file
        for command in sqlCommands:
            try:
                with conn.cursor() as cur:
                    cur.execute(command)
                    print('---')
                    print(command)
            except Exception as e:
                print(e)

    except Exception as e:
        print(e)

def _export_to_s3():
    try:
        # Generate data and populate database:
        fd = open(work_dir + '/export.sql', 'r')
        sqlFile = fd.read()
        fd.close()
        sqlCommands = sqlFile.split(';')
        # Execute every command from the input file
        for command in sqlCommands:
            try:
                with conn.cursor() as cur:
                    cur.execute(command.replace("{{DATA_S3}}", DATA_S3))
                    print('---')
                    print(command)
            except Exception as e:
                print(e)

    except Exception as e:
        print(e)
```

è¦ä½¿ç”¨ AWS CLI éƒ¨ç½²æˆ‘ä»¬çš„å¾®æœåŠ¡ï¼Œè¯·åœ¨å‘½ä»¤è¡Œä¸­è¿è¡Œä»¥ä¸‹å‘½ä»¤ï¼ˆå‡è®¾ä½ åœ¨ ./stack æ–‡ä»¶å¤¹ä¸­ï¼‰ï¼š

```py
# Package Lambda code:
base=${PWD##*/}
zp=$base".zip" # This will return stack.zip if you are in stack folder.
echo $zp

rm -f $zp # remove old package if exists

pip install --target ./package pymysql 

cd package
zip -r ../${base}.zip .

cd $OLDPWD
zip -r $zp ./mysql_connector
```

ç¡®ä¿åœ¨è¿è¡Œä¸‹ä¸€éƒ¨åˆ†ä¹‹å‰ AWS Lambda è§’è‰²å·²ç»å­˜åœ¨ ` â€” role arn:aws:iam::<your-aws-account-id>:role/my-lambda-role`ã€‚

```py
# Deploy packaged Lambda using AWS CLI:
aws \
lambda create-function \
--function-name mysql-lambda \
--zip-file fileb://stack.zip \
--handler <path-to-your-lambda-handler>/app.lambda_handler \
--runtime python3.12 \
--role arn:aws:iam::<your-aws-account-id>:role/my-lambda-role

# # If already deployed then use this to update:
# aws --profile mds lambda update-function-code \
# --function-name mysql-lambda \
# --zip-file fileb://stack.zip;
```

æˆ‘ä»¬çš„ MySQL å®ä¾‹å¿…é¡»å…·å¤‡ **S3 é›†æˆ**ï¼Œä»¥ä¾¿ **å°†æ•°æ®å¯¼å‡ºåˆ° S3** æ¡¶ã€‚è¿™å¯ä»¥é€šè¿‡è¿è¡Œä»¥ä¸‹ SQL æŸ¥è¯¢å®ç°ï¼š

```py
-- Example query
-- Replace table names and S3 bucket location
SELECT * FROM myschema.transactions INTO OUTFILE S3 's3://<YOUR_S3_BUCKET>/data/myschema/transactions/transactions.scv' FIELDS TERMINATED BY ',' LINES TERMINATED BY '\n' OVERWRITE ON;
```

## å¦‚ä½•åˆ›å»º MySQL å®ä¾‹

æˆ‘ä»¬å¯ä»¥ä½¿ç”¨ CloudFormation æ¨¡æ¿å’ŒåŸºç¡€è®¾æ–½å³ä»£ç æ¥åˆ›å»º MySQL æ•°æ®åº“ã€‚è€ƒè™‘è¿™ä¸ª AWS å‘½ä»¤ï¼š

```py
aws \
cloudformation deploy \
--template-file cfn_mysql.yaml \
--stack-name MySQLDB \
--capabilities CAPABILITY_IAM
```

å®ƒå°†ä½¿ç”¨ `cfn_mysql.yaml` æ¨¡æ¿æ–‡ä»¶æ¥åˆ›å»ºåä¸º MySQLDB çš„ CloudFormation å †æ ˆã€‚æˆ‘ä¹‹å‰åœ¨è¿™é‡Œå†™è¿‡æœ‰å…³å®ƒçš„å†…å®¹ [4]ï¼š

[](/create-mysql-and-postgres-instances-using-aws-cloudformation-d3af3c46c22a?source=post_page-----7e60575ff39c--------------------------------) ## ä½¿ç”¨ AWS CloudFormation åˆ›å»º MySQL å’Œ Postgres å®ä¾‹

### æ•°æ®åº“ä»ä¸šäººå‘˜çš„åŸºç¡€è®¾æ–½å³ä»£ç 

towardsdatascience.com

æˆ‘ä»¬çš„ `cfn_mysql.yaml` åº”è¯¥å¦‚ä¸‹æ‰€ç¤ºï¼š

```py
AWSTemplateFormatVersion: 2010-09-09
Description: >-
  This
  template creates an Amazon Relational Database Service database instance. You
  will be billed for the AWS resources used if you create a stack from this
  template.
Parameters:
  DBUser:
    Default: root
    NoEcho: 'true'
    Description: The database admin account username
    Type: String
    MinLength: '1'
    MaxLength: '16'
    AllowedPattern: '[a-zA-Z][a-zA-Z0-9]*'
    ConstraintDescription: must begin with a letter and contain only alphanumeric characters.
  DBPassword:
    Default: AmazingPassword
    NoEcho: 'true'
    Description: The database admin account password
    Type: String
    MinLength: '8'
    MaxLength: '41'
    AllowedPattern: '[a-zA-Z0-9]*'
    ConstraintDescription: must contain only alphanumeric characters.
Resources:
### Role to output into s3
  MySQLRDSExecutionRole:
    Type: "AWS::IAM::Role"
    Properties:
      AssumeRolePolicyDocument:
        Version: "2012-10-17"
        Statement:
          - Effect: "Allow"
            Principal:
              Service:
                - !Sub rds.amazonaws.com
            Action: "sts:AssumeRole"
      Path: "/"
      Policies:
        - PolicyName: MySQLRDSExecutionPolicy
          PolicyDocument:
            Version: "2012-10-17"
            Statement:
              - Effect: Allow
                Action:
                  - "s3:*"
                Resource: "*"
###

  RDSCluster: 
    Properties: 
      DBClusterParameterGroupName: 
        Ref: RDSDBClusterParameterGroup
      Engine: aurora-mysql
      MasterUserPassword: 
        Ref: DBPassword
      MasterUsername: 
        Ref: DBUser

### Add a role to export to s3
      AssociatedRoles:
        - RoleArn: !GetAtt [ MySQLRDSExecutionRole, Arn ]
###
    Type: "AWS::RDS::DBCluster"
  RDSDBClusterParameterGroup: 
    Properties: 
      Description: "CloudFormation Sample Aurora Cluster Parameter Group"
      Family: aurora-mysql5.7
      Parameters: 
        time_zone: US/Eastern
        ### Add a role to export to s3
        aws_default_s3_role: !GetAtt [ MySQLRDSExecutionRole, Arn ]
        ###
    Type: "AWS::RDS::DBClusterParameterGroup"
  RDSDBInstance1:
    Type: 'AWS::RDS::DBInstance'
    Properties:
      DBClusterIdentifier: 
        Ref: RDSCluster
      # AllocatedStorage: '20'
      DBInstanceClass: db.t2.small
      # Engine: aurora
      Engine: aurora-mysql
      PubliclyAccessible: "true"
      DBInstanceIdentifier: MyMySQLDB
  RDSDBParameterGroup:
    Type: 'AWS::RDS::DBParameterGroup'
    Properties:
      Description: CloudFormation Sample Aurora Parameter Group
      # Family: aurora5.6
      Family: aurora-mysql5.7
      Parameters:
        sql_mode: IGNORE_SPACE
        max_allowed_packet: 1024
        innodb_buffer_pool_size: '{DBInstanceClassMemory*3/4}'
# Aurora instances need to be associated with a AWS::RDS::DBCluster via DBClusterIdentifier without the cluster you get these generic errors 
```

å¦‚æœä¸€åˆ‡é¡ºåˆ©ï¼Œæˆ‘ä»¬å°†çœ‹åˆ° Amazon è´¦æˆ·ä¸­å‡ºç°ä¸€ä¸ªæ–°çš„å †æ ˆï¼š

![](img/d6ff5754b68a1d8279412c0bb82d917b.png)

å¸¦æœ‰ MySQL å®ä¾‹çš„ CloudFormation å †æ ˆã€‚å›¾ç‰‡ç”±ä½œè€…æä¾›ã€‚

ç°åœ¨æˆ‘ä»¬å¯ä»¥åœ¨æˆ‘ä»¬çš„æ•°æ®ç®¡é“ä¸­ä½¿ç”¨è¿™ä¸ª MySQL å®ä¾‹ã€‚æˆ‘ä»¬å¯ä»¥åœ¨ä»»ä½• SQL å·¥å…·ä¸­å°è¯•æˆ‘ä»¬çš„ SQL æŸ¥è¯¢ï¼Œä¾‹å¦‚ SQL Workbenchï¼Œä»¥å¡«å……è¡¨æ•°æ®ã€‚è¿™äº›è¡¨å°†ç”¨äºç¨åä½¿ç”¨ Athena åˆ›å»ºå¤–éƒ¨è¡¨ï¼Œå¯ä»¥é€šè¿‡ SQL åˆ›å»ºï¼š

```py
CREATE TABLE IF NOT EXISTS
  myschema.users AS
SELECT
  1 AS id,
  CURRENT_DATE() AS registration_date
UNION ALL
SELECT
  2 AS id,
  DATE_SUB(CURRENT_DATE(), INTERVAL 1 day) AS registration_date;

CREATE TABLE IF NOT EXISTS
  myschema.transactions AS
SELECT
  1 AS transaction_id,
  1 AS user_id,
  10.99 AS total_cost,
  CURRENT_DATE() AS dt
UNION ALL
SELECT
  2 AS transaction_id,
  2 AS user_id,
  4.99 AS total_cost,
  CURRENT_DATE() AS dt
UNION ALL
SELECT
  3 AS transaction_id,
  2 AS user_id,
  4.99 AS total_cost,
  DATE_SUB(CURRENT_DATE(), INTERVAL 3 day) AS dt
UNION ALL
SELECT
  4 AS transaction_id,
  1 AS user_id,
  4.99 AS total_cost,
  DATE_SUB(CURRENT_DATE(), INTERVAL 3 day) AS dt
UNION ALL
SELECT
  5 AS transaction_id,
  1 AS user_id,
  5.99 AS total_cost,
  DATE_SUB(CURRENT_DATE(), INTERVAL 2 day) AS dt
UNION ALL
SELECT
  6 AS transaction_id,
  1 AS user_id,
  15.99 AS total_cost,
  DATE_SUB(CURRENT_DATE(), INTERVAL 1 day) AS dt
UNION ALL
SELECT
  7 AS transaction_id,
  1 AS user_id,
  55.99 AS total_cost,
  DATE_SUB(CURRENT_DATE(), INTERVAL 4 day) AS dt
;
```

## ä½¿ç”¨ Athena å¤„ç†æ•°æ®

ç°åœ¨æˆ‘ä»¬å¸Œæœ›æ·»åŠ ä¸€ä¸ªæ•°æ®ç®¡é“å·¥ä½œæµï¼Œè¯¥å·¥ä½œæµè§¦å‘æˆ‘ä»¬çš„ Lambda å‡½æ•°ä»¥ä» MySQL æå–æ•°æ®ï¼Œå°†å…¶ä¿å­˜åˆ°æ•°æ®æ¹–ä¸­ï¼Œç„¶ååœ¨ Athena ä¸­å¼€å§‹æ•°æ®è½¬æ¢ã€‚

æˆ‘ä»¬å¸Œæœ›ä½¿ç”¨ MySQL ä¸­çš„æ•°æ®åˆ›å»ºä¸¤ä¸ªå¤–éƒ¨ Athena è¡¨ï¼š

+   myschema.users

+   myschema.transactions

ç„¶åæˆ‘ä»¬å¸Œæœ›åˆ›å»ºä¸€ä¸ªä¼˜åŒ–çš„ ICEBERG è¡¨ **myschema.user_transactions**ï¼Œå°†å…¶è¿æ¥åˆ°æˆ‘ä»¬çš„ BI è§£å†³æ–¹æ¡ˆã€‚

æˆ‘ä»¬å¸Œæœ›ä½¿ç”¨ MERGE è¯­å¥å°†æ–°æ•°æ®æ’å…¥åˆ°è¯¥è¡¨ä¸­ã€‚

```py
CREATE EXTERNAL TABLE mydatabase.users (
    id                bigint
  , registration_date string
) 
ROW FORMAT DELIMITED
FIELDS TERMINATED BY ',' 
STORED AS INPUTFORMAT   'org.apache.hadoop.mapred.TextInputFormat'
OUTPUTFORMAT   'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat' 
LOCATION  's3://<YOUR_S3_BUCKET>/data/myschema/users/' TBLPROPERTIES (  'skip.header.line.count'='0')
;
select * from mydatabase.users;

CREATE EXTERNAL TABLE mydatabase.transactions (
    transaction_id    bigint
  , user_id           bigint
  , total_cost        double
  , dt                string
) 
ROW FORMAT DELIMITED
FIELDS TERMINATED BY ',' 
STORED AS INPUTFORMAT   'org.apache.hadoop.mapred.TextInputFormat'
OUTPUTFORMAT   'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat' 
LOCATION  's3://<YOUR_S3_BUCKET>/data/myschema/transactions/' TBLPROPERTIES (  'skip.header.line.count'='0')
;
select * from mydatabase.transactions;

CREATE TABLE IF NOT EXISTS mydatabase.user_transactions (
  dt date,
  user_id int,
  total_cost_usd float,
  registration_date string
) 
PARTITIONED BY (dt)
LOCATION 's3://<YOUR_S3_BUCKET>/data/myschema/optimized-data-iceberg-parquet/' 
TBLPROPERTIES (
  'table_type'='ICEBERG',
  'format'='parquet',
  'write_target_data_file_size_bytes'='536870912',
  'optimize_rewrite_delete_file_threshold'='10'
)
;

MERGE INTO mydatabase.user_transactions  as ut
USING (
  SELECT 
      date(dt) dt
    , user_id
    , sum(total_cost) total_cost_usd
    , registration_date
  FROM mydatabase.transactions 
  LEFT JOIN mydatabase.users
  ON users.id = transactions.user_id
  GROUP BY
    dt
    , user_id
    , registration_date
) as ut2
ON (ut.dt = ut2.dt and ut.user_id = ut2.user_id)
WHEN MATCHED
    THEN UPDATE
        SET total_cost_usd = ut2.total_cost_usd, registration_date = ut2.registration_date
WHEN NOT MATCHED 
THEN INSERT (
 dt
,user_id
,total_cost_usd
,registration_date
)
  VALUES (
 ut2.dt
,ut2.user_id
,ut2.total_cost_usd
,ut2.registration_date
)
;
```

å½“æ–°è¡¨å‡†å¤‡å¥½åï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡è¿è¡Œ `SELECT *` æ¥æ£€æŸ¥å®ƒï¼š

![](img/0067053393a777846c94f0d3acd48e90.png)

mydatabase.user_transactionsã€‚å›¾ç‰‡ç”±ä½œè€…æä¾›ã€‚

## ä½¿ç”¨ Step Functionsï¼ˆçŠ¶æ€æœºï¼‰ç¼–æ’æ•°æ®ç®¡é“

åœ¨ä¹‹å‰çš„æ­¥éª¤ä¸­ï¼Œæˆ‘ä»¬å­¦ä¹ äº†å¦‚ä½•åˆ†åˆ«éƒ¨ç½²æ•°æ®ç®¡é“çš„æ¯ä¸€æ­¥å¹¶è¿›è¡Œæµ‹è¯•ã€‚åœ¨è¿™ä¸€æ®µä¸­ï¼Œæˆ‘ä»¬å°†äº†è§£å¦‚ä½•ä½¿ç”¨åŸºç¡€è®¾æ–½ä»£ç å’Œç®¡é“ç¼–æ’å·¥å…·å¦‚ AWS Step Functionsï¼ˆçŠ¶æ€æœºï¼‰åˆ›å»ºä¸€ä¸ªå®Œæ•´çš„æ•°æ®ç®¡é“ã€‚å½“æˆ‘ä»¬å®Œæˆæ—¶ï¼Œç®¡é“å›¾å°†å¦‚ä¸‹æ‰€ç¤ºï¼š

![](img/3d6fa66e126398bfe0bd267f57bf7059.png)

ä½¿ç”¨ Step Functions è¿›è¡Œæ•°æ®ç®¡é“ç¼–æ’ã€‚å›¾åƒç”±ä½œè€…æä¾›ã€‚

æ•°æ®ç®¡é“ç¼–æ’æ˜¯ä¸€ç§å¾ˆå¥½çš„æ•°æ®å·¥ç¨‹æŠ€æœ¯ï¼Œå®ƒä¸ºæˆ‘ä»¬çš„æ•°æ®ç®¡é“å¢åŠ äº†äº’åŠ¨æ€§ã€‚è¿™ä¸ªæƒ³æ³•åœ¨æˆ‘ä¹‹å‰çš„ä¸€ç¯‡æ•…äº‹ä¸­å·²ç»è§£é‡Šè¿‡[5]ï¼š

[](/data-pipeline-orchestration-9887e1b5eb7a?source=post_page-----7e60575ff39c--------------------------------) ## æ•°æ®ç®¡é“ç¼–æ’

### æ•°æ®ç®¡é“ç®¡ç†å¾—å½“å¯ä»¥ç®€åŒ–éƒ¨ç½²å¹¶æé«˜æ•°æ®çš„å¯ç”¨æ€§å’Œå¯è®¿é—®æ€§â€¦â€¦

[towardsdatascience.com

è¦éƒ¨ç½²å®Œæ•´çš„**ç¼–æ’å™¨è§£å†³æ–¹æ¡ˆ**ï¼ŒåŒ…æ‹¬æ‰€æœ‰å¿…è¦çš„èµ„æºï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ CloudFormationï¼ˆåŸºç¡€è®¾æ–½å³ä»£ç ï¼‰ã€‚è€ƒè™‘ä¸‹é¢è¿™ä¸ªå¯ä»¥åœ¨`/stack`æ–‡ä»¶å¤¹ä¸­ä»å‘½ä»¤è¡Œè¿è¡Œçš„è„šæœ¬ã€‚ç¡®ä¿<YOUR_S3_BUCKET>å­˜åœ¨ï¼Œå¹¶å°†å…¶æ›¿æ¢ä¸ºæ‚¨çš„å®é™… S3 æ¡¶ï¼š

```py
#!/usr/bin/env bash
# chmod +x ./deploy-staging.sh
# Run ./deploy-staging.sh
PROFILE=<YOUR_AWS_PROFILE>
STACK_NAME=BatchETLpipeline
LAMBDA_BUCKET=<YOUR_S3_BUCKET> # Replace with unique bucket name in your account
APP_FOLDER=mysql_connector

date

TIME=`date +"%Y%m%d%H%M%S"`

base=${PWD##*/}
zp=$base".zip"
echo $zp

rm -f $zp

pip install --target ./package -r requirements.txt
# boto3 is not required unless we want a specific version for Lambda
# requirements.txt:
# pymysql==1.0.3
# requests==2.28.1
# pytz==2023.3
# pyyaml==6.0

cd package
zip -r ../${base}.zip .

cd $OLDPWD

zip -r $zp "./${APP_FOLDER}" -x __pycache__ 

# Check if Lambda bucket exists:
LAMBDA_BUCKET_EXISTS=$(aws --profile ${PROFILE} s3 ls ${LAMBDA_BUCKET} --output text)
#  If NOT:
if [[ $? -eq 254 ]]; then
    # create a bucket to keep Lambdas packaged files:
    echo  "Creating Lambda code bucket ${LAMBDA_BUCKET} "
    CREATE_BUCKET=$(aws --profile ${PROFILE} s3 mb s3://${LAMBDA_BUCKET} --output text)
    echo ${CREATE_BUCKET}
fi

# Upload the package to S3:
aws --profile $PROFILE s3 cp ./${base}.zip s3://${LAMBDA_BUCKET}/${APP_FOLDER}/${base}${TIME}.zip

aws --profile $PROFILE \
cloudformation deploy \
--template-file stack.yaml \
--stack-name $STACK_NAME \
--capabilities CAPABILITY_IAM \
--parameter-overrides \
"StackPackageS3Key"="${APP_FOLDER}/${base}${TIME}.zip" \
"AppFolder"=$APP_FOLDER \
"S3LambdaBucket"=$LAMBDA_BUCKET \
"Environment"="staging" \
"Testing"="false"
```

å®ƒå°†ä½¿ç”¨ stack.yaml åˆ›å»ºä¸€ä¸ªåä¸º BatchETLpipeline çš„ CloudFormation å †æ ˆã€‚å®ƒå°†æ‰“åŒ…æˆ‘ä»¬çš„ Lambda å‡½æ•°ï¼Œåˆ›å»ºä¸€ä¸ªåŒ…å¹¶å°†å…¶ä¸Šä¼ åˆ° S3 æ¡¶ä¸­ã€‚å¦‚æœè¯¥æ¡¶ä¸å­˜åœ¨ï¼Œå®ƒå°†åˆ›å»ºå®ƒã€‚ç„¶åå°†éƒ¨ç½²ç®¡é“ã€‚

```py
AWSTemplateFormatVersion: '2010-09-09'
Description: An example template for a Step Functions state machine.
Parameters:

  DataLocation:
    Description: Data lake bucket with source data files.
    Type: String
    Default: s3://your.datalake.aws/data/
  AthenaResultsLocation:
    Description: S3 location for Athena query results.
    Type: String
    Default: s3://your.datalake.aws/athena/
  AthenaDatabaseName:
    Description: Athena schema names for ETL pipeline.
    Type: String
    Default: mydatabase
  S3LambdaBucket:
    Description: Use this bucket to keep your Lambda package.
    Type: String
    Default: your.datalake.aws
  StackPackageS3Key:
    Type: String
    Default: mysql_connector/stack.zip
  ServiceName:
    Type: String
    Default: mysql-connector
  Testing:
    Type: String
    Default: 'false'
    AllowedValues: ['true','false']
  Environment:
    Type: String
    Default: 'staging'
    AllowedValues: ['staging','live','test']
  AppFolder:
    Description: app.py file location inside the package, i.e. mysql_connector when ./stack/mysql_connector/app.py.
    Type: String
    Default: mysql_connector

Resources:
  LambdaExecutionRole:
    Type: "AWS::IAM::Role"
    Properties:
      AssumeRolePolicyDocument:
        Version: "2012-10-17"
        Statement:
          - Effect: Allow
            Principal:
              Service: lambda.amazonaws.com
            Action: "sts:AssumeRole"

  MyLambdaFunction:
    Type: "AWS::Lambda::Function"
    Properties:
      Handler: "index.handler"
      Role: !GetAtt [ LambdaExecutionRole, Arn ]
      Code:
        ZipFile: |
          exports.handler = (event, context, callback) => {
              callback(null, "Hello World!");
          };
      Runtime: "nodejs18.x"
      Timeout: "25"

### MySQL Connector Lmabda ###
  MySqlConnectorLambda:
    Type: AWS::Lambda::Function
    DeletionPolicy: Delete
    DependsOn: LambdaPolicy
    Properties:
      FunctionName: !Join ['-', [!Ref ServiceName, !Ref Environment] ]
      Handler: !Sub '${AppFolder}/app.lambda_handler'
      Description: Microservice that extracts data from RDS.
      Environment:
        Variables:
          DEBUG: true
          LAMBDA_PATH: !Sub '${AppFolder}/'
          TESTING: !Ref Testing
          ENV: !Ref Environment
      Role: !GetAtt LambdaRole.Arn
      Code:
        S3Bucket: !Sub '${S3LambdaBucket}'
        S3Key:
          Ref: StackPackageS3Key
      Runtime: python3.8
      Timeout: 360
      MemorySize: 128
      Tags:
        -
          Key: Service
          Value: Datalake

  StatesExecutionRole:
    Type: "AWS::IAM::Role"
    Properties:
      AssumeRolePolicyDocument:
        Version: "2012-10-17"
        Statement:
          - Effect: "Allow"
            Principal:
              Service:
                - !Sub states.${AWS::Region}.amazonaws.com
            Action: "sts:AssumeRole"
      Path: "/"
      Policies:
        - PolicyName: StatesExecutionPolicy
          PolicyDocument:
            Version: "2012-10-17"
            Statement:
              - Effect: Allow
                Action:
                  - "lambda:InvokeFunction"
                Resource: "*"
              - Effect: Allow
                Action:
                  - "athena:*"

                Resource: "*"
              - Effect: Allow
                Action:
                  - "s3:*"
                Resource: "*"
              - Effect: Allow
                Action:
                  - "glue:*"
                Resource: "*"

  MyStateMachine:
    Type: AWS::StepFunctions::StateMachine
    Properties:
      # StateMachineName: ETL-StateMachine
      StateMachineName: !Join ['-', ['ETL-StateMachine', !Ref ServiceName, !Ref Environment] ]
      DefinitionString:
        !Sub
          - |-
            {
              "Comment": "A Hello World example using an AWS Lambda function",
              "StartAt": "HelloWorld",
              "States": {
                "HelloWorld": {
                  "Type": "Task",
                  "Resource": "${lambdaArn}",
                  "Next": "Extract from MySQL"
                },
                "Extract from MySQL": {
                  "Resource": "${MySQLLambdaArn}",
                  "Type": "Task",
                  "Next": "Create Athena DB"
                },
                "Create Athena DB": {
                  "Resource": "arn:aws:states:::athena:startQueryExecution.sync",
                  "Parameters": {
                    "QueryString": "CREATE DATABASE if not exists ${AthenaDatabaseName}",
                    "WorkGroup": "primary",
                    "ResultConfiguration": {
                      "OutputLocation": "${AthenaResultsLocation}"
                    }
                  },
                  "Type": "Task",
                  "Next": "Show tables"
                },
                "Show tables": {
                  "Resource": "arn:aws:states:::athena:startQueryExecution.sync",
                  "Parameters": {
                    "QueryString": "show tables in ${AthenaDatabaseName}",
                    "WorkGroup": "primary",
                    "ResultConfiguration": {
                      "OutputLocation": "${AthenaResultsLocation}"
                    }
                  },
                  "Type": "Task",
                  "Next": "Get Show tables query results"
                },
                "Get Show tables query results": {
                  "Resource": "arn:aws:states:::athena:getQueryResults",
                  "Parameters": {
                    "QueryExecutionId.$": "$.QueryExecution.QueryExecutionId"
                  },
                  "Type": "Task",
                  "Next": "Decide what next"
                },
                "Decide what next": {
                  "Comment": "Based on the input table name, a choice is made for moving to the next step.",
                  "Type": "Choice",
                  "Choices": [
                    {
                      "Not": {
                        "Variable": "$.ResultSet.Rows[0].Data[0].VarCharValue",
                        "IsPresent": true
                      },
                      "Next": "Create users table (external)"
                    },
                    {
                      "Variable": "$.ResultSet.Rows[0].Data[0].VarCharValue",
                      "IsPresent": true,
                      "Next": "Check All Tables"
                    }
                  ],
                  "Default": "Check All Tables"
                },
                "Create users table (external)": {
                  "Resource": "arn:aws:states:::athena:startQueryExecution.sync",
                  "Parameters": {
                    "QueryString": "CREATE EXTERNAL TABLE ${AthenaDatabaseName}.users ( id                bigint , registration_date string ) ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' STORED AS INPUTFORMAT   'org.apache.hadoop.mapred.TextInputFormat' OUTPUTFORMAT   'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat' LOCATION  's3://datalake.staging.liveproject/data/myschema/users/' TBLPROPERTIES (  'skip.header.line.count'='0') ;",
                    "WorkGroup": "primary",
                    "ResultConfiguration": {
                      "OutputLocation": "${AthenaResultsLocation}"
                    }
                  },
                  "Type": "Task",
                  "Next": "Create transactions table (external)"
                },
                "Create transactions table (external)": {
                  "Resource": "arn:aws:states:::athena:startQueryExecution.sync",
                  "Parameters": {
                    "QueryString": "CREATE EXTERNAL TABLE ${AthenaDatabaseName}.transactions ( transaction_id    bigint , user_id           bigint , total_cost        double , dt                string ) ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' STORED AS INPUTFORMAT   'org.apache.hadoop.mapred.TextInputFormat' OUTPUTFORMAT   'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat' LOCATION  's3://datalake.staging.liveproject/data/myschema/transactions/' TBLPROPERTIES (  'skip.header.line.count'='0') ;",
                    "WorkGroup": "primary",
                    "ResultConfiguration": {
                      "OutputLocation": "${AthenaResultsLocation}"
                    }
                  },
                  "Type": "Task",
                  "Next": "Create report table (parquet)"
                },
                "Create report table (parquet)": {
                  "Resource": "arn:aws:states:::athena:startQueryExecution.sync",
                  "Parameters": {
                    "QueryString": "CREATE TABLE IF NOT EXISTS ${AthenaDatabaseName}.user_transactions ( dt date, user_id int, total_cost_usd float, registration_date string ) PARTITIONED BY (dt) LOCATION 's3://datalake.staging.liveproject/data/myschema/optimized-data-iceberg-parquet/' TBLPROPERTIES ( 'table_type'='ICEBERG', 'format'='parquet', 'write_target_data_file_size_bytes'='536870912', 'optimize_rewrite_delete_file_threshold'='10' ) ;",
                    "WorkGroup": "primary",
                    "ResultConfiguration": {
                      "OutputLocation": "${AthenaResultsLocation}"
                    }
                  },
                  "Type": "Task",
                  "End": true
                },
                "Check All Tables": {
                  "Type": "Map",
                  "InputPath": "$.ResultSet",
                  "ItemsPath": "$.Rows",
                  "MaxConcurrency": 0,
                  "Iterator": {
                    "StartAt": "CheckTable",
                    "States": {
                      "CheckTable": {
                        "Type": "Choice",
                        "Choices": [
                          {
                            "Variable": "$.Data[0].VarCharValue",
                            "StringMatches": "*users",
                            "Next": "passstep"
                          },
                          {
                            "Variable": "$.Data[0].VarCharValue",
                            "StringMatches": "*user_transactions",
                            "Next": "Insert New parquet Data"
                          }
                        ],
                        "Default": "passstep"
                      },
                      "Insert New parquet Data": {
                        "Resource": "arn:aws:states:::athena:startQueryExecution.sync",
                        "Parameters": {
                          "QueryString": "MERGE INTO ${AthenaDatabaseName}.user_transactions  as ut USING ( SELECT date(dt) dt , user_id , sum(total_cost) total_cost_usd , registration_date FROM ${AthenaDatabaseName}.transactions LEFT JOIN ${AthenaDatabaseName}.users ON users.id = transactions.user_id GROUP BY dt , user_id , registration_date ) as ut2 ON (ut.dt = ut2.dt and ut.user_id = ut2.user_id) WHEN MATCHED THEN UPDATE SET total_cost_usd = ut2.total_cost_usd, registration_date = ut2.registration_date WHEN NOT MATCHED THEN INSERT ( dt ,user_id ,total_cost_usd ,registration_date ) VALUES ( ut2.dt ,ut2.user_id ,ut2.total_cost_usd ,ut2.registration_date ) ;",
                          "WorkGroup": "primary",
                          "ResultConfiguration": {
                            "OutputLocation": "${AthenaResultsLocation}"
                          }
                        },
                        "Type": "Task",
                        "End": true
                      },
                      "passstep": {
                        "Type": "Pass",
                        "Result": "NA",
                        "End": true
                      }
                    }
                  },
                  "End": true
                }
              }
            }
          - {
            lambdaArn: !GetAtt [ MyLambdaFunction, Arn ],
            MySQLLambdaArn: !GetAtt [ MySqlConnectorLambda, Arn ],
            AthenaResultsLocation: !Ref AthenaResultsLocation,
            AthenaDatabaseName: !Ref AthenaDatabaseName
          }
      RoleArn: !GetAtt [ StatesExecutionRole, Arn ]
      Tags:
        -
          Key: "keyname1"
          Value: "value1"
        -
          Key: "keyname2"
          Value: "value2"

# IAM role for mysql-data-connector Lambda:
  LambdaRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: "2012-10-17"
        Statement:
          -
            Effect: Allow
            Principal:
              Service:
                - "lambda.amazonaws.com"
            Action:
              - "sts:AssumeRole"

  LambdaPolicy:
    Type: AWS::IAM::Policy
    DependsOn: LambdaRole
    Properties:
      Roles:
        - !Ref LambdaRole
      PolicyName: !Join ['-', [!Ref ServiceName, !Ref Environment, 'lambda-policy']] 
      PolicyDocument:
        {
            "Version": "2012-10-17",
            "Statement": [
                {
                    "Effect": "Allow",
                    "Action": [
                        "logs:CreateLogGroup",
                        "logs:CreateLogStream",
                        "logs:PutLogEvents"
                    ],
                    "Resource": "*"
                }
            ]
        }
```

å¦‚æœä¸€åˆ‡é¡ºåˆ©ï¼Œæˆ‘ä»¬çš„æ–°æ•°æ®ç®¡é“çš„å †æ ˆå°†è¢«éƒ¨ç½²ï¼š

![](img/2b97e39b295eadfc8d179a499197fad0.png)

BatchETLpipeline å †æ ˆå’Œèµ„æºã€‚å›¾åƒç”±ä½œè€…æä¾›ã€‚

å¦‚æœæˆ‘ä»¬ç‚¹å‡»çŠ¶æ€æœºèµ„æºï¼Œç„¶åç‚¹å‡»â€˜ç¼–è¾‘â€™ï¼Œæˆ‘ä»¬å°†çœ‹åˆ°æˆ‘ä»¬çš„ ETL ç®¡é“ä½œä¸ºå›¾å½¢å±•ç¤ºï¼š

![](img/cb9292383481ab8b5951980275a644a9.png)

æ‰¹é‡æ•°æ®ç®¡é“çš„å·¥ä½œæµå·¥ä½œå®¤ã€‚å›¾åƒç”±ä½œè€…æä¾›ã€‚

ç°åœ¨æˆ‘ä»¬å¯ä»¥æ‰§è¡Œç®¡é“ä»¥è¿è¡Œæ‰€æœ‰å¿…è¦çš„æ•°æ®è½¬æ¢æ­¥éª¤ã€‚ç‚¹å‡»â€˜å¼€å§‹æ‰§è¡Œâ€™ã€‚

![](img/3d6fa66e126398bfe0bd267f57bf7059.png)

æˆåŠŸæ‰§è¡Œã€‚å›¾åƒç”±ä½œè€…æä¾›ã€‚

ç°åœ¨æˆ‘ä»¬å¯ä»¥å°†æˆ‘ä»¬çš„ Athena è¡¨è¿æ¥åˆ°æˆ‘ä»¬çš„**BI è§£å†³æ–¹æ¡ˆ**ã€‚è¿æ¥æˆ‘ä»¬æœ€ç»ˆçš„ Athena æ•°æ®é›†`mydataset.user_transactions`ä»¥åˆ›å»ºä»ªè¡¨ç›˜ã€‚

![](img/3427a98727ee0bbf52ef9d94f868f567.png)

è¿æ¥ Quicksight ä¸­çš„æ•°æ®é›†ã€‚å›¾åƒç”±ä½œè€…æä¾›ã€‚

æˆ‘ä»¬åªéœ€è°ƒæ•´å‡ ä¸ªè®¾ç½®ï¼Œä½¿æˆ‘ä»¬çš„ä»ªè¡¨ç›˜çœ‹èµ·æ¥åƒè¿™æ ·ï¼š

![](img/2fc0d03bb10de92d3b88a7f4a7a3b3ed.png)

Quicksight ä»ªè¡¨ç›˜ã€‚å›¾åƒç”±ä½œè€…æä¾›ã€‚

æˆ‘ä»¬å¸Œæœ›ä½¿ç”¨`dt`ä½œä¸ºç»´åº¦ï¼Œ`total_cost_usd`ä½œä¸ºæŒ‡æ ‡ã€‚æˆ‘ä»¬è¿˜å¯ä»¥ä¸ºæ¯ä¸ª`user_id`è®¾ç½®ä¸€ä¸ªæ‹†åˆ†ç»´åº¦ã€‚

## ç»“è®º

æ‰¹å¤„ç†æ•°æ®ç®¡é“å¾ˆå—æ¬¢è¿ï¼Œå› ä¸ºå†å²ä¸Šå·¥ä½œè´Ÿè½½ä¸»è¦æ˜¯æ‰¹å¤„ç†å‹çš„æ•°æ®ç¯å¢ƒã€‚æˆ‘ä»¬åˆšåˆšå»ºç«‹äº†ä¸€ä¸ª ETL æ•°æ®ç®¡é“ï¼Œä» MySQL ä¸­æå–æ•°æ®å¹¶åœ¨æ•°æ®æ¹–ä¸­è½¬æ¢ã€‚è¯¥æ¨¡å¼æœ€é€‚ç”¨äºæ•°æ®é›†ä¸å¤§ä¸”éœ€è¦æŒç»­å¤„ç†çš„æƒ…å†µï¼Œå› ä¸º Athena æ ¹æ®æ‰«æçš„æ•°æ®é‡æ”¶è´¹ã€‚è¿™ç§æ–¹æ³•åœ¨å°†æ•°æ®è½¬æ¢ä¸ºåˆ—å¼æ ¼å¼å¦‚ Parquet æˆ– ORC æ—¶è¡¨ç°è‰¯å¥½ï¼Œç»“åˆå‡ ä¸ªå°æ–‡ä»¶æˆè¾ƒå¤§çš„æ–‡ä»¶ï¼Œæˆ–è¿›è¡Œåˆ†æ¡¶å’Œæ·»åŠ åˆ†åŒºã€‚æˆ‘ä»¥å‰åœ¨æˆ‘çš„ä¸€ä¸ªæ•…äº‹ä¸­å†™è¿‡è¿™äº›å¤§æ•°æ®æ–‡ä»¶æ ¼å¼[6]ã€‚

[](/big-data-file-formats-explained-275876dc1fc9?source=post_page-----7e60575ff39c--------------------------------) ## å¤§æ•°æ®æ–‡ä»¶æ ¼å¼è§£æ

### Parquet ä¸ ORC ä¸ AVRO ä¸ JSONã€‚è¯¥é€‰æ‹©å“ªä¸€ä¸ªï¼Œå¦‚ä½•ä½¿ç”¨å®ƒä»¬ï¼Ÿ

towardsdatascience.com

æˆ‘ä»¬å­¦ä¹ äº†å¦‚ä½•ä½¿ç”¨ Step Functions æ¥ç¼–æ’æ•°æ®ç®¡é“ï¼Œè§†è§‰åŒ–æ•°æ®æµä»æºå¤´åˆ°æœ€ç»ˆç”¨æˆ·ï¼Œå¹¶ä½¿ç”¨åŸºç¡€è®¾æ–½å³ä»£ç è¿›è¡Œéƒ¨ç½²ã€‚è¿™ä¸ªè®¾ç½®ä½¿å¾—æˆ‘ä»¬å¯ä»¥å¯¹æ•°æ®ç®¡é“åº”ç”¨ CI/CD æŠ€æœ¯[7]ã€‚

å¸Œæœ›è¿™ä¸ªæ•™ç¨‹å¯¹ä½ æœ‰å¸®åŠ©ã€‚å¦‚æœä½ æœ‰ä»»ä½•é—®é¢˜ï¼Œè¯·å‘Šè¯‰æˆ‘ã€‚

## æ¨èé˜…è¯»

[1] `towardsdatascience.com/data-pipeline-design-patterns-100afa4b93e3`

[2] [`medium.com/towards-data-science/introduction-to-apache-iceberg-tables-a791f1758009`](https://medium.com/towards-data-science/introduction-to-apache-iceberg-tables-a791f1758009)

[3] [`medium.com/towards-data-science/advanced-sql-techniques-for-beginners-211851a28488`](https://medium.com/towards-data-science/advanced-sql-techniques-for-beginners-211851a28488)

[4] [`medium.com/towards-data-science/create-mysql-and-postgres-instances-using-aws-cloudformation-d3af3c46c22a`](https://medium.com/towards-data-science/create-mysql-and-postgres-instances-using-aws-cloudformation-d3af3c46c22a)

[5] [`medium.com/towards-data-science/data-pipeline-orchestration-9887e1b5eb7a`](https://medium.com/towards-data-science/data-pipeline-orchestration-9887e1b5eb7a)

[6] [`medium.com/towards-data-science/big-data-file-formats-explained-275876dc1fc9`](https://medium.com/towards-data-science/big-data-file-formats-explained-275876dc1fc9)

[7] [`medium.com/towards-data-science/continuous-integration-and-deployment-for-data-platforms-817bf1b6bed1`](https://medium.com/towards-data-science/continuous-integration-and-deployment-for-data-platforms-817bf1b6bed1)
