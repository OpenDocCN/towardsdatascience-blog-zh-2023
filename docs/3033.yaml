- en: What are Query, Key, and Value in the Transformer Architecture and Why Are They
    Used?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/what-are-query-key-and-value-in-the-transformer-architecture-and-why-are-they-used-acbe73f731f2?source=collection_archive---------2-----------------------#2023-10-05](https://towardsdatascience.com/what-are-query-key-and-value-in-the-transformer-architecture-and-why-are-they-used-acbe73f731f2?source=collection_archive---------2-----------------------#2023-10-05)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: An analysis of the intuition behind the notion of Key, Query, and Value in Transformer
    architecture and why is it used.
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://ebrahimpichka.medium.com/?source=post_page-----acbe73f731f2--------------------------------)[![Ebrahim
    Pichka](../Images/8add6e8e875d9e921caf7f5eaa77d545.png)](https://ebrahimpichka.medium.com/?source=post_page-----acbe73f731f2--------------------------------)[](https://towardsdatascience.com/?source=post_page-----acbe73f731f2--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----acbe73f731f2--------------------------------)
    [Ebrahim Pichka](https://ebrahimpichka.medium.com/?source=post_page-----acbe73f731f2--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fcf08d1e97a71&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-are-query-key-and-value-in-the-transformer-architecture-and-why-are-they-used-acbe73f731f2&user=Ebrahim+Pichka&userId=cf08d1e97a71&source=post_page-cf08d1e97a71----acbe73f731f2---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----acbe73f731f2--------------------------------)
    ·10 min read·Oct 5, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Facbe73f731f2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-are-query-key-and-value-in-the-transformer-architecture-and-why-are-they-used-acbe73f731f2&user=Ebrahim+Pichka&userId=cf08d1e97a71&source=-----acbe73f731f2---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Facbe73f731f2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-are-query-key-and-value-in-the-transformer-architecture-and-why-are-they-used-acbe73f731f2&source=-----acbe73f731f2---------------------bookmark_footer-----------)![](../Images/36bd08a0784d8f29ebfaca4463809896.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Image by author — generated by [**Midjourney**](https://www.midjourney.com/)
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Recent years have seen the Transformer architecture make waves in the field
    of natural language processing (NLP), achieving state-of-the-art results in a
    variety of tasks including machine translation, language modeling, and text summarization,
    as well as other domains of AI i.e. Vision, Speech, RL, etc.
  prefs: []
  type: TYPE_NORMAL
- en: Vaswani et al. (2017), first introduced the transformer in their paper *“Attention
    Is All You Need”*, in which they used the self-attention mechanism without incorporating
    recurrent connections while the model can focus selectively on specific portions
    of input sequences.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5c8d9226a835a3ef8723261c98b8ff73.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The Transformer model architecture — Image from the Vaswani et al. (2017) paper
    (Source: [**arXiv:1706.03762v7**](https://arxiv.org/abs/1706.03762v7))'
  prefs: []
  type: TYPE_NORMAL
- en: In particular, previous sequence models, such as recurrent encoder-decoder models,
    were limited in their ability to capture long-term dependencies and parallel computations.
    In fact, right before the Transformers paper came out in 2017, state-of-the-art
    performance in most NLP…
  prefs: []
  type: TYPE_NORMAL
