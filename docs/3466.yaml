- en: How to co-design software/hardware architecture for AI/ML in a new era?
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何在新纪元中共同设计 AI/ML 的软件/硬件架构？
- en: 原文：[https://towardsdatascience.com/how-to-co-design-software-hardware-architecture-for-ai-ml-in-a-new-era-b296f2842fe2?source=collection_archive---------1-----------------------#2023-11-22](https://towardsdatascience.com/how-to-co-design-software-hardware-architecture-for-ai-ml-in-a-new-era-b296f2842fe2?source=collection_archive---------1-----------------------#2023-11-22)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/how-to-co-design-software-hardware-architecture-for-ai-ml-in-a-new-era-b296f2842fe2?source=collection_archive---------1-----------------------#2023-11-22](https://towardsdatascience.com/how-to-co-design-software-hardware-architecture-for-ai-ml-in-a-new-era-b296f2842fe2?source=collection_archive---------1-----------------------#2023-11-22)
- en: A holistic view of designing efficient architecture for AI/ML
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 设计高效 AI/ML 架构的整体视角
- en: '[](https://medium.com/@LizLiAI?source=post_page-----b296f2842fe2--------------------------------)[![Liz
    Li](../Images/78846add1618c8c095dd97adeca87f81.png)](https://medium.com/@LizLiAI?source=post_page-----b296f2842fe2--------------------------------)[](https://towardsdatascience.com/?source=post_page-----b296f2842fe2--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----b296f2842fe2--------------------------------)
    [Liz Li](https://medium.com/@LizLiAI?source=post_page-----b296f2842fe2--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@LizLiAI?source=post_page-----b296f2842fe2--------------------------------)[![Liz
    Li](../Images/78846add1618c8c095dd97adeca87f81.png)](https://medium.com/@LizLiAI?source=post_page-----b296f2842fe2--------------------------------)[](https://towardsdatascience.com/?source=post_page-----b296f2842fe2--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----b296f2842fe2--------------------------------)
    [Liz Li](https://medium.com/@LizLiAI?source=post_page-----b296f2842fe2--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fdc3f793d765a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-co-design-software-hardware-architecture-for-ai-ml-in-a-new-era-b296f2842fe2&user=Liz+Li&userId=dc3f793d765a&source=post_page-dc3f793d765a----b296f2842fe2---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----b296f2842fe2--------------------------------)
    ·8 min read·Nov 22, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fb296f2842fe2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-co-design-software-hardware-architecture-for-ai-ml-in-a-new-era-b296f2842fe2&user=Liz+Li&userId=dc3f793d765a&source=-----b296f2842fe2---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fdc3f793d765a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-co-design-software-hardware-architecture-for-ai-ml-in-a-new-era-b296f2842fe2&user=Liz+Li&userId=dc3f793d765a&source=post_page-dc3f793d765a----b296f2842fe2---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----b296f2842fe2--------------------------------)
    ·8 min read·2023年11月22日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fb296f2842fe2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-co-design-software-hardware-architecture-for-ai-ml-in-a-new-era-b296f2842fe2&user=Liz+Li&userId=dc3f793d765a&source=-----b296f2842fe2---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb296f2842fe2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-co-design-software-hardware-architecture-for-ai-ml-in-a-new-era-b296f2842fe2&source=-----b296f2842fe2---------------------bookmark_footer-----------)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb296f2842fe2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-co-design-software-hardware-architecture-for-ai-ml-in-a-new-era-b296f2842fe2&source=-----b296f2842fe2---------------------bookmark_footer-----------)'
- en: State-of-the-art generative AI technologies in computer vision, natural language
    processing, etc. have been exploding recently with research breakthroughs on innovative
    model architectures including stable diffusion, neural rendering (NeRF), text
    to 3D, large language models(LLM), and more. These advanced technologies require
    more complex AI networks, and demand orders of magnitude more compute resources
    and energy-efficient architecture.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 最新的生成型人工智能技术在计算机视觉、自然语言处理等领域最近取得了爆炸性的进展，涉及到稳定扩散、神经渲染（NeRF）、文本到3D、大型语言模型（LLM）等创新模型架构的研究突破。这些先进技术需要更复杂的人工智能网络，并且需要数量级更多的计算资源和高效能的架构。
- en: To fulfill the above-mentioned architectural requirement, bridging the gap between
    hardware(HW) and software(SW)/algorithm design is essential. Co-design involves
    an iterative process of designing, testing, and refining both HW and SW components
    until the system meets the desired performance requirements. However, SW and HW
    are traditionally designed independently as SW programmers seldom need to think
    about which HW to run on, and HW is typically designed to support a wide range
    of SW. There are very limited studies on SW/HW co-design which is strongly required
    for supporting AI workloads efficiently.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 为了满足上述架构要求，弥合硬件（HW）和软件（SW）/算法设计之间的差距是必不可少的。共同设计涉及一个迭代的过程，即设计、测试和优化硬件和软件组件，直到系统达到所需的性能要求。然而，软件和硬件通常是独立设计的，因为软件程序员很少需要考虑运行的硬件，而硬件通常设计为支持广泛的软件。关于软件/硬件共同设计的研究非常有限，但这是有效支持
    AI 工作负载所强烈需要的。
- en: '![](../Images/81d64f98c997268b102e79d7fa02960f.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/81d64f98c997268b102e79d7fa02960f.png)'
- en: 'Source: Image by the author.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 来源：作者提供的图像。
- en: We have to design efficient ‘SW/algorithm aware’ HW and ‘HW aware’ SW/algorithm,
    so they can tightly intertwine to squeeze every bit of our limited compute resources.
    How should we do it? Below is an evolving methodology that can be used as a reference
    if you are architecting new HW/SW features for AI.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 我们必须设计高效的“SW/算法感知”硬件和“硬件感知”软件/算法，以便它们能够紧密结合，最大限度地利用我们有限的计算资源。我们应该如何做呢？以下是一种不断发展的方法论，可作为您在设计新的
    AI 硬件/软件功能时的参考。
- en: '**1\. Identify proxy AI workloads.**'
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**1\. 确定代理 AI 工作负载。**'
- en: To initiate the analysis, we need proxy AI models and define a priority list
    to investigate further. There are multiple resources you can refer to, including
    the latest research paper (CVPR, Siggraph, research labs from big tech) with open-source
    code, customer feedback or requests, industry trends, etc. Filter out several
    representative models based on your expert judgment. This step is crucial as you
    are using them to design your ‘future architecture’
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 要启动分析，我们需要代理 AI 模型并定义一个优先级列表以进一步调查。您可以参考多个资源，包括最新的研究论文（CVPR、Siggraph、大型科技公司的研究实验室）及其开源代码、客户反馈或请求、行业趋势等。根据您的专家判断筛选出几个代表性的模型。这一步骤至关重要，因为您将使用这些模型来设计您的“未来架构”。
- en: 2\. Thorough model architecture analysis.
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2\. 彻底分析模型架构。
- en: Be sure to investigate the model architecture comprehensively to understand
    its functionality, innovation, and break it down into detailed pieces as much
    as you can. Are there new operators unsupported in the current tech stack? Where
    are the compute-intensive layers? Is it a data transfer (memory) heavy model?
    What is the datatype required and what type of quantization techniques can be
    applied without sacrificing accuracy? Which part of the model can be HW accelerated
    and where are the potential performance optimizations?
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 一定要全面调查模型架构，以了解其功能、创新，并尽可能将其分解为详细的组件。是否有当前技术栈中不支持的新操作符？计算密集的层在哪里？这是一种数据传输（内存）密集的模型吗？需要什么数据类型，哪些量化技术可以在不牺牲准确性的情况下应用？模型的哪部分可以进行硬件加速，潜在的性能优化在哪里？
- en: For example, in neural rendering, the model requires both rendering and computing
    (matrix multiplication) working in parallel, you need to check if the current
    SW stack supports render/compute simultaneously. In LLMs, key-value(KV) cache
    size is increasing w.r.t. input sequence length, it’s critical to understand the
    memory requirement and potential data transfer/memory hierarchy optimization to
    handle large KV cache.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在神经渲染中，模型需要同时进行渲染和计算（矩阵乘法），您需要检查当前的软件栈是否支持同时进行渲染和计算。在大型语言模型（LLMs）中，随着输入序列长度的增加，键值（KV）缓存的大小也在增加，了解内存需求和潜在的数据传输/内存层次优化以处理大型
    KV 缓存至关重要。
- en: '![](../Images/fd60ff26a3c617803d90e973317fab91.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fd60ff26a3c617803d90e973317fab91.png)'
- en: 'Left: Memory layout on NVIDIA A100 when serving an LLM with 13B parameters;
    Right: Memory usage and Throughput v.s. Batch Size. Figure source: [Efficient
    Memory Management for Large Language Model Serving with Paged Attention](https://arxiv.org/abs/2309.06180)
    [1]'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 左图：服务于 13B 参数的 LLM 时 NVIDIA A100 上的内存布局；右图：内存使用和吞吐量与批量大小的关系。图源：[大规模语言模型服务中的高效内存管理与分页注意力](https://arxiv.org/abs/2309.06180)
    [1]
- en: '**3\. SW enabling and prototyping**'
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**3\. 软件启用和原型设计**'
- en: Download the open-source code for the model identified in Step 2, and run it
    on the ‘target’ SW framework/HW. This step is not straightforward, especially
    for new/disruptive models. As the goal is to enable a workable solution for performance
    analysis, it’s not necessary to deliver product-quality code at this stage. A
    dirty fix on SW without performance tunning is acceptable to proceed to Step 4\.
    A major step is to convert the pre-trained model in the development framework
    (Pytorch) to a new format required by the targeted new framework.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 下载第2步中识别的开源代码，并在“目标”SW框架/HW上运行。这一步并不简单，尤其是对于新的/颠覆性模型。由于目标是实现可行的性能分析解决方案，因此此阶段不必交付产品级别的代码。对SW进行临时修复而不进行性能调优是可以接受的，以便继续进行第4步。一个主要步骤是将开发框架（Pytorch）中的预训练模型转换为目标新框架所需的新格式。
- en: '[PRE0]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: However, there are often cases where significant support effort is required.
    For example, to run differentiable rendering models, autograd needs to be supported.
    It’s very likely that this feature is not ready in the new framework, and requires
    months of effort from the development team. Another example is GPTQ quantization
    for LLMs, which might not be supported in the inference framework initially. Instead
    of waiting for the engineering team, architects can run the workload on the Nvidia
    system for performance analysis, as Nvidia is the choice of HW for academic development.
    This enables the development of a list of SW requirements based on the gaps observed
    during SW enabling.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，通常会有大量的支持工作。例如，要运行可微分渲染模型，需要支持 autograd。很可能在新框架中尚未准备好此功能，并且需要开发团队几个月的努力。另一个例子是
    LLMs 的 GPTQ 量化，这在推理框架中可能最初不受支持。架构师可以在 Nvidia 系统上运行工作负载进行性能分析，而不是等待工程团队，因为 Nvidia
    是学术开发的硬件选择。这使得可以根据在 SW 启用过程中观察到的差距制定 SW 需求列表。
- en: '**4\. Performance analysis and architectural innovation.**'
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**4. 性能分析和架构创新。**'
- en: There are numerous metrics to judge an AI model’s performance. Below are the
    major ones we should consider.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多指标可以判断 AI 模型的性能。以下是我们应考虑的主要指标。
- en: '**4.1 FLOPs** (Floating Point Operations) and **MACs** (Multiply-Accumulate
    Operations).'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '**4.1 FLOPs**（浮点运算次数）和**MACs**（乘加运算次数）。'
- en: These metrics are commonly used to calculate the computational complexity of
    deep learning models. They provide a quick and easy way to understand the number
    of arithmetic operations required. FLOPs can be calculated through methods such
    as paper analysis, Vtune reports, or tools like flops-counter.pytorch and pytorch-OpCounter.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 这些指标通常用于计算深度学习模型的计算复杂性。它们提供了一种快速简便的方法来了解所需的算术操作次数。FLOPs 可以通过论文分析、Vtune 报告或像
    flops-counter.pytorch 和 pytorch-OpCounter 这样的工具来计算。
- en: '**4.2 Memory Footprint and Bandwidth (BW)**'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '**4.2 内存占用和带宽（BW）**'
- en: Memory footprint mainly consists of weights (network parameters) and input data.
    For example, a Llama model with 13B parameters in FP16 consumes about 13*2 (FP16=2
    bytes) = 26GB memory (input is negligible as weight takes much more space). Another
    key factor for LLMs is the KV cache size. KV cache takes up to 30% of total memory
    and it is dynamic (refer to picture in Step 2). Large models are usually memory
    bound as the speed is dependent on how quickly to move data from system memory
    to local memory, or from local memory to local caches/registers. Available memory
    BW is far better in predicting inference latency (token generation time for LLMs)
    than peak compute TOPS. One performance indicator is memory bandwidth utilization
    (MBU) which is defined as actual BW/peak BW. Ideally, MBU close to 100% indicates
    memory BW is fully utilized.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 内存占用主要包括权重（网络参数）和输入数据。例如，一个具有 13B 参数的 Llama 模型在 FP16 中消耗约 13*2（FP16=2 字节）= 26GB
    内存（输入数据可以忽略，因为权重占用更多空间）。另一个关键因素是 KV 缓存大小。KV 缓存占用最多 30% 的总内存，并且是动态的（请参见第2步中的图片）。大型模型通常受到内存限制，因为速度取决于从系统内存移动数据到本地内存的速度，或者从本地内存到本地缓存/寄存器的速度。可用内存带宽在预测推理延迟（LLMs
    的 token 生成时间）方面远比峰值计算 TOPS 更有效。一个性能指标是内存带宽利用率（MBU），定义为实际带宽/峰值带宽。理想情况下，MBU 接近 100%
    表示内存带宽被完全利用。
- en: '**Enough Memory is Not Enough!**'
  id: totrans-30
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**足够的内存是不够的！**'
- en: '![](../Images/44c8e73f4c4049e29c07f22e3c0bf6ea.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/44c8e73f4c4049e29c07f22e3c0bf6ea.png)'
- en: 'Nvidia is increasing Flops by orders of magnitude but not memory BW. Source:
    Substack/SemiAnalysis'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: Nvidia 正在以数量级的增加 FLOPs，但内存带宽没有增加。来源：Substack/SemiAnalysis
- en: 'As memory is a bottleneck, exploration in advanced model compression and memory/caching
    technologies are required. A few pioneer works are listed below:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: 'MemGPT: it utilizes different level memory hierarchy resources, such as a combination
    of fast and small RAM, and large and slow storage storage memory. Information
    must be explicitly transferred between them. [2]'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Low precision quantization (GPTQ, AWQ, GGML) to reduce the memory footprint
    of models
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In-memory computing (PIM) : Reduce power and improve performance by eliminating
    the need of data movement.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**4.3 Latency/throughput.**'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: 'In computer vision, latency is the time to generate one frame. In the context
    of LLMs, it is the time between the first token and the next token generation.
    Throughput is the number of tokens/frames per second. Latency is a critical metric
    for measuring AI system performance and is a compound factor of both SW/HW performance.
    There are various optimization strategies to consider, to name a few below:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: Optimization of bandwidth-constrained operations like normalizations, pointwise
    operations, SoftMax, and ReLU. It is estimated that normalization and pointwise
    operations consume nearly 40% more runtime than matrix multiplications, while
    only achieving 250x and 700x less FLOPS than matrix multiplications respectively.
    To solve the problem, kernel fusion can be utilized to fuse multiple operators
    to save data transfer costs or replace expensive operators (softmax) with light
    ones (ReLU).
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/5f043133b92755ff68a5df5aedd4f94b.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
- en: 'Proportions of operator classes. Source: [Data movement is all you need](https://arxiv.org/pdf/2007.00072.pdf)'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: Specialized HW architecture. The integration of specialized hardware(AVX, GPUs,
    TPU, NPU) can lead to significant speedups and energy savings, which is particularly
    important for applications that require real-time processing on resource-constrained
    devices. For example, Intel AVX instructions can lead up to 60,000 times more
    speed-up than native Python code.
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/d98b5be95079870a147edb0259bd824c.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
- en: 'Speedups from performance engineering a program that multiplies two 4096-by-4096
    matrices. Source: [There’s plenty of room at the Top: What will drive computer
    performance after Moore’s law?](https://www.science.org/doi/10.1126/science.aam9744)[3]'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: Tensor cores on Nvidia graphics (V100, A100, H100, etc.) can multiply and add
    two FP16 and/or FP32 matrices in one clock cycle, compared to Cuda cores which
    can only perform 1 operation per cycle. However, the utilization of tensor core
    is very low (3% — 9% for end-to-end training), resulting in high energy cost and
    low performance. There is active research on improving systolic array utilization
    (FlexSA, multi-directional SA, etc. ) that I will write in the next series of
    posts.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/19ef15d03c9beb4c8297c88e9671ef35.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
- en: 'Systolic array. source: [Telesens](https://www.telesens.co/2018/07/30/systolic-architectures/)'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: In addition, as memory and data traffic is always a bottleneck for large AI
    models, it’s crucial to explore advanced architecture considering bigger and more
    efficient memory on chip. One example is the Cerebras core memory design, where
    memory is independently addressed per core.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，由于内存和数据流量始终是大型 AI 模型的瓶颈，因此探索更先进的架构以考虑更大、更高效的芯片内存至关重要。一个例子是 Cerebras 核心内存设计，其中内存按核心独立寻址。
- en: '![](../Images/57cdd1c3a2b49ffb249e1e09bbcda521.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/57cdd1c3a2b49ffb249e1e09bbcda521.png)'
- en: Cerebras memory architecture
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: Cerebras 内存架构
- en: 'There are a lot of other optimizations: Parallelism, KV cache quantization
    for LLMs, sparse activation, and End-to-End Optimization — I will explain more
    in upcoming posts'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 还有许多其他优化：并行性、LLMs 的 KV 缓存量化、稀疏激活和端到端优化——我将在即将发布的帖子中详细解释
- en: '**4.4 Power and energy efficiency**'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '**4.4 功率和能效**'
- en: Power is another beast we need to look into, especially for low-power user scenarios.
    It is always a tradeoff between performance and power. As illustrated below, the
    memory access operation takes a couple of orders of magnitude more energy than
    computation operations. Reduction in memory transfer is strongly required to save
    power.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 电力是我们需要关注的另一个问题，尤其是在低功耗用户场景下。性能和功耗之间总是存在权衡。如下面所示，内存访问操作所消耗的能量比计算操作高几个数量级。为了节省电力，强烈需要减少内存传输。
- en: '![](../Images/609e1272fef035cfc6def19460cbd88d.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/609e1272fef035cfc6def19460cbd88d.png)'
- en: 'Energy cost for compute and memory. Source: [Song H from Stanford](https://arxiv.org/pdf/1506.02626v3.pdf)
    [4]'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '计算和内存的能量成本。来源: [Stanford 的 Song H](https://arxiv.org/pdf/1506.02626v3.pdf)
    [4]'
- en: '**Conclusion**'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '**结论**'
- en: Above are major metrics we need to measure for AI model performance, using performance
    profilers like Vtune, Nsight, or other modeling tools, architects can deep dive
    into the next level of performance details (layer-wise compute/memory traffic,
    compute efficiency, BW utilization, etc. ) and identify hotspots leveraging traces.
    Very often, performance could be unexpectedly low because of software inefficiencies.
    It is an iterative process to co-design the AI SW/HW based on proxy models.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 上述是我们需要测量的 AI 模型性能的主要指标，通过使用性能分析工具如 Vtune、Nsight 或其他建模工具，架构师可以深入到性能细节的下一层（层级计算/内存流量、计算效率、带宽利用率等），并利用跟踪数据识别热点。性能往往因为软件低效而意外地低。基于代理模型进行
    AI 软件/硬件的协同设计是一个迭代过程。
- en: It is a collaborative effort to conduct a thorough architectural analysis of
    emerging AI models. The investigation cycle could be pretty long, and there are
    always tradeoffs between product schedule and analysis depth. Strong expertise
    in HW, SW, and AI technologies is required at the same time to architect efficient
    AI solutions.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 对新兴 AI 模型进行彻底的架构分析是一个协作努力。调查周期可能相当长，并且在产品计划和分析深度之间总是存在权衡。同时需要在硬件、软件和 AI 技术方面具备强大的专业知识，以设计高效的
    AI 解决方案。
- en: Reference
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody
    Hao Yu, Joseph E. Gonzalez, Hao Zhang, Ion Stoica, [Efficient Memory Management
    for Large Language Model Serving with Paged Attention](https://arxiv.org/abs/2309.06180),
    2023, arxiv'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody
    Hao Yu, Joseph E. Gonzalez, Hao Zhang, Ion Stoica, [Efficient Memory Management
    for Large Language Model Serving with Paged Attention](https://arxiv.org/abs/2309.06180),
    2023, arxiv'
- en: '[2] Charles Packer, Vivian Fang, Shishir G. Patil, Kevin Lin, Sarah Wooders,
    Joseph E. Gonzalez, [MemGPT: Towards LLMs as Operating Systems](https://arxiv.org/abs/2310.08560),
    2023, arxiv'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] Charles Packer, Vivian Fang, Shishir G. Patil, Kevin Lin, Sarah Wooders,
    Joseph E. Gonzalez, [MemGPT: Towards LLMs as Operating Systems](https://arxiv.org/abs/2310.08560),
    2023, arxiv'
- en: '[3] Charles Leiserson, Neil Thompson, Joel Emer, Bradley Kuszmaul, Butler Lampson,
    Daniel Sanchez, and Tao Schardl, [There’s plenty of room at the Top: What will
    drive computer performance after Moore’s law?](https://www.science.org/doi/10.1126/science.aam9744)
    2020, Science'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] Charles Leiserson, Neil Thompson, Joel Emer, Bradley Kuszmaul, Butler Lampson,
    Daniel Sanchez, 和 Tao Schardl, [There’s plenty of room at the Top: What will drive
    computer performance after Moore’s law?](https://www.science.org/doi/10.1126/science.aam9744)
    2020, Science'
- en: '[4] Song Han, Jeff Pool, John Tran, William J. Dally, [Learning both Weights
    and Connections for Efficient Neural Networks](https://arxiv.org/pdf/1506.02626v3.pdf),
    2015, arxiv'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] Song Han, Jeff Pool, John Tran, William J. Dally, [Learning both Weights
    and Connections for Efficient Neural Networks](https://arxiv.org/pdf/1506.02626v3.pdf),
    2015, arxiv'
