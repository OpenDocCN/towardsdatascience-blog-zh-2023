- en: 'Precision Clustering Made Simple: kscorer‚Äôs Guide to Auto-Selecting Optimal
    K-means Clusters'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ÂéüÊñáÔºö[https://towardsdatascience.com/precision-clustering-made-simple-kscorers-guide-to-auto-selecting-optimal-k-means-clusters-51fb39fde44c?source=collection_archive---------3-----------------------#2023-11-10](https://towardsdatascience.com/precision-clustering-made-simple-kscorers-guide-to-auto-selecting-optimal-k-means-clusters-51fb39fde44c?source=collection_archive---------3-----------------------#2023-11-10)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: kscorer streamlines the process of clustering and provides practical approach
    to data analysis through advanced scoring and parallelization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://wldmrgml.medium.com/?source=post_page-----51fb39fde44c--------------------------------)[![Volodymyr
    Holomb](../Images/ff4a34f4dc4ee397d4d30512aa8f177c.png)](https://wldmrgml.medium.com/?source=post_page-----51fb39fde44c--------------------------------)[](https://towardsdatascience.com/?source=post_page-----51fb39fde44c--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----51fb39fde44c--------------------------------)
    [Volodymyr Holomb](https://wldmrgml.medium.com/?source=post_page-----51fb39fde44c--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ¬∑
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F95923fba037b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fprecision-clustering-made-simple-kscorers-guide-to-auto-selecting-optimal-k-means-clusters-51fb39fde44c&user=Volodymyr+Holomb&userId=95923fba037b&source=post_page-95923fba037b----51fb39fde44c---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----51fb39fde44c--------------------------------)
    ¬∑7 min read¬∑Nov 10, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F51fb39fde44c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fprecision-clustering-made-simple-kscorers-guide-to-auto-selecting-optimal-k-means-clusters-51fb39fde44c&user=Volodymyr+Holomb&userId=95923fba037b&source=-----51fb39fde44c---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F51fb39fde44c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fprecision-clustering-made-simple-kscorers-guide-to-auto-selecting-optimal-k-means-clusters-51fb39fde44c&source=-----51fb39fde44c---------------------bookmark_footer-----------)![](../Images/0d4ed07360aaaec601e971efa3c3fee4.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Made by DALL-E-2 according to the author‚Äôs description
  prefs: []
  type: TYPE_NORMAL
- en: Unsupervised machine learning, particularly clustering, is a challenging task
    in data science. It is crucial to a wide range of practical business analytics
    projects. Clustering can operate on its own, but it is also a valuable component
    in complex data processing pipelines that enhance the efficiency of other algorithms.
    For instance, clustering plays a crucial role when developing a [recommender system](https://medium.com/towards-data-science/building-memory-efficient-meta-hybrid-recommender-engine-back-to-front-part-2-51a7d4546e90).
  prefs: []
  type: TYPE_NORMAL
- en: Well, Scikit-Learn notoriously offers various proven clustering algorithms.
    Nonetheless, most of them are parametric and require setting the number of clusters,
    which is one of the most significant challenges in clustering.
  prefs: []
  type: TYPE_NORMAL
- en: Commonly, an iterative method is used to decide on the optimal number of clusters
    when working with data. It means that you carry out clustering multiple times,
    each time with a different number of clusters, and evaluate the corresponding
    result. While this technique is useful, it does have limitations.
  prefs: []
  type: TYPE_NORMAL
- en: The [yellowbrick package](https://www.scikit-yb.org/en/latest/api/cluster/index.html)
    is a commonly employed tool that makes it easy to identify the optimal number
    of clusters. However, it also has some drawbacks. One significant drawback is
    the possibility of conflicting outcomes when evaluating multiple metrics and the
    challenge of identifying an [elbow](https://en.wikipedia.org/wiki/Elbow_method_(clustering))
    on the diagram.
  prefs: []
  type: TYPE_NORMAL
- en: In addition, the dataset size poses another problem, regardless of the package
    used. When working with large datasets, resource consumption difficulties may
    impede your ability to efficiently iterate through a wide range of clusters. If
    this is the case, consider exploring techniques such as [MiniBatchKMeans](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.MiniBatchKMeans.html),
    which can afford parallel clustering.
  prefs: []
  type: TYPE_NORMAL
- en: But advanced optimization of your clustering routine may require lesser-known
    techniques, described further. You will also get to know the [kscorer package](https://pypi.org/project/kscorer/),
    which streamlines these techniques, offering a more robust and efficient approach
    to determining the optimal number of clusters.
  prefs: []
  type: TYPE_NORMAL
- en: 'Without a hitch, those techniques are:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Dimensionality Reduction.** It may be beneficial to perform a Principal Component
    Analysis (PCA) on the data before applying the clustering algorithm. This will
    reduce data interference and lead to a more reliable clustering process.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cosine Similarity.** There is a straightforward way to use (approx.) cosine
    distances in K-means via applying Euclidean normalisation to the data. So that
    you do not need to pre-calculate distance matrix, such as while performing agglomerative
    clustering.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Many-Metrics-At-Hand.** To find the optimal number of clusters, one should
    rely on a multi-metric assessment instead of depending on a single metric.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data Sampling.** To address resource consumption issues and improve clustering
    results one can get random samples from data to perform clustering operations
    and asses metrics. Averaging scores from multiple iterations can reduce the effect
    of randomness and produce more consistent results.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This workflow is illustrated below.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c39dd7640649cc00f95f26c863aab0f2.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by an author
  prefs: []
  type: TYPE_NORMAL
- en: Luckily, there is no need to build this entire pipeline from scratch as an implementation
    is currently available in the [kscorer package](https://pypi.org/project/kscorer/).
  prefs: []
  type: TYPE_NORMAL
- en: Now, let‚Äôs delve a bit deeper
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'I once heard a data scientist state at a conference talk: ‚ÄúBasically, **you
    can do what you want, as long as you know what you are doing**.‚Äù ¬© [Alex2006](https://datascience.stackexchange.com/a/36003/101016)'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: It is recommended to scale your data before clustering to ensure all features
    are on an equal footing and none dominate due to their magnitude. **Standardisation**
    (centred around the mean and scaled by the standard deviation) or **Min-Max scaling**
    (scaling values to a specified range) are common techniques used for scaling.
  prefs: []
  type: TYPE_NORMAL
- en: It‚Äôs worth noting that the significance of feature scaling, perfectly [illustrated
    here](https://scikit-learn.org/stable/auto_examples/preprocessing/plot_scaling_importance.html),
    isn‚Äôt restricted to just KNeighbors models but also applies to various data science
    methodologies. Standardising features through z-score normalisation ensures that
    all features are on the same scale, preventing any feature from dominating the
    model adjustment due to their magnitudes. This scaling procedure can significantly
    impact the model‚Äôs performance, resulting in different model adjustments when
    compared to using unscaled data.
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, there is a fundamental link between K-means clustering and **PCA**,
    explored in Ding and He‚Äôs paper [‚ÄúK-means Clustering via Principal Component Analysis‚Äù](https://ranger.uta.edu/%7Echqding/papers/KmeansPCA1.pdf).
    Although initially serving distinct purposes, these techniques ultimately aim
    to efficiently represent data whilst minimising reconstruction errors. PCA aims
    to represent data vectors as a combination of a reduced number of eigenvectors.
    In contrast, K-means clustering aims to represent data vectors as a combination
    of cluster centroid vectors. Both approaches strive to minimise the mean-squared
    reconstruction error.
  prefs: []
  type: TYPE_NORMAL
- en: After applying PCA, we‚Äôll scale again our data due to computational issues that
    might arise (some values may be close to zero while others will be quite large).
    This perfectly makes sense as we already have lost track of our initial features
    (after PCA), so there will be no interpretation of data.
  prefs: []
  type: TYPE_NORMAL
- en: Another interesting correlation that may not be commonly known is between **Cosine
    Similarity** and Euclidean Distance. [Understanding the relationship](https://medium.com/ai-for-real/relationship-between-cosine-similarity-and-euclidean-distance-7e283a277dff)
    between these measures is crucial when they are used interchangeably, albeit indirectly.
    This knowledge has practical application in transforming the traditional K-means
    Clustering Algorithm into the Spherical K-means Clustering Algorithm, where cosine
    similarity is a vital metric for clustering data. As mentioned previously, we
    can ‚Äúestablish‚Äù the connection between cosine similarity and Euclidean distance
    by applying Euclidean normalisation to data.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the absence of ground truth cluster labels, the evaluation of clustering
    models must rely on intrinsic measures, and the [kscorer package](https://pypi.org/project/kscorer/)
    offers a comprehensive set of indicators to assess the quality of clustering.
    These indicators suggest valuable insight into the degree of separation among
    recognised clusters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[**Silhouette Coefficient**](https://scikit-learn.org/stable/modules/clustering.html#silhouette-coefficient).
    It quantifies the separation of clusters by calculating the difference between
    the mean distance to the nearest cluster that a data point does not belong to
    and the mean intra-cluster distance for each data point. The outcome is standardised
    and expressed as the ratio between the two, with elevated values indicating superior
    cluster separation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[**Calinski-Harabasz Index**](https://scikit-learn.org/stable/modules/clustering.html#calinski-harabasz-index).
    It calculates the ratio of between-cluster scattering to within-cluster scattering.
    A higher score on the Calinski-Harabasz test indicates better clustering performance,
    indicating well-defined clusters.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[**Davies-Bouldin Index**](https://scikit-learn.org/stable/modules/clustering.html#davies-bouldin-index).
    It measures the ratio of between-cluster dispersion to within-cluster dispersion,
    with a lower value indicating superior clustering performance and more distinct
    clusters.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[**Dunn Index.**](https://en.wikipedia.org/wiki/Dunn_index) It assesses cluster
    quality by comparing intercluster distance (the smallest distance between any
    two cluster centroids) to intracluster distance (the largest distance between
    any two points within a cluster). A higher Dunn Index indicates more well-defined
    clusters.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The Python calculation for the index utilized in the package is outlined as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[**Bayesian Information Criterion (BIC)**](https://stackoverflow.com/a/35379657/6025592).
    BIC serves as an additional and to some extent an independent metric. While K-means
    does not offer a direct probabilistic model, BIC can help estimate the data‚Äôs
    distribution after applying a K-means model. This approach provides a more comprehensive
    assessment of the cluster quality.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All metrics are standardised, guaranteeing that higher scores consistently indicate
    well-defined clusters. This thorough evaluation is crucial in identifying the
    optimal number of clusters in a dataset.
  prefs: []
  type: TYPE_NORMAL
- en: To overcome memory limitations and execute data preprocessing and scoring operations
    expediently for K-means clustering, the [kscorer package](https://pypi.org/project/kscorer/)
    utilises N random data samples. This approach ensures seamless execution and adapts
    to datasets of different sizes and structures. Similar to cross-validation techniques,
    it maintains robust results, even though each iteration focuses on a limited subset
    of the data.
  prefs: []
  type: TYPE_NORMAL
- en: Hands-on with kscorer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So, we have some data for clustering. Please note that we pretend not to know
    the exact number of clusters in this scenario.
  prefs: []
  type: TYPE_NORMAL
- en: 'Moving forward, we will split our dataset into train and test sets and fit
    a model to detect the optimal number of clusters. The model will automatically
    search for the optimal number of clusters between 3 and 15\. This can be effortlessly
    achieved as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: After completing the fitting process, we can review the scaled scores for all
    the metrics applied. This will help us to determine the best number of clusters
    for our available data. When checking the plot, you will notice that some clusters
    are highlighted with corresponding scores. These labelled points correspond with
    the local maxima in the average scores across all metrics and thus represent the
    best options for selecting the optimal number of clusters.
  prefs: []
  type: TYPE_NORMAL
- en: Now, we can evaluate how well our new cluster labels match the true labels.
    Be sure that this option is usually not available in practical business scenarios
    üòâ
  prefs: []
  type: TYPE_NORMAL
- en: In an unusual move in clustering, you could try to cluster data that hasn‚Äôt
    been seen before. But note that this isn‚Äôt a typical clustering task. A different
    and often more useful strategy would be to make a classifier using the cluster
    labels as targets. This will make it easier to assign cluster labels to new data.
  prefs: []
  type: TYPE_NORMAL
- en: And, finally, a fresh [interactive perspective](https://medium.com/analytics-vidhya/visualizing-data-made-easy-with-prosphera-40f8994ee60f)
    on our data.
  prefs: []
  type: TYPE_NORMAL
- en: So, that is how we‚Äôve delved into K-means clustering using the [kscorer package](https://pypi.org/project/kscorer/),
    which streamlines the process of finding the optimal number of clusters. Due to
    its complex metrics and parallel processing, it has proved to be a practical tool
    for data analysis.
  prefs: []
  type: TYPE_NORMAL
