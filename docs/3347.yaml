- en: 'Precision Clustering Made Simple: kscorer’s Guide to Auto-Selecting Optimal
    K-means Clusters'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 精准聚类简化版：kscorer 的自动选择最佳 K-means 聚类指南
- en: 原文：[https://towardsdatascience.com/precision-clustering-made-simple-kscorers-guide-to-auto-selecting-optimal-k-means-clusters-51fb39fde44c?source=collection_archive---------3-----------------------#2023-11-10](https://towardsdatascience.com/precision-clustering-made-simple-kscorers-guide-to-auto-selecting-optimal-k-means-clusters-51fb39fde44c?source=collection_archive---------3-----------------------#2023-11-10)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/precision-clustering-made-simple-kscorers-guide-to-auto-selecting-optimal-k-means-clusters-51fb39fde44c?source=collection_archive---------3-----------------------#2023-11-10](https://towardsdatascience.com/precision-clustering-made-simple-kscorers-guide-to-auto-selecting-optimal-k-means-clusters-51fb39fde44c?source=collection_archive---------3-----------------------#2023-11-10)
- en: kscorer streamlines the process of clustering and provides practical approach
    to data analysis through advanced scoring and parallelization
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: kscorer 简化了聚类过程，通过先进的评分和并行化提供了实用的数据分析方法
- en: '[](https://wldmrgml.medium.com/?source=post_page-----51fb39fde44c--------------------------------)[![Volodymyr
    Holomb](../Images/ff4a34f4dc4ee397d4d30512aa8f177c.png)](https://wldmrgml.medium.com/?source=post_page-----51fb39fde44c--------------------------------)[](https://towardsdatascience.com/?source=post_page-----51fb39fde44c--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----51fb39fde44c--------------------------------)
    [Volodymyr Holomb](https://wldmrgml.medium.com/?source=post_page-----51fb39fde44c--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://wldmrgml.medium.com/?source=post_page-----51fb39fde44c--------------------------------)[![Volodymyr
    Holomb](../Images/ff4a34f4dc4ee397d4d30512aa8f177c.png)](https://wldmrgml.medium.com/?source=post_page-----51fb39fde44c--------------------------------)[](https://towardsdatascience.com/?source=post_page-----51fb39fde44c--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----51fb39fde44c--------------------------------)
    [Volodymyr Holomb](https://wldmrgml.medium.com/?source=post_page-----51fb39fde44c--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F95923fba037b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fprecision-clustering-made-simple-kscorers-guide-to-auto-selecting-optimal-k-means-clusters-51fb39fde44c&user=Volodymyr+Holomb&userId=95923fba037b&source=post_page-95923fba037b----51fb39fde44c---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----51fb39fde44c--------------------------------)
    ·7 min read·Nov 10, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F51fb39fde44c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fprecision-clustering-made-simple-kscorers-guide-to-auto-selecting-optimal-k-means-clusters-51fb39fde44c&user=Volodymyr+Holomb&userId=95923fba037b&source=-----51fb39fde44c---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F95923fba037b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fprecision-clustering-made-simple-kscorers-guide-to-auto-selecting-optimal-k-means-clusters-51fb39fde44c&user=Volodymyr+Holomb&userId=95923fba037b&source=post_page-95923fba037b----51fb39fde44c---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----51fb39fde44c--------------------------------)
    · 7分钟阅读·2023年11月10日'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F51fb39fde44c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fprecision-clustering-made-simple-kscorers-guide-to-auto-selecting-optimal-k-means-clusters-51fb39fde44c&source=-----51fb39fde44c---------------------bookmark_footer-----------)![](../Images/0d4ed07360aaaec601e971efa3c3fee4.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F51fb39fde44c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fprecision-clustering-made-simple-kscorers-guide-to-auto-selecting-optimal-k-means-clusters-51fb39fde44c&source=-----51fb39fde44c---------------------bookmark_footer-----------)![](../Images/0d4ed07360aaaec601e971efa3c3fee4.png)'
- en: Made by DALL-E-2 according to the author’s description
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 由 DALL-E-2 根据作者的描述制作
- en: Unsupervised machine learning, particularly clustering, is a challenging task
    in data science. It is crucial to a wide range of practical business analytics
    projects. Clustering can operate on its own, but it is also a valuable component
    in complex data processing pipelines that enhance the efficiency of other algorithms.
    For instance, clustering plays a crucial role when developing a [recommender system](https://medium.com/towards-data-science/building-memory-efficient-meta-hybrid-recommender-engine-back-to-front-part-2-51a7d4546e90).
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 无监督机器学习，尤其是聚类，在数据科学中是一项具有挑战性的任务。它对广泛的实际商业分析项目至关重要。聚类可以独立运行，但它也是复杂数据处理管道中的一个有价值的组成部分，这些管道提升了其他算法的效率。例如，聚类在开发[推荐系统](https://medium.com/towards-data-science/building-memory-efficient-meta-hybrid-recommender-engine-back-to-front-part-2-51a7d4546e90)时起着关键作用。
- en: Well, Scikit-Learn notoriously offers various proven clustering algorithms.
    Nonetheless, most of them are parametric and require setting the number of clusters,
    which is one of the most significant challenges in clustering.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧，Scikit-Learn著名地提供了各种经过验证的聚类算法。尽管如此，其中大多数都是参数化的，需要设置簇数，这是聚类中最重要的挑战之一。
- en: Commonly, an iterative method is used to decide on the optimal number of clusters
    when working with data. It means that you carry out clustering multiple times,
    each time with a different number of clusters, and evaluate the corresponding
    result. While this technique is useful, it does have limitations.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，使用迭代方法来决定最佳的簇数。这意味着你多次进行聚类，每次使用不同的簇数，并评估相应的结果。虽然这种技术很有用，但也有其局限性。
- en: The [yellowbrick package](https://www.scikit-yb.org/en/latest/api/cluster/index.html)
    is a commonly employed tool that makes it easy to identify the optimal number
    of clusters. However, it also has some drawbacks. One significant drawback is
    the possibility of conflicting outcomes when evaluating multiple metrics and the
    challenge of identifying an [elbow](https://en.wikipedia.org/wiki/Elbow_method_(clustering))
    on the diagram.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '[yellowbrick包](https://www.scikit-yb.org/en/latest/api/cluster/index.html)是一个常用工具，可以轻松识别最佳簇数。然而，它也有一些缺点。一个显著的缺点是评估多个指标时可能会出现冲突的结果，以及在图表上识别[肘部](https://en.wikipedia.org/wiki/Elbow_method_(clustering))的挑战。'
- en: In addition, the dataset size poses another problem, regardless of the package
    used. When working with large datasets, resource consumption difficulties may
    impede your ability to efficiently iterate through a wide range of clusters. If
    this is the case, consider exploring techniques such as [MiniBatchKMeans](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.MiniBatchKMeans.html),
    which can afford parallel clustering.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，数据集的大小也是一个问题，不论使用何种包。当处理大数据集时，资源消耗困难可能会妨碍你有效地迭代多个簇。如果是这种情况，可以考虑探索诸如[MiniBatchKMeans](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.MiniBatchKMeans.html)等技术，它可以实现并行聚类。
- en: But advanced optimization of your clustering routine may require lesser-known
    techniques, described further. You will also get to know the [kscorer package](https://pypi.org/project/kscorer/),
    which streamlines these techniques, offering a more robust and efficient approach
    to determining the optimal number of clusters.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 但对你的聚类程序进行高级优化可能需要一些鲜为人知的技术，稍后会详细介绍。你还将了解[kscorer包](https://pypi.org/project/kscorer/)，它简化了这些技术，提供了一种更强大和高效的方法来确定最佳簇数。
- en: 'Without a hitch, those techniques are:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 这些技术毫无疑问是：
- en: '**Dimensionality Reduction.** It may be beneficial to perform a Principal Component
    Analysis (PCA) on the data before applying the clustering algorithm. This will
    reduce data interference and lead to a more reliable clustering process.'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**降维。** 在应用聚类算法之前，对数据进行主成分分析（PCA）可能会有益。这将减少数据干扰，并导致更可靠的聚类过程。'
- en: '**Cosine Similarity.** There is a straightforward way to use (approx.) cosine
    distances in K-means via applying Euclidean normalisation to the data. So that
    you do not need to pre-calculate distance matrix, such as while performing agglomerative
    clustering.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**余弦相似度。** 有一种简单的方法可以在K-means中使用（近似）余弦距离，即通过对数据进行欧几里得归一化。这样你就不需要预先计算距离矩阵，比如在进行凝聚聚类时。'
- en: '**Many-Metrics-At-Hand.** To find the optimal number of clusters, one should
    rely on a multi-metric assessment instead of depending on a single metric.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**多指标手头。** 为了找到最佳簇数，应该依赖多指标评估，而不是仅仅依靠单一指标。'
- en: '**Data Sampling.** To address resource consumption issues and improve clustering
    results one can get random samples from data to perform clustering operations
    and asses metrics. Averaging scores from multiple iterations can reduce the effect
    of randomness and produce more consistent results.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据采样。** 为了解决资源消耗问题并改进聚类结果，可以从数据中获取随机样本以执行聚类操作并评估指标。通过多次迭代的平均得分可以减少随机性的影响，从而产生更一致的结果。'
- en: This workflow is illustrated below.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 下面展示了这个工作流程。
- en: '![](../Images/c39dd7640649cc00f95f26c863aab0f2.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c39dd7640649cc00f95f26c863aab0f2.png)'
- en: Image by an author
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供
- en: Luckily, there is no need to build this entire pipeline from scratch as an implementation
    is currently available in the [kscorer package](https://pypi.org/project/kscorer/).
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，无需从头开始构建整个管道，因为 [kscorer 包](https://pypi.org/project/kscorer/)中已经有现成的实现。
- en: Now, let’s delve a bit deeper
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 现在，让我们深入探讨一下
- en: 'I once heard a data scientist state at a conference talk: “Basically, **you
    can do what you want, as long as you know what you are doing**.” © [Alex2006](https://datascience.stackexchange.com/a/36003/101016)'
  id: totrans-25
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 我曾在一次会议演讲中听到一位数据科学家说：“基本上，只要你知道自己在做什么，**你可以做任何你想做的事情**。” © [Alex2006](https://datascience.stackexchange.com/a/36003/101016)
- en: It is recommended to scale your data before clustering to ensure all features
    are on an equal footing and none dominate due to their magnitude. **Standardisation**
    (centred around the mean and scaled by the standard deviation) or **Min-Max scaling**
    (scaling values to a specified range) are common techniques used for scaling.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在聚类之前建议对数据进行缩放，以确保所有特征处于同一水平，避免因特征的大小而主导。**标准化**（以均值为中心并按标准差缩放）或**最小-最大缩放**（将值缩放到指定范围）是常用的缩放技术。
- en: It’s worth noting that the significance of feature scaling, perfectly [illustrated
    here](https://scikit-learn.org/stable/auto_examples/preprocessing/plot_scaling_importance.html),
    isn’t restricted to just KNeighbors models but also applies to various data science
    methodologies. Standardising features through z-score normalisation ensures that
    all features are on the same scale, preventing any feature from dominating the
    model adjustment due to their magnitudes. This scaling procedure can significantly
    impact the model’s performance, resulting in different model adjustments when
    compared to using unscaled data.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，特征缩放的重要性在[这里](https://scikit-learn.org/stable/auto_examples/preprocessing/plot_scaling_importance.html)得到了完美的说明，它不仅限于
    KNeighbors 模型，还适用于各种数据科学方法。通过 z-score 归一化对特征进行标准化可以确保所有特征在相同的尺度上，防止任何特征因其大小而主导模型调整。这个缩放过程可能会显著影响模型的性能，与使用未缩放数据时进行的模型调整相比，可能会导致不同的模型调整。
- en: Furthermore, there is a fundamental link between K-means clustering and **PCA**,
    explored in Ding and He’s paper [“K-means Clustering via Principal Component Analysis”](https://ranger.uta.edu/%7Echqding/papers/KmeansPCA1.pdf).
    Although initially serving distinct purposes, these techniques ultimately aim
    to efficiently represent data whilst minimising reconstruction errors. PCA aims
    to represent data vectors as a combination of a reduced number of eigenvectors.
    In contrast, K-means clustering aims to represent data vectors as a combination
    of cluster centroid vectors. Both approaches strive to minimise the mean-squared
    reconstruction error.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，K-means 聚类与**主成分分析（PCA）**之间存在基本联系，这在丁和赫的论文 [“K-means Clustering via Principal
    Component Analysis”](https://ranger.uta.edu/%7Echqding/papers/KmeansPCA1.pdf)中进行了探讨。尽管最初这两种技术的目的不同，但最终它们都旨在有效地表示数据，同时最小化重构误差。PCA
    旨在将数据向量表示为减少数量的特征向量的组合。而 K-means 聚类则旨在将数据向量表示为簇中心向量的组合。这两种方法都力求最小化均方重构误差。
- en: After applying PCA, we’ll scale again our data due to computational issues that
    might arise (some values may be close to zero while others will be quite large).
    This perfectly makes sense as we already have lost track of our initial features
    (after PCA), so there will be no interpretation of data.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在应用 PCA 后，由于可能出现的计算问题（有些值可能接近零，而其他值可能非常大），我们需要再次对数据进行缩放。这完全有意义，因为我们在 PCA 后已经失去了对初始特征的跟踪，因此数据将无法解释。
- en: Another interesting correlation that may not be commonly known is between **Cosine
    Similarity** and Euclidean Distance. [Understanding the relationship](https://medium.com/ai-for-real/relationship-between-cosine-similarity-and-euclidean-distance-7e283a277dff)
    between these measures is crucial when they are used interchangeably, albeit indirectly.
    This knowledge has practical application in transforming the traditional K-means
    Clustering Algorithm into the Spherical K-means Clustering Algorithm, where cosine
    similarity is a vital metric for clustering data. As mentioned previously, we
    can “establish” the connection between cosine similarity and Euclidean distance
    by applying Euclidean normalisation to data.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个可能不为人知的有趣相关性是**余弦相似度**与欧几里得距离之间的关系。[理解这种关系](https://medium.com/ai-for-real/relationship-between-cosine-similarity-and-euclidean-distance-7e283a277dff)在这些度量间接互换使用时至关重要。这些知识在将传统的K-means聚类算法转换为球面K-means聚类算法时具有实际应用，其中余弦相似度是聚类数据的关键指标。如前所述，我们可以通过对数据应用欧几里得归一化来“建立”余弦相似度与欧几里得距离之间的联系。
- en: 'In the absence of ground truth cluster labels, the evaluation of clustering
    models must rely on intrinsic measures, and the [kscorer package](https://pypi.org/project/kscorer/)
    offers a comprehensive set of indicators to assess the quality of clustering.
    These indicators suggest valuable insight into the degree of separation among
    recognised clusters:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在缺乏真实簇标签的情况下，聚类模型的评估必须依赖于内在度量，而[kscorer包](https://pypi.org/project/kscorer/)提供了一套全面的指标来评估聚类质量。这些指标提供了有关识别簇之间分离程度的有价值见解：
- en: '[**Silhouette Coefficient**](https://scikit-learn.org/stable/modules/clustering.html#silhouette-coefficient).
    It quantifies the separation of clusters by calculating the difference between
    the mean distance to the nearest cluster that a data point does not belong to
    and the mean intra-cluster distance for each data point. The outcome is standardised
    and expressed as the ratio between the two, with elevated values indicating superior
    cluster separation.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**轮廓系数**](https://scikit-learn.org/stable/modules/clustering.html#silhouette-coefficient)。它通过计算数据点到其不属于的最近簇的平均距离与每个数据点的簇内平均距离之间的差异来量化簇的分离程度。结果经过标准化，并表示为两者之间的比例，值越高表示簇分离越优越。'
- en: '[**Calinski-Harabasz Index**](https://scikit-learn.org/stable/modules/clustering.html#calinski-harabasz-index).
    It calculates the ratio of between-cluster scattering to within-cluster scattering.
    A higher score on the Calinski-Harabasz test indicates better clustering performance,
    indicating well-defined clusters.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**Calinski-Harabasz指数**](https://scikit-learn.org/stable/modules/clustering.html#calinski-harabasz-index)。它计算簇间散度与簇内散度的比率。Calinski-Harabasz测试得分越高，表示聚类性能越好，簇定义越清晰。'
- en: '[**Davies-Bouldin Index**](https://scikit-learn.org/stable/modules/clustering.html#davies-bouldin-index).
    It measures the ratio of between-cluster dispersion to within-cluster dispersion,
    with a lower value indicating superior clustering performance and more distinct
    clusters.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**Davies-Bouldin指数**](https://scikit-learn.org/stable/modules/clustering.html#davies-bouldin-index)。它衡量簇间离散度与簇内离散度的比率，值越低表示聚类性能越优越，簇的区分度越高。'
- en: '[**Dunn Index.**](https://en.wikipedia.org/wiki/Dunn_index) It assesses cluster
    quality by comparing intercluster distance (the smallest distance between any
    two cluster centroids) to intracluster distance (the largest distance between
    any two points within a cluster). A higher Dunn Index indicates more well-defined
    clusters.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**邓恩指数**](https://en.wikipedia.org/wiki/Dunn_index)。它通过比较簇间距离（任意两个簇质心之间的最小距离）与簇内距离（簇内任意两点之间的最大距离）来评估簇的质量。邓恩指数越高，表示簇定义越清晰。'
- en: 'The Python calculation for the index utilized in the package is outlined as
    follows:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 包中使用的指标的Python计算方法如下：
- en: '[**Bayesian Information Criterion (BIC)**](https://stackoverflow.com/a/35379657/6025592).
    BIC serves as an additional and to some extent an independent metric. While K-means
    does not offer a direct probabilistic model, BIC can help estimate the data’s
    distribution after applying a K-means model. This approach provides a more comprehensive
    assessment of the cluster quality.'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**贝叶斯信息准则 (BIC)**](https://stackoverflow.com/a/35379657/6025592)。BIC作为一个额外的、在某种程度上是独立的度量。虽然K-means没有提供直接的概率模型，但BIC可以帮助估计应用K-means模型后的数据分布。这种方法提供了对簇质量更全面的评估。'
- en: All metrics are standardised, guaranteeing that higher scores consistently indicate
    well-defined clusters. This thorough evaluation is crucial in identifying the
    optimal number of clusters in a dataset.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 所有指标都经过标准化，确保较高的评分始终表示定义明确的聚类。这种彻底的评估对于识别数据集中最佳聚类数至关重要。
- en: To overcome memory limitations and execute data preprocessing and scoring operations
    expediently for K-means clustering, the [kscorer package](https://pypi.org/project/kscorer/)
    utilises N random data samples. This approach ensures seamless execution and adapts
    to datasets of different sizes and structures. Similar to cross-validation techniques,
    it maintains robust results, even though each iteration focuses on a limited subset
    of the data.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 为了克服记忆限制，并迅速执行数据预处理和评分操作以进行K-means聚类，[kscorer包](https://pypi.org/project/kscorer/)利用N个随机数据样本。这种方法确保了无缝执行，并能适应不同大小和结构的数据集。类似于交叉验证技术，它能够保持稳健的结果，即使每次迭代只关注数据的一个有限子集。
- en: Hands-on with kscorer
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用kscorer动手操作
- en: So, we have some data for clustering. Please note that we pretend not to know
    the exact number of clusters in this scenario.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们有一些数据用于聚类。请注意，我们在此场景中假设不知道确切的聚类数。
- en: 'Moving forward, we will split our dataset into train and test sets and fit
    a model to detect the optimal number of clusters. The model will automatically
    search for the optimal number of clusters between 3 and 15\. This can be effortlessly
    achieved as follows:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将把数据集分成训练集和测试集，并拟合一个模型来检测最佳聚类数。该模型将自动在3到15之间搜索最佳聚类数。这可以轻松实现，如下所示：
- en: After completing the fitting process, we can review the scaled scores for all
    the metrics applied. This will help us to determine the best number of clusters
    for our available data. When checking the plot, you will notice that some clusters
    are highlighted with corresponding scores. These labelled points correspond with
    the local maxima in the average scores across all metrics and thus represent the
    best options for selecting the optimal number of clusters.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 完成拟合过程后，我们可以查看所有应用指标的标准化评分。这将帮助我们确定适合我们数据的最佳聚类数。当查看图表时，你会注意到一些聚类被突出显示，并带有相应的评分。这些标记点对应于所有指标的平均评分中的局部最大值，因此代表了选择最佳聚类数的最佳选项。
- en: Now, we can evaluate how well our new cluster labels match the true labels.
    Be sure that this option is usually not available in practical business scenarios
    😉
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以评估我们新的聚类标签与真实标签的匹配程度。请确保这种选项在实际商业场景中通常是不可用的😉
- en: In an unusual move in clustering, you could try to cluster data that hasn’t
    been seen before. But note that this isn’t a typical clustering task. A different
    and often more useful strategy would be to make a classifier using the cluster
    labels as targets. This will make it easier to assign cluster labels to new data.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在聚类中，你可以尝试对之前未见过的数据进行聚类。但请注意，这并不是一个典型的聚类任务。一种不同且通常更有用的策略是使用聚类标签作为目标来构建分类器。这将使得将聚类标签分配给新数据变得更容易。
- en: And, finally, a fresh [interactive perspective](https://medium.com/analytics-vidhya/visualizing-data-made-easy-with-prosphera-40f8994ee60f)
    on our data.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，提供一个新的[互动视角](https://medium.com/analytics-vidhya/visualizing-data-made-easy-with-prosphera-40f8994ee60f)来观察我们的数据。
- en: So, that is how we’ve delved into K-means clustering using the [kscorer package](https://pypi.org/project/kscorer/),
    which streamlines the process of finding the optimal number of clusters. Due to
    its complex metrics and parallel processing, it has proved to be a practical tool
    for data analysis.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，这就是我们如何利用[kscorer包](https://pypi.org/project/kscorer/)深入研究K-means聚类，该包简化了寻找最佳聚类数的过程。由于其复杂的指标和并行处理，它已被证明是一个实用的数据分析工具。
