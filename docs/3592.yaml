- en: A Guide on 12 Tuning Strategies for Production-Ready RAG Applications
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 《生产就绪的RAG应用的12种调整策略指南》
- en: 原文：[https://towardsdatascience.com/a-guide-on-12-tuning-strategies-for-production-ready-rag-applications-7ca646833439?source=collection_archive---------0-----------------------#2023-12-06](https://towardsdatascience.com/a-guide-on-12-tuning-strategies-for-production-ready-rag-applications-7ca646833439?source=collection_archive---------0-----------------------#2023-12-06)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/a-guide-on-12-tuning-strategies-for-production-ready-rag-applications-7ca646833439?source=collection_archive---------0-----------------------#2023-12-06](https://towardsdatascience.com/a-guide-on-12-tuning-strategies-for-production-ready-rag-applications-7ca646833439?source=collection_archive---------0-----------------------#2023-12-06)
- en: How to improve the performance of your Retrieval-Augmented Generation (RAG)
    pipeline with these “hyperparameters” and tuning strategies
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何通过这些“超参数”和调整策略来提升你的检索增强生成（RAG）管道的性能
- en: '[](https://medium.com/@iamleonie?source=post_page-----7ca646833439--------------------------------)[![Leonie
    Monigatti](../Images/4044b1685ada53a30160b03dc78f9626.png)](https://medium.com/@iamleonie?source=post_page-----7ca646833439--------------------------------)[](https://towardsdatascience.com/?source=post_page-----7ca646833439--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----7ca646833439--------------------------------)
    [Leonie Monigatti](https://medium.com/@iamleonie?source=post_page-----7ca646833439--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@iamleonie?source=post_page-----7ca646833439--------------------------------)[![Leonie
    Monigatti](../Images/4044b1685ada53a30160b03dc78f9626.png)](https://medium.com/@iamleonie?source=post_page-----7ca646833439--------------------------------)[](https://towardsdatascience.com/?source=post_page-----7ca646833439--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----7ca646833439--------------------------------)
    [Leonie Monigatti](https://medium.com/@iamleonie?source=post_page-----7ca646833439--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F3a38da70d8dc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-guide-on-12-tuning-strategies-for-production-ready-rag-applications-7ca646833439&user=Leonie+Monigatti&userId=3a38da70d8dc&source=post_page-3a38da70d8dc----7ca646833439---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----7ca646833439--------------------------------)
    ·10 min read·Dec 6, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F7ca646833439&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-guide-on-12-tuning-strategies-for-production-ready-rag-applications-7ca646833439&user=Leonie+Monigatti&userId=3a38da70d8dc&source=-----7ca646833439---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F3a38da70d8dc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-guide-on-12-tuning-strategies-for-production-ready-rag-applications-7ca646833439&user=Leonie+Monigatti&userId=3a38da70d8dc&source=post_page-3a38da70d8dc----7ca646833439---------------------post_header-----------)
    发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----7ca646833439--------------------------------)
    · 10 min 阅读 · 2023年12月6日 [](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F7ca646833439&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-guide-on-12-tuning-strategies-for-production-ready-rag-applications-7ca646833439&user=Leonie+Monigatti&userId=3a38da70d8dc&source=-----7ca646833439---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F7ca646833439&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-guide-on-12-tuning-strategies-for-production-ready-rag-applications-7ca646833439&source=-----7ca646833439---------------------bookmark_footer-----------)![](../Images/b8c76b67a513278e3f87e66e0d22bb7a.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F7ca646833439&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-guide-on-12-tuning-strategies-for-production-ready-rag-applications-7ca646833439&source=-----7ca646833439---------------------bookmark_footer-----------)![](../Images/b8c76b67a513278e3f87e66e0d22bb7a.png)'
- en: Tuning Strategies for Retrieval-Augmented Generation Applications
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 《检索增强生成（RAG）应用的调整策略》
- en: Data Science is an experimental science. It starts with the “No Free Lunch Theorem,”
    which states that there is no one-size-fits-all algorithm that works best for
    every problem. And it results in data scientists using [experiment tracking systems](https://medium.com/@iamleonie/intro-to-mlops-experiment-tracking-for-machine-learning-858e432bd133)
    to help them [tune the hyperparameters of their Machine Learning (ML) projects
    to achieve the best performance](https://medium.com/towards-data-science/intermediate-deep-learning-with-transfer-learning-f1aba5a814f).
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 数据科学是一门实验性科学。它以“无免费午餐定理”开始，该定理指出，没有一种万能的算法可以适用于所有问题。这导致数据科学家使用[实验跟踪系统](https://medium.com/@iamleonie/intro-to-mlops-experiment-tracking-for-machine-learning-858e432bd133)来帮助他们[调整机器学习（ML）项目的超参数，以实现最佳性能](https://medium.com/towards-data-science/intermediate-deep-learning-with-transfer-learning-f1aba5a814f).
- en: This article looks at a [Retrieval-Augmented Generation (RAG) pipeline](https://medium.com/towards-data-science/retrieval-augmented-generation-rag-from-theory-to-langchain-implementation-4e9bd5f6a4f2)
    through the eyes of a data scientist. It discusses potential “hyperparameters”
    you can experiment with to improve your RAG pipeline’s performance. Similar to
    experimentation in Deep Learning, where, e.g., data augmentation techniques are
    not a hyperparameter but a knob you can tune and experiment with, this article
    will also cover different strategies you can apply, which are not per se hyperparameters.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 本文从数据科学家的角度审视了[检索增强生成（RAG）管道](https://medium.com/towards-data-science/retrieval-augmented-generation-rag-from-theory-to-langchain-implementation-4e9bd5f6a4f2)。讨论了可以实验的潜在“超参数”以提高RAG管道的性能。类似于深度学习中的实验，在深度学习中，例如，数据增强技术不是超参数，而是一个可以调整和实验的控制旋钮，本文还将涵盖可以应用的不同策略，这些策略本身不一定是超参数。
- en: '[](/retrieval-augmented-generation-rag-from-theory-to-langchain-implementation-4e9bd5f6a4f2?source=post_page-----7ca646833439--------------------------------)
    [## Retrieval-Augmented Generation (RAG): From Theory to LangChain Implementation'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/retrieval-augmented-generation-rag-from-theory-to-langchain-implementation-4e9bd5f6a4f2?source=post_page-----7ca646833439--------------------------------)
    [## 检索增强生成（RAG）：从理论到LangChain实现'
- en: From the theory of the original academic paper to its Python implementation
    with OpenAI, Weaviate, and LangChain
  id: totrans-12
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 从原始学术论文的理论到使用OpenAI、Weaviate和LangChain的Python实现
- en: towardsdatascience.com](/retrieval-augmented-generation-rag-from-theory-to-langchain-implementation-4e9bd5f6a4f2?source=post_page-----7ca646833439--------------------------------)
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/retrieval-augmented-generation-rag-from-theory-to-langchain-implementation-4e9bd5f6a4f2?source=post_page-----7ca646833439--------------------------------)
- en: 'This article covers the following “hyperparameters” sorted by their relevant
    stage. In the [ingestion stage](#4142) of a RAG pipeline, you can achieve performance
    improvements by:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 本文涵盖了按相关阶段排序的“超参数”。在RAG管道的[吞吐阶段](#4142)，你可以通过以下方法实现性能提升：
- en: '[Data cleaning](#196c)'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[数据清洗](#196c)'
- en: '[Chunking](#e45f)'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[数据分块](#e45f)'
- en: '[Embedding models](#156e)'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[嵌入模型](#156e)'
- en: '[Metadata](#2b47)'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[元数据](#2b47)'
- en: '[Multi-indexing](#ce6c)'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[多索引](#ce6c)'
- en: '[Indexing algorithms](#4daa)'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[索引算法](#4daa)'
- en: 'And in the [inferencing stage (retrieval and generation)](#ac53), you can tune:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在[推理阶段（检索和生成）](#ac53)，你可以调整：
- en: '[Query transformations](#a5e2)'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[查询转换](#a5e2)'
- en: '[Retrieval parameters](#fa73)'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[检索参数](#fa73)'
- en: '[Advanced retrieval strategies](#a3bb)'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[高级检索策略](#a3bb)'
- en: '[Re-ranking models](#341d)'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[重新排序模型](#341d)'
- en: '[LLMs](#e9f9)'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[大型语言模型（LLMs）](#e9f9)'
- en: '[Prompt engineering](#9c1c)'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[提示工程](#9c1c)'
- en: Note that this article covers text-use cases of RAG. For multimodal RAG applications,
    different considerations may apply.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，本文涵盖了RAG的文本使用案例。对于多模态RAG应用，可能需要不同的考虑因素。
- en: Ingestion Stage
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 吞吐阶段
- en: 'The ingestion stage is a preparation step for building a RAG pipeline, similar
    to the data cleaning and preprocessing steps in an ML pipeline. Usually, the ingestion
    stage consists of the following steps:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 吞吐阶段是构建RAG管道的准备步骤，类似于ML管道中的数据清洗和预处理步骤。通常，吞吐阶段包括以下步骤：
- en: Collect data
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 收集数据
- en: Chunk data
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 数据分块
- en: Generate vector embeddings of chunks
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 生成数据块的向量嵌入
- en: Store vector embeddings and chunks in a vector database
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在向量数据库中存储向量嵌入和数据块
- en: '![](../Images/c4110b24f69ef4f71f5f3647a41220dc.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c4110b24f69ef4f71f5f3647a41220dc.png)'
- en: Ingestion stage of a RAG pipeline
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: RAG管道的吞吐阶段
- en: This section discusses impactful techniques and hyperparameters that you can
    apply and tune to improve the relevance of the retrieved contexts in the inferencing
    stage.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 本节讨论了在推理阶段可以应用和调整的有影响力的技术和超参数，以提高检索到的上下文的相关性。
- en: Data cleaning
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据清洗
- en: 'Like any Data Science pipeline, the quality of your data heavily impacts the
    outcome in your RAG pipeline [8, 9]. Before moving on to any of the following
    steps, ensure that your data meets the following criteria:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 像任何数据科学管道一样，你的数据质量对RAG管道的结果有着重要影响[8, 9]。在进行以下步骤之前，确保你的数据符合以下标准：
- en: '**Clean**: Apply at least some basic data cleaning techniques commonly used
    in Natural Language Processing, such as making sure all special characters are
    encoded correctly.'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**清洁**：应用至少一些自然语言处理常用的基本数据清理技术，如确保所有特殊字符都正确编码。'
- en: '**Correct**: Make sure your information is consistent and factually accurate
    to avoid conflicting information confusing your LLM.'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**正确**：确保你的信息一致且事实准确，以避免信息冲突让你的LLM感到困惑。'
- en: Chunking
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分块
- en: Chunking your documents is an essential preparation step for your external knowledge
    source in a RAG pipeline that can impact the performance [1, 8, 9]. It is a technique
    to generate logically coherent snippets of information, usually by breaking up
    long documents into smaller sections (but it can also combine smaller snippets
    into coherent paragraphs).
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在RAG管道中，分块你的文档是对外部知识源的一个关键准备步骤，可能会影响性能[1, 8, 9]。这是一种生成逻辑上连贯的信息片段的技术，通常通过将长文档拆分成较小的部分（但也可以将较小的片段合并成连贯的段落）。
- en: One consideration you need to make is the **choice of the chunking technique**.
    For example, in [LangChain, different text splitters](https://python.langchain.com/docs/modules/data_connection/document_transformers/)
    split up documents by different logics, such as by characters, tokens, etc. This
    depends on the type of data you have. For example, you will need to use different
    chunking techniques if your input data is code vs. if it is a Markdown file.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要考虑的一个方面是**分块技术的选择**。例如，在[LangChain中，不同的文本分割器](https://python.langchain.com/docs/modules/data_connection/document_transformers/)根据不同的逻辑拆分文档，如按字符、标记等。这取决于你拥有的数据类型。例如，如果你的输入数据是代码与Markdown文件，你将需要使用不同的分块技术。
- en: 'The ideal **length of your chunk (**`**chunk_size**`**)** depends on your use
    case: If your use case is question answering, you may need shorter specific chunks,
    but if your use case is summarization, you may need longer chunks. Additionally,
    if a chunk is too short, it might not contain enough context. On the other hand,
    if a chunk is too long, it might contain too much irrelevant information.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 理想的**块长度（**`**chunk_size**`**）**取决于你的使用场景：如果你的使用场景是问答，你可能需要较短的具体块；但如果你的使用场景是摘要，你可能需要较长的块。此外，如果块太短，可能包含的上下文不够。另一方面，如果块太长，可能包含过多的无关信息。
- en: Additionally, you will need to think about a **“rolling window” between chunks
    (**`**overlap**`**)** to introduce some additional context.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，你还需要考虑**块之间的“滚动窗口”（**`**overlap**`**）**以引入一些额外的上下文。
- en: Embedding models
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 嵌入模型
- en: Embedding models are at the core of your retrieval. The **quality of your embeddings**
    heavily impacts your retrieval results [1, 4]. Usually, the higher the dimensionality
    of the generated embeddings, the higher the precision of your embeddings.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入模型是你检索的核心。**嵌入的质量**对检索结果有着重大影响[1, 4]。通常，生成的嵌入维度越高，嵌入的精度也越高。
- en: For an idea of what alternative embedding models are available, you can look
    at the [Massive Text Embedding Benchmark (MTEB) Leaderboard](https://huggingface.co/spaces/mteb/leaderboard),
    which covers 164 text embedding models (at the time of this writing).
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 关于可用的替代嵌入模型，你可以查看[MASSIVE TEXT EMBEDDING BENCHMARK (MTEB)排行榜](https://huggingface.co/spaces/mteb/leaderboard)，该排行榜涵盖了164种文本嵌入模型（截至本文撰写时）。
- en: '[](https://huggingface.co/spaces/mteb/leaderboard?source=post_page-----7ca646833439--------------------------------)
    [## MTEB Leaderboard - a Hugging Face Space by mteb'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://huggingface.co/spaces/mteb/leaderboard?source=post_page-----7ca646833439--------------------------------)
    [## MTEB排行榜 - 由mteb提供的Hugging Face空间'
- en: Discover amazing ML apps made by the community
  id: totrans-51
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 发现社区制作的精彩ML应用
- en: huggingface.co](https://huggingface.co/spaces/mteb/leaderboard?source=post_page-----7ca646833439--------------------------------)
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: huggingface.co](https://huggingface.co/spaces/mteb/leaderboard?source=post_page-----7ca646833439--------------------------------)
- en: While you can use general-purpose embedding models out-of-the-box, it may make
    sense to **fine-tune your embedding model** to your specific use case in some
    cases to avoid out-of-domain issues later on [9]. According to experiments conducted
    by LlamaIndex, fine-tuning your embedding model can lead to a [5–10% performance
    increase in retrieval evaluation metrics](https://github.com/run-llama/finetune-embedding/blob/main/evaluate.ipynb)
    [2].
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然你可以直接使用通用的嵌入模型，但在某些情况下，**对你的嵌入模型进行微调**可能更有意义，以避免之后出现领域外的问题 [9]。根据LlamaIndex进行的实验，微调你的嵌入模型可以导致[检索评估指标性能提高5–10%](https://github.com/run-llama/finetune-embedding/blob/main/evaluate.ipynb)
    [2]。
- en: Note that you cannot fine-tune all embedding models (e.g., [OpenAI's](https://platform.openai.com/docs/guides/fine-tuning)
    `[text-ebmedding-ada-002](https://platform.openai.com/docs/guides/fine-tuning)`
    [can’t be fine-tuned at the moment](https://platform.openai.com/docs/guides/fine-tuning)).
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，并非所有嵌入模型都可以微调（例如，[OpenAI的](https://platform.openai.com/docs/guides/fine-tuning)
    `[text-embedding-ada-002](https://platform.openai.com/docs/guides/fine-tuning)`
    [目前不能进行微调](https://platform.openai.com/docs/guides/fine-tuning)）。
- en: Metadata
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 元数据
- en: When you store vector embeddings in a [vector database](https://medium.com/towards-data-science/explaining-vector-databases-in-3-levels-of-difficulty-fc392e48ab78),
    some vector databases let you store them together with metadata (or data that
    is not vectorized). **Annotating vector embeddings with metadata** can be helpful
    for additional post-processing of the search results, such as **metadata filtering**
    [1, 3, 8, 9]. For example, you could add metadata, such as the date, chapter,
    or subchapter reference.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 当你将向量嵌入存储在一个[向量数据库](https://medium.com/towards-data-science/explaining-vector-databases-in-3-levels-of-difficulty-fc392e48ab78)中时，一些向量数据库允许你将它们与元数据（或未向量化的数据）一起存储。**用元数据注释向量嵌入**对搜索结果的额外后处理可能是有帮助的，例如**元数据过滤**
    [1, 3, 8, 9]。例如，你可以添加元数据，如日期、章节或子章节参考。
- en: Multi-indexing
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 多重索引
- en: If the metadata is not sufficient enough to provide additional information to
    separate different types of context logically, you may want to **experiment with
    multiple indexes** [1, 9]. For example, you can use different indexes for different
    types of documents. Note that you will have to incorporate some index routing
    at retrieval time [1, 9]. If you are interested in a deeper dive into metadata
    and separate collections, you might want to learn more about the concept of [native
    multi-tenancy](https://www.youtube.com/watch?v=KT2RFMTJKGs).
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 如果元数据不足以提供额外的信息以逻辑地分隔不同类型的上下文，你可能需要**尝试多重索引** [1, 9]。例如，你可以为不同类型的文档使用不同的索引。注意，在检索时你需要进行一些索引路由
    [1, 9]。如果你对元数据和分离集合有更深入的兴趣，你可能想了解更多关于[原生多租户](https://www.youtube.com/watch?v=KT2RFMTJKGs)的概念。
- en: Indexing algorithms
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 索引算法
- en: To enable lightning-fast similarity search at scale, vector databases and vector
    indexing libraries use an Approximate Nearest Neighbor (ANN) search instead of
    a k-nearest neighbor (kNN) search. As the name suggests, ANN algorithms approximate
    the nearest neighbors and thus can be less precise than a kNN algorithm.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在大规模上实现快速相似性搜索，向量数据库和向量索引库使用近似最近邻（ANN）搜索而不是k-最近邻（kNN）搜索。顾名思义，ANN算法近似最近邻，因此可能不如kNN算法精确。
- en: There are **different ANN algorithms** you could experiment with, such as [Facebook
    Faiss](https://github.com/facebookresearch/faiss) (clustering), [Spotify Annoy](https://github.com/spotify/annoy)
    (trees), [Google ScaNN](https://github.com/google-research/google-research/tree/master/scann)
    (vector compression), and [HNSWLIB](https://github.com/nmslib/hnswlib) (proximity
    graphs). Also, many of these ANN algorithms have some parameters you could tune,
    such as `ef`, `efConstruction`, and `maxConnections` for HNSW [1].
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以尝试**不同的ANN算法**，例如[Facebook Faiss](https://github.com/facebookresearch/faiss)（聚类）、[Spotify
    Annoy](https://github.com/spotify/annoy)（树）、[Google ScaNN](https://github.com/google-research/google-research/tree/master/scann)（向量压缩）和[HNSWLIB](https://github.com/nmslib/hnswlib)（邻近图）。此外，这些ANN算法中的许多都有一些参数可以调整，例如HNSW的`ef`、`efConstruction`和`maxConnections`
    [1]。
- en: Additionally, you can enable vector compression for these indexing algorithms.
    Analogous to ANN algorithms, you will lose some precision with vector compression.
    However, depending on the choice of the vector compression algorithm and its tuning,
    you can optimize this as well.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，你可以为这些索引算法启用向量压缩。类似于ANN算法，向量压缩会导致一定的精度损失。然而，根据向量压缩算法的选择及其调整，你也可以对此进行优化。
- en: 'However, in practice, these parameters are already tuned by research teams
    of vector databases and vector indexing libraries during benchmarking experiments
    and not by developers of RAG systems. However, if you want to experiment with
    these parameters to squeeze out the last bits of performance, I recommend this
    article as a starting point:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在实践中，这些参数通常由向量数据库和向量索引库的研究团队在基准测试实验期间进行调整，而不是由RAG系统的开发人员进行调整。不过，如果你想通过调整这些参数来挤出最后的性能提升，我推荐这篇文章作为起点：
- en: '[](https://weaviate.io/blog/rag-evaluation?source=post_page-----7ca646833439--------------------------------#indexing-knobs)
    [## An Overview on RAG Evaluation | Weaviate - vector database'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://weaviate.io/blog/rag-evaluation?source=post_page-----7ca646833439--------------------------------#indexing-knobs)
    [## 关于RAG评估的概述 | Weaviate - 向量数据库'
- en: Learn about new trends in RAG evaluation and the current state of the art.
  id: totrans-65
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 了解RAG评估中的新趋势及当前的最新技术。
- en: weaviate.io](https://weaviate.io/blog/rag-evaluation?source=post_page-----7ca646833439--------------------------------#indexing-knobs)
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: weaviate.io](https://weaviate.io/blog/rag-evaluation?source=post_page-----7ca646833439--------------------------------#indexing-knobs)
- en: Inferencing Stage (Retrieval & Generation)
  id: totrans-67
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 推理阶段（检索与生成）
- en: The main components of the RAG pipeline are the retrieval and the generative
    components. This section mainly discusses strategies to improve the retrieval
    ([Query transformations](#a5e2), [retrieval parameters](#fa73), [advanced retrieval
    strategies](#a3bb), and [re-ranking models](#341d)) as this is the more impactful
    component of the two. But it also briefly touches on some strategies to improve
    the generation ([LLM](#e9f9) and [prompt engineering](#9c1c)).
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: RAG管道的主要组成部分是检索和生成组件。本节主要讨论提高检索（[查询转换](#a5e2)、[检索参数](#fa73)、[高级检索策略](#a3bb)和[重新排序模型](#341d)）的策略，因为这是两个组件中影响更大的部分。但它也简要涉及一些提高生成（[LLM](#e9f9)和[提示工程](#9c1c)）的策略。
- en: '![](../Images/e5bac153da7e619c683447cd0d1dc8b8.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e5bac153da7e619c683447cd0d1dc8b8.png)'
- en: Inference stage of a RAG pipeline
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: RAG管道的推理阶段
- en: Query transformations
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 查询转换
- en: 'Since the search query to retrieve additional context in a RAG pipeline is
    also embedded into the vector space, its phrasing can also impact the search results.
    Thus, if your search query doesn’t result in satisfactory search results, you
    can experiment with various [query transformation techniques](https://gpt-index.readthedocs.io/en/v0.6.9/how_to/query/query_transformations.html)
    [5, 8, 9], such as:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 由于在RAG管道中检索附加上下文的搜索查询也被嵌入到向量空间中，因此其措辞也会影响搜索结果。因此，如果你的搜索查询没有产生令人满意的结果，你可以尝试各种[查询转换技术](https://gpt-index.readthedocs.io/en/v0.6.9/how_to/query/query_transformations.html)
    [5, 8, 9]，例如：
- en: '**Rephrasing:** Use an LLM to rephrase the query and try again.'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**改写：** 使用LLM改写查询并重试。'
- en: '**Hypothetical Document Embeddings (HyDE):** Use an LLM to generate a hypothetical
    response to the search query and use both for retrieval.'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**假设文档嵌入（HyDE）：** 使用LLM生成对搜索查询的假设响应，并将两者用于检索。'
- en: '**Sub-queries:** Break down longer queries into multiple shorter queries.'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**子查询：** 将较长的查询拆分为多个较短的查询。'
- en: Retrieval parameters
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 检索参数
- en: The retrieval is an essential component of the RAG pipeline. The first consideration
    is whether semantic search will be sufficient for your use case or if you want
    to experiment with hybrid search.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 检索是RAG管道的一个重要组成部分。首先需要考虑的是语义搜索是否足够满足你的用例，还是你想尝试混合搜索。
- en: In the latter case, you need to experiment with weighting the aggregation of
    sparse and dense retrieval methods in hybrid search [1, 4, 9]. Thus, tuning the
    parameter `**alpha**`**, which controls the weighting between semantic (**`**alpha
    = 1**`**) and keyword-based search (**`**alpha = 0**`**)**, will become necessary.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在后一种情况下，你需要尝试对混合搜索中的稀疏和密集检索方法的加权进行实验[1, 4, 9]。因此，调整参数`**alpha**`**，即控制语义搜索（**`**alpha
    = 1**`**）和基于关键词的搜索（**`**alpha = 0**`**）之间加权的参数，将变得必要。
- en: '[](/improving-retrieval-performance-in-rag-pipelines-with-hybrid-search-c75203c2f2f5?source=post_page-----7ca646833439--------------------------------)
    [## Improving Retrieval Performance in RAG Pipelines with Hybrid Search'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/improving-retrieval-performance-in-rag-pipelines-with-hybrid-search-c75203c2f2f5?source=post_page-----7ca646833439--------------------------------)
    [## 通过混合搜索提高RAG管道中的检索性能'
- en: How to find more relevant search results by combining traditional keyword-based
    search with modern vector search
  id: totrans-80
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 如何通过将传统的基于关键词的搜索与现代向量搜索相结合来找到更相关的搜索结果
- en: towardsdatascience.com](/improving-retrieval-performance-in-rag-pipelines-with-hybrid-search-c75203c2f2f5?source=post_page-----7ca646833439--------------------------------)
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://towardsdatascience.com/improving-retrieval-performance-in-rag-pipelines-with-hybrid-search-c75203c2f2f5?source=post_page-----7ca646833439--------------------------------](https://towardsdatascience.com/improving-retrieval-performance-in-rag-pipelines-with-hybrid-search-c75203c2f2f5?source=post_page-----7ca646833439--------------------------------)'
- en: Also, the **number of search results to retrieve** will play an essential role.
    The number of retrieved contexts will impact the length of the used context window
    (see [Prompt Engineering](#9c1c)). Also, if you are using a re-ranking model,
    you need to consider how many contexts to input to the model (see [Re-ranking
    models](#341d)).
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，**检索的搜索结果数量**将发挥重要作用。检索的上下文数量将影响所用上下文窗口的长度（见 [Prompt Engineering](#9c1c)）。此外，如果你使用的是重排序模型，你需要考虑输入模型的上下文数量（见
    [Re-ranking models](#341d)）。
- en: Note, while the used similarity measure for semantic search is a parameter you
    can change, you should not experiment with it but instead set it according to
    the used embedding model (e.g., `[text-embedding-ada-002](https://platform.openai.com/docs/guides/embeddings/what-are-embeddings)`
    supports cosine similarity or `[multi-qa-MiniLM-l6-cos-v1](https://huggingface.co/sentence-transformers/multi-qa-MiniLM-L6-cos-v1#technical-details)`
    supports cosine similarity, dot product, and Euclidean distance).
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，虽然用于语义搜索的相似度度量是一个可以更改的参数，你不应随意实验，而是应根据所用的嵌入模型设置（例如，`[text-embedding-ada-002](https://platform.openai.com/docs/guides/embeddings/what-are-embeddings)`
    支持余弦相似度或 `[multi-qa-MiniLM-l6-cos-v1](https://huggingface.co/sentence-transformers/multi-qa-MiniLM-L6-cos-v1#technical-details)`
    支持余弦相似度、点积和欧几里得距离）。
- en: Advanced retrieval strategies
  id: totrans-84
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 高级检索策略
- en: 'This section could technically be its own article. For this overview, we will
    keep this as concise as possible. For an in-depth explanation of the following
    techniques, I recommend this DeepLearning.AI course:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 本节技术上可以作为一篇独立的文章。为了本概述，我们将尽量简洁。有关以下技术的详细说明，我推荐这个 DeepLearning.AI 课程：
- en: '[](https://www.deeplearning.ai/short-courses/building-evaluating-advanced-rag/?source=post_page-----7ca646833439--------------------------------)
    [## Building and Evaluating Advanced RAG Applications'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://www.deeplearning.ai/short-courses/building-evaluating-advanced-rag/?source=post_page-----7ca646833439--------------------------------](https://www.deeplearning.ai/short-courses/building-evaluating-advanced-rag/?source=post_page-----7ca646833439--------------------------------)
    [## 构建和评估高级 RAG 应用'
- en: Learn methods like sentence-window retrieval and auto-merging retrieval, improving
    your RAG pipeline’s performance…
  id: totrans-87
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 学习句子窗口检索和自动合并检索等方法，提高你的 RAG 流水线的性能……
- en: www.deeplearning.ai](https://www.deeplearning.ai/short-courses/building-evaluating-advanced-rag/?source=post_page-----7ca646833439--------------------------------)
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://www.deeplearning.ai/short-courses/building-evaluating-advanced-rag/?source=post_page-----7ca646833439--------------------------------](https://www.deeplearning.ai/short-courses/building-evaluating-advanced-rag/?source=post_page-----7ca646833439--------------------------------)'
- en: The underlying idea of this section is that the chunks for retrieval shouldn’t
    necessarily be the same chunks used for the generation. Ideally, you would embed
    smaller chunks for retrieval (see [Chunking](#e45f)) but retrieve bigger contexts.
    [7]
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 本节的基本思想是检索的块不一定要与生成所用的块相同。理想情况下，你会为检索嵌入较小的块（见 [Chunking](#e45f)），但检索更大的上下文。[7]
- en: '**Sentence-window retrieval:** Do not just retrieve the relevant sentence,
    but the window of appropriate sentences before and after the retrieved one.'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**句子窗口检索：** 不仅检索相关句子，还要检索在检索句子之前和之后的适当句子。'
- en: '**Auto-merging retrieval:** The documents are organized in a tree-like structure.
    At query time, separate but related, smaller chunks can be consolidated into a
    larger context.'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**自动合并检索：** 文档以树状结构组织。在查询时，可以将分开但相关的小块合并成一个更大的上下文。'
- en: Re-ranking models
  id: totrans-92
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 重排序模型
- en: While semantic search retrieves context based on its semantic similarity to
    the search query, “most similar” doesn’t necessarily mean “most relevant”. **Re-ranking
    models**, such as [Cohere’s Rerank](https://cohere.com/rerank?ref=txt.cohere.com&__hstc=14363112.8fc20f6b1a1ad8c0f80dcfed3741d271.1697800567394.1701091033915.1701173515537.7&__hssc=14363112.1.1701173515537&__hsfp=3638092843)
    model, can help eliminate irrelevant search results by computing a score for the
    relevance of the query for each retrieved context [1, 9].
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然语义搜索根据与搜索查询的语义相似性检索上下文，但“最相似”并不一定意味着“最相关”。**重排序模型**，如 [Cohere’s Rerank](https://cohere.com/rerank?ref=txt.cohere.com&__hstc=14363112.8fc20f6b1a1ad8c0f80dcfed3741d271.1697800567394.1701091033915.1701173515537.7&__hssc=14363112.1.1701173515537&__hsfp=3638092843)
    模型，可以通过计算每个检索上下文对查询的相关性分数来帮助消除不相关的搜索结果 [1, 9]。
- en: “most similar” doesn’t necessarily mean “most relevant”
  id: totrans-94
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “最相似”并不一定意味着“最相关”
- en: If you are using a re-ranker model, you may need to re-tune the **number of
    search results** for the input of the re-ranker and how many of the reranked results
    you want to feed into the LLM.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你使用的是重排序模型，你可能需要重新调整**搜索结果数量**以供重排序模型输入，并决定你希望将多少个重排序的结果输入到LLM中。
- en: As with the [embedding models](#156e), you may want to experiment with **fine-tuning
    the re-ranker** to your specific use case.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 与[嵌入模型](#156e)一样，你可能还想尝试**对重排序模型进行微调**以适应你的特定用例。
- en: LLMs
  id: totrans-97
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: LLMs
- en: The **LLM is the core component** for generating the response. Similarly to
    the embedding models, there is a wide range of LLMs you can choose from depending
    on your requirements, such as open vs. proprietary models, inferencing costs,
    context length, etc. [1]
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '**LLM是核心组件**，用于生成响应。类似于嵌入模型，根据你的要求（如开放 vs. 专有模型、推理成本、上下文长度等），你可以选择不同的LLM。[1]'
- en: As with the [embedding models](#156e) or [re-ranking models](#341d), you may
    want to experiment with **fine-tuning the LLM** to your specific use case to incorporate
    specific wording or tone of voice.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 与[嵌入模型](#156e)或[重排序模型](#341d)一样，你可能想要尝试**对LLM进行微调**以适应你的特定用例，以融入特定的措辞或语气。
- en: Prompt engineering
  id: totrans-100
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 提示工程
- en: How you phrase or [**engineer your prompt**](/how-to-write-expert-prompts-for-chatgpt-gpt-4-and-other-language-models-23133dc85550)
    will significantly impact the LLM’s completion [1, 8, 9].
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 你如何表述或[**工程化你的提示**](/how-to-write-expert-prompts-for-chatgpt-gpt-4-and-other-language-models-23133dc85550)将显著影响LLM的完成质量[1,
    8, 9]。
- en: '[PRE0]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '[PRE1]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Additionally, using **few-shot examples** in your prompt can improve the quality
    of the completions.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，在提示中使用**少量示例**可以提高完成的质量。
- en: As mentioned in [Retrieval parameters](#fa73), the **number of contexts** fed
    into the prompt is a parameter you should experiment with [1]. While the performance
    of your RAG pipeline can improve with increasing relevant context, you can also
    run into a “Lost in the Middle” [6] effect where relevant context is not recognized
    as such by the LLM if it is placed in the middle of many contexts.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 如[检索参数](#fa73)中提到的，**输入提示的上下文数量**是你应该尝试的一个参数[1]。虽然随着相关上下文的增加，你的RAG管道性能可能会提高，但你也可能会遇到“在中间迷失”[6]效应，即如果相关上下文被放置在许多上下文的中间，LLM可能不会将其识别为相关。
- en: Summary
  id: totrans-106
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: 'As more and more developers gain experience with prototyping RAG pipelines,
    it becomes more important to discuss strategies to bring RAG pipelines to production-ready
    performances. This article discussed different “hyperparameters” and other knobs
    you can tune in a RAG pipeline according to the relevant stages:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 随着越来越多的开发者获得原型开发RAG管道的经验，讨论将RAG管道带到生产就绪性能的策略变得越来越重要。本文讨论了不同的“超参数”和在RAG管道的相关阶段中可以调整的其他参数：
- en: 'This article covered the following strategies in the [ingestion stage](#4142):'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 本文涵盖了[摄取阶段](#4142)中的以下策略：
- en: '[Data cleaning](#196c): Ensure data is clean and correct.'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[数据清理](#196c)：确保数据是干净和正确的。'
- en: '[Chunking](#e45f): Choice of chunking technique, chunk size (`chunk_size`)
    and chunk overlap (`overlap`).'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[分块](#e45f)：选择分块技术、分块大小（`chunk_size`）和分块重叠（`overlap`）。'
- en: '[Embedding models](#156e): Choice of the embedding model, incl. dimensionality,
    and whether to fine-tune it.'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[嵌入模型](#156e)：选择嵌入模型，包括维度，以及是否进行微调。'
- en: '[Metadata](#2b47): Whether to use metadata and choice of metadata.'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[元数据](#2b47)：是否使用元数据及其选择。'
- en: '[Multi-indexing](#ce6c): Decide whether to use multiple indexes for different
    data collections.'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[多索引](#ce6c)：决定是否对不同的数据集合使用多个索引。'
- en: '[Indexing algorithms](#4daa): Choice and tuning of ANN and vector compression
    algorithms can be tuned but are usually not tuned by practitioners.'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[索引算法](#4daa)：选择和调整ANN和向量压缩算法，通常不由从业者进行调整。'
- en: 'And the following strategies in the [inferencing stage (retrieval and generation)](#ac53):'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 以及在[推理阶段（检索和生成）](#ac53)中的以下策略：
- en: '[Query transformations](#a5e2): Experiment with rephrasing, HyDE, or sub-queries.'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[查询转换](#a5e2)：尝试重新表述、HyDE或子查询。'
- en: '[Retrieval parameters](#fa73): Choice of search technique (`alpha` if you have
    hybrid search enabled) and the number of retrieved search results.'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[检索参数](#fa73)：选择搜索技术（如果启用了混合搜索，则为`alpha`）和检索结果的数量。'
- en: '[Advanced retrieval strategies](#a3bb): Whether to use advanced retrieval strategies,
    such as sentence-window or auto-merging retrieval.'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[高级检索策略](#a3bb)：是否使用高级检索策略，如句子窗口或自动合并检索。'
- en: '[Re-ranking models](#341d): Whether to use a re-ranking model, choice of re-ranking
    model, number of search results to input into the re-ranking model, and whether
    to fine-tune the re-ranking model.'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Re-ranking models](#341d)：是否使用重新排序模型、选择重新排序模型、输入到重新排序模型中的搜索结果数量以及是否对重新排序模型进行微调。'
- en: '[LLMs](#e9f9): Choice of LLM and whether to fine-tune it.'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[LLMs](#e9f9)：选择LLM和是否对其进行微调。'
- en: '[Prompt engineering](#9c1c): Experiment with different phrasing and few-shot
    examples.'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Prompt engineering](#9c1c)：尝试不同的措辞和少量示例。'
- en: Enjoyed This Story?
  id: totrans-122
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 享受了这个故事吗？
- en: '[*Subscribe for free*](https://medium.com/subscribe/@iamleonie) *to get notified
    when I publish a new story.*'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '[*免费订阅*](https://medium.com/subscribe/@iamleonie) *以获取我发布新故事时的通知。*'
- en: '[](https://medium.com/@iamleonie/subscribe?source=post_page-----7ca646833439--------------------------------)
    [## Get an email whenever Leonie Monigatti publishes.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@iamleonie/subscribe?source=post_page-----7ca646833439--------------------------------)
    [## 每当Leonie Monigatti发布新内容时获取电子邮件通知。'
- en: Get an email whenever Leonie Monigatti publishes. By signing up, you will create
    a Medium account if you don’t already…
  id: totrans-125
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 每当Leonie Monigatti发布新内容时，获取电子邮件通知。注册后，如果你还没有Medium账户，将会创建一个…
- en: medium.com](https://medium.com/@iamleonie/subscribe?source=post_page-----7ca646833439--------------------------------)
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: medium.com](https://medium.com/@iamleonie/subscribe?source=post_page-----7ca646833439--------------------------------)
- en: '*Find me on* [*LinkedIn*](https://www.linkedin.com/in/804250ab/),[*Twitter*](https://twitter.com/helloiamleonie)*,
    and* [*Kaggle*](https://www.kaggle.com/iamleonie)*!*'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '*在* [*LinkedIn*](https://www.linkedin.com/in/804250ab/)，[*Twitter*](https://twitter.com/helloiamleonie)*，以及*
    [*Kaggle*](https://www.kaggle.com/iamleonie)*上找到我*！'
- en: References
  id: totrans-128
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: Literature
  id: totrans-129
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 文献
- en: '[1] [Connor Shorten](https://medium.com/u/59216259c525?source=post_page-----7ca646833439--------------------------------)
    and [Erika Cardenas](https://medium.com/u/91b27bdf28df?source=post_page-----7ca646833439--------------------------------)
    (2023). Weaviate Blog. [An Overview on RAG Evaluation](https://weaviate.io/blog/rag-evaluation)
    (accessed Nov. 27, 2023)'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] [Connor Shorten](https://medium.com/u/59216259c525?source=post_page-----7ca646833439--------------------------------)
    和 [Erika Cardenas](https://medium.com/u/91b27bdf28df?source=post_page-----7ca646833439--------------------------------)（2023）。Weaviate
    博客。[RAG评估概述](https://weaviate.io/blog/rag-evaluation)（访问日期：2023年11月27日）'
- en: '[2] [Jerry Liu](https://medium.com/u/e76da1c45ef7?source=post_page-----7ca646833439--------------------------------)
    (2023). LlamaIndex Blog. [Fine-Tuning Embeddings for RAG with Synthetic Data](https://blog.llamaindex.ai/fine-tuning-embeddings-for-rag-with-synthetic-data-e534409a3971)
    (accessed Nov. 28, 2023)'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] [Jerry Liu](https://medium.com/u/e76da1c45ef7?source=post_page-----7ca646833439--------------------------------)（2023）。LlamaIndex
    博客。[使用合成数据对RAG进行嵌入微调](https://blog.llamaindex.ai/fine-tuning-embeddings-for-rag-with-synthetic-data-e534409a3971)（访问日期：2023年11月28日）'
- en: '[3] LlamaIndex Documentation (2023). [Building Performant RAG Applications
    for Production](https://gpt-index.readthedocs.io/en/stable/optimizing/production_rag.html)
    (accessed Nov. 28, 2023)'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] LlamaIndex 文档（2023）。[为生产构建高性能RAG应用程序](https://gpt-index.readthedocs.io/en/stable/optimizing/production_rag.html)（访问日期：2023年11月28日）'
- en: '[4] Voyage AI (2023). [Embeddings Drive the Quality of RAG: A Case Study of
    Chat.LangChain](https://blog.voyageai.com/2023/10/29/a-case-study-of-chat-langchain/)
    (accessed Dec. 5, 2023)'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] Voyage AI（2023）。[嵌入推动RAG的质量：Chat.LangChain的案例研究](https://blog.voyageai.com/2023/10/29/a-case-study-of-chat-langchain/)（访问日期：2023年12月5日）'
- en: '[5] LlamaIndex Documentation (2023). [Query Transformations](https://gpt-index.readthedocs.io/en/v0.6.9/how_to/query/query_transformations.html)
    (accessed Nov. 28, 2023)'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] LlamaIndex 文档（2023）。[查询转换](https://gpt-index.readthedocs.io/en/v0.6.9/how_to/query/query_transformations.html)（访问日期：2023年11月28日）'
- en: '[6] Liu, N. F., Lin, K., Hewitt, J., Paranjape, A., Bevilacqua, M., Petroni,
    F., & Liang, P. (2023). Lost in the middle: How language models use long contexts.
    *arXiv preprint arXiv:2307.03172*.'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '[6] Liu, N. F., Lin, K., Hewitt, J., Paranjape, A., Bevilacqua, M., Petroni,
    F., & Liang, P.（2023）。《迷失在中间：语言模型如何使用长上下文》。*arXiv 预印本 arXiv:2307.03172*。'
- en: '[7] DeepLearning.AI (2023). [Building and Evaluating Advanced RAG Applications](https://www.deeplearning.ai/short-courses/building-evaluating-advanced-rag/)
    (accessed Dec 4, 2023)'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '[7] DeepLearning.AI（2023）。[构建和评估高级RAG应用程序](https://www.deeplearning.ai/short-courses/building-evaluating-advanced-rag/)（访问日期：2023年12月4日）'
- en: '[8] [Ahmed Besbes](https://medium.com/u/adc8ea174c69?source=post_page-----7ca646833439--------------------------------)
    (2023). Towards Data Science. [Why Your RAG Is Not Reliable in a Production Environment](/why-your-rag-is-not-reliable-in-a-production-environment-9e6a73b3eddb)
    (accessed Nov. 27, 2023)'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '[8] [Ahmed Besbes](https://medium.com/u/adc8ea174c69?source=post_page-----7ca646833439--------------------------------)（2023）。Towards
    Data Science。[为什么你的RAG在生产环境中不可靠](/why-your-rag-is-not-reliable-in-a-production-environment-9e6a73b3eddb)（访问日期：2023年11月27日）'
- en: '[9] [Matt Ambrogi](https://medium.com/u/1e23ad8f92c5?source=post_page-----7ca646833439--------------------------------)
    (2023). Towards Data Science. [10 Ways to Improve the Performance of Retrieval
    Augmented Generation Systems](/10-ways-to-improve-the-performance-of-retrieval-augmented-generation-systems-5fa2cee7cd5c)
    (accessed Nov. 27, 2023)'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '[9] [Matt Ambrogi](https://medium.com/u/1e23ad8f92c5?source=post_page-----7ca646833439--------------------------------)（2023年）。面向数据科学。[提高检索增强生成系统性能的10种方法](/10-ways-to-improve-the-performance-of-retrieval-augmented-generation-systems-5fa2cee7cd5c)（访问日期：2023年11月27日）'
- en: Images
  id: totrans-139
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 图片
- en: If not otherwise stated, all images are created by the author.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 除非另有说明，所有图片均由作者创作。
