- en: A Guide on 12 Tuning Strategies for Production-Ready RAG Applications
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/a-guide-on-12-tuning-strategies-for-production-ready-rag-applications-7ca646833439?source=collection_archive---------0-----------------------#2023-12-06](https://towardsdatascience.com/a-guide-on-12-tuning-strategies-for-production-ready-rag-applications-7ca646833439?source=collection_archive---------0-----------------------#2023-12-06)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: How to improve the performance of your Retrieval-Augmented Generation (RAG)
    pipeline with these “hyperparameters” and tuning strategies
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@iamleonie?source=post_page-----7ca646833439--------------------------------)[![Leonie
    Monigatti](../Images/4044b1685ada53a30160b03dc78f9626.png)](https://medium.com/@iamleonie?source=post_page-----7ca646833439--------------------------------)[](https://towardsdatascience.com/?source=post_page-----7ca646833439--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----7ca646833439--------------------------------)
    [Leonie Monigatti](https://medium.com/@iamleonie?source=post_page-----7ca646833439--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F3a38da70d8dc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-guide-on-12-tuning-strategies-for-production-ready-rag-applications-7ca646833439&user=Leonie+Monigatti&userId=3a38da70d8dc&source=post_page-3a38da70d8dc----7ca646833439---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----7ca646833439--------------------------------)
    ·10 min read·Dec 6, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F7ca646833439&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-guide-on-12-tuning-strategies-for-production-ready-rag-applications-7ca646833439&user=Leonie+Monigatti&userId=3a38da70d8dc&source=-----7ca646833439---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F7ca646833439&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-guide-on-12-tuning-strategies-for-production-ready-rag-applications-7ca646833439&source=-----7ca646833439---------------------bookmark_footer-----------)![](../Images/b8c76b67a513278e3f87e66e0d22bb7a.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Tuning Strategies for Retrieval-Augmented Generation Applications
  prefs: []
  type: TYPE_NORMAL
- en: Data Science is an experimental science. It starts with the “No Free Lunch Theorem,”
    which states that there is no one-size-fits-all algorithm that works best for
    every problem. And it results in data scientists using [experiment tracking systems](https://medium.com/@iamleonie/intro-to-mlops-experiment-tracking-for-machine-learning-858e432bd133)
    to help them [tune the hyperparameters of their Machine Learning (ML) projects
    to achieve the best performance](https://medium.com/towards-data-science/intermediate-deep-learning-with-transfer-learning-f1aba5a814f).
  prefs: []
  type: TYPE_NORMAL
- en: This article looks at a [Retrieval-Augmented Generation (RAG) pipeline](https://medium.com/towards-data-science/retrieval-augmented-generation-rag-from-theory-to-langchain-implementation-4e9bd5f6a4f2)
    through the eyes of a data scientist. It discusses potential “hyperparameters”
    you can experiment with to improve your RAG pipeline’s performance. Similar to
    experimentation in Deep Learning, where, e.g., data augmentation techniques are
    not a hyperparameter but a knob you can tune and experiment with, this article
    will also cover different strategies you can apply, which are not per se hyperparameters.
  prefs: []
  type: TYPE_NORMAL
- en: '[](/retrieval-augmented-generation-rag-from-theory-to-langchain-implementation-4e9bd5f6a4f2?source=post_page-----7ca646833439--------------------------------)
    [## Retrieval-Augmented Generation (RAG): From Theory to LangChain Implementation'
  prefs: []
  type: TYPE_NORMAL
- en: From the theory of the original academic paper to its Python implementation
    with OpenAI, Weaviate, and LangChain
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/retrieval-augmented-generation-rag-from-theory-to-langchain-implementation-4e9bd5f6a4f2?source=post_page-----7ca646833439--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: 'This article covers the following “hyperparameters” sorted by their relevant
    stage. In the [ingestion stage](#4142) of a RAG pipeline, you can achieve performance
    improvements by:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Data cleaning](#196c)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Chunking](#e45f)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Embedding models](#156e)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Metadata](#2b47)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Multi-indexing](#ce6c)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Indexing algorithms](#4daa)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'And in the [inferencing stage (retrieval and generation)](#ac53), you can tune:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Query transformations](#a5e2)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Retrieval parameters](#fa73)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Advanced retrieval strategies](#a3bb)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Re-ranking models](#341d)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[LLMs](#e9f9)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Prompt engineering](#9c1c)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note that this article covers text-use cases of RAG. For multimodal RAG applications,
    different considerations may apply.
  prefs: []
  type: TYPE_NORMAL
- en: Ingestion Stage
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The ingestion stage is a preparation step for building a RAG pipeline, similar
    to the data cleaning and preprocessing steps in an ML pipeline. Usually, the ingestion
    stage consists of the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Collect data
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Chunk data
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Generate vector embeddings of chunks
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Store vector embeddings and chunks in a vector database
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](../Images/c4110b24f69ef4f71f5f3647a41220dc.png)'
  prefs: []
  type: TYPE_IMG
- en: Ingestion stage of a RAG pipeline
  prefs: []
  type: TYPE_NORMAL
- en: This section discusses impactful techniques and hyperparameters that you can
    apply and tune to improve the relevance of the retrieved contexts in the inferencing
    stage.
  prefs: []
  type: TYPE_NORMAL
- en: Data cleaning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Like any Data Science pipeline, the quality of your data heavily impacts the
    outcome in your RAG pipeline [8, 9]. Before moving on to any of the following
    steps, ensure that your data meets the following criteria:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Clean**: Apply at least some basic data cleaning techniques commonly used
    in Natural Language Processing, such as making sure all special characters are
    encoded correctly.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Correct**: Make sure your information is consistent and factually accurate
    to avoid conflicting information confusing your LLM.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chunking
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Chunking your documents is an essential preparation step for your external knowledge
    source in a RAG pipeline that can impact the performance [1, 8, 9]. It is a technique
    to generate logically coherent snippets of information, usually by breaking up
    long documents into smaller sections (but it can also combine smaller snippets
    into coherent paragraphs).
  prefs: []
  type: TYPE_NORMAL
- en: One consideration you need to make is the **choice of the chunking technique**.
    For example, in [LangChain, different text splitters](https://python.langchain.com/docs/modules/data_connection/document_transformers/)
    split up documents by different logics, such as by characters, tokens, etc. This
    depends on the type of data you have. For example, you will need to use different
    chunking techniques if your input data is code vs. if it is a Markdown file.
  prefs: []
  type: TYPE_NORMAL
- en: 'The ideal **length of your chunk (**`**chunk_size**`**)** depends on your use
    case: If your use case is question answering, you may need shorter specific chunks,
    but if your use case is summarization, you may need longer chunks. Additionally,
    if a chunk is too short, it might not contain enough context. On the other hand,
    if a chunk is too long, it might contain too much irrelevant information.'
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, you will need to think about a **“rolling window” between chunks
    (**`**overlap**`**)** to introduce some additional context.
  prefs: []
  type: TYPE_NORMAL
- en: Embedding models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Embedding models are at the core of your retrieval. The **quality of your embeddings**
    heavily impacts your retrieval results [1, 4]. Usually, the higher the dimensionality
    of the generated embeddings, the higher the precision of your embeddings.
  prefs: []
  type: TYPE_NORMAL
- en: For an idea of what alternative embedding models are available, you can look
    at the [Massive Text Embedding Benchmark (MTEB) Leaderboard](https://huggingface.co/spaces/mteb/leaderboard),
    which covers 164 text embedding models (at the time of this writing).
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://huggingface.co/spaces/mteb/leaderboard?source=post_page-----7ca646833439--------------------------------)
    [## MTEB Leaderboard - a Hugging Face Space by mteb'
  prefs: []
  type: TYPE_NORMAL
- en: Discover amazing ML apps made by the community
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: huggingface.co](https://huggingface.co/spaces/mteb/leaderboard?source=post_page-----7ca646833439--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: While you can use general-purpose embedding models out-of-the-box, it may make
    sense to **fine-tune your embedding model** to your specific use case in some
    cases to avoid out-of-domain issues later on [9]. According to experiments conducted
    by LlamaIndex, fine-tuning your embedding model can lead to a [5–10% performance
    increase in retrieval evaluation metrics](https://github.com/run-llama/finetune-embedding/blob/main/evaluate.ipynb)
    [2].
  prefs: []
  type: TYPE_NORMAL
- en: Note that you cannot fine-tune all embedding models (e.g., [OpenAI's](https://platform.openai.com/docs/guides/fine-tuning)
    `[text-ebmedding-ada-002](https://platform.openai.com/docs/guides/fine-tuning)`
    [can’t be fine-tuned at the moment](https://platform.openai.com/docs/guides/fine-tuning)).
  prefs: []
  type: TYPE_NORMAL
- en: Metadata
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When you store vector embeddings in a [vector database](https://medium.com/towards-data-science/explaining-vector-databases-in-3-levels-of-difficulty-fc392e48ab78),
    some vector databases let you store them together with metadata (or data that
    is not vectorized). **Annotating vector embeddings with metadata** can be helpful
    for additional post-processing of the search results, such as **metadata filtering**
    [1, 3, 8, 9]. For example, you could add metadata, such as the date, chapter,
    or subchapter reference.
  prefs: []
  type: TYPE_NORMAL
- en: Multi-indexing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If the metadata is not sufficient enough to provide additional information to
    separate different types of context logically, you may want to **experiment with
    multiple indexes** [1, 9]. For example, you can use different indexes for different
    types of documents. Note that you will have to incorporate some index routing
    at retrieval time [1, 9]. If you are interested in a deeper dive into metadata
    and separate collections, you might want to learn more about the concept of [native
    multi-tenancy](https://www.youtube.com/watch?v=KT2RFMTJKGs).
  prefs: []
  type: TYPE_NORMAL
- en: Indexing algorithms
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To enable lightning-fast similarity search at scale, vector databases and vector
    indexing libraries use an Approximate Nearest Neighbor (ANN) search instead of
    a k-nearest neighbor (kNN) search. As the name suggests, ANN algorithms approximate
    the nearest neighbors and thus can be less precise than a kNN algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: There are **different ANN algorithms** you could experiment with, such as [Facebook
    Faiss](https://github.com/facebookresearch/faiss) (clustering), [Spotify Annoy](https://github.com/spotify/annoy)
    (trees), [Google ScaNN](https://github.com/google-research/google-research/tree/master/scann)
    (vector compression), and [HNSWLIB](https://github.com/nmslib/hnswlib) (proximity
    graphs). Also, many of these ANN algorithms have some parameters you could tune,
    such as `ef`, `efConstruction`, and `maxConnections` for HNSW [1].
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, you can enable vector compression for these indexing algorithms.
    Analogous to ANN algorithms, you will lose some precision with vector compression.
    However, depending on the choice of the vector compression algorithm and its tuning,
    you can optimize this as well.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, in practice, these parameters are already tuned by research teams
    of vector databases and vector indexing libraries during benchmarking experiments
    and not by developers of RAG systems. However, if you want to experiment with
    these parameters to squeeze out the last bits of performance, I recommend this
    article as a starting point:'
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://weaviate.io/blog/rag-evaluation?source=post_page-----7ca646833439--------------------------------#indexing-knobs)
    [## An Overview on RAG Evaluation | Weaviate - vector database'
  prefs: []
  type: TYPE_NORMAL
- en: Learn about new trends in RAG evaluation and the current state of the art.
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: weaviate.io](https://weaviate.io/blog/rag-evaluation?source=post_page-----7ca646833439--------------------------------#indexing-knobs)
  prefs: []
  type: TYPE_NORMAL
- en: Inferencing Stage (Retrieval & Generation)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The main components of the RAG pipeline are the retrieval and the generative
    components. This section mainly discusses strategies to improve the retrieval
    ([Query transformations](#a5e2), [retrieval parameters](#fa73), [advanced retrieval
    strategies](#a3bb), and [re-ranking models](#341d)) as this is the more impactful
    component of the two. But it also briefly touches on some strategies to improve
    the generation ([LLM](#e9f9) and [prompt engineering](#9c1c)).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e5bac153da7e619c683447cd0d1dc8b8.png)'
  prefs: []
  type: TYPE_IMG
- en: Inference stage of a RAG pipeline
  prefs: []
  type: TYPE_NORMAL
- en: Query transformations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Since the search query to retrieve additional context in a RAG pipeline is
    also embedded into the vector space, its phrasing can also impact the search results.
    Thus, if your search query doesn’t result in satisfactory search results, you
    can experiment with various [query transformation techniques](https://gpt-index.readthedocs.io/en/v0.6.9/how_to/query/query_transformations.html)
    [5, 8, 9], such as:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Rephrasing:** Use an LLM to rephrase the query and try again.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Hypothetical Document Embeddings (HyDE):** Use an LLM to generate a hypothetical
    response to the search query and use both for retrieval.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Sub-queries:** Break down longer queries into multiple shorter queries.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Retrieval parameters
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The retrieval is an essential component of the RAG pipeline. The first consideration
    is whether semantic search will be sufficient for your use case or if you want
    to experiment with hybrid search.
  prefs: []
  type: TYPE_NORMAL
- en: In the latter case, you need to experiment with weighting the aggregation of
    sparse and dense retrieval methods in hybrid search [1, 4, 9]. Thus, tuning the
    parameter `**alpha**`**, which controls the weighting between semantic (**`**alpha
    = 1**`**) and keyword-based search (**`**alpha = 0**`**)**, will become necessary.
  prefs: []
  type: TYPE_NORMAL
- en: '[](/improving-retrieval-performance-in-rag-pipelines-with-hybrid-search-c75203c2f2f5?source=post_page-----7ca646833439--------------------------------)
    [## Improving Retrieval Performance in RAG Pipelines with Hybrid Search'
  prefs: []
  type: TYPE_NORMAL
- en: How to find more relevant search results by combining traditional keyword-based
    search with modern vector search
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/improving-retrieval-performance-in-rag-pipelines-with-hybrid-search-c75203c2f2f5?source=post_page-----7ca646833439--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: Also, the **number of search results to retrieve** will play an essential role.
    The number of retrieved contexts will impact the length of the used context window
    (see [Prompt Engineering](#9c1c)). Also, if you are using a re-ranking model,
    you need to consider how many contexts to input to the model (see [Re-ranking
    models](#341d)).
  prefs: []
  type: TYPE_NORMAL
- en: Note, while the used similarity measure for semantic search is a parameter you
    can change, you should not experiment with it but instead set it according to
    the used embedding model (e.g., `[text-embedding-ada-002](https://platform.openai.com/docs/guides/embeddings/what-are-embeddings)`
    supports cosine similarity or `[multi-qa-MiniLM-l6-cos-v1](https://huggingface.co/sentence-transformers/multi-qa-MiniLM-L6-cos-v1#technical-details)`
    supports cosine similarity, dot product, and Euclidean distance).
  prefs: []
  type: TYPE_NORMAL
- en: Advanced retrieval strategies
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This section could technically be its own article. For this overview, we will
    keep this as concise as possible. For an in-depth explanation of the following
    techniques, I recommend this DeepLearning.AI course:'
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://www.deeplearning.ai/short-courses/building-evaluating-advanced-rag/?source=post_page-----7ca646833439--------------------------------)
    [## Building and Evaluating Advanced RAG Applications'
  prefs: []
  type: TYPE_NORMAL
- en: Learn methods like sentence-window retrieval and auto-merging retrieval, improving
    your RAG pipeline’s performance…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: www.deeplearning.ai](https://www.deeplearning.ai/short-courses/building-evaluating-advanced-rag/?source=post_page-----7ca646833439--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: The underlying idea of this section is that the chunks for retrieval shouldn’t
    necessarily be the same chunks used for the generation. Ideally, you would embed
    smaller chunks for retrieval (see [Chunking](#e45f)) but retrieve bigger contexts.
    [7]
  prefs: []
  type: TYPE_NORMAL
- en: '**Sentence-window retrieval:** Do not just retrieve the relevant sentence,
    but the window of appropriate sentences before and after the retrieved one.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Auto-merging retrieval:** The documents are organized in a tree-like structure.
    At query time, separate but related, smaller chunks can be consolidated into a
    larger context.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Re-ranking models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: While semantic search retrieves context based on its semantic similarity to
    the search query, “most similar” doesn’t necessarily mean “most relevant”. **Re-ranking
    models**, such as [Cohere’s Rerank](https://cohere.com/rerank?ref=txt.cohere.com&__hstc=14363112.8fc20f6b1a1ad8c0f80dcfed3741d271.1697800567394.1701091033915.1701173515537.7&__hssc=14363112.1.1701173515537&__hsfp=3638092843)
    model, can help eliminate irrelevant search results by computing a score for the
    relevance of the query for each retrieved context [1, 9].
  prefs: []
  type: TYPE_NORMAL
- en: “most similar” doesn’t necessarily mean “most relevant”
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: If you are using a re-ranker model, you may need to re-tune the **number of
    search results** for the input of the re-ranker and how many of the reranked results
    you want to feed into the LLM.
  prefs: []
  type: TYPE_NORMAL
- en: As with the [embedding models](#156e), you may want to experiment with **fine-tuning
    the re-ranker** to your specific use case.
  prefs: []
  type: TYPE_NORMAL
- en: LLMs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The **LLM is the core component** for generating the response. Similarly to
    the embedding models, there is a wide range of LLMs you can choose from depending
    on your requirements, such as open vs. proprietary models, inferencing costs,
    context length, etc. [1]
  prefs: []
  type: TYPE_NORMAL
- en: As with the [embedding models](#156e) or [re-ranking models](#341d), you may
    want to experiment with **fine-tuning the LLM** to your specific use case to incorporate
    specific wording or tone of voice.
  prefs: []
  type: TYPE_NORMAL
- en: Prompt engineering
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: How you phrase or [**engineer your prompt**](/how-to-write-expert-prompts-for-chatgpt-gpt-4-and-other-language-models-23133dc85550)
    will significantly impact the LLM’s completion [1, 8, 9].
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Additionally, using **few-shot examples** in your prompt can improve the quality
    of the completions.
  prefs: []
  type: TYPE_NORMAL
- en: As mentioned in [Retrieval parameters](#fa73), the **number of contexts** fed
    into the prompt is a parameter you should experiment with [1]. While the performance
    of your RAG pipeline can improve with increasing relevant context, you can also
    run into a “Lost in the Middle” [6] effect where relevant context is not recognized
    as such by the LLM if it is placed in the middle of many contexts.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As more and more developers gain experience with prototyping RAG pipelines,
    it becomes more important to discuss strategies to bring RAG pipelines to production-ready
    performances. This article discussed different “hyperparameters” and other knobs
    you can tune in a RAG pipeline according to the relevant stages:'
  prefs: []
  type: TYPE_NORMAL
- en: 'This article covered the following strategies in the [ingestion stage](#4142):'
  prefs: []
  type: TYPE_NORMAL
- en: '[Data cleaning](#196c): Ensure data is clean and correct.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Chunking](#e45f): Choice of chunking technique, chunk size (`chunk_size`)
    and chunk overlap (`overlap`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Embedding models](#156e): Choice of the embedding model, incl. dimensionality,
    and whether to fine-tune it.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Metadata](#2b47): Whether to use metadata and choice of metadata.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Multi-indexing](#ce6c): Decide whether to use multiple indexes for different
    data collections.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Indexing algorithms](#4daa): Choice and tuning of ANN and vector compression
    algorithms can be tuned but are usually not tuned by practitioners.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'And the following strategies in the [inferencing stage (retrieval and generation)](#ac53):'
  prefs: []
  type: TYPE_NORMAL
- en: '[Query transformations](#a5e2): Experiment with rephrasing, HyDE, or sub-queries.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Retrieval parameters](#fa73): Choice of search technique (`alpha` if you have
    hybrid search enabled) and the number of retrieved search results.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Advanced retrieval strategies](#a3bb): Whether to use advanced retrieval strategies,
    such as sentence-window or auto-merging retrieval.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Re-ranking models](#341d): Whether to use a re-ranking model, choice of re-ranking
    model, number of search results to input into the re-ranking model, and whether
    to fine-tune the re-ranking model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[LLMs](#e9f9): Choice of LLM and whether to fine-tune it.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Prompt engineering](#9c1c): Experiment with different phrasing and few-shot
    examples.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Enjoyed This Story?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[*Subscribe for free*](https://medium.com/subscribe/@iamleonie) *to get notified
    when I publish a new story.*'
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/@iamleonie/subscribe?source=post_page-----7ca646833439--------------------------------)
    [## Get an email whenever Leonie Monigatti publishes.'
  prefs: []
  type: TYPE_NORMAL
- en: Get an email whenever Leonie Monigatti publishes. By signing up, you will create
    a Medium account if you don’t already…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: medium.com](https://medium.com/@iamleonie/subscribe?source=post_page-----7ca646833439--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: '*Find me on* [*LinkedIn*](https://www.linkedin.com/in/804250ab/),[*Twitter*](https://twitter.com/helloiamleonie)*,
    and* [*Kaggle*](https://www.kaggle.com/iamleonie)*!*'
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Literature
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] [Connor Shorten](https://medium.com/u/59216259c525?source=post_page-----7ca646833439--------------------------------)
    and [Erika Cardenas](https://medium.com/u/91b27bdf28df?source=post_page-----7ca646833439--------------------------------)
    (2023). Weaviate Blog. [An Overview on RAG Evaluation](https://weaviate.io/blog/rag-evaluation)
    (accessed Nov. 27, 2023)'
  prefs: []
  type: TYPE_NORMAL
- en: '[2] [Jerry Liu](https://medium.com/u/e76da1c45ef7?source=post_page-----7ca646833439--------------------------------)
    (2023). LlamaIndex Blog. [Fine-Tuning Embeddings for RAG with Synthetic Data](https://blog.llamaindex.ai/fine-tuning-embeddings-for-rag-with-synthetic-data-e534409a3971)
    (accessed Nov. 28, 2023)'
  prefs: []
  type: TYPE_NORMAL
- en: '[3] LlamaIndex Documentation (2023). [Building Performant RAG Applications
    for Production](https://gpt-index.readthedocs.io/en/stable/optimizing/production_rag.html)
    (accessed Nov. 28, 2023)'
  prefs: []
  type: TYPE_NORMAL
- en: '[4] Voyage AI (2023). [Embeddings Drive the Quality of RAG: A Case Study of
    Chat.LangChain](https://blog.voyageai.com/2023/10/29/a-case-study-of-chat-langchain/)
    (accessed Dec. 5, 2023)'
  prefs: []
  type: TYPE_NORMAL
- en: '[5] LlamaIndex Documentation (2023). [Query Transformations](https://gpt-index.readthedocs.io/en/v0.6.9/how_to/query/query_transformations.html)
    (accessed Nov. 28, 2023)'
  prefs: []
  type: TYPE_NORMAL
- en: '[6] Liu, N. F., Lin, K., Hewitt, J., Paranjape, A., Bevilacqua, M., Petroni,
    F., & Liang, P. (2023). Lost in the middle: How language models use long contexts.
    *arXiv preprint arXiv:2307.03172*.'
  prefs: []
  type: TYPE_NORMAL
- en: '[7] DeepLearning.AI (2023). [Building and Evaluating Advanced RAG Applications](https://www.deeplearning.ai/short-courses/building-evaluating-advanced-rag/)
    (accessed Dec 4, 2023)'
  prefs: []
  type: TYPE_NORMAL
- en: '[8] [Ahmed Besbes](https://medium.com/u/adc8ea174c69?source=post_page-----7ca646833439--------------------------------)
    (2023). Towards Data Science. [Why Your RAG Is Not Reliable in a Production Environment](/why-your-rag-is-not-reliable-in-a-production-environment-9e6a73b3eddb)
    (accessed Nov. 27, 2023)'
  prefs: []
  type: TYPE_NORMAL
- en: '[9] [Matt Ambrogi](https://medium.com/u/1e23ad8f92c5?source=post_page-----7ca646833439--------------------------------)
    (2023). Towards Data Science. [10 Ways to Improve the Performance of Retrieval
    Augmented Generation Systems](/10-ways-to-improve-the-performance-of-retrieval-augmented-generation-systems-5fa2cee7cd5c)
    (accessed Nov. 27, 2023)'
  prefs: []
  type: TYPE_NORMAL
- en: Images
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If not otherwise stated, all images are created by the author.
  prefs: []
  type: TYPE_NORMAL
