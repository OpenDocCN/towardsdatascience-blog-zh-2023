# Python æ•°æ®å·¥ç¨‹å¸ˆ

> åŸæ–‡ï¼š[`towardsdatascience.com/python-for-data-engineers-f3d5db59b6dd`](https://towardsdatascience.com/python-for-data-engineers-f3d5db59b6dd)

## åˆå­¦è€…çš„é«˜çº§ ETL æŠ€å·§

[](https://mshakhomirov.medium.com/?source=post_page-----f3d5db59b6dd--------------------------------)![ğŸ’¡Mike Shakhomirov](https://mshakhomirov.medium.com/?source=post_page-----f3d5db59b6dd--------------------------------)[](https://towardsdatascience.com/?source=post_page-----f3d5db59b6dd--------------------------------)![Towards Data Science](https://towardsdatascience.com/?source=post_page-----f3d5db59b6dd--------------------------------) [ğŸ’¡Mike Shakhomirov](https://mshakhomirov.medium.com/?source=post_page-----f3d5db59b6dd--------------------------------)

Â·å‘å¸ƒäº [Towards Data Science](https://towardsdatascience.com/?source=post_page-----f3d5db59b6dd--------------------------------) Â·é˜…è¯»æ—¶é—´ 17 åˆ†é’ŸÂ·2023 å¹´ 10 æœˆ 21 æ—¥

--

![](img/9c664876939298ade749c4c53cb490c8.png)

å›¾ç‰‡ç”± [Boitumelo](https://unsplash.com/@writecodenow?utm_source=medium&utm_medium=referral) æä¾›ï¼Œæ¥è‡ª [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)

åœ¨è¿™ä¸ªæ•…äº‹ä¸­ï¼Œæˆ‘å°†è®²è¿° Python ä¸­çš„é«˜çº§æ•°æ®å·¥ç¨‹æŠ€æœ¯ã€‚æ¯«æ— ç–‘é—®ï¼ŒPython æ˜¯æœ€å—æ¬¢è¿çš„æ•°æ®ç¼–ç¨‹è¯­è¨€ã€‚åœ¨æˆ‘è¿‘åäºŒå¹´çš„æ•°æ®å·¥ç¨‹èŒä¸šç”Ÿæ¶¯ä¸­ï¼Œæˆ‘é‡åˆ°è¿‡å„ç§ä»£ç é—®é¢˜ã€‚è¿™ä¸ªæ•…äº‹ç®€è¦æ€»ç»“äº†æˆ‘å¦‚ä½•è§£å†³è¿™äº›é—®é¢˜å¹¶å­¦ä¼šå†™å‡ºæ›´å¥½çš„ä»£ç ã€‚æˆ‘å°†å±•ç¤ºä¸€äº›ä½¿æˆ‘ä»¬çš„ ETL æ›´å¿«å¹¶æœ‰åŠ©äºæé«˜ä»£ç æ€§èƒ½çš„æŠ€æœ¯ã€‚

## åˆ—è¡¨æ¨å¯¼å¼

æƒ³è±¡ä¸€ä¸‹ä½ æ­£åœ¨éå†ä¸€ä¸ªè¡¨çš„åˆ—è¡¨ã€‚é€šå¸¸ï¼Œæˆ‘ä»¬ä¼šè¿™æ ·åšï¼š

```py
data_pipelines = ['p1','p2','p3']
processed_tables = []
for table in data_pipelines:
    processed_tables.append(table)
```

ä½†æˆ‘ä»¬å¯ä»¥ä½¿ç”¨åˆ—è¡¨æ¨å¯¼å¼ã€‚å®ƒä»¬ä¸ä»…æ›´å¿«ï¼Œè¿˜å‡å°‘äº†ä»£ç ï¼Œä½¿å…¶æ›´ç®€æ´ï¼š

```py
processed_tables = [table for table in data_pipelines]
```

ä¾‹å¦‚ï¼Œå¾ªç¯å¤„ç†ä¸€ä¸ªè¶…å¤§çš„æ–‡ä»¶ä»¥è½¬æ¢ï¼ˆETLï¼‰æ¯ä¸€è¡Œï¼Œä»æœªå¦‚æ­¤ç®€å•ï¼š

```py
def etl(item):
    # Do some data transformation here
    return json.dumps(item)

data = u"\n".join(etl(item) for item in json_data)
```

åˆ—è¡¨æ¨å¯¼å¼å¯¹äº ETL å¤„ç†å¤§æ•°æ®æ–‡ä»¶éå¸¸æœ‰ç”¨ã€‚å‡è®¾æˆ‘ä»¬æœ‰ä¸€ä¸ªéœ€è¦è½¬æ¢ä¸ºæ¢è¡Œç¬¦åˆ†éš”æ ¼å¼çš„æ•°æ®æ–‡ä»¶ã€‚åœ¨ä½ çš„ Python ç¯å¢ƒä¸­å°è¯•è¿è¡Œè¿™ä¸ªç¤ºä¾‹ï¼š

```py
 import io
import json

def etl(item):
    return json.dumps(item)

# Text file loaded as a blob
blob = """
        [
{"id":"1","first_name":"John"},
{"id":"2","first_name":"Mary"}
]
"""
json_data = json.loads(blob)
data_str = u"\n".join(etl(item) for item in json_data)

print(data_str)
data_file = io.BytesIO(data_str.encode())

# This data file is ready for BigQuery as Newline delimited JSON
print(data_file)
```

è¾“å‡ºå°†æ˜¯**æ¢è¡Œç¬¦åˆ†éš”çš„ JSON**ã€‚è¿™æ˜¯ BigQuery æ•°æ®ä»“åº“ä¸­çš„ä¸€ç§æ ‡å‡†æ ¼å¼ï¼Œå‡†å¤‡å¥½åŠ è½½åˆ°è¡¨ä¸­äº†ï¼š

```py
{"id": "1", "first_name": "John"}
{"id": "2", "first_name": "Mary"}
<_io.BytesIO object at 0x10c732430>
```

## ç”Ÿæˆå™¨

å¦‚æœæˆ‘ä»¬å¤„ç†çš„æ˜¯**é€è¡Œå­˜å‚¨**çš„ CSV å’Œ DAT æ–‡ä»¶ï¼Œé‚£ä¹ˆæˆ‘ä»¬çš„æ–‡ä»¶å¯¹è±¡å·²ç»æ˜¯ä¸€ä¸ª**ç”Ÿæˆå™¨**ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨åˆ—è¡¨æ¨å¯¼å¼æ¥å¤„ç†æ•°æ®ï¼Œ**ä¸ä¼šæ¶ˆè€—å¤ªå¤šå†…å­˜**ï¼š

```py
for line in open('very_big_file.csv'):
    validate_schema(line)

# or the same using list comprehension:
data_errors = [validate_schema(line) for line in open('very_big_file.csv')]
```

åœ¨æˆ‘ä»¬å®é™…å°†è®°å½•æ’å…¥æ•°æ®ä»“åº“è¡¨ä¹‹å‰éªŒè¯è®°å½•ï¼Œå¯¹äºæ‰¹é‡æ•°æ®å¤„ç†ç®¡é“å¯èƒ½æ˜¯æœ‰ç”¨çš„ã€‚

> æˆ‘ä»¬ç»å¸¸éœ€è¦åœ¨å°†æ•°æ®æ–‡ä»¶åŠ è½½åˆ°æ•°æ®ä»“åº“ä¹‹å‰éªŒè¯å®ƒä»¬ã€‚å¦‚æœä¸€ä¸ªè®°å½•å¤±è´¥ï¼Œé‚£ä¹ˆæ•´ä¸ªæ‰¹æ¬¡éƒ½ä¼šå¤±è´¥ã€‚

æˆ‘ä»¬å¯ä»¥ç”¨å®ƒæ¥åˆ›å»ºæ¥è¿‘å®æ—¶çš„åˆ†æç®¡é“ã€‚è¿™ä¹Ÿæ˜¯ä¸€ç§éå¸¸ç»æµé«˜æ•ˆçš„æ–¹å¼æ¥å¤„ç†æ•°æ®ï¼Œç›¸æ¯”äºæµæ•°æ®ç®¡é“è®¾è®¡æ¨¡å¼ã€‚æˆ‘ä¹‹å‰åœ¨è¿™é‡Œå†™è¿‡ï¼š

[](/data-pipeline-design-patterns-100afa4b93e3?source=post_page-----f3d5db59b6dd--------------------------------) ## æ•°æ®ç®¡é“è®¾è®¡æ¨¡å¼

### é€‰æ‹©åˆé€‚çš„æ¶æ„å’Œç¤ºä¾‹

[towardsdatascience.com

æˆ–è€…ï¼Œåœ¨å¤„ç†å¤§æ•°æ®æ—¶ï¼Œå¦‚æœæˆ‘ä»¬çš„æ–‡ä»¶ä¸æ˜¯æ¢è¡Œç¬¦åˆ†éš”çš„æ–‡æœ¬ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨`yield`ã€‚è¿™å§‹ç»ˆæ˜¯ä¸€ä¸ªå¥½ä¹ æƒ¯ï¼Œå› ä¸ºæˆ‘ä»¬å¸Œæœ›ä»¥å†…å­˜é«˜æ•ˆçš„æ–¹å¼å¤„ç†æ•°æ®æ–‡ä»¶ã€‚ä¾‹å¦‚ï¼š

```py
# Create a file first: ./very_big_file.csv as:
# transaction_id,user_id,total_cost,dt
# 1,John,10.99,2023-04-15
# 2,Mary, 4.99,2023-04-12

# Example.py
def etl(item):
    # Do some etl here
    return item.replace("John", '****') 

# Create a generator 
def batch_read_file(file_object, batch_size=19):
    """Lazy function (generator) can read a file in chunks.
    Default chunk: 1024 bytes."""
    while True:
        data = file_object.read(batch_size)
        if not data:
            break
        yield data
# and read in chunks
with open('very_big_file.csv') as f:
    for batch in batch_read_file(f):
        print(etl(batch))

# In command line run
# Python example.py 
```

è¿™å°†è¯»å–ä¸€ä¸ªæœ¬åœ°æ–‡ä»¶å¹¶ä»¥ 19 å­—èŠ‚ä¸ºå—è¿›è¡Œå¤„ç†ã€‚è¾“å‡ºå°†æ˜¯ï¼š

```py
transaction_id,user
_id,total_cost,dt
1
,****,10.99,2023-04
-15
2,Mary, 4.99,20
23-04-12
```

è¿™åªæ˜¯å¤„ç†**äºŒè¿›åˆ¶**æ•°æ®çš„ä¸€ä¸ªç¤ºä¾‹ã€‚åœ¨ç°å®ç”Ÿæ´»ä¸­ï¼Œå°†æ–‡ä»¶å†…å®¹åˆ†å‰²æˆæ®µä½¿ç”¨**åˆ†éš”ç¬¦**ï¼ˆå³æ¢è¡Œç¬¦`'\nâ€™`æˆ–`'}{'`ï¼‰å¯èƒ½ä¼šæ›´å®¹æ˜“ï¼Œè¿™å–å†³äºæˆ‘ä»¬çš„æ•°æ®ç»“æ„ã€‚

å‡è®¾æ–‡æœ¬æ•°æ®æ¥è‡ªæŸä¸ª**å¤–éƒ¨**ä½ç½®ï¼Œå³**äº‘å­˜å‚¨**ã€‚æˆ‘ä»¬ä¹Ÿå¯ä»¥å°†å…¶å¤„ç†ä¸º**æµ**ã€‚æˆ‘ä»¬ä¸å¸Œæœ›åŠ è½½æ•´ä¸ªæ•°æ®æ–‡ä»¶å¹¶è¿è¡Œ`split('\n')`é€è¡Œå¤„ç†ï¼Œè¿™ä¼šæ¶ˆè€—å¤§é‡å†…å­˜ã€‚æˆ‘ä»¬å¯ä»¥ä½¿ç”¨`re.finditer`ï¼Œå®ƒåƒ**ç”Ÿæˆå™¨**ä¸€æ ·ï¼Œä»¥å—çš„æ–¹å¼è¯»å–æ•°æ®æ–‡ä»¶ï¼Œè¿™æ ·æˆ‘ä»¬å°±å¯ä»¥è¿è¡Œæ‰€éœ€çš„ ETL è€Œ**ä¸ä¼š**æ¶ˆè€—å¤ªå¤šå†…å­˜ã€‚

```py
import io
import re
def etl(item):
    print(f'Transforming item: {item}')
    return item.replace("John", '****')

# Helper function to split our text file into chunks
# using separator
def splitStr(string, sep="\s+"):
    if sep=='':
        return (c for c in string)
    else:
        return (_.group(1) for _ in re.finditer(f'(?:^|{sep})((?:(?!{sep}).)*)', string))

# Text file loaded as a blob
blob = """transaction_id,user_id,total_cost,dt
1,John,10.99,2023-04-15
2,Mary, 4.99,2023-04-12
"""

# data = blob.split("\n") # We wouldn't want to do this on large datasets 
# as it would require to load big data file as a whole in the first place
# consuming lots of memory

# We would want to use our generator helper function
# and process data in chunks
data = splitStr(blob, sep='\n')
data_str = u"\n".join(etl(item) for item in data)

print('New file contents:')
print(data_str)
data_file = io.BytesIO(data_str.encode())

print('This data file is ready for BigQuery:')
print(data_file) 
```

è¾“å‡ºï¼š

```py
python example.py                                 î‚² âœ” î‚² 48 î‚² 19:52:06 î‚² dataform_env
Transforming item: transaction_id,user_id,total_cost,dt
Transforming item: 1,John,10.99,2023-04-15
Transforming item: 2,Mary, 4.99,2023-04-12
Transforming item:
New file contents:
transaction_id,user_id,total_cost,dt
1,****,10.99,2023-04-15
2,Mary, 4.99,2023-04-12

This data file is ready for BigQuery:
<_io.BytesIO object at 0x103672980>
```

## Python **æ•°æ®éªŒè¯å±æ€§**

æˆ‘ä»¬å¯ä»¥ä½¿ç”¨**Python å±æ€§** [2]æ¥éªŒè¯æ•°æ®è®°å½•ã€‚å¦‚æœè®°å½•ä¸æ˜¯æˆ‘ä»¬å®šä¹‰çš„ç±»çš„å®ä¾‹ï¼Œåˆ™å¿…é¡»æŠ›å‡ºå¼‚å¸¸ã€‚

> æˆ‘ä»¬å¯ä»¥å°†æ•°æ®å­˜å‚¨ä¸ºæ•°æ®ç±»çš„å¯¹è±¡ã€‚

å°±æ˜¯è¿™ä¹ˆç®€å•ã€‚å‡è®¾æˆ‘ä»¬æœ‰ä¸€ä¸ª**æµæ•°æ®ç®¡é“**ï¼Œæˆ‘ä»¬æƒ³è¦éªŒè¯è®°å½•ä¸­çš„ä¸€äº›å­—æ®µã€‚

> ç®€å•æ¥è¯´â€”â€”å®ƒä»¬å¿…é¡»åŒ¹é…ç°æœ‰çš„è¡¨æ ¼æ¨¡å¼ã€‚

æˆ‘ä»¬å¯ä»¥ä½¿ç”¨ Python å±æ€§æ¥å®ç°ã€‚è¯·çœ‹ä¸‹é¢çš„ç¤ºä¾‹ã€‚

```py
class ConnectionDataRecord(object):
    def __init__(self, user, ts):
        self.user = user
        self.ts = ts

    @property
    def user(self):
        return self._user

    @description.setter
    def user(self, d):
        if not d: raise Exception("user cannot be empty")
        self._user = d

    @property
    def ts(self):
        return self._ts

    @value.setter
    def ts(self, v):
        if not (v > 0): raise Exception("value must be greater than zero")
        self._ts = v
```

å¦‚æœæˆ‘ä»¬é€‰æ‹©æ‰“ç ´è§„åˆ™å¹¶åˆ†é…ä¸€äº›ä¸ç¬¦åˆæˆ‘ä»¬æ ‡å‡†çš„å€¼ï¼Œåˆ™ä¼šæŠ›å‡ºå¼‚å¸¸ã€‚ä¾‹å¦‚ï¼Œå¦‚æœæˆ‘ä»¬å°è¯•è°ƒç”¨`ConnectionDataRecord('', 1)`ï¼Œå°†ä¼šæŠ›å‡ºå¼‚å¸¸ã€‚

æˆ–è€…ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ä¸€ä¸ªåä¸º`Pydantic`çš„åº“ã€‚è¯·çœ‹ä¸‹é¢çš„ä»£ç ã€‚å¦‚æœæˆ‘ä»¬ç”¨ä¸ç¬¦åˆè¦æ±‚çš„å¯¹è±¡è°ƒç”¨è¯¥å‡½æ•°ï¼Œå®ƒå°†æŠ›å‡ºä¸€ä¸ªé”™è¯¯ã€‚

```py
from pydantic import BaseModel

class ConnectionDataRecord(BaseModel):
    user: str
    ts: int

record = ConnectionDataRecord(user="user1", ts=123456789)
```

## è£…é¥°å™¨

è£…é¥°å™¨çš„è®¾è®¡ç›®çš„æ˜¯ä½¿æˆ‘ä»¬çš„ä»£ç çœ‹èµ·æ¥æ›´ç®€æ´ï¼Œå¹¶ä¸ºå…¶æ·»åŠ é¢å¤–çš„åŠŸèƒ½ã€‚æˆ‘ä»¬å¯ä»¥ç®€å•åœ°å°†ä¸€ä¸ªå‡½æ•°ä½œä¸ºå‚æ•°ä¼ é€’ç»™å¦ä¸€ä¸ªå‡½æ•°ï¼ˆè£…é¥°å™¨ï¼‰ï¼Œå¹¶åœ¨è¿™ä¸ªåŒ…è£…å™¨å†…éƒ¨è¿›è¡Œä¸€äº›æ•°æ®è½¬æ¢ã€‚æƒ³è±¡ä¸€ä¸‹ï¼Œæˆ‘ä»¬æœ‰å¾ˆå¤šä¸åŒçš„ ETL å‡½æ•°æ¥å¤„ç†æ•°æ®ï¼Œä½†æˆ‘ä»¬åªéœ€è¦ä¸€ä¸ªå°†ç»“æœä¸Šä¼ åˆ°æ•°æ®æ¹–çš„å‡½æ•°ã€‚è¿™å°±æ˜¯æˆ‘ä»¬å¦‚ä½•åšåˆ°çš„ï¼š

> å¦‚æœä¸€äº›ä»£ç é€»è¾‘é‡å¤ï¼Œä½¿ç”¨**è£…é¥°å™¨**æ˜¯ä¸€ä¸ªå¥½ä¹ æƒ¯ã€‚

è¿™æœ‰åŠ©äºæ›´å®¹æ˜“ç»´æŠ¤ä»£ç åº“ï¼Œå¹¶èŠ‚çœäº†æˆ‘ä»¬åœ¨éœ€è¦æ›´æ”¹é‡å¤é€»è¾‘æ—¶çš„å¾ˆå¤šæ—¶é—´ã€‚

```py
def etl_decorator(func):
    def wrapper():
        result = func()
        return f'Processing {result}' 
    return wrapper

@etl_decorator
def unzip_data():
    return "unzipped data"

print(unzip_data())  # Output: Processing unzipped data
```

è£…é¥°å™¨å› å…¶æœ‰æ•ˆæ€§è¢«å¹¿æ³›ä½¿ç”¨ã€‚è€ƒè™‘è¿™ä¸ª Airflow DAG ç¤ºä¾‹ï¼š

```py
@dag(default_args=default_args, tags=['etl'])
def etl_pipeline():

    @task()
    def extract():
        return json.loads(data_string)    
    @task(multiple_outputs=True)
    def transform(order_data_dict: dict):
        return {"total_count": len(order_data_dict)}    
    @task()
    def load(total_order_value: float):
        print(f"Total order value is: {total_count}")    

    extracted = extract()
    transformed = transform(extracted)
    load(transformed["total_count"])
```

## ä¸ API çš„å·¥ä½œ

ä½œä¸ºæ•°æ®å·¥ç¨‹å¸ˆï¼Œä½ ä¼šç»å¸¸æ‰§è¡Œ HTTP è¯·æ±‚ï¼Œè°ƒç”¨å„ç§ API ç«¯ç‚¹ã€‚ä¸‹é¢æ˜¯ä¸€ä¸ª GET è¯·æ±‚çš„ç¤ºä¾‹ã€‚

```py
response = requests.get('https://api.nasa.gov/neo/rest/v1/feed?start_date=2015-09-07&end_date=2015-09-08&api_key=your_api_key'))
print(response.json())
```

å®ƒä»**å…è´¹**çš„**NASA å°è¡Œæ˜Ÿ API**ä¸­æå–ä¸€äº›æ•°æ®ï¼Œå¹¶è¿”å›æ‰€æœ‰åœ¨è¯¥æ—¥æœŸæ¥è¿‘åœ°çƒçš„å°è¡Œæ˜Ÿã€‚åªéœ€åœ¨ä¸Šé¢çš„ URL è·¯å¾„ä¸­æ›¿æ¢ä½ çš„ API å¯†é’¥æˆ–ä½¿ç”¨æˆ‘åˆ›å»ºçš„å¯†é’¥ã€‚**requests**åº“å¤„ç†æ‰€æœ‰äº‹æƒ…ï¼Œä½†è¿˜æœ‰æ›´å¥½çš„æ–¹æ³•ã€‚

> æˆ‘ä»¬å¯ä»¥ä½¿ç”¨ä¼šè¯å¹¶ä»¥**æµ**çš„å½¢å¼å¤„ç†æ¥è‡ªæˆ‘ä»¬ API ç«¯ç‚¹çš„æ•°æ®ã€‚

è¿™å°†ç¡®ä¿æˆ‘ä»¬ä¸ä¼šé‡åˆ°ä»»ä½•å†…å­˜é—®é¢˜ï¼Œå¹¶ä»¥æµå¼æ–¹å¼å¤„ç†æˆ‘ä»¬çš„ GET è¯·æ±‚[3]ï¼š

```py
import requests
session = requests.Session()

url="https://api.nasa.gov/neo/rest/v1/feed"
apiKey="your_api_key"
requestParams = {
    'api_key': apiKey,
    'start_date': '2023-04-20',
    'end_date': '2023-04-21'
}
response = session.get(url, params = requestParams, stream=True)
print(response.status_code)
```

ç†è§£ HTTP è¯·æ±‚çš„å·¥ä½œåŸç†åœ¨æ•°æ®å·¥ç¨‹ä¸­è‡³å…³é‡è¦ã€‚

> æˆ‘æ¯å¤©å¤„ç†å„ç§ API è¯·æ±‚ï¼Œä¸å¿…ä¾èµ–å…¶ä»–æ¡†æ¶æˆ–åº“ã€‚

ä¾‹å¦‚ï¼Œå°±åœ¨å‡ å‘¨å‰ï¼Œæˆ‘åœ¨**Dataform è¿ç§»é¡¹ç›®**ä¸Šå·¥ä½œï¼Œæ„è¯†åˆ°ç°æœ‰çš„ Google åº“ï¼ˆ`from google.cloud import dataform_v1beta1`ï¼‰æ— æ³•åˆ›å»ºè°ƒåº¦ã€‚è§£å†³æ–¹æ³•æ˜¯ä½¿ç”¨ Dataform API [4]ï¼Œè¿™å°±åƒå‘ç‰¹å®šç«¯ç‚¹å‘å‡º POST è¯·æ±‚ä¸€æ ·ç®€å•ï¼š

```py
from google.cloud import dataform_v1beta1
import requests
import google.auth.transport.requests
from google.oauth2 import service_account
...
# Get Dataform and BigQuery credentials from encrypted file:
print(f'Getting BigQuery credentials from encrypted file...')
credentials = service_account.Credentials.from_service_account_file(
    './../credentials.json'
    , scopes=['https://www.googleapis.com/auth/cloud-platform'])

def create_dataform_workflow_config(credentials, id, workflow_config, repository_id):
    '''
    The function will create a schedule (workflow) in Dataform configs by making a direct API call
    and using request_params with requests module
    , i.e.
    https://cloud.google.com/dataform/reference/rest/v1beta1/projects.locations.repositories.workflowConfigs/create
    https://cloud.google.com/dataform/reference/rest/v1beta1/projects.locations.repositories.workflowConfigs#WorkflowConfig
    If successful will create a workflow:
    {'name': 'projects/my-project-data-staging/locations/us-central1/repositories/dataform-poc/workflowConfigs/test_live_20230831', 'releaseConfig': 'projects/my-project-data-staging/locations/us-central1/repositories/dataform-poc/releaseConfigs/staging', 'invocationConfig': {'includedTags': ['test']}, 'cronSchedule': '40 13 * * *', 'timeZone': 'Africa/Abidjan'}

    If the workflow exists an error will be sent:
    {'error': {'code': 409, 'message': "Resource 'projects/123456789/locations/us-central1/repositories/dataform-poc/workflowConfigs/test_live_20230831' already exists", 'status': 'ALREADY_EXISTS', 'details': [{'@type': 'type.googleapis.com/google.rpc.ResourceInfo', 'resourceName': 'projects/123456789/locations/us-central1/repositories/dataform-poc/workflowConfigs/test_live_20230831'}]}}

    Accepts workflow_config as request_body, i.e.
     request_body = {
             # "name": "projects/123456789/locations/us-central1/repositories/dataform-poc/workflowConfigs/test_live_20230830",
             "releaseConfig": "projects/my-project-data-staging/locations/us-central1/repositories/dataform-poc/releaseConfigs/staging",
             "invocationConfig": {
                 "includedTags": [
                     "test"
                 ]
             },
             "cronSchedule": "40 13 * * *",
             "timeZone": "Africa/Abidjan"
     }
    '''
    request = google.auth.transport.requests.Request()
    credentials.refresh(request)

    print('Creating a workflow...')
    # Make the request
    try:
        session = requests
        url=f'https://dataform.googleapis.com/v1beta1/projects/123456789/locations/us-central1/repositories/{repository_id}/workflowConfigs/'

        headers = {
            "Authorization": "Bearer " + credentials.token,
            "Content-Type" : "application/json; charset=utf-8"

        }
        query_params = {
            "workflowConfigId": id
        }
        request_body = workflow_config

        page_result = session.post(url, params=query_params, json=request_body, headers=headers)
        print(page_result.json())
    except Exception as e:
        print(e)
```

è¿™ä¸ªè¯·æ±‚çš„æ ¸å¿ƒåœ¨äºæˆ‘ä»¬å°†`workflow_config`ä½œä¸º**json**å‘é€ï¼Œå¹¶ä½¿ç”¨æ¥è‡ª Google æ–‡æ¡£[4]çš„çŸ¥è¯†åœ¨è·¯å¾„å‚æ•°ä¸­æ·»åŠ `workflowConfigId`ã€‚

> è¿™å°†åˆ›å»ºä¸€ä¸ªå¿…è¦çš„è°ƒåº¦ï¼Œä»¥åœ¨ BigQuery çš„ Dataform ä¸­è¿è¡Œæˆ‘ä»¬çš„æ•°æ®è½¬æ¢è„šæœ¬ã€‚

ç±»ä¼¼åœ°ï¼Œæ­£å¦‚æˆ‘ä»¬åœ¨ GET è¯·æ±‚ä¸­æ‰€åšçš„é‚£æ ·ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ Python **ç”Ÿæˆå™¨**å°†æ•°æ®æµå…¥æˆ‘ä»¬çš„ POST API ç«¯ç‚¹ï¼š

```py
import time
import requests

def etl_data_generator():
    yield b"Foo"
    time.sleep(3)
    yield b"Bar"

requests.post("http://some.api.endpoint", data=etl_data_generator())
```

æ€è·¯å¾ˆæ¸…æ¥šã€‚æˆ‘ä»¬å¯ä»¥ä»¥èŠ‚çœå†…å­˜çš„æ–¹å¼å¤„ç†å’Œå‘é€æ•°æ®ã€‚

## å¤„ç† API é€Ÿç‡é™åˆ¶

æ‰€æœ‰ API éƒ½æœ‰é€Ÿç‡é™åˆ¶ï¼Œæˆ‘ä»¬åœ¨æå–æ•°æ®æ—¶è¦è®°ä½è¿™ä¸€ç‚¹ã€‚æˆ‘ä»¬å¯ä»¥ä½¿ç”¨è£…é¥°å™¨æ¥å¤„ç†å®ƒã€‚ç®€å•çš„è£…é¥°å¯ä»¥åƒè¿™æ ·å®ç°ï¼š

```py
from ratelimit import limits
import requests
CALLS = 10
TIME_PERIOD = 900   # time period in seconds

@limits(calls=CALLS, period=TIME_PERIOD)
def call_api():
    response = requests.get('https://api.example.com/data')
    if response.status_code != 200:
        raise Exception('API response: {}'.format(response.status_code))
    return response.json()
```

ä½¿ç”¨è¿™ä¸ªè£…é¥°å™¨ï¼Œæˆ‘ä»¬çš„å‡½æ•°åœ¨ 15 åˆ†é’Ÿå†…ä¸ä¼šå‘èµ·è¶…è¿‡ 10 æ¬¡ API è°ƒç”¨ã€‚

å¤„ç†è¿™ç§æƒ…å†µçš„æœ€ç®€å•æ–¹æ³•æ˜¯ä½¿ç”¨`time.sleep()`ï¼Œä½† Python é€Ÿç‡é™åˆ¶å…è®¸æˆ‘ä»¬ä»¥è¿™ç§ä¼˜é›…çš„æ–¹å¼åšåˆ°è¿™ä¸€ç‚¹ã€‚

## Python ä¸­çš„ Async å’Œ`await`

ä»¥**å¼‚æ­¥**æ–¹å¼æ‰§è¡Œ ETL æ˜¯å¦ä¸€ä¸ªæå…¶æœ‰ç”¨çš„åŠŸèƒ½ã€‚æˆ‘ä»¬å¯ä»¥ä½¿ç”¨`asyncio`åº“æ¥åŒæ—¶è¿è¡Œä»»åŠ¡ã€‚è®©æˆ‘ä»¬è€ƒè™‘è¿™ä¸ªç®€å•çš„åŒæ­¥ç¤ºä¾‹ï¼Œå…¶ä¸­æˆ‘ä»¬åœ¨`for`å¾ªç¯ä¸­å¤„ç†è¡¨ï¼š

```py
import requests

def pull_data(url, requestParams):
    return requests.get(url, params = requestParams, stream=True)

for table in api_endpoints_list:
    data = pull_data(table.api_url, table.requestParams)
    etl(data)
```

è¿è¡Œè¿™æ®µä»£ç æ—¶ï¼Œæˆ‘ä»¬å¿…é¡»ç­‰å¾…æ¯ä¸ªè¡¨å®Œæˆ`pull_data()`ä»»åŠ¡ï¼Œä½†ä½¿ç”¨`Async`ï¼Œæˆ‘ä»¬å¯ä»¥å¹¶è¡Œå¤„ç†å®ƒä»¬ã€‚

è€ƒè™‘ä½¿ç”¨ä»¥ä¸‹ä»£ç ï¼š

```py
import asyncio
import aiohttp

async def pull_data(session, url, requestParams):
  async with session.get(url, params = requestParams, stream=True) as response:
    return await response

async def main():
    async with aiohttp.ClientSession() as session:
        tasks = [pull_data(session, url, requestParams) for table in api_endpoints_list:]
        tasks_data = await asyncio.gather(*tasks)
        for data in tasks_data:
            etl(task_data)

asyncio.run(main())
```

å®ƒå°†åŒæ—¶ä»æŠ¥å‘Š API ä¸­æå–æ•°æ®ï¼Œå¹¶æ˜¾è‘—æé«˜æˆ‘ä»¬çš„ ETL æ€§èƒ½ã€‚

> å®ƒå¸®åŠ©ç®¡ç† ETL ä»»åŠ¡ï¼ŒåŒæ—¶ç³»ç»Ÿèµ„æºä»¥æœ€ä½³æ–¹å¼åˆ†é…ã€‚

ä¾‹å¦‚ï¼Œæˆ‘ä»¬å¯ä»¥åŒæ—¶è¿è¡Œä¸¤ä¸ª ETL ä½œä¸šï¼Œä½†æˆ‘ä»¬å¯ä»¥å®šä¹‰æ‰§è¡Œé¡ºåºï¼š

```py
async def etl():
    job1 = asyncio.create_task(perform_etl_script1())
    job2 = asyncio.create_task(read_s3_data())
    job3 = asyncio.create_task(upload_s3_data())

    await job2
    await job1
    await job3
```

## ä½¿ç”¨ Map å’Œ Filter

æ˜ å°„å’Œè¿‡æ»¤æ¯”åˆ—è¡¨æ¨å¯¼å¼çš„é€Ÿåº¦æ›´å¿«ã€‚

æˆ‘ä»¬å¯ä»¥é€è¡Œè½¬æ¢æ•°æ®ï¼Œå°†`map`å‡½æ•°åº”ç”¨äºæ•°æ®é›†ä¸­çš„é¡¹ç›®ï¼Œå°†å…¶å¤„ç†ä¸º`iterable`ï¼š

```py
import math 
numbers = [10,20]
factorials = list(map(lambda i: math.factorial(int(math.sqrt(i**3))), numbers))
print(factorials)
# Output:
# [8222838654177922817725562880000000, 16507955160908461081216919262453619309839666236496541854913520707833171034378509739399912570787600662729080382999756800000000000000000000]
```

æˆ‘ä»¬å¸Œæœ›ä½¿ç”¨`filter`æ¥æå–ç¬¦åˆç‰¹å®šæ¡ä»¶çš„å¯¹è±¡ï¼Œå³

```py
 numbers = [10,21]
even_numbers = list(filter(lambda i: i% 2 == 0, numbers))
print(even_numbers)
# Output:
# [10]
```

## ä½¿ç”¨ Pandas å¤„ç†å¤§å‹æ•°æ®é›†

åæ¥çš„ Pandas åº“ç‰ˆæœ¬æä¾›äº†ä¸€ä¸ªæ–¹ä¾¿çš„ä¸Šä¸‹æ–‡ç®¡ç†å™¨ï¼Œå¯ä»¥åƒè¿™æ ·ä½¿ç”¨ï¼š

```py
batchsize = 10 ** 5
with pd.read_csv(filename, chunksize=batchsize) as reader:
    for batch in reader:
        etl(batch)
```

> å®ƒå°†ä»¥æ‰¹å¤„ç†æ¨¡å¼å¤„ç†æ•°æ®ï¼Œå‡è®¾æˆ‘ä»¬ä¸éœ€è¦ä¸€æ¬¡æ€§å°†æ•´ä¸ªæ•°æ®é›†åŠ è½½åˆ°æ•°æ®æ¡†ä¸­ã€‚

å®ƒæœ‰å¹¿æ³›çš„åº”ç”¨ï¼Œä» OLAP æŠ¥å‘Šåˆ°æœºå™¨å­¦ä¹ ï¼ˆMLï¼‰ç®¡é“ã€‚ä¾‹å¦‚ï¼Œæˆ‘ä»¬å¯èƒ½æƒ³è¦åˆ›å»ºä¸€ä¸ªæ¨èæ¨¡å‹è®­ç»ƒä»»åŠ¡ï¼Œå¹¶éœ€è¦åƒè¿™æ ·å‡†å¤‡æ•°æ®é›†ï¼š

```py
batch_data=pd.read_table('recommendation_data.csv',chunksize=100000,sep=';',\
       names=['group','user','rating','date','id'],index_col='id',\
       header=None,parse_dates=['date'])

df=pd.DataFrame()
%time df=pd.concat(batch.groupby(['group','user',batch['date'].map(lambda x: x.year)])['rating'].agg(['sum']) for batch in batch_data)
```

è¿™æ ·ï¼ŒPandas å°†ç¡®ä¿æˆ‘ä»¬çš„åº”ç”¨ç¨‹åºå§‹ç»ˆæœ‰è¶³å¤Ÿçš„å†…å­˜æ¥å¤„ç†æ•°æ®ã€‚

## ä½¿ç”¨ joblib è¿›è¡Œç®¡é“å¤„ç†å’Œå¹¶è¡Œè®¡ç®—

`joblib.dump()`å’Œ`joblib.load()`æ–¹æ³•å…è®¸æˆ‘ä»¬é«˜æ•ˆåœ°ç®¡é“å¤§å‹æ•°æ®é›†è½¬æ¢ã€‚`joblib`å°†å­˜å‚¨å’Œåºåˆ—åŒ–å¤§æ•°æ®ï¼Œå¤„ç†ä»»æ„ Python å¯¹è±¡ï¼Œå¦‚`numpy`æ•°ç»„ã€‚

> ä½ è®¤ä¸º`scikit-learn`ç”¨ä»€ä¹ˆæ¥ä¿å­˜å’ŒåŠ è½½æœºå™¨å­¦ä¹ æ¨¡å‹ï¼Ÿæ­£ç¡®çš„ç­”æ¡ˆæ˜¯ - `joblib`ã€‚

é¦–å…ˆï¼Œä¸ºä»€ä¹ˆè¦ä¿å­˜æ¨¡å‹ï¼Ÿâ€”â€”ç®€å•æ¥è¯´ï¼Œå› ä¸ºæˆ‘ä»¬å¯èƒ½åœ¨ç®¡é“åé¢éœ€è¦å®ƒï¼Œå³ä½¿ç”¨æ–°æ•°æ®è¿›è¡Œé¢„æµ‹ç­‰ã€‚

> æˆ‘ä»¬ä¸å¸Œæœ›é‡æ–°è®­ç»ƒæˆ‘ä»¬çš„æœºå™¨å­¦ä¹ æ¨¡å‹ï¼Œå› ä¸ºè¿™æ˜¯ä¸€ä¸ªéå¸¸è€—æ—¶çš„ä»»åŠ¡ã€‚

å¦ä¸€ä¸ªåŸå› æ˜¯æˆ‘ä»¬å¯èƒ½å¸Œæœ›ä¿å­˜ç›¸åŒæ¨¡å‹çš„ä¸åŒç‰ˆæœ¬ï¼Œä»¥ä¾¿æŸ¥çœ‹å“ªä¸ªç‰ˆæœ¬è¡¨ç°æ›´å¥½ã€‚`joblib`æœ‰åŠ©äºå®Œæˆæ‰€æœ‰è¿™äº›å·¥ä½œ[5]ï¼š

```py
from joblib import dump, load
import os
import numpy as np
import joblib

filename = os.path.join(savedir, 'test.joblib')
to_persist = [('foo', [1, 2, 3]), ('bar', np.arange(5))]

# Save a model
joblib.dump(to_persist, filename)  
# ['...test.joblib']

# Load a model
joblib.load(filename)
# [('foo', [1, 2, 3]), ('bar', array([0, 1, 2, 3, 4]))]
```

è¿™äº›å‡½æ•°æ˜ç¡®è¿æ¥äº†æˆ‘ä»¬åœ¨ç£ç›˜ä¸Šä¿å­˜çš„æ–‡ä»¶å’ŒåŸå§‹ Python å¯¹è±¡çš„æ‰§è¡Œä¸Šä¸‹æ–‡ã€‚å› æ­¤ï¼Œé™¤äº†æ–‡ä»¶åï¼Œ`joblib`è¿˜æ¥å—æ–‡ä»¶å¯¹è±¡ï¼š

```py
 # WRITE
with open(filename, 'wb') as fo:
   joblib.dump(model, fo)

# READ
with open(filename, 'rb') as fo:  
   model = joblib.load(fo)
```

**AWS S3 æ¨¡å‹è½¬å‚¨/åŠ è½½ç¤ºä¾‹ï¼š**

```py
import tempfile
import boto3
import joblib

s3_client = boto3.client('s3')
bucket_name = "my-bucket"
key = "model.pkl"

# WRITE
with tempfile.TemporaryFile() as fp:
    joblib.dump(model, fp)
    fp.seek(0)
    s3_client.put_object(Body=fp.read(), Bucket=bucket_name, Key=key)

# READ
with tempfile.TemporaryFile() as fp:
    s3_client.download_fileobj(Fileobj=fp, Bucket=bucket_name, Key=key)
    fp.seek(0)
    model = joblib.load(fp)

# DELETE
s3_client.delete_object(Bucket=bucket_name, Key=key)
```

## ä½¿ç”¨ joblib è¿›è¡Œå¹¶è¡Œè®¡ç®—

è¿™éå¸¸é«˜æ•ˆï¼Œå› ä¸ºå®ƒä¾èµ–äºå¤šè¿›ç¨‹ï¼Œå¹¶ä¸”ä¼šä½¿ç”¨å¤šä¸ª Python å·¥ä½œè€…åœ¨æ‰€æœ‰ CPU æ ¸å¿ƒä¸Šæˆ–è·¨å¤šå°æœºå™¨å¹¶å‘æ‰§è¡Œä»»åŠ¡ã€‚è€ƒè™‘è¿™ä¸ªä¾‹å­ï¼š

```py
import time 
from joblib import Parallel,delayed 
import math 

t1 = time.time() 

# Normal 
results = [math.factorial(int(math.sqrt(i**3))) for i in range(1000,2000)] 

t2 = time.time() 

print('\nComputing time {:.2f} s'
      .format(t2 - t1))

# Using all CPU cores
t1 = time.time()
results = Parallel(n_jobs=-1)(delayed(math.factorial) (int(math.sqrt(i**3))) for i in range(1000,2000)) 

t2 = time.time()
print('\nComputing time {:.2f} s'
      .format(t2 - t1))
```

> æˆ‘ä»¬å¯ä»¥åˆ©ç”¨æ‰€æœ‰ CPU æ ¸å¿ƒæ¥é‡Šæ”¾ç¡¬ä»¶çš„å…¨éƒ¨æ½œåŠ›ã€‚

åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å‘Šè¯‰`Parallel`ä½¿ç”¨æ‰€æœ‰æ ¸å¿ƒï¼ˆ-1ï¼‰ï¼Œè®¡ç®—é€Ÿåº¦**æé«˜äº† 5 å€ï¼š**

```py
# The output:
Computing time 59.67 s

Computing time 12.18 s
```

## å•å…ƒæµ‹è¯• ETL ç®¡é“

åœ¨æˆ‘æ•´ä¸ªæ•°æ®å·¥ç¨‹å¸ˆèŒä¸šç”Ÿæ¶¯ä¸­ï¼Œæˆ‘å­¦åˆ°çš„æœ€é‡è¦çš„ä¸€ç‚¹æ˜¯æ‰€æœ‰ä¸œè¥¿éƒ½å¿…é¡»è¿›è¡Œå•å…ƒæµ‹è¯•ã€‚è¿™ä¸ä»…åŒ…æ‹¬**SQL**ï¼Œè¿˜åŒ…æ‹¬**ETL ä½œä¸š**å’Œä¸æˆ‘ä»¬æ•°æ®ç®¡é“ä¸­ä½¿ç”¨çš„å…¶ä»–æœåŠ¡çš„**é›†æˆ**ã€‚

æˆ‘ä»¬å¯ä»¥ä½¿ç”¨`unittest` Python åº“æ¥æµ‹è¯•æˆ‘ä»¬çš„ä»£ç ã€‚å‡è®¾æˆ‘ä»¬æœ‰ä¸€ä¸ªåŠ©æ‰‹æ¨¡å—ï¼Œç”¨äºæ£€æŸ¥ä¸€ä¸ªæ•°å­—æ˜¯å¦æ˜¯ç´ æ•°ï¼š

```py
# ./prime.py
import math

def is_prime(num):
    '''Check if num is prime or not.
    '''
    for i in range(2,int(math.sqrt(num))+1):
        if num%i==0:
            return False
    return True
```

> æˆ‘ä»¬å¦‚ä½•æµ‹è¯•è¿™ä¸ªå‡½æ•°å†…éƒ¨çš„é€»è¾‘ï¼Ÿ

`unittest`ä½¿è¿™ä¸€åˆ‡å˜å¾—éå¸¸ç®€å•ï¼š

```py
# ./test.py
import unittest
from prime import is_prime

class TestPrime(unittest.TestCase):

    def test_thirteen(self):
        self.assertTrue(is_prime(13))
```

ç°åœ¨å¦‚æœæˆ‘ä»¬åœ¨å‘½ä»¤è¡Œä¸­è¿è¡Œè¿™ä¸ªï¼Œæˆ‘ä»¬å°†æµ‹è¯•é€»è¾‘ï¼š

```py
python -m unittest test.py
# Output:
# .
# ----------------------------------------------------------------------
# Ran 1 test in 0.000s

# OK
```

è¿™æ˜¯æ­£ç¡®çš„ï¼Œå› ä¸º 13 æ˜¯ä¸€ä¸ªç´ æ•°ã€‚è®©æˆ‘ä»¬è¿›ä¸€æ­¥æµ‹è¯•ä¸€ä¸‹ã€‚æˆ‘ä»¬çŸ¥é“ 4 ä¸æ˜¯ä¸€ä¸ªç´ æ•°ï¼Œå› æ­¤æˆ‘ä»¬å¸Œæœ›é’ˆå¯¹è¿™ä¸ªç‰¹å®šå‡½æ•°çš„å•å…ƒæµ‹è¯•åœ¨æ–­è¨€ä¸º False æ—¶è¿”å›é€šè¿‡ï¼š

```py
# ./test.py
import unittest
from prime import is_prime

class TestPrime(unittest.TestCase):

    def test_thirteen(self):
        self.assertTrue(is_prime(13))
    def test_four(self):
        self.assertFalse(is_prime(4))
```

```py
python -m unittest test.py
# Output:
# ..
# ----------------------------------------------------------------------
# Ran 2 tests in 0.000s

# OK
```

å¾ˆç®€å•ã€‚è®©æˆ‘ä»¬çœ‹ä¸€ä¸ªæ›´é«˜çº§çš„ç¤ºä¾‹ã€‚

> è®©æˆ‘ä»¬å‡è®¾æˆ‘ä»¬æœ‰ä¸€ä¸ª ETL æœåŠ¡ï¼Œä»æŸä¸ª API ä¸­æå–æ•°æ®ï¼Œè¿™éœ€è¦å¾ˆå¤šæ—¶é—´ã€‚ç„¶åæˆ‘ä»¬çš„æœåŠ¡å°†è½¬æ¢è¿™ä¸ªæ•°æ®é›†ï¼Œæˆ‘ä»¬å¸Œæœ›æµ‹è¯•è¿™ä¸ª ETL è½¬æ¢é€»è¾‘æ˜¯å¦æŒç»­å­˜åœ¨ã€‚
> 
> æˆ‘ä»¬è¯¥æ€ä¹ˆåšå‘¢ï¼Ÿ

æˆ‘ä»¬å¯ä»¥ä½¿ç”¨`unittest`åº“ä¸­çš„ mock å’Œ patch æ–¹æ³•ã€‚è€ƒè™‘è¿™ä¸ªåº”ç”¨ç¨‹åºæ–‡ä»¶`asteroids.py`

```py
# ./asteroids.py
import requests
API_KEY="fsMlsu69Y7KdMNB4P2m9sqIpw5TGuF9IuYkhURzW"
ASTEROIDS_API_URL="https://api.nasa.gov/neo/rest/v1/feed"

def get_data():
    print('Fetching data from NASA Asteroids API...')
    session = requests.Session()
    url=ASTEROIDS_API_URL
    apiKey=API_KEY
    requestParams = {
        'api_key': apiKey,
        'start_date': '2023-04-20',
        'end_date': '2023-04-21'
    }
    response = requests.get(url, params = requestParams)
    print(response.status_code)
    near_earth_objects = (response.json())['near_earth_objects']
    return near_earth_objects

def save_data():
    # Do some ETL here
    data = get_data()
    return data

print(save_data())
```

å¦‚æœæˆ‘ä»¬è¿è¡Œ app.pyï¼Œè¾“å‡ºå°†ä¼šæ˜¯åˆ—å‡ºåœ¨ç‰¹å®šæ—¥æœŸæ¥è¿‘åœ°çƒçš„å°è¡Œæ˜Ÿï¼š

```py
# python ./asteroids.py

Fetching data from NASA Asteroids API...
200
{'2023-04-20': [{'links': {'self': 'http://api.nasa.gov/neo/rest/v1/neo/2326291?api_key=fsMlsu69Y7KdMNB4P2m9sqIpw5TGuF9IuYkhURzW'}, 'id': '2326291', 'neo_reference_id': '2326291', 'name': '326291 (1998 HM3)', 'nasa_jpl_url': 'http://ssd.jpl.nasa.gov/sbdb.cgi?sstr=2326291', 'absolute_magnitude_h': 19.0, 'estimated_diameter': {'kilometers': {'estimated_diameter_min': 0.4212646106, 'estimated_diameter_max': 0.9419763057}, 'meters': {'estimated_diameter_min': 421.2646105562, 'estimated_diameter_max': 941.9763057186}, 'miles': {'estimated_diameter_min': 0.2617616123, 'estimated_diameter_max': 0.5853167591}, 'feet': {'estimated_diameter_min': 1382.1017848971, 'estimated_diameter_max': 3090.4735428537}}, 'is_potentially_hazardous_asteroid': False, 'close_approach_data':
....
```

ä» API ä¸­æå–æ•°æ®å¯èƒ½éœ€è¦å¾ˆå¤šæ—¶é—´ï¼Œä½†æˆ‘ä»¬å¸Œæœ›æˆ‘ä»¬çš„å•å…ƒæµ‹è¯•è¿è¡Œå¾—æ›´å¿«ã€‚æˆ‘ä»¬å¯ä»¥**æ¨¡æ‹Ÿ**ä¸€äº›å‡çš„ API å“åº”åˆ°æˆ‘ä»¬çš„`get_data()`å‡½æ•°ä¸­ï¼Œç„¶åä½¿ç”¨å®ƒæ¥æµ‹è¯• `save_data()`å‡½æ•°ä¸­çš„ ETL é€»è¾‘ï¼š

```py
# ./test_etl.py
import unittest
from asteroids import *

import unittest.mock as mock

class TestEtl(unittest.TestCase):

    def test_asteroids_etl(self): 
        with mock.patch('asteroids.get_data') as GetDataMock:
            GetDataMock.return_value = ['asteroid_1', 'asteroid_2']
            self.assertEqual(['1', '2'], save_data())
```

è¾“å‡ºå°†æ˜¯ï¼š

```py
AssertionError: Lists differ: ['1', '2'] != ['asteroid_1', 'asteroid_2']

First differing element 0:
'1'
'asteroid_1'

- ['1', '2']
+ ['asteroid_1', 'asteroid_2']

----------------------------------------------------------------------
Ran 1 test in 0.001s

FAILED (failures=1)
```

åœ¨æˆ‘ä»¬çš„å•å…ƒæµ‹è¯•ä¸­ï¼Œæˆ‘ä»¬æ›¿æ¢äº†ï¼ˆä½¿ç”¨`mock`ï¼‰`asteroids.get_data`å‡½æ•°è¿”å›çš„å€¼ï¼Œå¹¶æœŸæœ›å®ƒä»¬è¢«è½¬æ¢ä¸ºï¼ˆETLï¼‰`['asteroid_1', 'asteroid_2']`ï¼Œè€Œæˆ‘ä»¬çš„ ETL å‡½æ•°æœªèƒ½åšåˆ°è¿™ä¸€ç‚¹ã€‚å•å…ƒæµ‹è¯•å¤±è´¥äº†ã€‚

> å•å…ƒæµ‹è¯•æ˜¯éå¸¸å¼ºå¤§çš„ã€‚

å®ƒå¸®åŠ©æˆ‘ä»¬å¤„ç†åœ¨ ETL ç®¡é“ä¸­éƒ¨ç½²æ–°åŠŸèƒ½æ—¶çš„äººä¸ºé”™è¯¯ã€‚æ›´å¤šé«˜çº§ç¤ºä¾‹å¯ä»¥åœ¨æˆ‘ä¹‹å‰çš„æ•…äº‹ä¸­æ‰¾åˆ°ã€‚æˆ‘åœ¨ CI/CD ç®¡é“ä¸­éå¸¸é¢‘ç¹åœ°ä½¿ç”¨å®ƒ [6]ï¼š

[](/test-data-pipelines-the-fun-and-easy-way-d0f974a93a59?source=post_page-----f3d5db59b6dd--------------------------------) ## ä»¥æœ‰è¶£å’Œç®€å•çš„æ–¹å¼æµ‹è¯•æ•°æ®ç®¡é“

### åˆå­¦è€…æŒ‡å—ï¼šä¸ºä»€ä¹ˆå•å…ƒæµ‹è¯•å’Œé›†æˆæµ‹è¯•å¯¹ä½ çš„æ•°æ®å¹³å°å¦‚æ­¤é‡è¦

[towardsdatascience.com

## ç›‘æ§å†…å­˜ä½¿ç”¨æƒ…å†µ

æˆ‘ç»å¸¸ä½¿ç”¨æ— æœåŠ¡å™¨éƒ¨ç½² ETL å¾®æœåŠ¡ã€‚è¿™æ˜¯ä¸€ç§éå¸¸æ•´æ´ä¸”å…·æœ‰æˆæœ¬æ•ˆç›Šçš„å·¥å…·ã€‚æˆ‘éƒ¨ç½² Lambdas å’Œ Cloud Functionsï¼Œä¸å¸Œæœ›å®ƒä»¬å› å†…å­˜è¿‡å¤šè€Œè¢«è¿‡åº¦é…ç½®ã€‚

æˆ‘ä¹‹å‰åœ¨è¿™é‡Œå†™è¿‡ï¼š

[## åˆå­¦è€…çš„åŸºç¡€è®¾æ–½å³ä»£ç ](https://levelup.gitconnected.com/infrastructure-as-code-for-beginners-a4e36c805316?source=post_page-----f3d5db59b6dd--------------------------------)

### ä½¿ç”¨è¿™äº›æ¨¡æ¿åƒä¸“ä¸šäººå£«ä¸€æ ·éƒ¨ç½²æ•°æ®ç®¡é“

[levelup.gitconnected.com](https://levelup.gitconnected.com/infrastructure-as-code-for-beginners-a4e36c805316?source=post_page-----f3d5db59b6dd--------------------------------)

> ç¡®å®ï¼Œæˆ‘ä»¬ä¸ºä»€ä¹ˆè¦ç»™æˆ‘ä»¬çš„ Lambda é…ç½® 3Gb çš„å†…å­˜å¹¶æ”¯ä»˜æ›´å¤šè´¹ç”¨ï¼Œè€Œæ•°æ®å¯ä»¥åœ¨ 256Mb çš„å†…å­˜ä¸­å¤„ç†ï¼Ÿ

æœ‰å¤šç§æ–¹æ³•å¯ä»¥ç›‘æ§æˆ‘ä»¬çš„ ETL åº”ç”¨ç¨‹åºå†…å­˜ä½¿ç”¨æƒ…å†µã€‚å…¶ä¸­ä¸€ç§æœ€å—æ¬¢è¿çš„æ–¹æ³•æ˜¯`tracemalloc` [7]åº“ã€‚

å®ƒå¯ä»¥è·Ÿè¸ª Python å†…å­˜å—ï¼Œå¹¶ä»¥ï¼ˆ<current>, <peak memory>ï¼‰å­—èŠ‚æ ¼å¼è¿”å›ç»“æœã€‚è€ƒè™‘è¿™ä¸ªä¾‹å­ï¼Œä»å°è¡Œæ˜Ÿ API ä¸­æå–æ•°æ®åˆ°ä¸€ä¸ªå¤§å—ä¸­å¹¶ä¿å­˜åˆ°ç£ç›˜ï¼š

```py
# asteroids.py
import requests
import json
import tracemalloc

tracemalloc.start()

API_KEY="fsMlsu69Y7KdMNB4P2m9sqIpw5TGuF9IuYkhURzW"
ASTEROIDS_API_URL="https://api.nasa.gov/neo/rest/v1/feed"

@profile
def get_data():
    print('Fetching data from NASA Asteroids API...')
    session = requests.Session()
    url=ASTEROIDS_API_URL
    apiKey=API_KEY
    requestParams = {
        'api_key': apiKey,
        'start_date': '2023-04-20',
        'end_date': '2023-04-27'
    }
    response = requests.get(url, params = requestParams).text
    with open('out.csv', 'w') as fd:
        fd.write(response)

get_data()

print(tracemalloc.get_traced_memory())

tracemalloc.stop()
```

è¾“å‡ºå°†æ˜¯ï¼š

```py
Fetching data from NASA Asteroids API...
(85629, 477039)
```

> **æˆ‘ä»¬å¯ä»¥çœ‹åˆ°å³°å€¼ä½¿ç”¨é‡çº¦ä¸º 540Kbã€‚**

è®©æˆ‘ä»¬çœ‹çœ‹å¦‚ä½•é€šè¿‡ä½¿ç”¨`stream`æ¥è¿›è¡Œç®€å•çš„ä¼˜åŒ–ï¼š

```py
# asteroids_stream.py
import requests
import json
import tracemalloc

tracemalloc.start()

API_KEY="fsMlsu69Y7KdMNB4P2m9sqIpw5TGuF9IuYkhURzW"
ASTEROIDS_API_URL="https://api.nasa.gov/neo/rest/v1/feed"

def get_data():
    print('Fetching data from NASA Asteroids API...')
    session = requests.Session()
    url=ASTEROIDS_API_URL
    apiKey=API_KEY
    requestParams = {
        'api_key': apiKey,
        'start_date': '2023-04-20',
        'end_date': '2023-04-27'
    }
    response = session.get(url, params = requestParams, stream = True)
    print('Saving to disk...')
    with open('out.csv', 'wb') as fd:
        for chunk in response.iter_content(chunk_size=1024):
            fd.write(chunk)

get_data()

print(tracemalloc.get_traced_memory())

tracemalloc.stop()
```

```py
# asteroids_stream.py
Fetching data from NASA Asteroids API...
Saving to disk...
(85456, 215260)
```

**æˆ‘ä»¬å¯ä»¥çœ‹åˆ°å³°å€¼å†…å­˜ä½¿ç”¨é‡å‡å°‘äº†ä¸€åŠã€‚**

## ä½¿ç”¨ SDK

ä½œä¸ºæ•°æ®å·¥ç¨‹å¸ˆï¼Œæˆ‘ä»¬é€šå¸¸éœ€è¦ä¸äº‘æœåŠ¡æä¾›å•†é¢‘ç¹åˆä½œã€‚ç®€è€Œè¨€ä¹‹ï¼ŒSDK æ˜¯ä¸€ç»„æœåŠ¡åº“ï¼Œå…è®¸ä»¥ç¼–ç¨‹æ–¹å¼è®¿é—®äº‘æœåŠ¡ã€‚æˆ‘ä»¬å¸Œæœ›å­¦ä¹ å¹¶æŒæ¡å¸‚åœºé¢†å¯¼è€…å¦‚ Amazonã€Azure æˆ– Google çš„ä¸€ä¸¤ä¸ª SDKã€‚

æˆ‘ç»å¸¸ä»¥ç¼–ç¨‹æ–¹å¼è®¿é—®çš„æœåŠ¡ä¹‹ä¸€æ˜¯ Cloud Storageã€‚å®é™…ä¸Šï¼Œåœ¨æ•°æ®å·¥ç¨‹ä¸­ï¼Œå‡ ä¹æ¯ä¸ªæ•°æ®ç®¡é“éƒ½ä¾èµ–äºäº‘ä¸­çš„æ•°æ®å­˜å‚¨ï¼Œå³ Google Cloud Storage æˆ– AWS S3ã€‚

æœ€å¸¸è§çš„æ•°æ®ç®¡é“è®¾è®¡æ˜¯å›´ç»•æ•°æ®å­˜å‚¨æ¡¶åˆ›å»ºçš„ã€‚æˆ‘åœ¨ä¹‹å‰çš„æ•…äº‹ä¸­æè¿°äº†è¿™ä¸€æ¨¡å¼ [9]ã€‚

[](/data-pipeline-design-patterns-100afa4b93e3?source=post_page-----f3d5db59b6dd--------------------------------) ## æ•°æ®ç®¡é“è®¾è®¡æ¨¡å¼

### é€‰æ‹©åˆé€‚çš„æ¶æ„åŠç¤ºä¾‹

[towardsdatascience.com

åœ¨äº‘å­˜å‚¨ä¸­åˆ›å»ºçš„å¯¹è±¡å¯ä»¥è§¦å‘å…¶ä»– ETL æœåŠ¡ã€‚è¿™åœ¨ä½¿ç”¨è¿™äº›äº‹ä»¶ç¼–æ’æ•°æ®ç®¡é“æ—¶å˜å¾—éå¸¸æœ‰ç”¨ã€‚

åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å¸Œæœ›èƒ½å¤Ÿåœ¨ç”¨ä½œæ•°æ®å¹³å°çš„æ•°æ®æ¹–çš„äº‘å­˜å‚¨ä¸­è¯»å–å’Œå†™å…¥æ•°æ®ã€‚

![](img/851620512cf55064bfb03114e16ea2ab.png)

å…¸å‹çš„æ•°æ®ç®¡é“ã€‚ä½œè€…æä¾›çš„å›¾åƒ

åœ¨è¿™ä¸ªå›¾ç¤ºä¸­ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°æˆ‘ä»¬é¦–å…ˆå°†æ•°æ®æå–å¹¶ä¿å­˜åˆ°æ•°æ®æ¹–å­˜å‚¨æ¡¶ä¸­ã€‚ç„¶åï¼Œå®ƒå°†è§¦å‘æ•°æ®ä»“åº“çš„æ•°æ®æ‘„å–ï¼Œå¹¶å°†æ•°æ®åŠ è½½åˆ°æˆ‘ä»¬çš„è¡¨ä¸­ï¼Œä»¥ä¾¿ä½¿ç”¨å•†ä¸šæ™ºèƒ½ï¼ˆBIï¼‰å·¥å…·è¿›è¡Œ OLAP åˆ†æã€‚

ä¸‹é¢çš„ä»£ç ç‰‡æ®µè§£é‡Šäº†å¦‚ä½•ä½¿ç”¨ AWS SDK ä»¥æµçš„å½¢å¼ä¿å­˜æ•°æ®ã€‚

```py
# nasa.py
import boto3
import requests
import os
S3_DATA = os.environ['S3_DATA_BUCKET'] #"your.datalake.bucket"
API_KEY="fsMlsu69Y7KdMNB4P2m9sqIpw5TGuF9IuYkhURzW"
ASTEROIDS_API_URL="https://api.nasa.gov/neo/rest/v1/feed"

print('Fetching data from NASA Asteroids API...')
session = requests.Session()
url=ASTEROIDS_API_URL
apiKey=API_KEY
requestParams = {
    'api_key': apiKey,
    'start_date': '2023-04-20',
    'end_date': '2023-04-21'
}
response = session.get(url, params = requestParams, stream=True)
print(response.status_code)
# Perform Multi-part upload to AWS S3 datalake:
s3_bucket = S3_DATA # i.e. 'data.staging.aws'
s3_file_path = 'nasa/test_nasa_.csv' # i.e. "path_in_s3"
s3 = boto3.client('s3')
print('Saving to S3\. Run to download: aws s3 cp s3://{}/{} ./'.format(s3_bucket,s3_file_path))
with response as part:
    part.raw.decode_content = True
    conf = boto3.s3.transfer.TransferConfig(multipart_threshold=10000, max_concurrency=4)
    s3.upload_fileobj(part.raw, s3_bucket, s3_file_path, Config=conf)
```

åœ¨ä½ çš„å‘½ä»¤è¡Œä¸­è¿è¡Œä»¥ä¸‹å‘½ä»¤ä»¥ä» NASA API æå–å°è¡Œæ˜Ÿæ•°æ®ï¼š

```py
S3_DATA_BUCKET="your.staging.databucket" python nasa.py
# Output:
# Fetching data from NASA Asteroids API...
# 200
# Saving to S3\. Run to download: aws s3 cp s3://your.staging.databucket/nasa/test_nasa_.csv ./
```

## ç»“è®º

è¿™ä¸ªæ•…äº‹æ€»ç»“äº†æˆ‘åœ¨ ETL æœåŠ¡ä¸­å‡ ä¹æ¯å¤©ä½¿ç”¨çš„ Python ä»£ç æŠ€æœ¯ã€‚æˆ‘å¸Œæœ›ä½ ä¹Ÿèƒ½å‘ç°å®ƒæœ‰ç”¨ã€‚å®ƒæœ‰åŠ©äºä¿æŒä»£ç çš„æ•´æ´ï¼Œå¹¶é«˜æ•ˆåœ°æ‰§è¡Œæ•°æ®ç®¡é“è½¬æ¢ã€‚æ— æœåŠ¡å™¨åº”ç”¨æ¨¡å‹æ˜¯ä¸€ä¸ªéå¸¸å…·æœ‰æˆæœ¬æ•ˆç›Šçš„æ¡†æ¶ï¼Œæˆ‘ä»¬å¯ä»¥åœ¨å…¶ä¸­éƒ¨ç½²å‡ ä¹ä¸èŠ±è´¹ä»»ä½•è´¹ç”¨çš„ ETL å¾®æœåŠ¡ã€‚æˆ‘ä»¬åªéœ€è¦ä¼˜åŒ–å†…å­˜ä½¿ç”¨ï¼Œå¹¶ä»¥åŸå­æ–¹å¼éƒ¨ç½²å®ƒä»¬ï¼Œä»¥ä¾¿å®ƒä»¬è¿è¡Œå¾—æ›´å¿«ã€‚å®ƒå‡ ä¹å¯ä»¥å¤„ç†æˆ‘ä»¬æ•°æ®å¹³å°ä¸­çš„ä»»ä½•ç±»å‹çš„æ•°æ®ç®¡é“ã€‚åœ¨æˆ‘ä¹‹å‰çš„æ•…äº‹ä¸­å¯ä»¥æ‰¾åˆ°è¿™äº›æ¶æ„ç±»å‹å’Œè®¾è®¡æ¨¡å¼çš„è‰¯å¥½æ€»ç»“ã€‚

[](/data-platform-architecture-types-f255ac6e0b7?source=post_page-----f3d5db59b6dd--------------------------------) ## æ•°æ®å¹³å°æ¶æ„ç±»å‹

### å®ƒåœ¨å¤šå¤§ç¨‹åº¦ä¸Šæ»¡è¶³ä½ çš„ä¸šåŠ¡éœ€æ±‚ï¼Ÿé€‰æ‹©çš„å›°å¢ƒã€‚

[towardsdatascience.com

ç†è§£åŸºæœ¬çš„ HTTP æ–¹æ³•åœ¨æ•°æ®å·¥ç¨‹ä¸­è‡³å…³é‡è¦ï¼Œå®ƒæœ‰åŠ©äºä¸ºæˆ‘ä»¬çš„æ•°æ®ç®¡é“åˆ›å»ºç¨³å¥çš„ API äº¤äº’ã€‚ä½¿ç”¨`joblib`å¯¹æˆ‘ä»¬çš„å‡½æ•°å’Œæ¨¡å‹è¿›è¡Œç®¡é“åŒ–å¯ä»¥ç¼–å†™å¿«é€Ÿé«˜æ•ˆçš„ä»£ç ã€‚é€šè¿‡æµçš„æ–¹å¼ä» API æ‹‰å–æ•°æ®ï¼Œå¹¶ä»¥å†…å­˜é«˜æ•ˆçš„æ–¹å¼è¿è¡Œ ETL ä»»åŠ¡ï¼Œå¯ä»¥é˜²æ­¢èµ„æºè¿‡åº¦åˆ†é…ï¼Œå¹¶ç¡®ä¿æˆ‘ä»¬çš„æ•°æ®æœåŠ¡ä¸ä¼šè€—å°½å†…å­˜ã€‚å¯ä»¥ä½¿ç”¨ CI/CD å·¥å…·æŒç»­è¿è¡Œå•å…ƒæµ‹è¯•ï¼Œè¿™æœ‰åŠ©äºåœ¨æˆ‘ä»¬çš„ä»£ç æ›´æ”¹è¾¾åˆ°ç”Ÿäº§ç¯å¢ƒä¹‹å‰ï¼ŒåŠæ—©å‘ç°é”™è¯¯å’Œäººä¸ºå¤±è¯¯ã€‚å¸Œæœ›ä½ å–œæ¬¢é˜…è¯»è¿™ç¯‡æ–‡ç« ã€‚

## æ¨èé˜…è¯»ï¼š

[1] [`stackoverflow.com/questions/519633/lazy-method-for-reading-big-file-in-python`](https://stackoverflow.com/questions/519633/lazy-method-for-reading-big-file-in-python)

[2] [`docs.python.org/3/library/functions.html#property`](https://docs.python.org/3/library/functions.html#property)

[3] [`stackoverflow.com/questions/60343944/how-does-requests-stream-true-option-streams-data-one-block-at-a-time`](https://stackoverflow.com/questions/60343944/how-does-requests-stream-true-option-streams-data-one-block-at-a-time)

[4] [`cloud.google.com/dataform/reference/rest/v1beta1/projects.locations.repositories.workflowConfigs/create`](https://cloud.google.com/dataform/reference/rest/v1beta1/projects.locations.repositories.workflowConfigs/create)

[5] [`joblib.readthedocs.io/en/stable/persistence.html#persistence`](https://joblib.readthedocs.io/en/stable/persistence.html#persistence)

[6] [`medium.com/towards-data-science/test-data-pipelines-the-fun-and-easy-way-d0f974a93a59`](https://medium.com/towards-data-science/test-data-pipelines-the-fun-and-easy-way-d0f974a93a59)

[7] [`docs.python.org/3/library/tracemalloc.html`](https://docs.python.org/3/library/tracemalloc.html)

[8] [`levelup.gitconnected.com/infrastructure-as-code-for-beginners-a4e36c805316`](https://levelup.gitconnected.com/infrastructure-as-code-for-beginners-a4e36c805316)

[9] [`medium.com/towards-data-science/data-pipeline-design-patterns-100afa4b93e3`](https://medium.com/towards-data-science/data-pipeline-design-patterns-100afa4b93e3)
