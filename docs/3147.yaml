- en: Time Series Augmentations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ÂéüÊñáÔºö[https://towardsdatascience.com/time-series-augmentations-16237134b29b?source=collection_archive---------0-----------------------#2023-10-19](https://towardsdatascience.com/time-series-augmentations-16237134b29b?source=collection_archive---------0-----------------------#2023-10-19)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: A simple yet effective way to increase the amount of time series data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@an231?source=post_page-----16237134b29b--------------------------------)[![Alexander
    Nikitin](../Images/decc36cddc4c7a23952569e293e7d209.png)](https://medium.com/@an231?source=post_page-----16237134b29b--------------------------------)[](https://towardsdatascience.com/?source=post_page-----16237134b29b--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----16237134b29b--------------------------------)
    [Alexander Nikitin](https://medium.com/@an231?source=post_page-----16237134b29b--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ¬∑
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F5afaa29ee25a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftime-series-augmentations-16237134b29b&user=Alexander+Nikitin&userId=5afaa29ee25a&source=post_page-5afaa29ee25a----16237134b29b---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----16237134b29b--------------------------------)
    ¬∑8 min read¬∑Oct 19, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F16237134b29b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftime-series-augmentations-16237134b29b&user=Alexander+Nikitin&userId=5afaa29ee25a&source=-----16237134b29b---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F16237134b29b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftime-series-augmentations-16237134b29b&source=-----16237134b29b---------------------bookmark_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: '*This blog post is available as a* [*jupyter notebook on GitHub*](https://github.com/AlexanderVNikitin/tsgm/blob/main/tutorials/augmentations.ipynb)*.*'
  prefs: []
  type: TYPE_NORMAL
- en: Augmentations have become an indispensable component in the realm of computer
    vision pipelines. However, their popularity hasn‚Äôt reached the same heights in
    other domains, such as time series. In this tutorial, I will delve into the world
    of time series augmentations, shedding light on their significance and providing
    concrete examples of their application using the powerful generative time series
    modeling library, TSGM [5].
  prefs: []
  type: TYPE_NORMAL
- en: Our starting point is a dataset denoted (ùêó, ùê≤). Here, ùê±·µ¢ ‚àà ùêó are multivariate
    (meaning, each time point is a multiple dimensional feature vector) time series,
    and y are labels. Predicting labels y is called a downstream task. Our goal is
    to use (ùêó, ùê≤) to produce additional samples (ùêó*, ùê≤*), which could help us solve
    the downstream task more effectively (in terms of predictive performance or robustness).
    For simplicity, we won‚Äôt work with labels in this tutorial, but the methods we
    describe here are straightforward to generalize to the case with labels, and the
    software implementations we use are easily extended to the supervised case by
    adding additional parameters to the `.generate` method (see examples below).
  prefs: []
  type: TYPE_NORMAL
- en: Without further ado, let‚Äôs consider time series augmentations one by one.
  prefs: []
  type: TYPE_NORMAL
- en: In TSGM, all augmentations are neatly organized in `tsgm.models.augmentations`,
    and you can check out the comprehensive documentation available at [TSGM documentation](https://tsgm.readthedocs.io/en/latest/guides/introduction.html#augmentations).
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let‚Äôs kickstart coding examples by installing tsgm:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Moving forward, we import tsgm, and load an exemplary dataset. A tensor `X`
    now contains 100 sine time series of length 64, with 2 features each. With random
    shift, frequencies, and amplitudes (maximum amplitude is 20).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Jittering / Gaussian noise
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As the first augmentation we consider jittering.
  prefs: []
  type: TYPE_NORMAL
- en: Time series data are augmented with random Gaussian noise ([Wikipedia](https://en.wikipedia.org/wiki/Gaussian_noise)))
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/44c18aa6fd22c7037051e5a7cefa7519.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In tsgm, Gaussian noise augmentation can be applied as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: The idea behind Gaussian noise augmentation is that adding a small amount of
    jittering to time series probably will not change it significantly but will increase
    the amount of such noisy samples in our dataset. It often makes the downstream
    models more robust to noisy samples or improves predictive performance.
  prefs: []
  type: TYPE_NORMAL
- en: The hyperparameters of Gaussian noise and the way of adding the noise (e.g.,
    Gaussian noise can increase towards the end of a time series) is a difficult question
    and depends on a particular dataset and downstream problem. It is often worth
    experimenting and seeing how those parameters affect the performance of the target
    model.
  prefs: []
  type: TYPE_NORMAL
- en: Here, we provide a visualization of samples from the original sine dataset and
    augmented samples.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/bc5f06309b37e786f90ecfeff2727c18.png)'
  prefs: []
  type: TYPE_IMG
- en: Original time series and synthetic data generated via Jittering.
  prefs: []
  type: TYPE_NORMAL
- en: Shuffle Features
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Another approach to time series augmentation is simply shuffle the features.
    This approach is suitable only for particular multivariate time series, where
    they are invariant to all or particular permutations of features. For instance,
    it can be applied to time series where each feature represents same independent
    measurements from various sensors.
  prefs: []
  type: TYPE_NORMAL
- en: To explain this approach, let‚Äôs take the example of five identical sensors,
    labeled as S_1, S_2, S_3, S_4, and S_5\. For the sake of illustration, let‚Äôs assume
    that sensors 1‚Äì4 are probably exchangeable with respect to rotations. Then it
    makes sense to try augmenting data with feature rotations with respect to rotations
    of S_1, ‚Ä¶, S_5 sensors.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/33ef023a670f580bf559f0e92d317013.png)'
  prefs: []
  type: TYPE_IMG
- en: In this example, five sensors are present, and measurements from those sensors
    generate five-dimensional time series data. Sensors 1 to 4 can be arbitrarily
    rotated to generate new synthetic samples (e.g., 1->2, 2->3, 3->4, 4->1). Thus,
    by applying such transformations to original data, one can generate novel synthetic
    samples.
  prefs: []
  type: TYPE_NORMAL
- en: 'Similarly to the previous example, the augmentation can work as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Here, we show one sample from a timeseries with 5 features, and an augmented
    sample, analogously to the image above.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5c82f025b90a71ba39c5d85fa4431094.png)'
  prefs: []
  type: TYPE_IMG
- en: Original time series and synthetic data generated via shuffling features.
  prefs: []
  type: TYPE_NORMAL
- en: Slice and Shuffle
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Slice and shuffle augmentation [3] cuts a time series into slices and shuffles
    those pieces. This augmentation can be performed for time series that exhibit
    some form of invariance over time. For instance, imagine a time series measured
    from wearable devices for several days. The good strategy for this case is to
    slice time series by days and, by shuffling those days, get additional samples.
    Slice and shuffle augmentation is visualized in the following image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/fb2647d33c5989842439315ba9b28271.png)'
  prefs: []
  type: TYPE_IMG
- en: Slice and Shuffle schematic visualization.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Let‚Äôs view augmented and original samples:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/49b868a4660f4f006921f821a774773b.png)'
  prefs: []
  type: TYPE_IMG
- en: Original time series and synthetic data generated via slice and shuffle.
  prefs: []
  type: TYPE_NORMAL
- en: Magnitude Warping
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Magnitude warping [3] changes the magnitude of each sample in a time series
    dataset by multiplication of the original time series with a cubic spline curve.
    This process scales the magnitude of time series, which can be beneficial in many
    cases, such as our synthetic example with sines `n_knots` number of knots at random
    magnitudes distributed as *N(1, œÉ^2*) where *œÉ* is set by a parameter `sigma`
    in function `.generate`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Here is an example of original data and augmented samples generated with `MagnitudeWarping`.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0e3f871956ba908f9ed5e0b9726b5e43.png)'
  prefs: []
  type: TYPE_IMG
- en: Original time series and synthetic data generated via magnitude warping.
  prefs: []
  type: TYPE_NORMAL
- en: Window Warping
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this technique [4], the selected windows in time series data are either
    speeding up or down. Then, the whole resulting time series is scaled back to the
    original size in order to keep the timesteps at the original length. See an example
    of such augmentation below:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/bb3e8162230f88cfb55b95b90b7bb6e7.png)'
  prefs: []
  type: TYPE_IMG
- en: Such augmentation can be beneficial, e.g., in modeling equipment. In such applications,
    sensor measurements can change the speed of change depending on how those pieces
    of equipment are used.
  prefs: []
  type: TYPE_NORMAL
- en: In tsgm, as always, the generation can be done via
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: An example of a generated time series can be found below.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f125afd7137a500f20d74a2099619531.png)'
  prefs: []
  type: TYPE_IMG
- en: Original time series and synthetic data generated via window warping.
  prefs: []
  type: TYPE_NORMAL
- en: Dynamic Time Warping Barycentric Average (DTWBA)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Dynamic Time Warping Barycentric Average (DTWBA)[2] is an augmentation method
    that is based on Dynamic Time Warping (DTW). DTW is a method of measuring similarity
    between time series. The idea is to ‚Äúsync‚Äù those time series, as it is demonstrated
    in the following picture.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6a525a2c686cff6601fbd76b6414d4b0.png)'
  prefs: []
  type: TYPE_IMG
- en: DTW is measured for two time series signals sin(x) and sin(2x). DTW measurement
    is shown with the white line. Also, a cross-similarity matrix is visualized.
  prefs: []
  type: TYPE_NORMAL
- en: More details on DTW computation are available at [https://rtavenar.github.io/blog/dtw.html](https://rtavenar.github.io/blog/dtw.html).
  prefs: []
  type: TYPE_NORMAL
- en: 'DTWBA goes like this:'
  prefs: []
  type: TYPE_NORMAL
- en: 1\. The algorithm picks one time series to initialize the DTWBA result. This
    time series can either be given explicitly or can be chosen randomly from the
    dataset
  prefs: []
  type: TYPE_NORMAL
- en: 2\. For each of the `N` time series, the algorithm computes DTW distance and
    the path (the path is the mapping that minimizes the distance)
  prefs: []
  type: TYPE_NORMAL
- en: 3\. After computing all `N` DTW distances, the algorithm updates the DTWBA result
    by doing the average with respect to all the paths found above
  prefs: []
  type: TYPE_NORMAL
- en: 4\. The algorithm repeats steps (2) and (3) until the DTWBA result converges
  prefs: []
  type: TYPE_NORMAL
- en: A reference implementation can be found in [tslearn](https://github.com/tslearn-team/tslearn/blob/main/tslearn/barycenters/dba.py#L60),
    and a description can be found in [2].
  prefs: []
  type: TYPE_NORMAL
- en: In tsgm, the samples can be generated as follows
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/5f90e46a8f02f3b1b3b87f23a54fec58.png)'
  prefs: []
  type: TYPE_IMG
- en: Original time series and synthetic data generated via DTWBA.
  prefs: []
  type: TYPE_NORMAL
- en: Augmentation with Generative Machine Learning Models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Another approach to augmentation is to train a machine learning model on historical
    data and train it to generate novel synthetic samples. It is a blackbox method
    because it is hard to interpret how new samples were generated. Several methods
    can be applied in the case of time series; in particular, tsgm has VAE, GANs,
    and Gaussian processes. An example of the generation of synthetic time series
    with VAEs is
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We explored several methods for synthetic time series generation. Many of them
    introduce inductive biases into the model and are useful in practical settings.
  prefs: []
  type: TYPE_NORMAL
- en: How to choose? First, analyze whether your problem contains invariances. Is
    it invariant to random noise? Is it invariant to feature shuffling?
  prefs: []
  type: TYPE_NORMAL
- en: Next, choose a broad set of methods and verify whether any of the selected methods
    improve the performance of your downstream problem (tsgm has [downstream performance
    metric](https://tsgm.readthedocs.io/en/latest/autoapi/tsgm/metrics/index.html#tsgm.metrics.DownstreamPerformanceMetric)).
    Then, select the set of augmentation methods that gives the largest performance
    boost.
  prefs: []
  type: TYPE_NORMAL
- en: '*Last, but not least, I thank Letizia Iannucci and Georgy Gritsenko for help
    and useful discussions about writing of this post. Unless otherwise noted, all
    images are by the author.*'
  prefs: []
  type: TYPE_NORMAL
- en: 'This blog post is a part of the project TSGM, in which we are creating a tool
    for enhancing time series pipelines via augmentation and synthetic data generation.
    If you found it helpful, take a look at [our repo](https://github.com/AlexanderVNikitin/tsgm)
    and consider citing [the paper about TSGM](https://arxiv.org/abs/2305.11567):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[1] H. Sakoe and S. Chiba, ‚ÄúDynamic programming algorithm optimization for
    spoken word recognition‚Äù. IEEE Transactions on Acoustics, Speech, and Signal Processing,
    26(1), 43‚Äì49 (1978).'
  prefs: []
  type: TYPE_NORMAL
- en: '[2] F. Petitjean, A. Ketterlin & P. Gancarski. A global averaging method for
    dynamic time warping, with applications to clustering. Pattern Recognition, Elsevier,
    2011, Vol. 44, Num. 3, pp. 678‚Äì693'
  prefs: []
  type: TYPE_NORMAL
- en: '[3] Um TT, Pfister FM, Pichler D, Endo S, Lang M, Hirche S,'
  prefs: []
  type: TYPE_NORMAL
- en: 'Fietzek U, Kulic¬¥ D (2017) Data augmentation of wearable sensor data for parkinson‚Äôs
    disease monitoring using convolutional neural networks. In: Proceedings of the
    19th ACM international conference on multimodal interaction, pp. 216‚Äì220'
  prefs: []
  type: TYPE_NORMAL
- en: '[4] Rashid, K.M. and Louis, J., 2019\. Window-warping: a time series data augmentation
    of IMU data for construction equipment activity identification. In ISARC. Proceedings
    of the international symposium on automation and robotics in construction (Vol.
    36, pp. 651‚Äì657). IAARC Publications.'
  prefs: []
  type: TYPE_NORMAL
- en: '[5] Nikitin, A., Iannucci, L. and Kaski, S., 2023\. TSGM: A Flexible Framework
    for Generative Modeling of Synthetic Time Series. *arXiv preprint arXiv:2305.11567*.
    [Arxiv link](https://arxiv.org/abs/2305.11567).'
  prefs: []
  type: TYPE_NORMAL
