- en: Recurrent Neural Networks, Explained and Visualized from the Ground Up
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 递归神经网络，基础讲解与可视化
- en: 原文：[https://towardsdatascience.com/recurrent-neural-networks-explained-and-visualized-from-the-ground-up-51c023f2b6fe?source=collection_archive---------3-----------------------#2023-06-22](https://towardsdatascience.com/recurrent-neural-networks-explained-and-visualized-from-the-ground-up-51c023f2b6fe?source=collection_archive---------3-----------------------#2023-06-22)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/recurrent-neural-networks-explained-and-visualized-from-the-ground-up-51c023f2b6fe?source=collection_archive---------3-----------------------#2023-06-22](https://towardsdatascience.com/recurrent-neural-networks-explained-and-visualized-from-the-ground-up-51c023f2b6fe?source=collection_archive---------3-----------------------#2023-06-22)
- en: '![](../Images/0b3a777d2e37126f432db24797c1b6ba.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0b3a777d2e37126f432db24797c1b6ba.png)'
- en: '[Unsplash](https://unsplash.com/photos/rSrK-P0Wips)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[Unsplash](https://unsplash.com/photos/rSrK-P0Wips)'
- en: With an application to machine translation
  id: totrans-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 应用于机器翻译
- en: '[](https://andre-ye.medium.com/?source=post_page-----51c023f2b6fe--------------------------------)[![Andre
    Ye](../Images/c69b022a665d3ee13f9fec2f1f73bf32.png)](https://andre-ye.medium.com/?source=post_page-----51c023f2b6fe--------------------------------)[](https://towardsdatascience.com/?source=post_page-----51c023f2b6fe--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----51c023f2b6fe--------------------------------)
    [Andre Ye](https://andre-ye.medium.com/?source=post_page-----51c023f2b6fe--------------------------------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://andre-ye.medium.com/?source=post_page-----51c023f2b6fe--------------------------------)[![Andre
    Ye](../Images/c69b022a665d3ee13f9fec2f1f73bf32.png)](https://andre-ye.medium.com/?source=post_page-----51c023f2b6fe--------------------------------)[](https://towardsdatascience.com/?source=post_page-----51c023f2b6fe--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----51c023f2b6fe--------------------------------)
    [Andre Ye](https://andre-ye.medium.com/?source=post_page-----51c023f2b6fe--------------------------------)'
- en: ·
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fbe743a65b006&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frecurrent-neural-networks-explained-and-visualized-from-the-ground-up-51c023f2b6fe&user=Andre+Ye&userId=be743a65b006&source=post_page-be743a65b006----51c023f2b6fe---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----51c023f2b6fe--------------------------------)
    ·23 min read·Jun 22, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F51c023f2b6fe&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frecurrent-neural-networks-explained-and-visualized-from-the-ground-up-51c023f2b6fe&user=Andre+Ye&userId=be743a65b006&source=-----51c023f2b6fe---------------------clap_footer-----------)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fbe743a65b006&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frecurrent-neural-networks-explained-and-visualized-from-the-ground-up-51c023f2b6fe&user=Andre+Ye&userId=be743a65b006&source=post_page-be743a65b006----51c023f2b6fe---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----51c023f2b6fe--------------------------------)
    · 23分钟阅读·2023年6月22日 [](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F51c023f2b6fe&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frecurrent-neural-networks-explained-and-visualized-from-the-ground-up-51c023f2b6fe&user=Andre+Ye&userId=be743a65b006&source=-----51c023f2b6fe---------------------clap_footer-----------)'
- en: --
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F51c023f2b6fe&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frecurrent-neural-networks-explained-and-visualized-from-the-ground-up-51c023f2b6fe&source=-----51c023f2b6fe---------------------bookmark_footer-----------)'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F51c023f2b6fe&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frecurrent-neural-networks-explained-and-visualized-from-the-ground-up-51c023f2b6fe&source=-----51c023f2b6fe---------------------bookmark_footer-----------)'
- en: Recurrent Neural Networks (RNNs) are neural networks that can operate sequentially.
    Although they’re not as popular as they were even just several years ago, they
    represent an important development in the progression of deep learning and are
    a natural extension of feedforward networks.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 递归神经网络（RNN）是可以顺序操作的神经网络。尽管它们现在不如几年前那么流行，但它们仍然在深度学习的发展中代表了一个重要的进步，并且是前馈网络的自然延伸。
- en: 'In this post, we’ll cover the following:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们将涵盖以下内容：
- en: The step from feedforward to recurrent networks
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从前馈网络到递归网络的过渡
- en: Multilayer recurrent networks
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多层递归网络
- en: Long short-term memory networks (LSTMs)
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 长短期记忆网络（LSTM）
- en: Sequential output (‘text output’)
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 顺序输出（‘文本输出’）
- en: Bidirectionality
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 双向性
- en: Autoregressive generation
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自回归生成
- en: An application to machine translation (a high-level understanding of Google
    Translate’s 2016 model architecture)
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器翻译的应用（对Google Translate 2016年模型架构的高层次理解）
- en: The aim of the post is not only to explain how RNNs work (there are plenty of
    posts which do that), but to explore their design choices and high-level intuitive
    logic with the aid of illustrations. I hope this article will provide some unique
    value not only to your grasp of this particular technical topic but also more
    generally the flexibility of deep learning design.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 这篇文章的目的不仅仅是解释RNN的工作原理（这方面已有很多文章），而是通过插图深入探讨它们的设计选择和高层次的直观逻辑。希望这篇文章不仅能为你对这一特定技术话题的理解提供一些独特的价值，还能更普遍地提升你对深度学习设计灵活性的理解。
