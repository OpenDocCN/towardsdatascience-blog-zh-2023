- en: Recurrent Neural Networks, Explained and Visualized from the Ground Up
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/recurrent-neural-networks-explained-and-visualized-from-the-ground-up-51c023f2b6fe?source=collection_archive---------3-----------------------#2023-06-22](https://towardsdatascience.com/recurrent-neural-networks-explained-and-visualized-from-the-ground-up-51c023f2b6fe?source=collection_archive---------3-----------------------#2023-06-22)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/0b3a777d2e37126f432db24797c1b6ba.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
- en: '[Unsplash](https://unsplash.com/photos/rSrK-P0Wips)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: With an application to machine translation
  id: totrans-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://andre-ye.medium.com/?source=post_page-----51c023f2b6fe--------------------------------)[![Andre
    Ye](../Images/c69b022a665d3ee13f9fec2f1f73bf32.png)](https://andre-ye.medium.com/?source=post_page-----51c023f2b6fe--------------------------------)[](https://towardsdatascience.com/?source=post_page-----51c023f2b6fe--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----51c023f2b6fe--------------------------------)
    [Andre Ye](https://andre-ye.medium.com/?source=post_page-----51c023f2b6fe--------------------------------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: ·
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fbe743a65b006&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frecurrent-neural-networks-explained-and-visualized-from-the-ground-up-51c023f2b6fe&user=Andre+Ye&userId=be743a65b006&source=post_page-be743a65b006----51c023f2b6fe---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----51c023f2b6fe--------------------------------)
    ·23 min read·Jun 22, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F51c023f2b6fe&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frecurrent-neural-networks-explained-and-visualized-from-the-ground-up-51c023f2b6fe&user=Andre+Ye&userId=be743a65b006&source=-----51c023f2b6fe---------------------clap_footer-----------)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: --
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F51c023f2b6fe&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frecurrent-neural-networks-explained-and-visualized-from-the-ground-up-51c023f2b6fe&source=-----51c023f2b6fe---------------------bookmark_footer-----------)'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: Recurrent Neural Networks (RNNs) are neural networks that can operate sequentially.
    Although they’re not as popular as they were even just several years ago, they
    represent an important development in the progression of deep learning and are
    a natural extension of feedforward networks.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: 'In this post, we’ll cover the following:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: The step from feedforward to recurrent networks
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Multilayer recurrent networks
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Long short-term memory networks (LSTMs)
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sequential output (‘text output’)
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bidirectionality
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Autoregressive generation
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An application to machine translation (a high-level understanding of Google
    Translate’s 2016 model architecture)
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器翻译的应用（对Google Translate 2016年模型架构的高层次理解）
- en: The aim of the post is not only to explain how RNNs work (there are plenty of
    posts which do that), but to explore their design choices and high-level intuitive
    logic with the aid of illustrations. I hope this article will provide some unique
    value not only to your grasp of this particular technical topic but also more
    generally the flexibility of deep learning design.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 这篇文章的目的不仅仅是解释RNN的工作原理（这方面已有很多文章），而是通过插图深入探讨它们的设计选择和高层次的直观逻辑。希望这篇文章不仅能为你对这一特定技术话题的理解提供一些独特的价值，还能更普遍地提升你对深度学习设计灵活性的理解。
