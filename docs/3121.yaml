- en: Extending Context Length in Large Language Models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/extending-context-length-in-large-language-models-74e59201b51f?source=collection_archive---------0-----------------------#2023-10-15](https://towardsdatascience.com/extending-context-length-in-large-language-models-74e59201b51f?source=collection_archive---------0-----------------------#2023-10-15)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: How to turn your Llama into a Giraffe
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://donatoriccio.medium.com/?source=post_page-----74e59201b51f--------------------------------)[![Donato
    Riccio](../Images/0af2a026e72a023db4635522cbca50eb.png)](https://donatoriccio.medium.com/?source=post_page-----74e59201b51f--------------------------------)[](https://towardsdatascience.com/?source=post_page-----74e59201b51f--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----74e59201b51f--------------------------------)
    [Donato Riccio](https://donatoriccio.medium.com/?source=post_page-----74e59201b51f--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fe384fc71d292&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fextending-context-length-in-large-language-models-74e59201b51f&user=Donato+Riccio&userId=e384fc71d292&source=post_page-e384fc71d292----74e59201b51f---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----74e59201b51f--------------------------------)
    ·9 min read·Oct 15, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F74e59201b51f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fextending-context-length-in-large-language-models-74e59201b51f&user=Donato+Riccio&userId=e384fc71d292&source=-----74e59201b51f---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F74e59201b51f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fextending-context-length-in-large-language-models-74e59201b51f&source=-----74e59201b51f---------------------bookmark_footer-----------)![](../Images/8bb0da11ed7b43b5f0cc752db52b0aad.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Image by the author. (AI generated Llamas)
  prefs: []
  type: TYPE_NORMAL
- en: Context length refers to the maximum number of tokens the model can remember
    when generating text. A longer context window allows the model to understand long-range
    dependencies in text better. Models with longer contexts can build connections
    between ideas far apart in the text, generating more globally coherent outputs.
  prefs: []
  type: TYPE_NORMAL
- en: During training, the model processes the text data in chunks or fixed-length
    windows. Models need to be trained on lengthy texts to actually leverage long
    contexts. Training sequences must contain documents, books, articles, etc., with
    thousands of tokens.
  prefs: []
  type: TYPE_NORMAL
- en: The length of training data sets a limit on usable context length.
  prefs: []
  type: TYPE_NORMAL
- en: So, why don’t we train models on longer sequences?
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Not so fast.
  prefs: []
  type: TYPE_NORMAL
- en: Increasing context length increases the number of possible token combinations
    the model must learn to predict accurately.
  prefs: []
  type: TYPE_NORMAL
- en: This enables more robust long-range modeling but also require more memory and
    processing power, leading to higher training costs.
  prefs: []
  type: TYPE_NORMAL
- en: Without any optimization, computation scales quadratically with context length
    — meaning that a 4096 token model will need 64 times more…
  prefs: []
  type: TYPE_NORMAL
