- en: Decision Trees for Classification — Complete Example
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/decision-trees-for-classification-complete-example-d0bc17fcf1c2?source=collection_archive---------1-----------------------#2023-01-01](https://towardsdatascience.com/decision-trees-for-classification-complete-example-d0bc17fcf1c2?source=collection_archive---------1-----------------------#2023-01-01)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: A detailed example how to construct a Decision Tree for classification
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@pumaline?source=post_page-----d0bc17fcf1c2--------------------------------)[![Datamapu](../Images/63b0c7f9a3d160c5bb039bbebd791f7e.png)](https://medium.com/@pumaline?source=post_page-----d0bc17fcf1c2--------------------------------)[](https://towardsdatascience.com/?source=post_page-----d0bc17fcf1c2--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----d0bc17fcf1c2--------------------------------)
    [Datamapu](https://medium.com/@pumaline?source=post_page-----d0bc17fcf1c2--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ffcd72d75ae6e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdecision-trees-for-classification-complete-example-d0bc17fcf1c2&user=Datamapu&userId=fcd72d75ae6e&source=post_page-fcd72d75ae6e----d0bc17fcf1c2---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----d0bc17fcf1c2--------------------------------)
    ·8 min read·Jan 1, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fd0bc17fcf1c2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdecision-trees-for-classification-complete-example-d0bc17fcf1c2&user=Datamapu&userId=fcd72d75ae6e&source=-----d0bc17fcf1c2---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fd0bc17fcf1c2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdecision-trees-for-classification-complete-example-d0bc17fcf1c2&source=-----d0bc17fcf1c2---------------------bookmark_footer-----------)![](../Images/a45b59b3ef0c4737f791d8f710bbf118.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Photo by Fabrice Villard on Unsplash
  prefs: []
  type: TYPE_NORMAL
- en: This article explains how we can use decision trees for classification problems.
    After explaining important terms, we will develop a decision tree for a simple
    example dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[A decision tree is a decision support tool that uses a tree-like model of
    decisions and their possible consequences, including chance event outcomes, resource
    costs, and utility. It is one way to display an algorithm that only contains conditional
    control statements.](https://en.wikipedia.org/wiki/Decision_tree)'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Traditionally decision trees are drawn manually, but they can be learned using
    Machine Learning. They can be used for both regression and classification problems.
    In this article we will focus on classification problems. Let’s consider the following
    example data:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0ff178c454666a8b72804aec2f7de623.png)'
  prefs: []
  type: TYPE_IMG
- en: Example data (constructed by the author)
  prefs: []
  type: TYPE_NORMAL
- en: Using this simplified example we will predict whether a person is going to be
    an astronaut, depending on their age, whether they like dogs, and whether they
    like gravity. Before discussing how to construct a decision tree, let’s have a
    look at the resulting decision tree for our example data.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/36dbbe9e24de192c40ec14269cf0efa3.png)'
  prefs: []
  type: TYPE_IMG
- en: Final decision tree for example data
  prefs: []
  type: TYPE_NORMAL
- en: We can follow the paths to come to a decision. For example, we can see that
    a person who doesn’t like gravity is not going to be an astronaut, independent
    of the other features. On the other side, we can also see, that a person who likes
    gravity and likes dogs is going to be an astronaut independent of the age.
  prefs: []
  type: TYPE_NORMAL
- en: Before going into detail how this tree is constructed, let’s define some important
    terms.
  prefs: []
  type: TYPE_NORMAL
- en: Terms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Root Node
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The top-level node. The first decision that is taken. In our example the root
    node is ‘likes gravity’.
  prefs: []
  type: TYPE_NORMAL
- en: Branches
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Branches represent sub-trees. Our example has two branches. One branch is, e.g.
    the sub-tree from ‘likes dogs’ and the second one from ‘age < 40.5’ on.
  prefs: []
  type: TYPE_NORMAL
- en: Node
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A node represents a split into further (child) nodes. In our example the nodes
    are ‘likes gravity’, ‘likes dogs’ and ‘age < 40.5’.
  prefs: []
  type: TYPE_NORMAL
- en: Leaf
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Leafs are at the end of the branches, i.e. they don’t split any more. They represent
    possible outcomes for each action. In our example the leafs are represented by
    ‘yes’ and ‘no’.
  prefs: []
  type: TYPE_NORMAL
- en: Parent Node
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A node which precedes a (child) node is called a parent node. In our example
    ‘likes gravity’ is a parent node of ‘likes dogs’ and ‘likes dogs’ is a parent
    node of ‘age < 40.5’.
  prefs: []
  type: TYPE_NORMAL
- en: Child Node
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A node under another node is a child node. In our example ‘likes dogs’ is a
    child node of ‘likes gravity’ and ‘age < 40.5’ is a child node of ‘likes dogs’.
  prefs: []
  type: TYPE_NORMAL
- en: Splitting
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The process of dividing a node into two (child) nodes.
  prefs: []
  type: TYPE_NORMAL
- en: Pruning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Removing the (child) nodes of a parent node is called pruning. A tree is grown
    through splitting and shrunk through pruning. In our example, if we would remove
    the node ‘age < 40.5’ we would prune the tree.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1556654976019b99810dc7f15c6695ca.png)'
  prefs: []
  type: TYPE_IMG
- en: Decision tree illustration
  prefs: []
  type: TYPE_NORMAL
- en: We can also observe, that a decision tree allows us to mix data types. We can
    use numerical data (‘age’) and categorical data (‘likes dogs’, ‘likes gravity’)
    in the same tree.
  prefs: []
  type: TYPE_NORMAL
- en: Create a Decision Tree
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The most important step in creating a decision tree, is the *splitting* of
    the data. We need to find a way to split the data set (*D*) into two data sets
    (*D_1*) and (*D_2*). There are different criteria that can be used in order to
    find the next split, for an overview see e.g. [here](https://www.analyticsvidhya.com/blog/2020/06/4-ways-split-decision-tree/#:~:text=Steps%20to%20split%20a%20decision%20tree%20using%20Information%20Gain%3A,entropy%20or%20highest%20information%20gain).
    We will concentrate on one of them: the [*Gini Impurity*](https://www.learndatasci.com/glossary/gini-impurity/#:~:text=a%20simple%20dataset-,What%20is%20Gini%20Impurity%3F,nodes%20to%20form%20the%20tree.),
    which is a criterion for categorical target variables and also the criterion used
    by the Python library [scikit-learn](https://scikit-learn.org/stable/modules/tree.html#classification).'
  prefs: []
  type: TYPE_NORMAL
- en: Gini Impurity
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The Gini Impurity for a data set *D* is calculated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/dcd4ca062f20d515c6c01822250af8fa.png)'
  prefs: []
  type: TYPE_IMG
- en: with n = n_1 + n_2 the size of the data set (D) and
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/55268b5346592788b91d1ebd51acdd75.png)'
  prefs: []
  type: TYPE_IMG
- en: with *D_1* and *D_2* subsets of *D*, 𝑝_𝑗 the probability of samples belonging
    to class 𝑗 at a given node, and 𝑐 the number of classes. The lower the Gini Impurity,
    the higher is the homogeneity of the node. The Gini Impurity of a pure node is
    zero. To split a decision tree using Gini Impurity, the following steps need to
    be performed.
  prefs: []
  type: TYPE_NORMAL
- en: For each possible split, calculate the Gini Impurity of each child node
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Calculate the Gini Impurity of each split as the weighted average Gini Impurity
    of child nodes
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select the split with the lowest value of Gini Impurity
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeat steps 1–3 until no further split is possible.
  prefs: []
  type: TYPE_NORMAL
- en: To understand this better, let’s have a look at an example.
  prefs: []
  type: TYPE_NORMAL
- en: 'First Example: Decision Tree with two binary features'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Before creating the decision tree for our entire dataset, we will first consider
    a subset, that only considers two features: ‘likes gravity’ and ‘likes dogs’.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The first thing we have to decide is, which feature is going to be the *root
    node*. We do that by predicting the target with only one of the features and then
    use the feature, that has the lowest Gini Impurity as the root node. That is,
    in our case we build two shallow trees, with just the root node and two leafs.
    In the first case we use ‘likes gravity’ as a root node and in the second case
    ‘likes dogs’. We then calculate the Gini Impurity for both. The trees look like
    this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a7cbf81c91434e4974dd0fa52e21864a.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by the author
  prefs: []
  type: TYPE_NORMAL
- en: 'The Gini Impurity for these trees are calculated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Case 1:**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Dataset 1:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/079a8cfb48bd78791b73edbf569b2bd3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Dataset 2:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/39cfa1633b7fbf37959ff0cbc29f8383.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The Gini Impurity is the weighted mean of both:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/bc97ce3e7eae3f23478251857c2e9b13.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Case 2:**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Dataset 1:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/bee46fd6beab0ec0c42e071c3899739c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Dataset 2:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/220e31774ee921f5ec8ecd3b0865c8a8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The Gini Impurity is the weighted mean of both:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7b00d2fb5b4c651548228a07a25bdcd0.png)'
  prefs: []
  type: TYPE_IMG
- en: That is, the first case has lower Gini Impurity and is the chosen split. In
    this simple example, only one feature remains, and we can build the final decision
    tree.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/74ab2bd34dbd66e8504ea62a5a88a2b2.png)'
  prefs: []
  type: TYPE_IMG
- en: Final Decision Tree considering only the features ‘likes gravity’ and ‘likes
    dogs’
  prefs: []
  type: TYPE_NORMAL
- en: 'Second Example: Add a numerical Variable'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Until now, we considered only a subset of our data set - the categorical variables.
    Now we will add the numerical variable ‘age’. The criterion for splitting is the
    same. We already know the Gini Impurities for ‘likes gravity’ and ‘likes dogs’.
    The calculation for the Gini Impurity of a numerical variable is similar, however
    the decision takes more calculations. The following steps need to be done
  prefs: []
  type: TYPE_NORMAL
- en: Sort the data frame by the numerical variable (‘age’)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Calculate the mean of neighbouring values
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Calculate the Gini Impurity for all splits for each of these means
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This is again our data, sorted by age, and the mean of neighbouring values is
    given on the left-hand side.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d825cb583abd383d6979f1e7996dddaf.png)'
  prefs: []
  type: TYPE_IMG
- en: The data set sorted by age. The left hand side shows the mean of neighbouring
    values for age.
  prefs: []
  type: TYPE_NORMAL
- en: We then have the following possible splits.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/260539e5a4dd3b1394dc0e5015ae71d8.png)'
  prefs: []
  type: TYPE_IMG
- en: Possible splits for age and their Gini Imputity.
  prefs: []
  type: TYPE_NORMAL
- en: We can see that the Gini Impurity of all possible ‘age’ splits is higher than
    the one for ‘likes gravity’ and ‘likes dogs’. The lowest Gini Impurity is, when
    using ‘likes gravity’, i.e. this is our *root node* and the first split.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/cd3c81e701f1cdd6bf1fe61bddf6ce7d.png)'
  prefs: []
  type: TYPE_IMG
- en: The first split of the tree. ‘likes gravity’ is the root node.
  prefs: []
  type: TYPE_NORMAL
- en: 'The subset Dataset 2 is already pure, that is, this node is a *leaf* and no
    further splitting is necessary. The *branch* on the left-hand side, Dataset 1
    is not pure and can be split further. We do this in the same way as before: We
    calculate the Gini Impurity for each feature: ‘likes dogs’ and ‘age’.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/70d756f4890d3d097c10e1e251c8af18.png)'
  prefs: []
  type: TYPE_IMG
- en: Possible splits for Dataset 2.
  prefs: []
  type: TYPE_NORMAL
- en: We see that the lowest Gini Impurity is given by the split “likes dogs”. We
    now can build our final tree.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/36dbbe9e24de192c40ec14269cf0efa3.png)'
  prefs: []
  type: TYPE_IMG
- en: Final Decision Tree.
  prefs: []
  type: TYPE_NORMAL
- en: Using Python
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In Python, we can use the scikit-learn method [DecisionTreeClassifier](https://scikit-learn.org/stable/modules/tree.html#classification)
    for building a Decision Tree for classification. Note, that scikit-learn also
    provides [DecisionTreeRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html#sklearn.tree.DecisionTreeRegressor),
    a method for using Decision Trees for Regression. Assume that our data is stored
    in a data frame ‘df’, we then can train it using the ‘fit’ method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: We can visualize the resulting tree using the ‘*plot_tree*’ method. It is the
    same as we built, only the splitting criteria is named with ‘<=’ instead of ‘<’,
    and the ‘true’ and ‘false’ paths go to the other direction. That is, there are
    some differences in the appearance.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/4acd653ec0c964848a0e2b58da27225f.png)'
  prefs: []
  type: TYPE_IMG
- en: Resulting Decision Tree using scikit-learn.
  prefs: []
  type: TYPE_NORMAL
- en: Advantages and Disadvantages of Decision Trees
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When working with decision trees, it is important to know their advantages and
    disadvantages. Below you can find a list of pros and cons. This list, however,
    is by no means complete.
  prefs: []
  type: TYPE_NORMAL
- en: Pros
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Decision trees are intuitive, easy to understand and interpret.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Decision trees are not effected by outliers and missing values.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The data doesn’t need to be scaled.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Numerical and categorical data can be combined.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Decision trees are non-parametric algorithms.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cons
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Overfitting is a common problem. Pruning may help to overcome this.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Although decision trees can be used for regression problems, they cannot really
    predict continuous variables as the predictions must be separated in categories.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training a decision tree is relatively expensive.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this article, we discussed a simple but detailed example of how to construct
    a decision tree for a classification problem and how it can be used to make predictions.
    A crucial step in creating a decision tree is to find the best split of the data
    into two subsets. A common way to do this is the Gini Impurity. This is also used
    in the scikit-learn library from Python, which is often used in practice to build
    a Decision Tree. It’s important to keep in mind the limitations of decision trees,
    of which the most prominent one is the tendency to overfit.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Chris Nicholson, Decision Trees (2020), pathmind — A.I. Wiki, A Beginner’s Guide
    to Important Topics in AI, Machine Learning, and Deep https://wiki.pathmind.com/decision-tree.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Abhishek Sharma, [4 Simple Ways to Split a Decision Tree in Machine LearningOverview
    over splitting methods](https://www.analyticsvidhya.com/blog/2020/06/4-ways-split-decision-tree/#:~:text=Steps%20to%20split%20a%20decision%20tree%20using%20Information%20Gain%3A,entropy%20or%20highest%20information%20gain) (2020),
    analyticsvidhya
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All images unless otherwise noted are by the author.
  prefs: []
  type: TYPE_NORMAL
- en: 'Find more Data Science and Machine Learning posts here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[## More'
  prefs: []
  type: TYPE_NORMAL
- en: Data Science and Machine Learning Blog
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: datamapu.com](https://datamapu.com/?source=post_page-----d0bc17fcf1c2--------------------------------)
    [](https://medium.com/@pumaline/subscribe?source=post_page-----d0bc17fcf1c2--------------------------------)
    [## Get an email whenever Pumaline publishes.
  prefs: []
  type: TYPE_NORMAL
- en: Get an email whenever Pumaline publishes. By signing up, you will create a Medium
    account if you don't already have…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: medium.com](https://medium.com/@pumaline/subscribe?source=post_page-----d0bc17fcf1c2--------------------------------)
    [](https://www.buymeacoffee.com/pumaline?source=post_page-----d0bc17fcf1c2--------------------------------)
    [## Pumaline
  prefs: []
  type: TYPE_NORMAL
- en: Hey, I like to learn and share knowledge about Data Science and Machine Learning.
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: www.buymeacoffee.com](https://www.buymeacoffee.com/pumaline?source=post_page-----d0bc17fcf1c2--------------------------------)
  prefs: []
  type: TYPE_NORMAL
