- en: Evaluation Metrics for Recommendation Systems — An Overview
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/evaluation-metrics-for-recommendation-systems-an-overview-71290690ecba?source=collection_archive---------3-----------------------#2023-08-09](https://towardsdatascience.com/evaluation-metrics-for-recommendation-systems-an-overview-71290690ecba?source=collection_archive---------3-----------------------#2023-08-09)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Understanding the purpose and functionality of common metrics in ML packages
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@pratikaher?source=post_page-----71290690ecba--------------------------------)[![Pratik
    Aher](../Images/5648c040ff967717c94657ebfff11e2b.png)](https://medium.com/@pratikaher?source=post_page-----71290690ecba--------------------------------)[](https://towardsdatascience.com/?source=post_page-----71290690ecba--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----71290690ecba--------------------------------)
    [Pratik Aher](https://medium.com/@pratikaher?source=post_page-----71290690ecba--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fc2e5b1d7be67&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fevaluation-metrics-for-recommendation-systems-an-overview-71290690ecba&user=Pratik+Aher&userId=c2e5b1d7be67&source=post_page-c2e5b1d7be67----71290690ecba---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----71290690ecba--------------------------------)
    ·7 min read·Aug 9, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F71290690ecba&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fevaluation-metrics-for-recommendation-systems-an-overview-71290690ecba&user=Pratik+Aher&userId=c2e5b1d7be67&source=-----71290690ecba---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F71290690ecba&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fevaluation-metrics-for-recommendation-systems-an-overview-71290690ecba&source=-----71290690ecba---------------------bookmark_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: Recently, while experimenting with a recommendation system project, I found
    myself using a variety of evaluation metrics. So I compiled a list of metrics
    that I found helpful and some other things to consider while evaluating recommendation
    systems. These metrics are commonly found in ML packages, yet understanding their
    purpose and functionality is essential.
  prefs: []
  type: TYPE_NORMAL
- en: '**Recall @K**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Recall@K gives a measure of how many of the relevant items are present in top
    K out of all the relevant items, where K is the number of recommendations generated
    for a user. For example, if we are building a movie recommender system where we
    recommend 10 movies for every user. If a user has seen 5 movies, and our recommendation
    list has 3 of them (out of the 10 recommendations), the Recall@10 for a user is
    calculated as 3/5 = 0.6\. Usually, the average is taken across all users for evaluation.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d0c36e0fe6fda64f9531f4bcb1d5fa0a.png)'
  prefs: []
  type: TYPE_IMG
- en: It is a simple yet significant metric from a business point of view, as we can
    show how good a system is in bringing real value in terms of predicting user behavior.
  prefs: []
  type: TYPE_NORMAL
- en: '*Range : 0–1*'
  prefs: []
  type: TYPE_NORMAL
- en: Precision @K
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Precision@K gives a measure of “out of K” items recommended to a user and how
    many are relevant, where K is the number of recommendations generated for a user..
  prefs: []
  type: TYPE_NORMAL
- en: For a recommendation system where we recommend 10 movies for every user. If
    a user has watched 5 movies and we are able to predict 3 out of them ( 3 movies
    are present in our recommendation list) then our Precision@10 is 3/10.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f59344f4ce4203ade68d292b95a1ee72.png)'
  prefs: []
  type: TYPE_IMG
- en: 'It is a very important metric from a scale and ranking point of view because,
    in the real world, there is a limit to how many recommendations you can serve
    to the user. This can be related to: attention span (users want to able to see
    relevant recommendations at first glance, so having relevant recommendations at
    the top is crucial), and, memory requirements: suppose you are only able to store
    100 recommendations per user, then you want to be precise in what you choose.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*Range : 0–1*'
  prefs: []
  type: TYPE_NORMAL
- en: F1 @K
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/77eeb665432fb9a1e1f98ec85059f210.png)'
  prefs: []
  type: TYPE_IMG
- en: F1 Score is a combination of Precision and Recall using harmonic mean. This
    is the same as the regular [F1 score](/the-f1-score-bec2bbc38aa6) and does not
    differ in the context of the recommendation systems. The harmonic mean nature
    makes sure if either Precision or Recall has a really high value, then it does
    not dominate the score. F1 Score has a high value when both precision and recall
    values are close to 1.
  prefs: []
  type: TYPE_NORMAL
- en: '*Range : 0–1*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Ranking related metrics:'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we discussed above, when talking about precision, it is crucial to have relevant
    recommendations at the top. There are various methods to measure if relevant recommendations
    are indeed at the top. These measurements are not only used in evaluation but
    also used as a loss metric for ranking models.
  prefs: []
  type: TYPE_NORMAL
- en: Mean Average Precision @K
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One way of measuring how good a recommendation list is at predicting relevant
    items based on their position in the list is using “Mean Average Precision”.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s first understand what Average Precision is. If we recommended K items,
    out of which Q is relevant then the Average precision is defined as :'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/616ba0043e39dc4c94fcf33651bb8984.png)'
  prefs: []
  type: TYPE_IMG
- en: In case, if all the relevant items are at the top then Average Precision score
    for that user is high.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example :'
  prefs: []
  type: TYPE_NORMAL
- en: 'List of Recommendations : [”Top Gun”, “Arrival”, “Gladiator”]'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Ground truth : [“Arrival”, “Gladiator”]'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Precision @K’s = [0, 1/2, 2/3]
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Average Precision (AP) = (1/3)[(1/2) + (2/3)] = 0.38
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'The mean in MAP is just average precision(AP) values across all users :'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4aa0b195678bbbcb96c9a2d6c919f67a.png)'
  prefs: []
  type: TYPE_IMG
- en: '*Range : 0–1*'
  prefs: []
  type: TYPE_NORMAL
- en: Mean Reciprocal Rank (MRR)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Mean Reciprocal Rank measures the position of the first relevant item discovered
    within a recommendation list. Reciprocal Rank (RR) is used when we only care about
    the position of highest ranked result. Here, rank is the position of an item in
    the list of recommendations.
  prefs: []
  type: TYPE_NORMAL
- en: The reciprocal is useful because it makes sure that items that have a lower
    rank (e.g. Rank 20) get a lower score because the reciprocal of a big value is
    a really small value. So it benefits if most relevant items are predicted to be
    at the top of the list.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/079e4d7f5fbc82131e45b5c1224fe2c7.png)'
  prefs: []
  type: TYPE_IMG
- en: Reciprocal Rank only cares about the first relevant item. For Example,
  prefs: []
  type: TYPE_NORMAL
- en: 'List of Recommendations : [”Top Gun”, “Arrival”, “Gladiator”]'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Ground truth : “Arrival”'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Then, Reciprocal Rank (RR) = (1/2) = 0.5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: In the context of recommendation systems we could also use MRR , if we have
    multiple values in recommendation systems, we can average them.
  prefs: []
  type: TYPE_NORMAL
- en: 'List of Recommendations : [”Top Gun”, “Arrival”, “Gladiator”]'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Ground truth : [“Arrival”, “Gladiator”]'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Then, Mean Reciprocal Rank (MRR) = 1/2* ((1/2) + (1/3)) = 0.41
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*Range : 0–1*'
  prefs: []
  type: TYPE_NORMAL
- en: Normalized Cumulative Discounted Gain (NDCG)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Normalized Discounted Cumulative Gain (NDCG) is the measure of how good a ranked
    list is. [](http://is.it/) The idea is that if relevant items are ordered from
    most relevant to least relevant then the NDCG score is maximized if the most relevant
    items are recommended at the top of the list.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s break this down using an example :'
  prefs: []
  type: TYPE_NORMAL
- en: 'To try to stick to the previous example: if we identify a user as an action
    movie watcher, then let’s assume relevancy scores as :'
  prefs: []
  type: TYPE_NORMAL
- en: '“Top Gun”, “Gladiator”: 2 (most relevant)'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '“Toy Story”: 1'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '“The Whale” : 0 (least relevant)'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'List of Recommendations :'
  prefs: []
  type: TYPE_NORMAL
- en: '[”Top Gun”, “Toy Story”, “The Whale”, “Gladiator”] ⇒ [2, 1, 0, 2]'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Cumulative Gain (CG): Cumulative gain at a position p is the relevancy score
    at that position. So for the entire list, it is: 2 + 1 + 0 + 2 = 5'
  prefs: []
  type: TYPE_NORMAL
- en: The cumulative gain does not take into account the position of items. So, if
    an item the most relevant item is at the end of the list (like “Gladiator”) then
    it is not reflected in the CG score.
  prefs: []
  type: TYPE_NORMAL
- en: To deal with that, we introduce **Discounted Cumulative Gain** (DCG), where
    we assign a score/discount to each position by which the relevancy score will
    be penalized.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f8078aca600bf3b0f670d305e7b787d1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'So, if a relevant item like “Gladiator” is put at recommended at the end of
    the list, it will be discounted by 1/log2(n) (where n is the size of the list
    : It will be multiplied by a much smaller number like 0.2 so its contribution
    to score will be really small) compared to the first item which will not be discounted.'
  prefs: []
  type: TYPE_NORMAL
- en: DCG scores are highest if all the relevant items are at the top.
  prefs: []
  type: TYPE_NORMAL
- en: 'For the items, Set A: [2, 1, 0, 2] :'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a1c1db7d4cd6e576eed67c6b02b86eb4.png)'
  prefs: []
  type: TYPE_IMG
- en: 'let’s compare this to Set B: [2, 2, 1, 0], where all the relevant items are
    at the top :'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f4e8dc7e5e7004195b8f5404c2d5dc0e.png)'
  prefs: []
  type: TYPE_IMG
- en: Clearly, the DCG of set B is higher than the DCG of set A. Also, et B is what
    we call Ideal Discounted Cumulative Gain (IDCG), which gives us the DCG of the
    ideal list where items are perfectly sorted according to their relevancy scores.
  prefs: []
  type: TYPE_NORMAL
- en: '*What if we need to compare DCG scores to two lists of different sizes?*'
  prefs: []
  type: TYPE_NORMAL
- en: That is the case where IDCG comes into the picture, we divide our DCG scores
    by IDCG scores and get a value between 0–1\. This score is called Normalized Discounted
    Cumulative Gain (nDCG).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/67f63be77e3dd159988453a9c23bf83f.png)'
  prefs: []
  type: TYPE_IMG
- en: Now we can compare nDCG scores of two lists of different sizes.
  prefs: []
  type: TYPE_NORMAL
- en: '*nDCG Range : 0–1*'
  prefs: []
  type: TYPE_NORMAL
- en: This is an overview of some of the metrics that are widely used to evaluate
    recommender systems.
  prefs: []
  type: TYPE_NORMAL
- en: 'Things to consider while evaluating recommender systems:'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: I plan to put together articles on these topics soon, and in that case, I will
    link them here.
  prefs: []
  type: TYPE_NORMAL
- en: Popularity Bias
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: There can be a great amount of popularity bias in recommender systems and it
    is difficult to detect and fix because popular items are relevant and score high
    on most of the standard metrics. There are various ways to measure and fix popularity
    bias and I plan to put together an article that talks about them.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e102872dec6e39765e07114bf606c440.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Position bias
  prefs: []
  type: TYPE_NORMAL
- en: Position bias occurs when items placed higher on a list are more likely to be
    viewed or bought, irrespective of their actual relevance. As a result, items with
    lower rankings receive less engagement. This might also affect other metrics,
    and there are methods to mitigate this.
  prefs: []
  type: TYPE_NORMAL
- en: 3\. Degenerate Feedback loop
  prefs: []
  type: TYPE_NORMAL
- en: When users are limited to interacting with suggested items, and algorithms rely
    on user feedback from these suggestions for training, a negative feedback loop
    can emerge. This loop reinforces displaying of items that have been previously
    shown, potentially resulting in a negative user experience over time due to a
    significant portion of relevant items remaining undiscovered by the user.
  prefs: []
  type: TYPE_NORMAL
- en: '**References :**'
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://www.wikiwand.com/en/Mean_reciprocal_rank?source=post_page-----71290690ecba--------------------------------)
    [## Wikiwand - Mean reciprocal rank'
  prefs: []
  type: TYPE_NORMAL
- en: The mean reciprocal rank is a statistic measure for evaluating any process that
    produces a list of possible responses…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: www.wikiwand.com](https://www.wikiwand.com/en/Mean_reciprocal_rank?source=post_page-----71290690ecba--------------------------------)
    [](https://sdsawtelle.github.io/blog/output/mean-average-precision-MAP-for-recommender-systems.html?source=post_page-----71290690ecba--------------------------------)
    [## Mean Average Precision (MAP) For Recommender Systems
  prefs: []
  type: TYPE_NORMAL
- en: (Ok there's one pun.) Since you're reading this you've probably just encountered
    the term "Mean Average Precision", or…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: sdsawtelle.github.io](https://sdsawtelle.github.io/blog/output/mean-average-precision-MAP-for-recommender-systems.html?source=post_page-----71290690ecba--------------------------------)
    [](/evaluation-metrics-for-recommender-systems-df56c6611093?source=post_page-----71290690ecba--------------------------------)
    [## Evaluation Metrics for Recommender Systems
  prefs: []
  type: TYPE_NORMAL
- en: Recommender systems are growing progressively more popular in online retail.
    Take a look at some metrics used to…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/evaluation-metrics-for-recommender-systems-df56c6611093?source=post_page-----71290690ecba--------------------------------)
  prefs: []
  type: TYPE_NORMAL
