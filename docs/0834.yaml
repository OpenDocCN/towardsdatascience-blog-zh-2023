- en: 'Embeddings: ChatGPT’s Secret Weapon'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 嵌入技术：ChatGPT 的终极武器
- en: 原文：[https://towardsdatascience.com/embeddings-chatgpts-secret-weapon-1870e590f32c?source=collection_archive---------1-----------------------#2023-03-06](https://towardsdatascience.com/embeddings-chatgpts-secret-weapon-1870e590f32c?source=collection_archive---------1-----------------------#2023-03-06)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/embeddings-chatgpts-secret-weapon-1870e590f32c?source=collection_archive---------1-----------------------#2023-03-06](https://towardsdatascience.com/embeddings-chatgpts-secret-weapon-1870e590f32c?source=collection_archive---------1-----------------------#2023-03-06)
- en: Embeddings, and how they help ChatGPT predict the next word
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 嵌入技术，以及它们如何帮助 ChatGPT 预测下一个词
- en: '[](https://emmaccode.medium.com/?source=post_page-----1870e590f32c--------------------------------)[![Emma
    Boudreau](../Images/f7201d012b733643d6e97957f73fd1fa.png)](https://emmaccode.medium.com/?source=post_page-----1870e590f32c--------------------------------)[](https://towardsdatascience.com/?source=post_page-----1870e590f32c--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----1870e590f32c--------------------------------)
    [Emma Boudreau](https://emmaccode.medium.com/?source=post_page-----1870e590f32c--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://emmaccode.medium.com/?source=post_page-----1870e590f32c--------------------------------)[![艾玛·布德罗](../Images/f7201d012b733643d6e97957f73fd1fa.png)](https://emmaccode.medium.com/?source=post_page-----1870e590f32c--------------------------------)[](https://towardsdatascience.com/?source=post_page-----1870e590f32c--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----1870e590f32c--------------------------------)
    [艾玛·布德罗](https://emmaccode.medium.com/?source=post_page-----1870e590f32c--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fea170050148c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fembeddings-chatgpts-secret-weapon-1870e590f32c&user=Emma+Boudreau&userId=ea170050148c&source=post_page-ea170050148c----1870e590f32c---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----1870e590f32c--------------------------------)
    ·5 min read·Mar 6, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F1870e590f32c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fembeddings-chatgpts-secret-weapon-1870e590f32c&user=Emma+Boudreau&userId=ea170050148c&source=-----1870e590f32c---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fea170050148c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fembeddings-chatgpts-secret-weapon-1870e590f32c&user=Emma+Boudreau&userId=ea170050148c&source=post_page-ea170050148c----1870e590f32c---------------------post_header-----------)
    发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----1870e590f32c--------------------------------)
    ·5分钟阅读·2023年3月6日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F1870e590f32c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fembeddings-chatgpts-secret-weapon-1870e590f32c&user=Emma+Boudreau&userId=ea170050148c&source=-----1870e590f32c---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F1870e590f32c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fembeddings-chatgpts-secret-weapon-1870e590f32c&source=-----1870e590f32c---------------------bookmark_footer-----------)![](../Images/b73140f2b26fd1fb712cffe14b2f354e.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F1870e590f32c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fembeddings-chatgpts-secret-weapon-1870e590f32c&source=-----1870e590f32c---------------------bookmark_footer-----------)![](../Images/b73140f2b26fd1fb712cffe14b2f354e.png)'
- en: (image by author)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: (图片由作者提供)
- en: transformers and attention
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 转换器和注意力机制
- en: If you have been browsing the web a lot recently, or reading technical news
    stories, it is likely you have heard or read something about ChatGPT at some point.
    ChatGPT is OpenAI’s new **language transformer** model, and as far as these models
    go this is quite an accurate one that has produced some very compelling — sometimes
    viral — results. A **transformer** in this context refers to a machine-learning
    model which adopts **self-attention**. **Self-attention** is a Data Science word
    that simply means this model attempts to mimic human cognitive function, or human
    cognitive attention. The ***language*** part of this model is also significant;
    as this describes what the transformer hopes to predict, human language. This
    is often referred to as **Natural Language Processing**, or **NLP**, though NLP
    typically refers to the processing that takes place on the language data in order
    to turn it into numerical weights that a computer or neural network can understand.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你最近经常浏览网络或阅读技术新闻，你可能在某个时刻听说过或读到过有关 ChatGPT 的信息。 ChatGPT 是 OpenAI 的新款 **语言
    transformer** 模型，就这些模型而言，这是一个相当准确的模型，产生了一些非常引人注目的——有时甚至是病毒式的——结果。 在这个上下文中，**transformer**
    指的是一种采用 **自注意力** 的机器学习模型。 **自注意力** 是一个数据科学术语，简单来说，这个模型尝试模拟人类的认知功能，或人类的认知注意力。 这个模型的
    ***语言*** 部分也很重要，因为这描述了 transformer 希望预测的内容，即人类语言。这通常被称为 **自然语言处理**，或 **NLP**，尽管
    NLP 通常指的是对语言数据进行处理，以将其转化为计算机或神经网络可以理解的数值权重。
- en: Transformers have a number of significant properties that make their definition
    important to remember whenever we are discussing language transformers like ChatGPT.
    There are a few things that transformers, and more broadly attention models in
    general, that posses…
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: Transformers 具有许多重要特性，这些特性使得在讨论像 ChatGPT 这样的语言 transformer 时，记住它们的定义非常重要。 transformers
    和更广泛的 attention 模型通常具有的一些特性包括…
