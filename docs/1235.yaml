- en: Knowledge Graph Embeddings 101
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/knowledge-graph-embeddings-101-2cc1ca5db44f?source=collection_archive---------1-----------------------#2023-04-09](https://towardsdatascience.com/knowledge-graph-embeddings-101-2cc1ca5db44f?source=collection_archive---------1-----------------------#2023-04-09)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: A summary of knowledge graph embeddings (KGE) algorithms
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@amine.dadoun?source=post_page-----2cc1ca5db44f--------------------------------)[![Amine
    Dadoun](../Images/ebde3241f8071b9c6dbcd2e498ab10c0.png)](https://medium.com/@amine.dadoun?source=post_page-----2cc1ca5db44f--------------------------------)[](https://towardsdatascience.com/?source=post_page-----2cc1ca5db44f--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----2cc1ca5db44f--------------------------------)
    [Amine Dadoun](https://medium.com/@amine.dadoun?source=post_page-----2cc1ca5db44f--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F338cc4ecc244&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fknowledge-graph-embeddings-101-2cc1ca5db44f&user=Amine+Dadoun&userId=338cc4ecc244&source=post_page-338cc4ecc244----2cc1ca5db44f---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----2cc1ca5db44f--------------------------------)
    ·7 min read·Apr 9, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F2cc1ca5db44f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fknowledge-graph-embeddings-101-2cc1ca5db44f&user=Amine+Dadoun&userId=338cc4ecc244&source=-----2cc1ca5db44f---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F2cc1ca5db44f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fknowledge-graph-embeddings-101-2cc1ca5db44f&source=-----2cc1ca5db44f---------------------bookmark_footer-----------)![](../Images/1dd2a9f480d213ce8696ef763aa43aa3.png)'
  prefs: []
  type: TYPE_NORMAL
- en: '[Image Source](https://pixabay.com/illustrations/artificial-intelligence-network-3706562/).
    The image is free to use under the content licence of Pixabay.'
  prefs: []
  type: TYPE_NORMAL
- en: In our latest article of the series on *How to design recommender systems based
    on graphs?* we introduced an emerging category of recommender system algorithm
    known as knowledge graph-based recommender systems. These systems leverage the
    semantic structure of knowledge graphs and the powerful capabilities of knowledge
    graph embedding (KGE) algorithms to provide users with more precise product recommendations.
  prefs: []
  type: TYPE_NORMAL
- en: What is a Knowledge Graph Embedding?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As mentioned in [the previous article](https://medium.com/@amine.dadoun/introduction-to-knowledge-graph-based-recommender-systems-34254efd1960),
    Knowledge graphs (KG) are effective in representing structured data and incorporating
    data coming from different sources, however the underlying symbolic nature of
    knowledge graph triples usually makes KGs hard to manipulate for Machine Learning
    applications.
  prefs: []
  type: TYPE_NORMAL
- en: A Knowledge graph embedding (KGE) is a representation of a KG element into a
    continuous vector space. The objective of learning those embeddings is to ease
    the manipulation of graph elements (entities, relations) for prediction tasks
    such as entity classification, link prediction or recommender systems.
  prefs: []
  type: TYPE_NORMAL
- en: Most of the proposed methods rely solely on graph triples with the goal to embed
    KG entities and relations into continuous vector space. The idea is to preserve
    the inherent structure of the KG and simplify the use of KG elements. Once KG
    elements are represented as embeddings, a scoring function is used to measure
    the plausibility of a triple (e.g. ‘George’, ‘is A’, ‘Person’).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/36d58933b4d0376105de940243f1c6e5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The figure above shows embedding representations of nodes and relations projected
    in a 2D vectorial space. Image source: [https://docs.ampligraph.org/en/1.1.0/](https://docs.ampligraph.org/en/1.1.0/).
    Ampligraph is a free open source python library for KGE. Copyright AmpliGraph
    is licensed under the Apache 2.0'
  prefs: []
  type: TYPE_NORMAL
- en: Inspired by Word Embeddings
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Embeddings have become popular thanks to the release of Word2vec [1] in 2013\.
    Word2vec efficiently learns word embeddings by training a shallow neural network
    to predict the context of a word included in a vocabulary, defined by a sliding
    window of a given amplitude, with the key idea to preserve the semantic of the
    words. Two different architectures are proposed as shown in the figure below,
    namely Continuous Bag-of-Words [2] (CBOW) that implements a neural network where
    the input corresponds to the context words wₜ₋ᵢ ,wₜ₋ᵢ₊₁ …wₜ₊ᵢ₋₁,wₜ₊ᵢ and the output
    to predict is the target word wₜ and Skip-Gram [1] that implements a two layer
    neural network where the input corresponds to the target word wₜ and the output
    to the context words.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/05d39e10686b168cc2eab5677714be2f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The CBOW architecture predicts the current word based on the context, and the
    Skip-gram model predicts surrounding words given the current word. Image source:
    [Mikolov et al.](https://arxiv.org/abs/1301.3781)'
  prefs: []
  type: TYPE_NORMAL
- en: Following the same logic, the authors of *DeepWalk* [3] and *node2vec* [4] generalized
    embeddings to graphs by suggesting to make use of neural language models such
    as Word2vec to build graph embeddings. In DeepWalk, the authors proposed to extract
    sequences of nodes — which represent entities — in the graph by relying on a random
    uniform walk in the graph. This sequence of nodes can be seen as a text, and then
    CBOW or SkipGram is applied to construct embeddings of these nodes.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2ec29bf0e858eb1aef61864cc808a81f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'DeepWalk seeks to preserve the local structure of the graph in the embedding
    space. Image source: [Bryan Perozzi, Rami Al-Rfou, Steven Skiena](https://arxiv.org/abs/1403.6652)'
  prefs: []
  type: TYPE_NORMAL
- en: '*Node2vec* went further by introducing a more sophisticated random walk strategy
    that can be more easily adapted to a diversity of graph connectivity patterns,
    outperforming DeepWalk in link prediction and knowledge graph completion tasks.'
  prefs: []
  type: TYPE_NORMAL
- en: Leveraging properties of KGs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Considering only the graph structure to encode KG elements is nevertheless not
    sufficient, hence other methods have emerged to consider also properties and entity
    types of the graph. In [5], the authors classified the knowledge graph embedding
    algorithms into two main categories namely ***translational distance models***
    that are based on a scoring function that measures the plausibility of a triple
    by measuring distances in the vector space, typically after performing a translation
    operation and ***semantic matching models*** that are based on a similarity-based
    scoring function that measures the plausibility of a triple by matching the semantics
    of the latent representations of entities and relations.
  prefs: []
  type: TYPE_NORMAL
- en: Translation Distance Models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For the first category, TransE [6] is often mentioned as the most used translational
    distance model. TransE represents both entities and relations vectors in the same
    space Rₘ. Given a triple (s,p,o), the relation is interpreted as a translation
    vector r so that the embedded entities s (subject) and o (object) can be connected
    by p with low error, i.e., s + p ≈ o when the triple (s,p,o) holds in the knowledge
    graph. In other terms, the goal is to minimize the scoring function represented
    below.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2ccf06940e6f2f8b2d342412f6530b17.png)'
  prefs: []
  type: TYPE_IMG
- en: TransH [6] introduces relation-specific hyper-planes, each property p being
    represented on a hyperplane as wₚ its normal vector. TransR [8] follows the same
    idea as TransH, but instead of projecting the relations into a hyper-plane, it
    proposes to create a specific space per relation. We represent in the figure below
    the embedding space of the different translational distance models presented above.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7c3b15de4a2b888bc7c897f6d6840937.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Distance between embeddings are computed in the same embedding space for TransE
    regardless the relation while for TransH and TransR, they are computed in relation
    specific spaces. (h, r, t) is a triple in the KG. Image source: [Wang et al.](https://ieeexplore.ieee.org/document/8047276)'
  prefs: []
  type: TYPE_NORMAL
- en: Semantic Matching Models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: On the other hand, *semantic matching models* exploit similarity-based scoring
    functions. In [9], the authors proposed RESCAL, a model that associates each entity
    with a vector to capture its latent semantics. Each relation is represented as
    a matrix that models pairwise interactions between latent factors. The score of
    a triple (s, p, o) is defined by a bi-linear scoring function minimized through
    tensor factorization based on ALS optimization technique.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0ecdaa6706996c1560834c0dc5542c3b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Tensor Embeddings of RESCAL method. Image source: [Bhattarai et al.](https://arxiv.org/pdf/2202.09512.pdf)'
  prefs: []
  type: TYPE_NORMAL
- en: Other methods that extend *RESCAL* emerged. NTN [10] (Neural Tensor Network)
    is a neural network that learns representations using non-linear layers. ER-MLP
    (Multi layer perceptron), where each relation (as well as entity) is associated
    with a single vector. More specifically, given a triple (s, p, o), the vector
    embeddings of s, p, and o are concatenated in the input layer, and mapped to a
    non-linear hidden layer.
  prefs: []
  type: TYPE_NORMAL
- en: Other methods emerged such as *DistMul* which simplifies *RESCAL* by representing
    relations with diagonal matrices, thus reducing its complexity, *ComplEX* whichextends
    *DistMul* using complex numbers in place of real numbers. In recent years, numerous
    methods have emerged with the aim of simplifying the existing literature and improving
    the accuracy of existing algorithms for knowledge graph (KG) tasks, such as the
    link prediction task. These methods employ various techniques such as neural network-based
    models, factorization-based models, and random walk-based models, to name a few.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d6f444a0b9bbde4bf0bafd1d0261fa42.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Evolution of KGE algorithms. Image source: [Knowledge graph embedding](https://en.wikipedia.org/wiki/Knowledge_graph_embedding)'
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In conclusion, knowledge graph embedding algorithms have become a powerful tool
    for representing and reasoning about complex structured data. These algorithms
    learn low-dimensional embeddings of entities and relations in a knowledge graph,
    allowing for efficient computation of similarity and inference tasks.
  prefs: []
  type: TYPE_NORMAL
- en: In this blog post, we have discussed various embedding algorithms, including
    TransE, TransH, TransR, DistMult, ComplEx, and highlighted their strengths and
    weaknesses. Overall, knowledge graph embedding algorithms have shown great promise
    in a wide range of applications, including question answering, recommender systems,
    and natural language processing. As the field continues to evolve, we can expect
    to see even more powerful and effective embedding algorithms that can handle increasingly
    large and complex knowledge graphs.
  prefs: []
  type: TYPE_NORMAL
- en: In the next blog post of this series, we will present some concrete recommendation
    system use cases where the use of KGE is beneficial to improve the recommendation
    accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: 'References:'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[1] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, and Jeffrey Dean.
    Distributed representations of words and phrases and their compositionality. In
    Proceedings of the 26th International Conference on Neural Information Processing
    Systems — Volume 2, NIPS’13, page 3111–3119, Red Hook, NY, USA, 2013\. Curran
    Associates Inc.'
  prefs: []
  type: TYPE_NORMAL
- en: '[2] Tomás Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efficient estimation
    of word representations in vector space. In Yoshua Bengio and Yann LeCun, editors,
    1st International Conference on Learning Representations, ICLR 2013, Scottsdale,
    Arizona, USA, May 2–4, 2013, Workshop Track Proceedings, 2013.'
  prefs: []
  type: TYPE_NORMAL
- en: '[3] Bryan Perozzi, Rami Al-Rfou, and Steven Skiena. Deepwalk: Online learning
    of social representations. In Proceedings of the 20th ACM SIGKDD international
    conference on Knowledge discovery and data mining, pages 701–710, 2014.'
  prefs: []
  type: TYPE_NORMAL
- en: '[4] Aditya Grover and Jure Leskovec. Node2vec: Scalable feature learning for
    networks. In 22Nd ACM SIGKDD International Conference on Knowledge Discovery and
    Data Mining, pages 855–864, New York, NY, USA, 2016\. ACM.'
  prefs: []
  type: TYPE_NORMAL
- en: '[5] Nan Wang, Hongning Wang, Yiling Jia, and Yue Yin. Explainable recommendation
    via multi-task learning in opinionated text data. In The 41st International ACM
    SIGIR Conference on Research and Development in Information Retrieval, SIGIR ’18,
    page 165–174, New York, NY, USA, 2018\. Association for Computing Machinery.'
  prefs: []
  type: TYPE_NORMAL
- en: '[6] Antoine Bordes, Nicolas Usunier, Alberto Garcia-Duran, Jason Weston, and
    Oksana Yakhnenko. Translating embeddings for modeling multi-relational data. In
    C. J. C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K. Q. Weinberger, editors,
    Advances in Neural Information Processing Systems, volume 26, Lake Tahoe, Nevada,
    United States, 2013\. Curran Associates, Inc.'
  prefs: []
  type: TYPE_NORMAL
- en: '[7] Zhen Wang, Jianwen Zhang, Jianlin Feng, and Zheng Chen. Knowledge graph
    embedding by translating on hyperplanes. In Proceedings of the Twenty-Eighth AAAI
    Conference on Artificial Intelligence, AAAI’14, pages 1112–1119, Québec City,
    Québec, Canada, 2014\. AAAI Press.'
  prefs: []
  type: TYPE_NORMAL
- en: '[8] Yankai Lin, Zhiyuan Liu, Maosong Sun, Yang Liu, and Xuan Zhu. Learning
    entity and relation embeddings for knowledge graph completion. In Proceedings
    of the TwentyNinth AAAI Conference on Artificial Intelligence, AAAI’15, pages
    2181–2187, Austin, Texas, 2015\. AAAI Press.'
  prefs: []
  type: TYPE_NORMAL
- en: '[9] Maximilian Nickel, Volker Tresp, and Hans-Peter Kriegel. A three-way model
    for collective learning on multi-relational data. In Proceedings of the 28th International
    Conference on International Conference on Machine Learning, ICML’11, pages 809–816,
    Madison, WI, USA, 2011\. Omnipress.'
  prefs: []
  type: TYPE_NORMAL
- en: '[10] Richard Socher, Danqi Chen, Christopher D Manning, and Andrew Ng. Reasoning
    with neural tensor networks for knowledge base completion. In C. J. C. Burges,
    L. Bottou, M. Welling, Z. Ghahramani, and K. Q. Weinberger, editors, Advances
    in Neural Information Processing Systems, volume 26, Lake Tahoe, Nevada, United
    States, 2013\. Curran Associates, Inc'
  prefs: []
  type: TYPE_NORMAL
