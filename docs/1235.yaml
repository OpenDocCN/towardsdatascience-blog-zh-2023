- en: Knowledge Graph Embeddings 101
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 知识图谱嵌入基础
- en: 原文：[https://towardsdatascience.com/knowledge-graph-embeddings-101-2cc1ca5db44f?source=collection_archive---------1-----------------------#2023-04-09](https://towardsdatascience.com/knowledge-graph-embeddings-101-2cc1ca5db44f?source=collection_archive---------1-----------------------#2023-04-09)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/knowledge-graph-embeddings-101-2cc1ca5db44f?source=collection_archive---------1-----------------------#2023-04-09](https://towardsdatascience.com/knowledge-graph-embeddings-101-2cc1ca5db44f?source=collection_archive---------1-----------------------#2023-04-09)
- en: A summary of knowledge graph embeddings (KGE) algorithms
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 知识图谱嵌入（KGE）算法的总结
- en: '[](https://medium.com/@amine.dadoun?source=post_page-----2cc1ca5db44f--------------------------------)[![Amine
    Dadoun](../Images/ebde3241f8071b9c6dbcd2e498ab10c0.png)](https://medium.com/@amine.dadoun?source=post_page-----2cc1ca5db44f--------------------------------)[](https://towardsdatascience.com/?source=post_page-----2cc1ca5db44f--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----2cc1ca5db44f--------------------------------)
    [Amine Dadoun](https://medium.com/@amine.dadoun?source=post_page-----2cc1ca5db44f--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@amine.dadoun?source=post_page-----2cc1ca5db44f--------------------------------)[![Amine
    Dadoun](../Images/ebde3241f8071b9c6dbcd2e498ab10c0.png)](https://medium.com/@amine.dadoun?source=post_page-----2cc1ca5db44f--------------------------------)[](https://towardsdatascience.com/?source=post_page-----2cc1ca5db44f--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----2cc1ca5db44f--------------------------------)
    [Amine Dadoun](https://medium.com/@amine.dadoun?source=post_page-----2cc1ca5db44f--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F338cc4ecc244&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fknowledge-graph-embeddings-101-2cc1ca5db44f&user=Amine+Dadoun&userId=338cc4ecc244&source=post_page-338cc4ecc244----2cc1ca5db44f---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----2cc1ca5db44f--------------------------------)
    ·7 min read·Apr 9, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F2cc1ca5db44f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fknowledge-graph-embeddings-101-2cc1ca5db44f&user=Amine+Dadoun&userId=338cc4ecc244&source=-----2cc1ca5db44f---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F338cc4ecc244&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fknowledge-graph-embeddings-101-2cc1ca5db44f&user=Amine+Dadoun&userId=338cc4ecc244&source=post_page-338cc4ecc244----2cc1ca5db44f---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----2cc1ca5db44f--------------------------------)
    ·7分钟阅读·2023年4月9日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F2cc1ca5db44f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fknowledge-graph-embeddings-101-2cc1ca5db44f&user=Amine+Dadoun&userId=338cc4ecc244&source=-----2cc1ca5db44f---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F2cc1ca5db44f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fknowledge-graph-embeddings-101-2cc1ca5db44f&source=-----2cc1ca5db44f---------------------bookmark_footer-----------)![](../Images/1dd2a9f480d213ce8696ef763aa43aa3.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F2cc1ca5db44f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fknowledge-graph-embeddings-101-2cc1ca5db44f&source=-----2cc1ca5db44f---------------------bookmark_footer-----------)![](../Images/1dd2a9f480d213ce8696ef763aa43aa3.png)'
- en: '[Image Source](https://pixabay.com/illustrations/artificial-intelligence-network-3706562/).
    The image is free to use under the content licence of Pixabay.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '[图片来源](https://pixabay.com/illustrations/artificial-intelligence-network-3706562/)。该图片根据Pixabay的内容许可可以自由使用。'
- en: In our latest article of the series on *How to design recommender systems based
    on graphs?* we introduced an emerging category of recommender system algorithm
    known as knowledge graph-based recommender systems. These systems leverage the
    semantic structure of knowledge graphs and the powerful capabilities of knowledge
    graph embedding (KGE) algorithms to provide users with more precise product recommendations.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们最新的一篇系列文章中，*如何基于图设计推荐系统？*我们介绍了一种新兴的推荐系统算法类别，即基于知识图谱的推荐系统。这些系统利用知识图谱的语义结构和知识图谱嵌入（KGE）算法的强大能力，为用户提供更精准的产品推荐。
- en: What is a Knowledge Graph Embedding?
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是知识图谱嵌入？
- en: As mentioned in [the previous article](https://medium.com/@amine.dadoun/introduction-to-knowledge-graph-based-recommender-systems-34254efd1960),
    Knowledge graphs (KG) are effective in representing structured data and incorporating
    data coming from different sources, however the underlying symbolic nature of
    knowledge graph triples usually makes KGs hard to manipulate for Machine Learning
    applications.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 如[前一篇文章](https://medium.com/@amine.dadoun/introduction-to-knowledge-graph-based-recommender-systems-34254efd1960)所述，知识图谱（KG）在表示结构化数据和整合来自不同来源的数据方面非常有效。然而，知识图谱三元组的符号化特性通常使得知识图谱在机器学习应用中难以操作。
- en: A Knowledge graph embedding (KGE) is a representation of a KG element into a
    continuous vector space. The objective of learning those embeddings is to ease
    the manipulation of graph elements (entities, relations) for prediction tasks
    such as entity classification, link prediction or recommender systems.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 知识图谱嵌入（KGE）是将KG元素表示为连续向量空间的一种表示方法。学习这些嵌入的目标是简化图元素（实体、关系）的操作，以用于预测任务，例如实体分类、链接预测或推荐系统。
- en: Most of the proposed methods rely solely on graph triples with the goal to embed
    KG entities and relations into continuous vector space. The idea is to preserve
    the inherent structure of the KG and simplify the use of KG elements. Once KG
    elements are represented as embeddings, a scoring function is used to measure
    the plausibility of a triple (e.g. ‘George’, ‘is A’, ‘Person’).
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数提出的方法仅依赖于图三元组，目的是将KG实体和关系嵌入到连续的向量空间中。这个想法是保留KG的固有结构并简化KG元素的使用。一旦KG元素被表示为嵌入，就使用评分函数来测量三元组的合理性（例如‘George’，‘是
    A’，‘Person’）。
- en: '![](../Images/36d58933b4d0376105de940243f1c6e5.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/36d58933b4d0376105de940243f1c6e5.png)'
- en: 'The figure above shows embedding representations of nodes and relations projected
    in a 2D vectorial space. Image source: [https://docs.ampligraph.org/en/1.1.0/](https://docs.ampligraph.org/en/1.1.0/).
    Ampligraph is a free open source python library for KGE. Copyright AmpliGraph
    is licensed under the Apache 2.0'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 上图展示了节点和关系在二维向量空间中的嵌入表示。图片来源：[https://docs.ampligraph.org/en/1.1.0/](https://docs.ampligraph.org/en/1.1.0/)。Ampligraph是一个免费的开源Python库，用于知识图谱嵌入。版权所有AmpliGraph，许可证为Apache
    2.0。
- en: Inspired by Word Embeddings
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 受词嵌入启发
- en: Embeddings have become popular thanks to the release of Word2vec [1] in 2013\.
    Word2vec efficiently learns word embeddings by training a shallow neural network
    to predict the context of a word included in a vocabulary, defined by a sliding
    window of a given amplitude, with the key idea to preserve the semantic of the
    words. Two different architectures are proposed as shown in the figure below,
    namely Continuous Bag-of-Words [2] (CBOW) that implements a neural network where
    the input corresponds to the context words wₜ₋ᵢ ,wₜ₋ᵢ₊₁ …wₜ₊ᵢ₋₁,wₜ₊ᵢ and the output
    to predict is the target word wₜ and Skip-Gram [1] that implements a two layer
    neural network where the input corresponds to the target word wₜ and the output
    to the context words.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入技术因2013年发布的Word2vec [1] 而变得流行。Word2vec通过训练一个浅层神经网络来预测词汇表中一个词的上下文，从而高效地学习词嵌入，关键思想是保留词的语义。图下所示的两种不同架构分别是Continuous
    Bag-of-Words [2] (CBOW)，它实现了一个神经网络，其中输入是上下文词wₜ₋ᵢ ,wₜ₋ᵢ₊₁ …wₜ₊ᵢ₋₁,wₜ₊ᵢ，而输出是预测的目标词wₜ；另一个是Skip-Gram
    [1]，它实现了一个两层神经网络，其中输入是目标词wₜ，输出是上下文词。
- en: '![](../Images/05d39e10686b168cc2eab5677714be2f.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/05d39e10686b168cc2eab5677714be2f.png)'
- en: 'The CBOW architecture predicts the current word based on the context, and the
    Skip-gram model predicts surrounding words given the current word. Image source:
    [Mikolov et al.](https://arxiv.org/abs/1301.3781)'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: CBOW架构基于上下文预测当前词，而Skip-gram模型给定当前词预测周围词。图片来源：[Mikolov et al.](https://arxiv.org/abs/1301.3781)
- en: Following the same logic, the authors of *DeepWalk* [3] and *node2vec* [4] generalized
    embeddings to graphs by suggesting to make use of neural language models such
    as Word2vec to build graph embeddings. In DeepWalk, the authors proposed to extract
    sequences of nodes — which represent entities — in the graph by relying on a random
    uniform walk in the graph. This sequence of nodes can be seen as a text, and then
    CBOW or SkipGram is applied to construct embeddings of these nodes.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 按照相同的逻辑，*DeepWalk* [3] 和 *node2vec* [4] 的作者通过建议使用神经语言模型如Word2vec来构建图嵌入，将嵌入扩展到图中。在DeepWalk中，作者提出通过依赖图中的随机均匀游走来提取图中的节点序列——这些节点代表实体。这个节点序列可以看作是文本，然后应用CBOW或SkipGram来构建这些节点的嵌入。
- en: '![](../Images/2ec29bf0e858eb1aef61864cc808a81f.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2ec29bf0e858eb1aef61864cc808a81f.png)'
- en: 'DeepWalk seeks to preserve the local structure of the graph in the embedding
    space. Image source: [Bryan Perozzi, Rami Al-Rfou, Steven Skiena](https://arxiv.org/abs/1403.6652)'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: DeepWalk 旨在在嵌入空间中保持图的局部结构。图像来源：[Bryan Perozzi, Rami Al-Rfou, Steven Skiena](https://arxiv.org/abs/1403.6652)
- en: '*Node2vec* went further by introducing a more sophisticated random walk strategy
    that can be more easily adapted to a diversity of graph connectivity patterns,
    outperforming DeepWalk in link prediction and knowledge graph completion tasks.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '*Node2vec* 通过引入更复杂的随机游走策略进一步改进，这种策略可以更容易地适应多样的图连接模式，在链接预测和知识图谱补全任务中优于 DeepWalk。'
- en: Leveraging properties of KGs
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 利用知识图谱的属性
- en: Considering only the graph structure to encode KG elements is nevertheless not
    sufficient, hence other methods have emerged to consider also properties and entity
    types of the graph. In [5], the authors classified the knowledge graph embedding
    algorithms into two main categories namely ***translational distance models***
    that are based on a scoring function that measures the plausibility of a triple
    by measuring distances in the vector space, typically after performing a translation
    operation and ***semantic matching models*** that are based on a similarity-based
    scoring function that measures the plausibility of a triple by matching the semantics
    of the latent representations of entities and relations.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 仅考虑图结构以编码 KG 元素仍然不够，因此出现了其他方法，也考虑图的属性和实体类型。在 [5] 中，作者将知识图谱嵌入算法分为两大类，即基于评分函数的***转换距离模型***，通过在向量空间中测量距离来评估三元组的可信度，通常是在执行转换操作后，以及基于相似度评分函数的***语义匹配模型***，通过匹配实体和关系的潜在表示的语义来评估三元组的可信度。
- en: Translation Distance Models
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 转换距离模型
- en: For the first category, TransE [6] is often mentioned as the most used translational
    distance model. TransE represents both entities and relations vectors in the same
    space Rₘ. Given a triple (s,p,o), the relation is interpreted as a translation
    vector r so that the embedded entities s (subject) and o (object) can be connected
    by p with low error, i.e., s + p ≈ o when the triple (s,p,o) holds in the knowledge
    graph. In other terms, the goal is to minimize the scoring function represented
    below.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 对于第一类，TransE [6] 通常被提及为最常用的转换距离模型。TransE 在相同的空间 Rₘ 中表示实体和关系向量。给定一个三元组 (s,p,o)，关系被解释为一个转换向量
    r，使得嵌入的实体 s（主体）和 o（客体）可以通过 p 以低误差连接，即，当三元组 (s,p,o) 在知识图谱中成立时，s + p ≈ o。换句话说，目标是最小化下面表示的评分函数。
- en: '![](../Images/2ccf06940e6f2f8b2d342412f6530b17.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2ccf06940e6f2f8b2d342412f6530b17.png)'
- en: TransH [6] introduces relation-specific hyper-planes, each property p being
    represented on a hyperplane as wₚ its normal vector. TransR [8] follows the same
    idea as TransH, but instead of projecting the relations into a hyper-plane, it
    proposes to create a specific space per relation. We represent in the figure below
    the embedding space of the different translational distance models presented above.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: TransH [6] 引入了关系特定的超平面，每个属性 p 在超平面上由其法向量 wₚ 表示。TransR [8] 采用了与 TransH 相同的思路，但不是将关系投影到超平面中，而是建议为每个关系创建一个特定的空间。下图表示了上述不同的转换距离模型的嵌入空间。
- en: '![](../Images/7c3b15de4a2b888bc7c897f6d6840937.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7c3b15de4a2b888bc7c897f6d6840937.png)'
- en: 'Distance between embeddings are computed in the same embedding space for TransE
    regardless the relation while for TransH and TransR, they are computed in relation
    specific spaces. (h, r, t) is a triple in the KG. Image source: [Wang et al.](https://ieeexplore.ieee.org/document/8047276)'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 TransE，无论关系如何，嵌入之间的距离都在相同的嵌入空间中计算，而对于 TransH 和 TransR，它们在关系特定的空间中计算。(h, r,
    t) 是 KG 中的一个三元组。图像来源：[Wang et al.](https://ieeexplore.ieee.org/document/8047276)
- en: Semantic Matching Models
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 语义匹配模型
- en: On the other hand, *semantic matching models* exploit similarity-based scoring
    functions. In [9], the authors proposed RESCAL, a model that associates each entity
    with a vector to capture its latent semantics. Each relation is represented as
    a matrix that models pairwise interactions between latent factors. The score of
    a triple (s, p, o) is defined by a bi-linear scoring function minimized through
    tensor factorization based on ALS optimization technique.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，*语义匹配模型*利用基于相似度的评分函数。在 [9] 中，作者提出了 RESCAL，这是一种将每个实体与一个向量关联起来以捕捉其潜在语义的模型。每个关系被表示为一个矩阵，模型化潜在因素之间的配对交互。三元组
    (s, p, o) 的得分由一个双线性评分函数定义，通过基于 ALS 优化技术的张量分解来最小化。
- en: '![](../Images/0ecdaa6706996c1560834c0dc5542c3b.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0ecdaa6706996c1560834c0dc5542c3b.png)'
- en: 'Tensor Embeddings of RESCAL method. Image source: [Bhattarai et al.](https://arxiv.org/pdf/2202.09512.pdf)'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: RESCAL 方法的张量嵌入。图片来源：[Bhattarai 等](https://arxiv.org/pdf/2202.09512.pdf)
- en: Other methods that extend *RESCAL* emerged. NTN [10] (Neural Tensor Network)
    is a neural network that learns representations using non-linear layers. ER-MLP
    (Multi layer perceptron), where each relation (as well as entity) is associated
    with a single vector. More specifically, given a triple (s, p, o), the vector
    embeddings of s, p, and o are concatenated in the input layer, and mapped to a
    non-linear hidden layer.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 扩展 *RESCAL* 的其他方法出现了。NTN [10]（神经张量网络）是一种通过非线性层学习表示的神经网络。ER-MLP（多层感知器），其中每个关系（以及实体）都与一个向量关联。更具体地说，给定一个三元组
    (s, p, o)，s、p 和 o 的向量嵌入在输入层中连接，并映射到一个非线性隐藏层。
- en: Other methods emerged such as *DistMul* which simplifies *RESCAL* by representing
    relations with diagonal matrices, thus reducing its complexity, *ComplEX* whichextends
    *DistMul* using complex numbers in place of real numbers. In recent years, numerous
    methods have emerged with the aim of simplifying the existing literature and improving
    the accuracy of existing algorithms for knowledge graph (KG) tasks, such as the
    link prediction task. These methods employ various techniques such as neural network-based
    models, factorization-based models, and random walk-based models, to name a few.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 其他方法也出现了，比如 *DistMul*，它通过用对角矩阵表示关系来简化 *RESCAL*，从而减少其复杂性，*ComplEX* 扩展了 *DistMul*，使用复数代替实数。近年来，许多方法出现了，旨在简化现有文献并提高现有知识图谱（KG）任务的算法准确性，如链接预测任务。这些方法采用了各种技术，如基于神经网络的模型、基于分解的模型和基于随机游走的模型等。
- en: '![](../Images/d6f444a0b9bbde4bf0bafd1d0261fa42.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d6f444a0b9bbde4bf0bafd1d0261fa42.png)'
- en: 'Evolution of KGE algorithms. Image source: [Knowledge graph embedding](https://en.wikipedia.org/wiki/Knowledge_graph_embedding)'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: KGE 算法的发展。图片来源：[知识图谱嵌入](https://en.wikipedia.org/wiki/Knowledge_graph_embedding)
- en: Conclusion
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: In conclusion, knowledge graph embedding algorithms have become a powerful tool
    for representing and reasoning about complex structured data. These algorithms
    learn low-dimensional embeddings of entities and relations in a knowledge graph,
    allowing for efficient computation of similarity and inference tasks.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，知识图谱嵌入算法已经成为表示和推理复杂结构化数据的强大工具。这些算法学习知识图谱中实体和关系的低维嵌入，从而允许高效地计算相似性和推理任务。
- en: In this blog post, we have discussed various embedding algorithms, including
    TransE, TransH, TransR, DistMult, ComplEx, and highlighted their strengths and
    weaknesses. Overall, knowledge graph embedding algorithms have shown great promise
    in a wide range of applications, including question answering, recommender systems,
    and natural language processing. As the field continues to evolve, we can expect
    to see even more powerful and effective embedding algorithms that can handle increasingly
    large and complex knowledge graphs.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇博客文章中，我们讨论了各种嵌入算法，包括 TransE、TransH、TransR、DistMult、ComplEx，并强调了它们的优缺点。总体而言，知识图谱嵌入算法在问答系统、推荐系统和自然语言处理等广泛应用中表现出巨大的潜力。随着领域的不断发展，我们可以期待看到更强大和有效的嵌入算法，它们能够处理越来越大和复杂的知识图谱。
- en: In the next blog post of this series, we will present some concrete recommendation
    system use cases where the use of KGE is beneficial to improve the recommendation
    accuracy.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在本系列的下一篇博客中，我们将介绍一些具体的推荐系统用例，其中 KGE 的使用有助于提高推荐准确性。
- en: 'References:'
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献：
- en: '[1] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, and Jeffrey Dean.
    Distributed representations of words and phrases and their compositionality. In
    Proceedings of the 26th International Conference on Neural Information Processing
    Systems — Volume 2, NIPS’13, page 3111–3119, Red Hook, NY, USA, 2013\. Curran
    Associates Inc.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, 和 Jeffrey Dean.
    词汇和短语的分布式表征及其组合性。发表于第26届神经信息处理系统国际会议论文集——第2卷，NIPS’13，页码3111–3119，美国纽约红钩，2013年。Curran
    Associates Inc.'
- en: '[2] Tomás Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efficient estimation
    of word representations in vector space. In Yoshua Bengio and Yann LeCun, editors,
    1st International Conference on Learning Representations, ICLR 2013, Scottsdale,
    Arizona, USA, May 2–4, 2013, Workshop Track Proceedings, 2013.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] Tomás Mikolov, Kai Chen, Greg Corrado, 和 Jeffrey Dean. 向量空间中词汇表征的高效估计。由Yoshua
    Bengio和Yann LeCun编辑，第1届国际学习表征会议，ICLR 2013，美国亚利桑那州斯科茨代尔，2013年5月2–4日，研讨会论文集，2013年。'
- en: '[3] Bryan Perozzi, Rami Al-Rfou, and Steven Skiena. Deepwalk: Online learning
    of social representations. In Proceedings of the 20th ACM SIGKDD international
    conference on Knowledge discovery and data mining, pages 701–710, 2014.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] Bryan Perozzi, Rami Al-Rfou, 和 Steven Skiena. Deepwalk：社交表征的在线学习。发表于第20届ACM
    SIGKDD国际知识发现与数据挖掘会议论文集，页码701–710，2014年。'
- en: '[4] Aditya Grover and Jure Leskovec. Node2vec: Scalable feature learning for
    networks. In 22Nd ACM SIGKDD International Conference on Knowledge Discovery and
    Data Mining, pages 855–864, New York, NY, USA, 2016\. ACM.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] Aditya Grover 和 Jure Leskovec. Node2vec：网络的可扩展特征学习。发表于第22届ACM SIGKDD国际知识发现与数据挖掘会议论文集，页码855–864，美国纽约，2016年。ACM出版社。'
- en: '[5] Nan Wang, Hongning Wang, Yiling Jia, and Yue Yin. Explainable recommendation
    via multi-task learning in opinionated text data. In The 41st International ACM
    SIGIR Conference on Research and Development in Information Retrieval, SIGIR ’18,
    page 165–174, New York, NY, USA, 2018\. Association for Computing Machinery.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] Nan Wang, Hongning Wang, Yiling Jia, 和 Yue Yin. 通过多任务学习解释性推荐于带有观点的文本数据。发表于第41届国际ACM
    SIGIR信息检索研究与发展会议，SIGIR ’18，页码165–174，美国纽约，2018年。计算机协会。'
- en: '[6] Antoine Bordes, Nicolas Usunier, Alberto Garcia-Duran, Jason Weston, and
    Oksana Yakhnenko. Translating embeddings for modeling multi-relational data. In
    C. J. C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K. Q. Weinberger, editors,
    Advances in Neural Information Processing Systems, volume 26, Lake Tahoe, Nevada,
    United States, 2013\. Curran Associates, Inc.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '[6] Antoine Bordes, Nicolas Usunier, Alberto Garcia-Duran, Jason Weston, 和
    Oksana Yakhnenko. 翻译嵌入用于建模多关系数据。由C. J. C. Burges, L. Bottou, M. Welling, Z. Ghahramani,
    和 K. Q. Weinberger编辑，神经信息处理系统进展，第26卷，美国内华达州湖塔霍，2013年。Curran Associates, Inc.'
- en: '[7] Zhen Wang, Jianwen Zhang, Jianlin Feng, and Zheng Chen. Knowledge graph
    embedding by translating on hyperplanes. In Proceedings of the Twenty-Eighth AAAI
    Conference on Artificial Intelligence, AAAI’14, pages 1112–1119, Québec City,
    Québec, Canada, 2014\. AAAI Press.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '[7] Zhen Wang, Jianwen Zhang, Jianlin Feng, 和 Zheng Chen. 通过在超平面上转换进行知识图谱嵌入。发表于第二十八届AAAI人工智能会议论文集，AAAI’14，页码1112–1119，加拿大魁北克市，2014年。AAAI出版社。'
- en: '[8] Yankai Lin, Zhiyuan Liu, Maosong Sun, Yang Liu, and Xuan Zhu. Learning
    entity and relation embeddings for knowledge graph completion. In Proceedings
    of the TwentyNinth AAAI Conference on Artificial Intelligence, AAAI’15, pages
    2181–2187, Austin, Texas, 2015\. AAAI Press.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '[8] Yankai Lin, Zhiyuan Liu, Maosong Sun, Yang Liu, 和 Xuan Zhu. 学习实体和关系嵌入以完成知识图谱。发表于第二十九届AAAI人工智能会议论文集，AAAI’15，页码2181–2187，美国德克萨斯州奥斯汀，2015年。AAAI出版社。'
- en: '[9] Maximilian Nickel, Volker Tresp, and Hans-Peter Kriegel. A three-way model
    for collective learning on multi-relational data. In Proceedings of the 28th International
    Conference on International Conference on Machine Learning, ICML’11, pages 809–816,
    Madison, WI, USA, 2011\. Omnipress.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '[9] Maximilian Nickel, Volker Tresp, 和 Hans-Peter Kriegel. 一种用于多关系数据集体学习的三元模型。发表于第28届国际机器学习会议论文集，ICML’11，页码809–816，美国威斯康星州麦迪逊，2011年。Omnipress出版社。'
- en: '[10] Richard Socher, Danqi Chen, Christopher D Manning, and Andrew Ng. Reasoning
    with neural tensor networks for knowledge base completion. In C. J. C. Burges,
    L. Bottou, M. Welling, Z. Ghahramani, and K. Q. Weinberger, editors, Advances
    in Neural Information Processing Systems, volume 26, Lake Tahoe, Nevada, United
    States, 2013\. Curran Associates, Inc'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '[10] Richard Socher, Danqi Chen, Christopher D Manning, 和 Andrew Ng. 使用神经张量网络进行知识库补全推理。由C.
    J. C. Burges, L. Bottou, M. Welling, Z. Ghahramani, 和 K. Q. Weinberger编辑，神经信息处理系统进展，第26卷，美国内华达州湖塔霍，2013年。Curran
    Associates, Inc.'
