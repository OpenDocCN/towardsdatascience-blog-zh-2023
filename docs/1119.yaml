- en: Using GPT-3.5-Turbo and GPT-4 for Predicting Humanitarian Data Categories
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用GPT-3.5-Turbo和GPT-4进行人道主义数据类别预测
- en: 原文：[https://towardsdatascience.com/using-gpt-3-5-turbo-and-gpt-4-to-apply-text-defined-data-quality-checks-on-humanitarian-datasets-6f02219c693c?source=collection_archive---------5-----------------------#2023-03-29](https://towardsdatascience.com/using-gpt-3-5-turbo-and-gpt-4-to-apply-text-defined-data-quality-checks-on-humanitarian-datasets-6f02219c693c?source=collection_archive---------5-----------------------#2023-03-29)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/using-gpt-3-5-turbo-and-gpt-4-to-apply-text-defined-data-quality-checks-on-humanitarian-datasets-6f02219c693c?source=collection_archive---------5-----------------------#2023-03-29](https://towardsdatascience.com/using-gpt-3-5-turbo-and-gpt-4-to-apply-text-defined-data-quality-checks-on-humanitarian-datasets-6f02219c693c?source=collection_archive---------5-----------------------#2023-03-29)
- en: '[](https://medium.com/@astrobagel?source=post_page-----6f02219c693c--------------------------------)[![Matthew
    Harris](../Images/4fa3264bb8a028633cd8d37093c16214.png)](https://medium.com/@astrobagel?source=post_page-----6f02219c693c--------------------------------)[](https://towardsdatascience.com/?source=post_page-----6f02219c693c--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----6f02219c693c--------------------------------)
    [Matthew Harris](https://medium.com/@astrobagel?source=post_page-----6f02219c693c--------------------------------)'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@astrobagel?source=post_page-----6f02219c693c--------------------------------)[![Matthew
    Harris](../Images/4fa3264bb8a028633cd8d37093c16214.png)](https://medium.com/@astrobagel?source=post_page-----6f02219c693c--------------------------------)[](https://towardsdatascience.com/?source=post_page-----6f02219c693c--------------------------------)[![数据科学之道](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----6f02219c693c--------------------------------)
    [Matthew Harris](https://medium.com/@astrobagel?source=post_page-----6f02219c693c--------------------------------)'
- en: ·
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F4a2cd25b8ff9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fusing-gpt-3-5-turbo-and-gpt-4-to-apply-text-defined-data-quality-checks-on-humanitarian-datasets-6f02219c693c&user=Matthew+Harris&userId=4a2cd25b8ff9&source=post_page-4a2cd25b8ff9----6f02219c693c---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----6f02219c693c--------------------------------)
    ·23 min read·Mar 29, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F6f02219c693c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fusing-gpt-3-5-turbo-and-gpt-4-to-apply-text-defined-data-quality-checks-on-humanitarian-datasets-6f02219c693c&user=Matthew+Harris&userId=4a2cd25b8ff9&source=-----6f02219c693c---------------------clap_footer-----------)'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F4a2cd25b8ff9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fusing-gpt-3-5-turbo-and-gpt-4-to-apply-text-defined-data-quality-checks-on-humanitarian-datasets-6f02219c693c&user=Matthew+Harris&userId=4a2cd25b8ff9&source=post_page-4a2cd25b8ff9----6f02219c693c---------------------post_header-----------)
    发表在[数据科学之道](https://towardsdatascience.com/?source=post_page-----6f02219c693c--------------------------------)
    · 23分钟阅读 · 2023年3月29日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F6f02219c693c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fusing-gpt-3-5-turbo-and-gpt-4-to-apply-text-defined-data-quality-checks-on-humanitarian-datasets-6f02219c693c&user=Matthew+Harris&userId=4a2cd25b8ff9&source=-----6f02219c693c---------------------clap_footer-----------)'
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F6f02219c693c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fusing-gpt-3-5-turbo-and-gpt-4-to-apply-text-defined-data-quality-checks-on-humanitarian-datasets-6f02219c693c&source=-----6f02219c693c---------------------bookmark_footer-----------)![](../Images/cdad9a981470d96cf832f828beeed3b8.png)'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F6f02219c693c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fusing-gpt-3-5-turbo-and-gpt-4-to-apply-text-defined-data-quality-checks-on-humanitarian-datasets-6f02219c693c&source=-----6f02219c693c---------------------bookmark_footer-----------)![](../Images/cdad9a981470d96cf832f828beeed3b8.png)'
- en: Image created by [Stable Diffusion](https://stablediffusionweb.com/#demo) with
    prompt ‘Predicting Cats’.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图像由[稳定扩散](https://stablediffusionweb.com/#demo)创建，提示词为“预测猫”。
- en: '*TL;DR*'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '*总结*'
- en: '*In this article, I explore using GPT-3.5-Turbo and GPT-4 to categorize datasets
    without the need for labeled data or model training, by prompting the model with
    data excerpts and category definitions. Using a small sample of categorized ‘Data
    Grid’ datasets found on the amazing Humanitarian Data Exchange (HDX), zero-shot
    prompting of GPT-4 resulted in 96% accuracy when predicting category and 89% accuracy
    when predicting both category and sub-category. GPT-4 outperformed GPT-3.5-turbo
    for the same prompts, with 96% accuracy versus 66% for category. Especially useful
    was that the model could provide reasoning for its predictions which helped to
    identify improvements to the process. This is just a quick analysis involving
    a small number of records due to cost limitations, but it shows some promise for
    using Large Language Models for data quality checks and summarization. Limitations
    exist due to the maximum number of tokens allowed in prompts affecting the amount
    of data that can be included in data excerpts, as well as performance and cost
    challenges — especially if you’re a small non-profit! — at this early stage of
    commercial generative AI.*'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '*在本文中，我探讨了使用GPT-3.5-Turbo和GPT-4对数据集进行分类，而不需要标记数据或模型训练，通过向模型提供数据摘录和类别定义。使用从令人惊叹的人道主义数据交换（HDX）找到的一小部分已分类的“数据网格”数据集，GPT-4的零-shot提示在预测类别时达到了96%的准确率，而在预测类别和子类别时达到了89%的准确率。GPT-4在相同提示下的表现优于GPT-3.5-Turbo，类别准确率为96%对66%。尤其有用的是，模型能够提供其预测的推理，这有助于识别改进过程。这只是由于成本限制而涉及少量记录的快速分析，但它显示了使用大型语言模型进行数据质量检查和总结的一些前景。由于提示中允许的最大令牌数量影响数据摘录中可以包含的数据量，以及性能和成本挑战——特别是如果你是一个小型非营利组织！——在商业生成AI的早期阶段存在局限性。*'
- en: The [Humanitarian Data Exchange](https://data.humdata.org/) (HDX) platform has
    a great feature called the [HDX Data Grid](https://centre.humdata.org/introducing-the-hdx-data-grid-a-way-to-find-and-fill-data-gaps/)
    which provides an overview of high-quality data coverage in six key crisis categories
    by country, see [here](https://data.humdata.org/group/tcd) for an example for
    Chad. The datasets which make it into the grid undergo a [series of rigorous tests](https://humanitarian.atlassian.net/wiki/spaces/HDX/pages/682393601/Data+Grid+Data+Completeness+Curation+Procedures)
    by the HDX team to determine coverage and quality, the first of which is to determine
    if the dataset is in an approved category.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '[人道主义数据交换](https://data.humdata.org/)（HDX）平台有一个很棒的功能叫做[HDX数据网格](https://centre.humdata.org/introducing-the-hdx-data-grid-a-way-to-find-and-fill-data-gaps/)，它提供了按国家划分的六个关键危机类别的高质量数据覆盖概述，查看[这里](https://data.humdata.org/group/tcd)了解乍得的例子。进入网格的数据集会经过HDX团队[一系列严格的测试](https://humanitarian.atlassian.net/wiki/spaces/HDX/pages/682393601/Data+Grid+Data+Completeness+Curation+Procedures)以确定覆盖范围和质量，其中第一个测试是确定数据集是否在批准的类别中。'
- en: I wondered if perhaps Large Language Models (LLMs) might be an efficient way
    to apply data quality and classification rules in situations where there might
    not be any labeled training data. It would also be convenient to provide rules
    in a human-readable text form that non-technical teams could easily maintain,
    and use these directly in order to eliminate the requirement for features engineering
    and model management.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 我在想，也许大型语言模型（LLMs）可能是一个有效的方法来应用数据质量和分类规则，在那些可能没有标记训练数据的情况下。这也很方便，以人类可读的文本形式提供规则，非技术团队可以轻松维护，并直接使用这些规则以消除对特征工程和模型管理的需求。
- en: Oh, and I also recently got early access to GPT-4 and wanted to take it for
    a bit of a spin! 🙂 … So decided to also do some analysis comparing performance
    with GPT-3.5-Turbo.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 哦，我最近也获得了GPT-4的早期访问权限，想要试一试！🙂……所以决定也进行一些分析，比较GPT-3.5-Turbo的表现。
- en: Is the Dataset in an Approved Category?
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据集是否在已批准的类别中？
- en: Looking at [The State of Humanitarian Data 2023 Annex B](https://data.humdata.org/dataset/2048a947-5714-4220-905b-e662cbcd14c8/resource/9d4121c6-b32b-4eb8-a707-209c79241970/download/state-of-open-humanitarian-data-2023.pdf),
    which outlines the criteria and categories used when assessing if data is of sufficient
    quality and coverage …
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 查看[《2023年人道主义数据现状 附录B》](https://data.humdata.org/dataset/2048a947-5714-4220-905b-e662cbcd14c8/resource/9d4121c6-b32b-4eb8-a707-209c79241970/download/state-of-open-humanitarian-data-2023.pdf)，其中概述了在评估数据是否具有足够质量和覆盖范围时使用的标准和类别……
- en: The first step in determining whether a dataset should be included in a Data
    Grid is to check whether the dataset meets the thematic requirement defined in
    Annex A. Datasets that are not considered relevant are automatically excluded.
  id: totrans-15
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 确定数据集是否应包含在数据网格中的第一步是检查数据集是否符合附录A中定义的主题要求。不相关的数据集将被自动排除。
- en: The categories in Annex A are …
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 附录A中的类别是……
- en: '![](../Images/e935a601b4060d8d5788b7c76f5754aa.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e935a601b4060d8d5788b7c76f5754aa.png)'
- en: Accepted data categories for HDX Data Grid datasets (See HDX’s yearly report,
    Annex A [[1](https://data.humdata.org/dataset/2048a947-5714-4220-905b-e662cbcd14c8/resource/9d4121c6-b32b-4eb8-a707-209c79241970/download/state-of-open-humanitarian-data-2023.pdf)])
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: HDX数据网格数据集的接受数据类别（见HDX年度报告，附录A [[1](https://data.humdata.org/dataset/2048a947-5714-4220-905b-e662cbcd14c8/resource/9d4121c6-b32b-4eb8-a707-209c79241970/download/state-of-open-humanitarian-data-2023.pdf)]）
- en: We could write a classifier to assign these categories to our datasets based
    on the data they contain, but we only know categories for the subset of datasets
    that were approved for HDX Data Grid. If prompting alone can classify our data
    without having to manually label it, that would be fantastic. This is a zero-shot
    task[[2](https://arxiv.org/pdf/2005.14165.pdf)], one of the amazing properties
    of Large Language Models where classification can occur without training specifically
    for the task or providing examples.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以编写分类器将这些类别分配给我们的数据集，但我们只知道已批准的HDX数据网格数据集的子集的类别。如果仅通过提示就能对我们的数据进行分类而不需要手动标记，那将是太棒了。这是一个零-shot任务[[2](https://arxiv.org/pdf/2005.14165.pdf)]，这是大语言模型的一个惊人特性，即可以在没有专门为任务训练或提供示例的情况下进行分类。
- en: Predicting Dataset Category for a Single Table
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为单个表预测数据集类别
- en: Let’s read the categories data and use it to generate prompt text defining each
    one …
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们读取类别数据，并使用它生成定义每一类的提示文本……
- en: '[PRE0]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Which gives …
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 这给出……
- en: '[PRE1]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Here is a test file related to agriculture which is an unsupported category
    and which doesn’t appear on HDX’s Data Grid …
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一个与农业相关的测试文件，这是一个不受支持的类别，并且不出现在HDX的数据网格中……
- en: '[PRE2]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '![](../Images/e2483882c28d40f0030fe8f55976ba5d.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e2483882c28d40f0030fe8f55976ba5d.png)'
- en: Excerpt of a dataset table which doesn’t fall into one of the supported HDX
    categories
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 一个数据集表的摘录，该表不属于HDX支持的类别之一
- en: In the above, I have intentionally avoided parsing the table to tidy things
    up (see [here](https://medium.com/towards-data-science/parsing-irregular-spreadsheet-tables-in-humanitarian-datasets-with-some-help-from-gpt-3-57efb3d80d45)
    for more on that). Instead, we’ll throw the raw table at GPT to see how it performs.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述内容中，我故意避免了对表格进行解析以整理内容（有关更多信息，请参见[这里](https://medium.com/towards-data-science/parsing-irregular-spreadsheet-tables-in-humanitarian-datasets-with-some-help-from-gpt-3-57efb3d80d45)）。相反，我们将原始表格扔给GPT，看看它的表现如何。
- en: Represented as a CSV string for the prompt, the table looks like this …
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 作为提示的CSV字符串表示，表格如下所示……
- en: '[PRE3]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: For the prompt, we’ll combine category definitions into one chat prompt, and
    some instructions and the table being analyzed into a second …
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 对于提示，我们将类别定义合并为一个聊天提示，并将一些指令和正在分析的表格合并为第二个……
- en: '[PRE4]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: So prompt 1 …
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 所以提示1……
- en: '[PRE5]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: And prompt 2 …
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 然后提示2……
- en: '[PRE6]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Let’s try this with both **GPT-3.5-turbo** and **GPT-4** …
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试使用**GPT-3.5-turbo**和**GPT-4**……
- en: '[PRE7]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: We get …
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到……
- en: '[PRE8]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Both GPT-3.5-turbo and GPT-4 worked perfectly and identified that our table
    does *not* fall into one of the required categories (it’s related to agriculture).
    I also like the reasoning, which is exactly correct at least in this one example.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '**GPT-3.5-turbo**和**GPT-4**都完美地工作，并识别出我们的表格*不*属于所需类别之一（它与农业相关）。我也喜欢这种推理，至少在这个例子中完全正确。'
- en: Let’s try with a table that is in a supported category, [Food prices for Chad](https://data.humdata.org/dataset/wfp-food-prices-for-chad)
    as found on the [Chad HDX Data Grid](https://data.humdata.org/group/tcd). The
    CSV string for this file, taking the top 20 rows, looks like this …
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 我们用一个在受支持类别中的表进行尝试，[查德的食品价格](https://data.humdata.org/dataset/wfp-food-prices-for-chad)，如在[查德HDX数据网格](https://data.humdata.org/group/tcd)上找到的。这个文件的CSV字符串，取前20行，如下所示……
- en: '[PRE9]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Prompting with the same format we get …
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 使用相同格式的提示，我们得到……
- en: '[PRE10]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'So again, both models were correct. The category for this dataset is indeed
    ‘Food Security & Nutrition : Food Prices’.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 所以再说一次，两种模型都是正确的。该数据集的类别确实是“食品安全与营养：食品价格”。
- en: OK, looking good for some quick examples using a single table. What about identifying
    the category based on the contents of *multiple* tables?
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，对于使用单个表进行的一些快速示例，看起来不错。那么，基于*多个*表的内容来识别类别呢？
- en: Predicting a Dataset Category Using Excerpts from Multiple Tables
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用来自多个表的摘录预测数据集类别
- en: In HDX a dataset can have multiple ‘Resources’ (files), and for data in Excel,
    these files can have multiple tables in sheets. So just looking at just one table
    from the dataset might not tell the whole story, we need to make a decision based
    on multiple tables. This is important because among all the tables in a dataset
    there might be tabs for documentation about the dataset, field lookups, and more,
    which by themselves wouldn’t be enough to deduce the category of all data in the
    dataset.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在HDX中，一个数据集可以有多个“资源”（文件），而对于Excel中的数据，这些文件可能在工作表中包含多个表。因此，只查看数据集中的一个表可能无法完全了解情况，我们需要根据多个表做出决策。这一点很重要，因为在数据集中的所有表中可能会有关于数据集、字段查找等的文档标签，而这些标签本身不足以推断数据集中所有数据的类别。
- en: Before ChatGPT API was launched, this would have been difficult due to token
    limits. However, ChatGPT allows us to specify [multiple prompts](https://platform.openai.com/docs/guides/chat)
    as well as having [increased token limits](https://platform.openai.com/docs/guides/rate-limits/overview).
    As we’ll see, still a limiting factor, but an improvement on previous models.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在ChatGPT API推出之前，这会由于令牌限制而变得困难。然而，ChatGPT允许我们指定[多个提示](https://platform.openai.com/docs/guides/chat)，并且有[增加的令牌限制](https://platform.openai.com/docs/guides/rate-limits/overview)。如我们所见，这仍然是一个限制因素，但比以前的模型有所改进。
- en: The sample data for this analysis — provided in the notebook repo — was extracted
    from HDX by …
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 本分析的样本数据——在笔记本仓库中提供——是从HDX中提取的，由…
- en: Looping over datasets
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 遍历数据集
- en: For each dataset loop over files
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于每个数据集，遍历文件
- en: For each tabular file, download it
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于每个表格文件，下载它
- en: For each tab in the file create an excerpt of the table (first 20 rows) in CSV
    format
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于文件中的每个标签，创建一个表格摘录（前20行）以CSV格式
- en: '*Note: I haven’t included this code to avoid too much traffic on HDX, but if
    interested in this code, message me here on Medium.*'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '*注意：我没有包含这段代码以避免HDX上的流量过多，但如果对这段代码感兴趣，可以在Medium上给我留言。*'
- en: So each dataset has a field like this …
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 所以每个数据集都有一个这样的字段…
- en: '[PRE11]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Having this structure for each dataset allows us to generate multiple prompts
    for each table …
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个数据集，这种结构允许我们为每个表生成多个提示…
- en: '[PRE12]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Which generates prompts like this …
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 这会生成像这样的提示…
- en: '**Prompt 1 — Defining the categories**'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '**提示 1 — 定义类别**'
- en: '[PRE13]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '**Prompt 2 — Specifying Dataset name and introducing table excerpts**'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '**提示 2 — 指定数据集名称并介绍表格摘录**'
- en: '[PRE14]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '**Prompt 3, 4, etc —Providing Table Excerpts**'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '**提示 3、4 等 — 提供表格摘录**'
- en: '[PRE15]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '**Final Prompt — Our request to classify the data**'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '**最终提示 — 我们请求对数据进行分类**'
- en: '[PRE16]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'You’ll notice in the last prompt that we’ve been a bit demanding:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 你会注意到在最后的提示中，我们要求有点多：
- en: We request that the model indicates if the data *doesn’t* align with our categories
    so we catch negative cases and the model doesn’t try and assign a category to
    every dataset. Some will fall outside of the approved categories
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们要求模型指明数据*不*符合我们的类别，以便我们捕捉负面情况，模型不会尝试为每个数据集分配一个类别。有些将不符合批准的类别
- en: Request that category ‘exactly matches’. Without this GPT-3.5-Turbo would merrily
    construct new ones!
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 请求类别“完全匹配”。如果没有这个要求，GPT-3.5-Turbo可能会随意构造新的类别！
- en: If the model does identify a category, wrap it in ‘|’ for easier parsing
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果模型确实识别出一个类别，将其用‘|’括起来，以便更容易解析
- en: We ask the model to provide its reasoning as this has been shown to improve
    results [[3](https://arxiv.org/pdf/2205.11916.pdf)]. It’s also useful to see why
    the category decision was made to highlight cases of hallucination
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们要求模型提供其推理过程，因为这已被证明可以改善结果[[3](https://arxiv.org/pdf/2205.11916.pdf)]。了解类别决策的原因也有助于突出虚假信息的情况
- en: Finally, for our discussion later, we request the second most likely category
    as well
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，为了后续讨论，我们还请求第二个最可能的类别
- en: Also, if you look closely at the code in the predict function, I have used a
    [temperature](https://platform.openai.com/docs/api-reference/chat/create) of 0.0
    for this study. Temperature controls how random the output is, and since we want
    things to be nice and specific rather than text describing quantum physics in
    the voice of [cookie monster](https://www.youtube.com/watch?v=Ye8mB6VsUHw), I
    set it as low as possible.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，如果你仔细查看预测函数中的代码，我在这项研究中使用了[温度](https://platform.openai.com/docs/api-reference/chat/create)为0.0。温度控制输出的随机程度，由于我们希望结果既准确又具体，而不是描述量子物理的文本，所以我将其设置为尽可能低。
- en: Making our predictions …
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 生成我们的预测…
- en: '[PRE17]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: How did we do?
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 我们做得怎么样？
- en: '[PRE18]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '*Note: Though we passed in 150 datasets to predict, the API timed out quite
    a bit for GPT-4 and calls weren’t retried to save costs. This is entirely to be
    expected for GPT-4 which is in early preview. Some prompts also exceeded the token
    length for GPT-3.5-Turbo. The following results therefore apply to 53 predictions
    made by both GPT-3.5-turbo and GPT-4.*'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '*注意：虽然我们提供了150个数据集进行预测，但GPT-4的API时常超时，且未重试调用以节省成本。这对于处于早期预览阶段的GPT-4是完全可以预期的。有些提示也超出了GPT-3.5-Turbo的令牌长度。因此，以下结果适用于GPT-3.5-turbo和GPT-4做出的53个预测。*'
- en: 'For predicting the category only, for example, “Coordination & Context” when
    the full category and sub-category is “Coordination & Context : Humanitarian Access”
    …'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '比如，仅预测类别，如“Coordination & Context”，当完整类别和子类别为“Coordination & Context : Humanitarian
    Access”时……'
- en: '[PRE19]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: GPT-4 was nearly always able to identify the correct category (**96%** accuracy),
    performing significantly better than GPT-3.5-turbo for the same prompts (66% accuracy).
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-4几乎总是能够识别正确的类别（**96%**准确率），在相同提示下表现显著优于GPT-3.5-turbo（66%准确率）。
- en: For predicting the whole category *and* sub-category together …
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 对于同时预测整个类别*和*子类别……
- en: '[PRE20]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Again GPT-4 outperformed GPT-3.5 by a significant margin. **89%** accuracy is
    actually pretty good given ….
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 再次强调，GPT-4比GPT-3.5表现显著优越。**89%**的准确率实际上相当不错，鉴于……
- en: '**We *only* provided a set of text rules and didn’t label data, train a classifier
    or provide any examples**.'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**我们*仅仅*提供了一组文本规则，没有标记数据、训练分类器或提供任何示例**。'
- en: In fact, if we look at the examples where it failed the predictions …
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，如果我们查看那些预测失败的示例……
- en: '[PRE21]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: We get …
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到……
- en: '[PRE22]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: A couple of things jump out. Datasets like ‘mozambique-attacks-on-aid-operations-education-health-and-protection’,
    have a mix of data files related to both healthcare and attacks.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 有几件事引起了注意。像‘mozambique-attacks-on-aid-operations-education-health-and-protection’这样的数据集，包含了与医疗保健和攻击相关的数据文件。
- en: '![](../Images/0f406a7ab9b745351ab0fb8adb6ef54a.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0f406a7ab9b745351ab0fb8adb6ef54a.png)'
- en: So assuming one category per dataset might not be the best way to frame the
    problem, datasets are reused across categories.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，假设每个数据集只有一个类别可能不是解决问题的最佳方式，数据集在类别之间被重复使用。
- en: In about half the cases where GPT-4 was incorrect, the second-place category
    it predicted was correct. Looking at the model output for one of these cases,
    [Ukranian border crossings](https://data.humdata.org/dataset/ukraine-border-crossings)
    …
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在GPT-4错误的约一半的案例中，它预测的第二名类别是正确的。查看这些案例中的一个模型输出，[乌克兰边界穿越](https://data.humdata.org/dataset/ukraine-border-crossings)……
- en: '[PRE23]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'What is pretty cool is its explanation of *why* it didn’t choose ‘Coordination
    & Context : Humanitarian Access’, because ‘*…it does not specifically focus on
    access constraints*’. Here is the category definition …'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '很酷的一点是它解释了*为什么*没有选择‘Coordination & Context : Humanitarian Access’，因为‘*……它并不特别关注访问限制*’。这是类别定义……'
- en: 'Coordination & Context : Humanitarian Access: Tabular or vector data describing
    the location of natural hazards, permissions, active fighting, or other access
    constraints that impact the delivery of humanitarian interventions.'
  id: totrans-100
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 'Coordination & Context : Humanitarian Access：表格或矢量数据，描述自然灾害、许可、激烈战斗或其他影响人道干预交付的访问限制的位置。'
- en: So GPT-4 seems to be following the category rule to the word. There is some
    more nuance to the classification that the HDX team applies where border-crossing
    datasets are very reasonably related to humanitarian access. So perhaps one way
    to improve model prediction in this particular case would be to add additional
    text to the category definition indicating border crossing can relate to humanitarian
    access.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，GPT-4似乎严格遵循类别规则。HDX团队应用的分类有一些更细致的区别，其中跨境数据集与人道主义访问有很合理的关系。因此，也许提高模型在这种情况下预测的一个方法是向类别定义中添加额外的文本，指明边界穿越可能与人道主义访问相关。
- en: The takeaway here is that GPT-4 performed amazingly well and that the few incorrect
    predictions were due to how the problem was framed poorly by me (datasets can
    have multiple categories) and perhaps text used to define categories.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的关键是GPT-4表现非常出色，而少数不正确的预测是由于我对问题的框定不当（数据集可以有多个类别），以及定义类别的文本可能存在的问题。
- en: Conclusion
  id: totrans-103
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: The technique seems to be quite promising. We were able to achieve some good
    results **without any requirement to set labels, train models, or provide examples
    in prompts.** Additionally, the data summarization capabilities of models like
    GPT-4 are really impressive, helping with debugging model predictions, and might
    also be a nice way to provide quick data overviews.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 该技术看起来非常有前途。我们能够获得一些良好的结果，**无需设置标签、训练模型或在提示中提供示例**。此外，像 GPT-4 这样的模型的数据总结能力确实令人印象深刻，帮助调试模型预测，也可能是提供快速数据概览的不错方法。
- en: 'However, there are some caveats:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，存在一些警示：
- en: The amount of data used for this study was very limited due to cost and the
    fact GPT-4 is only in an early preview. Future studies would need to use more
    data of course.
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于成本和 GPT-4 仍处于早期预览阶段，这项研究所使用的数据量非常有限。未来的研究当然需要使用更多的数据。
- en: Prompt length is currently a limiting factor, the study above only included
    datasets with less than 4 tables to avoid breaching token limits when prompting
    with table excerpts. HDX datasets can have more tables than this and having larger
    table excerpts might have been desirable in some cases. Vendors such as OpenAI
    seem to be progressively increasing token limits, so over time perhaps this becomes
    less of an issue.
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 目前提示长度是一个限制因素，上述研究只包括了少于4个表的数据集，以避免在提示表格摘录时超出 token 限制。HDX 数据集可能包含比这更多的表格，在某些情况下，拥有更大的表格摘录可能会更有价值。像
    OpenAI 这样的供应商似乎在逐步增加 token 限制，因此随着时间的推移，这可能会变得不那么成为问题。
- en: Likely related to being an early preview, GPT-4 model performance was very slow,
    taking 20 seconds per prompt to complete.
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于早期预览的原因，GPT-4 模型的性能非常慢，每个提示完成需要20秒。
- en: The framing of the problem was not ideal, for example, assuming a dataset can
    only have one category. It sufficed to illustrate the potential of Large Language
    Models for assessing data quality and summarization, but a slightly different
    approach in the future might yield better results. For example, predicting top
    dataset candidates for a given category per country for datasets on the HDX platform.
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 问题的框架并不理想，例如，假设一个数据集只能有一个类别。虽然足以展示大型语言模型在评估数据质量和总结方面的潜力，但未来稍微不同的方法可能会产生更好的结果。例如，为
    HDX 平台上的数据集预测每个国家的顶级数据集候选者。
- en: Being able to specify data tests and questions about data in natural language
    is still pretty cool though!
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 能够用自然语言指定数据测试和数据问题仍然很酷！
- en: References
  id: totrans-111
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] OCHA, [State of Open Humanitarian Data 2023](https://data.humdata.org/dataset/2048a947-5714-4220-905b-e662cbcd14c8/resource/9d4121c6-b32b-4eb8-a707-209c79241970/download/state-of-open-humanitarian-data-2023.pdf)'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] OCHA, [2023年开放人道数据状态](https://data.humdata.org/dataset/2048a947-5714-4220-905b-e662cbcd14c8/resource/9d4121c6-b32b-4eb8-a707-209c79241970/download/state-of-open-humanitarian-data-2023.pdf)'
- en: '[2] Brown et al, [Language Models are Few Shot Learners](https://arxiv.org/pdf/2005.14165.pdf)
    (2020).'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] Brown 等人, [语言模型是少样本学习者](https://arxiv.org/pdf/2005.14165.pdf)（2020年）。'
- en: '[3] Kojima et al, [Large Language Models are Zero-shot reasoners](https://arxiv.org/abs/2205.11916).'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] Kojima 等人, [大型语言模型是零样本推理者](https://arxiv.org/abs/2205.11916)。'
- en: 'Schopf et al, [Evaluating Unsupervised Text Classification: Zero-Shot and similarity-based
    approaches](https://arxiv.org/abs/2211.16285)'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: Schopf 等人, [评估无监督文本分类：零样本和基于相似性的 approaches](https://arxiv.org/abs/2211.16285)
- en: Code for this analysis can be found in [this notebook](https://github.com/datakind/gpt-3-meta-data-discovery/blob/main/gpt-4-data-quality-tests.ipynb).
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 这个分析的代码可以在[这个笔记本](https://github.com/datakind/gpt-3-meta-data-discovery/blob/main/gpt-4-data-quality-tests.ipynb)中找到。
