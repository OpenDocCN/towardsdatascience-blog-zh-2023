["```py\nimport torch\nimport torch_explain as te\nfrom torch_explain import datasets\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\n\nx, c, y = datasets.xor(500)\nx_train, x_test, c_train, c_test, y_train, y_test = train_test_split(x, c, y, test_size=0.33, random_state=42)\n```", "```py\nconcept_encoder = torch.nn.Sequential(\n    torch.nn.Linear(x.shape[1], 10),\n    torch.nn.LeakyReLU(),\n    torch.nn.Linear(10, 8),\n    torch.nn.LeakyReLU(),\n    torch.nn.Linear(8, c.shape[1]),\n    torch.nn.Sigmoid(),\n)\ntask_predictor = torch.nn.Sequential(\n    torch.nn.Linear(c.shape[1], 8),\n    torch.nn.LeakyReLU(),\n    torch.nn.Linear(8, 1),\n)\nmodel = torch.nn.Sequential(concept_encoder, task_predictor)\n```", "```py\noptimizer = torch.optim.AdamW(model.parameters(), lr=0.01)\nloss_form_c = torch.nn.BCELoss()\nloss_form_y = torch.nn.BCEWithLogitsLoss()\nmodel.train()\nfor epoch in range(2001):\n    optimizer.zero_grad()\n\n    # generate concept and task predictions\n    c_pred = concept_encoder(x_train)\n    y_pred = task_predictor(c_pred)\n\n    # update loss\n    concept_loss = loss_form_c(c_pred, c_train)\n    task_loss = loss_form_y(y_pred, y_train)\n    loss = concept_loss + 0.2*task_loss\n\n    loss.backward()\n    optimizer.step()\n```", "```py\nc_pred = concept_encoder(x_test)\ny_pred = task_predictor(c_pred)\n\nconcept_accuracy = accuracy_score(c_test, c_pred > 0.5)\ntask_accuracy = accuracy_score(y_test, y_pred > 0)\n```", "```py\nc_different = torch.FloatTensor([0, 1])\nprint(f\"f({c_different}) = {int(task_predictor(c_different).item() > 0)}\")\n\nc_equal = torch.FloatTensor([1, 1])\nprint(f\"f({c_different}) = {int(task_predictor(c_different).item() > 0)}\")\n```", "```py\nx, c, y = datasets.trigonometry(500)\nx_train, x_test, c_train, c_test, y_train, y_test = train_test_split(x, c, y, test_size=0.33, random_state=42)\n```", "```py\nx, c, y = datasets.trigonometry(500)\nx_train, x_test, c_train, c_test, y_train, y_test = train_test_split(x, c, y, test_size=0.33, random_state=42)\n```", "```py\nembedding_size = 8\nconcept_encoder = torch.nn.Sequential(\n    torch.nn.Linear(x.shape[1], 10),\n    torch.nn.LeakyReLU(),\n    te.nn.ConceptEmbedding(10, c.shape[1], embedding_size),\n)\ntask_predictor = torch.nn.Sequential(\n    torch.nn.Linear(c.shape[1]*embedding_size, 8),\n    torch.nn.LeakyReLU(),\n    torch.nn.Linear(8, 1),\n)\nmodel = torch.nn.Sequential(concept_encoder, task_predictor)\n```", "```py\noptimizer = torch.optim.AdamW(model.parameters(), lr=0.01)\nloss_form_c = torch.nn.BCELoss()\nloss_form_y = torch.nn.BCEWithLogitsLoss()\nmodel.train()\nfor epoch in range(2001):\n    optimizer.zero_grad()\n\n    # generate concept and task predictions\n    c_emb, c_pred = concept_encoder(x_train)\n    y_pred = task_predictor(c_emb.reshape(len(c_emb), -1))\n\n    # compute loss\n    concept_loss = loss_form_c(c_pred, c_train)\n    task_loss = loss_form_y(y_pred, y_train)\n    loss = concept_loss + 0.2*task_loss\n\n    loss.backward()\n    optimizer.step()\n```", "```py\nc_emb, c_pred = concept_encoder.forward(x_test)\ny_pred = task_predictor(c_emb.reshape(len(c_emb), -1))\n\nconcept_accuracy = accuracy_score(c_test, c_pred > 0.5)\ntask_accuracy = accuracy_score(y_test, y_pred > 0)\n```", "```py\nfrom torch_explain.nn.concepts import ConceptReasoningLayer\nimport torch.nn.functional as F\n\nx, c, y = datasets.xor(500)\nx_train, x_test, c_train, c_test, y_train, y_test = train_test_split(x, c, y, test_size=0.33, random_state=42)\ny_train = F.one_hot(y_train.long().ravel()).float()\ny_test = F.one_hot(y_test.long().ravel()).float()\n\nembedding_size = 8\nconcept_encoder = torch.nn.Sequential(\n    torch.nn.Linear(x.shape[1], 10),\n    torch.nn.LeakyReLU(),\n    te.nn.ConceptEmbedding(10, c.shape[1], embedding_size),\n)\ntask_predictor = ConceptReasoningLayer(embedding_size, y_train.shape[1])\nmodel = torch.nn.Sequential(concept_encoder, task_predictor)\n```", "```py\noptimizer = torch.optim.AdamW(model.parameters(), lr=0.01)\nloss_form = torch.nn.BCELoss()\nmodel.train()\nfor epoch in range(2001):\n    optimizer.zero_grad()\n\n    # generate concept and task predictions\n    c_emb, c_pred = concept_encoder(x_train)\n    y_pred = task_predictor(c_emb, c_pred)\n\n    # compute loss\n    concept_loss = loss_form(c_pred, c_train)\n    task_loss = loss_form(y_pred, y_train)\n    loss = concept_loss + 0.2*task_loss\n\n    loss.backward()\n    optimizer.step()\n```", "```py\nc_emb, c_pred = concept_encoder.forward(x_test)\ny_pred = task_predictor(c_emb, c_pred)\n\nconcept_accuracy = accuracy_score(c_test, c_pred > 0.5)\ntask_accuracy = accuracy_score(y_test, y_pred > 0.5)\n```", "```py\nlocal_explanations = task_predictor.explain(c_emb, c_pred, 'local')\n```", "```py\n{'sample-id': 0,\n 'class': 'y_1',\n 'explanation': '~c_0 & c_1',\n 'attention': [-1.0, 1.0]}\n```", "```py\nglobal_explanations = task_predictor.explain(c_emb, c_pred, 'global')\n```", "```py\n[{'class': 'y_0', 'explanation': 'c_0 & c_1', 'count': 39},\n {'class': 'y_0', 'explanation': '~c_0 & ~c_1', 'count': 46},\n {'class': 'y_1', 'explanation': '~c_0 & c_1', 'count': 45},\n {'class': 'y_1', 'explanation': 'c_0 & ~c_1', 'count': 35}]\n```"]