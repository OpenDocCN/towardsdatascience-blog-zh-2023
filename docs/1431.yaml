- en: Build your own Transformer from scratch using Pytorch
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/build-your-own-transformer-from-scratch-using-pytorch-84c850470dcb?source=collection_archive---------0-----------------------#2023-04-26](https://towardsdatascience.com/build-your-own-transformer-from-scratch-using-pytorch-84c850470dcb?source=collection_archive---------0-----------------------#2023-04-26)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Building a Transformer model step by step in Pytorch
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://arjun-sarkar786.medium.com/?source=post_page-----84c850470dcb--------------------------------)[![Arjun
    Sarkar](../Images/de141f1ab68c2b85c9d7a1f31be0d9b5.png)](https://arjun-sarkar786.medium.com/?source=post_page-----84c850470dcb--------------------------------)[](https://towardsdatascience.com/?source=post_page-----84c850470dcb--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----84c850470dcb--------------------------------)
    [Arjun Sarkar](https://arjun-sarkar786.medium.com/?source=post_page-----84c850470dcb--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ffa31366b2eda&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuild-your-own-transformer-from-scratch-using-pytorch-84c850470dcb&user=Arjun+Sarkar&userId=fa31366b2eda&source=post_page-fa31366b2eda----84c850470dcb---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----84c850470dcb--------------------------------)
    ·7 min read·Apr 26, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F84c850470dcb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuild-your-own-transformer-from-scratch-using-pytorch-84c850470dcb&user=Arjun+Sarkar&userId=fa31366b2eda&source=-----84c850470dcb---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F84c850470dcb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuild-your-own-transformer-from-scratch-using-pytorch-84c850470dcb&source=-----84c850470dcb---------------------bookmark_footer-----------)![](../Images/3f54c2f78af8d6ca5052e7b2b898034a.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 1\. Photo by [Kevin Ku](https://unsplash.com/@ikukevk?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    on [Unsplash](https://unsplash.com/s/photos/deep-learning?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
  prefs: []
  type: TYPE_NORMAL
- en: In this tutorial, we will build a basic Transformer model from scratch using
    PyTorch. The Transformer model, introduced by Vaswani et al. in the paper “Attention
    is All You Need,” is a deep learning architecture designed for sequence-to-sequence
    tasks, such as machine translation and text summarization. It is based on self-attention
    mechanisms and has become the foundation for many state-of-the-art natural language
    processing models, like GPT and BERT.
  prefs: []
  type: TYPE_NORMAL
- en: 'To understand Transformer models in detail kindly visit these two articles:'
  prefs: []
  type: TYPE_NORMAL
- en: '[1\. All you need to know about ‘Attention’ and ‘Transformers’ — In-depth Understanding
    — Part 1](https://medium.com/towards-data-science/all-you-need-to-know-about-attention-and-transformers-in-depth-understanding-part-1-552f0b41d021)'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[2\. All you need to know about ‘Attention’ and ‘Transformers’ — In-depth Understanding
    — Part 2](https://medium.com/towards-data-science/all-you-need-to-know-about-attention-and-transformers-in-depth-understanding-part-2-bf2403804ada)'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To build our Transformer model, we’ll follow these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Import necessary libraries and modules
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Define the basic building blocks: Multi-Head Attention, Position-wise Feed-Forward
    Networks, Positional Encoding'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Build the Encoder and Decoder layers
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Combine Encoder and Decoder layers to create the complete Transformer model
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Prepare sample data
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Train the model
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Let’s start by importing the necessary libraries and modules.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Now, we’ll define the basic building blocks of the Transformer model.
  prefs: []
  type: TYPE_NORMAL
- en: Multi-Head Attention
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/e8b7aaf2849d3b0b0b0bd006f7dae3aa.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2\. Multi-Head Attention (source: image created by author)'
  prefs: []
  type: TYPE_NORMAL
- en: The Multi-Head Attention mechanism computes the attention between each pair
    of positions in a sequence. It consists of multiple “attention heads” that capture
    different aspects of the input sequence.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The MultiHeadAttention code initializes the module with input parameters and
    linear transformation layers. It calculates attention scores, reshapes the input
    tensor into multiple heads, and combines the attention outputs from all heads.
    The forward method computes the multi-head self-attention, allowing the model
    to focus on some different aspects of the input sequence.
  prefs: []
  type: TYPE_NORMAL
- en: Position-wise Feed-Forward Networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: The PositionWiseFeedForward class extends PyTorch’s nn.Module and implements
    a position-wise feed-forward network. The class initializes with two linear transformation
    layers and a ReLU activation function. The forward method applies these transformations
    and activation function sequentially to compute the output. This process enables
    the model to consider the position of input elements while making predictions.
  prefs: []
  type: TYPE_NORMAL
- en: Positional Encoding
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Positional Encoding is used to inject the position information of each token
    in the input sequence. It uses sine and cosine functions of different frequencies
    to generate the positional encoding.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: The PositionalEncoding class initializes with input parameters d_model and max_seq_length,
    creating a tensor to store positional encoding values. The class calculates sine
    and cosine values for even and odd indices, respectively, based on the scaling
    factor div_term. The forward method computes the positional encoding by adding
    the stored positional encoding values to the input tensor, allowing the model
    to capture the position information of the input sequence.
  prefs: []
  type: TYPE_NORMAL
- en: Now, we’ll build the Encoder and Decoder layers.
  prefs: []
  type: TYPE_NORMAL
- en: Encoder Layer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/9e8ab3e83decd7c9ea8305b7563b5cb0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3\. The Encoder part of the transformer network (Source: image from
    the original paper)'
  prefs: []
  type: TYPE_NORMAL
- en: An Encoder layer consists of a Multi-Head Attention layer, a Position-wise Feed-Forward
    layer, and two Layer Normalization layers.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: The EncoderLayer class initializes with input parameters and components, including
    a MultiHeadAttention module, a PositionWiseFeedForward module, two layer normalization
    modules, and a dropout layer. The forward methods computes the encoder layer output
    by applying self-attention, adding the attention output to the input tensor, and
    normalizing the result. Then, it computes the position-wise feed-forward output,
    combines it with the normalized self-attention output, and normalizes the final
    result before returning the processed tensor.
  prefs: []
  type: TYPE_NORMAL
- en: Decoder Layer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/28187d88b42ade5225fcc50d80e20c31.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4\. The Decoder part of the Transformer network (Souce: Image from the
    original paper)'
  prefs: []
  type: TYPE_NORMAL
- en: A Decoder layer consists of two Multi-Head Attention layers, a Position-wise
    Feed-Forward layer, and three Layer Normalization layers.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: The DecoderLayer initializes with input parameters and components such as MultiHeadAttention
    modules for masked self-attention and cross-attention, a PositionWiseFeedForward
    module, three layer normalization modules, and a dropout layer.
  prefs: []
  type: TYPE_NORMAL
- en: 'The forward method computes the decoder layer output by performing the following
    steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Calculate the masked self-attention output and add it to the input tensor, followed
    by dropout and layer normalization.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the cross-attention output between the decoder and encoder outputs,
    and add it to the normalized masked self-attention output, followed by dropout
    and layer normalization.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Calculate the position-wise feed-forward output and combine it with the normalized
    cross-attention output, followed by dropout and layer normalization.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Return the processed tensor.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: These operations enable the decoder to generate target sequences based on the
    input and the encoder output.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s combine the Encoder and Decoder layers to create the complete Transformer
    model.
  prefs: []
  type: TYPE_NORMAL
- en: Transformer Model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/c141dc670d42086d53956ee42bada0be.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5\. The Transformer Network (Source: Image from the original paper)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Merging it all together:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: The Transformer class combines the previously defined modules to create a complete
    Transformer model. During initialization, the Transformer module sets up input
    parameters and initializes various components, including embedding layers for
    source and target sequences, a PositionalEncoding module, EncoderLayer and DecoderLayer
    modules to create stacked layers, a linear layer for projecting decoder output,
    and a dropout layer.
  prefs: []
  type: TYPE_NORMAL
- en: 'The generate_mask method creates binary masks for source and target sequences
    to ignore padding tokens and prevent the decoder from attending to future tokens.
    The forward method computes the Transformer model’s output through the following
    steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Generate source and target masks using the generate_mask method.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute source and target embeddings, and apply positional encoding and dropout.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Process the source sequence through encoder layers, updating the enc_output
    tensor.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Process the target sequence through decoder layers, using enc_output and masks,
    and updating the dec_output tensor.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Apply the linear projection layer to the decoder output, obtaining output logits.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: These steps enable the Transformer model to process input sequences and generate
    output sequences based on the combined functionality of its components.
  prefs: []
  type: TYPE_NORMAL
- en: Preparing Sample Data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this example, we will create a toy dataset for demonstration purposes. In
    practice, you would use a larger dataset, preprocess the text, and create vocabulary
    mappings for source and target languages.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Training the Model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now we’ll train the model using the sample data. In practice, you would use
    a larger dataset and split it into training and validation sets.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: We can use this way to build a simple Transformer from scratch in Pytorch. All
    Large Language Models use these Transformer encoder or decoder blocks for training.
    Hence understanding the network that started it all is extremely important. Hope
    this article helps all looking to deep dive into LLM’s.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Attention is all you need
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[A. Vaswani](https://www.bibsonomy.org/person/1c9bf08cbcb15680c807e12a01dd8c929/author/0),
    [N. Shazeer](https://www.bibsonomy.org/person/1c9bf08cbcb15680c807e12a01dd8c929/author/1),
    [N. Parmar](https://www.bibsonomy.org/person/1c9bf08cbcb15680c807e12a01dd8c929/author/2),
    [J. Uszkoreit](https://www.bibsonomy.org/person/1c9bf08cbcb15680c807e12a01dd8c929/author/3),
    [L. Jones](https://www.bibsonomy.org/person/1c9bf08cbcb15680c807e12a01dd8c929/author/4),
    [A. Gomez](https://www.bibsonomy.org/person/1c9bf08cbcb15680c807e12a01dd8c929/author/5),
    [{. Kaiser](https://www.bibsonomy.org/person/1c9bf08cbcb15680c807e12a01dd8c929/author/6),
    and [I. Polosukhin](https://www.bibsonomy.org/person/1c9bf08cbcb15680c807e12a01dd8c929/author/7).
    *Advances in Neural Information Processing Systems , page 5998–6008\.* (*2017*)'
  prefs: []
  type: TYPE_NORMAL
