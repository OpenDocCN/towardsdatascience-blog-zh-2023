- en: 'ULTRA: Foundation Models for Knowledge Graph Reasoning'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 'ULTRA: çŸ¥è¯†å›¾è°±æ¨ç†çš„åŸºç¡€æ¨¡å‹'
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/ultra-foundation-models-for-knowledge-graph-reasoning-9f8f4a0d7f09?source=collection_archive---------0-----------------------#2023-11-03](https://towardsdatascience.com/ultra-foundation-models-for-knowledge-graph-reasoning-9f8f4a0d7f09?source=collection_archive---------0-----------------------#2023-11-03)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://towardsdatascience.com/ultra-foundation-models-for-knowledge-graph-reasoning-9f8f4a0d7f09?source=collection_archive---------0-----------------------#2023-11-03](https://towardsdatascience.com/ultra-foundation-models-for-knowledge-graph-reasoning-9f8f4a0d7f09?source=collection_archive---------0-----------------------#2023-11-03)
- en: Whatâ€™s new in Graph ML?
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å›¾å½¢æœºå™¨å­¦ä¹ æœ‰ä»€ä¹ˆæ–°è¿›å±•ï¼Ÿ
- en: One model to rule them all
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªæ¨¡å‹ç»Ÿæ²»ä¸€åˆ‡
- en: '[](https://mgalkin.medium.com/?source=post_page-----9f8f4a0d7f09--------------------------------)[![Michael
    Galkin](../Images/c5eb13334712ca0462d8a5df4a268ad0.png)](https://mgalkin.medium.com/?source=post_page-----9f8f4a0d7f09--------------------------------)[](https://towardsdatascience.com/?source=post_page-----9f8f4a0d7f09--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----9f8f4a0d7f09--------------------------------)
    [Michael Galkin](https://mgalkin.medium.com/?source=post_page-----9f8f4a0d7f09--------------------------------)'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://mgalkin.medium.com/?source=post_page-----9f8f4a0d7f09--------------------------------)[![Michael
    Galkin](../Images/c5eb13334712ca0462d8a5df4a268ad0.png)](https://mgalkin.medium.com/?source=post_page-----9f8f4a0d7f09--------------------------------)[](https://towardsdatascience.com/?source=post_page-----9f8f4a0d7f09--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----9f8f4a0d7f09--------------------------------)
    [Michael Galkin](https://mgalkin.medium.com/?source=post_page-----9f8f4a0d7f09--------------------------------)'
- en: Â·
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: Â·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F4d4f8ddd1e68&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fultra-foundation-models-for-knowledge-graph-reasoning-9f8f4a0d7f09&user=Michael+Galkin&userId=4d4f8ddd1e68&source=post_page-4d4f8ddd1e68----9f8f4a0d7f09---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----9f8f4a0d7f09--------------------------------)
    Â·10 min readÂ·Nov 3, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F9f8f4a0d7f09&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fultra-foundation-models-for-knowledge-graph-reasoning-9f8f4a0d7f09&user=Michael+Galkin&userId=4d4f8ddd1e68&source=-----9f8f4a0d7f09---------------------clap_footer-----------)'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '[å…³æ³¨](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F4d4f8ddd1e68&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fultra-foundation-models-for-knowledge-graph-reasoning-9f8f4a0d7f09&user=Michael+Galkin&userId=4d4f8ddd1e68&source=post_page-4d4f8ddd1e68----9f8f4a0d7f09---------------------post_header-----------)
    å‘è¡¨åœ¨ [Towards Data Science](https://towardsdatascience.com/?source=post_page-----9f8f4a0d7f09--------------------------------)
    Â·10åˆ†é’Ÿé˜…è¯»Â·2023å¹´11æœˆ3æ—¥[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F9f8f4a0d7f09&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fultra-foundation-models-for-knowledge-graph-reasoning-9f8f4a0d7f09&user=Michael+Galkin&userId=4d4f8ddd1e68&source=-----9f8f4a0d7f09---------------------clap_footer-----------)'
- en: --
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F9f8f4a0d7f09&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fultra-foundation-models-for-knowledge-graph-reasoning-9f8f4a0d7f09&source=-----9f8f4a0d7f09---------------------bookmark_footer-----------)'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F9f8f4a0d7f09&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fultra-foundation-models-for-knowledge-graph-reasoning-9f8f4a0d7f09&source=-----9f8f4a0d7f09---------------------bookmark_footer-----------)'
- en: Training a single generic model for solving arbitrary datasets is always a dream
    for ML researchers, especially in the era of foundation models. While such dreams
    have been realized in perception domains like images or natural languages, whether
    they can be reproduced in reasoning domains (like graphs) remains an open challenge.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: è®­ç»ƒä¸€ä¸ªé€šç”¨æ¨¡å‹ä»¥è§£å†³ä»»æ„æ•°æ®é›†å§‹ç»ˆæ˜¯æœºå™¨å­¦ä¹ ç ”ç©¶è€…çš„æ¢¦æƒ³ï¼Œç‰¹åˆ«æ˜¯åœ¨åŸºç¡€æ¨¡å‹æ—¶ä»£ã€‚è™½ç„¶è¿™ç§æ¢¦æƒ³åœ¨å›¾åƒæˆ–è‡ªç„¶è¯­è¨€ç­‰æ„ŸçŸ¥é¢†åŸŸå·²å¾—ä»¥å®ç°ï¼Œä½†æ˜¯å¦èƒ½å¤Ÿåœ¨æ¨ç†é¢†åŸŸï¼ˆå¦‚å›¾å½¢ï¼‰ä¸­é‡ç°ä»ç„¶æ˜¯ä¸€ä¸ªæœªè§£çš„æŒ‘æˆ˜ã€‚
- en: '![](../Images/420c91170bb69d5fbbd70d11d432c415.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/420c91170bb69d5fbbd70d11d432c415.png)'
- en: Image by Authors edited from the output of DALL-E 3.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ç‰‡ç”±ä½œè€…ç¼–è¾‘ï¼Œæºè‡ª DALL-E 3 çš„è¾“å‡ºã€‚
- en: In this blog post, we prove such a generic reasoning model exists, at least
    for knowledge graphs (KGs). We create **ULTRA**, a single pre-trained reasoning
    model that generalizes to new KGs of arbitrary entity and relation vocabularies,
    which serves as a default solution for any KG reasoning problem.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: '*This post is based on our recent paper (*[*preprint*](https://arxiv.org/abs/2310.04562)*)
    and was written together with* [*Xinyu Yuan*](https://github.com/KatarinaYuan)
    *(Mila),* [*Zhaocheng Zhu*](https://kiddozhu.github.io/) *(Mila), and* [*Bruno
    Ribeiro*](https://www.cs.purdue.edu/homes/ribeirob/) *(Purdue / Stanford). Follow*
    [*Michael*](https://twitter.com/michael_galkin)*,* [*Xinyu*](https://twitter.com/XinyuYuan402)*,*
    [*Zhaocheng*](https://twitter.com/zhu_zhaocheng)*, and* [*Bruno*](https://twitter.com/brunofmr)
    *on Twitter for more Graph ML content.*'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: Outline
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[Why KG representation learning is stuck in 2018](#974c)'
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Theory: What makes a model inductive and transferable?](#062b)'
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Theory: Equivariance in multi-relational graphs](#fbb0)'
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[ULTRA: A Foundation Model for KG Reasoning](#86f8)'
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Experiments: Best even in the zero-shot inference, Scaling behavior](#2517)'
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Code, Data, Checkpoints](#71ab)'
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why KG representation learning is stuck in 2018
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The pretrain-finetune paradigm has been with us since 2018 when [ELMo](https://arxiv.org/abs/1802.05365)
    and [ULMFit](https://arxiv.org/abs/1801.06146) showed first promising results
    and they were later cemented with [BERT](https://arxiv.org/abs/1810.04805) and
    [GPT](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf).
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: In the era of *large language models* (LLM) and more general *foundation models*
    (FMs), we often have a single model (like GPT-4 or Llama-2) pre-trained on enormous
    amounts of data and capable of performing a sheer variety of language tasks in
    the zero-shot manner (or at least be fine-tuned on the specific dataset). These
    days, multimodal FMs even support language, vision, audio, and other modalities
    in the same one model.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: 'Things work a little differently in Graph ML. Particularly, **whatâ€™s up with
    representation learning on KGs at the end of 2023?** The main tasks here are edge-level:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: 'Entity prediction (or knowledge graph completion) `(h,r,?)`: given a head node
    and relation, rank all nodes in the graph that can potentially be true tails.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Relation prediction `(h,?,t)`: given two nodes, predict a relation type between
    them'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Turns out, up until now it has been somewhere in pre-2018\. The key problem
    is:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: Each KG has its own set of entities and relations, there is no single pre-trained
    model that would transfer to any graph.
  id: totrans-28
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: For example, if we look at Freebase (a KG behind Google Knowledge Graph) and
    Wikidata (the largest open-source KG), they have absolutely different sets of
    entities (86M vs 100M) and relations (1500 vs 6000). Is there any hope for current
    KG representation learning methods to be trained on one graph and transfer to
    another?
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: ä¾‹å¦‚ï¼Œå¦‚æœæˆ‘ä»¬æŸ¥çœ‹Freebaseï¼ˆGoogleçŸ¥è¯†å›¾è°±èƒŒåçš„çŸ¥è¯†å›¾è°±ï¼‰å’ŒWikidataï¼ˆæœ€å¤§çš„å¼€æºçŸ¥è¯†å›¾è°±ï¼‰ï¼Œå®ƒä»¬å…·æœ‰å®Œå…¨ä¸åŒçš„å®ä½“é›†åˆï¼ˆ86Må¯¹100Mï¼‰å’Œå…³ç³»ï¼ˆ1500å¯¹6000ï¼‰ã€‚å½“å‰çš„çŸ¥è¯†å›¾è°±è¡¨ç¤ºå­¦ä¹ æ–¹æ³•æ˜¯å¦æœ‰å¸Œæœ›åœ¨ä¸€ä¸ªå›¾ä¸Šè®­ç»ƒå¹¶è½¬ç§»åˆ°å¦ä¸€ä¸ªå›¾ä¸Šï¼Ÿ
- en: '![](../Images/c68c80ce7bd14dbad8070aadc789c5d0.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c68c80ce7bd14dbad8070aadc789c5d0.png)'
- en: Different vocabularies of Freebase and Wikidata. Image by Authors.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: Freebaseå’ŒWikidataçš„è¯æ±‡è¡¨ä¸åŒã€‚å›¾ç‰‡ç”±ä½œè€…æä¾›ã€‚
- en: âŒ Classical transductive methods like TransE, ComplEx, RotatE, and hundreds
    of other embedding-based methods learn a **fixed set of entities and relation
    types** from the training graph and cannot even support new nodes added to the
    same graph. Shallow embedding-based methods do not transfer (in fact, we believe
    there is no point in developing such methods anymore except for some student project
    exercises).
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: âŒ åƒTransEã€ComplExã€RotatEç­‰ä¼ ç»Ÿè½¬å¯¼æ–¹æ³•ï¼Œä»¥åŠå…¶ä»–æ•°ç™¾ç§åŸºäºåµŒå…¥çš„æ–¹æ³•ï¼Œä»è®­ç»ƒå›¾ä¸­å­¦ä¹ **å›ºå®šçš„å®ä½“å’Œå…³ç³»ç±»å‹**ï¼Œç”šè‡³æ— æ³•æ”¯æŒåŒä¸€å›¾ä¸­æ·»åŠ çš„æ–°èŠ‚ç‚¹ã€‚æµ…å±‚åµŒå…¥æ–¹æ³•æ— æ³•è½¬ç§»ï¼ˆäº‹å®ä¸Šï¼Œæˆ‘ä»¬è®¤ä¸ºé™¤äº†æŸäº›å­¦ç”Ÿé¡¹ç›®ç»ƒä¹ å¤–ï¼Œå†ä¹Ÿæ²¡æœ‰å¿…è¦å¼€å‘è¿™æ ·çš„æŠ€æœ¯ï¼‰ã€‚
- en: ğŸŸ¡ Inductive entity methods like [NodePiece](https://openreview.net/forum?id=xMJWUKJnFSw)
    and [Neural Bellman-Ford Nets](https://arxiv.org/pdf/2106.06935.pdf) do not learn
    entity embeddings. Instead, they parameterize training (seen) and new inference
    (unseen) nodes as a function of fixed relations. Since they **learn only relation
    embeddings**, it does allow them to transfer to graphs with new nodes but transfer
    to new graphs with different relations (like Freebase to Wikidata) is still beyond
    reach.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸŸ¡ åƒ[NodePiece](https://openreview.net/forum?id=xMJWUKJnFSw)å’Œ[Neural Bellman-Ford
    Nets](https://arxiv.org/pdf/2106.06935.pdf)è¿™æ ·çš„å½’çº³å®ä½“æ–¹æ³•ä¸ä¼šå­¦ä¹ å®ä½“åµŒå…¥ã€‚ç›¸åï¼Œå®ƒä»¬å°†è®­ç»ƒï¼ˆå·²è§ï¼‰å’Œæ–°çš„æ¨ç†ï¼ˆæœªè§ï¼‰èŠ‚ç‚¹å‚æ•°åŒ–ä¸ºå›ºå®šå…³ç³»çš„å‡½æ•°ã€‚ç”±äºå®ƒä»¬**åªå­¦ä¹ å…³ç³»åµŒå…¥**ï¼Œè¿™ä½¿å¾—å®ƒä»¬èƒ½å¤Ÿè½¬ç§»åˆ°å…·æœ‰æ–°èŠ‚ç‚¹çš„å›¾ä¸­ï¼Œä½†è½¬ç§»åˆ°å…·æœ‰ä¸åŒå…³ç³»çš„æ–°å›¾ï¼ˆä¾‹å¦‚ä»Freebaseåˆ°Wikidataï¼‰ä»ç„¶è¶…å‡ºèŒƒå›´ã€‚
- en: '![](../Images/fe9e903615e0e5c9ed37b6ea92aff2d0.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fe9e903615e0e5c9ed37b6ea92aff2d0.png)'
- en: Relative entity representations enable inductive GNNs. Image by Authors.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: ç›¸å¯¹å®ä½“è¡¨ç¤ºä½¿å¾—å½’çº³GNNæˆä¸ºå¯èƒ½ã€‚å›¾ç‰‡ç”±ä½œè€…æä¾›ã€‚
- en: What to do if you have **both** new entities and relations at inference time
    (a completely new graph)? If you donâ€™t learn entity or relation embeddings, is
    the transfer theoretically possible? Letâ€™s look into the theory then.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœåœ¨æ¨ç†æ—¶**åŒæ—¶**å‡ºç°æ–°çš„å®ä½“å’Œå…³ç³»ï¼ˆä¸€ä¸ªå…¨æ–°çš„å›¾ï¼‰è¯¥æ€ä¹ˆåŠï¼Ÿå¦‚æœä½ ä¸å­¦ä¹ å®ä½“æˆ–å…³ç³»åµŒå…¥ï¼Œç†è®ºä¸Šè½¬ç§»æ˜¯å¦å¯èƒ½ï¼Ÿé‚£æˆ‘ä»¬å°±æ¥æ¢è®¨ä¸€ä¸‹ç†è®ºå§ã€‚
- en: 'Theory: What makes a model inductive and transferable?'
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç†è®ºï¼šæ˜¯ä»€ä¹ˆä½¿å¾—æ¨¡å‹å…·æœ‰å½’çº³æ€§å’Œå¯è½¬ç§»æ€§ï¼Ÿ
- en: 'Letâ€™s define the setup more formally:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬æ›´æ­£å¼åœ°å®šä¹‰è¿™ä¸ªè®¾ç½®ï¼š
- en: KGs are directed, multi-relational graphs with arbitrary sets of nodes and relation
    types
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: çŸ¥è¯†å›¾è°±æ˜¯æœ‰å‘çš„ã€å¤šå…³ç³»çš„å›¾ï¼Œå…·æœ‰ä»»æ„çš„èŠ‚ç‚¹å’Œå…³ç³»ç±»å‹é›†åˆ
- en: Graphs arrive **without features**, that is, we donâ€™t assume the existence of
    textual descriptions (nor pre-computed feature vectors) of entities and relations.
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å›¾å½¢åˆ°è¾¾æ—¶**æ²¡æœ‰ç‰¹å¾**ï¼Œå³ï¼Œæˆ‘ä»¬ä¸å‡è®¾å­˜åœ¨å®ä½“å’Œå…³ç³»çš„æ–‡æœ¬æè¿°ï¼ˆä¹Ÿä¸å‡è®¾æœ‰é¢„è®¡ç®—çš„ç‰¹å¾å‘é‡ï¼‰ã€‚
- en: Given a query (head, relation, ?), we want to rank all nodes in the underlying
    graph (inference graph) and maximize the probability of returning a true tail.
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç»™å®šä¸€ä¸ªæŸ¥è¯¢ï¼ˆå¤´ï¼Œå…³ç³»ï¼Œï¼Ÿï¼‰ï¼Œæˆ‘ä»¬å¸Œæœ›å¯¹åº•å±‚å›¾ï¼ˆæ¨ç†å›¾ï¼‰ä¸­çš„æ‰€æœ‰èŠ‚ç‚¹è¿›è¡Œæ’åï¼Œå¹¶æœ€å¤§åŒ–è¿”å›çœŸå®å°¾éƒ¨çš„æ¦‚ç‡ã€‚
- en: '*Transductive* setup: the set of nodes and entities is the same at training
    and inference time.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*è½¬å¯¼*è®¾ç½®ï¼šè®­ç»ƒå’Œæ¨ç†æ—¶èŠ‚ç‚¹å’Œå®ä½“çš„é›†åˆæ˜¯ç›¸åŒçš„ã€‚'
- en: '*Inductive* (entity) setup: the set of relations has to be fixed at training
    time, but nodes can be different at training and inference'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*å½’çº³*ï¼ˆå®ä½“ï¼‰è®¾ç½®ï¼šå…³ç³»çš„é›†åˆå¿…é¡»åœ¨è®­ç»ƒæ—¶å›ºå®šï¼Œä½†èŠ‚ç‚¹åœ¨è®­ç»ƒå’Œæ¨ç†æ—¶å¯ä»¥ä¸åŒ'
- en: '*Inductive* (entity and relation) setup: both new unseen entities and relations
    are allowed at inference'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*å½’çº³*ï¼ˆå®ä½“å’Œå…³ç³»ï¼‰è®¾ç½®ï¼šåœ¨æ¨ç†æ—¶å…è®¸å‡ºç°æ–°çš„æœªè§è¿‡çš„å®ä½“å’Œå…³ç³»'
- en: What do neural networks learn to be able to generalize to new data? The primary
    referenceâ€” the book on [Geometric Deep Learning by Bronstein, Bruna, Cohen, and
    VeliÄkoviÄ‡](https://geometricdeeplearning.com/)â€”posits that it is a question of
    *symmetries and invariances*.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: ç¥ç»ç½‘ç»œå­¦ä¹ ä»€ä¹ˆä»¥èƒ½å¤Ÿå¯¹æ–°æ•°æ®è¿›è¡Œæ³›åŒ–ï¼Ÿä¸»è¦å‚è€ƒä¹¦ç±â€”[Geometric Deep Learning by Bronstein, Bruna, Cohen,
    and VeliÄkoviÄ‡](https://geometricdeeplearning.com/)â€”è®¤ä¸ºè¿™æ˜¯ä¸€ä¸ª*å¯¹ç§°æ€§å’Œä¸å˜æ€§*çš„é—®é¢˜ã€‚
- en: What are the learnable invariances in foundation models? LLMs are trained on
    a fixed vocabulary of tokens (sub-word units, bytes, or even randomly initialized
    vectors as in [Lexinvariant LLMs](https://arxiv.org/abs/2305.16349)), vision models
    learn functions to project image patches, audio models learn to project audio
    patches.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: åŸºç¡€æ¨¡å‹ä¸­å¯å­¦ä¹ çš„ä¸å˜æ€§æ˜¯ä»€ä¹ˆï¼ŸLLM åœ¨å›ºå®šçš„è¯æ±‡è¡¨ä¸Šè¿›è¡Œè®­ç»ƒï¼ˆå­è¯å•å…ƒã€å­—èŠ‚ï¼Œæˆ–å¦‚ [Lexinvariant LLMs](https://arxiv.org/abs/2305.16349)
    ä¸­æ‰€ç¤ºçš„éšæœºåˆå§‹åŒ–å‘é‡ï¼‰ï¼Œè§†è§‰æ¨¡å‹å­¦ä¹ æŠ•å½±å›¾åƒå—çš„å‡½æ•°ï¼ŒéŸ³é¢‘æ¨¡å‹å­¦ä¹ æŠ•å½±éŸ³é¢‘å—ã€‚
- en: What are the learnable invariances for multi-relational graphs?
  id: totrans-47
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: å¤šå…³ç³»å›¾ä¸­çš„å¯å­¦ä¹ ä¸å˜æ€§æ˜¯ä»€ä¹ˆï¼Ÿ
- en: First, we will introduce the invariances (equivariances) in standard **homogeneous**
    graphs.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: é¦–å…ˆï¼Œæˆ‘ä»¬å°†ä»‹ç»æ ‡å‡† **é½æ¬¡** å›¾ä¸­çš„ä¸å˜æ€§ï¼ˆç­‰å˜æ€§ï¼‰ã€‚
- en: '*Standard (single) permutation equivariant graph models:* A great leap in graph
    ML came when early GNN work ([Scarselli et al. 2008](https://ro.uow.edu.au/cgi/viewcontent.cgi?article=10501&context=infopapers),
    [Xu et al. 2018](https://arxiv.org/abs/1810.00826), [Morris et al. 2018](https://ojs.aaai.org/index.php/AAAI/article/view/4384))
    has shown that inductive tasks on graphs benefited enormously from assuming that
    vertex IDs are arbitrary, such that the predictions of a graph model should not
    change if we reassigned vertex ID. This is known as *permutation equivariance*
    of the neural network on node IDs. This realization has created great excitement
    and a profusion of novel graph representation methods since, as long as the neural
    network is equivariant to node ID permutations, we can call it a graph model.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '*æ ‡å‡†ï¼ˆå•ä¸€ï¼‰ç½®æ¢ç­‰å˜å›¾æ¨¡å‹ï¼š* å½“æ—©æœŸçš„ GNN ç ”ç©¶ï¼ˆ[Scarselli et al. 2008](https://ro.uow.edu.au/cgi/viewcontent.cgi?article=10501&context=infopapers)ï¼Œ[Xu
    et al. 2018](https://arxiv.org/abs/1810.00826)ï¼Œ[Morris et al. 2018](https://ojs.aaai.org/index.php/AAAI/article/view/4384)ï¼‰å±•ç¤ºå‡ºå‡è®¾é¡¶ç‚¹
    ID æ˜¯ä»»æ„çš„ï¼Œå›¾æ¨¡å‹çš„é¢„æµ‹ä¸åº”å› é‡æ–°åˆ†é…é¡¶ç‚¹ ID è€Œæ”¹å˜æ—¶ï¼Œå›¾ ML å–å¾—äº†é‡å¤§è¿›å±•ã€‚è¿™è¢«ç§°ä¸ºç¥ç»ç½‘ç»œåœ¨èŠ‚ç‚¹ ID ä¸Šçš„ *ç½®æ¢ç­‰å˜æ€§*ã€‚è¿™ä¸€è®¤è¯†æ¿€å‘äº†æå¤§çš„å…´å¥‹ï¼Œå¹¶äº§ç”Ÿäº†å¤§é‡æ–°é¢–çš„å›¾è¡¨ç¤ºæ–¹æ³•ï¼Œåªè¦ç¥ç»ç½‘ç»œå¯¹èŠ‚ç‚¹
    ID ç½®æ¢æ˜¯ç­‰å˜çš„ï¼Œæˆ‘ä»¬å°±å¯ä»¥ç§°å…¶ä¸ºå›¾æ¨¡å‹ã€‚'
- en: '![](../Images/be3ea4a2e3127b59fe07092c439734ca.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/be3ea4a2e3127b59fe07092c439734ca.png)'
- en: '*Single-relational graphs. GNNs are equivariant to node permutations: Michael
    Jacksonâ€™s node vector will have the same value even after re-labeling node IDs.
    Image by Authors.*'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '*å•å…³ç³»å›¾ã€‚GNN å¯¹èŠ‚ç‚¹ç½®æ¢æ˜¯ç­‰å˜çš„ï¼šå³ä½¿åœ¨é‡æ–°æ ‡è®°èŠ‚ç‚¹ ID åï¼ŒMichael Jackson çš„èŠ‚ç‚¹å‘é‡ä¹Ÿä¼šä¿æŒç›¸åŒçš„å€¼ã€‚å›¾åƒæ¥è‡ªä½œè€…ã€‚*'
- en: The permutation equivariance on node IDs allows GNNs to inductively (zero-shot)
    transfer the patterns learned from a training graph to another (different) test
    graph. This is a consequence of the equivariance, since the neural network cannot
    use node IDs to produce embeddings, it must use the graph structure. This creates
    what we know as *structural representations* in graphs (see [Srinivasan & Ribeiro
    (ICLR 2020)](https://iclr.cc/virtual_2020/poster_SJxzFySKwH.html)).
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: èŠ‚ç‚¹ ID ä¸Šçš„ç½®æ¢ç­‰å˜æ€§å…è®¸ GNN ä»¥å½’çº³ï¼ˆé›¶-shotï¼‰æ–¹å¼å°†ä»è®­ç»ƒå›¾ä¸­å­¦åˆ°çš„æ¨¡å¼è½¬ç§»åˆ°å¦ä¸€ä¸ªï¼ˆä¸åŒçš„ï¼‰æµ‹è¯•å›¾ä¸­ã€‚è¿™æ˜¯ç­‰å˜æ€§çš„ç»“æœï¼Œå› ä¸ºç¥ç»ç½‘ç»œä¸èƒ½ä½¿ç”¨èŠ‚ç‚¹
    ID ç”ŸæˆåµŒå…¥ï¼Œå®ƒå¿…é¡»ä½¿ç”¨å›¾ç»“æ„ã€‚è¿™å°±äº§ç”Ÿäº†æˆ‘ä»¬æ‰€çŸ¥çš„ *ç»“æ„è¡¨ç¤º*ï¼ˆè§ [Srinivasan & Ribeiro (ICLR 2020)](https://iclr.cc/virtual_2020/poster_SJxzFySKwH.html)ï¼‰ã€‚
- en: Equivariance in multi-relational graphs
  id: totrans-53
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å¤šå…³ç³»å›¾ä¸­çš„ç­‰å˜æ€§
- en: Now edges in the graphs might have different relation types â€” is there any GNN
    theory for such graphs?
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨å›¾ä¸­çš„è¾¹å¯èƒ½æœ‰ä¸åŒçš„å…³ç³»ç±»å‹â€”â€”æ˜¯å¦æœ‰é’ˆå¯¹è¿™ç§å›¾çš„ GNN ç†è®ºï¼Ÿ
- en: 1ï¸âƒ£ In our previous work, [Weisfeiler and Leman Go Relational](https://arxiv.org/abs/2211.17113)
    (with Pablo BarcelÃ³, Christopher Morris, and Miguel Romero Orth, LoG 2022), we
    derived Relational WL â€” a WL expressiveness hierarchy for multi-relational graphs
    focusing more on node-level tasks. The great [follow-up work by Huang et al (NeurIPS
    2023)](https://arxiv.org/abs/2302.02209) extended the theory to link prediction,
    formalized *conditional message passing,* and logical expressiveness using Relational
    WL. âœï¸ Letâ€™s remember **conditional message passing** â€” weâ€™ll need it later â€”
    it provably improves link prediction performance.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 1ï¸âƒ£ åœ¨æˆ‘ä»¬ä¹‹å‰çš„å·¥ä½œä¸­ï¼Œ[Weisfeiler and Leman Go Relational](https://arxiv.org/abs/2211.17113)ï¼ˆä¸
    Pablo BarcelÃ³ã€Christopher Morris å’Œ Miguel Romero Orth åˆä½œï¼ŒLoG 2022ï¼‰ï¼Œæˆ‘ä»¬æ¨å¯¼å‡ºäº† Relational
    WL â€”â€” ä¸€ä¸ªæ›´ä¾§é‡äºèŠ‚ç‚¹çº§ä»»åŠ¡çš„å¤šå…³ç³»å›¾çš„ WL è¡¨è¾¾å±‚çº§ã€‚é»„ç­‰äººï¼ˆNeurIPS 2023ï¼‰[çš„ä¼Ÿå¤§åç»­å·¥ä½œ](https://arxiv.org/abs/2302.02209)
    å°†ç†è®ºæ‰©å±•åˆ°é“¾æ¥é¢„æµ‹ï¼Œå½¢å¼åŒ–äº† *æ¡ä»¶æ¶ˆæ¯ä¼ é€’* å’Œä½¿ç”¨ Relational WL çš„é€»è¾‘è¡¨è¾¾èƒ½åŠ›ã€‚âœï¸ è®©æˆ‘ä»¬è®°ä½ **æ¡ä»¶æ¶ˆæ¯ä¼ é€’** â€”â€” æˆ‘ä»¬ç¨åä¼šéœ€è¦å®ƒ
    â€”â€” å®ƒè¢«è¯æ˜èƒ½æ”¹å–„é“¾æ¥é¢„æµ‹æ€§èƒ½ã€‚
- en: The proposed addition of a global readout vector induced by incoming/outgoing
    edge direction resembles the [recent work of Emanuele Rossi et al](https://arxiv.org/abs/2305.10498)
    on studying directionality in homogeneous MPNNs (read [the blog post on Medium](/direction-improves-graph-learning-170e797e94fe)
    for more details). Still, those works do not envision the case when even relations
    at test time are unseen.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: '*2ï¸âƒ£ Double permutation equivariant (multi-relational) graph models:* Recently,
    [Gao et al. 2023](https://arxiv.org/abs/2302.01313) proposed the concept of **double
    equivariance** for multi-relational graphs. Double equivariance forces the neural
    network to be equivariant to the joint permutations of both node IDs and relation
    IDs. This ensures the neural network learns structural patterns between nodes
    and relations, which allows it to inductively (zero-shot) transfer the learned
    patterns to another graph with new nodes and new relations.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1ad9763efc24de9bca5a6c3a6bace408.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
- en: '*Double equivariance in multi-relational graphs. Permuting both node IDs and
    relation IDs does not change the relational structure. Hence, the output node
    states should be the same (but permuted). Image by Authors.*'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: 'â¡ï¸ In our work, we find *the invariance of relation interactions*, that is,
    even if relation identities are different, their fundamental interactions remain
    the same, and those fundamental interactions can be captured by a **graph of relations.**
    In the graph of relations, each node is a relation type from the original graph.
    Two nodes in this graph will be connected if edges with those relation types in
    the original graph are incident (that is, they share a head or tail node). Depending
    on the incidence, we distinguish **4 edge types** in the graph of relations:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: '*Head-to-head (h2h)* â€” two relations can start from the same head entity;'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Tail-to-head (t2h)* â€” tail entity of one relation can be a head of another
    relation;'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Head-to-tail (h2t)* â€” head entity of one relation can be a tail of another
    relation;'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Tail-to-tail (t2t)* â€” two relations can have the same tail entity.'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/5170699f4814c0eb228294f5ea19e974.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
- en: '*Different incidence patterns in the original graph produce different interactions
    in the graph of relations. The right-most: the example relation graph (inverse
    edges are omitted for clarity). Image by Authors*'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
- en: 'A few nice properties of the relation graph:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: It can be built from absolutely any multi-relational graph (with simple sparse
    matrix multiplications)
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The 4 fundamental interactions never change because they just encode the basic
    topology â€” in directed graphs there always will be head and tail nodes, and we
    relations would have those incidence patterns
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Essentially, learning representations over the relations graph can transfer
    to any multi-relational graph! This is the *learnable invariance*.
  id: totrans-70
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: In fact, it can be shown (we are already working on the formal proofs which
    will be available in an upcoming work ğŸ˜‰) that representing relations via their
    interactions in a graph of relations is a double equivariant model! This means
    that learned relational representations are independent of identities but rather
    rely on the joint interactions between relations, nodes, and nodes & relations.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: å®é™…ä¸Šï¼Œå¯ä»¥è¯æ˜ï¼ˆæˆ‘ä»¬å·²ç»åœ¨è¿›è¡Œæ­£å¼è¯æ˜ï¼Œè¿™äº›å°†åœ¨å³å°†å‘å¸ƒçš„å·¥ä½œä¸­æä¾›ğŸ˜‰ï¼‰ï¼Œé€šè¿‡å…³ç³»å›¾ä¸­çš„äº¤äº’æ¥è¡¨ç¤ºå…³ç³»æ˜¯ä¸€ä¸ªåŒç­‰å˜æ¨¡å‹ï¼è¿™æ„å‘³ç€å­¦ä¹ åˆ°çš„å…³ç³»è¡¨ç¤ºæ˜¯ç‹¬ç«‹äºèº«ä»½çš„ï¼Œè€Œæ˜¯ä¾èµ–äºå…³ç³»ã€èŠ‚ç‚¹åŠèŠ‚ç‚¹ä¸å…³ç³»ä¹‹é—´çš„è”åˆäº¤äº’ã€‚
- en: 'ULTRA: A Foundation Model for KG Reasoning'
  id: totrans-72
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 'ULTRA: ä¸€ä¸ªç”¨äºKGæ¨ç†çš„åŸºç¡€æ¨¡å‹'
- en: With all the theoretical foundations backing us up, we are now ready to introduce
    ULTRA.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ‰€æœ‰ç†è®ºåŸºç¡€çš„æ”¯æŒä¸‹ï¼Œæˆ‘ä»¬ç°åœ¨å‡†å¤‡ä»‹ç»ULTRAã€‚
- en: ULTRA is a method for unified, learnable, and transferable graph representations.
    ULTRA leverages the invariances (and equivariances) of the **graph of relations**
    with its fundamental interactions and applies **conditional message passing**
    to get relative relational representations. Perhaps the coolest fact is that
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: ULTRAæ˜¯ä¸€ç§ç»Ÿä¸€çš„ã€å¯å­¦ä¹ çš„ã€å¯è¿ç§»çš„å›¾è¡¨ç¤ºæ–¹æ³•ã€‚ULTRAåˆ©ç”¨**å…³ç³»å›¾**çš„åŸºæœ¬äº¤äº’çš„ä¸å˜æ€§ï¼ˆå’Œç­‰å˜æ€§ï¼‰ï¼Œå¹¶åº”ç”¨**æ¡ä»¶æ¶ˆæ¯ä¼ é€’**æ¥è·å¾—ç›¸å¯¹å…³ç³»è¡¨ç¤ºã€‚ä¹Ÿè®¸æœ€é…·çš„äº‹å®æ˜¯
- en: a single pre-trained ULTRA model can run 0-shot inference on any possible multi-relational
    graph and be fine-tuned on any graph.
  id: totrans-75
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªç»è¿‡é¢„è®­ç»ƒçš„ULTRAæ¨¡å‹å¯ä»¥åœ¨ä»»ä½•å¯èƒ½çš„å¤šå…³ç³»å›¾ä¸Šè¿›è¡Œ0-shotæ¨ç†ï¼Œå¹¶åœ¨ä»»ä½•å›¾ä¸Šè¿›è¡Œå¾®è°ƒã€‚
- en: In other words, ULTRA is pretty much a foundation model that can run inference
    on any graph input (with already good performance) and be fine-tuned on any target
    graph of interest.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: æ¢å¥è¯è¯´ï¼ŒULTRAå®é™…ä¸Šæ˜¯ä¸€ä¸ªåŸºç¡€æ¨¡å‹ï¼Œå¯ä»¥åœ¨ä»»ä½•å›¾è¾“å…¥ä¸Šè¿›è¡Œæ¨ç†ï¼ˆè¡¨ç°å·²ç›¸å½“å‡ºè‰²ï¼‰ï¼Œå¹¶ä¸”å¯ä»¥åœ¨ä»»ä½•ç›®æ ‡å›¾ä¸Šè¿›è¡Œå¾®è°ƒã€‚
- en: The crucial component of ULTRA is in *relative* relation representations constructed
    from the graph of relations. Given a query `(Michael Jackson, genre, ?)`, we first
    initialize the `genre` node in the graph of relations with the all-ones vector
    (all other nodes are initialized with zeros). Running a GNN, the resulting node
    embeddings of the relation graph are conditioned on the `genre` node â€” it means
    that each starting initialized relation will have its own matrix of relational
    features, and thatâ€™s very helpful from many theoretical and practical aspects!
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: ULTRAçš„å…³é”®ç»„æˆéƒ¨åˆ†æ˜¯ä»å…³ç³»å›¾ä¸­æ„å»ºçš„*ç›¸å¯¹*å…³ç³»è¡¨ç¤ºã€‚ç»™å®šä¸€ä¸ªæŸ¥è¯¢ `(Michael Jackson, genre, ?)`ï¼Œæˆ‘ä»¬é¦–å…ˆç”¨å…¨1å‘é‡åˆå§‹åŒ–å…³ç³»å›¾ä¸­çš„`genre`èŠ‚ç‚¹ï¼ˆæ‰€æœ‰å…¶ä»–èŠ‚ç‚¹åˆå§‹åŒ–ä¸º0ï¼‰ã€‚è¿è¡ŒGNNåï¼Œå…³ç³»å›¾çš„èŠ‚ç‚¹åµŒå…¥ä»¥`genre`èŠ‚ç‚¹ä¸ºæ¡ä»¶â€”â€”è¿™æ„å‘³ç€æ¯ä¸ªèµ·å§‹åˆå§‹åŒ–å…³ç³»å°†æ‹¥æœ‰è‡ªå·±çš„ä¸€ç»„å…³ç³»ç‰¹å¾çŸ©é˜µï¼Œè¿™åœ¨è®¸å¤šç†è®ºå’Œå®é™…æ–¹é¢éƒ½éå¸¸æœ‰ç”¨ï¼
- en: '![](../Images/3a2a3e03826f084d0bd9909871b263b3.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3a2a3e03826f084d0bd9909871b263b3.png)'
- en: '*ULTRA employs relative relation representations (a labeling trick over the
    graph of relations) such that each relation (eg, â€œgenreâ€) has its own unique matrix
    of all relation representations. Image by Authors.*'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '*ULTRAé‡‡ç”¨ç›¸å¯¹å…³ç³»è¡¨ç¤ºï¼ˆåœ¨å…³ç³»å›¾ä¸Šçš„æ ‡è®°æŠ€å·§ï¼‰ï¼Œä½¿å¾—æ¯ä¸ªå…³ç³»ï¼ˆä¾‹å¦‚ï¼Œâ€œç±»å‹â€ï¼‰éƒ½æœ‰å…¶å”¯ä¸€çš„å…³ç³»è¡¨ç¤ºçŸ©é˜µã€‚å›¾ç‰‡æ¥æºï¼šä½œè€…ã€‚*'
- en: 'Practically, given an input KG and a (h, r, ?) query, ULTRA executes the following
    actions:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: å®é™…ä¸Šï¼Œç»™å®šä¸€ä¸ªè¾“å…¥KGå’Œä¸€ä¸ª(h, r, ?)æŸ¥è¯¢ï¼ŒULTRAæ‰§è¡Œä»¥ä¸‹æ“ä½œï¼š
- en: Construction of the graph of relations;
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æ„å»ºå…³ç³»å›¾ï¼›
- en: Get relation features from the conditional message passing GNN on the graph
    of relations (conditioned on the initialized query relation r);
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ä»æ¡ä»¶æ¶ˆæ¯ä¼ é€’GNNä¸­è·å–å…³ç³»ç‰¹å¾ï¼ˆä»¥åˆå§‹åŒ–æŸ¥è¯¢å…³ç³»rä¸ºæ¡ä»¶ï¼‰ï¼›
- en: Use the obtained relational representations for the inductive link predictor
    GNN conditioned on the initialized head node h;
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ä½¿ç”¨è·å¾—çš„å…³ç³»è¡¨ç¤ºæ¥è¿›è¡Œæ¡ä»¶åŒ–çš„é“¾æ¥é¢„æµ‹GNNï¼Œæ¡ä»¶ä¸ºåˆå§‹åŒ–çš„å¤´èŠ‚ç‚¹hï¼›
- en: Steps 2 and 3 are implemented via slightly different modifications of the [Neural
    Bellman-Ford net (NBFNet)](https://arxiv.org/pdf/2106.06935.pdf). ULTRA only learns
    embeddings of the 4 fundamental interactions (h2t, t2t, t2h, h2h) and GNN weights
    â€” pretty small overall. The main model we experimented with has only 177k parameters.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¥éª¤2å’Œ3é€šè¿‡å¯¹[ç¥ç»è´å°”æ›¼-ç¦ç‰¹ç½‘ç»œ (NBFNet)](https://arxiv.org/pdf/2106.06935.pdf)è¿›è¡Œç•¥å¾®ä¸åŒçš„ä¿®æ”¹æ¥å®ç°ã€‚ULTRAä»…å­¦ä¹ 4ç§åŸºæœ¬äº¤äº’ï¼ˆh2t,
    t2t, t2h, h2hï¼‰å’ŒGNNæƒé‡â€”â€”æ€»ä½“ä¸Šéå¸¸å°ã€‚æˆ‘ä»¬å®éªŒçš„ä¸»è¦æ¨¡å‹ä»…æœ‰177kå‚æ•°ã€‚
- en: '![](../Images/69c48ade8d1640695e8a31d647795722.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/69c48ade8d1640695e8a31d647795722.png)'
- en: '*Three main steps taken by ULTRA: (1) building a relation graph; (2) running
    conditional message passing over the relation graph to get relative relation representations;
    (3) use those representations for inductive link predictor GNN on the entity level.
    Image by Authors.*'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '*ULTRA é‡‡å–çš„ä¸‰ä¸ªä¸»è¦æ­¥éª¤ï¼š(1) æ„å»ºå…³ç³»å›¾ï¼›(2) åœ¨å…³ç³»å›¾ä¸Šè¿›è¡Œæ¡ä»¶æ¶ˆæ¯ä¼ é€’ä»¥è·å–ç›¸å¯¹å…³ç³»è¡¨ç¤ºï¼›(3) ä½¿ç”¨è¿™äº›è¡¨ç¤ºè¿›è¡Œå®ä½“çº§åˆ«çš„å½’çº³é“¾æ¥é¢„æµ‹
    GNNã€‚å›¾ç‰‡æ¥æºï¼šä½œè€…ã€‚*'
- en: 'Experiments: Best even in the zero-shot inference and Fine-tuning'
  id: totrans-87
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å®éªŒï¼šåœ¨é›¶æ ·æœ¬æ¨ç†å’Œå¾®è°ƒä¸­è¡¨ç°æœ€ä½³
- en: We pre-trained ULTRA on 3 standard KGs based on Freebase, Wikidata, and Wordnet,
    and ran 0-shot link prediction on 50+ other KGs of various sizes from 1k â€” 120k
    nodes and 2k edges â€” 1.1M edges.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬åœ¨åŸºäº Freebaseã€Wikidata å’Œ Wordnet çš„ 3 ä¸ªæ ‡å‡†çŸ¥è¯†å›¾è°±ä¸Šé¢„è®­ç»ƒäº† ULTRAï¼Œå¹¶åœ¨ 50 å¤šä¸ªå…¶ä»–çŸ¥è¯†å›¾è°±ä¸Šè¿›è¡Œäº†é›¶æ ·æœ¬é“¾æ¥é¢„æµ‹ï¼Œè¿™äº›çŸ¥è¯†å›¾è°±çš„è§„æ¨¡ä»
    1k åˆ° 120k èŠ‚ç‚¹å’Œ 2k è¾¹åˆ° 1.1M è¾¹ä¸ç­‰ã€‚
- en: Averaged across the datasets with known SOTA, a single pre-trained ULTRA model
    is **better in the 0-shot inference mode** than existing SOTA models trained specifically
    on each graph ğŸš€Fine-tuning improves the performance even 10% further. Itâ€™s particularly
    amazing that a single trained ULTRA model can scale to graphs of such different
    sizes (100x difference in node size and 500x in the edge sizes) whereas GNNs are
    known to suffer from size generalization issues (see the prominent works by [Yehudai
    et al, ICML 2021](https://arxiv.org/abs/2010.08853) and [Zhou et al, NeurIPS 2022](https://arxiv.org/abs/2205.15117)).
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å·²çŸ¥ SOTA çš„æ•°æ®é›†ä¸Šå¹³å‡ï¼Œå•ä¸ªé¢„è®­ç»ƒçš„ ULTRA æ¨¡å‹åœ¨**é›¶æ ·æœ¬æ¨ç†æ¨¡å¼ä¸‹è¡¨ç°ä¼˜äº**é’ˆå¯¹æ¯ä¸ªå›¾ä¸“é—¨è®­ç»ƒçš„ç°æœ‰ SOTA æ¨¡å‹ ğŸš€ å¾®è°ƒç”šè‡³èƒ½ä½¿æ€§èƒ½æå‡
    10%ã€‚ç‰¹åˆ«ä»¤äººæƒŠè®¶çš„æ˜¯ï¼Œå•ä¸ªè®­ç»ƒçš„ ULTRA æ¨¡å‹èƒ½å¤Ÿé€‚åº”å¦‚æ­¤ä¸åŒè§„æ¨¡çš„å›¾ï¼ˆèŠ‚ç‚¹å¤§å°å·®å¼‚ä¸º 100 å€ï¼Œè¾¹å¤§å°å·®å¼‚ä¸º 500 å€ï¼‰ï¼Œè€Œ GNN é€šå¸¸ä¼šé‡åˆ°è§„æ¨¡æ³›åŒ–é—®é¢˜ï¼ˆå‚è§
    [Yehudai et al, ICML 2021](https://arxiv.org/abs/2010.08853) å’Œ [Zhou et al, NeurIPS
    2022](https://arxiv.org/abs/2205.15117) çš„çªå‡ºå·¥ä½œï¼‰ã€‚
- en: '![](../Images/c040b43c06c13861b944f7734708cda5.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c040b43c06c13861b944f7734708cda5.png)'
- en: A single pre-trained ULTRA is better even in the 0-shot inference mode than
    supervised SOTA modes trained end-to-end on specific graphs (look at the Average
    column). Fine-tuning improves the performance even further. Image by Authors
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: å•ä¸ªé¢„è®­ç»ƒçš„ ULTRA åœ¨é›¶æ ·æœ¬æ¨ç†æ¨¡å¼ä¸‹çš„è¡¨ç°ä¼˜äºåœ¨ç‰¹å®šå›¾ä¸Šç«¯åˆ°ç«¯è®­ç»ƒçš„ç›‘ç£ SOTA æ¨¡å‹ï¼ˆå‚è§å¹³å‡åˆ—ï¼‰ã€‚å¾®è°ƒè¿›ä¸€æ­¥æå‡äº†æ€§èƒ½ã€‚å›¾ç‰‡æ¥æºï¼šä½œè€…
- en: ğŸ™ƒ In fact, with 57 tested graphs, we ran a bit out of KGs to test ULTRA on.
    So if you have a fresh new benchmark hidden somewhere â€” let us know!
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ™ƒ å®é™…ä¸Šï¼Œç»è¿‡ 57 ä¸ªæµ‹è¯•å›¾ï¼Œæˆ‘ä»¬æœ‰ç‚¹ç”¨å°½äº†æµ‹è¯• ULTRA çš„çŸ¥è¯†å›¾è°±ã€‚å¦‚æœä½ æœ‰æ–°çš„åŸºå‡†è—åœ¨æŸå¤„â€”â€”å‘Šè¯‰æˆ‘ä»¬ï¼
- en: Scaling Behavior
  id: totrans-93
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ‰©å±•è¡Œä¸º
- en: We can bump the zero-shot performance even more by adding more graphs to the
    pre-training mixture although we do observe certain performance saturation after
    training on 4+ graphs.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥é€šè¿‡å°†æ›´å¤šå›¾è°±æ·»åŠ åˆ°é¢„è®­ç»ƒæ··åˆä¸­è¿›ä¸€æ­¥æå‡é›¶æ ·æœ¬æ€§èƒ½ï¼Œå°½ç®¡æˆ‘ä»¬ç¡®å®è§‚å¯Ÿåˆ°åœ¨è®­ç»ƒ 4 ä¸ªä»¥ä¸Šå›¾è°±åæ€§èƒ½å‡ºç°ä¸€å®šçš„é¥±å’Œã€‚
- en: The church of [Scaling Laws](https://arxiv.org/abs/2001.08361) predicts even
    better performance with bigger models trained on more qualitative data, so itâ€™s
    definitely on our agenda.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '[Scaling Laws](https://arxiv.org/abs/2001.08361) é¢„è¨€äº†ä½¿ç”¨æ›´å¤§çš„æ¨¡å‹åœ¨æ›´å¤šä¼˜è´¨æ•°æ®ä¸Šè®­ç»ƒä¼šæœ‰æ›´å¥½çš„è¡¨ç°ï¼Œå› æ­¤è¿™ç»å¯¹åœ¨æˆ‘ä»¬çš„è®¡åˆ’ä¹‹ä¸­ã€‚'
- en: '![](../Images/ef48bedd78ce63c719bf0a0f8a343dd8.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ef48bedd78ce63c719bf0a0f8a343dd8.png)'
- en: Zero-shot performance increases with more diverse graphs in the pre-training
    mix. Image by Authors.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: é›¶æ ·æœ¬æ€§èƒ½éšç€é¢„è®­ç»ƒæ··åˆå›¾è°±çš„å¤šæ ·æ€§è€Œå¢åŠ ã€‚å›¾ç‰‡æ¥æºï¼šä½œè€…ã€‚
- en: 'Conclusion: Code, Data, Checkpoints'
  id: totrans-98
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç»“è®ºï¼šä»£ç ã€æ•°æ®ã€æ£€æŸ¥ç‚¹
- en: So foundation models for KG reasoning are finally here, we are past that 2018
    threshold! A single pre-trained ULTRA model can perform link prediction on any
    KG (multi-relational graph) from any domain. You really just need a graph with
    more than 1 edge type to get going.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥ï¼ŒçŸ¥è¯†å›¾è°±æ¨ç†çš„åŸºç¡€æ¨¡å‹ç»ˆäºæ¥äº†ï¼Œæˆ‘ä»¬å·²ç»è¿‡äº† 2018 å¹´çš„é—¨æ§›ï¼å•ä¸ªé¢„è®­ç»ƒçš„ ULTRA æ¨¡å‹å¯ä»¥å¯¹ä»»ä½•é¢†åŸŸçš„çŸ¥è¯†å›¾è°±ï¼ˆå¤šå…³ç³»å›¾ï¼‰è¿›è¡Œé“¾æ¥é¢„æµ‹ã€‚ä½ åªéœ€ä¸€ä¸ªå…·æœ‰è¶…è¿‡
    1 ç§è¾¹ç±»å‹çš„å›¾è°±å³å¯å¼€å§‹ã€‚
- en: ğŸ“ˆ Practically, ULTRA demonstrates very promising performance on a variety of
    KG benchmarks already in the 0-shot mode, but you can bump the performance even
    further with a short fine-tuning.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ“ˆ å®é™…ä¸Šï¼ŒULTRA åœ¨å¤šç§çŸ¥è¯†å›¾è°±åŸºå‡†æµ‹è¯•ä¸­çš„é›¶æ ·æœ¬æ¨¡å¼ä¸‹å·²ç»å±•ç¤ºäº†éå¸¸æœ‰å‰æ™¯çš„æ€§èƒ½ï¼Œä½†ä½ å¯ä»¥é€šè¿‡çŸ­æ—¶é—´çš„å¾®è°ƒè¿›ä¸€æ­¥æå‡æ€§èƒ½ã€‚
- en: We make all the code, training data, and pre-trained model checkpoints available
    on GitHub so you can start running ULTRA on your data right away!
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬åœ¨ GitHub ä¸Šæä¾›äº†æ‰€æœ‰ä»£ç ã€è®­ç»ƒæ•°æ®å’Œé¢„è®­ç»ƒæ¨¡å‹æ£€æŸ¥ç‚¹ï¼Œä»¥ä¾¿ä½ å¯ä»¥ç«‹å³åœ¨ä½ çš„æ•°æ®ä¸Šè¿è¡Œ ULTRAï¼
- en: 'ğŸ“œ preprint: [arxiv](https://arxiv.org/abs/2310.04562)'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ“œ é¢„å°æœ¬ï¼š[arxiv](https://arxiv.org/abs/2310.04562)
- en: 'ğŸ› ï¸ Code, data: [Githtub repo](https://github.com/DeepGraphLearning/ULTRA)'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ› ï¸ ä»£ç ã€æ•°æ®ï¼š[Githtub repo](https://github.com/DeepGraphLearning/ULTRA)
- en: 'ğŸª Checkpoints: 2 checkpoints (2 MB each) in the [Github repo](https://github.com/DeepGraphLearning/ULTRA)'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸª æ£€æŸ¥ç‚¹ï¼šåœ¨ [Github repo](https://github.com/DeepGraphLearning/ULTRA) ä¸­æœ‰ 2 ä¸ªæ£€æŸ¥ç‚¹ï¼ˆæ¯ä¸ª
    2 MBï¼‰
- en: 'ğŸŒ Project website: [here](https://deepgraphlearning.github.io/project/ultra)'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸŒ é¡¹ç›®ç½‘ç«™ï¼š[è¿™é‡Œ](https://deepgraphlearning.github.io/project/ultra)
- en: As a closing remark, KG reasoning just represents a fraction of the many interesting
    problems in the reasoning domain, and the majority still donâ€™t have a generic
    solution. We believe the success of KG reasoning will bring more breakthroughs
    in other reasoning domains (for example, we recently found that [LLMs can actually
    learn and employ textual rules](https://arxiv.org/abs/2310.07064)). Letâ€™s stay
    optimistic about the future of reasoning!
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: ä½œä¸ºæ€»ç»“ï¼ŒKG æ¨ç†åªæ˜¯æ¨ç†é¢†åŸŸä¸­è®¸å¤šæœ‰è¶£é—®é¢˜çš„ä¸€éƒ¨åˆ†ï¼Œå¤§å¤šæ•°é—®é¢˜ä»ç„¶æ²¡æœ‰é€šç”¨çš„è§£å†³æ–¹æ¡ˆã€‚æˆ‘ä»¬ç›¸ä¿¡ï¼ŒKG æ¨ç†çš„æˆåŠŸå°†ä¸ºå…¶ä»–æ¨ç†é¢†åŸŸå¸¦æ¥æ›´å¤šçªç ´ï¼ˆä¾‹å¦‚ï¼Œæˆ‘ä»¬æœ€è¿‘å‘ç°äº†[LLMs
    å®é™…ä¸Šå¯ä»¥å­¦ä¹ å’Œè¿ç”¨æ–‡æœ¬è§„åˆ™](https://arxiv.org/abs/2310.07064)ï¼‰ã€‚è®©æˆ‘ä»¬å¯¹æ¨ç†çš„æœªæ¥ä¿æŒä¹è§‚ï¼
