- en: 'ULTRA: Foundation Models for Knowledge Graph Reasoning'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 'ULTRA: 知识图谱推理的基础模型'
- en: 原文：[https://towardsdatascience.com/ultra-foundation-models-for-knowledge-graph-reasoning-9f8f4a0d7f09?source=collection_archive---------0-----------------------#2023-11-03](https://towardsdatascience.com/ultra-foundation-models-for-knowledge-graph-reasoning-9f8f4a0d7f09?source=collection_archive---------0-----------------------#2023-11-03)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/ultra-foundation-models-for-knowledge-graph-reasoning-9f8f4a0d7f09?source=collection_archive---------0-----------------------#2023-11-03](https://towardsdatascience.com/ultra-foundation-models-for-knowledge-graph-reasoning-9f8f4a0d7f09?source=collection_archive---------0-----------------------#2023-11-03)
- en: What’s new in Graph ML?
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 图形机器学习有什么新进展？
- en: One model to rule them all
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 一个模型统治一切
- en: '[](https://mgalkin.medium.com/?source=post_page-----9f8f4a0d7f09--------------------------------)[![Michael
    Galkin](../Images/c5eb13334712ca0462d8a5df4a268ad0.png)](https://mgalkin.medium.com/?source=post_page-----9f8f4a0d7f09--------------------------------)[](https://towardsdatascience.com/?source=post_page-----9f8f4a0d7f09--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----9f8f4a0d7f09--------------------------------)
    [Michael Galkin](https://mgalkin.medium.com/?source=post_page-----9f8f4a0d7f09--------------------------------)'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://mgalkin.medium.com/?source=post_page-----9f8f4a0d7f09--------------------------------)[![Michael
    Galkin](../Images/c5eb13334712ca0462d8a5df4a268ad0.png)](https://mgalkin.medium.com/?source=post_page-----9f8f4a0d7f09--------------------------------)[](https://towardsdatascience.com/?source=post_page-----9f8f4a0d7f09--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----9f8f4a0d7f09--------------------------------)
    [Michael Galkin](https://mgalkin.medium.com/?source=post_page-----9f8f4a0d7f09--------------------------------)'
- en: ·
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F4d4f8ddd1e68&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fultra-foundation-models-for-knowledge-graph-reasoning-9f8f4a0d7f09&user=Michael+Galkin&userId=4d4f8ddd1e68&source=post_page-4d4f8ddd1e68----9f8f4a0d7f09---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----9f8f4a0d7f09--------------------------------)
    ·10 min read·Nov 3, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F9f8f4a0d7f09&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fultra-foundation-models-for-knowledge-graph-reasoning-9f8f4a0d7f09&user=Michael+Galkin&userId=4d4f8ddd1e68&source=-----9f8f4a0d7f09---------------------clap_footer-----------)'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F4d4f8ddd1e68&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fultra-foundation-models-for-knowledge-graph-reasoning-9f8f4a0d7f09&user=Michael+Galkin&userId=4d4f8ddd1e68&source=post_page-4d4f8ddd1e68----9f8f4a0d7f09---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----9f8f4a0d7f09--------------------------------)
    ·10分钟阅读·2023年11月3日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F9f8f4a0d7f09&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fultra-foundation-models-for-knowledge-graph-reasoning-9f8f4a0d7f09&user=Michael+Galkin&userId=4d4f8ddd1e68&source=-----9f8f4a0d7f09---------------------clap_footer-----------)'
- en: --
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F9f8f4a0d7f09&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fultra-foundation-models-for-knowledge-graph-reasoning-9f8f4a0d7f09&source=-----9f8f4a0d7f09---------------------bookmark_footer-----------)'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F9f8f4a0d7f09&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fultra-foundation-models-for-knowledge-graph-reasoning-9f8f4a0d7f09&source=-----9f8f4a0d7f09---------------------bookmark_footer-----------)'
- en: Training a single generic model for solving arbitrary datasets is always a dream
    for ML researchers, especially in the era of foundation models. While such dreams
    have been realized in perception domains like images or natural languages, whether
    they can be reproduced in reasoning domains (like graphs) remains an open challenge.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 训练一个通用模型以解决任意数据集始终是机器学习研究者的梦想，特别是在基础模型时代。虽然这种梦想在图像或自然语言等感知领域已得以实现，但是否能够在推理领域（如图形）中重现仍然是一个未解的挑战。
- en: '![](../Images/420c91170bb69d5fbbd70d11d432c415.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/420c91170bb69d5fbbd70d11d432c415.png)'
- en: Image by Authors edited from the output of DALL-E 3.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者编辑，源自 DALL-E 3 的输出。
- en: In this blog post, we prove such a generic reasoning model exists, at least
    for knowledge graphs (KGs). We create **ULTRA**, a single pre-trained reasoning
    model that generalizes to new KGs of arbitrary entity and relation vocabularies,
    which serves as a default solution for any KG reasoning problem.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: '*This post is based on our recent paper (*[*preprint*](https://arxiv.org/abs/2310.04562)*)
    and was written together with* [*Xinyu Yuan*](https://github.com/KatarinaYuan)
    *(Mila),* [*Zhaocheng Zhu*](https://kiddozhu.github.io/) *(Mila), and* [*Bruno
    Ribeiro*](https://www.cs.purdue.edu/homes/ribeirob/) *(Purdue / Stanford). Follow*
    [*Michael*](https://twitter.com/michael_galkin)*,* [*Xinyu*](https://twitter.com/XinyuYuan402)*,*
    [*Zhaocheng*](https://twitter.com/zhu_zhaocheng)*, and* [*Bruno*](https://twitter.com/brunofmr)
    *on Twitter for more Graph ML content.*'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: Outline
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[Why KG representation learning is stuck in 2018](#974c)'
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Theory: What makes a model inductive and transferable?](#062b)'
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Theory: Equivariance in multi-relational graphs](#fbb0)'
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[ULTRA: A Foundation Model for KG Reasoning](#86f8)'
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Experiments: Best even in the zero-shot inference, Scaling behavior](#2517)'
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Code, Data, Checkpoints](#71ab)'
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why KG representation learning is stuck in 2018
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The pretrain-finetune paradigm has been with us since 2018 when [ELMo](https://arxiv.org/abs/1802.05365)
    and [ULMFit](https://arxiv.org/abs/1801.06146) showed first promising results
    and they were later cemented with [BERT](https://arxiv.org/abs/1810.04805) and
    [GPT](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf).
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: In the era of *large language models* (LLM) and more general *foundation models*
    (FMs), we often have a single model (like GPT-4 or Llama-2) pre-trained on enormous
    amounts of data and capable of performing a sheer variety of language tasks in
    the zero-shot manner (or at least be fine-tuned on the specific dataset). These
    days, multimodal FMs even support language, vision, audio, and other modalities
    in the same one model.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: 'Things work a little differently in Graph ML. Particularly, **what’s up with
    representation learning on KGs at the end of 2023?** The main tasks here are edge-level:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: 'Entity prediction (or knowledge graph completion) `(h,r,?)`: given a head node
    and relation, rank all nodes in the graph that can potentially be true tails.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Relation prediction `(h,?,t)`: given two nodes, predict a relation type between
    them'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Turns out, up until now it has been somewhere in pre-2018\. The key problem
    is:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: Each KG has its own set of entities and relations, there is no single pre-trained
    model that would transfer to any graph.
  id: totrans-28
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: For example, if we look at Freebase (a KG behind Google Knowledge Graph) and
    Wikidata (the largest open-source KG), they have absolutely different sets of
    entities (86M vs 100M) and relations (1500 vs 6000). Is there any hope for current
    KG representation learning methods to be trained on one graph and transfer to
    another?
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果我们查看Freebase（Google知识图谱背后的知识图谱）和Wikidata（最大的开源知识图谱），它们具有完全不同的实体集合（86M对100M）和关系（1500对6000）。当前的知识图谱表示学习方法是否有希望在一个图上训练并转移到另一个图上？
- en: '![](../Images/c68c80ce7bd14dbad8070aadc789c5d0.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c68c80ce7bd14dbad8070aadc789c5d0.png)'
- en: Different vocabularies of Freebase and Wikidata. Image by Authors.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: Freebase和Wikidata的词汇表不同。图片由作者提供。
- en: ❌ Classical transductive methods like TransE, ComplEx, RotatE, and hundreds
    of other embedding-based methods learn a **fixed set of entities and relation
    types** from the training graph and cannot even support new nodes added to the
    same graph. Shallow embedding-based methods do not transfer (in fact, we believe
    there is no point in developing such methods anymore except for some student project
    exercises).
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: ❌ 像TransE、ComplEx、RotatE等传统转导方法，以及其他数百种基于嵌入的方法，从训练图中学习**固定的实体和关系类型**，甚至无法支持同一图中添加的新节点。浅层嵌入方法无法转移（事实上，我们认为除了某些学生项目练习外，再也没有必要开发这样的技术）。
- en: 🟡 Inductive entity methods like [NodePiece](https://openreview.net/forum?id=xMJWUKJnFSw)
    and [Neural Bellman-Ford Nets](https://arxiv.org/pdf/2106.06935.pdf) do not learn
    entity embeddings. Instead, they parameterize training (seen) and new inference
    (unseen) nodes as a function of fixed relations. Since they **learn only relation
    embeddings**, it does allow them to transfer to graphs with new nodes but transfer
    to new graphs with different relations (like Freebase to Wikidata) is still beyond
    reach.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 🟡 像[NodePiece](https://openreview.net/forum?id=xMJWUKJnFSw)和[Neural Bellman-Ford
    Nets](https://arxiv.org/pdf/2106.06935.pdf)这样的归纳实体方法不会学习实体嵌入。相反，它们将训练（已见）和新的推理（未见）节点参数化为固定关系的函数。由于它们**只学习关系嵌入**，这使得它们能够转移到具有新节点的图中，但转移到具有不同关系的新图（例如从Freebase到Wikidata）仍然超出范围。
- en: '![](../Images/fe9e903615e0e5c9ed37b6ea92aff2d0.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fe9e903615e0e5c9ed37b6ea92aff2d0.png)'
- en: Relative entity representations enable inductive GNNs. Image by Authors.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 相对实体表示使得归纳GNN成为可能。图片由作者提供。
- en: What to do if you have **both** new entities and relations at inference time
    (a completely new graph)? If you don’t learn entity or relation embeddings, is
    the transfer theoretically possible? Let’s look into the theory then.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 如果在推理时**同时**出现新的实体和关系（一个全新的图）该怎么办？如果你不学习实体或关系嵌入，理论上转移是否可能？那我们就来探讨一下理论吧。
- en: 'Theory: What makes a model inductive and transferable?'
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理论：是什么使得模型具有归纳性和可转移性？
- en: 'Let’s define the setup more formally:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更正式地定义这个设置：
- en: KGs are directed, multi-relational graphs with arbitrary sets of nodes and relation
    types
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 知识图谱是有向的、多关系的图，具有任意的节点和关系类型集合
- en: Graphs arrive **without features**, that is, we don’t assume the existence of
    textual descriptions (nor pre-computed feature vectors) of entities and relations.
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图形到达时**没有特征**，即，我们不假设存在实体和关系的文本描述（也不假设有预计算的特征向量）。
- en: Given a query (head, relation, ?), we want to rank all nodes in the underlying
    graph (inference graph) and maximize the probability of returning a true tail.
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 给定一个查询（头，关系，？），我们希望对底层图（推理图）中的所有节点进行排名，并最大化返回真实尾部的概率。
- en: '*Transductive* setup: the set of nodes and entities is the same at training
    and inference time.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*转导*设置：训练和推理时节点和实体的集合是相同的。'
- en: '*Inductive* (entity) setup: the set of relations has to be fixed at training
    time, but nodes can be different at training and inference'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*归纳*（实体）设置：关系的集合必须在训练时固定，但节点在训练和推理时可以不同'
- en: '*Inductive* (entity and relation) setup: both new unseen entities and relations
    are allowed at inference'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*归纳*（实体和关系）设置：在推理时允许出现新的未见过的实体和关系'
- en: What do neural networks learn to be able to generalize to new data? The primary
    reference— the book on [Geometric Deep Learning by Bronstein, Bruna, Cohen, and
    Veličković](https://geometricdeeplearning.com/)—posits that it is a question of
    *symmetries and invariances*.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络学习什么以能够对新数据进行泛化？主要参考书籍—[Geometric Deep Learning by Bronstein, Bruna, Cohen,
    and Veličković](https://geometricdeeplearning.com/)—认为这是一个*对称性和不变性*的问题。
- en: What are the learnable invariances in foundation models? LLMs are trained on
    a fixed vocabulary of tokens (sub-word units, bytes, or even randomly initialized
    vectors as in [Lexinvariant LLMs](https://arxiv.org/abs/2305.16349)), vision models
    learn functions to project image patches, audio models learn to project audio
    patches.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 基础模型中可学习的不变性是什么？LLM 在固定的词汇表上进行训练（子词单元、字节，或如 [Lexinvariant LLMs](https://arxiv.org/abs/2305.16349)
    中所示的随机初始化向量），视觉模型学习投影图像块的函数，音频模型学习投影音频块。
- en: What are the learnable invariances for multi-relational graphs?
  id: totrans-47
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 多关系图中的可学习不变性是什么？
- en: First, we will introduce the invariances (equivariances) in standard **homogeneous**
    graphs.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将介绍标准 **齐次** 图中的不变性（等变性）。
- en: '*Standard (single) permutation equivariant graph models:* A great leap in graph
    ML came when early GNN work ([Scarselli et al. 2008](https://ro.uow.edu.au/cgi/viewcontent.cgi?article=10501&context=infopapers),
    [Xu et al. 2018](https://arxiv.org/abs/1810.00826), [Morris et al. 2018](https://ojs.aaai.org/index.php/AAAI/article/view/4384))
    has shown that inductive tasks on graphs benefited enormously from assuming that
    vertex IDs are arbitrary, such that the predictions of a graph model should not
    change if we reassigned vertex ID. This is known as *permutation equivariance*
    of the neural network on node IDs. This realization has created great excitement
    and a profusion of novel graph representation methods since, as long as the neural
    network is equivariant to node ID permutations, we can call it a graph model.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '*标准（单一）置换等变图模型：* 当早期的 GNN 研究（[Scarselli et al. 2008](https://ro.uow.edu.au/cgi/viewcontent.cgi?article=10501&context=infopapers)，[Xu
    et al. 2018](https://arxiv.org/abs/1810.00826)，[Morris et al. 2018](https://ojs.aaai.org/index.php/AAAI/article/view/4384)）展示出假设顶点
    ID 是任意的，图模型的预测不应因重新分配顶点 ID 而改变时，图 ML 取得了重大进展。这被称为神经网络在节点 ID 上的 *置换等变性*。这一认识激发了极大的兴奋，并产生了大量新颖的图表示方法，只要神经网络对节点
    ID 置换是等变的，我们就可以称其为图模型。'
- en: '![](../Images/be3ea4a2e3127b59fe07092c439734ca.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/be3ea4a2e3127b59fe07092c439734ca.png)'
- en: '*Single-relational graphs. GNNs are equivariant to node permutations: Michael
    Jackson’s node vector will have the same value even after re-labeling node IDs.
    Image by Authors.*'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '*单关系图。GNN 对节点置换是等变的：即使在重新标记节点 ID 后，Michael Jackson 的节点向量也会保持相同的值。图像来自作者。*'
- en: The permutation equivariance on node IDs allows GNNs to inductively (zero-shot)
    transfer the patterns learned from a training graph to another (different) test
    graph. This is a consequence of the equivariance, since the neural network cannot
    use node IDs to produce embeddings, it must use the graph structure. This creates
    what we know as *structural representations* in graphs (see [Srinivasan & Ribeiro
    (ICLR 2020)](https://iclr.cc/virtual_2020/poster_SJxzFySKwH.html)).
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 节点 ID 上的置换等变性允许 GNN 以归纳（零-shot）方式将从训练图中学到的模式转移到另一个（不同的）测试图中。这是等变性的结果，因为神经网络不能使用节点
    ID 生成嵌入，它必须使用图结构。这就产生了我们所知的 *结构表示*（见 [Srinivasan & Ribeiro (ICLR 2020)](https://iclr.cc/virtual_2020/poster_SJxzFySKwH.html)）。
- en: Equivariance in multi-relational graphs
  id: totrans-53
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 多关系图中的等变性
- en: Now edges in the graphs might have different relation types — is there any GNN
    theory for such graphs?
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 现在图中的边可能有不同的关系类型——是否有针对这种图的 GNN 理论？
- en: 1️⃣ In our previous work, [Weisfeiler and Leman Go Relational](https://arxiv.org/abs/2211.17113)
    (with Pablo Barceló, Christopher Morris, and Miguel Romero Orth, LoG 2022), we
    derived Relational WL — a WL expressiveness hierarchy for multi-relational graphs
    focusing more on node-level tasks. The great [follow-up work by Huang et al (NeurIPS
    2023)](https://arxiv.org/abs/2302.02209) extended the theory to link prediction,
    formalized *conditional message passing,* and logical expressiveness using Relational
    WL. ✍️ Let’s remember **conditional message passing** — we’ll need it later —
    it provably improves link prediction performance.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 1️⃣ 在我们之前的工作中，[Weisfeiler and Leman Go Relational](https://arxiv.org/abs/2211.17113)（与
    Pablo Barceló、Christopher Morris 和 Miguel Romero Orth 合作，LoG 2022），我们推导出了 Relational
    WL —— 一个更侧重于节点级任务的多关系图的 WL 表达层级。黄等人（NeurIPS 2023）[的伟大后续工作](https://arxiv.org/abs/2302.02209)
    将理论扩展到链接预测，形式化了 *条件消息传递* 和使用 Relational WL 的逻辑表达能力。✍️ 让我们记住 **条件消息传递** —— 我们稍后会需要它
    —— 它被证明能改善链接预测性能。
- en: The proposed addition of a global readout vector induced by incoming/outgoing
    edge direction resembles the [recent work of Emanuele Rossi et al](https://arxiv.org/abs/2305.10498)
    on studying directionality in homogeneous MPNNs (read [the blog post on Medium](/direction-improves-graph-learning-170e797e94fe)
    for more details). Still, those works do not envision the case when even relations
    at test time are unseen.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: '*2️⃣ Double permutation equivariant (multi-relational) graph models:* Recently,
    [Gao et al. 2023](https://arxiv.org/abs/2302.01313) proposed the concept of **double
    equivariance** for multi-relational graphs. Double equivariance forces the neural
    network to be equivariant to the joint permutations of both node IDs and relation
    IDs. This ensures the neural network learns structural patterns between nodes
    and relations, which allows it to inductively (zero-shot) transfer the learned
    patterns to another graph with new nodes and new relations.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1ad9763efc24de9bca5a6c3a6bace408.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
- en: '*Double equivariance in multi-relational graphs. Permuting both node IDs and
    relation IDs does not change the relational structure. Hence, the output node
    states should be the same (but permuted). Image by Authors.*'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: '➡️ In our work, we find *the invariance of relation interactions*, that is,
    even if relation identities are different, their fundamental interactions remain
    the same, and those fundamental interactions can be captured by a **graph of relations.**
    In the graph of relations, each node is a relation type from the original graph.
    Two nodes in this graph will be connected if edges with those relation types in
    the original graph are incident (that is, they share a head or tail node). Depending
    on the incidence, we distinguish **4 edge types** in the graph of relations:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: '*Head-to-head (h2h)* — two relations can start from the same head entity;'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Tail-to-head (t2h)* — tail entity of one relation can be a head of another
    relation;'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Head-to-tail (h2t)* — head entity of one relation can be a tail of another
    relation;'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Tail-to-tail (t2t)* — two relations can have the same tail entity.'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/5170699f4814c0eb228294f5ea19e974.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
- en: '*Different incidence patterns in the original graph produce different interactions
    in the graph of relations. The right-most: the example relation graph (inverse
    edges are omitted for clarity). Image by Authors*'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
- en: 'A few nice properties of the relation graph:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: It can be built from absolutely any multi-relational graph (with simple sparse
    matrix multiplications)
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The 4 fundamental interactions never change because they just encode the basic
    topology — in directed graphs there always will be head and tail nodes, and we
    relations would have those incidence patterns
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Essentially, learning representations over the relations graph can transfer
    to any multi-relational graph! This is the *learnable invariance*.
  id: totrans-70
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: In fact, it can be shown (we are already working on the formal proofs which
    will be available in an upcoming work 😉) that representing relations via their
    interactions in a graph of relations is a double equivariant model! This means
    that learned relational representations are independent of identities but rather
    rely on the joint interactions between relations, nodes, and nodes & relations.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，可以证明（我们已经在进行正式证明，这些将在即将发布的工作中提供😉），通过关系图中的交互来表示关系是一个双等变模型！这意味着学习到的关系表示是独立于身份的，而是依赖于关系、节点及节点与关系之间的联合交互。
- en: 'ULTRA: A Foundation Model for KG Reasoning'
  id: totrans-72
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 'ULTRA: 一个用于KG推理的基础模型'
- en: With all the theoretical foundations backing us up, we are now ready to introduce
    ULTRA.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在所有理论基础的支持下，我们现在准备介绍ULTRA。
- en: ULTRA is a method for unified, learnable, and transferable graph representations.
    ULTRA leverages the invariances (and equivariances) of the **graph of relations**
    with its fundamental interactions and applies **conditional message passing**
    to get relative relational representations. Perhaps the coolest fact is that
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: ULTRA是一种统一的、可学习的、可迁移的图表示方法。ULTRA利用**关系图**的基本交互的不变性（和等变性），并应用**条件消息传递**来获得相对关系表示。也许最酷的事实是
- en: a single pre-trained ULTRA model can run 0-shot inference on any possible multi-relational
    graph and be fine-tuned on any graph.
  id: totrans-75
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 一个经过预训练的ULTRA模型可以在任何可能的多关系图上进行0-shot推理，并在任何图上进行微调。
- en: In other words, ULTRA is pretty much a foundation model that can run inference
    on any graph input (with already good performance) and be fine-tuned on any target
    graph of interest.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，ULTRA实际上是一个基础模型，可以在任何图输入上进行推理（表现已相当出色），并且可以在任何目标图上进行微调。
- en: The crucial component of ULTRA is in *relative* relation representations constructed
    from the graph of relations. Given a query `(Michael Jackson, genre, ?)`, we first
    initialize the `genre` node in the graph of relations with the all-ones vector
    (all other nodes are initialized with zeros). Running a GNN, the resulting node
    embeddings of the relation graph are conditioned on the `genre` node — it means
    that each starting initialized relation will have its own matrix of relational
    features, and that’s very helpful from many theoretical and practical aspects!
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: ULTRA的关键组成部分是从关系图中构建的*相对*关系表示。给定一个查询 `(Michael Jackson, genre, ?)`，我们首先用全1向量初始化关系图中的`genre`节点（所有其他节点初始化为0）。运行GNN后，关系图的节点嵌入以`genre`节点为条件——这意味着每个起始初始化关系将拥有自己的一组关系特征矩阵，这在许多理论和实际方面都非常有用！
- en: '![](../Images/3a2a3e03826f084d0bd9909871b263b3.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3a2a3e03826f084d0bd9909871b263b3.png)'
- en: '*ULTRA employs relative relation representations (a labeling trick over the
    graph of relations) such that each relation (eg, “genre”) has its own unique matrix
    of all relation representations. Image by Authors.*'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '*ULTRA采用相对关系表示（在关系图上的标记技巧），使得每个关系（例如，“类型”）都有其唯一的关系表示矩阵。图片来源：作者。*'
- en: 'Practically, given an input KG and a (h, r, ?) query, ULTRA executes the following
    actions:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，给定一个输入KG和一个(h, r, ?)查询，ULTRA执行以下操作：
- en: Construction of the graph of relations;
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 构建关系图；
- en: Get relation features from the conditional message passing GNN on the graph
    of relations (conditioned on the initialized query relation r);
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从条件消息传递GNN中获取关系特征（以初始化查询关系r为条件）；
- en: Use the obtained relational representations for the inductive link predictor
    GNN conditioned on the initialized head node h;
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用获得的关系表示来进行条件化的链接预测GNN，条件为初始化的头节点h；
- en: Steps 2 and 3 are implemented via slightly different modifications of the [Neural
    Bellman-Ford net (NBFNet)](https://arxiv.org/pdf/2106.06935.pdf). ULTRA only learns
    embeddings of the 4 fundamental interactions (h2t, t2t, t2h, h2h) and GNN weights
    — pretty small overall. The main model we experimented with has only 177k parameters.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 步骤2和3通过对[神经贝尔曼-福特网络 (NBFNet)](https://arxiv.org/pdf/2106.06935.pdf)进行略微不同的修改来实现。ULTRA仅学习4种基本交互（h2t,
    t2t, t2h, h2h）和GNN权重——总体上非常小。我们实验的主要模型仅有177k参数。
- en: '![](../Images/69c48ade8d1640695e8a31d647795722.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/69c48ade8d1640695e8a31d647795722.png)'
- en: '*Three main steps taken by ULTRA: (1) building a relation graph; (2) running
    conditional message passing over the relation graph to get relative relation representations;
    (3) use those representations for inductive link predictor GNN on the entity level.
    Image by Authors.*'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '*ULTRA 采取的三个主要步骤：(1) 构建关系图；(2) 在关系图上进行条件消息传递以获取相对关系表示；(3) 使用这些表示进行实体级别的归纳链接预测
    GNN。图片来源：作者。*'
- en: 'Experiments: Best even in the zero-shot inference and Fine-tuning'
  id: totrans-87
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实验：在零样本推理和微调中表现最佳
- en: We pre-trained ULTRA on 3 standard KGs based on Freebase, Wikidata, and Wordnet,
    and ran 0-shot link prediction on 50+ other KGs of various sizes from 1k — 120k
    nodes and 2k edges — 1.1M edges.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在基于 Freebase、Wikidata 和 Wordnet 的 3 个标准知识图谱上预训练了 ULTRA，并在 50 多个其他知识图谱上进行了零样本链接预测，这些知识图谱的规模从
    1k 到 120k 节点和 2k 边到 1.1M 边不等。
- en: Averaged across the datasets with known SOTA, a single pre-trained ULTRA model
    is **better in the 0-shot inference mode** than existing SOTA models trained specifically
    on each graph 🚀Fine-tuning improves the performance even 10% further. It’s particularly
    amazing that a single trained ULTRA model can scale to graphs of such different
    sizes (100x difference in node size and 500x in the edge sizes) whereas GNNs are
    known to suffer from size generalization issues (see the prominent works by [Yehudai
    et al, ICML 2021](https://arxiv.org/abs/2010.08853) and [Zhou et al, NeurIPS 2022](https://arxiv.org/abs/2205.15117)).
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在已知 SOTA 的数据集上平均，单个预训练的 ULTRA 模型在**零样本推理模式下表现优于**针对每个图专门训练的现有 SOTA 模型 🚀 微调甚至能使性能提升
    10%。特别令人惊讶的是，单个训练的 ULTRA 模型能够适应如此不同规模的图（节点大小差异为 100 倍，边大小差异为 500 倍），而 GNN 通常会遇到规模泛化问题（参见
    [Yehudai et al, ICML 2021](https://arxiv.org/abs/2010.08853) 和 [Zhou et al, NeurIPS
    2022](https://arxiv.org/abs/2205.15117) 的突出工作）。
- en: '![](../Images/c040b43c06c13861b944f7734708cda5.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c040b43c06c13861b944f7734708cda5.png)'
- en: A single pre-trained ULTRA is better even in the 0-shot inference mode than
    supervised SOTA modes trained end-to-end on specific graphs (look at the Average
    column). Fine-tuning improves the performance even further. Image by Authors
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 单个预训练的 ULTRA 在零样本推理模式下的表现优于在特定图上端到端训练的监督 SOTA 模型（参见平均列）。微调进一步提升了性能。图片来源：作者
- en: 🙃 In fact, with 57 tested graphs, we ran a bit out of KGs to test ULTRA on.
    So if you have a fresh new benchmark hidden somewhere — let us know!
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 🙃 实际上，经过 57 个测试图，我们有点用尽了测试 ULTRA 的知识图谱。如果你有新的基准藏在某处——告诉我们！
- en: Scaling Behavior
  id: totrans-93
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 扩展行为
- en: We can bump the zero-shot performance even more by adding more graphs to the
    pre-training mixture although we do observe certain performance saturation after
    training on 4+ graphs.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过将更多图谱添加到预训练混合中进一步提升零样本性能，尽管我们确实观察到在训练 4 个以上图谱后性能出现一定的饱和。
- en: The church of [Scaling Laws](https://arxiv.org/abs/2001.08361) predicts even
    better performance with bigger models trained on more qualitative data, so it’s
    definitely on our agenda.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '[Scaling Laws](https://arxiv.org/abs/2001.08361) 预言了使用更大的模型在更多优质数据上训练会有更好的表现，因此这绝对在我们的计划之中。'
- en: '![](../Images/ef48bedd78ce63c719bf0a0f8a343dd8.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ef48bedd78ce63c719bf0a0f8a343dd8.png)'
- en: Zero-shot performance increases with more diverse graphs in the pre-training
    mix. Image by Authors.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 零样本性能随着预训练混合图谱的多样性而增加。图片来源：作者。
- en: 'Conclusion: Code, Data, Checkpoints'
  id: totrans-98
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论：代码、数据、检查点
- en: So foundation models for KG reasoning are finally here, we are past that 2018
    threshold! A single pre-trained ULTRA model can perform link prediction on any
    KG (multi-relational graph) from any domain. You really just need a graph with
    more than 1 edge type to get going.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，知识图谱推理的基础模型终于来了，我们已经过了 2018 年的门槛！单个预训练的 ULTRA 模型可以对任何领域的知识图谱（多关系图）进行链接预测。你只需一个具有超过
    1 种边类型的图谱即可开始。
- en: 📈 Practically, ULTRA demonstrates very promising performance on a variety of
    KG benchmarks already in the 0-shot mode, but you can bump the performance even
    further with a short fine-tuning.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 📈 实际上，ULTRA 在多种知识图谱基准测试中的零样本模式下已经展示了非常有前景的性能，但你可以通过短时间的微调进一步提升性能。
- en: We make all the code, training data, and pre-trained model checkpoints available
    on GitHub so you can start running ULTRA on your data right away!
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在 GitHub 上提供了所有代码、训练数据和预训练模型检查点，以便你可以立即在你的数据上运行 ULTRA！
- en: '📜 preprint: [arxiv](https://arxiv.org/abs/2310.04562)'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 📜 预印本：[arxiv](https://arxiv.org/abs/2310.04562)
- en: '🛠️ Code, data: [Githtub repo](https://github.com/DeepGraphLearning/ULTRA)'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 🛠️ 代码、数据：[Githtub repo](https://github.com/DeepGraphLearning/ULTRA)
- en: '🍪 Checkpoints: 2 checkpoints (2 MB each) in the [Github repo](https://github.com/DeepGraphLearning/ULTRA)'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 🍪 检查点：在 [Github repo](https://github.com/DeepGraphLearning/ULTRA) 中有 2 个检查点（每个
    2 MB）
- en: '🌎 Project website: [here](https://deepgraphlearning.github.io/project/ultra)'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 🌎 项目网站：[这里](https://deepgraphlearning.github.io/project/ultra)
- en: As a closing remark, KG reasoning just represents a fraction of the many interesting
    problems in the reasoning domain, and the majority still don’t have a generic
    solution. We believe the success of KG reasoning will bring more breakthroughs
    in other reasoning domains (for example, we recently found that [LLMs can actually
    learn and employ textual rules](https://arxiv.org/abs/2310.07064)). Let’s stay
    optimistic about the future of reasoning!
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 作为总结，KG 推理只是推理领域中许多有趣问题的一部分，大多数问题仍然没有通用的解决方案。我们相信，KG 推理的成功将为其他推理领域带来更多突破（例如，我们最近发现了[LLMs
    实际上可以学习和运用文本规则](https://arxiv.org/abs/2310.07064)）。让我们对推理的未来保持乐观！
