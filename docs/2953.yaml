- en: Mastering Customer Segmentation with LLM
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/mastering-customer-segmentation-with-llm-3d9008235f41?source=collection_archive---------0-----------------------#2023-09-26](https://towardsdatascience.com/mastering-customer-segmentation-with-llm-3d9008235f41?source=collection_archive---------0-----------------------#2023-09-26)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Unlock advanced customer segmentation techniques using LLMs, and improve your
    clustering models with advanced techniques
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@damiangilgonzalez?source=post_page-----3d9008235f41--------------------------------)[![Damian
    Gil](../Images/8b378c321ee21b0bd40faa14db7e9487.png)](https://medium.com/@damiangilgonzalez?source=post_page-----3d9008235f41--------------------------------)[](https://towardsdatascience.com/?source=post_page-----3d9008235f41--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----3d9008235f41--------------------------------)
    [Damian Gil](https://medium.com/@damiangilgonzalez?source=post_page-----3d9008235f41--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F87864cbc1dda&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmastering-customer-segmentation-with-llm-3d9008235f41&user=Damian+Gil&userId=87864cbc1dda&source=post_page-87864cbc1dda----3d9008235f41---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----3d9008235f41--------------------------------)
    ·24 min read·Sep 26, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F3d9008235f41&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmastering-customer-segmentation-with-llm-3d9008235f41&user=Damian+Gil&userId=87864cbc1dda&source=-----3d9008235f41---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F3d9008235f41&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmastering-customer-segmentation-with-llm-3d9008235f41&source=-----3d9008235f41---------------------bookmark_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: Content Table
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**·** [**Intro**](#fbea) **·** [**Data**](#7ac3) **·** [**Method 1: Kmeans**](#c662)
    **·** [**Method 2: K-Prototype**](#1422) **·** [**Method 3: LLM + Kmeans**](#3a33)
    **·** [**Conclusion**](#e320)'
  prefs: []
  type: TYPE_NORMAL
- en: Intro
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A customer segmentation project can be approached in multiple ways. In this
    article I will teach you advanced techniques, not only to define the clusters,
    but to analyze the result. This post is intended for those data scientists who
    want to have several tools to address clustering problems and be one step closer
    to being seniors DS.
  prefs: []
  type: TYPE_NORMAL
- en: What will we see in this article?
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Let’s see 3 methods to approach this type of project:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Kmeans**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**K-Prototype**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**LLM + Kmeans**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'As a small preview I will show the following comparison of the 2D representation
    (PCA) of the different models created:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c1271719684d43f409e5765fd0eb7aa7.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Graphic comparison of the three methods** (Image by Author).'
  prefs: []
  type: TYPE_NORMAL
- en: 'You will also learn dimensionality reduction techniques such as:'
  prefs: []
  type: TYPE_NORMAL
- en: '**PCA**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**t-SNE**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**MCA**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Some of the results being these:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f9eb2a950a822ada5b1ad3afadc701fe.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Graphical comparison of the three dimensionality reduction methods** (Image
    by Author).'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can find the project with the notebooks [**here**](https://github.com/damiangilgonzalez1995/Clustering-with-LLM)**.**
    And you can also take a look at my github:'
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://github.com/damiangilgonzalez1995?source=post_page-----3d9008235f41--------------------------------)
    [## damiangilgonzalez1995 - Overview'
  prefs: []
  type: TYPE_NORMAL
- en: Passionate about data, I transitioned from physics to data science. Worked at
    Telefonica, HP, and now CTO at…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: github.com](https://github.com/damiangilgonzalez1995?source=post_page-----3d9008235f41--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: 'A very important clarification is that this is not an end-to-end project. This
    is because we have skipped one of the most important parts in this type of project:
    **The exploratory data analysis (EDA) phase or the selection of variables.**'
  prefs: []
  type: TYPE_NORMAL
- en: Data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The original data used in this project is from a public Kaggle: [Banking Dataset
    — Marketing Targets](https://www.kaggle.com/datasets/prakharrathi25/banking-dataset-marketing-targets).
    Each row in this data set contains information about a company’s customers. Some
    fields are numerical and others are categorical, we will see that this expands
    the possible ways to approach the problem.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We will only be left with the first 8 columns. Our dataset looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a7f07db265fac93d2a304ea879240b76.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let’s see a brief description of the columns of our dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '**age** (numeric)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**job** : type of job (categorical: “admin.” ,”unknown”,”unemployed”, ”management”,
    ”housemaid”, ”entrepreneur”, ”student”, “blue-collar”, ”self-employed”, ”retired”,
    ”technician”, ”services”)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**marital** : marital status (categorical: “married”,”divorced”,”single”; note:
    “divorced” means divorced or widowed)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**education** (categorical: “unknown”,”secondary”,”primary”,”tertiary”)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**defaul**t: has credit in default? (binary: “yes”,”no”)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**balance**: average yearly balance, in euros (numeric)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**housing**: has housing loan? (binary: “yes”,”no”)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**loan**: has personal loan? (binary: “yes”,”no”)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For the project, I’ve utilized the training dataset by Kaggle.[**In the project
    repository**](https://github.com/damiangilgonzalez1995/Clustering-with-LLM), you
    can locate the ***“data”*** folder where a compressed file of the dataset used
    in the project is stored. Additionally, you will find two CSV files inside of
    the compressed file. One is the training dataset provided by Kaggle **(train.csv)**,
    and the other is the dataset after performing an embedding (**embedding_train.csv**),
    which we will explain further later on.
  prefs: []
  type: TYPE_NORMAL
- en: 'To further clarify how the project is structured, the project tree is shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Method 1: Kmeans'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This is the most common method and the one you will surely know. Anyway, we
    are going to study it because I will show advanced analysis techniques in these
    cases. The Jupyter notebook where you will find the complete procedure is called
    [**kmeans.ipynb**](https://github.com/damiangilgonzalez1995/Clustering-with-LLM/blob/main/kmeans.ipynb)
  prefs: []
  type: TYPE_NORMAL
- en: Preprocessed
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A preprocessing of the variables is carried out:'
  prefs: []
  type: TYPE_NORMAL
- en: It consists of converting categorical variables into numeric ones.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We are going to apply Onehot encoder for the nominal variables and a OrdinalEncoder
    for the ordinals features (*education*).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: We try to ensure that the numerical variables have a Gaussian distribution.
    For them we will apply a PowerTransformer.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Let’s see how it looks in code.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6e0a4a2e7e601e8d71cc086968d5c904.png)'
  prefs: []
  type: TYPE_IMG
- en: Outliers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: It is crucial that there are as few outliers in our data as Kmeans is very sensitive
    to this. We can apply the typical method of choosing outliers using the z score,
    but in this post I will show you a much more advanced and cool method.
  prefs: []
  type: TYPE_NORMAL
- en: Well, what is this method? Well, we will use the [Python Outlier Detection (PyOD)
    library](https://pyod.readthedocs.io/en/latest/). This library is focused on detecting
    outliers for different cases. To be more specific we will use the **ECOD** method
    (“**empirical cumulative distribution functions for outlier detection**”).
  prefs: []
  type: TYPE_NORMAL
- en: This method seeks to obtain the distribution of the data and thus know which
    are the values ​​where the probability density is lower (outliers). Take a look
    at the [Github](https://medium.com/r?url=https%3A%2F%2Fgithub.com%2Fyzhao062%2Fpyod)
    if you want.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Modeling
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'One of the disadvantages of using the Kmeans algorithm is that you must choose
    the number of clusters you want to use. In this case, in order to obtain that
    data, we will use [Elbow Method](https://www.geeksforgeeks.org/elbow-method-for-optimal-value-of-k-in-kmeans/).
    It consists of calculating the distortion that exists between the points of a
    cluster and its centroid. The objective is clear, to obtain the least possible
    distortion. In this case we use the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c1f9dbd17d6bf7a464b92bc1bf1eacad.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Elbow score for different numbers of clusters** (Image by Author).'
  prefs: []
  type: TYPE_NORMAL
- en: 'We see that from **k=5**, the distortion does not vary drastically. It is true
    that the ideal is that the behavior starting from k= 5 would be almost flat. This
    rarely happens and other methods can be applied to be sure of the most optimal
    number of clusters. To be sure, we could perform a [**Silhoutte**](https://www.scikit-yb.org/en/latest/api/cluster/silhouette.html)
    **visualization**. The code is the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'It can be seen that the highest silhouette score is obtained with n_cluster=9,
    but it is also true that the variation in the score is quite small if we compare
    it with the other scores. At the moment the previous result does not provide us
    with much information. On the other hand, the previous code creates the Silhouette
    visualization, which gives us more information:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8cbb7e81daf794103550e8d419037673.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Graphic representation of the silhouette method for different numbers of
    clusters** (Image by Author).'
  prefs: []
  type: TYPE_NORMAL
- en: Since understanding these representations well is not the goal of this post,
    I will conclude that there seems to be no very clear decision as to which number
    is best. After viewing the previous representations, we can choose **K=5 or K=
    6**. This is because for the different clusters, their Silhouette score is above
    the average value and there is no imbalance in cluster size. Furthermore, in some
    situations, the marketing department may be interested in having the smallest
    number of clusters/types of customers (This may or may not be the case).
  prefs: []
  type: TYPE_NORMAL
- en: Finally we can create our Kmeans model with K=5.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Evaluation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The way of evaluating kmeans models is somewhat more open than for other models.
    We can use
  prefs: []
  type: TYPE_NORMAL
- en: metrics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: visualizations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: interpretation (Something very important for companies).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In relation to the **model evaluation metrics**, we can use the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: As far as can be shown, we do not have an excessively good model. **Davies’
    score** is telling us that the distance between clusters is quite small.
  prefs: []
  type: TYPE_NORMAL
- en: This may be due to several factors, but keep in mind that the energy of a model
    is the data; if the data does not have sufficient predictive power, you cannot
    expect to achieve exceptional results.
  prefs: []
  type: TYPE_NORMAL
- en: For **visualizations**, we can use the method to **reduce dimensionality, PCA**.
    For them we are going to use the [**Prince**](https://github.com/MaxHalford/prince)library,
    focused on exploratory analysis and dimensionality reduction. If you prefer, you
    can use Sklearn’s PCA, they are identical.
  prefs: []
  type: TYPE_NORMAL
- en: 'First we will calculate the principal components in 3D, and then we will make
    the representation. These are the two functions performed by the previous steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Don’t worry too much about those functions, use them as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b991037bdf1dd3d0c83db66f2a6ddd13.png)'
  prefs: []
  type: TYPE_IMG
- en: '**PCA space and the clusters created by the model** (Image by Author).'
  prefs: []
  type: TYPE_NORMAL
- en: It can be seen that the clusters have almost no separation between them and
    there is no clear division. This is in accordance with the information provided
    by the metrics.
  prefs: []
  type: TYPE_NORMAL
- en: Something to keep in mind and that very few people keep in mind is the PCA and
    the **variability of the eigenvectors.**
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Let’s say that each field contains a certain amount of information, and this
    adds its bit of information. If the accumulated sum of the 3 main components adds
    up to around 80% variability, we can say that it is acceptable, obtaining good
    results in the representations. If the value is lower, we have to take the visualizations
    with a grain of salt since we are missing a lot of information that is contained
    in other eigenvectors.
  prefs: []
  type: TYPE_NORMAL
- en: 'The next question is obvious: What is the variability of the PCA executed?'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'The answer is the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/26a4fbdc878720fdb241a57f77e1d75d.png)'
  prefs: []
  type: TYPE_IMG
- en: As can be seen, we have 27.98% variability with the first 3 components, something
    insufficient to draw informed conclusions.
  prefs: []
  type: TYPE_NORMAL
- en: When we apply the PCA method, since it is a linear algorithm, it is not capable
    of capturing more complex relationships. Luckily there is a method called **t-SNE**,
    which is capable of capturing ***these complex polynomial relationships***. This
    can help us visualize, since with the previous method we have not had much success.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you try it on your computers, keep in mind that it has a higher computational
    cost. For this reason, I sampled my original dataset and it still took me about
    5 minutes to get the result. The code is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: As a result, I got the following image. It shows a clearer separation between
    clusters but unfortunately, we still don’t have good results.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/fa9abe56ddb5c100324777929b894326.png)'
  prefs: []
  type: TYPE_IMG
- en: '**t-SNE space and the clusters created by the model** (Image by Author).'
  prefs: []
  type: TYPE_NORMAL
- en: In fact, we can compare the reduction carried out by the **PCA and by the t-SNE,
    in 2 dimensions**. The improvement is clear using the second method.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e53af2aade94ed7dc1d6b79ab146e31c.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Different results for different dimensionality reduction methods and clusters
    defined by the model** (Image by Author).'
  prefs: []
  type: TYPE_NORMAL
- en: Finally, let’s explore a little how the model works, in which features are the
    most important and what are the main characteristics of the clusters.
  prefs: []
  type: TYPE_NORMAL
- en: To see the importance of each of the variables we will use a typical “trick”
    in this type of situation. We are going to create a classification model where
    the “X” is the inputs of the Kmeans model, and the “y” is the clusters predicted
    by the Kmeans model.
  prefs: []
  type: TYPE_NORMAL
- en: 'The chosen model is an [**LGBMClassifier**](https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.LGBMClassifier.html).
    This model is quite powerful and works well having categorical and numerical variables.
    Having the new model trained, using the [**SHAP**](https://shap.readthedocs.io/en/latest/)library,
    we can obtain the importance of each of the features in the prediction. The code
    is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0e4c8e00f1bf8d52176769cc8a90303c.png)'
  prefs: []
  type: TYPE_IMG
- en: '**The importance of the variables in the model** (Image by Author).'
  prefs: []
  type: TYPE_NORMAL
- en: It can be seen that feature ***age*** has the greatest predictive power. It
    can also be seen that cluster number 3 (green) is mainly differentiated by the
    ***balance*** variable.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally we must analyze the characteristics of the clusters. This part of the
    study is what is decisive for the business. For them we are going to obtain the
    means (for the numerical variables) and the most frequent value (categorical variables)
    of each of the features of the dataset for each of the clusters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/95880af6faec1e9d360bc420abaf2c5d.png)'
  prefs: []
  type: TYPE_IMG
- en: We see that the clusters with **job=blue-collar** do not have a great differentiation
    between their characteristics, except by the age feature. This is something that
    is not desirable since it is difficult to differentiate the clients of each of
    the clusters. In the **job=management** case, we obtain better differentiation.
  prefs: []
  type: TYPE_NORMAL
- en: 'After carrying out the analysis in different ways, they converge on the same
    conclusion: **“We need to improve the results”.**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Method 2: K-Prototype'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If we remember our original dataset, we see that we have categorical and numerical
    variables. Unfortunately, the Kmeans algorithm provided by Skelearn does not accept
    categorical variables, forcing the original dataset to be modified and drastically
    altered.
  prefs: []
  type: TYPE_NORMAL
- en: Luckily, you’ve taken with me and my post. But above all, thanks to **ZHEXUE
    HUANG** and his article [**Extensions to the k-Means Algorithm for Clustering
    Large Data Sets with Categorical Values**](https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.15.4028&rep=rep1&type=pdf),
    there is an algorithm that accepts categorical variables for clustering. This
    algorithm is called **K-Prototype**. The bookstore that provides it is [**Prince**](https://github.com/MaxHalford/prince/blob/master/README.md).
  prefs: []
  type: TYPE_NORMAL
- en: The procedure is the same as in the previous case. In order not to make this
    article eternal, let’s go to the most interesting parts. But remember that you
    can access the [**Jupyter notebook here**](https://github.com/damiangilgonzalez1995/Clustering-with-LLM/blob/main/kprototypes.ipynb).
  prefs: []
  type: TYPE_NORMAL
- en: Preprocessed
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Because we have numerical variables, we must make certain modifications to
    them. It is always recommended that all numerical variables be on similar scales
    and with distributions as close as possible to Gaussian ones. The dataset that
    we will use to create the models is created as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/11cc86976685f0fd416311a6ab0fbc45.png)'
  prefs: []
  type: TYPE_IMG
- en: Outliers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Because the method that I have presented for outlier detection **(ECOD)** only
    accepts numerical variables, the same transformation must be performed as for
    the kmeans method. We apply the outlier detection model that will provide us with
    which rows to eliminate, finally leaving the dataset that we will use as input
    for the K-Prototype model:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a98ec365178e9a9c8db1123e62d92d38.png)'
  prefs: []
  type: TYPE_IMG
- en: Modeling
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We create the model and to do this we first need to obtain the optimal k. To
    do this we use the **Elbow Method** and this piece of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7edaed91d38897cafd9b86388121b704.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Elbow score for different numbers of clusters** (Image by Author).'
  prefs: []
  type: TYPE_NORMAL
- en: We can see that the best option is **K=5**.
  prefs: []
  type: TYPE_NORMAL
- en: Be careful, since this algorithm takes a little longer than those normally used.
    For the previous graph, 86 minutes were needed, something to keep in mind.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4449fc4681f03208a1bbea5558a8c25c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Well, we are now clear about the number of clusters, we just have to create
    the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: We already have our model and its predictions, we just need to evaluate it.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As we have seen before we can apply several visualizations to obtain an intuitive
    idea of how good our model is. Unfortunately the PCA method and t-SNE do not admit
    categorical variables. But don’t worry, since the [**Prince**](https://github.com/MaxHalford/prince/blob/master/README.md)
    library contains the [**MCA (Multiple correspondence analysis)**](https://maxhalford.github.io/prince/mca/)
    method and it does accept a mixed dataset. In fact, I encourage you to visit the
    [**Github**](https://github.com/MaxHalford/prince) of this library, it has several
    super useful methods for different situations, see the following image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a2500d92228134365fd7622e8bc462d4.png)'
  prefs: []
  type: TYPE_IMG
- en: '**The different methods of dimensionality reduction by type of case** (Image
    by Author and Prince Documentation).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Well, the plan is to apply a MCA to reduce the dimensionality and be able to
    make graphical representations. For this we use the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '**Remember that if you want to follow each step 100%, you can take a look at**
    [**Jupyter notebook.**](https://github.com/damiangilgonzalez1995/Clustering-with-LLM/blob/main/kprototypes.ipynb)'
  prefs: []
  type: TYPE_NORMAL
- en: 'The dataset named ***mca_3d_df*** contains that information:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ee1d6c7977ffd3adb5dffd1be597b218.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let’s make a plot using the reduction provided by the MCA method:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8ee972b3bc56c7b24139a73a20832f7a.png)'
  prefs: []
  type: TYPE_IMG
- en: '**MCA space and the clusters created by the model** (Image by Author)'
  prefs: []
  type: TYPE_NORMAL
- en: Wow, it doesn’t look very good… It is not possible to differentiate the clusters
    from each other. We can say then that the model is not good enough, right?
  prefs: []
  type: TYPE_NORMAL
- en: 'I hope you said something like:'
  prefs: []
  type: TYPE_NORMAL
- en: '**“Hey Damian, don’t go so fast!! Have you looked at the variability of the
    3 components provided by the MCA?”**'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Indeed, we must see if the variability of the first 3 components is sufficient
    to be able to draw conclusions. The MCA method allows us to obtain these values
    in a very simple way:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/bf26aeea4e957ace0aa9a5106cc11687.png)'
  prefs: []
  type: TYPE_IMG
- en: Aha, here we have something interesting. Due to our data we obtain basically
    zero variability.
  prefs: []
  type: TYPE_NORMAL
- en: '**In other words, we cannot draw clear conclusions from our model with the
    information provided by the dimensionality reduction provided by MCA.**'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: By showing these results I try to give an example of what happens in real data
    projects. Good results are not always obtained, but a good data scientist knows
    how to recognize the causes.
  prefs: []
  type: TYPE_NORMAL
- en: 'We have one last option to visually determine if the model created by the K-Prototype
    method is suitable or not. This path is simple:'
  prefs: []
  type: TYPE_NORMAL
- en: This is applying PCA to the dataset to which preprocessing has been performed
    to transform the categorical variables into numerical ones.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Obtain the components of the PCA
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Make a representation using the PCA components such as the axes and the color
    of the points to predict the K-Prototype model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Note that the components provided by the PCA will be the same as for method
    1: Kmeans, since it is the same dataframe.'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s see what we get…
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/76483d5d757f720120829057b3750c34.png)'
  prefs: []
  type: TYPE_IMG
- en: '**PCA space and the clusters created by the model** (Image by Author).'
  prefs: []
  type: TYPE_NORMAL
- en: It doesn’t look bad, in fact it has a certain resemblance to what has been obtained
    in Kmeans.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally we obtain the average value of the clusters and the importance of each
    of the variables:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e89e459dbfc54cb895d47ce3c551cb51.png)'
  prefs: []
  type: TYPE_IMG
- en: '**The importance of the variables in the model. The table represents the most
    frequent value of each of the clusters** (Image by Author).'
  prefs: []
  type: TYPE_NORMAL
- en: The variables with the greatest weight are the numerical ones, notably seeing
    that the confinement of these two features is almost sufficient to differentiate
    each cluster.
  prefs: []
  type: TYPE_NORMAL
- en: In short, it can be said that results similar to those of Kmeans have been obtained.
  prefs: []
  type: TYPE_NORMAL
- en: 'Method 3: LLM + Kmeans'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This combination can be quite powerful and improve the results obtained. Let’s
    get to the point!
  prefs: []
  type: TYPE_NORMAL
- en: '**LLMs** cannot understand written text directly, we need to transform the
    input of this type of models. For this, **Sentence** **Embedding** is carried
    out. It consists of transforming the text into numerical vectors. The following
    image can clarify the idea:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3d290fea0c92e1135c7398c91a83cebb.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Concept of embedding and similarity** (Image by Author).'
  prefs: []
  type: TYPE_NORMAL
- en: 'This coding is done intelligently, that is, phrases that contain a similar
    meaning will have a more similar vector. See the following image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0842dedc820bb2634fb26a9e3c1621af.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Concept of embedding and similarity** (Image by Author).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Sentence embedding is carried out by so-called transforms, algorithms specialized
    in this coding. Typically you can choose what the size of the numerical vector
    coming from this encoding is. And here is one of the key points:'
  prefs: []
  type: TYPE_NORMAL
- en: Thanks to the large dimension of the vector created by embedding, small variations
    in the data can be seen with greater precision.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**Therefore, if we provide input to our information-rich Kmeans model, it will
    return better predictions.** This is the idea we are pursuing and these are its
    steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Transform our original dataset through Sentence embedding
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a Kmeans model
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Evaluate it
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Well, the first step is to encode the information through Sentence embedding.
    What is intended is to take the information of each client and unify it into text
    that contains all its characteristics. This part takes a lot of computing time.
    That’s why I created a script that did this job, call [**embedding_creation.py**](https://github.com/damiangilgonzalez1995/Clustering-with-LLM/blob/main/embedding_creation.py).
    This script collects the values contained in the training dataset and creates
    a new dataset provided by the embedding. This is the script code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'As it is quite important that this step is understood. Let’s go by points:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 1**: The text is created for each row, which contains the complete customer/row
    information. We also store it in a python list for later use. See the following
    image that exemplifies it.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/6efefc20ec2db48dfc909a07ea91ed06.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Graphic description of the first step** (Image by Author).'
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 2**: This is when the call to the transformer is made. For this we are
    going to use the model stored in [**HuggingFace**](https://huggingface.co). This
    model is specifically trained to perform embedding at the sentence level, unlike
    **Bert’s model**, which is focused on encoding at the level of tokens and words.
    To call the model you only have to give the repository address, which in this
    case is ***“sentence-transformers/paraphrase-MiniLM-L6-v2”***. The numerical vector
    that is returned to us for each text will be normalized, since the Kmeans model
    is sensitive to the scales of the inputs. The vectors created have a length of
    **384**. With them what we do is create a dataframe with the same number of columns.
    See the following image:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/b658af3dd5b4fd45b9bc9acec31cb458.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Graphic description of the second step** (Image by Author),'
  prefs: []
  type: TYPE_NORMAL
- en: Finally we obtain the dataframe from the embedding, which will be the input
    of our Kmeans model.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/af3900f4d0c391bb833854baf8a987a0.png)'
  prefs: []
  type: TYPE_IMG
- en: This step has been one of the most interesting and important, since we have
    created the input for the Kmeans model that we will create.
  prefs: []
  type: TYPE_NORMAL
- en: The creation and evaluation procedure is similar to that shown above. In order
    not to make the post excessively long, only the results of each point will be
    shown. Don’t worry, all the code is contained in the [**jupyter notebook called
    *embedding***](https://github.com/damiangilgonzalez1995/Clustering-with-LLM/blob/main/embedding.ipynb)**,**
    so you can reproduce the results for yourself.
  prefs: []
  type: TYPE_NORMAL
- en: In addition, the dataset resulting from applying the Sentence embedding has
    been saved in a csv file. This csv file is called ***embedding_train.csv***. In
    the Jupyter notebook you will see that we access that dataset and create our model
    based on it.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Preprocessed
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We could consider embedding as preprocessing.
  prefs: []
  type: TYPE_NORMAL
- en: Outliers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We apply the method already presented to detect outliers, **ECOD**. We create
    a dataset that does not contain these types of points.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Modeling
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: First we must find out what the optimal number of clusters is. For this we use
    **Elbow Method**.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b8bf00f56eb101982c8bb2709a943f8d.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Elbow score for different numbers of clusters** (Image by Author).'
  prefs: []
  type: TYPE_NORMAL
- en: After viewing the graph, we choose **k=5** as our number of clusters.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Evaluation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The next thing is to create our Kmeans model with k=5\. Next we can obtain
    some metrics like these:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Seeing then that the values are really similar to those obtained in the previous
    case. Let’s study the representations obtained with PCA analysis:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2fb3b3166194753e03ac576a46b9539f.png)'
  prefs: []
  type: TYPE_IMG
- en: '**PCA space and the clusters created by the model** (Image by Author).'
  prefs: []
  type: TYPE_NORMAL
- en: It can be seen that the clusters are much better differentiated than with the
    traditional method. This is good news. Let us remember that it is important to
    take into account the variability contained in the first 3 components of our PCA
    analysis. From experience, I can say that when it is around 50% (3D PCA) more
    or less clear conclusions can be drawn.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/425669b9db5825251def7820d142dd08.png)'
  prefs: []
  type: TYPE_IMG
- en: '**PCA space and the clusters created by the model. The variability of the first
    3 components of the PCA is also shown** (Image by Author).'
  prefs: []
  type: TYPE_NORMAL
- en: We see then that it is 40.44% cumulative variability of the 3 components, it
    is acceptable but not ideal.
  prefs: []
  type: TYPE_NORMAL
- en: 'One way I can visually see how compact the clusters are is by modifying the
    opacity of the points in the 3D representation. This means that when the points
    are agglomerated in a certain space, a black spot can be observed. In order to
    understand what I’m saying, I show the following gif:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/1624503423cef2263b7c21f07799acfa.png)'
  prefs: []
  type: TYPE_IMG
- en: '**PCA space and the clusters created by the model** (Image by Author).'
  prefs: []
  type: TYPE_NORMAL
- en: As can be seen, there are several points in space where the points of the same
    cluster cluster together. This indicates that they are well differentiated from
    the other points and that the model knows how to recognize them quite well.
  prefs: []
  type: TYPE_NORMAL
- en: 'Even so, it can be seen that various clusters cannot be differentiated well
    (Ex: cluster 1 and 3). For this reason, we carry out a **t-SNE** analysis, which
    we remember is a method that allows reducing dimensionality, taking into account
    complex polynomial relationships.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1720bda644e33898311476d74e34ac27.png)'
  prefs: []
  type: TYPE_IMG
- en: '**t-SNE space and the clusters created by the model** (Image by Author).'
  prefs: []
  type: TYPE_NORMAL
- en: 'A noticeable improvement is seen. The clusters do not overlap each other and
    there is a clear differentiation between points. The improvement obtained using
    the second dimensionality reduction method is notable. Let’s see a 2D comparison:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c5058a99e44ac62a383ad3c7b7c7965c.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Different results for different dimensionality reduction methods and clusters
    defined by the model** (Image by Author).'
  prefs: []
  type: TYPE_NORMAL
- en: Again, it can be seen that the clusters in the t-SNE are more separated and
    better differentiated than with the PCA. Furthermore, the difference between the
    two methods in terms of quality is smaller than when using the traditional Kmeans
    method.
  prefs: []
  type: TYPE_NORMAL
- en: 'To understand which variables our Kmeans model relies on, we do the same move
    as before: we create a *classification model (LGBMClassifier) and analyze the
    importance of the features.*'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/606c851fed35b10f19d97c135ebff89d.png)'
  prefs: []
  type: TYPE_IMG
- en: '**The importance of the variables in the model** (Image by Author).'
  prefs: []
  type: TYPE_NORMAL
- en: We see then that this model is based above all on the “***marital***” and “***job***”
    variables. On the other hand we see that there are variables that do not provide
    much information. In a real case, a new version of the model should be created
    without these variables with little information.
  prefs: []
  type: TYPE_NORMAL
- en: '**The Kmeans + Embedding model is more optimal since it needs fewer variables
    to be able to give good predictions**. Good news!'
  prefs: []
  type: TYPE_NORMAL
- en: We finish with the part that is most revealing and important.
  prefs: []
  type: TYPE_NORMAL
- en: Managers and the business are not interested in PCA, t-SNE or embedding. What
    they want is to be able to know what the main traits are, in this case, of their
    clients.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'To do this, we create a table with information about the predominant profiles
    that we can find in each of the clusters:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e65d0d21fb227b2310e3ecfe29c98eb6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Something very curious happens: the clusters where the most frequent position
    is that of “***management***” are 3\. In them we find a very peculiar behavior
    where the single managers are younger, those who are married are older and the
    divorced are the how older they are. On the other hand, the balance behaves differently,
    single people have a higher average balance than divorced people, and married
    people have a higher average balance. What was said can be summarized in the following
    image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f4b304ccef2795bed37f9d5dce880a27.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Different customer profiles defined by the model** (Image by Author).'
  prefs: []
  type: TYPE_NORMAL
- en: This revelation is in line with reality and social aspects. It also reveals
    very specific customer profiles. **This is the magic of data science.**
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The conclusion is clear:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/63460fbab29ae05068d9b91c87bc35b9.png)'
  prefs: []
  type: TYPE_IMG
- en: (Image by Author)
  prefs: []
  type: TYPE_NORMAL
- en: You have to have different tools because in a real project, not all strategies
    work and you must have resources to add value. It is clearly seen that the model
    created with the help of the LLMs stands out.
  prefs: []
  type: TYPE_NORMAL
- en: '*Thank you for reading!*'
  prefs: []
  type: TYPE_NORMAL
- en: '*If you find my work useful, you can subscribe to* [***get an email every time
    that I publish a new article***](https://medium.com/@damiangilgonzalez/subscribe)***.***'
  prefs: []
  type: TYPE_NORMAL
- en: '*If you’d like,* [***follow******me on Linkedin****!*](https://www.linkedin.com/in/damiangilgonzalez/)'
  prefs: []
  type: TYPE_NORMAL
