- en: Which GPT-like Model Engineering Techniques Work on System Logs?
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 哪些 GPT 类似模型工程技术适用于系统日志？
- en: 原文：[https://towardsdatascience.com/which-gpt-like-engineering-strategies-work-on-system-logs-6b0a3a1ebcad?source=collection_archive---------4-----------------------#2023-04-21](https://towardsdatascience.com/which-gpt-like-engineering-strategies-work-on-system-logs-6b0a3a1ebcad?source=collection_archive---------4-----------------------#2023-04-21)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/which-gpt-like-engineering-strategies-work-on-system-logs-6b0a3a1ebcad?source=collection_archive---------4-----------------------#2023-04-21](https://towardsdatascience.com/which-gpt-like-engineering-strategies-work-on-system-logs-6b0a3a1ebcad?source=collection_archive---------4-----------------------#2023-04-21)
- en: Evaluation of Transformer Neural Network Modeling Methodologies applied to Behavior
    Malware Traces
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 对行为恶意软件痕迹应用的变换器神经网络建模方法的评估
- en: '[](https://ditrizna.medium.com/?source=post_page-----6b0a3a1ebcad--------------------------------)[![Dmitrijs
    Trizna](../Images/4a7a925e4383f4b3b087e85c0fba0ca4.png)](https://ditrizna.medium.com/?source=post_page-----6b0a3a1ebcad--------------------------------)[](https://towardsdatascience.com/?source=post_page-----6b0a3a1ebcad--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----6b0a3a1ebcad--------------------------------)
    [Dmitrijs Trizna](https://ditrizna.medium.com/?source=post_page-----6b0a3a1ebcad--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://ditrizna.medium.com/?source=post_page-----6b0a3a1ebcad--------------------------------)[![Dmitrijs
    Trizna](../Images/4a7a925e4383f4b3b087e85c0fba0ca4.png)](https://ditrizna.medium.com/?source=post_page-----6b0a3a1ebcad--------------------------------)[](https://towardsdatascience.com/?source=post_page-----6b0a3a1ebcad--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----6b0a3a1ebcad--------------------------------)
    [Dmitrijs Trizna](https://ditrizna.medium.com/?source=post_page-----6b0a3a1ebcad--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F2fbea2ebee7a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhich-gpt-like-engineering-strategies-work-on-system-logs-6b0a3a1ebcad&user=Dmitrijs+Trizna&userId=2fbea2ebee7a&source=post_page-2fbea2ebee7a----6b0a3a1ebcad---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----6b0a3a1ebcad--------------------------------)
    ·11 min read·Apr 21, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F6b0a3a1ebcad&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhich-gpt-like-engineering-strategies-work-on-system-logs-6b0a3a1ebcad&user=Dmitrijs+Trizna&userId=2fbea2ebee7a&source=-----6b0a3a1ebcad---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F2fbea2ebee7a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhich-gpt-like-engineering-strategies-work-on-system-logs-6b0a3a1ebcad&user=Dmitrijs+Trizna&userId=2fbea2ebee7a&source=post_page-2fbea2ebee7a----6b0a3a1ebcad---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----6b0a3a1ebcad--------------------------------)
    ·11 分钟阅读·2023年4月21日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F6b0a3a1ebcad&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhich-gpt-like-engineering-strategies-work-on-system-logs-6b0a3a1ebcad&user=Dmitrijs+Trizna&userId=2fbea2ebee7a&source=-----6b0a3a1ebcad---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F6b0a3a1ebcad&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhich-gpt-like-engineering-strategies-work-on-system-logs-6b0a3a1ebcad&source=-----6b0a3a1ebcad---------------------bookmark_footer-----------)![](../Images/5967ed06f8450798de7f9b1ede580fd8.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F6b0a3a1ebcad&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhich-gpt-like-engineering-strategies-work-on-system-logs-6b0a3a1ebcad&source=-----6b0a3a1ebcad---------------------bookmark_footer-----------)![](../Images/5967ed06f8450798de7f9b1ede580fd8.png)'
- en: Figure 1\. Self-attention activations from the Transformer model trained on
    malware emulation reports. Image by the author.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 图1. 从训练于恶意软件仿真报告的变换器模型中提取的自注意力激活。图像由作者提供。
- en: This article evaluates various Transformer Neural Network (AI model powering
    GPT) engineering methodologies applied to machine data — malware behavioral logs
    from the [Speakeasy emulator](https://github.com/mandiant/speakeasy). Data used
    in these experiments have been freely available since released as part of this
    publication on [hybrid malware analysis [Trizna]](https://dl.acm.org/doi/10.1145/3560830.3563726),
    with archives downloadable [here](https://github.com/dtrizna/quo.vadis/tree/main/data/emulation.dataset).
    You have access to data and are free to replicate or advance results!
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 本文评估了应用于机器数据（即来自[Speakeasy模拟器](https://github.com/mandiant/speakeasy)的恶意软件行为日志）的各种Transformer神经网络（GPT的AI模型）工程方法。这些实验中使用的数据自发布以来一直可以自由获取，作为[混合恶意软件分析
    [Trizna]](https://dl.acm.org/doi/10.1145/3560830.3563726)的一部分，档案可以[在这里](https://github.com/dtrizna/quo.vadis/tree/main/data/emulation.dataset)下载。您可以访问这些数据，随意复制或推进结果！
- en: Originally, Transformer was presented as an encoder/decoder architecture suitable
    for sequence-to-sequence tasks like natural language translation. Later it was
    adopted to suit other tasks like masked decoder-only models of GPT to be good
    at text generation. Since we will use Transformer for inference, the *model* only
    consists of the *encoder layers* ([model’s PyTorch code](https://github.com/dtrizna/nebula/blob/dev/nebula/models/attention.py#L165)),
    similar to architectures used, for instance, in BERT [[Devlin et al.](https://arxiv.org/abs/1810.04805)].
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 最初，Transformer被提出作为一种适用于序列到序列任务（如自然语言翻译）的编码器/解码器架构。后来它被调整以适应其他任务，如GPT的掩码解码器模型，以便擅长文本生成。由于我们将使用Transformer进行推理，*模型*仅包含*编码器层*（[模型的PyTorch代码](https://github.com/dtrizna/nebula/blob/dev/nebula/models/attention.py#L165)），类似于例如BERT
    [[Devlin et al.](https://arxiv.org/abs/1810.04805)]中使用的架构。
- en: Speculatively, the same conclusions about the Transformer engineering methods
    drawn in this article can be expanded to any set of system logs, for instance,
    operating system telemetry like Sysmon on Windows or corresponding Linux frameworks
    like *auditd,* application level logs as *kube-audit* events from Kubernetes API
    server, or access log from HTTP servers, and counting.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 推测地，本文得出的关于Transformer工程方法的相同结论可以扩展到任何系统日志集，例如Windows上的操作系统遥测如Sysmon，或对应的Linux框架如*auditd*，应用级日志如来自Kubernetes
    API服务器的*kube-audit*事件，或HTTP服务器的访问日志及计数。
- en: '![](../Images/ae080fd4ed8153a5de56ab3a30845c9a.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ae080fd4ed8153a5de56ab3a30845c9a.png)'
- en: Figure 2\. A schematic and simplified view of pre-processing filtered JSON events
    to a sequence of tokens with filters and normalization. Image by the author.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 图2\. 预处理过滤后的JSON事件到令牌序列的示意图和简化视图，包括过滤器和归一化。图片由作者提供。
- en: 'While part of an ongoing study, the article *avoids broader discussion* on
    this data and architecture combination and instead focuses on the methodology
    and PyTorch code of Transformer modeling techniques to evaluate the **relative**
    *utility* of *engineering strategies* rather than absolute performance. List of
    evaluated configurations:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然是正在进行的研究的一部分，本文*避免了更广泛的讨论*关于数据和架构组合，而是专注于Transformer建模技术的方法论和PyTorch代码，以评估**相对**的*实用性*和*工程策略*，而非绝对性能。评估的配置列表：
- en: triangular, cosine, step, and one-cycle learning rate schedulers;
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 三角形、余弦、阶梯和一周期学习率调度器；
- en: accumulated gradients;
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 累积梯度；
- en: gradient clipping;
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 梯度裁剪；
- en: attention block’s pre-norm vs. post-norm;
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 注意力块的预归一化与后归一化；
- en: data optimizations;
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据优化；
- en: learning rate dependence on model size.
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习率对模型大小的依赖。
- en: For all the configuration options, I do a three-fold cross-validation run and
    report mean ROC curves for validation and test sets, as well as training losses.
    This article *does not cover* the models’ *self-supervised pre-training* but *focuses
    on* malware classification’s *downstream task* (i.e., supervised learning).
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 对于所有配置选项，我进行三次交叉验证运行，并报告验证集和测试集的平均ROC曲线以及训练损失。本文*不涵盖*模型的*自监督预训练*，而是*专注于*恶意软件分类的*下游任务*（即监督学习）。
- en: Several publications have already systematically analyzed Transformer’s engineering
    advancements in *natural languages*, which we will explore on *machine data*.
    Given the desire to further investigate the ideas presented here, I suggest studying,
    for instance, [”cramming” paper [Geiping and Goldstein]](https://arxiv.org/abs/2212.14034).
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 许多出版物已经系统地分析了Transformer在*自然语言*方面的工程进展，我们将在*机器数据*上探讨这些进展。考虑到进一步研究这里提出的观点，我建议学习，例如，[“临时性”论文
    [Geiping and Goldstein]](https://arxiv.org/abs/2212.14034)。
- en: Optimizing the Dataset
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 优化数据集
- en: The data consists of JSON reports representing emulation results from ~120k
    malware and benignware samples, with malicious examples spanning seven malware
    types like ransomware, trojans, backdoors, etc. However, I limit experiments to
    binary classification, with the *Clean* label representing the benign class and
    all others representing malicious samples. Worth noting that the test set was
    collected three months after the training set to introduce an evaluation in the
    presence of concept drift.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 数据由 ~120k 恶意软件和良性软件样本的 JSON 报告组成，恶意样本涵盖了如勒索软件、木马、后门等七种恶意软件类型。然而，我将实验限制为二分类，*Clean*
    标签表示良性类，所有其他标签表示恶意样本。值得注意的是，测试集是在训练集收集后三个月收集的，以引入概念漂移下的评估。
- en: Since emulation is an imperfect way to obtain the behavior of evasive malware
    samples, further [filters and normalization](https://github.com/dtrizna/nebula/blob/dev/nebula/preprocessing/pe.py#L90)
    are applied to the reports mentioned above to drop failed or incomplete emulations,
    resulting in **76126** samples within the training set and **17407** samples in
    the test set.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 由于模拟是一种不完美的方式来获取回避型恶意软件样本的行为，进一步 [过滤和规范化](https://github.com/dtrizna/nebula/blob/dev/nebula/preprocessing/pe.py#L90)
    被应用于上述报告，以剔除失败或不完整的模拟，从而在训练集中得到 **76126** 个样本，在测试集中得到 **17407** 个样本。
- en: Field filters for sequence length reduction
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 序列长度减少的字段过滤器
- en: Based on our observations, any semantical logic stored in machine data spans
    a *very* long sequence of tokens, including countless occurrences of metadata
    and environmental specifics that represent little relevancy for modeling tasks.
    Arguably, this is one of the most drastic differences between natural and machine
    languages. The former has concise semantics in relatively short-length sentences
    with no or little futile components.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 根据我们的观察，存储在机器数据中的任何语义逻辑都跨越了*非常*长的标记序列，包括无数次的元数据和环境特定信息，这些信息对于建模任务几乎没有相关性。可以说，这是自然语言和机器语言之间最显著的差异之一。前者在相对较短的句子中具有简洁的语义，没有或很少有无用的组件。
- en: This is unfortunate for self-attention models since it has **quadratic** complexity
    on input length, meaning longer sequences is exponentially more expensive to train.
    This is because every element in the input to self-attention must attend to every
    other member in the same sequence — see the title Figure 1 above for visualization.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 对自注意力模型来说，这很不幸，因为其在输入长度上的复杂度是**平方**的，这意味着较长的序列训练成本指数增长。这是因为自注意力中的每个元素都必须关注同一序列中的每个其他成员——请参见上面的图
    1 标题以获取可视化。
- en: Therefore, applying a domain knowledge-based filter when working with machine
    data is *mandatory*. In our case, JSON events [are filtered](https://github.com/dtrizna/nebula/blob/dev/nebula/constants.py#L40)
    based on manipulations on (1) files, (2) registry, (3) network access, as well
    as (4) API call names, arguments, and return values. This significantly increased
    epistemic density (i.e., relevant knowledge per token ratio) and data quality.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在处理机器数据时应用基于领域知识的过滤器是*必不可少*的。在我们的案例中，JSON 事件 [会被过滤](https://github.com/dtrizna/nebula/blob/dev/nebula/constants.py#L40)，过滤依据包括（1）文件，（2）注册表，（3）网络访问，以及（4）API
    调用名称、参数和返回值。这显著提高了知识密度（即每个标记的相关知识比率）和数据质量。
- en: 'Sequence length is limited to **512** tokens during the experiments below to
    iterate over configuration combinations faster, but keeping longer sequences for
    final performance evaluations or production implementations is advisable. There
    is evidence that task-related performances benefit from longer sequences, as depicted
    in the heatmap below:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 序列长度在下面的实验中限制为**512**个标记，以便更快地迭代配置组合，但为了最终性能评估或生产实现，建议保留较长的序列。有证据表明，任务相关的性能从更长的序列中受益，如下图的热图所示：
- en: '![](../Images/cc4aeebd30b5faa015b1cd7ab5758f4b.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cc4aeebd30b5faa015b1cd7ab5758f4b.png)'
- en: Figure 3\. Mean True Positive Rates under False Positive Rate of 0.001 (one
    false alert per 1000 detections), depending on variable sequence lengths and vocabulary
    sizes. Image by the author.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3\. 在假阳性率为 0.001（每 1000 次检测一个假警报）下的平均真正率，取决于变量序列长度和词汇大小。图片由作者提供。
- en: Tokenization
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 标记化
- en: Machine data can have a significantly larger vocabulary than natural language,
    as no distinct lexical boundaries or grammatical rules often define the language
    being used.
  id: totrans-34
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 机器数据的词汇量可能比自然语言大得多，因为通常没有明确的词汇边界或语法规则来定义所使用的语言。
- en: In system logs, it is common to see arbitrary character combinations like */tmp/83afba/setup.bin*
    or *jre1.8.0_311,* which explode vocabulary given improper handling. For instance,
    I observe ~3 000 unique API names, with ~1 000 appearing only once in the training
    data.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在系统日志中，常见的任意字符组合如*/tmp/83afba/setup.bin*或*jre1.8.0_311*，由于处理不当，会导致词汇量爆炸。例如，我观察到大约有3000个独特的API名称，其中约1000个在训练数据中仅出现一次。
- en: 'Therefore, every extra field from the original JSON you include for analysis
    by the model significantly increases the vocabulary size. Consider the logarithmic
    plot below, which visualizes the frequency distribution of tokens in the training
    sets with different JSON fields filter setups:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，您为模型分析包含的每一个额外字段都会显著增加词汇量。请参见下面的对数图，显示了不同JSON字段过滤器设置下的训练集标记的频率分布：
- en: '![](../Images/066b58af3d4822f4a8aa4bfd16a694e5.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/066b58af3d4822f4a8aa4bfd16a694e5.png)'
- en: Figure 4\. Token frequency distribution concerning different JSON field filters
    applied to the training set. Image by the author.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 图4\. 针对训练集应用不同JSON字段过滤器的标记频率分布。图片由作者提供。
- en: For instance, given a field filter that uses API calls, file, network, and registry
    records total vocabulary size is about 250k tokens. Given no filters were applied,
    this number jumps close to 800k tokens, exploding vocabulary more than three times
    and significantly reducing the epistemic density (valuable information per token)
    of the data. This emphasizes the importance of appropriate domain knowledge-influenced
    filters to raise the data quality the model receives.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，给定一个使用API调用、文件、网络和注册表记录的字段过滤器，总词汇量大约为25万标记。假如没有应用过滤器，这个数字会接近80万标记，词汇量爆炸超过三倍，并显著降低数据的认知密度（每个标记的有价值信息）。这强调了合适的领域知识影响的过滤器对提高模型接收的数据质量的重要性。
- en: 'The same applies to normalization on arbitrary value fields like hashes and
    IP addresses. For instance, leaving hashes untreated drastically expands vocabulary
    while normalizing it to values like *<sha256>* and *<md5>* with the function below
    yields only a few easy-to-interpret placeholders that the model can use:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 对于任意值字段如哈希值和IP地址的归一化也是一样的。例如，将哈希值不处理会极大地扩展词汇量，而使用下面的函数将其归一化为像*<sha256>*和*<md5>*这样的值，则会得到一些易于解释的占位符，模型可以利用这些占位符：
- en: This is just a single and simple example, while all normalization implemented
    in my pipeline can be found [here](https://github.com/dtrizna/nebula/blob/main/nebula/preprocessing/normalization.py).
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 这只是一个简单的例子，而我管道中实现的所有归一化可以在[这里](https://github.com/dtrizna/nebula/blob/main/nebula/preprocessing/normalization.py)找到。
- en: However, another essential trick is improving tokenization itself so that technical
    variability in machine data does not indefinitely create new tokens. Natural languages
    already offer a solution that fits machine data perfectly.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，另一个关键技巧是改进标记化本身，以便技术数据中的变异不会无限期地创建新标记。自然语言已经提供了适合机器数据的解决方案。
- en: 'I define a custom JSON Byte Pair Encoding (BPE) tokenizer ([code](https://github.com/dtrizna/nebula/blob/dev/nebula/preprocessing/json.py#L253))
    based on Google’s [SentencePiece](https://github.com/google/sentencepiece/), which
    analyzes the relative co-occurrence of bytes. That way, tokens do not represent
    distinct values in technical language but parts of longer value sequences, as
    exemplified below:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 我定义了一个基于Google的[SentencePiece](https://github.com/google/sentencepiece/)的自定义JSON
    Byte Pair Encoding (BPE)标记器（[代码](https://github.com/dtrizna/nebula/blob/dev/nebula/preprocessing/json.py#L253)），它分析字节的相对共现方式。这样，标记并不代表技术语言中的独立值，而是较长值序列的一部分，如下面的例子所示：
- en: '![](../Images/3375d3246d908127be865a862d03aed1.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3375d3246d908127be865a862d03aed1.png)'
- en: Figure 5\. Example of whitespace tokens. Image by the author.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 图5\. 空白标记示例。图片由作者提供。
- en: '![](../Images/056ee6ee14f9ac78052f4b45adeffca0.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/056ee6ee14f9ac78052f4b45adeffca0.png)'
- en: Figure 6\. Example of sentencepiece’s BPE tokens. Image by the author.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 图6\. SentencePiece的BPE标记示例。图片由作者提供。
- en: I limit further experiments to the vocabulary of **50k** BPE tokens.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 我将进一步的实验限制在**50k** BPE标记的词汇量内。
- en: Model size and learning rate
  id: totrans-49
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型大小和学习率
- en: The learning rate (LR) choice is straightforward and known to any deep learning
    practitioner. However, I want to emphasize that for Transformer architectures,
    selecting an appropriate LR range for a given model size is especially crucial.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 学习率（LR）的选择很直接，对任何深度学习从业者来说都是已知的。然而，我想强调的是，对于Transformer架构，选择适合的学习率范围对于特定模型大小尤其重要。
- en: 'Consider this table from the [GPT-3 paper [Brown et al.]](https://arxiv.org/pdf/2005.14165.pdf)
    that reports the configuration of different models and variability in the learning
    rate used:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 请参阅 [GPT-3 论文 [Brown et al.]](https://arxiv.org/pdf/2005.14165.pdf)中的此表格，该表格报告了不同模型的配置以及使用的学习率的变化：
- en: '![](../Images/d2ae834f8402acab18417442e34f9d17.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d2ae834f8402acab18417442e34f9d17.png)'
- en: Table 1 from [GPT-3 paper [Brown et al.]](https://arxiv.org/pdf/2005.14165.pdf).
    Note how the learning rate and batch size vary depending on the number of trainable
    model parameters.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 表格 1 来自 [GPT-3 论文 [Brown et al.]](https://arxiv.org/pdf/2005.14165.pdf)。注意学习率和批量大小如何根据可训练模型参数的数量而变化。
- en: Note the significance of learning rate reduction once the model grows, with
    ~10x reduction in LR with ~1000x increase in model size.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 注意到模型增长后的学习率减少的意义，模型大小增加 ~1000 倍时，学习率降低 ~10 倍。
- en: Because of the hardware setup (single consumer type GPU), I use a model of modest
    size with ~5–6 M parameters. Since this article evaluates *relative configuration
    utility* rather than achieving state-of-the-art absolute values, it is a good
    size that allows iterating over many configurations fast, providing a good benchmark
    on the larger model’s behavior. Industry shows that model size (as the number
    of parameters in non-embedding layers) strongly predicts performance [[Kaplan
    et al.](https://arxiv.org/abs/2001.08361)] — this property is referred to as “scaling
    law.” Therefore, model’s size and dataset increase should yield necessary detection
    rates upon production release.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 由于硬件设置（单个消费级 GPU），我使用了一个具有 ~5–6 M 参数的适中大小的模型。由于本文评估的是*相对配置效用*而不是达到最先进的绝对值，因此这是一个适合快速迭代多种配置的良好大小，为更大模型的行为提供了良好的基准。行业数据显示，模型大小（非嵌入层中的参数数量）强烈预测性能
    [[Kaplan et al.](https://arxiv.org/abs/2001.08361)]——这一特性被称为“规模定律”。因此，模型的大小和数据集的增加应在生产发布时产生必要的检测率。
- en: 'I decided to loop over a set of learning rates from 0.003 to 0.0001, decreasing
    each next ~3 times. The results are as follows:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 我决定循环测试从 0.003 到 0.0001 的一系列学习率，每次降低约 ~3 倍。结果如下：
- en: '![](../Images/bcd94fc8936edb3d12662b01173374f7.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bcd94fc8936edb3d12662b01173374f7.png)'
- en: Figure 7\. Mean values of validation and test set ROC curves and training losses
    for variable learning rates over three cv folds. Image by the author.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7\. 在三个交叉验证折叠中，变量学习率下的验证集和测试集 ROC 曲线的平均值以及训练损失。图像由作者提供。
- en: It is clearly seen that 0.003 is too big, whereas 0.0001 is too small, with
    the optimal value being somewhere between 0.0003–0.001\. I selected LR close to
    the bottom line ~0.0003 for further tests.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 可以明显看出，0.003 太大，而 0.0001 太小，最佳值在 0.0003–0.001 之间。我选择了接近底线 ~0.0003 的学习率进行进一步测试。
- en: Learning Rate Scheduler
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 学习率调度器
- en: Once an appropriate value of maximal learning rate for a given model is selected,
    I explored various schedulers of learning rate value. Scheduler refers to a function
    that modifies learning rate value during training.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦为给定模型选择了适当的最大学习率，我探索了各种学习率值的调度器。调度器是指在训练过程中修改学习率值的函数。
- en: 'I [define multiple schedulers](https://github.com/dtrizna/nebula/blob/main/nebula/optimization.py)
    based on ones reported in the most promising publications, specifically: (1) step,
    (2) triangular, (3) one-cycle, and (4) cosine with LR transformation over training
    time as depicted below:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 我 [定义了多种调度器](https://github.com/dtrizna/nebula/blob/main/nebula/optimization.py)，这些调度器基于最有前景的出版物中报告的调度器，具体为：（1）阶梯型，（2）三角形，（3）单周期，以及（4）余弦型，学习率随训练时间的变化如下面所示：
- en: '![](../Images/29cb9d0cebbdb0a5139fffd8376f0647.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/29cb9d0cebbdb0a5139fffd8376f0647.png)'
- en: Figure 8\. Learning rate changes over training steps with different schedulers.
    Image by the author.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8\. 不同调度器下的训练步骤中的学习率变化。图像由作者提供。
- en: 'The results of experiments (including a run without any scheduler at all) are
    reported below:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 实验结果（包括一个完全不使用任何调度器的运行）如下所示：
- en: '![](../Images/10cffad1da2da2f5bc973998016d0d18.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/10cffad1da2da2f5bc973998016d0d18.png)'
- en: Figure 9\. Results of various scheduler applications. Image by the author.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9\. 各种调度器应用的结果。图像由作者提供。
- en: We clearly see that schedulers that have a “warmup” stage — i.e., do not stop
    from max LR value (“triangular” and “one-cycle”), have higher training loss during
    the first ~2000 updates. From ROC on the test set, we might conclude that one-cycle
    performs the worst, and no scheduler might have higher True Positive Rate (TPR,
    aka detection rate) values under low False Positive Rate (FPR) demands.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: 'To resolve ambiguity, here are exact AUC and TPR values under specific FPR
    with the lowest of 0.0001 (meaning one false alert in 10 000 analyses) on the
    test set:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f3e2c8de4873d17b72ab35c982316619.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
- en: Table 1\. Mean True-Positive Rates (values) under specific False-Positive Rates
    (columns) and AUC on the test set over 3 cv folds. Table by the author.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: And this reveals that training with the“step” schedule has the best results
    under especially low False Positive requirements, while overall best AUC has run
    with the “triangular” scheduler. Indeed, [there is evidence](https://twitter.com/giffmana/status/1608568411855937536?s=20)
    that “warmup” might be important, especially for self-supervised pre-training.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: Interesting to note that training with no scheduler provides poor results under
    the lowest FPR. This might indicate that “cooldown” (i.e., gradual reduction of
    LR value closer to the end) is especially important to find the best model’s checkpoint
    in local minima. Using a scheduler with “cooldown” might be crucial for information
    security applications, where False Positives drain a human analyst’s alert budget
    and are costly.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
- en: If you are further interested in thoroughly examining the topic of LR schedulers,
    the following [TDS article](/a-visual-guide-to-learning-rate-schedulers-in-pytorch-24bbb262c863)
    might interest you.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: Accumulating gradients
  id: totrans-75
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A curious reader noted batch size difference in GPT-3 Table 1 above when we
    discussed learning rate variability on model size. Different model sizes require
    a distinctive number of samples to produce an optimal gradient update. For the
    smallest models, GPT authors used batch sizes of 500k samples, while for the largest,
    including GPT-3 itself, they processed 3.2M samples for each weight update.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: 'Given limited resource training, batch sizes that fit on a single GPU are under-optimal.
    Gradient vector computed from 64 or 96 samples might have skewed direction, slowing
    convergence. Therefore, resources that discuss training under limited constraints
    refer to the technique of accumulating gradients before updating weights (i.e.,
    calling `optimizer.step()` in PyTorch). Training loop with gradient accumulation
    looks like follows:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: 'The tricky point is that training speed is different under the various configurations
    of `accumulation_steps`. For example, updating gradients every step is slower
    than doing that every 16 steps. Therefore, I set up the same time budget of ten
    minutes per training run — to evaluate the efficiency of every configuration with
    the same amount of computing resources. This allowed runs with lower update rates
    to iterate over more batches within the same time (thus, different lengths of
    training losses below). The results are as follows:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 关键点在于，`accumulation_steps`的不同配置下训练速度有所不同。例如，每步更新梯度的速度比每16步更新一次要慢。因此，我设置了每次训练运行的相同时间预算为十分钟——以评估每种配置在相同计算资源下的效率。这使得更新率较低的运行能够在相同的时间内处理更多的批次（因此，训练损失的长度不同）。结果如下：
- en: '![](../Images/bbd73532369c9bf6e1e1460ad35353d4.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bbd73532369c9bf6e1e1460ad35353d4.png)'
- en: Figure 10\. Mean values of validation and test set ROC curves and training losses
    for variable accumulated gradient batch values over three cv folds. Image by the
    author.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 图10\. 不同累计梯度批次值在三次交叉验证折叠中的验证集和测试集ROC曲线均值及训练损失。图片由作者提供。
- en: For some reason, we see that this technique drastically decreases the performance
    of the final model. The same pattern appears whether gradient accumulation is
    implemented in native PyTorch as exemplified above or [using accelerate from HuggingFace](https://huggingface.co/docs/accelerate/usage_guides/gradient_accumulation).
    Therefore, it shouldn’t be a bug in the implementation. Even though models with
    accumulation loop over significantly more batches within the same training budget,
    a classical implementation with ~5x fewer data achieve higher detection rates.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 出于某种原因，我们发现这种技术显著降低了最终模型的性能。无论是使用本地PyTorch实现梯度累积，如上所示，还是[使用HuggingFace的accelerate](https://huggingface.co/docs/accelerate/usage_guides/gradient_accumulation)，都有相同的模式。因此，这不应该是实现中的bug。即使具有累积循环的模型在相同训练预算内处理的批次数明显更多，经典实现的数据量减少约5倍仍然能达到更高的检测率。
- en: Gradient Clipping
  id: totrans-82
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 梯度裁剪
- en: 'Gradient clipping is a method of setting an upper limit to the gradient value
    during a parameter update. Exploding gradients were a significant drawback of
    Recurrent Neural Networks (RNNs), and gradient clipping was introduced as a cure
    for exploding gradients in RNNs. However, practically all recent Transformer-focused
    papers also implement this technique. It stabilizes the training process with
    no significant drawbacks. PyTorch provides a dedicated [clip_grad_norm_](https://pytorch.org/docs/stable/generated/torch.nn.utils.clip_grad_norm_.html)
    function to implement this logic with a single line of code within the training
    loop (just before `optimizer.step()` a call, for [example](https://github.com/dtrizna/nebula/blob/dev/nebula/__init__.py#L189)).
    The results with variable values in a range from 0 to 1 are as follows:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度裁剪是一种在参数更新过程中设置梯度值上限的方法。梯度爆炸是递归神经网络（RNN）的一个显著缺陷，而梯度裁剪被引入作为解决RNN梯度爆炸的问题的方法。然而，实际上，所有近期关注Transformer的论文也都实现了这一技术。它通过没有明显缺陷的方式稳定训练过程。PyTorch提供了一个专门的[clip_grad_norm_](https://pytorch.org/docs/stable/generated/torch.nn.utils.clip_grad_norm_.html)函数，用一行代码在训练循环中实现这一逻辑（例如，在`optimizer.step()`调用之前，[参见](https://github.com/dtrizna/nebula/blob/dev/nebula/__init__.py#L189)）。变量值在0到1范围内的结果如下：
- en: '![](../Images/b0228dc09a485ad2739c90e30c9aef91.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b0228dc09a485ad2739c90e30c9aef91.png)'
- en: Figure 11\. Mean values of validation and test set ROC curves and training losses
    for variable gradient clipping limits over three cv folds. Image by the author.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 图11\. 不同梯度裁剪限制在三次交叉验证折叠中的验证集和测试集ROC曲线均值及训练损失。图片由作者提供。
- en: 'We clearly see that clipping values at 0.1 are sub-optimal, while there are
    no clear conclusions on the other three options by curves. Therefore, as before,
    looking at TPR at specific FPR and AUC values in the table below is highly informative:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 我们清楚地看到，0.1的裁剪值是次优的，而其他三个选项的曲线没有得出明确结论。因此，如前所述，查看下表中特定FPR和AUC值的TPR是非常有帮助的：
- en: '![](../Images/23ab31c3854c75b5edf29fd4b26bdcc9.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/23ab31c3854c75b5edf29fd4b26bdcc9.png)'
- en: Table 2\. Mean True-Positive Rates (values) under specific False-Positive Rates
    (columns) and AUC on the test set over 3 cv folds. Table by the author.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 表2\. 在3次交叉验证折叠中的特定假阳性率（列）下的平均真正率（值）和测试集上的AUC。表格由作者提供。
- en: Here it is obvious that gradient clipping of 1.0 provides the best metrics,
    with the highest overall AUC and at least a 3% benefit over other options under
    extremely low FP conditions.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，很明显 `1.0` 的梯度裁剪提供了最佳的指标，在极低的 FP 条件下具有最高的整体 AUC，并且至少比其他选项多出 3% 的好处。
- en: Layer Normalization — Input or Output
  id: totrans-90
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 层归一化 — 输入或输出
- en: Original Transformer implementation applies [Layer Normalization](https://arxiv.org/abs/1607.06450)
    on the output from a self-attention block. Contemporary literature, however, agrees
    that input normalization outperforms the vanilla setup (for more details, see
    *PreNorm vs. PostNorm* discussions, e.g. [[1](https://arxiv.org/abs/2002.04745)],
    [[2](https://sh-tsang.medium.com/review-pre-ln-transformer-on-layer-normalization-in-the-transformer-architecture-b6c91a89e9ab)]).
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 原始 Transformer 实现将 [层归一化](https://arxiv.org/abs/1607.06450) 应用于自注意力块的输出。然而，现代文献一致认为输入归一化优于经典设置（有关更多细节，请参见
    *PreNorm vs. PostNorm* 讨论，如 [[1](https://arxiv.org/abs/2002.04745)]、[[2](https://sh-tsang.medium.com/review-pre-ln-transformer-on-layer-normalization-in-the-transformer-architecture-b6c91a89e9ab)]）。
- en: 'Notable that PyTorch’s default behavior is still classical *PostNorm*, but
    it is easy to modify with a single [norm_first](https://github.com/pytorch/pytorch/blob/master/torch/nn/modules/transformer.py#L38)
    boolean of a Transformer Encoder block. Our results confirm public opinions, with
    input normalization outperforming classical realization:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，PyTorch 的默认行为仍然是经典的 *PostNorm*，但通过修改 Transformer Encoder 块中的一个 [norm_first](https://github.com/pytorch/pytorch/blob/master/torch/nn/modules/transformer.py#L38)
    布尔值可以轻松更改。我们的结果证实了公众的观点，输入归一化优于经典实现。
- en: '![](../Images/55e1a146fa7878844c5cd1c44cc9b831.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/55e1a146fa7878844c5cd1c44cc9b831.png)'
- en: Figure 12\. Mean values of validation and test set ROC curves and training losses
    for pre-norm and post-norm configurations. Image by the author.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12\. 预归一化和后归一化配置的验证集和测试集 ROC 曲线以及训练损失的均值。图片由作者提供。
- en: An ongoing debate is whether [RMSNorm](https://arxiv.org/abs/1910.07467) outperforms
    conventional LayerNorm, where implementations like [LLaMa](https://ai.facebook.com/blog/large-language-model-llama-meta-ai/)
    use RMSNorm, while [other experiments](https://arxiv.org/abs/2212.14034) reveal
    no benefits. We excluded this from our analysis.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 一个持续的争论是 [RMSNorm](https://arxiv.org/abs/1910.07467) 是否优于传统的 LayerNorm，其中 [LLaMa](https://ai.facebook.com/blog/large-language-model-llama-meta-ai/)
    等实现使用 RMSNorm，而 [其他实验](https://arxiv.org/abs/2212.14034) 则显示没有明显的好处。我们将其排除在我们的分析之外。
- en: Summary
  id: totrans-96
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: Through experimentation, we found that certain configurations, such as (1) input
    normalization, (2) “triangular” or “step” learning rate schedulers, and (3) gradient
    clipping around 1.0, effectively improved model performance. On the other hand,
    gradient accumulation does not improve performance, which is subject to further
    exploration.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 通过实验，我们发现某些配置，如 (1) 输入归一化、(2) “三角形”或“阶梯”学习率调度器，以及 (3) 大约 `1.0` 的梯度裁剪，有效地提高了模型性能。另一方面，梯度累积没有改善性能，这需要进一步探索。
- en: Additionally, we discussed important caveats of engineering a Transformer model
    for machine data, such as (1) domain knowledge influenced filter, normalization,
    and tokenization to reduce vocabulary and (2) learning rate choice concerning
    the model size.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们讨论了为机器数据工程化 Transformer 模型时的重要注意事项，如 (1) 领域知识影响的过滤、归一化和分词以减少词汇表和 (2) 关于模型大小的学习率选择。
- en: As mentioned above, speculatively, the same conclusions about Transformer engineering
    methods could be extended to other types of system logs, such as operating system
    telemetry or access logs from HTTP servers, and I hope that this article contributes
    to the challenging task of adopting modern AI techniques to ease the day-to-day
    tasks of industry professionals.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 如上所述，推测性地，可以将关于 Transformer 工程方法的相同结论扩展到其他类型的系统日志，如操作系统遥测或 HTTP 服务器的访问日志，希望本文能为将现代
    AI 技术应用于减轻行业专业人士日常任务的挑战性工作做出贡献。
