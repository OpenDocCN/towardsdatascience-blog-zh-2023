- en: 'Making the Right Decisions: AI Advice, Decision Aids, and the Promise of LLMs'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/making-the-right-decisions-ai-advice-decision-aids-and-the-promise-of-llms-65b70682ee08?source=collection_archive---------9-----------------------#2023-09-28](https://towardsdatascience.com/making-the-right-decisions-ai-advice-decision-aids-and-the-promise-of-llms-65b70682ee08?source=collection_archive---------9-----------------------#2023-09-28)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Exploring the new dawn of decision-making with LLMs
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@ujwal07?source=post_page-----65b70682ee08--------------------------------)[![Ujwal
    Gadiraju](../Images/ee7345cf2e7fbf6ee29866659b60b18e.png)](https://medium.com/@ujwal07?source=post_page-----65b70682ee08--------------------------------)[](https://towardsdatascience.com/?source=post_page-----65b70682ee08--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----65b70682ee08--------------------------------)
    [Ujwal Gadiraju](https://medium.com/@ujwal07?source=post_page-----65b70682ee08--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fc529f92bfe0d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmaking-the-right-decisions-ai-advice-decision-aids-and-the-promise-of-llms-65b70682ee08&user=Ujwal+Gadiraju&userId=c529f92bfe0d&source=post_page-c529f92bfe0d----65b70682ee08---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----65b70682ee08--------------------------------)
    ·10 min read·Sep 28, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F65b70682ee08&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmaking-the-right-decisions-ai-advice-decision-aids-and-the-promise-of-llms-65b70682ee08&user=Ujwal+Gadiraju&userId=c529f92bfe0d&source=-----65b70682ee08---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F65b70682ee08&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmaking-the-right-decisions-ai-advice-decision-aids-and-the-promise-of-llms-65b70682ee08&source=-----65b70682ee08---------------------bookmark_footer-----------)![](../Images/087d06c143f7cd0a5b5f62c7bc6176c7.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: Photo by [Robert Ruggiero](https://unsplash.com/@robert2301?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    on [Unsplash](https://unsplash.com/photos/X2bcQMMhaow?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The democratization of AI has led to the adoption of AI systems in a variety
    of domains. The recent wave of generative models, such as pre-trained large language
    models (LLMs), has led to their widespread use in different activities in our
    daily lives — from increasing productivity by aiding the framing of emails to
    helping solve the dreaded “blank page” obstacle for novice and expert writers
    alike. Due to the increasing reliance on LLMs to aid decision-making, this article
    presents a synthesis of human decision-making and the evolution of human-AI decision-making.
    Finally, the article reflects on the opportunities that LLMs offer for aiding
    decision-making tasks and the concomitant threats of reliance on LLMs for decision-making.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 人工智能的民主化导致了AI系统在各种领域的采用。最近一波生成模型，例如预训练的大型语言模型（LLMs），已广泛应用于我们日常生活中的不同活动——从通过帮助撰写邮件提高生产力，到帮助解决令新手和专家作家都感到困扰的“空白页”难题。由于对LLMs在决策中的依赖日益增加，本文提供了人类决策和人类与AI决策演变的综合分析。最后，文章反思了LLMs在辅助决策任务中提供的机会以及对LLMs决策依赖的相关威胁。
- en: Human Decision-Making
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 人类决策
- en: In a world that is characterized by a growing spectrum of choices with respect
    to nearly every single decision we encounter in our daily lives (*e.g.*, food
    to buy or clothes to wear, books to read, music to listen to, or movies to watch,
    from lifestyle choices to travel destinations), the quality of decision-making
    has received renewed interest. In his influential work exposing the “*paradox
    of choice,*” Barry Schwartz articulated this growing difficulty in decision-making
    on the heels of technological advances at the stroke of the millennium [12]. Schwarz
    elucidates this with an example of a ​​doctor offering a patient an array of treatments,
    conveying the potential risks and weighing them against the benefits for each.
    In such a situation, the burden of a high-stakes decision is shifted from the
    expert doctor to the non-expert patient. Among other factors, an abundance of
    choices often tends to impede effective human decision-making.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在一个几乎每个日常决策（*例如*，购买食品或穿着衣物、阅读书籍、听音乐或观看电影，从生活方式选择到旅行目的地）的选择范围不断扩大的世界中，决策质量受到了重新关注。在他有影响力的作品中揭示“*选择悖论*”的巴里·施瓦茨（Barry
    Schwartz）阐述了这种随着千年技术进步而逐渐增加的决策困难。施瓦茨通过一个例子来解释这一点：医生向患者提供一系列治疗方案，传达潜在风险并将其与每种治疗的收益进行权衡。在这种情况下，高风险决策的负担从专家医生转移到非专家患者。除了其他因素外，选择的过多往往会阻碍有效的人类决策。
- en: Different research communities, ranging from evolutionary psychology to cognitive
    sciences and neuroscience, have explored the nature of human decision-making and
    various factors that shape decision-making processes among humans [2,10]. It is
    no secret that human decision-making is plagued by cognitive biases and punctuated
    with irrationality. This was most famously documented by Nobel prize-winning behavioral
    economist Daniel Kahneman in *Thinking, Fast, and Slow* [6]*.*
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 不同的研究社区，从进化心理学到认知科学和神经科学，已经探索了人类决策的性质以及塑造人类决策过程的各种因素。人类决策受认知偏差困扰，充满了非理性，这一点并不鲜见。这一点在诺贝尔奖获奖行为经济学家丹尼尔·卡尼曼的*《思考，快与慢》*一书中得到了最著名的记录。
- en: '![](../Images/b1c22ad1ee60a9ba1c68c1dc31d44dc6.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b1c22ad1ee60a9ba1c68c1dc31d44dc6.png)'
- en: Photo by [Robynne Hu](https://unsplash.com/@robynnexy?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    on [Unsplash](https://unsplash.com/photos/HOrhCnQsxnQ?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 由 [Robynne Hu](https://unsplash.com/@robynnexy?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    提供的照片，来源于 [Unsplash](https://unsplash.com/photos/HOrhCnQsxnQ?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
- en: Human-AI Decision Making
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 人类与人工智能的决策
- en: The advent of technologies has ushered in growth in decision-support systems
    that can support humans in overcoming obstacles in their decision-making processes.
    Decision-support systems take various shapes and forms in broader socio-technical
    contexts– from algorithms that power user interactions to complex machine-learning
    models that aid users with predictions and forecasting. For example, recommender
    systems can help users by presenting them with content or products likely to best
    satisfy their needs. Other algorithmic systems can mine through large volumes
    of data to offer advice to users in a plethora of decision-making tasks.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: A central objective common to all human-AI decision-making contexts is the potential
    to improve decision-making effectiveness by combining human intelligence with
    the computational power of algorithmic systems. This, however, is far from how
    many collaborative human-AI decision-making processes unfold in the real world.
    Humans fail to rely appropriately on AI systems in decision-making tasks, leading
    to sub-optimal team performance. *Appropriate reliance* has been conceptualized
    as the reliance of humans on AI advice when it is correct and self-reliance when
    the AI is incorrect [11]. There are a number of factors that play a role in shaping
    such outcomes — human factors (e.g., domain knowledge, affinity to technology
    interaction, prior experience); system factors (e.g., accuracy or confidence of
    the AI system); task factors (e.g., task complexity, task uncertainty, stakes).
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: Empirical explorations of human-AI decision-making in various contexts, including
    loan application decisions and medical diagnosis, have revealed that humans either
    under-rely on AI advice and lose out on the opportunity to improve the outcomes
    of their decisions or over-rely on AI advice and achieve sub-optimal outcomes.
    To tackle the problem of over-reliance and under-reliance and foster appropriate
    reliance on AI advice, prior works have proposed the use of explanations [13],
    *cognitive forcing functions* (i.e., interventions that force critical consideration
    and reflection during the decision-making process) [4], tutorials or training
    sessions that communicate the strengths and weaknesses of AI systems, and initiatives
    to increase the general AI literacy of populations. Recent work has proposed an
    alternative framework called “*evaluative AI”* to promote appropriate reliance
    on AI advice. This framework suggests that decision support tools should provide
    evidence for and against decisions made by people rather than provide recommendations
    to accept or reject [7].
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: Cognitive biases have also influenced human-AI decision-making [1, 3]. Rastogi
    et al. [9] argued that our general perception and understanding of decision-making
    tasks can be distorted by cognitive biases, such as confirmation bias, anchoring
    bias, and availability bias. They explored the role of anchoring bias and proposed
    methods to mitigate their negative effects on collaborative decision-making performance.
    He et al. [22] showed that the Dunning-Kruger effect, a metacognitive bias, can
    influence how people rely on advice from AI systems. They revealed that users
    who overestimate their ability or performance tend to exhibit under-reliance on
    AI systems, hindering optimal team performance in decision-making tasks. Other
    factors, such as algorithmic aversion and appreciation, have also been shown to
    be influential in determining the fruitfulness of human-AI decision-making [17].
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 认知偏差也影响了人机决策[1, 3]。Rastogi等人[9]认为，我们对决策任务的整体感知和理解可能会受到认知偏差的扭曲，例如确认偏差、锚定偏差和可得性偏差。他们探讨了锚定偏差的作用，并提出了减少其对协作决策表现负面影响的方法。He等人[22]展示了厄尔斯效应（一种元认知偏差）如何影响人们对AI系统建议的依赖。他们揭示了那些高估自己能力或表现的用户倾向于对AI系统表现出过少的依赖，从而阻碍了决策任务中的最佳团队表现。其他因素，如算法厌恶和欣赏，也被证明对人机决策的成果有影响[17]。
- en: Despite the ongoing work in the broad realm of human-AI collaboration, fostering
    appropriate reliance on AI systems in decision-making tasks remains an unsolved
    problem. Different research communities at the intersection of Artificial Intelligence,
    Machine Learning, and Human-Computer Interaction are actively working on advancing
    our understanding of this area and developing methods, tools, and frameworks that
    can help us benefit from the potential of human-AI collaboration.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管在人机协作的广泛领域中正在进行持续的工作，但在决策任务中如何适当地依赖AI系统仍然是一个未解的问题。人工智能、机器学习和人机交互交汇的不同研究社区正积极推进我们对这一领域的理解，并开发方法、工具和框架，帮助我们从人机协作的潜力中受益。
- en: At this moment, large language models (LLMs) have found widespread application
    and adoption across domains. In the remainder of this article, we will explore
    the opportunities that LLMs provide in aiding human decision-making and intertwined
    with potential benefits.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，大型语言模型（LLMs）在各个领域得到了广泛的应用和采纳。在本文的余下部分，我们将深入探讨LLMs在辅助人类决策中提供的机会以及潜在的好处。
- en: LLMs for Decision-Making Tasks
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LLMs用于决策任务
- en: LLMs are being increasingly used in a variety of sociotechnical systems despite
    demonstrating biases and the potential to cause harm. Having said that, they have
    also shown promise to have a positive impact at scale — for instance, through
    supporting auditing processes as demonstrated by Rostagi et al. [8] via an auditing
    tool that is powered by a generative LLM. The authors proposed to leverage the
    complementary strengths of humans and generative models in the collaborative auditing
    of commercial language models. Wu et al. [14] proposed *AutoGen*, a framework
    that enables complex LLM-based workflows using multi-agent conversations. AutoGen
    can support online decision-making tasks such as game-playing or web interactions.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管大型语言模型（LLMs）表现出偏见和可能造成伤害的潜在风险，它们在各种社会技术系统中的使用仍在不断增加。话虽如此，它们也显示出了在大规模上产生积极影响的潜力——例如，通过支持审计过程，如Rostagi等人[8]所展示的，通过一个由生成型LLM驱动的审计工具。作者提议在商业语言模型的协同审计中利用人类和生成模型的互补优势。Wu等人[14]提出了*AutoGen*，一个通过多代理对话支持复杂LLM-based工作流程的框架。AutoGen可以支持在线决策任务，如游戏或网络互动。
- en: On the one hand, there is evidence that LLMs like GPT-3 exhibit behavior that
    strikingly resembles human-like intuition — and the cognitive errors that come
    with it [16]. Recent research has highlighted the feasibility of using ChatGPT
    for radiologic decision-making, potentially improving clinical workflows and the
    responsible use of radiology services [18]. To enhance AI safety in decision-making
    processes, Jin et al. [15] have aimed to replicate and empower LLMs with the ability
    to determine when a rule should be broken, especially in novel or unusual situations.
    On the other hand, LLMs can inadvertently perpetuate stereotypes toward marginalized
    groups [20] and exhibit bias involving race, gender, religion, and political orientation.
    Similar to the challenges in fostering appropriate trust and reliance among users
    in decision-support systems, if humans are to rely on LLMs for decision-making,
    we will need to better understand the benefits and pitfalls of such interactions.
    A particularly magnified risk in LLM-powered interactions is the apparent ease
    with which conversational interactions can be facilitated. Prior work has already
    uncovered the illusionary role of explanatory depth created by using explanations
    in decision-making tasks, resulting in overreliance on AI systems. If human interactions
    with decision-support systems are made even more *seamless* (for example, through
    interactive or conversational interfaces), we can expect to unearth more instances
    of inappropriate reliance.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: It is increasingly difficult to study LLMs through the lenses of their architecture
    and hyperparameters. There is sufficient evidence, at this point, to understand
    that generative AI can produce high-quality written and visual content that may
    be used for the greater good or misused to cause harm. Potsdam Mann et al. [19]
    argue that a credit–blame asymmetry arises for assigning responsibility for LLMs
    outputs and ethical and policy implications focused on LLMs.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: What Needs To Be Done Next?
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'It is evident that more research and empirical work is needed to inform the
    safe and robust use of LLMs in decision-making tasks. This is particularly evident
    when considering the current limitations with respect to multimodal and multilingual
    LLMs. Here is a compilation of some questions that remain critical in determining
    the extent to which we can consistently benefit from marrying LLMs into everyday
    decision-making:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: '*How can we facilitate appropriate reliance on LLMs or LLM-infused systems
    for effective decision-making?*'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*How can we increase the robustness, reliability, and trustworthiness of LLM-infused
    decision-support systems?*'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*How can we foster appropriate trust and reliance on LLMs in multimodal and
    multilingual decision-making contexts?*'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*How can people of varying abilities, individual traits, prior knowledge, education
    and qualifications, and other demographics be equally supported by LLMs in decision-making
    tasks?*'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**So, if you have an LLM at your fingertips, do not be in a hurry to rely on
    it as a black-box decision-making aid just yet!**'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '**因此，如果你手边有一个大语言模型，还是不要急于把它当作黑箱决策辅助工具来依赖！**'
- en: Dr. ir. Ujwal Gadiraju is a tenured Assistant Professor at [*Delft University
    of Technology*](https://www.tudelft.nl/en/). He co-directs the [Delft “Design@Scale”
    AI Lab](https://www.tudelft.nl/en/ai/design-at-scale-lab) and co-leads a Human-Centered
    AI and Crowd Computing research line. He is a Distinguished Speaker of the ACM
    and a board member of [*CHI Netherlands*](https://chinederland.nl/). Ujwal spends
    a part of his time working at [Toloka AI](https://toloka.ai/) with their AI, data,
    and research teams and is also an advisory board member for [Deeploy](https://www.deeploy.ml/),
    a growing MLOps company.
  id: totrans-34
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Dr. ir. Ujwal Gadiraju 是[*代尔夫特理工大学*](https://www.tudelft.nl/en/)的终身副教授。他共同指导了[Delft
    “Design@Scale” AI实验室](https://www.tudelft.nl/en/ai/design-at-scale-lab)并领导了一个以人为本的AI与众包计算研究方向。他是ACM的杰出演讲者，也是[*CHI
    Netherlands*](https://chinederland.nl/)的董事会成员。Ujwal在[Toloka AI](https://toloka.ai/)的AI、数据和研究团队中花费部分时间工作，同时也是[Deeploy](https://www.deeploy.ml/)的顾问委员会成员，Deeploy是一家正在成长的MLOps公司。
- en: References
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Bertrand, A., Belloum, R., Eagan, J. R., & Maxwell, W. (2022, July). *How cognitive
    biases affect XAI-assisted decision-making: A systematic review.* In Proceedings
    of the 2022 AAAI/ACM Conference on AI, Ethics, and Society (pp. 78–91).'
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Bertrand, A., Belloum, R., Eagan, J. R., & Maxwell, W. (2022年7月). *认知偏差如何影响XAI辅助决策：系统评估.*
    见于2022 AAAI/ACM AI、伦理与社会会议论文集（第78–91页）。
- en: Bossaerts, P., & Murawski, C. (2017). *Computational complexity and human decision-making.*
    Trends in cognitive sciences, 21(12), 917–929.
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Bossaerts, P., & Murawski, C. (2017). *计算复杂性与人类决策.* 认知科学趋势, 21(12), 917–929。
- en: Boonprakong, N., He, G., Gadiraju, U., van Berkel, N., Wang, D., Chen, S., Liu,
    J., Tag, B., Goncalves, J. and Dingler, T., 2023\. *Workshop on Understanding
    and Mitigating Cognitive Biases in Human-AI Collaboration.*
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Boonprakong, N., He, G., Gadiraju, U., van Berkel, N., Wang, D., Chen, S., Liu,
    J., Tag, B., Goncalves, J. 和 Dingler, T., 2023\. *关于理解和缓解人类与AI协作中的认知偏差的研讨会。*
- en: 'Buçinca, Z., Malaya, M. B., & Gajos, K. Z. (2021). *To trust or to think: cognitive
    forcing functions can reduce overreliance on AI in AI-assisted decision-making.*
    Proceedings of the ACM on Human-Computer Interaction, 5(CSCW1), 1–21.'
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Buçinca, Z., Malaya, M. B., & Gajos, K. Z. (2021). *信任还是思考：认知强制函数可以减少对AI的过度依赖.*
    ACM人机交互学报, 5(CSCW1), 1–21。
- en: Haupt, C. E., & Marks, M. (2023). *AI-generated medical advice — GPT and beyond.*
    Jama, 329(16), 1349–1350.
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Haupt, C. E., & Marks, M. (2023). *AI生成的医学建议——GPT及其超越。* Jama, 329(16), 1349–1350。
- en: Kahneman, D. (2011). *Thinking, Fast and Slow.* Macmillan.
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Kahneman, D. (2011). *思考，快与慢。* 麦克米伦。
- en: Miller, T. (2023, June). *Explainable AI is Dead, Long Live Explainable AI!
    Hypothesis-driven Decision Support using Evaluative AI.* In Proceedings of the
    2023 ACM Conference on Fairness, Accountability, and Transparency (pp. 333–342).
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Miller, T. (2023年6月). *可解释AI已死，万岁可解释AI！使用评估性AI的假设驱动决策支持.* 见于2023 ACM公平性、问责性与透明度会议论文集（第333–342页）。
- en: Rastogi, C., Tulio Ribeiro, M., King, N., Nori, H., & Amershi, S. (2023, August).
    *Supporting human-ai collaboration in auditing llms with llms.* In Proceedings
    of the 2023 AAAI/ACM Conference on AI, Ethics, and Society (pp. 913–926).
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Rastogi, C., Tulio Ribeiro, M., King, N., Nori, H., & Amershi, S. (2023年8月).
    *支持人类与AI协作的审计LLMs.* 见于2023 AAAI/ACM AI、伦理与社会会议论文集（第913–926页）。
- en: 'Rastogi, C., Zhang, Y., Wei, D., Varshney, K. R., Dhurandhar, A., & Tomsett,
    R. (2022). *Deciding fast and slow: The role of cognitive biases in ai-assisted
    decision-making.* Proceedings of the ACM on Human-Computer Interaction, 6(CSCW1),
    1–22.'
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Rastogi, C., Zhang, Y., Wei, D., Varshney, K. R., Dhurandhar, A., & Tomsett,
    R. (2022). *快速与缓慢决策：认知偏差在AI辅助决策中的作用.* ACM人机交互学报, 6(CSCW1), 1–22。
- en: Santos, L. R., & Rosati, A. G. (2015). *The evolutionary roots of human decision
    making.* Annual review of psychology, 66, 321–347.
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Santos, L. R., & Rosati, A. G. (2015). *人类决策的进化根源.* 心理学年鉴, 66, 321–347。
- en: Schemmer, M., Hemmer, P., Kühl, N., Benz, C., & Satzger, G. (2022). *Should
    I follow AI-based advice? Measuring appropriate reliance in human-AI decision-making.*
    arXiv preprint arXiv:2204.06916.
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Schemmer, M., Hemmer, P., Kühl, N., Benz, C., & Satzger, G. (2022). *我是否应该听从基于AI的建议？衡量人类与AI决策中的适当依赖性.*
    arXiv预印本 arXiv:2204.06916。
- en: 'Schwartz, B. (2004). *The paradox of choice: Why more is less.* New York.'
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Schwartz, B. (2004). *选择的悖论：为何更多更少。* 纽约。
- en: Vasconcelos, H., Jörke, M., Grunde-McLaughlin, M., Gerstenberg, T., Bernstein,
    M. S., & Krishna, R. (2023). *Explanations can reduce overreliance on ai systems
    during decision-making.* Proceedings of the ACM on Human-Computer Interaction,
    7(CSCW1), 1–38.
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Wu, Qingyun, Gagan Bansal, Jieyu Zhang, Yiran Wu, Shaokun Zhang, Erkang Zhu,
    Beibin Li, Li Jiang, Xiaoyun Zhang, and Chi Wang. *AutoGen: Enabling next-gen
    LLM applications via multi-agent conversation framework.* arXiv preprint arXiv:2308.08155
    (2023).'
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Jin, Z., Levine, S., Gonzalez Adauto, F., Kamal, O., Sap, M., Sachan, M., Mihalcea,
    R., Tenenbaum, J. and Schölkopf, B., 2022\. *When to make exceptions: Exploring
    language models as accounts of human moral judgment.* Advances in neural information
    processing systems, 35, pp.28458–28473.'
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Hagendorff, T., Fabi, S., & Kosinski, M. (2022). *Machine intuition: Uncovering
    human-like intuitive decision-making in GPT-3.5.* arXiv preprint arXiv:2212.05206.'
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Erlei, A., Das, R., Meub, L., Anand, A., & Gadiraju, U. (2022, April). *For
    what it’s worth: Humans overwrite their economic self-interest to avoid bargaining
    with AI systems.* In Proceedings of the 2022 CHI Conference on Human Factors in
    Computing Systems (pp. 1–18).'
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Rao, A., Kim, J., Kamineni, M., Pang, M., Lie, W., & Succi, M. D. (2023). *Evaluating
    ChatGPT as an adjunct for radiologic decision-making.* medRxiv, 2023–02.
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Porsdam Mann, S., Earp, B. D., Nyholm, S., Danaher, J., Møller, N., Bowman-Smart,
    H., … & Savulescu, J. (2023). *Generative AI entails a credit–blame asymmetry.*
    Nature Machine Intelligence, 1–4.
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Dhingra, H., Jayashanker, P., Moghe, S., & Strubell, E. (2023). *Queer people
    are people first: Deconstructing sexual identity stereotypes in large language
    models.* arXiv preprint arXiv:2307.00101.'
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'He, G., Kuiper, L., & Gadiraju, U. (2023, April). Knowing About Knowing: An
    Illusion of Human Competence Can Hinder Appropriate Reliance on AI Systems. In
    *Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems*
    (pp. 1–18).'
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
