# 神经图数据库

> 原文：[https://towardsdatascience.com/neural-graph-databases-cc35c9e1d04f?source=collection_archive---------0-----------------------#2023-03-28](https://towardsdatascience.com/neural-graph-databases-cc35c9e1d04f?source=collection_archive---------0-----------------------#2023-03-28)

## 图神经网络数据库的最新进展

## 图数据管理的新里程碑

[](https://mgalkin.medium.com/?source=post_page-----cc35c9e1d04f--------------------------------)[![Michael Galkin](../Images/c5eb13334712ca0462d8a5df4a268ad0.png)](https://mgalkin.medium.com/?source=post_page-----cc35c9e1d04f--------------------------------)[](https://towardsdatascience.com/?source=post_page-----cc35c9e1d04f--------------------------------)[![Towards Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----cc35c9e1d04f--------------------------------) [Michael Galkin](https://mgalkin.medium.com/?source=post_page-----cc35c9e1d04f--------------------------------)

·

[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F4d4f8ddd1e68&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fneural-graph-databases-cc35c9e1d04f&user=Michael+Galkin&userId=4d4f8ddd1e68&source=post_page-4d4f8ddd1e68----cc35c9e1d04f---------------------post_header-----------) 发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----cc35c9e1d04f--------------------------------) ·14 分钟阅读·2023年3月28日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fcc35c9e1d04f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fneural-graph-databases-cc35c9e1d04f&user=Michael+Galkin&userId=4d4f8ddd1e68&source=-----cc35c9e1d04f---------------------clap_footer-----------)

--

[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fcc35c9e1d04f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fneural-graph-databases-cc35c9e1d04f&source=-----cc35c9e1d04f---------------------bookmark_footer-----------)

我们引入了神经图数据库的概念，作为图数据库发展的下一步。神经图数据库专为大规模不完整图设计，并利用图表示学习进行缺失边的即时推理。神经推理保持了较高的表达能力，支持类似于标准图查询语言的复杂逻辑查询。

![](../Images/52796205e90c035d95249244ab623490.png)

图片由作者提供，辅助工具为 Stable Diffusion。

*本文由* [*Hongyu Ren*](http://hyren.me/)*,* [*Michael Cochez*](https://www.cochez.nl/)* 和* [*Zhaocheng Zhu*](https://kiddozhu.github.io/) *共同撰写，基于我们最新的论文* [*Neural Graph Reasoning: Complex Logical Query Answering Meets Graph Databases*](https://arxiv.org/abs/2303.14617)*。你也可以关注* [*我*](https://twitter.com/michael_galkin)*,* [*Hongyu*](https://twitter.com/ren_hongyu)*,* [*Michael*](https://twitter.com/michaelcochez)* 和* [*Zhaocheng*](https://twitter.com/zhu_zhaocheng) *在Twitter上的动态。查看我们的* [*项目网站*](https://www.ngdb.org/) *获取更多资料。*

# **概述**：

1.  神经图数据库：什么和为什么？

1.  NGDBs的蓝图

1.  神经图存储

1.  神经查询引擎

1.  查询引擎的神经图推理

1.  NGDBs的开放挑战

1.  了解更多

# 神经图数据库：什么和为什么？

🍨香草图数据库几乎随处可见，这要归功于不断增长的生产图、灵活的图数据模型和富有表现力的查询语言。经典的符号图数据库在一个重要假设下运行得又快又酷：

> 完整性。查询引擎假设经典图数据库中的图是完整的。

在完整性假设下，我们可以构建索引，以多种读写优化格式存储图，并期望数据库返回**有什么**。

但这一假设在实际中往往不成立（我们会说，几乎总是不成立）。例如在一些突出的知识图谱（KGs）中：在Freebase中，93.8%的人没有出生地，[78.5%没有国籍](https://aclanthology.org/P09-1113.pdf)，约68%的人[没有任何职业](https://dl.acm.org/doi/abs/10.1145/2566486.2568032)，而在Wikidata中，[约50%的艺术家没有出生日期](https://arxiv.org/abs/2207.00143)，只有[0.4%的已知建筑有高度信息](https://dl.acm.org/doi/abs/10.1145/3485447.3511932)。这仅仅是由数百名爱好者公开编辑的最大KG，100M节点和1B语句并不是行业中最大的图，所以你可以想象其不完整性的程度。

显然，为了考虑不完整性，除了**“有什么？”**我们还必须问**“缺少什么？”**（或“可以有什么？”）。让我们来看一个例子：

![](../Images/f323dce3ed36b383b14b5194429efd45.png)

(a) - 输入查询；(b) — 带有预测边（虚线）的不完整图；(c) — 通过图遍历返回一个答案（UofT）的SPARQL查询；(d) — 神经执行恢复缺失的边，并返回两个新答案（UdeM, NYU）。图片来源：作者。

在这里，给定一个不完整的图（缺失边 `(Turing Award, win, Bengio)` 和 `(Deep Learning, field, LeCun)`）以及一个查询 *“在深度学习领域的图灵奖得主在哪些大学工作？”*（以逻辑形式或类似 SPARQL 的语言表达），符号图数据库只会返回一个通过图遍历得到的答案 **UofT**。我们将这种答案称为 *简单* 答案，或现有答案。考虑到缺失的边，我们可以恢复两个更多的答案 **UdeM** 和 **NYU**（*困难* 答案，或推断答案）。

如何推断缺失的边？

+   在经典数据库中，我们选择不多。基于 RDF 的数据库具有一些形式语义，可以由庞大的 OWL 本体支持，但根据图的大小和推理的复杂性，在 [SPARQL 推理规则](https://www.w3.org/TR/sparql11-entailment/) 中完成推理可能需要无限的时间。标记属性图（LPG）数据库完全没有内置的推断缺失边的手段。

+   得益于图机器学习的进展，我们通常可以在潜在（嵌入）空间中以线性时间执行链接预测！然后，我们可以将这种机制扩展到在嵌入空间中执行复杂的、类似数据库的查询。

> 神经图数据库结合了传统图数据库和现代图机器学习的优势。

即，数据库原则如（1）图作为一等公民，（2）高效存储，以及（3）统一查询接口，现在由图 ML 技术支持，如（1）几何表示，（2）对噪声输入的鲁棒性，（3）大规模预训练和微调，以弥合不完整性差距并实现神经图推理和推断。

一般来说，NGDB 的设计原则包括：

+   **数据不完整性假设** — 潜在数据可能在节点、链接和图级别上缺少信息，我们希望推断并在查询回答中加以利用；

+   **归纳性和可更新性** — 类似于传统数据库，允许更新和即时查询，构建图潜变量的表示学习算法必须具有归纳性，并以零样本（或少样本）方式对未见数据（新实体和关系）进行泛化，以防止昂贵的再训练（例如，浅层节点嵌入）；

+   **表达能力** — 潜在表示在数据中编码逻辑和语义关系的能力，类似于 FOL（或其片段），并在查询回答中加以利用。实际上，神经推理支持的逻辑操作符集应接近或等同于标准图数据库语言，如 SPARQL 或 Cypher；

+   **超越知识图谱的多模态性**——任何可以作为节点或记录存储在经典数据库中的图结构数据（例如图像、文本、分子图或带时间戳的序列），并且可以赋予向量表示的，都是神经图存储和神经查询引擎的有效来源。

解决NGDB原则的关键方法是：

+   **向量表示作为原子元素**——虽然传统的图数据库在许多索引中对邻接矩阵（或边列表）进行哈希处理，但不完全性假设意味着给定的边**和**图潜在（向量表示）都成为*真理的来源*，在*神经图存储*中。

+   **在潜在空间中的神经查询执行**——由于不完全性假设，基本操作如边遍历不能仅通过符号操作来执行。相反，*神经查询引擎*在邻接和图潜在空间上操作，以将可能缺失的数据纳入查询回答中；

实际上，通过在潜在空间中回答查询（且不牺牲遍历性能），我们可以完全抛弃符号数据库索引。

![](../Images/9e2a728bb9c7fb27b599ef19bf69ac7f.png)

符号图数据库和神经图数据库之间的主要区别：传统的数据库通过边遍历回答“有什么？”的问题，而神经图数据库还会回答“缺少什么？”的问题。图像来源：作者。

# NGDBs的蓝图

在深入了解NGDBs之前，我们先来看一下**神经数据库**的一般情况——事实证明它们已经存在了一段时间，你可能已经注意到了。许多机器学习系统在数据被编码为模型参数时，已经在这一范式下运行，而查询相当于前向传播，可以为下游任务输出新的表示或预测。

## **神经数据库概述**

神经数据库的现状如何？它的不同种类之间有什么区别，NGDBs（神经图数据库）有什么特别之处？

![](../Images/a2a55a65281df449a1cecf755ec66364.png)

向量数据库、自然语言数据库和神经图数据库之间的区别。图像来源：作者

1.  **向量数据库**属于存储导向的系统，这些系统通常基于近似最近邻库（ANN），如[Faiss](https://github.com/facebookresearch/faiss)或[ScaNN](https://github.com/google-research/google-research/tree/master/scann)（或定制解决方案）来回答基于距离的查询，使用最大内积搜索（MIPS）、L1、L2或其他距离。由于向量数据库与编码器无关（即，任何生成向量表示的编码器，如ResNet或BERT，都可以作为来源），它们速度很快，但缺乏复杂的查询回答能力。

1.  最近，随着大规模预训练模型的崛起——或称为[基础模型](https://en.wikipedia.org/wiki/Foundation_models)——我们见证了它们在自然语言处理和计算机视觉任务中的巨大成功。我们认为，这些基础模型也是神经数据库的一个重要例子。在这些模型中，*存储模块*可能直接以模型参数的形式呈现，或者外包给一个外部索引，这在[检索增强模型](https://arxiv.org/abs/2002.08909)中常常使用，因为将所有世界知识编码到即便是数十亿个模型参数中也是困难的。*查询模块*通过填充编码器模型（BERT或T5风格）中的空白或通过解码器模型（GPT风格）中的提示，进行上下文学习，这些提示可以跨越多种模式，例如[视觉应用的可学习标记](https://arxiv.org/abs/2205.10337)或甚至[调用外部工具](https://arxiv.org/abs/2302.07842)。

1.  [Thorne et al](https://arxiv.org/abs/2106.01074)介绍的**自然语言数据库 (NLDB)**将原子元素建模为通过预训练语言模型（LM）编码为向量的文本事实。对NLDB的查询以自然语言表达的形式发送，这些查询被编码为向量，查询处理采用*检索器-阅读器*方法。

神经图数据库并不是一个新名词——许多图机器学习方法尝试将图嵌入与数据库索引结合起来，或许[RDF2Vec](http://rdf2vec.org/)和[LPG2Vec](https://openreview.net/forum?id=p0sMj8oH2O)是一些最显著的例子，展示了如何将嵌入插件到**现有**图数据库中，并在符号索引之上运行。

相比之下，我们认为NGDB可以在潜在空间中**无需符号索引**直接工作。如下面所示，存在能够模拟嵌入空间中精确边遍历行为的机器学习算法，以检索“**那里有什么**”，并进行神经推理以回答“**缺少什么**”。

## **神经图数据库：架构**

![](../Images/223c920f6d8c52a35526536fdd7523b5.png)

神经图数据库的概念图。输入查询由神经查询引擎处理，其中规划器导出查询的计算图，执行器在潜在空间中执行查询。神经图存储使用图存储和特征存储在嵌入存储中获取潜在表示。执行器与嵌入存储通信，以检索和返回结果。图像来源于作者

在更高层次上，NGDB包含两个主要组件：**神经图存储**和**神经查询引擎**。查询回答流程从某些应用程序或下游任务发送的已结构化格式的查询开始（例如，通过[语义解析](https://arxiv.org/abs/2209.15003)将初始自然语言查询转换为结构化格式）。

查询首先到达神经查询引擎，特别是到*查询规划器*模块。查询规划器的任务是根据查询复杂性、预测任务和底层数据存储（如可能的图划分）生成一个高效的原子操作计算图。

生成的计划随后被送往*查询执行器*，该执行器将查询编码到潜在空间中，执行对底层图及其潜在表示的原子操作，并将原子操作的结果聚合成最终答案集。执行是通过与*神经图存储*通信的*检索*模块完成的。

存储层包括

1️⃣ *图存储* 用于以空间和时间高效的方式保存多关系邻接矩阵（例如，以各种稀疏格式如COO和CSR）。

2️⃣ *特征存储* 用于保存与底层图相关的节点和边级多模态特征。

3️⃣ *嵌入存储* 利用编码器模块生成基于底层邻接和相关特征的潜在空间中的图表示。

检索模块查询编码后的图表示，以构建潜在答案的分布。

# **神经图存储**

![](../Images/59c2e89f7e466bbfb47e484f362dd3ab.png)

在传统的图数据库（右侧），查询被优化为一个计划（通常是一个连接操作符的树），并执行于数据库索引的存储中。在神经图数据库（左侧）中，我们将查询（或其步骤）编码到一个潜在空间中，并在底层图的潜在空间中执行。图像由作者提供。

在传统的图数据库中，存储设计通常取决于图建模范式。

两种最流行的范式是资源描述框架（RDF）图和标记属性图（LPG）。然而，我们认为新的 [RDF-star](https://w3c.github.io/rdf-star/cg-spec/editors_draft.html)（及其伴随的SPARQL-star）将统一这两种范式，将RDF图的逻辑表达性与LPG的属性特性融合起来。许多现有的知识图谱已经遵循了类似RDF-star的范式，如 [超关系知识图谱](/representation-learning-on-rdf-and-lpg-knowledge-graphs-6a92f2660241) 和 [Wikidata Statement Model](https://www.wikidata.org/wiki/Help:Statements)。

> 如果我们展望未来几年的骨干图建模范式，我们会选择RDF-star。

在神经图存储中，输入图及其向量表示都是事实来源。为了在潜在空间中回答查询，我们需要：

+   查询编码器

+   图编码器

+   检索机制用于将查询表示与图表示进行匹配

图编码（嵌入）过程可以视为一个压缩步骤，但保留了实体/关系的语义和结构相似性。嵌入空间中实体/关系之间的距离应该与语义/结构相似性正相关。编码器的架构有很多选择——我们建议坚持使用**归纳**型的，以遵循NGDB设计原则。在我们最近的[NeurIPS 2022工作](https://arxiv.org/abs/2210.08008)中，我们展示了两个这样的归纳模型。

查询编码通常与自然图编码相匹配，使得它们处于同一空间。一旦我们有了潜在表示，检索模块就会启动以提取相关答案。

检索过程可以被视为在嵌入空间中对输入向量的最近邻搜索，并且具有3个直接好处：

1.  每个检索项的置信度评分——多亏了嵌入空间中预定义的距离函数。

1.  潜在空间和距离函数的不同定义——针对不同的图，例如，树状图在双曲空间中更易于处理。

1.  效率和可扩展性——检索可以扩展到包含数十亿节点和边的极大图。

# **神经查询引擎**

![](../Images/d34c8122344f986bba31e4b69db9153e.png)

NGDBs（左）和传统图数据库（右）的查询规划。NGDB规划（假设图不完整）可以逐步自回归执行（1）或完全生成一个步骤（2）。传统数据库规划是基于成本的，并且依赖于元数据（假设图完整并从中提取），例如中间答案的数量来构建连接操作符的树。图片由作者提供

在传统数据库中，典型的查询引擎执行三个主要操作。(1) **查询解析**以验证语法正确性（通常会进行更深层次的语义分析）；(2) **查询规划**和优化以得出一个有效的查询计划（通常是关系操作符的树），以最小化计算成本；(3) **查询执行**根据查询计划扫描存储并处理中间结果。

将这些操作扩展到NGDBs是相当简单的。

1️⃣ 查询解析可以通过语义解析转化为结构化查询格式。我们故意将NGDBs的查询语言讨论留待未来的工作和热烈的公众讨论😉

2️⃣ 查询规划器得出原子操作（投影和逻辑操作符）的有效查询计划，最大化完整性（必须返回所有现有边上的答案）和推断（即时预测缺失边）同时考虑查询复杂性和底层图。

3️⃣ 一旦查询计划完成，查询执行器将查询（或其部分）编码到潜在空间，与图存储及其检索模块进行通信，并将中间结果聚合到最终答案集中。查询执行存在两种常见机制：

+   *原子*，类似于传统数据库，当查询计划按顺序执行，通过编码原子模式、检索其答案和执行逻辑操作作为中间步骤；

+   *全局*，当整个查询图在一个步骤中被编码并在潜在空间中执行。

神经查询执行的主要挑战是将查询表达能力与 SPARQL 或 Cypher 等符号语言匹配——迄今为止，神经方法可以执行接近一阶逻辑表达能力的查询，但在符号语言方面还差一半。

# 神经图推理的分类学用于查询引擎

自 2018 年以来，关于复杂逻辑查询回答的神经方法（即*查询嵌入*）的文献不断增加，特别是 [Hamilton 等人](https://proceedings.neurips.cc/paper/7473-embedding-logical-queries-on-knowledge-graphs) 在**图查询嵌入**（GQE）方面的开创性 NeurIPS 工作。GQE 能够回答带有交集的联接查询，并实时预测缺失的链接。

> GQE 可以被视为对 NGDBs 的神经查询引擎的第一次尝试。

GQE 开创了图机器学习的整个子领域，随后出现了一些著名的例子，如 [Query2Box (ICLR 2020)](https://openreview.net/forum?id=BJgr4kSFDS) 和 [Continuous Query Decomposition (ICLR 2021)](https://openreview.net/forum?id=Mos9F9kDwkz)。我们进行了一项重大工作，将所有这些（约 50 项）工作按 3 个主要方向进行了分类：

⚛️ **图**——我们回答查询的基础结构是什么；

🛠️ **建模**——我们如何回答查询以及采用了哪些归纳偏差；

🗣️ **查询**——我们回答什么，查询结构是什么，以及预期的答案是什么。

![](../Images/96529703229bccd94ad6c37088dd2a2b.png)

复杂逻辑查询回答的神经方法分类。有关更多详细信息，请参见<paper>。图像由作者提供

⚛️ 说到**图**，我们进一步将其细分为**模态**（经典的三元组图、超关系图、超图等）、**推理领域**（离散实体或包括连续输出）和**语义**（神经编码器如何捕捉更高阶关系，如 OWL 本体）。

🛠️ 在**建模**中，我们遵循编码器-处理器-解码器范式，对现有模型的归纳偏差进行分类，例如，具有神经或神经符号处理器的传递性或归纳编码器。

🗣️ 在 **查询** 中，我们的目标是将神经方法能够回答的查询集与符号图查询语言的查询集进行映射。我们讨论**查询操作符**（超越标准的与/或/非），**查询模式**（从链状查询到DAG和循环模式），以及**投影变量**（你喜欢的关系代数）。

# NGDB的开放挑战

分析分类法时，我们发现目前没有银弹，例如，大多数处理器只能在离散模式下处理基于树的查询。但这也意味着未来有很大的工作空间——可能包括你的贡献！

更具体地说，以下是未来几年NGDB的主要挑战。

沿着 **图** 分支：

+   **模态**：支持更多图的模态：从经典的仅三元组图到超关系图、超图以及结合图、文本、图像等的多模态源。

+   **推理领域**：支持对时间和连续（文本和数值）数据进行逻辑推理和神经查询回答——字面量构成了图的大部分以及对字面量的相关查询。

+   **背景语义**：支持复杂公理和形式语义，这些语义编码了（潜在的）实体类及其层次结构之间的高阶关系，例如，支持对描述逻辑和OWL片段进行神经推理。

在 **建模** 分支：

+   **编码器**：支持在推理时处理未见过的关系——这是（1）*可更新性*的关键，无需重新训练即可更新神经数据库；（2）启用*预训练-微调*策略，将查询回答推广到具有自定义关系模式的自定义图。

+   **处理器**：表达性处理器网络能够有效且高效地执行类似于SPARQL和Cypher操作符的复杂查询操作符。提高神经处理器的样本效率对于*训练时间与质量*权衡至关重要——在保持高预测质量的同时减少训练时间。

+   **解码器**：迄今为止，所有神经查询回答解码器仅在离散节点上操作。扩展答案范围到连续输出对于回答现实世界的查询至关重要。

+   **复杂性**：由于处理器网络的主要计算瓶颈是嵌入空间的维度（对于纯神经模型）和/或节点数（对于神经-符号模型），新型高效的神经逻辑操作符和检索方法是将NGDB扩展到数十亿节点和万亿边的关键。

在 **查询** 中：

+   **操作符**：使更复杂的查询操作符具备与声明式图查询语言相匹配的表达能力，例如，支持克林星号和加号、属性路径、过滤器。

+   **模式**：回答比树状查询更复杂的模式，包括DAG和循环图。

+   **投影变量**：允许投影超出最终叶节点实体，即允许返回中间变量、关系以及组织在元组（绑定）中的多个变量。

+   **表达能力**：回答超出简单EPFO和EFO片段的查询，并追求数据库语言的表达能力。

最后，在**数据集**和**评估**方面：

+   需要更大且**多样化的基准**，涵盖更多图模式、更具表现力的查询语义、更多查询操作符和查询模式。

+   由于现有的评估协议似乎有限（仅关注推断*硬*答案），需要一个更**有原则的评估框架和指标**，涵盖查询回答工作流的各个方面。

关于神经图存储和NGDB的一般情况，我们识别出以下挑战：

+   需要一个**可扩展检索**机制来将神经推理扩展到数十亿节点的图。检索与查询处理器及其建模先验紧密相关。现有的可扩展ANN库只能处理基本的L1、L2和余弦距离，这限制了神经查询引擎中可能的处理器空间。

+   目前，所有复杂查询数据集提供了一个硬编码的查询执行计划，可能不是最优的。需要一个**神经查询规划器**，能够将输入查询转换为最优执行序列，考虑预测任务、查询复杂性、神经处理器类型和存储层配置。

由于编码器的归纳性和可更新性而无需重新训练，运行推理时在比训练图更大的图上存在需要缓解**持续学习**、**灾难性遗忘**和**规模泛化**的问题。

# 了解更多

NGDB仍然是一个新兴概念，面临许多未来研究的挑战。如果你想了解更多关于NGDB的内容，可以查看

+   📜 我们的论文 ([arxiv](https://arxiv.org/abs/2303.14617))，

+   🌐 [我们的网站](https://www.ngdb.org/)，

+   🔧 [GitHub 仓库](https://github.com/neuralgraphdatabases/awesome-logical-query)包含最新的相关论文、数据集和分类，欢迎提出问题和PR。

我们还将组织研讨会，请关注最新动态！
