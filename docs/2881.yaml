- en: 'Something-of-Thoughts in LLM Prompting: An Overview of Structured LLM Reasoning'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LLM 提示中的思维链：结构化 LLM 推理概述
- en: 原文：[https://towardsdatascience.com/something-of-thought-in-llm-prompting-an-overview-of-structured-llm-reasoning-70302752b390?source=collection_archive---------1-----------------------#2023-09-16](https://towardsdatascience.com/something-of-thought-in-llm-prompting-an-overview-of-structured-llm-reasoning-70302752b390?source=collection_archive---------1-----------------------#2023-09-16)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/something-of-thought-in-llm-prompting-an-overview-of-structured-llm-reasoning-70302752b390?source=collection_archive---------1-----------------------#2023-09-16](https://towardsdatascience.com/something-of-thought-in-llm-prompting-an-overview-of-structured-llm-reasoning-70302752b390?source=collection_archive---------1-----------------------#2023-09-16)
- en: Chain-of-Thoughts (CoT), Tree-of-Thoughts (ToT), Graph-of-Thoughts (GoT), and
    beyond, … What are these thoughts?
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 思维链（CoT）、思维树（ToT）、思维图（GoT）以及其他相关概念，这些思维究竟是什么？
- en: '[](https://wangyz.medium.com/?source=post_page-----70302752b390--------------------------------)[![Yunzhe
    Wang](../Images/2e6f7fdae8cf731a616927bf7742d514.png)](https://wangyz.medium.com/?source=post_page-----70302752b390--------------------------------)[](https://towardsdatascience.com/?source=post_page-----70302752b390--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----70302752b390--------------------------------)
    [Yunzhe Wang](https://wangyz.medium.com/?source=post_page-----70302752b390--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://wangyz.medium.com/?source=post_page-----70302752b390--------------------------------)[![Yunzhe
    Wang](../Images/2e6f7fdae8cf731a616927bf7742d514.png)](https://wangyz.medium.com/?source=post_page-----70302752b390--------------------------------)[](https://towardsdatascience.com/?source=post_page-----70302752b390--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----70302752b390--------------------------------)
    [Yunzhe Wang](https://wangyz.medium.com/?source=post_page-----70302752b390--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F31c691ae725d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsomething-of-thought-in-llm-prompting-an-overview-of-structured-llm-reasoning-70302752b390&user=Yunzhe+Wang&userId=31c691ae725d&source=post_page-31c691ae725d----70302752b390---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----70302752b390--------------------------------)
    ·9 min read·Sep 16, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F70302752b390&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsomething-of-thought-in-llm-prompting-an-overview-of-structured-llm-reasoning-70302752b390&user=Yunzhe+Wang&userId=31c691ae725d&source=-----70302752b390---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F31c691ae725d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsomething-of-thought-in-llm-prompting-an-overview-of-structured-llm-reasoning-70302752b390&user=Yunzhe+Wang&userId=31c691ae725d&source=post_page-31c691ae725d----70302752b390---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----70302752b390--------------------------------)
    ·9分钟阅读·2023年9月16日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F70302752b390&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsomething-of-thought-in-llm-prompting-an-overview-of-structured-llm-reasoning-70302752b390&user=Yunzhe+Wang&userId=31c691ae725d&source=-----70302752b390---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F70302752b390&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsomething-of-thought-in-llm-prompting-an-overview-of-structured-llm-reasoning-70302752b390&source=-----70302752b390---------------------bookmark_footer-----------)![](../Images/ee9435141d0b3a2d40140e778b071961.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F70302752b390&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsomething-of-thought-in-llm-prompting-an-overview-of-structured-llm-reasoning-70302752b390&source=-----70302752b390---------------------bookmark_footer-----------)![](../Images/ee9435141d0b3a2d40140e778b071961.png)'
- en: “Tree of Thought”, generated by Midjourney
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: “思维树”，由 Midjourney 生成
- en: 'In the age of smartphones and smart homes, imagine an AI that doesn’t merely
    follow instructions, but actually thinks, grappling with complex logic just as
    we do. Sounds like science fiction, doesn’t it? However, if you’ve played around
    with ChatGPT, you’ve likely witnessed this astonishing capability firsthand. Even
    Hector Levesque, a renowned figure in AI reasoning, was so astounded that he once
    commented to AI legend Geoffrey Hinton: [*“How can such a stupid method (referring
    to neural networks) can deal with reasoning?”*](https://www.youtube.com/watch?v=rGgGOccMEiY&t=1598s)'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在智能手机和智能家居的时代，想象一个不仅仅按照指令执行的 AI，而是像我们一样进行复杂逻辑思考。这听起来像是科幻小说，对吗？然而，如果你玩过 ChatGPT，你可能已经亲眼目睹了这种惊人的能力。即使是著名的
    AI 推理专家 Hector Levesque 也曾对此感到惊讶，并曾对 AI 传奇人物 Geoffrey Hinton 评论道：[*“这种愚蠢的方法（指神经网络）怎么能进行推理？”*](https://www.youtube.com/watch?v=rGgGOccMEiY&t=1598s)
- en: While this story underscores the monumental advances in AI, the true essence
    of these advancements is found in the intricate dance of Large Language Models
    (LLMs) with reasoning. The entry point to this dance is Prompt Engineering — the
    art and science of optimizing the textual input provided to LLMs to elicit desired
    outputs. At its core, it’s about understanding the intricacies of how language
    models like ChatGPT, Bard, Claude, LLama, and others respond to different prompts,
    and then leveraging this knowledge to achieve specific results.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这个故事突出了人工智能的巨大进步，但这些进步的真正精髓在于大型语言模型（LLMs）与推理的复杂互动。进入这种互动的入口是提示工程——优化提供给 LLM
    的文本输入以引出期望输出的艺术与科学。从本质上讲，它涉及理解像 ChatGPT、Bard、Claude、LLama 等语言模型如何响应不同提示的复杂性，然后利用这些知识来实现特定结果。
- en: Think of LLMs as vast knowledge reservoirs. The way you phrase your question
    or statement (the prompt) determines how you tap into that reservoir. Just as
    humans might offer different answers based on how a question is posed, LLMs too
    can give varied responses based on the input.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 把大型语言模型（LLMs）看作是庞大的知识库。你提问或陈述的方式（提示）决定了如何挖掘这个知识库。就像人类根据问题的表述可能会给出不同的答案一样，LLMs
    也可能根据输入给出不同的回应。
- en: 'In this article, you’ll receive a concise overview of various prompt engineering
    frameworks designed to enhance LLM reasoning, including:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，你将获得关于各种提示工程框架的简要概述，这些框架旨在提升 LLM 推理能力，包括：
- en: Chain-of-Thought
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chain-of-Thought
- en: Chain-of-Thought-Self-Consistency
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chain-of-Thought-Self-Consistency
- en: Tree-of-Thoughts
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tree-of-Thoughts
- en: Graph-of-Thoughts
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Graph-of-Thoughts
- en: Algorithm-of-Thoughts
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Algorithm-of-Thoughts
- en: Skeleton-of-Thought
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Skeleton-of-Thought
- en: Program-of-Thoughts
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Program-of-Thoughts
- en: Chain-of-Thought (CoT)
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Chain-of-Thought (CoT)
- en: Instead of directly outputting an answer, provide the language model with intermediate
    reasoning examples to guide its response.
  id: totrans-21
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 不是直接给出答案，而是为语言模型提供中间推理示例，以引导其回答。
- en: '[Chain-of-Thought (CoT)](https://arxiv.org/abs/2201.11903) prompting has been
    recognized as one of the pioneering and most impactful prompt engineering techniques,
    enhancing the decision-making processes in large language models. Distinct from
    conventional prompting methodologies that emphasize direct input-output interactions,
    CoT compels a model to segment its reasoning into intermediary steps. This method
    draws parallels to human cognitive processes wherein intricate challenges are
    segmented into smaller, more manageable components.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '[Chain-of-Thought (CoT)](https://arxiv.org/abs/2201.11903) 提示已被认可为开创性和最具影响力的提示工程技术之一，增强了大型语言模型的决策过程。不同于强调直接输入输出交互的传统提示方法，CoT
    强制模型将其推理分解为中间步骤。这种方法与人类认知过程类似，其中复杂的挑战被分解为更小、更易管理的组成部分。'
- en: 'To illustrate, consider a mathematical problem: “Roger possesses 5 tennis balls
    and subsequently purchases 2 cans of tennis balls, with each can containing 3
    balls. How many tennis balls does he possess now?”. Rather than directly deducing
    the answer as 11, an individual might rationalize: “Initially, Roger has 5 balls.
    The combined total of 2 cans, each containing 3 balls, amounts to 6 balls. Summing
    the values, 5 + 6, yields 11.” Integrating such step-by-step analytical reasoning
    into the input prompt not only augments the accuracy of the model’s response but
    also accomplishes this without necessitating additional training datasets or alterations
    to the fundamental model configuration.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 举个例子，考虑一个数学问题：“Roger 拥有 5 个网球，然后购买了 2 罐网球，每罐包含 3 个球。他现在拥有多少个网球？”。与其直接推导出答案是
    11，一个人可能会推理：“最初，Roger 有 5 个球。2 罐球的总数，每罐包含 3 个球，总计 6 个球。将这些值相加，5 + 6，得到 11。”将这种逐步分析推理整合到输入提示中，不仅提高了模型响应的准确性，还可以在不需要额外训练数据集或改变基本模型配置的情况下完成。
- en: '![](../Images/7ba1090dc166a973d850c78f042dc18b.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7ba1090dc166a973d850c78f042dc18b.png)'
- en: 'Chain-of-Thought Prompting, source: [Wei et al. (2022)](https://arxiv.org/abs/2201.11903)'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '思维链提示，来源: [Wei et al. (2022)](https://arxiv.org/abs/2201.11903)'
- en: Chain-of-Thought-Self-Consistency (CoT-SC)
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 思维链自一致性 (CoT-SC)
- en: Construct multiple chains of thought, evaluate each one, and ultimately select
    the most effective and coherent chain.
  id: totrans-27
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 构建多个思维链，评估每一个链，最终选择最有效且连贯的链。
- en: A subsequent advancement from the Chain of Thought framework is [CoT-Self-consistency](https://arxiv.org/abs/2203.11171).
    This method instigates multiple concurrent reasoning pathways in response to a
    query and applies weighting mechanisms prior to finalizing an answer. This approach
    resembles ensemble techniques observed in traditional machine learning but is
    applied to thought sequences in large language models.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 从思维链框架发展而来的一个进步是 [CoT自一致性](https://arxiv.org/abs/2203.11171)。该方法在响应查询时发起多个并行的推理路径，并在最终确定答案之前应用加权机制。这种方法类似于传统机器学习中观察到的集成技术，但它被应用于大语言模型中的思维序列。
- en: Tree-of-Thoughts (ToT)
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 思维树 (ToT)
- en: Expand on the chains of thought in a tree format. This allows for backtracking,
    exploring multiple branches of reasoning stemming from a single root idea.
  id: totrans-30
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 扩展思维链以树形结构呈现。这允许回溯，探索从一个根思想衍生出的多个推理分支。
- en: '[Tree-of-Thoughts (ToT)](https://arxiv.org/abs/2305.10601) offers a more structured
    prompting framework for LLM reasoning by breaking down complex problems into more
    manageable parts. Unlike the CoT which reasons in a linked chain, ToT organizes
    its problem-solving strategy in a tree format. Each node, referred to as a ‘thought,’
    is a coherent language sequence serving as a step towards the final answer. By
    dividing problems into these discrete ‘thought’ units — from a brief series of
    words in a crossword puzzle to a component of a mathematical equation — ToT ensures
    that each phase of the problem is systematically addressed.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '[思维树 (ToT)](https://arxiv.org/abs/2305.10601) 提供了一个更结构化的 LLM 推理框架，通过将复杂问题分解为更可管理的部分来进行思考。与通过链条链接进行推理的
    CoT 不同，ToT 将其问题解决策略组织为树形结构。每个节点，被称为“思维”，是一个连贯的语言序列，作为通向最终答案的步骤。通过将问题分解为这些离散的“思维”单元——从填字游戏中的简短词组到数学方程的一个组件——ToT
    确保问题的每个阶段都得到系统性地解决。'
- en: '![](../Images/180d5a85d8fc814621426807b4fb77c1.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/180d5a85d8fc814621426807b4fb77c1.png)'
- en: 'ToT reasoning in the game of 24, source: [Yao et al. (2023)](https://arxiv.org/abs/2305.10601)'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 'ToT 在 24 点游戏中的推理，来源: [Yao et al. (2023)](https://arxiv.org/abs/2305.10601)'
- en: The operational strength of ToT lies in its methodical organization. First,
    the system breaks down a problem and, from its current state, generates a list
    of potential reasoning steps or ‘thought’ candidates. These thoughts are then
    evaluated, with the system gauging the likelihood that each one will lead to the
    desired solution. Standard search algorithms, such as Breadth-first search (BFS)
    and Depth-first search (DFS), are used to navigate this tree, aiding the model
    in identifying the most effective sequence of thoughts.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: ToT 的操作优势在于其系统化的组织。首先，系统将问题分解，并从当前状态生成一系列潜在的推理步骤或“思维”候选。然后对这些思维进行评估，系统会评估每个思维引导到期望解决方案的可能性。标准搜索算法，如宽度优先搜索（BFS）和深度优先搜索（DFS），用于遍历这棵树，帮助模型识别最有效的思维序列。
- en: ToT’s importance stems from its holistic design, adaptability, and efficiency.
    The Chain-of-Thought prompting can be viewed as a specific instance within the
    ToT framework. Its modular nature indicates that individual components, from the
    initial breakdown of a problem to the search algorithms employed, can operate
    independently.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: ToT 的重要性源于其整体设计、适应性和效率。链式思维提示可以被视为 ToT 框架中的一个具体实例。其模块化特性表明，从问题的初步分解到所使用的搜索算法，单独的组件可以独立运行。
- en: Graph-of-Thoughts (GoT)
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 思维图谱（GoT）
- en: Evolve the tree structure into Direct Acyclic Graphs. This introduces self-loops
    which can either solidify a particular line of thought or aggregate multiple thoughts
    into a cohesive one.
  id: totrans-37
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 将树结构演变为有向无环图。这引入了自环，可以巩固特定的思路或将多个思路汇聚成一个连贯的想法。
- en: The [Graph-of-Thoughts (GoT)](https://arxiv.org/abs/2308.09687) framework represents
    an advanced progression from CoT and ToT methodologies. Central to the GoT framework
    is the conceptualization of ideas as vertices in a Directed Acyclic Graph (DAG).
    In this context, each vertex corresponds to a specific thought or solution — be
    it preliminary, intermediary, or terminal — elicited by an input stimulus. The
    directed edges within this graph depict the interdependency among these thoughts.
    Specifically, if an edge extends from thought t1​ to t2​, it signifies that t2​
    was conceived based on t1​. This systematization permits a multiplicity of thoughts
    since nodes may be classified into distinct categories such as “plans” or “outcomes”.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '[思维图谱 (GoT)](https://arxiv.org/abs/2308.09687) 框架代表了从 CoT 和 ToT 方法的先进进展。GoT
    框架的核心是将想法概念化为有向无环图（DAG）中的顶点。在这种情况下，每个顶点对应于一个特定的思路或解决方案——无论是初步的、中间的还是最终的——由输入刺激引发。图中的有向边描绘了这些思路之间的相互依赖性。具体来说，如果一条边从思路
    t1 延伸到 t2，则表示 t2 是基于 t1 形成的。这种系统化允许多种思路，因为节点可以被分类为“计划”或“结果”等不同类别。'
- en: '![](../Images/f832191be1dfbcabf2682a84c8859370.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f832191be1dfbcabf2682a84c8859370.png)'
- en: 'Graph-of-Thoughts, source: [Besta et al. (2023)](https://arxiv.org/abs/2308.09687)'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 思维图谱，来源：[Besta 等 (2023)](https://arxiv.org/abs/2308.09687)
- en: GoT’s novelty lies in its ability to apply transformations to these thoughts,
    further refining the reasoning process. The cardinal transformations encompass
    Aggregation, which allows for the fusion of several thoughts into a consolidated
    idea; Refinement, where continual iterations are performed on a singular thought
    to improve its precision; and Generation, which facilitates the conception of
    novel thoughts stemming from extant ones. Such transformations, with an emphasis
    on the amalgamation of reasoning routes, deliver a more intricate viewpoint relative
    to preceding models like CoT or ToT.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: GoT 的新颖性在于其对这些思路应用变换，从而进一步精炼推理过程。主要的变换包括聚合，允许将几个思路融合为一个统一的想法；精炼，通过对单一思路进行不断迭代以提高其准确性；以及生成，促进从现有思路中产生新想法。这些变换，强调推理路径的融合，相较于以前的模型如
    CoT 或 ToT，提供了更复杂的观点。
- en: Furthermore, GoT introduces an evaluative dimension through Scoring and Ranking.
    Each individual thought, represented by a vertex, undergoes an assessment based
    on its pertinence and quality, facilitated by a designated scoring function. Importantly,
    this function contemplates the entire chain of reasoning, assigning scores that
    might be contextualized vis-a-vis other vertices in the graph. The framework also
    equips the system with the competence to hierarchize these thoughts predicated
    on their respective scores, a feature that proves instrumental when discerning
    which ideas warrant precedence or implementation.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，GoT 通过评分和排名引入了评估维度。每个个体思路，由一个顶点表示，都根据其相关性和质量进行评估，由指定的评分函数辅助。重要的是，这个函数考虑了整个推理链，分配的分数可能会与图中的其他顶点相关联。该框架还为系统提供了根据各自分数对这些思路进行分层的能力，这一特性在判断哪些想法值得优先考虑或实施时非常有用。
- en: Algorithm-of-Thoughts (AoT)
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 思维算法（AoT）
- en: Maintains a single evolving context chain, eliminating the need for redundant
    queries as in the Tree-of-Thought. It explores a mutable path of reasoning.
  id: totrans-44
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 保持一个不断发展的上下文链，消除了像思维树那样的冗余查询。它探索了一条可变的推理路径。
- en: While ToT and GoT address the LLM reasoning challenge through search-based mechanisms,
    producing a myriad of reasoning paths in graph forms. However, their heavy reliance
    on numerous LLM queries, sometimes numbering in the hundreds for a singular problem,
    poses computational inefficiencies.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管 ToT 和 GoT 通过基于搜索的机制解决 LLM 推理挑战，产生大量图形形式的推理路径。然而，它们对众多 LLM 查询的高度依赖，有时对于单个问题需要数百次查询，导致计算效率低下。
- en: The [Algorithm-of-Thoughts (AoT)](https://arxiv.org/abs/2308.10379) offers an
    innovative method that features a dynamic and mutable reasoning path. By maintaining
    a single evolving thought context chain, AoT consolidates thought exploration,
    enhancing efficiency and reducing computational overhead.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '[思维算法（AoT）](https://arxiv.org/abs/2308.10379)提供了一种创新的方法，具有动态和可变的推理路径。通过维持单一的不断演变的思想上下文链，AoT
    整合了思想探索，提高了效率，减少了计算开销。'
- en: '![](../Images/36b10dfbbf7392043cdf07558effb4d0.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/36b10dfbbf7392043cdf07558effb4d0.png)'
- en: 'Algorithm-of-Thoughts. Each box signifies a distinct thought. Greens are promising
    thoughts while reds are less promising ones. Note: ToT has multiple queries while
    AoT keeps a single context, source: [Sel et al. (2023)](https://arxiv.org/abs/2308.10379)'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 思维算法。每个框表示一个独特的思想。绿色代表有前景的思想，红色代表较少有前景的思想。注意：ToT 有多个查询，而 AoT 保持单一上下文，来源：[Sel
    等（2023）](https://arxiv.org/abs/2308.10379)
- en: The ingenuity behind AoT springs from the observation that LLMs, although powerful,
    occasionally revert to prior solutions when faced with new yet familiar problems.
    To overcome this, AoT assimilates in-context examples, drawing from time-tested
    search algorithms such as depth-first search (DFS) and breadth-first search (BFS).
    By emulating algorithmic behavior, AoT underscores the importance of achieving
    successful outcomes and gleaning insights from unsuccessful attempts.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: AoT 的独创性来自于观察到 LLM 尽管强大，但在面对新而熟悉的问题时，有时会回到之前的解决方案。为了克服这一点，AoT 吸收了上下文示例，借鉴了深度优先搜索（DFS）和广度优先搜索（BFS）等经过时间考验的搜索算法。通过模拟算法行为，AoT
    强调了实现成功结果和从失败尝试中汲取见解的重要性。
- en: 'The cornerstone of AoT lies in its four main components: 1) Decomposing complex
    problems into digestible subproblems, considering both their interrelation and
    the ease with which they can be individually addressed; 2) Proposing coherent
    solutions for these subproblems in a continuous and uninterrupted manner; 3) Intuitively
    evaluating the viability of each solution or subproblem without relying on explicit
    external prompts; and 4) Determining the most promising paths to explore or backtrack
    to, based on in-context examples and algorithmic guidelines.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: AoT 的基石在于其四个主要组成部分：1) 将复杂问题分解为易于处理的子问题，考虑它们之间的相互关系以及单独处理的难易程度；2) 对这些子问题提出连贯的解决方案，保持连续和不间断的方式；3)
    直观地评估每个解决方案或子问题的可行性，无需依赖明确的外部提示；4) 根据上下文示例和算法指南，确定最有前景的探索路径或回溯路径。
- en: Skeleton-of-Thought (SoT)
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 思维骨架（SoT）
- en: Generate an answer blueprint first before parallelly fleshing out the details,
    reducing the time taken to generate a complete response.
  id: totrans-52
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 首先生成答案蓝图，然后平行展开细节，从而减少生成完整响应的时间。
- en: The [Skeleton-of-Thought (SoT)](https://arxiv.org/abs/2307.15337) paradigm is
    distinctively designed not primarily to augment the reasoning capabilities of
    Large Language Models (LLMs), but to address the pivotal challenge of minimizing
    end-to-end generation latency. The methodology operates based on a dual-stage
    approach that focuses on producing a preliminary blueprint of the answer, followed
    by its comprehensive expansion.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '[思维骨架（SoT）](https://arxiv.org/abs/2307.15337)范式独特地设计，并非主要为了增强大型语言模型（LLM）的推理能力，而是解决最关键的挑战——最小化端到端生成延迟。该方法基于双阶段方法，首先生成初步答案蓝图，然后进行全面扩展。'
- en: '![](../Images/dd7fb01b086fb80b4f666696ac0dd5d9.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/dd7fb01b086fb80b4f666696ac0dd5d9.png)'
- en: 'Skeleton-of-Thought, source: [Ning et al. (2023)](https://arxiv.org/abs/2307.15337)'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 思维骨架，来源：[宁等（2023）](https://arxiv.org/abs/2307.15337)
- en: In the initial “Skeleton Stage,” rather than producing a comprehensive response,
    the model is prompted to generate a concise answer skeleton. This abbreviated
    representation prompted through a meticulously crafted skeleton template, captures
    the core elements of the prospective answer, thus establishing a foundation for
    the subsequent stage.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在初始的“骨架阶段”，模型不是生成一个全面的响应，而是生成一个简洁的答案骨架。这个通过精心设计的骨架模板生成的简略表示，捕捉了潜在答案的核心要素，从而为后续阶段奠定基础。
- en: In the ensuing “Point-Expanding Stage,” the LLM systematically amplifies each
    component delineated in the answer skeleton. Leveraging a point-expanding prompt
    template, the model concurrently elaborates on each segment of the skeleton. This
    dichotomous approach, which separates the generative process into preliminary
    skeletal formulation and parallelized detailed expansion, not only accelerates
    response generation but also strives to uphold the coherence and precision of
    the outputs.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在随后的“扩展点阶段”，LLM系统地扩展答案骨架中每个组件。利用扩展点提示模板，模型同时对骨架的每个部分进行详细说明。这种将生成过程分为初步骨架制定和并行详细扩展的二分法方法，不仅加速了响应生成，还努力保持输出的连贯性和精确性。
- en: Program-of-Thoughts (PoT)
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 思维程序（PoT）
- en: Formulate the reasoning behind question answering into an executable program,
    incorporated the program intepretor output as part of the final answer.
  id: totrans-59
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 将问题回答的推理形成一个可执行程序，将程序解释器的输出作为最终答案的一部分。
- en: '[Program-of-Thoughts (PoT)](https://arxiv.org/abs/2211.12588) is a unique approach
    to LLM reasoning, instead of merely generating an answer in natural language,
    PoT mandates the creation of an executable program, which means it can be run
    on a program interpreter, like Python, to produce tangible outcomes. This method
    stands in contrast to more direct models, emphasizing its ability to break down
    reasoning into sequential steps and associate semantic meanings with variables.
    As a result, PoT offers a clearer, more expressive, and grounded model of how
    answers are derived, enhancing accuracy and understanding, especially for math-type
    logical questions where numerical calculations are needed.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '[思维程序（PoT）](https://arxiv.org/abs/2211.12588)是一种独特的LLM推理方法，它不仅仅是生成自然语言的答案，PoT要求创建一个可执行程序，这意味着它可以在程序解释器（如Python）上运行，从而产生实际结果。这种方法与更直接的模型形成对比，强调其将推理分解为顺序步骤并将语义与变量关联的能力。因此，PoT提供了一个更清晰、更具表现力且有实据的答案生成模型，提高了准确性和理解，特别适用于需要数字计算的数学类型逻辑问题。'
- en: It is important to note that the program execution of PoT is not necessarily
    targeting the final answer but can be part of the intermediate step to the final
    answer.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，PoT 的程序执行不一定以最终答案为目标，而可能是最终答案的一个中间步骤。
- en: '![](../Images/9aa904330a995f6724768bd6b6088c44.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9aa904330a995f6724768bd6b6088c44.png)'
- en: 'Comparison between CoT and PoT, source: [Chen et al. (2022)](https://arxiv.org/abs/2211.12588)'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: CoT与PoT的比较，来源：[Chen et al. (2022)](https://arxiv.org/abs/2211.12588)
- en: In the ever-evolving realm of AI, structured reasoning frameworks like Chain-of-Thought
    have dramatically transformed how we perceive and harness the power of Large Language
    Models. They symbolize a shift towards models that not only regurgitate information
    but also engage in intricate reasoning, much akin to human cognitive processes.
    As we look ahead, the potential horizons seem limitless. Imagine an AI, adept
    at generating not only accurate answers but also robust, programmable solutions
    or having the ability to visualize its thought processes, making AI-human collaboration
    even more seamless. Such advancements, building upon the foundational frameworks
    explored in this article, herald a future where LLMs become indispensable companions
    in problem-solving, creativity, and decision-making, catalyzing a paradigm shift
    in our symbiotic relationship with technology.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在不断发展的AI领域中，结构化推理框架如链式思维（Chain-of-Thought）极大地改变了我们对大型语言模型（LLM）能力的认知和利用方式。它们象征着一种转变，向着不仅仅是信息重复的模型迈进，同时也进行复杂的推理，这与人类认知过程非常相似。展望未来，潜在的前景似乎无穷无尽。设想一个AI，能够生成不仅准确的答案，还能提供强大、可编程的解决方案，或具备可视化其思维过程的能力，使AI与人类的合作更加无缝。这些进步建立在本文探索的基础框架上，预示着一个LLM成为问题解决、创造力和决策不可或缺的伴侣的未来，推动我们与技术的共生关系发生范式转变。
