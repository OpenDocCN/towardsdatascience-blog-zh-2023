- en: Tips and Tricks for Upgrading to PyTorch 2.0
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 升级到 PyTorch 2.0 的技巧和窍门
- en: 原文：[https://towardsdatascience.com/tips-and-tricks-for-upgrading-to-pytorch-2-3127db1d1f3d?source=collection_archive---------3-----------------------#2023-05-21](https://towardsdatascience.com/tips-and-tricks-for-upgrading-to-pytorch-2-3127db1d1f3d?source=collection_archive---------3-----------------------#2023-05-21)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/tips-and-tricks-for-upgrading-to-pytorch-2-3127db1d1f3d?source=collection_archive---------3-----------------------#2023-05-21](https://towardsdatascience.com/tips-and-tricks-for-upgrading-to-pytorch-2-3127db1d1f3d?source=collection_archive---------3-----------------------#2023-05-21)
- en: What to look out for when moving to the all-new “*Compiled Mode”*
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 迁移到全新 “*编译模式*” 时需要注意的事项
- en: '[](https://chaimrand.medium.com/?source=post_page-----3127db1d1f3d--------------------------------)[![Chaim
    Rand](../Images/c52659c389f167ad5d6dc139940e7955.png)](https://chaimrand.medium.com/?source=post_page-----3127db1d1f3d--------------------------------)[](https://towardsdatascience.com/?source=post_page-----3127db1d1f3d--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----3127db1d1f3d--------------------------------)
    [Chaim Rand](https://chaimrand.medium.com/?source=post_page-----3127db1d1f3d--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://chaimrand.medium.com/?source=post_page-----3127db1d1f3d--------------------------------)[![Chaim
    Rand](../Images/c52659c389f167ad5d6dc139940e7955.png)](https://chaimrand.medium.com/?source=post_page-----3127db1d1f3d--------------------------------)[](https://towardsdatascience.com/?source=post_page-----3127db1d1f3d--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----3127db1d1f3d--------------------------------)
    [Chaim Rand](https://chaimrand.medium.com/?source=post_page-----3127db1d1f3d--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F9440b37e27fe&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftips-and-tricks-for-upgrading-to-pytorch-2-3127db1d1f3d&user=Chaim+Rand&userId=9440b37e27fe&source=post_page-9440b37e27fe----3127db1d1f3d---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----3127db1d1f3d--------------------------------)
    ·19 min read·May 21, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F3127db1d1f3d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftips-and-tricks-for-upgrading-to-pytorch-2-3127db1d1f3d&user=Chaim+Rand&userId=9440b37e27fe&source=-----3127db1d1f3d---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F9440b37e27fe&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftips-and-tricks-for-upgrading-to-pytorch-2-3127db1d1f3d&user=Chaim+Rand&userId=9440b37e27fe&source=post_page-9440b37e27fe----3127db1d1f3d---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----3127db1d1f3d--------------------------------)
    · 19 分钟阅读 · 2023年5月21日 [](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F3127db1d1f3d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftips-and-tricks-for-upgrading-to-pytorch-2-3127db1d1f3d&user=Chaim+Rand&userId=9440b37e27fe&source=-----3127db1d1f3d---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F3127db1d1f3d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftips-and-tricks-for-upgrading-to-pytorch-2-3127db1d1f3d&source=-----3127db1d1f3d---------------------bookmark_footer-----------)![](../Images/060c7e18eb124cb75770d6516eabd6ba.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F3127db1d1f3d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftips-and-tricks-for-upgrading-to-pytorch-2-3127db1d1f3d&source=-----3127db1d1f3d---------------------bookmark_footer-----------)![](../Images/060c7e18eb124cb75770d6516eabd6ba.png)'
- en: Photo by [Mohamed Nohassi](https://unsplash.com/@coopery?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由 [Mohamed Nohassi](https://unsplash.com/@coopery?utm_source=medium&utm_medium=referral)
    提供，刊登在 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: '![](../Images/26d692c970946df49e1809c2e6ee619e.png)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/26d692c970946df49e1809c2e6ee619e.png)'
- en: by Author
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 作者
- en: Any new release of an AI development framework, AI accelerator, or AI computing
    platform, brings with it the potential for runtime optimization and cost reduction
    in our AI development life-cycle. The recent release of PyTorch 2.0 is no exception.
    Highlighted by the introduction of [torch.compile](https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html),
    PyTorch 2.x can, reportedly, [enable significant speedups](https://pytorch.org/get-started/pytorch-2.0/)
    for both training and inference. Contrary to the all-familiar PyTorch eager execution
    mode in which each PyTorch operation is run “eagerly”, the [compile](https://pytorch.org/docs/stable/generated/torch.compile.html)
    API converts your model into an intermediate computation graph (an FX graph) which
    it then compiles into low-level compute kernels in a manner that is optimal for
    the underlying training accelerator, using techniques such as *kernel fusion*
    and *out-of-order execution* (see [here](https://pytorch.org/docs/stable/torch.compiler_faq.html#how-are-you-speeding-up-my-code)
    for more details).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: In this post we will demonstrate the use of this exciting new feature as well
    as some of the issues and behaviors you might encounter when using it. You may
    have already come across some posts that highlight how easy it is to use torch
    compilation or how much it improves performance. Or (like me), you may have spent
    the last two weeks grappling with the new API trying to get it to work and perform
    well on your model. Indeed, for many public models all that is required is to
    wrap them with a torch.compile call (as reported [here](https://pytorch.org/docs/stable/generated/torch.compile.html)).
    However, as we will see, there are a number of things that can interfere with
    graph compilation and/or with reaching the desired performance improvement. Adapting
    your models and/or succeeding to reach optimal performance might require you to
    redesign your project or modify some of your coding habits.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: A few things we should mention before we get started. Our intention in this
    post is to share just a few examples of the issues that we encountered while adapting
    the torch.compile API. The examples we will share are by no means comprehensive.
    It is very possible that you might run into an issue not mentioned here. Also
    keep in mind that torch.compile is still under active development. Some of the
    stuff we write might no longer be relevant by the time you read this. Be sure
    to stay up to date with the latest releases and documentation.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: There are a number of innovative technologies underlying torch compilation,
    including [TorchDynamo](https://github.com/pytorch/torchdynamo/tree/0b8aaf340dad4777a080ef24bf09623f1aa6f3dd),
    [FX Graph](https://pytorch.org/docs/stable/fx.html), [TorchInductor](https://dev-discuss.pytorch.org/t/torchinductor-a-pytorch-native-compiler-with-define-by-run-ir-and-symbolic-shapes/747),
    [Triton](https://github.com/openai/triton), and more. While we will not dive into
    the different components in this post, we encourage you to learn about them from
    the [PyTorch documentation](https://pytorch.org/get-started/pytorch-2.0/#technology-overview),
    from the [2022 PyTorch conference](https://www.youtube.com/watch?v=vbtGZL7IrAw&ab_channel=PyTorch),
    or from [this helpful hands-on TDS post](/how-pytorch-2-0-accelerates-deep-learning-with-operator-fusion-and-cpu-gpu-code-generation-35132a85bd26).
    Often times, a good understanding of what is happening behind the scenes can help
    you figure out why your model is not compiling and what you can do to fix it.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在torch编译中，存在许多创新技术，包括[TorchDynamo](https://github.com/pytorch/torchdynamo/tree/0b8aaf340dad4777a080ef24bf09623f1aa6f3dd)、[FX
    Graph](https://pytorch.org/docs/stable/fx.html)、[TorchInductor](https://dev-discuss.pytorch.org/t/torchinductor-a-pytorch-native-compiler-with-define-by-run-ir-and-symbolic-shapes/747)、[Triton](https://github.com/openai/triton)等。虽然我们在此不会深入探讨这些不同的组件，但我们鼓励你从[PyTorch文档](https://pytorch.org/get-started/pytorch-2.0/#technology-overview)、[2022年PyTorch大会](https://www.youtube.com/watch?v=vbtGZL7IrAw&ab_channel=PyTorch)或[这篇有用的TDS帖子](/how-pytorch-2-0-accelerates-deep-learning-with-operator-fusion-and-cpu-gpu-code-generation-35132a85bd26)中了解它们。通常，对幕后发生的事情有一个好的理解可以帮助你弄清楚为什么模型没有编译成功以及如何解决这个问题。
- en: This post should not — in any way — be viewed as a replacement for the official
    PyTorch documentation (e.g., [here](https://pytorch.org/get-started/pytorch-2.0/)).
    This post should also not be viewed as an endorsement for PyTorch over TensorFlow
    (or other ML training framework), for compile mode over eager mode, or for any
    other tool, library, or platform we should mention. I have found that all frameworks
    have their strengths and weaknesses. I do not have a strong preference or passion
    for any particular one. My passions lie in solving interesting technical challenges
    — the harder the better — regardless of the platform or framework upon which they
    reside. You could say that I am framework agnostic. All the same, allow me to
    indulge in two completely unimportant observations on how the PyTorch and TensorFlow
    libraries have evolved over time. Feel free to skip ahead to get back to the real
    stuff.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 本文绝不应被视为官方PyTorch文档的替代品（例如，[这里](https://pytorch.org/get-started/pytorch-2.0/)）。本文也不应被视为对PyTorch相对于TensorFlow（或其他ML训练框架）、编译模式相对于急切模式，或任何我们提到的工具、库或平台的认可。我发现所有框架都有其优缺点。我对任何特定框架没有强烈的偏好或热情。我的热情在于解决有趣的技术挑战——挑战越难越好——无论它们存在于何种平台或框架上。你可以说我对框架是中立的。尽管如此，请允许我对PyTorch和TensorFlow库如何随时间演变进行两个完全无关紧要的观察。可以跳过这些观察，直接回到正题。
- en: Two Completely Unimportant Observations on the TensorFlow vs. PyTorch Wars
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: TensorFlow与PyTorch战争的两个完全无关紧要的观察
- en: '**Observation 1**: In the olden days, when life was simple, there was a clear
    distinction between PyTorch and TensorFlow. PyTorch used eager execution mode,
    TensorFlow used graph mode, and everyone was happy because we all knew what we
    were fighting about. But then came TensorFlow 2 that introduced eager execution
    as the default execution mode and TensorFlow became a little bit more like PyTorch.
    And now PyTorch has come along, introduced its own graph compilation solution
    and become a little bit more like TensorFlow. The TensorFlow vs. PyTorch wars
    continue, but the differences between the two are slowly disappearing. See [this
    tweet](https://twitter.com/cHHillee/status/1601371638913638402?lang=en) for one
    commentary on the PyTorch evolution that I found interesting.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '**观察1**：在过去，当生活很简单时，PyTorch和TensorFlow之间有明显的区别。PyTorch使用急切执行模式，TensorFlow使用图模式，大家都很满意，因为我们都知道自己在争论什么。但后来出现了TensorFlow
    2，它将急切执行作为默认执行模式，TensorFlow变得有点像PyTorch。现在，PyTorch也推出了自己的图编译解决方案，变得有点像TensorFlow。TensorFlow与PyTorch的战争依然继续，但两者之间的差异正在慢慢消失。请参见[这条推文](https://twitter.com/cHHillee/status/1601371638913638402?lang=en)，其中对PyTorch演变的评论我觉得很有趣。'
- en: '**Observation 2**: AI development is a trendy business. Not unlike the fashion
    industry, the popular AI models, model architectures, learning algorithms, **training
    frameworks**, etc., change from season to season. Not unlike the fashion industry,
    AI has its own publications and conventions during which you can keep up with
    the latest trends. Until a few years ago, most of the models we worked on were
    written in TensorFlow. And people were unhappy. Their two primary complaints were
    that the high-level model.fit API limited their development flexibility and that
    graph mode made it impossible for them to debug. “We have to move to PyTorch”,
    they said, “where we can build our models any way we want and debug them easily".
    Fast forward a few years and the same folks are now saying “we have to adapt PyTorch
    Lightening (or some other high-level API) and we must speed up our training with
    torch.compile”. Just to be clear... I’m not judging. All I’m saying is that maybe
    we should be a bit more self-aware.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: Back to the Real Stuff
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The rest of the post is organized as a collection of tips for getting started
    with the PyTorch 2 compile API as well as some of the potential issues you might
    face. Depending on the specific details of your project, adapting your model to
    PyTorch’s graph mode may require a non-trivial effort. Our hope is that this post
    will help you better assess this effort and decide on the best way to take this
    step.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: Installing PyTorch 2
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: From the [PyTorch installation documentation](https://pytorch.org/), it would
    seem that installing PyTorch 2 is no different than installing any other PyTorch
    version. In practice there are some issues you may encounter. For one, PyTorch
    2.0 appears (as of the time of this writing) to require Python version 3.8 or
    higher (see [here](https://pytorch.org/blog/pytorch-2.0-release/#deprecation-of-cuda-116-and-python-37-support-for-pytorch-20)).
    Hopefully, you are already up to date with one of the latest Python versions and
    this will not pose a problem for you, but in the unlikely (and unfortunate) case
    that you are not, this might be one more motivation for you to upgrade. Furthermore,
    PyTorch 2 contains package dependencies (most notably [pytorch-triton](https://pypi.org/project/pytorch-triton/))
    that did not exist in previous versions and may introduce new conflicts. To add
    to that, even if you succeed in building a PyTorch 2 environment, you might find
    that calling torch.compile results in a crushing and wholly unexplained [segmentation
    fault](https://discuss.pytorch.org/t/segmentation-fault-core-dumped-with-torch-compile/167835).
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: One way to save yourself a lot of trouble is to take a pre-built and pre-validated
    PyTorch 2.0 Docker image. In the examples below, we will use an official [AWS
    Deep Learning Container](https://github.com/aws/deep-learning-containers) with
    PyTorch 2.0\. Specifically, we will use the **763104351884.dkr.ecr.us-east-1.amazonaws.com/pytorch-training:2.0.0-gpu-py310-cu118-ubuntu20.04-sagemaker**
    image designed for training on a GPU instance in [Amazon SageMaker](https://aws.amazon.com/pm/sagemaker/?trk=ps_a134p000007BxdvAAC&trkCampaign=acq_paid_search_brand&sc_channel=PS&sc_campaign=acquisition_IL&sc_publisher=Google&sc_category=Machine+Learning&sc_country=IL&sc_geo=EMEA&sc_outcome=acq&sc_detail=amazon+sagemaker&sc_content=Sagemaker_e&sc_matchtype=e&sc_segment=532435490322&sc_medium=ACQ-P%7CPS-GO%7CBrand%7CDesktop%7CSU%7CMachine+Learning%7CSagemaker%7CIL%7CEN%7CText&s_kwcid=AL%214422%213%21532435490322%21e%21%21g%21%21amazon+sagemaker&ef_id=Cj0KCQiAhMOMBhDhARIsAPVml-HxIwfeABmnxXbZ9ia_5DV_TckDGpMSH2mFhSpu8jrCgntII8hcHB4aAuhfEALw_wcB%3AG%3As)
    with Python 3.10 and PyTorch 2.0.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 节省麻烦的一种方法是使用一个预构建并经过验证的 PyTorch 2.0 Docker 镜像。在下面的示例中，我们将使用一个官方的 [AWS Deep Learning
    Container](https://github.com/aws/deep-learning-containers) 镜像，其中包含 PyTorch 2.0。具体来说，我们将使用**763104351884.dkr.ecr.us-east-1.amazonaws.com/pytorch-training:2.0.0-gpu-py310-cu118-ubuntu20.04-sagemaker**
    镜像，该镜像设计用于在 [Amazon SageMaker](https://aws.amazon.com/pm/sagemaker/?trk=ps_a134p000007BxdvAAC&trkCampaign=acq_paid_search_brand&sc_channel=PS&sc_campaign=acquisition_IL&sc_publisher=Google&sc_category=Machine+Learning&sc_country=IL&sc_geo=EMEA&sc_outcome=acq&sc_detail=amazon+sagemaker&sc_content=Sagemaker_e&sc_matchtype=e&sc_segment=532435490322&sc_medium=ACQ-P%7CPS-GO%7CBrand%7CDesktop%7CSU%7CMachine+Learning%7CSagemaker%7CIL%7CEN%7CText&s_kwcid=AL%214422%213%21532435490322%21e%21%21g%21%21amazon+sagemaker&ef_id=Cj0KCQiAhMOMBhDhARIsAPVml-HxIwfeABmnxXbZ9ia_5DV_TckDGpMSH2mFhSpu8jrCgntII8hcHB4aAuhfEALw_wcB%3AG%3As)
    上的 GPU 实例进行训练，使用 Python 3.10 和 PyTorch 2.0。
- en: Backward Compatibility
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 向后兼容性
- en: One of the nice things about PyTorch 2 is that it is fully backward compatible.
    Thus, even if you choose to stick with eager execution mode and not use torch.compile
    at this time, you are still highly encouraged to upgrade to PyTorch 2.0 and benefit
    from the other [new features and enhancements](https://pytorch.org/blog/pytorch-2.0-release/#stable-features).
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch 2 的一个好处是它完全向后兼容。因此，即使你选择继续使用 eager 执行模式而不使用 torch.compile，你仍然被强烈鼓励升级到
    PyTorch 2.0 并从其他 [新功能和增强](https://pytorch.org/blog/pytorch-2.0-release/#stable-features)
    中受益。
- en: Toy Example
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 玩具示例
- en: Let’s jump right in with a toy example of an image classification model. In
    the following code block we build a basic [Vision Transformer](https://en.wikipedia.org/wiki/Vision_transformer)
    (ViT) model using the [timm](https://pypi.org/project/timm/) Python package (version
    0.6.12) and train it on a fake dataset for 500 steps. We define the *use_compile*
    flag to control whether to perform model compilation (torch.compile) and the *use_amp*
    to control whether to run using [Automatic Mixed Precision (AMP)](https://pytorch.org/docs/stable/amp.html)
    or full precision (FP).
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从一个图像分类模型的玩具示例开始。在下面的代码块中，我们使用 [timm](https://pypi.org/project/timm/) Python
    包（版本 0.6.12）构建一个基本的 [Vision Transformer](https://en.wikipedia.org/wiki/Vision_transformer)
    (ViT) 模型，并在一个假数据集上训练 500 步。我们定义 *use_compile* 标志以控制是否进行模型编译（torch.compile），并定义
    *use_amp* 以控制是否使用 [自动混合精度 (AMP)](https://pytorch.org/docs/stable/amp.html) 还是全精度
    (FP)。
- en: '[PRE0]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: In the table below we demonstrate the comparative performance results when running
    the training script on an [*ml.g5.xlarge*](https://aws.amazon.com/ec2/instance-types/g5/)
    instance type using [Amazon SageMaker](https://aws.amazon.com/pm/sagemaker/?trk=ps_a134p000007BxdvAAC&trkCampaign=acq_paid_search_brand&sc_channel=PS&sc_campaign=acquisition_IL&sc_publisher=Google&sc_category=Machine+Learning&sc_country=IL&sc_geo=EMEA&sc_outcome=acq&sc_detail=amazon+sagemaker&sc_content=Sagemaker_e&sc_matchtype=e&sc_segment=532435490322&sc_medium=ACQ-P%7CPS-GO%7CBrand%7CDesktop%7CSU%7CMachine+Learning%7CSagemaker%7CIL%7CEN%7CText&s_kwcid=AL%214422%213%21532435490322%21e%21%21g%21%21amazon+sagemaker&ef_id=Cj0KCQiAhMOMBhDhARIsAPVml-HxIwfeABmnxXbZ9ia_5DV_TckDGpMSH2mFhSpu8jrCgntII8hcHB4aAuhfEALw_wcB%3AG%3As).
    The impact of model compilation will differ from platform to platform (e.g., see
    [here](https://pytorch.org/get-started/pytorch-2.0/#pytorch-2x-faster-more-pythonic-and-as-dynamic-as-ever)).
    Generally speaking the speed-up will be higher on more modern server-class GPUs.
    Keep in mind that these are just examples of the types of results that you might
    see. The actual results will be highly dependent on the specific details of your
    project.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在下表中，我们展示了在使用[*ml.g5.xlarge*](https://aws.amazon.com/ec2/instance-types/g5/)实例类型和[Amazon
    SageMaker](https://aws.amazon.com/pm/sagemaker/?trk=ps_a134p000007BxdvAAC&trkCampaign=acq_paid_search_brand&sc_channel=PS&sc_campaign=acquisition_IL&sc_publisher=Google&sc_category=Machine+Learning&sc_country=IL&sc_geo=EMEA&sc_outcome=acq&sc_detail=amazon+sagemaker&sc_content=Sagemaker_e&sc_matchtype=e&sc_segment=532435490322&sc_medium=ACQ-P%7CPS-GO%7CBrand%7CDesktop%7CSU%7CMachine+Learning%7CSagemaker%7CIL%7CEN%7CText&s_kwcid=AL%214422%213%21532435490322%21e%21%21g%21%21amazon+sagemaker&ef_id=Cj0KCQiAhMOMBhDhARIsAPVml-HxIwfeABmnxXbZ9ia_5DV_TckDGpMSH2mFhSpu8jrCgntII8hcHB4aAuhfEALw_wcB%3AG%3As)
    上运行训练脚本时的性能比较结果。模型编译的影响会因平台而异（例如，参见[这里](https://pytorch.org/get-started/pytorch-2.0/#pytorch-2x-faster-more-pythonic-and-as-dynamic-as-ever)）。一般而言，现代服务器级
    GPU 上的加速效果会更高。请记住，这些仅是您可能看到的结果类型示例。实际结果将高度依赖于项目的具体细节。
- en: '![](../Images/3f44b9117b23239c20b8e4695af3b66e.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3f44b9117b23239c20b8e4695af3b66e.png)'
- en: Performance Results (By Author)
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 性能结果（按作者）
- en: We can see that the performance boost from model compilation is far more pronounced
    when using [AMP](https://pytorch.org/docs/stable/amp.html) (28.6%) than when using
    FP (4.5%). This is a well-known discrepancy (e.g., see [here](https://pytorch.org/get-started/pytorch-2.0/#pytorch-2x-faster-more-pythonic-and-as-dynamic-as-ever)).
    If you don’t already train with AMP, you might find that the most significant
    performance gain can be achieved by transitioning from FP to AMP. We can also
    see that in the case of our model, the performance boost came with a very slight
    increase in GPU memory utilization.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，使用[AMP](https://pytorch.org/docs/stable/amp.html)（28.6%）相较于使用 FP（4.5%），模型编译带来的性能提升显著。这是一个众所周知的差异（例如，参见[这里](https://pytorch.org/get-started/pytorch-2.0/#pytorch-2x-faster-more-pythonic-and-as-dynamic-as-ever)）。如果您尚未使用
    AMP 进行训练，您可能会发现从 FP 到 AMP 的过渡可以实现最显著的性能增益。我们还可以看到，在我们的模型案例中，性能提升伴随着 GPU 内存利用的轻微增加。
- en: Note that the comparative performance might change when scaling to multiple
    GPUs due to the way in which [distributed](https://pytorch.org/tutorials/beginner/dist_overview.html)
    training is implemented on compiled graphs. See [here](https://pytorch.org/get-started/pytorch-2.0/#distributed)
    for more details.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，由于在编译图上实现分布式训练的方式，当扩展到多个 GPU 时，性能对比可能会发生变化。有关更多详细信息，请参见[这里](https://pytorch.org/tutorials/beginner/dist_overview.html)。
- en: Advanced Compilation Options
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 高级编译选项
- en: The torch.compile API includes a number of options for controlling the graph
    creation. These enable you to fine-tune the compilation for your specific model
    and potentially boost performance even more. The code block below contains the
    function signature (from this [source](https://pytorch.org/docs/stable/_modules/torch.html#compile)).
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '`torch.compile` API 包含多个选项，用于控制图的创建。这些选项使您能够针对您的特定模型进行精细调整编译，并可能进一步提升性能。下面的代码块包含了函数签名（来自这个[source](https://pytorch.org/docs/stable/_modules/torch.html#compile)）。'
- en: '[PRE1]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '**Compilation Mode**: The compilation mode allows you to choose between minimizing
    the overhead required by compilation (“reduce-overhead”) and maximizing potential
    performance boost (“max-autotune”). See [here](https://pytorch.org/get-started/pytorch-2.0/#modes)
    for more details.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '**编译模式**：编译模式允许您选择减少编译所需开销（“reduce-overhead”）和最大化潜在性能提升（“max-autotune”）之间的权衡。有关更多详细信息，请参见[这里](https://pytorch.org/get-started/pytorch-2.0/#modes)。'
- en: In the table below we compare the results of compiling the ViT model above with
    different compilation modes.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在下表中，我们比较了上述 ViT 模型在不同编译模式下的编译结果。
- en: '![](../Images/fdbbadd482b933054be20692b2811160.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fdbbadd482b933054be20692b2811160.png)'
- en: Performance Results (By Author)
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 性能结果（作者提供）
- en: We can see that the compilation modes behave pretty much as advertised, with
    “reduce-overhead” reducing the compilation time at the cost of extra memory utilization
    and “max-autotune” resulting in maximum performance at the expense of high overhead
    in compilation time.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到编译模式的表现基本符合宣传，"reduce-overhead" 在额外内存利用的代价下减少了编译时间，而 "max-autotune" 在编译时间开销高的情况下实现了最大性能。
- en: '**Compiler Backend**: The compile API allows you determine which backend to
    use to convert the intermediate representation (IR) computation graph (the [*FX
    graph*](https://pytorch.org/docs/stable/fx.html#torch.fx.Graph)) into low-level
    kernel operations. This option is useful for [debugging graph compilation issues](https://pytorch.org/docs/stable/dynamo/troubleshooting.html#diagnosing-runtime-errors)
    and for gaining a better understanding for the torch.compile internals (as demonstrated
    in [this cool example](https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html#torchdynamo-and-fx-graphs)).
    In most cases (as of the time of this writing) the default, [TorchInductor](https://pytorch.org/get-started/pytorch-2.0/#torchinductor-fast-codegen-using-a-define-by-run-ir)
    backend, appears to provide the best training performance results. See [here](https://github.com/pytorch/torchdynamo/blob/0b8aaf340dad4777a080ef24bf09623f1aa6f3dd/README.md#existing-backends)
    for the current list of existing backends, or run the code below to see the ones
    that are supported in your environment. And if you really want, you can also [add
    your own backend](https://github.com/pytorch/torchdynamo/blob/0b8aaf340dad4777a080ef24bf09623f1aa6f3dd/README.md#adding-backends)
    :).'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '**编译器后端**：编译 API 允许您确定使用哪个后端将中间表示（IR）计算图（[ *FX 图*](https://pytorch.org/docs/stable/fx.html#torch.fx.Graph)）转换为低级内核操作。这个选项对于[调试图编译问题](https://pytorch.org/docs/stable/dynamo/troubleshooting.html#diagnosing-runtime-errors)和更好地了解
    torch.compile 的内部机制（如在[这个有趣的示例](https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html#torchdynamo-and-fx-graphs)中所示）非常有用。在大多数情况下（截至撰写本文时），默认的[
    TorchInductor](https://pytorch.org/get-started/pytorch-2.0/#torchinductor-fast-codegen-using-a-define-by-run-ir)
    后端似乎提供了最佳的训练性能结果。请参见[这里](https://github.com/pytorch/torchdynamo/blob/0b8aaf340dad4777a080ef24bf09623f1aa6f3dd/README.md#existing-backends)获取当前现有后端的列表，或者运行下面的代码查看您的环境中支持的后端。如果您愿意，也可以[添加自己的后端](https://github.com/pytorch/torchdynamo/blob/0b8aaf340dad4777a080ef24bf09623f1aa6f3dd/README.md#adding-backends)
    :).'
- en: '[PRE2]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: For example, by modifying the code above to use the [*nvprims-nvfuser*](https://pytorch.org/blog/introducing-nvfuser-a-deep-learning-compiler-for-pytorch/)backend
    we get an 13% performance boost over eager mode (compared to the 28.6% boost with
    the default backend).
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，通过修改上述代码以使用[ *nvprims-nvfuser*](https://pytorch.org/blog/introducing-nvfuser-a-deep-learning-compiler-for-pytorch/)后端，我们比急切模式获得了
    13% 的性能提升（相比于默认后端的 28.6% 提升）。
- en: '**Force a Single Graph**: The *fullgraph* flag is an extremely useful control
    for ensuring that you do not have any undesired *graph-breaks*. More on this topic
    below.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '**强制单一图**：*fullgraph*标志是确保您没有任何不希望的*图断裂*的极其有用的控制。有关更多信息，请参见下文。'
- en: '**Dynamic Shape Flag**: As of the time of this writing, compilation support
    for tensors that have dynamic shapes is somewhat limited. A common byproduct of
    compiling a model with dynamic shapes is [excessive recompilation](https://pytorch.org/docs/stable/dynamo/troubleshooting.html#excessive-recompilation)
    which can significantly increase overhead and slow your training down considerably.
    If your model *does* include dynamic shapes, setting the *dynamic* flag to *True*
    will result in better performance and, in particular, reduce the number of recompilations.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '**动态形状标志**：截至撰写本文时，对具有动态形状的张量的编译支持仍然有限。编译具有动态形状的模型的常见副作用是[过度重新编译](https://pytorch.org/docs/stable/dynamo/troubleshooting.html#excessive-recompilation)，这可能显著增加开销并大幅减慢训练速度。如果您的模型*确实*包含动态形状，将*dynamic*标志设置为*True*将会带来更好的性能，特别是减少重新编译的次数。'
- en: Performance Profiling
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 性能分析
- en: We have written extensively (e.g., [here](/cloud-ml-performance-checklist-caa51e798002))
    about the importance of profiling the training performance as a means to accelerating
    training speed and reducing cost. One of the key tools we use for profiling performance
    of PyTorch models is the [PyTorch Profiler](https://pytorch.org/tutorials/recipes/recipes/profiler_recipe.html).
    The PyTorch profiler allows us to assess and analyze the manner in which graph
    compilation optimizes the training step. In the code block below we wrap our training
    loop with a [torch.profiler](https://pytorch.org/docs/stable/profiler.html) and
    generate the results for [TensorBoard](http://orch.profiler.tensorboard_trace_handler).
    We save the output in the [*SM_MODEL_DIR*](https://github.com/aws/sagemaker-training-toolkit/blob/master/ENVIRONMENT_VARIABLES.md#sm_model_dir)
    which is automatically uploaded to persistent storage at the end of the training
    job.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经广泛讨论了（例如，[这里](/cloud-ml-performance-checklist-caa51e798002)）对训练性能进行分析的重要性，作为加速训练速度和降低成本的一种手段。我们用来分析
    PyTorch 模型性能的关键工具之一是 [PyTorch Profiler](https://pytorch.org/tutorials/recipes/recipes/profiler_recipe.html)。PyTorch
    Profiler 允许我们评估和分析图编译如何优化训练步骤。在下面的代码块中，我们用 [torch.profiler](https://pytorch.org/docs/stable/profiler.html)
    包装了我们的训练循环，并为 [TensorBoard](http://orch.profiler.tensorboard_trace_handler) 生成了结果。我们将输出保存在
    [*SM_MODEL_DIR*](https://github.com/aws/sagemaker-training-toolkit/blob/master/ENVIRONMENT_VARIABLES.md#sm_model_dir)
    中，该目录会在训练任务结束时自动上传到持久存储。
- en: '[PRE3]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The image below was captured from the *GPU Kernel* view of the TensorBoard PyTorch
    Profiler tab. It provides details of the kernels that are run on the GPU during
    the training step of the compiled model trial from above.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 下图截取自 TensorBoard PyTorch Profiler 标签的 *GPU 内核* 视图。它提供了在编译模型试验的训练步骤中，运行在 GPU
    上的内核的详细信息。
- en: '![](../Images/3928d90159e9c441ef4e0adb7d6a5bca.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3928d90159e9c441ef4e0adb7d6a5bca.png)'
- en: Screen Capture of Kernel View from TensorBoard PyTorch Profiler Tab (By Author)
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: TensorBoard PyTorch Profiler 标签下的内核视图截图（作者提供）
- en: By comparing these charts to the ones from the eager execution run, we are able
    to see that graph compilation increases the utilization of the GPU’s [Tensor Cores](https://www.nvidia.com/en-us/data-center/tensor-cores/)
    (from 51% to 60%) and that it introduces the use of GPU kernels developed using
    [Triton](https://openai.com/research/triton).
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将这些图表与急切执行运行的图表进行比较，我们可以看到图编译增加了 GPU 的 [Tensor Cores](https://www.nvidia.com/en-us/data-center/tensor-cores/)
    的使用率（从 51% 增加到 60%），并且引入了使用 [Triton](https://openai.com/research/triton) 开发的 GPU
    内核。
- en: Diagnosing Model Compilation Issues
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 诊断模型编译问题
- en: PyTorch compilation is still under active development (currently in beta) and
    it is not at all unlikely that you will encounter issues when compiling your model.
    If you are lucky, you will get an informative error and will have an easy (and
    reasonable) way to work around it. If you are less lucky, you may have to work
    a bit harder to find the root of the issue, and/or may come to the conclusion
    that, at its current maturity level, model compilation does not address your needs.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch 编译仍在积极开发中（目前处于测试阶段），你在编译模型时遇到问题是完全有可能的。如果你运气好，你会得到一个有用的错误信息，并有一种简单（且合理）的解决办法。如果你运气不好，你可能需要更加努力地找出问题的根源，和/或得出结论：在目前的成熟度水平下，模型编译无法满足你的需求。
- en: The primary resource for addressing compilation issues is the [TorchDynamo troubleshooting
    page](https://pytorch.org/docs/stable/dynamo/troubleshooting.html) which includes
    a list of debugging tools and offers a step-by-step guide for [diagnosing errors](https://pytorch.org/docs/stable/dynamo/troubleshooting.html#diagnosing-runtime-errors).
    Unfortunately, as of the time of this writing, the tools and techniques appear
    to be targeted more towards PyTorch developers than PyTorch users. They can be
    helpful in root-causing compilation issues, providing some hints as to how you
    might be able to work around them, and/or reporting them to PyTorch. However,
    you might find that they don’t help in actually resolving your issues.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 解决编译问题的主要资源是 [TorchDynamo 故障排除页面](https://pytorch.org/docs/stable/dynamo/troubleshooting.html)，其中包含了调试工具的列表，并提供了
    [诊断错误](https://pytorch.org/docs/stable/dynamo/troubleshooting.html#diagnosing-runtime-errors)
    的逐步指南。不幸的是，撰写本文时，这些工具和技术似乎更多地面向 PyTorch 开发者，而非 PyTorch 用户。它们可以帮助找出编译问题的根本原因，提供一些关于如何绕过这些问题的提示，和/或将问题报告给
    PyTorch。然而，你可能会发现它们在实际解决问题上并没有帮助。
- en: 'In the code block below we show a simple [distributed](https://pytorch.org/tutorials/beginner/dist_overview.html)
    model that includes a call to [torch.distributed.all_reduce](https://pytorch.org/docs/stable/distributed.html).
    This model runs as expected in eager mode, but fails (as of the time of this writing)
    with an “attribute error” during graph compilation (*torch.classes.c10d.ProcessGroup
    does not have a field with name ‘shape’*). By increasing the log level to *INFO*
    we find that the error is in “step #3” of the calculation, the TorchInductor.
    We can confirm this by verifying that compilation succeeds with the “eager” and
    “aot_eager” backends. Finally, we can create a minimal code sample that reproduces
    the failure using the [PyTorch Minifier](https://pytorch.org/docs/stable/dynamo/troubleshooting.html#minifying-torchinductor-errors).'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Sadly, in our example, running the generated minifier_launcher.py script results
    in a different attribute error (‘Repro’ object has no attribute ‘_tensor_constant0’),
    and despite having enjoyed the whole experience, the documented debugging steps
    did not help all that much in solving the compilation issue we demonstrated.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: 'Obviously, we hope that you do not run into any compilation issues. In case
    you do, know that: 1\. you are not alone :), and 2\. although they are likely
    to be different than the one demonstrated here, following the same steps described
    in the [troubleshooting guide](https://pytorch.org/docs/stable/dynamo/troubleshooting.html)
    may give some indication as to their source.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: Common Graph Breaks
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One of the most touted advantages of Pytorch eager mode is the ability to interleave
    pure Pythonic code with your PyTorch operations. Unfortunately, this freedom (as
    of the time of this writing) is significantly restricted when using torch.compile.
    The reason for this is that certain Pythonic operations cause TorchDynamo to split
    the computation graph into multiple components, thus hindering the potential for
    performance gains. Your goal should be to minimize such [*graph breaks*](https://pytorch.org/docs/stable/dynamo/troubleshooting.html#graph-breaks)
    to the extent possible. As a best practice, you might consider compiling your
    model with the [*fullgraph*](https://pytorch.org/docs/stable/generated/torch.compile.html#torch-compile)flag
    when you are porting your model to PyTorch 2\. Not only will this encourage you
    to remove any code that causes graph breaks, but it will also teach you how to
    best adapt your PyTorch development habits for using graph mode. However, note
    that you will have to disable this flag to run [distributed](https://pytorch.org/tutorials/beginner/dist_overview.html)
    code as the current way that communication between GPUs is implemented requires
    graph breaks (e.g., see [here](https://pytorch.org/get-started/pytorch-2.0/#distributed)).
    Alternatively, you can use the *torch._dynamo.explain* utility to analyze graph
    breaks, as described [here](https://pytorch.org/docs/stable/dynamo/troubleshooting.html#identifying-the-cause-of-a-graph-break).
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: The following code block demonstrates a simple model with four potential graph
    breaks in its forward pass (as of the time of this writing). It is not uncommon
    to see any one of these kinds of operations in a typical PyTorch model.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: It is important to emphasize that graph breaks do *not* fail the compilation
    (unless the *fullgraph* flag is set). Thus, it is perfectly possible that your
    model is compiling and running but actually contains multiple graph breaks that
    are slowing it down.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: Troubleshooting Training Issues
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: While succeeding in compiling your model is a worthy achievement, it is not
    a guarantee that training will succeed. As noted above, the low-level kernels
    that run on the GPU will differ between eager mode and graph mode. Consequently,
    certain high-level operations may exhibit different behaviors. In particular,
    you might find that operations that run in eager mode fail in graph mode (e.g.,
    [this torch.argmin failure](https://github.com/pytorch/pytorch/issues/99879) that
    we encountered). Alternatively, you might find that numerical differences in computation
    have an impact on your training.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: To make matters worse, debugging in graph mode is much more difficult than in
    eager mode. In eager mode each line of code is executed independently, allowing
    us to place a breakpoint at any point in our code and evaluate the current tensor
    values. In graph mode, on the other hand, the model defined by our code undergoes
    multiple transitions before being processed and, consequently, your breakpoint
    may not be triggered.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: In the past, [we expanded on the difficulties of debugging](/debugging-in-tensorflow-392b193d0b8)
    in graph mode in TensorFlow and proposed a few ways to address them. Here is a
    two-step approach you could try when you encounter an issue. First, revert back
    to eager mode where debugging is less difficult and pray that the issue reproduces.
    If it does not, evaluate intermediate tensors of interest in your compiled computation
    graph by purposely inserting graph breaks in your model. You can do this by either
    explicitly breaking your model into two (or more) portions and applying torch.compile
    to each portion separately, or by generating a graph break by inserting a *print*,
    and/or a [Tensor.numpy](https://pytorch.org/docs/stable/generated/torch.Tensor.numpy.html)
    invocation as described in the previous section. Depending on how you do this,
    you may even succeed in triggering breakpoints in your code. Still, keep in mind
    that breaking up your graph in this manner can modify the sequence of low-level
    operations so it may not accurately reproduce the fully compiled graph execution.
    But it certainly gives you more flexibility in trying to get to the bottom of
    your issue.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: See the [accuracy-debugging](https://pytorch.org/docs/stable/dynamo/troubleshooting.html#accuracy-debugging)
    portion of the [troubleshooting guide](https://pytorch.org/docs/stable/dynamo/troubleshooting.html)
    if you encounter discrepancies between compile mode and eager mode that are unexpected.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: Including the Loss Function in the Graph
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As we demonstrated in the examples above, graph execution mode is enabled by
    wrapping a PyTorch model (or function) with a torch.compile invocation. You may
    have observed that the loss function is not part of the compilation call and,
    as a result, not part of the generated graph. In many cases, including the ones
    that we have demonstrated, the loss function is a relatively small portion of
    the training step and running it eagerly will not incur much overhead. However,
    if you have a particularly heavy loss you may be able to further boost performance
    by including it in the compiled computation graph. For example, in the code block
    below, we define a loss function for (naively) performing [model distillation](https://en.wikipedia.org/wiki/Knowledge_distillation)
    from a large ViT model (with 24 ViT blocks) to a smaller ViT model (with 12 ViT
    blocks).
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Our implementation includes a loss function that calls the large model on each
    input batch. This is a much more compute-heavy loss function than the [CrossEntropyLoss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html)
    above and running it *eagerly* would not be ideal.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的实现包括一个在每个输入批次上调用大模型的损失函数。这是一个比上面提到的 [CrossEntropyLoss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html)
    更加计算密集的损失函数，*急切* 运行它并不理想。
- en: 'We describe two ways to solve this. The first is to simply wrap the loss function
    in a torch.compile invocation of its own, as shown here:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 我们描述了解决这个问题的两种方法。第一种方法是将损失函数简单地包装在一个 torch.compile 调用中，如下所示：
- en: '[PRE7]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The disadvantage of this option is that the compiled graph of the loss function
    is disjoint from the compiled graph of the model. The second option compiles the
    model and loss together by creating a wrapper model that includes both and returns
    the resultant loss as its output. This option is demonstrated in the code block
    below:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 这种选项的缺点是，损失函数的编译图与模型的编译图不相交。第二种选项通过创建一个包含两者的包装模型并返回结果损失作为输出，来将模型和损失函数一起编译。此选项在下面的代码块中演示：
- en: '[PRE8]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The disadvantage of this approach is that the internal model will need to be
    extracted from the wrapper model when the time comes to run the model in *inference*
    mode.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法的缺点是，当需要以 *推理* 模式运行模型时，内部模型需要从包装模型中提取出来。
- en: In our case, both options result in roughly the same 8% performance boost, demonstrating
    the importance of this kind of optimization. When the loss is run eagerly, the
    total step time is 0.37 seconds, and when the loss is compiled, the total step
    time is 0.34 seconds.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的案例中，两种选项都带来了大约 8% 的性能提升，展示了这种优化的重要性。当损失函数被急切运行时，总步长时间为 0.37 秒，而当损失函数被编译时，总步长时间为
    0.34 秒。
- en: Dynamic Shapes
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 动态形状
- en: As reported in the [documentation](https://pytorch.org/get-started/pytorch-2.0/#dynamic-shapes),
    compilation support for models with dynamic shapes is limited (as of the time
    of this writing). Depending on the details of the dynamism, dynamic models could
    incur significant performance overhead, either by introducing graph breaks and/or
    triggering an excessive number of [graph recompilations](https://pytorch.org/docs/stable/dynamo/troubleshooting.html#excessive-recompilation).
    Graph recompilations occur when one of the assumptions (referred to as [guards](https://github.com/pytorch/torchdynamo/tree/0b8aaf340dad4777a080ef24bf09623f1aa6f3dd#guards))
    about the model that were made during the original compilation is violated.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 根据 [文档](https://pytorch.org/get-started/pytorch-2.0/#dynamic-shapes) 的报告，动态形状模型的编译支持有限（截至本文撰写时）。根据动态性的细节，动态模型可能会带来显著的性能开销，可能会引入图断裂和/或触发过多的
    [图重编译](https://pytorch.org/docs/stable/dynamo/troubleshooting.html#excessive-recompilation)。图重编译发生在原始编译期间对模型所做的假设（称为
    [guards](https://github.com/pytorch/torchdynamo/tree/0b8aaf340dad4777a080ef24bf09623f1aa6f3dd#guards)）被违反时。
- en: The [torch.compile](https://pytorch.org/docs/stable/generated/torch.compile.html)
    API includes the *dynamic* flag for signaling to the compiler to optimize for
    dynamic shapes. However, as of the time of this writing, the degree to which this
    will help is questionable. If you are trying to compile and optimize a dynamic
    graph and facing issues, you might choose to hold off on this until the level
    of support matures.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '[torch.compile](https://pytorch.org/docs/stable/generated/torch.compile.html)
    API 包含 *动态* 标志，用于指示编译器优化动态形状。然而，截至本文撰写时，这种优化的效果尚不明确。如果你在编译和优化动态图时遇到问题，可能需要等到支持水平成熟后再考虑使用这一功能。'
- en: Summary
  id: totrans-84
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: PyTorch 2.0 compile mode comes with the potential for a considerable boost to
    the speed of training and inference and, consequently, meaningful savings in cost.
    However, the amount of work that your model will require to realize this potential
    can vary greatly. Many public models require nothing more than changing a single
    line of code. Other models, especially ones that include non-standard operations,
    dynamic shapes, and/or a lot of interleaved Python code, might require more considerable
    effort. However, there may be no better time to start adapting your models than
    today, as it appears that compile mode is here to stay.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch 2.0 编译模式具有显著提高训练和推理速度的潜力，因此可以实现显著的成本节约。然而，实现这一潜力所需的工作量可能差异很大。许多公共模型只需更改一行代码即可完成。其他模型，尤其是包含非标准操作、动态形状和/或大量交织的
    Python 代码的模型，可能需要更多的努力。然而，现在可能是开始调整你的模型的最佳时机，因为编译模式似乎会长期存在。
