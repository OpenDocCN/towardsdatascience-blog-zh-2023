# 大型语言模型：BERT — Transformer 的双向编码器表示

> 原文：[https://towardsdatascience.com/bert-3d1bf880386a?source=collection_archive---------0-----------------------#2023-08-30](https://towardsdatascience.com/bert-3d1bf880386a?source=collection_archive---------0-----------------------#2023-08-30)

## 理解 BERT 如何构建最先进的嵌入

[](https://medium.com/@slavahead?source=post_page-----3d1bf880386a--------------------------------)[![Vyacheslav Efimov](../Images/db4b02e75d257063e8e9d3f1f75d9d6d.png)](https://medium.com/@slavahead?source=post_page-----3d1bf880386a--------------------------------)[](https://towardsdatascience.com/?source=post_page-----3d1bf880386a--------------------------------)[![Towards Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----3d1bf880386a--------------------------------) [Vyacheslav Efimov](https://medium.com/@slavahead?source=post_page-----3d1bf880386a--------------------------------)

·

[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fc8a0ca9d85d8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbert-3d1bf880386a&user=Vyacheslav+Efimov&userId=c8a0ca9d85d8&source=post_page-c8a0ca9d85d8----3d1bf880386a---------------------post_header-----------) 发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----3d1bf880386a--------------------------------) · 11 分钟阅读 · 2023 年 8 月 30 日 [](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F3d1bf880386a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbert-3d1bf880386a&user=Vyacheslav+Efimov&userId=c8a0ca9d85d8&source=-----3d1bf880386a---------------------clap_footer-----------)

--

[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F3d1bf880386a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbert-3d1bf880386a&source=-----3d1bf880386a---------------------bookmark_footer-----------)![](../Images/450d6ac933335232d6ae951d5b7a4e0b.png)

# 介绍

2017 年是机器学习的历史性一年，当时**Transformer**模型首次亮相。它在许多基准测试中表现出色，适用于数据科学中的许多问题。由于其高效的架构，后来开发了许多其他基于 Transformer 的模型，这些模型在特定任务上有了更多的专业化。

其中一个这样的模型是 BERT。它主要以能够构建非常准确的文本嵌入而著称，这些嵌入可以表示文本信息并存储长文本序列的语义含义。因此，BERT 嵌入在机器学习中得到了广泛应用。理解 BERT 如何构建文本表示是至关重要的，因为这为处理自然语言处理中的大量任务打开了大门。

在本文中，我们将参考 [原始 BERT 论文](https://arxiv.org/pdf/1810.04805.pdf)，查看 BERT 架构并理解其核心机制。在前几部分中，我们将给出 BERT 的高级概述。之后，我们将逐步深入其内部工作流程及信息在模型中的传递方式。最后，我们将了解如何对 BERT 进行微调，以解决 NLP 中的特定问题。

# 高级概述

**Transformer** 的架构由两个主要部分组成：编码器和解码器。堆叠编码器的目标是为输入构建有意义的嵌入，以保持其主要上下文。最后一个编码器的输出传递给所有解码器的输入，试图生成新的信息。

> **BERT** 是 Transformer 的继承者，继承了其堆叠的双向编码器。BERT 中的大部分架构原则与原始 Transformer 相同。

![](../Images/1de7776be934f0ebf7ede8d9a5864184.png)

Transformer 架构

# BERT 版本

BERT 存在两个主要版本：base 和 large。它们的架构完全相同，只是参数数量不同。总体而言，BERT large 比 BERT base 多了 3.09 倍的参数进行调优。

![](../Images/a1037ce1121042a532a1aff8403e9307.png)

BERT base 和 BERT large 的比较

# 双向表示

从 BERT 名称中的字母“B”来看，重要的是要记住 BERT 是一个 **双向** 模型，这意味着它可以更好地捕捉词汇之间的连接，因为信息是双向传递的（从左到右和从右到左）。显然，这与单向模型相比需要更多的训练资源，但同时也导致更好的预测准确性。

为了更好地理解，我们可以将 BERT 架构与其他流行的 NLP 模型进行比较。

![](../Images/51e3b04c18a481f9f7a95887afc9a581.png)

从 [原始论文](https://arxiv.org/pdf/1810.04805.pdf) 中比较 BERT、OpenAI GPT 和 ElMo 架构。由作者采用。

# 输入标记化

> 注：在官方论文中，作者使用“**sentence**”一词来表示传递给输入的文本。为了统一术语，本文系列中我们将使用“**sequence**”一词。这是为了避免混淆，因为“sentence” 通常指一个由句点分隔的单独短语，而许多其他 NLP 研究论文中“sequence”一词在类似情况下被使用。

在深入了解 BERT 的训练方法之前，有必要了解 BERT 接受数据的格式。对于输入，BERT 接受单个序列或一对序列。每个序列被拆分为标记。此外，两个特殊标记会被传递到输入中：

> 注意。官方论文使用了“**句子**”这个术语，它指的是传递给 BERT 的输入**序列**，该序列实际上可以由多个句子组成。为了简化，我们将遵循这个符号，并在本文中使用相同的术语。

+   ***[CLS]*** — 在第一个序列之前传递，表示其开始。同时，* [CLS] * 也用于训练中的分类目标（在下面的章节中讨论）。

+   ***[SEP]*** — 在序列之间传递，用以表示第一个序列的结束和第二个序列的开始。

传递两个序列使得 BERT 能够处理各种任务，其中输入包含一对序列（例如，问题和答案，假设和前提等）。

# 输入嵌入

在分词之后，为每个标记构建一个嵌入。为了使输入嵌入更具代表性，BERT 为每个标记构建了三种类型的嵌入：

+   **标记嵌入** 捕捉标记的语义意义。

+   **段嵌入**有两个可能的值，表示标记属于哪个序列。

+   **位置嵌入** 包含关于标记在序列中相对位置的信息。

![](../Images/436c75c0a4550006be88a01948fc4e1c.png)

输入处理

这些嵌入被加总，然后结果被传递给 BERT 模型的第一个编码器。

# 输出

每个编码器接受*n*个嵌入作为输入，然后输出相同数量、相同维度的处理后的嵌入。最终，整个 BERT 输出也包含*n*个嵌入，每个嵌入对应其初始的标记。

![](../Images/349ec9a449ceb70406ce5910691d12fa.png)

# 训练

BERT 训练分为两个阶段：

1.  **预训练**。BERT 在未标记的序列对上进行训练，涉及两个预测任务：**掩码语言建模（MLM）** 和 **自然语言推理（NLI）**。对于每对序列，模型会对这两个任务进行预测，并根据损失值进行反向传播来更新权重。

1.  **微调**。BERT 使用预训练的权重进行初始化，然后在标记数据上为特定问题进行优化。

# 预训练

与微调相比，预训练通常需要较长的时间，因为模型是在大量数据上训练的。因此，存在许多在线预训练模型的库，这些模型可以被相对快速地微调以解决特定任务。

我们将详细查看 BERT 在预训练期间解决的两个问题。

## 掩码语言建模

作者建议通过掩盖初始文本中的一定数量的标记来训练 BERT 并预测它们。*这使得 BERT 能够构建出具有弹性的嵌入，可以利用周围的上下文来猜测某个词，从而为遗漏的词构建合适的嵌入*。这个过程的工作方式如下：

1.  在分词后，15% 的标记被随机选择进行掩盖。选择的标记将在迭代结束时进行预测。

1.  选择的标记被以三种方式之一替换：

    *-* 80% 的标记被替换为 *[MASK]* 标记。

    示例*: 我买了一本书 → 我买了一个[MASK]*

    - 10% 的标记被随机标记替代。

    示例: *他在吃水果 → 他在画水果*

    - 10% 的标记保持不变。

    示例: *一栋房子在我附近 → 一栋房子在我附近*

1.  所有标记被传递给 BERT 模型，模型输出每个接收到的输入标记的嵌入。

4\. 对应于步骤 2 中处理的标记的输出嵌入被独立用于预测被掩盖的标记。每个预测的结果是词汇表中所有标记的概率分布。

5\. 交叉熵损失通过将概率分布与真实掩盖标记进行比较来计算。

6\. 模型权重通过反向传播进行更新。

## 自然语言推断

对于这个分类任务，BERT 尝试预测第二个序列是否跟随第一个序列。整个预测仅使用来自 *[CLS]* 标记的最终隐藏状态的嵌入，该标记应包含来自两个序列的聚合信息。

类似于 MLM，使用构建的概率分布（二进制的情况下）来计算模型的损失，并通过反向传播更新模型的权重。

对于自然语言推断（NLI），作者建议选择 50% 的序列对，这些序列在语料库中是紧接着的（*正对*），以及 50% 的序列对，其中序列是从语料库中随机选取的（*负对*）。

![](../Images/49b97c621037edf7eeb72626ce174ad9.png)

BERT 预训练

## 训练细节

根据论文，BERT 在 BooksCorpus（8 亿单词）和英文维基百科（25 亿单词）上进行预训练。为了提取较长的连续文本，作者仅从维基百科中提取阅读段落，忽略表格、标题和列表。

BERT 在大小为 256 的一百万批次上进行训练，这相当于在 33 亿个单词上进行 40 个周期。每个序列包含最多 128（90% 的时间）或 512（10% 的时间）个标记。

根据原始论文，训练参数如下：

+   优化器：Adam（学习率 *l* = 1e-4，权重衰减 L₂ = 0.01，β₁ = 0.9，β₂ = 0.999，ε = 1e-6）。

+   学习率预热在前 10,000 步内进行，然后线性降低。

+   在所有层上使用 Dropout（α = 0.1）层。

+   激活函数：GELU。

+   训练损失是平均 MLM 和平均下一个句子预测似然的总和。

# 微调

> 一旦预训练完成，BERT 可以字面上理解单词的语义，并构建几乎完全表示其意义的嵌入。微调的目标是逐渐调整 BERT 的权重，以解决特定的下游任务。

## 数据格式

由于自注意力机制的鲁棒性，BERT可以轻松地为特定下游任务进行微调。BERT的另一个优势是能够构建*双向*文本表示。这在处理对时提供了更高的发现两个序列之间正确关系的机会。以前的方法包括独立编码两个序列，然后对它们应用双向交叉注意力。BERT统一了这两个阶段。

> 根据具体问题，BERT接受几种输入格式。用BERT解决所有下游任务的框架是相同的：输入一个文本序列，BERT输出一组标记嵌入，然后将这些嵌入送入模型。大多数时候，并不是所有的输出嵌入都会被使用。

让我们看看常见的问题以及通过微调BERT解决这些问题的方法。

**句子对分类**

句子对分类的目标是理解给定序列对之间的关系。常见的任务类型包括：

+   *自然语言推理*：确定第二个序列是否跟随第一个序列。

+   *相似性分析*：找到序列之间的相似程度。

![](../Images/1a1aeb6fad79249756ebcb7b571f565a.png)

句子对分类

对于微调，两个序列都传递给BERT。一般来说，* [CLS] *标记的输出嵌入被用来进行分类任务。根据研究人员的说法，* [CLS] *标记应该包含关于句子关系的主要信息。

当然，也可以使用其他输出嵌入，但在实际应用中通常会被省略。

**问答任务**

*问答*的目标是在文本段落中找到对应于特定问题的答案。大多数时候，答案以两个数字的形式给出：片段的开始和结束标记位置。

![](../Images/f78ff21a106291ce5a36445a3a251b6f.png)

问答任务

对于输入，BERT接收问题和段落，并输出一组对应的嵌入。由于答案包含在段落中，我们只对与段落标记对应的输出嵌入感兴趣。

为了找到段落中答案起始标记的位置，计算每个输出嵌入与一个特殊的可训练向量Tₛₜₐᵣₓ的标量积。在大多数情况下，当模型和向量Tₛₜₐᵣₓ经过相应训练时，标量积应该与相应标记实际上是起始答案标记的可能性成正比。为了规范化标量积，它们会传递到softmax函数，并可以看作是概率。对应于最高概率的标记嵌入被预测为起始答案标记。根据真实的概率分布，计算损失值并进行反向传播。预测结束标记时会使用向量Tₑₙ𝒹进行类似的过程。

**单句分类**

与之前的下游任务相比，区别在于这里只传递单个句子给BERT。此配置解决的典型问题如下：

+   *情感分析*：理解一个句子是否具有积极或消极的态度。

+   *主题分类*：根据句子的内容将句子分类到几个类别之一。

![](../Images/114d1219106d34b8c716f7d7e22f50f9.png)

单句分类

预测工作流程与句子对分类的工作流程相同：*`[CLS]`*标记的输出嵌入被用作分类模型的输入。

**单句标注**

*命名实体识别（NER）*是一个机器学习问题，旨在将序列中的每个标记映射到相应的实体之一。

![](../Images/8191bce69a8570d357c72ff10b20896e.png)

单句标注

为了实现这个目标，通常会计算输入句子的词嵌入。然后，将每个嵌入（除了*`[CLS]`*和*`[SEP]`*）独立传递给一个模型，该模型将每个嵌入映射到给定的NER类别（如果不能映射，则不进行映射）。

# 特征提取

使用最后一个BERT层作为嵌入并不是从输入文本中提取特征的唯一方法。实际上，研究人员完成了几种不同方式的嵌入聚合实验，以解决CoNLL-2003数据集上的NER任务。为了进行实验，他们将提取的嵌入作为输入传递给一个随机初始化的两层768维BiLSTM，然后应用分类层。

嵌入的提取方式（来自BERT基础模型）在下面的图中展示了。如图所示，最有效的方法是连接最后四个BERT隐藏层。

> 根据已完成的实验，重要的是要记住，隐藏层的聚合是一种可能的改进嵌入表示以在各种NLP任务中取得更好结果的方法。

![](../Images/b3d9a0b4565828c42c75f85f18c9f5bd.png)

左侧的图表展示了带有隐藏层的扩展BERT结构。右侧的表格则说明了嵌入的构建方式以及通过应用相应策略所取得的得分。

# 将BERT与其他特征结合

有时我们不仅处理文本，还处理数值特征。例如，自然希望构建能够融合文本和其他非文本特征信息的嵌入。以下是推荐应用的策略：

+   **将文本与非文本特征进行串联**。例如，如果我们处理的是以文本形式存在的人物简介，并且有其他独立的特征如姓名或年龄，那么可以得到新的文本描述，如：“我的名字是<*name*>。<*profile description*>。我<*age*>岁。”最后，这样的文本描述可以输入到BERT模型中。

+   **将嵌入与特征进行串联**。可以如上所述构建BERT嵌入，然后将其与其他特征进行串联。唯一改变的是配置中必须接受更高维度的输入向量用于下游任务的分类模型。

# 结论

在本文中，我们深入探讨了BERT的训练和微调过程。实际上，这些知识足以解决大多数自然语言处理任务，感谢BERT几乎完全将文本数据纳入嵌入中的能力。

最近，出现了其他类似BERT的模型（SBERT、RoBERTa等）。甚至存在一个专门研究领域，称为“*BERTology*”，它深入分析BERT的能力，以开发新的高性能模型。这些事实进一步证明了BERT在机器学习领域引发了革命，并使自然语言处理得以显著进步。

# 资源

+   [BERT: 深度双向变换器的预训练用于语言理解](https://arxiv.org/pdf/1810.04805.pdf)

*除非另有说明，否则所有图像均为作者提供*
