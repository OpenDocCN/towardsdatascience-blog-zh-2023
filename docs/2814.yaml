- en: Training an Agent to Master a Simple Game Through Self-Play
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: é€šè¿‡è‡ªæˆ‘å¯¹å¼ˆè®­ç»ƒä¸€ä¸ªä»£ç†ä»¥æŒæ¡ç®€å•æ¸¸æˆ
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/training-an-agent-to-master-a-simple-game-through-self-play-88bdd0d60928?source=collection_archive---------5-----------------------#2023-09-06](https://towardsdatascience.com/training-an-agent-to-master-a-simple-game-through-self-play-88bdd0d60928?source=collection_archive---------5-----------------------#2023-09-06)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://towardsdatascience.com/training-an-agent-to-master-a-simple-game-through-self-play-88bdd0d60928?source=collection_archive---------5-----------------------#2023-09-06](https://towardsdatascience.com/training-an-agent-to-master-a-simple-game-through-self-play-88bdd0d60928?source=collection_archive---------5-----------------------#2023-09-06)
- en: Simulate games and predict the outcomes.
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ¨¡æ‹Ÿæ¸¸æˆå¹¶é¢„æµ‹ç»“æœã€‚
- en: '[](https://sebastiengilbert.medium.com/?source=post_page-----88bdd0d60928--------------------------------)[![SÃ©bastien
    Gilbert](../Images/380f6588c3ef718947bcf82061f190eb.png)](https://sebastiengilbert.medium.com/?source=post_page-----88bdd0d60928--------------------------------)[](https://towardsdatascience.com/?source=post_page-----88bdd0d60928--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----88bdd0d60928--------------------------------)
    [SÃ©bastien Gilbert](https://sebastiengilbert.medium.com/?source=post_page-----88bdd0d60928--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://sebastiengilbert.medium.com/?source=post_page-----88bdd0d60928--------------------------------)[![SÃ©bastien
    Gilbert](../Images/380f6588c3ef718947bcf82061f190eb.png)](https://sebastiengilbert.medium.com/?source=post_page-----88bdd0d60928--------------------------------)[](https://towardsdatascience.com/?source=post_page-----88bdd0d60928--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----88bdd0d60928--------------------------------)
    [SÃ©bastien Gilbert](https://sebastiengilbert.medium.com/?source=post_page-----88bdd0d60928--------------------------------)'
- en: Â·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Â·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F975aef8c496a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftraining-an-agent-to-master-a-simple-game-through-self-play-88bdd0d60928&user=S%C3%A9bastien+Gilbert&userId=975aef8c496a&source=post_page-975aef8c496a----88bdd0d60928---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----88bdd0d60928--------------------------------)
    Â·8 min readÂ·Sep 6, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F88bdd0d60928&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftraining-an-agent-to-master-a-simple-game-through-self-play-88bdd0d60928&user=S%C3%A9bastien+Gilbert&userId=975aef8c496a&source=-----88bdd0d60928---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[å…³æ³¨](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F975aef8c496a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftraining-an-agent-to-master-a-simple-game-through-self-play-88bdd0d60928&user=S%C3%A9bastien+Gilbert&userId=975aef8c496a&source=post_page-975aef8c496a----88bdd0d60928---------------------post_header-----------)
    å‘è¡¨äº [Towards Data Science](https://towardsdatascience.com/?source=post_page-----88bdd0d60928--------------------------------)
    Â·8 min readÂ·2023å¹´9æœˆ6æ—¥[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F88bdd0d60928&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftraining-an-agent-to-master-a-simple-game-through-self-play-88bdd0d60928&user=S%C3%A9bastien+Gilbert&userId=975aef8c496a&source=-----88bdd0d60928---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F88bdd0d60928&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftraining-an-agent-to-master-a-simple-game-through-self-play-88bdd0d60928&source=-----88bdd0d60928---------------------bookmark_footer-----------)![](../Images/d109dfb5700af262af5a3d2b43e7547e.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F88bdd0d60928&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftraining-an-agent-to-master-a-simple-game-through-self-play-88bdd0d60928&source=-----88bdd0d60928---------------------bookmark_footer-----------)![](../Images/d109dfb5700af262af5a3d2b43e7547e.png)'
- en: A robot computing some additions. Image by the author, with help from DALL-E
    2.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªè®¡ç®—ä¸€äº›åŠ æ³•çš„æœºå™¨äººã€‚å›¾ç‰‡ç”±ä½œè€…æä¾›ï¼Œå€ŸåŠ© DALL-E 2ã€‚
- en: Introduction
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ä»‹ç»
- en: Isnâ€™t it amazing that everything you need to excel in a perfect information
    game is there for everyone to see in the rules of the game?
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: éš¾é“ä¸ä»¤äººæƒŠå¹å—ï¼Ÿåœ¨å®Œç¾ä¿¡æ¯æ¸¸æˆä¸­ï¼Œä½ éœ€è¦çš„ä¸€åˆ‡éƒ½åœ¨æ¸¸æˆè§„åˆ™ä¸­å¯è§ã€‚
- en: Unfortunately, for mere mortals like me, reading the rules of a new game is
    only a tiny fraction of the journey to learn to play a complex game. Most of the
    time is spent playing, ideally against a player of comparable strength (or a better
    player who is patient enough to help us expose our weaknesses). Losing often and
    hopefully winning sometimes provides the psychological punishments and rewards
    that steer us towards playing incrementally better.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸å¹¸çš„æ˜¯ï¼Œå¯¹äºåƒæˆ‘è¿™æ ·çš„æ™®é€šäººæ¥è¯´ï¼Œé˜…è¯»æ–°æ¸¸æˆçš„è§„åˆ™åªæ˜¯å­¦ä¹ ç©å¤æ‚æ¸¸æˆæ—…ç¨‹çš„ä¸€å°éƒ¨åˆ†ã€‚å¤§éƒ¨åˆ†æ—¶é—´éƒ½èŠ±åœ¨ç©æ¸¸æˆä¸Šï¼Œç†æƒ³æƒ…å†µä¸‹æ˜¯ä¸å®åŠ›ç›¸å½“çš„ç©å®¶ï¼ˆæˆ–ä¸€ä¸ªè¶³å¤Ÿè€å¿ƒçš„æ›´å¼ºçš„ç©å®¶ï¼Œå¸®åŠ©æˆ‘ä»¬æš´éœ²æˆ‘ä»¬çš„å¼±ç‚¹ï¼‰å¯¹æˆ˜ã€‚ç»å¸¸å¤±è´¥å¹¶å¸Œæœ›å¶å°”è·èƒœæä¾›äº†å¿ƒç†ä¸Šçš„æƒ©ç½šå’Œå¥–åŠ±ï¼Œä¿ƒä½¿æˆ‘ä»¬é€æ­¥æé«˜æ¸¸æˆæ°´å¹³ã€‚
- en: 'Perhaps, in a not-too-far future, a language model will read the rules of a
    complex game such as chess and, right from the start, play at the highest possible
    level. In the meantime, I propose a more modest challenge: learning by self-play.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ–è®¸ï¼Œåœ¨ä¸ä¹…çš„å°†æ¥ï¼Œè¯­è¨€æ¨¡å‹å°†èƒ½å¤Ÿé˜…è¯»åƒå›½é™…è±¡æ£‹è¿™æ ·çš„å¤æ‚æ¸¸æˆçš„è§„åˆ™ï¼Œå¹¶ä»ä¸€å¼€å§‹å°±ä»¥æœ€é«˜æ°´å¹³è¿›è¡Œæ¸¸æˆã€‚åœ¨æ­¤æœŸé—´ï¼Œæˆ‘æå‡ºä¸€ä¸ªæ›´ä¸ºè°¦è™šçš„æŒ‘æˆ˜ï¼šé€šè¿‡è‡ªæˆ‘å¯¹å¼ˆå­¦ä¹ ã€‚
- en: In this project, weâ€™ll train an agent to learn to **play perfect information,
    two player games** by observing the results of matches played by previous versions
    of itself. The agent will approximate a value (the game expected result) for any
    game state. As an additional challenge, our agent wonâ€™t be allowed to maintain
    a lookup table of the state space, as this approach would not be manageable for
    complex games.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ä¸ªé¡¹ç›®ä¸­ï¼Œæˆ‘ä»¬å°†è®­ç»ƒä¸€ä¸ªä»£ç†ï¼Œé€šè¿‡è§‚å¯Ÿä¹‹å‰ç‰ˆæœ¬çš„æ¯”èµ›ç»“æœæ¥å­¦ä¹ **å®Œç¾ä¿¡æ¯çš„ä¸¤äººæ¸¸æˆ**ã€‚è¯¥ä»£ç†å°†ä¸ºä»»ä½•æ¸¸æˆçŠ¶æ€è¿‘ä¼¼ä¸€ä¸ªå€¼ï¼ˆæ¸¸æˆæœŸæœ›ç»“æœï¼‰ã€‚ä½œä¸ºé¢å¤–æŒ‘æˆ˜ï¼Œæˆ‘ä»¬çš„ä»£ç†ä¸å…è®¸ç»´æŒçŠ¶æ€ç©ºé—´çš„æŸ¥æ‰¾è¡¨ï¼Œå› ä¸ºè¿™ç§æ–¹æ³•å¯¹äºå¤æ‚æ¸¸æˆè€Œè¨€æ˜¯ä¸åˆ‡å®é™…çš„ã€‚
- en: Solving SumTo100
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: è§£å†³SumTo100
- en: The game
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ¸¸æˆ
- en: 'The game that we are going to discuss is SumTo100\. The game goal is to reach
    a sum of 100 by adding numbers between 1 and 10\. Here are the rules:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬è¦è®¨è®ºçš„æ¸¸æˆæ˜¯SumTo100ã€‚æ¸¸æˆçš„ç›®æ ‡æ˜¯é€šè¿‡å°†1åˆ°10ä¹‹é—´çš„æ•°å­—ç›¸åŠ æ¥è¾¾åˆ°100ã€‚ä»¥ä¸‹æ˜¯è§„åˆ™ï¼š
- en: Initialize sum = 0.
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: åˆå§‹åŒ–å’Œ = 0ã€‚
- en: Choose a first player. The two players take turns.
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: é€‰æ‹©ä¸€ä¸ªé¦–ä½ç©å®¶ã€‚ä¸¤ä½ç©å®¶è½®æµè¿›è¡Œã€‚
- en: 'While sum < 100:'
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å½“å’Œ < 100 æ—¶ï¼š
- en: The player chooses a number between 1 and 10 inclusively. The selected number
    gets added to the sum without exceeding 100.
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç©å®¶é€‰æ‹©ä¸€ä¸ª1åˆ°10ä¹‹é—´çš„æ•°å­—ï¼ˆåŒ…æ‹¬1å’Œ10ï¼‰ã€‚é€‰å®šçš„æ•°å­—ä¼šè¢«åŠ åˆ°æ€»å’Œä¸­ï¼Œä½†ä¸ä¼šè¶…è¿‡100ã€‚
- en: If sum < 100, the other player plays (i.e., we go back to the top of point 3).
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¦‚æœå’Œ < 100ï¼Œå¦ä¸€ä½ç©å®¶ç»§ç»­è¿›è¡Œï¼ˆå³ï¼Œæˆ‘ä»¬è¿”å›åˆ°ç¬¬3ç‚¹çš„é¡¶éƒ¨ï¼‰ã€‚
- en: 4\. The player that added the last number (reaching 100) wins.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 4. åŠ ä¸Šæœ€åä¸€ä¸ªæ•°å­—ï¼ˆè¾¾åˆ°100ï¼‰çš„ç©å®¶è·èƒœã€‚
- en: '![](../Images/795b2aff5a3db4ea402f215dfb555414.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/795b2aff5a3db4ea402f215dfb555414.png)'
- en: Two snails minding their own business. Image by the author, with help from DALL-E
    2.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸¤åªèœ—ç‰›å„è‡ªå¿™ç€è‡ªå·±çš„äº‹ã€‚å›¾åƒç”±ä½œè€…æä¾›ï¼ŒDALL-E 2ååŠ©å®Œæˆã€‚
- en: 'Starting with such a simple game has many advantages:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: ä»è¿™æ ·ä¸€ä¸ªç®€å•çš„æ¸¸æˆå¼€å§‹æœ‰è®¸å¤šå¥½å¤„ï¼š
- en: The state space has only 101 possible values.
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: çŠ¶æ€ç©ºé—´åªæœ‰101ä¸ªå¯èƒ½çš„å€¼ã€‚
- en: The states can get plotted on a 1D grid. This peculiarity will allow us to represent
    the state value function learned by the agent as a 1D bar graph.
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: çŠ¶æ€å¯ä»¥è¢«ç»˜åˆ¶åœ¨ä¸€ç»´ç½‘æ ¼ä¸Šã€‚è¿™ä¸€ç‰¹ç‚¹å°†ä½¿æˆ‘ä»¬èƒ½å¤Ÿå°†ä»£ç†å­¦ä¹ åˆ°çš„çŠ¶æ€å€¼å‡½æ•°è¡¨ç¤ºä¸ºä¸€ç»´æ¡å½¢å›¾ã€‚
- en: 'The optimal strategy is known:'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æœ€ä½³ç­–ç•¥å·²çŸ¥ï¼š
- en: '- Reach a sum of 11n + 1, where n âˆˆ {0, 1, 2, â€¦, 9}'
  id: totrans-29
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '- è¾¾åˆ°ä¸€ä¸ªå’Œä¸º11n + 1ï¼Œå…¶ä¸­n âˆˆ {0, 1, 2, â€¦, 9}'
- en: 'We can visualize the state value of the optimal strategy:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥å¯è§†åŒ–æœ€ä½³ç­–ç•¥çš„çŠ¶æ€å€¼ï¼š
- en: '![](../Images/9fa578e690e7ca71bc5d28f13819cf2c.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9fa578e690e7ca71bc5d28f13819cf2c.png)'
- en: 'Figure 1: The optimal state values for SumTo100\. Image by the author.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾1ï¼šSumTo100çš„æœ€ä½³çŠ¶æ€å€¼ã€‚å›¾åƒç”±ä½œè€…æä¾›ã€‚
- en: The game state is the sum after an agent has completed its turn. A value of
    1.0 means that the agent is sure to win (or has won), while a value of -1.0 means
    that the agent is sure to lose (assuming the opponent plays optimally). An intermediary
    value represents the estimated return. For example, a state value of 0.2 means
    a slightly positive state, while a state value of -0.8 represents a likely loss.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: æ¸¸æˆçŠ¶æ€æ˜¯ä»£ç†å®Œæˆå…¶å›åˆåçš„æ€»å’Œã€‚å€¼ä¸º1.0æ„å‘³ç€ä»£ç†è‚¯å®šä¼šèµ¢ï¼ˆæˆ–å·²ç»èµ¢äº†ï¼‰ï¼Œè€Œå€¼ä¸º-1.0åˆ™æ„å‘³ç€ä»£ç†è‚¯å®šä¼šè¾“ï¼ˆå‡è®¾å¯¹æ‰‹è¿›è¡Œæœ€ä½³ç©æ³•ï¼‰ã€‚ä¸­é—´å€¼ä»£è¡¨ä¼°è®¡å›æŠ¥ã€‚ä¾‹å¦‚ï¼ŒçŠ¶æ€å€¼ä¸º0.2è¡¨ç¤ºç¨å¾®ç§¯æçš„çŠ¶æ€ï¼Œè€ŒçŠ¶æ€å€¼ä¸º-0.8è¡¨ç¤ºå¯èƒ½çš„å¤±è´¥ã€‚
- en: If you want to dive in the code, the script that performs the whole training
    procedure is *learn_sumTo100.sh*, [in this repository](https://github.com/sebastiengilbert73/tutorial_learnbyplay).
    Otherwise, bear with me as weâ€™ll go through a high level description of how our
    agent learns by self-play.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä½ æƒ³æ·±å…¥äº†è§£ä»£ç ï¼Œæ‰§è¡Œæ•´ä¸ªè®­ç»ƒè¿‡ç¨‹çš„è„šæœ¬æ˜¯*learn_sumTo100.sh*ï¼Œ[åœ¨è¿™ä¸ªä»“åº“](https://github.com/sebastiengilbert73/tutorial_learnbyplay)ã€‚å¦åˆ™ï¼Œè¯·è€å¿ƒç­‰å¾…ï¼Œæˆ‘ä»¬å°†é€šè¿‡é«˜å±‚æ¬¡çš„æè¿°æ¥ä»‹ç»æˆ‘ä»¬çš„ä»£ç†å¦‚ä½•é€šè¿‡è‡ªæˆ‘å¯¹å¼ˆè¿›è¡Œå­¦ä¹ ã€‚
- en: Generation of games played by random players
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç”±éšæœºç©å®¶è¿›è¡Œçš„æ¸¸æˆç”Ÿæˆ
- en: We want our agent to learn from games played by previous versions of itself,
    but in the first iteration, since the agent has not learned anything yet, weâ€™ll
    have to simulate games played by *random players*. At each turn, the players will
    get the list of legal moves from the game authority (the class that encodes the
    game rules), given the current game state. The random players will select a move
    randomly from this list.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¸Œæœ›ä»£ç†ä»å…¶å…ˆå‰ç‰ˆæœ¬æ‰€ç©çš„æ¸¸æˆä¸­å­¦ä¹ ï¼Œä½†åœ¨ç¬¬ä¸€è½®ä¸­ï¼Œç”±äºä»£ç†å°šæœªå­¦ä¹ ä»»ä½•ä¸œè¥¿ï¼Œæˆ‘ä»¬å°†ä¸å¾—ä¸æ¨¡æ‹Ÿ*éšæœºç©å®¶*è¿›è¡Œçš„æ¸¸æˆã€‚åœ¨æ¯ä¸€æ­¥ï¼Œç©å®¶å°†ä»æ¸¸æˆæƒå¨ï¼ˆç¼–ç æ¸¸æˆè§„åˆ™çš„ç±»ï¼‰å¤„è·å¾—åˆæ³•ç§»åŠ¨çš„åˆ—è¡¨ï¼ŒåŸºäºå½“å‰çš„æ¸¸æˆçŠ¶æ€ã€‚éšæœºç©å®¶å°†ä»æ­¤åˆ—è¡¨ä¸­éšæœºé€‰æ‹©ä¸€ä¸ªåŠ¨ä½œã€‚
- en: 'Figure 2 is an example of a game played by two random players:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾2æ˜¯ä¸¤ä¸ªéšæœºç©å®¶è¿›è¡Œæ¸¸æˆçš„ç¤ºä¾‹ï¼š
- en: '![](../Images/621b06f67e6b39bb1b66c0de6fe48c53.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/621b06f67e6b39bb1b66c0de6fe48c53.png)'
- en: 'Figure 2: Example of a game played by random players. Image by the author.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾2ï¼šéšæœºç©å®¶è¿›è¡Œæ¸¸æˆçš„ç¤ºä¾‹ã€‚å›¾åƒç”±ä½œè€…æä¾›ã€‚
- en: In this case, the second player won the game by reaching a sum of 100.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œç¬¬äºŒä¸ªç©å®¶é€šè¿‡è¾¾åˆ°æ€»å’Œ100èµ¢å¾—äº†æ¸¸æˆã€‚
- en: Weâ€™ll implement an agent that has access to a neural network that takes as input
    a game state (after the agent has played) and outputs the expected return of this
    game. For any given state (before the agent has played), the agent gets the list
    of legal actions and their corresponding candidate states (we only consider games
    having deterministic transitions).
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†å®ç°ä¸€ä¸ªä»£ç†ï¼Œè¯¥ä»£ç†è®¿é—®ä¸€ä¸ªç¥ç»ç½‘ç»œï¼Œè¯¥ç½‘ç»œä»¥æ¸¸æˆçŠ¶æ€ï¼ˆåœ¨ä»£ç†å·²ç»ç©è¿‡ä¹‹åï¼‰ä½œä¸ºè¾“å…¥ï¼Œå¹¶è¾“å‡ºè¯¥æ¸¸æˆçš„é¢„æœŸå›æŠ¥ã€‚å¯¹äºä»»ä½•ç»™å®šçš„çŠ¶æ€ï¼ˆåœ¨ä»£ç†å°šæœªç©è¿‡ä¹‹å‰ï¼‰ï¼Œä»£ç†å°†è·å¾—åˆæ³•åŠ¨ä½œåˆ—è¡¨åŠå…¶å¯¹åº”çš„å€™é€‰çŠ¶æ€ï¼ˆæˆ‘ä»¬ä»…è€ƒè™‘å…·æœ‰ç¡®å®šæ€§è½¬ç§»çš„æ¸¸æˆï¼‰ã€‚
- en: 'Figure 3 shows the interactions between the agent, the opponent (whose move
    selection mechanism is unknown), and the game authority:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾3å±•ç¤ºäº†ä»£ç†ã€å¯¹æ‰‹ï¼ˆå…¶ç§»åŠ¨é€‰æ‹©æœºåˆ¶æœªçŸ¥ï¼‰å’Œæ¸¸æˆæƒå¨ä¹‹é—´çš„äº¤äº’ï¼š
- en: '![](../Images/73c3f1248a7589182a154460b1ec050c.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/73c3f1248a7589182a154460b1ec050c.png)'
- en: 'Figure 3: Interactions between the agent, the opponent, and the game authority.
    Image by the author.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾3ï¼šä»£ç†ã€å¯¹æ‰‹å’Œæ¸¸æˆæƒå¨ä¹‹é—´çš„äº¤äº’ã€‚å›¾åƒç”±ä½œè€…æä¾›ã€‚
- en: In this setting, the agent relies on its regression neural network to predict
    the expected return of game states. The better the neural network can predict
    which candidate move yields the highest return, the better the agent will play.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ç§è®¾ç½®ä¸‹ï¼Œä»£ç†ä¾èµ–äºå…¶å›å½’ç¥ç»ç½‘ç»œæ¥é¢„æµ‹æ¸¸æˆçŠ¶æ€çš„é¢„æœŸå›æŠ¥ã€‚ç¥ç»ç½‘ç»œé¢„æµ‹å“ªä¸ªå€™é€‰åŠ¨ä½œäº§ç”Ÿæœ€é«˜å›æŠ¥çš„èƒ½åŠ›è¶Šå¼ºï¼Œä»£ç†çš„è¡¨ç°å°±ä¼šè¶Šå¥½ã€‚
- en: 'Our list of randomly played matches will provide us with the dataset for our
    first pass of training. Taking the example game from Figure 2, we want to punish
    the moves made by player 1 since its behaviour led to a loss. The state resulting
    from the last action gets a value of -1.0 since it allowed the opponent to win.
    The other states get discounted negative values by a factor of Î³áµˆ , where d is
    the distance with respect to the last state reached by the agent. Î³ (gamma) is
    the discount factor, a number âˆˆ [0, 1], that expresses the uncertainty in the
    evolution of a game: we donâ€™t want to punish early decisions as hard as the last
    decisions. Figure 4 shows the state values associated with the decisions made
    by player 1:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬çš„éšæœºæ¸¸æˆåˆ—è¡¨å°†ä¸ºæˆ‘ä»¬ç¬¬ä¸€æ¬¡è®­ç»ƒæä¾›æ•°æ®é›†ã€‚ä»¥å›¾2ä¸­çš„ç¤ºä¾‹æ¸¸æˆä¸ºä¾‹ï¼Œæˆ‘ä»¬å¸Œæœ›æƒ©ç½šç©å®¶1çš„ç§»åŠ¨ï¼Œå› ä¸ºå…¶è¡Œä¸ºå¯¼è‡´äº†å¤±è´¥ã€‚ç”±äºå…è®¸å¯¹æ‰‹è·èƒœï¼Œæœ€åä¸€ä¸ªåŠ¨ä½œäº§ç”Ÿçš„çŠ¶æ€å€¼ä¸º-1.0ã€‚å…¶ä»–çŠ¶æ€é€šè¿‡æŠ˜æ‰£å› å­Î³áµˆè·å¾—è´Ÿå€¼ï¼Œå…¶ä¸­dæ˜¯ç›¸å¯¹äºä»£ç†è¾¾åˆ°çš„æœ€åçŠ¶æ€çš„è·ç¦»ã€‚Î³ï¼ˆgammaï¼‰æ˜¯æŠ˜æ‰£å› å­ï¼Œä¸€ä¸ªâˆˆ
    [0, 1]çš„æ•°å­—ï¼Œè¡¨ç¤ºæ¸¸æˆæ¼”å˜ä¸­çš„ä¸ç¡®å®šæ€§ï¼šæˆ‘ä»¬ä¸å¸Œæœ›åƒå¯¹æœ€åå†³ç­–é‚£æ ·ä¸¥å‰åœ°æƒ©ç½šæ—©æœŸå†³ç­–ã€‚å›¾4å±•ç¤ºäº†ç©å®¶1åšå‡ºçš„å†³ç­–ç›¸å…³çš„çŠ¶æ€å€¼ï¼š
- en: '![](../Images/fd36467f54afe8bb0ebd7d69304985b0.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fd36467f54afe8bb0ebd7d69304985b0.png)'
- en: 'Figure 4: The state values, from the point of view of player 1\. Image by the
    author.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾4ï¼šä»ç©å®¶1çš„è§’åº¦çœ‹çŠ¶æ€å€¼ã€‚å›¾åƒç”±ä½œè€…æä¾›ã€‚
- en: The random games generate states with their target expected return. For example,
    reaching a sum of 97 has a target expected return of -1.0, and a sum of 73 has
    a target expected return of -Î³Â³. Half the states take the point of view of player
    1, and the other half take the point of view of player 2 (although it doesnâ€™t
    matter in the case of the game SumTo100). When a game ends with a win for the
    agent, the corresponding states get similarly discounted positive values.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: éšæœºæ¸¸æˆç”Ÿæˆå…·æœ‰ç›®æ ‡é¢„æœŸå›æŠ¥çš„çŠ¶æ€ã€‚ä¾‹å¦‚ï¼Œè¾¾åˆ°97çš„æ€»å’Œæœ‰ä¸€ä¸ªç›®æ ‡é¢„æœŸå›æŠ¥ä¸º-1.0ï¼Œè€Œè¾¾åˆ°73çš„æ€»å’Œæœ‰ä¸€ä¸ªç›®æ ‡é¢„æœŸå›æŠ¥ä¸º-Î³Â³ã€‚çŠ¶æ€çš„ä¸€åŠä»¥ç©å®¶1çš„è§†è§’ä¸ºå‡†ï¼Œå¦ä¸€åŠä»¥ç©å®¶2çš„è§†è§’ä¸ºå‡†ï¼ˆè™½ç„¶åœ¨æ¸¸æˆSumTo100çš„æƒ…å†µä¸‹å¹¶ä¸é‡è¦ï¼‰ã€‚å½“æ¸¸æˆä»¥ä»£ç†è·èƒœç»“æŸæ—¶ï¼Œç›¸åº”çš„çŠ¶æ€å°†è·å¾—ç±»ä¼¼æŠ˜æ‰£çš„æ­£å€¼ã€‚
- en: Training an agent to predict the return of games
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è®­ç»ƒä¸€ä¸ªä»£ç†ä»¥é¢„æµ‹æ¸¸æˆçš„å›æŠ¥
- en: 'We have all we need to start our training: a neural network (weâ€™ll use a two-layers
    perceptron) and a dataset of (state, expected return) pairs. Letâ€™s see how the
    loss on the predicted expected return evolves:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å·²ç»æ‹¥æœ‰äº†å¼€å§‹è®­ç»ƒæ‰€éœ€çš„ä¸€åˆ‡ï¼šä¸€ä¸ªç¥ç»ç½‘ç»œï¼ˆæˆ‘ä»¬å°†ä½¿ç”¨ä¸€ä¸ªä¸¤å±‚æ„ŸçŸ¥æœºï¼‰å’Œä¸€ä¸ª (çŠ¶æ€, æœŸæœ›å›æŠ¥) å¯¹çš„æ•°æ®é›†ã€‚è®©æˆ‘ä»¬çœ‹çœ‹é¢„æµ‹çš„æœŸæœ›å›æŠ¥çš„æŸå¤±å¦‚ä½•æ¼”å˜ï¼š
- en: '![](../Images/aed34ba90cca092159f457e82761b0e2.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/aed34ba90cca092159f457e82761b0e2.png)'
- en: 'Figure 5: Evolution of the loss as a function of the epoch. Image by the author.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 5ï¼šæŸå¤±éšè½®æ¬¡æ¼”å˜çš„æƒ…å†µã€‚å›¾ç‰‡ç”±ä½œè€…æä¾›ã€‚
- en: We shouldnâ€™t be surprised that the neural network doesnâ€™t show much predicting
    power over the outcome of games played by random players.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸è¶³ä¸ºå¥‡çš„æ˜¯ï¼Œç¥ç»ç½‘ç»œåœ¨é¢„æµ‹ç”±éšæœºç©å®¶è¿›è¡Œçš„æ¸¸æˆç»“æœæ–¹é¢å¹¶æ²¡æœ‰è¡¨ç°å‡ºå¤ªå¤§çš„é¢„æµ‹èƒ½åŠ›ã€‚
- en: Did the neural network learn anything at all?
  id: totrans-55
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ç¥ç»ç½‘ç»œç©¶ç«Ÿå­¦åˆ°äº†ä»€ä¹ˆï¼Ÿ
- en: 'Fortunately, because the states can get represented as a 1D grid of numbers
    between 0 and 100, we can plot the predicted returns of the neural network after
    the first training round and compare them with the optimal state values of Figure
    1:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: å¹¸è¿çš„æ˜¯ï¼Œç”±äºçŠ¶æ€å¯ä»¥è¡¨ç¤ºä¸º 0 åˆ° 100 ä¹‹é—´çš„ 1D æ•°å­—ç½‘æ ¼ï¼Œæˆ‘ä»¬å¯ä»¥ç»˜åˆ¶ç¥ç»ç½‘ç»œåœ¨ç¬¬ä¸€æ¬¡è®­ç»ƒè½®æ¬¡åçš„é¢„æµ‹å›æŠ¥ï¼Œå¹¶å°†å…¶ä¸å›¾ 1 ä¸­çš„æœ€ä½³çŠ¶æ€å€¼è¿›è¡Œæ¯”è¾ƒï¼š
- en: '![](../Images/7e29ae6d8508e4435fde819a26e55cba.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7e29ae6d8508e4435fde819a26e55cba.png)'
- en: 'Figure 6: The predicted returns after training on a dataset of games played
    by random players. Image by the author.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 6ï¼šåœ¨ç”±éšæœºç©å®¶è¿›è¡Œçš„æ¸¸æˆæ•°æ®é›†ä¸Šè®­ç»ƒåçš„é¢„æµ‹å›æŠ¥ã€‚å›¾ç‰‡ç”±ä½œè€…æä¾›ã€‚
- en: 'As it turns out, through the chaos of random games, the neural network learned
    two things:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: äº‹å®è¯æ˜ï¼Œé€šè¿‡éšæœºæ¸¸æˆçš„æ··ä¹±ï¼Œç¥ç»ç½‘ç»œå­¦åˆ°äº†ä¸¤ä»¶äº‹ï¼š
- en: If you can reach a sum of 100, do it. Thatâ€™s good to know, considering it is
    the goal of the game.
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¦‚æœä½ èƒ½è¾¾åˆ° 100ï¼Œå°±å»åšå§ã€‚è€ƒè™‘åˆ°è¿™æ˜¯æ¸¸æˆçš„ç›®æ ‡ï¼ŒçŸ¥é“è¿™ä¸€ç‚¹æ˜¯å¥½çš„ã€‚
- en: If you reach a sum of 99, youâ€™re sure to lose. Indeed, in this situation, the
    opponent has only one legal action and that action yields to a loss for the agent.
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¦‚æœä½ è¾¾åˆ° 99ï¼Œä½ è‚¯å®šä¼šè¾“ã€‚å®é™…ä¸Šï¼Œåœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œå¯¹æ‰‹åªæœ‰ä¸€ä¸ªåˆæ³•çš„è¡ŒåŠ¨ï¼Œè€Œè¯¥è¡ŒåŠ¨ä¼šå¯¼è‡´ä»£ç†çš„å¤±è´¥ã€‚
- en: The neural network learned essentially to finish the game.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: ç¥ç»ç½‘ç»œåŸºæœ¬ä¸Šå­¦ä¼šäº†å¦‚ä½•å®Œæˆæ¸¸æˆã€‚
- en: To learn to play a little better, we must rebuild the dataset by simulating
    games played between copies of the agent with their freshly trained neural network.
    To avoid generating identical games, the players play a bit randomly. An approach
    that works well is choosing moves with the epsilon-greedy algorithm, using Îµ =
    0.5 for each players first move, then Îµ = 0.1 for the rest of the game.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†å­¦ä¼šç¨å¾®å¥½ä¸€ç‚¹çš„æ¸¸æˆç©æ³•ï¼Œæˆ‘ä»¬å¿…é¡»é€šè¿‡æ¨¡æ‹Ÿåœ¨å…¶æ–°è®­ç»ƒç¥ç»ç½‘ç»œä¸‹çš„ä»£ç†ä¹‹é—´è¿›è¡Œçš„æ¸¸æˆæ¥é‡å»ºæ•°æ®é›†ã€‚ä¸ºäº†é¿å…ç”Ÿæˆç›¸åŒçš„æ¸¸æˆï¼Œç©å®¶ä¼šæœ‰äº›éšæœºã€‚ä¸€ä¸ªæœ‰æ•ˆçš„æ–¹æ³•æ˜¯ä½¿ç”¨Îµ-è´ªå©ªç®—æ³•é€‰æ‹©è¡ŒåŠ¨ï¼Œç©å®¶çš„ç¬¬ä¸€æ­¥ä½¿ç”¨Îµ
    = 0.5ï¼Œç„¶åæ¸¸æˆçš„å…¶ä½™éƒ¨åˆ†ä½¿ç”¨Îµ = 0.1ã€‚
- en: Repeating the training loop with better and better players
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä½¿ç”¨è¶Šæ¥è¶Šå¥½çš„ç©å®¶é‡å¤è®­ç»ƒå¾ªç¯
- en: 'Since both players now know that they must reach 100, reaching a sum between
    90 and 99 should be punished, because the opponent would jump on the opportunity
    to win the match. This phenomenon is visible in the predicted state values after
    the second round of training:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±äºä¸¤åç©å®¶ç°åœ¨éƒ½çŸ¥é“ä»–ä»¬å¿…é¡»è¾¾åˆ° 100ï¼Œè¾¾åˆ° 90 åˆ° 99 ä¹‹é—´çš„æ€»å’Œåº”è¯¥å—åˆ°æƒ©ç½šï¼Œå› ä¸ºå¯¹æ‰‹ä¼šæŠ“ä½æœºä¼šèµ¢å¾—æ¯”èµ›ã€‚è¿™ç§ç°è±¡åœ¨ç¬¬äºŒè½®è®­ç»ƒåçš„é¢„æµ‹çŠ¶æ€å€¼ä¸­æ˜¯æ˜æ˜¾çš„ï¼š
- en: '![](../Images/56665c735e94d184694716d505e7211b.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/56665c735e94d184694716d505e7211b.png)'
- en: 'Figure 7: Predicted state values after two rounds of training. Sums from 90
    to 99 show values close to -1\. Image by the author.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 7ï¼šç»è¿‡ä¸¤è½®è®­ç»ƒåçš„é¢„æµ‹çŠ¶æ€å€¼ã€‚90 åˆ° 99 çš„æ€»å’Œæ˜¾ç¤ºå‡ºæ¥è¿‘ -1 çš„å€¼ã€‚å›¾ç‰‡ç”±ä½œè€…æä¾›ã€‚
- en: We see a pattern emerging. The first training round informs the neural network
    about the last action; the second training round informs about the penultimate
    action, and so on. We need to repeat the cycle of games generation and training
    on prediction at least as many times as there are actions in a game.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬çœ‹åˆ°ä¸€ç§æ¨¡å¼æ­£åœ¨å‡ºç°ã€‚ç¬¬ä¸€æ¬¡è®­ç»ƒè½®æ¬¡å°†æœ€åä¸€ä¸ªè¡ŒåŠ¨çš„ä¿¡æ¯ä¼ é€’ç»™ç¥ç»ç½‘ç»œï¼›ç¬¬äºŒæ¬¡è®­ç»ƒè½®æ¬¡ä¼ é€’å€’æ•°ç¬¬äºŒä¸ªè¡ŒåŠ¨çš„ä¿¡æ¯ï¼Œä»¥æ­¤ç±»æ¨ã€‚æˆ‘ä»¬éœ€è¦è‡³å°‘é‡å¤ç”Ÿæˆæ¸¸æˆå’Œé¢„æµ‹è®­ç»ƒçš„å‘¨æœŸå¤šæ¬¡ï¼Œä»¥è¦†ç›–æ¸¸æˆä¸­çš„æ‰€æœ‰è¡ŒåŠ¨ã€‚
- en: 'The following animation shows the evolution of the predicted state values after
    25 training rounds:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥ä¸‹åŠ¨ç”»å±•ç¤ºäº†ç»è¿‡ 25 è½®è®­ç»ƒåçš„é¢„æµ‹çŠ¶æ€å€¼çš„æ¼”å˜ï¼š
- en: '![](../Images/77fdfa0b81871f7545cb49bdda1617c7.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/77fdfa0b81871f7545cb49bdda1617c7.png)'
- en: 'Figure 8: Animation of the state values learned along the training rounds.
    Image by the author.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 8ï¼šè®­ç»ƒè½®æ¬¡ä¸­å­¦åˆ°çš„çŠ¶æ€å€¼çš„åŠ¨ç”»ã€‚å›¾ç‰‡ç”±ä½œè€…æä¾›ã€‚
- en: The envelope of the predicted returns decays exponentially, as we go from the
    end towards the beginning of the game. Is this a problem?
  id: totrans-72
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: é¢„æµ‹å›æŠ¥çš„åŒ…ç»œçº¿éšç€æ¸¸æˆçš„è¿›è¡Œä»ç»“å°¾å‘å¼€å§‹æ–¹å‘å‘ˆæŒ‡æ•°è¡°å‡ã€‚è¿™æ˜¯ä¸ªé—®é¢˜å—ï¼Ÿ
- en: 'Two factors contribute to this phenomenon:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸¤ä¸ªå› ç´ å¯¼è‡´äº†è¿™ç§ç°è±¡ï¼š
- en: Î³ directly damps the target expected returns, as we move away from the end of
    the game.
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Î³ ç›´æ¥æŠ‘åˆ¶ç›®æ ‡é¢„æœŸå›æŠ¥ï¼Œå› ä¸ºæˆ‘ä»¬ç¦»æ¸¸æˆç»“æŸè¶Šæ¥è¶Šè¿œã€‚
- en: The epsilon-greedy algorithm injects randomness in the player behaviours, making
    the outcomes harder to predict. There is an incentive to predict a value close
    to zero to protect against cases of extremely high losses. However, the randomness
    is desirable because we donâ€™t want the neural network to learn a single line of
    play. We want the neural network to witness blunders and unexpected good moves,
    both from the agent and the opponent.
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: epsilon-è´ªå©ªç®—æ³•åœ¨ç©å®¶è¡Œä¸ºä¸­æ³¨å…¥äº†éšæœºæ€§ï¼Œä½¿ç»“æœæ›´éš¾é¢„æµ‹ã€‚å­˜åœ¨ä¸€ç§æ¿€åŠ±æœºåˆ¶æ¥é¢„æµ‹æ¥è¿‘é›¶çš„å€¼ï¼Œä»¥é˜²æ­¢æé«˜æŸå¤±çš„æƒ…å†µã€‚ç„¶è€Œï¼Œè¿™ç§éšæœºæ€§æ˜¯å¯å–çš„ï¼Œå› ä¸ºæˆ‘ä»¬ä¸å¸Œæœ›ç¥ç»ç½‘ç»œå­¦ä¹ å•ä¸€çš„æ¸¸æˆç­–ç•¥ã€‚æˆ‘ä»¬å¸Œæœ›ç¥ç»ç½‘ç»œèƒ½è§è¯åˆ°é”™è¯¯å’Œæ„å¤–çš„å¥½åŠ¨ä½œï¼Œè¿™äº›éƒ½å¯èƒ½æ¥è‡ªä»£ç†å’Œå¯¹æ‰‹ã€‚
- en: In practice, it shouldnâ€™t be a problem because in any situation, we will compare
    values among the legal moves in a given state, which share comparable scales,
    at least for the game SumTo100\. The scale of the values doesnâ€™t matter when we
    choose the greedy move.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: å®é™…ä¸Šï¼Œè¿™ä¸åº”è¯¥æ˜¯é—®é¢˜ï¼Œå› ä¸ºåœ¨ä»»ä½•æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å°†æ¯”è¾ƒç»™å®šçŠ¶æ€ä¸‹æ‰€æœ‰åˆæ³•ç§»åŠ¨çš„å€¼ï¼Œè¿™äº›å€¼åœ¨è§„æ¨¡ä¸Šæ˜¯ç›¸ä¼¼çš„ï¼Œè‡³å°‘å¯¹äºSumTo100æ¸¸æˆè€Œè¨€ã€‚é€‰æ‹©è´ªå©ªç§»åŠ¨æ—¶ï¼Œå€¼çš„è§„æ¨¡å¹¶ä¸é‡è¦ã€‚
- en: Conclusion
  id: totrans-77
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç»“è®º
- en: 'We challenged ourselves to create an agent that can learn to master a game
    of perfect information involving two players, with deterministic transitions from
    a state to the next, given an action. No hand coded strategies nor tactics were
    allowed: everything had to be learned by self-play.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬æŒ‘æˆ˜è‡ªå·±åˆ›å»ºä¸€ä¸ªå¯ä»¥å­¦ä¹ æŒæ¡å®Œç¾ä¿¡æ¯æ¸¸æˆçš„ä»£ç†ï¼Œè¿™ä¸ªæ¸¸æˆæ¶‰åŠä¸¤ä¸ªç©å®¶ï¼ŒçŠ¶æ€åˆ°çŠ¶æ€ä¹‹é—´æœ‰ç¡®å®šçš„è¿‡æ¸¡ï¼Œç»™å®šä¸€ä¸ªåŠ¨ä½œã€‚ä¸å¾—ä½¿ç”¨æ‰‹åŠ¨ç¼–ç çš„ç­–ç•¥æˆ–æˆ˜æœ¯ï¼šä¸€åˆ‡éƒ½å¿…é¡»é€šè¿‡è‡ªæˆ‘å¯¹å¼ˆæ¥å­¦ä¹ ã€‚
- en: We could solve the simple game of SumTo100 by running multiple rounds of pitching
    copies of the agent against each other, and training a regression neural network
    to predict the expected return of the generated games.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬é€šè¿‡å¤šè½®å¯¹æŠ—æ€§çš„ä»£ç†æ‹·è´æ¥è§£å†³ç®€å•çš„SumTo100æ¸¸æˆï¼Œå¹¶è®­ç»ƒä¸€ä¸ªå›å½’ç¥ç»ç½‘ç»œæ¥é¢„æµ‹ç”Ÿæˆæ¸¸æˆçš„é¢„æœŸå›æŠ¥ã€‚
- en: The gained insight prepares us well for the next ladder in game complexity,
    but that will be for my next post! ğŸ˜Š
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: è·å¾—çš„è§è§£ä¸ºæˆ‘ä»¬åšå¥½äº†è¿æ¥ä¸‹ä¸€ä¸ªæ¸¸æˆå¤æ‚æ€§é˜¶æ¢¯çš„å‡†å¤‡ï¼Œä½†é‚£å°†æ˜¯æˆ‘ä¸‹ä¸€ä¸ªå¸–å­ä¸­çš„å†…å®¹ï¼ğŸ˜Š
- en: Thank you for your time.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: æ„Ÿè°¢ä½ çš„æ—¶é—´ã€‚
