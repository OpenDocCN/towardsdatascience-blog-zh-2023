- en: 'Mastering the Art of Machine Learning Workflows: A Comprehensive Guide to Transformer,
    Estimator, and Pipeline'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/mastering-the-art-of-machine-learning-workflows-a-comprehensive-guide-to-transformer-estimator-6254f4e2d2f8?source=collection_archive---------7-----------------------#2023-06-09](https://towardsdatascience.com/mastering-the-art-of-machine-learning-workflows-a-comprehensive-guide-to-transformer-estimator-6254f4e2d2f8?source=collection_archive---------7-----------------------#2023-06-09)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Write seamless code with optimal results
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@andreas030503?source=post_page-----6254f4e2d2f8--------------------------------)[![Andreas
    Lukita](../Images/8660ca1fea5da34ce3475281c1f52152.png)](https://medium.com/@andreas030503?source=post_page-----6254f4e2d2f8--------------------------------)[](https://towardsdatascience.com/?source=post_page-----6254f4e2d2f8--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----6254f4e2d2f8--------------------------------)
    [Andreas Lukita](https://medium.com/@andreas030503?source=post_page-----6254f4e2d2f8--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F955ef38ea7b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmastering-the-art-of-machine-learning-workflows-a-comprehensive-guide-to-transformer-estimator-6254f4e2d2f8&user=Andreas+Lukita&userId=955ef38ea7b&source=post_page-955ef38ea7b----6254f4e2d2f8---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----6254f4e2d2f8--------------------------------)
    ·14 min read·Jun 9, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F6254f4e2d2f8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmastering-the-art-of-machine-learning-workflows-a-comprehensive-guide-to-transformer-estimator-6254f4e2d2f8&user=Andreas+Lukita&userId=955ef38ea7b&source=-----6254f4e2d2f8---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F6254f4e2d2f8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmastering-the-art-of-machine-learning-workflows-a-comprehensive-guide-to-transformer-estimator-6254f4e2d2f8&source=-----6254f4e2d2f8---------------------bookmark_footer-----------)![](../Images/750da8ba509c1ea6c28de2648e625fa5.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Photo by [Rick Hyne](https://unsplash.com/de/@quinley1770?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: '*“It’s okay to write it this way as long as I understand it now, and the good
    thing is, it works! I manage to magically churns out a pretty good result with
    my model, what a good one to end off the day.”*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: No, I’m here to tell you that’s not good enough. Indeed, upon starting a Machine
    Learning project, many novices and intermediate-level analysts alike rush to produce
    mediocre-level models with a lack of proper workflow. While at times the problem
    at hand is simple, failure to follow proper workflow often leads to insidious
    problems that can be hard to detect — data leakage, for example.
  prefs: []
  type: TYPE_NORMAL
- en: “As long as it works, it’s good enough.” Let me tell you it’s not. Let’s role-play
    a quick scenario where you have to explain your work to the Senior Analyst. Here
    are some questions. If it works today, is it guaranteed that it will work tomorrow
    and will be reproducible easily? Could you explain the preprocessing steps of
    your model workflow in a Notebook consisting of more than 200 cells? If you perform
    cross-validation this way, wouldn’t that expose the testing dataset and bloat
    up the model performance? Tough questions, aren’t they?
  prefs: []
  type: TYPE_NORMAL
- en: Let me tell you, actually, you are not alone and you are not that far. Even
    after attending multiple Business Analytics and Machine Learning courses, none
    of my instructors have shared the tools and tips that I am sharing below. I would
    say they are not the spotlight classes that everyone is paying attention to when
    first introduced to Scikit-Learn. Yet, they produce a consistent result that improves
    your code-writing dramatically. Imagine effortlessly wrangling your data, seamlessly
    transforming features, and training sophisticated models, all while maintaining
    the elegance and simplicity of your code. Yes, that is our objective by the end
    of this comprehensive guide, and hopefully, you will be convinced to adopt the
    practice below. Let’s get started.
  prefs: []
  type: TYPE_NORMAL
- en: '**Table of Contents**'
  prefs: []
  type: TYPE_NORMAL
- en: '[Reasons to Adopt Pipeline](#8d5b)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Estimators](#29de)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Transformers](#4358)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Pipeline](#fd4a)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Custom Estimator](#0457)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[FeatureUnion](#5660)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Real-world Dataset Examples: Bank Marketing with Grid Search CV](#547b)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reasons to Adopt Pipeline
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**1\. Streamlined Workflow.** Leveraging Pipeline allows seamless integration
    of multiple steps in your data preprocessing and modeling journey. It enables
    you to chain together various transformers and estimators, ensuring a clear, concise,
    and automated flow from data preprocessing to model training and evaluation. By
    encapsulating your preprocessing and modeling steps in a Pipeline, your code becomes
    more organized, modular, and easier to understand. It improves the way your code
    looks and can be maintained because each step is clearly defined. Treat each step
    in the Pipeline as independent, you can change or add steps without worrying about
    how one preprocessing step will affect the other!'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3a976ef31dac95508e385c36c6278c77.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: '**2\. Prevent Data Leakage.** The dreaded antagonist, the nemesis of every
    analyst. Data Leakage may occur when information from the test dataset unintentionally
    influences the preprocessing steps or model training, leading to overly optimistic
    performance estimates. In a way, you are leaking information about what is going
    to be tested and making your learning model see what is going to be tested in
    advance. Obviously, *“he tryna gas it up”.* Generally, the rule of thumb is to
    fit the training dataset only, then transform both the training and testing dataset.
    ***The code below shows where some people went wrong.*** Also, often you would
    have multiple preprocessing steps that typically involve transformers such as
    `StandardScaler()`, `MinMaxScaler()`, `OneHotEncoder()`, etc. Imagine having to
    do the fit and transform process multiple times throughout your workflow, wouldn’t
    that be confusing and inconvenient?'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '**3\. Hyperparameter Tuning and Cross-Validation.** Easily tune hyperparameters
    across all the steps in your pipeline using techniques such as GridSearchCV. Error
    often goes unnoticed in this particular step, however. Let’s look at a simple
    illustration.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Try running both examples: although the cross-validation score is not far off,
    the part without the Pipeline leaks information since the feature selection step
    is performed on the entire dataset. When we reach the cross-validation step in
    which the dataset is separated into training and validation sets, they are essentially
    from the same source (the training set has information learned previously from
    the validation set when we perform feature selection). If you find it hard to
    understand this part, try rereading the paragraph and code it out yourself to
    internalize.'
  prefs: []
  type: TYPE_NORMAL
- en: Estimators
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before we dive deeper into what Pipeline can do, let’s digress towards the components
    that form a Pipeline — Estimators. We will touch on the other components—Transformers,
    Predictors, and Models in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: A lot of people often get confused with the term Estimator in Scikit-learn.
    People tend to associate estimators with the ability to predict—that is, with
    the `predict` method in particular. While there are some truths in that statement,
    it is unfortunately only a half-truth at best. Estimators are basically the building
    block of the Scikit-learn library. An estimator is a tool that can learn from
    your training set to create a model that can make predictions or inferences about
    new data. Since all estimators have the `fit` method to learn from the training
    set, they inherit from `BaseEstimator`
  prefs: []
  type: TYPE_NORMAL
- en: From `BaseEstimator` itself, there is no `predict` method, only `fit`. An estimator
    does not necessarily need to have a `predict` method, although some do. An estimator
    with predict method attempts to make predictions on new, unseen data based on
    the learned model. For example, regressors and classifiers such as Linear Regression,
    Random Forest Classifier, Gradient Boosting Classifier, etc. are estimators with
    the`predict` method.
  prefs: []
  type: TYPE_NORMAL
- en: Going one step further, let’s peek into the original documentation for `LogisticRegression`
    class[²](#11e5). In the snippet below, we observe that the class inherits from
    `BaseEstimator` for the `fit` method, and `LinearClassifierMixin` for the `predict`
    method.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/040dc7cbdee785137308b46706d97c64.png)'
  prefs: []
  type: TYPE_IMG
- en: '[Scikit-learn GitHub](https://github.com/scikit-learn/scikit-learn/blob/364c77e047ca08a95862becf40a04fe9d4cd2c98/sklearn/linear_model/_logistic.py)
    (BSD-3)'
  prefs: []
  type: TYPE_NORMAL
- en: Transformers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Transformer is a type of estimator with a `transform` method. Note that the
    word “transformer” here refers to the Scikit-learn context specifically. It should
    not be confused or mistaken with the transformer in neural network architecture,
    which has gained more attention and prominence in recent years.
  prefs: []
  type: TYPE_NORMAL
- en: In short, what a transformer does is transform/manipulate the predictors (`X`)
    in some ways such that it is ready to be consumed by machine learning algorithms.
    This could be scaling of continuous predictors using prominent tools such as `StandardScaler`
    and `MinMaxScaler`, or encoding categorical predictors using `OneHotEncoder` or
    `OrdinalEncoder`.
  prefs: []
  type: TYPE_NORMAL
- en: Going a step further, a transformer has a fit-transform mechanism, where it
    learns from the training data using the `fit` method and then applies the learned
    transformations to both the training and test data using the `transform` method.
    This ensures that the same transformations are consistently applied throughout.
  prefs: []
  type: TYPE_NORMAL
- en: Going two steps further, to follow the Scikit-learn API implementation rule,
    a transformer usually inherits from `BaseEstimator` for its `fit` method, and
    `TransformerMixin` for its `transform` method. Let’s peek into the original documentation
    for `StandardScaler` library[³](#2e21).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4ade96aaf2b16470dcdafba3f3fa35a3.png)'
  prefs: []
  type: TYPE_IMG
- en: '[Scikit-learn GitHub](https://github.com/scikit-learn/scikit-learn/blob/364c77e04/sklearn/preprocessing/_data.py#L644)'
  prefs: []
  type: TYPE_NORMAL
- en: ColumnTransformer[⁵](#ea73)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: At times, you would need to apply transformations specific to certain columns
    only depending on your needs. For example, applying `OneHotEncoder` to categorical
    features with no specific hierarchy, and `OrdinalEncoder` to categorical features
    with specific hierarchy and ordering (i.e. for t-shirt sizes, we usually have
    sizes ordering to follow such as XS<S<M<L<XL). We can achieve this separation
    using `ColumnTransformer`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: As you might expect, we are going to put the variable `col_trans` above as part
    of our big overall Pipeline later on in the code. Simple and elegant.
  prefs: []
  type: TYPE_NORMAL
- en: '**Pipeline**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The `Pipeline[⁶](#8591)` class executes the estimators in the pipe in a sequential
    manner, passing the output of one step as the input to the next. This essentially
    allows the concept of chaining to take place. From the [Scikit-learn documentation](https://scikit-learn.org/stable/developers/develop.html)[⁴](#74fc)
    itself, here are the criteria for an estimator to be eligible to be incorporated
    as part of a Pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: For an estimator to be usable together with `pipeline.Pipeline` in any but the
    last step, it needs to provide a `fit` or `fit_transform` function. To be able
    to evaluate the pipeline on any data but the training set, it also needs to provide
    a `transform` function. There are no special requirements for the last step in
    a pipeline, except that it has a `fit` function.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Using `Pipeline`, we remove the redundant steps of having to call the method
    `fit` and `transform` on every estimator and/or transformer. Calling the method
    `fit` once directly from the pipeline suffices. How this works behind the scene
    is that it calls `fit` on the first estimator, then`transform` the input and pass
    it on to the next estimator. Indeed, the pipeline is as best as what the last
    estimator can do (it has all the methods of the last estimator in the pipe). If
    the last estimator is a regressor, then the Pipeline can be used as a regressor.
    If the last estimator is a transformer, so is the pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: Below is an illustration of how to use the `Pipeline` class.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: In short, the argument to `Pipeline` is a list of tuples executed sequentially.
    The first element of the tuple is the arbitrary name you set as per your wish
    to identify the estimator, sort of like the ID. Meanwhile, the second element
    is the estimator object. Simple isn’t it? If you are not good with names, Scikit-learn
    provides the shorthand `make_pipeline` method that removes the headache of having
    to come up with names.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Custom Estimator
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, methods such as `StandardScaler` and `MinMaxScaler` look good and work
    for many cases. The question is, what if you have your own customized method to
    manipulate and preprocess your dataset for example? Can you still incorporate
    it neatly into the `Pipeline` class? The answer is a resounding yes! There are
    two ways of achieving this—leveraging on `FunctionTransformer` or writing your
    own custom class.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s say you want to do a Box-Cox transformation on part of your dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: The second method is to write your own custom class that inherits from `BaseEstimator`
    and `TransformerMixin` if you are writing a transformer estimator. If you are
    writing an estimator with a classification task for example, then inherit from
    `ClassifierMixin` instead.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s say you want to write a class that removes outliers and incorporates it
    into your Pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: I want to bring your focus, particularly on the `OutlierRemove` class. Here,
    we have the `fit` method that returns `self` to allow us to continue chaining
    and the`transform` method that does the removal of the outliers. After this, we
    can simply incorporate the class into our `Pipeline` like the following
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: FeatureUnion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Here comes the confusing part—`FeatureUnion` serves the same purpose as `Pipeline`,
    but they work quite differently. In `FeatureUnion`, the `fit` and `transform`
    methods are not done sequentially one after the other. Each transformer estimator
    is `fit` independently to the data, and then the `transform` methods are applied
    in parallel. The end results are then combined together. Picture the code below.
    Here, we can run the preprocessing for numerical and categorical predictors in
    parallel using `FeatureUnion[⁷](#2758)` as they are independent of one another.
    This results in faster and more efficient operation.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Real-world Dataset Examples: Bank Marketing with Grid Search CV'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Here, I wish to end off by illustrating the contents above using a real-world
    dataset inspired by a Portuguese Financial Institution. The dataset is available
    on [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/Bank+Marketing)[¹](#ed3c)
    for public use with citation.
  prefs: []
  type: TYPE_NORMAL
- en: Allow me to skip all the exploratory data analysis and visualization, and zoom
    straight into the modeling of the Pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: '**1\. Importing the dataset**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'In short, what the code above does are the followings:'
  prefs: []
  type: TYPE_NORMAL
- en: Import the dataset with a comma separator
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Rename the column ‘y’ to ‘deposit’
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Encode the column deposit from no and yes to 0 and 1
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**2\. Train-Test Split**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '**3\. Writing additional 3 custom classes**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'In short, what the code above does are the followings:'
  prefs: []
  type: TYPE_NORMAL
- en: The class `ClfSwitcher` inherits from `BaseEstimator`. This class serves the
    purpose of switching between classifiers easily. We set the default classifier
    to be XGBoost Classifier.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The method `outlier_thresholds` and `delete_potential_outlier_list` identify
    outliers in each column and set them to `NaN`. The class `OutlierTrans` is a transformer
    that inherits from both `BaseEstimator` and `TransformerMixin`. The `transform`
    method returns the previously mentioned 2 methods.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The class `TweakBankMarketing` is a custom class to do custom transformations
    such as creating new columns, dropping undesirable columns, and changing data
    types accordingly.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**4\. Preparing Pipeline**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'In short, what the code above does are the followings:'
  prefs: []
  type: TYPE_NORMAL
- en: Scale numerical columns using `StandardScaler` and `MinMaxScaler`
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Encode categorical columns using `OneHotEncoder` and `OrdinalEncoder`
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use `ColumnTransformer` to do the transformations for different columns of the
    datasets separately.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, `Pipeline` encapsulates everything seamlessly.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: At this stage, this is our constructed Pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b0bd308ca51fa6e1ae7316665e165d05.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: '**5\. Define hyperparameters for Grid Search CV**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'In short, what the code above does are the followings:'
  prefs: []
  type: TYPE_NORMAL
- en: Define parameter grids for 4 different classifiers, namely, `SGDClassifier`,
    `LogisticRegression`, `RandomForestClassifier`, `XGBClassifier`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**6\. Perform Grid Search CV**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'In short, what the code above does are the followings:'
  prefs: []
  type: TYPE_NORMAL
- en: Placing our pipeline object as the first argument to the `GridSearchCV` parameter.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**7\. Printing best estimator**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Here, we obtain a validation score of 0.74, with an AUC score of 0.74 as well.
  prefs: []
  type: TYPE_NORMAL
- en: '**8\. Plot the ROC-AUC curve**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/704e0e23207a8a15c9b496876cad3d55.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: Afterword
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There you have it! Pipeline with Estimators and Transformers. Next time when
    you approach an ML project, consider using this technique. It may seem difficult
    to adopt at first, but keep practicing and soon you will create robust and efficient
    Machine Learning pipelines.
  prefs: []
  type: TYPE_NORMAL
- en: If you pick up something useful from this article, do consider giving me a [***Follow***](https://medium.com/@andreas030503)
    on Medium. Easy, 1 article a week to keep yourself updated and stay ahead of the
    curve!
  prefs: []
  type: TYPE_NORMAL
- en: Connect With Me!
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[*LinkedIn*](https://www.linkedin.com/in/andreaslukita7/)👔'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Twitter*](https://twitter.com/andreaslukita7)🖊'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Bank Marketing Data Set [Moro et al., 2014] S. Moro, P. Cortez, and P. Rita.
    A Data-Driven Approach to Predict the Success of Bank Telemarketing. Decision
    Support Systems, Elsevier, 62:22–31, June 2014: [https://archive.ics.uci.edu/ml/datasets/Bank+Marketing](https://archive.ics.uci.edu/ml/datasets/Bank+Marketing)
    (CC BY 4.0)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Scikit-learn Linear Model Logistic: [https://github.com/scikit-learn/scikit-learn/blob/364c77e047ca08a95862becf40a04fe9d4cd2c98/sklearn/linear_model/_logistic.py](https://github.com/scikit-learn/scikit-learn/blob/364c77e047ca08a95862becf40a04fe9d4cd2c98/sklearn/linear_model/_logistic.py)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Scikit-learn Preprocessing: [https://github.com/scikit-learn/scikit-learn/blob/364c77e04/sklearn/preprocessing/_data.py#L644](https://github.com/scikit-learn/scikit-learn/blob/364c77e04/sklearn/preprocessing/_data.py#L644)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Developing Scikit-learn estimators: [https://scikit-learn.org/stable/developers/develop.html](https://scikit-learn.org/stable/developers/develop.html)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Scikit-learn ColumnTransformer: [https://scikit-learn.org/stable/modules/generated/sklearn.compose.ColumnTransformer.html](https://scikit-learn.org/stable/modules/generated/sklearn.compose.ColumnTransformer.html)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Scikit-learn Pipeline: [https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Scikit-learn FeatureUnion: [https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.FeatureUnion.html](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.FeatureUnion.html)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
