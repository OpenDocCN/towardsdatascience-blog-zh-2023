- en: 'BERT vs GPT: Comparing the NLP Giants'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/bert-vs-gpt-comparing-the-nlp-giants-329d105e34ec?source=collection_archive---------1-----------------------#2023-08-20](https://towardsdatascience.com/bert-vs-gpt-comparing-the-nlp-giants-329d105e34ec?source=collection_archive---------1-----------------------#2023-08-20)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: How different are their structure, and how do the differences impact the model’s
    ability?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@vuphuongthao9611?source=post_page-----329d105e34ec--------------------------------)[![Thao
    Vu](../Images/9d44a2f199cdc9c29da72d9dc4971561.png)](https://medium.com/@vuphuongthao9611?source=post_page-----329d105e34ec--------------------------------)[](https://towardsdatascience.com/?source=post_page-----329d105e34ec--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----329d105e34ec--------------------------------)
    [Thao Vu](https://medium.com/@vuphuongthao9611?source=post_page-----329d105e34ec--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fa836aac352ca&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbert-vs-gpt-comparing-the-nlp-giants-329d105e34ec&user=Thao+Vu&userId=a836aac352ca&source=post_page-a836aac352ca----329d105e34ec---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----329d105e34ec--------------------------------)
    ·7 min read·Aug 20, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F329d105e34ec&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbert-vs-gpt-comparing-the-nlp-giants-329d105e34ec&user=Thao+Vu&userId=a836aac352ca&source=-----329d105e34ec---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F329d105e34ec&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbert-vs-gpt-comparing-the-nlp-giants-329d105e34ec&source=-----329d105e34ec---------------------bookmark_footer-----------)![](../Images/2a6c1c6d9546726bba7434946b0bcdc0.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Image generated by the author using Stable Diffusion.
  prefs: []
  type: TYPE_NORMAL
- en: 'In 2018, NLP researchers were all amazed by the BERT paper [1]. The approach
    was simple, yet the result was impressive: it set new benchmarks for 11 NLP tasks.'
  prefs: []
  type: TYPE_NORMAL
- en: In a little over a year, BERT has become a ubiquitous baseline in Natural Language
    Processing (NLP) experiments counting over 150 research publications analysing
    and improving the model. [2]
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: In 2022, ChatGPT [3] blew up the whole Internet with its ability to generate
    human-like responses. The model can comprehend a wide range of topics and carry
    the conversation naturally for an extended period, which sets it apart from all
    traditional chatbots.
  prefs: []
  type: TYPE_NORMAL
- en: BERT and ChatGPT are significant breakthroughs in NLP, yet their approaches
    are different. How do their structures differ, and how do they impact the models’
    ability? Let’s dive in!
  prefs: []
  type: TYPE_NORMAL
- en: Attention
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We must first recall the commonly-used attention to understand the model structure
    fully. Attention mechanisms are designed to capture and model relationships between
    tokens in a sequence, which is one of the reasons why they have been so successful
    in NLP tasks.
  prefs: []
  type: TYPE_NORMAL
- en: An intuitive…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
