- en: 'BERT vs GPT: Comparing the NLP Giants'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/bert-vs-gpt-comparing-the-nlp-giants-329d105e34ec?source=collection_archive---------1-----------------------#2023-08-20](https://towardsdatascience.com/bert-vs-gpt-comparing-the-nlp-giants-329d105e34ec?source=collection_archive---------1-----------------------#2023-08-20)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: How different are their structure, and how do the differences impact the model’s
    ability?
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@vuphuongthao9611?source=post_page-----329d105e34ec--------------------------------)[![Thao
    Vu](../Images/9d44a2f199cdc9c29da72d9dc4971561.png)](https://medium.com/@vuphuongthao9611?source=post_page-----329d105e34ec--------------------------------)[](https://towardsdatascience.com/?source=post_page-----329d105e34ec--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----329d105e34ec--------------------------------)
    [Thao Vu](https://medium.com/@vuphuongthao9611?source=post_page-----329d105e34ec--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fa836aac352ca&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbert-vs-gpt-comparing-the-nlp-giants-329d105e34ec&user=Thao+Vu&userId=a836aac352ca&source=post_page-a836aac352ca----329d105e34ec---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----329d105e34ec--------------------------------)
    ·7 min read·Aug 20, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F329d105e34ec&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbert-vs-gpt-comparing-the-nlp-giants-329d105e34ec&user=Thao+Vu&userId=a836aac352ca&source=-----329d105e34ec---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F329d105e34ec&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbert-vs-gpt-comparing-the-nlp-giants-329d105e34ec&source=-----329d105e34ec---------------------bookmark_footer-----------)![](../Images/2a6c1c6d9546726bba7434946b0bcdc0.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: Image generated by the author using Stable Diffusion.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: 'In 2018, NLP researchers were all amazed by the BERT paper [1]. The approach
    was simple, yet the result was impressive: it set new benchmarks for 11 NLP tasks.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: In a little over a year, BERT has become a ubiquitous baseline in Natural Language
    Processing (NLP) experiments counting over 150 research publications analysing
    and improving the model. [2]
  id: totrans-10
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: In 2022, ChatGPT [3] blew up the whole Internet with its ability to generate
    human-like responses. The model can comprehend a wide range of topics and carry
    the conversation naturally for an extended period, which sets it apart from all
    traditional chatbots.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: BERT and ChatGPT are significant breakthroughs in NLP, yet their approaches
    are different. How do their structures differ, and how do they impact the models’
    ability? Let’s dive in!
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: BERT 和 ChatGPT 是自然语言处理领域的重大突破，但它们的方法不同。它们的结构有何不同，它们如何影响模型的能力？让我们深入探讨一下吧！
- en: Attention
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 注意事项
- en: We must first recall the commonly-used attention to understand the model structure
    fully. Attention mechanisms are designed to capture and model relationships between
    tokens in a sequence, which is one of the reasons why they have been so successful
    in NLP tasks.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 我们必须首先回顾常用的注意力机制，以充分理解模型结构。注意力机制旨在捕捉和建模序列中标记之间的关系，这也是它们在自然语言处理任务中取得巨大成功的原因之一。
- en: An intuitive…
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 一个直观的…
