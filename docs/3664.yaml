- en: 'TDS Best of 2023: On ChatGPT and LLMs'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 2023年最佳：关于ChatGPT和LLMs
- en: 原文：[https://towardsdatascience.com/tds-best-of-2023-on-chatgpt-and-llms-83bdfbb2136d?source=collection_archive---------3-----------------------#2023-12-14](https://towardsdatascience.com/tds-best-of-2023-on-chatgpt-and-llms-83bdfbb2136d?source=collection_archive---------3-----------------------#2023-12-14)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/tds-best-of-2023-on-chatgpt-and-llms-83bdfbb2136d?source=collection_archive---------3-----------------------#2023-12-14](https://towardsdatascience.com/tds-best-of-2023-on-chatgpt-and-llms-83bdfbb2136d?source=collection_archive---------3-----------------------#2023-12-14)
- en: '[](https://towardsdatascience.medium.com/?source=post_page-----83bdfbb2136d--------------------------------)[![TDS
    Editors](../Images/4b2d1beaf4f6dcf024ffa6535de3b794.png)](https://towardsdatascience.medium.com/?source=post_page-----83bdfbb2136d--------------------------------)[](https://towardsdatascience.com/?source=post_page-----83bdfbb2136d--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----83bdfbb2136d--------------------------------)
    [TDS Editors](https://towardsdatascience.medium.com/?source=post_page-----83bdfbb2136d--------------------------------)'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://towardsdatascience.medium.com/?source=post_page-----83bdfbb2136d--------------------------------)[![TDS
    Editors](../Images/4b2d1beaf4f6dcf024ffa6535de3b794.png)](https://towardsdatascience.medium.com/?source=post_page-----83bdfbb2136d--------------------------------)[](https://towardsdatascience.com/?source=post_page-----83bdfbb2136d--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----83bdfbb2136d--------------------------------)
    [TDS Editors](https://towardsdatascience.medium.com/?source=post_page-----83bdfbb2136d--------------------------------)'
- en: ·
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F7e12c71dfa81&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftds-best-of-2023-on-chatgpt-and-llms-83bdfbb2136d&user=TDS+Editors&userId=7e12c71dfa81&source=post_page-7e12c71dfa81----83bdfbb2136d---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----83bdfbb2136d--------------------------------)
    ·Sent as a [Newsletter](/newsletter?source=post_page-----83bdfbb2136d--------------------------------)
    ·5 min read·Dec 14, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F83bdfbb2136d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftds-best-of-2023-on-chatgpt-and-llms-83bdfbb2136d&user=TDS+Editors&userId=7e12c71dfa81&source=-----83bdfbb2136d---------------------clap_footer-----------)'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F7e12c71dfa81&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftds-best-of-2023-on-chatgpt-and-llms-83bdfbb2136d&user=TDS+Editors&userId=7e12c71dfa81&source=post_page-7e12c71dfa81----83bdfbb2136d---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----83bdfbb2136d--------------------------------)
    · 发送为 [通讯](https://towardsdatascience.com/newsletter?source=post_page-----83bdfbb2136d--------------------------------)
    · 阅读时间5分钟·2023年12月14日 [](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F83bdfbb2136d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftds-best-of-2023-on-chatgpt-and-llms-83bdfbb2136d&user=TDS+Editors&userId=7e12c71dfa81&source=-----83bdfbb2136d---------------------clap_footer-----------)'
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F83bdfbb2136d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftds-best-of-2023-on-chatgpt-and-llms-83bdfbb2136d&source=-----83bdfbb2136d---------------------bookmark_footer-----------)'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F83bdfbb2136d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftds-best-of-2023-on-chatgpt-and-llms-83bdfbb2136d&source=-----83bdfbb2136d---------------------bookmark_footer-----------)'
- en: You might say 2023 was an eventful year for data scientists and ML professionals,
    but that wouldn’t *quite* capture the amount of hectic activity we’ve seen in
    the field in the past 12 months.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会说2023年对数据科学家和机器学习专业人士来说是多事之年，但这并不能*完全*捕捉到过去12个月我们在这个领域所经历的繁忙活动量。
- en: As much as we always aim to resist hype and hyperbole, we have to concede that
    yes, we’ve seen some dramatic changes in the way both practitioners and society
    at large view AI and its effects on our daily lives. The launch of ChatGPT in
    the final weeks of 2022 was far from the only factor in this transition, but it’s
    difficult to deny its role as both catalyst and symbolic focal point.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们总是力图抵制炒作和夸张，但我们不得不承认，是的，我们确实看到了一些戏剧性的变化，既包括从业者的观点，也包括社会整体对人工智能及其对我们日常生活影响的看法。ChatGPT在2022年最后几周的发布远不是这种过渡的唯一因素，但很难否认它既是催化剂又是象征性的焦点。
- en: When we considered how we can take stock of our authors’ best and most popular
    work in 2023, then, turning to articles on LLMs in general—and on that one ubiquitous
    chatbot in particular—became a very natural choice. The selection of articles
    we present here is by no means exhaustive, but it *does* offer a representative
    sample of the articles that you, our readers, responded to the most—whether it’s
    the ones you couldn’t stop reading and sharing, or those that generated the most
    thoughtful conversations on TDS and beyond.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们考虑如何盘点2023年我们作者的最佳和最受欢迎作品时，回顾关于大语言模型的文章——尤其是那个无处不在的聊天机器人——成为了一个非常自然的选择。我们在这里呈现的文章并不全面，但*确实*提供了一个具有代表性的样本，展示了你们这些读者对哪些文章反响最强烈——无论是你们无法停止阅读和分享的文章，还是那些在TDS及其他地方引发了最深刻讨论的文章。
- en: Before we dive into the articles that made the biggest splash in the past year,
    we’d like to take a moment to thank our entire community for your support. We
    owe a special debt of gratitude to our incredible authors, to our partners at
    [Medium](https://medium.com/u/a32c340ea342?source=post_page-----83bdfbb2136d--------------------------------),
    to a dedicated group of volunteer Editorial Associates who generously offer us
    their expertise, and to our two former colleagues and extraordinary editors, [Caitlin
    Kindig](https://medium.com/u/2155e1f99318?source=post_page-----83bdfbb2136d--------------------------------)
    and [Katherine Prairie](https://medium.com/u/29c9531ee6f5?source=post_page-----83bdfbb2136d--------------------------------).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们深入探讨过去一年中最引起关注的文章之前，我们想花一点时间感谢我们的整个社区对我们的支持。我们特别感谢我们了不起的作者们、[Medium](https://medium.com/u/a32c340ea342?source=post_page-----83bdfbb2136d--------------------------------)的合作伙伴、慷慨提供专业知识的志愿编辑组，以及我们的两位前同事和杰出编辑，[凯特琳·金迪格](https://medium.com/u/2155e1f99318?source=post_page-----83bdfbb2136d--------------------------------)和[凯瑟琳·普雷里](https://medium.com/u/29c9531ee6f5?source=post_page-----83bdfbb2136d--------------------------------)。
- en: '[**How ChatGPT Works: The Model Behind The Bot**](/how-chatgpt-works-the-models-behind-the-bot-1ce5fca96286)In
    the least shocking development ever, [Molly Ruby](https://medium.com/u/7a38f8e9fb80?source=post_page-----83bdfbb2136d--------------------------------)’s
    accessible and informative explainer became our most popular post of 2023\. If
    you haven’t read it already, it’s not too late to catch up!'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**ChatGPT 如何运作：模型背后的机器人**](/how-chatgpt-works-the-models-behind-the-bot-1ce5fca96286)在最不令人惊讶的发展中，[莫莉·鲁比](https://medium.com/u/7a38f8e9fb80?source=post_page-----83bdfbb2136d--------------------------------)的易于理解且信息丰富的解释成为了我们2023年最受欢迎的文章。如果你还没读过，现在也不算太晚！'
- en: '[**Closed AI Models Make Bad Baselines**](/closed-ai-models-make-bad-baselines-4bf6e47c9e6a)What
    direction will NLP research take in a post-ChatGPT world? [Anna Rogers](https://medium.com/u/201bcd64e17?source=post_page-----83bdfbb2136d--------------------------------)
    examines the current state of a rapidly changing field.'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**封闭的 AI 模型造成糟糕的基准**](/closed-ai-models-make-bad-baselines-4bf6e47c9e6a)在一个后
    ChatGPT 世界中，自然语言处理研究将会采取什么方向？[安娜·罗杰斯](https://medium.com/u/201bcd64e17?source=post_page-----83bdfbb2136d--------------------------------)考察了这个迅速变化领域的现状。'
- en: '[**Can ChatGPT Write Better SQL than a Data Analyst?**](/can-chatgpt-write-better-sql-than-a-data-analyst-f079518efab2)While
    the extent to which LLMs pose a threat to entire professions remains to be seen,
    [Marie Truong](https://medium.com/u/4cfa1d0b321f?source=post_page-----83bdfbb2136d--------------------------------)
    took the time to survey ChatGPT’s coding skills soon after its launch.'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**ChatGPT 能写出比数据分析师更好的 SQL 吗？**](/can-chatgpt-write-better-sql-than-a-data-analyst-f079518efab2)虽然大语言模型是否对整个职业构成威胁仍有待观察，[玛丽·陈](https://medium.com/u/4cfa1d0b321f?source=post_page-----83bdfbb2136d--------------------------------)在
    ChatGPT 发布后不久便花时间调查了其编程技能。'
- en: '[**GPT Is an Unreliable Information Store**](/chatgpt-insists-i-am-dead-and-the-problem-with-language-models-db5a36c22f11)In
    a prescient look at AI hallucinations, [Noble Ackerson](https://medium.com/u/68605bd278a3?source=post_page-----83bdfbb2136d--------------------------------)
    zoomed in on the emerging risks of using LLMs as if they were reliable search
    engines.'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**GPT 是一个不可靠的信息库**](/chatgpt-insists-i-am-dead-and-the-problem-with-language-models-db5a36c22f11)在对人工智能幻觉的前瞻性观察中，[诺布尔·阿克森](https://medium.com/u/68605bd278a3?source=post_page-----83bdfbb2136d--------------------------------)深入探讨了将大语言模型当作可靠搜索引擎使用的潜在风险。'
- en: '![](../Images/ad425592ddeb01443641c0c130a09902.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ad425592ddeb01443641c0c130a09902.png)'
- en: Photo by [Mick Haupt](https://unsplash.com/@rocinante_11?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由 [米奇·豪普特](https://unsplash.com/@rocinante_11?utm_source=medium&utm_medium=referral)
    拍摄，来源于 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: '[**How to Convert Any Text Into a Graph of Concepts**](/how-to-convert-any-text-into-a-graph-of-concepts-110844f22a1a)Exploring
    the realm of new possibilities in NLP thanks to LLMs, [Rahul Nayak](https://medium.com/u/473e87f4b733?source=post_page-----83bdfbb2136d--------------------------------)
    offers a hands-on approach to converting a text corpus into a knowledge graph.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**如何将任何文本转换成概念图**](/how-to-convert-any-text-into-a-graph-of-concepts-110844f22a1a)
    由于 LLM，NLP 领域的可能性得到了探索，[Rahul Nayak](https://medium.com/u/473e87f4b733?source=post_page-----83bdfbb2136d--------------------------------)
    提供了一种将文本语料库转换为知识图谱的实用方法。'
- en: '[**Not All Rainbows and Sunshine: The Darker Side of ChatGPT**](/not-all-rainbows-and-sunshine-the-darker-side-of-chatgpt-75917472b9c)From
    baked-in bias to issues around privacy and plagiarism, [Mary Reagan PhD](https://medium.com/u/4a596f4380a0?source=post_page-----83bdfbb2136d--------------------------------)
    unpacked some of the major risks that have emerged in the wake of ChatGPT’s ascendancy.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**并非所有美好：ChatGPT 的阴暗面**](/not-all-rainbows-and-sunshine-the-darker-side-of-chatgpt-75917472b9c)
    从内建偏见到隐私和剽窃问题，[Mary Reagan PhD](https://medium.com/u/4a596f4380a0?source=post_page-----83bdfbb2136d--------------------------------)
    揭示了 ChatGPT 崛起后出现的一些主要风险。'
- en: '[**Zero-ETL, ChatGPT, and the Future of Data Engineering**](/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c)How
    will ChatGPT and similar tools affect day-to-day data-engineering workflows? [Barr
    Moses](https://medium.com/u/2818bac48708?source=post_page-----83bdfbb2136d--------------------------------)
    shared insights on the future of the “post-modern data stack.”'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**零ETL、ChatGPT 与数据工程的未来**](/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c)
    ChatGPT 和类似工具将如何影响日常的数据工程工作流？[Barr Moses](https://medium.com/u/2818bac48708?source=post_page-----83bdfbb2136d--------------------------------)
    分享了对“后现代数据栈”未来的见解。'
- en: '[**All You Need to Know to Build Your First LLM App**](/all-you-need-to-know-to-build-your-first-llm-app-eb982c78ffac)2023
    was the year in which the process of building LLM-powered apps became meaningfully
    democratized, thanks in no small part to contributions like [Dominik Polzer](https://medium.com/u/3ab8d3143e32?source=post_page-----83bdfbb2136d--------------------------------)’s
    widely shared tutorial.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**构建你的第一个 LLM 应用所需了解的一切**](/all-you-need-to-know-to-build-your-first-llm-app-eb982c78ffac)
    2023 年是 LLM 驱动的应用程序构建过程变得实际民主化的一年，这在很大程度上得益于像 [Dominik Polzer](https://medium.com/u/3ab8d3143e32?source=post_page-----83bdfbb2136d--------------------------------)
    的广泛分享的教程。'
- en: '[**GPT-4 vs. ChatGPT: An Exploration of Training, Performance, Capabilities,
    and Limitations**](/gpt-4-vs-chatgpt-an-exploration-of-training-performance-capabilities-and-limitations-35c990c133c5)Months
    after releasing ChatGPT, OpenAI upped the ante with its latest model, GPT-4, and
    [Mary Newhauser](https://medium.com/u/6b27bdb820b9?source=post_page-----83bdfbb2136d--------------------------------)
    was quick to provide a thorough comparison of the two products.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**GPT-4 与 ChatGPT：训练、性能、能力与局限性的探讨**](/gpt-4-vs-chatgpt-an-exploration-of-training-performance-capabilities-and-limitations-35c990c133c5)
    在发布 ChatGPT 几个月后，OpenAI 通过最新的 GPT-4 提升了标准，[Mary Newhauser](https://medium.com/u/6b27bdb820b9?source=post_page-----83bdfbb2136d--------------------------------)
    迅速提供了这两款产品的详细对比。'
- en: '[**TimeGPT: The First Foundation Model for Time Series Forecasting**](/timegpt-the-first-foundation-model-for-time-series-forecasting-bf0a75e63b3a)As
    the year progressed, we encountered more and more LLM solutions targeting specific
    use cases. [Marco Peixeiro](https://medium.com/u/741c1c8fcfbd?source=post_page-----83bdfbb2136d--------------------------------)
    wrote a great explainer on TimeGPT, one such example of a customized foundation
    model.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**TimeGPT：首个时间序列预测的基础模型**](/timegpt-the-first-foundation-model-for-time-series-forecasting-bf0a75e63b3a)
    随着年份的推进，我们遇到了越来越多针对特定用例的 LLM 解决方案。[Marco Peixeiro](https://medium.com/u/741c1c8fcfbd?source=post_page-----83bdfbb2136d--------------------------------)
    对 TimeGPT 进行了解释，它是一个定制化基础模型的示例。'
- en: '[**Mastering Customer Segmentation with LLM**](/mastering-customer-segmentation-with-llm-3d9008235f41)The
    practical use cases for LLMs and the products they support continue to grow every
    day; [Damian Gil](https://medium.com/u/87864cbc1dda?source=post_page-----83bdfbb2136d--------------------------------)
    outlined a promising direction for marketers and business strategists.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**掌握客户细分与 LLM**](/mastering-customer-segmentation-with-llm-3d9008235f41) LLM
    的实际应用案例及其支持的产品每天都在不断增长；[Damian Gil](https://medium.com/u/87864cbc1dda?source=post_page-----83bdfbb2136d--------------------------------)
    为营销人员和商业战略家概述了一个有前途的方向。'
- en: '[**Getting Started with LangChain: A Beginner’s Guide to Building LLM-Powered
    Applications**](/getting-started-with-langchain-a-beginners-guide-to-building-llm-powered-applications-95fc8898732c)Alongside
    ChatGPT, LangChain emerged as a popular tool for builders of LLM-based products;
    [Leonie Monigatti](https://medium.com/u/3a38da70d8dc?source=post_page-----83bdfbb2136d--------------------------------)
    wrote the go-to resource for anyone interested in tinkering with it.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[**New ChatGPT Prompt Engineering Technique: Program Simulation**](/new-chatgpt-prompt-engineering-technique-program-simulation-56f49746aa7b)Translating
    our needs and goals into language LLMs can decipher correctly remains a challenge.
    [Giuseppe Scalamogna](https://medium.com/u/e039aa8b7221?source=post_page-----83bdfbb2136d--------------------------------)
    unveiled an innovative framework for more effective prompt design.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[**How GPT Models Work**](/how-gpt-models-work-b5f4517d5b5)For a thorough and
    accessible primer on the math and theory behind GPT models, [Beatriz Stollnitz](https://medium.com/u/1c8863892480?source=post_page-----83bdfbb2136d--------------------------------)’s
    deep dive remains a top-notch choice for beginners and more seasoned practitioners
    alike.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[**How to Build an LLM from Scratch**](/how-to-build-an-llm-from-scratch-8c477768f1f9)If
    you prefer a more hands-on approach to your learning, [Shawhin Talebi](https://medium.com/u/f3998e1cd186?source=post_page-----83bdfbb2136d--------------------------------)’s
    tutorial on building LLMs will take you from data curation to model evaluation—it’s
    worth exploring even if you’re not planning on creating the next Llama or Falcon
    model in your home office!'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[**RAG vs Finetuning — Which Is the Best Tool to Boost Your LLM Application?**](/rag-vs-finetuning-which-is-the-best-tool-to-boost-your-llm-application-94654b1eaba7)As
    we learned about the limitations of pre-trained models, new approaches emerged
    for boosting their performance. [Heiko Hotz](https://medium.com/u/993c21f1b30f?source=post_page-----83bdfbb2136d--------------------------------)
    delivered a useful comparison of the two leading options: fine-tuning and retrieval-augmented
    generation (RAG).'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[**Running Llama 2 on CPU Inference Locally for Document Q&A**](/running-llama-2-on-cpu-inference-for-document-q-a-3d636037a3d8)The
    ability to “speak” with our own text documents, PDFs, and audio recordings has
    become a popular everyday use case for LLMs. [Kenneth Leung](https://medium.com/u/dcd08e36f2d0?source=post_page-----83bdfbb2136d--------------------------------)’s
    step-by-step guide showed how we can create such a workflow on a local machine.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[**A Gentle Intro to Chaining LLMs, Agents, and utils via LangChain**](/a-gentle-intro-to-chaining-llms-agent-and-utils-via-langchain-16cd385fca81)For
    anyone taking their first steps working with LLMs, [Dr. Varshita Sher](https://medium.com/u/f8ca36def59?source=post_page-----83bdfbb2136d--------------------------------)’s
    helpful and comprehensive tutorial on the building blocks of LangChain is an essential
    read.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**LangChain中链式LLMs、代理和工具的温和介绍**](/a-gentle-intro-to-chaining-llms-agent-and-utils-via-langchain-16cd385fca81)
    对于任何刚刚开始使用LLMs的人，[Dr. Varshita Sher](https://medium.com/u/f8ca36def59?source=post_page-----83bdfbb2136d--------------------------------)关于LangChain构建模块的有用且全面的教程是必读之作。'
- en: '[**Large Language Models in Molecular Biology**](/large-language-models-in-molecular-biology-9eb6b65d8a30)Exploring
    LLMs’ potential to transform scientific research, [Serafim Batzoglou](https://medium.com/u/ccf342949c4?source=post_page-----83bdfbb2136d--------------------------------)’s
    eye-opening deep dive focused on its impact within molecular biology, with applications
    ranging from gene-structure prediction to pharmaceutical discovery.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**分子生物学中的大型语言模型**](/large-language-models-in-molecular-biology-9eb6b65d8a30)
    探索LLMs在科学研究中的潜力，[Serafim Batzoglou](https://medium.com/u/ccf342949c4?source=post_page-----83bdfbb2136d--------------------------------)的深度挖掘关注了其在分子生物学中的影响，应用范围从基因结构预测到药物发现。'
- en: '**Stay tuned!** Throughout 2023 we’ve published countless excellent articles
    on a wide range of topics that go far beyond LLMs and ChatGPT. Next week, we’ll
    devote our final Variable edition of the year to standout posts on data science
    and programming skills, career paths, and special projects.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '**敬请关注！** 在2023年，我们发布了大量优秀文章，涵盖了远超LLMs和ChatGPT的广泛话题。下周，我们将把今年的最后一期Variable专注于数据科学和编程技能、职业道路以及特别项目的精彩文章。'
- en: 'Thank you, once again, for supporting the work of our authors throughout 2023!
    If you’ve enjoyed the articles you read on TDS, consider [becoming a Friend of
    Medium Member](https://blog.medium.com/become-a-friend-of-medium-dd2fa7bf16c3):
    it’s a new membership level that offers your favorite authors bigger rewards for
    their top-notch writing.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 再次感谢您在2023年支持我们作者的工作！如果您喜欢TDS上的文章，可以考虑[成为Medium的朋友会员](https://blog.medium.com/become-a-friend-of-medium-dd2fa7bf16c3)：这是一个新的会员等级，能为您喜爱的作者提供更大的奖励。
- en: Until the next Variable,
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 直到下一个Variable，
- en: TDS Editors
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: TDS 编辑部
