- en: Geometrical Interpretation of Linear Regression in Machine Learning versus Classical
    Statistics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/geometrical-interpretation-of-linear-regression-in-machine-learning-versus-classical-statistics-bc39409efb72?source=collection_archive---------4-----------------------#2023-12-12](https://towardsdatascience.com/geometrical-interpretation-of-linear-regression-in-machine-learning-versus-classical-statistics-bc39409efb72?source=collection_archive---------4-----------------------#2023-12-12)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**Demystifying the confusion about Linear Regression Visually and Analytically**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@rishabh29288?source=post_page-----bc39409efb72--------------------------------)[![Rishabh
    Raman](../Images/041c1c6dd8713bbd2fab5408d5236213.png)](https://medium.com/@rishabh29288?source=post_page-----bc39409efb72--------------------------------)[](https://towardsdatascience.com/?source=post_page-----bc39409efb72--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----bc39409efb72--------------------------------)
    [Rishabh Raman](https://medium.com/@rishabh29288?source=post_page-----bc39409efb72--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F12323e77e1e2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgeometrical-interpretation-of-linear-regression-in-machine-learning-versus-classical-statistics-bc39409efb72&user=Rishabh+Raman&userId=12323e77e1e2&source=post_page-12323e77e1e2----bc39409efb72---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----bc39409efb72--------------------------------)
    ·9 min read·Dec 12, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fbc39409efb72&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgeometrical-interpretation-of-linear-regression-in-machine-learning-versus-classical-statistics-bc39409efb72&user=Rishabh+Raman&userId=12323e77e1e2&source=-----bc39409efb72---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fbc39409efb72&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgeometrical-interpretation-of-linear-regression-in-machine-learning-versus-classical-statistics-bc39409efb72&source=-----bc39409efb72---------------------bookmark_footer-----------)![](../Images/facef23d7c152cc1e5e168e448bdcba6.png)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Image: Linear regression illustration, by Stpasha, via Wikimedia Commons (Public
    Domain). Original Image Link: [https://upload.wikimedia.org/wikipedia/commons/8/87/OLS_geometric_interpretation.svg](https://upload.wikimedia.org/wikipedia/commons/8/87/OLS_geometric_interpretation.svg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'The above image represents a geometric interpretation of Ordinary Least Squares
    (OLS) or Linear Regression (words used interchangeably in classical statistics).
    Let’s break down what we’re seeing in an intuitive way:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Variables (X1 and X2):** Imagine you have two variables, X1 and X2\. These
    could represent anything, like the hours you study and the number of practice
    exams you take, respectively.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data Points (y):** Now, you have your outcome or what you’re trying to predict,
    which we call ‘y’. In our example, this could be your score on the actual exam.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Plane (colX):** The plane represents all possible predicted values that you
    can get by combining different amounts of your variables X1 and X2\. In our example,
    it could represent all possible exam scores you might predict based on different
    amounts of studying and practice exams.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Estimated Coefficients (Beta1 and Beta2):** These are the best guesses the
    OLS method makes for how much each variable affects your score. So, Beta 1 might
    tell you how much your score is predicted to increase for each extra hour of studying,
    and Beta 2 might tell you how much it increases for each additional practice exam
    you take.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Predicted Point (XB^):** This is the predicted score you’d get based on the
    estimated coefficients. It lies on the plane because it’s a combination of your
    variables X1 and X2 using the estimates from OLS.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Actual Point (y):** This is your actual exam score.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Error (ε):** This is the difference between your actual score and the predicted
    score. In other words, it’s how off the prediction was from reality.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Now, how does OLS work with all this?**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: OLS tries to find the values for Beta1 and Beta 2 so that when you predict ‘y’
    (the exam score) using X1 and X2 (study hours and practice exams), the error (ε)
    is as small as possible for all your data points. In the graphic, it’s like adjusting
    the plane until the vertical dotted lines (which represent errors) are collectively
    as short as they can be. The shortest distance from the actual data point (y)
    to the plane (colX) is always a straight line that’s perpendicular to the plane.
    OLS finds the particular plane where these perpendicular distances are minimized
    for all points.
  prefs: []
  type: TYPE_NORMAL
- en: In other words, OLS is trying to “fit” the plane as close as possible to your
    actual scores, while recognizing that it won’t usually pass through all the actual
    points because real life is rarely that perfect.
  prefs: []
  type: TYPE_NORMAL
- en: '**It’s like fitting the best sheet of paper beneath a scatter of pencil dots
    such that the paper is as close as possible to all the dots at the same time.**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s go over the main assumptions of OLS and connect them with the visual
    above:'
  prefs: []
  type: TYPE_NORMAL
- en: '**1\. Linearity**'
  prefs: []
  type: TYPE_NORMAL
- en: '**Assumption:** The relationship between the independent variables (X1, X2)
    and the dependent variable (y) is linear.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Visual Interpretation:** In the image, this is why we use a plane (colX)
    to represent the combination of X1 and X2\. If the relationship were not linear,
    we wouldn’t be able to represent it with a flat plane; it would be curved or of
    some other shape.'
  prefs: []
  type: TYPE_NORMAL
- en: '**2\. Independence**'
  prefs: []
  type: TYPE_NORMAL
- en: '**Assumption:** Observations are independent of each other.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Visual Interpretation:** Each data point (representing an observation) is
    plotted independently of the others. If there was dependence, we would see a systematic
    pattern in the errors (ε), like them all lying on one side of the plane, which
    would suggest that the way one data point is positioned could predict another,
    violating this assumption.'
  prefs: []
  type: TYPE_NORMAL
- en: '**3\. Homoscedasticity**'
  prefs: []
  type: TYPE_NORMAL
- en: '**Assumption:** The variance of the error terms (ε) is constant across all
    levels of the independent variables.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Visual Interpretation:** Ideally, the perpendicular distances from the actual
    data points (y) to the prediction plane (colX) should be uniformly scattered.
    There shouldn’t be any funnel shape or pattern in these distances; they should
    look random. If the errors get larger or smaller as X1 or X2 increases, this would
    violate homoscedasticity.'
  prefs: []
  type: TYPE_NORMAL
- en: '**4\. No perfect multicollinearity**'
  prefs: []
  type: TYPE_NORMAL
- en: '**Assumption:** The independent variables are not perfectly correlated with
    each other.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Visual Interpretation:** In the diagram, X1 and X2 are represented by two
    arrows pointing in different directions. If they were perfectly correlated, they
    would point in exactly the same direction, and we would not have a plane but a
    line. This would make it impossible to estimate the unique effect of X1 and X2
    on y.'
  prefs: []
  type: TYPE_NORMAL
- en: '**5\. No auto-correlation**'
  prefs: []
  type: TYPE_NORMAL
- en: '**Assumption:** The error terms are not correlated with each other.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Visual Interpretation:** This assumption is about the error terms, which
    are not explicitly shown in the image, but we infer that each error term (ε) is
    random and not influenced by the previous or next error term. If there was a pattern
    (like if one error was always followed by another error of similar size), we would
    suspect auto-correlation.'
  prefs: []
  type: TYPE_NORMAL
- en: '**6\. Exogeneity**'
  prefs: []
  type: TYPE_NORMAL
- en: '**Assumption:** The error terms have an expected value of zero.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Visual Interpretation:** This means that the plane should be positioned such
    that the errors, on average, cancel each other out. Some data points will be above
    the plane, and some below, but there’s no systematic bias making them all above
    or below.'
  prefs: []
  type: TYPE_NORMAL
- en: '**7\. Normality of errors (often an assumption for hypothesis testing)**'
  prefs: []
  type: TYPE_NORMAL
- en: '**Assumption:** The error terms are normally distributed.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Visual Interpretation:** While the normality assumption is not something
    we can visualize in a 3D plot of the data and the model, if we were to look at
    a histogram of the error terms, we would expect to see the familiar bell curve
    of a normal distribution.'
  prefs: []
  type: TYPE_NORMAL
- en: How does Linear Regression in Machine Learning Universe differ from Ordinary
    Least Squares based Linear Regression in Classical Statistics?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In classical statistics, Ordinary Least Squares (OLS) can be approached through
    the lens of Maximum Likelihood Estimation (MLE). Both MLE and OLS aim to find
    the best parameters for a model, but they come from different philosophies and
    use different methods to achieve this.
  prefs: []
  type: TYPE_NORMAL
- en: '**Maximum Likelihood Estimation (MLE) Approach:**'
  prefs: []
  type: TYPE_NORMAL
- en: 'MLE is based on probability. It asks the question: “Given a set of data points,
    what are the most likely parameters of the model that could have generated this
    data?” MLE assumes a certain probability distribution for the errors (often a
    normal distribution) and then finds the parameter values that maximize the likelihood
    of observing the actual data. In the geometric visual, this is akin to adjusting
    the angle and position of the plane (colX) in such a way that the probability
    of seeing the actual data points (y) is the highest. The likelihood incorporates
    not just the distances from the points to the plane (the errors) but also the
    shape of the error distribution.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Minimization of an Objective Function in Machine Learning (ML):**'
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, ML approaches typically frame regression as an optimization
    problem. The goal is to find the parameters that minimize some objective function,
    which is usually the sum of squared errors (SSE). This is more of a direct approach
    than MLE, as it doesn’t make as many assumptions about the underlying probability
    distribution of the errors. It simply tries to make the distance from the data
    points to the predicted plane as small as possible, in a squared sense to penalize
    larger errors more severely. The geometric interpretation is that you’re tilting
    and moving the plane (colX) to minimize the sum of the squares of the perpendicular
    distances (the dotted lines) from the actual points (y) to the plane.
  prefs: []
  type: TYPE_NORMAL
- en: '**Comparing the Two:**'
  prefs: []
  type: TYPE_NORMAL
- en: Although the procedures differ — one being a probability-based method and the
    other an optimization technique — they often yield the same result in the case
    of OLS. This is because when the errors are normally distributed, the MLE for
    the coefficients of a linear model leads to the same equations as minimizing the
    sum of squared errors. In the visual, both methods are effectively trying to position
    the same plane in the space of the variables X1 and X2 such that it best represents
    the relationship with y.
  prefs: []
  type: TYPE_NORMAL
- en: The main difference is in interpretation and potential generalization. MLE’s
    framework allows for more flexibility in modeling the error structure and can
    be extended to models where errors are not assumed to be normally distributed.
    The ML approach is typically more straightforward and computationally direct,
    focusing solely on the sum of the squared differences without concerning itself
    with the underlying probability distribution.
  prefs: []
  type: TYPE_NORMAL
- en: In summary, while MLE and ML minimization approaches might arrive at the same
    coefficients for an OLS regression, they are conceptually distinct. MLE is probabilistic
    and rooted in the likelihood of observing the data under a given model, while
    ML minimization is algorithmic, focusing on the direct reduction of error. The
    geometric visual remains the same for both, but the rationale behind the position
    of the plane differs.
  prefs: []
  type: TYPE_NORMAL
- en: '**Bonus: What happens when you introduce Regularization into the above interpretation?**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Regularization is a technique used to prevent overfitting in models, which
    can occur when a model is too complex and starts to capture the noise in the data
    rather than just the true underlying pattern. There are different types of regularization,
    but the two most common are:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Lasso Regression (L1 regularization):** This adds a penalty equal to the
    absolute value of the magnitude of coefficients. It can reduce some coefficients
    to zero, effectively performing feature selection.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Ridge Regression (L2 regularization):** This adds a penalty equal to the
    square of the magnitude of coefficients. All coefficients are shrunk by the same
    factor and none are zeroed out.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s use the example of fitting a blanket (representing our regression model)
    over a bed (representing our data). In OLS without regularization, we are trying
    to fit the blanket so that it touches as many points (data points) on the bed
    as possible, minimizing the distance between the blanket and the bed’s surface
    (the errors).
  prefs: []
  type: TYPE_NORMAL
- en: Now, imagine if the bed is quite bumpy and our blanket is very flexible. Without
    regularization, we might fit the blanket so snugly that it fits every single bump
    and dip, even the small ones that are just due to someone not smoothing out the
    bedspread — this is overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: 'Introducing Regularization:'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**With Lasso (L1):** It’s like saying, “I want the blanket to fit well, but
    I also want it to be smooth with as few folds as possible.” Each fold represents
    a feature in our model, and L1 regularization tries to minimize the number of
    folds. In the end, you’ll have a blanket that fits the bed well but might not
    get into every nook and cranny, especially if those are just noise. In the geometric
    visual, Lasso would try to keep the plane (colX) well-fitted to the points but
    might flatten out in the direction of less important variables (shrinking coefficients
    to zero).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**With Ridge (L2):** This is like wanting a snug fit but also wanting the blanket
    to be evenly spread out without any part being too far from the bed. So even though
    the blanket still fits the bed closely, it won’t get overly contorted to fit the
    minor bumps. In the geometric visual, Ridge adds a penalty that constrains the
    coefficients, shrinking them towards zero but not exactly to zero. This keeps
    the plane close to the points but prevents it from tilting too sharply to fit
    any particular points too closely, thus maintaining a bit of a distance (bias)
    to prevent overfitting to the noise.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Visual Interpretation with Regularization:'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'When regularization is added to the geometrical interpretation:'
  prefs: []
  type: TYPE_NORMAL
- en: The plane (colX) may no longer pass as close to each individual data point (y)
    as it did before. Regularization introduces a bit of bias on purpose.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The plane will tend to be more stable and less tilted towards any individual
    outlier points, as the penalty for having large coefficients means the model can’t
    be too sensitive to any single data point or feature.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The length of the vectors (Beta1X1 and Beta2X2) might be shorter, reflecting
    the fact that the influence of each variable on the prediction is being deliberately
    restrained.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In essence, regularization trades off a little bit of the model’s ability to
    fit the training data perfectly in return for improved model generalization, which
    means it will perform better on unseen data, not just the data it was trained
    on. It’s like choosing a slightly looser blanket fit that’s good enough for all
    practical purposes, rather than one that fits every single contour but might be
    impractical or too specific to just one bed.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In summary, the geometric interpretation of linear regression bridges the gap
    between classical statistics and machine learning, offering an intuitive understanding
    of this fundamental technique. While classical statistics approach linear regression
    through Ordinary Least Squares and machine learning often employs Maximum Likelihood
    Estimation or objective function minimization, both methods ultimately seek to
    minimize prediction error in a visually comprehensible way.
  prefs: []
  type: TYPE_NORMAL
- en: The introduction of regularization techniques like Lasso and Ridge further enriches
    this interpretation, highlighting the balance between model accuracy and generalizability.
    These methods prevent overfitting, ensuring that the model remains robust and
    effective for new, unseen data.
  prefs: []
  type: TYPE_NORMAL
- en: Overall, this geometric perspective not only demystifies linear regression but
    also underscores the importance of foundational concepts in the evolving landscape
    of data analysis and machine learning. It’s a powerful reminder of how complex
    algorithms can be rooted in simple, yet profound, geometric principles.
  prefs: []
  type: TYPE_NORMAL
