- en: Geometrical Interpretation of Linear Regression in Machine Learning versus Classical
    Statistics
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/geometrical-interpretation-of-linear-regression-in-machine-learning-versus-classical-statistics-bc39409efb72?source=collection_archive---------4-----------------------#2023-12-12](https://towardsdatascience.com/geometrical-interpretation-of-linear-regression-in-machine-learning-versus-classical-statistics-bc39409efb72?source=collection_archive---------4-----------------------#2023-12-12)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**Demystifying the confusion about Linear Regression Visually and Analytically**'
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@rishabh29288?source=post_page-----bc39409efb72--------------------------------)[![Rishabh
    Raman](../Images/041c1c6dd8713bbd2fab5408d5236213.png)](https://medium.com/@rishabh29288?source=post_page-----bc39409efb72--------------------------------)[](https://towardsdatascience.com/?source=post_page-----bc39409efb72--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----bc39409efb72--------------------------------)
    [Rishabh Raman](https://medium.com/@rishabh29288?source=post_page-----bc39409efb72--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F12323e77e1e2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgeometrical-interpretation-of-linear-regression-in-machine-learning-versus-classical-statistics-bc39409efb72&user=Rishabh+Raman&userId=12323e77e1e2&source=post_page-12323e77e1e2----bc39409efb72---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----bc39409efb72--------------------------------)
    ·9 min read·Dec 12, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fbc39409efb72&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgeometrical-interpretation-of-linear-regression-in-machine-learning-versus-classical-statistics-bc39409efb72&user=Rishabh+Raman&userId=12323e77e1e2&source=-----bc39409efb72---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fbc39409efb72&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgeometrical-interpretation-of-linear-regression-in-machine-learning-versus-classical-statistics-bc39409efb72&source=-----bc39409efb72---------------------bookmark_footer-----------)![](../Images/facef23d7c152cc1e5e168e448bdcba6.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: 'Image: Linear regression illustration, by Stpasha, via Wikimedia Commons (Public
    Domain). Original Image Link: [https://upload.wikimedia.org/wikipedia/commons/8/87/OLS_geometric_interpretation.svg](https://upload.wikimedia.org/wikipedia/commons/8/87/OLS_geometric_interpretation.svg)'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: 'The above image represents a geometric interpretation of Ordinary Least Squares
    (OLS) or Linear Regression (words used interchangeably in classical statistics).
    Let’s break down what we’re seeing in an intuitive way:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 上述图像表示了普通最小二乘法（OLS）或线性回归的几何解释（在经典统计中这两个词可以互换使用）。让我们以直观的方式解析我们所看到的内容：
- en: '**Variables (X1 and X2):** Imagine you have two variables, X1 and X2\. These
    could represent anything, like the hours you study and the number of practice
    exams you take, respectively.'
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**变量（X1和X2）：** 想象你有两个变量X1和X2。这些变量可以代表任何东西，比如你学习的小时数和你参加的练习考试次数。'
- en: '**Data Points (y):** Now, you have your outcome or what you’re trying to predict,
    which we call ‘y’. In our example, this could be your score on the actual exam.'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据点（y）：** 现在，你有了你的结果或你试图预测的内容，我们称之为‘y’。在我们的例子中，这可能是你在实际考试中的成绩。'
- en: '**Plane (colX):** The plane represents all possible predicted values that you
    can get by combining different amounts of your variables X1 and X2\. In our example,
    it could represent all possible exam scores you might predict based on different
    amounts of studying and practice exams.'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**平面（colX）：** 平面代表通过组合不同数量的变量X1和X2可以得到的所有可能预测值。在我们的例子中，它可能代表基于不同的学习时间和练习考试次数你可能预测的所有可能考试成绩。'
- en: '**Estimated Coefficients (Beta1 and Beta2):** These are the best guesses the
    OLS method makes for how much each variable affects your score. So, Beta 1 might
    tell you how much your score is predicted to increase for each extra hour of studying,
    and Beta 2 might tell you how much it increases for each additional practice exam
    you take.'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**估计系数（Beta1和Beta2）：** 这些是OLS方法对每个变量对你的成绩影响程度的最佳猜测。因此，Beta1可能告诉你每多学一个小时成绩预计会增加多少，而Beta2可能告诉你每增加一个练习考试成绩会增加多少。'
- en: '**Predicted Point (XB^):** This is the predicted score you’d get based on the
    estimated coefficients. It lies on the plane because it’s a combination of your
    variables X1 and X2 using the estimates from OLS.'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**预测点（XB^）：** 这是基于估计系数你得到的预测成绩。它位于平面上，因为它是使用OLS估计的变量X1和X2的组合。'
- en: '**Actual Point (y):** This is your actual exam score.'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**实际点（y）：** 这是你的实际考试成绩。'
- en: '**Error (ε):** This is the difference between your actual score and the predicted
    score. In other words, it’s how off the prediction was from reality.'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**误差（ε）：** 这是你实际成绩和预测成绩之间的差异。换句话说，就是预测与现实的偏差。'
- en: '**Now, how does OLS work with all this?**'
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**那么，OLS如何运作呢？**'
- en: OLS tries to find the values for Beta1 and Beta 2 so that when you predict ‘y’
    (the exam score) using X1 and X2 (study hours and practice exams), the error (ε)
    is as small as possible for all your data points. In the graphic, it’s like adjusting
    the plane until the vertical dotted lines (which represent errors) are collectively
    as short as they can be. The shortest distance from the actual data point (y)
    to the plane (colX) is always a straight line that’s perpendicular to the plane.
    OLS finds the particular plane where these perpendicular distances are minimized
    for all points.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: OLS尝试找到Beta1和Beta2的值，使得当你使用X1和X2（学习小时数和练习考试次数）来预测‘y’（考试成绩）时，所有数据点的误差（ε）尽可能小。在图形中，这就像是调整平面，直到垂直虚线（表示误差）总长度尽可能短。实际数据点（y）到平面（colX）的最短距离总是垂直于平面的直线。OLS找到一个特定的平面，使这些垂直距离对于所有点都达到最小。
- en: In other words, OLS is trying to “fit” the plane as close as possible to your
    actual scores, while recognizing that it won’t usually pass through all the actual
    points because real life is rarely that perfect.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，OLS试图“拟合”平面，使其尽可能接近你的实际成绩，同时认识到它通常不会通过所有实际点，因为现实生活中很少有那么完美的情况。
- en: '**It’s like fitting the best sheet of paper beneath a scatter of pencil dots
    such that the paper is as close as possible to all the dots at the same time.**'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '**这就像把最好的纸张贴在一群铅笔点下面，使纸张尽可能接近所有点。**'
- en: 'Let’s go over the main assumptions of OLS and connect them with the visual
    above:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回顾OLS的主要假设，并将其与上述视觉图像联系起来：
- en: '**1\. Linearity**'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '**1\. 线性性**'
- en: '**Assumption:** The relationship between the independent variables (X1, X2)
    and the dependent variable (y) is linear.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '**假设：** 自变量（X1，X2）与因变量（y）之间的关系是线性的。'
- en: '**Visual Interpretation:** In the image, this is why we use a plane (colX)
    to represent the combination of X1 and X2\. If the relationship were not linear,
    we wouldn’t be able to represent it with a flat plane; it would be curved or of
    some other shape.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '**视觉解释：** 在图像中，这就是我们使用平面（colX）来表示 X1 和 X2 组合的原因。如果关系不是线性的，我们无法用平面来表示，它会是曲线或其他形状。'
- en: '**2\. Independence**'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '**2\. 独立性**'
- en: '**Assumption:** Observations are independent of each other.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '**假设：** 观察值彼此独立。'
- en: '**Visual Interpretation:** Each data point (representing an observation) is
    plotted independently of the others. If there was dependence, we would see a systematic
    pattern in the errors (ε), like them all lying on one side of the plane, which
    would suggest that the way one data point is positioned could predict another,
    violating this assumption.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '**视觉解释：** 每个数据点（表示一个观察值）是独立绘制的。如果存在依赖性，我们会在误差（ε）中看到系统的模式，比如它们都集中在平面的一侧，这表明一个数据点的位置可能预测另一个，违反了这一假设。'
- en: '**3\. Homoscedasticity**'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '**3\. 同方差性**'
- en: '**Assumption:** The variance of the error terms (ε) is constant across all
    levels of the independent variables.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '**假设：** 误差项（ε）的方差在所有自变量水平上保持不变。'
- en: '**Visual Interpretation:** Ideally, the perpendicular distances from the actual
    data points (y) to the prediction plane (colX) should be uniformly scattered.
    There shouldn’t be any funnel shape or pattern in these distances; they should
    look random. If the errors get larger or smaller as X1 or X2 increases, this would
    violate homoscedasticity.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '**视觉解释：** 理想情况下，实际数据点（y）到预测平面（colX）的垂直距离应该均匀分布。这些距离不应有漏斗形状或模式；它们应显得随机。如果误差随着
    X1 或 X2 的增加而增大或减小，这将违反同方差性。'
- en: '**4\. No perfect multicollinearity**'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '**4\. 无完美多重共线性**'
- en: '**Assumption:** The independent variables are not perfectly correlated with
    each other.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '**假设：** 自变量之间没有完全相关性。'
- en: '**Visual Interpretation:** In the diagram, X1 and X2 are represented by two
    arrows pointing in different directions. If they were perfectly correlated, they
    would point in exactly the same direction, and we would not have a plane but a
    line. This would make it impossible to estimate the unique effect of X1 and X2
    on y.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '**视觉解释：** 在图示中，X1 和 X2 由两个方向不同的箭头表示。如果它们完全相关，它们会指向完全相同的方向，我们将不会有平面而是直线。这将使我们无法估计
    X1 和 X2 对 y 的独特影响。'
- en: '**5\. No auto-correlation**'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '**5\. 无自相关性**'
- en: '**Assumption:** The error terms are not correlated with each other.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '**假设：** 误差项之间不相关。'
- en: '**Visual Interpretation:** This assumption is about the error terms, which
    are not explicitly shown in the image, but we infer that each error term (ε) is
    random and not influenced by the previous or next error term. If there was a pattern
    (like if one error was always followed by another error of similar size), we would
    suspect auto-correlation.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '**视觉解释：** 这个假设涉及误差项，虽然在图像中没有明确显示，但我们推断每个误差项（ε）是随机的，不受前一个或下一个误差项的影响。如果存在模式（例如，一个误差总是跟随另一个相似大小的误差），我们会怀疑自相关性。'
- en: '**6\. Exogeneity**'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '**6\. 外生性**'
- en: '**Assumption:** The error terms have an expected value of zero.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '**假设：** 误差项的期望值为零。'
- en: '**Visual Interpretation:** This means that the plane should be positioned such
    that the errors, on average, cancel each other out. Some data points will be above
    the plane, and some below, but there’s no systematic bias making them all above
    or below.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '**视觉解释：** 这意味着平面应当被定位，使得误差在平均值上互相抵消。一些数据点会在平面之上，一些会在平面之下，但没有系统性偏差使它们都在平面之上或之下。'
- en: '**7\. Normality of errors (often an assumption for hypothesis testing)**'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '**7\. 误差的正态性（通常是假设检验的假设）**'
- en: '**Assumption:** The error terms are normally distributed.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '**假设：** 误差项呈正态分布。'
- en: '**Visual Interpretation:** While the normality assumption is not something
    we can visualize in a 3D plot of the data and the model, if we were to look at
    a histogram of the error terms, we would expect to see the familiar bell curve
    of a normal distribution.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '**视觉解释：** 虽然正态性假设在数据和模型的三维图中无法直观呈现，但如果我们查看误差项的直方图，我们期望看到正态分布的熟悉钟形曲线。'
- en: How does Linear Regression in Machine Learning Universe differ from Ordinary
    Least Squares based Linear Regression in Classical Statistics?
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 线性回归在机器学习宇宙中如何不同于经典统计学中的普通最小二乘法线性回归？
- en: In classical statistics, Ordinary Least Squares (OLS) can be approached through
    the lens of Maximum Likelihood Estimation (MLE). Both MLE and OLS aim to find
    the best parameters for a model, but they come from different philosophies and
    use different methods to achieve this.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在经典统计学中，普通最小二乘法（OLS）可以通过最大似然估计（MLE）的视角来接近。MLE和OLS都旨在找到模型的最佳参数，但它们来自不同的理论，并使用不同的方法来实现这一目标。
- en: '**Maximum Likelihood Estimation (MLE) Approach:**'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '**最大似然估计 (MLE) 方法：**'
- en: 'MLE is based on probability. It asks the question: “Given a set of data points,
    what are the most likely parameters of the model that could have generated this
    data?” MLE assumes a certain probability distribution for the errors (often a
    normal distribution) and then finds the parameter values that maximize the likelihood
    of observing the actual data. In the geometric visual, this is akin to adjusting
    the angle and position of the plane (colX) in such a way that the probability
    of seeing the actual data points (y) is the highest. The likelihood incorporates
    not just the distances from the points to the plane (the errors) but also the
    shape of the error distribution.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: MLE基于概率。它提出的问题是：“给定一组数据点，什么是最可能的模型参数，这些参数能够生成这些数据？”MLE假设误差具有某种概率分布（通常是正态分布），然后找到最大化观察到实际数据的可能性的参数值。在几何视觉中，这类似于调整平面（colX）的角度和位置，使得看到实际数据点（y）的概率最高。似然不仅考虑点到平面（误差）的距离，还考虑误差分布的形状。
- en: '**Minimization of an Objective Function in Machine Learning (ML):**'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '**机器学习 (ML) 中目标函数的最小化：**'
- en: On the other hand, ML approaches typically frame regression as an optimization
    problem. The goal is to find the parameters that minimize some objective function,
    which is usually the sum of squared errors (SSE). This is more of a direct approach
    than MLE, as it doesn’t make as many assumptions about the underlying probability
    distribution of the errors. It simply tries to make the distance from the data
    points to the predicted plane as small as possible, in a squared sense to penalize
    larger errors more severely. The geometric interpretation is that you’re tilting
    and moving the plane (colX) to minimize the sum of the squares of the perpendicular
    distances (the dotted lines) from the actual points (y) to the plane.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，ML方法通常将回归框架作为优化问题。目标是找到那些使某个目标函数（通常是平方误差总和（SSE））最小的参数。这是一种比MLE更直接的方法，因为它对误差的底层概率分布假设较少。它只是试图在平方意义上将数据点到预测平面的距离尽可能小，以更严厉地惩罚较大的误差。几何解释是你在倾斜和移动平面（colX），以最小化从实际点（y）到平面的垂直距离的平方和（虚线）。
- en: '**Comparing the Two:**'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '**两者比较：**'
- en: Although the procedures differ — one being a probability-based method and the
    other an optimization technique — they often yield the same result in the case
    of OLS. This is because when the errors are normally distributed, the MLE for
    the coefficients of a linear model leads to the same equations as minimizing the
    sum of squared errors. In the visual, both methods are effectively trying to position
    the same plane in the space of the variables X1 and X2 such that it best represents
    the relationship with y.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管程序有所不同——一种是基于概率的方法，另一种是优化技术——但在OLS的情况下，它们往往会产生相同的结果。这是因为当误差服从正态分布时，线性模型系数的MLE会得出与最小化平方误差总和相同的方程。从视觉上看，两种方法实际上都是在尝试在变量X1和X2的空间中定位同一个平面，以便它最好地表示与y的关系。
- en: The main difference is in interpretation and potential generalization. MLE’s
    framework allows for more flexibility in modeling the error structure and can
    be extended to models where errors are not assumed to be normally distributed.
    The ML approach is typically more straightforward and computationally direct,
    focusing solely on the sum of the squared differences without concerning itself
    with the underlying probability distribution.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 主要区别在于解释和潜在的泛化。MLE的框架允许在建模误差结构时有更多灵活性，并且可以扩展到误差不被假设为正态分布的模型中。ML方法通常更直接，专注于平方差的总和，而不关注底层概率分布。
- en: In summary, while MLE and ML minimization approaches might arrive at the same
    coefficients for an OLS regression, they are conceptually distinct. MLE is probabilistic
    and rooted in the likelihood of observing the data under a given model, while
    ML minimization is algorithmic, focusing on the direct reduction of error. The
    geometric visual remains the same for both, but the rationale behind the position
    of the plane differs.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，虽然最大似然估计 (MLE) 和最小化方法可能会得到相同的最小二乘回归系数，但它们在概念上是不同的。MLE 是概率性的，根植于在给定模型下观察数据的可能性，而最小化方法是算法性的，专注于直接减少误差。几何视觉对两者来说是相同的，但平面位置的原理不同。
- en: '**Bonus: What happens when you introduce Regularization into the above interpretation?**'
  id: totrans-53
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**附加：当你在上述解释中引入正则化时会发生什么？**'
- en: 'Regularization is a technique used to prevent overfitting in models, which
    can occur when a model is too complex and starts to capture the noise in the data
    rather than just the true underlying pattern. There are different types of regularization,
    but the two most common are:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 正则化是一种用于防止模型过拟合的技术，过拟合可能发生在模型过于复杂，开始捕捉数据中的噪声而不仅仅是数据的真实底层模式。正则化有不同的类型，但最常见的两种是：
- en: '**Lasso Regression (L1 regularization):** This adds a penalty equal to the
    absolute value of the magnitude of coefficients. It can reduce some coefficients
    to zero, effectively performing feature selection.'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**套索回归 (L1 正则化)：** 这增加了一个等于系数绝对值的惩罚项。它可以将一些系数减少为零，有效地进行特征选择。'
- en: '**Ridge Regression (L2 regularization):** This adds a penalty equal to the
    square of the magnitude of coefficients. All coefficients are shrunk by the same
    factor and none are zeroed out.'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**岭回归 (L2 正则化)：** 这增加了一个等于系数平方的惩罚项。所有系数都被相同的因子缩小，没有一个被归零。'
- en: Let’s use the example of fitting a blanket (representing our regression model)
    over a bed (representing our data). In OLS without regularization, we are trying
    to fit the blanket so that it touches as many points (data points) on the bed
    as possible, minimizing the distance between the blanket and the bed’s surface
    (the errors).
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们用将被子（代表我们的回归模型）铺在床上（代表我们的数据）的例子来说明。在没有正则化的最小二乘法中，我们试图将被子铺得尽可能多地接触床上的点（数据点），最小化被子与床面之间的距离（误差）。
- en: Now, imagine if the bed is quite bumpy and our blanket is very flexible. Without
    regularization, we might fit the blanket so snugly that it fits every single bump
    and dip, even the small ones that are just due to someone not smoothing out the
    bedspread — this is overfitting.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，想象一下床非常不平坦，而我们的被子非常柔韧。如果没有正则化，我们可能会把被子贴得非常紧，以至于它适应了每一个小凸起和凹陷，甚至是由于有人没有铺平床单的小问题——这就是过拟合。
- en: 'Introducing Regularization:'
  id: totrans-59
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 引入正则化：
- en: '**With Lasso (L1):** It’s like saying, “I want the blanket to fit well, but
    I also want it to be smooth with as few folds as possible.” Each fold represents
    a feature in our model, and L1 regularization tries to minimize the number of
    folds. In the end, you’ll have a blanket that fits the bed well but might not
    get into every nook and cranny, especially if those are just noise. In the geometric
    visual, Lasso would try to keep the plane (colX) well-fitted to the points but
    might flatten out in the direction of less important variables (shrinking coefficients
    to zero).'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**使用套索回归 (L1)：** 就像是说：“我希望被子合适，但也希望它尽量平整，褶皱尽可能少。”每一个褶皱代表模型中的一个特征，而 L1 正则化尝试最小化褶皱的数量。最后，你会有一个合适的被子，但可能不会进入每一个缝隙，尤其是那些仅仅是噪声的缝隙。在几何视觉中，套索回归会试图让平面（colX）很好地贴合点，但可能会在不重要的变量方向上变得平坦（将系数缩小为零）。'
- en: '**With Ridge (L2):** This is like wanting a snug fit but also wanting the blanket
    to be evenly spread out without any part being too far from the bed. So even though
    the blanket still fits the bed closely, it won’t get overly contorted to fit the
    minor bumps. In the geometric visual, Ridge adds a penalty that constrains the
    coefficients, shrinking them towards zero but not exactly to zero. This keeps
    the plane close to the points but prevents it from tilting too sharply to fit
    any particular points too closely, thus maintaining a bit of a distance (bias)
    to prevent overfitting to the noise.'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**使用岭回归 (L2)：** 这就像是想要被子贴合得很好，但同时也希望被子均匀展开，没有任何部分离床太远。因此，即使被子仍然紧贴床面，它也不会过度扭曲以适应细微的凸起。在几何视觉中，岭回归添加了一个惩罚项，约束了系数，使它们缩小到接近零但不完全为零。这保持了平面接近点，但防止它倾斜得太厉害以适应特定点，从而保持了一定的距离（偏差）以防止对噪声的过拟合。'
- en: 'Visual Interpretation with Regularization:'
  id: totrans-62
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 带有正则化的视觉解释：
- en: 'When regularization is added to the geometrical interpretation:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 当将正则化添加到几何解释中时：
- en: The plane (colX) may no longer pass as close to each individual data point (y)
    as it did before. Regularization introduces a bit of bias on purpose.
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 平面（colX）可能不会像之前那样靠近每个单独的数据点（y）。正则化故意引入了一些偏差。
- en: The plane will tend to be more stable and less tilted towards any individual
    outlier points, as the penalty for having large coefficients means the model can’t
    be too sensitive to any single data point or feature.
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 平面将趋向于更加稳定，较少倾斜于任何单个异常值点，因为对于大系数的惩罚意味着模型不能过于敏感于任何单一的数据点或特征。
- en: The length of the vectors (Beta1X1 and Beta2X2) might be shorter, reflecting
    the fact that the influence of each variable on the prediction is being deliberately
    restrained.
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 向量（Beta1X1 和 Beta2X2）的长度可能会更短，反映了每个变量对预测的影响被故意限制的事实。
- en: In essence, regularization trades off a little bit of the model’s ability to
    fit the training data perfectly in return for improved model generalization, which
    means it will perform better on unseen data, not just the data it was trained
    on. It’s like choosing a slightly looser blanket fit that’s good enough for all
    practical purposes, rather than one that fits every single contour but might be
    impractical or too specific to just one bed.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 从本质上讲，正则化在让模型的训练数据拟合能力稍微打折的同时，换取了改进的模型泛化能力，这意味着模型在未见过的数据上表现更好，而不仅仅是在训练数据上。就像选择一个稍微宽松的毯子尺寸，它在所有实际用途上都足够好，而不是一个完美贴合每一个轮廓但可能不切实际或过于特定于一个床的尺寸。
- en: Conclusion
  id: totrans-68
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: In summary, the geometric interpretation of linear regression bridges the gap
    between classical statistics and machine learning, offering an intuitive understanding
    of this fundamental technique. While classical statistics approach linear regression
    through Ordinary Least Squares and machine learning often employs Maximum Likelihood
    Estimation or objective function minimization, both methods ultimately seek to
    minimize prediction error in a visually comprehensible way.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，线性回归的几何解释弥合了经典统计学与机器学习之间的差距，提供了对这一基本技术的直观理解。虽然经典统计学通过普通最小二乘法（Ordinary Least
    Squares）来处理线性回归，而机器学习则通常使用最大似然估计（Maximum Likelihood Estimation）或目标函数最小化，但这两种方法最终都旨在以视觉上易于理解的方式最小化预测误差。
- en: The introduction of regularization techniques like Lasso and Ridge further enriches
    this interpretation, highlighting the balance between model accuracy and generalizability.
    These methods prevent overfitting, ensuring that the model remains robust and
    effective for new, unseen data.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 引入像Lasso和Ridge这样的正则化技术进一步丰富了这种解释，突出了模型准确性和泛化能力之间的平衡。这些方法防止了过拟合，确保模型对新的、未见过的数据保持稳健和有效。
- en: Overall, this geometric perspective not only demystifies linear regression but
    also underscores the importance of foundational concepts in the evolving landscape
    of data analysis and machine learning. It’s a powerful reminder of how complex
    algorithms can be rooted in simple, yet profound, geometric principles.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，这种几何视角不仅揭示了线性回归的奥秘，还强调了在不断发展的数据分析和机器学习领域中基础概念的重要性。这是一个强有力的提醒，表明复杂的算法可以根植于简单而深刻的几何原理中。
