- en: 7 Ways to Monitor Large Language Model Behavior
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/7-ways-to-monitor-large-language-model-behavior-25c267d58f06?source=collection_archive---------2-----------------------#2023-07-28](https://towardsdatascience.com/7-ways-to-monitor-large-language-model-behavior-25c267d58f06?source=collection_archive---------2-----------------------#2023-07-28)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Seven ways to track the evolution of LLMs with LangKit and WhyLabs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://felipe-p-adachi.medium.com/?source=post_page-----25c267d58f06--------------------------------)[![Felipe
    de Pontes Adachi](../Images/58c9544ae85f43548c5e5b56fda31bb4.png)](https://felipe-p-adachi.medium.com/?source=post_page-----25c267d58f06--------------------------------)[](https://towardsdatascience.com/?source=post_page-----25c267d58f06--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----25c267d58f06--------------------------------)
    [Felipe de Pontes Adachi](https://felipe-p-adachi.medium.com/?source=post_page-----25c267d58f06--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fa038269245d5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F7-ways-to-monitor-large-language-model-behavior-25c267d58f06&user=Felipe+de+Pontes+Adachi&userId=a038269245d5&source=post_page-a038269245d5----25c267d58f06---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----25c267d58f06--------------------------------)
    ·12 min read·Jul 28, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F25c267d58f06&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F7-ways-to-monitor-large-language-model-behavior-25c267d58f06&user=Felipe+de+Pontes+Adachi&userId=a038269245d5&source=-----25c267d58f06---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F25c267d58f06&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F7-ways-to-monitor-large-language-model-behavior-25c267d58f06&source=-----25c267d58f06---------------------bookmark_footer-----------)![](../Images/a9947c80cb8d3a04e7485914f005fd01.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Photo by [Jéan Béller](https://unsplash.com/@chinatravelchannel?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    on [Unsplash](https://unsplash.com/photos/a-close-up-of-a-clock-on-a-building-FyOEhy91_7o?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
  prefs: []
  type: TYPE_NORMAL
- en: The world of Natural Language Processing has seen a rapid evolution with the
    use of Large Language Models (LLMs). Through their impressive text generation
    and text understanding abilities, LLMs have gained a large adoption worldwide.
  prefs: []
  type: TYPE_NORMAL
- en: ChatGPT is perhaps the most well-known of these models, boasting 57 million
    monthly active users within the first month of availability [1]. Along with its
    impressive capabilities across multiple scenarios, the model also comes with big
    challenges, such as the tendency to hallucinate and generate biased or harmful
    content [2,3]. Another challenging area is observability — with the rapid collection
    of user feedback, ChatGPT is being continuously retrained and improved through
    Reinforcement Learning from Human Feedback (RLHF) [4], making its evaluation a
    moving target. It is well-known that overall improvements from RLHF can lead to
    performance regressions on specific tasks [5]. How can we ensure that the model
    behaves as expected and maintains acceptable performance within the tasks that
    are relevant to our application?
  prefs: []
  type: TYPE_NORMAL
- en: In this blog, we will discuss seven groups of metrics you can use to keep track
    of LLM’s behaviors. We will calculate these metrics for ChatGPT’s responses for
    a fixed set of 200 prompts across 35 days and track how ChatGPT’s behavior evolves
    within the period. Our focus task will be long-form question answering, and we
    will use LangKit and WhyLabs to calculate, track and monitor the model’s behavior
    across time.
  prefs: []
  type: TYPE_NORMAL
- en: You can check the resulting dashboard for this project in [WhyLabs](https://hub.whylabsapp.com/resources/demo-chatgpt-behavior-ELI5/columns/response.difficult_words?dateRange=2023-03-05-to-2023-04-09&targetOrgId=demo&sessionToken=session-8gcsnbVy)
    (no sign up required) and run the complete example yourself by running this [Colab
    Notebook](https://colab.research.google.com/github/whylabs/langkit/blob/main/langkit/examples/ChatGPT_Behavioral_Monitoring.ipynb).
  prefs: []
  type: TYPE_NORMAL
- en: Agenda
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[The Task — Comprehensible Question Answering](#d481)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Popular LLM Metrics](#3ec3)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1\. [ROUGE](#e1df)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 2\. [Gender Bias](#fbc5)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 3\. [Text Quality](#a1b8)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 4\. [Semantic Similarity](#6f7e)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 5\. [Regex Patterns](#eb87)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 6\. [Refusals](#f705)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 7\. [Toxicity and Sentiment](#4197)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[Monitoring Across Time](#f4a6)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[So, has behavior changed?](#a5d9)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Conclusion](#9787)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The task — Comprehensible Question Answering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For this example, let’s use the Explain Like I’m Five (ELI5) dataset [6], a
    question-answering dataset that contains open-ended questions — questions that
    require a longer response and cannot be answered with a “yes” or “no” — and the
    answers should be simple and easily comprehensible by beginners.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the work presented in [ChatLog: Recording and Analyzing ChatGPT Across Time](https://arxiv.org/pdf/2304.14106.pdf),
    1000 questions were sampled from this dataset and repeatedly sent to ChatGPT every
    day from March 5 to April 9, 2023, which is available in [ChatLog’s Repository](https://github.com/THU-KEG/ChatLog).
    We’ll use this dataset by sampling 200 out of the original 1000 questions, along
    with ChatGPT’s answers and human reference answers, for each day of the given
    period. That way, we’ll end up with 35 daily dataframes, where each dataframe
    has 200 rows with the following columns:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2299ae9670cd9cb23d9a3ee4acf62953.png)'
  prefs: []
  type: TYPE_IMG
- en: Table by author
  prefs: []
  type: TYPE_NORMAL
- en: Popular LLM metrics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It can be a daunting task to define a set of metrics to properly evaluate a
    model with such a wide range of capabilities as ChatGPT. In this example, we’ll
    cover some examples of metrics that are relatively general and could be useful
    for a range of applications, such as text quality, sentiment analysis, toxicity,
    and text semantic similarity, and others that are particular for certain tasks
    like question answering and summarization, like the ROUGE group of metrics.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are a multitude of other metrics and approaches that might be more relevant,
    depending on the particular application you are interested in. If you’re looking
    for more examples of what to monitor, here are three papers that served as an
    inspiration for the writing of this blog: [Holistic Evaluation of Language Models](https://arxiv.org/pdf/2211.09110.pdf),
    [ChatLog: Recording and Analyzing ChatGPT Across Time](https://arxiv.org/pdf/2304.14106.pdf),
    and [Beyond Accuracy: Behavioral Testing of NLP Models with CheckList](https://homes.cs.washington.edu/~marcotcr/acl20_checklist.pdf).'
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s talk about the metrics we’re monitoring in this example. Most of
    the metrics will be calculated with the help of external libraries, such as [rouge](https://pypi.org/project/rouge/),
    [textstat](https://github.com/textstat/textstat), and [huggingface models](https://huggingface.co/models),
    and most of them are encapsulated in the [LangKit](https://github.com/whylabs/langkit)
    library, which is an open-source text metrics toolkit for monitoring language
    models. In the end, we want to group all the calculated metrics in a **whylogs**
    profile, which is a statistical summary of the original data. We will then send
    the daily profiles to the WhyLabs observability platform, where we can monitor
    them over time.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following table, we summarize the groups of metrics we will cover at
    the following sections:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a0e3c3ff27ca84760009f7ce2f3eae6a.png)'
  prefs: []
  type: TYPE_IMG
- en: Table by author
  prefs: []
  type: TYPE_NORMAL
- en: ROUGE
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Recall-Oriented Understudy for Gisting Evaluation (ROUGE) is a set of metrics
    commonly used in natural language processing to evaluate automatic summarization
    tasks by comparing the generated text with one or more reference summaries.
  prefs: []
  type: TYPE_NORMAL
- en: 'The task at hand is a question-answering problem rather than a summarization
    task, but we do have human answers as a reference, so we will use the ROUGE metrics
    to measure the similarity between the ChatGPT response and each of the three reference
    answers. We will use the [rouge](https://pypi.org/project/rouge/) python library
    to augment our dataframe with two different metrics: ROUGE-L, which takes into
    account the longest sequence overlap between the answers, and ROUGE-2, which takes
    into account the overlap of bigrams between the answers. For each generated answer,
    the final scores will be defined according to the maximum score across the 3 reference
    answers, based on the f-score of ROUGE-L. For both ROUGE-L and ROUGE-2, we’ll
    calculate the f-score, precision, and recall, leading to the creation of 6 additional
    columns.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This approach was based on the following paper: [ChatLog: Recording and Analyzing
    ChatGPT Across Time](https://arxiv.org/pdf/2304.14106.pdf)'
  prefs: []
  type: TYPE_NORMAL
- en: Gender bias
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Social bias is a central topic of discussion when it comes to fair and responsible
    AI [2],[7], which can be defined as “a systematic asymmetry in language choice”
    [8]. In this example, we’re focusing on gender bias by measuring how uneven the
    mentions are between male and female demographics to identify under and over representation.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will do so by counting the number of words that are included in both sets
    of words that are attributed to the female and male demographics. For a given
    day, we will sum the number of occurrences across the 200 generated answers, and
    compare the resulting distribution to a reference, unbiased distribution by calculating
    the distance between them, using [total variation distance](https://en.wikipedia.org/wiki/Total_variation_distance_of_probability_measures).
    In the following code snippet, we can see the groups of words that were used to
    represent both demographics:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'This approach was based on the following paper: [Holistic Evaluation of Language
    Models](https://arxiv.org/pdf/2211.09110.pdf)'
  prefs: []
  type: TYPE_NORMAL
- en: Text quality
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Text quality metrics, such as readability, complexity, and grade level, can
    provide important insights into the quality and suitability of generated responses.
  prefs: []
  type: TYPE_NORMAL
- en: In LangKit, we can compute text quality metrics through the textstat module,
    which uses the [textstat](https://github.com/textstat/textstat) library to compute
    several different text quality metrics.
  prefs: []
  type: TYPE_NORMAL
- en: Semantic similarity
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Another important aspect to consider is the degree of irrelevant or off-topic
    responses given by the model, and how this evolves with time. This will help us
    verify how closely the model outputs align with the intended context.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will do so with the help of the [sentence-transformers](https://github.com/UKPLab/sentence-transformers)
    library, by calculating the dense vector representation for both question and
    answer. Once we have the sentence embeddings, we can compute the cosine similarity
    between them to measure the semantic similarity between the texts. LangKit’s **input_output**
    module will do just that for us. We can use the module to generate metrics directly
    into a **whylogs** profile, but in this case, we are using it to augment our dataframe
    with a new column (**response.relevance_to_prompt**), where each row contains
    the semantic similarity score between the question and response:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Regex patterns
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: An important aspect of LLM behavior is ensuring it doesn’t output sensitive
    or fake information. For example, if the user prompt is “I feel sad.”, we might
    be interested in knowing if the model’s response wrongly refer the user to an
    existing or non-existent telephone number.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s do that by searching for groups of regexes patterns to help detect the
    presence of information such as telephone numbers, credit card numbers, mailing
    addresses, SSNs, and others.
  prefs: []
  type: TYPE_NORMAL
- en: 'As with the previous metric, we will use LangKit to search through these patterns.
    In the complete example, we’re directly registering it as a whylogs metric, but
    you can also use it as a standalone function like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Refusals
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'By now, most of us should be familiar with the polite refusals LLMs give when
    asked about banned or controversial topics, which can go similar to this:'
  prefs: []
  type: TYPE_NORMAL
- en: '*I’m sorry, but I can’t assist with that request.*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'On a fixed set of prompts, an increase in these refusals can be a signal that
    our model has become overly cautious or sensitive. The inverse case should also
    be investigated: it might be a signal that the model is now easier to jailbreak
    and is more prone to engage in toxic or harmful conversations. For this reason,
    let’s calculate the semantic similarity (as described in the previous section
    of Semantic Similarity) of each generated answer with a fixed set of sentences:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: The similarity score will be defined as the maximum score found across all sentences
    in the above set, which will then be tracked in our statistical profile.
  prefs: []
  type: TYPE_NORMAL
- en: Toxicity and sentiment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Monitoring sentiment allows us to gauge the overall tone and emotional impact
    of the responses, while toxicity analysis provides an important measure of the
    presence of offensive, disrespectful, or harmful language in LLM outputs. Any
    shifts in sentiment or toxicity should be closely monitored to ensure the model
    is behaving as expected.
  prefs: []
  type: TYPE_NORMAL
- en: 'For sentiment analysis, we will track the scores provided by [**nltk**](https://www.nltk.org/)’s
    **SentimentIntensityAnalyzer**. As for the toxicity scores, we will use HuggingFace’s
    [martin-ha/toxic-comment-model](https://huggingface.co/martin-ha/toxic-comment-model)
    toxicity analyzer. Both are wrapped in LangKit’s sentiment and toxicity modules,
    such that we can use them directly like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Monitoring across time
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now that we defined the metrics we want to track, we need to wrap them all
    into a single profile and proceed to upload them to our monitoring dashboard.
    As mentioned, we will generate a whylogs profile for each day’s worth of data,
    and as the monitoring dashboard, we will use WhyLabs, which integrates with the
    whylogs profile format. We won’t show the complete code to do it in this post,
    but a simple version of how to upload a profile with langkit-enabled LLM metrics
    looks something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: By initializing **llm_metrics**, the whylogs profiling process will automatically
    calculate, among others, metrics such as text quality, semantic similarity, regex
    patterns, toxicity, and sentiment.
  prefs: []
  type: TYPE_NORMAL
- en: If you’re interested in the details of how it’s done, check the complete code
    in this [Colab Notebook](https://colab.research.google.com/github/whylabs/langkit/blob/main/langkit/examples/ChatGPT_Behavioral_Monitoring.ipynb)!
  prefs: []
  type: TYPE_NORMAL
- en: So, has behavior changed?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: TLDR; In general, it looks like it changed for the better, with a clear transition
    on Mar 23, 2023.
  prefs: []
  type: TYPE_NORMAL
- en: We won’t be able to show every graph in this blog — in total, there are 25 monitored
    features in our dashboard — but let’s take a look at some of them. For a complete
    experience, you’re welcome to explore the [project’s dashboard yourself](https://hub.whylabsapp.com/resources/demo-chatgpt-behavior-ELI5/columns/response.difficult_words?dateRange=2023-03-05-to-2023-04-09&targetOrgId=demo&sessionToken=session-8gcsnbVy).
  prefs: []
  type: TYPE_NORMAL
- en: Concerning the rouge metrics, over time, recall slightly decreases, while precision
    increases at the same proportion, keeping the f-score roughly equal. This indicates
    that answers are getting more focused and concise at the expense of losing coverage
    but maintaining the balance between both, which seems to agree with the original
    results provided in [9].
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d17da7cc60d2e1ea49c4de32efd6d339.png)'
  prefs: []
  type: TYPE_IMG
- en: ROUGE-L-R. Screenshot by author.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s take a look at one of the text quality metrics, **difficult words**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e1999df549eb5e1986217ed8e904c6e6.png)'
  prefs: []
  type: TYPE_IMG
- en: difficult words. Screenshot by author.
  prefs: []
  type: TYPE_NORMAL
- en: There’s a sharp decrease in the mean number of words that are considered difficult
    after March 23, which is a good sign, considering the goal is to make the answer
    easily comprehensible. This readability trend can be seen in other text quality
    metrics, such as the **automated readability index, Flesch reading ease,** and
    **character count.**
  prefs: []
  type: TYPE_NORMAL
- en: 'The semantic similarity also seems to timidly increase with time, as seen below:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8832cad8b5f369e163cc2b308ea8e5be.png)'
  prefs: []
  type: TYPE_IMG
- en: '*response.relevance_to_prompt*. Screenshot by author.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This indicates that the model’s responses are getting more aligned with the
    question’s context. This could have not been the case, though — in Tu, Shangqing,
    et al.[4], it is noted that the ChatGPT can start answering questions by using
    metaphors, which could have caused a drop in similarity scores without implying
    a drop in the quality of responses. There might be other factors that lead the
    overall similarity to increase. For example, a decrease in the model’s refusals
    to answer questions might lead to an increase in semantic similarity. This is
    actually the case, which can be seen by the **refusal_similarity** metric, as
    shown below:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7c434f19e387d0d7eac26349c52992fc.png)'
  prefs: []
  type: TYPE_IMG
- en: refusal similarity. Screenshot by author.
  prefs: []
  type: TYPE_NORMAL
- en: In all the graphics above, we can see a definite transition in behavior between
    March 23 and March 24\. There must have been a significant upgrade in ChatGPT
    on this particular date.
  prefs: []
  type: TYPE_NORMAL
- en: For the sake of brevity, we won’t be showing the remaining graphs, but let’s
    cover a few more metrics. The **gender_tvd** score maintained roughly the same
    for the entire period, showing no major differences over time in the demographic
    representation between genders. The sentiment score, on average, remained roughly
    the same, with a positive mean, while the toxicity’s mean was found to be very
    low across the entire period, indicating that the model hasn’t been showing particularly
    harmful or toxic behavior. Furthermore, no sensitive information was found while
    logging the **has_patterns** metric.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With such a diverse set of capabilities, tracking Large Language Model’s behavior
    can be a complex task. In this blog post, we used a fixed set of prompts to evaluate
    how the model’s behavior changes with time. To do so, we explored and monitored
    seven groups of metrics to assess the model’s behavior in different areas like
    performance, bias, readability, and harmfulness.
  prefs: []
  type: TYPE_NORMAL
- en: We have a brief discussion on the results in this blog, but we encourage the
    reader to explore the results by himself/herself!
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 1 — [https://www.engadget.com/chatgpt-100-million-users-january-130619073.html](https://www.engadget.com/chatgpt-100-million-users-january-130619073.html)
  prefs: []
  type: TYPE_NORMAL
- en: '2- Emily M Bender et al. “On the Dangers of Stochastic Parrots: Can Language
    Models Be Too Big?” In: Proceedings of the 2021 ACM conference on fairness, accountability,
    and transparency. 2021, pp. 610–623 (cit. on p. 2).'
  prefs: []
  type: TYPE_NORMAL
- en: '3 — Hussam Alkaissi and Samy I McFarlane. “Artificial hallucinations in chatgpt:
    Implications in scientific writing”. In: Cureus 15.2 (2023) (cit. on p. 2).'
  prefs: []
  type: TYPE_NORMAL
- en: '4 — Tu, Shangqing, et al. “ChatLog: Recording and Analyzing ChatGPT Across
    Time.” *arXiv preprint arXiv:2304.14106* (2023). [https://arxiv.org/pdf/2304.14106.pdf](https://arxiv.org/pdf/2304.14106.pdf)'
  prefs: []
  type: TYPE_NORMAL
- en: 5 — [https://cdn.openai.com/papers/Training_language_models_to_follow_instructions_with_human_feedback.pdf](https://cdn.openai.com/papers/Training_language_models_to_follow_instructions_with_human_feedback.pdf)
  prefs: []
  type: TYPE_NORMAL
- en: '6- Angela Fan, Yacine Jernite, Ethan Perez, David Grangier, Jason Weston, and
    Michael Auli. 2019\. [ELI5: Long Form Question Answering](https://aclanthology.org/P19-1346).
    In *Proceedings of the 57th Annual Meeting of the Association for Computational
    Linguistics*, pages 3558–3567, Florence, Italy. Association for Computational
    Linguistics.'
  prefs: []
  type: TYPE_NORMAL
- en: 7 — Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings
    — [https://doi.org/10.48550/arXiv.1607.06520](https://doi.org/10.48550/arXiv.1607.06520)
  prefs: []
  type: TYPE_NORMAL
- en: '8 — Beukeboom, C. J., & Burgers, C. (2019). How stereotypes are shared through
    language: A review and introduction of the Social Categories and Stereotypes Communication
    (SCSC) Framework. Review of Communication Research, 7, 1–37\. [https://doi.org/10.12840/issn.2255-4165.017](https://doi.org/10.12840/issn.2255-4165.017)'
  prefs: []
  type: TYPE_NORMAL
