- en: 7 Ways to Monitor Large Language Model Behavior
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 监控大型语言模型行为的7种方法
- en: 原文：[https://towardsdatascience.com/7-ways-to-monitor-large-language-model-behavior-25c267d58f06?source=collection_archive---------2-----------------------#2023-07-28](https://towardsdatascience.com/7-ways-to-monitor-large-language-model-behavior-25c267d58f06?source=collection_archive---------2-----------------------#2023-07-28)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/7-ways-to-monitor-large-language-model-behavior-25c267d58f06?source=collection_archive---------2-----------------------#2023-07-28](https://towardsdatascience.com/7-ways-to-monitor-large-language-model-behavior-25c267d58f06?source=collection_archive---------2-----------------------#2023-07-28)
- en: Seven ways to track the evolution of LLMs with LangKit and WhyLabs
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用LangKit和WhyLabs跟踪LLMs发展的七种方法
- en: '[](https://felipe-p-adachi.medium.com/?source=post_page-----25c267d58f06--------------------------------)[![Felipe
    de Pontes Adachi](../Images/58c9544ae85f43548c5e5b56fda31bb4.png)](https://felipe-p-adachi.medium.com/?source=post_page-----25c267d58f06--------------------------------)[](https://towardsdatascience.com/?source=post_page-----25c267d58f06--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----25c267d58f06--------------------------------)
    [Felipe de Pontes Adachi](https://felipe-p-adachi.medium.com/?source=post_page-----25c267d58f06--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://felipe-p-adachi.medium.com/?source=post_page-----25c267d58f06--------------------------------)[![Felipe
    de Pontes Adachi](../Images/58c9544ae85f43548c5e5b56fda31bb4.png)](https://felipe-p-adachi.medium.com/?source=post_page-----25c267d58f06--------------------------------)[](https://towardsdatascience.com/?source=post_page-----25c267d58f06--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----25c267d58f06--------------------------------)
    [Felipe de Pontes Adachi](https://felipe-p-adachi.medium.com/?source=post_page-----25c267d58f06--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fa038269245d5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F7-ways-to-monitor-large-language-model-behavior-25c267d58f06&user=Felipe+de+Pontes+Adachi&userId=a038269245d5&source=post_page-a038269245d5----25c267d58f06---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----25c267d58f06--------------------------------)
    ·12 min read·Jul 28, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F25c267d58f06&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F7-ways-to-monitor-large-language-model-behavior-25c267d58f06&user=Felipe+de+Pontes+Adachi&userId=a038269245d5&source=-----25c267d58f06---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fa038269245d5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F7-ways-to-monitor-large-language-model-behavior-25c267d58f06&user=Felipe+de+Pontes+Adachi&userId=a038269245d5&source=post_page-a038269245d5----25c267d58f06---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----25c267d58f06--------------------------------)
    ·12分钟阅读·2023年7月28日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F25c267d58f06&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F7-ways-to-monitor-large-language-model-behavior-25c267d58f06&user=Felipe+de+Pontes+Adachi&userId=a038269245d5&source=-----25c267d58f06---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F25c267d58f06&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F7-ways-to-monitor-large-language-model-behavior-25c267d58f06&source=-----25c267d58f06---------------------bookmark_footer-----------)![](../Images/a9947c80cb8d3a04e7485914f005fd01.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F25c267d58f06&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F7-ways-to-monitor-large-language-model-behavior-25c267d58f06&source=-----25c267d58f06---------------------bookmark_footer-----------)![](../Images/a9947c80cb8d3a04e7485914f005fd01.png)'
- en: Photo by [Jéan Béller](https://unsplash.com/@chinatravelchannel?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    on [Unsplash](https://unsplash.com/photos/a-close-up-of-a-clock-on-a-building-FyOEhy91_7o?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由 [Jéan Béller](https://unsplash.com/@chinatravelchannel?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    提供，来源于 [Unsplash](https://unsplash.com/photos/a-close-up-of-a-clock-on-a-building-FyOEhy91_7o?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
- en: The world of Natural Language Processing has seen a rapid evolution with the
    use of Large Language Models (LLMs). Through their impressive text generation
    and text understanding abilities, LLMs have gained a large adoption worldwide.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 自然语言处理领域在使用大型语言模型（LLMs）方面经历了快速的演变。通过其令人印象深刻的文本生成和理解能力，LLMs在全球范围内得到了广泛的采用。
- en: ChatGPT is perhaps the most well-known of these models, boasting 57 million
    monthly active users within the first month of availability [1]. Along with its
    impressive capabilities across multiple scenarios, the model also comes with big
    challenges, such as the tendency to hallucinate and generate biased or harmful
    content [2,3]. Another challenging area is observability — with the rapid collection
    of user feedback, ChatGPT is being continuously retrained and improved through
    Reinforcement Learning from Human Feedback (RLHF) [4], making its evaluation a
    moving target. It is well-known that overall improvements from RLHF can lead to
    performance regressions on specific tasks [5]. How can we ensure that the model
    behaves as expected and maintains acceptable performance within the tasks that
    are relevant to our application?
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: ChatGPT 可能是这些模型中最知名的，在上线的第一个月就拥有了 5700 万的月活跃用户[1]。尽管其在多种场景下具有令人印象深刻的能力，但模型也面临着很大的挑战，例如容易出现幻觉以及生成偏见或有害内容的倾向[2,3]。另一个具有挑战性的领域是可观察性——随着用户反馈的快速收集，ChatGPT
    正通过人类反馈强化学习（RLHF）[4] 不断被再训练和改进，使得其评估成为一个不断变化的目标。众所周知，从 RLHF 获得的整体改进可能会导致特定任务上的性能回退[5]。我们如何确保模型的行为符合预期，并在与我们应用相关的任务中保持可接受的性能？
- en: In this blog, we will discuss seven groups of metrics you can use to keep track
    of LLM’s behaviors. We will calculate these metrics for ChatGPT’s responses for
    a fixed set of 200 prompts across 35 days and track how ChatGPT’s behavior evolves
    within the period. Our focus task will be long-form question answering, and we
    will use LangKit and WhyLabs to calculate, track and monitor the model’s behavior
    across time.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在本博客中，我们将讨论七组指标，你可以用来跟踪 LLM 的行为。我们将计算这些指标，以 ChatGPT 对一组固定的 200 个提示的回应为例，覆盖 35
    天的时间，并跟踪 ChatGPT 行为在这一时期的变化。我们的重点任务将是长篇问答，我们将使用 LangKit 和 WhyLabs 来计算、跟踪和监控模型的行为。
- en: You can check the resulting dashboard for this project in [WhyLabs](https://hub.whylabsapp.com/resources/demo-chatgpt-behavior-ELI5/columns/response.difficult_words?dateRange=2023-03-05-to-2023-04-09&targetOrgId=demo&sessionToken=session-8gcsnbVy)
    (no sign up required) and run the complete example yourself by running this [Colab
    Notebook](https://colab.research.google.com/github/whylabs/langkit/blob/main/langkit/examples/ChatGPT_Behavioral_Monitoring.ipynb).
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在[WhyLabs](https://hub.whylabsapp.com/resources/demo-chatgpt-behavior-ELI5/columns/response.difficult_words?dateRange=2023-03-05-to-2023-04-09&targetOrgId=demo&sessionToken=session-8gcsnbVy)（无需注册）查看这个项目的仪表盘，并通过运行这个[Colab
    Notebook](https://colab.research.google.com/github/whylabs/langkit/blob/main/langkit/examples/ChatGPT_Behavioral_Monitoring.ipynb)自己运行完整的示例。
- en: Agenda
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 议程
- en: '[The Task — Comprehensible Question Answering](#d481)'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[任务——可理解的问答](#d481)'
- en: '[Popular LLM Metrics](#3ec3)'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[流行的 LLM 指标](#3ec3)'
- en: 1\. [ROUGE](#e1df)
  id: totrans-16
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 1\. [ROUGE](#e1df)
- en: 2\. [Gender Bias](#fbc5)
  id: totrans-17
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 2\. [性别偏见](#fbc5)
- en: 3\. [Text Quality](#a1b8)
  id: totrans-18
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 3\. [文本质量](#a1b8)
- en: 4\. [Semantic Similarity](#6f7e)
  id: totrans-19
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 4\. [语义相似性](#6f7e)
- en: 5\. [Regex Patterns](#eb87)
  id: totrans-20
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 5\. [正则表达式模式](#eb87)
- en: 6\. [Refusals](#f705)
  id: totrans-21
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 6\. [拒绝](#f705)
- en: 7\. [Toxicity and Sentiment](#4197)
  id: totrans-22
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 7\. [毒性和情感](#4197)
- en: '[Monitoring Across Time](#f4a6)'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[跨时间监控](#f4a6)'
- en: '[So, has behavior changed?](#a5d9)'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[那么，行为发生了变化吗？](#a5d9)'
- en: '[Conclusion](#9787)'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[结论](#9787)'
- en: The task — Comprehensible Question Answering
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 任务——可理解的问答
- en: For this example, let’s use the Explain Like I’m Five (ELI5) dataset [6], a
    question-answering dataset that contains open-ended questions — questions that
    require a longer response and cannot be answered with a “yes” or “no” — and the
    answers should be simple and easily comprehensible by beginners.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们使用 Explain Like I’m Five (ELI5) 数据集[6]，这是一个包含开放性问题的问答数据集——这些问题需要较长的回答，不能用“是”或“否”来回答——且回答应简单易懂，适合初学者。
- en: 'In the work presented in [ChatLog: Recording and Analyzing ChatGPT Across Time](https://arxiv.org/pdf/2304.14106.pdf),
    1000 questions were sampled from this dataset and repeatedly sent to ChatGPT every
    day from March 5 to April 9, 2023, which is available in [ChatLog’s Repository](https://github.com/THU-KEG/ChatLog).
    We’ll use this dataset by sampling 200 out of the original 1000 questions, along
    with ChatGPT’s answers and human reference answers, for each day of the given
    period. That way, we’ll end up with 35 daily dataframes, where each dataframe
    has 200 rows with the following columns:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '在[ChatLog: 记录与分析ChatGPT随时间的变化](https://arxiv.org/pdf/2304.14106.pdf)中展示的工作中，从该数据集中抽取了1000个问题，并在2023年3月5日至4月9日期间每天反复发送给ChatGPT，这些数据可以在[ChatLog的仓库](https://github.com/THU-KEG/ChatLog)中找到。我们将使用这个数据集，通过从原始1000个问题中抽取200个问题，连同ChatGPT的回答和人工参考答案，来覆盖给定期间的每一天。这样，我们将得到35个每日数据框，每个数据框有200行，包含以下列：'
- en: '![](../Images/2299ae9670cd9cb23d9a3ee4acf62953.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2299ae9670cd9cb23d9a3ee4acf62953.png)'
- en: Table by author
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的表格
- en: Popular LLM metrics
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 流行的LLM指标
- en: It can be a daunting task to define a set of metrics to properly evaluate a
    model with such a wide range of capabilities as ChatGPT. In this example, we’ll
    cover some examples of metrics that are relatively general and could be useful
    for a range of applications, such as text quality, sentiment analysis, toxicity,
    and text semantic similarity, and others that are particular for certain tasks
    like question answering and summarization, like the ROUGE group of metrics.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 为了准确评估像ChatGPT这样具有广泛功能的模型，定义一组合适的指标可能是一项艰巨的任务。在这个示例中，我们将涵盖一些相对通用的指标示例，这些指标对各种应用可能有用，例如文本质量、情感分析、毒性和文本语义相似性，还有一些特定于某些任务的指标，如问答和总结，例如ROUGE指标组。
- en: 'There are a multitude of other metrics and approaches that might be more relevant,
    depending on the particular application you are interested in. If you’re looking
    for more examples of what to monitor, here are three papers that served as an
    inspiration for the writing of this blog: [Holistic Evaluation of Language Models](https://arxiv.org/pdf/2211.09110.pdf),
    [ChatLog: Recording and Analyzing ChatGPT Across Time](https://arxiv.org/pdf/2304.14106.pdf),
    and [Beyond Accuracy: Behavioral Testing of NLP Models with CheckList](https://homes.cs.washington.edu/~marcotcr/acl20_checklist.pdf).'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '还有许多其他可能更相关的指标和方法，具体取决于你感兴趣的应用。如果你在寻找更多的监控示例，这里有三篇论文作为撰写本博客的灵感来源：[语言模型的整体评估](https://arxiv.org/pdf/2211.09110.pdf)、[ChatLog:
    记录与分析ChatGPT随时间的变化](https://arxiv.org/pdf/2304.14106.pdf)和[超越准确性：使用CheckList对NLP模型进行行为测试](https://homes.cs.washington.edu/~marcotcr/acl20_checklist.pdf)。'
- en: Now, let’s talk about the metrics we’re monitoring in this example. Most of
    the metrics will be calculated with the help of external libraries, such as [rouge](https://pypi.org/project/rouge/),
    [textstat](https://github.com/textstat/textstat), and [huggingface models](https://huggingface.co/models),
    and most of them are encapsulated in the [LangKit](https://github.com/whylabs/langkit)
    library, which is an open-source text metrics toolkit for monitoring language
    models. In the end, we want to group all the calculated metrics in a **whylogs**
    profile, which is a statistical summary of the original data. We will then send
    the daily profiles to the WhyLabs observability platform, where we can monitor
    them over time.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们探讨一下在这个示例中我们正在监控的指标。大多数指标将通过外部库计算，例如[rouge](https://pypi.org/project/rouge/)、[textstat](https://github.com/textstat/textstat)和[huggingface
    models](https://huggingface.co/models)，其中大部分封装在[LangKit](https://github.com/whylabs/langkit)库中，这是一个用于监控语言模型的开源文本指标工具包。最终，我们希望将所有计算出的指标汇总到一个**whylogs**配置文件中，这是一种原始数据的统计摘要。然后，我们将把每日配置文件发送到WhyLabs的可观察性平台，在那里我们可以对其进行长期监控。
- en: 'In the following table, we summarize the groups of metrics we will cover at
    the following sections:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下表格中，我们总结了我们将在接下来的部分中涵盖的指标组：
- en: '![](../Images/a0e3c3ff27ca84760009f7ce2f3eae6a.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a0e3c3ff27ca84760009f7ce2f3eae6a.png)'
- en: Table by author
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的表格
- en: ROUGE
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ROUGE
- en: Recall-Oriented Understudy for Gisting Evaluation (ROUGE) is a set of metrics
    commonly used in natural language processing to evaluate automatic summarization
    tasks by comparing the generated text with one or more reference summaries.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 召回导向的摘要评价（ROUGE）是一组常用于自然语言处理中的指标，用于通过将生成的文本与一个或多个参考摘要进行比较来评估自动摘要任务。
- en: 'The task at hand is a question-answering problem rather than a summarization
    task, but we do have human answers as a reference, so we will use the ROUGE metrics
    to measure the similarity between the ChatGPT response and each of the three reference
    answers. We will use the [rouge](https://pypi.org/project/rouge/) python library
    to augment our dataframe with two different metrics: ROUGE-L, which takes into
    account the longest sequence overlap between the answers, and ROUGE-2, which takes
    into account the overlap of bigrams between the answers. For each generated answer,
    the final scores will be defined according to the maximum score across the 3 reference
    answers, based on the f-score of ROUGE-L. For both ROUGE-L and ROUGE-2, we’ll
    calculate the f-score, precision, and recall, leading to the creation of 6 additional
    columns.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 当前任务是一个问答问题而非总结任务，但我们有人工回答作为参考，因此我们将使用ROUGE指标来衡量ChatGPT回应与三个参考答案之间的相似度。我们将使用[rouge](https://pypi.org/project/rouge/)
    Python库来扩展我们的数据框，计算两种不同的指标：ROUGE-L，它考虑答案之间的最长序列重叠，和ROUGE-2，它考虑答案之间的二元组重叠。对于每个生成的答案，最终分数将根据ROUGE-L的f-score在3个参考答案中的最高分确定。对于ROUGE-L和ROUGE-2，我们将计算f-score、精确度和召回率，最终生成6个额外的列。
- en: 'This approach was based on the following paper: [ChatLog: Recording and Analyzing
    ChatGPT Across Time](https://arxiv.org/pdf/2304.14106.pdf)'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '该方法基于以下论文：[ChatLog: Recording and Analyzing ChatGPT Across Time](https://arxiv.org/pdf/2304.14106.pdf)'
- en: Gender bias
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 性别偏见
- en: Social bias is a central topic of discussion when it comes to fair and responsible
    AI [2],[7], which can be defined as “a systematic asymmetry in language choice”
    [8]. In this example, we’re focusing on gender bias by measuring how uneven the
    mentions are between male and female demographics to identify under and over representation.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 社会偏见是讨论公平和负责任的人工智能时的一个核心话题[2],[7]，可以定义为“语言选择中的系统性不对称”[8]。在这个例子中，我们专注于性别偏见，通过衡量男性和女性人群之间提及的不均衡程度来识别过度或不足的代表性。
- en: 'We will do so by counting the number of words that are included in both sets
    of words that are attributed to the female and male demographics. For a given
    day, we will sum the number of occurrences across the 200 generated answers, and
    compare the resulting distribution to a reference, unbiased distribution by calculating
    the distance between them, using [total variation distance](https://en.wikipedia.org/wiki/Total_variation_distance_of_probability_measures).
    In the following code snippet, we can see the groups of words that were used to
    represent both demographics:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将通过统计同时包含在归属于女性和男性人群的两个词汇集合中的单词数量来进行此操作。对于某一天，我们将汇总200个生成答案中的出现次数，并通过计算它们之间的距离来将结果分布与参考的无偏分布进行比较，使用[总变差距离](https://en.wikipedia.org/wiki/Total_variation_distance_of_probability_measures)。在以下代码片段中，我们可以看到用于代表这两个人群的单词组：
- en: '[PRE0]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'This approach was based on the following paper: [Holistic Evaluation of Language
    Models](https://arxiv.org/pdf/2211.09110.pdf)'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 该方法基于以下论文：[Holistic Evaluation of Language Models](https://arxiv.org/pdf/2211.09110.pdf)
- en: Text quality
  id: totrans-47
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 文本质量
- en: Text quality metrics, such as readability, complexity, and grade level, can
    provide important insights into the quality and suitability of generated responses.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 文本质量指标，如可读性、复杂性和年级水平，可以提供对生成回应的质量和适用性的关键见解。
- en: In LangKit, we can compute text quality metrics through the textstat module,
    which uses the [textstat](https://github.com/textstat/textstat) library to compute
    several different text quality metrics.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在LangKit中，我们可以通过textstat模块计算文本质量指标，该模块使用[textstat](https://github.com/textstat/textstat)库来计算几种不同的文本质量指标。
- en: Semantic similarity
  id: totrans-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 语义相似性
- en: Another important aspect to consider is the degree of irrelevant or off-topic
    responses given by the model, and how this evolves with time. This will help us
    verify how closely the model outputs align with the intended context.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个重要的方面是考虑模型所给出的无关或离题回应的程度，以及这种情况随时间的变化。这将帮助我们验证模型输出与预期背景的契合程度。
- en: 'We will do so with the help of the [sentence-transformers](https://github.com/UKPLab/sentence-transformers)
    library, by calculating the dense vector representation for both question and
    answer. Once we have the sentence embeddings, we can compute the cosine similarity
    between them to measure the semantic similarity between the texts. LangKit’s **input_output**
    module will do just that for us. We can use the module to generate metrics directly
    into a **whylogs** profile, but in this case, we are using it to augment our dataframe
    with a new column (**response.relevance_to_prompt**), where each row contains
    the semantic similarity score between the question and response:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将借助于 [sentence-transformers](https://github.com/UKPLab/sentence-transformers)
    库，通过计算问题和回答的密集向量表示来实现这一点。一旦得到句子嵌入，我们可以计算它们之间的余弦相似度，以衡量文本之间的语义相似性。LangKit 的 **input_output**
    模块将为我们完成这一任务。我们可以使用该模块直接将指标生成到 **whylogs** 配置文件中，但在本例中，我们使用它来为数据框架添加一个新列 (**response.relevance_to_prompt**)，每行包含问题和回答之间的语义相似度得分：
- en: '[PRE1]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Regex patterns
  id: totrans-54
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 正则表达式模式
- en: An important aspect of LLM behavior is ensuring it doesn’t output sensitive
    or fake information. For example, if the user prompt is “I feel sad.”, we might
    be interested in knowing if the model’s response wrongly refer the user to an
    existing or non-existent telephone number.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: LLM 行为的一个重要方面是确保其不会输出敏感或虚假的信息。例如，如果用户的提示是“我感到难过。”，我们可能会关注模型的回答是否错误地将用户引导到一个真实或虚构的电话号码。
- en: Let’s do that by searching for groups of regexes patterns to help detect the
    presence of information such as telephone numbers, credit card numbers, mailing
    addresses, SSNs, and others.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 通过搜索正则表达式模式的组来帮助检测诸如电话号码、信用卡号码、邮寄地址、社会安全号码等信息的存在。
- en: 'As with the previous metric, we will use LangKit to search through these patterns.
    In the complete example, we’re directly registering it as a whylogs metric, but
    you can also use it as a standalone function like this:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 与前一个指标一样，我们将使用 LangKit 搜索这些模式。在完整的示例中，我们直接将其注册为 whylogs 指标，但你也可以像这样将其用作独立函数：
- en: '[PRE2]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[PRE3]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Refusals
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 拒绝
- en: 'By now, most of us should be familiar with the polite refusals LLMs give when
    asked about banned or controversial topics, which can go similar to this:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们大多数人应该对 LLM 在被询问有关禁忌或争议话题时的礼貌拒绝感到熟悉，这通常类似于这样：
- en: '*I’m sorry, but I can’t assist with that request.*'
  id: totrans-62
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*对不起，我不能协助处理这个请求。*'
- en: 'On a fixed set of prompts, an increase in these refusals can be a signal that
    our model has become overly cautious or sensitive. The inverse case should also
    be investigated: it might be a signal that the model is now easier to jailbreak
    and is more prone to engage in toxic or harmful conversations. For this reason,
    let’s calculate the semantic similarity (as described in the previous section
    of Semantic Similarity) of each generated answer with a fixed set of sentences:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在固定的提示集上，这些拒绝的增加可能是我们模型变得过于谨慎或敏感的信号。相反的情况也应进行调查：这可能是模型现在更容易被破解，且更倾向于参与有毒或有害的对话。因此，让我们计算每个生成的答案与固定句子集的语义相似度（如前述的语义相似度部分所述）：
- en: '[PRE4]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The similarity score will be defined as the maximum score found across all sentences
    in the above set, which will then be tracked in our statistical profile.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 相似度得分将被定义为在上述集合中所有句子中找到的最大得分，这将被追踪在我们的统计资料中。
- en: Toxicity and sentiment
  id: totrans-66
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 毒性和情感
- en: Monitoring sentiment allows us to gauge the overall tone and emotional impact
    of the responses, while toxicity analysis provides an important measure of the
    presence of offensive, disrespectful, or harmful language in LLM outputs. Any
    shifts in sentiment or toxicity should be closely monitored to ensure the model
    is behaving as expected.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 监控情感可以帮助我们衡量响应的总体语气和情感影响，而毒性分析则提供了一个重要的衡量标准，用于判断 LLM 输出中是否存在冒犯、不尊重或有害的语言。任何情感或毒性的变化都应被密切监控，以确保模型按预期行为。
- en: 'For sentiment analysis, we will track the scores provided by [**nltk**](https://www.nltk.org/)’s
    **SentimentIntensityAnalyzer**. As for the toxicity scores, we will use HuggingFace’s
    [martin-ha/toxic-comment-model](https://huggingface.co/martin-ha/toxic-comment-model)
    toxicity analyzer. Both are wrapped in LangKit’s sentiment and toxicity modules,
    such that we can use them directly like this:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 对于情感分析，我们将跟踪 [**nltk**](https://www.nltk.org/) 的 **SentimentIntensityAnalyzer**
    提供的得分。至于毒性得分，我们将使用 HuggingFace 的 [martin-ha/toxic-comment-model](https://huggingface.co/martin-ha/toxic-comment-model)
    毒性分析器。两者都被封装在 LangKit 的情感和毒性模块中，因此我们可以直接像这样使用它们：
- en: '[PRE5]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '[PRE6]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Monitoring across time
  id: totrans-71
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 随着时间的推移进行监控
- en: 'Now that we defined the metrics we want to track, we need to wrap them all
    into a single profile and proceed to upload them to our monitoring dashboard.
    As mentioned, we will generate a whylogs profile for each day’s worth of data,
    and as the monitoring dashboard, we will use WhyLabs, which integrates with the
    whylogs profile format. We won’t show the complete code to do it in this post,
    but a simple version of how to upload a profile with langkit-enabled LLM metrics
    looks something like this:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经定义了要跟踪的指标，我们需要将它们全部封装到一个单一的配置文件中，然后将其上传到我们的监控仪表板。如前所述，我们将为每天的数据生成一个whylogs配置文件，并且我们将使用WhyLabs作为监控仪表板，它与whylogs配置文件格式兼容。我们不会在这篇文章中展示完整的代码，但一个简单的如何使用langkit启用LLM指标上传配置文件的示例如下：
- en: '[PRE7]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: By initializing **llm_metrics**, the whylogs profiling process will automatically
    calculate, among others, metrics such as text quality, semantic similarity, regex
    patterns, toxicity, and sentiment.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 通过初始化**llm_metrics**，whylogs分析过程将自动计算包括文本质量、语义相似性、正则表达式模式、毒性和情感等指标。
- en: If you’re interested in the details of how it’s done, check the complete code
    in this [Colab Notebook](https://colab.research.google.com/github/whylabs/langkit/blob/main/langkit/examples/ChatGPT_Behavioral_Monitoring.ipynb)!
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你对如何实现这一点的详细信息感兴趣，可以查看这份[Colab Notebook](https://colab.research.google.com/github/whylabs/langkit/blob/main/langkit/examples/ChatGPT_Behavioral_Monitoring.ipynb)中的完整代码！
- en: So, has behavior changed?
  id: totrans-76
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 那么，行为发生了变化吗？
- en: TLDR; In general, it looks like it changed for the better, with a clear transition
    on Mar 23, 2023.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: TLDR; 总的来说，它似乎有所改善，2023年3月23日出现了明显的过渡。
- en: We won’t be able to show every graph in this blog — in total, there are 25 monitored
    features in our dashboard — but let’s take a look at some of them. For a complete
    experience, you’re welcome to explore the [project’s dashboard yourself](https://hub.whylabsapp.com/resources/demo-chatgpt-behavior-ELI5/columns/response.difficult_words?dateRange=2023-03-05-to-2023-04-09&targetOrgId=demo&sessionToken=session-8gcsnbVy).
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 我们无法在这篇博客中展示每一个图表——我们的仪表板总共有25个监控特性——但让我们看看其中的一些。为了获得完整的体验，欢迎你探索[项目的仪表板](https://hub.whylabsapp.com/resources/demo-chatgpt-behavior-ELI5/columns/response.difficult_words?dateRange=2023-03-05-to-2023-04-09&targetOrgId=demo&sessionToken=session-8gcsnbVy)！
- en: Concerning the rouge metrics, over time, recall slightly decreases, while precision
    increases at the same proportion, keeping the f-score roughly equal. This indicates
    that answers are getting more focused and concise at the expense of losing coverage
    but maintaining the balance between both, which seems to agree with the original
    results provided in [9].
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 关于ROUGE指标，随着时间的推移，召回率略有下降，而精确度则按相同比例上升，使得F-score大致保持不变。这表明，回答变得更加专注和简洁，虽然牺牲了一些覆盖范围，但在两者之间保持了平衡，这似乎与[9]中提供的原始结果一致。
- en: '![](../Images/d17da7cc60d2e1ea49c4de32efd6d339.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d17da7cc60d2e1ea49c4de32efd6d339.png)'
- en: ROUGE-L-R. Screenshot by author.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: ROUGE-L-R。截图由作者提供。
- en: 'Now, let’s take a look at one of the text quality metrics, **difficult words**:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们来看看其中一个文本质量指标——**困难词汇**：
- en: '![](../Images/e1999df549eb5e1986217ed8e904c6e6.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e1999df549eb5e1986217ed8e904c6e6.png)'
- en: difficult words. Screenshot by author.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 难度词汇。截图由作者提供。
- en: There’s a sharp decrease in the mean number of words that are considered difficult
    after March 23, which is a good sign, considering the goal is to make the answer
    easily comprehensible. This readability trend can be seen in other text quality
    metrics, such as the **automated readability index, Flesch reading ease,** and
    **character count.**
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在3月23日之后，困难词汇的平均数量急剧下降，这是一个好迹象，因为我们的目标是使答案易于理解。这种可读性趋势也可以在其他文本质量指标中看到，例如**自动可读性指数、Flesch阅读易度**和**字符计数**。
- en: 'The semantic similarity also seems to timidly increase with time, as seen below:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 语义相似性似乎也随着时间的推移而略有增加，如下所示：
- en: '![](../Images/8832cad8b5f369e163cc2b308ea8e5be.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8832cad8b5f369e163cc2b308ea8e5be.png)'
- en: '*response.relevance_to_prompt*. Screenshot by author.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '*response.relevance_to_prompt*。截图由作者提供。'
- en: 'This indicates that the model’s responses are getting more aligned with the
    question’s context. This could have not been the case, though — in Tu, Shangqing,
    et al.[4], it is noted that the ChatGPT can start answering questions by using
    metaphors, which could have caused a drop in similarity scores without implying
    a drop in the quality of responses. There might be other factors that lead the
    overall similarity to increase. For example, a decrease in the model’s refusals
    to answer questions might lead to an increase in semantic similarity. This is
    actually the case, which can be seen by the **refusal_similarity** metric, as
    shown below:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 这表明模型的回答越来越符合问题的上下文。不过，这种情况也可能不存在——在Tu, Shangqing等人的研究[4]中指出，ChatGPT可能会通过使用隐喻开始回答问题，这可能导致相似度分数的下降，而并不意味着回答质量的下降。可能还有其他因素导致整体相似度的提高。例如，模型拒绝回答问题的次数减少可能会导致语义相似度的增加。实际上情况确实如此，这可以通过下面显示的**refusal_similarity**指标看到：
- en: '![](../Images/7c434f19e387d0d7eac26349c52992fc.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7c434f19e387d0d7eac26349c52992fc.png)'
- en: refusal similarity. Screenshot by author.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 拒绝相似度。截图由作者提供。
- en: In all the graphics above, we can see a definite transition in behavior between
    March 23 and March 24\. There must have been a significant upgrade in ChatGPT
    on this particular date.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述所有图形中，我们可以看到3月23日和3月24日之间行为的明显过渡。这表明ChatGPT在这个特定日期进行了重要升级。
- en: For the sake of brevity, we won’t be showing the remaining graphs, but let’s
    cover a few more metrics. The **gender_tvd** score maintained roughly the same
    for the entire period, showing no major differences over time in the demographic
    representation between genders. The sentiment score, on average, remained roughly
    the same, with a positive mean, while the toxicity’s mean was found to be very
    low across the entire period, indicating that the model hasn’t been showing particularly
    harmful or toxic behavior. Furthermore, no sensitive information was found while
    logging the **has_patterns** metric.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简洁起见，我们不会展示剩余的图表，但我们可以再讨论几个指标。**gender_tvd**分数在整个期间内大致保持不变，显示出性别之间的群体代表性没有重大差异。情感分数平均大致保持不变，具有正的均值，而毒性均值在整个期间内非常低，表明模型没有表现出特别有害或有毒的行为。此外，在记录**has_patterns**指标时没有发现敏感信息。
- en: Conclusion
  id: totrans-94
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: With such a diverse set of capabilities, tracking Large Language Model’s behavior
    can be a complex task. In this blog post, we used a fixed set of prompts to evaluate
    how the model’s behavior changes with time. To do so, we explored and monitored
    seven groups of metrics to assess the model’s behavior in different areas like
    performance, bias, readability, and harmfulness.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 具有如此多样化的能力，追踪大型语言模型的行为可能是一项复杂的任务。在这篇博客文章中，我们使用了一组固定的提示来评估模型行为如何随时间变化。为此，我们探索和监测了七组指标，以评估模型在性能、偏见、可读性和有害性等不同领域的行为。
- en: We have a brief discussion on the results in this blog, but we encourage the
    reader to explore the results by himself/herself!
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这篇博客中简要讨论了结果，但我们鼓励读者自行探索结果！
- en: References
  id: totrans-97
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: 1 — [https://www.engadget.com/chatgpt-100-million-users-january-130619073.html](https://www.engadget.com/chatgpt-100-million-users-january-130619073.html)
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 1 — [https://www.engadget.com/chatgpt-100-million-users-january-130619073.html](https://www.engadget.com/chatgpt-100-million-users-january-130619073.html)
- en: '2- Emily M Bender et al. “On the Dangers of Stochastic Parrots: Can Language
    Models Be Too Big?” In: Proceedings of the 2021 ACM conference on fairness, accountability,
    and transparency. 2021, pp. 610–623 (cit. on p. 2).'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 2- Emily M Bender 等人。“随机鹦鹉的危险：语言模型能否过大？”刊登于：2021年ACM公平性、问责制和透明度会议论文集。2021年，第610–623页（引自第2页）。
- en: '3 — Hussam Alkaissi and Samy I McFarlane. “Artificial hallucinations in chatgpt:
    Implications in scientific writing”. In: Cureus 15.2 (2023) (cit. on p. 2).'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 3 — Hussam Alkaissi 和 Samy I McFarlane. “ChatGPT中的人工幻觉：对科学写作的影响”。刊登于：Cureus
    15.2 (2023)（引自第2页）。
- en: '4 — Tu, Shangqing, et al. “ChatLog: Recording and Analyzing ChatGPT Across
    Time.” *arXiv preprint arXiv:2304.14106* (2023). [https://arxiv.org/pdf/2304.14106.pdf](https://arxiv.org/pdf/2304.14106.pdf)'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 4 — Tu, Shangqing 等人。“ChatLog：记录和分析ChatGPT随时间的变化。”*arXiv预印本arXiv:2304.14106*
    (2023)。[https://arxiv.org/pdf/2304.14106.pdf](https://arxiv.org/pdf/2304.14106.pdf)
- en: 5 — [https://cdn.openai.com/papers/Training_language_models_to_follow_instructions_with_human_feedback.pdf](https://cdn.openai.com/papers/Training_language_models_to_follow_instructions_with_human_feedback.pdf)
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 5 — [https://cdn.openai.com/papers/Training_language_models_to_follow_instructions_with_human_feedback.pdf](https://cdn.openai.com/papers/Training_language_models_to_follow_instructions_with_human_feedback.pdf)
- en: '6- Angela Fan, Yacine Jernite, Ethan Perez, David Grangier, Jason Weston, and
    Michael Auli. 2019\. [ELI5: Long Form Question Answering](https://aclanthology.org/P19-1346).
    In *Proceedings of the 57th Annual Meeting of the Association for Computational
    Linguistics*, pages 3558–3567, Florence, Italy. Association for Computational
    Linguistics.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '6- Angela Fan, Yacine Jernite, Ethan Perez, David Grangier, Jason Weston 和
    Michael Auli. 2019\. [ELI5: 长篇问答](https://aclanthology.org/P19-1346). 收录于*第57届计算语言学协会年会论文集*，第3558–3567页，意大利佛罗伦萨。计算语言学协会。'
- en: 7 — Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings
    — [https://doi.org/10.48550/arXiv.1607.06520](https://doi.org/10.48550/arXiv.1607.06520)
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 7 — 人对计算机程序员如同女人对家庭主妇？去偏见词嵌入 — [https://doi.org/10.48550/arXiv.1607.06520](https://doi.org/10.48550/arXiv.1607.06520)
- en: '8 — Beukeboom, C. J., & Burgers, C. (2019). How stereotypes are shared through
    language: A review and introduction of the Social Categories and Stereotypes Communication
    (SCSC) Framework. Review of Communication Research, 7, 1–37\. [https://doi.org/10.12840/issn.2255-4165.017](https://doi.org/10.12840/issn.2255-4165.017)'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 8 — Beukeboom, C. J., & Burgers, C. (2019). 刻板印象如何通过语言传播：社会类别与刻板印象交流（SCSC）框架的综述与介绍。传播研究评论，7，1–37\.
    [https://doi.org/10.12840/issn.2255-4165.017](https://doi.org/10.12840/issn.2255-4165.017)
