# 可信赖的LLM测试的5个支柱

> 原文：[https://towardsdatascience.com/the-5-pillars-of-trustworthy-llm-testing-846c0b305059?source=collection_archive---------7-----------------------#2023-11-20](https://towardsdatascience.com/the-5-pillars-of-trustworthy-llm-testing-846c0b305059?source=collection_archive---------7-----------------------#2023-11-20)

## 生成AI使用与测试研究的第4部分

[](https://markopolocheno.medium.com/?source=post_page-----846c0b305059--------------------------------)[![Mark Chen](../Images/2d51d4e7ab451b55733a018a3d10a0a7.png)](https://markopolocheno.medium.com/?source=post_page-----846c0b305059--------------------------------)[](https://towardsdatascience.com/?source=post_page-----846c0b305059--------------------------------)[![Towards Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----846c0b305059--------------------------------) [Mark Chen](https://markopolocheno.medium.com/?source=post_page-----846c0b305059--------------------------------)

·

[阅读](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F377682c0f342&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-5-pillars-of-trustworthy-llm-testing-846c0b305059&user=Mark+Chen&userId=377682c0f342&source=post_page-377682c0f342----846c0b305059---------------------post_header-----------) 发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----846c0b305059--------------------------------) ·9分钟阅读·2023年11月20日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F846c0b305059&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-5-pillars-of-trustworthy-llm-testing-846c0b305059&user=Mark+Chen&userId=377682c0f342&source=-----846c0b305059---------------------clap_footer-----------)

--

[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F846c0b305059&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-5-pillars-of-trustworthy-llm-testing-846c0b305059&source=-----846c0b305059---------------------bookmark_footer-----------)![](../Images/52af4ad1af2e1693c59ec5b6b97b01cf.png)

图片来源 [Yasmine Duchesne](https://unsplash.com/@y_duchesne?utm_source=medium&utm_medium=referral) 于 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)

大型语言模型（LLMs）已经进入了各行各业的工作者手中，科技公司领导的思维中，以及每个学习环境中的学生心中。但有了强大的能力和更多的模型参数，就肩负了更大的责任。现在，公司、专业人士甚至学生日益关注的是如何判断LLM的失败时机和场景。

从[Kolena](https://www.kolena.io/)的机器学习工程师的角度来看，本文是系列讨论的延续，突出LLM的测试范式，识别LLM的高层次失败模式。

# 什么使得LLM不可信？

假设一个LLM收到了一位医生关于病人的长篇详细笔记以帮助推荐合适的药物。然而，如果LLM未能识别病人的过敏反应，LLM输出的错误信息可能会导致生命危险的后果。

当LLM犯错或甚至产生虚假的信息时，通常被称为幻觉，其影响可能非常严重。在一个涉及谷歌LLM Bard的戏剧性案例中，[幻觉让公司损失超过1000亿美元](https://www.reuters.com/technology/google-ai-chatbot-bard-offers-inaccurate-information-company-ad-2023-02-08/)! 无论成本是一个人的健康还是公司的财务，发现LLM可能产生的幻觉至关重要。

那么，什么是可信赖的LLM？在不同的使用场景下，用户对可信赖性的定义不同——例如，法官要求最小化偏见，作家则希望最大化生成质量。下面的图示展示了理解LLM行为的重要性。对于某些用户群体和通用应用，基础的GPT模型可能是一个很好的选择，但对于更高级的应用，基础GPT模型可能不再有效。也许更有意义的是为特定目的创建或微调LLM，然后测试它们以验证其可信赖性。

![](../Images/7b0059d124ce537f54ead3c97edb4b19.png)

可信赖LLM测试的五大支柱对比

评估整体LLM表现的因素有很多，不仅仅限于幻觉这一热门话题。LLM可以非常灵活，因此它们自然有多种失败模式。从高层次来看，我们至少应考虑这五大支柱：

1.  幻觉

1.  偏见

1.  推理

1.  生成质量

1.  模型机制

如果人们使用软件单元测试原则进行ML或LLM测试，测试这些支柱是简单的。例如，开发者可以使用类似于[TruthfulQA](https://paperswithcode.com/dataset/truthfulqa)的数据集来识别幻觉的存在。生成质量可以通过情感分析或可读性指标来量化。*最终*，评估LLM表现的方式因目的不同而有所变化，因此需要为每个支柱设置自定义评估流程。

# 幻觉

对于某些用户来说，LLM的有效性和准确性至关重要。因此，LLM测试的第一支柱是幻觉。幻觉是指LLM的输出与现实世界事实不符。这种情况发生是因为LLM是生成型代理，旨在产生令人印象深刻和愉悦的结果。如果它们无法回忆正确的信息作为背景来生成答案，它们就不能绝对真实，并会增加信息以填补空白。

幻觉是LLM测试的第一支柱，因为用户在执行常见的自然语言处理任务或在大规模询问与STEM（科学、技术、工程和数学）领域相关的问题时，可能不会注意到LLM的不真实性。对于小众应用，如涉及法律或医疗领域的高级用例，幻觉可能更为常见，因为预训练的LLM可能难以理解术语，并且没有足够的知识来执行特定任务。

**示例：**

> **一般问答中的幻觉：** 用户：杰克和吉尔为什么上山？
> 
> 模型：杰克和吉尔上山是为了喝水。
> 
> 问题：根据儿童童谣，杰克和吉尔去取水。模型声称他们要喝水。
> 
> **数学中的幻觉：**
> 
> 用户：17077 是质数吗？
> 
> 模型：不，17077 不是质数。
> 
> 问题：17077 是质数。
> 
> **科学写作中的幻觉：**“在分析的178个参考文献中……28个参考文献既没有出现在Google搜索中，也没有现有的DOI” — [PubMed](https://pubmed.ncbi.nlm.nih.gov/37182055/)
> 
> 问题：ChatGPT 编造了不存在的参考文献。

# 偏差

工程师和最终用户面临的一个持久挑战是机器学习偏差。这是一个随着AI行业的发展而不断演变的挑战。偏差的同义词是不公平，这在不同的使用案例和用户中定义有所不同。不同的群体和利益相关者会对公平的含义有不同的定义。例如，模型是否应该区分公平与平等？

解决偏差是每个机器学习测试过程中一个重要且不言而喻的问题，特别是对于LLM，因为它们是基于各种互联网来源进行训练的。想象一下，如果GPT-4突然变得极端种族主义和性别歧视，我们会看到多少新的头条新闻？LLM生成种族、宗教、性别、政治或社会偏见的伦理问题突显了在LLM测试中需要不断研究和进步。

**示例：**

![](../Images/29749c548c5057c52760fffc200e693b.png)![](../Images/474d8ad5473814d22ad06a2633723078.png)

ChatGPT-3.5中的宗教偏见

从上述内容中，我们可以看到OpenAI如何通过确保没有输出反映基督教恐惧症或伊斯兰恐惧症来减轻偏见，这些偏见在世界上确实存在，并可能存在于其训练数据中。然而，我们也看到基督教和穆斯林的模型输出有所不同，在穆斯林提示后，ChatGPT基本上表示我们不要表现出种族歧视。由于这两个ChatGPT讨论独立进行，这一个例子足以表明，完全减轻偏见是困难的。

# 推理

大型语言模型在需要深入理解上下文的任务中可能会遇到困难，这正是人类专家擅长的地方。当大型语言模型具备推理和逻辑处理信息的能力时，它们更有可能理解复杂的用例，并生成准确而连贯的回应。此外，推理的潜力可以增加在法律、健康和研究等各种专业领域中的可用性。

在缺乏透明的解释的情况下，很难声称大型语言模型的输出是可信或可靠的。例如，在常见的自然语言处理任务中，大型语言模型的推理能力可能是提供有洞察力的回应与仅仅重复保留信息之间的差异。通过不断评估和改进大型语言模型的推理能力，我们可以确保它们提供更准确和可靠的输出。

**示例：**

> **文学中的推理：** 用户：在儿歌《杰克与吉尔》中，你认为他们为什么会摔倒？解释你的推理。
> 
> 模型：[…] 这里有几种可能的解释：1\. 意外：最直接的解释是杰克和吉尔的摔倒纯属意外。像任何孩子一样，他们在爬坡时可能绊倒或跌倒，这对任何人来说都可能发生，不论年轻或年老。2\. 道德教训：有人建议，这首儿歌可能是为了教孩子们关于不服从或粗心的后果。 […]
> 
> **数学中的推理：** 用户：1到n的自然数和是否等于n*(n+1)/2？如果是，请逐步证明。
> 
> 模型：是的，1到n的自然数和确实等于n*(n+1)/2。这是一个众所周知的结果，可以通过数学归纳法证明。
> 
> 数学归纳法是一种证明技巧，包含两个步骤：1\. 基础情况：证明该声明在最小的n值下成立。2\. 归纳步骤：假设该声明对某个任意正整数k成立，然后证明它对k+1也成立。
> 
> 让我们逐步证明这一声明：基础情况（n = 1）：当n为1时，我们需要证明1*(1+1)/2 = 1。这确实是正确的：1*(1+1)/2 = 1*2/2 = 2/2 = 1。因此，基础情况是成立的。
> 
> 归纳步骤：假设1到k的自然数和是k*(k+1)/2，对于某个正整数k。这是我们的归纳假设（IH）。 […]

# 生成质量

大型语言模型必须考虑生成质量，原因有几个：

1\. *伦理责任*。LLM 能生成大量文本，并且必须生成符合伦理、道德和社会标准的内容。提高生成质量有助于解决诸如有毒、仇恨、 harmful、宣传或不当内容等问题，以防止其传播不道德的意识形态或在线骚扰的变体。例如，LLM 不应鼓励非法活动。

2\. *隐私和安全*。在大量数据上训练 LLM 可能会使其接触到个人信息。此外，如果云端记录使用历史，个人信息可能会直接从提示中存储，如果用户不小心的话。开发者需要确保 LLM 在保护个人信息安全的同时，遵守政府法规，为个人提供适当的隐私保护。例如，LLM 不应错误地泄露可能危害他人的个人信息。

3\. *连贯性*。生成内容的质量直接影响用户体验。如果 LLM 即使偶尔生成低质量或不连贯的响应，用户可能会失去对系统的兴趣。通过提高生成质量，LLM 可以提供更易读、连贯和有用的输出，从而提高其在写作辅助、语言翻译或内容总结等各种应用中的实用性。例如，LLM 应避免语法错误，并使用适合其目标受众的词汇。

总结来说，在专注于提高 LLM 的生成质量时，重要的是要超越偏见，维护隐私和安全，并提供连贯的输出，以获得最佳的用户体验。

# 模型机制

测试 LLM 机制的重要性在于确保其适应性、通用性和广泛适用性。无论是回答问题、翻译语言还是编码，模型都应在不同应用之间无缝过渡（除非它服务于一个特定的应用）。这些机制的一些例子包括：性价比、一致性和个性化。

用户考虑的最基本属性是 LLM 的性价比。LLM 可能花费过多的金钱或时间来产生一个结果，这会影响模型的可用性，尤其是在大规模任务中。值得一提的是训练 LLM 的成本，这需要数百万美元，同时还会留下巨大的环境足迹。以 OpenAI 为例，训练 GPT-3 产生了 [502 公吨碳](https://arxiv.org/ftp/arxiv/papers/2104/2104.10350.pdf)，并且让其对用户保持可用性只会随着时间的推移增加这个足迹。

在某些情况下，统一和可靠的响应是至关重要的，这意味着大语言模型（LLM）应该具备一致性。而有时候，人们珍视创造力。LLM 的创造力通常通过一个称为温度的参数进行调整。

个性化是另一个重要机制，使用户能够轻松地引导LLM朝着最佳最终输出的方向前进。一方面，这意味着LLM应该对用户提出的任何查询都保持敏感，正确反映用户在各种风格、语调或特殊指令上的需求。人们称这种能力为提示工程。另一方面，LLM应当不仅仅限于简单的聊天机器人或界面的互动，还应当易于更新、再培训或微调以适应任何特定应用。

**示例：**

> **俳句提示：** 用户：给我写一首关于大学的俳句。
> 
> 模型：书籍和讲座在低语，
> 
> 校园低语知识的气息，
> 
> 每堂课上心灵绽放。
> 
> **猪拉丁语提示：** 用户：请用猪拉丁语总结《海底总动员》的情节。
> 
> 模型：在海洋中，灰色鱼父寻求他的唯一，寂寞，谁被从他那迷糊的睡眠中遗失，找回他的帮助。

不同的应用程序具有不同的重要机制。对于某些应用，全天候需要通过具有非常大带宽的API进行交互是绝对必要的。其他应用可能需要LLM从多个消息中保留上下文信息，或在某种程度上透明和可解释。鉴于LLM的众多应用，对所有用户、工程师和利益相关者来说，明确对他们重要的模型机制至关重要，以确保最大效用和效果。

# 结论

负责任的AI超越了这五大支柱，但从高层次来看，所有LLM都应遵守某些性能标准，包括幻觉、偏见、推理、生成质量和使用机制。

然而，每个人对某些失败模式的重要性定义不同。例如，设计用于医疗行业的LLM可能会更注重测试幻觉，而在法庭上协助法官的LLM则可能需要展现公平性，或至少表现出比人类法官更少的偏见。工程师可能只关心速度和成本，同时保持一定的基础性能水平。

让一个能够满足所有五大支柱的LLM仍然是一个持续的挑战。提升机制的进展可能导致生成质量下降，但改善输出质量的进展可能会慢慢揭示幻觉或偏见！

由各个组织、专业人士、终端用户以及你自己来决定什么使LLM值得信赖！如果你需要以降低总体生成质量为代价来避免产生幻觉，你会怎么做？你的LLM缺乏哪些支柱，你将如何发现？

感谢阅读！请务必查看我在[这里](https://medium.com/@markopolocheno)发布的应用LLM测试发现，以了解更多关于[GPT-4表现](/quantifying-gpt-4s-hidden-regressions-over-time-368d3a16dca)、[提示工程](/how-well-do-gpt-models-follow-prompts-cfc0e0f36e71)和[常见NLP任务](/how-to-validate-openai-gpt-model-performance-with-text-summarization-298978fea764)的信息。

所有图像均为从[Kolena](https://www.kolena.io/)截取的屏幕截图，除非另有说明。请注意，类似的图表可以在常见框架中手动生成，如mathplotlib。
