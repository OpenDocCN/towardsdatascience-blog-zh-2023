- en: Batched K-Means with Python Numba and CUDA C
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 批量 K-Means 与 Python Numba 和 CUDA C
- en: 原文：[https://towardsdatascience.com/batched-k-means-with-python-numba-and-cuda-c-3d4946c587b9?source=collection_archive---------2-----------------------#2023-11-27](https://towardsdatascience.com/batched-k-means-with-python-numba-and-cuda-c-3d4946c587b9?source=collection_archive---------2-----------------------#2023-11-27)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/batched-k-means-with-python-numba-and-cuda-c-3d4946c587b9?source=collection_archive---------2-----------------------#2023-11-27](https://towardsdatascience.com/batched-k-means-with-python-numba-and-cuda-c-3d4946c587b9?source=collection_archive---------2-----------------------#2023-11-27)
- en: How to Speed Up Your Data Analysis 1600x vs Scikit-Learn (With Code!)
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何加速你的数据分析 1600x 与 Scikit-Learn（附代码！）
- en: '[](https://medium.com/@alex.w.shao?source=post_page-----3d4946c587b9--------------------------------)[![Alex
    Shao](../Images/07a2cd8de23969f5732995b0fda2a25e.png)](https://medium.com/@alex.w.shao?source=post_page-----3d4946c587b9--------------------------------)[](https://towardsdatascience.com/?source=post_page-----3d4946c587b9--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----3d4946c587b9--------------------------------)
    [Alex Shao](https://medium.com/@alex.w.shao?source=post_page-----3d4946c587b9--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@alex.w.shao?source=post_page-----3d4946c587b9--------------------------------)[![Alex
    Shao](../Images/07a2cd8de23969f5732995b0fda2a25e.png)](https://medium.com/@alex.w.shao?source=post_page-----3d4946c587b9--------------------------------)[](https://towardsdatascience.com/?source=post_page-----3d4946c587b9--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----3d4946c587b9--------------------------------)
    [Alex Shao](https://medium.com/@alex.w.shao?source=post_page-----3d4946c587b9--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F723362c2a30f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbatched-k-means-with-python-numba-and-cuda-c-3d4946c587b9&user=Alex+Shao&userId=723362c2a30f&source=post_page-723362c2a30f----3d4946c587b9---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----3d4946c587b9--------------------------------)
    ·15 min read·Nov 27, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F3d4946c587b9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbatched-k-means-with-python-numba-and-cuda-c-3d4946c587b9&user=Alex+Shao&userId=723362c2a30f&source=-----3d4946c587b9---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F723362c2a30f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbatched-k-means-with-python-numba-and-cuda-c-3d4946c587b9&user=Alex+Shao&userId=723362c2a30f&source=post_page-723362c2a30f----3d4946c587b9---------------------post_header-----------)
    发表在[Towards Data Science](https://towardsdatascience.com/?source=post_page-----3d4946c587b9--------------------------------)
    ·15分钟阅读·2023年11月27日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F3d4946c587b9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbatched-k-means-with-python-numba-and-cuda-c-3d4946c587b9&user=Alex+Shao&userId=723362c2a30f&source=-----3d4946c587b9---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F3d4946c587b9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbatched-k-means-with-python-numba-and-cuda-c-3d4946c587b9&source=-----3d4946c587b9---------------------bookmark_footer-----------)![](../Images/79476c6455fbebdbb31e6dad7cb9a6ee.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F3d4946c587b9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbatched-k-means-with-python-numba-and-cuda-c-3d4946c587b9&source=-----3d4946c587b9---------------------bookmark_footer-----------)![](../Images/79476c6455fbebdbb31e6dad7cb9a6ee.png)'
- en: Image generated using [Midjourney](https://www.midjourney.com/) based on author’s
    drawing
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 图像由[Midjourney](https://www.midjourney.com/)基于作者的绘图生成
- en: Parallelizing data analysis workloads can be a daunting task, especially when
    there is no efficient off-the-shelf implementation available for your specific
    use case. In this tutorial, I will walk through the principles of writing CUDA
    kernels in both C and Python Numba, and how those principles can be applied to
    the classic k-means clustering algorithm. By the end of this article, you will
    be able to write a custom parallelized implementation of batched k-means in both
    C and Python, achieving up to 1600x speedup compared to the standard scikit-learn
    implementation. If you want to jump straight to the code, it is available in [Colab](https://colab.research.google.com/drive/1Il_OyESH92iVapFas0oTarF74IQcM3sq?usp=sharing).
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 并行化数据分析工作负载可能是一个令人畏惧的任务，尤其是当你的特定用例没有有效的现成实现时。在本教程中，我将讲解如何用 C 和 Python Numba
    编写 CUDA 内核的原则，以及这些原则如何应用于经典的 K-means 聚类算法。到文章结束时，你将能够用 C 和 Python 编写自定义的并行化批处理
    K-means 实现，与标准的 scikit-learn 实现相比，实现高达 1600 倍的加速。如果你想直接跳到代码，它可以在 [Colab](https://colab.research.google.com/drive/1Il_OyESH92iVapFas0oTarF74IQcM3sq?usp=sharing)
    中找到。
- en: Introduction
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 介绍
- en: 'We will study two frameworks for parallelizing on NVIDIA GPUs: Python Numba
    and CUDA C. We will then implement the same batched k-means algorithm in both
    and benchmark them against scikit-learn.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将研究在 NVIDIA GPU 上进行并行化的两个框架：Python Numba 和 CUDA C。然后，我们将在这两者中实现相同的批处理 K-means
    算法，并将它们与 scikit-learn 进行基准测试。
- en: Numba is a gentler learning curve for those who prefer Python over C. Numba
    compiles portions of your code into specialized CUDA functions called **kernels**.
    CUDA C, is a layer lower on the abstraction tree, which offers more refined control.
    The Colab examples are designed to mirror the algorithm implementations tightly,
    so once you understand one of them, you can easily understand the other.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: Numba 对于那些更喜欢 Python 而不是 C 的人来说，是一个更为温和的学习曲线。Numba 将你的代码部分编译成称为**内核**的专用 CUDA
    函数。CUDA C 是一个更低层次的抽象层，提供更细致的控制。Colab 示例设计得非常紧密地反映算法实现，因此一旦你理解了其中一个，就能轻松理解其他的。
- en: This tutorial grew out of a custom batched **k-means** implementation I had
    to write, which I will use as an illustrative example. Normally, k-means libraries
    are optimized to run a single instance of the algorithm on an extremely large
    dataset. However, in our project we needed to cluster millions of individual small
    datasets in parallel, which is not something we could find an off-the shelf library
    for.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 本教程来源于我必须编写的自定义批处理 **k-means** 实现，我将以此作为说明示例。通常，k-means 库是针对在极大数据集上运行算法的单实例进行优化的。然而，在我们的项目中，我们需要并行处理数百万个独立的小数据集，这不是我们能找到现成的库的。
- en: 'This article is split into four main sections: CUDA Basics, Parallelized K-means
    Algorithm, Python Numba, and CUDA C. I will try to keep the material relatively
    self-contained and briefly review concepts as needed.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 本文分为四个主要部分：CUDA 基础、并行化 K-means 算法、Python Numba 和 CUDA C。我将尽量使材料相对自包含，并在需要时简要回顾概念。
- en: CUDA Basics
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: CUDA 基础
- en: '**CPUs** are designed for fast sequential processing and **GPUs** are designed
    for massive parallelism. For our algorithm, we needed to run k-means on millions
    of small independent datasets, which lends itself perfectly to the GPU implementation.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '**CPU** 设计用于快速的顺序处理，而 **GPU** 设计用于大规模并行处理。对于我们的算法，我们需要在数百万个小型独立数据集上运行 K-means，这非常适合
    GPU 实现。'
- en: Our implementation will be using **CUDA**, short for Compute Unified Device
    Architecture, which is a C library and computing platform developed by NVIDIA
    to leverage GPUs for parallel computation.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的实现将使用 **CUDA**，即计算统一设备架构，是 NVIDIA 开发的 C 库和计算平台，用于利用 GPU 进行并行计算。
- en: 'To understand GPU kernels and device functions, here are some definitions:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 要理解 GPU 内核和设备函数，这里有一些定义：
- en: A **thread** is a single sequence of instructions that can be executed independently.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '**线程** 是可以独立执行的单个指令序列。'
- en: A **core** is a processing unit that can execute a single thread.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '**核心** 是可以执行单个线程的处理单元。'
- en: A **warp** is the smallest thread scheduling unit. Each warp consists of 32
    threads and schedules them onto cores.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '**warp** 是最小的线程调度单元。每个 warp 由 32 个线程组成，并将它们调度到核心上。'
- en: A **multiprocessor**, also referred to as a Streaming Multiprocessor (SM) is
    made up of a fixed number of cores.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '**多处理器**，也称为流式多处理器（SM），由固定数量的核心组成。'
- en: A **GPU kernel** is a function that is designed to be executed in parallel across
    many GPU threads. Kernel functions are be invoked by the CPU and executed on the
    GPU.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '**GPU 内核** 是一种设计为在多个 GPU 线程上并行执行的函数。内核函数由 CPU 调用，并在 GPU 上执行。'
- en: A **device function** is a function that can be called by a kernel function
    or another device function. This function is executed on the GPU.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '**设备函数** 是一种可以被内核函数或另一个设备函数调用的函数。此函数在 GPU 上执行。'
- en: These kernels are organized into a hierarchy of **grid**, **blocks**, and **threads**.
    In the context of a GPU, each thread is connected to a single core and executes
    a copy of the kernel.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 这些内核被组织成 **网格**、**块** 和 **线程** 的层次结构。在 GPU 的上下文中，每个线程连接到一个核心，并执行内核的一个副本。
- en: '![](../Images/7ca4bde115664dba1490f8d5a3508a04.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7ca4bde115664dba1490f8d5a3508a04.png)'
- en: GPUFunction[(x, y), z](Data Structure). Image by Author.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: GPUFunction[(x, y), z](数据结构)。图片来自作者。
- en: A **block** is a collection of threads that runs on one multiprocessor.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '**块** 是在一个多处理器上运行的一组线程。'
- en: A **grid** is an abstract array to organize blocks of threads. Grids are used
    to map kernel instances to threads by index.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '**网格** 是一种抽象数组，用于组织线程块。网格用于通过索引将内核实例映射到线程。'
- en: 'On the GPU, there are two types of memory: global memory and shared memory.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在 GPU 上，有两种类型的内存：全局内存和共享内存。
- en: '**Global memory** is stored on DRAM (Dynamic Random Access Memory). All threads
    can access its contents at the cost of high memory latency.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '**全局内存** 存储在 DRAM（动态随机存取内存）中。所有线程都可以访问其内容，但代价是较高的内存延迟。'
- en: '**Shared memory** is stored on cache and is private to threads within the same
    block. It can be accessed much faster than global memory.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '**共享内存** 存储在缓存中，且对同一块内的线程是私有的。它的访问速度远快于全局内存。'
- en: Parallelized K-Means Algorithm
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 并行化 K-Means 算法
- en: 'Now let’s introduce the k-means algorithm. K-means is an unsupervised clustering
    algorithm that partitions a dataset into k distinct, non-overlapping clusters.
    Given a set of data points, we first initialize k centroids, or the starting cluster
    centers:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们介绍 k-means 算法。K-means 是一种无监督的聚类算法，它将数据集划分为 k 个不同且不重叠的簇。给定一组数据点，我们首先初始化
    k 个质心，即起始的簇中心：
- en: '![](../Images/4a070de7a2e4b4e264a3e5d204301933.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4a070de7a2e4b4e264a3e5d204301933.png)'
- en: Centroid Initialization (k=3). Image by Author.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 质心初始化（k=3）。图片来自作者。
- en: 'Then, after choosing initial centroids, iteratively perform two steps:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，在选择初始质心后，迭代执行两个步骤：
- en: '**Assignment Step:** Each data point is assigned to its closest centroid based
    on Euclidean distance.'
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**分配步骤：** 每个数据点根据欧几里得距离被分配给离它最近的质心。'
- en: '**Update Step:** The centroid locations are reassigned to the mean of all the
    points assigned to that centroid in the previous step.'
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**更新步骤：** 质心位置重新分配为上一步骤中分配给该质心的所有点的均值。'
- en: These steps are repeated until convergence, or when centroid locations no longer
    change significantly. The output is the set of k cluster centroid coordinates
    together with an array marking cluster indices (called **labels**) for each of
    the original data points.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 这些步骤会重复进行，直到收敛，或当质心位置不再显著变化时。输出的是一组 k 个簇质心坐标，以及一个标记簇索引（称为**标签**）的数组，用于每个原始数据点。
- en: '![](../Images/afea9c8c722a794cc07e166dd1149978.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/afea9c8c722a794cc07e166dd1149978.png)'
- en: K-means Algorithm (k = 3). Image by Author.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: K-means 算法（k = 3）。图片来自作者。
- en: For a large dataset, centroid initialization can significantly affect the algorithm
    output. Therefore, the program tries multiple initial centroids, called initial
    seeds, and returns the result from the best seed. Each seed is selected from the
    initial dataset without replacement–meaning no initial centroid will be repeated.
    The optimal number of seeds for our algorithm is one third of the number of datapoints.
    In our program, because we run k-means on individual rows of 100 data points,
    the optimal number of seeds would be 33.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 对于大型数据集，质心初始化可能会显著影响算法的输出。因此，程序会尝试多个初始质心，称为初始种子，并返回最佳种子的结果。每个种子从初始数据集中选择，且不重复——这意味着没有初始质心会被重复。我们算法的最佳种子数量是数据点数量的三分之一。在我们的程序中，因为我们对每一行100个数据点运行
    k-means，最佳的种子数量为33。
- en: In our k-means function, the one million rows are represented by blocks while
    the threads represent the seeds. Threads in a block are organized into warps,
    the smallest thread scheduling unit in the hardware architecture. Each warp consists
    of 32 threads, making it optimal to set block sizes as multiples of 32\. Each
    block then outputs the data from the seed that has the smallest inertia, measured
    by the sum of Euclidean distance between cluster centers and their assigned points.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的 k-means 函数中，一百万行由块表示，而线程代表种子。块中的线程被组织成 warps，这是硬件架构中最小的线程调度单元。每个 warp 包含
    32 个线程，因此将块大小设置为 32 的倍数是最佳的。每个块然后输出具有最小惯性的种子数据，惯性由簇中心与其分配点之间的欧几里得距离之和来衡量。
- en: '![](../Images/3adcb0e40e317278702305fbf5431809.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3adcb0e40e317278702305fbf5431809.png)'
- en: Parallelized K-means — GPU side. Image by Author.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 并行化 K-means — GPU 端。图像由作者提供。
- en: Linked here is the [Colab](https://colab.research.google.com/drive/1Il_OyESH92iVapFas0oTarF74IQcM3sq?usp=sharing)
    where you can follow along in either Python or C.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '[Colab](https://colab.research.google.com/drive/1Il_OyESH92iVapFas0oTarF74IQcM3sq?usp=sharing)
    在这里链接，你可以在 Python 或 C 中跟随。'
- en: '[PRE0]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: In our k-means algorithm, we begin by setting global variables. We will need
    to reference them from both the CPU and GPU.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的 k-means 算法中，我们开始时会设置全局变量。我们需要从 CPU 和 GPU 中引用这些变量。
- en: While global variables are accessible from the kernel, the kernel cannot directly
    return values to the host. To circumvent this limitation, the CPU portion of our
    code passes two empty arrays to the kernel alongside the input data. These arrays
    will be used to copy the final centroid and label arrays back to the CPU.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管全局变量可以从内核访问，但内核不能直接将值返回给主机。为了绕过这一限制，我们的代码的 CPU 部分将两个空数组传递给内核，并附带输入数据。这些数组将用于将最终的质心和标签数组复制回
    CPU。
- en: With our data structures instantiated, we invoke the kernel, defining the grid
    and block dimensions as a tuple. The call to the kernel includes passing in our
    data, memory allocation for centroids and labels, and a state for the initial
    random centroid initialization.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在实例化了我们的数据结构后，我们调用内核，定义网格和块的维度作为一个元组。对内核的调用包括传递我们的数据、质心和标签的内存分配，以及初始随机质心初始化的状态。
- en: '[PRE1]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Once on **device** (aka the GPU), our code now exists simultaneously on the
    entire grid. To figure out where we are on the grid, we access the block and thread
    indices.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦在**设备**（即 GPU）上，我们的代码现在同时存在于整个网格中。为了确定我们在网格上的位置，我们访问块和线程索引。
- en: On the GPU, memory defaults to **global memory** which is stored on **DRAM**.
    **Shared memory** is private to threads within the same block.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在 GPU 上，内存默认为**全局内存**，存储在**DRAM**上。**共享内存**对同一块中的线程是私有的。
- en: By transferring the single row of data (that all threads in a block must read
    from) to shared memory, we cut down on memory access time. In the cuda_kmeans
    function, we create shared memory to store the row of data, centroids, labels,
    and the accuracy measure of each seed, called inertia.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将数据的单行（所有线程必须读取的行）转移到共享内存中，我们减少了内存访问时间。在 cuda_kmeans 函数中，我们创建了共享内存来存储数据行、质心、标签和每个种子的准确性度量，称为惯性。
- en: In our program, each thread corresponds to one seed, and all threads in a block
    work on the same row of data. For each **block**, one thread sequentially creates
    32 seeds and aggregates their outcomes into a single data structure for the other
    threads in the block.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的程序中，每个线程对应一个种子，所有线程在一个块中处理相同的数据行。对于每个**块**，一个线程按顺序创建 32 个种子，并将它们的结果聚合到一个数据结构中，以供块中的其他线程使用。
- en: When a subsequent step in the algorithm relies on this aggregation to have been
    completed, the threads must be **synchronized** using the built-in CUDA syncthreads()
    function. **NB:** one must be **very careful** about the placement of syncthreads()
    calls, since attempting to synchronize threads when not all of them have had a
    chance to complete can lead to **deadlocks** and hanging of the entire program.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 当算法中的后续步骤依赖于此聚合已完成时，线程必须使用内置的 CUDA syncthreads() 函数进行**同步**。**注意：** 必须对 syncthreads()
    调用的位置**非常小心**，因为尝试在不是所有线程都已完成时同步线程可能会导致**死锁**和整个程序的挂起。
- en: Our kernel function, which is described in pseudocode below, is called cuda_kmeans.
    This function takes care of arranging the process outlined above, making space
    for the results from all 32 seeds and selecting the best seed for the final output
    in centroids and labels.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: From cuda_kmeans we then call the actual k-means algorithm, passing the newly
    instantiated shared memory. In our k-means algorithm, we start by picking the
    initial centroids and then sorting them from smallest to largest. We iteratively
    assign the data points to the nearest centroid and update centroid locations until
    convergence.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: To be able to determine whether convergence has been achieved, we use a helper
    function called find_yard_stick. This function calculates and returns the smallest
    distance between two initial centroids (yard_stick). Our convergence condition
    is met when none of the centroids have moved by more than yard_stick times epsilon
    during a single iteration.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: After convergence, we return to cuda_kmeans. Here, we determine the best seed
    by calculating the squared Euclidean distance between each centroid and its data
    points. The seed with the most effective grouping–indicated by the smallest **inertia**–is
    considered the best. We then take the centroids and labels from this seed and
    copy them to a single row in our output arrays. Once all blocks are done, these
    outputs are then copied back to the host (CPU).
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e023eb19ea4a1684ef6fee357567e11a.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
- en: Data Transfers During K-means Algorithm. Image by Author.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: An Intro to Numba
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The easiest way to design custom kernels is with Numba. **Numba** is a Python
    library that can be used to compile Python code into CUDA kernels.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/934081746e8272fba1084b302a68e739.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
- en: Levels of Abstraction. Image by Author.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: 'Under the surface, Numba interfaces with CUDA. To parallelize your code, Numba
    compiles your designated GPU code into a kernel and passes it to the GPU, dividing
    the program logic into two main parts:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: CPU level code
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: GPU level code
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Using Numba, you separate and hand off the sequential and parallelizable parts
    of code to the CPU and GPU. To compile a function for the GPU, the programmer
    uses the **@cuda.jit** decorator above the function definition, thereby transforming
    this function into a **kernel** which is called from the CPU (host) but is executed
    in parallel on the GPU (device).
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: Python Numba
  id: totrans-73
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Link to the [Colab](https://colab.research.google.com/drive/1Il_OyESH92iVapFas0oTarF74IQcM3sq?usp=sharing).
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: Numba serves as a bridge between Python code and the CUDA platform. Because
    the Python code is nearly identical to the algorithm pseudocode above, I am only
    going to provide a couple of examples of key relevant syntax.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: After instantiating the necessary global variables and data structures, we can
    call the kernel, cuda_kmeans, from the host. Numba requires two tuples for the
    dimensionality of the blocks and threads. Because we will be using single-dimensional
    blocks and threads, the second index in each tuple is empty. We also pass in our
    data structures and an array of random states for random seed instantiation.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在实例化了必要的全局变量和数据结构后，我们可以从主机调用内核 cuda_kmeans。Numba 需要两个元组来表示块和线程的维度。由于我们将使用一维块和线程，因此每个元组中的第二个索引为空。我们还传入了我们的数据结构和一个随机状态数组用于随机种子的实例化。
- en: '[PRE4]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: We use the Numba @cuda.jit() decorator to mark the for GPU compilation. Using
    the notation cuda.blockIdx.x and cuda.threadIdx.x we obtain the current index
    of the kernel. The shared arrays are instantiated using cuda.shared.array with
    two arguments, shape and the type, both of which must be known at compile time.
    After getting centroids and filling the row with data, we call the kmeans function,
    populate shared arrays and make a call to cuda.syncthreads().
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用 Numba 的 @cuda.jit() 装饰器来标记进行 GPU 编译。使用 cuda.blockIdx.x 和 cuda.threadIdx.x
    符号，我们可以获得内核的当前索引。共享数组通过 cuda.shared.array 使用两个参数进行实例化，形状和类型，这两者必须在编译时已知。在获取质心并填充行数据后，我们调用
    kmeans 函数，填充共享数组，并调用 cuda.syncthreads()。
- en: '[PRE5]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'K-means is a **device function** because it is called from the kernel function.
    Therefore, we must specify device=True in the decorator: **@cuda.jit(device=True)**.
    The k-means function then gets the current row for labels and centroids and runs
    until convergence.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: K-means 是一个 **设备函数**，因为它是从内核函数中调用的。因此，我们必须在装饰器中指定 device=True：**@cuda.jit(device=True)**。k-means
    函数随后获取当前行的标签和质心，并运行直到收敛。
- en: With just an extra dozen lines of code and a little bit of effort, your Python
    code can become an optimized kernel ready for parallel use.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 只需额外增加十几行代码和一点点努力，你的 Python 代码就可以成为一个准备好进行并行处理的优化内核。
- en: Our parallelized k-means does cut down our compute time drastically–however,
    wrapping and compiling a high level language like Python is not necessarily optimal.
    Curious to see if writing the code in C would speed up our project, I dove into
    the realm of CUDA C.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的并行 k-means 确实大幅减少了计算时间——然而，将像 Python 这样的高级语言进行包装和编译并不一定是最优的。出于好奇，我想看看用 C
    语言编写代码是否能加速我们的项目，于是我深入了 CUDA C 的领域。
- en: An Intro to C
  id: totrans-84
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: C 语言简介
- en: In Python, memory and type allocation are automatic, whereas in C, memory can
    be allocated on the stack or the heap, both of which require explicit type declaration
    and are allocated a fixed amount of memory. Stack memory is automatically allocated
    and freed by the compiler, while heap memory is manually allocated at runtime
    using functions like malloc(), with the programmer responsible for its deallocation.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Python 中，内存和类型分配是自动的，而在 C 语言中，内存可以分配在栈上或堆上，这两者都需要显式的类型声明，并分配固定量的内存。栈内存由编译器自动分配和释放，而堆内存由程序员在运行时使用
    malloc() 等函数手动分配，程序员负责其释放。
- en: Pointers are tools that hold the memory addresses of variables. The type of
    data the pointer refers to is defined during declaration. Pointers are specified
    using an asterisk (*). To retrieve the address of a variable, called referencing,
    you use an ampersand (&). To access value from a pointer, you dereference the
    pointer again using an asterisk.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 指针是持有变量内存地址的工具。指针所指向的数据类型在声明时定义。指针使用星号 (*) 指定。要获取变量的地址，称为引用，你需要使用与符号 (&)。要从指针中访问值，你再次使用星号来解除引用。
- en: A double pointer, or a pointer to a pointer, stores the address of a pointer.
    This is useful for modifying addresses of other pointers when passing arrays to
    functions. When passing arrays to functions, they’re passed as pointers to their
    first element, without size information, relying on pointer arithmetic to index
    through the array. To return an array from a function, you would pass the address
    of a pointer using & and receive it with a double pointer **, allowing you to
    modify the address of the original pointer, thereby passing the array.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 双指针，或指向指针的指针，存储指针的地址。这在将数组传递给函数时修改其他指针的地址非常有用。当将数组传递给函数时，它们作为指向其第一个元素的指针传递，没有大小信息，依赖于指针算术来索引数组。要从函数返回数组，你需要使用
    & 传递指针的地址，并用双指针 ** 接收它，这样你就可以修改原始指针的地址，从而传递数组。
- en: '[PRE6]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: CUDA C
  id: totrans-89
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: CUDA C
- en: Link to the [Colab](https://colab.research.google.com/drive/1Il_OyESH92iVapFas0oTarF74IQcM3sq?usp=sharing).
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 链接到 [Colab](https://colab.research.google.com/drive/1Il_OyESH92iVapFas0oTarF74IQcM3sq?usp=sharing)。
- en: CUDA is a computing platform that leverages NVIDIA GPUs to parallelize complex
    computational problems. Building on your newfound mastery of C (joking), the CUDA
    C code structure is literally identical to the pseudocode structure we stepped
    through.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: CUDA 是一个计算平台，利用 NVIDIA GPU 来并行处理复杂的计算问题。在你刚掌握 C 的基础上（开玩笑的），CUDA C 代码的结构与我们步进的伪代码结构完全相同。
- en: On the CPU side, we set up some constants that tell the algorithm what to expect,
    import libraries, initialize our variables, and define some macros.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在 CPU 端，我们设置一些常量来告诉算法预期的结果，导入库，初始化变量，并定义一些宏。
- en: '[PRE7]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Let us break the differences down.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们分解这些差异。
- en: The main function starts by initializing the data by creating a pointer and
    passing it to initInputData as an address to a pointer. The function receives
    inputData as a pointer to a pointer (float** input), which allows the function
    to modify the address held by the original pointer. Input is then pointed to a
    GPU memory address that is initialized using cudaMalloc and filled using cudaMemcpy,
    copying data from the temporary host array sample_data, already populated with
    random numbers.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 主函数首先通过创建一个指针并将其作为地址传递给 initInputData 来初始化数据。该函数接收 inputData 作为指向指针的指针（float**
    input），这使得函数能够修改原始指针所持有的地址。然后，输入被指向通过 cudaMalloc 初始化的 GPU 内存地址，并使用 cudaMemcpy
    填充，从临时主机数组 sample_data 中复制数据，该数组已经填充了随机数。
- en: Then, the code allocates memory on the device to hold the results from our k-means
    function. The function uses make_cudaExtent to create a cudaExtent object whose
    purpose is to encapsulate the dimensions of the multidimensional array.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，代码在设备上分配内存来保存来自 k-means 函数的结果。该函数使用 make_cudaExtent 创建一个 cudaExtent 对象，目的是封装多维数组的维度。
- en: The type cudaPitchedPointer is used to define a pointer that can address this
    pitched memory space. This type of pointer is specifically designed to work with
    memory allocated by cudaMalloc3D, which takes both a cudaPitchedPtr and cudaExtent
    object to allocate the linear memory on the GPU.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 类型 cudaPitchedPointer 用于定义一个能够寻址这个倾斜内存空间的指针。这种指针专门用于处理由 cudaMalloc3D 分配的内存，后者需要
    cudaPitchedPtr 和 cudaExtent 对象来分配 GPU 上的线性内存。
- en: '[PRE8]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Going into the GPU code,we define the grid such that each block corresponds
    to a row of data and each thread corresponds to a seed.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 进入 GPU 代码时，我们定义网格，使每个块对应于一行数据，每个线程对应于一个种子。
- en: The seeds are initialized by a single thread in each block, ensuring entirely
    different seeds.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 种子由每个块中的一个线程初始化，确保种子完全不同。
- en: '[PRE9]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The CUDA C code uses shared memory for the data, centroids, labels, and errors.
    However, unlike Python, the code takes the pointers to shared memory and stores
    them in a struct, which is just a method to pass variables en masse. Finally,
    cuda_kmeans calls the actual k-means algorithm and passes core_params.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: CUDA C 代码使用共享内存存储数据、质心、标签和错误。然而，与 Python 不同的是，代码将共享内存的指针存储在一个结构体中，这只是传递变量的一种方法。最后，cuda_kmeans
    调用实际的 k-means 算法并传递 core_params。
- en: '[PRE10]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: On the device function, before anything, we extract the values from the core_params
    struct into variables using the DECLARE_CORE_PARAMS macro.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 在设备函数中，首先我们使用 DECLARE_CORE_PARAMS 宏从 core_params 结构体中提取值到变量中。
- en: Then, we run the same k-means algorithm as in Python, with the only difference
    being we pass the struct instead of variables and have to manage memory, pointers,
    and types.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们运行与 Python 中相同的 k-means 算法，唯一的区别是我们传递结构体而不是变量，并且需要管理内存、指针和类型。
- en: Benchmarks
  id: totrans-106
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基准测试
- en: To compare our algorithms against non-parallelized k-means, we import the scikit-learn
    k-means module.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将我们的算法与非并行化的 k-means 进行比较，我们导入了 scikit-learn 的 k-means 模块。
- en: For our benchmark, we run 100,000 rows by 100 columns with three clusters. Because
    scikit-learn doesn’t have a parallelized k-means for different rows, we run the
    rows sequentially in a for loop.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的基准测试中，我们运行 100,000 行和 100 列的三簇数据。由于 scikit-learn 没有针对不同行的并行化 k-means，我们在
    for 循环中顺序运行这些行。
- en: For our benchmarks in colab, we use the free T4 GPU Colab instance.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在 colab 的基准测试中，我们使用免费的 T4 GPU Colab 实例。
- en: '![](../Images/43b25f0e293f72b6503a1ede9ed7c5c2.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/43b25f0e293f72b6503a1ede9ed7c5c2.png)'
- en: Image by Author.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供。
- en: The results are good — the Python Numba code is two orders of magnitude faster
    than non-parallelized CPU code and the CUDA C code is three orders of magnitude
    faster. The kernel functions are easily scalable and the algorithm can be altered
    to support clustering in higher dimensions.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 结果很好——Python Numba代码比非并行化的CPU代码快两个数量级，而CUDA C代码快三个数量级。内核函数易于扩展，算法可以修改以支持更高维度的聚类。
- en: Note that the randomized initial centroid generation in both the C and Python
    algorithms is not fully optimized to use all cores. When improved, the Python
    algorithm may very well approach the C code run time.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，C和Python算法中的随机初始质心生成并没有完全优化以使用所有核心。当优化后，Python算法的运行时间可能会非常接近C代码的运行时间。
- en: '![](../Images/a29d96b77f789ca427e1d575f9d8871d.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a29d96b77f789ca427e1d575f9d8871d.png)'
- en: Runtimes based on free Colab T4 GPU on 11/23/2023\. Image by Author.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 基于2023年11月23日的免费Colab T4 GPU的运行时间。图像由作者提供。
- en: After running the k-means function a hundred times on different datasets and
    recording the resulting times–we notice that the first iteration is significantly
    slower due to the time it takes to compile both C and Python in Colab.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 在不同数据集上运行k-means函数一百次并记录结果时间后，我们注意到第一次迭代显著较慢，因为编译C和Python代码在Colab中需要时间。
- en: Conclusion
  id: totrans-117
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: Now you should be ready to write your own custom GPU kernels! One remaining
    question may be — should you use CUDA C or Numba for parallelized data processing
    workloads? It depends. Both are vastly faster than off-the-shelf scikit-learn.
    Even though in my case the CUDA C batched k-means implementation turned out to
    be about 3.5x faster than an equivalent written using Numba, Python offers some
    important advantages such as readability and less reliance on specialized C programming
    skills in teams that mostly work in Python. Additionally, the runtime of your
    specific implementation will depend on how well you have optimized your code to
    e.g. not trigger serialized operations on the GPU. In conclusion, if you are new
    to both C and parallelized programming, I would suggest starting with Numba to
    prototype your algorithm, and then translate it into CUDA C if you need additional
    speedup.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你应该准备好编写自己的自定义GPU内核了！一个剩下的问题可能是——你应该使用CUDA C还是Numba来处理并行的数据处理工作负载？这要看情况。两者都比现成的scikit-learn快得多。虽然在我的案例中，CUDA
    C的批处理k-means实现比使用Numba编写的等效实现快了大约3.5倍，但Python提供了一些重要的优势，比如可读性以及对专门C编程技能的依赖较少，特别是对于主要使用Python的团队。此外，你的具体实现的运行时间将取决于你是否优化了代码，例如，避免在GPU上触发序列化操作。总之，如果你对C和并行编程都不熟悉，我建议先使用Numba来原型化你的算法，然后如果需要额外的加速，再将其转化为CUDA
    C。
- en: References
  id: totrans-119
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[Scikit-learn: Machine Learning in Python](https://jmlr.csail.mit.edu/papers/v12/pedregosa11a.html),
    Pedregosa *et al.*, JMLR 12, pp. 2825–2830, 2011.'
  id: totrans-120
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[Scikit-learn: Python中的机器学习](https://jmlr.csail.mit.edu/papers/v12/pedregosa11a.html)，Pedregosa
    *等人*，JMLR 12，第2825–2830页，2011年。'
- en: 'NVIDIA, Vingelmann, P. & Fitzek, F.H.P., 2020\. CUDA, release: 10.2\. 89, Available
    at: [https://developer.nvidia.com/cuda-toolkit.](https://developer.nvidia.com/cuda-toolkit.)'
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: NVIDIA, Vingelmann, P. & Fitzek, F.H.P., 2020\. CUDA，版本：10.2\. 89，获取地址：[https://developer.nvidia.com/cuda-toolkit.](https://developer.nvidia.com/cuda-toolkit.)
- en: 'Lam, Siu Kwan, Antoine Pitrou, and Stanley Seibert. [“Numba: A llvm-based python
    jit compiler.”](https://numba.pydata.org) *Proceedings of the Second Workshop
    on the LLVM Compiler Infrastructure in HPC*. 2015.'
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Lam, Siu Kwan, Antoine Pitrou, 和 Stanley Seibert. [“Numba: 基于llvm的python JIT编译器。”](https://numba.pydata.org)
    *第二届LLVM编译器基础设施在HPC研讨会论文集*。2015年。'
- en: 'Harris, C.R., Millman, K.J., van der Walt, S.J. et al. “*Array programming
    with NumPy*.” Nature 585, 357–362 (2020). DOI: [10.1038/s41586–020–2649–2](https://doi.org/10.1038/s41586-020-2649-2).
    ([Publisher link](https://www.nature.com/articles/s41586-020-2649-2)).'
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Harris, C.R., Millman, K.J., van der Walt, S.J. 等. “*使用NumPy的数组编程*。” Nature
    585，357–362（2020年）。DOI: [10.1038/s41586–020–2649–2](https://doi.org/10.1038/s41586-020-2649-2)。
    ([出版商链接](https://www.nature.com/articles/s41586-020-2649-2))'
