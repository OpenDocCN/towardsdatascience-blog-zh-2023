["```py\nrail_str = \"\"\"\n<rail version=\"0.1\">\n<output>\n   <string\n       name=\"generated_sql\"\n       description=\"Generate SQL for the given natural language instruction.\"\n       format=\"bug-free-sql\"\n       on-fail-bug-free-sql=\"reask\" \n   />\n</output>\n\n<prompt>\nGenerate a valid SQL query for the following natural language instruction:\n{{nl_instruction}}\n@complete_json_suffix\n</prompt>\n\n</rail>\n\"\"\"\n```", "```py\nimport guardrails as gd\nfrom rich import print\nguard = gd.Guard.from_rail_string(rail_str)\n```", "```py\nONLY return a valid JSON object (no other text is necessary), where the key of the field in JSON is the `name` \nattribute of the corresponding XML, and the value is of the type specified by the corresponding XML's tag. The JSON\nMUST conform to the XML format, including any types and format requests e.g. requests for lists, objects and \nspecific types. Be correct and concise. If you are unsure anywhere, enter `None`.\n```", "```py\nimport openai\nraw_llm_response, validated_response = guard(\nopenai.Completion.create,\nprompt_params={\n\"nl_instruction\": \"Select the name of the employee who has the highest salary.\"\n},\nengine=\"text-davinci-003\",\nmax_tokens=2048,\ntemperature=0,)\n```", "```py\n{'generated_sql': 'SELECT name FROM employee ORDER BY salary DESC LIMIT 1'}\n```", "```py\nfrom rich import print\nfrom langchain.output_parsers import GuardrailsOutputParser\nfrom langchain.prompts import PromptTemplate\nfrom langchain.llms import OpenAI\n\noutput_parser = GuardrailsOutputParser.from_rail_string(rail_str, api=openai.ChatCompletion.create)\n```", "```py\nprompt = PromptTemplate(\ntemplate=output_parser.guard.base_prompt,\ninput_variables=output_parser.guard.prompt.variable_names,\n)\n```", "```py\ndefine user express greeting\n  \"hello there\"\n  \"hi\"\n\ndefine user request help\n  \"I need help with something.\"\n  \"I need your help.\"\n```", "```py\ndefine bot express greeting\n  \"Hello there!\"\n  \"Hi!\"\ndefine bot ask welfare\n  \"How are you feeling today?\"\n```", "```py\ndefine flow hello\n  user express greeting\n  bot express greeting\n  bot ask welfare\n```", "```py\ndefine flow\n  ...\n  $name = \"John\"\n  $allowed = execute check_if_allowed\n```", "```py\ndefine user express greeting\n  \"Hello\"\n  \"Hi\"\n  \"What's uup?\"\n\ndefine bot express greeting\n  \"Hi there!\"\n\ndefine bot ask how are you\n  \"How are you doing?\"\n  \"How's it going?\"\n  \"How are you feeling today?\"\n```", "```py\ndefine flow greeting\n  user express greeting\n  bot express greeting\n\n  bot ask how are you\n\n  when user express feeling good\n   bot express positive emotion\n\n  else when user express feeling bad\n   bot express empathy\n```", "```py\ndefine user ask about politics\n  \"What do you think about the government?\"\n  \"Which party should I vote for?\"\n\ndefine user ask about stock market\n  \"Which stock should I invest in?\"\n  \"Would this stock 10x over the next year?\"\n```", "```py\ndefine flow politics\n  user ask about politics\n  bot inform cannot respond\n\ndefine flow stock market\n  user ask about stock market\n  bot inform cannot respond\n```", "```py\ndefine user express insult\n  \"You are stupid\"\n\n# Basic guardrail against insults.\ndefine flow\n  user express insult\n  bot express calmly willingness to help\n\n# Here we use the QA chain for anything else.\ndefine flow\n  user ...\n  $answer = execute qa_chain(query=$last_user_message)\n  bot $answer\n```", "```py\nfrom nemoguardrails import LLMRails, RailsConfig\n\nconfig = RailsConfig.from_path(\"path/to/config\")\napp = LLMRails(config)\n\nqa_chain = RetrievalQA.from_chain_type(\n    llm=app.llm, chain_type=\"stuff\", retriever=docsearch.as_retriever())\napp.register_action(qa_chain, name=\"qa_chain\")\n\nhistory = [\n    {\"role\": \"user\", \"content\": \"What is the current unemployment rate?\"}\n]\nresult = app.generate(messages=history)\n```"]