["```py\nimport ray\nfrom typing import Dict, Tuple\nimport numpy as np\nimport torchvision.transforms as transforms\nfrom ray.data.preprocessors import Chain, BatchMapper, TorchVisionPreprocessor\n\ndef get_ds(batch_size, num_records):\n    # create a raw Ray tabular dataset\n    ds = ray.data.range(num_records)\n\n    # map an integer to a random image-label pair\n    def synthetic_ds(batch: Tuple[int]) -> Dict[str, np.ndarray]:\n        labels = batch['id']\n        batch_size = len(labels)\n        images = np.random.randn(batch_size, 224, 224, 3).astype(np.float32)\n        labels = np.array([label % 1000 for label in labels]).astype(\n                                                               dtype=np.int64)\n        return {\"image\": images, \"label\": labels}\n\n    # the first step of the prepocessor maps batches of ints to\n    # random image-label pairs\n    synthetic_data = BatchMapper(synthetic_ds, \n                                 batch_size=batch_size, \n                                 batch_format=\"numpy\")\n\n    # we define a torchvision transform that converts the numpy pairs to \n    # tensors and then applies a series of gaussian blurs to simulate\n    # heavy preprocessing   \n    transform = transforms.Compose(\n        [transforms.ToTensor()] + [transforms.GaussianBlur(11)]*10\n    )\n\n    # the second step of the prepocessor appplies the torchvision tranform\n    vision_preprocessor = TorchVisionPreprocessor(columns=[\"image\"], \n                                                  transform=transform)\n\n    # combine the preprocessing steps\n    preprocessor = Chain(synthetic_data, vision_preprocessor)\n    return ds, preprocessor\n```", "```py\nimport time\nfrom ray import train\nfrom ray.air import session\nimport torch.nn as nn\nimport torch.optim as optim\nfrom timm.models.vision_transformer import VisionTransformer\n\n# build a ViT model using timm\ndef build_model():\n    return VisionTransformer()\n\n# define the training loop per worker\ndef train_loop_per_worker(config):\n    # wrap the PyTorch model with a Ray object\n    model = train.torch.prepare_model(build_model())\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n\n    # get the appropriate dataset shard\n    train_dataset_shard = session.get_dataset_shard(\"train\")\n\n    # create an iterator that returns batches from the dataset\n    train_dataset_batches = train_dataset_shard.iter_torch_batches(\n        batch_size=config[\"batch_size\"],\n        prefetch_batches=config[\"prefetch_batches\"],\n        device=train.torch.get_device()\n    )\n\n    t0 = time.perf_counter()\n\n    for i, batch in enumerate(train_dataset_batches):\n        # get the inputs and labels\n        inputs, labels = batch[\"image\"], batch[\"label\"]\n\n        # zero the parameter gradients\n        optimizer.zero_grad()\n\n        # forward + backward + optimize\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n\n        # print statistics\n        if i % 100 == 99:  # print every 100 mini-batches\n            avg_time = (time.perf_counter()-t0)/100\n            print(f\"Iteration {i+1}: avg time per step {avg_time:.3f}\")\n            t0 = time.perf_counter()\n\n    metrics = dict(running_loss=loss.item())\n    session.report(metrics)\n```", "```py\nfrom ray.train.torch import TorchTrainer\nfrom ray.air.config import ScalingConfig\n\ndef train_model():\n    # we will configure the number of workers, the size of our\n    # dataset, and the size of the data storage according to the\n    # available resources \n    num_gpus = int(ray.available_resources().get(\"GPU\", 0))\n\n    # set the number of training workers according to the number of GPUs\n    num_workers = num_gpus if num_gpus > 0 else 1\n\n    # we set the batch size based on the GPU memory capacity of the\n    # Amazon EC2 g5 instance family\n    batch_size = 64\n\n    # create a synthetic dataset with enough data to train for 1000 steps\n    num_records = batch_size * 1000 * num_workers\n    ds, preprocessor = get_ds(batch_size, num_records)\n\n    ds = preprocessor(ds) \n    trainer = TorchTrainer(\n        train_loop_per_worker=train_loop_per_worker,\n        train_loop_config={\"batch_size\": batch_size},\n        datasets={\"train\": ds},\n        scaling_config=ScalingConfig(num_workers=num_workers, \n                                     use_gpu=num_gpus > 0),\n    )\n    trainer.fit()\n```", "```py\nimport time\nimport subprocess\nfrom sagemaker_training import environment\n\nif __name__ == \"__main__\":\n    # use the Environment() class to auto-discover the SageMaker cluster\n    env = environment.Environment()\n    if env.current_instance_group == 'gpu' and \\\n             env.current_instance_group_hosts.index(env.current_host) == 0:\n        # the head node starts a ray cluster\n        p = subprocess.Popen('ray start --head --port=6379',\n                             shell=True).wait()\n        ray.init()\n\n        # calculate the total number of nodes in the cluster\n        groups = env.instance_groups_dict.values()\n        cluster_size = sum(len(v['hosts']) for v in list(groups))\n\n        # wait until all SageMaker nodes have connected to the Ray cluster\n        connected_nodes = 1\n        while connected_nodes < cluster_size:\n            time.sleep(1)\n            resources = ray.available_resources().keys()\n            connected_nodes = sum(1 for s in list(resources) if 'node' in s)\n\n        # call the training sequence\n        train_model()\n\n        # tear down the ray cluster\n        p = subprocess.Popen(\"ray down\", shell=True).wait()\n    else:\n        # worker nodes attach to the head node\n        head = env.instance_groups_dict['gpu']['hosts'][0]\n        p = subprocess.Popen(\n            f\"ray start --address='{head}:6379'\",\n            shell=True).wait()\n\n        # utility for checking if the cluster is still alive\n        def is_alive():\n            from subprocess import Popen\n            p = Popen('ray status', shell=True)\n            p.communicate()[0]\n            return p.returncode\n\n        # keep node alive until the process on head node completes\n        while is_alive() == 0:\n            time.sleep(10)\n```", "```py\nfrom sagemaker.pytorch import PyTorch\nfrom sagemaker.instance_group import InstanceGroup\ncpu_group = InstanceGroup(\"cpu\", \"ml.c5.4xlarge\", 1)\ngpu_group = InstanceGroup(\"gpu\", \"ml.g5.xlarge\", 1)\n\nestimator = PyTorch(\n    entry_point='train.py',\n    source_dir='./source_dir',\n    framework_version='2.0.0',\n    role='<arn role>',\n    py_version='py310',\n    job_name='hetero-cluster',\n    instance_groups=[gpu_group, cpu_group]\n)\n\nestimator.fit()\n```"]