- en: 'De-coded: Transformers explained in plain English'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/de-coded-transformers-explained-in-plain-english-877814ba6429?source=collection_archive---------0-----------------------#2023-10-09](https://towardsdatascience.com/de-coded-transformers-explained-in-plain-english-877814ba6429?source=collection_archive---------0-----------------------#2023-10-09)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: No code, maths, or mention of Keys, Queries and Values
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@chris.p.hughes10?source=post_page-----877814ba6429--------------------------------)[![Chris
    Hughes](../Images/87b16cd8677739b12294380fb00fde85.png)](https://medium.com/@chris.p.hughes10?source=post_page-----877814ba6429--------------------------------)[](https://towardsdatascience.com/?source=post_page-----877814ba6429--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----877814ba6429--------------------------------)
    [Chris Hughes](https://medium.com/@chris.p.hughes10?source=post_page-----877814ba6429--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ff13df9df155e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fde-coded-transformers-explained-in-plain-english-877814ba6429&user=Chris+Hughes&userId=f13df9df155e&source=post_page-f13df9df155e----877814ba6429---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----877814ba6429--------------------------------)
    ·15 min read·Oct 9, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F877814ba6429&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fde-coded-transformers-explained-in-plain-english-877814ba6429&user=Chris+Hughes&userId=f13df9df155e&source=-----877814ba6429---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F877814ba6429&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fde-coded-transformers-explained-in-plain-english-877814ba6429&source=-----877814ba6429---------------------bookmark_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: Since their [introduction in 2017](https://arxiv.org/abs/1706.03762), transformers
    have emerged as a prominent force in the field of Machine Learning, revolutionizing
    the capabilities of [major translation](https://blog.research.google/2020/06/recent-advances-in-google-translate.html)
    and [autocomplete](https://github.blog/2023-05-17-how-github-copilot-is-getting-better-at-understanding-your-code/)
    services.
  prefs: []
  type: TYPE_NORMAL
- en: Recently, the popularity of transformers has soared even higher with the advent
    of large language models like OpenAI’s [ChatGPT](https://openai.com/blog/chatgpt),
    [GPT-4](https://cdn.openai.com/papers/gpt-4.pdf#:~:text=This%20technical%20report%20presents%20GPT-4%2C%20a%20large%20multimodal,as%20dialogue%20systems%2C%20text%20summarization%2C%20and%20machine%20translation.),
    and Meta’s [LLama](https://ai.meta.com/blog/large-language-model-llama-meta-ai/).
    These models, which have garnered immense attention and excitement, are all built
    on the foundation of the transformer architecture. By leveraging the power of
    transformers, these models have achieved remarkable breakthroughs in natural language
    understanding and generation; exposing these to the general public.
  prefs: []
  type: TYPE_NORMAL
- en: Despite a lot of [good resources](http://jalammar.github.io/illustrated-transformer/)
    which break down how transformers work, I found myself in a position where I understood
    the how the mechanics worked mathematically but found it difficult to explain
    how a transformer works intuitively. After conducting many interviews, speaking
    to my colleagues, and giving a lightning talk on the subject, it seems that many
    people share this problem!
  prefs: []
  type: TYPE_NORMAL
- en: In this blog post, I shall aim to provide a high-level explanation of how transformers
    work without relying on code or mathematics. My goal is to avoid confusing technical
    jargon and comparisons with previous architectures. Whilst I’ll try to keep things
    as simple as possible, this won’t be easy as transformers are quite complex, but
    I hope it will provide a better intuition of what they do and how they do it.
  prefs: []
  type: TYPE_NORMAL
- en: What is a Transformer?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A transformer is a type of neural network architecture which is well suited
    for tasks that involve processing sequences as inputs. Perhaps the most common
    example of a sequence in this context is a sentence, which we can think of as
    an ordered set of words.
  prefs: []
  type: TYPE_NORMAL
- en: The aim of these models is to create a numerical representation for each element
    within a sequence; encapsulating essential information about the element and its
    neighbouring context. The resulting numerical representations can then be passed
    on to downstream networks, which can leverage this information to perform various
    tasks, including generation and classification.
  prefs: []
  type: TYPE_NORMAL
- en: By creating such rich representations, these models enable downstream networks
    to better understand the underlying patterns and relationships within the input
    sequence, which enhances their ability to generate coherent and contextually relevant
    outputs.
  prefs: []
  type: TYPE_NORMAL
- en: The key advantage of transformers lies in their ability to handle long-range
    dependencies within sequences, as well as being highly efficient; capable of processing
    sequences in parallel. This is particularly useful for tasks such as machine translation,
    sentiment analysis, and text generation.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4d9874a6d0be28e2def6d15c3e1efe41.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Image generated by the [Azure OpenAI Service DALL-E model](https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/models#dall-e-preview)
    with the following prompt: “*The green and black Matrix code in the shape of Optimus
    Prime”*'
  prefs: []
  type: TYPE_NORMAL
- en: '**What goes into the Transformer?**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To feed an input into a transformer, we must first convert it into a sequence
    of tokens; a set of integers that represent our input.
  prefs: []
  type: TYPE_NORMAL
- en: As transformers were first applied in the NLP domain, let’s consider this scenario
    first. The simplest way to convert a sentence into a series of tokens is to define
    a *vocabulary* which acts as a lookup table, mapping words to integers; we can
    reserve a specific number to represent any word which is not contained in this
    vocabulary, so that we can always assign an integer value.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/271353078c1b211d9723ec89e696c761.png)'
  prefs: []
  type: TYPE_IMG
- en: In practice, this is a naïve way of encoding text, as words such as *cat* and
    *cats* are treated as completely different tokens, despite them being singular
    and plural descriptions of the same animal! To overcome this, different tokenisation
    strategies — such as [byte-pair encoding](https://huggingface.co/learn/nlp-course/chapter6/5?fw=pt)
    — have been devised which break words up into smaller chunks before indexing them.
    Additionally, it is often useful to add special tokens to represent characteristics
    such as the start and end of a sentence, to provide additional context to the
    model.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s consider the following example, to better understand the tokenization
    process.
  prefs: []
  type: TYPE_NORMAL
- en: '*“Hello there, isn’t the weather nice today in Drosval?”*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Drosval is a name generated by GPT-4 using the following prompt: *“Can you
    create a fictional place name that sounds like it could belong to* [*David Gemmell’s
    Drenai universe?*](https://davidgemmell.fandom.com/wiki/Drenai_series)*”*; chosen
    deliberately as it shouldn’t appear in the vocabulary of any trained model.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Using the `[bert-base-uncased](https://huggingface.co/bert-base-uncased)` tokenizer
    from the [transformers library](https://huggingface.co/docs/transformers/index),
    this is converted to the following sequence of tokens:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3f08e4204076bf092f9c2c3042ed9645.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The integers that represent each word will change depending on the specific
    model training and tokenization strategy. Decoding this, we can see the word that
    each token represents:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/572e2fc47ce9691a370aefd1e24dfcf1.png)'
  prefs: []
  type: TYPE_IMG
- en: Interestingly, we can see that this is not the same as our input. Special tokens
    have been added, our abbreviation has been split into multiple tokens, and our
    fictional place name is represented by different ‘chunks’. As we used the ‘uncased’
    model, we have also lost all capitalization context.
  prefs: []
  type: TYPE_NORMAL
- en: However, whilst we used a sentence for our example, transformers are not limited
    to text inputs; this architecture has also [demonstrated good results on vision
    tasks](https://arxiv.org/abs/2010.11929v2). To convert an image into a sequence,
    the authors of ViT sliced the image into non-overlapping 16x16 pixel patches and
    concatenated these into a long vector before passing it into the model. If we
    were using a transformer in a Recommender system, one approach could be to use
    the item ids of the last *n* items browsed by a user as an input to our network.
    If we can create a meaningful representation of input tokens for our domain, we
    can feed this into a transformer network.
  prefs: []
  type: TYPE_NORMAL
- en: Embedding our tokens
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Once we have a sequence of integers which represents our input, we can convert
    them into [embeddings](https://huggingface.co/blog/getting-started-with-embeddings)*.*
    Embeddings are a way of representing information that can be easily processed
    by machine learning algorithms; they aim to capture the meaning of the token being
    encoded in a compressed format, by representing the information as a sequence
    of numbers. Initially, embeddings are initialised as sequences of random numbers,
    and meaningful representations are learned during training. However, these embeddings
    have an inherent limitation: they do not take into account the context in which
    the token appears. There are two aspects to this.'
  prefs: []
  type: TYPE_NORMAL
- en: Depending on the task, when we embed our tokens, we may also wish to preserve
    the ordering of our tokens; this is especially important in domains such as NLP,
    or we essentially end up with a [*bag of words* approach](https://machinelearningmastery.com/gentle-introduction-bag-words-model/).
    To overcome this, we apply *positional encoding* to our embeddings. Whilst there
    are [multiple ways of creating positional embeddings](https://arxiv.org/abs/2104.09864),
    the main idea is that we have *another* set of embeddings which represent the
    position of each token in the input sequence, which are combined with our token
    embeddings.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/dc7ababa63186505d97d97bb6099fbdd.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The other issue is that tokens can have different meanings depending on the
    tokens that surround it. Consider the following sentences:'
  prefs: []
  type: TYPE_NORMAL
- en: '*It’s dark, who turned off the light?*'
  prefs: []
  type: TYPE_NORMAL
- en: '*Wow, this parcel is really light!*'
  prefs: []
  type: TYPE_NORMAL
- en: Here, the word *light* is used in two different contexts, where it has completely
    different meanings! However, it is likely that — depending on the tokenisation
    strategy — the embedding will be the same. In a transformer, this is handled by
    its *attention* mechanism.
  prefs: []
  type: TYPE_NORMAL
- en: Conceptually, what is attention?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Perhaps the most important mechanism used by the transformer architecture is
    known as *attention*, which enables the network to understand which parts of the
    input sequence are the most relevant for the given task. For each token in the
    sequence, the attention mechanism identifies which other tokens are important
    for understanding the current token in the given context. Before we explore how
    this is implemented within a transformer, let’s start simple and try to understand
    what the attention mechanism is trying to achieve conceptually, to build our intuition.
  prefs: []
  type: TYPE_NORMAL
- en: One way to understand attention is to think of it as a method which replaces
    each token embedding with an embedding that includes information about its neighbouring
    tokens; instead of using the same embedding for every token regardless of its
    context. If we knew which tokens were relevant to the current token, one way of
    capturing this context would be to create a weighted average — or, more generally,
    a linear combination — of these embeddings.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/566be9fdc10f7026db59b5958d5d66a1.png)'
  prefs: []
  type: TYPE_IMG
- en: Let’s consider a simple example of how this could look for one of the sentences
    we saw earlier. Before attention is applied, the embeddings in the sequence have
    no context of their neighbours. Therefore, we can visualise the embedding for
    the word *light* as the following linear combination.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/37c2bd186bb6520960ceafb770f8a5fb.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, we can see that our weights are just the identity matrix. After applying
    our attention mechanism, we would like to learn a weight matrix such that we could
    express our *light* embedding in a way similar to the following.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ab0c5b6d7a3fb8fdfcd507c33fbd0fb2.png)'
  prefs: []
  type: TYPE_IMG
- en: This time, larger weights are given to the embeddings that correspond to the
    most relevant parts of the sequence for our chosen token; which should ensure
    that the most important context is captured in the new embedding vector.
  prefs: []
  type: TYPE_NORMAL
- en: Embeddings which contain information about their current context are sometimes
    known as *contextualised embeddings,* and this is ultimately what we are trying
    to create.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have a high level understanding of what attention is trying to achieve,
    let’s explore how this is actually implemented in the following section.
  prefs: []
  type: TYPE_NORMAL
- en: '**How is attention calculated?**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are multiple types of attention, and the main differences lie in the way
    that the weights used to perform the linear combination are calculated. Here,
    we shall consider [scaled dot-product attention](https://paperswithcode.com/method/scaled),
    as introduced in the [original paper](https://arxiv.org/abs/1706.03762), as this
    is the most common approach. In this section, assume that all of our embeddings
    have been positionally encoded.
  prefs: []
  type: TYPE_NORMAL
- en: Recalling that our aim is to create contextualised embeddings using linear combinations
    of our original embeddings, let’s start simple and assume that we can encode all
    of the necessary information needed into our learned embedding vectors, and all
    we need to calculate are the weights.
  prefs: []
  type: TYPE_NORMAL
- en: To calculate the weights, we must first determine which tokens are relevant
    to each other. To achieve this, we need to establish a notion of similarity between
    two embeddings. One way to represent this similarity is by using the dot product,
    where we would like to learn embeddings such that higher scores indicate that
    two words are more similar.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/fc0f44b7a748f5b568ba290ebbd34af0.png)'
  prefs: []
  type: TYPE_IMG
- en: As, for each token, we need to calculate its relevance with every other token
    in the sequence, we can generalise this to a matrix multiplication, which provides
    us with our weight matrix; which are often referred to as *attention scores*.
    To ensure that our weights sum to one, we also apply the [SoftMax function](https://en.wikipedia.org/wiki/Softmax_function).
    However, as matrix multiplications can produce arbitrarily large numbers, this
    could result in the SoftMax function returning very small gradients for large
    attention scores; which may lead to the [vanishing gradient problem](https://en.wikipedia.org/wiki/Vanishing_gradient_problem)
    during training. To counteract this, the attention scores are multiplied by a
    scaling factor, before applying the SoftMax.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f368c6edaece31c4813beea23a5cf557.png)'
  prefs: []
  type: TYPE_IMG
- en: Now, to get our contextualised embedding matrix, we can multiply our attention
    scores with our original embedding matrix; which is the equivalent of taking linear
    combinations of our embeddings.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/df5e39b1e5c61b24645c7de35f229e91.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Simplified attention calculation: assuming embeddings are positionally encoded'
  prefs: []
  type: TYPE_NORMAL
- en: Whilst it may be possible for a model to learn embeddings which are complex
    enough to generate attention scores and subsequent contextualised embeddings;
    we are trying to condense a lot of information into the embedding dimension, which
    is usually quite small.
  prefs: []
  type: TYPE_NORMAL
- en: 'Therefore, to make the task slightly easier for the model to learn, let’s introduce
    some more learnable parameters! Instead of using our embedding matrix directly,
    lets pass this through three, independent linear layers (matrix multiplications);
    this should enable the model to ‘focus’ on different parts of the embeddings.
    This is depicted in the image below:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/691be809db9692bf8adc0deb68ef4c1f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Scaled dot-product Self Attention: assuming embeddings are positionally encoded'
  prefs: []
  type: TYPE_NORMAL
- en: From the image, we can see that the linear projections are labelled Q, K and
    V. In the original paper, these projections were named *Query, Key and Value*,
    supposedly taking inspiration from information retrieval. Personally, I never
    found that this analogy helped my understanding, so I tend not to focus on this;
    I have followed the terminology here for consistency with the literature, and
    to make it explicit that these linear layers are distinct.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we understand how this process works, we can think of the attention
    calculation as a single block with three inputs, which will be passed to Q, K
    and V.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/07ffd6affcf03eca718d7d59446b70e2.png)'
  prefs: []
  type: TYPE_IMG
- en: When we pass the same embedding matrix to Q, K and V, this is known as *self-attention*.
  prefs: []
  type: TYPE_NORMAL
- en: '**What is multi-head attention?**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In practice, we often use multiple self-attention blocks in parallel, to enable
    the transformer to attend to different parts of the input sequence simultaneously–
    this is known as *multi-head attention*.
  prefs: []
  type: TYPE_NORMAL
- en: The idea behind multi-head attention is quite simple, the outputs of multiple
    independent self-attention blocks are concatenated together, and then passed through
    a linear layer. This linear layer enables the model to learn to combine the contextual
    information from each attention head.
  prefs: []
  type: TYPE_NORMAL
- en: In practice, the hidden dimension size used in each self-attention block is
    usually chosen to be the original embedding size divided by the number of attention
    heads; to preserve the shape of the embedding matrix.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/04f88198fb56767adc1f4dd86c41a10b.png)'
  prefs: []
  type: TYPE_IMG
- en: '**What else makes up a Transformer?**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Although the paper that introduced the transformer was (now infamously) named
    [*Attention is all you need*](https://arxiv.org/abs/1706.03762), this is slightly
    confusing, as there are more components to a transformer than just attention!
  prefs: []
  type: TYPE_NORMAL
- en: 'A transformer block also contains the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Feedforward Neural Network** (FFN): A two-layer neural network which is applied
    to each token embedding in the batch and sequence independently. The purpose of
    the FFN block is to introduce additional learnable parameters into the transformer,
    which are [responsible for ensuring that the contextual embeddings are distinct
    and spread out](https://arxiv.org/pdf/2305.13297.pdf). The original paper used
    a [GeLU activation function](https://paperswithcode.com/method/gelu), but the
    components of the FFN can vary depending on the architecture.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[**Layer Normalisation**](https://arxiv.org/abs/1607.06450): helps stabilize
    the training of deep neural networks, including transformers. It normalizes the
    activations for each sequence, preventing them from becoming too large or too
    small during training; which can lead to gradient-related issues like [vanishing
    or exploding gradients](https://en.wikipedia.org/wiki/Vanishing_gradient_problem).
    This stability is crucial for effectively training very deep transformer models.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Skip connections**: Like in [ResNet architectures](https://arxiv.org/abs/1512.03385),
    residual connections are used to mitigate the vanishing gradient problem and improve
    training stability.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Whilst the transformer architecture has remained fairly constant since its
    introduction, the positioning of the layer normalisation blocks can vary depending
    on the transformer architecture. The original architecture , now known as *post-layer
    norm* is presented below:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/00b8b1ab46c026969fcf9a2a9acffa19.png)'
  prefs: []
  type: TYPE_IMG
- en: The most common placement in recent architectures, as seen in the figure below,
    is *pre-layer norm*, which places the normalisation blocks before the self-attention
    and FFN blocks, within the skip connections.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/92c7363ca0353c06a66962158f54fc5f.png)'
  prefs: []
  type: TYPE_IMG
- en: '**What are the different types of Transformer?**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Whilst there are now many different transformer architectures, most can be categorised
    into three main types.
  prefs: []
  type: TYPE_NORMAL
- en: Encoder Architectures
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Encoder models aim to produce contextual embeddings that can be used for downstream
    tasks such as classification or named entity recognition, as the attention mechanism
    is able to attend over the entire input sequence; this is the type of architecture
    that has been explored in this article so far. The most popular family of encoder-only
    transformers are [BERT](https://arxiv.org/abs/1810.04805), and its variants.
  prefs: []
  type: TYPE_NORMAL
- en: After passing our data through one or more transformer blocks, we have a complex
    contextualised embedding matrix, representing an embedding for each token in the
    sequence. However, to use this for a downstream task such as classification, we
    only need to make one prediction. Traditionally, the first token is taken, and
    passed through a classification head; which usually contains Dropout and Linear
    layers. The output of these layers can be passed through a SoftMax function to
    convert these into class probabilities. An example of how this could look is depicted
    below.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/32ef39197e778affc8d0afc72b0ee8ab.png)'
  prefs: []
  type: TYPE_IMG
- en: Decoder Architectures
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Almost identical to Encoder architectures, the key difference is that Decoder
    architectures employ a *masked (or causal)* *self-attention layer*, so that the
    attention mechanism is only able to attend to the current and previous elements
    of the input sequence; this means that the contextual embedding produced only
    considers the previous context. Popular decoder-only models include the [GPT family](https://arxiv.org/abs/2005.14165).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/99336020869964d867175ac62d505d56.png)'
  prefs: []
  type: TYPE_IMG
- en: This is usually achieved by masking the attention scores with a binary lower
    triangular matrix and replacing the non-masked elements with negative infinity;
    when passed through the following SoftMax operation, this will ensure that the
    attention scores for these positions are equal to zero. We can update our previous
    self-attention figure to include this as below.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/03f498f51e541ef9e56d9d13593b1b3a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Masked self-attention calculation: assuming positionally encoded embeddings'
  prefs: []
  type: TYPE_NORMAL
- en: As they can only attend from the current position and backwards, Decoder architectures
    are usually employed for autoregressive tasks, such as sequence generation. However,
    when using contextual embeddings to generate sequences, there are some additional
    considerations when compared to using an Encoder. An example of this is shown
    below.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9a0bda9202f68abe52c409227bb1c833.png)'
  prefs: []
  type: TYPE_IMG
- en: We can notice that, whilst the decoder produces a contextual embedding for each
    token in the input sequence, we usually use the embedding corresponding to the
    final token as the input to subsequent layers when generating sequences.
  prefs: []
  type: TYPE_NORMAL
- en: 'Additionally, after applying the SoftMax function to the logits, if no filtering
    is applied, we would receive a probability distribution over every token in the
    model’s vocabulary; this can be incredibly large! Often, we wish to reduce the
    number of potential options using various filtering strategies, some of the most
    common methods are:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Temperature Adjustment:** temperature is a parameter which is applied inside
    of the SoftMax operation that influences the randomness of the generated text.
    It determines how creative or focused the model’s output is by altering the probability
    distribution over the output words. A higher temperature flattens the distribution,
    making outputs more diverse.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Top-P Sampling:** this approach filters the number of potential candidates
    for the next token based on a given probability threshold and redistributes the
    probability distribution based on candidates above this threshold.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Top-K Sampling:** this approach restricts the number of potential candidates
    to the *K* most likely tokens based on their logit or probability score (depending
    on implementation)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: More details on these methods [can be found here](https://peterchng.com/blog/2023/05/02/token-selection-strategies-top-k-top-p-and-temperature/).
  prefs: []
  type: TYPE_NORMAL
- en: Once we have altered or reduced our probability distribution over the potential
    candidate for the next token, we can sample from this to get our prediction —
    this is just sampling from a [multinomial distribution](https://en.wikipedia.org/wiki/Multinomial_distribution).
    The predicted token is then appended to the input sequence and fed back into the
    model, until the desired number of tokens have been generated, or the model produces
    a *stop* token; a special token to signify the end of a sequence.
  prefs: []
  type: TYPE_NORMAL
- en: Encoder-decoder Architectures
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Originally, the transformer was presented as an architecture for machine translation
    and used both an encoder and decoder to accomplish this goal; using the encoder
    to create an intermediate representation, before using the decoder to translate
    to the desired output format. Whilst encoder-decoder transformers have become
    less common, [architectures such as T5](https://arxiv.org/abs/1910.10683) demonstrate
    how tasks such as question answering, summarisation and classification can be
    framed as sequence-to-sequence problems and solved using this approach.
  prefs: []
  type: TYPE_NORMAL
- en: The key difference with encoder-decoder architectures is that the decoder uses
    *encoder-decoder attention*, which uses both the outputs of the encoder (as K
    and V) and the inputs of the decoder block (as Q) during its attention calculation.
    This contrasts with self-attention, where the same input embedding matrix is used
    for all inputs. Aside from this, the overall generation process is very similar
    to using a decoder only architecture.
  prefs: []
  type: TYPE_NORMAL
- en: We can visualise an encoder-decoder architecture as seen in the figure below.
    Here, to simplify the figure, I chose to depict the *post-layer norm* variant
    of the transformer as seen in the original paper; where the layer norm layers
    are situated after the attention blocks.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ea8723275525bea7281566f1b01e2f0a.png)'
  prefs: []
  type: TYPE_IMG
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Hopefully this has provided an intuition on how transformers work, helped to
    break down some of the details in a somewhat digestible way and has acted as good
    starting point into demystifying modern transformer architectures!
  prefs: []
  type: TYPE_NORMAL
- en: '[*Chris Hughes*](https://www.linkedin.com/in/chris-hughes1/) *is on LinkedIn*'
  prefs: []
  type: TYPE_NORMAL
- en: Unless otherwise stated, all images were created by the author.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[[1706.03762] Attention Is All You Need (arxiv.org)](https://arxiv.org/abs/1706.03762)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Recent Advances in Google Translate — Google Research Blog](https://blog.research.google/2020/06/recent-advances-in-google-translate.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[How GitHub Copilot is getting better at understanding your code — The GitHub
    Blog](https://github.blog/2023-05-17-how-github-copilot-is-getting-better-at-understanding-your-code/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Introducing ChatGPT (openai.com)](https://openai.com/blog/chatgpt)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[gpt-4.pdf (openai.com)](https://cdn.openai.com/papers/gpt-4.pdf#:~:text=This%20technical%20report%20presents%20GPT-4%2C%20a%20large%20multimodal,as%20dialogue%20systems%2C%20text%20summarization%2C%20and%20machine%20translation.)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Introducing LLaMA: A foundational, 65-billion-parameter language model (meta.com)](https://ai.meta.com/blog/large-language-model-llama-meta-ai/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[The Illustrated Transformer — Jay Alammar — Visualizing machine learning one
    concept at a time. (jalammar.github.io)](http://jalammar.github.io/illustrated-transformer/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Byte-Pair Encoding tokenization — Hugging Face NLP Course](https://huggingface.co/learn/nlp-course/chapter6/5?fw=pt)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Drenai series | David Gemmell Wiki | Fandom](https://davidgemmell.fandom.com/wiki/Drenai_series)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[bert-base-uncased · Hugging Face](https://huggingface.co/bert-base-uncased)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[🤗 Transformers (huggingface.co)](https://huggingface.co/docs/transformers/index)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[[2010.11929v2] An Image is Worth 16x16 Words: Transformers for Image Recognition
    at Scale (arxiv.org)](https://arxiv.org/abs/2010.11929v2)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Getting Started With Embeddings (huggingface.co)](https://huggingface.co/blog/getting-started-with-embeddings)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[A Gentle Introduction to the Bag-of-Words Model — MachineLearningMastery.com](https://machinelearningmastery.com/gentle-introduction-bag-words-model/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[[2104.09864] RoFormer: Enhanced Transformer with Rotary Position Embedding
    (arxiv.org)](https://arxiv.org/abs/2104.09864)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Scaled Dot-Product Attention Explained | Papers With Code](https://paperswithcode.com/method/scaled)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Softmax function — Wikipedia](https://en.wikipedia.org/wiki/Softmax_function)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[[1607.06450] Layer Normalization (arxiv.org)](https://arxiv.org/abs/1607.06450)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Vanishing gradient problem — Wikipedia](https://en.wikipedia.org/wiki/Vanishing_gradient_problem)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[[1512.03385] Deep Residual Learning for Image Recognition (arxiv.org)](https://arxiv.org/abs/1512.03385)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[[1810.04805] BERT: Pre-training of Deep Bidirectional Transformers for Language
    Understanding (arxiv.org)](https://arxiv.org/abs/1810.04805)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[[2005.14165] Language Models are Few-Shot Learners (arxiv.org)](https://arxiv.org/abs/2005.14165)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Token selection strategies: Top-K, Top-P, and Temperature (peterchng.com)](https://peterchng.com/blog/2023/05/02/token-selection-strategies-top-k-top-p-and-temperature/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Multinomial distribution — Wikipedia](https://en.wikipedia.org/wiki/Multinomial_distribution)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[[1910.10683] Exploring the Limits of Transfer Learning with a Unified Text-to-Text
    Transformer (arxiv.org)](https://arxiv.org/abs/1910.10683)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
