- en: 'De-coded: Transformers explained in plain English'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 解码：用简单英语解释 Transformers
- en: 原文：[https://towardsdatascience.com/de-coded-transformers-explained-in-plain-english-877814ba6429?source=collection_archive---------0-----------------------#2023-10-09](https://towardsdatascience.com/de-coded-transformers-explained-in-plain-english-877814ba6429?source=collection_archive---------0-----------------------#2023-10-09)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/de-coded-transformers-explained-in-plain-english-877814ba6429?source=collection_archive---------0-----------------------#2023-10-09](https://towardsdatascience.com/de-coded-transformers-explained-in-plain-english-877814ba6429?source=collection_archive---------0-----------------------#2023-10-09)
- en: No code, maths, or mention of Keys, Queries and Values
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 无需代码、数学或提及 Keys、Queries 和 Values
- en: '[](https://medium.com/@chris.p.hughes10?source=post_page-----877814ba6429--------------------------------)[![Chris
    Hughes](../Images/87b16cd8677739b12294380fb00fde85.png)](https://medium.com/@chris.p.hughes10?source=post_page-----877814ba6429--------------------------------)[](https://towardsdatascience.com/?source=post_page-----877814ba6429--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----877814ba6429--------------------------------)
    [Chris Hughes](https://medium.com/@chris.p.hughes10?source=post_page-----877814ba6429--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@chris.p.hughes10?source=post_page-----877814ba6429--------------------------------)[![Chris
    Hughes](../Images/87b16cd8677739b12294380fb00fde85.png)](https://medium.com/@chris.p.hughes10?source=post_page-----877814ba6429--------------------------------)[](https://towardsdatascience.com/?source=post_page-----877814ba6429--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----877814ba6429--------------------------------)
    [Chris Hughes](https://medium.com/@chris.p.hughes10?source=post_page-----877814ba6429--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ff13df9df155e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fde-coded-transformers-explained-in-plain-english-877814ba6429&user=Chris+Hughes&userId=f13df9df155e&source=post_page-f13df9df155e----877814ba6429---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----877814ba6429--------------------------------)
    ·15 min read·Oct 9, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F877814ba6429&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fde-coded-transformers-explained-in-plain-english-877814ba6429&user=Chris+Hughes&userId=f13df9df155e&source=-----877814ba6429---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ff13df9df155e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fde-coded-transformers-explained-in-plain-english-877814ba6429&user=Chris+Hughes&userId=f13df9df155e&source=post_page-f13df9df155e----877814ba6429---------------------post_header-----------)
    发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----877814ba6429--------------------------------)
    ·15分钟阅读·2023年10月9日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F877814ba6429&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fde-coded-transformers-explained-in-plain-english-877814ba6429&user=Chris+Hughes&userId=f13df9df155e&source=-----877814ba6429---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F877814ba6429&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fde-coded-transformers-explained-in-plain-english-877814ba6429&source=-----877814ba6429---------------------bookmark_footer-----------)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F877814ba6429&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fde-coded-transformers-explained-in-plain-english-877814ba6429&source=-----877814ba6429---------------------bookmark_footer-----------)'
- en: Since their [introduction in 2017](https://arxiv.org/abs/1706.03762), transformers
    have emerged as a prominent force in the field of Machine Learning, revolutionizing
    the capabilities of [major translation](https://blog.research.google/2020/06/recent-advances-in-google-translate.html)
    and [autocomplete](https://github.blog/2023-05-17-how-github-copilot-is-getting-better-at-understanding-your-code/)
    services.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 自从[2017年引入](https://arxiv.org/abs/1706.03762)以来，transformers 已成为机器学习领域的显著力量，彻底改变了[主要翻译](https://blog.research.google/2020/06/recent-advances-in-google-translate.html)和[自动补全](https://github.blog/2023-05-17-how-github-copilot-is-getting-better-at-understanding-your-code/)服务的能力。
- en: Recently, the popularity of transformers has soared even higher with the advent
    of large language models like OpenAI’s [ChatGPT](https://openai.com/blog/chatgpt),
    [GPT-4](https://cdn.openai.com/papers/gpt-4.pdf#:~:text=This%20technical%20report%20presents%20GPT-4%2C%20a%20large%20multimodal,as%20dialogue%20systems%2C%20text%20summarization%2C%20and%20machine%20translation.),
    and Meta’s [LLama](https://ai.meta.com/blog/large-language-model-llama-meta-ai/).
    These models, which have garnered immense attention and excitement, are all built
    on the foundation of the transformer architecture. By leveraging the power of
    transformers, these models have achieved remarkable breakthroughs in natural language
    understanding and generation; exposing these to the general public.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，随着大型语言模型如OpenAI的[ChatGPT](https://openai.com/blog/chatgpt)、[GPT-4](https://cdn.openai.com/papers/gpt-4.pdf#:~:text=This%20technical%20report%20presents%20GPT-4%2C%20a%20large%20multimodal,as%20dialogue%20systems%2C%20text%20summarization%2C%20and%20machine%20translation.)和Meta的[LLama](https://ai.meta.com/blog/large-language-model-llama-meta-ai/)的出现，变换器的受欢迎程度更高了。这些模型获得了极大的关注和兴奋，都建立在变换器架构的基础上。通过利用变换器的力量，这些模型在自然语言理解和生成方面取得了显著突破，向公众展示了这些成就。
- en: Despite a lot of [good resources](http://jalammar.github.io/illustrated-transformer/)
    which break down how transformers work, I found myself in a position where I understood
    the how the mechanics worked mathematically but found it difficult to explain
    how a transformer works intuitively. After conducting many interviews, speaking
    to my colleagues, and giving a lightning talk on the subject, it seems that many
    people share this problem!
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管有很多[优质资源](http://jalammar.github.io/illustrated-transformer/)详细讲解了变换器的工作原理，但我发现自己在理解其数学机制的同时，很难直观地解释变换器是如何工作的。在进行许多访谈、与同事讨论以及在相关主题上做简短讲座之后，似乎很多人都面临这个问题！
- en: In this blog post, I shall aim to provide a high-level explanation of how transformers
    work without relying on code or mathematics. My goal is to avoid confusing technical
    jargon and comparisons with previous architectures. Whilst I’ll try to keep things
    as simple as possible, this won’t be easy as transformers are quite complex, but
    I hope it will provide a better intuition of what they do and how they do it.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇博客文章中，我将旨在提供一个高级解释，讲解变换器是如何工作的，而不依赖于代码或数学。我的目标是避免令人困惑的技术术语和与以前架构的比较。虽然我会尽量保持简单，但由于变换器相当复杂，这不会很容易，但我希望它能提供更好的直观理解，了解它们的作用及其工作方式。
- en: What is a Transformer?
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是变换器？
- en: A transformer is a type of neural network architecture which is well suited
    for tasks that involve processing sequences as inputs. Perhaps the most common
    example of a sequence in this context is a sentence, which we can think of as
    an ordered set of words.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 变换器是一种神经网络架构，特别适用于处理序列输入的任务。在这个上下文中，最常见的序列例子可能是句子，我们可以把它看作是一个有序的单词集合。
- en: The aim of these models is to create a numerical representation for each element
    within a sequence; encapsulating essential information about the element and its
    neighbouring context. The resulting numerical representations can then be passed
    on to downstream networks, which can leverage this information to perform various
    tasks, including generation and classification.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 这些模型的目标是为序列中的每个元素创建一个数值表示；封装关于元素及其邻近上下文的关键信息。生成的数值表示可以传递给下游网络，这些网络可以利用这些信息来执行各种任务，包括生成和分类。
- en: By creating such rich representations, these models enable downstream networks
    to better understand the underlying patterns and relationships within the input
    sequence, which enhances their ability to generate coherent and contextually relevant
    outputs.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 通过创建如此丰富的表示，这些模型使下游网络能够更好地理解输入序列中的潜在模式和关系，从而增强它们生成连贯且具有上下文相关性的输出的能力。
- en: The key advantage of transformers lies in their ability to handle long-range
    dependencies within sequences, as well as being highly efficient; capable of processing
    sequences in parallel. This is particularly useful for tasks such as machine translation,
    sentiment analysis, and text generation.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 变换器的主要优势在于其处理序列中的长距离依赖的能力，以及高度的效率；能够并行处理序列。这对于机器翻译、情感分析和文本生成等任务尤其有用。
- en: '![](../Images/4d9874a6d0be28e2def6d15c3e1efe41.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4d9874a6d0be28e2def6d15c3e1efe41.png)'
- en: 'Image generated by the [Azure OpenAI Service DALL-E model](https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/models#dall-e-preview)
    with the following prompt: “*The green and black Matrix code in the shape of Optimus
    Prime”*'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: '**What goes into the Transformer?**'
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To feed an input into a transformer, we must first convert it into a sequence
    of tokens; a set of integers that represent our input.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: As transformers were first applied in the NLP domain, let’s consider this scenario
    first. The simplest way to convert a sentence into a series of tokens is to define
    a *vocabulary* which acts as a lookup table, mapping words to integers; we can
    reserve a specific number to represent any word which is not contained in this
    vocabulary, so that we can always assign an integer value.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/271353078c1b211d9723ec89e696c761.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
- en: In practice, this is a naïve way of encoding text, as words such as *cat* and
    *cats* are treated as completely different tokens, despite them being singular
    and plural descriptions of the same animal! To overcome this, different tokenisation
    strategies — such as [byte-pair encoding](https://huggingface.co/learn/nlp-course/chapter6/5?fw=pt)
    — have been devised which break words up into smaller chunks before indexing them.
    Additionally, it is often useful to add special tokens to represent characteristics
    such as the start and end of a sentence, to provide additional context to the
    model.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: Let’s consider the following example, to better understand the tokenization
    process.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: '*“Hello there, isn’t the weather nice today in Drosval?”*'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: 'Drosval is a name generated by GPT-4 using the following prompt: *“Can you
    create a fictional place name that sounds like it could belong to* [*David Gemmell’s
    Drenai universe?*](https://davidgemmell.fandom.com/wiki/Drenai_series)*”*; chosen
    deliberately as it shouldn’t appear in the vocabulary of any trained model.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: 'Using the `[bert-base-uncased](https://huggingface.co/bert-base-uncased)` tokenizer
    from the [transformers library](https://huggingface.co/docs/transformers/index),
    this is converted to the following sequence of tokens:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3f08e4204076bf092f9c2c3042ed9645.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
- en: 'The integers that represent each word will change depending on the specific
    model training and tokenization strategy. Decoding this, we can see the word that
    each token represents:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/572e2fc47ce9691a370aefd1e24dfcf1.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
- en: Interestingly, we can see that this is not the same as our input. Special tokens
    have been added, our abbreviation has been split into multiple tokens, and our
    fictional place name is represented by different ‘chunks’. As we used the ‘uncased’
    model, we have also lost all capitalization context.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: However, whilst we used a sentence for our example, transformers are not limited
    to text inputs; this architecture has also [demonstrated good results on vision
    tasks](https://arxiv.org/abs/2010.11929v2). To convert an image into a sequence,
    the authors of ViT sliced the image into non-overlapping 16x16 pixel patches and
    concatenated these into a long vector before passing it into the model. If we
    were using a transformer in a Recommender system, one approach could be to use
    the item ids of the last *n* items browsed by a user as an input to our network.
    If we can create a meaningful representation of input tokens for our domain, we
    can feed this into a transformer network.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，尽管我们用句子作为例子，变压器模型并不限于文本输入；这种架构在[视觉任务上也展示了良好的结果](https://arxiv.org/abs/2010.11929v2)。为了将图像转换为序列，ViT的作者将图像切割成不重叠的16x16像素块，并将这些块串联成一个长向量，然后输入到模型中。如果我们在推荐系统中使用变压器，一种方法可能是将用户浏览过的最后*n*个项目的ID作为输入传递到我们的网络中。如果我们能够为我们的领域创建有意义的输入标记表示，我们可以将其输入到变压器网络中。
- en: Embedding our tokens
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 嵌入我们的标记
- en: 'Once we have a sequence of integers which represents our input, we can convert
    them into [embeddings](https://huggingface.co/blog/getting-started-with-embeddings)*.*
    Embeddings are a way of representing information that can be easily processed
    by machine learning algorithms; they aim to capture the meaning of the token being
    encoded in a compressed format, by representing the information as a sequence
    of numbers. Initially, embeddings are initialised as sequences of random numbers,
    and meaningful representations are learned during training. However, these embeddings
    have an inherent limitation: they do not take into account the context in which
    the token appears. There are two aspects to this.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们得到一个表示输入的整数序列，我们可以将它们转换为[词嵌入](https://huggingface.co/blog/getting-started-with-embeddings)。词嵌入是一种表示信息的方式，能够被机器学习算法轻松处理；其目的是通过将信息表示为数字序列，以压缩的格式捕捉编码的标记的意义。最初，词嵌入被初始化为随机数字序列，有意义的表示在训练过程中被学习。然而，这些词嵌入有一个固有的限制：它们不考虑标记出现的上下文。这有两个方面。
- en: Depending on the task, when we embed our tokens, we may also wish to preserve
    the ordering of our tokens; this is especially important in domains such as NLP,
    or we essentially end up with a [*bag of words* approach](https://machinelearningmastery.com/gentle-introduction-bag-words-model/).
    To overcome this, we apply *positional encoding* to our embeddings. Whilst there
    are [multiple ways of creating positional embeddings](https://arxiv.org/abs/2104.09864),
    the main idea is that we have *another* set of embeddings which represent the
    position of each token in the input sequence, which are combined with our token
    embeddings.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 根据任务的不同，当我们嵌入标记时，我们可能还希望保留标记的顺序；这在NLP等领域尤为重要，否则我们基本上会得到一个[*词袋模型*方法](https://machinelearningmastery.com/gentle-introduction-bag-words-model/)。为了解决这个问题，我们对词嵌入应用*位置编码*。虽然有[多种创建位置嵌入的方法](https://arxiv.org/abs/2104.09864)，但主要思想是我们有*另一*组嵌入，表示输入序列中每个标记的位置，这些位置嵌入与我们的标记嵌入结合。
- en: '![](../Images/dc7ababa63186505d97d97bb6099fbdd.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/dc7ababa63186505d97d97bb6099fbdd.png)'
- en: 'The other issue is that tokens can have different meanings depending on the
    tokens that surround it. Consider the following sentences:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个问题是标记的含义可以根据周围的标记而变化。考虑以下句子：
- en: '*It’s dark, who turned off the light?*'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '*天黑了，谁关掉了灯？*'
- en: '*Wow, this parcel is really light!*'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '*哇，这个包裹真的很轻！*'
- en: Here, the word *light* is used in two different contexts, where it has completely
    different meanings! However, it is likely that — depending on the tokenisation
    strategy — the embedding will be the same. In a transformer, this is handled by
    its *attention* mechanism.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*light* 这个词在两个不同的上下文中使用，其含义完全不同！然而，根据分词策略的不同，词嵌入可能是相同的。在变压器模型中，这由其*注意力*机制处理。
- en: Conceptually, what is attention?
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从概念上讲，什么是注意力机制？
- en: Perhaps the most important mechanism used by the transformer architecture is
    known as *attention*, which enables the network to understand which parts of the
    input sequence are the most relevant for the given task. For each token in the
    sequence, the attention mechanism identifies which other tokens are important
    for understanding the current token in the given context. Before we explore how
    this is implemented within a transformer, let’s start simple and try to understand
    what the attention mechanism is trying to achieve conceptually, to build our intuition.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 也许变换器架构中最重要的机制被称为*注意力*，它使网络能够理解输入序列中哪些部分对于给定任务最为相关。对于序列中的每个标记，注意力机制确定哪些其他标记在给定上下文中对理解当前标记是重要的。在我们探讨变换器中如何实现这一机制之前，让我们从简单开始，试图从概念上理解注意力机制的目标，以建立我们的直觉。
- en: One way to understand attention is to think of it as a method which replaces
    each token embedding with an embedding that includes information about its neighbouring
    tokens; instead of using the same embedding for every token regardless of its
    context. If we knew which tokens were relevant to the current token, one way of
    capturing this context would be to create a weighted average — or, more generally,
    a linear combination — of these embeddings.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 理解注意力的一种方式是将其视为一种方法，用来将每个标记嵌入替换为包含其邻近标记信息的嵌入；而不是在不考虑上下文的情况下对每个标记使用相同的嵌入。如果我们知道哪些标记与当前标记相关，那么捕捉这种上下文的一种方式是创建一个加权平均——或者更一般地说，是一个线性组合——这些嵌入。
- en: '![](../Images/566be9fdc10f7026db59b5958d5d66a1.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/566be9fdc10f7026db59b5958d5d66a1.png)'
- en: Let’s consider a simple example of how this could look for one of the sentences
    we saw earlier. Before attention is applied, the embeddings in the sequence have
    no context of their neighbours. Therefore, we can visualise the embedding for
    the word *light* as the following linear combination.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑一个简单的例子，看看这对于我们之前看到的一个句子会是什么样子。在应用注意力之前，序列中的嵌入没有邻居的上下文。因此，我们可以将*light*一词的嵌入可视化为以下线性组合。
- en: '![](../Images/37c2bd186bb6520960ceafb770f8a5fb.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/37c2bd186bb6520960ceafb770f8a5fb.png)'
- en: Here, we can see that our weights are just the identity matrix. After applying
    our attention mechanism, we would like to learn a weight matrix such that we could
    express our *light* embedding in a way similar to the following.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们可以看到我们的权重只是单位矩阵。应用我们的注意力机制后，我们希望学习一个权重矩阵，以便我们能够以类似于以下的方式表达我们的*light*嵌入。
- en: '![](../Images/ab0c5b6d7a3fb8fdfcd507c33fbd0fb2.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ab0c5b6d7a3fb8fdfcd507c33fbd0fb2.png)'
- en: This time, larger weights are given to the embeddings that correspond to the
    most relevant parts of the sequence for our chosen token; which should ensure
    that the most important context is captured in the new embedding vector.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 这次，较大的权重被赋予对应于我们选择的标记最相关部分的嵌入；这应该确保最重要的上下文被捕捉到新的嵌入向量中。
- en: Embeddings which contain information about their current context are sometimes
    known as *contextualised embeddings,* and this is ultimately what we are trying
    to create.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 含有当前上下文信息的嵌入有时被称为*上下文化嵌入*，这就是我们最终试图创建的东西。
- en: Now that we have a high level understanding of what attention is trying to achieve,
    let’s explore how this is actually implemented in the following section.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们对注意力机制的目标有了一个高层次的理解，让我们探讨一下它在下一部分是如何实际实现的。
- en: '**How is attention calculated?**'
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**注意力是如何计算的？**'
- en: There are multiple types of attention, and the main differences lie in the way
    that the weights used to perform the linear combination are calculated. Here,
    we shall consider [scaled dot-product attention](https://paperswithcode.com/method/scaled),
    as introduced in the [original paper](https://arxiv.org/abs/1706.03762), as this
    is the most common approach. In this section, assume that all of our embeddings
    have been positionally encoded.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力有多种类型，主要区别在于用于执行线性组合的权重计算方式。这里，我们将考虑[缩放点积注意力](https://paperswithcode.com/method/scaled)，如在[原始论文](https://arxiv.org/abs/1706.03762)中介绍的，这是最常见的方法。在这一部分中，假设我们所有的嵌入都已经进行位置编码。
- en: Recalling that our aim is to create contextualised embeddings using linear combinations
    of our original embeddings, let’s start simple and assume that we can encode all
    of the necessary information needed into our learned embedding vectors, and all
    we need to calculate are the weights.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，我们的目标是通过对原始嵌入的线性组合来创建上下文化嵌入，让我们从简单开始，假设我们可以将所有必要的信息编码到学习到的嵌入向量中，我们只需要计算权重。
- en: To calculate the weights, we must first determine which tokens are relevant
    to each other. To achieve this, we need to establish a notion of similarity between
    two embeddings. One way to represent this similarity is by using the dot product,
    where we would like to learn embeddings such that higher scores indicate that
    two words are more similar.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 为了计算权重，我们必须首先确定哪些 tokens 彼此相关。为此，我们需要建立两个嵌入之间相似性的概念。表示这种相似性的一种方法是使用点积，我们希望学习嵌入，使得较高的分数表示两个词更相似。
- en: '![](../Images/fc0f44b7a748f5b568ba290ebbd34af0.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fc0f44b7a748f5b568ba290ebbd34af0.png)'
- en: As, for each token, we need to calculate its relevance with every other token
    in the sequence, we can generalise this to a matrix multiplication, which provides
    us with our weight matrix; which are often referred to as *attention scores*.
    To ensure that our weights sum to one, we also apply the [SoftMax function](https://en.wikipedia.org/wiki/Softmax_function).
    However, as matrix multiplications can produce arbitrarily large numbers, this
    could result in the SoftMax function returning very small gradients for large
    attention scores; which may lead to the [vanishing gradient problem](https://en.wikipedia.org/wiki/Vanishing_gradient_problem)
    during training. To counteract this, the attention scores are multiplied by a
    scaling factor, before applying the SoftMax.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们需要计算每个 token 与序列中其他每个 token 的相关性，我们可以将其概括为矩阵乘法，这为我们提供了权重矩阵，这些矩阵通常被称为*注意力分数*。为了确保我们的权重总和为
    1，我们还应用了 [SoftMax 函数](https://en.wikipedia.org/wiki/Softmax_function)。然而，由于矩阵乘法可能产生任意大的数字，这可能导致
    SoftMax 函数对较大的注意力分数返回非常小的梯度，这可能在训练过程中导致 [梯度消失问题](https://en.wikipedia.org/wiki/Vanishing_gradient_problem)。为了应对这一问题，在应用
    SoftMax 之前，注意力分数会乘以一个缩放因子。
- en: '![](../Images/f368c6edaece31c4813beea23a5cf557.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f368c6edaece31c4813beea23a5cf557.png)'
- en: Now, to get our contextualised embedding matrix, we can multiply our attention
    scores with our original embedding matrix; which is the equivalent of taking linear
    combinations of our embeddings.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，为了得到我们的上下文化嵌入矩阵，我们可以将注意力分数与原始嵌入矩阵相乘；这相当于对我们的嵌入进行线性组合。
- en: '![](../Images/df5e39b1e5c61b24645c7de35f229e91.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/df5e39b1e5c61b24645c7de35f229e91.png)'
- en: 'Simplified attention calculation: assuming embeddings are positionally encoded'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 简化的注意力计算：假设嵌入是位置编码的
- en: Whilst it may be possible for a model to learn embeddings which are complex
    enough to generate attention scores and subsequent contextualised embeddings;
    we are trying to condense a lot of information into the embedding dimension, which
    is usually quite small.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然模型可能能够学习到足够复杂的嵌入来生成注意力分数和后续的上下文化嵌入，但我们试图将大量信息压缩到通常非常小的嵌入维度中。
- en: 'Therefore, to make the task slightly easier for the model to learn, let’s introduce
    some more learnable parameters! Instead of using our embedding matrix directly,
    lets pass this through three, independent linear layers (matrix multiplications);
    this should enable the model to ‘focus’ on different parts of the embeddings.
    This is depicted in the image below:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，为了让模型学习任务稍微简单一些，我们引入一些可学习的参数！我们不直接使用嵌入矩阵，而是通过三个独立的线性层（矩阵乘法）来处理它；这应该使模型能够“关注”嵌入的不同部分。如下图所示：
- en: '![](../Images/691be809db9692bf8adc0deb68ef4c1f.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/691be809db9692bf8adc0deb68ef4c1f.png)'
- en: 'Scaled dot-product Self Attention: assuming embeddings are positionally encoded'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 缩放的点积自注意力：假设嵌入是位置编码的
- en: From the image, we can see that the linear projections are labelled Q, K and
    V. In the original paper, these projections were named *Query, Key and Value*,
    supposedly taking inspiration from information retrieval. Personally, I never
    found that this analogy helped my understanding, so I tend not to focus on this;
    I have followed the terminology here for consistency with the literature, and
    to make it explicit that these linear layers are distinct.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 从图像中，我们可以看到线性投影被标记为Q、K和V。在原始论文中，这些投影被称为 *查询、键和值*，显然受到信息检索的启发。就个人而言，我发现这个类比并没有帮助我理解，因此我通常不会关注这一点；我在这里遵循文献中的术语，以保持一致，并明确这些线性层是不同的。
- en: Now that we understand how this process works, we can think of the attention
    calculation as a single block with three inputs, which will be passed to Q, K
    and V.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们了解了这个过程如何运作，我们可以把注意力计算视作一个有三个输入的单一块，这些输入将被传递到Q、K和V。
- en: '![](../Images/07ffd6affcf03eca718d7d59446b70e2.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/07ffd6affcf03eca718d7d59446b70e2.png)'
- en: When we pass the same embedding matrix to Q, K and V, this is known as *self-attention*.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们将相同的嵌入矩阵传递给Q、K和V时，这被称为 *自注意力*。
- en: '**What is multi-head attention?**'
  id: totrans-70
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**什么是多头注意力？**'
- en: In practice, we often use multiple self-attention blocks in parallel, to enable
    the transformer to attend to different parts of the input sequence simultaneously–
    this is known as *multi-head attention*.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，我们通常并行使用多个自注意力块，以使变换器能够同时关注输入序列的不同部分——这被称为 *多头注意力*。
- en: The idea behind multi-head attention is quite simple, the outputs of multiple
    independent self-attention blocks are concatenated together, and then passed through
    a linear layer. This linear layer enables the model to learn to combine the contextual
    information from each attention head.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 多头注意力的想法相当简单，多个独立的自注意力块的输出被连接在一起，然后通过一个线性层。这个线性层使模型能够学习如何结合来自每个注意力头的上下文信息。
- en: In practice, the hidden dimension size used in each self-attention block is
    usually chosen to be the original embedding size divided by the number of attention
    heads; to preserve the shape of the embedding matrix.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，每个自注意力块中使用的隐藏维度大小通常选择为原始嵌入大小除以注意力头的数量；以保持嵌入矩阵的形状。
- en: '![](../Images/04f88198fb56767adc1f4dd86c41a10b.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/04f88198fb56767adc1f4dd86c41a10b.png)'
- en: '**What else makes up a Transformer?**'
  id: totrans-75
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**变换器还包含什么？**'
- en: Although the paper that introduced the transformer was (now infamously) named
    [*Attention is all you need*](https://arxiv.org/abs/1706.03762), this is slightly
    confusing, as there are more components to a transformer than just attention!
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管引入变换器的论文（现在臭名昭著）被命名为[*Attention is all you need*](https://arxiv.org/abs/1706.03762)，但这有些令人困惑，因为变换器的组件不仅仅是注意力！
- en: 'A transformer block also contains the following:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 一个变换器块还包含以下内容：
- en: '**Feedforward Neural Network** (FFN): A two-layer neural network which is applied
    to each token embedding in the batch and sequence independently. The purpose of
    the FFN block is to introduce additional learnable parameters into the transformer,
    which are [responsible for ensuring that the contextual embeddings are distinct
    and spread out](https://arxiv.org/pdf/2305.13297.pdf). The original paper used
    a [GeLU activation function](https://paperswithcode.com/method/gelu), but the
    components of the FFN can vary depending on the architecture.'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**前馈神经网络**（FFN）：一个两层神经网络，独立应用于批次和序列中的每个令牌嵌入。FFN块的目的是将额外的可学习参数引入变换器，这些参数是[确保上下文嵌入是独特且分散的](https://arxiv.org/pdf/2305.13297.pdf)。原始论文使用了一个[GeLU激活函数](https://paperswithcode.com/method/gelu)，但FFN的组件可以根据架构的不同而有所变化。'
- en: '[**Layer Normalisation**](https://arxiv.org/abs/1607.06450): helps stabilize
    the training of deep neural networks, including transformers. It normalizes the
    activations for each sequence, preventing them from becoming too large or too
    small during training; which can lead to gradient-related issues like [vanishing
    or exploding gradients](https://en.wikipedia.org/wiki/Vanishing_gradient_problem).
    This stability is crucial for effectively training very deep transformer models.'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**层归一化**](https://arxiv.org/abs/1607.06450)：有助于稳定深度神经网络的训练，包括变换器。它对每个序列的激活进行归一化，防止它们在训练过程中变得过大或过小，这可能导致梯度相关问题，如[梯度消失或梯度爆炸](https://en.wikipedia.org/wiki/Vanishing_gradient_problem)。这种稳定性对于有效训练非常深的变换器模型至关重要。'
- en: '**Skip connections**: Like in [ResNet architectures](https://arxiv.org/abs/1512.03385),
    residual connections are used to mitigate the vanishing gradient problem and improve
    training stability.'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**跳跃连接**：如在[ResNet架构](https://arxiv.org/abs/1512.03385)中，残差连接用于缓解梯度消失问题并提高训练稳定性。'
- en: 'Whilst the transformer architecture has remained fairly constant since its
    introduction, the positioning of the layer normalisation blocks can vary depending
    on the transformer architecture. The original architecture , now known as *post-layer
    norm* is presented below:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管自引入以来，转换器架构保持了相当稳定，但层归一化块的位置可能会根据转换器架构而有所不同。原始架构，现在称为*后层归一化*，如下所示：
- en: '![](../Images/00b8b1ab46c026969fcf9a2a9acffa19.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/00b8b1ab46c026969fcf9a2a9acffa19.png)'
- en: The most common placement in recent architectures, as seen in the figure below,
    is *pre-layer norm*, which places the normalisation blocks before the self-attention
    and FFN blocks, within the skip connections.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在最近的架构中，如下图所示，最常见的放置位置是*预层归一化*，它将归一化块放在自注意力和FFN块之前，位于跳跃连接中。
- en: '![](../Images/92c7363ca0353c06a66962158f54fc5f.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/92c7363ca0353c06a66962158f54fc5f.png)'
- en: '**What are the different types of Transformer?**'
  id: totrans-85
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**不同类型的Transformer有哪些？**'
- en: Whilst there are now many different transformer architectures, most can be categorised
    into three main types.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然现在有许多不同的转换器架构，但大多数可以归纳为三种主要类型。
- en: Encoder Architectures
  id: totrans-87
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 编码器架构
- en: Encoder models aim to produce contextual embeddings that can be used for downstream
    tasks such as classification or named entity recognition, as the attention mechanism
    is able to attend over the entire input sequence; this is the type of architecture
    that has been explored in this article so far. The most popular family of encoder-only
    transformers are [BERT](https://arxiv.org/abs/1810.04805), and its variants.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 编码器模型旨在生成可以用于下游任务（如分类或命名实体识别）的上下文嵌入，因为注意力机制能够覆盖整个输入序列；这就是本文迄今为止探索的架构类型。最受欢迎的编码器-only转换器家族是[BERT](https://arxiv.org/abs/1810.04805)及其变体。
- en: After passing our data through one or more transformer blocks, we have a complex
    contextualised embedding matrix, representing an embedding for each token in the
    sequence. However, to use this for a downstream task such as classification, we
    only need to make one prediction. Traditionally, the first token is taken, and
    passed through a classification head; which usually contains Dropout and Linear
    layers. The output of these layers can be passed through a SoftMax function to
    convert these into class probabilities. An example of how this could look is depicted
    below.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在通过一个或多个转换器块处理数据后，我们得到了一个复杂的上下文化嵌入矩阵，表示序列中每个标记的嵌入。然而，为了用于下游任务，如分类，我们只需要做一个预测。传统上，取第一个标记，并通过分类头；分类头通常包含Dropout和线性层。这些层的输出可以通过SoftMax函数转换为类别概率。下面展示了这可能的样子。
- en: '![](../Images/32ef39197e778affc8d0afc72b0ee8ab.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/32ef39197e778affc8d0afc72b0ee8ab.png)'
- en: Decoder Architectures
  id: totrans-91
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解码器架构
- en: Almost identical to Encoder architectures, the key difference is that Decoder
    architectures employ a *masked (or causal)* *self-attention layer*, so that the
    attention mechanism is only able to attend to the current and previous elements
    of the input sequence; this means that the contextual embedding produced only
    considers the previous context. Popular decoder-only models include the [GPT family](https://arxiv.org/abs/2005.14165).
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 几乎与编码器架构相同，关键区别在于解码器架构使用了*掩蔽（或因果）* *自注意力层*，使得注意力机制只能关注输入序列的当前和先前元素；这意味着生成的上下文嵌入仅考虑先前的上下文。流行的解码器-only模型包括[GPT家族](https://arxiv.org/abs/2005.14165)。
- en: '![](../Images/99336020869964d867175ac62d505d56.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/99336020869964d867175ac62d505d56.png)'
- en: This is usually achieved by masking the attention scores with a binary lower
    triangular matrix and replacing the non-masked elements with negative infinity;
    when passed through the following SoftMax operation, this will ensure that the
    attention scores for these positions are equal to zero. We can update our previous
    self-attention figure to include this as below.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 这通常是通过用二进制下三角矩阵掩蔽注意力分数，并用负无穷替换未掩蔽的元素来实现的；当通过以下SoftMax操作时，这将确保这些位置的注意力分数为零。我们可以更新之前的自注意力图以包括如下内容。
- en: '![](../Images/03f498f51e541ef9e56d9d13593b1b3a.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/03f498f51e541ef9e56d9d13593b1b3a.png)'
- en: 'Masked self-attention calculation: assuming positionally encoded embeddings'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 掩码自注意力计算：假设位置编码嵌入
- en: As they can only attend from the current position and backwards, Decoder architectures
    are usually employed for autoregressive tasks, such as sequence generation. However,
    when using contextual embeddings to generate sequences, there are some additional
    considerations when compared to using an Encoder. An example of this is shown
    below.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 由于它们只能从当前位置及之前的位置进行注意力计算，解码器架构通常用于自回归任务，如序列生成。然而，在使用上下文嵌入生成序列时，与使用编码器相比，有一些额外的考虑因素。下方展示了一个例子。
- en: '![](../Images/9a0bda9202f68abe52c409227bb1c833.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9a0bda9202f68abe52c409227bb1c833.png)'
- en: We can notice that, whilst the decoder produces a contextual embedding for each
    token in the input sequence, we usually use the embedding corresponding to the
    final token as the input to subsequent layers when generating sequences.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以注意到，尽管解码器为输入序列中的每个 token 生成了一个上下文嵌入，但我们通常使用与最终 token 对应的嵌入作为生成序列时输入到后续层的内容。
- en: 'Additionally, after applying the SoftMax function to the logits, if no filtering
    is applied, we would receive a probability distribution over every token in the
    model’s vocabulary; this can be incredibly large! Often, we wish to reduce the
    number of potential options using various filtering strategies, some of the most
    common methods are:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，在对 logits 应用 SoftMax 函数后，如果没有进行过滤，我们将会得到模型词汇表中每个 token 的概率分布；这可能非常庞大！通常，我们希望使用各种过滤策略减少潜在选项的数量，一些常见的方法包括：
- en: '**Temperature Adjustment:** temperature is a parameter which is applied inside
    of the SoftMax operation that influences the randomness of the generated text.
    It determines how creative or focused the model’s output is by altering the probability
    distribution over the output words. A higher temperature flattens the distribution,
    making outputs more diverse.'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**温度调整：** 温度是一个在 SoftMax 操作中应用的参数，影响生成文本的随机性。它通过改变输出单词的概率分布来决定模型输出的创造性或专注性。较高的温度会使分布变平，生成的输出更具多样性。'
- en: '**Top-P Sampling:** this approach filters the number of potential candidates
    for the next token based on a given probability threshold and redistributes the
    probability distribution based on candidates above this threshold.'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Top-P 采样：** 这种方法根据给定的概率阈值筛选下一个 token 的潜在候选数量，并基于超过该阈值的候选重新分配概率分布。'
- en: '**Top-K Sampling:** this approach restricts the number of potential candidates
    to the *K* most likely tokens based on their logit or probability score (depending
    on implementation)'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Top-K 采样：** 这种方法将潜在候选数量限制为基于其 logit 或概率评分（取决于实现）的 *K* 个最可能的 token。'
- en: More details on these methods [can be found here](https://peterchng.com/blog/2023/05/02/token-selection-strategies-top-k-top-p-and-temperature/).
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 这些方法的更多细节 [可以在这里找到](https://peterchng.com/blog/2023/05/02/token-selection-strategies-top-k-top-p-and-temperature/)。
- en: Once we have altered or reduced our probability distribution over the potential
    candidate for the next token, we can sample from this to get our prediction —
    this is just sampling from a [multinomial distribution](https://en.wikipedia.org/wiki/Multinomial_distribution).
    The predicted token is then appended to the input sequence and fed back into the
    model, until the desired number of tokens have been generated, or the model produces
    a *stop* token; a special token to signify the end of a sequence.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们调整或减少了对下一个 token 潜在候选的概率分布，我们可以从中进行采样以获得预测结果——这只是从一个 [多项分布](https://en.wikipedia.org/wiki/Multinomial_distribution)
    进行采样。预测的 token 然后附加到输入序列中，并反馈到模型中，直到生成所需数量的 tokens，或模型生成一个 *停止* token；一个特殊的 token，用于标记序列的结束。
- en: Encoder-decoder Architectures
  id: totrans-106
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 编码器-解码器架构
- en: Originally, the transformer was presented as an architecture for machine translation
    and used both an encoder and decoder to accomplish this goal; using the encoder
    to create an intermediate representation, before using the decoder to translate
    to the desired output format. Whilst encoder-decoder transformers have become
    less common, [architectures such as T5](https://arxiv.org/abs/1910.10683) demonstrate
    how tasks such as question answering, summarisation and classification can be
    framed as sequence-to-sequence problems and solved using this approach.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: The key difference with encoder-decoder architectures is that the decoder uses
    *encoder-decoder attention*, which uses both the outputs of the encoder (as K
    and V) and the inputs of the decoder block (as Q) during its attention calculation.
    This contrasts with self-attention, where the same input embedding matrix is used
    for all inputs. Aside from this, the overall generation process is very similar
    to using a decoder only architecture.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: We can visualise an encoder-decoder architecture as seen in the figure below.
    Here, to simplify the figure, I chose to depict the *post-layer norm* variant
    of the transformer as seen in the original paper; where the layer norm layers
    are situated after the attention blocks.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ea8723275525bea7281566f1b01e2f0a.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
- en: Conclusion
  id: totrans-111
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Hopefully this has provided an intuition on how transformers work, helped to
    break down some of the details in a somewhat digestible way and has acted as good
    starting point into demystifying modern transformer architectures!
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: '[*Chris Hughes*](https://www.linkedin.com/in/chris-hughes1/) *is on LinkedIn*'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: Unless otherwise stated, all images were created by the author.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: References
  id: totrans-115
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[[1706.03762] Attention Is All You Need (arxiv.org)](https://arxiv.org/abs/1706.03762)'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Recent Advances in Google Translate — Google Research Blog](https://blog.research.google/2020/06/recent-advances-in-google-translate.html)'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[How GitHub Copilot is getting better at understanding your code — The GitHub
    Blog](https://github.blog/2023-05-17-how-github-copilot-is-getting-better-at-understanding-your-code/)'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Introducing ChatGPT (openai.com)](https://openai.com/blog/chatgpt)'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[gpt-4.pdf (openai.com)](https://cdn.openai.com/papers/gpt-4.pdf#:~:text=This%20technical%20report%20presents%20GPT-4%2C%20a%20large%20multimodal,as%20dialogue%20systems%2C%20text%20summarization%2C%20and%20machine%20translation.)'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Introducing LLaMA: A foundational, 65-billion-parameter language model (meta.com)](https://ai.meta.com/blog/large-language-model-llama-meta-ai/)'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[The Illustrated Transformer — Jay Alammar — Visualizing machine learning one
    concept at a time. (jalammar.github.io)](http://jalammar.github.io/illustrated-transformer/)'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Byte-Pair Encoding tokenization — Hugging Face NLP Course](https://huggingface.co/learn/nlp-course/chapter6/5?fw=pt)'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Drenai series | David Gemmell Wiki | Fandom](https://davidgemmell.fandom.com/wiki/Drenai_series)'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[bert-base-uncased · Hugging Face](https://huggingface.co/bert-base-uncased)'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[bert-base-uncased · Hugging Face](https://huggingface.co/bert-base-uncased)'
- en: '[🤗 Transformers (huggingface.co)](https://huggingface.co/docs/transformers/index)'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[🤗 Transformers (huggingface.co)](https://huggingface.co/docs/transformers/index)'
- en: '[[2010.11929v2] An Image is Worth 16x16 Words: Transformers for Image Recognition
    at Scale (arxiv.org)](https://arxiv.org/abs/2010.11929v2)'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[[2010.11929v2] 一张图片值 16x16 个词：大规模图像识别的变换器 (arxiv.org)](https://arxiv.org/abs/2010.11929v2)'
- en: '[Getting Started With Embeddings (huggingface.co)](https://huggingface.co/blog/getting-started-with-embeddings)'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[开始使用嵌入 (huggingface.co)](https://huggingface.co/blog/getting-started-with-embeddings)'
- en: '[A Gentle Introduction to the Bag-of-Words Model — MachineLearningMastery.com](https://machinelearningmastery.com/gentle-introduction-bag-words-model/)'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[温和介绍词袋模型 — MachineLearningMastery.com](https://machinelearningmastery.com/gentle-introduction-bag-words-model/)'
- en: '[[2104.09864] RoFormer: Enhanced Transformer with Rotary Position Embedding
    (arxiv.org)](https://arxiv.org/abs/2104.09864)'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[[2104.09864] RoFormer: 带有旋转位置嵌入的增强型变换器 (arxiv.org)](https://arxiv.org/abs/2104.09864)'
- en: '[Scaled Dot-Product Attention Explained | Papers With Code](https://paperswithcode.com/method/scaled)'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[缩放点积注意力解释 | Papers With Code](https://paperswithcode.com/method/scaled)'
- en: '[Softmax function — Wikipedia](https://en.wikipedia.org/wiki/Softmax_function)'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Softmax函数 — Wikipedia](https://en.wikipedia.org/wiki/Softmax_function)'
- en: '[[1607.06450] Layer Normalization (arxiv.org)](https://arxiv.org/abs/1607.06450)'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[[1607.06450] 层归一化 (arxiv.org)](https://arxiv.org/abs/1607.06450)'
- en: '[Vanishing gradient problem — Wikipedia](https://en.wikipedia.org/wiki/Vanishing_gradient_problem)'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[消失梯度问题 — Wikipedia](https://en.wikipedia.org/wiki/Vanishing_gradient_problem)'
- en: '[[1512.03385] Deep Residual Learning for Image Recognition (arxiv.org)](https://arxiv.org/abs/1512.03385)'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[[1512.03385] 深度残差学习用于图像识别 (arxiv.org)](https://arxiv.org/abs/1512.03385)'
- en: '[[1810.04805] BERT: Pre-training of Deep Bidirectional Transformers for Language
    Understanding (arxiv.org)](https://arxiv.org/abs/1810.04805)'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[[1810.04805] BERT: 深度双向变换器的预训练用于语言理解 (arxiv.org)](https://arxiv.org/abs/1810.04805)'
- en: '[[2005.14165] Language Models are Few-Shot Learners (arxiv.org)](https://arxiv.org/abs/2005.14165)'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[[2005.14165] 语言模型是少量样本学习者 (arxiv.org)](https://arxiv.org/abs/2005.14165)'
- en: '[Token selection strategies: Top-K, Top-P, and Temperature (peterchng.com)](https://peterchng.com/blog/2023/05/02/token-selection-strategies-top-k-top-p-and-temperature/)'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[令牌选择策略：Top-K、Top-P 和温度 (peterchng.com)](https://peterchng.com/blog/2023/05/02/token-selection-strategies-top-k-top-p-and-temperature/)'
- en: '[Multinomial distribution — Wikipedia](https://en.wikipedia.org/wiki/Multinomial_distribution)'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[多项分布 — Wikipedia](https://en.wikipedia.org/wiki/Multinomial_distribution)'
- en: '[[1910.10683] Exploring the Limits of Transfer Learning with a Unified Text-to-Text
    Transformer (arxiv.org)](https://arxiv.org/abs/1910.10683)'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[[1910.10683] 探索统一的文本到文本变换器的迁移学习极限 (arxiv.org)](https://arxiv.org/abs/1910.10683)'
