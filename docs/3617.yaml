- en: Exploding & Vanishing Gradient Problem in Deep Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/vanishing-exploding-gradient-problem-neural-networks-101-c8f48ec6a80b?source=collection_archive---------5-----------------------#2023-12-08](https://towardsdatascience.com/vanishing-exploding-gradient-problem-neural-networks-101-c8f48ec6a80b?source=collection_archive---------5-----------------------#2023-12-08)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: How to ensure your neural network doesn’t “die” or “blow-up”
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@egorhowell?source=post_page-----c8f48ec6a80b--------------------------------)[![Egor
    Howell](../Images/1f796e828f1625440467d01dcc3e40cd.png)](https://medium.com/@egorhowell?source=post_page-----c8f48ec6a80b--------------------------------)[](https://towardsdatascience.com/?source=post_page-----c8f48ec6a80b--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----c8f48ec6a80b--------------------------------)
    [Egor Howell](https://medium.com/@egorhowell?source=post_page-----c8f48ec6a80b--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F1cac491223b2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fvanishing-exploding-gradient-problem-neural-networks-101-c8f48ec6a80b&user=Egor+Howell&userId=1cac491223b2&source=post_page-1cac491223b2----c8f48ec6a80b---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----c8f48ec6a80b--------------------------------)
    ·9 min read·Dec 8, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fc8f48ec6a80b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fvanishing-exploding-gradient-problem-neural-networks-101-c8f48ec6a80b&user=Egor+Howell&userId=1cac491223b2&source=-----c8f48ec6a80b---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc8f48ec6a80b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fvanishing-exploding-gradient-problem-neural-networks-101-c8f48ec6a80b&source=-----c8f48ec6a80b---------------------bookmark_footer-----------)![](../Images/8ec02d8429b5e34801dd4ccb016d5e74.png)'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://www.flaticon.com/free-icons/neural-network](https://www.flaticon.com/free-icons/neural-network).
    title=”neural network icons.” Neural network icons created by Paul J. — Flaticon.'
  prefs: []
  type: TYPE_NORMAL
- en: What are Vanishing & Exploding Gradients?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In one of my previous posts, we explained neural networks learn through the
    backpropagation algorithm. The main idea is that we start on the output layer
    and move or “propagate” the error all the way to the input layer updating the
    weights with respect to the loss function as we go. If you are unfamiliar with
    this, then I highly recommend you check that post:'
  prefs: []
  type: TYPE_NORMAL
- en: '[](/forward-pass-backpropagation-neural-networks-101-3a75996ada3b?source=post_page-----c8f48ec6a80b--------------------------------)
    [## Forward Pass & Backpropagation: Neural Networks 101'
  prefs: []
  type: TYPE_NORMAL
- en: Explaining how neural networks “train” and “learn” patterns in data by hand
    and in code using PyTorch
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/forward-pass-backpropagation-neural-networks-101-3a75996ada3b?source=post_page-----c8f48ec6a80b--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: The weights are updated using their partial derivative with respect to the loss
    function. The problem is that these gradients get smaller and smaller as we approach
    the lower layers of the network. This leads to the lower layers’ weights barely
    changing when training the network. This is known as the *vanishing gradient problem.*
  prefs: []
  type: TYPE_NORMAL
- en: The opposite can be true where gradients continue getting larger through the
    layers. This is the *exploding gradient problem* whichis mainly an issue in [***recurrent***](https://en.wikipedia.org/wiki/Recurrent_neural_network)…
  prefs: []
  type: TYPE_NORMAL
