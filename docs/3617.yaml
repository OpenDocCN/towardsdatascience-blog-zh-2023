- en: Exploding & Vanishing Gradient Problem in Deep Learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习中的梯度消失与爆炸问题
- en: 原文：[https://towardsdatascience.com/vanishing-exploding-gradient-problem-neural-networks-101-c8f48ec6a80b?source=collection_archive---------5-----------------------#2023-12-08](https://towardsdatascience.com/vanishing-exploding-gradient-problem-neural-networks-101-c8f48ec6a80b?source=collection_archive---------5-----------------------#2023-12-08)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/vanishing-exploding-gradient-problem-neural-networks-101-c8f48ec6a80b?source=collection_archive---------5-----------------------#2023-12-08](https://towardsdatascience.com/vanishing-exploding-gradient-problem-neural-networks-101-c8f48ec6a80b?source=collection_archive---------5-----------------------#2023-12-08)
- en: How to ensure your neural network doesn’t “die” or “blow-up”
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何确保你的神经网络不会“死亡”或“爆炸”
- en: '[](https://medium.com/@egorhowell?source=post_page-----c8f48ec6a80b--------------------------------)[![Egor
    Howell](../Images/1f796e828f1625440467d01dcc3e40cd.png)](https://medium.com/@egorhowell?source=post_page-----c8f48ec6a80b--------------------------------)[](https://towardsdatascience.com/?source=post_page-----c8f48ec6a80b--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----c8f48ec6a80b--------------------------------)
    [Egor Howell](https://medium.com/@egorhowell?source=post_page-----c8f48ec6a80b--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@egorhowell?source=post_page-----c8f48ec6a80b--------------------------------)[![Egor
    Howell](../Images/1f796e828f1625440467d01dcc3e40cd.png)](https://medium.com/@egorhowell?source=post_page-----c8f48ec6a80b--------------------------------)[](https://towardsdatascience.com/?source=post_page-----c8f48ec6a80b--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----c8f48ec6a80b--------------------------------)
    [Egor Howell](https://medium.com/@egorhowell?source=post_page-----c8f48ec6a80b--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F1cac491223b2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fvanishing-exploding-gradient-problem-neural-networks-101-c8f48ec6a80b&user=Egor+Howell&userId=1cac491223b2&source=post_page-1cac491223b2----c8f48ec6a80b---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----c8f48ec6a80b--------------------------------)
    ·9 min read·Dec 8, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fc8f48ec6a80b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fvanishing-exploding-gradient-problem-neural-networks-101-c8f48ec6a80b&user=Egor+Howell&userId=1cac491223b2&source=-----c8f48ec6a80b---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F1cac491223b2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fvanishing-exploding-gradient-problem-neural-networks-101-c8f48ec6a80b&user=Egor+Howell&userId=1cac491223b2&source=post_page-1cac491223b2----c8f48ec6a80b---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----c8f48ec6a80b--------------------------------)
    ·9 min read·2023年12月8日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fc8f48ec6a80b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fvanishing-exploding-gradient-problem-neural-networks-101-c8f48ec6a80b&user=Egor+Howell&userId=1cac491223b2&source=-----c8f48ec6a80b---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc8f48ec6a80b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fvanishing-exploding-gradient-problem-neural-networks-101-c8f48ec6a80b&source=-----c8f48ec6a80b---------------------bookmark_footer-----------)![](../Images/8ec02d8429b5e34801dd4ccb016d5e74.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc8f48ec6a80b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fvanishing-exploding-gradient-problem-neural-networks-101-c8f48ec6a80b&source=-----c8f48ec6a80b---------------------bookmark_footer-----------)![](../Images/8ec02d8429b5e34801dd4ccb016d5e74.png)'
- en: '[https://www.flaticon.com/free-icons/neural-network](https://www.flaticon.com/free-icons/neural-network).
    title=”neural network icons.” Neural network icons created by Paul J. — Flaticon.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://www.flaticon.com/free-icons/neural-network](https://www.flaticon.com/free-icons/neural-network).
    title=”neural network icons.” 神经网络图标由 Paul J. 创建 — Flaticon。'
- en: What are Vanishing & Exploding Gradients?
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是梯度消失与爆炸？
- en: 'In one of my previous posts, we explained neural networks learn through the
    backpropagation algorithm. The main idea is that we start on the output layer
    and move or “propagate” the error all the way to the input layer updating the
    weights with respect to the loss function as we go. If you are unfamiliar with
    this, then I highly recommend you check that post:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在我之前的一篇文章中，我们解释了神经网络如何通过反向传播算法进行学习。主要思想是我们从输出层开始，将错误“传播”到输入层，并在此过程中根据损失函数更新权重。如果你对这个概念不熟悉，我强烈推荐你查看那篇文章：
- en: '[](/forward-pass-backpropagation-neural-networks-101-3a75996ada3b?source=post_page-----c8f48ec6a80b--------------------------------)
    [## Forward Pass & Backpropagation: Neural Networks 101'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/forward-pass-backpropagation-neural-networks-101-3a75996ada3b?source=post_page-----c8f48ec6a80b--------------------------------)
    [## 前向传播与反向传播：神经网络基础 101'
- en: Explaining how neural networks “train” and “learn” patterns in data by hand
    and in code using PyTorch
  id: totrans-12
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 解释神经网络如何通过手动操作和使用 PyTorch 代码“训练”和“学习”数据中的模式。
- en: towardsdatascience.com](/forward-pass-backpropagation-neural-networks-101-3a75996ada3b?source=post_page-----c8f48ec6a80b--------------------------------)
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '[towardsdatascience.com](/forward-pass-backpropagation-neural-networks-101-3a75996ada3b?source=post_page-----c8f48ec6a80b--------------------------------)'
- en: The weights are updated using their partial derivative with respect to the loss
    function. The problem is that these gradients get smaller and smaller as we approach
    the lower layers of the network. This leads to the lower layers’ weights barely
    changing when training the network. This is known as the *vanishing gradient problem.*
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 权重是通过相对于损失函数的偏导数进行更新的。问题在于，随着我们接近网络的更低层，这些梯度变得越来越小。这导致在训练网络时，较低层的权重几乎没有变化。这被称为*梯度消失问题*。
- en: The opposite can be true where gradients continue getting larger through the
    layers. This is the *exploding gradient problem* whichis mainly an issue in [***recurrent***](https://en.wikipedia.org/wiki/Recurrent_neural_network)…
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 相反的情况也可能发生，即梯度在层间继续增大。这就是*梯度爆炸问题*，主要出现在[***递归神经网络***](https://en.wikipedia.org/wiki/Recurrent_neural_network)中……
