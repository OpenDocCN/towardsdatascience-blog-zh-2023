["```py\nimport string\n\ndef step1(word):\n    trans = str.maketrans(\"\", \"\", string.punctuation)\n    return word.replace(\"\\n\", \" \").translate(trans)\n\ndef step2(word):\n    return word.lower()\n\ndef step3(word):\n    trans = str.maketrans(\"\", \"\", \"0123456789\")\n    return word.replace(\"\\n\", \" \").translate(trans)\n\ndef step4(word):\n    return (all([char in string.ascii_letters for char in word]) and \n            len(word) > 0)\n```", "```py\n>>> step1(\"Testing---123;\")\n'Testing123'\n```", "```py\n>>> step3(step2(step1(\"Testing---123;\")))\n'testing'\n```", "```py\ndef apply(step, values):\n    return [step(value) for value in values]\n```", "```py\n>>> apply(step3, \n          apply(step2, \n                apply(step1, \n                      [\"Testing---123;\", \"456---\", \"Hello!\"])))\n['testing', '', 'hello']\n```", "```py\n>>> list(filter(step4, \n            apply(step3, \n                  apply(step2, \n                        apply(step1, \n                              [\"Testing---123;\", \"456---\", \"Hello!\"])))))\n['testing', 'hello']\n```", "```py\n# First we create a pipeline function:\np = my_pipeline(step1, step2, step3)\n\n# And then we apply it to a dataset:\np([\"Testing---123;\", \"456---\", \"Hello!\"])\n```", "```py\ndef my_pipeline(*steps):\n    def wrapper(inputs):\n        for step in steps:\n            inputs = apply(step, inputs)\n        return inputs\n    return wrapper\n```", "```py\n>>> p = my_pipeline(step1, step2, step3)\n>>> p([\"Testing---123;\", \"456---\", \"Hello!\"])\n['testing', '', 'hello']\n```", "```py\nfrom essential_generators import DocumentGenerator\nimport os\n\ngen = DocumentGenerator()\n\ndef generate_documents(\n    count=10_000, \n    paragraphs=10, \n    output_folder=\"documents\", \n    overwrite=False\n):\n    os.makedirs(output_folder, exist_ok=True)\n    for n in range(count):\n        filename = os.path.join(\n            output_folder, \n            \"doc_%05d.txt\" % (n + 1)\n        )\n        if overwrite or not os.path.exists(filename):\n            with open(filename, \"w\") as fp:\n                for p in range(paragraphs):\n                    fp.write(gen.paragraph() + \"\\n\\n\")\n\ngenerate_documents()\n```", "```py\ndef step0(filename):\n    return open(filename).read().split(\" \")\n```", "```py\ndef apply(step, outputs):\n    return (step(input) if not isinstance(input, list) else \n            [step(i) for i in input] for input in outputs)\n```", "```py\np = my_pipeline(step0, step1, step2, step3)\nlist(p([\"documents/doc_00001.txt\"]))\n```", "```py\npip install picopipe\n```", "```py\nfrom picopipe import pipeline, pfilter\n\np = pipeline(step0, step1, step2, step3, pfilter(step4))\nlist(p([\"documents/doc_00001.txt\"])[0])\n```", "```py\nimport glob\n\ndataset = glob.glob(\"documents/doc_*.txt\")\n```", "```py\nresults = list(p(dataset))\n```", "```py\nimport time\n\nx = []\ny = []\nfor i in range(10):\n    start = time.time()\n    results = list(p(dataset, n_jobs=i))\n    total_time = time.time() - start\n    x.append(i)\n    y.append(total_time)\n```", "```py\nfrom picopipe import to_mermaid\n\nwith open(\"pipeline.mmd\", \"w\") as fp:\n    fp.write(to_mermaid(p))\n```"]