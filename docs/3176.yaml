- en: From Data Platform to ML Platform
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/from-data-platform-to-ml-platform-4a8192edab5d?source=collection_archive---------0-----------------------#2023-10-22](https://towardsdatascience.com/from-data-platform-to-ml-platform-4a8192edab5d?source=collection_archive---------0-----------------------#2023-10-22)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: How Data/ML platforms evolve and support complex MLOps practices
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@ming.gao.gm?source=post_page-----4a8192edab5d--------------------------------)[![ming
    gao](../Images/4eeb08e6f2f47f789801694b82fe3057.png)](https://medium.com/@ming.gao.gm?source=post_page-----4a8192edab5d--------------------------------)[](https://towardsdatascience.com/?source=post_page-----4a8192edab5d--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----4a8192edab5d--------------------------------)
    [ming gao](https://medium.com/@ming.gao.gm?source=post_page-----4a8192edab5d--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F56b61a38427c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffrom-data-platform-to-ml-platform-4a8192edab5d&user=ming+gao&userId=56b61a38427c&source=post_page-56b61a38427c----4a8192edab5d---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----4a8192edab5d--------------------------------)
    ·9 min read·Oct 22, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F4a8192edab5d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffrom-data-platform-to-ml-platform-4a8192edab5d&user=ming+gao&userId=56b61a38427c&source=-----4a8192edab5d---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4a8192edab5d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffrom-data-platform-to-ml-platform-4a8192edab5d&source=-----4a8192edab5d---------------------bookmark_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: Data/ML has been the most popular topic in our tech landscape. I want to share
    my understanding of Data/ML Platform and how would those platforms evolve from
    basic to complex. At last, I try my best to cover MLOps, a principle for managing
    ML projects.
  prefs: []
  type: TYPE_NORMAL
- en: About who-I-am, here is [my LinkedIn](https://www.linkedin.com/in/ming-gao-57509a101/).
  prefs: []
  type: TYPE_NORMAL
- en: 'Starting of the Journey: Online Service + OLTP + OLAP'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: At the starts, data infrastructures could be fairly simple. Analytical queries
    might be sent to the read replica of a online [OLTP database](https://en.wikipedia.org/wiki/Online_transaction_processing)
    or setting up a OLAP database serve as data warehouse.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the infrastructure might look like:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/91b189e685127aef067dfd947f9df224.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by the author
  prefs: []
  type: TYPE_NORMAL
- en: There is nothing wrong with those systems as long as it fulfil business requirements.
    All systems that fulfil our business need are good systems. If there are simple,
    it is even better.
  prefs: []
  type: TYPE_NORMAL
- en: 'At this stage, there are multiple ways of doing data analysis:'
  prefs: []
  type: TYPE_NORMAL
- en: Simply submit queries to OLTP database’s replica node. (Not recommended).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Enabling CDC(Change Data Capture) of OLTP databse and ingest those data to OLAP
    database. Come to the option of ingestion service for CDC logs, you can choose
    based on the OLAP database you have selected. For example, [Flink data streaming
    with CDC connectors](https://github.com/ververica/flink-cdc-connectors) is a way
    to handle this. Many enterprise services come with their own suggested solution,
    e.g. [Snowpipe](https://quickstarts.snowflake.com/guide/CDC_SnowpipeStreaming_DynamicTables/index.html?index=..%2F..index#0)
    for Snowflake. It is also recommended to load data from replica node to preserve
    the CPU/IO bandwidth of master node for online traffic.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In this stage, ML workloads might be running in your local environment. You
    can set up a [Jupyter](https://jupyter.org/install) notebook locally, and load
    structured data from OLAP Database, then train your ML model locally.
  prefs: []
  type: TYPE_NORMAL
- en: 'The potential challenges of this architecture are but not limited to:'
  prefs: []
  type: TYPE_NORMAL
- en: It is hard to manage unstructured or semi-structured data with OLAP database.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: OLAP might have performance regression when come to massive data processing.
    (more than TB data required for a single ETL task)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lack of supporting for various compute engines, e.g. Spark or Presto. Most of
    compute engine do support connecting to OLAP with JDBC endpoint, but the parallel
    processing will be badly limited by the IO bottleneck of JDBC endpoint itself.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The cost of storing massive data in OLAP database is high.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You might know the direction to solve this already. Build a Data lake! Bringing
    in Data lake do not necessary mean you need to completely sunset OLAP Database.
    It is still common to see company having two systems co-exist for different use-cases.
  prefs: []
  type: TYPE_NORMAL
- en: 'Data lake: Storage-Compute Separation + Schema on Write'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A data lake allows you to persist unstructured and semi-structure data, and
    performs schema-on-read. It allows you reduce cost by storing large data volume
    with specialised storage solution and spun up compute cluster based on your demand.
    It further allows you to manage TB/PB dataset effortlessly by scaling up the compute
    clusters.
  prefs: []
  type: TYPE_NORMAL
- en: 'There is how your infrastructure might look like:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c23044e4526d4ac5dee67ac3bd0cf016.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by the author
  prefs: []
  type: TYPE_NORMAL
- en: This is an oversimplified graph indeed. The actually implementation of a data
    lake can be much more complicated.
  prefs: []
  type: TYPE_NORMAL
- en: Many cloud provider now have quite established store solution for Data lake,
    e.g. AWS S3 and Azure ADLS. There is still a lot of tasks need to be done on top
    of those storage solutions. For example, there should be a Hive metastore to manage
    your table metadata and a [Datahub](https://github.com/datahub-project/datahub)
    to provide data visibility. There are also challenging topics like *fine-grain
    permission control in data lake* and *data lineage analysis(e.g.* [spline](https://absaoss.github.io/spline/)*)*.
  prefs: []
  type: TYPE_NORMAL
- en: To maximum the value and efficiency of your data lake, we should carefully choose
    file format and average file sizes for each layers of your data lake.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b7f14745cfb589ea9bac421913334c57.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by the author
  prefs: []
  type: TYPE_NORMAL
- en: 'The general tips are:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Avoid small files: Small files are one of major causes for high storage cost
    and poor performance in Data lake.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Balance between *latency*, *compress* *ratio* and *performance*: A low latency
    Data lake table with file format like [Hudi](https://hudi.apache.org/blog/2021/07/21/streaming-data-lake-platform/)
    might not give you the best compress ratio, and large ORC files with high compress
    ratio might give your performance nightmare. You might want to choose file format
    wisely based on the usage pattern of the table, latency requirement and table
    sizes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are some quite established SaaS/PaaS provider like [Databricks](https://www.databricks.com/)
    which provide a decent Data lake(or LakeHouse now) solution. You also can explore
    [ByteHouse](https://bytehouse.cloud/) to have a unified experience of big data
    analysis.
  prefs: []
  type: TYPE_NORMAL
- en: On the ML side, team might start exploring well established ML framework like
    Tensenflow and Pytorch in remote environment. Furthermore, trained ML models could
    been deployed to production environment for online model inferrence. Both Tensorflow
    and Pytorch come with serving solution, e.g. TensorFlow Serving and Pytorch Serving.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, our journey will not stop here. We might have following challenges
    now:'
  prefs: []
  type: TYPE_NORMAL
- en: Lack of realtime metric and features management which are critical for online
    ML model serving.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lack of model performance monitoring.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s level up our game further.
  prefs: []
  type: TYPE_NORMAL
- en: 'Realtime Data/ML Infra: Data River + Data Streaming + Feature Store + Metric
    Server'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It usually a joint effort from multiple departments of companies to build realtime
    data infra. The initial rationale of building Data River usually is not for data/ML
    system but allowing micro-services to further scale up by removing synchronised
    call. Instead, micro-services will gain efficiency by communicating with a message
    broker like [Kafka](https://kafka.apache.org/) (at the cost of lower consistency
    level).
  prefs: []
  type: TYPE_NORMAL
- en: The overall architecture might look like this.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/11709d7d8e95a361f21cc6d4ff616520.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by the author
  prefs: []
  type: TYPE_NORMAL
- en: With data available in Data River (e.g. [Kafka](https://kafka.apache.org/)),
    we can build data streaming pipeline to process realtime data. Those data can
    be used directly in online feature store or sync to an metric server like [Pinot](https://github.com/apache/pinot).
    Metric server can further process/aggregate those metric point to more useful
    model performance metrics and business metrics. You can also adopt streaming database
    like [RisingWave](https://www.risingwave.com/) which can joining/aggregating streaming
    data with SQL syntax.
  prefs: []
  type: TYPE_NORMAL
- en: For building data streaming itself, [Flink](https://flink.apache.org/) is quite
    popular. You can also use [Flink with CDC connector](https://ververica.github.io/flink-cdc-connectors/release-2.1/content/about.html)
    to extract data from OLTP database and sink data to message brokers and data lake.
  prefs: []
  type: TYPE_NORMAL
- en: There should be a online feature store backed by key-value database like [ScyllaDB](https://www.scylladb.com/)
    or AWS Dynamo DB. Online feature store can help you enrich the the request sent
    to Model Serving service with a feature vector associated with certain reference
    ID (user-id, product uuid). It can greatly de-couple the dependency between backend
    service team who build micro-services and ML engineer team who build ML models.
    It allow ML engineers rollout new ML feature with new ML model independently (*Your
    model serving API signature expose to micro-services will remain the same when
    you update feature vector).*
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/630bcf3480c7796bd8d4878d436ad999.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by the author
  prefs: []
  type: TYPE_NORMAL
- en: In the book, [Designing Machine Learning System](https://www.oreilly.com/library/view/designing-machine-learning/9781098107956/),
    it has shared about Model Stacking ([Jen Wadkin’s](https://threnjen.medium.com/)
    [medium post](/simple-model-stacking-explained-and-automated-1b54e4357916) about
    model stacking). It is quite common for people to use model stacking in model
    serving as well. An orchestrator is required when you want to to stack hererogenous
    models together, e.g. stacking pytorch and tensorflow model together. You potential
    can make your orchestrator even more complicated by having a dynamic weightage
    based on model performance when routing request to different models.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we have a complicated system. It looks pretty cool but it carry new challenges:'
  prefs: []
  type: TYPE_NORMAL
- en: Debt of the system will soaring high if leave it unmanage.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: High cognitive load for ML engineers.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: That’s probably when you need to thinking how MLOps can help you.
  prefs: []
  type: TYPE_NORMAL
- en: 'MLOps: Abstraction, Observability and Scalability'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'MLOps is never a specific solution. It is more like a set of principles for
    managing ML system. Different with a typical software project. ML systems are
    greatly affected by data shifting, and data dependency management is not a easy
    task. Paper [Hidden Technical Debt in Machine Learning Systems](https://proceedings.neurips.cc/paper_files/paper/2015/file/86df7dcfd896fcaf2674f757a2463eba-Paper.pdf)
    has described those challenges in details. Therefore, a MLOps driven ML platform
    must able to:'
  prefs: []
  type: TYPE_NORMAL
- en: Data change monitoring & data quality monitoring.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Manage ML features across offline and online environment.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reproducible ML Pipeline which fulfil experimental-operational symmetry.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Concise ML pipeline configuration which can abstract away infra details.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This article, [MLOps: Continuous delivery and automation pipelines in machine
    learning](https://cloud.google.com/architecture/mlops-continuous-delivery-and-automation-pipelines-in-machine-learning),
    highlighted the importance of [experimental-operational symmetric](https://cloud.google.com/architecture/mlops-continuous-delivery-and-automation-pipelines-in-machine-learning#characteristics_2).
    It also described MLOps automation level from level-0, level-1 to finally level-2\.
    I really like the graph from this doc and will just borrow it to explain what
    level-1 MLOps looks like.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/995a8cd1a179f9c0570df09ceb34025e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Image by Author. Describing MLOps Level-1 in [MLOps: Continuous delivery and
    automation pipelines in machine learning](https://cloud.google.com/architecture/mlops-continuous-delivery-and-automation-pipelines-in-machine-learning)'
  prefs: []
  type: TYPE_NORMAL
- en: To scale such MLOps practise in your organisation, you need to provide concise
    ML pipeline configuration which can abstract infrastructure implementation details
    away for ML Engineers. By doing this, platform engineers also gain flexibility
    for upgrading ML platform without causing too much disruption to platform users.
    You can consider using configuration files like yaml to describe ML pipelines
    and rely on your ML Pipeline controllers to translate them to actual workload.
  prefs: []
  type: TYPE_NORMAL
- en: So let’s re-organised realtime data/ML infra with following graph to highlight
    how MLOps shapes our platforms.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3e97f33cbd4aabce938a02c91af58155.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by the author
  prefs: []
  type: TYPE_NORMAL
- en: To give you a better ideal of what the ML pipelines might look like. Here are
    the possible abstraction examples for each stage in ML pipeline. The following
    graph only help your to further understand what is the configuration might look
    like. It does not represent any actual implementation. It does not cover all aspects
    required neither.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1de18b307f8c4b3a05838f7fbba7a76f.png)'
  prefs: []
  type: TYPE_IMG
- en: A general idea of configurations in ML pipeline. Image by the author
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes is a popular solution to orchestrate ML workload(or maybe all workload
    nowadays). You can use CRDs to provide concise interfaces between users and platforms.
    In article [My thinking of Kubebuilder](https://towardsdev.com/my-thinking-about-kubebuilder-443a9d45f1e),
    I have shared some of my thinking when I build CRD with kubebuilder.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/816be3f43007666345b98180577530a5.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by the author
  prefs: []
  type: TYPE_NORMAL
- en: 'Of course, I didn’t cover many important sub-topics which include but not limited
    to:'
  prefs: []
  type: TYPE_NORMAL
- en: Hyperparameter Optimization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Distributed Training architecture
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What Next
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You can see ***MLOps only give a known mission a proper name***. It is far from
    a job done. What I shared is a opinionated strategy for implement ML Ops platform.
    Even with that, the bar of creating high quality ML product is still high, and
    the effort of collecting, processing, mining data is still heavy.
  prefs: []
  type: TYPE_NORMAL
- en: Besides those challenges remains, I also want to share the trends in ML landscape
    I have observed. It is surely not a completed list given how fast this domain
    evolves.
  prefs: []
  type: TYPE_NORMAL
- en: 'Serviceless: We have put ML’s value too far behind because the foundation a
    ML platform is usually a Data platform. It is like forcing users to buy computers
    to engage on social media platforms when we are already in mobile time. Serviceless
    data services and data engine are addressing this challenge. Many service providers
    explore their own ***serveless*** solution to lower the bar of adoption, e.g.
    [Databricks](https://docs.databricks.com/en/serverless-compute/index.html), Snowflake
    , Bytehouse. Companies can start building their ML products after bootstrapping
    data warehouses, or data lakes, or lakehouses.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'AI driven feature engineering: Well, AI can do everything now, can’t it?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'MaaS trends: More powerful Model-as-a-Service will pop up. Companies can directly
    leverage on ML power without even building their own ML service to enjoy a great
    lifting to their business.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As we all have noticed, ML space evolves so fast. At this very moment, when
    I am typing this, this article might already been expired. More ideas had already
    popped up and been translated to reality. Please do let me know what do you think
    about of ML Ops, or where should I further my learning. Let’s keep up the pace
    together!
  prefs: []
  type: TYPE_NORMAL
