# 逻辑回归中的矩阵和向量运算

> 原文：[https://towardsdatascience.com/matrix-and-vector-operations-in-logistic-regression-e35714c4810f?source=collection_archive---------8-----------------------#2023-07-07](https://towardsdatascience.com/matrix-and-vector-operations-in-logistic-regression-e35714c4810f?source=collection_archive---------8-----------------------#2023-07-07)

## 向量化逻辑回归

[](https://murali-kashaboina.medium.com/?source=post_page-----e35714c4810f--------------------------------)[![穆拉利·卡沙博伊纳](../Images/ff1118f3c317dab87fe4b625a614fb93.png)](https://murali-kashaboina.medium.com/?source=post_page-----e35714c4810f--------------------------------)[](https://towardsdatascience.com/?source=post_page-----e35714c4810f--------------------------------)[![Towards Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----e35714c4810f--------------------------------) [穆拉利·卡沙博伊纳](https://murali-kashaboina.medium.com/?source=post_page-----e35714c4810f--------------------------------)

·

[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F2d02cbc7d153&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmatrix-and-vector-operations-in-logistic-regression-e35714c4810f&user=Murali+Kashaboina&userId=2d02cbc7d153&source=post_page-2d02cbc7d153----e35714c4810f---------------------post_header-----------) 发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----e35714c4810f--------------------------------) ·10分钟阅读·2023年7月7日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fe35714c4810f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmatrix-and-vector-operations-in-logistic-regression-e35714c4810f&user=Murali+Kashaboina&userId=2d02cbc7d153&source=-----e35714c4810f---------------------clap_footer-----------)

--

[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fe35714c4810f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmatrix-and-vector-operations-in-logistic-regression-e35714c4810f&source=-----e35714c4810f---------------------bookmark_footer-----------)![](../Images/28c90d52b5f07dc1c15a0dd6b6832cf0.png)

照片由 [fabio](https://unsplash.com/@fabioha?utm_source=medium&utm_medium=referral) 提供，来源于 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)

任何人工神经网络（ANN）算法背后的数学基础可能令人难以理解。此外，用于表示模型在批量训练过程中前向传播和反向传播计算的矩阵和向量操作会增加理解的难度。虽然简洁的矩阵和向量符号是有意义的，但深入这些符号以了解矩阵操作的细节会带来更多的清晰度。我意识到，理解这些微妙细节的最佳方法是考虑一个最基本的网络模型。我找不到比逻辑回归更好的算法来探索底层机制，因为它具备了ANN的所有特点，如多维输入、网络权重、偏差、前向传播操作、应用非线性函数的激活函数、损失函数和基于梯度的反向传播。我的博客意图是分享我对逻辑回归模型核心的矩阵和向量操作的笔记和发现。

## 逻辑回归简要概述

尽管名字里有“回归”，逻辑回归实际上是一种分类算法，而不是回归算法。它通常用于二分类任务，以预测某个实例属于两个类别之一的概率，例如，预测一封电子邮件是否是垃圾邮件。因此，在逻辑回归中，因变量或目标变量被视为分类变量。例如，垃圾邮件用1表示，而非垃圾邮件用0表示。逻辑回归模型的主要目标是建立输入变量（特征）与目标变量概率之间的关系。例如，给定一封电子邮件的特征作为输入特征集合，逻辑回归模型会找到这些特征与电子邮件是垃圾邮件的概率之间的关系。如果‘Y’表示输出类别，比如电子邮件是垃圾邮件，‘X’表示输入特征，则概率可以表示为 π = Pr( Y = 1 | X, βi)，其中 βi 表示包括模型权重‘*wi*’和偏置参数‘b’在内的逻辑回归参数。实际上，逻辑回归预测给定输入特征和模型参数下 Y = 1 的概率。具体来说，概率 π 被建模为一个 S 形的逻辑函数，称为 Sigmoid 函数，公式为 π = e^z/(1 + e^z) 或等效地 π = 1/(1 + e^-z)，其中 z = βi . X。Sigmoid 函数允许在0和1之间平滑曲线，非常适合于估计概率。本质上，逻辑回归模型在输入特征的线性组合上应用 Sigmoid 函数，以预测0和1之间的概率。确定实例输出类别的常见方法是对预测概率进行阈值处理。例如，如果预测概率大于或等于0.5，则该实例被分类为类别1；否则，分类为类别0。

![](../Images/b54db4b515dc708bb21e7fd64b96dcfb.png)

逻辑回归模型示意图 — 由作者创建

逻辑回归模型通过将模型拟合到训练数据上来训练，然后通过最小化损失函数来调整模型参数。损失函数估计预测概率与实际概率之间的差异。用于训练逻辑回归模型的最常见损失函数是对数损失函数，也称为二元交叉熵损失函数。对数损失函数的公式如下：

L = — ( y * ln(p) + (1 — y) * ln(1 — p) )

其中：

+   L 代表对数损失。

+   y 是实际的二元标签（0 或 1）。

+   p 是输出类别的预测概率。

逻辑回归模型通过使用梯度下降等技术来最小化损失函数，从而调整其参数。给定一批输入特征及其真实类别标签，模型的训练在多个迭代（称为epoch）中进行。在每个epoch中，模型进行正向传播操作来估计损失，并进行反向传播操作以最小化损失函数并调整参数。所有这些操作都涉及矩阵和向量计算，如下一节所示。

## 矩阵和向量表示

***请注意*** ***我使用了LaTeX脚本来创建嵌入在此博客中的数学方程和矩阵/向量表示的图片***。如果有人对LaTeX脚本感兴趣，请随时联系我；我很乐意分享。

如上图所示，使用二元逻辑回归分类器作为示例，以简化插图。如下所示，矩阵X表示‘m’个输入实例。每个输入实例包含’n’个特征，并表示为矩阵X中的一列，即输入特征向量，使其成为一个（n x m）大小的矩阵。上标（i）表示矩阵X中输入向量的序号。下标‘j’表示输入向量中特征的序号。大小为（1 x m）的矩阵Y捕捉了矩阵X中每个输入向量的真实标签。模型权重由大小为（n x 1）的列向量W表示，其中包含’n’个权重参数，对应于输入向量中的每个特征。虽然只有一个偏置参数‘b’，为了说明矩阵/向量操作，考虑一个大小为（1 x m）的矩阵B，其中包含‘m’个相同的偏置b参数。

![](../Images/8dc491fa91410a44f21b226d5071c1a1.png)

## 正向传播

正向传播操作的第一步是计算模型参数和输入特征的线性组合。如下所示，此矩阵操作的符号表示一个新的矩阵Z的计算：

![](../Images/2b585602eac790182831465b1353ad1a.png)

注意权重矩阵W的转置使用。上述矩阵的扩展表示如下：

![](../Images/996d0df64ffcda4a7f145cded84df8be.png)

上述矩阵运算的结果是计算出大小为（1 x m）的矩阵Z，如下所示：

![](../Images/64c31281cd6dc7a89b44d97669bedcd8.png)

下一步是通过对计算出的线性组合应用sigmoid函数来推导激活值，如以下矩阵操作所示。这会生成一个大小为（1 x m）的激活矩阵A。

![](../Images/a632cfef5c85e0507f38fc6664521332.png)

## 反向传播

反向传播或称为反向传播是一种计算每个参数对最终预测错误或损失的贡献的技术。通过计算损失函数对每个模型参数的梯度来评估各个损失的贡献。函数的梯度或导数是该函数相对于一个参数的变化率或斜率，同时将其他参数视为常数。当在特定的参数值或点上进行评估时，梯度的符号指示函数增加的方向，梯度的大小指示斜率的陡峭程度。如下所示的对数损失函数是一个碗状的凸函数，具有一个全局最小点。因此，在大多数情况下，对数损失函数的梯度相对于参数指向全局最小值的相反方向。一旦评估了梯度，就使用参数的梯度更新每个参数值，通常使用称为梯度下降的技术。

![](../Images/2b6c87bb5f875c07583c4520046798d4.png)

每个参数的梯度使用链式法则计算。链式法则使得能够计算由其他函数组成的函数的导数。在逻辑回归的情况下，对数损失 L 是激活‘a’和真实标签‘y’的函数，而‘a’本身是‘z’的 sigmoid 函数，‘z’是权重‘w’和偏置‘b’的线性函数，这意味着损失函数 L 是由其他函数组成的函数，如下所示。

![](../Images/d18cc74dc6680d666b87836fe816a93e.png)

利用偏导数链式法则，权重和偏置参数的梯度可以如下计算：

![](../Images/08b68c624cd2e55cc919baa9ae1285c5.png)

**单个输入实例的梯度推导**

在我们回顾作为更新参数的一部分的矩阵和向量表示之前，我们将首先使用单个输入实例推导梯度，以便更好地理解这些表示的基础。

假设‘a’和‘z’表示单个输入实例的计算值，并且真实标签为‘y’，则损失函数相对于‘a’的梯度可以推导如下。请注意，这个梯度是评估链式法则以推导参数梯度所需的第一个量。

![](../Images/1a5ef1c9ec58b70cf69d43b665b045d9.png)

给定损失函数相对于‘a’的梯度，可以使用以下链式法则推导损失函数相对于‘z’的梯度：

![](../Images/c933a583975437fa2b718414c6adb48d.png)

上述链式法则意味着必须推导出‘a’相对于‘z’的梯度。请注意，‘a’是通过对‘z’应用 sigmoid 函数计算得出的。因此，‘a’相对于‘z’的梯度可以通过如下所示的 sigmoid 函数表达式推导出来：

![](../Images/745ffa7d390b9b867fbf99a305118d0f.png)

上述推导以‘e’为基础，似乎需要额外的计算来评估‘a’相对于‘z’的梯度。我们知道‘a’是在前向传播过程中计算的。因此，为了消除任何额外的计算，上述导数可以完全用‘a’表示，如下：

![](../Images/79d9fdac00d680ce8c2d30a24e4c63f4.png)

插入用‘a’表示的上述术语，‘a’相对于‘z’的梯度如下：

![](../Images/947d2e4cb99ed35cab7c6745a9881945.png)

现在我们有了损失函数对‘a’的梯度以及‘a’对‘z’的梯度，损失函数对‘z’的梯度可以如下评估：

![](../Images/88780a8cdf6de1acae26aac7d8bcbc8a.png)

我们在评估损失函数对‘z’的梯度方面已经取得了很大进展。我们仍然需要评估损失函数对模型参数的梯度。我们知道‘z’是模型参数和输入实例‘x’特征的线性组合，如下所示：

![](../Images/d9e3562dd7682af65faee75cc4ba3fae.png)

使用链式法则，损失函数对权重参数‘wi’的梯度被评估如下：

![](../Images/ca91815d745cb0ec89622252206cdcd5.png)

同样，损失函数对‘b’的梯度被评估如下：

![](../Images/d534432ca71074eafd4a8c363b73ae58.png)

**使用梯度的参数更新的矩阵和向量表示**

现在我们理解了使用单个输入实例导出的模型参数的梯度公式，我们可以将这些公式表示为矩阵和向量形式，以考虑整个训练批次。我们将首先对损失函数对‘z’的梯度进行向量化，其表达式如下：

![](../Images/25adb16335677d0af11dd642241580c4.png)

上述所有‘m’实例的向量形式是：

![](../Images/7a29beb97558bb18cb4f9c528acd4f05.png)

同样，损失函数对每个权重‘wi’的梯度可以进行向量化。单个实例的损失函数对权重‘wi’的梯度由下式给出：

![](../Images/edffa69ddf2006375955e7bb552351cb.png)

上述所有权重在所有‘m’输入实例中的向量形式被计算为‘m’梯度的均值，如下：

![](../Images/d1e8cd1878a049bd2fe28fa8e606fa7b.png)

同样，损失函数对‘b’的梯度在所有‘m’输入实例中的结果是通过如下方式计算的各个实例梯度的均值：

![](../Images/b56b93f539efe842c6c8d05ddea7b2d7.png)

给定模型权重梯度向量和偏置的整体梯度，模型参数将按以下方式更新。如下所示的参数更新基于称为梯度下降的技术，其中使用了学习率。学习率是优化技术（如梯度下降）中使用的超参数，用于控制每次迭代时对模型参数进行调整的步长。有效地说，学习率充当缩放因子，影响优化算法的速度和收敛性。

![](../Images/e9a533630ad1f2321ee5231b4b956290.png)

## 结论

从本博客中说明的矩阵和向量表示可以看出，逻辑回归使得一个基本的网络模型能够理解这些矩阵和向量操作的细微工作细节。大多数机器学习库封装了这些琐碎的数学细节，但却在更高层次上暴露了定义良好的编程接口，如前向传播或反向传播。虽然理解所有这些细微的细节可能不是使用这些库开发模型的必要条件，但这些细节确实揭示了这些算法背后的数学直觉。然而，这种理解肯定有助于将底层数学直觉应用到其他模型，如人工神经网络（ANN）、递归神经网络（RNN）、卷积神经网络（CNN）和生成对抗网络（GAN）。
