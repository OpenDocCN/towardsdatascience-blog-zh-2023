- en: The Bias-Variance Tradeoff
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/the-bias-variance-tradeoff-cf18d3ec54f9?source=collection_archive---------6-----------------------#2023-02-23](https://towardsdatascience.com/the-bias-variance-tradeoff-cf18d3ec54f9?source=collection_archive---------6-----------------------#2023-02-23)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*Understanding this famous concept and how it impacts Machine Learning models*'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://alexandrerossetolemos.medium.com/?source=post_page-----cf18d3ec54f9--------------------------------)[![Alexandre
    Rosseto Lemos](../Images/875245dfdc6d10a20a94791a5d88c5ae.png)](https://alexandrerossetolemos.medium.com/?source=post_page-----cf18d3ec54f9--------------------------------)[](https://towardsdatascience.com/?source=post_page-----cf18d3ec54f9--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----cf18d3ec54f9--------------------------------)
    [Alexandre Rosseto Lemos](https://alexandrerossetolemos.medium.com/?source=post_page-----cf18d3ec54f9--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fde43cc058e79&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-bias-variance-tradeoff-cf18d3ec54f9&user=Alexandre+Rosseto+Lemos&userId=de43cc058e79&source=post_page-de43cc058e79----cf18d3ec54f9---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----cf18d3ec54f9--------------------------------)
    ·5 min read·Feb 23, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fcf18d3ec54f9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-bias-variance-tradeoff-cf18d3ec54f9&user=Alexandre+Rosseto+Lemos&userId=de43cc058e79&source=-----cf18d3ec54f9---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fcf18d3ec54f9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-bias-variance-tradeoff-cf18d3ec54f9&source=-----cf18d3ec54f9---------------------bookmark_footer-----------)![](../Images/fd8652be4059aab424bdc145093e5dd8.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Photo by [Piret Ilver](https://unsplash.com/@saltsup?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: The bias-variance tradeoff is a fundamental and widely discussed concept in
    the area of Data Science. Understanding the bias-variance tradeoff is essential
    for developing accurate and reliable machine learning models, as it can help us
    optimize model performance and avoid common pitfalls such as underfitting and
    overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: Before defining it, it is necessary to define what is bias and variance separately.
  prefs: []
  type: TYPE_NORMAL
- en: Bias and Variance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/5c047d342e6526139a4159105bf5c2e5.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [PEIWEN HE](https://unsplash.com/@peiwenhe?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: Bias
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Bias refers to the error that is introduced by approximating a real-life problem
    with a simplified model. A model with high bias is not able to capture the true
    complexity of the data and tends to underfit, leading to poor performance on both
    the training and test data. The bias is represented by the difference between
    the expected or true value of the target variable and the predicted value of the
    model.
  prefs: []
  type: TYPE_NORMAL
- en: Variance
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Variance refers to the error introduced by the model’s sensitivity to small
    fluctuations in the training data. A model with high variance tends to overfit
    the training data, leading to poor performance on new, unseen data. Variance is
    represented by the degree of variability or spreads in the model’s predictions
    for different training sets.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the bias-variance tradeoff is essential for developing accurate
    and reliable Machine Learning models. It can help to optimize the model performance
    and avoid common pitfalls such as underfitting and overfitting. One of the best
    ways to visualize the bias and variance concepts is through a dartboard like the
    one shown below.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5b9e6254f0e442aecea78bd469cd4846.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Source: [V. Gudivada, A. Apon & J. Ding, 2017](https://www.researchgate.net/publication/318432363_Data_Quality_Considerations_for_Big_Data_and_Machine_Learning_Going_Beyond_Data_Cleaning_and_Transformations)'
  prefs: []
  type: TYPE_NORMAL
- en: 'The figure shows how variance and bias are related:'
  prefs: []
  type: TYPE_NORMAL
- en: A model with high bias and high variance is a model that makes a lot of mistakes
    and is very inconsistent.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A model with high variance and low bias tends to be more accurate, but the results
    suffer a lot of variations.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A model with high bias and low variance is a model that makes a lot of bad predictions
    but is very consistent in its results.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lastly, a model with low bias and variance makes good predictions and is consistent
    with its results.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Looking at the figure, it’s intuitive that all the models should have a low
    bias and low variance, since this combination generates the best results. However,
    this is where the bias-variance tradeoff appears.
  prefs: []
  type: TYPE_NORMAL
- en: The tradeoff
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/ba02eb36962b32424cfa04d52257ec97.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Possessed Photography](https://unsplash.com/@possessedphotography?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: The bias-variance tradeoff arises because increasing the model’s complexity
    can reduce the bias but increase the variance. On the other hand, decreasing the
    complexity can reduce the variance but increase the bias. The goal is to find
    the optimal balance between bias and variance, which results in the best generalization
    performance on new, unseen data.
  prefs: []
  type: TYPE_NORMAL
- en: This is directly related to the complexity of the model used, as shown in the
    figure below.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3cba0dadaf2d2e62be859bb093e2a90b.png)'
  prefs: []
  type: TYPE_IMG
- en: Bias-variance tradeoff and error relationship (image by author)
  prefs: []
  type: TYPE_NORMAL
- en: The graph shows how the complexity of the model is related to the values of
    bias and variance. Models that have a low complexity can be too simple to understand
    the patterns of the data used in the training, a phenomenon called ***underfitting****.*Consequently,
    it won’t be able to make good predictions on the test data, resulting in a high
    bias.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, a model with too much degree of liberty can result in what
    is called ***overfitting***,which is when the model has an excellent performance
    in the training data, but has a significant decrease in performance when evaluating
    the test data. This happens when the model becomes extremely accustomed to the
    training data, thus losing its generalization capability and, when it needs to
    interpret a data sample never seen before, it cannot get a good result.
  prefs: []
  type: TYPE_NORMAL
- en: As the model’s complexity increases, the bias decreases (the model fits the
    training data better) but the variance increases (the model becomes more sensitive
    to the training data). The optimal tradeoff occurs at the point where the error
    is minimized, which in this case is at a moderate level of complexity.
  prefs: []
  type: TYPE_NORMAL
- en: To help understanding, let’s look at a practical example that illustrates the
    concept of bias-variance tradeoff.
  prefs: []
  type: TYPE_NORMAL
- en: The example
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/3ee904742bb7f356b0a0fece88b9a7c2.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Steve Johnson](https://unsplash.com/@steve_j?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: To illustrate the impacts of the bias-variance tradeoff in Machine Learning
    models, let’s see how models with different levels of complexity will perform
    when trained and tested on the same datasets.
  prefs: []
  type: TYPE_NORMAL
- en: For this example, a random dataset with a quadratic relationship between the
    input `X` and the output `y` will be generated. We then split the data into training
    and test sets and fit three polynomial regression models of different degrees
    (1, 2, and 20). We plot the resulting models along with the training and test
    data and calculate the mean squared error for both the training and test sets.
  prefs: []
  type: TYPE_NORMAL
- en: 'The resulting plot shows the bias-variance tradeoff for the different polynomial
    regression models:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2e7ece233b1e3a727a88044cffab99d9.png)'
  prefs: []
  type: TYPE_IMG
- en: Results obtained by the model for different degrees (image by author).
  prefs: []
  type: TYPE_NORMAL
- en: The model with degree = 1 is way too simplistic and has high bias and low variance,
    resulting in underfitting and high errors on both the training and test data.
    The model with degree = 20 is too complex and has low bias and high variance,
    resulting in overfitting and low error on the training data but a high error on
    the test data. The model with degree = 2 has a good balance between bias and variance
    and results in the lowest test error.
  prefs: []
  type: TYPE_NORMAL
- en: This example demonstrates the importance of finding the right level of complexity
    for a machine learning model to balance bias and variance and achieve good generalization
    performance on new, unseen data.
  prefs: []
  type: TYPE_NORMAL
- en: Hopefully, this article was able to help you understand the bias-variance tradeoff
    and how to consider it when developing Machine Learning models.
  prefs: []
  type: TYPE_NORMAL
- en: Any comments and suggestions are more than welcome.
  prefs: []
  type: TYPE_NORMAL
- en: Feel free to reach me on my LinkedIn and to check my GitHub.
  prefs: []
  type: TYPE_NORMAL
- en: '[Linkedin](https://www.linkedin.com/in/alexandre-rosseto-lemos/)'
  prefs: []
  type: TYPE_NORMAL
- en: '[GitHub](https://github.com/alerlemos)'
  prefs: []
  type: TYPE_NORMAL
