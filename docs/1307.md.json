["```py\nfrom sklearn.datasets import fetch_openml\nfrom sklearn.decomposition import PCA\nimport pandas as pd\n\n# Fetch MNIST data\nmnist = fetch_openml('mnist_784', version=1, as_frame=False)\nmnist.target = mnist.target.astype(np.uint8)\n\nX_total = pd.DataFrame(mnist[\"data\"])\ny_total = pd.DataFrame(mnist[\"target\"])\n\nX_reduced = X_total.sample(n=1000)\ny_reduced = y_total.loc[X_total.index]\n\n# PCA to keep 30 components\nX = PCA(n_components=30).fit_transform(X_reduced) \n```", "```py\ndef get_original_pairwise_affinities(X: np.ndarray, perplexity: int = 10) -> np.ndarray:\n    \"\"\"\n    Function to obtain affinities matrix.\n\n    Parameters:\n    X (np.ndarray): The input data array.\n    perplexity (int): The perplexity value for the grid search.\n\n    Returns:\n    np.ndarray: The pairwise affinities matrix.\n    \"\"\"\n\n    n = len(X)\n\n    print(\"Computing Pairwise Affinities....\")\n\n    p_ij = np.zeros(shape=(n, n))\n    for i in range(0, n):\n        # Equation 1 numerator\n        diff = X[i] - X\n        σ_i = grid_search(diff, i, perplexity)  # Grid Search for σ_i\n        norm = np.linalg.norm(diff, axis=1)\n        p_ij[i, :] = np.exp(-(norm**2) / (2 * σ_i**2))\n\n        # Set p = 0 when j = i\n        np.fill_diagonal(p_ij, 0)\n\n        # Equation 1\n        p_ij[i, :] = p_ij[i, :] / np.sum(p_ij[i, :])\n\n    # Set 0 values to minimum numpy value (ε approx. = 0)\n    ε = np.nextafter(0, 1)\n    p_ij = np.maximum(p_ij, ε)\n\n    print(\"Completed Pairwise Affinities Matrix. \\n\")\n\n    return p_ij\n```", "```py\ndef grid_search(diff_i: np.ndarray, i: int, perplexity: int) -> float:\n    \"\"\"\n    Helper function to obtain σ's based on user-specified perplexity.\n\n    Parameters:\n        diff_i (np.ndarray): Array containing the pairwise differences between data points.\n        i (int): Index of the current data point.\n        perplexity (int): User-specified perplexity value.\n\n    Returns:\n        float: The value of σ that satisfies the perplexity condition.\n    \"\"\"\n\n    result = np.inf  # Set first result to be infinity\n\n    norm = np.linalg.norm(diff_i, axis=1)\n    std_norm = np.std(norm)  # Use standard deviation of norms to define search space\n\n    for σ_search in np.linspace(0.01 * std_norm, 5 * std_norm, 200):\n        # Equation 1 Numerator\n        p = np.exp(-(norm**2) / (2 * σ_search**2))\n\n        # Set p = 0 when i = j\n        p[i] = 0\n\n        # Equation 1 (ε -> 0)\n        ε = np.nextafter(0, 1)\n        p_new = np.maximum(p / np.sum(p), ε)\n\n        # Shannon Entropy\n        H = -np.sum(p_new * np.log2(p_new))\n\n        # Get log(perplexity equation) as close to equality\n        if np.abs(np.log(perplexity) - H * np.log(2)) < np.abs(result):\n            result = np.log(perplexity) - H * np.log(2)\n            σ = σ_search\n\n    return σ\n```", "```py\ndef get_symmetric_p_ij(p_ij: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Function to obtain symmetric affinities matrix utilized in t-SNE.\n\n    Parameters:\n    p_ij (np.ndarray): The input affinity matrix.\n\n    Returns:\n    np.ndarray: The symmetric affinities matrix.\n\n    \"\"\"\n    print(\"Computing Symmetric p_ij matrix....\")\n\n    n = len(p_ij)\n    p_ij_symmetric = np.zeros(shape=(n, n))\n    for i in range(0, n):\n        for j in range(0, n):\n            p_ij_symmetric[i, j] = (p_ij[i, j] + p_ij[j, i]) / (2 * n)\n\n    # Set 0 values to minimum numpy value (ε approx. = 0)\n    ε = np.nextafter(0, 1)\n    p_ij_symmetric = np.maximum(p_ij_symmetric, ε)\n\n    print(\"Completed Symmetric p_ij Matrix. \\n\")\n\n    return p_ij_symmetric\n```", "```py\ndef initialization(\n    X: np.ndarray, n_dimensions: int = 2, initialization: str = \"random\"\n) -> np.ndarray:\n    \"\"\"\n    Obtain initial solution for t-SNE either randomly or using PCA.\n\n    Parameters:\n        X (np.ndarray): The input data array.\n        n_dimensions (int): The number of dimensions for the output solution. Default is 2.\n        initialization (str): The initialization method. Can be 'random' or 'PCA'. Default is 'random'.\n\n    Returns:\n        np.ndarray: The initial solution for t-SNE.\n\n    Raises:\n        ValueError: If the initialization method is neither 'random' nor 'PCA'.\n    \"\"\"\n\n    # Sample Initial Solution\n    if initialization == \"random\" or initialization != \"PCA\":\n        y0 = np.random.normal(loc=0, scale=1e-4, size=(len(X), n_dimensions))\n    elif initialization == \"PCA\":\n        X_centered = X - X.mean(axis=0)\n        _, _, Vt = np.linalg.svd(X_centered)\n        y0 = X_centered @ Vt.T[:, :n_dimensions]\n    else:\n        raise ValueError(\"Initialization must be 'random' or 'PCA'\")\n\n    return y0\n```", "```py\ndef get_low_dimensional_affinities(Y: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Obtain low-dimensional affinities.\n\n    Parameters:\n    Y (np.ndarray): The low-dimensional representation of the data points.\n\n    Returns:\n    np.ndarray: The low-dimensional affinities matrix.\n    \"\"\"\n\n    n = len(Y)\n    q_ij = np.zeros(shape=(n, n))\n\n    for i in range(0, n):\n        # Equation 4 Numerator\n        diff = Y[i] - Y\n        norm = np.linalg.norm(diff, axis=1)\n        q_ij[i, :] = (1 + norm**2) ** (-1)\n\n    # Set p = 0 when j = i\n    np.fill_diagonal(q_ij, 0)\n\n    # Equation 4\n    q_ij = q_ij / q_ij.sum()\n\n    # Set 0 values to minimum numpy value (ε approx. = 0)\n    ε = np.nextafter(0, 1)\n    q_ij = np.maximum(q_ij, ε)\n\n    return q_ij\n```", "```py\ndef get_gradient(p_ij: np.ndarray, q_ij: np.ndarray, Y: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Obtain gradient of cost function at current point Y.\n\n    Parameters:\n    p_ij (np.ndarray): The joint probability distribution matrix.\n    q_ij (np.ndarray): The Student's t-distribution matrix.\n    Y (np.ndarray): The current point in the low-dimensional space.\n\n    Returns:\n    np.ndarray: The gradient of the cost function at the current point Y.\n    \"\"\"\n\n    n = len(p_ij)\n\n    # Compute gradient\n    gradient = np.zeros(shape=(n, Y.shape[1]))\n    for i in range(0, n):\n        # Equation 5\n        diff = Y[i] - Y\n        A = np.array([(p_ij[i, :] - q_ij[i, :])])\n        B = np.array([(1 + np.linalg.norm(diff, axis=1)) ** (-1)])\n        C = diff\n        gradient[i] = 4 * np.sum((A * B).T * C, axis=0)\n\n    return gradient\n```", "```py\ndef tsne(\n    X: np.ndarray,\n    perplexity: int = 10,\n    T: int = 1000,\n    η: int = 200,\n    early_exaggeration: int = 4,\n    n_dimensions: int = 2,\n) -> list[np.ndarray, np.ndarray]:\n    \"\"\"\n    t-SNE (t-Distributed Stochastic Neighbor Embedding) algorithm implementation.\n\n    Args:\n        X (np.ndarray): The input data matrix of shape (n_samples, n_features).\n        perplexity (int, optional): The perplexity parameter. Default is 10.\n        T (int, optional): The number of iterations for optimization. Default is 1000.\n        η (int, optional): The learning rate for updating the low-dimensional embeddings. Default is 200.\n        early_exaggeration (int, optional): The factor by which the pairwise affinities are exaggerated\n            during the early iterations of optimization. Default is 4.\n        n_dimensions (int, optional): The number of dimensions of the low-dimensional embeddings. Default is 2.\n\n    Returns:\n        list[np.ndarray, np.ndarray]: A list containing the final low-dimensional embeddings and the history\n            of embeddings at each iteration.\n\n    \"\"\"\n    n = len(X)\n\n    # Get original affinities matrix\n    p_ij = get_original_pairwise_affinities(X, perplexity)\n    p_ij_symmetric = get_symmetric_p_ij(p_ij)\n\n    # Initialization\n    Y = np.zeros(shape=(T, n, n_dimensions))\n    Y_minus1 = np.zeros(shape=(n, n_dimensions))\n    Y[0] = Y_minus1\n    Y1 = initialization(X, n_dimensions)\n    Y[1] = np.array(Y1)\n\n    print(\"Optimizing Low Dimensional Embedding....\")\n    # Optimization\n    for t in range(1, T - 1):\n        # Momentum & Early Exaggeration\n        if t < 250:\n            α = 0.5\n            early_exaggeration = early_exaggeration\n        else:\n            α = 0.8\n            early_exaggeration = 1\n\n        # Get Low Dimensional Affinities\n        q_ij = get_low_dimensional_affinities(Y[t])\n\n        # Get Gradient of Cost Function\n        gradient = get_gradient(early_exaggeration * p_ij_symmetric, q_ij, Y[t])\n\n        # Update Rule\n        Y[t + 1] = Y[t] - η * gradient + α * (Y[t] - Y[t - 1])  # Use negative gradient\n\n        # Compute current value of cost function\n        if t % 50 == 0 or t == 1:\n            cost = np.sum(p_ij_symmetric * np.log(p_ij_symmetric / q_ij))\n            print(f\"Iteration {t}: Value of Cost Function is {cost}\")\n\n    print(\n        f\"Completed Low Dimensional Embedding: Final Value of Cost Function is {np.sum(p_ij_symmetric * np.log(p_ij_symmetric / q_ij))}\"\n    )\n    solution = Y[-1]\n\n    return solution, Y\n```"]