- en: Training a Custom Detectron2 Model for Object Detection Using PDF Documents
    (Part 1)
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 PDF 文档训练自定义 Detectron2 模型进行目标检测（第 1 部分）
- en: 原文：[https://towardsdatascience.com/training-and-deploying-a-custom-detectron2-model-for-object-detection-using-pdf-documents-part-1-c724f61d8b4b?source=collection_archive---------1-----------------------#2023-11-29](https://towardsdatascience.com/training-and-deploying-a-custom-detectron2-model-for-object-detection-using-pdf-documents-part-1-c724f61d8b4b?source=collection_archive---------1-----------------------#2023-11-29)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/training-and-deploying-a-custom-detectron2-model-for-object-detection-using-pdf-documents-part-1-c724f61d8b4b?source=collection_archive---------1-----------------------#2023-11-29](https://towardsdatascience.com/training-and-deploying-a-custom-detectron2-model-for-object-detection-using-pdf-documents-part-1-c724f61d8b4b?source=collection_archive---------1-----------------------#2023-11-29)
- en: Making your machine learn how to see PDFs like a human
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 让你的机器学会像人类一样读取 PDF 文档
- en: '[](https://medium.com/@noahhaglund?source=post_page-----c724f61d8b4b--------------------------------)[![Noah
    Haglund](../Images/edfcc90677444ebced16549a1524d7fe.png)](https://medium.com/@noahhaglund?source=post_page-----c724f61d8b4b--------------------------------)[](https://towardsdatascience.com/?source=post_page-----c724f61d8b4b--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----c724f61d8b4b--------------------------------)
    [Noah Haglund](https://medium.com/@noahhaglund?source=post_page-----c724f61d8b4b--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@noahhaglund?source=post_page-----c724f61d8b4b--------------------------------)[![Noah
    Haglund](../Images/edfcc90677444ebced16549a1524d7fe.png)](https://medium.com/@noahhaglund?source=post_page-----c724f61d8b4b--------------------------------)[](https://towardsdatascience.com/?source=post_page-----c724f61d8b4b--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----c724f61d8b4b--------------------------------)
    [Noah Haglund](https://medium.com/@noahhaglund?source=post_page-----c724f61d8b4b--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fe93013369dfc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftraining-and-deploying-a-custom-detectron2-model-for-object-detection-using-pdf-documents-part-1-c724f61d8b4b&user=Noah+Haglund&userId=e93013369dfc&source=post_page-e93013369dfc----c724f61d8b4b---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----c724f61d8b4b--------------------------------)
    ·11 min read·Nov 29, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fc724f61d8b4b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftraining-and-deploying-a-custom-detectron2-model-for-object-detection-using-pdf-documents-part-1-c724f61d8b4b&user=Noah+Haglund&userId=e93013369dfc&source=-----c724f61d8b4b---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fe93013369dfc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftraining-and-deploying-a-custom-detectron2-model-for-object-detection-using-pdf-documents-part-1-c724f61d8b4b&user=Noah+Haglund&userId=e93013369dfc&source=post_page-e93013369dfc----c724f61d8b4b---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----c724f61d8b4b--------------------------------)
    · 11 分钟阅读 · 2023 年 11 月 29 日 [](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fc724f61d8b4b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftraining-and-deploying-a-custom-detectron2-model-for-object-detection-using-pdf-documents-part-1-c724f61d8b4b&user=Noah+Haglund&userId=e93013369dfc&source=-----c724f61d8b4b---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc724f61d8b4b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftraining-and-deploying-a-custom-detectron2-model-for-object-detection-using-pdf-documents-part-1-c724f61d8b4b&source=-----c724f61d8b4b---------------------bookmark_footer-----------)![](../Images/ae9cddf5c27f7be9a2175d7ecdc2f571.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc724f61d8b4b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftraining-and-deploying-a-custom-detectron2-model-for-object-detection-using-pdf-documents-part-1-c724f61d8b4b&source=-----c724f61d8b4b---------------------bookmark_footer-----------)![](../Images/ae9cddf5c27f7be9a2175d7ecdc2f571.png)'
- en: I have been trying for the better half of a year to solve a business case by
    making PDF documents machine readable, at least in the sense that headers/titles
    (text specifying a section) can be extracted from a document, along with their
    associated contents, thus forming some semblance of a relational data structure.
    Initially, I approached this by using a convolutional neural net (CNN), along
    with the combination of both a CNN and recurrent neural network (RNN), to classify
    the structure of a document using text and textual features (font, font size,
    weight, etc). A framework for this has been implemented by [Rahman & Finin](https://arxiv.org/pdf/1910.03678.pdf)
    and goes beyond inferring document structure by also using Long Short-Term Memory
    (LSTM) for semantic classification of the various divided sections. The problem
    that I faced is that I had neither the time or energy to prepare and annotate
    enough data in order to make accurate models using a similar framework.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 我尝试了大半年的时间解决一个商业案例，通过使 PDF 文档机器可读，至少在头部/标题（指定一个部分的文本）可以从文档中提取出来，以及它们相关的内容，从而形成一些类似关系的数据结构。最初，我采用了卷积神经网络（CNN），以及
    CNN 和递归神经网络（RNN）的结合，来使用文本和文本特征（字体、字体大小、粗细等）对文档结构进行分类。这个框架由 [Rahman & Finin](https://arxiv.org/pdf/1910.03678.pdf)
    实现，并通过使用长短期记忆（LSTM）进行语义分类超越了推断文档结构的能力。我面临的问题是没有足够的时间和精力来准备和标注足够的数据，以便使用类似的框架制作准确的模型。
- en: 'To make this process much easier as a one man team, I turned to computer vision!
    Instead of trying to extract the text and textual features from a PDF and infer
    document structure this way, I would convert PDF pages to images, infer the document
    structure visually via object detection, and then OCR the corresponding inferences
    (ex: a page header) and their associated contents (ex: the header’s contents).
    In this article, I’m going to show you how to do the same using Detectron2!'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 为了作为一个人团队让这个过程变得更简单，我转向了计算机视觉！与其尝试从 PDF 中提取文本和文本特征并推断文档结构，我决定将 PDF 页面转换为图像，通过目标检测可视化地推断文档结构，然后对相应的推断（例如：页面标题）及其相关内容（例如：标题的内容）进行
    OCR。在这篇文章中，我将展示如何使用 Detectron2 来实现相同的目标！
- en: Detectron2 is Facebook AI Research’s next generation library that simplifies
    the process of building computer vision applications. Within the various possible
    use cases for computer vision (image recognition, semantic segmentation, object
    detection, and instance segmentation), this article will be training a custom
    Detectron2 model for object detection, which is easily differentiated from the
    others by its use of bounding boxes. Once trained, we will then dockerize the
    application and deploy it to Heroku/AWS, as well as explore other topics such
    as memory management and batch inference to help you customize your scripts and
    model to your use case.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: Detectron2 是 Facebook AI 研究的下一代库，简化了构建计算机视觉应用程序的过程。在计算机视觉的各种可能用例（图像识别、语义分割、目标检测和实例分割）中，本文将训练一个自定义的
    Detectron2 模型用于目标检测，其特点是使用边界框，易于与其他用例区分。训练完成后，我们将对应用进行 docker 化，并将其部署到 Heroku/AWS，同时探索内存管理和批处理推断等其他主题，以帮助你定制你的脚本和模型以适应你的用例。
- en: 'For following along with this article, it will be helpful to have in advance:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 为了跟随本文，你最好提前准备：
- en: Strong Python Knowledge
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 扎实的 Python 知识
- en: Familiarity with AWS
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 熟悉 AWS
- en: '**Detectron2 Installation**'
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**Detectron2 安装**'
- en: 'If you are a Mac or Linux user, you are in luck! This process will be relatively
    simple by running the following command:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你是 Mac 或 Linux 用户，你真幸运！通过运行以下命令，这个过程将相对简单：
- en: '[PRE0]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Please note that this command will compile the library, so you will need to
    wait a bit. If you want to install Detectron2 with GPU support, please refer to
    the official Detectron2 [installation instruction](https://detectron2.readthedocs.io/en/latest/tutorials/install.html)
    for detailed information.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，这个命令将编译库，所以你需要等待一会儿。如果你想安装支持 GPU 的 Detectron2，请参考官方 Detectron2 [安装说明](https://detectron2.readthedocs.io/en/latest/tutorials/install.html)获取详细信息。
- en: If however you are a Windows user, this process will be a bit of a pain, but
    I was able to manage doing this on Windows myself.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如果你是 Windows 用户，这个过程会有点麻烦，但我自己在 Windows 上也成功完成了。
- en: Follow closely with the instructions [laid out here](https://layout-parser.readthedocs.io/en/latest/notes/installation.html#for-windows-users)
    by the Layout Parser package for Python (which is also a helpful package to use
    if you don’t care about training your own Detectron2 model for PDF structure/content
    inference and want to rely on pre-annotated data! This is certainly more time
    friendly, but you will find that with specific use cases, you can train a much
    more accurate and smaller model on your own, which is good for memory management
    in deployment, as I will discuss later). Ensure you install pycocotools, along
    with Detectron2, as this package will assist in loading, parsing and visualizing
    [COCO](https://cocodataset.org/#home) data, the format we need our data in to
    train a Detectron2 model.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 紧密按照 [这里](https://layout-parser.readthedocs.io/en/latest/notes/installation.html#for-windows-users)
    Layout Parser 包提供的说明进行操作（这是一个很有用的包，如果你不想训练自己的 Detectron2 模型用于 PDF 结构/内容推断而希望依赖预标注数据！这无疑更省时间，但你会发现针对特定用例，你可以在自己的数据上训练一个更准确且更小的模型，这对于部署中的内存管理非常有利，稍后我会讨论）。确保安装
    pycocotools，以及 Detectron2，因为这个包将帮助加载、解析和可视化 [COCO](https://cocodataset.org/#home)
    数据，这是我们训练 Detectron2 模型所需的数据格式。
- en: The local Detectron2 installation will be used in Part 2 of this article series,
    as we will be using an AWS EC2 instance later on in this article for Detectron2
    training.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 本文系列的第2部分将使用本地 Detectron2 安装，我们将在文章的后续部分使用 AWS EC2 实例进行 Detectron2 训练。
- en: '**Detectron2 Custom Training — Annotation using LabelMe**'
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**Detectron2 自定义训练 — 使用 LabelMe 进行标注**'
- en: 'For image annotation, we need two things: (1) the images we will be annotating
    and (2) an annotation tool. Assemble a directory with all the images you want
    to annotate, but if you are following along with my use case and would like to
    use PDF images, assemble a dir of PDFs, install the pdftoimage package:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 对于图像标注，我们需要两个东西：（1）我们将要标注的图像，以及（2）一个标注工具。将所有要标注的图像整理到一个目录中，如果你跟随我的用例并希望使用 PDF
    图像，则整理一个 PDF 目录，并安装 pdftoimage 包：
- en: '[PRE1]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'And then use the following script to convert each PDF page to an image:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 然后使用以下脚本将每个 PDF 页面转换为图像：
- en: '[PRE2]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Once you have a dir of images, we are going to use the LabelMe tool, see installation
    instructions [here](https://github.com/wkentaro/labelme#installation). Once installed,
    just run the command labelme from the command line or a terminal. This will open
    a window with the following layout:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你有了一个图像目录，我们将使用 LabelMe 工具，安装说明请见 [这里](https://github.com/wkentaro/labelme#installation)。安装完成后，只需在命令行或终端中运行
    labelme 命令。这将打开一个具有以下布局的窗口：
- en: '![](../Images/602961b6c75732530f4b0da3152bc93d.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/602961b6c75732530f4b0da3152bc93d.png)'
- en: Click the “Open Dir” option on the left hand side and open the dir where your
    images are saved (and let’s name this dir “train” as well). LabelMe will open
    the first image in the dir and allow you to annotate over each of them. Right
    click the image to find various options for annotations, such as Create Polygons
    to click each point of a polygon around a given object in your image or Create
    Rectangle to capture an object while ensuring 90 degree angles.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 点击左侧的“打开目录”选项，打开保存图像的目录（我们也称这个目录为“train”）。LabelMe 将打开目录中的第一张图像，并允许你对每张图像进行标注。右键单击图像以找到各种标注选项，例如创建多边形以点击图像中给定对象周围的每个点，或创建矩形以捕捉对象，同时确保90度角。
- en: Once the bounding box/polygon has been placed, LabelMe will ask for a label.
    In the example below, I provided the label header for each of the header instances
    found on the page. You can use multiple labels, identifying various objects found
    in an image (for the PDF example this could be Title/Header, Tables, Paragraphs,
    Lists, etc), but for my purpose, I will just be identifying headers/titles and
    then algorithmically associating each header with its respective contents after
    model inferencing (see Part 2).
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦放置了边界框/多边形，LabelMe 会要求输入标签。在下面的示例中，我为页面上的每个标题实例提供了标签头。你可以使用多个标签来识别图像中的各种对象（对于
    PDF 示例，这可能是标题/头部、表格、段落、列表等），但出于我的目的，我将仅识别标题/头部，然后在模型推断后通过算法将每个标题与其相应的内容关联起来（见第2部分）。
- en: '![](../Images/e10b5f7e2ac5c5de670c867b7fde0d81.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e10b5f7e2ac5c5de670c867b7fde0d81.png)'
- en: Once labeled, click the Save button and then click Next Image to annotate the
    next image in the given dir. Detectron2 is excellent at detecting inferences with
    minimal data, so feel free to annotate up to about 100 images for initial training
    and testing, and then annotate and train further to increase the model’s accuracy
    (keep in mind that training a model on more than one label category will decrease
    the accuracy a bit, requiring a larger dataset for improved accuracy).
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 注释完成后，点击保存按钮，然后点击下一张图片以注释给定目录中的下一张图片。Detectron2 在使用最少的数据进行推断时表现出色，因此可以随意注释大约
    100 张图片进行初步训练和测试，然后再进一步注释和训练以提高模型的准确性（请记住，对多个标签类别进行训练会降低一点准确性，需要更大的数据集来提高准确性）。
- en: Once each image in the train dir has been annotated, let’s take about 20% of
    these image/annotation pairs and move them to a separate dir labeled test.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦训练目录中的每张图片都被注释过，我们就可以将这些图片/注释对的约 20% 移动到一个单独的名为 test 的目录中。
- en: If you are familiar with Machine Learning, a simple rule of thumb is that there
    needs to be a test/train/validation split (60–80% training data, 10–20% validation
    data, and 10–20% test data). For this purpose, we are just going to do a test/train
    split that is 20% test and 80% train.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你熟悉机器学习，一个简单的经验法则是需要有测试/训练/验证的拆分（60–80% 的训练数据，10–20% 的验证数据，以及 10–20% 的测试数据）。在这里，我们只是做一个测试/训练的拆分，即
    20% 的测试和 80% 的训练。
- en: '**Detectron2 Custom Training — COCO Format**'
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**Detectron2 自定义训练 — COCO 格式**'
- en: Now that we have our folders of annotations, we need to convert the labelme
    annotations to COCO format. You can do that simply with the labelme2coco.py file
    in [the repo I have here](https://github.com/nzh2534/detectron2tutorial/tree/main).
    I refactored this script from [Tony607](https://github.com/Tony607/labelme2coco/tree/master)
    which will convert both the polygram annotations *and* any rectangle annotations
    that were made (as the initial script didn’t properly convert the rectangle annotations
    to COCO format).
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了注释的文件夹，我们需要将 labelme 注释转换为 COCO 格式。你可以通过 [我这里的这个仓库](https://github.com/nzh2534/detectron2tutorial/tree/main)
    中的 labelme2coco.py 文件来简单完成这个操作。我从 [Tony607](https://github.com/Tony607/labelme2coco/tree/master)
    重构了这个脚本，它将转换多边形注释 *和* 任何矩形注释（因为初始脚本没有正确转换矩形注释到 COCO 格式）。
- en: 'Once you download the labelme2coco.py file, run it in the terminal with the
    command:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦下载了 labelme2coco.py 文件，通过在终端运行以下命令来执行它：
- en: '[PRE3]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: and it will output a train.json file. Run the command a second time for the
    test folder and edit line 172 in labelme2coco.py to change the default output
    name to test.json (otherwise it will overwrite the train.json file).
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 它会输出一个 train.json 文件。对于测试文件夹，再次运行命令，并编辑 labelme2coco.py 的第 172 行，将默认输出名称更改为
    test.json（否则它会覆盖 train.json 文件）。
- en: '**Detectron2 Custom Training — EC2**'
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**Detectron2 自定义训练 — EC2**'
- en: Now that the tedious process of annotation is over, we can get to the fun part,
    training!
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 既然繁琐的注释过程已经结束，我们可以进入有趣的部分——训练！
- en: If your computer doesn’t come with Nvidia GPU capabilities, we will need to
    spin up an EC2 instance using AWS. The Detectron2 model can be trained on the
    CPU, but if you try this, you will notice that it will take an extremely long
    time, whereas using [Nvidia CUDA](https://developer.nvidia.com/cuda-toolkit) on
    a GPU based instance would train the model in a matter of minutes.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你的计算机没有 Nvidia GPU 功能，我们需要使用 AWS 启动一个 EC2 实例。Detectron2 模型可以在 CPU 上进行训练，但如果你这样做，你会发现这将花费非常长的时间，而使用
    [Nvidia CUDA](https://developer.nvidia.com/cuda-toolkit) 的 GPU 实例则可以在几分钟内完成模型训练。
- en: To start, sign into the [AWS console](https://aws.amazon.com/console/). Once
    signed in, search EC2 in the search bar to go to the EC2 dashboard. From here,
    click Instances on the left side of the screen and then click the Launch Instances
    button
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，登录到 [AWS 控制台](https://aws.amazon.com/console/)。登录后，在搜索栏中搜索 EC2 以进入 EC2 仪表板。在这里，点击屏幕左侧的实例，然后点击启动实例按钮。
- en: '![](../Images/ad3c3b76c2e8bfcd9367144dafe96ac0.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ad3c3b76c2e8bfcd9367144dafe96ac0.png)'
- en: 'The bare minimum level of detail you will need to provide for the instance
    is:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要为实例提供的最低详细信息是：
- en: '**A Name**'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**一个名字**'
- en: '**The Amazon Machine Image (AMI)** which specifies the software configuration.
    Make sure to use one with GPU and PyTorch capabilities, as it will have the packages
    needed for CUDA and additional dependencies needed for Detectron2, such as Torch.
    To follow along with this tutorial, also use an Ubuntu AMI. I used the AMI — Deep
    Learning AMI GPU PyTorch 2.1.0 (Ubuntu 20.04).'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Amazon 机器镜像（AMI）** 指定了软件配置。确保使用一个具有 GPU 和 PyTorch 功能的 AMI，因为它将包含 CUDA 所需的包和
    Detectron2 额外依赖项，如 Torch。为了跟随本教程，也使用 Ubuntu AMI。我使用了 AMI —— Deep Learning AMI
    GPU PyTorch 2.1.0（Ubuntu 20.04）。'
- en: '**The Instance type** which specifies the hardware configuration. Check out
    a guide [here](https://aws.amazon.com/ec2/instance-types/) on the various instance
    types for your reference. We want to use a performance optimized instance, such
    as one from the P or G instance families. I used p3.2xlarge which comes with all
    the computing power, and more specifically GPU capabilities, that we will need.'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**实例类型** 指定了硬件配置。请参考 [这里](https://aws.amazon.com/ec2/instance-types/) 的指南了解各种实例类型。我们希望使用性能优化的实例，例如
    P 或 G 实例系列中的一个。我使用了 p3.2xlarge，它提供了我们所需的所有计算能力，特别是 GPU 能力。'
- en: 'PLEASE NOTE: instances from the P family will require you to contact AWS customer
    service for a quota increase (as they don’t immediately allow base users to access
    higher performing instances due to the cost associated). If you use the p3.2xlarge
    instance, you will need to request a quota increase to 8 vCPU.'
  id: totrans-48
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 请注意：P 系列的实例需要您联系 AWS 客户服务以申请配额增加（因为由于相关费用，它们不会立即允许基本用户访问性能更高的实例）。如果您使用 p3.2xlarge
    实例，您需要申请将配额增加到 8 vCPU。
- en: Specify a **Key pair (login).** Create this if you don’t already have oneand
    feel free to name it p3key as I did.
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 指定一个 **密钥对（登录）。** 如果您尚未创建此密钥对，请创建一个，并可以随意将其命名为 p3key，正如我所做的那样。
- en: '![](../Images/e2389169d19bea071a94db6806fc3986.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e2389169d19bea071a94db6806fc3986.png)'
- en: Finally, **Configure Storage.** If you used the same AMI and Instance type as
    I, you will see a starting default storage of 45gb. Feel free to up this to around
    60gb or more as needed, depending on your training dataset size in order to ensure
    the instance has enough space for your images.
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，**配置存储。** 如果您使用了与我相同的 AMI 和实例类型，您将看到起始默认存储为 45gb。根据您的训练数据集大小，可以将其增加到约 60gb
    或更多，以确保实例有足够的空间存储您的图像。
- en: 'Go ahead and launch your instance and click the instance id hyperlink to view
    it in the EC2 dashboard. When the instance is running, open a Command Prompt window
    and we will SSH into the EC2 instance using the following command (and make sure
    to replace the bold text with (1) the path to your .pem Key Pair and (2) the address
    for your EC2 instance):'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 启动您的实例并点击实例 ID 超链接以在 EC2 仪表板中查看它。当实例运行时，打开命令提示符窗口，我们将使用以下命令通过 SSH 连接到 EC2 实例（并确保将粗体文本替换为
    (1) 您的 .pem 密钥对的路径和 (2) 您的 EC2 实例的地址）：
- en: '[PRE4]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'As this is a new host, say yes to the following message:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这是一个新主机，请对以下消息回答“是”：
- en: '![](../Images/1bf5b52c248ee53ee2c20a84d7135356.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1bf5b52c248ee53ee2c20a84d7135356.png)'
- en: 'And then Ubuntu will start along with a prepackaged virtual environment called
    PyTorch (from the AWS AMI). Activate the venv and start up a preinstalled jupyter
    notebook using the following two commands:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 然后 Ubuntu 将启动，并带有一个名为 PyTorch（来自 AWS AMI）的预打包虚拟环境。激活 venv 并使用以下两个命令启动预安装的 Jupyter
    Notebook：
- en: '![](../Images/8302201c5df6ed23bbf6e20081fabdfe.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8302201c5df6ed23bbf6e20081fabdfe.png)'
- en: 'This will return URLs for you to copy and paste into your browser. Copy the
    one with localhost into your browser and change 8888 to 8000\. This will take
    you to a Jupyter Notebook that looks similar to this:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 这将返回供您复制并粘贴到浏览器中的 URL。将包含 localhost 的 URL 复制到浏览器中，并将 8888 改为 8000。这样会带您到一个看起来类似于此的
    Jupyter Notebook：
- en: '![](../Images/ffe5c78684dddb1228cf7479ae0717ea.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ffe5c78684dddb1228cf7479ae0717ea.png)'
- en: From [my github repo](https://github.com/nzh2534/detectron2tutorial/blob/main/Detectron2__Tutorial.ipynb),
    upload the Detectron2_Tutorial.ipynb file into the notebook. From here, run the
    lines under the Installation header to fully install Detectron2\. Then, restart
    the runtime to make sure the installation took effect.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 从 [我的 GitHub 仓库](https://github.com/nzh2534/detectron2tutorial/blob/main/Detectron2__Tutorial.ipynb)
    中，将 Detectron2_Tutorial.ipynb 文件上传到笔记本中。从这里，运行安装头下的命令以完全安装 Detectron2。然后，重启运行时以确保安装生效。
- en: 'Once back into the restarted notebook, we need to upload some additional files
    before beginning the training process:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 返回到重启的笔记本中后，我们需要上传一些额外的文件，然后才能开始训练过程：
- en: The utils.py file from the [github repo](https://github.com/nzh2534/detectron2tutorial/blob/main/utils.py).
    This provides the .ipynb files with configuration details for Detectron2 (see
    documentation [here](https://detectron2.readthedocs.io/en/latest/modules/config.html)
    for reference if you’re interested on configuration specifics). Also included
    in this file is a *plot_samples* function that is referenced in the .ipynb file,
    **but has been commented out in both**. You can uncomment and use this to plot
    the training data if you’d like to see visuals of the samples during the process.
    Please note that you will need to further install cv2 to use the plot_samples
    feature.
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 来自 [github repo](https://github.com/nzh2534/detectron2tutorial/blob/main/utils.py)
    的 utils.py 文件。此文件为 Detectron2 提供了包含配置详细信息的 .ipynb 文件（如果你对配置细节感兴趣，请参阅文档 [这里](https://detectron2.readthedocs.io/en/latest/modules/config.html)）。该文件中还包含一个
    *plot_samples* 函数，在 .ipynb 文件中有引用，但**在两个文件中都被注释掉了**。如果你希望在训练过程中查看样本的可视化，可以取消注释并使用此函数。请注意，你需要进一步安装
    cv2 才能使用 plot_samples 功能。
- en: Both the train.json and test.json files that were made using the labelme2coco.py
    script.
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 labelme2coco.py 脚本生成的 train.json 和 test.json 文件。
- en: 'A zip file of both the Train images dir and Test images dir (zipping the dirs
    allows you to only upload one item to the notebook; you can keep the labelme annotation
    files in the dir, this won’t affect the training). Once both of these zip files
    have been uploaded, open a terminal in the notebook by clicking (1) New and then
    (2) Terminal on the top right hand side of the notebook and use the following
    commands to unzip each of the files, creating a separate Train and Test dir of
    images in the notebook:'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个包含 Train 图像目录和 Test 图像目录的 zip 文件（压缩目录允许你只上传一个项目到笔记本中；你可以将 labelme 注释文件保留在目录中，这不会影响训练）。上传这两个
    zip 文件后，通过点击笔记本右上角的 (1) New 和 (2) Terminal 打开一个终端，并使用以下命令解压每个文件，在笔记本中创建一个单独的 Train
    和 Test 图像目录：
- en: '[PRE5]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Finally, run the notebook cells under the Training section in the .ipynb file.
    The last cell will output responses similar to the following:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，运行 .ipynb 文件中 Training 部分的笔记本单元。最后一个单元将输出类似于以下的响应：
- en: '![](../Images/309393f75f12959eba0a6bbc4f9434e5.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/309393f75f12959eba0a6bbc4f9434e5.png)'
- en: This will show the amount of images being used for training, as well as the
    count of instances that you had annotated in the training dataset (here, 470 instances
    of the “title” category, were found prior to training). Detectron2 then serializes
    the data and loads the data in batches as specified in the configurations (utils.py).
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 这将显示用于训练的图像数量，以及你在训练数据集中标注的实例计数（这里，在训练之前找到“title”类别的 470 个实例）。然后，Detectron2
    将数据序列化，并按配置中指定的批次加载数据（utils.py）。
- en: 'Once training begins, you will see Detectron2 printing events:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦训练开始，你将看到 Detectron2 打印事件：
- en: '![](../Images/8b88dbef3bd6f86760a4aac08ddea6f4.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8b88dbef3bd6f86760a4aac08ddea6f4.png)'
- en: 'This lets you know information such as: the estimated training time left, the
    number of iterations performed by Detectron2, and most importantly to monitor
    accuracy, the total_loss, which is an index of the other loss calculations, indicating
    how bad the model’s prediction was on a single example. If the model’s prediction
    is perfect, the loss is zero; otherwise, the loss is greater. Don’t fret if the
    model isn’t perfect! We can always add in more annotated data to improve the model’s
    accuracy or use the final trained model’s inferences that have a high score (indicating
    how confident the model is that an inference is accurate) in our application.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 这让你了解以下信息：估计剩余的训练时间、Detectron2 已执行的迭代次数，以及最重要的监控准确度的 total_loss，它是其他损失计算的指标，表示模型在单个示例上的预测有多差。如果模型的预测是完美的，则损失为零；否则，损失会更大。如果模型不完美也不要担心！我们可以随时添加更多标注数据来提高模型的准确度，或者在我们的应用中使用最终训练好的模型的高分推断（表示模型对推断准确性的信心）。
- en: Once completed, a dir called output will be created in the notebook with a sub
    dir, object detection, that contains files related to the training events and
    metrics, a file that records a checkpoint for the model, and lastly a .pth file
    titled model_final.pth. This is the saved and trained Detectron2 model that can
    now be used to make inferences in a deployed application! Make sure to download
    this before shutting down or terminating the AWS EC2 instance.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 完成后，在笔记本中会创建一个名为 output 的目录，其中包含一个名为 object detection 的子目录，该子目录包含与训练事件和指标相关的文件、记录模型检查点的文件，以及一个名为
    model_final.pth 的 .pth 文件。这是已经保存和训练好的 Detectron2 模型，现在可以用于在部署的应用程序中进行推断！在关闭或终止
    AWS EC2 实例之前，请确保下载此文件。
- en: 'Now that we have the model_final.pth, follow along for a *Part 2: Design and
    Deployment* article that will cover the deployment process of an application that
    uses Machine Learning, with some keys tips on how to make this process efficient.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经有了 model_final.pth，请继续阅读 *第2部分：设计与部署* 文章，该文章将涵盖使用机器学习的应用程序的部署过程，并提供一些关键提示，帮助提高这一过程的效率。
- en: '*Unless otherwise noted, all images used in this article are by the author*'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '*除非另有说明，本文中使用的所有图片均为作者所用*'
