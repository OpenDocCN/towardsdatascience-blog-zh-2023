- en: Class Imbalance Strategies — A Visual Guide with Code
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/class-imbalance-strategies-a-visual-guide-with-code-8bc8fae71e1a?source=collection_archive---------2-----------------------#2023-04-24](https://towardsdatascience.com/class-imbalance-strategies-a-visual-guide-with-code-8bc8fae71e1a?source=collection_archive---------2-----------------------#2023-04-24)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Understand Random Undersampling, Oversampling, SMOTE, ADASYN, and Tomek Links
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://travis-tang.medium.com/?source=post_page-----8bc8fae71e1a--------------------------------)[![Travis
    Tang](../Images/8372ea73b8cf8fe344de6274b5d9ad17.png)](https://travis-tang.medium.com/?source=post_page-----8bc8fae71e1a--------------------------------)[](https://towardsdatascience.com/?source=post_page-----8bc8fae71e1a--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----8bc8fae71e1a--------------------------------)
    [Travis Tang](https://travis-tang.medium.com/?source=post_page-----8bc8fae71e1a--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F169b6a57c01e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fclass-imbalance-strategies-a-visual-guide-with-code-8bc8fae71e1a&user=Travis+Tang&userId=169b6a57c01e&source=post_page-169b6a57c01e----8bc8fae71e1a---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----8bc8fae71e1a--------------------------------)
    ·13 min read·Apr 24, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F8bc8fae71e1a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fclass-imbalance-strategies-a-visual-guide-with-code-8bc8fae71e1a&user=Travis+Tang&userId=169b6a57c01e&source=-----8bc8fae71e1a---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F8bc8fae71e1a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fclass-imbalance-strategies-a-visual-guide-with-code-8bc8fae71e1a&source=-----8bc8fae71e1a---------------------bookmark_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: Class imbalance occurs when one class in a classification problem significantly
    outweighs the other class. It’s common in many machine learning problems. Examples
    include fraud detection, anomaly detection, and medical diagnosis.
  prefs: []
  type: TYPE_NORMAL
- en: The Curse of Class Imbalance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A model trained on an imbalanced dataset perform poorly on the minority class.
    At best, this can cause loss to the business in the case of a churn analysis.
    At worst, it can pervade systemic bias of a face recognition system.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7e0577b405b22eaa703f8b6695043510.png)'
  prefs: []
  type: TYPE_IMG
- en: 'A balanced dataset might just be the missing ingredient (Source:'
  prefs: []
  type: TYPE_NORMAL
- en: Elena Mozhvilo on [Unsplash](https://unsplash.com/photos/j06gLuKK0GM))
  prefs: []
  type: TYPE_NORMAL
- en: The common approach to class imbalance is resampling. These can entail oversampling
    the majority class, undersampling the minority class, or a combination of both.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this post, I use vivid visuals and code to illustrate these strategies for
    class imbalance:'
  prefs: []
  type: TYPE_NORMAL
- en: Random oversampling
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Random undersampling
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Oversampling with SMOTE
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Oversampling with ADASYN
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Undersampling with Tomek Link
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Oversampling with SMOTE, then undersample with TOMEK Link (SMOTE-Tomek)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: I will also be using these strategies on a real-world dataset, and evaluate
    their impact on a machine learning model. Let’s go.
  prefs: []
  type: TYPE_NORMAL
- en: All source code is here.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Using Imbalance-learn
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will use the `imbalanced-learn` package in python to solve our imbalanced
    class problem. It is an open-sourced library relying on scikit-learn and provides
    tools when dealing with classification with imbalanced classes.
  prefs: []
  type: TYPE_NORMAL
- en: To install it, use the command.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The dataset that we are using is the [**Communities and Crime Data Set by UCI
    (CC BY 4.0)**.](https://archive.ics.uci.edu/ml/datasets/communities+and+crime)
    It contains 100 attributes of 1994 U.S. communities. We can use this to predict
    if the **crime rate is high** (defined as having **per capita violent crime**
    above 0.65). The data source is available in the UCI Machine Learning Repository
    and is created by Michael Redmond from La Salle University (Published in 2009).
  prefs: []
  type: TYPE_NORMAL
- en: The variables included in the dataset involve the community, such as the percent
    of the population considered urban, and the median family income, and involving
    law enforcement, such as per capita number of police officers, and percent of
    officers assigned to drug units.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: This dataset is imbalanced. It has 12 communities with low crime rates for every
    1 community of high crime rate. This is perfect to illustrate our use case.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: We will convert this dictionary to a Pandas dataframe, then split it into train-test
    splits.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Note that we will only perform under- and over-sampling only on the train dataset.
    We will *not* change the test sets with under- and over-sampling.
  prefs: []
  type: TYPE_NORMAL
- en: Preprocessing the dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Our goal is to have a visualize an imbalanced dataset. In order to visualize
    the 128-dimensional dataset in a 2D graph, we do the following on the train set.
  prefs: []
  type: TYPE_NORMAL
- en: scale the dataset,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: perform Principle Component Analysis (PCA) on the features to convert the 100
    features to 2 principle components,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: visualize the data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Here’s the data, visualized in 2D.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/86931b188116c9c87693d8ef51c041c2.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: 'Code for the above graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: With the preprocessing done, we are ready to resample our dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Strategy 1\. Random Oversampling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Random oversampling duplicates existing examples from the minority class with
    replacement. Each data point in the minority class has an equal probability of
    being duplicated.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ceecf5a6b76f1149935c8efbbb4780d8.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: Here’s how to we can perform oversampling on our dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Let’s compare the data before (left) and after (right) random oversampling.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e8d5473774d60edf316cace663f86b4a.png)'
  prefs: []
  type: TYPE_IMG
- en: Code for plotting in Github. Image by author
  prefs: []
  type: TYPE_NORMAL
- en: The only difference? After random oversampling, there are more **overlapping
    data points in the minority class**. As a result, the data points of the minority
    class appear darker.
  prefs: []
  type: TYPE_NORMAL
- en: '**Strategy 2\. Random Undersampling**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Conversely, random undersampling removes existing samples from the majority
    class. Each data point in the majority class has an equal chance of being removed.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8eeea9953f7b07db8e240e34bbf4b009.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: We can do this with the following code.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Let’s compare the data before (left) and after (right) random undersampling.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/783b6e0d87a1ebec3168b582b8d21975.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: After undersampling, the overall number of data points decreased significantly.
    That’s because the data points in the majority class are removed at random until
    the classes are balanced.
  prefs: []
  type: TYPE_NORMAL
- en: Applying machine learning to under- and over-sampled sets
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s compare the performance of a classification machine learning model (SVM
    model) trained on three datasets above (unmodified, under- dataset, and over-sampled
    dataset)
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, we train three Support Vector Machine Classifiers (SVC) on three datasets:'
  prefs: []
  type: TYPE_NORMAL
- en: Original data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Randomly over-sampled data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: randomly under-sampled data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Then, we can visualize what each SVC has learnt from the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c790f8a28ebcf0e5a45dbd055b11cc7f.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: 'The graphs above summarize what the algorithms have learnt from the dataset.
    In particular, they have learned that:'
  prefs: []
  type: TYPE_NORMAL
- en: A new point falls that falls into the **yellow** region is predicted as a **yellow**
    point (‘*High crime rate community*’)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A new point falls that falls into the **purple** region is predicted as a **purple**
    point (‘*Low crime rate community*’)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here are some observations:'
  prefs: []
  type: TYPE_NORMAL
- en: The SVC trained on the *original* dataset is… quite useless. It essentially
    predicts all communities as purple. It learns to ignore all yellow points.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The SVCs trained on oversampled and undersampled datasets are less biased. They
    are less likely to call misclassfy the minority class.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The decision boundaries of SVCs trained on over-sampled and under-sampled dataset
    differ.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using ROC to evaluate resampled models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To evaluate which SVC is the best, we will have to evaluate the performance
    of the SVCs on a test set. The metric that we will use is the receiver operating
    curve (ROC) to find the area under curve (AUC). Please search (Cmd+F) for “**Appendix
    1”** for an introduction to ROC.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/efe1e7a50daeb633bc071de887eb0593.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: The SVC trained on the original data performed poorly. It did worse than if
    we were to randomly guess the output.
  prefs: []
  type: TYPE_NORMAL
- en: The randomly oversampled dataset outperformed the under-sampled dataset. One
    possible reason is that there is a loss of information from removing data points
    from the undersampling procedure. Conversely, no information is lost from oversampling
    the data.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have an understanding of oversampling and undersampling, let’s delve
    deeper into oversampling and undersampling techniques.
  prefs: []
  type: TYPE_NORMAL
- en: Strategy 3\. Oversampling with SMOTE
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: SMOTE is a method of oversampling. Intuitively, SMOTE creates synthetic data
    points by interpolating between the minority data points that are close by to
    one another.
  prefs: []
  type: TYPE_NORMAL
- en: Here’s how SMOTE works (simplified).
  prefs: []
  type: TYPE_NORMAL
- en: Randomly select some data points in the minority class.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For every selected point, identify its *k* nearest neighbour(s).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For every neighbor, add a new point somewhere between the data point and the
    neighbor.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeat steps 2 to 4 until sufficient synthetic data points are created.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Please search (Cmd+F) for “Appendix 2” for the exact algorithm of SMOTE in
    the words of its creator.*'
  prefs: []
  type: TYPE_NORMAL
- en: Here’s a visualization.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s oversample our dataset with SMOTE and train an SVC on it.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Here’s the result.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/93afb26c0ba1103f7dee31647bb1d0ea.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: Strategy 4\. Oversampling with ADASYN (+ How it’s different from SMOTE)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'ADASYN is a cousin of SMOTE: both SMOTE and ADASYN generate new samples by
    interpolation.'
  prefs: []
  type: TYPE_NORMAL
- en: But there’s on critical difference. ADASYN generates samples next to the original
    samples that are wrongly classified by a KNN classifier. Conversely, SMOTE differentiates
    between samples that are correctly or wrongly classified by the KNN classifier.
  prefs: []
  type: TYPE_NORMAL
- en: Here’s a visualization of how ADASYN works.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s oversample our dataset with ADASYN and train an SVC on it.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Let’s compare SMOTE, ADASYN, and the original dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b0508f63d53bdbdac8559a5ced5afb8c.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: Here are a few observations.
  prefs: []
  type: TYPE_NORMAL
- en: First, both over-sampling approaches cause more synthetic data points to be
    created *in between* original data points. That’s because both SMOTE and ADASYN
    use *interpolation* to create new data points.
  prefs: []
  type: TYPE_NORMAL
- en: Second, comparing SMOTE and ADASYN, we notice ADASYN creates data points for
    minority (*yellow*) points near the majority (*purple*) data points.
  prefs: []
  type: TYPE_NORMAL
- en: Comparing the regions circled in blue above, ADASYN created *fewer* yellow data
    points in regions with only a few purple data points.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Comparing the regions circled in brown above, ADASYN created *more* yellow data
    points in regions with more purple data points.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s compare the ROC of all over-sampling methods we have described so far.
    In this example, they perform equally well.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3efbe5c0f92b3f979af1a495623abe81.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: Strategy 5\. Under-sampling with Tomek Links
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A tomek link is a pair of points that are very close to one another but are
    of different classes. *Tomek Link’s mathematical definition can be found in the
    Appendix 3.*
  prefs: []
  type: TYPE_NORMAL
- en: Here’s a visualization.
  prefs: []
  type: TYPE_NORMAL
- en: To under-sample with Tomek Links, we will identify all Tomek Links in the data
    set. For each pair of data point in the Tomek Link, we will remove the majority
    class.
  prefs: []
  type: TYPE_NORMAL
- en: Here’s an animation that illustrates undersampling with Tomek Link.
  prefs: []
  type: TYPE_NORMAL
- en: We will apply Tomek Link undersampling to our dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Now let’s compare Tomek undersampling with random undersampling.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a0d832d1141b4bd1673e53d1bef2c71f.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: In our particular dataset, removing Tomek Link did little to ease the class
    imbalance. This is because there are limited number of Tomek Links in the dataaset.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s see how the performance of undersampling with Tomek Link differs from
    that of random undersampling.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b45b058004a2d9c4af8eec5d33d36a39.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: We observe that **random undersampling did better than Tomek Link undersampling.**
    This is because Tomek Link did not remove the class imbalance completely like
    random undersampling did.
  prefs: []
  type: TYPE_NORMAL
- en: 'Strategy 6\. SMOTEK: Oversample with SMOTE, then Undersample with Tomek Links'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we have learnt about oversampling and undersampling. Can we combine
    these techniques?
  prefs: []
  type: TYPE_NORMAL
- en: Of course! **SMOTE-TOMEK** is a technique that combines oversampling (SMOTE)
    with undersampling (with Tomek Links).
  prefs: []
  type: TYPE_NORMAL
- en: We will apply it to our dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Let’s compare SMOTE, Tomek, and SMOTE-Tomek.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/798077ce2d76cec375453ddcf4493c0a.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: Comparing SMOTE-Tomek with SMOTE Only, we see the difference being circled in
    brown. SMOTE-Tomek removes the points that are close to the boundary.
  prefs: []
  type: TYPE_NORMAL
- en: For the grand finale, we will compare all the techniques that we have described
    above. Viola, SMOTE-TOMEK performed the best.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1bf9ed2d4e34e8377859560edbbfc7c3.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: Closing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Overall, you can use oversampling, undersampling or a combination of both to
    deal with data imbalance. If you have the computational resources, it is often
    better to use a combination of over- and under-sampling; Oversampling is a good
    strategy when you have few datapoints; while undersampling is good when there
    are potentially many similar data points.
  prefs: []
  type: TYPE_NORMAL
- en: Dealing with imbalance dataset is not easy. I would encourage you to explore
    many other resampling strategies (including the different [undersampling methods](https://imbalanced-learn.org/dev/references/under_sampling.html)
    and [oversampling methods](https://imbalanced-learn.org/dev/references/over_sampling.html))
    to see which strategy performs the best on your dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Also, measuring the performance of imbalance dataset can be tricky. Make sure
    you use the right classification metrics. Luckily, metrics like [ROC Curve](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.RocCurveDisplay.html),
    [F1 score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html)
    and [geometric mean scores](https://imbalanced-learn.org/dev/references/metrics.html)
    are already available to us.
  prefs: []
  type: TYPE_NORMAL
- en: I am Travis Tang. I post data science content on LinkedIn and Medium. Follow
    me for more :)
  prefs: []
  type: TYPE_NORMAL
- en: Appendix
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Appendix 1\. Using ROC to evaluate models in class imbalance problems
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: ROC is insensitive to class imbalance, making it a great tool to evaluate models
    with class imbalance. It does not depend on the class prevalence. This is in contrast
    to evaluation metrics such as accuracy, which can be misleading in the presence
    of class imbalance.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ea7535e53d252dd13478c5d82f190260.png)'
  prefs: []
  type: TYPE_IMG
- en: Drawn by CMG Lee based on [http://commons.wikimedia.org/wiki/File:roc-draft-xkcd-style.svg](https://commons.wikimedia.org/wiki/File:roc-draft-xkcd-style.svg)
    .
  prefs: []
  type: TYPE_NORMAL
- en: An ROC curve plots the true positive rate (TPR) on the y-axis against the false
    positive rate (FPR) on the x-axis for all possible classification thresholds.
    The TPR is the proportion of positive instances that are correctly classified
    as positive, and the FPR is the proportion of negative instances that are incorrectly
    classified as positive.
  prefs: []
  type: TYPE_NORMAL
- en: A model with good performance will have a ROC curve that is closer to the top-left
    corner of the plot, as this indicates a higher TPR and a lower FPR. A model makes
    completely random guesses will fall on the line with TPR = FPR.
  prefs: []
  type: TYPE_NORMAL
- en: Appendix 2\. The exact algorithm of SMOTE algorithm
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The minority class is over-sampled by taking each minority class sample and
    introducing synthetic examples along the line segments joining any/all of the
    k minority class nearest neighbors. Depending upon the amount of over-sampling
    required, neighbors from the k nearest neighbors are randomly chosen. Our implementation
    currently uses five nearest neighbors. For instance, if the amount of over-sampling
    needed is 200%, only two neighbors from the five nearest neighbors are chosen
    and one sample is generated in the direction of each. Synthetic samples are generated
    in the following way: Take the difference between the feature vector (sample)
    under consideration and its nearest neighbor. Multiply this difference by a random
    number between 0 and 1, and add it to the feature vector under consideration.
    This causes the selection of a random point along the line segment between two
    specific features. This approach effectively forces the decision region of the
    minority class to become more general. [2]'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Appendix 3\. Definition of **Tomek Links**
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Given two examples Ei and Ej belonging to diﬀerent classes, and d(Ei, Ej) is
    the distance between Ei and Ej. A (Ei, Ej) pair is called a Tomek link if there
    is not an example El, such that d(Ei, El)< d(Ei, Ej) or d(Ej, El)< d(Ei, Ej).
    *[1]*
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[1] Batista, Gustavo EAPA, Ronaldo C. Prati, and Maria Carolina Monard. “[A
    Study of the Behavior of Several Methods for Balancing Machine Learning Training
    Data](https://www.researchgate.net/publication/220520041_A_Study_of_the_Behavior_of_Several_Methods_for_Balancing_machine_Learning_Training_Data)”
    *ACM SIGKDD explorations newsletter* 6.1 (2004): 20–29.'
  prefs: []
  type: TYPE_NORMAL
- en: '[2] Chawla, Nitesh V., et al. “[SMOTE: synthetic minority over-sampling technique.](https://dl.acm.org/doi/10.5555/1622407.1622416)”
    *Journal of artificial intelligence research* 16 (2002): 321–357.'
  prefs: []
  type: TYPE_NORMAL
