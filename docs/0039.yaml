- en: The 1958 Perceptron as a tumour classifier
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/the-1958-perceptron-as-a-breast-cancer-classifier-672556186156?source=collection_archive---------18-----------------------#2023-01-03](https://towardsdatascience.com/the-1958-perceptron-as-a-breast-cancer-classifier-672556186156?source=collection_archive---------18-----------------------#2023-01-03)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: A practical example implemented in Mathematica
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@m.emmanuel?source=post_page-----672556186156--------------------------------)[![Mario
    Emmanuel](../Images/e6461a139937d40d09ba6af7078b3b8e.png)](https://medium.com/@m.emmanuel?source=post_page-----672556186156--------------------------------)[](https://towardsdatascience.com/?source=post_page-----672556186156--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----672556186156--------------------------------)
    [Mario Emmanuel](https://medium.com/@m.emmanuel?source=post_page-----672556186156--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F7728df51142e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-1958-perceptron-as-a-breast-cancer-classifier-672556186156&user=Mario+Emmanuel&userId=7728df51142e&source=post_page-7728df51142e----672556186156---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----672556186156--------------------------------)
    ·14 min read·Jan 3, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F672556186156&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-1958-perceptron-as-a-breast-cancer-classifier-672556186156&user=Mario+Emmanuel&userId=7728df51142e&source=-----672556186156---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F672556186156&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-1958-perceptron-as-a-breast-cancer-classifier-672556186156&source=-----672556186156---------------------bookmark_footer-----------)![](../Images/102748f6bc62bb86ea6dbffa91874926.png)'
  prefs: []
  type: TYPE_NORMAL
- en: The US Navy using the Mark I Perceptron to read letters. National Museum of
    the US Navy, 1960\. Image from Wikimedia Commons (Public Domain).
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Rosenblatt Perceptron, developed by Frank Rosenblatt in 1958[1][2], is considered
    the origin of neural networks because it was the first algorithm that demonstrated
    the ability of a machine to learn from data.
  prefs: []
  type: TYPE_NORMAL
- en: The Perceptron was a simple model that consisted of a single layer of artificial
    neurons, or units, that could be trained to recognize patterns in input data.
    This marked the beginning of the field of artificial intelligence and paved the
    way for the development of more complex neural networks that have been used in
    a variety of applications.
  prefs: []
  type: TYPE_NORMAL
- en: The first implementation was the Mark I Perceptron developed by the MIT Lincoln
    Laboratory in the late 1950s. It was the first machine that was capable of learning
    from examples, and it was used to perform pattern recognition tasks such as reading
    handwritten letters and recognizing spoken words. The Mark I Perceptron was built
    using vacuum tubes and other electronic components, and it consisted of a series
    of units that were connected together in a layered structure. Each unit was able
    to process input data and adjust its internal weights and biases in order to recognize
    patterns in the data.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d8c9547ec9fa34d7bacbb8857ac7f3f1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1\. A detail of the Mark 1\. Source: Wikimedia Commons.'
  prefs: []
  type: TYPE_NORMAL
- en: This machine was used by the US Navy in the 1960s to read handwritten letters.
    At the time, the Navy was receiving a large volume of correspondence and needed
    a way to quickly and accurately process the letters. They turned to the Perceptron,
    which had the ability to learn from examples and recognize patterns in input data.
    By training the Perceptron on a large dataset of handwritten letters, the Navy
    was able to develop a system that could accurately read and classify the letters
    with minimal human intervention. This was a significant achievement at the time,
    as it demonstrated the potential of artificial intelligence and machine learning
    to automate tasks that had previously been done by humans. The success of the
    Perceptron in this application helped to establish the technology as a powerful
    tool for pattern recognition and opened the door to its use in a wide range of
    applications.
  prefs: []
  type: TYPE_NORMAL
- en: The single neuron network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Rosenblatt Perceptron is a simple model of an artificial neuron. In this
    model, the single neuron has multiple inputs, which are the values of the features
    in the input data, and a single output, which is a binary value that indicates
    whether the input data belongs to one of two classes. The Perceptron works by
    assigning weights to each input, which represent the importance of that input
    in determining the output. The output is then calculated using a mathematical
    function that combines the weighted inputs with a bias term. The bias term is
    a fixed value that can be adjusted to shift the output of the Perceptron. The
    Perceptron can be trained to recognize patterns in the input data by adjusting
    the weights and bias based on the errors in its predictions. This process of adjusting
    the weights and bias to minimize the errors is known as training the Perceptron.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/cb1d39036ae60dac0006ed76d1f6a917.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2\. The Perceptron corresponds to a one neuron Neural Network. In the
    image no bias is shown, it has to be either added separately, applied as a constant
    to one of the weights or not added at all. Image by [Chrislb from Wikimedia Commons](https://commons.wikimedia.org/wiki/File:ArtificialNeuronModel_english.png).
  prefs: []
  type: TYPE_NORMAL
- en: The model consists of a single layer of units, each of which has *d* features
    as inputs and a binary output that can take on the values *-1* and *+1*. The Perceptron
    uses these inputs and output to learn how to classify data into two classes.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/bfac8c1d1aa0eaab69f8d4128677c41e.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3\. Training observation Equations.
  prefs: []
  type: TYPE_NORMAL
- en: In order to train the Perceptron, we need a training set that consists of multiple
    observations, each of which consists of the d features (**X** vector) and the
    actual output (**y**). The Perceptron uses these observations to learn how to
    predict the output based on the features. The output of the Perceptron is calculated
    using the sign of the dot product between the weight vector and the feature vector.
  prefs: []
  type: TYPE_NORMAL
- en: Prediction function
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The prediction function of the Rosenblatt Perceptron is used to calculate the
    output of the Perceptron based on the input data and the internal weights and
    bias of the unit. The output of the Perceptron is a binary value that indicates
    whether the input data belongs to one of two classes. The prediction function
    is defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/cab0f212948c62ab66919af75f2a2807.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4\. Prediction function without bias.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f07eac91f3fbf969e6c77d542ab76d36.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5\. Prediction function with bias.
  prefs: []
  type: TYPE_NORMAL
- en: Where **W** is a vector of weights that corresponds to the input features, **X**
    is a vector of the input values for the features. The model can be implemented
    with and without bias, being **b** the bias term. The sign function returns **-1**
    if the value inside the parentheses is negative and +1 if it is positive. The
    Perceptron adjusts the weights and bias during training in order to minimize the
    errors in its predictions and improve its accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: The bias
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The bias term in the Rosenblatt Perceptron represents an additional weight
    that is applied to all of the inputs. It is used to shift the output of the Perceptron
    and can be adjusted during training to improve the accuracy of the model. The
    bias can be implemented in one of two ways: as a constant +1 in one of the features,
    or as an external parameter that is adjusted separately from the other weights.'
  prefs: []
  type: TYPE_NORMAL
- en: If the bias is implemented as a constant +1 feature (*feature trick*), it is
    treated like any other input feature and is given its own weight. This means that
    the bias term is included in the calculation of the output.
  prefs: []
  type: TYPE_NORMAL
- en: Alternatively, the bias can be implemented as an external parameter that is
    adjusted separately from the other weights. The bias is then added to the output
    calculated with the weight vector and the feature vectors.
  prefs: []
  type: TYPE_NORMAL
- en: The bias term in the Rosenblatt Perceptron is useful when the feature variables
    are mean centered, but the mean of the binary class prediction is not 0, because
    it allows the model to adjust the decision boundary of the model in order to better
    fit the data. This can be especially important when the binary class distribution
    is highly imbalanced, as the model may tend to predict the majority class more
    often in order to minimize errors. In this case, the bias can be used to adjust
    the position of the decision boundary and improve the ability of the model to
    correctly classify the minority class.
  prefs: []
  type: TYPE_NORMAL
- en: The loss function
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The Rosenblatt Perceptron model did not include a formal definition of a loss
    function, even when its goal was to minimize the error between the predicted and
    actual values. In order to achieve this, the Perceptron adjusts the weights and
    bias of the model based on the errors in its predictions. One way to define the
    error in the Perceptron model is to use the least-squares method, which involves
    minimizing the sum of the squared differences between the predicted and actual
    values. This loss function can be expressed mathematically as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/197b6c5a61a83f23cc86d87a846aa239.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6\. A loss function for the Perceptron.
  prefs: []
  type: TYPE_NORMAL
- en: While gradient descent is the usual way to minimize loss functions in Machine
    Learning, it can not be applied to Rosenblatt Perceptron, the reason being that
    the function is not continuous. Instead, the Perceptron used a learning rule called
    the perceptron convergence theorem, which was based on the idea of adjusting the
    weights and bias of the model in order to minimize the errors in its predictions.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/dea74c6d89c3cac39287de9a69823209.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7\. Equivalent to gradient descent of the Loss function.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/37cd374c01c82be42f8406c75a4ef80e.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8\. Iterative equations to optimise Weight vector.
  prefs: []
  type: TYPE_NORMAL
- en: The learning parameter in the Rosenblatt Perceptron model is a hyperparameter
    that determines the step size of the learning algorithm. It is used to control
    the rate at which the weights and bias of the model are updated based on the errors
    in the predictions. A larger learning parameter will result in larger updates
    to the weights and bias, which can lead to faster learning but may also increase
    the risk of overfitting. A smaller learning parameter will result in smaller updates,
    which can lead to slower learning but may also reduce the risk of overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: The Perceptron model is often described as having a stochastic gradient descent
    (SGD) learning algorithm, despite the fact that it was developed before the concept
    of gradient descent was introduced. This is because the Perceptron learning rule,
    known as the perceptron convergence theorem, is similar in spirit to gradient
    descent, as it involves iteratively adjusting the weights and bias of the model
    in order to minimize the errors in the predictions. Like gradient descent, the
    Perceptron uses a learning parameter to control the step size of the updates,
    and it can be seen as a form of online learning, as it processes the training
    data one sample at a time.
  prefs: []
  type: TYPE_NORMAL
- en: Overall, the learning parameter in the Perceptron model plays a crucial role
    in controlling the speed and accuracy of the learning process. By adjusting the
    learning parameter, it is possible to fine-tune the performance of the Perceptron
    model and achieve better results on a variety of classification tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Perceptron in action: a Breast Cancer predictor'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Wisconsin Breast Cancer dataset[3] is a commonly used dataset for demonstrating
    the capabilities of the Rosenblatt Perceptron model. This dataset consists of
    699 samples of breast cancer biopsy images taken from 1989 until 1992, which have
    been classified as benign or malignant based on the presence of certain features.
    The dataset includes a total of 9 features that were calculated from the images,
    including the clump thickness, uniformity of the cell size, uniformity of the
    cell shape, marginal adhesion, single epithelial cell size, bare nuclei, bland
    chromatin, normal nucleoli and mitoses of the tumors.
  prefs: []
  type: TYPE_NORMAL
- en: The dataset is a good example to see the Perceptron in action because it is
    a relatively simple dataset with a clear separation between the benign and malignant
    classes. This means that the Perceptron should be able to learn to classify the
    samples correctly with a high degree of accuracy. In addition, the features in
    the dataset are well-defined and easy to understand, which makes it easy to interpret
    the results of the Perceptron model.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Wisconsin Breast Cancer dataset includes the following features:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: In this example the Perceptron will be implemented using Wolfram Mathematica
    Language (it can be adapted to any other language easily).
  prefs: []
  type: TYPE_NORMAL
- en: 'The steps to implement will be:'
  prefs: []
  type: TYPE_NORMAL
- en: Define the features.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Load the data (including cleaning).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Divide dataset into training and test.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Assign an initial Weight vector.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Train the model (optimise weight vector).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use the trained model to compare test data set.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Calculate the accuracy.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Calculate the recall.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Calculate the confussion matrix.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Step 1\. Define the features
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Step 2\. Load the data and clean it
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Step 3\. Divide dataset into training and test
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Step 4\. Assign an initial Weight vector
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Step 5\. Train the model (optimise the weight vector)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Step 6\. Use the trained model to compare test data set
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Step 7\. Calculate accuracy
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Step 8\. Calculate recall
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Step 9\. Confussion matrix
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/1c94e665e64e9aac879f20cfc3a1abcb.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9\. Confussion Matrix of our Perceptron
  prefs: []
  type: TYPE_NORMAL
- en: Measuring performance of our classifier
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One way to measure how good a Perceptron (or any other classifier) is to evaluate
    its performance on a test dataset. There are several metrics that can be used
    to evaluate the performance of a classifier, such as accuracy, precision, recall,
    and F1 score.
  prefs: []
  type: TYPE_NORMAL
- en: Accuracy is the percentage of correct predictions made by the classifier. It
    is calculated as the number of correct predictions divided by the total number
    of predictions. However, accuracy can be misleading if the classes are imbalanced
    (i.e., one class is much more common than the other).
  prefs: []
  type: TYPE_NORMAL
- en: Recall is the percentage of positive cases that were correctly identified by
    the classifier. It is calculated as the number of true positive predictions divided
    by the total number of positive cases. Recall is particularly important in applications
    where it is important to minimize the number of false negatives (e.g., a cancer
    detector). In our example we get an 86%, which means that our predictor is missing
    14% of the malignant tumours.
  prefs: []
  type: TYPE_NORMAL
- en: A confusion matrix is a table that shows the number of true positive, true negative,
    false positive, and false negative predictions made by a classifier. It is a useful
    tool for understanding the strengths and weaknesses of a classifier, and for comparing
    the performance of different classifiers.
  prefs: []
  type: TYPE_NORMAL
- en: In a cancer detector, the confusion matrix is particularly important because
    it can help identify cases where the classifier is making incorrect predictions.
    For example, if the classifier is making a large number of false negatives (i.e.,
    it is missing a lot of cancer cases), it may be necessary to adjust the classifier
    or to gather more training data to improve its performance. On the other hand,
    if the classifier is making a large number of false positives (i.e., it is identifying
    a lot of benign cases as cancerous), it may be necessary to adjust the classifier
    to be more conservative in its predictions.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This article provided a basic mathematical description of the Perceptron, a
    type of single-layer neural network that was developed in the 1950s. It explained
    the mathematics behind Perceptrons, including how they can be used to classify
    data into different categories.
  prefs: []
  type: TYPE_NORMAL
- en: The Perceptron was applied to a well-known dataset in data science, the *Wisconsin
    Breast Cancer dataset* from 1995, and demonstrated how different metrics can be
    used to evaluate the performance of the classifier. The implementation of the
    Perceptron in Mathematica showed how easily these concepts can be represented
    in modern programming languages, and the different steps described in the implementation
    demonstrate how a classifier can be designed from scratch and its performance
    evaluated.
  prefs: []
  type: TYPE_NORMAL
- en: While Perceptrons are no longer used in modern machine learning practice due
    to the development of more advanced neural network architectures, the article
    showed that they are still a valuable tool for understanding the fundamental principles
    of neural networks.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[1] [https://news.cornell.edu/stories/2019/09/professors-perceptron-paved-way-ai-60-years-too-soon](https://news.cornell.edu/stories/2019/09/professors-perceptron-paved-way-ai-60-years-too-soon)'
  prefs: []
  type: TYPE_NORMAL
- en: '[2] [https://psycnet.apa.org/record/1959-09865-001](https://psycnet.apa.org/record/1959-09865-001)'
  prefs: []
  type: TYPE_NORMAL
- en: '[3] [https://archive.ics.uci.edu/ml/datasets/breast+cancer+wisconsin+(diagnostic)](https://archive.ics.uci.edu/ml/datasets/breast+cancer+wisconsin+(diagnostic))
    | [https://archive-beta.ics.uci.edu/dataset/15/breast+cancer+wisconsin+original](https://archive-beta.ics.uci.edu/dataset/15/breast+cancer+wisconsin+original)
    (CC BY 4.0 license, see acknowledges).'
  prefs: []
  type: TYPE_NORMAL
- en: Acknowledges
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The dataset used was made available by the UCI Machine Learning Repository.
    Dataset created by:'
  prefs: []
  type: TYPE_NORMAL
- en: 1\. Dr. William H. Wolberg, General Surgery Dept.
  prefs: []
  type: TYPE_NORMAL
- en: University of Wisconsin, Clinical Sciences Center
  prefs: []
  type: TYPE_NORMAL
- en: Madison, WI 53792
  prefs: []
  type: TYPE_NORMAL
- en: 2\. W. Nick Street, Computer Sciences Dept.
  prefs: []
  type: TYPE_NORMAL
- en: University of Wisconsin, 1210 West Dayton St., Madison, WI 53706
  prefs: []
  type: TYPE_NORMAL
- en: 3\. Olvi L. Mangasarian, Computer Sciences Dept.
  prefs: []
  type: TYPE_NORMAL
- en: University of Wisconsin, 1210 West Dayton St., Madison, WI 53706
  prefs: []
  type: TYPE_NORMAL
- en: 'Donor: Nick Street.'
  prefs: []
  type: TYPE_NORMAL
- en: 'UCI Machine Learning Repository [http://archive.ics.uci.edu/ml]. Irvine, CA:
    University of California, School of Information and Computer Science. ([https://archive.ics.uci.edu/ml/about.html](https://archive.ics.uci.edu/ml/about.html)
    / [https://archive.ics.uci.edu/ml/citation_policy.html](https://archive.ics.uci.edu/ml/citation_policy.html)).'
  prefs: []
  type: TYPE_NORMAL
