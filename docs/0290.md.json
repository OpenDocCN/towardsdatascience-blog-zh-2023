["```py\npip install hdx-python-api\n```", "```py\nfrom hdx.utilities.easy_logging import setup_logging\nfrom hdx.api.configuration import Configuration\nfrom hdx.data.dataset import Dataset\n\nsetup_logging()\nConfiguration.create(hdx_site=\"prod\", user_agent=\"my_agent_name\", hdx_read_only=True)\n```", "```py\ndef is_supported_filetype(format):\n    \"\"\"\n    Checks if the file format is currently supported for extracting meta data.\n\n    Parameters\n    ----------\n    format : str\n        The file format to check.\n\n    Returns\n    -------\n    bool\n        True if the file format is supported, False otherwise.\n    \"\"\"\n    matches = [\"CSV\", \"XLSX\", \"XLS\", \"TSV\"]\n    if any(x in format for x in matches):\n        return True\n    else:\n        return False\n\ndef download_data(datasets, output_folder):\n    \"\"\"\n    Downloads data from HDX. Will save dataset and resource meta data for each file\n\n    Parameters\n    ----------\n    datasets : pandas.DataFrame\n        A dataframe containing the datasets to download.\n    output_folder : str\n        The folder to download the data to.\n    \"\"\"\n    if not os.path.exists(output_folder):\n        os.mkdir(output_folder)\n\n    for index, row in datasets.iterrows():\n        dataset = Dataset.read_from_hdx(row[\"id\"])\n        resources = dataset.get_resources()\n        for resource in resources:\n            dir = f\"./{output_folder}/{row['name']}_{row['id']}\"\n            print(\n                f\"Downloading {row['name']} - {resource['name']} - {resource['format']}\"\n            )\n            resource[\"dataset_name\"] = row[\"name\"]\n            if not os.path.exists(dir):\n                dump_hdx_meta_file(dataset, dir, \"dataset.json\")\n            try:\n                dir = f'{dir}/{get_safe_name(resource[\"name\"])}_{get_safe_name(resource[\"id\"])}'\n                if not os.path.exists(dir):\n                    dump_hdx_meta_file(resource, dir, \"resource.json\")\n                    if is_supported_filetype(resource[\"format\"]):\n                        url, path = resource.download(dir)\n                    else:\n                        print(\n                            f\"*** Skipping file as it is not a supported filetype *** {resource['name']}\"\n                        )\n                else:\n                    print(f\"Skipping {dir} as it already exists\")\n            except Exception as e:\n                traceback.print_exc()\n                sys.exit()\n\n    print(\"Done\")\n```", "```py\ndatasets_hxl = pd.DataFrame(Dataset.search_in_hdx(\"HXL\"))\ndownload_data(datasets_hxl, output_folder)\n```", "```py\ndef check_hdx_header(first_row):\n    \"\"\"\n    This function checks if the first row of a csv file likely an HDX header.\n    \"\"\"\n    matches = [\"#meta\", \"#country\", \"#data\", \"#loc\", \"#geo\"]\n    if any(x in first_row for x in matches):\n        return True\n    else:\n        return False\n\ndef set_meta_data_fields(data, file, dataset, resource, sheet, type):\n    \"\"\"\n    This function create a data frame with meta data about the data, as well as a snippet of its\n    first nrows.\n\n    Parameters:\n        data: a dataframe\n        file: the name of the data file\n        dataset: the dataset JSON object from HDX\n        resource: the resource JSON object from HDX\n        sheet: the sheet name if the data was a tab in a sheet\n        type: the type of file, CSV, XLSX, etc.\n    Returns:\n        dict: a dictionary with metadata about the dataframe\n    \"\"\"\n\n    nrows = 10\n\n    # Data preview to only include values\n    data = data.dropna(axis=1, how=\"all\")\n\n    cols = str(list(data.columns))\n    if data.shape[0] > 0:\n        first_row = str(list(data.iloc[0]))\n        has_hxl_header = check_hdx_header(first_row)\n        num_rows = int(data.shape[0])\n        num_cols = int(data.shape[1])\n        first_nrows = data.head(nrows)\n    else:\n        first_row = \"No data\"\n        has_hxl_header = \"No data\"\n        num_rows = 0\n        num_cols = 0\n        first_nrows = None\n\n    dict = {}\n\n    dict[\"resource_id\"] = resource[\"id\"]\n    dict[\"resource_name\"] = resource[\"name\"]\n    dict[\"resource_format\"] = resource[\"format\"]\n    dict[\"dataset_id\"] = dataset[\"id\"]\n    dict[\"dataset_name\"] = dataset[\"name\"]\n    dict[\"dataset_org_title\"] = dataset[\"organization\"][\"title\"]\n    dict[\"dataset_last_modified\"] = dataset[\"last_modified\"]\n    dict[\"dataset_tags\"] = dataset[\"tags\"]\n    dict[\"dataset_groups\"] = dataset[\"groups\"]\n    dict[\"dataset_total_res_downloads\"] = dataset[\"total_res_downloads\"]\n    dict[\"dataset_pageviews_last_14_days\"] = dataset[\"pageviews_last_14_days\"]\n    dict[\"file\"] = file\n    dict[\"type\"] = type\n    dict[\"dataset\"] = dataset\n    dict[\"sheet\"] = sheet\n    dict[\"resource\"] = resource\n    dict[\"num_rows\"] = num_rows\n    dict[\"num_cols\"] = num_cols\n    dict[\"columns\"] = cols\n    dict[\"first_row\"] = first_row\n    dict[\"has_hxl_header\"] = has_hxl_header\n    dict[\"first_nrows\"] = first_nrows\n    return dict\n\ndef extract_data_details(f, dataset, resource, nrows, data_details):\n    \"\"\"\n\n    Reads saved CVS and XLSX HDX files and extracts headers, HDX tags and sample data.\n    For XLSX files, it extracts data from all sheets.\n\n    Parameters\n    ----------\n    f : str\n        The file name\n    dataset : str\n        The dataset name\n    resource : str\n        The resource name\n    nrows : int\n        The number of rows to read\n    data_details : list\n        The list of data details\n\n    Returns\n    -------\n    data_details : list\n        The list of data details\n\n    \"\"\"\n    if f.endswith(\".xlsx\") or f.endswith(\".xls\"):\n        print(f\"Loading xslx file {f} ...\")\n        try:\n            sheet_to_df_map = pd.read_excel(f, sheet_name=None)\n        except Exception:\n            print(\"An exception occurred trying to read the file {f}\")\n            return data_details\n        for sheet in sheet_to_df_map:\n            data = sheet_to_df_map[sheet]\n            data_details.append(\n                set_meta_data_fields(data, f, dataset, resource, sheet, \"xlsx\")\n            )\n    elif f.endswith(\".csv\"):\n        print(f\"Loading csv file {f}\")\n        # Detect encoding\n        with open(f, \"rb\") as rawdata:\n            r = chardet.detect(rawdata.read(100000))\n        try:\n            data = pd.read_csv(f, encoding=r[\"encoding\"], encoding_errors=\"ignore\")\n        except Exception:\n            print(\"An exception occurred trying to read the file {f}\")\n            return data_details\n        data_details.append(set_meta_data_fields(data, f, dataset, resource, \"\", \"csv\"))\n    else:\n        type = f.split(\".\")[-1]\n        print(f\"Type {type} for {f}\")\n        data = pd.DataFrame()\n        data_details.append(set_meta_data_fields(data, f, dataset, resource, \"\", type))\n\n    return data_details\n\n# Loop through downloaded folders\ndef extract_all_data_details(startpath, data_details):\n    \"\"\"\n    Extracts all data details for downloaded HDX files in a given directory.\n\n    Parameters\n    ----------\n    startpath : str\n        The path to the directory containing all datasets.\n    data_details : list\n        Results\n\n    Returns\n    -------\n    data_details : pandas.DataFrame\n        Results, to which new meta data was appended. \n        See function set_meta_data_fields for columns\n    \"\"\"\n    for d in os.listdir(startpath):\n          d = f\"{startpath}/{d}\"\n          with open(f\"{d}/dataset.json\") as f:\n              dataset = json.load(f)\n          for r in os.listdir(d):\n              if \"dataset.json\" not in r:\n                  with open(f\"{d}/{r}/resource.json\") as f:\n                      resource = json.load(f)\n                  for f in os.listdir(f\"{d}/{r}\"):\n                      file = str(f\"{d}/{r}/{f}\")\n                      if \".json\" not in file:\n                          data_details = extract_data_details(\n                              file, dataset, resource, 5, data_details\n                          )\n    data_details = pd.DataFrame(data_details)\n    return data_details\n```", "```py\nhxl_resources_data_details = extract_all_data_details(f\"./data/hxl_datasets/\", [])\nprint(hxl_resources_data_details.shape)\n\n(25695, 22)\n```", "```py\n{\"prompt\": \" 'scheduled_service' | \\\"['1', '1', '0', '0', '0', '0', '0', '0']\\\"\", \"completion\": \" #status+scheduled\"}\n```", "```py\n def get_hxl_tags_list(resources):\n    \"\"\"\n    Build a list of the HXL tags found in a dataframe of HDX resources.\n\n    Parameters\n    ----------\n    resources : pandas dataframe\n        A dataframe of HDX resources\n\n    Returns\n    -------\n    hxl_tags : list\n        A list of HXL tags.\n    \"\"\"\n    hxl_tags = []\n    for row, d in resources.iterrows():\n        if d[\"has_hxl_header\"] == True:\n            fr = d[\"first_row\"].replace(\" \", \"\")\n            for c in fr.split(\",\"):\n                fr = re.sub(\"\\[|\\]|\\\"|\\'\",\"\", c)\n                hdxs = fr.split(\"+\")\n                for h in hdxs:\n                    if h not in hxl_tags and len(h) > 0:\n                        hxl_tags.append(h.lower())\n    hxl_tags = list(set(hxl_tags))\n    hxl_tags.remove('nan')\n    return hxl_tags\n\ndef get_prompt(col_name, data):\n    \"\"\"\n    Builds the prompt for GPT-3 for predicting HXL tags and attributes\n\n    Parameters\n    ----------\n    col_name : str\n        Column name\n    data : list\n        A list of sample data for the column\n\n    Returns\n    -------\n    prompt : string\n        A prompt for GPT-3.\n    \"\"\"\n    ld = len(data) - 1\n    col_data = json.dumps(str(list(data.iloc[1:ld])))\n    prompt = f\" {col_name} | {col_data}\".lower()\n    return prompt\n\ndef create_training_set(resources):\n    \"\"\"\n    Builds a jsonl training data file for GPT-3 where each row is a prompt for a column HXL tag.\n\n    It will only output prompts where the sample data for the column didn't contain nans.\n\n    Parameters\n    ----------\n    resources : pandas dataframe\n        A dataframe of HDX resources\n\n    Returns\n    -------\n    train_data : list\n        A list of prompts and completions for the HXL tag autocomplete feature.\n    \"\"\"\n    train_data = []\n    for row, d in resources.iterrows():\n        if d[\"has_hxl_header\"] == True:\n            cols = d[\"columns\"][1:-1].split(\",\")\n            hdxs = d[\"first_row\"][1:-1].split(\",\")\n            data = d[\"first_nrows\"]\n            has_hxl_header = d[\"has_hxl_header\"]\n            if len(cols) == len(hdxs) and len(cols) > 1:\n                ld = len(data) - 1\n                for i in range(0, len(cols)):\n                    if i < len(hdxs):\n                        hdx = re.sub(\"'|\\\"\", \"\", hdxs[i])\n                    # Only include is has HXL tags and good sample data in column\n                    if has_hxl_header == True and hdx != np.nan:\n                        prompt = get_prompt(cols[i], data.iloc[:,i])\n                        if 'nan' not in hdx and 'nan, nan' not in prompt:\n                            p = {\n                                \"prompt\": prompt,\n                                \"completion\": f\" {hdx}\",\n                            }\n                            train_data.append(p)\n    return train_data\n```", "```py\n# Create training set\nX_train = create_training_set(X_train_resources)\nprint(f\"Training records: {len(X_train)}\")\n\ntrain_file = \"fine_tune_openai_train.jsonl\"\n\nwith open(train_file, \"w\") as f:\n    for p in X_train:\n        json.dump(p, f)\n        f.write(\"\\n\")\n\nprint(\"Done\")\n```", "```py\n{\"prompt\": \"  'Country ISO3' | \\\"['COD', 'COD', 'COD', 'COD', 'COD', 'COD', 'COD', 'COD']\\\"\", \"completion\": \"  #country+code\"}\n{\"prompt\": \"  'Year' | \\\"['2010', '2005', '2000', '1995', '1990', '1985', '1980', '1975']\\\"\", \"completion\": \"  #date+year\"}\n{\"prompt\": \"  'Indicator Name' | \\\"['Barro-Lee: Percentage of female population age 15-19 with no education', 'Barro-Lee: Percentage of female population age 15-19 with no education', 'Barro-Lee: Percentage of female population age 15-19 with no education', 'Barro-Lee: Percentage of female population age 15-19 with no education', 'Barro-Lee: Percentage of female population age 15-19 with no education', 'Barro-Lee: Percentage of female population age 15-19 with no education', 'Barro-Lee: Percentage of female population age 15-19 with no education', 'Barro-Lee: Percentage of female population age 15-19 with no education']\\\"\", \"completion\": \"  #indicator+name\"}\n{\"prompt\": \"  'Indicator Code' | \\\"['BAR.NOED.1519.FE.ZS', 'BAR.NOED.1519.FE.ZS', 'BAR.NOED.1519.FE.ZS', 'BAR.NOED.1519.FE.ZS', 'BAR.NOED.1519.FE.ZS', 'BAR.NOED.1519.FE.ZS', 'BAR.NOED.1519.FE.ZS', 'BAR.NOED.1519.FE.ZS']\\\"\", \"completion\": \"  #indicator+code\"}\n{\"prompt\": \"  'Value' | \\\"['48.1', '51.79', '52.1', '43.62', '35.44', '38.02', '43.47', '49.08']\\\"\", \"completion\": \"  #indicator+value+num\"}\n{\"prompt\": \" 'Country ISO3' | \\\"['COD', 'COD', 'COD', 'COD', 'COD', 'COD', 'COD', 'COD']\\\"\", \"completion\": \" #country+code\"}\n{\"prompt\": \"  'Year' | \\\"['2015', '2014', '2013', '2012', '2011', '2010', '2009', '2008']\\\"\", \"completion\": \"  #date+year\"}\n```", "```py\nimport openai\nfrom openai import cli\n\n# Open AI API key should be put into this file\nopenai.api_key_path = \"./api_key.txt\"\n\nprint(\"Uploading training file ...\")\ntraining_id = cli.FineTune._get_or_upload(train_file, True)\n# validation_id = cli.FineTune._get_or_upload(validation_file_name, True)\n\nprint(\"Fine-tuning model ...\")\ncreate_args = {\n    \"training_file\": training_id,\n    # \"validation_file\": test_file,\n    \"model\": \"ada\",\n}\n# https://beta.openai.com/docs/api-reference/fine-tunes/create\nresp = openai.FineTune.create(**create_args)\njob_id = resp[\"id\"]\nstatus = resp[\"status\"]\n\nprint(f\"Fine-tunning model with jobID: {job_id}.\")\n```", "```py\nresult = openai.FineTune.retrieve(id=job_id)\nprint(result['status'])\n```", "```py\nresult = openai.FineTune.retrieve(id=job_id)\nmodel = result[\"fine_tuned_model\"]\n```", "```py\ndef create_prediction_dataset_from_resources(resources):\n    \"\"\"\n    Generate a list of model column-level prompts from a list of resources (tables).\n\n    It will only output prompts where the sample data for the column didn't contain nans.\n\n    Parameters\n    ----------\n    resources : list\n        A list of dictionaries containing the resource name, columns, first_row, and first_nrows.\n\n    Returns\n    -------\n    prediction_data : list\n        A list of dictionaries containing GPT-3 prompts (one per column in resource table)\n    \"\"\"\n\n    prediction_data = []\n    for index, d in resources.iterrows():\n        cols = d[\"columns\"][1:-1].split(\",\")\n        hdxs = d[\"first_row\"][1:-1].split(\",\")\n        data = d[\"first_nrows\"]\n        has_hxl_header = d[\"has_hxl_header\"]\n        if len(cols) == len(hdxs) and len(cols) > 1:\n            ld = len(data) - 1\n            # Loop through columns \n            for i in range(0, len(cols)):\n                if i < len(hdxs) and i < data.shape[1]:\n                    prompt = get_prompt(cols[i], data.iloc[:,i])\n                    # Skip any prompts with at least two nan values in sample data\n                    if 'nan, nan' not in prompt:\n                        r = {\n                                \"prompt\": prompt\n                            }\n                        # If we were called with HXL tags (ie for test set), populate 'expected'\n                        if has_hxl_header == True:\n                            hdx = re.sub(\"'|\\\"| \", \"\", hdxs[i])\n                            # Row has HXL tags, but this particular column doesn't have tags\n                            if hdx == 'nan':\n                                continue\n                            else:\n                                r[\"expected\"]= hdx\n                        prediction_data.append(r)\n    return prediction_data\n\ndef make_gpt3_prediction(prompt, model, temperature=0.99, max_tokens=13):\n    \"\"\"\n    Wrapper to call GPT-3 to make a prediction (completion) on a single prompt.\n\n    Parameters\n    ----------\n    prompt : str\n        Prompt to use for prediction\n    model : str\n        GPT-3 model to use\n    temperature : float\n        Temperature to use for sampling\n    max_tokens : int\n        Maximum number of tokens to use for sampling\n\n    Returns\n    -------\n    result : dict\n        Dictionary with prompt, predicted, and \n        log probabilities of each completed token\n    \"\"\"\n    result = {}\n    result[\"prompt\"] = prompt\n    model_result = openai.Completion.create(\n        engine=model,\n        prompt=prompt,\n        temperature=temperature,\n        max_tokens=max_tokens,\n        top_p=1,\n        frequency_penalty=0,\n        presence_penalty=0,\n        stop=[\"\\n\"],\n        logprobs=1\n    )\n    result[\"predicted\"] = model_result[\"choices\"][0][\"text\"].replace(\" \",\"\")\n    result[\"logprobs\"]  = model_result['choices'][0]['logprobs']['top_logprobs']\n    return result\n\ndef make_gpt3_predictions(\n    sample_size, prediction_data, model, temperature=0.99, max_tokens=13, logprob_cutoff=-0.01\n):\n\n    \"\"\"\n    Wrapper to call GPT-3 to make predictions on test file for sample_size samples.\n\n    Parameters\n    ----------\n    sample_size : int\n        Number of predictions to make from test file\n    prediction_data : list\n        List of dictionaries with prompts\n    model : str\n        GPT-3 model to use\n    postprocess : bool\n        Whether to postprocess the predictions\n    temperature : float\n        Temperature to use for sampling\n    max_tokens : int\n        Maximum number of tokens to use for sampling\n    prob_cutoff : float\n        Logprob cutoff for filtering out low probability tokens\n\n    Returns\n    -------\n    results : list\n        List of dictionaries with prompt, predicted, predicted_post_processed\n    \"\"\"\n    results = []\n    prediction_data = sample(prediction_data, sample_size)\n    for i in range(0, sample_size):\n        prompt = prediction_data[i][\"prompt\"]\n        res = make_gpt3_prediction(\n            prompt, model, temperature, max_tokens\n        )\n\n        # Filter out low logprob predictions\n        pred = \"\"\n        seen_tokens = []\n        for w in res[\"logprobs\"]:\n            token = list(w.keys())[0]\n            prob = w[token]\n            if prob > logprob_cutoff and token not in seen_tokens:\n                pred += token\n                if '+' not in token:\n                    seen_tokens.append(token)\n            else:\n                break\n        pred = re.sub(r\" |\\+$|\\+v_$\", \"\", pred)\n\n        r = {\n                \"prompt\": prompt,\n                \"predicted\": res[\"predicted\"],\n                \"predicted_log_prob_cutoff\": pred,\n                #\"logprobs\": res[\"logprobs\"]\n            }\n        # For test sets we have expected values, add back for performance reporting\n        if \"expected\" in prediction_data[i]:\n            r['expected'] = prediction_data[i]['expected'].replace(' ', '')\n        results.append(r)\n    return results\n```", "```py\n# Generate the prompts we want GPT-3 to complete\nprint(\"Building model input ...\")\nprediction_data = create_prediction_dataset_from_resources(X_test_resources)\n\n# How many predictions to try from the test set\nsample_size = 500\n\n# Make the predictions\nprint(\"Making GPT-3 predictions (completions) ...\")\nresults = make_gpt3_predictions(\n   sample_size, prediction_data, model, temperature=0.99, max_tokens=20, logprob_cutoff=-0.001\n) \n```", "```py\ndef output_prediction_metrics(results, prediction_field=\"predicted_post_processed\"):\n    \"\"\"\n    Prints out model performance report if provided results in the format:\n\n    [\n        {\n            'prompt': ' \\'ISO3\\' | \"[\\'RWA\\', \\'RWA\\', \\'RWA\\', \\'RWA\\', \\'RWA\\', \\'RWA\\', \\'RWA\\', \\'RWA\\']\"', \n            'predicted': ' #country+code+iso3+v_iso3+', \n            'predicted_post_processed': '#country+code', \n            'expected': '#country+code'\n        }, \n        ... etc ...\n    ]\n\n    Parameters\n    ----------\n    results : list\n        See above for format\n    prediction_field : str\n        Field name of element with prediction. Handy for comparing raw and post-processed predictions.\n    \"\"\"\n    y_test = []\n    y_pred = []\n    y_justtag_test = []\n    y_justtag_pred = []\n    for r in results:\n        if \"expected\" not in r:\n            print(\"Provided results do not contain expected values.\")\n            sys.exit()\n        y_pred.append(r[prediction_field])\n        y_test.append(r[\"expected\"])\n        expected_tag = r[\"expected\"].split(\"+\")[0]\n        predicted_tag = r[prediction_field].split(\"+\")[0]\n        y_justtag_test.append(expected_tag)\n        y_justtag_pred.append(predicted_tag)\n\n    print(f\"GPT-3 results for {prediction_field}, {len(results)} predictions ...\")\n    print(\"\\nJust HXL tags ...\\n\")\n    print(f\"Accuracy: {round(accuracy_score(y_justtag_test, y_justtag_pred),2)}\")\n    print(\n        f\"Precision: {round(precision_score(y_justtag_test, y_justtag_pred, average='weighted', zero_division=0),2)}\"\n    )\n    print(\n        f\"Recall: {round(recall_score(y_justtag_test, y_justtag_pred, average='weighted', zero_division=0),2)}\"\n    )\n    print(\n        f\"F1: {round(f1_score(y_justtag_test, y_justtag_pred, average='weighted', zero_division=0),2)}\"\n    )\n\n    print(f\"\\nTags and attributes with {prediction_field} ...\\n\")\n    print(f\"Accuracy: {round(accuracy_score(y_test, y_pred),2)}\")\n    print(\n        f\"Precision: {round(precision_score(y_test, y_pred, average='weighted', zero_division=0),2)}\"\n    )\n    print(\n        f\"Recall: {round(recall_score(y_test, y_pred, average='weighted', zero_division=0),2)}\"\n    )\n    print(\n        f\"F1: {round(f1_score(y_test, y_pred, average='weighted', zero_division=0),2)}\"\n    )\n\n    return \n```", "```py\noutput_prediction_metrics(results, prediction_field=\"predicted\")\n\nGPT-3 results for predicted, 500 predictions ...\n\nJust HXL tags ...\n\nAccuracy: 0.99\nPrecision: 0.99\nRecall: 0.99\nF1: 0.99\n\nTags and attributes with predicted ...\n\nAccuracy: 0.0\nPrecision: 0.0\nRecall: 0.0\nF1: 0.0\n```", "```py\n {\n    \"prompt\": \" 'gho (code)' | \\\"['mort_100', 'mort_100', 'mort_100', 'mort_100', 'mort_100', 'mort_100', 'mort_100', 'mort_100']\\\"\",\n    \"predicted\": \"#indicator+code+v_hor_funder_\",\n    \"expected\": \"#indicator+code\"\n}\n{\n    \"prompt\": \"  'region (code)' | \\\"['afr', 'afr', 'afr', 'afr', 'afr', 'afr', 'afr', 'afr']\\\"\",\n    \"predicted\": \"#region+code+v_reliefweb+f\",\n    \"expected\": \"#region+code\"\n}\n{\n    \"prompt\": \"  'dataid' | \\\"['310633', '310634', '310635', '310636', '310629', '310631', '310630', '511344']\\\"\",\n    \"predicted\": \"#meta+id+fts_internal_view_all\",\n    \"expected\": \"#meta+id\"\n}\n{\n    \"prompt\": \"  'gho (url)' | \\\"['https://www.who.int/data/gho/indicator-metadata-registry/imr-details/5580', 'https://www.who.int/data/gho/indicator-metadata-registry/imr-details/5580']\\\"\",\n    \"predicted\": \"#indicator+url+name+has_more_\",\n    \"expected\": \"#indicator+url\"\n}\n{\n    \"prompt\": \"  'year (display)' | \\\"['2014', '2014', '2014', '2014', '2014', '2014', '2014', '2014']\\\"\",\n    \"predicted\": \"#date+year+name+tariff+for+\",\n    \"expected\": \"#date+year\"\n}\n{\n    \"prompt\": \"  'byvariablelabel' | \\\"[nan]\\\"\",\n    \"predicted\": \"#indicator+label+code+placeholder+Hubble\",\n    \"expected\": \"#indicator+label\"\n}\n{\n    \"prompt\": \" 'gho (code)' | \\\"['ntd_bejelstatus', 'ntd_pintastatus', 'ntd_yawsend', 'ntd_leishcend', 'ntd_leishvend', 'ntd_leishcnum_im', 'ntd_leishcnum_im', 'ntd_leishcnum_im']\\\"\",\n    \"predicted\": \"#indicator+code+v_ind+olk_ind\",\n    \"expected\": \"#indicator+code\"\n}\n{\n    \"prompt\": \"  'enddate' | \\\"['2002-12-31', '2003-12-31', '2004-12-31', '2005-12-31', '2006-12-31', '2007-12-31', '2008-12-31', '2009-12-31']\\\"\",\n    \"predicted\": \"#date+enddate+enddate+usd+\",\n    \"expected\": \"#date+end\"\n}\n{\n    \"prompt\": \"  'endyear' | \\\"['2013', '2013', '2013', '2013', '2013', '2013', '2013', '2013']\\\"\",\n    \"predicted\": \"#date+year+endyear+end_of_\",\n    \"expected\": \"#date+year+end\"\n}\n{\n    \"prompt\": \"  'country (code)' | \\\"['dnk', 'dnk', 'dnk', 'dnk', 'dnk', 'dnk', 'dnk', 'dnk']\\\"\",\n    \"predicted\": \"#country+code+v_iso2+v_\",\n    \"expected\": \"#country+code\"\n}\n```", "```py\n\"predicted\": \"#country+code+v_iso2+v_\",\n\"expected\": \"#country+code\"\n```", "```py\npasses = 0\nfails = 0\nfor r in results:\n    if r[\"predicted\"].startswith(r[\"expected\"]):\n        passes += 1\n    else:\n        fails += 1\n        #print(json.dumps(r, indent=4, sort_keys=False))\n\nprint(f\" Out of {passes + fails} predictions, the expected tags and attributes where in the predicted tags and attributes {round(100*passes/(passes+fails),1)}% of the time.\")\n\nOut of 500 predictions, the expected tags and attributes where in the predicted tags and attributes 99.0% of the time.\n```", "```py\n# Filter out low logprob predictions\npred = \"\"\nseen_tokens = []\nfor w in res[\"logprobs\"]:\n    token = list(w.keys())[0]\n    prob = w[token]\n    if prob > logprob_cutoff and token not in seen_tokens:\n        pred += token\n        if '+' not in token:\n            seen_tokens.append(token)\n    else:\n        break\npred = re.sub(r\" |\\+$|\\+v_$\", \"\", pred)\n```", "```py\noutput_prediction_metrics(results, prediction_field=\"predicted_log_prob_cutoff\")\n\nJust HXL tags ...\n\nAccuracy: 0.99\nPrecision: 1.0\nRecall: 0.99\nF1: 0.99\n\nTags and attributes with predicted_log_prob_cutoff ...\n\nAccuracy: 0.94\nPrecision: 0.99\nRecall: 0.94\nF1: 0.95\n```"]