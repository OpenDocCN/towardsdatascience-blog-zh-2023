["```py\nimport dask_ml.model_selection as dcv\nfrom sklearn.datasets import make_classification\nfrom sklearn.svm import SVC\n\n# Create a large dataset\nX, y = make_classification(n_samples=100000, n_features=20, random_state=42)\n\n# Define your model\nmodel = SVC()\n\n# Train your model in parallel using Dask-ML\nparams = {\"C\": dcv.Categorical([0.1, 1, 10]), \"kernel\": dcv.Categorical([\"linear\", \"rbf\"])}\nsearch = dcv.RandomizedSearchCV(model, params, n_iter=10, cv=3)\nsearch.fit(X, y)\n```", "```py\nimport featuretools as ft\n\n# Load your raw data into an entityset\nes = ft.EntitySet(id=\"my_data\")\nes = es.entity_from_dataframe(entity_id=\"customers\", dataframe=data, index=\"customer_id\")\n\n# Define relationships between entities\n# ...\n\n# Automatically generate new features\nfeature_matrix, feature_defs = ft.dfs(entityset=es, target_entity=\"customers\", max_depth=2)\n```", "```py\nimport tensorflow as tf\nfrom tensorflow.keras.callbacks import TensorBoard\n\n# Define your model\nmodel = tf.keras.Sequential([...])\n\n# Compile your model\nmodel.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n\n# Create a TensorBoard callback\ntensorboard_callback = TensorBoard(log_dir=\"./logs\")\n\n# Train your model with the TensorBoard callback\nmodel.fit(X_train, y_train, epochs=10, validation_data=(X_test, y_test), callbacks=[tensorboard_callback])\n```", "```py\n# Save your TensorFlow model in the SavedModel format\nmodel.save(\"my_model/1/\")\n\n# Install TensorFlow Serving\necho \"deb [arch=amd64] http://storage.googleapis.com/tensorflow-serving-apt stable tensorflow-model-server tensorflow-model-server-universal\" | sudo tee /etc/apt/sources.list.d/tensorflow-serving.list && \\\ncurl https://storage.googleapis.com/tensorflow-serving-apt/tensorflow-serving.release.pub.gpg | sudo apt-key add -\nsudo apt-get update && sudo apt-get install tensorflow-model-server\n\n# Start TensorFlow Serving with your model\ntensorflow_model_server --rest_api_port=8501 --model_name=my_model --model_base_path=$(pwd)/my_model\n```", "```py\nimport optuna\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.ensemble import RandomForestClassifier\n\ndef objective(trial):\n    n_estimators = trial.suggest_int(\"n_estimators\", 10, 200)\n    max_depth = trial.suggest_int(\"max_depth\", 3, 20)\n\n    clf = RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth)\n    score = cross_val_score(clf, X_train, y_train, cv=5).mean()\n\n    return score\n\nstudy = optuna.create_study(direction=\"maximize\")\nstudy.optimize(objective, n_trials=50)\n\nbest_params = study.best_params\n```", "```py\nimport shap\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor\n\n# Load and prepare your data\n# ...\n\n# Train a RandomForestRegressor\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\nmodel = RandomForestRegressor()\nmodel.fit(X_train, y_train)\n\n# Explain the model's predictions using SHAP\nexplainer = shap.Explainer(model)\nshap_values = explainer(X_test)\n\n# Plot the SHAP values for a single prediction\nshap.plots.waterfall(shap_values[0])\n```", "```py\nfrom ray import tune\nfrom ray.tune.schedulers import ASHAScheduler\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.ensemble import RandomForestClassifier\n\ndef train_model(config):\n    n_estimators = config[\"n_estimators\"]\n    max_depth = config[\"max_depth\"]\n\n    clf = RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth)\n    score = cross_val_score(clf, X_train, y_train, cv=3).mean()\n\n    tune.report(mean_accuracy=score)\n\n# Load your data\nX, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n\n# Define the search space for hyperparameters\nconfig = {\n    \"n_estimators\": tune.randint(10, 200),\n    \"max_depth\": tu:ne.randint(3, 20)\n}\n\n# Set up Ray Tune\nscheduler = ASHAScheduler(metric=\"mean_accuracy\", mode=\"max\")\nanalysis = tune.run(train_model, config=config, scheduler=scheduler, num_samples=50)\n\n# Get the best hyperparameters\nbest_params = analysis.best_config\n```", "```py\nimport mlflow\nimport mlflow.sklearn\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\n\n# Load your data\niris = load_iris()\nX, y = iris.data, iris.target\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n\n# Train your model and log metrics with MLflow\nwith mlflow.start_run():\n    clf = RandomForestClassifier(n_estimators=100, max_depth=10)\n    clf.fit(X_train, y_train)\n\n    train_accuracy = clf.score(X_train, y_train)\n    test_accuracy = clf.score(X_test, y_test)\n\n    mlflow.log_param(\"n_estimators\", 100)\n    mlflow.log_param(\"max_depth\", 10)\n    mlflow.log_metric(\"train_accuracy\", train_accuracy)\n    mlflow.log_metric(\"test_accuracy\", test_accuracy)\n\n    mlflow.sklearn.log_model(clf, \"model\")\n```", "```py\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\n\npipe = Pipeline([\n    (\"scaler\", StandardScaler()),\n    (\"classifier\", LogisticRegression())\n])\n\npipe.fit(X_train, y_train)\n```", "```py\nfrom sklearn.model_selection import GridSearchCV\n\nparam_grid = {\n    \"classifier__C\": [0.1, 1, 10],\n    \"classifier__penalty\": [\"l1\", \"l2\"]\n}\n\ngrid_search = GridSearchCV(pipe, param_grid, cv=5)\ngrid_search.fit(X_train, y_train)\n```", "```py\nimport joblib\n\n# Save the model\njoblib.dump(grid_search.best_estimator_, \"model.pkl\")\n\n# Load the model\nloaded_model = joblib.load(\"model.pkl\")\n```", "```py\nimport tensorflow as tf\n\nmodel = tf.keras.Sequential([\n    tf.keras.layers.Dense(64, activation=\"relu\", input_shape=(10,)),\n    tf.keras.layers.Dense(32, activation=\"relu\"),\n    tf.keras.layers.Dense(1, activation=\"sigmoid\")\n])\n\nmodel.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n```", "```py\nearly_stopping = tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=3)\n\nhistory = model.fit(X_train, y_train, epochs=100, validation_split=0.2, callbacks=[early_stopping])\n```", "```py\n# Save the model\nmodel.save(\"model.h5\")\n\n# Load the model\nloaded_model = tf.keras.models.load_model(\"model.h5\")\n```", "```py\nimport dask.array as da\n\nx = da.ones((10000, 10000), chunks=(1000, 1000))\ny = x + x.T\nz = y.sum(axis=0)\nresult = z.compute()\n```", "```py\nfrom tpot import TPOTClassifier\n\ntpot = TPOTClassifier(generations=5, population_size=20, verbosity=2)\ntpot.fit(X_train, y_train)\n```", "```py\nimport category_encoders as ce\n\nencoder = ce.TargetEncoder()\nX_train_encoded = encoder.fit_transform(X_train, y_train)\nX_test_encoded = encoder.transform(X_test)\n```", "```py\nfrom imblearn.over_sampling import SMOTE\n\nsmote = SMOTE()\nX_resampled, y_resampled = smote.fit_resample(X_train, y_train)\n```", "```py\nfrom autosklearn.classification import AutoSklearnClassifier\n\nauto_classifier = AutoSklearnClassifier(time_left_for_this_task=600)\nauto_classifier.fit(X_train, y_train)\n```", "```py\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        (\"num\", StandardScaler(), [\"numerical_feature_1\", \"numerical_feature_2\"]),\n        (\"cat\", OneHotEncoder(), [\"categorical_feature\"]),\n    ]\n)\n\nX_train_transformed = preprocessor.fit_transform(X_train)\nX_test_transformed = preprocessor.transform(X_test)\n```", "```py\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom scipy.stats import uniform\n\nparam_dist = {\n    \"classifier__C\": uniform(loc=0, scale=4),\n    \"preprocessor__num__with_mean\": [True, False],\n}\n\nrandom_search = RandomizedSearchCV(pipe, param_dist, n_iter=10, cv=5, scoring=\"accuracy\")\nrandom_search.fit(X_train, y_train)\n```", "```py\nimport tensorflow_data_validation as tfdv\n\nstats = tfdv.generate_statistics_from_csv(data_location=\"train.csv\")\nschema = tfdv.infer_schema(statistics=stats)\ntfdv.display_schema(schema=schema)\n```", "```py\nimport tensorflow_model_analysis as tfma\n\neval_shared_model = tfma.default_eval_shared_model(\n    eval_saved_model_path=\"path/to/saved_model\"\n)\nresults = tfma.run_model_analysis(\n    eval_shared_model=eval_shared_model,\n    data_location=\"test.tfrecords\",\n    file_format=\"tfrecords\",\n    slice_spec=[tfma.slicer.SingleSliceSpec()]\n)\ntfma.view.render_slicing_metrics(results)\n```", "```py\nimport tensorflow_transform as tft\n\ndef preprocessing_fn(inputs):\n    outputs = {}\n    outputs[\"scaled_feature\"] = tft.scale_to_z_score(inputs[\"numerical_feature\"])\n    outputs[\"one_hot_feature\"] = tft.compute_and_apply_vocabulary(inputs[\"categorical_feature\"])\n    return outputs\n```", "```py\nfrom tfx.components import CsvExampleGen, Trainer\nfrom tfx.orchestration.experimental.interactive.interactive_context import InteractiveContext\n\ncontext = InteractiveContext()\n\nexample_gen = CsvExampleGen(input_base=\"path/to/data\")\ncontext.run(example_gen)\n\ntrainer = Trainer(\n    module_file=\"path/to/trainer_module.py\",\n    examples=example_gen.outputs[\"examples\"],\n    train_args=trainer_pb2.TrainArgs(num_steps=10000),\n    eval_args=trainer_pb2.EvalArgs(num_steps=5000)\n)\ncontext.run(trainer)\n```", "```py\nimport cupy as cp\n\nx = cp.array([1, 2, 3, 4, 5])\ny = cp.array([6, 7, 8, 9, 10])\n\nz = cp.dot(x, y)\n```", "```py\nimport cudf\nimport cuml\n\ndf = cudf.read_csv(\"data.csv\")\nkmeans_model = cuml.KMeans(n_clusters=5)\nkmeans_model.fit(df)\n```", "```py\nfrom fastapi import FastAPI\nimport uvicorn\n\napp = FastAPI()\n\n@app.post(\"/predict\")\nasync def predict(text: str):\n    prediction = model.predict([text])\n    return {\"prediction\": prediction}\n\nif __name__ == \"__main__\":\n    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n```", "```py\nimport streamlit as st\n\nst.title(\"My Streamlit App\")\n\ninput_text = st.text_input(\"Enter some text:\")\nst.write(f\"You entered: {input_text}\")\n\nslider_value = st.slider(\"Select a value:\", 0, 100, 50)\nst.write(f\"Slider value: {slider_value}\")\n```", "```py\nFROM python:3.8\n\nWORKDIR /app\nCOPY requirements.txt .\nRUN pip install --no-cache-dir -r requirements.txt\n\nCOPY . .\n\nCMD [\"python\", \"app.py\"]\n```", "```py\ndocker build -t my_ml_app:latest .\n```", "```py\ndocker run -p 5000:5000 my_ml_app:latest\n```", "```py\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: my-ml-app\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: my-ml-app\n  template:\n    metadata:\n      labels:\n        app: my-ml-app\n    spec:\n      containers:\n      - name: my-ml-app-container\n        image: my_ml_app:latest\n        ports:\n        - containerPort: 5000\n\n---\n\napiVersion: v1\nkind: Service\nmetadata:\n  name: my-ml-app-service\nspec:\n  selector:\n    app: my-ml-app\n  ports:\n    - protocol: TCP\n      port: 80\n      targetPort: 5000\n  type: LoadBalancer\n```", "```py\n# Apply the Kubernetes configuration file\nkubectl apply -f my_ml_app.yaml\n\n# List all deployments\nkubectl get deployments\n\n# List all services\nkubectl get services\n\n# Scale the deployment\nkubectl scale deployment my-ml-app --replicas=5\n\n# Delete the deployment and service\nkubectl delete -f my_ml_app.yaml\n```", "```py\nmy_ml_project/\n|-- data/\n|   |-- raw/\n|   |-- processed/\n|-- models/\n|-- notebooks/\n|-- src/\n|   |-- features/\n|   |-- models/\n|   |-- utils/\n|-- Dockerfile\n|-- requirements.txt\n```", "```py\nimport papermill as pm\n\npm.execute_notebook(\n    input_path='input_notebook.ipynb',\n    output_path='output_notebook.ipynb',\n    parameters={'param1': 'value1', 'param2': 'value2'}\n)\n```", "```py\n# Create a new Conda environment\nconda create -n my_ml_env python=3.8\n\n# Activate the environment\nconda activate my_ml_env\n\n# Install packages\nconda install pandas scikit-learn\n\n# Deactivate the environment\nconda deactivate\n```", "```py\nimport numpy as np\nimport pandas as pd\nfrom sklearn.linear_model import LinearRegression\n\nchunksize = 10000\nmodel = LinearRegression()\n\nfor i, chunk in enumerate(pd.read_csv(\"large_dataset.csv\", chunksize=chunksize)):\n    X_chunk = chunk.drop(\"target\", axis=1)\n    y_chunk = chunk[\"target\"]\n    model.partial_fit(X_chunk, y_chunk)\n    print(f\"Processed chunk {i + 1}\")\n```", "```py\nimport pandas as pd\nfrom sklearn.preprocessing import OneHotEncoder\n\ndata = pd.DataFrame({\"Category\": [\"A\", \"B\", \"A\", \"C\"]})\n\nencoder = OneHotEncoder()\nencoded_data = encoder.fit_transform(data)\n\nprint(encoded_data.toarray())\n```", "```py\nimport pandera as pa\nfrom pandera import DataFrameSchema, Column, Check\n\nschema = DataFrameSchema({\n    \"age\": Column(pa.Int, Check(lambda x: 18 <= x <= 100)),\n    \"income\": Column(pa.Float, Check(lambda x: x >= 0)),\n    \"gender\": Column(pa.String, Check(lambda x: x in [\"M\", \"F\", \"Other\"])),\n})\n\n# Validate your DataFrame\nvalidated_df = schema.validate(df)\n```", "```py\n# Initialize DVC in your project\ndvc init\n\n# Add your dataset to DVC\ndvc add data/my_dataset\n\n# Commit the changes to your Git repository\ngit add data/my_dataset.dvc .dvc/config\ngit commit -m \"Add my_dataset to DVC\"\n```", "```py\nfrom feast import FeatureStore\n\n# Initialize the feature store\nstore = FeatureStore(repo_path=\"path/to/your/feature_store\")\n\n# Fetch features for training\ntraining_df = store.get_historical_features(\n    entity_df=entity_df,\n    feature_refs=[\"your_feature_name\"]\n).to_df()\n\n# Fetch features for serving\nfeature_vector = store.get_online_features(\n    feature_refs=[\"your_feature_name\"],\n    entity_rows=[{\"your_entity_key\": \"your_value\"}]\n).to_dict()\n```", "```py\nfrom sklearn.datasets import load_iris\nfrom sklearn.preprocessing import StandardScaler\n\nX, y = load_iris(return_X_y=True)\n\n# Scale features using standard scaling\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n```", "```py\nfrom sklearn.datasets import load_iris\nfrom sklearn.decomposition import PCA\n\nX, y = load_iris(return_X_y=True)\n\n# Apply PCA to reduce the dimensionality of the dataset\npca = PCA(n_components=2)\nX_reduced = pca.fit_transform(X)\n```", "```py\nimport pandas as pd\n\ndata = pd.read_csv(\"my_data.csv\")\n\n# Chain Pandas operations\nresult = (\n    data.query(\"age >= 30\")\n    .groupby(\"city\")\n    .agg({\"salary\": \"mean\"})\n    .sort_values(\"salary\", ascending=False)\n)\n```", "```py\nimport pandas as pd\n\ndef custom_operation(df, column, value):\n    return df[df[column] > value]\n\ndata = pd.read_csv(\"data.csv\")\n\n# Integrate custom operations using 'pipe'\nresult = (\n    data.pipe(custom_operation, \"age\", 18)\n    .groupby(\"city\")\n    .agg({\"salary\": \"mean\"})\n    .sort_values(\"salary\", ascending=False)\n)\n```", "```py\nimport pandas as pd\n\ndata = pd.read_csv(\"my_data.csv\")\n\n# Create a bar plot of average salary by city\ndata.groupby(\"city\")[\"salary\"].mean().plot(kind=\"bar\")\n```", "```py\nimport pandas as pd\nimport missingno as msno\n\ndata = pd.read_csv(\"data.csv\")\n\n# Visualize missing data\nmsno.matrix(data)\n```", "```py\nimport sqlite3\n\n# Connect to an SQLite database\nconn = sqlite3.connect('example.db')\n\n# Create a table\nconn.execute('CREATE TABLE IF NOT EXISTS my_table (id INTEGER PRIMARY KEY, name TEXT)')\n\n# Insert some data\nconn.execute('INSERT INTO my_table (id, name) VALUES (?, ?)', (1, 'John'))\nconn.execute('INSERT INTO my_table (id, name) VALUES (?, ?)', (2, 'Jane'))\n\n# Commit the changes\nconn.commit()\n\n# Retrieve data\ncursor = conn.execute('SELECT * FROM my_table')\nfor row in cursor:\n    print(row)\n```", "```py\nimport requests\n\n# make a GET request to a website\nresponse = requests.get('https://www.google.com')\n\n# print the response content\nprint(response.content)\n```", "```py\nimport os\n\n# create a directory\nos.mkdir('my_directory')\n```", "```py\nimport json\n\ndata = {\n    \"name\": \"Mark\",\n    \"age\": 28,\n    \"gender\": \"Male\"\n}\n\njson_data = json.dumps(data)\nprint(json_data)\n```", "```py\nimport json\n\njson_data = '{\"name\": \"Mark\", \"age\": 28, \"gender\": \"Male\"}'\n\ndata = json.loads(json_data)\nprint(data)\n```", "```py\nimport csv\n\n# Reading a CSV file\nwith open('example.csv', 'r') as file:\n    csv_reader = csv.reader(file)\n    for row in csv_reader:\n        print(row)\n\n# Writing to a CSV file\nwith open('example.csv', 'w', newline='') as file:\n    csv_writer = csv.writer(file)\n    csv_writer.writerow(['Name', 'Age', 'Gender'])\n    csv_writer.writerow(['John', 25, 'Male'])\n    csv_writer.writerow(['Jane', 30, 'Female'])\n```", "```py\nfrom sqlalchemy import create_engine\n\n# Connect to a PostgreSQL database\nengine = create_engine('postgresql://username:password@host:port/database_name')\n\n# Execute a SQL query and return the results as a dataframe\nquery = \"SELECT * FROM table_name WHERE column_name > 100\"\ndf = pd.read_sql(query, engine)\n\n# Write a dataframe to a new table in the database\ndf.to_sql('new_table_name', engine)\n```", "```py\nfrom sklearn.datasets import load_iris\nfrom sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LogisticRegression\n\n# Load your data\niris = load_iris()\nX, y = iris.data, iris.target\n\n# Create a Logistic Regression model\nmodel = LogisticRegression()\n\n# Perform Recursive Feature Elimination\nrfe = RFE(model, n_features_to_select=2)\nrfe.fit(X, y)\n\n# Get the most important features\nimportant_features = rfe.support_\n```", "```py\nimport pandas as pd\nimport pyarrow as pa\nimport pyarrow.parquet as pq\n\n# Read a CSV file using pandas\ndata = pd.read_csv(\"data.csv\")\n\n# Convert the pandas DataFrame to an Apache Arrow Table\ntable = pa.Table.from_pandas(data)\n\n# Write the Arrow Table to a Parquet file\npq.write_table(table, \"data.parquet\")\n\n# Read the Parquet file into a pandas DataFrame\ndata_from_parquet = pq.read_table(\"data.parquet\").to_pandas()\n```", "```py\nfrom kafka import KafkaProducer, KafkaConsumer\n\n# Create a Kafka producer\nproducer = KafkaProducer(bootstrap_servers=\"localhost:9092\")\n\n# Send a message to a Kafka topic\nproducer.send(\"my_topic\", b\"Hello, Kafka!\")\n\n# Create a Kafka consumer\nconsumer = KafkaConsumer(\"my_topic\", bootstrap_servers=\"localhost:9092\")\n\n# Consume messages from the Kafka topic\nfor msg in consumer:\n    print(msg.value)\n```", "```py\nimport pandas as pd\nimport pyarrow as pa\nimport pyarrow.parquet as pq\n\n# Read a CSV file using pandas\ndata = pd.read_csv(\"data.csv\")\n\n# Convert the pandas DataFrame to an Apache Arrow Table\ntable = pa.Table.from_pandas(data)\n\n# Write the Arrow Table to a partitioned Parquet dataset\npq.write_to_dataset(table, root_path=\"partitioned_data\", partition_cols=[\"state\"])\n\n# Read the partitioned Parquet dataset into a pandas DataFrame\ndata_from_partitioned_parquet = pq.ParquetDataset(\"partitioned_data\").read().to_pandas()\n```", "```py\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\n\n# Define an image data generator for data augmentation\ndatagen = ImageDataGenerator(\n    rotation_range=20,\n    width_shift_range=0.2,\n    height_shift_range=0.2,\n    horizontal_flip=True,\n)\n\n# Load your data\n(x_train, y_train), (_, _) = tf.keras.datasets.cifar10.load_data()\nx_train = x_train.astype(np.float32) / 255.0\n\n# Fit the data generator to your data\ndatagen.fit(x_train)\n\n# Train your model with augmented data\nmodel = create_your_model()\nmodel.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\nmodel.fit(datagen.flow(x_train, y_train, batch_size=32), epochs=10)\n```", "```py\nfrom flask import Flask, request, jsonify\nimport joblib\n\napp = Flask(__name__)\n\n@app.route('/predict', methods=['POST'])\ndef predict():\n    data = request.get_json()\n    features = [data['feature1'], data['feature2'], data['feature3']]\n    model = joblib.load('model.pkl')\n    prediction = model.predict([features])[0]\n    response = {'prediction': int(prediction)}\n    return jsonify(response)\n\nif __name__ == '__main__':\n    app.run()\n```", "```py\n# math_operations.py\n\ndef add(a, b):\n    return a + b\n\ndef multiply(a, b):\n    return a * b\n```", "```py\n# test_math_operations.py\n\nimport math_operations\n\ndef test_add():\n    assert math_operations.add(2, 3) == 5\n    assert math_operations.add(-1, 1) == 0\n    assert math_operations.add(0, 0) == 0\n\ndef test_multiply():\n    assert math_operations.multiply(2, 3) == 6\n    assert math_operations.multiply(-1, 1) == -1\n    assert math_operations.multiply(0, 0) == 0\n```", "```py\npytest test_math_operations.py\n```", "```py\nfrom airflow import DAG\nfrom airflow.operators.python_operator import PythonOperator\nfrom datetime import datetime\n\ndef preprocess_data():\n    # Preprocess data here\n    pass\n\ndef train_model():\n    # Train model here\n    pass\n\ndefault_args = {\n    'owner': 'myname',\n    'start_date': datetime(2023, 3, 15),\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5),\n}\n\nwith DAG('my_dag', default_args=default_args, schedule_interval='@daily') as dag:\n    preprocess_task = PythonOperator(task_id='preprocess_task', python_callable=preprocess_data)\n    train_task = PythonOperator(task_id='train_task', python_callable=train_model)\n\n    preprocess_task >> train_task\n```", "```py\nimport tensorflow as tf\nfrom tensorflow.keras.applications import VGG16\n\n# Load pre-trained model\nbase_model = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n\n# Freeze base layers\nfor layer in base_model.layers:\n    layer.trainable = False\n\n# Add custom top layers\nx = base_model.output\nx = tf.keras.layers.GlobalAveragePooling2D()(x)\nx = tf.keras.layers.Dense(256, activation='relu')(x)\npredictions = tf.keras.layers.Dense(10, activation='softmax')(x)\n\n# Create new model\nmodel = tf.keras.models.Model(inputs=base_model.input, outputs=predictions)\n\n# Compile and train model\nmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\nmodel.fit(X_train, y_train, epochs=10)\n```", "```py\nimport h2o\nfrom h2o.automl import H2OAutoML\n\n# Start H2O cluster\nh2o.init()\n\n# Load data\ndata = h2o.import_file('my-data.csv')\n\n# Define target variable\ntarget = 'label'\n\n# Split data into train and test sets\ntrain, test = data.split_frame(ratios=[0.8])\n\n# Define AutoML settings\nautoml = H2OAutoML(max_models=10, seed=1234)\n\n# Train AutoML model\nautoml.train(x=data.columns, y=target, training_frame=train)\n\n# Evaluate AutoML model\npredictions = automl.leader.predict(test)\naccuracy = (predictions['predict'] == test[target]).mean()\nprint(f'Accuracy: {accuracy}')\n```", "```py\nimport numpy as np\nfrom pyod.models.knn import KNN\n\n# Load data\nX = np.load('my-data.npy')\n\n# Define anomaly detector\ndetector = KNN(n_neighbors=5)\n\n# Train detector\ndetector.fit(X)\n\n# Detect anomalies\nanomaly_scores = detector.decision_scores_\nthreshold = np.percentile(anomaly_scores, 95)\nanomalies = np.where(anomaly_scores > threshold)\n\n# Print anomalies\nprint(f'Anomalies: {anomalies}')\n```", "```py\nimport wandb\nimport tensorflow as tf\n\n# Initialize W&B\nwandb.init(project='my-project')\n\n# Load data\ndata = tf.data.TFRecordDataset('my-data.tfrecord')\n\n# Define hyperparameters\nconfig = wandb.config\nconfig.learning_rate = 0.1\nconfig.num_epochs = 10\n\n# Define model\nmodel = tf.keras.models.Sequential([\n    tf.keras.layers.Dense(32, activation='relu'),\n    tf.keras.layers.Dense(10, activation='softmax')\n])\nmodel.compile(optimizer=tf.keras.optimizers.Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n\n# Train model\nhistory = model.fit(data.batch(32), epochs=config.num_epochs)\n\n# Log metrics and artifacts to W&B\nwandb.log({'accuracy': history.history['accuracy'][-1]})\nwandb.log_artifact('my-model.h5')\n```", "```py\nimport zlib\n\n# Compress data with zlib\ndata_compressed = zlib.compress(data)\n\n# Decompress data with zlib\ndata_decompressed = zlib.decompress(data_compressed)\n```", "```py\nimport json\n\n# Serialize data to JSON\ndata_json = json.dumps(data)\n\n# Deserialize data from JSON\ndata_deserialized = json.loads(data_json) \n```", "```py\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\n\n# Standardize data with Z-score normalization\nscaler = StandardScaler()\ndata_normalized = scaler.fit_transform(data)\n\n# Scale data with min-max scaling\nscaler = MinMaxScaler()\ndata_scaled = scaler.fit_transform(data)\n```", "```py\nfrom cryptography.fernet import Fernet\n\n# Generate encryption key with Fernet\nkey = Fernet.generate_key()\n\n# Encrypt data with Fernet\ncipher_suite = Fernet(key)\nencrypted_data = cipher_suite.encrypt(data)\n\n# Decrypt data with Fernet\ndecrypted_data = cipher_suite.decrypt(encrypted_data)\n\nimport hashlib\n\n# Hash data with hashlib\nhash_value = hashlib.sha256(data.encode('utf-8')).hexdigest()\n\nimport tokenizers\n\n# Define tokenization with tokenizers\ntokenizer = tokenizers.Tokenizer(tokenizers.models.WordPiece('vocab.txt', unk_token='[UNK]'))\nencoded_data = tokenizer.encode(data).ids\n```", "```py\nimport great_expectations as ge\n\n# Load a dataset (e.g., a Pandas DataFrame)\ndata = ge.read_csv(\"data.csv\")\n\n# Create an Expectation Suite\nexpectation_suite = data.create_expectation_suite(\"my_suite\")\n\n# Add expectations\ndata.expect_column_values_to_be_unique(\"id\")\ndata.expect_column_values_to_not_be_null(\"name\")\ndata.expect_column_mean_to_be_between(\"age\", min_value=20, max_value=40)\n\n# Validate data against the Expectation Suite\nvalidation_result = data.validate(expectation_type=\"basic\")\n\n# Save the Expectation Suite and the validation result\nge.save_expectation_suite(expectation_suite, \"my_suite.json\")\nge.save_validation_result(validation_result, \"my_suite_validation.json\")\n```", "```py\nimport logging\n\nlogging.basicConfig(level=logging.INFO)\nlogging.info(\"This is an info message.\")\nlogging.error(\"This is an error message.\")\n```", "```py\nimport dask.dataframe as dd\n\n# Read CSV file using Dask (file is partitioned into smaller chunks)\nddf = dd.read_csv('large_file.csv')\n\n# Perform operations on the data (lazy evaluation)\nfiltered_ddf = ddf[ddf['column_A'] > 10]\nmean_value = filtered_ddf['column_B'].mean()\n\n# Compute the result (operations are executed in parallel)\nresult = mean_value.compute()\nprint(\"Mean of column B for rows where column A > 10:\", result)\n```"]