- en: What Is Partial Information Decomposition and How Features Interact
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/what-is-partial-information-decomposition-and-how-features-interact-a713456a1029?source=collection_archive---------3-----------------------#2023-12-08](https://towardsdatascience.com/what-is-partial-information-decomposition-and-how-features-interact-a713456a1029?source=collection_archive---------3-----------------------#2023-12-08)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: How information about a target variable is distributed across its multiple features
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@rodrigopesilva?source=post_page-----a713456a1029--------------------------------)[![Rodrigo
    Silva](../Images/d260f05ed9887c5072e0590db1481be2.png)](https://medium.com/@rodrigopesilva?source=post_page-----a713456a1029--------------------------------)[](https://towardsdatascience.com/?source=post_page-----a713456a1029--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----a713456a1029--------------------------------)
    [Rodrigo Silva](https://medium.com/@rodrigopesilva?source=post_page-----a713456a1029--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F222e82adf972&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-is-partial-information-decomposition-and-how-features-interact-a713456a1029&user=Rodrigo+Silva&userId=222e82adf972&source=post_page-222e82adf972----a713456a1029---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----a713456a1029--------------------------------)
    ·10 min read·Dec 8, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fa713456a1029&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-is-partial-information-decomposition-and-how-features-interact-a713456a1029&user=Rodrigo+Silva&userId=222e82adf972&source=-----a713456a1029---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fa713456a1029&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-is-partial-information-decomposition-and-how-features-interact-a713456a1029&source=-----a713456a1029---------------------bookmark_footer-----------)![](../Images/1b6c41b4f07a4b381d9da086ab84a80c.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Photo by Alina Grubnyak, via [Unsplash](https://unsplash.com/photos/low-angle-photography-of-metal-structure-ZiQkhI7417A).
  prefs: []
  type: TYPE_NORMAL
- en: When a target variable is influenced by multiple sources of information, it
    is crucial (and yet not trivial) to understand how each source contributes to
    the overall information provided.
  prefs: []
  type: TYPE_NORMAL
- en: In this article I'll start with the basic concept of surprise, then I'll proceed
    to explain how entropy consists of the average amount of surprise distributed
    over a random variable, and this gives us the conditions to define mutual information.
    After this, I talk about partial information decomposition for cases where we
    have multiple sources of information.
  prefs: []
  type: TYPE_NORMAL
- en: Surprise and Entropy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Maybe one of the most intuitive ways of defining Entropy from an Information
    standpoint is to first talk about *surprise*. The measure of surprise works just
    as how we expect: less probable events are more surprising, while more probable
    events are less surprising. The mathematical definition that encompasses these
    properties is the one shown below:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/504cc18591fa38a92ef3cbae196f492a.png)'
  prefs: []
  type: TYPE_IMG
- en: We can see by the graph in Figure 1 that this definition is pretty related to
    the properties we talked about. When some event has a high chance of happening
    (p closer to 1), then the surprise is close to zero. On the other hand, if an
    event has a very low probability of happening, its surprise gets arbitrarily large.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/30eaeac06aa5aba94f96381493b2d47c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: The graph of surprise. Image by author.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, what does entropy have to do with surprise? Well, entropy is the average
    surprise over all the values of a random variable. Therefore, if we have some
    random variable X, and the set of all possible outcomes of X is called A_X (we
    call it the "alphabet of X"), then entropy H is defined as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/fe145a377dca1c49c7254829cdcbf2cd.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Great. Now we tied up entropy with surprise, we can understand one useful interpretation
    of entropy:'
  prefs: []
  type: TYPE_NORMAL
- en: Entropy is a measure of ignorance.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'How can this be? I will explain it with a silly example. Imagine that you have
    to take a final physics exam. Within the language we have developed so far, we
    can consider the test as a random variable with some alphabet of possible questions.
    Suppose now two scenarios:'
  prefs: []
  type: TYPE_NORMAL
- en: You studied hard for this exam and you know what kind of questions will be in
    the exam, so *on average*, you will not be so surprised by your exam.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You didn't really study and you don't know which type of question will be in
    the exam, so your level of surprise will be pretty high throughout the exam.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: So when your average surprise is bigger coincides perfectly with the scenario
    where you don't have as much information.
  prefs: []
  type: TYPE_NORMAL
- en: Speaking from a technical standpoint, more peaked distributions (e.g. distributions
    where certain values are more likely to happen than others) have a lower entropy
    than more dispersed ones, where every event has about the same chance of happening.
    That is why we say that the distribution with the highest entropy is the uniform
    distribution, where any value can happen with the same chance.
  prefs: []
  type: TYPE_NORMAL
- en: Entropy and (Mutual) Information
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we have established a measure of average surprise on a system described
    by a random variable (this is the entropy), we can create the link of entropy
    with information.
  prefs: []
  type: TYPE_NORMAL
- en: 'Since entropy is a measure of ignorance over some system, the lack of it represents…
    *information*. In this sense, it is pretty natural to create a measure called
    mutual information: it measures the information you gain once you know some information
    about the system:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c1a89b1c6cd1d70018c7e024903efd3f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This definition says: take the average surprise of a random variable X, then
    take the average surprise of the random variable X, but now consider that we know
    the outcome of another random variable Y. Subtract the former by the latter, and
    you know how much *ignorance* you removed from your system X by knowing about
    Y.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s come back to our silly example: suppose you don''t know what questions
    will be asked within your test, and this is X. Now suppose that a friend of yours
    has made a test from the same teacher, about the same subject, one week before
    your test. He tells you everything that *his* test covered (which happens to be
    another random variable Y). The most plausible to say is that your ignorance from
    your test has dropped, which means your test X and your friend''s test Y share
    information.'
  prefs: []
  type: TYPE_NORMAL
- en: In Figure 2 there is a nice, comprehensible Venn Diagram showing the relation
    between the entropies and the information shared between a pair of variables X
    and Y.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0014751e4795774787d549300583f9c7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: Mutual Information scheme. Image by author, heavily inspired by many
    others.'
  prefs: []
  type: TYPE_NORMAL
- en: But what if we have multiple sources of information?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far we have only talked about cases where we have one feature X and one target
    variable Y, but it is quite obvious that this does not generalize well. Hence,
    now imagine we have a random variable Y (say, a target variable from a classification
    model) and we want to know the amount of information provided by each one of the
    n features of the model X_1, X_2, …, X_n. One could say that it suffices to calculate
    the mutual information shared by X_1 and Y, then by X_2 and Y, and so on. Well,
    in the real world, our features can interact among them and create non-trivial
    relations, and if we want to have a consistent framework we have to take these
    interactions into account.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s take the case where we have two input signals X_1 and X_2, and we want
    to quantify the mutual information between these two features and a target feature
    Y. That is, we want to calculate I(Y; {X_1, X_2}). The Partial Information Decomposition
    framework states that this information can be divided into four non-negative components:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Syn**(Y; {X_1, X_2}): the Synergy of the two features. This is an amount
    of information about Y provided solely by the two features together.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Rdn**(Y; {X_1, X_2}): the Redundancy of the two features. This quantity accounts
    for the information about Y that can be explained either by X_1 or X_2 alone.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Unq**(Y; X_1) and **Unq**(Y; X_2): the Unique Information, which measures
    the information about Y that only X_1 can explain for Unq(Y; X_1) or that only
    X_2 can explain for Unq(Y; X_2).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Notice that only **Unq**(Y; X_1) and **Unq**(Y; X_2) correspond to a scenario
    of no interaction between features. Hence, the mutual information **I**(Y; {X_1,
    X_2}) can be decomposed into its four components:'
  prefs: []
  type: TYPE_NORMAL
- en: '**I**(Y; {X_1, X_2}) = **Syn**(Y; {X_1, X_2}) + **Rdn**(Y; {X_1, X_2}) + **Unq**(Y;
    X_1) + **Unq**(Y; X_2)'
  prefs: []
  type: TYPE_NORMAL
- en: Just as before, we can draw a nice Venn diagram that summarizes the dependency
    of these quantities.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/290f0c460177727a0f6657892086ec8c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Venn diagram for partial information decomposition. Image by author,
    heavily inspired by [1].'
  prefs: []
  type: TYPE_NORMAL
- en: Each of these terms is called an *atom of information*. Any non-atomic information
    can be decomposed into atomic parts, that cannot be decomposed.
  prefs: []
  type: TYPE_NORMAL
- en: 'It was Williams and Beer [1] who first proposed this framework (and came up
    with a way of calculating partial information). It turns out that calculating
    these quantities is not trivial and deserves an article by itself. There is more
    than one measure of partial information decomposition, and all of them follow
    the same process: they imagine a measure that satisfies a series of nice-to-have
    characteristics and that is consistent with what we expect to happen with some
    quantity called "information". All these measurements have strong and weak spots,
    and they are nicely implemented in `dit` library, which will be briefly introduced
    and used to give some examples in the following section.'
  prefs: []
  type: TYPE_NORMAL
- en: Partial Information Decomposition examples and the dit library
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To tie these concepts together, let’s see some examples. The `dit` library is
    a great tool for these experiments when it comes to information theory concepts.
    It is a lib that consists of creating customized probability distributions, and
    then performing measurements over them. There are several features within this
    library, and they can be found on their [Github](https://github.com/dit/dit) or
    at the official [documentation page](https://dit.readthedocs.io/en/latest/generalinfo.html).
  prefs: []
  type: TYPE_NORMAL
- en: For all examples to come, we can think of two features X_1 and X_2, both of
    them binary, and the target variable Y is some boolean operation with the features.
    All forms of measuring partial information will be due to Williams and Beer [1],
    but other forms proposed by other authors are also implemented in `dit` .
  prefs: []
  type: TYPE_NORMAL
- en: Unique information
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For this example, imagine that the target variable Y is given by Fig. 4\. Notice
    that the output is always equal to the feature X_1, which makes the feature X_2
    completely irrelevant.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/62395ed9352922437eda396dc65460ea.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: Diagram of AND gate and a unique source of information.'
  prefs: []
  type: TYPE_NORMAL
- en: For this reason, the information that X_1 and X_2 provide about Y is fully concentrated
    in X_1\. In the formalism we have developed so far, we can say that the information
    about Y is *unique* to X_1.
  prefs: []
  type: TYPE_NORMAL
- en: 'In `dit` library, we can create this as:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The `dit` library encodes the type of information as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '{0:1}: the synergistic information between X_1 and X_2'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '{0}: unique information provided by X_1'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '{1}: unique information provided by X_2'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '{0}{1}: redundant information provided by X_1 and X_2'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can see by the output that the only partial information (the "pi" column)
    provided is from X_1.
  prefs: []
  type: TYPE_NORMAL
- en: Redundant Information
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The next example serves to show the redundant information. Here, both X_1, X_2,
    and Y are equal as shown in Fig. 5, so the redundant information about Y provided
    by X_1 and X_2 is maximal.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8fbdf8d7dbbd179f741c0705abdb5843.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: Fully redundant information.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Using `dit` the code goes as:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: As we can see, the only information about Y provided by X_1 and X_2 is redundant,
    in other words, provided by both of them.
  prefs: []
  type: TYPE_NORMAL
- en: Synergistic Information
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A classic example of synergistic information is the XOR gate. The diagram for
    the XOR gate is shown in Fig. 6.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/76e8db6da9b520b9098e9af1875d24ed.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: The XOR gate with fully synergistic information'
  prefs: []
  type: TYPE_NORMAL
- en: 'Notice by this distribution that we can only know the target variable Y once
    we know both X_1 and X_2\. It is not possible to know Y without X_1 and X_2, simply
    because for each value of X_1 we have both values for Y; and the same goes for
    X_2\. The code in`dit` goes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: As expected, the only information about Y that X_1 and X_2 convey is {0:1},
    which is the synergistic information.
  prefs: []
  type: TYPE_NORMAL
- en: Final comments and takeaways
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Finally, we can see that the interaction between variables can pose a difficult
    challenge when we have at our disposal only mutual information. There needs to
    be some tool to measure information coming from multiple sources (and possibly
    the interaction between these sources of information). This is a perfect ground
    for the Partial Information Decomposition (PID) framework.
  prefs: []
  type: TYPE_NORMAL
- en: 'Usually, the measurements in this field are convoluted and involve some formal
    logic: this can be left for another thorough article about this topic, but now
    it suffices to say that these tools are not only important, but their need arises
    naturally from the information approach.'
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[1] P. L. Williams and R. D. Beer, [Nonnegative decomposition of multivariate
    information](https://arxiv.org/pdf/1004.2515.pdf), *arXiv preprint arXiv:1004.2515*,
    2010'
  prefs: []
  type: TYPE_NORMAL
- en: '[2] Shujian Yu, et al., [Understanding Convolutional Neural Networks with Information
    Theory: An Initial Exploration](https://arxiv.org/pdf/1804.06537.pdf), *arXiv
    preprint arXiv:*1804.06537v5, 2020'
  prefs: []
  type: TYPE_NORMAL
- en: '[3] A. J. Gutknecht, M. Wibral and A. Makkeh, [Bits and pieces: understanding
    information decomposition from part-whole relationships and formal logic](https://arxiv.org/pdf/2008.09535.pdf),
    *arXiv preprint arXiv:*2008.09535v2, 2022'
  prefs: []
  type: TYPE_NORMAL
- en: '[4] James, R. G., Ellison, C. J. and Crutchfield, J. P., [dit: a Python package
    for discrete information theory](https://joss.theoj.org/papers/10.21105/joss.00738.pdf),
    The Journal of Open Source Software, 2018'
  prefs: []
  type: TYPE_NORMAL
