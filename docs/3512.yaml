- en: Improving k-Means Clustering with Disentanglement
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/improving-k-means-clustering-with-disentanglement-caf59a8c57bd?source=collection_archive---------4-----------------------#2023-11-26](https://towardsdatascience.com/improving-k-means-clustering-with-disentanglement-caf59a8c57bd?source=collection_archive---------4-----------------------#2023-11-26)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Learning the dataset class neighborhood structure improves clustering
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@afagarap?source=post_page-----caf59a8c57bd--------------------------------)[![Abien
    Fred Agarap](../Images/8f616478044e721a31c1c1df3d8e8b62.png)](https://medium.com/@afagarap?source=post_page-----caf59a8c57bd--------------------------------)[](https://towardsdatascience.com/?source=post_page-----caf59a8c57bd--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----caf59a8c57bd--------------------------------)
    [Abien Fred Agarap](https://medium.com/@afagarap?source=post_page-----caf59a8c57bd--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F782adfd45f71&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimproving-k-means-clustering-with-disentanglement-caf59a8c57bd&user=Abien+Fred+Agarap&userId=782adfd45f71&source=post_page-782adfd45f71----caf59a8c57bd---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----caf59a8c57bd--------------------------------)
    ·8 min read·Nov 26, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fcaf59a8c57bd&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimproving-k-means-clustering-with-disentanglement-caf59a8c57bd&user=Abien+Fred+Agarap&userId=782adfd45f71&source=-----caf59a8c57bd---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fcaf59a8c57bd&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimproving-k-means-clustering-with-disentanglement-caf59a8c57bd&source=-----caf59a8c57bd---------------------bookmark_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: An accompanying article for the paper “[Improving k-Means Clustering with Disentangled
    Internal Representations](https://arxiv.org/abs/2006.04535)” by A.F. Agarap and
    A.P. Azcarraga presented at the 2020 International Joint Conference on Neural
    Networks (IJCNN)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Background
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Clustering is an unsupervised learning task that groups a set of objects in
    a way that the objects in a group share more similarities among them than those
    from other groups. It is a widely-studied task as its applications include but
    are not limited to its use in data analysis and visualization, anomaly detection,
    sequence analysis, and natural language processing.
  prefs: []
  type: TYPE_NORMAL
- en: Like other machine learning methods, clustering algorithms heavily rely on the
    choice of feature representation. In our work, we improve the quality of feature
    representation through disentanglement.
  prefs: []
  type: TYPE_NORMAL
- en: We define *disentanglement* as how far class-different data points from each
    other are, relative to class-similar data points. This is similar to the way the
    aforementioned term was treated in [Frosst et al. (2019)](https://arxiv.org/abs/1902.01889)
    So, maximizing disentanglement during representation learning means the distance
    among class-similar data points are minimized.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/da008ac7da864e5fa797bb0eb272a952.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure by the author.
  prefs: []
  type: TYPE_NORMAL
- en: In doing so, it would preserve the class memberships of the examples from the
    dataset, i.e. how data points reside in the feature space as a function of their
    classes or labels. If the class memberships are preserved, we would have a feature
    representation space in which a nearest neighbor classifier or a clustering algorithm
    would perform well.
  prefs: []
  type: TYPE_NORMAL
- en: Clustering
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Clustering is a machine learning task that finds the grouping of data points
    wherein the points in a group share more similarities among themselves relative
    to points in a different group.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/07b4c01ef371936a078d891401134dca.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure by the author.
  prefs: []
  type: TYPE_NORMAL
- en: Like other machine learning algorithms, the success of clustering algorithms
    relies on the choice of feature representation. One representation may be superior
    than another with respect to the dataset used. However, in deep learning, this
    is not the case since the feature representations are learned as an implicit task
    of a neural network.
  prefs: []
  type: TYPE_NORMAL
- en: Deep Clustering
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: And so, recent works such as [Deep Embedding Clustering or DEC](https://arxiv.org/abs/1511.06335)
    and [Variational Deep Embedding or VADE](https://arxiv.org/abs/1611.05148) in
    2016, and [ClusterGAN](https://arxiv.org/abs/1809.03627) in 2018, took advantage
    of the feature representation learning capability of neural networks.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7426b046529309ac105122d3f9e8e05f.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure from [DEC](https://arxiv.org/abs/1511.06335) (Xie et al., 2016). The
    network structure of DEC.
  prefs: []
  type: TYPE_NORMAL
- en: We will not discuss them in detail in this article, but the fundamental idea
    among these works is essentially the same, and that is to simultaneously learn
    the feature representations and the cluster assignments using a deep neural network.
    This approach is known as *deep clustering*.
  prefs: []
  type: TYPE_NORMAL
- en: Motivation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Can we preserve the class memberships of the data points in the dataset before
    clustering?
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Although deep clustering methods learn the clustering assignment together with
    feature representations, what they do not explicitly set out to do is to preserve
    the class neighbourhood structure of the dataset. This serves as our motivation
    for our research, and that is can we preserve the class neighbourhood structure
    of the dataset and then perform clustering on the learned representation of a
    deep network.
  prefs: []
  type: TYPE_NORMAL
- en: In 2019, the [Not Too Deep](https://ieeexplore.ieee.org/abstract/document/9413131/)
    or N2D Clustering method was proposed wherein they learned a latent code representation
    of a dataset, in which they further searched for an underlying manifold using
    techniques such as t-SNE, Isomap, and UMAP. The resulting manifold is a clustering-friendly
    representation of the dataset. So, after manifold learning, they used the learned
    manifold as the dataset features for clustering. Using this approach, they were
    able to have a good clustering performance. The N2D is a relatively simpler approach
    compared to deep clustering algorithms, and we propose a similar approach.
  prefs: []
  type: TYPE_NORMAL
- en: Learning Disentangled Representations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We also use an autoencoder network to learn the latent code representation of
    a dataset, and then use the representation for clustering. We draw the line of
    difference on how we learn a more clustering-friendly representation. Instead
    of using manifold learning techniques, we propose to disentangle the learned representations
    of an autoencoder network.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6ba7a98d115d4688d3a2cde8c5743698.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure by the author. The distances among class-similar data points are minimized,
    thus enforcing better separation of class-different data points.
  prefs: []
  type: TYPE_NORMAL
- en: To disentangle the learned representations, we use the soft nearest neighbour
    loss or SNNL which measures the entanglement of class-similar data points. What
    this loss function does is it minimizes the distances among class-similar data
    points in each of the hidden layer of a neural network. The work by Frosst, Papernot,
    and Hinton on this loss function used a fixed temperature value denoted by *T*.
    The temperature factor dictates how to control the importance given to the distances
    between pairs of points, for instance, at low temperatures, the loss is dominated
    by small distances while actual distances between widely separated representations
    become less relevant. They used SNNL for discriminative and generative tasks in
    their 2019 paper.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a2f9b75901e8cacb14d6da99992afd35.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure by the author. We got the exponent from [Neelakantan et al., 2015](https://arxiv.org/abs/1511.06807),
    but it could be of any value.
  prefs: []
  type: TYPE_NORMAL
- en: In our work, we used SNNL for clustering, and we introduce the use of an annealing
    temperature instead of a fixed temperature. Our annealing temperature is an inverse
    function with respect to the training epoch number which is denoted by τ.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/475eee7c704fde65f7a6181d3d176e96.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure by the author. Comparing the soft nearest neighbor loss with annealing
    temperature and with fixed temperature. We sampled and randomly labelled 300 data
    points from a Gaussian distribution, and ran gradient descent on them with soft
    nearest neighbor loss. The figure at the left shows the initial condition of the
    labelled points. We can see the separation of clusters in the latent code from
    epoch 20 to epoch 50, rendering the classes more isolated. We present disentangled
    representations on benchmark datasets in the [paper](https://arxiv.org/abs/2006.04535).
  prefs: []
  type: TYPE_NORMAL
- en: Running a gradient descent on a randomly sampled and labelled 300 data points
    from a Gaussian distribution, we can see that using our annealing temperature
    for SNNL, we found faster disentanglement compared to using a fixed temperature.
    As we can see, even as early as the 20th epoch, the class-similar data points
    are more clustered together or entangled when using an annealing temperature than
    when using a fixed temperature, as it is also numerically shown by the SNNL value.
  prefs: []
  type: TYPE_NORMAL
- en: Our Method
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So, our contributions are the use of SNNL for disentanglement of feature representations
    for clustering, the use of an annealing temperature for SNNL, and a simpler clustering
    approach compared to deep clustering methods.
  prefs: []
  type: TYPE_NORMAL
- en: Our method can be summarized in the following manner,
  prefs: []
  type: TYPE_NORMAL
- en: We train an autoencoder with a composite loss of binary cross entropy as the
    reconstruction loss, and the soft nearest neighbour loss as a regularizer. The
    SNNL for each hidden layer of the autoencoder is minimized to preserve the class
    neighbourhood structure of the dataset.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: After training, we use the latent code representation of a dataset as the dataset
    features for clustering.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Clustering on Disentangled Representations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Our experiment configuration is as follows,
  prefs: []
  type: TYPE_NORMAL
- en: We used the [MNIST](https://paperswithcode.com/dataset/mnist), [Fashion-MNIST](https://github.com/zalandoresearch/fashion-mnist),
    and [EMNIST](https://paperswithcode.com/dataset/emnist) Balanced benchmark datasets.
    Each image in the datasets were flattened to a 784-dimensional vector. We used
    their ground-truth labels as the pseudo-clustering labels for measuring the clustering
    accuracy of our model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We did not perform hyperparameter tuning or other training tricks due to computational
    constraints and to keep our approach simple.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Other regularizers like dropout and batch norm were omitted since they might
    affect the disentangling process.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We computed the average performance of our model across four runs, each run
    having a different random seed.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Clustering Performance
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: However, autoencoding and clustering are both unsupervised learning tasks, while
    we use SNNL, a loss function that uses labels to preserve the class neighbourhood
    structure of the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9b7eb3abb26050f8d44da63e00773af1.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure by the author.
  prefs: []
  type: TYPE_NORMAL
- en: With this in mind, we simulated the lack of labelled data by using a small subset
    of the labelled training data of the benchmark datasets. The number of labelled
    examples we used were arbitrarily chosen.
  prefs: []
  type: TYPE_NORMAL
- en: We retrieved the reported clustering accuracy of DEC, VaDE, ClusterGAN, and
    N2D from literature as baseline results, and in the table above, we can see the
    summary of our findings where our approach outperformed the baseline models.
  prefs: []
  type: TYPE_NORMAL
- en: Note that these results are the best clustering accuracy among the four runs
    for each dataset since the baseline results from literature are also the reported
    best clustering accuracy by the respective authors.
  prefs: []
  type: TYPE_NORMAL
- en: Visualizing Disentangled Representations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To further support our findings, we visualized the disentangled representations
    by our network for each of the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: For the EMNIST Balanced dataset, we randomly chose 10 classes to visualize for
    easier and cleaner visualization.
  prefs: []
  type: TYPE_NORMAL
- en: From these visualizations, we can see that the latent code representation for
    each dataset indeed became more clustering-friendly by having well-defined clusters
    as indicated by the cluster dispersion.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0cc91f7c9119650d1036dab1920bf6b3.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure by the author. 3D visualization comparing the original representation
    and the disentangled latent representation of the three datasets. To achieve this
    visualization, the representations were encoded using t-SNE with perplexity =
    50 and learning rate = 10, optimized for 5,000 iterations, with the same random
    seed set for all computations. However, for clustering, we used higher dimensionality
    to achieve better clustering performance.
  prefs: []
  type: TYPE_NORMAL
- en: Training on Fewer Labelled Examples
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We also tried training our model on fewer labelled examples.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0ff8dfcd895fbfd7171e96bbf48ccb3e.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure by the author. Test clustering accuracy on the MNIST and Fashion-MNIST
    test sets when small subsets of labelled data are used for training. Both the
    original representation and the baseline autoencoder do not take advantage of
    the labelled dataset.
  prefs: []
  type: TYPE_NORMAL
- en: In the figure above, we can see that even with fewer labelled training examples,
    the clustering performance on the disentangled representations is still on par
    with our baseline models from the literature.
  prefs: []
  type: TYPE_NORMAL
- en: This entails that in situations where labelled datasets is scarce, this method
    could be used to produce good results.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Compared to deep clustering methods, we employed a simpler clustering approach
    by using a composite loss of autoencoder reconstruction loss and soft nearest
    neighbor loss to learn a more clustering-friendly representation that improves
    the performance of a k-Means clustering algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: Our expansion of the soft nearest neighbor loss used an annealing temperature
    which helps with faster and better disentanglement that helped improve the clustering
    performance on the benchmark datasets. Thus concluding our work.
  prefs: []
  type: TYPE_NORMAL
- en: Since the publication of our work, [several other papers](https://scholar.google.com/scholar?cites=11092372353656200973&as_sdt=2005&sciodt=0%2C5&hl=en&oi=gsb)
    have built on the soft nearest neighbor loss, or were regarded to be quite similar.
    Most notably, the [supervised contrastive (SupCon) learning paper](https://proceedings.neurips.cc/paper_files/paper/2020/file/d89a66c7c80a29b1bdbab0f2a1a94af8-Paper.pdf)
    from Google, but with the difference being the SupCon approach proposed normalization
    of embeddings, an increased use of data augmentation, a disposable contrastive
    head and two-stage training.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, our work requires relatively lower hardware resources while
    achieving good results.
  prefs: []
  type: TYPE_NORMAL
- en: We discuss how to implement the soft nearest neighbor loss in the [next article](https://medium.com/@afagarap/implementing-soft-nearest-neighbor-loss-in-pytorch-b9ed2a371760).
  prefs: []
  type: TYPE_NORMAL
- en: '**References**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Frosst, Nicholas, Nicolas Papernot, and Geoffrey Hinton. “Analyzing and improving
    representations with the soft nearest neighbor loss.” *International conference
    on machine learning*. PMLR, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Goldberger, Jacob, et al. “Neighbourhood components analysis.” *Advances in
    neural information processing systems*. 2005.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Khosla, Prannay, et al. “Supervised contrastive learning.” *Advances in neural
    information processing systems* 33 (2020): 18661–18673.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Salakhutdinov, Ruslan, and Geoff Hinton. “Learning a nonlinear embedding by
    preserving class neighbourhood structure.” *Artificial Intelligence and Statistics*.
    2007.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
