["```py\n# 0\\. Clone repository\n# Navigate to the directory where you want to clone the repository and type:\ngit clone https://github.com/lenlan/clinical-trial-prediction.git\ncd clinical-trial-prediction\n\n# 1\\. Download data\nmkdir -p raw_data\ncd raw_data\nwget https://clinicaltrials.gov/AllPublicXML.zip # This will take 10-20 minutes to download\n\n# 2\\. Unzip the ZIP file.\n# The unzipped file occupies approximately 11 GB. Please make sure you have enough space. \nunzip AllPublicXML.zip # This might take over an hour to run, depending on your system\ncd ../\n\n# 3\\. Collect and sort all the XML files and put output in all_xml\nfind raw_data/ -name NCT*.xml | sort > data/all_xml\nhead -3 data/all_xml\n\n### Output:\n# raw_data/NCT0000xxxx/NCT00000102.xml\n# raw_data/NCT0000xxxx/NCT00000104.xml\n# raw_data/NCT0000xxxx/NCT00000105.xml\n\n# NCTID is the identifier of a clinical trial. `NCT00000102`, `NCT00000104`, `NCT00000105` are all NCTIDs. \n\n# 4\\. Remove ZIP file to recover some disk space\nrm raw_data/AllPublicXML.zip\n```", "```py\nfrom xml.etree import ElementTree as ET\n# function adapted from https://github.com/futianfan/clinical-trial-outcome-prediction\ndef xmlfile2results(xml_file):\n    tree = ET.parse(xml_file)\n    root = tree.getroot()\n    nctid = root.find('id_info').find('nct_id').text ### nctid: 'NCT00000102'\n    print(\"nctid is\", nctid)\n    study_type = root.find('study_type').text\n    print(\"study type is\", study_type)\n    interventions = [i for i in root.findall('intervention')]\n    drug_interventions = [i.find('intervention_name').text for i in interventions \\\n              if i.find('intervention_type').text=='Drug']\n    print(\"drug intervention:\", drug_interventions)\n    ### remove 'biologics', \n    ### non-interventions \n    if len(drug_interventions)==0:\n        return (None,)\n\n    try:\n        status = root.find('overall_status').text \n        print(\"status:\", status)\n    except:\n        status = ''\n\n    try:\n        why_stop = root.find('why_stopped').text\n        print(\"why stop:\", why_stop)\n    except:\n        why_stop = ''\n\n    try:\n        phase = root.find('phase').text\n        print(\"phase:\", phase)\n    except:\n        phase = ''\n    conditions = [i.text for i in root.findall('condition')] ### disease \n    print(\"disease\", conditions)\n\n    try:\n        criteria = root.find('eligibility').find('criteria').find('textblock').text\n        print('found criteria')\n    except:\n        criteria = ''\n\n    try:\n        enrollment = root.find('enrollment').text\n        print(\"enrollment:\", enrollment)\n    except:\n        enrollment = ''\n\n    try:\n        lead_sponsor = root.find('sponsors').find('lead_sponsor').find('agency').text \n        print(\"lead_sponsor:\", lead_sponsor)\n    except:\n        lead_sponsor = ''\n\n    data = {'nctid':nctid,\n           'study_type':study_type,\n           'drug_interventions':[drug_interventions],\n           'overall_status':status,\n           'why_stopped':why_stop,\n           'phase':phase,\n           'indications':[conditions],\n           'criteria':criteria,\n           'enrollment':enrollment,\n           'lead_sponsor':lead_sponsor}\n    return pd.DataFrame(data)\n\n### Output:\n# nctid is NCT00040014\n# study type is Interventional\n# drug intervention: ['exemestane']\n# status: Terminated\n# phase: Phase 2\n# disease ['Breast Neoplasms']\n# found criteria\n# enrollment: 100\n# lead_sponsor: Pfizer\n```", "```py\npip install -U sentence-transformers\n```", "```py\nfrom sentence_transformers import SentenceTransformer\nsentences = [\"This is an example sentence\", \"Each sentence is converted\"]\n#all-MiniLM-L6-v2 encodes each sentence into a 312-dimensional vector\nmodel = SentenceTransformer('all-MiniLM-L6-v2')\nembeddings = model.encode(sentences)\nprint(embeddings.shape)\n\n### Output:\n# (2, 312)\n```", "```py\ndef create_indication2embedding_dict():\n    # Import toy dataset\n    toy_df = pd.read_pickle('data/toy_df.pkl')\n\n    # Create list with all indications and encode each one into a 312-dimensional vector\n    all_indications = sorted(set(reduce(lambda x, y: x + y, toy_df['indications'].tolist())))     \n\n    # Using 'nlpie/tiny-biobert', a smaller version of BioBERT\n    model = SentenceTransformer('nlpie/tiny-biobert')\n    embeddings = model.encode(all_indications, show_progress_bar=True)\n\n    # Create dictionary mapping indications to embeddings\n    indication2embedding_dict = {}\n    for key, row in zip(all_indications, embeddings):\n        indication2embedding_dict[key] = row\n    pickle.dump(indication2embedding_dict, open('data/indication2embedding_dict.pkl', 'wb')) \n\n    embedding = []\n    for indication_lst in tqdm(toy_df['indications'].tolist()):\n        vec = []\n        for indication in indication_lst:\n            vec.append(indication2embedding_dict[indication])\n        print(np.array(vec).shape) # DEBUG\n        vec = np.mean(np.array(vec), axis=0)\n        print(vec.shape) # DEBUG\n        embedding.append(vec)\n    print(np.array(embedding).shape)\n\n    dict = zip(toy_df['nctid'], np.array(embedding))\n    nctid2disease_embedding_dict = {}\n    for key, row in zip(toy_df['nctid'], np.array(embedding)):\n        nctid2disease_embedding_dict[key] = row\n    pickle.dump(nctid2disease_embedding_dict, open('data/nctid2disease_embedding_dict.pkl', 'wb'))  \n\ncreate_indication2embedding_dict()\n```", "```py\ndef create_nctid2protocol_embedding_dict():\n     # Import toy dataset\n    toy_df = pd.read_pickle('data/toy_df.pkl')\n\n    # Using 'nlpie/tiny-biobert', a smaller version of BioBERT\n    model = SentenceTransformer('nlpie/tiny-biobert')\n\n    def criteria2vec(criteria):\n        embeddings = model.encode(criteria)\n#         print(embeddings.shape) # DEBUG\n        embeddings_avg = np.mean(embeddings, axis=0)\n#         print(embeddings_avg.shape) # DEBUG\n        return embeddings_avg\n\n    nctid_2_protocol_embedding = dict()\n    print(f\"Embedding {len(toy_df)*2} inclusion/exclusion criteria..\")\n    for nctid, protocol in tqdm(zip(toy_df['nctid'].tolist(), toy_df['criteria'].tolist())):    \n#         if(nctid == 'NCT00003567'): break #DEBUG\n        split = split_protocol(protocol)\n        if len(split)==2:\n            embedding = np.concatenate((criteria2vec(split[0]), criteria2vec(split[1])))\n        else: \n            embedding = np.concatenate((criteria2vec(split[0]), np.zeros(312)))\n        nctid_2_protocol_embedding[nctid] = embedding\n#         for key in nctid_2_protocol_embedding: #DEBUG\n#             print(f\"{key}:{nctid_2_protocol_embedding[key].shape}\") #DEBUG\n    pickle.dump(nctid_2_protocol_embedding, open('data/nctid_2_protocol_embedding_dict.pkl', 'wb'))   \n    return \n\ncreate_nctid2protocol_embedding_dict()\n```", "```py\ndef create_sponsor2embedding_dict():\n    # Import toy dataset\n    toy_df = pd.read_pickle('data/toy_df.pkl')\n\n    # Create list with all indications and encode each one into a 384-dimensional vector\n    all_sponsors = sorted(set(toy_df['lead_sponsor'].tolist()))     \n\n    # Using 'all-MiniLM-L6-v2', a pre-trained model with excellent performance and speed\n    model = SentenceTransformer('all-MiniLM-L6-v2')\n    embeddings = model.encode(all_sponsors, show_progress_bar=True)\n    print(embeddings.shape)\n\n    # Create dictionary mapping indications to embeddings\n    sponsor2embedding_dict = {}\n    for key, row in zip(all_sponsors, embeddings):\n        sponsor2embedding_dict[key] = row\n    pickle.dump(sponsor2embedding_dict, open('data/sponsor2embedding_dict.pkl', 'wb'))\n\ncreate_sponsor2embedding_dict()\n```", "```py\nimport requests\n\ndef get_smiles(drug_name):\n    # URL for the CIR API\n    base_url = \"https://cactus.nci.nih.gov/chemical/structure\"\n    url = f\"{base_url}/{drug_name}/smiles\"\n\n    try:\n        # Send a GET request to retrieve the SMILES representation\n        response = requests.get(url)\n\n        if response.status_code == 200:\n            smiles = response.text.strip()  # Get the SMILES string\n            print(f\"Drug Name: {drug_name}\")\n            print(f\"SMILES: {smiles}\")\n        else:\n            print(f\"Failed to retrieve SMILES for {drug_name}. Status code: {response.status_code}\")\n            smiles = ''\n\n    except requests.exceptions.RequestException as e:\n        print(f\"An error occurred: {e}\")\n\n    return smiles\n\n# Define the drug name you want to convert\ndrug_name = \"aspirin\"  # Replace with the drug name of your choice\nget_smiles(drug_name)\n\n### Output:\n# Drug Name: aspirin\n# SMILES: CC(=O)Oc1ccccc1C(O)=O\n```", "```py\npip install DeepPurpose\n```", "```py\ndef create_smiles2morgan_dict():\n    from DeepPurpose.utils import smiles2morgan \n\n    # Import toy dataset\n    toy_df = pd.read_csv('data/toy_df.csv')\n\n    smiles_lst = list(map(txt_to_lst, toy_df['smiless'].tolist()))\n    unique_smiles = set(reduce(lambda x, y: x + y, smiles_lst))\n\n    morgan = pd.Series(list(unique_smiles)).apply(smiles2morgan)\n    smiles2morgan_dict = dict(zip(unique_smiles, morgan))\n    pickle.dump(smiles2morgan_dict, open('data/smiles2morgan_dict.pkl', 'wb'))\n\ndef create_nctid2molecule_embedding_dict():\n    # Import toy dataset\n    toy_df = pd.read_csv('data/toy_df.csv')\n    smiles_lst = list(map(txt_to_lst, toy_df['smiless'].tolist()))\n    smiles2morgan_dict = load_smiles2morgan_dict()\n\n    embedding = []\n    for drugs in tqdm(smiles_lst):\n        vec = []\n        for drug in drugs:\n            vec.append(smiles2morgan_dict[drug])\n        # print(np.array(vec).shape) # DEBUG\n        vec = np.mean(np.array(vec), axis=0)\n        # print(vec.shape) # DEBUG\n        embedding.append(vec)\n    print(np.array(embedding).shape)\n\n    dict = zip(toy_df['nctid'], np.array(embedding))\n    nctid2molecule_embedding_dict = {}\n    for key, row in zip(toy_df['nctid'], np.array(embedding)):\n        nctid2molecule_embedding_dict[key] = row\n    pickle.dump(nctid2molecule_embedding_dict, open('data/nctid2molecule_embedding_dict.pkl', 'wb'))  \n\ncreate_nctid2molecule_embedding_dict()\n```"]