- en: The Subtleties of Converting a Model from TensorFlow to PyTorch
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从 TensorFlow 转换到 PyTorch 的细微差别
- en: 原文：[https://towardsdatascience.com/the-subtleties-of-converting-a-model-from-tensorflow-to-pytorch-e9acc199b8bb?source=collection_archive---------10-----------------------#2023-02-07](https://towardsdatascience.com/the-subtleties-of-converting-a-model-from-tensorflow-to-pytorch-e9acc199b8bb?source=collection_archive---------10-----------------------#2023-02-07)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/the-subtleties-of-converting-a-model-from-tensorflow-to-pytorch-e9acc199b8bb?source=collection_archive---------10-----------------------#2023-02-07](https://towardsdatascience.com/the-subtleties-of-converting-a-model-from-tensorflow-to-pytorch-e9acc199b8bb?source=collection_archive---------10-----------------------#2023-02-07)
- en: Advice and techniques to ensure success
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 确保成功的建议和技巧
- en: '[](https://medium.com/@gil.shomron?source=post_page-----e9acc199b8bb--------------------------------)[![Gil
    Shomron](../Images/4cd9c4770757a863f477a381d119de32.png)](https://medium.com/@gil.shomron?source=post_page-----e9acc199b8bb--------------------------------)[](https://towardsdatascience.com/?source=post_page-----e9acc199b8bb--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----e9acc199b8bb--------------------------------)
    [Gil Shomron](https://medium.com/@gil.shomron?source=post_page-----e9acc199b8bb--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@gil.shomron?source=post_page-----e9acc199b8bb--------------------------------)[![Gil
    Shomron](../Images/4cd9c4770757a863f477a381d119de32.png)](https://medium.com/@gil.shomron?source=post_page-----e9acc199b8bb--------------------------------)[](https://towardsdatascience.com/?source=post_page-----e9acc199b8bb--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----e9acc199b8bb--------------------------------)
    [Gil Shomron](https://medium.com/@gil.shomron?source=post_page-----e9acc199b8bb--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F6f73b54221a8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-subtleties-of-converting-a-model-from-tensorflow-to-pytorch-e9acc199b8bb&user=Gil+Shomron&userId=6f73b54221a8&source=post_page-6f73b54221a8----e9acc199b8bb---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----e9acc199b8bb--------------------------------)
    ·5 min read·Feb 7, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fe9acc199b8bb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-subtleties-of-converting-a-model-from-tensorflow-to-pytorch-e9acc199b8bb&user=Gil+Shomron&userId=6f73b54221a8&source=-----e9acc199b8bb---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F6f73b54221a8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-subtleties-of-converting-a-model-from-tensorflow-to-pytorch-e9acc199b8bb&user=Gil+Shomron&userId=6f73b54221a8&source=post_page-6f73b54221a8----e9acc199b8bb---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----e9acc199b8bb--------------------------------)
    · 5分钟阅读 · 2023年2月7日 [](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fe9acc199b8bb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-subtleties-of-converting-a-model-from-tensorflow-to-pytorch-e9acc199b8bb&user=Gil+Shomron&userId=6f73b54221a8&source=-----e9acc199b8bb---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fe9acc199b8bb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-subtleties-of-converting-a-model-from-tensorflow-to-pytorch-e9acc199b8bb&source=-----e9acc199b8bb---------------------bookmark_footer-----------)![](../Images/33e5c81524956775a1c3ab84475d8eb7.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fe9acc199b8bb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-subtleties-of-converting-a-model-from-tensorflow-to-pytorch-e9acc199b8bb&source=-----e9acc199b8bb---------------------bookmark_footer-----------)![](../Images/33e5c81524956775a1c3ab84475d8eb7.png)'
- en: Photo by [Jan Böttinger](https://unsplash.com/@bttngr?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由 [Jan Böttinger](https://unsplash.com/@bttngr?utm_source=medium&utm_medium=referral)
    提供，来自 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: The conversion of a model checkpoint from one framework to another could be
    a complex process if you want to maintain the same level of performance.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 将模型检查点从一个框架转换到另一个框架可能是一个复杂的过程，如果你想保持相同的性能水平。
- en: In the past, I was asked to evaluate [my work](https://arxiv.org/abs/2004.09309)
    on the [MLPerf inference benchmark suite](https://arxiv.org/abs/1911.02549). MLPerf
    is an industry benchmark for measuring the performance of machine learning (ML)
    models in deployment scenarios. The benchmark provides a standardized framework
    for comparing the performance of different ML systems, making it a valuable tool
    for evaluating the hardware performance of ML models. In my work, the use of models
    and checkpoints from the MLPerf inference benchmark suite allowed me to present
    results that were compliant with widely accepted industry standards, which increased
    the confidence of readers in my work. However, my entire simulation framework
    was built on top of PyTorch, whereas the model weights I needed were coded in
    TensorFlow.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 过去，我被要求评估 [我的工作](https://arxiv.org/abs/2004.09309) 在 [MLPerf 推理基准套件](https://arxiv.org/abs/1911.02549)
    上的表现。MLPerf 是一个用于测量机器学习（ML）模型在部署场景中表现的行业基准。该基准提供了一个标准化框架，用于比较不同 ML 系统的性能，使其成为评估
    ML 模型硬件性能的宝贵工具。在我的工作中，使用 MLPerf 推理基准套件中的模型和检查点使我能够呈现符合广泛接受的行业标准的结果，从而提高了读者对我工作的信心。然而，我的整个模拟框架建立在
    PyTorch 之上，而我所需的模型权重则编码在 TensorFlow 中。
- en: In this blog post, I would like to share the process of migrating a TensorFlow
    model to PyTorch, highlighting the nuances involved. It's important to note that,
    in general, small differences may exist between *any* two different frameworks;
    therefore, consider the tips here when converting between any two frameworks.
    The end results, which is MLPerf ResNet-50 and MobileNet-v1 PyTorch checkpoints,
    can be obtained from my [GitHub repository](https://github.com/gilshm/mlperf-pytorch).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇博文中，我想分享将 TensorFlow 模型迁移到 PyTorch 的过程，突显其中的细微差别。重要的是要注意，一般来说，*任何* 两个不同框架之间可能存在小差异；因此，在转换任何两个框架时，请考虑这里的提示。最终结果，即
    MLPerf ResNet-50 和 MobileNet-v1 PyTorch 检查点，可以从我的 [GitHub 仓库](https://github.com/gilshm/mlperf-pytorch)
    获得。
- en: Using the PB File
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 PB 文件
- en: 'A TensorFlow `pb` (protobuf) file comprises a description of the model graph
    as well as the layer weights and parameters. The first stage is, therefore, parsing
    the model pb file. MLPerf checkpoint files can be download from [here](https://github.com/mlperf/inference/tree/master/vision/classification_and_detection).
    Inspired by [this](https://leimao.github.io/blog/Save-Load-Inference-From-TF2-Frozen-Graph/)
    blog post, I used this class to extract all the necessary data from the pb file:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow `pb`（protobuf）文件包含模型图的描述以及层权重和参数。因此，第一步是解析模型 pb 文件。MLPerf 检查点文件可以从
    [这里](https://github.com/mlperf/inference/tree/master/vision/classification_and_detection)
    下载。受到 [这个](https://leimao.github.io/blog/Save-Load-Inference-From-TF2-Frozen-Graph/)
    博文的启发，我使用了这个类来从 pb 文件中提取所有必要的数据：
- en: Once the NeuralNetPB class has been created, it will contain all the weights
    in its “weights” attribute. With the weight tensors at hand, it’s time to assign
    them to the appropriate layers in the PyTorch model.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦创建了 `NeuralNetPB` 类，它将包含所有的权重在其“weights”属性中。拥有权重张量后，就可以将它们分配到 PyTorch 模型中的适当层。
- en: Layer Mapping
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 层映射
- en: First, it’s important to note that we don’t have the model definition as a TensorFlow
    Python file. Nevertheless, since we are working with well-known model architectures,
    the layers are largely similar.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，重要的是要注意，我们没有模型定义的 TensorFlow Python 文件。尽管如此，由于我们使用的是知名的模型架构，这些层大体上是相似的。
- en: '**Tip #1:** Use [Netron](https://lutzroeder.github.io/netron/) to visualize
    your model graph. This will be helpful in understanding the model structure within
    the `pb` file, as well as mapping the layers from the pb file to PyTorch.'
  id: totrans-17
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**提示 #1：** 使用 [Netron](https://lutzroeder.github.io/netron/) 来可视化你的模型图。这将有助于理解
    `pb` 文件中的模型结构，以及将 pb 文件中的层映射到 PyTorch。'
- en: 'Now, let’s start mapping. I will give a couple of examples from MobileNet-v1:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们开始映射。我将给出一些 MobileNet-v1 的例子：
- en: '`MobilenetV1/Conv2d_0/weights` (3-3-3-32) — This is the first convolution layer.
    We map it to `model.0.1.weight`.'
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`MobilenetV1/Conv2d_0/weights` (3-3-3-32) — 这是第一个卷积层。我们将其映射到 `model.0.1.weight`。'
- en: '`MobilenetV1/Conv2d_0/BatchNorm/gamma` (32) — This is the first batch normalization
    layer. gamma corresponds to the layer weights in PyTorch, so we map it to `model.0.2.weight`.
    The same BatchNorm layer consists a `beta`, `moving_mean`, and `moving_variance`
    fields, which corresponds to PyTorch `bias`, `running_mean`, and `running_var`,
    respectively.'
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`MobilenetV1/Conv2d_0/BatchNorm/gamma` (32) — 这是第一个批量归一化层。gamma对应于PyTorch中的层权重，因此我们将其映射为`model.0.2.weight`。相同的BatchNorm层还包含`beta`、`moving_mean`和`moving_variance`字段，它们分别对应于PyTorch中的`bias`、`running_mean`和`running_var`。'
- en: '`MobilenetV1/Conv2d_1_depthwise/depthwise_weights` (3, 3, 32, 1) — This is
    the second convolution layer (or the first depthwise convolution layer), and it
    is mapped to `model.1.weight`.'
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`MobilenetV1/Conv2d_1_depthwise/depthwise_weights` (3, 3, 32, 1) — 这是第二个卷积层（或第一个深度卷积层），它映射到`model.1.weight`。'
- en: DNN model structures are usually repetitive, so once you get the idea, you’ll
    be able to write parts in `for` loops.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: DNN模型结构通常是重复的，所以一旦掌握了思路，你就可以在`for`循环中编写部分代码。
- en: Since the `NeuralNetPB` attributes are not PyTorch tensors, cast them with `torch.FloatTensor(...)`.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 由于`NeuralNetPB`属性不是PyTorch张量，因此需要使用`torch.FloatTensor(...)`进行转换。
- en: '**Nuance** **#1: Permute.** TensorFlow convolution layers’ weight tensors are
    ordered differently. According to TensorFlow [documentation](https://www.tensorflow.org/api_docs/python/tf/nn/conv2d)
    the weight tensor shape is `[H, W, IN_C, OUT_C]`, whereas PyTorch weight tensor
    shape is `[OUT_C, IN_C, H, W]`. Therefore, rearrange the tensor: `torch.FloatTensor(...).permute(3,
    2, 0, 1)`. Having said that, the depthwise convolution should be rearranged with
    `.permute(2, 3, 0, 1)`.'
  id: totrans-24
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**细节 #1: 排列。** TensorFlow卷积层的权重张量排列方式不同。根据TensorFlow [文档](https://www.tensorflow.org/api_docs/python/tf/nn/conv2d)，权重张量的形状是`[H,
    W, IN_C, OUT_C]`，而PyTorch的权重张量形状是`[OUT_C, IN_C, H, W]`。因此，重新排列张量：`torch.FloatTensor(...).permute(3,
    2, 0, 1)`。话虽如此，深度卷积应该用`.permute(2, 3, 0, 1)`进行重新排列。'
- en: Once you are done here, you should probably save everything with `torch.save(...)`.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 完成后，你应该保存所有内容，使用`torch.save(...)`。
- en: Model Modifications
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型修改
- en: Upon completing the previous section, your PyTorch model should match the TensorFlow
    reference model in terms of layer composition. However, there are some additional
    nuances to keep in mind.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 完成前一部分后，你的PyTorch模型应该在层组成方面与TensorFlow参考模型匹配。然而，还有一些额外的细节需要注意。
- en: '**Nuance #2: Params.** Weights are not the only parameters. For example, the
    batch normalization layer consists of an `epsilon` attribute (for numerical stability).
    `epsilon` default value is different in TensorFlow and PyTorch. But even if it
    was equal, TensorFlow ResNet-50 model modifies epsilon, so be aware. Another example
    is Dropout layers, if exist.'
  id: totrans-28
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**细节 #2: 参数。** 权重并不是唯一的参数。例如，批量归一化层包含一个`epsilon`属性（用于数值稳定性）。`epsilon`的默认值在TensorFlow和PyTorch中是不同的。但即使它们相等，TensorFlow
    ResNet-50模型也会修改epsilon，所以要注意。另一个例子是Dropout层（如果存在的话）。'
- en: ''
  id: totrans-29
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**Nuance #3: Padding.** The padding argument in TensorFlow consists an option
    that does not exist in PyTorch: `SAME`. `SAME` means that the input feature map
    will be padded so the output feature map (i.e., the convolution operation result)
    spatial dimensions will be equal to the input dimensions. However, what happens
    if padding is asymmetric? will TensorFlow pad more on the left or on the right?
    **TensorFlow pads on right, whereas PyTorch pads on the left.** The same logic
    applies vertically, that is, **there may be an extra row of zeros at the bottom,
    whereas with PyTorch the extra row will come at the top.**'
  id: totrans-30
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**细节 #3: 填充。** TensorFlow中的填充参数包含一个PyTorch中不存在的选项：`SAME`。`SAME`意味着输入特征图将被填充，使输出特征图（即卷积操作结果）的空间维度与输入维度相等。然而，如果填充是不对称的，会发生什么？TensorFlow会在左侧还是右侧填充更多？**TensorFlow在右侧填充，而PyTorch在左侧填充。**
    同样的逻辑适用于垂直方向，即**底部可能会有一行额外的零，而在PyTorch中，额外的行将出现在顶部。**'
- en: You can check out my slightly modified [ResNet-50](https://github.com/gilshm/mlperf-pytorch/blob/master/models/resnet.py)
    and [MobileNet-v1](https://github.com/gilshm/mlperf-pytorch/blob/master/models/mobilenet_v1.py)
    models and see how I modified them accordingly.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以查看我稍微修改过的[ResNet-50](https://github.com/gilshm/mlperf-pytorch/blob/master/models/resnet.py)和[MobileNet-v1](https://github.com/gilshm/mlperf-pytorch/blob/master/models/mobilenet_v1.py)模型，看看我如何相应地进行修改。
- en: Preprocessing
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 预处理
- en: If you want to reproduce the exact same result as the reference model (in this
    case, the MLPerf models) then you have to reproduce the exact same preprocessing
    stages. The common preprocessing stages when dealing with the ImageNet dataset
    are (1) resizing to 256; (2) center cropping to 224; (3) normalization to values
    between [0, 1] (that is, division by 255); and (4) mean and std correction of
    each of the RGB channels. However, with MLPerf, the std is not normalized, and
    the bias is normalized differently (see [here](https://github.com/mlperf/inference/blob/master/vision/classification_and_detection/python/dataset.py)).
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想要复现与参考模型（在这个例子中是 MLPerf 模型）完全相同的结果，那么你必须复现完全相同的预处理阶段。处理 ImageNet 数据集时的常见预处理阶段包括（1）调整大小为
    256；（2）中心裁剪为 224；（3）归一化为[0, 1]之间的值（即除以 255）；（4）每个 RGB 通道的均值和标准差修正。然而，MLPerf 中的标准差没有归一化，偏差的归一化方式也不同（详见[此处](https://github.com/mlperf/inference/blob/master/vision/classification_and_detection/python/dataset.py)）。
- en: Moreover, even after doing the exact same preprocessing, I found that the same
    interpolation algorithm is implemented differently in MLPerf, which uses OpenCV,
    and PyTorch, which uses PIL. Since MLPerf preprocesses the entire ImageNet validation
    set and saves it as NumPy arrays, I decided to avoid PyTorch preprocessing and
    just load the NumPy arrays directly. At this point, I also achieved the exact
    same performance as MLPerf reports — 76.456% and 71.676% top-1 accuracies for
    ResNet-50 and MobileNet-v1, respectively.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，即使进行完全相同的预处理，我发现 MLPerf 中的插值算法与使用 PIL 的 PyTorch 实现方式不同。由于 MLPerf 预处理整个 ImageNet
    验证集并将其保存为 NumPy 数组，我决定避免使用 PyTorch 预处理，直接加载 NumPy 数组。在这一点上，我也达到了与 MLPerf 报告完全相同的性能——ResNet-50
    和 MobileNet-v1 的顶级 1 准确率分别为 76.456% 和 71.676%。
- en: Debugging
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 调试
- en: So you mapped the weights, and (you think) you have fine-tuned all the parameters
    and pre-processing stages according to your reference model. You start inference
    and nothing happens — 0% accuracy. What’s next? there’s no easy answer, you’ll
    have to debug it. What I did is going layer-by-layer and comparing the tensor
    values. Things should be almost identical (with some floating-point accepted differences).
    Use NeuralNetPB test function, for example, with `nn_pb.test(input, 'import/MobilenetV1/MobilenetV1/Conv2d_1_depthwise/Relu6:0')`
    you’ll get one of the ReLU result. You should know all layer names by now from
    the mapping stage.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 所以你映射了权重，并且（你认为）已经根据参考模型微调了所有参数和预处理阶段。你开始推断却什么也没有发生——准确率为 0%。接下来怎么办？没有简单的答案，你必须进行调试。我所做的是逐层比较张量值。结果应该几乎相同（允许有一些浮点差异）。例如，使用
    NeuralNetPB 测试函数 `nn_pb.test(input, 'import/MobilenetV1/MobilenetV1/Conv2d_1_depthwise/Relu6:0')`
    可以得到其中一个 ReLU 结果。你应该已经从映射阶段知道了所有的层名称。
- en: Conclusions
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: Be patient, that’s the most important thing. Migrating any model between frameworks
    requires attention to all the fine details. There might be some tools out there
    that help with automatic conversions, but even if you manage to get them working,
    they still may ignore the subtle differences I mentioned here, thus not creating
    a one-to-one copy, which means a model that performs differently — this is what
    happened to me.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 要有耐心，这最为重要。将任何模型在框架之间迁移都需要注意所有细节。可能有一些工具可以帮助自动转换，但即使你成功使用了它们，它们也可能忽略了我在这里提到的微妙差异，从而无法创建一对一的副本，这意味着模型表现会有所不同——这就是我遇到的情况。
- en: You can find my MLPerf ResNet-50 and MobileNet-v1 PyTorch checkpoints and models
    at [my GitHub repository](https://github.com/gilshm/mlperf-pytorch).
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在[我的 GitHub 仓库](https://github.com/gilshm/mlperf-pytorch)中找到我的 MLPerf ResNet-50
    和 MobileNet-v1 PyTorch 检查点和模型。
- en: Feel free to connect on [LinkedIn](https://www.linkedin.com/in/gilsho/) and/or
    follow me here on Medium.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 随时可以在[LinkedIn](https://www.linkedin.com/in/gilsho/)上联系我，或者在 Medium 上关注我。
- en: References
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] Shomron, Gil, and Uri Weiser. “Non-Blocking Simultaneous Multithreading:
    Embracing the Resiliency of Deep Neural Networks.” I*nternational Symposium on
    Microarchitecture (MICRO)*. IEEE, 2020.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] Shomron, Gil, and Uri Weiser. “Non-Blocking Simultaneous Multithreading:
    Embracing the Resiliency of Deep Neural Networks.” *国际微架构研讨会 (MICRO)*. IEEE, 2020.'
- en: '[2] Reddi, Vijay Janapa, et al. “MLPerf Inference Benchmark.” *International
    Symposium on Computer Architecture (ISCA)*. IEEE, 2020.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] Reddi, Vijay Janapa, et al. “MLPerf Inference Benchmark.” *国际计算机架构研讨会 (ISCA)*.
    IEEE, 2020.'
