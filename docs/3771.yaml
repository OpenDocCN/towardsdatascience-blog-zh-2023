- en: A Subtle Bias that Could Impact Your Decision Trees and Random Forests
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/a-subtle-bias-that-could-impact-your-decision-trees-and-random-forests-3480756f7552?source=collection_archive---------5-----------------------#2023-12-28](https://towardsdatascience.com/a-subtle-bias-that-could-impact-your-decision-trees-and-random-forests-3480756f7552?source=collection_archive---------5-----------------------#2023-12-28)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: That can be easily eliminated
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@gyuriofkovacs?source=post_page-----3480756f7552--------------------------------)[![Gyorgy
    Kovacs](../Images/aa5d1fcc59d738acc1056de3f0cbe7ca.png)](https://medium.com/@gyuriofkovacs?source=post_page-----3480756f7552--------------------------------)[](https://towardsdatascience.com/?source=post_page-----3480756f7552--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----3480756f7552--------------------------------)
    [Gyorgy Kovacs](https://medium.com/@gyuriofkovacs?source=post_page-----3480756f7552--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F4563dd81810c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-subtle-bias-that-could-impact-your-decision-trees-and-random-forests-3480756f7552&user=Gyorgy+Kovacs&userId=4563dd81810c&source=post_page-4563dd81810c----3480756f7552---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----3480756f7552--------------------------------)
    ·10 min read·Dec 28, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F3480756f7552&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-subtle-bias-that-could-impact-your-decision-trees-and-random-forests-3480756f7552&user=Gyorgy+Kovacs&userId=4563dd81810c&source=-----3480756f7552---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F3480756f7552&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-subtle-bias-that-could-impact-your-decision-trees-and-random-forests-3480756f7552&source=-----3480756f7552---------------------bookmark_footer-----------)![](../Images/97c9e93a77a35e474acf2ef359534ccd.png)'
  prefs: []
  type: TYPE_NORMAL
- en: An artistic decision tree generated by Dall-E
  prefs: []
  type: TYPE_NORMAL
- en: There is a chance that your decision trees and random forests suffer from a
    small bias, which can be eliminated with ease, for basically no cost. This is
    what we explore in this post.
  prefs: []
  type: TYPE_NORMAL
- en: '*Disclaimer: the post discusses a recent research conducted by the author.*'
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Decision trees and random forests are widely adopted classification and regression
    techniques in machine learning. Decision trees are favored for their interpretability,
    while random forests stand out as highly competitive and general-purpose state-of-the-art
    techniques. Commonly used CART implementations, such as those in the Python package
    [sklearn](https://scikit-learn.org/stable/index.html) and the R packages [tree](https://cran.r-project.org/web/packages/tree/index.html)
    and [caret](https://cran.r-project.org/web/packages/caret/index.html), assume
    that all features are continuous. Despite this silent assumption of continuity,
    both techniques are routinely applied to datasets with diverse feature types.
  prefs: []
  type: TYPE_NORMAL
- en: In a recent paper, we investigated the practical implications of violating the
    assumption of continuity and found that it leads to a bias. Importantly, these
    assumptions are almost always violated in practice. In this article, we present
    and discuss our findings, illustrate and explain the background, and propose some
    simple techniques to mitigate the bias.
  prefs: []
  type: TYPE_NORMAL
- en: A motivating example
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let’s jump into it with an example using the [CPU performance](https://archive.ics.uci.edu/dataset/29/computer+hardware)
    dataset from the UCI repository. We’ll import it through the [common-datasets](https://pypi.org/project/common-datasets/)
    package, to simplify the preprocessing and bypass the need for feature encoding
    and missing data imputation.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: In the experiment, we evaluate the performance of the random forest regressor
    on both the original data and its mirrored version (each feature multiplied by
    -1). The hyperparameter for the regressor (`max_depth=11` ) was chosen in a dedicated
    model selection step, maximizing the r2 score across a reasonable range of depths.
    The cross-validation employed for evaluation is significantly more comprehensive
    than what is typically used in machine learning, encompassing a total of 2000
    folds.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: In terms of r2 scores, we observe a deterioration of 0.2 percentage points when
    the attributes are mirrored. Furthermore, the difference is statistically significant
    at conventional levels (p << 0.01).
  prefs: []
  type: TYPE_NORMAL
- en: 'The results are somewhat surprising and counter-intuitive. Machine learning
    techniques are typically *invariant* to certain types of transformations. For
    example, k Nearest Neighbors is invariant to any orthogonal transformation (like
    rotation), and linear-ish techniques are typically invariant to the scaling of
    attributes. Since the space partitioning in decision trees is axis aligned, it
    cannot be expected to be invariant to rotations. However, it is invariant to scaling:
    applying any positive multiplier to any feature will lead to the exact same tree.
    Consequently, there must be something going on with the mirroring of the axes.'
  prefs: []
  type: TYPE_NORMAL
- en: 'An intriguing question arises: what if mirroring the axes leads to better results?
    Should we consider another degree of freedom (multiplying by -1) in model selection
    beyond determining the optimal depth? Well, in the rest of the post we figure
    out what is going on here!'
  prefs: []
  type: TYPE_NORMAL
- en: '**Binary decision tree induction and inference**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now, let’s briefly review some important characteristics of building and making
    inferences with binary Classification And Regression Trees (CART), which are used
    by most implementations. A notable difference compared to other tree induction
    techniques like [ID3](https://en.wikipedia.org/wiki/ID3_algorithm) and [C4.5](https://en.wikipedia.org/wiki/C4.5_algorithm)
    is that CART trees do not treat categorical features in any special way. **CART
    trees assume that all features are continuous**.
  prefs: []
  type: TYPE_NORMAL
- en: Given a training set (classification or regression), decision trees are induced
    by recursively partitioning the training set alongside the feature space using
    conditions like *feature < threshold* or alternatively, *features <= threshold*.
    **The choice of conditioning is usually an intrinsic property of the implementations**.
    For example, the Python package [sklearn](https://scikit-learn.org/stable/index.html)
    uses conditions of the form *feature <= threshold*, while the R package [tree](https://cran.r-project.org/web/packages/tree/index.html)
    uses *feature < threshold*. Note that these conditions are aligned with the presumption
    of all features being continuous.
  prefs: []
  type: TYPE_NORMAL
- en: 'Nevertheless, the presumption of continuous features is not a limitation. Integer
    features, category features through some encoding, or binary features can still
    be fed into these trees. Let’s examine an example tree in a hypothetical loan
    approval scenario (a binary classification problem), based on three attributes:'
  prefs: []
  type: TYPE_NORMAL
- en: 'graduated (binary): 0 if the applicant did not graduate, 1 if the applicant
    graduated;'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'income (float): the yearly gross income of the applicant;'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'dependents (int): the number of dependents;'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'and the target variable is binary: whether the applicant defaults (1) or pays
    back (0).'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/22da8afdd9c919cd18b2cf30bf16d644.png)'
  prefs: []
  type: TYPE_IMG
- en: A decision tree built for a hypothetical loan approval scenario
  prefs: []
  type: TYPE_NORMAL
- en: The structure of the tree, as well as the conditions in the nodes (which threshold
    on which feature), are inferred from the training data. For more details about
    tree induction, refer to [decision tree learning on Wikipedia](https://en.wikipedia.org/wiki/Decision_tree_learning).
  prefs: []
  type: TYPE_NORMAL
- en: Given a tree like this, inference for a new record is conducted by starting
    from the leaf node, recursively applying the conditions, and routing the record
    to the branch corresponding to the output of the condition. When a leaf node is
    encountered, the label (or eventually distribution) recorded in the leaf node
    is returned as the prediction.
  prefs: []
  type: TYPE_NORMAL
- en: '**The conditioning and the threshold**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A finite set of training records cannot imply a unique partitioning of the
    feature space. For example, the tree in the figure above could be induced from
    data where there is no record with graduation = 0 and income in the range ]60k,
    80k[. The tree induction method identifies that a split should be made between
    the income values 60k and 80k. In the absence of further information, the midpoint
    of the interval (70k) is used as the threshold. Generally, it could be 65k or
    85k as well. Using the midpoints of the unlabeled intervals is a common practice
    and a reasonable choice: in line with the assumption of continuous features, 50%
    of the unlabeled interval is assigned to the left and 50% to the right branches.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Due to the use of midpoints as thresholds, **the tree induction is completely
    independent of the choice of the conditioning operator**: using both <= and <
    leads to the same tree structure, with the same thresholds, except for the conditioning
    operator.'
  prefs: []
  type: TYPE_NORMAL
- en: However, **inference does depend on the conditioning operator**. In the example,
    if a record representing an applicant with a 70k income is to be inferred, then
    in the depicted setup, it will be routed to the left branch. However, using the
    operator <, it will be routed to the right branch. With truly continuous features,
    there is a negligible chance for a record with exactly 70k income to be inferred.
    However, in reality, the income might be quoted in units of 1k, 5k, or eventually
    10k, which makes it probable that the choice of the conditioning operator has
    a notable impact on predictions.
  prefs: []
  type: TYPE_NORMAL
- en: '**The relation of conditioning and mirroring**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Why do we talk about the conditioning when the problem we observed is about
    the mirroring of features?* The two are basically the same. A condition “feature
    < threshold” is equivalent to the condition “-feature <= -threshold” in the sense
    that they lead to the same, but mirrored partitioning of the real axis. Namely,
    in both cases, if the feature value equals the threshold, that value is in the
    same partition where the feature values greater than the threshold are. For example,
    compare the two trees below, the one we used for illustration earlier, except
    all conditioning operators are changed to <, and another tree where the operator
    is kept, but the tree is mirrored: one can readily see that for any record they
    lead to the same predictions.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/35dbb474d158758d688420288278fe48.png)'
  prefs: []
  type: TYPE_IMG
- en: The previous tree with the conditioning operator <
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b2462e58505a5011b91b009749a92bfb.png)'
  prefs: []
  type: TYPE_IMG
- en: The tree built on the mirrored data (still using the conditioning operator ≤)
  prefs: []
  type: TYPE_NORMAL
- en: Since tree induction is independent of the choice of conditioning, **building
    a tree on mirrored data and then predicting mirrored vectors is equivalent to
    using the non-default conditioning operator (<) for inference on non-mirrored
    records**. When the trees of the forest were fitted to the mirrored data, even
    though *sklearn* uses the ‘<=’ operator for conditioning, it worked as if it used
    the ‘<’ operator. Consequently, **the performance deterioration we discovered
    with mirroring is due to thresholds coinciding with feature values**, leading
    to different predictions during the evaluation of the test sets.
  prefs: []
  type: TYPE_NORMAL
- en: For the sake of completeness, we note that the randomization in certain steps
    of tree induction might lead to slightly different trees when fitted to the mirrored
    data. However, these differences smooth out in random forests, especially in 2k
    folds of evaluation. The observed performance deterioration is a consequence of
    the systematic effect of thresholds coinciding with feature values.
  prefs: []
  type: TYPE_NORMAL
- en: '**When can this happen?**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Primarily, two circumstances increase the likelihood of the phenomenon:'
  prefs: []
  type: TYPE_NORMAL
- en: '**When a feature domain contains highly probable equidistant values**: This
    sets the stage for a threshold (being the mid-point of two observations) to coincide
    with a feature value with high probability.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Relatively deep trees are built**: Generally, as a tree gets deeper, the
    training data becomes sparser at the nodes. When certain observations are absent
    at greater depths, thresholds might fall on those values.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Interestingly, features taking a handful of equidistant values are very common
    in numerous domains. For example:'
  prefs: []
  type: TYPE_NORMAL
- en: The age feature in medical datasets.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rounded decimals (observed to the, say, 2nd digit will form a lattice).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Monetary figures quoted in units of millions or billions in financial datasets.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Additionally, almost 97% of features in the toy regression and classification
    datasets in `sklearn.datasets` are of this kind. Therefore, it is not an over-exaggeration
    to say that features taking equidistant values with high probability are present
    everywhere. Consequently, as a rule of thumb, **the deeper trees or forests one
    builds, the more likely it becomes that thresholds interfere with feature values**.
  prefs: []
  type: TYPE_NORMAL
- en: '**It is a bias, model selection cannot help!**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have seen that the two conditioning operators (the non-default one imitated
    by the mirroring of data) can lead to different prediction results with statistical
    significance. The two predictions cannot be unbiased at the same time. Therefore,
    we consider the use of either form of conditioning introducing a bias when thresholds
    coincide with feature values.
  prefs: []
  type: TYPE_NORMAL
- en: Alternatively, it is tempting to consider one form of conditioning to be luckily
    aligned with the data and improving the performance. Thus, model selection could
    be used to select the most suitable form of conditioning (or whether the data
    should be mirrored). However, in a particular model selection scenario, using
    some k-fold cross-validation scheme, we can only test which operator is typically
    favorable if, say, 20% of the data is removed (5-fold) from training and then
    used for evaluation. **When a model is trained on all data, other thresholds might
    interfere with feature values, and we have no information on which conditioning
    would improve the performance.**
  prefs: []
  type: TYPE_NORMAL
- en: '**Mitigating the bias in random forests**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A natural way to eliminate the bias is to integrate out the effect of the choice
    of conditioning operators. This involves carrying out predictions with both operators
    and averaging the results.
  prefs: []
  type: TYPE_NORMAL
- en: In practice, with random forests, exploiting the equivalence of data mirroring
    and changing the conditioning operator, this can be approximated for basically
    no cost. Instead of using a forest of N_e estimators, one can build two forests
    of half the size, fit one to the original data, the other to the mirrored data,
    and take the average of the results. Note that this approach is applicable with
    any random forest implementation, and has only marginal additional cost (like
    multiplying the data by -1 and averaging the results).
  prefs: []
  type: TYPE_NORMAL
- en: For example, we implement this strategy in Python below, aiming to integrate
    out the bias from the *sklearn* random forest.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we can execute the same experiments as before, using the exact same folds:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Let’s compare the results!
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: According to expectations, the r2 score of the unbiased forest falls between
    the scores achieved by the original forest with and without mirroring the data.
    It might seem that eliminating the bias is detrimental to the performance; however,
    we emphasize again that once the forest is fit with all data, the relations might
    be reversed, and the original model might lead to worse predictions than the mirrored
    model. **Eliminating the bias by integrating out the dependence on the conditioning
    operator eliminates the risk of deteriorated performance due to relying on the
    default conditioning operator.**
  prefs: []
  type: TYPE_NORMAL
- en: '**Conclusion**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The presence of a bias related to the interaction of the choice of conditioning
    and features taking equidistant values has been established and demonstrated.
    Given the common occurrence of features of this kind, the bias is likely to be
    present in sufficiently deep decision trees and random forests. The potentially
    detrimental effect can be eliminated by averaging the predictions carried out
    by the two conditioning operators. Interestingly, in the case of random forests,
    this can be done at basically no cost. In the example we used, the improvement
    reached the level of 0.1–0.2 percentage points of r2 scores. Finally, we emphasize
    that the results generalize to classification problems and single decision trees
    as well (see [preprint](https://www.researchgate.net/publication/376591586_The_Conditioning_Bias_in_Binary_Decision_Trees_and_Random_Forests_and_Its_Elimination)).
  prefs: []
  type: TYPE_NORMAL
- en: '**Further reading**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For further details, I recommend:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Our preprint discussing the topic in detail, with many more tests, illustrations
    and alternative bias mitigation techniques: [preprint](https://www.researchgate.net/publication/376591586_The_Conditioning_Bias_in_Binary_Decision_Trees_and_Random_Forests_and_Its_Elimination),'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The GitHub repository with the reproducible analysis: [https://github.com/gykovacs/conditioning_bias](https://github.com/gykovacs/conditioning_bias)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The notebook behind this post: [https://github.com/gykovacs/conditioning_bias/blob/main/blogpost/001-analysis.ipynb](https://github.com/gykovacs/conditioning_bias/blob/main/blogpost/001-analysis.ipynb)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*If you are interested in more content like this, don’t forget to subscribe!
    You can also find me on* [LinkedIn](https://www.linkedin.com/in/gyorgy-kovacs-a9799727/)*!*'
  prefs: []
  type: TYPE_NORMAL
