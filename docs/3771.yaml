- en: A Subtle Bias that Could Impact Your Decision Trees and Random Forests
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 可能影响你的决策树和随机森林的微妙偏差
- en: 原文：[https://towardsdatascience.com/a-subtle-bias-that-could-impact-your-decision-trees-and-random-forests-3480756f7552?source=collection_archive---------5-----------------------#2023-12-28](https://towardsdatascience.com/a-subtle-bias-that-could-impact-your-decision-trees-and-random-forests-3480756f7552?source=collection_archive---------5-----------------------#2023-12-28)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/a-subtle-bias-that-could-impact-your-decision-trees-and-random-forests-3480756f7552?source=collection_archive---------5-----------------------#2023-12-28](https://towardsdatascience.com/a-subtle-bias-that-could-impact-your-decision-trees-and-random-forests-3480756f7552?source=collection_archive---------5-----------------------#2023-12-28)
- en: That can be easily eliminated
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 这可以很容易地被消除
- en: '[](https://medium.com/@gyuriofkovacs?source=post_page-----3480756f7552--------------------------------)[![Gyorgy
    Kovacs](../Images/aa5d1fcc59d738acc1056de3f0cbe7ca.png)](https://medium.com/@gyuriofkovacs?source=post_page-----3480756f7552--------------------------------)[](https://towardsdatascience.com/?source=post_page-----3480756f7552--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----3480756f7552--------------------------------)
    [Gyorgy Kovacs](https://medium.com/@gyuriofkovacs?source=post_page-----3480756f7552--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@gyuriofkovacs?source=post_page-----3480756f7552--------------------------------)[![Gyorgy
    Kovacs](../Images/aa5d1fcc59d738acc1056de3f0cbe7ca.png)](https://medium.com/@gyuriofkovacs?source=post_page-----3480756f7552--------------------------------)[](https://towardsdatascience.com/?source=post_page-----3480756f7552--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----3480756f7552--------------------------------)
    [Gyorgy Kovacs](https://medium.com/@gyuriofkovacs?source=post_page-----3480756f7552--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F4563dd81810c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-subtle-bias-that-could-impact-your-decision-trees-and-random-forests-3480756f7552&user=Gyorgy+Kovacs&userId=4563dd81810c&source=post_page-4563dd81810c----3480756f7552---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----3480756f7552--------------------------------)
    ·10 min read·Dec 28, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F3480756f7552&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-subtle-bias-that-could-impact-your-decision-trees-and-random-forests-3480756f7552&user=Gyorgy+Kovacs&userId=4563dd81810c&source=-----3480756f7552---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F4563dd81810c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-subtle-bias-that-could-impact-your-decision-trees-and-random-forests-3480756f7552&user=Gyorgy+Kovacs&userId=4563dd81810c&source=post_page-4563dd81810c----3480756f7552---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----3480756f7552--------------------------------)
    ·10分钟阅读·2023年12月28日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F3480756f7552&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-subtle-bias-that-could-impact-your-decision-trees-and-random-forests-3480756f7552&user=Gyorgy+Kovacs&userId=4563dd81810c&source=-----3480756f7552---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F3480756f7552&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-subtle-bias-that-could-impact-your-decision-trees-and-random-forests-3480756f7552&source=-----3480756f7552---------------------bookmark_footer-----------)![](../Images/97c9e93a77a35e474acf2ef359534ccd.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F3480756f7552&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-subtle-bias-that-could-impact-your-decision-trees-and-random-forests-3480756f7552&source=-----3480756f7552---------------------bookmark_footer-----------)![](../Images/97c9e93a77a35e474acf2ef359534ccd.png)'
- en: An artistic decision tree generated by Dall-E
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: Dall-E 生成的艺术决策树
- en: There is a chance that your decision trees and random forests suffer from a
    small bias, which can be eliminated with ease, for basically no cost. This is
    what we explore in this post.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 你的决策树和随机森林可能会受到一些小的偏差影响，这种偏差可以轻松消除，几乎不需要成本。这就是我们在本文中探讨的内容。
- en: '*Disclaimer: the post discusses a recent research conducted by the author.*'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '*免责声明：这篇文章讨论了作者最近进行的研究。*'
- en: Introduction
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: Decision trees and random forests are widely adopted classification and regression
    techniques in machine learning. Decision trees are favored for their interpretability,
    while random forests stand out as highly competitive and general-purpose state-of-the-art
    techniques. Commonly used CART implementations, such as those in the Python package
    [sklearn](https://scikit-learn.org/stable/index.html) and the R packages [tree](https://cran.r-project.org/web/packages/tree/index.html)
    and [caret](https://cran.r-project.org/web/packages/caret/index.html), assume
    that all features are continuous. Despite this silent assumption of continuity,
    both techniques are routinely applied to datasets with diverse feature types.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树和随机森林是机器学习中广泛应用的分类和回归技术。决策树因其可解释性而受到青睐，而随机森林则作为高度竞争且通用的最先进技术脱颖而出。常用的 CART
    实现，如 Python 包中的 [sklearn](https://scikit-learn.org/stable/index.html) 和 R 包中的
    [tree](https://cran.r-project.org/web/packages/tree/index.html) 及 [caret](https://cran.r-project.org/web/packages/caret/index.html)，假设所有特征都是连续的。尽管存在这种隐含的连续性假设，这两种技术仍然被广泛应用于具有各种特征类型的数据集。
- en: In a recent paper, we investigated the practical implications of violating the
    assumption of continuity and found that it leads to a bias. Importantly, these
    assumptions are almost always violated in practice. In this article, we present
    and discuss our findings, illustrate and explain the background, and propose some
    simple techniques to mitigate the bias.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在一篇最近的论文中，我们研究了违反连续性假设的实际影响，并发现它会导致偏差。重要的是，这些假设在实际中几乎总是被违反。在这篇文章中，我们展示并讨论了我们的发现，说明和解释了背景，并提出了一些简单的技术来减轻这种偏差。
- en: A motivating example
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 一个激励性示例
- en: Let’s jump into it with an example using the [CPU performance](https://archive.ics.uci.edu/dataset/29/computer+hardware)
    dataset from the UCI repository. We’ll import it through the [common-datasets](https://pypi.org/project/common-datasets/)
    package, to simplify the preprocessing and bypass the need for feature encoding
    and missing data imputation.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过 UCI 资源库中的 [CPU 性能](https://archive.ics.uci.edu/dataset/29/computer+hardware)
    数据集来进行示例。我们将通过 [common-datasets](https://pypi.org/project/common-datasets/) 包导入数据，以简化预处理过程，避免特征编码和缺失数据插补的需求。
- en: '[PRE0]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: In the experiment, we evaluate the performance of the random forest regressor
    on both the original data and its mirrored version (each feature multiplied by
    -1). The hyperparameter for the regressor (`max_depth=11` ) was chosen in a dedicated
    model selection step, maximizing the r2 score across a reasonable range of depths.
    The cross-validation employed for evaluation is significantly more comprehensive
    than what is typically used in machine learning, encompassing a total of 2000
    folds.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在实验中，我们评估了随机森林回归器在原始数据及其镜像版本（每个特征乘以 -1）上的表现。回归器的超参数（`max_depth=11`）是在专门的模型选择步骤中选择的，通过在合理的深度范围内最大化
    r2 得分来确定。用于评估的交叉验证比机器学习中通常使用的交叉验证要全面得多，总共进行了 2000 次折叠。
- en: '[PRE1]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: In terms of r2 scores, we observe a deterioration of 0.2 percentage points when
    the attributes are mirrored. Furthermore, the difference is statistically significant
    at conventional levels (p << 0.01).
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在 r2 得分方面，当属性被镜像时，我们观察到降低了 0.2 个百分比点。此外，这一差异在传统水平下统计上显著（p << 0.01）。
- en: 'The results are somewhat surprising and counter-intuitive. Machine learning
    techniques are typically *invariant* to certain types of transformations. For
    example, k Nearest Neighbors is invariant to any orthogonal transformation (like
    rotation), and linear-ish techniques are typically invariant to the scaling of
    attributes. Since the space partitioning in decision trees is axis aligned, it
    cannot be expected to be invariant to rotations. However, it is invariant to scaling:
    applying any positive multiplier to any feature will lead to the exact same tree.
    Consequently, there must be something going on with the mirroring of the axes.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 结果有些令人惊讶且不符合直觉。机器学习技术通常对某些类型的变换具有*不变性*。例如，k 最近邻对任何正交变换（如旋转）不变，线性技术通常对属性的缩放不变。由于决策树中的空间划分是轴对齐的，因此不能期望它对旋转不变。然而，它对缩放是不变的：对任何特征应用任何正的乘数都会导致完全相同的树。因此，轴的镜像处理一定存在一些问题。
- en: 'An intriguing question arises: what if mirroring the axes leads to better results?
    Should we consider another degree of freedom (multiplying by -1) in model selection
    beyond determining the optimal depth? Well, in the rest of the post we figure
    out what is going on here!'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 一个有趣的问题出现了：如果镜像坐标轴可以带来更好的结果怎么办？我们是否应该在模型选择中考虑另一个自由度（乘以 -1），除了确定最优深度之外？好吧，在接下来的文章中我们将搞清楚这里发生了什么！
- en: '**Binary decision tree induction and inference**'
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**二叉决策树生成与推理**'
- en: Now, let’s briefly review some important characteristics of building and making
    inferences with binary Classification And Regression Trees (CART), which are used
    by most implementations. A notable difference compared to other tree induction
    techniques like [ID3](https://en.wikipedia.org/wiki/ID3_algorithm) and [C4.5](https://en.wikipedia.org/wiki/C4.5_algorithm)
    is that CART trees do not treat categorical features in any special way. **CART
    trees assume that all features are continuous**.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们简要回顾一下使用**二叉分类与回归树（CART）**的构建和推理的一些重要特征，这些特征被大多数实现所使用。与其他树生成技术如 [ID3](https://en.wikipedia.org/wiki/ID3_algorithm)
    和 [C4.5](https://en.wikipedia.org/wiki/C4.5_algorithm) 相比，CART 树的一个显著不同之处在于它们不会以任何特殊方式处理类别特征。**CART
    树假设所有特征都是连续的**。
- en: Given a training set (classification or regression), decision trees are induced
    by recursively partitioning the training set alongside the feature space using
    conditions like *feature < threshold* or alternatively, *features <= threshold*.
    **The choice of conditioning is usually an intrinsic property of the implementations**.
    For example, the Python package [sklearn](https://scikit-learn.org/stable/index.html)
    uses conditions of the form *feature <= threshold*, while the R package [tree](https://cran.r-project.org/web/packages/tree/index.html)
    uses *feature < threshold*. Note that these conditions are aligned with the presumption
    of all features being continuous.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个训练集（分类或回归），决策树通过递归地使用诸如 *feature < threshold* 或 *features <= threshold*
    的条件来划分训练集和特征空间来生成。**条件选择通常是实现的一个固有属性**。例如，Python 包 [sklearn](https://scikit-learn.org/stable/index.html)
    使用形式为 *feature <= threshold* 的条件，而 R 包 [tree](https://cran.r-project.org/web/packages/tree/index.html)
    使用 *feature < threshold*。请注意，这些条件与所有特征都是连续的假设相一致。
- en: 'Nevertheless, the presumption of continuous features is not a limitation. Integer
    features, category features through some encoding, or binary features can still
    be fed into these trees. Let’s examine an example tree in a hypothetical loan
    approval scenario (a binary classification problem), based on three attributes:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，假设特征是连续的并不是一个限制。整数特征、通过某些编码的类别特征或二元特征仍然可以输入到这些树中。让我们以一个假设的贷款审批场景中的示例树为例（一个二元分类问题），基于三个属性：
- en: 'graduated (binary): 0 if the applicant did not graduate, 1 if the applicant
    graduated;'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'graduated (binary): 如果申请人未毕业则为 0，如果申请人毕业则为 1；'
- en: 'income (float): the yearly gross income of the applicant;'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'income (float): 申请人的年收入；'
- en: 'dependents (int): the number of dependents;'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'dependents (int): 依赖人数；'
- en: 'and the target variable is binary: whether the applicant defaults (1) or pays
    back (0).'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 目标变量是二元的：申请人是否违约（1）或偿还（0）。
- en: '![](../Images/22da8afdd9c919cd18b2cf30bf16d644.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/22da8afdd9c919cd18b2cf30bf16d644.png)'
- en: A decision tree built for a hypothetical loan approval scenario
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 为假设的贷款审批场景构建的决策树
- en: The structure of the tree, as well as the conditions in the nodes (which threshold
    on which feature), are inferred from the training data. For more details about
    tree induction, refer to [decision tree learning on Wikipedia](https://en.wikipedia.org/wiki/Decision_tree_learning).
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 树的结构以及节点中的条件（即哪个特征的阈值）是从训练数据中推断出来的。有关树生成的更多细节，请参考 [维基百科上的决策树学习](https://en.wikipedia.org/wiki/Decision_tree_learning)。
- en: Given a tree like this, inference for a new record is conducted by starting
    from the leaf node, recursively applying the conditions, and routing the record
    to the branch corresponding to the output of the condition. When a leaf node is
    encountered, the label (or eventually distribution) recorded in the leaf node
    is returned as the prediction.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 给定这样一棵树，新记录的推理是通过从叶子节点开始，递归应用条件，并将记录路由到条件输出对应的分支。当遇到叶子节点时，记录在叶子节点中的标签（或最终分布）将被返回作为预测。
- en: '**The conditioning and the threshold**'
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**条件设置与阈值**'
- en: 'A finite set of training records cannot imply a unique partitioning of the
    feature space. For example, the tree in the figure above could be induced from
    data where there is no record with graduation = 0 and income in the range ]60k,
    80k[. The tree induction method identifies that a split should be made between
    the income values 60k and 80k. In the absence of further information, the midpoint
    of the interval (70k) is used as the threshold. Generally, it could be 65k or
    85k as well. Using the midpoints of the unlabeled intervals is a common practice
    and a reasonable choice: in line with the assumption of continuous features, 50%
    of the unlabeled interval is assigned to the left and 50% to the right branches.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 一个有限的训练记录集不能暗示特征空间的唯一分割。例如，上图中的树可能是从数据中归纳出的，其中没有毕业=0且收入在]60k，80k[范围内的记录。树的归纳方法确定应在收入值60k和80k之间进行分割。在没有更多信息的情况下，中点（70k）被用作阈值。一般来说，也可以是65k或85k。使用未标记区间的中点是一种常见的做法和合理的选择：与连续特征的假设一致，将未标记区间的50%分配到左分支，50%分配到右分支。
- en: 'Due to the use of midpoints as thresholds, **the tree induction is completely
    independent of the choice of the conditioning operator**: using both <= and <
    leads to the same tree structure, with the same thresholds, except for the conditioning
    operator.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 由于使用中点作为阈值，**树的归纳完全独立于条件操作符的选择**：使用<=和<都会导致相同的树结构，阈值也相同，只是在条件操作符上有所不同。
- en: However, **inference does depend on the conditioning operator**. In the example,
    if a record representing an applicant with a 70k income is to be inferred, then
    in the depicted setup, it will be routed to the left branch. However, using the
    operator <, it will be routed to the right branch. With truly continuous features,
    there is a negligible chance for a record with exactly 70k income to be inferred.
    However, in reality, the income might be quoted in units of 1k, 5k, or eventually
    10k, which makes it probable that the choice of the conditioning operator has
    a notable impact on predictions.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，**推断确实依赖于条件操作符**。在示例中，如果要推断一个收入为70k的申请人记录，则在所示的设置中，它将被路由到左分支。然而，使用<操作符时，它将被路由到右分支。对于真正的连续特征，收入恰好为70k的记录被推断的可能性微不足道。然而，实际上，收入可能以1k、5k或最终10k为单位，这使得条件操作符的选择对预测有显著影响。
- en: '**The relation of conditioning and mirroring**'
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**条件操作符和镜像的关系**'
- en: '*Why do we talk about the conditioning when the problem we observed is about
    the mirroring of features?* The two are basically the same. A condition “feature
    < threshold” is equivalent to the condition “-feature <= -threshold” in the sense
    that they lead to the same, but mirrored partitioning of the real axis. Namely,
    in both cases, if the feature value equals the threshold, that value is in the
    same partition where the feature values greater than the threshold are. For example,
    compare the two trees below, the one we used for illustration earlier, except
    all conditioning operators are changed to <, and another tree where the operator
    is kept, but the tree is mirrored: one can readily see that for any record they
    lead to the same predictions.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '*为什么我们谈论条件操作符，而我们观察到的问题是特征的镜像？* 这两者基本上是相同的。条件“feature < threshold”与条件“-feature
    <= -threshold”在本质上是等价的，因为它们导致相同但镜像的实数轴分割。也就是说，在这两种情况下，如果特征值等于阈值，该值位于特征值大于阈值的相同分区。例如，比较下面的两棵树，一棵是我们之前用于说明的树，除了所有条件操作符都改为<，另一棵是保持操作符不变，但树是镜像的：可以很容易看到，对于任何记录，它们都会导致相同的预测。'
- en: '![](../Images/35dbb474d158758d688420288278fe48.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/35dbb474d158758d688420288278fe48.png)'
- en: The previous tree with the conditioning operator <
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 上一个使用条件操作符<的树
- en: '![](../Images/b2462e58505a5011b91b009749a92bfb.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b2462e58505a5011b91b009749a92bfb.png)'
- en: The tree built on the mirrored data (still using the conditioning operator ≤)
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 基于镜像数据构建的树（仍然使用条件操作符≤）
- en: Since tree induction is independent of the choice of conditioning, **building
    a tree on mirrored data and then predicting mirrored vectors is equivalent to
    using the non-default conditioning operator (<) for inference on non-mirrored
    records**. When the trees of the forest were fitted to the mirrored data, even
    though *sklearn* uses the ‘<=’ operator for conditioning, it worked as if it used
    the ‘<’ operator. Consequently, **the performance deterioration we discovered
    with mirroring is due to thresholds coinciding with feature values**, leading
    to different predictions during the evaluation of the test sets.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 由于树生成与条件选择无关，**在镜像数据上构建树，然后预测镜像向量，相当于使用非默认的条件运算符（<）对非镜像记录进行推断**。当森林的树拟合到镜像数据时，即使*sklearn*使用‘<=’运算符进行条件处理，它也会像使用‘<’运算符一样工作。因此，**我们发现镜像处理的性能下降是由于阈值与特征值重合**，导致在测试集评估过程中预测结果不同。
- en: For the sake of completeness, we note that the randomization in certain steps
    of tree induction might lead to slightly different trees when fitted to the mirrored
    data. However, these differences smooth out in random forests, especially in 2k
    folds of evaluation. The observed performance deterioration is a consequence of
    the systematic effect of thresholds coinciding with feature values.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 为了完整性，我们注意到树生成某些步骤中的随机化可能会导致在拟合镜像数据时出现略微不同的树。然而，这些差异在随机森林中平滑，特别是在2k折的评估中。观察到的性能下降是阈值与特征值重合的系统性效应的结果。
- en: '**When can this happen?**'
  id: totrans-46
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**这种情况何时会发生？**'
- en: 'Primarily, two circumstances increase the likelihood of the phenomenon:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 主要有两种情况会增加这种现象的可能性：
- en: '**When a feature domain contains highly probable equidistant values**: This
    sets the stage for a threshold (being the mid-point of two observations) to coincide
    with a feature value with high probability.'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**当特征域包含高概率的等距值**：这为阈值（作为两个观察值的中点）与特征值重合提供了条件。'
- en: '**Relatively deep trees are built**: Generally, as a tree gets deeper, the
    training data becomes sparser at the nodes. When certain observations are absent
    at greater depths, thresholds might fall on those values.'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**建立了相对较深的树**：通常情况下，树的深度增加时，训练数据在节点处变得更加稀疏。当某些观察值在更深的层次上缺失时，阈值可能会落在这些值上。'
- en: 'Interestingly, features taking a handful of equidistant values are very common
    in numerous domains. For example:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，许多领域中具有少量等距值的特征非常普遍。例如：
- en: The age feature in medical datasets.
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 医疗数据集中的年龄特征。
- en: Rounded decimals (observed to the, say, 2nd digit will form a lattice).
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 四舍五入的十进制数字（观察到第2位数字的，将形成一个网格）。
- en: Monetary figures quoted in units of millions or billions in financial datasets.
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 财务数据集中以百万或十亿为单位引用的货币金额。
- en: Additionally, almost 97% of features in the toy regression and classification
    datasets in `sklearn.datasets` are of this kind. Therefore, it is not an over-exaggeration
    to say that features taking equidistant values with high probability are present
    everywhere. Consequently, as a rule of thumb, **the deeper trees or forests one
    builds, the more likely it becomes that thresholds interfere with feature values**.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，在`sklearn.datasets`中的玩具回归和分类数据集中，几乎97%的特征都是这种情况。因此，毫不夸张地说，高概率的等距特征几乎无处不在。因此，作为经验法则，**建立更深的树或森林，阈值干扰特征值的可能性就越大**。
- en: '**It is a bias, model selection cannot help!**'
  id: totrans-55
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**这是一种偏差，模型选择无济于事！**'
- en: We have seen that the two conditioning operators (the non-default one imitated
    by the mirroring of data) can lead to different prediction results with statistical
    significance. The two predictions cannot be unbiased at the same time. Therefore,
    we consider the use of either form of conditioning introducing a bias when thresholds
    coincide with feature values.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到，这两个条件运算符（由数据镜像模仿的非默认运算符）可能导致具有统计显著性的不同预测结果。这两个预测不能同时无偏。因此，我们认为，无论使用哪种形式的条件，当阈值与特征值重合时，都引入了偏差。
- en: Alternatively, it is tempting to consider one form of conditioning to be luckily
    aligned with the data and improving the performance. Thus, model selection could
    be used to select the most suitable form of conditioning (or whether the data
    should be mirrored). However, in a particular model selection scenario, using
    some k-fold cross-validation scheme, we can only test which operator is typically
    favorable if, say, 20% of the data is removed (5-fold) from training and then
    used for evaluation. **When a model is trained on all data, other thresholds might
    interfere with feature values, and we have no information on which conditioning
    would improve the performance.**
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 另外，考虑到某种形式的条件可能与数据巧妙对齐并提升性能，这是一种诱人的想法。因此，可以使用模型选择来选择最合适的条件形式（或是否应该镜像数据）。然而，在特定的模型选择场景中，使用一些
    k 折交叉验证方案，我们只能测试在例如，20% 的数据被移除（5 折）用于训练和评估时，哪种操作符通常是更有利的。**当模型在所有数据上训练时，其他阈值可能会干扰特征值，我们没有信息来判断哪种条件会改善性能。**
- en: '**Mitigating the bias in random forests**'
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**减轻随机森林中的偏差**'
- en: A natural way to eliminate the bias is to integrate out the effect of the choice
    of conditioning operators. This involves carrying out predictions with both operators
    and averaging the results.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 消除偏差的自然方法是去除条件操作符选择的影响。这涉及使用两种操作符进行预测并平均结果。
- en: In practice, with random forests, exploiting the equivalence of data mirroring
    and changing the conditioning operator, this can be approximated for basically
    no cost. Instead of using a forest of N_e estimators, one can build two forests
    of half the size, fit one to the original data, the other to the mirrored data,
    and take the average of the results. Note that this approach is applicable with
    any random forest implementation, and has only marginal additional cost (like
    multiplying the data by -1 and averaging the results).
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，通过随机森林，利用数据镜像与改变条件操作符的等效性，这可以基本上以零成本近似完成。与其使用 N_e 个估计器的森林，不如构建两个规模为一半的森林，一个拟合原始数据，另一个拟合镜像数据，并取结果的平均值。请注意，这种方法适用于任何随机森林实现，仅需边际附加成本（如将数据乘以
    -1 并平均结果）。
- en: For example, we implement this strategy in Python below, aiming to integrate
    out the bias from the *sklearn* random forest.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，我们在下面的 Python 代码中实现了这一策略，旨在从*sklearn*随机森林中去除偏差。
- en: '[PRE2]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Next, we can execute the same experiments as before, using the exact same folds:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们可以执行与之前相同的实验，使用完全相同的折叠：
- en: '[PRE3]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Let’s compare the results!
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们比较一下结果！
- en: '[PRE4]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: According to expectations, the r2 score of the unbiased forest falls between
    the scores achieved by the original forest with and without mirroring the data.
    It might seem that eliminating the bias is detrimental to the performance; however,
    we emphasize again that once the forest is fit with all data, the relations might
    be reversed, and the original model might lead to worse predictions than the mirrored
    model. **Eliminating the bias by integrating out the dependence on the conditioning
    operator eliminates the risk of deteriorated performance due to relying on the
    default conditioning operator.**
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 根据预期，无偏森林的 r2 分数介于原始森林的镜像数据有无之间的分数之间。消除偏差可能对性能有害；然而，我们再次强调，一旦森林用所有数据拟合，关系可能会颠倒，原始模型可能会比镜像模型产生更差的预测。**通过去除对条件操作符的依赖来消除偏差，从而消除了由于依赖默认条件操作符而导致性能恶化的风险。**
- en: '**Conclusion**'
  id: totrans-68
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**结论**'
- en: The presence of a bias related to the interaction of the choice of conditioning
    and features taking equidistant values has been established and demonstrated.
    Given the common occurrence of features of this kind, the bias is likely to be
    present in sufficiently deep decision trees and random forests. The potentially
    detrimental effect can be eliminated by averaging the predictions carried out
    by the two conditioning operators. Interestingly, in the case of random forests,
    this can be done at basically no cost. In the example we used, the improvement
    reached the level of 0.1–0.2 percentage points of r2 scores. Finally, we emphasize
    that the results generalize to classification problems and single decision trees
    as well (see [preprint](https://www.researchgate.net/publication/376591586_The_Conditioning_Bias_in_Binary_Decision_Trees_and_Random_Forests_and_Its_Elimination)).
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 已经确认并展示了与条件选择和特征取等距值的交互相关的偏差的存在。考虑到这类特征的普遍存在，这种偏差很可能出现在足够深的决策树和随机森林中。通过对两个条件操作符进行预测平均，可以消除潜在的有害影响。有趣的是，对于随机森林而言，这几乎没有成本。在我们使用的例子中，改进达到了
    `0.1–0.2` 个百分比点的 r2 分数水平。最后，我们强调这些结果同样适用于分类问题和单一决策树（见 [预印本](https://www.researchgate.net/publication/376591586_The_Conditioning_Bias_in_Binary_Decision_Trees_and_Random_Forests_and_Its_Elimination)）。
- en: '**Further reading**'
  id: totrans-70
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**进一步阅读**'
- en: 'For further details, I recommend:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 对于更多细节，我推荐：
- en: 'Our preprint discussing the topic in detail, with many more tests, illustrations
    and alternative bias mitigation techniques: [preprint](https://www.researchgate.net/publication/376591586_The_Conditioning_Bias_in_Binary_Decision_Trees_and_Random_Forests_and_Its_Elimination),'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们详细讨论此主题的预印本，包含更多测试、插图和替代偏差缓解技术：[预印本](https://www.researchgate.net/publication/376591586_The_Conditioning_Bias_in_Binary_Decision_Trees_and_Random_Forests_and_Its_Elimination)。
- en: 'The GitHub repository with the reproducible analysis: [https://github.com/gykovacs/conditioning_bias](https://github.com/gykovacs/conditioning_bias)'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 包含可重复分析的 GitHub 仓库：[https://github.com/gykovacs/conditioning_bias](https://github.com/gykovacs/conditioning_bias)
- en: 'The notebook behind this post: [https://github.com/gykovacs/conditioning_bias/blob/main/blogpost/001-analysis.ipynb](https://github.com/gykovacs/conditioning_bias/blob/main/blogpost/001-analysis.ipynb)'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 本文背后的笔记本：[https://github.com/gykovacs/conditioning_bias/blob/main/blogpost/001-analysis.ipynb](https://github.com/gykovacs/conditioning_bias/blob/main/blogpost/001-analysis.ipynb)
- en: '*If you are interested in more content like this, don’t forget to subscribe!
    You can also find me on* [LinkedIn](https://www.linkedin.com/in/gyorgy-kovacs-a9799727/)*!*'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '*如果你对类似内容感兴趣，不要忘记订阅！你还可以在* [LinkedIn](https://www.linkedin.com/in/gyorgy-kovacs-a9799727/)*上找到我！*'
