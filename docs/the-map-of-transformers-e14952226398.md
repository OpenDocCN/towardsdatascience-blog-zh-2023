# 变换器的地图

> 原文：[https://towardsdatascience.com/the-map-of-transformers-e14952226398?source=collection_archive---------4-----------------------#2023-04-18](https://towardsdatascience.com/the-map-of-transformers-e14952226398?source=collection_archive---------4-----------------------#2023-04-18)

## 变换器

## 变换器研究的广泛概述

[](https://soran-ghaderi.medium.com/?source=post_page-----e14952226398--------------------------------)[![Soran Ghaderi](../Images/49d2b0022c2962d8ad7af5017383374f.png)](https://soran-ghaderi.medium.com/?source=post_page-----e14952226398--------------------------------)[](https://towardsdatascience.com/?source=post_page-----e14952226398--------------------------------)[![Towards Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----e14952226398--------------------------------) [Soran Ghaderi](https://soran-ghaderi.medium.com/?source=post_page-----e14952226398--------------------------------)

·

[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fd2b75b0bb761&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-map-of-transformers-e14952226398&user=Soran+Ghaderi&userId=d2b75b0bb761&source=post_page-d2b75b0bb761----e14952226398---------------------post_header-----------) 在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----e14952226398--------------------------------) 上发表 · 25 分钟阅读 · 2023年4月18日 [](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fe14952226398&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-map-of-transformers-e14952226398&user=Soran+Ghaderi&userId=d2b75b0bb761&source=-----e14952226398---------------------clap_footer-----------)

--

[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fe14952226398&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-map-of-transformers-e14952226398&source=-----e14952226398---------------------bookmark_footer-----------)![](../Images/4c821282032da4ddc63dc6024fcab1a0.png)

图 1\. 等距地图。由 [vectorpocket / Freepik](http://www.freepik.com) 设计。

# 1\. 介绍

近年来，深度学习的研究进展显著加快，使得跟踪所有最新发展的难度越来越大。尽管如此，有一个特定的研究方向由于其在自然语言处理、计算机视觉和音频处理等多个领域的显著成功，受到了广泛关注。这在很大程度上归功于其高度适应的架构。这个模型被称为Transformer，它利用了该领域的一系列机制和技术（即注意力机制）。你可以在以下文章中深入了解这些构建块及其实现，并查看多个插图：

[](/transformers-in-action-attention-is-all-you-need-ac10338a023a?source=post_page-----e14952226398--------------------------------) [## Transformer的应用：注意力即是全部

### 简要调查、插图和实现

[towardsdatascience.com](/transformers-in-action-attention-is-all-you-need-ac10338a023a?source=post_page-----e14952226398--------------------------------)

这篇文章提供了更多关于注意力机制的细节，我将在本文中讨论这些机制：

[](/rethinking-thinking-how-do-attention-mechanisms-actually-work-a6f67d313f99?source=post_page-----e14952226398--------------------------------) [## 重新思考：注意力机制究竟是如何工作的？

### 大脑、数学与深度学习——研究前沿

[towardsdatascience.com](/rethinking-thinking-how-do-attention-mechanisms-actually-work-a6f67d313f99?source=post_page-----e14952226398--------------------------------)

# 2\. Transformer的分类

到目前为止，基于原始Transformer已经探索了广泛的模型，这些模型大致可以分为三类：

+   架构修改

+   预训练方法

+   应用

![](../Images/7491b188970493b11bc9aac813510f92.png)

图2. Transformer变体修改。照片由[作者](https://github.com/soran-ghaderi)提供。

上述每个类别包含几个子类别，我将在接下来的部分中深入探讨。图2展示了研究人员对Transformer的修改类别。

# 3\. 注意力

自注意力在Transformer中发挥了基本作用，但在实践中存在两个主要缺点[1]。

1.  **复杂性**：对于长序列，这个模块变成了瓶颈，因为其计算复杂度为O(T²·D)。

1.  **结构先验**：它没有处理输入的结构偏差，需要在训练数据中注入额外的机制，模型才能后续学习（例如学习输入序列的顺序信息）。

![](../Images/56cdb5c915c769faa741c72e1dd844bc.png)

图3. 注意力修改类别及示例论文。照片由[作者](https://www.linkedin.com/in/soran-ghaderi/)提供。

因此，研究人员探索了各种技术来克服这些缺陷。

1.  **稀疏注意力：** 该技术通过只考虑输入序列的一小部分而不是整个输入序列来降低注意力机制的计算时间和内存需求，从而产生稀疏矩阵，相较于全矩阵。

1.  **线性化注意力：** 通过使用核特征映射解构注意力矩阵，这种方法尝试以反向顺序计算注意力，从而将资源需求降低到线性复杂度。

1.  **原型和内存压缩：** 这种修改线试图减少查询和键-值对，以实现较小的注意力矩阵，从而减少时间和计算复杂度。

1.  **低秩自注意力：** 通过显式建模自注意力矩阵的低秩特性，使用参数化或用低秩近似替代，以期提高变换器的性能。

1.  **带有先验的注意力：** 利用来自其他来源的先验注意力分布，这种方法将其他注意力分布与从输入中获得的注意力分布结合起来。

1.  **修改的多头机制：** 有多种方法可以修改和提高多头机制的性能，这些方法可以归类于这一研究方向。

## 3.1. 稀疏注意力

变换器中的标准自注意力机制要求每个标记都关注所有其他标记。然而，已经观察到在许多情况下，注意力矩阵通常非常稀疏，这意味着只有少量标记实际上彼此关注[2]。这表明，可以通过限制每个查询关注的查询-键对的数量来减少自注意力机制的计算复杂度。通过仅计算预定义模式的查询-键对的相似性分数，可以在不牺牲性能的情况下显著减少所需的计算量。

![](../Images/91697d5a6d8416a50cbc58e164a206d9.png)

Eq. 1

在非规范化的注意力矩阵Â中，-∞项通常不会存储在内存中，以减少内存占用。这是为了降低实现矩阵所需的内存量，从而提高系统的效率和性能。

我们可以将注意力矩阵映射到一个二分图，其中标准注意力机制可以被认为是一个完整的二分图，每个查询从记忆中的所有节点接收信息，并利用这些信息更新其表示。这样，注意力机制允许每个查询关注记忆中的所有其他节点，并将它们的信息纳入其表示中。这使得模型能够捕捉记忆节点之间复杂的关系和依赖性。另一方面，稀疏注意力机制可以被认为是一个稀疏图。这意味着图中的并非所有节点都相互连接，这可以减少系统的计算复杂性，提高其效率和性能。通过限制节点之间的连接数量，稀疏注意力机制仍然可以捕捉重要的关系和依赖性，但计算开销较小。

稀疏注意力方法主要有两类，基于用于确定节点之间稀疏连接的指标[1]。这两类是**基于位置**和**基于内容**的稀疏注意力。

## 3.1.1\. 基于位置的稀疏注意力

在这种类型的注意力中，注意力矩阵中的连接是根据预定模式进行限制的。它们可以表示为更简单模式的组合，这对理解和分析注意力机制的行为非常有用。

![](../Images/654f80182028882725b4412ae9ed3394.png)

图 4\. 主要的原子稀疏注意力模式。彩色方块展示了相应的计算注意力分数。图片来源于[[1]](https://arxiv.org/abs/2106.04554v2)。

**3.1.1.1\. 原子稀疏注意力：** 有五种基本的原子稀疏注意力模式可以用来构建各种不同的稀疏注意力机制，这些机制在计算复杂性和性能之间有不同的权衡，如图 4 所示。

1.  **全局注意力：** 全局节点可以作为信息中心，能够关注序列中的所有其他节点，反之亦然，如图 4 (a) 所示。

1.  **带状注意力（也称为滑动窗口注意力或局部注意力）：** 数据不同部分之间的关系和依赖性通常是局部的而非全局的。在带状注意力中，注意力矩阵是一个带状矩阵，查询仅关注两侧一定数量的邻近节点，如图 4 (b) 所示。

1.  **扩张注意力：** 类似于扩张卷积神经网络（CNNs）可以在不增加计算复杂性的情况下扩大感受野，通过使用扩张窗口（扩张*𝑤_d* >= 1）可以实现带状注意力的类似效果，如图 4 (c) 所示。此外，它也可以扩展到步幅注意力，其中扩张𝑤 𝑑 被认为是一个较大的值。

1.  **随机注意力：** 为了提高注意力机制捕捉非局部交互的能力，可以为每个查询随机采样一些边，如图4(d)所示。

1.  **块级局部注意力：** 输入序列被分割成几个互不交叉的查询块，每个查询块都关联一个局部内存块。每个查询块中的查询仅关注相应内存块中的键，如图3(e)所示。

**3.1.1.2\. 复合稀疏注意力：** 如图5所示，许多现有的稀疏注意力机制由上述描述的多个原子模式组成。

![](../Images/fc2ed23f12fc57d7c3cda43bcedf436c.png)

图5\. 四种不同的复合稀疏注意力模式。图片来自[[1]](https://arxiv.org/abs/2106.04554v2)。

**3.1.1.3\. 扩展稀疏注意力：** 还有其他类型的模式已被探索用于特定数据类型。例如，BP-Transformer [3] 使用二叉树来捕捉输入序列中全局和局部注意力的组合。令牌是叶节点，内部节点是包含多个令牌的跨度节点。图6展示了多种扩展稀疏注意力模式。

![](../Images/33205e4eb8177520d005fdfc0cac64b5.png)

图6\. 不同的扩展稀疏注意力模式。图片来自[[1]](https://arxiv.org/abs/2106.04554v2)。

## 3.1.2\. 基于内容的稀疏注意力

在这种方法中，构建一个稀疏图，其中稀疏连接基于输入。它选择与给定查询具有高相似度的键。构建此图的高效方法是使用最大内积搜索（MIPS），该方法在不计算所有点积的情况下找到键与查询之间的最大点积。

![](../Images/ef21f3839a71e0eaac3848b36f39baca.png)

图7\. Routing Transformer的2-D注意力方案，与局部注意力和跨步注意力相比。图片来自[[4](https://arxiv.org/abs/2003.05997)]

Routing Transformer [4] 如图7所示，通过使用在线k-means聚类对键和值进行同心心向量聚类，为自注意力机制配备了稀疏路由模块。它将查询隔离，只关注同一簇内的键。Reformer [5] 使用局部敏感哈希（LSH）代替点积注意力，为每个查询选择键和值。它使查询仅关注来自LSH生成的同一桶中的令牌。使用LSTM边预测器，稀疏自适应连接（SAC） [6] 从输入序列中构建图，并通过利用自适应稀疏连接来增强任务特定的性能。

## 3.2\. 线性化注意力

点积注意力机制的计算复杂度（softmax(QK^⊤)V）随着输入的空间时间大小（长度）的增加而呈二次增加。因此，当暴露于大输入（如视频、长序列或高分辨率图像）时，它阻碍了其使用。通过将 softmax(QK^⊤) 解开成 Q′ K′^⊤，(Q′ K′^⊤ V) 可以按相反顺序计算，结果是线性复杂度 O(𝑇)。

![](../Images/4dc671f6875350317d9d0f7768061ba3.png)

图 8\. 标准自注意力和线性化自注意力的复杂度差异。图片来自 [[1](https://arxiv.org/abs/2106.04554v2)]。

假设 Â = exp(QK^⊤) 表示未归一化的注意力矩阵，其中 exp(.) 逐元素应用，线性化注意力是一种近似未归一化注意力矩阵 exp(QK^⊤) 的技术，其形式为 𝜙(Q) 𝜙(K)^⊤，其中 𝜙 是逐行特征映射。通过应用这种技术，我们可以执行 𝜙(Q) (𝜙(K)^⊤ V)，这是未归一化注意力矩阵的线性化计算，如图 8 所示。

为了更深入地理解线性化注意力，我将探索向量形式的公式。我将检查注意力的一般形式，以获得进一步的洞见。

![](../Images/a652c4b566db4a5e99dd1caa2935f3b5.png)

公式 2

在这个背景下，sim(·, ·) 是衡量输入向量相似性的评分函数。在标准 Transformer 中，评分函数是内积的指数形式，exp(⟨·, ·⟩)。一个适合的选择是核函数 sim(·, ·) = K(x, y) = 𝜙(x)𝜙(y)^⊤ ，这进一步揭示了线性化注意力的洞见。

![](../Images/c3d9b54c2ac39325feb71330701caa5d.png)

公式 3

在这个公式中，向量的外积用 ⊗ 表示。注意力可以通过首先计算突出显示的术语来线性化，这允许自回归模型，即 Transformer 解码器像 RNN 一样运行。

公式 2 表明，通过聚合（特征映射后的）键和查询的外积，它保留了一个内存矩阵。稍后通过将内存矩阵与特征映射后的查询乘以适当的归一化来检索它。

这种方法由两个基础组件组成：

+   **特征映射 𝜙 (·)：** 每种注意力实现的核特征映射（例如，Linear Transformer 提出的𝜙𝑖(x) = elu(𝑥 𝑖 )+1）。

+   **聚合规则：** 通过简单求和将关联 {𝜙 (k)𝑗 ⊗ v𝑗} 聚合到内存矩阵中。

## 3.3\. 查询原型化和内存压缩

除了使用稀疏注意力或基于核的线性化注意力，还可以通过减少查询或键值对的数量来缓解注意力的复杂性，从而引入查询原型和内存压缩技术。

![](../Images/7c0ff26beef060b9c57b31245f7634db.png)

图 9\. 查询原型化和内存压缩。照片来自 [[1](https://arxiv.org/abs/2106.04554v2)]

**3.3.1\. 带原型查询的注意力：** 实施带原型查询的注意力涉及使用一组查询原型作为计算注意力分布的主要依据。模型采用两种不同的方法，要么将计算得到的分布复制到代表查询的位置，要么在这些位置填充离散均匀分布。该过程的计算流程如图9(a)所示。

聚类注意力，如[7]所述，涉及将查询聚合到几个簇中，注意力分布则是针对这些簇的质心计算的。簇内所有查询都被分配给相应质心计算出的注意力分布。

Informer，如[8]所述，采用了显式查询稀疏度测量的方法，该方法源自于对查询的注意力分布与离散均匀分布之间Kullback-Leibler散度的近似，以选择查询原型。然后，仅对由查询稀疏度测量确定为前𝑢个的查询计算注意力分布，而其余查询则分配离散均匀分布。

**3.3.2\. 压缩键值内存的注意力：** 这种技术通过在应用注意力之前减少键-值对的数量来降低Transformer中注意力机制的复杂性，如图9(b)所示。这通过压缩键值内存来实现。压缩内存然后用于计算注意力分数。这种技术可以显著降低注意力的计算成本，同时在各种自然语言处理任务上保持良好的性能。

*Liu et al. [9]* 在其论文中提出了一种名为 *Memory Compressed Attention (MCA)* 的技术。*MCA* 使用跨步卷积来减少键和值的数量。*MCA* 与本文中也提出的局部注意力一起使用。通过将键和值的数量减少到卷积核大小的因子，*MCA* 能够捕获全局上下文并处理比标准Transformer模型更长的序列，同时保持相同的计算资源。

*Set Transformer* [10] 和 *Luna* [11] 是两个利用外部可训练的全局节点来压缩输入信息的模型。压缩表示然后作为输入的注意力的压缩内存，有效地将自注意力的二次复杂度降低到与输入序列长度线性相关的复杂度。

*Linformer* [12]将自注意力的计算复杂度线性降低，通过将键和值从长度为 *n* 线性投影到更小的长度 *n_k*。这种方法的缺点是预设的输入序列长度，因此不适合自回归注意力模型。

*Poolingformer* [13] 采用了一个两级注意力机制，将滑动窗口注意力与压缩内存注意力相结合。压缩内存注意力有助于扩大感受野。为了减少键和值的数量，探索了几种池化操作，包括最大池化和基于动态卷积的池化。

## 3.4\. 低秩自注意力

根据各种研究者 [14, 12] 进行的实证和理论分析，自注意力矩阵 A ∈ R𝑇 ×𝑇 在许多情况下表现出低秩特性。这一观察提供了两个含义：首先，可以使用参数化显式建模低秩特性。这可能会导致开发利用这一特性以提高性能的新模型。其次，可以用低秩近似代替完整的自注意力矩阵。这种方法可以实现更高效的计算，并进一步提高基于自注意力的模型的可扩展性。

**3.4.1\. 低秩参数化：** 当注意力矩阵的秩低于序列长度时，这表明通过设置 𝐷𝑘 > 𝑇 来过度参数化模型会导致在输入通常较短的情况下出现过拟合。因此，限制 𝐷𝑘 的维度并利用低秩特性作为归纳偏差是明智的。为此，Guo 等人 [14] 提出了将自注意力矩阵分解为一个小的 𝐷𝑘 低秩注意力模块，用于捕捉长距离的非局部交互，以及一个带状注意力模块，用于捕捉局部依赖。这种方法在输入较短且需要有效建模局部和非局部依赖的场景中可能会有所帮助。

**3.4.2\. 低秩近似：** 也可以利用注意力矩阵的低秩特性，通过使用低秩矩阵近似来降低自注意力的复杂性。这种方法与核矩阵的低秩近似密切相关，一些现有工作受到核近似的启发。例如，Performer（如第 3.2 节所讨论的）使用了一种最初用于近似高斯核的随机特征映射，将注意力分布矩阵 A 分解为 C𝑄 GC𝐾，其中 G 是高斯核矩阵，随机特征映射近似 G。

处理注意力矩阵低秩特性的一种替代方法是使用基于 Nyström 的方法 [15, 16]。在这些方法中，从输入序列中选择一部分地标节点，使用下采样技术，如步长平均池化。选择的地标节点被用作查询和键，以近似注意力矩阵。具体而言，注意力计算包括对原始查询与选择的键的乘积进行 softmax 归一化，然后计算选择的查询与归一化结果的乘积。这可以表示为：

![](../Images/262220a7ba22b64149ec700c89c9b3a4.png)

方程 4

注意，矩阵 **M**^-1 = (softmax(Q̃K̃^T))^-1 的逆可能并不总是存在，但可以通过多种方式减轻此问题。例如，CSALR [15] 向 **M** 添加单位矩阵以确保逆矩阵始终存在，而 Nyström-former [16] 使用 **M** 的 Moore-Penrose 伪逆来处理奇异情况。

## 3.5\. 带先验的注意力

注意力机制是一种关注输入序列中特定部分的方法。它通过生成序列中向量的加权和来实现，其中权重由注意力分布决定。注意力分布可以从输入中生成，也可以来自其他来源，如先验知识。在大多数情况下，输入的注意力分布和先验注意力分布通过计算它们分数的加权和后结合，然后应用 softmax，从而使神经网络能够从输入和先验知识中学习。

![](../Images/f3d9acc3d75dd3afcdc64f246b3f0bdb.png)

图 10\. 带先验的注意力将生成的注意力分数和先验注意力分数结合起来计算最终的注意力分数。图片来自 [[1]](https://arxiv.org/abs/2106.04554v2)。

**3.5.1\. 建模局部性的先验：** 为了建模某些类型数据的局部性，如文本，可以使用位置上的高斯分布作为先验注意力。这涉及到将生成的注意力分布与高斯密度相乘，并对生成的注意力分数进行归一化或添加偏置项 G，其中更高的 G 表示对特定输入的先验概率更高。

Yang 等人 [17] 提出了预测每个输入的中心位置并相应地定义高斯偏置的方法：

![](../Images/af94364ef136d40e6cb207f656871ecb.png)

方程 5

其中 𝜎 表示高斯的标准差。高斯偏置定义为中心位置与输入位置之间的平方距离的负值，除以高斯分布的标准差。标准差可以作为超参数确定，也可以从输入中预测。

![](../Images/fbbdad900d98f2d1f54606ee25c4d9bc.png)

图 11\. 使用窗口大小为 2 (D = 2) 的[17]提出的方法进行说明。图片来自 [[17](https://aclanthology.org/D18-1475/)]。

高斯 Transformer [18] 模型假设每个输入查询 𝑞𝑖 的中心位置为 𝑖，并将生成的注意力分数的偏置项 𝐺𝑖 𝑗 定义为

![](../Images/00d462bccc16e5a6fdbe1ea8805bdf2b.png)

方程 6

其中 𝑤 是一个非负标量参数，控制偏差，𝑏 是一个负标量参数，减少中心位置的权重。

**3.5.2\. 来源于低层模块：** 在 Transformer 架构中，相邻层之间的注意力分布通常被发现是相似的。因此，使用来自低层的注意力分布作为计算高层注意力的先验是合理的。这可以通过将当前层的注意力分数与前一层注意力分数的加权和以及将前一层分数映射到要应用的先验的转换函数相结合来实现。

![](../Images/97d7e1995b33ec4e42a969251bb4f0eb.png)

Eq. 7

其中 A(𝑙) 代表 *l-* 层注意力分数，而 *w*1 和 *w*2 控制之前的注意力分数和当前注意力分数的相对重要性。此外，函数 𝑔: R𝑛×𝑛 → R𝑛×𝑛 将之前的注意力分数转化为应用于当前注意力分数的先验。

论文 [19] 中提出的 *Predictive Attention Transformer* 建议在之前的注意力分数上使用 2D 卷积层来计算最终的注意力分数，该分数是生成的注意力分数和卷积分数的凸组合。换句话说，生成和卷积分数的权重参数分别设置为 𝛼 和 1-𝛼，而 Eq. (6) 中的函数 𝑔(·) 是一个卷积层。论文中的实验表明，无论是从头开始训练模型还是在适配预训练 BERT 模型后进行微调，都比基线模型有了改进。

论文 [20] 中提出的 *Realformer* 模型通过直接将之前的注意力分数添加到新生成的分数中，引入了对注意力图的残差跳跃连接。这可以视为在 Eq. (6) 中将 𝑤 1 = 𝑤 2 = 1 和 𝑔(·) 设置为恒等映射。作者在该模型上进行的预训练实验报告称，该模型在多个数据集上优于基线 BERT 模型，即使在显著降低预训练预算的情况下。

*Lazyformer* [21] 提出了一个创新方法，通过在相邻层之间共享注意力图来降低计算成本。这是通过将 𝑔(·) 设为恒等映射并交替切换 𝑤 1 = 0, 𝑤 2 = 1 和 𝑤 1 = 1, 𝑤 2 = 0 的设置来实现的。这种方法使得只需计算一次注意力图，并在后续层中重复使用。Lazyformer 进行的预训练实验表明，他们的模型不仅高效，而且有效，超越了基线模型，并显著降低了计算预算。

**3.5.3\. 作为多任务适配器的先验：** 作为多任务适配器的先验方法使用可训练的注意力先验，这些先验使得跨任务的参数共享更为高效 [22]。条件自适应多任务学习（CAMTL）[23] 框架是一种多任务学习技术，它使得在任务之间高效共享预训练模型成为可能。CAMTL 使用依赖于任务编码的可训练注意力先验，作为多任务诱导知识转移的适配器。具体来说，注意力先验被表示为块对角矩阵，添加到预训练 Transformer 的上层注意力分数中：

![](../Images/cca478938fd9c353b29199a52fe294f5.png)

Eq. 8

在其中，⊕ 表示直接和，𝐴𝑗 是具有 (𝑛/𝑚)×(𝑛/𝑚) 维度的可训练参数，𝛾𝑗 和 𝛽𝑗 是具有输入和输出维度为 R𝐷𝑧 和 (𝑛/𝑚)×(𝑛/𝑚) 的特征线性调制函数 [24]。CAMTL 框架在实现中规定了最大序列长度 𝑛𝑚𝑎𝑥。注意力先验是一个可训练矩阵，它被添加到预训练 Transformer 的上层注意力分数中。这种添加创建了一个适配器，使得多任务诱导知识转移在参数上更高效。先验被组织为块对角矩阵以提高计算效率。

**3.5.4\. 仅使用先验的注意力：** Zhang 等人 [25] 开发了一种替代的注意力分布方法，该方法不依赖于输入之间的成对交互。他们的方法称为“平均注意力网络”，它使用离散均匀分布作为注意力分布的唯一来源。然后将这些值聚合为所有值的累积平均值。为了增强网络的表达能力，在平均注意力模块上添加了一个前馈门控层。这种方法的好处是，修改后的 Transformer 解码器可以以并行方式进行训练，并且能够像 RNN 一样解码，避免了与解码相关的 O(𝑇²) 复杂性。

类似于 Yang 等人 [17] 和 Guo 等人 [18]，它们使用固定的局部窗口进行注意力分布，You 等人 [26] 将硬编码的高斯分布注意力用于注意力计算。然而，他们完全忽略了计算得到的注意力，只使用高斯分布进行注意力计算，其中均值和方差是超参数。只要在自注意力上实现，它就可以在机器翻译任务中产生接近基线模型的结果。

Synthesizer [27] 提出了一种在 Transformers 中生成注意力分数的新方法。他们用两种变体替代传统的生成注意力分数的方法：(1) 可学习的、随机初始化的注意力分数，以及 (2) 由仅对输入进行条件处理的前馈网络输出的注意力分数。他们在机器翻译和语言建模任务上的实验结果表明，这些变体的表现与标准 Transformer 模型相当。然而，这些变体为何有效尚未完全解释，仍有进一步研究的空间。

## 3.6\. 改进的多头机制

多头注意力是一种强大的技术，因为它允许模型同时关注输入的不同部分。然而，不能保证每个注意力头都会学习到独特且互补的特征。因此，一些研究人员探索了确保每个注意力头捕捉到不同信息的方法。

**3.6.1\. 头部行为建模：** 多头注意力是自然语言处理模型中的一个有用工具，因为它允许同时处理多个输入和特征表示[28]。然而，传统的 Transformer 模型缺乏确保不同注意力头捕捉到不同且非冗余特征的机制。此外，也没有头部之间相互作用的规定。为了解决这些局限性，近期的研究集中于引入新颖的机制来指导注意力头的行为或使它们之间能够进行交互。

为了促进不同注意力头之间的多样性，Li 等人 [29] 在损失函数中提出了额外的正则化项。这个正则化由两部分组成：前两部分旨在最大化输入子空间和输出表示之间的余弦距离，而后者通过元素级乘法鼓励多个头部关注的位置的分散。通过添加这一辅助项，模型被鼓励在不同头部之间学习到更多样的注意力模式，从而提高在各种任务上的性能。

许多研究表明，预训练的 Transformer 模型展示了某些自注意力模式，这些模式与自然语言处理并不完全契合。Kovaleva 等人 [30] 在 BERT 中识别出其中的几种模式，包括专注于特殊标记 [CLS] 和 [SEP] 的注意力头。为了改进训练，Deshpande 和 Narasimhan [31] 提出了使用辅助损失函数，该函数测量注意力分布图与预定义注意力模式之间的 Frobenius 范数。这种方法引入了约束，以鼓励更有意义的注意力模式。

在Shen等人[32]的论文中，提出了一种名为“Talking-head Attention”的新机制，该机制旨在鼓励模型以可学习的方式在不同的注意力头之间传递信息。该机制包括将生成的注意力分数从隐藏维度线性投影到具有h_k个头的新空间，在该空间中应用softmax，然后将结果投影到另一个具有h_v个头的空间以进行值聚合。通过这种方式，注意力机制可以学习在不同注意力头之间动态传递信息，从而提高各种自然语言处理任务的性能。

协作多头注意力是一种在[33]中提出的机制，涉及使用共享的查询和键投影，分别记作W𝑄和W𝐾，以及一个混合向量m𝑖。该混合向量用于过滤𝑖-th头的投影参数。具体而言，注意力计算被调整以反映这一机制，从而得出修改后的方程（3）。

![](../Images/bc4e7c6e5518925240d2d2681e63b0a7.png)

Eq. 9

其中所有头共享W^q和W^k。

**3.6.2\. 具有限制范围的多头注意力：**

标准的注意力机制通常假设全范围注意力，允许查询对所有键值对进行注意。然而，已经观察到一些注意力头更倾向于关注局部上下文，而其他注意力头则关注更广泛的上下文。因此，对特定目的施加注意力范围的约束可能是有利的：

+   局部性：限制注意力范围可以明确施加局部约束，这在局部性是重要考虑因素的情况下尤为有益。

+   效率：如果实施得当，这种模型可以在不增加额外内存使用或计算时间的情况下扩展到更长的序列。

限制注意力范围涉及将每个注意力分布值与掩码值相乘，然后进行重新归一化。掩码值可以由一个非递增函数确定，该函数将距离映射到[0, 1]范围内的一个值。在标准注意力中，对于所有距离分配掩码值为1，如图12(a)所示。

![](../Images/6398180ac8234a6b560c662fad7c4831.png)

图12展示了三种不同类型的范围掩码函数，记作𝑚(𝑥)。水平轴表示距离𝑥，而垂直轴表示相应的掩码值。这一视觉表示提供了这些掩码函数展示的不同行为和模式的洞见，清晰地展示了掩码值如何随着键值对之间的距离变化。图片来源于[[1]](https://arxiv.org/abs/2106.04554v2)。

在Sukhbaatar等人 [34] 的研究中，提出了一种新颖的方法，引入了一个可学习的注意力范围，如有趣的图12(b)所示。这种创新技术利用了由可学习标量 𝑧 参数化的掩码，并结合超参数 𝑅，自适应地调节注意力范围。实验结果表明，这些自适应范围模型在字符级语言建模上优于基线模型，同时需要显著更少的FLOPS。值得注意的是，一个有趣的观察是模型的低层通常显示出较小的学习范围，而高层则表现出较大的范围。这一有趣发现表明模型可以自主学习特征的层次组合，展示了其捕捉数据中复杂模式和结构的卓越能力。

*多尺度变换器* [35] 提出了一个新的注意力范围方法，挑战了传统范式。与假设所有头部具有统一注意力范围的普通注意力不同，该创新模型引入了一个具有动态缩放的固定注意力范围。如图12(c)所示，固定注意力范围作为一个可以缩放的窗口，由标记为 𝑤 的尺度值控制。

![](../Images/6ac644da107d37c3434399808e584639.png)

图13\. 多尺度多头自注意力，其中描绘了三个头部，每个头部代表不同的尺度。蓝色、绿色和红色框分别表示尺度 ω = 1、ω = 3 和 ω = 5。照片由 [作者](https://www.linkedin.com/in/soran-ghaderi/) 提供。

尺度值各异，高层倾向于使用较大的尺度以获取更广泛的上下文依赖，而低层则选择较小的尺度以获得更局部的关注，如图13所示。多尺度变换器的实验结果显示，它在各种任务上表现出优于基线模型的性能，展示了其在语言处理上的更高效和有效的潜力。

**3.6.3\. 多头与精细化聚合：**

普通的多头注意力机制，如Vaswani等人 [28] 提出的，涉及计算多个并行操作的注意力头，以生成各自的输出表示。这些表示然后被串联，并进行线性变换，如Eq. (11) 定义，以获得最终的输出表示。通过结合 Eq. (10)、(11) 和 (12)，可以观察到这种串联与投影的形式等同于对重新参数化的注意力输出进行求和。这种方法允许高效地聚合多样的注意力头输出，使模型能够捕捉输入数据中的复杂依赖关系和关系。

![](../Images/07b6abd9b22daba4413bd081f9b09d01.png)

Eq. 10

和

![](../Images/c6fe42c1c28247a1f9ad3f0c572be9dc.png)

Eq. 11

其中

![](../Images/9d6b8fb6740b786dda46a499be5a1038.png)

Eq. 12

为了便于聚合过程，线性变换使用的权重矩阵W𝑂 ∈ R𝐷𝑚 ×𝐷𝑚 被划分为𝐻个块，其中𝐻表示注意力头的数量。

![](../Images/eef0efcb393851c8ccb50c23f836825a.png)

Eq. 13

权重矩阵W𝑂_𝑖，维度为𝐷𝑣 × 𝐷𝑚，用于每个注意力头中的线性变换，通过连接-投影公式重新参数化注意力输出，如Eq. (14)所定义：

![](../Images/f2ecb33a4229e43be136ee5f3e0da112.png)

Eq. 14

一些研究人员可能会争辩说，简单的加和聚合方法可能无法充分利用多头注意力的表达能力，更复杂的聚合方案可能更为理想。

Gu 和 Feng [36] 以及 Li 等人 [37] 提出了使用最初为胶囊网络 [38] 设计的路由方法，作为进一步聚合来自不同注意力头信息的一种手段。通过将注意力头的输出转化为输入胶囊并随后经历迭代路由过程，获得输出胶囊。这些输出胶囊随后被连接作为多头注意力机制的最终输出。值得注意的是，这些工作中使用的动态路由 [38] 和 EM 路由 [39] 机制引入了额外的参数和计算开销。然而，Li 等人 [37] 实证表明，选择性地将路由机制应用于模型的较低层次能在翻译性能和计算效率之间取得最佳平衡。

**3.6.4\. 其他多头修改：**

除了上述修改之外，还提出了几种其他方法来增强多头注意力机制的性能。Shazeer [40] 引入了多查询注意力的概念，其中键值对在所有注意力头之间共享。这减少了解码过程中的内存带宽需求，并使解码速度更快，尽管与基线相比存在轻微的质量下降。另一方面，Bhojanapalli 等人 [41] 发现注意力键的大小可能会影响它们表示任意分布的能力。为了解决这个问题，他们提出将头的大小与头的数量分开，这与传统上将头的大小设定为𝐷𝑚/ℎ的做法相反，其中𝐷𝑚是模型维度，ℎ是头的数量。

# 4\. 总结

总结来说，变换器的分类及注意机制的各种进展显著扩展了基于变换器的模型的能力和效率。稀疏注意技术，如基于位置和基于内容的稀疏注意，以及线性化注意，解决了传统稠密注意的计算限制。查询原型和内存压缩方法引入了创新的方法来提高注意机制的效率。低秩自注意使得参数化和近似技术用于更高效的注意计算。引入先验，如局部建模、较低模块先验和多任务适配器，在提高注意机制方面表现出令人鼓舞的结果。最后，对多头机制的修改，如头行为建模、受限跨度、精细聚合及其他变体，展示了进一步提升基于变换器的模型性能的潜力。

注意机制的这些进展为未来在自然语言处理、计算机视觉和机器翻译等多个领域的研究和应用提供了令人兴奋的前景。通过利用这些创新技术，基于变换器的模型可以继续突破性能和效率的界限，为高级机器学习应用打开新的可能性。

**请在下方评论区分享你的想法、问题和意见。**

# 5\. 即将到来的主题：揭示变换器旅程中的下一个章节

在未来的文章中，将更详细地讨论以下主题：

1.  **其他模块级修改：** 这些涵盖了模块级的其他修改，如位置表示、层归一化和位置-wise 前馈网络（FFN），它们在基于变换器的模型的性能和效率中发挥了至关重要的作用。

1.  **架构级变体：** 本节将探讨变换器的各种架构级变体，包括将变换器调整为轻量级、增强跨块连接、适应性计算时间、采用分而治之策略的变换器，以及探索替代架构设计以进一步提升变换器的能力和效率。

1.  **预训练变换器：** 深入探讨预训练变换器，这些变换器近年来因其利用大规模预训练数据提高下游任务性能的能力而受到广泛关注，本节将讨论不同的预训练技术，如 BERT、GPT 和 T5。

1.  **变压器的应用：** 在这一部分中将突出显示变压器在各种领域展示出卓越性能的多样化应用，包括自然语言处理、计算机视觉、语音识别和推荐系统等。将讨论变压器在各个领域中的潜力和多样性。

1.  **研究方向：** 提供关于变压器研究和开发未来方向的见解，讨论变压器模型在新兴趋势、挑战和机会方面的进一步发展，展示变压器在未来几年里的令人兴奋的可能性。

通过涵盖这些主题，本文旨在全面介绍变压器的进展、修改、应用和未来发展方向，揭示这些强大模型在推动下一代机器学习和人工智能应用中的潜力。

# 参与TransformerX项目：参与方式

[](https://github.com/tensorops/TransformerX?source=post_page-----e14952226398--------------------------------) [## GitHub - tensorops/TransformerX: Flexible Python library providing building blocks (layers) for…

### 灵活的Python库，为可重现的Transformer研究提供构建块（层）（Tensorflow ✅…

[github.com](https://github.com/tensorops/TransformerX?source=post_page-----e14952226398--------------------------------)

我感谢那些在开发TransformerX库中给予启发和鼓励的人。我们一直期待您以以下形式做出贡献：

+   **贡献代码和开发新层次：** 查看[特定问题](https://github.com/tensorops/TransformerX/issues?q=is%3Aissue+is%3Aopen+label%3A%22issue+list%22)标记为`issue_list`的想法，并在我们的指南的帮助下开始实施它们。

+   **建议新功能或报告错误：** 创建问题来分享你的建议或报告你遇到的任何问题。

+   **编写文档和教程资源：** 帮助我们改进库的文档。

+   **在社交媒体上分享和撰写TransformerX相关内容：** 在Twitter上提及我们，进行转发，或在LinkedIn上分享。

如果你不确定编码，不用担心，我们会在每一步都**帮助你**进行第一次贡献。

🌟 在GitHub上为TransformerX点星，展示你对项目的支持！你的贡献帮助我们不断改进和发展这个库。谢谢！🚀

# 喜欢看更多相关内容吗？

关注我的[**Twitter**](https://twitter.com/soranghadri)🐦，[**GitHub**](https://github.com/soran-ghaderi)🚀（我最活跃的平台），让我们在[**LinkedIn**](https://www.linkedin.com/in/soran-ghaderi/)💼上连接，当然，关注我的[Medium](http://soran-ghaderi.medium.com) 📝，并[订阅](https://soran-ghaderi.medium.com/subscribe)我的文章。

此外，这里是我的[**网站**](http://soran-ghaderi.github.io/)。

# 参考文献

有关参考文献的列表，请访问这个[gist](https://gist.github.com/soran-ghaderi/41abae1310007bd962c7b7bb5406556a)。这将为您提供一份全面的参考资料列表，以便进一步阅读和深入了解该主题。祝您探索愉快！
