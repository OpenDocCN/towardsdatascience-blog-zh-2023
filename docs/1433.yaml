- en: 'LLM Economics: ChatGPT vs Open-Source'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/llm-economics-chatgpt-vs-open-source-dfc29f69fec1?source=collection_archive---------2-----------------------#2023-04-26](https://towardsdatascience.com/llm-economics-chatgpt-vs-open-source-dfc29f69fec1?source=collection_archive---------2-----------------------#2023-04-26)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: How much does it cost to deploy LLMs like ChatGPT? Are open-source LLMs cheaper
    to deploy? What are the tradeoffs?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://skanda-vivek.medium.com/?source=post_page-----dfc29f69fec1--------------------------------)[![Skanda
    Vivek](../Images/9d25bee2fb75176ca7f7ea6eff7d7ab5.png)](https://skanda-vivek.medium.com/?source=post_page-----dfc29f69fec1--------------------------------)[](https://towardsdatascience.com/?source=post_page-----dfc29f69fec1--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----dfc29f69fec1--------------------------------)
    [Skanda Vivek](https://skanda-vivek.medium.com/?source=post_page-----dfc29f69fec1--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F220d9bbb8014&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fllm-economics-chatgpt-vs-open-source-dfc29f69fec1&user=Skanda+Vivek&userId=220d9bbb8014&source=post_page-220d9bbb8014----dfc29f69fec1---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----dfc29f69fec1--------------------------------)
    ·6 min read·Apr 26, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fdfc29f69fec1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fllm-economics-chatgpt-vs-open-source-dfc29f69fec1&user=Skanda+Vivek&userId=220d9bbb8014&source=-----dfc29f69fec1---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fdfc29f69fec1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fllm-economics-chatgpt-vs-open-source-dfc29f69fec1&source=-----dfc29f69fec1---------------------bookmark_footer-----------)![](../Images/fa975d94faf37f22c807696c80da0324.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Cartoon schematic for comparing LLM costs | Skanda Vivek
  prefs: []
  type: TYPE_NORMAL
- en: 'TLDR: For lower usage in the 1000’s of requests per day range ChatGPT works
    out cheaper than using open-sourced LLMs deployed to AWS. For millions of requests
    per day, open-sourced models deployed in AWS work out cheaper. (As of writing
    this article on April 24th, 2023.)'
  prefs: []
  type: TYPE_NORMAL
- en: Large Language Models are taking the world by storm. Transformers were introduced
    in 2017, followed by breakthrough models like BERT, GPT, and BART — 100’s of millions
    of parameters; and capable to performing multiple Language tasks like sentiment
    analysis, Q&A, classification, etc.
  prefs: []
  type: TYPE_NORMAL
- en: A couple of years ago — researchers from OpenAI and Google documented multiple
    papers showing that large language models with more than 10’s of Billions of parameters
    started showing emergent capabilities where they seemingly understand complex
    aspects of language and are almost human-like in their responses.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f39567219981d5f1f495ab8bd9f9174f.png)'
  prefs: []
  type: TYPE_IMG
- en: '[GPT-3 paper](https://arxiv.org/abs/2005.14165) showing the impressive learning
    capabilities of Large Language Models'
  prefs: []
  type: TYPE_NORMAL
- en: The GPT-3 paper showed that models > 10-100 Billion parameters show impressive
    learning…
  prefs: []
  type: TYPE_NORMAL
