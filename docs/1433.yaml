- en: 'LLM Economics: ChatGPT vs Open-Source'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LLM经济学：ChatGPT与开源模型的对比
- en: 原文：[https://towardsdatascience.com/llm-economics-chatgpt-vs-open-source-dfc29f69fec1?source=collection_archive---------2-----------------------#2023-04-26](https://towardsdatascience.com/llm-economics-chatgpt-vs-open-source-dfc29f69fec1?source=collection_archive---------2-----------------------#2023-04-26)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/llm-economics-chatgpt-vs-open-source-dfc29f69fec1?source=collection_archive---------2-----------------------#2023-04-26](https://towardsdatascience.com/llm-economics-chatgpt-vs-open-source-dfc29f69fec1?source=collection_archive---------2-----------------------#2023-04-26)
- en: How much does it cost to deploy LLMs like ChatGPT? Are open-source LLMs cheaper
    to deploy? What are the tradeoffs?
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 部署像ChatGPT这样的LLM需要多少成本？开源LLM的部署成本是否更低？有什么权衡？
- en: '[](https://skanda-vivek.medium.com/?source=post_page-----dfc29f69fec1--------------------------------)[![Skanda
    Vivek](../Images/9d25bee2fb75176ca7f7ea6eff7d7ab5.png)](https://skanda-vivek.medium.com/?source=post_page-----dfc29f69fec1--------------------------------)[](https://towardsdatascience.com/?source=post_page-----dfc29f69fec1--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----dfc29f69fec1--------------------------------)
    [Skanda Vivek](https://skanda-vivek.medium.com/?source=post_page-----dfc29f69fec1--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://skanda-vivek.medium.com/?source=post_page-----dfc29f69fec1--------------------------------)[![Skanda
    Vivek](../Images/9d25bee2fb75176ca7f7ea6eff7d7ab5.png)](https://skanda-vivek.medium.com/?source=post_page-----dfc29f69fec1--------------------------------)[](https://towardsdatascience.com/?source=post_page-----dfc29f69fec1--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----dfc29f69fec1--------------------------------)
    [Skanda Vivek](https://skanda-vivek.medium.com/?source=post_page-----dfc29f69fec1--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F220d9bbb8014&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fllm-economics-chatgpt-vs-open-source-dfc29f69fec1&user=Skanda+Vivek&userId=220d9bbb8014&source=post_page-220d9bbb8014----dfc29f69fec1---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----dfc29f69fec1--------------------------------)
    ·6 min read·Apr 26, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fdfc29f69fec1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fllm-economics-chatgpt-vs-open-source-dfc29f69fec1&user=Skanda+Vivek&userId=220d9bbb8014&source=-----dfc29f69fec1---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F220d9bbb8014&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fllm-economics-chatgpt-vs-open-source-dfc29f69fec1&user=Skanda+Vivek&userId=220d9bbb8014&source=post_page-220d9bbb8014----dfc29f69fec1---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----dfc29f69fec1--------------------------------)
    ·6分钟阅读·2023年4月26日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fdfc29f69fec1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fllm-economics-chatgpt-vs-open-source-dfc29f69fec1&user=Skanda+Vivek&userId=220d9bbb8014&source=-----dfc29f69fec1---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fdfc29f69fec1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fllm-economics-chatgpt-vs-open-source-dfc29f69fec1&source=-----dfc29f69fec1---------------------bookmark_footer-----------)![](../Images/fa975d94faf37f22c807696c80da0324.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fdfc29f69fec1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fllm-economics-chatgpt-vs-open-source-dfc29f69fec1&source=-----dfc29f69fec1---------------------bookmark_footer-----------)![](../Images/fa975d94faf37f22c807696c80da0324.png)'
- en: Cartoon schematic for comparing LLM costs | Skanda Vivek
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 对比LLM成本的卡通示意图 | Skanda Vivek
- en: 'TLDR: For lower usage in the 1000’s of requests per day range ChatGPT works
    out cheaper than using open-sourced LLMs deployed to AWS. For millions of requests
    per day, open-sourced models deployed in AWS work out cheaper. (As of writing
    this article on April 24th, 2023.)'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 总结：在每天使用量为1000次请求范围内，ChatGPT的成本低于在AWS上部署的开源LLM。对于每天请求数达百万的情况，AWS上的开源模型更便宜。（截至本文撰写日期2023年4月24日）
- en: Large Language Models are taking the world by storm. Transformers were introduced
    in 2017, followed by breakthrough models like BERT, GPT, and BART — 100’s of millions
    of parameters; and capable to performing multiple Language tasks like sentiment
    analysis, Q&A, classification, etc.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型正在席卷全球。变压器模型于2017年问世，随后出现了突破性的模型如BERT、GPT和BART——这些模型拥有数亿个参数，能够执行多种语言任务，如情感分析、问答、分类等。
- en: A couple of years ago — researchers from OpenAI and Google documented multiple
    papers showing that large language models with more than 10’s of Billions of parameters
    started showing emergent capabilities where they seemingly understand complex
    aspects of language and are almost human-like in their responses.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 几年前——来自 OpenAI 和 Google 的研究人员记录了多篇论文，表明拥有超过 10 亿参数的大型语言模型开始展现出涌现的能力，这些模型似乎能够理解语言的复杂方面，并且在回应上几乎像人类一样。
- en: '![](../Images/f39567219981d5f1f495ab8bd9f9174f.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f39567219981d5f1f495ab8bd9f9174f.png)'
- en: '[GPT-3 paper](https://arxiv.org/abs/2005.14165) showing the impressive learning
    capabilities of Large Language Models'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '[GPT-3 论文](https://arxiv.org/abs/2005.14165)展示了大语言模型令人印象深刻的学习能力。'
- en: The GPT-3 paper showed that models > 10-100 Billion parameters show impressive
    learning…
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-3 论文显示，参数大于 10-100 亿的模型展现了令人印象深刻的学习能力……
