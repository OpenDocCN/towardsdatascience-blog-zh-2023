- en: Prompt Engineering for Arithmetic Reasoning Problems
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/prompt-engineering-for-arithmetic-reasoning-problems-28c8bcd5bf0e?source=collection_archive---------1-----------------------#2023-11-18](https://towardsdatascience.com/prompt-engineering-for-arithmetic-reasoning-problems-28c8bcd5bf0e?source=collection_archive---------1-----------------------#2023-11-18)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Explore various prompt engineering techniques for arithmetic reasoning problems,
    best practices, and rapid experimentations for production-grade prompts with Vellum.ai
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@kaustubhbhavsar?source=post_page-----28c8bcd5bf0e--------------------------------)[![Kaustubh
    Bhavsar](../Images/82103ac231ab9e8a5581fd12d7d61c2a.png)](https://medium.com/@kaustubhbhavsar?source=post_page-----28c8bcd5bf0e--------------------------------)[](https://towardsdatascience.com/?source=post_page-----28c8bcd5bf0e--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----28c8bcd5bf0e--------------------------------)
    [Kaustubh Bhavsar](https://medium.com/@kaustubhbhavsar?source=post_page-----28c8bcd5bf0e--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F3c727e10b97f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fprompt-engineering-for-arithmetic-reasoning-problems-28c8bcd5bf0e&user=Kaustubh+Bhavsar&userId=3c727e10b97f&source=post_page-3c727e10b97f----28c8bcd5bf0e---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----28c8bcd5bf0e--------------------------------)
    ·14 min read·Nov 18, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F28c8bcd5bf0e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fprompt-engineering-for-arithmetic-reasoning-problems-28c8bcd5bf0e&user=Kaustubh+Bhavsar&userId=3c727e10b97f&source=-----28c8bcd5bf0e---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F28c8bcd5bf0e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fprompt-engineering-for-arithmetic-reasoning-problems-28c8bcd5bf0e&source=-----28c8bcd5bf0e---------------------bookmark_footer-----------)![](../Images/ad64d3dd5f3b645f4ecda8a7905df74b.png)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Architecture of four different prompting techniques: Input-Output, Chain-of-Thought
    (CoT), Self Consistency with Chain-of-Thought (CoT), and Tree of Thought (ToT)
    (Image from [Yao et el. (2023)](https://arxiv.org/abs/2305.10601))'
  prefs: []
  type: TYPE_NORMAL
- en: '**Introduction**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Large Language Models (LLMs) have been drawing increasing attention from both
    academic researchers and industry experts due to their proficiency in understanding
    and generating language. The reason behind their text comprehension skills lies
    in their training process, which involves exposure to vast amounts of data with
    the primary goal of predicting subsequent words. To optimize these models for
    specific tasks, fine-tuning is essential. This can be achieved through two methods:
    ‘pre-training and fine-tuning’ or ‘prompt fine-tuning’.'
  prefs: []
  type: TYPE_NORMAL
- en: In the conventional ‘pretraining and fine-tuning’ approach, the LLM is fine-tuned
    on a dataset pertinent to the tasks it will perform later, thereby updating parameters
    in the fine-tuning phase. Conversely, ‘prompt fine-tuning’ guides the model with
    a text segment to execute the task.
  prefs: []
  type: TYPE_NORMAL
- en: A prompt is an input provided by the user that the model is designed to respond
    to. Prompts can contain instruction, context, question or an output indicator.
    Prompt engineering is an emerging field dedicated to the development and refinement
    of prompts for the effective utilization of language models.
  prefs: []
  type: TYPE_NORMAL
- en: However, a significant challenge lies in ensuring the model can tackle problems
    necessitating arithmetic and commonsense reasoning. In this article, our focus
    is specifically on the engineering of prompts for arithmetic problems.
  prefs: []
  type: TYPE_NORMAL
- en: '**Prerequisites**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: No prior knowledge is required. All the demonstrations provided below can be
    executed either on the [OpenAI Playground](https://platform.openai.com/playground)
    or run through the [OpenAI API](https://openai.com/blog/openai-api). Although
    this article primarily utilizes OpenAI models, it’s important to note that these
    are merely prompting techniques that we’ll be exploring, and you’re free to apply
    any LLM available in the market.
  prefs: []
  type: TYPE_NORMAL
- en: '**Prompt Engineering for Arithmetic Problems**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: All the problems mentioned henceforth are taken from the [GSM8K](https://huggingface.co/datasets/gsm8k)
    dataset and are tested using OpenAI’s **GPT-3.5 Turbo Instruct** model with default
    configurations.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will be testing various techniques specifically on the following arithmetic
    problem:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The correct answer to the above problem is **100 litres**.
  prefs: []
  type: TYPE_NORMAL
- en: '**Zero-Shot Prompting**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A **shot** essentially refers to **an example**. Zero-shot is a basic prompting
    technique in which a question is posed to the model without providing any demonstrations.
    This technique, in general, produces favourable results in larger models that
    have been trained on extensive amounts of data.
  prefs: []
  type: TYPE_NORMAL
- en: '***Prompt:***'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '***Output:***'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding prompt, we didn’t prepend the question with any demonstrations,
    leading to the models' failure to provide the correct answer for the arithmetic
    problem. Reasoning and commonsense problems seldom yield satisfactory results
    with zero-shot prompts. Therefore, it’s essential to include examples when dealing
    with such problems.
  prefs: []
  type: TYPE_NORMAL
- en: Few-Shot Prompting
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With few-shot prompting, we provide the model with a small number of demonstrations,
    also known as exemplars. These exemplars serve to guide the model so that the
    subsequent questions can be similarly handled. The number of demonstrations needed
    varies based on the complexity of the problem and the specific model in use.
  prefs: []
  type: TYPE_NORMAL
- en: 'Prompt:'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Output:'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Although this technique works well in tackling difficult problems, the model
    may still fail to find satisfactory results in arithmetic reasoning problems.
    This is because the model fails to reason immediate steps.
  prefs: []
  type: TYPE_NORMAL
- en: '**Few-Shot Chain-of-Thought (CoT) Prompting**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[Few-shot chain-of-thought (CoT)](https://arxiv.org/abs/2201.11903) prompting
    equips the model with a few examples to guide it through the process of reaching
    a solution. This guidance, also known as intermediate reasoning steps, is crucial
    in assisting the model to work through the steps and generate the desired output
    logically.'
  prefs: []
  type: TYPE_NORMAL
- en: '***Prompt:***'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '***Output:***'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'In the above output, the model attempted to reason through the steps but arrived
    at the incorrect final answer. We observe that this discrepancy may be attributed
    to the irrelevant example provided. There are two ways to address this issue:
    either provide the model with more examples or ensure that the examples given
    are relevant. Below, we attempt to rectify this with a relevant example.'
  prefs: []
  type: TYPE_NORMAL
- en: '***Prompt:***'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '***Output:***'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: The final answer obtained is now correct. Consider the task, the choice of the
    model, and the relevance of examples, as all contribute to arriving at the correct
    solution.
  prefs: []
  type: TYPE_NORMAL
- en: Program-Aided Language Model (PAL)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A [program-aided language model (PAL)](https://arxiv.org/abs/2211.10435) is
    not a prompting technique; instead, it is an application of prompt engineering.
    Similar to CoT, PAL aims to reason out the intermediate steps. However, instead
    of using regular text as reasoning steps, PAL uses intermediate programmatic steps
    using programmatic runtime such as Python interpreter.
  prefs: []
  type: TYPE_NORMAL
- en: '***Prompt:***'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '***Output:***'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Even with the irrelevant example that failed to yield a correct answer using
    the few-shot CoT technique, PAL successfully found the correct final solution.
  prefs: []
  type: TYPE_NORMAL
- en: Zero-Shot Chain-of-Thought (CoT) Prompting
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In a situation where demonstrations for the few-shot chain-of-thought (CoT)
    technique are not available, a straightforward and effective approach is to ask
    the model itself to reason through the intermediate steps. This is called the
    zero-shot chain-of-thought (CoT). You can achieve this by adding a text similar
    to ‘Let’s think step by step…’ to the prompt question. This simple technique has
    shown exceptional results.
  prefs: []
  type: TYPE_NORMAL
- en: '***Prompt:***'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '***Output:***'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: From the above example, we observe that the model successfully reasons through
    the steps and arrives at the correct solution without requiring any examples or
    guidance.
  prefs: []
  type: TYPE_NORMAL
- en: Tree-of-Thought (ToT) Prompting
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[Tree-of-thought (ToT)](https://arxiv.org/abs/2305.08291) framework involves
    exploring potential solutions in a manner akin to navigating a tree structure
    of thoughts, similar to human problem-solving. This approach enables the possibility
    of retracing steps when needed, mirroring the way humans may reassess and adjust
    their thinking during the problem-solving process. In essence, ToT aims to replicate
    the adaptive and iterative nature of human reasoning through trial and error.'
  prefs: []
  type: TYPE_NORMAL
- en: Based on the ToT framework, [Dave Hulbert](https://github.com/dave1010/tree-of-thought-prompting)
    proposed the tree-of-thought prompting. This technique employs a straightforward
    single-prompt approach, asking the model to assess intermediate steps.
  prefs: []
  type: TYPE_NORMAL
- en: 'Prompt:'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Output:'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: The ToT technique employed in the above example yields an accurate solution.
    It’s worth mentioning that the ToT prompt used is taken from [Dave Hulbert’s ToT
    prompts](https://github.com/dave1010/tree-of-thought-prompting/blob/main/tree-of-thought-prompts.txt).
    However, one drawback of this technique is a significant increase in the prompt
    size.
  prefs: []
  type: TYPE_NORMAL
- en: Besides the engineering techniques mentioned above, a few additional methods
    can be adopted for solving arithmetic reasoning problems. However, it’s important
    to note that these techniques — [**Automatic Chain-of-Thought (Auto-CoT)**](https://arxiv.org/abs/2210.03493),
    [**Self-Consistency**](https://arxiv.org/abs/2203.11171), [**Automatic Reasoning
    and Tool-Use (ART)**](https://arxiv.org/abs/2303.09014), and [**Active Prompt**](https://arxiv.org/pdf/2302.12246.pdf)
    - are beyond the scope of this article.
  prefs: []
  type: TYPE_NORMAL
- en: Best Practices
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Models perform effectively when fine-tuned with well-designed prompts. [Elvis
    Saravia](https://www.promptingguide.ai/introduction/elements) noted that a prompt
    may consist of four elements, though not all are necessary each time:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Instruction:** This specifies a particular task or action you want the model
    to undertake.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Context:** External information or additional details that guide the model
    to generate more accurate responses.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Input Data:** The specific input or question for which we seek a response.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Output Indicator:** The expected type or format of the output.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It is advisable to place the instructions at the beginning, separated from the
    rest of the prompt using designated separators like ‘#.’
  prefs: []
  type: TYPE_NORMAL
- en: '[Santu and Feng (2023)](https://arxiv.org/pdf/2305.11430.pdf) introduced a
    comprehensive taxonomy, TELeR (Turn, Expression, Level of Details, Role), designed
    to enable more meaningful comparisons among multiple LLMs concerning their performance
    across various tasks. The prompt taxonomy categorizes prompt directives into six
    levels based on the level of detail provided. The sixth level can be effectively
    utilized to optimize the model’s performance based on the given prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: Provide a brief description of the high-level goal.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Present a detailed bulleted list of sub-tasks.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Specify the criteria for evaluating the Language Model (LLM) output or include
    a few shot examples.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Include additional pertinent information obtained through retrieval-based techniques.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Request an explicit statement from the LLM, explaining its output.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In general, it’s advisable to be specific, offer clear details, continuously
    refine the prompt through iteration, properly format the prompt with clear separations,
    and mention do’s and not don’ts.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Let’s revisit the prompt, considering the above-mentioned practices and applying
    the zero-shot prompting technique.
  prefs: []
  type: TYPE_NORMAL
- en: '**Prompt:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '**Output:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Initially, the model failed to provide the correct solution using the zero-shot
    prompting technique. However, with the application of appropriate instructions
    and formatting, the same model successfully generates the correct response using
    the zero-shot prompting technique.
  prefs: []
  type: TYPE_NORMAL
- en: '**Disclaimer:** The following section provides information about Vellum, a
    developer platform useful for the rapid productionization of prompts. The mention
    of Vellum is for informational purposes only and does not constitute an endorsement
    or promotion. I have no affiliation with Vellum and receive no compensation for
    its inclusion.'
  prefs: []
  type: TYPE_NORMAL
- en: Vellum
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Identifying the appropriate prompt can pose a challenge. The process begins
    with a basic prompt, which is then incrementally refined through multiple iterations
    — a task that can be tough to oversee. Spreadsheets might be adequate for a handful
    of experiments, but they falter when faced with intricate issues that demand a
    multitude of iterations. So, the question arises — how can we efficiently monitor
    our prompts? What if our goal is to seamlessly share our prompt experiments with
    our team? Introducing Vellum - answer to these questions.
  prefs: []
  type: TYPE_NORMAL
- en: '[Vellum](https://www.vellum.ai/) provides a suite of tools designed for prompt
    engineering, semantic search, version control, quantitative testing, and performance
    monitoring to help develop production-grade LLM applications. In the context of
    prompt engineering, it enables testing and evaluating a given prompt across all
    major LLMs available in the market. It also facilitates collaboration on prompts.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e6bcc602b5a9c2e18a7ecc48a18f35d1.png)'
  prefs: []
  type: TYPE_IMG
- en: Screenshot of Vellum sandbox (Screenshot by author)
  prefs: []
  type: TYPE_NORMAL
- en: 'The image above is a representative screenshot from the [Vellum sandbox](https://app.vellum.ai/prompt-sandboxes).
    Vellum enables easy switching between text and chat models and allows for effortless
    adjustment of model parameters. It also offers the advantage of latency tracking
    and quantitative assessment across four evaluation metrics: exact match, regex
    match, semantic similarity, and webhook.'
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://www.vellum.ai/?source=post_page-----28c8bcd5bf0e--------------------------------)
    [## Vellum - Developer platform for LLM applications'
  prefs: []
  type: TYPE_NORMAL
- en: Vellum is the development platform for building LLM apps with tools for prompt
    engineering, semantic search, version…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: www.vellum.ai](https://www.vellum.ai/?source=post_page-----28c8bcd5bf0e--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This article begins by introducing prompts and then delves into prompt engineering
    for arithmetic reasoning problems. We explored various prompt engineering techniques,
    including zero-shot, few-shot, few-shot chain-of-thought, zero-shot chain-of-thought,
    program-aided language model, and tree-of-thought.
  prefs: []
  type: TYPE_NORMAL
- en: Later, we learned some best practices for crafting better prompts. In general,
    it’s recommended to be specific, provide clear details, refine the prompt continuously
    through iteration, format it properly with clear separations, and mention dos
    and not don’ts.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, to address the challenge of tracking and evaluating prompts, as well
    as sharing them among team members, we investigated the Vellum tool.
  prefs: []
  type: TYPE_NORMAL
- en: If you like this article, make sure to follow me [here](https://medium.com/@kaustubhbhavsar).
    You can connect and reach out to me via [LinkedIn](http://www.linkedin.com/in/kaustubhbhavsar)
    and [X](https://twitter.com/bhavsarkaustubh) (formally, Twitter)*.*
  prefs: []
  type: TYPE_NORMAL
