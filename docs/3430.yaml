- en: Prompt Engineering for Arithmetic Reasoning Problems
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 算术推理问题的提示工程
- en: 原文：[https://towardsdatascience.com/prompt-engineering-for-arithmetic-reasoning-problems-28c8bcd5bf0e?source=collection_archive---------1-----------------------#2023-11-18](https://towardsdatascience.com/prompt-engineering-for-arithmetic-reasoning-problems-28c8bcd5bf0e?source=collection_archive---------1-----------------------#2023-11-18)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/prompt-engineering-for-arithmetic-reasoning-problems-28c8bcd5bf0e?source=collection_archive---------1-----------------------#2023-11-18](https://towardsdatascience.com/prompt-engineering-for-arithmetic-reasoning-problems-28c8bcd5bf0e?source=collection_archive---------1-----------------------#2023-11-18)
- en: Explore various prompt engineering techniques for arithmetic reasoning problems,
    best practices, and rapid experimentations for production-grade prompts with Vellum.ai
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 探索针对算术推理问题的各种提示工程技术、最佳实践以及通过 Vellum.ai 进行生产级提示的快速实验。
- en: '[](https://medium.com/@kaustubhbhavsar?source=post_page-----28c8bcd5bf0e--------------------------------)[![Kaustubh
    Bhavsar](../Images/82103ac231ab9e8a5581fd12d7d61c2a.png)](https://medium.com/@kaustubhbhavsar?source=post_page-----28c8bcd5bf0e--------------------------------)[](https://towardsdatascience.com/?source=post_page-----28c8bcd5bf0e--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----28c8bcd5bf0e--------------------------------)
    [Kaustubh Bhavsar](https://medium.com/@kaustubhbhavsar?source=post_page-----28c8bcd5bf0e--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@kaustubhbhavsar?source=post_page-----28c8bcd5bf0e--------------------------------)[![Kaustubh
    Bhavsar](../Images/82103ac231ab9e8a5581fd12d7d61c2a.png)](https://medium.com/@kaustubhbhavsar?source=post_page-----28c8bcd5bf0e--------------------------------)[](https://towardsdatascience.com/?source=post_page-----28c8bcd5bf0e--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----28c8bcd5bf0e--------------------------------)
    [Kaustubh Bhavsar](https://medium.com/@kaustubhbhavsar?source=post_page-----28c8bcd5bf0e--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F3c727e10b97f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fprompt-engineering-for-arithmetic-reasoning-problems-28c8bcd5bf0e&user=Kaustubh+Bhavsar&userId=3c727e10b97f&source=post_page-3c727e10b97f----28c8bcd5bf0e---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----28c8bcd5bf0e--------------------------------)
    ·14 min read·Nov 18, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F28c8bcd5bf0e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fprompt-engineering-for-arithmetic-reasoning-problems-28c8bcd5bf0e&user=Kaustubh+Bhavsar&userId=3c727e10b97f&source=-----28c8bcd5bf0e---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F3c727e10b97f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fprompt-engineering-for-arithmetic-reasoning-problems-28c8bcd5bf0e&user=Kaustubh+Bhavsar&userId=3c727e10b97f&source=post_page-3c727e10b97f----28c8bcd5bf0e---------------------post_header-----------)
    发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----28c8bcd5bf0e--------------------------------)
    · 14分钟阅读 · 2023年11月18日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F28c8bcd5bf0e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fprompt-engineering-for-arithmetic-reasoning-problems-28c8bcd5bf0e&user=Kaustubh+Bhavsar&userId=3c727e10b97f&source=-----28c8bcd5bf0e---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F28c8bcd5bf0e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fprompt-engineering-for-arithmetic-reasoning-problems-28c8bcd5bf0e&source=-----28c8bcd5bf0e---------------------bookmark_footer-----------)![](../Images/ad64d3dd5f3b645f4ecda8a7905df74b.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F28c8bcd5bf0e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fprompt-engineering-for-arithmetic-reasoning-problems-28c8bcd5bf0e&source=-----28c8bcd5bf0e---------------------bookmark_footer-----------)![](../Images/ad64d3dd5f3b645f4ecda8a7905df74b.png)'
- en: 'Architecture of four different prompting techniques: Input-Output, Chain-of-Thought
    (CoT), Self Consistency with Chain-of-Thought (CoT), and Tree of Thought (ToT)
    (Image from [Yao et el. (2023)](https://arxiv.org/abs/2305.10601))'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 四种不同提示技术的架构：输入-输出、链式思维（CoT）、链式思维下的自我一致性（CoT）和思维树（ToT）（图源：[Yao et al. (2023)](https://arxiv.org/abs/2305.10601)）
- en: '**Introduction**'
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**简介**'
- en: 'Large Language Models (LLMs) have been drawing increasing attention from both
    academic researchers and industry experts due to their proficiency in understanding
    and generating language. The reason behind their text comprehension skills lies
    in their training process, which involves exposure to vast amounts of data with
    the primary goal of predicting subsequent words. To optimize these models for
    specific tasks, fine-tuning is essential. This can be achieved through two methods:
    ‘pre-training and fine-tuning’ or ‘prompt fine-tuning’.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）由于其理解和生成语言的能力，越来越受到学术研究人员和行业专家的关注。这些文本理解能力的原因在于它们的训练过程，涉及大量数据，以预测后续单词为主要目标。为了使这些模型在特定任务上表现更佳，精细调整是必需的。这可以通过两种方法实现：‘预训练和精细调整’或‘提示精细调整’。
- en: In the conventional ‘pretraining and fine-tuning’ approach, the LLM is fine-tuned
    on a dataset pertinent to the tasks it will perform later, thereby updating parameters
    in the fine-tuning phase. Conversely, ‘prompt fine-tuning’ guides the model with
    a text segment to execute the task.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在传统的‘预训练和精细调整’方法中，LLM 在与其后续任务相关的数据集上进行精细调整，从而在精细调整阶段更新参数。相反，‘提示精细调整’则通过文本片段引导模型执行任务。
- en: A prompt is an input provided by the user that the model is designed to respond
    to. Prompts can contain instruction, context, question or an output indicator.
    Prompt engineering is an emerging field dedicated to the development and refinement
    of prompts for the effective utilization of language models.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 提示是用户提供的输入，模型旨在对其作出响应。提示可以包含指令、上下文、问题或输出指示符。提示工程是一个新兴领域，致力于开发和完善提示，以有效利用语言模型。
- en: However, a significant challenge lies in ensuring the model can tackle problems
    necessitating arithmetic and commonsense reasoning. In this article, our focus
    is specifically on the engineering of prompts for arithmetic problems.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，一个重大挑战在于确保模型能够处理需要算术和常识推理的问题。在本文中，我们专注于算术问题的提示工程。
- en: '**Prerequisites**'
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**先决条件**'
- en: No prior knowledge is required. All the demonstrations provided below can be
    executed either on the [OpenAI Playground](https://platform.openai.com/playground)
    or run through the [OpenAI API](https://openai.com/blog/openai-api). Although
    this article primarily utilizes OpenAI models, it’s important to note that these
    are merely prompting techniques that we’ll be exploring, and you’re free to apply
    any LLM available in the market.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 不需要任何先前知识。下面提供的所有示例可以在 [OpenAI Playground](https://platform.openai.com/playground)
    上执行，或通过 [OpenAI API](https://openai.com/blog/openai-api) 运行。虽然本文主要使用 OpenAI 模型，但需要注意的是，这些仅仅是我们将要探讨的提示技术，你可以自由使用市场上任何可用的
    LLM。
- en: '**Prompt Engineering for Arithmetic Problems**'
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**算术问题的提示工程**'
- en: All the problems mentioned henceforth are taken from the [GSM8K](https://huggingface.co/datasets/gsm8k)
    dataset and are tested using OpenAI’s **GPT-3.5 Turbo Instruct** model with default
    configurations.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 下面提到的所有问题均取自 [GSM8K](https://huggingface.co/datasets/gsm8k) 数据集，并使用 OpenAI 的
    **GPT-3.5 Turbo Instruct** 模型，配置为默认设置进行测试。
- en: 'We will be testing various techniques specifically on the following arithmetic
    problem:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将对以下算术问题测试各种技术：
- en: '[PRE0]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The correct answer to the above problem is **100 litres**.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 上述问题的正确答案是 **100 升**。
- en: '**Zero-Shot Prompting**'
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**零-shot 提示**'
- en: A **shot** essentially refers to **an example**. Zero-shot is a basic prompting
    technique in which a question is posed to the model without providing any demonstrations.
    This technique, in general, produces favourable results in larger models that
    have been trained on extensive amounts of data.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '**shot** 本质上指的是 **一个示例**。零-shot 是一种基本的提示技术，在这种技术中，问题被直接提问给模型，而不提供任何示例。一般来说，这种技术在经过大量数据训练的大型模型中会产生良好的效果。'
- en: '***Prompt:***'
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '***提示：***'
- en: '[PRE1]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '***Output:***'
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '***输出：***'
- en: '[PRE2]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: In the preceding prompt, we didn’t prepend the question with any demonstrations,
    leading to the models' failure to provide the correct answer for the arithmetic
    problem. Reasoning and commonsense problems seldom yield satisfactory results
    with zero-shot prompts. Therefore, it’s essential to include examples when dealing
    with such problems.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的提示中，我们没有在问题前加上任何示例，导致模型未能提供正确的算术问题答案。推理和常识问题在零-shot 提示下很少产生令人满意的结果。因此，处理此类问题时必须包含示例。
- en: Few-Shot Prompting
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Few-Shot 提示
- en: With few-shot prompting, we provide the model with a small number of demonstrations,
    also known as exemplars. These exemplars serve to guide the model so that the
    subsequent questions can be similarly handled. The number of demonstrations needed
    varies based on the complexity of the problem and the specific model in use.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 通过少量示例提示，我们向模型提供少量演示，也称为示例。这些示例用于指导模型，以便后续问题可以类似处理。所需的演示数量根据问题的复杂性和所使用的特定模型而异。
- en: 'Prompt:'
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 提示：
- en: '[PRE3]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Output:'
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 输出：
- en: '[PRE4]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Although this technique works well in tackling difficult problems, the model
    may still fail to find satisfactory results in arithmetic reasoning problems.
    This is because the model fails to reason immediate steps.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这种技术在解决困难问题时效果良好，但模型在算术推理问题上仍可能无法找到令人满意的结果。这是因为模型未能推理出即时步骤。
- en: '**Few-Shot Chain-of-Thought (CoT) Prompting**'
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**少量示例链式思维（CoT）提示**'
- en: '[Few-shot chain-of-thought (CoT)](https://arxiv.org/abs/2201.11903) prompting
    equips the model with a few examples to guide it through the process of reaching
    a solution. This guidance, also known as intermediate reasoning steps, is crucial
    in assisting the model to work through the steps and generate the desired output
    logically.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '[少量示例链式思维（CoT）](https://arxiv.org/abs/2201.11903)提示为模型提供了少量示例，以指导其完成解决方案的过程。这种指导，也称为中间推理步骤，对于帮助模型逐步工作并逻辑地产生所需输出至关重要。'
- en: '***Prompt:***'
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '***提示：***'
- en: '[PRE5]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '***Output:***'
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '***输出：***'
- en: '[PRE6]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'In the above output, the model attempted to reason through the steps but arrived
    at the incorrect final answer. We observe that this discrepancy may be attributed
    to the irrelevant example provided. There are two ways to address this issue:
    either provide the model with more examples or ensure that the examples given
    are relevant. Below, we attempt to rectify this with a relevant example.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述输出中，模型尝试推理步骤但得出了不正确的最终答案。我们观察到这种差异可能是由于提供了无关示例。解决此问题有两种方法：要么为模型提供更多示例，要么确保提供的示例是相关的。下面，我们尝试用相关示例来纠正这一点。
- en: '***Prompt:***'
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '***提示：***'
- en: '[PRE7]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '***Output:***'
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '***输出：***'
- en: '[PRE8]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The final answer obtained is now correct. Consider the task, the choice of the
    model, and the relevance of examples, as all contribute to arriving at the correct
    solution.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 最终得到的答案现在是正确的。考虑任务、模型的选择以及示例的相关性，这些都对得出正确解决方案有贡献。
- en: Program-Aided Language Model (PAL)
  id: totrans-47
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 程序辅助语言模型（PAL）
- en: A [program-aided language model (PAL)](https://arxiv.org/abs/2211.10435) is
    not a prompting technique; instead, it is an application of prompt engineering.
    Similar to CoT, PAL aims to reason out the intermediate steps. However, instead
    of using regular text as reasoning steps, PAL uses intermediate programmatic steps
    using programmatic runtime such as Python interpreter.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '[程序辅助语言模型（PAL）](https://arxiv.org/abs/2211.10435)不是一种提示技术；相反，它是提示工程的应用。与CoT类似，PAL旨在推理中间步骤。然而，PAL使用程序运行时的中间编程步骤，如Python解释器，而不是使用常规文本作为推理步骤。'
- en: '***Prompt:***'
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '***提示：***'
- en: '[PRE9]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '***Output:***'
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '***输出：***'
- en: '[PRE10]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Even with the irrelevant example that failed to yield a correct answer using
    the few-shot CoT technique, PAL successfully found the correct final solution.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 即使使用少量示例CoT技术的无关示例未能得出正确答案，PAL仍成功找到了正确的最终解决方案。
- en: Zero-Shot Chain-of-Thought (CoT) Prompting
  id: totrans-54
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 零-Shot链式思维（CoT）提示
- en: In a situation where demonstrations for the few-shot chain-of-thought (CoT)
    technique are not available, a straightforward and effective approach is to ask
    the model itself to reason through the intermediate steps. This is called the
    zero-shot chain-of-thought (CoT). You can achieve this by adding a text similar
    to ‘Let’s think step by step…’ to the prompt question. This simple technique has
    shown exceptional results.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在没有少量示例链式思维（CoT）技术演示的情况下，一个直接且有效的方法是让模型自行推理中间步骤。这被称为零-shot链式思维（CoT）。你可以通过在提示问题中添加类似于“我们一步一步来想…”的文本来实现这一点。这种简单的技术已经显示出卓越的效果。
- en: '***Prompt:***'
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '***提示：***'
- en: '[PRE11]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '***Output:***'
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '***输出：***'
- en: '[PRE12]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: From the above example, we observe that the model successfully reasons through
    the steps and arrives at the correct solution without requiring any examples or
    guidance.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 从上述示例中，我们观察到模型成功地推理了步骤，并得出了正确的解决方案，而无需任何示例或指导。
- en: Tree-of-Thought (ToT) Prompting
  id: totrans-61
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 思维树（ToT）提示
- en: '[Tree-of-thought (ToT)](https://arxiv.org/abs/2305.08291) framework involves
    exploring potential solutions in a manner akin to navigating a tree structure
    of thoughts, similar to human problem-solving. This approach enables the possibility
    of retracing steps when needed, mirroring the way humans may reassess and adjust
    their thinking during the problem-solving process. In essence, ToT aims to replicate
    the adaptive and iterative nature of human reasoning through trial and error.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '[思维树 (ToT)](https://arxiv.org/abs/2305.08291) 框架涉及以类似于导航思维树结构的方式探索潜在解决方案，这类似于人类的问题解决过程。这种方法使得在需要时能够重新追溯步骤，反映了人类在问题解决过程中可能重新评估和调整思维的方式。本质上，ToT
    旨在通过试错法复制人类推理的适应性和迭代性。'
- en: Based on the ToT framework, [Dave Hulbert](https://github.com/dave1010/tree-of-thought-prompting)
    proposed the tree-of-thought prompting. This technique employs a straightforward
    single-prompt approach, asking the model to assess intermediate steps.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 基于 ToT 框架，[Dave Hulbert](https://github.com/dave1010/tree-of-thought-prompting)
    提出了思维树提示法。这种技术采用了简单的单一提示方法，要求模型评估中间步骤。
- en: 'Prompt:'
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 提示：
- en: '[PRE13]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Output:'
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 输出：
- en: '[PRE14]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: The ToT technique employed in the above example yields an accurate solution.
    It’s worth mentioning that the ToT prompt used is taken from [Dave Hulbert’s ToT
    prompts](https://github.com/dave1010/tree-of-thought-prompting/blob/main/tree-of-thought-prompts.txt).
    However, one drawback of this technique is a significant increase in the prompt
    size.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 上述示例中使用的 ToT 技术能够产生准确的解决方案。值得一提的是，所使用的 ToT 提示来源于 [Dave Hulbert 的 ToT 提示](https://github.com/dave1010/tree-of-thought-prompting/blob/main/tree-of-thought-prompts.txt)。不过，这种技术的一个缺点是提示的尺寸显著增加。
- en: Besides the engineering techniques mentioned above, a few additional methods
    can be adopted for solving arithmetic reasoning problems. However, it’s important
    to note that these techniques — [**Automatic Chain-of-Thought (Auto-CoT)**](https://arxiv.org/abs/2210.03493),
    [**Self-Consistency**](https://arxiv.org/abs/2203.11171), [**Automatic Reasoning
    and Tool-Use (ART)**](https://arxiv.org/abs/2303.09014), and [**Active Prompt**](https://arxiv.org/pdf/2302.12246.pdf)
    - are beyond the scope of this article.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 除了上述工程技术之外，还可以采用一些额外的方法来解决算术推理问题。然而，值得注意的是，这些技术 — [**自动链式思维 (Auto-CoT)**](https://arxiv.org/abs/2210.03493)、[**自我一致性**](https://arxiv.org/abs/2203.11171)、[**自动推理和工具使用
    (ART)**](https://arxiv.org/abs/2303.09014) 和 [**主动提示**](https://arxiv.org/pdf/2302.12246.pdf)
    — 超出了本文的范围。
- en: Best Practices
  id: totrans-70
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 最佳实践
- en: 'Models perform effectively when fine-tuned with well-designed prompts. [Elvis
    Saravia](https://www.promptingguide.ai/introduction/elements) noted that a prompt
    may consist of four elements, though not all are necessary each time:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 模型在使用设计良好的提示进行微调时表现效果良好。[Elvis Saravia](https://www.promptingguide.ai/introduction/elements)
    指出，一个提示可能由四个元素组成，尽管每次并不都需要所有元素：
- en: '**Instruction:** This specifies a particular task or action you want the model
    to undertake.'
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**指令：** 这指定了你希望模型执行的特定任务或行动。'
- en: '**Context:** External information or additional details that guide the model
    to generate more accurate responses.'
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**背景：** 外部信息或额外细节，用于指导模型生成更准确的响应。'
- en: '**Input Data:** The specific input or question for which we seek a response.'
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**输入数据：** 我们寻求响应的特定输入或问题。'
- en: '**Output Indicator:** The expected type or format of the output.'
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**输出指标：** 期望的输出类型或格式。'
- en: It is advisable to place the instructions at the beginning, separated from the
    rest of the prompt using designated separators like ‘#.’
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 建议将指令放在开头，与其他提示内容用指定的分隔符（如‘#’）分开。
- en: '[Santu and Feng (2023)](https://arxiv.org/pdf/2305.11430.pdf) introduced a
    comprehensive taxonomy, TELeR (Turn, Expression, Level of Details, Role), designed
    to enable more meaningful comparisons among multiple LLMs concerning their performance
    across various tasks. The prompt taxonomy categorizes prompt directives into six
    levels based on the level of detail provided. The sixth level can be effectively
    utilized to optimize the model’s performance based on the given prompt:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '[Santu 和 Feng (2023)](https://arxiv.org/pdf/2305.11430.pdf) 介绍了一种全面的分类法 TELeR（Turn、Expression、Level
    of Details、Role），旨在使多个 LLM 在各种任务中的表现之间进行更有意义的比较。该提示分类法将提示指令根据提供的细节水平分为六个级别。第六级可以有效地用于优化模型在给定提示下的表现。'
- en: Provide a brief description of the high-level goal.
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 提供高层目标的简要描述。
- en: Present a detailed bulleted list of sub-tasks.
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 提供详细的子任务的项目列表。
- en: Specify the criteria for evaluating the Language Model (LLM) output or include
    a few shot examples.
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 指定评估语言模型（LLM）输出的标准或包含几个示例。
- en: Include additional pertinent information obtained through retrieval-based techniques.
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 包括通过检索技术获得的额外相关信息。
- en: Request an explicit statement from the LLM, explaining its output.
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 请求 LLM 提供明确的声明，解释其输出结果。
- en: In general, it’s advisable to be specific, offer clear details, continuously
    refine the prompt through iteration, properly format the prompt with clear separations,
    and mention do’s and not don’ts.
  id: totrans-83
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 通常建议具体明确，提供清晰的细节，通过迭代不断完善提示，适当格式化提示并进行清晰分隔，并提及应做和不应做的事项。
- en: Let’s revisit the prompt, considering the above-mentioned practices and applying
    the zero-shot prompting technique.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们重新审视这个提示，考虑上述提到的做法，并应用零-shot 提示技术。
- en: '**Prompt:**'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '**提示：**'
- en: '[PRE15]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '**Output:**'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '**输出：**'
- en: '[PRE16]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Initially, the model failed to provide the correct solution using the zero-shot
    prompting technique. However, with the application of appropriate instructions
    and formatting, the same model successfully generates the correct response using
    the zero-shot prompting technique.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 最初，模型未能使用零-shot 提示技术提供正确的解决方案。然而，通过应用适当的指令和格式化，相同的模型成功地使用零-shot 提示技术生成了正确的响应。
- en: '**Disclaimer:** The following section provides information about Vellum, a
    developer platform useful for the rapid productionization of prompts. The mention
    of Vellum is for informational purposes only and does not constitute an endorsement
    or promotion. I have no affiliation with Vellum and receive no compensation for
    its inclusion.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '**免责声明：** 以下部分提供关于 Vellum 的信息，这是一个对快速生产化提示非常有用的开发平台。提及 Vellum 仅供参考，并不构成推荐或推广。我与
    Vellum 没有任何关联，也没有因其包含而获得任何报酬。'
- en: Vellum
  id: totrans-91
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Vellum
- en: Identifying the appropriate prompt can pose a challenge. The process begins
    with a basic prompt, which is then incrementally refined through multiple iterations
    — a task that can be tough to oversee. Spreadsheets might be adequate for a handful
    of experiments, but they falter when faced with intricate issues that demand a
    multitude of iterations. So, the question arises — how can we efficiently monitor
    our prompts? What if our goal is to seamlessly share our prompt experiments with
    our team? Introducing Vellum - answer to these questions.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 确定合适的提示可能是一项挑战。该过程从基本提示开始，然后通过多次迭代逐步完善——这是一项难以监督的任务。电子表格可能适用于少量实验，但面对需要大量迭代的复杂问题时，它们就显得不足。因此，问题是——我们如何有效地监控我们的提示？如果我们的目标是与团队无缝分享我们的提示实验呢？引入
    Vellum —— 这些问题的答案。
- en: '[Vellum](https://www.vellum.ai/) provides a suite of tools designed for prompt
    engineering, semantic search, version control, quantitative testing, and performance
    monitoring to help develop production-grade LLM applications. In the context of
    prompt engineering, it enables testing and evaluating a given prompt across all
    major LLMs available in the market. It also facilitates collaboration on prompts.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '[Vellum](https://www.vellum.ai/) 提供了一套工具，旨在支持提示工程、语义搜索、版本控制、定量测试和性能监控，以帮助开发生产级
    LLM 应用。在提示工程的背景下，它支持在市场上所有主要 LLM 中测试和评估给定的提示。它还促进了对提示的协作。'
- en: '![](../Images/e6bcc602b5a9c2e18a7ecc48a18f35d1.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e6bcc602b5a9c2e18a7ecc48a18f35d1.png)'
- en: Screenshot of Vellum sandbox (Screenshot by author)
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: Vellum 沙箱的截图（作者截图）
- en: 'The image above is a representative screenshot from the [Vellum sandbox](https://app.vellum.ai/prompt-sandboxes).
    Vellum enables easy switching between text and chat models and allows for effortless
    adjustment of model parameters. It also offers the advantage of latency tracking
    and quantitative assessment across four evaluation metrics: exact match, regex
    match, semantic similarity, and webhook.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的图像是 [Vellum 沙箱](https://app.vellum.ai/prompt-sandboxes) 的代表性截图。Vellum 允许在文本和聊天模型之间轻松切换，并可以轻松调整模型参数。它还提供延迟跟踪和四种评估指标的定量评估的优势：精确匹配、正则匹配、语义相似性和
    Webhook。
- en: '[](https://www.vellum.ai/?source=post_page-----28c8bcd5bf0e--------------------------------)
    [## Vellum - Developer platform for LLM applications'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://www.vellum.ai/?source=post_page-----28c8bcd5bf0e--------------------------------)
    [## Vellum - 适用于 LLM 应用的开发平台'
- en: Vellum is the development platform for building LLM apps with tools for prompt
    engineering, semantic search, version…
  id: totrans-98
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Vellum 是一个用于构建 LLM 应用的开发平台，提供提示工程、语义搜索、版本…
- en: www.vellum.ai](https://www.vellum.ai/?source=post_page-----28c8bcd5bf0e--------------------------------)
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-100
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This article begins by introducing prompts and then delves into prompt engineering
    for arithmetic reasoning problems. We explored various prompt engineering techniques,
    including zero-shot, few-shot, few-shot chain-of-thought, zero-shot chain-of-thought,
    program-aided language model, and tree-of-thought.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: Later, we learned some best practices for crafting better prompts. In general,
    it’s recommended to be specific, provide clear details, refine the prompt continuously
    through iteration, format it properly with clear separations, and mention dos
    and not don’ts.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: Finally, to address the challenge of tracking and evaluating prompts, as well
    as sharing them among team members, we investigated the Vellum tool.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: If you like this article, make sure to follow me [here](https://medium.com/@kaustubhbhavsar).
    You can connect and reach out to me via [LinkedIn](http://www.linkedin.com/in/kaustubhbhavsar)
    and [X](https://twitter.com/bhavsarkaustubh) (formally, Twitter)*.*
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
