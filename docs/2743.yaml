- en: 'Large Language Models: BERT â€” Bidirectional Encoder Representations from Transformer'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å¤§å‹è¯­è¨€æ¨¡å‹ï¼šBERT â€” Transformer çš„åŒå‘ç¼–ç å™¨è¡¨ç¤º
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/bert-3d1bf880386a?source=collection_archive---------0-----------------------#2023-08-30](https://towardsdatascience.com/bert-3d1bf880386a?source=collection_archive---------0-----------------------#2023-08-30)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://towardsdatascience.com/bert-3d1bf880386a?source=collection_archive---------0-----------------------#2023-08-30](https://towardsdatascience.com/bert-3d1bf880386a?source=collection_archive---------0-----------------------#2023-08-30)
- en: Understand how BERT constructs state-of-the-art embeddings
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç†è§£ BERT å¦‚ä½•æ„å»ºæœ€å…ˆè¿›çš„åµŒå…¥
- en: '[](https://medium.com/@slavahead?source=post_page-----3d1bf880386a--------------------------------)[![Vyacheslav
    Efimov](../Images/db4b02e75d257063e8e9d3f1f75d9d6d.png)](https://medium.com/@slavahead?source=post_page-----3d1bf880386a--------------------------------)[](https://towardsdatascience.com/?source=post_page-----3d1bf880386a--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----3d1bf880386a--------------------------------)
    [Vyacheslav Efimov](https://medium.com/@slavahead?source=post_page-----3d1bf880386a--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@slavahead?source=post_page-----3d1bf880386a--------------------------------)[![Vyacheslav
    Efimov](../Images/db4b02e75d257063e8e9d3f1f75d9d6d.png)](https://medium.com/@slavahead?source=post_page-----3d1bf880386a--------------------------------)[](https://towardsdatascience.com/?source=post_page-----3d1bf880386a--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----3d1bf880386a--------------------------------)
    [Vyacheslav Efimov](https://medium.com/@slavahead?source=post_page-----3d1bf880386a--------------------------------)'
- en: Â·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Â·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fc8a0ca9d85d8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbert-3d1bf880386a&user=Vyacheslav+Efimov&userId=c8a0ca9d85d8&source=post_page-c8a0ca9d85d8----3d1bf880386a---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----3d1bf880386a--------------------------------)
    Â·11 min readÂ·Aug 30, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F3d1bf880386a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbert-3d1bf880386a&user=Vyacheslav+Efimov&userId=c8a0ca9d85d8&source=-----3d1bf880386a---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[å…³æ³¨](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fc8a0ca9d85d8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbert-3d1bf880386a&user=Vyacheslav+Efimov&userId=c8a0ca9d85d8&source=post_page-c8a0ca9d85d8----3d1bf880386a---------------------post_header-----------)
    å‘è¡¨åœ¨ [Towards Data Science](https://towardsdatascience.com/?source=post_page-----3d1bf880386a--------------------------------)
    Â· 11 åˆ†é’Ÿé˜…è¯» Â· 2023 å¹´ 8 æœˆ 30 æ—¥ [](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F3d1bf880386a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbert-3d1bf880386a&user=Vyacheslav+Efimov&userId=c8a0ca9d85d8&source=-----3d1bf880386a---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F3d1bf880386a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbert-3d1bf880386a&source=-----3d1bf880386a---------------------bookmark_footer-----------)![](../Images/450d6ac933335232d6ae951d5b7a4e0b.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F3d1bf880386a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbert-3d1bf880386a&source=-----3d1bf880386a---------------------bookmark_footer-----------)![](../Images/450d6ac933335232d6ae951d5b7a4e0b.png)'
- en: Introduction
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ä»‹ç»
- en: 2017 was a historical year in machine learning when the **Transformer** model
    made its first appearance on the scene. It has been performing amazingly on many
    benchmarks and has become suitable for lots of problems in Data Science. Thanks
    to its efficient architecture, many other Transformer-based models have been developed
    later which specialise more on particular tasks.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 2017 å¹´æ˜¯æœºå™¨å­¦ä¹ çš„å†å²æ€§ä¸€å¹´ï¼Œå½“æ—¶**Transformer**æ¨¡å‹é¦–æ¬¡äº®ç›¸ã€‚å®ƒåœ¨è®¸å¤šåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œé€‚ç”¨äºæ•°æ®ç§‘å­¦ä¸­çš„è®¸å¤šé—®é¢˜ã€‚ç”±äºå…¶é«˜æ•ˆçš„æ¶æ„ï¼Œåæ¥å¼€å‘äº†è®¸å¤šå…¶ä»–åŸºäº
    Transformer çš„æ¨¡å‹ï¼Œè¿™äº›æ¨¡å‹åœ¨ç‰¹å®šä»»åŠ¡ä¸Šæœ‰äº†æ›´å¤šçš„ä¸“ä¸šåŒ–ã€‚
- en: One of such models is BERT. It is primarily known for being able to construct
    embeddings which can very accurately represent text information and store semantic
    meanings of long text sequences. As a result, BERT embeddings became widely used
    in machine learning. Understanding how BERT builds text representations is crucial
    because it opens the door for tackling a large range of tasks in NLP.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­ä¸€ä¸ªè¿™æ ·çš„æ¨¡å‹æ˜¯ BERTã€‚å®ƒä¸»è¦ä»¥èƒ½å¤Ÿæ„å»ºéå¸¸å‡†ç¡®çš„æ–‡æœ¬åµŒå…¥è€Œè‘—ç§°ï¼Œè¿™äº›åµŒå…¥å¯ä»¥è¡¨ç¤ºæ–‡æœ¬ä¿¡æ¯å¹¶å­˜å‚¨é•¿æ–‡æœ¬åºåˆ—çš„è¯­ä¹‰å«ä¹‰ã€‚å› æ­¤ï¼ŒBERT åµŒå…¥åœ¨æœºå™¨å­¦ä¹ ä¸­å¾—åˆ°äº†å¹¿æ³›åº”ç”¨ã€‚ç†è§£
    BERT å¦‚ä½•æ„å»ºæ–‡æœ¬è¡¨ç¤ºæ˜¯è‡³å…³é‡è¦çš„ï¼Œå› ä¸ºè¿™ä¸ºå¤„ç†è‡ªç„¶è¯­è¨€å¤„ç†ä¸­çš„å¤§é‡ä»»åŠ¡æ‰“å¼€äº†å¤§é—¨ã€‚
- en: In this article, we will refer to the [original BERT paper](https://arxiv.org/pdf/1810.04805.pdf)
    and have a look at BERT architecture and understand the core mechanisms behind
    it. In the first sections, we will give a high-level overview of BERT. After that,
    we will gradually dive into its internal workflow and how information is passed
    throughout the model. Finally, we will learn how BERT can be fine-tuned for solving
    particular problems in NLP.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å°†å‚è€ƒ [åŸå§‹ BERT è®ºæ–‡](https://arxiv.org/pdf/1810.04805.pdf)ï¼ŒæŸ¥çœ‹ BERT æ¶æ„å¹¶ç†è§£å…¶æ ¸å¿ƒæœºåˆ¶ã€‚åœ¨å‰å‡ éƒ¨åˆ†ä¸­ï¼Œæˆ‘ä»¬å°†ç»™å‡º
    BERT çš„é«˜çº§æ¦‚è¿°ã€‚ä¹‹åï¼Œæˆ‘ä»¬å°†é€æ­¥æ·±å…¥å…¶å†…éƒ¨å·¥ä½œæµç¨‹åŠä¿¡æ¯åœ¨æ¨¡å‹ä¸­çš„ä¼ é€’æ–¹å¼ã€‚æœ€åï¼Œæˆ‘ä»¬å°†äº†è§£å¦‚ä½•å¯¹ BERT è¿›è¡Œå¾®è°ƒï¼Œä»¥è§£å†³ NLP ä¸­çš„ç‰¹å®šé—®é¢˜ã€‚
- en: High level overview
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: é«˜çº§æ¦‚è¿°
- en: '**Transformer**â€™s architecture consists of two primary parts: encoders and
    decoders. The goal of stacked encoders is to construct a meaningful embedding
    for an input which would preserve its main context. The output of the last encoder
    is passed to inputs of all decoders trying to generate new information.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '**Transformer** çš„æ¶æ„ç”±ä¸¤ä¸ªä¸»è¦éƒ¨åˆ†ç»„æˆï¼šç¼–ç å™¨å’Œè§£ç å™¨ã€‚å †å ç¼–ç å™¨çš„ç›®æ ‡æ˜¯ä¸ºè¾“å…¥æ„å»ºæœ‰æ„ä¹‰çš„åµŒå…¥ï¼Œä»¥ä¿æŒå…¶ä¸»è¦ä¸Šä¸‹æ–‡ã€‚æœ€åä¸€ä¸ªç¼–ç å™¨çš„è¾“å‡ºä¼ é€’ç»™æ‰€æœ‰è§£ç å™¨çš„è¾“å…¥ï¼Œè¯•å›¾ç”Ÿæˆæ–°çš„ä¿¡æ¯ã€‚'
- en: '**BERT** is a Transformer successor which inherits its stacked bidirectional
    encoders. Most of the architectural principles in BERT are the same as in the
    original Transformer.'
  id: totrans-14
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**BERT** æ˜¯ Transformer çš„ç»§æ‰¿è€…ï¼Œç»§æ‰¿äº†å…¶å †å çš„åŒå‘ç¼–ç å™¨ã€‚BERT ä¸­çš„å¤§éƒ¨åˆ†æ¶æ„åŸåˆ™ä¸åŸå§‹ Transformer ç›¸åŒã€‚'
- en: '![](../Images/1de7776be934f0ebf7ede8d9a5864184.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1de7776be934f0ebf7ede8d9a5864184.png)'
- en: Transformer architecture
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer æ¶æ„
- en: BERT versions
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: BERT ç‰ˆæœ¬
- en: 'There exist two main versions of BERT: base and large. Their architecture is
    absolutely identical except for the fact that they use different numbers of parameters.
    Overall, BERT large has 3.09 times more parameters to tune, compared to BERT base.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: BERT å­˜åœ¨ä¸¤ä¸ªä¸»è¦ç‰ˆæœ¬ï¼šbase å’Œ largeã€‚å®ƒä»¬çš„æ¶æ„å®Œå…¨ç›¸åŒï¼Œåªæ˜¯å‚æ•°æ•°é‡ä¸åŒã€‚æ€»ä½“è€Œè¨€ï¼ŒBERT large æ¯” BERT base å¤šäº†
    3.09 å€çš„å‚æ•°è¿›è¡Œè°ƒä¼˜ã€‚
- en: '![](../Images/a1037ce1121042a532a1aff8403e9307.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a1037ce1121042a532a1aff8403e9307.png)'
- en: Comparison of BERT base and BERT large
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: BERT base å’Œ BERT large çš„æ¯”è¾ƒ
- en: Bidirectional representations
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: åŒå‘è¡¨ç¤º
- en: From the letter â€œBâ€ in the BERTâ€™s name, it is important to remember that BERT
    is a **bidirectional** model meaning that it can better capture word connections
    due to the fact that the information is passed in both directions (left-to-right
    and right-to-left). Obviously, this results in more training resources, compared
    to unidirectional models, but at the same time leads to a better prediction accuracy.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: ä» BERT åç§°ä¸­çš„å­—æ¯â€œBâ€æ¥çœ‹ï¼Œé‡è¦çš„æ˜¯è¦è®°ä½ BERT æ˜¯ä¸€ä¸ª **åŒå‘** æ¨¡å‹ï¼Œè¿™æ„å‘³ç€å®ƒå¯ä»¥æ›´å¥½åœ°æ•æ‰è¯æ±‡ä¹‹é—´çš„è¿æ¥ï¼Œå› ä¸ºä¿¡æ¯æ˜¯åŒå‘ä¼ é€’çš„ï¼ˆä»å·¦åˆ°å³å’Œä»å³åˆ°å·¦ï¼‰ã€‚æ˜¾ç„¶ï¼Œè¿™ä¸å•å‘æ¨¡å‹ç›¸æ¯”éœ€è¦æ›´å¤šçš„è®­ç»ƒèµ„æºï¼Œä½†åŒæ—¶ä¹Ÿå¯¼è‡´æ›´å¥½çš„é¢„æµ‹å‡†ç¡®æ€§ã€‚
- en: For a better understanding, we can visualise BERT architecture in comparison
    with other popular NLP models.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†æ›´å¥½åœ°ç†è§£ï¼Œæˆ‘ä»¬å¯ä»¥å°† BERT æ¶æ„ä¸å…¶ä»–æµè¡Œçš„ NLP æ¨¡å‹è¿›è¡Œæ¯”è¾ƒã€‚
- en: '![](../Images/51e3b04c18a481f9f7a95887afc9a581.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/51e3b04c18a481f9f7a95887afc9a581.png)'
- en: Comparison of BERT, OpenAI GPT and ElMo architectures from the [ogirinal paper](https://arxiv.org/pdf/1810.04805.pdf).
    Adopted by the author.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: ä» [åŸå§‹è®ºæ–‡](https://arxiv.org/pdf/1810.04805.pdf) ä¸­æ¯”è¾ƒ BERTã€OpenAI GPT å’Œ ElMo æ¶æ„ã€‚ç”±ä½œè€…é‡‡ç”¨ã€‚
- en: Input tokenisation
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: è¾“å…¥æ ‡è®°åŒ–
- en: Note. In the official paper, authors use the term â€œ**sentence**â€ to indicate
    text that is passed to the input. To designate the same term, throughout this
    article series we will be using the term â€œ**sequence**â€. It is done to avoid confusion
    as â€œsentenceâ€ usually means a single phrase separated by a point and due to the
    fact that in many other NLP research papers the term â€œsequenceâ€ is utilised in
    similar circumstances.
  id: totrans-27
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: æ³¨ï¼šåœ¨å®˜æ–¹è®ºæ–‡ä¸­ï¼Œä½œè€…ä½¿ç”¨â€œ**sentence**â€ä¸€è¯æ¥è¡¨ç¤ºä¼ é€’ç»™è¾“å…¥çš„æ–‡æœ¬ã€‚ä¸ºäº†ç»Ÿä¸€æœ¯è¯­ï¼Œæœ¬æ–‡ç³»åˆ—ä¸­æˆ‘ä»¬å°†ä½¿ç”¨â€œ**sequence**â€ä¸€è¯ã€‚è¿™æ˜¯ä¸ºäº†é¿å…æ··æ·†ï¼Œå› ä¸ºâ€œsentenceâ€
    é€šå¸¸æŒ‡ä¸€ä¸ªç”±å¥ç‚¹åˆ†éš”çš„å•ç‹¬çŸ­è¯­ï¼Œè€Œè®¸å¤šå…¶ä»– NLP ç ”ç©¶è®ºæ–‡ä¸­â€œsequenceâ€ä¸€è¯åœ¨ç±»ä¼¼æƒ…å†µä¸‹è¢«ä½¿ç”¨ã€‚
- en: 'Before diving into how BERT is trained, it is necessary to understand in what
    format it accepts data. For the input, BERT takes a single sequence or a pair
    of sequences. Each sequence is split into tokens. Additionally, two special tokens
    are passed to the input:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ·±å…¥äº†è§£ BERT çš„è®­ç»ƒæ–¹æ³•ä¹‹å‰ï¼Œæœ‰å¿…è¦äº†è§£ BERT æ¥å—æ•°æ®çš„æ ¼å¼ã€‚å¯¹äºè¾“å…¥ï¼ŒBERT æ¥å—å•ä¸ªåºåˆ—æˆ–ä¸€å¯¹åºåˆ—ã€‚æ¯ä¸ªåºåˆ—è¢«æ‹†åˆ†ä¸ºæ ‡è®°ã€‚æ­¤å¤–ï¼Œä¸¤ä¸ªç‰¹æ®Šæ ‡è®°ä¼šè¢«ä¼ é€’åˆ°è¾“å…¥ä¸­ï¼š
- en: Note. The official paper uses the term â€œ**sentence**â€ which designates an input
    **sequence** passed to BERT which can actually consist of several sentences. For
    simplicity, we are going to follow the notation and use the same term throughout
    this article.
  id: totrans-29
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: æ³¨æ„ã€‚å®˜æ–¹è®ºæ–‡ä½¿ç”¨äº†â€œ**å¥å­**â€è¿™ä¸ªæœ¯è¯­ï¼Œå®ƒæŒ‡çš„æ˜¯ä¼ é€’ç»™ BERT çš„è¾“å…¥**åºåˆ—**ï¼Œè¯¥åºåˆ—å®é™…ä¸Šå¯ä»¥ç”±å¤šä¸ªå¥å­ç»„æˆã€‚ä¸ºäº†ç®€åŒ–ï¼Œæˆ‘ä»¬å°†éµå¾ªè¿™ä¸ªç¬¦å·ï¼Œå¹¶åœ¨æœ¬æ–‡ä¸­ä½¿ç”¨ç›¸åŒçš„æœ¯è¯­ã€‚
- en: '***[CLS]*** â€” passed before the first sequence indicating its beginning. At
    the same time, *[CLS]* is also used for a classification objective during training
    (discussed in the sections below).'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***[CLS]*** â€” åœ¨ç¬¬ä¸€ä¸ªåºåˆ—ä¹‹å‰ä¼ é€’ï¼Œè¡¨ç¤ºå…¶å¼€å§‹ã€‚åŒæ—¶ï¼Œ* [CLS] * ä¹Ÿç”¨äºè®­ç»ƒä¸­çš„åˆ†ç±»ç›®æ ‡ï¼ˆåœ¨ä¸‹é¢çš„ç« èŠ‚ä¸­è®¨è®ºï¼‰ã€‚'
- en: '***[SEP]*** â€” passed between sequences to indicate the end of the first sequence
    and the beginning of the second.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***[SEP]*** â€” åœ¨åºåˆ—ä¹‹é—´ä¼ é€’ï¼Œç”¨ä»¥è¡¨ç¤ºç¬¬ä¸€ä¸ªåºåˆ—çš„ç»“æŸå’Œç¬¬äºŒä¸ªåºåˆ—çš„å¼€å§‹ã€‚'
- en: Passing two sequences makes it possible for BERT to handle a large variety of
    tasks where an input contains a pair of sequences (e.g. question and answer, hypothesis
    and premise, etc.).
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: ä¼ é€’ä¸¤ä¸ªåºåˆ—ä½¿å¾— BERT èƒ½å¤Ÿå¤„ç†å„ç§ä»»åŠ¡ï¼Œå…¶ä¸­è¾“å…¥åŒ…å«ä¸€å¯¹åºåˆ—ï¼ˆä¾‹å¦‚ï¼Œé—®é¢˜å’Œç­”æ¡ˆï¼Œå‡è®¾å’Œå‰æç­‰ï¼‰ã€‚
- en: Input embedding
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: è¾“å…¥åµŒå…¥
- en: 'After tokenisation, an embedding is built for each token. To make input embeddings
    more representative, BERT constructs three types of embeddings for each token:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨åˆ†è¯ä¹‹åï¼Œä¸ºæ¯ä¸ªæ ‡è®°æ„å»ºä¸€ä¸ªåµŒå…¥ã€‚ä¸ºäº†ä½¿è¾“å…¥åµŒå…¥æ›´å…·ä»£è¡¨æ€§ï¼ŒBERT ä¸ºæ¯ä¸ªæ ‡è®°æ„å»ºäº†ä¸‰ç§ç±»å‹çš„åµŒå…¥ï¼š
- en: '**Token embeddings** capture the semantic meaning of tokens.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**æ ‡è®°åµŒå…¥** æ•æ‰æ ‡è®°çš„è¯­ä¹‰æ„ä¹‰ã€‚'
- en: '**Segment embeddings** have one of two possible values and indicate to which
    sequence a token belongs.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**æ®µåµŒå…¥**æœ‰ä¸¤ä¸ªå¯èƒ½çš„å€¼ï¼Œè¡¨ç¤ºæ ‡è®°å±äºå“ªä¸ªåºåˆ—ã€‚'
- en: '**Position embeddings** contain information about a relative position of a
    token in a sequence.'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ä½ç½®åµŒå…¥** åŒ…å«å…³äºæ ‡è®°åœ¨åºåˆ—ä¸­ç›¸å¯¹ä½ç½®çš„ä¿¡æ¯ã€‚'
- en: '![](../Images/436c75c0a4550006be88a01948fc4e1c.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/436c75c0a4550006be88a01948fc4e1c.png)'
- en: Input processing
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: è¾“å…¥å¤„ç†
- en: These embeddings are summed up and the result is passed to the first encoder
    of the BERT model.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›åµŒå…¥è¢«åŠ æ€»ï¼Œç„¶åç»“æœè¢«ä¼ é€’ç»™ BERT æ¨¡å‹çš„ç¬¬ä¸€ä¸ªç¼–ç å™¨ã€‚
- en: Output
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: è¾“å‡º
- en: Each encoder takes *n* embeddings as input and then outputs the same number
    of processed embeddings of the same dimensionality. Ultimately, the whole BERT
    output also contains *n* embeddings each of which corresponds to its initial token.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: æ¯ä¸ªç¼–ç å™¨æ¥å—*n*ä¸ªåµŒå…¥ä½œä¸ºè¾“å…¥ï¼Œç„¶åè¾“å‡ºç›¸åŒæ•°é‡ã€ç›¸åŒç»´åº¦çš„å¤„ç†åçš„åµŒå…¥ã€‚æœ€ç»ˆï¼Œæ•´ä¸ª BERT è¾“å‡ºä¹ŸåŒ…å«*n*ä¸ªåµŒå…¥ï¼Œæ¯ä¸ªåµŒå…¥å¯¹åº”å…¶åˆå§‹çš„æ ‡è®°ã€‚
- en: '![](../Images/349ec9a449ceb70406ce5910691d12fa.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/349ec9a449ceb70406ce5910691d12fa.png)'
- en: Training
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: è®­ç»ƒ
- en: 'BERT training consists of two stages:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: BERT è®­ç»ƒåˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µï¼š
- en: '**Pre-training**. BERT is trained on unlabeled pair of sequences over two prediction
    tasks: **masked language modeling (MLM)** and **natural language inference (NLI)**.
    For each pair of sequences, the model makes predictions for these two tasks and
    based on the loss values, it performs backpropagation to update weights.'
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**é¢„è®­ç»ƒ**ã€‚BERT åœ¨æœªæ ‡è®°çš„åºåˆ—å¯¹ä¸Šè¿›è¡Œè®­ç»ƒï¼Œæ¶‰åŠä¸¤ä¸ªé¢„æµ‹ä»»åŠ¡ï¼š**æ©ç è¯­è¨€å»ºæ¨¡ï¼ˆMLMï¼‰** å’Œ **è‡ªç„¶è¯­è¨€æ¨ç†ï¼ˆNLIï¼‰**ã€‚å¯¹äºæ¯å¯¹åºåˆ—ï¼Œæ¨¡å‹ä¼šå¯¹è¿™ä¸¤ä¸ªä»»åŠ¡è¿›è¡Œé¢„æµ‹ï¼Œå¹¶æ ¹æ®æŸå¤±å€¼è¿›è¡Œåå‘ä¼ æ’­æ¥æ›´æ–°æƒé‡ã€‚'
- en: '**Fine-tuning**. BERT is initialised with pre-trained weights which are then
    optimised for a particular problem on labeled data.'
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**å¾®è°ƒ**ã€‚BERT ä½¿ç”¨é¢„è®­ç»ƒçš„æƒé‡è¿›è¡Œåˆå§‹åŒ–ï¼Œç„¶ååœ¨æ ‡è®°æ•°æ®ä¸Šä¸ºç‰¹å®šé—®é¢˜è¿›è¡Œä¼˜åŒ–ã€‚'
- en: Pre-training
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: é¢„è®­ç»ƒ
- en: Compared to fine-tuning, pre-training usually takes a significant proportion
    of time because the model is trained on a large corpus of data. That is why there
    exist a lot of online repositories of pre-trained models which can be then fine-tined
    relatively fast to solve a particular task.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸å¾®è°ƒç›¸æ¯”ï¼Œé¢„è®­ç»ƒé€šå¸¸éœ€è¦è¾ƒé•¿çš„æ—¶é—´ï¼Œå› ä¸ºæ¨¡å‹æ˜¯åœ¨å¤§é‡æ•°æ®ä¸Šè®­ç»ƒçš„ã€‚å› æ­¤ï¼Œå­˜åœ¨è®¸å¤šåœ¨çº¿é¢„è®­ç»ƒæ¨¡å‹çš„åº“ï¼Œè¿™äº›æ¨¡å‹å¯ä»¥è¢«ç›¸å¯¹å¿«é€Ÿåœ°å¾®è°ƒä»¥è§£å†³ç‰¹å®šä»»åŠ¡ã€‚
- en: We are going to look in detail at both problems solved by BERT during pre-training.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†è¯¦ç»†æŸ¥çœ‹ BERT åœ¨é¢„è®­ç»ƒæœŸé—´è§£å†³çš„ä¸¤ä¸ªé—®é¢˜ã€‚
- en: Masked Language Modeling
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ©ç è¯­è¨€å»ºæ¨¡
- en: 'Authors propose training BERT by masking a certain amount of tokens in the
    initial text and predicting them. *This gives BERT the ability to construct resilient
    embeddings that can use the surrounding context to guess a certain word which
    also leads to building an appropriate embedding for the missed word as well*.
    This process works in the following way:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: ä½œè€…å»ºè®®é€šè¿‡æ©ç›–åˆå§‹æ–‡æœ¬ä¸­çš„ä¸€å®šæ•°é‡çš„æ ‡è®°æ¥è®­ç»ƒ BERT å¹¶é¢„æµ‹å®ƒä»¬ã€‚*è¿™ä½¿å¾— BERT èƒ½å¤Ÿæ„å»ºå‡ºå…·æœ‰å¼¹æ€§çš„åµŒå…¥ï¼Œå¯ä»¥åˆ©ç”¨å‘¨å›´çš„ä¸Šä¸‹æ–‡æ¥çŒœæµ‹æŸä¸ªè¯ï¼Œä»è€Œä¸ºé—æ¼çš„è¯æ„å»ºåˆé€‚çš„åµŒå…¥*ã€‚è¿™ä¸ªè¿‡ç¨‹çš„å·¥ä½œæ–¹å¼å¦‚ä¸‹ï¼š
- en: After tokenization, 15% of tokens are randomly chosen to be masked. The chosen
    tokens will be then predicted at the end of the iteration.
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: åœ¨åˆ†è¯åï¼Œ15% çš„æ ‡è®°è¢«éšæœºé€‰æ‹©è¿›è¡Œæ©ç›–ã€‚é€‰æ‹©çš„æ ‡è®°å°†åœ¨è¿­ä»£ç»“æŸæ—¶è¿›è¡Œé¢„æµ‹ã€‚
- en: 'The chosen tokens are replaced in one of three ways:'
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: é€‰æ‹©çš„æ ‡è®°è¢«ä»¥ä¸‰ç§æ–¹å¼ä¹‹ä¸€æ›¿æ¢ï¼š
- en: '*-* 80% of the tokens are replaced by the *[MASK]* token.'
  id: totrans-55
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*-* 80% çš„æ ‡è®°è¢«æ›¿æ¢ä¸º *[MASK]* æ ‡è®°ã€‚'
- en: 'Example*: I bought a book â†’ I bought a [MASK]*'
  id: totrans-56
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'ç¤ºä¾‹*: æˆ‘ä¹°äº†ä¸€æœ¬ä¹¦ â†’ æˆ‘ä¹°äº†ä¸€ä¸ª[MASK]*'
- en: '- 10% of the tokens are replaced by a random token.'
  id: totrans-57
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '- 10% çš„æ ‡è®°è¢«éšæœºæ ‡è®°æ›¿ä»£ã€‚'
- en: 'Example: *He is eating a fruit â†’ He is drawing a fruit*'
  id: totrans-58
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'ç¤ºä¾‹: *ä»–åœ¨åƒæ°´æœ â†’ ä»–åœ¨ç”»æ°´æœ*'
- en: '- 10% of the tokens remain unchanged.'
  id: totrans-59
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '- 10% çš„æ ‡è®°ä¿æŒä¸å˜ã€‚'
- en: 'Example: *A house is near me â†’ A house is near me*'
  id: totrans-60
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'ç¤ºä¾‹: *ä¸€æ ‹æˆ¿å­åœ¨æˆ‘é™„è¿‘ â†’ ä¸€æ ‹æˆ¿å­åœ¨æˆ‘é™„è¿‘*'
- en: All tokens are passed to the BERT model which outputs an embedding for each
    token it received as input.
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æ‰€æœ‰æ ‡è®°è¢«ä¼ é€’ç»™ BERT æ¨¡å‹ï¼Œæ¨¡å‹è¾“å‡ºæ¯ä¸ªæ¥æ”¶åˆ°çš„è¾“å…¥æ ‡è®°çš„åµŒå…¥ã€‚
- en: 4\. Output embeddings corresponding to the tokens processed at step 2 are independently
    used to predict the masked tokens. The result of each prediction is a probability
    distribution across all the tokens in the vocabulary.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 4\. å¯¹åº”äºæ­¥éª¤ 2 ä¸­å¤„ç†çš„æ ‡è®°çš„è¾“å‡ºåµŒå…¥è¢«ç‹¬ç«‹ç”¨äºé¢„æµ‹è¢«æ©ç›–çš„æ ‡è®°ã€‚æ¯ä¸ªé¢„æµ‹çš„ç»“æœæ˜¯è¯æ±‡è¡¨ä¸­æ‰€æœ‰æ ‡è®°çš„æ¦‚ç‡åˆ†å¸ƒã€‚
- en: 5\. The cross-entropy loss is calculated by comparing probability distributions
    with the true masked tokens.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 5\. äº¤å‰ç†µæŸå¤±é€šè¿‡å°†æ¦‚ç‡åˆ†å¸ƒä¸çœŸå®æ©ç›–æ ‡è®°è¿›è¡Œæ¯”è¾ƒæ¥è®¡ç®—ã€‚
- en: 6\. The model weights are updated by using backpropagation.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 6\. æ¨¡å‹æƒé‡é€šè¿‡åå‘ä¼ æ’­è¿›è¡Œæ›´æ–°ã€‚
- en: Natural Language Inference
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è‡ªç„¶è¯­è¨€æ¨æ–­
- en: For this classification task, BERT tries to predict whether the second sequence
    follows the first. The whole prediction is made by using only the embedding from
    the final hidden state of the *[CLS]* token which is supposed to contain aggregated
    information from both sequences.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºè¿™ä¸ªåˆ†ç±»ä»»åŠ¡ï¼ŒBERT å°è¯•é¢„æµ‹ç¬¬äºŒä¸ªåºåˆ—æ˜¯å¦è·Ÿéšç¬¬ä¸€ä¸ªåºåˆ—ã€‚æ•´ä¸ªé¢„æµ‹ä»…ä½¿ç”¨æ¥è‡ª *[CLS]* æ ‡è®°çš„æœ€ç»ˆéšè—çŠ¶æ€çš„åµŒå…¥ï¼Œè¯¥æ ‡è®°åº”åŒ…å«æ¥è‡ªä¸¤ä¸ªåºåˆ—çš„èšåˆä¿¡æ¯ã€‚
- en: Similarly to MLM, a constructed probability distribution (binary in this case)
    is used to calculate the modelâ€™s loss and update the weights of the model through
    backpropagation.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: ç±»ä¼¼äº MLMï¼Œä½¿ç”¨æ„å»ºçš„æ¦‚ç‡åˆ†å¸ƒï¼ˆäºŒè¿›åˆ¶çš„æƒ…å†µä¸‹ï¼‰æ¥è®¡ç®—æ¨¡å‹çš„æŸå¤±ï¼Œå¹¶é€šè¿‡åå‘ä¼ æ’­æ›´æ–°æ¨¡å‹çš„æƒé‡ã€‚
- en: For NLI, authors recommend choosing 50% of pairs of sequences which follow each
    other in the corpus (*positive pairs*) and 50% of pairs where sequences are taken
    randomly from the corpus (*negative pairs*).
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºè‡ªç„¶è¯­è¨€æ¨æ–­ï¼ˆNLIï¼‰ï¼Œä½œè€…å»ºè®®é€‰æ‹© 50% çš„åºåˆ—å¯¹ï¼Œè¿™äº›åºåˆ—åœ¨è¯­æ–™åº“ä¸­æ˜¯ç´§æ¥ç€çš„ï¼ˆ*æ­£å¯¹*ï¼‰ï¼Œä»¥åŠ 50% çš„åºåˆ—å¯¹ï¼Œå…¶ä¸­åºåˆ—æ˜¯ä»è¯­æ–™åº“ä¸­éšæœºé€‰å–çš„ï¼ˆ*è´Ÿå¯¹*ï¼‰ã€‚
- en: '![](../Images/49b97c621037edf7eeb72626ce174ad9.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/49b97c621037edf7eeb72626ce174ad9.png)'
- en: BERT pre-training
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: BERT é¢„è®­ç»ƒ
- en: Training details
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è®­ç»ƒç»†èŠ‚
- en: According to the paper, BERT is pre-trained on BooksCorpus (800M words) and
    English Wikipedia (2,500M words). For extracting longer continuous texts, authors
    took from Wikipedia only reading passages ignoring tables, headers and lists.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: æ ¹æ®è®ºæ–‡ï¼ŒBERT åœ¨ BooksCorpusï¼ˆ8 äº¿å•è¯ï¼‰å’Œè‹±æ–‡ç»´åŸºç™¾ç§‘ï¼ˆ25 äº¿å•è¯ï¼‰ä¸Šè¿›è¡Œé¢„è®­ç»ƒã€‚ä¸ºäº†æå–è¾ƒé•¿çš„è¿ç»­æ–‡æœ¬ï¼Œä½œè€…ä»…ä»ç»´åŸºç™¾ç§‘ä¸­æå–é˜…è¯»æ®µè½ï¼Œå¿½ç•¥è¡¨æ ¼ã€æ ‡é¢˜å’Œåˆ—è¡¨ã€‚
- en: BERT is trained on a million batches of size equal to 256 sequences which is
    equivalent to 40 epochs on 3.3 billion words. Each sequence contains up to 128
    (90% of the time) or 512 (10% of the time) tokens.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: BERT åœ¨å¤§å°ä¸º 256 çš„ä¸€ç™¾ä¸‡æ‰¹æ¬¡ä¸Šè¿›è¡Œè®­ç»ƒï¼Œè¿™ç›¸å½“äºåœ¨ 33 äº¿ä¸ªå•è¯ä¸Šè¿›è¡Œ 40 ä¸ªå‘¨æœŸã€‚æ¯ä¸ªåºåˆ—åŒ…å«æœ€å¤š 128ï¼ˆ90% çš„æ—¶é—´ï¼‰æˆ– 512ï¼ˆ10%
    çš„æ—¶é—´ï¼‰ä¸ªæ ‡è®°ã€‚
- en: 'According to the original paper, the training parameters are the following:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: æ ¹æ®åŸå§‹è®ºæ–‡ï¼Œè®­ç»ƒå‚æ•°å¦‚ä¸‹ï¼š
- en: 'Optimisator: Adam (learning rate *l* = 1e-4, weight decay Lâ‚‚ = 0.01, Î²â‚ = 0.9,
    Î²â‚‚ = 0.999, Îµ = 1e-6).'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¼˜åŒ–å™¨ï¼šAdamï¼ˆå­¦ä¹ ç‡ *l* = 1e-4ï¼Œæƒé‡è¡°å‡ Lâ‚‚ = 0.01ï¼ŒÎ²â‚ = 0.9ï¼ŒÎ²â‚‚ = 0.999ï¼ŒÎµ = 1e-6ï¼‰ã€‚
- en: Learning rate warmup is performed over the first 10 000 steps and then reduced
    linearly.
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å­¦ä¹ ç‡é¢„çƒ­åœ¨å‰ 10,000 æ­¥å†…è¿›è¡Œï¼Œç„¶åçº¿æ€§é™ä½ã€‚
- en: Dropout (Î± = 0.1) layer is used on all layers.
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åœ¨æ‰€æœ‰å±‚ä¸Šä½¿ç”¨ Dropoutï¼ˆÎ± = 0.1ï¼‰å±‚ã€‚
- en: 'Activation function: GELU.'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ¿€æ´»å‡½æ•°ï¼šGELUã€‚
- en: Training loss is the sum of mean MLM and mean next sentence prediction likelihoods.
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è®­ç»ƒæŸå¤±æ˜¯å¹³å‡ MLM å’Œå¹³å‡ä¸‹ä¸€ä¸ªå¥å­é¢„æµ‹ä¼¼ç„¶çš„æ€»å’Œã€‚
- en: Fine-tuning
  id: totrans-80
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å¾®è°ƒ
- en: Once pre-training is completed, BERT can literally understand the semantic meanings
    of words and construct embeddings which can almost fully represent their meanings.
    The goal of fine-tuning is to gradually modify BERT weights for solving a particular
    downstream task.
  id: totrans-81
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ä¸€æ—¦é¢„è®­ç»ƒå®Œæˆï¼ŒBERT å¯ä»¥å­—é¢ä¸Šç†è§£å•è¯çš„è¯­ä¹‰ï¼Œå¹¶æ„å»ºå‡ ä¹å®Œå…¨è¡¨ç¤ºå…¶æ„ä¹‰çš„åµŒå…¥ã€‚å¾®è°ƒçš„ç›®æ ‡æ˜¯é€æ¸è°ƒæ•´ BERT çš„æƒé‡ï¼Œä»¥è§£å†³ç‰¹å®šçš„ä¸‹æ¸¸ä»»åŠ¡ã€‚
- en: Data format
  id: totrans-82
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ•°æ®æ ¼å¼
- en: Thanks to the robustness of the self-attention mechanism, BERT can be easily
    fine-tuned for a particular downstream task. Another advantage of BERT is the
    ability to build *bidirectional* text representations. This gives a higher chance
    of discovering correct relations between two sequences when working with pairs.
    Previous approaches consisted of independently encoding both sequences and then
    applying bidirectional cross-attention to them. BERT unifies these two stages.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±äºè‡ªæ³¨æ„åŠ›æœºåˆ¶çš„é²æ£’æ€§ï¼ŒBERTå¯ä»¥è½»æ¾åœ°ä¸ºç‰¹å®šä¸‹æ¸¸ä»»åŠ¡è¿›è¡Œå¾®è°ƒã€‚BERTçš„å¦ä¸€ä¸ªä¼˜åŠ¿æ˜¯èƒ½å¤Ÿæ„å»º*åŒå‘*æ–‡æœ¬è¡¨ç¤ºã€‚è¿™åœ¨å¤„ç†å¯¹æ—¶æä¾›äº†æ›´é«˜çš„å‘ç°ä¸¤ä¸ªåºåˆ—ä¹‹é—´æ­£ç¡®å…³ç³»çš„æœºä¼šã€‚ä»¥å‰çš„æ–¹æ³•åŒ…æ‹¬ç‹¬ç«‹ç¼–ç ä¸¤ä¸ªåºåˆ—ï¼Œç„¶åå¯¹å®ƒä»¬åº”ç”¨åŒå‘äº¤å‰æ³¨æ„åŠ›ã€‚BERTç»Ÿä¸€äº†è¿™ä¸¤ä¸ªé˜¶æ®µã€‚
- en: 'Depending on a certain problem, BERT accepts several input formats. The framework
    for solving all downstream tasks with BERT is the same: by taking as an input
    a sequence of text, BERT outputs a set of token embeddings which are then fed
    to the model. Most of the time, not all of the output embeddings are used.'
  id: totrans-84
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: æ ¹æ®å…·ä½“é—®é¢˜ï¼ŒBERTæ¥å—å‡ ç§è¾“å…¥æ ¼å¼ã€‚ç”¨BERTè§£å†³æ‰€æœ‰ä¸‹æ¸¸ä»»åŠ¡çš„æ¡†æ¶æ˜¯ç›¸åŒçš„ï¼šè¾“å…¥ä¸€ä¸ªæ–‡æœ¬åºåˆ—ï¼ŒBERTè¾“å‡ºä¸€ç»„æ ‡è®°åµŒå…¥ï¼Œç„¶åå°†è¿™äº›åµŒå…¥é€å…¥æ¨¡å‹ã€‚å¤§å¤šæ•°æ—¶å€™ï¼Œå¹¶ä¸æ˜¯æ‰€æœ‰çš„è¾“å‡ºåµŒå…¥éƒ½ä¼šè¢«ä½¿ç”¨ã€‚
- en: Let us have a look at common problems and the ways they are solved by fine-tuning
    BERT.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬çœ‹çœ‹å¸¸è§çš„é—®é¢˜ä»¥åŠé€šè¿‡å¾®è°ƒBERTè§£å†³è¿™äº›é—®é¢˜çš„æ–¹æ³•ã€‚
- en: '**Sentence pair classification**'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '**å¥å­å¯¹åˆ†ç±»**'
- en: 'The goal of sentence pair classification is to understand the relationship
    between a given pair of sequences. Most of common types of tasks are:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: å¥å­å¯¹åˆ†ç±»çš„ç›®æ ‡æ˜¯ç†è§£ç»™å®šåºåˆ—å¯¹ä¹‹é—´çš„å…³ç³»ã€‚å¸¸è§çš„ä»»åŠ¡ç±»å‹åŒ…æ‹¬ï¼š
- en: '*Natural language inference*: determining whether the second sequence follows
    the first.'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*è‡ªç„¶è¯­è¨€æ¨ç†*ï¼šç¡®å®šç¬¬äºŒä¸ªåºåˆ—æ˜¯å¦è·Ÿéšç¬¬ä¸€ä¸ªåºåˆ—ã€‚'
- en: '*Similarity analysis*: finding a degree of similarity between sequences.'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*ç›¸ä¼¼æ€§åˆ†æ*ï¼šæ‰¾åˆ°åºåˆ—ä¹‹é—´çš„ç›¸ä¼¼ç¨‹åº¦ã€‚'
- en: '![](../Images/1a1aeb6fad79249756ebcb7b571f565a.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1a1aeb6fad79249756ebcb7b571f565a.png)'
- en: Sentence pair classification
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: å¥å­å¯¹åˆ†ç±»
- en: For fine-tuning, both sequences are passed to BERT. As a rule of thumb, the
    output embedding of the *[CLS]* token is then used for the classification task.
    According to the researchers, the *[CLS]* token is supposed to contain the main
    information about sentence relationships.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºå¾®è°ƒï¼Œä¸¤ä¸ªåºåˆ—éƒ½ä¼ é€’ç»™BERTã€‚ä¸€èˆ¬æ¥è¯´ï¼Œ* [CLS] *æ ‡è®°çš„è¾“å‡ºåµŒå…¥è¢«ç”¨æ¥è¿›è¡Œåˆ†ç±»ä»»åŠ¡ã€‚æ ¹æ®ç ”ç©¶äººå‘˜çš„è¯´æ³•ï¼Œ* [CLS] *æ ‡è®°åº”è¯¥åŒ…å«å…³äºå¥å­å…³ç³»çš„ä¸»è¦ä¿¡æ¯ã€‚
- en: Of course, other output embeddings can also be used but they are usually omitted
    in practice.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: å½“ç„¶ï¼Œä¹Ÿå¯ä»¥ä½¿ç”¨å…¶ä»–è¾“å‡ºåµŒå…¥ï¼Œä½†åœ¨å®é™…åº”ç”¨ä¸­é€šå¸¸ä¼šè¢«çœç•¥ã€‚
- en: '**Question answering task**'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '**é—®ç­”ä»»åŠ¡**'
- en: 'The objective of *question answering* is to find an answer in a text paragraph
    corresponding to a particular question. Most of the time, the answer is given
    in the form of two numbers: the start and end token positions of the passage.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '*é—®ç­”*çš„ç›®æ ‡æ˜¯åœ¨æ–‡æœ¬æ®µè½ä¸­æ‰¾åˆ°å¯¹åº”äºç‰¹å®šé—®é¢˜çš„ç­”æ¡ˆã€‚å¤§å¤šæ•°æ—¶å€™ï¼Œç­”æ¡ˆä»¥ä¸¤ä¸ªæ•°å­—çš„å½¢å¼ç»™å‡ºï¼šç‰‡æ®µçš„å¼€å§‹å’Œç»“æŸæ ‡è®°ä½ç½®ã€‚'
- en: '![](../Images/f78ff21a106291ce5a36445a3a251b6f.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f78ff21a106291ce5a36445a3a251b6f.png)'
- en: Question answering task
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: é—®ç­”ä»»åŠ¡
- en: For the input, BERT takes the question and the paragraph and outputs a set of
    embeddings for them. Since the answer is contained within the paragraph, we are
    only interested in output embeddings corresponding to paragraph tokens.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºè¾“å…¥ï¼ŒBERTæ¥æ”¶é—®é¢˜å’Œæ®µè½ï¼Œå¹¶è¾“å‡ºä¸€ç»„å¯¹åº”çš„åµŒå…¥ã€‚ç”±äºç­”æ¡ˆåŒ…å«åœ¨æ®µè½ä¸­ï¼Œæˆ‘ä»¬åªå¯¹ä¸æ®µè½æ ‡è®°å¯¹åº”çš„è¾“å‡ºåµŒå…¥æ„Ÿå…´è¶£ã€‚
- en: For finding a position of the start answer token in the paragraph, the scalar
    product between every output embedding and a special trainable vector Tâ‚›â‚œâ‚áµ£â‚œ is
    calculated. For most cases when the model and the vector Tâ‚›â‚œâ‚áµ£â‚œ are trained accordingly,
    the scalar product should be proportional to the likelihood that a corresponding
    token is in reality the start answer token. To normalise scalar products, they
    are then passed to the softmax function and can be thought as probabilities. The
    token embedding corresponding to the highest probability is predicted as the start
    answer token. Based on the true probability distribution, the loss value is calculated
    and the backpropagation is performed. The analogous process is performed with
    the vector Tâ‚‘â‚™ğ’¹ for predicting the end token.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†æ‰¾åˆ°æ®µè½ä¸­ç­”æ¡ˆèµ·å§‹æ ‡è®°çš„ä½ç½®ï¼Œè®¡ç®—æ¯ä¸ªè¾“å‡ºåµŒå…¥ä¸ä¸€ä¸ªç‰¹æ®Šçš„å¯è®­ç»ƒå‘é‡Tâ‚›â‚œâ‚áµ£â‚“çš„æ ‡é‡ç§¯ã€‚åœ¨å¤§å¤šæ•°æƒ…å†µä¸‹ï¼Œå½“æ¨¡å‹å’Œå‘é‡Tâ‚›â‚œâ‚áµ£â‚“ç»è¿‡ç›¸åº”è®­ç»ƒæ—¶ï¼Œæ ‡é‡ç§¯åº”è¯¥ä¸ç›¸åº”æ ‡è®°å®é™…ä¸Šæ˜¯èµ·å§‹ç­”æ¡ˆæ ‡è®°çš„å¯èƒ½æ€§æˆæ­£æ¯”ã€‚ä¸ºäº†è§„èŒƒåŒ–æ ‡é‡ç§¯ï¼Œå®ƒä»¬ä¼šä¼ é€’åˆ°softmaxå‡½æ•°ï¼Œå¹¶å¯ä»¥çœ‹ä½œæ˜¯æ¦‚ç‡ã€‚å¯¹åº”äºæœ€é«˜æ¦‚ç‡çš„æ ‡è®°åµŒå…¥è¢«é¢„æµ‹ä¸ºèµ·å§‹ç­”æ¡ˆæ ‡è®°ã€‚æ ¹æ®çœŸå®çš„æ¦‚ç‡åˆ†å¸ƒï¼Œè®¡ç®—æŸå¤±å€¼å¹¶è¿›è¡Œåå‘ä¼ æ’­ã€‚é¢„æµ‹ç»“æŸæ ‡è®°æ—¶ä¼šä½¿ç”¨å‘é‡Tâ‚‘â‚™ğ’¹è¿›è¡Œç±»ä¼¼çš„è¿‡ç¨‹ã€‚
- en: '**Single sentence classification**'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '**å•å¥åˆ†ç±»**'
- en: 'The difference, compared to previous downstream tasks, is that here only a
    single sentence is passed BERT. Typical problems solved by this configuration
    are the following:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ä¹‹å‰çš„ä¸‹æ¸¸ä»»åŠ¡ç›¸æ¯”ï¼ŒåŒºåˆ«åœ¨äºè¿™é‡Œåªä¼ é€’å•ä¸ªå¥å­ç»™BERTã€‚æ­¤é…ç½®è§£å†³çš„å…¸å‹é—®é¢˜å¦‚ä¸‹ï¼š
- en: '*Sentiment analysis*: understanding whether a sentence has a positive or negative
    attitude.'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*æƒ…æ„Ÿåˆ†æ*ï¼šç†è§£ä¸€ä¸ªå¥å­æ˜¯å¦å…·æœ‰ç§¯ææˆ–æ¶ˆæçš„æ€åº¦ã€‚'
- en: '*Topic classification*: classifying a sentence into one of several categories
    based on its contents.'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*ä¸»é¢˜åˆ†ç±»*ï¼šæ ¹æ®å¥å­çš„å†…å®¹å°†å¥å­åˆ†ç±»åˆ°å‡ ä¸ªç±»åˆ«ä¹‹ä¸€ã€‚'
- en: '![](../Images/114d1219106d34b8c716f7d7e22f50f9.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/114d1219106d34b8c716f7d7e22f50f9.png)'
- en: Single sentence classification
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: å•å¥åˆ†ç±»
- en: 'The prediction workflow is the same as for sentence pair classification: the
    output embedding for the *[CLS]* token is used as the input for the classification
    model.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: é¢„æµ‹å·¥ä½œæµç¨‹ä¸å¥å­å¯¹åˆ†ç±»çš„å·¥ä½œæµç¨‹ç›¸åŒï¼š*`[CLS]`*æ ‡è®°çš„è¾“å‡ºåµŒå…¥è¢«ç”¨ä½œåˆ†ç±»æ¨¡å‹çš„è¾“å…¥ã€‚
- en: '**Single sentence tagging**'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '**å•å¥æ ‡æ³¨**'
- en: '*Named entity recognition (NER)* is a machine learning problem which aims to
    map every token of a sequence to one of respective entities.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '*å‘½åå®ä½“è¯†åˆ«ï¼ˆNERï¼‰*æ˜¯ä¸€ä¸ªæœºå™¨å­¦ä¹ é—®é¢˜ï¼Œæ—¨åœ¨å°†åºåˆ—ä¸­çš„æ¯ä¸ªæ ‡è®°æ˜ å°„åˆ°ç›¸åº”çš„å®ä½“ä¹‹ä¸€ã€‚'
- en: '![](../Images/8191bce69a8570d357c72ff10b20896e.png)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8191bce69a8570d357c72ff10b20896e.png)'
- en: Single sentence tagging
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: å•å¥æ ‡æ³¨
- en: For this objective, embeddings are computed for tokens of an input sentence,
    as usual. Then every embedding (except for *[CLS]* and *[SEP]*) is passed independently
    to a model which maps each of them to a given NER class (or not, if it cannot).
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†å®ç°è¿™ä¸ªç›®æ ‡ï¼Œé€šå¸¸ä¼šè®¡ç®—è¾“å…¥å¥å­çš„è¯åµŒå…¥ã€‚ç„¶åï¼Œå°†æ¯ä¸ªåµŒå…¥ï¼ˆé™¤äº†*`[CLS]`*å’Œ*`[SEP]`*ï¼‰ç‹¬ç«‹ä¼ é€’ç»™ä¸€ä¸ªæ¨¡å‹ï¼Œè¯¥æ¨¡å‹å°†æ¯ä¸ªåµŒå…¥æ˜ å°„åˆ°ç»™å®šçš„NERç±»åˆ«ï¼ˆå¦‚æœä¸èƒ½æ˜ å°„ï¼Œåˆ™ä¸è¿›è¡Œæ˜ å°„ï¼‰ã€‚
- en: Feature extraction
  id: totrans-112
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç‰¹å¾æå–
- en: Taking the last BERT layer and using it as embeddings is not the only way to
    extract features from the input text. In fact, the researchers completed several
    experiments of aggregating embeddings in different manners for solving a NER task
    on the CoNLL-2003 dataset. To conduct the experiment, they used the extracted
    embeddings as input for a randomly initialized two-layer 768-dimensional BiLSTM
    before applying the classification layer.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨æœ€åä¸€ä¸ªBERTå±‚ä½œä¸ºåµŒå…¥å¹¶ä¸æ˜¯ä»è¾“å…¥æ–‡æœ¬ä¸­æå–ç‰¹å¾çš„å”¯ä¸€æ–¹æ³•ã€‚å®é™…ä¸Šï¼Œç ”ç©¶äººå‘˜å®Œæˆäº†å‡ ç§ä¸åŒæ–¹å¼çš„åµŒå…¥èšåˆå®éªŒï¼Œä»¥è§£å†³CoNLL-2003æ•°æ®é›†ä¸Šçš„NERä»»åŠ¡ã€‚ä¸ºäº†è¿›è¡Œå®éªŒï¼Œä»–ä»¬å°†æå–çš„åµŒå…¥ä½œä¸ºè¾“å…¥ä¼ é€’ç»™ä¸€ä¸ªéšæœºåˆå§‹åŒ–çš„ä¸¤å±‚768ç»´BiLSTMï¼Œç„¶ååº”ç”¨åˆ†ç±»å±‚ã€‚
- en: The ways the embeddings were extracted (from the BERT base) are demonstrated
    in the figure below. As shown, the most performant way was to concatenate the
    four last BERT hidden layers.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: åµŒå…¥çš„æå–æ–¹å¼ï¼ˆæ¥è‡ªBERTåŸºç¡€æ¨¡å‹ï¼‰åœ¨ä¸‹é¢çš„å›¾ä¸­å±•ç¤ºäº†ã€‚å¦‚å›¾æ‰€ç¤ºï¼Œæœ€æœ‰æ•ˆçš„æ–¹æ³•æ˜¯è¿æ¥æœ€åå››ä¸ªBERTéšè—å±‚ã€‚
- en: Based on the conducted experiments, it is important to keep in mind that aggregation
    of hidden layers is a potential way of improving embeddingsâ€™ representation for
    achieving better results on a variety of NLP tasks.
  id: totrans-115
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: æ ¹æ®å·²å®Œæˆçš„å®éªŒï¼Œé‡è¦çš„æ˜¯è¦è®°ä½ï¼Œéšè—å±‚çš„èšåˆæ˜¯ä¸€ç§å¯èƒ½çš„æ”¹è¿›åµŒå…¥è¡¨ç¤ºä»¥åœ¨å„ç§NLPä»»åŠ¡ä¸­å–å¾—æ›´å¥½ç»“æœçš„æ–¹æ³•ã€‚
- en: '![](../Images/b3d9a0b4565828c42c75f85f18c9f5bd.png)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b3d9a0b4565828c42c75f85f18c9f5bd.png)'
- en: The diagram on the left shows the expanded BERT structure with hidden layers.
    The table on the right illustrates the ways the embeddings were constructed and
    the corresponding scores that were achieved by applying respective strategies.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: å·¦ä¾§çš„å›¾è¡¨å±•ç¤ºäº†å¸¦æœ‰éšè—å±‚çš„æ‰©å±•BERTç»“æ„ã€‚å³ä¾§çš„è¡¨æ ¼åˆ™è¯´æ˜äº†åµŒå…¥çš„æ„å»ºæ–¹å¼ä»¥åŠé€šè¿‡åº”ç”¨ç›¸åº”ç­–ç•¥æ‰€å–å¾—çš„å¾—åˆ†ã€‚
- en: Combining BERT with other features
  id: totrans-118
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å°†BERTä¸å…¶ä»–ç‰¹å¾ç»“åˆ
- en: 'Sometimes we deal not only with text but with numerical features, for example,
    as well. It is naturally desirable to build embeddings that can incorporate information
    from both text and other non-text features. Here are the recommended strategies
    to apply:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: æœ‰æ—¶æˆ‘ä»¬ä¸ä»…å¤„ç†æ–‡æœ¬ï¼Œè¿˜å¤„ç†æ•°å€¼ç‰¹å¾ã€‚ä¾‹å¦‚ï¼Œè‡ªç„¶å¸Œæœ›æ„å»ºèƒ½å¤Ÿèåˆæ–‡æœ¬å’Œå…¶ä»–éæ–‡æœ¬ç‰¹å¾ä¿¡æ¯çš„åµŒå…¥ã€‚ä»¥ä¸‹æ˜¯æ¨èåº”ç”¨çš„ç­–ç•¥ï¼š
- en: '**Concatenation of text with non-text features**. For instance, if we work
    with profile descriptions about people in the form of text and there are other
    separate features like their name or age, then a new text description can be obtained
    in the form: â€œMy name is <*name*>. <*profile description*>. I am <*age*> years
    oldâ€. Finally, such a text description can be fed into the BERT model.'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**å°†æ–‡æœ¬ä¸éæ–‡æœ¬ç‰¹å¾è¿›è¡Œä¸²è”**ã€‚ä¾‹å¦‚ï¼Œå¦‚æœæˆ‘ä»¬å¤„ç†çš„æ˜¯ä»¥æ–‡æœ¬å½¢å¼å­˜åœ¨çš„äººç‰©ç®€ä»‹ï¼Œå¹¶ä¸”æœ‰å…¶ä»–ç‹¬ç«‹çš„ç‰¹å¾å¦‚å§“åæˆ–å¹´é¾„ï¼Œé‚£ä¹ˆå¯ä»¥å¾—åˆ°æ–°çš„æ–‡æœ¬æè¿°ï¼Œå¦‚ï¼šâ€œæˆ‘çš„åå­—æ˜¯<*name*>ã€‚<*profile
    description*>ã€‚æˆ‘<*age*>å²ã€‚â€æœ€åï¼Œè¿™æ ·çš„æ–‡æœ¬æè¿°å¯ä»¥è¾“å…¥åˆ°BERTæ¨¡å‹ä¸­ã€‚'
- en: '**Concatenation of embeddings with features**. It is possible to build BERT
    embeddings, as discussed above, and then concatenate them with other features.
    The only thing that changes in the configuration is the fact a classification
    model for a downstream task has to accept now input vectors of higher dimensionality.'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**å°†åµŒå…¥ä¸ç‰¹å¾è¿›è¡Œä¸²è”**ã€‚å¯ä»¥å¦‚ä¸Šæ‰€è¿°æ„å»ºBERTåµŒå…¥ï¼Œç„¶åå°†å…¶ä¸å…¶ä»–ç‰¹å¾è¿›è¡Œä¸²è”ã€‚å”¯ä¸€æ”¹å˜çš„æ˜¯é…ç½®ä¸­å¿…é¡»æ¥å—æ›´é«˜ç»´åº¦çš„è¾“å…¥å‘é‡ç”¨äºä¸‹æ¸¸ä»»åŠ¡çš„åˆ†ç±»æ¨¡å‹ã€‚'
- en: Conclusion
  id: totrans-122
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç»“è®º
- en: In this article, we have dived into the processes of BERT training and fine-tuning.
    As a matter of fact, this knowledge is enough to solve the majority of tasks in
    NLP thankfully to the fact that BERT allows to almost fully incorporate text data
    into embeddings.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æ·±å…¥æ¢è®¨äº†BERTçš„è®­ç»ƒå’Œå¾®è°ƒè¿‡ç¨‹ã€‚å®é™…ä¸Šï¼Œè¿™äº›çŸ¥è¯†è¶³ä»¥è§£å†³å¤§å¤šæ•°è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ï¼Œæ„Ÿè°¢BERTå‡ ä¹å®Œå…¨å°†æ–‡æœ¬æ•°æ®çº³å…¥åµŒå…¥ä¸­çš„èƒ½åŠ›ã€‚
- en: In recent times, other BERT-like models have appeared (SBERT, RoBERTa, etc.).
    There even exists a special sphere of study called â€œ*BERTology*â€ which analyses
    BERT capabilities in depth for deriving new high-performant models. These facts
    reinforce the fact that BERT designated a revolution in machine learning and made
    it possible to significantly advance in NLP.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€è¿‘ï¼Œå‡ºç°äº†å…¶ä»–ç±»ä¼¼BERTçš„æ¨¡å‹ï¼ˆSBERTã€RoBERTaç­‰ï¼‰ã€‚ç”šè‡³å­˜åœ¨ä¸€ä¸ªä¸“é—¨ç ”ç©¶é¢†åŸŸï¼Œç§°ä¸ºâ€œ*BERTology*â€ï¼Œå®ƒæ·±å…¥åˆ†æBERTçš„èƒ½åŠ›ï¼Œä»¥å¼€å‘æ–°çš„é«˜æ€§èƒ½æ¨¡å‹ã€‚è¿™äº›äº‹å®è¿›ä¸€æ­¥è¯æ˜äº†BERTåœ¨æœºå™¨å­¦ä¹ é¢†åŸŸå¼•å‘äº†é©å‘½ï¼Œå¹¶ä½¿è‡ªç„¶è¯­è¨€å¤„ç†å¾—ä»¥æ˜¾è‘—è¿›æ­¥ã€‚
- en: Resources
  id: totrans-125
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: èµ„æº
- en: '[BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/pdf/1810.04805.pdf)'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[BERT: æ·±åº¦åŒå‘å˜æ¢å™¨çš„é¢„è®­ç»ƒç”¨äºè¯­è¨€ç†è§£](https://arxiv.org/pdf/1810.04805.pdf)'
- en: '*All images unless otherwise noted are by the author*'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '*é™¤éå¦æœ‰è¯´æ˜ï¼Œå¦åˆ™æ‰€æœ‰å›¾åƒå‡ä¸ºä½œè€…æä¾›*'
