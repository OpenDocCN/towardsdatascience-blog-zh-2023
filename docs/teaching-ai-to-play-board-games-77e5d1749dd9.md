# 教人工智能玩棋盘游戏

> 原文：[`towardsdatascience.com/teaching-ai-to-play-board-games-77e5d1749dd9`](https://towardsdatascience.com/teaching-ai-to-play-board-games-77e5d1749dd9)

## 使用从零开始的强化学习教计算机玩井字棋

[](https://heiko-hotz.medium.com/?source=post_page-----77e5d1749dd9--------------------------------)![Heiko Hotz](https://heiko-hotz.medium.com/?source=post_page-----77e5d1749dd9--------------------------------)[](https://towardsdatascience.com/?source=post_page-----77e5d1749dd9--------------------------------)![Towards Data Science](https://towardsdatascience.com/?source=post_page-----77e5d1749dd9--------------------------------) [Heiko Hotz](https://heiko-hotz.medium.com/?source=post_page-----77e5d1749dd9--------------------------------)

·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----77e5d1749dd9--------------------------------) ·18 分钟阅读·2023 年 12 月 12 日

--

![](img/60b0afcb68a3b3d4c14f8bdf7964d2ef.png)

图片由作者提供（由 ChatGPT 创建）

# 这是什么内容？

目前，人工智能领域似乎每个人都在提升他们的强化学习（RL）技能，特别是在 Q-learning 方面，跟随关于 OpenAI 新 AI 模型 [*Q**](https://en.wikipedia.org/wiki/OpenAI#Q*) 的最新传闻，我也参与其中。然而，我决定用我对棋盘游戏的热情来介绍 Q-learning 🤓，而不是对 *Q** 进行猜测或重温 Q-learning 的旧论文和示例。

在这篇博客文章中，我将从头开始创建一个简单的程序，教一个模型如何玩井字棋（TTT）。我将避免使用任何强化学习库，比如 [*Gym*](https://github.com/openai/gym) 或 [*Stable Baselines*](https://github.com/DLR-RM/stable-baselines3)；所有内容都是用原生 Python 手动编写的，脚本只有 100 行。如果你对如何指导人工智能玩游戏感到好奇，请继续阅读。

你可以在 GitHub 上找到所有代码，链接为 [`github.com/marshmellow77/tictactoe-q`](https://github.com/marshmellow77/tictactoe-q)。

# 为什么这很重要？

教人工智能玩井字棋（TTT）可能看起来并不那么重要。然而，它确实提供了一个（希望）清晰且易于理解的 Q-learning 和 RL 的介绍，这在生成式人工智能（GenAI）领域可能是重要的，因为有人猜测像 GPT-4 这样的独立 GenAI 模型对于显著的进步是不够的。它们的局限性在于只能预测下一个标记，而无法进行任何推理。RL 被认为能够解决这个问题，并可能增强 GenAI 模型的响应能力。

无论你是为了迎接这些进展而提升你的 RL 技能，还是仅仅寻求一个有趣的 Q 学习入门教程，这个教程都适合这两种情况🤗

# **理解 Q 学习**

从本质上讲，Q 学习是一种算法，它学习特定状态下一个动作的价值，然后利用这些信息找到最佳动作。让我们考虑*Frozen Lake*游戏的例子，这是一款用于演示 Q 学习的流行单人游戏。

在 Frozen Lake 中，玩家（从单元格 0 开始）在冰和水的网格上移动，目标是到达目标（单元格 15）而不掉入水中。每个单元格代表一个状态，玩家可以向四个方向移动：上、下、左或右。

![](img/445f990a7fb1ddd8e01d917fc85bd76b.png)

作者提供的图片（使用 Stable Diffusion 创建）

在游戏开始时，***代理***（这就是 AI 玩家通常的称呼）没有任何信息，只会随机尝试一些动作。在 Q 学习的背景下，这个探索阶段至关重要。代理通过根据其动作获得奖励或惩罚来学习。在 Frozen Lake 中，达到目标会获得高奖励，而掉入水中则会受到惩罚。这种奖励和惩罚的系统引导代理学习最有效的到达目标的路径。

Q 学习使用一个表格，称为 Q-表，用于记录每个状态下每个动作的价值。随着代理探索环境，这个表格会不断更新。Q-表条目，称为 Q 值，表示在给定状态下采取某个动作的预期效用，它们通过[贝尔曼方程](https://en.wikipedia.org/wiki/Bellman_equation)进行更新。这个方程考虑了动作的即时奖励和可能的最高未来奖励（稍后会详细讲解）。

基本上，Q-表是代理的备忘单或查找表：根据游戏的状态，代理会查找该状态，确定哪个动作具有最高效用（即哪个是最佳动作），然后执行该动作。以下是 Q-表可能的示例：

![](img/1b92d5b9413528dfdb03ec6cdcf27250.png)

作者提供的图片

在这个例子中，如果玩家处于状态 1（即在单元格 1），他会选择动作*右*，因为这是具有最高价值的动作。

随着时间的推移，代理探索环境并更新 Q-表，它在导航 Frozen Lake 时变得更加熟练，最终学会了一种最佳或接近最佳的策略，以可靠地到达目标。Q 学习在这种情况下的美妙之处在于它的无模型性质，这意味着它不需要环境模型，可以仅通过交互学习，使其广泛适用于各种 RL 问题。

存在许多教程演示了如何利用和实现 Q-learning 来解决 Frozen Lake 游戏，例如 `towardsdatascience.com/q-learning-for-beginners-2837b777741`。然而，正如前面提到的，作为一个棋盘游戏爱好者，我对将这种方法适用于双人游戏，甚至更多玩家的游戏更感兴趣。

# **双人游戏中的挑战**

将 Q-learning 应用于双人游戏，如井字棋，需要进行一些小的修改。在 Frozen Lake 游戏中，下一状态仅由代理的行动决定。然而，在井字棋中，尽管玩家可能采取一个回合，但随后的状态还依赖于对手的行动。例如，如果我在左上角放置一个‘X’，那么下一状态是不确定的，因为我的对手有几个潜在的移动：

![](img/db3bd40a35d9275cb4e3928f83be3e65.png)

作者提供的图片

可以采用几种方法来解决这个问题。一种方法是模拟对手所有可能的行动及其相应结果。这需要生成所有潜在后续状态的概率分布，并根据这些状态的预期结果更新 Q 值。然而，这种方法可能计算量较大。在本教程中，我们将采用一种更简单的方法，为对手随机采取一个动作，并根据这个动作的实际结果更新 Q 表。这很好地反映了对手的不可预测性，正如我们后面将看到的那样。通过这种方法，Q-learning 可以有效地适应双人游戏，使 AI 不仅能够学习最佳移动，还能（最终）适应人类对手的策略。

这种方法原则上与训练 [AlphaGo Zero](https://en.wikipedia.org/wiki/AlphaGo_Zero) 的方法类似。该 AI 程序在快速连续的对弈中自我对弈了 490 万局围棋。在这个过程中，它不断提高自己的技能，自主学习和调整策略。这种自学习方法，绕过了模拟对手每一个可能的移动的需求，为 AI 系统提供了一种高效且有效的学习和适应复杂任务的方法。

![](img/bf55b0ed4cb304a37d134c2305ff771f.png)

李世石与 AlphaGo 的第 2 局比赛，AlphaGo 的著名第 37 步。图片来源：[`commons.wikimedia.org/wiki/File:Lee_Sedol_(W)_vs_AlphaGo_(B)_-_Game_2.svg`](https://commons.wikimedia.org/wiki/File:Lee_Sedol_(W)_vs_AlphaGo_(B)_-_Game_2.svg)（许可 CC BY-SA 4.0）

在接下来的部分中，我们将深入探讨这些原则如何在井字棋的具体情况下应用，展示在双人环境中 Q-learning 的实现。

# **井字棋的 Q-Learning**

当我们开始将 Q-learning 应用于井字棋时，了解我们程序的设置以及 AI 代理将要操作的环境非常重要。

## 概述

这段代码旨在训练一个 AI（我们称之为*玩家 1*或*智能体*），通过 Q 学习（一种强化学习形式）来玩类似井字棋的游戏。它首先设置学习参数并初始化一个 Q 表来存储不同状态下不同动作的值。脚本定义了几个函数来管理游戏机制，如确定可能的移动、检查胜者、更新游戏状态，以及在移动后计算下一个状态和奖励。

在脚本的主要部分中，实现了 Q 学习算法。它运行多个回合，模拟智能体与其对手（我们称之为*玩家 2*）之间的游戏。在每一回合中，AI 要么探索一个随机动作，要么利用 Q 表中的知识来进行决策，从结果中学习以更新 Q 表的值。这个过程涉及随着时间的推移调整探索率，从随机探索转向更具策略性的动作。

我们设置的一个关键方面是 AI 的对手。与对手可能拥有复杂策略的更复杂场景不同，我们的 AI 将与一个随机移动的对手进行游戏。这一选择简化了学习环境，使我们可以专注于 AI 的学习过程，而不是对手策略的复杂性。

## Q 学习设置

我们的 Q 学习设置涉及一些关键参数，这些参数将影响 AI 的学习方式：

```py
learning_rate = 0.2
discount_factor = 0.9
num_episodes = int(1e7)
epsilon = 1.0  # Exploration rate
epsilon_min = 0.01
epsilon_decay = 0.999
```

+   **学习率 (**`**learning_rate**`**):** 这决定了新信息对现有知识的影响程度。较高的学习率加速了学习过程，但可能导致不稳定。学习率为 0.2 在学习新策略和保留之前的学习之间取得了平衡。

+   **折扣因子 (**`**discount_factor**`**):** 这反映了未来奖励的重要性，影响 AI 策略的远见程度。折扣因子为 0.9 时，AI 会特别重视未来奖励，鼓励 AI 前瞻性思考，而不仅仅关注即时收益。

+   **回合数 (**`**num_episodes**`**):** 这是 AI 学习的游戏数量，为 AI 提供了充足的机会来体验各种游戏场景。将其设置为 1000 万 (`1e7`) 允许广泛的训练，为 AI 提供了从各种游戏场景中学习的充足机会。

+   **探索率 (**`**epsilon**`**):** 探索率（epsilon）最初设置较高，以允许 AI 探索各种动作，而不是仅仅利用已知策略。最初，AI 会更多地进行探索（由于 `epsilon` 为 1.0）。随着时间的推移，`epsilon` 逐渐减小到 `epsilon_min`，AI 将开始更多地利用其学习到的策略。

> **关于探索率的附注**
> 
> 在 Q 学习中，探索率通常用符号 ε（epsilon）表示，这是一个关键参数，决定了探索（尝试新动作）和利用（使用已知最佳动作）之间的平衡。最初，智能体对环境了解不多，因此它需要广泛探索，通过尝试不同的动作。探索率通常在开始时设定为较高的值（例如 1 或接近 1），决定了智能体选择随机动作而不是根据 Q 表选择最佳已知动作的概率。
> 
> 然而，随着智能体对环境的了解越来越多，Q 表变得更加可靠，探索的必要性减少，利用已获得的知识变得更加有益。这时，探索率衰减就发挥作用了。探索率衰减是一个随着时间推移而减少探索率的因素。它确保智能体在学习和收集更多信息的过程中，逐渐从探索环境转向利用 Q 表中学到的值。
> 
> 这种平衡在 Q 学习中很重要，因为它可以避免两个主要问题：
> 
> **陷入局部最优：** 如果智能体只利用已知信息（低探索），可能会陷入局部最优。这意味着它会根据有限的信息反复选择看似最佳的动作，但可能错过发现能带来更好长期奖励的动作。
> 
> **低效学习：** 另一方面，如果智能体过度探索（高探索）且时间过长，可能导致低效学习。智能体可能会不断尝试次优动作而没有充分利用已经获得的知识，从而导致收敛到最优策略的速度变慢。
> 
> 通过适当设置探索率及其衰减，Q-learning 算法可以有效地平衡这两个方面，使智能体能够最初探索环境，然后逐渐更多地专注于利用它所学到的最佳策略。这种平衡对于在复杂环境中学习的效率和有效性至关重要。

在接下来的章节中，我们将深入代码，看看 AI 如何使用 Q-learning 来做决策、更新策略，并最终掌握 Tic-Tac-Toe。

# 代码深度解析

## 训练脚本

这是 [train.py](https://github.com/marshmellow77/tictactoe-q/blob/master/train.py) 文件的详细解读。

训练从 for 循环开始（大致在脚本的中间），我们将在其中进行一定数量的回合：

```py
for episode in range(num_episodes):
    state = [0] * 9  # Starting state - empty board
```

接着，我们随机确定起始玩家。一个更简单的方法是让我们的智能体总是作为起始玩家。然而，实现一个随机起始玩家并不比直接总是让智能体作为起始玩家多花费多少精力，并且这种方法使 Q 表模式更加通用，即我们的智能体将学习如何作为起始玩家以及非起始玩家进行游戏。

如果玩家 2 开始游戏，那么我们将为玩家 2 进行随机移动：

```py
 # If Player 2 starts, make a random move
    if current_player == 2:
        actions = get_possible_actions(state)
        random_action = random.choice(actions)
        state = update_state(state, random_action, 2)
        current_player = 1  # Switch to Player 1
```

现在我们进入实际的 TTT 游戏训练循环，只有在游戏结束时才会停止。一个关键机制是之前讨论的开发 vs 探索机制。它的实现如下：

```py
if random.uniform(0, 1) < epsilon:
    # Explore: choose a random action
    action = random.choice(actions)
else:
    # Exploit: choose the best action based on Q-table
    action = max(actions, key=lambda x: Q_table[state_str][x])
```

epsilon 值越低，智能体通过随机移动进行的探索越少，它将更多地利用 Q 表。

一旦选择了智能体的动作，我们将执行它并确定下一状态（以及适用的奖励）：

```py
# Take action and observe new state and reward
new_state, reward = get_next_state_and_reward(state, action)
```

处理所有这些操作的函数值得更仔细地查看：

```py
def get_next_state_and_reward(state, action):
    new_state = update_state(state, action, 1)  # Player 1's move
    if is_winner(new_state, 1):
        return (new_state, 1)  # Reward for winning
    elif 0 not in new_state:
        return (new_state, 0.1)  # Draw
    else:
        # Player 2 (random) makes a move
        actions = get_possible_actions(new_state)
        random_action = random.choice(actions)
        new_state = update_state(new_state, random_action, 2)
        if is_winner(new_state, 2):
            return (new_state, -1)  # Penalty for losing
        else:
            return (new_state, 0)  # No immediate reward or penalty
```

在这个函数中，我们首先更新棋盘的状态并检查我们的智能体是否赢得了游戏。如果没有，我们为对手进行随机移动，并再次检查对手是否赢得了游戏。根据结果，我们返回 0（游戏仍在进行中）、0.1（平局）、+1（智能体获胜）或 -1（对手获胜）。我们选择 0.1 作为平局的奖励是为了激励智能体尽快结束游戏。

现在我们已经确定了奖励，接下来是整个程序中最关键的部分：通过 Bellman 方程更新 Q 表：

```py
Q_table[state_str][action] += learning_rate * (
            reward + discount_factor * max(Q_table[new_state_str]) - Q_table[state_str][action])
```

这个 Bellman 方程在其他博客文章中解释得更好（再次参考 `towardsdatascience.com/q-learning-for-beginners-2837b777741`）。但简要解释如下：

> 如前所述，Q 表本质上是一个大的备忘单：它跟踪游戏中的所有可能状态以及从该状态开始的每个可能移动的价值。它告诉智能体在给定情况下每个移动的好坏，基于它迄今为止学到的知识。
> 
> Bellman 方程更新这个 Q 表。它通过查看智能体收到的*即时*奖励（赢、输、平局）和它可以移动到的未来状态（即*未来*奖励）的质量来实现。因此，在每局游戏后，智能体使用 Bellman 方程来修订其 Q 表，学习哪些移动可能导致胜利、失败或平局。

最后，我们调整探索率，以便在未来的游戏中，智能体更多地使用 Q 表而较少进行探索。

```py
epsilon = max(epsilon_min, epsilon_decay * epsilon)
```

## 运行训练

一旦训练脚本准备好，我们就可以执行它。幸运的是，这个过程计算需求不高，完成得非常快，不需要特别的计算能力。例如，我在 MacBook M1 Air 上执行了这个过程，它在 1000 万局游戏中不到 5 分钟就完成了。训练完成后，我们将保存 Q 表（它不是特别大），以便我们可以用它来测试智能体，与 AI 对战，并可能在稍后的阶段继续训练，以进一步增强表格。我们来看看吧 🧐

# Q 表的人工检查

这个表格相对容易理解：每一行代表了棋盘状态、可采取的行动及其质量。让我们来看看一些有趣的状态。请注意，你的表格可能会有不同（但希望是相似）的值：

![](img/f47b55da1649ba7c93d541cd6511960e.png)

图片由作者提供

棋盘状态显示了每个玩家已经放置的位置（前 3 个数字代表第一行，接下来的 3 个代表第二行，最后 3 个代表最后一行。动作对应棋盘上的位置，每个动作的数字表示该动作的质量。在这个例子中，我们看到一个状态，似乎只有一个动作（动作 7）被认为是好的，其他所有动作都显得较差。

> 注意：棋盘位置的索引如下：

![](img/3d7945604a7896490040cbf64047977d.png)

图片由作者提供

所以，让我们来可视化 Q 表中的这个特定条目的棋盘状态：

![](img/5fd8c1971270157298573d23b371f20c.png)

图片由作者提供

的确，在这个位置，代理（玩家 1）唯一的好选择是选择位置 7。所有其他移动可能会导致输掉比赛（请记住，玩家 2 将在下一轮随机移动，因此输掉比赛并非必然）。

让我们再看一个例子：

![](img/1c365fcb2f84711d6b1ca0d27a1629ce.png)

图片由作者提供

![](img/854666655addde48dd4dfeb51047bb9f.png)

图片由作者提供

在这个例子中，显然最佳移动是选择位置 8（右下角）并赢得比赛。如果代理选择其他位置，它很可能会输掉比赛。因此，Q 表将指示我们的代理采取动作 8。

# 测试新代理

现在我们已经训练了模型，我们可以用 GH 仓库中的脚本[test.py](https://github.com/marshmellow77/tictactoe-q/blob/master/test.py)来测试它。在脚本中，我们将让代理与一个随机移动的对手进行若干局比赛，看看它的表现如何。我们首先初始化我们的代理并加载 Q 表以便在游戏环境中用于决策。`play_game`函数模拟了一场比赛，使用加载的 Q 表来指导代理的决策。这里的游戏环境是一个简单的 3x3 棋盘，每个状态代表棋盘的不同配置。

代理以玩家 1 的身份，根据 Q 表做出决策——选择当前状态下值最高的行动。如果在 Q 表中找不到状态，代理将做出随机移动。这种学习行为和随机性的结合有助于评估训练的鲁棒性。玩家 2 的移动完全随机，为代理提供了多样化的场景。

这些游戏的结果会被跟踪，量化胜利、失败和平局的数量。这有助于评估训练模型的效果。如果设置了`log_lost_games`标志，将保存详细的失败游戏日志，这对于进一步分析和改进模型是非常宝贵的。这一测试过程，通过进行大量游戏，提供了对训练后代理能力的全面了解。

![](img/e88dbab492830f205ffd07e030e1ebd6.png)

作者提供的图片

# 与 AI 对战

看起来对随机机器人进行的测试很成功。我们的 AI 赢得了超过 95%的比赛。现在，我们想亲自与 AI 对战。我们可以使用[play.py](https://github.com/marshmellow77/tictactoe-q/blob/master/play.py)来实现这一点。

在这个程序中，我们通过一个简单的控制台界面与 AI 互动。游戏板表示为一个 3x3 的网格，每个位置从 0 到 8 编号。当轮到我们时，我们会被提示输入一个数字，以选择我们想要移动的位置。

AI 使用从 CSV 文件加载的 Q 表来做出决策。这个 Q 表来源于之前的训练过程，引导 AI 根据当前的游戏板状态选择最佳可能的移动。如果 AI 遇到 Q 表中没有的状态，它将默认进行随机移动。

游戏在我们的回合和 AI 的回合之间交替进行。每次移动后，更新后的棋盘会被显示，程序会检查是否有赢家。如果玩家获胜或游戏结果为平局，游戏结束，结果将被宣布——无论是我们获胜、AI 获胜还是平局。

这个互动游戏提供了一个很好的机会来实时测试 AI 的能力。让我们开始吧：

![](img/eac09908ab709e6708a660a156074384.png)

作者提供的图片

在这个游戏中，如果我们不选择动作 0（左上角），AI 将有机会赢得比赛。它会意识到这一点吗？

![](img/3bf5a11009097edc79b18d607d9ef811.png)

作者提供的图片

确实做到了！很好😊

# **结论**

在这篇文章中，我们训练了我们的 AI 代理对抗一个进行随机移动的玩家。这已经足够好，能够在对抗进行随机移动的对手时达到超过 95%的胜率。但是，有方法可以改进训练过程，希望也能提高 AI 的表现。

## 参数调整的影响

将 Q 学习应用于井字游戏揭示了强化学习的一个关键方面：调整参数的艺术。正确设置这些参数，如开发与探索之间的平衡、学习率和折扣因子，是 RL 代理成功的关键。

+   **探索与利用：** 由`epsilon`值控制，这一平衡决定了智能体尝试新策略的频率与依赖已知策略的比例。高探索率鼓励智能体尝试新事物，可能导致创新策略，而高利用率使智能体依赖现有知识，虽然可能更高效，但可能会错过更好的策略。

+   **学习率：** 高学习率意味着智能体迅速采纳新信息，这在动态环境中可能有利，但如果智能体过快地覆盖有用的学习，可能导致不稳定。相反，低学习率意味着智能体更多依赖过去的知识，导致稳定但可能较慢的学习。

+   **折扣因子：** 这个参数影响智能体对未来奖励的重视程度。高折扣因子使智能体更具前瞻性，考虑其行动的长期后果。相反，低折扣因子则使智能体目光短浅，专注于即时奖励。

这些参数的变化可以显著改变 RL 智能体的行为。例如，折扣因子低的智能体可能会以攻击性方式玩井字棋，专注于即时胜利，而不是制定未来的策略。相反，折扣因子高的智能体可能会更具策略性，考虑每一步对游戏未来状态的影响。

同样，高学习率的智能体可能迅速适应新策略，不断发展其游戏玩法，而低学习率的智能体可能坚持经过验证的策略，游戏中的变化较小。

## 轮到你来实验了

这就是强化学习真正的激动所在。每个参数都可以进行微调，以观察它如何影响 AI 智能体的学习和表现。我邀请你深入这个实验的世界。调整学习率、探索率和折扣因子，观察这些变化如何影响 AI 在井字棋游戏中的策略。

## 更高级的技术

为了进一步提高模型的表现，实施自我对弈机制，即 AI 与来自不同训练阶段的自身版本对弈（而不是与进行随机移动的对手对弈），可能是一种有效的策略。这种方法在 AlphaGo 等系统中成功应用过，并可能导致更强大和适应性更强的 AI 玩家。

对于更复杂的游戏，如国际象棋和围棋，维持一个 Q 表将不再可行，因为它变得过于庞大。在这些游戏中，采用像深度 Q 学习这样的技术可以显著增强 AI 的学习能力。通过使用神经网络来逼近 Q 表，AI 可以处理超出简单 3x3 井字棋网格的更复杂状态，使其在更复杂的游戏中具备可扩展性。

总之，目前的设置已经展示了有希望的结果。然而，这些建议的改进可能会进一步提升 AI 的表现，将其从一个合格的井字棋玩家转变为一个能够应对更复杂战略游戏的高级 AI。

## 进一步的相关资料

如果你对学习更多关于强化学习如何应用于棋盘游戏感兴趣，可以查看下面的两个视频。第一个视频非常简短，*深入探讨了现代象棋 AI 机器人如何进行游戏*：

第二个视频是电影*AlphaGo*（在 YouTube 上免费观看），讲述了 AlphaGo 模型的开发过程以及它如何击败当时的世界冠军：

# Heiko Hotz

👋 在[Medium](https://heiko-hotz.medium.com/)和[LinkedIn](https://www.linkedin.com/in/heikohotz/)关注我，阅读更多关于生成 AI、机器学习和自然语言处理的内容。

👥 如果你在伦敦，可以参加我们的[NLP London Meetups](https://www.meetup.com/nlp_london/)。

![](img/adb4c021b5bd4b11847ab1787aefa7aa.png)
