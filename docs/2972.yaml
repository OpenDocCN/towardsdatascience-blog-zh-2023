- en: Data Access API over Data Lake Tables Without the Complexity
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据访问 API 无需复杂性即可访问数据湖表
- en: 原文：[https://towardsdatascience.com/data-access-api-over-data-lake-tables-without-the-complexity-4deb68ee88b3?source=collection_archive---------3-----------------------#2023-09-28](https://towardsdatascience.com/data-access-api-over-data-lake-tables-without-the-complexity-4deb68ee88b3?source=collection_archive---------3-----------------------#2023-09-28)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/data-access-api-over-data-lake-tables-without-the-complexity-4deb68ee88b3?source=collection_archive---------3-----------------------#2023-09-28](https://towardsdatascience.com/data-access-api-over-data-lake-tables-without-the-complexity-4deb68ee88b3?source=collection_archive---------3-----------------------#2023-09-28)
- en: Build a robust GraphQL API service on top of your S3 data lake files with DuckDB
    and Go
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 DuckDB 和 Go 在 S3 数据湖文件上构建一个稳健的 GraphQL API 服务
- en: '[](https://medium.com/@alon.agmon?source=post_page-----4deb68ee88b3--------------------------------)[![Alon
    Agmon](../Images/c64e33ca886da4f4984c96acc7116a4d.png)](https://medium.com/@alon.agmon?source=post_page-----4deb68ee88b3--------------------------------)[](https://towardsdatascience.com/?source=post_page-----4deb68ee88b3--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----4deb68ee88b3--------------------------------)
    [Alon Agmon](https://medium.com/@alon.agmon?source=post_page-----4deb68ee88b3--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@alon.agmon?source=post_page-----4deb68ee88b3--------------------------------)[![Alon
    Agmon](../Images/c64e33ca886da4f4984c96acc7116a4d.png)](https://medium.com/@alon.agmon?source=post_page-----4deb68ee88b3--------------------------------)[](https://towardsdatascience.com/?source=post_page-----4deb68ee88b3--------------------------------)[![数据科学前沿](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----4deb68ee88b3--------------------------------)
    [Alon Agmon](https://medium.com/@alon.agmon?source=post_page-----4deb68ee88b3--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fbcd1e3126cdc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdata-access-api-over-data-lake-tables-without-the-complexity-4deb68ee88b3&user=Alon+Agmon&userId=bcd1e3126cdc&source=post_page-bcd1e3126cdc----4deb68ee88b3---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----4deb68ee88b3--------------------------------)
    ·11 min read·Sep 28, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F4deb68ee88b3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdata-access-api-over-data-lake-tables-without-the-complexity-4deb68ee88b3&user=Alon+Agmon&userId=bcd1e3126cdc&source=-----4deb68ee88b3---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fbcd1e3126cdc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdata-access-api-over-data-lake-tables-without-the-complexity-4deb68ee88b3&user=Alon+Agmon&userId=bcd1e3126cdc&source=post_page-bcd1e3126cdc----4deb68ee88b3---------------------post_header-----------)
    发表在 [数据科学前沿](https://towardsdatascience.com/?source=post_page-----4deb68ee88b3--------------------------------)
    · 11分钟阅读 · 2023年9月28日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F4deb68ee88b3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdata-access-api-over-data-lake-tables-without-the-complexity-4deb68ee88b3&user=Alon+Agmon&userId=bcd1e3126cdc&source=-----4deb68ee88b3---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4deb68ee88b3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdata-access-api-over-data-lake-tables-without-the-complexity-4deb68ee88b3&source=-----4deb68ee88b3---------------------bookmark_footer-----------)![](../Images/14f46d42b110bbb3aae6c698ca6ddf55.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4deb68ee88b3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdata-access-api-over-data-lake-tables-without-the-complexity-4deb68ee88b3&source=-----4deb68ee88b3---------------------bookmark_footer-----------)![](../Images/14f46d42b110bbb3aae6c698ca6ddf55.png)'
- en: Photo by [Joshua Sortino](https://unsplash.com/@sortino?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：[Joshua Sortino](https://unsplash.com/@sortino?utm_source=medium&utm_medium=referral)
    选自 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: '**1\. Intro**'
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**1\. 简介**'
- en: 'Data lake tables are mostly utilized by data engineering teams using big data
    compute engines, such as Spark or Flink, as well as by data analysts and scientists
    creating models and reports with heavy SQL query engines, such as Trino or Redshift.
    These compute engines have become the standard for accessing data in data lakes
    because they were designed to efficiently handle the challenges big data wrangling
    typically involves: scanning large volumes of data, dealing with cloud-based object
    storage, reading and writing query-optimized formatted files such as Parquet or
    ORC, etc.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 数据湖表主要由数据工程团队使用大数据计算引擎（如Spark或Flink）利用，也被数据分析师和科学家使用，后者使用重型SQL查询引擎（如Trino或Redshift）创建模型和报告。这些计算引擎已成为访问数据湖数据的标准，因为它们被设计来有效处理大数据处理通常涉及的挑战：扫描大量数据，处理基于云的对象存储，读取和写入查询优化的格式化文件，如Parquet或ORC等。
- en: However, it is also a common requirement to make big data products (or some
    aggregated view of them) accessible to *thinner clients,* such as internal micro
    services, using some sort of API. Suppose that we have a data lake table that
    stores real-time statistics about our customers generated by some Spark application.
    This data might be primarily used for internal reporting, but might also be valuable
    for other services in our organization. Despite the fact that this is a common
    requirement, it is far from being simple, mostly because it requires quite a different
    tool-set. Making parquet files in an S3 bucket available for low latency HTTP-based
    API is not straightforward (especially when the files are continuously updated
    and some transformation is required before making them accessible).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，将大数据产品（或其某些汇总视图）通过某种API提供给*轻量客户端*，如内部微服务，也是一个常见的需求。假设我们有一个数据湖表，存储了由某些Spark应用生成的实时客户统计数据。这些数据可能主要用于内部报告，但对我们组织的其他服务也可能很有价值。尽管这是一个常见的需求，但它远非简单，主要因为它需要完全不同的工具集。将S3桶中的parquet文件提供给低延迟的HTTP
    API并不简单（尤其是当文件持续更新并且在提供之前需要进行某些转换时）。
- en: To make such use case work, we will typically need a database that will be able
    to process queries in a fast customer-facing latency. Likewise, we will need some
    ETL jobs that can handle and transform data files in S3 and load them into the
    database. Finally, we will have to create a proper API endpoint to serve client’s
    queries.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使这种用例有效，我们通常需要一个能够以快速客户面向延迟处理查询的数据库。同样，我们需要一些ETL作业来处理和转换S3中的数据文件并将其加载到数据库中。最后，我们必须创建一个适当的API端点来服务客户的查询。
- en: '![](../Images/592433b37191450dd71588937c41f62e.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/592433b37191450dd71588937c41f62e.png)'
- en: Indeed, as the illustration shows, providing thin clients the ability to query
    data lake files fast usually comes at the price of adding more moving parts and
    processes to our pipeline, in order to either copy and ingest data to more expensive
    customer-facing warehouses or aggregate and transform it to fit low-latency databases.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 确实，如插图所示，提供给轻量客户端快速查询数据湖文件的能力通常会带来更多的移动部件和流程，以便要么将数据复制并摄取到更昂贵的客户面仓库中，要么将其汇总和转换以适应低延迟数据库。
- en: The purpose of this post is to explore and demonstrate a different and simpler
    approach to tackle this requirement using lighter in-process query engines. Specifically,
    I show how we can use in-process engines, such as *DuckDB* and *Arrow Data Fusion*,
    in order to create services that can both handle data lake files and volumes *and*
    act as a fast memory store that serves low-latency API calls. Using this approach,
    we can efficiently consolidate the required functionality into a single query
    service, which can be horizontally scaled, that will load data, aggregate and
    store it in memory, and serve API calls efficiently and fast.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 本文的目的是探讨和演示一种不同且更简单的方法，利用轻量级的内进程查询引擎来解决这一需求。具体来说，我将展示如何使用内进程引擎，如*DuckDB*和*Arrow
    Data Fusion*，以创建能够处理数据湖文件和数据量*并且*作为快速内存存储来服务低延迟API调用的服务。采用这种方法，我们可以将所需的功能高效整合到一个可以水平扩展的单一查询服务中，该服务将加载数据、汇总并存储在内存中，并高效、快速地处理API调用。
- en: '![](../Images/447d52b79ad2fe45ac20baa00260a41d.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/447d52b79ad2fe45ac20baa00260a41d.png)'
- en: In what follows, Section 2 will outline the main requirements and building blocks
    for this service and explain how they help us solve the main challenges it involves.
    Section 3 will delve into the core of the service — the data loading and query
    functionality, and demonstrate how it can be implemented using DuckDB and Go.
    (This post will be focused on DuckDB and Go, but you can find an implementation
    of this concept in Rust using Arrow Data Fusion in the repo linked below). Section
    4 will add the GraphQL API serving layer on top. Section 5 will conclude.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，第2节将概述该服务的主要要求和构建模块，并解释它们如何帮助我们解决主要挑战。第3节将*深入*服务的核心——数据加载和查询功能，并演示如何使用 DuckDB
    和 Go 实现它。（本文将专注于 DuckDB 和 Go，但你可以在下面链接的仓库中找到使用 Arrow Data Fusion 实现这一概念的 Rust
    实现）。第4节将在其上添加 GraphQL API 服务层。第5节将作结。
- en: 2\. Main Building Blocks
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 2. 主要构建模块
- en: It should be noted that the basic assumption in this approach is that the data
    we want to make available for our API can fit in the memory of our service, or
    the machine that runs it. For some use cases this might be a problematic restriction,
    but I believe that this is less restrictive that it seems. For example, I have
    used this approach with an in-memory relational table that consists of 2M records
    and 10 columns with memory usage of around 350MB. We tend to forget that the data
    that we actually *serve* is often much smaller than the data we store or process.
    At any rate, that’s an important consideration to keep in mind.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 应注意，这种方法的基本假设是我们希望提供给 API 的数据可以适配到我们服务的内存中，或运行服务的机器中。对于某些用例来说，这可能是一个问题，但我认为这比看起来的限制要小。例如，我曾经使用这种方法处理一个包含2M记录和10列的内存关系表，内存使用量约为350MB。我们往往忘记，我们实际*提供*的数据通常比我们存储或处理的数据要小得多。无论如何，这都是一个需要牢记的重要因素。
- en: 'An independent service that will operate as a simpler and more attractive alternative
    to the common architecture described above should answer, at minimum, the following
    requirements:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 一个独立的服务，它将作为一个更简单、更有吸引力的替代方案来取代上述常见架构，应该至少满足以下要求：
- en: It should be able to comfortably read and transform data files directly from
    our data lake or object storage.
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它应该能够直接从我们的数据湖或对象存储中舒适地读取和转换数据文件。
- en: It should be able to store relational data in memory and respond to queries
    with low latency.
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它应该能够在内存中存储关系数据，并以低延迟响应查询。
- en: It should be horizontally scalable
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它应该具备水平扩展性。
- en: It should be easy and declarative to query, transform, and load data — SQL will
    be the most convenient approach.
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 查询、转换和加载数据应该简单且声明性强——SQL将是最方便的方法。
- en: Simply put, instead of extending our application’s infrastructure with a database
    and an additional ETL process, ideally, we would like to create a service that
    can load and transform data directly from the source, store it efficiently, and
    provide us the ability to query it fast.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，我们希望创建一个可以直接从源加载和转换数据、有效存储数据并快速查询的服务，而不是扩展应用程序的基础设施，增加数据库和额外的ETL过程。
- en: In my view, the combination of these 3 features is one of the greatest advantages
    that *DuckDB* (which is the focus of this post) and *Arrow Data Fusion* bring
    with them. It's true that in memory DBs are not something new but the game changer
    with DuckDB and Arrow Data Fusion is their extensibility, which enables us to
    use extensions that easily add capabilities allowing us to directly read and write
    data in different formats and locations, as well as to do it in scale and fast.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在我看来，这3个特性组合是*DuckDB*（本文的重点）和*Arrow Data Fusion*带来的最大优势之一。虽然内存数据库并不是新事物，但 DuckDB
    和 Arrow Data Fusion 的颠覆性在于它们的可扩展性，这使得我们可以使用扩展轻松地添加能力，直接读取和写入不同格式和位置的数据，并且以规模化和快速的方式进行。
- en: 'Therefore, our service will consist of 3 main components or layers that will
    wrap or encapsulate each other: a low-level data component that will encapsulate
    a DuckDB connection (to which I shall refer as a *DataDriver*), a *DAO* component
    that will use the driver to execute queries and handle API requests, and an API
    resolver that will serve it.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们的服务将由3个主要组件或层组成，这些组件或层将互相包装或封装：一个低级数据组件，它将封装一个 DuckDB 连接（我将其称为*DataDriver*），一个*DAO*组件，它将使用驱动程序来执行查询和处理
    API 请求，以及一个提供服务的 API 解析器。
- en: '![](../Images/84ad378e76e8cd9b340e65c5b9adf94f.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/84ad378e76e8cd9b340e65c5b9adf94f.png)'
- en: 'In other words, in terms of dependencies and their relations, we have the following
    structure:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，依赖关系及其关系方面，我们有以下结构：
- en: '`**API-Resolver** *encapsulates* a **DAO struct** *encapsulates* a **DataDriver**
    **struct** *encapsulates* a **DuckDB Connection**`'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '`**API-Resolver** *封装* 一个 **DAO结构体** *封装* 一个 **DataDriver** **结构体** *封装* 一个
    **DuckDB连接**`'
- en: The next section will start by focusing on the lower layers (the DAO struct
    and DataDriver) while the section that follows will discuss the top API layer
    and how it brings it all together.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 下一节将重点讨论较低层（DAO结构体和DataDriver），而接下来的部分将讨论顶部API层以及它如何将所有内容整合在一起。
- en: 3\. Data Loading and Query with DuckDB
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 3\. 数据加载和DuckDB查询
- en: In this section, we are going to create the driver component that wraps a DuckDB
    connection. It will take care of initializing DuckDB and expose an interface to
    execute SQL statements and queries. We will use the excellent [go-duckdb](https://github.com/marcboeker/go-duckdb)
    library, which provides a *database/sql* interface for DuckDB by statically linking
    to its C lib.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们将创建一个封装DuckDB连接的驱动程序组件。它将负责初始化DuckDB，并暴露一个接口来执行SQL语句和查询。我们将使用优秀的[go-duckdb](https://github.com/marcboeker/go-duckdb)库，它通过静态链接到其C库，为DuckDB提供了一个*database/sql*接口。
- en: Initializing sql.DB Connection to DuckDB
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 初始化sql.DB连接到DuckDB
- en: 'As mentioned, we will wrap the *sql.DB* interface provided by the go-duckdb
    library with a struct named *DuckDBDriver* that will take care of initializing
    it properly. We initialize DuckDB by executing a number of initialization statements
    (`bootQueries` ) using a Connector object. The connector executes statements that
    set AWS credentials (as we want to load data from S3), as well as loading and
    installing the extensions that are required by our service: the *parquet* extension
    (for reading parquet files) and *httpfs* (for directly reading from HTTP-based
    object storage such as S3).'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，我们将用一个名为*DuckDBDriver*的结构体封装*sql.DB*接口，该结构体将负责正确初始化它。我们通过使用Connector对象执行多个初始化语句（`bootQueries`）来初始化DuckDB。该连接器执行设置AWS凭证的语句（因为我们要从S3加载数据），以及加载和安装我们的服务所需的扩展：*parquet*扩展（用于读取parquet文件）和*httpfs*（用于直接从基于HTTP的对象存储（如S3）中读取数据）。
- en: 'As shown in the code block above, the function *getBootQueries*() simply returns
    a collection of init statements as strings (you can see the statements [here](https://github.com/a-agmon/gql-parquet-api/blob/757bef6325899b6669baef414335549bbd814e0a/pkg/data/duckdb.go#L18)).
    The init statements are executed by the connector, so that when we call *OpenDB*()
    we get a DuckDB as an sql.DB connection loaded with the required extensions and
    secrets. Because go-duckdb provides a *database/sql* interface to DuckDB connection
    then its main query functionality can be implemented and exposed quite simply
    as:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 如上面的代码块所示，函数*getBootQueries*()简单地返回一组初始化语句作为字符串（你可以[在这里](https://github.com/a-agmon/gql-parquet-api/blob/757bef6325899b6669baef414335549bbd814e0a/pkg/data/duckdb.go#L18)查看这些语句）。初始化语句由连接器执行，因此当我们调用*OpenDB*()时，我们会得到一个已加载所需扩展和秘密的DuckDB作为sql.DB连接。由于go-duckdb为DuckDB连接提供了*database/sql*接口，因此其主要查询功能可以相当简单地实现和暴露：
- en: As mentioned, the DuckDB data driver struct will simply act as a utility class
    to wrap the connection to DuckDB’s DB, while all query executions will be effectively
    managed by a DAO struct that will have a set of biz logic functions that use the
    driver’s methods. The DAO struct will in turn wrap the DuckDBDriver struct.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，DuckDB数据驱动程序结构体将简单地充当一个实用类，用于封装到DuckDB数据库的连接，而所有查询执行将由一个DAO结构体有效管理，该结构体将拥有一组使用驱动程序方法的业务逻辑函数。DAO结构体将进一步封装DuckDBDriver结构体。
- en: Loading Cached Data
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 加载缓存数据
- en: In the final stage of the initialization of the service’s data back-end , we
    load the data we want to serve from parquet files in S3 into a memory table. To
    do so, we will use our driver’*s execute*() function with a CTAS query, that will
    create a named table using any kind of transformation we can express in SQL, from
    a *read_parquet()* function. An example will make this clearer.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在服务数据后端初始化的最后阶段，我们将把我们想要提供的数据从S3中的parquet文件加载到内存表中。为此，我们将使用驱动程序的*execute*()函数和CTAS查询，该查询将使用我们可以在SQL中表达的任何变换创建一个命名表，从*read_parquet()*函数获取数据。一个示例将使这点更加清楚。
- en: 'Suppose we have a parquet table that consists of data that describes our *users*.
    We want to create a service that will expose just 3 fields from this parquet table
    for fast API access: *name*, *last_name*, and *age*. We also want to make sure
    that the *age* field will be accessible as an Integer although it is saved in
    the parquet file as a string.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一个包含描述我们 *users* 的数据的 parquet 表。我们想创建一个服务，该服务将仅从此 parquet 表中暴露 3 个字段以便于快速
    API 访问：*name*、*last_name* 和 *age*。我们还希望确保 *age* 字段能够作为整数访问，尽管它在 parquet 文件中以字符串形式保存。
- en: To do this, after our DuckDB driver has been initialized with the required extensions,
    and the required AWS credentials have been set, we simply execute an SQL statement
    that selects the data we want, directly from parquet files in S3, into memory,
    using the *read_parquet()* function.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，在我们的 DuckDB 驱动程序初始化了所需的扩展，并设置了所需的 AWS 凭证后，我们只需执行一个 SQL 语句，将我们想要的数据直接从 S3
    中的 parquet 文件加载到内存中，使用 *read_parquet()* 函数。
- en: '[PRE0]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: In this statement, we essentially create an in-memory table named *Users* that
    consists of the fields we select from the parquet files in the location given
    to the *read_parquet()* function. We could use any SQL function and syntax that
    DuckDB supports, including complex query statements and aggregations. Here is
    a fuller example of how we use this approach to initialize our service.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个语句中，我们基本上创建了一个名为 *Users* 的内存表，它由我们从 parquet 文件中选择的字段组成，这些文件的位置在 *read_parquet()*
    函数中给出。我们可以使用 DuckDB 支持的任何 SQL 函数和语法，包括复杂的查询语句和聚合。以下是如何使用这种方法初始化我们的服务的更完整示例。
- en: This is actually the core of the this service — when the service is created
    and booting up, its executing a statement that essentially loads data from S3
    to its memory
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 这实际上是该服务的核心——当服务创建并启动时，它执行的语句基本上是从 S3 加载数据到其内存中。
- en: After the service has been initialized and the data has been loaded, we can
    execute any SQL query we need directly on the memory table, represented by our
    data driver , in order to serve our API with sub-second latency.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在服务初始化并加载数据后，我们可以直接在内存表上执行所需的任何 SQL 查询，由我们的数据驱动程序表示，以便以子秒级延迟服务我们的 API。
- en: 4\. Serving GraphQL
  id: totrans-46
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 4\. 提供 GraphQL 服务
- en: Now that we have a connection to a memory table loaded with cached parquet data,
    the last step would be to create a GraphQL endpoint over it in order to efficiently
    answer data queries. To do so, we are going to use the library [*gqlgen* by 99designs](http://github.com/99designs/gqlgen),
    which makes this task rather straightforward.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经连接到一个加载了缓存 parquet 数据的内存表，最后一步是创建一个 GraphQL 端点，以便高效地响应数据查询。为此，我们将使用库 [*gqlgen*
    by 99designs](http://github.com/99designs/gqlgen)，它使这项任务变得相当简单。
- en: An in-depth introduction to *gqlgen* is, unfortunately, beyond my scope here.
    Interested readers that are less familiar with GraphQL are encouraged to skim
    its [documentation](http://gqlgen.com) which is very clearly presented and explained.
    However, I believe that some familiarity with GraphQL concepts is sufficient to
    follow this section and get the idea.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 对 *gqlgen* 的深入介绍，不幸的是超出了我的范围。对于那些对 GraphQL 不太熟悉的读者，建议浏览其 [documentation](http://gqlgen.com)，文档非常清晰明了。然而，我相信对
    GraphQL 概念的一些熟悉即可跟随本节内容并理解要点。
- en: 'Exposing a GraphQL endpoint using gqlgen typically involves 3 main steps: (1)
    creating a schema, (2) generating resolver code and stubs, and (3) adding resolver
    code that implements the API functions.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 gqlgen 暴露一个 GraphQL 端点通常涉及 3 个主要步骤：（1）创建架构，（2）生成解析器代码和存根，（3）添加实现 API 函数的解析器代码。
- en: 'We are going to start with this schema that describes a User in our table and
    2 main functions to fetch users data: a general fetch-all, and a fetch by email.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从描述我们表中一个用户的架构开始，并提供两个主要函数来获取用户数据：一个通用的获取所有功能和一个按邮箱获取功能。
- en: '[PRE1]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'After creating our schema, we call gqlgen code generation procedure in our
    project directory:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 创建架构后，我们在项目目录中调用 gqlgen 代码生成过程：
- en: '[PRE2]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Running the *generate* procedure will generate a lot of code, including the
    actual *User* struct (our data model struct), a corresponding *resolver* deceleration
    template and the *resolver implementation*.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 运行 *generate* 过程将生成大量代码，包括实际的 *User* 结构（我们的数据模型结构）、相应的 *resolver* 声明模板和 *resolver
    implementation*。
- en: Lets discuss them in order.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们按顺序讨论这些内容。
- en: The resolver struct was generated in a file named *resolver.go* with just 2
    statements — a struct type declaration with no properties or members, and a constructor
    ( a new() method) that will initialize it. As we shall shortly see, the resolver
    is our API serving layer that implements a function for each of the API’s methods.
    The purpose of the *resolver.go* file is for us to inject any required dependency
    to the resolver or add to it whatever we need to serve queries for our API. Recall
    that this is exactly the purpose of our DAO struct. Our DAO struct wraps the DuckDB
    data driver which holds a connection to our in memory tables, and is in charge
    on “translating” API requests for data to sql queries. Therefore, we simply *inject*
    an initialized DAO object to the resolver, so that the resolver can use it to
    execute queries.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 解析器结构体是在一个名为*resolver.go*的文件中生成的，只有2个语句——一个没有属性或成员的结构体类型声明和一个将初始化它的构造函数（一个new()方法）。正如我们将很快看到的那样，解析器是我们的API服务层，它为API的每个方法实现一个函数。*resolver.go*文件的目的是为了让我们向解析器注入任何所需的依赖项，或向其中添加任何我们需要的内容来处理API的查询。请记住，这正是我们DAO结构体的目的。我们的DAO结构体包装了DuckDB数据驱动程序，它持有对内存表的连接，并负责将API数据请求“翻译”为SQL查询。因此，我们只需*注入*一个初始化的DAO对象到解析器中，以便解析器可以使用它来执行查询。
- en: '[PRE3]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The next file that was generated (and will be re-generated every time we run
    *gqlgen generate* procedure) is *schema.resolvers.go* which is the implementation
    of the resolver’s methods. The generated *schema.resolvers* file essentially contains
    the method signatures of the API functions declared in the schema. In our case,
    it will therefore consist of the 2 methods
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 生成的下一个文件（每次我们运行*gqlgen generate*过程时都会重新生成）是*schema.resolvers.go*，它是解析器方法的实现。生成的*schema.resolvers*文件本质上包含了在模式中声明的API函数的方法签名。在我们的例子中，它将包括这2个方法。
- en: '[PRE4]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: To implement these functions, we first need to have the corresponding method
    in our DAO struct, but lets first implement one for the sake of example, and then
    complete the required DAO code.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现这些函数，我们首先需要在DAO结构体中拥有相应的方法，但为了举例说明，让我们先实现一个，然后完成所需的DAO代码。
- en: '[PRE5]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: As you can see, because our DAO was injected to the resolver struct, we can
    simply call its functions using our resolver. This structure makes the API layer’s
    code very clean and simple.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所见，由于我们的DAO被注入到解析器结构体中，我们可以简单地通过我们的解析器调用其函数。这种结构使得API层的代码非常干净和简单。
- en: Now let’s write the actual implementation of the required function in the DAO
    struct. As you can see below, the required code is fairly simple. Although I’m
    using some helper functions (that you can see in the companion github repo) the
    *GetUsers()* function simply executes an SQL query on our in-mem DuckDB table
    and builds a list of users (recall the model.User struct was generated by gqlgen
    using our schema).
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们编写DAO结构体中所需函数的实际实现。正如你所见，所需的代码非常简单。虽然我使用了一些辅助函数（你可以在配套的github repo中看到），但*GetUsers()*函数只是对我们内存中的DuckDB表执行一个SQL查询，并构建一个用户列表（请记住，model.User结构体是由gqlgen使用我们的模式生成的）。
- en: '[PRE6]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Now we essentially have all the required layers that we need to chain together.
    That is, a *data driver* struct (that encapsulates a db connection), which is
    injected into a *DAO struct* that implements and serves as an interface for all
    the required API functions, which are invoked by the resolver — our API handler.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们基本上拥有了我们需要链在一起的所有层。也就是说，一个*数据驱动程序*结构体（封装了数据库连接），它被注入到一个*DAO结构体*中，该结构体实现并作为所有所需API函数的接口，由解析器——我们的API处理程序调用。
- en: The relationship between the components and role is very clearly expressed in
    the way they are chained together in the main *server.go* file that bootstraps
    our service and its dependencies.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 组件和角色之间的关系通过它们在主*server.go*文件中的链式连接表达得非常清楚，该文件引导我们的服务及其依赖项。
- en: '[PRE7]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: When our service is initialized, we first initialize our driver which gets a
    connection to our DuckDB in-mem store. Next, we inject the driver to the *NewStore*
    method that creates a DAO and uses the driver to load the data from the parquet
    files to memory. Finally, we inject the DAO struct to the API handler that invokes
    its function when serving API requests.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们的服务被初始化时，我们首先初始化我们的驱动程序，它获取与DuckDB内存存储的连接。接下来，我们将驱动程序注入到*NewStore*方法中，该方法创建一个DAO并使用驱动程序将数据从parquet文件加载到内存中。最后，我们将DAO结构体注入到API处理程序中，当服务API请求时它会调用其函数。
- en: 5\. Conclusion
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5\. 结论
- en: The purpose of this post was to offer an alternative approach toward enabling
    HTTP API access to data lake tables for thin clients. This use case is getting
    increasingly common and usually requires adding a number of moving parts, monitoring,
    and resources to our pipeline. In this post I proposed a simpler alternative,
    which I believe can fit many uses cases. I demonstrated how we can use DuckDB’s
    query performance and extensions in order to enable our service to load data from
    remote object storage, save it in an in-memory relational table *and* enable us
    to query it with sub-second latency. More generally, I tried to give an example
    of the great capabilities that DuckDB’s extensions can bring to our services as
    well as the ease in which in can be embedded.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 这篇文章的目的是提供一种替代方法，使薄客户端能够访问数据湖表的 HTTP API。这个用例越来越普遍，通常需要在我们的管道中添加许多动态组件、监控和资源。在这篇文章中，我提出了一种更简单的替代方案，我相信它适用于许多用例。我演示了如何利用
    DuckDB 的查询性能和扩展来使我们的服务能够从远程对象存储中加载数据，将其保存在内存中的关系表中，并使我们能够以亚秒级延迟进行查询。更一般地说，我试图举例说明
    DuckDB 扩展为我们的服务带来的强大能力以及它如何容易嵌入。
- en: Hope this will be useful!
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 希望这会对你有用！
- en: The companion github repo with an example code can be found [here](https://github.com/a-agmon/gql-parquet-api)
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 附带的 GitHub 仓库及示例代码可以在[这里](https://github.com/a-agmon/gql-parquet-api)找到。
- en: A github repo with an implementation of the same concept using Rust and Arrow
    Data Fusion can be found [here](https://github.com/a-agmon/rs-parquet-gql).
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Rust 和 Arrow Data Fusion 实现相同概念的 GitHub 仓库可以在[这里](https://github.com/a-agmon/rs-parquet-gql)找到。
- en: '** *All images, unless otherwise noted, are by the author*'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '** *所有图像，除非另有说明，均由作者提供*'
