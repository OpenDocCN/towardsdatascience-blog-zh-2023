- en: How Prejudice Creeps into AI Systems
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/how-prejudice-creeps-into-ai-systems-3673646ae8e3?source=collection_archive---------8-----------------------#2023-02-03](https://towardsdatascience.com/how-prejudice-creeps-into-ai-systems-3673646ae8e3?source=collection_archive---------8-----------------------#2023-02-03)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Where do AI biases actually originate from?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@boris-ruf?source=post_page-----3673646ae8e3--------------------------------)[![Boris
    Ruf](../Images/96dc4fc2f32add89fef6911195590cd8.png)](https://medium.com/@boris-ruf?source=post_page-----3673646ae8e3--------------------------------)[](https://towardsdatascience.com/?source=post_page-----3673646ae8e3--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----3673646ae8e3--------------------------------)
    [Boris Ruf](https://medium.com/@boris-ruf?source=post_page-----3673646ae8e3--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fed341456850c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-prejudice-creeps-into-ai-systems-3673646ae8e3&user=Boris+Ruf&userId=ed341456850c&source=post_page-ed341456850c----3673646ae8e3---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----3673646ae8e3--------------------------------)
    ·4 min read·Feb 3, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F3673646ae8e3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-prejudice-creeps-into-ai-systems-3673646ae8e3&user=Boris+Ruf&userId=ed341456850c&source=-----3673646ae8e3---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F3673646ae8e3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-prejudice-creeps-into-ai-systems-3673646ae8e3&source=-----3673646ae8e3---------------------bookmark_footer-----------)![](../Images/68abad72609d8df6d60b05d3f299ac8f.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Photo by [Lucas Benjamin](https://unsplash.com/@aznbokchoy?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: '*One challenge for systems powered with artificial intelligence (AI) is the
    biases that may be embedded in the algorithms. In* [*my previous article*](/biased-ai-a-look-under-the-hood-5d0a41968f16)*,
    I explained the inner processes which take place when AI goes rogue. In the following,
    I will deepen the question where these biases actually come from — and how those
    sources are different from well-studied bias problems in conventional technologies.*'
  prefs: []
  type: TYPE_NORMAL
- en: Machine learning (ML) algorithms identify patterns in data. Their major strength
    is the desired capability to find and discriminate classes in training data, and
    to use those insights to make predictions for new, unseen data.
  prefs: []
  type: TYPE_NORMAL
- en: In the era of “big data”, a large quantity of data is available, with all sorts
    of variables. The general assumption is that the more data is used, the more precise
    the algorithm and its predictions become. When using a large amount of data, it
    clearly contains many correlations. However, not all correlations imply causation.
    No matter how large the dataset is, it still only remains a snapshot of reality.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s take an example:'
  prefs: []
  type: TYPE_NORMAL
- en: In a training data set on claims of a car insurance, red cars may have caused
    more accidents than cars of another colour. The ML algorithm detects this correlation.
    However, there is no scientific proof of causation between the colour of a car
    and the risk of accidents.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Solely for the sake of the algorithm’s performance it is crucial to notice and
    eliminate this kind of unwanted correlations. Otherwise, the algorithm is biased
    and results on new data in production may be poor. In the case of the example,
    a competitor with a better algorithm, which does not falsely attribute a higher
    risk to drivers of red cars, can offer a lower price to those customers and entice
    them away.
  prefs: []
  type: TYPE_NORMAL
- en: Beside the performance aspect, there is a second, even more severe problem which
    appears when the predictions impact people, and when the algorithm is biased to
    favour privileged groups over unprivileged groups, resulting in discrimination.
  prefs: []
  type: TYPE_NORMAL
- en: It is important to note that these unwanted discriminations may happen without
    explicitly providing sensitive personal data. In fact, other attributes can implicitly
    reveal this information serving as proxy. For example, a car model can hint at
    the owner’s gender, or the zip code may correlate with a resident’s ethnicity
    or religion.
  prefs: []
  type: TYPE_NORMAL
- en: Where does it come from?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The problem of bias is obviously not new. Even before the emergence of AI, [plenty
    of different forms of bias](https://en.wikipedia.org/wiki/List_of_cognitive_biases)
    were known to possibly cause unwanted and unexpected results in technical systems.
    Automation bias for example is what happens when people trust suggestions of automated
    systems over human reasoning. [Several severe airplane accidents happened](https://www.newyorker.com/science/maria-konnikova/hazards-automation)
    in the past because the pilots had trusted the autopilot over their own judgment.
    Another type of bias may occur when an algorithm is deployed in an environment
    for which it was not trained in the first place. For example, if it is applied
    in a different geographical region or on a different group of people.
  prefs: []
  type: TYPE_NORMAL
- en: While explicitly programmed rules in algorithms or the way they are used in
    practice may produce biased results, this is a long-known problem which already
    applies to conventional deterministic algorithms. In this article, I focus on
    the **new sources of bias**, which gain in importance with the rise of machine
    learning technologies. More specifically, I discuss human bias in data and selection
    bias.
  prefs: []
  type: TYPE_NORMAL
- en: Human bias
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The first source of bias which comes naturally to mind is human bias. Different
    types of this kind of well-studied bias are outlined below.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f9710dec7d1831383ee2ab85ed2521f9.png)'
  prefs: []
  type: TYPE_IMG
- en: Different sources of human bias
  prefs: []
  type: TYPE_NORMAL
- en: Training data can consist of labels of objective observations, as for instance
    coming from a measuring device. However, training data may also involve human
    assessment. Data labels which include human judgment may have been labelled with
    prejudice. Since the labels serve as *ground truth*, the algorithm’s performance
    directly depends on them, and any bias contained gets reproduced at scale in the
    model. So basically, a well-studied form of bias finds its way into a new technology
    here.
  prefs: []
  type: TYPE_NORMAL
- en: Selection bias
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Another, less obvious source of bias is the process of how the data are collected.
    If the data do not reflect the real distribution, a ML algorithm which is using
    it for training will learn and enforce the bias. The table below provides a list
    of different types of biases which may cause selection bias in data.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/34af303bfe8f31aa1c978efd7dd28816.png)'
  prefs: []
  type: TYPE_IMG
- en: Different sources of selection bias
  prefs: []
  type: TYPE_NORMAL
- en: So what
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ML algorithms are strongly dependent on the data they use to create the predictive
    model. These training data may be biased, in particular in form of human biases
    or selection biases. Because the algorithms are prone to any such effects, and
    due to the potential of getting deployed at scale, even minimal systematic errors
    in the algorithms can lead to reinforced discrimination.
  prefs: []
  type: TYPE_NORMAL
- en: '*Many thanks to Antoine Pietri for his valuable support in writing this post.
    In* [*my next article*](https://medium.com/towards-data-science/to-guarantee-impartial-ai-decisions-lady-justice-needs-to-blink-2992167b2591)*,
    I explain why deleting the sensitive attributes is not a simple solution to AI
    fairness.*'
  prefs: []
  type: TYPE_NORMAL
