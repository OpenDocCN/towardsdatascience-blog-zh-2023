["```py\nimport tensorflow as tf\nimport tensorflow.experimental.numpy.isclose\n\nTOL = 1e-5\n\ndef compute_loss(self, x, y, u, dudxx, dudyy, eval=False):\n    \"\"\"\n    Computes the physics-informed loss for Helmholtz's PDE.\n\n    Parameters\n    ----------\n    x : tf.Tensor of shape (batch_size, 1)\n        x coordinate of the points in the current batch\n    y : tf.Tensor of shape (batch_size, 1)\n        y coordinate of the points in the current batch\n    u : tf.Tensor of shape (batch_size, 1)\n        predictions made by our PINN (dim 0)\n    dudxx : tf.Tensor of shape (batch_size, 1)\n        second-order derivative of the predictions w.r.t. x\n    dudyy : tf.Tensor of shape (batch_size, 1)\n        second-order derivative of the predictions w.r.t. y\n    \"\"\"\n\n    # governing equation loss\n    L_f = (dudxx + dudyy + self.k**2 * u - \\\n          (-np.pi**2 - (4 * np.pi)**2 + self.k**2) * tf.math.sin(np.pi * x) * tf.math.sin(4 * np.pi * y))**2\n\n    # determine which points are on the boundaries of the domain\n    # if a point is on either of the boundaries, its value is 1 and 0 otherwise\n    x_lower = tf.cast(isclose(x, -1, rtol=0., atol=EPS), dtype=tf.float32)\n    x_upper = tf.cast(isclose(x,  1, rtol=0., atol=EPS), dtype=tf.float32)\n    y_lower = tf.cast(isclose(y, -1, rtol=0., atol=EPS), dtype=tf.float32)\n    y_upper = tf.cast(isclose(y,  1, rtol=0., atol=EPS), dtype=tf.float32)\n\n    # compute 0th order boundary condition loss\n    L_b = ((x_lower + x_upper + y_lower + y_upper) * u)**2\n\n    if eval:\n        L_u = (tf.math.sin(np.pi*x) * tf.math.sin(4*np.pi*y) - u)**2\n        return L_f, L_b, L_u\n\n    return L_f, L_b\n```", "```py\nimport tensorflow as tf\n\nclass ReLoBRaLoLoss(tf.keras.losses.Loss):\n    \"\"\"\n    Class for the ReLoBRaLo Loss. \n    This class extends the keras Loss class to have dynamic weighting for each term.\n    \"\"\"\n    def __init__(self, pde:HelmholtzPDE, alpha:float=0.999, temperature:float=0.1, rho:float=0.99,\n                 name='ReLoBRaLoLoss', **kwargs):\n        \"\"\"\n        Parameters\n        ----------\n        pde : PDE\n            An instance of a PDE class containing the PDE-specific `compute_loss` function.\n        alpha, optional : float\n            Controls the exponential weight decay rate. \n            Value between 0 and 1\\. The smaller, the more stochasticity.\n            0 means no historical information is transmitted to the next iteration.\n            1 means only first calculation is retained. Defaults to 0.999.\n        temperature, optional : float\n            Softmax temperature coefficient. Controlls the \"sharpness\" of the softmax operation. \n            Defaults to 0.1.\n        rho, optional : float\n            Probability of the Bernoulli random variable controlling the frequency of random lookbacks.\n            Value berween 0 and 1\\. The smaller, the fewer lookbacks happen.\n            0 means lambdas are always calculated w.r.t. the initial loss values.\n            1 means lambdas are always calculated w.r.t. the loss values in the previous training iteration.\n            Defaults to 0.99.\n        \"\"\"\n        super().__init__(name=name, **kwargs)\n        self.pde = pde\n        self.alpha = alpha\n        self.temperature = temperature\n        self.rho = rho\n        self.call_count = tf.Variable(0, trainable=False, dtype=tf.int16)\n\n        self.lambdas = [tf.Variable(1., trainable=False) for _ in range(pde.num_terms)]\n        self.last_losses = [tf.Variable(1., trainable=False) for _ in range(pde.num_terms)]\n        self.init_losses = [tf.Variable(1., trainable=False) for _ in range(pde.num_terms)]\n\n    def call(self, xy, preds):\n        x, y = xy[:, :1], xy[:, 1:]\n\n        # obtain the unscaled values for each term \n        losses = [tf.reduce_mean(loss) for loss in self.pde.compute_loss(x, y, preds)]\n\n        # in first iteration (self.call_count == 0), drop lambda_hat and use init lambdas, i.e. lambda = 1\n        #   i.e. alpha = 1 and rho = 1\n        # in second iteration (self.call_count == 1), drop init lambdas and use only lambda_hat\n        #   i.e. alpha = 0 and rho = 1\n        # afterwards, default procedure (see paper)\n        #   i.e. alpha = self.alpha and rho = Bernoully random variable with p = self.rho\n        alpha = tf.cond(tf.equal(self.call_count, 0), \n                lambda: 1., \n                lambda: tf.cond(tf.equal(self.call_count, 1), \n                                lambda: 0., \n                                lambda: self.alpha))\n        rho = tf.cond(tf.equal(self.call_count, 0), \n              lambda: 1., \n              lambda: tf.cond(tf.equal(self.call_count, 1), \n                              lambda: 1., \n                              lambda: tf.cast(tf.random.uniform(shape=()) < self.rho, dtype=tf.float32)))\n\n        # compute new lambdas w.r.t. the losses in the previous iteration\n        lambdas_hat = [losses[i] / (self.last_losses[i] * self.temperature + EPS) for i in range(len(losses))]\n        lambdas_hat = tf.nn.softmax(lambdas_hat - tf.reduce_max(lambdas_hat)) * tf.cast(len(losses), dtype=tf.float32)\n\n        # compute new lambdas w.r.t. the losses in the first iteration\n        init_lambdas_hat = [losses[i] / (self.init_losses[i] * self.temperature + EPS) for i in range(len(losses))]\n        init_lambdas_hat = tf.nn.softmax(init_lambdas_hat - tf.reduce_max(init_lambdas_hat)) * tf.cast(len(losses), dtype=tf.float32)\n\n        # use rho for deciding, whether a random lookback should be performed\n        new_lambdas = [(rho * alpha * self.lambdas[i] + (1 - rho) * alpha * init_lambdas_hat[i] + (1 - alpha) * lambdas_hat[i]) for i in range(len(losses))]\n        self.lambdas = [var.assign(tf.stop_gradient(lam)) for var, lam in zip(self.lambdas, new_lambdas)]\n\n        # compute weighted loss\n        loss = tf.reduce_sum([lam * loss for lam, loss in zip(self.lambdas, losses)])\n\n        # store current losses in self.last_losses to be accessed in the next iteration\n        self.last_losses = [var.assign(tf.stop_gradient(loss)) for var, loss in zip(self.last_losses, losses)]\n        # in first iteration, store losses in self.init_losses to be accessed in next iterations\n        first_iteration = tf.cast(self.call_count < 1, dtype=tf.float32)\n        self.init_losses = [var.assign(tf.stop_gradient(loss * first_iteration + var * (1 - first_iteration))) for var, loss in zip(self.init_losses, losses)]\n\n        self.call_count.assign_add(1)\n\n        return loss\n```"]