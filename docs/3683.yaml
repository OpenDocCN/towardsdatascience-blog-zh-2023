- en: SW/HW Co-optimization Strategy for Large Language Models (LLMs)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/sw-hw-co-optimization-strategy-for-large-language-models-llms-855f20a14629?source=collection_archive---------3-----------------------#2023-12-16](https://towardsdatascience.com/sw-hw-co-optimization-strategy-for-large-language-models-llms-855f20a14629?source=collection_archive---------3-----------------------#2023-12-16)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: How to stretch every bit out of your system to run LLMs faster? — best practice
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@LizLiAI?source=post_page-----855f20a14629--------------------------------)[![Liz
    Li](../Images/78846add1618c8c095dd97adeca87f81.png)](https://medium.com/@LizLiAI?source=post_page-----855f20a14629--------------------------------)[](https://towardsdatascience.com/?source=post_page-----855f20a14629--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----855f20a14629--------------------------------)
    [Liz Li](https://medium.com/@LizLiAI?source=post_page-----855f20a14629--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fdc3f793d765a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsw-hw-co-optimization-strategy-for-large-language-models-llms-855f20a14629&user=Liz+Li&userId=dc3f793d765a&source=post_page-dc3f793d765a----855f20a14629---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----855f20a14629--------------------------------)
    ·5 min read·Dec 16, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F855f20a14629&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsw-hw-co-optimization-strategy-for-large-language-models-llms-855f20a14629&user=Liz+Li&userId=dc3f793d765a&source=-----855f20a14629---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F855f20a14629&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsw-hw-co-optimization-strategy-for-large-language-models-llms-855f20a14629&source=-----855f20a14629---------------------bookmark_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: Leading Large Language Models (LLMs) like ChatGPT, Llama, etc. are revolutionizing
    the tech industry and impacting everyone’s lives. However, their cost poses a
    significant hurdle. Applications utilizing OpenAI APIs incur substantial expenses
    for continuous operation ($0.03 per 1,000 prompt tokens and $0.06 per 1,000 sampled
    tokens).
  prefs: []
  type: TYPE_NORMAL
- en: To cut costs, companies tend to host their own LLMs, with expenses varying widely
    based on model size (larger LLMs with 100–200B parameters can cost ~10 times more
    compared to smaller ones with 7–15B parameters). This trend has spurred the AI
    chip race, as major tech companies aim to develop their own AI chips, reducing
    reliance on expensive hardware.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c20161743022e1acac02f5a2f4dd573f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Trend of model size. Source: AWS reInvent'
  prefs: []
  type: TYPE_NORMAL
- en: How to squeeze every bit of computing power to run LLMs? In this article, I
    am going to do a thorough analysis of LLM optimization strategy across models,
    software, and hardware. It follows the [AI SW/HW co-design methodology](/how-to-co-design-software-hardware-architecture-for-ai-ml-in-a-new-era-b296f2842fe2)
    I wrote in previous article, with much more in-depth discussion on LLM-specific
    cost and performance optimization.
  prefs: []
  type: TYPE_NORMAL
