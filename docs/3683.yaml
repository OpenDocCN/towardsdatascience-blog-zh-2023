- en: SW/HW Co-optimization Strategy for Large Language Models (LLMs)
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/sw-hw-co-optimization-strategy-for-large-language-models-llms-855f20a14629?source=collection_archive---------3-----------------------#2023-12-16](https://towardsdatascience.com/sw-hw-co-optimization-strategy-for-large-language-models-llms-855f20a14629?source=collection_archive---------3-----------------------#2023-12-16)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: How to stretch every bit out of your system to run LLMs faster? — best practice
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@LizLiAI?source=post_page-----855f20a14629--------------------------------)[![Liz
    Li](../Images/78846add1618c8c095dd97adeca87f81.png)](https://medium.com/@LizLiAI?source=post_page-----855f20a14629--------------------------------)[](https://towardsdatascience.com/?source=post_page-----855f20a14629--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----855f20a14629--------------------------------)
    [Liz Li](https://medium.com/@LizLiAI?source=post_page-----855f20a14629--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fdc3f793d765a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsw-hw-co-optimization-strategy-for-large-language-models-llms-855f20a14629&user=Liz+Li&userId=dc3f793d765a&source=post_page-dc3f793d765a----855f20a14629---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----855f20a14629--------------------------------)
    ·5 min read·Dec 16, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F855f20a14629&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsw-hw-co-optimization-strategy-for-large-language-models-llms-855f20a14629&user=Liz+Li&userId=dc3f793d765a&source=-----855f20a14629---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F855f20a14629&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsw-hw-co-optimization-strategy-for-large-language-models-llms-855f20a14629&source=-----855f20a14629---------------------bookmark_footer-----------)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: Leading Large Language Models (LLMs) like ChatGPT, Llama, etc. are revolutionizing
    the tech industry and impacting everyone’s lives. However, their cost poses a
    significant hurdle. Applications utilizing OpenAI APIs incur substantial expenses
    for continuous operation ($0.03 per 1,000 prompt tokens and $0.06 per 1,000 sampled
    tokens).
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: To cut costs, companies tend to host their own LLMs, with expenses varying widely
    based on model size (larger LLMs with 100–200B parameters can cost ~10 times more
    compared to smaller ones with 7–15B parameters). This trend has spurred the AI
    chip race, as major tech companies aim to develop their own AI chips, reducing
    reliance on expensive hardware.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 为了降低成本，公司倾向于托管自己的LLMs，费用根据模型大小差异很大（较大的LLMs，参数为100–200B，相较于参数为7–15B的小型模型，成本可能高出~10倍）。这一趋势催生了AI芯片竞赛，因为主要科技公司旨在开发自己的AI芯片，从而减少对昂贵硬件的依赖。
- en: '![](../Images/c20161743022e1acac02f5a2f4dd573f.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c20161743022e1acac02f5a2f4dd573f.png)'
- en: 'Trend of model size. Source: AWS reInvent'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 模型大小的趋势。来源：AWS reInvent
- en: How to squeeze every bit of computing power to run LLMs? In this article, I
    am going to do a thorough analysis of LLM optimization strategy across models,
    software, and hardware. It follows the [AI SW/HW co-design methodology](/how-to-co-design-software-hardware-architecture-for-ai-ml-in-a-new-era-b296f2842fe2)
    I wrote in previous article, with much more in-depth discussion on LLM-specific
    cost and performance optimization.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 如何充分挖掘每一份计算能力来运行LLMs？在这篇文章中，我将对LLM优化策略进行全面分析，涵盖模型、软件和硬件。它遵循我在上一篇文章中写的[AI SW/HW协同设计方法](https://example.org/how-to-co-design-software-hardware-architecture-for-ai-ml-in-a-new-era-b296f2842fe2)，并对LLM特定的成本和性能优化进行了更深入的讨论。
