["```py\ngcloud compute instances create test-vm \\\n    --zone=us-east1-d \\\n    --machine-type=g2-standard-48 \\\n    --image-family=common-cu121-debian-11-py310 \\\n    --image-project=deeplearning-platform-release \\\n    --service-account=my-account@my-project.iam.gserviceaccount.com \\\n    --maintenance-policy=TERMINATE\n```", "```py\ngcloud compute instances create test-vm \\\n    --zone=us-east1-d \\\n    --machine-type=g2-standard-48 \\\n    --image-family=common-cu121-debian-11-py310 \\\n    --image-project=deeplearning-platform-release \\\n    --service-account=my-account@my-project.iam.gserviceaccount.com \\\n    --maintenance-policy=TERMINATE \\\n    --metadata=startup-script='#! /bin/bash \\\n    export PATH=\"/opt/conda/bin:$PATH\" \\\n    gsutil cp gs://my-bucket/test-vm/my-code.tar . \\\n    tar -xvf my-code.tar \\\n    python3 -m pip install -r requirements.txt \\\n    python3 train.py'\n```", "```py\ngcloud compute instances create test-vm \\\n    --zone=us-east1-d \\\n    --machine-type=g2-standard-48 \\\n    --image-family=common-cu121-debian-11-py310 \\\n    --image-project=deeplearning-platform-release \\\n    --service-account=my-account@my-project.iam.gserviceaccount.com \\\n    --maintenance-policy=TERMINATE \\\n    --scopes=https://www.googleapis.com/auth/cloud-platform \\\n    --metadata=startup-script='#! /bin/bash \\\n    export PATH=\"/opt/conda/bin:$PATH\" \\\n    gsutil cp gs://my-bucket/test-vm/my-code.tar . \\\n    tar -xvf my-code.tar \\\n    python3 -m pip install -r requirements.txt \\\n    python3 train.py \\\n    yes | gcloud compute instances delete $(hostname) --zone=zone=us-east1-d'\n```", "```py\nresource.type=\"gce_instance\"\nresource.labels.instance_id=\"6019608135494011466\"\n```", "```py\ngcloud compute instances describe test-vm \\\n    --zone=us-east1-d --format=\"value(id)\" > test-vm-instance-id\ngsutil cp test-vm-instance-id gs://my-bucket/test-vm/metadata/\n```", "```py\ngsutil cp create-instance.sh gs://my-bucket/test-vm/metadata/\n```", "```py\ngcloud compute instances create test-vm \\\n    --zone=us-east1-d \\\n    --machine-type=g2-standard-48 \\\n    --image-family=common-cu121-debian-11-py310 \\\n    --image-project=deeplearning-platform-release \\\n    --service-account=my-account@my-project.iam.gserviceaccount.com \\\n    --maintenance-policy=TERMINATE \\\n    --scopes=https://www.googleapis.com/auth/cloud-platform \\\n    --metadata=startup-script='#! /bin/bash \\\n    export PATH=\"/opt/conda/bin:$PATH\" \\\n    gsutil cp gs://my-bucket/test-vm/my-code.tar . \\\n    tar -xvf my-code.tar \\\n    python3 -m pip install -r requirements.txt \\\n    python3 train.py \\\n    gsutil -m rsync -r output gs://my-bucket/test-vm/output \\\n    yes | gcloud compute instances delete $(hostname) --zone=zone=us-east1-d'\n```", "```py\ngcloud compute instances bulk create \\\n    --name-pattern=\"test-vm-#\" \\\n    --count=2 \\\n    --region=us-east1 \\\n    --target-distribution-shape=any_single_zone \\\n    --image-family=common-cu121-debian-11-py310 \\\n    --image-project=deeplearning-platform-release \\\n    --service-account=my-account@my-project.iam.gserviceaccount.com \\\n    --on-host-maintenance=TERMINATE \\\n    --scopes=https://www.googleapis.com/auth/cloud-platform \\\n    --metadata=startup-script='#! /bin/bash \\\n    export MASTER_ADDR=test-vm-1 \\\n    export MASTER_PORT=7777 \\\n    export NODES=\"test-vm-1 test-vm-2\" \\\n    export PATH=\"/opt/conda/bin:$PATH\" \\\n    gsutil cp gs://my-bucket/test-vm/my-code.tar . \\\n    tar -xvf my-code.tar \\\n    python3 -m pip install -r requirements.txt \\\n    python3 train.py \\\n    gsutil -m rsync -r output gs://my-bucket/test-vm/output \\\n    HN=\"$(hostname)\" \\\n    ZN=\"$(gcloud compute instances list --filter=name=${HN} --format=\"value(zone)\")\" \\\n    yes | gcloud compute instances delete $HN --zone=${ZN}'\n\ngcloud compute instances describe test-vm-1 \\\n    --zone=us-east1-d --format=\"value(id)\" > test-vm-1-instance-id\ngsutil cp test-vm-1-instance-id gs://my-bucket/test-vm/metadata/\n\ngcloud compute instances describe test-vm-2 \\\n    --zone=us-east1-d --format=\"value(id)\" > test-vm-2-instance-id\ngsutil cp test-vm-2-instance-id gs://my-bucket/test-vm/metadata/\n```", "```py\nimport os, ast, socket\nimport torch\nimport torch.distributed as dist\nimport torch.multiprocessing as mp\n\ndef mp_fn(local_rank, *args):\n    # discover topology settings\n    gpus_per_node = torch.cuda.device_count()\n    nodes = os.environ['NODES'].split()\n    num_nodes = len(nodes)\n    world_size = num_nodes * gpus_per_node\n    node_rank = nodes.index(socket.gethostname())\n    global_rank = (node_rank * gpus_per_node) + local_rank\n    print(f'local rank {local_rank} '\n          f'global rank {global_rank} '\n          f'world size {world_size}')\n    dist.init_process_group(backend='nccl',\n                            rank=global_rank, \n                            world_size=world_size)\n    torch.cuda.set_device(local_rank)\n    # Add training logic\n\nif __name__ == '__main__':\n    mp.spawn(mp_fn,\n             args=(),\n             nprocs=torch.cuda.device_count())\n```", "```py\nimport os\nimport subprocess\nfrom tabulate import tabulate\nfrom google.cloud import storage\n\n# get job list\np=subprocess.run([\"gsutil\",\"ls\",\"gs://my-bucket/\"], capture_output=True)\noutput = p.stdout.decode(\"utf-8\").split(\"\\n\")\njobs = [i for i in output if i.endswith(\"/\")]\n\nstorage_client = storage.Client()\nbucket_name = \"my-bucket\"\nbucket = storage_client.get_bucket(bucket_name)\n\nentries = [] \nfor job in jobs:\n    blob = bucket.blob(f'{job}/metadata/{job}-instance-id')\n    inst_id = blob.download_as_string().decode('utf-8').strip()\n    try:\n        blob = bucket.blob(f'{job}/metadata/status-code')\n        status = blob.download_as_string().decode('utf-8').strip()\n        status = 'SUCCESS' if status==0 else 'FAILED'\n    except:\n        status = 'RUNNING'\n    print(inst_id)\n    entries.append([job,status,inst_id,f'gs://my-bucket/{job}/my-code.tar'])\n\nprint(tabulate(entries,\n            headers=['Job', 'Status', 'Instance/Log ID', 'Code Location']))\n```", "```py\n#!/bin/bash\n\nMY_PROGRAM=\"python\"\n# Find the newest copy of $MY_PROGRAM\nPID=\"$(pgrep -n \"$MY_PROGRAM\")\"\nif [[ \"$?\" -ne 0 ]]; then\n  echo \"${MY_PROGRAM} not running shutting down immediately.\"\n  exit 0\nfi\n\necho \"Termination in progress registering job...\"\ngsutil cp /dev/null gs://my-bucket/preempted-jobs/$(hostname)\necho \"Registration complete shutting down.\"\n```", "```py\ngcloud compute instances create test-vm \\\n    --zone=us-east1-d \\\n    --machine-type=g2-standard-48 \\\n    --image-family=common-cu121-debian-11-py310 \\\n    --image-project=deeplearning-platform-release \\\n    --service-account=my-account@my-project.iam.gserviceaccount.com \\\n    --maintenance-policy=TERMINATE \\\n    --scopes=https://www.googleapis.com/auth/cloud-platform \\\n    --provisioning-model=SPOT \\\n    --instance-termination-action=DELETE \\\n    --metadata-from-file=shutdown-script=shutdown.sh \\\n    --metadata=startup-script='#! /bin/bash \\\n    export PATH=\"/opt/conda/bin:$PATH\" \\\n    gsutil cp gs://my-bucket/test-vm/my-code.tar . \\\n    tar -xvf my-code.tar \\\n    python3 -m pip install -r requirements.txt \\\n    python3 train.py \\\n    gsutil -m rsync -r output gs://my-bucket/test-vm/output \\\n    yes | gcloud compute instances delete $(hostname) --zone=zone=us-east1-d'\n```", "```py\nimport os\nimport subprocess\nfrom google.cloud import storage\nstorage_client = storage.Client()\nbucket = storage_client.get_bucket('my-bucket')\njobs=bucket.list_blobs(prefix='users/preempted-jobs/')\nfor job in jobs:\n    job_name=job.name.split('/')[-1]\n    job_name='test-vm'\n    script_path=f'{job_name}/metadata/create-instance.sh'\n    blob=bucket.blob(script_path)\n    blob.download_to_filename('script.sh')\n    os.chmod('script.sh', 0o0777)\n    p = subprocess.Popen('script.sh', stdout=subprocess.PIPE)\n    p.wait()\n    if(p.returncode==0):\n        job.delete()\n```"]