- en: 10 Decision Trees are Better Than 1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/10-decision-trees-are-better-than-1-719406680564?source=collection_archive---------2-----------------------#2023-02-27](https://towardsdatascience.com/10-decision-trees-are-better-than-1-719406680564?source=collection_archive---------2-----------------------#2023-02-27)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Breaking down bagging, boosting, Random Forest, and AdaBoost
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://shawhin.medium.com/?source=post_page-----719406680564--------------------------------)[![Shaw
    Talebi](../Images/1449cc7c08890e2078f9e5d07897e3df.png)](https://shawhin.medium.com/?source=post_page-----719406680564--------------------------------)[](https://towardsdatascience.com/?source=post_page-----719406680564--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----719406680564--------------------------------)
    [Shaw Talebi](https://shawhin.medium.com/?source=post_page-----719406680564--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ff3998e1cd186&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F10-decision-trees-are-better-than-1-719406680564&user=Shaw+Talebi&userId=f3998e1cd186&source=post_page-f3998e1cd186----719406680564---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----719406680564--------------------------------)
    ·9 min read·Feb 27, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F719406680564&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F10-decision-trees-are-better-than-1-719406680564&user=Shaw+Talebi&userId=f3998e1cd186&source=-----719406680564---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F719406680564&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F10-decision-trees-are-better-than-1-719406680564&source=-----719406680564---------------------bookmark_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: This is the 2nd article in a series on decision trees. In the [last post](https://medium.com/towards-data-science/decision-trees-introduction-intuition-dac9592f4b7f),
    we introduced decision trees and discussed how to *grow* them using data. While
    they are an intuitive machine learning approach, decision trees are prone to overfitting.
    Here we discuss a solution to this overfitting problem via **decision tree ensembles**.
  prefs: []
  type: TYPE_NORMAL
- en: 'Key points:'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Decision tree ensembles combine several decision trees into a single estimator
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tree ensembles are less prone to overfitting than a single decision tree
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Decision trees**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the [previous article](https://medium.com/towards-data-science/decision-trees-introduction-intuition-dac9592f4b7f)
    of this series, I reviewed decision trees and how we can use them to make predictions.
    However, for many real-world problems, a single decision tree is often prone to
    bias and overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: We saw this in our example from the last blog, where even after a little hyperparameter
    tuning, our decision tree was still wrong 35% of the time.
  prefs: []
  type: TYPE_NORMAL
