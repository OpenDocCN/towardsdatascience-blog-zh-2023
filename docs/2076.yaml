- en: 'T5: Text-to-Text Transformers (Part One)'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/t5-text-to-text-transformers-part-one-6b655f27c79a?source=collection_archive---------5-----------------------#2023-06-27](https://towardsdatascience.com/t5-text-to-text-transformers-part-one-6b655f27c79a?source=collection_archive---------5-----------------------#2023-06-27)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Creating a unified framework for language modeling
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://wolfecameron.medium.com/?source=post_page-----6b655f27c79a--------------------------------)[![Cameron
    R. Wolfe, Ph.D.](../Images/52bb88d7cf1105501be2fae5ccbe7a03.png)](https://wolfecameron.medium.com/?source=post_page-----6b655f27c79a--------------------------------)[](https://towardsdatascience.com/?source=post_page-----6b655f27c79a--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----6b655f27c79a--------------------------------)
    [Cameron R. Wolfe, Ph.D.](https://wolfecameron.medium.com/?source=post_page-----6b655f27c79a--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F28aa6026c553&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ft5-text-to-text-transformers-part-one-6b655f27c79a&user=Cameron+R.+Wolfe%2C+Ph.D.&userId=28aa6026c553&source=post_page-28aa6026c553----6b655f27c79a---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----6b655f27c79a--------------------------------)
    ·14 min read·Jun 27, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F6b655f27c79a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ft5-text-to-text-transformers-part-one-6b655f27c79a&user=Cameron+R.+Wolfe%2C+Ph.D.&userId=28aa6026c553&source=-----6b655f27c79a---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F6b655f27c79a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ft5-text-to-text-transformers-part-one-6b655f27c79a&source=-----6b655f27c79a---------------------bookmark_footer-----------)![](../Images/03be7b5832a9d134961e51b08dd7a5a7.png)'
  prefs: []
  type: TYPE_NORMAL
- en: (Photo by [Patrick Tomasso](https://unsplash.com/@impatrickt?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    on [Unsplash](https://unsplash.com/s/photos/text?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText))
  prefs: []
  type: TYPE_NORMAL
- en: The transfer learning paradigm is comprised of two main stages. First, we pre-train
    a deep neural network over a bunch of data. Then, we fine-tune this model (i.e.,
    train it some more) over a more specific, downstream dataset. The exact implementation
    of these stages may take many different forms. In computer vision, for example,
    we often pre-train models on the ImageNet dataset using a supervised learning
    objective. Then, these models perform supervised fine-tuning on the downstream
    dataset (i.e., the task that we are actually trying to solve). Alternatively,
    in natural language processing (NLP), we often perform [self-supervised](https://cameronrwolfe.substack.com/i/76273144/self-supervised-learning)
    pre-training over an unlabeled textual corpus.
  prefs: []
  type: TYPE_NORMAL
- en: Combining large, deep neural networks with massive (pre-)training datasets often
    leads to impressive results. This finding was found to be especially true for
    NLP. Given that raw textual data is freely available on the internet, we can simply
    download a massive textual corpus, pre-train a large neural net on this data,
    then fine-tune the model on a variety of downstream tasks (or just use zero/few-shot
    learning techniques). This large-scale transfer learning approach was initially
    explored by BERT [2], which pre-trained a [transformer encoder](https://cameronrwolfe.substack.com/i/76273144/transformer-encoders)
    over unlabeled data using a [masking objective](https://cameronrwolfe.substack.com/i/76273144/training-bert),
    then…
  prefs: []
  type: TYPE_NORMAL
