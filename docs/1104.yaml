- en: Neural Graph Databases
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 神经图数据库
- en: 原文：[https://towardsdatascience.com/neural-graph-databases-cc35c9e1d04f?source=collection_archive---------0-----------------------#2023-03-28](https://towardsdatascience.com/neural-graph-databases-cc35c9e1d04f?source=collection_archive---------0-----------------------#2023-03-28)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/neural-graph-databases-cc35c9e1d04f?source=collection_archive---------0-----------------------#2023-03-28](https://towardsdatascience.com/neural-graph-databases-cc35c9e1d04f?source=collection_archive---------0-----------------------#2023-03-28)
- en: What’s New in Graph ML?
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 图神经网络数据库的最新进展
- en: A new milestone in graph data management
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 图数据管理的新里程碑
- en: '[](https://mgalkin.medium.com/?source=post_page-----cc35c9e1d04f--------------------------------)[![Michael
    Galkin](../Images/c5eb13334712ca0462d8a5df4a268ad0.png)](https://mgalkin.medium.com/?source=post_page-----cc35c9e1d04f--------------------------------)[](https://towardsdatascience.com/?source=post_page-----cc35c9e1d04f--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----cc35c9e1d04f--------------------------------)
    [Michael Galkin](https://mgalkin.medium.com/?source=post_page-----cc35c9e1d04f--------------------------------)'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://mgalkin.medium.com/?source=post_page-----cc35c9e1d04f--------------------------------)[![Michael
    Galkin](../Images/c5eb13334712ca0462d8a5df4a268ad0.png)](https://mgalkin.medium.com/?source=post_page-----cc35c9e1d04f--------------------------------)[](https://towardsdatascience.com/?source=post_page-----cc35c9e1d04f--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----cc35c9e1d04f--------------------------------)
    [Michael Galkin](https://mgalkin.medium.com/?source=post_page-----cc35c9e1d04f--------------------------------)'
- en: ·
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F4d4f8ddd1e68&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fneural-graph-databases-cc35c9e1d04f&user=Michael+Galkin&userId=4d4f8ddd1e68&source=post_page-4d4f8ddd1e68----cc35c9e1d04f---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----cc35c9e1d04f--------------------------------)
    ·14 min read·Mar 28, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fcc35c9e1d04f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fneural-graph-databases-cc35c9e1d04f&user=Michael+Galkin&userId=4d4f8ddd1e68&source=-----cc35c9e1d04f---------------------clap_footer-----------)'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F4d4f8ddd1e68&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fneural-graph-databases-cc35c9e1d04f&user=Michael+Galkin&userId=4d4f8ddd1e68&source=post_page-4d4f8ddd1e68----cc35c9e1d04f---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----cc35c9e1d04f--------------------------------)
    ·14 分钟阅读·2023年3月28日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fcc35c9e1d04f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fneural-graph-databases-cc35c9e1d04f&user=Michael+Galkin&userId=4d4f8ddd1e68&source=-----cc35c9e1d04f---------------------clap_footer-----------)'
- en: --
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fcc35c9e1d04f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fneural-graph-databases-cc35c9e1d04f&source=-----cc35c9e1d04f---------------------bookmark_footer-----------)'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fcc35c9e1d04f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fneural-graph-databases-cc35c9e1d04f&source=-----cc35c9e1d04f---------------------bookmark_footer-----------)'
- en: We introduce the concept of Neural Graph Databases as the next step in the evolution
    of graph databases. Tailored for large incomplete graphs and on-the-fly inference
    of missing edges using graph representation learning, neural reasoning maintains
    high expressiveness and supports complex logical queries similar to standard graph
    query languages.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 我们引入了神经图数据库的概念，作为图数据库发展的下一步。神经图数据库专为大规模不完整图设计，并利用图表示学习进行缺失边的即时推理。神经推理保持了较高的表达能力，支持类似于标准图查询语言的复杂逻辑查询。
- en: '![](../Images/52796205e90c035d95249244ab623490.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/52796205e90c035d95249244ab623490.png)'
- en: Image by Authors, assisted by Stable Diffusion.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供，辅助工具为 Stable Diffusion。
- en: '*This post was written together with* [*Hongyu Ren*](http://hyren.me/)*,* [*Michael
    Cochez*](https://www.cochez.nl/)*, and* [*Zhaocheng Zhu*](https://kiddozhu.github.io/)
    *based on our newest paper* [*Neural Graph Reasoning: Complex Logical Query Answering
    Meets Graph Databases*](https://arxiv.org/abs/2303.14617)*. You can also follow*
    [*me*](https://twitter.com/michael_galkin)*,* [*Hongyu*](https://twitter.com/ren_hongyu)*,*
    [*Michael*](https://twitter.com/michaelcochez)*, and* [*Zhaocheng*](https://twitter.com/zhu_zhaocheng)
    *on Twitter. Check our* [*project website*](https://www.ngdb.org/) *for more materials.*'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '*本文由* [*Hongyu Ren*](http://hyren.me/)*,* [*Michael Cochez*](https://www.cochez.nl/)*
    和* [*Zhaocheng Zhu*](https://kiddozhu.github.io/) *共同撰写，基于我们最新的论文* [*Neural Graph
    Reasoning: Complex Logical Query Answering Meets Graph Databases*](https://arxiv.org/abs/2303.14617)*。你也可以关注*
    [*我*](https://twitter.com/michael_galkin)*,* [*Hongyu*](https://twitter.com/ren_hongyu)*,*
    [*Michael*](https://twitter.com/michaelcochez)* 和* [*Zhaocheng*](https://twitter.com/zhu_zhaocheng)
    *在Twitter上的动态。查看我们的* [*项目网站*](https://www.ngdb.org/) *获取更多资料。*'
- en: '**Outline**:'
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**概述**：'
- en: 'Neural Graph Databases: What and Why?'
  id: totrans-14
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 神经图数据库：什么和为什么？
- en: The blueprint of NGDBs
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: NGDBs的蓝图
- en: Neural Graph Storage
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 神经图存储
- en: Neural Query Engine
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 神经查询引擎
- en: Neural Graph Reasoning for Query Engines
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 查询引擎的神经图推理
- en: Open Challenges for NGDBs
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: NGDBs的开放挑战
- en: Learn More
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 了解更多
- en: 'Neural Graph Databases: What and Why?'
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 神经图数据库：什么和为什么？
- en: '🍨Vanilla graph databases are pretty much everywhere thanks to the ever-growing
    graphs in production, flexible graph data models, and expressive query languages.
    Classical, symbolic graph DBs are fast and cool under one important assumption:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 🍨香草图数据库几乎随处可见，这要归功于不断增长的生产图、灵活的图数据模型和富有表现力的查询语言。经典的符号图数据库在一个重要假设下运行得又快又酷：
- en: Completeness. Query engines assume that graphs in classical graph DBs are complete.
  id: totrans-23
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 完整性。查询引擎假设经典图数据库中的图是完整的。
- en: Under the completeness assumption, we can build indexes, store the graphs in
    a variety of read/write-optimized formats and expect the DB would return **what
    is there**.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在完整性假设下，我们可以构建索引，以多种读写优化格式存储图，并期望数据库返回**有什么**。
- en: 'But this assumption does not often hold in practice (we’d say, doesn’t hold
    way too often). If we look at some prominent knowledge graphs (KGs): in Freebase,
    93.8% of people have no place of birth and [78.5% have no nationality](https://aclanthology.org/P09-1113.pdf),
    about 68% of people [do not have any profession](https://dl.acm.org/doi/abs/10.1145/2566486.2568032),
    while in Wikidata, about [50% of artists have no date of birth](https://arxiv.org/abs/2207.00143),
    and only [0.4% of known buildings have information about height](https://dl.acm.org/doi/abs/10.1145/3485447.3511932).
    And that’s for the largest KG openly curated by hundreds of enthusiasts. Surely,
    100M nodes and 1B statements are not the largest ever graph in the industry, so
    you can imagine the degree of incompleteness there.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 但这一假设在实际中往往不成立（我们会说，几乎总是不成立）。例如在一些突出的知识图谱（KGs）中：在Freebase中，93.8%的人没有出生地，[78.5%没有国籍](https://aclanthology.org/P09-1113.pdf)，约68%的人[没有任何职业](https://dl.acm.org/doi/abs/10.1145/2566486.2568032)，而在Wikidata中，[约50%的艺术家没有出生日期](https://arxiv.org/abs/2207.00143)，只有[0.4%的已知建筑有高度信息](https://dl.acm.org/doi/abs/10.1145/3485447.3511932)。这仅仅是由数百名爱好者公开编辑的最大KG，100M节点和1B语句并不是行业中最大的图，所以你可以想象其不完整性的程度。
- en: 'Clearly, to account for incompleteness, in addition to **“what is there?”**
    we have to also ask **“what is missing?”** (or “what can be there?”). Let’s look
    at the example:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，为了考虑不完整性，除了**“有什么？”**我们还必须问**“缺少什么？”**（或“可以有什么？”）。让我们来看一个例子：
- en: '![](../Images/f323dce3ed36b383b14b5194429efd45.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f323dce3ed36b383b14b5194429efd45.png)'
- en: (a) - input query; (b) — incomplete graph with predicted edges (dashed lines);
    (c) — a SPARQL query returning one answer (UofT) via graph traversal; (d) — neural
    execution that recovers missing edges and returns two new answers (UdeM, NYU).
    Image by Authors.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: (a) - 输入查询；(b) — 带有预测边（虚线）的不完整图；(c) — 通过图遍历返回一个答案（UofT）的SPARQL查询；(d) — 神经执行恢复缺失的边，并返回两个新答案（UdeM,
    NYU）。图片来源：作者。
- en: Here, given an incomplete graph (edges `(Turing Award, win, Bengio)` and `(Deep
    Learning, field, LeCun)` are missing) and a query *“At what universities do the
    Turing Award winners in the field of Deep Learning work?”* (expressed in a logical
    form or in some language like SPARQL), a symbolic graph DB would return only one
    answer **UofT** reachable by graph traversal. We refer to such answers as *easy*
    answers, or existing answers. Accounting for missing edges, we would recover two
    more answers **UdeM** and **NYU** (*hard* answers, or inferred answers).
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，给定一个不完整的图（缺失边 `(Turing Award, win, Bengio)` 和 `(Deep Learning, field, LeCun)`）以及一个查询
    *“在深度学习领域的图灵奖得主在哪些大学工作？”*（以逻辑形式或类似 SPARQL 的语言表达），符号图数据库只会返回一个通过图遍历得到的答案 **UofT**。我们将这种答案称为
    *简单* 答案，或现有答案。考虑到缺失的边，我们可以恢复两个更多的答案 **UdeM** 和 **NYU**（*困难* 答案，或推断答案）。
- en: How to infer missing edges?
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 如何推断缺失的边？
- en: In classical DBs, we don’t have much choice. RDF-based databases have some formal
    semantics and can be backed by hefty OWL ontologies but, depending on graph size
    and complexity of inference, it might take an infinite amount of time to complete
    the inference in [SPARQL entailment regimes](https://www.w3.org/TR/sparql11-entailment/).
    Labeled Property Graph (LPG) graph databases do not have built-in means for inferring
    missing edges at all.
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在经典数据库中，我们选择不多。基于 RDF 的数据库具有一些形式语义，可以由庞大的 OWL 本体支持，但根据图的大小和推理的复杂性，在 [SPARQL
    推理规则](https://www.w3.org/TR/sparql11-entailment/) 中完成推理可能需要无限的时间。标记属性图（LPG）数据库完全没有内置的推断缺失边的手段。
- en: Thanks to the advances in Graph Machine Learning, we can often perform link
    prediction in a latent (embedding) space in linear time! We can then extend this
    mechanism to executing complex, database-like queries right in the embedding space.
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 得益于图机器学习的进展，我们通常可以在潜在（嵌入）空间中以线性时间执行链接预测！然后，我们可以将这种机制扩展到在嵌入空间中执行复杂的、类似数据库的查询。
- en: Neural Graph Databases combine the advantages of traditional graph DBs with
    modern graph machine learning.
  id: totrans-33
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 神经图数据库结合了传统图数据库和现代图机器学习的优势。
- en: That is, DB principles like (1) graphs as a first-class citizen, (2) efficient
    storage, and (3) uniform querying interface are now backed by Graph ML techniques
    such as (1) geometric representations, (2) robustness to noisy inputs, (3) large-scale
    pretraining and fine-tuning in order to bridge the incompleteness gap and enable
    neural graph reasoning and inference.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 即，数据库原则如（1）图作为一等公民，（2）高效存储，以及（3）统一查询接口，现在由图 ML 技术支持，如（1）几何表示，（2）对噪声输入的鲁棒性，（3）大规模预训练和微调，以弥合不完整性差距并实现神经图推理和推断。
- en: 'In general, the design principles for NGDBs are:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，NGDB 的设计原则包括：
- en: The **data incompleteness assumption** — the underlying data might have missing
    information on node-, link-, and graph-levels which we would like to infer and
    leverage in query answering;
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据不完整性假设** — 潜在数据可能在节点、链接和图级别上缺少信息，我们希望推断并在查询回答中加以利用；'
- en: '**Inductiveness and updatability** — similar to traditional databases that
    allow updates and instant querying, representation learning algorithms for building
    graph latents have to be inductive and generalize to unseen data (new entities
    and relation at inference time) in the zero-shot (or few-shot) manner to prevent
    costly re-training (for instance, of shallow node embeddings);'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**归纳性和可更新性** — 类似于传统数据库，允许更新和即时查询，构建图潜变量的表示学习算法必须具有归纳性，并以零样本（或少样本）方式对未见数据（新实体和关系）进行泛化，以防止昂贵的再训练（例如，浅层节点嵌入）；'
- en: '**Expressiveness** — the ability of latent representations to encode logical
    and semantic relations in the data akin to FOL (or its fragments) and leverage
    them in query answering. Practically, the set of supported logical operators for
    neural reasoning should be close to or equivalent to standard graph database languages
    like SPARQL or Cypher;'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**表达能力** — 潜在表示在数据中编码逻辑和语义关系的能力，类似于 FOL（或其片段），并在查询回答中加以利用。实际上，神经推理支持的逻辑操作符集应接近或等同于标准图数据库语言，如
    SPARQL 或 Cypher；'
- en: '**Multimodality** beyond knowledge graphs — any graph-structured data that
    can be stored as a node or record in classical databases (consisting, for example,
    of images, texts, molecular graphs, or timestamped sequences) and can be imbued
    with a vector representation is a valid source for the Neural Graph Storage and
    Neural Query Engine.'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**超越知识图谱的多模态性**——任何可以作为节点或记录存储在经典数据库中的图结构数据（例如图像、文本、分子图或带时间戳的序列），并且可以赋予向量表示的，都是神经图存储和神经查询引擎的有效来源。'
- en: 'The key methods to address the NGDB principles are:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 解决NGDB原则的关键方法是：
- en: '**Vector representation as the atomic element** — while traditional graph DBs
    hash the adjacency matrix (or edge list) in many indexes, the incompleteness assumption
    implies that both given edges **and** graph latents (vector representations) become
    the *sources of truth* in the *Neural Graph Storage*;'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**向量表示作为原子元素**——虽然传统的图数据库在许多索引中对邻接矩阵（或边列表）进行哈希处理，但不完全性假设意味着给定的边**和**图潜在（向量表示）都成为*真理的来源*，在*神经图存储*中。'
- en: '**Neural query execution in the latent space** –, basic operations such as
    edge traversal cannot be performed solely symbolically due to the incompleteness
    assumption. Instead, the *Neural Query Engine* operates on both adjacency and
    graph latents to incorporate possibly missing data into query answering;'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**在潜在空间中的神经查询执行**——由于不完全性假设，基本操作如边遍历不能仅通过符号操作来执行。相反，*神经查询引擎*在邻接和图潜在空间上操作，以将可能缺失的数据纳入查询回答中；'
- en: In fact, by answering queries in the latent space (and not sacrificing traversal
    performance) we can ditch symbolic database indexes altogether.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，通过在潜在空间中回答查询（且不牺牲遍历性能），我们可以完全抛弃符号数据库索引。
- en: '![](../Images/9e2a728bb9c7fb27b599ef19bf69ac7f.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9e2a728bb9c7fb27b599ef19bf69ac7f.png)'
- en: 'The main difference between symbolic graph DBs and neural graph DBs: traditional
    DBs answer the question “What is there?” by edge traversal while neural graph
    DBs also answer “What is missing?”. Image by Authors.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 符号图数据库和神经图数据库之间的主要区别：传统的数据库通过边遍历回答“有什么？”的问题，而神经图数据库还会回答“缺少什么？”的问题。图像来源：作者。
- en: The Blueprint of NGDBs
  id: totrans-46
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: NGDBs的蓝图
- en: Before diving into NGDBs, let’s take a look at **neural databases** in general
    — turns out they have been around for a while and you might have noticed that.
    Many machine learning systems already operate in this paradigm when data is encoded
    into model parameters and querying is equivalent to a forward pass that can output
    a new representation or prediction for a downstream task.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入了解NGDBs之前，我们先来看一下**神经数据库**的一般情况——事实证明它们已经存在了一段时间，你可能已经注意到了。许多机器学习系统在数据被编码为模型参数时，已经在这一范式下运行，而查询相当于前向传播，可以为下游任务输出新的表示或预测。
- en: '**Neural Databases: Overview**'
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**神经数据库概述**'
- en: What is the current state of neural databases? What are the differences between
    its kinds and what’s special about NGDBs?
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 神经数据库的现状如何？它的不同种类之间有什么区别，NGDBs（神经图数据库）有什么特别之处？
- en: '![](../Images/a2a55a65281df449a1cecf755ec66364.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a2a55a65281df449a1cecf755ec66364.png)'
- en: Differences between Vector DBs, natural language DBs, and neural graph DBs.
    Image by Authors
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 向量数据库、自然语言数据库和神经图数据库之间的区别。图像来源：作者
- en: '**Vector databases** belong to the family of storage-oriented systems commonly
    built around approximate nearest neighbor libraries (ANN) like [Faiss](https://github.com/facebookresearch/faiss)
    or [ScaNN](https://github.com/google-research/google-research/tree/master/scann)
    (or custom solutions) to answer distance-based queries using Maximum Inner-Product
    Search (MIPS), L1, L2, or other distances. Being encoder-independent (that is,
    any encoder yielding vector representations can be a source like a ResNet or BERT),
    vector databases are fast but lack complex query answering capabilities.'
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**向量数据库**属于存储导向的系统，这些系统通常基于近似最近邻库（ANN），如[Faiss](https://github.com/facebookresearch/faiss)或[ScaNN](https://github.com/google-research/google-research/tree/master/scann)（或定制解决方案）来回答基于距离的查询，使用最大内积搜索（MIPS）、L1、L2或其他距离。由于向量数据库与编码器无关（即，任何生成向量表示的编码器，如ResNet或BERT，都可以作为来源），它们速度很快，但缺乏复杂的查询回答能力。'
- en: With the recent rise of large-scale pretrained models — or, [foundation models](https://en.wikipedia.org/wiki/Foundation_models)
    — we have witnessed their huge success in natural language processing and computer
    vision tasks. We argue that such foundation models are also a prominent example
    of neural databases. There, the *storage module* might be presented directly with
    model parameters or outsourced to an external index often used in [retrieval-augmented
    models](https://arxiv.org/abs/2002.08909) since encoding all world knowledge even
    into billions of model parameters is hard. The *query module* performs in-context
    learning either via filling in the blanks in encoder models (BERT or T5 style)
    or via prompts in decoder-only models (GPT-style) that can span multiple modalities,
    eg, [learnable tokens for vision applications](https://arxiv.org/abs/2205.10337)
    or even [calling external tools](https://arxiv.org/abs/2302.07842).
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最近，随着大规模预训练模型的崛起——或称为[基础模型](https://en.wikipedia.org/wiki/Foundation_models)——我们见证了它们在自然语言处理和计算机视觉任务中的巨大成功。我们认为，这些基础模型也是神经数据库的一个重要例子。在这些模型中，*存储模块*可能直接以模型参数的形式呈现，或者外包给一个外部索引，这在[检索增强模型](https://arxiv.org/abs/2002.08909)中常常使用，因为将所有世界知识编码到即便是数十亿个模型参数中也是困难的。*查询模块*通过填充编码器模型（BERT或T5风格）中的空白或通过解码器模型（GPT风格）中的提示，进行上下文学习，这些提示可以跨越多种模式，例如[视觉应用的可学习标记](https://arxiv.org/abs/2205.10337)或甚至[调用外部工具](https://arxiv.org/abs/2302.07842)。
- en: '**Natural Language Databases (NLDB)** introduced by [Thorne et al](https://arxiv.org/abs/2106.01074)
    model atomic elements as textual facts encoded to a vector via a pre-trained language
    model (LM). Queries to NLDB are sent as natural language utterances that get encoded
    to vectors and query processing employs the *retriever-reader* approach.'
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[Thorne et al](https://arxiv.org/abs/2106.01074)介绍的**自然语言数据库 (NLDB)**将原子元素建模为通过预训练语言模型（LM）编码为向量的文本事实。对NLDB的查询以自然语言表达的形式发送，这些查询被编码为向量，查询处理采用*检索器-阅读器*方法。'
- en: Neural Graph Databases is not a novel term — many graph ML approaches tried
    to combine graph embeddings with database indexes, perhaps [RDF2Vec](http://rdf2vec.org/)
    and [LPG2Vec](https://openreview.net/forum?id=p0sMj8oH2O) are some of the most
    prominent examples how embeddings can be plugged into **existing** graph DBs and
    run on top of symbolic indexes.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 神经图数据库并不是一个新名词——许多图机器学习方法尝试将图嵌入与数据库索引结合起来，或许[RDF2Vec](http://rdf2vec.org/)和[LPG2Vec](https://openreview.net/forum?id=p0sMj8oH2O)是一些最显著的例子，展示了如何将嵌入插件到**现有**图数据库中，并在符号索引之上运行。
- en: In contrast, we posit that NGDBs can **work without symbolic indexes** right
    in the latent space. As we show below, there exist ML algorithms that can simulate
    exact edge traversal-like behavior in embedding space to retrieve “**what is there**”
    as well as perform neural reasoning to answer “**what is missing**”.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，我们认为NGDB可以在潜在空间中**无需符号索引**直接工作。如下面所示，存在能够模拟嵌入空间中精确边遍历行为的机器学习算法，以检索“**那里有什么**”，并进行神经推理以回答“**缺少什么**”。
- en: '**Neural Graph Databases: Architecture**'
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**神经图数据库：架构**'
- en: '![](../Images/223c920f6d8c52a35526536fdd7523b5.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/223c920f6d8c52a35526536fdd7523b5.png)'
- en: A conceptual scheme of Neural Graph Databases. An input query is processed by
    the Neural Query Engine where the Planner derives a computation graph of the query
    and the Executor executes the query in the latent space. The Neural Graph Storage
    employs Graph Store and Feature Store to obtain latent representations in the
    Embedding Store. The Executor communicates with the embedding store to retrieve
    and return results. Image by Authors
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 神经图数据库的概念图。输入查询由神经查询引擎处理，其中规划器导出查询的计算图，执行器在潜在空间中执行查询。神经图存储使用图存储和特征存储在嵌入存储中获取潜在表示。执行器与嵌入存储通信，以检索和返回结果。图像来源于作者
- en: On a higher level, NGDB contains two main components, **Neural Graph Storage**
    and **Neural Query Engine**. The query answering pipeline starts with the query
    sent by some application or downstream task already in a structured format (obtained,
    for example, via [semantic parsing](https://arxiv.org/abs/2209.15003) if an initial
    query is in natural language to transform it into a structured format).
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在更高层次上，NGDB包含两个主要组件：**神经图存储**和**神经查询引擎**。查询回答流程从某些应用程序或下游任务发送的已结构化格式的查询开始（例如，通过[语义解析](https://arxiv.org/abs/2209.15003)将初始自然语言查询转换为结构化格式）。
- en: The query first arrives to the Neural Query Engine, and, in particular, to the
    *Query Planner* module. The task of the Query Planner is to derive an efficient
    computation graph of atomic operations (projections and logical operations) with
    respect to the query complexity, prediction tasks, and underlying data storage
    such as possible graph partitioning.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 查询首先到达神经查询引擎，特别是到*查询规划器*模块。查询规划器的任务是根据查询复杂性、预测任务和底层数据存储（如可能的图划分）生成一个高效的原子操作计算图。
- en: The derived plan is then sent to the *Query Executor* that encodes the query
    in a latent space, executes the atomic operations over the underlying graph and
    its latent representations, and aggregates the results of atomic operations into
    a final answer set. The execution is done via the *Retrieval* module that communicates
    with the *Neural Graph Storage*.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 生成的计划随后被送往*查询执行器*，该执行器将查询编码到潜在空间中，执行对底层图及其潜在表示的原子操作，并将原子操作的结果聚合成最终答案集。执行是通过与*神经图存储*通信的*检索*模块完成的。
- en: The storage layer consists of
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 存储层包括
- en: 1️⃣ *Graph Store* for keeping the multi-relational adjacency matrix in space-
    and time-efficient manner (eg, in various sparse formats like COO and CSR);
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 1️⃣ *图存储* 用于以空间和时间高效的方式保存多关系邻接矩阵（例如，以各种稀疏格式如COO和CSR）。
- en: 2️⃣ *Feature Store* for keeping node- and edge-level multimodal features associated
    with the underlying graph.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 2️⃣ *特征存储* 用于保存与底层图相关的节点和边级多模态特征。
- en: 3️⃣ *Embedding Store* that leverages an Encoder module to produce graph representations
    in a latent space based on the underlying adjacency and associated features.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 3️⃣ *嵌入存储* 利用编码器模块生成基于底层邻接和相关特征的潜在空间中的图表示。
- en: The Retrieval module queries the encoded graph representations to build a distribution
    of potential answers to atomic operations.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 检索模块查询编码后的图表示，以构建潜在答案的分布。
- en: '**Neural Graph Storage**'
  id: totrans-68
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**神经图存储**'
- en: '![](../Images/59c2e89f7e466bbfb47e484f362dd3ab.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/59c2e89f7e466bbfb47e484f362dd3ab.png)'
- en: In traditional graph DBs (right), queries are optimized into a plan (often,
    a tree of join operators) and executed against the storage of DB indexes. In Neural
    Graph DBs (left), we encode the query (or its steps) in a latent space and execute
    against the latent space of the underlying graph. Image by Authors.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在传统的图数据库（右侧），查询被优化为一个计划（通常是一个连接操作符的树），并执行于数据库索引的存储中。在神经图数据库（左侧）中，我们将查询（或其步骤）编码到一个潜在空间中，并在底层图的潜在空间中执行。图像由作者提供。
- en: In traditional graph DBs, storage design often depends on the graph modeling
    paradigm.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在传统的图数据库中，存储设计通常取决于图建模范式。
- en: The two most popular paradigms are Resource Description Framework (RDF) graphs
    and Labeled Property Graphs (LPG). We posit, however, that the new [RDF-star](https://w3c.github.io/rdf-star/cg-spec/editors_draft.html)
    (and accompanying SPARQL-star) is going to unify those two merging logical expressiveness
    of RDF graphs with attributed nature of LPG. Many existing KGs already follow
    the RDF-star (-like) paradigm like [hyper-relational KGs](/representation-learning-on-rdf-and-lpg-knowledge-graphs-6a92f2660241)
    and [Wikidata Statement Model](https://www.wikidata.org/wiki/Help:Statements).
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 两种最流行的范式是资源描述框架（RDF）图和标记属性图（LPG）。然而，我们认为新的 [RDF-star](https://w3c.github.io/rdf-star/cg-spec/editors_draft.html)（及其伴随的SPARQL-star）将统一这两种范式，将RDF图的逻辑表达性与LPG的属性特性融合起来。许多现有的知识图谱已经遵循了类似RDF-star的范式，如
    [超关系知识图谱](/representation-learning-on-rdf-and-lpg-knowledge-graphs-6a92f2660241)
    和 [Wikidata Statement Model](https://www.wikidata.org/wiki/Help:Statements)。
- en: If we are to envision the backbone graph modeling paradigm in the next years,
    we’d go for RDF-star.
  id: totrans-73
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 如果我们展望未来几年的骨干图建模范式，我们会选择RDF-star。
- en: 'In the Neural Graph Storage, both the input graph and its vector representations
    are sources of truth. For answering queries in the latent space, we need:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在神经图存储中，输入图及其向量表示都是事实来源。为了在潜在空间中回答查询，我们需要：
- en: Query Encoder
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 查询编码器
- en: Graph Encoder
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图编码器
- en: Retrieval mechanism to match query representation against the graph representation
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 检索机制用于将查询表示与图表示进行匹配
- en: The graph encoding (embedding) process can be viewed as a compression step but
    the semantic and structure similarity of entities/relations is kept. The distance
    between entities/relations in the embedding space should be positively correlated
    with the semantic/structure similarity. There are many options for the architecture
    of the encoder — and we recommend sticking to **inductive** ones to adhere to
    the NGDB design principles. In our recent [NeurIPS 2022 work](https://arxiv.org/abs/2210.08008),
    we presented two such inductive models.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 图编码（嵌入）过程可以视为一个压缩步骤，但保留了实体/关系的语义和结构相似性。嵌入空间中实体/关系之间的距离应该与语义/结构相似性正相关。编码器的架构有很多选择——我们建议坚持使用**归纳**型的，以遵循NGDB设计原则。在我们最近的[NeurIPS
    2022工作](https://arxiv.org/abs/2210.08008)中，我们展示了两个这样的归纳模型。
- en: Query encoding is usually matched with the nature graph encoding such that both
    of them will be in the same space. Once we have latent representations, the Retrieval
    module kicks in to extract relevant answers.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 查询编码通常与自然图编码相匹配，使得它们处于同一空间。一旦我们有了潜在表示，检索模块就会启动以提取相关答案。
- en: 'The retrieval process can be seen as a nearest neighbor search of the input
    vector in the embedding space and has 3 direct benefits:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 检索过程可以被视为在嵌入空间中对输入向量的最近邻搜索，并且具有3个直接好处：
- en: Confidence scores for each retrieved item — thanks to a predefined distance
    function in the embedding space
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 每个检索项的置信度评分——多亏了嵌入空间中预定义的距离函数。
- en: Different definitions of the latent space and the distance function — catering
    for different graphs, eg, tree-like graphs are easier to work in hyperbolic spaces
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 潜在空间和距离函数的不同定义——针对不同的图，例如，树状图在双曲空间中更易于处理。
- en: Efficiency and scalability — retrieval scales to extremely large graphs with
    billions of nodes and edges
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 效率和可扩展性——检索可以扩展到包含数十亿节点和边的极大图。
- en: '**Neural Query Engine**'
  id: totrans-84
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**神经查询引擎**'
- en: '![](../Images/d34c8122344f986bba31e4b69db9153e.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d34c8122344f986bba31e4b69db9153e.png)'
- en: Query planning in NGDBs (left) and traditional graph DBs (right). The NGDB planning
    (assuming incomplete graphs) can be performed autoregressively step-by-step (1)
    or generated entirely in one step (2). The traditional DB planning is cost-based
    and resorts to metadata (assuming complete graphs and extracted from them) such
    as the number of intermediate answers to build a tree of join operators. Image
    by Authors
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: NGDBs（左）和传统图数据库（右）的查询规划。NGDB规划（假设图不完整）可以逐步自回归执行（1）或完全生成一个步骤（2）。传统数据库规划是基于成本的，并且依赖于元数据（假设图完整并从中提取），例如中间答案的数量来构建连接操作符的树。图片由作者提供
- en: In traditional DBs, a typical query engine performs three major operations.
    (1) **Query parsing** to verify syntax correctness (often enriched with a deeper
    semantic analysis of query terms); (2) **Query planning** and optimization to
    derive an efficient query plan (usually, a tree of relational operators) that
    minimizes computational costs; (3) **Query execution** that scans the storage
    and processes intermediate results according to the query plan.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在传统数据库中，典型的查询引擎执行三个主要操作。(1) **查询解析**以验证语法正确性（通常会进行更深层次的语义分析）；(2) **查询规划**和优化以得出一个有效的查询计划（通常是关系操作符的树），以最小化计算成本；(3)
    **查询执行**根据查询计划扫描存储并处理中间结果。
- en: It is rather straightforward to extend those operations to NGDBs.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 将这些操作扩展到NGDBs是相当简单的。
- en: 1️⃣ Query Parsing can be achieved via semantic parsing to a structured query
    format. We intentionally leave the discussion on a query language for NGDBs for
    future works and heated public discussions 😉
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 1️⃣ 查询解析可以通过语义解析转化为结构化查询格式。我们故意将NGDBs的查询语言讨论留待未来的工作和热烈的公众讨论😉
- en: 2️⃣ Query Planner derives an efficient query plan of atomic operations (projections
    and logical operators) maximizing completeness (all answers over existing edges
    must be returned) and inference (of missing edges predicted on the fly) taking
    into account query complexity and underlying graph.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 2️⃣ 查询规划器得出原子操作（投影和逻辑操作符）的有效查询计划，最大化完整性（必须返回所有现有边上的答案）和推断（即时预测缺失边）同时考虑查询复杂性和底层图。
- en: '3️⃣ Once the query plan is finalized, the Query Executor encodes the query
    (or its parts) into a latent space, communicates with the Graph Storage and its
    Retrieval module, and aggregates intermediate results into the final answer set.
    There exist two common mechanisms for query execution:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 3️⃣ 一旦查询计划完成，查询执行器将查询（或其部分）编码到潜在空间，与图存储及其检索模块进行通信，并将中间结果聚合到最终答案集中。查询执行存在两种常见机制：
- en: '*Atomic*, resembling traditional DBs, when a query plan is executed sequentially
    by encoding atomic patterns, retrieving their answers, and executing logical operators
    as intermediate steps;'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*原子*，类似于传统数据库，当查询计划按顺序执行，通过编码原子模式、检索其答案和执行逻辑操作作为中间步骤；'
- en: '*Global*, when the entire query graph is encoded and executed in a latent space
    in one step.'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*全局*，当整个查询图在一个步骤中被编码并在潜在空间中执行。'
- en: The main challenge for neural query execution is matching query expressiveness
    to that of symbolic languages like SPARQL or Cypher — so far, neural methods can
    execute queries close to First-Order Logic expressiveness, but we are somewhat
    halfway there to symbolic languages.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 神经查询执行的主要挑战是将查询表达能力与 SPARQL 或 Cypher 等符号语言匹配——迄今为止，神经方法可以执行接近一阶逻辑表达能力的查询，但在符号语言方面还差一半。
- en: A Taxonomy of Neural Graph Reasoning for Query Engines
  id: totrans-95
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 神经图推理的分类学用于查询引擎
- en: The literature on neural methods for complex logical query answering (aka, *query
    embedding*) has been growing since 2018 and the seminal NeurIPS work of [Hamilton
    et al](https://proceedings.neurips.cc/paper/7473-embedding-logical-queries-on-knowledge-graphs)
    on **Graph Query Embedding** (GQE). GQE was able to answer conjunctive queries
    with intersections and predict missing links on the fly.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 自 2018 年以来，关于复杂逻辑查询回答的神经方法（即*查询嵌入*）的文献不断增加，特别是 [Hamilton 等人](https://proceedings.neurips.cc/paper/7473-embedding-logical-queries-on-knowledge-graphs)
    在**图查询嵌入**（GQE）方面的开创性 NeurIPS 工作。GQE 能够回答带有交集的联接查询，并实时预测缺失的链接。
- en: GQE can be considered as the first take on Neural Query Engines for NGDBs.
  id: totrans-97
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: GQE 可以被视为对 NGDBs 的神经查询引擎的第一次尝试。
- en: 'GQE started the whole subfield of Graph Machine Learning followed by some prominent
    examples like [Query2Box (ICLR 2020)](https://openreview.net/forum?id=BJgr4kSFDS)
    and [Continuous Query Decomposition (ICLR 2021)](https://openreview.net/forum?id=Mos9F9kDwkz).
    We undertook a major effort categorizing all those (about 50) works along 3 main
    directions:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: GQE 开创了图机器学习的整个子领域，随后出现了一些著名的例子，如 [Query2Box (ICLR 2020)](https://openreview.net/forum?id=BJgr4kSFDS)
    和 [Continuous Query Decomposition (ICLR 2021)](https://openreview.net/forum?id=Mos9F9kDwkz)。我们进行了一项重大工作，将所有这些（约
    50 项）工作按 3 个主要方向进行了分类：
- en: ⚛️ **Graphs** — what is the underlying structure against which we answer queries;
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: ⚛️ **图**——我们回答查询的基础结构是什么；
- en: 🛠️ **Modeling** — how we answer queries and which inductive biases are employed;
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 🛠️ **建模**——我们如何回答查询以及采用了哪些归纳偏差；
- en: 🗣️ **Queries** — what we answer, what are the query structures and what are
    the expected answers.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 🗣️ **查询**——我们回答什么，查询结构是什么，以及预期的答案是什么。
- en: '![](../Images/96529703229bccd94ad6c37088dd2a2b.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/96529703229bccd94ad6c37088dd2a2b.png)'
- en: The taxonomy of neural approaches for complex logical query answering. See the
    <paper> for more details. Image by Authors
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 复杂逻辑查询回答的神经方法分类。有关更多详细信息，请参见<paper>。图像由作者提供
- en: ⚛️ Talking about **Graphs**, we further break them down into **Modality** (classic
    triple-only graphs, hyper-relational graphs, hypergraphs, and more), **Reasoning
    Domain** (discrete entities or including continuous outputs), and **Semantics**
    (how neural encoders capture higher-order relationships like OWL ontologies).
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: ⚛️ 说到**图**，我们进一步将其细分为**模态**（经典的三元组图、超关系图、超图等）、**推理领域**（离散实体或包括连续输出）和**语义**（神经编码器如何捕捉更高阶关系，如
    OWL 本体）。
- en: 🛠️ In **Modeling**, we follow the Encoder-Processor-Decoder paradigm classifying
    inductive biases of existing models, eg, transductive or inductive encoders with
    neural or neuro-symbolic processors.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 🛠️ 在**建模**中，我们遵循编码器-处理器-解码器范式，对现有模型的归纳偏差进行分类，例如，具有神经或神经符号处理器的传递性或归纳编码器。
- en: 🗣️ In **Queries**, we aim at mapping the set of queries answerable by neural
    methods with that of symbolic graph query languages. We talk about **Query Operators**
    (going beyond standard And/Or/Not), **Query Patterns** (from chain-like queries
    to DAGs and cyclic patterns), and **Projected Variables** (your favorite relational
    algebra ).
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 🗣️ 在 **查询** 中，我们的目标是将神经方法能够回答的查询集与符号图查询语言的查询集进行映射。我们讨论**查询操作符**（超越标准的与/或/非），**查询模式**（从链状查询到DAG和循环模式），以及**投影变量**（你喜欢的关系代数）。
- en: Open Challenges for NGDBs
  id: totrans-107
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: NGDB的开放挑战
- en: Analyzing the taxonomy we find that there is no silver bullet at the moment,
    eg, most processors can only work in discrete mode with tree-based queries. But
    it also means there is a lot of room for future work — possibly your contribution!
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 分析分类法时，我们发现目前没有银弹，例如，大多数处理器只能在离散模式下处理基于树的查询。但这也意味着未来有很大的工作空间——可能包括你的贡献！
- en: To be more precise, here are the main NGDB challenges for the following years.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 更具体地说，以下是未来几年NGDB的主要挑战。
- en: 'Along the **Graph** branch:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 沿着 **图** 分支：
- en: '**Modality**: Supporting more graph modalities: from classic triple-only graphs
    to hyper-relational graphs, hypergraphs, and multimodal sources combining graphs,
    texts, images, and more.'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模态**：支持更多图的模态：从经典的仅三元组图到超关系图、超图以及结合图、文本、图像等的多模态源。'
- en: '**Reasoning Domain**: Supporting logical reasoning and neural query answering
    over temporal and continuous (textual and numerical) data — literals constitute
    a major portion of graphs as well as relevant queries over literals.'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**推理领域**：支持对时间和连续（文本和数值）数据进行逻辑推理和神经查询回答——字面量构成了图的大部分以及对字面量的相关查询。'
- en: '**Background Semantics**: Supporting complex axioms and formal semantics that
    encode higher-order relationships between (latent) classes of entities and their
    hierarchies, \eg, enabling neural reasoning over description logics and OWL fragments.'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**背景语义**：支持复杂公理和形式语义，这些语义编码了（潜在的）实体类及其层次结构之间的高阶关系，例如，支持对描述逻辑和OWL片段进行神经推理。'
- en: 'In the **Modeling** branch:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 在 **建模** 分支：
- en: '**Encoder**: Inductive encoders supporting unseen relation at inference time
    — this a key for (1) *updatability* of neural databases without the need of retraining;
    (2) enabling the *pretrain-finetune* strategy generalizing query answering to
    custom graphs with custom relational schema.'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**编码器**：支持在推理时处理未见过的关系——这是（1）*可更新性*的关键，无需重新训练即可更新神经数据库；（2）启用*预训练-微调*策略，将查询回答推广到具有自定义关系模式的自定义图。'
- en: '**Processor**: Expressive processor networks able to effectively and efficiently
    execute complex query operators akin to SPARQL and Cypher operators. Improving
    sample efficiency of neural processors is crucial for the *training time vs quality*
    tradeoff — reducing training time while maintaining high predictive qualities.'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**处理器**：表达性处理器网络能够有效且高效地执行类似于SPARQL和Cypher操作符的复杂查询操作符。提高神经处理器的样本效率对于*训练时间与质量*权衡至关重要——在保持高预测质量的同时减少训练时间。'
- en: '**Decoder**: So far, all neural query answering decoders operate exclusively
    on discrete nodes. Extending the range of answers to continuous outputs is crucial
    for answering real-world queries.'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**解码器**：迄今为止，所有神经查询回答解码器仅在离散节点上操作。扩展答案范围到连续输出对于回答现实世界的查询至关重要。'
- en: '**Complexity**: As the main computational bottleneck of processor networks
    is the dimensionality of embedding space (for purely neural models) and/or the
    number of nodes (for neuro-symbolic), new efficient algorithms for neural logical
    operators and retrieval methods are the key to scaling NGDBs to billions of nodes
    and trillions of edges.'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**复杂性**：由于处理器网络的主要计算瓶颈是嵌入空间的维度（对于纯神经模型）和/或节点数（对于神经-符号模型），新型高效的神经逻辑操作符和检索方法是将NGDB扩展到数十亿节点和万亿边的关键。'
- en: 'In **Queries**:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 在 **查询** 中：
- en: '**Operators**: Neuralizing more complex query operators matching the expressiveness
    of declarative graph query languages, e.g., supporting Kleene plus and star, property
    paths, filters.'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**操作符**：使更复杂的查询操作符具备与声明式图查询语言相匹配的表达能力，例如，支持克林星号和加号、属性路径、过滤器。'
- en: '**Patterns**: Answering more complex patterns beyond tree-like queries. This
    includes DAGs and cyclic graphs.'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模式**：回答比树状查询更复杂的模式，包括DAG和循环图。'
- en: '**Projected Variables**: Allowing projecting more than a final leaf node entity,
    that is, allowing returning intermediate variables, relations, and multiple variables
    organized in tuples (bindings).'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**投影变量**：允许投影超出最终叶节点实体，即允许返回中间变量、关系以及组织在元组（绑定）中的多个变量。'
- en: '**Expressiveness**: Answering queries outside simple EPFO and EFO fragments
    and aiming for the expressiveness of database languages.'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**表达能力**：回答超出简单EPFO和EFO片段的查询，并追求数据库语言的表达能力。'
- en: 'Finally, in **Datasets** and **Evaluation**:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，在**数据集**和**评估**方面：
- en: The need for larger and **diverse benchmarks** covering more graph modalities,
    more expressive query semantics, more query operators, and query patterns.
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 需要更大且**多样化的基准**，涵盖更多图模式、更具表现力的查询语义、更多查询操作符和查询模式。
- en: As the existing evaluation protocol appears to be limited (focusing only on
    inferring *hard* answers) there is a need for a more **principled evaluation framework
    and metrics** covering various aspects of the query answering workflow.
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于现有的评估协议似乎有限（仅关注推断*硬*答案），需要一个更**有原则的评估框架和指标**，涵盖查询回答工作流的各个方面。
- en: 'Pertaining to the Neural Graph Storage and NGDB in general, we identify the
    following challenges:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 关于神经图存储和NGDB的一般情况，我们识别出以下挑战：
- en: The need for a **scalable retrieval** mechanism to scale neural reasoning to
    graphs of billions of nodes. Retrieval is tightly connected to the Query Processor
    and its modeling priors. Existing scalable ANN libraries can only work with basic
    L1, L2, and cosine distances that limit the space of possible processors in the
    neural query engine.
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 需要一个**可扩展检索**机制来将神经推理扩展到数十亿节点的图。检索与查询处理器及其建模先验紧密相关。现有的可扩展ANN库只能处理基本的L1、L2和余弦距离，这限制了神经查询引擎中可能的处理器空间。
- en: Currently, all complex query datasets provide a hardcoded query execution plan
    that might not be optimal. There is a need for a **neural query planner** that
    would transform an input query into an optimal execution sequence taking into
    account prediction tasks, query complexity, type of the neural processor, and
    configuration of the Storage layer.
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 目前，所有复杂查询数据集提供了一个硬编码的查询执行计划，可能不是最优的。需要一个**神经查询规划器**，能够将输入查询转换为最优执行序列，考虑预测任务、查询复杂性、神经处理器类型和存储层配置。
- en: Due to encoder inductiveness and updatability without retraining, there is a
    need to alleviate the issues of **continual learning**, **catastrophic forgetting**,
    and **size generalization** when running inference on much larger graphs than
    training ones.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 由于编码器的归纳性和可更新性而无需重新训练，运行推理时在比训练图更大的图上存在需要缓解**持续学习**、**灾难性遗忘**和**规模泛化**的问题。
- en: Learn More
  id: totrans-131
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 了解更多
- en: NGDB is still an emerging concept with many open challenges for future research.
    If you want to learn more about NGDB, feel free to check out
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: NGDB仍然是一个新兴概念，面临许多未来研究的挑战。如果你想了解更多关于NGDB的内容，可以查看
- en: 📜 our paper ([arxiv](https://arxiv.org/abs/2303.14617)),
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 📜 我们的论文 ([arxiv](https://arxiv.org/abs/2303.14617))，
- en: 🌐 [our website](https://www.ngdb.org/),
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 🌐 [我们的网站](https://www.ngdb.org/)，
- en: 🔧 [GitHub repo](https://github.com/neuralgraphdatabases/awesome-logical-query)
    with the most up-to-date list of relevant papers, datasets, and categorization,
    feel free to open issues and PRs.
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 🔧 [GitHub 仓库](https://github.com/neuralgraphdatabases/awesome-logical-query)包含最新的相关论文、数据集和分类，欢迎提出问题和PR。
- en: We will also be organizing workshops, stay tuned for the updates!
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将组织研讨会，请关注最新动态！
