- en: 'Courage to Learn ML: An In-Depth Guide to the Most Common Loss Functions'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 勇敢学习机器学习：最常见损失函数的深入指南
- en: 原文：[https://towardsdatascience.com/courage-to-learn-ml-an-in-depth-guide-to-the-most-common-loss-functions-84a6b07cca17?source=collection_archive---------3-----------------------#2023-12-28](https://towardsdatascience.com/courage-to-learn-ml-an-in-depth-guide-to-the-most-common-loss-functions-84a6b07cca17?source=collection_archive---------3-----------------------#2023-12-28)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/courage-to-learn-ml-an-in-depth-guide-to-the-most-common-loss-functions-84a6b07cca17?source=collection_archive---------3-----------------------#2023-12-28](https://towardsdatascience.com/courage-to-learn-ml-an-in-depth-guide-to-the-most-common-loss-functions-84a6b07cca17?source=collection_archive---------3-----------------------#2023-12-28)
- en: MSE, Log Loss, Cross Entropy, RMSE, and the Foundational Principles of Popular
    Loss Functions
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: MSE、对数损失、交叉熵、RMSE，以及流行损失函数的基础原则
- en: '[](https://amyma101.medium.com/?source=post_page-----84a6b07cca17--------------------------------)[![Amy
    Ma](../Images/2edf55456a1f92724535a1441fa2bef5.png)](https://amyma101.medium.com/?source=post_page-----84a6b07cca17--------------------------------)[](https://towardsdatascience.com/?source=post_page-----84a6b07cca17--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----84a6b07cca17--------------------------------)
    [Amy Ma](https://amyma101.medium.com/?source=post_page-----84a6b07cca17--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://amyma101.medium.com/?source=post_page-----84a6b07cca17--------------------------------)[![Amy
    Ma](../Images/2edf55456a1f92724535a1441fa2bef5.png)](https://amyma101.medium.com/?source=post_page-----84a6b07cca17--------------------------------)[](https://towardsdatascience.com/?source=post_page-----84a6b07cca17--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----84a6b07cca17--------------------------------)
    [Amy Ma](https://amyma101.medium.com/?source=post_page-----84a6b07cca17--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fd6d8df787b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcourage-to-learn-ml-an-in-depth-guide-to-the-most-common-loss-functions-84a6b07cca17&user=Amy+Ma&userId=d6d8df787b&source=post_page-d6d8df787b----84a6b07cca17---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----84a6b07cca17--------------------------------)
    ·13 min read·Dec 28, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F84a6b07cca17&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcourage-to-learn-ml-an-in-depth-guide-to-the-most-common-loss-functions-84a6b07cca17&user=Amy+Ma&userId=d6d8df787b&source=-----84a6b07cca17---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fd6d8df787b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcourage-to-learn-ml-an-in-depth-guide-to-the-most-common-loss-functions-84a6b07cca17&user=Amy+Ma&userId=d6d8df787b&source=post_page-d6d8df787b----84a6b07cca17---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----84a6b07cca17--------------------------------)
    ·13 min 阅读·2023年12月28日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F84a6b07cca17&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcourage-to-learn-ml-an-in-depth-guide-to-the-most-common-loss-functions-84a6b07cca17&user=Amy+Ma&userId=d6d8df787b&source=-----84a6b07cca17---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F84a6b07cca17&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcourage-to-learn-ml-an-in-depth-guide-to-the-most-common-loss-functions-84a6b07cca17&source=-----84a6b07cca17---------------------bookmark_footer-----------)![](../Images/847cb932c67abb07544b56bac23b87a8.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F84a6b07cca17&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcourage-to-learn-ml-an-in-depth-guide-to-the-most-common-loss-functions-84a6b07cca17&source=-----84a6b07cca17---------------------bookmark_footer-----------)![](../Images/847cb932c67abb07544b56bac23b87a8.png)'
- en: Photo by [William Warby](https://unsplash.com/@wwarby?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由 [William Warby](https://unsplash.com/@wwarby?utm_source=medium&utm_medium=referral)
    提供，来源于 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: 'Welcome back! In the ‘[Courage to Learn ML](http://towardsdatascience.com/tagged/courage-to-learn-ml)’
    series, where we conquer machine learning fears one challenge at a time. Today,
    we’re diving headfirst into the world of loss functions: the silent superheroes
    guiding our models to learn from mistakes. In this post, we’d cover the following
    topics:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 欢迎回来！在‘[勇敢学习机器学习](http://towardsdatascience.com/tagged/courage-to-learn-ml)’系列中，我们一一征服机器学习的挑战。今天，我们将深入探讨损失函数的世界：这些无声的超级英雄指导我们的模型从错误中学习。在这篇文章中，我们将涵盖以下主题：
- en: What is a loss function?
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 什么是损失函数？
- en: Difference between loss functions and metrics
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 损失函数和指标之间的区别
- en: Explaining MSE and MAE from two perspectives
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从两个角度解释**均方误差**（MSE）和**平均绝对误差**（MAE）
- en: Three basic ideas when designing loss functions
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设计损失函数时的三个基本思想
- en: Using those three basic ideas to interpret MSE, log loss, and cross-entropy
    loss
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 利用这三个基本思想来解读**均方误差**（MSE）、**对数损失**和**交叉熵损失**
- en: Connection between log loss and cross-entropy loss
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**对数损失**和**交叉熵损失**之间的联系'
- en: How to handle multiple loss functions (objectives) in practice
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实践中如何处理多个损失函数（目标）
- en: Difference between MSE and RMSE
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**均方误差**（MSE）和**均方根误差**（RMSE）之间的区别'
- en: What are loss functions, and why are they important in machine learning models?
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 损失函数是什么？它们在机器学习模型中为何重要？
- en: Loss functions are crucial in evaluating a model’s effectiveness during its
    learning process, akin to an exam or a…
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 损失函数在评估模型学习过程中的有效性方面至关重要，就像考试或…
