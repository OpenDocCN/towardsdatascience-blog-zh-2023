["```py\nimport urllib.request\nsearch_url = f'http://eutils.ncbi.nlm.nih.gov/entrez//eutils/esearch.fcgi/' + \\\n              f'?db=pubmed' + \\\n              f'&term=myoglobin[mesh]' + \\\n              f'&mindate=2022' + \\\n              f'&maxdate=2023' + \\\n              f'&retmode=json' + \\\n              f'&retmax=50'\n\nlink_list = urllib.request.urlopen(search_url).read().decode('utf-8')\nlink_list\n```", "```py\nimport json\nresult = json.loads( link_list )\nid_list = ','.join( result['esearchresult']['idlist'] )\n\nsummary_url = f'http://eutils.ncbi.nlm.nih.gov/entrez//eutils/esummary.fcgi?db=pubmed&id={id_list}&retmode=json'\n\nsummary_list = urllib.request.urlopen(summary_url).read().decode('utf-8')\n```", "```py\nsummary = json.loads( summary_list )\nsummary['result']['37047528']\n```", "```py\nuid = [ x for x in summary['result'] if x != 'uids' ]\njournals = [ summary['result'][x]['fulljournalname'] for x in summary['result'] if x != 'uids' ]\ntitles = [ summary['result'][x]['title'] for x in summary['result'] if x != 'uids' ]\nfirst_authors = [ summary['result'][x]['sortfirstauthor'] for x in summary['result'] if x != 'uids' ]\nlast_authors = [ summary['result'][x]['lastauthor'] for x in summary['result'] if x != 'uids' ]\nlinks = [ summary['result'][x]['elocationid'] for x in summary['result'] if x != 'uids' ]\npubdates = [ summary['result'][x]['pubdate'] for x in summary['result'] if x != 'uids' ]\n\nlinks = [ re.sub('doi:\\s','http://dx.doi.org/',x) for x in links ]\nresults_df = pd.DataFrame( {'ID':uid,'Journal':journals,'PublicationDate':pubdates,'Title':titles,'URL':links,'FirstAuthor':first_authors,'LastAuthor':last_authors} )\n```", "```py\n'**uid**','**pubdate**','**epubdate**','**source**','**authors**','**lastauthor**','**title**',\n'**sorttitle**','**volume**','**issue**','**pages**','**lang**','**nlmuniqueid**','**issn**',\n'**essn**','**pubtype**','**recordstatus**','**pubstatus**','**articleids**','**history**',\n'**references**','**attributes**','**pmcrefcount**','**fulljournalname**','**elocationid**',\n'**doctype**','**srccontriblist**','**booktitle**','**medium**','**edition**',\n'**publisherlocation**','**publishername**','**srcdate**','**reportnumber**',\n'**availablefromurl**','**locationlabel**','**doccontriblist**','**docdate**',\n'**bookname**','**chapter**','**sortpubdate**','**sortfirstauthor**','**vernaculartitle**'\n```", "```py\nfrom bs4 import BeautifulSoup\nimport lxml\nimport pandas as pd\n\nabstract_url = f'http://eutils.ncbi.nlm.nih.gov/entrez//eutils/efetch.fcgi?db=pubmed&id={id_list}'\nabstract_ = urllib.request.urlopen(abstract_url).read().decode('utf-8')\nabstract_bs = BeautifulSoup(abstract_,features=\"xml\")\n\narticles_iterable = abstract_bs.find_all('PubmedArticle')\n\n# Abstracts\nabstract_texts = [ x.find('AbstractText').text for x in articles_iterable ]\n\n# Conflict of Interest statements\ncoi_texts = [ x.find('CoiStatement').text if x.find('CoiStatement') is not None else '' for x in articles_iterable ]\n\n# MeSH terms\nmeshheadings_all = list()\nfor article in articles_iterable:\n  result = article.find('MeshHeadingList').find_all('MeshHeading')\n  meshheadings_all.append( [ x.text for x in result ] )\n\n# ReferenceList\nreferences_all = list()\nfor article in articles_:\n  if article.find('ReferenceList') is not None:\n    result = article.find('ReferenceList').find_all('Citation')\n    references_all.append( [ x.text for x in result ] )\n  else:\n    references_all.append( [] )\n\nresults_table = pd.DataFrame( {'COI':coi_texts, 'Abstract':abstract_texts, 'MeSH_Terms':meshheadings_all, 'References':references_all} )\n```", "```py\nefetch_url = f'http://eutils.ncbi.nlm.nih.gov/entrez//eutils/efetch.fcgi?db=pubmed&id={id_list}'\nefetch_result = urllib.request.urlopen( efetch_url ).read().decode('utf-8')\nefetch_bs = BeautifulSoup(efetch_result,features=\"xml\")\n\ntags = efetch_bs.find_all()\n\nfor tag in tags:\n  print(tag)\n```", "```py\nimport urllib.request\nimport json\n\nid_ = '37055458'\nelink_url = f'http://eutils.ncbi.nlm.nih.gov/entrez//eutils/elink.fcgi?db=pubmed&id={id_}&retmode=json&cmd=neighbor_score'\nelinks = urllib.request.urlopen(elink_url).read().decode('utf-8')\n\nelinks_json = json.loads( elinks )\n\nids_=[];score_=[];\nall_links = elinks_json['linksets'][0]['linksetdbs'][0]['links']\nfor link in all_links:\n  [ (ids_.append( link['id'] ),score_.append( link['score'] )) for id,s in link.items() ]\n\npd.DataFrame( {'id':ids_,'score':score_} ).drop_duplicates(['id','score'])\n```", "```py\nid_ = '37055458'\nelink_url = f'http://eutils.ncbi.nlm.nih.gov/entrez//eutils/elink.fcgi?db=pubmed&id={id_}&retmode=json&cmd=prlinks'\nelinks = urllib.request.urlopen(elink_url).read().decode('utf-8')\n\nelinks_json = json.loads( elinks )\n\n[ x['url']['value'] for x in elinks_json['linksets'][0]['idurllist'][0]['objurls'] ]\n```", "```py\nid_list = '37055458,574140'\nelink_url = f'http://eutils.ncbi.nlm.nih.gov/entrez//eutils/elink.fcgi?db=pubmed&id={id_list}&retmode=json&cmd=prlinks'\nelinks = urllib.request.urlopen(elink_url).read().decode('utf-8')\n\nelinks_json = json.loads( elinks )\n\nelinks_json\nurls_ = elinks_json['linksets'][0]['idurllist']\nfor url_ in urls_:\n  [ print( url_['id'], x['url']['value'] ) for x in url_['objurls'] ]\n```", "```py\nimport pandas as pd\nimport time\nfrom bs4 import BeautifulSoup\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nimport itertools\nfrom collections import Counter\nfrom numpy import array_split\nfrom urllib.request import urlopen\n\nclass Searcher:\n    # Any instance of searcher will search for the terms and return the number of results on a per year basis #\n    def __init__(self, start_, end_, term_, **kwargs):\n        self.raw_ = input\n        self.name_ = 'searcher'\n        self.description_ = 'searcher'\n        self.duration_ = end_ - start_\n        self.start_ = start_\n        self.end_ = end_\n        self.term_ = term_\n        self.search_results = list()\n        self.count_by_year = list()\n        self.options = list()\n\n        # Parse keyword arguments\n\n        if 'count' in kwargs and kwargs['count'] == 1:\n            self.options = 'rettype=count'\n\n        if 'retmax' in kwargs:\n            self.options = f'retmax={kwargs[\"retmax\"]}'\n\n        if 'run' in kwargs and kwargs['run'] == 1:\n            self.do_search()\n            self.parse_results()\n\n    def do_search(self):\n        datestr_ = [self.start_ + x for x in range(self.duration_)]\n        options = \"\".join(self.options)\n        for year in datestr_:\n            this_url = f'http://eutils.ncbi.nlm.nih.gov/entrez//eutils/esearch.fcgi/' + \\\n                       f'?db=pubmed&term={self.term_}' + \\\n                       f'&mindate={year}&maxdate={year + 1}&{options}'\n            print(this_url)\n            self.search_results.append(\n                urlopen(this_url).read().decode('utf-8'))\n            time.sleep(.33)\n\n    def parse_results(self):\n        for result in self.search_results:\n            xml_ = BeautifulSoup(result, features=\"xml\")\n            self.count_by_year.append(xml_.find('Count').text)\n            self.ids = [id.text for id in xml_.find_all('Id')]\n\n    def __repr__(self):\n        return repr(f'Search PubMed from {self.start_} to {self.end_} with search terms {self.term_}')\n\n    def __str__(self):\n        return self.description\n\n# Create a list which will contain searchers, that retrieve results for each of the search queries\nsearchers = list()\nsearchers.append(Searcher(2022, 2023, 'CEO[cois]+OR+CTO[cois]+OR+CSO[cois]', run=1, retmax=10000))\nsearchers.append(Searcher(2021, 2022, 'CEO[cois]+OR+CTO[cois]+OR+CSO[cois]', run=1, retmax=10000))\nsearchers.append(Searcher(2020, 2021, 'CEO[cois]+OR+CTO[cois]+OR+CSO[cois]', run=1, retmax=10000))\nsearchers.append(Searcher(2019, 2020, 'CEO[cois]+OR+CTO[cois]+OR+CSO[cois]', run=1, retmax=10000))\nsearchers.append(Searcher(2018, 2019, 'CEO[cois]+OR+CTO[cois]+OR+CSO[cois]', run=1, retmax=10000))\n\n# Create a dictionary to store keywords for all articles from a particular year\nkeywords_dict = dict()\n\n# Each searcher obtained results for a particular start and end year\n# Iterate over searchers\nfor this_search in searchers:\n\n    # Split the results from one search into batches for URL formatting\n    chunk_size = 200\n    batches = array_split(this_search.ids, len(this_search.ids) // chunk_size + 1)\n\n    # Create a dict key for this searcher object based on the years of coverage\n    this_dict_key = f'{this_search.start_}to{this_search.end_}'\n\n    # Each value in the dictionary will be a list that gets appended with keywords for each article\n    keywords_all = list()\n\n    for this_batch in batches:\n        ids_ = ','.join(this_batch)\n\n        # Pull down the website containing XML for all the results in a batch\n        abstract_url = f'http://eutils.ncbi.nlm.nih.gov/entrez//eutils/efetch.fcgi?db=pubmed&id={ids_}'\n\n        abstract_ = urlopen(abstract_url).read().decode('utf-8')\n        abstract_bs = BeautifulSoup(abstract_, features=\"xml\")\n        articles_iterable = abstract_bs.find_all('PubmedArticle')\n\n        # Iterate over all of the articles from the website\n        for article in articles_iterable:\n            result = article.find_all('Keyword')\n            if result is not None:\n                keywords_all.append([x.text for x in result])\n            else:\n                keywords_all.append([])\n\n        # Take a break between batches!\n        time.sleep(1)\n\n    # Once all the keywords are assembled for a searcher, add them to the dictionary\n    keywords_dict[this_dict_key] = keywords_all\n\n    # Print the key once it's been dumped to the pickle\n    print(this_dict_key)\n\n# Limit to words that appeared approx five times or more in any given year\n\nmapping_ = {'2018to2019':2018,'2019to2020':2019,'2020to2021':2020,'2021to2022':2021,'2022to2023':2022}\nglobal_word_list = list()\n\nfor key_,value_ in keywords_dict.items():\n  Ntitles = len( value_ )\n  flattened_list = list( itertools.chain(*value_) )\n\n  flattened_list = [ x.lower() for x in flattened_list ]\n  counter_ = Counter( flattened_list )\n  words_this_year = [ ( item , frequency/Ntitles , mapping_[key_] ) for item, frequency in counter_.items() if frequency/Ntitles >= .005 ]\n  global_word_list.extend(words_this_year)\n\n# Plot results as clustermap\n\nglobal_word_df = pd.DataFrame(global_word_list)\nglobal_word_df.columns = ['word', 'frequency', 'year']\npivot_df = global_word_df.loc[:, ['word', 'year', 'frequency']].pivot(index=\"word\", columns=\"year\",\n                                                                    values=\"frequency\").fillna(0)\n\npivot_df.drop('covid-19', axis=0, inplace=True)\npivot_df.drop('sars-cov-2', axis=0, inplace=True)\n\nsns.set(font_scale=0.7)\nplt.figure(figsize=(22, 2))\nres = sns.clustermap(pivot_df, col_cluster=False, yticklabels=True, cbar=True)\n```"]