- en: From RAGs to Riches
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/from-rags-to-riches-53ba89087966?source=collection_archive---------4-----------------------#2023-10-25](https://towardsdatascience.com/from-rags-to-riches-53ba89087966?source=collection_archive---------4-----------------------#2023-10-25)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 10 Applications of vector search to deeply understand your data and models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@jacob_marks?source=post_page-----53ba89087966--------------------------------)[![Jacob
    Marks, Ph.D.](../Images/94d9832b8706d1044e3195386613bfab.png)](https://medium.com/@jacob_marks?source=post_page-----53ba89087966--------------------------------)[](https://towardsdatascience.com/?source=post_page-----53ba89087966--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----53ba89087966--------------------------------)
    [Jacob Marks, Ph.D.](https://medium.com/@jacob_marks?source=post_page-----53ba89087966--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ff7dc0c0eae92&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffrom-rags-to-riches-53ba89087966&user=Jacob+Marks%2C+Ph.D.&userId=f7dc0c0eae92&source=post_page-f7dc0c0eae92----53ba89087966---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----53ba89087966--------------------------------)
    ·10 min read·Oct 25, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F53ba89087966&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffrom-rags-to-riches-53ba89087966&user=Jacob+Marks%2C+Ph.D.&userId=f7dc0c0eae92&source=-----53ba89087966---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F53ba89087966&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffrom-rags-to-riches-53ba89087966&source=-----53ba89087966---------------------bookmark_footer-----------)![](../Images/bdbf65ccdee0abe12ca276688bfdacda.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Artistic rendering of vector search for data exploration. Image generated by
    DALLE-3.
  prefs: []
  type: TYPE_NORMAL
- en: As large language models (LLMs) have eaten the world, vector search engines
    have tagged along for the ride. Vector databases form the foundation of the long-term
    memory systems for LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: By efficiently finding relevant information to pass in as context to the language
    model, vector search engines can provide up-to-date information beyond the training
    cutoff and enhance the quality of the model’s output without fine-tuning. This
    process, commonly referred to as retrieval augmented generation (RAG), has thrust
    the once-esoteric algorithmic challenge of approximate nearest neighbor (ANN)
    search into the spotlight!
  prefs: []
  type: TYPE_NORMAL
- en: Amidst all of the commotion, one could be forgiven for thinking that vector
    search engines are inextricably linked to large language models. But there’s so
    much more to the story. Vector search has a plethora of powerful applications
    that go well beyond improving RAG for LLMs!
  prefs: []
  type: TYPE_NORMAL
- en: In this article, I will show you ten of my favorite uses of vector search for
    data understanding, data exploration, model interpretability and more.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are the applications we will cover, in roughly increasing order of complexity:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Image Similarity Search](#e595)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Reverse Image Search](#dd35)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Object Similarity Search](#26bd)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Robust OCR Document Search](#c20f)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Semantic Search](#8c7d)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Cross-modal Retrieval](#23f9)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Probing Perceptual Similarity](#c565)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Comparing Model Representations](#e42e)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Concept Interpolation](#391f)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Concept Space Traversal](#6967)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Image Similarity Search
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/c65fcd3032940efee4d09558aface781.png)'
  prefs: []
  type: TYPE_IMG
- en: '*Image similarity search on images from the* [*Oxford-IIIT Pet Dataset*](https://www.robots.ox.ac.uk/~vgg/data/pets/)
    *(*[*LICENSE*](https://www.robots.ox.ac.uk/~vgg/data/pets/#:~:text=License)*).
    Image courtesy of the author.*'
  prefs: []
  type: TYPE_NORMAL
- en: Perhaps the simplest place to start is image similarity search. In this task,
    you have a dataset consisting of images — this can be anything from a personal
    photo album to a massive repository of billions of images captured by thousands
    of distributed cameras over the course of years.
  prefs: []
  type: TYPE_NORMAL
- en: 'The setup is simple: compute embeddings for every image in this dataset, and
    generate a vector index out of these embedding vectors. After this initial batch
    of computation, no further inference is required. A great way to explore the structure
    of your dataset is to select an image from the dataset and query the vector index
    for the `k` nearest neighbors — the most similar images. This can provide an intuitive
    sense for how densely the space of images is populated around query images.'
  prefs: []
  type: TYPE_NORMAL
- en: For more information and working code, see [here](https://docs.voxel51.com/user_guide/brain.html#image-similarity).
  prefs: []
  type: TYPE_NORMAL
- en: Reverse Image Search
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/799b67f1470b0370e9d9fda684c7ba8c.png)'
  prefs: []
  type: TYPE_IMG
- en: '*Reverse image search on an* [*image from Unsplash*](https://images.unsplash.com/photo-1568034097584-a3063e104ac3?ixlib=rb-4.0.3&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D&auto=format&fit=crop&w=3464&q=80)
    *(courtesy Mladen Šćekić) against the* [*Oxford-IIIT Pet Dataset*](https://www.robots.ox.ac.uk/~vgg/data/pets/)*.
    Image courtesy of the author.*'
  prefs: []
  type: TYPE_NORMAL
- en: In a similar vein, a natural extension of image similarity search is to find
    the most similar images within the dataset to an *external* image. This can be
    an image from your local filesystem, or an image from the internet!
  prefs: []
  type: TYPE_NORMAL
- en: To perform a reverse image search, you create the vector index for the dataset
    as in the image similarity search example. The difference comes at run-time, when
    you compute the embedding for the query image, and then query the vector database
    with this vector.
  prefs: []
  type: TYPE_NORMAL
- en: For more information and working code, see [here](https://github.com/jacobmarks/reverse-image-search-plugin).
  prefs: []
  type: TYPE_NORMAL
- en: Object Similarity Search
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/6b1842613c97550a31e177d205bec030.png)'
  prefs: []
  type: TYPE_IMG
- en: '*Object similarity search for sheep in the* [*COCO-2017 dataset*](https://cocodataset.org/#home)*’s
    validation split (*[*LICENSE*](https://viso.ai/computer-vision/coco-dataset/#:~:text=the%20MS%20COCO%20images%20dataset%20is%20licensed%20under%20a%20Creative%20Commons%20Attribution%204.0%20License)*).
    Image courtesy of the author.*'
  prefs: []
  type: TYPE_NORMAL
- en: If you want to delve deeper into the content *within* the images, then object,
    or “patch” similarity search may be what you’re after. One example of this is
    person re-identification, where you have a single image with a person of interest
    in it, and you want to find all instances of that person across your dataset.
  prefs: []
  type: TYPE_NORMAL
- en: The person may only take up small portions of each image, so the embeddings
    for the entire images they are in might depend strongly on the other content in
    these images. For instance, there might be *multiple* people in an image.
  prefs: []
  type: TYPE_NORMAL
- en: A better solution is to treat each object detection patch as if it were a separate
    entity and compute an embedding for each. Then, create a vector index with these
    patch embeddings, and run a similarity search against a patch of the person you
    want to re-identify. As a starting point you may want to try using a ResNet model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Two subtleties here:'
  prefs: []
  type: TYPE_NORMAL
- en: In the vector index, you need to store metadata that maps each patch back to
    its corresponding image in the dataset.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You will need to run an object detection model to generate these detection patches
    before instantiating the index. You may also want to only compute patch embeddings
    for certain classes of objects, like `person`, and not others — `chair`, `table`,
    etc.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For more information and working code, see [here](https://docs.voxel51.com/user_guide/brain.html#object-similarity).
  prefs: []
  type: TYPE_NORMAL
- en: Robust OCR Document Search
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/2df3cf0f279735309d732e9f0d9ff25e.png)'
  prefs: []
  type: TYPE_IMG
- en: '*Fuzzy/semantic search through blocks of text generated by the Tesseract OCR
    engine on the pages of my Ph.D. thesis. Embeddings computed using GTE-base model.
    Image courtesy of the author.*'
  prefs: []
  type: TYPE_NORMAL
- en: Optical Character Recognition (OCR) is a technique that allows you to digitize
    documents like handwritten notes, old journal articles, medical records, and those
    love letters squirreled away in your closet. OCR engines like [Tesseract](https://github.com/tesseract-ocr/tesseract)
    and [PaddleOCR](https://learnopencv.com/optical-character-recognition-using-paddleocr/)
    work by identifying individual characters and symbols in images and creating contiguous
    “blocks” of text — think paragraphs.
  prefs: []
  type: TYPE_NORMAL
- en: Once you have this text, you can then perform traditional natural language keyword
    searches over the predicted blocks of text, as illustrated [here](https://github.com/jacobmarks/keyword-search-plugin).
    However, this method of search is susceptible to single-character errors. If the
    OCR engine accidentally recognizes an “l” as a “1”, a keyword search for “control”
    would fail (how about that irony!).
  prefs: []
  type: TYPE_NORMAL
- en: We can overcome this challenge using vector search! Embed the blocks of text
    using a text embedding model like [GTE-base](https://huggingface.co/thenlper/gte-base)
    from Hugging Face’s [Sentence Transformers](https://huggingface.co/sentence-transformers)
    library, and create a vector index. We can then perform fuzzy and/or semantic
    search across our digitized documents by embedding the search text and querying
    the index. At a high level, the blocks of text within these documents are analogous
    to the object detection patches in object similarity searches!
  prefs: []
  type: TYPE_NORMAL
- en: For more information and working code, see [here](https://github.com/jacobmarks/semantic-document-search-plugin).
  prefs: []
  type: TYPE_NORMAL
- en: Semantic Search
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/8f8c589cf85a3bddb18ab437d7ffff8b.png)'
  prefs: []
  type: TYPE_IMG
- en: '*Semantic image search using natural language on the COCO 2017 validation split.
    Image courtesy of the author.*'
  prefs: []
  type: TYPE_NORMAL
- en: With multimodal models, we can extend the notion of semantic search from text
    to images. Models like [CLIP](https://github.com/openai/CLIP), [OpenCLIP](https://github.com/mlfoundations/open_clip),
    and [MetaCLIP](https://github.com/facebookresearch/metaclip) were trained to find
    common representations of images and their captions, so that the embedding vector
    for an image of a dog would be very similar to the embedding vector for the text
    prompt “a photo of a dog”.
  prefs: []
  type: TYPE_NORMAL
- en: This means that it is sensible (i.e. we are “allowed”) to create a vector index
    out of the CLIP embeddings for the images in our dataset and then run a vector
    search query against this vector database where the query vector is the CLIP embedding
    of a *text prompt*.
  prefs: []
  type: TYPE_NORMAL
- en: 💡By treating the individual frames in a video as images and adding each frame’s
    embedding to a vector index, you can also [semantically search through videos](https://medium.com/voxel51/a-google-search-experience-for-computer-vision-data-voxel51-a9ee41390986#:~:text=Find%20video%20frames%20with%20cars%20in%20an%20intersection)!
  prefs: []
  type: TYPE_NORMAL
- en: For more information and working code, see [here](https://docs.voxel51.com/user_guide/brain.html#text-similarity).
  prefs: []
  type: TYPE_NORMAL
- en: Cross-modal Retrieval
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Cross-modal retrieval of images matching an input audio file of a train. Implemented
    using ImageBind with a Qdrant vector index, on the COCO 2017 validation split.
    Video courtesy of the author.*'
  prefs: []
  type: TYPE_NORMAL
- en: In a sense, semantically searching through a dataset of images is a form of
    cross-modal retrieval. One way of conceptualizing it is that we are retrieving
    images corresponding to a text query. With models like [ImageBind](https://github.com/facebookresearch/ImageBind),
    we can take this a step further!
  prefs: []
  type: TYPE_NORMAL
- en: 'ImageBind embeds data from six different modalities in the same embedding space:
    images, text, audio, depth, thermal, and inertial measurement unit (IMU). That
    means that we can generate a vector index for data in *any* of these modalities
    and query this index with a sample of any other of these modalities. For instance,
    we can take an audio clip of a car honking and retrieve all images of cars!'
  prefs: []
  type: TYPE_NORMAL
- en: For more information and working code, see [here](https://github.com/jacobmarks/audio-retrieval-plugin).
  prefs: []
  type: TYPE_NORMAL
- en: Probing Perceptual Similarity
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One very important part of the vector search story which we have only glossed
    over thus far is the *model*. The elements in our vector index are embeddings
    from a model. These embeddings can be the final output of a tailored embedding
    model, or they can be hidden or *latent* representations from a model trained
    on another task like classification.
  prefs: []
  type: TYPE_NORMAL
- en: Regardless, the model we use to embed our samples can have a substantial impact
    on which samples are deemed most similar to which other samples. A CLIP model
    captures semantic concepts, but struggles to represent structural information
    within images. A ResNet model on the other hand is very good at representing similarity
    in structure and layout, operating on the level of pixels and patches. Then there
    are embedding models like [DreamSim](https://dreamsim-nights.github.io/), which
    aim to bridge the gap and capture mid-level similarity — aligning the model’s
    notion of similarity with what is perceived by humans.
  prefs: []
  type: TYPE_NORMAL
- en: Vector search provides a way for us to probe how a model is “seeing” the world.
    By creating a separate vector index for each model we are interested in (on the
    same data), we can rapidly develop an intuition for how different models are representing
    data under the hood, so to speak.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is an example showcasing similarity searches with CLIP, ResNet, and DreamSim
    model embeddings for the same query image on the NIGHTS dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9def889010a979d55035d834539bb6fb.png)'
  prefs: []
  type: TYPE_IMG
- en: Similarity search with ResNet50 embeddings on an image in the [NIGHTS dataset](https://dreamsim-nights.github.io/)
    (Images generated by Stable Diffusion — [MIT RAIL LICENSE](https://stability.ai/blog/stable-diffusion-public-release)).
    ResNet models operate on the level of pixels and patches. Hence the retrieved
    images are structurally similar to the query but not always semantically similar.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f7acd455ddae7d434d94270b8d965e60.png)'
  prefs: []
  type: TYPE_IMG
- en: Similarity search with CLIP embeddings on the same query image. CLIP models
    respect the underlying semantics of the images but not their layout.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0abe740139d3f32a9a563ab8da5dea5c.png)'
  prefs: []
  type: TYPE_IMG
- en: Similarity search with DreamSim embeddings on the same query image. DreamSim
    bridges the gap, seeking the best mid-level similarity compromise between semantic
    and structural features.
  prefs: []
  type: TYPE_NORMAL
- en: For more information and working code, see [here](https://medium.com/voxel51/teaching-androids-to-dream-of-sheep-18d72f44f2b).
  prefs: []
  type: TYPE_NORMAL
- en: Comparing Model Representations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/e115d4ca7f1bb4808737096b7c24fcea.png)'
  prefs: []
  type: TYPE_IMG
- en: '*Heuristic comparison of ResNet50 and CLIP model representations of the NIGHTS
    dataset. ResNet embeddings have been reduced to 2D using UMAP. Selecting a point
    in the embeddings plot and highlighting nearby samples, we can see how ResNet
    captures compositional and palette similarity, not semantic similarity. Running
    a vector search on the selected sample with CLIP embeddings, we can see that the
    most samples according to CLIP are not localized according to ResNet.*'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can gain new insight into the differences between two models by combining
    vector search and dimensionality reduction techniques like uniform manifold approximation
    ([UMAP](https://umap-learn.readthedocs.io/en/latest/)). Here’s how:'
  prefs: []
  type: TYPE_NORMAL
- en: Each model’s embeddings contain information about how the model is representing
    the data. Using UMAP (or t-SNE or PCA), we can generate lower dimensional (either
    2D or 3D) representations of the embeddings from model1\. By doing so, we sacrifice
    some detail, but hopefully preserve some information about which samples are perceived
    as similar to other samples. What we gain is the ability to visualize this data.
  prefs: []
  type: TYPE_NORMAL
- en: With model1’s embedding visualization as a backdrop, we can choose a point in
    this plot and perform a vector search query on that sample with respect to model2’s
    embeddings. You can then see where within the 2D visualization the retrieved points
    lie!
  prefs: []
  type: TYPE_NORMAL
- en: The example above uses the same NIGHTS dataset as in the last section, visualizing
    ResNet embeddings, which capture more compositional and structural similarity,
    and performing a similarity search with CLIP (semantic) embeddings.
  prefs: []
  type: TYPE_NORMAL
- en: Concept Interpolation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/e06bf1b4ad8310f8c3379048991f5cd2.png)'
  prefs: []
  type: TYPE_IMG
- en: '*Interpolation between the concepts “husky” and “chihuahua” with CLIP embeddings
    on the Oxford-IIIT Pet Dataset*'
  prefs: []
  type: TYPE_NORMAL
- en: We’re reaching the end of the ten applications, but lucky for you I saved a
    few of the best for last. So far, the only vectors we’ve worked with are embeddings
    — the vector index is populated with embeddings, and the query vectors are also
    embeddings. But sometimes there is additional structure in the *space* of embeddings
    that we can leverage to interact with our data more dynamically.
  prefs: []
  type: TYPE_NORMAL
- en: 'One example of such a dynamic interaction is something I like to call “concept
    interpolation”. Here’s how it works: take a dataset of images and generate a vector
    index using a multimodal model (text and image). Pick two text prompts like “sunny”
    and “rainy”, which stand in for concepts, and set a value `alpha` in the range
    `[0,1]`. We can generate the embedding vectors for each text concept, and add
    these vectors in a linear combination specified by `alpha`. We then normalize
    the vector and use it as the query to our vector index of image embeddings.'
  prefs: []
  type: TYPE_NORMAL
- en: Because we are linearly interpolating between the embedding vectors for the
    two text prompts (concepts), we are in a very loose sense interpolating between
    the concepts themselves! We can dynamically change `alpha` and query our vector
    database each time there is an interaction.
  prefs: []
  type: TYPE_NORMAL
- en: '💡This notion of concept interpolation is experimental (read: not always a well
    defined operation). I find it works best when the text prompts are conceptually
    related and the dataset is diverse enough to have different results for different
    places along the interpolation spectrum.'
  prefs: []
  type: TYPE_NORMAL
- en: For more information and working code, see [here](https://github.com/jacobmarks/concept-interpolation).
  prefs: []
  type: TYPE_NORMAL
- en: Concept Space Traversal
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/ac25af2786564b86cc7934e50f36eada.png)'
  prefs: []
  type: TYPE_IMG
- en: '*Traversing the space of “concepts” by moving in the direction of various text
    prompts via their embeddings, illustrated for the test split of the COCO 2017
    dataset. Images and text embedded with a CLIP model. Image courtesy of the author.*'
  prefs: []
  type: TYPE_NORMAL
- en: Last, but certainly not least, we have what I like to call “concept space traversal”.
    As with concept interpolation, start with a dataset of images and generate embeddings
    with a multimodal model like CLIP. Next, select an image from the dataset. This
    image will serve as your starting point, from which you will be “traversing” the
    space of concepts.
  prefs: []
  type: TYPE_NORMAL
- en: From there, you can define a direction you want to move in by providing a text
    string as a stand-in for a concept. Set the magnitude of the “step” you want to
    take in that direction, and that text string’s embedding vector (with a multiplicative
    coefficient) will be added to the embedding vector of the initial image. The “destination”
    vector will be used to query the vector database. You can add arbitrarily many
    concepts in arbitrary quantities, and watch as the set of retrieved images updates
    in real time.
  prefs: []
  type: TYPE_NORMAL
- en: As with “concept interpolation”, this is not always a strictly well-defined
    process. However, I find it to be captivating, and to perform reasonably well
    when the coefficient applied to the text embeddings is high enough that they are
    sufficiently taken into account.
  prefs: []
  type: TYPE_NORMAL
- en: For more information and working code, see [here](https://github.com/jacobmarks/concept-space-traversal-plugin).
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Vector search engines are incredibly powerful tools. Sure, they are the stars
    of the best show in town, *RAG*-time. But vector databases are far more versatile
    than that. They enable deeper understanding of data, give insights into how models
    represent that data, and offer new avenues for us to interact with our data.
  prefs: []
  type: TYPE_NORMAL
- en: Vector databases are not bound to LLMs. They prove useful whenever embeddings
    are involved, and embeddings lie right at the intersection of *model* and *data*.
    The more rigorously we understand the structure of embedding spaces, the more
    dynamic and pervasive our vector search-enabled data and model interactions will
    become.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you found this post interesting, you may also want to check out these vector
    search powered posts:'
  prefs: []
  type: TYPE_NORMAL
- en: 🔗 [How I Turned My Company’s Docs into a Searchable Database with OpenAI](/how-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 🔗 [How I Turned ChatGPT into an SQL-Like Translator for Image and Video Datasets](/how-i-turned-chatgpt-into-an-sql-like-translator-for-image-and-video-datasets-7b22b318400a)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 🔗 [What I Learned Pushing Prompt Engineering to the Limit](/what-i-learned-pushing-prompt-engineering-to-the-limit-c40f0740641f)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
