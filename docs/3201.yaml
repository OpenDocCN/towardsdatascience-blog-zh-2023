- en: From RAGs to Riches
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ä» RAG åˆ°è´¢å¯Œ
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/from-rags-to-riches-53ba89087966?source=collection_archive---------4-----------------------#2023-10-25](https://towardsdatascience.com/from-rags-to-riches-53ba89087966?source=collection_archive---------4-----------------------#2023-10-25)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://towardsdatascience.com/from-rags-to-riches-53ba89087966?source=collection_archive---------4-----------------------#2023-10-25](https://towardsdatascience.com/from-rags-to-riches-53ba89087966?source=collection_archive---------4-----------------------#2023-10-25)
- en: 10 Applications of vector search to deeply understand your data and models
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å‘é‡æœç´¢åœ¨æ·±å…¥ç†è§£æ‚¨çš„æ•°æ®å’Œæ¨¡å‹ä¸­çš„ 10 ç§åº”ç”¨
- en: '[](https://medium.com/@jacob_marks?source=post_page-----53ba89087966--------------------------------)[![Jacob
    Marks, Ph.D.](../Images/94d9832b8706d1044e3195386613bfab.png)](https://medium.com/@jacob_marks?source=post_page-----53ba89087966--------------------------------)[](https://towardsdatascience.com/?source=post_page-----53ba89087966--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----53ba89087966--------------------------------)
    [Jacob Marks, Ph.D.](https://medium.com/@jacob_marks?source=post_page-----53ba89087966--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@jacob_marks?source=post_page-----53ba89087966--------------------------------)[![Jacob
    Marks, Ph.D.](../Images/94d9832b8706d1044e3195386613bfab.png)](https://medium.com/@jacob_marks?source=post_page-----53ba89087966--------------------------------)[](https://towardsdatascience.com/?source=post_page-----53ba89087966--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----53ba89087966--------------------------------)
    [Jacob Marks, Ph.D.](https://medium.com/@jacob_marks?source=post_page-----53ba89087966--------------------------------)'
- en: Â·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Â·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ff7dc0c0eae92&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffrom-rags-to-riches-53ba89087966&user=Jacob+Marks%2C+Ph.D.&userId=f7dc0c0eae92&source=post_page-f7dc0c0eae92----53ba89087966---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----53ba89087966--------------------------------)
    Â·10 min readÂ·Oct 25, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F53ba89087966&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffrom-rags-to-riches-53ba89087966&user=Jacob+Marks%2C+Ph.D.&userId=f7dc0c0eae92&source=-----53ba89087966---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[å…³æ³¨](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ff7dc0c0eae92&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffrom-rags-to-riches-53ba89087966&user=Jacob+Marks%2C+Ph.D.&userId=f7dc0c0eae92&source=post_page-f7dc0c0eae92----53ba89087966---------------------post_header-----------)
    å‘è¡¨åœ¨ [Towards Data Science](https://towardsdatascience.com/?source=post_page-----53ba89087966--------------------------------)
    Â·10 min readÂ·2023å¹´10æœˆ25æ—¥[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F53ba89087966&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffrom-rags-to-riches-53ba89087966&user=Jacob+Marks%2C+Ph.D.&userId=f7dc0c0eae92&source=-----53ba89087966---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F53ba89087966&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffrom-rags-to-riches-53ba89087966&source=-----53ba89087966---------------------bookmark_footer-----------)![](../Images/bdbf65ccdee0abe12ca276688bfdacda.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F53ba89087966&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffrom-rags-to-riches-53ba89087966&source=-----53ba89087966---------------------bookmark_footer-----------)![](../Images/bdbf65ccdee0abe12ca276688bfdacda.png)'
- en: Artistic rendering of vector search for data exploration. Image generated by
    DALLE-3.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: å‘é‡æœç´¢ç”¨äºæ•°æ®æ¢ç´¢çš„è‰ºæœ¯æ•ˆæœå›¾ã€‚å›¾åƒç”± DALLE-3 ç”Ÿæˆã€‚
- en: As large language models (LLMs) have eaten the world, vector search engines
    have tagged along for the ride. Vector databases form the foundation of the long-term
    memory systems for LLMs.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å¸­å·å…¨çƒï¼Œå‘é‡æœç´¢å¼•æ“ä¹Ÿéšä¹‹è€Œæ¥ã€‚å‘é‡æ•°æ®åº“æ„æˆäº† LLMs é•¿æœŸè®°å¿†ç³»ç»Ÿçš„åŸºç¡€ã€‚
- en: By efficiently finding relevant information to pass in as context to the language
    model, vector search engines can provide up-to-date information beyond the training
    cutoff and enhance the quality of the modelâ€™s output without fine-tuning. This
    process, commonly referred to as retrieval augmented generation (RAG), has thrust
    the once-esoteric algorithmic challenge of approximate nearest neighbor (ANN)
    search into the spotlight!
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: é€šè¿‡é«˜æ•ˆåœ°æŸ¥æ‰¾ç›¸å…³ä¿¡æ¯å¹¶å°†å…¶ä½œä¸ºä¸Šä¸‹æ–‡è¾“å…¥åˆ°è¯­è¨€æ¨¡å‹ä¸­ï¼Œå‘é‡æœç´¢å¼•æ“å¯ä»¥æä¾›è¶…å‡ºè®­ç»ƒæˆªæ­¢æ—¥æœŸçš„æœ€æ–°ä¿¡æ¯ï¼Œå¹¶åœ¨ä¸è¿›è¡Œå¾®è°ƒçš„æƒ…å†µä¸‹æé«˜æ¨¡å‹è¾“å‡ºçš„è´¨é‡ã€‚è¿™ä¸ªè¿‡ç¨‹ï¼Œé€šå¸¸ç§°ä¸ºæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ï¼Œå°†æ›¾ç»æ™¦æ¶©çš„è¿‘ä¼¼æœ€è¿‘é‚»ï¼ˆANNï¼‰æœç´¢ç®—æ³•æŒ‘æˆ˜æ¨å‘äº†èšå…‰ç¯ä¸‹ï¼
- en: Amidst all of the commotion, one could be forgiven for thinking that vector
    search engines are inextricably linked to large language models. But thereâ€™s so
    much more to the story. Vector search has a plethora of powerful applications
    that go well beyond improving RAG for LLMs!
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ‰€æœ‰çš„å–§åš£ä¸­ï¼Œäººä»¬å¯èƒ½ä¼šè¯¯ä»¥ä¸ºå‘é‡æœç´¢å¼•æ“ä¸å¤§å‹è¯­è¨€æ¨¡å‹ä¸å¯åˆ†å‰²ã€‚ä½†äº‹å®è¿œä¸æ­¢äºæ­¤ã€‚å‘é‡æœç´¢æœ‰è®¸å¤šå¼ºå¤§çš„åº”ç”¨ï¼Œè¿œè¿œè¶…å‡ºäº†æ”¹å–„ RAG å¯¹ LLMs çš„åº”ç”¨ï¼
- en: In this article, I will show you ten of my favorite uses of vector search for
    data understanding, data exploration, model interpretability and more.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘å°†å±•ç¤ºæˆ‘æœ€å–œæ¬¢çš„åç§å‘é‡æœç´¢çš„åº”ç”¨ï¼Œç”¨äºæ•°æ®ç†è§£ã€æ•°æ®æ¢ç´¢ã€æ¨¡å‹è§£é‡Šç­‰ã€‚
- en: 'Here are the applications we will cover, in roughly increasing order of complexity:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥ä¸‹æ˜¯æˆ‘ä»¬å°†æ¶µç›–çš„åº”ç”¨ï¼ŒæŒ‰å¤æ‚åº¦å¤§è‡´é€’å¢çš„é¡ºåºï¼š
- en: '[Image Similarity Search](#e595)'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[å›¾åƒç›¸ä¼¼æ€§æœç´¢](#e595)'
- en: '[Reverse Image Search](#dd35)'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[åå‘å›¾åƒæœç´¢](#dd35)'
- en: '[Object Similarity Search](#26bd)'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[å¯¹è±¡ç›¸ä¼¼æ€§æœç´¢](#26bd)'
- en: '[Robust OCR Document Search](#c20f)'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[å¼ºå¥çš„ OCR æ–‡æ¡£æœç´¢](#c20f)'
- en: '[Semantic Search](#8c7d)'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[è¯­ä¹‰æœç´¢](#8c7d)'
- en: '[Cross-modal Retrieval](#23f9)'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[è·¨æ¨¡æ€æ£€ç´¢](#23f9)'
- en: '[Probing Perceptual Similarity](#c565)'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[æ¢æµ‹æ„ŸçŸ¥ç›¸ä¼¼æ€§](#c565)'
- en: '[Comparing Model Representations](#e42e)'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[æ¯”è¾ƒæ¨¡å‹è¡¨ç¤º](#e42e)'
- en: '[Concept Interpolation](#391f)'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[æ¦‚å¿µæ’å€¼](#391f)'
- en: '[Concept Space Traversal](#6967)'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[æ¦‚å¿µç©ºé—´éå†](#6967)'
- en: Image Similarity Search
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å›¾åƒç›¸ä¼¼æ€§æœç´¢
- en: '![](../Images/c65fcd3032940efee4d09558aface781.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c65fcd3032940efee4d09558aface781.png)'
- en: '*Image similarity search on images from the* [*Oxford-IIIT Pet Dataset*](https://www.robots.ox.ac.uk/~vgg/data/pets/)
    *(*[*LICENSE*](https://www.robots.ox.ac.uk/~vgg/data/pets/#:~:text=License)*).
    Image courtesy of the author.*'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '*å¯¹æ¥è‡ª* [*Oxford-IIIT Pet æ•°æ®é›†çš„å›¾åƒè¿›è¡Œç›¸ä¼¼æ€§æœç´¢*](https://www.robots.ox.ac.uk/~vgg/data/pets/)
    *(*[*LICENSE*](https://www.robots.ox.ac.uk/~vgg/data/pets/#:~:text=License)*).
    å›¾ç‰‡ç”±ä½œè€…æä¾›ã€‚*'
- en: Perhaps the simplest place to start is image similarity search. In this task,
    you have a dataset consisting of images â€” this can be anything from a personal
    photo album to a massive repository of billions of images captured by thousands
    of distributed cameras over the course of years.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: ä¹Ÿè®¸æœ€ç®€å•çš„èµ·ç‚¹æ˜¯å›¾åƒç›¸ä¼¼æ€§æœç´¢ã€‚åœ¨è¿™ä¸ªä»»åŠ¡ä¸­ï¼Œä½ æœ‰ä¸€ä¸ªç”±å›¾åƒç»„æˆçš„æ•°æ®é›†â€”â€”è¿™å¯ä»¥æ˜¯ä¸ªäººç…§ç‰‡é›†ï¼Œä¹Ÿå¯ä»¥æ˜¯æ•°ä»¥äº¿è®¡çš„å›¾åƒä»“åº“ï¼Œè¿™äº›å›¾åƒç”±æ•°åƒä¸ªåˆ†å¸ƒå¼ç›¸æœºæ‹æ‘„ï¼Œå†æ—¶å¤šå¹´ã€‚
- en: 'The setup is simple: compute embeddings for every image in this dataset, and
    generate a vector index out of these embedding vectors. After this initial batch
    of computation, no further inference is required. A great way to explore the structure
    of your dataset is to select an image from the dataset and query the vector index
    for the `k` nearest neighbors â€” the most similar images. This can provide an intuitive
    sense for how densely the space of images is populated around query images.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: è®¾ç½®å¾ˆç®€å•ï¼šè®¡ç®—æ•°æ®é›†ä¸­æ¯å¼ å›¾åƒçš„åµŒå…¥å‘é‡ï¼Œå¹¶ä»è¿™äº›åµŒå…¥å‘é‡ç”Ÿæˆä¸€ä¸ªå‘é‡ç´¢å¼•ã€‚åœ¨è¿™åˆå§‹æ‰¹æ¬¡è®¡ç®—ä¹‹åï¼Œä¸éœ€è¦è¿›ä¸€æ­¥çš„æ¨æ–­ã€‚æ¢ç´¢æ•°æ®é›†ç»“æ„çš„ä¸€ä¸ªå¥½æ–¹æ³•æ˜¯é€‰æ‹©æ•°æ®é›†ä¸­çš„ä¸€å¼ å›¾åƒï¼Œå¹¶æŸ¥è¯¢å‘é‡ç´¢å¼•è·å–
    `k` ä¸ªæœ€è¿‘é‚»â€”â€”æœ€ç›¸ä¼¼çš„å›¾åƒã€‚è¿™å¯ä»¥ç›´è§‚åœ°äº†è§£æŸ¥è¯¢å›¾åƒå‘¨å›´å›¾åƒç©ºé—´çš„å¯†åº¦ã€‚
- en: For more information and working code, see [here](https://docs.voxel51.com/user_guide/brain.html#image-similarity).
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: æ¬²äº†è§£æ›´å¤šä¿¡æ¯å’Œç¤ºä¾‹ä»£ç ï¼Œè¯·è§ [è¿™é‡Œ](https://docs.voxel51.com/user_guide/brain.html#image-similarity)ã€‚
- en: Reverse Image Search
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: åå‘å›¾åƒæœç´¢
- en: '![](../Images/799b67f1470b0370e9d9fda684c7ba8c.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/799b67f1470b0370e9d9fda684c7ba8c.png)'
- en: '*Reverse image search on an* [*image from Unsplash*](https://images.unsplash.com/photo-1568034097584-a3063e104ac3?ixlib=rb-4.0.3&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D&auto=format&fit=crop&w=3464&q=80)
    *(courtesy Mladen Å Ä‡ekiÄ‡) against the* [*Oxford-IIIT Pet Dataset*](https://www.robots.ox.ac.uk/~vgg/data/pets/)*.
    Image courtesy of the author.*'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '*å¯¹æ¥è‡ª* [*Unsplash çš„å›¾åƒè¿›è¡Œåå‘å›¾åƒæœç´¢*](https://images.unsplash.com/photo-1568034097584-a3063e104ac3?ixlib=rb-4.0.3&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D&auto=format&fit=crop&w=3464&q=80)
    *(å›¾ç‰‡ç”± Mladen Å Ä‡ekiÄ‡ æä¾›)*ï¼Œä¸* [*Oxford-IIIT Pet æ•°æ®é›†*](https://www.robots.ox.ac.uk/~vgg/data/pets/)*è¿›è¡Œå¯¹æ¯”ã€‚å›¾ç‰‡ç”±ä½œè€…æä¾›ã€‚*'
- en: In a similar vein, a natural extension of image similarity search is to find
    the most similar images within the dataset to an *external* image. This can be
    an image from your local filesystem, or an image from the internet!
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: ç±»ä¼¼åœ°ï¼Œå›¾åƒç›¸ä¼¼æ€§æœç´¢çš„è‡ªç„¶å»¶ä¼¸æ˜¯æ‰¾åˆ°æ•°æ®é›†ä¸­ä¸*å¤–éƒ¨*å›¾åƒæœ€ç›¸ä¼¼çš„å›¾åƒã€‚è¿™å¯ä»¥æ˜¯æ¥è‡ªæœ¬åœ°æ–‡ä»¶ç³»ç»Ÿçš„å›¾åƒï¼Œä¹Ÿå¯ä»¥æ˜¯æ¥è‡ªäº’è”ç½‘çš„å›¾åƒï¼
- en: To perform a reverse image search, you create the vector index for the dataset
    as in the image similarity search example. The difference comes at run-time, when
    you compute the embedding for the query image, and then query the vector database
    with this vector.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: è¦æ‰§è¡Œåå‘å›¾åƒæœç´¢ï¼Œä½ éœ€è¦åƒå›¾åƒç›¸ä¼¼æ€§æœç´¢ç¤ºä¾‹é‚£æ ·åˆ›å»ºæ•°æ®é›†çš„å‘é‡ç´¢å¼•ã€‚ä¸åŒä¹‹å¤„åœ¨äºè¿è¡Œæ—¶ï¼Œä½ è®¡ç®—æŸ¥è¯¢å›¾åƒçš„åµŒå…¥å‘é‡ï¼Œç„¶åç”¨è¿™ä¸ªå‘é‡æŸ¥è¯¢å‘é‡æ•°æ®åº“ã€‚
- en: For more information and working code, see [here](https://github.com/jacobmarks/reverse-image-search-plugin).
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: æ¬²äº†è§£æ›´å¤šä¿¡æ¯å’Œç¤ºä¾‹ä»£ç ï¼Œè¯·å‚è§ [è¿™é‡Œ](https://github.com/jacobmarks/reverse-image-search-plugin)ã€‚
- en: Object Similarity Search
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å¯¹è±¡ç›¸ä¼¼æ€§æœç´¢
- en: '![](../Images/6b1842613c97550a31e177d205bec030.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6b1842613c97550a31e177d205bec030.png)'
- en: '*Object similarity search for sheep in the* [*COCO-2017 dataset*](https://cocodataset.org/#home)*â€™s
    validation split (*[*LICENSE*](https://viso.ai/computer-vision/coco-dataset/#:~:text=the%20MS%20COCO%20images%20dataset%20is%20licensed%20under%20a%20Creative%20Commons%20Attribution%204.0%20License)*).
    Image courtesy of the author.*'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '*åœ¨* [*COCO-2017 æ•°æ®é›†*](https://cocodataset.org/#home)*çš„éªŒè¯åˆ†å‰²ä¸­è¿›è¡Œç¾Šçš„å¯¹è±¡ç›¸ä¼¼æ€§æœç´¢ (*[*LICENSE*](https://viso.ai/computer-vision/coco-dataset/#:~:text=the%20MS%20COCO%20images%20dataset%20is%20licensed%20under%20a%20Creative%20Commons%20Attribution%204.0%20License)*ï¼‰ã€‚å›¾ç‰‡ç”±ä½œè€…æä¾›ã€‚*'
- en: If you want to delve deeper into the content *within* the images, then object,
    or â€œpatchâ€ similarity search may be what youâ€™re after. One example of this is
    person re-identification, where you have a single image with a person of interest
    in it, and you want to find all instances of that person across your dataset.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä½ æƒ³æ·±å…¥ç ”ç©¶å›¾åƒ*ä¸­çš„*å†…å®¹ï¼Œé‚£ä¹ˆå¯¹è±¡æˆ–â€œè¡¥ä¸â€ç›¸ä¼¼æ€§æœç´¢å¯èƒ½æ­£æ˜¯ä½ æ‰€è¿½æ±‚çš„ã€‚ä¾‹å¦‚ï¼Œäººç‰©å†è¯†åˆ«å°±æ˜¯å…¶ä¸­ä¹‹ä¸€ï¼Œåœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œä½ æœ‰ä¸€å¼ åŒ…å«ç›®æ ‡äººç‰©çš„å•ä¸€å›¾åƒï¼Œä½ æƒ³åœ¨æ•°æ®é›†ä¸­æ‰¾åˆ°æ‰€æœ‰è¯¥äººçš„å®ä¾‹ã€‚
- en: The person may only take up small portions of each image, so the embeddings
    for the entire images they are in might depend strongly on the other content in
    these images. For instance, there might be *multiple* people in an image.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: äººç‰©å¯èƒ½åªå æ®æ¯å¼ å›¾åƒçš„å°éƒ¨åˆ†ï¼Œå› æ­¤è¿™äº›å›¾åƒä¸­åŒ…å«çš„æ•´ä¸ªå›¾åƒçš„åµŒå…¥å¯èƒ½ä¼šå¼ºçƒˆä¾èµ–äºè¿™äº›å›¾åƒä¸­çš„å…¶ä»–å†…å®¹ã€‚ä¾‹å¦‚ï¼Œå›¾åƒä¸­å¯èƒ½æœ‰*å¤šä¸ª*äººã€‚
- en: A better solution is to treat each object detection patch as if it were a separate
    entity and compute an embedding for each. Then, create a vector index with these
    patch embeddings, and run a similarity search against a patch of the person you
    want to re-identify. As a starting point you may want to try using a ResNet model.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: æ›´å¥½çš„è§£å†³æ–¹æ¡ˆæ˜¯å°†æ¯ä¸ªå¯¹è±¡æ£€æµ‹è¡¥ä¸è§†ä¸ºä¸€ä¸ªç‹¬ç«‹çš„å®ä½“ï¼Œå¹¶ä¸ºæ¯ä¸ªè¡¥ä¸è®¡ç®—åµŒå…¥ã€‚ç„¶åï¼Œä½¿ç”¨è¿™äº›è¡¥ä¸åµŒå…¥åˆ›å»ºä¸€ä¸ªå‘é‡ç´¢å¼•ï¼Œå¹¶é’ˆå¯¹ä½ æƒ³è¦é‡æ–°è¯†åˆ«çš„äººçš„è¡¥ä¸è¿›è¡Œç›¸ä¼¼æ€§æœç´¢ã€‚ä½œä¸ºèµ·ç‚¹ï¼Œä½ å¯èƒ½æƒ³å°è¯•ä½¿ç”¨
    ResNet æ¨¡å‹ã€‚
- en: 'Two subtleties here:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œæœ‰ä¸¤ä¸ªç»†å¾®ä¹‹å¤„ï¼š
- en: In the vector index, you need to store metadata that maps each patch back to
    its corresponding image in the dataset.
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: åœ¨å‘é‡ç´¢å¼•ä¸­ï¼Œä½ éœ€è¦å­˜å‚¨å…ƒæ•°æ®ï¼Œå°†æ¯ä¸ªè¡¥ä¸æ˜ å°„å›æ•°æ®é›†ä¸­çš„ç›¸åº”å›¾åƒã€‚
- en: You will need to run an object detection model to generate these detection patches
    before instantiating the index. You may also want to only compute patch embeddings
    for certain classes of objects, like `person`, and not others â€” `chair`, `table`,
    etc.
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ä½ éœ€è¦è¿è¡Œä¸€ä¸ªå¯¹è±¡æ£€æµ‹æ¨¡å‹æ¥ç”Ÿæˆè¿™äº›æ£€æµ‹è¡¥ä¸ï¼Œç„¶åå†å®ä¾‹åŒ–ç´¢å¼•ã€‚ä½ å¯èƒ½è¿˜æƒ³ä»…ä¸ºæŸäº›å¯¹è±¡ç±»åˆ«ï¼ˆå¦‚`person`ï¼‰è®¡ç®—è¡¥ä¸åµŒå…¥ï¼Œè€Œä¸æ˜¯å…¶ä»–ç±»åˆ«â€”â€”`chair`ã€`table`ç­‰ã€‚
- en: For more information and working code, see [here](https://docs.voxel51.com/user_guide/brain.html#object-similarity).
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: æ¬²äº†è§£æ›´å¤šä¿¡æ¯å’Œç¤ºä¾‹ä»£ç ï¼Œè¯·å‚è§ [è¿™é‡Œ](https://docs.voxel51.com/user_guide/brain.html#object-similarity)ã€‚
- en: Robust OCR Document Search
  id: totrans-46
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å¼ºå¤§çš„ OCR æ–‡æ¡£æœç´¢
- en: '![](../Images/2df3cf0f279735309d732e9f0d9ff25e.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2df3cf0f279735309d732e9f0d9ff25e.png)'
- en: '*Fuzzy/semantic search through blocks of text generated by the Tesseract OCR
    engine on the pages of my Ph.D. thesis. Embeddings computed using GTE-base model.
    Image courtesy of the author.*'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '*é€šè¿‡ Tesseract OCR å¼•æ“ç”Ÿæˆçš„åšå£«è®ºæ–‡é¡µé¢ä¸Šçš„æ–‡æœ¬å—è¿›è¡Œæ¨¡ç³Š/è¯­ä¹‰æœç´¢ã€‚ä½¿ç”¨ GTE-base æ¨¡å‹è®¡ç®—çš„åµŒå…¥ã€‚å›¾ç‰‡ç”±ä½œè€…æä¾›ã€‚*'
- en: Optical Character Recognition (OCR) is a technique that allows you to digitize
    documents like handwritten notes, old journal articles, medical records, and those
    love letters squirreled away in your closet. OCR engines like [Tesseract](https://github.com/tesseract-ocr/tesseract)
    and [PaddleOCR](https://learnopencv.com/optical-character-recognition-using-paddleocr/)
    work by identifying individual characters and symbols in images and creating contiguous
    â€œblocksâ€ of text â€” think paragraphs.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: å…‰å­¦å­—ç¬¦è¯†åˆ«ï¼ˆOCRï¼‰æ˜¯ä¸€ç§å…è®¸ä½ æ•°å­—åŒ–æ–‡æ¡£çš„æŠ€æœ¯ï¼Œå¦‚æ‰‹å†™ç¬”è®°ã€æ—§æœŸåˆŠæ–‡ç« ã€åŒ»ç–—è®°å½•ä»¥åŠé‚£äº›è—åœ¨ä½ è¡£æŸœé‡Œçš„æƒ…ä¹¦ã€‚åƒ [Tesseract](https://github.com/tesseract-ocr/tesseract)
    å’Œ [PaddleOCR](https://learnopencv.com/optical-character-recognition-using-paddleocr/)
    è¿™æ ·çš„ OCR å¼•æ“é€šè¿‡è¯†åˆ«å›¾åƒä¸­çš„å•ä¸ªå­—ç¬¦å’Œç¬¦å·å¹¶åˆ›å»ºè¿ç»­çš„â€œæ–‡æœ¬å—â€æ¥å·¥ä½œâ€”â€”æƒ³è±¡æˆæ®µè½ã€‚
- en: Once you have this text, you can then perform traditional natural language keyword
    searches over the predicted blocks of text, as illustrated [here](https://github.com/jacobmarks/keyword-search-plugin).
    However, this method of search is susceptible to single-character errors. If the
    OCR engine accidentally recognizes an â€œlâ€ as a â€œ1â€, a keyword search for â€œcontrolâ€
    would fail (how about that irony!).
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€æ—¦ä½ æœ‰äº†è¿™äº›æ–‡æœ¬ï¼Œä½ å¯ä»¥å¯¹é¢„æµ‹çš„æ–‡æœ¬å—è¿›è¡Œä¼ ç»Ÿçš„è‡ªç„¶è¯­è¨€å…³é”®è¯æœç´¢ï¼Œå¦‚[è¿™é‡Œ](https://github.com/jacobmarks/keyword-search-plugin)æ‰€ç¤ºã€‚ç„¶è€Œï¼Œè¿™ç§æœç´¢æ–¹æ³•å®¹æ˜“å—åˆ°å•å­—ç¬¦é”™è¯¯çš„å½±å“ã€‚å¦‚æœOCRå¼•æ“é”™è¯¯åœ°å°†â€œlâ€è¯†åˆ«ä¸ºâ€œ1â€ï¼Œé‚£ä¹ˆå¯¹â€œcontrolâ€çš„å…³é”®è¯æœç´¢å°†å¤±è´¥ï¼ˆè¿™çœŸæ˜¯æœ‰è¶£ï¼ï¼‰ã€‚
- en: We can overcome this challenge using vector search! Embed the blocks of text
    using a text embedding model like [GTE-base](https://huggingface.co/thenlper/gte-base)
    from Hugging Faceâ€™s [Sentence Transformers](https://huggingface.co/sentence-transformers)
    library, and create a vector index. We can then perform fuzzy and/or semantic
    search across our digitized documents by embedding the search text and querying
    the index. At a high level, the blocks of text within these documents are analogous
    to the object detection patches in object similarity searches!
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥é€šè¿‡ä½¿ç”¨å‘é‡æœç´¢æ¥å…‹æœè¿™ä¸ªæŒ‘æˆ˜ï¼ä½¿ç”¨åƒ[GTE-base](https://huggingface.co/thenlper/gte-base)è¿™æ ·çš„æ–‡æœ¬åµŒå…¥æ¨¡å‹å°†æ–‡æœ¬å—åµŒå…¥ï¼Œå¹¶åˆ›å»ºä¸€ä¸ªå‘é‡ç´¢å¼•ã€‚ç„¶åï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡åµŒå…¥æœç´¢æ–‡æœ¬å¹¶æŸ¥è¯¢è¯¥ç´¢å¼•ï¼Œå¯¹æˆ‘ä»¬çš„æ•°å­—åŒ–æ–‡æ¡£è¿›è¡Œæ¨¡ç³Šå’Œ/æˆ–è¯­ä¹‰æœç´¢ã€‚ä»é«˜å±‚æ¬¡æ¥çœ‹ï¼Œè¿™äº›æ–‡æ¡£ä¸­çš„æ–‡æœ¬å—ç±»ä¼¼äºç›®æ ‡ç›¸ä¼¼æ€§æœç´¢ä¸­çš„ç›®æ ‡æ£€æµ‹è¡¥ä¸ï¼
- en: For more information and working code, see [here](https://github.com/jacobmarks/semantic-document-search-plugin).
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: æ¬²äº†è§£æ›´å¤šä¿¡æ¯å’Œç¤ºä¾‹ä»£ç ï¼Œè¯·è§[è¿™é‡Œ](https://github.com/jacobmarks/semantic-document-search-plugin)ã€‚
- en: Semantic Search
  id: totrans-53
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: è¯­ä¹‰æœç´¢
- en: '![](../Images/8f8c589cf85a3bddb18ab437d7ffff8b.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8f8c589cf85a3bddb18ab437d7ffff8b.png)'
- en: '*Semantic image search using natural language on the COCO 2017 validation split.
    Image courtesy of the author.*'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '*ä½¿ç”¨è‡ªç„¶è¯­è¨€åœ¨COCO 2017éªŒè¯æ•°æ®é›†ä¸Šè¿›è¡Œè¯­ä¹‰å›¾åƒæœç´¢ã€‚å›¾åƒç”±ä½œè€…æä¾›ã€‚*'
- en: With multimodal models, we can extend the notion of semantic search from text
    to images. Models like [CLIP](https://github.com/openai/CLIP), [OpenCLIP](https://github.com/mlfoundations/open_clip),
    and [MetaCLIP](https://github.com/facebookresearch/metaclip) were trained to find
    common representations of images and their captions, so that the embedding vector
    for an image of a dog would be very similar to the embedding vector for the text
    prompt â€œa photo of a dogâ€.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨å¤šæ¨¡æ€æ¨¡å‹ï¼Œæˆ‘ä»¬å¯ä»¥å°†è¯­ä¹‰æœç´¢çš„æ¦‚å¿µä»æ–‡æœ¬æ‰©å±•åˆ°å›¾åƒã€‚åƒ[CLIP](https://github.com/openai/CLIP)ã€[OpenCLIP](https://github.com/mlfoundations/open_clip)å’Œ[MetaCLIP](https://github.com/facebookresearch/metaclip)è¿™æ ·çš„æ¨¡å‹è¢«è®­ç»ƒä»¥æ‰¾åˆ°å›¾åƒåŠå…¶æ ‡é¢˜çš„å…±åŒè¡¨ç¤ºï¼Œå› æ­¤ä¸€å¼ ç‹—çš„å›¾åƒçš„åµŒå…¥å‘é‡å°†ä¸â€œç‹—çš„ç…§ç‰‡â€è¿™ä¸€æ–‡æœ¬æç¤ºçš„åµŒå…¥å‘é‡éå¸¸ç›¸ä¼¼ã€‚
- en: This means that it is sensible (i.e. we are â€œallowedâ€) to create a vector index
    out of the CLIP embeddings for the images in our dataset and then run a vector
    search query against this vector database where the query vector is the CLIP embedding
    of a *text prompt*.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ„å‘³ç€åˆ›å»ºä¸€ä¸ªç”±CLIPåµŒå…¥ç”Ÿæˆçš„å›¾åƒå‘é‡ç´¢å¼•ï¼Œå¹¶ä½¿ç”¨è¯¥å‘é‡æ•°æ®åº“è¿›è¡Œå‘é‡æœç´¢æ˜¯åˆç†çš„ï¼ˆå³æˆ‘ä»¬â€œè¢«å…è®¸â€è¿™æ ·åšï¼‰ï¼Œå…¶ä¸­æŸ¥è¯¢å‘é‡æ˜¯*æ–‡æœ¬æç¤º*çš„CLIPåµŒå…¥ã€‚
- en: ğŸ’¡By treating the individual frames in a video as images and adding each frameâ€™s
    embedding to a vector index, you can also [semantically search through videos](https://medium.com/voxel51/a-google-search-experience-for-computer-vision-data-voxel51-a9ee41390986#:~:text=Find%20video%20frames%20with%20cars%20in%20an%20intersection)!
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ’¡é€šè¿‡å°†è§†é¢‘ä¸­çš„æ¯ä¸€å¸§è§†ä½œå›¾åƒï¼Œå¹¶å°†æ¯å¸§çš„åµŒå…¥æ·»åŠ åˆ°å‘é‡ç´¢å¼•ä¸­ï¼Œä½ è¿˜å¯ä»¥[å¯¹è§†é¢‘è¿›è¡Œè¯­ä¹‰æœç´¢](https://medium.com/voxel51/a-google-search-experience-for-computer-vision-data-voxel51-a9ee41390986#:~:text=Find%20video%20frames%20with%20cars%20in%20an%20intersection)ï¼
- en: For more information and working code, see [here](https://docs.voxel51.com/user_guide/brain.html#text-similarity).
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: æ¬²äº†è§£æ›´å¤šä¿¡æ¯å’Œç¤ºä¾‹ä»£ç ï¼Œè¯·è§[è¿™é‡Œ](https://docs.voxel51.com/user_guide/brain.html#text-similarity)ã€‚
- en: Cross-modal Retrieval
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: è·¨æ¨¡æ€æ£€ç´¢
- en: '*Cross-modal retrieval of images matching an input audio file of a train. Implemented
    using ImageBind with a Qdrant vector index, on the COCO 2017 validation split.
    Video courtesy of the author.*'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '*è·¨æ¨¡æ€æ£€ç´¢ä¸è¾“å…¥éŸ³é¢‘æ–‡ä»¶åŒ¹é…çš„å›¾åƒã€‚ä½¿ç”¨ImageBindå’ŒQdrantå‘é‡ç´¢å¼•å®ç°ï¼ŒåŸºäºCOCO 2017éªŒè¯æ•°æ®é›†ã€‚è§†é¢‘ç”±ä½œè€…æä¾›ã€‚*'
- en: In a sense, semantically searching through a dataset of images is a form of
    cross-modal retrieval. One way of conceptualizing it is that we are retrieving
    images corresponding to a text query. With models like [ImageBind](https://github.com/facebookresearch/ImageBind),
    we can take this a step further!
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: ä»æŸç§æ„ä¹‰ä¸Šè¯´ï¼Œå¯¹å›¾åƒæ•°æ®é›†è¿›è¡Œè¯­ä¹‰æœç´¢æ˜¯ä¸€ç§è·¨æ¨¡æ€æ£€ç´¢ã€‚ä¸€ä¸ªæ¦‚å¿µåŒ–çš„æ–¹æ³•æ˜¯æˆ‘ä»¬æ­£åœ¨æ£€ç´¢ä¸æ–‡æœ¬æŸ¥è¯¢å¯¹åº”çš„å›¾åƒã€‚ä½¿ç”¨åƒ[ImageBind](https://github.com/facebookresearch/ImageBind)è¿™æ ·çš„æ¨¡å‹ï¼Œæˆ‘ä»¬å¯ä»¥æ›´è¿›ä¸€æ­¥ï¼
- en: 'ImageBind embeds data from six different modalities in the same embedding space:
    images, text, audio, depth, thermal, and inertial measurement unit (IMU). That
    means that we can generate a vector index for data in *any* of these modalities
    and query this index with a sample of any other of these modalities. For instance,
    we can take an audio clip of a car honking and retrieve all images of cars!'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: For more information and working code, see [here](https://github.com/jacobmarks/audio-retrieval-plugin).
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: Probing Perceptual Similarity
  id: totrans-65
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One very important part of the vector search story which we have only glossed
    over thus far is the *model*. The elements in our vector index are embeddings
    from a model. These embeddings can be the final output of a tailored embedding
    model, or they can be hidden or *latent* representations from a model trained
    on another task like classification.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
- en: Regardless, the model we use to embed our samples can have a substantial impact
    on which samples are deemed most similar to which other samples. A CLIP model
    captures semantic concepts, but struggles to represent structural information
    within images. A ResNet model on the other hand is very good at representing similarity
    in structure and layout, operating on the level of pixels and patches. Then there
    are embedding models like [DreamSim](https://dreamsim-nights.github.io/), which
    aim to bridge the gap and capture mid-level similarity â€” aligning the modelâ€™s
    notion of similarity with what is perceived by humans.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: Vector search provides a way for us to probe how a model is â€œseeingâ€ the world.
    By creating a separate vector index for each model we are interested in (on the
    same data), we can rapidly develop an intuition for how different models are representing
    data under the hood, so to speak.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is an example showcasing similarity searches with CLIP, ResNet, and DreamSim
    model embeddings for the same query image on the NIGHTS dataset:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9def889010a979d55035d834539bb6fb.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
- en: Similarity search with ResNet50 embeddings on an image in the [NIGHTS dataset](https://dreamsim-nights.github.io/)
    (Images generated by Stable Diffusion â€” [MIT RAIL LICENSE](https://stability.ai/blog/stable-diffusion-public-release)).
    ResNet models operate on the level of pixels and patches. Hence the retrieved
    images are structurally similar to the query but not always semantically similar.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f7acd455ddae7d434d94270b8d965e60.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
- en: Similarity search with CLIP embeddings on the same query image. CLIP models
    respect the underlying semantics of the images but not their layout.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0abe740139d3f32a9a563ab8da5dea5c.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
- en: Similarity search with DreamSim embeddings on the same query image. DreamSim
    bridges the gap, seeking the best mid-level similarity compromise between semantic
    and structural features.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: For more information and working code, see [here](https://medium.com/voxel51/teaching-androids-to-dream-of-sheep-18d72f44f2b).
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: Comparing Model Representations
  id: totrans-77
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/e115d4ca7f1bb4808737096b7c24fcea.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e115d4ca7f1bb4808737096b7c24fcea.png)'
- en: '*Heuristic comparison of ResNet50 and CLIP model representations of the NIGHTS
    dataset. ResNet embeddings have been reduced to 2D using UMAP. Selecting a point
    in the embeddings plot and highlighting nearby samples, we can see how ResNet
    captures compositional and palette similarity, not semantic similarity. Running
    a vector search on the selected sample with CLIP embeddings, we can see that the
    most samples according to CLIP are not localized according to ResNet.*'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '*å¯¹æ¯” ResNet50 å’Œ CLIP æ¨¡å‹åœ¨ NIGHTS æ•°æ®é›†ä¸Šçš„è¡¨ç¤ºã€‚ResNet åµŒå…¥é€šè¿‡ UMAP é™ç»´åˆ° 2Dã€‚é€‰æ‹©åµŒå…¥å›¾ä¸­çš„ä¸€ä¸ªç‚¹å¹¶çªå‡ºæ˜¾ç¤ºé™„è¿‘çš„æ ·æœ¬ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°
    ResNet å¦‚ä½•æ•æ‰ç»„æˆå’Œè‰²å½©ç›¸ä¼¼æ€§ï¼Œè€Œä¸æ˜¯è¯­ä¹‰ç›¸ä¼¼æ€§ã€‚å¯¹é€‰å®šæ ·æœ¬ä½¿ç”¨ CLIP åµŒå…¥è¿›è¡Œå‘é‡æœç´¢ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°æ ¹æ® CLIP çš„ç»“æœï¼Œå¤§å¤šæ•°æ ·æœ¬åœ¨ ResNet
    ä¸­å¹¶æ²¡æœ‰å±€éƒ¨åŒ–ã€‚*'
- en: 'We can gain new insight into the differences between two models by combining
    vector search and dimensionality reduction techniques like uniform manifold approximation
    ([UMAP](https://umap-learn.readthedocs.io/en/latest/)). Hereâ€™s how:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥é€šè¿‡ç»“åˆå‘é‡æœç´¢å’Œé™ç»´æŠ€æœ¯ï¼Œå¦‚å‡åŒ€æµå½¢é€¼è¿‘ ([UMAP](https://umap-learn.readthedocs.io/en/latest/))ï¼Œè·å¾—å¯¹ä¸¤ä¸ªæ¨¡å‹å·®å¼‚çš„æ–°è§è§£ã€‚æ“ä½œæ–¹æ³•å¦‚ä¸‹ï¼š
- en: Each modelâ€™s embeddings contain information about how the model is representing
    the data. Using UMAP (or t-SNE or PCA), we can generate lower dimensional (either
    2D or 3D) representations of the embeddings from model1\. By doing so, we sacrifice
    some detail, but hopefully preserve some information about which samples are perceived
    as similar to other samples. What we gain is the ability to visualize this data.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: æ¯ä¸ªæ¨¡å‹çš„åµŒå…¥åŒ…å«äº†æ¨¡å‹å¦‚ä½•è¡¨ç¤ºæ•°æ®çš„ä¿¡æ¯ã€‚ä½¿ç”¨ UMAPï¼ˆæˆ– t-SNE æˆ– PCAï¼‰ï¼Œæˆ‘ä»¬å¯ä»¥ç”Ÿæˆæ¥è‡ª model1 çš„ä½ç»´ï¼ˆ2D æˆ– 3Dï¼‰è¡¨ç¤ºã€‚è¿™æ ·åšè™½ç„¶ç‰ºç‰²äº†ä¸€äº›ç»†èŠ‚ï¼Œä½†å¸Œæœ›ä¿ç•™äº†æ ·æœ¬ä¹‹é—´çš„ç›¸ä¼¼æ€§ä¿¡æ¯ã€‚æˆ‘ä»¬è·å¾—çš„æ˜¯å¯è§†åŒ–è¿™äº›æ•°æ®çš„èƒ½åŠ›ã€‚
- en: With model1â€™s embedding visualization as a backdrop, we can choose a point in
    this plot and perform a vector search query on that sample with respect to model2â€™s
    embeddings. You can then see where within the 2D visualization the retrieved points
    lie!
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥ model1 çš„åµŒå…¥å¯è§†åŒ–ä¸ºèƒŒæ™¯ï¼Œæˆ‘ä»¬å¯ä»¥åœ¨è¿™ä¸ªå›¾ä¸­é€‰æ‹©ä¸€ä¸ªç‚¹ï¼Œå¹¶å¯¹è¯¥æ ·æœ¬æ‰§è¡Œä¸€ä¸ªé’ˆå¯¹ model2 åµŒå…¥çš„å‘é‡æœç´¢æŸ¥è¯¢ã€‚ç„¶åä½ å¯ä»¥çœ‹åˆ°æ£€ç´¢åˆ°çš„ç‚¹åœ¨
    2D å¯è§†åŒ–ä¸­çš„ä½ç½®ï¼
- en: The example above uses the same NIGHTS dataset as in the last section, visualizing
    ResNet embeddings, which capture more compositional and structural similarity,
    and performing a similarity search with CLIP (semantic) embeddings.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸Šè¿°ç¤ºä¾‹ä½¿ç”¨äº†ä¸ä¸Šä¸€èŠ‚ç›¸åŒçš„ NIGHTS æ•°æ®é›†ï¼Œè§†è§‰åŒ– ResNet åµŒå…¥ï¼Œæ•æ‰æ›´å¤šçš„ç»„æˆå’Œç»“æ„ç›¸ä¼¼æ€§ï¼Œå¹¶ç”¨ CLIPï¼ˆè¯­ä¹‰ï¼‰åµŒå…¥è¿›è¡Œç›¸ä¼¼æ€§æœç´¢ã€‚
- en: Concept Interpolation
  id: totrans-84
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ¦‚å¿µæ’å€¼
- en: '![](../Images/e06bf1b4ad8310f8c3379048991f5cd2.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e06bf1b4ad8310f8c3379048991f5cd2.png)'
- en: '*Interpolation between the concepts â€œhuskyâ€ and â€œchihuahuaâ€ with CLIP embeddings
    on the Oxford-IIIT Pet Dataset*'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '*ä½¿ç”¨ CLIP åµŒå…¥åœ¨ Oxford-IIIT å® ç‰©æ•°æ®é›†ä¸Šè¿›è¡Œâ€œå“ˆå£«å¥‡â€å’Œâ€œå‰å¨ƒå¨ƒâ€æ¦‚å¿µçš„æ’å€¼*'
- en: Weâ€™re reaching the end of the ten applications, but lucky for you I saved a
    few of the best for last. So far, the only vectors weâ€™ve worked with are embeddings
    â€” the vector index is populated with embeddings, and the query vectors are also
    embeddings. But sometimes there is additional structure in the *space* of embeddings
    that we can leverage to interact with our data more dynamically.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬æ¥è¿‘è¿™åä¸ªåº”ç”¨çš„å°¾å£°ï¼Œä½†å¹¸è¿çš„æ˜¯ï¼Œæˆ‘æŠŠä¸€äº›æœ€ä½³çš„ç•™åˆ°äº†æœ€åã€‚åˆ°ç›®å‰ä¸ºæ­¢ï¼Œæˆ‘ä»¬å¤„ç†çš„å”¯ä¸€å‘é‡æ˜¯åµŒå…¥â€”å‘é‡ç´¢å¼•ä¸­å¡«å……äº†åµŒå…¥ï¼ŒæŸ¥è¯¢å‘é‡ä¹Ÿæ˜¯åµŒå…¥ã€‚ä½†æœ‰æ—¶åœ¨åµŒå…¥çš„*ç©ºé—´*ä¸­è¿˜æœ‰é¢å¤–çš„ç»“æ„ï¼Œæˆ‘ä»¬å¯ä»¥åˆ©ç”¨è¿™äº›ç»“æ„ä»¥æ›´åŠ¨æ€çš„æ–¹å¼ä¸æ•°æ®äº’åŠ¨ã€‚
- en: 'One example of such a dynamic interaction is something I like to call â€œconcept
    interpolationâ€. Hereâ€™s how it works: take a dataset of images and generate a vector
    index using a multimodal model (text and image). Pick two text prompts like â€œsunnyâ€
    and â€œrainyâ€, which stand in for concepts, and set a value `alpha` in the range
    `[0,1]`. We can generate the embedding vectors for each text concept, and add
    these vectors in a linear combination specified by `alpha`. We then normalize
    the vector and use it as the query to our vector index of image embeddings.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ç§åŠ¨æ€äº¤äº’çš„ä¸€ä¸ªä¾‹å­æ˜¯æˆ‘å–œæ¬¢ç§°ä¹‹ä¸ºâ€œæ¦‚å¿µæ’å€¼â€ã€‚æ“ä½œæ–¹æ³•å¦‚ä¸‹ï¼šæ‹¿ä¸€ä¸ªå›¾åƒæ•°æ®é›†ï¼Œä½¿ç”¨å¤šæ¨¡æ€æ¨¡å‹ï¼ˆæ–‡æœ¬å’Œå›¾åƒï¼‰ç”Ÿæˆä¸€ä¸ªå‘é‡ç´¢å¼•ã€‚é€‰æ‹©ä¸¤ä¸ªæ–‡æœ¬æç¤ºè¯ï¼Œæ¯”å¦‚â€œæ™´å¤©â€å’Œâ€œé›¨å¤©â€ï¼Œè¿™ä¸¤ä¸ªè¯ä»£è¡¨äº†ä¸åŒçš„æ¦‚å¿µï¼Œå¹¶è®¾ç½®ä¸€ä¸ªåœ¨
    `[0,1]` èŒƒå›´å†…çš„å€¼ `alpha`ã€‚æˆ‘ä»¬å¯ä»¥ä¸ºæ¯ä¸ªæ–‡æœ¬æ¦‚å¿µç”ŸæˆåµŒå…¥å‘é‡ï¼Œå¹¶å°†è¿™äº›å‘é‡æŒ‰ç…§ `alpha` æŒ‡å®šçš„çº¿æ€§ç»„åˆæ–¹å¼ç›¸åŠ ã€‚ç„¶åæˆ‘ä»¬å¯¹è¿™ä¸ªå‘é‡è¿›è¡Œå½’ä¸€åŒ–ï¼Œå¹¶å°†å…¶ä½œä¸ºæŸ¥è¯¢å‘é‡ç”¨äºæˆ‘ä»¬çš„å›¾åƒåµŒå…¥å‘é‡ç´¢å¼•ã€‚
- en: Because we are linearly interpolating between the embedding vectors for the
    two text prompts (concepts), we are in a very loose sense interpolating between
    the concepts themselves! We can dynamically change `alpha` and query our vector
    database each time there is an interaction.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: 'ğŸ’¡This notion of concept interpolation is experimental (read: not always a well
    defined operation). I find it works best when the text prompts are conceptually
    related and the dataset is diverse enough to have different results for different
    places along the interpolation spectrum.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: For more information and working code, see [here](https://github.com/jacobmarks/concept-interpolation).
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: Concept Space Traversal
  id: totrans-92
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/ac25af2786564b86cc7934e50f36eada.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
- en: '*Traversing the space of â€œconceptsâ€ by moving in the direction of various text
    prompts via their embeddings, illustrated for the test split of the COCO 2017
    dataset. Images and text embedded with a CLIP model. Image courtesy of the author.*'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: Last, but certainly not least, we have what I like to call â€œconcept space traversalâ€.
    As with concept interpolation, start with a dataset of images and generate embeddings
    with a multimodal model like CLIP. Next, select an image from the dataset. This
    image will serve as your starting point, from which you will be â€œtraversingâ€ the
    space of concepts.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: From there, you can define a direction you want to move in by providing a text
    string as a stand-in for a concept. Set the magnitude of the â€œstepâ€ you want to
    take in that direction, and that text stringâ€™s embedding vector (with a multiplicative
    coefficient) will be added to the embedding vector of the initial image. The â€œdestinationâ€
    vector will be used to query the vector database. You can add arbitrarily many
    concepts in arbitrary quantities, and watch as the set of retrieved images updates
    in real time.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: As with â€œconcept interpolationâ€, this is not always a strictly well-defined
    process. However, I find it to be captivating, and to perform reasonably well
    when the coefficient applied to the text embeddings is high enough that they are
    sufficiently taken into account.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: For more information and working code, see [here](https://github.com/jacobmarks/concept-space-traversal-plugin).
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  id: totrans-99
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Vector search engines are incredibly powerful tools. Sure, they are the stars
    of the best show in town, *RAG*-time. But vector databases are far more versatile
    than that. They enable deeper understanding of data, give insights into how models
    represent that data, and offer new avenues for us to interact with our data.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: Vector databases are not bound to LLMs. They prove useful whenever embeddings
    are involved, and embeddings lie right at the intersection of *model* and *data*.
    The more rigorously we understand the structure of embedding spaces, the more
    dynamic and pervasive our vector search-enabled data and model interactions will
    become.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: 'If you found this post interesting, you may also want to check out these vector
    search powered posts:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: ğŸ”— [How I Turned My Companyâ€™s Docs into a Searchable Database with OpenAI](/how-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736)
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ğŸ”— [å¦‚ä½•å°†æˆ‘çš„å…¬å¸æ–‡æ¡£è½¬åŒ–ä¸ºå¯æœç´¢çš„æ•°æ®åº“ä¸ OpenAI](/how-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736)
- en: ğŸ”— [How I Turned ChatGPT into an SQL-Like Translator for Image and Video Datasets](/how-i-turned-chatgpt-into-an-sql-like-translator-for-image-and-video-datasets-7b22b318400a)
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ğŸ”— [å¦‚ä½•å°† ChatGPT è½¬å˜ä¸ºç±»ä¼¼ SQL çš„å›¾åƒå’Œè§†é¢‘æ•°æ®é›†ç¿»è¯‘å™¨](/how-i-turned-chatgpt-into-an-sql-like-translator-for-image-and-video-datasets-7b22b318400a)
- en: ğŸ”— [What I Learned Pushing Prompt Engineering to the Limit](/what-i-learned-pushing-prompt-engineering-to-the-limit-c40f0740641f)
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ğŸ”— [æˆ‘åœ¨æ¨åŠ¨ Prompt Engineering æé™ä¸­å­¦åˆ°çš„ä¸œè¥¿](/what-i-learned-pushing-prompt-engineering-to-the-limit-c40f0740641f)
