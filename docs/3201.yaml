- en: From RAGs to Riches
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从 RAG 到财富
- en: 原文：[https://towardsdatascience.com/from-rags-to-riches-53ba89087966?source=collection_archive---------4-----------------------#2023-10-25](https://towardsdatascience.com/from-rags-to-riches-53ba89087966?source=collection_archive---------4-----------------------#2023-10-25)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/from-rags-to-riches-53ba89087966?source=collection_archive---------4-----------------------#2023-10-25](https://towardsdatascience.com/from-rags-to-riches-53ba89087966?source=collection_archive---------4-----------------------#2023-10-25)
- en: 10 Applications of vector search to deeply understand your data and models
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 向量搜索在深入理解您的数据和模型中的 10 种应用
- en: '[](https://medium.com/@jacob_marks?source=post_page-----53ba89087966--------------------------------)[![Jacob
    Marks, Ph.D.](../Images/94d9832b8706d1044e3195386613bfab.png)](https://medium.com/@jacob_marks?source=post_page-----53ba89087966--------------------------------)[](https://towardsdatascience.com/?source=post_page-----53ba89087966--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----53ba89087966--------------------------------)
    [Jacob Marks, Ph.D.](https://medium.com/@jacob_marks?source=post_page-----53ba89087966--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@jacob_marks?source=post_page-----53ba89087966--------------------------------)[![Jacob
    Marks, Ph.D.](../Images/94d9832b8706d1044e3195386613bfab.png)](https://medium.com/@jacob_marks?source=post_page-----53ba89087966--------------------------------)[](https://towardsdatascience.com/?source=post_page-----53ba89087966--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----53ba89087966--------------------------------)
    [Jacob Marks, Ph.D.](https://medium.com/@jacob_marks?source=post_page-----53ba89087966--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ff7dc0c0eae92&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffrom-rags-to-riches-53ba89087966&user=Jacob+Marks%2C+Ph.D.&userId=f7dc0c0eae92&source=post_page-f7dc0c0eae92----53ba89087966---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----53ba89087966--------------------------------)
    ·10 min read·Oct 25, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F53ba89087966&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffrom-rags-to-riches-53ba89087966&user=Jacob+Marks%2C+Ph.D.&userId=f7dc0c0eae92&source=-----53ba89087966---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ff7dc0c0eae92&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffrom-rags-to-riches-53ba89087966&user=Jacob+Marks%2C+Ph.D.&userId=f7dc0c0eae92&source=post_page-f7dc0c0eae92----53ba89087966---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----53ba89087966--------------------------------)
    ·10 min read·2023年10月25日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F53ba89087966&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffrom-rags-to-riches-53ba89087966&user=Jacob+Marks%2C+Ph.D.&userId=f7dc0c0eae92&source=-----53ba89087966---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F53ba89087966&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffrom-rags-to-riches-53ba89087966&source=-----53ba89087966---------------------bookmark_footer-----------)![](../Images/bdbf65ccdee0abe12ca276688bfdacda.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F53ba89087966&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffrom-rags-to-riches-53ba89087966&source=-----53ba89087966---------------------bookmark_footer-----------)![](../Images/bdbf65ccdee0abe12ca276688bfdacda.png)'
- en: Artistic rendering of vector search for data exploration. Image generated by
    DALLE-3.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 向量搜索用于数据探索的艺术效果图。图像由 DALLE-3 生成。
- en: As large language models (LLMs) have eaten the world, vector search engines
    have tagged along for the ride. Vector databases form the foundation of the long-term
    memory systems for LLMs.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 随着大型语言模型（LLMs）席卷全球，向量搜索引擎也随之而来。向量数据库构成了 LLMs 长期记忆系统的基础。
- en: By efficiently finding relevant information to pass in as context to the language
    model, vector search engines can provide up-to-date information beyond the training
    cutoff and enhance the quality of the model’s output without fine-tuning. This
    process, commonly referred to as retrieval augmented generation (RAG), has thrust
    the once-esoteric algorithmic challenge of approximate nearest neighbor (ANN)
    search into the spotlight!
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 通过高效地查找相关信息并将其作为上下文输入到语言模型中，向量搜索引擎可以提供超出训练截止日期的最新信息，并在不进行微调的情况下提高模型输出的质量。这个过程，通常称为检索增强生成（RAG），将曾经晦涩的近似最近邻（ANN）搜索算法挑战推向了聚光灯下！
- en: Amidst all of the commotion, one could be forgiven for thinking that vector
    search engines are inextricably linked to large language models. But there’s so
    much more to the story. Vector search has a plethora of powerful applications
    that go well beyond improving RAG for LLMs!
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在所有的喧嚣中，人们可能会误以为向量搜索引擎与大型语言模型不可分割。但事实远不止于此。向量搜索有许多强大的应用，远远超出了改善 RAG 对 LLMs 的应用！
- en: In this article, I will show you ten of my favorite uses of vector search for
    data understanding, data exploration, model interpretability and more.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我将展示我最喜欢的十种向量搜索的应用，用于数据理解、数据探索、模型解释等。
- en: 'Here are the applications we will cover, in roughly increasing order of complexity:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是我们将涵盖的应用，按复杂度大致递增的顺序：
- en: '[Image Similarity Search](#e595)'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[图像相似性搜索](#e595)'
- en: '[Reverse Image Search](#dd35)'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[反向图像搜索](#dd35)'
- en: '[Object Similarity Search](#26bd)'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[对象相似性搜索](#26bd)'
- en: '[Robust OCR Document Search](#c20f)'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[强健的 OCR 文档搜索](#c20f)'
- en: '[Semantic Search](#8c7d)'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[语义搜索](#8c7d)'
- en: '[Cross-modal Retrieval](#23f9)'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[跨模态检索](#23f9)'
- en: '[Probing Perceptual Similarity](#c565)'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[探测感知相似性](#c565)'
- en: '[Comparing Model Representations](#e42e)'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[比较模型表示](#e42e)'
- en: '[Concept Interpolation](#391f)'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[概念插值](#391f)'
- en: '[Concept Space Traversal](#6967)'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[概念空间遍历](#6967)'
- en: Image Similarity Search
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 图像相似性搜索
- en: '![](../Images/c65fcd3032940efee4d09558aface781.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c65fcd3032940efee4d09558aface781.png)'
- en: '*Image similarity search on images from the* [*Oxford-IIIT Pet Dataset*](https://www.robots.ox.ac.uk/~vgg/data/pets/)
    *(*[*LICENSE*](https://www.robots.ox.ac.uk/~vgg/data/pets/#:~:text=License)*).
    Image courtesy of the author.*'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '*对来自* [*Oxford-IIIT Pet 数据集的图像进行相似性搜索*](https://www.robots.ox.ac.uk/~vgg/data/pets/)
    *(*[*LICENSE*](https://www.robots.ox.ac.uk/~vgg/data/pets/#:~:text=License)*).
    图片由作者提供。*'
- en: Perhaps the simplest place to start is image similarity search. In this task,
    you have a dataset consisting of images — this can be anything from a personal
    photo album to a massive repository of billions of images captured by thousands
    of distributed cameras over the course of years.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 也许最简单的起点是图像相似性搜索。在这个任务中，你有一个由图像组成的数据集——这可以是个人照片集，也可以是数以亿计的图像仓库，这些图像由数千个分布式相机拍摄，历时多年。
- en: 'The setup is simple: compute embeddings for every image in this dataset, and
    generate a vector index out of these embedding vectors. After this initial batch
    of computation, no further inference is required. A great way to explore the structure
    of your dataset is to select an image from the dataset and query the vector index
    for the `k` nearest neighbors — the most similar images. This can provide an intuitive
    sense for how densely the space of images is populated around query images.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 设置很简单：计算数据集中每张图像的嵌入向量，并从这些嵌入向量生成一个向量索引。在这初始批次计算之后，不需要进一步的推断。探索数据集结构的一个好方法是选择数据集中的一张图像，并查询向量索引获取
    `k` 个最近邻——最相似的图像。这可以直观地了解查询图像周围图像空间的密度。
- en: For more information and working code, see [here](https://docs.voxel51.com/user_guide/brain.html#image-similarity).
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 欲了解更多信息和示例代码，请见 [这里](https://docs.voxel51.com/user_guide/brain.html#image-similarity)。
- en: Reverse Image Search
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 反向图像搜索
- en: '![](../Images/799b67f1470b0370e9d9fda684c7ba8c.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/799b67f1470b0370e9d9fda684c7ba8c.png)'
- en: '*Reverse image search on an* [*image from Unsplash*](https://images.unsplash.com/photo-1568034097584-a3063e104ac3?ixlib=rb-4.0.3&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D&auto=format&fit=crop&w=3464&q=80)
    *(courtesy Mladen Šćekić) against the* [*Oxford-IIIT Pet Dataset*](https://www.robots.ox.ac.uk/~vgg/data/pets/)*.
    Image courtesy of the author.*'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '*对来自* [*Unsplash 的图像进行反向图像搜索*](https://images.unsplash.com/photo-1568034097584-a3063e104ac3?ixlib=rb-4.0.3&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D&auto=format&fit=crop&w=3464&q=80)
    *(图片由 Mladen Šćekić 提供)*，与* [*Oxford-IIIT Pet 数据集*](https://www.robots.ox.ac.uk/~vgg/data/pets/)*进行对比。图片由作者提供。*'
- en: In a similar vein, a natural extension of image similarity search is to find
    the most similar images within the dataset to an *external* image. This can be
    an image from your local filesystem, or an image from the internet!
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，图像相似性搜索的自然延伸是找到数据集中与*外部*图像最相似的图像。这可以是来自本地文件系统的图像，也可以是来自互联网的图像！
- en: To perform a reverse image search, you create the vector index for the dataset
    as in the image similarity search example. The difference comes at run-time, when
    you compute the embedding for the query image, and then query the vector database
    with this vector.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 要执行反向图像搜索，你需要像图像相似性搜索示例那样创建数据集的向量索引。不同之处在于运行时，你计算查询图像的嵌入向量，然后用这个向量查询向量数据库。
- en: For more information and working code, see [here](https://github.com/jacobmarks/reverse-image-search-plugin).
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 欲了解更多信息和示例代码，请参见 [这里](https://github.com/jacobmarks/reverse-image-search-plugin)。
- en: Object Similarity Search
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 对象相似性搜索
- en: '![](../Images/6b1842613c97550a31e177d205bec030.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6b1842613c97550a31e177d205bec030.png)'
- en: '*Object similarity search for sheep in the* [*COCO-2017 dataset*](https://cocodataset.org/#home)*’s
    validation split (*[*LICENSE*](https://viso.ai/computer-vision/coco-dataset/#:~:text=the%20MS%20COCO%20images%20dataset%20is%20licensed%20under%20a%20Creative%20Commons%20Attribution%204.0%20License)*).
    Image courtesy of the author.*'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '*在* [*COCO-2017 数据集*](https://cocodataset.org/#home)*的验证分割中进行羊的对象相似性搜索 (*[*LICENSE*](https://viso.ai/computer-vision/coco-dataset/#:~:text=the%20MS%20COCO%20images%20dataset%20is%20licensed%20under%20a%20Creative%20Commons%20Attribution%204.0%20License)*）。图片由作者提供。*'
- en: If you want to delve deeper into the content *within* the images, then object,
    or “patch” similarity search may be what you’re after. One example of this is
    person re-identification, where you have a single image with a person of interest
    in it, and you want to find all instances of that person across your dataset.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想深入研究图像*中的*内容，那么对象或“补丁”相似性搜索可能正是你所追求的。例如，人物再识别就是其中之一，在这种情况下，你有一张包含目标人物的单一图像，你想在数据集中找到所有该人的实例。
- en: The person may only take up small portions of each image, so the embeddings
    for the entire images they are in might depend strongly on the other content in
    these images. For instance, there might be *multiple* people in an image.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 人物可能只占据每张图像的小部分，因此这些图像中包含的整个图像的嵌入可能会强烈依赖于这些图像中的其他内容。例如，图像中可能有*多个*人。
- en: A better solution is to treat each object detection patch as if it were a separate
    entity and compute an embedding for each. Then, create a vector index with these
    patch embeddings, and run a similarity search against a patch of the person you
    want to re-identify. As a starting point you may want to try using a ResNet model.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 更好的解决方案是将每个对象检测补丁视为一个独立的实体，并为每个补丁计算嵌入。然后，使用这些补丁嵌入创建一个向量索引，并针对你想要重新识别的人的补丁进行相似性搜索。作为起点，你可能想尝试使用
    ResNet 模型。
- en: 'Two subtleties here:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有两个细微之处：
- en: In the vector index, you need to store metadata that maps each patch back to
    its corresponding image in the dataset.
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在向量索引中，你需要存储元数据，将每个补丁映射回数据集中的相应图像。
- en: You will need to run an object detection model to generate these detection patches
    before instantiating the index. You may also want to only compute patch embeddings
    for certain classes of objects, like `person`, and not others — `chair`, `table`,
    etc.
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你需要运行一个对象检测模型来生成这些检测补丁，然后再实例化索引。你可能还想仅为某些对象类别（如`person`）计算补丁嵌入，而不是其他类别——`chair`、`table`等。
- en: For more information and working code, see [here](https://docs.voxel51.com/user_guide/brain.html#object-similarity).
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 欲了解更多信息和示例代码，请参见 [这里](https://docs.voxel51.com/user_guide/brain.html#object-similarity)。
- en: Robust OCR Document Search
  id: totrans-46
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 强大的 OCR 文档搜索
- en: '![](../Images/2df3cf0f279735309d732e9f0d9ff25e.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2df3cf0f279735309d732e9f0d9ff25e.png)'
- en: '*Fuzzy/semantic search through blocks of text generated by the Tesseract OCR
    engine on the pages of my Ph.D. thesis. Embeddings computed using GTE-base model.
    Image courtesy of the author.*'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '*通过 Tesseract OCR 引擎生成的博士论文页面上的文本块进行模糊/语义搜索。使用 GTE-base 模型计算的嵌入。图片由作者提供。*'
- en: Optical Character Recognition (OCR) is a technique that allows you to digitize
    documents like handwritten notes, old journal articles, medical records, and those
    love letters squirreled away in your closet. OCR engines like [Tesseract](https://github.com/tesseract-ocr/tesseract)
    and [PaddleOCR](https://learnopencv.com/optical-character-recognition-using-paddleocr/)
    work by identifying individual characters and symbols in images and creating contiguous
    “blocks” of text — think paragraphs.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 光学字符识别（OCR）是一种允许你数字化文档的技术，如手写笔记、旧期刊文章、医疗记录以及那些藏在你衣柜里的情书。像 [Tesseract](https://github.com/tesseract-ocr/tesseract)
    和 [PaddleOCR](https://learnopencv.com/optical-character-recognition-using-paddleocr/)
    这样的 OCR 引擎通过识别图像中的单个字符和符号并创建连续的“文本块”来工作——想象成段落。
- en: Once you have this text, you can then perform traditional natural language keyword
    searches over the predicted blocks of text, as illustrated [here](https://github.com/jacobmarks/keyword-search-plugin).
    However, this method of search is susceptible to single-character errors. If the
    OCR engine accidentally recognizes an “l” as a “1”, a keyword search for “control”
    would fail (how about that irony!).
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你有了这些文本，你可以对预测的文本块进行传统的自然语言关键词搜索，如[这里](https://github.com/jacobmarks/keyword-search-plugin)所示。然而，这种搜索方法容易受到单字符错误的影响。如果OCR引擎错误地将“l”识别为“1”，那么对“control”的关键词搜索将失败（这真是有趣！）。
- en: We can overcome this challenge using vector search! Embed the blocks of text
    using a text embedding model like [GTE-base](https://huggingface.co/thenlper/gte-base)
    from Hugging Face’s [Sentence Transformers](https://huggingface.co/sentence-transformers)
    library, and create a vector index. We can then perform fuzzy and/or semantic
    search across our digitized documents by embedding the search text and querying
    the index. At a high level, the blocks of text within these documents are analogous
    to the object detection patches in object similarity searches!
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过使用向量搜索来克服这个挑战！使用像[GTE-base](https://huggingface.co/thenlper/gte-base)这样的文本嵌入模型将文本块嵌入，并创建一个向量索引。然后，我们可以通过嵌入搜索文本并查询该索引，对我们的数字化文档进行模糊和/或语义搜索。从高层次来看，这些文档中的文本块类似于目标相似性搜索中的目标检测补丁！
- en: For more information and working code, see [here](https://github.com/jacobmarks/semantic-document-search-plugin).
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 欲了解更多信息和示例代码，请见[这里](https://github.com/jacobmarks/semantic-document-search-plugin)。
- en: Semantic Search
  id: totrans-53
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 语义搜索
- en: '![](../Images/8f8c589cf85a3bddb18ab437d7ffff8b.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8f8c589cf85a3bddb18ab437d7ffff8b.png)'
- en: '*Semantic image search using natural language on the COCO 2017 validation split.
    Image courtesy of the author.*'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '*使用自然语言在COCO 2017验证数据集上进行语义图像搜索。图像由作者提供。*'
- en: With multimodal models, we can extend the notion of semantic search from text
    to images. Models like [CLIP](https://github.com/openai/CLIP), [OpenCLIP](https://github.com/mlfoundations/open_clip),
    and [MetaCLIP](https://github.com/facebookresearch/metaclip) were trained to find
    common representations of images and their captions, so that the embedding vector
    for an image of a dog would be very similar to the embedding vector for the text
    prompt “a photo of a dog”.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 使用多模态模型，我们可以将语义搜索的概念从文本扩展到图像。像[CLIP](https://github.com/openai/CLIP)、[OpenCLIP](https://github.com/mlfoundations/open_clip)和[MetaCLIP](https://github.com/facebookresearch/metaclip)这样的模型被训练以找到图像及其标题的共同表示，因此一张狗的图像的嵌入向量将与“狗的照片”这一文本提示的嵌入向量非常相似。
- en: This means that it is sensible (i.e. we are “allowed”) to create a vector index
    out of the CLIP embeddings for the images in our dataset and then run a vector
    search query against this vector database where the query vector is the CLIP embedding
    of a *text prompt*.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着创建一个由CLIP嵌入生成的图像向量索引，并使用该向量数据库进行向量搜索是合理的（即我们“被允许”这样做），其中查询向量是*文本提示*的CLIP嵌入。
- en: 💡By treating the individual frames in a video as images and adding each frame’s
    embedding to a vector index, you can also [semantically search through videos](https://medium.com/voxel51/a-google-search-experience-for-computer-vision-data-voxel51-a9ee41390986#:~:text=Find%20video%20frames%20with%20cars%20in%20an%20intersection)!
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 💡通过将视频中的每一帧视作图像，并将每帧的嵌入添加到向量索引中，你还可以[对视频进行语义搜索](https://medium.com/voxel51/a-google-search-experience-for-computer-vision-data-voxel51-a9ee41390986#:~:text=Find%20video%20frames%20with%20cars%20in%20an%20intersection)！
- en: For more information and working code, see [here](https://docs.voxel51.com/user_guide/brain.html#text-similarity).
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 欲了解更多信息和示例代码，请见[这里](https://docs.voxel51.com/user_guide/brain.html#text-similarity)。
- en: Cross-modal Retrieval
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 跨模态检索
- en: '*Cross-modal retrieval of images matching an input audio file of a train. Implemented
    using ImageBind with a Qdrant vector index, on the COCO 2017 validation split.
    Video courtesy of the author.*'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '*跨模态检索与输入音频文件匹配的图像。使用ImageBind和Qdrant向量索引实现，基于COCO 2017验证数据集。视频由作者提供。*'
- en: In a sense, semantically searching through a dataset of images is a form of
    cross-modal retrieval. One way of conceptualizing it is that we are retrieving
    images corresponding to a text query. With models like [ImageBind](https://github.com/facebookresearch/ImageBind),
    we can take this a step further!
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 从某种意义上说，对图像数据集进行语义搜索是一种跨模态检索。一个概念化的方法是我们正在检索与文本查询对应的图像。使用像[ImageBind](https://github.com/facebookresearch/ImageBind)这样的模型，我们可以更进一步！
- en: 'ImageBind embeds data from six different modalities in the same embedding space:
    images, text, audio, depth, thermal, and inertial measurement unit (IMU). That
    means that we can generate a vector index for data in *any* of these modalities
    and query this index with a sample of any other of these modalities. For instance,
    we can take an audio clip of a car honking and retrieve all images of cars!'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: For more information and working code, see [here](https://github.com/jacobmarks/audio-retrieval-plugin).
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: Probing Perceptual Similarity
  id: totrans-65
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One very important part of the vector search story which we have only glossed
    over thus far is the *model*. The elements in our vector index are embeddings
    from a model. These embeddings can be the final output of a tailored embedding
    model, or they can be hidden or *latent* representations from a model trained
    on another task like classification.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
- en: Regardless, the model we use to embed our samples can have a substantial impact
    on which samples are deemed most similar to which other samples. A CLIP model
    captures semantic concepts, but struggles to represent structural information
    within images. A ResNet model on the other hand is very good at representing similarity
    in structure and layout, operating on the level of pixels and patches. Then there
    are embedding models like [DreamSim](https://dreamsim-nights.github.io/), which
    aim to bridge the gap and capture mid-level similarity — aligning the model’s
    notion of similarity with what is perceived by humans.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: Vector search provides a way for us to probe how a model is “seeing” the world.
    By creating a separate vector index for each model we are interested in (on the
    same data), we can rapidly develop an intuition for how different models are representing
    data under the hood, so to speak.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is an example showcasing similarity searches with CLIP, ResNet, and DreamSim
    model embeddings for the same query image on the NIGHTS dataset:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9def889010a979d55035d834539bb6fb.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
- en: Similarity search with ResNet50 embeddings on an image in the [NIGHTS dataset](https://dreamsim-nights.github.io/)
    (Images generated by Stable Diffusion — [MIT RAIL LICENSE](https://stability.ai/blog/stable-diffusion-public-release)).
    ResNet models operate on the level of pixels and patches. Hence the retrieved
    images are structurally similar to the query but not always semantically similar.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f7acd455ddae7d434d94270b8d965e60.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
- en: Similarity search with CLIP embeddings on the same query image. CLIP models
    respect the underlying semantics of the images but not their layout.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0abe740139d3f32a9a563ab8da5dea5c.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
- en: Similarity search with DreamSim embeddings on the same query image. DreamSim
    bridges the gap, seeking the best mid-level similarity compromise between semantic
    and structural features.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: For more information and working code, see [here](https://medium.com/voxel51/teaching-androids-to-dream-of-sheep-18d72f44f2b).
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: Comparing Model Representations
  id: totrans-77
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/e115d4ca7f1bb4808737096b7c24fcea.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e115d4ca7f1bb4808737096b7c24fcea.png)'
- en: '*Heuristic comparison of ResNet50 and CLIP model representations of the NIGHTS
    dataset. ResNet embeddings have been reduced to 2D using UMAP. Selecting a point
    in the embeddings plot and highlighting nearby samples, we can see how ResNet
    captures compositional and palette similarity, not semantic similarity. Running
    a vector search on the selected sample with CLIP embeddings, we can see that the
    most samples according to CLIP are not localized according to ResNet.*'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '*对比 ResNet50 和 CLIP 模型在 NIGHTS 数据集上的表示。ResNet 嵌入通过 UMAP 降维到 2D。选择嵌入图中的一个点并突出显示附近的样本，我们可以看到
    ResNet 如何捕捉组成和色彩相似性，而不是语义相似性。对选定样本使用 CLIP 嵌入进行向量搜索，我们可以看到根据 CLIP 的结果，大多数样本在 ResNet
    中并没有局部化。*'
- en: 'We can gain new insight into the differences between two models by combining
    vector search and dimensionality reduction techniques like uniform manifold approximation
    ([UMAP](https://umap-learn.readthedocs.io/en/latest/)). Here’s how:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过结合向量搜索和降维技术，如均匀流形逼近 ([UMAP](https://umap-learn.readthedocs.io/en/latest/))，获得对两个模型差异的新见解。操作方法如下：
- en: Each model’s embeddings contain information about how the model is representing
    the data. Using UMAP (or t-SNE or PCA), we can generate lower dimensional (either
    2D or 3D) representations of the embeddings from model1\. By doing so, we sacrifice
    some detail, but hopefully preserve some information about which samples are perceived
    as similar to other samples. What we gain is the ability to visualize this data.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 每个模型的嵌入包含了模型如何表示数据的信息。使用 UMAP（或 t-SNE 或 PCA），我们可以生成来自 model1 的低维（2D 或 3D）表示。这样做虽然牺牲了一些细节，但希望保留了样本之间的相似性信息。我们获得的是可视化这些数据的能力。
- en: With model1’s embedding visualization as a backdrop, we can choose a point in
    this plot and perform a vector search query on that sample with respect to model2’s
    embeddings. You can then see where within the 2D visualization the retrieved points
    lie!
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 以 model1 的嵌入可视化为背景，我们可以在这个图中选择一个点，并对该样本执行一个针对 model2 嵌入的向量搜索查询。然后你可以看到检索到的点在
    2D 可视化中的位置！
- en: The example above uses the same NIGHTS dataset as in the last section, visualizing
    ResNet embeddings, which capture more compositional and structural similarity,
    and performing a similarity search with CLIP (semantic) embeddings.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 上述示例使用了与上一节相同的 NIGHTS 数据集，视觉化 ResNet 嵌入，捕捉更多的组成和结构相似性，并用 CLIP（语义）嵌入进行相似性搜索。
- en: Concept Interpolation
  id: totrans-84
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 概念插值
- en: '![](../Images/e06bf1b4ad8310f8c3379048991f5cd2.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e06bf1b4ad8310f8c3379048991f5cd2.png)'
- en: '*Interpolation between the concepts “husky” and “chihuahua” with CLIP embeddings
    on the Oxford-IIIT Pet Dataset*'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '*使用 CLIP 嵌入在 Oxford-IIIT 宠物数据集上进行“哈士奇”和“吉娃娃”概念的插值*'
- en: We’re reaching the end of the ten applications, but lucky for you I saved a
    few of the best for last. So far, the only vectors we’ve worked with are embeddings
    — the vector index is populated with embeddings, and the query vectors are also
    embeddings. But sometimes there is additional structure in the *space* of embeddings
    that we can leverage to interact with our data more dynamically.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 我们接近这十个应用的尾声，但幸运的是，我把一些最佳的留到了最后。到目前为止，我们处理的唯一向量是嵌入—向量索引中填充了嵌入，查询向量也是嵌入。但有时在嵌入的*空间*中还有额外的结构，我们可以利用这些结构以更动态的方式与数据互动。
- en: 'One example of such a dynamic interaction is something I like to call “concept
    interpolation”. Here’s how it works: take a dataset of images and generate a vector
    index using a multimodal model (text and image). Pick two text prompts like “sunny”
    and “rainy”, which stand in for concepts, and set a value `alpha` in the range
    `[0,1]`. We can generate the embedding vectors for each text concept, and add
    these vectors in a linear combination specified by `alpha`. We then normalize
    the vector and use it as the query to our vector index of image embeddings.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 这种动态交互的一个例子是我喜欢称之为“概念插值”。操作方法如下：拿一个图像数据集，使用多模态模型（文本和图像）生成一个向量索引。选择两个文本提示词，比如“晴天”和“雨天”，这两个词代表了不同的概念，并设置一个在
    `[0,1]` 范围内的值 `alpha`。我们可以为每个文本概念生成嵌入向量，并将这些向量按照 `alpha` 指定的线性组合方式相加。然后我们对这个向量进行归一化，并将其作为查询向量用于我们的图像嵌入向量索引。
- en: Because we are linearly interpolating between the embedding vectors for the
    two text prompts (concepts), we are in a very loose sense interpolating between
    the concepts themselves! We can dynamically change `alpha` and query our vector
    database each time there is an interaction.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: '💡This notion of concept interpolation is experimental (read: not always a well
    defined operation). I find it works best when the text prompts are conceptually
    related and the dataset is diverse enough to have different results for different
    places along the interpolation spectrum.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: For more information and working code, see [here](https://github.com/jacobmarks/concept-interpolation).
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: Concept Space Traversal
  id: totrans-92
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/ac25af2786564b86cc7934e50f36eada.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
- en: '*Traversing the space of “concepts” by moving in the direction of various text
    prompts via their embeddings, illustrated for the test split of the COCO 2017
    dataset. Images and text embedded with a CLIP model. Image courtesy of the author.*'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: Last, but certainly not least, we have what I like to call “concept space traversal”.
    As with concept interpolation, start with a dataset of images and generate embeddings
    with a multimodal model like CLIP. Next, select an image from the dataset. This
    image will serve as your starting point, from which you will be “traversing” the
    space of concepts.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: From there, you can define a direction you want to move in by providing a text
    string as a stand-in for a concept. Set the magnitude of the “step” you want to
    take in that direction, and that text string’s embedding vector (with a multiplicative
    coefficient) will be added to the embedding vector of the initial image. The “destination”
    vector will be used to query the vector database. You can add arbitrarily many
    concepts in arbitrary quantities, and watch as the set of retrieved images updates
    in real time.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: As with “concept interpolation”, this is not always a strictly well-defined
    process. However, I find it to be captivating, and to perform reasonably well
    when the coefficient applied to the text embeddings is high enough that they are
    sufficiently taken into account.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: For more information and working code, see [here](https://github.com/jacobmarks/concept-space-traversal-plugin).
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  id: totrans-99
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Vector search engines are incredibly powerful tools. Sure, they are the stars
    of the best show in town, *RAG*-time. But vector databases are far more versatile
    than that. They enable deeper understanding of data, give insights into how models
    represent that data, and offer new avenues for us to interact with our data.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: Vector databases are not bound to LLMs. They prove useful whenever embeddings
    are involved, and embeddings lie right at the intersection of *model* and *data*.
    The more rigorously we understand the structure of embedding spaces, the more
    dynamic and pervasive our vector search-enabled data and model interactions will
    become.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: 'If you found this post interesting, you may also want to check out these vector
    search powered posts:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: 🔗 [How I Turned My Company’s Docs into a Searchable Database with OpenAI](/how-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736)
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 🔗 [如何将我的公司文档转化为可搜索的数据库与 OpenAI](/how-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736)
- en: 🔗 [How I Turned ChatGPT into an SQL-Like Translator for Image and Video Datasets](/how-i-turned-chatgpt-into-an-sql-like-translator-for-image-and-video-datasets-7b22b318400a)
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 🔗 [如何将 ChatGPT 转变为类似 SQL 的图像和视频数据集翻译器](/how-i-turned-chatgpt-into-an-sql-like-translator-for-image-and-video-datasets-7b22b318400a)
- en: 🔗 [What I Learned Pushing Prompt Engineering to the Limit](/what-i-learned-pushing-prompt-engineering-to-the-limit-c40f0740641f)
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 🔗 [我在推动 Prompt Engineering 极限中学到的东西](/what-i-learned-pushing-prompt-engineering-to-the-limit-c40f0740641f)
