["```py\nfrom sklearn.model_selection import train_test_split\n\nfeatures = df.columns[1:]\ncategorical_features = [feature for feature in features if feature[0] == \"C\" ]\nx_train, x_test, y_train, y_test = train_test_split(df[features], df[\"label\"])\n```", "```py\nfrom category_encoders.target_encoder import TargetEncoder\n\nenc = TargetEncoder(cols = categorical_features).fit(x_train, y_train)\nX_train_encoded = enc.transform(x_train)\nX_test_encoded = enc.transform(x_test)\n```", "```py\nfrom category_encoders.count import CountEncoder\n\nenc = CountEncoder(cols = categorical_features).fit(x_train, y_train)\nX_train_encoded = enc.transform(x_train)\nX_test_encoded = enc.transform(x_test)\n```", "```py\nfrom category_encoders.hashing import HashingEncoder\n\nenc = HashingEncoder(\n    cols = categorical_features, \n    n_components=20*len(categorical_features)\n).fit(x_train, y_train)\nX_train_encoded = enc.transform(x_train)\nX_test_encoded = enc.transform(x_test)\n```", "```py\nimport tensorflow as tf\n\ndef build_input_layers(features):\n    input_layers = {}\n    for feature in features:\n        if feature in categorical_features:\n            input_layers[feature] = tf.keras.layers.Input(\n                shape=(1,),\n                name=feature,\n                dtype=tf.string\n            )\n        else:\n            input_layers[feature] = tf.keras.layers.Input(\n                shape=(1,),\n                name=feature,\n                dtype=tf.float32\n            )\n    return input_layers\n\ndef build_embeddings(size=None):\n    input_layers = build_input_layers(features)\n    embedded_layers = []\n\n    for feature in input_layers.keys():\n        if feature in categorical_features:\n            # Get the vocabulary of the categorical feature\n            vocabulary = sorted(\n                    [str(value) for value in list(x_train[feature].unique())]\n                )\n            # convert the string input values into integer indices\n            cardinality = x_train[feature].nunique()\n            pre_processing_layer = tf.keras.layers.StringLookup(\n              vocabulary=vocabulary, \n              num_oov_indices=cardinality,\n              name=feature+\"_preprocessed\"\n            )\n            pre_processed_input = pre_processing_layer(input_layers[feature])\n            # Create an embedding layer with the specified dimensions\n            embedding_size = int(math.sqrt(cardinality))\n            embedding_layer = tf.keras.layers.Embedding(\n                input_dim=2*cardinality+1,\n                output_dim=embedding_size,\n                name=feature+\"_embedded\",\n\n            )\n            embedded_layers.append(embedding_layer(pre_processed_input))   \n        else:\n            # return numerical feature as it is\n            embedded_layers.append(input_layers[feature])\n\n    # Concatenate all the encoded features.\n    encoded_features = tf.keras.layers.Concatenate()([\n                tf.keras.layers.Flatten()(layer) for layer in embedded_layers\n            ])\n\n    # Apply dropout.\n    encoded_features = tf.keras.layers.Dropout(rate=0.25)(encoded_features)\n\n    # Perform non-linearity projection.\n    encoded_features = tf.keras.layers.Dense(\n        units=size if size else encoded_features.shape[-1], activation=\"gelu\"\n    )(encoded_features)\n    return tf.keras.Model(inputs=input_layers, outputs=encoded_features)\n\ndef build_neural_network_model(embedding_encoder):\n    input_layers = build_input_layers(features)\n    embeddings = embedding_encoder(input_layers)\n    output = tf.keras.layers.Dense(units=1, activation=\"sigmoid\")(embeddings)\n\n    model = keras.Model(inputs=input_layers, \n                        outputs=output)\n\n    model.compile(\n        optimizer=tf.keras.optimizers.Adam(),\n        loss=tf.keras.losses.BinaryCrossentropy(),\n        metrics=[tf.keras.metrics.AUC()]\n    )\n\n    return model\n\nembedding_encoder = build_embeddings(64)\nneural_network_model = build_neural_network_model(embedding_encoder)\n\n# Training\ndef build_dataset(x, y):\n    dataset = {}\n    for feat in features:\n        if feat in categorical_features:\n            dataset[feat] = np.array(x[feat]).reshape(-1,1).astype(str)\n        else:\n            dataset[feat] = np.array(x[feat]).reshape(-1,1).astype(float)\n\n    return dataset, np.array(y).reshape(-1,1)\n\nx_train, y_train = build_dataset(x_train, y_train)\nx_test, y_test = build_dataset(x_test, y_test)\nhistory = neural_network_model.fit(x_train, y_train, batch_size=1024, epochs=5)\n\nX_train_encoded = embedding_encoder.predict(x_train, batch_size=1024)\nX_test_encoded = embedding_encoder.predict(x_test, batch_size=1024)\n```", "```py\nfrom sklearn.linear_model import LogisticRegression\nmodel = LogisticRegression(max_iter=10000)\nmodel.fit(X_train_encoded, y_train)\ny_pred = model.predict(X_test_encoded)\n```"]