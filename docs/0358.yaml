- en: How to Train Time Series Forecasting Faster using Ray, part 3 of 3
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Ray加速时间序列预测的训练，第3部分，共3部分
- en: 原文：[https://towardsdatascience.com/faster-time-series-forecasting-with-ray-air-distributed-computing-part-3-of-3-632c96974774?source=collection_archive---------10-----------------------#2023-01-24](https://towardsdatascience.com/faster-time-series-forecasting-with-ray-air-distributed-computing-part-3-of-3-632c96974774?source=collection_archive---------10-----------------------#2023-01-24)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/faster-time-series-forecasting-with-ray-air-distributed-computing-part-3-of-3-632c96974774?source=collection_archive---------10-----------------------#2023-01-24](https://towardsdatascience.com/faster-time-series-forecasting-with-ray-air-distributed-computing-part-3-of-3-632c96974774?source=collection_archive---------10-----------------------#2023-01-24)
- en: Train many models faster using distributed computing with Ray and Ray AIR
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用Ray和Ray AIR通过分布式计算更快地训练多个模型
- en: '[](https://medium.com/@christybergman?source=post_page-----632c96974774--------------------------------)[![Christy
    Bergman](../Images/b8431b925cfe7bd0d3a035761fd1e7f8.png)](https://medium.com/@christybergman?source=post_page-----632c96974774--------------------------------)[](https://towardsdatascience.com/?source=post_page-----632c96974774--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----632c96974774--------------------------------)
    [Christy Bergman](https://medium.com/@christybergman?source=post_page-----632c96974774--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@christybergman?source=post_page-----632c96974774--------------------------------)[![Christy
    Bergman](../Images/b8431b925cfe7bd0d3a035761fd1e7f8.png)](https://medium.com/@christybergman?source=post_page-----632c96974774--------------------------------)[](https://towardsdatascience.com/?source=post_page-----632c96974774--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----632c96974774--------------------------------)
    [Christy Bergman](https://medium.com/@christybergman?source=post_page-----632c96974774--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ff18ab4254b46&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffaster-time-series-forecasting-with-ray-air-distributed-computing-part-3-of-3-632c96974774&user=Christy+Bergman&userId=f18ab4254b46&source=post_page-f18ab4254b46----632c96974774---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----632c96974774--------------------------------)
    ·11 min read·Jan 24, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F632c96974774&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffaster-time-series-forecasting-with-ray-air-distributed-computing-part-3-of-3-632c96974774&user=Christy+Bergman&userId=f18ab4254b46&source=-----632c96974774---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ff18ab4254b46&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffaster-time-series-forecasting-with-ray-air-distributed-computing-part-3-of-3-632c96974774&user=Christy+Bergman&userId=f18ab4254b46&source=post_page-f18ab4254b46----632c96974774---------------------post_header-----------)
    发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----632c96974774--------------------------------)
    ·11分钟阅读·2023年1月24日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F632c96974774&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffaster-time-series-forecasting-with-ray-air-distributed-computing-part-3-of-3-632c96974774&user=Christy+Bergman&userId=f18ab4254b46&source=-----632c96974774---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F632c96974774&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffaster-time-series-forecasting-with-ray-air-distributed-computing-part-3-of-3-632c96974774&source=-----632c96974774---------------------bookmark_footer-----------)![](../Images/0b4cdf6bfa4d1843765d0442ef94d0b7.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F632c96974774&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffaster-time-series-forecasting-with-ray-air-distributed-computing-part-3-of-3-632c96974774&source=-----632c96974774---------------------bookmark_footer-----------)![](../Images/0b4cdf6bfa4d1843765d0442ef94d0b7.png)'
- en: Image by [StableDiffusion](https://stablediffusionweb.com/#demo), drawn on Jan
    5, 2022, with the query “draw an image representing many different deep neural
    network time series models training at once cy twombly style”.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由 [StableDiffusion](https://stablediffusionweb.com/#demo) 绘制，日期为2022年1月5日，查询内容为“绘制一幅图像，展示多种不同的深度神经网络时间序列模型同时训练的样子，风格类似Cy
    Twombly”。
- en: Introduction / Motivation
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 介绍 / 动机
- en: 'Even in the current age of Generative AI (Stable Diffusion, ChatGPT) and LLM
    (large language models), **Time Series Forecasting is still a fundamental part
    of running any business that depends on a supply chain or resources.** For example
    it can be used in:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 即使在当前生成式 AI（如稳定扩散、ChatGPT）和大语言模型（LLM）时代，**时间序列预测仍然是任何依赖供应链或资源的业务的基础部分。** 例如，它可以用于：
- en: '[Fulfillment prediction](https://www.youtube.com/watch?v=3t26ucTy0Rs) for inventory
    management by geography.'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[根据地理位置进行库存管理的履行预测](https://www.youtube.com/watch?v=3t26ucTy0Rs)。'
- en: '[Demand forecasting](https://www.anyscale.com/blog/how-anastasia-implements-ray-and-anyscale-to-speed-up-ml-processes-9x)
    different products by product category.'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[不同产品类别的需求预测](https://www.anyscale.com/blog/how-anastasia-implements-ray-and-anyscale-to-speed-up-ml-processes-9x)。'
- en: '[Utilization prediction](https://medium.com/kocdigital/using-ray-for-time-series-forecasting-at-scale-3355c9e4e28c)
    for datacenter provisioning by resource.'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[资源利用预测](https://medium.com/kocdigital/using-ray-for-time-series-forecasting-at-scale-3355c9e4e28c)
    用于数据中心资源配置。'
- en: '**One thing all these use cases have in common is** [**training many models
    on different segments of data**](https://www.anyscale.com/blog/training-one-million-machine-learning-models-in-record-time-with-ray)**.**
    Training, tuning, and deploying thousands of machine learning models in parallel
    using distributed computing can be a challenging task! Typical time series modeling
    software is not distributed by itself.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '**所有这些用例的共同点是** [**在不同数据片段上训练多个模型**](https://www.anyscale.com/blog/training-one-million-machine-learning-models-in-record-time-with-ray)**。**
    使用分布式计算并行训练、调优和部署成千上万的机器学习模型可能是一项挑战！ 典型的时间序列建模软件本身并不具备分布式功能。'
- en: '**This blog will show my tips to get started converting your forecasting workloads
    to distributed computing.** I’ll use the newest [Ray v2 APIs](https://docs.ray.io/en/latest/)
    with ARIMA using [statsforecast](https://github.com/Nixtla/statsforecast), [Prophet](https://facebook.github.io/prophet/),
    and [PyTorch Forecasting](https://pytorch-forecasting.readthedocs.io/en/stable/index.html)
    libraries. For the data, I will use the popular [NYC Taxi dataset](https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page),
    which contains historical taxi pickups by timestamp and location in NYC.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '**本博客将展示我开始将预测工作负载转换为分布式计算的技巧。** 我将使用最新的 [Ray v2 API](https://docs.ray.io/en/latest/)，结合
    ARIMA 使用 [statsforecast](https://github.com/Nixtla/statsforecast)、[Prophet](https://facebook.github.io/prophet/)
    和 [PyTorch Forecasting](https://pytorch-forecasting.readthedocs.io/en/stable/index.html)
    库。数据方面，我将使用流行的 [NYC Taxi 数据集](https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page)，该数据集包含按时间戳和地点记录的历史出租车接送信息。'
- en: Ray is an open-source framework for scaling AI workloads using distributed computing.
    For an overview of Ray, check out the [Ray documentation](https://docs.ray.io/en/latest/)
    or this [introductory blog post](/modern-parallel-and-distributed-python-a-quick-tutorial-on-ray-99f8d70369b8).
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: Ray 是一个开源框架，用于通过分布式计算扩展 AI 工作负载。有关 Ray 的概述，请查看 [Ray 文档](https://docs.ray.io/en/latest/)
    或这篇 [介绍博客文章](/modern-parallel-and-distributed-python-a-quick-tutorial-on-ray-99f8d70369b8)。
- en: '![](../Images/936ddd06d94b1c8950dcf87f360c7154.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/936ddd06d94b1c8950dcf87f360c7154.png)'
- en: '[Ray “AIR](https://docs.ray.io/en/latest/ray-air/getting-started.html)” (AI
    Runtime), available since Ray 2.0, includes [Ray Data](https://docs.ray.io/en/latest/data/dataset.html),
    [Ray Train](https://docs.ray.io/en/latest/train/train.html), [Ray Tune](https://docs.ray.io/en/latest/tune/index.html),
    [RLlib](https://docs.ray.io/en/latest/rllib/index.html), and [Ray Serve](https://docs.ray.io/en/latest/serve/index.html).
    Image by Author.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '[Ray “AIR](https://docs.ray.io/en/latest/ray-air/getting-started.html)”（AI
    Runtime），自 Ray 2.0 起提供，包括 [Ray Data](https://docs.ray.io/en/latest/data/dataset.html)、[Ray
    Train](https://docs.ray.io/en/latest/train/train.html)、[Ray Tune](https://docs.ray.io/en/latest/tune/index.html)、[RLlib](https://docs.ray.io/en/latest/rllib/index.html)
    和 [Ray Serve](https://docs.ray.io/en/latest/serve/index.html)。图片来源于作者。'
- en: 'This blog is organized in 4 sections:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 本博客分为 4 个部分：
- en: Multi-model distributed training using Ray Core Multiprocessing.
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Ray Core 多进程进行多模型分布式训练。
- en: Multi-model distributed tuning using Ray AIR.
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Ray AIR 进行多模型分布式调优。
- en: Multi-model distributed tuning fewer, larger models using Ray AIR.
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Ray AIR 进行多模型分布式调优，针对更少但更大的模型。
- en: Multi-model distributed deployment using Ray AIR and Ray Serve.
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Ray AIR 和 Ray Serve 进行多模型分布式部署。
- en: 'Section 1: Multi-Model Distributed Training using Ray Core Multiprocessing'
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第一部分：使用 Ray Core 多进程进行多模型分布式训练
- en: Back in November 2021, I wrote a [blog article](https://medium.com/towards-data-science/scaling-time-series-forecasting-with-ray-arima-and-prophet-e6c856e605ee)
    demonstrating how to train many forecast models (either ARIMA or Prophet) in parallel
    using Ray Core on AWS cloud. **Since then,** [**Ray Multiprocessing**](https://docs.ray.io/en/latest/ray-more-libs/multiprocessing.html)
    **is a big improvement that makes things easier than Ray Core APIs.**
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 回到2021年11月，我写了一篇 [博客文章](https://medium.com/towards-data-science/scaling-time-series-forecasting-with-ray-arima-and-prophet-e6c856e605ee)
    演示如何在 AWS 云上使用 Ray Core 并行训练多个预测模型（无论是 ARIMA 还是 Prophet）。**从那时起，** [**Ray 多进程**](https://docs.ray.io/en/latest/ray-more-libs/multiprocessing.html)
    **是一个巨大的改进，使事情比 Ray Core API 更加简单。**
- en: 'Below is the code outline. The full, updated code is on [my github](https://github.com/christy/AnyscaleDemos/blob/main/forecasting_demos/Ray_v2/train_prophet_blog.ipynb).
    First, let’s start with a couple imports:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是代码大纲。完整的更新代码在 [我的 GitHub](https://github.com/christy/AnyscaleDemos/blob/main/forecasting_demos/Ray_v2/train_prophet_blog.ipynb)
    上。首先，让我们从几个导入开始：
- en: '[PRE0]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Next, let’s define Python functions to preprocess data, train and evaluate a
    model. To get to the distributed computing concepts quicker, we are going to pretend
    the time series data is already prepared and split into separate files per desired
    model.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们定义 Python 函数来预处理数据、训练和评估模型。为了更快地了解分布式计算概念，我们将假设时间序列数据已经准备好并根据所需模型拆分为不同的文件。
- en: '[PRE1]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: We could parallelize this directly using Ray core API calls`[ray.remote](https://docs.ray.io/en/latest/ray-core/walkthrough.html)`,
    but Ray’s [multiprocessing library](https://docs.ray.io/en/latest/ray-more-libs/multiprocessing.html),
    one of Ray’s distributed libraries, makes this easier.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以直接使用 Ray 核心 API 调用 `[ray.remote](https://docs.ray.io/en/latest/ray-core/walkthrough.html)`
    进行并行化，但 Ray 的 [多进程库](https://docs.ray.io/en/latest/ray-more-libs/multiprocessing.html)
    作为 Ray 的分布式库之一，使这变得更简单。
- en: Below, wrapping the call to `pool` with `tqdm` gives a nice progress bar to
    monitor progress. Internally, Ray dispatches tasks to workers in the Ray cluster,
    which automatically handles issues such as fault tolerance and batching optimizations.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 下面，将对 `pool` 的调用包装在 `tqdm` 中，可以获得一个很好的进度条来监控进度。在内部，Ray 将任务分配给 Ray 集群中的工作节点，这自动处理如容错和批处理优化等问题。
- en: '[PRE2]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '![](../Images/fdca6909d17c7ae9f3020f481ada53d7.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fdca6909d17c7ae9f3020f481ada53d7.png)'
- en: Screenshot while running the above example on my MacBook pro laptop with 8 CPU.
    Time to run Ray Multiprocessing was about a half-minute, which was *3.5x, or 300.0%
    speedup over serial Python. More speedup would be possible with a bigger cluster
    and/or bigger data. Image by Author.*
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在我的 MacBook Pro 笔记本电脑（配备 8 个 CPU）上运行上述示例时的截图。Ray 多进程的运行时间大约为半分钟，比串行 Python 快
    *3.5 倍，即 300.0% 的加速。使用更大的集群和/或更大的数据可以获得更多加速。图片由作者提供。*
- en: Above, we can see the Ray job took less than 1 minute to train 12 models.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 上面，我们可以看到 Ray 作业在不到 1 分钟的时间内训练了 12 个模型。
- en: 'Section 2: Multi-Model Distributed Tuning using Ray AIR'
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第 2 部分：使用 Ray AIR 进行多模型分布式调优
- en: The astute reader may have noticed, in the above section, Ray Multiprocessing
    required the data to already be organized into one file per model you want to
    train. **But what if your data isn’t already organized by model?** With Ray AIR,
    you can preprocess data in the same pipeline while training different models.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 精明的读者可能已经注意到，在上述部分中，Ray 多进程要求数据已经按模型组织成一个文件。**但如果你的数据尚未按模型组织怎么办？** 使用 Ray AIR，你可以在训练不同模型的同时在同一管道中预处理数据。
- en: Another problem, **what if you want to mix-and-match algorithms from more than
    one library at a time?** Ray Tune, which is part of Ray AIR, lets you run parallel
    trials to find the best choice of algorithm from any Python AI/ML libraries and
    hyperparameters, per segment of data.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个问题是，**如果你想同时混合使用来自多个库的算法怎么办？** Ray Tune，作为 Ray AIR 的一部分，允许你运行并行试验，从任何 Python
    AI/ML 库和超参数中找到最佳算法选择，每个数据段。
- en: 'Below are the steps to preprocess data and automatically tune models. Although
    specific to [Ray AIR](https://docs.ray.io/en/latest/ray-air/getting-started.html)
    and its [APIs](https://docs.ray.io/en/latest/ray-air/package-ref.html), these
    steps apply in general to converting serial Python to distributed Python:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是预处理数据和自动调整模型的步骤。虽然这些步骤特定于 [Ray AIR](https://docs.ray.io/en/latest/ray-air/getting-started.html)
    及其 [API](https://docs.ray.io/en/latest/ray-air/package-ref.html)，但这些步骤通常适用于将串行
    Python 转换为分布式 Python：
- en: Define Python functions to `**preprocess**` a segment of data.
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义 Python 函数来 `**preprocess**` 一个数据段。
- en: Define Python functions to `**train**` and `**evaluate**` a model on a segment
    of data.
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义 Python 函数来 `**train**` 和 `**evaluate**` 一个数据段上的模型。
- en: Define a calling function `**train_models**`, which calls all the above functions,
    and will be called in parallel for every permutation in the Tune search space!
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个调用函数`**train_models**`，该函数调用所有上述函数，并将为Tune搜索空间中的每个排列并行调用！
- en: 'Inside this [**train_models**](https://docs.ray.io/en/latest/tune/api_docs/trainable.html#trainable-docs)
    function:'
  id: totrans-43
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在这个[**train_models**](https://docs.ray.io/en/latest/tune/api_docs/trainable.html#trainable-docs)函数中：
- en: 📖 The input parameters must include a config dictionary argument.
  id: totrans-44
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 📖 输入参数必须包括一个配置字典参数。
- en: 📈 The tuning metric (a model’s loss or error) must be calculated and reported
    using `session.report()`.
  id: totrans-45
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 📈 调优指标（模型的损失或错误）必须使用`session.report()`计算并报告。
- en: ✔️ `Checkpoint` (save) the model is recommended for fault tolerance and easy
    deployment later.
  id: totrans-46
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ✔️ 推荐`Checkpoint`（保存）模型以便于容错和后续部署。
- en: '**Configure distributed compute scaling**.'
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**配置分布式计算缩放**。'
- en: '**Define a Tune search space** of all training parameters.'
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**定义Tune搜索空间**的所有训练参数。'
- en: (Optional) Specify a hyperparameter search strategy.
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: （可选）指定超参数搜索策略。
- en: '**Run the experiment**.'
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**运行实验**。'
- en: Below is the additional code we would add; full code is on [my github](https://github.com/christy/AnyscaleDemos/blob/6b1cea50a8c3b75bf9b680e77216f6927bdd2f85/forecasting_demos/Ray_v2/train_prophet_blog.ipynb).
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是我们将添加的附加代码；完整代码在[我的GitHub](https://github.com/christy/AnyscaleDemos/blob/6b1cea50a8c3b75bf9b680e77216f6927bdd2f85/forecasting_demos/Ray_v2/train_prophet_blog.ipynb)上。
- en: The `preprocess_data` and `train_model` functions below are exactly the same
    as before, except they take in a list of files instead of a single file.
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 下面的`preprocess_data`和`train_model`函数与之前完全相同，只是它们接受一个文件列表而不是单个文件。
- en: The `train_models` function is exactly the same as `train_and_evaluate` except
    it takes in a list of files instead of a single file. It also trains an algorithm
    passed in the config instead of a fixed algorithm, and does checkpointing.
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`train_models`函数与`train_and_evaluate`完全相同，只是它接受一个文件列表而不是单个文件。它还训练配置中传递的算法，而不是固定的算法，并进行检查点保存。'
- en: '[PRE3]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '![](../Images/0256c490f5411ce2ac7c0261144e0f9f.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0256c490f5411ce2ac7c0261144e0f9f.png)'
- en: '**Top**: Screenshot Ray Tune Trial Status showing the error of 768 candidate
    models (3 algorithm choices per 256 NYC taxi pickup locations). Trained in under
    45 minutes. **Bottom left**: Prophet model inference+prediction plot (actuals
    as black dots, forecast with confidence interval in blue) for NYC taxi pickup
    location=165\. **Bottom right**: ARIMA model inference+prediction plot (actuals
    in blue, forecast in orange) for NYC taxi pickup location=237\. All models trained
    using Anyscale on 10-node AWS cluster of [m5.4xlarges](https://aws.amazon.com/ec2/instance-types/m5/)
    worker nodes and one m5.2xlarge head node.Image by Author.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '**顶部**：Ray Tune试验状态的截图，显示了768个候选模型的错误（每256个NYC出租车接送位置有3种算法选择）。在不到45分钟内完成训练。**左下**：Prophet模型推断+预测图（实际值为黑点，带置信区间的预测为蓝色）用于NYC出租车接送位置=165。**右下**：ARIMA模型推断+预测图（实际值为蓝色，预测为橙色）用于NYC出租车接送位置=237。所有模型使用Anyscale在10节点的AWS集群上训练，集群包括[
    m5.4xlarges](https://aws.amazon.com/ec2/instance-types/m5/)工作节点和一个m5.2xlarge头节点。图像由作者提供。'
- en: In the above screen shots, data since January 2018 was grouped and aggregated
    to daily-level. I have tried in the past to do this on SageMaker and just the
    data processing alone took too long, let alone Tuning that many models at once.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述截图中，自2018年1月以来的数据被分组并汇总到日级别。我曾尝试在SageMaker上完成这项工作，仅数据处理就花费了太长时间，更不用说同时调优如此多的模型了。
- en: 'Section 3: Multi-model Distributed Tuning (larger PyTorch models)'
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第三节：多模型分布式调优（更大的PyTorch模型）
- en: Often the goal is to create a few, larger models, such as one model by geographic
    zone, where you only have a handful of such zones. One year ago, in December 2021,
    I wrote a [blog article](https://medium.com/towards-data-science/how-to-train-time-series-forecasting-faster-using-ray-part-2-of-2-aacba89ca49a)
    demonstrating how to use Ray Lightning to train larger PyTorch Forecasting models.
    **Since then, a big improvement is the code development switch between laptop
    and cloud is more seamless, thanks to** [**Anyscale Workspaces**](https://docs.anyscale.com/user-guide/develop-and-debug/workspaces)**.**
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 目标通常是创建一些更大的模型，例如按地理区域划分的模型，其中只有少数几个这样的区域。一年前，即2021年12月，我写了一篇[博客文章](https://medium.com/towards-data-science/how-to-train-time-series-forecasting-faster-using-ray-part-2-of-2-aacba89ca49a)，展示了如何使用Ray
    Lightning训练更大的PyTorch预测模型。**自那时以来，一个重大改进是，感谢** [**Anyscale Workspaces**](https://docs.anyscale.com/user-guide/develop-and-debug/workspaces)**，笔记本电脑和云之间的代码开发切换更加无缝。**
- en: These larger models are sometimes called “global models”, because only 1 deep
    neural network model is trained across many different time series at once. Instead
    of 1 model per time series (Prophet or ARIMA).
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 这些较大的模型有时被称为“全球模型”，因为只有一个深度神经网络模型在多个不同时间序列上进行训练，而不是每个时间序列一个模型（Prophet 或 ARIMA）。
- en: 'See [my github](https://github.com/christy/AnyscaleDemos/blob/main/forecasting_demos/Ray_v2/ray_air/pytorch_forecasting.ipynb)
    for the full [PyTorch Forecasting](https://pytorch-forecasting.readthedocs.io/en/stable/index.html)
    code showing the latest Ray AIR APIs with [Ray Lightning](https://docs.ray.io/en/latest/tune/examples/tune-pytorch-lightning.html).
    You need to add a cluster ID to your data, then the steps for Tuning are the same
    as we saw in Section 2:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 请查看 [我的 GitHub](https://github.com/christy/AnyscaleDemos/blob/main/forecasting_demos/Ray_v2/ray_air/pytorch_forecasting.ipynb)
    以获取完整的 [PyTorch Forecasting](https://pytorch-forecasting.readthedocs.io/en/stable/index.html)
    代码，其中展示了最新的 Ray AIR API 与 [Ray Lightning](https://docs.ray.io/en/latest/tune/examples/tune-pytorch-lightning.html)。你需要将集群
    ID 添加到数据中，然后调优步骤与第 2 节中看到的相同：
- en: '[PRE4]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '![](../Images/dc844e0d7e1c866b57556638403bbad8.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/dc844e0d7e1c866b57556638403bbad8.png)'
- en: '**Top**. Screenshot Ray Tune Trial Status while tuning six PyTorch Forecasting
    TemporalFusionTransformer models. (3 learning rates, 2 clusters of NYC taxi locations).
    Runtime less than 2 minutes total. Ran on a 2-node AWS cluster of m5.4xlarge worker
    nodes and one m5.2xlarge head node, within 2 minutes. **Bottom**: Inference forecast
    plots for several taxi pickup locations using a single model.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '**顶部**。在调优六个 PyTorch Forecasting TemporalFusionTransformer 模型时截图 Ray Tune Trial
    状态。（3 个学习率，2 个纽约市出租车位置簇）。总运行时间少于 2 分钟。在 2 节点的 AWS 集群（m5.4xlarge 工作节点和一个 m5.2xlarge
    主节点）上运行，时间不超过 2 分钟。**底部**：使用单个模型对多个出租车接客地点进行的推断预测图。'
- en: Note that unlike the ARIMA and Prophet models, which were a single model per
    unique_id, each one of these larger models contains inferences for many unique_ids
    at once in a single model.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，与 ARIMA 和 Prophet 模型（每个 unique_id 一个模型）不同，这些较大的模型每次包含对多个 unique_ids 的推断。
- en: 'Section 4: Multi-model Distributed Deploying using Ray AIR with Ray Serve'
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第 4 节：使用 Ray AIR 和 Ray Serve 进行多模型分布式部署
- en: Before you deploy, you have to decide if your deployment needs to be an online,
    always running http service or offline (Python service called on demand). Below,
    I demonstrate offline deployment using the new [Ray AIR Predictors](https://docs.ray.io/en/latest/ray-air/predictors.html)
    with [Ray Serve](https://docs.ray.io/en/latest/serve/index.html).
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在部署之前，你必须决定你的部署是需要一个在线、始终运行的 http 服务，还是离线的（按需调用的 Python 服务）。下面，我演示了如何使用新的 [Ray
    AIR Predictors](https://docs.ray.io/en/latest/ray-air/predictors.html) 和 [Ray
    Serve](https://docs.ray.io/en/latest/serve/index.html) 进行离线部署。
- en: 'The steps for offline deployment are:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 离线部署的步骤是：
- en: '**Step 1**. Instantiate a batch predictor using Ray AIR checkpoints.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤 1**。使用 Ray AIR 检查点实例化一个批量预测器。'
- en: '**Step 2**. Create some test data.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤 2**。创建一些测试数据。'
- en: '**Step 3**. Run `batch_predictor.predict(test_data)`.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤 3**。运行 `batch_predictor.predict(test_data)`。'
- en: 'Replace Step 3 above with these steps for a custom predictor:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 将上述步骤 3 替换为自定义预测器的以下步骤：
- en: '**Step 3**. Define a Ray Serve deployment class by using a Ray decorator `@serve.deployment`.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤 3**。通过使用 Ray 装饰器 `@serve.deployment` 定义一个 Ray Serve 部署类。'
- en: '**Step 4**. Deploy the predictor.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤 4**。部署预测器。'
- en: '**Step 5**. Query the deployment and get the result.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤 5**。查询部署并获取结果。'
- en: Steps 3–5 above are only required if you are using a custom predictor (such
    as ARIMA, Prophet, or PyTorch Forecasting).
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 上述步骤 3–5 仅在使用自定义预测器（如 ARIMA、Prophet 或 PyTorch Forecasting）时需要。
- en: Otherwise for Ray AIR-integrated ML Libraries (HuggingFace transformers, PyTorch,
    TensorFlow, Scikit-learn, XGBoost, or LightGBM), all you have to do is call `batch_predictor.predict(test_data)`.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 否则，对于集成了 Ray AIR 的 ML 库（HuggingFace transformers、PyTorch、TensorFlow、Scikit-learn、XGBoost
    或 LightGBM），你只需调用 `batch_predictor.predict(test_data)` 即可。
- en: Continuing the example for PyTorch Forecasting from the previous section, below
    is the deployment code. Full code is on [my github](https://github.com/christy/AnyscaleDemos/blob/main/forecasting_demos/Ray_v2/ray_air/pytorch_forecasting.ipynb).
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 继续上一节中关于 PyTorch Forecasting 的示例，下面是部署代码。完整代码在 [我的 GitHub](https://github.com/christy/AnyscaleDemos/blob/main/forecasting_demos/Ray_v2/ray_air/pytorch_forecasting.ipynb)
    上。
- en: '[PRE5]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '![](../Images/352284bfab1b2d33c66eb92fe5f433e5.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/352284bfab1b2d33c66eb92fe5f433e5.png)'
- en: '**Left**: Screenshot from Ray dashboard (accessible by default at `localhost:8265
    on the head node`) while serving. **Right**: A screenshot from the Ray dashboard
    while running the above example. You can see 5 spikes in autoscaling while I made
    5 different iterations on the training code before deploying the final trained
    model. Ran Anyscale on 3-node AWS cluster of [m5.4xlarges](https://aws.amazon.com/ec2/instance-types/m5/)
    worker nodes and 1-m5.2xlarge head node.Image by Author.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '**左侧**：Ray 仪表板的截图（默认在`localhost:8265`的主节点上访问）在服务期间的情况。**右侧**：Ray 仪表板在运行上述示例时的截图。你可以看到在自动缩放中有
    5 个峰值，因为在部署最终训练模型之前，我对训练代码进行了 5 次不同的迭代。Anyscale 运行在 3 节点 AWS 集群上，其中包括[ m5.4xlarges](https://aws.amazon.com/ec2/instance-types/m5/)
    工作节点和 1 个 m5.2xlarge 主节点。图片由作者提供。'
- en: The screenshot above right shows Ray cluster observability while training and
    serving. If you need to do post-processing on the predictor results, I have an
    example of that [at the end of this other notebook here](https://github.com/christy/AnyscaleDemos/blob/main/forecasting_demos/Ray_v2/train_prophet_blog.ipynb).
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 上述右侧的截图展示了在训练和服务期间 Ray 集群的可观察性。如果你需要对预测结果进行后处理，我在[这个笔记本的末尾有一个示例](https://github.com/christy/AnyscaleDemos/blob/main/forecasting_demos/Ray_v2/train_prophet_blog.ipynb)。
- en: Conclusion
  id: totrans-83
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结论
- en: In conclusion, this blog showed steps how to train and tune many models in parallel
    using distributed computing with open-source Ray. The models did not have to be
    all the same type, they could be mixed-and-matched from any AI/ML Python libraries.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，本博客展示了如何使用开源 Ray 在分布式计算中并行训练和调整多个模型。模型不必都是相同类型的，可以从任何 AI/ML Python 库中混合匹配。
- en: Ray AIR APIs were clear, intuitive, and hid a lot of distributed computing complexity
    so it was easy to do a lot of complicated things, such as early stopping, ASHA
    scheduling, checkpointing, and deployment.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: Ray AIR API 清晰、直观，并隐藏了许多分布式计算的复杂性，因此可以轻松完成许多复杂的任务，如早期停止、ASHA 调度、检查点和部署。
- en: 'To take your learning further:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 进一步学习：
- en: Read the [Ray docs](https://docs.ray.io/en/latest/), for detailed explanations
    and examples.
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 阅读[Ray 文档](https://docs.ray.io/en/latest/)，获取详细的解释和示例。
- en: Ask questions on [Slack](https://docs.google.com/forms/d/e/1FAIpQLSfAcoiLCHOguOm8e7Jnn-JJdZaCxPGjgVCvFijHB5PLaQLeig/viewform)
    and [Discuss](https://discuss.ray.io/).
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在[Slack](https://docs.google.com/forms/d/e/1FAIpQLSfAcoiLCHOguOm8e7Jnn-JJdZaCxPGjgVCvFijHB5PLaQLeig/viewform)和[Discuss](https://discuss.ray.io/)上提问。
- en: Use [Anyscale](https://docs.anyscale.com/user-guide/develop-and-debug/workspaces#workspaces-tutorial),
    which makes it easy to spin up a cluster and run your code on a Cloud (get the
    invite code [here](https://www.anyscale.com/signup)).
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用[Anyscale](https://docs.anyscale.com/user-guide/develop-and-debug/workspaces#workspaces-tutorial)，这使得启动集群并在云上运行你的代码变得简单（获取邀请码[这里](https://www.anyscale.com/signup)）。
