- en: Fine-tune a Large Language Model with Python
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/fine-tune-a-large-language-model-with-python-b1c09dbc58b2?source=collection_archive---------6-----------------------#2023-04-18](https://towardsdatascience.com/fine-tune-a-large-language-model-with-python-b1c09dbc58b2?source=collection_archive---------6-----------------------#2023-04-18)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/1d8727d8fba25a5fd2ed429d267bf0c1.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Manouchehr Hejazi](https://unsplash.com/@patrol?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: Learn how to fine-tune a BERT from scratch on a custom dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@marcellopoliti?source=post_page-----b1c09dbc58b2--------------------------------)[![Marcello
    Politi](../Images/484e44571bd2e75acfe5fef3146ab3c2.png)](https://medium.com/@marcellopoliti?source=post_page-----b1c09dbc58b2--------------------------------)[](https://towardsdatascience.com/?source=post_page-----b1c09dbc58b2--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----b1c09dbc58b2--------------------------------)
    [Marcello Politi](https://medium.com/@marcellopoliti?source=post_page-----b1c09dbc58b2--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F7390355d40fe&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffine-tune-a-large-language-model-with-python-b1c09dbc58b2&user=Marcello+Politi&userId=7390355d40fe&source=post_page-7390355d40fe----b1c09dbc58b2---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----b1c09dbc58b2--------------------------------)
    ·4 min read·Apr 18, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fb1c09dbc58b2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffine-tune-a-large-language-model-with-python-b1c09dbc58b2&user=Marcello+Politi&userId=7390355d40fe&source=-----b1c09dbc58b2---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb1c09dbc58b2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffine-tune-a-large-language-model-with-python-b1c09dbc58b2&source=-----b1c09dbc58b2---------------------bookmark_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: In this article, we will deal with the **fine-tuning of BERT for sentiment classification**
    using PyTorch. BERT is a large language model that offers a good balance between
    popularity and model size, which can be fine-tuned **using a simple GPU**. We
    can download a **pre-trained BERT from Hugging Face (HF)**, so there is no need
    to train it from scratch. In particular, we will use the distilled (smaller) version
    of BERT, called **Distil-BERT.**
  prefs: []
  type: TYPE_NORMAL
- en: '[Distil-BERT](https://huggingface.co/docs/transformers/model_doc/distilbert#:~:text=DistilBERT%20is%20a%20small%2C%20fast,the%20GLUE%20language%20understanding%20benchmark.)
    is widely used in production since it has **40% fewer parameters** than BERT uncased.
    It runs **60% faster and retains 95% performance** in the [GLUE](https://arxiv.org/abs/1804.07461)
    language comprehension benchmark.'
  prefs: []
  type: TYPE_NORMAL
- en: We start by installing all the necessary libraries. The first line is to capture
    the output of the installation and keep your notebook clean.
  prefs: []
  type: TYPE_NORMAL
- en: I will use Deepnote to run the code in this article but you also use Google
    Colab if you prefer.
  prefs: []
  type: TYPE_NORMAL
