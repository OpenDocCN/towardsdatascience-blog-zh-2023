- en: Fine-Tune Your Own Llama 2 Model in a Colab Notebook
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/fine-tune-your-own-llama-2-model-in-a-colab-notebook-df9823a04a32?source=collection_archive---------0-----------------------#2023-07-25](https://towardsdatascience.com/fine-tune-your-own-llama-2-model-in-a-colab-notebook-df9823a04a32?source=collection_archive---------0-----------------------#2023-07-25)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: A practical introduction to LLM fine-tuning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@mlabonne?source=post_page-----df9823a04a32--------------------------------)[![Maxime
    Labonne](../Images/a7efdd305e3cc77d5509bbb1076d57d8.png)](https://medium.com/@mlabonne?source=post_page-----df9823a04a32--------------------------------)[](https://towardsdatascience.com/?source=post_page-----df9823a04a32--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----df9823a04a32--------------------------------)
    [Maxime Labonne](https://medium.com/@mlabonne?source=post_page-----df9823a04a32--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fdc89da634938&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffine-tune-your-own-llama-2-model-in-a-colab-notebook-df9823a04a32&user=Maxime+Labonne&userId=dc89da634938&source=post_page-dc89da634938----df9823a04a32---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----df9823a04a32--------------------------------)
    ·12 min read·Jul 25, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fdf9823a04a32&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffine-tune-your-own-llama-2-model-in-a-colab-notebook-df9823a04a32&user=Maxime+Labonne&userId=dc89da634938&source=-----df9823a04a32---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fdf9823a04a32&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffine-tune-your-own-llama-2-model-in-a-colab-notebook-df9823a04a32&source=-----df9823a04a32---------------------bookmark_footer-----------)![](../Images/deab49c0869889d83573906da9f2c40b.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: 'With the release of LLaMA v1, we saw a Cambrian explosion of fine-tuned models,
    including [Alpaca](https://github.com/tatsu-lab/stanford_alpaca), [Vicuna](https://huggingface.co/lmsys/vicuna-13b-v1.3),
    and [WizardLM](https://huggingface.co/WizardLM/WizardLM-13B-V1.1), among others.
    This trend encouraged different businesses to launch their own base models with
    licenses suitable for commercial use, such as [OpenLLaMA](https://github.com/openlm-research/open_llama),
    [Falcon](https://falconllm.tii.ae/), [XGen](https://github.com/salesforce/xgen),
    etc. The release of Llama 2 now combines the best elements from both sides: it
    offers a **highly efficient base model along with a more permissive license**.'
  prefs: []
  type: TYPE_NORMAL
- en: 'During the first half of 2023, the software landscape was significantly shaped
    by the **widespread use of APIs** (like OpenAI API) to create infrastructures
    based on Large Language Models (LLMs). Libraries such as [LangChain](https://python.langchain.com/docs/get_started/introduction.html)
    and [LlamaIndex](https://www.llamaindex.ai/) played a critical role in this trend.
    Moving into the latter half of the year, the process of **fine-tuning (or instruction
    tuning) these models is set to become a standard procedure** in the LLMOps workflow.
    This trend is driven by various factors: the potential for cost savings, the ability
    to process confidential data, and even the potential to develop models that exceed
    the performance of prominent models like ChatGPT and GPT-4 in certain specific
    tasks.'
  prefs: []
  type: TYPE_NORMAL
- en: In this article, we will see why instruction tuning works and how to implement
    it in a Google Colab notebook to create…
  prefs: []
  type: TYPE_NORMAL
