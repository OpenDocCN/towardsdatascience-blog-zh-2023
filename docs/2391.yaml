- en: Fine-Tune Your Own Llama 2 Model in a Colab Notebook
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/fine-tune-your-own-llama-2-model-in-a-colab-notebook-df9823a04a32?source=collection_archive---------0-----------------------#2023-07-25](https://towardsdatascience.com/fine-tune-your-own-llama-2-model-in-a-colab-notebook-df9823a04a32?source=collection_archive---------0-----------------------#2023-07-25)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: A practical introduction to LLM fine-tuning
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@mlabonne?source=post_page-----df9823a04a32--------------------------------)[![Maxime
    Labonne](../Images/a7efdd305e3cc77d5509bbb1076d57d8.png)](https://medium.com/@mlabonne?source=post_page-----df9823a04a32--------------------------------)[](https://towardsdatascience.com/?source=post_page-----df9823a04a32--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----df9823a04a32--------------------------------)
    [Maxime Labonne](https://medium.com/@mlabonne?source=post_page-----df9823a04a32--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fdc89da634938&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffine-tune-your-own-llama-2-model-in-a-colab-notebook-df9823a04a32&user=Maxime+Labonne&userId=dc89da634938&source=post_page-dc89da634938----df9823a04a32---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----df9823a04a32--------------------------------)
    ·12 min read·Jul 25, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fdf9823a04a32&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffine-tune-your-own-llama-2-model-in-a-colab-notebook-df9823a04a32&user=Maxime+Labonne&userId=dc89da634938&source=-----df9823a04a32---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fdf9823a04a32&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffine-tune-your-own-llama-2-model-in-a-colab-notebook-df9823a04a32&source=-----df9823a04a32---------------------bookmark_footer-----------)![](../Images/deab49c0869889d83573906da9f2c40b.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: Image by author
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: 'With the release of LLaMA v1, we saw a Cambrian explosion of fine-tuned models,
    including [Alpaca](https://github.com/tatsu-lab/stanford_alpaca), [Vicuna](https://huggingface.co/lmsys/vicuna-13b-v1.3),
    and [WizardLM](https://huggingface.co/WizardLM/WizardLM-13B-V1.1), among others.
    This trend encouraged different businesses to launch their own base models with
    licenses suitable for commercial use, such as [OpenLLaMA](https://github.com/openlm-research/open_llama),
    [Falcon](https://falconllm.tii.ae/), [XGen](https://github.com/salesforce/xgen),
    etc. The release of Llama 2 now combines the best elements from both sides: it
    offers a **highly efficient base model along with a more permissive license**.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 随着 LLaMA v1 的发布，我们见证了微调模型的寒武纪大爆发，包括[Alpaca](https://github.com/tatsu-lab/stanford_alpaca)、[Vicuna](https://huggingface.co/lmsys/vicuna-13b-v1.3)和[WizardLM](https://huggingface.co/WizardLM/WizardLM-13B-V1.1)等。这一趋势鼓励不同企业推出适合商业使用的基础模型许可证，如[OpenLLaMA](https://github.com/openlm-research/open_llama)、[Falcon](https://falconllm.tii.ae/)和[XGen](https://github.com/salesforce/xgen)等。Llama
    2 的发布现在结合了两者的最佳元素：它提供了**高效的基础模型和更宽松的许可证**。
- en: 'During the first half of 2023, the software landscape was significantly shaped
    by the **widespread use of APIs** (like OpenAI API) to create infrastructures
    based on Large Language Models (LLMs). Libraries such as [LangChain](https://python.langchain.com/docs/get_started/introduction.html)
    and [LlamaIndex](https://www.llamaindex.ai/) played a critical role in this trend.
    Moving into the latter half of the year, the process of **fine-tuning (or instruction
    tuning) these models is set to become a standard procedure** in the LLMOps workflow.
    This trend is driven by various factors: the potential for cost savings, the ability
    to process confidential data, and even the potential to develop models that exceed
    the performance of prominent models like ChatGPT and GPT-4 in certain specific
    tasks.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在2023年上半年，**API的广泛使用**（如 OpenAI API）在基于大语言模型（LLMs）构建基础设施方面发挥了重要作用。诸如[LangChain](https://python.langchain.com/docs/get_started/introduction.html)和[LlamaIndex](https://www.llamaindex.ai/)这样的库在这一趋势中发挥了关键作用。进入下半年，**对这些模型的微调（或指令调优）将成为LLMOps工作流程中的标准程序**。这一趋势受到多种因素驱动：节省成本的潜力、处理机密数据的能力，甚至可能开发出在某些特定任务上超越像
    ChatGPT 和 GPT-4 这样的知名模型的模型。
- en: In this article, we will see why instruction tuning works and how to implement
    it in a Google Colab notebook to create…
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们将探讨为什么指令调优有效以及如何在 Google Colab 笔记本中实现它以创建……
