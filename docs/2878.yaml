- en: An Intuition for AUC and Harrell’s C
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/get-an-intuition-for-auc-and-harrells-c-18df56220969?source=collection_archive---------7-----------------------#2023-09-15](https://towardsdatascience.com/get-an-intuition-for-auc-and-harrells-c-18df56220969?source=collection_archive---------7-----------------------#2023-09-15)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: A graphical approach
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://elena-jolkver.medium.com/?source=post_page-----18df56220969--------------------------------)[![Elena
    Jolkver](../Images/ec9e69fb674ce968174886dfa48ed292.png)](https://elena-jolkver.medium.com/?source=post_page-----18df56220969--------------------------------)[](https://towardsdatascience.com/?source=post_page-----18df56220969--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----18df56220969--------------------------------)
    [Elena Jolkver](https://elena-jolkver.medium.com/?source=post_page-----18df56220969--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fd6af391a93a9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fget-an-intuition-for-auc-and-harrells-c-18df56220969&user=Elena+Jolkver&userId=d6af391a93a9&source=post_page-d6af391a93a9----18df56220969---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----18df56220969--------------------------------)
    ·6 min read·Sep 15, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F18df56220969&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fget-an-intuition-for-auc-and-harrells-c-18df56220969&user=Elena+Jolkver&userId=d6af391a93a9&source=-----18df56220969---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F18df56220969&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fget-an-intuition-for-auc-and-harrells-c-18df56220969&source=-----18df56220969---------------------bookmark_footer-----------)![](../Images/863341544228b533c5ea23dd56dbdb87.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Photo by author
  prefs: []
  type: TYPE_NORMAL
- en: 'Everyone venturing into the realm of machine learning or predictive modeling
    comes across the concept of model performance testing. Textbooks usually differ
    only in what the reader learns first: regression with its MSE (mean standard error)
    or classification with a plethora of performance indicators, like accuracy, sensitivity,
    or precision, to name a few. While the letter can be calculated as a simple fraction
    of correct/incorrect predictions and is hence very intuitive, the ROC AUC can
    be daunting at first. Nevertheless, it is also a frequently used parameter to
    assess predictor quality. Let’s unpack its mechanics first to understand the nitty-gritty
    details.'
  prefs: []
  type: TYPE_NORMAL
- en: Get your head around the AUC first
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let’s assume we have built a binary classifier predicting the probability of
    a sample belonging to a certain class. Our test dataset with known classes yielded
    the following results, which can be summarized in a confusion matrix and reported
    in more detail in a table, where the samples have been sorted by the predicted
    probability of being of class P (positive):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/730deb60b522213f6c9455ab92ad3ecb.png)'
  prefs: []
  type: TYPE_IMG
- en: Confusion matrix and detailed prediction table with individual samples’ probabilities.
    Image by author.
  prefs: []
  type: TYPE_NORMAL
- en: The ROC AUC is defined as the area under the ROC (receiver operating characteristic)
    curve. The ROC curve is the plot of the true positive rate (TPR) against the false
    positive rate (FPR) [[Wikipedia](https://en.wikipedia.org/wiki/Receiver_operating_characteristic)].
    The TPR (aka sensitivity) is the ratio of correctly identiefied positive cases
    out of all positive cases. In our case, the TPR is calculated as 4/5 (four out
    of five cases have been classified correctly as positive). FPR is calculated as
    the ratio between the number of negative cases wrongly categorized as positive
    (false positives) and the total number of actual negative cases. In our case,
    the FPR is calculated as 2/6 (two out of 6 negative cases were misclassified as
    positives, if we set the “positivity”-threshold at the probability of 0.5).
  prefs: []
  type: TYPE_NORMAL
- en: 'We can plot the ROC curve from TPR and FPR values and calculate the AUC (Area
    Under Curve):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/16259017e360e5c029118e0ac5e2df98.png)'
  prefs: []
  type: TYPE_IMG
- en: ROC Curve based on prediction probabilities. Image by author.
  prefs: []
  type: TYPE_NORMAL
- en: 'Where did the individual TPR/FPR values for the AUC curve come from? To this
    end, we consider our probabilities table and calculate TPR/FPR for each sample,
    setting the probability, at which we consider a sample to be positive, as the
    one given in the table. Even when we cross the usual level of 0.5, at which samples
    are usually declared as “negative”, we continue to assign them as positive. Let’s
    follow this procedure in our example:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/acf683b88a386589d3518851c7e7f96f.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: 'One sample out of five positives has been classified correctly as being positive
    at a threshold of 0.81, no sample has been predicted negative. We continue until
    we encounter the first negative example:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4a127b56f2b430a3c55e884228fddb2c.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, our TPR stalls at the previous value (3 out of 5 positive samples have
    been predicted correctly), but FPR increments, we have erroneously assigned one
    out of six negative samples to the positive class. We continue until the very
    end:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6c1c1f15c5819c1d6ad87eea7eee76c9.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: 'Et voilà: we arrive at the complete table which is used to create the ROC curve.'
  prefs: []
  type: TYPE_NORMAL
- en: Why Harrell’s C is nothing but the AUC
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: But what about Harrell’s C-index (also known as the concordance index or C-index)?
    Consider the particular task to predict death upon the occurrence of a particular
    disease, say cancer. Eventually, all patients will die, irrespective of cancer
    — a simple binary classifier won’t be of much help. Survival models take into
    account the duration until the outcome (death). The sooner the event occurs, the
    higher the risk of the individual to encounter the outcome. If you were to assess
    the quality of a survival model, you would look at the C-index (aka Concordance,
    aka Harrell’s C).
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to understand the calculation of the C-index, we need to introduce
    two new concepts: permissible and concordant pairs. Permissible pairs are pairs
    of samples (say: patients) with different outcomes during observation, i.e. while
    the experiment was conducted, one patient of such a pair experienced the outcome,
    while the other was censored (i.e. has not reached the outcome yet). These permissible
    pairs are then analyzed for whether the individual with the higher risk score
    experienced the event, while the censored one has not. These cases are called
    concordant pairs.'
  prefs: []
  type: TYPE_NORMAL
- en: Simplifying a bit, the C-index is calculated as the ratio of the number of concordant
    pairs to the number of permissible pairs (I omit the case of risk ties for simplicity).
    Let’s walk through our example, assuming, that we used a survival model which
    calculated the risk rather than the probability. The following table contains
    permissible pairs only. The column “Concordance” is set to 1, if the patient with
    the higher risk score experienced the event (was one of our “positive” group).
    The id is simply the row number from the previous table. Pay special attention
    to the comparison of individual 4 with 5 or 7.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/fd387b2d9378b7e7041c43bfdc4b4793.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: This leaves us with 27 concordant pairs out of 30 permissible ones. The ratio
    (the simplified Harrell’s C) is C = 0.9, which suspiciously reminds us of the
    previously calculated AUC.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can construct a concordance matrix that visualizes how the C statistic is
    computed, as suggested by [Carrington et al](https://doi.org/10.1186/s12911-019-1014-6).
    The plot shows the risk scores of actual positives vs. the risk scores of actual
    negatives and displays the proportion of correctly ranked pairs (green) out of
    all pairs (green + red) if we interpret each grid square as the representation
    of a sample:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ff9123665080b7d1a50219e7c44f331a.png)'
  prefs: []
  type: TYPE_IMG
- en: Concordance matrix for the calculation of Harrell’s C. Image by author
  prefs: []
  type: TYPE_NORMAL
- en: The concordance matrix shows the correctly ranked pairs in concordance toward
    the bottom-right, incorrectly ranked pairs toward the top-left, and a border in
    between which exactly corresponds to the ROC curve we have seen before.
  prefs: []
  type: TYPE_NORMAL
- en: 'Unpacking the process of building up a ROC curve and the concordance matrix,
    we recognize a similarity: in either case we ranked our samples according to their
    probability/risk score and checked, whether the ranking corresponded to the ground
    truth. The higher we set the probability threshold for classification, the more
    false-positives we get. The lower the risk of actual positive cases, the more
    likely will an actual negative case be misclassified as positive. Plotting our
    ranked data accordingly, yielded a curve with the same shape and area, which we
    call AUC or Harrell’s C, depending on the context.'
  prefs: []
  type: TYPE_NORMAL
- en: I hope this example helped to develop an intuition for both, the AUC and Harrell’s
    C.
  prefs: []
  type: TYPE_NORMAL
- en: Acknowledgment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The idea to compare these two parameters arose from a fruitful discussion during
    the Advanced Machine Learning Study Group meetup, kudos Torsten!
  prefs: []
  type: TYPE_NORMAL
- en: '***Reference****: Carrington, A.M., Fieguth, P.W., Qazi, H. et al. A new concordant
    partial AUC and partial c statistic for imbalanced data in the evaluation of machine
    learning algorithms. BMC Med Inform Decis Mak* ***20****, 4 (2020).* [*https://doi.org/10.1186/s12911-019-1014-6*](https://doi.org/10.1186/s12911-019-1014-6)'
  prefs: []
  type: TYPE_NORMAL
