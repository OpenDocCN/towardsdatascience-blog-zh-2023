- en: How You Should Validate Machine Learning Models
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何验证机器学习模型
- en: 原文：[https://towardsdatascience.com/how-you-should-validate-machine-learning-models-f16e9f8a8f7a?source=collection_archive---------5-----------------------#2023-07-21](https://towardsdatascience.com/how-you-should-validate-machine-learning-models-f16e9f8a8f7a?source=collection_archive---------5-----------------------#2023-07-21)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/how-you-should-validate-machine-learning-models-f16e9f8a8f7a?source=collection_archive---------5-----------------------#2023-07-21](https://towardsdatascience.com/how-you-should-validate-machine-learning-models-f16e9f8a8f7a?source=collection_archive---------5-----------------------#2023-07-21)
- en: Learn to build trust in your machine learning solutions
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 学会建立对你机器学习解决方案的信任
- en: '[](https://medium.com/@patryk.miziula?source=post_page-----f16e9f8a8f7a--------------------------------)[![Patryk
    Miziuła, PhD](../Images/3eec03ac6e06beaae7b7d4a998b49f4f.png)](https://medium.com/@patryk.miziula?source=post_page-----f16e9f8a8f7a--------------------------------)[](https://towardsdatascience.com/?source=post_page-----f16e9f8a8f7a--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----f16e9f8a8f7a--------------------------------)
    [Patryk Miziuła, PhD](https://medium.com/@patryk.miziula?source=post_page-----f16e9f8a8f7a--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@patryk.miziula?source=post_page-----f16e9f8a8f7a--------------------------------)[![Patryk
    Miziuła, PhD](../Images/3eec03ac6e06beaae7b7d4a998b49f4f.png)](https://medium.com/@patryk.miziula?source=post_page-----f16e9f8a8f7a--------------------------------)[](https://towardsdatascience.com/?source=post_page-----f16e9f8a8f7a--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----f16e9f8a8f7a--------------------------------)
    [Patryk Miziuła, PhD](https://medium.com/@patryk.miziula?source=post_page-----f16e9f8a8f7a--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F1c427b50c38d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-you-should-validate-machine-learning-models-f16e9f8a8f7a&user=Patryk+Miziu%C5%82a%2C+PhD&userId=1c427b50c38d&source=post_page-1c427b50c38d----f16e9f8a8f7a---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----f16e9f8a8f7a--------------------------------)
    ·14 min read·Jul 21, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Ff16e9f8a8f7a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-you-should-validate-machine-learning-models-f16e9f8a8f7a&user=Patryk+Miziu%C5%82a%2C+PhD&userId=1c427b50c38d&source=-----f16e9f8a8f7a---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F1c427b50c38d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-you-should-validate-machine-learning-models-f16e9f8a8f7a&user=Patryk+Miziu%C5%82a%2C+PhD&userId=1c427b50c38d&source=post_page-1c427b50c38d----f16e9f8a8f7a---------------------post_header-----------)
    发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----f16e9f8a8f7a--------------------------------)
    ·14 min read·2023年7月21日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Ff16e9f8a8f7a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-you-should-validate-machine-learning-models-f16e9f8a8f7a&user=Patryk+Miziu%C5%82a%2C+PhD&userId=1c427b50c38d&source=-----f16e9f8a8f7a---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff16e9f8a8f7a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-you-should-validate-machine-learning-models-f16e9f8a8f7a&source=-----f16e9f8a8f7a---------------------bookmark_footer-----------)![](../Images/37542a3bd2ac447921ba3f6a0f87a592.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff16e9f8a8f7a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-you-should-validate-machine-learning-models-f16e9f8a8f7a&source=-----f16e9f8a8f7a---------------------bookmark_footer-----------)![](../Images/37542a3bd2ac447921ba3f6a0f87a592.png)'
- en: '[https://www.shutterstock.com/image-photo/desert-island-palm-tree-on-beach-71305345](https://www.shutterstock.com/image-photo/desert-island-palm-tree-on-beach-71305345)'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://www.shutterstock.com/image-photo/desert-island-palm-tree-on-beach-71305345](https://www.shutterstock.com/image-photo/desert-island-palm-tree-on-beach-71305345)'
- en: Large language models have already transformed the data science industry in
    a major way. One of the biggest advantages is the fact that for most applications,
    they can be used as is — we don’t have to train them ourselves. This requires
    us to reexamine some of the common assumptions about the whole machine learning
    process — many practitioners consider validation to be “part of the training”,
    which would suggest that it is no longer needed. We hope that the reader shuddered
    slightly at the suggestion of validation being obsolete — it most certainly is
    not.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型已经在很大程度上改变了数据科学行业。其中一个最大的优势是对于大多数应用，它们可以直接使用——我们不必自己训练它们。这要求我们重新审视一些关于整个机器学习过程的常见假设——许多从业者认为验证是“训练的一部分”，这意味着它不再需要。我们希望读者对验证是否过时的建议略感震惊——它绝对不是。
- en: Here, we examine the very idea of model validation and testing. If you believe
    yourself to be perfectly fluent in the foundations of machine learning, you can
    skip this article. Otherwise, strap in — we’ve got some far-fetched scenarios
    for you to suspend your disbelief on.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们深入探讨模型验证和测试的基本概念。如果你认为自己在机器学习的基础知识上已经非常熟练，你可以跳过这篇文章。否则，请系好安全带——我们有一些离奇的情境让你暂时放下怀疑。
- en: This article is a joint work of [Patryk Miziuła, PhD](https://medium.com/u/1c427b50c38d?source=post_page-----f16e9f8a8f7a--------------------------------)
    and [Jan Kanty Milczek](https://medium.com/u/134993f2df6e?source=post_page-----f16e9f8a8f7a--------------------------------).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 这篇文章是[Patryk Miziuła博士](https://medium.com/u/1c427b50c38d?source=post_page-----f16e9f8a8f7a--------------------------------)和[Jan
    Kanty Milczek](https://medium.com/u/134993f2df6e?source=post_page-----f16e9f8a8f7a--------------------------------)的共同工作。
- en: Learning on a desert island
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在荒岛上学习
- en: Imagine that you want to teach someone to recognize the languages of tweets
    on Twitter. So you take him to a desert island, give him 100 tweets in 10 languages,
    tell him what language each tweet is in, and leave him alone for a couple of days.
    After that, you return to the island to check whether he has indeed learned to
    recognize languages. But how can you examine it?
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下，你想教某人识别推特上的语言。于是你把他带到一个荒岛上，给他100条推文，告诉他每条推文的语言，然后让他独自待几天。之后，你回到岛上检查他是否真的学会了识别语言。但是你怎么检查呢？
- en: Your first thought may be to ask him about the languages of the tweets he got.
    So you challenge him this way and he answers correctly for all 100 tweets. Does
    it really mean he is able to recognize languages *in general*? Possibly, but maybe
    he just memorized these 100 tweets! And you have no way of knowing which scenario
    is true!
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 你的第一反应可能是问他关于他获得的推文的语言。所以你这样挑战他，他回答了所有100条推文的语言。这样真的意味着他能够*总体上*识别语言吗？可能是，但也许他只是记住了这100条推文！而且你无法知道哪种情况是真的！
- en: Here you didn’t check what you wanted to check. Based on such an examination,
    you simply can’t know whether you can rely on his tweet language recognition skills
    in a life-or-death situation (those tend to happen when desert islands are involved).
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，你没有检查你想要检查的内容。根据这样的检查，你根本无法知道在生死攸关的情况下（这些情况通常涉及到荒岛时），你能否依赖他的推文语言识别技能。
- en: What should we do instead? How to make sure he *learned*, rather than simply
    *memorizing*? Give him another 50 tweets and have him tell you their languages!
    If he gets them right, he is indeed able to recognize the language. But if he
    fails entirely, you know he simply learned the first 100 tweets off by heart —
    which wasn’t the point of the whole thing.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该怎么做呢？如何确保他*学会了*，而不是仅仅*记住*了？再给他50条推文，让他告诉你这些推文的语言！如果他答对了，他确实能识别语言。但是如果他完全失败了，你就知道他只是把前100条推文背下来了——这不是整个事情的重点。
- en: But how is all this stuff related to machine learning models?
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 那么这些东西如何与机器学习模型相关联呢？
- en: 'The story above figuratively describes how machine learning models learn and
    how we should check their quality:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 上述故事形象地描述了机器学习模型是如何学习的，以及我们应该如何检查它们的质量：
- en: The man in the tale stands for a **machine learning model**. To disconnect a
    human from the world you need to take him to a desert island. For a machine learning
    model it’s easier — it’s just a computer program, so it doesn’t inherently understand
    the idea of the world.
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 故事中的人代表了**机器学习模型**。要将一个人从世界中隔离开来，你需要把他带到一个荒岛上。对于机器学习模型来说，这更简单——它只是一个计算机程序，所以它本身不理解世界的概念。
- en: Recognizing the language of a tweet is a **classification** task, with 10 possible
    **classes,** aka **categories**, as we chose 10 languages.
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 识别推文的语言是一个**分类**任务，涉及10个可能的**类别**，也就是**种类**，因为我们选择了10种语言。
- en: The first 100 tweets used for learning are called the **training set**. The
    correct languages attached are called **labels**.
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 前100条用于学习的推文称为**训练集**。附加的正确语言称为**标签**。
- en: The other 50 tweets only used to examine the man/model are called the **test
    set**. Note that we know its labels, but the man/model doesn’t.
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 另外50条推文仅用于考察该人/模型，称为**测试集**。注意，我们知道它的标签，但该人/模型并不知道。
- en: 'The graph below shows how to correctly train and test the model:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了如何正确地训练和测试模型：
- en: '![](../Images/ec2a71a3a4c10edc90183226ce0dfe8c.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ec2a71a3a4c10edc90183226ce0dfe8c.png)'
- en: 'Image 1: scheme for training and testing the model properly. Image by author.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：模型训练和测试的方案。图像由作者提供。
- en: 'So the main rule is:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 所以主要规则是：
- en: '**Test a machine learning model on a different piece of data than you trained
    it on.**'
  id: totrans-27
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**在与训练数据不同的数据上测试机器学习模型。**'
- en: If the model does well on the training set, but it performs poorly on the test
    set, we say that the model is **overfitted**. “Overfitting” means memorizing the
    training data. That’s definitely not what we want to achieve. Our goal is to have
    a **trained** model — good for both the training and the test set. Only this kind
    of model can be trusted. And only then may we believe that it will perform as
    well in the final application it’s being built for as it did on the test set.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 如果模型在训练集上表现良好，但在测试集上表现不佳，我们称模型为**过拟合**。“过拟合”意味着记住训练数据。这绝不是我们想要实现的目标。我们的目标是拥有一个**训练好的**模型——在训练集和测试集上表现都好。只有这种模型才能被信任。只有这样，我们才能相信它在为之构建的最终应用中会像在测试集上表现一样好。
- en: Now let’s take it a step further.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们更进一步。
- en: 1000 men on 1000 desert islands
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 1000个人在1000个荒岛上
- en: Imagine you really really want to teach a man to recognize the languages of
    tweets on Twitter. So you find 1000 candidates, take each to a different desert
    island, give each the same 100 tweets in 10 languages, tell each what language
    each tweet is in and leave them all alone for a couple of days. After that, you
    examine each candidate with the same set of 50 different tweets.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下你非常非常想教一个人识别Twitter上推文的语言。因此你找到了1000个候选人，把每个人送到一个不同的荒岛上，给每个人相同的100条推文，涵盖10种语言，告诉每个人每条推文的语言，然后把他们全部放在一起几天。之后，你用相同的50条不同推文来检查每个候选人。
- en: Which candidate will you choose? Of course, the one who did the best on the
    50 tweets. But how good is he *really*? Can we truly believe that he’s going to
    perform as well in the final application as he did on these 50 tweets?
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 你会选择哪个候选人？当然，是在50条推文上表现最好的那个。但他*真的*有多好呢？我们是否真的相信他在最终应用中能像在这50条推文上表现一样好？
- en: The answer is no! Why not? To put it simply, if every candidate knows some answers
    and guesses some of the others, then you choose the one who got the most answers
    right, not the one who *knew* the most. He is indeed the best candidate, but his
    result is inflated by “lucky guesses.” It was likely a big part of the reason
    why he was chosen.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 答案是否定的！为什么呢？简单来说，如果每个候选人知道一些答案并猜测其他答案，那么你应该选择那些答对最多的人，而不是那些*知道*最多的人。他确实是最好的候选人，但他的结果被“幸运猜测”所膨胀。这可能是他被选择的一个重要原因。
- en: 'To show this phenomenon in numerical form, imagine that 47 tweets were easy
    for all the candidates, but the 3 remaining messages were so hard for all the
    competitors that they all simply guessed the languages blindly. Probability says
    that the chance that somebody (possibly more than one person) got all the 3 hard
    tweets is above 63% (info for math nerds: it’s almost 1–1/*e*). So you’ll probably
    choose someone who scored perfectly, but *in fact* he’s not perfect for what you
    need.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 为了以数字形式展示这种现象，假设47条推文对所有候选人都很简单，但剩下的3条消息对所有竞争者来说都非常困难，以至于他们都只是盲目猜测语言。概率说，某人（可能是多个人）全部猜对这3条困难推文的概率超过63%（数学迷的信息：几乎是1–1/*e*）。所以你可能会选择得分完美的人，但*实际上*他并不完全适合你的需求。
- en: Perhaps 3 out of 50 tweets in our example don’t sound astonishing, but for many
    real-life cases this discrepancy tends to be much more pronounced.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 也许我们例子中的50条推文中有3条听起来并不惊人，但在许多实际案例中，这种差异往往更加明显。
- en: So how can we check how good the winner *actually* is? Yes, we have to procure
    yet another set of 50 tweets, and examine him once again! Only this way will we
    get a score we can trust. This level of accuracy is what we expect from the final
    application.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 那么我们如何检查获胜者*实际上*有多好呢？是的，我们需要再获取一组50条推文，并再次检查！只有这样，我们才能得到一个值得信赖的评分。这种准确度是我们期望最终应用达到的水平。
- en: Let’s go back to machine learning terminology
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 让我们回到机器学习术语
- en: 'In terms of names:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 从名字上看：
- en: The first set of 100 tweets is now still the **training set**, as we use it
    to train the models.
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第一组100条推文现在仍然是**训练集**，因为我们用它来训练模型。
- en: But now the purpose of the second set of 50 tweets has changed. This time it
    was used to compare different models. Such a set is called the **validation set**.
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 但现在第二组50条推文的目的发生了变化。这一次它被用来比较不同的模型。这样的集合被称为**验证集**。
- en: We already understand that the result of the best model examined on the validation
    set is artificially boosted. This is why we need one more set of 50 tweets to
    play the role of the **test set** and give us reliable information about the quality
    of the best model.
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们已经理解了，在验证集上检查到的最佳模型结果是被人为提升的。这就是为什么我们需要另外一组50条推文来充当**测试集**，为我们提供有关最佳模型质量的可靠信息。
- en: 'You can find the flow of using the training, validation and test set in the
    image below:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在下面的图片中找到使用训练集、验证集和测试集的流程：
- en: '![](../Images/cc04786ed3b594332d9bbb5bf694cffa.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cc04786ed3b594332d9bbb5bf694cffa.png)'
- en: 'Image 2: scheme for training, validating and testing the models properly. Image
    by author.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 图片2：正确训练、验证和测试模型的示意图。图片由作者提供。
- en: All right, and why did we use sets of exactly 100, 50 and 50 tweets?
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 好了，我们为什么使用100条、50条和50条推文的集合呢？
- en: 'Here are the two general ideas behind these numbers:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是这些数字背后的两个基本理念：
- en: '**Put as much data as possible into the training set.**'
  id: totrans-47
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**将尽可能多的数据放入训练集。**'
- en: The more training data we have, the broader the look the models take and the
    greater the chance of training instead of overfitting. The only limits should
    be data availability and the costs of processing the data.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 我们拥有的训练数据越多，模型的视野就越广，训练的机会就越大，过拟合的可能性就越小。唯一的限制应是数据的可用性和处理数据的成本。
- en: '**Put as small an amount of data as possible into the validation and test sets,
    but make sure they’re big enough.**'
  id: totrans-49
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**将尽可能少的数据放入验证集和测试集中，但要确保它们足够大。**'
- en: Why? Because you don’t want to waste much data for anything but training. But
    on the other hand you probably feel that evaluating the model based on a single
    tweet would be risky. So you need a set of tweets big enough not to be afraid
    of score disruption in case of a small number of really weird tweets.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么？因为你不想浪费太多数据在训练之外。但是另一方面，你可能觉得仅凭一条推文来评估模型是有风险的。所以你需要一组足够大的推文集合，以免在遇到少量非常奇怪的推文时担心评分波动。
- en: And how to convert these two guidelines into exact numbers? If you have 200
    tweets available then the 100/50/50 split seems fine as it obeys both the rules
    above. But if you’ve got 1,000,000 tweets then you can easily go into 800,000/100,000/100,000
    or even 900,000/50,000/50,000\. Maybe you saw some percentage clues somewhere,
    like 60%/20%/20% or so. Well, they are only an oversimplification of the two main
    rules written above, so it’s better to simply stick to the original guidelines.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 那么如何将这两个指导方针转换成具体的数字呢？如果你有200条推文，那么100/50/50的划分似乎是合适的，因为它遵循了上述两个规则。但是，如果你有1,000,000条推文，你可以很容易地将其划分为800,000/100,000/100,000，甚至900,000/50,000/50,000。也许你看到过一些百分比提示，比如60%/20%/20%之类的。不过，它们只是上述两个主要规则的过度简化，所以最好还是坚持原始指导方针。
- en: OK, but how to choose which tweets will go into the training/validation/test
    set?
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 好的，但如何选择哪些推文进入训练集/验证集/测试集呢？
- en: 'We believe this main rule appears clear to you at this point:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 我们相信这个主要规则现在对你来说很清楚了：
- en: '**Use three different pieces of data for training, validating, and testing
    the models.**'
  id: totrans-54
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**使用三种不同的数据来训练、验证和测试模型。**'
- en: So what if this rule is broken? What if the same or *almost the same* data,
    whether by accident or a failure to pay attention, go into more than one of the
    three datasets? This is what we call **data leakage**. The validation and test
    sets are no longer trustworthy. We can’t tell whether the model is trained or
    overfitted. We simply can’t trust the model. Not good.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 如果这个规则被打破会怎样？如果相同或*几乎相同*的数据，无论是因为意外还是没有注意，进入了三个数据集中中的一个以上，这就是我们所说的**数据泄露**。验证集和测试集就不再可信了。我们无法判断模型是被训练还是过拟合。我们根本无法信任这个模型。这不好。
- en: Perhaps you think these problems don’t concern our desert island story. We just
    take 100 tweets for training, another 50 for validating and yet another 50 for
    testing and that’s it. Unfortunately, it’s not so simple. We have to be very careful.
    Let’s go through some examples.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 也许你认为这些问题与我们的荒岛故事无关。我们只是取 100 条推文用于训练，再取 50 条用于验证，再取 50 条用于测试，仅此而已。不幸的是，事情并非如此简单。我们必须非常小心。让我们来看一些例子。
- en: 'Example 1: many random tweets'
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 示例 1：许多随机推文
- en: Assume that you scraped 1,000,000 completely random tweets from Twitter. Different
    authors, time, topics, localizations, numbers of reactions, etc. Just random.
    And they are in 10 languages and you want to use them to teach the model to recognize
    the language. Then you don’t have to worry about anything and you can simply draw
    900,000 tweets for the training set, 50,000 for the validation set and 50,000
    for the test set. This is called the **random split**.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你从 Twitter 上抓取了 1,000,000 条完全随机的推文。不同的作者、时间、主题、定位、反应数量等等。完全随机。而且这些推文有 10 种语言，你想用它们来训练模型识别语言。那么你不用担心任何问题，你可以简单地将
    900,000 条推文用于训练集，50,000 条用于验证集，50,000 条用于测试集。这被称为**随机分割**。
- en: Why draw at random, and not put the *first* 900,000 tweets in the training set,
    the *next* 50,000 in the validation set and the *last* 50,000 in the test set?
    Because the tweets can initially be sorted in a way that wouldn’t help, such as
    alphabetically or by the number of characters. And we have no interest in only
    putting tweets starting with ‘Z’ or the longest ones in the test set, right? So
    it’s just safer to draw them randomly.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么要随机抽取，而不是将*前* 900,000 条推文放入训练集，将*接下来的* 50,000 条推文放入验证集，将*最后的* 50,000 条推文放入测试集？因为推文最初可能会以对我们无益的方式排序，例如按字母顺序或按字符数量排序。我们并不希望仅将以‘Z’开头或最长的推文放入测试集，对吧？所以随机抽取会更安全。
- en: '![](../Images/cc306599720df5eefe68abad56c591f1.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cc306599720df5eefe68abad56c591f1.png)'
- en: 'Image 3: random data split. Image by author.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 图像 3：随机数据分割。图片来源于作者。
- en: The assumption that the tweets are *completely random* is strong. Always think
    twice if that’s true. In the next examples you’ll see what happens if it’s not.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 假设推文是*完全随机*的，这个假设是强的。总是要仔细考虑这是否真实。在接下来的示例中，你将看到如果不完全随机会发生什么。
- en: 'Example 2: not so many random tweets'
  id: totrans-63
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 示例 2：不是那么多随机推文
- en: 'If we only have 200 *completely random* tweets in 10 languages then we can
    still split them randomly. But then a new risk arises. Suppose that a language
    is predominant with 128 tweets and there are 8 tweets for each of the other 9
    languages. Probability says that then the chance that not all the languages will
    go to the 50-element test set is above 61% (info for math nerds: use the inclusion-exclusion
    principle). But we definitely want to test the model on all 10 languages, so we
    definitely need all of them in the test set. What should we do?'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们只有 200 条*完全随机*的推文在 10 种语言中，我们仍然可以随机分割它们。但这时就会出现新的风险。假设某种语言占主导地位，有 128 条推文，而其他
    9 种语言每种语言只有 8 条推文。概率表明，所有语言都进入 50 个测试集的可能性超过 61%（数学爱好者的说明：使用包容-排除原理）。但我们确实希望在所有
    10 种语言上测试模型，所以我们确实需要所有语言都在测试集中。我们应该怎么做？
- en: We can draw tweets class-by-class. So take the predominant class of 128 tweets,
    draw the 64 tweets for the training set, 32 for the validation set and 32 for
    the test set. Then do the same for all the other classes — draw 4, 2 and 2 tweets
    for training, validating and testing for each class respectively. This way, you’ll
    form three sets of the sizes you need, each with all classes in the same proportions.
    This strategy is called the **stratified random split**.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以按类别绘制推文。因此，取 128 条推文中占主导地位的类别，绘制 64 条用于训练集，32 条用于验证集和 32 条用于测试集。然后对所有其他类别执行相同的操作——每个类别分别绘制
    4 条、2 条和 2 条用于训练、验证和测试。这样，你就能形成三个所需大小的集合，每个集合中所有类别的比例相同。这种策略称为**分层随机分割**。
- en: The stratified random split seems better/safer than the ordinary random split,
    so why didn’t we use it in Example 1? Because we didn’t have to! What often defies
    intuition is that if 5% out of 1,000,000 tweets are in English and we draw 50,000
    tweets with no regard for language, then 5% of the tweets drawn will also be in
    English. This is how probability works. But probability needs big enough numbers
    to work properly, so if you have 1,000,000 tweets then you don’t care, but if
    you only have 200, watch out.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 分层随机拆分似乎比普通随机拆分更好/更安全，那么为什么我们在示例 1 中没有使用它呢？因为我们不需要！常常违反直觉的是，如果1,000,000条推文中有5%是英文的，而我们随机抽取50,000条推文而不考虑语言，那么抽取的推文中也有5%是英文的。这就是概率的工作原理。但概率需要足够大的数字才能正常工作，所以如果你有1,000,000条推文你就不用担心，但如果只有200条，就要小心了。
- en: 'Example 3: tweets from several institutions'
  id: totrans-67
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 示例 3：来自几个机构的推文
- en: Now assume that we’ve got 100,000 tweets, but they are from only 20 institutions
    (let’s say a news TV station, a big soccer club, etc.), and each of them runs
    10 Twitter accounts in 10 languages. And again our goal is to recognize the Twitter
    language *in general*. Can we simply use the random split?
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 现在假设我们有100,000条推文，但这些推文仅来自20个机构（比如一家新闻电视台、一家大型足球俱乐部等），而且每个机构在10种语言中运行10个Twitter账号。我们的目标仍然是识别Twitter语言*一般性*的。我们可以简单地使用随机拆分吗？
- en: 'You’re right — if we could, we wouldn’t have asked. But why not? To understand
    this, first let’s consider an even simpler case: what if we trained, validated
    and tested a model on tweets from one institution only? Could we use this model
    on any other institution’s tweets? We don’t know! Maybe the model would overfit
    the unique tweeting style of this institution. We wouldn’t have any tools to check
    it!'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 你说得对——如果可以，我们就不会提出这个问题了。但为什么不呢？为了理解这一点，我们首先考虑一个更简单的案例：如果我们仅在一个机构的推文上训练、验证和测试模型呢？我们可以将这个模型用于其他机构的推文吗？我们不知道！也许模型会过度拟合这个机构的独特推文风格。我们将没有任何工具来检查这一点！
- en: Let’s return to our case. The point is the same. The total number of 20 institutions
    is on the small side. So if we use data from the same 20 institutions to train,
    compare and score the models, then maybe the model overfits the 20 unique styles
    of these 20 institutions and will fail on any other author. And again there is
    no way to check it. Not good.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回到我们的案例。要点是一样的。总共有20个机构的数量较少。因此，如果我们使用这20个机构的数据来训练、比较和评分模型，那么模型可能会过度拟合这20个机构的独特风格，并且在其他作者上表现不佳。再次强调，我们没有方法来检查这一点。这不太好。
- en: 'So what to do? Let’s follow one more main rule:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 那么该怎么办呢？让我们遵循另一个主要规则：
- en: '**Validation and test sets should simulate the real case which the model will
    be applied to as faithfully as possible.**'
  id: totrans-72
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**验证集和测试集应尽可能真实地模拟模型应用的实际情况。**'
- en: Now the situation is clearer. Since we expect different authors in the final
    application than we have in our data, we should also have different authors in
    the validation and test sets than we have in the training set! And the way to
    do so is to **split data by institutions**! If we draw, for example, 10 institutions
    for the training set, another 5 for the validation set and put the last 5 in the
    test set, the problem is solved.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 现在情况更清楚了。由于我们期望最终应用中的作者与我们数据中的作者不同，因此我们在验证集和测试集中也应包含与训练集中不同的作者！实现这一点的方法是**按机构划分数据**！如果我们从10个机构中抽取用于训练集，从另外5个机构中抽取用于验证集，并将最后5个机构的数据放入测试集中，问题就解决了。
- en: '![](../Images/54fd63d7f09ce9b72d6148874d1efd9a.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/54fd63d7f09ce9b72d6148874d1efd9a.png)'
- en: 'Image 4: stratified data split. Image by author.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 图像 4：分层数据拆分。图像由作者提供。
- en: Note that any less strict split by institution (like putting the whole of 4
    institutions and a small part of the 16 remaining ones in the test set) would
    be a data leak, which is bad, so we have to be uncompromising when it comes to
    separating the institutions.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，任何较不严格的按机构拆分（例如将4个机构的全部数据和其余16个机构的一小部分数据放入测试集）都属于数据泄露，这样是不好的，因此我们在分离机构时必须不妥协。
- en: 'A sad final note: for a correct validation split by institution, we may trust
    our solution for tweets from different institutions. But tweets from private accounts
    may — and do — look different, so we can’t be sure the model we have will perform
    well for them. With the data we have, we have no tool to check it…'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 一个令人遗憾的最终说明：对于按机构进行的正确验证拆分，我们可以信任来自不同机构的推文。但来自私人账户的推文可能会——确实会——看起来不同，因此我们不能确定我们拥有的模型对这些推文表现良好。根据我们现有的数据，我们没有工具来检查这一点……
- en: 'Example 4: same tweets, different goal'
  id: totrans-78
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 示例 4：相同的推文，不同的目标
- en: Example 3 is hard, but if you went through it carefully then this one will be
    fairly easy. So, assume that we have exactly the same data as in Example 3, but
    now the goal is different. This time we want to recognize the language of other
    tweets from the same 20 institutions that we have in our data. Will the random
    split be OK now?
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 示例3比较难，但如果你仔细研究过，那么这个就相对容易了。假设我们有与示例3完全相同的数据，但这次目标不同。我们现在想识别来自相同20个机构的其他推文的语言。随机拆分现在还会合适吗？
- en: 'The answer is: yes. The random split perfectly follows the last main rule above
    as we are ultimately only interested in the institutions we have in our data.'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 答案是：可以。随机拆分完全遵循了上述最后一个主要规则，因为我们最终只对数据中存在的机构感兴趣。
- en: Examples 3 and 4 show us that the way we should split the data does not depend
    only on the data we have. It depends on both the data and the task. Please bear
    that in mind whenever you design the training/validation/test split.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 示例3和4告诉我们，数据拆分的方式不仅仅依赖于我们拥有的数据。它依赖于数据和任务。设计训练/验证/测试拆分时，请牢记这一点。
- en: 'Example 5: still the same tweets, yet another goal'
  id: totrans-82
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 示例5：仍然是相同的推文，但目标不同
- en: In the last example let’s keep the data we have, but now let’s try to teach
    a model to predict the institution from future tweets. So we once again have a
    classification task, but this time with 20 classes as we’ve got tweets from 20
    institutions. What about this case? Can we split our data randomly?
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在最后一个例子中，让我们保留我们拥有的数据，但现在尝试教一个模型预测未来推文中的机构。因此我们再次有一个分类任务，但这次有20个类别，因为我们有来自20个机构的推文。这个情况下怎么样？我们可以随机拆分数据吗？
- en: As before, let’s think about a simpler case for a while. Suppose we only have
    two institutions — a TV news station and a big soccer club. What do they tweet
    about? Both like to jump from one hot topic to another. Three days about Trump
    or Messi, then three days about Biden and Ronaldo, and so on. Clearly, in their
    tweets we can find *keywords* that change every couple of days. And what keywords
    will we see in a month? Which politician or villain or soccer player or soccer
    coach will be ‘hot’ then? Possibly one that is completely unknown right now. So
    if you want to learn to recognize the institution, you shouldn’t focus on *temporary*
    keywords, but rather try to catch the *general style*.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 和以前一样，让我们先考虑一个更简单的情况。假设我们只有两个机构——一个电视新闻台和一个大型足球俱乐部。他们会发什么推文？两者都喜欢从一个热门话题跳到另一个。三天谈特朗普或梅西，然后三天谈拜登和罗纳尔多，依此类推。显然，在他们的推文中我们可以找到*关键词*，这些关键词每隔几天就会变化。一个月后我们会看到什么关键词？哪个政治家、恶棍、足球运动员或足球教练会成为“热门”？可能是现在完全未知的。因此，如果你想学会识别机构，你不应该关注*暂时的*关键词，而是应该尝试捕捉*总体风格*。
- en: 'OK, let’s move back to our 20 institutions. The above observation remains valid:
    the topics of tweets change over time, so as we want our solution to work for
    future tweets, we shouldn’t focus on short-lived keywords. But a machine learning
    model is lazy. If it finds an easy way to fulfill the task, it doesn’t look any
    further. And sticking to keywords is just such an easy way. So how can we check
    whether the model learned properly or just memorized the temporary keywords?'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，让我们回到我们的20个机构。之前的观察仍然有效：推文的主题会随时间变化，因此我们希望我们的解决方案能适用于未来的推文，我们不应关注短期的关键词。但机器学习模型是懒惰的。如果它找到了一种简单的方式来完成任务，它不会再进一步查找。坚持使用关键词就是这种简单的方式。那么，我们如何检查模型是否学得正确，而不是仅仅记住了暂时的关键词呢？
- en: We’re pretty sure you realize that if you use the random split, you should expect
    tweets about every hero-of-the-week in all the three sets. So this way, you end
    up with the same keywords in the training, validation and test sets. This is not
    what we’d like to have. We need to split smarter. But how?
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 我们非常确定你会意识到，如果你使用随机拆分，你应该期待在所有三个集合中都包含关于每个周英雄的推文。这样，你最终会在训练、验证和测试集合中得到相同的关键词。这不是我们想要的。我们需要更聪明地拆分。但怎么做呢？
- en: When we go back to the last main rule, it becomes easy. We want to use our solution
    in future, so validation and test sets should be the future with respect to the
    training set! We should **split data by time**. So if we have, say, 12 months
    of data — from July 2022 up to June 2023 — then putting July 2022 — April 2023
    in the test set, May 2023 in the validation set and June 2023 in the test set
    should do the job.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们回到最后一个主要规则时，它变得简单了。我们希望在未来使用我们的解决方案，因此验证集和测试集应该是相对于训练集的未来！我们应该**按时间拆分数据**。所以如果我们有，比如说，12个月的数据——从2022年7月到2023年6月——那么把2022年7月到2023年4月放在测试集中，2023年5月放在验证集中，2023年6月放在测试集中应该就可以了。
- en: '![](../Images/cb008a58e8804a93d8dd32cb5b598152.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cb008a58e8804a93d8dd32cb5b598152.png)'
- en: 'Image 5: data split by time. Image by author.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：按时间划分的数据。图片由作者提供。
- en: 'Maybe you are concerned that with the split by time we don’t check the model’s
    quality throughout the seasons. You’re right, that’s a problem. But still a smaller
    problem than we’d get if we split randomly. You can also consider, for example,
    the following split: 1st-20th of every month to the training set, 20th-25th of
    every month to the validation set, 25th-last of every month to the test set. In
    any case, choosing a validation strategy is a trade-off between potential data
    leaks. As long as you understand it and consciously choose the safest option,
    you’re doing well.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 也许你会担心通过时间划分我们无法检查模型在整个季节中的质量。你说得对，这确实是一个问题。但这仍然是一个比随机划分要小的问题。你还可以考虑，例如，以下的划分方式：每月的1号到20号作为训练集，每月的20号到25号作为验证集，每月的25号到最后一天作为测试集。无论如何，选择一个验证策略是对潜在数据泄漏的权衡。只要你理解这一点并有意识地选择最安全的选项，你就做得很好。
- en: Summary
  id: totrans-91
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: We set our story on a desert island and tried our best to avoid any and all
    complexities — to isolate the issue of model validation and testing from all possible
    real-world considerations. Even then, we stumbled upon pitfall after pitfall.
    Fortunately, the rules for avoiding them are easy to learn. As you’ll likely learn
    along the way, they are also hard to master. You will not always notice the data
    leak immediately. Nor will you always be able to prevent it. Still, careful consideration
    of the believability of your validation scheme is bound to pay off in better models.
    This is something that remains relevant even as new models are invented and new
    frameworks are released.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将故事设定在一个沙漠岛屿上，并尽力避免任何复杂情况——将模型验证和测试的问题从所有可能的现实世界因素中隔离出来。即便如此，我们仍然遇到了一次又一次的陷阱。幸运的是，避免这些陷阱的规则很容易学习。正如你可能会在过程中学到的，它们也很难掌握。你不会总是立刻发现数据泄漏，也不总是能够防止它。尽管如此，仔细考虑你的验证方案的可信度一定会带来更好的模型。这一点即使在新模型被发明和新框架被发布时依然相关。
- en: Also, we’ve got 1000 men stranded on desert islands. A good model might be just
    what we need to rescue them in a timely manner.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们有1000名男子被困在沙漠岛屿上。一个好的模型可能正是我们需要的，以便及时救援他们。
