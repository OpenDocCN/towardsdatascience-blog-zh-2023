["```py\nfrom transformers import AutoTokenizer \nfrom langchain.text_splitter import RecursiveCharacterTextSplitter \n\n#This is the Flan-T5-Large model I used for the Kaggle competition \nllm = \"/mystuff/llm/flan-t5-large/flan-t5-large\" \ntokenizer = AutoTokenizer.from_pretrained(llm, local_files_only=True) \ntext_splitter = RecursiveCharacterTextSplitter\n                 .from_huggingface_tokenizer(tokenizer, chunk_size=12,\n                                             chunk_overlap=2,                        \n                                             separators=[\"\\n\\n\", \"\\n\", \". \"]) \nsection_text=\"Hello. This is some text to split. With a few \"\\ \n             \"uncharacteristic words to chunk, expecting 2 chunks.\" \ntexts = text_splitter.split_text(section_text) \nprint(texts)\n```", "```py\n['Hello. This is some text to split',\n '. With a few uncharacteristic words to chunk, expecting 2 chunks.']\n```", "```py\nsection_text=\"Hello. This is some text to split. With a few \"\\ \n             \"uncharacteristic words to chunk, expecting 2 chunks.\" \nencoded_text = tokenizer(section_text) \ntokens = tokenizer.convert_ids_to_tokens(encoded_text['input_ids']) \nprint(tokens)\n```", "```py\n['▁Hello', '.', '▁This', '▁is', '▁some', '▁text', '▁to', '▁split', '.', \n '▁With', '▁', 'a', '▁few', '▁un', 'character', 'istic', '▁words', \n '▁to', '▁chunk', ',', '▁expecting', '▁2', '▁chunk', 's', '.', '</s>']\n```", "```py\nq_embeddings[:10]\narray([-0.45518905, -0.6450379, 0.3097812, -0.4861114 , -0.08480848,\n -0.1664767 , 0.1875889, 0.3513346, -0.04495572, 0.12551129],\n```", "```py\nfrom sentence_transformers import SentenceTransformer, util\n\nembedding_model_path = \"/mystuff/llm/bge-small-en\" \nembedding_model = SentenceTransformer(embedding_model_path, device='cuda')\n```", "```py\nquestion = \"what is google bard?\" \nq_embeddings = embedding_model.encode(question)\n```", "```py\nq_embeddings.shape\n(, 384)\n\nq_embeddings[:10]\narray([-0.45518905, -0.6450379, 0.3097812, -0.4861114 , -0.08480848,\n       -0.1664767 , 0.1875889, 0.3513346, -0.04495572, 0.12551129],\n       dtype=float32)\n```", "```py\narticle_embeddings = embedding_model.encode(article_chunks)\n```", "```py\n[[ 0.042624 -0.131264 -0.266858 ... -0.329627 0.178211 0.248001]\n [-0.120318 -0.110153 -0.059611 ... -0.297150 -0.043165 0.558150]\n [ 0.116761 -0.066759 -0.498548 ... -0.330301 0.019448 0.326484]\n [-0.517585 0.183634 0.186501 ... 0.134235 -0.033262 0.498731]\n [-0.245819 -0.189427 0.159848 ... -0.077107 -0.111901 0.483461]]\n```", "```py\nimport torch \nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer \n\nrerank_model_path = \"/mystuff/llm/bge-reranker-base\"\nrerank_tokenizer = AutoTokenizer.from_pretrained(rerank_model_path) \nrerank_model = AutoModelForSequenceClassification \n                  .from_pretrained(rerank_model_path) \nrerank_model.eval() \n\ndef calculate_rerank_scores(pairs): \n    with torch.no_grad(): inputs = rerank_tokenizer(pairs, padding=True, \n                                          truncation=True, return_tensors='pt',\n                                          max_length=512) \n    scores = rerank_model(**inputs, return_dict=True) \n                         .logits.view(-1, ).float() \n    return scores \n\nquestion = questions[idx]\npairs = [(question, chunk) for chunk in doc_chunks_all[idx]] \nrerank_scores = calculate_rerank_scores(pairs) \ndf[\"rerank_score\"] = rerank_scores\n```", "```py\nfrom transformers import AutoTokenizer, AutoModelForCausalLM \nimport torch \n\nllm_answer_path = \"/mystuff/llm/zephyr-7b-beta\" \ntorch_device = \"cuda:0\" \ntokenizer = AutoTokenizer.from_pretrained(llm_answer_path, \n                                          local_files_only=True) \nllm_answer = AutoModelForCausalLM.from_pretrained(llm_answer_path, \n                           device_map=torch_device, local_files_only=True, \n                           torch_dtype=torch.float16) \n# assuming here that \"context\" contains the pre-built context \nquery = \"answer the following question, \"\\ \n        \"based on your knowledge and the provided context. \"\\n \n        \"Keep the answer concise.\\n\\nquestion:\" + question + \n        \"\\n\\ncontext:\"+context \n\ninput_ids = tokenizer.encode(query+\"\\n\\nANSWER:\", return_tensors='pt', \n                             return_attention_mask=False).to(torch_device) \ngreedy_output = llm_answer.generate(input_ids, max_new_tokens=1024, \n                                    do_sample=True) \nanswer = tokenizer.decode(greedy_output[0], skip_special_tokens=True) \nprint(answer[len(query):])\n```", "```py\nquery = \"what is google bard?\" \ninput_ids = tokenizer.encode(query+\"\\n\\nANSWER:\", return_tensors='pt', \n                             return_attention_mask=False).to(torch_device) \ngreedy_output = llm_answer.generate(input_ids, max_new_tokens=1024, \n                                    do_sample=True) \nanswer = tokenizer.decode(greedy_output[0], skip_special_tokens=True) \nprint(answer[len(query):])\n```", "```py\nANSWER:\nGoogle Bard is an experimental, AI-based language model developed by \nGoogle's sister company, DeepMind. Its primary use is to generate \nhuman-like text responses to prompts, which can help in tasks such as \ncontent creation, idea generation, and text summarization. Bard is \ntrained on a vast amount of textual data and can provide highly \nrelevant and contextually accurate responses, making it a useful tool \nin various applications where text generation is required. However, as \nan experimental feature, Bard's accuracy and effectiveness may still be \nimproving, and it is not yet publicly available for use.\n```", "```py\nquery = \"answer the following question, \"\\\n        \"based on your knowledge and the provided context. \"\\\n        \"Keep the answer concise.\\n\\n\"\\\n        \"question:\" + question + \"\\n\\ncontext:\"+context\ninput_ids = tokenizer.encode(query+\"\\n\\nANSWER:\", return_tensors='pt',\n                             return_attention_mask=False).to(torch_device)\ngreedy_output = llm_answer.generate(input_ids, max_new_tokens=1024, \n                                    do_sample=True)\nanswer = tokenizer.decode(greedy_output[0], skip_special_tokens=True)\nprint(answer[len(query):])\n```", "```py\nANSWER: \nBard is a conversational generative artificial intelligence (AI) \nchatbot developed by Google based initially on the LaMDA family of \nlarge language models (LLMs) and later PaLM. It was launched in a \nlimited capacity in March 2023 to mixed reviews, and expanded to other \ncountries in May, following the launch of Microsoft's Bing Chat. Google \nresearcher Jacob Devlin resigned from the company after alleging Bard \nhad surreptitiously leveraged data from ChatGPT. Tenor is an online GIF \nsearch engine and database owned by Google, available on Android, iOS, \nand macOS. Its main product is the GIF Keyboard, which is integrated \ninto numerous apps and messaging services. Bård is a Norwegian \nmasculine given name of Old Norse origin. It is sometimes used as a \nsurname. It may refer to several people. Tenor and Bård are not related.\n```", "```py\nANSWER:\nGoogle Bard is a conversational generative artificial intelligence (AI) \nchatbot, based initially on the LaMDA family of large language models \n(LLMs) and later on PaLM, developed by Google to compete with OpenAI's \nChatGPT. It was rolled out in a limited capacity in March 2023 and \nexpanded to more countries in May, prompting a mixed reception from \ncritics, who raised safety and ethical concerns about its accuracy and \nusefulness. Google has promised that Bard will be tightly integrated \nwith other Google AI products and services, leading to claims that a \nnew AI-powered version of the Google Assistant, dubbed \"Assistant with \nBard\", is being prepared for launch. Google has also stressed that Bard \nis still in its early stages and being continuously refined, with plans \nto upgrade it with new personalization and productivity features, while \nstressing that it remains distinct from Google Search.\n```", "```py\nimport seaborn as sns \nimport numpy as np \n\nfp_embeddings = embedding_model.encode(first_chunks) \nq_embeddings_reshaped = q_embeddings.reshape(1, -1) \ncombined_embeddings = np.concatenate((fp_embeddings, q_embeddings_reshaped)) \n\ndf_embedded_pca = pd.DataFrame(X_pca, columns=[\"x\", \"y\"]) \n# text is short version of chunk text (plot title) \ndf_embedded_pca[\"text\"] = titles \n# row_type = article or question per each embedding \ndf_embedded_pca[\"row_type\"] = row_types \n\nX = combined_embeddings pca = PCA(n_components=2).fit(X) \nX_pca = pca.transform(X) \n\nplt.figure(figsize=(16,10)) \nsns.scatterplot(x=\"x\", y=\"y\", hue=\"row_type\", \n                palette={\"article\": \"blue\", \"question\": \"red\"}, \n                data=df_embedded_pca, #legend=\"full\", \n                alpha=0.8, s=100 ) \nfor i in range(df_embedded_pca.shape[0]): \n   plt.annotate(df_embedded_pca[\"text\"].iloc[i], \n                (df_embedded_pca[\"x\"].iloc[i], df_embedded_pca[\"y\"].iloc[i]), \n                fontsize=20 ) \nplt.legend(fontsize='20') \n# Change the font size for x and y axis ticks plt.xticks(fontsize=16) \nplt.yticks(fontsize=16) \n# Change the font size for x and y axis labels \nplt.xlabel('X', fontsize=16) \nplt.ylabel('Y', fontsize=16)\n```", "```py\n['[CLS]', 'what', 'is', 'google', 'bard', '?', '[SEP]']\n```", "```py\nWhich of the following statements accurately describes the impact of \nModified Newtonian Dynamics (MOND) on the observed \"missing baryonic mass\" \ndiscrepancy in galaxy clusters?\n```"]