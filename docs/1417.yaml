- en: Linear Regression to GPT in Seven Steps
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将线性回归应用于 GPT 的七个步骤
- en: 原文：[https://towardsdatascience.com/linear-regression-to-gpt-in-seven-steps-cb3ab3173a14?source=collection_archive---------7-----------------------#2023-04-25](https://towardsdatascience.com/linear-regression-to-gpt-in-seven-steps-cb3ab3173a14?source=collection_archive---------7-----------------------#2023-04-25)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/linear-regression-to-gpt-in-seven-steps-cb3ab3173a14?source=collection_archive---------7-----------------------#2023-04-25](https://towardsdatascience.com/linear-regression-to-gpt-in-seven-steps-cb3ab3173a14?source=collection_archive---------7-----------------------#2023-04-25)
- en: How the humble prediction method shows us the way to Generative AI
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 这项朴素的预测方法如何指引我们进入生成式 AI 的世界
- en: '[](https://deveshrajadhyax.medium.com/?source=post_page-----cb3ab3173a14--------------------------------)[![Devesh
    Rajadhyax](../Images/35a81a5f148f83aea8b9353e030f79c7.png)](https://deveshrajadhyax.medium.com/?source=post_page-----cb3ab3173a14--------------------------------)[](https://towardsdatascience.com/?source=post_page-----cb3ab3173a14--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----cb3ab3173a14--------------------------------)
    [Devesh Rajadhyax](https://deveshrajadhyax.medium.com/?source=post_page-----cb3ab3173a14--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://deveshrajadhyax.medium.com/?source=post_page-----cb3ab3173a14--------------------------------)[![Devesh
    Rajadhyax](../Images/35a81a5f148f83aea8b9353e030f79c7.png)](https://deveshrajadhyax.medium.com/?source=post_page-----cb3ab3173a14--------------------------------)[](https://towardsdatascience.com/?source=post_page-----cb3ab3173a14--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----cb3ab3173a14--------------------------------)
    [Devesh Rajadhyax](https://deveshrajadhyax.medium.com/?source=post_page-----cb3ab3173a14--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fff46afc9b9c2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flinear-regression-to-gpt-in-seven-steps-cb3ab3173a14&user=Devesh+Rajadhyax&userId=ff46afc9b9c2&source=post_page-ff46afc9b9c2----cb3ab3173a14---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----cb3ab3173a14--------------------------------)
    ·9 min read·Apr 25, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fcb3ab3173a14&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flinear-regression-to-gpt-in-seven-steps-cb3ab3173a14&user=Devesh+Rajadhyax&userId=ff46afc9b9c2&source=-----cb3ab3173a14---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fff46afc9b9c2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flinear-regression-to-gpt-in-seven-steps-cb3ab3173a14&user=Devesh+Rajadhyax&userId=ff46afc9b9c2&source=post_page-ff46afc9b9c2----cb3ab3173a14---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----cb3ab3173a14--------------------------------)
    · 9 分钟阅读 · 2023 年 4 月 25 日'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fcb3ab3173a14&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flinear-regression-to-gpt-in-seven-steps-cb3ab3173a14&source=-----cb3ab3173a14---------------------bookmark_footer-----------)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fcb3ab3173a14&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flinear-regression-to-gpt-in-seven-steps-cb3ab3173a14&source=-----cb3ab3173a14---------------------bookmark_footer-----------)'
- en: There are numerous writings about Generative AI. There are essays dedicated
    to its applications, ethical and moral issues, and its risk to human society.
    If you want to understand the technology itself, there is a range of available
    material from the original research papers to introductory articles and videos.
    Depending on your current level and interest, you can find the right resources
    for study.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 关于生成式 AI 的写作非常多。有专门讨论其应用、伦理道德问题以及对人类社会风险的文章。如果你想了解技术本身，有从原始研究论文到入门文章和视频的各种材料可供选择。根据你当前的水平和兴趣，你可以找到合适的学习资源。
- en: This article is written for a specific class of readers. These readers have
    studied machine learning, though not as a major subject. They are aware that prediction
    and classification are the two main use cases of ML that cover most of its applications.
    They have also studied common machine learning algorithms for prediction and classification
    such as linear regression, logistic regression, support vector machines, decision
    trees and a bit of neural networks. They might have coded a few small projects
    in python using libraries such as scikit-learn and even used some pre-trained
    TensorFlow models like ResNet. I think a lot of students and professionals will
    be able to relate to this description.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 这篇文章是为特定读者群体写的。这些读者已经学习了机器学习，但不是作为主修科目。他们知道预测和分类是涵盖大多数应用的 ML 的两个主要用例。他们还学习了常见的预测和分类机器学习算法，如线性回归、逻辑回归、支持向量机、决策树以及一些神经网络。他们可能用
    Python 和 scikit-learn 等库编写过一些小项目，甚至使用过一些预训练的 TensorFlow 模型，如 ResNet。我认为许多学生和专业人士能够与这种描述产生共鸣。
- en: 'For these readers it is natural to wonder: is generative AI a new type of ML
    use case? It certainly seems different from both prediction and classification.
    There is enough jargon going around to discourage venturing into understanding
    generative AI. Terms such as transformers, multi-head attention, large language
    models, foundational models, sequence to sequence, and prompt engineering can
    easily persuade you that this is a very different world than the cozy prediction-classification
    one we used to know.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 对这些读者来说，生成式 AI 是一种新的 ML 用例吗？它似乎确实不同于预测和分类。流行的术语如变换器、多头注意力、大型语言模型、基础模型、序列到序列和提示工程，容易让人觉得这与我们熟悉的预测-分类世界截然不同。
- en: The message of this article is that generative AI is just a special case of
    prediction. If you fit the description of ML enthusiasts I gave earlier, then
    you can understand the basic working of generative AI in seven simple steps. I
    start with linear regression (LinReg), the ML technique that everyone knows. In
    this article I have treated a particular branch of generative AI called Large
    Language Models (LLM), largely because the wildly popular ChatGPT belongs to this
    branch.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 本文的核心观点是生成式 AI 只是预测的一个特例。如果你符合我之前描述的 ML 爱好者的特点，那么你可以在七个简单的步骤中理解生成式 AI 的基本工作原理。我从线性回归（LinReg）开始，这是每个人都知道的
    ML 技术。在本文中，我探讨了生成式 AI 的一个特定分支——大型语言模型（LLM），主要是因为广受欢迎的 ChatGPT 就属于这个分支。
- en: '![](../Images/bb968188821ce9ae6de24665eb49c2b8.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bb968188821ce9ae6de24665eb49c2b8.png)'
- en: Image by Rajashree Rajadhyax
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来自 Rajashree Rajadhyax
- en: 'Step 1: Prediction by Linear Regression'
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第 1 步：通过线性回归进行预测
- en: LinReg identifies the best line that represents the given data points. Once
    this line is found, it is used to predict the output for a new input.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: LinReg 确定代表给定数据点的最佳直线。一旦找到这条直线，就用它来预测新输入的输出。
- en: '![](../Images/bf77e70265bbc669331743fe3dffd3dc.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bf77e70265bbc669331743fe3dffd3dc.png)'
- en: Image by author
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来自作者
- en: 'We can write the LinReg model as a mathematical function. Written in an easy
    to understand way, it looks like:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将 LinReg 模型写成一个数学函数。以易于理解的方式写出来，它看起来像：
- en: '[PRE0]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'We can also draw a schematic for it:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也可以为其绘制一个示意图：
- en: '![](../Images/cc5f44b34cb1b8544c9f88151afc84d1.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cc5f44b34cb1b8544c9f88151afc84d1.png)'
- en: This is prediction at the most basic level. A LinReg model ‘learns’ the best
    line and uses it for prediction.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 这是最基本的预测。LinReg 模型‘学习’最佳直线并用它进行预测。
- en: 'Step 2: Prediction by Neural Networks'
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第 2 步：通过神经网络进行预测
- en: You can use LinReg only if you know that the data will fit a line. This is usually
    easy to do for single-input-single-output problems. We can simply draw a plot
    and inspect it visually. But in most real life problems, there are multiple inputs.
    We cannot visualize such a plot.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 只有在知道数据适合直线时，你才可以使用 LinReg。这通常对单输入单输出问题很容易做到。我们可以简单地绘制图表并直观检查。但在大多数现实问题中，有多个输入。我们无法可视化这样的图表。
- en: 'In addition, real world data does not always follow a linear path. Many times,
    the best fitting shape is non-linear. See the below plot:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，现实世界的数据并不总是遵循线性路径。许多时候，最佳拟合形状是非线性的。见下图：
- en: '![](../Images/ab7cf3aa9ddecf853a771a22d706ffbd.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ab7cf3aa9ddecf853a771a22d706ffbd.png)'
- en: Image by author
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来自作者
- en: Learning such a function in multi-dimensional data is not possible by simple
    methods like LinReg. This is where neural networks (NN) come in. NNs do not require
    us to decide which function they should learn. They find it themselves and then
    go on to learn the function, however complex it may be. Once a NN learns the complex,
    multi-input function, they use this function for prediction.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在多维数据中学习这样一个函数不可能通过简单的方法如LinReg来实现。这就是神经网络（NN）发挥作用的地方。NN不需要我们决定它们应该学习哪个函数。它们自己找到并学习函数，无论它多么复杂。一旦NN学习了复杂的、多输入的函数，它们就会使用这个函数进行预测。
- en: We can again write a similar equation, but with a change. Our inputs are now
    many, so we have to represent them by a vector. In fact, the output can also be
    many and we will use a vector for them too.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以再次写出类似的方程，但有所变化。我们的输入现在是多个，因此我们必须用一个向量来表示它们。实际上，输出也可以是多个，我们也会为它们使用一个向量。
- en: '[PRE1]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'We will draw the schematic of this new, more powerful prediction:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将绘制这种新的、更强大预测的示意图：
- en: '![](../Images/c8ea3d4bee71906a1eb97dee5bb3afe2.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c8ea3d4bee71906a1eb97dee5bb3afe2.png)'
- en: Image by author
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 作者的图片
- en: 'Step 3: Prediction on a word'
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 步骤 3：预测一个单词
- en: Now consider that we have a problem in which the input to the NN is a word from
    some language. Neural networks can only accept numbers and vectors. To suit this,
    words are converted into vectors. You can imagine them to be the residents of
    a many dimensional space, where related words are close to each other. For example,
    the vector for ‘Java’ will be close to other vectors for programming techniques;
    but it will also be close to the vectors for places in the Far-east, such as Sumatra.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 现在考虑我们有一个问题，输入到神经网络中的是某种语言中的一个单词。神经网络只能接受数字和向量。为了适应这一点，单词被转换为向量。你可以把它们想象成是多维空间中的居民，相关的单词彼此靠近。例如，‘Java’的向量将接近其他编程技术的向量；但它也会接近远东地区的地方，如苏门答腊的向量。
- en: '![](../Images/3c2a8a1c641f3e005a2f215d86c610d0.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3c2a8a1c641f3e005a2f215d86c610d0.png)'
- en: A (very imaginary) word embedding, image by author
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 作者的（非常虚构的）单词嵌入
- en: Such a set of vectors corresponding to words in a language is called an ‘Embedding’.
    There are many methods to create the embeddings; Word2Vec and GloVe being two
    popular examples. Typical sizes of such embeddings are 256, 512 or 1024.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 这样一组与语言中单词对应的向量称为‘嵌入’。有许多方法可以创建这些嵌入；Word2Vec 和 GloVe 是两个流行的例子。这些嵌入的典型大小为256、512或1024。
- en: Once we have vectors for words, we can use the NN for prediction on them. But
    what can we achieve by prediction on words? We can do a lot of things. We can
    translate a word to another language, get a synonym for the word or find its past
    tense. The equation and schematic for this prediction will look very similar to
    Step 3.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们有了单词的向量，我们可以在它们上使用NN进行预测。但是通过对单词进行预测，我们能实现什么呢？我们可以做很多事情。我们可以将一个单词翻译成另一种语言，找到该单词的同义词或找到它的过去式。这个预测的方程式和示意图看起来与步骤
    3 非常相似。
- en: '[PRE2]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '![](../Images/0dab89793568a814fc8408f37b47e359.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0dab89793568a814fc8408f37b47e359.png)'
- en: Image by author
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 作者的图片
- en: 'Step 4: Prediction for Naive Translation'
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 步骤 4：简单翻译的预测
- en: In a translation problem, the input is a sentence in one language and the output
    is a sentence in another language. How can we implement translation using what
    we already know about prediction on a word? Here we take a naive approach to translation.
    We convert each word of the input sentence to its equivalent in another language.
    Of course, real translation will not work like this; but for this step, pretend
    as if it will.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在翻译问题中，输入是一种语言的句子，输出是另一种语言的句子。我们如何利用对单词预测的已有知识来实现翻译呢？这里我们采用一种简单的翻译方法。我们将输入句子的每个单词转换为另一种语言中的等效单词。当然，真正的翻译不会像这样工作；但在这一步骤中，假装它会。
- en: 'This time we will draw the schematic first:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 这次我们先画出示意图：
- en: '![](../Images/20818d988257c4b43d22f9706eb20506.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/20818d988257c4b43d22f9706eb20506.png)'
- en: Image by author
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 作者的图片
- en: 'The equation for the first word will be:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个单词的方程式将是：
- en: '[PRE3]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: We can similarly write the equations for the other words.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以类似地为其他单词写出方程式。
- en: The neural network used here has learned the translation function by looking
    at many examples of word pairs, one from each language. We are using one such
    NN for each word.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 此处使用的神经网络通过查看许多单词对的示例来学习翻译功能。我们为每个单词使用一个这样的神经网络。
- en: We thus have a translation system using prediction. I have already admitted
    that this is a naive approach to translation. What are the additions that will
    make it work in the real world? We will see that in the next two steps.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们有了一个使用预测的翻译系统。我已经承认这是一种幼稚的翻译方法。有哪些改进可以使其在现实世界中有效？我们将在接下来的两步中看到。
- en: 'Step 5: Prediction with Context'
  id: totrans-53
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '步骤 5: 使用上下文的预测'
- en: 'The first problem with the naive approach is that the translation of one word
    depends on other words in the sentences. As an example, consider the following
    English to Hindi translation:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 幼稚方法的第一个问题是，一个词的翻译依赖于句子中的其他词。例如，考虑下面的英语到印地语的翻译：
- en: 'Input (English): ‘Ashok sent a *letter* to Sagar’'
  id: totrans-55
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '输入（英语）: ‘Ashok sent a *letter* to Sagar’'
- en: ''
  id: totrans-56
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Output (Hindi): ’Ashok ne Sagar ko *khat* bheja’.'
  id: totrans-57
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '输出（印地语）: ’Ashok ne Sagar ko *khat* bheja’。'
- en: 'The word ‘sent’ is translated as ‘bheja’ in the output. However, if the input
    sentence is:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 词‘sent’在输出中被翻译为‘bheja’。然而，如果输入句子是：
- en: 'Input (English): ‘Ashok sent *sweets* to Sagar’'
  id: totrans-59
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '输入（英语）: ‘Ashok sent *sweets* to Sagar’'
- en: Then the same word is translated as ‘bheji’.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，相同的词被翻译为‘bheji’。
- en: 'Output (Hindi): ’Ashok ne Sagar ko mithai bheji’.'
  id: totrans-61
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '输出（印地语）: ’Ashok ne Sagar ko mithai bheji’。'
- en: 'Thus it is necessary to add the context from other other words in the sentence
    while predicting the output. We will draw the schematic only for one word:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在预测输出时，必须添加句子中其他词的上下文。我们只为一个词绘制示意图：
- en: '![](../Images/21b37b734d502deb965f9b7b5e46d1b4.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/21b37b734d502deb965f9b7b5e46d1b4.png)'
- en: Image by author
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供
- en: There are many methods to generate the context. The most powerful and state-of-the-art
    is called ‘attention’. The neural networks that use attention for context generation
    are called ‘transformers’. Bert and GPT are examples of transformers.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 生成上下文的方法有很多种。最强大、最先进的方法称为‘注意力’。使用注意力进行上下文生成的神经网络称为‘变换器’。Bert 和 GPT 就是变换器的例子。
- en: 'We now have a kind of prediction that uses context. We can write the equation
    as:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在有了一种使用上下文的预测方法。我们可以将方程式写成：
- en: '[PRE4]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Step 6: Prediction of Next Word'
  id: totrans-68
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '步骤 6: 下一个词的预测'
- en: 'We will now handle the second problem in the naive approach to translation.
    Translation is not a one-to-one mapping of words. See the example from the previous
    step:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将处理幼稚翻译方法中的第二个问题。翻译不是单词的一对一映射。请参见前一步的例子：
- en: 'Input (English): ‘Ashok sent a letter to Sagar’'
  id: totrans-70
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '输入（英语）: ‘Ashok sent a letter to Sagar’'
- en: ''
  id: totrans-71
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Output (Hindi): ’Ashok ne Sagar ko khat bheja’.'
  id: totrans-72
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '输出（印地语）: ’Ashok ne Sagar ko khat bheja’。'
- en: You will notice that the order of the words is different, and there is no equivalent
    of the word ‘a’ in input, or the word ‘ne’ in the output. Our one-NN-per-word
    approach will not work in this case. In fact it will not work in most cases.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 你会注意到词的顺序不同，输入中没有‘a’的对应词，也没有输出中的‘ne’。我们每个词一个神经网络的方法在这种情况下不起作用。事实上，在大多数情况下它都不起作用。
- en: 'Fortunately there is a better method available. After giving the input sentence
    to an NN, we ask it to predict just one word, the word that will be the first
    word of the output sentence. We can represent this as:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，已经有了更好的方法。在将输入句子提供给神经网络（NN）之后，我们要求它仅预测一个词，即将作为输出句子第一个词的词。我们可以将其表示为：
- en: '![](../Images/d777acbdf5b1d53dab0771ebb7b1baf3.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d777acbdf5b1d53dab0771ebb7b1baf3.png)'
- en: Image by author
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供
- en: 'In our letter sending example, we can write this as:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们发送信件的例子中，我们可以将其写为：
- en: '[PRE5]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'To get the second word in output, we change our input to:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 为了获取输出中的第二个词，我们将输入更改为：
- en: '[PRE6]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'We have to also include this new input in the context:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还必须在上下文中包含这一新的输入：
- en: '[PRE7]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The NN will then predict the next (second) word in the sentence:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络将预测句子的下一个（第二个）词：
- en: '![](../Images/dcca414b204059a6613ed6a713233cdf.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/dcca414b204059a6613ed6a713233cdf.png)'
- en: 'This can be written as:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 这可以写成：
- en: '[PRE8]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: We can continue this process till the NN predicts the embedding for ‘.’, in
    other words, it signals that the output has ended.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以继续这个过程，直到神经网络预测出‘.’的嵌入，换句话说，它会发出输出已经结束的信号。
- en: We have thus reduced the translation problem to the ‘predict the next word’
    problem. In the next step we will see how this approach to translation leads us
    to a more generic Generative AI capability.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 我们因此将翻译问题简化为‘预测下一个词’的问题。在下一步中，我们将看到这种翻译方法如何引导我们走向更通用的生成 AI 能力。
- en: 'Step 7: Prediction for Generative AI'
  id: totrans-89
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '步骤 7: 生成 AI 的预测'
- en: 'The ‘prediction of next word’ method is not limited to translation. The NN
    can be trained to predict the next word in such a way that the output is the answer
    to a question, or an essay on the topic you specify. Imagine that the input to
    such Generative NN is the sentence:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: ‘下一个词的预测’方法不限于翻译。神经网络可以被训练来预测下一个词，使输出成为对问题的回答，或是你指定主题的文章。想象一下，输入到这样的生成型神经网络中的句子是：
- en: ‘Write an article on the effect of global warming on the Himalayan glaciers’.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: ‘写一篇关于全球变暖对喜马拉雅冰川影响的文章’。
- en: 'The input to the generative model is called a ‘prompt’. The GNN predicts the
    first word of the article, and then goes on predicting further words till it generates
    a whole nice essay. This is what the Large Language Models such as ChatGPT do.
    As you can imagine, the internals of such models are much more complex than what
    I have described here. But they contain the same essential components that we
    have seen: embeddings, attention and a next-word-prediction NN.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 生成模型的输入被称为‘提示’。生成型神经网络预测文章的第一个词，然后继续预测后续的词，直到生成整篇漂亮的文章。这就是大型语言模型如 ChatGPT 所做的。正如你可以想象的那样，这些模型的内部机制远比我在这里描述的要复杂。但它们包含了我们看到的相同基本组件：嵌入、注意力和下一个词预测神经网络。
- en: '![](../Images/e46a217c32951d76eb1348afffca73b0.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e46a217c32951d76eb1348afffca73b0.png)'
- en: Image by author
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片
- en: Apart from translation and content generation, LLMs can also answer questions,
    plan travel and do many other wonderful things. The basic method used in all these
    tasks remains prediction of the next word.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 除了翻译和内容生成，LLMs 还可以回答问题、规划旅行和做许多其他奇妙的事情。所有这些任务中使用的基本方法仍然是预测下一个词。
- en: Summary
  id: totrans-96
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: 'We started from the basic prediction technique using LinReg. We made the prediction
    problem more complex by adding vectors and then word embeddings. We learned how
    to apply prediction to language by solving the naive translation problem. While
    enhancing the naive method to handle real translation we got oriented with the
    essential elements of LLMs: context and next word prediction. We realized that
    Large Language Models are all about prediction on text sequences. We got familiar
    with important terms such as prompt, embeddings, attention and transformers.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从使用 LinReg 的基本预测技术开始。我们通过添加向量和词嵌入使预测问题变得更加复杂。我们通过解决简单的翻译问题学会了如何将预测应用于语言。在增强简单方法以处理真实翻译的过程中，我们熟悉了
    LLMs 的基本元素：上下文和下一个词预测。我们意识到大型语言模型的核心是对文本序列的预测。我们熟悉了如提示、嵌入、注意力和变换器等重要术语。
- en: 'The sequences don’t have to be text really. We can have any sequence such as
    images or sounds. The message of this article, which thus covers the whole gamut
    of generative AI, is:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 序列不一定非得是文本。我们可以使用任何序列，例如图片或声音。本文的信息涵盖了生成型人工智能的全部内容，即：
- en: Generative AI is prediction on sequences using neural networks.
  id: totrans-99
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 生成型人工智能是利用神经网络对序列进行预测。
