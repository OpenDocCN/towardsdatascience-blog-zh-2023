- en: 'Curse of Dimensionality: An Intuitive Exploration'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/curse-of-dimensionality-an-intuitive-exploration-1fbf155e1411?source=collection_archive---------3-----------------------#2023-12-30](https://towardsdatascience.com/curse-of-dimensionality-an-intuitive-exploration-1fbf155e1411?source=collection_archive---------3-----------------------#2023-12-30)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[](https://medium.com/@salih.salih?source=post_page-----1fbf155e1411--------------------------------)[![Salih
    Salih](../Images/220f3c5363989d94c5593eca7ff72c67.png)](https://medium.com/@salih.salih?source=post_page-----1fbf155e1411--------------------------------)[](https://towardsdatascience.com/?source=post_page-----1fbf155e1411--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----1fbf155e1411--------------------------------)
    [Salih Salih](https://medium.com/@salih.salih?source=post_page-----1fbf155e1411--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F2037cbb08e24&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcurse-of-dimensionality-an-intuitive-exploration-1fbf155e1411&user=Salih+Salih&userId=2037cbb08e24&source=post_page-2037cbb08e24----1fbf155e1411---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----1fbf155e1411--------------------------------)
    ·11 min read·Dec 30, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F1fbf155e1411&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcurse-of-dimensionality-an-intuitive-exploration-1fbf155e1411&user=Salih+Salih&userId=2037cbb08e24&source=-----1fbf155e1411---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F1fbf155e1411&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcurse-of-dimensionality-an-intuitive-exploration-1fbf155e1411&source=-----1fbf155e1411---------------------bookmark_footer-----------)![](../Images/8e1fab29575e7aec7aa57966239b2c2f.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Photo by Mathew Schwartz on Unsplash
  prefs: []
  type: TYPE_NORMAL
- en: '**Introduction**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous article, we discussed the [surprising behavior of data in higher
    dimensions](/the-surprising-behavior-of-data-in-higher-dimensions-1c49bca9bbee).
    We found that volume tends to accumulate in the corners of spaces in a strange
    way, and we simulated a hypersphere inscribed inside a hypercube to investigate
    this, observing an interesting decrease in their volume ratio as the dimensions
    grew. Examples that demonstrated the advantages of multi-dimensional thinking
    were the DVD-paper experiment and the kernel trick in support vector machines(SVMs).
  prefs: []
  type: TYPE_NORMAL
- en: Today, we will be looking at some of the difficult aspects of high-dimensional
    data which is referred to as curse of dimensionality. Our goal is to have an intuitive
    understanding of this concept and its practical implications. The diagram below
    outlines how our article is structured.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0bcf5fff1009e705ae4a9da30c8ce22e.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the Curse of Dimensionality
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: “Curse of dimensionality” is a term that was first used by Richard E. Bellman
    back in the 1960s. It began as Bellman’s idea from dynamic optimization and it
    turned out to be a fundamental concept for understanding complexity in high-dimensional
    spaces.
  prefs: []
  type: TYPE_NORMAL
- en: Good, but what is “curse of dimensionality”?
  prefs: []
  type: TYPE_NORMAL
- en: It is at its core the difficulties and unique characteristics one faces when
    working with data in high-dimensional spaces( in our case this refers to having
    many features, columns or attributes in datasets). These spaces go far beyond
    our experience of everyday life in three-dimensional space.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: When we increase the number of dimensions on a dataset, the volume it occupies
    expands exponentially. This might appear initially as an advantage — more space
    could mean more data and probably more insights? However, that is not the case
    because having many dimensions comes with a number of challenges which change
    how we need to deal with and understand these high-dimensional data.
  prefs: []
  type: TYPE_NORMAL
- en: Key Challenges
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The shift from low-dimensional to high-dimensional data faces several harsh
    challenges. There are two, which stand out because they have the most significant
    effects: 1) sparsity of data; 2) the issue with distance metric. Each of them
    makes analysis in higher dimensions even more complex.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Data Sparsity: Islands in an Ocean of Emptiness'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Data sparsity in highly dimensional spaces is like few small islands lost within
    a vast ocean. When dimensionality increases, data points that were close together
    in lower dimensions become increasingly separated. This is due to the fact that
    the amount of space expands exponentially with each new addition of another dimension.
    Just imagine a cube becoming a hypercube; its corners move further away from its
    center and make it more empty inside. This growing emptiness is what we refer
    to as data sparsity.
  prefs: []
  type: TYPE_NORMAL
- en: Many data analysis techniques struggle with sparsity. For example, many clustering
    algorithms depend on closely situated data points to form meaningful clusters.
    However, when data points become too dispersed, these algorithms face difficulties.
  prefs: []
  type: TYPE_NORMAL
- en: 'Distance Metric Problems: When Proximity Loses Meaning'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In high-dimensional spaces, distance metrics encounter significant challenges.
    Metrics like Euclidean or Manhattan distances, which are useful for measuring
    proximity between data points in lower dimensions, lose their effectiveness. In
    these expanded spaces, distances start to converge. This means that most pairs
    of points become nearly equidistant from each other and from a reference point.
    This convergence makes it harder to distinguish between close neighbors and distant
    ones.
  prefs: []
  type: TYPE_NORMAL
- en: In tasks like classification, where distance measurements are crucial for categorizing
    new data points, these metrics become less effective. As a result, algorithm performance
    drops, leading to less accurate predictions and analyses.
  prefs: []
  type: TYPE_NORMAL
- en: To better understand how distance behavior changes in higher dimensions, let’s
    perform a simple simulation. We will generate random points in both low and high-dimensional
    spaces. This will allow us to observe and compare the distribution of distances,
    showing us how these distances evolve as we move to higher dimensions.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/bf5c048e94e5f1e2a0de989d9924256c.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: The code output shows how distances change across dimensions. In 3D, there are
    different distances between points. In 100D, distances between points tend to
    be similar. Graphs to the right also show that as dimensions increase, the mean
    distance between points gets bigger, but the standard deviation stays roughly
    the same as it was on 2D or 3D space.
  prefs: []
  type: TYPE_NORMAL
- en: Another note here is that as dimensions increase, the mean distance between
    points gets bigger and approaches the maximum distance. This happens because most
    of the space is concentrated in the corners.
  prefs: []
  type: TYPE_NORMAL
- en: To better understand this, we can simulate random points in dimensions up to
    100D. This will let us compare the average distance to the maximum distance.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/dd0ddb65e50f1495c27b26f232ce9068.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: The graph shows that as we go into higher dimensions, the average distance gets
    closer to the maximum distance. We used normalization in here to make sure the
    scales were accurate.
  prefs: []
  type: TYPE_NORMAL
- en: It’s important to understand the difference between absolute and relative distances.
    While absolute distances generally increase with more dimensions, it’s the relative
    differences that matter more. Clustering algorithms like K-means or DBSCAN work
    by looking at how points are positioned compared to each other, not their exact
    distances. This lets us find patterns and relationships that we might miss if
    we only looked at the distances.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'But this leads to an interesting question: why do pairs of points in high-dimensional
    spaces tend to be roughly the same distance apart as we add more dimensions? What
    causes this to happen?'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/00dea8767aeac0aefd50ef6f96d1b33d.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by Aakash Dhage on Unsplash
  prefs: []
  type: TYPE_NORMAL
- en: To understand why pairs of points in high-dimensional spaces become equidistant,
    we can look at the Law of Large Numbers (LLN). This statistical principle suggests
    that as we increase our sample size or the number of dimensions, the average of
    our observations gets closer to the expected value.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s consider the example of rolling a fair six-sided dice. The expected mean
    of a roll is 3.5, which is the average of all possible outcomes. Initially, with
    just a few rolls, like 5 or 10, the average might be significantly different from
    3.5 due to randomness. But as we increase the number of rolls to hundreds or thousands,
    the average roll value gets closer to 3.5\. This phenomenon, where the average
    of many trials aligns with the expected value, shows the essence of the LLN. It
    demonstrates that while individual outcomes are unpredictable, the average becomes
    highly predictable over many trials.
  prefs: []
  type: TYPE_NORMAL
- en: Now, how does this relate to distances in high-dimensional spaces?
  prefs: []
  type: TYPE_NORMAL
- en: The Euclidean distance between two points in an n-dimensional space is calculated
    by summing the squared differences across each dimension. We can think of each
    squared difference as a random variable, similar to a roll of a dice. As the number
    of dimensions (or rolls) increases, the sum of these ‘rolls’ gets closer to an
    expected value.
  prefs: []
  type: TYPE_NORMAL
- en: 'A crucial requirement for the LLN is the independence of random variables.
    In high-dimensional vectors, this independence might be shown through an interesting
    geometric property: **the vectors tend to be almost orthogonal to each other.**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Try running the code above and editing the number of dimensions/ trials, and
    you can notice that vectors in higher dimensions are almost orthogonal.
  prefs: []
  type: TYPE_NORMAL
- en: 'The angle between two vectors, A and B, is determined by the cosine of the
    angle, which is derived from their dot product and magnitudes. The formula is
    expressed as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c4ac1aade2a563f24149ae4c11fd32de.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, *A*⋅*B* represents the dot product of vectors A and B, and ∥*A*∥ and ∥*B*∥
    are their respective magnitudes. For two vectors to be orthogonal, the angle between
    them must be 90 degrees, making cos(*θ*) equal to zero. Typically, this is achieved
    when the dot product *A*⋅*B* is zero, a condition familiar in lower dimensions.
  prefs: []
  type: TYPE_NORMAL
- en: However, in high-dimensional spaces, another phenomenon emerges. **The ratio
    of the dot product to the magnitude of the vectors becomes so small that we can
    consider the vectors to be ‘almost orthogonal.’**
  prefs: []
  type: TYPE_NORMAL
- en: But what does it mean for two vectors to be ‘independent’ in this context?
  prefs: []
  type: TYPE_NORMAL
- en: 'Navigating a Grid City: An Analogy for Independence in High Dimensions'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Imagine you are in a city laid out in a grid, like Manhattan’s streets. Picture
    yourself at an intersection, trying to reach another point in this city. In this
    analogy, each street represents a dimension in a high-dimensional space. Moving
    along a street is like changing the value in one dimension of a high-dimensional
    vector. Moving along one street doesn’t affect your position on another street,
    just like changing one dimension doesn’t affect the others.
  prefs: []
  type: TYPE_NORMAL
- en: To reach a specific intersection, you make a series of independent decisions,
    like calculating distance in high-dimensional space. Each decision contributes
    independently but leads you to your destination.
  prefs: []
  type: TYPE_NORMAL
- en: This analogy also applies to the concept of orthogonality in high-dimensional
    vectors. When vectors are almost orthogonal, they follow their own paths without
    significantly influencing each other. This condition complements the need for
    statistical independence for the LLN.
  prefs: []
  type: TYPE_NORMAL
- en: 'An important note: while this analogy of LLN offers a helpful perspective,
    it may not capture all the idea or causes behind this behavior. However, it serves
    as a useful proxy, providing an understanding of what the reason **might** be
    for pairs of point to be almost equidistant.'
  prefs: []
  type: TYPE_NORMAL
- en: Practical Implications
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One way the curse of dimensionality problems show up is overfitting. Overfitting
    happens when a complex model learns noise instead of the patterns in the data.
    This is especially true in high-dimensional spaces where there are many features.
    The model can make false connections or correlations and perform poorly when it
    sees new data(failing to generalize).
  prefs: []
  type: TYPE_NORMAL
- en: The curse also makes it hard to find patterns in big datasets. High-dimensional
    data is spread out and sparse, so it’s challenging for traditional analysis methods
    to find meaningful insights. Some changes or specialized methods are needed to
    navigate and understand this type of data.
  prefs: []
  type: TYPE_NORMAL
- en: Another implication is that processing high-dimensional data takes a lot of
    computational power and memory. Algorithms that work well in lower dimensions
    become much more complex and resource-heavy as the number of dimensions increases.
    This means either having more powerful hardware or optimizing algorithms to handle
    the increased computational load efficiently.
  prefs: []
  type: TYPE_NORMAL
- en: '**How to address curse of dimensionality?**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are several strategies to deal with the curse of dimensionality. One way
    is to reduce the dimensionality while keeping the important information(ex. PCA
    algorithm). Another method is manifold learning(can be considered as a type of
    dimensionality reduction).which uncovers the structure within the high-dimensional
    data. The key idea behind manifold learning is that many high-dimensional datasets
    actually lie on a lower-dimensional manifold within the high-dimensional space(ex.
    Isomaps)
  prefs: []
  type: TYPE_NORMAL
- en: Note here that -generally speaking- traditional dimensionality reduction techniques
    like PCA (Principal Component Analysis) focus on preserving global data structure
    and variance in a linear fashion. In contrast, manifold learning techniques like
    Isomap(Isometric Mapping) emphasize uncovering the underlying non-linear structure(manifold)
    of data, aiming to preserve local relationships and geometrical features.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Feature selection is also an option, where relevant features are chosen to improve
    model performance. Regularization techniques prevent overfitting by shrinking
    less important features. Increasing the sample size can also help, although it
    may not always be possible. These methods can help us analyze high-dimensional
    data more accurately and efficiently.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The curse of dimensionality is one of the most important problems in data science
    and machine learning. It happens when dealing with high-dimensional spaces. Two
    significant challenges that arise are data sparsity and issues with distance metrics.
    These challenges can cause overfitting in machine learning models and make computations
    more complex. To tackle these challenges, strategies like dimensionality reduction,
    feature selection, and regularization techniques can be used.
  prefs: []
  type: TYPE_NORMAL
- en: If you have made it this far, I would like to thank you for spending time reading
    this! I hope you found the topic enjoyable and at least inspiring enough to delve
    deeper into the world of high-dimensional data. Please feel free to suggest any
    edits or point out any mistakes or inaccuracies.
  prefs: []
  type: TYPE_NORMAL
