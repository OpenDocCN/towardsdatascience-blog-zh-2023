- en: 'Large Language Models: DeBERTa — Decoding-Enhanced BERT with Disentangled Attention'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/large-language-models-deberta-decoding-enhanced-bert-with-disentangled-attention-90016668db4b?source=collection_archive---------2-----------------------#2023-11-28](https://towardsdatascience.com/large-language-models-deberta-decoding-enhanced-bert-with-disentangled-attention-90016668db4b?source=collection_archive---------2-----------------------#2023-11-28)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Exploring the advanced version of the attention mechanism in Transformers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@slavahead?source=post_page-----90016668db4b--------------------------------)[![Vyacheslav
    Efimov](../Images/db4b02e75d257063e8e9d3f1f75d9d6d.png)](https://medium.com/@slavahead?source=post_page-----90016668db4b--------------------------------)[](https://towardsdatascience.com/?source=post_page-----90016668db4b--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----90016668db4b--------------------------------)
    [Vyacheslav Efimov](https://medium.com/@slavahead?source=post_page-----90016668db4b--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fc8a0ca9d85d8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flarge-language-models-deberta-decoding-enhanced-bert-with-disentangled-attention-90016668db4b&user=Vyacheslav+Efimov&userId=c8a0ca9d85d8&source=post_page-c8a0ca9d85d8----90016668db4b---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----90016668db4b--------------------------------)
    ·9 min read·Nov 28, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F90016668db4b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flarge-language-models-deberta-decoding-enhanced-bert-with-disentangled-attention-90016668db4b&user=Vyacheslav+Efimov&userId=c8a0ca9d85d8&source=-----90016668db4b---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F90016668db4b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flarge-language-models-deberta-decoding-enhanced-bert-with-disentangled-attention-90016668db4b&source=-----90016668db4b---------------------bookmark_footer-----------)![](../Images/bb49940006ced82ce8dc2dc788b1226d.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In recent years, BERT has become the number one tool in many natural language
    processing tasks. Its outstanding ability to process, understand information and
    construct word embeddings with high accuracy reach state-of-the-art performance.
  prefs: []
  type: TYPE_NORMAL
- en: As a well-known fact, BERT is based on the **attention** mechanism derived from
    the Transformer architecture. Attention is the key component of most large language
    models nowadays.
  prefs: []
  type: TYPE_NORMAL
- en: '[](/bert-3d1bf880386a?source=post_page-----90016668db4b--------------------------------)
    [## Large Language Models: BERT — Bidirectional Encoder Representations from Transformer'
  prefs: []
  type: TYPE_NORMAL
- en: Understand how BERT constructs state-of-the-art embeddings
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/bert-3d1bf880386a?source=post_page-----90016668db4b--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: Nevertheless, new ideas and approaches evolve regularly in the machine learning
    world. One of the most innovative techniques in BERT-like models appeared in 2021
    and introduced an enhanced attention version called “**Disentangled attention**”.
    The implementation of this concept gave rise to **DeBERTa** — the model incorporating
    disentangled attention. Though DeBERTa introduces only a pair of new architecture
    principles, its improvements are prominent on top NLP benchmarks, compared to
    other large models.
  prefs: []
  type: TYPE_NORMAL
- en: In this article, we will refer to the original [DeBERTa paper](https://arxiv.org/pdf/2006.03654.pdf)
    and cover all the necessary details to understand how it works.
  prefs: []
  type: TYPE_NORMAL
- en: 1\. Disentangled attention
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the original Transformer block, each token is represented by a single vector
    which contains information about token content and position in the form of the
    element-wise embedding sum. The disadvantage of this approach is potential information
    loss: the model might not differentiate whether a word itself or its position
    gives more importance to a certain embedded vector component.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d1b3f4d2a1192c1a465ec4c12966365c.png)'
  prefs: []
  type: TYPE_IMG
- en: Embedding construction in BERT and DeBERTa. Instead of storing all the information
    in a single vector, DeBERTa uses two separate vectors to store word and position
    embeddings.
  prefs: []
  type: TYPE_NORMAL
- en: DeBERTa proposes a novel mechanism in which the same information is stored in
    two different vectors. Furthermore, the algorithm for attention computation is
    also modified to explicitly take into account the relations between the content
    and positions of tokens. For instance, the words *“research”* and *“paper”* are
    much more dependent when they appear near each other than in different text parts.
    This example clearly justifies why it is necessary to consider content-to-position
    relations as well.
  prefs: []
  type: TYPE_NORMAL
- en: 'The introduction of disentangled attention requires modification in attention
    score computation. As it turns out, this process is very simple. Calculation of
    cross-attention scores between two embeddings each consisting of two vectors can
    be easily decomposed into the sum of four pairwise multiplication of their subvectors:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/fac234d13e63f69ecd245e2272935594.png)'
  prefs: []
  type: TYPE_IMG
- en: Computation of cross-attentin score between two embedding vectors.
  prefs: []
  type: TYPE_NORMAL
- en: 'The same methodology can be generalized in the matrix form. From the diagram,
    we can observe four different types of matrices (vectors) each representing a
    certain combination of content and position information:'
  prefs: []
  type: TYPE_NORMAL
- en: '*content-to-content* matrix;'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*content-to-position* matrix;'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*position-to-content* matrix;'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*position-to-position* matrix.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is possible to observe position-to-position matrix does not store any valuable
    information as it does not have any details on the words’ content. This is the
    reason why this term is discarded in disentangled attention.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: For the resting three terms, the final output attention matrix is calculated
    similarly as in the original Transformer.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9764f1b5ed0a7066a6f49ab5019453d1.png)'
  prefs: []
  type: TYPE_IMG
- en: Output disentangled attention calculation in DeBERTa
  prefs: []
  type: TYPE_NORMAL
- en: Even though the calculation process looks similar, there is a pair of subtleties
    that need to be taken into consideration.
  prefs: []
  type: TYPE_NORMAL
- en: From the diagram above, we can notice that the multiplication symbol *** used
    for multiplication between *query-content Qc and key-position Krᵀ matrices* &
    *key-content Kc and query-position Qrᵀ matrices* differs from the normal matrix
    multiplication symbol *x*. In reality, this is done not by accident as the mentioned
    pairs of matrices in DeBERTa are multiplied in slightly another way to take into
    account the relative positioning of tokens.
  prefs: []
  type: TYPE_NORMAL
- en: According to the normal matrix multiplication rules, if *C = A x B*, then the
    element *C[i][j]* is computed by element-wise multiplication of the *i*-th row
    of *A* by the *j*-th column of *B*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In a special case of DeBERTa, if *C = A * B*, then *C[i][j]* is calculated
    as the multiplication of the *i*-th row of *A* by *δ(i, j)*-th column of *B* where
    *δ* denotes a relative distance function between indexes *i* and *j* which is
    defined by the formula below:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/3dd94b4a3f79eb8bfb1be35a58acb77a.png)'
  prefs: []
  type: TYPE_IMG
- en: Relative distance definition between indexes i and j. k is a hyperparameter
    controlling the maximum possible relative distance. Image adopted by the author.
  prefs: []
  type: TYPE_NORMAL
- en: '*k* can be thought of as a hyperparameter controlling the maximum possible
    relative distance between indexes *i* and *j*. In DeBERTa, *k* is set to 512\.
    To get a better sense of the formula, let us plot a heatmap visualising relative
    distances (*k = 6*) for different indexes of *i* and *j*.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b8a55757372bc75fa189973b2517bc74.png)'
  prefs: []
  type: TYPE_IMG
- en: For example, if *k = 6*, *i = 15* and *j = 13*, then the relative distance *δ*
    between *i* and *j* is equal to 8\. To obtain a content-to-position score for
    indexes *i = 15* and *j = 13*, during the multiplication of query-content *Qc*
    and key-position *Kr* matrices, the 15-th row of *Qc* should be multiplied by
    the 8-th column of *Kr*ᵀ.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1669db0b584b51badfa0185f0658d4db.png)'
  prefs: []
  type: TYPE_IMG
- en: Content-to-position score computation for tokens i and j
  prefs: []
  type: TYPE_NORMAL
- en: 'However, for position-to-content scores, the algorithm works a bit differently:
    instead of the relative distance being *δ(i, j)*, this time the algorithm uses
    the value of *δ(j, i)* in matrix multiplication. As the authors of the paper explain:
    “*this is because* *for a given position i, position-to-content computes the attention
    weight of the key content at j with respect to the query position at i, thus the
    relative distance is δ(j, i)”.*'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/415637a6a2592a6ad3ba601bbdc4da59.png)'
  prefs: []
  type: TYPE_IMG
- en: Position-to-content score computation for tokens i and j
  prefs: []
  type: TYPE_NORMAL
- en: δ(i, j) ≠ δ(j, i), i.e. δ is not a symmetric function meaning that the distance
    between i and j is not the same as the distance between j and i.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Before applying the softmax transformation, attention scores are divided by
    a constant *√(3d)* for more stable training. This scaling factor is different
    to the one used in the original Transformer (*√d*). This difference in *√*3 timesis
    justified by larger magnitudes resulting from the summation of 3 matrices in the
    DeBERTa attention mechanism (instead of a single matrix in Transformer).
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Enhanced mask decoder
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Disentangled attention takes into account only content and relative positioning.
    However, no information about absolute positioning is considered which might actually
    play an important role in ultimate prediction. The authors of the DeBERTa paper
    give a concrete example of such a situation: a sentence “*a new store opened beside
    the new mall*” which is fed to BERT with the masked words “*store*” and “*mall*”
    for prediction. Though the masked words have a similar meaning and local context
    (the adjective “*new*”), they represent completely different roles in the sentence.
    Without the full information about starting and ending positions of the masked
    words, it becomes much harder to correctly restore the original sentence.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/def2c9300938cc8902ce7f883fa659a9.png)'
  prefs: []
  type: TYPE_IMG
- en: Text example from the [DeBERTa paper](https://arxiv.org/pdf/2006.03654.pdf).
    By using only disentangled attention, the model cannot correctly restore the original
    phrase.
  prefs: []
  type: TYPE_NORMAL
- en: To better understand this problem, imagine that you are aware that in reality
    the mall opened first and the store opened after it. Then you have to fill in
    the sentence *a new ___ store opened beside the new ___* . As an English speaker,
    you are aware that whatever comes after the construction *opened beside,* grammatically
    indicates that it opened first. At the same time, whatever comes before these
    words, opened later. Therefore, you confidently fill it with the words *store*
    and *mall* respectively. Why was it easy to do it for you? Because as a human,
    you naturally consider the absolute positions of the masked words.
  prefs: []
  type: TYPE_NORMAL
- en: Right now imagine that you did not know anything about the absolute positions
    of the masked words. Thus it would be impossible for you to use the mentioned
    hint about the grammatical word order around the construction *opened beside*.
    As a result, despite having semantic meanings of the words and their local context,
    you would still not be able to give a correct answer. This is the analogous situation
    for the model when it does not have access to absolute positioning.
  prefs: []
  type: TYPE_NORMAL
- en: In a language there can be numerous similar examples, which is why it is crucial
    to incorporate **absolute positioning** into the model.
  prefs: []
  type: TYPE_NORMAL
- en: In BERT, absolute positioning is taken into account in input embeddings. Speaking
    of DeBERTa, it incorporates absolute positioning after all Transformer layers
    but before applying the softmax layer. It was shown in experiments that capturing
    relative positioning in all Transformer layers and only after introducing absolute
    positioning improves the model’s performance. According to the researchers, doing
    it inversely could prevent the model from learning sufficient information about
    relative positioning.
  prefs: []
  type: TYPE_NORMAL
- en: Architecture
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'According to the [paper](https://arxiv.org/pdf/2006.03654.pdf), the enhanced
    mask decoder (EMD) has two input blocks:'
  prefs: []
  type: TYPE_NORMAL
- en: '*H* — the hidden states from the previous Transformer layer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*I* — any necessary information for decoding (e.g. hidden states *H*, absolute
    position embedding or output from the previous EMD layer).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/9b644de9e542c66fd583e2b171cae780.png)'
  prefs: []
  type: TYPE_IMG
- en: Enhanced mask decoder in DeBERTa. Image adopted by the author.
  prefs: []
  type: TYPE_NORMAL
- en: 'In general, there can be multiple *n* EMD blocks inside a model. If so, they
    are constructed with the following rules:'
  prefs: []
  type: TYPE_NORMAL
- en: the output of each EMD layer is the input *I* for the next EMD layer;
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: the output of the last EMD layer is fed to the language model head.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the case of DeBERTa, the number of EMD layers is set to *n = 2* with the
    position embedding used for *I* in the first EMD layer.
  prefs: []
  type: TYPE_NORMAL
- en: Another frequently used technique in NLP is weights sharing across different
    layers with the objective of reducing the model complexity (e.g. [ALBERT](https://medium.com/towards-data-science/albert-22983090d062)).
    This idea is also implemented in EMD blocks of DeBERTa.
  prefs: []
  type: TYPE_NORMAL
- en: '[](/albert-22983090d062?source=post_page-----90016668db4b--------------------------------)
    [## Large Language Models, ALBERT — A Lite BERT for Self-supervised Learning'
  prefs: []
  type: TYPE_NORMAL
- en: Understand essential techniques behind BERT architecture choices for producing
    a compact and efficient model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/albert-22983090d062?source=post_page-----90016668db4b--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: When I = H and n = 1, EMD becomes the equivalent of the BERT decoder layer.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: DeBERTa settings
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Ablation studies
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Experiments demonstrated that all introduced components in DeBERTa (position-to-content
    attention, content-to-position attention and enhanced mask decoder) boost performance.
    Removing any of them would result in inferior metrics.
  prefs: []
  type: TYPE_NORMAL
- en: Scale-invariant-fine Tuning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Additionally, the authors proposed a new adversarial algorithm called “**Scale
    Invariant Fine-Tuning**” to improving the model’s generalization. The idea is
    to incorporate small perturbations to input sequences making the model more resilient
    to adversial examples. In DeBERTa, perturbations are applied to normalized input
    word embeddings. This technique works even better for larger fine-tuned DeBERTa
    models.
  prefs: []
  type: TYPE_NORMAL
- en: DeBERTa variants
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: DeBERTa’s paper presents three models. The comparison between them is shown
    in the diagram below.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9a59d7f6a0a41036ca13841be378a2b4.png)'
  prefs: []
  type: TYPE_IMG
- en: DeBERTa variants
  prefs: []
  type: TYPE_NORMAL
- en: Data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For pre-training, the base and large versions of DeBERTa use a combination
    of the following datasets:'
  prefs: []
  type: TYPE_NORMAL
- en: English Wikipedia + BookCorpus (16 GB)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'OpenWebText (public Reddit content: 38 GB)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Stories (31 GB)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: After data deduplication, the resulting dataset size is reduced to 78 GB. For
    DeBERTa 1.5B, the authors used more twice more data (160 GB) with an impressive
    vocabulary size of 128K.
  prefs: []
  type: TYPE_NORMAL
- en: In comparison, other large models like RoBERTa, XLNet and ELECTRA are pre-trained
    on 160 GB of data. At the same time, DeBERTa shows a comparable or better performance
    than these models on a variety of NLP tasks.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Spearking of training, DeBERTa is pre-trained for one million steps with 2K
    samples in each step.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have walked through the main aspects of DeBERTa architecture. By possessing
    disentangled attention and enhanced masked encoding algorithms inside, DeBERTa
    has become an extremely popular choice in NLP pipelines for many data scientists
    and also a winning ingredient in many Kaggle competitions. Another amazing fact
    about DeBERTa is that it is one of the first NLP models which outperforms humans
    on the SuperGLUE benchmark. This single piece of evidence is enough to conclude
    that DeBERTa will remain for a long time in the history of LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: Resources
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[DeBERTa: Decoding-Enhanced BERT with Disentangled Attention](https://arxiv.org/pdf/2006.03654.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*All images unless otherwise noted are by the author*'
  prefs: []
  type: TYPE_NORMAL
