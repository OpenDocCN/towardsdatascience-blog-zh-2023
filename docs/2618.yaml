- en: Building a Conversational Agent with Memory Microservice with OpenAI and FastAPI
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/building-a-conversational-agent-with-memory-microservice-with-openai-and-fastapi-5d0102bc8df9?source=collection_archive---------1-----------------------#2023-08-17](https://towardsdatascience.com/building-a-conversational-agent-with-memory-microservice-with-openai-and-fastapi-5d0102bc8df9?source=collection_archive---------1-----------------------#2023-08-17)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/2cd55f0d3ad54800a9951729f2d331c1.png)'
  prefs: []
  type: TYPE_IMG
- en: A conversation full of memories, Photo by [Juri Gianfrancesco](https://unsplash.com/@jurigianfra?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral).
  prefs: []
  type: TYPE_NORMAL
- en: 'Crafting Context-Aware Conversational Agents: A Deep Dive into OpenAI and FastAPI
    Integration'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@cfloressuazo?source=post_page-----5d0102bc8df9--------------------------------)[![Cesar
    Flores](../Images/4345d9053171db08f1afbedcb32a1006.png)](https://medium.com/@cfloressuazo?source=post_page-----5d0102bc8df9--------------------------------)[](https://towardsdatascience.com/?source=post_page-----5d0102bc8df9--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----5d0102bc8df9--------------------------------)
    [Cesar Flores](https://medium.com/@cfloressuazo?source=post_page-----5d0102bc8df9--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F37afeaaf9b9a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-a-conversational-agent-with-memory-microservice-with-openai-and-fastapi-5d0102bc8df9&user=Cesar+Flores&userId=37afeaaf9b9a&source=post_page-37afeaaf9b9a----5d0102bc8df9---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----5d0102bc8df9--------------------------------)
    ·30 min read·Aug 17, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F5d0102bc8df9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-a-conversational-agent-with-memory-microservice-with-openai-and-fastapi-5d0102bc8df9&user=Cesar+Flores&userId=37afeaaf9b9a&source=-----5d0102bc8df9---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F5d0102bc8df9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-a-conversational-agent-with-memory-microservice-with-openai-and-fastapi-5d0102bc8df9&source=-----5d0102bc8df9---------------------bookmark_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this tutorial, we will explore the process of creating a Conversational Agent
    with a memory microservice using OpenAI and FastAPI. Conversational Agents have
    become a crucial component in various applications, including customer support,
    virtual assistants, and information retrieval systems. However, many traditional
    chatbot implementations lack the ability to retain context during a conversation,
    resulting in limited capabilities and frustrating user experiences. This is challenging,
    especially when building agent services following a microservice architecture.
  prefs: []
  type: TYPE_NORMAL
- en: The link to the GitHub repository is at the bottom of the article.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Motivation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The motivation behind this tutorial is to address the limitation of traditional
    chatbot implementations and create a Conversational Agent with a memory microservice,
    which becomes especially crucial when deploying agents within complex environments
    like Kubernetes. In Kubernetes or similar container orchestration systems, microservices
    are subject to frequent restarts, updates, and scaling operations. During these
    events, the state of the conversation in traditional buffers for chatbots would
    be lost, leading to disjointed interactions and poor user experiences.
  prefs: []
  type: TYPE_NORMAL
- en: By building a Conversational Agent with a memory microservice, we can ensure
    that crucial conversation context is preserved even in the face of microservice
    restarts or updates or when interactions are not continuous. This preservation
    of state allows the agent to seamlessly pick up conversations where they left
    off, maintaining continuity and providing a more natural and personalized user
    experience. Furthermore, this approach aligns with the best practices of modern
    application development, where containerized microservices often interact with
    other components, making the memory microservice a valuable addition to the conversational
    agent’s architecture in such distributed setups.
  prefs: []
  type: TYPE_NORMAL
- en: The Stack We Will Be Using
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For this project, we will primarily work with the following technologies and
    tools:'
  prefs: []
  type: TYPE_NORMAL
- en: 'OpenAI GPT-3.5: We will leverage OpenAI’s GPT-3.5 language model, which is
    capable of performing various natural language processing tasks, including text
    generation, conversation management, and context retention. We will need to generate
    an OpenAI API Key, make sure you visit this [URL](https://platform.openai.com/account/api-keys)
    to manage your keys.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'FastAPI: FastAPI will serve as the backbone of our microservice, providing
    the infrastructure for handling HTTP requests, managing conversation states, and
    integrating with the OpenAI API. FastAPI is great for building microservices with
    Python.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The Development Cycle
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we will dive into the step-by-step process of building our
    Conversational Agent with a memory microservice. The development cycle will include:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Environment Setup: We’ll create a virtual environment and install the necessary
    dependencies, including OpenAI’s Python library and FastAPI.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Designing the Memory Microservice: We’ll outline the architecture and design
    of the memory microservice, which will be responsible for storing and managing
    conversation context.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Integrating OpenAI: We’ll integrate OpenAI’s GPT-3.5 model into our application
    and define the logic for processing user messages and generating responses.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Testing: We’ll gradually test our conversational agent.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Environment Setup
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For this setup, we will use the following structure to build the microservice.
    This is convenient for more expansions of other services under the same project,
    and I personally like this structure.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: We will need to craft in the project a folder named `src` which will contain
    the Python code for the services; in our case `agents` contains all the code associated
    with our conversational agents and the API, and `agentsfwrk` is our common framework
    for usage across services.
  prefs: []
  type: TYPE_NORMAL
- en: The `Dockerfile` contains the instructions to build the image, once the code
    is ready, the `requirements.txt` contains the libraries to use in our project
    and the `setup.py` contains the instructions to build and distribute our project.
  prefs: []
  type: TYPE_NORMAL
- en: For now, just create the services folders along with the `__init__.py` files
    and add the following to the `requirements.txt` and `setup.py` to the root of
    the project, leave the `Dockerfile` empty, as we will come back to it in the Deployment
    Cycle section.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Let’s activate the virtual environment, and we will run `pip install -r requirements.txt`
    in the terminal. We will not run the setup file yet, so let’s get into the next
    section.
  prefs: []
  type: TYPE_NORMAL
- en: Designing the Common Framework
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will design our common framework, so we can use it across all the microservices
    built-in the project. This is not strictly necessary for small projects, but thinking
    about the future, you can expand it to use multiple LLM providers, add other libraries
    to interact with your own data (i.e. [LangChain](https://python.langchain.com/),
    [VoCode](https://docs.vocode.dev/what-is-vocode)), and other common capabilities
    such as voice and image services, without the need of implementing them in each
    microservice.
  prefs: []
  type: TYPE_NORMAL
- en: 'Create the folder and the files following the `agentsfwrk` structure. Each
    file and its description are below:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The logger is a very basic utility to set up a common logging module, and you
    can define it as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Next, our integration layer is done via the integration module. This file acts
    as a middleman between the microservices logic and OpenAI, and it’s designed to
    expose LLM providers in a common manner for our application. Here, we can implement
    common ways to handle exceptions, errors, retries, and timeouts in requests or
    in responses. I learned from a very good manager to always place an integration
    layer between external services/APIs and the inside world of our application.
  prefs: []
  type: TYPE_NORMAL
- en: 'The integration code is defined below:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Some notes about the integration module:'
  prefs: []
  type: TYPE_NORMAL
- en: The OpenAI Key is defined as an environment variable named “OPENAI_API_KEY”,
    we should download this key and define it in our terminal or using the [python-dotenv](https://pypi.org/project/python-dotenv/)
    library.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are two methods to integrate with GPT models, one for the chat endpoint
    (`answer_to_prompt`) and one for the completion endpoint (`answer_to_simple_prompt`).
    We will focus on the usage of the first one.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There is a method to check the goal of a conversation — `verify_goal_conversation`,
    which simply follows the instructions of agents and creates a summary of it.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Designing the (Memory) Microservice
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The best exercise is to design and consequentially draw a diagram to visualize
    what the service needs to do, including the actors and their actions when interacting with it.
    Let’s start by describing our application in simple terms:'
  prefs: []
  type: TYPE_NORMAL
- en: Our microservice is a provider of artificially intelligent agents, which are
    experts on a subject and are expected to have conversations in response to an
    outbound message and following prompts.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Our agents can hold multiple conversations and are packed with memory that is
    to be persisted, which means they must be able to retain the conversation history
    regardless of the session of the client who is interacting with the agents.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The agents should receive, at creation, clear instructions on how to handle
    a conversation and respond accordingly during the course of it.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For programmatic integration, the agents should also follow an expected response
    shape.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Our design looks like the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/066900bb9a87cfd3c504036ad45705be.png)'
  prefs: []
  type: TYPE_IMG
- en: Conversational Agents Design — Image by author
  prefs: []
  type: TYPE_NORMAL
- en: 'With this simple diagram, we know that our microservice needs to implement
    methods that are responsible for these specific tasks:'
  prefs: []
  type: TYPE_NORMAL
- en: Creation of agents & definition of instructions
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Conversation starter & preservation of conversation history
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Chat with agents
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We will code these functionalities in their order, and before we dive into that
    we will build the skeleton of our application
  prefs: []
  type: TYPE_NORMAL
- en: Application Skeleton
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To kickstart the development, we begin by building the FastAPI app skeleton.
    The app skeleton consists of essential components, including the main application
    script, database configuration, processing script, and routing modules. The main
    script serves as the entry point for the application, where we set up the FastAPI
    instance.
  prefs: []
  type: TYPE_NORMAL
- en: '**Main File**'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s create/open the `main.py` file in your `agents` folder and type the following
    code, which simply defines a root endpoint.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '**Database Configuration**'
  prefs: []
  type: TYPE_NORMAL
- en: We then create/open the database configuration script called `database.py`,
    which establishes the connection to our local database for storing and retrieving
    conversation context. We will start by using a local SQLite for simplicity, but
    feel free to try other databases for your environment.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '**API Routes**'
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we define routing modules that handle incoming HTTP requests, encompassing
    endpoints responsible for processing user interactions. Let’s create the `api`folder
    and create/open the `routes.py` file and paste the following code.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: With this structured skeleton, we are ready to start coding the application
    we designed.
  prefs: []
  type: TYPE_NORMAL
- en: Create Agents and Assign Instructions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we will focus on implementing the “**Create Agent”** endpoint.
    This endpoint enables users to initiate new conversations and interact with agents,
    providing a context and a set of instructions for the agent to follow throughout
    the rest of the conversation. We will start by introducing two Data Models for
    this process: One for the Database and another one for the API. We will be using
    [Pydantic](https://docs.pydantic.dev/latest/) for our data models. Create/Open
    the `schemas.py` file in the `api` folder, and define the Agent base, Agent Create,
    and Agent data model.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The fields in the agent’s data model are detailed below:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Context**: This is an overall context of what the agent is.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**First message**: Our agents are intended to start a conversation with the
    users. This can be as simple as “Hello, how can I help you?” or something like
    “Hi, you requested an agent to help you find information about stocks, is that
    correct?”.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Response shape**: This field is mainly used for specifying the output format
    of our agent’s response and should be used for transforming the text output of
    our LLM to a desired shape for programmatic communication. For example, we may
    want to specify that our agent should wrap the response in a JSON format with
    a key named `response`, i.e. `{''response'': "string"}`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Instructions:** This field holds the instructions and guidelines each agent
    should follow during the whole conversation, such as “Gather the following entities
    *[e1, e2, e3, …]* during each interaction” or “Reply to the user until he is no
    longer interested in the conversation” or “Don’t deviate from the main topic and
    drive the conversation back to the main goal when needed”.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We now proceed to open the `models.py` file, where we will code our database
    table that belongs to the agent's entity.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: This code is pretty similar to the Pydantic model, it defines the table of the
    agent in our database.
  prefs: []
  type: TYPE_NORMAL
- en: 'With our two data models in place, we are ready to implement the creation of
    the Agent. For this, we will start by modifying the `routes.py` file and adding
    the endpoint:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: We need to create a new function that receives an Agent object from the request
    and creates it into the database. For this, we will create/open the `crud.py`
    file which will hold all the interactions to the database **(CREATE, READ, UPDATE,
    DELETE).**
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: With our function created, we can now go back to the`routes.py`, import the
    `crud` module, and use it in the endpoint’s method.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Now let’s go back to the `main.py` file and add the “agents” router. The modifications
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Let’s test this functionality. First, we will need to install our services as
    a Python package, secondly, start the application on port 8000.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Navigate to [http://0.0.0.0:8000/docs](http://0.0.0.0:8000/docs), where you
    will see the Swagger UI with the endpoint to test. Submit your payload and check
    the output.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/fccecb2f4b76bfdbc7bd9b27f97f9044.png)'
  prefs: []
  type: TYPE_IMG
- en: create-agent endpoint from Swagger UI — Image by author
  prefs: []
  type: TYPE_NORMAL
- en: We will continue developing our application, but testing the first endpoint
    is a good sign of progress.
  prefs: []
  type: TYPE_NORMAL
- en: Create Conversations & Preserve Conversation History
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Our next step is to allow users to interact with our agents. We want users
    to interact with specific agents, so we will need to pass the ID of the agent
    along with the first interaction message from the user. Let’s make some modifications
    to the Agent data model so each agent can have multiple conversations by introducing
    the `Conversation` entity. Open the `schemas.py` file and add the following models:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Note that we have modified the `Agent` data model and added conversations to
    it, this is so each agent can hold multiple conversations as designed in our diagram.
  prefs: []
  type: TYPE_NORMAL
- en: 'We have to modify our database object and include the conversation table in
    the database model script. We will open the `models.py` file and modify the code
    as follow:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Note how we added the relationship between the conversations per each agent
    in the `agents` table, and also the relationship between a conversation with an
    agent in the `conversations` table.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will now create a set of CRUD functions to retrieve the agent and conversations
    by their IDs, which will help us to craft our process of creating a conversation
    and preserving its history. Let’s open the `crud.py` file and add the following
    functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: These new functions will help us during the normal workflow of our application,
    we can now get an agent by its ID, get a conversation by its ID, and create a
    conversation by providing an ID as optional, and the agent ID that should hold
    the conversation.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can go ahead and create an endpoint that creates a conversation. Open the
    `routes.py` and add the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: With this method ready we are still one step away from having an actual conversational
    endpoint, which we will review next.
  prefs: []
  type: TYPE_NORMAL
- en: It is important to make a distinction here when we initialize an agent, we can
    create a conversation without triggering a two-way exchange of messages or another
    way is to trigger the creation of a conversation when the “**Chat with an agent**”
    endpoint is called. This provides some flexibility in orchestrating the workflows
    outside the microservice, in some cases you may want to initialize the agents,
    pre kick-off conversations to clients and as messages start to come in, you start
    preserving the history of the messages.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3d0777673ee2498ca60aac063e7d35d5.png)'
  prefs: []
  type: TYPE_IMG
- en: create-conversation endpoint from Swagger UI — Image by author
  prefs: []
  type: TYPE_NORMAL
- en: '**Important Note:** if you are following step by step this guide and see an
    error related to the database schema in this step, it is because we are not applying
    migrations to the database with each modification of the schemas, so make sure
    you close the application (exit the terminal command) and delete the `agents.db`
    file that is created at runtime. You will need to run each endpoint again and
    take notes of the IDs.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Chat with an agent
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We are going to introduce the last entity type in our application which is
    the `Message` entity. This one is responsible for modeling the interaction between
    a client’s message and an agent’s message (two-way exchange of messages). We will
    also add API data models that are used to define the structure of the response
    of our endpoints. Let’s go ahead and create the data models and API response types
    first; open the `schemas.py` file, and modify the code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'We now have to add the data model in our database models script which represents
    the table in our database. Open the `models.py` file and modify as below:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Note that we have modified our `Conversations` table to define the relationship
    between messages and conversation and we created a new table that represents the
    interactions (exchange of messages) that should belong to a conversation.
  prefs: []
  type: TYPE_NORMAL
- en: 'We are now going to add a new CRUD function to interact with the database and
    create a message for a conversation. Let’s open the `crud.py` file and add the
    following function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Now we are ready to build the final and most interesting endpoint, the **chat-agent**
    endpoint. Let’s open the `routes.py` file and follow the code along as we will
    be implementing some processing functions on the way.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: '{'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '"conversation_id": "string",'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '"response": "string"'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '}'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In this section of the endpoint, we are making sure to create or raise an exception
    if the conversation does not exist. The next step is to prepare the data that
    will be sent to OpenAI via our integration, for this we will create a set of processing
    functions in the `processing.py` file that will craft the context, first message,
    instructions, and expected response shape from the LLM.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: Note the last function that expects the `response_shape` defined during the
    creation of the agent, this input will be appended to the LLM during the course
    of a conversation and will guide the agent to follow the guidelines and return
    the response as a JSON object.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s go back to the `routes.py` file and finish our endpoint implementation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: '{'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '"conversation_id": "string",'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '"response": "string"'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '}'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Voilà! This is our final endpoint implementation, if we look at the **Notes**
    added to the code, we see that the process is quite straightforward:'
  prefs: []
  type: TYPE_NORMAL
- en: We make sure the conversation exists in our database (or we create one)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We craft the context and instructions to the agent from our database
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We make use of the “memory” of the agent by pulling the conversation history
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, we request the agent response through OpenAI’s GPT-3.5 Turbo model
    and return the response to the client.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Local Testing Our Agents
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now we are ready to test the complete workflow of our microservice, we will
    start by going to our terminal and typing `uvicorn agents.main:app — host 0.0.0.0
    — port 8000 — reload` to launch the app. Next, we will navigate to our Swagger
    UI by going to [http://0.0.0.0:8000/docs](http://0.0.0.0:8000/docs) and we will
    submit the following requests:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create the agent: Give a payload that you’d like to test. I will submit the
    following:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Create the conversation: assign the conversation to the `agent_id` one that
    you have gotten from the previous response.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s chat with our agent through the outbound message: Our agent will initialize
    the conversation by asking us a question or approaching us with a message. We
    will follow the conversation by submitting a message back — I’ll use this one:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/ab0c299002e37eaffcb9d1cc4e592c31.png)'
  prefs: []
  type: TYPE_IMG
- en: chat-agent endpoint response from Swagger UI — Image by author
  prefs: []
  type: TYPE_NORMAL
- en: Our agent replied to us with a response and we can continue this conversation
    by replying in a natural way.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: Go ahead and play around with the code and your new agents. In the next section,
    I will focus on the deployment of this service.
  prefs: []
  type: TYPE_NORMAL
- en: The Deployment Cycle
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will deploy our application under a container environment in the cloud such
    as Kubernetes, Azure Container Service, or AWS Elastic Container Service. Here
    is where we create a docker image and upload our code so we can run it in one
    of these environments, go ahead and open the `Dockerfile` one we created at the
    start and paste the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: The Dockerfile installs the app and then it runs it via the CMD which is commented
    out. You should uncomment the command if you want to run it locally as a standalone,
    but for other services such as Kubernetes, this is defined when defining the deployment
    or pods in the command section of the manifest.
  prefs: []
  type: TYPE_NORMAL
- en: 'Build the image, wait until the build is completed, and then test it by running
    the run command, which is below:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: Great you are ready to start using the application in your deployment environment.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we will try to integrate this microservice with a front-end application
    that will serve the agents and the conversations by calling the endpoints internally,
    which is the common way of building and interacting between services using this
    architecture.
  prefs: []
  type: TYPE_NORMAL
- en: The Usage Cycle
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We can use this new service in multiple ways, and I will only focus on building
    a front-end application that calls the endpoints from our agents and makes it
    possible for users to interact via a UI. We will use [Streamlit](https://streamlit.io/)
    for this, as it is a simple way to spin up a front-end using Python.
  prefs: []
  type: TYPE_NORMAL
- en: '**Important Note:** There are additional utilities that I added to our agents’
    service that you can copy directly from the repository. Search for `get_agents()`
    ,`get_conversations()`, `get_messages()` from the `crud.py` module and the `api/routes.py`
    routes.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**Install Streamlit** and add it to our requirements.txt.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: '**Create the application** by creating first a folder in our `src` folder with
    the name `frontend`. Create a new file named `main.py` and place the following
    code.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: The code below connects to our agent's microservice via API calls and allows
    the user to select the Agent and the Conversations and chat with the agent, similar
    to what ChatGPT provides. Let’s run this app by opening another terminal (make
    sure you have the agents microservice up and running on port 8000) and type `$
    streamlit run src/frontend/main.py` and you are ready to go!
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0419fd4efca131f9a2bcf34f395b163d.png)'
  prefs: []
  type: TYPE_IMG
- en: AI Chat Streamlit App — Image by author
  prefs: []
  type: TYPE_NORMAL
- en: Future Improvements and Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Future Improvements
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There are several exciting opportunities for enhancing our Conversational Agent
    with a memory microservice. These improvements introduce advanced capabilities
    that can extend user interactions and expand the scope of our applications or
    overall system.
  prefs: []
  type: TYPE_NORMAL
- en: '**Enhanced Error Handling:** To ensure robust and reliable conversations, we
    could implement code to gracefully handle unexpected user inputs, API failures
    — dealing with OpenAI or other services, and potential issues that could arise
    during real-time interactions.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Integrated Buffers and Conversation Summaries:** The integration of buffers
    implemented by the LangChain framework, offers the potential to optimize token
    management, enabling conversations to span more extended periods without running
    into token limitations. Additionally, incorporating conversation summaries allows
    users to review the ongoing discussion, aiding in context retention and improving
    the overall user experience. *Take note of the agent instructions and the response
    shape to extend this easily in our code*.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Data-aware Applications:** We could create agents with unique and internal
    knowledge by connecting our agents’ models to other sources of data such as internal
    databases. This involves training or integrating models that can understand and
    respond to complex queries based on an understanding of your organization’s unique
    data and information — Check [LangChain’s data connection](https://python.langchain.com/docs/modules/data_connection/)
    modules.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Model Diversification:** While we’ve only used OpenAI’s GPT-3.5 model, the
    landscape of language model providers is expanding rapidly. Testing models from
    other providers can lead to comparative analysis, uncovering strengths and weaknesses,
    and enabling us to choose the best fit for specific use cases — Try playing with
    different LLM integrations such as [HuggingFace](https://huggingface.co/models?other=LLM),
    [Cohere](https://cohere.com/), [Google’s](https://developers.generativeai.google/products/palm),
    etc.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We have developed a microservice that provides intelligent agents powered by
    OpenAI GPT models and have proven how these agents can be packed with memory that
    lives outside of the client’s session. By adopting this architecture, we have
    unlocked a world of possibilities. From context-aware conversations to seamlessly
    integrating with sophisticated language models, our stack has become capable of
    providing new features to our products.
  prefs: []
  type: TYPE_NORMAL
- en: This implementation and the tangible benefits of it, make it clear that using
    AI is at the hands of anyone with the right tools and approach. The use of **AI-powered
    agents is not only about prompt engineering** but how we build tools and engage
    with them more effectively, offering personalized experiences, and tackling complex
    tasks with the finesse and precision that AI and software engineering can provide.
    So, whether you’re building a customer support system, a sales virtual assistant,
    a personal chef, or something entirely new, remember that the journey starts with
    a touch of code and an abundance of imagination — The possibilities are limitless.
  prefs: []
  type: TYPE_NORMAL
- en: '*The whole code for this article is in* [*GitHub*](https://github.com/cfloressuazo/conversational-ai)
    *— You can find me on* [*LinkedIn*](https://www.linkedin.com/in/cfloressuazo/)*,
    feel free to connect!*'
  prefs: []
  type: TYPE_NORMAL
