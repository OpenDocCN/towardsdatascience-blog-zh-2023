# 当机器学习出现错误时，这意味着什么？

> 原文：[https://towardsdatascience.com/what-does-it-mean-when-machine-learning-makes-a-mistake-37b213200697?source=collection_archive---------2-----------------------#2023-09-17](https://towardsdatascience.com/what-does-it-mean-when-machine-learning-makes-a-mistake-37b213200697?source=collection_archive---------2-----------------------#2023-09-17)

## 我们对“错误”的定义在机器学习/人工智能领域是否合理？如果不合理，为什么？

[](https://medium.com/@s.kirmer?source=post_page-----37b213200697--------------------------------)[![Stephanie Kirmer](../Images/f9d9ef9167febde974c223dd4d8d6293.png)](https://medium.com/@s.kirmer?source=post_page-----37b213200697--------------------------------)[](https://towardsdatascience.com/?source=post_page-----37b213200697--------------------------------)[![Towards Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----37b213200697--------------------------------) [Stephanie Kirmer](https://medium.com/@s.kirmer?source=post_page-----37b213200697--------------------------------)

·

[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fa8dc77209ef3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-does-it-mean-when-machine-learning-makes-a-mistake-37b213200697&user=Stephanie+Kirmer&userId=a8dc77209ef3&source=post_page-a8dc77209ef3----37b213200697---------------------post_header-----------) 发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----37b213200697--------------------------------) · 11分钟阅读 · 2023年9月17日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F37b213200697&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-does-it-mean-when-machine-learning-makes-a-mistake-37b213200697&user=Stephanie+Kirmer&userId=a8dc77209ef3&source=-----37b213200697---------------------clap_footer-----------)

--

[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F37b213200697&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-does-it-mean-when-machine-learning-makes-a-mistake-37b213200697&source=-----37b213200697---------------------bookmark_footer-----------)![](../Images/f53d085f3208ccdbd80d9f536cade3d6.png)

图片来源：[Kind and Curious](https://unsplash.com/@kindandcurious?utm_source=medium&utm_medium=referral) 在 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)

我最近关于机器学习公众认知的帖子中有一个评论让我思考机器学习中“错误”的含义。读者问我是否认为机器学习模型会一直“犯错误”。正如我在那篇帖子中所描述的，人们有很强的倾向将机器学习模型拟人化。当我们与LLM聊天机器人互动时，我们会运用从与其他人交流中学到的技巧——说服、措辞、论证等。然而，这通常效果不好，最终会导致不满意的回应。

在我自己日常的工作中，我也会看到与分类器和回归模型相关的类似问题。我和我的团队花费了大量的时间和精力来帮助客户和同事理解机器学习并不完美（而且现实中也永远不会完美）。“为什么模型说X，而事实是X-5？”是一个永恒的主题。我不完全责怪提问者，因为正如我在上一篇文章中所写，我们广泛的机器学习社区在教授基础机器学习素养方面做得不够好。

但这提出了一个核心问题，我们需要更多的探讨才能真正解决这个素养问题。

> 当我们说一个模型犯了错误、失败了、产生了幻觉或说谎时，这是什么意思（以及其他人是什么意思）？

在我们能回答这个问题之前，我们需要从头开始。

# 什么是机器学习模型？

从一个非常、非常通用的角度来看，机器学习模型是一个接受一些输入或提示并返回某种概率确定的响应的算法。它决定响应应该是什么的方式可能会有很大差异——它可能使用决策树、神经网络、线性回归或其他任何类型的机器学习方法。

要创建一个模型，我们从反映我们寻找结果的样本数据开始。输入样本可以是各种各样的东西——对于生成式AI，它们可能是大量的人类书写的文本、音乐或图像。对于其他类型的机器学习，它们可能是包含对象特征的大型数据集，或者将图像或文本等分类到类别中的数据集，或更多。

有时候，这些回应会被“标记”以便模型学习哪些是可取的，哪些不是，或者哪些属于特定类别，哪些不属于。其他时候，模型会学习底层样本中的模式，并形成自己对这些模式的理解，以复制输入的特征、在选项之间做出选择、将输入分组或进行其他活动。

# 生成式AI模型的训练方式

我们训练生成模型的方式是特定的，比训练一个简单的概率模型要复杂得多。相反，这些模型是估计许多不同元素的概率，并将它们组合在一起以生成响应。以下是我们进行这种训练的一些非常简单的解释。（这些都是极度的过度简化，请原谅细节的缺乏和任何概括。）

当我们生成声音或图像时，可能会使用生成对抗网络。在这种情况下，模型相互对抗，一个模型生成新内容，另一个模型尝试判断该内容是否来自某个模型，来回交替。这两个模型在成千上万的案例中竞争，每个模型在过程中逐渐变得更好。最终，生成模型将能够产生几乎无法与现实所产生的内容区分开来的内容。（作为副作用，区分模型也会变得非常擅长识别输入是否是人类生成的。）

对于大型语言模型（LLMs）和像 GPT 模型这样的文本生成，我们使用所谓的 Transformer。这种训练包括教模型理解单词的意义如何相互关联，以及如何生成接近于人类生产的文本内容。结果听起来非常有说服力，因为模型知道哪些单词可能会一起出现（基于训练中真实人类语言如何将它们组合在一起的概率）。

为了从文本输入生成图像，例如 Dall-E，我们使用扩散。在这种情况下，我们教模型计算图像中最可能需要的特征，基于所提供的文本。模型本质上从一个静态图像开始，根据你的文本应用细节/颜色/特征。这是基于模型关于文本*通常*如何与图像对应的学习结果。

使用这些技术，我们教会模型解读输入中的模式——有时是我们自己甚至无法真正解释或检测的模式（尤其对于深度学习来说），然后模型能够解释和应用这些模式。所有这些在表面下都是数学，即使模式可能存在于文本、图像或许多其他事物中。

现在我们知道了这些内容，我们可以讨论输出是什么，以及当输出不是我们想要的结果时意味着什么。

# 输出

机器学习模型生成的东西可以有很大的不同。特别是生成型人工智能生成的图像、视频、音频和各种文本无所不包。其他类型的模型则给出事件/现象的可能性、未知值的估计、文本翻译成不同语言、内容的标签或分组等等。

在所有这些情况下，复杂的数学计算被用来根据给定输入估算最佳响应。然而，“最佳”是一个非常具体的概念。在创建模型的过程中，你已经向模型指明了你希望其响应具备什么特性。

> 在创建模型的过程中，你已经向模型指明了你希望其响应具备什么特性。

# 当我们得到意料之外的结果时，这意味着什么？

**这与我们本身一样重要，也与模型一样重要。** 这基本上就像任何技术领域的产品一样。产品设计师和创作者在开发要出售给用户的产品时，会制定“用户故事”，这些故事包括谁将使用该产品、如何使用以及为什么使用，以及他们希望从中获得什么。

例如，假设我们正在设计一个电子表格工具。我们会使用用户故事来考虑安妮这位会计师，并与会计师交谈，以确定会计师在电子表格软件中需要什么样的功能。然后我们会考虑商业分析师鲍勃，并与商业智能分析师讨论他们的功能需求。我们会将所有这些列入电子表格工具的规划清单，并以此来指导我们的设计。你明白了吧。

**谁是机器学习模型的用户？** 这完全取决于模型的类型。例如，如果你的模型预测基于房产特征的房价，可能的用户是房地产经纪人、抵押贷款机构或购房者。具有明确、有限应用范围的模型容易为用户量身定制。我们数据科学家可以确保这个模型满足使用者的期望。

有时预测结果可能不准确，但这是一个数学问题，我们可能能够解码为什么会这样。也许我们给模型提供了错误的数据，或者这个房子有某种我们无法告诉模型的特殊原因。例如，如果模型从未被教会如何解释后院动物园对房价的影响，它将无法整合这些信息。如果出现了房价崩溃？我们不久前看到过这样的情况，你可以想象模型从崩溃前学到的模式将不再适用。

然而，在这种情况下，我们有两个问题：

1.  模型旨在实现的明确目标，数据科学家和用户都知道；

1.  一种可量化的方法来衡量模型是否接近其目标。

这意味着当我们想要定义模型是否成功时，这一过程是明确和简单的。在做出这个判断之后，我们可以探讨模型为何会有这样的表现——这就是在该领域中所说的“模型可解释性”或“模型解释性”。

# 但对于LLMs（大语言模型）呢？

这个整体框架对像LLM这样的东西意味着什么？谁是ChatGPT的用户？（你刚刚在脑海里回答“每个人”了吗？）当模型的输出可以像LLM那样复杂和多样时，我们开始产生疑问。

对于那些构建生成性AI模型的数据科学家来说，虽然他们可能采用不同的训练方法，但我们通常始终尝试创造尽可能接近训练数据的内容，这些数据通常是由人类或自然生成的。为了实现这一目标，模型是通过人类或自然产生的样本内容进行训练的。我们尽力给模型一个数学方式来理解这些内容为何感觉“真实”，以便它可以复制这一点。这就是生成性AI模型能够创造效率并使某些人类工作变得过时的方式。

> 对于那些构建生成性AI模型的数据科学家来说，目标是创造尽可能接近训练数据的内容，这些数据通常是由人类或自然生成的。

这些模型在这方面做得非常出色！然而，这也带来了一些陷阱。由于LLM模型在模仿人类回应方面非常逼真，用户在思维上容易把它们当作人来看待。这就像孩子学习动物的方式——你教孩子说一只有四条腿和湿鼻子的毛茸茸的动物是狗，但当你把一只猫放在他们面前时，他们会倾向于认为那也是狗，因为基本特征似乎非常相似。只有当你解释猫是另一种动物时，他们才开始理解差异并建立不同的心理模型。

> 由于这些模型在模仿人类回应方面非常逼真，用户在思维上容易把它们当作人来看待。

目前，我认为大多数公众仍在建立不同的心理模型，以区分LLM和人类。（正如我之前所写，数据科学家需要像大人一样解释狗和猫的不同，以继续这个比喻。）

不过我稍微跑题了。这真正意味着与一个非常基础的模型（例如房价）互动的人理解这是一个有限的算法。它更像一个电子表格公式，而不像一个人，这塑造了我们的期望。但当我们使用ChatGPT时，它带有许多人类在线聊天的特征，这影响了我们。我们开始期望陈述总是准确的，结果包括连贯的批判性思维，并且期望从模型中检索到今天新闻的事实，即使它是去年训练的。

> [P]与一个非常基础的模型互动的人理解这是一个有限的算法。……但当我们使用ChatGPT时，它带有许多人类在线聊天的特征，这影响了我们。

在模型结果中出现批判性思维的迹象，是因为模型学会了我们从真实人类来源中解释为“批判性思维”的文本排列听起来更“人类”，因此它模仿这些排列以达到这一目的。当我们与人交谈时，我们会从他们说的话中推断出他们正在进行批判性思维。然而，我们不能用机器学习来做到这一点。

记住我上面描述的房价模型的两个关键要素：

> 1\. 模型旨在实现的明确目标，数据科学家和用户都应当了解；
> 
> 2\. 一种量化衡量模型是否接近其目标的方式。

对于生成式 AI，包括但不限于大语言模型，我们在第1点上存在问题，部分原因是目标实际上并不清晰（“返回与人类产生的内容无法区分的材料”），但主要原因是数据科学家确实没有成功地向用户传达这个目标。数据科学家在这些模型上达到了第2点，因为他们使用复杂的数学系统来教模型何时生成足够“真实”或类似人类的内容。然而，对于普通用户来说，这要困难得多。判断模型是否做得好的过程更像是评分论文，而不是检查数学问题的结果。主观性悄然渗入。

即使测量更为简单，我仍然坚决认为，即使是一些技术娴熟且受过高等教育的用户，也未必真正清楚这些模型已经训练成什么样，因此也无法知道什么是现实的期望，什么不是。因此，对于模型而言完全合适的结果，比如一个流畅、雄辩、完美“人类风格”的段落描述月亮是由绿色奶酪制成的，也会被视为“错误”。然而这并不是错误——这个输出达到了它的训练目标——这也是我们许多困惑的根源。

# 调整期望

这表明我们需要调整对这些模型的期望，我希望这篇文章能有所帮助。要成功使用机器学习模型，并区分错误和预期行为，你需要了解模型被训练来执行的任务以及训练数据的性质。如果你能更深入一点，你还需要清楚数据科学家如何衡量成功，因为这会极大地影响模型的行为。

通过融合这些元素，你将拥有理解模型结果含义所需的背景信息，并能够准确解读它们——你的期望将会是合理的，你也会知道这些期望是否得到了满足。而且，当涉及到机器学习时，你会真正理解“错误”意味着什么。

目前有一些有用的材料澄清了许多关于流行生成式机器学习模型的内容（如它们是如何训练的，回应的真正含义等），我在下面添加了一些链接。（我并不 endorsing 这些材料中的所有观点，只是提供给那些想了解生成式 AI 的人作为参考。）

[](https://techcrunch.com/2023/09/04/are-language-models-doomed-to-always-hallucinate/?source=post_page-----37b213200697--------------------------------) [## AI 模型注定总是要产生幻觉吗？ | TechCrunch

### 像 ChatGPT 这样的语言模型有一个不好的习惯，就是捏造事实。但这能在技术层面上解决吗？

techcrunch.com](https://techcrunch.com/2023/09/04/are-language-models-doomed-to-always-hallucinate/?source=post_page-----37b213200697--------------------------------) [](https://www.cloudskillsboost.google/journeys/118?source=post_page-----37b213200697--------------------------------) [## Google Cloud Skills Boost

### Qwiklabs 提供真实的 Google Cloud 环境，帮助开发者和 IT 专业人员学习云平台以及其他相关内容……

www.cloudskillsboost.google](https://www.cloudskillsboost.google/journeys/118?source=post_page-----37b213200697--------------------------------)

Garon, Jon M., 《生成式 AI、合成媒体及最新媒体中的信息实用入门》 (2023年3月14日)。可在 SSRN 上获取：[https://ssrn.com/abstract=4388437](https://ssrn.com/abstract=4388437) 或 [http://dx.doi.org/10.2139/ssrn.4388437](https://dx.doi.org/10.2139/ssrn.4388437)

*查看更多我的工作请访问* [*www.stephaniekirmer.com*](http://www.stephaniekirmer.com/)*.*

*注意：我通常说“机器学习”而不是 AI，但在“生成式 AI”这种情况下，我选择使用这个短语，因为它在这个领域中得到了广泛的应用。*
