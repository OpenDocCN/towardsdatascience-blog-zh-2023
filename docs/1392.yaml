- en: TorchServe & Flask for Image Style Transfer
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/torchserve-flask-for-image-style-transfer-113f73bd1d70?source=collection_archive---------7-----------------------#2023-04-20](https://towardsdatascience.com/torchserve-flask-for-image-style-transfer-113f73bd1d70?source=collection_archive---------7-----------------------#2023-04-20)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: An example of web app backed by TorchServe model server
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@summit.mnr?source=post_page-----113f73bd1d70--------------------------------)[![Andrey
    Golovin](../Images/3afbee89a80374b346e57c8f317c9b3a.png)](https://medium.com/@summit.mnr?source=post_page-----113f73bd1d70--------------------------------)[](https://towardsdatascience.com/?source=post_page-----113f73bd1d70--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----113f73bd1d70--------------------------------)
    [Andrey Golovin](https://medium.com/@summit.mnr?source=post_page-----113f73bd1d70--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: Â·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fc18c39659707&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftorchserve-flask-for-image-style-transfer-113f73bd1d70&user=Andrey+Golovin&userId=c18c39659707&source=post_page-c18c39659707----113f73bd1d70---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----113f73bd1d70--------------------------------)
    Â·6 min readÂ·Apr 20, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F113f73bd1d70&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftorchserve-flask-for-image-style-transfer-113f73bd1d70&user=Andrey+Golovin&userId=c18c39659707&source=-----113f73bd1d70---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F113f73bd1d70&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftorchserve-flask-for-image-style-transfer-113f73bd1d70&source=-----113f73bd1d70---------------------bookmark_footer-----------)![](../Images/31d5f10b49cf379849d5f0002f5cd65d.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: Image by author*. Exposing an ML model as decoupled model server is way more
    scalable, extensible and maintainable pattern.*
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: In the [previous post](https://medium.com/p/1578eca5aa20) I showed an example
    of serving an image classification model with the TorchServe framework. Now letâ€™s
    extend the example and make it a bit closer to the real world scenarios.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: Letâ€™s say I want to develop a web app to let users apply the filters to their
    images. As you know thereâ€™s a lot of such applications. One of the feature could
    be neural style transfer â€” users can upload an image with content and an image
    with style (or select a filter in the app) and get the new image of the content
    in desired style. Letâ€™s build this example from end to end.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: The focus of the post will be, of course, not how to send a request from Flask
    app to another url ğŸ˜ Iâ€™ll try to make it a bit more useful. First of all, Iâ€™ll
    show an **example of a complex handler** with additional dependencies. Then the
    **model server will return an image** instead of simply the labels or probabilities.
    Finally the code could be helpful as a **working example of how to pass images
    in appropriate format between the browser, Flask app and model server.** And just
    for fun letâ€™s deploy the whole solution in Kubernetes.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: GitHub repo with code is [here](https://github.com/quasi-researcher/style_transfer).
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: '*(btw, if you want to see a very simple example of web app + TorchServe for
    image classification then checkout to the feature/classify branch in git)*'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: In this post I assume youâ€™re already familiar with basics of TorchServe (handler,
    model files etc.). If not refer to the previous post.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: Neural style transfer
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In case you donâ€™t remember how the style transfer works here is a short description.
    Itâ€™s important to get the high-level overview in order to understand what will
    be going on in the handler for TorchServe.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: â€œInferenceâ€ in style transfer is not just one pass of the input tensor through
    the net. Instead a tensor (which is going to be the output picture at the end)
    is passed many times and the tensor itself is modified so that to minimize the
    content and style loss functions. At each iteration the image is changed. And
    the â€œinferenceâ€ is a sequence of the iterations.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: 'For the solution it means the following:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: the inference function in handler will be pretty complex. Itâ€™d be messy to put
    everything in *handler.py*. So, Iâ€™ll put it in additional module and show how
    to include it into TorchServe artifacts
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'a side effect: â€œinferenceâ€ will take some time. To not ask a user for waiting
    a minute while Flask will reload the entire page Iâ€™ll use ajax request from browser
    to the app. So, the page in browser wonâ€™t be frozen'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Model file
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The VGG19 pretrained model is used in the solution. Generally speaking I just
    followed the style transfer official example from PyTorch: [https://pytorch.org/tutorials/advanced/neural_style_tutorial.html](https://pytorch.org/tutorials/advanced/neural_style_tutorial.html)'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: I had to slightly modify the state dict of the model to get rid of classifier
    layers (80M vs. 500M of full vgg19 model). You can check how the .pth file was
    produced in the notebook *model_saving.ipynb* in the repo. It generates the *vgg19.pth*
    artifact. The *model_nst.py* contains the definition of the model architecture.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: Handler
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Preprocess function**'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: It is almost the same as you saw in the first post. The difference is that there
    are two images as input and the function must return only one tensor. So, the
    two will be just stacked in one to be split back later.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: '**Postprocess function**'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: The function is pretty straightforward. The only thing here is how to pass the
    image to the Flask app so that it can be correctly read from json. Passing it
    as bytes buffer worked for me.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: '**Inference function**'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: The code for â€œinferenceâ€ is noticeable long. So, I didnâ€™t place it directly
    in the handler module. Instead, itâ€™s located in the *utils.py*. As I mentioned
    the code is from the official PyTorch example of style transfer, I wonâ€™t go into
    details.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: 'Letâ€™s see how to include additional modules into TorchServe artifacts. For
    model archiver you need to specify the extra files you want to include:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Now weâ€™re good to go from the TorchServe side. Let me briefly walk you through
    the web app.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: Flask app
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The app has only two endpoints â€” to check the status of the model server and
    to generate the image with desired style. When the generation endpoint is called
    the app just forwards the content and style images to the TorchServe model server.
    Then it decodes the received generated image and return it back as json object.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: 'As I mentioned to not waiting long time for reloading the whole page the generation
    request is sent as ajax. So, there is also a simple JQuery script for that:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: Run the application
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'If you donâ€™t want to run the whole solution in Kubernetes you can stop here
    and just start the model server from its directory as:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'And application server:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Now everything shall be functional. Go to localhost:5000 in browser, upload
    the content and style images, click â€œ*transfer style*â€ and in a while you will
    get the generated image with desired style in your browser.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: Run with Kubernetes in a single-node cluster
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For the Kubernetes run on your local machine create two Docker images with
    the names *flask_server* and *model_server* from the Dockerfiles in the repo.
    There is also yaml file for Kubernetes. So, just apply it:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Donâ€™t forget about port forwarding (e.g. bind 8700 port of your machine to
    the podâ€™s 5000 port):'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: And here we are. Go to your browser and open *localhost:8700*.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/fce46f319881ff3988c067a71dd4a120.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
- en: Image by author
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: Forgive me the design and UI, Iâ€™m just a DS/ML engineer. I know itâ€™s ugly. ğŸ˜…
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: Live demo
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For the screen recording below Iâ€™ll use my own pictures I have at my fingertips
    to avoid any author rights violation. Let me check if I can do a crazy thing:
    draw my cat with the symbols of International Specification for Orienteering Maps
    ([ISOM](https://orienteering.sport/iof/mapping/)). They are the symbols that are
    used to draw maps for sport orienteering competitions.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5d7d7ddf62446fd5808ff4b858d9e390.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
- en: Images by author
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/489f717964f935a6a13a73f9ce1d0fb6.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
- en: Image by author
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: Well, looks interesting ğŸ¤ª
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/072ad99dc33defcc757c92c26ed33b61.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
- en: Image by author
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  id: totrans-61
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç»“è®º
- en: This post together with the previous one shows how to serve your ML models with
    a dedicated serving framework and how to use an approach of model server detached
    from application server.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬æ–‡ä¸å‰ä¸€ç¯‡æ–‡ç« ä¸€èµ·å±•ç¤ºäº†å¦‚ä½•ä½¿ç”¨ä¸“ç”¨æœåŠ¡æ¡†æ¶æ¥æœåŠ¡ä½ çš„ ML æ¨¡å‹ï¼Œä»¥åŠå¦‚ä½•ä½¿ç”¨ä¸åº”ç”¨æœåŠ¡å™¨åˆ†ç¦»çš„æ¨¡å‹æœåŠ¡å™¨æ–¹æ³•ã€‚
- en: Itâ€™s shown that TorchServe allows flexible customisation of pre-, postprocessing
    and inference functions. Thus , you can incorporate any complex logic of your
    models.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: ç ”ç©¶è¡¨æ˜ï¼ŒTorchServe å…è®¸çµæ´»å®šåˆ¶å‰å¤„ç†ã€åå¤„ç†å’Œæ¨ç†åŠŸèƒ½ã€‚å› æ­¤ï¼Œä½ å¯ä»¥èå…¥æ¨¡å‹ä¸­çš„ä»»ä½•å¤æ‚é€»è¾‘ã€‚
- en: Also the GitHub repo with the end-to-end example can be used as a starting point
    for your own experiments.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: åŒæ ·ï¼Œå¸¦æœ‰ç«¯åˆ°ç«¯ç¤ºä¾‹çš„ GitHub ä»“åº“å¯ä»¥ä½œä¸ºä½ è‡ªå·±å®éªŒçš„èµ·ç‚¹ã€‚
- en: 'As a conclusion let me mention just few benefits that an approach with a model
    server offers:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: ä½œä¸ºæ€»ç»“ï¼Œè®©æˆ‘æåˆ°ä¸€äº›æ¨¡å‹æœåŠ¡å™¨æ–¹æ³•æ‰€æä¾›çš„å¥½å¤„ï¼š
- en: more efficient use of hardware (e.g. model server can be deployed on a machine
    with GPUs while the application server may not need it)
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ›´é«˜æ•ˆåœ°åˆ©ç”¨ç¡¬ä»¶ï¼ˆä¾‹å¦‚ï¼Œæ¨¡å‹æœåŠ¡å™¨å¯ä»¥éƒ¨ç½²åœ¨é…æœ‰GPUçš„æœºå™¨ä¸Šï¼Œè€Œåº”ç”¨æœåŠ¡å™¨å¯èƒ½ä¸éœ€è¦GPUï¼‰
- en: dedicated serving frameworks offer features to serve models at scale (e.g. threads
    and workers in TorchServe)
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¸“ç”¨æœåŠ¡æ¡†æ¶æä¾›äº†å¤§è§„æ¨¡æœåŠ¡æ¨¡å‹çš„åŠŸèƒ½ï¼ˆä¾‹å¦‚ï¼ŒTorchServeä¸­çš„çº¿ç¨‹å’Œå·¥ä½œè¿›ç¨‹ï¼‰
- en: 'serving frameworks also provide the features to speed up the development (and
    to not reinvent the wheel): model versioning, logs, metrics etc.'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æœåŠ¡æ¡†æ¶è¿˜æä¾›äº†åŠ é€Ÿå¼€å‘çš„åŠŸèƒ½ï¼ˆè€Œä¸”é¿å…é‡å¤å‘æ˜è½®å­ï¼‰ï¼šæ¨¡å‹ç‰ˆæœ¬æ§åˆ¶ã€æ—¥å¿—ã€æŒ‡æ ‡ç­‰ã€‚
- en: message queuing service can be easily added to scale the solution
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ¶ˆæ¯é˜Ÿåˆ—æœåŠ¡å¯ä»¥è½»æ¾æ·»åŠ ä»¥æ‰©å±•è§£å†³æ–¹æ¡ˆ
- en: dev and ML/DS teams can work more independently
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¼€å‘å’Œ ML/DS å›¢é˜Ÿå¯ä»¥æ›´ç‹¬ç«‹åœ°å·¥ä½œ
- en: Itâ€™s not the complete list but just few reasons to think about serving ML models
    with dedicated frameworks.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸æ˜¯å®Œæ•´çš„åˆ—è¡¨ï¼Œåªæ˜¯ä¸€äº›è€ƒè™‘ä½¿ç”¨ä¸“ç”¨æ¡†æ¶æ¥æœåŠ¡ ML æ¨¡å‹çš„ç†ç”±ã€‚
- en: Hope you could find some helpful and practical stuff in this post.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: å¸Œæœ›ä½ èƒ½åœ¨è¿™ç¯‡æ–‡ç« ä¸­æ‰¾åˆ°ä¸€äº›æœ‰ç”¨å’Œå®ç”¨çš„å†…å®¹ã€‚
