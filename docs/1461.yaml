- en: 'Beyond Accuracy: Exploring Exotic Metrics for Holistic Evaluation of Machine
    Learning Models'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/beyond-accuracy-exploring-exotic-metrics-for-holistic-evaluation-of-machine-learning-models-8a093875dcc9?source=collection_archive---------12-----------------------#2023-04-27](https://towardsdatascience.com/beyond-accuracy-exploring-exotic-metrics-for-holistic-evaluation-of-machine-learning-models-8a093875dcc9?source=collection_archive---------12-----------------------#2023-04-27)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[](https://adrienpavao.medium.com/?source=post_page-----8a093875dcc9--------------------------------)[![Adrien
    Pavao](../Images/de76baf2393108f8107685d352d4602b.png)](https://adrienpavao.medium.com/?source=post_page-----8a093875dcc9--------------------------------)[](https://towardsdatascience.com/?source=post_page-----8a093875dcc9--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----8a093875dcc9--------------------------------)
    [Adrien Pavao](https://adrienpavao.medium.com/?source=post_page-----8a093875dcc9--------------------------------)'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: ·
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F9a9236d5c7a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbeyond-accuracy-exploring-exotic-metrics-for-holistic-evaluation-of-machine-learning-models-8a093875dcc9&user=Adrien+Pavao&userId=9a9236d5c7a&source=post_page-9a9236d5c7a----8a093875dcc9---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----8a093875dcc9--------------------------------)
    ·12 min read·Apr 27, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F8a093875dcc9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbeyond-accuracy-exploring-exotic-metrics-for-holistic-evaluation-of-machine-learning-models-8a093875dcc9&user=Adrien+Pavao&userId=9a9236d5c7a&source=-----8a093875dcc9---------------------clap_footer-----------)'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F8a093875dcc9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbeyond-accuracy-exploring-exotic-metrics-for-holistic-evaluation-of-machine-learning-models-8a093875dcc9&source=-----8a093875dcc9---------------------bookmark_footer-----------)'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: Machine learning has undoubtedly become a powerful tool in today’s data-driven
    world, but are we truly tapping into its full potential? Traditional evaluation
    metrics like accuracy, precision, and recall have long held the spotlight, but
    there’s so much more to consider when measuring a model’s real-world impact. In
    this article, we’ll dive into the lesser-known, unconventional metrics that are
    reshaping the way we assess machine learning models. From fairness, privacy and
    calibration, to enery consumption, data consumption or even psychological and
    behavioral tests, these innovative evaluation techniques will change how you think
    about model performance and pave the way for a more responsible, holistic approach
    to machine learning.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a39fac5298babc18184673acba891cce.png)'
  id: totrans-8
  prefs: []
  type: TYPE_IMG
- en: 'Image source: pexels.com'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: Fairness
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Even if the mathematical definition of machine learning models does not necessarily
    contains unfair or biased elements, trained models can be unfair, depending on
    the quality of their input data or their training procedure. A model learned on
    biased data may not only lead to unfair and inaccurate predictions, but also significantly
    disadvantage certain subgroups, and lead to unfairness.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: 'In other words, the notion of fairness of models describe the fact that models
    can behave differently on some subgroups of the data. The issue is especially
    significant when it pertains to demographic groups, commonly defined by factors
    such as gender, age, ethnicity, or religious beliefs. As machine learning is increasingly
    applied in the society, this problem is getting more attention and research [1,
    2, 3, 4, 5]. Quantifying fairness in machine learning is subject to debate. Some
    interesting ways to measure fairness include:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: '**Demographic Parity**: This measure checks if the predicted classes are equally
    distributed across different demographic groups. The formula is as follows:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ec8c6b87349bbc492cbdf684d8d51e91.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
- en: where *A* is a protected attribute (such as race or gender), *Y* is the target
    variable (such as approval or denial) and *Ŷ* is the predicted value of *Y*. Demographic
    parity is a condition to be achieved, that the predictions are statistically independent
    of the protected attributes.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: '**Statistical Parity Difference**: This measures if the positive classification
    rate is equal across different demographic groups. The formula is:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/20328934bb800c73d4000338fd219747.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
- en: '**Disparate impact**: It calculates the ratio of the positive classification
    rate for a protected group to the positive classification rate for another group.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7dfcc79c659ea00a446c7fb1d29e80a1.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
- en: A value of 1 indicates that the positive classification rate is the same for
    both groups, suggesting fairness. A value greater than 1 indicates a higher positive
    classification rate for the group with *A=0*, while a value less than 1 suggests
    a higher positive classification rate for the group with *A=1*.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: However, it is important to note that disparate impact is a limited measure
    of fairness and should not be used in isolation. There may be cases where a higher
    positive classification rate for one group is justifiable, for example if the
    group is underrepresented in the training data. Additionally, disparate impact
    does not consider other factors such as false positive and false negative rates,
    which may provide a more comprehensive view of fairness.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，需要注意的是，差异影响是一个有限的公平性衡量标准，不应孤立使用。例如，如果某一组在训练数据中代表性不足，则可能有理由提高该组的正分类率。此外，差异影响没有考虑其他因素，如假阳性和假阴性率，这些可能提供更全面的公平性视角。
- en: These are just a few of the metrics that can be used to quantify fairness in
    machine learning. It is important to note that fairness is a complex issue, and
    these metrics should not be used in isolation. Instead, they should be considered
    in the context of the specific problem and the desired outcome.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 这些只是用于量化机器学习中公平性的一些指标。需要注意的是，公平性是一个复杂的问题，这些指标不应孤立使用。相反，它们应该在特定问题和期望结果的背景下进行考虑。
- en: Calibration
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 校准
- en: As defined in [6, 7], the notion of miscalibration represents the difference
    in expectation between the confidence level (or probability) returned by the algorithm,
    and the actual performance obtained.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 如[6, 7]中定义的那样，误校准的概念表示算法返回的置信水平（或概率）与实际表现之间的期望差异。
- en: 'In other words, calibration measurement answers the following question: is
    the confidence of the algorithm about its own predictions correct?'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，校准测量回答了以下问题：算法对其自身预测的置信度是否正确？
- en: Promoting well calibrated models is important in potentially dangerous decision
    making problems, such as disease detection or mushroom identification.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在潜在危险的决策问题中，如疾病检测或蘑菇识别，促进良好校准模型是重要的。
- en: The importance of calibration measurement lies in the fact that it is essential
    to have a clear understanding of the confidence level that the algorithm has in
    its own predictions. A well-calibrated algorithm will produce confidence levels
    that accurately reflect the likelihood of a prediction being correct. In contrast,
    a miscalibrated algorithm will either over or under estimate its confidence in
    its predictions, leading to incorrect or unreliable outcomes.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 校准测量的重要性在于，必须清楚了解算法对其预测的置信水平。一个校准良好的算法会产生准确反映预测正确可能性的置信水平。相反，一个校准不良的算法会过高或过低估计其预测的置信度，从而导致不正确或不可靠的结果。
- en: In applications where the consequences of incorrect decisions can be severe,
    it is of utmost importance to have a well-calibrated algorithm. Misclassification
    of a disease can lead to incorrect medical treatment and harm to the patient.
    Similarly, misidentification of a mushroom can result in serious health consequences.
    In these scenarios, well-calibrated models can help ensure that the right decisions
    are made based on reliable predictions.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在决策错误后果可能严重的应用中，拥有一个良好校准的算法至关重要。疾病的误诊可能导致错误的医疗处理和对患者的伤害。类似地，蘑菇的误识别可能会导致严重的健康后果。在这些情况下，良好校准的模型可以帮助确保根据可靠的预测做出正确的决策。
- en: 'It can be computed using the **Expected Calibration Error** (ECE): This score
    measures the difference between the average predicted probability and the accuracy
    (i.e., the proportion of positive samples) in bins of predicted probability. The
    formula for the ECE is given by:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用**期望校准误差**（ECE）来计算：该分数测量平均预测概率与在预测概率区间中的准确度（即正样本的比例）之间的差异。ECE的公式如下：
- en: '![](../Images/6b535c4523c64042b1f2cf61be87ee94.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6b535c4523c64042b1f2cf61be87ee94.png)'
- en: where *M* is the number of bins, *Bm* is the set of samples in the *m*-th bin,
    *n* is the total number of samples in the test data, *accm* is the accuracy of
    the *m*-th bin, and *confm* is the average predicted probability in the *m*-th
    bin.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 其中*M*是区间的数量，*Bm*是第*m*个区间中的样本集合，*n*是测试数据中的样本总数，*accm*是第*m*个区间的准确度，*confm*是第*m*个区间的平均预测概率。
- en: Variance
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 方差
- en: 'The variance itself can be used as a secondary objective metric. The variability
    of the performance of the algorithms is something that we want to minimize. It
    can be simply computed using the **standard deviation σ** of the average score.
    It can also be defined as the **rate of convergence**: when re-trained *n* times,
    how many times the algorithm does converge to a satisfying solution? Using these
    calculations as a secondary objective metrics does not mean to take into account
    error bars of the scores when ranking the models, but to rank the models *based
    on the variance*, or at least to break ties by giving advantage to the more stable
    candidate model.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: The variance of a machine learning model’s performance is an important secondary
    objective metric that should not be overlooked. Variability in the performance
    of algorithms can result in unreliable and inconsistent outcomes. Minimizing this
    variability is crucial for ensuring the stability and robustness of the model.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: There are several ways to measure the variance of a machine learning model.
    One common method is to calculate the standard deviation of the average score
    of the model over multiple runs or folds. This provides a measure of the spread
    of the performance around the average and indicates how much the performance may
    vary in different situations.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: 'The formula for the standard deviation of the average score of the model is
    given by:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b08c1d6553c80b3583be6dfb4f289b6c.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
- en: where *n* is the number of runs or folds, *xi* is the performance score of the
    model in the *i*-th run or fold, and *x̄* (x bar) is the average performance score
    across all runs or folds.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: Interpretability and explainability
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Interpretability and explainability are related but distinct concepts in machine
    learning.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: '**Interpretability** refers to the degree to which a human can understand the
    cause of a model’s predictions. It refers to the ability to understand the internal
    workings of the model and how it arrived at its decisions.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: '**Explainability** refers to the ability to provide a human-understandable
    explanation of the model’s decision making process. It is concerned with the presentation
    of the reasons behind the predictions to humans in a understandable form, e.g.,
    through feature importance, decision trees, etc.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: In summary, interpretability focuses on the transparency of the model itself,
    while explainability focuses on the communication of the model’s behavior to a
    human audience.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: A wide survey on interpretability is proposed by [8]. They stressed out how
    interpretability is greatly valuable in one hand, but **hard to define** in the
    other hand. Another way to explain algorithms, automatically, is the sensitivity
    analysis [9]. Sensitivity analysis is a technique used to determine how changes
    in input variables of a model or system affect the output or outcomes of interest.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: Privacy
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Privacy should be measured in the case where the candidates algorithms are
    generative models, modelling a distribution of potentially confidential data.
    The goal in such case is to use the generative models in order to create artificial
    data that reassembles sufficiently the real data to use it in actual applications,
    but not too much so that private information are leaked. A metric that compute
    exactly this is the **adversarial accuracy** [10]. Here is its definition:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 隐私应在候选算法为生成模型、建模潜在机密数据的情况下进行衡量。在这种情况下，目标是使用生成模型创建足够接近实际数据的人工数据，以便在实际应用中使用，但又不至于泄露私人信息。一个准确计算这个的指标是**对抗准确度**
    [10]。这是它的定义：
- en: '![](../Images/42a02652e879827ac2378adbe7d7dcc4.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/42a02652e879827ac2378adbe7d7dcc4.png)'
- en: where the indicator function **1** takes value 1 if its argument is true and
    0 otherwise, *T* and *S* are true and synthetic data respectively.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 其中指标函数**1**在其参数为真时取值1，否则取值0，*T*和*S*分别是真实数据和合成数据。
- en: It is basically the accuracy of a 1-nearest-neighbor classifier, but the score
    we are aiming at is not 1 (perfect classification accuracy) but 0.5\. Indeed,
    a perfect score means that each generated data point has its closest neighbor
    in the real data, which means that the two distributions are too close. A score
    of 0 would mean that the two distributions are too different so the utility is
    low. Hence, a 0.5 score, where the closest neighbor of each data point can either
    be fake or real with the same probability, is what guarantee a good privacy.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 基本上这是一个1最近邻分类器的准确度，但我们追求的分数不是1（完美分类准确度），而是0.5。确实，完美分数意味着每个生成的数据点在真实数据中有其最近邻，这意味着两个分布过于接近。0的分数意味着两个分布过于不同，因此效用低。因此，0.5的分数，即每个数据点的最近邻可以是虚假或真实的概率相同，保证了良好的隐私。
- en: One limitation of this method is that a proper measure of distance is needed.
    This is also a strength because it means that the method is general and can be
    applied in different fields, by selecting an adequate distance measure.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法的一个限制是需要适当的距离度量。这也是一个优点，因为它意味着该方法是通用的，可以通过选择合适的距离度量应用于不同领域。
- en: Time and memory consumption
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 时间和内存消耗
- en: 'Simple and useful secondary objective metrics are the consumption of time,
    memory and energy of the models. There are two main approaches to take it into
    account: **limit the resources** and **track the use of resources**.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 简单而有用的次要目标指标是模型的时间、内存和能源消耗。有两种主要方法来考虑这些：**限制资源**和**跟踪资源使用**。
- en: The **training and inference time**, the **size of the model**, the **memory
    used** during the process or even the **energy consumption** are variables that
    can be limited by design or measured.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '**训练和推理时间**、**模型的大小**、**过程中的内存使用**甚至**能源消耗**都是可以通过设计进行限制或测量的变量。'
- en: The number of lines of code, or the number of characters, of a method can also
    be used as an indicator of the **simplicity and practicability** of the solution.
    However, obviously, this indicator can be easily tricked by calling external packages
    and may need a manual review.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 方法的代码行数或字符数也可以作为**简洁性和实用性**的指标。然而，显然，这个指标很容易被通过调用外部包来欺骗，可能需要人工审查。
- en: The simplest models that solve the task is preferable, are they are better for
    the environment, less costly, can be deployed in weaker devices and are easier
    to interpret.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 解决任务的最简单模型是更可取的，因为它们对环境更友好、成本更低、可以部署在较弱的设备上，并且更易于解释。
- en: A model that can produce the same results in less time is more desirable, as
    it reduces the computational resources required and can lead to cost savings.
    This is especially important in light of the current ecological crisis, as reducing
    energy consumption in computing can have a significant impact on reducing the
    carbon footprint of technology. Additionally, models that are faster to train
    and make predictions are more scalable and can be deployed in real-time applications,
    further enhancing their utility. Thus, optimizing time consumption is a key factor
    in the development of efficient and environmentally sustainable machine learning
    models.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 生产相同结果但时间更短的模型更为理想，因为这减少了所需的计算资源，并且可以节省成本。这在当前的生态危机背景下尤为重要，因为减少计算中的能源消耗可以对降低技术的碳足迹产生显著影响。此外，更快训练和预测的模型更具可扩展性，可以实时应用，进一步提升其效用。因此，优化时间消耗是开发高效且环保的机器学习模型的关键因素。
- en: Data consumption
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据消耗
- en: Data consumption, or the amount of training data required by a machine learning
    algorithm, is another crucial metric to consider when comparing different models.
    As the saying goes, “data is the new oil”, but not every situation allows for
    the luxury of vast datasets. In many real-world applications, gathering sufficient
    labeled data can be time-consuming, expensive, or even impossible. Tracking and
    limiting data consumption is, therefore, an essential aspect of model evaluation.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 数据消耗，或者说机器学习算法所需的训练数据量，在比较不同模型时是另一个关键指标。正如俗语所说，“数据是新石油”，但并非每种情况都允许使用大规模数据集。在许多实际应用中，收集足够的标记数据可能耗时、昂贵，甚至是不可能的。因此，追踪和限制数据消耗是模型评估的一个重要方面。
- en: Monitoring data consumption can help identify algorithms that perform well with
    limited data, making them more suitable for scenarios with data scarcity or for
    faster deployment. On the other hand, constraining the quantity of available training
    data can encourage the development of models that are more efficient in learning
    from smaller samples. This is typically called few-shots learning. This is where
    meta-learning techniques, like the k-shot n-way approach, come into play. In this
    method, models are trained to quickly adapt to new tasks using only a limited
    number of examples *k* from each class *n*.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 监控数据消耗可以帮助识别在有限数据下表现良好的算法，使其更适合数据稀缺或更快速部署的场景。另一方面，限制可用训练数据的数量可以鼓励开发能够更高效地从较小样本中学习的模型。这通常称为少样本学习。这就是元学习技术如k-shot
    n-way方法发挥作用的地方。在这种方法中，模型被训练以仅使用每个类别n中的有限数量的示例*k*来快速适应新任务。
- en: By intentionally limiting data consumption, meta-learning promotes the development
    of models capable of generalizing better from smaller datasets, ultimately enhancing
    their utility and adaptability in diverse situations.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 通过有意限制数据消耗，元学习促进了能够从较小数据集中更好地泛化的模型的发展，从而提高了它们在各种情况下的效用和适应性。
- en: Human-centric approaches
  id: totrans-61
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 以人为本的方法
- en: In addition to the quantitative evaluation metrics discussed earlier, it is
    essential to consider more “human” evaluation techniques when assessing machine
    learning models. These approaches place emphasis on qualitative aspects and subjective
    interpretation, bringing a human touch to the evaluation process. For instance,
    in the case of text-to-image algorithms, manual assessment of generated images
    can help determine whether the outcomes are visually appealing, coherent, and
    contextually relevant. Similarly, large language models can be subjected to psychological
    or behavioral tests, where human evaluators rate the model’s responses based on
    factors like coherence, empathy, and ethical considerations. Such human-centric
    evaluation methods can reveal insights that purely numerical metrics might overlook,
    providing a more nuanced understanding of a model’s strengths and weaknesses.
    By integrating these human-oriented techniques into our evaluation toolbox, we
    can ensure that our machine learning models are not only effective in solving
    problems but also resonate with the complex and multifaceted nature of human experiences
    and expectations.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 除了前面讨论的定量评估指标之外，在评估机器学习模型时，考虑更多“人”的评估技术至关重要。这些方法强调定性方面和主观解释，为评估过程增添了人文色彩。例如，在文本生成图像算法中，对生成的图像进行手动评估可以帮助确定结果在视觉上是否吸引人、连贯和相关性是否适当。类似地，大型语言模型可以接受心理或行为测试，评估者根据连贯性、移情和伦理考虑等因素评价模型的响应。这些以人为中心的评估方法可以揭示出纯数值指标可能忽视的见解，从而更全面地理解模型的优势和劣势。通过将这些面向人的技术融入我们的评估工具箱，我们可以确保我们的机器学习模型不仅在解决问题时有效，而且能够与复杂多面的人类体验和期望相
    resonant。
- en: Following this idea, a clear example is how the Generative Pre-trained Transformers
    (GPT) [11], the famous large language models, was tested using psychology tests
    [12, 13], high-school tests [14] and mathematics tests [15].
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 遵循这一思路，一个明显的例子是如何测试生成式预训练转换器（GPT），即著名的大型语言模型，使用心理学测试，高中测试和数学测试。
- en: Conclusion
  id: totrans-64
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: In conclusion, evaluating machine learning models goes far beyond traditional
    accuracy metrics. By embracing a holistic approach, we can better understand the
    various dimensions of a model’s performance and its impact on the real world.
    By exploring unconventional metrics such as fairness, privacy, energy consumption,
    calibration, time, memory, and data consumption, we can drive the development
    of more responsible, efficient, and adaptable models that address the diverse
    challenges we face today.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，评估机器学习模型远远超出了传统的准确性指标。通过采用整体方法，我们可以更好地理解模型性能的各个维度及其对现实世界的影响。通过探索如公平性、隐私、能源消耗、校准、时间、内存和数据消耗等非常规指标，我们可以推动开发更负责任、高效和适应性强的模型，解决我们今天面临的各种挑战。
- en: It is crucial to recognize that no one-size-fits-all solution exists when it
    comes to machine learning. By broadening our evaluation criteria and shedding
    light on these lesser-known metrics, we can foster innovation in the field, ensuring
    that our models not only perform well but also align with ethical considerations
    and practical constraints. As we continue to push the boundaries of machine learning,
    let us strive to create models that not only solve complex problems but also do
    so in a way that is responsible, equitable, and mindful of the world we live in.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 认识到没有一刀切的解决方案对于机器学习至关重要。通过扩展我们的评估标准并揭示这些鲜为人知的指标，我们可以促进该领域的创新，确保我们的模型不仅表现良好，还符合伦理考虑和实际限制。在我们继续推动机器学习的边界时，让我们努力创造既能解决复杂问题，又以负责任、公平和关注我们生活的世界的方式解决问题的模型。
- en: Thank you for reading!
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 感谢您的阅读！
- en: References
  id: totrans-68
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] Philipp Benz, Chaoning Zhang, Adil Karjauv, and In So Kweon. *Robustness
    may be at odds with fairness: An empirical study on class-wise accuracy.* 2020\.
    URL [https://arxiv.org/abs/2010.13365](https://arxiv.org/abs/2010.13365).'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] Philipp Benz, Chaoning Zhang, Adil Karjauv, 和 In So Kweon。*鲁棒性可能与公平性相悖：关于类别精度的实证研究*。2020\.
    URL [https://arxiv.org/abs/2010.13365](https://arxiv.org/abs/2010.13365)。'
- en: '[2] Mariya I. Vasileva. *The dark side of machine learning algorithms: How
    and why they can leverage bias, and what can be done to pursue algorithmic fairness.*
    In 26th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, Virtual
    Event, CA, USA. ACM, 2020\. URL [https://doi.org/10.1145/3394486.3411068](https://doi.org/10.1145/3394486.3411068).'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] Mariya I. Vasileva。*机器学习算法的黑暗面：它们如何以及为什么会利用偏见，以及如何追求算法公平性*。在第26届ACM SIGKDD知识发现与数据挖掘会议，虚拟会议，加州，美国。ACM，2020\.
    URL [https://doi.org/10.1145/3394486.3411068](https://doi.org/10.1145/3394486.3411068)。'
- en: '[3] Alexandra Chouldechova and Aaron Roth. *The frontiers of fairness in machine
    learning*. 2018\. URL [http://arxiv.org/abs/1810.08810](http://arxiv.org/abs/1810.08810).'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] Alexandra Chouldechova 和 Aaron Roth。*机器学习中的公平性前沿*。2018\. URL [http://arxiv.org/abs/1810.08810](http://arxiv.org/abs/1810.08810)。'
- en: '[4] Irene Y. Chen, Fredrik D. Johansson, and David A. Sontag. *Why is my classifier
    discriminatory?* In Neural Information Processing Systems (NeurIPS) 2018, Montréal,
    Canada. URL [https://proceedings.neurips.cc/paper/2018/hash/1f1baa5b8edac74eb4eaa329f14a0361-Abstract.html](https://proceedings.neurips.cc/paper/2018/hash/1f1baa5b8edac74eb4eaa329f14a0361-Abstract.html).'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] Irene Y. Chen, Fredrik D. Johansson, 和 David A. Sontag。*为什么我的分类器具有歧视性？*
    在2018年神经信息处理系统（NeurIPS），加拿大蒙特利尔。URL [https://proceedings.neurips.cc/paper/2018/hash/1f1baa5b8edac74eb4eaa329f14a0361-Abstract.html](https://proceedings.neurips.cc/paper/2018/hash/1f1baa5b8edac74eb4eaa329f14a0361-Abstract.html)。'
- en: '[5] Ludovico Boratto, Gianni Fenu, and Mirko Marras. *Interplay between upsampling
    and regularization for provider fairness in recommender systems.* User Model.
    User Adapt. Interact., 2021\. URL [https://doi.org/10.1007/s11257–021–09294–8](https://doi.org/10.1007/s11257–021–09294–8).'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] Ludovico Boratto, Gianni Fenu, 和 Mirko Marras。*上采样与正则化在推荐系统中对提供者公平性的相互作用*。用户模型。用户适应。互动，2021\.
    URL [https://doi.org/10.1007/s11257–021–09294–8](https://doi.org/10.1007/s11257–021–09294–8)。'
- en: '[6] Mahdi Pakdaman Naeini, Gregory F. Cooper, and Milos Hauskrecht. *Obtaining
    well calibrated probabilities using bayesian binning.* In Proceedings of the Twenty-Ninth
    AAAI Conference on Artificial Intelligence, 2015, Austin, Texas, USA. URL [http://www.aaai.org/ocs/index.php/AAAI/AAAI15/paper/view/9667](http://www.aaai.org/ocs/index.php/AAAI/AAAI15/paper/view/9667).'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '[6] Mahdi Pakdaman Naeini, Gregory F. Cooper, 和 Milos Hauskrecht。*使用贝叶斯分箱获得良好校准的概率*。在《第二十九届美国人工智能会议论文集》，2015年，德克萨斯州奥斯汀，美国。URL
    [http://www.aaai.org/ocs/index.php/AAAI/AAAI15/paper/view/9667](http://www.aaai.org/ocs/index.php/AAAI/AAAI15/paper/view/9667)。'
- en: '[7] Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q. Weinberger. *On calibration
    of modern neural networks.* In Proceedings of the 34th International Conference
    on Machine Learning, ICML 2017, Sydney, NSW, Australia, volume 70 of PMLR 2017\.
    URL [http://proceedings.mlr.press/v70/guo17a.html](http://proceedings.mlr.press/v70/guo17a.html).'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '[7] Chuan Guo, Geoff Pleiss, Yu Sun 和 Kilian Q. Weinberger. *现代神经网络的校准问题*。发表于第34届国际机器学习会议（ICML
    2017），澳大利亚悉尼，PMLR 2017年第70卷。URL [http://proceedings.mlr.press/v70/guo17a.html](http://proceedings.mlr.press/v70/guo17a.html)。'
- en: '[8] Diogo V. Carvalho, Eduardo M. Pereira, and Jaime S. Cardoso. *Machine learning
    interpretability: A survey on methods and metrics.* MDPI Electronics, 2019\. URL
    [https://www.mdpi.com/2079–9292/8/8/832/pdf](https://www.mdpi.com/2079–9292/8/8/832/pdf).'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '[8] Diogo V. Carvalho, Eduardo M. Pereira 和 Jaime S. Cardoso. *机器学习可解释性：方法与指标综述*。MDPI
    Electronics，2019\. URL [https://www.mdpi.com/2079–9292/8/8/832/pdf](https://www.mdpi.com/2079–9292/8/8/832/pdf)。'
- en: '[9] Bertrand Iooss, Vincent Chabridon, and Vincent Thouvenot. *Variance-based
    importance measures for machine learning model interpretability.* In Actes du
    Congrès, 2022\. URL [https://hal.archives-ouvertes.fr/hal-03741384](https://hal.archives-ouvertes.fr/hal-03741384).'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '[9] Bertrand Iooss, Vincent Chabridon 和 Vincent Thouvenot. *基于方差的重要性度量用于机器学习模型可解释性*。发表于2022年会议录。URL
    [https://hal.archives-ouvertes.fr/hal-03741384](https://hal.archives-ouvertes.fr/hal-03741384)。'
- en: '[10] Andrew Yale, Saloni Dash, Ritik Dutta, Isabelle Guyon, Adrien Pavao, and
    Kristin P. Bennett. *Privacy preserving synthetic health data.* In 27th European
    Symposium on Artificial Neural Networks (ESANN) 2019, Bruges, Belgium. URL [http://www.elen.ucl.ac.be/Proceedings/esann/esannpdf/es2019–29.pdf](http://www.elen.ucl.ac.be/Proceedings/esann/esannpdf/es2019–29.pdf).'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '[10] Andrew Yale, Saloni Dash, Ritik Dutta, Isabelle Guyon, Adrien Pavao 和
    Kristin P. Bennett. *隐私保护的合成健康数据*。发表于第27届欧洲人工神经网络研讨会（ESANN）2019，比利时布鲁日。URL [http://www.elen.ucl.ac.be/Proceedings/esann/esannpdf/es2019–29.pdf](http://www.elen.ucl.ac.be/Proceedings/esann/esannpdf/es2019–29.pdf)。'
- en: '[11] OpenAI. *GPT-4 technical report.* 2023\. URL [https://doi.org/10.48550/arXiv.2303.08774](https://doi.org/10.48550/arXiv.2303.08774).'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '[11] OpenAI. *GPT-4 技术报告。* 2023\. URL [https://doi.org/10.48550/arXiv.2303.08774](https://doi.org/10.48550/arXiv.2303.08774)。'
- en: '[12] Kadir Uludag and Jiao Tong. *Testing creativity of chatgpt in psychology:
    interview with chatGPT*. Preprint, 2023.'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '[12] Kadir Uludag 和 Jiao Tong. *测试 ChatGPT 在心理学中的创造力：与 ChatGPT 的访谈*。预印本，2023。'
- en: '[13] Xingxuan Li, Yutong Li, Linlin Liu, Lidong Bing, and Shafiq R. Joty. *Is
    GPT-3 a psychopath? evaluating large language models from a psychological perspective*.
    2022\. URL [https://doi.org/10.48550/arXiv.2212.10529](https://doi.org/10.48550/arXiv.2212.10529).'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '[13] Xingxuan Li, Yutong Li, Linlin Liu, Lidong Bing 和 Shafiq R. Joty. *GPT-3
    是心理变态者吗？从心理学角度评估大型语言模型*。2022\. URL [https://doi.org/10.48550/arXiv.2212.10529](https://doi.org/10.48550/arXiv.2212.10529)。'
- en: '[14] Joost de Winter. *Can chatgpt pass high school exams on english language
    comprehension?* Preprint, 2023.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '[14] Joost de Winter. *ChatGPT 能否通过高中英语语言理解考试？* 预印本，2023。'
- en: '[15] Simon Frieder, Luca Pinchetti, Ryan-Rhys Griffiths, Tommaso Salvatori,
    Thomas Lukasiewicz, Philipp Christian Petersen, Alexis Chevalier, and Julius Berner.
    *Mathematical capabilities of chatGPT.* URL [https://doi.org/10.48550/arXiv.2301.13867](https://doi.org/10.48550/arXiv.2301.13867).'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '[15] Simon Frieder, Luca Pinchetti, Ryan-Rhys Griffiths, Tommaso Salvatori,
    Thomas Lukasiewicz, Philipp Christian Petersen, Alexis Chevalier 和 Julius Berner.
    *ChatGPT 的数学能力*。URL [https://doi.org/10.48550/arXiv.2301.13867](https://doi.org/10.48550/arXiv.2301.13867)。'
