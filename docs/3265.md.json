["```py\nfor f in camera.capture_continuous(rawCapture, format=\"bgr\", use_video_port=True):\n\n    frame = f.array # grab the raw NumPy array representing the image\n    text = \"No piece\" # initialize the occupied/unoccupied text\n\n    # resize the frame, convert it to grayscale, and blur it\n    frame = imutils.resize(frame, width=500)\n    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n    gray = cv2.GaussianBlur(gray, (21, 21), 0)\n\n    # if the average frame is None, initialize it\n    if avg is None:\n        print(\"[INFO] starting background model...\")\n        avg = gray.copy().astype(\"float\")\n        rawCapture.truncate(0)\n        continue\n\n    # accumulate the weighted average between the current frame and\n    # previous frames, then compute the difference between the current\n    # frame and running average\n    cv2.accumulateWeighted(gray, avg, 0.5)\n    frameDelta = cv2.absdiff(gray, cv2.convertScaleAbs(avg))    \n\n    # threshold the delta image, dilate the thresholded image to fill\n    # in holes, then find contours on thresholded image\n    thresh = cv2.threshold(frameDelta, conf[\"delta_thresh\"], 255,\n        cv2.THRESH_BINARY)[1]\n    thresh = cv2.dilate(thresh, None, iterations=2)\n    cnts = cv2.findContours(thresh.copy(), cv2.RETR_EXTERNAL,\n        cv2.CHAIN_APPROX_SIMPLE)\n    cnts = imutils.grab_contours(cnts)\n\n    # loop over the contours\n\n    for c in cnts:\n        # if the contour is too small, ignore it\n        if cv2.contourArea(c) < conf[\"min_area\"]:\n            continue\n\n        # compute the bounding box for the contour, draw it on the frame,\n        # and update the text\n        (x, y, w, h) = cv2.boundingRect(c)\n        cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n        piece_image = frame[y:y+h,x:x+w]\n        text = \"Piece found\"\n        # cv2.imshow(\"Image\", image)\n```", "```py\nif text == \"Piece found\":\n        # to save images of bounding boxes\n        motionCounter += 1\n        print(\"motionCounter= \", motionCounter)\n        print(\"image_number= \", image_number)\n\n        # Save image if motion is detected for 8 or more successive frames\n        if motionCounter >= 8:\n            image_number +=1\n            image_name = str(image_number)+\"image.jpg\"\n            cv2.imwrite(os.path.join(path, image_name), piece_image)\n            motionCounter = 0 #reset the motion counter\n\n # classify the saved image with our model, see below\n```", "```py\ndata_augmentation = keras.Sequential([\n    layers.RandomFlip(\"horizontal\",\n                        input_shape=(img_height,\n                                    img_width,\n                                    1)),\n    layers.RandomRotation(0.2),\n    layers.RandomZoom(0.1),\n    ])\n\nmodel = keras.Sequential(\n    [\n        data_augmentation,\n\n        layers.Rescaling(1./255, input_shape = (img_height,img_width,1)), #normalize the data input\n\n        layers.Conv2D(128, 3, padding=\"same\", activation='relu'),\n        layers.MaxPooling2D(pool_size=(2,2)),\n\n        layers.Conv2D(64, 3, padding=\"same\", activation='relu'), #should this be 16 or 32 units? try with more data\n        layers.MaxPooling2D(pool_size=(2,2)),\n\n        layers.Conv2D(32, 3, padding=\"same\", activation='relu'),\n        layers.MaxPooling2D(pool_size=(2,2)),\n\n        layers.Conv2D(16, 3, padding=\"same\", activation='relu'),\n        layers.MaxPooling2D(pool_size=(2,2)),\n\n        layers.Dropout(0.1),\n        layers.Flatten(),\n        layers.Dense(10,activation = 'relu'),\n        layers.Dense(7,activation='softmax'), # number of output classes\n\n    ]\n)        \n\nmodel.compile(\n    optimizer=keras.optimizers.Adam(),\n    loss=[keras.losses.SparseCategoricalCrossentropy(from_logits=False),],\n    metrics=[\"accuracy\"],\n)\n\nmodel_history = model.fit(x_train, y_train, epochs=200, verbose=2, validation_data=(x_test,y_test), batch_size=25)  #i think 25/32 is the best batch size\n```", "```py\nfrom tflite_runtime.interpreter import Interpreter\n\n# Load TFLite model and allocate tensors.\ninterpreter = Interpreter(model_path=\"lego_tflite_model/detect.tflite\") # insert path to the tflite model\ninterpreter.allocate_tensors()\n```", "```py\n # continuing from if text == \"Piece found\":\n            # Open the image, resize it and increase its contrast\n            input_image = Image.open('lego-pieces/'+ image_name)\n            input_image = ImageOps.grayscale(input_image)\n            input_image = input_image.resize((128,128))\n            input_data = img_to_array(input_image)\n            input_data = increase_contrast_more(input_data)\n            input_data.resize(1,128,128,1)\n\n            # Pass the np.array of the image through the tflite model. This will output a probablity vector\n            interpreter.set_tensor(input_details[0]['index'], input_data)\n            interpreter.invoke()\n            output_data = interpreter.get_tensor(output_details[0]['index'])\n\n            # Get the index of the highest value in the probability vector.\n            # This index value will correspond to the labels vector created above (i.e index value 1 will mean the object is most likely labels[1])\n            category_number = np.argmax(output_data[0])\n\n            # Return the classification label of the image    \n            classification_label = labels[category_number]                \n            print(\"Image Label for \" + image_name + \" is :\", classification_label)\n\n    else:\n        motionCounter = 0 # reset motion counter to look for new objects\n```"]