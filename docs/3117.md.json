["```py\ntorch.nn.Linear.__init__(self, linear_module.in_features, linear_module.out_features)\nLoraLayer.__init__(self, linear_module.in_features//group_size, linear_module.out_features)\n```", "```py\n\"pad_token_id\": 0,\n```", "```py\ngit clone -b v0.3.0 https://github.com/PanQiWei/AutoGPTQ.git \ngit clone https://github.com/yuhuixu1993/qa-lora.git\ncp qa-lora/peft_utils.py ./AutoGPTQ/auto_gptq/utils/\n```", "```py\nwget https://about.benjaminmarie.com/data/py/qalora/qalora.py\ncp qalora.py qa-lora/\n```", "```py\ncd AutoGPTQ\npip install .[triton]\ncd ..\n```", "```py\ncd qa-lora\npip install -r requirements.txt\ncd ..\n```", "```py\ngit clone https://github.com/timdettmers/bitsandbytes.git\ncd bitsandbytes\n# CUDA_VERSIONS in {110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 120}\n# make argument in {cuda110, cuda11x, cuda12x}\n# if you do not know what CUDA you have, try looking at the output of: python -m bitsandbytes\nCUDA_VERSION=118 make cuda11x\npython setup.py install\npip install -r requirements.txt\npip install protobuf==3.20.*\ncd ..\n```", "```py\ncd /content/qa-lora/\npython qalora.py  --model_path kaitchup/Llama-2-7b-4bit-32g-autogptq \\\n                  --save_steps 10 \\\n                  --output_dir output \\\n                  --per_device_train_batch_size 1 \\\n                  --gradient_accumulation_steps 16 \\\n                  --max_steps 100 \\\n                  --lora_r 16\n```", "```py\nimport torch\n\n#Path the quantized base model\nmodel_path = 'Llama-2-7b-4bit-32g-autogptq/gptq_model-4bit-32g.bin'\n#Path to the adapter fine-tuned with QA-LoRA\nlora_path = 'output/adapter_model.bin'\n#Where the merged model will be saved\nmerged_path = 'output_model'\n#The scale is the LoRA alpha divided by the LoRA rank. I trained with LoRA_alpha = LoRA_rank = 16\nscale = 16 / 16\n#The group size of the quantized base LLN\ngroup_size = 32\n\n#We merge using the CPU\nmodel = torch.load(model_path, map_location='cpu')\nlora = torch.load(lora_path, map_location='cpu')\ntmp_keys = [key[17:-14] for key in lora.keys() if 'lora_A' in key]\nfor tmp_key in tmp_keys:\n    model[tmp_key+'.qzeros'] -= (lora['base_model.model.'+tmp_key+'.lora_B.weight'] @ lora['base_model.model.'+tmp_key+'.lora_A.weight']).t() * scale / group_size /model[tmp_key+'.scales']\n\ntorch.save(model, merged_path)\n```"]