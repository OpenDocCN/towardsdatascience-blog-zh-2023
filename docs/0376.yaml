- en: Everything You Need To Know About Regularization
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 关于正则化你需要了解的一切
- en: 原文：[https://towardsdatascience.com/everything-you-need-to-know-about-regularization-64734f240622?source=collection_archive---------4-----------------------#2023-01-25](https://towardsdatascience.com/everything-you-need-to-know-about-regularization-64734f240622?source=collection_archive---------4-----------------------#2023-01-25)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/everything-you-need-to-know-about-regularization-64734f240622?source=collection_archive---------4-----------------------#2023-01-25](https://towardsdatascience.com/everything-you-need-to-know-about-regularization-64734f240622?source=collection_archive---------4-----------------------#2023-01-25)
- en: '![](../Images/707b2d036e756284d5078b0cf9dd7876.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/707b2d036e756284d5078b0cf9dd7876.png)'
- en: Image by Dall-E 2.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：Dall-E 2。
- en: Different ways to prevent overfitting in machine learning
  id: totrans-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 防止机器学习中过拟合的不同方法
- en: '[](https://hennie-de-harder.medium.com/?source=post_page-----64734f240622--------------------------------)[![Hennie
    de Harder](../Images/3e4f2cccd6cb976ca3f8bf15597daea8.png)](https://hennie-de-harder.medium.com/?source=post_page-----64734f240622--------------------------------)[](https://towardsdatascience.com/?source=post_page-----64734f240622--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----64734f240622--------------------------------)
    [Hennie de Harder](https://hennie-de-harder.medium.com/?source=post_page-----64734f240622--------------------------------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://hennie-de-harder.medium.com/?source=post_page-----64734f240622--------------------------------)[![Hennie
    de Harder](../Images/3e4f2cccd6cb976ca3f8bf15597daea8.png)](https://hennie-de-harder.medium.com/?source=post_page-----64734f240622--------------------------------)[](https://towardsdatascience.com/?source=post_page-----64734f240622--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----64734f240622--------------------------------)
    [Hennie de Harder](https://hennie-de-harder.medium.com/?source=post_page-----64734f240622--------------------------------)'
- en: ·
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ffb96be98b7b9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Feverything-you-need-to-know-about-regularization-64734f240622&user=Hennie+de+Harder&userId=fb96be98b7b9&source=post_page-fb96be98b7b9----64734f240622---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----64734f240622--------------------------------)
    ·6 min read·Jan 25, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F64734f240622&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Feverything-you-need-to-know-about-regularization-64734f240622&user=Hennie+de+Harder&userId=fb96be98b7b9&source=-----64734f240622---------------------clap_footer-----------)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ffb96be98b7b9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Feverything-you-need-to-know-about-regularization-64734f240622&user=Hennie+de+Harder&userId=fb96be98b7b9&source=post_page-fb96be98b7b9----64734f240622---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----64734f240622--------------------------------)
    · 6分钟阅读 · 2023年1月25日 [](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F64734f240622&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Feverything-you-need-to-know-about-regularization-64734f240622&user=Hennie+de+Harder&userId=fb96be98b7b9&source=-----64734f240622---------------------clap_footer-----------)'
- en: --
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F64734f240622&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Feverything-you-need-to-know-about-regularization-64734f240622&source=-----64734f240622---------------------bookmark_footer-----------)'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F64734f240622&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Feverything-you-need-to-know-about-regularization-64734f240622&source=-----64734f240622---------------------bookmark_footer-----------)'
- en: '**If you’re working with machine learning models, you’ve probably heard of
    regularization. But do you know what it is and how it works? Regularization is
    a technique used to prevent overfitting and improve the performance of models.
    In this post, we’ll break down the different types of regularization and how you
    can use them to improve your models. Besides, you learn when to apply the different
    types.**'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '**如果你正在使用机器学习模型，你可能听说过正则化。你知道它是什么以及它是如何工作的么？正则化是一种用于防止过拟合和提高模型性能的技术。在这篇文章中，我们将详细讲解不同类型的正则化以及如何利用它们来改进你的模型。此外，你还会学到何时应用不同类型的正则化。**'
- en: Regularization in machine learning means ‘simplifying the outcome’. In case
    a model is overfitting and too complex, you can use regularization to make the
    model generalize better. You should use regularization if the gap in performance
    between train and test is big. This means the model grasps too much details of
    the train set. Overfitting is related to high variance, which means the model
    is sensitive to specific samples of the train set.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习中的正则化意味着“简化结果”。如果模型出现过拟合且过于复杂，你可以使用正则化使模型更好地泛化。如果训练集和测试集之间的性能差距很大，你应该使用正则化。这意味着模型捕捉了训练集的过多细节。过拟合与高方差有关，这意味着模型对训练集的特定样本很敏感。
- en: '![](../Images/831ab37188815563db4d8358459e8987.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/831ab37188815563db4d8358459e8987.png)'
- en: Classification problem. Black lines are decision boundaries of the models. Image
    by author.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 分类问题。黑线是模型的决策边界。图像作者提供。
- en: 'Let’s start discussing different regularization techniques. Every technique
    falls into one of the following two categories: explicit or implicit regularization.
    It is possible to combine multiple regularization techniques in the same problem.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始讨论不同的正则化技术。每种技术都属于以下两类之一：显式正则化或隐式正则化。可以在同一个问题中结合多种正则化技术。
- en: Explicit Regularization
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 显式正则化
- en: All techniques that fall into explicit regularization are explicitly adding
    a term to the problem. We will dive into the three most common types of explicit
    regularization. These types are L1, L2 and Elastic Net regularization.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 所有属于显式正则化的技术都会显式地向问题中添加一个项。我们将深入探讨三种最常见的显式正则化类型。这些类型包括 L1、L2 和弹性网正则化。
- en: 'As example we take a linear regression model with independent variables *x₁*
    and *x₂*, and dependent variable *y*. The model can be represented by the following
    equation:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 作为示例，我们以一个具有独立变量*x₁*和*x₂*，以及因变量*y*的线性回归模型为例。该模型可以用以下方程表示：
- en: '![](../Images/fe979e98dcb2ad315ce68eb302e4d31f.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fe979e98dcb2ad315ce68eb302e4d31f.png)'
- en: 'We want to determine the weights: *w₁*, *w₂* and *b.* The cost function is
    equal to the mean squared error, which we want to minimize. Below the cost function
    *J*:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望确定权重：*w₁*、*w₂* 和 *b*。代价函数等于均方误差，我们希望将其最小化。下方是代价函数 *J*：
- en: '![](../Images/97e30736f21c790028e8a78b02a87e60.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/97e30736f21c790028e8a78b02a87e60.png)'
- en: In the following examples, you can see how the cost function changes for different
    types of explicit regularization.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下示例中，你可以看到不同类型显式正则化的代价函数如何变化。
- en: L1 regularization
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: L1 正则化
- en: 'This type of regularization is also known as Lasso regularization. It adds
    a term to the cost function that is proportional to the absolute value of the
    weight coefficients:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 这种类型的正则化也被称为 Lasso 正则化。它向代价函数中添加了一个与权重系数的绝对值成正比的项：
- en: '![](../Images/51ae34b728ff166f414cccf614f9f69a.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/51ae34b728ff166f414cccf614f9f69a.png)'
- en: It tends to shrink some of the weight coefficients to zero. The sum of the term
    is multiplied by lambda, which controls the amount of regularization. If lambda
    is too high, the model will be simple, and the risk of underfitting arises.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 它倾向于将一些权重系数缩小到零。该项的总和乘以 lambda，它控制正则化的程度。如果 lambda 太高，模型会变得简单，从而产生欠拟合的风险。
- en: L2 regularization
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: L2 正则化
- en: 'L2 regularization, or Ridge regularization, adds a term to the cost function
    that is proportional to the square of the weight coefficients:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: L2 正则化，或称为岭回归，向代价函数中添加了一个与权重系数的平方成正比的项：
- en: '![](../Images/afcef2f5ae3ed74eb31e152332370195.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/afcef2f5ae3ed74eb31e152332370195.png)'
- en: This term tends to shrink all of the weight coefficients, but unlike L1 regularization,
    it does not set any weight coefficients to zero.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 这种项倾向于缩小所有权重系数，但与 L1 正则化不同，它不会将任何权重系数设置为零。
- en: Elastic Net regularization
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 弹性网正则化
- en: This is a combination of both L1 and L2 regularization. As you would expect,
    with Elastic Net regularization, both of the L1 and L2 terms are added to the
    cost function. And a new hyperparameter *α* is added to control the balance between
    L1 and L2.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一种 L1 和 L2 正则化的组合。正如你所期望的，使用弹性网正则化时，L1 和 L2 的项都会被添加到代价函数中。而且还添加了一个新的超参数 *α*，用以控制
    L1 和 L2 之间的平衡。
- en: '![](../Images/25ebac0e7e1c9f4754649980160f38a8.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/25ebac0e7e1c9f4754649980160f38a8.png)'
- en: When to use L1, L2 or Elastic Net?
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 何时使用 L1、L2 或弹性网？
- en: 'In many scikit-learn models L2 is the default (see LogisticRegression and SupportVectorMachines).
    This is for a reason: L1 tends to shrink some of the weight coefficients to zero,
    which means the features are removed from te model. So L1 regularization is more
    useful for feature selection. To really prevent overfitting, L2 might be the better
    choice, because it does not set any of the weight coefficients to zero.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多scikit-learn模型中，L2是默认的（参见LogisticRegression和SupportVectorMachines）。这是有原因的：L1往往会将一些权重系数缩减为零，这意味着特征从模型中移除。因此，L1正则化更适合特征选择。为了真正防止过拟合，L2可能是更好的选择，因为它不会将任何权重系数设为零。
- en: Elastic Net regularization is a good choice when you have correlated features
    and you want to balance the feature selection and overfitting prevention. It’s
    also useful when you’re not sure whether L1 or L2 regularization would be more
    appropriate for your data and model.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 当你有相关特征并希望在特征选择和防止过拟合之间取得平衡时，弹性网正则化是一个不错的选择。当你不确定L1或L2正则化对于你的数据和模型哪个更合适时，它也很有用。
- en: In general, L2 regularization is recommended when you have a large number of
    features and you want to keep most of them in the model, and L1 regularization
    is recommended when you have a high-dimensional dataset with many correlated features
    and you want to select a subset of them.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，当你有大量特征并且希望在模型中保留大部分特征时，推荐使用L2正则化；当你有高维数据集且有许多相关特征，并希望选择其中一个子集时，推荐使用L1正则化。
- en: Implicit Regularization
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 隐式正则化
- en: The techniques that aren’t explicit, automatically fall into the implicit category.
    There are many different implicit techniques, here I will discuss some of them
    that are used often.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 那些不显式的技术通常会自动归入隐式类别。这里有许多不同的隐式技术，我将讨论一些常用的。
- en: Model parameters
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型参数
- en: 'An easy way to make a model generalize better is by specifying some of the
    model hyperparameters. Below some examples:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 使模型更好地泛化的一个简单方法是指定一些模型超参数。以下是一些示例：
- en: For tree models, it’s easy to overfit if the `model_depth` parameter isn’t set.
    Then, the model keeps growing until all leaves are pure or until the leave contains
    less than `min_samples_split` samples. You can try different values with hyperparameter
    tuning to discover the best depth.
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于树模型，如果`model_depth`参数没有设置，容易出现过拟合。此时，模型会继续增长，直到所有叶子都是纯净的，或直到叶子包含少于`min_samples_split`个样本。你可以通过超参数调优尝试不同的值，以发现最佳深度。
- en: 'In XGBoost, many parameters are related to overfitting. Here are some of them:
    `colsample_bytree` contains the ratio of features used in the tree, when you use
    less features you reduce the overfitting effect. Another parameter closely related
    is `subsample`, where you specify the ratio of samples used. Setting the learning
    rate `eta` to a lower number helps too. And `gamma` is used to control the minimum
    loss reduction needed for a split.'
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在XGBoost中，许多参数与过拟合相关。以下是其中一些：`colsample_bytree`包含树中使用的特征比例，当你使用较少的特征时，可以减少过拟合效应。另一个密切相关的参数是`subsample`，它指定使用的样本比例。将学习率`eta`设置为较低的值也有帮助。`gamma`用于控制分裂所需的最小损失减少量。
- en: 'In logistic regression and support vector machines, the `C` parameter controls
    the amount of regularization. The strength of the regularization is inversely
    proportional to `C`. Or: the higher you set it, the lower the amount of regularization.
    (This is actually not completely implicit, because it’s related to the regularization
    term.)'
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在逻辑回归和支持向量机中，`C`参数控制正则化的强度。正则化的强度与`C`成反比。也就是说，你设置得越高，正则化的程度就越低。（这实际上不是完全隐式的，因为它与正则化项有关。）
- en: In neural networks, the number of layers and the number of neurons per layer
    can cause overfitting. You can remove layers and neurons to decrease the complexity
    of the model.
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在神经网络中，层数和每层的神经元数量可能导致过拟合。你可以删除层和神经元来降低模型的复杂性。
- en: There are many more parameters for different models, you can search online or
    read documentation to discover them and improve your model.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 不同模型有许多其他参数，你可以在线搜索或阅读文档来发现它们并改进你的模型。
- en: Dropout regularization
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Dropout正则化
- en: Dropout regularization is applied during the training of deep neural networks.
    It randomly drops out a certain number of neurons from the network during each
    iteration of training, forcing the network to rely on multiple subsets of the
    neurons and therefore reduce overfitting.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: Dropout正则化在深度神经网络的训练过程中应用。它在每次训练迭代中随机丢弃一定数量的神经元，迫使网络依赖多个神经元子集，从而减少过拟合。
- en: '![](../Images/92b97102b3c367cb932c4763ea10bb7f.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/92b97102b3c367cb932c4763ea10bb7f.png)'
- en: 'Dropout layer: removing neurons from the network. Image by author.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: Dropout 层：从网络中移除神经元。图像由作者提供。
- en: Early stopping
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 早期停止
- en: This is a type of regularization that is used to prevent overfitting by monitoring
    the performance of the model on a validation set and stopping the training when
    the performance starts to degrade. Some models have early stopping built in. [XGBoost](https://xgboost.readthedocs.io/en/stable/python/python_intro.html#early-stopping)
    offers a parameter `early_stopping_rounds`, and here is [how to do it with Keras](https://keras.io/api/callbacks/early_stopping/).
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一种正则化方法，通过监控模型在验证集上的表现来防止过拟合，当表现开始下降时停止训练。一些模型内置了早期停止功能。[XGBoost](https://xgboost.readthedocs.io/en/stable/python/python_intro.html#early-stopping)
    提供了一个参数`early_stopping_rounds`，以及[如何在 Keras 中实现](https://keras.io/api/callbacks/early_stopping/)。
- en: '![](../Images/0a01d7970e3585d79f8123a02fc80e68.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0a01d7970e3585d79f8123a02fc80e68.png)'
- en: Instead of continuing training the model, you should stop at the point where
    the loss on the test set starts to increase. Image by author.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 与其继续训练模型，不如在测试集上的损失开始增加时停止。图像由作者提供。
- en: Data augmentation
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据增强
- en: 'This is a technique used to prevent overfitting by artificially enlarging the
    size of the training data by applying random but realistic transformations to
    the existing data. Data augmentation is often applied to image data: below you
    can see some different augmentation techniques (mirroring, cropping and adding
    effects). There are more, like adjusting lighting, blurring, shifting the subject,
    etcetera.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一种通过对现有数据应用随机但现实的变换来人为扩大训练数据规模的方法，以防止过拟合。数据增强通常应用于图像数据：下方可以看到一些不同的增强技术（镜像、裁剪和添加效果）。还有更多技术，如调整光照、模糊、移动对象等。
- en: '![](../Images/ea948320e60083442cac5155421aa8aa.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ea948320e60083442cac5155421aa8aa.png)'
- en: Data augmentation on image data. Image by author.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 图像数据上的数据增强。图像由作者提供。
- en: Conclusion
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: This post gives you an explanation and overview of regularization techniques.
    Preventing overfitting is an important part when using machine learning models
    and you have to make sure you handle it in the right way. Besides the techniques
    from this post, it can help to use cross validation or take care of outliers to
    reduce the generalization error.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 本文为你提供了正则化技术的解释和概述。防止过拟合是使用机器学习模型时的重要部分，你必须确保以正确的方式处理它。除了本文介绍的技术外，使用交叉验证或处理异常值也有助于减少泛化误差。
- en: Related
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 相关
- en: '[](/are-you-using-feature-distributions-to-detect-outliers-48e2ae3309?source=post_page-----64734f240622--------------------------------)
    [## Are You Using Feature Distributions to Detect Outliers?'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '[## 你是否使用特征分布来检测异常值？'
- en: Here are three better ways.
  id: totrans-62
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 这里有三种更好的方法。
- en: towardsdatascience.com](/are-you-using-feature-distributions-to-detect-outliers-48e2ae3309?source=post_page-----64734f240622--------------------------------)
    [](/model-agnostic-methods-for-interpreting-any-machine-learning-model-4f10787ef504?source=post_page-----64734f240622--------------------------------)
    [## Model-Agnostic Methods for Interpreting any Machine Learning Model
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '[## 面向模型的方法用于解释任何机器学习模型'
- en: 'An overview of interpretation methods: permutation feature importance, partial
    dependence plots, LIME, SHAP and more.'
  id: totrans-64
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 解释方法概述：置换特征重要性、部分依赖图、LIME、SHAP 等。
- en: towardsdatascience.com](/model-agnostic-methods-for-interpreting-any-machine-learning-model-4f10787ef504?source=post_page-----64734f240622--------------------------------)
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '[## 面向模型的方法用于解释任何机器学习模型'
