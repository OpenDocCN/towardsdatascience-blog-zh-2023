- en: Everything You Need To Know About Regularization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/everything-you-need-to-know-about-regularization-64734f240622?source=collection_archive---------4-----------------------#2023-01-25](https://towardsdatascience.com/everything-you-need-to-know-about-regularization-64734f240622?source=collection_archive---------4-----------------------#2023-01-25)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/707b2d036e756284d5078b0cf9dd7876.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Dall-E 2.
  prefs: []
  type: TYPE_NORMAL
- en: Different ways to prevent overfitting in machine learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://hennie-de-harder.medium.com/?source=post_page-----64734f240622--------------------------------)[![Hennie
    de Harder](../Images/3e4f2cccd6cb976ca3f8bf15597daea8.png)](https://hennie-de-harder.medium.com/?source=post_page-----64734f240622--------------------------------)[](https://towardsdatascience.com/?source=post_page-----64734f240622--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----64734f240622--------------------------------)
    [Hennie de Harder](https://hennie-de-harder.medium.com/?source=post_page-----64734f240622--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ffb96be98b7b9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Feverything-you-need-to-know-about-regularization-64734f240622&user=Hennie+de+Harder&userId=fb96be98b7b9&source=post_page-fb96be98b7b9----64734f240622---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----64734f240622--------------------------------)
    ·6 min read·Jan 25, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F64734f240622&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Feverything-you-need-to-know-about-regularization-64734f240622&user=Hennie+de+Harder&userId=fb96be98b7b9&source=-----64734f240622---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F64734f240622&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Feverything-you-need-to-know-about-regularization-64734f240622&source=-----64734f240622---------------------bookmark_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: '**If you’re working with machine learning models, you’ve probably heard of
    regularization. But do you know what it is and how it works? Regularization is
    a technique used to prevent overfitting and improve the performance of models.
    In this post, we’ll break down the different types of regularization and how you
    can use them to improve your models. Besides, you learn when to apply the different
    types.**'
  prefs: []
  type: TYPE_NORMAL
- en: Regularization in machine learning means ‘simplifying the outcome’. In case
    a model is overfitting and too complex, you can use regularization to make the
    model generalize better. You should use regularization if the gap in performance
    between train and test is big. This means the model grasps too much details of
    the train set. Overfitting is related to high variance, which means the model
    is sensitive to specific samples of the train set.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/831ab37188815563db4d8358459e8987.png)'
  prefs: []
  type: TYPE_IMG
- en: Classification problem. Black lines are decision boundaries of the models. Image
    by author.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s start discussing different regularization techniques. Every technique
    falls into one of the following two categories: explicit or implicit regularization.
    It is possible to combine multiple regularization techniques in the same problem.'
  prefs: []
  type: TYPE_NORMAL
- en: Explicit Regularization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: All techniques that fall into explicit regularization are explicitly adding
    a term to the problem. We will dive into the three most common types of explicit
    regularization. These types are L1, L2 and Elastic Net regularization.
  prefs: []
  type: TYPE_NORMAL
- en: 'As example we take a linear regression model with independent variables *x₁*
    and *x₂*, and dependent variable *y*. The model can be represented by the following
    equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/fe979e98dcb2ad315ce68eb302e4d31f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We want to determine the weights: *w₁*, *w₂* and *b.* The cost function is
    equal to the mean squared error, which we want to minimize. Below the cost function
    *J*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/97e30736f21c790028e8a78b02a87e60.png)'
  prefs: []
  type: TYPE_IMG
- en: In the following examples, you can see how the cost function changes for different
    types of explicit regularization.
  prefs: []
  type: TYPE_NORMAL
- en: L1 regularization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This type of regularization is also known as Lasso regularization. It adds
    a term to the cost function that is proportional to the absolute value of the
    weight coefficients:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/51ae34b728ff166f414cccf614f9f69a.png)'
  prefs: []
  type: TYPE_IMG
- en: It tends to shrink some of the weight coefficients to zero. The sum of the term
    is multiplied by lambda, which controls the amount of regularization. If lambda
    is too high, the model will be simple, and the risk of underfitting arises.
  prefs: []
  type: TYPE_NORMAL
- en: L2 regularization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'L2 regularization, or Ridge regularization, adds a term to the cost function
    that is proportional to the square of the weight coefficients:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/afcef2f5ae3ed74eb31e152332370195.png)'
  prefs: []
  type: TYPE_IMG
- en: This term tends to shrink all of the weight coefficients, but unlike L1 regularization,
    it does not set any weight coefficients to zero.
  prefs: []
  type: TYPE_NORMAL
- en: Elastic Net regularization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This is a combination of both L1 and L2 regularization. As you would expect,
    with Elastic Net regularization, both of the L1 and L2 terms are added to the
    cost function. And a new hyperparameter *α* is added to control the balance between
    L1 and L2.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/25ebac0e7e1c9f4754649980160f38a8.png)'
  prefs: []
  type: TYPE_IMG
- en: When to use L1, L2 or Elastic Net?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In many scikit-learn models L2 is the default (see LogisticRegression and SupportVectorMachines).
    This is for a reason: L1 tends to shrink some of the weight coefficients to zero,
    which means the features are removed from te model. So L1 regularization is more
    useful for feature selection. To really prevent overfitting, L2 might be the better
    choice, because it does not set any of the weight coefficients to zero.'
  prefs: []
  type: TYPE_NORMAL
- en: Elastic Net regularization is a good choice when you have correlated features
    and you want to balance the feature selection and overfitting prevention. It’s
    also useful when you’re not sure whether L1 or L2 regularization would be more
    appropriate for your data and model.
  prefs: []
  type: TYPE_NORMAL
- en: In general, L2 regularization is recommended when you have a large number of
    features and you want to keep most of them in the model, and L1 regularization
    is recommended when you have a high-dimensional dataset with many correlated features
    and you want to select a subset of them.
  prefs: []
  type: TYPE_NORMAL
- en: Implicit Regularization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The techniques that aren’t explicit, automatically fall into the implicit category.
    There are many different implicit techniques, here I will discuss some of them
    that are used often.
  prefs: []
  type: TYPE_NORMAL
- en: Model parameters
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'An easy way to make a model generalize better is by specifying some of the
    model hyperparameters. Below some examples:'
  prefs: []
  type: TYPE_NORMAL
- en: For tree models, it’s easy to overfit if the `model_depth` parameter isn’t set.
    Then, the model keeps growing until all leaves are pure or until the leave contains
    less than `min_samples_split` samples. You can try different values with hyperparameter
    tuning to discover the best depth.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In XGBoost, many parameters are related to overfitting. Here are some of them:
    `colsample_bytree` contains the ratio of features used in the tree, when you use
    less features you reduce the overfitting effect. Another parameter closely related
    is `subsample`, where you specify the ratio of samples used. Setting the learning
    rate `eta` to a lower number helps too. And `gamma` is used to control the minimum
    loss reduction needed for a split.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In logistic regression and support vector machines, the `C` parameter controls
    the amount of regularization. The strength of the regularization is inversely
    proportional to `C`. Or: the higher you set it, the lower the amount of regularization.
    (This is actually not completely implicit, because it’s related to the regularization
    term.)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In neural networks, the number of layers and the number of neurons per layer
    can cause overfitting. You can remove layers and neurons to decrease the complexity
    of the model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: There are many more parameters for different models, you can search online or
    read documentation to discover them and improve your model.
  prefs: []
  type: TYPE_NORMAL
- en: Dropout regularization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Dropout regularization is applied during the training of deep neural networks.
    It randomly drops out a certain number of neurons from the network during each
    iteration of training, forcing the network to rely on multiple subsets of the
    neurons and therefore reduce overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/92b97102b3c367cb932c4763ea10bb7f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Dropout layer: removing neurons from the network. Image by author.'
  prefs: []
  type: TYPE_NORMAL
- en: Early stopping
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This is a type of regularization that is used to prevent overfitting by monitoring
    the performance of the model on a validation set and stopping the training when
    the performance starts to degrade. Some models have early stopping built in. [XGBoost](https://xgboost.readthedocs.io/en/stable/python/python_intro.html#early-stopping)
    offers a parameter `early_stopping_rounds`, and here is [how to do it with Keras](https://keras.io/api/callbacks/early_stopping/).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0a01d7970e3585d79f8123a02fc80e68.png)'
  prefs: []
  type: TYPE_IMG
- en: Instead of continuing training the model, you should stop at the point where
    the loss on the test set starts to increase. Image by author.
  prefs: []
  type: TYPE_NORMAL
- en: Data augmentation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This is a technique used to prevent overfitting by artificially enlarging the
    size of the training data by applying random but realistic transformations to
    the existing data. Data augmentation is often applied to image data: below you
    can see some different augmentation techniques (mirroring, cropping and adding
    effects). There are more, like adjusting lighting, blurring, shifting the subject,
    etcetera.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ea948320e60083442cac5155421aa8aa.png)'
  prefs: []
  type: TYPE_IMG
- en: Data augmentation on image data. Image by author.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This post gives you an explanation and overview of regularization techniques.
    Preventing overfitting is an important part when using machine learning models
    and you have to make sure you handle it in the right way. Besides the techniques
    from this post, it can help to use cross validation or take care of outliers to
    reduce the generalization error.
  prefs: []
  type: TYPE_NORMAL
- en: Related
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](/are-you-using-feature-distributions-to-detect-outliers-48e2ae3309?source=post_page-----64734f240622--------------------------------)
    [## Are You Using Feature Distributions to Detect Outliers?'
  prefs: []
  type: TYPE_NORMAL
- en: Here are three better ways.
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/are-you-using-feature-distributions-to-detect-outliers-48e2ae3309?source=post_page-----64734f240622--------------------------------)
    [](/model-agnostic-methods-for-interpreting-any-machine-learning-model-4f10787ef504?source=post_page-----64734f240622--------------------------------)
    [## Model-Agnostic Methods for Interpreting any Machine Learning Model
  prefs: []
  type: TYPE_NORMAL
- en: 'An overview of interpretation methods: permutation feature importance, partial
    dependence plots, LIME, SHAP and more.'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/model-agnostic-methods-for-interpreting-any-machine-learning-model-4f10787ef504?source=post_page-----64734f240622--------------------------------)
  prefs: []
  type: TYPE_NORMAL
