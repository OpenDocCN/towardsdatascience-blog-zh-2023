["```py\nclass LoraRobertaSelfAttention(RobertaSelfAttention):\n    \"\"\"\n    Extends RobertaSelfAttention with LoRA (Low-Rank Adaptation) matrices.\n    LoRA enhances efficiency by only updating the query and value matrices.\n    This class adds LoRA matrices and applies LoRA logic in the forward method.\n\n    Parameters:\n    - r (int): Rank for LoRA matrices.\n    - config: Configuration of the Roberta Model.\n    \"\"\"\n    def __init__(self, r=8, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        d = self.all_head_size\n\n        # Initialize LoRA matrices for query and value\n        self.lora_query_matrix_B = nn.Parameter(torch.zeros(d, r))\n        self.lora_query_matrix_A = nn.Parameter(torch.randn(r, d))\n        self.lora_value_matrix_B = nn.Parameter(torch.zeros(d, r))\n        self.lora_value_matrix_A = nn.Parameter(torch.randn(r, d))\n```", "```py\nclass LoraRobertaSelfAttention(RobertaSelfAttention):\n    # ...\n\n    def lora_query(self, x):\n        \"\"\"\n        Applies LoRA to the query component. Computes a modified query output by adding \n        the LoRA adaptation to the standard query output. Requires the regular linear layer \n        to be frozen before training.\n        \"\"\"\n        lora_query_weights = torch.matmul(self.lora_query_matrix_B, self.lora_query_matrix_A)\n        return self.query(x) + F.linear(x, lora_query_weights)\n\n    def lora_value(self, x):\n        \"\"\"\n        Applies LoRA to the value component. Computes a modified value output by adding \n        the LoRA adaptation to the standard value output. Requires the regular linear layer \n        to be frozen before training.\n        \"\"\"\n        lora_value_weights = torch.matmul(self.lora_value_matrix_B, self.lora_value_matrix_A)\n        return self.value(x) + F.linear(x, lora_value_weights)\n```", "```py\nclass LoraRobertaSelfAttention(RobertaSelfAttention):\n    # ...\n    def forward(self, hidden_states, *args, **kwargs):\n        \"\"\"Copied from\nhttps://github.com/huggingface/transformers/blob/main/src/transformers/models/roberta/modeling_roberta.py\n        but replaced the query and value calls with calls to the\n        lora_query and lora_value functions.\n        We will just sketch of how to adjust this here. \n        Change every call to self.value and self.query in the actual version.\n        \"\"\"\n        # original code for query:\n        ## mixed_query_layer = self.query(hidden_states)\n        # updated query for LoRA:\n        mixed_query_layer = self.lora_query(hidden_states)\n\n        # The key has no LoRA, thus leave these calls unchanged\n        key_layer = self.transpose_for_scores(self.key(hidden_states))\n\n        # original code for value:\n        ## value_layer = self.transpose_for_scores(self.value(hidden_states))\n        # updated value for LoRA:\n        value_layer = self.transpose_for_scores(self.lora_value(hidden_states))\n\n        # ... (rest of the forward code, unchanged)\n```", "```py\nclass LoraWrapperRoberta(nn.Module):\n    def __init__(self, task_type, num_classes=None, dropout_rate=0.1, model_id=\"roberta-large\",\n                 lora_rank=8, train_biases=True, train_embedding=False, train_layer_norms=True):\n        \"\"\"\n        A wrapper for RoBERTa with Low-Rank Adaptation (LoRA) for various NLP tasks.\n        - task_type: Type of NLP task ('glue', 'squad_v1', 'squad_v2').\n        - num_classes: Number of classes for classification (varies with task).\n        - dropout_rate: Dropout rate in the model.\n        - model_id: Pre-trained RoBERTa model ID.\n        - lora_rank: Rank for LoRA adaptation.\n        - train_biases, train_embedding, train_layer_norms: \n            Flags whether to keep certain parameters trainable \n            after initializing LoRA.\n\n        Example:\n            model = LoraWrapperRoberta(task_type='glue')\n        \"\"\"\n        super().__init__()\n        # 1\\. Initialize the base model with parameters\n        self.model_id = model_id\n        self.tokenizer = RobertaTokenizer.from_pretrained(model_id)\n        self.model = RobertaModel.from_pretrained(model_id)\n        self.model_config = self.model.config\n\n        # 2\\. Add the layer for the benchmark tasks\n        d_model = self.model_config.hidden_size\n        self.finetune_head_norm = nn.LayerNorm(d_model)\n        self.finetune_head_dropout = nn.Dropout(dropout_rate)\n        self.finetune_head_classifier = nn.Linear(d_model, num_classes)\n\n        # 3\\. Set up the LoRA model for training\n        self.replace_multihead_attention()\n        self.freeze_parameters_except_lora_and_bias()\n```", "```py\nclass LoraWrapperRoberta(nn.Module):\n    # ...\n\n    def replace_multihead_attention_recursion(self, model):\n        \"\"\"\n        Replaces RobertaSelfAttention with LoraRobertaSelfAttention in the model.\n        This method applies the replacement recursively to all sub-components.\n\n        Parameters\n        ----------\n        model : nn.Module\n            The PyTorch module or model to be modified.\n        \"\"\"\n        for name, module in model.named_children():\n            if isinstance(module, RobertaSelfAttention):\n                # Replace RobertaSelfAttention with LoraRobertaSelfAttention\n                new_layer = LoraRobertaSelfAttention(r=self.lora_rank, config=self.model_config)\n                new_layer.load_state_dict(module.state_dict(), strict=False)\n                setattr(model, name, new_layer)\n            else:\n                # Recursive call for child modules\n                self.replace_multihead_attention_recursion(module)\n```", "```py\nclass LoraWrapperRoberta(nn.Module):\n    # ...\n\n    def freeze_parameters_except_lora_and_bias(self):\n        \"\"\"\n        Freezes all model parameters except for specific layers and types based on the configuration.\n        Parameters in LoRA layers, the finetune head, bias parameters, embeddings, and layer norms \n        can be set as trainable based on class settings.\n        \"\"\"\n        for name, param in self.model.named_parameters():\n            is_trainable = (\n                \"lora_\" in name or\n                \"finetune_head_\" in name or\n                (self.train_biases and \"bias\" in name) or\n                (self.train_embeddings and \"embeddings\" in name) or\n                (self.train_layer_norms and \"LayerNorm\" in name)\n            )\n            param.requires_grad = is_trainable\n```", "```py\nclass LoraLinear(nn.Linear):\n    \"\"\"\n    Extends a PyTorch linear layer with Low-Rank Adaptation (LoRA).\n    LoRA adds two matrices to the layer, allowing for efficient training of large models.\n    \"\"\"\n    def __init__(self, in_features, out_features, r=8, *args, **kwargs):\n        super().__init__(in_features, out_features, *args, **kwargs)\n\n        # Initialize LoRA matrices\n        self.lora_matrix_B = nn.Parameter(torch.zeros(out_features, r))\n        self.lora_matrix_A = nn.Parameter(torch.randn(r, in_features))\n\n        # Freeze the original weight matrix\n        self.weight.requires_grad = False\n\n    def forward(self, x: Tensor) -> Tensor:\n        # Compute LoRA weight adjustment\n        lora_weights = torch.matmul(self.lora_matrix_B, self.lora_matrix_A)\n        # Apply the original and LoRA-adjusted linear transformations\n        return super().forward(x) + F.linear(x, lora_weights)\n```", "```py\nimport bitsandbytes as bnb\nfrom transformers import AutoModel, AutoModelForSequenceClassification, BitsAndBytesConfig\n\n# Configuration to load a quantized model\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,  # Enable 4-bit loading\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.bfloat16,\n    llm_int8_skip_modules=['classifier', 'qa_outputs'],  # Skip these for quantization\n)\n\n# Load the model from Huggingface with quantization\nmodel = AutoModelForSequenceClassification.from_pretrained('roberta-base',\n          torch_dtype=\"auto\", quantization_config=bnb_config)\n```", "```py\n# Verify 4-bit loading\nprint(\"Verifying 4-bit elements (Linear4bit) in the attention layer:\")\nprint(model.roberta.encoder.layer[4].attention)\n\nprint(\"Checking for uint8 data type:\")\nprint(model.roberta.encoder.layer[4].attention.self.query.weight.dtype)\n```", "```py\nModule                                                        Parameters\n----------------------------------------------------------  ------------\nroberta.embeddings.word_embeddings.weight                     38_603_520\nroberta.embeddings.position_embeddings.weight                    394_752\nroberta.embeddings.token_type_embeddings.weight                      768\nroberta.embeddings.LayerNorm.weight                                  768\nroberta.embeddings.LayerNorm.bias                                    768\nroberta.encoder.layer.0.attention.self.query.weight              589_824\nroberta.encoder.layer.0.attention.self.query.bias                    768\nroberta.encoder.layer.0.attention.self.key.weight                589_824\nroberta.encoder.layer.0.attention.self.key.bias                      768\nroberta.encoder.layer.0.attention.self.value.weight              589_824\nroberta.encoder.layer.0.attention.self.value.bias                    768\nroberta.encoder.layer.0.attention.output.dense.weight            589_824\nroberta.encoder.layer.0.attention.output.dense.bias                  768\nroberta.encoder.layer.0.attention.output.LayerNorm.weight            768\nroberta.encoder.layer.0.attention.output.LayerNorm.bias              768\nroberta.encoder.layer.0.intermediate.dense.weight              2_359_296\nroberta.encoder.layer.0.intermediate.dense.bias                    3_072\nroberta.encoder.layer.0.output.dense.weight                    2_359_296\nroberta.encoder.layer.0.output.dense.bias                            768\nroberta.encoder.layer.0.output.LayerNorm.weight                      768\nroberta.encoder.layer.0.output.LayerNorm.bias                        768\nroberta.encoder.layer.1.attention.self.query.weight              589_824\n...\nroberta.encoder.layer.11.output.LayerNorm.bias                       768\nclassifier.dense.weight                                          589_824\nclassifier.dense.bias                                                768\nclassifier.out_proj.weight                                         1_536\nclassifier.out_proj.bias                                               2\n----------------------------------------------------------  ------------\nTOTAL                                                        124_647_170\n```", "```py\nimport peft\n\n# Config for the LoRA Injection via PEFT\npeft_config = peft.LoraConfig(\n    r=2, # rank dimension of the LoRA injected matrices\n    lora_alpha=8, # parameter for scaling, use 8 here to make it comparable with our own implementation\n    target_modules=['query', 'key', 'value', 'intermediate.dense', 'output.dense'], # be precise about dense because classifier has dense too\n    modules_to_save=[\"LayerNorm\", \"classifier\", \"qa_outputs\"], # Retrain the layer norm; classifier is the fine-tune head; qa_outputs is for SQuAD\n    lora_dropout=0.1, # dropout probability for layers\n    bias=\"all\", # none, all, or lora_only\n)\n\nmodel = peft.get_peft_model(model, peft_config)\n```", "```py\nimport torch\nimport bitsandbytes as bnb\n\n# replace this\noptimizer = torch.optim.AdamW(args here)\n# with this\noptimizer = bnb.optim.AdamW8bit(same args here)\n```"]