- en: 'Simplifying Transformers: State of the Art NLP Using Words You Understand—
    Part 1 — Intro'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/transformers-part-1-2a2755a2af0e?source=collection_archive---------7-----------------------#2023-07-26](https://towardsdatascience.com/transformers-part-1-2a2755a2af0e?source=collection_archive---------7-----------------------#2023-07-26)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[](https://medium.com/@chenmargalit?source=post_page-----2a2755a2af0e--------------------------------)[![Chen
    Margalit](../Images/fb37720654b3d1068b448d4d9ad624d5.png)](https://medium.com/@chenmargalit?source=post_page-----2a2755a2af0e--------------------------------)[](https://towardsdatascience.com/?source=post_page-----2a2755a2af0e--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----2a2755a2af0e--------------------------------)
    [Chen Margalit](https://medium.com/@chenmargalit?source=post_page-----2a2755a2af0e--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ff8e6113b0479&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransformers-part-1-2a2755a2af0e&user=Chen+Margalit&userId=f8e6113b0479&source=post_page-f8e6113b0479----2a2755a2af0e---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----2a2755a2af0e--------------------------------)
    ·3 min read·Jul 26, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F2a2755a2af0e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransformers-part-1-2a2755a2af0e&user=Chen+Margalit&userId=f8e6113b0479&source=-----2a2755a2af0e---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F2a2755a2af0e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransformers-part-1-2a2755a2af0e&source=-----2a2755a2af0e---------------------bookmark_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Transformers are a deep learning architecture that has made an outstanding
    contribution to the advancement of AI. It’s a significant stage within the realm
    of both AI and technology as a whole, but it’s also a bit complicated. As of today,
    there are quite a few good resources on Transformers, so why make another one?
    Two reasons:'
  prefs: []
  type: TYPE_NORMAL
- en: I’m well versed in self-learning and from my experience, being able to read
    how different people describe the same ideas greatly enhances understanding.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: I very rarely read an article and think it's explained simply enough. Tech content
    creators tend to overcomplicate or under-explain concepts all the time. It should
    be well understood that nothing is rocket science, not even rocket science. You
    can understand anything, you just need a good enough explanation. In this series,
    I try to make good enough explanations.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Moreover, As someone who owes his career to articles and open-source code, I
    see myself as obliged to return the favor.
  prefs: []
  type: TYPE_NORMAL
- en: This series will try to provide a reasonable guide both to people who know almost
    nothing about AI **and** to those who know how machines learn. How am I planning
    to do that? First and foremost — explain. I probably read something close to 1000
    technical papers (such as this) in my career, and the main problem I faced is
    that authors (subconsciously probably) assume you know so many things. In this
    series, I am planning to assume you know less, than the Transformers articles
    I read in preparation for this one.
  prefs: []
  type: TYPE_NORMAL
- en: 'Furthermore, I’ll be combining intuition, math, code, and visualizations so
    the series is designed like a candy store — something for everyone. Taking into
    account that this is an advanced concept in quite a complicated field, I’ll take
    the risk of you thinking: “wow this is slow, stop explaining obvious stuff”, but
    much less so if you think to yourself: “What the hell is he talking about?”.'
  prefs: []
  type: TYPE_NORMAL
- en: Transformers, is it worth your time?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: What's the fuss about? Is it really so important? well, as it is the basis of
    some of the world’s most advanced AI-driven technological tools (e.g. GPT et al),
    it probably is.
  prefs: []
  type: TYPE_NORMAL
- en: Although as with many scientific advancements, some of the ideas were previously
    described, the actual in-depth, full description of the architecture came from
    the “[Attention is all you need](https://arxiv.org/pdf/1706.03762.pdf)” paper
    which claims the following to be a “simple network architecture”.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/14f1ba4c207045991554b11ad20ca08a.png)'
  prefs: []
  type: TYPE_IMG
- en: Image from the [original paper](https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf)
  prefs: []
  type: TYPE_NORMAL
- en: 'If you are like most people, you **don’t** think this is a simple network architecture.
    Therefore my job is to make a good effort so that by the time you finish reading
    this series, you think to yourself: this is still not simple, but I **do** get
    it.'
  prefs: []
  type: TYPE_NORMAL
- en: So, this crazy diagram, what the heck?
  prefs: []
  type: TYPE_NORMAL
- en: What we are seeing is a Deep Learning architecture, which means that each of
    those squares should be translated to some piece of code and all that bunch of
    code together will do something that as of now, people don’t really know how to
    do otherwise.
  prefs: []
  type: TYPE_NORMAL
- en: Transformers can be applied to many different use cases, but probably the most
    famous one is an automated chat. A software that can speak about many subjects
    as if it knew a lot. Resembles the [Matrix](https://en.wikipedia.org/wiki/The_Matrix)
    in a way.
  prefs: []
  type: TYPE_NORMAL
- en: I want to make it easy for people to only read what they actually need so the
    series will be broken down according to the way I think the Transformer story
    should be told. The first part is [***here***](https://medium.com/@chenmargalit/transformers-part-2-input-2a8c3a141c7d)
    and it will be about the first part of the architecture**— inputs.**
  prefs: []
  type: TYPE_NORMAL
