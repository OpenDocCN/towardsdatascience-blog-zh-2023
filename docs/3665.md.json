["```py\nuse punkt::{SentenceTokenizer, TrainingData};\nuse punkt::params::Standard;\n\nstatic STOPWORDS: [ &str ; 127 ] = [ \"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \n    \"your\", \"yours\", \"yourself\", \"yourselves\", \"he\", \"him\", \"his\", \"himself\", \"she\", \"her\", \"hers\", \"herself\", \n    \"it\", \"its\", \"itself\", \"they\", \"them\", \"their\", \"theirs\", \"themselves\", \"what\", \"which\", \"who\", \"whom\", \"this\",\n     \"that\", \"these\", \"those\", \"am\", \"is\", \"are\", \"was\", \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\", \"having\", \n     \"do\", \"does\", \"did\", \"doing\", \"a\", \"an\", \"the\", \"and\", \"but\", \"if\", \"or\", \"because\", \"as\", \"until\", \"while\", \"of\", \n     \"at\", \"by\", \"for\", \"with\", \"about\", \"against\", \"between\", \"into\", \"through\", \"during\", \"before\", \"after\", \"above\",\n     \"below\", \"to\", \"from\", \"up\", \"down\", \"in\", \"out\", \"on\", \"off\", \"over\", \"under\", \"again\", \"further\", \"then\", \"once\",\n       \"here\", \"there\", \"when\", \"where\", \"why\", \"how\", \"all\", \"any\", \"both\", \"each\", \"few\", \"more\", \"most\", \"other\", \n       \"some\", \"such\", \"no\", \"nor\", \"not\", \"only\", \"own\", \"same\", \"so\", \"than\", \"too\", \"very\", \"s\", \"t\", \"can\",\n        \"will\", \"just\", \"don\", \"should\", \"now\" ] ;\n\n/// Transform a `text` into a list of sentences\n/// It uses the popular Punkt sentence tokenizer from a Rust port: \n/// <`/`>https://github.com/ferristseng/rust-punkt<`/`>\npub fn text_to_sentences( text: &str ) -> Vec<String> {\n    let english = TrainingData::english();\n    let mut sentences: Vec<String> = Vec::new() ; \n    for s in SentenceTokenizer::<Standard>::new(text, &english) {\n        sentences.push( s.to_owned() ) ; \n    }\n    sentences\n}\n\n/// Transforms the sentence into a list of words (tokens)\n/// eliminating stopwords while doing so\npub fn sentence_to_tokens( sentence: &str ) -> Vec<&str> {\n    let tokens: Vec<&str> = sentence.split_ascii_whitespace().collect() ; \n    let filtered_tokens: Vec<&str> = tokens\n                                .into_iter()\n                                .filter( |token| !STOPWORDS.contains( &token.to_lowercase().as_str() ) )\n                                .collect() ;\n    filtered_tokens\n}\n```", "```py\nuse std::collections::HashMap;\n\n/// Given a list of words, build a frequency map\n/// where keys are words and values are the frequencies of those words\n/// This method will be used to compute the term frequencies of each word\n/// present in a sentence\npub fn get_freq_map<'a>( words: &'a Vec<&'a str> ) -> HashMap<&'a str,usize> {\n    let mut freq_map: HashMap<&str,usize> = HashMap::new() ; \n    for word in words {\n        if freq_map.contains_key( word ) {\n            freq_map\n                .entry( word )\n                .and_modify( | e | { \n                    *e += 1 ; \n                } ) ; \n        }\n        else {\n            freq_map.insert( *word , 1 ) ; \n        }\n    }\n    freq_map\n}\n```", "```py\n// Compute the term frequency of tokens present in the given sentence (tokenized)\n// Term frequency TF of token 'w' is expressed as,\n// TF(w) = (frequency of w in the sentence) / (total number of tokens in the sentence)\nfn compute_term_frequency<'a>(\n    tokenized_sentence: &'a Vec<&str>\n) -> HashMap<&'a str,f32> {\n    let words_frequencies = Tokenizer::get_freq_map( tokenized_sentence ) ;\n    let mut term_frequency: HashMap<&str,f32> = HashMap::new() ;  \n    let num_tokens = tokenized_sentence.len() ; \n    for (word , count) in words_frequencies {\n        term_frequency.insert( word , ( count as f32 ) / ( num_tokens as f32 ) ) ; \n    }\n    term_frequency\n}\n```", "```py\n// Compute the inverse document frequency of tokens present in the given sentence (tokenized)\n// Inverse document frequency IDF of token 'w' is expressed as,\n// IDF(w) = log( N / (Number of documents in which w appears) )\nfn compute_inverse_doc_frequency<'a>(\n    tokenized_sentence: &'a Vec<&str> ,\n    tokens: &'a Vec<Vec<&'a str>>\n) -> HashMap<&'a str,f32> {\n    let num_docs = tokens.len() as f32 ; \n    let mut idf: HashMap<&str,f32> = HashMap::new() ; \n    for word in tokenized_sentence {\n        let mut word_count_in_docs: usize = 0 ; \n        for doc in tokens {\n            word_count_in_docs += doc.iter().filter( |&token| token == word ).count() ;\n        }\n        idf.insert( word , ( (num_docs) / (word_count_in_docs as f32) ).log10() ) ;\n    }\n    idf\n}\n```", "```py\npub fn compute( \n    text: &str , \n    reduction_factor: f32\n ) -> String {\n    let sentences_owned: Vec<String> = Tokenizer::text_to_sentences( text ) ; \n    let mut sentences: Vec<&str> = sentences_owned\n                                            .iter()\n                                            .map( String::as_str )\n                                            .collect() ; \n    let mut tokens: Vec<Vec<&str>> = Vec::new() ; \n    for sentence in &sentences {\n        tokens.push( Tokenizer::sentence_to_tokens(sentence) ) ; \n    }\n\n    let mut sentence_scores: HashMap<&str,f32> = HashMap::new() ; \n\n    for ( i , tokenized_sentence ) in tokens.iter().enumerate() {\n        let tf: HashMap<&str,f32> = Summarizer::compute_term_frequency(tokenized_sentence) ; \n        let idf: HashMap<&str,f32> = Summarizer::compute_inverse_doc_frequency(tokenized_sentence, &tokens) ; \n        let mut tfidf_sum: f32 = 0.0 ; \n\n        // Compute TFIDF score for each word\n        // and add it to tfidf_sum\n        for word in tokenized_sentence {\n            tfidf_sum += tf.get( word ).unwrap() * idf.get( word ).unwrap() ; \n        }\n        sentence_scores.insert( sentences[i] , tfidf_sum ) ; \n    }\n\n    // Sort sentences by their scores\n    sentences.sort_by( | a , b | \n        sentence_scores.get(b).unwrap().total_cmp(sentence_scores.get(a).unwrap()) ) ; \n\n    // Compute number of sentences to be included in the summary\n    // and return the extracted summary\n    let num_summary_sents = (reduction_factor * (sentences.len() as f32) ) as usize;\n    sentences[ 0..num_summary_sents ].join( \" \" )\n}\n```", "```py\npub fn par_compute( \n    text: &str , \n    reduction_factor: f32\n ) -> String {\n    let sentences_owned: Vec<String> = Tokenizer::text_to_sentences( text ) ; \n    let mut sentences: Vec<&str> = sentences_owned\n                                            .iter()\n                                            .map( String::as_str )\n                                            .collect() ; \n\n    // Tokenize sentences in parallel with Rayon\n    // Declare a thread-safe Vec<Vec<&str>> to hold the tokenized sentences\n    let tokens_ptr: Arc<Mutex<Vec<Vec<&str>>>> = Arc::new( Mutex::new( Vec::new() ) ) ; \n    sentences.par_iter()\n             .for_each( |sentence| { \n                let sent_tokens: Vec<&str> = Tokenizer::sentence_to_tokens(sentence) ; \n                tokens_ptr.lock().unwrap().push( sent_tokens ) ; \n             } ) ; \n    let tokens = tokens_ptr.lock().unwrap() ; \n\n    // Compute scores for sentences in parallel\n    // Declare a thread-safe Hashmap<&str,f32> to hold the sentence scores\n    let sentence_scores_ptr: Arc<Mutex<HashMap<&str,f32>>> = Arc::new( Mutex::new( HashMap::new() ) ) ; \n    tokens.par_iter()\n          .zip( sentences.par_iter() )\n          .for_each( |(tokenized_sentence , sentence)| {\n        let tf: HashMap<&str,f32> = Summarizer::compute_term_frequency(tokenized_sentence) ; \n        let idf: HashMap<&str,f32> = Summarizer::compute_inverse_doc_frequency(tokenized_sentence, &tokens ) ; \n        let mut tfidf_sum: f32 = 0.0 ; \n\n        for word in tokenized_sentence {\n            tfidf_sum += tf.get( word ).unwrap() * idf.get( word ).unwrap() ; \n        }\n        tfidf_sum /= tokenized_sentence.len() as f32 ; \n        sentence_scores_ptr.lock().unwrap().insert( sentence , tfidf_sum ) ; \n    } ) ; \n    let sentence_scores = sentence_scores_ptr.lock().unwrap() ;\n\n    // Sort sentences by their scores\n    sentences.sort_by( | a , b | \n        sentence_scores.get(b).unwrap().total_cmp(sentence_scores.get(a).unwrap()) ) ; \n\n    // Compute number of sentences to be included in the summary\n    // and return the extracted summary\n    let num_summary_sents = (reduction_factor * (sentences.len() as f32) ) as usize;\n    sentences[ 0..num_summary_sents ].join( \". \" ) \n}\n```", "```py\n[lib]\nname = \"summarizer\"\ncrate_type = [ \"staticlib\" ]\n```", "```py\n/// functions exposing Rust methods as C interfaces\n/// These methods are accessible with the ABI (compiled object code)\nmod c_binding {\n\n    use std::ffi::CString;\n    use crate::summarizer::Summarizer;\n\n    #[no_mangle]\n    pub extern \"C\" fn summarize( text: *const u8 , length: usize , reduction_factor: f32 ) -> *const u8 {\n        ...  \n    }\n\n    #[no_mangle]\n    pub extern \"C\" fn par_summarize( text: *const u8 , length: usize , reduction_factor: f32 ) -> *const u8 {\n        ...\n    }\n\n}\n```"]