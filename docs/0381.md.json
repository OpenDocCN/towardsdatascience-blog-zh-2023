["```py\nimport os, boto3\n\nKB = 1024\nMB = KB * KB\n\ndef write_and_upload():\n    # write 2 GB file\n    with open('2GB.bin', 'wb') as f:\n        for i in range(2*KB):\n            f.write(os.urandom(MB))\n\n    # upload to S3\n    bucket = '<s3 bucket name>'\n    key = '<s3 key>'\n    s3 = boto3.client('s3')\n    s3.upload_file('2GB.bin', bucket, key)\n\ndef read_sequential(f, t0):\n    t1 = time.time()\n    x = f.read(MB)\n    print(f'time of first sample: {time.time() - t1}')\n    print(f'total to first sample: {time.time() - t0}')\n    t1 = time.time()\n    count = 0\n    while True:\n        x = f.read(MB)\n        if len(x) == 0:\n            break\n        count += 1\n    print(f'time of avg read: {(time.time() - t1)/count}')\n\ndef fast_forward(f):\n    t1 = time.time()\n    total = 10\n    for i in range(total):\n        f.seek(i * 100 * MB)\n        t1 = time.time()\n        x = f.read(MB)\n    print(f'time of avg random read: {(time.time() - t1)/total}')\n```", "```py\nimport boto3, time\n\nbucket = '<s3 bucket name>'\nkey = '<s3 key>'\nlocal_path = '<local path>'\n\ns3 = boto3.client('s3')\n\nconfig = boto3.s3.transfer.TransferConfig(\n    multipart_threshold=8 * MB,\n    max_concurrency=10,\n    multipart_chunksize=8 * MB,\n    num_download_attempts=5,\n    max_io_queue=100,\n    io_chunksize=256 * KB,\n    use_threads=True,\n    max_bandwidth=None)\n\nt0 = time.time()\ns3.download_file(bucket, key, local_path, Config=config)\n\nwith open(local_path, 'rb') as f:\n    read_sequential(f,t0)\n\nprint(f'total time: {time.time()-t0}')\n```", "```py\nimport shlex, time\nfrom subprocess import Popen\n\nbucket = '<s3 bucket name>'\nkey = '<s3 key>'\nlocal_path = '<local path>'\n\ncmd = f'aws s3 cp s3://{bucket}/{key} {local_path}'\np = Popen(shlex.split(cmd)).wait()\n\nwith open(local_path, 'rb') as f:\n    read_sequential(f,t0)\n\nprint(f'total time: {time.time()-t0}')\n```", "```py\nimport shlex, time\nfrom subprocess import Popen\n\nbucket = '<s3 bucket name>'\nkey = '<s3 key>'\nlocal_path = '<local path>'\n\ns5cmd = f's5cmd cp --concurrency 10 s3://{bucket}/{key} {local_path}'\np = Popen(shlex.split(cmd)).wait()\n\nwith open(local_path, 'rb') as f:\n    read_sequential(f,t0)\n\nprint(f'total time: {time.time()-t0}')\n```", "```py\nimport os, boto3, time, multiprocessing as mp\n\nbucket = '<s3 bucket name>'\nkey = '<s3 key>'\n\nt0 = time.time()\n\nos.mkfifo(local_path)\n\ndef stream_file():\n    s3 = boto3.client('s3')\n    with open(local_path, 'wb') as f:\n        s3.download_fileobj(bucket, key, f)\n\nproc = mp.Process(target=stream_file)\nproc.start()\n\nwith open(local_path, 'rb') as f:\n    read_sequential(f, t0)\n\nprint(f'total time: {time.time()-t0}')\n```", "```py\nimport boto3, time\n\nbucket = '<s3 bucket name>'\nkey = '<s3 key>'\n\ns3 = boto3.client('s3')\n\n# stream entire file\nt0 = time.time()\nresponse = s3.get_object(\n    Bucket=bucket,\n    Key=key\n)\nf = response['Body']\nread_sequential(f,t0)\n\nprint(f'total time: {time.time()-t0}')\n\n# fast forward\ntotal = 10\nt0 = time.time()\n\nfor i in range(total):\n    response = s3.get_object(\n        Bucket=bucket,\n        Key=key,\n        Range=f'bytes={i*100*MB}-{i*100*MB+MB-1}'\n    )\n    f = response['Body']\n    x = f.read()\n\nprint(f'time of avg random read: {(time.time() - t0)/total}')\n```", "```py\ngoofys -o ro -f <s3 bucket name> <local path>\n```", "```py\nbucket = '<s3 bucket name>'\nkey = '<s3 key>'\nmount_dir = '<local goofys mount>'\nsequential = True # toggle flag to run fast_forward\n\nt0 = time.time()\nwith open(f'{mount_dir}/{key}', 'rb') as f:\n    if sequential:\n        read_sequential(f, t0)\n        print(f'total time: {time.time()-t0}')\n    else:\n        fast_forward(f)\n```"]