- en: 'Applying LLMs to Enterprise Data: Concepts, Concerns, and Hot-Takes'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将 LLM 应用于企业数据：概念、关注点和热点观点
- en: 原文：[https://towardsdatascience.com/applying-llms-to-enterprise-data-concepts-concerns-and-hot-takes-e19ded4bde88?source=collection_archive---------0-----------------------#2023-05-28](https://towardsdatascience.com/applying-llms-to-enterprise-data-concepts-concerns-and-hot-takes-e19ded4bde88?source=collection_archive---------0-----------------------#2023-05-28)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/applying-llms-to-enterprise-data-concepts-concerns-and-hot-takes-e19ded4bde88?source=collection_archive---------0-----------------------#2023-05-28](https://towardsdatascience.com/applying-llms-to-enterprise-data-concepts-concerns-and-hot-takes-e19ded4bde88?source=collection_archive---------0-----------------------#2023-05-28)
- en: '[](https://medium.com/@sjstone1987?source=post_page-----e19ded4bde88--------------------------------)[![Sam
    Stone](../Images/c241f50c3904ef8ee56c826f813fa8e1.png)](https://medium.com/@sjstone1987?source=post_page-----e19ded4bde88--------------------------------)[](https://towardsdatascience.com/?source=post_page-----e19ded4bde88--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----e19ded4bde88--------------------------------)
    [Sam Stone](https://medium.com/@sjstone1987?source=post_page-----e19ded4bde88--------------------------------)'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@sjstone1987?source=post_page-----e19ded4bde88--------------------------------)[![Sam
    Stone](../Images/c241f50c3904ef8ee56c826f813fa8e1.png)](https://medium.com/@sjstone1987?source=post_page-----e19ded4bde88--------------------------------)[](https://towardsdatascience.com/?source=post_page-----e19ded4bde88--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----e19ded4bde88--------------------------------)
    [Sam Stone](https://medium.com/@sjstone1987?source=post_page-----e19ded4bde88--------------------------------)'
- en: ·
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fcbfd810ae7b5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fapplying-llms-to-enterprise-data-concepts-concerns-and-hot-takes-e19ded4bde88&user=Sam+Stone&userId=cbfd810ae7b5&source=post_page-cbfd810ae7b5----e19ded4bde88---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----e19ded4bde88--------------------------------)
    ·11 min read·May 28, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fe19ded4bde88&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fapplying-llms-to-enterprise-data-concepts-concerns-and-hot-takes-e19ded4bde88&user=Sam+Stone&userId=cbfd810ae7b5&source=-----e19ded4bde88---------------------clap_footer-----------)'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fcbfd810ae7b5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fapplying-llms-to-enterprise-data-concepts-concerns-and-hot-takes-e19ded4bde88&user=Sam+Stone&userId=cbfd810ae7b5&source=post_page-cbfd810ae7b5----e19ded4bde88---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----e19ded4bde88--------------------------------)
    ·11 分钟阅读·2023年5月28日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fe19ded4bde88&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fapplying-llms-to-enterprise-data-concepts-concerns-and-hot-takes-e19ded4bde88&user=Sam+Stone&userId=cbfd810ae7b5&source=-----e19ded4bde88---------------------clap_footer-----------)'
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fe19ded4bde88&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fapplying-llms-to-enterprise-data-concepts-concerns-and-hot-takes-e19ded4bde88&source=-----e19ded4bde88---------------------bookmark_footer-----------)![](../Images/e5099f80b99f02339e08046a067ee45c.png)'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fe19ded4bde88&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fapplying-llms-to-enterprise-data-concepts-concerns-and-hot-takes-e19ded4bde88&source=-----e19ded4bde88---------------------bookmark_footer-----------)![](../Images/e5099f80b99f02339e08046a067ee45c.png)'
- en: 'Source: [DreamStudio](https://beta.dreamstudio.ai/) (generated by author)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 来源：[DreamStudio](https://beta.dreamstudio.ai/)（由作者生成）
- en: 'Ask GPT-4 to prove there are infinite prime numbers — while rhyming — [and
    it delivers](https://arxiv.org/pdf/2303.12712.pdf). But ask it how your team performed
    vs plan last quarter, and it will fail miserably. This illustrates a fundamental
    challenge of large language models (“LLMs”): they have a good grasp of general,
    public knowledge (like prime number theory), but are entirely unaware of proprietary,
    non-public information (how your team did last quarter.)[1] And proprietary information
    is critical to the vast majority of enterprise use workflows. A model that understands
    the public internet is cute, but little use in its raw form to most organizations.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 让GPT-4证明有无限个素数——而且要押韵——[它可以做到](https://arxiv.org/pdf/2303.12712.pdf)。但让它回答你的团队上季度的表现如何，它将惨败。这说明了大型语言模型（“LLMs”）的一个根本性挑战：它们对一般的、公开的知识（如素数理论）有很好的理解，但对专有的、非公开的信息（例如你团队上季度的表现）完全不了解。[1]
    专有信息对于绝大多数企业使用工作流至关重要。一个理解公共互联网的模型虽然有趣，但对于大多数组织在原始形式下作用不大。
- en: Over the past year, I’ve had the privilege of working with a number of organizations
    applying LLMs to enterprise use cases. This post details key concepts and concerns
    that anyone embarking on such a journey should know, as well as a few hot-takes
    on how I think LLMs will evolve and implications for ML product strategy. It’s
    intended for product managers, designers, engineers and other readers with limited
    or no knowledge of how LLMs work “under the hood”, but some interest in learning
    the concepts without going into technical details.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去的一年里，我有幸与多个组织合作，将大型语言模型（LLMs）应用于企业案例。本文详细介绍了任何开始此类旅程的人应该了解的关键概念和关注点，以及对LLMs如何演变及其对机器学习产品策略的影响的一些见解。它面向产品经理、设计师、工程师以及其他对LLMs“底层”工作原理了解有限或没有了解，但有兴趣学习这些概念而无需深入技术细节的读者。
- en: Four Concepts
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 四个概念
- en: 'Retrieval-Augmented Generation (RAG): Context Windows and Embeddings'
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 检索增强生成（RAG）：上下文窗口和嵌入
- en: 'The simplest way to make an LLM reason about proprietary data is to provide
    the proprietary data in the model’s prompt. Most LLMs would have no problem answering
    the following correctly: “We have 2 customers, A and B, who spent $100K and $200K,
    respectively. Who was our largest customer and how much did they spend?” We’ve
    just done some basic prompt engineering, by prepending our query (the second sentence)
    with context (the first sentence).'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 让LLM对专有数据进行推理的最简单方法是将专有数据提供给模型的提示。大多数LLM可以正确回答以下问题：“我们有2个客户，A和B，分别花费了$100K和$200K。谁是我们最大的客户，他们花了多少钱？”我们只是通过将查询（第二句话）与上下文（第一句话）预先添加来进行了基本的提示工程。
- en: But in the real world, we may have thousands or millions of customers. How do
    we decide which information should go into the context — considering that each
    word included in the context costs money ? This is where embeddings come in. Embeddings
    are a method in which text is transformed into numerical vectors, in which similar
    text generates similar vectors (vectors that are “close together” in N-dimensional
    space).[2] We might embed website text, documents, maybe even an entire corpus
    from SharePoint, Google Docs, or Notion. Then, for each user prompt, we embed
    it and find the vectors from our text corpus that are most similar to our prompt
    vector.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 但在现实世界中，我们可能有成千上万或数百万的客户。我们如何决定哪些信息应该进入上下文——考虑到每个包含在上下文中的单词都需要付费？这就是嵌入的作用。嵌入是一种将文本转换为数值向量的方法，相似的文本生成相似的向量（在N维空间中“接近”的向量）。[2]
    我们可能会嵌入网站文本、文档，甚至是来自SharePoint、Google Docs或Notion的整个语料库。然后，对于每个用户提示，我们将其嵌入，并找到与我们的提示向量最相似的文本语料库中的向量。
- en: For example, if we embedded Wikipedia pages on animals, and the user asked a
    question about safaris, our search would rank highly the Wikipedia articles about
    lions, zebra, and giraffes. This allows us to identify the text chunks most similar
    to the prompt — and thus most likely to answer it.[3] We include these most similar
    text chunks in the context that is prepended to the prompt, so that the prompt
    hopefully contains all the information necessary for the LLM to answer the question.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果我们嵌入了有关动物的维基百科页面，当用户询问有关野生动物园的问题时，我们的搜索将高度排名有关狮子、斑马和长颈鹿的维基百科文章。这使我们能够识别与提示最相似的文本块——因此最有可能回答问题的文本块。[3]
    我们将这些最相似的文本块包含在预先添加到提示中的上下文中，以便提示中包含LLM回答问题所需的所有信息。
- en: '**Fine-Tuning**'
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**微调**'
- en: A downside of embeddings is that every call to the LLM requires all the context
    to be passed with the prompt. The LLM has no “memory” of even the most basic enterprise-specific
    concepts. And since most cloud-based LLM providers charge per prompt token, this
    can get expensive fast.[4]
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入的一个缺点是每次调用LLM都需要将所有上下文与提示一起传递。LLM没有对最基本的企业特定概念的“记忆”。而且由于大多数基于云的LLM提供商按提示令牌收费，这可能会很快变得昂贵。[4]
- en: Fine-tuning allows an LLM to understand enterprise-specific concepts without
    including them in each prompt. We take a foundation model, which already encodes
    general knowledge across billions of learned parameters, and tweak those parameters
    to reflect specific enterprise knowledge, while still retaining the underlying
    general knowledge.[5] When we generate inferences with the new fine-tuned model,
    we get that enterprise knowledge “for free”.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 微调使LLM能够理解特定企业的概念，而无需在每个提示中包含这些概念。我们取一个基础模型，该模型已经通过数十亿个学习参数编码了一般知识，并调整这些参数以反映特定企业知识，同时仍保留底层的一般知识。[5]
    当我们使用新的微调模型生成推断时，我们可以“免费”获得这些企业知识。
- en: In contrast to embeddings/prompt engineering, where the underlying model is
    a third-party black box, fine-tuning is closer to classical machine learning,
    where ML teams created their own models from scratch. Fine-tuning requires a training
    dataset with labeled observations; the fine-tuned model is highly sensitive to
    the quality and volume of that training data. We also need to make configuration
    decisions (number of epochs, learning rate, etc), orchestrate long-running training
    jobs, and track model versions. Some foundation model providers provide APIs that
    abstract away some of this complexity, some do not.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 与嵌入/提示工程不同，后者的底层模型是第三方黑箱，微调更接近经典机器学习，机器学习团队从零开始创建自己的模型。微调需要一个带有标记观察的数据集；微调后的模型对训练数据的质量和数量非常敏感。我们还需要做出配置决策（如轮次、学习率等），协调长期训练任务，并跟踪模型版本。一些基础模型提供商提供了抽象化这种复杂性的API，但有些则没有。
- en: While inferences may be cheaper with fine-tuned models, that can be outweighed
    by costly training jobs.[6] And some foundation model providers (like OpenAI)
    only support fine-tuning of lagging-edge models (so [not ChatGPT or GPT-4](https://platform.openai.com/docs/guides/fine-tuning)).
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管微调模型的推断可能更便宜，但这可能会被昂贵的训练任务所抵消。[6] 一些基础模型提供商（如OpenAI）仅支持落后模型的微调（所以[不包括ChatGPT或GPT-4](https://platform.openai.com/docs/guides/fine-tuning)）。
- en: '**Evals**'
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**Evals**'
- en: One of the novel, significant challenges presented by LLMs is measuring the
    quality of complex outputs. Classical ML teams have tried-and-true methods for
    measuring the accuracy of simple outputs, like numerical predictions or categorizations.
    But most enterprise use cases for LLMs involve generating responses that are tens
    to thousands of words. Concepts sophisticated enough to require more than ten
    words can normally be worded in many ways. So even if we have a human-validated
    “expert” response, doing an exact string match of a model response to the expert
    response is too stringent a test, and would underestimate model response quality.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: LLMs带来了一个新颖且重要的挑战，即衡量复杂输出的质量。经典的机器学习团队有经过验证的方法来衡量简单输出的准确性，如数值预测或分类。但大多数企业使用LLM的场景涉及生成数十到数千个单词的响应。概念复杂到需要超过十个单词时，通常可以用多种方式表达。因此，即使我们有一个经过人工验证的“专家”响应，对模型响应与专家响应进行精确字符串匹配也是过于严格的测试，会低估模型响应的质量。
- en: 'The [Evals](https://github.com/openai/evals) framework, open-sourced by OpenAI,
    is one approach to tackling this problem. This framework requires a labeled test
    set (where prompts are matched to “expert” responses), but it allows broad types
    of comparison between model and expert responses. For example, is the model-generated
    answer: a subset or superset of the expert answer; factually equivalent to the
    expert answer; more or less concise than the expert answer? The caveat is that
    Evals perform these checks using an LLM. If there’s a flaw in the “checker” LLM,
    the eval results may themselves be inaccurate.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '[Evals](https://github.com/openai/evals)框架，由OpenAI开源，是应对这一问题的一种方法。该框架需要一个标记的测试集（其中提示与“专家”响应匹配），但它允许对模型和专家响应进行广泛的比较。例如，模型生成的答案是否是专家答案的子集或超集；是否在事实上一致；比专家答案更简洁或不如专家答案简洁？警告是Evals使用LLM来进行这些检查。如果“检查者”LLM存在缺陷，评估结果本身可能也会不准确。'
- en: '**Adversarial Examples**'
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**对抗样本**'
- en: If you’re using an LLM in production, you need to have confidence that it will
    handle misguided or malicious user inputs safely. For most enterprises, the starting
    point is ensuring the model doesn’t spread false information. That means a system
    that knows its own limitations and when to say “I don’t know.” There are many
    tactical approaches here. It can be done via prompt engineering with prompt language
    like “Respond ‘I don’t know’ if the question cannot be answered with the context
    provided above”). It can be done with fine-tuning, by providing out-of-scope training
    examples, where the expert response is “I don’t know”.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你在生产环境中使用LLM，你需要确信它能够安全地处理误导性或恶意的用户输入。对于大多数企业来说，起点是确保模型不会传播虚假信息。这意味着系统需要知道自己的局限性以及何时说“不知道”。这里有许多战术方法。可以通过提示工程来实现，例如使用提示语言“如果无法用上述上下文回答问题，请回答‘我不知道’”。也可以通过微调来实现，提供超出范围的训练示例，其中专家的回答是“我不知道”。
- en: Enterprises also need to guard against malicious user inputs, e.g. [prompt hacking](https://learnprompting.org/docs/category/-prompt-hacking).
    Limiting the format and length of the system’s acceptable inputs and outputs can
    be an easy and effective start. Precautions are a good idea if you’re only serving
    internal users and they’re essential if you’re serving external users.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 企业还需要防范恶意用户输入，例如[提示黑客攻击](https://learnprompting.org/docs/category/-prompt-hacking)。限制系统可接受的输入和输出的格式及长度可以是一个简单而有效的起点。如果你只是为内部用户服务，采取预防措施是一个好主意；如果你为外部用户服务，这些预防措施则是必不可少的。
- en: Three Concerns
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 三个关注点
- en: '**Bias Perpetuation**'
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**偏见延续**'
- en: The developers of the most popular LLMs (OpenAI / GPT-4, Google / Bard) have
    taken pains to align their models with human preferences and deploy sophisticated
    moderation layers. If you ask GPT-4 or Bard to tell you a racist or misogynistic
    joke, they will politely refuse.[7]
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 最受欢迎的LLM（OpenAI / GPT-4、Google / Bard）的开发者们已经费尽心力地将他们的模型与人类偏好对齐，并部署了复杂的调节层。如果你让GPT-4或Bard讲一个种族歧视或厌女的笑话，它们会礼貌地拒绝。[7]
- en: That’s good news. The bad news is that this moderation, which targets societal
    biases, doesn’t necessarily prevent institutional biases. Imagine our customer
    support team has a history of being rude to a particular type of customer. If
    historical customer support conversations are naively used to construct a new
    AI system (for example, via fine-tuning) that system is likely to replicate that
    bias.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 这是个好消息。不过坏消息是，这种针对社会偏见的调节措施，并不一定能防止机构偏见。设想我们的客户支持团队对某一特定类型的客户有粗鲁的历史。如果历史的客户支持对话被天真地用于构建一个新的AI系统（例如，通过微调），这个系统可能会复制这种偏见。
- en: If you’re using past data to train an AI model (be it a classical model or a
    generative model), closely scrutinize which past situations you want to perpetuate
    into the future and which you do not. Sometimes it’s easier to set principles
    and work from those (for example, via prompt engineering), without using past
    data directly.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你使用过去的数据来训练一个AI模型（无论是经典模型还是生成模型），需要仔细审视你希望将哪些过去的情况延续到未来，哪些则不希望。 有时，制定原则并以此为基础工作（例如，通过提示工程），而不是直接使用过去的数据，会更为简便。
- en: '**Model Lock-In**'
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**模型锁定**'
- en: Unless you’ve been living under a rock, you know generative AI models are advancing
    incredibly rapidly. Given an enterprise use case, the best LLM for it today may
    not be the best solution in six months and almost certainly will not be the best
    solution in six years. Smart ML teams know they will need to switch models at
    some point.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 除非你一直生活在石头下，否则你应该知道生成型AI模型正在迅速进步。考虑到企业使用场景，今天最好的LLM可能在六个月后就不是最佳解决方案，六年后几乎肯定不会是最佳解决方案。聪明的机器学习团队知道，他们会在某个时候需要更换模型。
- en: But there are two other major reasons to build for easy LLM “swapping”. First,
    many foundation model providers have struggled to support exponentially-growing
    user volume, leading to [outages and degraded service](https://isdown.app/integrations/openai).
    Building a fallback foundation model into your system is a good idea. Second,
    it can be quite useful to test multiple foundation models in your system (“a horse
    race”) to get a sense of which performs best. Per the section above on Evals,
    it’s often difficult to measure model quality analytically, so sometimes you just
    want to run two models and qualitatively compare the responses.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 但还有另外两个主要原因需要轻松“替换” LLMs。首先，许多基础模型提供者在支持指数增长的用户量方面遇到了困难，导致了[停机和服务质量下降](https://isdown.app/integrations/openai)。在你的系统中构建一个备份基础模型是一个好主意。其次，在你的系统中测试多个基础模型（“赛马”）来了解哪个表现最好是非常有用的。根据上面的评估部分，分析地衡量模型质量通常很困难，因此有时你只想运行两个模型并进行定性比较。
- en: '**Data Leakage**'
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**数据泄露**'
- en: Read the terms and conditions of any foundation model you’re considering using.
    If the model provider has the right to use user inputs for future model training,
    that’s worrisome. LLMs are so large it’s possible that specific user queries/responses
    become directly encoded in a future model version, and could then become accessible
    to any user of that version. Imagine a user at your organization queries “how
    can I clean up this code that does XYZ? [your proprietary, confidential code here]”
    If this query is then used by the model provider to retrain their LLM, that new
    version of the LLM may learn that your proprietary code is a great way to solve
    use case XYZ. If a competitor asks how to do XYZ, the LLM could “leak” your source
    code, or something very similar.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 阅读你考虑使用的任何基础模型的条款和条件。如果模型提供者有权使用用户输入进行未来的模型训练，那是令人担忧的。LLMs 体积如此庞大，以至于特定用户的查询/响应可能会直接编码到未来的模型版本中，并可能对该版本的任何用户开放。想象一下你组织中的用户查询“我如何清理这段做
    XYZ 的代码？ [你的专有机密代码在这里]”。如果这个查询被模型提供者用来重新训练他们的 LLM，那么新版本的 LLM 可能会学习到你的专有代码是解决 XYZ
    用例的好方法。如果一个竞争对手询问如何做 XYZ，这个 LLM 可能会“泄露”你的源代码，或者类似的内容。
- en: OpenAI now [allows users to opt-out of their data being used to train models](https://openai.com/blog/new-ways-to-manage-your-data-in-chatgpt),
    which is a good precedent, but not every model provider has followed their example.
    Some organizations are also exploring running LLMs within their own virtual private
    clouds; this is a key reason for much of the interest in open-source LLMs.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI 现在[允许用户选择不让他们的数据用于训练模型](https://openai.com/blog/new-ways-to-manage-your-data-in-chatgpt)，这设立了一个良好的先例，但并不是所有模型提供者都遵循了他们的例子。一些组织也在探索在自己的虚拟私有云中运行
    LLMs；这也是对开源 LLMs 感兴趣的一个主要原因。
- en: Two Hot-Takes
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 两个热点问题
- en: '**Prompt Engineering Will Dominate Fine Tuning**'
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**提示工程将主导微调**'
- en: 'When I first started adapting LLMs for enterprise use, I was much more interested
    in fine tuning than prompt engineering. Fine tuning felt like it adhered to the
    principles of classical ML systems to which I was accustomed: wrangle some data,
    produce a train/test dataset, kick off a training job, wait a while, evaluate
    the results against some metric.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 当我首次开始为企业使用调整大型语言模型（LLMs）时，我对微调（fine tuning）的兴趣远大于提示工程（prompt engineering）。微调感觉更符合我所熟悉的经典机器学习系统的原则：处理一些数据，生成训练/测试数据集，启动训练任务，等待一段时间，评估结果与某些指标。
- en: But I’ve come to believe that prompt engineering (with embeddings) is a better
    approach for most enterprise use cases. First, the iteration cycle for prompt
    engineering is far faster than for fine tuning, because there is no model training,
    which can take hours or days. Changing a prompt and generating new responses can
    be done in minutes. Conversely, fine-tuning is an irreversible process in terms
    of model training; if you used incorrect training data or a better base model
    comes out, you need to restart your fine-tuning jobs. Second, prompt engineering
    requires far less knowledge of ML concepts like neural network hyperparameter
    optimization, training job orchestration or data wrangling. Fine-tuning often
    requires experienced ML engineers, while prompt engineering can often be done
    by software engineers without ML experience. Third, prompt engineering works better
    for the fast-growing strategy of [model chaining](https://python.langchain.com/en/latest/index.html),
    in which complex requests are decomposed into smaller, constituent requests, each
    of which can be assigned to a different LLM. Sometimes the best “constituent model”
    is a fine-tuned model.[8] But most of the value-add work for enterprises is (i)
    figuring out how to break apart their problem, (ii) write the prompts for each
    constituent part, and (iii) identify the best off-the-shelf model for each part;
    it’s not in creating their own fine-tuned models.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: The advantages of prompt engineering are likely to widen over time. Today, prompt
    engineering requires long, expensive prompts (since context must be included in
    each prompt). But I’d bet on rapidly declining cost per token, as the model provider
    space gets more competitive and providers figure out how to train LLMs more cheaply.
    Prompt engineering is also limited today by maximum prompt sizes — but , OpenAI
    already accepts 32K tokens (~40 pages of average English text) per prompt for
    GPT-4, and [Anthropic’s Claude accepts 100K tokens](https://www.anthropic.com/index/100k-context-windows)
    (~15 pages). And I’d bet on even larger context windows coming out in the near
    future.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: '*[August 2023 Update] This is a great piece on* [*why you probably don’t need
    fine-tuning.*](https://www.tidepool.so/2023/08/17/why-you-probably-dont-need-to-fine-tune-an-llm/)'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: '**Data Won’t Be The Moat It Once Was**'
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As LLMs have become better at producing human-interpretable reasoning, its useful
    to consider how humans use data to reason, and what that implies for LLMs.[9]
    Humans don’t actually use much data! Most of the time, we do “zero shot learning”,
    which simply means we answer questions without the question being accompanied
    by a set of example question-answer pairs. The questioner just provides the question,
    and we answer based on logic, principles, heuristics, biases, etc.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: This is different from the LLMs of just a few years ago, which were only good
    at few-shot learning, where you needed to include a handful of example question-answer
    pairs in your prompt. And it’s very different from classical ML, where the model
    needed to be trained on hundreds, thousands, or millions of question-answer pairs.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 这与几年前的LLMs有所不同，当时它们只能进行少量学习，你需要在提示中包含少量示例问答对。这与经典的机器学习有很大不同，后者需要对数百、数千或数百万个问答对进行训练。
- en: I strongly believe that an increasing, dominant share of LLM use cases will
    be “zero-shot”. LLMs will be able to answer most questions without any user-provided
    examples. They will need prompt engineering, in the form of instructions, policies,
    assumptions, etc. For example, [this post](https://betterprogramming.pub/i-used-gpt-3-to-find-213-security-vulnerabilities-in-a-single-codebase-cc3870ba9411)
    uses GPT-4 to review code for security vulnerabilities; the approach requires
    no data on past instances of vulnerable code. Having clear instructions, policies,
    and assumptions will become increasingly important — but having large volumes
    of high-quality, labeled, proprietary data will become less important.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 我坚信，LLM使用案例中占主导地位的将会是“零样本”使用。LLMs将能够在没有任何用户提供示例的情况下回答大多数问题。它们将需要提示工程，以指令、政策、假设等形式存在。例如，[这篇文章](https://betterprogramming.pub/i-used-gpt-3-to-find-213-security-vulnerabilities-in-a-single-codebase-cc3870ba9411)使用GPT-4审查代码中的安全漏洞；这种方法不需要关于过去易受攻击代码的数据。清晰的指令、政策和假设将变得越来越重要——但拥有大量高质量、标记好的专有数据将变得不那么重要。
- en: If you’re actively working on applying LLMs to your enterprise data, I’d love
    to hear about what you’ve found works and what does not. Please leave a comment!
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你正在积极地将LLMs应用于你的企业数据中，我很想听听你发现了什么有效，什么无效。请留下评论！
- en: Footnotes
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 脚注
- en: '[1] Until recently, LLMs were also unaware of recent public knowledge — for
    example GPT-4 was trained on information collected through Sept 2021\. However,
    the consumer interfaces for GPT-4 and Bard are now able to query the open internet
    and collect information about recent events. So recency is quickly fading as a
    knowledge limitation for LLMs.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] 直到最近，LLMs对最新的公开知识也不了解——例如，GPT-4是在2021年9月之前收集的信息上进行训练的。然而，现在GPT-4和Bard的消费者接口可以查询开放互联网，并收集有关近期事件的信息。因此，时效性正迅速消退，成为LLMs的知识限制。'
- en: '[2] Embeddings can work on all kinds of data structures, not just text.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] 嵌入可以处理各种数据结构，而不仅仅是文本。'
- en: '[3] The entire embedding workflow occurs prior to calling the LLM. For example,
    OpenAI recommends using its ada-002 model for embeddings, which is cheaper and
    faster than any of the leading-edge GPT models.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] 整个嵌入工作流程发生在调用LLM之前。例如，OpenAI推荐使用其ada-002模型进行嵌入，这比任何前沿的GPT模型都更便宜、更快。'
- en: '[4] Tokens are words or word parts. [Here’s a good explanation of why language
    models use tokens, not words.](https://blog.quickchat.ai/post/tokens-entropy-question/#:~:text=Why%20tokens%3F,up%20on%20features%20that%20matter.)'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] 令牌是词语或词语的一部分。[这是一个很好的解释，说明为什么语言模型使用令牌而不是词语。](https://blog.quickchat.ai/post/tokens-entropy-question/#:~:text=Why%20tokens%3F,up%20on%20features%20that%20matter.)'
- en: '[5] Learned parameter count could be anywhere from millions to trillions. Most
    widely-used LLMs today have billions.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] 学习到的参数数量可以从百万到万亿不等。目前大多数广泛使用的LLM具有数十亿个参数。'
- en: '[6] Cheaper inferences are not a given; [OpenAI charges](https://docs.google.com/document/d/1iJRm5vmT3Udv2mGTD4LsLobYaHEj2OaBywIzI83WdQo/edit#)
    $0.03–0.06 per 1k tokens for GPT-4 with an 8K context window (depending on whether
    the tokens are inputs or outputs, respectively). It charges $0.12 per 1k tokens
    for a fine-tuned version of Davinci, a lagging-edge model.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '[6] 更便宜的推理并非理所当然；[OpenAI收费](https://docs.google.com/document/d/1iJRm5vmT3Udv2mGTD4LsLobYaHEj2OaBywIzI83WdQo/edit#)
    每1千个令牌$0.03–0.06，适用于具有8K上下文窗口的GPT-4（具体取决于令牌是输入还是输出）。针对经过微调的Davinci模型，它的收费为每1千个令牌$0.12，该模型已相对滞后。'
- en: '[7] Of course, these are humans employed by OpenAI and Google. And since a
    lot of people disagree with those organizations’ values, they disagree with the
    moderation policies.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '[7] 当然，这些人类是由OpenAI和Google雇佣的。由于许多人不同意这些组织的价值观，他们也不同意这些审查政策。'
- en: '[8] For example, [GOAT](https://huggingface.co/papers/2305.14201) is a version
    of the open-source model LLaMA fine-tuned for arithmetic. It outperforms GPT-4
    on many arithmetic benchmarks. Most enterprises have workflows that require arithmetic;
    under the chaining approach, the parts of the workflow that involve arithmetic
    would be identified and routed to GOAT. It makes sense for such an enterprise
    to invest heavily in good routing and integration with GOAT, but, in my opinion,
    not to fine-tune their own arithmetic LLM.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '[8] 例如，[GOAT](https://huggingface.co/papers/2305.14201) 是一个针对算术进行了微调的开源模型 LLaMA
    的版本。它在许多算术基准测试中超越了 GPT-4。大多数企业的工作流程都需要进行算术运算；在链式处理方法下，涉及算术的工作流程部分会被识别并路由到 GOAT。对于这样的企业来说，投资于良好的路由和与
    GOAT 的集成是有意义的，但在我看来，不值得自己微调算术 LLM。'
- en: '[9] There is much debate about whether today’s LLMs can actually reason, and
    what actual reasoning even means (does it require consciousness? self-awareness?
    agency?) Sidestepping that debate, which is more philosophical than empirical,
    it’s worth noting that LLMs are unequivocally getting better at producing explanations
    that conform to widely-held notions of good reasoning; [this paper](https://arxiv.org/pdf/2303.12712.pdf)
    has many great examples.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '[9] 关于今天的语言模型（LLMs）是否能够真正进行推理，以及真正的推理到底意味着什么（是否需要意识？自我意识？主动性？）存在很多争论。绕过这个哲学上的而非实证性的争论，值得注意的是，LLMs
    在产生符合广泛认同的良好推理观念的解释方面确实在不断进步；[这篇论文](https://arxiv.org/pdf/2303.12712.pdf)中有许多很好的例子。'
