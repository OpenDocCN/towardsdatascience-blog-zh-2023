["```py\nimport os, boto3, time, numpy as np\nimport torch\nfrom torch.utils.data import Dataset\nfrom statistics import mean, variance\n\nKB = 1024\nMB = KB * KB\nGB = KB ** 3\n\nsample_size = MB\nnum_samples = 100000\n\n# modify to vary the size of the files\nsamples_per_file = 2000 # for 2GB files\nnum_files = num_samples//samples_per_file\nbucket = '<s3 bucket>'\nsingle_sample_path = '<path in s3>'\nlarge_file_path = '<path in s3>'\n\nclass SingleSampleDataset(Dataset):\n    def __init__(self):\n        super().__init__()\n        self.bucket = bucket\n        self.path = single_sample_path\n        self.client = boto3.client(\"s3\")\n\n    def __len__(self):\n        return num_samples\n\n    def get_bytes(self, key):\n        response = self.client.get_object(\n            Bucket=self.bucket,\n            Key=key\n        )\n        return response['Body'].read()\n\n    def __getitem__(self, index: int):\n        key = f'{self.path}/{index}.image'\n        image = np.frombuffer(self.get_bytes(key),np.uint8)\n        return {\"image\": image}\n\nclass LargeFileDataset(Dataset):\n    def __init__(self):\n        super().__init__()\n        self.bucket = bucket\n        self.path = large_file_path\n        self.client = boto3.client(\"s3\")\n\n    def __len__(self):\n        return num_samples\n\n    def get_bytes(self, file_index, sample_index):\n        response = self.client.get_object(\n            Bucket=self.bucket,\n            Key=f'{self.path}/{file_index}.bin',\n            Range=f'bytes={sample_index*MB}-{(sample_index+1)*MB-1}'\n        )\n        return response['Body'].read()\n\n    def __getitem__(self, index: int):\n        file_index = index // num_files\n        sample_index = index % samples_per_file\n        image = np.frombuffer(self.get_bytes(file_index, sample_index),\n                              np.uint8)\n        return {\"image\": image}\n\n# toggle between single sample files and large files\nuse_grouped_samples = True\n\nif use_grouped_samples:\n    dataset = LargeFileDataset()\nelse:\n    dataset = SingleSampleDataset()\n\n# set the number of parallel workers according to the number of vCPUs\ndl = torch.utils.data.DataLoader(dataset, shuffle=True,\n                                 batch_size=4, num_workers=16)\n\nstats_lst = []\nt0 = time.perf_counter()\nfor batch_idx, batch in enumerate(dl, start=1):\n    if batch_idx % 100 == 0:\n        t = time.perf_counter() - t0\n        stats_lst.append(t)\n        t0 = time.perf_counter()\n\nmean_calc = mean(stats_lst)\nvar_calc = variance(stats_lst)\nprint(f'mean {mean_calc} variance {var_calc}')\n```", "```py\nimport boto3, time\nKB = 1024\nMB = KB * KB\nGB = KB ** 3\n\ns3 = boto3.client('s3')\nbucket = '<bucket name>'\nkey = '<key of 2 GB file>'\nlocal_path = '/tmp/2GB.bin'\nnum_trials = 10\n\nfor size in [8*MB, 100*MB, 500*MB, 2*GB]:\n    print(f'multi-part size: {size}')\n    stats = []\n    for i in range(num_trials):\n        config = boto3.s3.transfer.TransferConfig(multipart_threshold=size,\n                                              multipart_chunksize=size)\n        t0 = time.time()\n        s3.download_file(bucket, key, local_path, Config=config)\n        stats.append(time.time()-t0)\n    print(f'multi-part size {size} mean {mean(stats)}')\n```", "```py\nimport os, boto3, time, math\nfrom multiprocessing import Pool\nfrom statistics import mean, variance\n\nKB = 1024\nMB = KB * KB\n\nsample_size = MB\nnum_files = 64\nsamples_per_file = 500\nfile_size = sample_size*samples_per_file\nnum_processes = 16\nbucket = '<s3 bucket>'\nlarge_file_path = '<path in s3>'\nlocal_path = '/tmp'\nnum_trials = 5\ncost_per_get = 4e-7\ncost_per_put = 5e-6\n\nfor multipart_chunksize in [1*MB, 8*MB, 100*MB, 200*MB, 500*MB]:\n    def empty_transform(file_index):\n        s3 = boto3.client('s3')\n        config = boto3.s3.transfer.TransferConfig(\n                             multipart_threshold=multipart_chunksize,\n                             multipart_chunksize=multipart_chunksize\n                             )\n        s3.download_file(bucket, \n                         f'{large_file_path}/{file_index}.bin', \n                         f'{local_path}/{file_index}.bin', \n                         Config=config)\n        s3.upload_file(f'{local_path}/{file_index}.bin',\n                       bucket,\n                       f'{large_file_path}/{file_index}.out.bin',\n                       Config=config)\n\n    stats = []\n    for i in range(num_trials):\n        with Pool(processes=num_processes) as pool:\n            t0 = time.perf_counter()\n            pool.map(empty_transform, range(num_files))\n            transform_time = time.perf_counter() - t0\n            stats.append(transform_time)\n\n    num_chunks = math.ceil(file_size/multipart_chunksize)\n    num_operations = num_files*num_chunks\n    transform_cost = num_operations * (cost_per_get + cost_per_put)\n    if num_chunks > 1:\n        # if multi-part is used add cost of\n        # CreateMultipartUpload and CompleteMultipartUpload API calls\n        transform_cost += 2 * num_files * cost_per_put\n    print(f'chunk size {multipart_chunksize}')\n    print(f'transform time {mean(stats)} variance {variance(stats)}\n    print(f'cost of API calls {transform_cost}')\n```"]