- en: A Beginner’s Guide to Building High-Quality Datasets for Machine Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/a-beginners-guide-to-building-high-quality-datasets-for-machine-learning-586a2ce7a565?source=collection_archive---------1-----------------------#2023-11-11](https://towardsdatascience.com/a-beginners-guide-to-building-high-quality-datasets-for-machine-learning-586a2ce7a565?source=collection_archive---------1-----------------------#2023-11-11)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Tools and techniques for data cleaning, visualization, augmentation, and synthetic
    data generation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@miriam.santos?source=post_page-----586a2ce7a565--------------------------------)[![Miriam
    Santos](../Images/decbc6528a641e7b02934a03e136284a.png)](https://medium.com/@miriam.santos?source=post_page-----586a2ce7a565--------------------------------)[](https://towardsdatascience.com/?source=post_page-----586a2ce7a565--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----586a2ce7a565--------------------------------)
    [Miriam Santos](https://medium.com/@miriam.santos?source=post_page-----586a2ce7a565--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F243289394aaa&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-beginners-guide-to-building-high-quality-datasets-for-machine-learning-586a2ce7a565&user=Miriam+Santos&userId=243289394aaa&source=post_page-243289394aaa----586a2ce7a565---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----586a2ce7a565--------------------------------)
    ·9 min read·Nov 11, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F586a2ce7a565&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-beginners-guide-to-building-high-quality-datasets-for-machine-learning-586a2ce7a565&user=Miriam+Santos&userId=243289394aaa&source=-----586a2ce7a565---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F586a2ce7a565&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-beginners-guide-to-building-high-quality-datasets-for-machine-learning-586a2ce7a565&source=-----586a2ce7a565---------------------bookmark_footer-----------)![](../Images/37b184efcb3a5a08d4483fdffd2ea4f8.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Finding the real source of complexity on data can look a lot like playing “data
    detective” until you find the “golden key” that unlocks the really useful insights.
    Photo by [Michael Dziedzic](https://unsplash.com/@lazycreekimages?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: Smart Data over Big Data. That’s the postulate of the “Data-Centric AI” paradigm.
  prefs: []
  type: TYPE_NORMAL
- en: '**More than just “simply preprocessing” data, data scientists should a build
    a continuous and systematic practice of understanding and improving their datasets.**'
  prefs: []
  type: TYPE_NORMAL
- en: This will ultimately drive our focus from blindly pursuing higher classification
    results by throwing ever more complex algorithms to the problem to a **deep understanding
    of why the classification results are what they are, what is indeed the source
    of complexity of the problem, and how we can adjust the data so that classifiers
    can learn the problem better,** thus increasing their performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you’re new to machine learning, this might seem a little bit daunting: *“What
    are the best practices of building high-quality datasets and how to put them in
    place?”*'
  prefs: []
  type: TYPE_NORMAL
- en: In this tutorial, we’ll go through a simple case of leveraging the Data-Centric
    AI paradigm to achieve high-quality data and improve our machine learning classification
    results.
  prefs: []
  type: TYPE_NORMAL
- en: Following the mantra of Data-Centric AI — *it’s all about the data—*at no point
    will we delve into the model itself (honestly, it will be a simple decision tree).
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ll use the [Pima Indians Diabetes Dataset](https://www.kaggle.com/datasets/uciml/pima-indians-diabetes-database)
    , freely available on Kaggle (License: [CC0: Public Domain](https://creativecommons.org/publicdomain/zero/1.0/)).
    You’ll can also find all the code and additional materials at the [Data-Centric
    AI Community GitHub](https://github.com/Data-Centric-AI-Community/awesome-python-for-data-science).'
  prefs: []
  type: TYPE_NORMAL
- en: '*Shall we get started?*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 1: Performing Data Profiling for Data Understanding'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before we start curating our dataset, we need to **understand the problem**
    we’re trying to solve and the peculiarities of the data we’re working with. Thoroughly
    understanding our data characteristics, problem complexity, and use case domain
    is one of the first principles of Data-Centric AI.
  prefs: []
  type: TYPE_NORMAL
- en: '**This will help us determine the next steps to move along your machine learning
    pipeline.**'
  prefs: []
  type: TYPE_NORMAL
- en: 'When it comes to data profiling, there are several interesting open-source
    tools that you can explore: [I’ve made a review of a few myself](/awesome-data-science-tools-to-master-in-2023-data-profiling-edition-29d29310f779),
    including `[ydata-profiling](https://github.com/ydataai/ydata-profiling)`, `[dataprep](https://github.com/sfu-db/dataprep)`,
    `[sweetviz](https://github.com/fbdesignpro/sweetviz)`, `[autoviz](https://github.com/AutoViML/AutoViz)`,
    and `[lux](https://github.com/lux-org/lux)`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'I currently use mostly [ydata-profiling](https://github.com/ydataai/ydata-profiling):
    I find it to be a top-notch tool for data practitioners that, rather than making
    us jump through pandas hoops to get the most of our data’s characteristics and
    visualizations, lets us do it all in a few lines of code.'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, you’ll need to install ydata-profiling (better use a virtual environment
    for this — if you don’t know how, you can [check this 2-min video](https://www.youtube.com/watch?v=fvXZcpTwbtA),
    or this [full tutorial](https://www.youtube.com/watch?v=jj9X1_cKRwI) if you’ve
    never worked in conda environments before):'
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, we can get a complete overview of the data by saving a `.html` report
    of all the characteristics and visualizations we need to get started:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The data report let’s us know immediately the overall characteristics of our
    data and highlights some warnings that we might need to take into account:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d33cfdfc97325f58a47c17349f181605.png)'
  prefs: []
  type: TYPE_IMG
- en: 'YData Profiling Report: Finding the dataset’s basis statistics, visualizations,
    and quality warnings.'
  prefs: []
  type: TYPE_NORMAL
- en: The datasets contains 768 observations and 9 variables/features. While 8 are
    numeric, 1 is identified as categorical (`Outcome` seems to be our target). There
    are no duplicate rows and ***apparently*** there are no missing values. Finally,
    some `High Correlation` warning sare found among the the features. Moreover, several
    features have a large number of `Zeros`.
  prefs: []
  type: TYPE_NORMAL
- en: Now it’s time to play data detective. `High Correlations` are rather expected
    in biological features, but how about these `Zero` values?
  prefs: []
  type: TYPE_NORMAL
- en: 'Looking at some of the features highlighted (e.g., `BMI`), we can see that
    these values are quite far off from the overall distribution. And resorting to
    domain knowledge, these “0” values are actually nonsensical: a 0 value is OK for
    `Pregnancies` but for BMI, Glucose, Insulin, Blood Pressure, or Skin Thickness
    is invalid.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6592ca40c0a8d3d714a7124930b80a97.png)'
  prefs: []
  type: TYPE_IMG
- en: 'YData Profiling Report: BMI feature, indicating that zero values are rather
    “off” from the distribution.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We quickly realize what these zeros are encoding: **missing data**.'
  prefs: []
  type: TYPE_NORMAL
- en: For now we’ll get on fixing this issue, but a thorough process of EDA can comprehend
    a lot more. Check this [Essential Guide to Exploratory Data Analysis](https://medium.com/towards-data-science/a-data-scientists-essential-guide-to-exploratory-data-analysis-25637eee0cf6)
    to see what else you could uncover from your data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 2: Investigating Data Quality Issues'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we found that some columns have invalid zero values, we can start by
    handling the missing data issue on our dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '**Many machine learning models and scikit-learn estimators do not natively
    support missing values**, so we need to handle these NaNs somehow before feeding
    our dataset to the estimator.'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, lets mark these 0 values as NaN values:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Now, we can use data imputation to replace the NaN observations with plausible
    replacement values.**'
  prefs: []
  type: TYPE_NORMAL
- en: 'The “no free lunch” theorem tells us that there is no best solution for every
    situation — we should investigate how different solutions affect the complexity
    of our training data and determine what boosts our machine learning model the
    best. That’s actually another principle of Data-Centric AI: *constant iteration
    and improvement*.'
  prefs: []
  type: TYPE_NORMAL
- en: 'For now will use a very simple method — `SimpleImputer` — to replace the zero
    values with the mean value of each feature. This is a very naive approach that
    is likely to create some undesired “spikes” in our distributions, but the goal
    is simply to showcase how to highlight and impute missing data, we can try better
    approaches later on:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we can then try out a very simple decision tree classifier and see what
    would be the baseline of our classification results. As a side note, decision
    trees can be extended to support missing values naturally, via surrogate splits
    or other methods. Indeed, in [scikit-learn’s documentation](https://scikit-learn.org/stable/modules/tree.html#missing-values-support)
    seems like decision trees do have built-in support for missing values in some
    conditions in the current version ( `1.3.2`). However, as I was using version
    `1.2.2` , I stumbled upon this error:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3d5570a443c728f87f81aa6fe8bd82df.png)'
  prefs: []
  type: TYPE_IMG
- en: Still, even if the NaN values are dealt with internally, **training your models
    with missing data is not a good practice**, as it will jeopardize the concepts
    that the model learns from messy and limited information.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5b0297f4de098b794bcbdcb1e364000f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here’s the confusion matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: The classification results are not great. Keep in mind that we’re using a simple
    decision tree, but still… there is a significant different between the prediction
    for our target categories. **Why is the classifier performing better for class
    “0” than class “1”?**
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 3: Augmenting Underrepresented Classes'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'If we were paying attention in Step 1 (*maybe you’ve discovered it already*),
    our target class, `Outcome`, is imbalanced. Maybe not at the point to trigger
    a warning in the default settings (the [default threshold](https://docs.profiling.ydata.ai/4.5/advanced_settings/available_settings/#variable-summary-settings)
    is `0.5`), but enough to still bias the classifier towards the majority class,
    neglecting the minority one. This is clear from the data visualization presented
    in the Profiling Report:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8cb5549bd2baeac466ac5c50f245051e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'YData Profiling Report: Outcome classes “0” and “1” are not equally represented.
    Classifiers will naturally be more biased towards the well-represented class,
    “0”, neglecting class “1”.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that while missing data can be caused by several errors during data collection,
    transmission or storage, **the class imbalance may reflect a natural characteristic
    of the domain**: e.g., there are simply less patients diagnosed with diabetes
    in this medical center.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Nevertheless, it is still important to act over the training data to guarantee
    that the model does not overlook the minority cases: in fact, that’s what we’re
    trying to predict more accurately.**'
  prefs: []
  type: TYPE_NORMAL
- en: '**A false positive is bad since it will give the wrong information to a healthy
    patient the she has diabetes.** But when additional tests are made, this will
    be just a “scare”.'
  prefs: []
  type: TYPE_NORMAL
- en: '**However, in this case, a false negative is worse.** We’d be telling a patient
    with diabetes that everything is OK, she passes undiagnosed, and the disease progresses.'
  prefs: []
  type: TYPE_NORMAL
- en: '**One way to increase these numbers is by using data oversampling techniques.**
    Data Oversampling is a popular technique among data practitioners to adjust the
    distributions of a dataset — i.e., the ratio between the existing classes or categories
    in data — thus mitigating the imbalanced data problem.'
  prefs: []
  type: TYPE_NORMAL
- en: '**And is just one of many interesting and useful applications of Synthetic
    Data.**'
  prefs: []
  type: TYPE_NORMAL
- en: While [synthetic data can have several interpretations](https://ydata.ai/resources/10-most-frequently-asked-questions-about-synthetic-data)
    — e.g., “fake data”, “dummy data”, “simulated data” — here we’re referring to
    “data-driven” synthetic data generation.
  prefs: []
  type: TYPE_NORMAL
- en: '**In that sense, synthetic data is artificially generated that that preserves
    the characteristics of real data — its structure, statistical properties, dependencies,
    and correlations.**'
  prefs: []
  type: TYPE_NORMAL
- en: There are a plethora of methods and open-source tools to generate synthetic
    data — `[ydata-synthetic](https://github.com/ydataai/ydata-synthetic)`, `[sdv](https://github.com/sdv-dev/SDV)`,
    `[gretel-synthetics](https://github.com/gretelai/gretel-synthetics)`, `[nbsynthetic](https://github.com/NextBrain-ai/nbsynthetic)`,
    and `[synthcity](https://github.com/vanderschaarlab/synthcity)` are just some
    that I’ve experimented with in the past.
  prefs: []
  type: TYPE_NORMAL
- en: 'And again… there are “no free lunches”: choosing the most appropriate method
    will invariably depend on the objective for which the synthetic data is needed.'
  prefs: []
  type: TYPE_NORMAL
- en: To have a quick grasp of how synthetic data can be used for augmentation, we’ll
    leverage the `[ydata-synthetic](https://github.com/ydataai/ydata-synthetic)` package,
    and experiment with their [Gaussian Mixture Models](https://ydata.ai/resources/synthetic-data-generation-with-gaussian-mixture-models).
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we’ll need to install the package:'
  prefs: []
  type: TYPE_NORMAL
- en: 'And once that’s done, creating synthetic data is super straightforward:'
  prefs: []
  type: TYPE_NORMAL
- en: 'After we have our synthetic data, we can simply take out a subset of the newly
    generated minority class samples generated by sampling from the synthetic data,
    and add it to the training data to create a balanced (i.e., 50%-50%) distribution:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s see how that impacts the learning of our decision tree and its subsequent
    results:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5188a10a712e1e78cfab9912fd45de6b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'And the confusion matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: Note how such a simple modification of our training set resulted in a performance
    boost of our F-score in 10% and a significant improvement of the minority class
    sensitivity results (from 53% to 73%).
  prefs: []
  type: TYPE_NORMAL
- en: '**Here lies the beauty of the Data-Centric AI paradigm: without ever touching
    our model parametrization, we’ve significantly improved the quality of our training
    set with very simple heuristics and standard techniques** — imagine what we could
    do with more advanced strategies and specialized data preparation pipelines!'
  prefs: []
  type: TYPE_NORMAL
- en: 'Sure, class 0''s recall suffered a bit, but ultimately, we need **this model
    to be more sensitive than specific** (i.e., detect better the positive class than
    the negative class) due to the particular constraints that we’re dealing with:
    disease diagnosis — again, **another principle of Data-Centric AI: methods and
    results need to be evaluated based on the domain’s needs and constraints.**'
  prefs: []
  type: TYPE_NORMAL
- en: Final Thoughts and Further Directions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Throughout this article, we’ve experimented with the Data-Centric AI paradigm
    with a very hands-on, practical use case.
  prefs: []
  type: TYPE_NORMAL
- en: We started, as always, by understanding our data. We discovered, investigated,
    and solved particular data quality issues such as **missing data**, **and improving
    our training data with synthetic data to overcome the imbalanced nature of the
    domain.** Of course, for such a quick and simple case study, we focused on simple
    heuristics to get the job done, but the work of a data scientist never stop theres.
  prefs: []
  type: TYPE_NORMAL
- en: '*How would the results change if we considered a different imputation method?
    How could we have achieved a better fit in our synthetic data generation? Should
    we have balanced both classes equally or perhaps increase the representation of
    the minority class even higher? Could some feature transformation or dimensionality
    reduction helped the classification result? Should we have removed some cofounding
    features?*'
  prefs: []
  type: TYPE_NORMAL
- en: All of these questions seem unknowable at the start of any machine learning
    project. **But as we start to measure and uncover the source of complexity in
    each dataset, we get better insights on what methods could improve classification
    results (a sort of “meta-learning” approach).** And sure enough, the data needs
    to be manipulated and improved according to both the data characteristics and
    the ultimate goal of the project.
  prefs: []
  type: TYPE_NORMAL
- en: Producing a pre-defined pipeline and treating data preparation like a one-fits-all
    solution is similar to flying blind. **Instead, a skilled data scientist continuously
    plays data detective and tries to find the best techniques based on the clues
    that the data leaves out for us to catch.** And it usually does. We just need
    to keep our eyes sharp!
  prefs: []
  type: TYPE_NORMAL
- en: I hope you’ve enjoyed the tutorial, and as always, feedback, questions, and
    suggestions are much appreciated. Let me know what other topics you would like
    me to write about in the comments!
  prefs: []
  type: TYPE_NORMAL
- en: About me
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Ph.D., Machine Learning Researcher, Educator, Data Advocate, and overall “jack-of-all-trades”.
    Here on Medium, I write about **Data-Centric AI and Data Quality**, educating
    the Data Science & Machine Learning communities on how to move from imperfect
    to intelligent data.
  prefs: []
  type: TYPE_NORMAL
- en: '[Data-Centric AI Community](https://tiny.ydata.ai/dcai-medium) | [GitHub](https://github.com/Data-Centric-AI-Community)
    | [Google Scholar](https://scholar.google.com/citations?user=isaI6u8AAAAJ&hl=en)
    | [LinkedIn](https://www.linkedin.com/in/miriamseoanesantos/)'
  prefs: []
  type: TYPE_NORMAL
