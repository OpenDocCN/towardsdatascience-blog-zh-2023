- en: One Step to Make Decision Trees Produce Better Results
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/one-step-to-make-decision-trees-produce-better-results-b0ccd6738200?source=collection_archive---------10-----------------------#2023-11-23](https://towardsdatascience.com/one-step-to-make-decision-trees-produce-better-results-b0ccd6738200?source=collection_archive---------10-----------------------#2023-11-23)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Background, implementation, and model improvement
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://gabeverzino.medium.com/?source=post_page-----b0ccd6738200--------------------------------)[![Gabe
    Verzino](../Images/36452afec54430c55594a26247136f6f.png)](https://gabeverzino.medium.com/?source=post_page-----b0ccd6738200--------------------------------)[](https://towardsdatascience.com/?source=post_page-----b0ccd6738200--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----b0ccd6738200--------------------------------)
    [Gabe Verzino](https://gabeverzino.medium.com/?source=post_page-----b0ccd6738200--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fb4abbbfdcbbb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fone-step-to-make-decision-trees-produce-better-results-b0ccd6738200&user=Gabe+Verzino&userId=b4abbbfdcbbb&source=post_page-b4abbbfdcbbb----b0ccd6738200---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----b0ccd6738200--------------------------------)
    ·7 min read·Nov 23, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fb0ccd6738200&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fone-step-to-make-decision-trees-produce-better-results-b0ccd6738200&user=Gabe+Verzino&userId=b4abbbfdcbbb&source=-----b0ccd6738200---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb0ccd6738200&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fone-step-to-make-decision-trees-produce-better-results-b0ccd6738200&source=-----b0ccd6738200---------------------bookmark_footer-----------)![](../Images/a66ae08a47ffbe5c590a3f659448b239.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: Among the trees (photo by author)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: Decision trees (DT) get ditched much too soon.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: 'It happens like this:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: The DT is trained. Natural overfitting presents. Hyper-parameters get tuned
    (unsatisfactorily). Finally, the tree is replaced with Random Forest.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: While that may be a quick win for performance, the replacement prioritizes a
    “black box” algorithm. That’s not ideal. Only a DT can produce intuitive results,
    offer business leaders the ability to compare trade-offs, and gives them a critical
    role in process improvement.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: What you can’t understand or even explain, doesn’t make it to production. This
    is especially true in industries where even small failures present extreme risks,
    like healthcare.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你无法理解甚至解释某件事情，那么它就不会进入生产环节。在那些即使小的失败也会带来极端风险的行业中，这一点尤为真实，比如医疗保健领域。
- en: '*(Side note: People often ask “Random Forests produce feature importances,
    doesn’t that explain what features are important?” Not really. Feature importances
    almost immediately get interpreted as* ***causal*** *drivers (e.g., features that
    have dependency to target), but they’re nothing more than* ***model*** *drivers.
    While helpful to the technician in that respect only, feature importances are
    generally: (1) useless in weak models (2) inflate in features with high cardinality,
    and (3) bias toward correlated features. This is another line of inquiry all together,
    but that’s basically the rub.)*'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '*(旁注：人们经常问“随机森林生成特征重要性，这难道不解释了哪些特征是重要的吗？”并不完全如此。特征重要性几乎立即被解释为* ***因果*** *驱动因素（例如，具有与目标的依赖性的特征），但它们只是*
    ***模型*** *驱动因素。虽然在这方面对技术人员有所帮助，但特征重要性通常是：（1）在弱模型中无用（2）在具有高基数的特征中膨胀，以及（3）偏向于相关特征。这是另一条完全不同的探索路径，但基本上就是这样。)*'
- en: How Decision Trees make decisions
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 决策树如何做出决策
- en: Sticking with DTs will preserve your ability to communicate results well, but
    how do you make them performant? Hyperparameter tuning only gets you so far. And
    thoughtful feature engineering should be done regardless.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 坚持使用决策树将保留您有效沟通结果的能力，但如何使它们性能卓越呢？超参数调整只能帮助到一定程度。无论如何，都应该进行深思熟虑的特征工程。
- en: As it turns out, the specific structure of feature data may allow it to adapt
    better to the underlying DT algorithm, which in turn allows the DT to produce
    better decisions.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 事实证明，特征数据的特定结构可能使其更好地适应底层的决策树算法，从而使决策树能够做出更好的决策。
- en: Under the hood, DTs separate classes by creating orthogonal decision boundaries
    (perpendicular splits) between classes among all the data you supply it. It does
    this in a greedy algorithmic way — taking the features that split the best first,
    then moving to less optimal splits in other features.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在底层，决策树通过在您提供的所有数据中创建正交决策边界（垂直分割）来分离类别。它以一种贪婪的算法方式进行这一操作 —— 首先选择最佳分割的特征，然后转向其他特征中不那么优化的分割。
- en: We can visually inspect our features for orthogonal decision boundaries. Let’s
    view features from the publicly available breast cancer dataset below. On the
    top plot below, plotting “Worst Perimeter” and “Mean Perimeter” produces a good
    orthogonal decision boundary that can separate Malignant and Benign classes well.
    So, these would be great features for DT model inclusion.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以直观地检查我们的特征以寻找正交的决策边界。让我们查看以下公开可用的乳腺癌数据集中的特征。在下面的顶部图中，绘制“最差周长”和“平均周长”可以产生良好的正交决策边界，可以很好地分离恶性和良性类别。因此，这些特征将是
    DT 模型中的很好的选择。
- en: '![](../Images/7310102f30cae9cd1ec17e361208719b.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7310102f30cae9cd1ec17e361208719b.png)'
- en: Picture by author
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片
- en: The bottom plot above shows “Mean Area” and “Mean Perimeter” for which the DT
    made orthogonal decision boundaries (as it inherently does), but these are unnecessarily
    convoluted. Probably, a diagonal separation would have been better here, but that’s
    not how DT classifiers split. Furthermore, DTs are very sensitive to even small
    variations in training data, such as outliers, which are known to produce vastly
    different trees.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 上图显示的底部显示了“平均面积”和“平均周长”，DT 生成了正交决策边界（因其固有性质），但这些是不必要复杂的。也许，对角线分隔在这里会更好，但这不是
    DT 分类器的分割方式。此外，DT 对训练数据中甚至是小变化（如异常值）非常敏感，这些变化已知会产生完全不同的树结构。
- en: To accommodate this unique and underlying mechanism of DTs — and ultimately
    improve performance and generalizability — Principal Component Analysis (PCA)
    can be applied.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 为了适应决策树的这种独特和基础机制 —— 并最终改善性能和泛化能力 —— 可以应用主成分分析（PCA）。
- en: 'PCA improves DT performance in two important ways:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: PCA 在两个重要方面提升了 DT 的性能：
- en: (1) orients key features together (that explain the most variance)
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: (1) 将关键特征定向在一起（解释最大方差的特征）
- en: (2) reduces the feature space
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: (2) 减少特征空间
- en: In fact, the PCA + DT process naturally surfaced the “Worst Perimeter” and “Mean
    Perimeter” features that you see above in the top plot. These are two of the most
    *predictive* variables and not surprisingly have an excellent orthogonal decision
    boundary.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，PCA + DT 过程自然地展现了上述顶部图中您看到的“最差周长”和“平均周长”特征。这两个是最具*预测性*的变量，毫不奇怪地具有出色的正交决策边界。
- en: Implementing the Process
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实施过程
- en: 'Recall, PCA is designed for continuous data. The breast cancer dataset is entirely
    comprised of continuous variables. *(Another Side Note: I see PCA being used on
    categorical variables — not recommended. Nominal levels don’t have implied distances,
    ordinal levels aren’t always equidistant, and enforcing distance representations
    on discrete features generally reconstitutes the variable into something meaningless.
    Another tangent for another time).*'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: Let’s begin by downloading the needed packages, and transform our breast cancer
    dataset into features **X** and target variable **y**.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: For inspection, the dataframe head of this dataset can be called.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '![](../Images/fc4d0bfa4333ed484c878022b7399ab9.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
- en: Picture by author
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: First, train the DecisionTreeClassifier without PCA, and collect those predictions
    (original_predictions).
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Now, apply PCA to select the minimum number of dimensions that can explain the
    most variance in the training set. Instead of arbitrarily choosing that number
    of dimensions, the “elbow method” can be used to identify the number of dimensions
    that would explain 99% of the variance (as hard-coded below).
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Based visually on where the graph makes an “elbow”, it finds that 6 PCA components
    explain 99% of the training set variance.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d3b04fc1506cca7b606ecce62b806a74.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
- en: Picture by author
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: Now apply PCA to capture 6 principal components on the training set. You can
    do so using Singular Value Decomposition (SVD) which is a standard matrix factorization
    technique (a process outside the scope here). As before, train the DecisionTreeClassifier
    on the PCA-transformed training set and collect those predictions (pca_predictions).
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[PRE5]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Now we can compare our original_predictions (non-PCA-transformed) to the pca_predictions
    (PCA-transformed) to observe any relative improvement in our evaluation metrics
    of accuracy, precision and recall.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: 'In comparison to the original DT trained data, when we PCA-transform the dataset
    first and then perform DT training, we get improvement across the board:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3c6d9fd23fc9c6a6528b8be8cd563bcf.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
- en: We can plot the confusion matrices to show the relative improvement of classification
    for Malignant and Benign tumors between the two DTs.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '![](../Images/a215acbd7cea6ae1dbf632f4d7c1a012.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
- en: Picture by author
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: Finally, it‘s valuable to identify which of our original features are being
    used to generate the 6 principal components. Technically, PCA generates new features
    that are linear combinations of the original features. These new features are
    orthogonal to each other and are ranked in order of the variance they explain.
    However, calling *components_attribute* can identify the features *used* in the
    creation of those components.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'And thus, for the 6 principal components that we selected, the model created
    those using the following 5 features:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7191d4b53e502acac5824a68b4f36050.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
- en: Picture by author
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Decision Trees get ditched too soon in lieu of more performant algorithms. While
    highest performance is important, it may not be best — that decision ultimately
    depends on your stakeholder needs and explaining why a model is suggesting a particular
    outcome (see “[explainable AI](https://www.ibm.com/topics/explainable-ai)”).
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树往往被过早放弃，转而使用更高性能的算法。虽然最高性能很重要，但这可能不是最好的选择——这个决定最终取决于你的利益相关者需求以及解释模型为什么会建议特定结果（参见“[可解释人工智能](https://www.ibm.com/topics/explainable-ai)”）。
- en: Instead of reaching for the most technically advanced algorithm, work to optimally
    prep your data through thoughtful feature engineering and Principal Component
    Analysis to give decision trees their best chance of revealing their intuitive
    decision-making capabilities.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 与其寻求最先进的技术算法，不如通过深思熟虑的特征工程和主成分分析来优化数据准备，从而给决策树提供最佳机会，以展示其直观的决策能力。
- en: Thanks for reading. Happy to connect with anyone on [LinkedIn](https://www.linkedin.com/in/gabe-verzino-71401137/)!
    If you would like to share any interesting data science challenges that you are
    facing currently, please leave a comment or DM and I’ll be happy to try and explore/write
    about it.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 感谢阅读。很高兴在[LinkedIn](https://www.linkedin.com/in/gabe-verzino-71401137/)上与任何人建立联系！如果你想分享你当前面临的有趣的数据科学挑战，请留下评论或发私信，我会很乐意尝试探讨/撰写相关内容。
- en: 'My most recent articles:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 我最近的文章：
- en: '[**Debugging Logistic Regression Errors — What they mean, how to do**](https://medium.com/towards-data-science/how-to-fix-errors-in-logistic-regression-32b8dd9fe6d7)'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '[**调试逻辑回归错误——它们的含义及修复方法**](https://medium.com/towards-data-science/how-to-fix-errors-in-logistic-regression-32b8dd9fe6d7)'
- en: '[**Using Bayesian Networks to Forecast Ancillary Service Volume in Hospitals**](https://medium.com/towards-data-science/using-bayesian-networks-to-forecast-ancillary-service-volume-in-hospitals-48968a978cb5)'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '[**使用贝叶斯网络预测医院辅助服务量**](https://medium.com/towards-data-science/using-bayesian-networks-to-forecast-ancillary-service-volume-in-hospitals-48968a978cb5)'
- en: '[**Why Balancing Classes is Over-Hyped**](https://medium.com/towards-data-science/why-balancing-classes-is-over-hyped-e382a8a410f7)'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '[**为什么平衡类别被过分炒作**](https://medium.com/towards-data-science/why-balancing-classes-is-over-hyped-e382a8a410f7)'
