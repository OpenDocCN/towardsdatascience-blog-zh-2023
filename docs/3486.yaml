- en: One Step to Make Decision Trees Produce Better Results
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/one-step-to-make-decision-trees-produce-better-results-b0ccd6738200?source=collection_archive---------10-----------------------#2023-11-23](https://towardsdatascience.com/one-step-to-make-decision-trees-produce-better-results-b0ccd6738200?source=collection_archive---------10-----------------------#2023-11-23)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Background, implementation, and model improvement
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://gabeverzino.medium.com/?source=post_page-----b0ccd6738200--------------------------------)[![Gabe
    Verzino](../Images/36452afec54430c55594a26247136f6f.png)](https://gabeverzino.medium.com/?source=post_page-----b0ccd6738200--------------------------------)[](https://towardsdatascience.com/?source=post_page-----b0ccd6738200--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----b0ccd6738200--------------------------------)
    [Gabe Verzino](https://gabeverzino.medium.com/?source=post_page-----b0ccd6738200--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fb4abbbfdcbbb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fone-step-to-make-decision-trees-produce-better-results-b0ccd6738200&user=Gabe+Verzino&userId=b4abbbfdcbbb&source=post_page-b4abbbfdcbbb----b0ccd6738200---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----b0ccd6738200--------------------------------)
    ·7 min read·Nov 23, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fb0ccd6738200&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fone-step-to-make-decision-trees-produce-better-results-b0ccd6738200&user=Gabe+Verzino&userId=b4abbbfdcbbb&source=-----b0ccd6738200---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb0ccd6738200&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fone-step-to-make-decision-trees-produce-better-results-b0ccd6738200&source=-----b0ccd6738200---------------------bookmark_footer-----------)![](../Images/a66ae08a47ffbe5c590a3f659448b239.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Among the trees (photo by author)
  prefs: []
  type: TYPE_NORMAL
- en: Decision trees (DT) get ditched much too soon.
  prefs: []
  type: TYPE_NORMAL
- en: 'It happens like this:'
  prefs: []
  type: TYPE_NORMAL
- en: The DT is trained. Natural overfitting presents. Hyper-parameters get tuned
    (unsatisfactorily). Finally, the tree is replaced with Random Forest.
  prefs: []
  type: TYPE_NORMAL
- en: While that may be a quick win for performance, the replacement prioritizes a
    “black box” algorithm. That’s not ideal. Only a DT can produce intuitive results,
    offer business leaders the ability to compare trade-offs, and gives them a critical
    role in process improvement.
  prefs: []
  type: TYPE_NORMAL
- en: What you can’t understand or even explain, doesn’t make it to production. This
    is especially true in industries where even small failures present extreme risks,
    like healthcare.
  prefs: []
  type: TYPE_NORMAL
- en: '*(Side note: People often ask “Random Forests produce feature importances,
    doesn’t that explain what features are important?” Not really. Feature importances
    almost immediately get interpreted as* ***causal*** *drivers (e.g., features that
    have dependency to target), but they’re nothing more than* ***model*** *drivers.
    While helpful to the technician in that respect only, feature importances are
    generally: (1) useless in weak models (2) inflate in features with high cardinality,
    and (3) bias toward correlated features. This is another line of inquiry all together,
    but that’s basically the rub.)*'
  prefs: []
  type: TYPE_NORMAL
- en: How Decision Trees make decisions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Sticking with DTs will preserve your ability to communicate results well, but
    how do you make them performant? Hyperparameter tuning only gets you so far. And
    thoughtful feature engineering should be done regardless.
  prefs: []
  type: TYPE_NORMAL
- en: As it turns out, the specific structure of feature data may allow it to adapt
    better to the underlying DT algorithm, which in turn allows the DT to produce
    better decisions.
  prefs: []
  type: TYPE_NORMAL
- en: Under the hood, DTs separate classes by creating orthogonal decision boundaries
    (perpendicular splits) between classes among all the data you supply it. It does
    this in a greedy algorithmic way — taking the features that split the best first,
    then moving to less optimal splits in other features.
  prefs: []
  type: TYPE_NORMAL
- en: We can visually inspect our features for orthogonal decision boundaries. Let’s
    view features from the publicly available breast cancer dataset below. On the
    top plot below, plotting “Worst Perimeter” and “Mean Perimeter” produces a good
    orthogonal decision boundary that can separate Malignant and Benign classes well.
    So, these would be great features for DT model inclusion.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7310102f30cae9cd1ec17e361208719b.png)'
  prefs: []
  type: TYPE_IMG
- en: Picture by author
  prefs: []
  type: TYPE_NORMAL
- en: The bottom plot above shows “Mean Area” and “Mean Perimeter” for which the DT
    made orthogonal decision boundaries (as it inherently does), but these are unnecessarily
    convoluted. Probably, a diagonal separation would have been better here, but that’s
    not how DT classifiers split. Furthermore, DTs are very sensitive to even small
    variations in training data, such as outliers, which are known to produce vastly
    different trees.
  prefs: []
  type: TYPE_NORMAL
- en: To accommodate this unique and underlying mechanism of DTs — and ultimately
    improve performance and generalizability — Principal Component Analysis (PCA)
    can be applied.
  prefs: []
  type: TYPE_NORMAL
- en: 'PCA improves DT performance in two important ways:'
  prefs: []
  type: TYPE_NORMAL
- en: (1) orients key features together (that explain the most variance)
  prefs: []
  type: TYPE_NORMAL
- en: (2) reduces the feature space
  prefs: []
  type: TYPE_NORMAL
- en: In fact, the PCA + DT process naturally surfaced the “Worst Perimeter” and “Mean
    Perimeter” features that you see above in the top plot. These are two of the most
    *predictive* variables and not surprisingly have an excellent orthogonal decision
    boundary.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing the Process
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Recall, PCA is designed for continuous data. The breast cancer dataset is entirely
    comprised of continuous variables. *(Another Side Note: I see PCA being used on
    categorical variables — not recommended. Nominal levels don’t have implied distances,
    ordinal levels aren’t always equidistant, and enforcing distance representations
    on discrete features generally reconstitutes the variable into something meaningless.
    Another tangent for another time).*'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s begin by downloading the needed packages, and transform our breast cancer
    dataset into features **X** and target variable **y**.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: For inspection, the dataframe head of this dataset can be called.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/fc4d0bfa4333ed484c878022b7399ab9.png)'
  prefs: []
  type: TYPE_IMG
- en: Picture by author
  prefs: []
  type: TYPE_NORMAL
- en: First, train the DecisionTreeClassifier without PCA, and collect those predictions
    (original_predictions).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Now, apply PCA to select the minimum number of dimensions that can explain the
    most variance in the training set. Instead of arbitrarily choosing that number
    of dimensions, the “elbow method” can be used to identify the number of dimensions
    that would explain 99% of the variance (as hard-coded below).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Based visually on where the graph makes an “elbow”, it finds that 6 PCA components
    explain 99% of the training set variance.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d3b04fc1506cca7b606ecce62b806a74.png)'
  prefs: []
  type: TYPE_IMG
- en: Picture by author
  prefs: []
  type: TYPE_NORMAL
- en: Now apply PCA to capture 6 principal components on the training set. You can
    do so using Singular Value Decomposition (SVD) which is a standard matrix factorization
    technique (a process outside the scope here). As before, train the DecisionTreeClassifier
    on the PCA-transformed training set and collect those predictions (pca_predictions).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Now we can compare our original_predictions (non-PCA-transformed) to the pca_predictions
    (PCA-transformed) to observe any relative improvement in our evaluation metrics
    of accuracy, precision and recall.
  prefs: []
  type: TYPE_NORMAL
- en: 'In comparison to the original DT trained data, when we PCA-transform the dataset
    first and then perform DT training, we get improvement across the board:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3c6d9fd23fc9c6a6528b8be8cd563bcf.png)'
  prefs: []
  type: TYPE_IMG
- en: We can plot the confusion matrices to show the relative improvement of classification
    for Malignant and Benign tumors between the two DTs.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/a215acbd7cea6ae1dbf632f4d7c1a012.png)'
  prefs: []
  type: TYPE_IMG
- en: Picture by author
  prefs: []
  type: TYPE_NORMAL
- en: Finally, it‘s valuable to identify which of our original features are being
    used to generate the 6 principal components. Technically, PCA generates new features
    that are linear combinations of the original features. These new features are
    orthogonal to each other and are ranked in order of the variance they explain.
    However, calling *components_attribute* can identify the features *used* in the
    creation of those components.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'And thus, for the 6 principal components that we selected, the model created
    those using the following 5 features:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7191d4b53e502acac5824a68b4f36050.png)'
  prefs: []
  type: TYPE_IMG
- en: Picture by author
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Decision Trees get ditched too soon in lieu of more performant algorithms. While
    highest performance is important, it may not be best — that decision ultimately
    depends on your stakeholder needs and explaining why a model is suggesting a particular
    outcome (see “[explainable AI](https://www.ibm.com/topics/explainable-ai)”).
  prefs: []
  type: TYPE_NORMAL
- en: Instead of reaching for the most technically advanced algorithm, work to optimally
    prep your data through thoughtful feature engineering and Principal Component
    Analysis to give decision trees their best chance of revealing their intuitive
    decision-making capabilities.
  prefs: []
  type: TYPE_NORMAL
- en: Thanks for reading. Happy to connect with anyone on [LinkedIn](https://www.linkedin.com/in/gabe-verzino-71401137/)!
    If you would like to share any interesting data science challenges that you are
    facing currently, please leave a comment or DM and I’ll be happy to try and explore/write
    about it.
  prefs: []
  type: TYPE_NORMAL
- en: 'My most recent articles:'
  prefs: []
  type: TYPE_NORMAL
- en: '[**Debugging Logistic Regression Errors — What they mean, how to do**](https://medium.com/towards-data-science/how-to-fix-errors-in-logistic-regression-32b8dd9fe6d7)'
  prefs: []
  type: TYPE_NORMAL
- en: '[**Using Bayesian Networks to Forecast Ancillary Service Volume in Hospitals**](https://medium.com/towards-data-science/using-bayesian-networks-to-forecast-ancillary-service-volume-in-hospitals-48968a978cb5)'
  prefs: []
  type: TYPE_NORMAL
- en: '[**Why Balancing Classes is Over-Hyped**](https://medium.com/towards-data-science/why-balancing-classes-is-over-hyped-e382a8a410f7)'
  prefs: []
  type: TYPE_NORMAL
