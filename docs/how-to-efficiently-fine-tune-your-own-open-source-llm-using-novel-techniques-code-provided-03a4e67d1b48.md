# ä½¿ç”¨æœ€æ–°æŠ€æœ¯å¾®è°ƒä½ çš„å¼€æº LLM

> åŸæ–‡ï¼š[`towardsdatascience.com/how-to-efficiently-fine-tune-your-own-open-source-llm-using-novel-techniques-code-provided-03a4e67d1b48`](https://towardsdatascience.com/how-to-efficiently-fine-tune-your-own-open-source-llm-using-novel-techniques-code-provided-03a4e67d1b48)

## åœ¨è¿™ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘è°ƒæ•´äº†ä¸€ä¸ªåŸºç¡€çš„ LLama2 LLMï¼Œä»¥è¾“å‡º SQL ä»£ç ã€‚æˆ‘ä½¿ç”¨äº†å‚æ•°é«˜æ•ˆå¾®è°ƒæŠ€æœ¯æ¥ä¼˜åŒ–è¿™ä¸€è¿‡ç¨‹ã€‚

[](https://medium.com/@christopher_karg?source=post_page-----03a4e67d1b48--------------------------------)![Christopher Karg](https://medium.com/@christopher_karg?source=post_page-----03a4e67d1b48--------------------------------)[](https://towardsdatascience.com/?source=post_page-----03a4e67d1b48--------------------------------)![æ•°æ®ç§‘å­¦å‰æ²¿](https://towardsdatascience.com/?source=post_page-----03a4e67d1b48--------------------------------) [Christopher Karg](https://medium.com/@christopher_karg?source=post_page-----03a4e67d1b48--------------------------------)

Â·å‘è¡¨äº[æ•°æ®ç§‘å­¦å‰æ²¿](https://towardsdatascience.com/?source=post_page-----03a4e67d1b48--------------------------------) Â·13 åˆ†é’Ÿé˜…è¯»Â·2023 å¹´ 12 æœˆ 15 æ—¥

--

![](img/389987bb764f9aceba43b821ecf0128e.png)

æ¥æº: [`www.pexels.com/photo/calm-body-of-lake-between-mountains-346529/`](https://www.pexels.com/photo/calm-body-of-lake-between-mountains-346529/)

åœ¨[ä¸Šä¸€ç¯‡æ–‡ç« ](https://medium.com/towards-data-science/quantisation-and-co-reducing-inference-times-on-llms-by-80-671db9349bdb)ä¸­ï¼Œæˆ‘å¼€å§‹é˜è¿°ä¸ºä½•ä½ åº”è¯¥è€ƒè™‘è®­ç»ƒè‡ªå·±çš„ LLMã€‚æˆ‘è¿˜ç®€è¦ä»‹ç»äº†ç¡¬ä»¶è¦æ±‚ä»¥åŠä¼˜åŒ–è®­ç»ƒå’Œæ¨ç†çš„æ–¹æ³•ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘å°†è¯¦ç»†ä»‹ç»å¦‚ä½•å¾®è°ƒä¸€ä¸ªå¼€æº LLMï¼Œå¹¶æä¾›ä»£ç ç‰‡æ®µä¾›ä½ è·Ÿéšå’Œé‡ç°ç»“æœã€‚æˆ‘ä»¬å°†è°ƒæ•´ä¸€ä¸ª Llama2â€“7B æ¨¡å‹ï¼Œä½¿å…¶æ ¹æ®è‡ªç„¶è¯­è¨€è¾“å…¥æä¾› SQL è¾“å‡ºâ€”â€”æ¢å¥è¯è¯´ï¼Œæ¨¡å‹å°†æŠŠæˆ‘ä»¬ç”¨è‡ªç„¶è¯­è¨€æå‡ºçš„é—®é¢˜è½¬æ¢ä¸º SQL æŸ¥è¯¢ï¼š

*â€œåœ¨ 11 æœˆä»½ï¼Œæœ‰å¤šå°‘å®¢æˆ·å†³å®šè´­ä¹°é¸¡è›‹ï¼Ÿâ€*

åˆ°ä¸€ä¸ª SQL æŸ¥è¯¢ï¼Œè¯¥æŸ¥è¯¢è·å–ç›¸åº”çš„ç»“æœï¼š

```py
SELECT COUNT(DISTINCT customer_id) AS num_customers
FROM purchases
WHERE product_name = 'eggs'
AND EXTRACT(MONTH FROM purchase_date) = 11;
```

åœ¨æ¯ç§æƒ…å†µä¸‹ï¼Œæ•°æ®åº“ï¼ˆDBï¼‰çš„æ¶æ„å°†ä½œä¸º LLM çš„å·¥ä½œä¸Šä¸‹æ–‡æä¾›ï¼š

```py
CREATE TABLE purchases (
    purchase_id INT PRIMARY KEY,
    customer_id INT,
    product_name VARCHAR(255),
    purchase_date DATE
);
```

æˆ‘ä»¬å°†åœ¨è°ƒæ•´è¿‡ç¨‹ä¸­ä½¿ç”¨[è¿™ä¸ªæ•°æ®é›†](https://huggingface.co/datasets/knowrohit07/know_sql)ã€‚è™½ç„¶è¿™ç¯‡æ–‡ç« ä¸»è¦é›†ä¸­åœ¨å®ç°ä¸Šè¿°ä»»åŠ¡ä¸Šï¼Œä½†æ–¹æ³•å°†ä»¥ä¸€ç§ä½ å¯ä»¥è°ƒæ•´ä»¥æ»¡è¶³ä½ è¦æ±‚çš„æ–¹å¼æä¾›ã€‚

åœ¨è¿™ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘å°†ä½¿ç”¨[Google Colab](https://colab.google/)æ¥å¾®è°ƒ LLMã€‚æˆ‘ä»¬å°†ä½¿ç”¨ä¹‹å‰æåˆ°çš„[know_sql æ•°æ®é›†](https://huggingface.co/datasets/knowrohit07/know_sql)ï¼ˆOpenRAIL è®¸å¯è¯ï¼‰ã€‚æˆ‘ä»¬è¿˜å°†ä½¿ç”¨[axolotl æ¡†æ¶](https://github.com/OpenAccess-AI-Collective/axolotl)æ¥å¤„ç†å¾®è°ƒè¿‡ç¨‹ã€‚ä»–ä»¬åœ¨ GitHub é¡µé¢ä¸Šæœ‰ä¸€äº›å¾ˆå¥½çš„æ–‡æ¡£ã€‚ä¸å…¶ç¼–å†™çº¦ 100 è¡Œä»£ç æ¥æ‰‹åŠ¨å¤„ç†å¾®è°ƒè¿‡ç¨‹ï¼Œaxolotl å…è®¸æˆ‘ä»¬ç®€å•åœ°ç¼–è¾‘ç›¸åº”æ¨¡å‹çš„ YAML é…ç½®æ–‡ä»¶ã€‚æˆ‘å°†åœ¨æœ¬æ–‡ä¸­è¯¦ç»†ä»‹ç»å…·ä½“è¿‡ç¨‹ï¼Œä½†å¦‚æœæœ‰ä»»ä½•ä¸æ¸…æ¥šçš„åœ°æ–¹ï¼Œå»ºè®®æŸ¥çœ‹ axolotl æ–‡æ¡£ã€‚

å¾®è°ƒ LLM çš„è¿‡ç¨‹æ¯”ä»…åœ¨ç°æœ‰ LLM ä¸Šè¿è¡Œæ¨ç†è¦è®¡ç®—å¯†é›†å¾—å¤šï¼ˆå†æ­¤æä¾›[é“¾æ¥](https://medium.com/towards-data-science/quantisation-and-co-reducing-inference-times-on-llms-by-80-671db9349bdb)åˆ°æˆ‘çš„ä¸Šä¸€ç¯‡æ–‡ç« ï¼Œæ¶µç›–äº†è¿è¡Œæ¨ç†å’Œä¼˜åŒ–ç°æœ‰ LLM çš„å„ä¸ªé˜¶æ®µï¼‰ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æ— æ³•ä½¿ç”¨ Google Colab çš„å…è´¹ç‰ˆï¼Œå› ä¸ºä»»åŠ¡éœ€è¦[NVIDIA çš„ Ampere GPU æ¶æ„](https://www.nvidia.com/en-us/data-center/ampere-architecture/)ï¼ˆæˆ–æ›´é«˜ç‰ˆæœ¬ï¼‰ã€‚æ­¤æ¶æ„å¯é€šè¿‡ Google Colab ä¸Šçš„â€œA100 GPUâ€è¿è¡Œæ—¶ç±»å‹è·å¾—ã€‚æœ‰å…³ä¸è¿è¡Œæ—¶ç±»å‹äº¤äº’çš„æ›´å¤šä¿¡æ¯ï¼Œæˆ‘å»ºè®®æŸ¥çœ‹æˆ‘çš„[ä¸Šä¸€ç¯‡æ–‡ç« ](https://medium.com/towards-data-science/quantisation-and-co-reducing-inference-times-on-llms-by-80-671db9349bdb)ã€‚

åœ¨è¿™ä¸ªé˜¶æ®µï¼Œæˆ‘åº”è¯¥æåˆ° Google æä¾› A100 GPU æ˜¯â€œå…ˆåˆ°å…ˆå¾—â€æˆ–â€œè§†æƒ…å†µè€Œå®šâ€ã€‚æˆ‘å‘ç°æœ‰æ—¶è·å– A100 GPU çš„è®¿é—®æƒé™æ¯”è¾ƒéº»çƒ¦ï¼Œå› ä¸ºå®ƒä»¬ä¸å¸¸æœ‰ã€‚å¦‚æœä¸å¯ç”¨ï¼Œä½ å°†è‡ªåŠ¨åˆ‡æ¢åˆ°ä¸€ä¸ªè¾ƒæ—§çš„æ¶æ„ï¼Œè¿™ä¸è¶³ä»¥å®Œæˆè¿™ä¸ªä»»åŠ¡ã€‚ä½ çš„ä½“éªŒå¯èƒ½ä¼šæœ‰æ‰€ä¸åŒã€‚å¦‚æœä½ ä¸æƒ³ç­‰å¾…æ­£ç¡®çš„ GPU å˜å¾—å¯ç”¨ï¼Œæˆ‘å»ºè®®ä½ æŸ¥çœ‹æˆ‘åœ¨[ä¸Šä¸€ç¯‡æ–‡ç« ](https://medium.com/towards-data-science/quantisation-and-co-reducing-inference-times-on-llms-by-80-671db9349bdb)ä¸­æåˆ°çš„ä¸€äº›ç¡¬ä»¶å»ºè®®ã€‚

å·¥ä½œæµç¨‹å¦‚ä¸‹ï¼š

Â· è®¾ç½® Google Colab å¹¶æŒ‚è½½ GDriveï¼ˆæˆ‘ä»¬éœ€è¦æŒ‚è½½ GDriveï¼Œå› ä¸ºéœ€è¦ä» Colab ç¬”è®°æœ¬ä¸­è®¿é—®æŸäº›æ–‡ä»¶ï¼‰ã€‚

Â· å®‰è£…ä¾èµ–é¡¹

Â· é€šè¿‡ HuggingFace CLI è¿›è¡Œèº«ä»½éªŒè¯ï¼ˆå¦‚æœä½ ä»æœªè·å¾— Meta æˆæƒä½¿ç”¨ Llama-2 LLMsï¼Œæˆ‘å†æ¬¡å»ºè®®ä½ æŸ¥çœ‹æˆ‘çš„[ä¸Šä¸€ç¯‡æ–‡ç« ](https://medium.com/towards-data-science/quantisation-and-co-reducing-inference-times-on-llms-by-80-671db9349bdb)ï¼Œè¯¥æ–‡ç« æ¶µç›–äº†è·å–è®¿é—®æƒé™çš„è¿‡ç¨‹ï¼‰ã€‚

Â· åŠ è½½æ•°æ®é›†

Â· å°† axolotl ä»“åº“å…‹éš†åˆ° Gdrive

Â· æ›´æ–° axolotl é…ç½®æ–‡ä»¶

Â· å¾®è°ƒ Llama-7B æ¨¡å‹ï¼ˆè¿™å¤§çº¦éœ€è¦ 2 å°æ—¶ã€‚ä½ çš„ä½“éªŒå¯èƒ½ä¼šæœ‰æ‰€ä¸åŒï¼‰ã€‚

Â· åœ¨å¾®è°ƒåçš„æ¨¡å‹ä¸Šè¿è¡Œæ¨ç†

åœ¨è¿›å…¥ä»£ç ä¹‹å‰ï¼Œæˆ‘å¸Œæœ›æ¾„æ¸…å¾®è°ƒåˆ°åº•æ˜¯ä»€ä¹ˆï¼Œä»¥åŠä½ å¯èƒ½ä½•æ—¶è€ƒè™‘å¾®è°ƒä½ çš„ LLMã€‚

å¾®è°ƒå¯ä»¥é€šè¿‡å¤šç§æ–¹å¼è¿›è¡Œã€‚ä½ é€‰æ‹©çš„æ–¹æ³•å–å†³äºä½ å¯ç”¨çš„èµ„æºå’Œä½ æƒ³è¦å®ç°çš„ä»»åŠ¡ã€‚å¯èƒ½â€˜åŸºç¡€â€™æ¨¡å‹å·²ç»èƒ½äº§ç”Ÿä»¤äººæ»¡æ„çš„ç»“æœï¼Œè¿™ç§æƒ…å†µä¸‹ä½ å¯ä»¥å®Œå…¨è·³è¿‡å¾®è°ƒè¿‡ç¨‹ã€‚

å†æ¬¡ï¼Œæˆ‘çš„ [ä¸Šä¸€ç¯‡æ–‡ç« ](https://medium.com/towards-data-science/quantisation-and-co-reducing-inference-times-on-llms-by-80-671db9349bdb) è®¨è®ºäº†ä¸ºä»€ä¹ˆä½ å¯èƒ½ä¼šé€‰æ‹©ä½¿ç”¨å¼€æºçš„ LLMï¼Œè€Œä¸æ˜¯åƒ OpenAI çš„ ChatGPT è¿™æ ·çš„äº§å“ã€‚è€ƒè™‘æ˜¯å¦åº”å¾®è°ƒ LLM çš„ä¸€ä¸ªé€šç”¨åŸåˆ™å¦‚ä¸‹ï¼š

Â· æˆ‘çš„ä»»åŠ¡æ˜¯å¦éœ€è¦é¢†åŸŸç‰¹å®šçš„çŸ¥è¯†/ä¸“ä¸šæŠ€èƒ½ï¼Ÿ

Â· æˆ‘æ˜¯å¦ä¼šå‘æˆ‘çš„ LLM è¾“å…¥ä¸“æœ‰æ•°æ®ï¼Ÿ

Â· æˆ‘æ˜¯å¦æœ‰è¶³å¤Ÿçš„é¢†åŸŸç‰¹å®šæ•°æ®æ¥è®­ç»ƒ/è°ƒæ•´æˆ‘çš„æ¨¡å‹ï¼Ÿ

Â· æˆ‘æ˜¯å¦æœ‰è¶³å¤Ÿçš„æ—¶é—´å’Œè®¡ç®—èµ„æºæ¥å®Œæˆä»»åŠ¡ï¼Ÿ

å¦‚æœä½ å¯¹ä¸€ä¸ªæˆ–å¤šä¸ªä»¥ä¸Šçš„é—®é¢˜å›ç­”æ˜¯ï¼Œä½ åº”è¯¥è€ƒè™‘å¾®è°ƒ LLMã€‚å¦‚æœä½ å¯¹æ‰€æœ‰é—®é¢˜éƒ½å›ç­”æ˜¯ï¼Œå¯èƒ½å€¼å¾—ä»å¤´å¼€å§‹è®­ç»ƒä¸€ä¸ª LLMã€‚å¯ä»¥é‡‡ç”¨é€æ­¥çš„æ–¹æ³•ï¼šåˆå§‹ä½¿ç”¨ä¸€ä¸ªâ€˜åŸºç¡€â€™ LLMï¼Œç„¶åå°è¯•ä¸€ä¸ªå¾®è°ƒè¿‡çš„ LLMï¼Œå†å°è¯•ä¸€ä¸ªä½ ä»å¤´è®­ç»ƒçš„ LLMã€‚æ¯ä¸€æ­¥çš„æˆæœ¬ï¼ˆæ—¶é—´å’Œè®¡ç®—èµ„æºï¼‰é€æ¸å¢åŠ ï¼Œåªæœ‰å½“ä½ å½“å‰çš„ LLM æœªèƒ½äº§ç”Ÿä»¤äººæ»¡æ„çš„ç»“æœæ—¶ï¼Œæ‰ä¼šè¿›å…¥ä¸‹ä¸€æ­¥ã€‚è®­ç»ƒ LLM ä»é›¶å¼€å§‹çš„è¿‡ç¨‹è¶…å‡ºäº†æœ¬æ–‡çš„èŒƒå›´ï¼Œä½†æˆ‘å¾ˆä¹æ„å›ç­”ä½ å…³äºè¿™ä¸ªè¯é¢˜çš„ä»»ä½•é—®é¢˜ã€‚

ç°åœ¨æˆ‘ä»¬å·²ç»å†³å®šæ˜¯å¦å¾®è°ƒæˆ‘ä»¬çš„ LLMï¼Œæˆ‘å°†è¯¦ç»†ä»‹ç»è¿™ä¸ªè¿‡ç¨‹å®é™…ä¸ŠåŒ…æ‹¬ä»€ä¹ˆã€‚

æˆ‘ä»¬çš„â€˜åŸºç¡€â€™ LLM æ˜¯ä¸€ä¸ªé¢„è®­ç»ƒæ¨¡å‹ï¼Œæ‹¥æœ‰æ¥è‡ªåˆå§‹è®­ç»ƒçš„ä¸€ç»„æƒé‡å’Œåç½®ã€‚è¿™äº›æƒé‡å’Œåç½®ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿâ€˜å­¦ä¹ â€™åˆ°å¹¿æ³›çš„åŸºæœ¬è¯­è¨€æ¨¡å¼å’Œä¸€èˆ¬çŸ¥è¯†ã€‚åœ¨å¾®è°ƒè¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨æ ‡è®°çš„æ•°æ®é›†æ¥æ›´æ–°è¿™äº›æƒé‡å’Œåç½®ã€‚

ä¹‹å‰ï¼Œå…¸å‹çš„å¾®è°ƒè¿‡ç¨‹å¦‚ä¸‹ï¼š

**å‰å‘ä¼ æ’­**

Â· æ¨¡å‹å°è¯•åŸºäºè¾“å…¥æ•°æ®è¿›è¡Œé¢„æµ‹

**æŸå¤±è®¡ç®—** Â· æ¨¡å‹è®¡ç®—é¢„æµ‹ç»“æœçš„åå·®ã€‚

**åå‘ä¼ æ’­** Â· æ¨¡å‹åŸºæœ¬ä¸Šæ˜¯åœ¨å¼„æ¸…æ¥šæ¯ä¸ªå‚æ•°å¯¹æŸå¤±çš„è´¡çŒ®ç¨‹åº¦ã€‚

**æ›´æ–°å‚æ•°** Â· åœ¨è¿™ä¸€æ­¥ï¼Œå‚æ•°å°†è¢«æ›´æ–°å’Œè°ƒæ•´ä»¥å‡å°‘æŸå¤±ã€‚æ›´æ–°çš„å¤§å°ï¼ˆå­¦ä¹ ç‡ï¼‰æ˜¯ä¸€ä¸ªå¯ä»¥è°ƒæ•´çš„è¶…å‚æ•°ã€‚

**é‡å¤** Â· ä¸Šè¿°æ­¥éª¤ä¼šé‡å¤è¿›è¡Œä¸€å®šæ¬¡æ•°ï¼ˆè®­ç»ƒè½®æ¬¡ï¼‰ã€‚é€šå¸¸è¿™ä¸ªæ¬¡æ•°éå¸¸å°‘ã€‚åƒ BERT è¿™æ ·çš„å¸¸è§ LLM åœ¨ç»è¿‡ 2 ä¸ªè½®æ¬¡çš„å¾®è°ƒåè¡¨ç°æœ€ä½³ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å°†ä»…ä½¿ç”¨ 1 ä¸ªè½®æ¬¡æ¥å¾®è°ƒ Llama2â€“7B æ¨¡å‹ã€‚

éšç€éå¸¸å¤§å‹ LLM çš„å‡ºç°ï¼Œå·²ç»ç¡®å®šäº†ä½¿è¿™ä¸€è¿‡ç¨‹æ›´åŠ é«˜æ•ˆçš„æ–°ç­–ç•¥ã€‚å¼•å…¥ LoRAï¼ˆä½ç§©é€‚é…ï¼‰å’Œ QLoRAï¼ˆé‡åŒ– LoRAï¼‰ã€‚æˆ‘ä»¬å°†åœ¨åç»­çš„ SQL ç¤ºä¾‹ä¸­ä½¿ç”¨ LoRAï¼Œæ‰€ä»¥æˆ‘å°†é‡ç‚¹ä»‹ç»å®ƒã€‚è¿™ä¸¤ç§ç­–ç•¥å¯ä»¥å½’å…¥å‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆPEFTï¼‰çš„èŒƒç•´ã€‚è¿™äº›æŠ€æœ¯ä½¿æˆ‘ä»¬èƒ½å¤ŸæˆåŠŸåœ°å¾®è°ƒéå¸¸å¤§çš„è¯­è¨€æ¨¡å‹ï¼ŒåŒæ—¶æœ€å°åŒ–æ‰§è¡Œæ‰€éœ€çš„è®¡ç®—è¦æ±‚ã€‚å®ƒé€šè¿‡å‡å°‘å¾®è°ƒè¿‡ç¨‹ä¸­çš„å¯è®­ç»ƒå‚æ•°æ•°é‡æ¥å®ç°è¿™ä¸€ç‚¹ã€‚

> å°½ç®¡å‡å°‘å†…å­˜ä½¿ç”¨ã€å­˜å‚¨æˆæœ¬å’Œæ¨ç†å»¶è¿Ÿæ˜¯ä½¿ç”¨ PEFT æ–¹æ³•çš„å¥½å¤„ï¼Œä½†éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œåœ¨æ­¤è¿‡ç¨‹ä¸­è®­ç»ƒæ—¶é—´å¯èƒ½ä¼šå¢åŠ ï¼Œæ€§èƒ½å¯èƒ½å¯¹è¶…å‚æ•°é€‰æ‹©æ›´åŠ æ•æ„Ÿã€‚

æˆ‘ä»¬å¯ä»¥ä½¿ç”¨å„ç§ PEFT æŠ€æœ¯ã€‚æœ‰å…³è¿™äº›æŠ€æœ¯çš„æ›´å¤šé˜…è¯»ï¼Œæˆ‘å¼ºçƒˆæ¨è[è¿™ç¯‡åšå®¢æ–‡ç« ](https://www.leewayhertz.com/parameter-efficient-fine-tuning/)ã€‚

ä½¿ç”¨ LoRA æ—¶ï¼Œå‡è®¾ç°æœ‰æ¨¡å‹çš„æƒé‡æ¥è¿‘æœ€ä¼˜è§£ã€‚è¿™äº›æƒé‡éšåè¢«å›ºå®šï¼Œå› æ­¤åœ¨å¾®è°ƒè¿‡ç¨‹ä¸­ä¸èƒ½å†æ›´æ–°ã€‚ç›¸åï¼Œä½ç§©çŸ©é˜µè¢«å¼•å…¥åˆ°æˆ‘ä»¬çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¯ä¸€å±‚ä¸­ã€‚è¿™äº›çŸ©é˜µä½œä¸ºé€‚é…å™¨ï¼Œæ›¿ä»£äº†æˆ‘ä»¬å†»ç»“çš„å¯è®­ç»ƒå‚æ•°ã€‚æ›´æ–°è¿™äº›ä½ç§©çŸ©é˜µçš„è®¡ç®—è¦æ±‚æ˜¾è‘—ä½äºå¦‚æœæˆ‘ä»¬ä»ä½¿ç”¨åŸå§‹å‚æ•°æ—¶çš„è®¡ç®—è¦æ±‚ã€‚æ¢å¥è¯è¯´ï¼Œç”±äºåŸå§‹å‚æ•°ä¸­çš„å†—ä½™ä»¥åŠåŸå§‹å‚æ•°ä¸ä½ç§©çŸ©é˜µä¹‹é—´çš„ç›¸å…³æ€§ï¼ŒåŸå§‹å‚æ•°å¯ä»¥è¢«ä½ç§©çŸ©é˜µå¾ˆå¥½åœ°è¿‘ä¼¼ã€‚è¿™åœ¨æ‰€è°“çš„â€˜Rank-Deficiencyâ€™ä¸­å¾—åˆ°äº†ä½“ç°â€”â€”ç›¸å…³è®ºæ–‡å¯ä»¥åœ¨[è¿™é‡Œ](https://arxiv.org/abs/2106.09685)æ‰¾åˆ°ã€‚

æ‰€æœ‰è¿™äº›ç»“æœå‡å°‘äº†å‚æ•°å¼€é”€ï¼Œä»è€Œå¸¦æ¥äº†æˆ‘ä¹‹å‰åˆ—å‡ºçš„ä¼˜åŠ¿ã€‚å€¼å¾—æ³¨æ„çš„ä¸€ç‚¹æ˜¯å‡å°‘äº†æ¨ç†å»¶è¿Ÿã€‚é€šè¿‡å°†è¿™äº›ä½ç§©çŸ©é˜µå¼•å…¥æˆ‘ä»¬çš„æ¨¡å‹ï¼Œæˆ‘ä»¬å®é™…ä¸Šåˆ›å»ºäº†ä¸€ä¸ªçº¿æ€§è®¾è®¡ã€‚ä¸å®Œå…¨å¾®è°ƒçš„æ¨¡å‹ç›¸æ¯”ï¼Œæ¨ç†å»¶è¿Ÿæ˜¾è‘—å‡å°‘ã€‚è¿™ä½¿å¾—è¯¥æ–¹æ³•é€‚ç”¨äºå®æ—¶åº”ç”¨ã€‚

æˆ‘ä»¬å¯ä»¥é€šè¿‡å¼•å…¥é‡åŒ–è¿›ä¸€æ­¥ä¼˜åŒ–æ¨ç†æ—¶é—´â€”â€”è¿™æ˜¯æˆ‘åœ¨[ä¸Šä¸€ç¯‡æ–‡ç« ](https://medium.com/towards-data-science/quantisation-and-co-reducing-inference-times-on-llms-by-80-671db9349bdb)ä¸­å¹¿æ³›è®¨è®ºçš„ä¸»é¢˜ã€‚

[Axolotl](https://github.com/OpenAccess-AI-Collective/axolotl)æä¾›äº†ä¸€ä¸ªéå¸¸æ˜“äºä½¿ç”¨çš„å·¥å…·ï¼Œä½¿æˆ‘ä»¬èƒ½å¤Ÿå®ç°ä¸Šè¿°æ‰€æœ‰åŠŸèƒ½ã€‚

è®©æˆ‘ä»¬å¼€å§‹å§ã€‚

é¦–å…ˆæˆ‘ä»¬å°†è®¾ç½®æˆ‘ä»¬çš„ [colab notebook](https://colab.google/)ã€‚ä½ ç°åœ¨éœ€è¦é€‰æ‹© A100 GPU è¿è¡Œæ—¶ç±»å‹ã€‚å¦‚æœä½ ä½¿ç”¨çš„æ˜¯å…è´¹ç‰ˆæœ¬ï¼Œä½ éœ€è¦å‡çº§åˆ°ä»˜è´¹ç‰ˆæœ¬ã€‚è¯·æ³¨æ„ï¼ŒA100 GPUs å¯èƒ½ä¸å¯ç”¨ã€‚æˆ‘å»ºè®®è¿è¡Œä¸€ä¸ªåŒ…å«â€œhello worldâ€ç¤ºä¾‹çš„å•å…ƒï¼Œä»¥ç¡®ä¿ä½ ä¸ä¼šè¢«è½¬ç§»åˆ°è¾ƒæ—§çš„ V100 GPUã€‚å¦‚æœ A100 GPU ä¸å¯ç”¨ï¼Œä½ å¯ä»¥ç­‰å¾…ä¸€ä¸ªå¯ç”¨çš„ GPUï¼ˆæˆ‘ä¸å¹¸çš„æ˜¯æ— æ³•ç¡®å®š GPU å¯ç”¨æ€§çš„é«˜å³°å’Œä½å³°æ—¶é—´ï¼‰ã€‚æˆ–è€…ä½ å¯ä»¥æŸ¥çœ‹æˆ‘[ä¸Šä¸€ç¯‡æ–‡ç« ](https://medium.com/towards-data-science/quantisation-and-co-reducing-inference-times-on-llms-by-80-671db9349bdb)ä¸­åˆ—å‡ºçš„å…¶ä»–ç¡¬ä»¶é€‰é¡¹ã€‚

æ¥ä¸‹æ¥è®©æˆ‘ä»¬æŒ‚è½½ Gdriveã€‚ä½ å¯ä»¥é€šè¿‡è¾“å…¥ä»¥ä¸‹ä»£ç æ¥å®Œæˆï¼š

```py
from google.colab import drive
drive.mount('/content/drive')
```

ç„¶åä½ ä¼šæ”¶åˆ°ä¸€ä¸ªå¼¹å‡ºçª—å£ï¼Œéœ€è¦é€šè¿‡ä½ çš„ Google è´¦æˆ·è¿›è¡Œèº«ä»½éªŒè¯ã€‚æˆ‘ä»¬æ­£åœ¨æŒ‚è½½é©±åŠ¨å™¨ï¼Œå› ä¸ºæˆ‘ä»¬éœ€è¦å¤„ç†å­˜å‚¨åœ¨ Gdrive ä¸­çš„æ–‡ä»¶ã€‚

æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°†é€šè¿‡ HuggingFace CLI è¿›è¡Œ Llama2â€“7B çš„è®¿é—®è®¤è¯ã€‚è¿™å°†æ˜¯æˆ‘ä»¬å°†è¦å¾®è°ƒçš„åŸºç¡€æ¨¡å‹ã€‚å¦‚æœä½ ä¹‹å‰ä»æœªé€šè¿‡ Meta çš„å®˜æ–¹ç½‘ç«™ç”³è¯·è¿‡è¯¥æ¨¡å‹çš„è®¿é—®æƒé™ï¼Œè¯·å‚é˜…æˆ‘ä¹‹å‰çš„æ–‡ç« ä¸­çš„è¯´æ˜ã€‚

è¦è¿›è¡Œè®¤è¯ï¼Œè¯·åœ¨ä¸€ä¸ªæ–°å•å…ƒä¸­è¾“å…¥ä»¥ä¸‹ä»£ç ï¼š

```py
!huggingface-cli login
```

ç„¶åç³»ç»Ÿä¼šè¦æ±‚ä½ è¾“å…¥è®¿é—®å¯†é’¥ï¼ˆæˆ‘åœ¨æˆ‘çš„[ä¸Šä¸€ç¯‡æ–‡ç« ](https://medium.com/towards-data-science/quantisation-and-co-reducing-inference-times-on-llms-by-80-671db9349bdb)ä¸­è§£é‡Šäº†å¦‚ä½•è·å–æ­¤å¯†é’¥ï¼‰ã€‚

æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°†å¤„ç†æˆ‘ä»¬çš„å®‰è£…ã€‚æˆ‘å‘ç°è¿™é€šå¸¸æ˜¯è¿è¡Œ LLM å®éªŒæ—¶æœ€å¤§çš„éšœç¢ã€‚æŸäº›åŒ…æ˜¯ä¸ºç‰¹å®šç¡¬ä»¶æ„å»ºçš„ï¼Œå¦‚æœä½ æ²¡æœ‰è¯¥ç‰¹å®šç¡¬ä»¶ï¼Œå¯èƒ½ä¼šå¾ˆéº»çƒ¦åœ°å¤„ç†å®‰è£…å’Œæ›´æ–°ã€‚å¹¸è¿çš„æ˜¯ï¼Œaxolotl é”™è¯¯è¾“å‡ºéå¸¸è¯¦ç»†ï¼Œä½ åº”è¯¥èƒ½å¤Ÿç›¸å¯¹å®¹æ˜“åœ°è·Ÿè¸ªä»»ä½•å…¼å®¹æ€§é—®é¢˜ã€‚å¦‚æœä½ æŒ‰ç…§ axolotl readme ä¸­çš„è¯´æ˜æ“ä½œï¼Œä½ åº”è¯¥å¯ä»¥é¡ºåˆ©è¿›è¡Œã€‚å¦‚æœä½ é‡åˆ°å›°éš¾ï¼Œè¯·å‘Šè¯‰æˆ‘ã€‚

è®©æˆ‘ä»¬è¿è¡Œå®‰è£…ï¼Œæˆ‘ä»¬å°†å®‰è£…æ­£ç¡®ç‰ˆæœ¬çš„ CUDA é©±åŠ¨ç¨‹åºä»¥åŠ axolotl åº“çš„ä¾èµ–é¡¹ã€‚æˆ‘ä»¬è¿˜å°†å®‰è£… HuggingFace çš„æ•°æ®é›†åº“ï¼Œä»¥è®¿é—®æˆ‘ä»¬çš„ SQL è®­ç»ƒé›†ï¼š

```py
!pip install torch torchvision torchaudio
!pip install "axolotl[flash-attn,deepspeed] @ git+https://github.com/OpenAccess-AI-Collective/axolotl"
!pip install accelerate
!pip install datasets
!pip install torch==2.0.1+cu118 -f https://download.pytorch.org/whl/torch_stable.html
!pip uninstall xformers
!pip install xformers
!pip uninstall flash_attn -y
!pip install flash_attn
```

æ¥ä¸‹æ¥æˆ‘ä»¬å°†æŠŠ axolotl ä»“åº“å…‹éš†åˆ°æˆ‘ä»¬çš„ Gdriveã€‚å¦‚æœå‡ºç°ç›®å½•ä¸å­˜åœ¨çš„é”™è¯¯ï¼Œåªéœ€ä½¿ç”¨ Gdrive UI åˆ›å»ºè¿™äº›ç›®å½•å³å¯ï¼š

```py
!git clone https://github.com/OpenAccess-AI-Collective/axolotl /content/drive/MyDrive/fine_tune_llm/axolotl
```

è¿è¡Œä¸€äº›è¿›ä¸€æ­¥çš„å®‰è£…ï¼š

```py
!cd /content/drive/MyDrive/fine_tune_llm/axolotl && pip install packaging && pip install -e '.[flash-attn,deepspeed]'
```

æ¥ä¸‹æ¥æˆ‘ä»¬å°†è·å–æ•°æ®é›†å¹¶æ£€æŸ¥å…¶æ˜¯å¦æ­£ç¡®åŠ è½½ï¼š

```py
ds = datasets.load_dataset('knowrohit07/know_sql')
ds
trn = ds['validation']
trn[4500]

###output###
# {'answer': 'SELECT MAX(field_goals) FROM table_19722233_5 WHERE assists = 27',
# 'question': 'what is the highest number of goals did the player with 27 asists score',
# 'context': 'CREATE TABLE table_19722233_5 (field_goals INTEGER, assists VARCHAR)'}
```

å¾ˆå¥½ï¼Œç›®å‰æˆ‘ä»¬å·²ç»å®‰è£…äº†æ‰€æœ‰ä¾èµ–é¡¹ï¼Œå…‹éš†äº† axolotl ä»“åº“ï¼Œå¹¶è·å¾—äº†è®¿é—® Llama-2 7B æ¨¡å‹çš„æˆæƒã€‚åœ¨è¿™ä¸ªé˜¶æ®µï¼Œæˆ‘ä»¬éœ€è¦è¿›ä¸€æ­¥é…ç½®ï¼Œæ›´æ–° YAML æ–‡ä»¶ã€‚Axolotl ä½¿ç”¨è¿™ä¸ª YAML æ–‡ä»¶ä½œä¸ºå¾®è°ƒæ¨¡å‹çš„æŒ‡ä»¤é›†ã€‚æˆ‘å»ºè®®æŸ¥çœ‹ä¸€äº› Axolotl æä¾›çš„ç¤ºä¾‹ï¼Œè¿™äº›ç¤ºä¾‹ä½äºå…‹éš†ä»“åº“æ—¶åˆ›å»ºçš„ axolotl ç›®å½•ä¸­ã€‚è¿™å°†å¸®åŠ©ä½ äº†è§£å¯ä»¥æ›´æ”¹å“ªäº›è®¾ç½®ä»¥åŠå¯ä»¥ä½¿ç”¨å“ªäº›è¶…å‚æ•°ã€‚åŒæ ·ï¼Œä»“åº“çš„ readme å¯¹è¿™é‡Œéå¸¸æœ‰å¸®åŠ©ã€‚

ä»¥ä¸‹æ˜¯æˆ‘æœ€ç»ˆçš„ YAML é…ç½®æ–‡ä»¶å‰¯æœ¬ï¼š

```py
base_model: meta-llama/Llama-2-7b-hf
base_model_config: meta-llama/Llama-2-7b-hf
model_type: LlamaForCausalLM
tokenizer_type: LlamaTokenizer
is_llama_derived_model: true

load_in_8bit: false
load_in_4bit: true
strict: false

datasets:
  - path: knowrohit07/know_sql
    type: context_qa.load_v2
    train_on_split: validation
dataset_prepared_path: last_run_prepared
val_set_size: 0.01
output_dir: ./qlora-out

adapter: qlora
lora_model_dir:

sequence_len: 2048
sample_packing: false
pad_to_sequence_len: true

lora_r: 32
lora_alpha: 16
lora_dropout: 0.05
lora_target_modules:
lora_target_linear: true
lora_fan_in_fan_out:

wandb_project:
wandb_entity:
wandb_watch:
wandb_run_id:
wandb_log_model:

gradient_accumulation_steps: 4
micro_batch_size: 2
num_epochs: 1
optimizer: paged_adamw_32bit
lr_scheduler: cosine
learning_rate: 0.0002

train_on_inputs: false
group_by_length: false
bf16: false
fp16: true
tf32: false

gradient_checkpointing: true
early_stopping_patience:
resume_from_checkpoint:
local_rank:
logging_steps: 1
xformers_attention:
flash_attention: true

warmup_steps: 10
eval_steps: 20
eval_table_size: 5
save_steps:
debug:
deepspeed:
weight_decay: 0.0
fsdp:
fsdp_config:
special_tokens:
  bos_token: "<s>"
  eos_token: "</s>"
  unk_token: "<unk>"
```

ä½ ä¼šæ³¨æ„åˆ°æˆ‘ä»¬å¯ä»¥æ›´æ–°ä¹‹å‰æ–‡ç« ä¸­è®¨è®ºçš„å‚æ•°ï¼ˆfp16: trueï¼‰ï¼Œä»¥ä¼˜åŒ–è®­ç»ƒè¿‡ç¨‹å¹¶é™åˆ¶è¯¥æ¨¡å‹å¾®è°ƒæ‰€éœ€çš„å†…å­˜ã€‚å¦‚æœä½ ä½¿ç”¨çš„æ˜¯ä¸åŒçš„ç¡¬ä»¶ï¼Œå»ºè®®æŸ¥çœ‹æ–‡æ¡£å’Œåˆå§‹åŒ–å¾®è°ƒè¿‡ç¨‹æ—¶çš„ä»»ä½•é”™è¯¯ä¿¡æ¯ã€‚æ ¹æ®ä½ çš„ç›®æ ‡ï¼Œaxolotl æä¾›äº†è®¸å¤šç¤ºä¾‹é…ç½®æ–‡ä»¶ä¾›ä½ ä½¿ç”¨å’Œè°ƒæ•´ã€‚

å°†é…ç½®æ–‡ä»¶ä¿å­˜åˆ°ä¹‹å‰åˆ›å»ºçš„ fine_tune_llm ç›®å½•ä¸­ï¼Œå‘½åä¸º *sql.yml*ã€‚è¿˜éœ€è¦ä¸€ä¸ªé¢å¤–çš„ Python è„šæœ¬æ¥å¤„ç†åˆ†è¯ç­–ç•¥ã€‚è¿™ä¸ªè„šæœ¬ä¹Ÿåº”è¯¥ä¿å­˜åœ¨ fine_tune_llm ç›®å½•ä¸­ï¼Œå‘½åä¸º *context_qa2.py*ã€‚è¿™æ˜¯è„šæœ¬ï¼š

```py
"""Module containing the classes for Context QA Prompt Tokenization Strategies"""
from typing import Tuple
from axolotl.prompt_tokenizers import InstructionPromptTokenizingStrategy
from axolotl.prompters import AlpacaPrompter, PromptStyle

# article, unanswerable_question, question, answer
def load_404(tokenizer, cfg):
    return AlpacaMissingInfoContextPromptTokenizingStrategy(
        AlpacaContextPrompter(PromptStyle.CHAT.value),
        tokenizer,
        cfg.train_on_inputs,
        cfg.sequence_len,
    )

def load(tokenizer, cfg):
    return AlpacaContextPromptTokenizingStrategy(
        AlpacaContextPrompter(PromptStyle.CHAT.value),
        tokenizer,
        cfg.train_on_inputs,
        cfg.sequence_len,
    )

class AlpacaContextPrompter(AlpacaPrompter):
    """
    Customized system prompted for concise QA
    """

    system_prompt = (
        "Use the following contextual information to concisely answer the question.\n"
    )
    system_no_input_prompt = (
        "Use the following contextual information to concisely answer the question.\n"
    )

class AlpacaContextPromptTokenizingStrategy(InstructionPromptTokenizingStrategy):
    """
    Tokenization Strategy to combine in-context article with a question and answer
    """

    def parse_instruction_fields(self, prompt) -> Tuple[str, str, str]:
        return (
            prompt["context"] + "\n===\n" + prompt["question"],
            "",
            prompt["answer"],
        )

class AlpacaMissingInfoContextPromptTokenizingStrategy(
    InstructionPromptTokenizingStrategy
):
    """
    Tokenization Strategy to combine in-context article with a question that can't be answered
    from the context and a default response to that effect
    """

    def parse_instruction_fields(self, prompt) -> Tuple[str, str, str]:
        return (
            prompt["context"] + "\n===\n" + prompt["unanswerable_question"],
            "",
            "The context provided does not contain any information about your inquiry. "
            "Therefore, I'm unable to answer your question based on the given context.",
        )
```

å¾ˆå¥½â€”â€”ç°åœ¨æˆ‘ä»¬åº”è¯¥å·²ç»å‡†å¤‡å¥½åˆå§‹åŒ–å¾®è°ƒè¿‡ç¨‹ã€‚æˆ‘ä»¬çš„ä¾èµ–é¡¹å·²ç»å®‰è£…å¥½ï¼Œé…ç½®æ–‡ä»¶ä¹Ÿéƒ½åœ¨ Gdrive çš„å„è‡ªä½ç½®ä¸­ã€‚ä½ åº”è¯¥æœ‰ä¸€ä¸ªçœ‹èµ·æ¥åƒè¿™æ ·çš„ fine_tune_llm æ–‡ä»¶å¤¹ï¼š

![](img/7b32d36201feb8026a300dcb86f28e5c.png)

ä½¿ç”¨ Axolotl å¾®è°ƒ LLama2â€“7B çš„ç›®å½•å¸ƒå±€

*.yml* æ–‡ä»¶æ˜¯æˆ‘ä»¬çš„é…ç½®æ–‡ä»¶ã€‚*.py* æ–‡ä»¶æ˜¯å¤„ç†åˆ†è¯çš„è„šæœ¬ã€‚axolotl ç›®å½•æ˜¯æˆ‘ä»¬ä¹‹å‰ä»ä»“åº“å…‹éš†çš„ç›®å½•ã€‚

ç°åœ¨æˆ‘ä»¬éœ€è¦åšçš„å°±æ˜¯è¿è¡Œï¼š

```py
!accelerate launch -m axolotl.cli.train /content/drive/MyDrive/fine_tune_llm/sql.yml
```

è¿™å°†åˆå§‹åŒ–ä½¿ç”¨ *sql.yml* é…ç½®æ–‡ä»¶æ¥å¾®è°ƒæˆ‘ä»¬æŒ‡å®šçš„æ¨¡å‹ã€‚è¿™ä¸ªè¿‡ç¨‹å¯¹æˆ‘æ¥è¯´å¤§çº¦èŠ±äº† 2 å°æ—¶ã€‚ä½ çš„æƒ…å†µå¯èƒ½ä¼šæœ‰æ‰€ä¸åŒã€‚å¦‚æœæ­¤é˜¶æ®µå‡ºç°ä»»ä½•é”™è¯¯ï¼Œå¾ˆå¯èƒ½æ˜¯ç”±äºä¾èµ–é”™è¯¯ã€‚æˆ‘é‡åˆ°è¿‡å‡ æ¬¡é—®é¢˜ï¼Œéœ€è¦æ‰‹åŠ¨å®‰è£…æ­£ç¡®çš„ cuda é©±åŠ¨ç¨‹åºå’Œ flash_attnï¼š

```py
!pip install torch==2.0.1+cu118 -f https://download.pytorch.org/whl/torch_stable.html
!pip uninstall flash_attn -y
!pip install flash_attn
```

å¾ˆå¥½â€”â€”æˆ‘ä»¬ä½¿ç”¨äº† Axolotl æä¾›çš„ç›¸å½“ç®€å•çš„é…ç½®æ¥åˆå§‹åŒ–æˆ‘ä»¬ LLM çš„å¾®è°ƒã€‚

è®©æˆ‘ä»¬å¯¹å¾®è°ƒåçš„æ¨¡å‹è¿›è¡Œæ¨ç†ï¼Œçœ‹çœ‹å®ƒçš„è¡¨ç°å¦‚ä½•ã€‚

ç”±äºç”¨äºè°ƒæ•´æ¨¡å‹çš„æ•°æ®å…·æœ‰ç›¸å½“ç‰¹å®šçš„å¸ƒå±€ï¼Œæˆ‘ä»¬éœ€è¦æ‰‹åŠ¨åˆ›å»ºä¸€äº›æ¨¡å‹å¯ä»¥å¤„ç†çš„æç¤ºè¯ã€‚è®©æˆ‘ä»¬åˆ›å»ºä¸€ä¸ªåŸºäºç°æœ‰æ•°æ®åº“æ¨¡å¼çš„é—®é¢˜ã€‚æˆ‘ä»¬å°†ä½¿ç”¨ä¹‹å‰æ£€æŸ¥æ•°æ®é›†æ˜¯å¦æ­£ç¡®åŠ è½½æ—¶çš„ç›¸åŒç¤ºä¾‹ï¼Œä½†æˆ‘ä»¬å°†è¦†ç›–é—®é¢˜ï¼š

```py
tst = dict(**trn[4500])
tst['question'] = 'What is the highest number of goals for any player with less than 5 assists'
tst
###output###
# {'answer': 'SELECT MAX(field_goals) FROM table_19722233_5 WHERE assists = 27',
# 'question': 'What is the highest number of goals for any player with less than 5 assists',
# 'context': 'CREATE TABLE table_19722233_5 (field_goals INTEGER, assists VARCHAR)'}
```

ç°åœ¨æˆ‘ä»¬éœ€è¦ç¡®ä¿æ ¼å¼æ­£ç¡®ï¼š

```py
fmt = """SYSTEM: Use the following contextual information to concisely answer the question.

USER: {}
===
{}
ASSISTANT:"""
```

æˆ‘ä»¬è¿˜å°†åˆ›å»ºä¸€ä¸ªå¿«é€Ÿå‡½æ•°æ¥å¤„ç†è¿™äº›æ ¼å¼ï¼š

```py
def sql_prompt(d):
  return fmt.format(d["context"], d["question"])
```

è®©æˆ‘ä»¬æ£€æŸ¥ä¸€ä¸‹ä»¥ä¸Šå†…å®¹ï¼š

```py
print(sql_prompt(tst))
###output###
# SYSTEM: Use the following contextual information to concisely answer the question.

# USER: CREATE TABLE table_19722233_5 (field_goals INTEGER, assists VARCHAR)
# ===
# What is the highest number of goals for any player with less than 5 assists
# ASSISTANT:
```

å¾ˆå¥½ï¼Œç°åœ¨æˆ‘ä»¬çš„æç¤ºè¯å·²ç»å‡†å¤‡å¥½å¯ä»¥è¾“å…¥åˆ°æ¨¡å‹ä¸­ã€‚

ç°åœ¨æˆ‘ä»¬éœ€è¦å¯¹ä¿å­˜åœ¨ qlora-out ç›®å½•ä¸­çš„æ¨¡å‹è¿›è¡Œæ¨æ–­ï¼ˆå¦‚ yaml é…ç½®æ–‡ä»¶ä¸­æŒ‡å®šçš„ï¼‰ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬éœ€è¦å®‰è£…è¿è¡Œæ¨æ–­æ‰€éœ€çš„ä¾èµ–é¡¹ï¼Œå¹¶å¯¹æç¤ºè¿›è¡Œåˆ†è¯ï¼Œä»¥ä¾¿æ¨¡å‹å¯ä»¥å¤„ç†å®ƒã€‚æˆ‘ä»¬éœ€è¦ä» HuggingFace è·å– Llama2â€“7B åˆ†è¯å™¨ã€‚å·¥ä½œæµç¨‹å°†ä¸æˆ‘åœ¨[ä¸Šä¸€ç¯‡æ–‡ç« ](https://medium.com/towards-data-science/quantisation-and-co-reducing-inference-times-on-llms-by-80-671db9349bdb)ä¸­ä»‹ç»çš„éå¸¸ç›¸ä¼¼ã€‚ä»¥ä¸‹æ˜¯ä»£ç ï¼š

```py
import torch
from peft import PeftModel
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig

fine_tuned_model = '/content/drive/MyDrive/fine_tune_llm/axolotl/qlora-out'

tokr = AutoTokenizer.from_pretrained('meta-llama/Llama-2-7b-hf')

model = AutoModelForCausalLM.from_pretrained('meta-llama/Llama-2-7b-hf',
                                             torch_dtype=torch.bfloat16, device_map=0)
model = PeftModel.from_pretrained(model, fine_tuned_model)
model = model.merge_and_unload()
model.save_pretrained('sql-model')

toks = tokr(sql_prompt(tst), return_tensors="pt")

res = model.generate(**toks.to("cuda"), max_new_tokens=250).to('cpu')

print(tokr.batch_decode(res)[0])
###output###
# SYSTEM: Use the following contextual information to concisely answer the question.

# USER: CREATE TABLE table_19722233_5 (field_goals INTEGER, assists VARCHAR)
# ===
# What is the highest number of goals for any player with less than 5 assists
# ASSISTANT:
# SELECT MAX(field_goals) FROM table_19722233_5 WHERE assists < 5</s>
```

å¤ªæ£’äº†ï¼æ­£å¦‚ä½ ä»åŠ©æ‰‹çš„è¾“å‡ºä¸­çœ‹åˆ°çš„ï¼Œå®ƒæä¾›äº†æ­£ç¡®çš„ç­”æ¡ˆã€‚

è™½ç„¶è¿™å¯ä»¥è¯´æ˜¯ä¸€ä¸ªéå¸¸ç®€å•çš„é—®é¢˜ï¼Œä½†ä½ å¯ä»¥çœ‹åˆ°æˆ‘ä»¬æˆåŠŸè®­ç»ƒäº†ä¸€ä¸ªåŸºç¡€ LLMï¼Œè¿™ä¸ª LLM ä¹‹å‰åªèƒ½ç”¨äºä¸€èˆ¬è¯­è¨€æŸ¥è¯¢ï¼Œç°åœ¨å®ƒå¯ä»¥å†™ SQL ä»£ç äº†ï¼

é€šè¿‡ç†Ÿæ‚‰ä»¥ä¸Šå†…å®¹å¹¶æŸ¥çœ‹ axolotl æ–‡æ¡£ï¼Œä½ å°†èƒ½å¤Ÿäº†è§£å¦‚ä½•å°†æˆ‘çš„æ–¹æ³•é€‚åº”åˆ°ä¸åŒçš„ LLM ä»»åŠ¡ä¸­ï¼Œä¾‹å¦‚é—®ç­”ã€‚åªéœ€è¾“å…¥ä¸€ä¸ªç‰¹å®šä¸»é¢˜çš„é—®é¢˜æ•°æ®é›†åŠå…¶ç›¸åº”ç­”æ¡ˆï¼Œä½¿ç”¨ axolotl æä¾›çš„ç¤ºä¾‹æ›´æ–°ä½ çš„ yaml é…ç½®æ–‡ä»¶ï¼Œç„¶åå¼€å§‹å¦ä¸€ä¸ªå¾®è°ƒå®éªŒğŸ˜Š

æˆ‘å¸Œæœ›ä½ é˜…è¯»è¿™ç¯‡æ–‡ç« æ—¶çš„æ„Ÿå—ä¸æˆ‘å†™ä½œæ—¶ä¸€æ ·æ„‰å¿«ã€‚åƒå¾€å¸¸ä¸€æ ·ï¼Œå¦‚æœä½ æœ‰ä»»ä½•é—®é¢˜ï¼Œè¯·éšæ—¶è”ç³»æˆ‘ã€‚

*é™¤éå¦æœ‰è¯´æ˜ï¼Œå¦åˆ™æ‰€æœ‰å›¾åƒå‡å±äºä½œè€…ã€‚*
