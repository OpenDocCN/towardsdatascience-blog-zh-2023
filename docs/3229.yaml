- en: Super Charge Your ML Systems In 4 Simple Steps
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/super-charge-your-ml-systems-in-4-simple-steps-4485f0208440?source=collection_archive---------6-----------------------#2023-10-27](https://towardsdatascience.com/super-charge-your-ml-systems-in-4-simple-steps-4485f0208440?source=collection_archive---------6-----------------------#2023-10-27)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/f57a9771668a09508fbbd608c1e742be.png)'
  prefs: []
  type: TYPE_IMG
- en: Image generated with DALL.E-3
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/@donaljbyrne?source=post_page-----4485f0208440--------------------------------)[![Donal
    Byrne](../Images/4695fec999da472bbd5116de6ed7cc5d.png)](https://medium.com/@donaljbyrne?source=post_page-----4485f0208440--------------------------------)[](https://towardsdatascience.com/?source=post_page-----4485f0208440--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----4485f0208440--------------------------------)
    [Donal Byrne](https://medium.com/@donaljbyrne?source=post_page-----4485f0208440--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fdbf9e722f39d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsuper-charge-your-ml-systems-in-4-simple-steps-4485f0208440&user=Donal+Byrne&userId=dbf9e722f39d&source=post_page-dbf9e722f39d----4485f0208440---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----4485f0208440--------------------------------)
    ·8 min read·Oct 27, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F4485f0208440&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsuper-charge-your-ml-systems-in-4-simple-steps-4485f0208440&user=Donal+Byrne&userId=dbf9e722f39d&source=-----4485f0208440---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4485f0208440&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsuper-charge-your-ml-systems-in-4-simple-steps-4485f0208440&source=-----4485f0208440---------------------bookmark_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: Welcome to the rollercoaster of ML optimization! This post will take you through
    my process for optimizing any ML system for lightning-fast training and inference
    in 4 simple steps.
  prefs: []
  type: TYPE_NORMAL
- en: 'Imagine this: You finally get put on a cool new ML project where you are training
    your agent to count how many hot dogs are in a photo, the success of which could
    possibly make your company tens of dollars!'
  prefs: []
  type: TYPE_NORMAL
- en: You get the latest hotshot object detection model implemented in your favourite
    framework that has lots of GitHub stars, run some toy examples and after an hour
    or so it’s picking out hotdogs like a broke student in their 3rd repeat year of
    college, life is good.
  prefs: []
  type: TYPE_NORMAL
- en: The next steps are obvious, we want to scale it up to some harder problems,
    this means more data, a larger model and of course, longer training time. Now
    you are looking at days of training instead of hours. That’s fine though, you
    have been ignoring the rest of your team for 3 weeks now and should probably spend
    a day getting through the backlog of code reviews and passive-aggressive emails
    that have built up.
  prefs: []
  type: TYPE_NORMAL
- en: You come back a day later after feeling good about the insightful and absolutely
    necessary nitpicks you left on your colleagues MR’s, only to find your performance
    tanked and crashed post a 15-hour training stint (karma works fast).
  prefs: []
  type: TYPE_NORMAL
- en: 'The ensuing days morph into a whirlwind of trials, tests and experiments, with
    each potential idea taking more than a day to run. These quickly start racking
    up hundreds of dollars in compute costs, all leading to the big question: How
    can we make this faster and cheaper?'
  prefs: []
  type: TYPE_NORMAL
- en: 'Welcome to the emotional rollercoaster of ML optimization! Here’s a straightforward
    4-step process to turn the tides in your favour:'
  prefs: []
  type: TYPE_NORMAL
- en: Benchmark
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Simplify
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Optimize
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeat
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This is an iterative process, and there will be many times when you repeat some
    steps before moving on to the next, so it’s less of a 4 step system and more of
    a toolbox, but 4 steps sounds better.
  prefs: []
  type: TYPE_NORMAL
- en: 1 — Benchmark
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: “Measure twice, cut once” *— Someone wise*.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'The first (and probably second) thing you should always do, is profile your
    system. This can be something as simple as just timing how long it takes to run
    a specific block of code, or as complex as doing a full profile trace. What matters
    is you have enough information to identify the bottlenecks in your system. I carry
    out multiple benchmarks depending on where we are in the process and typically
    break it down into 2 types: high-level and low-level benchmarking.'
  prefs: []
  type: TYPE_NORMAL
- en: High Level
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This is the sort of stuff you will be showing your boss at the weekly “How f**cked
    are we?” meeting and would want these metrics as part of every run. These will
    give you a high-level sense of how performant your system is running.
  prefs: []
  type: TYPE_NORMAL
- en: '**Batches Per Second** — how quickly are we getting through each of our batches?
    this should be as high as possible'
  prefs: []
  type: TYPE_NORMAL
- en: '**Steps Per Second** — (RL specific) how quickly are we stepping through our
    environment to generate our data, should be as high as possible. There are some
    complicated interplays between step time and train batches that I won’t get into
    here.'
  prefs: []
  type: TYPE_NORMAL
- en: '**GPU Util** — how much of your GPU is being utilised during training? This
    should be consistently as close to 100%, if not then you have idle time that can
    be optimized.'
  prefs: []
  type: TYPE_NORMAL
- en: '**CPU Util** — how much of your CPUs are being utilised during training? Again,
    this should be as close to 100% as possible.'
  prefs: []
  type: TYPE_NORMAL
- en: '**FLOPS** — floating point operations per second, this gives you a view of
    how effectively are you using your total hardware.'
  prefs: []
  type: TYPE_NORMAL
- en: Low Level
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Using the metrics above you can then start to look deeper as to where your bottleneck
    might be. Once you have these, you want to start looking at more fine-grained
    metrics and profiling.
  prefs: []
  type: TYPE_NORMAL
- en: '**Time Profiling** — This is the simplest, and often most useful, experiment
    to run. Profiling tools like [cprofiler](https://docs.python.org/3/library/profile.html)
    can be used to get a bird’s eye view of the timing of each of your components
    as a whole or can look at the timing of specific components.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Memory Profiling** — Another staple of the optimization toolbox. Big systems
    require a lot of memory, so we have to make sure we are not wasting any of it!
    tools like [memory-profiler](https://pypi.org/project/memory-profiler/) will help
    you narrow down where your system is eating up your RAM.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Model Profiling** — Tools like [Tensorboard](https://www.tensorflow.org/tensorboard/get_started#:~:text=TensorBoard%20is%20a%20tool%20for,dimensional%20space%2C%20and%20much%20more.)
    come with excellent profiling tools for looking at what is eating up your performance
    within your model.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Network Profiling** — Network load is a common culprit for bottlenecking
    your system. There are tools like [wireshark](https://www.wireshark.org/) to help
    you profile this, but to be honest I never use it. Instead, I prefer to do time
    profiling on my components and measure the total time it is taking within my component
    and then isolate how much time is coming from the network I/O itself.'
  prefs: []
  type: TYPE_NORMAL
- en: Make sure to check out this great article on profiling in Python from [RealPython](https://realpython.com/python-profiling/)
    for more info!
  prefs: []
  type: TYPE_NORMAL
- en: 2 — Simplify
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Once you have identified an area in your profiling that needs to be optimized,
    simplify it. Cut out everything else except that part. Keep reducing the system
    down to smaller parts until you reach the bottleneck. Don’t be afraid to profile
    as you simplify, this will ensure that you are going in the right direction as
    you iterate. Keep repeating this until you find your bottleneck.
  prefs: []
  type: TYPE_NORMAL
- en: Tips
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Replace other components with stubs and mock functions that just provide expected
    data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Simulate heavy functions with `sleep` functions or dummy calculations.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use dummy data to remove the overhead of the data generation and processing.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Start with local, single-process versions of your system before moving to distributed.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Simulate multiple nodes and actors on a single machine to remove the network
    overhead.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Find the theoretical max performance for each part of the system. If all of
    the other bottlenecks in the system were gone except for this component, what
    is our expected performance?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Profile again! Each time you simplify the system, re-run your profiling.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Once we have zoned in on the bottleneck there are some key questions we want
    to answer
  prefs: []
  type: TYPE_NORMAL
- en: '**What is the theoretical max performance of this component?**'
  prefs: []
  type: TYPE_NORMAL
- en: If we have sufficiently isolated the bottlenecked component then we should be
    able to answer this.
  prefs: []
  type: TYPE_NORMAL
- en: '**How far away are we from the max?**'
  prefs: []
  type: TYPE_NORMAL
- en: This optimality gap will inform us on how optimized our system is. Now, it could
    be the case that there are other hard constraints once we introduce the component
    back into the system and that’s fine, but it is crucial to at least be aware of
    what the gap is.
  prefs: []
  type: TYPE_NORMAL
- en: '**Is there a deeper bottleneck?**'
  prefs: []
  type: TYPE_NORMAL
- en: Always ask yourself this, maybe the problem is deeper than you initially thought,
    in which case, we repeat the process of benchmarking and simplifying.
  prefs: []
  type: TYPE_NORMAL
- en: 3 — Optimize
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Okay, so let’s say we have identified the biggest bottleneck, now we get to
    the fun part, how do we improve things? There are usually 3 areas that we should
    be looking at for possible improvements
  prefs: []
  type: TYPE_NORMAL
- en: '**Compute**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Communication**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Memory**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In order to reduce computation bottlenecks we need to look at being as efficient
    as possible with the data and algorithms we are working with. This is obviously
    project-specific and there is a huge amount of things that can be done, but let’s
    look at some good rules of thumb.
  prefs: []
  type: TYPE_NORMAL
- en: '**Parallelising** — make sure that you carry out as much work as possible in
    parallel. This is the first big win in designing your system that can massively
    impact performance. Look at methods like vectorisation, batching, multi-threading
    and multi-processing.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Caching** — pre-compute and reuse calculations where you can. Many algorithms
    can take advantage of reusing pre-computed values and save critical compute for
    each of your training steps.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Offloading** — we all know that Python is not known for its speed. Luckily
    we can offload critical computations to lower level languages like C/C++.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Hardware Scaling** — This is kind of a cop-out, but when all else fails,
    we can always just throw more computers at the problem!'
  prefs: []
  type: TYPE_NORMAL
- en: Communication
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Any seasoned engineer will tell you that communication is key to delivering
    a successful project, and by that, we of course mean communication within our
    system (God forbid we ever have to talk to our colleagues). Some good rules of
    thumb are:'
  prefs: []
  type: TYPE_NORMAL
- en: '**No Idle Time** — All of your available hardware must be utilised at all times,
    otherwise you are leaving performance gains on the table. This is usually due
    to complications and overhead of communication across your system.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Stay Local** — Keep everything on a single machine for as long as possible
    before moving to a distributed system. This keeps your system simple as well as
    avoids the communication overhead of a distributed system.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Async > Sync** — Identify anything that can be done asynchronously, this
    will help offload the cost of communication by keeping work moving while data
    is being moved.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Avoid Moving Data** — moving data from CPU to GPU or from one process to
    another is expensive! Do as little of this as possible or reduce the impact of
    this by carrying it out asynchronously.'
  prefs: []
  type: TYPE_NORMAL
- en: Memory
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Last but not least is memory. Many of the areas mentioned above can be helpful
    in relieving your bottleneck, but it might not be possible if you have no memory
    available! Let’s look at some things to consider.
  prefs: []
  type: TYPE_NORMAL
- en: '**Data Types** — keep these as small as possible helping to reduce the cost
    of communication, and memory and with modern accelerators, it will also reduce
    computation.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Caching** — similar to reducing computation, smart caching can help save
    you memory. However, make sure your cached data is being used frequently enough
    to justify the caching.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Pre-Allocate** — not something we are used to in Python, but being strict
    with pre-allocating memory can mean you know exactly how much memory you need,
    reduces the risk of fragmentation and if you are able to write to shared memory,
    you will reduce communication between your processes!'
  prefs: []
  type: TYPE_NORMAL
- en: '**Garbage Collection** — luckily python handles most of this for us, but it
    is important to make sure you are not keeping large values in scope without needing
    them or worse, having a circular dependency that can cause a memory leak.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Be Lazy** — Evaluate expressions only when necessary. In Python, you can
    use generator expressions instead of list comprehensions for operations that can
    be lazily evaluated.'
  prefs: []
  type: TYPE_NORMAL
- en: 4 — Repeat
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So, when are we finished? Well, that really depends on your project, what the
    requirements are and how long it takes before your dwindling sanity finally breaks!
  prefs: []
  type: TYPE_NORMAL
- en: As you remove bottlenecks, you will get diminishing returns on the time and
    effort you are putting in to optimize your system. As you go through the process
    you need to decide when good is good enough. Remember, speed is a means to an
    end, don’t get caught in the trap of optimizing for the sake of it. If it is not
    going to have an impact on users, then it is probably time to move on.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Building large-scale ML systems is HARD. It’s like playing a twisted game of
    “Where's Waldo” crossed with Dark Souls. If you do manage to find the problem
    you have to take multiple attempts to beat it and you end up spending most of
    your time getting your ass kicked, asking yourself “Why am I spending my Friday
    night doing this?”. Having a simple and principled approach can help you get past
    that final boss battle and taste those sweet, sweet theoretical max FLOPs.
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://donalbyrne.substack.com/?utm_source=navbar&utm_medium=web&r=hk3pr&source=post_page-----4485f0208440--------------------------------)
    [## ML in Action | Donal Byrne | Substack'
  prefs: []
  type: TYPE_NORMAL
- en: The Machine Learning newsletter that provides unsolicited advice, practical
    insights and lessons learned in the rapidly…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: donalbyrne.substack.com](https://donalbyrne.substack.com/?utm_source=navbar&utm_medium=web&r=hk3pr&source=post_page-----4485f0208440--------------------------------)
  prefs: []
  type: TYPE_NORMAL
