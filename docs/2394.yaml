- en: The Transformer Architecture of GPT Models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/the-transformer-architecture-of-gpt-models-b8695b48728b?source=collection_archive---------3-----------------------#2023-07-25](https://towardsdatascience.com/the-transformer-architecture-of-gpt-models-b8695b48728b?source=collection_archive---------3-----------------------#2023-07-25)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Learn the details of the Transformer architecture
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@bea_684?source=post_page-----b8695b48728b--------------------------------)[![Beatriz
    Stollnitz](../Images/63a2a7daeca6d93e26b3ac0556c42aa1.png)](https://medium.com/@bea_684?source=post_page-----b8695b48728b--------------------------------)[](https://towardsdatascience.com/?source=post_page-----b8695b48728b--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----b8695b48728b--------------------------------)
    [Beatriz Stollnitz](https://medium.com/@bea_684?source=post_page-----b8695b48728b--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F1c8863892480&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-transformer-architecture-of-gpt-models-b8695b48728b&user=Beatriz+Stollnitz&userId=1c8863892480&source=post_page-1c8863892480----b8695b48728b---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----b8695b48728b--------------------------------)
    ·22 min read·Jul 25, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fb8695b48728b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-transformer-architecture-of-gpt-models-b8695b48728b&user=Beatriz+Stollnitz&userId=1c8863892480&source=-----b8695b48728b---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb8695b48728b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-transformer-architecture-of-gpt-models-b8695b48728b&source=-----b8695b48728b---------------------bookmark_footer-----------)![](../Images/b27fc545e177f9112abf27cfac0ceebb.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Photo by [fabio](https://unsplash.com/@fabioha?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: In 2017, authors from Google published a paper called [Attention is All You
    Need](https://arxiv.org/abs/1706.03762) in which they introduced the Transformer
    architecture. This new architecture achieved unparalleled success in language
    translation tasks, and the paper quickly became essential reading for anyone immersed
    in the area. Like many others, when I read the paper for the first time, I could
    see the value of its innovative ideas, but I didn’t realize just how disruptive
    the paper would be to other areas under the broader umbrella of AI. Within a few
    years, researchers adapted the Transformer architecture to many tasks other than
    language translation, including image classification, image generation, and protein
    folding problems. In particular, the Transformer architecture revolutionized text
    generation and paved the way for GPT models and the exponential growth we’re currently
    experiencing in AI.
  prefs: []
  type: TYPE_NORMAL
- en: Given how pervasive Transformer models are these days, both in the industry
    and academia, understanding the details of how they work is an important skill
    for every AI practitioner. This article will focus mostly on the architecture
    of GPT models, which are built using a subset of the original Transformer architecture,
    but it will also cover the original Transformer at the end. For the model code…
  prefs: []
  type: TYPE_NORMAL
