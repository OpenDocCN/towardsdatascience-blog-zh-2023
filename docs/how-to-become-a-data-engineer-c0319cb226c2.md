# å¦‚ä½•æˆä¸ºæ•°æ®å·¥ç¨‹å¸ˆ

> åŸæ–‡ï¼š[`towardsdatascience.com/how-to-become-a-data-engineer-c0319cb226c2`](https://towardsdatascience.com/how-to-become-a-data-engineer-c0319cb226c2)

## 2024 å¹´åˆå­¦è€…çš„æ·å¾„

[](https://mshakhomirov.medium.com/?source=post_page-----c0319cb226c2--------------------------------)![ğŸ’¡Mike Shakhomirov](https://mshakhomirov.medium.com/?source=post_page-----c0319cb226c2--------------------------------)[](https://towardsdatascience.com/?source=post_page-----c0319cb226c2--------------------------------)![Towards Data Science](https://towardsdatascience.com/?source=post_page-----c0319cb226c2--------------------------------) [ğŸ’¡Mike Shakhomirov](https://mshakhomirov.medium.com/?source=post_page-----c0319cb226c2--------------------------------)

Â·å‘å¸ƒåœ¨[æ•°æ®ç§‘å­¦å‰æ²¿](https://towardsdatascience.com/?source=post_page-----c0319cb226c2--------------------------------) Â·17 åˆ†é’Ÿé˜…è¯»Â·2023 å¹´ 10 æœˆ 7 æ—¥

--

![](img/b49c1deff780a74750c4dfda9056b149.png)

å›¾ç‰‡ç”±[Gabriel Vasiliu](https://unsplash.com/@gabimedia?utm_source=medium&utm_medium=referral)æä¾›ï¼Œ[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)ä¸Šå‘å¸ƒ

è¿™ä¸ªæ•…äº‹è§£é‡Šäº†ä¸€ç§åŠ é€Ÿè¿›å…¥æ•°æ®å·¥ç¨‹è§’è‰²çš„æ–¹æ³•ï¼Œé€šè¿‡å­¦ä¹ æ‰€éœ€çš„æŠ€èƒ½å¹¶ç†Ÿæ‚‰æ•°æ®å·¥ç¨‹å·¥å…·å’ŒæŠ€æœ¯ã€‚è¿™å¯¹åˆçº§ IT ä»ä¸šè€…å’Œå¸Œæœ›è½¬è¡Œçš„ä¸­çº§è½¯ä»¶å·¥ç¨‹å¸ˆå°†éå¸¸æœ‰ç”¨ã€‚é€šè¿‡æˆ‘ä½œä¸ºè‹±å›½å’Œä¸­ä¸œåœ°åŒºæœ€æˆåŠŸåˆåˆ›å…¬å¸çš„æ•°æ®å·¥ç¨‹ä¸»ç®¡çš„å¤šå¹´ç»éªŒï¼Œæˆ‘ä»èŒä¸šç”Ÿæ¶¯ä¸­å­¦åˆ°äº†å¾ˆå¤šï¼Œæˆ‘å¸Œæœ›ä¸æ‚¨åˆ†äº«è¿™äº›çŸ¥è¯†å’Œç»éªŒã€‚è¿™æ˜¯æˆ‘åœ¨æ•°æ®å·¥ç¨‹é¢†åŸŸè·å¾—çš„ä¸ªäººç»éªŒçš„åæ˜ ã€‚æˆ‘å¸Œæœ›è¿™å¯¹æ‚¨æœ‰ç”¨ã€‚

## æ•°æ®å·¥ç¨‹å¸ˆâ€”â€”è§’è‰²

é¦–å…ˆï¼Œä¸ºä»€ä¹ˆé€‰æ‹©æ•°æ®å·¥ç¨‹å¸ˆï¼Ÿ

æ•°æ®å·¥ç¨‹æ˜¯ä¸€ä¸ªä»¤äººå…´å¥‹ä¸”éå¸¸æœ‰å›æŠ¥çš„é¢†åŸŸã€‚è¿™æ˜¯ä¸€ä»½è¿·äººçš„å·¥ä½œï¼Œæˆ‘ä»¬æœ‰æœºä¼šå¤„ç†æ‰€æœ‰ä¸æ•°æ®ç›¸å…³çš„äº‹ç‰©â€”â€”APIã€æ•°æ®è¿æ¥å™¨ã€æ•°æ®å¹³å°ã€å•†ä¸šæ™ºèƒ½ä»¥åŠå¸‚åœºä¸Šæ•°åç§æ•°æ®å·¥å…·ã€‚æ•°æ®å·¥ç¨‹ä¸æœºå™¨å­¦ä¹ ï¼ˆMLï¼‰ç´§å¯†ç›¸å…³ã€‚ä½ å°†åˆ›å»ºå’Œéƒ¨ç½²å„ç§æ•°æ®å’Œ ML ç®¡é“ã€‚

> è¿™ä»½å·¥ä½œç»å¯¹ä¸ä¼šæ— èŠï¼Œå¹¶ä¸”è–ªèµ„ä¼˜åšã€‚

è¿™ä»½å·¥ä½œå›æŠ¥ä¸°åšï¼Œå› ä¸ºå»ºç«‹ä¸€ä¸ªè‰¯å¥½çš„æ•°æ®å¹³å°å¹¶ä¸å®¹æ˜“ã€‚å®ƒä»éœ€æ±‚æ”¶é›†å’Œè®¾è®¡å¼€å§‹ï¼Œéœ€è¦ç›¸å½“çš„ç»éªŒã€‚è¿™ä¸æ˜¯ä¸€é¡¹ç®€å•çš„ä»»åŠ¡ï¼Œä¹Ÿéœ€è¦ä¸€äº›çœŸæ­£å‡ºè‰²çš„ç¼–ç¨‹æŠ€èƒ½ã€‚å·¥ä½œæœ¬èº«æ˜¯å®‰å…¨çš„ï¼Œå› ä¸ºåªè¦ä¼ä¸šäº§ç”Ÿæ•°æ®ï¼Œè¿™ä»½å·¥ä½œå°±ä¼šæœ‰å¾ˆé«˜çš„éœ€æ±‚ã€‚

> å…¬å¸æ€»æ˜¯ä¼šè˜ç”¨é‚£äº›çŸ¥é“å¦‚ä½•é«˜æ•ˆå¤„ç†ï¼ˆETLï¼‰æ•°æ®çš„äººã€‚

æ•°æ®å·¥ç¨‹åœ¨è¿‡å»äº”å¹´é‡Œæˆä¸ºäº†è‹±å›½å¢é•¿æœ€å¿«çš„èŒä¸šä¹‹ä¸€ï¼Œåœ¨ 2023 å¹´ LinkedIn çš„æœ€å—æ¬¢è¿èŒä¸šæ¦œå•ä¸­æ’åç¬¬ 13 [1]ã€‚å¦ä¸€ä¸ªåŠ å…¥çš„ç†ç”±æ˜¯ç¨€ç¼ºæ€§ã€‚åœ¨ IT é¢†åŸŸï¼Œå¦‚ä»Šæ‰¾åˆ°ä¸€ä¸ªä¼˜ç§€çš„æ•°æ®å·¥ç¨‹å¸ˆæ˜¯éå¸¸å›°éš¾çš„ã€‚

> ä½œä¸ºâ€œæ•°æ®å·¥ç¨‹è´Ÿè´£äººâ€ï¼Œæˆ‘æ¯å‘¨åœ¨ LinkedIn ä¸Šæ”¶åˆ° 4 ä¸ªèŒä½é¢è¯•é‚€è¯·ã€‚å¹³å‡è€Œè¨€ï¼Œåˆçº§æ•°æ®å·¥ç¨‹è§’è‰²çš„éœ€æ±‚æ›´é«˜ã€‚

æ ¹æ® DICE çš„æŠ€æœ¯èŒä½ç ”ç©¶ï¼Œæ•°æ®å·¥ç¨‹å¸ˆæ˜¯å¢é•¿æœ€å¿«çš„æŠ€æœ¯èŒä¸šï¼š

![](img/ee08b504be177749be9112bfa0f2dd1c.png)

æ¥æºï¼š[DICE](http://marketing.dice.com/pdf/2020/Dice_2020_Tech_Job_Report.pdf)

## ç°ä»£æ•°æ®æ ˆ

ç°ä»£æ•°æ®æ ˆæŒ‡çš„æ˜¯ä¸€ç³»åˆ—æ•°æ®å¤„ç†å·¥å…·å’Œæ•°æ®å¹³å°ç±»å‹ã€‚

> ä½ åœ¨è¿™ä¸ªé¢†åŸŸå—ï¼Ÿ

â€œä½ åœ¨è¿™ä¸ªé¢†åŸŸå—ï¼Ÿâ€â€”â€”è¿™æ˜¯æˆ‘åœ¨ä¸€æ¬¡é¢è¯•ä¸­è¢«é—®åˆ°çš„é—®é¢˜ã€‚ä½ ä¼šå¸Œæœ›èƒ½å¤Ÿå›ç­”è¿™ä¸ªé—®é¢˜ï¼Œå¹¶äº†è§£ç›¸å…³æ–°é—»ã€é¦–æ¬¡å…¬å¼€å‹Ÿè‚¡ã€æœ€è¿‘çš„å‘å±•ã€çªç ´ã€å·¥å…·å’ŒæŠ€æœ¯ã€‚

ç†Ÿæ‚‰å¸¸è§çš„æ•°æ®å¹³å°æ¶æ„ç±»å‹ï¼Œå³æ•°æ®æ¹–ã€æ¹–ä»“ã€æ•°æ®ä»“åº“ï¼Œå¹¶å‡†å¤‡å¥½å›ç­”å®ƒä»¬ä½¿ç”¨å“ªäº›å·¥å…·ã€‚æŸ¥çœ‹è¿™ç¯‡æ–‡ç« äº†è§£ä¸€äº›ç¤ºä¾‹ï¼š

## æ•°æ®å¹³å°æ¶æ„ç±»å‹

### å®ƒåœ¨å¤šå¤§ç¨‹åº¦ä¸Šæ»¡è¶³äº†ä½ çš„ä¸šåŠ¡éœ€æ±‚ï¼Ÿé€‰æ‹©çš„å›°å¢ƒã€‚

## æ•°æ®å¹³å°æ¶æ„ç±»å‹

## æ•°æ®ç®¡é“

ä½œä¸ºä¸€åæ•°æ®å·¥ç¨‹å¸ˆï¼Œä½ å‡ ä¹æ¯å¤©éƒ½ä¼šè¢«è¦æ±‚è¿›è¡Œæ•°æ®ç®¡é“è®¾è®¡ã€‚ä½ ä¼šå¸Œæœ›ç†Ÿæ‚‰æ•°æ®ç®¡é“è®¾è®¡æ¨¡å¼ï¼Œå¹¶èƒ½å¤Ÿè§£é‡Šä½•æ—¶ä½¿ç”¨å®ƒä»¬ã€‚å°†è¿™äº›çŸ¥è¯†åº”ç”¨äºå®è·µæ˜¯è‡³å…³é‡è¦çš„ï¼Œå› ä¸ºå®ƒå†³å®šäº†ä½¿ç”¨å“ªäº›å·¥å…·ã€‚æ­£ç¡®çš„æ•°æ®è½¬æ¢å·¥å…·ç»„åˆå¯ä»¥ä½¿æ•°æ®ç®¡é“æå…¶é«˜æ•ˆã€‚

å› æ­¤ï¼Œæˆ‘ä»¬éœ€è¦å‡†ç¡®çŸ¥é“ä½•æ—¶åº”ç”¨æµæ•°æ®å¤„ç†ï¼Œä½•æ—¶åº”ç”¨æ‰¹å¤„ç†ã€‚ä¸€ç§å¯èƒ½éå¸¸æ˜‚è´µï¼Œè€Œå¦ä¸€ç§åˆ™å¯èƒ½èŠ‚çœæ•°åƒã€‚ç„¶è€Œï¼Œä¸šåŠ¡éœ€æ±‚å¯èƒ½åœ¨æ¯ç§æƒ…å†µä¸‹éƒ½ä¸åŒã€‚æœ¬æ–‡æä¾›äº†å…¨é¢çš„æ•°æ®ç®¡é“è®¾è®¡æ¨¡å¼åˆ—è¡¨ï¼š

## æ•°æ®ç®¡é“è®¾è®¡æ¨¡å¼

### é€‰æ‹©åˆé€‚çš„æ¶æ„åŠç¤ºä¾‹

## æ•°æ®ç®¡é“è®¾è®¡æ¨¡å¼

## æ•°æ®å»ºæ¨¡

æˆ‘è®¤ä¸ºæ•°æ®å»ºæ¨¡æ˜¯æ•°æ®å·¥ç¨‹çš„ä¸€ä¸ªå…³é”®éƒ¨åˆ†ã€‚è®¸å¤šæ•°æ®å¹³å°çš„è®¾è®¡æ–¹å¼æ˜¯å°†æ•°æ®â€œåŸæ ·â€åŠ è½½åˆ°æ•°æ®ä»“åº“è§£å†³æ–¹æ¡ˆä¸­ã€‚è¿™è¢«ç§°ä¸º ELT æ–¹æ³•ã€‚æ•°æ®å·¥ç¨‹å¸ˆç»å¸¸éœ€è¦ä½¿ç”¨æ ‡å‡† SQL æ–¹è¨€åˆ›å»ºæ•°æ®è½¬æ¢ç®¡é“ã€‚è‰¯å¥½çš„ SQL æŠ€èƒ½æ˜¯å¿…ä¸å¯å°‘çš„ã€‚ç¡®å®ï¼ŒSQL å¯¹äºåˆ†ææŸ¥è¯¢æ˜¯è‡ªç„¶çš„ï¼Œç°å¦‚ä»Šå‡ ä¹å·²ç»æˆä¸ºæ ‡å‡†ã€‚å®ƒæœ‰åŠ©äºé«˜æ•ˆæŸ¥è¯¢æ•°æ®ï¼Œå¹¶è®©æ‰€æœ‰ä¸šåŠ¡åˆ©ç›Šç›¸å…³è€…è½»æ¾ä½¿ç”¨åˆ†æåŠŸèƒ½ã€‚

æ•°æ®å·¥ç¨‹å¸ˆå¿…é¡»çŸ¥é“å¦‚ä½•**æ¸…æ´—ã€ä¸°å¯Œå’Œæ›´æ–°æ•°æ®é›†**ã€‚ä¾‹å¦‚ï¼Œä½¿ç”¨ MERGE æ‰§è¡Œå¢é‡æ›´æ–°ã€‚åœ¨ä½ çš„å·¥ä½œå°æˆ–æ•°æ®ä»“åº“ï¼ˆDWHï¼‰ä¸­è¿è¡Œæ­¤ SQLã€‚å®ƒè§£é‡Šäº†å®ƒçš„å·¥ä½œåŸç†ï¼š

```py
create temp table last_online as (
    select 1 as user_id
    , timestamp('2000-10-01 00:00:01') as last_online
)
;
create temp table connection_data  (
  user_id int64
  ,timestamp timestamp
)
PARTITION BY DATE(_PARTITIONTIME)
;
insert connection_data (user_id, timestamp)
    select 2 as user_id
    , timestamp_sub(current_timestamp(),interval 28 hour) as timestamp
union all
    select 1 as user_id
        , timestamp_sub(current_timestamp(),interval 28 hour) as timestamp
union all
    select 1 as user_id
        , timestamp_sub(current_timestamp(),interval 20 hour) as timestamp
union all
    select 1 as user_id
    , timestamp_sub(current_timestamp(),interval 1 hour) as timestamp
;

merge last_online t
using (
  select
      user_id
    , last_online
  from
    (
        select
            user_id
        ,   max(timestamp) as last_online

        from 
            connection_data
        where
            date(_partitiontime) >= date_sub(current_date(), interval 1 day)
        group by
            user_id

    ) y

) s
on t.user_id = s.user_id
when matched then
  update set last_online = s.last_online, user_id = s.user_id
when not matched then
  insert (last_online, user_id) values (last_online, user_id)
;
select * from last_online
;
```

ä¸€äº›é«˜çº§ SQL æç¤ºå’ŒæŠ€å·§å¯ä»¥åœ¨è¿™é‡Œæ‰¾åˆ°ï¼š

[## é¢å‘åˆå­¦è€…çš„é«˜çº§ SQL æŠ€å·§](https://towardsdatascience.com/advanced-sql-techniques-for-beginners-211851a28488?source=post_page-----c0319cb226c2--------------------------------)

### åœ¨ 1 åˆ° 10 çš„å°ºåº¦ä¸Šï¼Œä½ çš„æ•°æ®ä»“åº“æŠ€èƒ½æœ‰å¤šå¥½ï¼Ÿ

towardsdatascience.com

## ç¼–ç 

è¿™éå¸¸é‡è¦ï¼Œå› ä¸ºæ•°æ®å·¥ç¨‹ä¸ä»…ä»…æ¶‰åŠæ•°æ®å»ºæ¨¡å’Œ SQLã€‚å¯ä»¥å°†æ•°æ®å·¥ç¨‹å¸ˆè§†ä¸ºè½¯ä»¶å·¥ç¨‹å¸ˆã€‚ä»–ä»¬å¿…é¡»å…·å¤‡ ETL/ELT æŠ€æœ¯çš„è‰¯å¥½çŸ¥è¯†ï¼Œå¹¶ä¸”è‡³å°‘èƒ½å¤Ÿä½¿ç”¨ Python ç¼–ç ã€‚æ˜¯çš„ï¼Œæ˜¾ç„¶ Python æ— ç–‘æ˜¯æ•°æ®å·¥ç¨‹æœ€æ–¹ä¾¿çš„ç¼–ç¨‹è¯­è¨€ï¼Œä½†ä½ ç”¨ Python å¯ä»¥åšçš„ä»»ä½•äº‹æƒ…ï¼Œéƒ½å¯ä»¥ç”¨å…¶ä»–è¯­è¨€è½»æ¾å®Œæˆï¼Œæ¯”å¦‚ JavaScript æˆ– Javaã€‚ä¸è¦é™åˆ¶è‡ªå·±ï¼Œä½ ä¼šæœ‰æ—¶é—´å­¦ä¹ å…¬å¸é€‰æ‹©ä½œä¸ºå…¶æŠ€æœ¯æ ˆä¸»è¦è¯­è¨€çš„ä»»ä½•è¯­è¨€ã€‚

æˆ‘å»ºè®®ä»**æ•°æ® API å’Œè¯·æ±‚**å¼€å§‹ã€‚å°†è¿™äº›çŸ¥è¯†ä¸äº‘æœåŠ¡ç»“åˆï¼Œä¸ºæœªæ¥å¯èƒ½éœ€è¦çš„ä»»ä½• ETL è¿‡ç¨‹æä¾›äº†ä¸€ä¸ªå¾ˆå¥½çš„åŸºç¡€ã€‚

> æˆ‘ä»¬ä¸èƒ½çŸ¥é“ä¸€åˆ‡ï¼Œä¹Ÿä¸è¦æ±‚æˆä¸ºç¼–ç ä¸“å®¶ï¼Œä½†æˆ‘ä»¬å¿…é¡»çŸ¥é“å¦‚ä½•å¤„ç†æ•°æ®ã€‚

ä»¥å°†æ•°æ®åŠ è½½åˆ° BigQuery æ•°æ®ä»“åº“ä¸ºä¾‹ã€‚å®ƒå°†ä½¿ç”¨ BigQuery å®¢æˆ·ç«¯åº“ [5] å°†è¡Œæ’å…¥åˆ°è¡¨ä¸­ï¼š

```py
from google.cloud import bigquery
...
client = bigquery.Client(credentials=credentials, project=credentials.project_id)
...

def _load_table_from_csv(table_schema, table_name, dataset_id):
    '''Loads data into BigQuery table from a CSV file.
    ! source file must be comma delimited CSV:
    transaction_id,user_id,total_cost,dt
    1,1,10.99,2023-04-15
    blob = """transaction_id,user_id,total_cost,dt\n1,1,10.99,2023-04-15"""
    '''

    blob = """transaction_id,user_id,total_cost,dt
    1,1,10.99,2023-04-15
    2,2, 4.99,2023-04-12
    4,1, 4.99,2023-04-12
    5,1, 5.99,2023-04-14
    6,1,15.99,2023-04-14
    7,1,55.99,2023-04-14"""

    data_file = io.BytesIO(blob.encode())

    print(blob)
    print(data_file)

    table_id = client.dataset(dataset_id).table(table_name)
    job_config = bigquery.LoadJobConfig()
    schema = create_schema_from_yaml(table_schema)
    job_config.schema = schema

    job_config.source_format = bigquery.SourceFormat.CSV,
    job_config.write_disposition = 'WRITE_APPEND',
    job_config.field_delimiter =","
    job_config.null_marker ="null",
    job_config.skip_leading_rows = 1

    load_job = client.load_table_from_file(
        data_file,
        table_id,
        job_config=job_config,
        )

    load_job.result()
    print("Job finished.")
```

æˆ‘ä»¬ä¸èƒ½çŸ¥é“ä¸€åˆ‡ï¼Œä¹Ÿä¸è¦æ±‚æˆä¸ºç¼–ç ä¸“å®¶ï¼Œä½†æˆ‘ä»¬å¿…é¡»çŸ¥é“å¦‚ä½•å¤„ç†æ•°æ®ã€‚

æˆ‘ä»¬å¯ä»¥åœ¨æœ¬åœ°è¿è¡Œå®ƒï¼Œä¹Ÿå¯ä»¥å°†å…¶éƒ¨ç½²åˆ°äº‘ç«¯ä½œä¸ºæ— æœåŠ¡å™¨åº”ç”¨ç¨‹åºã€‚å®ƒå¯ä»¥ç”±æˆ‘ä»¬é€‰æ‹©çš„ä»»ä½•å…¶ä»–æœåŠ¡è§¦å‘ã€‚ä¾‹å¦‚ï¼Œéƒ¨ç½² AWS Lambda æˆ– GCP Cloud Function ä¼šéå¸¸é«˜æ•ˆã€‚å®ƒå°†è½»æ¾å¤„ç†æˆ‘ä»¬çš„æ•°æ®ç®¡é“äº‹ä»¶ï¼Œå‡ ä¹ä¸äº§ç”Ÿæˆæœ¬ã€‚æˆ‘çš„åšå®¢ä¸­æœ‰å¾ˆå¤šæ–‡ç« è§£é‡Šäº†å®ƒçš„ç®€ä¾¿æ€§å’Œçµæ´»æ€§ã€‚

## Airflowã€Airbyteã€Luigiã€Hudiâ€¦â€¦

ä½¿ç”¨å¸®åŠ©ç®¡ç†æ•°æ®å¹³å°å’Œç¼–æ’æ•°æ®ç®¡é“çš„ç¬¬ä¸‰æ–¹æ¡†æ¶å’Œåº“è¿›è¡Œå®éªŒã€‚è®¸å¤šæ¡†æ¶æ˜¯å¼€æºçš„ï¼Œå¦‚ Apache Hudi [6]ï¼Œå¹¶ä¸”ä»ä¸åŒè§’åº¦å¸®åŠ©ç†è§£æ•°æ®å¹³å°ç®¡ç†ã€‚å®ƒä»¬ä¸­çš„è®¸å¤šåœ¨ç®¡ç†æ‰¹å¤„ç†å’Œæµå¤„ç†å·¥ä½œè´Ÿè½½æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚æˆ‘é€šè¿‡ä½¿ç”¨å®ƒä»¬å­¦åˆ°äº†å¾ˆå¤šä¸œè¥¿ã€‚ä¾‹å¦‚ï¼ŒApache Airflow æä¾›äº†å¾ˆå¤šç°æˆçš„æ•°æ®è¿æ¥å™¨ã€‚æˆ‘ä»¬å¯ä»¥ä½¿ç”¨å®ƒä»¬è½»æ¾åœ°è¿è¡Œä»»ä½•äº‘ä¾›åº”å•†ï¼ˆAWSã€GCPã€Azureï¼‰çš„ ETL ä»»åŠ¡ã€‚

ä½¿ç”¨è¿™äº›æ¡†æ¶åˆ›å»ºæ‰¹å¤„ç†æ•°æ®å¤„ç†ä½œä¸šéå¸¸å®¹æ˜“ã€‚å¦‚æœæˆ‘ä»¬æ·±å…¥äº†è§£ï¼Œå®ƒç¡®å®å¯ä»¥ä½¿ ETL çš„å®é™…æ“ä½œæ›´åŠ æ¸…æ™°ã€‚

ä¾‹å¦‚ï¼Œæˆ‘ä½¿ç”¨ airflow è¿æ¥å™¨æ„å»ºäº†ä¸€ä¸ªæœºå™¨å­¦ä¹ ç®¡é“æ¥è®­ç»ƒæ¨èå¼•æ“ï¼š

```py
 """DAG definition for recommendation_bespoke model training."""

import airflow
from airflow import DAG
from airflow.contrib.operators.bigquery_operator import BigQueryOperator
from airflow.contrib.operators.bigquery_to_gcs import BigQueryToCloudStorageOperator
from airflow.hooks.base_hook import BaseHook
from airflow.operators.app_engine_admin_plugin import AppEngineVersionOperator
from airflow.operators.ml_engine_plugin import MLEngineTrainingOperator

import datetime

def _get_project_id():
  """Get project ID from default GCP connection."""

  extras = BaseHook.get_connection('google_cloud_default').extra_dejson
  key = 'extra__google_cloud_platform__project'
  if key in extras:
    project_id = extras[key]
  else:
    raise ('Must configure project_id in google_cloud_default '
           'connection from Airflow Console')
  return project_id

PROJECT_ID = _get_project_id()

# Data set constants, used in BigQuery tasks.  You can change these
# to conform to your data.
DATASET = 'staging' #'analytics'
TABLE_NAME = 'recommendation_bespoke'

# GCS bucket names and region, can also be changed.
BUCKET = 'gs://rec_wals_eu'
REGION = 'us-central1' #'europe-west2' #'us-east1'
JOB_DIR = BUCKET + '/jobs'

default_args = {
    'owner': 'airflow',
    'depends_on_past': False,
    'start_date': airflow.utils.dates.days_ago(2),
    'email': ['mike.shakhomirov@gmail.com'],
    'email_on_failure': True,
    'email_on_retry': False,
    'retries': 5,
    'retry_delay': datetime.timedelta(minutes=5)
}

# Default schedule interval using cronjob syntax - can be customized here
# or in the Airflow console.
schedule_interval = '00 21 * * *'

dag = DAG('recommendations_training_v6', default_args=default_args,
          schedule_interval=schedule_interval)

dag.doc_md = __doc__

#
#
# Task Definition
#
#

# BigQuery training data export to GCS

training_file = BUCKET + '/data/recommendations_small.csv' # just a few records for staging

t1 = BigQueryToCloudStorageOperator(
    task_id='bq_export_op',
    source_project_dataset_table='%s.recommendation_bespoke' % DATASET,
    destination_cloud_storage_uris=[training_file],
    export_format='CSV',
    dag=dag
)

# ML Engine training job
training_file = BUCKET + '/data/recommendations_small.csv'
job_id = 'recserve_{0}'.format(datetime.datetime.now().strftime('%Y%m%d%H%M'))
job_dir = BUCKET + '/jobs/' + job_id
output_dir = BUCKET
delimiter=','
data_type='user_groups'
master_image_uri='gcr.io/my-project/recommendation_bespoke_container:tf_rec_latest'

training_args = ['--job-dir', job_dir,
                 '--train-file', training_file,
                 '--output-dir', output_dir,
                 '--data-type', data_type]

master_config = {"imageUri": master_image_uri,}

t3 = MLEngineTrainingOperator(
    task_id='ml_engine_training_op',
    project_id=PROJECT_ID,
    job_id=job_id,
    training_args=training_args,
    region=REGION,
    scale_tier='CUSTOM',
    master_type='complex_model_m_gpu',
    master_config=master_config,
    dag=dag
)

t3.set_upstream(t1) 
```

å®ƒå°†åˆ›å»ºä¸€ä¸ªç®€å•çš„æ•°æ®ç®¡é“å›¾ï¼Œå°†æ•°æ®å¯¼å‡ºåˆ°äº‘å­˜å‚¨æ¡¶ä¸­ï¼Œç„¶åä½¿ç”¨ MLEngineTrainingOperator è®­ç»ƒ ML æ¨¡å‹ã€‚

![](img/e316a43ba542db4191a0a78ccfa00721.png)

ä½¿ç”¨ Airflow è¿›è¡Œ ML æ¨¡å‹è®­ç»ƒã€‚å›¾ç‰‡ç”±ä½œè€…æä¾›ã€‚

## ç¼–æ’æ•°æ®ç®¡é“

æ¡†æ¶éå¸¸é‡è¦ï¼Œä½†æ•°æ®å·¥ç¨‹å¸ˆå¿…é¡»çŸ¥é“å¦‚ä½•åˆ›å»ºè‡ªå·±çš„æ¡†æ¶æ¥ç¼–æ’æ•°æ®ç®¡é“ã€‚è¿™å°†æˆ‘ä»¬å¸¦å›åˆ°åŸå§‹çš„åŸºç¡€ç¼–ç ä»¥åŠä½¿ç”¨å®¢æˆ·ç«¯åº“å’Œ APIã€‚åœ¨è¿™é‡Œï¼Œä¸€ä¸ªå¥½çš„å»ºè®®æ˜¯ç†Ÿæ‚‰æ•°æ®å·¥å…·åŠå…¶ API ç«¯ç‚¹ã€‚é€šå¸¸ï¼Œåˆ›å»ºå¹¶éƒ¨ç½²æˆ‘ä»¬è‡ªå·±çš„å¾®æœåŠ¡æ¥æ‰§è¡Œ ETL/ELT ä»»åŠ¡æ›´åŠ ç›´è§‚å’Œå®¹æ˜“ã€‚

> ä½¿ç”¨ä½ è‡ªå·±çš„å·¥å…·ç¼–æ’æ•°æ®ç®¡é“

ä¾‹å¦‚ï¼Œæˆ‘ä»¬å¯ä»¥åˆ›å»ºä¸€ä¸ªç®€å•çš„æ— æœåŠ¡å™¨åº”ç”¨ç¨‹åºï¼Œå®ƒå°†ä»æ¶ˆæ¯ä»£ç†ï¼ˆå¦‚ SNSï¼‰ä¸­è·å–æ•°æ®ã€‚ç„¶åï¼Œå®ƒå¯ä»¥å¤„ç†è¿™äº›äº‹ä»¶å¹¶ç¼–æ’æˆ‘ä»¬åˆ›å»ºçš„å…¶ä»–å¾®æœåŠ¡æ¥æ‰§è¡Œ ETL ä»»åŠ¡ã€‚å¦ä¸€ä¸ªä¾‹å­æ˜¯ä¸€ä¸ªç®€å•çš„ AWS Lambdaï¼Œå®ƒç”±æ•°æ®æ¹–ä¸­åˆ›å»ºçš„æ–°æ–‡ä»¶è§¦å‘ï¼Œç„¶åæ ¹æ®å®ƒä»ç®¡é“é…ç½®æ–‡ä»¶ä¸­è¯»å–çš„ä¿¡æ¯ï¼Œå®ƒå¯ä»¥å†³å®šè°ƒç”¨å“ªä¸ªæœåŠ¡æˆ–å°†æ•°æ®åŠ è½½åˆ°å“ªä¸ªè¡¨ä¸­ã€‚

è¯·è€ƒè™‘ä¸‹é¢çš„è¿™ä¸ªåº”ç”¨ç¨‹åºã€‚å®ƒæ˜¯ä¸€ä¸ªéå¸¸ç®€å•çš„ AWS Lambdaï¼Œå¯ä»¥åœ¨æœ¬åœ°è¿è¡Œæˆ–åœ¨äº‘ä¸­éƒ¨ç½²æ—¶è¿è¡Œã€‚

```py
./stack
â”œâ”€â”€ deploy.sh                 # Shell script to deploy the Lambda
â”œâ”€â”€ stack.yaml                # Cloudformation template
â”œâ”€â”€ pipeline_manager
|   â”œâ”€â”€ env.json              # enviroment variables                
â”‚   â””â”€â”€ app.py                # Application
â”œâ”€â”€ response.json             # Lambda response when invoked locally
â””â”€â”€ stack.zip                 # Lambda package
```

`app.py` å¯ä»¥æ˜¯ä»»ä½• ETL ä»»åŠ¡ï¼Œæˆ‘ä»¬åªéœ€æ·»åŠ ä¸€äº›é€»è¾‘ï¼Œå°±åƒåœ¨å‰é¢çš„ç¤ºä¾‹ä¸­ä½¿ç”¨å‡ ä¸ªå®¢æˆ·ç«¯åº“å°†æ•°æ®åŠ è½½åˆ° BigQuery ä¸­ä¸€æ ·ï¼š

```py
# ./pipeline_manager/app.py
def lambda_handler(event, context):
   message = 'Hello {} {}!'.format(event['first_name'], event['last_name']) 
   return {
       'message' : message
   }
```

ç°åœ¨æˆ‘ä»¬å¯ä»¥åœ¨æœ¬åœ°è¿è¡Œå®ƒæˆ–ä½¿ç”¨åŸºç¡€è®¾æ–½å³ä»£ç è¿›è¡Œéƒ¨ç½²ã€‚è¿™ä¸ªå‘½ä»¤è¡Œè„šæœ¬å°†åœ¨æœ¬åœ°è¿è¡Œæ­¤æœåŠ¡ï¼š

```py
pip install python-lambda-local
cd stack
python-lambda-local -e pipeline_manager/env.json -f lambda_handler pipeline_manager/app.py event.json
```

å¦å¤–ï¼Œå®ƒä¹Ÿå¯ä»¥éƒ¨ç½²åˆ°äº‘ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥ä»é‚£é‡Œè°ƒç”¨å®ƒã€‚è¿™å°†æˆ‘ä»¬å¸¦å…¥äº†äº‘ç¯å¢ƒã€‚

## äº‘æœåŠ¡æä¾›å•†

ç°åœ¨ä¸€åˆ‡éƒ½åœ¨äº‘ç«¯ç®¡ç†ã€‚è¿™å°±æ˜¯ä¸ºä»€ä¹ˆè‡³å°‘å­¦ä¹ ä¸€ç§ä¾›åº”å•†çš„æœåŠ¡è‡³å…³é‡è¦ã€‚å¯ä»¥æ˜¯ AWSã€GCP æˆ– Azureã€‚å®ƒä»¬æ˜¯é¢†å¯¼è€…ï¼Œæˆ‘ä»¬å¸Œæœ›ä¸“æ³¨äºå…¶ä¸­ä¹‹ä¸€ã€‚è·å¾—äº‘è®¤è¯æ˜¯ç†æƒ³çš„ï¼Œæ¯”å¦‚â€œ[Google Cloud Professional Data Engineer](https://cloud.google.com/learn/certification/data-engineer)â€ [7] æˆ–ç±»ä¼¼çš„è®¤è¯ã€‚è¿™äº›è€ƒè¯•å¾ˆéš¾ï¼Œä½†å€¼å¾—è·å¾—ï¼Œå› ä¸ºå®ƒæä¾›äº†æ•°æ®å¤„ç†å·¥å…·çš„è‰¯å¥½æ¦‚è¿°ï¼Œå¹¶ä½¿æˆ‘ä»¬çœ‹èµ·æ¥éå¸¸å¯ä¿¡ã€‚æˆ‘è€ƒè¿‡ä¸€æ¬¡ï¼Œå·²åœ¨æ–‡ç« ä¸­è®°å½•äº†æˆ‘çš„ç»éªŒï¼Œä½ å¯ä»¥åœ¨æˆ‘çš„æ•…äº‹ä¸­æ‰¾åˆ°ã€‚

æ•°æ®å·¥ç¨‹å¸ˆä½¿ç”¨äº‘åŠŸèƒ½å’Œ/æˆ– docker åˆ›å»ºçš„æ‰€æœ‰å†…å®¹éƒ½å¯ä»¥éƒ¨ç½²åœ¨äº‘ä¸­ã€‚è€ƒè™‘ä»¥ä¸‹ AWS CloudFormation å †æ ˆæ¨¡æ¿ã€‚æˆ‘ä»¬å¯ä»¥ä½¿ç”¨å®ƒæ¥éƒ¨ç½²æˆ‘ä»¬çš„ç®€å• ETL å¾®æœåŠ¡ï¼š

```py
# stack.yaml
AWSTemplateFormatVersion: '2010-09-09'
Description: AWS S3 data lake stack.
Parameters:

  ServiceName:
    Description: Data lake microservice to process data files and load them into __ BigQuery.
    Type: String
    Default: datalake-manager
  StackPackageS3Key:
    Type: String
    Default: pipeline_manager/stack.zip
  Testing:
    Type: String
    Default: 'false'
    AllowedValues: ['true','false']
  Environment:
    Type: String
    Default: 'staging'
    AllowedValues: ['staging','live','test']
  AppFolder:
    Description: app.py file location inside the package, i.e. ./stack/pipeline_manager/app.py.
    Type: String
    Default: pipeline_manager
  LambdaCodeLocation:
    Description: Lambda package file location.
    Type: String
    Default: datalake-lambdas.aws

Resources:

  PipelineManagerLambda:
    Type: AWS::Lambda::Function
    DeletionPolicy: Delete
    DependsOn: LambdaPolicy
    Properties:
      FunctionName: !Join ['-', [!Ref ServiceName, !Ref Environment] ] # pipeline-manager-staging if staging.
      Handler: !Sub '${AppFolder}/app.lambda_handler'
      Description: Microservice that orchestrates data loading into BigQuery from AWS to BigQuery project your-project-name.schema.
      Environment:
        Variables:
          DEBUG: true
          LAMBDA_PATH: !Sub '${AppFolder}/' # i.e. 'pipeline_manager/'
          TESTING: !Ref Testing
          ENV: !Ref Environment
      Role: !GetAtt LambdaRole.Arn
      Code:
        S3Bucket: !Sub '${LambdaCodeLocation}' #datalake-lambdas.aws
        S3Key:
          Ref: StackPackageS3Key
      Runtime: python3.8
      Timeout: 360
      MemorySize: 128
      Tags:
        -
          Key: Service
          Value: Datalake

  LambdaRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: "2012-10-17"
        Statement:
          -
            Effect: Allow
            Principal:
              Service:
                - "lambda.amazonaws.com"
            Action:
              - "sts:AssumeRole"

  LambdaPolicy:
    Type: AWS::IAM::Policy
    DependsOn: LambdaRole
    Properties:
      Roles:
        - !Ref LambdaRole
      PolicyName: 'pipeline-manager-lambda-policy'
      PolicyDocument:
        {
            "Version": "2012-10-17",
            "Statement": [
                {
                    "Effect": "Allow",
                    "Action": [
                        "logs:CreateLogGroup",
                        "logs:CreateLogStream",
                        "logs:PutLogEvents"
                    ],
                    "Resource": "*"
                }
            ]
        }
```

å¦‚æœæˆ‘ä»¬åœ¨å‘½ä»¤è¡Œä¸­è¿è¡Œè¿™ä¸ª shell è„šæœ¬ï¼Œå®ƒå°†éƒ¨ç½²æˆ‘ä»¬çš„æœåŠ¡å’Œæ‰€æœ‰æ‰€éœ€çš„èµ„æºï¼Œå¦‚ IAM ç­–ç•¥åˆ°äº‘ä¸­ï¼š

```py
aws \
cloudformation deploy \
--template-file stack.yaml \
--stack-name $STACK_NAME \
--capabilities CAPABILITY_IAM \
--parameter-overrides \
"StackPackageS3Key"="${APP_FOLDER}/${base}${TIME}.zip" \
"AppFolder"=$APP_FOLDER \
"LambdaCodeLocation"=$LAMBDA_BUCKET \
"Environment"="staging" \
"Testing"="false"
```

ä¹‹åï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ SDK é€šè¿‡åœ¨å‘½ä»¤è¡Œå·¥å…·ä¸­è¿è¡Œæ­¤ CLI å‘½ä»¤æ¥è°ƒç”¨æˆ‘ä»¬çš„æœåŠ¡ï¼š

```py
aws lambda invoke \
    --function-name pipeline-manager \
    --payload '{ "first_name": "something" }' \
    response.json
```

## ç²¾é€šå‘½ä»¤è¡Œå·¥å…·

äº‘ä¾›åº”å•†çš„å‘½ä»¤è¡Œå·¥å…·éå¸¸æœ‰ç”¨ï¼Œæœ‰åŠ©äºåˆ›å»ºæµ‹è¯•æˆ‘ä»¬åœ¨äº‘ä¸­éƒ¨ç½²çš„ ETL æœåŠ¡çš„è„šæœ¬ã€‚æ•°æ®å·¥ç¨‹å¸ˆç»å¸¸ä½¿ç”¨å®ƒã€‚ä¸æ•°æ®æ¹–ä¸€èµ·å·¥ä½œæ—¶ï¼Œæˆ‘ä»¬å¸Œæœ›æŒæ¡å¸®åŠ©æˆ‘ä»¬ç®¡ç†äº‘å­˜å‚¨çš„ CLI å‘½ä»¤ï¼Œå³ä¸Šä¼ ã€ä¸‹è½½å’Œå¤åˆ¶æ–‡ä»¶å’Œå¯¹è±¡ã€‚ä¸ºä»€ä¹ˆæˆ‘ä»¬è¦è¿™æ ·åšï¼Ÿå› ä¸ºäº‘å­˜å‚¨ä¸­çš„æ–‡ä»¶ç»å¸¸è§¦å‘å„ç§ ETL æœåŠ¡ã€‚æ‰¹å¤„ç†æ˜¯ä¸€ä¸ªéå¸¸å¸¸è§çš„æ•°æ®è½¬æ¢æ¨¡å¼ï¼Œä¸ºäº†è°ƒæŸ¥é”™è¯¯å’Œé—®é¢˜ï¼Œæˆ‘ä»¬å¯èƒ½éœ€è¦åœ¨æ¡¶ä¹‹é—´ä¸‹è½½æˆ–å¤åˆ¶æ–‡ä»¶ã€‚

![](img/c59684bded8474031b90c752aed36ea9.png)

åœ¨æ•°æ®æ¹–å¯¹è±¡ä¸Šä½¿ç”¨ AWS Lambda è¿›è¡Œ ETLã€‚ä½œè€…æä¾›çš„å›¾åƒã€‚

åœ¨è¿™ä¸ªç¤ºä¾‹ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°æœåŠ¡å°†æ•°æ®è¾“å‡ºåˆ° Kinesisï¼Œç„¶åå­˜å‚¨åœ¨æ•°æ®æ¹–ä¸­ã€‚å½“ S3 ä¸­åˆ›å»ºæ–‡ä»¶å¯¹è±¡æ—¶ï¼Œå®ƒä»¬ä¼šè§¦å‘ç”± AWS Lambda å¤„ç†çš„ ETL è¿‡ç¨‹ã€‚ç»“æœè¢«ä¿å­˜åˆ° S3 æ¡¶ä¸­ï¼Œä»¥ä¾› AWS Athena ä½¿ç”¨ï¼Œä»è€Œç”Ÿæˆä¸€ä¸ªä½¿ç”¨ AWS Quicksight çš„ BI æŠ¥å‘Šã€‚

è¿™æ˜¯æˆ‘ä»¬å¯èƒ½åœ¨æŸä¸ªæ—¶åˆ»æƒ³è¦ä½¿ç”¨çš„ä¸€ç»„ AWS CLI å‘½ä»¤ï¼š

## å¤åˆ¶å’Œä¸Šä¼ æ–‡ä»¶

```py
mkdir data
cd data
echo transaction_id,user_id,dt \\n101,777,2021-08-01\\n102,777,2021-08-01\\n103,777,2021-08-01\\n > simple_transaction.csv
aws  s3 cp ./data/simple_transaction.csv s3://your.bucket.aws/simple_transaction_from_data.csv
```

## é€’å½’å¤åˆ¶/ä¸Šä¼ /ä¸‹è½½æ–‡ä»¶å¤¹ä¸­çš„æ‰€æœ‰æ–‡ä»¶

```py
aws  s3 cp ./data s3://your.bucket.aws --recursive
```

## é€’å½’åˆ é™¤æ‰€æœ‰å†…å®¹

```py
aws  s3 rm s3://your.bucket.aws/ --recursive --exclude ""
```

## åˆ é™¤ä¸€ä¸ªæ¡¶

```py
aws  s3 rb s3://your.bucket.aws/
```

è¿˜æœ‰æ›´å¤šé«˜çº§ç¤ºä¾‹ï¼Œä½†æˆ‘è®¤ä¸ºè¿™ä¸ªæ¦‚å¿µå·²ç»å¾ˆæ¸…æ¥šäº†ã€‚

> æˆ‘ä»¬å¸Œæœ›ä½¿ç”¨è„šæœ¬æœ‰æ•ˆåœ°ç®¡ç†äº‘å­˜å‚¨ã€‚

æˆ‘ä»¬å¯ä»¥å°†è¿™äº›å‘½ä»¤é“¾æ¥æˆ shell è„šæœ¬ï¼Œè¿™ä½¿å¾— CLI æˆä¸ºä¸€ä¸ªéå¸¸å¼ºå¤§çš„å·¥å…·ã€‚

ä¾‹å¦‚ï¼Œè€ƒè™‘è¿™ä¸ª shell è„šæœ¬ã€‚å®ƒå°†æ£€æŸ¥æ˜¯å¦å­˜åœ¨ lambda åŒ…çš„å­˜å‚¨æ¡¶ï¼Œä¸Šä¼ å¹¶éƒ¨ç½²æˆ‘ä»¬çš„ ETL æœåŠ¡ä½œä¸º Lambda å‡½æ•°ï¼š

```py
# ./deploy.sh
# Run ./deploy.sh
LAMBDA_BUCKET=$1 # your-lambda-packages.aws
STACK_NAME=SimpleETLService
APP_FOLDER=pipeline_manager
# Get date and time to create unique s3-key for deployment package:
date
TIME=`date +"%Y%m%d%H%M%S"`
# Get the name of the base application folder, i.e. pipeline_manager.
base=${PWD##*/}
# Use this name to name zip:
zp=$base".zip"
echo $zp
# Remove old package if exists:
rm -f $zp
# Package Lambda
zip -r $zp "./${APP_FOLDER}" -x deploy.sh

# Check if Lambda bucket exists:
LAMBDA_BUCKET_EXISTS=$(aws  s3 ls ${LAMBDA_BUCKET} --output text)
#  If NOT:
if [[ $? -eq 254 ]]; then
    # create a bucket to keep Lambdas packaged files:
    echo  "Creating Lambda code bucket ${LAMBDA_BUCKET} "
    CREATE_BUCKET=$(aws  s3 mb s3://${LAMBDA_BUCKET} --output text)
    echo ${CREATE_BUCKET}
fi

# Upload the package to S3:
aws s3 cp ./${base}.zip s3://${LAMBDA_BUCKET}/${APP_FOLDER}/${base}${TIME}.zip

# Deploy / Update:
aws --profile $PROFILE \
cloudformation deploy \
--template-file stack.yaml \
--stack-name $STACK_NAME \
--capabilities CAPABILITY_IAM \
--parameter-overrides \
"StackPackageS3Key"="${APP_FOLDER}/${base}${TIME}.zip" \
"AppFolder"=$APP_FOLDER \
"LambdaCodeLocation"=$LAMBDA_BUCKET \
"Environment"="staging" \
"Testing"="false"
```

æ›´é«˜çº§çš„ç¤ºä¾‹å¯ä»¥åœ¨æˆ‘ä¹‹å‰çš„æ•…äº‹ä¸­æ‰¾åˆ°ã€‚

## æ•°æ®è´¨é‡

ç°åœ¨ï¼Œå½“æˆ‘ä»¬çŸ¥é“å¦‚ä½•éƒ¨ç½² ETL æœåŠ¡ã€æ‰§è¡Œè¯·æ±‚å¹¶ä»å¤–éƒ¨ API æ‹‰å–æ•°æ®æ—¶ï¼Œæˆ‘ä»¬éœ€è¦å­¦ä¹ å¦‚ä½•è§‚å¯Ÿæˆ‘ä»¬åœ¨æ•°æ®å¹³å°ä¸Šæ‹¥æœ‰çš„æ•°æ®ã€‚ç†æƒ³æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å¸Œæœ›åœ¨æ•°æ®æµå…¥æ•°æ®å¹³å°æ—¶å®æ—¶æ£€æŸ¥æ•°æ®è´¨é‡ã€‚å¯ä»¥é€šè¿‡ ETL æˆ– ELT æ–¹æ³•æ¥å®ç°ã€‚ä½¿ç”¨ Kafka æˆ– Kinesis æ„å»ºçš„æµåº”ç”¨ç¨‹åºå…·æœ‰åˆ†ææ•°æ®è´¨é‡çš„åº“ï¼Œæ•°æ®åœ¨æ•°æ®ç®¡é“ä¸­æµåŠ¨æ—¶å¯ä»¥è¿›è¡Œåˆ†æã€‚å½“æ•°æ®å·¥ç¨‹å¸ˆå°†æ•°æ®å¯è§‚å¯Ÿæ€§å’Œæ•°æ®è´¨é‡ç®¡ç†å§”æ´¾ç»™å…¶ä»–åœ¨æ•°æ®ä»“åº“ä¸­å·¥ä½œçš„åˆ©ç›Šç›¸å…³è€…æ—¶ï¼ŒELT æ–¹æ³•æ›´ä¸ºå¯å–ã€‚å°±ä¸ªäººè€Œè¨€ï¼Œæˆ‘å–œæ¬¢åè€…ï¼Œå› ä¸ºå®ƒèŠ‚çœäº†æ—¶é—´ã€‚å°†æ•°æ®ä»“åº“è§£å†³æ–¹æ¡ˆè§†ä¸ºå…¬å¸ä¸­æ¯ä¸ªäººçš„å•ä¸€çœŸå®æ¥æºã€‚è´¢åŠ¡ã€å¸‚åœºè¥é”€å’Œå®¢æˆ·æœåŠ¡å›¢é˜Ÿå¯ä»¥è®¿é—®æ•°æ®å¹¶æ£€æŸ¥**ä»»ä½•æ½œåœ¨é—®é¢˜**ã€‚è¿™äº›ä¸­æˆ‘ä»¬é€šå¸¸ä¼šçœ‹åˆ°ä»¥ä¸‹æƒ…å†µï¼š

+   ç¼ºå¤±æ•°æ®

+   æ•°æ®æºä¸­æ–­

+   æ•°æ®æºåœ¨æ¨¡å¼å­—æ®µæ›´æ–°æ—¶å‘ç”Ÿå˜åŒ–

+   å„ç§æ•°æ®å¼‚å¸¸ï¼Œå¦‚å¼‚å¸¸å€¼æˆ–ä¸å¯»å¸¸çš„åº”ç”¨ç¨‹åº/ç”¨æˆ·è¡Œä¸ºã€‚

æ•°æ®å·¥ç¨‹å¸ˆåˆ›å»ºè­¦æŠ¥å¹¶å®‰æ’é€šçŸ¥ï¼Œä»¥ä¾¿å¯¹æ½œåœ¨çš„æ•°æ®é—®é¢˜ä¿æŒå…³æ³¨ã€‚

è€ƒè™‘è¿™ä¸ªä¾‹å­ï¼Œå½“æ¯å¤©å‘é€é‚®ä»¶é€šçŸ¥åˆ©ç›Šç›¸å…³è€…æœ‰å…³æ•°æ®ä¸­æ–­çš„æƒ…å†µï¼š

![](img/c9b2a060d7071e8e6c95c403b0901690.png)

é‚®ä»¶è­¦æŠ¥ã€‚å›¾ç‰‡ç”±ä½œè€…æä¾›ã€‚

åœ¨æˆ‘çš„æ•…äº‹ä¸­ï¼Œä½ å¯ä»¥æ‰¾åˆ°ä¸€ç¯‡æ–‡ç« ï¼Œè§£é‡Šå¦‚ä½•ä½¿ç”¨ SQL å®‰æ’è¿™æ ·çš„æ•°æ®ç›‘æ§å·¥ä½œæµã€‚

[](/automated-emails-and-data-quality-checks-for-your-data-1de86ed47cf0?source=post_page-----c0319cb226c2--------------------------------) ## è‡ªåŠ¨åŒ–é‚®ä»¶å’Œæ•°æ®è´¨é‡æ£€æŸ¥

### æ•°æ®ä»“åº“æŒ‡å—ä»¥ä¾¿æ›´å¥½ã€æ›´å¹²å‡€çš„æ•°æ®å’Œå®šæœŸé‚®ä»¶

towardsdatascience.com

è€ƒè™‘ä»¥ä¸‹ä»£ç ç‰‡æ®µã€‚å®ƒå°†æ£€æŸ¥æ˜¨å¤©çš„æ•°æ®æ˜¯å¦æœ‰ä»»ä½•ç¼ºå¤±å­—æ®µï¼Œå¹¶åœ¨å‘ç°æ—¶å‘é€é€šçŸ¥è­¦æŠ¥ï¼š

```py
with checks as (
    select
      count( transaction_id )                                                           as t_cnt
    , count(distinct transaction_id)                                                    as t_cntd
    , count(distinct (case when payment_date is null then transaction_id end))          as pmnt_date_null
    from
        production.user_transaction
)
, row_conditions as (

    select if(t_cnt = 0,'Data for yesterday missing; ', NULL)  as alert from checks
union all
    select if(t_cnt != t_cntd,'Duplicate transactions found; ', NULL) from checks
union all
    select if(pmnt_date_null != 0, cast(pmnt_date_null as string )||' NULL payment_date found', NULL) from checks
)

, alerts as (
select
        array_to_string(
            array_agg(alert IGNORE NULLS) 
        ,'.; ')                                         as stringify_alert_list

    ,   array_length(array_agg(alert IGNORE NULLS))     as issues_found
from
    row_conditions
)

select
    alerts.issues_found,
    if(alerts.issues_found is null, 'all good'
        , ERROR(FORMAT('ATTENTION: production.user_transaction has potential data quality issues for yesterday: %t. Check dataChecks.check_user_transaction_failed_v for more info.'
        , stringify_alert_list)))
from
    alerts
;
```

## æ•°æ®ç¯å¢ƒ

æ•°æ®å·¥ç¨‹å¸ˆæµ‹è¯•æ•°æ®ç®¡é“ã€‚æœ‰å¤šç§æ–¹æ³•æ¥å®ç°è¿™ä¸€ç‚¹ã€‚é€šå¸¸ï¼Œå®ƒéœ€è¦å°†æ•°æ®ç¯å¢ƒåˆ†ä¸ºç”Ÿäº§å’Œé¢„å‘å¸ƒç®¡é“ã€‚æˆ‘ä»¬é€šå¸¸å¯èƒ½éœ€è¦ä¸€ä¸ªé¢å¤–çš„æ²™ç›’ç”¨äºæµ‹è¯•ç›®çš„ï¼Œæˆ–è€…åœ¨æˆ‘ä»¬çš„ ETL æœåŠ¡è§¦å‘ CI/CD å·¥ä½œæµæ—¶è¿è¡Œæ•°æ®è½¬æ¢å•å…ƒæµ‹è¯•ã€‚

è¿™æ˜¯ä¸€ç§å¸¸è§åšæ³•ï¼Œé¢è¯•å®˜å¯èƒ½ä¼šé—®ä¸€äº›ç›¸å…³é—®é¢˜ã€‚åˆšå¼€å§‹å¯èƒ½æœ‰ç‚¹æ£˜æ‰‹ï¼Œä½†ä¸‹é¢çš„å›¾è§£è§£é‡Šäº†å®ƒæ˜¯å¦‚ä½•å·¥ä½œçš„ã€‚

![](img/081619c52103b0f55e7e9c6dca8bd0f8.png)

CI/CD å·¥ä½œæµç¤ºä¾‹ï¼Œç”¨äº ETL æœåŠ¡ã€‚å›¾ç‰‡ç”±ä½œè€…æä¾›ã€‚

ä¾‹å¦‚ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨åŸºç¡€è®¾æ–½å³ä»£ç å’Œ GitHub Actions æ¥éƒ¨ç½²å’Œæµ‹è¯•ä»»ä½•æ¥è‡ªå¼€å‘åˆ†æ”¯çš„æ‹‰å–è¯·æ±‚ä¸­çš„é¢„å‘å¸ƒèµ„æºã€‚å½“æ‰€æœ‰æµ‹è¯•é€šè¿‡ä¸”æˆ‘ä»¬å¯¹ ETL æœåŠ¡æ»¡æ„æ—¶ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡åˆå¹¶åˆ°ä¸»åˆ†æ”¯å°†å…¶æå‡åˆ°ç”Ÿäº§ç¯å¢ƒã€‚

è¯·çœ‹ä¸‹é¢è¿™ä¸ª GitHub Action å·¥ä½œæµã€‚å®ƒå°†ä¼šåœ¨é¢„ç”Ÿäº§ç¯å¢ƒä¸­éƒ¨ç½²æˆ‘ä»¬çš„ ETL æœåŠ¡å¹¶è¿›è¡Œæµ‹è¯•ã€‚è¿™æ ·çš„åšæ³•æœ‰åŠ©äºå‡å°‘é”™è¯¯å¹¶æ›´å¿«åœ°äº¤ä»˜æ•°æ®ç®¡é“ã€‚

```py
# .github/workflows/deploy_staging.yaml
name: STAGING AND TESTS

on:
  #when there is a push to the master
  push:
    branches: [ master ]
  #when there is a pull to the master
  pull_request:
    branches: [ master ]

jobs:
 compile:
   runs-on: ubuntu-latest
   steps:
     - name: Checkout code into workspace directory
       uses: actions/checkout@v2
     - name: Install AWS CLI v2
       run:  |
             curl "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip" -o /tmp/awscliv2.zip
             unzip -q /tmp/awscliv2.zip -d /tmp
             rm /tmp/awscliv2.zip
             sudo /tmp/aws/install --update
             rm -rf /tmp/aws/
     - name: test AWS connectivity
       run: aws s3 ls
       env:
         AWS_ACCESS_KEY_ID: ${{ secrets.MDS_AWS_ACCESS_KEY_ID }}
         AWS_SECRET_ACCESS_KEY: ${{ secrets.MDS_AWS_SECRET_ACCESS_KEY }}
         AWS_DEFAULT_REGION: "eu-west-1"

     - name: Deploy staging
       run: |
            cd stack
            date
            TIME=`date +"%Y%m%d%H%M%S"`
            base=${PWD##*/}
            zp=$base".zip"
            rm -f $zp
            pip install --target ./package pyyaml==6.0
            cd package
            zip -r ../${base}.zip .
            cd $OLDPWD
            zip -r $zp ./pipeline_manager
            # Upload Lambda code (replace with your S3 bucket):
            aws s3 cp ./${base}.zip s3://datalake-lambdas.aws/pipeline_manager/${base}${TIME}.zip
            STACK_NAME=SimpleCICDWithLambdaAndRole
            aws \
            cloudformation deploy \
            --template-file stack_cicd_service_and_role.yaml \
            --stack-name $STACK_NAME \
            --capabilities CAPABILITY_IAM \
            --parameter-overrides \
            "StackPackageS3Key"="pipeline_manager/${base}${TIME}.zip" \
            "Environment"="staging" \
            "Testing"="false"
       env:
         AWS_ACCESS_KEY_ID: ${{ secrets.MDS_AWS_ACCESS_KEY_ID }}
         AWS_SECRET_ACCESS_KEY: ${{ secrets.MDS_AWS_SECRET_ACCESS_KEY }}
         AWS_DEFAULT_REGION: "eu-west-1"
```

åœ¨æˆ‘çš„ä¸€ä¸ªæ•…äº‹ä¸­æœ‰ä¸€ä¸ªå®Œæ•´çš„è§£å†³æ–¹æ¡ˆç¤ºä¾‹ã€‚

## æœºå™¨å­¦ä¹ 

æ·»åŠ ä¸€ä¸ªæœºå™¨å­¦ä¹ ç»„ä»¶å°†ä½¿æˆ‘ä»¬æˆä¸ºæœºå™¨å­¦ä¹ å·¥ç¨‹å¸ˆã€‚æ•°æ®å·¥ç¨‹ä¸æœºå™¨å­¦ä¹ éå¸¸æ¥è¿‘ï¼Œå› ä¸ºæ•°æ®å·¥ç¨‹å¸ˆåˆ›å»ºçš„æ•°æ®ç®¡é“é€šå¸¸è¢«æœºå™¨å­¦ä¹ æœåŠ¡ä½¿ç”¨ã€‚

> æˆ‘ä»¬ä¸éœ€è¦äº†è§£æ¯ä¸€ç§æœºå™¨å­¦ä¹ æ¨¡å‹ã€‚

æˆ‘ä»¬æ— æ³•ä¸åƒäºšé©¬é€Šå’Œè°·æ­Œè¿™æ ·çš„äº‘æœåŠ¡æä¾›å•†åœ¨æœºå™¨å­¦ä¹ å’Œæ•°æ®ç§‘å­¦é¢†åŸŸç«äº‰ï¼Œä½†æˆ‘ä»¬éœ€è¦çŸ¥é“å¦‚ä½•ä½¿ç”¨å®ƒã€‚

äº‘ä¾›åº”å•†æä¾›äº†è®¸å¤šæ‰˜ç®¡çš„æœºå™¨å­¦ä¹ æœåŠ¡ï¼Œæˆ‘ä»¬å¸Œæœ›ç†Ÿæ‚‰è¿™äº›æœåŠ¡ã€‚æ•°æ®å·¥ç¨‹å¸ˆä¸ºè¿™äº›æœåŠ¡å‡†å¤‡æ•°æ®é›†ï¼Œåšå‡ ä¸ªç›¸å…³çš„æ•™ç¨‹è‚¯å®šä¼šå¾ˆæœ‰ç”¨ã€‚

ä¾‹å¦‚ï¼Œè€ƒè™‘ä¸€ä¸ªç”¨æˆ·æµå¤±é¢„æµ‹é¡¹ç›®ï¼Œä»¥äº†è§£ç”¨æˆ·æµå¤±æƒ…å†µä»¥åŠå¦‚ä½•ä½¿ç”¨æ‰˜ç®¡çš„æœºå™¨å­¦ä¹ æœåŠ¡ä¸ºç”¨æˆ·ç”Ÿæˆé¢„æµ‹ã€‚è¿™å¯ä»¥é€šè¿‡ BigQuery ML [9] è½»æ¾å®Œæˆï¼Œåªéœ€åˆ›å»ºä¸€ä¸ªç®€å•çš„é€»è¾‘å›å½’æ¨¡å‹ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š

```py
CREATE OR REPLACE MODEL sample_churn_model.churn_model

OPTIONS(
  MODEL_TYPE="LOGISTIC_REG",
  INPUT_LABEL_COLS=["churned"]
) AS

SELECT
  * except (
     user_pseudo_id
    ,first_seen_ts    
    ,last_seen_ts     
  )
FROM
  sample_churn_model.churn
```

ç„¶åæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ SQL ç”Ÿæˆé¢„æµ‹ï¼š

```py
SELECT
  user_pseudo_id,
  churned,
  predicted_churned,
  predicted_churned_probs[OFFSET(0)].prob as probability_churned
FROM
  ML.PREDICT(MODEL sample_churn_model.churn_model,
  (SELECT * FROM sample_churn_model.churn)) #can be replaced with a proper test dataset
order by 3 desc
```

## ç»“è®º

æˆ‘è¯•å›¾æ€»ç»“ä¸€ç»„æ•°æ®å·¥ç¨‹æŠ€èƒ½å’ŒæŠ€æœ¯ï¼Œè¿™äº›æŠ€èƒ½é€šå¸¸æ˜¯å…¥é—¨çº§æ•°æ®å·¥ç¨‹å¸ˆè§’è‰²æ‰€éœ€çš„ã€‚æ ¹æ®æˆ‘çš„ç»éªŒï¼Œè¿™äº›æŠ€èƒ½å¯ä»¥åœ¨ä¸¤åˆ°ä¸‰ä¸ªæœˆçš„ç§¯æå­¦ä¹ ä¸­æŒæ¡ã€‚æˆ‘å»ºè®®ä»äº‘æœåŠ¡æä¾›å•†å’Œ Python å¼€å§‹ï¼Œå»ºç«‹ä¸€ä¸ªç®€å•çš„ ETL æœåŠ¡ï¼Œå¹¶ä¸ºé¢„ç”Ÿäº§å’Œç”Ÿäº§ç¯å¢ƒè®¾ç½® CI/CD ç®¡é“ã€‚è¿™ä¸éœ€è¦èŠ±è´¹ä»»ä½•è´¹ç”¨ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡åœ¨æœ¬åœ°å’Œäº‘ä¸­è¿è¡Œè¿™äº›æœåŠ¡æ¥å¿«é€Ÿå­¦ä¹ ã€‚å½“å‰å¸‚åœºä¸Šå¯¹æ•°æ®å·¥ç¨‹å¸ˆçš„éœ€æ±‚å¾ˆé«˜ã€‚æˆ‘å¸Œæœ›è¿™ç¯‡æ–‡ç« èƒ½å¸®åŠ©ä½ å­¦ä¹ ä¸€äº›æ–°çŸ¥è¯†ï¼Œå¹¶ä¸ºå·¥ä½œé¢è¯•åšå‡†å¤‡ã€‚

## æ¨èé˜…è¯»

[1] [`www.linkedin.com/pulse/linkedin-jobs-rise-2023-25-uk-roles-growing-demand-linkedin-news-uk/`](https://www.linkedin.com/pulse/linkedin-jobs-rise-2023-25-uk-roles-growing-demand-linkedin-news-uk/)

[2] `towardsdatascience.com/data-platform-architecture-types-f255ac6e0b7`

[3] `towardsdatascience.com/data-pipeline-design-patterns-100afa4b93e3`

[4] [`medium.com/towards-data-science/advanced-sql-techniques-for-beginners-211851a28488`](https://medium.com/towards-data-science/advanced-sql-techniques-for-beginners-211851a28488)

[5] [`cloud.google.com/python/docs/reference/bigquery/latest`](https://cloud.google.com/python/docs/reference/bigquery/latest)

[6] [`hudi.apache.org/docs/overview/`](https://hudi.apache.org/docs/overview/)

[7] [`cloud.google.com/learn/certification/data-engineer`](https://cloud.google.com/learn/certification/data-engineer)

[8] `towardsdatascience.com/automated-emails-and-data-quality-checks-for-your-data-1de86ed47cf0`

[9] [`cloud.google.com/bigquery/docs/bqml-introduction`](https://cloud.google.com/bigquery/docs/bqml-introduction)
