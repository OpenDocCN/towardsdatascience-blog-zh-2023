- en: Breaking Linearity With ReLU
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/breaking-linearity-with-relu-d2cfa7ebf264?source=collection_archive---------14-----------------------#2023-03-01](https://towardsdatascience.com/breaking-linearity-with-relu-d2cfa7ebf264?source=collection_archive---------14-----------------------#2023-03-01)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Explaining how and why the ReLU activation function is non-linear
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@egorhowell?source=post_page-----d2cfa7ebf264--------------------------------)[![Egor
    Howell](../Images/1f796e828f1625440467d01dcc3e40cd.png)](https://medium.com/@egorhowell?source=post_page-----d2cfa7ebf264--------------------------------)[](https://towardsdatascience.com/?source=post_page-----d2cfa7ebf264--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----d2cfa7ebf264--------------------------------)
    [Egor Howell](https://medium.com/@egorhowell?source=post_page-----d2cfa7ebf264--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F1cac491223b2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbreaking-linearity-with-relu-d2cfa7ebf264&user=Egor+Howell&userId=1cac491223b2&source=post_page-1cac491223b2----d2cfa7ebf264---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----d2cfa7ebf264--------------------------------)
    ·4 min read·Mar 1, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fd2cfa7ebf264&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbreaking-linearity-with-relu-d2cfa7ebf264&user=Egor+Howell&userId=1cac491223b2&source=-----d2cfa7ebf264---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fd2cfa7ebf264&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbreaking-linearity-with-relu-d2cfa7ebf264&source=-----d2cfa7ebf264---------------------bookmark_footer-----------)![](../Images/e9eb0239dd7d7e0b58720e6193479978.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Photo by [Alina Grubnyak](https://unsplash.com/@alinnnaaaa?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[***Neural networks***](https://en.wikipedia.org/wiki/Artificial_neural_network)
    and [***deep learning***](https://en.wikipedia.org/wiki/Deep_learning) are assumably
    one of the most popular reasons people transition into data science. However,
    this excitement can lead to overlooking the core concepts that make neural networks
    tick. In this post, I want to go over probably the most key feature of neural
    networks, which I think most practitioners should be aware of to fully understand
    what is happening under the hood.'
  prefs: []
  type: TYPE_NORMAL
- en: Why We Need Activation Functions?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[***Activation functions***](https://en.wikipedia.org/wiki/Activation_function)
    are ubiquitous in data science and machine learning. They typically refer to the
    transformation that’s applied to the [***linear***](https://en.wikipedia.org/wiki/Linear_equation)
    input of a neuron in a neural network***:***'
  prefs: []
  type: TYPE_NORMAL
- en: Where ***f*** is the activation function, ***y*** is the output, ***b*** is
    the bias, ***and w_i*** and ***x_i*** are the [***weights***](https://en.wikipedia.org/wiki/Weighting)
    and their corresponding feature values.
  prefs: []
  type: TYPE_NORMAL
- en: But, why do we need activation functions?
  prefs: []
  type: TYPE_NORMAL
- en: The simple answer is that they allow us to model complex patterns and they do
    this by making the neural network [***non-linear***](https://en.wikipedia.org/wiki/Nonlinear_system).
    If there are no non-linear activation functions in the network, the whole model
    just becomes a [***linear regression***](https://en.wikipedia.org/wiki/Linear_regression)
    model!
  prefs: []
  type: TYPE_NORMAL
- en: Non-linear is a change to the input…
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
