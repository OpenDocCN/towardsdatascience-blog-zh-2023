- en: Breaking Linearity With ReLU
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 ReLU 打破线性
- en: 原文：[https://towardsdatascience.com/breaking-linearity-with-relu-d2cfa7ebf264?source=collection_archive---------14-----------------------#2023-03-01](https://towardsdatascience.com/breaking-linearity-with-relu-d2cfa7ebf264?source=collection_archive---------14-----------------------#2023-03-01)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/breaking-linearity-with-relu-d2cfa7ebf264?source=collection_archive---------14-----------------------#2023-03-01](https://towardsdatascience.com/breaking-linearity-with-relu-d2cfa7ebf264?source=collection_archive---------14-----------------------#2023-03-01)
- en: Explaining how and why the ReLU activation function is non-linear
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解释 ReLU 激活函数如何以及为何是非线性的
- en: '[](https://medium.com/@egorhowell?source=post_page-----d2cfa7ebf264--------------------------------)[![Egor
    Howell](../Images/1f796e828f1625440467d01dcc3e40cd.png)](https://medium.com/@egorhowell?source=post_page-----d2cfa7ebf264--------------------------------)[](https://towardsdatascience.com/?source=post_page-----d2cfa7ebf264--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----d2cfa7ebf264--------------------------------)
    [Egor Howell](https://medium.com/@egorhowell?source=post_page-----d2cfa7ebf264--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@egorhowell?source=post_page-----d2cfa7ebf264--------------------------------)[![Egor
    Howell](../Images/1f796e828f1625440467d01dcc3e40cd.png)](https://medium.com/@egorhowell?source=post_page-----d2cfa7ebf264--------------------------------)[](https://towardsdatascience.com/?source=post_page-----d2cfa7ebf264--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----d2cfa7ebf264--------------------------------)
    [Egor Howell](https://medium.com/@egorhowell?source=post_page-----d2cfa7ebf264--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F1cac491223b2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbreaking-linearity-with-relu-d2cfa7ebf264&user=Egor+Howell&userId=1cac491223b2&source=post_page-1cac491223b2----d2cfa7ebf264---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----d2cfa7ebf264--------------------------------)
    ·4 min read·Mar 1, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fd2cfa7ebf264&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbreaking-linearity-with-relu-d2cfa7ebf264&user=Egor+Howell&userId=1cac491223b2&source=-----d2cfa7ebf264---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F1cac491223b2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbreaking-linearity-with-relu-d2cfa7ebf264&user=Egor+Howell&userId=1cac491223b2&source=post_page-1cac491223b2----d2cfa7ebf264---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----d2cfa7ebf264--------------------------------)
    ·4 min read·2023年3月1日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fd2cfa7ebf264&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbreaking-linearity-with-relu-d2cfa7ebf264&user=Egor+Howell&userId=1cac491223b2&source=-----d2cfa7ebf264---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fd2cfa7ebf264&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbreaking-linearity-with-relu-d2cfa7ebf264&source=-----d2cfa7ebf264---------------------bookmark_footer-----------)![](../Images/e9eb0239dd7d7e0b58720e6193479978.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fd2cfa7ebf264&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbreaking-linearity-with-relu-d2cfa7ebf264&source=-----d2cfa7ebf264---------------------bookmark_footer-----------)![](../Images/e9eb0239dd7d7e0b58720e6193479978.png)'
- en: Photo by [Alina Grubnyak](https://unsplash.com/@alinnnaaaa?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由 [Alina Grubnyak](https://unsplash.com/@alinnnaaaa?utm_source=medium&utm_medium=referral)
    提供，来源于 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: Introduction
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 引言
- en: '[***Neural networks***](https://en.wikipedia.org/wiki/Artificial_neural_network)
    and [***deep learning***](https://en.wikipedia.org/wiki/Deep_learning) are assumably
    one of the most popular reasons people transition into data science. However,
    this excitement can lead to overlooking the core concepts that make neural networks
    tick. In this post, I want to go over probably the most key feature of neural
    networks, which I think most practitioners should be aware of to fully understand
    what is happening under the hood.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '[***神经网络***](https://en.wikipedia.org/wiki/Artificial_neural_network) 和 [***深度学习***](https://en.wikipedia.org/wiki/Deep_learning)
    可以说是人们转向数据科学的最热门原因之一。然而，这种兴奋感可能会导致忽视神经网络的核心概念。在这篇文章中，我想讲解神经网络的一个可能最关键的特性，我认为大多数从业者应该了解，以便完全理解其内部运作。'
- en: Why We Need Activation Functions?
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为什么我们需要激活函数？
- en: '[***Activation functions***](https://en.wikipedia.org/wiki/Activation_function)
    are ubiquitous in data science and machine learning. They typically refer to the
    transformation that’s applied to the [***linear***](https://en.wikipedia.org/wiki/Linear_equation)
    input of a neuron in a neural network***:***'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '[***激活函数***](https://en.wikipedia.org/wiki/Activation_function)在数据科学和机器学习中无处不在。它们通常指的是对神经网络中神经元的[***线性***](https://en.wikipedia.org/wiki/Linear_equation)输入应用的变换***：***'
- en: Where ***f*** is the activation function, ***y*** is the output, ***b*** is
    the bias, ***and w_i*** and ***x_i*** are the [***weights***](https://en.wikipedia.org/wiki/Weighting)
    and their corresponding feature values.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 其中***f***是激活函数，***y***是输出，***b***是偏置，***w_i***和***x_i***是[***权重***](https://en.wikipedia.org/wiki/Weighting)及其对应的特征值。
- en: But, why do we need activation functions?
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，我们为什么需要激活函数？
- en: The simple answer is that they allow us to model complex patterns and they do
    this by making the neural network [***non-linear***](https://en.wikipedia.org/wiki/Nonlinear_system).
    If there are no non-linear activation functions in the network, the whole model
    just becomes a [***linear regression***](https://en.wikipedia.org/wiki/Linear_regression)
    model!
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 简单的答案是，它们使我们能够建模复杂的模式，方式是通过使神经网络变得[***非线性***](https://en.wikipedia.org/wiki/Nonlinear_system)。如果网络中没有非线性激活函数，整个模型就变成了[***线性回归***](https://en.wikipedia.org/wiki/Linear_regression)模型！
- en: Non-linear is a change to the input…
  id: totrans-16
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 非线性是对输入的变化…
