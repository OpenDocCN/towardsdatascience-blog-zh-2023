- en: 'Towards AGI: LLMs and Foundational Models’ Roles in the Lifelong Learning Revolution'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/towards-agi-llms-and-foundational-models-roles-in-the-lifelong-learning-revolution-f8e56c17fa66?source=collection_archive---------9-----------------------#2023-12-15](https://towardsdatascience.com/towards-agi-llms-and-foundational-models-roles-in-the-lifelong-learning-revolution-f8e56c17fa66?source=collection_archive---------9-----------------------#2023-12-15)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Integrating Innovations in Continual Learning Advancements Towards Artificial
    General Intelligence (AGI), Including [VOYAGER](https://arxiv.org/pdf/2305.16291.pdf),
    [DEPS](https://arxiv.org/pdf/2302.01560.pdf), and [AutoGPT](https://github.com/Significant-Gravitas/AutoGPT).
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@InfiniteLearningLoop?source=post_page-----f8e56c17fa66--------------------------------)[![Elahe
    Aghapour & Salar Rahili](../Images/47a2023c566d50d8ecfcafdb69bb9bb7.png)](https://medium.com/@InfiniteLearningLoop?source=post_page-----f8e56c17fa66--------------------------------)[](https://towardsdatascience.com/?source=post_page-----f8e56c17fa66--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----f8e56c17fa66--------------------------------)
    [Elahe Aghapour & Salar Rahili](https://medium.com/@InfiniteLearningLoop?source=post_page-----f8e56c17fa66--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F8fbc3ea76035&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftowards-agi-llms-and-foundational-models-roles-in-the-lifelong-learning-revolution-f8e56c17fa66&user=Elahe+Aghapour+%26+Salar+Rahili&userId=8fbc3ea76035&source=post_page-8fbc3ea76035----f8e56c17fa66---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----f8e56c17fa66--------------------------------)
    ·11 min read·Dec 15, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Ff8e56c17fa66&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftowards-agi-llms-and-foundational-models-roles-in-the-lifelong-learning-revolution-f8e56c17fa66&user=Elahe+Aghapour+%26+Salar+Rahili&userId=8fbc3ea76035&source=-----f8e56c17fa66---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff8e56c17fa66&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftowards-agi-llms-and-foundational-models-roles-in-the-lifelong-learning-revolution-f8e56c17fa66&source=-----f8e56c17fa66---------------------bookmark_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Authors:** [Elahe Aghapour](https://medium.com/u/75214fb27311?source=post_page-----f8e56c17fa66--------------------------------)**,**
    [Salar Rahili](https://medium.com/u/6dff1eb2cc9f?source=post_page-----f8e56c17fa66--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Introduction:**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the past decade and especially with the success of deep learning, an ongoing
    discussion has formed around the possibility of building an Artificial General
    Intelligence (AGI). The ultimate goal in AGI is to create an agent that is able
    to perform any task that a human being is capable of. A core capability required
    for such an agent is to be able to continuously learn new skills and use its learned
    skills to learn more complicated skills faster. These skills must be broken up
    into sub-tasks, where the agent interacts with the environment, learning from
    its failures until success. And upon learning a new skill, the agent should integrate
    the skill into its existing repertoire of acquired skills for the future. Large
    language models (LLM) have shown that they have a good understanding of the world
    and how different tasks can be accomplished. There has been a series of interesting
    articles published in the past few years with the goal of using an LLM as the
    core decision maker for continual learning. These works have mostly chosen similar
    testing environments such as Crafter or Minecraft, since it can simulate the ultimate
    AGI goal of survival and thrival.
  prefs: []
  type: TYPE_NORMAL
- en: To explore the latest advancements in this field, we first outline the collaborative
    functioning of various building blocks that facilitate the learning process. Subsequently,
    we dive into the specifics of each component, comparing their implementation and
    functionality across different research articles.
  prefs: []
  type: TYPE_NORMAL
- en: '**Overview:**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/f99b1d14028de88344757e79348d1778.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig. 1: The key building blocks identified in the literature are compiled into
    a comprehensive block diagram. Blocks outlined with dashed lines are not included
    in every article (*Image by author*).'
  prefs: []
  type: TYPE_NORMAL
- en: To develop the iterative and continuous process of learning/accomplishing tasks,
    many recommended frameworks adopt a recognizable process. Those with a background
    in feedback control or reinforcement learning will notice a similar structure
    (see Fig. 1); however, there are notable additions that minimize human manual
    input and enhance process automation.
  prefs: []
  type: TYPE_NORMAL
- en: As the first step, a broadly defined task is assigned to the agent by a human,
    echoing the main goal of lifelong learning. This task often takes the form of
    a prompt outlining the primary objective, for instance, “explore the environment,
    and accomplish as many diverse tasks as possible”. The Planner block, conditioned
    on this broadly defined objective, breaks down the goal into a sequence of executable,
    understandable tasks. Such breakdown requires an understanding of the environment
    in which the agent operates. Since LLMs have been trained on a huge corpus of
    data, they could be the best candidates for being planners. In addition, any supplementary,
    explicit, or manual context can enhance their performance.
  prefs: []
  type: TYPE_NORMAL
- en: Within the Selector block, a set of derived subtasks is provided by the Planner.
    The Selector, guided by the main objective and insights from the Critic, determines
    the most suitable next subtask that not only will generate the best outcome, but
    also satisfies the prerequisites. The Controller’s job is to generate actions
    to accomplish the current subtask. To minimize redundant efforts and leverage
    previously acquired tasks, several studies propose incorporating a Memory block.
    This block is used to retrieve the most similar learned tasks, thus integrating
    them into its ongoing workflow.
  prefs: []
  type: TYPE_NORMAL
- en: The generated action is then introduced to the Environment. To assess the impact
    of recent actions, the Critic monitors the environment state providing the feedback
    that includes identifying any shortcomings, reasons for failure, or potential
    task completion. An LLM based Critic necessitates the text input which is accomplished
    by the Descriptor block, to describe/transform the state of the environment and
    agent into text. Critic then informs the Planner on what exactly happened in the
    last attempt and provides comprehensive feedback to assist Planner for the next
    trial.
  prefs: []
  type: TYPE_NORMAL
- en: '**Building Blocks description: Comparing Design and Implementation Across Studies**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we explore each block in detail, discussing the various approaches
    adopted by different researchers.
  prefs: []
  type: TYPE_NORMAL
- en: '**Planner**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This component organizes lifelong learning tasks in a given environment. The
    ultimate goal can be given manually as in [DEPS](https://arxiv.org/pdf/2302.01560.pdf),
    or be more like a guideline, i.e., encouraging learning of diverse behavior as
    a part of the Planner prompt, like in [VOYAGER](https://arxiv.org/pdf/2305.16291.pdf).
  prefs: []
  type: TYPE_NORMAL
- en: The LLM-based Planner orchestrates the learning process by setting tasks that
    align with the agent’s current state, its skill level, and the provided instructions
    in its prompt. Such functionality is integrated into LLMs, based on the assumption
    that they have been exposed to a similar task decomposition process during their
    training. However, this assumption was not valid in SPRING since they ran the
    experiment on the Crafter environment which was released after the data collection
    for GPT-3.5 and GPT-4 models. Therefore, they proposed a method to extract all
    relevant information from the environment manual text, and then summarize it into
    a small-size context that will be concatenated to prompts later. In real life
    applications, agents come across a variety of environments with differing levels
    of complexity and such straightforward yet efficient methods can be crucial in
    avoiding the need for fine-tuning of pre-trained models for newly developed tasks.
  prefs: []
  type: TYPE_NORMAL
- en: '[VOYAGER](https://arxiv.org/pdf/2305.16291.pdf) employed GPT-4 as an automatic
    curriculum module, attempting to propose increasingly difficult tasks based on
    the exploration progress and the agent’s state. Its prompt consists of several
    components, such as:'
  prefs: []
  type: TYPE_NORMAL
- en: (1) encouraging exploration while setting the constraints,
  prefs: []
  type: TYPE_NORMAL
- en: (2) the current agent’s state,
  prefs: []
  type: TYPE_NORMAL
- en: (3) previously completed and failed tasks,
  prefs: []
  type: TYPE_NORMAL
- en: (4) any additional context from another GPT-3.5 self-question answering module.
  prefs: []
  type: TYPE_NORMAL
- en: It then outputs a task to be completed by the agent.
  prefs: []
  type: TYPE_NORMAL
- en: '[DEPS](https://arxiv.org/pdf/2302.01560.pdf) used CODEX, GPT-4, ChatGPT, and
    GPT-3 as their LLM planner in different environments. The prompt includes:'
  prefs: []
  type: TYPE_NORMAL
- en: (1) the formidable ultimate goal (e.g., obtain a diamond in Minecraft environment),
  prefs: []
  type: TYPE_NORMAL
- en: (2) its most recently generated plan,
  prefs: []
  type: TYPE_NORMAL
- en: (3) description of the environment, and its explanation.
  prefs: []
  type: TYPE_NORMAL
- en: In order to improve the efficiency of the plan, [DEPS](https://arxiv.org/pdf/2302.01560.pdf)
    also proposed a state-aware Selector to choose the nearest goal based on the current
    state from the candidate goal sets generated by the Planner. In complex environments,
    there often exist multiple viable plans, while many of them prove inefficient
    in execution and some goals within a plan can be executed in any order, allowing
    flexibility. Prioritizing closer goals can enhance plan efficiency. To this end,
    they trained a neural network using offline trajectories to predict and rank based
    on the time step required for completing the given goals at the current state.
    The Planner in collaboration with the Selector will generate a sequence of tasks
    to be accomplished.accomplished.
  prefs: []
  type: TYPE_NORMAL
- en: '**Controller:**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The main responsibility of the controller is to choose the next action to accomplish
    the given task. The Controller could be another LLM, e.g., [VOYAGER](https://arxiv.org/pdf/2305.16291.pdf),
    or a deep reinforcement learning model, e.g., [DEPS](https://arxiv.org/pdf/2302.01560.pdf),
    generating actions based on the state and the given task. [VOYAGER](https://arxiv.org/pdf/2305.16291.pdf)
    employs GPT-4 in an interactive prompting to play the role of the controller.
    [VOYAGER](https://arxiv.org/pdf/2305.16291.pdf), [Progprompt](https://arxiv.org/pdf/2209.11302.pdf),
    and [CaP](https://arxiv.org/pdf/2209.07753.pdf) choose to use code as the action
    space instead of low-level motor commands. This is crucial for long horizon tasks
    since the code can naturally represent temporally extended and compositional actions.
    The prompt for code generation in [VOYAGER](https://arxiv.org/pdf/2305.16291.pdf)
    includes:'
  prefs: []
  type: TYPE_NORMAL
- en: (1) A code generation motivation guideline,
  prefs: []
  type: TYPE_NORMAL
- en: (2) A list of available control primitive APIs with their description,
  prefs: []
  type: TYPE_NORMAL
- en: (3) Relevant skills/codes retrieved from the memory,
  prefs: []
  type: TYPE_NORMAL
- en: (4) The generated code from the previous round, environment feedback, execution
    errors, and Critic’s output
  prefs: []
  type: TYPE_NORMAL
- en: (5) Current state
  prefs: []
  type: TYPE_NORMAL
- en: (6) Chain-of-thought prompting to do reasoning before code generation.
  prefs: []
  type: TYPE_NORMAL
- en: Another alternative for the controller is to train a deep reinforcement learning
    agent to generate actions based on the current state and goal. [DEPS](https://arxiv.org/pdf/2302.01560.pdf)
    used imitation learning to train such a model.
  prefs: []
  type: TYPE_NORMAL
- en: '**Memory:**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Humans use different types of memory to accomplish a given task. The main memory
    functionalities can be categorized into:'
  prefs: []
  type: TYPE_NORMAL
- en: '1- Short-term memory: Stores information that we are actively using for tasks
    like learning and reasoning. It’s thought to hold around 7 items and lasts for
    about 20–30 seconds [10]. To the best of our knowledge, all LLM based lifelong
    learning methods are using short term memory by in-context learning which is limited
    by the context length of the LLM.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c87e9ddf70c2f2d1f142bf6a06814cd9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig. 2: Skill library in [VOYAGER](https://arxiv.org/pdf/2305.16291.pdf). Top
    figure describes the process of adding a new skill and the bottom one is skill
    retrieval (image source [VOYAGER](https://arxiv.org/pdf/2305.16291.pdf) )'
  prefs: []
  type: TYPE_NORMAL
- en: '2- Long-term memory: Stores and retrieves information for a long time. This
    can be implemented as an external vector store with fast retrieval. [VOYAGER](https://arxiv.org/pdf/2305.16291.pdf)
    benefits from long term memory by adding/retrieving learned skills from external
    vector stores. Skills, as we discussed, are executable codes generated by the
    Controller which guide the steps needed to accomplish the task.'
  prefs: []
  type: TYPE_NORMAL
- en: When the Critic verifies that the code can complete the task, the GPT-3.5 is
    used to generate a description for the code. Then, the skill will be stored in
    the skill library, where the embedding of the description acts as the key and
    the code as the value (see Fig. 2). When a new task is suggested by the Planner,
    the GPT-3.5 generates a general suggestion for completing the task. They use the
    embedding of the suggested solution, augmented with environment feedback, to retrieve
    the top 5 relevant skills from the skill library (see Fig. 2).
  prefs: []
  type: TYPE_NORMAL
- en: Adding long-term memory can significantly boost the performance. Fig. 3 demonstrates
    how critical the skill library is for [VOYAGER](https://arxiv.org/pdf/2305.16291.pdf).
    This also indicates that adding a skill library to Auto-GPT can substantially
    improve its performance. Both short-term and long-term memory work with the Controller
    to generate and refine its policy in order to accomplish the goal.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2e04ad60b6620560a123af2d61c35b83.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig. 3: Adding skill library to AutoGPT boosts its performance in zero shot
    learning generalization to unseen tasks (image source [VOYAGER](https://arxiv.org/pdf/2305.16291.pdf)
    ).'
  prefs: []
  type: TYPE_NORMAL
- en: '**Critic:**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Critic or self-verification is an LLM based module which provides critique on
    the previously executed plan and provides feedback on how to refine the plan to
    accomplish the task. Reflexion enhances agent reasoning with dynamic memory and
    self-reflection. The self-reflection is a GPT-4, playing the role of Critic. It
    takes the reward signal, current trajectory and its persistent memory to generate
    verbal feedback for self-improvement for future trials. This feedback is more
    informative than a scalar reward and is stored in the memory to be used by the
    Planner to refine the plan.
  prefs: []
  type: TYPE_NORMAL
- en: '[VOYAGER](https://arxiv.org/pdf/2305.16291.pdf) and [DEPS](https://arxiv.org/pdf/2302.01560.pdf)
    execute the generated actions, code, by the Controller to obtain the Environment
    feedback and possibly execution errors. This information is incorporated into
    the Critic prompt, where it is asked to act as a critic and determine whether
    the goal is completed or not. Moreover, if the task has failed, it provides suggestions
    on how to complete the task.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Descriptor:**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In LLM based lifelong learning the Planner input and output is text. Some environments
    such as Crafter are text based while for the rest of the environments, they return
    a rendering of 2D or 3D image, or possibly a few state variables. A descriptor
    acts as a bridge, converting the modalities into text and incorporating them into
    an LLM’s prompt.
  prefs: []
  type: TYPE_NORMAL
- en: '**Autonomous AI agents:**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This blog primarily discusses recent studies that integrate foundational models
    with continual learning, a significant stride towards achieving AGI. However,
    it’s important to recognize that these approaches represent a subset of the broader
    endeavor to develop autonomous agents. Several notable initiatives likely served
    as catalysts for the research discussed here. We will briefly highlight these
    in the following section.
  prefs: []
  type: TYPE_NORMAL
- en: Recently, several works, such as [AutoGPT](https://github.com/Significant-Gravitas/AutoGPT)
    and [BabyAGI](https://github.com/yoheinakajima/babyagi), appear to be inspiring
    in using LLMs as the brain, and are designed to be an autonomous agent to solve
    complex problems. You provide them with a task. They run in a loop, breaking the
    task into subtasks, prompting themselves, responding to the prompt, and repeating
    the process until they achieve the provided goal. They also can have access to
    different APIs, such as internet access, which can substantially broaden their
    use cases.
  prefs: []
  type: TYPE_NORMAL
- en: '[AutoGPT](https://github.com/Significant-Gravitas/AutoGPT) is both a GPT-3.5
    and a GPT-4, teamed up with a companion bot that guides and tells them what to
    do. [AutoGPT](https://github.com/Significant-Gravitas/AutoGPT) has internet access
    and is able to interact with apps, software, and services, both online and local.
    To accomplish a higher-level goal given by humans, [AutoGPT](https://github.com/Significant-Gravitas/AutoGPT)
    uses a format of prompting called Reason and ACT (ReACT). ReACT enables the agent
    to receive an input, understand it, act based on it, reason upon the results,
    and then re-run that loop if needed. Since [AutoGPT](https://github.com/Significant-Gravitas/AutoGPT)
    can prompt itself, it can think and reason while accomplishing the task, looking
    for solutions, discard the unsuccessful ones, and consider different options.'
  prefs: []
  type: TYPE_NORMAL
- en: '[BabyAGI](https://github.com/yoheinakajima/babyagi) is another recently introduced
    autonomous AI agent. It has three LLM based components (See Fig. 4):'
  prefs: []
  type: TYPE_NORMAL
- en: 1- There is a task creation agent that comes up with a task list (similar to
    Planer)
  prefs: []
  type: TYPE_NORMAL
- en: 2- A prioritization agent tries to prioritize a task list by LLM prompting (similar
    to Selector)
  prefs: []
  type: TYPE_NORMAL
- en: 3- An execution agent (Similar to Controller) executes a task with highest priority.
  prefs: []
  type: TYPE_NORMAL
- en: Both [AutoGPT](https://github.com/Significant-Gravitas/AutoGPT) and [BabyAGI](https://github.com/yoheinakajima/babyagi)
    use a vector store under the hood to store intermediate results and learn from
    the experiences.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/85119b4c95c6e1b41da7935feec9592d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig. 4: [BabyAGI](https://github.com/yoheinakajima/babyagi) flowchart diagram
    (image source [Yohei Nakajima’s website](https://yoheinakajima.com/task-driven-autonomous-agent-utilizing-gpt-4-pinecone-and-langchain-for-diverse-applications/))'
  prefs: []
  type: TYPE_NORMAL
- en: '**Limitations and challenges:**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**1-** LLM based lifelong learning heavily depends on the reliability of LLMs
    to accurately understand the environment and effectively plan and critique. However,
    studies reveal that LLMs can sometimes produce hallucinations, make up facts,
    and assign tasks that do not exist. Notably, in some of the referenced studies,
    substituting GPT-4 with GPT-3.5 led to a significant decline in performance, underscoring
    the critical role of the LLM model employed.'
  prefs: []
  type: TYPE_NORMAL
- en: 2- LLMs exhibit inaccuracies when employed as a Planner or a Critic. The Critic
    may provide incorrect feedback or fail to accurately verify task completion. Similarly,
    the Planner might become trapped in a repetitive cycle, unable to adjust its plan
    even after several attempts. Adding a well-designed, event-triggered human intervention
    process can boost the performance of these models in such scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: 3- The limited context length in LLMs restricts the short-term memory capability,
    affecting their ability to retain detailed past experiences and their results,
    detailed instructions, and available control primitive APIs. A long context length
    is very critical, especially in self-verification, to learn from past experiences
    and failures. Despite ongoing research efforts, to extend context length or employ
    methods like Transformer-XL, in most cases authors used GPT-4 with the maximum
    of 8,192 tokens context length.
  prefs: []
  type: TYPE_NORMAL
- en: 4- Most of these works, except SPRING, assume that the LLM knows all the necessary
    information required to initiate lifelong learning before starting the experiment.
    However, this assumption might not always hold true. Providing Internet access
    to agents, as in AutoGPT, or providing textual material as input context, as in
    SPRING, can be helpful to address follow-up questions.
  prefs: []
  type: TYPE_NORMAL
- en: '**References:**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] VOYAGER: Wang, Guanzhi, et al. “Voyager: An open-ended embodied agent with
    large language models.”, 2023'
  prefs: []
  type: TYPE_NORMAL
- en: '[2] DEPS: Wang, Zihao, et al. “Describe, explain, plan and select: Interactive
    planning with large language models enables open-world multi-task agents.”, 2023'
  prefs: []
  type: TYPE_NORMAL
- en: '[3] SPRING: Wu, Yue, et al. “SPRING: GPT-4 Out-performs RL Algorithms by Studying
    Papers and Reasoning.”, 2023'
  prefs: []
  type: TYPE_NORMAL
- en: '[4] Reflexion: Shinn, Noah, et al. “Reflexion: Language agents with verbal
    reinforcement learning.”, 2023'
  prefs: []
  type: TYPE_NORMAL
- en: '[5] Progprompt: Singh, Ishika, et al. “Progprompt: Generating situated robot
    task plans using large language models.”, 2023'
  prefs: []
  type: TYPE_NORMAL
- en: '[6] React: Yao, Shunyu, et al. “React: Synergizing reasoning and acting in
    language models.”, 2022'
  prefs: []
  type: TYPE_NORMAL
- en: '[7] CaP: Liang, Jacky, et al. “Code as policies: Language model programs for
    embodied control.”, *2023*'
  prefs: []
  type: TYPE_NORMAL
- en: '[8] AutoGPT. [https://github.com/Significant-Gravitas/Auto-GPT](https://github.com/Significant-Gravitas/Auto-GPT)'
  prefs: []
  type: TYPE_NORMAL
- en: '[9] babyAGI: [https://github.com/yoheinakajima/babyagi](https://github.com/yoheinakajima/babyagi)'
  prefs: []
  type: TYPE_NORMAL
- en: '[10] Weng, Lilian, et al. LLM-powered Autonomous Agents”, 2023'
  prefs: []
  type: TYPE_NORMAL
