- en: 'Towards AGI: LLMs and Foundational Models’ Roles in the Lifelong Learning Revolution'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 向AGI迈进：LLMs和基础模型在终身学习革命中的角色
- en: 原文：[https://towardsdatascience.com/towards-agi-llms-and-foundational-models-roles-in-the-lifelong-learning-revolution-f8e56c17fa66?source=collection_archive---------9-----------------------#2023-12-15](https://towardsdatascience.com/towards-agi-llms-and-foundational-models-roles-in-the-lifelong-learning-revolution-f8e56c17fa66?source=collection_archive---------9-----------------------#2023-12-15)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/towards-agi-llms-and-foundational-models-roles-in-the-lifelong-learning-revolution-f8e56c17fa66?source=collection_archive---------9-----------------------#2023-12-15](https://towardsdatascience.com/towards-agi-llms-and-foundational-models-roles-in-the-lifelong-learning-revolution-f8e56c17fa66?source=collection_archive---------9-----------------------#2023-12-15)
- en: Integrating Innovations in Continual Learning Advancements Towards Artificial
    General Intelligence (AGI), Including [VOYAGER](https://arxiv.org/pdf/2305.16291.pdf),
    [DEPS](https://arxiv.org/pdf/2302.01560.pdf), and [AutoGPT](https://github.com/Significant-Gravitas/AutoGPT).
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 融合了在通用学习领域的创新进展，向**人工通用智能（AGI）**迈进，包括[VOYAGER](https://arxiv.org/pdf/2305.16291.pdf)、[DEPS](https://arxiv.org/pdf/2302.01560.pdf)和[AutoGPT](https://github.com/Significant-Gravitas/AutoGPT)。
- en: '[](https://medium.com/@InfiniteLearningLoop?source=post_page-----f8e56c17fa66--------------------------------)[![Elahe
    Aghapour & Salar Rahili](../Images/47a2023c566d50d8ecfcafdb69bb9bb7.png)](https://medium.com/@InfiniteLearningLoop?source=post_page-----f8e56c17fa66--------------------------------)[](https://towardsdatascience.com/?source=post_page-----f8e56c17fa66--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----f8e56c17fa66--------------------------------)
    [Elahe Aghapour & Salar Rahili](https://medium.com/@InfiniteLearningLoop?source=post_page-----f8e56c17fa66--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@InfiniteLearningLoop?source=post_page-----f8e56c17fa66--------------------------------)[![Elahe
    Aghapour & Salar Rahili](../Images/47a2023c566d50d8ecfcafdb69bb9bb7.png)](https://medium.com/@InfiniteLearningLoop?source=post_page-----f8e56c17fa66--------------------------------)[](https://towardsdatascience.com/?source=post_page-----f8e56c17fa66--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----f8e56c17fa66--------------------------------)
    [Elahe Aghapour & Salar Rahili](https://medium.com/@InfiniteLearningLoop?source=post_page-----f8e56c17fa66--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F8fbc3ea76035&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftowards-agi-llms-and-foundational-models-roles-in-the-lifelong-learning-revolution-f8e56c17fa66&user=Elahe+Aghapour+%26+Salar+Rahili&userId=8fbc3ea76035&source=post_page-8fbc3ea76035----f8e56c17fa66---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----f8e56c17fa66--------------------------------)
    ·11 min read·Dec 15, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Ff8e56c17fa66&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftowards-agi-llms-and-foundational-models-roles-in-the-lifelong-learning-revolution-f8e56c17fa66&user=Elahe+Aghapour+%26+Salar+Rahili&userId=8fbc3ea76035&source=-----f8e56c17fa66---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F8fbc3ea76035&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftowards-agi-llms-and-foundational-models-roles-in-the-lifelong-learning-revolution-f8e56c17fa66&user=Elahe+Aghapour+%26+Salar+Rahili&userId=8fbc3ea76035&source=post_page-8fbc3ea76035----f8e56c17fa66---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----f8e56c17fa66--------------------------------)
    ·11分钟阅读·2023年12月15日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Ff8e56c17fa66&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftowards-agi-llms-and-foundational-models-roles-in-the-lifelong-learning-revolution-f8e56c17fa66&user=Elahe+Aghapour+%26+Salar+Rahili&userId=8fbc3ea76035&source=-----f8e56c17fa66---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff8e56c17fa66&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftowards-agi-llms-and-foundational-models-roles-in-the-lifelong-learning-revolution-f8e56c17fa66&source=-----f8e56c17fa66---------------------bookmark_footer-----------)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff8e56c17fa66&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftowards-agi-llms-and-foundational-models-roles-in-the-lifelong-learning-revolution-f8e56c17fa66&source=-----f8e56c17fa66---------------------bookmark_footer-----------)'
- en: '**Authors:** [Elahe Aghapour](https://medium.com/u/75214fb27311?source=post_page-----f8e56c17fa66--------------------------------)**,**
    [Salar Rahili](https://medium.com/u/6dff1eb2cc9f?source=post_page-----f8e56c17fa66--------------------------------)'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '**作者：** [Elahe Aghapour](https://medium.com/u/75214fb27311?source=post_page-----f8e56c17fa66--------------------------------)**，**
    [Salar Rahili](https://medium.com/u/6dff1eb2cc9f?source=post_page-----f8e56c17fa66--------------------------------)'
- en: '**Introduction:**'
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**引言：**'
- en: In the past decade and especially with the success of deep learning, an ongoing
    discussion has formed around the possibility of building an Artificial General
    Intelligence (AGI). The ultimate goal in AGI is to create an agent that is able
    to perform any task that a human being is capable of. A core capability required
    for such an agent is to be able to continuously learn new skills and use its learned
    skills to learn more complicated skills faster. These skills must be broken up
    into sub-tasks, where the agent interacts with the environment, learning from
    its failures until success. And upon learning a new skill, the agent should integrate
    the skill into its existing repertoire of acquired skills for the future. Large
    language models (LLM) have shown that they have a good understanding of the world
    and how different tasks can be accomplished. There has been a series of interesting
    articles published in the past few years with the goal of using an LLM as the
    core decision maker for continual learning. These works have mostly chosen similar
    testing environments such as Crafter or Minecraft, since it can simulate the ultimate
    AGI goal of survival and thrival.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去十年，尤其是在深度学习取得成功之后，围绕构建通用人工智能（AGI）的可能性展开了持续的讨论。AGI的终极目标是创建一个能够执行任何人类能够完成的任务的智能体。这样一个智能体所需的核心能力是能够持续学习新技能，并利用已学会的技能更快地学习更复杂的技能。这些技能必须被拆分成子任务，智能体与环境进行互动，从失败中学习直到成功。而在学习新技能后，智能体应将该技能整合到现有的技能库中，以备将来使用。大型语言模型（LLM）已展示出对世界和如何完成不同任务的良好理解。近年来，出现了一系列有趣的文章，目标是利用LLM作为持续学习的核心决策者。这些研究大多选择了类似的测试环境，如Crafter或Minecraft，因为它们可以模拟AGI的终极目标——生存与繁荣。
- en: To explore the latest advancements in this field, we first outline the collaborative
    functioning of various building blocks that facilitate the learning process. Subsequently,
    we dive into the specifics of each component, comparing their implementation and
    functionality across different research articles.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 为了探索该领域的最新进展，我们首先概述了各种构建模块的协同功能，这些模块促进了学习过程。随后，我们深入探讨每个组件的具体细节，比较它们在不同研究文章中的实现和功能。
- en: '**Overview:**'
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**概述：**'
- en: '![](../Images/f99b1d14028de88344757e79348d1778.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f99b1d14028de88344757e79348d1778.png)'
- en: 'Fig. 1: The key building blocks identified in the literature are compiled into
    a comprehensive block diagram. Blocks outlined with dashed lines are not included
    in every article (*Image by author*).'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：文献中确定的关键构建模块被编制成一个全面的框图。用虚线勾勒的块并不是每篇文章中都包含的（*作者提供的图像*）。
- en: To develop the iterative and continuous process of learning/accomplishing tasks,
    many recommended frameworks adopt a recognizable process. Those with a background
    in feedback control or reinforcement learning will notice a similar structure
    (see Fig. 1); however, there are notable additions that minimize human manual
    input and enhance process automation.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 为了开发迭代和持续的学习/任务完成过程，许多推荐的框架采用了一个可识别的过程。那些有反馈控制或强化学习背景的人会注意到类似的结构（见图1）；然而，还有显著的增加项，能够减少人工输入并增强过程自动化。
- en: As the first step, a broadly defined task is assigned to the agent by a human,
    echoing the main goal of lifelong learning. This task often takes the form of
    a prompt outlining the primary objective, for instance, “explore the environment,
    and accomplish as many diverse tasks as possible”. The Planner block, conditioned
    on this broadly defined objective, breaks down the goal into a sequence of executable,
    understandable tasks. Such breakdown requires an understanding of the environment
    in which the agent operates. Since LLMs have been trained on a huge corpus of
    data, they could be the best candidates for being planners. In addition, any supplementary,
    explicit, or manual context can enhance their performance.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 作为第一步，人类将一个广义定义的任务分配给代理，呼应终身学习的主要目标。这个任务通常以一个提示的形式出现，概述主要目标，例如，“探索环境，完成尽可能多的多样化任务”。规划器块根据这个广泛定义的目标，将其分解成一系列可执行、易于理解的任务。这种分解需要对代理操作的环境有一定的理解。由于大规模语言模型（LLMs）已在大量数据上训练，它们可能是最适合担任规划者的候选者。此外，任何补充的、明确的或手动的上下文都可以提升它们的表现。
- en: Within the Selector block, a set of derived subtasks is provided by the Planner.
    The Selector, guided by the main objective and insights from the Critic, determines
    the most suitable next subtask that not only will generate the best outcome, but
    also satisfies the prerequisites. The Controller’s job is to generate actions
    to accomplish the current subtask. To minimize redundant efforts and leverage
    previously acquired tasks, several studies propose incorporating a Memory block.
    This block is used to retrieve the most similar learned tasks, thus integrating
    them into its ongoing workflow.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在选择器块中，规划器提供了一组衍生的子任务。选择器在主要目标和批评者的洞见指导下，确定最适合的下一个子任务，这不仅能生成最佳结果，还能满足先决条件。控制器的任务是生成行动来完成当前子任务。为了减少冗余工作并利用先前获得的任务，一些研究建议加入记忆块。这个块用于检索最相似的已学任务，从而将它们整合到当前的工作流程中。
- en: The generated action is then introduced to the Environment. To assess the impact
    of recent actions, the Critic monitors the environment state providing the feedback
    that includes identifying any shortcomings, reasons for failure, or potential
    task completion. An LLM based Critic necessitates the text input which is accomplished
    by the Descriptor block, to describe/transform the state of the environment and
    agent into text. Critic then informs the Planner on what exactly happened in the
    last attempt and provides comprehensive feedback to assist Planner for the next
    trial.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 生成的行动随后被引入环境中。为了评估最近行动的影响，批评者监控环境状态，提供反馈，包括识别任何不足之处、失败原因或潜在任务完成情况。基于LLM的批评者需要文本输入，这由描述符块完成，用于将环境和代理的状态描述/转化为文本。批评者然后向规划器通报上次尝试中发生的具体情况，并提供全面的反馈，以协助规划器进行下一次尝试。
- en: '**Building Blocks description: Comparing Design and Implementation Across Studies**'
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**构建块描述：跨研究的设计与实施比较**'
- en: In this section, we explore each block in detail, discussing the various approaches
    adopted by different researchers.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们详细探讨每个块，讨论不同研究人员采用的各种方法。
- en: '**Planner**'
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**规划器**'
- en: This component organizes lifelong learning tasks in a given environment. The
    ultimate goal can be given manually as in [DEPS](https://arxiv.org/pdf/2302.01560.pdf),
    or be more like a guideline, i.e., encouraging learning of diverse behavior as
    a part of the Planner prompt, like in [VOYAGER](https://arxiv.org/pdf/2305.16291.pdf).
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 这个组件在给定的环境中组织终身学习任务。最终目标可以像在[DEPS](https://arxiv.org/pdf/2302.01560.pdf)中那样手动给定，也可以更像是指南，即鼓励将多样化行为作为规划器提示的一部分，如在[VOYAGER](https://arxiv.org/pdf/2305.16291.pdf)中那样。
- en: The LLM-based Planner orchestrates the learning process by setting tasks that
    align with the agent’s current state, its skill level, and the provided instructions
    in its prompt. Such functionality is integrated into LLMs, based on the assumption
    that they have been exposed to a similar task decomposition process during their
    training. However, this assumption was not valid in SPRING since they ran the
    experiment on the Crafter environment which was released after the data collection
    for GPT-3.5 and GPT-4 models. Therefore, they proposed a method to extract all
    relevant information from the environment manual text, and then summarize it into
    a small-size context that will be concatenated to prompts later. In real life
    applications, agents come across a variety of environments with differing levels
    of complexity and such straightforward yet efficient methods can be crucial in
    avoiding the need for fine-tuning of pre-trained models for newly developed tasks.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 基于 LLM 的规划器通过设置与智能体当前状态、技能水平以及提示中的指示相一致的任务来组织学习过程。这种功能集成在 LLM 中，基于假设它们在训练过程中接触过类似的任务分解过程。然而，这一假设在
    SPRING 中并不成立，因为他们在 Crafter 环境中进行实验，而该环境在 GPT-3.5 和 GPT-4 模型的数据收集之后才发布。因此，他们提出了一种方法从环境手册文本中提取所有相关信息，然后将其总结成一个小型上下文，后续将与提示连接。在实际应用中，智能体会遇到各种不同复杂度的环境，这种直接而高效的方法对于避免对预训练模型进行微调以应对新开发的任务至关重要。
- en: '[VOYAGER](https://arxiv.org/pdf/2305.16291.pdf) employed GPT-4 as an automatic
    curriculum module, attempting to propose increasingly difficult tasks based on
    the exploration progress and the agent’s state. Its prompt consists of several
    components, such as:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '[VOYAGER](https://arxiv.org/pdf/2305.16291.pdf) 使用 GPT-4 作为自动课程模块，试图根据探索进展和智能体状态提出越来越困难的任务。它的提示包含几个组成部分，例如：'
- en: (1) encouraging exploration while setting the constraints,
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: (1) 鼓励探索同时设置约束，
- en: (2) the current agent’s state,
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: (2) 当前智能体的状态，
- en: (3) previously completed and failed tasks,
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: (3) 先前完成和失败的任务，
- en: (4) any additional context from another GPT-3.5 self-question answering module.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: (4) 来自另一个 GPT-3.5 自我提问回答模块的任何额外上下文。
- en: It then outputs a task to be completed by the agent.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 然后它输出一个待完成的任务给智能体。
- en: '[DEPS](https://arxiv.org/pdf/2302.01560.pdf) used CODEX, GPT-4, ChatGPT, and
    GPT-3 as their LLM planner in different environments. The prompt includes:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '[DEPS](https://arxiv.org/pdf/2302.01560.pdf) 在不同环境中使用了 CODEX、GPT-4、ChatGPT
    和 GPT-3 作为他们的 LLM 规划器。提示包括：'
- en: (1) the formidable ultimate goal (e.g., obtain a diamond in Minecraft environment),
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: (1) 令人畏惧的终极目标（例如，在 Minecraft 环境中获得一颗钻石），
- en: (2) its most recently generated plan,
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: (2) 最近生成的计划，
- en: (3) description of the environment, and its explanation.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: (3) 环境描述及其解释。
- en: In order to improve the efficiency of the plan, [DEPS](https://arxiv.org/pdf/2302.01560.pdf)
    also proposed a state-aware Selector to choose the nearest goal based on the current
    state from the candidate goal sets generated by the Planner. In complex environments,
    there often exist multiple viable plans, while many of them prove inefficient
    in execution and some goals within a plan can be executed in any order, allowing
    flexibility. Prioritizing closer goals can enhance plan efficiency. To this end,
    they trained a neural network using offline trajectories to predict and rank based
    on the time step required for completing the given goals at the current state.
    The Planner in collaboration with the Selector will generate a sequence of tasks
    to be accomplished.accomplished.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 为了提高计划的效率，[DEPS](https://arxiv.org/pdf/2302.01560.pdf) 还提出了一种状态感知选择器，以根据当前状态从规划器生成的候选目标集中选择最接近的目标。在复杂环境中，通常存在多个可行的计划，而许多计划在执行时证明效率低下，并且计划中的一些目标可以按任意顺序执行，允许灵活性。优先考虑更近的目标可以提高计划效率。为此，他们使用离线轨迹训练了一个神经网络，以预测和排名完成当前状态下给定目标所需的时间步。规划器与选择器合作生成需要完成的任务序列。
- en: '**Controller:**'
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**控制器：**'
- en: 'The main responsibility of the controller is to choose the next action to accomplish
    the given task. The Controller could be another LLM, e.g., [VOYAGER](https://arxiv.org/pdf/2305.16291.pdf),
    or a deep reinforcement learning model, e.g., [DEPS](https://arxiv.org/pdf/2302.01560.pdf),
    generating actions based on the state and the given task. [VOYAGER](https://arxiv.org/pdf/2305.16291.pdf)
    employs GPT-4 in an interactive prompting to play the role of the controller.
    [VOYAGER](https://arxiv.org/pdf/2305.16291.pdf), [Progprompt](https://arxiv.org/pdf/2209.11302.pdf),
    and [CaP](https://arxiv.org/pdf/2209.07753.pdf) choose to use code as the action
    space instead of low-level motor commands. This is crucial for long horizon tasks
    since the code can naturally represent temporally extended and compositional actions.
    The prompt for code generation in [VOYAGER](https://arxiv.org/pdf/2305.16291.pdf)
    includes:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 控制器的主要职责是选择下一步行动以完成给定任务。控制器可以是另一个 LLM，例如 [VOYAGER](https://arxiv.org/pdf/2305.16291.pdf)，或一个深度强化学习模型，例如
    [DEPS](https://arxiv.org/pdf/2302.01560.pdf)，根据状态和给定任务生成动作。[VOYAGER](https://arxiv.org/pdf/2305.16291.pdf)
    使用 GPT-4 进行交互提示，以扮演控制器的角色。[VOYAGER](https://arxiv.org/pdf/2305.16291.pdf)、[Progprompt](https://arxiv.org/pdf/2209.11302.pdf)
    和 [CaP](https://arxiv.org/pdf/2209.07753.pdf) 选择将代码作为动作空间，而不是低级别的运动指令。这对于长期任务至关重要，因为代码可以自然地表示时间上延续和组合的动作。[VOYAGER](https://arxiv.org/pdf/2305.16291.pdf)
    中的代码生成提示包括：
- en: (1) A code generation motivation guideline,
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: (1) 代码生成动机指南，
- en: (2) A list of available control primitive APIs with their description,
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: (2) 可用控制原语 API 列表及其描述，
- en: (3) Relevant skills/codes retrieved from the memory,
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: (3) 从记忆中检索的相关技能/代码，
- en: (4) The generated code from the previous round, environment feedback, execution
    errors, and Critic’s output
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: (4) 来自上一轮的生成代码、环境反馈、执行错误和批评者的输出
- en: (5) Current state
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: (5) 当前状态
- en: (6) Chain-of-thought prompting to do reasoning before code generation.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: (6) 在代码生成之前进行推理的链式思维提示。
- en: Another alternative for the controller is to train a deep reinforcement learning
    agent to generate actions based on the current state and goal. [DEPS](https://arxiv.org/pdf/2302.01560.pdf)
    used imitation learning to train such a model.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 控制器的另一种选择是训练一个深度强化学习代理，根据当前状态和目标生成动作。[DEPS](https://arxiv.org/pdf/2302.01560.pdf)
    使用模仿学习来训练这样的模型。
- en: '**Memory:**'
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**记忆：**'
- en: 'Humans use different types of memory to accomplish a given task. The main memory
    functionalities can be categorized into:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 人类使用不同类型的记忆来完成特定任务。主要的记忆功能可以分为：
- en: '1- Short-term memory: Stores information that we are actively using for tasks
    like learning and reasoning. It’s thought to hold around 7 items and lasts for
    about 20–30 seconds [10]. To the best of our knowledge, all LLM based lifelong
    learning methods are using short term memory by in-context learning which is limited
    by the context length of the LLM.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 1- 短期记忆：存储我们在学习和推理等任务中主动使用的信息。它被认为可以容纳大约 7 项信息，并持续约 20-30 秒[10]。根据我们所知，所有基于
    LLM 的终身学习方法都通过上下文学习使用短期记忆，而这种学习受到 LLM 上下文长度的限制。
- en: '![](../Images/c87e9ddf70c2f2d1f142bf6a06814cd9.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c87e9ddf70c2f2d1f142bf6a06814cd9.png)'
- en: 'Fig. 2: Skill library in [VOYAGER](https://arxiv.org/pdf/2305.16291.pdf). Top
    figure describes the process of adding a new skill and the bottom one is skill
    retrieval (image source [VOYAGER](https://arxiv.org/pdf/2305.16291.pdf) )'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '图 2: [VOYAGER](https://arxiv.org/pdf/2305.16291.pdf) 中的技能库。上图描述了添加新技能的过程，下图是技能检索（图片来源
    [VOYAGER](https://arxiv.org/pdf/2305.16291.pdf)）'
- en: '2- Long-term memory: Stores and retrieves information for a long time. This
    can be implemented as an external vector store with fast retrieval. [VOYAGER](https://arxiv.org/pdf/2305.16291.pdf)
    benefits from long term memory by adding/retrieving learned skills from external
    vector stores. Skills, as we discussed, are executable codes generated by the
    Controller which guide the steps needed to accomplish the task.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 2- 长期记忆：存储和检索长期信息。这可以实现为一个外部向量存储，通过快速检索提供长期记忆。[VOYAGER](https://arxiv.org/pdf/2305.16291.pdf)
    通过从外部向量存储中添加/检索学习到的技能来受益于长期记忆。正如我们讨论的，技能是由控制器生成的可执行代码，指导完成任务所需的步骤。
- en: When the Critic verifies that the code can complete the task, the GPT-3.5 is
    used to generate a description for the code. Then, the skill will be stored in
    the skill library, where the embedding of the description acts as the key and
    the code as the value (see Fig. 2). When a new task is suggested by the Planner,
    the GPT-3.5 generates a general suggestion for completing the task. They use the
    embedding of the suggested solution, augmented with environment feedback, to retrieve
    the top 5 relevant skills from the skill library (see Fig. 2).
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 当Critic验证代码可以完成任务时，GPT-3.5用于生成代码的描述。然后，技能将被存储在技能库中，其中描述的嵌入作为键，代码作为值（见图2）。当规划器建议一个新任务时，GPT-3.5会生成一个完成任务的一般建议。他们使用建议解决方案的嵌入，并结合环境反馈，从技能库中检索出前5个相关技能（见图2）。
- en: Adding long-term memory can significantly boost the performance. Fig. 3 demonstrates
    how critical the skill library is for [VOYAGER](https://arxiv.org/pdf/2305.16291.pdf).
    This also indicates that adding a skill library to Auto-GPT can substantially
    improve its performance. Both short-term and long-term memory work with the Controller
    to generate and refine its policy in order to accomplish the goal.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 添加长期记忆可以显著提升性能。图3展示了[VOYAGER](https://arxiv.org/pdf/2305.16291.pdf)中的技能库是多么关键。这也表明，将技能库添加到Auto-GPT中可以大大提高其性能。短期记忆和长期记忆与控制器协作，以生成和完善政策，以实现目标。
- en: '![](../Images/2e04ad60b6620560a123af2d61c35b83.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2e04ad60b6620560a123af2d61c35b83.png)'
- en: 'Fig. 3: Adding skill library to AutoGPT boosts its performance in zero shot
    learning generalization to unseen tasks (image source [VOYAGER](https://arxiv.org/pdf/2305.16291.pdf)
    ).'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：将技能库添加到AutoGPT中，可以提升其在零样本学习中的泛化能力（图片来源[VOYAGER](https://arxiv.org/pdf/2305.16291.pdf)）。
- en: '**Critic:**'
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**Critic：**'
- en: Critic or self-verification is an LLM based module which provides critique on
    the previously executed plan and provides feedback on how to refine the plan to
    accomplish the task. Reflexion enhances agent reasoning with dynamic memory and
    self-reflection. The self-reflection is a GPT-4, playing the role of Critic. It
    takes the reward signal, current trajectory and its persistent memory to generate
    verbal feedback for self-improvement for future trials. This feedback is more
    informative than a scalar reward and is stored in the memory to be used by the
    Planner to refine the plan.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: Critic或自我验证是一个基于LLM的模块，它对之前执行的计划提供批评，并提供关于如何完善计划以完成任务的反馈。Reflexion通过动态记忆和自我反思来增强代理推理。自我反思是一个GPT-4，扮演Critic的角色。它利用奖励信号、当前轨迹及其持久记忆生成口头反馈，以便为未来的尝试进行自我改进。这种反馈比标量奖励更具信息量，并存储在记忆中，以供规划器用来完善计划。
- en: '[VOYAGER](https://arxiv.org/pdf/2305.16291.pdf) and [DEPS](https://arxiv.org/pdf/2302.01560.pdf)
    execute the generated actions, code, by the Controller to obtain the Environment
    feedback and possibly execution errors. This information is incorporated into
    the Critic prompt, where it is asked to act as a critic and determine whether
    the goal is completed or not. Moreover, if the task has failed, it provides suggestions
    on how to complete the task.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '[VOYAGER](https://arxiv.org/pdf/2305.16291.pdf)和[DEPS](https://arxiv.org/pdf/2302.01560.pdf)由控制器执行生成的动作代码，以获取环境反馈和可能的执行错误。这些信息被纳入Critic提示中，要求其充当批评者，判断目标是否完成。此外，如果任务失败，它会提供如何完成任务的建议。'
- en: '**Descriptor:**'
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**描述符：**'
- en: In LLM based lifelong learning the Planner input and output is text. Some environments
    such as Crafter are text based while for the rest of the environments, they return
    a rendering of 2D or 3D image, or possibly a few state variables. A descriptor
    acts as a bridge, converting the modalities into text and incorporating them into
    an LLM’s prompt.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在基于LLM的终身学习中，规划器的输入和输出是文本。一些环境，如Crafter，是基于文本的，而其他环境则返回2D或3D图像的渲染，或者可能是一些状态变量。描述符作为桥梁，将这些模态转换为文本，并将其纳入LLM的提示中。
- en: '**Autonomous AI agents:**'
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**自主AI代理：**'
- en: This blog primarily discusses recent studies that integrate foundational models
    with continual learning, a significant stride towards achieving AGI. However,
    it’s important to recognize that these approaches represent a subset of the broader
    endeavor to develop autonomous agents. Several notable initiatives likely served
    as catalysts for the research discussed here. We will briefly highlight these
    in the following section.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 本博客主要讨论了将基础模型与持续学习整合的近期研究，这是朝着实现 AGI 迈出的重要一步。然而，重要的是要认识到，这些方法仅代表了开发自主代理这一广泛努力的一个子集。一些显著的举措可能作为了这里讨论研究的催化剂。我们将在接下来的部分中简要介绍这些举措。
- en: Recently, several works, such as [AutoGPT](https://github.com/Significant-Gravitas/AutoGPT)
    and [BabyAGI](https://github.com/yoheinakajima/babyagi), appear to be inspiring
    in using LLMs as the brain, and are designed to be an autonomous agent to solve
    complex problems. You provide them with a task. They run in a loop, breaking the
    task into subtasks, prompting themselves, responding to the prompt, and repeating
    the process until they achieve the provided goal. They also can have access to
    different APIs, such as internet access, which can substantially broaden their
    use cases.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，一些工作，例如 [AutoGPT](https://github.com/Significant-Gravitas/AutoGPT) 和 [BabyAGI](https://github.com/yoheinakajima/babyagi)，似乎在使用
    LLM 作为“大脑”方面具有启发性，并设计为一种自主代理来解决复杂问题。你给它们提供一个任务。它们在循环中运行，将任务拆分为子任务，自我提示，响应提示，并重复这一过程直到达到设定的目标。它们还可以访问不同的
    API，例如互联网访问，这可以大大拓宽它们的应用场景。
- en: '[AutoGPT](https://github.com/Significant-Gravitas/AutoGPT) is both a GPT-3.5
    and a GPT-4, teamed up with a companion bot that guides and tells them what to
    do. [AutoGPT](https://github.com/Significant-Gravitas/AutoGPT) has internet access
    and is able to interact with apps, software, and services, both online and local.
    To accomplish a higher-level goal given by humans, [AutoGPT](https://github.com/Significant-Gravitas/AutoGPT)
    uses a format of prompting called Reason and ACT (ReACT). ReACT enables the agent
    to receive an input, understand it, act based on it, reason upon the results,
    and then re-run that loop if needed. Since [AutoGPT](https://github.com/Significant-Gravitas/AutoGPT)
    can prompt itself, it can think and reason while accomplishing the task, looking
    for solutions, discard the unsuccessful ones, and consider different options.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '[AutoGPT](https://github.com/Significant-Gravitas/AutoGPT) 是 GPT-3.5 和 GPT-4
    的结合体，配有一个指导和指示它们该做什么的伴随机器人。[AutoGPT](https://github.com/Significant-Gravitas/AutoGPT)
    拥有互联网访问权限，并能够与应用程序、软件和服务（包括在线和本地）互动。为了完成由人类设定的高级目标，[AutoGPT](https://github.com/Significant-Gravitas/AutoGPT)
    使用了一种名为 Reason and ACT（ReACT）的提示格式。ReACT 使代理能够接收输入、理解它、根据它采取行动、对结果进行推理，并在必要时重新运行该循环。由于
    [AutoGPT](https://github.com/Significant-Gravitas/AutoGPT) 可以自我提示，它可以在完成任务的过程中进行思考和推理，寻找解决方案，丢弃不成功的方案，并考虑不同的选项。'
- en: '[BabyAGI](https://github.com/yoheinakajima/babyagi) is another recently introduced
    autonomous AI agent. It has three LLM based components (See Fig. 4):'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '[BabyAGI](https://github.com/yoheinakajima/babyagi) 是另一种最近推出的自主 AI 代理。它包含三个基于
    LLM 的组件（见图 4）：'
- en: 1- There is a task creation agent that comes up with a task list (similar to
    Planer)
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 1- 有一个任务创建代理，它生成任务列表（类似于 Planer）
- en: 2- A prioritization agent tries to prioritize a task list by LLM prompting (similar
    to Selector)
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 2- 一个优先级代理尝试通过 LLM 提示来优先排序任务列表（类似于 Selector）
- en: 3- An execution agent (Similar to Controller) executes a task with highest priority.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 3- 一个执行代理（类似于 Controller）执行优先级最高的任务。
- en: Both [AutoGPT](https://github.com/Significant-Gravitas/AutoGPT) and [BabyAGI](https://github.com/yoheinakajima/babyagi)
    use a vector store under the hood to store intermediate results and learn from
    the experiences.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '[AutoGPT](https://github.com/Significant-Gravitas/AutoGPT) 和 [BabyAGI](https://github.com/yoheinakajima/babyagi)
    都在底层使用向量存储来存储中间结果并从经验中学习。'
- en: '![](../Images/85119b4c95c6e1b41da7935feec9592d.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/85119b4c95c6e1b41da7935feec9592d.png)'
- en: 'Fig. 4: [BabyAGI](https://github.com/yoheinakajima/babyagi) flowchart diagram
    (image source [Yohei Nakajima’s website](https://yoheinakajima.com/task-driven-autonomous-agent-utilizing-gpt-4-pinecone-and-langchain-for-diverse-applications/))'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '图 4: [BabyAGI](https://github.com/yoheinakajima/babyagi) 流程图（图像来源 [Yohei Nakajima
    的网站](https://yoheinakajima.com/task-driven-autonomous-agent-utilizing-gpt-4-pinecone-and-langchain-for-diverse-applications/)）'
- en: '**Limitations and challenges:**'
  id: totrans-70
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**局限性和挑战：**'
- en: '**1-** LLM based lifelong learning heavily depends on the reliability of LLMs
    to accurately understand the environment and effectively plan and critique. However,
    studies reveal that LLMs can sometimes produce hallucinations, make up facts,
    and assign tasks that do not exist. Notably, in some of the referenced studies,
    substituting GPT-4 with GPT-3.5 led to a significant decline in performance, underscoring
    the critical role of the LLM model employed.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '**1-** 基于LLM的终身学习在很大程度上依赖于LLM对环境的准确理解以及有效的计划和批评。然而，研究表明，LLM有时会产生幻觉、编造事实，并分配不存在的任务。值得注意的是，在一些参考的研究中，将GPT-4替换为GPT-3.5导致性能显著下降，突显了所使用LLM模型的关键作用。'
- en: 2- LLMs exhibit inaccuracies when employed as a Planner or a Critic. The Critic
    may provide incorrect feedback or fail to accurately verify task completion. Similarly,
    the Planner might become trapped in a repetitive cycle, unable to adjust its plan
    even after several attempts. Adding a well-designed, event-triggered human intervention
    process can boost the performance of these models in such scenarios.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 2- 当LLM被用作规划者或批评者时，常常会出现不准确的情况。批评者可能提供不正确的反馈或无法准确验证任务完成情况。同样，规划者可能陷入重复循环，尽管尝试了多次也无法调整其计划。在这种情况下，添加一个设计良好的、事件驱动的人类干预过程可以提升这些模型的性能。
- en: 3- The limited context length in LLMs restricts the short-term memory capability,
    affecting their ability to retain detailed past experiences and their results,
    detailed instructions, and available control primitive APIs. A long context length
    is very critical, especially in self-verification, to learn from past experiences
    and failures. Despite ongoing research efforts, to extend context length or employ
    methods like Transformer-XL, in most cases authors used GPT-4 with the maximum
    of 8,192 tokens context length.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 3- LLM的有限上下文长度限制了短期记忆能力，影响了其保留详细过去经验及其结果、详细指令和可用控制原语API的能力。较长的上下文长度非常关键，尤其是在自我验证中，以便从过去的经验和失败中学习。尽管有持续的研究努力，以延长上下文长度或采用Transformer-XL等方法，但在大多数情况下，作者使用了上下文长度最大为8,192个令牌的GPT-4。
- en: 4- Most of these works, except SPRING, assume that the LLM knows all the necessary
    information required to initiate lifelong learning before starting the experiment.
    However, this assumption might not always hold true. Providing Internet access
    to agents, as in AutoGPT, or providing textual material as input context, as in
    SPRING, can be helpful to address follow-up questions.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 4- 除了SPRING之外，大多数这些工作假设LLM在实验开始之前就已经知道了启动终身学习所需的所有必要信息。然而，这一假设可能并不总是成立。为代理提供互联网访问权限，如AutoGPT中所示，或提供文本材料作为输入上下文，如SPRING中所示，可以帮助解决后续问题。
- en: '**References:**'
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**参考文献：**'
- en: '[1] VOYAGER: Wang, Guanzhi, et al. “Voyager: An open-ended embodied agent with
    large language models.”, 2023'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] VOYAGER: Wang, Guanzhi, et al. “Voyager: An open-ended embodied agent with
    large language models.”, 2023'
- en: '[2] DEPS: Wang, Zihao, et al. “Describe, explain, plan and select: Interactive
    planning with large language models enables open-world multi-task agents.”, 2023'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] DEPS: Wang, Zihao, et al. “Describe, explain, plan and select: Interactive
    planning with large language models enables open-world multi-task agents.”, 2023'
- en: '[3] SPRING: Wu, Yue, et al. “SPRING: GPT-4 Out-performs RL Algorithms by Studying
    Papers and Reasoning.”, 2023'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] SPRING: Wu, Yue, et al. “SPRING: GPT-4 Out-performs RL Algorithms by Studying
    Papers and Reasoning.”, 2023'
- en: '[4] Reflexion: Shinn, Noah, et al. “Reflexion: Language agents with verbal
    reinforcement learning.”, 2023'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] Reflexion: Shinn, Noah, et al. “Reflexion: Language agents with verbal
    reinforcement learning.”, 2023'
- en: '[5] Progprompt: Singh, Ishika, et al. “Progprompt: Generating situated robot
    task plans using large language models.”, 2023'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] Progprompt: Singh, Ishika, et al. “Progprompt: Generating situated robot
    task plans using large language models.”, 2023'
- en: '[6] React: Yao, Shunyu, et al. “React: Synergizing reasoning and acting in
    language models.”, 2022'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '[6] React: Yao, Shunyu, et al. “React: Synergizing reasoning and acting in
    language models.”, 2022'
- en: '[7] CaP: Liang, Jacky, et al. “Code as policies: Language model programs for
    embodied control.”, *2023*'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '[7] CaP: Liang, Jacky, et al. “Code as policies: Language model programs for
    embodied control.”, *2023*'
- en: '[8] AutoGPT. [https://github.com/Significant-Gravitas/Auto-GPT](https://github.com/Significant-Gravitas/Auto-GPT)'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '[8] AutoGPT. [https://github.com/Significant-Gravitas/Auto-GPT](https://github.com/Significant-Gravitas/Auto-GPT)'
- en: '[9] babyAGI: [https://github.com/yoheinakajima/babyagi](https://github.com/yoheinakajima/babyagi)'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '[9] babyAGI: [https://github.com/yoheinakajima/babyagi](https://github.com/yoheinakajima/babyagi)'
- en: '[10] Weng, Lilian, et al. LLM-powered Autonomous Agents”, 2023'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '[10] Weng, Lilian, et al. LLM-powered Autonomous Agents”, 2023'
