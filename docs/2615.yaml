- en: Why OpenAI’s API Is More Expensive for Non-English Languages
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/why-openais-api-is-more-expensive-for-non-english-languages-553da4a1eecc?source=collection_archive---------4-----------------------#2023-08-16](https://towardsdatascience.com/why-openais-api-is-more-expensive-for-non-english-languages-553da4a1eecc?source=collection_archive---------4-----------------------#2023-08-16)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Beyond words: How byte pair encoding and Unicode encoding factor into pricing
    disparities'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@iamleonie?source=post_page-----553da4a1eecc--------------------------------)[![Leonie
    Monigatti](../Images/4044b1685ada53a30160b03dc78f9626.png)](https://medium.com/@iamleonie?source=post_page-----553da4a1eecc--------------------------------)[](https://towardsdatascience.com/?source=post_page-----553da4a1eecc--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----553da4a1eecc--------------------------------)
    [Leonie Monigatti](https://medium.com/@iamleonie?source=post_page-----553da4a1eecc--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F3a38da70d8dc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhy-openais-api-is-more-expensive-for-non-english-languages-553da4a1eecc&user=Leonie+Monigatti&userId=3a38da70d8dc&source=post_page-3a38da70d8dc----553da4a1eecc---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----553da4a1eecc--------------------------------)
    ·7 min read·Aug 16, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F553da4a1eecc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhy-openais-api-is-more-expensive-for-non-english-languages-553da4a1eecc&user=Leonie+Monigatti&userId=3a38da70d8dc&source=-----553da4a1eecc---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F553da4a1eecc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhy-openais-api-is-more-expensive-for-non-english-languages-553da4a1eecc&source=-----553da4a1eecc---------------------bookmark_footer-----------)![](../Images/7dbc785874f6574c212f692b9a203709.png)'
  prefs: []
  type: TYPE_NORMAL
- en: How can it be that the phrase “Hello world” has two tokens in English and 12
    tokens in Hindi?
  prefs: []
  type: TYPE_NORMAL
- en: After publishing [my recent article on how to estimate the cost for OpenAI’s
    API,](https://medium.com/towards-data-science/easily-estimate-your-openai-api-costs-with-tiktoken-c17caf6d015e)
    I received an interesting comment that someone had noticed that the OpenAI API
    is much more expensive in other languages, such as ones using Chinese, Japanese,
    or Korean (CJK) characters, than in English.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8e0c5de69c8cc1d758147b74a591882f.png)'
  prefs: []
  type: TYPE_IMG
- en: Comment by a reader on [my recent article on how to estimate the cost for OpenAI’s
    API with the](https://medium.com/towards-data-science/easily-estimate-your-openai-api-costs-with-tiktoken-c17caf6d015e)
    `[tiktoken](https://medium.com/towards-data-science/easily-estimate-your-openai-api-costs-with-tiktoken-c17caf6d015e)`
    [library](https://medium.com/towards-data-science/easily-estimate-your-openai-api-costs-with-tiktoken-c17caf6d015e)
  prefs: []
  type: TYPE_NORMAL
- en: 'I wasn’t aware of this issue, but quickly realized that this is an active research
    field: At the beginning of this year, a paper called “Language Model Tokenizers
    Introduce Unfairness Between Languages” by Petrov et al. [2] showed that the “same
    text translated into different languages can have drastically different tokenization
    lengths, with differences up to 15 times in some cases.”'
  prefs: []
  type: TYPE_NORMAL
- en: As a refresher, tokenization is the process of splitting a text into a list
    of tokens, which are common sequences of characters in a text.
  prefs: []
  type: TYPE_NORMAL
