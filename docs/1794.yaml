- en: Word Embeddings, Explained
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/word-embeddings-explained-c07c5ea44d64?source=collection_archive---------10-----------------------#2023-05-30](https://towardsdatascience.com/word-embeddings-explained-c07c5ea44d64?source=collection_archive---------10-----------------------#2023-05-30)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[](https://medium.com/@dataemporium?source=post_page-----c07c5ea44d64--------------------------------)[![Ajay
    Halthor](../Images/1be821c8d8ed336b9ecedcf94f960ede.png)](https://medium.com/@dataemporium?source=post_page-----c07c5ea44d64--------------------------------)[](https://towardsdatascience.com/?source=post_page-----c07c5ea44d64--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----c07c5ea44d64--------------------------------)
    [Ajay Halthor](https://medium.com/@dataemporium?source=post_page-----c07c5ea44d64--------------------------------)'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: ·
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fb0a3e7e495ca&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fword-embeddings-explained-c07c5ea44d64&user=Ajay+Halthor&userId=b0a3e7e495ca&source=post_page-b0a3e7e495ca----c07c5ea44d64---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----c07c5ea44d64--------------------------------)
    ·7 min read·May 30, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fc07c5ea44d64&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fword-embeddings-explained-c07c5ea44d64&user=Ajay+Halthor&userId=b0a3e7e495ca&source=-----c07c5ea44d64---------------------clap_footer-----------)'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc07c5ea44d64&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fword-embeddings-explained-c07c5ea44d64&source=-----c07c5ea44d64---------------------bookmark_footer-----------)'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: In natural language processing, we work with words. However, computers cannot
    directly understand words, necessitating their conversion into numerical representations.
    These numeric representations, known as vectors or embeddings, comprise numbers
    that can be either interpretable or non-interpretable by humans. In this blog,
    we will delve into the advancements made in learning these word representations
    over time.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: 1 N-grams
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/b72ba194d24fb7590374f82b5317df2e.png)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: N-gram vector representation of a sentence (image by author)'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: Let’s take the example of n-grams to understand the process better. Imagine
    we have a sentence that we want the computer to comprehend. To achieve this, we
    convert the sentence into a numeric representation. This representation includes
    various combinations of words, such as unigrams (single words), bigrams (pairs
    of words), trigrams (groups of three words), and even higher-order n-grams. The
    result is a vector that could represent any English sentence.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解这个过程，我们以n-grams为例。假设我们有一个句子，希望计算机能够理解。为实现这一目标，我们将句子转换为数字表示。这个表示包括各种单词组合，例如unigrams（单个单词）、bigrams（词对）、trigrams（三词组），以及更高阶的n-grams。最终得到的结果是一个可以表示任何英文句子的向量。
- en: In Figure 1, let’s consider encoding the sentence “This is a good day“. Say
    the first position of the vector represents the number of cases the bigram “good
    day” occurs in the original sentence. Since it occurs once, the numeric representation
    is “1” for this first position. In the same way, we can represent every unigram,
    diagram and trigram with different positions in this vector.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在图1中，我们考虑对句子“This is a good day”进行编码。假设向量的第一个位置表示在原句中bigram“good day”出现的次数。由于它出现了一次，所以该位置的数字表示为“1”。以同样的方式，我们可以用向量中的不同位置表示每一个unigram、bigram和trigram。
