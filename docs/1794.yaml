- en: Word Embeddings, Explained
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/word-embeddings-explained-c07c5ea44d64?source=collection_archive---------10-----------------------#2023-05-30](https://towardsdatascience.com/word-embeddings-explained-c07c5ea44d64?source=collection_archive---------10-----------------------#2023-05-30)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[](https://medium.com/@dataemporium?source=post_page-----c07c5ea44d64--------------------------------)[![Ajay
    Halthor](../Images/1be821c8d8ed336b9ecedcf94f960ede.png)](https://medium.com/@dataemporium?source=post_page-----c07c5ea44d64--------------------------------)[](https://towardsdatascience.com/?source=post_page-----c07c5ea44d64--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----c07c5ea44d64--------------------------------)
    [Ajay Halthor](https://medium.com/@dataemporium?source=post_page-----c07c5ea44d64--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fb0a3e7e495ca&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fword-embeddings-explained-c07c5ea44d64&user=Ajay+Halthor&userId=b0a3e7e495ca&source=post_page-b0a3e7e495ca----c07c5ea44d64---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----c07c5ea44d64--------------------------------)
    ·7 min read·May 30, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fc07c5ea44d64&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fword-embeddings-explained-c07c5ea44d64&user=Ajay+Halthor&userId=b0a3e7e495ca&source=-----c07c5ea44d64---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc07c5ea44d64&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fword-embeddings-explained-c07c5ea44d64&source=-----c07c5ea44d64---------------------bookmark_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: In natural language processing, we work with words. However, computers cannot
    directly understand words, necessitating their conversion into numerical representations.
    These numeric representations, known as vectors or embeddings, comprise numbers
    that can be either interpretable or non-interpretable by humans. In this blog,
    we will delve into the advancements made in learning these word representations
    over time.
  prefs: []
  type: TYPE_NORMAL
- en: 1 N-grams
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/b72ba194d24fb7590374f82b5317df2e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: N-gram vector representation of a sentence (image by author)'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s take the example of n-grams to understand the process better. Imagine
    we have a sentence that we want the computer to comprehend. To achieve this, we
    convert the sentence into a numeric representation. This representation includes
    various combinations of words, such as unigrams (single words), bigrams (pairs
    of words), trigrams (groups of three words), and even higher-order n-grams. The
    result is a vector that could represent any English sentence.
  prefs: []
  type: TYPE_NORMAL
- en: In Figure 1, let’s consider encoding the sentence “This is a good day“. Say
    the first position of the vector represents the number of cases the bigram “good
    day” occurs in the original sentence. Since it occurs once, the numeric representation
    is “1” for this first position. In the same way, we can represent every unigram,
    diagram and trigram with different positions in this vector.
  prefs: []
  type: TYPE_NORMAL
