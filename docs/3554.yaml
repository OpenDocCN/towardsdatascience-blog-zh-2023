- en: 'Decoding LLMs: Creating Transformer Encoders and Multi-Head Attention Layers
    in Python from Scratch'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 解码LLMs：从零开始在Python中创建Transformer编码器和多头注意力层
- en: 原文：[https://towardsdatascience.com/decoding-llms-creating-transformer-encoders-and-multi-head-attention-layers-in-python-from-scratch-631429553ce8?source=collection_archive---------2-----------------------#2023-12-01](https://towardsdatascience.com/decoding-llms-creating-transformer-encoders-and-multi-head-attention-layers-in-python-from-scratch-631429553ce8?source=collection_archive---------2-----------------------#2023-12-01)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/decoding-llms-creating-transformer-encoders-and-multi-head-attention-layers-in-python-from-scratch-631429553ce8?source=collection_archive---------2-----------------------#2023-12-01](https://towardsdatascience.com/decoding-llms-creating-transformer-encoders-and-multi-head-attention-layers-in-python-from-scratch-631429553ce8?source=collection_archive---------2-----------------------#2023-12-01)
- en: Exploring the intricacies of encoder, multi-head attention, and positional encoding
    in large language models
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 探索大型语言模型中编码器、多头注意力和位置编码的复杂性
- en: '[](https://medium.com/@luisroque?source=post_page-----631429553ce8--------------------------------)[![Luís
    Roque](../Images/e281d470b403375ba3c6f521b1ccf915.png)](https://medium.com/@luisroque?source=post_page-----631429553ce8--------------------------------)[](https://towardsdatascience.com/?source=post_page-----631429553ce8--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----631429553ce8--------------------------------)
    [Luís Roque](https://medium.com/@luisroque?source=post_page-----631429553ce8--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@luisroque?source=post_page-----631429553ce8--------------------------------)[![Luís
    Roque](../Images/e281d470b403375ba3c6f521b1ccf915.png)](https://medium.com/@luisroque?source=post_page-----631429553ce8--------------------------------)[](https://towardsdatascience.com/?source=post_page-----631429553ce8--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----631429553ce8--------------------------------)
    [Luís Roque](https://medium.com/@luisroque?source=post_page-----631429553ce8--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F2195f049db86&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdecoding-llms-creating-transformer-encoders-and-multi-head-attention-layers-in-python-from-scratch-631429553ce8&user=Lu%C3%ADs+Roque&userId=2195f049db86&source=post_page-2195f049db86----631429553ce8---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----631429553ce8--------------------------------)
    ·13 min read·Dec 1, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F631429553ce8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdecoding-llms-creating-transformer-encoders-and-multi-head-attention-layers-in-python-from-scratch-631429553ce8&user=Lu%C3%ADs+Roque&userId=2195f049db86&source=-----631429553ce8---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F2195f049db86&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdecoding-llms-creating-transformer-encoders-and-multi-head-attention-layers-in-python-from-scratch-631429553ce8&user=Lu%C3%ADs+Roque&userId=2195f049db86&source=post_page-2195f049db86----631429553ce8---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----631429553ce8--------------------------------)
    ·13分钟阅读·2023年12月1日'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F631429553ce8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdecoding-llms-creating-transformer-encoders-and-multi-head-attention-layers-in-python-from-scratch-631429553ce8&source=-----631429553ce8---------------------bookmark_footer-----------)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F631429553ce8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdecoding-llms-creating-transformer-encoders-and-multi-head-attention-layers-in-python-from-scratch-631429553ce8&source=-----631429553ce8---------------------bookmark_footer-----------)'
- en: '*This post was co-authored with Rafael Nardi.*'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '*本文由Rafael Nardi共同撰写。*'
- en: Introduction
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: 'Today, Computational Natural Language Processing (NLP) is a rapidly evolving
    endeavour in which the power of computation meets linguistics. The linguistic
    side of it is mainly attributed to the theory of *Distributive Semantics* by John
    Rupert Firth. He once said the following:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 今天，计算自然语言处理（NLP）是一个快速发展的领域，其中计算能力与语言学相结合。其语言学方面主要归功于John Rupert Firth的*分布式语义*理论。他曾说过以下话：
- en: '*“You shall know a word by the company it keeps”*'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '*“你可以通过词语的上下文了解它的含义”*'
- en: So, the semantic representation of a word is determined by the context in which
    it is being used. It is precisely in attendance to this assumption that the paper
    “Attention is all you need” by Ashish Vaswani et. al. [[1]](https://github.com/zaai-ai/large-language-models-math/blob/main/attention_is_all_you_need.md#attention)
    assumes its groundbreaking relevance. It set the transformer architecture as the
    core of many of the rapidly growing tools like BERT, GPT4, Llama, etc.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，词语的语义表示由其使用的上下文决定。正是基于这一假设，Ashish Vaswani 等人所著的论文“Attention is all you need”[[1]](https://github.com/zaai-ai/large-language-models-math/blob/main/attention_is_all_you_need.md#attention)
    才具备了其开创性的意义。它将变换器架构确立为诸如 BERT、GPT4、Llama 等许多快速增长的工具的核心。
- en: In this article, we examine the key mathematical operations at the heart of
    the encoder segment in the transformer architecture.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们深入探讨了变换器架构中编码器部分核心的关键数学运算。
- en: '![](../Images/4ac257ecfec0d1b7c2570c38e383cfbc.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4ac257ecfec0d1b7c2570c38e383cfbc.png)'
- en: 'Figure 1: Self-Attention is complex (image by author)'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：自注意力机制复杂（图像作者提供）
