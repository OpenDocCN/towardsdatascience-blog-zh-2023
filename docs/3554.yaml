- en: 'Decoding LLMs: Creating Transformer Encoders and Multi-Head Attention Layers
    in Python from Scratch'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/decoding-llms-creating-transformer-encoders-and-multi-head-attention-layers-in-python-from-scratch-631429553ce8?source=collection_archive---------2-----------------------#2023-12-01](https://towardsdatascience.com/decoding-llms-creating-transformer-encoders-and-multi-head-attention-layers-in-python-from-scratch-631429553ce8?source=collection_archive---------2-----------------------#2023-12-01)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Exploring the intricacies of encoder, multi-head attention, and positional encoding
    in large language models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@luisroque?source=post_page-----631429553ce8--------------------------------)[![Luís
    Roque](../Images/e281d470b403375ba3c6f521b1ccf915.png)](https://medium.com/@luisroque?source=post_page-----631429553ce8--------------------------------)[](https://towardsdatascience.com/?source=post_page-----631429553ce8--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----631429553ce8--------------------------------)
    [Luís Roque](https://medium.com/@luisroque?source=post_page-----631429553ce8--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F2195f049db86&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdecoding-llms-creating-transformer-encoders-and-multi-head-attention-layers-in-python-from-scratch-631429553ce8&user=Lu%C3%ADs+Roque&userId=2195f049db86&source=post_page-2195f049db86----631429553ce8---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----631429553ce8--------------------------------)
    ·13 min read·Dec 1, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F631429553ce8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdecoding-llms-creating-transformer-encoders-and-multi-head-attention-layers-in-python-from-scratch-631429553ce8&user=Lu%C3%ADs+Roque&userId=2195f049db86&source=-----631429553ce8---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F631429553ce8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdecoding-llms-creating-transformer-encoders-and-multi-head-attention-layers-in-python-from-scratch-631429553ce8&source=-----631429553ce8---------------------bookmark_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: '*This post was co-authored with Rafael Nardi.*'
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Today, Computational Natural Language Processing (NLP) is a rapidly evolving
    endeavour in which the power of computation meets linguistics. The linguistic
    side of it is mainly attributed to the theory of *Distributive Semantics* by John
    Rupert Firth. He once said the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '*“You shall know a word by the company it keeps”*'
  prefs: []
  type: TYPE_NORMAL
- en: So, the semantic representation of a word is determined by the context in which
    it is being used. It is precisely in attendance to this assumption that the paper
    “Attention is all you need” by Ashish Vaswani et. al. [[1]](https://github.com/zaai-ai/large-language-models-math/blob/main/attention_is_all_you_need.md#attention)
    assumes its groundbreaking relevance. It set the transformer architecture as the
    core of many of the rapidly growing tools like BERT, GPT4, Llama, etc.
  prefs: []
  type: TYPE_NORMAL
- en: In this article, we examine the key mathematical operations at the heart of
    the encoder segment in the transformer architecture.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4ac257ecfec0d1b7c2570c38e383cfbc.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Self-Attention is complex (image by author)'
  prefs: []
  type: TYPE_NORMAL
