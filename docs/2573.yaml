- en: Text Classification with Transformer Encoders
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/text-classification-with-transformer-encoders-1dcaa50dabae?source=collection_archive---------4-----------------------#2023-08-11](https://towardsdatascience.com/text-classification-with-transformer-encoders-1dcaa50dabae?source=collection_archive---------4-----------------------#2023-08-11)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Step-by-step explanation of utilizing Transformer encoders to classify text
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@marcellusruben?source=post_page-----1dcaa50dabae--------------------------------)[![Ruben
    Winastwan](../Images/15ad0dd03bf5892510abdf166a1e91e1.png)](https://medium.com/@marcellusruben?source=post_page-----1dcaa50dabae--------------------------------)[](https://towardsdatascience.com/?source=post_page-----1dcaa50dabae--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----1dcaa50dabae--------------------------------)
    [Ruben Winastwan](https://medium.com/@marcellusruben?source=post_page-----1dcaa50dabae--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F5dae9da73c9b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftext-classification-with-transformer-encoders-1dcaa50dabae&user=Ruben+Winastwan&userId=5dae9da73c9b&source=post_page-5dae9da73c9b----1dcaa50dabae---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----1dcaa50dabae--------------------------------)
    ·15 min read·Aug 11, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F1dcaa50dabae&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftext-classification-with-transformer-encoders-1dcaa50dabae&user=Ruben+Winastwan&userId=5dae9da73c9b&source=-----1dcaa50dabae---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F1dcaa50dabae&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftext-classification-with-transformer-encoders-1dcaa50dabae&source=-----1dcaa50dabae---------------------bookmark_footer-----------)![](../Images/e7af55b2adc4de88204e68d923263996.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Photo by [Mel Poole](https://unsplash.com/@melpoole?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    on [Unsplash](https://unsplash.com/photos/lBsvzgYnzPU?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
  prefs: []
  type: TYPE_NORMAL
- en: Transformer is, without a doubt, one of the most important breakthroughs in
    the field of deep learning. The encoder-decoder architecture of this model has
    proven to be powerful in cross-domain applications.
  prefs: []
  type: TYPE_NORMAL
- en: Initially, Transformer was used solely for language modeling tasks, such as
    machine translation, text generation, text classification, question-answering,
    etc. However, recently, Transformer has also been used for computer vision tasks,
    such as image classification, object detection, and semantic segmentation.
  prefs: []
  type: TYPE_NORMAL
- en: Given its popularity and the existence of numerous Transformer-based sophisticated
    models such as BERT, Vision-Transformer, Swin-Transformer, and the GPT family,
    it is crucial for us to understand the inner workings of the Transformer architecture.
  prefs: []
  type: TYPE_NORMAL
- en: In this article, we will dissect only the encoder part of Transformer, which
    can be used mainly for classification purposes. Specifically, we will use the
    Transformer encoders to classify texts. Without further ado, let’s first take
    a look at the dataset that we’re going to use in this article.
  prefs: []
  type: TYPE_NORMAL
- en: About the Dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
