["```py\nfrom transformers import AutoTokenizer\nimport emoji\n```", "```py\ntokenizer = AutoTokenizer.from_pretrained('bert-base-uncased') \n```", "```py\nemoji_list = list(emoji.EMOJI_DATA.keys())\ncnt = 0\nfor e in emoji_list:\n    tokenized = tokenizer.decode(tokenizer.encode(e)).strip(\"</s>\").strip()\n    if e not in tokenized:\n        cnt += 1\nprint(f\"{cnt/len(emoji_list)} of the emojis are not identified by this tokenizer.\")\n```", "```py\ndef emoji2description(text):\n  return emoji.replace_emoji(text, replace=lambda chars, data_dict: ' '.join(data_dict['en'].split('_')).strip(':'))\n```", "```py\ndef emoji2concat_emoji(text):\n    emoji_list = emoji.emoji_list(text)\n    ret = emoji.replace_emoji(text, replace='').strip()\n    for json in emoji_list:\n        this_emoji = json['emoji']\n        ret += ' ' + this_emoji\n    return ret\n```", "```py\ndef emoji2concat_description(text):\n    emoji_list = emoji.emoji_list(text)\n    ret = emoji.replace_emoji(text, replace='').strip()\n    for json in emoji_list:\n        this_desc = ' '.join(emoji.EMOJI_DATA[json['emoji']]['en'].split('_')).strip(':')\n        ret += ' ' + this_desc\n    return ret\n```"]