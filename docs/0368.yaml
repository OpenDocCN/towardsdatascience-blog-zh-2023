- en: Can Reinforcement Learning Generalize Beyond Its Training?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/can-reinforcement-learning-generalize-beyond-its-training-3b9012d8e4cf?source=collection_archive---------20-----------------------#2023-01-24](https://towardsdatascience.com/can-reinforcement-learning-generalize-beyond-its-training-3b9012d8e4cf?source=collection_archive---------20-----------------------#2023-01-24)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: A case study in model generalization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@john_morrow?source=post_page-----3b9012d8e4cf--------------------------------)[![John
    Morrow](../Images/4a8ce62a0b4e1eb1cf77ecaba6b7ddcc.png)](https://medium.com/@john_morrow?source=post_page-----3b9012d8e4cf--------------------------------)[](https://towardsdatascience.com/?source=post_page-----3b9012d8e4cf--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----3b9012d8e4cf--------------------------------)
    [John Morrow](https://medium.com/@john_morrow?source=post_page-----3b9012d8e4cf--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fb4bcd051bb38&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcan-reinforcement-learning-generalize-beyond-its-training-3b9012d8e4cf&user=John+Morrow&userId=b4bcd051bb38&source=post_page-b4bcd051bb38----3b9012d8e4cf---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----3b9012d8e4cf--------------------------------)
    ·6 min read·Jan 24, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F3b9012d8e4cf&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcan-reinforcement-learning-generalize-beyond-its-training-3b9012d8e4cf&user=John+Morrow&userId=b4bcd051bb38&source=-----3b9012d8e4cf---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F3b9012d8e4cf&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcan-reinforcement-learning-generalize-beyond-its-training-3b9012d8e4cf&source=-----3b9012d8e4cf---------------------bookmark_footer-----------)![](../Images/7c6eb049b8373330c9019cd75174f39e.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Photo by [János Szüdi](https://unsplash.com/@szudi?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: 'The project detailed in the paper, [Reinforcement Learning: A Case Study in
    Model Generalization](https://github.com/jmorrow1000/RL-generalize/blob/main/Reinforcement%20Learning%20-%20A%20Case%20Study%20in%20Model%20Generalization.pdf?raw=true),
    explores the ability of a model trained with reinforcement learning (RL) to generalize,
    i.e., produce acceptable results when presented with data it was not exposed to
    during training. The application in this study is an industrial process with multiple
    controls that determine the effect on a product as it transitions through the
    process. Determining optimal control settings in this environment can be challenging.
    For example, when there are interactions between the controls, adjusting one setting
    can require the readjustment of other settings. Also, a complex relationship between
    a control and its effect complicates finding an optimal solution. The results
    presented here show that a model trained by an RL process performs well in this
    environment and is able to generalize to conditions different from those used
    for training.'
  prefs: []
  type: TYPE_NORMAL
- en: The [paper](https://morrowconsultants.com/rl-model-generalization-paper) describes
    an RL model trained to find the optimal control settings for a reflow oven used
    for soldering electronic components to a circuit board (Figure 1). The oven’s
    moving belt transports the product (i.e., the circuit board) through multiple
    heating zones. This process heats the product according to a temperature-time
    target profile required to produce reliable solder connections.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/449434f952710cedc7ac09afa5d05594.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: **Circuit boards on oven belt** (*Image via Adobe under license to
    John Morrow*)'
  prefs: []
  type: TYPE_NORMAL
- en: 'A human operator typically takes the following steps to determine the heater
    settings required to solder circuit boards successfully:'
  prefs: []
  type: TYPE_NORMAL
- en: • run one pass of the product through the oven
  prefs: []
  type: TYPE_NORMAL
- en: • observe the resulting temperature-time profile from the sensor readings
  prefs: []
  type: TYPE_NORMAL
- en: • adjust the heater settings to improve the profile toward the target profile
  prefs: []
  type: TYPE_NORMAL
- en: • wait for the oven temperature to stabilize to the new settings
  prefs: []
  type: TYPE_NORMAL
- en: • repeat this procedure until the profile from the sensor readings is acceptably
    close to the target profile
  prefs: []
  type: TYPE_NORMAL
- en: '**Learning the policy**'
  prefs: []
  type: TYPE_NORMAL
- en: An RL system replaces the operator steps with a two-stage process. In the first
    stage, an agent learns the dynamics of the oven and creates a policy for updating
    the heater settings under various oven conditions.
  prefs: []
  type: TYPE_NORMAL
- en: Since considerable time is required to stabilize an oven’s temperature after
    changing the heater settings and passing the product through the oven, an oven
    simulator is used to speed up the learning process. The simulator emulates a single
    pass of the product through the heating profile in a few seconds instead of the
    many minutes required by a physical oven. ([Part 2](https://medium.com/@john_morrow/can-reinforcement-learning-generalize-beyond-its-training-part-2-79d7b864dc55)
    presents the details of the simulator.)
  prefs: []
  type: TYPE_NORMAL
- en: In each pass of the learning stage, the agent takes an action from its current
    state by sending the simulator new settings for the eight heaters. After the simulation
    run, the simulator reports back the product temperature readings (three hundred
    readings taken at 1-second intervals).
  prefs: []
  type: TYPE_NORMAL
- en: The agent is rewarded for its action based on the difference between the returned
    readings and the target temperature-time profile. If the difference for the current
    run is less than the previous run, the reward is positive; otherwise, it is negative.
    A subset of the readings determines the new state of the system. The agent starts
    the next pass of the learning stage by taking action from the new state.
  prefs: []
  type: TYPE_NORMAL
- en: '**Planning with the policy**'
  prefs: []
  type: TYPE_NORMAL
- en: In the second stage, the agent follows the learned policy to find optimal heater
    settings. These settings will produce the closest match between the actual product
    profile and the target temperature-time profile. Figure 2 shows the result of
    the agent following the policy to find optimal settings. The blue trace is the
    target temperature-time profile, and the red trace is the actual profile produced
    by the optimal settings.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ae5249872da7b2f455c158de0603140e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: **Example planning result Blue trace: target profile. Red trace:
    actual product profile.**'
  prefs: []
  type: TYPE_NORMAL
- en: '**Reinforcement learning system**'
  prefs: []
  type: TYPE_NORMAL
- en: 'As discussed above, an RL system comprises an agent taking action in an environment
    to learn a policy to reach the target goal. The environment responds to each action
    with a reward indicating whether the action was good or bad toward achieving the
    goal. The environment also returns the state of the agent in the environment.
    The agent consists of two neural networks: the model network and the target network.
    The agent’s goal is to find heater settings that will produce a product time-temperature
    profile very close to the target profile. The environment is the reflow oven simulator.
    Figure 3shows the components of the RL system, each of which is described in detail
    in the paper.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/67227dda344339d0894f8f6dfddb2cb8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: **Reinforcement learning system**'
  prefs: []
  type: TYPE_NORMAL
- en: '**Generalization: state and reward definition**'
  prefs: []
  type: TYPE_NORMAL
- en: The state and reward definitions are critical to the RL model’s ability to generalize
    to new environments where the target profile and product parameters differ from
    those used during training. Specifically, both the state and reward are defined
    in terms of the relative difference between the product and target profile temperatures
    and normalized by the maximum range of allowed heater values.
  prefs: []
  type: TYPE_NORMAL
- en: State parameters are defined at the centers of the eight heater zones. Each
    state parameter is defined as the normalized difference between the temperature
    at the center of the product and the temperature of the profile at the center
    of each heater zone.
  prefs: []
  type: TYPE_NORMAL
- en: When the agent performs an action, the environment returns a reward indicating
    the effectiveness of the action in achieving the agent’s goal. The reward is based
    on whether the action reduced the total temperature difference between the actual
    temperatures and target profile. The state and reward functions are described
    in greater detail in the paper.
  prefs: []
  type: TYPE_NORMAL
- en: '**Results**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Following are two of the paper’s test results from running the planning process
    on various configurations of product materials and temperature-time profiles.
    All of the tests were run with a model neural network trained with the following
    product and profile parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/65ee37070838ea349f597be3f2bedfad.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Test 1: Baseline**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Test 1 is a baseline for testing the model’s performance with the same parameters
    used to train the model. Following are the test 1 errors, the optimal heat zone
    settings, and the target profile vs. actual temperature-time plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3e4804da2b61492040372400ab03a69e.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Test 1: Blue trace: target profile. Red trace: actual product profile.**'
  prefs: []
  type: TYPE_NORMAL
- en: '**Test 6**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Test 6 changes the product from FR4 to aluminum oxide (alumina 99%), changes
    the size of the product, and changes the profile. The oven parameter values are
    the same as used in the baseline of test 1, except that both the top and bottom
    heating elements are active. The following tables reflect the profile and product
    parameters used for this test (changes from the baseline training parameters are
    bold):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ed10da52ad97359a26fb5aee47904a36.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Following are the test 6 errors, the optimal heat zone settings, and the target
    profile vs. actual temperature/time plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/22a233e42efc39a0548e61a13f3038d3.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Conclusion**'
  prefs: []
  type: TYPE_NORMAL
- en: This project demonstrates that a reinforcement learning system can provide solutions
    to control a complex industrial process. Specifically, a reinforcement learning
    system successfully learns the optimal control settings of a reflow oven used
    to solder electronic components to a circuit board. Further, once trained, the
    system can generalize to produce acceptable results in environments with different
    requirements from those used during training.
  prefs: []
  type: TYPE_NORMAL
- en: 'Link to paper: [Reinforcement Learning: A Case Study in Model Generalization](https://github.com/jmorrow1000/RL-generalize/blob/main/Reinforcement%20Learning%20-%20A%20Case%20Study%20in%20Model%20Generalization.pdf?raw=true)'
  prefs: []
  type: TYPE_NORMAL
- en: All images, unless otherwise noted, are by the author.
  prefs: []
  type: TYPE_NORMAL
