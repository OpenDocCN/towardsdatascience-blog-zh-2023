- en: The Arrival of SDXL 1.0
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: SDXL 1.0 的到来
- en: 原文：[https://towardsdatascience.com/the-arrival-of-sdxl-1-0-4e739d5cc6c7?source=collection_archive---------3-----------------------#2023-08-02](https://towardsdatascience.com/the-arrival-of-sdxl-1-0-4e739d5cc6c7?source=collection_archive---------3-----------------------#2023-08-02)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/the-arrival-of-sdxl-1-0-4e739d5cc6c7?source=collection_archive---------3-----------------------#2023-08-02](https://towardsdatascience.com/the-arrival-of-sdxl-1-0-4e739d5cc6c7?source=collection_archive---------3-----------------------#2023-08-02)
- en: 'Introducing SDXL 1.0: Understanding the Diffusion Models'
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 介绍 SDXL 1.0：理解扩散模型
- en: '[](https://ertugruldemir.medium.com/?source=post_page-----4e739d5cc6c7--------------------------------)[![Ertuğrul
    Demir](../Images/bc35d3fcfade72c8a4e72d1fd952a73c.png)](https://ertugruldemir.medium.com/?source=post_page-----4e739d5cc6c7--------------------------------)[](https://towardsdatascience.com/?source=post_page-----4e739d5cc6c7--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----4e739d5cc6c7--------------------------------)
    [Ertuğrul Demir](https://ertugruldemir.medium.com/?source=post_page-----4e739d5cc6c7--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://ertugruldemir.medium.com/?source=post_page-----4e739d5cc6c7--------------------------------)[![Ertuğrul
    Demir](../Images/bc35d3fcfade72c8a4e72d1fd952a73c.png)](https://ertugruldemir.medium.com/?source=post_page-----4e739d5cc6c7--------------------------------)[](https://towardsdatascience.com/?source=post_page-----4e739d5cc6c7--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----4e739d5cc6c7--------------------------------)
    [Ertuğrul Demir](https://ertugruldemir.medium.com/?source=post_page-----4e739d5cc6c7--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fc2ba54d15a24&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-arrival-of-sdxl-1-0-4e739d5cc6c7&user=Ertu%C4%9Frul+Demir&userId=c2ba54d15a24&source=post_page-c2ba54d15a24----4e739d5cc6c7---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----4e739d5cc6c7--------------------------------)
    ·12 min read·Aug 2, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F4e739d5cc6c7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-arrival-of-sdxl-1-0-4e739d5cc6c7&user=Ertu%C4%9Frul+Demir&userId=c2ba54d15a24&source=-----4e739d5cc6c7---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fc2ba54d15a24&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-arrival-of-sdxl-1-0-4e739d5cc6c7&user=Ertu%C4%9Frul+Demir&userId=c2ba54d15a24&source=post_page-c2ba54d15a24----4e739d5cc6c7---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----4e739d5cc6c7--------------------------------)
    ·12分钟阅读·2023年8月2日'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4e739d5cc6c7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-arrival-of-sdxl-1-0-4e739d5cc6c7&source=-----4e739d5cc6c7---------------------bookmark_footer-----------)![](../Images/213e95221db2cd6cab8cee362df023f1.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4e739d5cc6c7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-arrival-of-sdxl-1-0-4e739d5cc6c7&source=-----4e739d5cc6c7---------------------bookmark_footer-----------)![](../Images/213e95221db2cd6cab8cee362df023f1.png)'
- en: A cute little robot learning how to paint — Created by Using SDXL 1.0
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 一个可爱的小机器人学习如何绘画——使用 SDXL 1.0 创建
- en: In the rapidly evolving world of machine learning, where new models and technologies
    flood our feeds almost daily, staying updated and making informed choices becomes
    a daunting task. Today, we direct our focus towards SDXL 1.0, a text to image
    generative model which has undoubtedly garnered considerable interest within the
    field.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在快速发展的机器学习领域，新的模型和技术几乎每天都在不断涌现，保持更新并做出明智的选择变得异常艰难。今天，我们将注意力转向 SDXL 1.0，这是一种文本到图像生成模型，无疑在该领域引起了相当大的兴趣。
- en: SDXL 1.0, short for “Stable Diffusion XL,” is touted as a latent text-to-image
    diffusion model that supposedly surpasses its predecessors with a range of promising
    enhancements. In the upcoming chapters, we will closely examine these claims and
    take a closer look at the new enhancements.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: SDXL 1.0，全称为“稳定扩散 XL”，被誉为一种潜在的文本到图像扩散模型，声称在多个方面超过了其前身。在接下来的章节中，我们将深入探讨这些说法，并仔细审视新的改进。
- en: Most notably, SDXL is an open-source model that addresses a major concern in
    the domain of generative models. While black-box models have gained recognition
    as state-of-the-art, their architecture’s opacity hinders a comprehensive assessment
    and validation of their performance, limiting broader community involvement.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 最值得注意的是，SDXL是一个开源模型，解决了生成模型领域的一个主要问题。虽然黑箱模型因其先进性而获得认可，但其架构的不透明性阻碍了对其性能的全面评估和验证，限制了更广泛的社区参与。
- en: In this article, we embark on a detailed exploration of this promising model,
    inspecting its capabilities, building blocks, and drawing insightful comparisons
    with previous stable diffusion models. My aim is to provide a clear understanding
    without delving too deeply into technical complexities, making this an engaging
    and accessible read for all. Let’s get started!
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们将详细探索这个有前景的模型，检查其能力、构建模块，并与之前的稳定扩散模型进行有益的比较。我的目标是提供清晰的理解，而不深入技术复杂性，使这篇文章对所有人都具有吸引力和可读性。让我们开始吧！
- en: 'Understanding Stable Diffusion: Unraveling the Magic of Text-to-Image Generation'
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解稳定扩散：揭示文本到图像生成的魔力
- en: If you feel confident about how Stable Diffusion works, or don’t get involved
    in technical parts feel free to skip this chapter.
  id: totrans-14
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 如果你对稳定扩散的工作原理感到自信，或者不涉及技术部分，可以跳过本章。
- en: Stable Diffusion, the groundbreaking deep learning text-to-image model, sent
    shockwaves through the AI world upon its release in 2022, harnessing the power
    of cutting-edge diffusion techniques.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 稳定扩散，这一突破性的深度学习文本到图像模型，于2022年发布时在AI界引起了震动，利用了前沿的扩散技术。
- en: This significant development represents a notable advancement in AI image generation,
    potentially expanding access to high-performance models for a broader audience.
    The intriguing capability to transform plain text descriptions into intricate
    visual outputs has captured the attention of those who have experienced it. Stable
    Diffusion exhibits proficiency in producing high-quality images while also demonstrating
    noteworthy speed and efficiency, thereby increasing the accessibility of AI-generated
    art creation.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 这一重要发展代表了AI图像生成的显著进步，有可能扩大高性能模型对更广泛受众的可及性。将普通文本描述转换为复杂视觉输出的吸引人能力，引起了体验过的人们的关注。稳定扩散展现了生成高质量图像的能力，同时也表现出显著的速度和效率，从而提高了AI生成艺术创作的可及性。
- en: Stable Diffusion’s training involved large public datasets like LAION-5B, leveraging
    a wide array of captioned images to refine its artistic abilities. However, a
    key aspect contributing to its progress lies in the active participation of the
    community, offering valuable feedback that drives the model’s ongoing development
    and enhances its capabilities over time.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 稳定扩散的训练涉及了像LAION-5B这样的大型公开数据集，利用广泛的带注释图像来提升其艺术能力。然而，推动其进展的一个关键方面在于社区的积极参与，提供的宝贵反馈推动了模型的持续发展，并随着时间的推移增强了其能力。
- en: '**How does it work?**'
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**它是如何工作的？**'
- en: Let’s start with main building blocks of a Stable Diffusion model and how one
    can train and make predictions with these models individually.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从稳定扩散模型的主要构建模块开始，了解如何分别训练和进行预测。
- en: '**U-Net and the Essence of Diffusion Process:**'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '**U-Net与扩散过程的本质：**'
- en: To generate images using computer vision models, we venture beyond the conventional
    approach of relying on labeled data, such as classification, detection, or segmentation.
    In the realm of Stable Diffusion, the goal is to enable models to learn the complex
    details of images themselves, capturing complex contexts with an innovative approach
    known as “Diffusion.”
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使用计算机视觉模型生成图像，我们超越了依赖标记数据（如分类、检测或分割）的传统方法。在稳定扩散的领域，目标是使模型能够学习图像本身的复杂细节，以一种被称为“扩散”的创新方法捕捉复杂的上下文。
- en: 'The diffusion process unfolds in two distinctive phases:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 扩散过程分为两个不同的阶段：
- en: In the first part, we take an image and introduce a controlled amount of random
    noise. This step is referred to as **forward diffusion**.
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在第一部分，我们获取一张图像并引入一定量的随机噪声。这一步被称为**正向扩散**。
- en: In the second part, we aim to denoise the image and reconstruct the original
    content. This process is known as **reverse diffusion**.
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在第二部分，我们的目标是去噪图像并重建原始内容。这个过程被称为**反向扩散**。
- en: '![](../Images/4df88aec58451e651b93b19350e58556.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4df88aec58451e651b93b19350e58556.png)'
- en: 'Noise Addition by steps. Source: [Tackling the Generative Learning Trilemma
    with Denoising Diffusion GANs](https://arxiv.org/abs/2112.07804)'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '按步骤添加噪声。来源: [通过去噪扩散GAN解决生成学习三难问题](https://arxiv.org/abs/2112.07804)'
- en: The first part, which involves adding Gaussian noise to the input image at each
    time step **t**, is relatively straightforward. However, the second stage poses
    a challenge, as directly computing the original image is not feasible. To overcome
    this obstacle, we employ a neural network, and this is where the ingenious **U-Net**
    comes into play.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 第一部分涉及在每个时间步**t**向输入图像添加高斯噪声，较为直接。然而，第二阶段则存在挑战，因为直接计算原始图像是不可行的。为了克服这一障碍，我们使用了神经网络，这就是巧妙的**U-Net**发挥作用的地方。
- en: Leveraging **U-Net**, we train our model to predict noise from a given randomly
    noised image at time step t and calculate the loss between the predicted and actual
    noise. With a sufficiently large dataset and multiple noise steps, the model gains
    the ability to make educated predictions on noise patterns. This trained U-Net
    model also proves invaluable for generating approximate reconstructions of images
    from given noise.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 利用**U-Net**，我们训练我们的模型来预测在时间步t的给定随机噪声图像中的噪声，并计算预测噪声与实际噪声之间的损失。通过使用足够大的数据集和多个噪声步骤，模型获得了对噪声模式进行有根据预测的能力。这个训练过的U-Net模型对于从给定噪声生成图像的近似重建也证明是非常宝贵的。
- en: '![](../Images/68fec94627024f8f0757b8d9a6a88a69.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/68fec94627024f8f0757b8d9a6a88a69.png)'
- en: 'Reverse Diffusion. Source: [Denoising Diffusion Probabilistic Models](https://arxiv.org/abs/2006.11239)'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '反向扩散。来源: [去噪扩散概率模型](https://arxiv.org/abs/2006.11239)'
- en: 'If you are familiar with basic probability and computer vision models, this
    process is relatively straightforward. However, there is one more issue worth
    noting. Training millions of noise-added images and rebuilding them would be extremely
    time-consuming and would deplete computing power. To address this challenge, researchers
    have revisited a well-known architecture: **Autoencoders**. As we have already
    utilized a similar approach with U-Net, incorporating transpose convolutions and
    residual blocks, some of these elements will also prove vital role in autoencoders
    too.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你对基本概率和计算机视觉模型熟悉，这个过程是相对直接的。然而，还有一个问题值得注意。训练数百万张添加了噪声的图像并重建它们将非常耗时，并消耗计算能力。为了应对这一挑战，研究人员重新审视了一种知名架构：**自编码器**。由于我们已经采用了类似的方法，如使用U-Net结合转置卷积和残差块，这些元素在自编码器中也将发挥重要作用。
- en: With autoencoders, one can “**encode**” the data into a much smaller “**latent**”
    space and “**decode**” it back into the original space. Actually that’s why the
    original Stable Diffusion paper is called **Latent Diffusion.** This allows us
    to effectively compress large images into lower dimensions.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 使用自编码器，可以将数据“**编码**”到一个更小的“**潜在**”空间中，然后将其“**解码**”回原始空间。实际上，这就是为什么原始稳定扩散论文被称为**潜在扩散**。这使我们能够有效地将大图像压缩到较低的维度。
- en: '![](../Images/3e9a2c8f30dc10b0825be665c2a3b99f.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3e9a2c8f30dc10b0825be665c2a3b99f.png)'
- en: Simple AutoEncoder representation. Image by author.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 简单的自编码器表示。图像由作者提供。
- en: The forward and reverse diffusion operations will now occur within significantly
    smaller latent spaces, resulting in reduced memory requirements and substantially
    faster processing.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 正向和反向扩散操作现在将在显著较小的潜在空间中进行，从而减少了内存需求并显著加快了处理速度。
- en: We are nearly finished with the renowned “Stable Diffusion” architecture; the
    only remaining part is the **conditioning**. Typically, this aspect is achieved
    using **Text Encoders**, though other methods using images as conditioning, such
    as **ControlNet**, exist, though it falls outside the scope of this article. **Text
    conditioning** plays a pivotal role in generating images based on text prompts,
    where the true magic of the Stable Diffusion model lies.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 我们几乎完成了著名的“稳定扩散”架构；唯一剩下的部分是**条件化**。通常，这个方面是通过**文本编码器**实现的，尽管也存在使用图像作为条件化的方法，如**ControlNet**，但这超出了本文的范围。**文本条件化**在基于文本提示生成图像中扮演着至关重要的角色，这也是稳定扩散模型的真正魔力所在。
- en: To achieve this, we can train a text embedding model like **BERT** or **CLIP**
    using images with captions and add token embedding vectors as conditioning inputs.
    Employing a **cross-attention** mechanism (queries, keys and values), we can map
    the conditional text embeddings into U-Net residual blocks. Consequently, we can
    incorporate image captions alongside the images themselves during the training
    process and effectively condition image generations based on the provided text.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，我们可以训练像 **BERT** 或 **CLIP** 这样的文本嵌入模型，使用带有标题的图像，并将标记嵌入向量作为条件输入。通过采用 **交叉注意力**
    机制（查询、键和值），我们可以将条件文本嵌入映射到 U-Net 残差块中。因此，我们可以在训练过程中将图像标题与图像本身一起纳入，并有效地基于提供的文本调整图像生成。
- en: '![](../Images/aafb3b7d7cfa3cff75c2e3e76a6c368f.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/aafb3b7d7cfa3cff75c2e3e76a6c368f.png)'
- en: 'Latent Diffusion Model — Source: [Paper](https://arxiv.org/pdf/2112.10752.pdf)'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 潜在扩散模型 — 来源：[论文](https://arxiv.org/pdf/2112.10752.pdf)
- en: Now that you know all about the building blocks of a Stable Diffusion model,
    armed with this knowledge, we can readily compare the previous Stable Diffusion
    models and make a more informed assessment of their strengths and limitations.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经了解了 Stable Diffusion 模型的构建块，凭借这些知识，我们可以更好地比较之前的 Stable Diffusion 模型，并对它们的优缺点做出更为全面的评估。
- en: What’s New in SDXL?
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: SDXL 有什么新变化？
- en: 'Now that we have grasped the fundamentals of SD models, let’s delve into the
    SDXL paper to uncover the transformative changes introduced in this novel model.
    In summary, SDXL presents the following advancements:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经掌握了 SD 模型的基础知识，让我们深入研究 SDXL 论文，揭示这一新模型引入的变革性变化。总的来说，SDXL 提出了以下改进：
- en: '**Increased Number of U-Net Parameters:** SDXL enhances its model capacity
    by incorporating a larger number of U-Net parameters, allowing for more sophisticated
    image generation.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**增加的 U-Net 参数数量：** SDXL 通过引入更多的 U-Net 参数来增强模型能力，从而实现更复杂的图像生成。'
- en: '**Heterogeneous Distribution of Transformer Blocks:** Departing from the uniform
    distribution of transformer blocks in previous models ([1,1,1,1]), SDXL adopts
    a heterogeneous distribution ([0,2,4]), introducing optimized and improved learning
    capabilities.'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**变换器块的异质分布：** 与之前模型中的均匀分布（[1,1,1,1]）不同，SDXL 采用了异质分布（[0,2,4]），引入了优化和改进的学习能力。'
- en: '**Enhanced Text Conditioning Encoder:** SDXL leverages a bigger text conditioning
    encoder, OpenCLIP ViT-bigG, to effectively incorporate textual information into
    the image generation process.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**增强的文本条件编码器：** SDXL 利用更大的文本条件编码器 OpenCLIP ViT-bigG，有效地将文本信息融入图像生成过程中。'
- en: '**Additional Text Encoder:** The model employs an additional text encoder,
    CLIP ViT-L, which concatenates its output, enriching the conditioning process
    with complementary textual features.'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**附加文本编码器：** 该模型采用了附加文本编码器 CLIP ViT-L，连接其输出，丰富了条件处理过程中的文本特征。'
- en: '**Introducing “Size-Conditioning”:** A novel conditioner called “Size-Conditioning”
    takes the original training image’s width and height as conditional input, enabling
    the model to adapt its image generation based on size-related cues.'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**引入“尺寸条件”：** 一种名为“尺寸条件”的新型调节器将原始训练图像的宽度和高度作为条件输入，使模型能够根据与尺寸相关的提示调整图像生成。'
- en: '**“Crop-Conditioning” Parameter:** SDXL introduces the “Crop-Conditioning”
    parameter, incorporating image cropping coordinates as conditional input.'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**“裁剪条件”参数：** SDXL 引入了“裁剪条件”参数，将图像裁剪坐标作为条件输入。'
- en: '**“Multi-Aspect Conditioning” Parameter:** By incorporating the bucket size
    for conditioning, the “Multi-Aspect Conditioning” parameter enables SDXL to cater
    to various aspect ratios.'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**“多方面条件”参数：** 通过纳入条件的桶大小，“多方面条件”参数使 SDXL 能够适应不同的纵横比。'
- en: '**Specialized Refiner Model:** SDXL introduces a second SD model specialized
    in handling high-quality, high-resolution data; essentially, it is an img2img
    model that effectively captures intricate local details.'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**专用精炼器模型：** SDXL 引入了第二个专门处理高质量、高分辨率数据的 SD 模型；本质上，它是一个 img2img 模型，能够有效捕捉复杂的局部细节。'
- en: Now, let’s take a closer look at how some of these additions compare to previous
    stable diffusion models.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们更详细地查看这些新增功能与之前稳定扩散模型的比较。
- en: '**Stability.ai’s Official Comparison:**'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '**Stability.ai 官方比较：**'
- en: Let’s begin by examining stability.ai’s official comparison, as presented by
    the authors. This comparison offers valuable insights into user preferences between
    SDXL and Stable Diffusion. However, we must approach the findings with a cautious
    eye…
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先查看稳定性.ai的官方比较，由作者呈现。这一比较提供了有关SDXL与Stable Diffusion之间用户偏好的有价值的见解。然而，我们必须谨慎对待这些发现……
- en: '![](../Images/7fb2471545e027cb6e0423d8ad515bc8.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7fb2471545e027cb6e0423d8ad515bc8.png)'
- en: 'Comparing user preferences between SDXL and previous models. Source: [Paper](https://arxiv.org/abs/2307.01952)'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 比较SDXL与之前模型的用户偏好。来源：[论文](https://arxiv.org/abs/2307.01952)
- en: This study demonstrates that participants chose SDXL models over the previous
    SD 1.5 and 2.1 models. In particular, the SDXL model with the Refiner addition
    achieved a win rate of 48.44%. It is important to note that while this result
    is statistically significant, we must also take into account the inherent biases
    introduced by the human element and the inherent randomness of generative models.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 这项研究表明，参与者选择了SDXL模型而非之前的SD 1.5和2.1模型。特别是，添加了Refiner的SDXL模型实现了48.44%的胜率。需要注意的是，尽管这一结果在统计上具有显著性，但我们还必须考虑由人为因素引入的固有偏差以及生成模型的随机性。
- en: '**Performance Against State-of-the-Art Black-Box Models:**'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '**与最先进黑箱模型的性能比较：**'
- en: '![](../Images/82839287979c47c2c1dce5bfa0e41e9a.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/82839287979c47c2c1dce5bfa0e41e9a.png)'
- en: 'Source: [Paper](https://arxiv.org/abs/2307.01952)'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 来源：[论文](https://arxiv.org/abs/2307.01952)
- en: Currently, Midjourney is highly popular among users, with some considering it
    to currently be a state-of-the-art solution. According to the official survey,
    SDXL exhibits a higher preference rate in categories such as “Food & Beverage”
    and “Animals.” However, in other categories like “Illustrations” and “Abstract,”
    users still prefer Midjourney V5.1.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，Midjourney在用户中非常受欢迎，有些人认为它目前是最先进的解决方案。根据官方调查，SDXL在“食品和饮料”和“动物”等类别中表现出较高的偏好率。然而，在“插图”和“抽象”等其他类别中，用户仍然偏好Midjourney
    V5.1。
- en: '![](../Images/e728916713a1086a5405b53311283ef8.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e728916713a1086a5405b53311283ef8.png)'
- en: 'Source: SDXL: [Improving Latent Diffusion Models for High-Resolution Image
    Synthesis](https://arxiv.org/abs/2307.01952)'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '来源：SDXL: [提升潜在扩散模型以实现高分辨率图像合成](https://arxiv.org/abs/2307.01952)'
- en: Once again, we observe a similar pattern in the case of complex prompts. The
    paper asserts that they are preferred in 7 out of 10 complex subjects. However,
    without knowledge of the specific prompts used, it becomes challenging to draw
    a conclusion. Additionally, the lack of information regarding the prompt encoder
    in Midjourney further complicates matters, and only time will reveal the true
    preferences.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 再次观察复杂提示的情况，我们看到类似的模式。论文声明它们在10个复杂主题中的7个中受到青睐。然而，如果不了解具体的提示内容，就很难得出结论。此外，关于Midjourney中提示编码器的信息缺乏进一步复杂化了问题，只有时间才能揭示真正的偏好。
- en: '**Number of U-Net Parameters**'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '**U-Net参数数量**'
- en: As mentioned earlier, U-Net models play a vital role in Stable Diffusion, facilitating
    the reconstruction of images from given noise. In SDXL, the authors have made
    a noteworthy improvement by incorporating a considerably larger U-Net model compared
    to previous versions of SD in total of 2.6B U-Net parameters compared to ~860M
    for its predecessors.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，U-Net模型在Stable Diffusion中发挥着至关重要的作用，帮助从给定的噪声中重建图像。在SDXL中，作者通过引入一个比之前版本SD大得多的U-Net模型做出了显著改进，总计2.6B的U-Net参数，相较于其前身的~860M。
- en: '![](../Images/873ccf66cd22308b7aecfe4f177c87fc.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/873ccf66cd22308b7aecfe4f177c87fc.png)'
- en: Image by Author.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供。
- en: Although having more parameters may seem promising initially, it is essential
    to consider the tradeoff between complexity and quality. As the number of parameters
    increases, so do the system requirements for both training and generating. While
    it is still premature to arrive at definitive conclusions regarding the product’s
    quality, the tradeoff between complexity and quality is already evident.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管拥有更多参数可能初看起来很有前景，但必须考虑复杂性与质量之间的权衡。随着参数数量的增加，系统在训练和生成时的需求也会增加。虽然对于产品质量的最终结论还为时尚早，但复杂性与质量之间的权衡已经显而易见。
- en: '**Number of Text Encoder Parameters**'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '**文本编码器参数数量**'
- en: Indeed, when examining the total number of text encoder parameter numbers, we
    observe a notable increase in SDXL 1.0 compared to its predecessors. The introduction
    of two text conditioners in SDXL, as opposed to a single one in previous versions,
    accounts for this significant growth in the text encoder’s parameter count. This
    expansion empowers SDXL to leverage a larger volume of textual information.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 确实，当审视文本编码器的总参数数量时，我们观察到SDXL 1.0相比于其前身有显著增加。SDXL中引入了两个文本条件器，而之前版本中只有一个，这使得文本编码器的参数数量大幅增长。这一扩展使得SDXL能够利用更大容量的文本信息。
- en: SDXL utilizes the OpenCLIP G/14 text encoder with 694.7 million parameters,
    compared to CLIP L/14 with 123.65 million parameters, resulting in a total of
    over 800 million parameters. This represents a substantial leap from its predecessors.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: SDXL使用具有6.947亿参数的OpenCLIP G/14文本编码器，相比之下CLIP L/14具有1.2365亿参数，总计超过8亿参数。这代表了与其前身相比的重大飞跃。
- en: '![](../Images/046b34d74d2f5620fcc5117322be9f75.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/046b34d74d2f5620fcc5117322be9f75.png)'
- en: Image by Author.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供。
- en: Once more, employing larger and multiple text encoders may appear appealing
    initially, but it introduces added complexity that could prove detrimental. Consider
    a scenario where you are fine-tuning a SDXL model with your own data. In such
    cases, determining optimal parameters becomes more challenging than previous SD
    models because finding the “Sweet Spot” of hyperparameters for both encoders becomes
    more elusive.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，使用更大和多个文本编码器最初可能看起来很有吸引力，但它引入的额外复杂性可能会产生不利影响。考虑到一个场景，其中你正在用自己的数据对SDXL模型进行微调。在这种情况下，确定最佳参数比以前的SD模型更具挑战性，因为找到两个编码器的超参数“甜蜜点”变得更加难以捉摸。
- en: '**The Workflow**'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '**工作流程**'
- en: 'Indeed, the term “XL” in SDXL is indicative of its expanded scale and increased
    complexity when compared to previous SD models. SDXL surpasses its predecessors
    in various aspects, boasting a larger number of parameters, including two text
    encoders and two U-Net models — the base model and the refiner, which essentially
    functions as an image-to-image model. Naturally this heightened complexity of
    SDXL pipelines:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 确实，SDXL中的“XL”一词表明了其相比于之前的SD模型在规模和复杂性上的扩展。SDXL在多个方面超越了其前身，拥有更多的参数，包括两个文本编码器和两个U-Net模型——基础模型和细化模型，后者本质上作为图像到图像模型使用。自然，这种SDXL管道的复杂性增加：
- en: '![](../Images/5753c3a3f33cd87e5a94f1fe4705ba86.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5753c3a3f33cd87e5a94f1fe4705ba86.png)'
- en: Usual SD 1.5 generation pipeline. Image by Author
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 通常的SD 1.5生成管道。图片由作者提供
- en: '![](../Images/4548ea3d34848d640c7881576aa6d8b8.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4548ea3d34848d640c7881576aa6d8b8.png)'
- en: SDXL generation pipeline. Image by Author
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: SDXL生成管道。图片由作者提供
- en: '**SDXL in Practice**'
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**SDXL实践**'
- en: The model weights of SDXL have been officially released and are freely accessible
    for use as Python scripts, thanks to the diffusers library from Hugging Face.
    Additionally, there is a user-friendly GUI option available known as ComfyUI.
    This GUI provides a highly customizable, node-based interface, allowing users
    to intuitively place building blocks of the Stable Diffusion model and visually
    connect them.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: SDXL的模型权重已经正式发布，并可以作为Python脚本自由使用，感谢Hugging Face的diffusers库。此外，还有一个用户友好的GUI选项，称为ComfyUI。这个GUI提供了一个高度可定制的基于节点的界面，允许用户直观地放置Stable
    Diffusion模型的构建块并进行可视化连接。
- en: With this readily available implementations, users can seamlessly integrate
    SDXL into their projects, enabling them to harness the power of this cutting-edge
    latent text-to-image diffusion model.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这些现成的实现，用户可以将SDXL无缝集成到他们的项目中，从而能够利用这一前沿的潜在文本到图像扩散模型的强大功能。
- en: Basic Example of diffusers Usage
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: Diffusers使用的基本示例
- en: '![](../Images/8ea16dcf10d62e35673e580dcb27c5fa.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8ea16dcf10d62e35673e580dcb27c5fa.png)'
- en: An Example of ComfyUI workflow pipeline. Image by author.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: ComfyUI工作流程管道示例。图片由作者提供。
- en: '**Current State of SDXL and Personal Experiences**'
  id: totrans-87
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**SDXL的当前状态和个人体验**'
- en: While the new features and additions in SDXL appear promising, some fine-tuned
    SD 1.5 models are still delivering better results. This outcome is primarily attributed
    to the great support from the thriving community — an advantage that stems from
    the open-source approach.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管SDXL中的新功能和附加功能看起来很有前景，但一些经过微调的SD 1.5模型仍然提供更好的结果。这一结果主要归因于蓬勃发展的社区的巨大支持——这是开放源代码方法带来的优势。
- en: At its initial model stage, SDXL exhibits improvements over 1.5, and I am confident
    that with continued community support, its performance will only grow stronger
    in the future. However, it is essential to acknowledge that as models become more
    complex, using and fine-tuning them computationally demands greater resources.
    But there’s no need for concern yet…
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在其初始模型阶段，SDXL 相比于 1.5 有了改进，我相信在持续的社区支持下，其性能未来只会更强。然而，需要认识到，随着模型的复杂性增加，使用和微调它们在计算上需要更多资源。但目前还不需要过于担忧……
- en: '**LoRA**s (Locally Rank-Adaptive Decompositions) has gained popularity for
    **fine-tuning Large Language Models**. This approach involves adding pairs of
    rank-decomposition weight matrices to existing weights and training only these
    newly added weights. As a result, training becomes faster and computationally
    more efficient. The incorporation of LoRAs is expected to pave the way for the
    community to create even better custom versions in the near future. Notably, SDXL
    already fully supports LoRAs.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '**LoRA**（局部秩自适应分解）在**大型语言模型的微调**中越来越受欢迎。这种方法涉及将秩分解权重矩阵对添加到现有权重中，并仅训练这些新添加的权重。因此，训练变得更快且计算上更高效。LoRA
    的引入预计将为社区创造出更好的定制版本铺平道路。值得注意的是，SDXL 已经完全支持 LoRA。'
- en: 'Despite the positive developments, it’s worth noting that SDXL still grapples
    with some of the usual Stable Diffusion shortcomings, as officially acknowledged
    by the authors:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管有积极的发展，但值得注意的是，SDXL 仍然面临一些常见的 Stable Diffusion 缺陷，正如作者们正式承认的那样：
- en: The model does not achieve perfect photorealism
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 该模型未能达到完美的照片现实主义。
- en: The model cannot render legible text
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 该模型无法渲染清晰的文本。
- en: The model struggles with more difficult tasks which involve compositionality,
    such as rendering an image corresponding to “A red cube on top of a blue sphere”
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 该模型在处理涉及组合性的更复杂任务时表现不佳，例如渲染对应于“一个红色立方体在一个蓝色球体上方”的图像。
- en: Faces and people in general may not be generated properly.
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 脸部和一般人物可能无法正确生成。
- en: The autoencoding part of the model is lossy.
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型的自动编码部分是有损的。
- en: '**Personal Observations**'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '**个人观察**'
- en: Personally, while experimenting with the SDXL model, I still find myself favoring
    the previous SD 1.5 community checkpoints in certain cases. With months of community
    support, it is relatively easy to find the right fine-tuned model that suits specific
    needs, such as photorealism or more cartoonish styles. However, it is currently
    challenging to find specific fine-tuned models for SDXL due to the high computing
    power requirements. Nevertheless, the base model of SDXL appears to perform better
    than the base models of SD 1.5 or 2.1 in terms of image quality and resolution,
    and with further optimizations and time, this might change in the near future.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 就个人而言，在实验 SDXL 模型时，我仍然发现自己在某些情况下更倾向于使用之前的 SD 1.5 社区检查点。在社区支持的几个月后，找到适合特定需求的微调模型（如照片现实主义或更卡通风格）相对容易。然而，由于对计算能力的高需求，目前难以找到针对
    SDXL 的特定微调模型。尽管如此，SDXL 的基础模型在图像质量和分辨率方面似乎优于 SD 1.5 或 2.1 的基础模型，随着进一步的优化和时间，这种情况在不久的将来可能会发生变化。
- en: It’s worth noting that with the increased model size, some users have reported
    difficulties running the model on their everyday laptops or PCs, which is unfortunate.
    I am hopeful that the quantization techniques commonly used in Large Language
    Models may find their place in this field too.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，随着模型规模的增大，一些用户报告在日常笔记本电脑或 PC 上运行模型时遇到困难，这令人遗憾。我希望大型语言模型中常用的量化技术也能在这一领域找到应用。
- en: Additionally, with the change in text encoders, my usual prompts no longer yield
    satisfactory results on SDXL. Although the developers of SDXL claim that prompting
    is easier with SDXL, I have yet to find the right approach myself. It may take
    some time to adjust to the new prompt style, especially for those coming from
    previous versions.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，由于文本编码器的变化，我的常用提示在 SDXL 上不再产生令人满意的结果。尽管 SDXL 的开发者声称在 SDXL 上提示更容易，但我自己尚未找到合适的方法。适应新的提示风格可能需要一些时间，尤其是对于那些来自早期版本的人。
- en: '**Conclusion**'
  id: totrans-101
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**结论**'
- en: In our article, we discovered the capabilities of Stable Diffusion XL, a model
    with the ability to transform plain text descriptions into intricate visual representations.
    We found that SDXL’s open-source nature and its approach to addressing concerns
    related to black-box models have contributed to its widespread appeal, allowing
    it to reach a broader audience.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '-   在我们的文章中，我们发现了 Stable Diffusion XL 的能力，这一模型能够将普通文本描述转化为复杂的视觉表现。我们发现 SDXL
    的开源特性及其解决黑箱模型相关问题的方法为其广泛吸引力做出了贡献，使其能够接触到更广泛的受众。'
- en: With its increased number of parameters and extra features SDXL has proven itself
    as an “XL” model, boasting heightened complexity compared to its predecessors.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '-   SDXL 通过增加参数数量和额外功能，证明了自己是一个“XL”模型，与前辈相比，具备了更高的复杂性。'
- en: Implementing SDXL in practice has been made easier with the official release
    of its model weights, freely available as Python scripts from huggingface, along
    with the user-friendly ComfyUI GUI option.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '-   实践中实施 SDXL 已变得更加容易，官方发布的模型权重可以作为 Python 脚本从 huggingface 自由获取，并提供了用户友好的
    ComfyUI GUI 选项。'
- en: While SDXL shows great promise, the journey towards perfection is still ongoing.
    Some finely-tuned SD 1.5 models continue to outperform SDXL in certain scenarios,
    thanks to the vibrant community support. However, with active engagement and support,
    I can see that SDXL will continue to evolve and improve over time.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '-   尽管 SDXL 展示了巨大的潜力，但通向完美的道路仍在继续。一些精细调整过的 SD 1.5 模型在某些情况下仍优于 SDXL，这得益于活跃的社区支持。然而，凭借积极的参与和支持，我相信
    SDXL 将继续随着时间的推移不断演进和改进。'
- en: Nonetheless, it is important to acknowledge that like its predecessors, SDXL
    does have some limitations. Achieving perfect photorealism, rendering legible
    text, handling compositionality challenges, and accurately generating faces and
    people are among the areas that the model is needs to be improved upon.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '-   然而，需要承认的是，像其前辈一样，SDXL 确实存在一些局限性。实现完美的照片现实主义、渲染清晰的文本、处理组合挑战以及准确生成面孔和人物是模型需要改进的领域。'
- en: In conclusion, SDXL 1.0 represents a significant leap forward in text-to-image
    generation, unleashing the creative potential of AI and pushing the boundaries
    of what’s possible. As the AI community continues to collaborate and innovate,
    we can look forward to witnessing even more astonishing developments in the fascinating
    world of SD models and beyond.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '-   总结来说，SDXL 1.0 代表了文本到图像生成的重大飞跃，释放了 AI 的创作潜力，并推动了可能性的边界。随着 AI 社区的持续合作和创新，我们可以期待在迷人的
    SD 模型及其未来的发展中见证更多令人惊叹的进展。'
- en: 'References:'
  id: totrans-108
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献：
- en: '[Denoising Diffusion Probabilistic Models — Jonathan Ho, Ajay Jain, Pieter
    Abbeel, 2020](https://arxiv.org/abs/2006.11239)'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[去噪扩散概率模型 — Jonathan Ho, Ajay Jain, Pieter Abbeel, 2020](https://arxiv.org/abs/2006.11239)'
- en: '[High-Resolution Image Synthesis with Latent Diffusion Models — Robin Rombach,
    Andreas Blattmann, Dominik Lorenz, Patrick Esser, Björn Ommer, 2021](https://arxiv.org/abs/2112.10752)'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[高分辨率图像合成与潜在扩散模型 — Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick
    Esser, Björn Ommer, 2021](https://arxiv.org/abs/2112.10752)'
- en: '[SDXL: Improving Latent Diffusion Models for High-Resolution Image Synthesis
    — Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas
    Müller, Joe Penna, Robin Rombach, 2023](https://arxiv.org/abs/2307.01952)'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[SDXL: 改进潜在扩散模型以实现高分辨率图像合成 — Dustin Podell, Zion English, Kyle Lacey, Andreas
    Blattmann, Tim Dockhorn, Jonas Müller, Joe Penna, Robin Rombach, 2023](https://arxiv.org/abs/2307.01952)'
