- en: 'BERxiT: Early Exiting for BERT'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 'BERxiT: 适用于BERT的早期退出'
- en: 原文：[https://towardsdatascience.com/berxit-early-exiting-for-bert-6f76b2f561c5?source=collection_archive---------3-----------------------#2023-01-14](https://towardsdatascience.com/berxit-early-exiting-for-bert-6f76b2f561c5?source=collection_archive---------3-----------------------#2023-01-14)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/berxit-early-exiting-for-bert-6f76b2f561c5?source=collection_archive---------3-----------------------#2023-01-14](https://towardsdatascience.com/berxit-early-exiting-for-bert-6f76b2f561c5?source=collection_archive---------3-----------------------#2023-01-14)
- en: '*Presenting the “early exiting” method for efficient inference in deep neural
    networks, and reviewing the “BERxiT” paper*'
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '*介绍用于深度神经网络高效推理的“早期退出”方法，并回顾“BERxiT”论文*'
- en: '[](https://medium.com/@odedmous?source=post_page-----6f76b2f561c5--------------------------------)[![Oded
    Mousai](../Images/61fc83236a1c9e1e5f42bf7b2deda1b1.png)](https://medium.com/@odedmous?source=post_page-----6f76b2f561c5--------------------------------)[](https://towardsdatascience.com/?source=post_page-----6f76b2f561c5--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----6f76b2f561c5--------------------------------)
    [Oded Mousai](https://medium.com/@odedmous?source=post_page-----6f76b2f561c5--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@odedmous?source=post_page-----6f76b2f561c5--------------------------------)[![Oded
    Mousai](../Images/61fc83236a1c9e1e5f42bf7b2deda1b1.png)](https://medium.com/@odedmous?source=post_page-----6f76b2f561c5--------------------------------)[](https://towardsdatascience.com/?source=post_page-----6f76b2f561c5--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----6f76b2f561c5--------------------------------)
    [Oded Mousai](https://medium.com/@odedmous?source=post_page-----6f76b2f561c5--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fefb071325af8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fberxit-early-exiting-for-bert-6f76b2f561c5&user=Oded+Mousai&userId=efb071325af8&source=post_page-efb071325af8----6f76b2f561c5---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----6f76b2f561c5--------------------------------)
    ·11 min read·Jan 14, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F6f76b2f561c5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fberxit-early-exiting-for-bert-6f76b2f561c5&user=Oded+Mousai&userId=efb071325af8&source=-----6f76b2f561c5---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fefb071325af8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fberxit-early-exiting-for-bert-6f76b2f561c5&user=Oded+Mousai&userId=efb071325af8&source=post_page-efb071325af8----6f76b2f561c5---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----6f76b2f561c5--------------------------------)
    ·11 分钟阅读·2023年1月14日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F6f76b2f561c5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fberxit-early-exiting-for-bert-6f76b2f561c5&user=Oded+Mousai&userId=efb071325af8&source=-----6f76b2f561c5---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F6f76b2f561c5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fberxit-early-exiting-for-bert-6f76b2f561c5&source=-----6f76b2f561c5---------------------bookmark_footer-----------)![](../Images/45682104a04c165f2cafa599859fb1df.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F6f76b2f561c5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fberxit-early-exiting-for-bert-6f76b2f561c5&source=-----6f76b2f561c5---------------------bookmark_footer-----------)![](../Images/45682104a04c165f2cafa599859fb1df.png)'
- en: Image by the author (created with Midjourney)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片（由Midjourney创建）
- en: 'This article contains two parts. In **Part I** I present the motivation for
    efficient inference time and introduce the idea of “early exiting” to achieve
    that. In **Part II** I review the interesting paper “**BERxiT: Early Exiting for
    BERT with Better Fine-Tuning and Extension to Regression**” (Xin, Ji et al.) [[1](https://aclanthology.org/2021.eacl-main.8/)]that
    was published in 2021 and aims to improve the early exiting method. Note that
    this paper focuses on the NLP domain (using a BERT model), but the idea can be
    easily applied to other domains.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '本文包含两个部分。在**第一部分**，我介绍了提高推理时间效率的动机，并引入了实现这一目标的“早期退出”概念。在**第二部分**，我回顾了有趣的论文“**BERxiT:
    通过更好的微调和回归扩展对BERT的早期退出**”（Xin, Ji等）[[1](https://aclanthology.org/2021.eacl-main.8/)]，该论文于2021年发表，旨在改进早期退出方法。请注意，这篇论文集中于NLP领域（使用BERT模型），但这一思想可以很容易地应用于其他领域。'
- en: '**Part I: Introduction**'
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**第一部分：介绍**'
- en: '**The importance of efficiency in inference time**'
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**推理时间效率的重要性**'
- en: Deep neural networks (DNNs) have grown significantly in size over the past few
    years, leading to longer training and inference times for these models. While
    training cost may seem higher at first, in many cases it is actually eclipsed
    by the cost of inference, as these models are usually trained only once but applied
    multiple millions of times.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 深度神经网络（DNN）在过去几年中规模显著增长，导致这些模型的训练和推理时间更长。虽然训练成本最初可能显得较高，但在许多情况下，推理成本实际上更高，因为这些模型通常只训练一次，但会被应用数百万次。
- en: 'Efficient inference is also important for the following reasons:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 高效的推理也很重要，原因如下：
- en: 'Resource constraints: In some cases, the devices on which DNNs are deployed
    may have limited resources, such as mobile devices. In these situations, fast
    inference times are necessary in order to ensure that the DNN can run efficiently
    and effectively.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 资源限制：在某些情况下，DNN 部署的设备可能资源有限，比如移动设备。在这些情况下，需要快速的推理时间以确保 DNN 能够高效且有效地运行。
- en: 'User experience: In many applications, DNNs are used to provide real-time responses
    to user requests. For example, in a speech recognition system, the DNN must process
    and classify the user’s speech in real time in order to provide an accurate transcription.
    If the inference time is too slow, the user experience will be poor.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 用户体验：在许多应用中，DNN 用于提供对用户请求的实时响应。例如，在语音识别系统中，DNN 必须实时处理和分类用户的语音，以提供准确的转录。如果推理时间过慢，用户体验将会很差。
- en: 'Cost: In some cases, the cost of running a DNN may be based on the amount of
    time it takes to perform inference. For example, in cloud computing environments
    users may be charged based on the amount of time their DNNs run.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 成本：在某些情况下，运行 DNN 的成本可能取决于推理所需的时间。例如，在云计算环境中，用户可能会根据 DNN 运行的时间来收费。
- en: 'Sustainability: There are many discussions about the energy consumption of
    DNNs and their potential impact on the environment (see [[2](https://arxiv.org/abs/1906.02243)],
    [[3](https://deepai.org/publication/compute-and-energy-consumption-trends-in-deep-learning-inference)]
    and [[4](https://arxiv.org/abs/1907.10597)] for example), and it appears that
    fast inference times tend to be more energy-efficient.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 可持续性：关于 DNN 的能源消耗及其对环境潜在影响的讨论很多（例如，参见 [[2](https://arxiv.org/abs/1906.02243)]、[[3](https://deepai.org/publication/compute-and-energy-consumption-trends-in-deep-learning-inference)]
    和 [[4](https://arxiv.org/abs/1907.10597)]），而且快速的推理时间似乎更具能源效率。
- en: '**Early-Exiting method**'
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**早期退出方法**'
- en: There are different methods to improve efficiency in inference time [[5](https://arxiv.org/abs/2209.00099)].
    The obvious direction is to reduce the model size, for example by pruning or knowledge
    distillation methods. However, since accuracy is generally gained by the complexity
    of the model, this has the potential to hurt the model's performance and typically
    requires another step besides the regular training phase.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 有不同的方法可以提高推理时间的效率 [[5](https://arxiv.org/abs/2209.00099)]。显而易见的方向是减少模型的大小，例如通过剪枝或知识蒸馏方法。然而，由于模型的复杂性通常会提高准确性，这可能会影响模型的性能，并且通常需要在常规训练阶段之外的额外步骤。
- en: 'Another approach is the “early exiting” method, which was also explored by
    RTJ3 [[6](https://arxiv.org/abs/2004.07453)], DeeBERT [[7](https://arxiv.org/abs/2004.12993)],
    and FastBERT [[8](https://arxiv.org/abs/2004.02178)]. The idea of early exiting
    derives from the observation that samples are not equally difficult [[6](https://arxiv.org/abs/2004.07453)].
    Longer sentences with complex structures would probably require more time and
    effort to analyze. Consider the following sentences for the task of sentiment
    analysis:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种方法是“早期退出”方法，这也被 RTJ3 [[6](https://arxiv.org/abs/2004.07453)]、DeeBERT [[7](https://arxiv.org/abs/2004.12993)]
    和 FastBERT [[8](https://arxiv.org/abs/2004.02178)] 探索过。早期退出的想法源于观察到样本的难度不一 [[6](https://arxiv.org/abs/2004.07453)]。较长且结构复杂的句子可能需要更多的时间和精力进行分析。考虑以下句子用于情感分析任务：
- en: (1) The restaurant was great.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: (1) 餐厅很棒。
- en: (2) I’m not sure if the chef is actually talented or if the food was just microwaved
    frozen meals.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: (2) 我不确定厨师是否真的有才华，还是食物只是微波加热的冷冻餐。
- en: Sentence 1 is easy to analyze because it is short and contains direct positive
    language, indicating a positive sentiment. Sentence 2 is more difficult to analyze
    because it contains both positive and negative terms, while the overall sentiment
    is negative. Additionally, the reviewer uses a sarcastic tone to express his doubt
    about the chef’s talent, which is hard to detect.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 句子1易于分析，因为它短且包含直接的积极语言，表示积极情感。句子2更难分析，因为它包含积极和消极的词语，而总体情感是消极的。此外，评论员使用讽刺的语气表达对厨师才华的怀疑，这很难被检测到。
- en: 'The above observation led to the following idea: Create multiple decision points
    at different depths within the network, and during inference let each sample exit
    at the earliest point in which the network is confident about its prediction on
    this sample. Hence, the inference of “easy” samples would probably terminate early,
    and only the “hardest” samples would need to pass through all layers. This way,
    the network can avoid performing unnecessary computations, which saves time and
    resources.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 上述观察导致了以下想法：在网络中创建多个决策点，在推理时，让每个样本在网络对其预测有信心的最早点退出。因此，“简单”的样本可能会早早终止，只有“最难”的样本需要通过所有层。这样，网络可以避免执行不必要的计算，从而节省时间和资源。
- en: In a BERT model, this idea is implemented practically by attaching a small classifier
    to the output of each Transformer layer (besides the last layer which already
    has a classifier). I call these classifiers “early exiting components”. Each classifier
    output is a vector of probabilities; the maximum probability in such a vector
    is called the “confidence score”. At the inference time of a sample, the confidence
    score at each layer is compared with a predefined threshold; if it is larger than
    the threshold at a certain layer, the sample exits with the current prediction,
    and future layers are skipped. The figure below illustrates this idea.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在BERT模型中，这一思想通过在每个Transformer层的输出处附加一个小型分类器（除了已经有分类器的最后一层）来实际实现。我称这些分类器为“早期退出组件”。每个分类器的输出是一个概率向量；在这样的向量中，最大概率称为“置信度分数”。在样本的推理时，将每层的置信度分数与预定义阈值进行比较；如果某层的置信度分数大于阈值，则样本以当前预测退出，跳过未来的层。下图展示了这一思想。
- en: '![](../Images/cd66af693cee592936671af45031db0d.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cd66af693cee592936671af45031db0d.png)'
- en: '***Left image:*** *Here the confidence score (0.95) in the second layer is
    larger than the predefined threshold (0.9) and hence the sample is exited from
    the model with a prediction of the label “Positive”.* ***Right image:*** *Here
    the confidence score is smaller than the predefined threshold (0.9) in all layers,
    so no early exiting is performed, and the prediction is the output of the final
    classifier. Source:* [*Link*](https://www.virtual2021.eacl.org/paper_main.44.html)'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '***左图：*** *这里第二层的置信度分数（0.95）大于预定义阈值（0.9），因此样本以“Positive”标签的预测从模型中退出。* ***右图：***
    *这里所有层的置信度分数都小于预定义阈值（0.9），因此没有进行早期退出，预测结果是最终分类器的输出。来源：* [*链接*](https://www.virtual2021.eacl.org/paper_main.44.html)'
- en: 'Part II: BERexiT'
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第二部分：BERexiT
- en: 'The BERxiT(BERT+exit) paper aims to address two weaknesses of previous work:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: BERxiT（BERT+exit）论文旨在解决之前工作的两个弱点：
- en: '**Fine-tuning strategy —** Previous fine-tuning strategies are not ideal for
    models with early exiting components.'
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**微调策略 —** 之前的微调策略对于具有早期退出组件的模型并不理想。'
- en: '**Regression tasks —** Previous works make early exiting decisions based on
    the confidence of the predicted probability distribution and are therefore limited
    to classification tasks.'
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**回归任务 —** 之前的工作基于预测概率分布的置信度做出早期退出决策，因此仅限于分类任务。'
- en: '![](../Images/0ed032f5206cbc22632785dd005af07b.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0ed032f5206cbc22632785dd005af07b.png)'
- en: 'BERxiT architecture. Source: [Link](https://aclanthology.org/2021.eacl-main.8/)'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: BERxiT架构。来源：[链接](https://aclanthology.org/2021.eacl-main.8/)
- en: '**1\. Fine-tuning strategy**'
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**1\. 微调策略**'
- en: 'In a “regular” neural network architecture, there is a single loss function
    that is being optimized. In our case, an Early Existing component is added to
    each Transformer layer and hence there are multiple loss terms. This imposes a
    challenge to the learning process since the Transformer layers have to provide
    hidden states for two competing purposes: immediate inference at the adjacent
    classifier and gradual feature extraction for future classifiers. Therefore, achieving
    a balance between the classifiers is critical, and that’s the goal of the fine-tuning
    strategy.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在“常规”神经网络架构中，优化的是单一损失函数。在我们的案例中，为每个Transformer层添加了一个早期退出组件，因此存在多个损失项。这对学习过程提出了挑战，因为Transformer层必须提供隐藏状态以满足两个竞争的目的：一个是相邻分类器的即时推断，另一个是未来分类器的渐进特征提取。因此，在分类器之间取得平衡至关重要，这也是微调策略的最终目标。
- en: '**Parameters**'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '**参数**'
- en: Before I present the different strategies, let’s understand which parameters
    need to be optimized. The first set of parameters is those of the backbone model
    Transformer layers, which are noted with θ₁, …, θₙ. Their job is to learn good
    features for the task. The second set of parameters is the N classifiers parameters.
    The parameters of the i-th classifier are noted by wᵢ. So w₁, …,wₙ are the parameters
    of the first n-1 classifiers (early exiting components), and wₙ are the parameters
    of the last classifier. Their job is to map the hidden states to a probability
    distribution over a set of classes.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在我介绍不同策略之前，让我们了解需要优化哪些参数。第一组参数是主干模型Transformer层的参数，用θ₁, …, θₙ表示。它们的工作是为任务学习良好的特征。第二组参数是N个分类器的参数。第i个分类器的参数用wᵢ表示。因此，w₁,
    …, wₙ是前n-1个分类器（早期退出组件）的参数，而wₙ是最后一个分类器的参数。它们的工作是将隐藏状态映射到一组类别的概率分布。
- en: 'Now let’s examine 3 fine-tuning strategies:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '现在让我们深入了解三种微调策略:'
- en: Joint
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 联合
- en: Two-Stage
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 两阶段
- en: Alternating
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 交替
- en: '**Joint**'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '**联合**'
- en: In this simple strategy, the loss function is defined to be the sum of all N
    classifiers' loss functions, and the backbone model and all the classifiers are
    trained jointly.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种简单策略中，损失函数定义为所有N个分类器的损失函数之和，主干模型和所有分类器共同训练。
- en: '![](../Images/9adfc0e21e3c97a33427d01c54536530.png)![](../Images/46da8d11f3202ffd6e36f93f6c59a33d.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9adfc0e21e3c97a33427d01c54536530.png)![](../Images/46da8d11f3202ffd6e36f93f6c59a33d.png)'
- en: 'Source: [Link](https://aclanthology.org/2021.eacl-main.8/)'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '来源: [链接](https://aclanthology.org/2021.eacl-main.8/)'
- en: 'Disadvantage: Joint treats all classifiers equally, hence doesn’t preserve
    the performance of the (original) final classifier. This is not optimal because
    the final classifier must provide highly accurate outputs; There is no other classifier
    after it to handle examples that were not early exited.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '缺点: 联合方法对所有分类器一视同仁，因此无法保持（原始）最终分类器的性能。这并不理想，因为最终分类器必须提供高度准确的输出；在它之后没有其他分类器来处理那些未被早期退出的样本。'
- en: '**Two-Stage**'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '**两阶段**'
- en: 'In this strategy the training phase is divided into two separate consecutive
    stages: In the first stage, only the final classifier is trained as well as the
    backbone model. In the second stage, only the first N-1 classifiers are trained
    (while the final classifier and the backbone model are frozen).'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '在这种策略中，训练阶段分为两个连续的独立阶段: 在第一阶段，仅训练最终分类器以及主干模型。在第二阶段，仅训练前N-1个分类器（而最终分类器和主干模型被冻结）。'
- en: '![](../Images/d98c3145619c0c929b102d2dc6746db8.png)![](../Images/dbbce6731b28f76c9a45d956133a0690.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d98c3145619c0c929b102d2dc6746db8.png)![](../Images/dbbce6731b28f76c9a45d956133a0690.png)'
- en: 'Source: [Link](https://aclanthology.org/2021.eacl-main.8/)'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '来源: [链接](https://aclanthology.org/2021.eacl-main.8/)'
- en: 'Disadvantage: This strategy produces a final classifier with optimal quality
    at the price of earlier classifiers, since the backbone model parameters (which
    are the majority parameters) are solely optimized for the final classifier.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '缺点: 这种策略产生了一个具有最佳质量的最终分类器，但以早期分类器为代价，因为主干模型参数（大多数参数）仅为最终分类器进行优化。'
- en: '**Alternating**'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '**交替**'
- en: This strategy is proposed in this paper to overcome the disadvantages of the
    previous strategies. In this strategy the training alternates between two different
    objectives for odd-numbered and even-numbered iterations. In both, the backbone
    model is trained, but in the odd iterations the final classifier is also trained,
    while in the even iterations the first N-1 classifiers are also trained. This
    way there is a potential to balance between the performance of the final classifier
    and the performance of the early existing components.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 本文提出了这种策略以克服之前策略的缺点。在该策略中，训练在奇数和偶数轮次之间交替进行不同的目标。在两者中，主干模型都会进行训练，但在奇数轮次中，还训练最终分类器，而在偶数轮次中，还训练前N-1个分类器。这样有可能在最终分类器的性能和早期退出组件的性能之间取得平衡。
- en: '![](../Images/b61a4c130cc2bb3082dfe734efeb7408.png)![](../Images/a1986d40c183b6961018dbfef5ccab08.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b61a4c130cc2bb3082dfe734efeb7408.png)![](../Images/a1986d40c183b6961018dbfef5ccab08.png)'
- en: 'Source: [Link](https://aclanthology.org/2021.eacl-main.8/)'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 来源：[链接](https://aclanthology.org/2021.eacl-main.8/)
- en: '**2\. Regression tasks**'
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**2\. 回归任务**'
- en: This method of “stop inference when the model has high confidence in its prediction”
    can’t be applied to regression tasks, because they output real numbers and not
    probabilities.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: “当模型对其预测有高置信度时停止推理”的方法不能应用于回归任务，因为回归任务输出的是实数而非概率。
- en: To extend this idea to regression tasks, the authors suggest a Learning-To-Exit
    (LTE) component which is shared for all layers. This component is a one-layer
    fully-connected network that takes as input a hidden state of some layer and outputs
    a confidence score for the prediction in this layer. So at the inference time
    of a sample, if the produced confidence score at some layer is higher than the
    threshold, the hidden state is also inserted into the adjacent regressor to produce
    the output for this sample, and the inference stops.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将这个思路扩展到回归任务，作者建议使用一个对所有层共享的学习退出（LTE）组件。这个组件是一个单层全连接网络，它将某层的隐藏状态作为输入，并输出该层预测的置信得分。因此，在样本的推理时间，如果某层生成的置信得分高于阈值，隐藏状态也会被插入到相邻的回归器中以生成该样本的输出，推理过程则停止。
- en: 'Note that LTE is another component that has parameters to train. The loss function
    for this component is a simple MSE between the produced confidence score uᵢ and
    the “ground truth” confidence score ũᵢ at the i-th layer: Jᵢ = ||uᵢ − ũᵢ||₂².
    ũᵢ is estimated by negating the prediction’s absolute error: ũᵢ = 1- tanh( |gᵢ(hᵢ
    ;wᵢ) − y| ), where y is the ground truth value and gᵢ(hᵢ ;wᵢ) is the i’th regressor
    prediction.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，LTE是另一个需要训练的参数组件。该组件的损失函数是生成的置信得分uᵢ与第i层的“真实值”置信得分ũᵢ之间的简单MSE：Jᵢ = ||uᵢ −
    ũᵢ||₂²。ũᵢ通过否定预测的绝对误差来估计：ũᵢ = 1- tanh( |gᵢ(hᵢ ;wᵢ) − y| )，其中y是真实值，gᵢ(hᵢ ;wᵢ)是第i个回归预测值。
- en: The LTE component is trained with the rest of the model by substituting Lᵢ with
    Lᵢ+Jᵢ (for i=1,…,n-1).
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: LTE组件通过将Lᵢ替换为Lᵢ+Jᵢ（i=1,…,n-1）与模型的其余部分一起训练。
- en: Experiments
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实验
- en: The paper conducted several experiments. I’ll review three of them.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 本文进行了几个实验。我将回顾其中的三个。
- en: '**Experiment 1: Comparison of fine-tuning strategies**'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '**实验 1：微调策略比较**'
- en: 'The first experiment compared the 3 fine-tuning strategies (over 6 different
    classification tasks), by showing their layer-wise score curves: Each point in
    the curve shows the output score at a certain exit layer, i.e., **all samples
    were required to exit** at this layer for evaluation. Note that the scores were
    converted to be relative to the BERTᵇᵃˢᵉ baseline model (value of 100%), which
    is a model without early exiting components.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个实验比较了3种微调策略（涵盖6种不同的分类任务），通过展示它们的逐层得分曲线来进行比较：曲线中的每一点表示在某一退出层的输出得分，即**所有样本必须在此层退出**以进行评估。注意，这些得分已转换为相对于BERTᵇᵃˢᵉ基准模型（值为100%）的相对得分，这是一种不包含早期退出组件的模型。
- en: '![](../Images/5dab0fa03f9ec71bf61c3a602a7d623e.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5dab0fa03f9ec71bf61c3a602a7d623e.png)'
- en: 'Comparing Two-Stages (2STG), Joint, and Alternating (ALT) fine-tuning strategies.
    Source: [Link](https://aclanthology.org/2021.eacl-main.8/)'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 比较了两阶段（2STG）、联合和交替（ALT）微调策略。 来源：[链接](https://aclanthology.org/2021.eacl-main.8/)
- en: 'Several observations from the figure:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 从图中得到的几项观察：
- en: The accuracy of the model increases as we exit it later, which makes sense because
    deeper layers have a greater amount of complexity.
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型的准确性随着退出层的延迟而提高，这有意义，因为较深的层具有更高的复杂度。
- en: The Two-Stage strategy is suboptimal in earlier layers, which again makes sense
    since this strategy heavily optimizes the last classifier on the cost of the existing
    layers.
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 两阶段策略在早期层次上表现不佳，这也有意义，因为该策略在现有层次的成本上重度优化了最后的分类器。
- en: The Alternating strategy is better than the Joint strategy in later layers and
    slightly weaker in earlier layers.
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 交替策略在后期层次上优于联合策略，而在早期层次上略显逊色。
- en: The conclusion is that the Alternating strategy provides good results on the
    early existing components while preserving the performance of the final classifier.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 结论是交替策略在早期退出组件上提供了良好的结果，同时保持了最终分类器的性能。
- en: '**Experiment 2: Quality–efficiency trade-offs**'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '**实验2：质量–效率权衡**'
- en: 'In this experiment several models were used:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在本实验中使用了几种模型：
- en: '**Raw** — BERTᵇᵃˢᵉ model with no early exiting components (baseline)'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Raw** — 没有早期退出组件的BERTᵇᵃˢᵉ模型（基准）'
- en: '**ALT** — BERTᵇᵃˢᵉ + early exiting components with Alternating fine-tuning
    strategy'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ALT** — BERTᵇᵃˢᵉ + 早期退出组件，采用交替微调策略'
- en: '**DB** — DistilBERT, a BERTᵇᵃˢᵉ model that is reduced to a smaller model using
    the knowledge distillation method'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**DB** — DistilBERT，使用知识蒸馏方法将BERTᵇᵃˢᵉ模型缩减为更小的模型'
- en: '**DB+ALT** — DistilBERT + early exiting components with Alternating fine-tuning
    strategy'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**DB+ALT** — DistilBERT + 早期退出组件，采用交替微调策略'
- en: 'These models were compared by two metrics to examine the quality–efficiency
    trade-off :'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 这些模型通过两个指标进行了比较，以检查质量–效率权衡：
- en: '**Metric for model quality**: Accuracy score for Raw and relative scores for
    the other models (w.r.t. Raw model).'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型质量指标**：Raw的准确率得分和其他模型的相对得分（相对于Raw模型）。'
- en: '**Metric for model efficiency**: Number of layers for RAW and relative saved
    layers for the other models (w.r.t. Raw model). For ALT and DB+ALT models, the
    number of saved layers is calculated by using the average exiting layer.'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型效率指标**：RAW的层数和其他模型相对节省的层数（相对于Raw模型）。对于ALT和DB+ALT模型，节省的层数通过使用平均退出层计算得出。'
- en: The experiment goal is to first examine the proposed model (ALT) quality-efficiency
    trade-off in comparison to the baseline model (RAW). Secondly, check whether the
    proposed model (ALT) is better than another strong efficient method (DB). Lastly,
    check if the DistilBert model (DB) can be improved by applying the proposed model
    on top of it (DB+ALT).
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 实验目标是首先检查所提出模型（ALT）与基准模型（RAW）的质量-效率权衡。其次，检查所提出模型（ALT）是否优于另一种强效的方法（DB）。最后，检查是否通过在DistilBert模型（DB）上应用所提出模型（DB+ALT）可以改进DistilBert模型（DB）。
- en: 'Note that in contrast to experiment 1, here a “regular” inference phase was
    applied for the models with the early exiting components (ALT and DB+ALT): the
    test set samples **were free to exit** when their confidence score at some layer
    was higher than the threshold. In addition, The three different rows in ALT were
    generated by varying the confidence threshold.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，与实验1相比，此处对早期退出组件（ALT和DB+ALT）的模型应用了“常规”推理阶段：测试集样本在某层的置信度分数高于阈值时**可以自由退出**。此外，ALT中的三行不同是通过调整置信度阈值生成的。
- en: '![](../Images/f7e37f3e71d2908b75a2366e525855c9.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f7e37f3e71d2908b75a2366e525855c9.png)'
- en: 'Quality–efficiency trade-offs. Source: [Link](https://aclanthology.org/2021.eacl-main.8/)'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 质量–效率权衡。来源：[链接](https://aclanthology.org/2021.eacl-main.8/)
- en: 'Let’s take an example of the results: the first ALT model on the MRPC dataset
    didn’t use 30% of the layers on average, but nonetheless achieved 99% of the RAW
    baseline model score! Decreasing the confidence threshold led to more efficient
    models (saving 56% and 74% on average), with a reasonable quality degradation
    (97% and 94%, respectively).'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 以结果为例：MRPC数据集上的第一个ALT模型平均未使用30%的层，但仍实现了RAW基准模型得分的99%！降低置信度阈值导致模型效率提高（平均节省56%和74%），质量降级合理（分别为97%和94%）。
- en: 'Main observations:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 主要观察结果：
- en: Using early exiting (with Alternating fine-tuning) can decrease inference computation
    while still achieving good scores, compared with a baseline model with no early
    exiting components.
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用早期退出（配合交替微调）可以减少推理计算，同时仍能获得良好的得分，与没有早期退出组件的基准模型相比。
- en: In most cases, Alternating outperforms DistilBERT, which requires distillation
    in pre-training and is therefore much more resource-demanding.
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在大多数情况下，交替策略优于DistilBERT，而DistilBERT需要在预训练中进行蒸馏，因此资源需求更高。
- en: Using Alternating further improves model efficiency on top of DistilBERT, indicating
    that early exiting is cumulative with other acceleration methods.
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用交替策略进一步提高了DistilBERT上的模型效率，表明早期退出与其他加速方法是累积的。
- en: '**Experiment 3: Regression task**'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '**实验3：回归任务**'
- en: In this experiment, the proposed model (ALT-LTE) is compared against a previous
    work model (PABEE) on a task of predicting similarity between two sentences (STS-B
    dataset).
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在此实验中，将所提出的模型（ALT-LTE）与先前的模型（PABEE）在预测两个句子相似度的任务（STS-B数据集）上进行了比较。
- en: '![](../Images/dd5d5a0a23dc38cbf58987d9a9054662.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/dd5d5a0a23dc38cbf58987d9a9054662.png)'
- en: 'Comparing LTE with PABEE on STS-B. Source: [Link](https://aclanthology.org/2021.eacl-main.8/)'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 比较LTE与PABEE在STS-B上的表现。来源：[链接](https://aclanthology.org/2021.eacl-main.8/)
- en: As can be seen, ALT-LTE achieved the same scores with a faster inference time.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 如图所示，ALT-LTE在推理时间上取得了相同的分数。
- en: '**Conclusions**'
  id: totrans-95
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**结论**'
- en: Fast inference time is crucial for DNNs that are deployed on resource-constrained
    devices, for providing real-time responses to user requests, and for cost and
    sustainability reasons. The “early exiting” method improves inference time by
    allowing samples to exit at different depths within the network, potentially making
    many “easier” samples to exit early and thus avoiding unnecessary computations
    while still maintaining accuracy.
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 快速推理时间对于部署在资源受限设备上的深度神经网络（DNN）至关重要，这不仅是为了向用户请求提供实时响应，还涉及到成本和可持续性问题。通过允许样本在网络中的不同深度退出，“早期退出”方法提高了推理时间，可能使许多“较容易”的样本提前退出，从而避免不必要的计算，同时保持准确性。
- en: The BERxiT paper improves this method by proposing the Alternating fine-tuning
    strategy, whose goal is to balance between the performance of the final classifier
    and the performance of the early existing components. In addition, BERxiT extends
    the early exiting method to regression tasks by proposing the Learning-To-Exit
    (LTE) component which learns to output confidence scores.
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: BERxiT论文通过提出交替微调策略改进了这一方法，其目标是在最终分类器的性能与早期退出组件的性能之间取得平衡。此外，BERxiT通过提出学习退出（LTE）组件，将早期退出方法扩展到回归任务，该组件学习输出置信度分数。
- en: The experiments showed that the Alternating strategy achieves better quality-efficiency
    trade-off, that the LTE component is indeed successful for regression tasks, and
    that the early exiting method can be combined with other acceleration methods.
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实验结果表明，交替策略在质量-效率权衡上表现更佳，LTE组件在回归任务中确实有效，并且早期退出方法可以与其他加速方法结合使用。
- en: '**References**'
  id: totrans-99
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**参考文献**'
- en: '[1] Xin, J., Tang, R., Yu, Y., & Lin, J.J. (2021). [BERxiT: Early Exiting for
    BERT with Better Fine-Tuning and Extension to Regression](https://www.semanticscholar.org/paper/BERxiT%3A-Early-Exiting-for-BERT-with-Better-and-to-Xin-Tang/7b37c0a4976c4d2a5a440d494fbb0f3daede2a00).
    *Conference of the European Chapter of the Association for Computational Linguistics*.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] Xin, J., Tang, R., Yu, Y., & Lin, J.J. (2021). [BERxiT：BERT的早期退出，改进微调及扩展至回归任务](https://www.semanticscholar.org/paper/BERxiT%3A-Early-Exiting-for-BERT-with-Better-and-to-Xin-Tang/7b37c0a4976c4d2a5a440d494fbb0f3daede2a00).
    *欧洲计算语言学协会会议*。'
- en: '[2] Strubell, E., Ganesh, A., & McCallum, A. (2019). [Energy and Policy Considerations
    for Deep Learning in NLP](https://www.semanticscholar.org/paper/Energy-and-Policy-Considerations-for-Deep-Learning-Strubell-Ganesh/d6a083dad7114f3a39adc65c09bfbb6cf3fee9ea).
    *ArXiv, abs/1906.02243*.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] Strubell, E., Ganesh, A., & McCallum, A. (2019). [自然语言处理中的深度学习的能耗与政策考虑](https://www.semanticscholar.org/paper/Energy-and-Policy-Considerations-for-Deep-Learning-Strubell-Ganesh/d6a083dad7114f3a39adc65c09bfbb6cf3fee9ea).
    *ArXiv, abs/1906.02243*。'
- en: '[3] Desislavov, R., Mart’inez-Plumed, F., & Hern’andez-Orallo, J. (2021). [Compute
    and Energy Consumption Trends in Deep Learning Inference](https://www.semanticscholar.org/paper/Compute-and-Energy-Consumption-Trends-in-Deep-Desislavov-Mart''inez-Plumed/01ce9f7d1c35e88f6ab3abe51bf1e1370da718b5).
    *ArXiv, abs/2109.05472*.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] Desislavov, R., Mart’inez-Plumed, F., & Hern’andez-Orallo, J. (2021). [深度学习推理中的计算和能耗趋势](https://www.semanticscholar.org/paper/Compute-and-Energy-Consumption-Trends-in-Deep-Desislavov-Mart''inez-Plumed/01ce9f7d1c35e88f6ab3abe51bf1e1370da718b5).
    *ArXiv, abs/2109.05472*。'
- en: '[4] Schwartz, R., Dodge, J., Smith, N., & Etzioni, O. (2019). [Green AI](https://www.semanticscholar.org/paper/Green-AI-Schwartz-Dodge/fb73b93de3734a996829caf31e4310e0054e9c6b).
    *Communications of the ACM, 63*, 54–63.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] Schwartz, R., Dodge, J., Smith, N., & Etzioni, O. (2019). [绿色人工智能](https://www.semanticscholar.org/paper/Green-AI-Schwartz-Dodge/fb73b93de3734a996829caf31e4310e0054e9c6b).
    *ACM通讯，63*, 54–63。'
- en: '[5] Treviso, M.V., Ji, T., Lee, J., van Aken, B., Cao, Q., Ciosici, M.R., Hassid,
    M., Heafield, K., Hooker, S., Martins, P.H., Martins, A., Milder, P., Raffel,
    C., Simpson, E., Slonim, N., Balasubramanian, N., Derczynski, L., & Schwartz,
    R. (2022). [Efficient Methods for Natural Language Processing: A Survey](https://arxiv.org/abs/2209.00099).
    *ArXiv, abs/2209.00099*.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] Treviso, M.V., Ji, T., Lee, J., van Aken, B., Cao, Q., Ciosici, M.R., Hassid,
    M., Heafield, K., Hooker, S., Martins, P.H., Martins, A., Milder, P., Raffel,
    C., Simpson, E., Slonim, N., Balasubramanian, N., Derczynski, L., & Schwartz,
    R. (2022). [自然语言处理的高效方法：综述](https://arxiv.org/abs/2209.00099)。*ArXiv, abs/2209.00099*。'
- en: '[6] Schwartz, R., Stanovsky, G., Swayamdipta, S., Dodge, J., & Smith, N.A.
    (2020). [The Right Tool for the Job: Matching Model and Instance Complexities](https://arxiv.org/abs/2004.07453).
    *Annual Meeting of the Association for Computational Linguistics*.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '[6] Schwartz, R., Stanovsky, G., Swayamdipta, S., Dodge, J., & Smith, N.A.
    (2020). [合适的工具：模型与实例复杂度的匹配](https://arxiv.org/abs/2004.07453)。*计算语言学协会年会*。'
- en: '[7] Xin, J., Tang, R., Lee, J., Yu, Y., & Lin, J.J. (2020). [DeeBERT: Dynamic
    Early Exiting for Accelerating BERT Inference](https://arxiv.org/abs/2004.12993).
    *Annual Meeting of the Association for Computational Linguistics*.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '[7] Xin, J., Tang, R., Lee, J., Yu, Y., & Lin, J.J. (2020). [DeeBERT：加速BERT推理的动态早期退出](https://arxiv.org/abs/2004.12993)。*计算语言学协会年会*。'
- en: '[8] Liu, W., Zhou, P., Zhao, Z., Wang, Z., Deng, H., & Ju, Q. (2020). [FastBERT:
    a Self-distilling BERT with Adaptive Inference Time](https://arxiv.org/abs/2004.02178).
    *Annual Meeting of the Association for Computational Linguistics*.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '[8] Liu, W., Zhou, P., Zhao, Z., Wang, Z., Deng, H., & Ju, Q. (2020). [FastBERT：一种自我蒸馏的BERT与自适应推理时间](https://arxiv.org/abs/2004.02178)。*计算语言学协会年会*。'
