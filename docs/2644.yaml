- en: The Gradient Descent Algorithm and the Intuition Behind It
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ¢¯åº¦ä¸‹é™ç®—æ³•åŠå…¶èƒŒåçš„ç›´è§‰
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/the-gradient-descent-algorithm-4d54e0d446cd?source=collection_archive---------6-----------------------#2023-08-19](https://towardsdatascience.com/the-gradient-descent-algorithm-4d54e0d446cd?source=collection_archive---------6-----------------------#2023-08-19)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://towardsdatascience.com/the-gradient-descent-algorithm-4d54e0d446cd?source=collection_archive---------6-----------------------#2023-08-19](https://towardsdatascience.com/the-gradient-descent-algorithm-4d54e0d446cd?source=collection_archive---------6-----------------------#2023-08-19)
- en: A technical description of the Gradient Descent method, complemented with a
    graphical representation of the algorithm at work
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å¯¹æ¢¯åº¦ä¸‹é™æ–¹æ³•çš„æŠ€æœ¯æè¿°ï¼Œå¹¶é…æœ‰ç®—æ³•è¿è¡Œçš„å›¾ç¤º
- en: '[](https://medium.com/@fyi.oamd?source=post_page-----4d54e0d446cd--------------------------------)[![Antonieta
    Mastrogiuseppe](../Images/3b9e70a54fcb887f5ccee6d305085675.png)](https://medium.com/@fyi.oamd?source=post_page-----4d54e0d446cd--------------------------------)[](https://towardsdatascience.com/?source=post_page-----4d54e0d446cd--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----4d54e0d446cd--------------------------------)
    [Antonieta Mastrogiuseppe](https://medium.com/@fyi.oamd?source=post_page-----4d54e0d446cd--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@fyi.oamd?source=post_page-----4d54e0d446cd--------------------------------)[![Antonieta
    Mastrogiuseppe](../Images/3b9e70a54fcb887f5ccee6d305085675.png)](https://medium.com/@fyi.oamd?source=post_page-----4d54e0d446cd--------------------------------)[](https://towardsdatascience.com/?source=post_page-----4d54e0d446cd--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----4d54e0d446cd--------------------------------)
    [Antonieta Mastrogiuseppe](https://medium.com/@fyi.oamd?source=post_page-----4d54e0d446cd--------------------------------)'
- en: Â·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Â·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fa8ee237975ec&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-gradient-descent-algorithm-4d54e0d446cd&user=Antonieta+Mastrogiuseppe&userId=a8ee237975ec&source=post_page-a8ee237975ec----4d54e0d446cd---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----4d54e0d446cd--------------------------------)
    Â·9 min readÂ·Aug 19, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F4d54e0d446cd&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-gradient-descent-algorithm-4d54e0d446cd&user=Antonieta+Mastrogiuseppe&userId=a8ee237975ec&source=-----4d54e0d446cd---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[å…³æ³¨](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fa8ee237975ec&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-gradient-descent-algorithm-4d54e0d446cd&user=Antonieta+Mastrogiuseppe&userId=a8ee237975ec&source=post_page-a8ee237975ec----4d54e0d446cd---------------------post_header-----------)
    å‘è¡¨åœ¨ [Towards Data Science](https://towardsdatascience.com/?source=post_page-----4d54e0d446cd--------------------------------)
    Â·9 åˆ†é’Ÿé˜…è¯»Â·2023å¹´8æœˆ19æ—¥[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F4d54e0d446cd&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-gradient-descent-algorithm-4d54e0d446cd&user=Antonieta+Mastrogiuseppe&userId=a8ee237975ec&source=-----4d54e0d446cd---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4d54e0d446cd&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-gradient-descent-algorithm-4d54e0d446cd&source=-----4d54e0d446cd---------------------bookmark_footer-----------)![](../Images/6e6a647dfadc374fcf9425097033183a.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4d54e0d446cd&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-gradient-descent-algorithm-4d54e0d446cd&source=-----4d54e0d446cd---------------------bookmark_footer-----------)![](../Images/6e6a647dfadc374fcf9425097033183a.png)'
- en: â€œOnce youâ€™re over the hill you begin to pick up speedâ€ by Arthur Schopenhauer.
    Photo taken by the author.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: äºšç‘ŸÂ·å”æœ¬åçš„åè¨€ï¼šâ€œä¸€æ—¦ä½ è¶Šè¿‡å±±ä¸˜ï¼Œä½ ä¼šå¼€å§‹åŠ é€Ÿâ€ã€‚ç…§ç‰‡ç”±ä½œè€…æ‹æ‘„ã€‚
- en: '**INTRODUCING SOME KEY DEFINITIONS**'
  id: totrans-9
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**å¼•å…¥ä¸€äº›å…³é”®å®šä¹‰**'
- en: Within optimization methods, and in the first order algorithm type, one has
    certainly heard of the one known as Gradient Descent. It is of the first-order
    optimization type as it requires the first-order derivative, namely the gradient.
    By optimizing, gradient descent aims to minimize the difference between the â€œactualâ€
    output and the predicted output of the model as measured by the objective function,
    namely a cost function. The gradient, or slope, is defined as the direction of
    the line drawn by such function (curved or straight) at a given point of such
    line. Iteratively, gradient descent aims to differentiate the cost function at
    different points of the line so to derive the degree of change in direction at
    these points and hence take steps towards the steepest descent, namely the local
    minimum. As its name indicates, the *gradient* is used as the direction towards
    the steepest *descent* in search for the local minimum where the parametersâ€™ values
    of the cost function being optimized are minimized hence at its lowest values.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä¼˜åŒ–æ–¹æ³•ä¸­ï¼Œå°¤å…¶æ˜¯åœ¨ä¸€é˜¶ç®—æ³•ç±»å‹ä¸­ï¼Œæ‚¨è‚¯å®šå¬è¯´è¿‡ä¸€ç§è¢«ç§°ä¸ºæ¢¯åº¦ä¸‹é™çš„æ–¹æ³•ã€‚å®ƒæ˜¯ä¸€é˜¶ä¼˜åŒ–ç±»å‹ï¼Œå› ä¸ºå®ƒéœ€è¦ä¸€é˜¶å¯¼æ•°ï¼Œå³æ¢¯åº¦ã€‚é€šè¿‡ä¼˜åŒ–ï¼Œæ¢¯åº¦ä¸‹é™æ—¨åœ¨æœ€å°åŒ–â€œå®é™…â€è¾“å‡ºå’Œæ¨¡å‹é¢„æµ‹è¾“å‡ºä¹‹é—´çš„å·®å¼‚ï¼Œè¿™ç§å·®å¼‚ç”±ç›®æ ‡å‡½æ•°ï¼ˆå³æˆæœ¬å‡½æ•°ï¼‰æµ‹é‡ã€‚æ¢¯åº¦æˆ–æ–œç‡è¢«å®šä¹‰ä¸ºåœ¨è¯¥å‡½æ•°ï¼ˆå¼¯æ›²æˆ–ç›´çº¿ï¼‰åœ¨ç»™å®šç‚¹å¤„ç»˜åˆ¶çš„çº¿çš„æ–¹å‘ã€‚è¿­ä»£åœ°ï¼Œæ¢¯åº¦ä¸‹é™æ—¨åœ¨åœ¨ä¸åŒç‚¹å¯¹æˆæœ¬å‡½æ•°è¿›è¡Œå¾®åˆ†ï¼Œä»è€Œæ¨å¯¼å‡ºè¿™äº›ç‚¹ä¸Šæ–¹å‘å˜åŒ–çš„ç¨‹åº¦ï¼Œå¹¶æœå‘æœ€é™¡ä¸‹é™çš„æ–¹å‘ï¼Œå³å±€éƒ¨æœ€å°å€¼ã€‚æ­£å¦‚å…¶åå­—æ‰€ç¤ºï¼Œ*æ¢¯åº¦*
    ç”¨ä½œå¯»æ‰¾å±€éƒ¨æœ€å°å€¼çš„*ä¸‹é™*æ–¹å‘ï¼Œåœ¨é‚£é‡Œä¼˜åŒ–çš„æˆæœ¬å‡½æ•°çš„å‚æ•°å€¼è¢«æœ€å°åŒ–ï¼Œä»è€Œè¾¾åˆ°æœ€ä½å€¼ã€‚
- en: Gradient Descent is mostly used (among others) to train machine learning models
    and deep learning models, the latter based on a neural network architectural type.
    From linear regression and logistic regression to neural networks, gradient descent
    aims to calculate the functionâ€™s best parameters values. In its simplest form,
    gradient descent aims to minimize the error term of the below linear regression
    by deriving the optimal values of the parameters for the independent variables.
    This is,
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: æ¢¯åº¦ä¸‹é™ä¸»è¦ç”¨äºï¼ˆåŒ…æ‹¬ä½†ä¸é™äºï¼‰è®­ç»ƒæœºå™¨å­¦ä¹ æ¨¡å‹å’Œæ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Œåè€…åŸºäºç¥ç»ç½‘ç»œç»“æ„ç±»å‹ã€‚ä»çº¿æ€§å›å½’å’Œé€»è¾‘å›å½’åˆ°ç¥ç»ç½‘ç»œï¼Œæ¢¯åº¦ä¸‹é™æ—¨åœ¨è®¡ç®—å‡½æ•°çš„æœ€ä½³å‚æ•°å€¼ã€‚åœ¨å…¶æœ€ç®€å•çš„å½¢å¼ä¸­ï¼Œæ¢¯åº¦ä¸‹é™æ—¨åœ¨é€šè¿‡æ¨å¯¼ç‹¬ç«‹å˜é‡çš„å‚æ•°çš„æœ€ä½³å€¼æ¥æœ€å°åŒ–ä¸‹é¢çº¿æ€§å›å½’çš„è¯¯å·®é¡¹ã€‚è¿™æ˜¯ï¼Œ
- en: y = Î²0 + Î²1 * X1 + â€¦ Î²k * Xk + Æ
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: y = Î²0 + Î²1 * X1 + â€¦ Î²k * Xk + Æ
- en: where,
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­ï¼Œ
- en: '*y* is the dependent variable'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '*y* æ˜¯å› å˜é‡'
- en: '*k* number of independent variables'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '*k* æ˜¯ç‹¬ç«‹å˜é‡çš„æ•°é‡'
- en: '*X* independent variable'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '*X* æ˜¯ç‹¬ç«‹å˜é‡'
- en: Î² parameter
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: Î² å‚æ•°
- en: '*Æ* error term component'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '*Æ* æ˜¯è¯¯å·®é¡¹æˆåˆ†'
- en: 'In its more complex form, gradient descent is most frequently defined as the
    optimizer when training a deep learning model, specifically on the compiling phase.
    Deep learning is based on an interconnected network to learn and improve continuously,
    namely a neural network. Inspired by the human brain, a neural network is a highly
    complex network represented by artificial neurons, known as nodes. At the top
    level, the nodes have the important role to process and analyse the data from
    a node in the previous layer and pass it to another node in the next layer. In
    a neural network, a weight, namely the parameters of interest for optimization,
    are the link between nodes. They are the link between inputs/features and hidden
    layers hence they represent the importance of a given feature in predicting the
    final output. Finding the optimal value of a single weight depends on the value
    of many weights. And this optimization is taking placing for many weights at once,
    which in a deep neural network can be substantially large even in millions. Here
    is where gradient descent is found to perform highly efficiently on the large
    number of calculations involved, these based on the three main layers of a neural
    network: 1) Input Layer 2) Hidden Layer and 3) Output Layer.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ›´å¤æ‚çš„å½¢å¼ä¸­ï¼Œæ¢¯åº¦ä¸‹é™æœ€å¸¸è¢«å®šä¹‰ä¸ºè®­ç»ƒæ·±åº¦å­¦ä¹ æ¨¡å‹æ—¶çš„ä¼˜åŒ–å™¨ï¼Œç‰¹åˆ«æ˜¯åœ¨ç¼–è¯‘é˜¶æ®µã€‚æ·±åº¦å­¦ä¹ åŸºäºä¸€ä¸ªäº’è¿çš„ç½‘ç»œè¿›è¡Œä¸æ–­çš„å­¦ä¹ å’Œæ”¹è¿›ï¼Œè¿™ä¸ªç½‘ç»œè¢«ç§°ä¸ºç¥ç»ç½‘ç»œã€‚ç¥ç»ç½‘ç»œå—åˆ°äººè„‘çš„å¯å‘ï¼Œæ˜¯ç”±äººå·¥ç¥ç»å…ƒï¼ˆç§°ä¸ºèŠ‚ç‚¹ï¼‰ç»„æˆçš„é«˜åº¦å¤æ‚çš„ç½‘ç»œã€‚åœ¨æœ€ä¸Šå±‚ï¼ŒèŠ‚ç‚¹åœ¨å¤„ç†å’Œåˆ†ææ¥è‡ªå‰ä¸€å±‚èŠ‚ç‚¹çš„æ•°æ®ï¼Œå¹¶å°†å…¶ä¼ é€’åˆ°ä¸‹ä¸€å±‚èŠ‚ç‚¹æ–¹é¢æ‰®æ¼”ç€é‡è¦è§’è‰²ã€‚åœ¨ç¥ç»ç½‘ç»œä¸­ï¼Œæƒé‡ï¼Œå³ä¼˜åŒ–çš„å‚æ•°ï¼Œæ˜¯èŠ‚ç‚¹ä¹‹é—´çš„è”ç³»ã€‚å®ƒä»¬è¿æ¥äº†è¾“å…¥/ç‰¹å¾å’Œéšè—å±‚ï¼Œå› æ­¤å®ƒä»¬è¡¨ç¤ºäº†ç‰¹å®šç‰¹å¾åœ¨é¢„æµ‹æœ€ç»ˆè¾“å‡ºæ—¶çš„é‡è¦æ€§ã€‚æ‰¾åˆ°å•ä¸ªæƒé‡çš„æœ€ä½³å€¼å–å†³äºè®¸å¤šæƒé‡çš„å€¼ã€‚è€Œè¿™ç§ä¼˜åŒ–æ˜¯åŒæ—¶è¿›è¡Œçš„ï¼Œåœ¨æ·±åº¦ç¥ç»ç½‘ç»œä¸­ï¼Œå³ä½¿æ˜¯æ•°ç™¾ä¸‡ä¸ªæƒé‡ä¹Ÿå¯èƒ½ä¼šå¤§å¤§å¢åŠ ã€‚åœ¨è¿™é‡Œï¼Œæ¢¯åº¦ä¸‹é™åœ¨æ¶‰åŠå¤§é‡è®¡ç®—æ—¶è¡¨ç°å¾—éå¸¸é«˜æ•ˆï¼Œè¿™äº›è®¡ç®—åŸºäºç¥ç»ç½‘ç»œçš„ä¸‰ä¸ªä¸»è¦å±‚ï¼š1ï¼‰è¾“å…¥å±‚
    2ï¼‰éšè—å±‚ å’Œ 3ï¼‰è¾“å‡ºå±‚ã€‚
- en: There are numerous papers that properly detail and expand on methods such deep
    learning and methods that estimates the value of a functionâ€™s parameters hence
    expand on the difference between gradient descent and Ordinary Least Square (OLS),
    in the case of linear regression for example. As this is not the focus of this
    paper, the reader is prompted to investigate further and expand for a good understanding
    on such methodologies.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: æœ‰å¤§é‡æ–‡çŒ®è¯¦ç»†é˜è¿°å’Œæ‰©å±•äº†è¯¸å¦‚æ·±åº¦å­¦ä¹ æ–¹æ³•åŠä¼°è®¡å‡½æ•°å‚æ•°å€¼çš„æ–¹æ³•ï¼Œè¿›è€Œæ‰©å±•äº†æ¢¯åº¦ä¸‹é™å’Œæ™®é€šæœ€å°äºŒä¹˜æ³•ï¼ˆOLSï¼‰ä¹‹é—´çš„å·®å¼‚ï¼Œä¾‹å¦‚åœ¨çº¿æ€§å›å½’çš„æƒ…å†µä¸‹ã€‚ç”±äºè¿™ä¸æ˜¯æœ¬æ–‡çš„é‡ç‚¹ï¼Œè¯»è€…å¯ä»¥è¿›ä¸€æ­¥è°ƒæŸ¥å’Œæ‰©å±•ï¼Œä»¥ä¾¿æ›´å¥½åœ°ç†è§£è¿™äº›æ–¹æ³•è®ºã€‚
- en: '**2\. TIME FOR SOME CALCULUS!**'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '**2\. è®¡ç®—æ—¶é—´ï¼**'
- en: 'For a good understanding of gradient descent, we need to expand on the definition
    of a differentiable function. A function, explicitly *Æ’(x),* is differentiable
    when the derivative can be defined at any point of the curved line derived by
    such function. This is, *for all* points in the domain of the function *Æ’(x).*
    Here, two concepts reinforce this definition: first-order derivative and second
    order derivative. The first-order derivative formula is defined as follow:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†æ›´å¥½åœ°ç†è§£æ¢¯åº¦ä¸‹é™ï¼Œæˆ‘ä»¬éœ€è¦æ‰©å±•å¯¹å¯å¾®åˆ†å‡½æ•°çš„å®šä¹‰ã€‚ä¸€ä¸ªå‡½æ•°ï¼Œæ˜ç¡®åœ°è¯´æ˜¯ *Æ’(x)*ï¼Œå½“å…¶å¯¼æ•°åœ¨è¯¥å‡½æ•°çš„æ›²çº¿ä¸Šçš„ä»»æ„ç‚¹éƒ½å¯ä»¥å®šä¹‰æ—¶ï¼Œè¯¥å‡½æ•°å°±æ˜¯å¯å¾®åˆ†çš„ã€‚è¿™å°±æ˜¯è¯´ï¼Œå¯¹äºå‡½æ•°
    *Æ’(x)* çš„å®šä¹‰åŸŸä¸­çš„ *æ‰€æœ‰* ç‚¹ã€‚è¿™é‡Œï¼Œæœ‰ä¸¤ä¸ªæ¦‚å¿µæ”¯æŒè¿™ä¸ªå®šä¹‰ï¼šä¸€é˜¶å¯¼æ•°å’ŒäºŒé˜¶å¯¼æ•°ã€‚ä¸€é˜¶å¯¼æ•°å…¬å¼å®šä¹‰å¦‚ä¸‹ï¼š
- en: '![](../Images/dbe88c79581bd5d59b2871fba3e17a34.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/dbe88c79581bd5d59b2871fba3e17a34.png)'
- en: Strictly speaking, the first-order derivative of a function, denoted *Æ’â€™(x)*
    or *df(x)/dx*, is the slope of the function *Æ’(x)* at a given point value of x.
    If the slope is positive (negative), it indicates the function is increasing (decreasing)
    and by how much. A positive slope is an indication that as the value of x increases,
    so is the function *Æ’(x).* A negative slope, on the other hand, indicates that
    as the value of x increases, *Æ’(x)* decreases. The second-order derivative is
    the derivative of the derivative of the function *Æ’(x).* Denoted *Æ’â€™â€™(x)* or *d2f(x)/dx2*,
    the second derivative indicates the shape of the function *Æ’(x).* This is, whether
    such function is concave or convex. Mathematically speaking, (and this is important!!!)
    a second derivative will distinguish a relative maximum from a relative minimum.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸¥æ ¼æ¥è¯´ï¼Œå‡½æ•°çš„ä¸€é˜¶å¯¼æ•°ï¼Œè®°ä½œ *Æ’â€™(x)* æˆ– *df(x)/dx*ï¼Œæ˜¯å‡½æ•° *Æ’(x)* åœ¨ç»™å®š x ç‚¹çš„æ–œç‡ã€‚å¦‚æœæ–œç‡ä¸ºæ­£ï¼ˆè´Ÿï¼‰ï¼Œåˆ™è¡¨æ˜å‡½æ•°åœ¨å¢åŠ ï¼ˆå‡å°‘ï¼‰ï¼Œä»¥åŠå¢åŠ ï¼ˆå‡å°‘ï¼‰çš„å¹…åº¦ã€‚æ­£æ–œç‡è¡¨ç¤ºéšç€
    x å€¼çš„å¢åŠ ï¼Œå‡½æ•° *Æ’(x)* ä¹Ÿå¢åŠ ã€‚ç›¸åï¼Œè´Ÿæ–œç‡è¡¨ç¤ºéšç€ x å€¼çš„å¢åŠ ï¼Œ*Æ’(x)* å‡å°‘ã€‚äºŒé˜¶å¯¼æ•°æ˜¯å‡½æ•° *Æ’(x)* çš„å¯¼æ•°çš„å¯¼æ•°ã€‚è®°ä½œ *Æ’â€™â€™(x)*
    æˆ– *d2f(x)/dx2*ï¼ŒäºŒé˜¶å¯¼æ•°æŒ‡ç¤ºäº†å‡½æ•° *Æ’(x)* çš„å½¢çŠ¶ï¼Œå³è¯¥å‡½æ•°æ˜¯å‡¹çš„è¿˜æ˜¯å‡¸çš„ã€‚ä»æ•°å­¦ä¸Šè®²ï¼ˆè¿™ä¸€ç‚¹å¾ˆé‡è¦ï¼ï¼ï¼ï¼‰ï¼ŒäºŒé˜¶å¯¼æ•°å¯ä»¥åŒºåˆ†ç›¸å¯¹æœ€å¤§å€¼å’Œç›¸å¯¹æœ€å°å€¼ã€‚
- en: where,
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­ï¼Œ
- en: '*Æ’â€™â€™(x)* > 0 then *Æ’(x)* is convex at x = *a*'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœ *Æ’â€™â€™(x)* > 0ï¼Œåˆ™ *Æ’(x)* åœ¨ x = *a* å¤„æ˜¯å‡¸çš„ã€‚
- en: and if *Æ’â€™(a)* = 0 then *a* is a critical point hence a relative minimum
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœ *Æ’â€™(a)* = 0ï¼Œåˆ™ *a* æ˜¯ä¸€ä¸ªä¸´ç•Œç‚¹ï¼Œå› æ­¤æ˜¯ç›¸å¯¹æœ€å°å€¼ã€‚
- en: '![](../Images/92b0ff98218d34de751a3104e0fd8502.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/92b0ff98218d34de751a3104e0fd8502.png)'
- en: Graph drawn by the author
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ç”±ä½œè€…ç»˜åˆ¶
- en: where,
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­ï¼Œ
- en: '*Æ’â€™â€™(x)* < 0 then *Æ’(x)* is concave at x = *a*'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœ *Æ’â€™â€™(x)* < 0ï¼Œåˆ™ *Æ’(x)* åœ¨ x = *a* å¤„æ˜¯å‡¹çš„ã€‚
- en: and if *Æ’â€™(a)* = 0 then *a* is a critical point hence a relative maximum
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœ *Æ’â€™(a)* = 0ï¼Œåˆ™ *a* æ˜¯ä¸€ä¸ªä¸´ç•Œç‚¹ï¼Œå› æ­¤æ˜¯ç›¸å¯¹æœ€å¤§å€¼ã€‚
- en: '![](../Images/15d54f3b0ef54d9df8b2ef80dba5c37b.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/15d54f3b0ef54d9df8b2ef80dba5c37b.png)'
- en: Graph drawn by the author
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ç”±ä½œè€…ç»˜åˆ¶
- en: 'or if the second derivative is equal to zero, then either 1) the function *Æ’(x)*
    is in a turning point, known as Inflection point, where it changes from concave
    to convex or vice versa or 2) the function at that point is undefined (i.e., non-continuous).
    In the case of the former:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœäºŒé˜¶å¯¼æ•°ç­‰äºé›¶ï¼Œåˆ™å¯èƒ½æœ‰ä¸¤ç§æƒ…å†µï¼š1) å‡½æ•° *Æ’(x)* å¤„äºè½¬æŠ˜ç‚¹ï¼Œå³æ‹ç‚¹ï¼Œåœ¨è¯¥ç‚¹å‡½æ•°ä»å‡¹å˜ä¸ºå‡¸ï¼Œæˆ–åä¹‹ï¼›2) åœ¨è¯¥ç‚¹å‡½æ•°æœªå®šä¹‰ï¼ˆå³ï¼Œä¸è¿ç»­ï¼‰ã€‚å¯¹äºå‰è€…ï¼š
- en: '*Æ’â€™â€™(x)* = 0 then *Æ’(x)* is at an inflection point at x = *2*'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœ *Æ’â€™â€™(x)* = 0ï¼Œåˆ™ *Æ’(x)* åœ¨ x = *2* å¤„æ˜¯æ‹ç‚¹ã€‚
- en: '![](../Images/705d5e1f36ea883f6348237a83a1dea8.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/705d5e1f36ea883f6348237a83a1dea8.png)'
- en: Graph drawn by the author
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ç”±ä½œè€…ç»˜åˆ¶
- en: The above has focused on a function with a single independent variable, namely
    a univariate function, y = *Æ’(x).* In the real world, one will be studying and
    modelling multivariable functions, where the variable under study is impacted
    by multiple factors, this is two or more independent variables, y = Æ’(x, z). To
    measure the impact of a change of the independent variable x in the dependent
    variable y, keeping z constant, the partial derivative of the function with respect
    to x is taken. Thus, partial derivatives calculate the rate of change in the cost
    function caused by a change in each of their inputs. Gradient descent iteratively
    calculates these changes in the cost function and at each different step updates
    the values of the parameters of such functions till reaching the minimum point
    where the value of such parameters is optimized hence the cost function is minimized.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸Šè¿°å†…å®¹é›†ä¸­äºä¸€ä¸ªå…·æœ‰å•ä¸€è‡ªå˜é‡çš„å‡½æ•°ï¼Œå³ä¸€å…ƒå‡½æ•° y = *Æ’(x)*ã€‚åœ¨ç°å®ä¸–ç•Œä¸­ï¼Œäººä»¬ä¼šç ”ç©¶å’Œå»ºæ¨¡å¤šå˜é‡å‡½æ•°ï¼Œå…¶ä¸­ç ”ç©¶çš„å˜é‡å—åˆ°å¤šä¸ªå› ç´ çš„å½±å“ï¼Œè¿™å°±æ˜¯ä¸¤ä¸ªæˆ–æ›´å¤šè‡ªå˜é‡
    y = Æ’(x, z)ã€‚ä¸ºäº†æµ‹é‡è‡ªå˜é‡ x å¯¹å› å˜é‡ y çš„å½±å“ï¼Œå¹¶ä¿æŒ z ä¸å˜ï¼Œéœ€è¦è®¡ç®—å‡½æ•°å…³äº x çš„åå¯¼æ•°ã€‚å› æ­¤ï¼Œåå¯¼æ•°è®¡ç®—äº†å› æ¯ä¸ªè¾“å…¥çš„å˜åŒ–å¼•èµ·çš„æˆæœ¬å‡½æ•°çš„å˜åŒ–ç‡ã€‚æ¢¯åº¦ä¸‹é™è¿­ä»£åœ°è®¡ç®—è¿™äº›æˆæœ¬å‡½æ•°çš„å˜åŒ–ï¼Œå¹¶åœ¨æ¯ä¸€æ­¥æ›´æ–°è¿™äº›å‡½æ•°å‚æ•°çš„å€¼ï¼Œç›´åˆ°è¾¾åˆ°ä½¿å‚æ•°å€¼ä¼˜åŒ–ã€ä»è€Œæœ€å°åŒ–æˆæœ¬å‡½æ•°çš„æœ€å°ç‚¹ã€‚
- en: '**3\. THE GRADIENT DESCENT ALGORITHM AT WORK**'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '**3\. æ¢¯åº¦ä¸‹é™ç®—æ³•çš„å®é™…åº”ç”¨**'
- en: The larger the absolute value of the slope, the further we can step, and/or
    we can keep taking steps towards the steepest descent, namely the local minimum.
    As we approach the lowest/minimum point, the slope diminishes so one can take
    smaller steps until reaching a flat surface where the slope is equal to zero (0),
    *Æ’â€™(x)* = 0, this is lowest value of Î²i as pointed by the red arrow in the graph
    below. This is where the local minimum of the curved line is, and optimum values
    of the functionâ€™s parameters are derived.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: æ–œç‡çš„ç»å¯¹å€¼è¶Šå¤§ï¼Œæˆ‘ä»¬å¯ä»¥è¿ˆå‡ºçš„æ­¥ä¼å°±è¶Šå¤§ï¼Œæˆ–è€…æˆ‘ä»¬å¯ä»¥ç»§ç»­æœç€æœ€é™¡ä¸‹é™çš„æ–¹å‘è¿ˆæ­¥ï¼Œå³å±€éƒ¨æœ€å°å€¼ã€‚éšç€æˆ‘ä»¬æ¥è¿‘æœ€ä½/æœ€å°ç‚¹ï¼Œæ–œç‡å‡å°ï¼Œå› æ­¤å¯ä»¥è¿ˆå‡ºæ›´å°çš„æ­¥ä¼ï¼Œç›´åˆ°åˆ°è¾¾æ–œç‡ä¸ºé›¶ï¼ˆ0ï¼‰çš„å¹³å¦è¡¨é¢ï¼Œå³*Æ’â€™(x)*
    = 0ï¼Œè¿™æ˜¯ä¸‹å›¾ä¸­çº¢è‰²ç®­å¤´æŒ‡å‘çš„Î²içš„æœ€ä½å€¼ã€‚è¿™å°±æ˜¯æ›²çº¿çš„å±€éƒ¨æœ€å°å€¼ï¼Œå‡½æ•°å‚æ•°çš„æœ€ä¼˜å€¼ä¹Ÿåœ¨æ­¤å¤„å¾—å‡ºã€‚
- en: '![](../Images/45f0e3a2e12d36989c395262e5df3fda.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/45f0e3a2e12d36989c395262e5df3fda.png)'
- en: Graph drawn by the author
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: ä½œè€…ç»˜åˆ¶çš„å›¾è¡¨
- en: Thus, if a function is *strictly* convex (concave), there will only be one critical
    point. Now, there is also the case where there are multiple local minima. In this
    case, the search is for the single lowest value the function can achieve. This
    is known as Global Minimum.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼Œå¦‚æœä¸€ä¸ªå‡½æ•°æ˜¯*ä¸¥æ ¼*å‡¸çš„ï¼ˆå‡¹çš„ï¼‰ï¼Œåˆ™åªæœ‰ä¸€ä¸ªä¸´ç•Œç‚¹ã€‚ç°åœ¨ï¼Œä¹Ÿæœ‰å¯èƒ½å­˜åœ¨å¤šä¸ªå±€éƒ¨æœ€å°å€¼çš„æƒ…å†µã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæœç´¢çš„æ˜¯å‡½æ•°èƒ½å¤Ÿè¾¾åˆ°çš„æœ€ä½å€¼ï¼Œè¿™è¢«ç§°ä¸ºå…¨å±€æœ€å°å€¼ã€‚
- en: '![](../Images/83c582728280d36bd59544b797ffe5a8.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/83c582728280d36bd59544b797ffe5a8.png)'
- en: Graph drawn by the author
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: ä½œè€…ç»˜åˆ¶çš„å›¾è¡¨
- en: 'The following two key questions arise:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥ä¸‹ä¸¤ä¸ªå…³é”®é—®é¢˜å‡ºç°ï¼š
- en: 1) In which direction to step?
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 1) åº”è¯¥æœå“ªä¸ªæ–¹å‘è¿ˆæ­¥ï¼Ÿ
- en: 2) How big the steps should be?
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 2) æ­¥é•¿åº”è¯¥æœ‰å¤šå¤§ï¼Ÿ
- en: Let us recap what have we have said so far. Gradient descent is an algorithm,
    that while in the training phase of a model, iteratively adjusts hence optimizes
    the values of the functionâ€™s parameters by taking the partial derivative of the
    function with respect to each of its inputs at every step it takes towards the
    steepest descent, defined as the local minimum. If the derivative is positive,
    the function is increasing. Thus, steps should be taken opposite direction. The
    gradient indicates in which direction the step should be taken. If gradient is
    large, namely large slope absolute value, larger steps should be taken towards
    the local minimum. In fact, gradient descent takes increasingly smaller steps
    on the direction of the local minima within each iteration as shown by the blue
    arrows in the graph above.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬å›é¡¾ä¸€ä¸‹åˆ°ç›®å‰ä¸ºæ­¢çš„å†…å®¹ã€‚æ¢¯åº¦ä¸‹é™æ˜¯ä¸€ç§ç®—æ³•ï¼Œåœ¨æ¨¡å‹çš„è®­ç»ƒé˜¶æ®µï¼Œé€šè¿‡åœ¨æ¯ä¸€æ­¥å‘æœ€é™¡ä¸‹é™çš„æ–¹å‘ï¼ˆå³å±€éƒ¨æœ€å°å€¼ï¼‰å‰è¿›æ—¶ï¼Œå¯¹å‡½æ•°çš„æ¯ä¸ªè¾“å…¥å–å…¶åå¯¼æ•°ï¼Œä»è€Œè¿­ä»£åœ°è°ƒæ•´å’Œä¼˜åŒ–å‡½æ•°å‚æ•°çš„å€¼ã€‚å¦‚æœå¯¼æ•°ä¸ºæ­£ï¼Œå‡½æ•°å€¼åœ¨å¢åŠ ã€‚å› æ­¤ï¼Œåº”è¯¥æœç›¸åæ–¹å‘è¿ˆæ­¥ã€‚æ¢¯åº¦æŒ‡ç¤ºäº†åº”è¯¥è¿ˆæ­¥çš„æ–¹å‘ã€‚å¦‚æœæ¢¯åº¦å¾ˆå¤§ï¼Œå³æ–œç‡çš„ç»å¯¹å€¼å¾ˆå¤§ï¼Œåˆ™åº”è¯¥æœç€å±€éƒ¨æœ€å°å€¼è¿ˆè¾ƒå¤§çš„æ­¥ä¼ã€‚å®é™…ä¸Šï¼Œæ¢¯åº¦ä¸‹é™åœ¨æ¯æ¬¡è¿­ä»£ä¸­æœç€å±€éƒ¨æœ€å°å€¼æ–¹å‘é‡‡å–è¶Šæ¥è¶Šå°çš„æ­¥ä¼ï¼Œå¦‚ä¸Šå›¾ä¸­çš„è“è‰²ç®­å¤´æ‰€ç¤ºã€‚
- en: 'How big the step to take relates to the learning rate. This is how fast the
    algorithm learns/moves towards the steepest descent. At the highest gradient,
    large absolute value of the slope, the fastest the algorithm learns. As it approaches
    the local minimum, smaller the steps to take. Thus, learning rate value as any
    hyperparameter is set after trying different values so that the cost function
    decreases across iterations. If too big, the local minimum can be missed. A small
    learning rate might lead to small updates to the weights causing the model not
    to improve significantly. If too small, it will take time to reach convergence.
    Convergence is reached when the cost function does not longer decrease. Thus,
    the cost function is the indicator of the algorithm performance. In a multivariate
    function world, this is denoted:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¥é•¿çš„å¤§å°ä¸å­¦ä¹ ç‡æœ‰å…³ã€‚è¿™å†³å®šäº†ç®—æ³•å­¦ä¹ /ç§»åŠ¨åˆ°æœ€é™¡ä¸‹é™æ–¹å‘çš„é€Ÿåº¦ã€‚åœ¨æœ€é«˜æ¢¯åº¦ä¸‹ï¼Œå³æ–œç‡çš„ç»å¯¹å€¼æœ€å¤§æ—¶ï¼Œç®—æ³•å­¦ä¹ æœ€å¿«ã€‚å½“æ¥è¿‘å±€éƒ¨æœ€å°å€¼æ—¶ï¼Œæ­¥é•¿ä¼šå˜å°ã€‚å› æ­¤ï¼Œå­¦ä¹ ç‡ä½œä¸ºè¶…å‚æ•°åœ¨å°è¯•ä¸åŒå€¼åè®¾ç½®ï¼Œä»¥ä¾¿æˆæœ¬å‡½æ•°åœ¨è¿­ä»£ä¸­å‡å°‘ã€‚å¦‚æœå­¦ä¹ ç‡è¿‡å¤§ï¼Œå¯èƒ½ä¼šé”™è¿‡å±€éƒ¨æœ€å°å€¼ã€‚å­¦ä¹ ç‡è¿‡å°å¯èƒ½å¯¼è‡´æƒé‡æ›´æ–°è¾ƒå°ï¼Œä½¿æ¨¡å‹æ²¡æœ‰æ˜¾è‘—æ”¹å–„ã€‚å¦‚æœå­¦ä¹ ç‡è¿‡å°ï¼Œå¯èƒ½éœ€è¦æ—¶é—´æ‰èƒ½æ”¶æ•›ã€‚æ”¶æ•›æ˜¯æŒ‡æˆæœ¬å‡½æ•°ä¸å†å‡å°‘ã€‚å› æ­¤ï¼Œæˆæœ¬å‡½æ•°æ˜¯ç®—æ³•æ€§èƒ½çš„æŒ‡æ ‡ã€‚åœ¨å¤šå˜é‡å‡½æ•°çš„ä¸–ç•Œä¸­ï¼Œè¿™è¡¨ç¤ºä¸ºï¼š
- en: '![](../Images/7967ef48d2b7ebc592bb70eace9eb39e.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7967ef48d2b7ebc592bb70eace9eb39e.png)'
- en: where,
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­ï¼Œ
- en: '*df/d*Î² partial derivative of the cost function with respect to the parameterÎ²'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '*df/d*Î² è¡¨ç¤ºæˆæœ¬å‡½æ•°å¯¹å‚æ•° Î² çš„åå¯¼æ•°ã€‚'
- en: '*m* number of data points'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '*m* æ•°æ®ç‚¹æ•°é‡ã€‚'
- en: '*yi* actual values of the dependent/target variable for the i-th data point'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '*yi* æ˜¯ç¬¬ i ä¸ªæ•°æ®ç‚¹çš„å®é™…ä¾èµ–/ç›®æ ‡å˜é‡å€¼ã€‚'
- en: '*yÌ‚i* predicted values by the model of the dependent/target variable for the
    i-th data point'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '*yÌ‚i* æ˜¯æ¨¡å‹é¢„æµ‹çš„ç¬¬ i ä¸ªæ•°æ®ç‚¹çš„ä¾èµ–/ç›®æ ‡å˜é‡å€¼ã€‚'
- en: '*xi* represents the i-th input associated with the data point.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '*xi* è¡¨ç¤ºä¸æ•°æ®ç‚¹ç›¸å…³çš„ç¬¬ i ä¸ªè¾“å…¥ã€‚'
- en: '![](../Images/650f42910dde7c968ff22971e7f731d7.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/650f42910dde7c968ff22971e7f731d7.png)'
- en: where,
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­ï¼Œ
- en: '*â–½f* represents the gradient vector of the function *f(x)* with respect to
    the parameters Î²'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '*â–½f* è¡¨ç¤ºå‡½æ•° *f(x)* å¯¹å‚æ•° Î² çš„æ¢¯åº¦å‘é‡ã€‚'
- en: '*df/d*Î²k represents the partial derivative of the function *f(x)* with respect
    to the k-th parameter Î²'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '*df/d*Î²k è¡¨ç¤ºå‡½æ•° *f(x)* å¯¹ç¬¬ k ä¸ªå‚æ•° Î² çš„åå¯¼æ•°ã€‚'
- en: '![](../Images/a47daebf7247b61566e975b9730e8eef.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a47daebf7247b61566e975b9730e8eef.png)'
- en: were,
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­ï¼Œ
- en: New Î² represents the current value of the i-th parameter Î²
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: æ–°çš„ Î² è¡¨ç¤ºç¬¬ i ä¸ªå‚æ•° Î² çš„å½“å‰å€¼ã€‚
- en: Old Î² represents the updated value of the i-th parameter Î²
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: æ—§çš„ Î² è¡¨ç¤ºç¬¬ i ä¸ªå‚æ•° Î² çš„æ›´æ–°å€¼ã€‚
- en: '*n* is the *learning rate: length of the step to take!*'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '*n* æ˜¯ *å­¦ä¹ ç‡ï¼šé‡‡å–æ­¥é•¿çš„é•¿åº¦ï¼*'
- en: '*â–½f* is the gradient vector pointing in the direction of the steepest descent
    of the function *f(x)* with respect to changes in the parameters Î² to minimize
    *f(x)*'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '*â–½f* æ˜¯æŒ‡å‘å‡½æ•° *f(x)* åœ¨å‚æ•° Î² å˜åŒ–æ–¹å‘ä¸Šçš„æœ€é™¡ä¸‹é™æ–¹å‘çš„æ¢¯åº¦å‘é‡ï¼Œä»¥æœ€å°åŒ– *f(x)*ã€‚'
- en: '**4\. LIMITATIONS OF GRADIENT DESCENT**'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '**4\. æ¢¯åº¦ä¸‹é™çš„å±€é™æ€§**'
- en: One of the limitations of gradient descent is related to one of the criteria
    mentioned above where the function must be differentiable at every point of its
    domain. When this is not the case and the algorithm finds a point that is undefined,
    (i.e., non-continuous) then it fails.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: æ¢¯åº¦ä¸‹é™çš„ä¸€ä¸ªå±€é™æ€§ä¸ä¸Šè¿°æåˆ°çš„æ ‡å‡†ä¹‹ä¸€æœ‰å…³ï¼Œå³å‡½æ•°å¿…é¡»åœ¨å…¶å®šä¹‰åŸŸçš„æ¯ä¸€ç‚¹éƒ½å¯å¾®åˆ†ã€‚å½“æƒ…å†µä¸æ˜¯è¿™æ ·ï¼Œç®—æ³•æ‰¾åˆ°ä¸€ä¸ªæœªå®šä¹‰çš„ç‚¹ï¼ˆå³ä¸è¿ç»­çš„ç‚¹ï¼‰æ—¶ï¼Œç®—æ³•ä¼šå¤±è´¥ã€‚
- en: Another limitation is related to the size of the steps, namely the learning
    rate (*n*), taken towards the steepest descent. If too large it is likely to miss
    the local minimum or even not reach converge at all. If too small, it will take
    much longer to converge. If the number of inputs is large, this becomes even more
    problematic.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: å¦ä¸€ä¸ªå±€é™æ€§ä¸æ­¥é•¿çš„å¤§å°æœ‰å…³ï¼Œå³å­¦ä¹ ç‡ (*n*)ï¼Œå³æœç€æœ€é™¡ä¸‹é™æ–¹å‘é‡‡å–çš„æ­¥ä¼ã€‚å¦‚æœæ­¥é•¿è¿‡å¤§ï¼Œå¯èƒ½ä¼šé”™è¿‡å±€éƒ¨æœ€å°å€¼ï¼Œç”šè‡³å¯èƒ½æ— æ³•æ”¶æ•›ã€‚å¦‚æœæ­¥é•¿è¿‡å°ï¼Œåˆ™éœ€è¦æ›´é•¿çš„æ—¶é—´æ‰èƒ½æ”¶æ•›ã€‚å¦‚æœè¾“å…¥æ•°é‡å¾ˆå¤§ï¼Œè¿™ç§æƒ…å†µä¼šæ›´åŠ ä¸¥é‡ã€‚
- en: Finally, gradient descent might never find the global minimum. The algorithm
    is not able to distinguish between a local and global minimum. As it descent in
    search of the local minimum, once it converges it will then stop. The local minimum
    will corner the algorithm in the local minimum own valley preventing the step
    to be large enough to exit.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€ç»ˆï¼Œæ¢¯åº¦ä¸‹é™å¯èƒ½æ°¸è¿œæ— æ³•æ‰¾åˆ°å…¨å±€æœ€å°å€¼ã€‚ç®—æ³•æ— æ³•åŒºåˆ†å±€éƒ¨æœ€å°å€¼å’Œå…¨å±€æœ€å°å€¼ã€‚å½“ç®—æ³•åœ¨å¯»æ‰¾å±€éƒ¨æœ€å°å€¼æ—¶ï¼Œä¸€æ—¦æ”¶æ•›å°±ä¼šåœæ­¢ã€‚å±€éƒ¨æœ€å°å€¼ä¼šæŠŠç®—æ³•å›°åœ¨å±€éƒ¨æœ€å°å€¼æ‰€åœ¨çš„è°·åº•ï¼Œé˜»æ­¢æ­¥ä¼è¶³å¤Ÿå¤§ä»¥ä¾¿é€€å‡ºã€‚
- en: '**5\. CONCLUSIONS**'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '**5\. ç»“è®º**'
- en: 'In summary, gradient descent is:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: æ€»ç»“æ¥è¯´ï¼Œæ¢¯åº¦ä¸‹é™æ˜¯ï¼š
- en: 1) An iterative, first-order optimization algorithm type
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 1) ä¸€ç§è¿­ä»£çš„ã€ä¸€é˜¶ä¼˜åŒ–ç®—æ³•ç±»å‹ã€‚
- en: 2) Within each iteration, the parameters of the differentiable function are
    updated, and the cost function is minimized.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 2) åœ¨æ¯æ¬¡è¿­ä»£ä¸­ï¼Œå¯¹å¯å¾®å‡½æ•°çš„å‚æ•°è¿›è¡Œæ›´æ–°ï¼Œå¹¶æœ€å°åŒ–æˆæœ¬å‡½æ•°ã€‚
- en: 3) Thus, convergence is reached on the local minimum.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 3) å› æ­¤ï¼Œæ”¶æ•›äºå±€éƒ¨æœ€å°å€¼ã€‚
- en: Based on the limitations of the gradient descent, there are motivations to explore
    different and more advanced type of gradient descent methods or even other types
    of algorithms in the realm of optimization such as the second-order type. This,
    however, will go out of the scope of this article hence I will leave it as a topic
    for my next article ğŸ˜Š
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: åŸºäºæ¢¯åº¦ä¸‹é™æ³•çš„å±€é™æ€§ï¼Œæœ‰åŠ¨æœºæ¢ç´¢ä¸åŒçš„ã€æ›´å…ˆè¿›çš„æ¢¯åº¦ä¸‹é™æ–¹æ³•ï¼Œç”šè‡³å…¶ä»–ç±»å‹çš„ä¼˜åŒ–ç®—æ³•ï¼Œå¦‚äºŒé˜¶æ–¹æ³•ã€‚ç„¶è€Œï¼Œè¿™è¶…å‡ºäº†æœ¬æ–‡çš„èŒƒå›´ï¼Œå› æ­¤æˆ‘ä¼šæŠŠå®ƒç•™ä½œæˆ‘ä¸‹ä¸€ç¯‡æ–‡ç« çš„ä¸»é¢˜
    ğŸ˜Š
- en: Thanks for reading!
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: æ„Ÿè°¢é˜…è¯»ï¼
- en: Please add any comment that you feel enhances the knowledge in the topic!
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: è¯·æ·»åŠ ä»»ä½•ä½ è®¤ä¸ºèƒ½å¢å¼ºè¯¥ä¸»é¢˜çŸ¥è¯†çš„è¯„è®ºï¼
