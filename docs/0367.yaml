- en: How to send tabular time series data to Apache Kafka with Python and Pandas
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/how-to-send-tabular-time-series-data-to-apache-kafka-with-python-and-pandas-39e2055373c3?source=collection_archive---------19-----------------------#2023-01-24](https://towardsdatascience.com/how-to-send-tabular-time-series-data-to-apache-kafka-with-python-and-pandas-39e2055373c3?source=collection_archive---------19-----------------------#2023-01-24)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Learn now to produce and consume data in Kafka using a sample log of online
    retail transactions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@tomasatquix?source=post_page-----39e2055373c3--------------------------------)[![Tomáš
    Neubauer](../Images/5eb14b73cfe100ef9a43148db6abd3a9.png)](https://medium.com/@tomasatquix?source=post_page-----39e2055373c3--------------------------------)[](https://towardsdatascience.com/?source=post_page-----39e2055373c3--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----39e2055373c3--------------------------------)
    [Tomáš Neubauer](https://medium.com/@tomasatquix?source=post_page-----39e2055373c3--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fd620afda25db&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-send-tabular-time-series-data-to-apache-kafka-with-python-and-pandas-39e2055373c3&user=Tom%C3%A1%C5%A1+Neubauer&userId=d620afda25db&source=post_page-d620afda25db----39e2055373c3---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----39e2055373c3--------------------------------)
    ·15 min read·Jan 24, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F39e2055373c3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-send-tabular-time-series-data-to-apache-kafka-with-python-and-pandas-39e2055373c3&user=Tom%C3%A1%C5%A1+Neubauer&userId=d620afda25db&source=-----39e2055373c3---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F39e2055373c3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-send-tabular-time-series-data-to-apache-kafka-with-python-and-pandas-39e2055373c3&source=-----39e2055373c3---------------------bookmark_footer-----------)![](../Images/33c2b7574c7e450bfa10a9e6192ff1ce.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Photo by [Tech Daily](https://unsplash.com/@techdailyca?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    on [Unsplash](https://unsplash.com/photos/ztYmIQecyH4?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
  prefs: []
  type: TYPE_NORMAL
- en: Time-series data comes in all shapes and sizes and it’s often produced in high
    frequencies in the form of sensor data and transaction logs. It’s also produced
    in huge volumes where the records are separated by milliseconds rather than hours
    or days.
  prefs: []
  type: TYPE_NORMAL
- en: But what kind of system that can handle such a constant stream of data? An older
    approach would be to dump the raw data in Data Lake and process it in huge batches
    with a long-running process. Nowadays, many companies prefer to process the raw
    data in real-time and write the aggregated results to a database.
  prefs: []
  type: TYPE_NORMAL
- en: For example, an online retailer could continuously aggregate transactional data
    by product and day rather than running expensive database queries on demand. But
    how would this work in practice? Let’s find out!
  prefs: []
  type: TYPE_NORMAL
- en: In this tutorial, we’ll use Python and Apache Kafka to process large volumes
    of time series data from that comes from a real online retailer.
  prefs: []
  type: TYPE_NORMAL
- en: What you’ll learn
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'By the end of this tutorial you’ll understand:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Why startups and online businesses use Apache Kafka**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**The unique qualities of time series data and how it works with Kafka**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**How to install and run Kafka on your local machine**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**How to send time series data to Kafka in batches using Python and the Pandas
    library**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What you should know already
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This article is intended for data scientists and engineers, so I’m assuming
    the following things about you:'
  prefs: []
  type: TYPE_NORMAL
- en: You know your way around Python and have used the Pandas library or at least
    know what it’s used for in data science.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You’ve heard of Apache Kafka and know roughly what it’s for.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: But don’t worry if you don’t meet these criteria. This tutorial is simple enough
    to follow along, and we’ll briefly explain these technologies. Just be aware that
    it’s not intended for absolute beginners.
  prefs: []
  type: TYPE_NORMAL
- en: Why use Apache Kafka for time series data?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Apache Kafka is an industry standard for processing data in real-time. You can
    use it to process a vast firehose of data (as long as you have the computing resources).
  prefs: []
  type: TYPE_NORMAL
- en: For example, some Formula One racing teams use Kafka in combination with Kubernetes
    to process huge quantities of sensor data coming in every millisecond. This data
    is analyzed in real-time to predict outcomes in races and to provide teams with
    insights for their Formula One pilots.
  prefs: []
  type: TYPE_NORMAL
- en: 'Aside from it’s ability to process data streams, there are other key reasons
    why an online business might choose to use Apache Kafka:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Scalability**: Kafka is designed to handle high volumes of data with low
    latency, making it well-suited for startups that expect rapid growth and need
    a solution that can scale with them.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Decoupling**: Ever heard of [event-driven architectures](https://www.tibco.com/reference-center/what-is-event-driven-architecture)?
    Kafka is one of the tools often touted to facilitate this pattern. It allows for
    decoupling of systems, meaning that different parts of the architecture can be
    developed and deployed independently. This can be particularly useful for startups
    that are iterating rapidly and need to be able to make changes to their systems
    without impacting other parts of the architecture.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Durability**: Kafka stores all published messages for a configurable amount
    of time, which means that it can serve as a durable log of all the data that has
    passed through the system. This can be useful for startups that need to maintain
    a record of all their data for auditing or compliance purposes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Wide adoption**: Apache Kafka is widely adopted in the industry, which means
    that it has a large user base and a strong ecosystem of tools and resources available.
    This can be very helpful for startups that want to leverage the collective experience
    of the Kafka community.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: That last point is especially critical, since Apache Kafka has a notoriously
    steep learning curve. Without a vast pool of tutorials and demos, many beginners
    would struggle to get it up and running.
  prefs: []
  type: TYPE_NORMAL
- en: Why use Python with Apache Kafka?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Because Python is the most popular language in the Data and ML communities.
    These communities could benefit a lot from Apache Kafka, but there aren’t yet
    enough Kafka tutorials that appeal to their skillset.
  prefs: []
  type: TYPE_NORMAL
- en: If you’re part of a data team, you’re more likely to know Python and Pandas
    than you are Java. Yet most older Kafka tutorials are written for software engineers
    who write in Java. This is because software engineers have traditionally built
    the components that interact with Kafka (and Kafka itself is written in Java/Scala).
  prefs: []
  type: TYPE_NORMAL
- en: Kafka’s user base is changing however. The responsibilities of software and
    data teams are beginning to converge. This has been driven by the growing importance
    of data in modern organizations and the increasing complexity of data management
    and processing tasks. Today, data professionals also contribute to software components
    that interact with Kafka — but they still encounter a lot of friction because
    they’re typically not familiar with technologies from the Java ecosystem. That’s
    why we’re using Python and Pandas in these examples.
  prefs: []
  type: TYPE_NORMAL
- en: Ok, that’s it for the preamble — lets get into the tutorial.
  prefs: []
  type: TYPE_NORMAL
- en: Prerequisites for this tutorial
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The first thing you need is time — about 30 minutes (once you have the required
    software installed).
  prefs: []
  type: TYPE_NORMAL
- en: Speaking of which, make sure that you have the following software installed
    before you proceed any further.
  prefs: []
  type: TYPE_NORMAL
- en: SOFTWARE
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Windows or a** Unix**-based operating system** We’ll be providing commands
    for both Windows and Unix-based operating systems such as macOS and Ubuntu.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[**Python 3.0+**](https://www.python.org/about/gettingstarted/) **and required
    libraries'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: —** You can download the installer from [the Python downloads page](https://www.python.org/downloads/).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: — Optionally, you might want to create a [virtual environment](https://docs.python.org/3/library/venv.html)
    to use for this tutorial so that you avoid conflicting dependencies.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Required Libraries'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '—** [Pandas](https://pypi.org/project/pandas/): Install with `pip3 install
    pandas`'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**—** [Kafka-python](https://pypi.org/project/kafka-python/): Install with
    `pip3 install kafka-python`'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Java 8+** This is a prerequisite for Apache Kafka. To install it, choose
    one of the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**—Ubuntu**: Install with `sudo apt install default-jre`'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**— macOS**: Install with `brew install java`'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '* *If you’re using a Macbook Pro with an M1 chip, you might need to install
    Java using the steps outlined in this guide: “*[*How to setup Java on Apple Mac
    M1 Pro (Dev.to)*](https://dev.to/docker/how-to-setup-java-on-macos-124-monterey-3l10)*”
    — it should take about 2 minutes.*'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**— Windows**: Download and run the [Java executable binary for Windows](https://www.java.com/en/download/)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[**Apache Kafka**](https://kafka.apache.org/documentation/#quickstart)You can
    download the Apache Kafka binary from the [Apache Kafka Download page](https://kafka.apache.org/downloads).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Extract the contents of the file to a convenient location. For example, you
    can create a project folder for this tutorial and extract it there.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Major steps
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before we get into the details, let’s go over the major steps that we’ll be
    covering.
  prefs: []
  type: TYPE_NORMAL
- en: '**Setting up Kafka:** We’ll first get to grips with Kafka’s command line tools,
    and use them to:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: — Start the Zookeeper and Kafka server
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: — Create a topic (where we’ll be sending our data)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Analyzing the Data:** Use Python and Pandas to:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: — Take a closer look at the structure of the online retail data set.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: — Read it into a DataFrame and understand the different data types.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: We’ll also look at the benefits of using data frames and time series data with
    Kafka.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Sending Data to a Kafka with a Producer:** Use the kafka-python library to:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: — Read the csv into a Data Frame and initialize the Kafka python producer.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: — Iterate through the rows and send them in batches to Kafka
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Read Data from Kafka with a Consumer:** Again, we’ll use the kafka-python
    library to:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: — Read the messages from our Kafka topic
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: — Take the batched message and convert it back into a data frame.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: — Perform some simple aggregation on it.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'You’ll find the code in the ‘*tabular-timeseries-kafka*’ subfolder. If you
    want to jump straight ahead to the code, you can clone our tutorials repo in GitHub
    with the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: git clone [https://github.com/quixai/tuto...](https://github.com/quixai/tutorial-code.git)
  prefs: []
  type: TYPE_NORMAL
- en: Setting up Apache Kafka
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you haven’t done so already, download Apache Kafka from the [Apache Kafka
    Download page](https://kafka.apache.org/downloads) (for example, “*kafka_2.12–3.3.1.tgz*”)
    and extract the contents of the file to a convenient location.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before you can do anything with Kafka, you have to start the core services.
    Each of these services needs to run in a separate terminal window. These two services
    are:'
  prefs: []
  type: TYPE_NORMAL
- en: The **zookeeper service** which is responsible for managing the coordination
    between all the different services, brokers, and clients that make up the Kafka
    cluster.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The **Kafka server service** which runs the core functionality of Apache Kafka,
    such as the message broker.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: When following these instructions, start each terminal window in the directory
    where you extracted Kafka (for example, “*C:\Users\demo\Downloads\kafka_2.13–3.3.1\*
    ”)
  prefs: []
  type: TYPE_NORMAL
- en: '**To start the core Kafka services, follow these steps:**'
  prefs: []
  type: TYPE_NORMAL
- en: '**1.** In the Kafka directory, open a terminal window and start the zookeeper
    service with the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Linux / macOS**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '**Windows**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: You should see a bunch of log messages indicating that zookeeper started successfully.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Leave the window open.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**2.** Open a second terminal window and start the Kafka server with the following
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Linux / macOS**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '**Windows**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Again, you should see similar log messages indicating the server started successfully.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Leave this window open too.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**3.** Next, you’ll need to create a topic called “transactions” to store the
    data.'
  prefs: []
  type: TYPE_NORMAL
- en: If the term “topic” is unfamiliar, think of it as a process that is constantly
    updating a log file. There are many ways to [explain what a topic does](https://dattell.com/data-architecture-blog/what-is-a-kafka-topic/)
    in detail, but for now, let’s just say that it’s a log file for events related
    to a specific type of data (such as, incoming transactions) including the data
    itself.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**To create a topic in Kafka:**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Open a third terminal window and enter the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Linux / macOS**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '**Windows**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: You should see the confirmation message “*Created topic transactions*”
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: That’s it for Kafka. Hopefully you’ve managed to set it up without any hassle.
    If you ran into any issues, this [troubleshooting guide](https://www.codeproject.com/Articles/5276053/Troubleshoot-Kafka-Setup-on-Windows)
    might help. Now, let’s move in to the exciting part.
  prefs: []
  type: TYPE_NORMAL
- en: Analyzing the data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For this exercise, we’ll be using a transnational data set which contains all
    the transactions occurring between 01/12/2010 and 09/12/2011 for a UK-based online
    retail store. It was obtained from the machine learning repository hosted by the
    University of California. You can find more details about the dataset on its [dedicated
    page](https://archive.ics.uci.edu/ml/datasets/online+retail) in the UCI Machine
    Learning Repository website*.
  prefs: []
  type: TYPE_NORMAL
- en: 'To prepare, follow these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Create a folder for this project (e.g. “*tabular-timeseries-kafka*”).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Download the zipped CSV file from [this storage location](https://quixdocsdev.blob.core.windows.net/docsartifacts/online_retail_II.zip)
    and extract it into the project folder
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We’ll be providing instructions as Python commands, but you can also use an
    IDE such as [Pycharm Community Edition](https://www.jetbrains.com/pycharm/download/).
  prefs: []
  type: TYPE_NORMAL
- en: 'This first task is to inspect the data in the file and look at how Pandas interprets
    the data by default:'
  prefs: []
  type: TYPE_NORMAL
- en: '**To inspect the data follow, these steps:**'
  prefs: []
  type: TYPE_NORMAL
- en: '**1.** Open a terminal window in your project directory and enter *python*
    start the Python console.'
  prefs: []
  type: TYPE_NORMAL
- en: '**2.** Read the CSV into a DataFrame by entering the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '**3.** Examine the output of `df.info`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Note that the `InvoiceDate` column has been read in as an object data type.
  prefs: []
  type: TYPE_NORMAL
- en: In Pandas, the object data type is used to represent string values or data that
    cannot be easily cast to a numerical data type.
  prefs: []
  type: TYPE_NORMAL
- en: To make this a true time series dataset, we need to have the time that the record
    was created (in this case `InvoiceDate`) in a proper DateTime format. We’ll explain
    why in a moment.
  prefs: []
  type: TYPE_NORMAL
- en: '**To convert the InvoiceDate column into a date format:**'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the Python console, enter the following commands:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: This conversion allows you to take advantage of pandas’ extensive [time series
    functionality](https://pandas.pydata.org/docs/user_guide/timeseries.html#timeseries-overview).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For example, setting it as a DatetimeIndex can help you optimize your time series
    data with precomputed and cached date ranges; fast and easy selection of dates,
    date ranges, and the values attached to them; and quick summarizations using time
    blocks (“year”, “month”).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What Kafka can do with time series data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'So we know that Pandas has many time series-related features, but what about
    Kafka? Indeed, Kafka also provides a number of features that can be used to optimize
    the processing of time series data. These include:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Compression**: Kafka supports multiple compression algorithms that can use
    time data to reduce the size of data streams and improve the efficiency of data
    processing.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data partitioning**: Kafka allows you to partition data streams by key and
    time, which can be useful for distributing data processing across multiple consumer
    instances.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Custom serialization**: Kafka provides a pluggable serialization framework
    that allows you to specify custom serializers and deserializers for your data,
    which can be useful for optimizing the performance of data processing.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All of these features depend in part, on having time in the correct format.
  prefs: []
  type: TYPE_NORMAL
- en: Note that we’re not going to use any of these features in this basic tutorial.
    But if you intend to use Kafka in production, it’s important to understand the
    role that time data can play when interacting with Kafka.
  prefs: []
  type: TYPE_NORMAL
- en: The advantages of using DataFrames with Kafka
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Pandas DataFrames are particularly useful when storing data in a tabular format,
    as each feature of your data set can be manipulated as a one-dimensional shape
    or Series while still collectively forming a multidimensional data set. DataFrames
    also come with a number of handy built-in functions that allow you to manipulate
    the data and process data very quickly. This is invaluable when working with Kafka.
  prefs: []
  type: TYPE_NORMAL
- en: For example, in the next major step, we’ll be sending the data in batches. We’re
    batching the records to replicate a production scenario where you might receive
    hundreds of records a second.
  prefs: []
  type: TYPE_NORMAL
- en: If you sent a message to Kafka for each individual record, you could risk bottlenecks
    or system failures. That’s why you send small batches of data in each message
    at a lower frequency (rather than tons of small messages at a high frequency).
  prefs: []
  type: TYPE_NORMAL
- en: As you’ll see, this process is very easy when we use the Pandas chunking feature.
    This is one of the many Kafka-friendly features that come with Pandas DataFrames.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a Kafka Producer to send the data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now, lets put what we’ve learned into a proper Python file and start sending
    data to Kafka. We’ll be using the [*kafka-python*](https://github.com/dpkp/kafka-python)
    library which is one of several libraries intended to connect python applications
    to Kafka (another is the [*confluent-kafka-python*](https://github.com/confluentinc/confluent-kafka-python)
    library).
  prefs: []
  type: TYPE_NORMAL
- en: '**To create a producer, follow these steps:**'
  prefs: []
  type: TYPE_NORMAL
- en: 'In your project directory, create a file called `producer.py` and insert the
    following code block:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: This imports all the required libraries and initializes the Kafka producer,
    telling it to connect to the server that you should (hopefully) still have running
    on your computer under ‘*localhost*’
  prefs: []
  type: TYPE_NORMAL
- en: '**2.** Next, add the ‘`for`’ loop that will iterate through the file and send
    the data in batches:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The code includes explanatory comments, but essentially it’s doing the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Reading the CSV in batches of 10 rows
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Serializing each batch into JSON and encoding the JSON as a Byte array
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sending that JSON as a message to the Kafka topic “transactions”
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can review the complete file in our Github repository. [https://github.com/quixai/tutorials/timeseries/producer.py](https://github.com/quixai/tutorials/timeseries/producer.py)
  prefs: []
  type: TYPE_NORMAL
- en: '**3.** Save your file and run your code with the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: python producer.py
  prefs: []
  type: TYPE_NORMAL
- en: 'In your terminal window, you should start seeing confirmations like this:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: “*Sent record to topic at time 2022–12–28 13:23:52.125664*” for each message
    sent.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: If you get an error about missing modules, make sure that you have the *kafka-python*
    library installed (`pip3 install kafka-python`).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Once the data is in the Kafka topic, it can be read by multiple consumers and
    extracted for more downstream processes.
  prefs: []
  type: TYPE_NORMAL
- en: Lets create a consumer to read those messages back in and before a basic operation
    on them.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a Kafka Consumer to read the data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The process for creating a Kafka consumer is very similar to the previous step.
    In this case we’ll be, reading each batched message, converting it back into a
    DataFrame.
  prefs: []
  type: TYPE_NORMAL
- en: We’ll pretend that this is consumer is for some kind of inventory analysis pipeline
    that only wants to know the total sales for each stock item. Thus, in the data,
    we’ll only look at the StockCode, the Quantity sold and the Price. We’ll calculate
    the total value of the sale for each record so that we can aggregate sales by
    StockCode.
  prefs: []
  type: TYPE_NORMAL
- en: In your project directory, create a file called `consumer.py` and insert the
    following code block to initialize the consumer.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: We’re initializing the consumer with a few more options than we did for the
    producer.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: First, we’re telling it what topic to read from, then where the Kafka server
    is running, and thirdly, we’re giving it a lambda function to use for deserializing
    the message values back into a Python dictionary.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 2\. Next, add the ‘for’ loop that will iterate through the messages and perform
    some processing on them.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: As you can see in the code comments, we are performing a simple calculation
    that outputs a summary of the revenue by `StockCode` for each message batch.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Of course, the end goal would be to keep a running total of the overall revenue
    by `StockCode`. This would require some further processing that would write the
    aggregations into a database which could in turn, power some kind of Dashboard.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'However, for the purposes of this tutorial, this is a good place to stop. If
    everything works correctly, you should see the aggregated result logged for each
    message. It should look something like this:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: If you see the logged aggregations, well done! Congratulations for making it
    to the end.
  prefs: []
  type: TYPE_NORMAL
- en: Wrapping up
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This process might seem unremarkable if you’re just doing it on your local
    machine, but it becomes more interesting when you consider how this pattern could
    scale. Let’s take a moment to recap what you did — you accomplished two key tasks:'
  prefs: []
  type: TYPE_NORMAL
- en: 1— You produced a high-frequency stream of messages and streamed them into a
    Kafka topic
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this case, you were “replaying” a historical log of customer transactions
    and artificially staggering each message by half a second.
  prefs: []
  type: TYPE_NORMAL
- en: '**How it could scale**:'
  prefs: []
  type: TYPE_NORMAL
- en: In production, it would be some kind of storefront web app that is producing
    the stream and we would have to do a bit of extra routing to get it into the topic
    (because the storefront and the Kafka cluster would live on different servers).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The messages might also be coming in at irregular frequencies rather than a
    standard half second, and the serialization would be optimized in some way.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 2 — You consumed a high-frequency stream of messages from a Kafka topic and
    performed an aggregation on the data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For the tutorial, you just streamed and consumed the data on the same machine,
    which might seem like nothing special — but in reality, there would be many consumers
    of different machines.
  prefs: []
  type: TYPE_NORMAL
- en: '**How it could scale**:'
  prefs: []
  type: TYPE_NORMAL
- en: You could run a whole array of applications on different servers, each consuming
    the stream in different ways.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: One consumer could be a fraud detection application that reads far back into
    the transaction history and looks at suspicious transaction patterns.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Another consumer could be an order fulfillment pipeline that is only interested
    in the latest unfulfilled orders.It would read the new messages and send the orders
    for processing.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yet another consumer could be a data aggregation pipeline that would enrich
    the data with data from a CRM and put it in a data warehouse for the marketing
    team to analyze.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As you can see, high-frequency time series data can be incredibly powerful when
    you use Apache Kafka to harness it. Tabular time series data is common in many
    applications such as financial analysis, sensor data analysis, and social media
    analytics.
  prefs: []
  type: TYPE_NORMAL
- en: By following the steps outlined in this tutorial, you should now have a solid
    foundation for sending tabular time series data to Apache Kafka and leveraging
    its capabilities to perform real-time data processing and analysis. Whether you
    are working on a small scale or a large scale project, Apache Kafka is an essential
    tool to have in your toolkit, and hopefully we’ve brought you one tiny step closer
    to mastering it.
  prefs: []
  type: TYPE_NORMAL
- en: You can find the source code for this and other tutorials in our [tutorials
    GitHub repository](https://github.com/quixai/tutorial-code.git).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Data Source: *Daqing Chen, Sai Liang Sain, and Kun Guo, Data mining for the
    online retail industry: “A case study of RFM model-based customer segmentation
    using data mining”, Journal of Database Marketing and Customer Strategy Management,
    Vol. 19, 2012 (Published online before print: 27 August 2012\. doi: 10.1057/dbm.2012.17)*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: About the author
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Tomáš Neubauer is CTO at [Quix](https://www.quix.io/company/) — a real-time
    data engineering stack that helps engineers manage the ever-greater volume and
    velocity of data. It makes streaming data easily accessible to Data and ML teams
    who prefer to work in Python.
  prefs: []
  type: TYPE_NORMAL
- en: In his spare time, Tomáš likes to go mountain biking in the hills around Prague,
    and he loves to ingest the finest beer that Czechia has to offer. If you have
    any questions about the tutorial, reach out to him [@Tomas Neubaue](https://stream-processing.slack.com/team/U035LUJJ4CQ)r
    in [The Stream](https://quix.io/slack-invite?_ga=2.115746945.682056252.1672214079-749957754.1668468796)
    — an open community for real-time data enthusiasts.
  prefs: []
  type: TYPE_NORMAL
- en: '*This article was originally published at:* [*https://www.quix.io/blog/send-timeseries-data-to-kafka-python*](https://www.quix.io/blog/send-timeseries-data-to-kafka-python)'
  prefs: []
  type: TYPE_NORMAL
