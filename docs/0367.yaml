- en: How to send tabular time series data to Apache Kafka with Python and Pandas
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何将表格时间序列数据发送到 Apache Kafka，使用 Python 和 Pandas
- en: 原文：[https://towardsdatascience.com/how-to-send-tabular-time-series-data-to-apache-kafka-with-python-and-pandas-39e2055373c3?source=collection_archive---------19-----------------------#2023-01-24](https://towardsdatascience.com/how-to-send-tabular-time-series-data-to-apache-kafka-with-python-and-pandas-39e2055373c3?source=collection_archive---------19-----------------------#2023-01-24)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/how-to-send-tabular-time-series-data-to-apache-kafka-with-python-and-pandas-39e2055373c3?source=collection_archive---------19-----------------------#2023-01-24](https://towardsdatascience.com/how-to-send-tabular-time-series-data-to-apache-kafka-with-python-and-pandas-39e2055373c3?source=collection_archive---------19-----------------------#2023-01-24)
- en: Learn now to produce and consume data in Kafka using a sample log of online
    retail transactions
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 现在学习如何使用在线零售交易的示例日志在 Kafka 中生成和消费数据
- en: '[](https://medium.com/@tomasatquix?source=post_page-----39e2055373c3--------------------------------)[![Tomáš
    Neubauer](../Images/5eb14b73cfe100ef9a43148db6abd3a9.png)](https://medium.com/@tomasatquix?source=post_page-----39e2055373c3--------------------------------)[](https://towardsdatascience.com/?source=post_page-----39e2055373c3--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----39e2055373c3--------------------------------)
    [Tomáš Neubauer](https://medium.com/@tomasatquix?source=post_page-----39e2055373c3--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@tomasatquix?source=post_page-----39e2055373c3--------------------------------)[![Tomáš
    Neubauer](../Images/5eb14b73cfe100ef9a43148db6abd3a9.png)](https://medium.com/@tomasatquix?source=post_page-----39e2055373c3--------------------------------)[](https://towardsdatascience.com/?source=post_page-----39e2055373c3--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----39e2055373c3--------------------------------)
    [Tomáš Neubauer](https://medium.com/@tomasatquix?source=post_page-----39e2055373c3--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fd620afda25db&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-send-tabular-time-series-data-to-apache-kafka-with-python-and-pandas-39e2055373c3&user=Tom%C3%A1%C5%A1+Neubauer&userId=d620afda25db&source=post_page-d620afda25db----39e2055373c3---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----39e2055373c3--------------------------------)
    ·15 min read·Jan 24, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F39e2055373c3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-send-tabular-time-series-data-to-apache-kafka-with-python-and-pandas-39e2055373c3&user=Tom%C3%A1%C5%A1+Neubauer&userId=d620afda25db&source=-----39e2055373c3---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fd620afda25db&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-send-tabular-time-series-data-to-apache-kafka-with-python-and-pandas-39e2055373c3&user=Tom%C3%A1%C5%A1+Neubauer&userId=d620afda25db&source=post_page-d620afda25db----39e2055373c3---------------------post_header-----------)
    发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----39e2055373c3--------------------------------)
    ·15分钟阅读·2023年1月24日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F39e2055373c3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-send-tabular-time-series-data-to-apache-kafka-with-python-and-pandas-39e2055373c3&user=Tom%C3%A1%C5%A1+Neubauer&userId=d620afda25db&source=-----39e2055373c3---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F39e2055373c3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-send-tabular-time-series-data-to-apache-kafka-with-python-and-pandas-39e2055373c3&source=-----39e2055373c3---------------------bookmark_footer-----------)![](../Images/33c2b7574c7e450bfa10a9e6192ff1ce.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F39e2055373c3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-send-tabular-time-series-data-to-apache-kafka-with-python-and-pandas-39e2055373c3&source=-----39e2055373c3---------------------bookmark_footer-----------)![](../Images/33c2b7574c7e450bfa10a9e6192ff1ce.png)'
- en: Photo by [Tech Daily](https://unsplash.com/@techdailyca?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    on [Unsplash](https://unsplash.com/photos/ztYmIQecyH4?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源于 [Tech Daily](https://unsplash.com/@techdailyca?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    在 [Unsplash](https://unsplash.com/photos/ztYmIQecyH4?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
- en: Time-series data comes in all shapes and sizes and it’s often produced in high
    frequencies in the form of sensor data and transaction logs. It’s also produced
    in huge volumes where the records are separated by milliseconds rather than hours
    or days.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: But what kind of system that can handle such a constant stream of data? An older
    approach would be to dump the raw data in Data Lake and process it in huge batches
    with a long-running process. Nowadays, many companies prefer to process the raw
    data in real-time and write the aggregated results to a database.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: For example, an online retailer could continuously aggregate transactional data
    by product and day rather than running expensive database queries on demand. But
    how would this work in practice? Let’s find out!
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: In this tutorial, we’ll use Python and Apache Kafka to process large volumes
    of time series data from that comes from a real online retailer.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: What you’ll learn
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'By the end of this tutorial you’ll understand:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: '**Why startups and online businesses use Apache Kafka**'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**The unique qualities of time series data and how it works with Kafka**'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**How to install and run Kafka on your local machine**'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**How to send time series data to Kafka in batches using Python and the Pandas
    library**'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What you should know already
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This article is intended for data scientists and engineers, so I’m assuming
    the following things about you:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: You know your way around Python and have used the Pandas library or at least
    know what it’s used for in data science.
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You’ve heard of Apache Kafka and know roughly what it’s for.
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: But don’t worry if you don’t meet these criteria. This tutorial is simple enough
    to follow along, and we’ll briefly explain these technologies. Just be aware that
    it’s not intended for absolute beginners.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: Why use Apache Kafka for time series data?
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Apache Kafka is an industry standard for processing data in real-time. You can
    use it to process a vast firehose of data (as long as you have the computing resources).
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: For example, some Formula One racing teams use Kafka in combination with Kubernetes
    to process huge quantities of sensor data coming in every millisecond. This data
    is analyzed in real-time to predict outcomes in races and to provide teams with
    insights for their Formula One pilots.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: 'Aside from it’s ability to process data streams, there are other key reasons
    why an online business might choose to use Apache Kafka:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: '**Scalability**: Kafka is designed to handle high volumes of data with low
    latency, making it well-suited for startups that expect rapid growth and need
    a solution that can scale with them.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Decoupling**: Ever heard of [event-driven architectures](https://www.tibco.com/reference-center/what-is-event-driven-architecture)?
    Kafka is one of the tools often touted to facilitate this pattern. It allows for
    decoupling of systems, meaning that different parts of the architecture can be
    developed and deployed independently. This can be particularly useful for startups
    that are iterating rapidly and need to be able to make changes to their systems
    without impacting other parts of the architecture.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**解耦**：你听说过 [事件驱动架构](https://www.tibco.com/reference-center/what-is-event-driven-architecture)
    吗？Kafka 是一种经常被用来促进这种模式的工具。它允许系统的解耦，这意味着架构的不同部分可以独立开发和部署。这对于快速迭代并需要能够在不影响其他架构部分的情况下修改系统的初创公司尤为有用。'
- en: '**Durability**: Kafka stores all published messages for a configurable amount
    of time, which means that it can serve as a durable log of all the data that has
    passed through the system. This can be useful for startups that need to maintain
    a record of all their data for auditing or compliance purposes.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**耐久性**：Kafka 会将所有发布的消息存储在可配置的时间内，这意味着它可以作为系统中所有数据的持久日志。这对于需要保持所有数据记录以满足审计或合规要求的初创公司非常有用。'
- en: '**Wide adoption**: Apache Kafka is widely adopted in the industry, which means
    that it has a large user base and a strong ecosystem of tools and resources available.
    This can be very helpful for startups that want to leverage the collective experience
    of the Kafka community.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**广泛采用**：Apache Kafka 在行业中得到了广泛的采用，这意味着它拥有庞大的用户基础和强大的工具及资源生态系统。这对于希望利用 Kafka
    社区集体经验的初创公司来说非常有帮助。'
- en: That last point is especially critical, since Apache Kafka has a notoriously
    steep learning curve. Without a vast pool of tutorials and demos, many beginners
    would struggle to get it up and running.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 这一点尤其重要，因为 Apache Kafka 有着 notoriously steep 的学习曲线。没有大量的教程和演示，许多初学者将难以使其正常运行。
- en: Why use Python with Apache Kafka?
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为什么要在 Apache Kafka 中使用 Python？
- en: Because Python is the most popular language in the Data and ML communities.
    These communities could benefit a lot from Apache Kafka, but there aren’t yet
    enough Kafka tutorials that appeal to their skillset.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 因为 Python 是数据和机器学习社区中最受欢迎的语言。这些社区可以从 Apache Kafka 中获益良多，但目前还没有足够吸引他们技能水平的 Kafka
    教程。
- en: If you’re part of a data team, you’re more likely to know Python and Pandas
    than you are Java. Yet most older Kafka tutorials are written for software engineers
    who write in Java. This is because software engineers have traditionally built
    the components that interact with Kafka (and Kafka itself is written in Java/Scala).
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你是数据团队的一部分，你更有可能知道 Python 和 Pandas，而不是 Java。然而，大多数旧版 Kafka 教程都是为使用 Java 的软件工程师编写的。这是因为软件工程师传统上构建了与
    Kafka 交互的组件（而 Kafka 本身是用 Java/Scala 编写的）。
- en: Kafka’s user base is changing however. The responsibilities of software and
    data teams are beginning to converge. This has been driven by the growing importance
    of data in modern organizations and the increasing complexity of data management
    and processing tasks. Today, data professionals also contribute to software components
    that interact with Kafka — but they still encounter a lot of friction because
    they’re typically not familiar with technologies from the Java ecosystem. That’s
    why we’re using Python and Pandas in these examples.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，Kafka 的用户基础正在发生变化。软件和数据团队的职责开始趋于融合。这是由于数据在现代组织中的重要性日益增长，以及数据管理和处理任务的复杂性增加。今天，数据专业人员也会参与与
    Kafka 交互的软件组件的开发，但他们仍然会遇到很多阻力，因为他们通常不熟悉 Java 生态系统中的技术。这就是为什么我们在这些示例中使用 Python
    和 Pandas 的原因。
- en: Ok, that’s it for the preamble — lets get into the tutorial.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 好了，前言部分就到这里——让我们进入教程。
- en: Prerequisites for this tutorial
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 本教程的先决条件
- en: The first thing you need is time — about 30 minutes (once you have the required
    software installed).
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要的第一件事是时间——大约 30 分钟（在你安装了所需的软件后）。
- en: Speaking of which, make sure that you have the following software installed
    before you proceed any further.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 说到这一点，请确保在继续之前已经安装了以下软件。
- en: SOFTWARE
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 软件
- en: '**Windows or a** Unix**-based operating system** We’ll be providing commands
    for both Windows and Unix-based operating systems such as macOS and Ubuntu.'
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**Windows 或基于 Unix 的操作系统** 我们将提供 Windows 和基于 Unix 的操作系统（如 macOS 和 Ubuntu）的命令。'
- en: '[**Python 3.0+**](https://www.python.org/about/gettingstarted/) **and required
    libraries'
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[**Python 3.0+**](https://www.python.org/about/gettingstarted/) **和所需的库**'
- en: —** You can download the installer from [the Python downloads page](https://www.python.org/downloads/).
  id: totrans-44
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: —** 你可以从 [Python 下载页面](https://www.python.org/downloads/) 下载安装程序。
- en: — Optionally, you might want to create a [virtual environment](https://docs.python.org/3/library/venv.html)
    to use for this tutorial so that you avoid conflicting dependencies.
  id: totrans-45
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: — 可选地，你可能想要创建一个 [虚拟环境](https://docs.python.org/3/library/venv.html) 用于本教程，以避免依赖冲突。
- en: '**Required Libraries'
  id: totrans-46
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '**所需库'
- en: '—** [Pandas](https://pypi.org/project/pandas/): Install with `pip3 install
    pandas`'
  id: totrans-47
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '—** [Pandas](https://pypi.org/project/pandas/): 使用 `pip3 install pandas` 安装'
- en: '**—** [Kafka-python](https://pypi.org/project/kafka-python/): Install with
    `pip3 install kafka-python`'
  id: totrans-48
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '**—** [Kafka-python](https://pypi.org/project/kafka-python/): 使用 `pip3 install
    kafka-python` 安装'
- en: '**Java 8+** This is a prerequisite for Apache Kafka. To install it, choose
    one of the following:'
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**Java 8+** 这是 Apache Kafka 的前提条件。要安装它，请选择以下之一：'
- en: '**—Ubuntu**: Install with `sudo apt install default-jre`'
  id: totrans-50
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '**—Ubuntu**：使用 `sudo apt install default-jre` 安装'
- en: '**— macOS**: Install with `brew install java`'
  id: totrans-51
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '**— macOS**：使用 `brew install java` 安装'
- en: '* *If you’re using a Macbook Pro with an M1 chip, you might need to install
    Java using the steps outlined in this guide: “*[*How to setup Java on Apple Mac
    M1 Pro (Dev.to)*](https://dev.to/docker/how-to-setup-java-on-macos-124-monterey-3l10)*”
    — it should take about 2 minutes.*'
  id: totrans-52
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '* *如果你使用的是带有 M1 芯片的 Macbook Pro，你可能需要按照本指南中概述的步骤安装 Java：“*[*如何在 Apple Mac M1
    Pro 上设置 Java (Dev.to)*](https://dev.to/docker/how-to-setup-java-on-macos-124-monterey-3l10)*”
    — 这大约需要 2 分钟。*'
- en: '**— Windows**: Download and run the [Java executable binary for Windows](https://www.java.com/en/download/)'
  id: totrans-53
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '**— Windows**：下载并运行 [Windows 的 Java 可执行文件](https://www.java.com/en/download/)'
- en: '[**Apache Kafka**](https://kafka.apache.org/documentation/#quickstart)You can
    download the Apache Kafka binary from the [Apache Kafka Download page](https://kafka.apache.org/downloads).'
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[**Apache Kafka**](https://kafka.apache.org/documentation/#quickstart) 你可以从
    [Apache Kafka 下载页面](https://kafka.apache.org/downloads) 下载 Apache Kafka 二进制文件。'
- en: Extract the contents of the file to a convenient location. For example, you
    can create a project folder for this tutorial and extract it there.
  id: totrans-55
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 将文件内容解压到一个方便的位置。例如，你可以为本教程创建一个项目文件夹并将其解压到那里。
- en: Major steps
  id: totrans-56
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 主要步骤
- en: Before we get into the details, let’s go over the major steps that we’ll be
    covering.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入细节之前，让我们回顾一下我们将要覆盖的主要步骤。
- en: '**Setting up Kafka:** We’ll first get to grips with Kafka’s command line tools,
    and use them to:'
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**设置 Kafka：** 我们首先将熟悉 Kafka 的命令行工具，并使用它们来：'
- en: — Start the Zookeeper and Kafka server
  id: totrans-59
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: — 启动 Zookeeper 和 Kafka 服务器
- en: — Create a topic (where we’ll be sending our data)
  id: totrans-60
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: — 创建一个主题（我们将向其发送数据）
- en: '**Analyzing the Data:** Use Python and Pandas to:'
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**数据分析：** 使用 Python 和 Pandas 来：'
- en: — Take a closer look at the structure of the online retail data set.
  id: totrans-62
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: — 仔细查看在线零售数据集的结构。
- en: — Read it into a DataFrame and understand the different data types.
  id: totrans-63
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: — 将其读入数据框并理解不同的数据类型。
- en: We’ll also look at the benefits of using data frames and time series data with
    Kafka.
  id: totrans-64
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们还将探讨使用数据框和时间序列数据与 Kafka 的好处。
- en: '**Sending Data to a Kafka with a Producer:** Use the kafka-python library to:'
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**使用生产者向 Kafka 发送数据：** 使用 kafka-python 库来：'
- en: — Read the csv into a Data Frame and initialize the Kafka python producer.
  id: totrans-66
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: — 将 csv 文件读取到数据框中并初始化 Kafka python 生产者。
- en: — Iterate through the rows and send them in batches to Kafka
  id: totrans-67
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: — 遍历行并将其批量发送到 Kafka
- en: '**Read Data from Kafka with a Consumer:** Again, we’ll use the kafka-python
    library to:'
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**从 Kafka 中读取数据：** 我们将再次使用 kafka-python 库来：'
- en: — Read the messages from our Kafka topic
  id: totrans-69
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: — 从我们的 Kafka 主题中读取消息
- en: — Take the batched message and convert it back into a data frame.
  id: totrans-70
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: — 将批处理消息转换回数据框。
- en: — Perform some simple aggregation on it.
  id: totrans-71
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: — 对其进行一些简单的聚合操作。
- en: 'You’ll find the code in the ‘*tabular-timeseries-kafka*’ subfolder. If you
    want to jump straight ahead to the code, you can clone our tutorials repo in GitHub
    with the following command:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 你会在 ‘*tabular-timeseries-kafka*’ 子文件夹中找到代码。如果你想直接跳到代码部分，可以使用以下命令克隆我们的教程仓库：
- en: git clone [https://github.com/quixai/tuto...](https://github.com/quixai/tutorial-code.git)
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: git clone [https://github.com/quixai/tuto...](https://github.com/quixai/tutorial-code.git)
- en: Setting up Apache Kafka
  id: totrans-74
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设置 Apache Kafka
- en: If you haven’t done so already, download Apache Kafka from the [Apache Kafka
    Download page](https://kafka.apache.org/downloads) (for example, “*kafka_2.12–3.3.1.tgz*”)
    and extract the contents of the file to a convenient location.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你还没这样做，从 [Apache Kafka 下载页面](https://kafka.apache.org/downloads) 下载 Apache
    Kafka（例如，“*kafka_2.12–3.3.1.tgz*”）并将文件内容解压到一个方便的位置。
- en: 'Before you can do anything with Kafka, you have to start the core services.
    Each of these services needs to run in a separate terminal window. These two services
    are:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在你能对 Kafka 进行任何操作之前，你需要启动核心服务。每个服务都需要在单独的终端窗口中运行。这两个服务是：
- en: The **zookeeper service** which is responsible for managing the coordination
    between all the different services, brokers, and clients that make up the Kafka
    cluster.
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**Zookeeper 服务**负责管理所有不同服务、代理和客户端之间的协调，这些组成了 Kafka 集群。'
- en: The **Kafka server service** which runs the core functionality of Apache Kafka,
    such as the message broker.
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**Kafka 服务器服务**运行 Apache Kafka 的核心功能，如消息代理。'
- en: When following these instructions, start each terminal window in the directory
    where you extracted Kafka (for example, “*C:\Users\demo\Downloads\kafka_2.13–3.3.1\*
    ”)
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 按照这些说明操作时，请在你解压 Kafka 的目录中启动每个终端窗口（例如，“*C:\Users\demo\Downloads\kafka_2.13–3.3.1\*”）。
- en: '**To start the core Kafka services, follow these steps:**'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '**要启动核心 Kafka 服务，请按照以下步骤操作：**'
- en: '**1.** In the Kafka directory, open a terminal window and start the zookeeper
    service with the following command:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '**1.** 在 Kafka 目录中，打开一个终端窗口，并使用以下命令启动 zookeeper 服务：'
- en: '**Linux / macOS**'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Linux / macOS**'
- en: '[PRE0]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '**Windows**'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Windows**'
- en: '[PRE1]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: You should see a bunch of log messages indicating that zookeeper started successfully.
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你应该会看到一堆日志消息，表明 zookeeper 启动成功。
- en: Leave the window open.
  id: totrans-87
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 保持窗口打开。
- en: '**2.** Open a second terminal window and start the Kafka server with the following
    command:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '**2.** 打开第二个终端窗口，并使用以下命令启动 Kafka 服务器：'
- en: '**Linux / macOS**'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Linux / macOS**'
- en: '[PRE2]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '**Windows**'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Windows**'
- en: '[PRE3]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Again, you should see similar log messages indicating the server started successfully.
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 再次，你应该会看到类似的日志消息，表明服务器启动成功。
- en: Leave this window open too.
  id: totrans-94
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 也保持这个窗口打开。
- en: '**3.** Next, you’ll need to create a topic called “transactions” to store the
    data.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '**3.** 接下来，你需要创建一个名为“transactions”的主题来存储数据。'
- en: If the term “topic” is unfamiliar, think of it as a process that is constantly
    updating a log file. There are many ways to [explain what a topic does](https://dattell.com/data-architecture-blog/what-is-a-kafka-topic/)
    in detail, but for now, let’s just say that it’s a log file for events related
    to a specific type of data (such as, incoming transactions) including the data
    itself.
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果“主题”这个术语不太熟悉，可以把它看作是一个不断更新日志文件的过程。有很多方式来 [详细解释主题的作用](https://dattell.com/data-architecture-blog/what-is-a-kafka-topic/)，但目前可以简单理解为，它是一个记录特定类型数据（如：传入的交易）及数据本身的事件日志文件。
- en: '**To create a topic in Kafka:**'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '**要在 Kafka 中创建一个主题：**'
- en: 'Open a third terminal window and enter the following command:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 打开第三个终端窗口并输入以下命令：
- en: '**Linux / macOS**'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Linux / macOS**'
- en: '[PRE4]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '**Windows**'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Windows**'
- en: '[PRE5]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: You should see the confirmation message “*Created topic transactions*”
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你应该会看到确认消息 “*创建了主题 transactions*”。
- en: That’s it for Kafka. Hopefully you’ve managed to set it up without any hassle.
    If you ran into any issues, this [troubleshooting guide](https://www.codeproject.com/Articles/5276053/Troubleshoot-Kafka-Setup-on-Windows)
    might help. Now, let’s move in to the exciting part.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: Kafka 部分就这些了。希望你已经顺利设置好了。如果遇到任何问题，可能会有帮助的 [故障排除指南](https://www.codeproject.com/Articles/5276053/Troubleshoot-Kafka-Setup-on-Windows)。现在，让我们进入激动人心的部分。
- en: Analyzing the data
  id: totrans-105
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分析数据
- en: For this exercise, we’ll be using a transnational data set which contains all
    the transactions occurring between 01/12/2010 and 09/12/2011 for a UK-based online
    retail store. It was obtained from the machine learning repository hosted by the
    University of California. You can find more details about the dataset on its [dedicated
    page](https://archive.ics.uci.edu/ml/datasets/online+retail) in the UCI Machine
    Learning Repository website*.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 对于本练习，我们将使用一个跨国数据集，该数据集包含了 2010 年 01 月 12 日至 2011 年 09 月 12 日之间发生的所有交易，数据来自于一个位于英国的在线零售商。该数据集来源于加州大学主办的机器学习库。你可以在
    UCI 机器学习库网站的 [专页](https://archive.ics.uci.edu/ml/datasets/online+retail) 上找到有关数据集的更多详细信息。
- en: 'To prepare, follow these steps:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 准备工作，按以下步骤操作：
- en: Create a folder for this project (e.g. “*tabular-timeseries-kafka*”).
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为此项目创建一个文件夹（例如，“*tabular-timeseries-kafka*”）。
- en: Download the zipped CSV file from [this storage location](https://quixdocsdev.blob.core.windows.net/docsartifacts/online_retail_II.zip)
    and extract it into the project folder
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从 [这个存储位置](https://quixdocsdev.blob.core.windows.net/docsartifacts/online_retail_II.zip)
    下载压缩的 CSV 文件，并将其解压到项目文件夹中。
- en: We’ll be providing instructions as Python commands, but you can also use an
    IDE such as [Pycharm Community Edition](https://www.jetbrains.com/pycharm/download/).
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将提供 Python 命令作为说明，但你也可以使用 [Pycharm Community Edition](https://www.jetbrains.com/pycharm/download/)
    等 IDE。
- en: 'This first task is to inspect the data in the file and look at how Pandas interprets
    the data by default:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个任务是检查文件中的数据，并查看 Pandas 默认如何解释这些数据：
- en: '**To inspect the data follow, these steps:**'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '**要检查数据，请按照以下步骤操作：**'
- en: '**1.** Open a terminal window in your project directory and enter *python*
    start the Python console.'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '**1.** 在你的项目目录中打开一个终端窗口并输入 *python* 启动 Python 控制台。'
- en: '**2.** Read the CSV into a DataFrame by entering the following commands:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '**2.** 通过输入以下命令将 CSV 读入 DataFrame：'
- en: '[PRE6]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '**3.** Examine the output of `df.info`:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '**3.** 检查 `df.info` 的输出：'
- en: '[PRE7]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Note that the `InvoiceDate` column has been read in as an object data type.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，`InvoiceDate` 列已被读取为对象数据类型。
- en: In Pandas, the object data type is used to represent string values or data that
    cannot be easily cast to a numerical data type.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Pandas 中，对象数据类型用于表示字符串值或无法轻易转换为数值数据类型的数据。
- en: To make this a true time series dataset, we need to have the time that the record
    was created (in this case `InvoiceDate`) in a proper DateTime format. We’ll explain
    why in a moment.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使数据集成为真正的时间序列数据集，我们需要将记录创建的时间（在此例中为 `InvoiceDate`）转为合适的 DateTime 格式。我们稍后会解释原因。
- en: '**To convert the InvoiceDate column into a date format:**'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '**将 InvoiceDate 列转换为日期格式：**'
- en: 'In the Python console, enter the following commands:'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 Python 控制台中，输入以下命令：
- en: '[PRE8]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: This conversion allows you to take advantage of pandas’ extensive [time series
    functionality](https://pandas.pydata.org/docs/user_guide/timeseries.html#timeseries-overview).
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这种转换允许你利用 pandas 的广泛 [时间序列功能](https://pandas.pydata.org/docs/user_guide/timeseries.html#timeseries-overview)。
- en: For example, setting it as a DatetimeIndex can help you optimize your time series
    data with precomputed and cached date ranges; fast and easy selection of dates,
    date ranges, and the values attached to them; and quick summarizations using time
    blocks (“year”, “month”).
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 例如，将其设置为 DatetimeIndex 可以帮助你优化时间序列数据，包括预计算和缓存的日期范围；快速且轻松地选择日期、日期范围及其相关值；以及使用时间块（“年”、“月”）进行快速汇总。
- en: What Kafka can do with time series data
  id: totrans-126
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Kafka 可以对时间序列数据做什么
- en: 'So we know that Pandas has many time series-related features, but what about
    Kafka? Indeed, Kafka also provides a number of features that can be used to optimize
    the processing of time series data. These include:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们知道 Pandas 有许多与时间序列相关的功能，但 Kafka 呢？确实，Kafka 也提供了许多功能来优化时间序列数据的处理。这些包括：
- en: '**Compression**: Kafka supports multiple compression algorithms that can use
    time data to reduce the size of data streams and improve the efficiency of data
    processing.'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**压缩**：Kafka 支持多种压缩算法，这些算法可以利用时间数据来减少数据流的大小，提高数据处理效率。'
- en: '**Data partitioning**: Kafka allows you to partition data streams by key and
    time, which can be useful for distributing data processing across multiple consumer
    instances.'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据分区**：Kafka 允许你按键和时间对数据流进行分区，这对于将数据处理分布到多个消费者实例中很有用。'
- en: '**Custom serialization**: Kafka provides a pluggable serialization framework
    that allows you to specify custom serializers and deserializers for your data,
    which can be useful for optimizing the performance of data processing.'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**自定义序列化**：Kafka 提供了一个可插拔的序列化框架，允许你为数据指定自定义序列化器和反序列化器，这对于优化数据处理性能可能非常有用。'
- en: All of these features depend in part, on having time in the correct format.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些功能部分依赖于时间的正确格式。
- en: Note that we’re not going to use any of these features in this basic tutorial.
    But if you intend to use Kafka in production, it’s important to understand the
    role that time data can play when interacting with Kafka.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在这个基础教程中我们不会使用这些功能。但是，如果你打算在生产环境中使用Kafka，了解时间数据在与Kafka交互时所扮演的角色是很重要的。
- en: The advantages of using DataFrames with Kafka
  id: totrans-133
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 DataFrames 与 Kafka 的优点
- en: Pandas DataFrames are particularly useful when storing data in a tabular format,
    as each feature of your data set can be manipulated as a one-dimensional shape
    or Series while still collectively forming a multidimensional data set. DataFrames
    also come with a number of handy built-in functions that allow you to manipulate
    the data and process data very quickly. This is invaluable when working with Kafka.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: Pandas DataFrames 在以表格格式存储数据时特别有用，因为数据集中的每个特征都可以作为一维形状或 Series 进行操作，同时仍然形成一个多维数据集。DataFrames
    还配备了一些方便的内置函数，允许你快速操作和处理数据。这在使用 Kafka 时非常宝贵。
- en: For example, in the next major step, we’ll be sending the data in batches. We’re
    batching the records to replicate a production scenario where you might receive
    hundreds of records a second.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在下一主要步骤中，我们将以批量的形式发送数据。我们将记录分批处理，以模拟生产场景，你可能每秒接收到数百条记录。
- en: If you sent a message to Kafka for each individual record, you could risk bottlenecks
    or system failures. That’s why you send small batches of data in each message
    at a lower frequency (rather than tons of small messages at a high frequency).
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你为每条单独的记录发送一条消息，你可能会遇到瓶颈或系统故障。这就是为什么你以较低频率发送小批量数据（而不是以高频率发送大量小消息）的原因。
- en: As you’ll see, this process is very easy when we use the Pandas chunking feature.
    This is one of the many Kafka-friendly features that come with Pandas DataFrames.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所见，当我们使用 Pandas 分块功能时，这个过程非常简单。这是 Pandas DataFrames 提供的众多 Kafka 友好功能之一。
- en: Creating a Kafka Producer to send the data
  id: totrans-138
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建一个 Kafka 生产者来发送数据
- en: Now, lets put what we’ve learned into a proper Python file and start sending
    data to Kafka. We’ll be using the [*kafka-python*](https://github.com/dpkp/kafka-python)
    library which is one of several libraries intended to connect python applications
    to Kafka (another is the [*confluent-kafka-python*](https://github.com/confluentinc/confluent-kafka-python)
    library).
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们将所学内容放入一个合适的 Python 文件中，并开始向 Kafka 发送数据。我们将使用[*kafka-python*](https://github.com/dpkp/kafka-python)库，这是多个用于将
    Python 应用程序连接到 Kafka 的库之一（另一个是[*confluent-kafka-python*](https://github.com/confluentinc/confluent-kafka-python)库）。
- en: '**To create a producer, follow these steps:**'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '**要创建一个生产者，请遵循以下步骤：**'
- en: 'In your project directory, create a file called `producer.py` and insert the
    following code block:'
  id: totrans-141
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在你的项目目录中，创建一个名为 `producer.py` 的文件，并插入以下代码块：
- en: '[PRE9]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: This imports all the required libraries and initializes the Kafka producer,
    telling it to connect to the server that you should (hopefully) still have running
    on your computer under ‘*localhost*’
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码导入了所有必需的库并初始化 Kafka 生产者，告诉它连接到你（希望）仍然在计算机上运行的服务器下的‘*localhost*’。
- en: '**2.** Next, add the ‘`for`’ loop that will iterate through the file and send
    the data in batches:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '**2.** 接下来，添加一个‘`for`’循环，它将遍历文件并以批量的形式发送数据：'
- en: '[PRE10]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The code includes explanatory comments, but essentially it’s doing the following:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 代码包括解释性的注释，但基本上它做了以下几件事：
- en: Reading the CSV in batches of 10 rows
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 按每 10 行批量读取 CSV 文件
- en: Serializing each batch into JSON and encoding the JSON as a Byte array
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将每个批次序列化为 JSON 并将 JSON 编码为字节数组
- en: Sending that JSON as a message to the Kafka topic “transactions”
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将 JSON 作为消息发送到 Kafka 主题“transactions”
- en: You can review the complete file in our Github repository. [https://github.com/quixai/tutorials/timeseries/producer.py](https://github.com/quixai/tutorials/timeseries/producer.py)
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在我们的 Github 仓库中查看完整的文件。 [https://github.com/quixai/tutorials/timeseries/producer.py](https://github.com/quixai/tutorials/timeseries/producer.py)
- en: '**3.** Save your file and run your code with the following command:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '**3.** 保存你的文件并使用以下命令运行代码：'
- en: python producer.py
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '`python producer.py`'
- en: 'In your terminal window, you should start seeing confirmations like this:'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在你的终端窗口中，你应该开始看到类似这样的确认信息：
- en: “*Sent record to topic at time 2022–12–28 13:23:52.125664*” for each message
    sent.
  id: totrans-154
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: “*将记录发送到主题的时间是 2022–12–28 13:23:52.125664*”为每条发送的消息。
- en: If you get an error about missing modules, make sure that you have the *kafka-python*
    library installed (`pip3 install kafka-python`).
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你收到缺少模块的错误，请确保你已安装 *kafka-python* 库（`pip3 install kafka-python`）。
- en: Once the data is in the Kafka topic, it can be read by multiple consumers and
    extracted for more downstream processes.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦数据进入 Kafka 主题，它可以被多个消费者读取，并提取用于更多下游处理。
- en: Lets create a consumer to read those messages back in and before a basic operation
    on them.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个消费者来读取这些消息并对其进行基本操作。
- en: Creating a Kafka Consumer to read the data
  id: totrans-158
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建一个 Kafka 消费者来读取数据
- en: The process for creating a Kafka consumer is very similar to the previous step.
    In this case we’ll be, reading each batched message, converting it back into a
    DataFrame.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 创建 Kafka 消费者的过程与前一步非常类似。在这种情况下，我们将读取每个批量的消息，并将其转换回 DataFrame。
- en: We’ll pretend that this is consumer is for some kind of inventory analysis pipeline
    that only wants to know the total sales for each stock item. Thus, in the data,
    we’ll only look at the StockCode, the Quantity sold and the Price. We’ll calculate
    the total value of the sale for each record so that we can aggregate sales by
    StockCode.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将假设这个消费者是用于某种库存分析管道，它只关心每个库存项目的总销售额。因此，在数据中，我们只查看StockCode、销售数量和价格。我们将计算每条记录的销售总值，以便按StockCode汇总销售数据。
- en: In your project directory, create a file called `consumer.py` and insert the
    following code block to initialize the consumer.
  id: totrans-161
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在你的项目目录中，创建一个名为`consumer.py`的文件，并插入以下代码块以初始化消费者。
- en: '[PRE11]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: We’re initializing the consumer with a few more options than we did for the
    producer.
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们正在用比我们为生产者设置的更多选项来初始化消费者。
- en: First, we’re telling it what topic to read from, then where the Kafka server
    is running, and thirdly, we’re giving it a lambda function to use for deserializing
    the message values back into a Python dictionary.
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 首先，我们告诉它从哪个主题读取，然后Kafka服务器运行在哪里，最后，我们提供了一个lambda函数用于将消息值反序列化回Python字典。
- en: 2\. Next, add the ‘for’ loop that will iterate through the messages and perform
    some processing on them.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 2. 接下来，添加‘for’循环，它将迭代消息并对其进行一些处理。
- en: '[PRE12]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: As you can see in the code comments, we are performing a simple calculation
    that outputs a summary of the revenue by `StockCode` for each message batch.
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 正如你在代码注释中看到的，我们正在进行一个简单的计算，输出按`StockCode`的每个消息批次的收入汇总。
- en: Of course, the end goal would be to keep a running total of the overall revenue
    by `StockCode`. This would require some further processing that would write the
    aggregations into a database which could in turn, power some kind of Dashboard.
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当然，最终目标是按`StockCode`保持总体收入的持续统计。这将需要一些进一步的处理，将汇总数据写入数据库，从而为某种仪表盘提供数据。
- en: 'However, for the purposes of this tutorial, this is a good place to stop. If
    everything works correctly, you should see the aggregated result logged for each
    message. It should look something like this:'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然而，为了本教程的目的，这里是一个很好的停止点。如果一切正常，你应该能看到每条消息的汇总结果被记录下来。它应该类似于这样：
- en: '[PRE13]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: If you see the logged aggregations, well done! Congratulations for making it
    to the end.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你看到记录的汇总，做得好！祝贺你完成了所有步骤。
- en: Wrapping up
  id: totrans-172
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结一下
- en: 'This process might seem unremarkable if you’re just doing it on your local
    machine, but it becomes more interesting when you consider how this pattern could
    scale. Let’s take a moment to recap what you did — you accomplished two key tasks:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你只是把它做在本地机器上，这个过程可能显得平凡无奇，但当你考虑到这种模式如何扩展时，它会变得更加有趣。让我们稍微回顾一下你做了什么——你完成了两个关键任务：
- en: 1— You produced a high-frequency stream of messages and streamed them into a
    Kafka topic
  id: totrans-174
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1— 你生成了高频率的消息流，并将其流入一个Kafka主题
- en: In this case, you were “replaying” a historical log of customer transactions
    and artificially staggering each message by half a second.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，你是在“重放”客户交易的历史日志，并人为地将每条消息延迟半秒。
- en: '**How it could scale**:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: '**它如何扩展**：'
- en: In production, it would be some kind of storefront web app that is producing
    the stream and we would have to do a bit of extra routing to get it into the topic
    (because the storefront and the Kafka cluster would live on different servers).
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在生产环境中，可能会有某种前端应用程序生成流数据，我们需要做一些额外的路由工作将其放入主题中（因为前端应用程序和Kafka集群将位于不同的服务器上）。
- en: The messages might also be coming in at irregular frequencies rather than a
    standard half second, and the serialization would be optimized in some way.
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 消息也可能以不规则的频率到达，而不是标准的半秒，并且序列化可能会以某种方式进行优化。
- en: 2 — You consumed a high-frequency stream of messages from a Kafka topic and
    performed an aggregation on the data
  id: totrans-179
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 — 你从Kafka主题中消费了高频率的消息流，并对数据进行了汇总处理
- en: For the tutorial, you just streamed and consumed the data on the same machine,
    which might seem like nothing special — but in reality, there would be many consumers
    of different machines.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个教程，你只是通过同一台机器进行数据流处理和消费，这可能看起来没什么特别的——但实际上，会有许多不同机器的消费者。
- en: '**How it could scale**:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '**它如何扩展**：'
- en: You could run a whole array of applications on different servers, each consuming
    the stream in different ways.
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你可以在不同的服务器上运行一整套应用程序，每个应用程序以不同的方式消费流数据。
- en: One consumer could be a fraud detection application that reads far back into
    the transaction history and looks at suspicious transaction patterns.
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个消费者可能是一个欺诈检测应用程序，它会深入交易历史，寻找可疑的交易模式。
- en: Another consumer could be an order fulfillment pipeline that is only interested
    in the latest unfulfilled orders.It would read the new messages and send the orders
    for processing.
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 另一个消费者可能是一个订单履行管道，它只关心最新的未完成订单。它会读取新消息，并将订单发送去处理。
- en: Yet another consumer could be a data aggregation pipeline that would enrich
    the data with data from a CRM and put it in a data warehouse for the marketing
    team to analyze.
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 另一个消费者可能是一个数据聚合管道，它将数据与CRM中的数据结合，并将其放入数据仓库中供市场营销团队分析。
- en: As you can see, high-frequency time series data can be incredibly powerful when
    you use Apache Kafka to harness it. Tabular time series data is common in many
    applications such as financial analysis, sensor data analysis, and social media
    analytics.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，当你使用 Apache Kafka 来利用高频时间序列数据时，它可以变得极其强大。表格时间序列数据在许多应用中都很常见，如金融分析、传感器数据分析和社交媒体分析。
- en: By following the steps outlined in this tutorial, you should now have a solid
    foundation for sending tabular time series data to Apache Kafka and leveraging
    its capabilities to perform real-time data processing and analysis. Whether you
    are working on a small scale or a large scale project, Apache Kafka is an essential
    tool to have in your toolkit, and hopefully we’ve brought you one tiny step closer
    to mastering it.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 按照本教程中概述的步骤，你现在应该具备了将表格时间序列数据发送到 Apache Kafka 并利用其能力进行实时数据处理和分析的坚实基础。无论你是在进行小规模还是大规模项目，Apache
    Kafka 都是你工具包中的一个重要工具，希望我们能让你更接近掌握它的终极目标。
- en: You can find the source code for this and other tutorials in our [tutorials
    GitHub repository](https://github.com/quixai/tutorial-code.git).
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你可以在我们的 [教程 GitHub 仓库](https://github.com/quixai/tutorial-code.git)中找到本教程及其他教程的源代码。
- en: 'Data Source: *Daqing Chen, Sai Liang Sain, and Kun Guo, Data mining for the
    online retail industry: “A case study of RFM model-based customer segmentation
    using data mining”, Journal of Database Marketing and Customer Strategy Management,
    Vol. 19, 2012 (Published online before print: 27 August 2012\. doi: 10.1057/dbm.2012.17)*'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '数据来源：*Daqing Chen, Sai Liang Sain, 和 Kun Guo, 数据挖掘在线零售行业：“基于RFM模型的客户细分案例研究”，《数据库营销与客户策略管理期刊》，第19卷，2012年（在线出版日期：2012年8月27日。doi:
    10.1057/dbm.2012.17）*'
- en: About the author
  id: totrans-190
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 关于作者
- en: Tomáš Neubauer is CTO at [Quix](https://www.quix.io/company/) — a real-time
    data engineering stack that helps engineers manage the ever-greater volume and
    velocity of data. It makes streaming data easily accessible to Data and ML teams
    who prefer to work in Python.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: Tomáš Neubauer 是 [Quix](https://www.quix.io/company/) 的首席技术官——一个实时数据工程堆栈，帮助工程师管理越来越庞大和快速的数据流。它使数据和机器学习团队更容易访问流数据，这些团队喜欢用
    Python 进行工作。
- en: In his spare time, Tomáš likes to go mountain biking in the hills around Prague,
    and he loves to ingest the finest beer that Czechia has to offer. If you have
    any questions about the tutorial, reach out to him [@Tomas Neubaue](https://stream-processing.slack.com/team/U035LUJJ4CQ)r
    in [The Stream](https://quix.io/slack-invite?_ga=2.115746945.682056252.1672214079-749957754.1668468796)
    — an open community for real-time data enthusiasts.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 在业余时间，Tomáš 喜欢在布拉格周围的山丘中骑山地自行车，他还喜欢品尝捷克提供的最优质啤酒。如果你对教程有任何问题，可以联系他 [@Tomas Neubaue](https://stream-processing.slack.com/team/U035LUJJ4CQ)r，或在
    [The Stream](https://quix.io/slack-invite?_ga=2.115746945.682056252.1672214079-749957754.1668468796)——一个针对实时数据爱好者的开放社区。
- en: '*This article was originally published at:* [*https://www.quix.io/blog/send-timeseries-data-to-kafka-python*](https://www.quix.io/blog/send-timeseries-data-to-kafka-python)'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '*本文最初发表于：* [*https://www.quix.io/blog/send-timeseries-data-to-kafka-python*](https://www.quix.io/blog/send-timeseries-data-to-kafka-python)'
