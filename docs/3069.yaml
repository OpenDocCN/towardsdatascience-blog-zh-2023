- en: Hierarchical Transformers — part 2
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/hierarchical-transformers-part-2-2616eecacb21?source=collection_archive---------14-----------------------#2023-10-07](https://towardsdatascience.com/hierarchical-transformers-part-2-2616eecacb21?source=collection_archive---------14-----------------------#2023-10-07)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Hierarchical attention is faster
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@mina.ghashami?source=post_page-----2616eecacb21--------------------------------)[![Mina
    Ghashami](../Images/745f53b94f5667a485299b49913c7a21.png)](https://medium.com/@mina.ghashami?source=post_page-----2616eecacb21--------------------------------)[](https://towardsdatascience.com/?source=post_page-----2616eecacb21--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----2616eecacb21--------------------------------)
    [Mina Ghashami](https://medium.com/@mina.ghashami?source=post_page-----2616eecacb21--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fc99ed9ed7b9a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhierarchical-transformers-part-2-2616eecacb21&user=Mina+Ghashami&userId=c99ed9ed7b9a&source=post_page-c99ed9ed7b9a----2616eecacb21---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----2616eecacb21--------------------------------)
    ·6 min read·Oct 7, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F2616eecacb21&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhierarchical-transformers-part-2-2616eecacb21&user=Mina+Ghashami&userId=c99ed9ed7b9a&source=-----2616eecacb21---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F2616eecacb21&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhierarchical-transformers-part-2-2616eecacb21&source=-----2616eecacb21---------------------bookmark_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: This article requires you to have knowledge of standard transformers and how
    they work. If you are a beginner and you’d like to know about transformers, please
    take a look at [Transformer for Beginners](https://medium.com/p/4deaf9b199f9/edit)
    article.
  prefs: []
  type: TYPE_NORMAL
- en: In [Hierarchical Transformer — part 1](https://medium.com/towards-data-science/hierarchical-transformers-54f6d59fa8fc)
    we defined, what we mean by “hierarchical transformers”, and we reviewed one of
    prominent work in this domain which was called *Hourglass*.
  prefs: []
  type: TYPE_NORMAL
- en: In this article, we will continue the line of work by looking into another well-known
    work called *Hierarchical Attention Transformers (HAT).*
  prefs: []
  type: TYPE_NORMAL
- en: Let’s get started.
  prefs: []
  type: TYPE_NORMAL
- en: Hierarchical Attention Transformer (HAT)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This method was initially proposed for classifying long documents, typically
    in length of thousands of words. A usecase of this is classifying legal documents
    or biomedical documents which are typically very long.
  prefs: []
  type: TYPE_NORMAL
- en: Tokenization and Segmentation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The **HAT** method works by taking an input document, and tokenizing it using
    Byte-Pair Encoding (BPE) tokenizer that breaks text into subwords/tokens. This
    tokenizer is used in many well-known large language models such as BERT, RoBERTA
    and GPT family.
  prefs: []
  type: TYPE_NORMAL
- en: Then it splits the tokenized document into *N* equally-sized chunks; i.e. if
    *S* denote the input document then *S = [C1, …., CN]* are *N* equally-sized chunks.
    (Through out this article, we…
  prefs: []
  type: TYPE_NORMAL
