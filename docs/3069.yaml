- en: Hierarchical Transformers — part 2
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/hierarchical-transformers-part-2-2616eecacb21?source=collection_archive---------14-----------------------#2023-10-07](https://towardsdatascience.com/hierarchical-transformers-part-2-2616eecacb21?source=collection_archive---------14-----------------------#2023-10-07)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Hierarchical attention is faster
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@mina.ghashami?source=post_page-----2616eecacb21--------------------------------)[![Mina
    Ghashami](../Images/745f53b94f5667a485299b49913c7a21.png)](https://medium.com/@mina.ghashami?source=post_page-----2616eecacb21--------------------------------)[](https://towardsdatascience.com/?source=post_page-----2616eecacb21--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----2616eecacb21--------------------------------)
    [Mina Ghashami](https://medium.com/@mina.ghashami?source=post_page-----2616eecacb21--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fc99ed9ed7b9a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhierarchical-transformers-part-2-2616eecacb21&user=Mina+Ghashami&userId=c99ed9ed7b9a&source=post_page-c99ed9ed7b9a----2616eecacb21---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----2616eecacb21--------------------------------)
    ·6 min read·Oct 7, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F2616eecacb21&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhierarchical-transformers-part-2-2616eecacb21&user=Mina+Ghashami&userId=c99ed9ed7b9a&source=-----2616eecacb21---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F2616eecacb21&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhierarchical-transformers-part-2-2616eecacb21&source=-----2616eecacb21---------------------bookmark_footer-----------)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: This article requires you to have knowledge of standard transformers and how
    they work. If you are a beginner and you’d like to know about transformers, please
    take a look at [Transformer for Beginners](https://medium.com/p/4deaf9b199f9/edit)
    article.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: In [Hierarchical Transformer — part 1](https://medium.com/towards-data-science/hierarchical-transformers-54f6d59fa8fc)
    we defined, what we mean by “hierarchical transformers”, and we reviewed one of
    prominent work in this domain which was called *Hourglass*.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: In this article, we will continue the line of work by looking into another well-known
    work called *Hierarchical Attention Transformers (HAT).*
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: Let’s get started.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: Hierarchical Attention Transformer (HAT)
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This method was initially proposed for classifying long documents, typically
    in length of thousands of words. A usecase of this is classifying legal documents
    or biomedical documents which are typically very long.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 该方法最初是为了对长文档进行分类而提出的，这些文档通常长达数千字。一个应用实例是对法律文档或生物医学文档进行分类，这些文档通常非常长。
- en: Tokenization and Segmentation
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 标记化和分割
- en: The **HAT** method works by taking an input document, and tokenizing it using
    Byte-Pair Encoding (BPE) tokenizer that breaks text into subwords/tokens. This
    tokenizer is used in many well-known large language models such as BERT, RoBERTA
    and GPT family.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '**HAT** 方法通过处理输入文档，并使用字节对编码（BPE）标记器将文本拆分为子词/标记来工作。这个标记器被许多著名的大型语言模型使用，如 BERT、RoBERTA
    和 GPT 系列。'
- en: Then it splits the tokenized document into *N* equally-sized chunks; i.e. if
    *S* denote the input document then *S = [C1, …., CN]* are *N* equally-sized chunks.
    (Through out this article, we…
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，它将标记化后的文档拆分成 *N* 个大小相等的块；即，如果 *S* 表示输入文档，则 *S = [C1, …., CN]* 是 *N* 个大小相等的块。
    （在本文中，我们…
