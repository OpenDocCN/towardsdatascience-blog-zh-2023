- en: Training XGBoost On A 1TB Dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/training-xgboost-on-a-1tb-dataset-8790e2bc8672?source=collection_archive---------9-----------------------#2023-02-08](https://towardsdatascience.com/training-xgboost-on-a-1tb-dataset-8790e2bc8672?source=collection_archive---------9-----------------------#2023-02-08)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: SageMaker Distributed Training Data Parallel
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://ram-vegiraju.medium.com/?source=post_page-----8790e2bc8672--------------------------------)[![Ram
    Vegiraju](../Images/07d9334e905f710d9f3c6187cf69a1a5.png)](https://ram-vegiraju.medium.com/?source=post_page-----8790e2bc8672--------------------------------)[](https://towardsdatascience.com/?source=post_page-----8790e2bc8672--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----8790e2bc8672--------------------------------)
    [Ram Vegiraju](https://ram-vegiraju.medium.com/?source=post_page-----8790e2bc8672--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F6e49569edd2b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftraining-xgboost-on-a-1tb-dataset-8790e2bc8672&user=Ram+Vegiraju&userId=6e49569edd2b&source=post_page-6e49569edd2b----8790e2bc8672---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----8790e2bc8672--------------------------------)
    ·7 min read·Feb 8, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F8790e2bc8672&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftraining-xgboost-on-a-1tb-dataset-8790e2bc8672&user=Ram+Vegiraju&userId=6e49569edd2b&source=-----8790e2bc8672---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F8790e2bc8672&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftraining-xgboost-on-a-1tb-dataset-8790e2bc8672&source=-----8790e2bc8672---------------------bookmark_footer-----------)![](../Images/8cc115fe1d75334d1b016c71f5e63cec.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Image from [Unsplash](https://unsplash.com/photos/LqKhnDzSF-8) by [Joshua Sortino](https://unsplash.com/@sortino)
  prefs: []
  type: TYPE_NORMAL
- en: As Machine Learning continues to evolve we’re seeing [larger models](https://docs.cohere.ai/docs/introduction-to-large-language-models)
    with more and more parameters. At the same time we also see incredibly large datasets,
    at the end of the day any model is only as good as the data that it’s trained
    on. Working with large models and datasets can be computationally expensive and
    difficult to iterate or experiment on in a timely manner. For this article we’ll
    focus on the large dataset portion of the problem. Specifically we will look into
    something known as [Distributed Data Parallel](https://docs.aws.amazon.com/sagemaker/latest/dg/data-parallel.html)
    utilizing Amazon SageMaker to optimize and reduce training time across a large
    real-world dataset.
  prefs: []
  type: TYPE_NORMAL
- en: For today’s example we’ll train the [SageMaker XGBoost algorithm](https://docs.aws.amazon.com/sagemaker/latest/dg/xgboost.html)
    on an artificially generated **1 TB dataset**. In this example we’ll get a deeper
    understanding of how to prepare and structure your data source for faster training
    as well as understand how to kick off **distributed training** with SageMaker’s
    built in [Data Parallelism Library](https://docs.aws.amazon.com/sagemaker/latest/dg/data-parallel.html).
  prefs: []
  type: TYPE_NORMAL
- en: '**NOTE**: This article will assume basic knowledge of AWS and SageMaker specific
    sub-features such as SageMaker Training and interacting with SageMaker and AWS
    as a whole via the [SageMaker Python SDK and Boto3 AWS Python SDK](/sagemaker-python-sdk-vs-boto3-sdk-45c424e8e250).
    For a proper introduction and overview of SageMaker Training, I would reference
    this [article](/training-and-deploying-custom-tensorflow-models-with-aws-sagemaker-72027722ad76).'
  prefs: []
  type: TYPE_NORMAL
