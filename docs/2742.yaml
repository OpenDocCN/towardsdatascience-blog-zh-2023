- en: Solving a Leetcode Problem Using Reinforcement Learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/solving-a-leetcode-problem-using-reinforcement-learning-a-practical-introduction-to-6ec9959dd309?source=collection_archive---------8-----------------------#2023-08-29](https://towardsdatascience.com/solving-a-leetcode-problem-using-reinforcement-learning-a-practical-introduction-to-6ec9959dd309?source=collection_archive---------8-----------------------#2023-08-29)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: A practical introduction to reinforcement learning
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@pratikaher?source=post_page-----6ec9959dd309--------------------------------)[![Pratik
    Aher](../Images/5648c040ff967717c94657ebfff11e2b.png)](https://medium.com/@pratikaher?source=post_page-----6ec9959dd309--------------------------------)[](https://towardsdatascience.com/?source=post_page-----6ec9959dd309--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----6ec9959dd309--------------------------------)
    [Pratik Aher](https://medium.com/@pratikaher?source=post_page-----6ec9959dd309--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fc2e5b1d7be67&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsolving-a-leetcode-problem-using-reinforcement-learning-a-practical-introduction-to-6ec9959dd309&user=Pratik+Aher&userId=c2e5b1d7be67&source=post_page-c2e5b1d7be67----6ec9959dd309---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----6ec9959dd309--------------------------------)
    ·7 min read·Aug 29, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F6ec9959dd309&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsolving-a-leetcode-problem-using-reinforcement-learning-a-practical-introduction-to-6ec9959dd309&user=Pratik+Aher&userId=c2e5b1d7be67&source=-----6ec9959dd309---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F6ec9959dd309&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsolving-a-leetcode-problem-using-reinforcement-learning-a-practical-introduction-to-6ec9959dd309&source=-----6ec9959dd309---------------------bookmark_footer-----------)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: 'Recently, I came across a question on leetcode : [**Shortest Path in a Grid
    with Obstacles Elimination**](https://leetcode.com/problems/shortest-path-in-a-grid-with-obstacles-elimination/)**.**
    The Shortest Path in a Grid with Obstacles Elimination problem involves finding
    the shortest path from a starting cell to a target cell in a 2D grid containing
    obstacles, where you are allowed to eliminate up to k obstacles that lie along
    the path. The grid is represented by an “**m x n**” 2D array of 0s (empty cells)
    and 1s (obstacle cells).'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: The goal is to find the shortest path from the starting cell **(0, 0)** to the
    target cell **(m-1, n-1)** by traversing through empty cells while eliminating
    at most k obstacles. Eliminating an obstacle means converting the obstacle cell
    (1) to an empty cell (0) so that the path can pass through it.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/df4176ec56c6a4dc88d009915f69507e.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
- en: Shortest Path in a grid with Obstacle Elimination Example (Image by Author)
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: As I solved this problem, I realized it could provide a useful framework for
    understanding reinforcement learning principles in action. Before we jump into
    that, let’s look at how this problem is solved traditionally.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: To solve this ideally, we need to do a graph search starting from the starting
    cell while tracking the number of obstacles eliminated so far. At each step, we
    consider moving to an adjacent empty cell or eliminating an adjacent obstacle
    cell if we have eliminations left. The shortest path is the one that reaches the
    target with the fewest number of steps and by eliminating at most k obstacles.
    We can use Breadth First Search, Depth First Search to find the optimal shortest
    path efficiently.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the python function using BFS approach for this problem :'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: A little primer of Reinforcement Learning
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Reinforcement Learning (RL) is an area of Machine Learning where an **agent**
    learns a **policy** through a **reward** mechanism to accomplish a task by learning
    about its **environment**. I have always been fascinated by reinforcement learning
    because I believe this framework closely mirrors the way humans learn through
    experience. The idea is to build a learnable agent who with trial and error learns
    about the environment to solve the problem.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: 'Let dive into these topics term by term :'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/48d4351283a45d8d305ae1f54da8f312.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
- en: Environment with Agent Raya (Image by Author)
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: '**Agent** : An agent is a hypothetical entity that controls the course of action.
    You can imagine a hypothetical robot, say Agent Raya, that starts at a position
    and explores its environment. For instance, Raya has two possible options : position
    (0, 0) that goes right or to position (0, 1) that goes down, and both these positions
    have different rewards.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Environment** : The environment is the context in which our agent operates
    which is a two dimensional—a grid in this case.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**State** : State represents current situation of the player. In our case,
    it gives current position of the player and the number of remaining violations.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Reward System** : reward is the score we get as a result of taking a certain
    action. In this case : -1 points for an empty cell, +20 points for reaching the
    destination and -10 points if we run out of the number of violations, k.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/40d3c955adae9ee5f4575d4046fff47a.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
- en: Iteration Process (Image by Author)
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: Through the process of iteration, we learn a optimal policy that allows us to
    find the best possible action at each time step that also maximized the total
    reward.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: To be able to find the best policy, we used something called as a **Q function**.
    You can think of this function as a repository of all the exploration that your
    agent did so far. The agent then uses this historical information to make better
    decisions in the future that maximizes the reward.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 为了找到最佳策略，我们使用了一个叫做**Q 函数**的东西。你可以将这个函数视为代理迄今为止所有探索的储存库。代理然后利用这些历史信息在未来做出更好的决策，从而最大化奖励。
- en: Q function
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Q 函数
- en: Q(s, a) represents the expected cumulative reward an agent can achieve by taking
    action a in state s, while following policy π.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: Q(s, a) 代表代理在状态 s 下采取动作 a 并遵循策略 π 时可以获得的期望累计奖励。
- en: '![](../Images/1cbcb1c577ff27720715d05cf948e0d5.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1cbcb1c577ff27720715d05cf948e0d5.png)'
- en: Q function (Image by Author)
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: Q 函数（作者提供的图片）
- en: Where
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 其中
- en: '**π** : the policy adopted by the agent.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**π**：代理采用的策略。'
- en: '**s** : the current state.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**s**：当前状态。'
- en: '**a** : the action taken by the agent in state s.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**a**：代理在状态 s 下采取的动作。'
- en: γ is the discount factor that balances exploration and exploitation. It determines
    how much priority the agent gives to immediate versus future rewards. A factor
    closer to 0 makes the agent focus on short-term rewards, while a factor closer
    to 1 makes it focus on long-term rewards.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: γ 是平衡探索和利用的折扣因子。它决定了代理对即时奖励和未来奖励的优先级。折扣因子接近 0 会使代理关注短期奖励，而折扣因子接近 1 会使代理关注长期奖励。
- en: The agent needs to balance exploiting known high-reward actions with exploring
    unknown actions that may potentially yield higher rewards. Using a discount factor
    between 0 and 1 helps prevent the agent from getting stuck in [locally optimal
    policies](https://ai-ml-analytics.com/reinforcement-learning-exploration-vs-exploitation-tradeoff/).
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 代理需要在利用已知的高奖励动作和探索可能带来更高奖励的未知动作之间取得平衡。使用 0 到 1 之间的折扣因子有助于防止代理陷入[局部最优策略](https://ai-ml-analytics.com/reinforcement-learning-exploration-vs-exploitation-tradeoff/)。
- en: '![](../Images/f0143d06f6cf15e2fe24d2a1b07d1db8.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f0143d06f6cf15e2fe24d2a1b07d1db8.png)'
- en: Given a state the Q function returns a vector which gives a score for all actions
    (Image by Author)
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个状态，Q 函数返回一个向量，该向量为所有动作提供一个评分（作者提供的图片）
- en: Let’s now jump into the code of how the whole thing works.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们跳到整个过程如何工作的代码部分。
- en: This is how we define an agent and the variables associated with it.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是我们定义代理及其相关变量的方式。
- en: '**Reward Function** : The reward function takes in the current state and returns
    the reward obtained for that state.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '**奖励函数**：奖励函数接受当前状态并返回该状态获得的奖励。'
- en: '**Bellman Equation :**'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '**贝尔曼方程：**'
- en: How should we update the Q-table so that it has the best possible value for
    every position and action ? For an arbitrary number of iterations, the agent starts
    at position (0, 0, k), where k denotes the permitted number of violations. At
    each time step, the agent transitions to a new state by either exploring randomly
    or exploiting the learned Q-values to move greedily.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应如何更新 Q 表，以使每个位置和动作的值尽可能最佳？对于任意次数的迭代，代理从位置 (0, 0, k) 开始，其中 k 表示允许的违规次数。在每个时间步，代理通过随机探索或利用学到的
    Q 值贪婪地移动，转移到新状态。
- en: Upon arriving in a new state, we evaluate the immediate reward and update the
    Q-value for that state-action pair based on the Bellman equation. This allows
    us to iteratively improve the Q-function by incorporating new rewards into the
    historical, aggregated rewards for each state-action.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在到达新状态后，我们评估即时奖励，并根据贝尔曼方程更新该状态-动作对的 Q 值。这使我们能够通过将新奖励纳入每个状态-动作的历史累计奖励中，迭代地改进
    Q 函数。
- en: 'Here is the equation for bellman equation :'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是贝尔曼方程的方程式：
- en: '![](../Images/adda2e0d616a50d33997a93ef04005dd.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/adda2e0d616a50d33997a93ef04005dd.png)'
- en: Q-Value equation (Image by Author)
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: Q 值方程（作者提供的图片）
- en: 'This is how the process of training looks like in code :'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是训练过程在代码中的样子：
- en: '**Build Path** : For the path, we utilize the maximum Q value for each grid
    position to determine the optimal action to take at that position. The Q values
    essentially encode the best possible action to take at each location based on
    long-term rewards. For example, across all actions k at position (0,0), the maximum
    Q value corresponds to action “1”, which represents moving right. By greedily
    choosing the action with the highest Q value at each step, we can construct an
    optimal path through the grid.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '**构建路径**：对于路径，我们利用每个网格位置的最大 Q 值来确定该位置采取的最佳行动。Q 值本质上编码了基于长期奖励在每个位置应采取的最佳行动。例如，在位置
    (0,0) 的所有行动 k 中，最大 Q 值对应的行动是“1”，代表向右移动。通过在每一步贪婪地选择具有最高 Q 值的行动，我们可以构建出一条穿越网格的最优路径。'
- en: If you run the provided code, you will find it generates the path **RBRBBB**,
    which is indeed one of the shortest paths accounting for the obstacle.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你运行提供的代码，你会发现它生成的路径是 **RBRBBB**，这确实是考虑到障碍物的最短路径之一。
- en: 'Here is the link to the entire code in one file : [*shortest_path_rl.py*](https://gist.github.com/pratikaher88/9fc6fcbe18ffd8f526b38c967c66629c)'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 这是完整代码的链接，包含在一个文件中：[*shortest_path_rl.py*](https://gist.github.com/pratikaher88/9fc6fcbe18ffd8f526b38c967c66629c)
- en: Conclusion
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结论
- en: In real-world reinforcement learning scenarios, the environment the agent interacts
    with can be massive, with only sparse rewards.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在现实世界的强化学习场景中，代理与之互动的环境可能非常庞大，仅有稀疏的奖励。
- en: If you alter the configuration of the board by changing the 0s and 1s, this
    hardcoded Q-table approach would fail to generalize. Our goal is to train an agent
    that learns a general representation of grid-world configurations and can find
    optimal paths in new layouts. For the next post, I am going to replace the hardcoded
    Q-table values with a Deep Q Network (DQN). The DQN is a neural network that takes
    as input a state-action combination and the full grid layout and outputs a Q-value
    estimation. This DQN should allow the agent to find optimal paths even in new
    grid layouts it hasn’t encountered during training.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你通过更改 0 和 1 来改变棋盘的配置，这种硬编码的 Q 表方法将无法推广。我们的目标是训练一个代理，使其学习网格世界配置的通用表示，并能够在新的布局中找到最优路径。在下一篇文章中，我将用深度
    Q 网络 (DQN) 替换硬编码的 Q 表值。DQN 是一个神经网络，它接受状态-行动组合和完整网格布局作为输入，并输出 Q 值估计。这个 DQN 应该能让代理即使在训练过程中未遇到的新网格布局中也能找到最优路径。
- en: 'Reach out to me on LinkedIn if you want to have a quick chat and want to connect
    : [https://www.linkedin.com/in/pratikdaher/](https://www.linkedin.com/in/pratikdaher/)'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想快速聊聊并建立联系，可以通过 LinkedIn 联系我：[https://www.linkedin.com/in/pratikdaher/](https://www.linkedin.com/in/pratikdaher/)
