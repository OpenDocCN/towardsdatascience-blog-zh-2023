- en: 'Private GPT: Fine-Tune LLM on Enterprise Data'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/private-gpt-fine-tune-llm-on-enterprise-data-7e663d808e6a?source=collection_archive---------0-----------------------#2023-07-05](https://towardsdatascience.com/private-gpt-fine-tune-llm-on-enterprise-data-7e663d808e6a?source=collection_archive---------0-----------------------#2023-07-05)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Doing cool things with data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://priya-dwivedi.medium.com/?source=post_page-----7e663d808e6a--------------------------------)[![Priya
    Dwivedi](../Images/73087cb699750466312cc4752e2044d4.png)](https://priya-dwivedi.medium.com/?source=post_page-----7e663d808e6a--------------------------------)[](https://towardsdatascience.com/?source=post_page-----7e663d808e6a--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----7e663d808e6a--------------------------------)
    [Priya Dwivedi](https://priya-dwivedi.medium.com/?source=post_page-----7e663d808e6a--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fb040ce924438&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fprivate-gpt-fine-tune-llm-on-enterprise-data-7e663d808e6a&user=Priya+Dwivedi&userId=b040ce924438&source=post_page-b040ce924438----7e663d808e6a---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----7e663d808e6a--------------------------------)
    ·9 min read·Jul 5, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F7e663d808e6a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fprivate-gpt-fine-tune-llm-on-enterprise-data-7e663d808e6a&user=Priya+Dwivedi&userId=b040ce924438&source=-----7e663d808e6a---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F7e663d808e6a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fprivate-gpt-fine-tune-llm-on-enterprise-data-7e663d808e6a&source=-----7e663d808e6a---------------------bookmark_footer-----------)![](../Images/b1c22ad1ee60a9ba1c68c1dc31d44dc6.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Photo by [Robynne Hu](https://unsplash.com/@robynnexy?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    on [Unsplash](https://unsplash.com/s/photos/technology?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
  prefs: []
  type: TYPE_NORMAL
- en: '**Introduction**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the era of big data and advanced artificial intelligence, language models
    have emerged as formidable tools capable of processing and generating human-like
    text. Large Language Models like ChatGPT are general-purpose bots capable of having
    conversations on many topics. However, LLMs can also be fine-tuned on domain-specific
    data making them more accurate and on-point on domain-specific enterprise questions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Many industries and applications will require a fine-tuned LLMs. Reasons include:'
  prefs: []
  type: TYPE_NORMAL
- en: Better performance from a chatbot trained on specific data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: OpenAI models like chatgpt are a black box and companies may be hesitant to
    share their confidential data over an API
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ChatGPT API costs may be prohibitive for large applications
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The challenge with fine-tuning an LLM is that the process is unknown and the
    computational resources required to train a billion-parameter model without optimizations
    can be prohibitive.
  prefs: []
  type: TYPE_NORMAL
- en: Fortunately, a lot of research has been done on training techniques that allow
    us now to fine-tune LLMs on smaller GPUs.
  prefs: []
  type: TYPE_NORMAL
