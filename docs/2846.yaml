- en: Highlights on Large Language Models at KDD 2023
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/highlights-on-large-language-models-at-kdd-2023-fc53440563c3?source=collection_archive---------3-----------------------#2023-09-11](https://towardsdatascience.com/highlights-on-large-language-models-at-kdd-2023-fc53440563c3?source=collection_archive---------3-----------------------#2023-09-11)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Were you not able to attend KDD? Learn from my summary about the hottest topic
    at the conference: LLMs'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://gspmoreira.medium.com/?source=post_page-----fc53440563c3--------------------------------)[![Gabriel
    Moreira](../Images/f191c90e2310c4f105b4287bb27178ee.png)](https://gspmoreira.medium.com/?source=post_page-----fc53440563c3--------------------------------)[](https://towardsdatascience.com/?source=post_page-----fc53440563c3--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----fc53440563c3--------------------------------)
    [Gabriel Moreira](https://gspmoreira.medium.com/?source=post_page-----fc53440563c3--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F351cff053b19&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhighlights-on-large-language-models-at-kdd-2023-fc53440563c3&user=Gabriel+Moreira&userId=351cff053b19&source=post_page-351cff053b19----fc53440563c3---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----fc53440563c3--------------------------------)
    ·8 min read·Sep 11, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Ffc53440563c3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhighlights-on-large-language-models-at-kdd-2023-fc53440563c3&user=Gabriel+Moreira&userId=351cff053b19&source=-----fc53440563c3---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ffc53440563c3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhighlights-on-large-language-models-at-kdd-2023-fc53440563c3&source=-----fc53440563c3---------------------bookmark_footer-----------)![](../Images/701204186bd64ea7fcdac60a750ddbdc.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Image by the author
  prefs: []
  type: TYPE_NORMAL
- en: A few weeks ago I had the opportunity to attend ACM SIGKDD (short for KDD) for
    the first time. [KDD 2023](https://kdd.org/kdd2023/) took place in Long Beach,
    CA, and is the oldest and most important academic conference in the data mining
    field, pioneering topics related to data science and big data.
  prefs: []
  type: TYPE_NORMAL
- en: It spanned 5 days and was attended by over 2,200 people, with a strong presence
    of attendees from industry. I was impressed by the diversity of topics covered,
    but the hot ones from my perspective were Large Language Models (LLMs) and Graph
    Learning. Also found lots of content on RecSys, for which I have some special
    attention.
  prefs: []
  type: TYPE_NORMAL
- en: In this post, I summarize my highlights about LLMs from workshops, tutorials,
    and paper presentations that I attended and liked, with links to online resources
    for additional information.
  prefs: []
  type: TYPE_NORMAL
- en: '*Warning*: long post full of resource links ahead!'
  prefs: []
  type: TYPE_NORMAL
- en: The LLM Revolution keynote (Ed Chi - Google)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Ed H. Chi, a distinguished scientist, and director at Google, presented at a
    much-awaited keynote on [The LLM Revolution](https://www.linkedin.com/posts/gabrielspmoreira_kdd23-activity-7094719988606935041-2VPH?utm_source=share&utm_medium=member_desktop).
    He reflected on the tech revolutions we faced from the internet, through mobile
    devices, the rise of deep learning, and now LLMs, which is by far mind-blowing.
  prefs: []
  type: TYPE_NORMAL
- en: He talked about what makes human intelligence different from ML - (1) learning
    from a few examples, (2) explaining their predictions/decisions, (3) strong out-of-distribution
    generalization abilities — and how LLM can finally start filling this gap.
  prefs: []
  type: TYPE_NORMAL
- en: 'He then talked about the techniques that are making LLM able to perform some
    reasoning: (1) chain-of-thought prompting, (2) self-consistency, (3) least-to-most
    prompting, and (4) instruction fine-tuning. More on this in the talk of Denny
    Zhou on the LLM day (next section).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, he shared his vision on what are the next challenges for LLMs: (1)
    responsibility and safety, (2) Factuality, Grounding, and Attribution, (3) Human
    <-> AI content loop an Ecosystem, and (4) Personalization and user memory.'
  prefs: []
  type: TYPE_NORMAL
- en: LLM Day
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: KDD devoted a special day to LLM, with 5 distinct researchers giving longer
    talks about how Microsoft, Google DeepMind, Meta, Zhipu AI, and OpenAI have been
    pushing forward the LLM technology, challenges, and what they foresee as the future
    evolution in this area. The [presentation slides](https://bigmodel.ai/llmday-kdd23/)
    are available and are highly recommended.
  prefs: []
  type: TYPE_NORMAL
- en: '**From Documents to Dialogues: How LLMs are Shaping the Future of Work (Jaime
    Teevan — Microsoft)** ([slides](https://lfs.aminer.cn/misc/teevan-kdd2023.pdf))'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The talk covered different research and applied topics on LLM quality issues
    (e.g. how to deal with low-resourced languages), efficient training on the cloud,
    Retrieval-Augmented Generation (RAG) as a sustainable way to leverage private
    Knowledge Bases (KB), differential privacy for fine-tuning, good practices on
    prompt engineering and chat log analysis.
  prefs: []
  type: TYPE_NORMAL
- en: '**Teach language models to reason (Denny Zhou - Google DeepMind) (**[**slides**](http://dennyzhou.github.io/Teach-LLMs-to-Reason-KDD-2023.pdf)**)**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'He focused on the holy grail for ML — Reasoning — as a way to learn from only
    a few examples. Some core techniques that make LLMs so powerful were summarized:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Chain-of-Thought (CoT)** — prompting technique to think step by step, providing
    some few-shot examples that outline the reasoning process.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Least-to-most prompting** (planning + reasoning) — Decompose a complex problem
    into a list of subproblems, which are sequentially solved'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[**Self-consistency (SC) decoding**](https://arxiv.org/abs/2203.11171) — a
    technique where different answers are generated from sampled diverse reasoning
    paths. Instead of taking the greedy answer, the final answer is the majority vote
    of those different answers. This technique seems to work pretty well for LLMs
    and reminds me of the power of model ensembles!'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Instruction Tuning** —Process of fine-tuning pre-trained LLMs to follow instructions.
    It enables zero-shot prompting for new tasks. This is essential to enable question-answering
    systems like Google Bard or Open.ai ChatGPT.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Llama 2: Open Foundation and Fine-Tuned Chat Models (Vedanuj Goswami - Meta
    FAIR) (**[**slides**](https://lfs.aminer.cn/misc/Llama%202%20at%20KDD%20LLM%20Final.pdf)**)**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'He presented Meta journey in training Llama foundation models and fine-tuning
    with instructions using SFT data (high-quality 27k collected samples). Their reward
    model was trained on 1M collected samples. He also describes their Iterative Finetuning
    with RLHF, evaluation (human, safety). He finished the presentation by talking
    about challenges ahead training and deploying LLMs, which I transcribed here:'
  prefs: []
  type: TYPE_NORMAL
- en: Getting more data, multilingual, multimodal
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scaling to 1000s of GPUs with high MFU (Model FLOPs Utilization)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Designing Architectures Efficient for training and inference, Hardware-Software
    Co-Design
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Continual Learning and Updating Knowledge
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Improving Factuality and Citing Sources
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reducing Hallucinations and Admitting Uncertainty
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Removing harmful, offensive, or biased content
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Adapting to world knowledge beyond training data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**From GLM-130B to ChatGLM (Peng Zhang - Zhipu AI) (**[**slides**](https://lfs.aminer.cn/misc/kdd23-chatglm-peng-0808.pdf)**)**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: I got to know Zhipu AI, a company that is challenging Open.ai for the Chinese
    language. They had a strong presence at KDD as the Diamond Sponsor and delivered
    a keynote at the Banquet Celebration. Zhipu presented results that show that they
    are the best LLM for Chinese in many tasks, even better than GPT-4\. They described
    how they developed ChatGLM and VisualGLRM on top of their base model (GLM-130B).
    They open-sourced the [ChatGLM-6B on HuggingFace](https://huggingface.co/THUDM/chatglm-6b).
  prefs: []
  type: TYPE_NORMAL
- en: '**The large language model renaissance: paradigms and challenges (Jason Wei
    — OpenAI) (**[**slides**](https://docs.google.com/presentation/d/1BF0icnNER3Fy4v-9RSm0fKqXx3QuvU0dIi6mp6VpZHg/edit?pli=1#slide=id.g16197112905_0_0)**)**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Very grounded talk on the **scaling laws** for reaching the current state of
    LLMs and the **emerging abilities** (including reasoning) that can be observed
    when LLMs get higher than 100B parameters. Also talked about **reasoning via prompting
    techniques**: Chain-of-Thought and Least-to-most prompting.'
  prefs: []
  type: TYPE_NORMAL
- en: Foundations and Applications in Large-scale AI Models Pre-training, Fine-tuning,
    and Prompt-based Learning workshop
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: I think that the [LLM-AI workshop](https://llm-ai.github.io/llmai/) was the
    most disputed one at the conference. I literally could not join it in the morning,
    as a crowd had completely filled the small room right after the KDD morning keynote.
    Fortunately, I could find a seat right after the coffee break and could attend
    a few sessions.
  prefs: []
  type: TYPE_NORMAL
- en: '**NLP Research in the Era of LLMs (Shafiq Joty - Salesforce)**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: He described SalesForce’s XGen LLM — an in-house JaxFormer library, which follows
    LLaMA-7B and is instruction-tuned with WizardLM, which can answer questions based
    on unstructured and structured data (e.g. Spark and SQL databases). Also presented
    some techniques they use for reasoning preparation, for breaking down the questions
    with [Chain-of-Thought](https://arxiv.org/pdf/2305.13269.pdf), and for selecting
    the most relevant knowledge base by training a model for Adaptive Query Generation
    with LoRA on natural sentences, SPARQL, and SQL. That process generates a query
    for each reasoning step, which is executed on the knowledge source.
  prefs: []
  type: TYPE_NORMAL
- en: '**Modular Large Language Model and Principle-Driven alignment with Minimal
    Human Supervision (YiKang Shen — IBM)**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This talk introduced IBM’s foundation models: (1) ***Sandstone*** — encoder-decoder
    architecture well suited for fine-tuning for specific tasks, (2) ***Granite***—
    decoder-only, GPT-like for generative tasks, (3) ***Obsidian*** — A new modular
    architecture providing high inference efficiency and levels of performance across
    a variety of tasks'
  prefs: []
  type: TYPE_NORMAL
- en: 'He also described some challenges they faced with LLM:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Efficiency** — how to train and serve Llama 65B models.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Extendability** — how to update the LLM with the growing training corpus,
    different languages, customers’ private data'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Flexibility** — be able to use different complexity LLM models on different
    devices, with different latency requirements'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: They present their [ModuleFormer](https://arxiv.org/pdf/2306.04640.pdf), which
    addresses the above problems powered by a Sparse Mixture of Experts (SMoE). It
    can activate only a subset of its modules for each input and is more immune to
    catastrophic forgetting than dense LLMs. And finetuning ModuleFormer can specialize
    a subset of the modules and the task-unrelated modules can be pruned for lightweight
    deployment.
  prefs: []
  type: TYPE_NORMAL
- en: '**Tutorials**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: These tutorials were presented at the same time, so I had to split my time to
    catch a bit of both. Fortunately, their great slides were made available and are
    very detailed.
  prefs: []
  type: TYPE_NORMAL
- en: '**Towards Next-Generation Intelligent Assistants Leveraging LLM Techniques
    — Meta (**[**slides**](https://drive.google.com/file/d/1DlItXlC17nPpuMxj0STrIVnNJX1B30SF/view)**)**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Very comprehensive tutorial on Intelligent Assistants that are multi-modal
    and can leverage as context the user location, what users can hear and see (e.g.
    using Google Glasses, Meta Quest 2). The tutorial described how the different
    modules are connected: ASR, CV, NLU, Dialog State Tracker, NLG, TTS, KB, Personalization/
    Recommendation, and Privacy-preserving, among others.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Pretrained Language Representations for Text Understanding: A Weakly-Supervised
    Perspective - University of Illinois at Urbana-Champaign (**[**slides**](https://yumeng5.github.io/kdd23-tutorial/)**)**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Covered advances in pre-training language models, compared them to traditional
    NLU tasks and described how LLMs can be used to extract entities and hierarchical
    relations, topics discovery, and document understanding. A good insight I got
    from this tutorial was using some NLU techniques to evaluate if a generated answer
    addresses the question.
  prefs: []
  type: TYPE_NORMAL
- en: Research Papers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Here is a short list of some NLP / LLM papers I enjoyed.
  prefs: []
  type: TYPE_NORMAL
- en: End-to-End Query Term Weighting (Google) ([paper](https://dl.acm.org/doi/10.1145/3580305.3599815))
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This great paper combines lexical and semantic retrieval systems. They build
    their solution on top of lexical retrievers by proposing a Term Weighting BERT
    (TW-BERT) model. TW-BERT learns to predict the weight for individual n-gram (e.g.,
    uni-grams and bi-grams) query input terms. These inferred weights and terms can
    be used directly by a retrieval system to perform a query search. The learned
    weights can be easily utilized by standard lexical retrievers (e.g. BM25) and
    by other retrieval techniques such as query expansion.
  prefs: []
  type: TYPE_NORMAL
- en: 'UnifieR: A Unified Retriever for Large-Scale Retrieval (Microsoft) ([paper](https://dl.acm.org/doi/10.1145/3580305.3599927))'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Another interesting proposal is to unify dense-vector and lexicon-based retrieval
    in one model with a dual-representing capability. It is trained with a two-stage
    self-learning pipeline and improves upon state-of-the-art lexical and dense retrieval
    models.
  prefs: []
  type: TYPE_NORMAL
- en: Learning to Relate to Previous Turns in Conversational Search ([paper](https://dl.acm.org/doi/10.1145/3580305.3599411))
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Typically in multi-turn conversations, the historical queries are used to expand
    the current query. However, not all previous queries are related to or useful
    to expand the next question. The paper proposes a method for selecting relevant
    historical queries that are useful for the current query. They use a pseudo-labeling
    mechanism to annotate the relevant historical queries and train a selection model
    together with the retriever training
  prefs: []
  type: TYPE_NORMAL
- en: 'GLM-Dialog: Noise-tolerant Pre-training for Knowledge-grounded Dialogue Generation
    ([paper](https://dl.acm.org/doi/10.1145/3580305.3599832))'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Describes how noisy private KB can be used by RAG-based dialogs. They propose
    a novel evaluation method to allow humans to converse with multiple deployed bots
    simultaneously and compare their performance implicitly instead of explicitly
    rating using multidimensional metrics.
  prefs: []
  type: TYPE_NORMAL
- en: '**Cluster Language Model for Improved E-Commerce Retrieval and Ranking: Leveraging
    Query Similarity and Fine-Tuning for Personalized Results (Home Depot) (**[**paper**](https://drive.google.com/file/d/1J3db1I59IvsaKFr4Q-3PTaLU49XLHo4b/view)**)**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This paper describes how Home Depot is improving semantic search for e-commerce
    by using a cluster-specific language model instead of the typical bi-encoder architecture.
    Their method first maps the user query to clusters using K-Means and uses the
    selected cluster-specific language model for retrieval.
  prefs: []
  type: TYPE_NORMAL
- en: '**Conclusion**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: These were my highlights on LLMs from KDD 2023\. I hope you can find some useful
    information and inspiration from this summary and the resources I compiled.
  prefs: []
  type: TYPE_NORMAL
- en: '*“Sorry for the long [post]. If I had more time, I would have written a shorter
    [one]”* :)'
  prefs: []
  type: TYPE_NORMAL
