- en: How Self-RAG Could Revolutionize Industrial LLMs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/how-self-rag-could-revolutionize-industrial-llms-b33d9f810264?source=collection_archive---------4-----------------------#2023-11-14](https://towardsdatascience.com/how-self-rag-could-revolutionize-industrial-llms-b33d9f810264?source=collection_archive---------4-----------------------#2023-11-14)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Let’s face it — vanilla RAG is pretty dumb. There’s no guarantee responses returned
    are relevant. Learn how Self-RAG can significantly help
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://skanda-vivek.medium.com/?source=post_page-----b33d9f810264--------------------------------)[![Skanda
    Vivek](../Images/9d25bee2fb75176ca7f7ea6eff7d7ab5.png)](https://skanda-vivek.medium.com/?source=post_page-----b33d9f810264--------------------------------)[](https://towardsdatascience.com/?source=post_page-----b33d9f810264--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----b33d9f810264--------------------------------)
    [Skanda Vivek](https://skanda-vivek.medium.com/?source=post_page-----b33d9f810264--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F220d9bbb8014&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-self-rag-could-revolutionize-industrial-llms-b33d9f810264&user=Skanda+Vivek&userId=220d9bbb8014&source=post_page-220d9bbb8014----b33d9f810264---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----b33d9f810264--------------------------------)
    ·7 min read·Nov 14, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fb33d9f810264&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-self-rag-could-revolutionize-industrial-llms-b33d9f810264&user=Skanda+Vivek&userId=220d9bbb8014&source=-----b33d9f810264---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb33d9f810264&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-self-rag-could-revolutionize-industrial-llms-b33d9f810264&source=-----b33d9f810264---------------------bookmark_footer-----------)![](../Images/69621ace143918aee012edf5e1d62323.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Self-RAG Demo | Skanda Vivek
  prefs: []
  type: TYPE_NORMAL
- en: Large language models (LLMs) are all set to revolutionize various industries.
    Let’s take the example of the financial sector, wherein LLMs can be used to pore
    over troves of documents and find trends in a fraction of time and at a fraction
    of the cost of analysts doing the same task. But here’s the catch — the answers
    you get are only partial and incomplete many times. Take, for example, the case
    where you have a document containing company X’s annual revenue over the past
    15 years, but in different sections. In the standard Retrieval Augmented Generation
    (RAG) architecture as pictured below, you typically retrieve the top-k documents,
    or choose documents within a fixed context length.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4041cfd00e99aed333dcc5090fda5094.png)'
  prefs: []
  type: TYPE_IMG
- en: RAG Prototype | Skanda Vivek
  prefs: []
  type: TYPE_NORMAL
- en: However, this can have several issues. One issue is wherein the top-k documents
    do not contain all the answers — maybe for example only corresponding to the last
    5 or 10 years. The other issue is that computing similarity between document chunks
    and prompt…
  prefs: []
  type: TYPE_NORMAL
