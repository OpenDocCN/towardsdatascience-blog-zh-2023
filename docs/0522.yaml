- en: How to Train a Word2Vec Model from Scratch with Gensim
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/how-to-train-a-word2vec-model-from-scratch-with-gensim-c457d587e031?source=collection_archive---------1-----------------------#2023-02-06](https://towardsdatascience.com/how-to-train-a-word2vec-model-from-scratch-with-gensim-c457d587e031?source=collection_archive---------1-----------------------#2023-02-06)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*In this article we will explore Gensim, a very popular Python library for
    training text-based machine learning models, to train a Word2Vec model from scratch*'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@theDrewDag?source=post_page-----c457d587e031--------------------------------)[![Andrea
    D''Agostino](../Images/58c7c218815f25278aae59cea44d8771.png)](https://medium.com/@theDrewDag?source=post_page-----c457d587e031--------------------------------)[](https://towardsdatascience.com/?source=post_page-----c457d587e031--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----c457d587e031--------------------------------)
    [Andrea D''Agostino](https://medium.com/@theDrewDag?source=post_page-----c457d587e031--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F4e8f67b0b09b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-train-a-word2vec-model-from-scratch-with-gensim-c457d587e031&user=Andrea+D%27Agostino&userId=4e8f67b0b09b&source=post_page-4e8f67b0b09b----c457d587e031---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----c457d587e031--------------------------------)
    ·9 min read·Feb 6, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fc457d587e031&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-train-a-word2vec-model-from-scratch-with-gensim-c457d587e031&user=Andrea+D%27Agostino&userId=4e8f67b0b09b&source=-----c457d587e031---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc457d587e031&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-train-a-word2vec-model-from-scratch-with-gensim-c457d587e031&source=-----c457d587e031---------------------bookmark_footer-----------)![](../Images/f5bacd1ddee7fde33777e0a5a0db4449.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Image by author.
  prefs: []
  type: TYPE_NORMAL
- en: Word2Vec is a machine learning algorithm that allows you to create vector representations
    of words.
  prefs: []
  type: TYPE_NORMAL
- en: These representations, called **embeddings**, are used in many natural language
    processing tasks, such as word clustering, classification, and text generation.
  prefs: []
  type: TYPE_NORMAL
- en: The Word2Vec algorithm marked the beginning of an era in the NLP world when
    it was first introduced by Google in 2013.
  prefs: []
  type: TYPE_NORMAL
- en: It is based on word representations created by a neural network trained on very
    large data corpuses.
  prefs: []
  type: TYPE_NORMAL
- en: '**The output of Word2Vec are vectors**, one for each word in the training dictionary,
    **that effectively capture relationships between words.**'
  prefs: []
  type: TYPE_NORMAL
- en: Vectors that are close together in vector space have similar meanings based
    on context, and vectors that are far apart have different meanings. For example,
    the words “strong” and “mighty” would be close together while “strong” and “Paris”
    would be relatively far away within the vector space.
  prefs: []
  type: TYPE_NORMAL
