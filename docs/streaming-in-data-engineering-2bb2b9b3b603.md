# æ•°æ®å·¥ç¨‹ä¸­çš„æµæ•°æ®

> åŸæ–‡ï¼š[`towardsdatascience.com/streaming-in-data-engineering-2bb2b9b3b603`](https://towardsdatascience.com/streaming-in-data-engineering-2bb2b9b3b603)

## æµæ•°æ®ç®¡é“å’Œå®æ—¶åˆ†æ

[](https://mshakhomirov.medium.com/?source=post_page-----2bb2b9b3b603--------------------------------)![ğŸ’¡Mike Shakhomirov](https://mshakhomirov.medium.com/?source=post_page-----2bb2b9b3b603--------------------------------)[](https://towardsdatascience.com/?source=post_page-----2bb2b9b3b603--------------------------------)![Towards Data Science](https://towardsdatascience.com/?source=post_page-----2bb2b9b3b603--------------------------------) [ğŸ’¡Mike Shakhomirov](https://mshakhomirov.medium.com/?source=post_page-----2bb2b9b3b603--------------------------------)

Â·å‘è¡¨äº[Towards Data Science](https://towardsdatascience.com/?source=post_page-----2bb2b9b3b603--------------------------------) Â·é˜…è¯»æ—¶é—´ 9 åˆ†é’ŸÂ·2023 å¹´ 12 æœˆ 12 æ—¥

--

![](img/b5bdd62c71b5b0a4888786ad3318772d.png)

å›¾ç‰‡ç”±[DESIGNECOLOGIST](https://unsplash.com/@designecologist?utm_source=medium&utm_medium=referral)æä¾›ï¼Œåœ¨[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)ä¸Š

**æµæ•°æ®**æ˜¯æœ€å—æ¬¢è¿çš„æ•°æ®ç®¡é“è®¾è®¡æ¨¡å¼ä¹‹ä¸€ã€‚å°†äº‹ä»¶ä½œä¸ºå•ä¸ªæ•°æ®ç‚¹åˆ›å»ºäº†ä»ä¸€ä¸ªç‚¹åˆ°å¦ä¸€ä¸ªç‚¹çš„æŒç»­æ•°æ®æµï¼Œä»è€Œä¸ºå®æ—¶æ•°æ®æ‘„å–å’Œåˆ†ææä¾›äº†æœºä¼šã€‚å¦‚æœä½ æƒ³äº†è§£æ•°æ®æµå¹¶å­¦ä¹ å¦‚ä½•æ„å»ºå®æ—¶æ•°æ®ç®¡é“ï¼Œè¿™ç¯‡æ–‡ç« é€‚åˆä½ ã€‚äº†è§£å¦‚ä½•æµ‹è¯•è§£å†³æ–¹æ¡ˆï¼Œå¹¶æ¨¡æ‹Ÿäº‹ä»¶æµçš„æµ‹è¯•æ•°æ®ã€‚è¿™ç¯‡æ–‡ç« æ˜¯ä¸€ä¸ªç»ä½³çš„æœºä¼šï¼Œè®©ä½ æŒæ¡ä¸€äº›å—æ¬¢è¿çš„æ•°æ®å·¥ç¨‹æŠ€èƒ½ï¼Œä½¿ç”¨æµè¡Œçš„æµå¤„ç†å·¥å…·å’Œæ¡†æ¶ï¼Œå³ Kinesisã€Kafka å’Œ Sparkã€‚æˆ‘æƒ³è°ˆè°ˆæ•°æ®æµçš„å¥½å¤„ã€ç¤ºä¾‹å’Œç”¨ä¾‹ã€‚

## æ•°æ®æµç©¶ç«Ÿæ˜¯ä»€ä¹ˆï¼Ÿ

æµæ•°æ®ï¼Œä¹Ÿç§°ä¸ºäº‹ä»¶æµå¤„ç†ï¼Œæ˜¯ä¸€ç§æ•°æ®ç®¡é“è®¾è®¡æ¨¡å¼ï¼Œå½“æ•°æ®ç‚¹ä¸æ–­ä»æºå¤´æµå‘ç›®çš„åœ°æ—¶ä½¿ç”¨ã€‚è¿™å¯ä»¥å®æ—¶å¤„ç†ï¼Œä½¿å®æ—¶åˆ†æåŠŸèƒ½èƒ½å¤Ÿå¿«é€Ÿå¯¹æ•°æ®æµå’Œåˆ†æäº‹ä»¶ä½œå‡ºååº”ã€‚ç”±äºæµå¤„ç†ï¼Œåº”ç”¨ç¨‹åºå¯ä»¥å¯¹æ–°æ•°æ®äº‹ä»¶è§¦å‘å³æ—¶å“åº”ï¼Œé€šå¸¸è¿™å°†æ˜¯å¤„ç†ä¼ä¸šçº§æ•°æ®æœ€å—æ¬¢è¿çš„è§£å†³æ–¹æ¡ˆä¹‹ä¸€ã€‚

> åªè¦åœ¨ç‚¹ A å’Œç‚¹ B ä¹‹é—´è¿›è¡Œæ•°æ®å¤„ç†ï¼Œå°±ä¼šæœ‰ä¸€ä¸ªæ•°æ®ç®¡é“ [1]ã€‚

![](img/334d567596ab164146151fe1c0d7a31c.png)

æµæ•°æ®ç®¡é“ç¤ºä¾‹ã€‚å›¾ç‰‡ç”±ä½œè€…æä¾›

åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥åˆ›å»ºä¸€ä¸ª**ELT æµå¤„ç†**æ•°æ®ç®¡é“åˆ°**AWS Redshift**ã€‚AWS **Firehose delivery stream**å¯ä»¥æä¾›è¿™ç§æ— ç¼é›†æˆï¼Œå°†æ•°æ®ç›´æ¥åˆ›å»ºåˆ°æ•°æ®ä»“åº“è¡¨ä¸­ã€‚ç„¶åï¼Œæ•°æ®å°†è¢«è½¬åŒ–ï¼Œä»¥ä½¿ç”¨**AWS Quicksight**ä½œä¸º BI å·¥å…·ç”ŸæˆæŠ¥å‘Šã€‚

å‡è®¾æˆ‘ä»¬éœ€è¦åˆ›å»ºä¸€ä¸ªæŠ¥å‘Šä»ªè¡¨ç›˜æ¥å±•ç¤ºå…¬å¸ä¸­çš„æ”¶å…¥æ¥æºã€‚åœ¨è®¸å¤šåœºæ™¯ä¸­ï¼Œä¸šåŠ¡éœ€æ±‚æ˜¯å®æ—¶ç”Ÿæˆæ´å¯Ÿã€‚è¿™æ­£æ˜¯æˆ‘ä»¬éœ€è¦ä½¿ç”¨**æµå¤„ç†**çš„æƒ…å†µã€‚

æ•°æ®æµå¯ä»¥ç”±å„ç§æ•°æ®æºç”Ÿæˆï¼Œä¾‹å¦‚ç‰©è”ç½‘ã€æœåŠ¡å™¨æ•°æ®æµã€è¥é”€åº”ç”¨å†…äº‹ä»¶ã€ç”¨æˆ·æ´»åŠ¨ã€æ”¯ä»˜äº¤æ˜“ç­‰ã€‚è¿™äº›æ•°æ®å¯ä»¥ä»¥ä¸åŒæ ¼å¼æµåŠ¨ï¼Œå¹¶ä¸”ç»å¸¸å˜åŒ–ã€‚æµå¤„ç†æ¨¡å¼çš„ç†å¿µæ˜¯å®æ—¶åº”ç”¨ ETL å¹¶æ— ç¼å¤„ç†äº‹ä»¶æµã€‚

æ¯å½“æˆ‘ä»¬éœ€è¦å¤„ç†æ¯«ç§’çº§çš„æ•°æ®å»¶è¿Ÿæ—¶ï¼Œæµå¤„ç†æ˜¯æ­£ç¡®çš„é€‰æ‹©ã€‚

è€ƒè™‘ä¸‹é¢çš„ä¾‹å­ä»¥æ›´å¥½åœ°ç†è§£å®ƒã€‚æ‰€æœ‰åº”ç”¨ç¨‹åºéƒ½ä½¿ç”¨ OLTP æ•°æ®åº“[4]ï¼Œä¾‹å¦‚ MySQLã€‚ä½ çš„åº”ç”¨ç¨‹åºä¹Ÿæ˜¯å…¶ä¸­ä¹‹ä¸€ï¼Œä½†ä½ éœ€è¦å°†è¿™äº›æ•°æ®å­˜å‚¨åˆ°æ•°æ®ä»“åº“è§£å†³æ–¹æ¡ˆï¼ˆDWHï¼‰ï¼Œå³ Snowflake æˆ– BigQueryã€‚

[## æ•°æ®å»ºæ¨¡ä¸ºæ•°æ®å·¥ç¨‹å¸ˆ](https://example.org/data-modelling-for-data-engineers-93d058efa302?source=post_page-----2bb2b9b3b603--------------------------------)

### åˆå­¦è€…çš„ç»ˆææŒ‡å—

[towardsdatascience.com](https://example.org/data-modelling-for-data-engineers-93d058efa302?source=post_page-----2bb2b9b3b603--------------------------------)

ä½¿ç”¨æ‰¹é‡æ•°æ®ç®¡é“è§£å†³æ–¹æ¡ˆï¼Œæˆ‘ä»¬å¯èƒ½å¸Œæœ›ä» MySQL åŠ è½½åˆ° DWH ä¸€æ¬¡/æ¯å¤©/æ¯å°æ—¶/æ¯äº”åˆ†é’Ÿç­‰ã€‚æµå¤„ç†åˆ™ç›¸åï¼Œå®ƒå°†ä½¿ç”¨ä¸“ç”¨ç³»ç»Ÿï¼Œä¾‹å¦‚ Kafka Connectã€‚å®ƒä¼šåœ¨æ•°æ®è¿›å…¥æ•°æ®åº“æ—¶ç«‹å³å¤„ç†æ•°æ®ã€‚

## æµè¡Œçš„æ•°æ®æµå·¥å…·

è®©æˆ‘ä»¬æ·±å…¥äº†è§£è¿‡å»å‡ å¹´ä¸­è¢«è¯æ˜æœ€æœ‰ç”¨çš„æµæ•°æ®å¹³å°å’Œæ¡†æ¶ã€‚

+   **Apache Spark** â€” ç”¨äºå¤§è§„æ¨¡åˆ†æå’Œå¤æ‚æ•°æ®è½¬æ¢çš„åˆ†å¸ƒå¼æ•°æ®è®¡ç®—æ¡†æ¶

+   **Apache Kafka** â€” ä¸€ä¸ªå®æ—¶æ•°æ®ç®¡é“å·¥å…·ï¼Œå…·æœ‰åˆ†å¸ƒå¼æ¶ˆæ¯ç³»ç»Ÿç”¨äºåº”ç”¨ç¨‹åº

+   **AWS Kinesis** â€” ä¸€ä¸ªç”¨äºåˆ†æå’Œåº”ç”¨ç¨‹åºçš„å®æ—¶æµå¹³å°

+   **Google Cloud Dataflow** â€” è°·æ­Œçš„å®æ—¶äº‹ä»¶å¤„ç†å’Œåˆ†æç®¡é“çš„æµå¹³å°

+   **Apache Flink** â€” ä¸€ä¸ªåˆ†å¸ƒå¼æµæ•°æ®å¹³å°ï¼Œæ—¨åœ¨è¿›è¡Œä½å»¶è¿Ÿæ•°æ®å¤„ç†ã€‚

å‡ ä¹æ‰€æœ‰è¿™äº›å¹³å°éƒ½æœ‰å®ƒä»¬çš„æ‰˜ç®¡äº‘æœåŠ¡ï¼ˆå¦‚ AWS Kinesisã€Google Cloud Dataflowï¼‰ï¼Œå¹¶ä¸”å¯ä»¥ä¸å…¶ä»–æœåŠ¡ï¼ˆå¦‚å­˜å‚¨ï¼ˆS3ï¼‰ã€é˜Ÿåˆ—ï¼ˆSQSã€pub/subï¼‰ã€æ•°æ®ä»“åº“ï¼ˆRedshiftï¼‰æˆ– AI å¹³å°ï¼‰æ— ç¼é›†æˆã€‚

æ‰€æœ‰è¿™äº›å·¥å…·éƒ½å¯ä»¥éƒ¨ç½²åœ¨ Kubernetesã€Docker æˆ– Hadoop ä¸Šï¼Œæ—¨åœ¨è§£å†³ä¸€ä¸ªé—®é¢˜â€”â€”å¤„ç†é«˜å®¹é‡å’Œé«˜é€Ÿåº¦çš„æ•°æ®æµã€‚

## æ•°æ®æµçš„å¥½å¤„

æµæ•°æ®ç®¡é“è®¾è®¡æ¨¡å¼å¸®åŠ©ç»„ç»‡ä¸»åŠ¨å‡è½»ä¸æ•°æ®å¤„ç†å»¶è¿Ÿç›¸å…³çš„ä¸åˆ©ä¸šåŠ¡äº‹ä»¶çš„å½±å“ï¼Œä¾‹å¦‚å„ç§æŸå¤±å’Œä¸­æ–­ã€å®¢æˆ·æµå¤±å’Œè´¢åŠ¡è¡°é€€ã€‚ç”±äºä»Šå¤©ä¸šåŠ¡éœ€æ±‚çš„å¤æ‚æ€§ï¼Œä¼ ç»Ÿçš„**æ‰¹å¤„ç†æ•°æ®å¤„ç†**æ˜¯â€˜ä¸å¯è¡Œâ€™çš„è§£å†³æ–¹æ¡ˆï¼Œå› ä¸ºå®ƒåªèƒ½å¤„ç†ç´¯ç§¯æ—¶é—´çš„äº‹åŠ¡æ•°æ®ç»„ã€‚

æ‰€ä»¥ä½¿ç”¨æ•°æ®æµçš„ä¸šåŠ¡ä¼˜åŠ¿å¦‚ä¸‹ï¼š

+   **æé«˜å®¢æˆ·æ»¡æ„åº¦**ï¼Œè¿›è€Œæé«˜å®¢æˆ·ä¿ç•™ç‡

+   **å‡å°‘æ“ä½œæŸå¤±**ï¼Œå› ä¸ºå®ƒå¯ä»¥æä¾›å…³äºç³»ç»Ÿä¸­æ–­å’Œæ¼æ´çš„å®æ—¶æ´å¯Ÿã€‚

+   **æŠ•èµ„å›æŠ¥ç‡æé«˜**ï¼Œå› ä¸ºå…¬å¸ç°åœ¨å¯ä»¥æ›´å¿«åœ°å¯¹ä¸šåŠ¡æ•°æ®åšå‡ºååº”ï¼Œå¯¹å®¢æˆ·éœ€æ±‚å’Œå¸‚åœºè¶‹åŠ¿çš„å“åº”èƒ½åŠ›æé«˜ã€‚

ä¸»è¦çš„**æŠ€æœ¯ä¼˜åŠ¿**åœ¨äºæ•°æ®å¤„ç†ï¼Œå› ä¸ºå®ƒä»¥ä¸¥æ ¼çš„***é€ä¸ªå¤„ç†***æ–¹å¼è¿è¡Œäº‹ä»¶å¤„ç†ã€‚ä¸æ‰¹å¤„ç†å¤„ç†ç›¸æ¯”ï¼Œå®ƒå…·æœ‰æ›´å¥½çš„æ•…éšœå®¹å¿æ€§ï¼Œå¦‚æœç®¡é“ä¸­çš„ä¸€ä¸ªäº‹ä»¶å› æŸäº›åŸå› æ— æ³•å¤„ç†ï¼Œé‚£ä¹ˆåªæœ‰è¿™ä¸ªäº‹ä»¶ä¼šå—åˆ°å½±å“ã€‚åœ¨æ‰¹å¤„ç†ç®¡é“ä¸­ï¼Œç”±äºå•ä¸ªæ•°æ®ç‚¹å¯èƒ½å…·æœ‰é”™è¯¯çš„æ¨¡å¼æˆ–æ•°æ®æ ¼å¼ï¼Œæ•´ä¸ªæ•°æ®å¤„ç†å—ä¼šå› æ­¤å¤±è´¥ã€‚

> æµæ•°æ®ç®¡é“çš„ä¸»è¦ç¼ºç‚¹æ˜¯æˆæœ¬

æ¯æ¬¡æˆ‘ä»¬çš„æµå¤„ç†å™¨è¾¾åˆ°ç«¯ç‚¹æ—¶ï¼Œå®ƒéƒ½éœ€è¦è®¡ç®—èƒ½åŠ›ã€‚é€šå¸¸ï¼Œæµæ•°æ®å¤„ç†ä¼šå¯¼è‡´ä¸ç‰¹å®šæ•°æ®ç®¡é“ç›¸å…³çš„æ›´é«˜æˆæœ¬ã€‚

## æ„å»ºæµæ•°æ®ç®¡é“çš„æŒ‘æˆ˜

+   **æ•…éšœå®¹å¿æ€§** â€” æˆ‘ä»¬èƒ½å¦è®¾è®¡å’Œæ„å»ºä¸€ä¸ªèƒ½å¤Ÿå¤„ç†å•ä¸€æ•°æ®äº‹ä»¶å¤„ç†å¤±è´¥çš„æ•°æ®å¹³å°ï¼Ÿæ•°æ®é€šå¸¸æ¥è‡ªä¸åŒçš„æ•°æ®æºï¼Œç”šè‡³å¯èƒ½ä»¥ä¸åŒçš„æ ¼å¼å‡ºç°ã€‚åœ¨è®¾è®¡å¸¦æœ‰æµç»„ä»¶çš„æ•°æ®å¹³å°æ—¶ï¼Œæ•°æ®çš„å¯ç”¨æ€§å’ŒæŒä¹…æ€§æˆä¸ºé‡è¦çš„è€ƒè™‘å› ç´ [3]ã€‚

## æ•°æ®å¹³å°æ¶æ„ç±»å‹

### å®ƒèƒ½å¤šå¥½åœ°æ»¡è¶³ä½ çš„ä¸šåŠ¡éœ€æ±‚ï¼Ÿé€‰æ‹©çš„å›°å¢ƒã€‚

towardsdatascience.com

+   **æ’é˜Ÿå’Œæ’åº** â€” æ•°æ®æµä¸­çš„äº‹ä»¶å¿…é¡»æ­£ç¡®æ’åºã€‚å¦åˆ™ï¼Œæ•°æ®å¤„ç†å¯èƒ½ä¼šå¤±è´¥ã€‚ä¾‹å¦‚ï¼Œå¦‚æœæ’åºä¸æ­£ç¡®ï¼Œåº”ç”¨å†…æ¶ˆæ¯å°†æ²¡æœ‰æ„ä¹‰ã€‚

+   **å¯æ‰©å±•æ€§** â€” åº”ç”¨ç¨‹åºéœ€è¦æ‰©å±•ã€‚å°±è¿™ä¹ˆç®€å•ã€‚è®¾è®¡ä¸€ä¸ªèƒ½å¤Ÿå¾ˆå¥½åº”å¯¹æ¥è‡ªæºçš„äº‹ä»¶æ•°é‡å¢åŠ çš„æ•°æ®ç®¡é“å¹¶éæ˜“äº‹ã€‚èƒ½å¤Ÿä¸ºæ•°æ®ç®¡é“æ·»åŠ æ›´å¤šèµ„æºå’Œæ•°æ®å¤„ç†èƒ½åŠ›æ˜¯ä¸€ä¸ªå¼ºå¤§æ•°æ®å¹³å°çš„é‡è¦ç»„æˆéƒ¨åˆ†ã€‚

+   **æ•°æ®ä¸€è‡´æ€§ â€”** åœ¨åˆ†å¸ƒå¼æ•°æ®å¹³å°ä¸­ï¼Œæ•°æ®ç»å¸¸æ˜¯å¹¶è¡Œå¤„ç†çš„ã€‚è¿™å¯èƒ½ä¼šæˆä¸ºæŒ‘æˆ˜ï¼Œå› ä¸ºåœ¨ä¸€ä¸ªæ•°æ®å¤„ç†å™¨ä¸­æ•°æ®å¯èƒ½å·²ç»è¢«ä¿®æ”¹ï¼Œè€Œåœ¨å¦ä¸€ä¸ªå¤„ç†å™¨ä¸­å˜å¾—é™ˆæ—§ã€‚

## ç°å®ä¸–ç•Œä¸­çš„ä¸€ä¸ªä¾‹å­

è®©æˆ‘ä»¬çœ‹ä¸€ä¸‹è¿™ä¸ªä½¿ç”¨ AWS Kinesis å’Œ Redshift æ„å»ºçš„æµæ•°æ®ç®¡é“ç¤ºä¾‹ã€‚

![](img/17b2472521ad2fec136aa2de06456924.png)

ç¤ºä¾‹ç®¡é“ã€‚å›¾ç‰‡ç”±ä½œè€…æä¾›

Amazon Kinesis Data Firehose æ˜¯ä¸€ä¸ª ETL æœåŠ¡ï¼Œå¯ä»¥é«˜å¯é æ€§åœ°æ”¶é›†ã€è½¬æ¢å¹¶åˆ†å‘æµæ•°æ®åˆ°æ•°æ®æ¹–ã€æ•°æ®å­˜å‚¨å’Œåˆ†ææœåŠ¡ã€‚

æˆ‘ä»¬å¯ä»¥ç”¨å®ƒå°†æ•°æ®æµä¼ è¾“åˆ° Amazon S3ï¼Œå¹¶å°†æ•°æ®è½¬æ¢ä¸ºåˆ†ææ‰€éœ€çš„æ ¼å¼ï¼Œæ— éœ€å¼€å‘å¤„ç†ç®¡é“ã€‚å®ƒå¯¹äºæœºå™¨å­¦ä¹ ï¼ˆMLï¼‰ç®¡é“ä¹Ÿéå¸¸é€‚åˆï¼Œå…¶ä¸­æ¨¡å‹ç”¨äºæ£€æŸ¥æ•°æ®å¹¶é¢„æµ‹æ¨æ–­ç«¯ç‚¹ï¼Œå› ä¸ºæ•°æ®æµå‘å…¶ç›®æ ‡ã€‚

**Kinesis Data Streams ä¸ Kinesis Data Firehose**

Kinesis Data Streams ä¸»è¦å…³æ³¨äºæ¶ˆè´¹å’Œå­˜å‚¨æ•°æ®æµã€‚Kinesis Data Firehose æ—¨åœ¨å°†æ•°æ®æµä¼ é€’åˆ°ç‰¹å®šçš„ç›®æ ‡ã€‚ä¸¤è€…éƒ½å¯ä»¥æ¶ˆè´¹æ•°æ®æµï¼Œä½†ä½¿ç”¨å“ªä¸ªå–å†³äºæˆ‘ä»¬å¸Œæœ›æ•°æ®æµå»å¾€ä½•å¤„ã€‚

AWS Kinesis Data Firehose å…è®¸æˆ‘ä»¬å°†æ•°æ®æµé‡å®šå‘åˆ° AWS æ•°æ®å­˜å‚¨ã€‚Kinesis Data Firehose æ˜¯æ”¶é›†ã€å¤„ç†å’ŒåŠ è½½æ•°æ®æµåˆ° AWS æ•°æ®å­˜å‚¨çš„æœ€ç›´æ¥æ–¹æ³•ã€‚

Amazon Kinesis Data Firehose æ”¯æŒæ‰¹å¤„ç†æ“ä½œã€åŠ å¯†å’Œæµæ•°æ®å‹ç¼©ï¼Œä»¥åŠè‡ªåŠ¨åŒ–çš„æ¯ç§’ TB çº§åˆ«çš„å¯æ‰©å±•æ€§ã€‚Firehose å¯ä»¥æ— ç¼é›†æˆ S3 æ•°æ®æ¹–ã€RedShift æ•°æ®ä»“åº“è§£å†³æ–¹æ¡ˆæˆ– ElasticSearch æœåŠ¡ã€‚

AWS Kinesis Data Streams æ˜¯ä¸€ä¸ª Amazon Kinesis å®æ—¶æ•°æ®æµè§£å†³æ–¹æ¡ˆï¼Œå…·æœ‰å“è¶Šçš„å¯æ‰©å±•æ€§å’Œè€ç”¨æ€§ï¼Œæ•°æ®æµå…¨å¤©å€™ 24/7 å¯ç”¨ç»™ä»»ä½•æ¶ˆè´¹è€…ã€‚è¿™ä½¿å¾—å®ƒæ¯” Kinesis Data Firehose æ›´æ˜‚è´µã€‚

**å¦‚ä½•ä½¿ç”¨ AWS CloudFormation åˆ›å»º Firehose èµ„æº**

è¯·æŸ¥çœ‹ä¸‹é¢çš„ CloudFormation æ¨¡æ¿ã€‚å®ƒéƒ¨ç½²äº†åŒ…æ‹¬æˆ‘ä»¬éœ€è¦çš„ Firehose åœ¨å†…çš„æ‰€æœ‰å¿…è¦èµ„æºã€‚

```py
AWSTemplateFormatVersion: 2010-09-09
Description: >
  Firehose resources relating to data generation.

Parameters:
  Environment:
    AllowedValues:
      - staging
      - production
    Description: Target environment
    Type: String
    Default: 'staging'
  DataLocation:
    Description: S3 data lake bucket name.
    Type: String
    Default: data.staging.aws

Resources:
  MyDataStream:
    Type: AWS::KinesisFirehose::DeliveryStream
    Properties: 
      DeliveryStreamName: !Sub 'my-event-${Environment}'
      DeliveryStreamType: DirectPut
      ExtendedS3DestinationConfiguration: 
        BucketARN: 
          - !Sub 'arn:aws:s3:::${DataLocation}' # For example: 'arn:aws:s3:::data.staging.aws'
        BufferingHints:
          IntervalInSeconds: 300
          SizeInMBs: 30
        CompressionFormat: UNCOMPRESSED
        Prefix: !Sub 'events/my-event-${Environment}/'
        RoleARN: !GetAtt AccessRole.Arn

  AccessRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Statement:
          - Effect: Allow
            Principal:
              Service:
                - firehose.amazonaws.com
            Action:
              - sts:AssumeRole
      Path: /
      Policies:
        - PolicyName: !Sub '${AWS::StackName}-AccessPolicy'
          PolicyDocument:
            Statement:
              - Effect: Allow
                Action:
                  - s3:AbortMultipartUpload
                  - s3:GetBucketLocation
                  - s3:GetObject
                  - s3:ListBucket
                  - s3:ListBucketMultipartUploads
                  - s3:PutObject
                Resource:
                  - !Sub 'arn:aws:s3:::${DataLocation}'
                  - !Sub 'arn:aws:s3:::${DataLocation}/*'
                  # - 'arn:aws:s3:::data.staging.aws' # replace with your S3 datalake bucket
                  # - 'arn:aws:s3:::data.staging.aws/*'
              - Effect: Allow
                Action:
                  - kinesis:DescribeStream
                  - kinesis:GetShardIterator
                  - kinesis:GetRecords
                Resource:
                  - !Sub 'arn:aws:kinesis:${AWS::Region}:${AWS::AccountId}:stream/my-event-${Environment}'
```

å¯ä»¥ä½¿ç”¨ AWS CLI å·¥å…·åœ¨ AWS ä¸­éƒ¨ç½²å®ƒã€‚æˆ‘ä»¬éœ€è¦åœ¨å‘½ä»¤è¡Œä¸­è¿è¡Œè¿™ä¸ªï¼ˆåœ¨ä½ çš„è´¦æˆ·ä¸­æ›¿æ¢ä¸ºå”¯ä¸€çš„å­˜å‚¨æ¡¶åç§°ï¼‰ï¼š

```py
./deploy-firehose-staging.sh s3-lambda-bucket s3-data-lake-bucket
```

æˆ‘ä»¬çš„ shell è„šæœ¬å¦‚ä¸‹æ‰€ç¤ºï¼š

```py
#!/usr/bin/env bash
# chmod +x ./deploy-firehose-staging.sh
# Run ./deploy-firehose-staging.sh s3-lambda-bucket s3-data-lake-bucket
STACK_NAME=FirehoseStackStaging
LAMBDA_BUCKET=$1 #datalake-lambdas.aws # Replace with unique bucket name in your account
S3_DATA_LOCATION=$2 #data.staging.aws # S3 bucket to save data, i.e. datalake
# Deploy using AWS CLI:
aws \
cloudformation deploy \
--template-file firehose_stack.yaml \
--stack-name $STACK_NAME \
--capabilities CAPABILITY_IAM \
--parameter-overrides \
"Environment"="staging" \
"DataLocation"=$S3_DATA_LOCATION #"data.staging.aws"
```

![](img/515379a7556b2ccb17c34b3d84a59dd7.png)

å·²åˆ›å»º Firehose èµ„æºã€‚å›¾ç‰‡ç”±ä½œè€…æä¾›

ç°åœ¨æˆ‘ä»¬éœ€è¦åˆ›å»ºä¸€ä¸ªäº‹ä»¶ç”Ÿäº§è€…ã€‚æˆ‘ä»¬å¯ä»¥ä½¿ç”¨ Python å®Œæˆè¿™ä¸ªæ“ä½œï¼Œ`app.py` çš„ä»£ç å¦‚ä¸‹ï¼š

```py
import boto3
kinesis_client = boto3.client('firehose', region_name='eu-west-1')
...
response = client.put_record_batch(
    DeliveryStreamName='string',
    Records=[
        {
            'Data': b'bytes'
        },
    ]
)
```

`put_record_batch` æ–¹æ³•å¯ä»¥åœ¨ä¸€æ¬¡è°ƒç”¨ä¸­å°†å¤šä¸ªæ•°æ®è®°å½•å†™å…¥äº¤ä»˜æµï¼Œè¿™æ¯”å•æ¡è®°å½•å†™å…¥æ–¹å¼èƒ½æä¾›æ›´å¥½çš„æ¯ç”Ÿäº§è€…ååé‡ã€‚`PutRecord` ç”¨äºå°†å•æ¡æ•°æ®è®°å½•å†™å…¥äº¤ä»˜æµã€‚åœ¨æœ¬æ•™ç¨‹ä¸­é€‰æ‹©å“ªä¸ªæ–¹æ³•ç”±ä½ å†³å®šã€‚

æˆ‘ä»¬å¯ä»¥åœ¨ `app.py` ä¸­ä½¿ç”¨ä¸‹é¢çš„è¾…åŠ©å‡½æ•°ç”Ÿæˆä¸€äº›åˆæˆæ•°æ®ã€‚

```py
def get_data():
    '''This function will generate random data for Firehose stream.'''
    return {
        'event_time': datetime.now().isoformat(),
        'event_name': random.choice(['JOIN', 'LEAVE', 'OPEN_CHAT', 'SUBSCRIBE', 'SEND_MESSAGE']),
        'user': round(random.random() * 100)}
```

ç°åœ¨è¿™äº›æ•°æ®å¯ä»¥é€šè¿‡ä»¥ä¸‹æ–¹å¼å‘é€åˆ°æˆ‘ä»¬çš„äº‹ä»¶ç”Ÿäº§è€…ï¼š

```py
 try:
        print('Sending events to Firehose...')
        for i in range(0, 5):
            data = get_data()
            print(i, " : ", data)
            kinesis_client.put_record(
                DeliveryStreamName=STREAM_NAME,
                Record={
                    "Data":json.dumps(data)
                }
            )
            processed += 1
        print('Wait for 5 minutes and Run to download: aws s3 cp s3://{}/events/ ./ --recursive'.format(S3_DATA))
        # For example, print('Wait for 5 minutes and Run to download: aws s3 cp s3://data.staging.aws/events/ ./ --recursive')
    except Exception as e:
        print(e)
```

å®Œæˆï¼æˆ‘ä»¬å·²ç»åˆ›å»ºäº†ä¸€ä¸ªç®€å•çš„æµæ•°æ®ç®¡é“ï¼Œå°†æ±‡æ€»ç»“æœè¾“å‡ºåˆ°äº‘å­˜å‚¨ï¼ˆAWS S3ï¼‰ã€‚

åœ¨å‘½ä»¤è¡Œä¸­è¿è¡Œ `python app.py`ï¼š

![](img/53af7f1ce72757790dce09aa4c3f438e.png)

äº‹ä»¶è¿æ¥å™¨ç¤ºä¾‹ã€‚ä½œè€…æä¾›çš„å›¾ç‰‡

æŸ¥çœ‹ä¸‹é¢çš„æ•™ç¨‹ï¼Œäº†è§£æ›´é«˜çº§çš„æ•°æ®ç®¡é“ç¤ºä¾‹ [2]

[](/building-a-streaming-data-pipeline-with-redshift-serverless-and-kinesis-04e09d7e85b2?source=post_page-----2bb2b9b3b603--------------------------------) ## ä½¿ç”¨ Redshift Serverless å’Œ Kinesis æ„å»ºæµæ•°æ®ç®¡é“

### é¢å‘åˆå­¦è€…çš„ç«¯åˆ°ç«¯æ•™ç¨‹

towardsdatascience.com

## ç»“è®º

é¡¹ç›®ç†æƒ³çš„æµæ•°æ®å¹³å°å¹¶ä¸å­˜åœ¨ã€‚æµè®¾è®¡æœ‰å…¶å¥½å¤„ï¼Œä½†åœ¨ä½¿ç”¨æ—¶ä¹Ÿä¼šé‡åˆ°ä¸€äº›æ˜æ˜¾çš„æŒ‘æˆ˜ã€‚é€‰æ‹©å“ªä¸ªæµå·¥å…·ä¸æ˜¯ä¸€ä¸ªå®¹æ˜“çš„å†³å®šã€‚è¿™å–å†³äºä½ çš„ä¸šåŠ¡ç›®æ ‡å’ŒåŠŸèƒ½æ•°æ®éœ€æ±‚ã€‚ä½ å¯èƒ½éœ€è¦å°è¯•å¹¶æ¯”è¾ƒå¤šä¸ªæµå¹³å°ï¼Œè€ƒè™‘åŠŸèƒ½ã€æ€§èƒ½ã€æˆæœ¬ã€æ˜“ç”¨æ€§å’Œå…¼å®¹æ€§ç­‰ç‰¹å¾ã€‚å®ƒä¼šæ˜¯ä¸€ä¸ªæœºå™¨å­¦ä¹ ç®¡é“å—ï¼Ÿæˆ‘ä»¬éœ€è¦å¤„ç†åˆ†åŒºã€çª—å£å’Œè¿æ¥å—ï¼Ÿæˆ‘ä»¬éœ€è¦é«˜ååé‡ã€å®¹é”™æ€§è¿˜æ˜¯ä½å»¶è¿Ÿï¼Ÿ

> ä¸åŒçš„æµæ¡†æ¶å…·æœ‰ä¸åŒçš„èƒ½åŠ›ï¼Œä¾‹å¦‚ï¼ŒKafka æœ‰ä¸€ä¸ªæ–¹ä¾¿çš„**ä¼šè¯** **åº“**ï¼Œå¯ä»¥å¾ˆå®¹æ˜“åœ°é›†æˆåˆ°ä½ çš„åˆ†æç®¡é“ä¸­ã€‚

æˆ‘ä»¬çš„ç®¡é“éœ€è¦ä»€ä¹ˆé¢‘ç‡çš„æ•°æ®ä¼ è¾“å’Œæ¶ˆè´¹ï¼Ÿå®ƒå°†äº¤ä»˜åˆ°æ•°æ®ä»“åº“è§£å†³æ–¹æ¡ˆè¿˜æ˜¯æ•°æ®æ¹–ä¸­ï¼Ÿä¸€äº›å¹³å°æ¯”å…¶ä»–å¹³å°æä¾›æ›´å¥½çš„é›†æˆåŠŸèƒ½ã€‚

å¦ä¸€ä¸ªé‡è¦çš„è€ƒè™‘å› ç´ æ˜¯å¿…é¡»å¯¹æµæ•°æ®è¿›è¡Œçš„**æ•°æ®å¤„ç†**å’Œåˆ†æçš„ç±»å‹å’Œ**å¤æ‚æ€§**ã€‚

æˆ‘å»ºè®®æ ¹æ®ä½ è‡ªå·±æ•°æ®ç®¡é“åœºæ™¯å’Œå…¬å¸ä¸»è¦åˆ©ç›Šç›¸å…³è€…æ”¶é›†çš„éœ€æ±‚æ¥åˆ›å»ºä¸€ä¸ªåŸå‹ã€‚æœ€ç»ˆçš„æµæ•°æ®ç®¡é“åº”è¯¥æ˜¯èƒ½å¤Ÿä¸ºä¸šåŠ¡å¢å€¼å¹¶æ»¡è¶³ä½ çš„æ•°æ®å·¥ç¨‹ç›®æ ‡çš„ã€‚

## æ¨èé˜…è¯»ï¼š

[1] `towardsdatascience.com/data-pipeline-design-patterns-100afa4b93e3`

[2] `towardsdatascience.com/building-a-streaming-data-pipeline-with-redshift-serverless-and-kinesis-04e09d7e85b2`

[3] [`medium.com/towards-data-science/data-platform-architecture-types-f255ac6e0b7`](https://medium.com/towards-data-science/data-platform-architecture-types-f255ac6e0b7)

[4] [`medium.com/towards-data-science/data-modelling-for-data-engineers-93d058efa302`](https://medium.com/towards-data-science/data-modelling-for-data-engineers-93d058efa302)
