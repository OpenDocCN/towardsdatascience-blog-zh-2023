- en: Dealing with MRI and Deep Learning with Python
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Python处理MRI和深度学习
- en: 原文：[https://towardsdatascience.com/dealing-with-mri-and-deep-learning-with-python-c88f3dae0620?source=collection_archive---------3-----------------------#2023-12-20](https://towardsdatascience.com/dealing-with-mri-and-deep-learning-with-python-c88f3dae0620?source=collection_archive---------3-----------------------#2023-12-20)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/dealing-with-mri-and-deep-learning-with-python-c88f3dae0620?source=collection_archive---------3-----------------------#2023-12-20](https://towardsdatascience.com/dealing-with-mri-and-deep-learning-with-python-c88f3dae0620?source=collection_archive---------3-----------------------#2023-12-20)
- en: A Comprehensive Guide to MRI Analysis through Deep Learning models in PyTorch
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用PyTorch的深度学习模型进行MRI分析的综合指南
- en: '[](https://medium.com/@carlapitarch?source=post_page-----c88f3dae0620--------------------------------)[![Carla
    Pitarch Abaigar](../Images/f0f7f963947f59399c8b3e6ac9d9aac9.png)](https://medium.com/@carlapitarch?source=post_page-----c88f3dae0620--------------------------------)[](https://towardsdatascience.com/?source=post_page-----c88f3dae0620--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----c88f3dae0620--------------------------------)
    [Carla Pitarch Abaigar](https://medium.com/@carlapitarch?source=post_page-----c88f3dae0620--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@carlapitarch?source=post_page-----c88f3dae0620--------------------------------)[![Carla
    Pitarch Abaigar](../Images/f0f7f963947f59399c8b3e6ac9d9aac9.png)](https://medium.com/@carlapitarch?source=post_page-----c88f3dae0620--------------------------------)[](https://towardsdatascience.com/?source=post_page-----c88f3dae0620--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----c88f3dae0620--------------------------------)
    [Carla Pitarch Abaigar](https://medium.com/@carlapitarch?source=post_page-----c88f3dae0620--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ff2cd70d9ae7e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdealing-with-mri-and-deep-learning-with-python-c88f3dae0620&user=Carla+Pitarch+Abaigar&userId=f2cd70d9ae7e&source=post_page-f2cd70d9ae7e----c88f3dae0620---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----c88f3dae0620--------------------------------)
    ·13 min read·Dec 20, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fc88f3dae0620&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdealing-with-mri-and-deep-learning-with-python-c88f3dae0620&user=Carla+Pitarch+Abaigar&userId=f2cd70d9ae7e&source=-----c88f3dae0620---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ff2cd70d9ae7e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdealing-with-mri-and-deep-learning-with-python-c88f3dae0620&user=Carla+Pitarch+Abaigar&userId=f2cd70d9ae7e&source=post_page-f2cd70d9ae7e----c88f3dae0620---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----c88f3dae0620--------------------------------)
    ·13分钟阅读·2023年12月20日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fc88f3dae0620&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdealing-with-mri-and-deep-learning-with-python-c88f3dae0620&user=Carla+Pitarch+Abaigar&userId=f2cd70d9ae7e&source=-----c88f3dae0620---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc88f3dae0620&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdealing-with-mri-and-deep-learning-with-python-c88f3dae0620&source=-----c88f3dae0620---------------------bookmark_footer-----------)![](../Images/c7797e5acea961e4be445d2986a13b7b.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc88f3dae0620&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdealing-with-mri-and-deep-learning-with-python-c88f3dae0620&source=-----c88f3dae0620---------------------bookmark_footer-----------)![](../Images/c7797e5acea961e4be445d2986a13b7b.png)'
- en: Photo by [Olga Rai](https://stock.adobe.com/es/contributor/209778624/olga-rai?load_type=author&prev_url=detail)
    on [Adobe Stock](https://stock.adobe.com/).
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：[Olga Rai](https://stock.adobe.com/es/contributor/209778624/olga-rai?load_type=author&prev_url=detail)
    于 [Adobe Stock](https://stock.adobe.com/)。
- en: Introduction
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: First of all, I’d like to introduce myself. My name is Carla Pitarch, and I
    am a PhD candidate in AI. My research centers on developing an automated brain
    tumor grade classification system by extracting information from Magnetic Resonance
    Images (MRIs) using Deep Learning (DL) models, particularly Convolutional Neural
    Networks (CNNs).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我想介绍一下自己。我叫Carla Pitarch，是一名人工智能（AI）博士候选人。我的研究重点是通过使用深度学习（DL）模型，特别是卷积神经网络（CNNs），从磁共振图像（MRI）中提取信息，以开发自动化的脑肿瘤分级分类系统。
- en: At the start of my PhD journey, diving into MRI data and DL was a whole new
    world. The initial steps for running models in this realm were not as straightforward
    as expected. Despite spending some time researching in this domain, I found a
    lack of comprehensive repositories guiding the initiation into both MRI and DL.
    Therefore, I decided to share some of the knowledge I have gained over this period,
    hoping it makes your journey a tad smoother.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在我的博士研究之初，深入研究MRI数据和DL是一个全新的领域。在这个领域中运行模型的初步步骤并不像预期的那样简单。尽管花了一些时间在这个领域进行研究，我发现缺乏全面的资源来指导MRI和DL的入门。因此，我决定分享一些我在这一期间获得的知识，希望它能让你的旅程更加顺利。
- en: 'Embarking on Computer Vision (CV) tasks through DL often involves using standard
    public image datasets such as `[ImageNe](https://image-net.org/about.php)t` ,
    characterized by 3-channel RGB natural images. PyTorch models are primed for these
    specifications, expecting input images to be in this format. However, when our
    image data comes from a distinct domain, like the medical field, diverging in
    both format and features from these natural image datasets, it presents challenges.
    This post delves into this issue, emphasizing two crucial preparatory steps before
    model implementation: aligning the data with the model’s requirements and preparing
    the model to effectively process our data.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 通过DL进行计算机视觉（CV）任务通常涉及使用标准公共图像数据集，如`[ImageNe](https://image-net.org/about.php)t`，这些数据集以3通道RGB自然图像为特征。PyTorch模型为这些规格做好了准备，期望输入图像为这种格式。然而，当我们的图像数据来自不同的领域，如医疗领域，与这些自然图像数据集在格式和特征上都有所不同时，就会带来挑战。本文深入探讨了这个问题，强调了在模型实施之前的两个关键准备步骤：使数据与模型的要求对齐，并准备模型以有效处理我们的数据。
- en: Background
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 背景
- en: Let’s start with a brief overview of the fundamental aspects of CNNs and MRI.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先简要概述一下CNNs和MRI的基本方面。
- en: Convolutional Neural Networks
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 卷积神经网络
- en: In this section, we delve into the realm of CNNs, assuming readers have a foundational
    understanding of DL. CNNs stand as the gold standard architectures in CV, specializing
    in the processing of 2D and 3D input image data. Our focus within this post will
    be centered on the processing of 2D image data.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将深入探讨CNNs领域，假设读者对深度学习（DL）有基本的理解。CNNs作为计算机视觉（CV）中的黄金标准架构，专注于处理2D和3D输入图像数据。我们在这篇文章中的重点将集中在2D图像数据的处理上。
- en: 'Image classification, associating output classes or labels with input images,
    is a core task in CNNs. The pioneering LeNet5 architecture introduced by LeCun
    et al.¹ in 1989 laid the groundwork for CNNs. This architecture can be summarized
    as follows:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 图像分类，将输出类别或标签与输入图像关联，是卷积神经网络（CNNs）的核心任务。由LeCun等人于1989年提出的开创性LeNet5架构为CNNs奠定了基础。该架构可以总结如下：
- en: '![](../Images/5559d225d2df07a468c8e1431111710a.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5559d225d2df07a468c8e1431111710a.png)'
- en: CNN architecture with two convolution layers, two pooling layers, and one fully-connected
    layer previous to the output layer.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: CNN架构包含两个卷积层、两个池化层，以及一个位于输出层之前的全连接层。
- en: '2D CNN architectures operate by receiving image pixels as input, expecting
    an image to be a tensor with shape `Height x Width x Channels`. Color images typically
    consist of 3 channels: red, green and blue (RGB), while grayscale images consist
    of a single channel.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 2D CNN架构通过接收图像像素作为输入来操作，期望图像是一个形状为`Height x Width x Channels`的张量。彩色图像通常包含3个通道：红色、绿色和蓝色（RGB），而灰度图像则包含一个通道。
- en: A fundamental operation in CNNs is *convolution*, executed by applying a set
    of *filters* or *kernels* across all areas of the input data. The figure below
    shows an example of how convolution works in a 2D context.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: CNNs中的一个基本操作是*卷积*，通过在输入数据的所有区域应用一组*滤波器*或*内核*来执行。下图展示了卷积在2D上下文中的工作原理。
- en: '![](../Images/b603b70f3daaf76704842d9beeb4c056.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b603b70f3daaf76704842d9beeb4c056.png)'
- en: Example of a convolution over a 5x5 image with a 3x3 filter that produces a
    3x3 convolved feature.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 一个对5x5图像进行3x3滤波器卷积的示例，生成一个3x3卷积特征。
- en: 'The process involves sliding the filter across the image to the right and compute
    the weighted sum to obtain a convolved feature map. The output will represent
    whether a specific visual pattern, for instance an edge, is recognized at that
    location in the input image. Following each convolutional layer, an activation
    function introduces non-linearity. Popular choices include: ReLU (Rectified Linear
    Unit), Leaky ReLu, Sigmoid, Tanh, and Softmax. For further details on each activation
    function, this post provides clear explanations [Activation Functions in Neural
    Networks | by SAGAR SHARMA | Towards Data Science](/activation-functions-neural-networks-1cbd9f8d91d6).'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 这个过程涉及将滤波器滑过图像并计算加权和，以获得一个卷积特征图。输出将表示输入图像的该位置是否识别到特定的视觉模式，例如边缘。每个卷积层后，激活函数会引入非线性。常见的选择包括：ReLU（修正线性单元）、Leaky
    ReLU、Sigmoid、Tanh 和 Softmax。有关每个激活函数的更多详细信息，本文提供了清晰的解释 [Activation Functions in
    Neural Networks | by SAGAR SHARMA | Towards Data Science](/activation-functions-neural-networks-1cbd9f8d91d6)。
- en: 'Different types of layers contribute to the construction of CNNs, each playing
    a distinct role in defining the network’s functionality. Alongside convolutional
    layers, several other prominent layers used in CNNs include:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 不同类型的层对 CNNs 的构建做出贡献，每一层在定义网络功能方面扮演着独特的角色。除了卷积层，CNNs 中还包括几种其他显著的层：
- en: '*Pooling layers*, like max-pooling or average-pooling, efffectively reduce
    feature map dimensions while preserving essential information.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*池化层*，如最大池化或平均池化，有效地减少特征图维度，同时保留关键信息。'
- en: '*Dropout layers* are used to prevent overfitting by randomly deactivating a
    fraction of neurons during training, thereby enhancing the network’s generalization
    ability.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Dropout 层* 通过在训练期间随机停用部分神经元来防止过拟合，从而增强网络的泛化能力。'
- en: '*Batch normalization layers* focus on standardizing inputs for each layer,
    which accelerates network training.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*批量归一化层* 关注对每层的输入进行标准化，从而加快网络训练速度。'
- en: '*Fully connected (FC) layers* establish connections between all neurons in
    one layer and all activations from the preceding layer, integrating learned features
    to facilitate final classifications.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*全连接（FC）层* 在一个层中的所有神经元和前一层中的所有激活之间建立连接，整合学到的特征以促进最终分类。'
- en: CNNs learn to identify patterns hierarchically. Initial layers focus on low-level
    features, progressively moving to highly abstract features in deeper layers. Upon
    reaching the FC layer, the *Softmax* activation function estimates class probabilities
    for the image input.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: CNNs 通过层次化的方式学习识别模式。初始层关注低级特征，逐渐移动到更深层次的高度抽象特征。当到达全连接（FC）层时，*Softmax* 激活函数会估计图像输入的类别概率。
- en: Beyond LeNet’s inception, notable CNN architectures like AlexNet², GoogLeNet³,
    VGGNet⁴, ResNet⁵, and more recent Transformer⁶ have significantly contributed
    to the realm of DL.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 除了 LeNet 的创始之外，像 AlexNet²、GoogLeNet³、VGGNet⁴、ResNet⁵ 和更新的 Transformer⁶ 等著名 CNN
    架构也显著推动了深度学习领域的发展。
- en: Natural Images Overview
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 自然图像概述
- en: Exploring natural 2D images provides a foundational understanding of image data.
    To begin, we will delve into some examples.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 探索自然 2D 图片可以提供对图像数据的基础理解。我们将从一些示例开始，深入探讨。
- en: For our first example we will select an image from the widely known `MNIST`
    dataset.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的第一个示例中，我们将从广泛使用的`MNIST`数据集中选择一张图片。
- en: '![](../Images/c7dea9d21bec3e04cacbd81542d31621.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c7dea9d21bec3e04cacbd81542d31621.png)'
- en: This image shape is `[28,28]` , representing a grayscale image with a single
    channel. Then, the image input for a neural network would be `(28*28*1)`.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 这个图像的形状是 `[28,28]`，表示一个具有单一通道的灰度图像。然后，神经网络的图像输入将是 `(28*28*1)`。
- en: Now let’s explore an image from the `ImageNet` dataset. You can access this
    dataset directly from ImageNet’s website [ImageNet (image-net.org)](https://www.image-net.org/download.php)
    or explore a subset available on Kaggle [ImageNet Object Localization Challenge
    | Kaggle](https://www.kaggle.com/c/imagenet-object-localization-challenge/data).
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们探索来自`ImageNet`数据集的一张图片。你可以直接从 ImageNet 的网站 [ImageNet (image-net.org)](https://www.image-net.org/download.php)
    访问该数据集，或者在 Kaggle 上探索一个可用的子集 [ImageNet Object Localization Challenge | Kaggle](https://www.kaggle.com/c/imagenet-object-localization-challenge/data)。
- en: '![](../Images/df30982867a148631172830db8d3e469.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/df30982867a148631172830db8d3e469.png)'
- en: 'We can decompose this image into its RGB channels:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将这张图片分解为其 RGB 通道：
- en: '![](../Images/b1e48588369c8dab689156f44807f580.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b1e48588369c8dab689156f44807f580.png)'
- en: Since the shape of this image is `[500, 402, 3]` the image input of a neural
    network would be represented as `(500*402*3)`.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 由于该图像的形状是 `[500, 402, 3]`，神经网络的图像输入将表示为 `(500*402*3)`。
- en: Magnetic Resonance Imaging
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 磁共振成像
- en: The MRI scan is the most widely used test in neurology and neurosurgery, offering
    a non-invasive method that provides good soft tissue contrast⁷. Beyond visualizing
    structural details, MR imaging delves deeper, providing valuable insights into
    both the structural and functional aspects of the brain.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: MRI 扫描是神经学和神经外科中使用最广泛的检查方法，提供了一种非侵入性的方法，能提供良好的软组织对比度⁷。除了可视化结构细节外，MR 成像还深入提供了对大脑结构和功能方面的宝贵见解。
- en: 'MRIs, constitute 3D volumes that enable visualization across the three anatomical
    planes: axial, coronal, and sagittal. These volumes are composed of 3D cubes known
    as *voxels,* in contrast to the standard 2D images, which are made up of 2D squares
    called *pixels*. While 3D volumes offer comprehensive data, they can also be decomposed
    into 2D slices.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: MRI 是由 3D 体积组成的，能够在三个解剖平面：轴向、冠状面和矢状面上进行可视化。这些体积由称为 *体素* 的 3D 立方体组成，与标准的 2D 图像相比，后者由称为
    *像素* 的 2D 方块构成。虽然 3D 体积提供了全面的数据，但也可以被分解为 2D 切片。
- en: Diverse MRI modalities or sequences, such as T1, T1 with gadolinium contrast
    enhancement (T1ce), T2, and FLAIR (Fluid Attenuated Inversion Recovery), are normally
    used for diagnosis. These sequences enable the differentiation of tumor compartments
    by offering distinct signal intensities that correspond to specific regions or
    tissues. Below is an illustration presenting these four sequences from a single
    patient diagnosed with glioblastoma, known as the most aggressive form among gliomas,
    the most prevalent primary brain tumors.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 各种 MRI 模态或序列，如 T1、T1 磁性增强（T1ce）、T2 和 FLAIR（液体衰减反转恢复），通常用于诊断。这些序列通过提供对应于特定区域或组织的不同信号强度，使肿瘤区分成为可能。以下插图展示了来自一个被诊断为胶质母细胞瘤的病人的这四种序列，胶质母细胞瘤是胶质瘤中最具侵袭性的类型，也是最常见的原发性脑肿瘤。
- en: '![](../Images/2c62aed363ebf89ac07d86dff9e62788.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2c62aed363ebf89ac07d86dff9e62788.png)'
- en: Brain Tumor Segmentation Data
  id: totrans-47
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 脑肿瘤分割数据
- en: '[Brain Tumor Segmentation (BraTS) Challenge](https://www.med.upenn.edu/cbica/brats/)
    made available one of the most extensive multi-modal brain MRI scans datasets
    of glioma patients spanning from 2012 to 2023\. The primary goal of the BraTS
    competition is to evaluate the state-of-the-art methodologies for segmenting brain
    tumors in multi-modal MRI scans, although additional tasks have been added over
    time.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '[脑肿瘤分割（BraTS）挑战赛](https://www.med.upenn.edu/cbica/brats/) 提供了一个最广泛的多模态脑 MRI
    扫描数据集，涵盖了 2012 到 2023 年的胶质瘤病人。BraTS 比赛的主要目标是评估最先进的方法在多模态 MRI 扫描中分割脑肿瘤的效果，尽管随着时间的推移还添加了额外的任务。'
- en: BraTS dataset provides clinical information about the tumors, including a binary
    label indicating the tumor grade (low-grade or high-grade). BraTS scans are available
    as NIfTI files and describe T1, T1ce, T2 and Flair modalities. The scans are provided
    after some pre-processing steps, including co-registration to the same anatomical
    template, interpolation to a uniform isotropic resolution (1mm³), and skull-stripping.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: BraTS 数据集提供了关于肿瘤的临床信息，包括一个二进制标签，指示肿瘤级别（低级别或高级别）。BraTS 扫描以 NIfTI 文件形式提供，描述了 T1、T1ce、T2
    和 Flair 模态。这些扫描在经过一些预处理步骤后提供，包括共配准到相同的解剖模板、插值到均匀的各向同性分辨率（1mm³）和去颅骨处理。
- en: In this post we will use the 2020 dataset from Kaggle [BraTS2020 Dataset (Training
    + Validation)](https://www.kaggle.com/datasets/awsaf49/brats20-dataset-training-validation/)
    to classify glioma MRIs into low or high grade.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们将使用 Kaggle 的 2020 数据集 [BraTS2020 数据集 (训练 + 验证)](https://www.kaggle.com/datasets/awsaf49/brats20-dataset-training-validation/)
    来将胶质瘤 MRI 分类为低级别或高级别。
- en: 'The following images display examples of low-grade and high-grade gliomas:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图片展示了低级别和高级别胶质瘤的例子：
- en: '![](../Images/fb787a68fa811c4b537df3a9baf3db00.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fb787a68fa811c4b537df3a9baf3db00.png)'
- en: MRI modalities and segmentation mask for BraTS patient 287.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: MRI 模态和 BraTS 病人 287 的分割掩膜。
- en: '![](../Images/51ada07cfa2f5f66a8fe47fd274f8eb6.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/51ada07cfa2f5f66a8fe47fd274f8eb6.png)'
- en: MRI modalities and segmentation mask for BraTS patient 006.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: MRI 模态和 BraTS 病人 006 的分割掩膜。
- en: 'The Kaggle repository comprises 369 directories, each representing a patient
    and containing corresponding image data. Additionally, it contains two `.csv`
    files: *name_mapping.csv* and *survival_info.csv*. For our purpose, we will utilize
    *name_mapping.csv*, which links BraTS patient names to the [TCGA-LGG](https://wiki.cancerimagingarchive.net/pages/viewpage.action?pageId=5309188)
    and [TCGA-GBM](https://wiki.cancerimagingarchive.net/pages/viewpage.action?pageId=1966258)
    public datasets available from the [Cancer Imaging Archive](https://www.cancerimagingarchive.net/).
    This file not only facilitates name mapping but also provides tumor grade labels
    (LGG-HGG).'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: Kaggle 存储库包含 369 个目录，每个目录代表一个患者并包含相应的图像数据。此外，它还包含两个 `.csv` 文件：*name_mapping.csv*
    和 *survival_info.csv*。为了我们的目的，我们将使用 *name_mapping.csv*，它将 BraTS 患者姓名与 [TCGA-LGG](https://wiki.cancerimagingarchive.net/pages/viewpage.action?pageId=5309188)
    和 [TCGA-GBM](https://wiki.cancerimagingarchive.net/pages/viewpage.action?pageId=1966258)
    来自 [Cancer Imaging Archive](https://www.cancerimagingarchive.net/) 的公开数据集相关联。这个文件不仅便于姓名映射，还提供了肿瘤等级标签（LGG-HGG）。
- en: '![](../Images/2fdfda70921bc12e6049c468e389269a.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2fdfda70921bc12e6049c468e389269a.png)'
- en: 'Let’s explore the contents of each patient folder, taking *Patient 006* as
    an example. Within the *BraTS20_Training_006* folder, we find 5 files, each corresponding
    to one MRI modality and the segmentation mask:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们探索每个患者文件夹的内容，以 *Patient 006* 为例。在 *BraTS20_Training_006* 文件夹中，我们可以找到 5 个文件，每个文件对应一种
    MRI 模态和分割掩模：
- en: '*BraTS20_Training_006_flair.nii*'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*BraTS20_Training_006_flair.nii*'
- en: '*BraTS20_Training_006_t1.nii*'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*BraTS20_Training_006_t1.nii*'
- en: '*BraTS20_Training_006_t1ce.nii*'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*BraTS20_Training_006_t1ce.nii*'
- en: '*BraTS20_Training_006_t2.nii*'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*BraTS20_Training_006_t2.nii*'
- en: '*BraTS20_Training_006_seg.nii*'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*BraTS20_Training_006_seg.nii*'
- en: These files are in the `.nii` format, which represents the NIfTI format — one
    of the most prevalent in neuroimaging.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 这些文件是 `.nii` 格式，代表 NIfTI 格式——神经成像中最流行的格式之一。
- en: MRI Data Preparation
  id: totrans-65
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: MRI 数据准备
- en: To handle NIfTI images in Python, we can use the [NiBabel](https://nipy.org/nibabel/)
    package. Below is an example of the data loading. By using the `get_fdata()` method
    we can interpret the image as a numpy array.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在 Python 中处理 NIfTI 图像，我们可以使用 [NiBabel](https://nipy.org/nibabel/) 包。以下是数据加载的示例。通过使用
    `get_fdata()` 方法，我们可以将图像解释为 numpy 数组。
- en: 'The array shape, `[240, 240, 155]`, indicates a 3D volume comprising 240 2D
    slices within the x and y dimensions, and 155 slices in the z dimension. These
    dimensions correspond to distinct anatomical perspectives: the axial view (x-y
    plane), coronal view (x-z plane), and sagittal view (y-z plane).'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 数组形状 `[240, 240, 155]` 表示一个 3D 体积，由 240 个 x 和 y 维度的 2D 切片和 155 个 z 维度的切片组成。这些维度对应不同的解剖视角：轴向视图（x-y
    平面）、冠状视图（x-z 平面）和矢状视图（y-z 平面）。
- en: '![](../Images/c9fb55cd80b92c300735cec86ff32d34.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c9fb55cd80b92c300735cec86ff32d34.png)'
- en: To make it simpler, we will only employ 2D slices in axial plane, then the resulting
    images will have shape `[240, 240]` .
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简化，我们将仅使用轴向平面的 2D 切片，生成的图像将具有 `[240, 240]` 的形状。
- en: In order to meet the specifications of PyTorch models, input tensors must have
    shape `[batch_size, num_channels, height, width]`. In our MRI data, each of the
    four modalities (FLAIR, T1ce, T1, T2) emphasizes distinct features within the
    image, akin to channels in an image. To align our data format with PyTorch requirements,
    we’ll stack these sequences as channels achieving a `[4, 240, 240]` tensor.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 为了符合 PyTorch 模型的规格，输入张量必须具有 `[batch_size, num_channels, height, width]` 的形状。在我们的
    MRI 数据中，四种模态（FLAIR、T1ce、T1、T2）各自强调图像中的不同特征，类似于图像中的通道。为了使数据格式符合 PyTorch 的要求，我们将这些序列堆叠为通道，从而实现
    `[4, 240, 240]` 张量。
- en: PyTorch provide two data utilities, `torch.utils.data.Dataset` and `torch.utils.data.DataLoader`,
    designed for iterating over datasets and loading data in batches. The `Dataset`
    includes subclasses for various standard datasets like `MNIST`, `CIFAR`, or `ImageNet`.
    Importing these datasets involves loading the respective class and initializing
    the `DataLoader`.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch 提供了两个数据工具，`torch.utils.data.Dataset` 和 `torch.utils.data.DataLoader`，旨在迭代数据集和批量加载数据。`Dataset`
    包含各种标准数据集的子类，如 `MNIST`、`CIFAR` 或 `ImageNet`。导入这些数据集涉及加载相应的类并初始化 `DataLoader`。
- en: 'Consider an example with the `MNIST` dataset:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一个 `MNIST` 数据集的示例：
- en: This enables us to obtain the final tensor with dimensions `[batch_size=32,
    num_channels=1, H=28, W=28]` tensor .
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 这使我们能够获得最终的张量，其维度为 `[batch_size=32, num_channels=1, H=28, W=28]` 张量。
- en: Since we have a non-trivial dataset, the creation of a custom `Dataset class`
    is necessary before using the `DataLoader`. While the detailed steps for creating
    this custom dataset are not covered in this post, readers are referred to the
    PyTorch tutorial on [Datasets & DataLoaders](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html)
    for comprehensive guidance.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: PyTorch Model Preparation
  id: totrans-75
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[PyTorch](https://pytorch.org/) is a DL framework developed by Facebook AI
    researchers in 2017\. The `torchvision` package contains popular datasets, image
    transformations and model architectures. In `[torchvision.models](https://pytorch.org/vision/0.8/models.html)`
    we can find the implementation of some DL architectures for different tasks, such
    as classification, segmentation or object detection.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: For our application we will load the `ResNet18`architecture.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: In a neural network, the input and output layers are inherently linked to the
    specific problem being addressed. PyTorch DL models typically expect 3-channel
    RGB images as input, as we can observe in the initial convolutional layer’s configuration
    `Conv2d(in_channels = 3, out_channels = 64, kernel_size=(7,7), stride=(2,2), padding=(3,3),
    bias=False)`. Additionaly, the final layer `Linear(in_features = 512, out_features
    = 1000, bias = True)` defaults to an output size of 1000, representing the number
    of classes in the ImageNet dataset.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
- en: Before training a classification model, it is essential to adjust `in_channels`
    and `out_features` to align with our specific dataset. We can access the first
    convolutional layer through `resnet.conv1`and update the `in_channels`. Similarly,
    we can access the last fully connected layer through`resnet.fc` and modify the
    `out_features`.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Image Classification
  id: totrans-82
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With our model and data prepared for training, we can proceed to its practical
    application.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: The following example illustrates how we can effectively utilize our ResNet18
    for classifying an image into low or high-grade. To manage the batch size dimension
    in our tensor, we will simply add a unit dimension (note that this is normally
    handled by the dataloader).
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: And that brings us to the end of the post. I hope this has been useful for those
    venturing into the intersection of MRI and Deep Learning. Thank you for taking
    the time to read. For deeper insights into my research, feel free to explore my
    recent paper! :)
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
- en: 'Pitarch, C.; Ribas, V.; Vellido, A. AI-Based Glioma Grading for a Trustworthy
    Diagnosis: An Analytical Pipeline for Improved Reliability. *Cancers* **2023**,
    *15*, 3369\. [https://doi.org/10.3390/cancers15133369](https://doi.org/10.3390/cancers15133369).'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
- en: '*Unless otherwise noted, all images are by the author.*'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: '[1] Y. LeCun et al. “Backpropagation Applied to Handwritten Zip Code'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: 'Recognition”. In: Neural Computation 1 (4 Dec. 1989), pp. 541–551.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: 'issn: 0899–7667\. doi: [10.1162/NECO.1989.1.4.541](https://direct.mit.edu/neco/article-abstract/1/4/541/5515/Backpropagation-Applied-to-Handwritten-Zip-Code?redirectedFrom=fulltext).'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: '[2] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. “ImageNet'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: 'Classification with Deep Convolutional Neural Networks”. In: Advances'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: in Neural Information Processing Systems 25 (2012).
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: '[3] Christian Szegedy et al. “Going Deeper with Convolutions”. In: Pro-'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: ceedings of the IEEE Computer Society Conference on Computer Vi-
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: sion and Pattern Recognition 07–12-June-2015 (Sept. 2014), pp. 1–9.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: 'issn: 10636919\. doi: [10.1109/CVPR.2015.7298594](https://ieeexplore.ieee.org/document/7298594/).'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
- en: '[4] Karen Simonyan and Andrew Zisserman. “Very Deep Convolutional'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
- en: 'Networks for Large-Scale Image Recognition”. In: 3rd International'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: Conference on Learning Representations, ICLR 2015 — Conference Track
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: Proceedings (Sept. 2014).
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: '[5] Kaiming He et al. “Deep Residual Learning for Image Recognition”.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: 'In: Proceedings of the IEEE Computer Society Conference on Computer'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: Vision and Pattern Recognition 2016-December (Dec. 2015), pp. 770–
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: '778\. issn: 10636919\. doi: [10.1109/CVPR.2016.90](https://ieeexplore.ieee.org/document/7780459).'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: '[6] Ashish Vaswani et al. “Attention Is All You Need”. In: Advances in'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: 'Neural Information Processing Systems 2017-December (June 2017), pp. 5999–6009\.
    ISSN: 10495258\. DOI: [10.48550/arxiv.1706.03762](https://arxiv.org/abs/1706.03762).'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: '[7] Lisa M. DeAngelis. “Brain Tumors”. In: New England journal of medicine
    344 (2 Aug. 2001), pp. 114–123\. issn: 0028–4793\. doi: [10.1056/NEJM200101113440207](https://www.nejm.org/doi/full/10.1056/NEJM200101113440207)'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
