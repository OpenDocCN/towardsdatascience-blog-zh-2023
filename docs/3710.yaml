- en: Dealing with MRI and Deep Learning with Python
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/dealing-with-mri-and-deep-learning-with-python-c88f3dae0620?source=collection_archive---------3-----------------------#2023-12-20](https://towardsdatascience.com/dealing-with-mri-and-deep-learning-with-python-c88f3dae0620?source=collection_archive---------3-----------------------#2023-12-20)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: A Comprehensive Guide to MRI Analysis through Deep Learning models in PyTorch
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@carlapitarch?source=post_page-----c88f3dae0620--------------------------------)[![Carla
    Pitarch Abaigar](../Images/f0f7f963947f59399c8b3e6ac9d9aac9.png)](https://medium.com/@carlapitarch?source=post_page-----c88f3dae0620--------------------------------)[](https://towardsdatascience.com/?source=post_page-----c88f3dae0620--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----c88f3dae0620--------------------------------)
    [Carla Pitarch Abaigar](https://medium.com/@carlapitarch?source=post_page-----c88f3dae0620--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ff2cd70d9ae7e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdealing-with-mri-and-deep-learning-with-python-c88f3dae0620&user=Carla+Pitarch+Abaigar&userId=f2cd70d9ae7e&source=post_page-f2cd70d9ae7e----c88f3dae0620---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----c88f3dae0620--------------------------------)
    ·13 min read·Dec 20, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fc88f3dae0620&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdealing-with-mri-and-deep-learning-with-python-c88f3dae0620&user=Carla+Pitarch+Abaigar&userId=f2cd70d9ae7e&source=-----c88f3dae0620---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc88f3dae0620&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdealing-with-mri-and-deep-learning-with-python-c88f3dae0620&source=-----c88f3dae0620---------------------bookmark_footer-----------)![](../Images/c7797e5acea961e4be445d2986a13b7b.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Photo by [Olga Rai](https://stock.adobe.com/es/contributor/209778624/olga-rai?load_type=author&prev_url=detail)
    on [Adobe Stock](https://stock.adobe.com/).
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: First of all, I’d like to introduce myself. My name is Carla Pitarch, and I
    am a PhD candidate in AI. My research centers on developing an automated brain
    tumor grade classification system by extracting information from Magnetic Resonance
    Images (MRIs) using Deep Learning (DL) models, particularly Convolutional Neural
    Networks (CNNs).
  prefs: []
  type: TYPE_NORMAL
- en: At the start of my PhD journey, diving into MRI data and DL was a whole new
    world. The initial steps for running models in this realm were not as straightforward
    as expected. Despite spending some time researching in this domain, I found a
    lack of comprehensive repositories guiding the initiation into both MRI and DL.
    Therefore, I decided to share some of the knowledge I have gained over this period,
    hoping it makes your journey a tad smoother.
  prefs: []
  type: TYPE_NORMAL
- en: 'Embarking on Computer Vision (CV) tasks through DL often involves using standard
    public image datasets such as `[ImageNe](https://image-net.org/about.php)t` ,
    characterized by 3-channel RGB natural images. PyTorch models are primed for these
    specifications, expecting input images to be in this format. However, when our
    image data comes from a distinct domain, like the medical field, diverging in
    both format and features from these natural image datasets, it presents challenges.
    This post delves into this issue, emphasizing two crucial preparatory steps before
    model implementation: aligning the data with the model’s requirements and preparing
    the model to effectively process our data.'
  prefs: []
  type: TYPE_NORMAL
- en: Background
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let’s start with a brief overview of the fundamental aspects of CNNs and MRI.
  prefs: []
  type: TYPE_NORMAL
- en: Convolutional Neural Networks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we delve into the realm of CNNs, assuming readers have a foundational
    understanding of DL. CNNs stand as the gold standard architectures in CV, specializing
    in the processing of 2D and 3D input image data. Our focus within this post will
    be centered on the processing of 2D image data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Image classification, associating output classes or labels with input images,
    is a core task in CNNs. The pioneering LeNet5 architecture introduced by LeCun
    et al.¹ in 1989 laid the groundwork for CNNs. This architecture can be summarized
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5559d225d2df07a468c8e1431111710a.png)'
  prefs: []
  type: TYPE_IMG
- en: CNN architecture with two convolution layers, two pooling layers, and one fully-connected
    layer previous to the output layer.
  prefs: []
  type: TYPE_NORMAL
- en: '2D CNN architectures operate by receiving image pixels as input, expecting
    an image to be a tensor with shape `Height x Width x Channels`. Color images typically
    consist of 3 channels: red, green and blue (RGB), while grayscale images consist
    of a single channel.'
  prefs: []
  type: TYPE_NORMAL
- en: A fundamental operation in CNNs is *convolution*, executed by applying a set
    of *filters* or *kernels* across all areas of the input data. The figure below
    shows an example of how convolution works in a 2D context.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b603b70f3daaf76704842d9beeb4c056.png)'
  prefs: []
  type: TYPE_IMG
- en: Example of a convolution over a 5x5 image with a 3x3 filter that produces a
    3x3 convolved feature.
  prefs: []
  type: TYPE_NORMAL
- en: 'The process involves sliding the filter across the image to the right and compute
    the weighted sum to obtain a convolved feature map. The output will represent
    whether a specific visual pattern, for instance an edge, is recognized at that
    location in the input image. Following each convolutional layer, an activation
    function introduces non-linearity. Popular choices include: ReLU (Rectified Linear
    Unit), Leaky ReLu, Sigmoid, Tanh, and Softmax. For further details on each activation
    function, this post provides clear explanations [Activation Functions in Neural
    Networks | by SAGAR SHARMA | Towards Data Science](/activation-functions-neural-networks-1cbd9f8d91d6).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Different types of layers contribute to the construction of CNNs, each playing
    a distinct role in defining the network’s functionality. Alongside convolutional
    layers, several other prominent layers used in CNNs include:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Pooling layers*, like max-pooling or average-pooling, efffectively reduce
    feature map dimensions while preserving essential information.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Dropout layers* are used to prevent overfitting by randomly deactivating a
    fraction of neurons during training, thereby enhancing the network’s generalization
    ability.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Batch normalization layers* focus on standardizing inputs for each layer,
    which accelerates network training.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Fully connected (FC) layers* establish connections between all neurons in
    one layer and all activations from the preceding layer, integrating learned features
    to facilitate final classifications.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: CNNs learn to identify patterns hierarchically. Initial layers focus on low-level
    features, progressively moving to highly abstract features in deeper layers. Upon
    reaching the FC layer, the *Softmax* activation function estimates class probabilities
    for the image input.
  prefs: []
  type: TYPE_NORMAL
- en: Beyond LeNet’s inception, notable CNN architectures like AlexNet², GoogLeNet³,
    VGGNet⁴, ResNet⁵, and more recent Transformer⁶ have significantly contributed
    to the realm of DL.
  prefs: []
  type: TYPE_NORMAL
- en: Natural Images Overview
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Exploring natural 2D images provides a foundational understanding of image data.
    To begin, we will delve into some examples.
  prefs: []
  type: TYPE_NORMAL
- en: For our first example we will select an image from the widely known `MNIST`
    dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c7dea9d21bec3e04cacbd81542d31621.png)'
  prefs: []
  type: TYPE_IMG
- en: This image shape is `[28,28]` , representing a grayscale image with a single
    channel. Then, the image input for a neural network would be `(28*28*1)`.
  prefs: []
  type: TYPE_NORMAL
- en: Now let’s explore an image from the `ImageNet` dataset. You can access this
    dataset directly from ImageNet’s website [ImageNet (image-net.org)](https://www.image-net.org/download.php)
    or explore a subset available on Kaggle [ImageNet Object Localization Challenge
    | Kaggle](https://www.kaggle.com/c/imagenet-object-localization-challenge/data).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/df30982867a148631172830db8d3e469.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can decompose this image into its RGB channels:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b1e48588369c8dab689156f44807f580.png)'
  prefs: []
  type: TYPE_IMG
- en: Since the shape of this image is `[500, 402, 3]` the image input of a neural
    network would be represented as `(500*402*3)`.
  prefs: []
  type: TYPE_NORMAL
- en: Magnetic Resonance Imaging
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The MRI scan is the most widely used test in neurology and neurosurgery, offering
    a non-invasive method that provides good soft tissue contrast⁷. Beyond visualizing
    structural details, MR imaging delves deeper, providing valuable insights into
    both the structural and functional aspects of the brain.
  prefs: []
  type: TYPE_NORMAL
- en: 'MRIs, constitute 3D volumes that enable visualization across the three anatomical
    planes: axial, coronal, and sagittal. These volumes are composed of 3D cubes known
    as *voxels,* in contrast to the standard 2D images, which are made up of 2D squares
    called *pixels*. While 3D volumes offer comprehensive data, they can also be decomposed
    into 2D slices.'
  prefs: []
  type: TYPE_NORMAL
- en: Diverse MRI modalities or sequences, such as T1, T1 with gadolinium contrast
    enhancement (T1ce), T2, and FLAIR (Fluid Attenuated Inversion Recovery), are normally
    used for diagnosis. These sequences enable the differentiation of tumor compartments
    by offering distinct signal intensities that correspond to specific regions or
    tissues. Below is an illustration presenting these four sequences from a single
    patient diagnosed with glioblastoma, known as the most aggressive form among gliomas,
    the most prevalent primary brain tumors.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2c62aed363ebf89ac07d86dff9e62788.png)'
  prefs: []
  type: TYPE_IMG
- en: Brain Tumor Segmentation Data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[Brain Tumor Segmentation (BraTS) Challenge](https://www.med.upenn.edu/cbica/brats/)
    made available one of the most extensive multi-modal brain MRI scans datasets
    of glioma patients spanning from 2012 to 2023\. The primary goal of the BraTS
    competition is to evaluate the state-of-the-art methodologies for segmenting brain
    tumors in multi-modal MRI scans, although additional tasks have been added over
    time.'
  prefs: []
  type: TYPE_NORMAL
- en: BraTS dataset provides clinical information about the tumors, including a binary
    label indicating the tumor grade (low-grade or high-grade). BraTS scans are available
    as NIfTI files and describe T1, T1ce, T2 and Flair modalities. The scans are provided
    after some pre-processing steps, including co-registration to the same anatomical
    template, interpolation to a uniform isotropic resolution (1mm³), and skull-stripping.
  prefs: []
  type: TYPE_NORMAL
- en: In this post we will use the 2020 dataset from Kaggle [BraTS2020 Dataset (Training
    + Validation)](https://www.kaggle.com/datasets/awsaf49/brats20-dataset-training-validation/)
    to classify glioma MRIs into low or high grade.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following images display examples of low-grade and high-grade gliomas:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/fb787a68fa811c4b537df3a9baf3db00.png)'
  prefs: []
  type: TYPE_IMG
- en: MRI modalities and segmentation mask for BraTS patient 287.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/51ada07cfa2f5f66a8fe47fd274f8eb6.png)'
  prefs: []
  type: TYPE_IMG
- en: MRI modalities and segmentation mask for BraTS patient 006.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Kaggle repository comprises 369 directories, each representing a patient
    and containing corresponding image data. Additionally, it contains two `.csv`
    files: *name_mapping.csv* and *survival_info.csv*. For our purpose, we will utilize
    *name_mapping.csv*, which links BraTS patient names to the [TCGA-LGG](https://wiki.cancerimagingarchive.net/pages/viewpage.action?pageId=5309188)
    and [TCGA-GBM](https://wiki.cancerimagingarchive.net/pages/viewpage.action?pageId=1966258)
    public datasets available from the [Cancer Imaging Archive](https://www.cancerimagingarchive.net/).
    This file not only facilitates name mapping but also provides tumor grade labels
    (LGG-HGG).'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2fdfda70921bc12e6049c468e389269a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let’s explore the contents of each patient folder, taking *Patient 006* as
    an example. Within the *BraTS20_Training_006* folder, we find 5 files, each corresponding
    to one MRI modality and the segmentation mask:'
  prefs: []
  type: TYPE_NORMAL
- en: '*BraTS20_Training_006_flair.nii*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*BraTS20_Training_006_t1.nii*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*BraTS20_Training_006_t1ce.nii*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*BraTS20_Training_006_t2.nii*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*BraTS20_Training_006_seg.nii*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These files are in the `.nii` format, which represents the NIfTI format — one
    of the most prevalent in neuroimaging.
  prefs: []
  type: TYPE_NORMAL
- en: MRI Data Preparation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To handle NIfTI images in Python, we can use the [NiBabel](https://nipy.org/nibabel/)
    package. Below is an example of the data loading. By using the `get_fdata()` method
    we can interpret the image as a numpy array.
  prefs: []
  type: TYPE_NORMAL
- en: 'The array shape, `[240, 240, 155]`, indicates a 3D volume comprising 240 2D
    slices within the x and y dimensions, and 155 slices in the z dimension. These
    dimensions correspond to distinct anatomical perspectives: the axial view (x-y
    plane), coronal view (x-z plane), and sagittal view (y-z plane).'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c9fb55cd80b92c300735cec86ff32d34.png)'
  prefs: []
  type: TYPE_IMG
- en: To make it simpler, we will only employ 2D slices in axial plane, then the resulting
    images will have shape `[240, 240]` .
  prefs: []
  type: TYPE_NORMAL
- en: In order to meet the specifications of PyTorch models, input tensors must have
    shape `[batch_size, num_channels, height, width]`. In our MRI data, each of the
    four modalities (FLAIR, T1ce, T1, T2) emphasizes distinct features within the
    image, akin to channels in an image. To align our data format with PyTorch requirements,
    we’ll stack these sequences as channels achieving a `[4, 240, 240]` tensor.
  prefs: []
  type: TYPE_NORMAL
- en: PyTorch provide two data utilities, `torch.utils.data.Dataset` and `torch.utils.data.DataLoader`,
    designed for iterating over datasets and loading data in batches. The `Dataset`
    includes subclasses for various standard datasets like `MNIST`, `CIFAR`, or `ImageNet`.
    Importing these datasets involves loading the respective class and initializing
    the `DataLoader`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider an example with the `MNIST` dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: This enables us to obtain the final tensor with dimensions `[batch_size=32,
    num_channels=1, H=28, W=28]` tensor .
  prefs: []
  type: TYPE_NORMAL
- en: Since we have a non-trivial dataset, the creation of a custom `Dataset class`
    is necessary before using the `DataLoader`. While the detailed steps for creating
    this custom dataset are not covered in this post, readers are referred to the
    PyTorch tutorial on [Datasets & DataLoaders](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html)
    for comprehensive guidance.
  prefs: []
  type: TYPE_NORMAL
- en: PyTorch Model Preparation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[PyTorch](https://pytorch.org/) is a DL framework developed by Facebook AI
    researchers in 2017\. The `torchvision` package contains popular datasets, image
    transformations and model architectures. In `[torchvision.models](https://pytorch.org/vision/0.8/models.html)`
    we can find the implementation of some DL architectures for different tasks, such
    as classification, segmentation or object detection.'
  prefs: []
  type: TYPE_NORMAL
- en: For our application we will load the `ResNet18`architecture.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: In a neural network, the input and output layers are inherently linked to the
    specific problem being addressed. PyTorch DL models typically expect 3-channel
    RGB images as input, as we can observe in the initial convolutional layer’s configuration
    `Conv2d(in_channels = 3, out_channels = 64, kernel_size=(7,7), stride=(2,2), padding=(3,3),
    bias=False)`. Additionaly, the final layer `Linear(in_features = 512, out_features
    = 1000, bias = True)` defaults to an output size of 1000, representing the number
    of classes in the ImageNet dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Before training a classification model, it is essential to adjust `in_channels`
    and `out_features` to align with our specific dataset. We can access the first
    convolutional layer through `resnet.conv1`and update the `in_channels`. Similarly,
    we can access the last fully connected layer through`resnet.fc` and modify the
    `out_features`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Image Classification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With our model and data prepared for training, we can proceed to its practical
    application.
  prefs: []
  type: TYPE_NORMAL
- en: The following example illustrates how we can effectively utilize our ResNet18
    for classifying an image into low or high-grade. To manage the batch size dimension
    in our tensor, we will simply add a unit dimension (note that this is normally
    handled by the dataloader).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: And that brings us to the end of the post. I hope this has been useful for those
    venturing into the intersection of MRI and Deep Learning. Thank you for taking
    the time to read. For deeper insights into my research, feel free to explore my
    recent paper! :)
  prefs: []
  type: TYPE_NORMAL
- en: 'Pitarch, C.; Ribas, V.; Vellido, A. AI-Based Glioma Grading for a Trustworthy
    Diagnosis: An Analytical Pipeline for Improved Reliability. *Cancers* **2023**,
    *15*, 3369\. [https://doi.org/10.3390/cancers15133369](https://doi.org/10.3390/cancers15133369).'
  prefs: []
  type: TYPE_NORMAL
- en: '*Unless otherwise noted, all images are by the author.*'
  prefs: []
  type: TYPE_NORMAL
- en: '[1] Y. LeCun et al. “Backpropagation Applied to Handwritten Zip Code'
  prefs: []
  type: TYPE_NORMAL
- en: 'Recognition”. In: Neural Computation 1 (4 Dec. 1989), pp. 541–551.'
  prefs: []
  type: TYPE_NORMAL
- en: 'issn: 0899–7667\. doi: [10.1162/NECO.1989.1.4.541](https://direct.mit.edu/neco/article-abstract/1/4/541/5515/Backpropagation-Applied-to-Handwritten-Zip-Code?redirectedFrom=fulltext).'
  prefs: []
  type: TYPE_NORMAL
- en: '[2] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. “ImageNet'
  prefs: []
  type: TYPE_NORMAL
- en: 'Classification with Deep Convolutional Neural Networks”. In: Advances'
  prefs: []
  type: TYPE_NORMAL
- en: in Neural Information Processing Systems 25 (2012).
  prefs: []
  type: TYPE_NORMAL
- en: '[3] Christian Szegedy et al. “Going Deeper with Convolutions”. In: Pro-'
  prefs: []
  type: TYPE_NORMAL
- en: ceedings of the IEEE Computer Society Conference on Computer Vi-
  prefs: []
  type: TYPE_NORMAL
- en: sion and Pattern Recognition 07–12-June-2015 (Sept. 2014), pp. 1–9.
  prefs: []
  type: TYPE_NORMAL
- en: 'issn: 10636919\. doi: [10.1109/CVPR.2015.7298594](https://ieeexplore.ieee.org/document/7298594/).'
  prefs: []
  type: TYPE_NORMAL
- en: '[4] Karen Simonyan and Andrew Zisserman. “Very Deep Convolutional'
  prefs: []
  type: TYPE_NORMAL
- en: 'Networks for Large-Scale Image Recognition”. In: 3rd International'
  prefs: []
  type: TYPE_NORMAL
- en: Conference on Learning Representations, ICLR 2015 — Conference Track
  prefs: []
  type: TYPE_NORMAL
- en: Proceedings (Sept. 2014).
  prefs: []
  type: TYPE_NORMAL
- en: '[5] Kaiming He et al. “Deep Residual Learning for Image Recognition”.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In: Proceedings of the IEEE Computer Society Conference on Computer'
  prefs: []
  type: TYPE_NORMAL
- en: Vision and Pattern Recognition 2016-December (Dec. 2015), pp. 770–
  prefs: []
  type: TYPE_NORMAL
- en: '778\. issn: 10636919\. doi: [10.1109/CVPR.2016.90](https://ieeexplore.ieee.org/document/7780459).'
  prefs: []
  type: TYPE_NORMAL
- en: '[6] Ashish Vaswani et al. “Attention Is All You Need”. In: Advances in'
  prefs: []
  type: TYPE_NORMAL
- en: 'Neural Information Processing Systems 2017-December (June 2017), pp. 5999–6009\.
    ISSN: 10495258\. DOI: [10.48550/arxiv.1706.03762](https://arxiv.org/abs/1706.03762).'
  prefs: []
  type: TYPE_NORMAL
- en: '[7] Lisa M. DeAngelis. “Brain Tumors”. In: New England journal of medicine
    344 (2 Aug. 2001), pp. 114–123\. issn: 0028–4793\. doi: [10.1056/NEJM200101113440207](https://www.nejm.org/doi/full/10.1056/NEJM200101113440207)'
  prefs: []
  type: TYPE_NORMAL
