- en: 'Efficient Image Segmentation Using PyTorch: Part 3'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用PyTorch进行高效的图像分割：第3部分
- en: 原文：[https://towardsdatascience.com/efficient-image-segmentation-using-pytorch-part-3-3534cf04fb89?source=collection_archive---------7-----------------------#2023-06-27](https://towardsdatascience.com/efficient-image-segmentation-using-pytorch-part-3-3534cf04fb89?source=collection_archive---------7-----------------------#2023-06-27)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/efficient-image-segmentation-using-pytorch-part-3-3534cf04fb89?source=collection_archive---------7-----------------------#2023-06-27](https://towardsdatascience.com/efficient-image-segmentation-using-pytorch-part-3-3534cf04fb89?source=collection_archive---------7-----------------------#2023-06-27)
- en: Depthwise separable convolutions
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 深度可分离卷积
- en: '[](https://medium.com/@dhruvbird?source=post_page-----3534cf04fb89--------------------------------)[![Dhruv
    Matani](../Images/d63bf7776c28a29c02b985b1f64abdd3.png)](https://medium.com/@dhruvbird?source=post_page-----3534cf04fb89--------------------------------)[](https://towardsdatascience.com/?source=post_page-----3534cf04fb89--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----3534cf04fb89--------------------------------)
    [Dhruv Matani](https://medium.com/@dhruvbird?source=post_page-----3534cf04fb89--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@dhruvbird?source=post_page-----3534cf04fb89--------------------------------)[![Dhruv
    Matani](../Images/d63bf7776c28a29c02b985b1f64abdd3.png)](https://medium.com/@dhruvbird?source=post_page-----3534cf04fb89--------------------------------)[](https://towardsdatascience.com/?source=post_page-----3534cf04fb89--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----3534cf04fb89--------------------------------)
    [Dhruv Matani](https://medium.com/@dhruvbird?source=post_page-----3534cf04fb89--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F63f5d5495279&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fefficient-image-segmentation-using-pytorch-part-3-3534cf04fb89&user=Dhruv+Matani&userId=63f5d5495279&source=post_page-63f5d5495279----3534cf04fb89---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----3534cf04fb89--------------------------------)
    ·12 min read·Jun 27, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F3534cf04fb89&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fefficient-image-segmentation-using-pytorch-part-3-3534cf04fb89&user=Dhruv+Matani&userId=63f5d5495279&source=-----3534cf04fb89---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F63f5d5495279&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fefficient-image-segmentation-using-pytorch-part-3-3534cf04fb89&user=Dhruv+Matani&userId=63f5d5495279&source=post_page-63f5d5495279----3534cf04fb89---------------------post_header-----------)
    发布于[Towards Data Science](https://towardsdatascience.com/?source=post_page-----3534cf04fb89--------------------------------)
    · 12分钟阅读 · 2023年6月27日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F3534cf04fb89&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fefficient-image-segmentation-using-pytorch-part-3-3534cf04fb89&user=Dhruv+Matani&userId=63f5d5495279&source=-----3534cf04fb89---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F3534cf04fb89&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fefficient-image-segmentation-using-pytorch-part-3-3534cf04fb89&source=-----3534cf04fb89---------------------bookmark_footer-----------)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F3534cf04fb89&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fefficient-image-segmentation-using-pytorch-part-3-3534cf04fb89&source=-----3534cf04fb89---------------------bookmark_footer-----------)'
- en: In this 4-part series, we’ll implement image segmentation step by step from
    scratch using deep learning techniques in PyTorch. This part will focus on optimizing
    our CNN baseline model using depthwise separable convolutions to reduce the number
    of trainable parameters, making the model deployable on mobile and other edge
    devices.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个四部分系列中，我们将逐步实现图像分割，使用PyTorch中的深度学习技术从零开始。本部分将重点优化我们的CNN基线模型，通过使用深度可分离卷积来减少可训练参数的数量，使模型可以在移动设备和其他边缘设备上部署。
- en: Co-authored with [Naresh Singh](https://medium.com/@brocolishbroxoli)
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 与[Naresh Singh](https://medium.com/@brocolishbroxoli)共同撰写
- en: '![](../Images/f29d9d6903f7abb12a21cd463b98a2b2.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f29d9d6903f7abb12a21cd463b98a2b2.png)'
- en: 'Figure 1: Result of running image segmentation using a CNN with depth-wise
    separable convolutions instead of regular convolutions. From top to bottom, input
    images, ground truth segmentation masks, and predicted segmentation masks. Source:
    Author(s)'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：使用深度可分离卷积而非普通卷积进行图像分割的结果。从上到下依次为输入图像、真实分割掩码和预测分割掩码。来源：作者
- en: Article outline
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 文章大纲
- en: In this article, we will augment the [Convolutional Neural Network (CNN)](https://en.wikipedia.org/wiki/Convolutional_neural_network)
    we [built earlier](https://medium.com/p/bed68cadd7c7/) to reduce the number of
    learnable parameters in our network. The task of identifying pet pixels (pixels
    belonging to cats, dogs, hamsters, etc…) in an input image remains unchanged.
    Our network of choice will remain [SegNet](https://arxiv.org/abs/1511.00561),
    and the only change we’ll make is to replace our convolutional layers with depth-wise-separable-convolutions
    (DSC). Before we do this, we will dive into the theory and practice of depth-wise-separable-convolutions,
    and appreciate the idea behind the technique.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们将增强我们[早期构建的卷积神经网络（CNN）](https://medium.com/p/bed68cadd7c7/)，以减少网络中可学习参数的数量。识别输入图像中的宠物像素（属于猫、狗、仓鼠等的像素）这一任务保持不变。我们选择的网络将继续是[SegNet](https://arxiv.org/abs/1511.00561)，我们唯一的改变是用深度可分卷积（DSC）替换卷积层。在此之前，我们将深入探讨深度可分卷积的理论与实践，并欣赏这一技术背后的理念。
- en: Throughout this article, we will reference code and results from this [notebook](https://github.com/dhruvbird/ml-notebooks/blob/main/pets_segmentation/oxford-iiit-pets-segmentation-using-pytorch-segnet-and-depth-wise-separable-convs.ipynb)
    for model training, and this [notebook](https://github.com/dhruvbird/ml-notebooks/blob/main/pets_segmentation/types%20of%20convolutions.ipynb)
    for a primer on DSC. If you wish to reproduce the results, you’ll need a GPU to
    ensure that the first notebook completes running in a reasonable amount of time.
    The second notebook can be run on a regular CPU.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们将引用来自这个[笔记本](https://github.com/dhruvbird/ml-notebooks/blob/main/pets_segmentation/oxford-iiit-pets-segmentation-using-pytorch-segnet-and-depth-wise-separable-convs.ipynb)的代码和结果用于模型训练，以及这个[笔记本](https://github.com/dhruvbird/ml-notebooks/blob/main/pets_segmentation/types%20of%20convolutions.ipynb)作为DSC的入门。如果你希望复现结果，你需要一个GPU以确保第一个笔记本在合理的时间内完成运行。第二个笔记本可以在普通的CPU上运行。
- en: Articles in this series
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 本系列文章
- en: 'This series is for readers at all experience levels with deep learning. If
    you want to learn about the practice of deep learning and vision AI along with
    some solid theory and hands-on experience, you’ve come to the right place! This
    is expected to be a 4-part series with the following articles:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 本系列适合所有深度学习经验水平的读者。如果你想学习深度学习和视觉AI的实践，了解一些扎实的理论和实践经验，你来对地方了！预计将会有4篇文章的系列，内容如下：
- en: '[Concepts and Ideas](https://medium.com/p/89e8297a0923/)'
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[概念与想法](https://medium.com/p/89e8297a0923/)'
- en: '[A CNN-based model](https://medium.com/p/bed68cadd7c7/)'
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[基于CNN的模型](https://medium.com/p/bed68cadd7c7/)'
- en: '**Depthwise separable convolutions (this article)**'
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**深度可分卷积（本文）**'
- en: '[A Vision Transformer-based model](https://medium.com/p/6c86da083432/)'
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[基于视觉变换器的模型](https://medium.com/p/6c86da083432/)'
- en: Introduction
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: Let’s start this discussion with a closer look at the convolutions from the
    perspective of model size and computation cost. The number of trainable parameters
    is a good indication of the size of a model and the number of the tensor operations
    reflects the model complexity or computation cost. Consider that we have a convolution
    layer with n filters with size dₖ x dₖ. Further assume that this layer processes
    input with shape *m x h x w,* where *m* is the number of input channels, and *h*
    and *w* are height and width dimensions respectively. In this case, the convolution
    layer will produce an output with shape *n x h x w* as shown in Figure 2\. We
    are assuming that the convolution uses *stride=1*. Let’s go ahead and evaluate
    this setup in terms of trainable parameters and computation cost.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从模型大小和计算成本的角度深入探讨卷积。可训练参数的数量是模型大小的一个很好的指标，而张量操作的数量反映了模型的复杂性或计算成本。考虑一个具有n个dₖ
    x dₖ大小滤波器的卷积层。进一步假设此层处理形状为*m x h x w*的输入，其中*m*是输入通道的数量，而*h*和*w*分别是高度和宽度维度。在这种情况下，卷积层将产生形状为*n
    x h x w*的输出，如图2所示。我们假设卷积使用*stride=1*。让我们继续评估这一设置的可训练参数和计算成本。
- en: '![](../Images/f05ac4a1e2764b6daf7b1c0384ff9c5f.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f05ac4a1e2764b6daf7b1c0384ff9c5f.png)'
- en: 'Figure 2: Regular convolutional filters applied to input to produce output.
    Assume stride=1 and padding=dₖ-2\. Source: [Efficient Deep Learning Book](https://efficientdlbook.com/)'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：常规卷积滤波器应用于输入以产生输出。假设stride=1和padding=dₖ-2。来源：[高效深度学习书](https://efficientdlbook.com/)
- en: '**Evaluation Of Trainable Parameters:** We have *n* filters, each of which
    has *m x dₖ x dₖ* learnable parameters. This results in a total of *n x m x dₖ
    x dₖ* learnable parameters. Bias terms are ignored to simplify this discussion.
    Let’s look at the PyTorch code below to validate our understanding.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '**可训练参数的评估：** 我们有 *n* 个滤波器，每个滤波器都有 *m x dₖ x dₖ* 个可训练参数。这总共会有 *n x m x dₖ x
    dₖ* 个可训练参数。为了简化讨论，忽略了偏置项。我们来看下面的 PyTorch 代码以验证我们的理解。'
- en: '[PRE0]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Prints the following.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 打印如下内容。
- en: '[PRE1]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Now, let’s evaluate the computation costs of convolution.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们评估卷积的计算成本。
- en: '**Evaluation Of Computational Cost:** A single convolutional filter of shape
    *m x dₖ x dₖ* when run with a *stride=1* and a *padding=dₖ-2* on an input with
    size *h x w* will apply the convolutional filter *h x w* times, once for each
    image section with size *dₖ x dₖ* amounting to a total of *h x w* sections. It
    results in a cost of *m x dₖ x dₖ x h x w* per filter or output channel. Since
    we wish to compute n output channels, the total cost will be *m x dₖ x dₖ x h
    x n*. Let’s go ahead and validate this using the torchinfo PyTorch package.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '**计算成本的评估：** 一个形状为 *m x dₖ x dₖ* 的单个卷积滤波器在对尺寸为 *h x w* 的输入进行 *stride=1* 和 *padding=dₖ-2*
    操作时，会对每个尺寸为 *dₖ x dₖ* 的图像区域应用卷积滤波器 *h x w* 次，总共 *h x w* 个区域。这导致每个滤波器或输出通道的成本为
    *m x dₖ x dₖ x h x w*。由于我们希望计算 n 个输出通道，因此总成本为 *m x dₖ x dₖ x h x n*。让我们使用 torchinfo
    PyTorch 包来验证这一点。'
- en: '[PRE2]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Will print the following.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 将打印如下内容。
- en: '[PRE3]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'If we ignore the implementation details of a convolution layer for a moment,
    we would realize that, on a high level, a convolution layer just transforms a
    *m x h x w* input into a *n x h x w* output. The transformation is achieved through
    trainable filters which progressively learn features as they *see* inputs. The
    question that follows is: Is it possible to achieve this transformation using
    fewer learnable parameters and simultaneously ensuring minimum compromise in the
    learning capabilities of the layer? Depthwise Separable Convolutions were proposed
    to answer this exact question. Let’s understand them in detail and learn how they
    stack up on our evaluation metrics.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们暂时忽略卷积层的实现细节，我们会发现，从高层次来看，卷积层只是将 *m x h x w* 的输入转换为 *n x h x w* 的输出。这一转换是通过可训练的滤波器实现的，滤波器在“看到”输入时逐步学习特征。接下来的问题是：是否可以使用更少的可训练参数来实现这一转换，同时确保层的学习能力不会受到最小妥协？深度可分离卷积旨在回答这个确切的问题。让我们详细了解它们，并学习它们在我们评估指标上的表现。
- en: Depthwise Separable Convolution
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度可分离卷积
- en: The concept of Depthwise Separable Convolutions (DSC) was first proposed by
    Laurent Sifre in their PhD thesis titled [Rigid-Motion Scattering For Image Classification](https://www.di.ens.fr/data/publications/papers/phd_sifre.pdf).
    Since then, they have been used successfully in various popular deep convolutional
    networks such as [XceptionNet](https://arxiv.org/abs/1610.02357) and [MobileNet](/review-mobilenetv1-depthwise-separable-convolution-light-weight-model-a382df364b69).
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 深度可分离卷积（DSC）的概念最早由 Laurent Sifre 在其博士论文《[刚性运动散射用于图像分类](https://www.di.ens.fr/data/publications/papers/phd_sifre.pdf)》中提出。从那时起，它们在各种流行的深度卷积网络中成功应用，例如
    [XceptionNet](https://arxiv.org/abs/1610.02357) 和 [MobileNet](/review-mobilenetv1-depthwise-separable-convolution-light-weight-model-a382df364b69)。
- en: 'The main difference between a regular convolution, and a DSC is that a DSC
    is composed of 2 convolutions as described below:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 普通卷积与 DSC 之间的主要区别在于 DSC 由 2 个卷积组成，如下所述：
- en: A **depthwise grouped convolution**, where the number of input channels m is
    equal to the number of output channels such that each output channel is affected
    only by a single input channel. In PyTorch, this is called a “grouped” convolution.
    You can read more about grouped convolutions in PyTorch [here](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html).
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**深度分组卷积**，其中输入通道 m 的数量等于输出通道的数量，使得每个输出通道仅受单个输入通道的影响。在 PyTorch 中，这被称为“分组”卷积。你可以在
    PyTorch 的 [这里](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html)
    阅读更多关于分组卷积的信息。'
- en: A **pointwise convolution** (filter size=1), which operates like a regular convolution
    such that each of the n filters operates on all m input channels to produce a
    single output value.
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**逐点卷积**（滤波器大小=1），其工作方式类似于普通卷积，每个 n 个滤波器作用于所有 m 个输入通道，以产生单个输出值。'
- en: '![](../Images/0f229cb577579524f10f7960aaa3e0b3.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0f229cb577579524f10f7960aaa3e0b3.png)'
- en: 'Figure 3: Depthwise Separable Convolution filters applied to input to produce
    output. Assume *stride=1* and *padding=dₖ-2*. Source: [Efficient Deep Learning
    Book](https://efficientdlbook.com/)'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：深度可分卷积滤波器应用于输入以生成输出。假设 *stride=1* 和 *padding=dₖ-2*。来源：[高效深度学习书](https://efficientdlbook.com/)
- en: Let’s perform the same exercise that we did for regular convolutions for DSCs
    and compute the number of trainable parameters and computations.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们对 DSC 进行与常规卷积相同的操作，计算可训练参数和计算量。
- en: '**Evaluation Of Trainable Parameters:** The “grouped” convolutions have m filters,
    each of which has *dₖ x dₖ* learnable parameters which produces m output channels.
    This results in a total of *m x dₖ x dₖ* learnable parameters. The pointwise convolution
    has n filters of size *m x 1 x 1* which adds up to *n x m x 1 x 1* learnable parameters.
    Let’s look at the PyTorch code below to validate our understanding.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '**可训练参数的评估：** “分组”卷积有 m 个滤波器，每个滤波器有 *dₖ x dₖ* 个可学习的参数，生成 m 个输出通道。这导致总共 *m x
    dₖ x dₖ* 个可学习的参数。点卷积有 n 个大小为 *m x 1 x 1* 的滤波器，合计为 *n x m x 1 x 1* 个可学习的参数。让我们查看下面的
    PyTorch 代码以验证我们的理解。'
- en: '[PRE4]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Which will print.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 将打印。
- en: '[PRE5]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: We can see that the DSC version has roughly *7x* less parameters. Next, let’s
    focus our attention on the computation costs for a DSC layer.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，DSC 版本的参数大约少 *7x*。接下来，让我们关注 DSC 层的计算成本。
- en: '**Evaluation Of Computational Cost:** Let’s assume our input has spatial dimensions
    *m x h x w*. In the grouped convolution segment of DSC, we have **m** filters,
    each with size *dₖ x dₖ*. A filter is applied to its corresponding input channel
    resulting in the segment cost of *m x dₖ x dₖ x h x w*. For the pointwise convolution,
    we apply **n** filters of size *m x 1 x 1*to produce **n** output channels. This
    results in the segment cost of *n x m x 1 x 1 x h x w*. We need to add up the
    costs of the grouped and pointwise operations to compute the total cost. Let’s
    go ahead and validate this using the [torchinfo](https://pypi.org/project/torchinfo/)
    PyTorch package.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '**计算成本的评估：** 假设我们的输入空间维度为 *m x h x w*。在 DSC 的分组卷积部分，我们有 **m** 个大小为 *dₖ x dₖ*
    的滤波器。一个滤波器应用于其对应的输入通道，结果是 *m x dₖ x dₖ x h x w* 的段成本。对于点卷积，我们应用 **n** 个大小为 *m
    x 1 x 1* 的滤波器以生成 **n** 个输出通道。这导致 *n x m x 1 x 1 x h x w* 的段成本。我们需要将分组和点卷积操作的成本加起来计算总成本。让我们使用
    [torchinfo](https://pypi.org/project/torchinfo/) PyTorch 包验证这一点。'
- en: '[PRE6]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Which will print.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 将打印。
- en: '[PRE7]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Let’s compare the sizes and costs of both the convolutions for a few examples
    to gain some intuition.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过一些示例比较两种卷积的大小和成本，以获得一些直观的理解。
- en: Size and Cost comparison for regular and depthwise separable convolutions
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 常规卷积与深度可分卷积的大小和成本比较
- en: To compare the size and cost of regular and depthwise separable convolution,
    we will assume an input size of *128 x 128* to the network, a kernel size of *3
    x 3*, and a network that progressively halves the spatial dimensions and doubles
    the number of channel dimensions. We assume a single 2d-conv layer at every step,
    but in practice, there could be more.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 为了比较常规卷积和深度可分卷积的大小和成本，我们将假设输入大小为 *128 x 128*，卷积核大小为 *3 x 3*，网络逐步将空间维度减半并将通道维度加倍。我们假设每一步有一个
    2d-conv 层，但实际上可能会有更多。
- en: '![](../Images/4646694c069f37f3ed9be0287e6d4d40.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4646694c069f37f3ed9be0287e6d4d40.png)'
- en: 'Figure 4: Comparing the number of trainable parameters (size) and multi-adds
    (cost) of regular and depthwise separable convolutions. We also show the ratio
    of the size and cost for the 2 types of convolutions. Source: Author(s).'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：比较常规卷积和深度可分卷积的可训练参数（大小）和多次加法（成本）。我们还展示了两种卷积的大小和成本的比例。来源：作者。
- en: You can see that on average both the size and computational cost of DSC is about
    11% to 12% of the cost of regular convolutions for the configuration mentioned
    above.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到，平均而言，DSC 的大小和计算成本大约是上述配置常规卷积成本的 11% 到 12%。
- en: '![](../Images/12addf86c5b96f25ef62b5277e3360ea.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/12addf86c5b96f25ef62b5277e3360ea.png)'
- en: 'Figure 5: Relative size and cost of regular v/s DSC. Source: Author(s).'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：常规卷积与深度可分卷积的相对大小和成本。来源：作者。
- en: Now that we have developed a good understanding of the types of convolutions
    and their relative costs, you must be wondering if there’s any downside of using
    DSCs. Everything we’ve seen so far seems to suggest that they are better in every
    way! Well, we haven’t yet considered an important aspect which is the impact they
    have on the accuracy of our model. Let’s dive into it via an experiment below.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经对各种卷积类型及其相对成本有了很好的理解，你一定在想使用DSC是否有任何缺点。到目前为止，我们看到的一切似乎都表明它们在各方面都更好！不过，我们还没有考虑一个重要方面，即它们对模型准确性的影响。让我们通过下面的实验来探讨这个问题。
- en: SegNet Using Depthwise Separable Convolutions
  id: totrans-61
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用深度可分离卷积的SegNet
- en: '[This notebook](https://github.com/dhruvbird/ml-notebooks/blob/main/pets_segmentation/oxford-iiit-pets-segmentation-using-pytorch-segnet-and-depth-wise-separable-convs.ipynb)
    contains all the code for this section.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '[这个笔记本](https://github.com/dhruvbird/ml-notebooks/blob/main/pets_segmentation/oxford-iiit-pets-segmentation-using-pytorch-segnet-and-depth-wise-separable-convs.ipynb)包含了本节的所有代码。'
- en: We will adapt our SegNet model from the [previous post](https://medium.com/p/bed68cadd7c7/)
    and replace all the regular convolutional layers with a DSC layer. Once we do
    this, we notice that the number of parameters in our notebook drops from 15.27M
    to 1.75M parameters, which is a reduction of 88.5%! This is inline with our earlier
    estimates of an 11% to 12% reduction in the number of trainable parameters of
    the network.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从[之前的帖子](https://medium.com/p/bed68cadd7c7/)中调整我们的SegNet模型，并将所有常规卷积层替换为DSC层。这样做后，我们发现笔记本中的参数数量从15.27M减少到1.75M，减少了88.5%！这与我们之前估计的网络可训练参数减少11%到12%的一致。
- en: A similar configuration as [before](https://medium.com/p/bed68cadd7c7/) was
    used during model training and validation. The configuration is specified below.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在模型训练和验证过程中使用了与[之前](https://medium.com/p/bed68cadd7c7/)相似的配置。配置如下所示。
- en: The *random horizontal flip* and *colour jitter* data augmentations are applied
    to the training set to prevent overfitting
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*随机水平翻转*和*颜色抖动*数据增强方法被应用于训练集，以防止过拟合。'
- en: The images are resized to 128x128 pixels in a non-aspect preserving resize operation
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 图像在进行不保持长宽比的调整操作时被调整为128x128像素。
- en: No input normalization is applied to the images — instead a [batch normalization
    layer is used as the first layer of the model](/replace-manual-normalization-with-batch-normalization-in-vision-ai-models-e7782e82193c)
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 图像没有进行输入归一化处理，而是[使用批量归一化层作为模型的第一层](/replace-manual-normalization-with-batch-normalization-in-vision-ai-models-e7782e82193c)。
- en: The model is trained for 20 epochs using the Adam optimizer with a LR of 0.001
    and no LR scheduler
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 模型使用Adam优化器进行20个周期的训练，学习率为0.001，并且没有学习率调度器。
- en: The cross-entropy loss function is used to classify a pixel as belonging to
    a pet, the background, or a pet border
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 交叉熵损失函数用于将像素分类为宠物、背景或宠物边界。
- en: The model achieved a validation accuracy of 86.96% after 20 training epochs.
    This is less than the 88.28% accuracy achieved by the model using regular convolutions
    for the same number of training epochs. We have determined experimentally that
    training for more epochs improves the accuracy of both models, so 20 epochs is
    definitely not the end of the training cycle. We stop at 20 epochs for the purposes
    of this article for demonstration purposes.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型在20个训练周期后达到了86.96%的验证准确率。这低于使用常规卷积在相同训练周期内达到的88.28%准确率。我们已经通过实验确定，训练更多周期会提高两个模型的准确性，因此20个周期绝对不是训练周期的结束。为了本文章的演示目的，我们在20个周期后停止训练。
- en: We plotted a gif showing how the model is learning to predict the segmentation
    masks for 21 images in the validation set.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 我们绘制了一个gif，展示了模型如何学习预测验证集中21张图像的分割掩码。
- en: '![](../Images/aa9aec7b78d408275a886ec1f9bcafc1.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/aa9aec7b78d408275a886ec1f9bcafc1.png)'
- en: 'Figure 6: A gif showing how the SegNet model with DSC is learning to predict
    segmentation masks for 21 images in the validation set. Source: Author(s)'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：一个gif展示了使用DSC的SegNet模型如何学习预测验证集中21张图像的分割掩码。来源：作者
- en: Now that we have seen how the model progresses through the training cycle, let’s
    compare the train cycles of models with regular convolutions and DSC.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经看到模型如何通过训练周期的进展，让我们比较一下使用常规卷积和DSC的模型的训练周期。
- en: Accuracy Comparisons
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准确性比较
- en: We found it useful to look at the training cycles of the models using regular
    convolutions and DSC. The main difference we noticed is in the early phases (epochs)
    of training, after which both models settled roughly into the same prediction
    flow. In fact after training both models for 100 epochs, we noticed that the accuracy
    of the model with DSC is just about 1% less than the model with regular convolutions.
    This is inline with our observations from just 20 epochs of training.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 我们发现查看使用常规卷积和DSC的模型训练周期很有用。我们注意到的主要区别是在训练的早期阶段（周期），之后两种模型大致趋于相同的预测流程。实际上，在训练了100个周期后，我们发现使用DSC的模型准确性比使用常规卷积的模型低约1%。这与我们从仅20个周期的训练中观察到的结果一致。
- en: '![](../Images/e1142979b7b9b8d83e1a5eac150d630c.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e1142979b7b9b8d83e1a5eac150d630c.png)'
- en: 'Figure 7: A gif showing the progression of segmentation masks predicted by
    the SegNet model using regular convolutions v/s DSC. Source: Author(s).'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 图7：一个动图展示了使用常规卷积与DSC的SegNet模型预测的分割掩码进展。来源：作者。
- en: You would have noticed that both models get the predictions roughly right after
    just 6 training epochs — i.e. one can visually see that the models are predicting
    something useful. Most of the hard work of training the model is then above ensuring
    that the borders of the predicted masks are as tight as possible and as close
    to the actual pets in the image as possible. This means that while one can expect
    a lesser absolute increase in accuracy in the later training epochs, the impact
    of this on the quality of predictions is much more. We’ve noticed that a single
    digit of accuracy improvement at higher absolute accuracy values (going from 89%
    to 90%) results in significant qualitative improvements to the predictions.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会注意到，两种模型在仅经过6个训练周期后预测大致正确——即可以直观地看到模型预测了一些有用的东西。训练模型的大部分艰巨工作在于确保预测掩码的边界尽可能紧密，并尽可能接近图像中的实际物体。这意味着尽管在后续训练周期中，准确性的绝对提升可能较少，但这对预测质量的影响要大得多。我们注意到，在较高的绝对准确性值（例如从89%提升到90%）下，准确性的单个位数提升会显著改善预测质量。
- en: Comparison with a UNet model
  id: totrans-80
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 与UNet模型的比较
- en: We ran an experiment that changed a lot of hyperparameters with a focus on improving
    the overall accuracy to get a sense of how far this setting is from close to optimal.
    Here’s the configuration of that experiment.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 我们进行了一个实验，调整了很多超参数，重点是提高整体准确性，以了解这种设置与最优设置的接近程度。以下是该实验的配置。
- en: 'Image size: 128 x 128 — same as the experiments so far'
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 图像大小：128 x 128——与之前的实验相同
- en: 'Train epochs: 100 — current experiments trained for 20 epochs'
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练周期：100——当前实验训练了20个周期
- en: 'Augmentations: A lot more augmentations such as image rotation, channel dropping,
    random block removal. We used Albumentations instead of torchvision transforms.
    Albumentations automatically transforms segmentation masks for us'
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 数据增强：增加了很多数据增强技术，例如图像旋转、通道丢弃、随机块移除。我们使用了Albumentations而不是torchvision transforms。Albumentations会自动为我们转换分割掩码。
- en: 'LR Scheduler: A StepLR scheduler was used with a decay of 0.8x every 25 train
    epochs'
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 学习率调度器：使用了StepLR调度器，每25个训练周期衰减0.8倍
- en: 'Loss function: We tried 4 different loss functions: Cross Entropy, Focal, Dice,
    Weighted Cross Entropy. Dice performed worst whereas the rest were pretty much
    comparable to each other. In fact the difference in best accuracy between the
    rest after 100 epochs was in the 4th digit after the decimal (assuming the accuracy
    is a number between 0.0 and 1.0)'
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 损失函数：我们尝试了4种不同的损失函数：交叉熵、焦点损失、Dice损失、加权交叉熵。Dice损失表现最差，而其他损失函数则相差无几。实际上，经过100个周期后，其他损失函数的最佳准确性差异在小数点后第四位（假设准确性在0.0到1.0之间）。
- en: 'Convolution type: Regular'
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 卷积类型：常规
- en: 'Model type: UNet — current experiments used a SegNet model'
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 模型类型：UNet——当前实验使用了SegNet模型
- en: We achieved a best validation accuracy of 91.3% for the setting above. We noticed
    that the image size significantly impacts the best validation accuracy. For example,
    when we changed the image size to 256 x 256, the best validation accuracy went
    up to 93.0%. However, training took much longer, and used more memory, which meant
    that we had to reduce the batch size.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在上述设置下取得了91.3%的最佳验证准确性。我们注意到图像大小对最佳验证准确性有显著影响。例如，当我们将图像大小更改为256 x 256时，最佳验证准确性上升到93.0%。然而，训练时间大大增加，并且使用了更多内存，这意味着我们不得不减少批量大小。
- en: '![](../Images/adf9fa994f9857ab5aaa8a0ce9b761c0.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/adf9fa994f9857ab5aaa8a0ce9b761c0.png)'
- en: 'Figure 8: Result of training a UNet model for 100 train epochs with the hyperparameters
    mentioned above. Source: Author(s).'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8：使用上述超参数训练 100 个训练周期的 UNet 模型的结果。来源：作者。
- en: You can see that the predictions are much smoother and crisper compared to the
    ones we have been seeing so far.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 你会发现，预测结果相比之前的要更平滑、更清晰。
- en: Conclusion
  id: totrans-93
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: In part-3 of this series, we learned about depth wise separable convolutions
    (DSC) as a technique to reduce model size and training/inference cost without
    a significant loss in validation accuracy. We learned about the size/cost tradeoff
    to expect between regular and DSC for a specific setting.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在本系列第 3 部分，我们了解了深度可分离卷积（DSC）作为一种在不显著降低验证准确性的情况下减少模型大小和训练/推理成本的技术。我们了解了在特定设置下，常规卷积与
    DSC 之间的大小/成本权衡。
- en: We showed how to adapt the SegNet model to use DSC in PyTorch. This technique
    can be applied to any deep CNN. In fact we can selectively replace some of the
    convolutional layers with DSC — i.e. we don’t need to necessarily replace all
    of them. Choosing which layers to replace will depend on the balance you wish
    to strike between model size/runtime-cost and prediction accuracy. This decision
    will depend on your specific use case and deployment setup.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 我们展示了如何在 PyTorch 中调整 SegNet 模型以使用 DSC。这项技术可以应用于任何深度 CNN。事实上，我们可以选择性地用 DSC 替换一些卷积层
    —— 即我们不需要全部替换。选择替换哪些层将取决于你希望在模型大小/运行成本和预测准确性之间达成的平衡。这个决定将取决于你的具体使用案例和部署设置。
- en: While this article trained models for 20 epochs, we explained that this is insufficient
    for production workloads, and provided a glimpse into what one can expect if one
    trains the model for more epochs. In addition, we provided an introduction to
    some of the hyperparameters that one can tune during model training. While this
    list is by no means comprehensive, it should allow you to appreciate the complexity
    and decision making needed to train an image segmentation model for production
    workloads.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然本文训练了模型 20 个周期，但我们解释了这对于生产工作负载来说是不足够的，并且提供了如果训练更多周期会得到什么的初步了解。此外，我们介绍了一些在模型训练过程中可以调整的超参数。虽然这个列表并不全面，但它应能让你理解训练一个用于生产工作的图像分割模型所需的复杂性和决策过程。
- en: In the [next part of this series](https://medium.com/p/6c86da083432/), we’ll
    take a look at Vision Transformers, and how we can use this model architecture
    to perform image segmentation for the pets segmentation task.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在[本系列的下一部分](https://medium.com/p/6c86da083432/)，我们将探讨 Vision Transformers，以及如何利用这种模型架构来执行宠物分割任务的图像分割。
- en: References and further reading
  id: totrans-98
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献和进一步阅读
- en: '[Efficient Deep Learning Book Chapter 04 — Efficient Architectures](https://github.com/EfficientDL/book/raw/main/book/%5BEDL%5D%20Chapter%204%20-%20Efficient%20Architectures.pdf)'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[高效深度学习书籍第04章 — 高效架构](https://github.com/EfficientDL/book/raw/main/book/%5BEDL%5D%20Chapter%204%20-%20Efficient%20Architectures.pdf)'
- en: '[A Basic Introduction to Separable Convolutions](/a-basic-introduction-to-separable-convolutions-b99ec3102728)'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[可分离卷积基础介绍](/a-basic-introduction-to-separable-convolutions-b99ec3102728)'
