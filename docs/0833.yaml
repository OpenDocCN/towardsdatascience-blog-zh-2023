- en: 'Kernels: Everything You Need to Know'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å†…æ ¸ï¼šä½ éœ€è¦äº†è§£çš„ä¸€åˆ‡
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/kernels-everything-you-need-to-know-f5d255d95785?source=collection_archive---------0-----------------------#2023-03-06](https://towardsdatascience.com/kernels-everything-you-need-to-know-f5d255d95785?source=collection_archive---------0-----------------------#2023-03-06)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://towardsdatascience.com/kernels-everything-you-need-to-know-f5d255d95785?source=collection_archive---------0-----------------------#2023-03-06](https://towardsdatascience.com/kernels-everything-you-need-to-know-f5d255d95785?source=collection_archive---------0-----------------------#2023-03-06)
- en: ğŸ‘¨â€ğŸ« [Mathematics](https://equipintelligence.medium.com/list/mathematics-demystified-7e4d1c18041f)
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ğŸ‘¨â€ğŸ« [æ•°å­¦](https://equipintelligence.medium.com/list/mathematics-demystified-7e4d1c18041f)
- en: Density Estimation, Dot Products, Convolutions and everythingâ€¦
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å¯†åº¦ä¼°è®¡ã€ç‚¹ç§¯ã€å·ç§¯åŠå…¶ä»–ä¸€åˆ‡â€¦â€¦
- en: '[](https://equipintelligence.medium.com/?source=post_page-----f5d255d95785--------------------------------)[![Shubham
    Panchal](../Images/d48aecd8b1ed27ab68fc2e7ff6716606.png)](https://equipintelligence.medium.com/?source=post_page-----f5d255d95785--------------------------------)[](https://towardsdatascience.com/?source=post_page-----f5d255d95785--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----f5d255d95785--------------------------------)
    [Shubham Panchal](https://equipintelligence.medium.com/?source=post_page-----f5d255d95785--------------------------------)'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://equipintelligence.medium.com/?source=post_page-----f5d255d95785--------------------------------)[![Shubham
    Panchal](../Images/d48aecd8b1ed27ab68fc2e7ff6716606.png)](https://equipintelligence.medium.com/?source=post_page-----f5d255d95785--------------------------------)[](https://towardsdatascience.com/?source=post_page-----f5d255d95785--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----f5d255d95785--------------------------------)
    [Shubham Panchal](https://equipintelligence.medium.com/?source=post_page-----f5d255d95785--------------------------------)'
- en: Â·
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: Â·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fd45a9465f044&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fkernels-everything-you-need-to-know-f5d255d95785&user=Shubham+Panchal&userId=d45a9465f044&source=post_page-d45a9465f044----f5d255d95785---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----f5d255d95785--------------------------------)
    Â·14 min readÂ·Mar 6, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Ff5d255d95785&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fkernels-everything-you-need-to-know-f5d255d95785&user=Shubham+Panchal&userId=d45a9465f044&source=-----f5d255d95785---------------------clap_footer-----------)'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '[å…³æ³¨](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fd45a9465f044&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fkernels-everything-you-need-to-know-f5d255d95785&user=Shubham+Panchal&userId=d45a9465f044&source=post_page-d45a9465f044----f5d255d95785---------------------post_header-----------)
    å‘è¡¨åœ¨ [Towards Data Science](https://towardsdatascience.com/?source=post_page-----f5d255d95785--------------------------------)
    Â·14 min é˜…è¯»Â·2023å¹´3æœˆ6æ—¥[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Ff5d255d95785&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fkernels-everything-you-need-to-know-f5d255d95785&user=Shubham+Panchal&userId=d45a9465f044&source=-----f5d255d95785---------------------clap_footer-----------)'
- en: --
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff5d255d95785&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fkernels-everything-you-need-to-know-f5d255d95785&source=-----f5d255d95785---------------------bookmark_footer-----------)![](../Images/ab190b0872063f05abcb2425835c5803.png)'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff5d255d95785&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fkernels-everything-you-need-to-know-f5d255d95785&source=-----f5d255d95785---------------------bookmark_footer-----------)![](../Images/ab190b0872063f05abcb2425835c5803.png)'
- en: 'Source: Image by author'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥æºï¼šä½œè€…æä¾›çš„å›¾ç‰‡
- en: Kernels or kernel functions are beautiful mathematical concepts that are used
    in machine learning and statistics with different forms. If youâ€™re a beginner
    you might feel tempted to know the exact definition of kernels, but you may get
    confused by multiple definitions of kernels that are explained across blogs/websites
    on the internet.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: å†…æ ¸æˆ–æ ¸å‡½æ•°æ˜¯ç¾ä¸½çš„æ•°å­¦æ¦‚å¿µï¼Œç”¨äºæœºå™¨å­¦ä¹ å’Œç»Ÿè®¡å­¦ä¸­ï¼Œå½¢å¼å„å¼‚ã€‚å¦‚æœä½ æ˜¯åˆå­¦è€…ï¼Œä½ å¯èƒ½ä¼šæƒ³çŸ¥é“å†…æ ¸çš„ç¡®åˆ‡å®šä¹‰ï¼Œä½†ä½ å¯èƒ½ä¼šè¢«äº’è”ç½‘ä¸Šå„ç§åšå®¢/ç½‘ç«™ä¸Šè§£é‡Šçš„å¤šç§å®šä¹‰å¼„æ··ã€‚
- en: The kernel function is a confusing concept, just because the knowledge around
    it is decentralized across its applications and a common intuition connecting
    them is missing. This (huge) blog undertakes the motive of unifying all knowledge
    on kernels used in different ML applications. Just as most beginners, the kernel
    function kept me in a state of confusion for a long time, until I developed an
    intuition that would connect all links.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: æ ¸å‡½æ•°æ˜¯ä¸€ä¸ªä»¤äººå›°æƒ‘çš„æ¦‚å¿µï¼Œå› ä¸ºå›´ç»•å®ƒçš„çŸ¥è¯†åœ¨ä¸åŒçš„åº”ç”¨ä¸­æ˜¯åˆ†æ•£çš„ï¼Œç¼ºä¹ä¸€ä¸ªå°†å®ƒä»¬è¿æ¥èµ·æ¥çš„å¸¸è§ç›´è§‰ã€‚è¿™ä¸ªï¼ˆåºå¤§çš„ï¼‰åšå®¢æ—¨åœ¨ç»Ÿä¸€ä¸åŒMLåº”ç”¨ä¸­ä½¿ç”¨çš„æ ¸å‡½æ•°çš„æ‰€æœ‰çŸ¥è¯†ã€‚å°±åƒå¤§å¤šæ•°åˆå­¦è€…ä¸€æ ·ï¼Œæ ¸å‡½æ•°è®©æˆ‘å›°æƒ‘äº†å¾ˆé•¿æ—¶é—´ï¼Œç›´åˆ°æˆ‘åŸ¹å…»å‡ºä¸€ç§å°†æ‰€æœ‰ç¯èŠ‚è¿æ¥èµ·æ¥çš„ç›´è§‰ã€‚
- en: 'We begin our journey with non-parametric models, then we start discussing different
    types of kernels and their typical applications across statistics and ML. Similar
    to kernel functions, I have an attempt to explain PCA mathematically, considering
    all perspectives. You may have a read:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ä»éå‚æ•°æ¨¡å‹å¼€å§‹æˆ‘ä»¬çš„æ—…ç¨‹ï¼Œç„¶åå¼€å§‹è®¨è®ºä¸åŒç±»å‹çš„æ ¸å‡½æ•°åŠå…¶åœ¨ç»Ÿè®¡å’Œæœºå™¨å­¦ä¹ ä¸­çš„å…¸å‹åº”ç”¨ã€‚ç±»ä¼¼äºæ ¸å‡½æ•°ï¼Œæˆ‘ä¹Ÿå°è¯•ä»æ•°å­¦è§’åº¦è§£é‡ŠPCAï¼Œè€ƒè™‘æ‰€æœ‰çš„è§†è§’ã€‚ä½ å¯ä»¥é˜…è¯»ä¸€ä¸‹ï¼š
- en: '[](/principal-component-analysis-everything-you-need-to-know-5f834c9eaa83?source=post_page-----f5d255d95785--------------------------------)
    [## Principal Component Analysis: Everything You Need To Know'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/principal-component-analysis-everything-you-need-to-know-5f834c9eaa83?source=post_page-----f5d255d95785--------------------------------)
    [## ä¸»æˆåˆ†åˆ†æï¼šä½ éœ€è¦çŸ¥é“çš„ä¸€åˆ‡'
- en: Covariance, eigenvalues, variance and everything â€¦
  id: totrans-14
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: åæ–¹å·®ã€ç‰¹å¾å€¼ã€æ–¹å·®ä»¥åŠä¸€åˆ‡â€¦â€¦
- en: towardsdatascience.com](/principal-component-analysis-everything-you-need-to-know-5f834c9eaa83?source=post_page-----f5d255d95785--------------------------------)
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/principal-component-analysis-everything-you-need-to-know-5f834c9eaa83?source=post_page-----f5d255d95785--------------------------------)
- en: Non-Parametric Models
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: éå‚æ•°æ¨¡å‹
- en: Non-parametric models are those statistical models that do not have parameters
    which grow with the size of the input. Note, non-parametric models do not mean
    â€˜*model with zero parameters*â€™ but they work with a fixed set of parameters, also
    called hyperparameters, which do not increase with the increase in the dimensionality
    of the inputs. A vanilla linear regression model, has parameters ***Î¸*** that
    determine the *slope* of the hyperplane whose size depends on the dimensionality
    of the input ***x***,
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: éå‚æ•°æ¨¡å‹æ˜¯é‚£äº›ç»Ÿè®¡æ¨¡å‹ï¼Œå®ƒä»¬çš„å‚æ•°ä¸ä¼šéšç€è¾“å…¥è§„æ¨¡çš„å¢é•¿è€Œå¢åŠ ã€‚éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œéå‚æ•°æ¨¡å‹å¹¶ä¸æ˜¯æŒ‡â€˜*é›¶å‚æ•°æ¨¡å‹*â€™ï¼Œè€Œæ˜¯æŒ‡å®ƒä»¬ä½¿ç”¨çš„æ˜¯ä¸€ç»„å›ºå®šçš„å‚æ•°ï¼Œä¹Ÿç§°ä¸ºè¶…å‚æ•°ï¼Œè¿™äº›å‚æ•°ä¸ä¼šéšç€è¾“å…¥ç»´åº¦çš„å¢åŠ è€Œå¢åŠ ã€‚ä¸€ä¸ªç®€å•çš„çº¿æ€§å›å½’æ¨¡å‹æœ‰å‚æ•°***Î¸***ï¼Œè¿™äº›å‚æ•°å†³å®šäº†è¶…å¹³é¢çš„*æ–œç‡*ï¼Œå…¶å¤§å°å–å†³äºè¾“å…¥***x***çš„ç»´åº¦ã€‚
- en: Eqn. 1\. A simple linear regression model with tunable parameters whose size
    depends on the number of features **N**. The function **f** represents the expected
    value of the dependent variables **y** given inputs **x** i.e. **E[y|x]**
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: æ–¹ç¨‹1\. ä¸€ä¸ªå…·æœ‰å¯è°ƒå‚æ•°çš„ç®€å•çº¿æ€§å›å½’æ¨¡å‹ï¼Œå…¶å‚æ•°å¤§å°å–å†³äºç‰¹å¾æ•°**N**ã€‚å‡½æ•°**f**è¡¨ç¤ºç»™å®šè¾“å…¥**x**çš„å› å˜é‡**y**çš„æœŸæœ›å€¼ï¼Œå³**E[y|x]**
- en: Next, consider the KNN model, where we determine the class of a test sample
    by analyzing the classes of its ***K*** nearest neighbors. If ***K = 1***, we
    assume that the test sample belongs to the same class as that of the nearest neighbor.
    This model does not have any parameters that would grow with the dimensionality
    of the inputs. For a vanilla implementation, we would only need a single parameter,
    ***K***, even if we working with large inputs (in terms of dimensionality).
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥ä¸‹æ¥ï¼Œè€ƒè™‘KNNæ¨¡å‹ï¼Œæˆ‘ä»¬é€šè¿‡åˆ†ææµ‹è¯•æ ·æœ¬çš„***K***ä¸ªæœ€è¿‘é‚»çš„ç±»åˆ«æ¥ç¡®å®šæµ‹è¯•æ ·æœ¬çš„ç±»åˆ«ã€‚å¦‚æœ***K = 1***ï¼Œæˆ‘ä»¬å‡è®¾æµ‹è¯•æ ·æœ¬ä¸æœ€è¿‘é‚»å±äºåŒä¸€ç±»åˆ«ã€‚è¿™ä¸ªæ¨¡å‹æ²¡æœ‰ä»»ä½•éšç€è¾“å…¥ç»´åº¦å¢åŠ è€Œå¢é•¿çš„å‚æ•°ã€‚å¯¹äºä¸€ä¸ªç®€å•çš„å®ç°ï¼Œå³ä½¿åœ¨å¤„ç†å¤§è§„æ¨¡è¾“å…¥ï¼ˆä»ç»´åº¦è§’åº¦çœ‹ï¼‰æ—¶ï¼Œæˆ‘ä»¬ä¹Ÿåªéœ€è¦ä¸€ä¸ªå•ä¸€çš„å‚æ•°***K***ã€‚
- en: '![](../Images/b91bc02863d3d8166c0029592927d18a.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b91bc02863d3d8166c0029592927d18a.png)'
- en: '1\. A visual depiction of the working of K Nearest Neighbors model. The assumption
    made here is that the test sample belongs to same class as that of the K nearest
    neighbors. Image Source: Image by Author'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 1\. Kæœ€è¿‘é‚»æ¨¡å‹å·¥ä½œåŸç†çš„å¯è§†åŒ–æè¿°ã€‚è¿™é‡Œçš„å‡è®¾æ˜¯æµ‹è¯•æ ·æœ¬å±äºä¸Kä¸ªæœ€è¿‘é‚»ç›¸åŒçš„ç±»åˆ«ã€‚å›¾ç‰‡æ¥æºï¼šä½œè€…æä¾›çš„å›¾ç‰‡
- en: KNN is a non-parametric model which has a hyperparameter ***K***, provided by
    the user. Non-parametric might seem to an *obvious-choice* at the first glance,
    as,
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: KNNæ˜¯ä¸€ä¸ªéå‚æ•°æ¨¡å‹ï¼Œå®ƒæœ‰ä¸€ä¸ªç”±ç”¨æˆ·æä¾›çš„è¶…å‚æ•°***K***ã€‚éå‚æ•°æ¨¡å‹ä¹ä¸€çœ‹å¯èƒ½æ˜¯ä¸€ä¸ª*æ˜æ˜¾çš„é€‰æ‹©*ï¼Œå› ä¸ºï¼Œ
- en: They make no prior assumptions regarding the distribution of the data. For instance,
    in case of *vanilla* linear regression, which is a parametric model, we assume
    that the conditional distribution of ***Y*** (dependent variables) given ***X***
    (features) is a Gaussian Distribution whose mean is a linear combination of the
    features (where weights are ***Î¸)*** and variance equal to ***ÏƒÂ²***
  id: totrans-23
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: å®ƒä»¬ä¸å¯¹æ•°æ®çš„åˆ†å¸ƒåšä»»ä½•å…ˆéªŒå‡è®¾ã€‚ä¾‹å¦‚ï¼Œåœ¨*æ™®é€š*çº¿æ€§å›å½’ä¸­ï¼Œå®ƒæ˜¯ä¸€ä¸ªå‚æ•°æ¨¡å‹ï¼Œæˆ‘ä»¬å‡è®¾***Y***ï¼ˆå› å˜é‡ï¼‰åœ¨ç»™å®š***X***ï¼ˆç‰¹å¾ï¼‰çš„æ¡ä»¶ä¸‹æœä»é«˜æ–¯åˆ†å¸ƒï¼Œå…¶å‡å€¼æ˜¯ç‰¹å¾çš„çº¿æ€§ç»„åˆï¼ˆå…¶ä¸­æƒé‡æ˜¯***Î¸***ï¼‰å’Œæ–¹å·®ç­‰äº***ÏƒÂ²***ã€‚
- en: 2\. The conditional distribution of **Y** given **X** and parameters ***Î¸***
    *is a normal distribution from which we wish to capture the mean from* ***X****.
    The constant variance is a result of the* [*assumption of* homoscedasticity](https://en.wikipedia.org/wiki/Homoscedasticity_and_heteroscedasticity)
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 2. æ¡ä»¶åˆ†å¸ƒ**Y**ç»™å®š**X**å’Œå‚æ•°***Î¸*** *æ˜¯ä¸€ä¸ªæ­£æ€åˆ†å¸ƒï¼Œæˆ‘ä»¬å¸Œæœ›ä»***X***ä¸­æ•æ‰å‡å€¼*ã€‚å¸¸é‡æ–¹å·®æ˜¯* [*åŒæ–¹å·®å‡è®¾*](https://en.wikipedia.org/wiki/Homoscedasticity_and_heteroscedasticity)
    çš„ç»“æœã€‚
- en: which might not hold always, because,
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å¯èƒ½å¹¶ä¸æ€»æ˜¯æˆç«‹ï¼Œå› ä¸ºï¼Œ
- en: For each test sample, they need to keep the entire training data in memory,
    which is also true for the KNN model. For each sample, we need to calculate its
    distance from each training sample, so we need to retrieve/store each sample in
    memory, which might not be feasible to large datasets or even smaller datasets
    which a large number of features.
  id: totrans-26
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: å¯¹äºæ¯ä¸ªæµ‹è¯•æ ·æœ¬ï¼Œä»–ä»¬éœ€è¦å°†æ•´ä¸ªè®­ç»ƒæ•°æ®ä¿å­˜åœ¨å†…å­˜ä¸­ï¼Œè¿™å¯¹KNNæ¨¡å‹ä¹Ÿæ˜¯é€‚ç”¨çš„ã€‚å¯¹äºæ¯ä¸ªæ ·æœ¬ï¼Œæˆ‘ä»¬éœ€è¦è®¡ç®—å®ƒä¸æ¯ä¸ªè®­ç»ƒæ ·æœ¬çš„è·ç¦»ï¼Œå› æ­¤æˆ‘ä»¬éœ€è¦æ£€ç´¢/å­˜å‚¨æ¯ä¸ªæ ·æœ¬åœ¨å†…å­˜ä¸­ï¼Œè¿™å¯¹äºå¤§å‹æ•°æ®é›†ç”šè‡³ç‰¹å¾æ•°é‡ä¼—å¤šçš„å°å‹æ•°æ®é›†å¯èƒ½ä¸å¯è¡Œã€‚
- en: The basic idea around non-parametric models is to gather some useful insight
    from the data and use to solve the given problem, without encoding information
    about the data in tunable parameters.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: éå‚æ•°æ¨¡å‹çš„åŸºæœ¬æ€æƒ³æ˜¯ä»æ•°æ®ä¸­è·å–ä¸€äº›æœ‰ç”¨çš„è§è§£ï¼Œå¹¶ç”¨æ¥è§£å†³ç»™å®šçš„é—®é¢˜ï¼Œè€Œä¸é€šè¿‡å¯è°ƒå‚æ•°å¯¹æ•°æ®è¿›è¡Œç¼–ç ã€‚
- en: Next, we focus on kernels, that have different use-cases across ML, and differ
    slightly with their meanings in each context. So far, after researching for this
    blog, and also from my previous attempts of understanding kernels as a whole,
    I feel that kernels are *machines* that provide information on the neighbors of
    a given datapoint (as an *input* to the machine). This *local* information i.e.
    the information on datapoints that lie in the proximity of the datapoint under
    consideration, is then used to solve the given problem. Once we use the kernel
    function on each of the datapoints, we can get a clear picture of *whatâ€™s going
    on in the locality of the data*.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬ä¸“æ³¨äºæ ¸å‡½æ•°ï¼Œå®ƒä»¬åœ¨æœºå™¨å­¦ä¹ ä¸­æœ‰ä¸åŒçš„ä½¿ç”¨åœºæ™¯ï¼Œå¹¶ä¸”åœ¨æ¯ä¸ªä¸Šä¸‹æ–‡ä¸­ç¨æœ‰ä¸åŒçš„å«ä¹‰ã€‚åˆ°ç›®å‰ä¸ºæ­¢ï¼Œé€šè¿‡å¯¹è¿™ä¸ªåšå®¢çš„ç ”ç©¶ï¼Œä»¥åŠæˆ‘ä¹‹å‰å¯¹æ ¸å‡½æ•°æ•´ä½“ç†è§£çš„å°è¯•ï¼Œæˆ‘è§‰å¾—æ ¸å‡½æ•°æ˜¯æä¾›ç»™å®šæ•°æ®ç‚¹é‚»å±…ä¿¡æ¯çš„*æœºå™¨*ï¼ˆä½œä¸º*è¾“å…¥*æä¾›ç»™æœºå™¨ï¼‰ã€‚è¿™äº›*å±€éƒ¨*ä¿¡æ¯ï¼Œå³åœ¨è€ƒè™‘çš„æ•°æ®ç‚¹å‘¨å›´çš„é‚»è¿‘æ•°æ®ç‚¹çš„ä¿¡æ¯ï¼Œç„¶åç”¨äºè§£å†³ç»™å®šçš„é—®é¢˜ã€‚ä¸€æ—¦æˆ‘ä»¬å¯¹æ¯ä¸ªæ•°æ®ç‚¹ä½¿ç”¨æ ¸å‡½æ•°ï¼Œæˆ‘ä»¬å°±èƒ½æ¸…æ¥šåœ°çœ‹åˆ°*æ•°æ®å±€éƒ¨çš„æƒ…å†µ*ã€‚
- en: Weâ€™ll explore these three aspects of a kernel, which are three different concepts
    with their major applications in ML,
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†æ¢è®¨æ ¸å‡½æ•°çš„è¿™ä¸‰ä¸ªæ–¹é¢ï¼Œå®ƒä»¬æ˜¯æœºå™¨å­¦ä¹ ä¸­å…·æœ‰ä¸»è¦åº”ç”¨çš„ä¸‰ä¸ªä¸åŒæ¦‚å¿µï¼Œ
- en: '[Density Kernels](#d69a)'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[å¯†åº¦æ ¸å‡½æ•°](#d69a)'
- en: '[Mercer Kernels](#daa0)'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Mercer æ ¸å‡½æ•°](#daa0)'
- en: '[Kernels in Image Processing](#ed44)'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[å›¾åƒå¤„ç†ä¸­çš„æ ¸å‡½æ•°](#ed44)'
- en: 'Density Kernels: Use of Kernels for Density Estimation'
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å¯†åº¦æ ¸å‡½æ•°ï¼šç”¨äºå¯†åº¦ä¼°è®¡çš„æ ¸å‡½æ•°
- en: Density kernels, kernel density estimation, kernel regression
  id: totrans-34
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: å¯†åº¦æ ¸å‡½æ•°ã€æ ¸å¯†åº¦ä¼°è®¡ã€æ ¸å›å½’
- en: We can use kernels to estimate the probability density for a given *test* sample,
    by modelling the underlying probability distribution with the *training* samples.
    The terms â€˜testâ€™ sample and â€˜trainingâ€™ samples simply refer to unobserved and
    observed samples respectively, considering ML lingo.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥ä½¿ç”¨æ ¸å‡½æ•°é€šè¿‡ç”¨*è®­ç»ƒ*æ ·æœ¬å»ºæ¨¡åŸºç¡€æ¦‚ç‡åˆ†å¸ƒæ¥ä¼°è®¡ç»™å®š*æµ‹è¯•*æ ·æœ¬çš„æ¦‚ç‡å¯†åº¦ã€‚â€˜æµ‹è¯•â€™æ ·æœ¬å’Œâ€˜è®­ç»ƒâ€™æ ·æœ¬åˆ†åˆ«æŒ‡æœªè§‚æµ‹å’Œå·²è§‚æµ‹çš„æ ·æœ¬ï¼Œè€ƒè™‘åˆ°æœºå™¨å­¦ä¹ çš„æœ¯è¯­ã€‚
- en: For continuous random variable ***X***, we can integrate the probability density
    function of ***X*** within a suitable range, say, from ***x_1*** to ***x_2***,
    and we get the probability of ***X*** assuming a value in the range ***[ x_1 ,
    x_2 ]***. If you arenâ€™t comfortable with the topic of probability density or random
    variables, hereâ€™s my 3-part series on probability distributions,
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºè¿ç»­éšæœºå˜é‡***X***ï¼Œæˆ‘ä»¬å¯ä»¥åœ¨é€‚å½“çš„èŒƒå›´å†…ç§¯åˆ†***X***çš„æ¦‚ç‡å¯†åº¦å‡½æ•°ï¼Œæ¯”å¦‚ä»***x_1***åˆ°***x_2***ï¼Œä»è€Œå¾—åˆ°***X***åœ¨èŒƒå›´***[
    x_1 , x_2 ]***å†…å–å€¼çš„æ¦‚ç‡ã€‚å¦‚æœä½ å¯¹æ¦‚ç‡å¯†åº¦æˆ–éšæœºå˜é‡çš„ä¸»é¢˜ä¸å¤ªç†Ÿæ‚‰ï¼Œè¿™é‡Œæ˜¯æˆ‘å…³äºæ¦‚ç‡åˆ†å¸ƒçš„ä¸‰éƒ¨åˆ†ç³»åˆ—ï¼Œ
- en: Density Kernels and Kernel Density Estimation (KDE)
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å¯†åº¦æ ¸å‡½æ•°å’Œæ ¸å¯†åº¦ä¼°è®¡ï¼ˆKDEï¼‰
- en: Let us start our discussion with a problem. Dr. Panchal lives in a crowded city
    block which is surrounded by houses on all sides. The local police have hired
    a detective whose job is to determine the number of persons who reside in the
    doctorâ€™s house or who are his family members, just to make sure things are going
    well. The detective could not ring the doctorâ€™s bell and ask how many family members
    reside there, as it would warn the doctor if thereâ€™s something suspicious.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬ä»ä¸€ä¸ªé—®é¢˜å¼€å§‹è®¨è®ºã€‚Panchalåšå£«ä½åœ¨ä¸€ä¸ªæ‹¥æŒ¤çš„åŸå¸‚è¡—åŒºï¼Œå››å‘¨éƒ½æ˜¯æˆ¿å­ã€‚å½“åœ°è­¦æ–¹é›‡ä½£äº†ä¸€åä¾¦æ¢ï¼Œå…¶å·¥ä½œæ˜¯ç¡®å®šåšå£«å®¶ä¸­å±…ä½çš„äººæ•°æˆ–å…¶å®¶å±äººæ•°ï¼Œä»¥ç¡®ä¿ä¸€åˆ‡é¡ºåˆ©ã€‚ä¾¦æ¢ä¸èƒ½æŒ‰åšå£«çš„é—¨é“ƒè¯¢é—®æœ‰å¤šå°‘å®¶åº­æˆå‘˜ï¼Œå› ä¸ºè¿™ä¼šè­¦å‘Šåšå£«å¦‚æœæœ‰ä»€ä¹ˆå¯ç–‘çš„æƒ…å†µã€‚
- en: The detective would start by interrogating the houses that are adjacent to Dr.
    Panchalâ€™s house, as they could have a clear sight of whatâ€™s in there. The detective
    is expected to give a higher weight/importance to the information gained from
    these direct neighbors. Next, in order to gain more insights, the detective interrogates
    houses that are slightly farther away that do not have a direct sight of Dr. Panchalâ€™s
    house but might have good information of their neighbor. The detective would give
    lesser importance to the information received from these neighbors, as their observations
    might not be that correct as that of the direct neighbors (whose houses are adjacent
    to Panchalâ€™s). The detective performs several such rounds reducing the importance,
    moving away from Dr. Panchalâ€™s house.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: ä¾¦æ¢ä¼šä»Panchalåšå£«æˆ¿å­çš„ç›¸é‚»æˆ¿å­å¼€å§‹è¯¢é—®ï¼Œå› ä¸ºè¿™äº›æˆ¿å­å¯ä»¥æ¸…æ¥šåœ°çœ‹åˆ°é‡Œé¢çš„æƒ…å†µã€‚é¢„è®¡ä¾¦æ¢ä¼šç»™äºˆè¿™äº›ç›´æ¥é‚»å±…çš„ä¿¡æ¯æ›´é«˜çš„æƒé‡/é‡è¦æ€§ã€‚æ¥ä¸‹æ¥ï¼Œä¸ºäº†è·å¾—æ›´å¤šè§è§£ï¼Œä¾¦æ¢ä¼šè¯¢é—®é‚£äº›ç¨å¾®è¿œç¦»ä½†å¯èƒ½å¯¹é‚»å±…æœ‰è‰¯å¥½ä¿¡æ¯çš„æˆ¿å­ã€‚å¯¹äºè¿™äº›é‚»å±…æä¾›çš„ä¿¡æ¯ï¼Œä¾¦æ¢ä¼šç»™äºˆè¾ƒä½çš„é‡è¦æ€§ï¼Œå› ä¸ºè¿™äº›é‚»å±…çš„è§‚å¯Ÿå¯èƒ½ä¸å¦‚ç›´æ¥é‚»å±…ï¼ˆå³é‚»è¿‘Panchalåšå£«çš„æˆ¿å­ï¼‰çš„è§‚å¯Ÿå‡†ç¡®ã€‚ä¾¦æ¢ä¼šè¿›è¡Œå‡ è½®è¿™æ ·çš„è¯¢é—®ï¼Œé€æ¸å‡å°‘é‡è¦æ€§ï¼Œè¿œç¦»Panchalåšå£«çš„æˆ¿å­ã€‚
- en: A density kernel does something similar to capture *neighboring* information
    around a given point. If weâ€™re given a dataset ***D*** with ***N*** samples where
    each sample is a real number,
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: å¯†åº¦æ ¸å‡½æ•°ç±»ä¼¼äºæ•æ‰*é‚»è¿‘*ç‚¹çš„ä¿¡æ¯ã€‚å¦‚æœæˆ‘ä»¬æœ‰ä¸€ä¸ªæ•°æ®é›†***D***ï¼Œå…¶ä¸­æœ‰***N***ä¸ªæ ·æœ¬ï¼Œæ¯ä¸ªæ ·æœ¬æ˜¯ä¸€ä¸ªå®æ•°ï¼Œ
- en: The kernel in the above snippet is an Epanechnikov (parabolic) kernel. The kernel
    function has some special properties here,
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸Šè¿°ä»£ç ç‰‡æ®µä¸­çš„æ ¸å‡½æ•°æ˜¯ä¸€ä¸ªEpanechnikovï¼ˆæŠ›ç‰©çº¿å‹ï¼‰æ ¸å‡½æ•°ã€‚è¯¥æ ¸å‡½æ•°åœ¨è¿™é‡Œå…·æœ‰ä¸€äº›ç‰¹æ®Šæ€§è´¨ï¼Œ
- en: '*Property 1*: The kernel function or the detective does not matter in which
    direction the x or some neighborâ€™s house lies. Information gained from two houses
    to the right or two houses to the left is the same.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*æ€§è´¨ 1*ï¼šæ ¸å‡½æ•°æˆ–ä¾¦æ¢ä¸åœ¨æ„xæˆ–æŸäº›é‚»å±…çš„æˆ¿å­ä½äºå“ªä¸ªæ–¹å‘ã€‚æ¥è‡ªå³è¾¹ä¸¤æ ‹æˆ¿å­æˆ–å·¦è¾¹ä¸¤æ ‹æˆ¿å­çš„è·å¾—çš„ä¿¡æ¯æ˜¯ç›¸åŒçš„ã€‚'
- en: '*Property 2*: The kernel function represents a valid PDF and it integrates
    to 1 over the entire real domain.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*æ€§è´¨ 2*ï¼šæ ¸å‡½æ•°è¡¨ç¤ºä¸€ä¸ªæœ‰æ•ˆçš„æ¦‚ç‡å¯†åº¦å‡½æ•°ï¼ˆPDFï¼‰ï¼Œå¹¶ä¸”åœ¨æ•´ä¸ªå®æ•°åŸŸä¸Šç§¯åˆ†ä¸º1ã€‚'
- en: '*Property 3*: Support of a kernel is the set of all values ***u*** such that
    ***K( u )*** is not equal to 0\. It represents the coverage area of the detective
    from where some non-zero importance will be given to information gathered. If
    the detective decides to interrogate in all houses in a radius of 5 km, the support
    would be all houses within that 5 km circle.'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*æ€§è´¨ 3*ï¼šæ ¸çš„æ”¯æŒæ˜¯æ‰€æœ‰å€¼***u***çš„é›†åˆï¼Œä½¿å¾—***K(u)***ä¸ç­‰äº0ã€‚å®ƒè¡¨ç¤ºä¾¦æ¢åœ¨ä»å“ªé‡Œæ”¶é›†ä¿¡æ¯æ—¶ä¼šç»™äºˆä¸€äº›éé›¶çš„é‡è¦æ€§ã€‚å¦‚æœä¾¦æ¢å†³å®šåœ¨åŠå¾„5å…¬é‡Œå†…è¯¢é—®æ‰€æœ‰æˆ¿å±‹ï¼Œé‚£ä¹ˆæ”¯æŒå°±æ˜¯è¿™ä¸ª5å…¬é‡Œåœ†åœˆå†…çš„æ‰€æœ‰æˆ¿å±‹ã€‚'
- en: Every type of kernel will perform a similar task of encoding neighboring information,
    which each one will have a different strategy to do so. Unlike our detective,
    who slowly reduced the importance of interrogation as he moved away from Dr. Panchalâ€™s
    house (a Gaussian kernel would do that), another detective might just continue
    giving equal importance to all interrogations, neglecting the distance upto a
    certain extent (a uniform kernel). Imagine, that from our dataset ***D***, the
    distribution of all ***x_i*** is,
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: æ¯ç§ç±»å‹çš„æ ¸å‡½æ•°éƒ½ä¼šæ‰§è¡Œç±»ä¼¼çš„ä»»åŠ¡æ¥ç¼–ç é‚»è¿‘ä¿¡æ¯ï¼Œæ¯ç§æ ¸å‡½æ•°ä¼šæœ‰ä¸åŒçš„ç­–ç•¥æ¥å®ç°è¿™ä¸€ç‚¹ã€‚ä¸æˆ‘ä»¬çš„ä¾¦æ¢ä¸åŒï¼Œä»–åœ¨ç¦»å¼€Panchalåšå£«çš„æˆ¿å­æ—¶é€æ¸å‡å°‘äº†å¯¹è¯¢é—®çš„é‡è§†ï¼ˆé«˜æ–¯æ ¸ä¼šè¿™æ ·åšï¼‰ï¼Œå¦ä¸€ä¸ªä¾¦æ¢å¯èƒ½ä¼šç»§ç»­å¯¹æ‰€æœ‰è¯¢é—®èµ‹äºˆç›¸åŒçš„é‡è¦æ€§ï¼Œå¿½ç•¥åˆ°ä¸€å®šç¨‹åº¦çš„è·ç¦»ï¼ˆå‡åŒ€æ ¸ï¼‰ã€‚æƒ³è±¡ä¸€ä¸‹ï¼Œä»æˆ‘ä»¬çš„æ•°æ®é›†***D***æ¥çœ‹ï¼Œæ‰€æœ‰***x_i***çš„åˆ†å¸ƒæ˜¯ï¼Œ
- en: '![](../Images/2d959ddb054dce433217e71275ced376.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2d959ddb054dce433217e71275ced376.png)'
- en: A Gaussian-like distribution whose probability density needs to be estimated
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: éœ€è¦ä¼°è®¡å…¶æ¦‚ç‡å¯†åº¦çš„ç±»ä¼¼é«˜æ–¯åˆ†å¸ƒ
- en: Our goal is to create an estimate of the probability distribution of ***X***.
    Weâ€™ll do this by estimating the density at each sample ***x_i*** and using the
    kernel to collect neighboring information,
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬çš„ç›®æ ‡æ˜¯åˆ›å»ºå¯¹ ***X*** çš„æ¦‚ç‡åˆ†å¸ƒçš„ä¼°è®¡ã€‚æˆ‘ä»¬é€šè¿‡åœ¨æ¯ä¸ªæ ·æœ¬ ***x_i*** å¤„ä¼°è®¡å¯†åº¦å¹¶ä½¿ç”¨æ ¸æ¥æ”¶é›†é‚»è¿‘ä¿¡æ¯æ¥å®ç°è¿™ä¸€ç›®æ ‡ã€‚
- en: '![](../Images/6a39f0c5d3d25de92a9fdfc4198e017e.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6a39f0c5d3d25de92a9fdfc4198e017e.png)'
- en: Estimated probability density with KDE
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ KDE ä¼°è®¡æ¦‚ç‡å¯†åº¦
- en: If ***x_i*** is away from ***x***, ***| x â€” x_i |*** will have a larger value
    thus yielding a very small value for ***K( x â€” x_i )*** and reducing the *say*
    of ***x_i*** in the determination of probability density at ***x***. The parameter
    ***h*** is the *smoothing parameter* called the *bandwidth* of the kernel. Greater
    the value of ***h***, smoother will be predicted probability density.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœ ***x_i*** ç¦» ***x*** è¾ƒè¿œï¼Œåˆ™ ***| x â€” x_i |*** å°†æœ‰æ›´å¤§çš„å€¼ï¼Œä»è€Œä½¿ ***K( x â€” x_i )***
    çš„å€¼éå¸¸å°ï¼Œå¹¶å‡å°‘ ***x_i*** åœ¨ç¡®å®š ***x*** å¤„çš„æ¦‚ç‡å¯†åº¦ä¸­çš„ *ä½œç”¨*ã€‚å‚æ•° ***h*** æ˜¯ç§°ä¸º *å¸¦å®½* çš„ *å¹³æ»‘å‚æ•°*ã€‚***h***
    çš„å€¼è¶Šå¤§ï¼Œé¢„æµ‹çš„æ¦‚ç‡å¯†åº¦è¶Šå¹³æ»‘ã€‚
- en: '![](../Images/081bed27008b194d65d79d0ac17ba022.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/081bed27008b194d65d79d0ac17ba022.png)'
- en: Probability density estimations obtained by changing the bandwidth of the kernel.
    The bottommost figure depicts the data distribution and we obtain a smoother density
    estimation by increasing the bandwidth of the kernel. The kernel used for estimation
    here, was the **Epanechnikov/Parabolic** kernel.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: é€šè¿‡æ”¹å˜æ ¸çš„å¸¦å®½è·å¾—çš„æ¦‚ç‡å¯†åº¦ä¼°è®¡ã€‚æœ€åº•éƒ¨çš„å›¾æç»˜äº†æ•°æ®åˆ†å¸ƒï¼Œé€šè¿‡å¢åŠ æ ¸çš„å¸¦å®½æˆ‘ä»¬è·å¾—äº†æ›´å¹³æ»‘çš„å¯†åº¦ä¼°è®¡ã€‚è¿™é‡Œç”¨äºä¼°è®¡çš„æ ¸æ˜¯ **Epanechnikov/Parabolic**
    æ ¸ã€‚
- en: Kernel Regression
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ ¸å›å½’
- en: Kernel regression is a non-parametric version of the linear regression model
    where we model the conditional expectation of the outcome variable. In the case
    of simple linear regression, we model the conditional expectation ***E[ Y | X
    ]*** directly by expressing it as a linear combination of the independent variables.
    This gives rise to a discriminative model, whereas kernel regression is a generative
    model, as weâ€™re modelling the joint probability distribution ***P( X , Y )***
    and ***P( X )*** by the means of kernel density estimation.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: æ ¸å›å½’æ˜¯çº¿æ€§å›å½’æ¨¡å‹çš„éå‚æ•°ç‰ˆæœ¬ï¼Œæˆ‘ä»¬åœ¨å…¶ä¸­å¯¹ç»“æœå˜é‡çš„æ¡ä»¶æœŸæœ›è¿›è¡Œå»ºæ¨¡ã€‚åœ¨ç®€å•çº¿æ€§å›å½’çš„æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬é€šè¿‡å°†å…¶è¡¨ç¤ºä¸ºè‡ªå˜é‡çš„çº¿æ€§ç»„åˆæ¥ç›´æ¥å»ºæ¨¡æ¡ä»¶æœŸæœ›
    ***E[ Y | X ]***ã€‚è¿™äº§ç”Ÿäº†ä¸€ä¸ªåˆ¤åˆ«æ¨¡å‹ï¼Œè€Œæ ¸å›å½’æ˜¯ä¸€ä¸ªç”Ÿæˆæ¨¡å‹ï¼Œå› ä¸ºæˆ‘ä»¬é€šè¿‡æ ¸å¯†åº¦ä¼°è®¡å»ºæ¨¡è”åˆæ¦‚ç‡åˆ†å¸ƒ ***P( X , Y )*** å’Œ
    ***P( X )***ã€‚
- en: Deriving an estimator for **E[ Y | X ]** which does not have any additional
    parameters. In case of simple linear regression, **E[ Y | X ]** would be modelled
    as a weighed linear combination of the all **X_i**â€™s where the weights are the
    parameters.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: æ¨å¯¼ä¸€ä¸ªæ²¡æœ‰é¢å¤–å‚æ•°çš„ **E[ Y | X ]** ä¼°è®¡é‡ã€‚åœ¨ç®€å•çº¿æ€§å›å½’çš„æƒ…å†µä¸‹ï¼Œ**E[ Y | X ]** å°†è¢«å»ºæ¨¡ä¸ºæ‰€æœ‰ **X_i** çš„åŠ æƒçº¿æ€§ç»„åˆï¼Œå…¶ä¸­æƒé‡æ˜¯å‚æ•°ã€‚
- en: On observing the resultant expression, youâ€™ll realize that the predicted outcome
    ***y_hat*** is a weighed combination of all ***y_i***, where the weights are determined
    by the values of the kernel function for all ***x_i***.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: è§‚å¯Ÿç»“æœè¡¨è¾¾å¼æ—¶ï¼Œä½ ä¼šå‘ç°é¢„æµ‹ç»“æœ ***y_hat*** æ˜¯æ‰€æœ‰ ***y_i*** çš„åŠ æƒç»„åˆï¼Œå…¶ä¸­æƒé‡ç”±æ‰€æœ‰ ***x_i*** çš„æ ¸å‡½æ•°å€¼å†³å®šã€‚
- en: 'Mercer Kernels: Dot Products in Higher Dimensions'
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ¢…ç‘Ÿæ ¸ï¼šé«˜ç»´ç©ºé—´ä¸­çš„ç‚¹ç§¯
- en: Mercer kernels and positive definiteness, use of Mercer kernels in SVM
  id: totrans-59
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: æ¢…ç‘Ÿæ ¸å’Œæ­£å®šæ€§ï¼Œæ¢…ç‘Ÿæ ¸åœ¨ SVM ä¸­çš„åº”ç”¨
- en: Mercer Kernels or Positive definite kernels are functions that take in two inputs
    and produce a real number which characterizes the proximity of the two inputs
    (or their high-dimensional representations) in some other space. It turns out
    that these kernels are useful from the computational perspective as they help
    us compute dot products of vectors in a high-dimensional without explicitly performing
    any transformation to bring our own vectors into that high-dimensional space.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: æ¢…ç‘Ÿæ ¸æˆ–æ­£å®šæ ¸æ˜¯å°†ä¸¤ä¸ªè¾“å…¥ä½œä¸ºè¾“å…¥å¹¶äº§ç”Ÿä¸€ä¸ªå®æ•°çš„å‡½æ•°ï¼Œè¯¥å®æ•°è¡¨å¾äº†è¿™ä¸¤ä¸ªè¾“å…¥ï¼ˆæˆ–å®ƒä»¬çš„é«˜ç»´è¡¨ç¤ºï¼‰åœ¨æŸä¸ªå…¶ä»–ç©ºé—´ä¸­çš„æ¥è¿‘åº¦ã€‚äº‹å®è¯æ˜ï¼Œè¿™äº›æ ¸åœ¨è®¡ç®—è§’åº¦ä¸Šå¾ˆæœ‰ç”¨ï¼Œå› ä¸ºå®ƒä»¬å¸®åŠ©æˆ‘ä»¬åœ¨é«˜ç»´ç©ºé—´ä¸­è®¡ç®—å‘é‡çš„ç‚¹ç§¯ï¼Œè€Œæ— éœ€æ˜¾å¼åœ°è¿›è¡Œä»»ä½•å˜æ¢å°†æˆ‘ä»¬çš„å‘é‡å¸¦å…¥è¯¥é«˜ç»´ç©ºé—´ã€‚
- en: Mercer Kernels
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ¢…ç‘Ÿæ ¸
- en: Let us start our discussion by defining the kernel function and some of its
    properties,
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬é€šè¿‡å®šä¹‰æ ¸å‡½æ•°åŠå…¶ä¸€äº›å±æ€§æ¥å¼€å§‹è®¨è®ºï¼Œ
- en: '![](../Images/c5183c93b4a4783dfae49df86d3b007e.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c5183c93b4a4783dfae49df86d3b007e.png)'
- en: The Mercer kernel is a function that takes in two datapoints from our dataset
    ***D***, and outputs a real number that represents the proximity of these two
    datapoints in the feature space.
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Merceræ ¸æ˜¯ä¸€ä¸ªå‡½æ•°ï¼Œå®ƒæ¥å—æ¥è‡ªæ•°æ®é›†***D***çš„ä¸¤ä¸ªæ•°æ®ç‚¹ï¼Œå¹¶è¾“å‡ºä¸€ä¸ªå®æ•°ï¼Œè¡¨ç¤ºè¿™ä¸¤ä¸ªæ•°æ®ç‚¹åœ¨ç‰¹å¾ç©ºé—´ä¸­çš„æ¥è¿‘ç¨‹åº¦ã€‚
- en: If we have ***n*** datapoints in our dataset ***D***, and we run our Mercer
    kernel on each pair of datapoints and arrange the resulting outputs in a matrix,
    we achieve a positive definite matrix. This matrix, that depicts the similarity
    amongst the datapoints, is called the Gram Matrix.
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å¦‚æœæˆ‘ä»¬åœ¨æ•°æ®é›†***D***ä¸­æœ‰***n***ä¸ªæ•°æ®ç‚¹ï¼Œå¹¶å¯¹æ¯å¯¹æ•°æ®ç‚¹åº”ç”¨Merceræ ¸ï¼Œå¹¶å°†ç»“æœè¾“å‡ºæ•´ç†æˆä¸€ä¸ªçŸ©é˜µï¼Œæˆ‘ä»¬ä¼šå¾—åˆ°ä¸€ä¸ªæ­£å®šçŸ©é˜µã€‚è¿™ä¸ªçŸ©é˜µæç»˜äº†æ•°æ®ç‚¹ä¹‹é—´çš„ç›¸ä¼¼æ€§ï¼Œç§°ä¸ºGramçŸ©é˜µã€‚
- en: '![](../Images/aefa15277ec3acde2e1b3eb8fe7fffb3.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/aefa15277ec3acde2e1b3eb8fe7fffb3.png)'
- en: The Gram Matrix
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: GramçŸ©é˜µ
- en: Positive definite matrices are special, considering their spectral properties.
    They have positive eigenvalues and the corresponding eigenvectors form an orthonormal
    basis. We have a special property, for mercer kernels, using which the value of
    the kernel function as be expressed as the dot product of two transformed vectors,
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: æ­£å®šçŸ©é˜µå› å…¶è°±å±æ€§è€Œç‹¬ç‰¹ã€‚å®ƒä»¬å…·æœ‰æ­£ç‰¹å¾å€¼ï¼Œå¹¶ä¸”ç›¸åº”çš„ç‰¹å¾å‘é‡å½¢æˆä¸€ä¸ªæ­£äº¤è§„èŒƒåŸºã€‚å¯¹äºMerceræ ¸ï¼Œæˆ‘ä»¬æœ‰ä¸€ä¸ªç‰¹æ®Šçš„å±æ€§ï¼Œå¯ä»¥ä½¿ç”¨è¯¥å±æ€§å°†æ ¸å‡½æ•°çš„å€¼è¡¨ç¤ºä¸ºä¸¤ä¸ªè½¬æ¢å‘é‡çš„ç‚¹ç§¯ã€‚
- en: '![](../Images/8731da29e52f913ef2df9a496489a9bb.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8731da29e52f913ef2df9a496489a9bb.png)'
- en: Each entry of the Gram matrix can be described as the dot product of two transformed
    samples.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: GramçŸ©é˜µçš„æ¯ä¸ªæ¡ç›®å¯ä»¥æè¿°ä¸ºä¸¤ä¸ªè½¬æ¢æ ·æœ¬ä¹‹é—´çš„ç‚¹ç§¯ã€‚
- en: One might have an urge to feel the intuition behind this statement, but it lives
    in the sanctum of Hilbert Spaces which deserves a separate blog. For time being,
    its good to understand that the value of the kernel function, with two input vectors,
    as be described as the dot product of some other two vectors that lie in higher
    dimensional.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: å¯èƒ½ä¼šæœ‰ä¸€ç§ç›´è§‰ä¸Šçš„å†²åŠ¨å»ç†è§£è¿™ä¸ªé™ˆè¿°ï¼Œä½†å®ƒå­˜åœ¨äºHilbertç©ºé—´çš„ç¥åœ£é¢†åŸŸï¼Œéœ€è¦å¦å†™ä¸€ç¯‡åšå®¢ã€‚ç›®å‰ï¼Œäº†è§£æ ¸å‡½æ•°çš„å€¼å¯ä»¥ç”¨é«˜ç»´ç©ºé—´ä¸­ä¸¤ä¸ªå‘é‡çš„ç‚¹ç§¯æ¥æè¿°ï¼Œè¿™æ˜¯å¾ˆå¥½çš„ã€‚
- en: Mercer kernels provide a shortcut for computing the dot product between those
    two high-dimensional vectors without explicitly computing those vectors. Hence,
    we can leverage the advantages of high-dimensional spaces are sometimes useful
    in machine learning, especially when samples are not linearly separable.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: Merceræ ¸æä¾›äº†ä¸€ç§è®¡ç®—è¿™ä¸¤ä¸ªé«˜ç»´å‘é‡ä¹‹é—´ç‚¹ç§¯çš„å¿«æ·æ–¹å¼ï¼Œè€Œæ— éœ€æ˜¾å¼è®¡ç®—è¿™äº›å‘é‡ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¯ä»¥åˆ©ç”¨é«˜ç»´ç©ºé—´çš„ä¼˜åŠ¿ï¼Œè¿™åœ¨æœºå™¨å­¦ä¹ ä¸­æœ‰æ—¶æ˜¯æœ‰ç”¨çš„ï¼Œç‰¹åˆ«æ˜¯å½“æ ·æœ¬ä¸çº¿æ€§å¯åˆ†æ—¶ã€‚
- en: '![](../Images/1ddba6803e3cdca1250a355e0e379d94.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1ddba6803e3cdca1250a355e0e379d94.png)'
- en: 'Samples which are linearly inseparable in lower dimensions may find an optimal
    hyperplane in higher dimensions. Source: Image by author'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä½ç»´ç©ºé—´ä¸­çº¿æ€§ä¸å¯åˆ†çš„æ ·æœ¬å¯èƒ½ä¼šåœ¨é«˜ç»´ç©ºé—´ä¸­æ‰¾åˆ°ä¸€ä¸ªæœ€ä¼˜çš„è¶…å¹³é¢ã€‚æ¥æºï¼šä½œè€…å›¾ç‰‡
- en: For some optimization problems, like the one encountered while optimizing SVMs,
    weâ€™ll need to compute dot products between two transformed samples which are two
    high-dimensional vectors. The use of kernel functions will help us compute this
    dot product easily without performing any explicit transformations on the samples.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºä¸€äº›ä¼˜åŒ–é—®é¢˜ï¼Œå¦‚ä¼˜åŒ–SVMæ—¶é‡åˆ°çš„é—®é¢˜ï¼Œæˆ‘ä»¬éœ€è¦è®¡ç®—ä¸¤ä¸ªé«˜ç»´å‘é‡ä¹‹é—´çš„ç‚¹ç§¯ã€‚ä½¿ç”¨æ ¸å‡½æ•°å¯ä»¥å¸®åŠ©æˆ‘ä»¬è½»æ¾è®¡ç®—è¿™ä¸ªç‚¹ç§¯ï¼Œè€Œä¸éœ€è¦å¯¹æ ·æœ¬è¿›è¡Œä»»ä½•æ˜¾å¼çš„è½¬æ¢ã€‚
- en: Use of Mercer Kernels in SVMs
  id: totrans-76
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Merceræ ¸åœ¨SVMä¸­çš„åº”ç”¨
- en: SVMs are linear classifiers that fit a hyperplane such that a decision boundary
    is formed between samples of two classes. To determine the best hyperplane i.e.
    the hyperplane which divides the samples into two classes and maximizes the â€˜marginâ€™,
    we need to solve an optimization problem that contains an objective function (a
    function which is either maximized or minimized) given some constraints on the
    parameters of the objective.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: SVMæ˜¯çº¿æ€§åˆ†ç±»å™¨ï¼Œå®ƒé€šè¿‡æ‹Ÿåˆä¸€ä¸ªè¶…å¹³é¢ï¼Œä½¿å¾—åœ¨ä¸¤ä¸ªç±»åˆ«çš„æ ·æœ¬ä¹‹é—´å½¢æˆä¸€ä¸ªå†³ç­–è¾¹ç•Œã€‚ä¸ºäº†ç¡®å®šæœ€ä½³çš„è¶…å¹³é¢ï¼Œå³å°†æ ·æœ¬åˆ’åˆ†ä¸ºä¸¤ä¸ªç±»åˆ«å¹¶æœ€å¤§åŒ–â€œè¾¹é™…â€çš„è¶…å¹³é¢ï¼Œæˆ‘ä»¬éœ€è¦è§£å†³ä¸€ä¸ªåŒ…å«ç›®æ ‡å‡½æ•°ï¼ˆä¸€ä¸ªå¯ä»¥æœ€å¤§åŒ–æˆ–æœ€å°åŒ–çš„å‡½æ•°ï¼‰ä»¥åŠä¸€äº›çº¦æŸæ¡ä»¶çš„ä¼˜åŒ–é—®é¢˜ã€‚
- en: The derivation of the SVM optimization problem is covered in these blogs extensively
    by [Saptashwa Bhattacharyya](https://medium.com/u/9a3c3c477239?source=post_page-----f5d255d95785--------------------------------)
    so reading them once would help us proceed further,
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: SVMä¼˜åŒ–é—®é¢˜çš„æ¨å¯¼åœ¨è¿™äº›åšå®¢ä¸­å¾—åˆ°äº†å¹¿æ³›çš„è®¨è®ºï¼Œ[Saptashwa Bhattacharyya](https://medium.com/u/9a3c3c477239?source=post_page-----f5d255d95785--------------------------------)çš„åšå®¢å¯ä»¥å¸®åŠ©æˆ‘ä»¬è¿›ä¸€æ­¥äº†è§£ï¼Œ
- en: '[](/understanding-support-vector-machine-part-2-kernel-trick-mercers-theorem-e1e6848c6c4d?source=post_page-----f5d255d95785--------------------------------)
    [## Understanding Support Vector Machine: Part 2: Kernel Trick; Mercerâ€™s Theorem'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/understanding-support-vector-machine-part-2-kernel-trick-mercers-theorem-e1e6848c6c4d?source=post_page-----f5d255d95785--------------------------------)
    [## ç†è§£æ”¯æŒå‘é‡æœºï¼šç¬¬äºŒéƒ¨åˆ†ï¼šæ ¸æŠ€å·§ï¼›Mercerå®šç†'
- en: Why Kernel ?
  id: totrans-80
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ä¸ºä»€ä¹ˆä½¿ç”¨æ ¸å‡½æ•°ï¼Ÿ
- en: towardsdatascience.com](/understanding-support-vector-machine-part-2-kernel-trick-mercers-theorem-e1e6848c6c4d?source=post_page-----f5d255d95785--------------------------------)
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/understanding-support-vector-machine-part-2-kernel-trick-mercers-theorem-e1e6848c6c4d?source=post_page-----f5d255d95785--------------------------------)
- en: The vectors w and b characterize the hyperplane which forms the decision boundary.
    The margin/width between the support vectors is given in the first expression
    below. Also, we would match the predictions made by the SVM and the target labels,
    or more precisely, the sign of ***w.xi + b*** and ***yi***,
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: å‘é‡***w***å’Œ***b***æè¿°äº†å½¢æˆå†³ç­–è¾¹ç•Œçš„è¶…å¹³é¢ã€‚æ”¯æŒå‘é‡ä¹‹é—´çš„è¾¹é™…/å®½åº¦ç”±ä¸‹è¿°ç¬¬ä¸€ä¸ªè¡¨è¾¾å¼ç»™å‡ºã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬ä¼šåŒ¹é…SVMåšå‡ºçš„é¢„æµ‹ä¸ç›®æ ‡æ ‡ç­¾ï¼Œæ›´ç¡®åˆ‡åœ°è¯´ï¼Œå°±æ˜¯***w.xi
    + b***å’Œ***yi***çš„ç¬¦å·ã€‚
- en: '![](../Images/60d8e8aac232d17c4d89d8f58af3474e.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/60d8e8aac232d17c4d89d8f58af3474e.png)'
- en: The optimization problem that we have to solve for an optimal decision boundary
    is,
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¿…é¡»è§£å†³çš„ä¼˜åŒ–é—®é¢˜æ˜¯ï¼š
- en: '![](../Images/66f7e846215745c4aefb5e7a7183d3db.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/66f7e846215745c4aefb5e7a7183d3db.png)'
- en: SVM optimization problem
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: SVMä¼˜åŒ–é—®é¢˜
- en: We solve this optimization problem with Lagrangian multipliers, so the first
    step would be build a Lagrangian and equating the partial derivatives w.r.t. its
    parameters to zero. This would yield an expression for ***w*** which minimizes
    the Lagrangian.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬é€šè¿‡æ‹‰æ ¼æœ—æ—¥ä¹˜æ•°æ¥è§£å†³è¿™ä¸ªä¼˜åŒ–é—®é¢˜ï¼Œå› æ­¤ç¬¬ä¸€æ­¥æ˜¯æ„å»ºä¸€ä¸ªæ‹‰æ ¼æœ—æ—¥å‡½æ•°ï¼Œå¹¶å°†å…¶å‚æ•°çš„åå¯¼æ•°è®¾ç½®ä¸ºé›¶ã€‚è¿™å°†å¾—åˆ°ä¸€ä¸ª***w***çš„è¡¨è¾¾å¼ï¼Œèƒ½å¤Ÿæœ€å°åŒ–æ‹‰æ ¼æœ—æ—¥å‡½æ•°ã€‚
- en: '![](../Images/4b5967a4fb4ffb82a83d3767d073c912.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4b5967a4fb4ffb82a83d3767d073c912.png)'
- en: Setting the partial derivations of the Lagrangian to zero.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: å°†æ‹‰æ ¼æœ—æ—¥å‡½æ•°çš„åå¯¼æ•°è®¾ç½®ä¸ºé›¶ã€‚
- en: After substituting these results into the Lagrangian, we obtain an expression
    which clearly depicts the role of kernel functions,
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: å°†è¿™äº›ç»“æœä»£å…¥æ‹‰æ ¼æœ—æ—¥å‡½æ•°åï¼Œæˆ‘ä»¬å¾—åˆ°ä¸€ä¸ªæ¸…æ™°æè¿°æ ¸å‡½æ•°ä½œç”¨çš„è¡¨è¾¾å¼ã€‚
- en: '![](../Images/2901e07fc0dc2663ff6520d723cac2d4.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2901e07fc0dc2663ff6520d723cac2d4.png)'
- en: In order to achieve the optimal hyperplane, we need to compute the dot product
    between pairs of samples from our dataset. In some cases, finding an optimal hyperplane
    isnâ€™t possible, as the samples may not be linearly separable i.e. the samples
    couldnâ€™t be divided into two classes by merely drawing a line/plane. We can increase
    the dimensionality of the samples, by which we can discover a separating hyperplane
    easily.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†å®ç°æœ€ä½³è¶…å¹³é¢ï¼Œæˆ‘ä»¬éœ€è¦è®¡ç®—æ•°æ®é›†ä¸­æ ·æœ¬å¯¹ä¹‹é—´çš„ç‚¹ç§¯ã€‚åœ¨æŸäº›æƒ…å†µä¸‹ï¼Œæ‰¾åˆ°ä¸€ä¸ªæœ€ä½³è¶…å¹³é¢æ˜¯ä¸å¯è¡Œçš„ï¼Œå› ä¸ºæ ·æœ¬å¯èƒ½ä¸æ˜¯çº¿æ€§å¯åˆ†çš„ï¼Œå³æ ·æœ¬ä¸èƒ½ä»…é€šè¿‡ç»˜åˆ¶ä¸€æ¡çº¿/å¹³é¢æ¥åˆ†æˆä¸¤ä¸ªç±»åˆ«ã€‚æˆ‘ä»¬å¯ä»¥é€šè¿‡å¢åŠ æ ·æœ¬çš„ç»´åº¦æ¥å‘ç°ä¸€ä¸ªåˆ†ç¦»è¶…å¹³é¢ã€‚
- en: Consider a feature map ***Ï•***, that transform the data sample ***x*** into
    a higher dimensional feature i.e. ***Ï•( x )***. In the Lagrangian of the SVM,
    if we use these features instead of the data samples, we need to compu
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: è€ƒè™‘ä¸€ä¸ªç‰¹å¾æ˜ å°„***Ï•***ï¼Œå®ƒå°†æ•°æ®æ ·æœ¬***x***è½¬æ¢ä¸ºæ›´é«˜ç»´åº¦çš„ç‰¹å¾ï¼Œå³***Ï•(x)***ã€‚åœ¨SVMçš„æ‹‰æ ¼æœ—æ—¥å‡½æ•°ä¸­ï¼Œå¦‚æœæˆ‘ä»¬ä½¿ç”¨è¿™äº›ç‰¹å¾ä»£æ›¿æ•°æ®æ ·æœ¬ï¼Œæˆ‘ä»¬éœ€è¦è®¡ç®—
- en: '![](../Images/9cb279bd2433213c53e0db20423a23c9.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9cb279bd2433213c53e0db20423a23c9.png)'
- en: We can replace the dot product of the features with a kernel function that takes
    into two data samples (not transformed features),
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥ç”¨ä¸€ä¸ªæ ¸å‡½æ•°æ¥æ›¿ä»£ç‰¹å¾çš„ç‚¹ç§¯ï¼Œè¯¥æ ¸å‡½æ•°å¯¹ä¸¤ä¸ªæ•°æ®æ ·æœ¬è¿›è¡Œæ“ä½œï¼ˆè€Œä¸æ˜¯è½¬æ¢åçš„ç‰¹å¾ï¼‰ã€‚
- en: '![](../Images/c753232a3736088926414bcb63d16d79.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c753232a3736088926414bcb63d16d79.png)'
- en: This technique is popularly known as the *kernel trick* and is a direct consequence
    of Mercerâ€™s theorem. Weâ€™re able to calculate the dot product of two high-dimensional
    features without explicitly transforming the data samples to that high-dimensional
    space. With more dimensions, we have greater degrees of freedom for determining
    the optimal hyperplane. By choosing different kernels, the dimensionality of the
    space in which are features lie, can be controlled.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ç§æŠ€æœ¯è¢«å¹¿æ³›ç§°ä¸º*æ ¸æŠ€å·§*ï¼Œæ˜¯Mercerå®šç†çš„ç›´æ¥ç»“æœã€‚æˆ‘ä»¬èƒ½å¤Ÿè®¡ç®—ä¸¤ä¸ªé«˜ç»´ç‰¹å¾çš„ç‚¹ç§¯ï¼Œè€Œæ— éœ€æ˜¾å¼åœ°å°†æ•°æ®æ ·æœ¬è½¬æ¢åˆ°é‚£ä¸ªé«˜ç»´ç©ºé—´ã€‚éšç€ç»´åº¦çš„å¢åŠ ï¼Œæˆ‘ä»¬åœ¨ç¡®å®šæœ€ä½³è¶…å¹³é¢æ—¶å…·æœ‰æ›´å¤§çš„è‡ªç”±åº¦ã€‚é€šè¿‡é€‰æ‹©ä¸åŒçš„æ ¸å‡½æ•°ï¼Œå¯ä»¥æ§åˆ¶ç‰¹å¾æ‰€åœ¨ç©ºé—´çš„ç»´åº¦ã€‚
- en: The kernel function has a simpler expression to be evaluated, like the ones
    listed below,
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: æ ¸å‡½æ•°å…·æœ‰æ›´ç®€å•çš„è¡¨è¾¾å¼ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼Œ
- en: '![](../Images/a7fd6c029df8659a79c7cc4d858bfcbc.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a7fd6c029df8659a79c7cc4d858bfcbc.png)'
- en: 'Kernels For Convolutions: Image Processing'
  id: totrans-100
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 'å·ç§¯çš„æ ¸: å›¾åƒå¤„ç†'
- en: Kernels used in convolutions and image processing
  id: totrans-101
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ç”¨äºå·ç§¯å’Œå›¾åƒå¤„ç†çš„æ ¸
- en: Kernels are fixed-size matrices that are convolved across an image or a feature
    map to extract useful information out of it. In image processing lingo, a kernel
    matrix is also called a convolution matrix, and it used to perform operations
    on an image. Each kernel has a specialized operation of its own, which transforms
    the image after the convolution.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: æ ¸æ˜¯å›ºå®šå¤§å°çš„çŸ©é˜µï¼Œç”¨äºåœ¨å›¾åƒæˆ–ç‰¹å¾å›¾ä¸Šå·ç§¯ï¼Œä»¥æå–æœ‰ç”¨çš„ä¿¡æ¯ã€‚åœ¨å›¾åƒå¤„ç†ä¸­ï¼Œæ ¸çŸ©é˜µä¹Ÿç§°ä¸ºå·ç§¯çŸ©é˜µï¼Œç”¨äºå¯¹å›¾åƒè¿›è¡Œæ“ä½œã€‚æ¯ä¸ªæ ¸éƒ½æœ‰è‡ªå·±ç‰¹å®šçš„æ“ä½œï¼Œè¿™åœ¨å·ç§¯åä¼šæ”¹å˜å›¾åƒã€‚
- en: Convolutions and Kernels
  id: totrans-103
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å·ç§¯ä¸æ ¸
- en: Convolution is a mathematical operator that takes in two functions and produces
    another function. If weâ€™re convoluting two functions or signals, the result of
    the convolution is a function that represents the area of overlap between the
    two functions. Mathematically, the convolution operation is defined as,
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: å·ç§¯æ˜¯ä¸€ä¸ªæ•°å­¦ç®—å­ï¼Œå®ƒæ¥å—ä¸¤ä¸ªå‡½æ•°å¹¶ç”Ÿæˆå¦ä¸€ä¸ªå‡½æ•°ã€‚å¦‚æœæˆ‘ä»¬å¯¹ä¸¤ä¸ªå‡½æ•°æˆ–ä¿¡å·è¿›è¡Œå·ç§¯ï¼Œé‚£ä¹ˆå·ç§¯çš„ç»“æœæ˜¯ä¸€ä¸ªè¡¨ç¤ºä¸¤ä¸ªå‡½æ•°ä¹‹é—´é‡å åŒºåŸŸçš„å‡½æ•°ã€‚ä»æ•°å­¦ä¸Šè®²ï¼Œå·ç§¯æ“ä½œè¢«å®šä¹‰ä¸ºï¼Œ
- en: '![](../Images/b51c62a4780e4dde25ed85c8a8aa0dca.png)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b51c62a4780e4dde25ed85c8a8aa0dca.png)'
- en: 'An animated plot that depicts the convolution operation, along with its mathematical
    definition. This is analogous to the â€˜sliding of kernelâ€™ performed in density
    estimation. We slide the kernel over the data distribution, collecting â€˜neighborhood
    informationâ€™ and then estimate the density at a specific point. Source: [Wikipedia
    â€” Convolution (Wikimedia Commons)](https://en.wikipedia.org/wiki/Convolution)
    â€” [CC BY-SA 3.0](https://creativecommons.org/licenses/by-sa/3.0/)'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 'æè¿°å·ç§¯æ“ä½œçš„åŠ¨ç”»å›¾ï¼Œä»¥åŠå®ƒçš„æ•°å­¦å®šä¹‰ã€‚è¿™ç±»ä¼¼äºåœ¨å¯†åº¦ä¼°è®¡ä¸­è¿›è¡Œçš„â€œæ ¸æ»‘åŠ¨â€ã€‚æˆ‘ä»¬å°†æ ¸æ»‘è¿‡æ•°æ®åˆ†å¸ƒï¼Œæ”¶é›†â€œé‚»åŸŸä¿¡æ¯â€ï¼Œç„¶ååœ¨ç‰¹å®šç‚¹ä¼°è®¡å¯†åº¦ã€‚æ¥æº: [ç»´åŸºç™¾ç§‘
    â€” å·ç§¯ (ç»´åŸºå…±äº«èµ„æº)](https://en.wikipedia.org/wiki/Convolution) â€” [CC BY-SA 3.0](https://creativecommons.org/licenses/by-sa/3.0/)'
- en: Notice the result of the convolution operation when function ***g*** passes
    through the wall placed at ***x = 0*** in the plot above. The result changes suddenly
    and begins to increase due to a change in neighborhood information around ***x
    = 0***. The function ***g***, analogous to a kernel that weâ€™ve studied in density
    estimation, was able to react to changes that happened in kernelâ€™s area of influence.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: æ³¨æ„å‡½æ•°***g***åœ¨å›¾ä¸Šç»è¿‡***x = 0***å¤„å¢™å£æ—¶çš„å·ç§¯ç»“æœã€‚ç»“æœçªç„¶å˜åŒ–å¹¶å¼€å§‹å¢åŠ ï¼Œè¿™æ˜¯ç”±äº***x = 0***å‘¨å›´é‚»åŸŸä¿¡æ¯çš„å˜åŒ–ã€‚å‡½æ•°***g***ï¼Œç±»ä¼¼äºæˆ‘ä»¬åœ¨å¯†åº¦ä¼°è®¡ä¸­ç ”ç©¶çš„æ ¸ï¼Œèƒ½å¤Ÿå¯¹æ ¸å½±å“åŒºåŸŸå‘ç”Ÿçš„å˜åŒ–åšå‡ºååº”ã€‚
- en: In a discrete sense, the convolution operation is performed by the sliding the
    kernel function across the signal, multiplying the corresponding values of the
    signal and the kernel, and placing the sum of all these products in the resultant
    signal. In a mathematical sense, its good to think of *summation* with discrete
    signals, instead of *integration* on continuous signals.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: ä»ç¦»æ•£çš„è§’åº¦æ¥çœ‹ï¼Œå·ç§¯æ“ä½œæ˜¯é€šè¿‡å°†æ ¸å‡½æ•°æ»‘åŠ¨åˆ°ä¿¡å·ä¸Šï¼Œä¹˜ä»¥ä¿¡å·å’Œæ ¸çš„å¯¹åº”å€¼ï¼Œç„¶åå°†æ‰€æœ‰è¿™äº›ä¹˜ç§¯çš„æ€»å’Œæ”¾ç½®åˆ°ç»“æœä¿¡å·ä¸­æ¥å®Œæˆçš„ã€‚åœ¨æ•°å­¦æ„ä¹‰ä¸Šï¼Œè€ƒè™‘åˆ°ç¦»æ•£ä¿¡å·çš„*æ±‚å’Œ*è¦æ¯”è€ƒè™‘è¿ç»­ä¿¡å·ä¸Šçš„*ç§¯åˆ†*æ›´ä¸ºåˆé€‚ã€‚
- en: '![](../Images/999a00da10ce33d206ec8bedbde4ff65.png)![](../Images/bc5768b181b6fe95e2797db01b18a2e2.png)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/999a00da10ce33d206ec8bedbde4ff65.png)![](../Images/bc5768b181b6fe95e2797db01b18a2e2.png)'
- en: 'Performing the convolution operation on 1D discrete signal. The expression
    above shows the mathematical formulation for the same. Image Source: [Convolution
    in one dimension for neural networks â€” Brandon Rohrer](https://e2eml.school/convolution_one_d.html)
    (Creative Commons License) â€” [CC0 1.0 Universal](https://creativecommons.org/publicdomain/zero/1.0/)'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 'å¯¹ä¸€ç»´ç¦»æ•£ä¿¡å·æ‰§è¡Œå·ç§¯æ“ä½œã€‚ä¸Šé¢çš„è¡¨è¾¾å¼æ˜¾ç¤ºäº†ç›¸åŒçš„æ•°å­¦å…¬å¼ã€‚å›¾åƒæ¥æº: [ç¥ç»ç½‘ç»œä¸­çš„ä¸€ç»´å·ç§¯ â€” Brandon Rohrer](https://e2eml.school/convolution_one_d.html)ï¼ˆåˆ›ä½œå…±ç”¨è®¸å¯è¯ï¼‰
    â€” [CC0 1.0 Universal](https://creativecommons.org/publicdomain/zero/1.0/)'
- en: For images, weâ€™ll slide a 2D kernel on the given image and the perform the same
    operation. The motion of the kernel will be 2D here, contrary to the 1D (unidirectional)
    motion of the kernel on a 1D signal. The output will be a matrix, as the convolution
    operation was also performed on a 2D input.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºå›¾åƒï¼Œæˆ‘ä»¬å°†ä¸€ä¸ªäºŒç»´æ ¸æ»‘åŠ¨åˆ°ç»™å®šçš„å›¾åƒä¸Šå¹¶æ‰§è¡Œç›¸åŒçš„æ“ä½œã€‚è¿™é‡Œï¼Œæ ¸çš„è¿åŠ¨å°†æ˜¯äºŒç»´çš„ï¼Œä¸åœ¨ä¸€ç»´ä¿¡å·ä¸Šè¿›è¡Œçš„ä¸€ç»´ï¼ˆå•å‘ï¼‰æ ¸è¿åŠ¨ç›¸å¯¹ã€‚è¾“å‡ºå°†æ˜¯ä¸€ä¸ªçŸ©é˜µï¼Œå› ä¸ºå·ç§¯æ“ä½œä¹Ÿæ˜¯åœ¨äºŒç»´è¾“å…¥ä¸Šæ‰§è¡Œçš„ã€‚
- en: '![](../Images/b498198a663e03fa42eec7cd3432e3b3.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b498198a663e03fa42eec7cd3432e3b3.png)'
- en: 'Convolution with a kernel matrix. Source: [Convolution â€” Wikipedia](https://en.wikipedia.org/wiki/Convolution)
    (Creative Commons License) â€” [CC BY-SA 3.0](https://creativecommons.org/licenses/by-sa/3.0/)'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨å†…æ ¸çŸ©é˜µè¿›è¡Œå·ç§¯ã€‚æ¥æºï¼š[å·ç§¯ â€” ç»´åŸºç™¾ç§‘](https://en.wikipedia.org/wiki/Convolution)ï¼ˆåˆ›ä½œå…±ç”¨è®¸å¯è¯ï¼‰â€”
    [CC BY-SA 3.0](https://creativecommons.org/licenses/by-sa/3.0/)
- en: We can use different kernels to extract various features from the input and
    or enhance the image for further operations. For instance, the *sharpen kernel*,
    would sharpen the edges present in an image. Many other kernels, on convolution,
    extract interesting features from an image,
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥ä½¿ç”¨ä¸åŒçš„å†…æ ¸ä»è¾“å…¥ä¸­æå–å„ç§ç‰¹å¾æˆ–å¢å¼ºå›¾åƒä»¥è¿›è¡Œè¿›ä¸€æ­¥æ“ä½œã€‚ä¾‹å¦‚ï¼Œ*é”åŒ–å†…æ ¸*ä¼šé”åŒ–å›¾åƒä¸­çš„è¾¹ç¼˜ã€‚è®¸å¤šå…¶ä»–å†…æ ¸åœ¨å·ç§¯è¿‡ç¨‹ä¸­ä»å›¾åƒä¸­æå–æœ‰è¶£çš„ç‰¹å¾ã€‚
- en: '![](../Images/f7c405525d85632f7b67bb55f22fa576.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f7c405525d85632f7b67bb55f22fa576.png)'
- en: 'Performing convolutions with different kernels used in image processing. Source:
    Image by author'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ä¸åŒçš„å†…æ ¸åœ¨å›¾åƒå¤„ç†ä¸­æ‰§è¡Œå·ç§¯ã€‚æ¥æºï¼šä½œè€…æä¾›çš„å›¾ç‰‡
- en: Kernels in CNNs
  id: totrans-117
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: CNNä¸­çš„å†…æ ¸
- en: The kernel that we just saw were constant but what if we can parameterize the
    kernel and control which features are being extracted? This would be helpful in
    convolutional neural networks, where we fine-tune kernels to minimize the overall
    loss incurred by the NN. The notion of non-parametric models built with kernels
    would diminish here, as CNNs can have a huge number of parameters, but the basic
    concept of *neighborhood information extraction* is still valid.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬åˆšåˆšçœ‹åˆ°çš„å†…æ ¸æ˜¯å¸¸é‡ï¼Œä½†å¦‚æœæˆ‘ä»¬å¯ä»¥å¯¹å†…æ ¸è¿›è¡Œå‚æ•°åŒ–å¹¶æ§åˆ¶æå–å“ªäº›ç‰¹å¾å‘¢ï¼Ÿè¿™åœ¨å·ç§¯ç¥ç»ç½‘ç»œä¸­ä¼šå¾ˆæœ‰ç”¨ï¼Œå› ä¸ºæˆ‘ä»¬å¾®è°ƒå†…æ ¸ä»¥æœ€å°åŒ–NNäº§ç”Ÿçš„æ•´ä½“æŸå¤±ã€‚åŸºäºå†…æ ¸çš„éå‚æ•°æ¨¡å‹çš„æ¦‚å¿µåœ¨è¿™é‡Œä¼šå‡å°‘ï¼Œå› ä¸ºCNNå¯ä»¥æ‹¥æœ‰å¤§é‡å‚æ•°ï¼Œä½†*é‚»åŸŸä¿¡æ¯æå–*çš„åŸºæœ¬æ¦‚å¿µä»ç„¶æœ‰æ•ˆã€‚
- en: The function of a kernel here, is similar to that of the *sharpen* or Sobel
    X kernel, but it considers the values in the matrix as parameters and not fix
    numbers. These trainable kernels are optimized with backpropagation to reduce
    the value of the loss value in a CNN. A convolutional layer can have many such
    kernels collectively known as filters.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œçš„å†…æ ¸åŠŸèƒ½ç±»ä¼¼äº*é”åŒ–*æˆ–Sobel Xå†…æ ¸ï¼Œä½†å®ƒå°†çŸ©é˜µä¸­çš„å€¼è§†ä¸ºå‚æ•°ï¼Œè€Œä¸æ˜¯å›ºå®šçš„æ•°å­—ã€‚è¿™äº›å¯è®­ç»ƒçš„å†…æ ¸é€šè¿‡åå‘ä¼ æ’­è¿›è¡Œä¼˜åŒ–ï¼Œä»¥å‡å°‘CNNä¸­çš„æŸå¤±å€¼ã€‚ä¸€ä¸ªå·ç§¯å±‚å¯ä»¥æœ‰è®¸å¤šè¿™æ ·çš„å†…æ ¸ï¼Œç»Ÿç§°ä¸ºæ»¤æ³¢å™¨ã€‚
- en: '[](/no-kernels-filters-are-not-the-same-b230ec192ac9?source=post_page-----f5d255d95785--------------------------------)
    [## No, Kernels & Filters Are Not The Same'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/no-kernels-filters-are-not-the-same-b230ec192ac9?source=post_page-----f5d255d95785--------------------------------)
    [## ä¸ï¼Œå†…æ ¸å’Œæ»¤æ³¢å™¨å¹¶ä¸ç›¸åŒ'
- en: Solving the usual confusion.
  id: totrans-121
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: è§£å†³å¸¸è§çš„æ··æ·†é—®é¢˜ã€‚
- en: towardsdatascience.com](/no-kernels-filters-are-not-the-same-b230ec192ac9?source=post_page-----f5d255d95785--------------------------------)
    ![](../Images/34ad39d12d16988049914ce614cddd3b.png)
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/no-kernels-filters-are-not-the-same-b230ec192ac9?source=post_page-----f5d255d95785--------------------------------)
    ![](../Images/34ad39d12d16988049914ce614cddd3b.png)
- en: 'A typical convolutional neural network with max pooling and linear (dense)
    layers. The convolutional and max pooling layers extract features from the input
    image which are passed to the linear layers. Source: Image by Author'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªå…¸å‹çš„å·ç§¯ç¥ç»ç½‘ç»œï¼ŒåŒ…æ‹¬æœ€å¤§æ± åŒ–å’Œçº¿æ€§ï¼ˆå…¨è¿æ¥ï¼‰å±‚ã€‚å·ç§¯å±‚å’Œæœ€å¤§æ± åŒ–å±‚ä»è¾“å…¥å›¾åƒä¸­æå–ç‰¹å¾ï¼Œç„¶åå°†è¿™äº›ç‰¹å¾ä¼ é€’ç»™çº¿æ€§å±‚ã€‚æ¥æºï¼šä½œè€…æä¾›çš„å›¾ç‰‡
- en: The outputs produced by the first convolutional layers are passed to the next
    layer. This creates a hierarchical feature extraction process, where low level
    features of the image are extracted by the initial convolutional layers and the
    high-level features are tracked by the final/last convolutional layers. Such a
    stack of convolutions combined with trainable kernels provides CNNs the power
    to identify objects in images with a great precision, opening the doors of modern
    computer vision.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: ç¬¬ä¸€ä¸ªå·ç§¯å±‚äº§ç”Ÿçš„è¾“å‡ºä¼ é€’åˆ°ä¸‹ä¸€ä¸ªå±‚ã€‚è¿™åˆ›å»ºäº†ä¸€ä¸ªåˆ†å±‚ç‰¹å¾æå–è¿‡ç¨‹ï¼Œå…¶ä¸­å›¾åƒçš„ä½çº§ç‰¹å¾ç”±åˆå§‹å·ç§¯å±‚æå–ï¼Œé«˜çº§ç‰¹å¾åˆ™ç”±æœ€åçš„å·ç§¯å±‚è·Ÿè¸ªã€‚è¿™æ ·çš„å·ç§¯å †æ ˆä¸å¯è®­ç»ƒçš„å†…æ ¸ç»“åˆï¼Œä½¿CNNèƒ½å¤Ÿä»¥æé«˜çš„ç²¾åº¦è¯†åˆ«å›¾åƒä¸­çš„ç‰©ä½“ï¼Œä¸ºç°ä»£è®¡ç®—æœºè§†è§‰å¼€è¾Ÿäº†æ–°çš„é¢†åŸŸã€‚
- en: The End
  id: totrans-125
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç»“æŸ
- en: I hope this journey through the world of kernels fascinated you towards the
    concept. Kernels are heavily confused amongst various topics, but their core idea
    remains the same, which weâ€™ve repeated several times in the blog as *neighborhood
    feature extraction*. Instead of using parameters to capture patterns within the
    data, kernel functions can encode relative proximity of samples to capture trends
    within the data. But, one must understand that parametric models have their own
    advantages and their use isnâ€™t obsolete. Most NN models are huge parametric models
    with millions of parameters and they can solve complex problems such as object
    detection, image classification, speech synthesis and more.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘å¸Œæœ›è¿™æ®µå…³äºå†…æ ¸ä¸–ç•Œçš„æ—…ç¨‹èƒ½è®©ä½ å¯¹è¿™ä¸€æ¦‚å¿µæ„Ÿåˆ°ç€è¿·ã€‚å†…æ ¸åœ¨å„ç§è¯é¢˜ä¸­ç»å¸¸å¼•èµ·æ··æ·†ï¼Œä½†å…¶æ ¸å¿ƒæ€æƒ³å§‹ç»ˆå¦‚ä¸€ï¼Œæˆ‘ä»¬åœ¨åšå®¢ä¸­å¤šæ¬¡æåˆ°çš„å°±æ˜¯*é‚»åŸŸç‰¹å¾æå–*ã€‚ä¸å…¶ä½¿ç”¨å‚æ•°æ•æ‰æ•°æ®ä¸­çš„æ¨¡å¼ï¼Œå†…æ ¸å‡½æ•°å¯ä»¥ç¼–ç æ ·æœ¬çš„ç›¸å¯¹æ¥è¿‘åº¦ï¼Œä»è€Œæ•æ‰æ•°æ®ä¸­çš„è¶‹åŠ¿ã€‚ç„¶è€Œï¼Œå¿…é¡»ç†è§£å‚æ•°æ¨¡å‹æœ‰å…¶è‡ªèº«çš„ä¼˜åŠ¿ï¼Œå®ƒä»¬çš„ä½¿ç”¨å¹¶æœªè¿‡æ—¶ã€‚å¤§å¤šæ•°ç¥ç»ç½‘ç»œæ¨¡å‹æ˜¯åºå¤§çš„å‚æ•°æ¨¡å‹ï¼Œå…·æœ‰æ•°ç™¾ä¸‡ä¸ªå‚æ•°ï¼Œå¯ä»¥è§£å†³å¤æ‚çš„é—®é¢˜ï¼Œå¦‚ç‰©ä½“æ£€æµ‹ã€å›¾åƒåˆ†ç±»ã€è¯­éŸ³åˆæˆç­‰ã€‚
- en: All images, unless otherwise noted, are by the author.
  id: totrans-127
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: é™¤éå¦æœ‰è¯´æ˜ï¼Œæ‰€æœ‰å›¾åƒå‡ç”±ä½œè€…æä¾›ã€‚
