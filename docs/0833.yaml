- en: 'Kernels: Everything You Need to Know'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ÂéüÊñáÔºö[https://towardsdatascience.com/kernels-everything-you-need-to-know-f5d255d95785?source=collection_archive---------0-----------------------#2023-03-06](https://towardsdatascience.com/kernels-everything-you-need-to-know-f5d255d95785?source=collection_archive---------0-----------------------#2023-03-06)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: üë®‚Äçüè´ [Mathematics](https://equipintelligence.medium.com/list/mathematics-demystified-7e4d1c18041f)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Density Estimation, Dot Products, Convolutions and everything‚Ä¶
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://equipintelligence.medium.com/?source=post_page-----f5d255d95785--------------------------------)[![Shubham
    Panchal](../Images/d48aecd8b1ed27ab68fc2e7ff6716606.png)](https://equipintelligence.medium.com/?source=post_page-----f5d255d95785--------------------------------)[](https://towardsdatascience.com/?source=post_page-----f5d255d95785--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----f5d255d95785--------------------------------)
    [Shubham Panchal](https://equipintelligence.medium.com/?source=post_page-----f5d255d95785--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ¬∑
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fd45a9465f044&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fkernels-everything-you-need-to-know-f5d255d95785&user=Shubham+Panchal&userId=d45a9465f044&source=post_page-d45a9465f044----f5d255d95785---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----f5d255d95785--------------------------------)
    ¬∑14 min read¬∑Mar 6, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Ff5d255d95785&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fkernels-everything-you-need-to-know-f5d255d95785&user=Shubham+Panchal&userId=d45a9465f044&source=-----f5d255d95785---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff5d255d95785&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fkernels-everything-you-need-to-know-f5d255d95785&source=-----f5d255d95785---------------------bookmark_footer-----------)![](../Images/ab190b0872063f05abcb2425835c5803.png)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Source: Image by author'
  prefs: []
  type: TYPE_NORMAL
- en: Kernels or kernel functions are beautiful mathematical concepts that are used
    in machine learning and statistics with different forms. If you‚Äôre a beginner
    you might feel tempted to know the exact definition of kernels, but you may get
    confused by multiple definitions of kernels that are explained across blogs/websites
    on the internet.
  prefs: []
  type: TYPE_NORMAL
- en: The kernel function is a confusing concept, just because the knowledge around
    it is decentralized across its applications and a common intuition connecting
    them is missing. This (huge) blog undertakes the motive of unifying all knowledge
    on kernels used in different ML applications. Just as most beginners, the kernel
    function kept me in a state of confusion for a long time, until I developed an
    intuition that would connect all links.
  prefs: []
  type: TYPE_NORMAL
- en: 'We begin our journey with non-parametric models, then we start discussing different
    types of kernels and their typical applications across statistics and ML. Similar
    to kernel functions, I have an attempt to explain PCA mathematically, considering
    all perspectives. You may have a read:'
  prefs: []
  type: TYPE_NORMAL
- en: '[](/principal-component-analysis-everything-you-need-to-know-5f834c9eaa83?source=post_page-----f5d255d95785--------------------------------)
    [## Principal Component Analysis: Everything You Need To Know'
  prefs: []
  type: TYPE_NORMAL
- en: Covariance, eigenvalues, variance and everything ‚Ä¶
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/principal-component-analysis-everything-you-need-to-know-5f834c9eaa83?source=post_page-----f5d255d95785--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: Non-Parametric Models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Non-parametric models are those statistical models that do not have parameters
    which grow with the size of the input. Note, non-parametric models do not mean
    ‚Äò*model with zero parameters*‚Äô but they work with a fixed set of parameters, also
    called hyperparameters, which do not increase with the increase in the dimensionality
    of the inputs. A vanilla linear regression model, has parameters ***Œ∏*** that
    determine the *slope* of the hyperplane whose size depends on the dimensionality
    of the input ***x***,
  prefs: []
  type: TYPE_NORMAL
- en: Eqn. 1\. A simple linear regression model with tunable parameters whose size
    depends on the number of features **N**. The function **f** represents the expected
    value of the dependent variables **y** given inputs **x** i.e. **E[y|x]**
  prefs: []
  type: TYPE_NORMAL
- en: Next, consider the KNN model, where we determine the class of a test sample
    by analyzing the classes of its ***K*** nearest neighbors. If ***K = 1***, we
    assume that the test sample belongs to the same class as that of the nearest neighbor.
    This model does not have any parameters that would grow with the dimensionality
    of the inputs. For a vanilla implementation, we would only need a single parameter,
    ***K***, even if we working with large inputs (in terms of dimensionality).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b91bc02863d3d8166c0029592927d18a.png)'
  prefs: []
  type: TYPE_IMG
- en: '1\. A visual depiction of the working of K Nearest Neighbors model. The assumption
    made here is that the test sample belongs to same class as that of the K nearest
    neighbors. Image Source: Image by Author'
  prefs: []
  type: TYPE_NORMAL
- en: KNN is a non-parametric model which has a hyperparameter ***K***, provided by
    the user. Non-parametric might seem to an *obvious-choice* at the first glance,
    as,
  prefs: []
  type: TYPE_NORMAL
- en: They make no prior assumptions regarding the distribution of the data. For instance,
    in case of *vanilla* linear regression, which is a parametric model, we assume
    that the conditional distribution of ***Y*** (dependent variables) given ***X***
    (features) is a Gaussian Distribution whose mean is a linear combination of the
    features (where weights are ***Œ∏)*** and variance equal to ***œÉ¬≤***
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 2\. The conditional distribution of **Y** given **X** and parameters ***Œ∏***
    *is a normal distribution from which we wish to capture the mean from* ***X****.
    The constant variance is a result of the* [*assumption of* homoscedasticity](https://en.wikipedia.org/wiki/Homoscedasticity_and_heteroscedasticity)
  prefs: []
  type: TYPE_NORMAL
- en: which might not hold always, because,
  prefs: []
  type: TYPE_NORMAL
- en: For each test sample, they need to keep the entire training data in memory,
    which is also true for the KNN model. For each sample, we need to calculate its
    distance from each training sample, so we need to retrieve/store each sample in
    memory, which might not be feasible to large datasets or even smaller datasets
    which a large number of features.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The basic idea around non-parametric models is to gather some useful insight
    from the data and use to solve the given problem, without encoding information
    about the data in tunable parameters.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we focus on kernels, that have different use-cases across ML, and differ
    slightly with their meanings in each context. So far, after researching for this
    blog, and also from my previous attempts of understanding kernels as a whole,
    I feel that kernels are *machines* that provide information on the neighbors of
    a given datapoint (as an *input* to the machine). This *local* information i.e.
    the information on datapoints that lie in the proximity of the datapoint under
    consideration, is then used to solve the given problem. Once we use the kernel
    function on each of the datapoints, we can get a clear picture of *what‚Äôs going
    on in the locality of the data*.
  prefs: []
  type: TYPE_NORMAL
- en: We‚Äôll explore these three aspects of a kernel, which are three different concepts
    with their major applications in ML,
  prefs: []
  type: TYPE_NORMAL
- en: '[Density Kernels](#d69a)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Mercer Kernels](#daa0)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Kernels in Image Processing](#ed44)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Density Kernels: Use of Kernels for Density Estimation'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Density kernels, kernel density estimation, kernel regression
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: We can use kernels to estimate the probability density for a given *test* sample,
    by modelling the underlying probability distribution with the *training* samples.
    The terms ‚Äòtest‚Äô sample and ‚Äòtraining‚Äô samples simply refer to unobserved and
    observed samples respectively, considering ML lingo.
  prefs: []
  type: TYPE_NORMAL
- en: For continuous random variable ***X***, we can integrate the probability density
    function of ***X*** within a suitable range, say, from ***x_1*** to ***x_2***,
    and we get the probability of ***X*** assuming a value in the range ***[ x_1 ,
    x_2 ]***. If you aren‚Äôt comfortable with the topic of probability density or random
    variables, here‚Äôs my 3-part series on probability distributions,
  prefs: []
  type: TYPE_NORMAL
- en: Density Kernels and Kernel Density Estimation (KDE)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let us start our discussion with a problem. Dr. Panchal lives in a crowded city
    block which is surrounded by houses on all sides. The local police have hired
    a detective whose job is to determine the number of persons who reside in the
    doctor‚Äôs house or who are his family members, just to make sure things are going
    well. The detective could not ring the doctor‚Äôs bell and ask how many family members
    reside there, as it would warn the doctor if there‚Äôs something suspicious.
  prefs: []
  type: TYPE_NORMAL
- en: The detective would start by interrogating the houses that are adjacent to Dr.
    Panchal‚Äôs house, as they could have a clear sight of what‚Äôs in there. The detective
    is expected to give a higher weight/importance to the information gained from
    these direct neighbors. Next, in order to gain more insights, the detective interrogates
    houses that are slightly farther away that do not have a direct sight of Dr. Panchal‚Äôs
    house but might have good information of their neighbor. The detective would give
    lesser importance to the information received from these neighbors, as their observations
    might not be that correct as that of the direct neighbors (whose houses are adjacent
    to Panchal‚Äôs). The detective performs several such rounds reducing the importance,
    moving away from Dr. Panchal‚Äôs house.
  prefs: []
  type: TYPE_NORMAL
- en: A density kernel does something similar to capture *neighboring* information
    around a given point. If we‚Äôre given a dataset ***D*** with ***N*** samples where
    each sample is a real number,
  prefs: []
  type: TYPE_NORMAL
- en: The kernel in the above snippet is an Epanechnikov (parabolic) kernel. The kernel
    function has some special properties here,
  prefs: []
  type: TYPE_NORMAL
- en: '*Property 1*: The kernel function or the detective does not matter in which
    direction the x or some neighbor‚Äôs house lies. Information gained from two houses
    to the right or two houses to the left is the same.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Property 2*: The kernel function represents a valid PDF and it integrates
    to 1 over the entire real domain.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Property 3*: Support of a kernel is the set of all values ***u*** such that
    ***K( u )*** is not equal to 0\. It represents the coverage area of the detective
    from where some non-zero importance will be given to information gathered. If
    the detective decides to interrogate in all houses in a radius of 5 km, the support
    would be all houses within that 5 km circle.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Every type of kernel will perform a similar task of encoding neighboring information,
    which each one will have a different strategy to do so. Unlike our detective,
    who slowly reduced the importance of interrogation as he moved away from Dr. Panchal‚Äôs
    house (a Gaussian kernel would do that), another detective might just continue
    giving equal importance to all interrogations, neglecting the distance upto a
    certain extent (a uniform kernel). Imagine, that from our dataset ***D***, the
    distribution of all ***x_i*** is,
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2d959ddb054dce433217e71275ced376.png)'
  prefs: []
  type: TYPE_IMG
- en: A Gaussian-like distribution whose probability density needs to be estimated
  prefs: []
  type: TYPE_NORMAL
- en: Our goal is to create an estimate of the probability distribution of ***X***.
    We‚Äôll do this by estimating the density at each sample ***x_i*** and using the
    kernel to collect neighboring information,
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6a39f0c5d3d25de92a9fdfc4198e017e.png)'
  prefs: []
  type: TYPE_IMG
- en: Estimated probability density with KDE
  prefs: []
  type: TYPE_NORMAL
- en: If ***x_i*** is away from ***x***, ***| x ‚Äî x_i |*** will have a larger value
    thus yielding a very small value for ***K( x ‚Äî x_i )*** and reducing the *say*
    of ***x_i*** in the determination of probability density at ***x***. The parameter
    ***h*** is the *smoothing parameter* called the *bandwidth* of the kernel. Greater
    the value of ***h***, smoother will be predicted probability density.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/081bed27008b194d65d79d0ac17ba022.png)'
  prefs: []
  type: TYPE_IMG
- en: Probability density estimations obtained by changing the bandwidth of the kernel.
    The bottommost figure depicts the data distribution and we obtain a smoother density
    estimation by increasing the bandwidth of the kernel. The kernel used for estimation
    here, was the **Epanechnikov/Parabolic** kernel.
  prefs: []
  type: TYPE_NORMAL
- en: Kernel Regression
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Kernel regression is a non-parametric version of the linear regression model
    where we model the conditional expectation of the outcome variable. In the case
    of simple linear regression, we model the conditional expectation ***E[ Y | X
    ]*** directly by expressing it as a linear combination of the independent variables.
    This gives rise to a discriminative model, whereas kernel regression is a generative
    model, as we‚Äôre modelling the joint probability distribution ***P( X , Y )***
    and ***P( X )*** by the means of kernel density estimation.
  prefs: []
  type: TYPE_NORMAL
- en: Deriving an estimator for **E[ Y | X ]** which does not have any additional
    parameters. In case of simple linear regression, **E[ Y | X ]** would be modelled
    as a weighed linear combination of the all **X_i**‚Äôs where the weights are the
    parameters.
  prefs: []
  type: TYPE_NORMAL
- en: On observing the resultant expression, you‚Äôll realize that the predicted outcome
    ***y_hat*** is a weighed combination of all ***y_i***, where the weights are determined
    by the values of the kernel function for all ***x_i***.
  prefs: []
  type: TYPE_NORMAL
- en: 'Mercer Kernels: Dot Products in Higher Dimensions'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Mercer kernels and positive definiteness, use of Mercer kernels in SVM
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Mercer Kernels or Positive definite kernels are functions that take in two inputs
    and produce a real number which characterizes the proximity of the two inputs
    (or their high-dimensional representations) in some other space. It turns out
    that these kernels are useful from the computational perspective as they help
    us compute dot products of vectors in a high-dimensional without explicitly performing
    any transformation to bring our own vectors into that high-dimensional space.
  prefs: []
  type: TYPE_NORMAL
- en: Mercer Kernels
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let us start our discussion by defining the kernel function and some of its
    properties,
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c5183c93b4a4783dfae49df86d3b007e.png)'
  prefs: []
  type: TYPE_IMG
- en: The Mercer kernel is a function that takes in two datapoints from our dataset
    ***D***, and outputs a real number that represents the proximity of these two
    datapoints in the feature space.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If we have ***n*** datapoints in our dataset ***D***, and we run our Mercer
    kernel on each pair of datapoints and arrange the resulting outputs in a matrix,
    we achieve a positive definite matrix. This matrix, that depicts the similarity
    amongst the datapoints, is called the Gram Matrix.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](../Images/aefa15277ec3acde2e1b3eb8fe7fffb3.png)'
  prefs: []
  type: TYPE_IMG
- en: The Gram Matrix
  prefs: []
  type: TYPE_NORMAL
- en: Positive definite matrices are special, considering their spectral properties.
    They have positive eigenvalues and the corresponding eigenvectors form an orthonormal
    basis. We have a special property, for mercer kernels, using which the value of
    the kernel function as be expressed as the dot product of two transformed vectors,
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8731da29e52f913ef2df9a496489a9bb.png)'
  prefs: []
  type: TYPE_IMG
- en: Each entry of the Gram matrix can be described as the dot product of two transformed
    samples.
  prefs: []
  type: TYPE_NORMAL
- en: One might have an urge to feel the intuition behind this statement, but it lives
    in the sanctum of Hilbert Spaces which deserves a separate blog. For time being,
    its good to understand that the value of the kernel function, with two input vectors,
    as be described as the dot product of some other two vectors that lie in higher
    dimensional.
  prefs: []
  type: TYPE_NORMAL
- en: Mercer kernels provide a shortcut for computing the dot product between those
    two high-dimensional vectors without explicitly computing those vectors. Hence,
    we can leverage the advantages of high-dimensional spaces are sometimes useful
    in machine learning, especially when samples are not linearly separable.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1ddba6803e3cdca1250a355e0e379d94.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Samples which are linearly inseparable in lower dimensions may find an optimal
    hyperplane in higher dimensions. Source: Image by author'
  prefs: []
  type: TYPE_NORMAL
- en: For some optimization problems, like the one encountered while optimizing SVMs,
    we‚Äôll need to compute dot products between two transformed samples which are two
    high-dimensional vectors. The use of kernel functions will help us compute this
    dot product easily without performing any explicit transformations on the samples.
  prefs: []
  type: TYPE_NORMAL
- en: Use of Mercer Kernels in SVMs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: SVMs are linear classifiers that fit a hyperplane such that a decision boundary
    is formed between samples of two classes. To determine the best hyperplane i.e.
    the hyperplane which divides the samples into two classes and maximizes the ‚Äòmargin‚Äô,
    we need to solve an optimization problem that contains an objective function (a
    function which is either maximized or minimized) given some constraints on the
    parameters of the objective.
  prefs: []
  type: TYPE_NORMAL
- en: The derivation of the SVM optimization problem is covered in these blogs extensively
    by [Saptashwa Bhattacharyya](https://medium.com/u/9a3c3c477239?source=post_page-----f5d255d95785--------------------------------)
    so reading them once would help us proceed further,
  prefs: []
  type: TYPE_NORMAL
- en: '[](/understanding-support-vector-machine-part-2-kernel-trick-mercers-theorem-e1e6848c6c4d?source=post_page-----f5d255d95785--------------------------------)
    [## Understanding Support Vector Machine: Part 2: Kernel Trick; Mercer‚Äôs Theorem'
  prefs: []
  type: TYPE_NORMAL
- en: Why Kernel ?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/understanding-support-vector-machine-part-2-kernel-trick-mercers-theorem-e1e6848c6c4d?source=post_page-----f5d255d95785--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: The vectors w and b characterize the hyperplane which forms the decision boundary.
    The margin/width between the support vectors is given in the first expression
    below. Also, we would match the predictions made by the SVM and the target labels,
    or more precisely, the sign of ***w.xi + b*** and ***yi***,
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/60d8e8aac232d17c4d89d8f58af3474e.png)'
  prefs: []
  type: TYPE_IMG
- en: The optimization problem that we have to solve for an optimal decision boundary
    is,
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/66f7e846215745c4aefb5e7a7183d3db.png)'
  prefs: []
  type: TYPE_IMG
- en: SVM optimization problem
  prefs: []
  type: TYPE_NORMAL
- en: We solve this optimization problem with Lagrangian multipliers, so the first
    step would be build a Lagrangian and equating the partial derivatives w.r.t. its
    parameters to zero. This would yield an expression for ***w*** which minimizes
    the Lagrangian.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4b5967a4fb4ffb82a83d3767d073c912.png)'
  prefs: []
  type: TYPE_IMG
- en: Setting the partial derivations of the Lagrangian to zero.
  prefs: []
  type: TYPE_NORMAL
- en: After substituting these results into the Lagrangian, we obtain an expression
    which clearly depicts the role of kernel functions,
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2901e07fc0dc2663ff6520d723cac2d4.png)'
  prefs: []
  type: TYPE_IMG
- en: In order to achieve the optimal hyperplane, we need to compute the dot product
    between pairs of samples from our dataset. In some cases, finding an optimal hyperplane
    isn‚Äôt possible, as the samples may not be linearly separable i.e. the samples
    couldn‚Äôt be divided into two classes by merely drawing a line/plane. We can increase
    the dimensionality of the samples, by which we can discover a separating hyperplane
    easily.
  prefs: []
  type: TYPE_NORMAL
- en: Consider a feature map ***œï***, that transform the data sample ***x*** into
    a higher dimensional feature i.e. ***œï( x )***. In the Lagrangian of the SVM,
    if we use these features instead of the data samples, we need to compu
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9cb279bd2433213c53e0db20423a23c9.png)'
  prefs: []
  type: TYPE_IMG
- en: We can replace the dot product of the features with a kernel function that takes
    into two data samples (not transformed features),
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c753232a3736088926414bcb63d16d79.png)'
  prefs: []
  type: TYPE_IMG
- en: This technique is popularly known as the *kernel trick* and is a direct consequence
    of Mercer‚Äôs theorem. We‚Äôre able to calculate the dot product of two high-dimensional
    features without explicitly transforming the data samples to that high-dimensional
    space. With more dimensions, we have greater degrees of freedom for determining
    the optimal hyperplane. By choosing different kernels, the dimensionality of the
    space in which are features lie, can be controlled.
  prefs: []
  type: TYPE_NORMAL
- en: The kernel function has a simpler expression to be evaluated, like the ones
    listed below,
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a7fd6c029df8659a79c7cc4d858bfcbc.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Kernels For Convolutions: Image Processing'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Kernels used in convolutions and image processing
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Kernels are fixed-size matrices that are convolved across an image or a feature
    map to extract useful information out of it. In image processing lingo, a kernel
    matrix is also called a convolution matrix, and it used to perform operations
    on an image. Each kernel has a specialized operation of its own, which transforms
    the image after the convolution.
  prefs: []
  type: TYPE_NORMAL
- en: Convolutions and Kernels
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Convolution is a mathematical operator that takes in two functions and produces
    another function. If we‚Äôre convoluting two functions or signals, the result of
    the convolution is a function that represents the area of overlap between the
    two functions. Mathematically, the convolution operation is defined as,
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b51c62a4780e4dde25ed85c8a8aa0dca.png)'
  prefs: []
  type: TYPE_IMG
- en: 'An animated plot that depicts the convolution operation, along with its mathematical
    definition. This is analogous to the ‚Äòsliding of kernel‚Äô performed in density
    estimation. We slide the kernel over the data distribution, collecting ‚Äòneighborhood
    information‚Äô and then estimate the density at a specific point. Source: [Wikipedia
    ‚Äî Convolution (Wikimedia Commons)](https://en.wikipedia.org/wiki/Convolution)
    ‚Äî [CC BY-SA 3.0](https://creativecommons.org/licenses/by-sa/3.0/)'
  prefs: []
  type: TYPE_NORMAL
- en: Notice the result of the convolution operation when function ***g*** passes
    through the wall placed at ***x = 0*** in the plot above. The result changes suddenly
    and begins to increase due to a change in neighborhood information around ***x
    = 0***. The function ***g***, analogous to a kernel that we‚Äôve studied in density
    estimation, was able to react to changes that happened in kernel‚Äôs area of influence.
  prefs: []
  type: TYPE_NORMAL
- en: In a discrete sense, the convolution operation is performed by the sliding the
    kernel function across the signal, multiplying the corresponding values of the
    signal and the kernel, and placing the sum of all these products in the resultant
    signal. In a mathematical sense, its good to think of *summation* with discrete
    signals, instead of *integration* on continuous signals.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/999a00da10ce33d206ec8bedbde4ff65.png)![](../Images/bc5768b181b6fe95e2797db01b18a2e2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Performing the convolution operation on 1D discrete signal. The expression
    above shows the mathematical formulation for the same. Image Source: [Convolution
    in one dimension for neural networks ‚Äî Brandon Rohrer](https://e2eml.school/convolution_one_d.html)
    (Creative Commons License) ‚Äî [CC0 1.0 Universal](https://creativecommons.org/publicdomain/zero/1.0/)'
  prefs: []
  type: TYPE_NORMAL
- en: For images, we‚Äôll slide a 2D kernel on the given image and the perform the same
    operation. The motion of the kernel will be 2D here, contrary to the 1D (unidirectional)
    motion of the kernel on a 1D signal. The output will be a matrix, as the convolution
    operation was also performed on a 2D input.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b498198a663e03fa42eec7cd3432e3b3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Convolution with a kernel matrix. Source: [Convolution ‚Äî Wikipedia](https://en.wikipedia.org/wiki/Convolution)
    (Creative Commons License) ‚Äî [CC BY-SA 3.0](https://creativecommons.org/licenses/by-sa/3.0/)'
  prefs: []
  type: TYPE_NORMAL
- en: We can use different kernels to extract various features from the input and
    or enhance the image for further operations. For instance, the *sharpen kernel*,
    would sharpen the edges present in an image. Many other kernels, on convolution,
    extract interesting features from an image,
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f7c405525d85632f7b67bb55f22fa576.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Performing convolutions with different kernels used in image processing. Source:
    Image by author'
  prefs: []
  type: TYPE_NORMAL
- en: Kernels in CNNs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The kernel that we just saw were constant but what if we can parameterize the
    kernel and control which features are being extracted? This would be helpful in
    convolutional neural networks, where we fine-tune kernels to minimize the overall
    loss incurred by the NN. The notion of non-parametric models built with kernels
    would diminish here, as CNNs can have a huge number of parameters, but the basic
    concept of *neighborhood information extraction* is still valid.
  prefs: []
  type: TYPE_NORMAL
- en: The function of a kernel here, is similar to that of the *sharpen* or Sobel
    X kernel, but it considers the values in the matrix as parameters and not fix
    numbers. These trainable kernels are optimized with backpropagation to reduce
    the value of the loss value in a CNN. A convolutional layer can have many such
    kernels collectively known as filters.
  prefs: []
  type: TYPE_NORMAL
- en: '[](/no-kernels-filters-are-not-the-same-b230ec192ac9?source=post_page-----f5d255d95785--------------------------------)
    [## No, Kernels & Filters Are Not The Same'
  prefs: []
  type: TYPE_NORMAL
- en: Solving the usual confusion.
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/no-kernels-filters-are-not-the-same-b230ec192ac9?source=post_page-----f5d255d95785--------------------------------)
    ![](../Images/34ad39d12d16988049914ce614cddd3b.png)
  prefs: []
  type: TYPE_NORMAL
- en: 'A typical convolutional neural network with max pooling and linear (dense)
    layers. The convolutional and max pooling layers extract features from the input
    image which are passed to the linear layers. Source: Image by Author'
  prefs: []
  type: TYPE_NORMAL
- en: The outputs produced by the first convolutional layers are passed to the next
    layer. This creates a hierarchical feature extraction process, where low level
    features of the image are extracted by the initial convolutional layers and the
    high-level features are tracked by the final/last convolutional layers. Such a
    stack of convolutions combined with trainable kernels provides CNNs the power
    to identify objects in images with a great precision, opening the doors of modern
    computer vision.
  prefs: []
  type: TYPE_NORMAL
- en: The End
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: I hope this journey through the world of kernels fascinated you towards the
    concept. Kernels are heavily confused amongst various topics, but their core idea
    remains the same, which we‚Äôve repeated several times in the blog as *neighborhood
    feature extraction*. Instead of using parameters to capture patterns within the
    data, kernel functions can encode relative proximity of samples to capture trends
    within the data. But, one must understand that parametric models have their own
    advantages and their use isn‚Äôt obsolete. Most NN models are huge parametric models
    with millions of parameters and they can solve complex problems such as object
    detection, image classification, speech synthesis and more.
  prefs: []
  type: TYPE_NORMAL
- en: All images, unless otherwise noted, are by the author.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
