- en: 'Kernels: Everything You Need to Know'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 内核：你需要了解的一切
- en: 原文：[https://towardsdatascience.com/kernels-everything-you-need-to-know-f5d255d95785?source=collection_archive---------0-----------------------#2023-03-06](https://towardsdatascience.com/kernels-everything-you-need-to-know-f5d255d95785?source=collection_archive---------0-----------------------#2023-03-06)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/kernels-everything-you-need-to-know-f5d255d95785?source=collection_archive---------0-----------------------#2023-03-06](https://towardsdatascience.com/kernels-everything-you-need-to-know-f5d255d95785?source=collection_archive---------0-----------------------#2023-03-06)
- en: 👨‍🏫 [Mathematics](https://equipintelligence.medium.com/list/mathematics-demystified-7e4d1c18041f)
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 👨‍🏫 [数学](https://equipintelligence.medium.com/list/mathematics-demystified-7e4d1c18041f)
- en: Density Estimation, Dot Products, Convolutions and everything…
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 密度估计、点积、卷积及其他一切……
- en: '[](https://equipintelligence.medium.com/?source=post_page-----f5d255d95785--------------------------------)[![Shubham
    Panchal](../Images/d48aecd8b1ed27ab68fc2e7ff6716606.png)](https://equipintelligence.medium.com/?source=post_page-----f5d255d95785--------------------------------)[](https://towardsdatascience.com/?source=post_page-----f5d255d95785--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----f5d255d95785--------------------------------)
    [Shubham Panchal](https://equipintelligence.medium.com/?source=post_page-----f5d255d95785--------------------------------)'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://equipintelligence.medium.com/?source=post_page-----f5d255d95785--------------------------------)[![Shubham
    Panchal](../Images/d48aecd8b1ed27ab68fc2e7ff6716606.png)](https://equipintelligence.medium.com/?source=post_page-----f5d255d95785--------------------------------)[](https://towardsdatascience.com/?source=post_page-----f5d255d95785--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----f5d255d95785--------------------------------)
    [Shubham Panchal](https://equipintelligence.medium.com/?source=post_page-----f5d255d95785--------------------------------)'
- en: ·
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fd45a9465f044&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fkernels-everything-you-need-to-know-f5d255d95785&user=Shubham+Panchal&userId=d45a9465f044&source=post_page-d45a9465f044----f5d255d95785---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----f5d255d95785--------------------------------)
    ·14 min read·Mar 6, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Ff5d255d95785&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fkernels-everything-you-need-to-know-f5d255d95785&user=Shubham+Panchal&userId=d45a9465f044&source=-----f5d255d95785---------------------clap_footer-----------)'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fd45a9465f044&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fkernels-everything-you-need-to-know-f5d255d95785&user=Shubham+Panchal&userId=d45a9465f044&source=post_page-d45a9465f044----f5d255d95785---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----f5d255d95785--------------------------------)
    ·14 min 阅读·2023年3月6日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Ff5d255d95785&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fkernels-everything-you-need-to-know-f5d255d95785&user=Shubham+Panchal&userId=d45a9465f044&source=-----f5d255d95785---------------------clap_footer-----------)'
- en: --
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff5d255d95785&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fkernels-everything-you-need-to-know-f5d255d95785&source=-----f5d255d95785---------------------bookmark_footer-----------)![](../Images/ab190b0872063f05abcb2425835c5803.png)'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff5d255d95785&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fkernels-everything-you-need-to-know-f5d255d95785&source=-----f5d255d95785---------------------bookmark_footer-----------)![](../Images/ab190b0872063f05abcb2425835c5803.png)'
- en: 'Source: Image by author'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 来源：作者提供的图片
- en: Kernels or kernel functions are beautiful mathematical concepts that are used
    in machine learning and statistics with different forms. If you’re a beginner
    you might feel tempted to know the exact definition of kernels, but you may get
    confused by multiple definitions of kernels that are explained across blogs/websites
    on the internet.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 内核或核函数是美丽的数学概念，用于机器学习和统计学中，形式各异。如果你是初学者，你可能会想知道内核的确切定义，但你可能会被互联网上各种博客/网站上解释的多种定义弄混。
- en: The kernel function is a confusing concept, just because the knowledge around
    it is decentralized across its applications and a common intuition connecting
    them is missing. This (huge) blog undertakes the motive of unifying all knowledge
    on kernels used in different ML applications. Just as most beginners, the kernel
    function kept me in a state of confusion for a long time, until I developed an
    intuition that would connect all links.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 核函数是一个令人困惑的概念，因为围绕它的知识在不同的应用中是分散的，缺乏一个将它们连接起来的常见直觉。这个（庞大的）博客旨在统一不同ML应用中使用的核函数的所有知识。就像大多数初学者一样，核函数让我困惑了很长时间，直到我培养出一种将所有环节连接起来的直觉。
- en: 'We begin our journey with non-parametric models, then we start discussing different
    types of kernels and their typical applications across statistics and ML. Similar
    to kernel functions, I have an attempt to explain PCA mathematically, considering
    all perspectives. You may have a read:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从非参数模型开始我们的旅程，然后开始讨论不同类型的核函数及其在统计和机器学习中的典型应用。类似于核函数，我也尝试从数学角度解释PCA，考虑所有的视角。你可以阅读一下：
- en: '[](/principal-component-analysis-everything-you-need-to-know-5f834c9eaa83?source=post_page-----f5d255d95785--------------------------------)
    [## Principal Component Analysis: Everything You Need To Know'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/principal-component-analysis-everything-you-need-to-know-5f834c9eaa83?source=post_page-----f5d255d95785--------------------------------)
    [## 主成分分析：你需要知道的一切'
- en: Covariance, eigenvalues, variance and everything …
  id: totrans-14
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 协方差、特征值、方差以及一切……
- en: towardsdatascience.com](/principal-component-analysis-everything-you-need-to-know-5f834c9eaa83?source=post_page-----f5d255d95785--------------------------------)
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/principal-component-analysis-everything-you-need-to-know-5f834c9eaa83?source=post_page-----f5d255d95785--------------------------------)
- en: Non-Parametric Models
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 非参数模型
- en: Non-parametric models are those statistical models that do not have parameters
    which grow with the size of the input. Note, non-parametric models do not mean
    ‘*model with zero parameters*’ but they work with a fixed set of parameters, also
    called hyperparameters, which do not increase with the increase in the dimensionality
    of the inputs. A vanilla linear regression model, has parameters ***θ*** that
    determine the *slope* of the hyperplane whose size depends on the dimensionality
    of the input ***x***,
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 非参数模型是那些统计模型，它们的参数不会随着输入规模的增长而增加。需要注意的是，非参数模型并不是指‘*零参数模型*’，而是指它们使用的是一组固定的参数，也称为超参数，这些参数不会随着输入维度的增加而增加。一个简单的线性回归模型有参数***θ***，这些参数决定了超平面的*斜率*，其大小取决于输入***x***的维度。
- en: Eqn. 1\. A simple linear regression model with tunable parameters whose size
    depends on the number of features **N**. The function **f** represents the expected
    value of the dependent variables **y** given inputs **x** i.e. **E[y|x]**
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 方程1\. 一个具有可调参数的简单线性回归模型，其参数大小取决于特征数**N**。函数**f**表示给定输入**x**的因变量**y**的期望值，即**E[y|x]**
- en: Next, consider the KNN model, where we determine the class of a test sample
    by analyzing the classes of its ***K*** nearest neighbors. If ***K = 1***, we
    assume that the test sample belongs to the same class as that of the nearest neighbor.
    This model does not have any parameters that would grow with the dimensionality
    of the inputs. For a vanilla implementation, we would only need a single parameter,
    ***K***, even if we working with large inputs (in terms of dimensionality).
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，考虑KNN模型，我们通过分析测试样本的***K***个最近邻的类别来确定测试样本的类别。如果***K = 1***，我们假设测试样本与最近邻属于同一类别。这个模型没有任何随着输入维度增加而增长的参数。对于一个简单的实现，即使在处理大规模输入（从维度角度看）时，我们也只需要一个单一的参数***K***。
- en: '![](../Images/b91bc02863d3d8166c0029592927d18a.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b91bc02863d3d8166c0029592927d18a.png)'
- en: '1\. A visual depiction of the working of K Nearest Neighbors model. The assumption
    made here is that the test sample belongs to same class as that of the K nearest
    neighbors. Image Source: Image by Author'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 1\. K最近邻模型工作原理的可视化描述。这里的假设是测试样本属于与K个最近邻相同的类别。图片来源：作者提供的图片
- en: KNN is a non-parametric model which has a hyperparameter ***K***, provided by
    the user. Non-parametric might seem to an *obvious-choice* at the first glance,
    as,
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: KNN是一个非参数模型，它有一个由用户提供的超参数***K***。非参数模型乍一看可能是一个*明显的选择*，因为，
- en: They make no prior assumptions regarding the distribution of the data. For instance,
    in case of *vanilla* linear regression, which is a parametric model, we assume
    that the conditional distribution of ***Y*** (dependent variables) given ***X***
    (features) is a Gaussian Distribution whose mean is a linear combination of the
    features (where weights are ***θ)*** and variance equal to ***σ²***
  id: totrans-23
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 它们不对数据的分布做任何先验假设。例如，在*普通*线性回归中，它是一个参数模型，我们假设***Y***（因变量）在给定***X***（特征）的条件下服从高斯分布，其均值是特征的线性组合（其中权重是***θ***）和方差等于***σ²***。
- en: 2\. The conditional distribution of **Y** given **X** and parameters ***θ***
    *is a normal distribution from which we wish to capture the mean from* ***X****.
    The constant variance is a result of the* [*assumption of* homoscedasticity](https://en.wikipedia.org/wiki/Homoscedasticity_and_heteroscedasticity)
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 2. 条件分布**Y**给定**X**和参数***θ*** *是一个正态分布，我们希望从***X***中捕捉均值*。常量方差是* [*同方差假设*](https://en.wikipedia.org/wiki/Homoscedasticity_and_heteroscedasticity)
    的结果。
- en: which might not hold always, because,
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能并不总是成立，因为，
- en: For each test sample, they need to keep the entire training data in memory,
    which is also true for the KNN model. For each sample, we need to calculate its
    distance from each training sample, so we need to retrieve/store each sample in
    memory, which might not be feasible to large datasets or even smaller datasets
    which a large number of features.
  id: totrans-26
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 对于每个测试样本，他们需要将整个训练数据保存在内存中，这对KNN模型也是适用的。对于每个样本，我们需要计算它与每个训练样本的距离，因此我们需要检索/存储每个样本在内存中，这对于大型数据集甚至特征数量众多的小型数据集可能不可行。
- en: The basic idea around non-parametric models is to gather some useful insight
    from the data and use to solve the given problem, without encoding information
    about the data in tunable parameters.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 非参数模型的基本思想是从数据中获取一些有用的见解，并用来解决给定的问题，而不通过可调参数对数据进行编码。
- en: Next, we focus on kernels, that have different use-cases across ML, and differ
    slightly with their meanings in each context. So far, after researching for this
    blog, and also from my previous attempts of understanding kernels as a whole,
    I feel that kernels are *machines* that provide information on the neighbors of
    a given datapoint (as an *input* to the machine). This *local* information i.e.
    the information on datapoints that lie in the proximity of the datapoint under
    consideration, is then used to solve the given problem. Once we use the kernel
    function on each of the datapoints, we can get a clear picture of *what’s going
    on in the locality of the data*.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们专注于核函数，它们在机器学习中有不同的使用场景，并且在每个上下文中稍有不同的含义。到目前为止，通过对这个博客的研究，以及我之前对核函数整体理解的尝试，我觉得核函数是提供给定数据点邻居信息的*机器*（作为*输入*提供给机器）。这些*局部*信息，即在考虑的数据点周围的邻近数据点的信息，然后用于解决给定的问题。一旦我们对每个数据点使用核函数，我们就能清楚地看到*数据局部的情况*。
- en: We’ll explore these three aspects of a kernel, which are three different concepts
    with their major applications in ML,
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将探讨核函数的这三个方面，它们是机器学习中具有主要应用的三个不同概念，
- en: '[Density Kernels](#d69a)'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[密度核函数](#d69a)'
- en: '[Mercer Kernels](#daa0)'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Mercer 核函数](#daa0)'
- en: '[Kernels in Image Processing](#ed44)'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[图像处理中的核函数](#ed44)'
- en: 'Density Kernels: Use of Kernels for Density Estimation'
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 密度核函数：用于密度估计的核函数
- en: Density kernels, kernel density estimation, kernel regression
  id: totrans-34
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 密度核函数、核密度估计、核回归
- en: We can use kernels to estimate the probability density for a given *test* sample,
    by modelling the underlying probability distribution with the *training* samples.
    The terms ‘test’ sample and ‘training’ samples simply refer to unobserved and
    observed samples respectively, considering ML lingo.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用核函数通过用*训练*样本建模基础概率分布来估计给定*测试*样本的概率密度。‘测试’样本和‘训练’样本分别指未观测和已观测的样本，考虑到机器学习的术语。
- en: For continuous random variable ***X***, we can integrate the probability density
    function of ***X*** within a suitable range, say, from ***x_1*** to ***x_2***,
    and we get the probability of ***X*** assuming a value in the range ***[ x_1 ,
    x_2 ]***. If you aren’t comfortable with the topic of probability density or random
    variables, here’s my 3-part series on probability distributions,
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 对于连续随机变量***X***，我们可以在适当的范围内积分***X***的概率密度函数，比如从***x_1***到***x_2***，从而得到***X***在范围***[
    x_1 , x_2 ]***内取值的概率。如果你对概率密度或随机变量的主题不太熟悉，这里是我关于概率分布的三部分系列，
- en: Density Kernels and Kernel Density Estimation (KDE)
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 密度核函数和核密度估计（KDE）
- en: Let us start our discussion with a problem. Dr. Panchal lives in a crowded city
    block which is surrounded by houses on all sides. The local police have hired
    a detective whose job is to determine the number of persons who reside in the
    doctor’s house or who are his family members, just to make sure things are going
    well. The detective could not ring the doctor’s bell and ask how many family members
    reside there, as it would warn the doctor if there’s something suspicious.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从一个问题开始讨论。Panchal博士住在一个拥挤的城市街区，四周都是房子。当地警方雇佣了一名侦探，其工作是确定博士家中居住的人数或其家属人数，以确保一切顺利。侦探不能按博士的门铃询问有多少家庭成员，因为这会警告博士如果有什么可疑的情况。
- en: The detective would start by interrogating the houses that are adjacent to Dr.
    Panchal’s house, as they could have a clear sight of what’s in there. The detective
    is expected to give a higher weight/importance to the information gained from
    these direct neighbors. Next, in order to gain more insights, the detective interrogates
    houses that are slightly farther away that do not have a direct sight of Dr. Panchal’s
    house but might have good information of their neighbor. The detective would give
    lesser importance to the information received from these neighbors, as their observations
    might not be that correct as that of the direct neighbors (whose houses are adjacent
    to Panchal’s). The detective performs several such rounds reducing the importance,
    moving away from Dr. Panchal’s house.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 侦探会从Panchal博士房子的相邻房子开始询问，因为这些房子可以清楚地看到里面的情况。预计侦探会给予这些直接邻居的信息更高的权重/重要性。接下来，为了获得更多见解，侦探会询问那些稍微远离但可能对邻居有良好信息的房子。对于这些邻居提供的信息，侦探会给予较低的重要性，因为这些邻居的观察可能不如直接邻居（即邻近Panchal博士的房子）的观察准确。侦探会进行几轮这样的询问，逐渐减少重要性，远离Panchal博士的房子。
- en: A density kernel does something similar to capture *neighboring* information
    around a given point. If we’re given a dataset ***D*** with ***N*** samples where
    each sample is a real number,
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 密度核函数类似于捕捉*邻近*点的信息。如果我们有一个数据集***D***，其中有***N***个样本，每个样本是一个实数，
- en: The kernel in the above snippet is an Epanechnikov (parabolic) kernel. The kernel
    function has some special properties here,
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码片段中的核函数是一个Epanechnikov（抛物线型）核函数。该核函数在这里具有一些特殊性质，
- en: '*Property 1*: The kernel function or the detective does not matter in which
    direction the x or some neighbor’s house lies. Information gained from two houses
    to the right or two houses to the left is the same.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*性质 1*：核函数或侦探不在意x或某些邻居的房子位于哪个方向。来自右边两栋房子或左边两栋房子的获得的信息是相同的。'
- en: '*Property 2*: The kernel function represents a valid PDF and it integrates
    to 1 over the entire real domain.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*性质 2*：核函数表示一个有效的概率密度函数（PDF），并且在整个实数域上积分为1。'
- en: '*Property 3*: Support of a kernel is the set of all values ***u*** such that
    ***K( u )*** is not equal to 0\. It represents the coverage area of the detective
    from where some non-zero importance will be given to information gathered. If
    the detective decides to interrogate in all houses in a radius of 5 km, the support
    would be all houses within that 5 km circle.'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*性质 3*：核的支持是所有值***u***的集合，使得***K(u)***不等于0。它表示侦探在从哪里收集信息时会给予一些非零的重要性。如果侦探决定在半径5公里内询问所有房屋，那么支持就是这个5公里圆圈内的所有房屋。'
- en: Every type of kernel will perform a similar task of encoding neighboring information,
    which each one will have a different strategy to do so. Unlike our detective,
    who slowly reduced the importance of interrogation as he moved away from Dr. Panchal’s
    house (a Gaussian kernel would do that), another detective might just continue
    giving equal importance to all interrogations, neglecting the distance upto a
    certain extent (a uniform kernel). Imagine, that from our dataset ***D***, the
    distribution of all ***x_i*** is,
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 每种类型的核函数都会执行类似的任务来编码邻近信息，每种核函数会有不同的策略来实现这一点。与我们的侦探不同，他在离开Panchal博士的房子时逐渐减少了对询问的重视（高斯核会这样做），另一个侦探可能会继续对所有询问赋予相同的重要性，忽略到一定程度的距离（均匀核）。想象一下，从我们的数据集***D***来看，所有***x_i***的分布是，
- en: '![](../Images/2d959ddb054dce433217e71275ced376.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2d959ddb054dce433217e71275ced376.png)'
- en: A Gaussian-like distribution whose probability density needs to be estimated
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 需要估计其概率密度的类似高斯分布
- en: Our goal is to create an estimate of the probability distribution of ***X***.
    We’ll do this by estimating the density at each sample ***x_i*** and using the
    kernel to collect neighboring information,
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的目标是创建对 ***X*** 的概率分布的估计。我们通过在每个样本 ***x_i*** 处估计密度并使用核来收集邻近信息来实现这一目标。
- en: '![](../Images/6a39f0c5d3d25de92a9fdfc4198e017e.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6a39f0c5d3d25de92a9fdfc4198e017e.png)'
- en: Estimated probability density with KDE
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 KDE 估计概率密度
- en: If ***x_i*** is away from ***x***, ***| x — x_i |*** will have a larger value
    thus yielding a very small value for ***K( x — x_i )*** and reducing the *say*
    of ***x_i*** in the determination of probability density at ***x***. The parameter
    ***h*** is the *smoothing parameter* called the *bandwidth* of the kernel. Greater
    the value of ***h***, smoother will be predicted probability density.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 ***x_i*** 离 ***x*** 较远，则 ***| x — x_i |*** 将有更大的值，从而使 ***K( x — x_i )***
    的值非常小，并减少 ***x_i*** 在确定 ***x*** 处的概率密度中的 *作用*。参数 ***h*** 是称为 *带宽* 的 *平滑参数*。***h***
    的值越大，预测的概率密度越平滑。
- en: '![](../Images/081bed27008b194d65d79d0ac17ba022.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/081bed27008b194d65d79d0ac17ba022.png)'
- en: Probability density estimations obtained by changing the bandwidth of the kernel.
    The bottommost figure depicts the data distribution and we obtain a smoother density
    estimation by increasing the bandwidth of the kernel. The kernel used for estimation
    here, was the **Epanechnikov/Parabolic** kernel.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 通过改变核的带宽获得的概率密度估计。最底部的图描绘了数据分布，通过增加核的带宽我们获得了更平滑的密度估计。这里用于估计的核是 **Epanechnikov/Parabolic**
    核。
- en: Kernel Regression
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 核回归
- en: Kernel regression is a non-parametric version of the linear regression model
    where we model the conditional expectation of the outcome variable. In the case
    of simple linear regression, we model the conditional expectation ***E[ Y | X
    ]*** directly by expressing it as a linear combination of the independent variables.
    This gives rise to a discriminative model, whereas kernel regression is a generative
    model, as we’re modelling the joint probability distribution ***P( X , Y )***
    and ***P( X )*** by the means of kernel density estimation.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 核回归是线性回归模型的非参数版本，我们在其中对结果变量的条件期望进行建模。在简单线性回归的情况下，我们通过将其表示为自变量的线性组合来直接建模条件期望
    ***E[ Y | X ]***。这产生了一个判别模型，而核回归是一个生成模型，因为我们通过核密度估计建模联合概率分布 ***P( X , Y )*** 和
    ***P( X )***。
- en: Deriving an estimator for **E[ Y | X ]** which does not have any additional
    parameters. In case of simple linear regression, **E[ Y | X ]** would be modelled
    as a weighed linear combination of the all **X_i**’s where the weights are the
    parameters.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 推导一个没有额外参数的 **E[ Y | X ]** 估计量。在简单线性回归的情况下，**E[ Y | X ]** 将被建模为所有 **X_i** 的加权线性组合，其中权重是参数。
- en: On observing the resultant expression, you’ll realize that the predicted outcome
    ***y_hat*** is a weighed combination of all ***y_i***, where the weights are determined
    by the values of the kernel function for all ***x_i***.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 观察结果表达式时，你会发现预测结果 ***y_hat*** 是所有 ***y_i*** 的加权组合，其中权重由所有 ***x_i*** 的核函数值决定。
- en: 'Mercer Kernels: Dot Products in Higher Dimensions'
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 梅瑟核：高维空间中的点积
- en: Mercer kernels and positive definiteness, use of Mercer kernels in SVM
  id: totrans-59
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 梅瑟核和正定性，梅瑟核在 SVM 中的应用
- en: Mercer Kernels or Positive definite kernels are functions that take in two inputs
    and produce a real number which characterizes the proximity of the two inputs
    (or their high-dimensional representations) in some other space. It turns out
    that these kernels are useful from the computational perspective as they help
    us compute dot products of vectors in a high-dimensional without explicitly performing
    any transformation to bring our own vectors into that high-dimensional space.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 梅瑟核或正定核是将两个输入作为输入并产生一个实数的函数，该实数表征了这两个输入（或它们的高维表示）在某个其他空间中的接近度。事实证明，这些核在计算角度上很有用，因为它们帮助我们在高维空间中计算向量的点积，而无需显式地进行任何变换将我们的向量带入该高维空间。
- en: Mercer Kernels
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 梅瑟核
- en: Let us start our discussion by defining the kernel function and some of its
    properties,
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过定义核函数及其一些属性来开始讨论，
- en: '![](../Images/c5183c93b4a4783dfae49df86d3b007e.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c5183c93b4a4783dfae49df86d3b007e.png)'
- en: The Mercer kernel is a function that takes in two datapoints from our dataset
    ***D***, and outputs a real number that represents the proximity of these two
    datapoints in the feature space.
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Mercer核是一个函数，它接受来自数据集***D***的两个数据点，并输出一个实数，表示这两个数据点在特征空间中的接近程度。
- en: If we have ***n*** datapoints in our dataset ***D***, and we run our Mercer
    kernel on each pair of datapoints and arrange the resulting outputs in a matrix,
    we achieve a positive definite matrix. This matrix, that depicts the similarity
    amongst the datapoints, is called the Gram Matrix.
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果我们在数据集***D***中有***n***个数据点，并对每对数据点应用Mercer核，并将结果输出整理成一个矩阵，我们会得到一个正定矩阵。这个矩阵描绘了数据点之间的相似性，称为Gram矩阵。
- en: '![](../Images/aefa15277ec3acde2e1b3eb8fe7fffb3.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/aefa15277ec3acde2e1b3eb8fe7fffb3.png)'
- en: The Gram Matrix
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: Gram矩阵
- en: Positive definite matrices are special, considering their spectral properties.
    They have positive eigenvalues and the corresponding eigenvectors form an orthonormal
    basis. We have a special property, for mercer kernels, using which the value of
    the kernel function as be expressed as the dot product of two transformed vectors,
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 正定矩阵因其谱属性而独特。它们具有正特征值，并且相应的特征向量形成一个正交规范基。对于Mercer核，我们有一个特殊的属性，可以使用该属性将核函数的值表示为两个转换向量的点积。
- en: '![](../Images/8731da29e52f913ef2df9a496489a9bb.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8731da29e52f913ef2df9a496489a9bb.png)'
- en: Each entry of the Gram matrix can be described as the dot product of two transformed
    samples.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: Gram矩阵的每个条目可以描述为两个转换样本之间的点积。
- en: One might have an urge to feel the intuition behind this statement, but it lives
    in the sanctum of Hilbert Spaces which deserves a separate blog. For time being,
    its good to understand that the value of the kernel function, with two input vectors,
    as be described as the dot product of some other two vectors that lie in higher
    dimensional.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 可能会有一种直觉上的冲动去理解这个陈述，但它存在于Hilbert空间的神圣领域，需要另写一篇博客。目前，了解核函数的值可以用高维空间中两个向量的点积来描述，这是很好的。
- en: Mercer kernels provide a shortcut for computing the dot product between those
    two high-dimensional vectors without explicitly computing those vectors. Hence,
    we can leverage the advantages of high-dimensional spaces are sometimes useful
    in machine learning, especially when samples are not linearly separable.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: Mercer核提供了一种计算这两个高维向量之间点积的快捷方式，而无需显式计算这些向量。因此，我们可以利用高维空间的优势，这在机器学习中有时是有用的，特别是当样本不线性可分时。
- en: '![](../Images/1ddba6803e3cdca1250a355e0e379d94.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1ddba6803e3cdca1250a355e0e379d94.png)'
- en: 'Samples which are linearly inseparable in lower dimensions may find an optimal
    hyperplane in higher dimensions. Source: Image by author'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在低维空间中线性不可分的样本可能会在高维空间中找到一个最优的超平面。来源：作者图片
- en: For some optimization problems, like the one encountered while optimizing SVMs,
    we’ll need to compute dot products between two transformed samples which are two
    high-dimensional vectors. The use of kernel functions will help us compute this
    dot product easily without performing any explicit transformations on the samples.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 对于一些优化问题，如优化SVM时遇到的问题，我们需要计算两个高维向量之间的点积。使用核函数可以帮助我们轻松计算这个点积，而不需要对样本进行任何显式的转换。
- en: Use of Mercer Kernels in SVMs
  id: totrans-76
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Mercer核在SVM中的应用
- en: SVMs are linear classifiers that fit a hyperplane such that a decision boundary
    is formed between samples of two classes. To determine the best hyperplane i.e.
    the hyperplane which divides the samples into two classes and maximizes the ‘margin’,
    we need to solve an optimization problem that contains an objective function (a
    function which is either maximized or minimized) given some constraints on the
    parameters of the objective.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: SVM是线性分类器，它通过拟合一个超平面，使得在两个类别的样本之间形成一个决策边界。为了确定最佳的超平面，即将样本划分为两个类别并最大化“边际”的超平面，我们需要解决一个包含目标函数（一个可以最大化或最小化的函数）以及一些约束条件的优化问题。
- en: The derivation of the SVM optimization problem is covered in these blogs extensively
    by [Saptashwa Bhattacharyya](https://medium.com/u/9a3c3c477239?source=post_page-----f5d255d95785--------------------------------)
    so reading them once would help us proceed further,
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: SVM优化问题的推导在这些博客中得到了广泛的讨论，[Saptashwa Bhattacharyya](https://medium.com/u/9a3c3c477239?source=post_page-----f5d255d95785--------------------------------)的博客可以帮助我们进一步了解，
- en: '[](/understanding-support-vector-machine-part-2-kernel-trick-mercers-theorem-e1e6848c6c4d?source=post_page-----f5d255d95785--------------------------------)
    [## Understanding Support Vector Machine: Part 2: Kernel Trick; Mercer’s Theorem'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/understanding-support-vector-machine-part-2-kernel-trick-mercers-theorem-e1e6848c6c4d?source=post_page-----f5d255d95785--------------------------------)
    [## 理解支持向量机：第二部分：核技巧；Mercer定理'
- en: Why Kernel ?
  id: totrans-80
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 为什么使用核函数？
- en: towardsdatascience.com](/understanding-support-vector-machine-part-2-kernel-trick-mercers-theorem-e1e6848c6c4d?source=post_page-----f5d255d95785--------------------------------)
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/understanding-support-vector-machine-part-2-kernel-trick-mercers-theorem-e1e6848c6c4d?source=post_page-----f5d255d95785--------------------------------)
- en: The vectors w and b characterize the hyperplane which forms the decision boundary.
    The margin/width between the support vectors is given in the first expression
    below. Also, we would match the predictions made by the SVM and the target labels,
    or more precisely, the sign of ***w.xi + b*** and ***yi***,
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 向量***w***和***b***描述了形成决策边界的超平面。支持向量之间的边际/宽度由下述第一个表达式给出。此外，我们会匹配SVM做出的预测与目标标签，更确切地说，就是***w.xi
    + b***和***yi***的符号。
- en: '![](../Images/60d8e8aac232d17c4d89d8f58af3474e.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/60d8e8aac232d17c4d89d8f58af3474e.png)'
- en: The optimization problem that we have to solve for an optimal decision boundary
    is,
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 我们必须解决的优化问题是：
- en: '![](../Images/66f7e846215745c4aefb5e7a7183d3db.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/66f7e846215745c4aefb5e7a7183d3db.png)'
- en: SVM optimization problem
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: SVM优化问题
- en: We solve this optimization problem with Lagrangian multipliers, so the first
    step would be build a Lagrangian and equating the partial derivatives w.r.t. its
    parameters to zero. This would yield an expression for ***w*** which minimizes
    the Lagrangian.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过拉格朗日乘数来解决这个优化问题，因此第一步是构建一个拉格朗日函数，并将其参数的偏导数设置为零。这将得到一个***w***的表达式，能够最小化拉格朗日函数。
- en: '![](../Images/4b5967a4fb4ffb82a83d3767d073c912.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4b5967a4fb4ffb82a83d3767d073c912.png)'
- en: Setting the partial derivations of the Lagrangian to zero.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 将拉格朗日函数的偏导数设置为零。
- en: After substituting these results into the Lagrangian, we obtain an expression
    which clearly depicts the role of kernel functions,
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 将这些结果代入拉格朗日函数后，我们得到一个清晰描述核函数作用的表达式。
- en: '![](../Images/2901e07fc0dc2663ff6520d723cac2d4.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2901e07fc0dc2663ff6520d723cac2d4.png)'
- en: In order to achieve the optimal hyperplane, we need to compute the dot product
    between pairs of samples from our dataset. In some cases, finding an optimal hyperplane
    isn’t possible, as the samples may not be linearly separable i.e. the samples
    couldn’t be divided into two classes by merely drawing a line/plane. We can increase
    the dimensionality of the samples, by which we can discover a separating hyperplane
    easily.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现最佳超平面，我们需要计算数据集中样本对之间的点积。在某些情况下，找到一个最佳超平面是不可行的，因为样本可能不是线性可分的，即样本不能仅通过绘制一条线/平面来分成两个类别。我们可以通过增加样本的维度来发现一个分离超平面。
- en: Consider a feature map ***ϕ***, that transform the data sample ***x*** into
    a higher dimensional feature i.e. ***ϕ( x )***. In the Lagrangian of the SVM,
    if we use these features instead of the data samples, we need to compu
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一个特征映射***ϕ***，它将数据样本***x***转换为更高维度的特征，即***ϕ(x)***。在SVM的拉格朗日函数中，如果我们使用这些特征代替数据样本，我们需要计算
- en: '![](../Images/9cb279bd2433213c53e0db20423a23c9.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9cb279bd2433213c53e0db20423a23c9.png)'
- en: We can replace the dot product of the features with a kernel function that takes
    into two data samples (not transformed features),
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以用一个核函数来替代特征的点积，该核函数对两个数据样本进行操作（而不是转换后的特征）。
- en: '![](../Images/c753232a3736088926414bcb63d16d79.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c753232a3736088926414bcb63d16d79.png)'
- en: This technique is popularly known as the *kernel trick* and is a direct consequence
    of Mercer’s theorem. We’re able to calculate the dot product of two high-dimensional
    features without explicitly transforming the data samples to that high-dimensional
    space. With more dimensions, we have greater degrees of freedom for determining
    the optimal hyperplane. By choosing different kernels, the dimensionality of the
    space in which are features lie, can be controlled.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 这种技术被广泛称为*核技巧*，是Mercer定理的直接结果。我们能够计算两个高维特征的点积，而无需显式地将数据样本转换到那个高维空间。随着维度的增加，我们在确定最佳超平面时具有更大的自由度。通过选择不同的核函数，可以控制特征所在空间的维度。
- en: The kernel function has a simpler expression to be evaluated, like the ones
    listed below,
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 核函数具有更简单的表达式，如下所示，
- en: '![](../Images/a7fd6c029df8659a79c7cc4d858bfcbc.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a7fd6c029df8659a79c7cc4d858bfcbc.png)'
- en: 'Kernels For Convolutions: Image Processing'
  id: totrans-100
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '卷积的核: 图像处理'
- en: Kernels used in convolutions and image processing
  id: totrans-101
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 用于卷积和图像处理的核
- en: Kernels are fixed-size matrices that are convolved across an image or a feature
    map to extract useful information out of it. In image processing lingo, a kernel
    matrix is also called a convolution matrix, and it used to perform operations
    on an image. Each kernel has a specialized operation of its own, which transforms
    the image after the convolution.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 核是固定大小的矩阵，用于在图像或特征图上卷积，以提取有用的信息。在图像处理中，核矩阵也称为卷积矩阵，用于对图像进行操作。每个核都有自己特定的操作，这在卷积后会改变图像。
- en: Convolutions and Kernels
  id: totrans-103
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 卷积与核
- en: Convolution is a mathematical operator that takes in two functions and produces
    another function. If we’re convoluting two functions or signals, the result of
    the convolution is a function that represents the area of overlap between the
    two functions. Mathematically, the convolution operation is defined as,
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积是一个数学算子，它接受两个函数并生成另一个函数。如果我们对两个函数或信号进行卷积，那么卷积的结果是一个表示两个函数之间重叠区域的函数。从数学上讲，卷积操作被定义为，
- en: '![](../Images/b51c62a4780e4dde25ed85c8a8aa0dca.png)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b51c62a4780e4dde25ed85c8a8aa0dca.png)'
- en: 'An animated plot that depicts the convolution operation, along with its mathematical
    definition. This is analogous to the ‘sliding of kernel’ performed in density
    estimation. We slide the kernel over the data distribution, collecting ‘neighborhood
    information’ and then estimate the density at a specific point. Source: [Wikipedia
    — Convolution (Wikimedia Commons)](https://en.wikipedia.org/wiki/Convolution)
    — [CC BY-SA 3.0](https://creativecommons.org/licenses/by-sa/3.0/)'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '描述卷积操作的动画图，以及它的数学定义。这类似于在密度估计中进行的“核滑动”。我们将核滑过数据分布，收集“邻域信息”，然后在特定点估计密度。来源: [维基百科
    — 卷积 (维基共享资源)](https://en.wikipedia.org/wiki/Convolution) — [CC BY-SA 3.0](https://creativecommons.org/licenses/by-sa/3.0/)'
- en: Notice the result of the convolution operation when function ***g*** passes
    through the wall placed at ***x = 0*** in the plot above. The result changes suddenly
    and begins to increase due to a change in neighborhood information around ***x
    = 0***. The function ***g***, analogous to a kernel that we’ve studied in density
    estimation, was able to react to changes that happened in kernel’s area of influence.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 注意函数***g***在图上经过***x = 0***处墙壁时的卷积结果。结果突然变化并开始增加，这是由于***x = 0***周围邻域信息的变化。函数***g***，类似于我们在密度估计中研究的核，能够对核影响区域发生的变化做出反应。
- en: In a discrete sense, the convolution operation is performed by the sliding the
    kernel function across the signal, multiplying the corresponding values of the
    signal and the kernel, and placing the sum of all these products in the resultant
    signal. In a mathematical sense, its good to think of *summation* with discrete
    signals, instead of *integration* on continuous signals.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 从离散的角度来看，卷积操作是通过将核函数滑动到信号上，乘以信号和核的对应值，然后将所有这些乘积的总和放置到结果信号中来完成的。在数学意义上，考虑到离散信号的*求和*要比考虑连续信号上的*积分*更为合适。
- en: '![](../Images/999a00da10ce33d206ec8bedbde4ff65.png)![](../Images/bc5768b181b6fe95e2797db01b18a2e2.png)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/999a00da10ce33d206ec8bedbde4ff65.png)![](../Images/bc5768b181b6fe95e2797db01b18a2e2.png)'
- en: 'Performing the convolution operation on 1D discrete signal. The expression
    above shows the mathematical formulation for the same. Image Source: [Convolution
    in one dimension for neural networks — Brandon Rohrer](https://e2eml.school/convolution_one_d.html)
    (Creative Commons License) — [CC0 1.0 Universal](https://creativecommons.org/publicdomain/zero/1.0/)'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '对一维离散信号执行卷积操作。上面的表达式显示了相同的数学公式。图像来源: [神经网络中的一维卷积 — Brandon Rohrer](https://e2eml.school/convolution_one_d.html)（创作共用许可证）
    — [CC0 1.0 Universal](https://creativecommons.org/publicdomain/zero/1.0/)'
- en: For images, we’ll slide a 2D kernel on the given image and the perform the same
    operation. The motion of the kernel will be 2D here, contrary to the 1D (unidirectional)
    motion of the kernel on a 1D signal. The output will be a matrix, as the convolution
    operation was also performed on a 2D input.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 对于图像，我们将一个二维核滑动到给定的图像上并执行相同的操作。这里，核的运动将是二维的，与在一维信号上进行的一维（单向）核运动相对。输出将是一个矩阵，因为卷积操作也是在二维输入上执行的。
- en: '![](../Images/b498198a663e03fa42eec7cd3432e3b3.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b498198a663e03fa42eec7cd3432e3b3.png)'
- en: 'Convolution with a kernel matrix. Source: [Convolution — Wikipedia](https://en.wikipedia.org/wiki/Convolution)
    (Creative Commons License) — [CC BY-SA 3.0](https://creativecommons.org/licenses/by-sa/3.0/)'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 使用内核矩阵进行卷积。来源：[卷积 — 维基百科](https://en.wikipedia.org/wiki/Convolution)（创作共用许可证）—
    [CC BY-SA 3.0](https://creativecommons.org/licenses/by-sa/3.0/)
- en: We can use different kernels to extract various features from the input and
    or enhance the image for further operations. For instance, the *sharpen kernel*,
    would sharpen the edges present in an image. Many other kernels, on convolution,
    extract interesting features from an image,
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用不同的内核从输入中提取各种特征或增强图像以进行进一步操作。例如，*锐化内核*会锐化图像中的边缘。许多其他内核在卷积过程中从图像中提取有趣的特征。
- en: '![](../Images/f7c405525d85632f7b67bb55f22fa576.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f7c405525d85632f7b67bb55f22fa576.png)'
- en: 'Performing convolutions with different kernels used in image processing. Source:
    Image by author'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 使用不同的内核在图像处理中执行卷积。来源：作者提供的图片
- en: Kernels in CNNs
  id: totrans-117
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: CNN中的内核
- en: The kernel that we just saw were constant but what if we can parameterize the
    kernel and control which features are being extracted? This would be helpful in
    convolutional neural networks, where we fine-tune kernels to minimize the overall
    loss incurred by the NN. The notion of non-parametric models built with kernels
    would diminish here, as CNNs can have a huge number of parameters, but the basic
    concept of *neighborhood information extraction* is still valid.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚刚看到的内核是常量，但如果我们可以对内核进行参数化并控制提取哪些特征呢？这在卷积神经网络中会很有用，因为我们微调内核以最小化NN产生的整体损失。基于内核的非参数模型的概念在这里会减少，因为CNN可以拥有大量参数，但*邻域信息提取*的基本概念仍然有效。
- en: The function of a kernel here, is similar to that of the *sharpen* or Sobel
    X kernel, but it considers the values in the matrix as parameters and not fix
    numbers. These trainable kernels are optimized with backpropagation to reduce
    the value of the loss value in a CNN. A convolutional layer can have many such
    kernels collectively known as filters.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的内核功能类似于*锐化*或Sobel X内核，但它将矩阵中的值视为参数，而不是固定的数字。这些可训练的内核通过反向传播进行优化，以减少CNN中的损失值。一个卷积层可以有许多这样的内核，统称为滤波器。
- en: '[](/no-kernels-filters-are-not-the-same-b230ec192ac9?source=post_page-----f5d255d95785--------------------------------)
    [## No, Kernels & Filters Are Not The Same'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/no-kernels-filters-are-not-the-same-b230ec192ac9?source=post_page-----f5d255d95785--------------------------------)
    [## 不，内核和滤波器并不相同'
- en: Solving the usual confusion.
  id: totrans-121
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 解决常见的混淆问题。
- en: towardsdatascience.com](/no-kernels-filters-are-not-the-same-b230ec192ac9?source=post_page-----f5d255d95785--------------------------------)
    ![](../Images/34ad39d12d16988049914ce614cddd3b.png)
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/no-kernels-filters-are-not-the-same-b230ec192ac9?source=post_page-----f5d255d95785--------------------------------)
    ![](../Images/34ad39d12d16988049914ce614cddd3b.png)
- en: 'A typical convolutional neural network with max pooling and linear (dense)
    layers. The convolutional and max pooling layers extract features from the input
    image which are passed to the linear layers. Source: Image by Author'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 一个典型的卷积神经网络，包括最大池化和线性（全连接）层。卷积层和最大池化层从输入图像中提取特征，然后将这些特征传递给线性层。来源：作者提供的图片
- en: The outputs produced by the first convolutional layers are passed to the next
    layer. This creates a hierarchical feature extraction process, where low level
    features of the image are extracted by the initial convolutional layers and the
    high-level features are tracked by the final/last convolutional layers. Such a
    stack of convolutions combined with trainable kernels provides CNNs the power
    to identify objects in images with a great precision, opening the doors of modern
    computer vision.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个卷积层产生的输出传递到下一个层。这创建了一个分层特征提取过程，其中图像的低级特征由初始卷积层提取，高级特征则由最后的卷积层跟踪。这样的卷积堆栈与可训练的内核结合，使CNN能够以极高的精度识别图像中的物体，为现代计算机视觉开辟了新的领域。
- en: The End
  id: totrans-125
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结束
- en: I hope this journey through the world of kernels fascinated you towards the
    concept. Kernels are heavily confused amongst various topics, but their core idea
    remains the same, which we’ve repeated several times in the blog as *neighborhood
    feature extraction*. Instead of using parameters to capture patterns within the
    data, kernel functions can encode relative proximity of samples to capture trends
    within the data. But, one must understand that parametric models have their own
    advantages and their use isn’t obsolete. Most NN models are huge parametric models
    with millions of parameters and they can solve complex problems such as object
    detection, image classification, speech synthesis and more.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 我希望这段关于内核世界的旅程能让你对这一概念感到着迷。内核在各种话题中经常引起混淆，但其核心思想始终如一，我们在博客中多次提到的就是*邻域特征提取*。与其使用参数捕捉数据中的模式，内核函数可以编码样本的相对接近度，从而捕捉数据中的趋势。然而，必须理解参数模型有其自身的优势，它们的使用并未过时。大多数神经网络模型是庞大的参数模型，具有数百万个参数，可以解决复杂的问题，如物体检测、图像分类、语音合成等。
- en: All images, unless otherwise noted, are by the author.
  id: totrans-127
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 除非另有说明，所有图像均由作者提供。
