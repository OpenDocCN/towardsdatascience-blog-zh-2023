["```py\nimport pandas as pd\nimport numpy as np\n\n# load the dataset into a dataframe\ndf = pd.read_csv('/content/data.tsv', sep='\\t')\n\n# see the distribution of the sentiment classes\ndf['sentiment'].value_counts()\n\n# -------\n# 0    337\n# 1    322\n# Name: sentiment, dtype: int64\n```", "```py\n!pip install emoji\nimport emoji\n\nclt = []\nfor comm in df['comment'].to_numpy():\n  clt.append(emoji.replace_emoji(comm, replace=\"\"))\n\ndf['comment'] = clt\ndf.head()\n```", "```py\nimport cyrtranslit\nlatin = []\ncyrillic = []\nfor comm in df['comment'].to_numpy():\n  latin.append(cyrtranslit.to_latin(comm, \"mk\"))\n  cyrillic.append(cyrtranslit.to_cyrillic(comm, \"mk\"))\n\ndf['comment_cyrillic'] = cyrillic\ndf['comment_latin'] = latin\ndf.head()\n```", "```py\n!pip install laserembeddings\n!python -m laserembeddings download-models\n\nfrom laserembeddings import Laser\n\n# create the embeddings\nlaser = Laser()\nembeddings_c = laser.embed_sentences(df['comment_cyrillic'].to_numpy(),lang='mk')\nembeddings_l = laser.embed_sentences(df['comment_latin'].to_numpy(),lang='mk')\n\n# save the embeddings\nnp.save('/content/laser_multi_c.npy', embeddings_c)\nnp.save('/content/laser_multi_l.npy', embeddings_l)\n```", "```py\n!pip install tensorflow_text\nimport tensorflow as tf\nimport tensorflow_hub as hub\nimport numpy as np\nimport tensorflow_text\n\n# load the MUSE module\nmodule_url = \"https://tfhub.dev/google/universal-sentence-encoder-multilingual-large/3\"\nembed = hub.load(module_url)\n\nsentences = df['comment_cyrillic'].to_numpy()\nmuse_c = embed(sentences)\nmuse_c = np.array(muse_c)\n\nsentences = df['comment_latin'].to_numpy()\nmuse_l = embed(sentences)\nmuse_l = np.array(muse_l)\n\nnp.save('/content/muse_c.npy', muse_c)\nnp.save('/content/muse_l.npy', muse_l)\n```", "```py\n!pip install openai\n\nimport openai\nopenai.api_key = 'YOUR_KEY_HERE'\n\nembeds_c = openai.Embedding.create(input = df['comment_cyrillic'].to_numpy().tolist(), model='text-embedding-ada-002')['data']\nembeds_l = openai.Embedding.create(input = df['comment_latin'].to_numpy().tolist(), model='text-embedding-ada-002')['data']\n\nfull_arr_c = []\nfor e in embeds_c:\n  full_arr_c.append(e['embedding'])\nfull_arr_c = np.array(full_arr_c)\n\nfull_arr_l = []\nfor e in embeds_l:\n  full_arr_l.append(e['embedding'])\nfull_arr_l = np.array(full_arr_l)\n\nnp.save('/content/openai_ada_c.npy', full_arr_c)\nnp.save('/content/openai_ada_l.npy', full_arr_l)\n```", "```py\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(embeddings_c, df['sentiment'], test_size=0.2, random_state=42)\n```", "```py\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report, confusion_matrix\n\nrfc = RandomForestClassifier(n_estimators=100)\nrfc.fit(X_train, y_train)\nprint(classification_report(y_test,rfc.predict(X_test)))\nprint(confusion_matrix(y_test,rfc.predict(X_test)))\n```", "```py\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import classification_report, confusion_matrix\n\nrfc = XGBClassifier(max_depth=15)\nrfc.fit(X_train, y_train)\nprint(classification_report(y_test,rfc.predict(X_test)))\nprint(confusion_matrix(y_test,rfc.predict(X_test)))\n```", "```py\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import classification_report, confusion_matrix\n\nrfc = SVC()\nrfc.fit(X_train, y_train)\nprint(classification_report(y_test,rfc.predict(X_test)))\nprint(confusion_matrix(y_test,rfc.predict(X_test)))\n```", "```py\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, confusion_matrix\n\nmodel = keras.Sequential()\nmodel.add(keras.layers.Dense(256, activation='relu', input_shape=(1024,)))\nmodel.add(keras.layers.Dropout(0.2))\nmodel.add(keras.layers.Dense(128, activation='relu'))\nmodel.add(keras.layers.Dense(1, activation='sigmoid'))\n\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n\nhistory = model.fit(X_train, y_train, epochs=11, validation_data=(X_test, y_test))\n\ntest_loss, test_acc = model.evaluate(X_test, y_test)\nprint('Test accuracy:', test_acc)\ny_pred = model.predict(X_test)\n\nprint(classification_report(y_test,y_pred.round()))\nprint(confusion_matrix(y_test,y_pred.round()))\n```", "```py\nimport matplotlib.pyplot as plt\n\ndef plot_accuracy(history):\n    plt.plot(history.history['accuracy'])\n    plt.plot(history.history['val_accuracy'])\n    plt.title('Model Accuracy')\n    plt.xlabel('Epoch')\n    plt.ylabel('Accuracy')\n    plt.legend(['Train', 'Validation'], loc='upper left')\n    plt.show()\n```", "```py\nfrom sklearn.model_selection import train_test_split\nfrom datasets import load_dataset\nfrom transformers import TrainingArguments, Trainer\nfrom sklearn.metrics import classification_report, confusion_matrix\n\n# create csv of train and test sets to be loaded by the dataset\ndf.rename(columns={\"sentiment\": \"label\"}, inplace=True)\ntrain, test = train_test_split(df, test_size=0.2)\npd.DataFrame(train).to_csv('train.csv',index=False)\npd.DataFrame(test).to_csv('test.csv',index=False)\n\n# load the dataset\ndataset = load_dataset(\"csv\", data_files={\"train\": \"train.csv\", \"test\": \"test.csv\"})\n\n# tokenize the text\ntokenizer = AutoTokenizer.from_pretrained('bert-base-multilingual-uncased')\nencoded_dataset = dataset.map(lambda t: tokenizer(t['comment_cyrillic'],  truncation=True), batched=True,load_from_cache_file=False)\n\n# load the pretrained model\nmodel = AutoModelForSequenceClassification.from_pretrained('bert-base-multilingual-uncased',num_labels =2)\n\n# fine-tune the model\narg = TrainingArguments(\n    \"mbert-sentiment-mk\",\n    learning_rate=5e-5,\n    num_train_epochs=5,\n    per_device_eval_batch_size=8,\n    per_device_train_batch_size=8,\n    seed=42,\n    push_to_hub=True\n)\ntrainer = Trainer(\n    model=model,\n    args=arg,\n    tokenizer=tokenizer,\n    train_dataset=encoded_dataset['train'],\n    eval_dataset=encoded_dataset['test']\n)\ntrainer.train()\n\n# get predictions\npredictions = trainer.predict(encoded_dataset[\"test\"])\npreds = np.argmax(predictions.predictions, axis=-1)\n\n# evaluate\nprint(classification_report(predictions.label_ids,preds))\nprint(confusion_matrix(predictions.label_ids,preds))\n```"]