- en: A Comprehensive Overview of Gaussian Splatting
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/a-comprehensive-overview-of-gaussian-splatting-e7d570081362?source=collection_archive---------0-----------------------#2023-12-23](https://towardsdatascience.com/a-comprehensive-overview-of-gaussian-splatting-e7d570081362?source=collection_archive---------0-----------------------#2023-12-23)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Everything you need to know about the new trend in the field of 3D representations
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://yurkovak.medium.com/?source=post_page-----e7d570081362--------------------------------)[![Kate
    Yurkova](../Images/c29a9d59d1b8227d189b12a8adb5bbfa.png)](https://yurkovak.medium.com/?source=post_page-----e7d570081362--------------------------------)[](https://towardsdatascience.com/?source=post_page-----e7d570081362--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----e7d570081362--------------------------------)
    [Kate Yurkova](https://yurkovak.medium.com/?source=post_page-----e7d570081362--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F16ecfab4b128&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-comprehensive-overview-of-gaussian-splatting-e7d570081362&user=Kate+Yurkova&userId=16ecfab4b128&source=post_page-16ecfab4b128----e7d570081362---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----e7d570081362--------------------------------)
    ·12 min read·Dec 23, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fe7d570081362&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-comprehensive-overview-of-gaussian-splatting-e7d570081362&user=Kate+Yurkova&userId=16ecfab4b128&source=-----e7d570081362---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fe7d570081362&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-comprehensive-overview-of-gaussian-splatting-e7d570081362&source=-----e7d570081362---------------------bookmark_footer-----------)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: Gaussian splatting is a method for representing 3D scenes and rendering novel
    views introduced in “3D Gaussian Splatting for Real-Time Radiance Field Rendering”¹.
    **It can be thought of as an alternative to NeRF²-like models**, and just like
    NeRF back in the day, Gaussian splatting led to [lots of new research works](https://github.com/MrNeRF/awesome-3D-gaussian-splatting)
    that chose to use it as an underlying representation of a 3D world for various
    use cases. So what’s so special about it and why is it better than NeRF? Or is
    it, even? Let’s find out!
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: 'Table of contents:'
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[TL;DR](#fbb7)'
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Representing a 3D world](#4012)'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Image formation model & rendering](#9bb2)'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Optimization](#6c97)'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[View-dependant colors with SH](#4cd8)'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[视图依赖颜色与SH](#4cd8)'
- en: '[Limitations](#3c15)'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[限制](#3c15)'
- en: '[Where to play with it](#1a92)'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[在哪里玩](#1a92)'
- en: TL;DR
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: TL;DR
- en: First and foremost, the main claim to fame of this work was **the high rendering
    speed** as can be understood from the title. This is due to the representation
    itself which will be covered below and thanks to the tailored implementation of
    a rendering algorithm with custom CUDA kernels.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，这项工作的主要亮点是**高渲染速度**，如标题所示。这归功于表示本身（将在下文中介绍）以及定制的CUDA内核渲染算法的实现。
- en: '![](../Images/448d1c345960722b064124d7883df986.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/448d1c345960722b064124d7883df986.png)'
- en: '**Figure 1:** A side-by-side comparison of previous high-quality representations
    and Gaussian Splatting (marked as “Ours”) in terms of rendering speed (fps), training
    time (min), and visual quality (Peak signal-to-noise ratio, the higher the better)
    [Source: taken from [1]]'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '**图1：** 以渲染速度（fps）、训练时间（min）和视觉质量（峰值信噪比，越高越好）对比之前的高质量表现和高斯溅射（标记为“我们的”）[来源：取自[1]]'
- en: Additionally, Gaussian splatting **doesn’t involve any neural network at all**.
    There isn’t even a small MLP, nothing “neural”, a scene is essentially just a
    set of points in space. This in itself is already an attention grabber. It is
    quite refreshing to see such a method gaining popularity in our AI-obsessed world
    with research companies chasing models comprised of more and more billions of
    parameters. Its idea stems from “Surface splatting”³ (2001) so it sets a cool
    example that classic computer vision approaches can still inspire relevant solutions.
    Its simple and explicit representation makes Gaussian splatting particularly **interpretable**,
    a very good reason to choose it over NeRFs for some applications.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，高斯溅射**完全不涉及任何神经网络**。甚至没有一个小的MLP，完全没有“神经”成分，场景本质上只是空间中的一组点。这本身已经非常吸引注意力。在我们对AI痴迷的世界中，看到这样的方法逐渐流行，与那些追逐包含更多数十亿参数的模型的研究公司相比，颇具清新感。它的理念来源于“表面溅射”³（2001），因此它为经典计算机视觉方法仍然可以激发相关解决方案树立了一个很好的例子。其简单而明确的表示使得高斯溅射特别**可解释**，这是在一些应用中选择它而不是NeRFs的一个非常好的理由。
- en: Representing a 3D world
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 代表一个3D世界
- en: As mentioned earlier, in Gaussian splatting a 3D world is represented with a
    set of 3D points, in fact, millions of them, in a ballpark of 0.5–5 million. Each
    point is a 3D Gaussian with its own **unique parameters that are fitted per scene**
    such that renders of this scene match closely to the known dataset images. The
    optimization and rendering processes will be discussed later so let’s focus for
    a moment on the necessary parameters.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，在高斯溅射中，一个3D世界是由一组3D点表示的，实际上是数百万个，数量在0.5-5百万的范围内。每个点是一个3D高斯，其**唯一的参数针对每个场景进行拟合**，使得该场景的渲染与已知数据集图像非常匹配。优化和渲染过程将在后面讨论，因此现在我们暂时关注必要的参数。
- en: '![](../Images/06cdb742a5dc51d20f3b47bbf79b848d.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/06cdb742a5dc51d20f3b47bbf79b848d.png)'
- en: '**Figure 2:** Centers of Gaussian (means) [Source: taken from Dynamic 3D Gaussians⁴]'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '**图2：** 高斯中心（均值）[来源：取自动态3D高斯⁴]'
- en: '**Each 3D Gaussian is parametrized by:**'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '**每个3D高斯由以下参数化：**'
- en: Mean **μ** interpretable as location x, y, z;
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 均值**μ**可以解释为位置x, y, z；
- en: Covariance **Σ**;
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 协方差**Σ**；
- en: Opacity **σ(𝛼)**, a sigmoid function is applied to map the parameter to the
    [0, 1] interval;
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不透明度**σ(𝛼)**，应用了一个 sigmoid 函数来将参数映射到[0, 1]区间；
- en: '**Color parameters**, either 3 values for (R, G, B) or spherical harmonics
    (SH) coefficients.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**颜色参数**，可以是（R, G, B）的3个值或球面谐波（SH）系数。'
- en: 'Two groups of parameters here need further discussion, a covariance matrix
    and SH. There is a separate section dedicated to the latter. As for the covariance,
    it is chosen to be anisotropic by design, that is, not [isotropic](https://stats.stackexchange.com/questions/204595/what-is-an-isotropic-spherical-covariance-matrix).
    Practically, it means that **a 3D point can be an ellipsoid rotated and stretched
    along any direction in space**. It could have required 9 parameters, however,
    they cannot be optimized directly because a covariance matrix has a physical meaning
    only if it’s [a positive semi-definite matrix](https://en.wikipedia.org/wiki/Definite_matrix).
    Using gradient descent for optimization makes it hard to pose such constraints
    on a matrix directly, that is why it is factorized instead as follows:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有两组参数需要进一步讨论，一组是协方差矩阵，另一组是SH。后者有一个专门的章节进行讨论。至于协方差，它被设计为各向异性的，即不是[各向同性](https://stats.stackexchange.com/questions/204595/what-is-an-isotropic-spherical-covariance-matrix)。实际上，这意味着**一个3D点可以是一个在空间中沿任意方向旋转和拉伸的椭球体**。它本来需要9个参数，但由于协方差矩阵只有在它是[半正定矩阵](https://en.wikipedia.org/wiki/Definite_matrix)时才有物理意义，因此这些参数不能直接优化。使用梯度下降进行优化使得直接对矩阵施加这样的约束变得困难，因此它被分解如下：
- en: '![](../Images/b7489bbf981a0b2fad6d3ca1ec4295e7.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b7489bbf981a0b2fad6d3ca1ec4295e7.png)'
- en: 'Such factorization is known as [eigendecomposition of a covariance matrix](https://www.visiondummy.com/2014/04/geometric-interpretation-covariance-matrix/#Covariance_matrix_as_a_linear_transformation)
    and can be understood as a configuration of an ellipsoid where:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 这种分解被称为[协方差矩阵的特征分解](https://www.visiondummy.com/2014/04/geometric-interpretation-covariance-matrix/#Covariance_matrix_as_a_linear_transformation)，可以理解为椭球体的配置，其中：
- en: S is a diagonal scaling matrix with 3 parameters for scale;
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: S 是一个具有3个缩放参数的对角缩放矩阵；
- en: R is a 3x3 rotation matrix analytically expressed with 4 quaternions.
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: R 是一个用4个四元数解析表示的3x3旋转矩阵。
- en: The beauty of using Gaussians lies in the two-fold impact of each point. On
    one hand, each point effectively **represents a limited area** in space close
    to its mean, according to its covariance. On the other hand, it has a **theoretically
    infinite extent** meaning that each Gaussian is defined on the whole 3D space
    and can be evaluated for any point. This is great because during optimization
    it allows gradients to flow from long distances.⁴
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 使用高斯分布的美在于每个点的双重影响。一方面，每个点根据其协方差有效地**表示了空间中接近其均值的有限区域**。另一方面，它具有**理论上的无限范围**，这意味着每个高斯分布在整个3D空间中定义，并且可以在任何点上进行评估。这一点很重要，因为在优化过程中，它允许梯度从较远的距离传播。⁴
- en: 'The impact of a 3D Gaussian *i* on an arbitrary 3D point *p* in 3D is defined
    as follows:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 3D高斯分布的 *i* 对任意3D点 *p* 的影响定义如下：
- en: '![](../Images/7cfa4b0ef239eaf9326f095e305af6e7.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7cfa4b0ef239eaf9326f095e305af6e7.png)'
- en: '**Figure 3:** An influence of a 3D Gaussian i on a point p in 3D [Source: Image
    by the author]'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '**图3：** 3D高斯分布 *i* 对3D点 *p* 的影响 [来源：作者提供的图像]'
- en: This equation looks almost like a probability density function of the [multivariate
    normal distribution](https://en.wikipedia.org/wiki/Multivariate_normal_distribution)
    except the normalization term with a determinant of covariance is ignored and
    it is weighting by the opacity instead.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 这个方程看起来几乎像是[多变量正态分布](https://en.wikipedia.org/wiki/Multivariate_normal_distribution)的概率密度函数，除了归一化项的协方差行列式被忽略，取而代之的是通过不透明度加权。
- en: '**Image formation model & r**endering'
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**图像形成模型与渲染**'
- en: Image formation model
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 图像形成模型
- en: 'Given a set of 3D points, possibly, the most interesting part is to see how
    it can be used for rendering. You might be previously familiar with a point-wise
    𝛼-blending used in NeRF. Turns out that **NeRFs and Gaussian splatting share the
    same image formation model**. To see this, let’s take a little detour and re-visit
    the volumetric rendering formula given in NeRF² and many of its follow-up works
    (1). We will also rewrite it using simple transitions (2):'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一组3D点，可能最有趣的部分是查看它如何用于渲染。你可能之前熟悉NeRF中使用的点对点𝛼混合。结果是**NeRF和高斯点投射共享相同的图像形成模型**。为了了解这一点，让我们稍作绕道，重新访问NeRF²及其许多后续工作的体积渲染公式(1)。我们还将使用简单的转换(2)对其进行重写：
- en: '![](../Images/b5c04ce9756b8ed2d8cd7c0e13e578ce.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b5c04ce9756b8ed2d8cd7c0e13e578ce.png)'
- en: 'You can refer to the NeRF paper for the definitions of σ and δ but conceptually
    this can be read as follows: in NeRF, color in an image pixel *p* is approximated
    by integrating over samples (MLP predictions) along the ray going through this
    pixel. The final color is **a weighted sum of colors of 3D points sampled along
    this ray, down-weighted by transmittance**. With this in mind, let’s finally look
    at the image formation model of Gaussian splatting:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1dc95bb2be718c79e04d032837b2ba01.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
- en: Indeed, formulas (2) and (3) are almost identical. **The only difference is
    how 𝛼 is computed** between the two. In Gaussian Splatting, the aggregation for
    each pixel is conducted over the contribution of an ordered list of projected
    2D Gaussians. This small discrepancy turns out extremely significant in practice
    and results in drastically different rendering speeds. In fact, it is **the foundation
    of the real-time performance** of Gaussian splatting.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: To understand why this is the case, we need to understand what *f^{2D}* means
    and which computational demands it poses. This function is simply a projection
    of *f(p)* we saw in the previous section into 2D, i.e. onto an image plane of
    the camera that is being rendered. **Both a 3D point and its projection are multivariate
    Gaussians** so the impact of a projected 2D Gaussian on a pixel can be computed
    using the same formula as the impact of a 3D Gaussian on other points in 3D (see
    Figure 3). The only difference is that the mean μ and covariance Σ must be projected
    into 2D which is done using derivations from EWA splatting⁵.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: 'Means in 2D can be trivially obtained by projecting a vector *μ* in homogeneous
    coordinates (with extra 1 coordinate) into an image plane using an intrinsic camera
    matrix *K* and an extrinsic camera matrix *W=*[*R*|*t*]:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/407712e079d453a1663f3cbcbde84275.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
- en: 'This can be also written in one line as follows:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a26ec4410ee5dd4848f7bb93562f5403.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
- en: Here “z” subscript stands for z-normalization. Covariance in 2D is defined using
    a Jacobian of (4), *J:*
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ef9295f4c347b373be0d195920f39264.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
- en: The whole process remains differentiatable, and that is of course crucial for
    optimization.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: Rendering
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The formula (3) tells us how to get a color in a single pixel. To render an
    entire image, it’s **still necessary to traverse through all the HxW pixels**,
    just like in NeRF, however, the process is much more lightweight because:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: For a given camera, *f(p)* of each 3D point **can be projected into 2D in advance**,
    before iterating over pixels. This way, when a Gaussian is blended for a few nearby
    pixels, we won’t need to re-project it over and over again.
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There is **no MLP to be inferenced** H·W·P times for a single image, 2D Gaussians
    are blended onto an image directly.
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There is **no ambiguity in which 3D point to evaluate** along the ray, no need
    to choose [a ray sampling strategy](https://docs.nerf.studio/nerfology/model_components/visualize_samplers.html).
    A set of 3D points overlapping the ray of each pixel (see *N* in (3)) is discrete
    and fixed after optimization.
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**沿射线评估哪个3D点没有歧义**，无需选择[射线采样策略](https://docs.nerf.studio/nerfology/model_components/visualize_samplers.html)。每个像素射线的3D点集（参见(3)中的*N*）是离散且在优化后固定的。'
- en: A pre-processing **sorting stage is done once per frame, on a GPU**, using a
    custom implementation of differentiable CUDA kernels.
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预处理的**排序阶段在每一帧中进行一次，使用GPU**，通过自定义的可微分CUDA内核实现。
- en: 'The conceptual difference can be seen in **Figure 4**:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 概念上的差异可以在**图4**中看到：
- en: '![](../Images/35c2fb9154bbdb55bc710a0fb1ba8356.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/35c2fb9154bbdb55bc710a0fb1ba8356.png)'
- en: '**Figure 4:** A conceptual difference between NeRF and GS, Left: Query a **continuous**
    MLP along the ray, Right: Blend a discrete set of Gaussians relevant to the given
    ray [Source: Image by the author]'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '**图4：** NeRF和GS之间的概念差异，左：沿射线查询**连续**的MLP，右：混合与给定射线相关的离散高斯集合 [来源：作者提供的图像]'
- en: 'The sorting algorithm mentioned above is one of the contributions of the paper.
    Its purpose is to prepare for color rendering with the formula (3): sorting of
    the 3D points by depth (proximity to an image plane) and grouping them by tiles.
    The first is needed to compute transmittance, and the latter allows to limit the
    weighted sum for each pixel to α-blending of the relevant 3D points only (or their
    2D projections, to be more specific). The grouping is achieved using simple 16x16
    pixel tiles and is implemented such that a Gaussian can land in a few tiles if
    it overlaps more than a single view frustum. Thanks to **sorting, the rendering
    of each pixel can be reduced to α-blending of pre-ordered points from the tile
    the pixel belongs to.**'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 上述排序算法是论文的一个贡献。其目的是为了配合公式(3)进行颜色渲染：按深度（靠近图像平面的距离）对3D点进行排序，并按图块进行分组。前者用于计算透射率，后者则限制每个像素的加权和只对相关的3D点进行α混合（或其2D投影，更具体地说）。分组是通过简单的16x16像素图块实现的，并且实现方式使得一个高斯可以落在多个图块中，如果它重叠了多个视锥体。得益于**排序，每个像素的渲染可以简化为从像素所属的图块中预排序点的α混合。**
- en: '![](../Images/5e97817449a482b9581c75363e79d003.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5e97817449a482b9581c75363e79d003.png)'
- en: '**Figure 5:** View frustums, each corresponding to a 16x16 image tile. Colors
    have no special meaning. The result of the sorting algorithm is a subset of 3D
    points within each tile sorted by depth. [Source: Based on the plots from [here](https://docs.nerf.studio/nerfology/model_components/visualize_samples.html#d-frustum)]'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '**图5：** 视锥体，每个视锥体对应一个16x16的图像块。颜色没有特殊含义。排序算法的结果是在每个图块内按深度排序的3D点的子集。[来源：基于[这里](https://docs.nerf.studio/nerfology/model_components/visualize_samples.html#d-frustum)的图]'
- en: '**Optimization**'
  id: totrans-68
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**优化**'
- en: 'A naive question might come to mind: how is it even possible to get a decent-looking
    image from a bunch of blobs in space? And well, it is true that if Gaussians aren’t
    optimized properly, you will get all kinds of pointy artifacts in renders. In
    Figure 6 you can observe an example of such artifacts, they look quite literally
    like ellipsoids. The key to getting good renders is 3 components: **good initialization,
    differentiable optimization, and adaptive densification**.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 可能会产生一个幼稚的问题：如何从空间中的一堆模糊物体中得到一个看起来不错的图像？的确，如果高斯没有被妥善优化，渲染结果中会出现各种尖锐的伪影。在图6中，你可以观察到这种伪影的一个示例，它们看起来确实像椭球体。获得良好渲染的关键在于3个组成部分：**良好的初始化、可微分优化和自适应稠密化**。
- en: '![](../Images/efee6cd91739022a65625107a91c4adc.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/efee6cd91739022a65625107a91c4adc.png)'
- en: '**Figure 6:** An example of renders of an under-optimized scene [Source: Image
    by the author]'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '**图6：** 一个未优化场景的渲染示例 [来源：作者提供的图像]'
- en: The initialization refers to the parameters of 3D points set at the start of
    training. For point locations (means), the authors propose to use a point cloud
    produced by SfM (Structure from Motion), see Figure 7\. The logic is that for
    any 3D reconstruction, be it with GS, NeRF, or something more classic, you must
    know camera matrices so you would probably run SfM anyway to obtain those. **Since
    SfM produces a sparse point cloud as a by-product, why not use it for initialization?**
    So that’s what the paper suggests. When a point cloud is not available for whatever
    reason, a random initialization can be used instead, under the risk of a potential
    loss of the final reconstruction quality.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 初始化指的是在训练开始时设定的 3D 点参数。对于点位置（均值），作者建议使用 SfM（结构光束法）生成的点云，见图 7。逻辑是，对于任何 3D 重建，无论是
    GS、NeRF 还是更经典的方法，你都必须知道相机矩阵，因此你可能还是会运行 SfM 来获取这些矩阵。**由于 SfM 生成稀疏点云作为副产品，为什么不利用它进行初始化？**
    这就是论文的建议。当点云由于某种原因不可用时，可以使用随机初始化，但有可能导致最终重建质量的损失。
- en: '![](../Images/5f546683705712828152da57840ffe4a.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5f546683705712828152da57840ffe4a.png)'
- en: '**Figure 7:** A sparse 3D point cloud produced by SfM, means initialization
    [Source: Taken from [here](https://speciale.ar/publication/privacypreservingsfm/)]'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 7：** 由 SfM 生成的稀疏 3D 点云，表示初始化 [来源：取自 [here](https://speciale.ar/publication/privacypreservingsfm/)]'
- en: Covariances are initialized to be isotropic, in other words, **3D points begin
    as spheres**. The radiuses are set based on mean distances to neighboring points
    such that the 3D world is nicely covered and has no “holes”.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 协方差初始化为各向同性，换句话说，**3D 点最初是球形**。半径基于与邻近点的平均距离进行设置，以确保 3D 世界被很好地覆盖，并且没有“孔洞”。
- en: After init, a simple Stochastic Gradient Descent is used to fit everything properly.
    The scene is optimized for **a loss function that is a combination of L1 and D-SSIM**
    (structural dissimilarity index measure) between a ground truth view and a current
    render.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 初始化后，使用简单的随机梯度下降法来正确拟合所有内容。场景经过优化以**使损失函数成为 L1 和 D-SSIM**（结构不相似性指数测量）之间的组合，比较真实视图和当前渲染效果。
- en: However, that’s not it, another crucial part remains and that is adaptive densification.
    It is launched once in a while during training, say, every 100 SGD steps and its
    purpose is to address under- and over-reconstruction. It’s important to emphasize
    that **SGD on its own can only do as much as adjust the existing points**. But
    it would struggle to find good parameters in areas that lack points altogether
    or have too many of them. That’s where adaptive densification comes in, **splitting
    points** with large gradients (Figure 8) and **removing points** that have converged
    to very low values of α (if a point is that transparent, why keep it?).
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这还不是全部，另一个关键部分是自适应稠密化。它在训练过程中不时启动，比如每`100 SGD`步，这个过程旨在解决欠重建和过度重建的问题。需要强调的是，**SGD
    本身只能调整现有点**。但在完全缺少点或点过多的区域，它会难以找到合适的参数。这时，自适应稠密化就派上用场了，它**拆分大梯度的点**（图 8）并**移除已收敛到非常低
    α 值的点**（如果一个点如此透明，为什么还要保留它？）。
- en: '![](../Images/43554f1bdf17135d3873a2d1f67ed947.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/43554f1bdf17135d3873a2d1f67ed947.png)'
- en: '**Figure 8:** Adaptive densification. A toy example of fitting a bean shape
    that we’d like to render with a few points. [Source: Taken from [1]]'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 8：** 自适应稠密化。一个玩具示例，展示了我们希望用少量点来呈现的豆形。[来源：取自[1]]'
- en: '**View-dependant colors with SH**'
  id: totrans-80
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**视图依赖色彩与 SH**'
- en: Spherical harmonics, SH for short, play a significant role in computer graphics
    and were first proposed as a way to learn a view-dependant color of discrete 3D
    voxels in Plenoxels⁶. View dependence is a nice-to-have property that **improves
    the quality of renders since it allows the model to represent non-Lambertian effects**,
    e.g. specularities of metallic surfaces. However, it is certainly not a must since
    it’s possible to make a simplification, choose to represent color with 3 RGB values,
    and still use Gaussian splatting like it was done in [4]. That is why we are reviewing
    this representation detail separately after the whole method is laid out.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 球面谐波（简称 SH）在计算机图形学中起着重要作用，最早提出作为一种学习离散 3D 体素视图依赖色彩的方法，在 Plenoxels⁶ 中得到应用。视图依赖性是一个可选属性，**它提高了渲染质量，因为它允许模型表示非朗伯特效应**，例如金属表面的高光。然而，这绝不是必须的，因为可以简化选择用
    3 个 RGB 值表示颜色，仍然使用高斯散射，就像在[4]中做的那样。因此，我们在方法完整描述后单独回顾这个表示细节。
- en: 'SH are special functions defined on the surface of a sphere. In other words,
    you can evaluate such a function for any point on the sphere and get a value.
    **All of these functions are derived from this single formula** by choosing positive
    integers for *ℓ* and −*ℓ* ≤ *m* ≤ *ℓ*, one *(ℓ, m)* pair per SH:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: SH 是定义在球面上的特殊函数。换句话说，你可以对球面上的任何点评估这样的函数并得到一个值。**所有这些函数都从这个单一的公式中推导而来**，通过选择正整数的*ℓ*
    和 - *ℓ* ≤ *m* ≤ *ℓ*，每个SH一个*(ℓ, m)*对：
- en: '![](../Images/ce8f701bf9ee54a8e4de80b75becda76.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ce8f701bf9ee54a8e4de80b75becda76.png)'
- en: While a bit intimidating at first, for small values of *l* this formula simplifies
    significantly. In fact, for *ℓ = 1, Y = ~0.282*, just a constant on the whole
    sphere. On the contrary, higher values of *ℓ* produce more complex surfaces. The
    theory tells us that spherical harmonics form an orthonormal basis so **each function
    defined on a sphere can be expressed through SH**.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然一开始有点令人生畏，但对于小值的*l*这个公式会大大简化。实际上，对于*ℓ = 1, Y = ~0.282*，在整个球面上只是一个常数。相反，较高的*ℓ*值会产生更复杂的表面。理论告诉我们，球面谐波形成一个正交基，因此**在球面上定义的每个函数都可以通过SH表示**。
- en: 'That’s why the idea to express view-dependant color goes like this: let’s limit
    ourselves to a certain degree of freedom *ℓ_max* and say that each **color (red,
    green, and blue) is a linear combination of the first *ℓ_max* SH functions**.
    For every 3D Gaussian, we want to learn the correct coefficients so that when
    we look at this 3D point from a certain direction it will convey a color the closest
    to the ground truth one. The whole process of obtaining a view-dependant color
    can be seen in Figure 9.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是为什么表达视角相关颜色的想法是这样的：让我们将自由度限制在一个特定的*ℓ_max*，并假设每种**颜色（红色、绿色和蓝色）是前*ℓ_max*个SH函数的线性组合**。对于每个3D高斯，我们希望学习正确的系数，以便当我们从某个方向查看这个3D点时，它能传达出最接近真实颜色的颜色。获得视角相关颜色的整个过程可以参见图9。
- en: '![](../Images/c53b7a7d485fa4e9d0ad103b9e3a4343.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c53b7a7d485fa4e9d0ad103b9e3a4343.png)'
- en: '**Figure 9:** A process of obtaining a view-dependant color (red component)
    of a point with *ℓ_max = 2 and 9 learned coefficients*. A sigmoid function maps
    the value into the [0, 1] interval. Oftentimes, clipping is used instead [Source:
    Image by the author]'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '**图9：** 获取一个点的视角相关颜色（红色分量）的过程，*ℓ_max = 2 和 9 个学习到的系数*。一个sigmoid函数将值映射到[0, 1]区间。通常，裁剪被用来代替
    [来源：作者图片]'
- en: Limitations
  id: totrans-88
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 限制
- en: 'Despite the overall great results and the impressive rendering speed, the simplicity
    of the representation comes with a price. The most significant consideration is
    various **regularization heuristics** that are introduced during optimization
    to guard the model **against “broken” Gaussians**: points that are too big, too
    long, redundant, etc. This part is crucial and the mentioned issues can be further
    amplified in tasks beyond novel view rendering.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管总体结果很出色，渲染速度也很快，但表示的简单性也有代价。最重要的考虑是优化过程中引入的各种**正则化启发式**，以保护模型**免受“破损”高斯的影响**：过大、过长、冗余的点等。这部分非常关键，这些问题在新视角渲染之外的任务中可能会进一步放大。
- en: The choice to step aside from a continuous representation in favor of a discrete
    one means that **the inductive bias of MLPs is lost**. In NeRFs, an MLP performs
    an implicit interpolation and smoothes out possible inconsistencies between given
    views, while 3D Gaussians are more sensitive, leading back to the problem described
    above.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 选择放弃连续表示而转向离散表示意味着**MLP的归纳偏差丢失**。在NeRF中，MLP执行隐式插值，并平滑给定视图之间可能的不一致，而3D高斯则更敏感，这会导致回到上述问题。
- en: 'Furthermore, Gaussian splatting is not free from some well-known **artifacts
    present in NeRFs** which they both inherit from the shared image formation model:
    lower quality in less seen or unseen regions, floaters close to an image plane,
    etc.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，高斯喷溅也不免受到一些在NeRF中存在的**已知伪影**的影响，这些伪影都来源于共享的图像形成模型：在少见或未见区域的质量较低、接近图像平面的浮动点等。
- en: The file size of a checkpoint is another property to take into account, even
    though novel view rendering is far from being deployed to edge devices. Considering
    the ballpark number of 3D points and the MLP architectures of popular NeRFs, both
    take **the same order of magnitude of disk space**, with GS being just a few times
    heavier on average.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 检查点的文件大小是另一个需要考虑的属性，即使新视角渲染距离边缘设备的部署还很远。考虑到3D点的大致数量和流行NeRF的MLP架构，两者**磁盘空间的量级相同**，其中GS平均重量仅重几倍。
- en: Where to play with it
  id: totrans-93
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 玩它的地方
- en: 'No blog post can do justice to a method as well as just running it and seeing
    the results for yourself. Here is where you can play around:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 没有哪个博客文章能比直接运行方法并亲自查看结果更能体现其价值。你可以在这里尝试：
- en: '[gaussian-splatting](https://github.com/graphdeco-inria/gaussian-splatting)
    — the official implementation with custom CUDA kernels;'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[gaussian-splatting](https://github.com/graphdeco-inria/gaussian-splatting)
    — 带有自定义CUDA内核的官方实现；'
- en: '[nerfstudio](https://github.com/nerfstudio-project/nerfstudio) —yes, Gaussian
    splatting in **nerf**studio. This is a framework originally dedicated to NeRF-like
    models but since December, ‘23, it also supports GS;'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[nerfstudio](https://github.com/nerfstudio-project/nerfstudio) — 是的，**nerf**studio中的高斯溅射。这是一个最初专注于NeRF类似模型的框架，但从2023年12月开始，它也支持GS；'
- en: '[threestudio-3dgs](https://github.com/DSaurus/threestudio-3dgs) — an extension
    for threestudio, another cross-model framework. You should use this one if you
    are interested in generating 3D models from a prompt rather than learning an existing
    set of images;'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[threestudio-3dgs](https://github.com/DSaurus/threestudio-3dgs) — 一个用于threestudio的扩展，另一个跨模型框架。如果你对从提示生成3D模型感兴趣，而不是学习现有的图像集，你应该使用这个；'
- en: '[UnityGaussianSplatting](https://github.com/aras-p/UnityGaussianSplatting)
    — if Unity is your thing, you can port a trained model into this plugin for visualization;'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[UnityGaussianSplatting](https://github.com/aras-p/UnityGaussianSplatting)
    — 如果你对Unity感兴趣，你可以将训练好的模型移植到这个插件中进行可视化；'
- en: '[gsplat](https://github.com/nerfstudio-project/gsplat) — a library for CUDA-accelerated
    rasterization of Gaussians that branched out of nerfstudio. It can be used for
    independent torch-based projects as a differentiatable module for splatting.'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[gsplat](https://github.com/nerfstudio-project/gsplat) — 一个用于高斯体素CUDA加速光栅化的库，源自nerfstudio。它可以作为可区分的溅射模块用于独立的基于torch的项目。'
- en: Have fun!
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 玩得开心！
- en: Acknowledgments
  id: totrans-101
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 致谢
- en: This blog post is based on a group meeting in the lab of Dr. Tali Dekel. Special
    thanks go to [Michal Geyer](https://medium.com/u/74c8897cdd08?source=post_page-----e7d570081362--------------------------------)
    for the discussions of the paper, to the authors of [4] for a coherent summary
    of Gaussian splatting, and to [Yuliang Guo](https://medium.com/u/0170f53deb62?source=post_page-----e7d570081362--------------------------------)
    for the suggested improvements.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 本博客文章基于Dr. Tali Dekel实验室的一个小组会议。特别感谢[Michal Geyer](https://medium.com/u/74c8897cdd08?source=post_page-----e7d570081362--------------------------------)对论文讨论的贡献，感谢[4]的作者对高斯溅射的连贯总结，以及[Yuliang
    Guo](https://medium.com/u/0170f53deb62?source=post_page-----e7d570081362--------------------------------)对改进建议的提供。
- en: References
  id: totrans-103
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: Kerbl, B., Kopanas, G., Leimkühler, T., & Drettakis, G. (2023). [3D Gaussian
    Splatting for Real-Time Radiance Field Rendering.](https://arxiv.org/abs/2308.04079)
    SIGGRAPH 2023.
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Kerbl, B., Kopanas, G., Leimkühler, T., & Drettakis, G. (2023). [用于实时辐射场渲染的3D高斯溅射。](https://arxiv.org/abs/2308.04079)
    SIGGRAPH 2023。
- en: 'Mildenhall, B., Srinivasan, P. P., Tancik, M., Barron, J. T., Ramamoorthi,
    R., & Ng, R. (2020). [NeRF: Representing Scenes as Neural Radiance Fields for
    View Synthesis.](https://arxiv.org/abs/2003.08934) ECCV 2020.'
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Mildenhall, B., Srinivasan, P. P., Tancik, M., Barron, J. T., Ramamoorthi,
    R., & Ng, R. (2020). [NeRF: 将场景表示为神经辐射场以实现视图合成。](https://arxiv.org/abs/2003.08934)
    ECCV 2020。'
- en: Zwicker, M., Pfister, H., van Baar, J., & Gross, M. (2001). [Surface Splatting.](https://www.cs.umd.edu/~zwicker/publications/SurfaceSplatting-SIG01.pdf)
    SIGGRAPH 2001
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Zwicker, M., Pfister, H., van Baar, J., & Gross, M. (2001). [表面溅射。](https://www.cs.umd.edu/~zwicker/publications/SurfaceSplatting-SIG01.pdf)
    SIGGRAPH 2001。
- en: 'Luiten, J., Kopanas, G., Leibe, B., & Ramanan, D. (2023). [Dynamic 3D Gaussians:
    Tracking by Persistent Dynamic View Synthesis.](https://arxiv.org/abs/2308.09713)
    International Conference on 3D Vision.'
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Luiten, J., Kopanas, G., Leibe, B., & Ramanan, D. (2023). [动态3D高斯：通过持久动态视图合成进行跟踪。](https://arxiv.org/abs/2308.09713)
    国际3D视觉会议。
- en: Zwicker, M., Pfister, H., van Baar, J., & Gross, M. (2001). [EWA Volume Splatting.](https://www.cs.umd.edu/~zwicker/publications/EWAVolumeSplatting-VIS01.pdf)
    IEEE Visualization 2001.
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Zwicker, M., Pfister, H., van Baar, J., & Gross, M. (2001). [EWA体积溅射。](https://www.cs.umd.edu/~zwicker/publications/EWAVolumeSplatting-VIS01.pdf)
    IEEE Visualization 2001。
- en: 'Yu, A., Fridovich-Keil, S., Tancik, M., Chen, Q., Recht, B., & Kanazawa, A.
    (2023). [Plenoxels: Radiance Fields without Neural Networks.](https://arxiv.org/abs/2112.05131)
    CVPR 2022.'
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Yu, A., Fridovich-Keil, S., Tancik, M., Chen, Q., Recht, B., & Kanazawa, A.
    (2023). [Plenoxels: 无需神经网络的辐射场。](https://arxiv.org/abs/2112.05131) CVPR 2022。'
