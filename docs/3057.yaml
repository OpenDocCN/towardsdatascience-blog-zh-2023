- en: 'Large Language Models: DistilBERT — Smaller, Faster, Cheaper and Lighter'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 大型语言模型：DistilBERT——更小、更快、更便宜、更轻便
- en: 原文：[https://towardsdatascience.com/distilbert-11c8810d29fc?source=collection_archive---------2-----------------------#2023-10-07](https://towardsdatascience.com/distilbert-11c8810d29fc?source=collection_archive---------2-----------------------#2023-10-07)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/distilbert-11c8810d29fc?source=collection_archive---------2-----------------------#2023-10-07](https://towardsdatascience.com/distilbert-11c8810d29fc?source=collection_archive---------2-----------------------#2023-10-07)
- en: 'Unlocking the secrets of BERT compression: a student-teacher framework for
    maximum efficiency'
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解锁 BERT 压缩的秘密：一个用于最大效率的学生-教师框架
- en: '[](https://medium.com/@slavahead?source=post_page-----11c8810d29fc--------------------------------)[![Vyacheslav
    Efimov](../Images/db4b02e75d257063e8e9d3f1f75d9d6d.png)](https://medium.com/@slavahead?source=post_page-----11c8810d29fc--------------------------------)[](https://towardsdatascience.com/?source=post_page-----11c8810d29fc--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----11c8810d29fc--------------------------------)
    [Vyacheslav Efimov](https://medium.com/@slavahead?source=post_page-----11c8810d29fc--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@slavahead?source=post_page-----11c8810d29fc--------------------------------)[![Vyacheslav
    Efimov](../Images/db4b02e75d257063e8e9d3f1f75d9d6d.png)](https://medium.com/@slavahead?source=post_page-----11c8810d29fc--------------------------------)[](https://towardsdatascience.com/?source=post_page-----11c8810d29fc--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----11c8810d29fc--------------------------------)
    [Vyacheslav Efimov](https://medium.com/@slavahead?source=post_page-----11c8810d29fc--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fc8a0ca9d85d8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdistilbert-11c8810d29fc&user=Vyacheslav+Efimov&userId=c8a0ca9d85d8&source=post_page-c8a0ca9d85d8----11c8810d29fc---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----11c8810d29fc--------------------------------)
    ·7 min read·Oct 7, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F11c8810d29fc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdistilbert-11c8810d29fc&user=Vyacheslav+Efimov&userId=c8a0ca9d85d8&source=-----11c8810d29fc---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fc8a0ca9d85d8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdistilbert-11c8810d29fc&user=Vyacheslav+Efimov&userId=c8a0ca9d85d8&source=post_page-c8a0ca9d85d8----11c8810d29fc---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----11c8810d29fc--------------------------------)
    ·7 min read·2023年10月7日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F11c8810d29fc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdistilbert-11c8810d29fc&user=Vyacheslav+Efimov&userId=c8a0ca9d85d8&source=-----11c8810d29fc---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F11c8810d29fc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdistilbert-11c8810d29fc&source=-----11c8810d29fc---------------------bookmark_footer-----------)![](../Images/96278a8b1b50077af63e7602a118a6e1.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F11c8810d29fc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdistilbert-11c8810d29fc&source=-----11c8810d29fc---------------------bookmark_footer-----------)![](../Images/96278a8b1b50077af63e7602a118a6e1.png)'
- en: Introduction
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: In recent years, the evolution of large language models has skyrocketed. BERT
    became one of the most popular and efficient models allowing to solve a wide range
    of NLP tasks with high accuracy. After BERT, a set of other models appeared later
    on the scene demonstrating outstanding results as well.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，大型语言模型的发展迅速。BERT 成为最受欢迎和高效的模型之一，能够以高精度解决各种自然语言处理任务。在 BERT 之后，出现了一系列其他模型，这些模型也展示了卓越的结果。
- en: The obvious trend that became easy to observe is the fact that **with time large
    language models (LLMs) tend to become more complex by exponentially augmenting
    the number of parameters and data they are trained on**. Research in deep learning
    showed that such techniques usually lead to better results. Unfortunately, the
    machine learning world has already dealt with several problems regarding LLMs
    and scalability has become the main obstacle in effective training, storing and
    using them.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 显而易见的趋势是**随着时间的推移，大型语言模型（LLMs）往往变得更加复杂，通过指数级增加参数和数据的数量**。深度学习研究表明，这些技术通常会带来更好的结果。不幸的是，机器学习领域已经遇到了一些关于
    LLMs 的问题，可扩展性已成为有效训练、存储和使用它们的主要障碍。
- en: 'By taking into consideration this issue, special techniques have been elaborated
    for compressing LLMs. The objectives of compressing algorithms are either decreasing
    training time, reducing memory consumption or accelerating model inference. The
    three most common compression techniques used in practice are the following:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 针对这个问题，已经制定了专门的技术来压缩 LLMs。压缩算法的目标是减少训练时间、降低内存消耗或加快模型推断。实际中使用的三种最常见的压缩技术如下：
- en: '**Knowledge distillation** involves training a smaller model trying to represent
    the behaviour of a larger model.'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**知识蒸馏**涉及训练一个较小的模型，试图表示较大模型的行为。'
- en: '**Quantization** is the process of reducing memory for storing numbers representing
    model’s weights.'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**量化**是减少存储表示模型权重的数字所需内存的过程。'
- en: '**Pruning** refers to discarding the least important model’s weights.'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**剪枝**指的是丢弃最不重要的模型权重。'
- en: In this article, we will understand the distillation mechanism applied to BERT
    which led to a new model called **DistilBERT**. By the way, the discussed techniques
    below can be applied to other NLP models as well.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们将理解应用于 BERT 的蒸馏机制，这导致了一个新模型叫做**DistilBERT**。顺便提一下，下面讨论的技术也可以应用于其他 NLP
    模型。
- en: Distillation basics
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 蒸馏基础
- en: The goal of distillation is to create a smaller model which can imitate a larger
    model. In practice, it means that if a large model predicts something, then a
    smaller model is expected to make a similar prediction.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 蒸馏的目标是创建一个较小的模型，可以模仿较大的模型。在实践中，这意味着如果一个大型模型预测某些内容，则期望较小的模型做出类似的预测。
- en: To achieve this, a larger model needs to be already pretrained (BERT in our
    case). Then an architecture of a smaller model needs to be chosen. To increase
    the possibility of successful imitation, it is usually recommended for the smaller
    model to have a similar architecture to the larger model with a reduced number
    of parameters. Finally, the smaller model learns from the predictions made by
    the larger model on a certain dataset. For this objective, it is vital to choose
    an appropriate loss function that will help the smaller model to learn better.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现这一点，需要一个已经预训练的大型模型（在我们的例子中是 BERT）。然后，需要选择一个较小模型的架构。为了增加成功模仿的可能性，通常建议较小模型的架构与大型模型相似，但参数数量减少。最后，较小模型从大型模型在某一数据集上做出的预测中学习。为了达到这一目标，选择一个适当的损失函数对于帮助较小模型更好地学习至关重要。
- en: In distillation notation, the larger model is called a **teacher** and the smaller
    model is referred to as a **student**.
  id: totrans-19
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 在蒸馏术语中，大型模型称为**教师**，较小模型称为**学生**。
- en: ''
  id: totrans-20
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Generally, the distillation procedure is applied during the pretaining but can
    be applied during the fine-tuning as well.
  id: totrans-21
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 一般来说，蒸馏过程应用于预训练阶段，但也可以在微调阶段应用。
- en: DistilBERT
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: DistilBERT
- en: 'DistilBERT learns from BERT and updates its weights by using the loss function
    which consists of three components:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: DistilBERT 从 BERT 学习，并通过使用包含三个组件的损失函数来更新其权重：
- en: Masked language modeling (MLM) loss
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 掩码语言建模（MLM）损失
- en: Distillation loss
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 蒸馏损失
- en: Similarity loss
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 相似性损失
- en: Below, we are going to discuss these loss components and undestand the necessity
    of each of them. Nevertheless, before diving into depth it is necessary to understand
    an important concept called **temperature** in softmax activation function. The
    temperature concept is used in the DistilBERT loss function.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将讨论这些损失组件，并了解每个组件的必要性。不过，在深入之前，有必要了解一个重要概念，即**Softmax 激活函数中的温度**。温度概念在
    DistilBERT 损失函数中使用。
- en: Softmax temperature
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Softmax 温度
- en: It is often to observe a softmax transformation as the last layer of a neural
    network. Softmax normalizes all model outputs, so they sum up to 1 and can be
    interpreted as probabilities.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: 'There exists a softmax formula where all the outputs of the model are divided
    by a **temperature** parameter T:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0ef39138409a1d7dc7d5b9c54d277c03.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
- en: Softmax temperature formula. pᵢ and zᵢ are the model output and the normalized
    probability for the i-th object respectively. T is the temperature parameter.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: 'The temperature T controls the smoothness of the output distribution:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: If T > 1, then the distribution becomes smoother.
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If T = 1, then the distribution is the same if the normal softmax was applied.
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If T < 1, then the distribution becomes more rough.
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To make things clear, let us look at an example. Consider a classification task
    with 5 labels in which a neural network produced 5 values indicating the confidence
    of an input object belonging to a corresponding class. Applying softmax with different
    values of T results in different output distributions.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d9f8c82bfca718e35bcc0ccb3b49fa80.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
- en: An example of a neural network producing different probability distributions
    based on the temperature T
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: The greater the temperature is, the smoother the probability distribution becomes.
  id: totrans-40
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/2e15f1e6f63d8e615e2cd873a3e2118b.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
- en: Softmax transformation of logits (natural numbers from 1 to 5) based on different
    values of temperature T. As the temperature increases, softmax values become more
    aligned with each other.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: Loss function
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Masked language modeling loss
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Similar to the teacher’s model (BERT), during pretraining, the student (DistilBERT)
    learns language by making predictions for the masked language modeling task. After
    producing a prediction for a certain token, the predicted probability distribution
    is compared to the one-hot encoded probability distribution of the teacher’s model.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: The one-hot encoded distribution designates a probability distribution where
    the probability of the most likely token is set to 1 and the probabilities of
    all other tokens are set to 0.
  id: totrans-46
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: As in most language models, the cross-entropy loss is calculated between predicted
    and true distribution and the weights of the student’s model are updated through
    backpropagation.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5fea9adb3ce64f303d23caf2e3157829.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
- en: Masked language modeling loss computation example
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: Distillation loss
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Actually it is possible to use only the student loss to train the student model.
    However, in many cases, it might not be enough. The common problem with using
    only the student loss lies in its softmax transformation in which the temperature
    T is set to 1\. In practice, the resulting distribution with T = 1 turns out to
    be in the form where one of the possible labels has a very high probability close
    to 1 and all other label probabilities become low being close to 0.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: 'Such a situation does not align well with cases where two or more classification
    labels are valid for a particular input: the softmax layer with T = 1 will be
    very likely to exclude all valid labels but one and will make the probability
    distribution close to one-hot encoding distribution. This results in a loss of
    potentially useful information that could be learned by the student model which
    makes it less diverse.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 这种情况与对特定输入有效的两个或多个分类标签不太一致：当T = 1的softmax层将很可能排除所有有效标签，除了一种，并将概率分布接近于one-hot编码分布。这会导致可能被学生模型学习的有用信息丢失，从而使其多样性降低。
- en: That is why the authors of the paper introduce **distillation loss** in which
    softmax probabilities are calculated with a temperature T > 1 making it possible
    to smoothly align probabilities, thus taking into consideration several possible
    answers for the student.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是为什么论文的作者引入了**蒸馏损失**，其中softmax概率使用温度T > 1进行计算，使得平滑对齐概率成为可能，从而考虑到学生的多个可能答案。
- en: In distillation loss, the same temperature T is applied both to the student
    and the teacher. One-hot encoding of the teacher’s distribution is removed.
  id: totrans-54
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 在蒸馏损失中，相同的温度T应用于学生和教师。移除了教师分布的one-hot编码。
- en: '![](../Images/fd5275d5b6ccd07dc4dcf4558ac3ffb4.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fd5275d5b6ccd07dc4dcf4558ac3ffb4.png)'
- en: Distillation loss computation example
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 蒸馏损失计算示例
- en: Instead of the cross-entropy loss, it is possible to use KL divergence loss.
  id: totrans-57
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 可以使用KL散度损失代替交叉熵损失。
- en: Similarity loss
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 相似度损失
- en: The researchers also state that it is beneficial to add cosine similarity loss
    between hidden state embeddings.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 研究人员还指出，在隐藏状态嵌入之间添加余弦相似度损失是有益的。
- en: '![](../Images/533d35280d9b4f50832dcfdbea177731.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/533d35280d9b4f50832dcfdbea177731.png)'
- en: Cosine loss formula
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 余弦损失公式
- en: This way, the student is likely not only to reproduce masked tokens correctly
    but also to construct embeddings that are similar to those of the teacher. It
    also opens the door for preserving the same relations between embeddings in both
    spaces of the models.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 这样，学生不仅有可能正确重建被掩盖的标记，还能构建与教师相似的嵌入。这也为在两个模型空间中保持嵌入之间的相同关系打开了大门。
- en: '![](../Images/68e9a28153ec8abac446648fb86690f2.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/68e9a28153ec8abac446648fb86690f2.png)'
- en: Similarity loss computation example
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 相似度损失计算示例
- en: Triple loss
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 三重损失
- en: Finally, a sum of the linear combination of all three loss functions is calculated
    which defines the loss function in DistilBERT. Based on the loss value, the backpropagation
    is performed on the student model to update its weights.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，计算所有三个损失函数的线性组合和，这定义了DistilBERT中的损失函数。根据损失值，对学生模型进行反向传播以更新其权重。
- en: '![](../Images/21322eac7070863f1c9e7d0a857d026a.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/21322eac7070863f1c9e7d0a857d026a.png)'
- en: DistilBERT loss function
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: DistilBERT损失函数
- en: As an interesting fact, among the three loss components, the masked language
    modeling loss has the least importance on the model’s performance. The distillation
    loss and similarity loss have a much higher impact.
  id: totrans-69
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 有趣的是，在三种损失组件中，掩盖语言建模损失对模型性能的影响最小。蒸馏损失和相似度损失的影响要大得多。
- en: Inference
  id: totrans-70
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 推断
- en: The inference process in DistilBERT works exactly as during the training phase.
    The only subtlety is that softmax temperature T is set to 1\. This is done to
    obtain probabilities close to those calculated by BERT.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: DistilBERT中的推断过程与训练阶段完全相同。唯一的细微之处是softmax温度T设置为1。这是为了获得接近BERT计算的概率。
- en: Architecture
  id: totrans-72
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 架构
- en: 'In general, DistilBERT uses the same architecture as BERT except for these
    changes:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，DistilBERT使用与BERT相同的架构，只是进行了以下更改：
- en: DistilBERT has only half of BERT layers. Each layer in the model is initialized
    by taking one BERT layer out of two.
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DistilBERT仅有BERT层的一半。模型中的每一层都通过从两个BERT层中选择一个来初始化。
- en: Token-type embeddings are removed.
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 移除了标记类型嵌入。
- en: The dense layer which is applied to the hidden state of the [CLS] token for
    a classification task is removed.
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应用于[CLS]标记的隐藏状态的分类任务的密集层被移除。
- en: 'For a more robust performance, authors use the best ideas proposed in RoBERTa:'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为了更强的性能，作者使用了RoBERTa中提出的最佳方法：
- en: '- usage of dynamic masking'
  id: totrans-78
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '- 使用动态掩蔽'
- en: '- removing the next sentence prediction objective'
  id: totrans-79
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '- 移除下一个句子预测目标'
- en: '- training on larger batches'
  id: totrans-80
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '- 在更大的批次上训练'
- en: '- gradient accumulation technique is applied for optimized gradient computations'
  id: totrans-81
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '- 应用梯度累积技术以优化梯度计算'
- en: The last hidden layer size (768) in DistilBERT is the same as in BERT. The authors
    reported that its reduction does not lead to considerable improvements in terms
    of computation efficiency. According to them, reducing the number of total layers
    has a much higher impact.
  id: totrans-82
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: DistilBERT 的最后一层隐藏层大小（768）与 BERT 相同。作者报告称，其减少不会在计算效率方面带来显著改进。他们认为，减少总层数有更高的影响。
- en: Data
  id: totrans-83
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据
- en: DistilBERT is trained on the same corpus of data as BERT which contains BooksCorpus
    (800M words) English Wikipedia (2500M words).
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: DistilBERT 在与 BERT 相同的数据语料上训练，这些数据包含 BooksCorpus（8 亿字）和英语维基百科（25 亿字）。
- en: BERT vs DistilBERT comparison
  id: totrans-85
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: BERT 与 DistilBERT 的比较
- en: 'The key performance parameters of BERT and DistilBERT were compared on the
    several most popular benchmarks. Here are the facts important to retain:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: BERT 和 DistilBERT 的关键性能参数在几个最流行的基准测试上进行了比较。以下是需要记住的重要事实：
- en: During inference, DistilBERT is 60% faster than BERT.
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在推理过程中，DistilBERT 比 BERT 快 60%。
- en: DistilBERT has 44M fewer parameters and in total is 40% smaller than BERT.
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DistilBERT 的参数比 BERT 少 4400 万，总体比 BERT 小 40%。
- en: DistilBERT retains 97% of BERT performance.
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DistilBERT 保留了 BERT 97% 的性能。
- en: '![](../Images/8a13681d73d760daa68e782d9a1c69e4.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8a13681d73d760daa68e782d9a1c69e4.png)'
- en: BERT vs DistilBERT comparison (on GLUE dataset)
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: BERT 与 DistilBERT 的比较（在 GLUE 数据集上）
- en: Conclusion
  id: totrans-92
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: 'DistilBERT made a huge step in BERT evolution by allowing it to significantly
    compress the model while achieving comparable performance on various NLP tasks.
    Apart from it, DistilBERT weighs only 207 MB making the integration on devices
    with restricted capacities easier. Knowledge distillation is not the only technique
    to apply: DistilBERT can be further compressed with quantization or pruning algorithms.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: DistilBERT 在 BERT 的进化中迈出了巨大的一步，它通过显著压缩模型的体积，同时在各种 NLP 任务上实现了相当的性能。除此之外，DistilBERT
    的体积仅为 207 MB，使得在容量有限的设备上集成更加容易。知识蒸馏并不是唯一可应用的技术：DistilBERT 可以通过量化或剪枝算法进一步压缩。
- en: Resources
  id: totrans-94
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 资源
- en: '[DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter](https://arxiv.org/pdf/1910.01108.pdf)'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[DistilBERT，BERT 的蒸馏版本：更小、更快、更便宜、更轻](https://arxiv.org/pdf/1910.01108.pdf)'
- en: '*All images unless otherwise noted are by the author*'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '*除非另有说明，所有图像均由作者提供*'
