- en: 'Large Language Models: DistilBERT — Smaller, Faster, Cheaper and Lighter'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/distilbert-11c8810d29fc?source=collection_archive---------2-----------------------#2023-10-07](https://towardsdatascience.com/distilbert-11c8810d29fc?source=collection_archive---------2-----------------------#2023-10-07)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Unlocking the secrets of BERT compression: a student-teacher framework for
    maximum efficiency'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@slavahead?source=post_page-----11c8810d29fc--------------------------------)[![Vyacheslav
    Efimov](../Images/db4b02e75d257063e8e9d3f1f75d9d6d.png)](https://medium.com/@slavahead?source=post_page-----11c8810d29fc--------------------------------)[](https://towardsdatascience.com/?source=post_page-----11c8810d29fc--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----11c8810d29fc--------------------------------)
    [Vyacheslav Efimov](https://medium.com/@slavahead?source=post_page-----11c8810d29fc--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fc8a0ca9d85d8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdistilbert-11c8810d29fc&user=Vyacheslav+Efimov&userId=c8a0ca9d85d8&source=post_page-c8a0ca9d85d8----11c8810d29fc---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----11c8810d29fc--------------------------------)
    ·7 min read·Oct 7, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F11c8810d29fc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdistilbert-11c8810d29fc&user=Vyacheslav+Efimov&userId=c8a0ca9d85d8&source=-----11c8810d29fc---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F11c8810d29fc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdistilbert-11c8810d29fc&source=-----11c8810d29fc---------------------bookmark_footer-----------)![](../Images/96278a8b1b50077af63e7602a118a6e1.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In recent years, the evolution of large language models has skyrocketed. BERT
    became one of the most popular and efficient models allowing to solve a wide range
    of NLP tasks with high accuracy. After BERT, a set of other models appeared later
    on the scene demonstrating outstanding results as well.
  prefs: []
  type: TYPE_NORMAL
- en: The obvious trend that became easy to observe is the fact that **with time large
    language models (LLMs) tend to become more complex by exponentially augmenting
    the number of parameters and data they are trained on**. Research in deep learning
    showed that such techniques usually lead to better results. Unfortunately, the
    machine learning world has already dealt with several problems regarding LLMs
    and scalability has become the main obstacle in effective training, storing and
    using them.
  prefs: []
  type: TYPE_NORMAL
- en: 'By taking into consideration this issue, special techniques have been elaborated
    for compressing LLMs. The objectives of compressing algorithms are either decreasing
    training time, reducing memory consumption or accelerating model inference. The
    three most common compression techniques used in practice are the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Knowledge distillation** involves training a smaller model trying to represent
    the behaviour of a larger model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Quantization** is the process of reducing memory for storing numbers representing
    model’s weights.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Pruning** refers to discarding the least important model’s weights.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this article, we will understand the distillation mechanism applied to BERT
    which led to a new model called **DistilBERT**. By the way, the discussed techniques
    below can be applied to other NLP models as well.
  prefs: []
  type: TYPE_NORMAL
- en: Distillation basics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The goal of distillation is to create a smaller model which can imitate a larger
    model. In practice, it means that if a large model predicts something, then a
    smaller model is expected to make a similar prediction.
  prefs: []
  type: TYPE_NORMAL
- en: To achieve this, a larger model needs to be already pretrained (BERT in our
    case). Then an architecture of a smaller model needs to be chosen. To increase
    the possibility of successful imitation, it is usually recommended for the smaller
    model to have a similar architecture to the larger model with a reduced number
    of parameters. Finally, the smaller model learns from the predictions made by
    the larger model on a certain dataset. For this objective, it is vital to choose
    an appropriate loss function that will help the smaller model to learn better.
  prefs: []
  type: TYPE_NORMAL
- en: In distillation notation, the larger model is called a **teacher** and the smaller
    model is referred to as a **student**.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Generally, the distillation procedure is applied during the pretaining but can
    be applied during the fine-tuning as well.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: DistilBERT
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'DistilBERT learns from BERT and updates its weights by using the loss function
    which consists of three components:'
  prefs: []
  type: TYPE_NORMAL
- en: Masked language modeling (MLM) loss
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Distillation loss
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Similarity loss
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Below, we are going to discuss these loss components and undestand the necessity
    of each of them. Nevertheless, before diving into depth it is necessary to understand
    an important concept called **temperature** in softmax activation function. The
    temperature concept is used in the DistilBERT loss function.
  prefs: []
  type: TYPE_NORMAL
- en: Softmax temperature
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It is often to observe a softmax transformation as the last layer of a neural
    network. Softmax normalizes all model outputs, so they sum up to 1 and can be
    interpreted as probabilities.
  prefs: []
  type: TYPE_NORMAL
- en: 'There exists a softmax formula where all the outputs of the model are divided
    by a **temperature** parameter T:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0ef39138409a1d7dc7d5b9c54d277c03.png)'
  prefs: []
  type: TYPE_IMG
- en: Softmax temperature formula. pᵢ and zᵢ are the model output and the normalized
    probability for the i-th object respectively. T is the temperature parameter.
  prefs: []
  type: TYPE_NORMAL
- en: 'The temperature T controls the smoothness of the output distribution:'
  prefs: []
  type: TYPE_NORMAL
- en: If T > 1, then the distribution becomes smoother.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If T = 1, then the distribution is the same if the normal softmax was applied.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If T < 1, then the distribution becomes more rough.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To make things clear, let us look at an example. Consider a classification task
    with 5 labels in which a neural network produced 5 values indicating the confidence
    of an input object belonging to a corresponding class. Applying softmax with different
    values of T results in different output distributions.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d9f8c82bfca718e35bcc0ccb3b49fa80.png)'
  prefs: []
  type: TYPE_IMG
- en: An example of a neural network producing different probability distributions
    based on the temperature T
  prefs: []
  type: TYPE_NORMAL
- en: The greater the temperature is, the smoother the probability distribution becomes.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/2e15f1e6f63d8e615e2cd873a3e2118b.png)'
  prefs: []
  type: TYPE_IMG
- en: Softmax transformation of logits (natural numbers from 1 to 5) based on different
    values of temperature T. As the temperature increases, softmax values become more
    aligned with each other.
  prefs: []
  type: TYPE_NORMAL
- en: Loss function
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Masked language modeling loss
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Similar to the teacher’s model (BERT), during pretraining, the student (DistilBERT)
    learns language by making predictions for the masked language modeling task. After
    producing a prediction for a certain token, the predicted probability distribution
    is compared to the one-hot encoded probability distribution of the teacher’s model.
  prefs: []
  type: TYPE_NORMAL
- en: The one-hot encoded distribution designates a probability distribution where
    the probability of the most likely token is set to 1 and the probabilities of
    all other tokens are set to 0.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: As in most language models, the cross-entropy loss is calculated between predicted
    and true distribution and the weights of the student’s model are updated through
    backpropagation.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5fea9adb3ce64f303d23caf2e3157829.png)'
  prefs: []
  type: TYPE_IMG
- en: Masked language modeling loss computation example
  prefs: []
  type: TYPE_NORMAL
- en: Distillation loss
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Actually it is possible to use only the student loss to train the student model.
    However, in many cases, it might not be enough. The common problem with using
    only the student loss lies in its softmax transformation in which the temperature
    T is set to 1\. In practice, the resulting distribution with T = 1 turns out to
    be in the form where one of the possible labels has a very high probability close
    to 1 and all other label probabilities become low being close to 0.
  prefs: []
  type: TYPE_NORMAL
- en: 'Such a situation does not align well with cases where two or more classification
    labels are valid for a particular input: the softmax layer with T = 1 will be
    very likely to exclude all valid labels but one and will make the probability
    distribution close to one-hot encoding distribution. This results in a loss of
    potentially useful information that could be learned by the student model which
    makes it less diverse.'
  prefs: []
  type: TYPE_NORMAL
- en: That is why the authors of the paper introduce **distillation loss** in which
    softmax probabilities are calculated with a temperature T > 1 making it possible
    to smoothly align probabilities, thus taking into consideration several possible
    answers for the student.
  prefs: []
  type: TYPE_NORMAL
- en: In distillation loss, the same temperature T is applied both to the student
    and the teacher. One-hot encoding of the teacher’s distribution is removed.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/fd5275d5b6ccd07dc4dcf4558ac3ffb4.png)'
  prefs: []
  type: TYPE_IMG
- en: Distillation loss computation example
  prefs: []
  type: TYPE_NORMAL
- en: Instead of the cross-entropy loss, it is possible to use KL divergence loss.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Similarity loss
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The researchers also state that it is beneficial to add cosine similarity loss
    between hidden state embeddings.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/533d35280d9b4f50832dcfdbea177731.png)'
  prefs: []
  type: TYPE_IMG
- en: Cosine loss formula
  prefs: []
  type: TYPE_NORMAL
- en: This way, the student is likely not only to reproduce masked tokens correctly
    but also to construct embeddings that are similar to those of the teacher. It
    also opens the door for preserving the same relations between embeddings in both
    spaces of the models.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/68e9a28153ec8abac446648fb86690f2.png)'
  prefs: []
  type: TYPE_IMG
- en: Similarity loss computation example
  prefs: []
  type: TYPE_NORMAL
- en: Triple loss
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Finally, a sum of the linear combination of all three loss functions is calculated
    which defines the loss function in DistilBERT. Based on the loss value, the backpropagation
    is performed on the student model to update its weights.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/21322eac7070863f1c9e7d0a857d026a.png)'
  prefs: []
  type: TYPE_IMG
- en: DistilBERT loss function
  prefs: []
  type: TYPE_NORMAL
- en: As an interesting fact, among the three loss components, the masked language
    modeling loss has the least importance on the model’s performance. The distillation
    loss and similarity loss have a much higher impact.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Inference
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The inference process in DistilBERT works exactly as during the training phase.
    The only subtlety is that softmax temperature T is set to 1\. This is done to
    obtain probabilities close to those calculated by BERT.
  prefs: []
  type: TYPE_NORMAL
- en: Architecture
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In general, DistilBERT uses the same architecture as BERT except for these
    changes:'
  prefs: []
  type: TYPE_NORMAL
- en: DistilBERT has only half of BERT layers. Each layer in the model is initialized
    by taking one BERT layer out of two.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Token-type embeddings are removed.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The dense layer which is applied to the hidden state of the [CLS] token for
    a classification task is removed.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For a more robust performance, authors use the best ideas proposed in RoBERTa:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '- usage of dynamic masking'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '- removing the next sentence prediction objective'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '- training on larger batches'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '- gradient accumulation technique is applied for optimized gradient computations'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The last hidden layer size (768) in DistilBERT is the same as in BERT. The authors
    reported that its reduction does not lead to considerable improvements in terms
    of computation efficiency. According to them, reducing the number of total layers
    has a much higher impact.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: DistilBERT is trained on the same corpus of data as BERT which contains BooksCorpus
    (800M words) English Wikipedia (2500M words).
  prefs: []
  type: TYPE_NORMAL
- en: BERT vs DistilBERT comparison
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The key performance parameters of BERT and DistilBERT were compared on the
    several most popular benchmarks. Here are the facts important to retain:'
  prefs: []
  type: TYPE_NORMAL
- en: During inference, DistilBERT is 60% faster than BERT.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: DistilBERT has 44M fewer parameters and in total is 40% smaller than BERT.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: DistilBERT retains 97% of BERT performance.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/8a13681d73d760daa68e782d9a1c69e4.png)'
  prefs: []
  type: TYPE_IMG
- en: BERT vs DistilBERT comparison (on GLUE dataset)
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'DistilBERT made a huge step in BERT evolution by allowing it to significantly
    compress the model while achieving comparable performance on various NLP tasks.
    Apart from it, DistilBERT weighs only 207 MB making the integration on devices
    with restricted capacities easier. Knowledge distillation is not the only technique
    to apply: DistilBERT can be further compressed with quantization or pruning algorithms.'
  prefs: []
  type: TYPE_NORMAL
- en: Resources
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter](https://arxiv.org/pdf/1910.01108.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*All images unless otherwise noted are by the author*'
  prefs: []
  type: TYPE_NORMAL
