["```py\nlibrary(kernlab)\nlibrary(drf)\nlibrary(Matrix)\nlibrary(Hmisc)\n\nsource(\"CIdrf.R\")\n```", "```py\n set.seed(2)\n\nn<-2000\nbeta1<-1\nbeta2<--1.8\n\n# Model Simulation\nX<-mvrnorm(n = n, mu=c(0,0), Sigma=matrix(c(1,0.7,0.7,1), nrow=2,ncol=2))\nu<-rnorm(n=n, sd = sqrt(exp(X[,1])))\nY<- matrix(beta1*X[,1] + beta2*X[,2] + u, ncol=1)\n```", "```py\n# Choose an x that is not too far out\nx<-matrix(c(1,1),ncol=2)\n\n# Choose alpha for CIs\nalpha<-0.05\n```", "```py\n## Fit the new DRF framework\ndrf_fit <- drfCI(X=X, Y=Y, min.node.size = 2, splitting.rule='FourierMMD', num.features=10, B=100)\n\n## predict weights\nDRF = predictdrf(drf_fit, x=x)\nweights <- DRF$weights[1,]\n```", "```py\n# Estimate the conditional expectation at x:\ncondexpest<- sum(weights*Y)\n\n# Use the distribution of weights, see below\ndistofcondexpest<-unlist(lapply(DRF$weightsb, function(wb)  sum(wb[1,]*Y)  ))\n\n# Can either use the above directly to build confidence interval, or can use the normal approximation.\n# We will use the latter\nvarest<-var(distofcondexpest-condexpest)\n\n# build 95%-CI\nlower<-condexpest - qnorm(1-alpha/2)*sqrt(varest)\nupper<-condexpest + qnorm(1-alpha/2)*sqrt(varest)\nc(lower, condexpest, upper)\n\n(-1.00, -0.69 -0.37)\n```", "```py\n# Estimate the conditional expectation at x:\ncondvarest<- sum(weights*Y^2) - condexpest^2\n\ndistofcondvarest<-unlist(lapply(DRF$weightsb, function(wb)  {\n  sum(wb[1,]*Y^2) - sum(wb[1,]*Y)^2\n  } ))\n\n# Can either use the above directly to build confidence interval, or can use the normal approximation.\n# We will use the latter\nvarest<-var(distofcondvarest-condvarest)\n\n# build 95%-CI\nlower<-condvarest - qnorm(1-alpha/2)*sqrt(varest)\nupper<-condvarest + qnorm(1-alpha/2)*sqrt(varest)\n\nc(lower, condvarest, upper)\n\n(1.89, 2.65, 3.42) \n```", "```py\n## Functions in CIdrf.R that is loaded above ##\n\ndrfCI <- function(X, Y, B, sampling = \"binomial\",...) {\n\n### Function that uses DRF with subsampling to obtain confidence regions as\n### as described in https://arxiv.org/pdf/2302.05761.pdf\n### X: Matrix of predictors\n### Y: Matrix of variables of interest\n### B: Number of half-samples/mini-forests\n\n  n <- dim(X)[1]\n\n  # compute point estimator and DRF per halfsample S\n  # weightsb: B times n matrix of weights\n  DRFlist <- lapply(seq_len(B), function(b) {\n\n    # half-sample index\n    indexb <- if (sampling == \"binomial\") {\n      seq_len(n)[as.logical(rbinom(n, size = 1, prob = 0.5))]\n    } else {\n      sample(seq_len(n), floor(n / 2), replace = FALSE)\n    }\n\n    ## Using refitting DRF on S\n    DRFb <- \n      drf(X = X[indexb, , drop = F], Y = Y[indexb, , drop = F],\n          ci.group.size = 1, ...)\n\n    return(list(DRF = DRFb, indices = indexb))\n  })\n\n  return(list(DRFlist = DRFlist, X = X, Y = Y) )\n}\n\npredictdrf<- function(DRF, x, ...) {\n\n### Function to predict from DRF with Confidence Bands\n### DRF: DRF object\n### x: Testpoint\n\n  ntest <- nrow(x)\n  n <- nrow(DRF$Y)\n\n  ## extract the weights w^S(x)\n  weightsb <- lapply(DRF$DRFlist, function(l) {\n\n    weightsbfinal <- Matrix(0, nrow = ntest, ncol = n , sparse = TRUE)\n\n    weightsbfinal[, l$indices] <- predict(l$DRF, x)$weights \n\n    return(weightsbfinal)\n  })\n\n  ## obtain the overall weights w\n  weights<- Reduce(\"+\", weightsb) / length(weightsb)\n\nreturn(list(weights = weights, weightsb = weightsb ))\n}\n\nWitdrf<- function(DRF, x, groupingvar, alpha=0.05, ...){\n\n### Function to calculate the conditional witness function with\n### confidence bands from DRF\n### DRF: DRF object\n### x: Testpoint\n\n  if (is.null(dim(x)) ){\n\n  stop(\"x needs to have dim(x) > 0\")\n  }\n\n  ntest <- nrow(x)\n  n <- nrow(DRF$Y)\n  coln<-colnames(DRF$Y)\n\n  ## Collect w^S\n  weightsb <- lapply(DRF$DRFlist, function(l) {\n\n    weightsbfinal <- Matrix(0, nrow = ntest, ncol = n , sparse = TRUE)\n\n    weightsbfinal[, l$indices] <- predict(l$DRF, x)$weights \n\n    return(weightsbfinal)\n  })\n\n  ## Obtain w\n  weightsall <- Reduce(\"+\", weightsb) / length(weightsb)\n\n  #weightsall0<-weightsall[, DRF$Y[, groupingvar]==0, drop=F]\n  #weightsall1<-weightsall[,DRF$Y[, groupingvar]==1, drop=F]\n\n  # Get the weights of the respective classes (need to standardize by propensity!)\n  weightsall0<-weightsall*(DRF$Y[, groupingvar]==0)/sum(weightsall*(DRF$Y[, groupingvar]==0))\n  weightsall1<-weightsall*(DRF$Y[, groupingvar]==1)/sum(weightsall*(DRF$Y[, groupingvar]==1))\n\n  bandwidth_Y <- drf:::medianHeuristic(DRF$Y)\n  k_Y <- rbfdot(sigma = bandwidth_Y)\n\n  K<-kernelMatrix(k_Y, DRF$Y[,coln[coln!=groupingvar]], y = DRF$Y[,coln[coln!=groupingvar]])\n\n  nulldist <- sapply(weightsb, function(wb){\n    # iterate over class 1\n\n    wb0<-wb*(DRF$Y[, groupingvar]==0)/sum(wb*(DRF$Y[, groupingvar]==0))\n    wb1<-wb*(DRF$Y[, groupingvar]==1)/sum(wb*(DRF$Y[, groupingvar]==1))\n\n    diag( ( wb0-weightsall0 - (wb1-weightsall1) )%*%K%*%t( wb0-weightsall0 - (wb1-weightsall1) )  )\n\n  })\n\n  # Choose the right quantile\n  c<-quantile(nulldist, 1-alpha)\n\n  return(list(c=c, k_Y=k_Y, Y=DRF$Y[,coln[coln!=groupingvar]], nulldist=nulldist, weightsall0=weightsall0, weightsall1=weightsall1))\n\n} \n```", "```py\nset.seed(2)\n\nn<-4000\np<-2\n\nX<-matrix(runif(n*p), ncol=2)\nW<-rbinom(n,size=1, prob= exp(-X[,2])/(1+exp(-X[,2])))\n\nY<-(W-0.2)*X[,1] + rnorm(n)\nY<-matrix(Y,ncol=1) \n```", "```py\n x<-matrix(runif(1*p), ncol=2)\nYall<-cbind(Y,W)\n## For the current version of the Witdrf function, we need to give\n## colnames to Yall\ncolnames(Yall) <- c(\"Y\", \"W\")\n\n## Fit the new DRF framework\ndrf_fit <- drfCI(X=X, Y=Yall, min.node.size = 5, splitting.rule='FourierMMD', num.features=10, B=100)\n\nWitobj<-Witdrf(drf_fit, x=x, groupingvar=\"W\", alpha=0.05)\n\nhatmun<-function(y,Witobj){\n\n  c<-Witobj$c\n  k_Y<-Witobj$k_Y\n  Y<-Witobj$Y\n  weightsall1<-Witobj$weightsall1\n  weightsall0<-Witobj$weightsall0\n  Ky=t(kernelMatrix(k_Y, Y , y = y))\n\n  #K1y <- t(kernelMatrix(k_Y, DRF$Y[DRF$Y[, groupingvar]==1,coln[coln!=groupingvar]], y = y))\n  #K0y <- t(kernelMatrix(k_Y, DRF$Y[DRF$Y[, groupingvar]==0,coln[coln!=groupingvar]], y = y))\n  out<-list()\n  out$val <- tcrossprod(Ky, weightsall1  ) - tcrossprod(Ky, weightsall0  )\n  out$upper<-  out$val+sqrt(c)\n  out$lower<-  out$val-sqrt(c)\n\n  return( out )\n\n}\n\nall<-hatmun(sort(Witobj$Y),Witobj)\n\nplot(sort(Witobj$Y),all$val , type=\"l\", col=\"darkblue\", lwd=2, ylim=c(min(all$lower), max(all$upper)),\n     xlab=\"y\", ylab=\"witness function\")\nlines(sort(Witobj$Y),all$upper , type=\"l\", col=\"darkgreen\", lwd=2 )\nlines(sort(Witobj$Y),all$lower , type=\"l\", col=\"darkgreen\", lwd=2 )\nabline(h=0)\n```", "```py\n# Simulate truth for a large number of samples ntest\nntest<-10000\nXtest<-matrix(runif(ntest*p), ncol=2)\n\nY1<-(1-0.2)*Xtest[,1] + rnorm(ntest)\nY0<-(0-0.2)*Xtest[,1] + rnorm(ntest)\n\n## Plot the test data without adjustment\nplotdf = data.frame(Y=c(Y1,Y0), W=c(rep(1,ntest),rep(0,ntest) ))\nplotdf$weight=1\nplotdf$plotweight[plotdf$W==0] = plotdf$weight[plotdf$W==0]/sum(plotdf$weight[plotdf$W==0])\nplotdf$plotweight[plotdf$W==1] = plotdf$weight[plotdf$W==1]/sum(plotdf$weight[plotdf$W==1])\n\nplotdf$W <- factor(plotdf$W)\n\n#plot pooled data\nggplot(plotdf, aes(Y)) +\n  geom_density(adjust=2.5, alpha = 0.3, show.legend=TRUE,  aes(fill=W, weight=plotweight)) +\n  theme_light()+\n  scale_fill_discrete(name = \"Group\", labels = c('0', \"1\"))+\n  theme(legend.position = c(0.83, 0.66),\n        legend.text=element_text(size=18),\n        legend.title=element_text(size=20),\n        legend.background = element_rect(fill=alpha('white', 0.5)),\n        axis.text.x = element_text(size=14),\n        axis.text.y = element_text(size=14),\n        axis.title.x = element_text(size=19),\n        axis.title.y = element_text(size=19))+\n  labs(x='y') \n```"]