- en: Training an Agent to Master Tic-Tac-Toe Through Self-Play
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/training-an-agent-to-master-tic-tac-toe-through-self-play-72038c3f33f7?source=collection_archive---------4-----------------------#2023-09-18](https://towardsdatascience.com/training-an-agent-to-master-tic-tac-toe-through-self-play-72038c3f33f7?source=collection_archive---------4-----------------------#2023-09-18)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Surprisingly, a software agent never gets tired of the game.
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://sebastiengilbert.medium.com/?source=post_page-----72038c3f33f7--------------------------------)[![Sébastien
    Gilbert](../Images/380f6588c3ef718947bcf82061f190eb.png)](https://sebastiengilbert.medium.com/?source=post_page-----72038c3f33f7--------------------------------)[](https://towardsdatascience.com/?source=post_page-----72038c3f33f7--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----72038c3f33f7--------------------------------)
    [Sébastien Gilbert](https://sebastiengilbert.medium.com/?source=post_page-----72038c3f33f7--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F975aef8c496a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftraining-an-agent-to-master-tic-tac-toe-through-self-play-72038c3f33f7&user=S%C3%A9bastien+Gilbert&userId=975aef8c496a&source=post_page-975aef8c496a----72038c3f33f7---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----72038c3f33f7--------------------------------)
    ·7 min read·Sep 18, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F72038c3f33f7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftraining-an-agent-to-master-tic-tac-toe-through-self-play-72038c3f33f7&user=S%C3%A9bastien+Gilbert&userId=975aef8c496a&source=-----72038c3f33f7---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F72038c3f33f7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftraining-an-agent-to-master-tic-tac-toe-through-self-play-72038c3f33f7&source=-----72038c3f33f7---------------------bookmark_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: Ah! primary school! This was the time when we learned valuable skills, such
    as literacy, arithmetic, and playing tic-tac-toe optimally.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4987ff3824a1167744cc7eac7a8a995e.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Solstice Hannan](https://unsplash.com/@darkersolstice?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: Playing a tic-tac-toe match with your friend without getting caught by the teacher
    is an art. You must discretely pass the game sheet under the desk while giving
    the impression of being attentive to the subject matter. The fun was probably
    more about the undercover operation than the game itself.
  prefs: []
  type: TYPE_NORMAL
- en: We cannot teach the art of avoiding getting caught in the classroom to a software
    agent, but can we train an agent to master the game?
  prefs: []
  type: TYPE_NORMAL
- en: In [my previous post](/training-an-agent-to-master-a-simple-game-through-self-play-88bdd0d60928),
    we studied an agent learning the game *SumTo100* through self-play. It was an
    easy game that allowed us to display the state value, which helped us build an
    intuition about how the agent learns the game. With tic-tac-toe, we are tackling
    a much larger state space.
  prefs: []
  type: TYPE_NORMAL
- en: You can find the Python code in [this repository](https://github.com/sebastiengilbert73/tutorial_learnbyplay).
    The script that performs the training is[*learn_tictactoe.sh*](https://github.com/sebastiengilbert73/tutorial_learnbyplay/blob/main/learn_tictactoe.sh)*:*
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The script loops through calls to two programs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[generate_positions_expectations.py](https://github.com/sebastiengilbert73/tutorial_learnbyplay/blob/main/preprocessing/generate_positions_expectations.py):
    Simulates matches and stores game states with the discounted expected return.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[train_agent.py](https://github.com/sebastiengilbert73/tutorial_learnbyplay/blob/main/train/train_agent.py):
    Trains the neural network for a few epochs on the most recently generated dataset.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The training cycle
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The learning of the game by the agent proceeds through a cycle of generating
    matches and training to predict the match outcome from the game state:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2857c9ef68ed84a281a997ede02e21cb.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: The cycle of match generation and neural network training. Image
    by the author.'
  prefs: []
  type: TYPE_NORMAL
- en: Generation of matches
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The cycle starts with the simulation of matches between random players, i.e.,
    players that choose randomly from the list of legal actions in a given game state.
  prefs: []
  type: TYPE_NORMAL
- en: Why are we generating matches played randomly?
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: This project is about learning by self-play, so we can’t give the agent any
    a priori information about *how to play*. In the first cycle, since the agent
    has no clue about good or bad moves, the matches must be generated by random play.
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2 shows an example of a match between random players:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/da8234cf38f237660b7fbdf3c20e3c8c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: Example of a tic-tac-toe match played randomly. Image by the author.'
  prefs: []
  type: TYPE_NORMAL
- en: What lesson can we learn by observing this match? From the point of view of
    the ‘X’ player, we can assume this is an example of poor play since it concluded
    in a loss. We don’t know which move(s) is/are responsible for the defeat, so we’ll
    assume that **all the decisions** made by the ‘X’ player were bad. If some decisions
    were good, we bet on statistics (other simulations could go through a similar
    state) to rectify their predicted state value.
  prefs: []
  type: TYPE_NORMAL
- en: The last action from player ‘X’ is given a value of -1\. The other actions receive
    a discounted negative value that geometrically decays by a factor of γ (gamma)
    ∈ [0, 1] as we go backward toward the first move.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c439c76cde3f0ff250db2afc3f0197f3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Target values for game states. Image by the author.'
  prefs: []
  type: TYPE_NORMAL
- en: States from matches that resulted in a win receive similar positive discounted
    values. States drawn from draws are given a value of zero. The agent takes the
    point of view of both the first and the second player.
  prefs: []
  type: TYPE_NORMAL
- en: The game states as tensors
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We need a tensor representation for the game state. We’ll use a [2x3x3] tensor
    where the first dimension represents the channels (0 for ‘X’ and 1 for ‘O’), and
    the two other dimensions are the rows and the columns. The occupancy of a (row,
    column) square gets encoded as a 1 in the (channel, row, column) entry.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/bc8d3134b11af7a2a484723be077ad5c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: The game state representation by a [2x3x3] tensor. Image by the author.'
  prefs: []
  type: TYPE_NORMAL
- en: The pairs of (state tensor, target value) obtained by the generation of matches
    constitute the dataset on which the neural network will train in each round. The
    dataset is built at the beginning of the cycle, taking advantage of the learning
    that has occurred in previous rounds. While the first round generates purely random
    play, the subsequent ones generate gradually more realistic matches.
  prefs: []
  type: TYPE_NORMAL
- en: Injecting randomness into the play
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The first round of match generation opposes random players. The subsequent
    rounds oppose the agent to itself (hence “self-play”). The agent is equipped with
    a regression neural network trained to predict the match outcome, which allows
    it to choose the legal action that yields the highest expected value. To promote
    diversity, the agent chooses actions based on an epsilon-greedy algorithm: with
    probability (1-ε), the best action gets chosen; otherwise, a random action gets
    chosen.'
  prefs: []
  type: TYPE_NORMAL
- en: Training
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Figure 5 shows the evolution of the validation losses along five epochs for
    a maximum of 17 training rounds:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/918c2b226de14d865912cc13b549f8d3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: The evolution of the validation loss for various training round numbers.
    Image by the author.'
  prefs: []
  type: TYPE_NORMAL
- en: We can see that the first few training rounds show a fast decrease in the validation
    loss, and then there seems to be a plateau around a mean squared error loss of
    0.2\. This trend shows that the agent’s regression neural network gets better
    at predicting the outcome of a match played against itself, from a given game
    state. Since the actions from both players are non-deterministic, there is a limit
    to the predictability of the match outcome. That explains why the validation loss
    stops improving after some rounds.
  prefs: []
  type: TYPE_NORMAL
- en: Improvement from round to round
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: With the game [*SumTo100*](https://medium.com/towards-data-science/training-an-agent-to-master-a-simple-game-through-self-play-88bdd0d60928)*,*
    we could represent the state on a 1D grid. However, with tic-tac-toe, we can’t
    directly display the state value evolution. One thing we can do to measure the
    improvement is to pit the agent against the previous version of itself and observe
    the difference between the wins and the losses.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using ε = 0.5 for the first action of both players and ε = 0.1 for the rest
    of the match, running 1000 matches per comparison, this is what we get:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d436f33f9948cc6bbba94e340dd29473.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: Comparison of the agent with its previous version. Image by the author.'
  prefs: []
  type: TYPE_NORMAL
- en: The number of wins exceeded the number of losses (showing an improvement) until
    10 training rounds. After that, the agent didn’t improve from round to round.
  prefs: []
  type: TYPE_NORMAL
- en: Test
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Time to see how our agent plays tic-tac-toe!
  prefs: []
  type: TYPE_NORMAL
- en: One useful feature of having a regression neural network is the possibility
    of displaying the agent’s evaluation of every legal move. Let’s play a game against
    the agent, showing how it judges its options.
  prefs: []
  type: TYPE_NORMAL
- en: Manual play
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The agent starts, playing ‘X’:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c687722e5b02c5ca5227a569d58e4ba7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: A match played against the agent, with the action evaluations. Image
    by the author.'
  prefs: []
  type: TYPE_NORMAL
- en: That’s how you get brutally crushed by a soulless tic-tac-toe machine!
  prefs: []
  type: TYPE_NORMAL
- en: As soon as I put the ‘O’ in the (1, 0) square, the expected return increased
    from 0.142 to 0.419, and my fate was sealed.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s see how it does when the agent plays second:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a9e4e2badfd31f24fe08fcbafa46a8ca.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8: A match played against the agent, with action evaluations. Image
    by the author.'
  prefs: []
  type: TYPE_NORMAL
- en: It didn’t fall into the trap, and the match was a draw.
  prefs: []
  type: TYPE_NORMAL
- en: Matches against a random player
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'If we [simulate](https://github.com/sebastiengilbert73/tutorial_learnbyplay/blob/main/test/tictactoe_run_multiple_games.py)
    a large number of matches against a random player, this is what we get:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/42a2362e86da81ba20a4b78922b38929.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9: Results of 1000 matches against a random player. Image by the author.'
  prefs: []
  type: TYPE_NORMAL
- en: Out of 1000 matches (the agent played first in half the matches), the agent
    won 950 matches, lost no one, and there were 50 draws. This is not proof that
    our agent is playing optimally, but it has certainly reached a decent level of
    play.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As a follow-up to [**Training an Agent to Master a Simple Game Through Self-Play**](/training-an-agent-to-master-a-simple-game-through-self-play-88bdd0d60928)
    where the game was easy to crack and the state space was small, we used the same
    technique to master tic-tac-toe. Although this is still a toy problem, the state
    space of tic-tac-toe is large enough that the agent’s regression neural network
    is required to find patterns in the state tensors. These patterns allow the generalization
    for unseen state tensors.
  prefs: []
  type: TYPE_NORMAL
- en: The code is available [in this repository](https://github.com/sebastiengilbert73/tutorial_learnbyplay).
    Give it a try, and let me know what you think!
  prefs: []
  type: TYPE_NORMAL
