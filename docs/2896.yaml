- en: Training an Agent to Master Tic-Tac-Toe Through Self-Play
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练一个智能体通过自我对弈掌握井字游戏
- en: 原文：[https://towardsdatascience.com/training-an-agent-to-master-tic-tac-toe-through-self-play-72038c3f33f7?source=collection_archive---------4-----------------------#2023-09-18](https://towardsdatascience.com/training-an-agent-to-master-tic-tac-toe-through-self-play-72038c3f33f7?source=collection_archive---------4-----------------------#2023-09-18)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/training-an-agent-to-master-tic-tac-toe-through-self-play-72038c3f33f7?source=collection_archive---------4-----------------------#2023-09-18](https://towardsdatascience.com/training-an-agent-to-master-tic-tac-toe-through-self-play-72038c3f33f7?source=collection_archive---------4-----------------------#2023-09-18)
- en: Surprisingly, a software agent never gets tired of the game.
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 令人惊讶的是，软件智能体永远不会对游戏感到厌倦。
- en: '[](https://sebastiengilbert.medium.com/?source=post_page-----72038c3f33f7--------------------------------)[![Sébastien
    Gilbert](../Images/380f6588c3ef718947bcf82061f190eb.png)](https://sebastiengilbert.medium.com/?source=post_page-----72038c3f33f7--------------------------------)[](https://towardsdatascience.com/?source=post_page-----72038c3f33f7--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----72038c3f33f7--------------------------------)
    [Sébastien Gilbert](https://sebastiengilbert.medium.com/?source=post_page-----72038c3f33f7--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://sebastiengilbert.medium.com/?source=post_page-----72038c3f33f7--------------------------------)[![Sébastien
    Gilbert](../Images/380f6588c3ef718947bcf82061f190eb.png)](https://sebastiengilbert.medium.com/?source=post_page-----72038c3f33f7--------------------------------)[](https://towardsdatascience.com/?source=post_page-----72038c3f33f7--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----72038c3f33f7--------------------------------)
    [Sébastien Gilbert](https://sebastiengilbert.medium.com/?source=post_page-----72038c3f33f7--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F975aef8c496a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftraining-an-agent-to-master-tic-tac-toe-through-self-play-72038c3f33f7&user=S%C3%A9bastien+Gilbert&userId=975aef8c496a&source=post_page-975aef8c496a----72038c3f33f7---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----72038c3f33f7--------------------------------)
    ·7 min read·Sep 18, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F72038c3f33f7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftraining-an-agent-to-master-tic-tac-toe-through-self-play-72038c3f33f7&user=S%C3%A9bastien+Gilbert&userId=975aef8c496a&source=-----72038c3f33f7---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F975aef8c496a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftraining-an-agent-to-master-tic-tac-toe-through-self-play-72038c3f33f7&user=S%C3%A9bastien+Gilbert&userId=975aef8c496a&source=post_page-975aef8c496a----72038c3f33f7---------------------post_header-----------)
    发表在[Towards Data Science](https://towardsdatascience.com/?source=post_page-----72038c3f33f7--------------------------------)
    ·7分钟阅读·2023年9月18日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F72038c3f33f7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftraining-an-agent-to-master-tic-tac-toe-through-self-play-72038c3f33f7&user=S%C3%A9bastien+Gilbert&userId=975aef8c496a&source=-----72038c3f33f7---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F72038c3f33f7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftraining-an-agent-to-master-tic-tac-toe-through-self-play-72038c3f33f7&source=-----72038c3f33f7---------------------bookmark_footer-----------)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F72038c3f33f7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftraining-an-agent-to-master-tic-tac-toe-through-self-play-72038c3f33f7&source=-----72038c3f33f7---------------------bookmark_footer-----------)'
- en: Ah! primary school! This was the time when we learned valuable skills, such
    as literacy, arithmetic, and playing tic-tac-toe optimally.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 啊！小学时光！那是我们学习宝贵技能的时候，比如识字、算术，以及如何最优地玩井字游戏。
- en: '![](../Images/4987ff3824a1167744cc7eac7a8a995e.png)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4987ff3824a1167744cc7eac7a8a995e.png)'
- en: Photo by [Solstice Hannan](https://unsplash.com/@darkersolstice?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 照片由[Solstice Hannan](https://unsplash.com/@darkersolstice?utm_source=medium&utm_medium=referral)提供，拍摄于[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: Playing a tic-tac-toe match with your friend without getting caught by the teacher
    is an art. You must discretely pass the game sheet under the desk while giving
    the impression of being attentive to the subject matter. The fun was probably
    more about the undercover operation than the game itself.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 和你的朋友玩井字棋比赛而不被老师发现是一门艺术。你必须在暗中将游戏纸悄悄传递到桌子下，同时表现出对课堂内容的关注。乐趣可能更多在于隐秘操作而不是游戏本身。
- en: We cannot teach the art of avoiding getting caught in the classroom to a software
    agent, but can we train an agent to master the game?
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 我们无法教给软件代理在课堂上避免被抓到的艺术，但我们能否训练一个代理掌握游戏？
- en: In [my previous post](/training-an-agent-to-master-a-simple-game-through-self-play-88bdd0d60928),
    we studied an agent learning the game *SumTo100* through self-play. It was an
    easy game that allowed us to display the state value, which helped us build an
    intuition about how the agent learns the game. With tic-tac-toe, we are tackling
    a much larger state space.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [我之前的文章](/training-an-agent-to-master-a-simple-game-through-self-play-88bdd0d60928)中，我们研究了一个通过自我对战学习游戏*SumTo100*的代理。这是一个简单的游戏，允许我们展示状态值，这帮助我们建立了关于代理如何学习游戏的直觉。对于井字棋，我们面对的是一个更大的状态空间。
- en: You can find the Python code in [this repository](https://github.com/sebastiengilbert73/tutorial_learnbyplay).
    The script that performs the training is[*learn_tictactoe.sh*](https://github.com/sebastiengilbert73/tutorial_learnbyplay/blob/main/learn_tictactoe.sh)*:*
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在 [这个仓库](https://github.com/sebastiengilbert73/tutorial_learnbyplay)中找到 Python
    代码。执行训练的脚本是 [*learn_tictactoe.sh*](https://github.com/sebastiengilbert73/tutorial_learnbyplay/blob/main/learn_tictactoe.sh)：
- en: '[PRE0]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The script loops through calls to two programs:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 该脚本循环调用两个程序：
- en: '[generate_positions_expectations.py](https://github.com/sebastiengilbert73/tutorial_learnbyplay/blob/main/preprocessing/generate_positions_expectations.py):
    Simulates matches and stores game states with the discounted expected return.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[generate_positions_expectations.py](https://github.com/sebastiengilbert73/tutorial_learnbyplay/blob/main/preprocessing/generate_positions_expectations.py)：模拟比赛并存储带有折现期望回报的游戏状态。'
- en: '[train_agent.py](https://github.com/sebastiengilbert73/tutorial_learnbyplay/blob/main/train/train_agent.py):
    Trains the neural network for a few epochs on the most recently generated dataset.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[train_agent.py](https://github.com/sebastiengilbert73/tutorial_learnbyplay/blob/main/train/train_agent.py)：在最新生成的数据集上对神经网络进行几个周期的训练。'
- en: The training cycle
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练周期
- en: 'The learning of the game by the agent proceeds through a cycle of generating
    matches and training to predict the match outcome from the game state:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 代理的游戏学习通过生成比赛和训练以预测比赛结果从游戏状态的周期进行：
- en: '![](../Images/2857c9ef68ed84a281a997ede02e21cb.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2857c9ef68ed84a281a997ede02e21cb.png)'
- en: 'Figure 1: The cycle of match generation and neural network training. Image
    by the author.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：比赛生成和神经网络训练的周期。图片由作者提供。
- en: Generation of matches
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 匹配的生成
- en: The cycle starts with the simulation of matches between random players, i.e.,
    players that choose randomly from the list of legal actions in a given game state.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 该周期以随机玩家之间的比赛模拟开始，即从给定游戏状态的合法动作列表中随机选择的玩家。
- en: Why are we generating matches played randomly?
  id: totrans-25
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 为什么我们要生成随机进行的比赛？
- en: This project is about learning by self-play, so we can’t give the agent any
    a priori information about *how to play*. In the first cycle, since the agent
    has no clue about good or bad moves, the matches must be generated by random play.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 这个项目涉及通过自我对战进行学习，因此我们不能给代理任何关于*如何玩*的先验信息。在第一个周期，由于代理对好坏动作一无所知，比赛必须通过随机对战生成。
- en: 'Figure 2 shows an example of a match between random players:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2 显示了随机玩家之间比赛的示例：
- en: '![](../Images/da8234cf38f237660b7fbdf3c20e3c8c.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/da8234cf38f237660b7fbdf3c20e3c8c.png)'
- en: 'Figure 2: Example of a tic-tac-toe match played randomly. Image by the author.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：随机进行的井字棋比赛示例。图片由作者提供。
- en: What lesson can we learn by observing this match? From the point of view of
    the ‘X’ player, we can assume this is an example of poor play since it concluded
    in a loss. We don’t know which move(s) is/are responsible for the defeat, so we’ll
    assume that **all the decisions** made by the ‘X’ player were bad. If some decisions
    were good, we bet on statistics (other simulations could go through a similar
    state) to rectify their predicted state value.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以从观察这个比赛中学到什么教训？从‘X’玩家的角度来看，我们可以认为这是一个糟糕的表现示例，因为它以失败告终。我们不知道哪些步棋导致了失败，所以我们假设**‘X’玩家做出的所有决策**都是错误的。如果有些决策是正确的，我们依靠统计数据（其他模拟可能会经历类似的状态）来纠正他们的预测状态值。
- en: The last action from player ‘X’ is given a value of -1\. The other actions receive
    a discounted negative value that geometrically decays by a factor of γ (gamma)
    ∈ [0, 1] as we go backward toward the first move.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 玩家‘X’的最后一个动作被赋值为-1。其他动作则根据一个几何递减因子γ（gamma）∈ [0, 1] 递减，作为我们回溯到第一次移动时的折扣负值。
- en: '![](../Images/c439c76cde3f0ff250db2afc3f0197f3.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c439c76cde3f0ff250db2afc3f0197f3.png)'
- en: 'Figure 3: Target values for game states. Image by the author.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：游戏状态的目标值。图片由作者提供。
- en: States from matches that resulted in a win receive similar positive discounted
    values. States drawn from draws are given a value of zero. The agent takes the
    point of view of both the first and the second player.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 从获胜的比赛中得到的状态会获得类似的正折扣值。平局的状态则赋值为零。代理从第一和第二玩家的角度来观察。
- en: The game states as tensors
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 游戏状态作为张量
- en: We need a tensor representation for the game state. We’ll use a [2x3x3] tensor
    where the first dimension represents the channels (0 for ‘X’ and 1 for ‘O’), and
    the two other dimensions are the rows and the columns. The occupancy of a (row,
    column) square gets encoded as a 1 in the (channel, row, column) entry.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要一个游戏状态的张量表示。我们将使用一个[2x3x3]张量，其中第一个维度表示通道（‘X’为0，‘O’为1），另外两个维度表示行和列。一个（行，列）方格的占用情况被编码为（通道，行，列）项中的1。
- en: '![](../Images/bc8d3134b11af7a2a484723be077ad5c.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bc8d3134b11af7a2a484723be077ad5c.png)'
- en: 'Figure 4: The game state representation by a [2x3x3] tensor. Image by the author.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：由一个[2x3x3]张量表示的游戏状态。图片由作者提供。
- en: The pairs of (state tensor, target value) obtained by the generation of matches
    constitute the dataset on which the neural network will train in each round. The
    dataset is built at the beginning of the cycle, taking advantage of the learning
    that has occurred in previous rounds. While the first round generates purely random
    play, the subsequent ones generate gradually more realistic matches.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 生成匹配所获得的（状态张量，目标值）对构成了神经网络在每轮训练时使用的数据集。数据集在周期开始时建立，利用了之前轮次中发生的学习。虽然第一轮生成完全随机的游戏，但随后的轮次会生成逐渐更逼真的匹配。
- en: Injecting randomness into the play
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 向游戏中注入随机性
- en: 'The first round of match generation opposes random players. The subsequent
    rounds oppose the agent to itself (hence “self-play”). The agent is equipped with
    a regression neural network trained to predict the match outcome, which allows
    it to choose the legal action that yields the highest expected value. To promote
    diversity, the agent chooses actions based on an epsilon-greedy algorithm: with
    probability (1-ε), the best action gets chosen; otherwise, a random action gets
    chosen.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 第一轮的比赛生成对抗随机玩家。随后的轮次则使代理对抗自身（因此称为“自我对弈”）。代理配备了一个回归神经网络，经过训练以预测比赛结果，这使其能够选择产生最高预期值的合法动作。为了促进多样性，代理基于epsilon贪婪算法选择动作：以概率（1-ε）选择最佳动作；否则选择随机动作。
- en: Training
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练
- en: 'Figure 5 shows the evolution of the validation losses along five epochs for
    a maximum of 17 training rounds:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 图5显示了在最多17轮训练中的五个周期内验证损失的演变：
- en: '![](../Images/918c2b226de14d865912cc13b549f8d3.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/918c2b226de14d865912cc13b549f8d3.png)'
- en: 'Figure 5: The evolution of the validation loss for various training round numbers.
    Image by the author.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：不同训练轮次下验证损失的演变。图片由作者提供。
- en: We can see that the first few training rounds show a fast decrease in the validation
    loss, and then there seems to be a plateau around a mean squared error loss of
    0.2\. This trend shows that the agent’s regression neural network gets better
    at predicting the outcome of a match played against itself, from a given game
    state. Since the actions from both players are non-deterministic, there is a limit
    to the predictability of the match outcome. That explains why the validation loss
    stops improving after some rounds.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，前几轮训练中验证损失快速下降，然后似乎在均方误差损失0.2左右出现了平稳期。这一趋势表明，代理的回归神经网络在预测对自身进行的比赛结果方面变得更好，因为动作是非确定性的，因此比赛结果的可预测性有限。这也解释了为什么验证损失在一些轮次后停止改进。
- en: Improvement from round to round
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从一轮到另一轮的改进
- en: With the game [*SumTo100*](https://medium.com/towards-data-science/training-an-agent-to-master-a-simple-game-through-self-play-88bdd0d60928)*,*
    we could represent the state on a 1D grid. However, with tic-tac-toe, we can’t
    directly display the state value evolution. One thing we can do to measure the
    improvement is to pit the agent against the previous version of itself and observe
    the difference between the wins and the losses.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 对于游戏[*SumTo100*](https://medium.com/towards-data-science/training-an-agent-to-master-a-simple-game-through-self-play-88bdd0d60928)*,*
    我们可以在1D网格上表示状态。然而，对于井字游戏，我们无法直接显示状态值的演变。我们可以做的一件事是让代理人与其先前版本对战，观察胜负差异。
- en: 'Using ε = 0.5 for the first action of both players and ε = 0.1 for the rest
    of the match, running 1000 matches per comparison, this is what we get:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 对于两位玩家的首个动作使用 ε = 0.5，对于其余比赛使用 ε = 0.1，每次比较运行1000场比赛，结果如下：
- en: '![](../Images/d436f33f9948cc6bbba94e340dd29473.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d436f33f9948cc6bbba94e340dd29473.png)'
- en: 'Figure 6: Comparison of the agent with its previous version. Image by the author.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：代理人与其先前版本的比较。图片由作者提供。
- en: The number of wins exceeded the number of losses (showing an improvement) until
    10 training rounds. After that, the agent didn’t improve from round to round.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 胜场数超过了负场数（显示了改进），直到第10轮训练。之后，代理人的表现没有随轮次改善。
- en: Test
  id: totrans-53
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 测试
- en: Time to see how our agent plays tic-tac-toe!
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 现在来看看我们的代理人如何玩井字游戏！
- en: One useful feature of having a regression neural network is the possibility
    of displaying the agent’s evaluation of every legal move. Let’s play a game against
    the agent, showing how it judges its options.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 回归神经网络的一个有用特性是可以显示代理人对每个合法动作的评估。让我们与代理人进行一场游戏，展示它如何判断自己的选择。
- en: Manual play
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 手动游戏
- en: 'The agent starts, playing ‘X’:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 代理人开始，进行‘X’：
- en: '![](../Images/c687722e5b02c5ca5227a569d58e4ba7.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c687722e5b02c5ca5227a569d58e4ba7.png)'
- en: 'Figure 7: A match played against the agent, with the action evaluations. Image
    by the author.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 图7：对抗代理人的比赛及其动作评估。图片由作者提供。
- en: That’s how you get brutally crushed by a soulless tic-tac-toe machine!
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是你如何被一个无灵魂的井字游戏机器无情击败！
- en: As soon as I put the ‘O’ in the (1, 0) square, the expected return increased
    from 0.142 to 0.419, and my fate was sealed.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 当我将‘O’放入(1, 0)方格时，预期回报从0.142增加到0.419，我的命运已定。
- en: 'Let’s see how it does when the agent plays second:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看当代理人第二个行动时的表现：
- en: '![](../Images/a9e4e2badfd31f24fe08fcbafa46a8ca.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a9e4e2badfd31f24fe08fcbafa46a8ca.png)'
- en: 'Figure 8: A match played against the agent, with action evaluations. Image
    by the author.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 图8：对抗代理人的比赛及其动作评估。图片由作者提供。
- en: It didn’t fall into the trap, and the match was a draw.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 它没有陷入陷阱，比赛以平局告终。
- en: Matches against a random player
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 对抗随机玩家的比赛
- en: 'If we [simulate](https://github.com/sebastiengilbert73/tutorial_learnbyplay/blob/main/test/tictactoe_run_multiple_games.py)
    a large number of matches against a random player, this is what we get:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们[模拟](https://github.com/sebastiengilbert73/tutorial_learnbyplay/blob/main/test/tictactoe_run_multiple_games.py)大量对抗随机玩家的比赛，结果如下：
- en: '![](../Images/42a2362e86da81ba20a4b78922b38929.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/42a2362e86da81ba20a4b78922b38929.png)'
- en: 'Figure 9: Results of 1000 matches against a random player. Image by the author.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 图9：对抗随机玩家的1000场比赛结果。图片由作者提供。
- en: Out of 1000 matches (the agent played first in half the matches), the agent
    won 950 matches, lost no one, and there were 50 draws. This is not proof that
    our agent is playing optimally, but it has certainly reached a decent level of
    play.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在1000场比赛中（代理人在一半比赛中先手），代理人赢得了950场比赛，没有输掉任何一场，平局50场。这不能证明我们的代理人表现最优，但它确实达到了一个不错的水平。
- en: Conclusion
  id: totrans-71
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: As a follow-up to [**Training an Agent to Master a Simple Game Through Self-Play**](/training-an-agent-to-master-a-simple-game-through-self-play-88bdd0d60928)
    where the game was easy to crack and the state space was small, we used the same
    technique to master tic-tac-toe. Although this is still a toy problem, the state
    space of tic-tac-toe is large enough that the agent’s regression neural network
    is required to find patterns in the state tensors. These patterns allow the generalization
    for unseen state tensors.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 作为[**通过自我对弈训练代理人掌握简单游戏**](/training-an-agent-to-master-a-simple-game-through-self-play-88bdd0d60928)的后续，其中游戏容易破解且状态空间较小，我们采用了相同的技术来掌握井字游戏。虽然这仍然是一个玩具问题，但井字游戏的状态空间足够大，需要代理人的回归神经网络在状态张量中找到模式。这些模式允许对未见过的状态张量进行泛化。
- en: The code is available [in this repository](https://github.com/sebastiengilbert73/tutorial_learnbyplay).
    Give it a try, and let me know what you think!
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 代码可以在[这个仓库](https://github.com/sebastiengilbert73/tutorial_learnbyplay)中找到。试试看，告诉我你的想法！
