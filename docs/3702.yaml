- en: Evaluation of Synthetic Time Series
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ÂéüÊñáÔºö[https://towardsdatascience.com/evaluation-of-synthetic-time-series-1b4fc4e2be39?source=collection_archive---------2-----------------------#2023-12-19](https://towardsdatascience.com/evaluation-of-synthetic-time-series-1b4fc4e2be39?source=collection_archive---------2-----------------------#2023-12-19)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Exploring various metrics for synthetic time series evaluation with hands-on
    code examples
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@an231?source=post_page-----1b4fc4e2be39--------------------------------)[![Alexander
    Nikitin](../Images/decc36cddc4c7a23952569e293e7d209.png)](https://medium.com/@an231?source=post_page-----1b4fc4e2be39--------------------------------)[](https://towardsdatascience.com/?source=post_page-----1b4fc4e2be39--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----1b4fc4e2be39--------------------------------)
    [Alexander Nikitin](https://medium.com/@an231?source=post_page-----1b4fc4e2be39--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ¬∑
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F5afaa29ee25a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fevaluation-of-synthetic-time-series-1b4fc4e2be39&user=Alexander+Nikitin&userId=5afaa29ee25a&source=post_page-5afaa29ee25a----1b4fc4e2be39---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----1b4fc4e2be39--------------------------------)
    ¬∑10 min read¬∑Dec 19, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F1b4fc4e2be39&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fevaluation-of-synthetic-time-series-1b4fc4e2be39&user=Alexander+Nikitin&userId=5afaa29ee25a&source=-----1b4fc4e2be39---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F1b4fc4e2be39&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fevaluation-of-synthetic-time-series-1b4fc4e2be39&source=-----1b4fc4e2be39---------------------bookmark_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: '*This blog post is available as a* [*jupyter notebook on GitHub*](https://github.com/AlexanderVNikitin/tsgm/blob/main/tutorials/evaluation.ipynb)
    *and is a part of* [*TSGM, a library for time series generative modeling*](https://github.com/AlexanderVNikitin/tsgm)*.*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Today, we will discuss the evaluation of synthetic time series datasets ‚Äî datasets
    that are artificially created to represent real data. Let‚Äôs say there is a synthetic
    dataset D* that is aimed at representing a real dataset D. It is essential to
    quantitatively evaluate how good these synthetic data are: Does D* represent D
    well? Are these data secure? Are these data valuable for downstream problems?
    In this tutorial, we‚Äôll journey through the methods used to quantitatively and
    qualitatively assess the quality of synthetic time series data.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e88a7f55a86866407f0c484546b07141.png)'
  prefs: []
  type: TYPE_IMG
- en: An example of original and synthetic sine data.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let‚Äôs consider two cases from [1] that describe possible usages of synthetic
    data:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Scenario 1.** An organization wishes to employ an outside agent to analyze
    sensitive data or study statistical methods for a given problem. Sharing real
    data can be complicated due to privacy or commercial concerns. Synthetic counterparts
    can provide a convenient solution to this problem.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Scenario 2.** An organization wishes to train a model on a relatively small
    dataset. However, the dataset is not sufficient for the desired quality of modeling.
    Such limited datasets can be augmented with synthetic data. This synthetic data,
    which must be similar to real data, aims to enhance the model‚Äôs performance or,
    in other cases, assist in model reliability tests.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Overall, we indicate and describe in this tutorial the following metrics:'
  prefs: []
  type: TYPE_NORMAL
- en: real data similarity (Sc. 1 and 2),
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: distance metric,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: discriminative metric,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Maximum mean discrepancy score
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 2\. predictive consistency (Sc. 1),
  prefs: []
  type: TYPE_NORMAL
- en: 3\. downstream effectiveness (Sc. 2),
  prefs: []
  type: TYPE_NORMAL
- en: 4\. privacy (Sc. 1),
  prefs: []
  type: TYPE_NORMAL
- en: 5\. diversity (Sc. 1 and Sc. 2),
  prefs: []
  type: TYPE_NORMAL
- en: 6\. Fairness (Sc. 1 and Sc.2),
  prefs: []
  type: TYPE_NORMAL
- en: 7\. Visual comparison (Sc. 1 and 2).
  prefs: []
  type: TYPE_NORMAL
- en: In TSGM, all metrics are neatly organized in `tsgm.metrics`. Dive into the details
    with [our comprehensive documentation](https://tsgm.readthedocs.io/en/latest/modules/root.html#metrics).
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let‚Äôs kickstart coding examples by installing tsgm:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '**Generating synthetic data.** Moving forward, we import tsgm, and load an
    exemplary dataset. A tensor `Xr` now will contain 100 sine time or constant time
    series (based on target class `yr`). We will use `(Xr, yr)` as a **real** (= historical
    = original) dataset. `Xs` contains synthetic data generated by a variational autoencoder.
    *(Note: we use only one epoch for demonstration; increase the number of epochs
    and check training convergence for practical applications).*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Similarity with Real Data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Distance Metric
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Starting off, it is convenient to measure similarity between real and synthetic
    data. One approach to doing this is calculate the distance between a vector of
    summary statistics of synthetic data and real data.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8ebdb2e72519e7a014038e2fc4b3e2be.png)'
  prefs: []
  type: TYPE_IMG
- en: The smaller the distance, the closer the synthetic data align with the realism
    of the actual data. Now, let‚Äôs define a set of statistics that will serve as the
    foundation for our distance metric. Methods `tsgm.metrics.statistics.axis_*_s`
    calculate statistics `*` over the provided axis.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Moving forward, let us establish the distance metric. For simplicity‚Äôs sake,
    we will opt for the Euclidean norm.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Bringing it all together, we will utilize `tsgm.metrics.DistanceMetric` object.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: MMD metric
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: An alternative approach involves comparing synthetic and real data distributions.
    In this context, the use of Maximum Mean Discrepancy (MMD) [3] proves to be convenient.
    MMD serves as a non-parametric two-sample test to determine if samples are drawn
    from the same distribution. Through empirical observations, we have identified
    the MMD metric as a particularly convenient method for assessing the similarity
    of real data.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Discriminative metric
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this approach, a model is trained to distinguish between real and synthetic
    data. In TSGM, `tsgm.metrics.DiscriminativeMetric`proves to be a valuable tool
    for this purpose. This metric facilitates the assessment of how effectively a
    model can discriminate between real and synthetic datasets, providing an additional
    perspective on data similarity.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Consistency Metric
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Next, we move on to consistency metric. The idea is aligned with **Scenario
    1** written above. Here, our focus is on gauging the consistency of a set of downstream
    models. In more detail, let us consider a set of models ‚Ñ≥, and an evaluator E:
    ‚Ñ≥ √ó ùíü ‚Üí ‚Ñù.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To evaluate consistency of ‚Ñ≥ on D and D*, we measure p(m‚ÇÅ ‚àº m‚ÇÇ| m‚ÇÅ, m‚ÇÇ ‚àà ‚Ñ≥,
    D, D*) where m‚ÇÅ ‚àº m‚ÇÇ means m‚ÇÅ consistent with m‚ÇÇ: ‚Äúif m‚ÇÅ outperforms m‚ÇÇ where
    the models trained on D, then it outperforms m‚ÇÇ on D* and vice versa.‚Äù Estimating
    this probability involves fixing a finite set ‚Ñ≥ and evaluating the models using
    real data, and separately evaluating them using synthetic data.'
  prefs: []
  type: TYPE_NORMAL
- en: In TSGM, our first step is to define a set of evaluators. For this purpose,
    we‚Äôll leverage a collection of LSTM models, ranging from one to three LSTM blocks.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Downstream Performance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now, let‚Äôs explore how generated data can contribute to improving predictive
    performance in a specific downstream problem. We‚Äôll consider two distinct approaches
    to evaluating downstream performance:'
  prefs: []
  type: TYPE_NORMAL
- en: 1\. **Augmenting Real Data with Synthetic Data.**
  prefs: []
  type: TYPE_NORMAL
- en: This approach proves valuable when data are limited. By supplementing real data
    with generated counterparts, we aim to enhance the training set for improved model
    performance. See our blogpost on data augmentations [here](https://medium.com/@an231/time-series-augmentations-16237134b29b)
    [2].
  prefs: []
  type: TYPE_NORMAL
- en: 2\. **Utilizing Generated Data Exclusively for Downstream Model Training.**
  prefs: []
  type: TYPE_NORMAL
- en: In scenarios where real data are scarce and private, this approach comes into
    play. Here, the downstream model is trained solely on the generated data and subsequently
    evaluated on real data.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: The result signifies accuracy gain from augmenting with synthetic data compared
    to the model trained exclusively on the training data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Privacy: Membership Inference Attack Metric'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/cd059109e90b1369ad28b0dea525824e.png)'
  prefs: []
  type: TYPE_IMG
- en: Membership inference attack visualization.
  prefs: []
  type: TYPE_NORMAL
- en: One approach to measuring privacy of synthetic data is measuring the susceptibility
    to membership inference attacks. Membership inference attack procedure is visually
    depicted in the figure above. The idea is the following. Imagine an attacker who
    has access to synthetic data and a particular data sample (which may or may not
    exist in the original dataset). The goal is to **infer** whether this sample is
    present in the real data.
  prefs: []
  type: TYPE_NORMAL
- en: '`tsgm.metrics.PrivacyMembershipInferenceMetric` measures the susceptibility
    to membership inference attacks using synthetic data. The step-by-step evaluation
    procedure is outlined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 1\. **Data Splitting**. Split the historical data into training and hold-out
    sets (denoted as D‚Çú and D‚Çï),
  prefs: []
  type: TYPE_NORMAL
- en: 2\. **Generative Model Training.** Train a generative model on D‚Çú and generate
    a synthetic dataset D*,
  prefs: []
  type: TYPE_NORMAL
- en: 3\. **One-Class Classification (OCC) Model Training.** Train a one-class classification
    (OCC) model on synthetic data D* and evaluate it on D‚Çú and D‚Çï,
  prefs: []
  type: TYPE_NORMAL
- en: 4\. **Target Score Calculation.** Use one minus the precision of the OCC model
    as the target score.
  prefs: []
  type: TYPE_NORMAL
- en: This evaluation process provides insights into the potential vulnerability to
    membership inference attacks leveraging synthetic data.
  prefs: []
  type: TYPE_NORMAL
- en: Let‚Äôs now introduce an attacker model. For the sake of demonstration, we‚Äôll
    define a one-class SVM classifier.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let‚Äôs define a test set and measure privacy metric:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Diversity
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With this metric, our goal is to quantify the diversity of synthetic data. Consider
    the image below, where red dots represent real data and blue dots signify synthetic
    data. Which option yields a superior synthetic dataset? The one on the right appears
    more favorable, but why? The answer lies in its diversity, making it potentially
    more versatile and useful. However, diversity alone is not sufficient; it‚Äôs crucial
    to consider other metrics in tandem, such as distance or downstream performance.
    In our exploration, we‚Äôll exemplify the concept using entropy.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d01738606e2f68c6cb465c09e203d557.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Synthetic data in 2D. A: non-diverse synthetic data; B: diverse synthetic data.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Fairness
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The topic of fairness intersects with synthetic time series generation in two
    significant ways. Firstly, it‚Äôs crucial to assess whether synthetic data introduces
    new biases. Secondly, synthetic data presents an opportunity to mitigate biases
    inherent in the original data. Defining standardized procedures for checking fairness
    proves challenging, as it often hinges on the specifics of downstream problems.
    Some illustrative metrics for measuring fairness encompass demographic parity,
    predictive rate parity paradigms, and equality of opportunity.
  prefs: []
  type: TYPE_NORMAL
- en: 'Take, for instance, equality of opportunity. Equality of opportunity serves
    as a fairness metric designed to gauge whether, for a preferred label (one that
    confers an advantage or benefit to a person) and a given attribute, a classifier
    predicts that preferred label equally well for all values of that attribute [6].
    This metric is instrumental in ensuring impartiality and equal treatment across
    diverse attribute values. A great example of this metric is provided in [6]: ‚ÄúSuppose
    Glubbdubdrib University admits both Lilliputians and Brobdingnagians to a rigorous
    mathematics program. Lilliputians‚Äô secondary schools offer a robust curriculum
    of math classes, and the vast majority of students are qualified for the university
    program. Brobdingnagians‚Äô secondary schools don‚Äôt offer math classes at all, and
    as a result, far fewer of their students are qualified. Equality of opportunity
    is satisfied for the preferred label of ‚Äúadmitted‚Äù with respect to nationality
    (Lilliputian or Brobdingnagian) if qualified students are equally likely to be
    admitted irrespective of whether they‚Äôre a Lilliputian or a Brobdingnagian.‚Äù'
  prefs: []
  type: TYPE_NORMAL
- en: Qualitative analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In order to evaluate the data qualitatively, it is convenient:'
  prefs: []
  type: TYPE_NORMAL
- en: a. draw samples and visualize individual samples from synthetic and real data,
  prefs: []
  type: TYPE_NORMAL
- en: 'b. build embeddings of the generated samples and visualize them using, for
    instance, TSNE. Let‚Äôs exemplify (b) with TSGM:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/65a9696637a67f6d3a280ba9ed8028dc.png)'
  prefs: []
  type: TYPE_IMG
- en: TSNE visualization of original and synthetic time series data.
  prefs: []
  type: TYPE_NORMAL
- en: Citation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This blog post is a part of the project TSGM, in which we are creating a tool
    for enhancing time series pipelines via augmentation and synthetic data generation.
    If you found it helpful, take a look at our repo and consider citing the paper
    about TSGM:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In conclusion, we‚Äôve explored various evaluation techniques for synthetic time
    series data, providing a comprehensive overview of different scenarios. To navigate
    through these methods effectively, it‚Äôs beneficial to consider the described scenarios.
    Ultimately, selecting the right metric is contingent on the downstream problem,
    application area, and legal regulations governing the data in use. The diverse
    set of metrics provided aims to assist in crafting a comprehensive evaluation
    pipeline tailored to your specific problem.
  prefs: []
  type: TYPE_NORMAL
- en: 'References:'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[1] Nikitin, A., Iannucci, L. and Kaski, S., 2023\. TSGM: A Flexible Framework
    for Generative Modeling of Synthetic Time Series. arXiv preprint arXiv:2305.11567\.
    [Arxiv Link](https://arxiv.org/pdf/2305.11567.pdf).'
  prefs: []
  type: TYPE_NORMAL
- en: '[2] Time Series Augmentations, TowardsDataScience post, [https://medium.com/towards-data-science/time-series-augmentations-16237134b29b](https://medium.com/towards-data-science/time-series-augmentations-16237134b29b).'
  prefs: []
  type: TYPE_NORMAL
- en: '[3] Gretton, A., Borgwardt, K.M., Rasch, M.J., Sch√∂lkopf, B. and Smola, A.,
    2012\. A kernel two-sample test. The Journal of Machine Learning Research, 13(1),
    pp.723‚Äì773\. [JMLR Link](https://jmlr.csail.mit.edu/papers/v13/gretton12a.html).'
  prefs: []
  type: TYPE_NORMAL
- en: '[4] Wen, Q., Sun, L., Yang, F., Song, X., Gao, J., Wang, X. and Xu, H., 2020\.
    Time series data augmentation for deep learning: A survey. arXiv preprint arXiv:2002.12478\.
    [Arxiv Link](https://arxiv.org/pdf/2002.12478.pdf).'
  prefs: []
  type: TYPE_NORMAL
- en: '[5] Wattenberg, M., Vi√©gas, F. and Hardt, M., 2016\. Attacking discrimination
    with smarter machine learning. Google Research, 17\. [Google Research Link](http://research.google.com/bigpicture/attacking-discrimination-in-ml/).'
  prefs: []
  type: TYPE_NORMAL
- en: '[6] Machine Learning Glossary: Fairness. Google Developers Blog.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Google Developers Blog Link](https://developers.google.com/machine-learning/glossary/fairness).'
  prefs: []
  type: TYPE_NORMAL
- en: '*Unless otherwise noted, all images are by the author. For additional materials
    on synthetic time series generation, see* [*TSGM on GitHub*](https://github.com/alexandervnikitin/tsgm)*,
    and* [*subscribe to Medium posts*](https://an231.medium.com/subscribe)*.*'
  prefs: []
  type: TYPE_NORMAL
