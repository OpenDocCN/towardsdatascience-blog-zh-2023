- en: 'Double Machine Learning, Simplified: Part 1 — Basic Causal Inference Applications'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 双重机器学习简化版：第1部分 — 基本的因果推断应用
- en: 原文：[https://towardsdatascience.com/double-machine-learning-simplified-part-1-basic-causal-inference-applications-3f7afc9852ee?source=collection_archive---------2-----------------------#2023-07-12](https://towardsdatascience.com/double-machine-learning-simplified-part-1-basic-causal-inference-applications-3f7afc9852ee?source=collection_archive---------2-----------------------#2023-07-12)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/double-machine-learning-simplified-part-1-basic-causal-inference-applications-3f7afc9852ee?source=collection_archive---------2-----------------------#2023-07-12](https://towardsdatascience.com/double-machine-learning-simplified-part-1-basic-causal-inference-applications-3f7afc9852ee?source=collection_archive---------2-----------------------#2023-07-12)
- en: '![](../Images/da07206f53b52134eaecbb622ac56914.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/da07206f53b52134eaecbb622ac56914.png)'
- en: All Images By Author
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 所有图片由作者提供
- en: Learn how to utilize DML in causal inference tasks
  id: totrans-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 学习如何在因果推断任务中利用DML
- en: '[](https://medium.com/@jakepenzak?source=post_page-----3f7afc9852ee--------------------------------)[![Jacob
    Pieniazek](../Images/2d9c6295d39fcaaec4e62f11c359cb29.png)](https://medium.com/@jakepenzak?source=post_page-----3f7afc9852ee--------------------------------)[](https://towardsdatascience.com/?source=post_page-----3f7afc9852ee--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----3f7afc9852ee--------------------------------)
    [Jacob Pieniazek](https://medium.com/@jakepenzak?source=post_page-----3f7afc9852ee--------------------------------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@jakepenzak?source=post_page-----3f7afc9852ee--------------------------------)[![Jacob
    Pieniazek](../Images/2d9c6295d39fcaaec4e62f11c359cb29.png)](https://medium.com/@jakepenzak?source=post_page-----3f7afc9852ee--------------------------------)[](https://towardsdatascience.com/?source=post_page-----3f7afc9852ee--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----3f7afc9852ee--------------------------------)
    [Jacob Pieniazek](https://medium.com/@jakepenzak?source=post_page-----3f7afc9852ee--------------------------------)'
- en: ·
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F6f0948d99b1c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdouble-machine-learning-simplified-part-1-basic-causal-inference-applications-3f7afc9852ee&user=Jacob+Pieniazek&userId=6f0948d99b1c&source=post_page-6f0948d99b1c----3f7afc9852ee---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----3f7afc9852ee--------------------------------)
    ·17 min read·Jul 12, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F3f7afc9852ee&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdouble-machine-learning-simplified-part-1-basic-causal-inference-applications-3f7afc9852ee&user=Jacob+Pieniazek&userId=6f0948d99b1c&source=-----3f7afc9852ee---------------------clap_footer-----------)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F6f0948d99b1c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdouble-machine-learning-simplified-part-1-basic-causal-inference-applications-3f7afc9852ee&user=Jacob+Pieniazek&userId=6f0948d99b1c&source=post_page-6f0948d99b1c----3f7afc9852ee---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----3f7afc9852ee--------------------------------)
    ·17分钟阅读·2023年7月12日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F3f7afc9852ee&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdouble-machine-learning-simplified-part-1-basic-causal-inference-applications-3f7afc9852ee&user=Jacob+Pieniazek&userId=6f0948d99b1c&source=-----3f7afc9852ee---------------------clap_footer-----------)'
- en: --
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F3f7afc9852ee&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdouble-machine-learning-simplified-part-1-basic-causal-inference-applications-3f7afc9852ee&source=-----3f7afc9852ee---------------------bookmark_footer-----------)'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F3f7afc9852ee&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdouble-machine-learning-simplified-part-1-basic-causal-inference-applications-3f7afc9852ee&source=-----3f7afc9852ee---------------------bookmark_footer-----------)'
- en: This article is the **1st** in a 2 part series on simplifying and democratizing
    Double Machine Learning. In the 1st part, we will be covering the fundamentals
    of Double Machine Learning, along with two basic causal inference applications
    in python. Then, in [pt. 2](https://medium.com/towards-data-science/double-machine-learning-simplified-part-2-extensions-the-cate-99926151cac),
    we will extend this knowledge to turn our causal inference problem into a prediction
    task, wherein we predict individual level treatment effects to aid in decision
    making and data-driven targeting.
  id: totrans-10
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 本文是**2部分**系列中的**第1部分**，旨在简化和普及双重机器学习。在第1部分中，我们将覆盖双重机器学习的基础知识，以及两个基本的因果推断应用示例（使用python）。然后，在[第2部分](https://medium.com/towards-data-science/double-machine-learning-simplified-part-2-extensions-the-cate-99926151cac)中，我们将扩展这些知识，将因果推断问题转化为预测任务，其中我们预测个体级别的处理效果，以辅助决策和数据驱动的目标定位。
- en: The conceptual & practical distinctions between statistical/machine learning
    (ML) and causal inference/econometric (CI) tasks have been established for years—
    ML seeks to predict, whereas CI seeks to infer a treatment effect or a “causal”
    relationship between variables. However, it was, and still is, common for the
    data scientist to draw causal conclusions from parameters of a trained machine
    learning model, or some other interpretable ML methodology. Despite this, there
    has been significant strides in industry and across many academic disciplines
    to push more rigorousness in making causal claims, and this has stimulated a much
    wider and open discourse on CI. In this stride, we have seen amazing work come
    out that has begun to bridge the conceptual gap between ML and CI, specifically
    tools in CI that take advantage of the power of ML methodologies.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 统计/机器学习（ML）与因果推断/计量经济学（CI）任务之间的概念性和实践性区别已经存在多年——ML旨在预测，而CI旨在推断处理效果或变量之间的“因果”关系。然而，数据科学家仍然常常从训练的机器学习模型或其他可解释的ML方法中得出因果结论。尽管如此，业界和许多学术学科在推动因果声明的严谨性方面取得了显著进展，这也刺激了因果推断领域的广泛讨论。在这一进展中，我们看到了一些惊人的工作，开始弥合ML与CI之间的概念差距，特别是CI工具利用了ML方法的强大能力。
- en: 'The primary motivation for this series is to democratize the usage of & applications
    of Double Machine Learning (DML), first introduced by Chernozhukov *et al.* in
    their pioneering paper “Double Machine Learning for Treatment and Causal Parameters”,
    and to enable the data scientist to utilize DML in their daily causal inference
    tasks.[1] In doing so, we will first dive into the fundamentals of DML. Specifically,
    we will cover some of the conceptual/theoretical underpinnings, including the
    regression framework for causality & the [Frisch-Waugh-Lovell Theorem](https://en.wikipedia.org/wiki/Frisch%E2%80%93Waugh%E2%80%93Lovell_theorem),
    and then we will use this framework to develop DML. Lastly, we will demonstrate
    two notable applications of Double Machine Learning:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 本系列的主要动机是使双重机器学习（DML）的使用和应用变得普及，这一方法首次由Chernozhukov *等人*在其开创性论文《用于处理和因果参数的双重机器学习》中介绍，并使数据科学家能够在日常因果推断任务中利用DML。[1]
    为此，我们将首先深入探讨DML的基础知识。具体而言，我们将涵盖一些概念/理论基础，包括因果关系的回归框架及[Frisch-Waugh-Lovell定理](https://en.wikipedia.org/wiki/Frisch%E2%80%93Waugh%E2%80%93Lovell_theorem)，然后我们将利用这一框架来发展DML。最后，我们将展示双重机器学习的两个显著应用：
- en: '*Converging towards Exogeneity/CIA/Ignorability in our Treatment given Non-Experimental/Observational
    Data (particularly when our set of covariates is of high dimensionality),* and'
  id: totrans-13
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*在我们的处理过程中趋向于外生性/CIA/可忽略性（尤其是当我们的协变量集具有高维度时）*'
- en: '*Improving Precision & Statistical Power in Experimental Data (Randomized Controlled
    Trial’s (RCTs) or A/B Tests)*'
  id: totrans-14
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*在实验数据（随机对照试验（RCTs）或A/B测试）中提高精确度和统计功效*'
- en: If this already all feels extremely foreign, I recommend checking out my [previous
    article](https://medium.com/towards-data-science/controlling-for-x-9cb51652f7ad)
    that covers the regression framework for causality and the Frisch-Waugh-Lovell
    Theorem. Nevertheless, I will cover these topic below and do my best to simplify
    and make this accessible to all. Let’s first dive into a quick overview of these
    theoretical underpinnings!
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 如果这些内容对你来说仍感到非常陌生，我推荐查看我的[上一篇文章](https://medium.com/towards-data-science/controlling-for-x-9cb51652f7ad)，其中涵盖了因果回归框架和Frisch-Waugh-Lovell定理。尽管如此，我会在下面涵盖这些主题，并尽力简化并使其对所有人都易于理解。让我们首先快速概述一下这些理论基础！
- en: '**Regression Framework for Causality & the FWL Theorem**'
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**因果回归框架与FWL定理**'
- en: 'The gold standard for establishing causality is an RCT or A/B test, wherein
    we randomly assign a subset of individuals to receive treatment, ***T***, (the
    test group) and others to not receive treatment (the control group), or a different
    treatment (in “A/B” testing). To estimate the average treatment effect (ATE) of
    treatment on outcome ***y***, we can estimate the following bivariate linear regression:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 确立因果关系的金标准是RCT或A/B测试，在这些测试中，我们随机分配一部分个体接受处理，***T***（测试组），而其他个体则不接受处理（对照组）或接受不同的处理（在“A/B”测试中）。为了估计处理对结果***y***的平均处理效应（ATE），我们可以估计以下双变量线性回归：
- en: '![](../Images/87b4dd0faf84275195a1089a75aeeeac.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/87b4dd0faf84275195a1089a75aeeeac.png)'
- en: (1)
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: (1)
- en: 'Because treatment is randomly assigned, we ensure that treatment is exogenous;
    that is, independent of the error term, ε, and thus there exists not confounders
    (a variable that effects both treatment and the outcome) that we have not controlled
    for— cov(T,ε)=0 (e.g., suppose, as a violation, y = earnings & T = years of education,
    then we can anticipate a variable such as IQ in ε to confound the true relationship).
    Because of this independence, the coefficient estimate on ***T*** takes on a causal
    interpretation — the ATE:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 由于处理是随机分配的，我们确保处理是外生的；即，独立于误差项ε，因此不存在我们未控制的混杂变量（影响处理和结果的变量）—— cov(T,ε)=0（例如，假设违反情况下，y
    = 收入 & T = 教育年限，那么我们可以预期例如IQ这样的变量会在ε中混杂真实关系）。由于这种独立性，***T***上的系数估计具有因果解释——ATE：
- en: '![](../Images/213cbd04b9c2a4f3c5ffa3c312ca5a9d.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/213cbd04b9c2a4f3c5ffa3c312ca5a9d.png)'
- en: (2) Discrete Treatment
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: (2) 离散处理
- en: '![](../Images/3e46a068bc011e05cddc440fe236c909.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3e46a068bc011e05cddc440fe236c909.png)'
- en: (3) Continuous Treatment
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: (3) 连续处理
- en: 'When we are dealing with non-experimental or observational data, it is almost
    always the case that the treatment of interest is *not* independent of ε, or endogenous
    — cov(T,ε)≠0, and there exists confounders that we have not accounted for. In
    other words, we no longer can parse out the true random variation in our treatment
    to explain our outcome. In this case, a simple bivariate regression will result
    in a biased estimate of the ATE (β (true ATE) + bias) due to [omitted variable
    bias](https://en.wikipedia.org/wiki/Omitted-variable_bias). However, if we can
    control for all possible confounders, ***X****, and the confounding functional
    form if using parametric models*, we can achieve exogeneity in our treatment,
    or what is also known as the conditional independence assumption (CIA), or [Ignorability](https://en.wikipedia.org/wiki/Ignorability).
    In other words again, the remaining variation in our treatment is “as good as
    random”. That is, there are no remaining confounders in the error term, or:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们处理非实验或观察数据时，几乎总是处理变量*不是*与ε独立的，或者是内生的——cov(T,ε)≠0，并且存在我们未考虑的混杂变量。换句话说，我们不能再将处理中的真实随机变化与结果解释分开。在这种情况下，简单的双变量回归将因[遗漏变量偏倚](https://en.wikipedia.org/wiki/Omitted-variable_bias)而导致ATE的偏倚估计（β（真实ATE）+偏倚）。然而，如果我们能控制所有可能的混杂变量，***X***，以及在使用参数模型时的混杂函数形式，我们可以在我们的处理上实现外生性，或者也称为条件独立假设（CIA），或[可忽略性](https://en.wikipedia.org/wiki/Ignorability)。换句话说，处理中的剩余变化是“好像是随机的”。即，在误差项中没有剩余的混杂因素，或者：
- en: '![](../Images/3a1755f6c8ffa8a77aa93b329115582c.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3a1755f6c8ffa8a77aa93b329115582c.png)'
- en: (4)
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: (4)
- en: 'If exogeneity holds (there are no confounders outside of ***X***), then controlling
    for ***X*** in a multiple regressionallows for the coefficient estimate on ***T***
    to take on the similar causal interpretation of the ATE:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 如果外生性成立（***X***之外没有混杂变量），那么在多元回归中控制***X***允许***T***上的系数估计具有类似的ATE因果解释：
- en: '![](../Images/477994ca9c5169f96e9c1b94ac8a385a.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/477994ca9c5169f96e9c1b94ac8a385a.png)'
- en: (5)
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: (5)
- en: '**Warning:** It is not best practice to control for every possible covariate,
    but rather variables that are hypothesized/known to influence both the outcome,
    y, and treatment of interest, T. This is the concept of a [confounder](https://en.wikipedia.org/wiki/Confounding).
    Conversely, if both y and T influence a variable, we do not want to control for
    this variable, as this can introduce a spurious association between y and T. This
    is the concept of a [collider](https://en.wikipedia.org/wiki/Collider_(statistics))
    variable. We will show an example of this in action further in this article. Additionally,
    we do not want to include variables that are mediators of our treatment; that
    is, a covariate that is impacted by the treatment that in turn impacts the outcome.
    The inclusion of this mediator variable can eat away at the estimate of our treatment
    effect. In short, we only want to include confounders (and, possibly, non-mediator
    & non-collider predictors of y to improve precision; this is discussed in example
    2 below).'
  id: totrans-31
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**警告：** 控制所有可能的协变量不是最佳实践，而是控制那些被假设/已知会影响结果***y***和感兴趣的处理***T***的变量。这是[混淆变量](https://en.wikipedia.org/wiki/Confounding)的概念。相反，如果***y***和***T***都影响某个变量，我们不希望控制该变量，因为这可能会引入***y***和***T***之间的虚假关联。这是[碰撞变量](https://en.wikipedia.org/wiki/Collider_(statistics))的概念。我们将在本文后面展示这一点的实际例子。此外，我们不希望包括那些是我们处理变量的中介变量的变量；即，一个受处理影响进而影响结果的协变量。包括这种中介变量可能会侵蚀我们处理效应的估计。简而言之，我们只希望包括混淆变量（以及可能的非中介和非碰撞预测变量以提高精确度；这将在下面的例子2中讨论）。'
- en: However, in practice, exogeneity/CIA/Ignorability is very difficult to obtain
    and justify as it is unlikely that we will be able to observe every confounder
    and control for potential non-linear relationships these confounders may take
    on. This provides one particular motivation for DML — however, let’s first discuss
    the FWL theorem, as this allows us to theoretically develop DML.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在实践中，外生性/CIA/可忽略性非常难以获得和证明，因为我们不太可能观察到所有的混淆变量并控制这些混淆变量可能出现的潜在非线性关系。这是DML的一个特定动机——然而，让我们首先讨论FWL定理，因为这使我们能够理论上开发DML。
- en: 'The [FWL theorem](https://en.wikipedia.org/wiki/Frisch%e2%80%93Waugh%e2%80%93Lovell_theorem)
    is a notable econometric theorem that allows us to obtain the ***identical***
    ATE parameter estimate, β₁, on the treatment, ***T***, in the multiple regression
    above (eq. 5) utilizing the following 3 step procedure:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '[FWL定理](https://en.wikipedia.org/wiki/Frisch%e2%80%93Waugh%e2%80%93Lovell_theorem)是一个重要的计量经济学定理，它允许我们在上述多重回归（方程5）中利用以下3步程序获得***相同的***ATE参数估计β₁，关于处理变量***T***：'
- en: Separately regress ***y*** on ***X*** and ***T*** on ***X***
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 分别对***y***进行回归***X***，对***T***进行回归***X***
- en: Save the residuals from step 1— call it ***y**** and ***T****
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 保存第1步的残差——称之为***y***和***T***。
- en: Regress ***y**** on ***T****
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对***y***进行回归***T***
- en: In quasi- python code,
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在准Python代码中，
- en: '[PRE0]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Intuitively, the FWL theorem partials out the variation in ***T*** and ***y***
    that is explained by the confounders, ***X***, and then uses the remaining variation
    to explain the key relationship of interest (ie, how ***T*** effects ***y***).
    More specifically, it exploits a special type of orthogonal projection matrix
    of ***X*** known as an annihilator matrix or residual-maker matrix to residualize
    ***T*** and ***y***. For a hands-on application of the FWL procedure, see my [previous
    post](/controlling-for-x-9cb51652f7ad). This theorem is pivotal in understanding
    DML.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 直观地说，FWL定理将***T***和***y***中的变异部分从混淆变量***X***中分离出来，然后利用剩余的变异来解释关键的关系（即，***T***如何影响***y***）。更具体地说，它利用***X***的一种特殊类型的正交投影矩阵，称为消除矩阵或残差生成矩阵，来残差化***T***和***y***。有关FWL程序的实际应用，请参见我之前的[帖子](/controlling-for-x-9cb51652f7ad)。该定理对于理解DML至关重要。
- en: Note that I have (intentionally) glossed over some additional causal inference
    assumptions, such as Positivity/Common Support & SUTVA/Counterfactual Consistency.
    In general, the CIA/Ignorability assumption is the most common assumption that
    needs to be defended. However, it is recommended that the interested reader familiarize
    themselves with the additional assumptuons. In brief, Positivity ensures we have
    non-treated households that are similar & comparable to treated households to
    enable counterfactual estimation & SUTVA ensures there is no spillover/network
    type effects (treatment of one individual impacts another).
  id: totrans-40
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 请注意，我（故意）略过了一些额外的因果推断假设，例如 Positivity/Common Support 和 SUTVA/反事实一致性。一般来说，CIA/可忽略性假设是需要辩护的最常见假设。然而，建议感兴趣的读者熟悉额外的假设。简言之，Positivity
    确保我们有与处理过的家庭相似且可比较的未处理家庭，以便进行反事实估计，而 SUTVA 确保没有溢出/网络类型效应（一个个体的处理影响另一个个体）。
- en: Double Machine Learning… Simplified!
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 双重机器学习… 简化版！
- en: 'Double Machine Learning, at its core, allows for the residualization/orthogonalization
    done in steps 1) and 2) of the FWL procedure to be conducted using any highly
    flexible ML model, thus constructing a partially linear model. That is, we can
    estimate the ATE via:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 双重机器学习的核心是允许在 FWL 程序的步骤 1) 和 2) 中进行的残差化/正交化使用任何高度灵活的 ML 模型，从而构造一个部分线性模型。也就是说，我们可以通过以下方式估计
    ATE：
- en: '![](../Images/ff40e6bdeac9222544390e90e9b8ed1a.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ff40e6bdeac9222544390e90e9b8ed1a.png)'
- en: (6)
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: (6)
- en: 'where **𝑀𝑦** and ***MT*** are both any *ML* models to predict ***y*** and ***T***
    given confounders and/or controls, ***X***, respectively. **𝑀𝑦** and ***MT***
    are also known as the “nuisance functions” as we are constructing functions to
    partial out the variation in ***y*** and ***T*** explained by X, which is not
    of primary interest. To avoid overfitting and to ensure robustness in this approach,
    we use *cross-validation prediction* via sample- & cross-fitting. I believe it
    will be useful again here to see this procedure outlined in quasi- python code:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 其中**𝑀𝑦**和***MT***都是任何用于预测***y***和***T***的*ML*模型，给定混淆因素和/或控制变量***X***。**𝑀𝑦**和***MT***也被称为“干扰函数”，因为我们正在构造函数来部分去除***y***和***T***中由
    X 解释的变化，这不是主要关注的内容。为了避免过拟合并确保这种方法的稳健性，我们使用*交叉验证预测*通过样本和交叉拟合。我相信这里再次看到这种程序在准 Python
    代码中的示例会很有用：
- en: '[PRE1]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Where the coefficient on *T_residual* will be our estimated ATE, with asymptotically
    normal inference around our estimate. And, that’s it!
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *T_residual* 上的系数将是我们估计的 ATE，并且围绕我们的估计有渐近正态推断。就这样！
- en: This is the procedure behind DML for estimating the ATE. *It enables our modeling
    of the confounding to be highly flexible and non-parametric, particularly in the
    presence of a high dimensional set of covariates.*
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 这是 DML 估计 ATE 背后的过程。*它使我们的混淆建模具有高度的灵活性和非参数特性，特别是在存在高维协变量的情况下。*
- en: I will not dive too deep into the technicalities of why this works, and I will
    refer the interested reader to the [original paper](https://arxiv.org/pdf/1608.00060.pdf)
    and [EconML documentation](https://econml.azurewebsites.net/index.html). However,
    in brief, DML satisfies a condition known as Neyman Orthogonality (ie, small perturbations
    in the nuisance functions around the true value has second order effects on the
    moment condition and thus does not impact our key parameter estimate), which solves
    for regularization bias, and when combined with the cross-validation procedure
    in DML, which solves for overfitting bias, we ensure robustness in this method.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 我不会深入探讨为什么这有效的技术细节，并且我会将感兴趣的读者引导到[原始论文](https://arxiv.org/pdf/1608.00060.pdf)和[EconML
    文档](https://econml.azurewebsites.net/index.html)。然而，简而言之，DML 满足一个称为 Neyman 正交性的条件（即，干扰函数在真实值附近的小扰动对矩条件有二阶效应，因此不会影响我们的关键参数估计），这解决了正则化偏差的问题，并且与
    DML 中的交叉验证程序结合使用，解决了过拟合偏差，从而确保了该方法的稳健性。
- en: There are some very cool extensions on DML that will be covered in [part 2](https://medium.com/towards-data-science/double-machine-learning-simplified-part-2-extensions-the-cate-99926151cac)
    of the series, but for now let’s see this in action via two applications.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: DML 有一些非常酷的扩展将在系列的[第 2 部分](https://medium.com/towards-data-science/double-machine-learning-simplified-part-2-extensions-the-cate-99926151cac)中讨论，但现在我们先通过两个应用来查看它的实际效果。
- en: DML Applications
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: DML 应用
- en: '**Application 1:** *Converging towards Exogeneity/CIA/Ignorability in our Treatment
    given Non-Experimental/Observational Data*'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '**应用 1：** *在处理非实验/观察数据时，趋向于外生性/CIA/可忽略性*'
- en: 'Recall that we discussed how in the absence of randomized experimental data
    we must control for all potential confounders to ensure we obtain exogeneity in
    our treatment of interest. In other words, when we control for all potential confounders,
    our treatment is “as good as randomly assigned”. There are two primary problems
    that still persist here:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 回顾一下，我们讨论了在没有随机实验数据的情况下，我们必须控制所有潜在的混杂因素，以确保我们获得我们感兴趣的处理的外生性。换句话说，当我们控制了所有潜在的混杂因素时，我们的处理是“如同随机分配”。这里仍然存在两个主要问题：
- en: It is difficult, and impossible in some cases, to truly know all of the confounders
    and, furthermore, to obtain the data for all these confounders. Solutioning this
    involves strong institutional knowledge of the data generating process, careful
    construction of the causal model (i.e., building a [DAG](https://en.wikipedia.org/wiki/Directed_acyclic_graph)
    while evaluating potential confounders and avoiding colliders), and/or exploiting
    [quasi-experimental](https://en.wikipedia.org/wiki/Quasi-experiment) designs.
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 了解所有混杂因素并且获取所有这些混杂因素的数据是困难的，在某些情况下甚至是不可能的。解决这个问题涉及对数据生成过程的深入了解，仔细构建因果模型（即，构建一个[DAG](https://en.wikipedia.org/wiki/Directed_acyclic_graph)的同时评估潜在混杂因素并避免碰撞器），以及/或者利用[准实验](https://en.wikipedia.org/wiki/Quasi-experiment)设计。
- en: If we do take manage to take care of point 1, we still have to specify the correct
    parametric form of confounding, including interactions and higher-order terms,
    when utilizing a parametric model (such as in the regression framework). Simply
    including linear terms in a regression may not sufficiently control for the confounding.
    This is where DML steps in; it can flexibly partial out the confounding in a highly
    non-parametric fashion. This is particularly beneficial in saving the data scientist
    the trouble of directly modeling the functional forms of confounding, and allows
    more attention to be directed towards identifying and measuring the confounders.
    Let’s see how this works!
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果我们确实处理了第一点，我们仍然需要在使用参数模型（例如在回归框架中）时指定混杂的正确参数形式，包括交互项和高阶项。仅在回归中包含线性项可能不足以控制混杂。这就是DML发挥作用的地方；它可以以高度非参数的方式灵活地部分控制混杂。这特别有利于节省数据科学家直接建模混杂的函数形式的麻烦，并使更多的注意力可以集中在识别和测量混杂因素上。让我们看看这是如何工作的！
- en: 'Suppose, as a highly stylized example, we work for an e-commerce company and
    we are tasked with estimating the ATE of an individuals time spent on the website
    on their purchase amount, or sales, in the past month. However, further assume
    we only have observational data to work with, but we have measured all potential
    confounders (those variables that influence both time spent on the website and
    sales). Let this causal process be outlined via the following Directed Acyclic
    Graph (DAG):'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 假设，作为一个高度简化的例子，我们在一家电子商务公司工作，并且我们的任务是估计个人在网站上花费的时间对他们在过去一个月的购买金额或销售额的平均处理效应（ATE）。然而，进一步假设我们只有观察数据可以使用，但我们已经测量了所有潜在的混杂因素（那些影响网站停留时间和销售额的变量）。让这个因果过程通过以下的有向无环图（DAG）进行描述：
- en: '![](../Images/53ff709528289a0762c49af8579e9f7a.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/53ff709528289a0762c49af8579e9f7a.png)'
- en: 'Let the data generating process be as follows (*note that all values & data
    are chosen and generated arbitrarily for demonstrative purposes, and thus should
    not necessarily represent a large degree of real world intuition per se outside
    of our estimates of the ATE*):'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 假设数据生成过程如下（*请注意，所有值和数据都是为了演示目的而随意选择和生成的，因此不一定代表真实世界直观的很大程度，除非我们对ATE的估计*）：
- en: '[PRE2]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'By construction, our treatment of interest (hours spent on the website in the
    past month) and our outcome (sales in the past month) have the following confounders:
    Age, Number of Social Media Accounts, & Years Member of Website, and this confounding
    is arbitrarily non-linear. Furthermore, we can see that the constructed **ground
    truth for the ATE is $5** (outlined in the DGP for sales in the code above above).
    *That is, on average, for every additional hour the individual spends on the website,
    they spend an additional $5\.* Note, we also include a collider variable (a variable
    that is influenced by both time spent on the website and sales), which will be
    utilized for demonstration below on how this biases the ATE.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 根据构建，我们的兴趣处理（过去一个月在网站上花费的时间）和我们的结果（过去一个月的销售额）有以下混杂因素：年龄、社交媒体账户数量和网站会员年限，这种混杂是任意非线性的。此外，我们可以看到构建的**ATE
    真实值是 $5**（在上面的销售 DGP 中说明）。*也就是说，平均而言，每增加一小时的在网站上花费的时间，个人会多花 $5。* 注意，我们还包括了一个碰撞器变量（受网站花费时间和销售额影响的变量），该变量将在下面的演示中用于说明这种偏差如何影响
    ATE。
- en: 'To demonstrate the ability of DML to flexibly partial out the highly non-linear
    confounding, we will run the 4 following models:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 为了展示 DML 灵活部分化高度非线性混杂因素的能力，我们将运行以下 4 个模型：
- en: Naïve OLS of sales (y) on hours spent on the website (T)
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 销售（y）对网站上花费的时间（T）进行天真的 OLS 回归
- en: Multiple OLS of sales (y) on hours spent on the website (T) and linear terms
    of all of the confounders
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 销售（y）对网站上花费的时间（T）及所有混杂因素的线性项进行多重 OLS 回归
- en: OLS utilizing DML residualization procedure outlined in eq. (5)
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 DML 残差化过程的 OLS 回归，详见公式 (5)
- en: OLS utilizing DML residualization procedure, including collider variable
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 包括碰撞器变量的 DML 残差化过程的 OLS 回归
- en: 'The code of this is as follows:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 代码如下：
- en: '[PRE3]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'with the corresponding results (see code in appendix for creating this table):'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 对应的结果（见附录中的代码以创建此表）：
- en: '![](../Images/6d270c529361708fb89a92c20fbb0fb1.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6d270c529361708fb89a92c20fbb0fb1.png)'
- en: 'Recall our simulated source of truth for the ATE is **$5**. Notice that the
    only model that is able to capture this value is the DML procedure! We can see
    that the naïve model has a significant positive bias in the estimate, whereas
    controlling only for linear terms of the confounders in the multiple regression
    slightly reduces this bias. Additionally, the DML procedure w/ a collider demonstrates
    a negative bias; this negative association between sales and our treatment that
    arises from controlling for the collider can be ***loosely*** demonstrated/observed
    by solving for sales in our collider DGP as such:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 回忆一下我们模拟的 ATE 真实值是**$5**。注意，唯一能够捕捉这个值的模型是 DML 过程！我们可以看到，天真的模型在估计中有显著的正偏差，而仅对混杂因素的线性项进行控制的多重回归则稍微减少了这种偏差。此外，带有碰撞器的
    DML 过程展示了一个负偏差；通过在我们的碰撞器 DGP 中求解销售额，可以*松散地*演示/观察到销售与我们处理之间的这种负相关。
- en: '[PRE4]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: These results demonstrate the unequivocal power of using flexible, non-parametric
    ML models in the DML procedure for residualizing out the confounding! Pretty satisfying,
    no? **DML removes the necessity for correct parametric specification of the confounding
    DGP (given all of the confounders are controlled for)!**
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 这些结果展示了使用灵活的非参数 ML 模型在 DML 过程中去除混杂的明确能力！相当令人满意，对吧？ **DML 去除了对混杂 DGP 的正确参数化规格的必要性（前提是所有混杂因素都被控制）！**
- en: The careful reader will have noticed that we included arbitrary covariate ***Z***
    in our data generating process for sales. However, note that ***Z*** does not
    directly influence time spent on the website, thus it does not meet the definition
    of a confounder and thus has no impact on the results (outside of possibly improving
    the precision of the estimate — see application 2)
  id: totrans-73
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 细心的读者会注意到，我们在销售的生成过程中包括了任意的协变量 ***Z***。然而，注意到 ***Z*** 并不会直接影响在网站上花费的时间，因此它不符合混杂因素的定义，因此对结果没有影响（除了可能提高估计的精确度——见应用程序
    2）
- en: '**Application 2:** *Improving Precision & Statistical Power in Experimental
    Data (Randomized Controlled Trial’s (RCTs) or A/B Tests)*'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '**应用程序 2：** *提高实验数据的精确度和统计效能（随机对照试验 (RCTs) 或 A/B 测试）*'
- en: It is a common misconception that if one run’s an experiment with a *large enough*
    sample size, one can obtain sufficient [statistical power](https://en.wikipedia.org/wiki/Power_of_a_test)
    to accurately measure the treatment of interest. However, one commonly overlooked
    component in determining statistical power in an experiment, and ultimately the
    precision in the ATE estimate, is the variation in the outcome you are trying
    measure.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 一个常见的误解是，如果一个实验有一个*足够大的*样本量，就可以获得足够的[统计功效](https://en.wikipedia.org/wiki/Power_of_a_test)来准确测量感兴趣的处理。然而，确定实验中的统计功效以及最终ATE估计精度的一个常被忽视的因素是你试图测量的结果的变异性。
- en: 'For example, suppose we are interested in measuring the impact of a specific
    advertisement on an individuals purchase amount, and we anticipate the effect
    to be small, but non-trivial — say an ATE of $5\. However, suppose the standard
    deviation in individual sales is very large… perhaps, in the $100s or even $1000s.
    In this case, it may be difficult to accurately capture the ATE given this high
    variation —that is, we may obtain very low precision (large standard errors) in
    our estimate. However, capturing this ATE of $5 may be economically significant
    (if we run the experiment on 100,000 households, this can amount to $500,000).
    This is where DML can come to the rescue. Before we demonstrate this in action,
    let’s first visit the formula for the standard error of our ATE estimate from
    the simple regression in equation (1):'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设我们对某个特定广告对个人购买金额的影响感兴趣，并且我们预计效果较小，但并非微不足道——例如，ATE为$5。然而，假设个人销售额的标准差非常大……可能在$100甚至$1000的范围内。在这种情况下，由于这种高变异性，准确捕捉ATE可能会非常困难——也就是说，我们的估计可能会获得非常低的精度（大的标准误差）。然而，捕捉到$5的ATE可能在经济上是有意义的（如果我们对100,000户家庭进行实验，这可能达到$500,000）。这就是DML能够发挥作用的地方。在我们展示实际操作之前，让我们先查看方程（1）中简单回归的ATE估计的标准误差公式：
- en: '![](../Images/10bfc0c5637eea234212e656f6f7d254.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/10bfc0c5637eea234212e656f6f7d254.png)'
- en: (7)
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: (7)
- en: 'Here we observe that the standard error of our estimate is directly influenced
    by the size of our residuals (ε). What does this tell us then? If our treatment
    is randomized, we can include covariates in a multiple OLS or DML procedure, not
    to obtain exogeneity, but to reduce the variation in our outcome. More specifically,
    we can include variables that are strong predictors of our outcome to reduce the
    residuals and, consequently, the standard error of our estimate. Let’s take a
    look at this in action. First, assume the following DAG (note treatment is randomized
    so there are no confounders):'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们观察到我们估计的标准误差直接受到残差（ε）大小的影响。那么这告诉我们什么呢？如果我们的处理是随机化的，我们可以在多重普通最小二乘法（OLS）或双重机器学习（DML）程序中包含协变量，这样做的目的是减少我们结果的变异性，而不是获得外生性。更具体地说，我们可以包含那些对结果有强预测作用的变量，从而减少残差，并因此降低我们估计的标准误差。让我们来看一下这个实际应用。首先，假设以下有向无环图（DAG）（注意处理是随机化的，所以没有混杂因素）：
- en: '![](../Images/5a4415a300956d8869404a4abecf0e4f.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5a4415a300956d8869404a4abecf0e4f.png)'
- en: 'Furthermore, suppose the following DGP:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，假设以下数据生成过程（DGP）：
- en: '[PRE5]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Here again, we artificially simulate our **ground truth ATE of $5**. This time,
    however, we generate sales such that we have a very large variance, thus making
    it difficult to detect the $5 ATE.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们再次人工模拟**真实的ATE为$5**。不过这次，我们生成的销售数据具有非常大的方差，因此难以检测到$5的ATE。
- en: 'To demonstrate how the inclusion of covariates that are strong predictors of
    our outcome in the DML procedure greatly improve the precision of our ATE estimates,
    we will run the following 3 models:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 为了演示在DML程序中包含那些对我们的结果有强预测作用的协变量如何大大提高ATE估计的精度，我们将运行以下三个模型：
- en: Naïve OLS of sales (y) on *randomized* exposure to advertisement (T)
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 销售（y）对*随机*广告暴露（T）的简单OLS
- en: Multiple OLS of sales (y) on *randomized* exposure to advertisement (T) and
    linear terms of all of the sales predictors
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 销售（y）对*随机*广告暴露（T）及所有销售预测变量的线性项的多重OLS
- en: OLS utilizing DML residualization procedure outlined in eq. (5)
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用DML残差化程序的OLS，如方程（5）所述
- en: 'The code is as follows:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 代码如下：
- en: '[PRE6]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: You may notice that we include the ML model to predict advertisement exposure
    as well. This is primarily for consistency with the DML procedure. However, because
    we know advertisement exposure is random this is not entirely necessary, but I
    would recommend verifying the model in our example truly is unable to learn anything
    (i.e., in our case it should predict ~0.50 probability for all individuals, thus
    the residuals will maintain the same variation as initial treatment assignment).
  id: totrans-90
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 您可能会注意到我们还包括了用于预测广告曝光的ML模型。这主要是为了与DML程序保持一致。 然而，因为我们知道广告曝光是随机的，这并非完全必要，但我建议验证我们的示例模型确实无法学到任何东西（即，在我们的案例中，它应该为所有个体预测
    ~0.50 的概率，因此残差将保持与初始处理分配相同的变异）。
- en: 'With the corresponding results of these models (see code in appendix for creating
    this table):'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 这些模型的相应结果（请参阅附录中的代码以创建此表）：
- en: '![](../Images/853d63b779cac7d5151c932063b68296.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/853d63b779cac7d5151c932063b68296.png)'
- en: First, note that b/c treatment was randomly assigned, there is no true confounding
    that is occurring above. The poor estimates of the ATE in (1) and (2) are the
    direct result of imprecise estimates (see the large standard error’s in the parenthesis).
    Notice how the standard error gets smaller (precision increasing) as we move from
    (1)-(3), with the DML procedure having the most precise estimate. Draw your attention
    to the “Residual Std. Error” row outlined in the red box above. We can see how
    the DML procedure was able to greatly reduce the variation in the ATE model residuals
    via partialling out the variation that was able to be learnt (non-parametrically)
    from the predictors in the ML model of our outcome, sales. Again, in this example,
    we see DML being the only model to obtain the true ATE!
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，请注意，由于处理是随机分配的，因此上面没有发生真正的混淆。 (1) 和 (2) 中的ATE估计误差是由于不精确的估计（见括号中的大标准误差）直接导致的。
    注意随着我们从 (1) 到 (3) 迁移，标准误差如何变小（精度提高），其中DML程序具有最精确的估计。 请注意上面红框中的“残差标准误差”行。 我们可以看到DML程序通过从我们结果的ML模型（销售）中的预测因子中部分消除可学习的变异，显著减少了ATE模型残差的变异。
    再次，在这个例子中，我们看到DML是唯一一个能够获得真实ATE的模型！
- en: These results demonstrate the benefit of using DML in an experimental setting
    to increase statistical power and precision of one’s ATE estimate. Specifically,
    this can be utilized in RCT or A/B testing settings where the variation in the
    outcome is very large and/or one is struggling with achieving precise estimates
    and one has access to strong predictors of the outcome of interest.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 这些结果展示了在实验环境中使用DML以提高统计能力和ATE估计精度的好处。 具体而言，这可以应用于RCT或A/B测试环境，其中结果的变异非常大和/或在精确估计方面遇到困难，并且可以访问强有力的结果预测因子。
- en: Conclusion
  id: totrans-95
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结论
- en: And there you have it — Double Machine Learning simplified (hopefully)! Thank
    you for taking the time to read through my article. I hope this article has provided
    you with a clear and intuitive understanding of the basics of DML and **the true
    power DML holds**, along with how you can utilize DML in your daily causal inference
    tasks.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样——双重机器学习简化版（希望如此）！ 感谢您花时间阅读我的文章。 我希望这篇文章能为您提供对DML基础知识的清晰而直观的理解，以及**DML所具备的真正力量**，以及如何在日常因果推断任务中利用DML。
- en: Stay tuned for [part 2](https://medium.com/towards-data-science/double-machine-learning-simplified-part-2-extensions-the-cate-99926151cac)
    of this series where we will dive into some very cool extensions of DML that turn
    our causal inference problem into a prediction task, where we go beyond the ATE
    & predict individual level treatment effects to aid in decision making and data-driven
    targeting.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 敬请关注[第2部分](https://medium.com/towards-data-science/double-machine-learning-simplified-part-2-extensions-the-cate-99926151cac)系列，我们将深入探讨一些非常酷的DML扩展，这些扩展将我们的因果推断问题转化为预测任务，我们超越ATE，预测个体级别的处理效应，以辅助决策和数据驱动的目标。
- en: As always, I hope you have enjoyed reading this as much as I enjoyed writing
    it!
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 一如既往，我希望你们读这篇文章时和我写这篇文章时一样愉快！
- en: Appendix
  id: totrans-99
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录
- en: '*Creating the pretty tables:*'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '*创建漂亮的表格：*'
- en: '[PRE7]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Resources
  id: totrans-102
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 资源
- en: '[1] V. Chernozhukov, D. Chetverikov, M. Demirer, E. Duflo, C. Hansen, and a.
    W. Newey. Double Machine Learning for Treatment and Causal Parameters. *ArXiv
    e-prints*, July 2016.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] V. Chernozhukov, D. Chetverikov, M. Demirer, E. Duflo, C. Hansen, 和 a.
    W. Newey. 双重机器学习用于处理和因果参数。*ArXiv 电子印刷品*，2016年7月。'
- en: '*Access all the code via this GitHub Repo:* [https://github.com/jakepenzak/Blog-Posts](https://github.com/jakepenzak/Blog-Posts)'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: '*I appreciate you reading my post! My posts on Medium seek to explore real-world
    and theoretical applications utilizing* ***econometric*** *and* ***statistical/machine
    learning*** *techniques. Additionally, I seek to provide posts on the theoretical
    underpinnings of various methodologies via theory and simulations. Most importantly,
    I write to learn and help others learn! I hope to make complex topics slightly
    more accessible to all. If you enjoyed this post, please consider* [***following
    me on Medium***](https://medium.com/@jakepenzak)*!*'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
