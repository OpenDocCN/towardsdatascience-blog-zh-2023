# 机器学习的公众认知问题

> 原文：[`towardsdatascience.com/machine-learnings-public-perception-problem-48daf587e7a8?source=collection_archive---------5-----------------------#2023-09-02`](https://towardsdatascience.com/machine-learnings-public-perception-problem-48daf587e7a8?source=collection_archive---------5-----------------------#2023-09-02)

## 为什么公众的机器学习素养需要成为数据科学的优先事项，以及我们能为此做些什么。

[](https://medium.com/@s.kirmer?source=post_page-----48daf587e7a8--------------------------------)![斯蒂芬妮·基尔默](https://medium.com/@s.kirmer?source=post_page-----48daf587e7a8--------------------------------)[](https://towardsdatascience.com/?source=post_page-----48daf587e7a8--------------------------------)![Towards Data Science](https://towardsdatascience.com/?source=post_page-----48daf587e7a8--------------------------------) [斯蒂芬妮·基尔默](https://medium.com/@s.kirmer?source=post_page-----48daf587e7a8--------------------------------)

·

[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fa8dc77209ef3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learnings-public-perception-problem-48daf587e7a8&user=Stephanie+Kirmer&userId=a8dc77209ef3&source=post_page-a8dc77209ef3----48daf587e7a8---------------------post_header-----------) 发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----48daf587e7a8--------------------------------) ·10 分钟阅读·2023 年 9 月 2 日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F48daf587e7a8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learnings-public-perception-problem-48daf587e7a8&user=Stephanie+Kirmer&userId=a8dc77209ef3&source=-----48daf587e7a8---------------------clap_footer-----------)

--

[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F48daf587e7a8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learnings-public-perception-problem-48daf587e7a8&source=-----48daf587e7a8---------------------bookmark_footer-----------)![](img/7753f5b7d45f1325fdb307af9dc33ad0.png)

图片由 [安德鲁·西曼](https://unsplash.com/@amseaman?utm_source=medium&utm_medium=referral) 提供，来源于 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)

最近我在听一个播客，里面有一些聪明、深思熟虑的普通人（我不会透露他们的名字，以示礼貌），讨论 AI 如何在医疗保健中使用。我已经有些担忧，因为他们使用了“AI”这个术语，我发现这常常意味着同时包括了所有和没有任何意义。但我继续听下去，他们讨论了如何将 AI 工具（实际上只是机器学习）融入医疗实践。这些工具包括根据症状建议诊断，以及根据患者的生命体征和病情调整药物剂量，这些看起来很有前景且实际。

然而，在接下来的瞬间，我有些震惊，因为一位发言者（一位医学博士）说（我概括一下）“似乎 AI 在数学方面变差了”。这一点不仅在整个播客中萦绕于我，整个周末都未曾离开。

当受过教育、聪明的普通人对机器学习感到如此困惑和误解时，我们就有了问题。（我会避免使用“AI”这个术语，因为我真的相信它比解释更多地混淆了我们的意义。在这个背景下，这些人讨论的是机器学习和使用它的产品，即使他们对此并不知晓。）

在这位医生的例子中，他可能是在谈到大型语言模型（LLMs）时提到数学。他不知为何认为一个被训练以复杂方式排列单词以响应提示的模型也应该能够进行数学计算。它在这方面并不擅长（因为它并没有被训练来做这些！），他的所有机器学习的印象都被这一现实所玷污。

与这种误解相反，数据科学家知道 LLMs 只是更广泛机器学习领域的一小部分。许多其他算法和模型在数学计算方面表现出色，因为那是它们的具体目的。（正如一个朋友说的，当我告诉他这个故事时，“机器学习模型本质上就是数学！”）然而，这段话的开头是问题所在——数据科学家知道这一点，但公众普遍并不知晓。

> …数据科学家明白 LLMs 只是更广泛机器学习领域的一小部分。

我可以花整篇文章讨论语言模型与其他形式的机器学习之间的区别，但这并不是我今天真正感兴趣的。相反，我想探讨一下为什么我们需要关注普通人缺乏这些信息，以及可能的影响。

# 为什么我们应该关心普通人是否了解机器学习？

作为一名转行的数据科学家，我非常关注人们如何与数据科学和机器学习互动。我对此有个人的哲学：如果你的机器学习在某种程度上没有惠及人们或我们周围的世界，那它真的不重要。我认为人类努力的目的需要是改善他人的生活，这同样适用于机器学习。

然而，即使你不认同这种观点，我认为你仍然应该关心大众是否理解机器学习的基本概念。如果人们缺乏这种理解，宝贵的、值得信赖的工具的采纳可能会停滞不前。

我的论点大致如下：

1.  人们天生不准备理解和互动机器学习。

1.  没有理解这些工具，一些人可能会避免或不信任它们。

1.  更糟糕的是，一些人可能因为误导信息而滥用这些工具，从而导致不利的结果。

1.  经历了滥用的负面后果后，人们可能会变得不愿意采纳未来可能改善他们生活和社区的机器学习工具。

机器学习的效果依赖于使用者能否最大限度地发挥其功能。我经常看到和听到像我开头提到的轶事一样的例子，人们对机器学习的理解充满了极端的误解，并在这个错误的基础上建立了思维框架。这导致了他们对机器学习的整个认知图谱都是不正确的。

这对数据科学领域的意义在于，我们在构建越来越先进的机器学习中的所有工作，其可能性并不受限于我们能获得多少 GPU，而是受限于我们解释我们所构建的内容和教育公众有关其意义及如何使用的能力。

> …我们在建设更先进机器学习服务中的工作，其可能性并不受我们能获得多少 GPU 的限制，而是受限于我们解释所构建内容的能力。

## 人们天生并不准备理解机器学习。

我最近读了一篇文章，题为 [“Why Johnny Can’t Prompt”](https://dl.acm.org/doi/abs/10.1145/3544548.3581388)（Zamfirescu-Pereira, Wong, Hartmann, 和 Yang, 2023 年 4 月）。这让我对非数据科学家如何看待和处理生成式 AI，特别是广义上的机器学习，有了很多思考。

我可能会另写一篇文章来详细讨论这个话题，但对于这个论点来说，有价值的一点是：人们倾向于将他们与其他**人**互动的既有框架应用于与**机器学习系统**的互动中，从而导致效果不佳和用户挫败感。

> 人们倾向于将他们与其他**人**互动的既有框架应用于与**机器学习系统**的互动中，从而导致效果不佳和用户挫败感。

现在，我不认为这是不可修复的。我实际上认为人类总是需要学习如何使用新工具，我们绝对可以做到。想想我们如何逐渐学会使用电脑和智能手机。最开始并不明显如何操作或如何让自己被面前的设备“理解”。

这主要是通过时间的推移、设备设计的改进使其更直观（例如，技术迎合我们现有的需求）和教育来解决的。当我年轻时，年长或技术水平较低的人可以在当地社区学院接受免费或低费用的计算机课程。目标不是学习编程，而是有效地使用计算机，因为它们是极其有用的工具。

我认为这个过程同样适用于机器学习，但有一些不同之处。首先，很多机器学习对我们来说是抽象的，或者它被包装在一个拟人化的界面中（例如，LLM 聊天机器人）。许多机器学习模型的结果进入我们的生活中而我们没有意识到，例如搜索结果个性化，或者基于对我们需求预测的应用程序提醒，仅举几例。在生成性 AI 的情况下，很多机器学习隐藏在对话型聊天机器人背后，我们自然倾向于像与任何人类对话伙伴一样与其互动。然而，正如我之前提到的文章中的作者所描述的，这是一种错误。在目前，LLM 的最佳结果不是通过“像对人一样”与其对话来实现的。

## 有些人不会使用他们不理解的东西。

这种现实产生了一些我们需要注意的条件。首先，许多人不会接受机器学习完全有益且简单的说法。许多人对新一代生成性 AI 感到惊恐而非兴奋。对于许多人来说，这是一种合理的反应。一方面，我们有很多文化参考和曝光教会我们，“过于聪明”的计算机是危险的，我们应该对此保持警惕。

人们对个人电脑也有这种感觉。一些人担心它们可能具有的能力和力量，或对自己理解和使用它们的实际能力感到紧张。社区学院的计算机课程让犹豫不决的人对计算机的概念有了舒适的接受。不幸的是，我没有看到数据科学领域对公众的不确定成员采取同样的关怀。

采用新技术总是充满挑战的，这不是因为人们不聪明或不好奇，而是出于对潜在风险的真实担忧。承认这些担忧并展示防止负面结果的承诺可以提高公众对机器学习的信任度。

## 其他人会误用和滥用他们不理解的东西。

另一方面，很多人已经全身心投入与机器学习，特别是 LLMs 的互动中。人们在各种行业和娱乐中使用它。炒作和媒体报道提高了对 LLM 技术及其潜力的认识，几乎每个有电脑的公司都在尝试将 AI 纳入他们的业务战略中。

然而，这种兴奋也有负面的一面。当人们开始使用机器学习，如 LLMs 时，他们开始注意到问题以及技术未能达到过高期望的方式。也许聊天机器人没有理解你的问题，或模型的预测并不总是准确，但最终用户期待机器不会犯错。他们为什么会有这种期望？因为他们对机器学习的了解来自流行文化和炒作。我们数据科学家没有花时间解释哪些期望是合理的，哪些仍然是科幻材料。

## 在误用他们不了解的工具之后，人们将害怕将来使用新的工具。

那么，当我们在机器学习解决方案中对普通用户过度承诺而未能兑现时，会发生什么呢？在很多情况下，我们将会失望和幻灭的用户，他们本可能成为新技术的伟大倡导者。他们会更不愿意尝试下一个版本，或将来使用机器学习，因为他们已经尝试过并受到了伤害。

想想这个例子：[使用 ChatGPT 获取简报引文的律师。](https://apnews.com/article/artificial-intelligence-chatgpt-fake-case-lawyers-d6ae9fa79d0542db9e1455397aef381c) 当这个故事出来时，数据科学界对这位律师进行了抨击。“谁会这么做？难道他们不知道不能依赖 ChatGPT 的准确性吗？”

我实际上对这位律师感到相当遗憾，即使这些后果是由于相当大的马虎造成的。ChatGPT 的承诺对很多公众而言似乎几乎是魔法般的，媒体对其近乎奇迹般能力的描述进一步助长了这种观念。很多人首次了解到 ChatGPT 会“撒谎”是通过阅读这个案例。

这些误解源于对大型语言模型（LLMs）的拟人化，假设它们具有类似人类的推理和真伪判断能力。实际上，ChatGPT 是一个非常复杂的模型，它根据你给出的提示将单词排列在一起。它经过训练，能够生成非常易于理解的语言。但 ChatGPT 没有“真相”或“谎言”的概念。它没有内部嵌入来表示某事是否准确。因此，当新闻谈论 ChatGPT 撒谎或“产生幻觉”时，这有些误导。

然而，重要的是，我们现在有一群人看到了这个消息，更不用说涉及的律师，他们对从 ChatGPT 中获得的任何信息是否可靠感到焦虑。这个情境并没有帮助他们理解 LLM 的概念，也确实没有帮助实现将机器学习应用于有益的更广泛目标。有人因为缺乏对模型工作原理的了解而受到了伤害，其他人对此嗤之以鼻，而现在我们制造了更多的怀疑者，他们将来可能会避免使用至少一些形式的机器学习。

所有这些都指向相同的问题——当技术缺乏适当的公众教育时，我们就把公众教育的任务留给了不可靠和有偏见的来源，这些来源的优先级与公众利益不一致。只需问问任何公共卫生专业人员，看看他们如何努力提高疫苗接种率。机器学习如果我们不能在公众教育方面走在前面，可能会沿着相同的不幸道路发展。

# 我们可以做些什么来解决这个问题？

作为数据科学的从业者，我们如何弥合技术专长与公众意识之间的差距？作为一名前教育工作者，我对此非常关注。公众是否真正理解机器学习能为我们做些什么是很重要的，因为我们有机会用它做很多有益的事情。

我认为我们可以做的一件事是将更多的时间和精力投入到公众教育中。现在，我并不是说街上的每个人都需要学习反向传播或编码器架构的教程。（这就像说人们需要研究微芯片才能成为有效的计算机用户一样。）但我确实认为，人们需要了解一些机器学习的基本要素，以便成为信息技术的知情用户，包括目前技术的伦理、风险和局限性。作为一个领域，数据科学需要了解一个人需要掌握哪些信息才能成为成功且有效的机器学习用户，并且我们如何能够分享这些信息。

如果我们没有看到如此戏剧性的转变，LLM（大型语言模型）正快速普及到公众手中，我们或许可以对此稍作等待。基本的预测模型结果通常由数据科学专业人员进行中介，即模型的输入经过精心设计，结果也以深思熟虑的方式呈现。然而，对于 LLM 聊天机器人来说，这种情况并不成立。人们可以输入任何他们想要的内容，没有人控制返回的结果。用户需要更多的知识来负责任地生成和消费这些信息。

其次，我认为数据科学作为一个领域，需要对机器学习的实际能力以及它能够做什么进行更多的发声和坚持反对过度炒作和夸大其词。我发现这种情况大多数存在于吸引眼球的媒体中，甚至一些理论上更可信的新闻报道中。不要误解我，机器学习确实令人惊叹，并且它可以做出令人难以置信的事情！然而，它并不完美，我们不应该让任何人假装它是完美的，而不进行反对。

忽视这个问题，我们可能会阻碍机器学习的进步——不仅仅是技术上的进步（尽管国会未能理解机器学习可能会产生这样的效果），还包括其在实际生活中的应用进步。我不希望看到这项技术的巨大潜力因为我们没有帮助公众做好准备而被边缘化或缩小。

*查看更多我的作品请访问* [*www.stephaniekirmer.com*](http://www.stephaniekirmer.com)*。*
