- en: Create Many-To-One relationships Between Columns in a Synthetic Table with PySpark
    UDFs
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 PySpark UDFs 在合成表中创建多对一关系
- en: 原文：[https://towardsdatascience.com/create-many-to-one-relationships-between-columns-in-a-synthetic-table-with-pyspark-udfs-41e39d97c936?source=collection_archive---------3-----------------------#2023-12-09](https://towardsdatascience.com/create-many-to-one-relationships-between-columns-in-a-synthetic-table-with-pyspark-udfs-41e39d97c936?source=collection_archive---------3-----------------------#2023-12-09)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/create-many-to-one-relationships-between-columns-in-a-synthetic-table-with-pyspark-udfs-41e39d97c936?source=collection_archive---------3-----------------------#2023-12-09](https://towardsdatascience.com/create-many-to-one-relationships-between-columns-in-a-synthetic-table-with-pyspark-udfs-41e39d97c936?source=collection_archive---------3-----------------------#2023-12-09)
- en: Leverage some simple equations to generate related columns in test tables.
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 利用一些简单的方程在测试表中生成相关列。
- en: '[](https://medium.com/@mc12338?source=post_page-----41e39d97c936--------------------------------)[![Matt
    Collins](../Images/b28ac8100d6fb287e3fa6926eec7939a.png)](https://medium.com/@mc12338?source=post_page-----41e39d97c936--------------------------------)[](https://towardsdatascience.com/?source=post_page-----41e39d97c936--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----41e39d97c936--------------------------------)
    [Matt Collins](https://medium.com/@mc12338?source=post_page-----41e39d97c936--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@mc12338?source=post_page-----41e39d97c936--------------------------------)[![Matt
    Collins](../Images/b28ac8100d6fb287e3fa6926eec7939a.png)](https://medium.com/@mc12338?source=post_page-----41e39d97c936--------------------------------)[](https://towardsdatascience.com/?source=post_page-----41e39d97c936--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----41e39d97c936--------------------------------)
    [Matt Collins](https://medium.com/@mc12338?source=post_page-----41e39d97c936--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fd1970f1605f1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcreate-many-to-one-relationships-between-columns-in-a-synthetic-table-with-pyspark-udfs-41e39d97c936&user=Matt+Collins&userId=d1970f1605f1&source=post_page-d1970f1605f1----41e39d97c936---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----41e39d97c936--------------------------------)
    ·7 min read·Dec 9, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F41e39d97c936&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcreate-many-to-one-relationships-between-columns-in-a-synthetic-table-with-pyspark-udfs-41e39d97c936&user=Matt+Collins&userId=d1970f1605f1&source=-----41e39d97c936---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fd1970f1605f1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcreate-many-to-one-relationships-between-columns-in-a-synthetic-table-with-pyspark-udfs-41e39d97c936&user=Matt+Collins&userId=d1970f1605f1&source=post_page-d1970f1605f1----41e39d97c936---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----41e39d97c936--------------------------------)
    ·7 分钟阅读·2023年12月9日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F41e39d97c936&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcreate-many-to-one-relationships-between-columns-in-a-synthetic-table-with-pyspark-udfs-41e39d97c936&user=Matt+Collins&userId=d1970f1605f1&source=-----41e39d97c936---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F41e39d97c936&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcreate-many-to-one-relationships-between-columns-in-a-synthetic-table-with-pyspark-udfs-41e39d97c936&source=-----41e39d97c936---------------------bookmark_footer-----------)![](../Images/8f6ff1c8009d98db1eaa04ccfe19ff4e.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F41e39d97c936&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcreate-many-to-one-relationships-between-columns-in-a-synthetic-table-with-pyspark-udfs-41e39d97c936&source=-----41e39d97c936---------------------bookmark_footer-----------)![](../Images/8f6ff1c8009d98db1eaa04ccfe19ff4e.png)'
- en: Image generated with DALL-E 3
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 DALL-E 3 生成的图像
- en: I’ve recently been playing around with Databricks Labs Data Generator to create
    completely synthetic datasets from scratch. As part of this, I’ve looked at building
    sales data around different stores, employees, and customers. As such, I wanted
    to create relationships between the columns I was artificially populating — such
    as mapping employees and customers to a certain store.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 我最近一直在使用 Databricks Labs 数据生成器从头开始创建完全合成的数据集。在此过程中，我研究了围绕不同商店、员工和客户构建销售数据。因此，我想在我人为填充的列之间创建关系，例如将员工和客户映射到某个商店。
- en: Through using PySpark UDFs and a bit of logic, we can generate related columns
    which follow a many-to-one relationship. With a bit of magic, we are even able
    to extend the logic to give some variance in this mapping — like a customer generally
    buying from their local store but sometimes from a different store.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: '**Use Databricks Labs Data Generator to generate our basic DataFrame**'
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '***Note: You can skip this section if not required!***'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: First up, we need to create a DataFrame with our first randomly-generated column.
    In our case, we’re going to start with the store, as logically we will have “many
    employees per store” and “many customers repeatedly shopping at a store”.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: With a Star Schema Data Model in mind, we’re going to start with our Sales Fact
    table — a transactional table which will contain key values for the Sale Id, Store
    Id, Employee Id and Customer Id, the Sale Amount along with some datetime data
    for the purchase. We can then fill out the specifics about the Store, Employee
    and Customer in dimension tables further down the line.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ll start small — a table with 1000 sales will do. We now need to decide
    how to split these sales up between stores, employees and customers. Let’s suggest
    the following:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: Stores = 20
  id: totrans-16
  prefs:
  - PREF_UL
  - PREF_H1
  type: TYPE_NORMAL
- en: Employees = 100
  id: totrans-17
  prefs:
  - PREF_UL
  - PREF_H1
  type: TYPE_NORMAL
- en: Customers = 700
  id: totrans-18
  prefs:
  - PREF_UL
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We can also say that the sales will be recorded over the course of last month:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: First Sale Date = 2023–11–01
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Last Sale Date = 2023–11–30
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Sale Id needs to be a unique column so we can generate an Id column for
    this. We now need to distribute the 1000 sales across the 20 stores. For simplicity
    we will assume this is random.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: 'Using Databricks Lab Generator we can do this with the following code:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: Now add some code to record when the sales were made and their amount. To keep
    things simple, we’ll round the timestamp of the sale to the nearest hour.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: To calculate the sale amount, we can use the “expr” parameter in our withColumn
    expression to allow us to generate a random number, with some rules/boundaries.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: 'In this case, the expression is quite straight-forward: produce a random number
    (between 0 and 1), add 0.1 (ensuring sale values are not 0) and multiply by 350.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ve got our basic shape for the DataFrame now, so put it all together:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: 'We can create a quick Data Profile to look at the distribution of values in
    the columns:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a82e88e7cd291b288a0986f984fac0b1.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
- en: 'Image by Author: Data profile generated in Databricks'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: We can see that the StoreId distribution is relatively even across the 20 stores,
    with no missing values and averages around the centre as we would expect. The
    same follows for the Timestamps and amount values.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: '**Add a dependent Many-To-One column**'
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now we can add our Employee Id column to the DataFrame. We’re done with Databricks
    Lab Data Generator now, so will just use PySpark operations to add columns to
    the DataFrame.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: 'Stepping back from the code, we want to model this as the following statements:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: There are 20 stores.
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each store has more than 1 employee.
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each employee works at a single store only.
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'First we need to split the employees between the stores. The following python
    function can be used to do so:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have our distribution of employees for each store, let’s start assigning
    Ids!
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: 'The employeesPerStore list ensures that the employee Ids per store do not overlap.
    We can use this to randomly assign an employee Id to a sale in the table with
    the following equation:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: This function currently only works for a single value — we need to put this
    into something that a PySpark DataFrame can work with (functionally, and quickly!)
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: 'We can pass PySpark UDFs to the *withColumn* method, so let’s reformat this
    logic into a function and set it to a UDF:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: 'Now call this as a new column in the DataFrame:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: We can quickly test this looks correct by using the Visualisation tool in Databricks
    to see the distinct count of Employee Ids per Store Id. This is my preference
    but you could also use group by logic or other plotting modules, if desired.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/159ea138f058375b07f161e8b09a7df3.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
- en: 'Image by Author: Distinct count of Employee Ids per Store'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: '**Important Note:** This logic allows for employees to be **missed** from the
    results. This means that it is possible for an employee to make 0 sales, and thus
    not be included in the DataFrame. We’ll look at how to ensure all customers have
    sales recorded against them in the next section.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: Add the customers column
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The customers column is a bit different… while our use-case suggests it is common
    for a customer to shop at a single store multiple times, it is entirely possible
    that they got to a different store one day. How do we model this?
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ve got the starting points with the work done for our employees column,
    so can repeat the *get_employees* function and UDF logic for customers as below:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ve again potentially missed a few customers off here. Here are a few approaches
    to rectify this:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: Recalculate in *while* loop until you converge on a DataFrame which contains
    all customers (inefficient, costly, could run indefinitely)
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Randomly update customer Ids in *while* loop until all customers in DataFrame
    (requires logic to only overwrite same stores, could also run indefinitely)
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Return a list of all customer Ids with more than 1 record in the sales table,
    and randomly overwrite until all missing Ids are added (also needs logic for overwriting
    customers in same store, may also require *while* loop logic)
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Reverse the process and start with employees. This ensures each employee
    is randomly assigned to rows. We can then use the mapping and apply the store
    Id.**'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hopefully it is clear why the last option is the lowest effort to compute —
    we have all the code required so just need to reformat things slightly.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: 'Our new scripts looks as follows:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b5bc57b56200b1ece8e679b6be856c58.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
- en: 'Image by Author: Databricks Data Profile for the new DataFrame'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: Adding Randomness to the customers
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'What we now need is a bit of randomness, which we need to define. For our example,
    let’s say that each customer has a 90% chance of shopping at the usual store (the
    “local” store). If we do not need all customers to be returned in the results
    set, we can simply adjust our *customers_udf* as follows, and use *df2*:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在需要的是一些随机性，我们需要定义它。以我们的例子为例，假设每个客户有90%的机会在通常的商店（“本地”商店）购物。如果我们不需要将所有客户返回在结果集中，我们可以简单地调整我们的*customers_udf*，并使用*df2*：
- en: The logic involves using the *random.choices* function to supply a weighted
    list and return a single value.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 该逻辑涉及使用*random.choices*函数来提供加权列表并返回单一值。
- en: To compute the weighted list, we have the weight of our “local” store for the
    customer, in this case 90%, so need to assign the remaining 10% to the other stores,
    in this case 19 stores. The probability of each other store being selected will
    therefore be 10/19 = 0.526%. We can populate an array with these percentages,
    which would look something like the following:[0.526,0.526,0.526,…,90,0.526,…0.526]
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 要计算加权列表，我们需要将客户的“本地”商店的权重设置为90%，因此需要将剩余的10%分配给其他商店，这里有19家商店。因此，每个其他商店被选择的概率将是10/19
    = 0.526%。我们可以用这些百分比填充一个数组，数组看起来类似于：[0.526,0.526,0.526,…,90,0.526,…0.526]
- en: Passing this into random.choices, we then randomly select a store Id from the
    list with the corresponding weights and use this as the input for the *customer_id*
    variable, as before.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 将其传递给`random.choices`，我们就可以根据对应的权重从列表中随机选择一个商店ID，并将其用作*customer_id*变量的输入，如前所述。
- en: '**Note:** The output of random.choices returns a list (as you can request k
    results), so access the 0th element of the list to get the store_id as an integer
    value.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意：** `random.choices`的输出返回一个列表（因为你可以请求k个结果），所以访问列表的第0个元素以获取store_id作为整数值。'
- en: 'If we need to combine this logic with a DataFrame including all customers,
    we can reverse the process slightly. The weights logic is still valid so we can
    just plug this into randomly select a store and return this as the result:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们需要将此逻辑与包含所有客户的 DataFrame 结合使用，我们可以稍微逆转该过程。权重逻辑仍然有效，所以我们可以将其插入以随机选择商店并将其作为结果返回：
- en: '![](../Images/abaf37bd9c9efc775dda58e0503258df.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/abaf37bd9c9efc775dda58e0503258df.png)'
- en: 'Image by Author: Sample of final DataFrame in Databricks'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图像：Databricks中的最终 DataFrame 示例
- en: Conclusion
  id: totrans-69
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: There we have it! A synthetically created DataFrame with both strict and loose
    mappings between columns. You can now progress the next steps to populate related
    tables which may contain more descriptive information, such as dimension tables
    of store names, addresses, employee names, roles, etc. This can also be done using
    Databricks Labs Data Generator or any other tool/process you are comfortable with.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样！一个合成创建的 DataFrame，其中列之间有严格和松散的映射。你现在可以进行下一步，填充可能包含更多描述性信息的相关表，例如商店名称、地址、员工姓名、角色等的维度表。这也可以使用
    Databricks Labs 数据生成器或任何其他你熟悉的工具/过程来完成。
- en: There are some great examples on the Databricks Labs Data Generator [GitHub
    Repo](https://github.com/databrickslabs/dbldatagen) along with documentation,
    so please do take a look at this if you are curious to learn more.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Databricks Labs 数据生成器的[GitHub Repo](https://github.com/databrickslabs/dbldatagen)上有一些很棒的示例和文档，如果你想了解更多内容，请查看一下。
- en: All of my code can be accessed from the following [GitHub Repo](https://github.com/MattPCollins/Dbldatagen-Related-Tables).
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 我所有的代码可以从以下[GitHub Repo](https://github.com/MattPCollins/Dbldatagen-Related-Tables)访问。
- en: If you have any thoughts, comments or alternatives to this demo, please reach
    out in the comments. Thanks!
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你对这个演示有任何想法、评论或替代方案，请在评论中联系我。谢谢！
