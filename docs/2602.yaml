- en: Why More Is More (in Artificial Intelligence)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/why-more-is-more-in-deep-learning-b28d7cedc9f5?source=collection_archive---------1-----------------------#2023-08-15](https://towardsdatascience.com/why-more-is-more-in-deep-learning-b28d7cedc9f5?source=collection_archive---------1-----------------------#2023-08-15)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: How Large Neural Networks Generalize
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://manuel-brenner.medium.com/?source=post_page-----b28d7cedc9f5--------------------------------)[![Manuel
    Brenner](../Images/f62843c79a9b378494cb83caf3ddc792.png)](https://manuel-brenner.medium.com/?source=post_page-----b28d7cedc9f5--------------------------------)[](https://towardsdatascience.com/?source=post_page-----b28d7cedc9f5--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----b28d7cedc9f5--------------------------------)
    [Manuel Brenner](https://manuel-brenner.medium.com/?source=post_page-----b28d7cedc9f5--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F1fde95441432&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhy-more-is-more-in-deep-learning-b28d7cedc9f5&user=Manuel+Brenner&userId=1fde95441432&source=post_page-1fde95441432----b28d7cedc9f5---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----b28d7cedc9f5--------------------------------)
    ·10 min read·Aug 15, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fb28d7cedc9f5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhy-more-is-more-in-deep-learning-b28d7cedc9f5&user=Manuel+Brenner&userId=1fde95441432&source=-----b28d7cedc9f5---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb28d7cedc9f5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhy-more-is-more-in-deep-learning-b28d7cedc9f5&source=-----b28d7cedc9f5---------------------bookmark_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: Less is more.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: -***Ludwig Mies van der Rohe***
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Less is more only when more is too much.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '- ***Frank Loyd Wright***'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Deep neural networks (DNNs) have profoundly transformed the landscape of machine
    learning, often becoming synonymous with the broader fields of artificial intelligence
    and machine learning. Yet, their rise would have been unimaginable without their
    partner-in-crime: stochastic gradient descent (SGD).'
  prefs: []
  type: TYPE_NORMAL
- en: 'SGD, along with its derivative optimizers, forms the core of many self-learning
    algorithms. At its heart, the concept is straightforward: calculate the task’s
    loss using training data, determine the gradients of this loss in relation to
    its parameters, and then adjust the parameters in a direction that minimizes the
    loss.'
  prefs: []
  type: TYPE_NORMAL
- en: 'It sounds simple, but in applications, it has proven to be immensely powerful:
    SGD can find solutions for all kinds of complex problems and training data, given
    it is used in conjunction with a sufficiently expressive architecture. It’s particularly
    good at finding parameter sets that make the network perform perfectly on the
    training data, something called the **interpolation regime**. But under which
    conditions are neural networks thought to **generalize well**, meaning that they
    perform well on unseen test data?'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/fcd15cc481d3711756d2673df3509c41.png)'
  prefs: []
  type: TYPE_IMG
- en: The quest to generalize lies at the heart of machine learning. Envisioned by
    DALL-E.
  prefs: []
  type: TYPE_NORMAL
