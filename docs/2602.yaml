- en: Why More Is More (in Artificial Intelligence)
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/why-more-is-more-in-deep-learning-b28d7cedc9f5?source=collection_archive---------1-----------------------#2023-08-15](https://towardsdatascience.com/why-more-is-more-in-deep-learning-b28d7cedc9f5?source=collection_archive---------1-----------------------#2023-08-15)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: How Large Neural Networks Generalize
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://manuel-brenner.medium.com/?source=post_page-----b28d7cedc9f5--------------------------------)[![Manuel
    Brenner](../Images/f62843c79a9b378494cb83caf3ddc792.png)](https://manuel-brenner.medium.com/?source=post_page-----b28d7cedc9f5--------------------------------)[](https://towardsdatascience.com/?source=post_page-----b28d7cedc9f5--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----b28d7cedc9f5--------------------------------)
    [Manuel Brenner](https://manuel-brenner.medium.com/?source=post_page-----b28d7cedc9f5--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F1fde95441432&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhy-more-is-more-in-deep-learning-b28d7cedc9f5&user=Manuel+Brenner&userId=1fde95441432&source=post_page-1fde95441432----b28d7cedc9f5---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----b28d7cedc9f5--------------------------------)
    ·10 min read·Aug 15, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fb28d7cedc9f5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhy-more-is-more-in-deep-learning-b28d7cedc9f5&user=Manuel+Brenner&userId=1fde95441432&source=-----b28d7cedc9f5---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb28d7cedc9f5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhy-more-is-more-in-deep-learning-b28d7cedc9f5&source=-----b28d7cedc9f5---------------------bookmark_footer-----------)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: Less is more.
  id: totrans-8
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: -***Ludwig Mies van der Rohe***
  id: totrans-9
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Less is more only when more is too much.
  id: totrans-10
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '- ***Frank Loyd Wright***'
  id: totrans-11
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Deep neural networks (DNNs) have profoundly transformed the landscape of machine
    learning, often becoming synonymous with the broader fields of artificial intelligence
    and machine learning. Yet, their rise would have been unimaginable without their
    partner-in-crime: stochastic gradient descent (SGD).'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: 'SGD, along with its derivative optimizers, forms the core of many self-learning
    algorithms. At its heart, the concept is straightforward: calculate the task’s
    loss using training data, determine the gradients of this loss in relation to
    its parameters, and then adjust the parameters in a direction that minimizes the
    loss.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: 'It sounds simple, but in applications, it has proven to be immensely powerful:
    SGD can find solutions for all kinds of complex problems and training data, given
    it is used in conjunction with a sufficiently expressive architecture. It’s particularly
    good at finding parameter sets that make the network perform perfectly on the
    training data, something called the **interpolation regime**. But under which
    conditions are neural networks thought to **generalize well**, meaning that they
    perform well on unseen test data?'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 这听起来简单，但在应用中，它已被证明是极其强大的：SGD可以解决各种复杂问题和训练数据，只要它与足够表达力的架构结合使用。它特别擅长找到使网络在训练数据上表现完美的参数集，这被称为**插值阶段**。但在什么条件下，神经网络被认为能够**良好地泛化**，即在未见过的测试数据上表现良好？
- en: '![](../Images/fcd15cc481d3711756d2673df3509c41.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fcd15cc481d3711756d2673df3509c41.png)'
- en: The quest to generalize lies at the heart of machine learning. Envisioned by
    DALL-E.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 对于机器学习来说，追求泛化是核心目标。这一理念由DALL-E构想。
