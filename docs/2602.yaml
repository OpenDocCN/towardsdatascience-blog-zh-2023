- en: Why More Is More (in Artificial Intelligence)
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为什么更多即是更多（在人工智能领域）
- en: 原文：[https://towardsdatascience.com/why-more-is-more-in-deep-learning-b28d7cedc9f5?source=collection_archive---------1-----------------------#2023-08-15](https://towardsdatascience.com/why-more-is-more-in-deep-learning-b28d7cedc9f5?source=collection_archive---------1-----------------------#2023-08-15)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/why-more-is-more-in-deep-learning-b28d7cedc9f5?source=collection_archive---------1-----------------------#2023-08-15](https://towardsdatascience.com/why-more-is-more-in-deep-learning-b28d7cedc9f5?source=collection_archive---------1-----------------------#2023-08-15)
- en: How Large Neural Networks Generalize
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何大型神经网络进行泛化
- en: '[](https://manuel-brenner.medium.com/?source=post_page-----b28d7cedc9f5--------------------------------)[![Manuel
    Brenner](../Images/f62843c79a9b378494cb83caf3ddc792.png)](https://manuel-brenner.medium.com/?source=post_page-----b28d7cedc9f5--------------------------------)[](https://towardsdatascience.com/?source=post_page-----b28d7cedc9f5--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----b28d7cedc9f5--------------------------------)
    [Manuel Brenner](https://manuel-brenner.medium.com/?source=post_page-----b28d7cedc9f5--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://manuel-brenner.medium.com/?source=post_page-----b28d7cedc9f5--------------------------------)[![Manuel
    Brenner](../Images/f62843c79a9b378494cb83caf3ddc792.png)](https://manuel-brenner.medium.com/?source=post_page-----b28d7cedc9f5--------------------------------)[](https://towardsdatascience.com/?source=post_page-----b28d7cedc9f5--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----b28d7cedc9f5--------------------------------)
    [Manuel Brenner](https://manuel-brenner.medium.com/?source=post_page-----b28d7cedc9f5--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F1fde95441432&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhy-more-is-more-in-deep-learning-b28d7cedc9f5&user=Manuel+Brenner&userId=1fde95441432&source=post_page-1fde95441432----b28d7cedc9f5---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----b28d7cedc9f5--------------------------------)
    ·10 min read·Aug 15, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fb28d7cedc9f5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhy-more-is-more-in-deep-learning-b28d7cedc9f5&user=Manuel+Brenner&userId=1fde95441432&source=-----b28d7cedc9f5---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F1fde95441432&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhy-more-is-more-in-deep-learning-b28d7cedc9f5&user=Manuel+Brenner&userId=1fde95441432&source=post_page-1fde95441432----b28d7cedc9f5---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----b28d7cedc9f5--------------------------------)
    ·10分钟阅读·2023年8月15日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fb28d7cedc9f5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhy-more-is-more-in-deep-learning-b28d7cedc9f5&user=Manuel+Brenner&userId=1fde95441432&source=-----b28d7cedc9f5---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb28d7cedc9f5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhy-more-is-more-in-deep-learning-b28d7cedc9f5&source=-----b28d7cedc9f5---------------------bookmark_footer-----------)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb28d7cedc9f5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhy-more-is-more-in-deep-learning-b28d7cedc9f5&source=-----b28d7cedc9f5---------------------bookmark_footer-----------)'
- en: Less is more.
  id: totrans-8
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 少即是多。
- en: -***Ludwig Mies van der Rohe***
  id: totrans-9
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '- ***路德维希·密斯·凡·德·罗***'
- en: Less is more only when more is too much.
  id: totrans-10
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 只有当更多的东西已经过多时，少才是多。
- en: '- ***Frank Loyd Wright***'
  id: totrans-11
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '- ***弗兰克·劳埃德·赖特***'
- en: 'Deep neural networks (DNNs) have profoundly transformed the landscape of machine
    learning, often becoming synonymous with the broader fields of artificial intelligence
    and machine learning. Yet, their rise would have been unimaginable without their
    partner-in-crime: stochastic gradient descent (SGD).'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 深度神经网络（DNNs）深刻地改变了机器学习的格局，往往与更广泛的人工智能和机器学习领域同义。然而，它们的兴起若没有它们的“罪魁祸首”——随机梯度下降（SGD），将是难以想象的。
- en: 'SGD, along with its derivative optimizers, forms the core of many self-learning
    algorithms. At its heart, the concept is straightforward: calculate the task’s
    loss using training data, determine the gradients of this loss in relation to
    its parameters, and then adjust the parameters in a direction that minimizes the
    loss.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: SGD 及其衍生优化器构成了许多自学习算法的核心。其核心概念很简单：使用训练数据计算任务的损失，确定该损失相对于其参数的梯度，然后调整参数，以减少损失。
- en: 'It sounds simple, but in applications, it has proven to be immensely powerful:
    SGD can find solutions for all kinds of complex problems and training data, given
    it is used in conjunction with a sufficiently expressive architecture. It’s particularly
    good at finding parameter sets that make the network perform perfectly on the
    training data, something called the **interpolation regime**. But under which
    conditions are neural networks thought to **generalize well**, meaning that they
    perform well on unseen test data?'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 这听起来简单，但在应用中，它已被证明是极其强大的：SGD可以解决各种复杂问题和训练数据，只要它与足够表达力的架构结合使用。它特别擅长找到使网络在训练数据上表现完美的参数集，这被称为**插值阶段**。但在什么条件下，神经网络被认为能够**良好地泛化**，即在未见过的测试数据上表现良好？
- en: '![](../Images/fcd15cc481d3711756d2673df3509c41.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fcd15cc481d3711756d2673df3509c41.png)'
- en: The quest to generalize lies at the heart of machine learning. Envisioned by
    DALL-E.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 对于机器学习来说，追求泛化是核心目标。这一理念由DALL-E构想。
