- en: Train Image Segmentation Models to Accept User Feedback via Voronoi Tiling,
    Part 1
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练图像分割模型以通过Voronoi平铺接受用户反馈，第1部分
- en: 原文：[https://towardsdatascience.com/train-image-segmentation-models-to-accept-user-feedback-via-voronoi-tiling-part-1-8ab85d410d29?source=collection_archive---------6-----------------------#2023-05-05](https://towardsdatascience.com/train-image-segmentation-models-to-accept-user-feedback-via-voronoi-tiling-part-1-8ab85d410d29?source=collection_archive---------6-----------------------#2023-05-05)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/train-image-segmentation-models-to-accept-user-feedback-via-voronoi-tiling-part-1-8ab85d410d29?source=collection_archive---------6-----------------------#2023-05-05](https://towardsdatascience.com/train-image-segmentation-models-to-accept-user-feedback-via-voronoi-tiling-part-1-8ab85d410d29?source=collection_archive---------6-----------------------#2023-05-05)
- en: How to train an off-the-shelf image segmentation model to respond to user feedback
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何训练现成的图像分割模型以响应用户反馈
- en: '[](https://florin-andrei.medium.com/?source=post_page-----8ab85d410d29--------------------------------)[![Florin
    Andrei](../Images/372ac3e80dbc03cbd20295ec1df5fa6f.png)](https://florin-andrei.medium.com/?source=post_page-----8ab85d410d29--------------------------------)[](https://towardsdatascience.com/?source=post_page-----8ab85d410d29--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----8ab85d410d29--------------------------------)
    [Florin Andrei](https://florin-andrei.medium.com/?source=post_page-----8ab85d410d29--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://florin-andrei.medium.com/?source=post_page-----8ab85d410d29--------------------------------)[![Florin
    Andrei](../Images/372ac3e80dbc03cbd20295ec1df5fa6f.png)](https://florin-andrei.medium.com/?source=post_page-----8ab85d410d29--------------------------------)[](https://towardsdatascience.com/?source=post_page-----8ab85d410d29--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----8ab85d410d29--------------------------------)
    [Florin Andrei](https://florin-andrei.medium.com/?source=post_page-----8ab85d410d29--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Faeaeb9d7d248&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftrain-image-segmentation-models-to-accept-user-feedback-via-voronoi-tiling-part-1-8ab85d410d29&user=Florin+Andrei&userId=aeaeb9d7d248&source=post_page-aeaeb9d7d248----8ab85d410d29---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----8ab85d410d29--------------------------------)
    ·13 min read·May 5, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F8ab85d410d29&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftrain-image-segmentation-models-to-accept-user-feedback-via-voronoi-tiling-part-1-8ab85d410d29&user=Florin+Andrei&userId=aeaeb9d7d248&source=-----8ab85d410d29---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Faeaeb9d7d248&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftrain-image-segmentation-models-to-accept-user-feedback-via-voronoi-tiling-part-1-8ab85d410d29&user=Florin+Andrei&userId=aeaeb9d7d248&source=post_page-aeaeb9d7d248----8ab85d410d29---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----8ab85d410d29--------------------------------)
    · 13分钟阅读·2023年5月5日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F8ab85d410d29&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftrain-image-segmentation-models-to-accept-user-feedback-via-voronoi-tiling-part-1-8ab85d410d29&user=Florin+Andrei&userId=aeaeb9d7d248&source=-----8ab85d410d29---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F8ab85d410d29&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftrain-image-segmentation-models-to-accept-user-feedback-via-voronoi-tiling-part-1-8ab85d410d29&source=-----8ab85d410d29---------------------bookmark_footer-----------)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F8ab85d410d29&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftrain-image-segmentation-models-to-accept-user-feedback-via-voronoi-tiling-part-1-8ab85d410d29&source=-----8ab85d410d29---------------------bookmark_footer-----------)'
- en: (The second part of this series is [here](/train-image-segmentation-models-to-accept-user-feedback-via-voronoi-tiling-part-2-1f02eebbddb9).)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: （本系列的第二部分在[这里](/train-image-segmentation-models-to-accept-user-feedback-via-voronoi-tiling-part-2-1f02eebbddb9)。）
- en: '[Image segmentation](https://en.wikipedia.org/wiki/Image_segmentation) is a
    popular topic in machine learning, with many practical applications. Vision models
    can be trained to partition images based on some criteria, usually following the
    contour of a familiar type of object. When the model can not only segment an image,
    but also distinguish between different types of objects, this is called semantic
    segmentation. Self-driving cars use semantic segmentation to identify the objects
    nearby: pedestrians, stop signs, the road, other cars, etc. Another application
    is in the field of medicine (radiology), where models could be trained to identify
    malignant tumors in ultrasound images. And the examples could continue.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '[图像分割](https://en.wikipedia.org/wiki/Image_segmentation)是机器学习中的一个热门话题，具有许多实际应用。视觉模型可以根据某些标准对图像进行分割，通常是沿着熟悉类型对象的轮廓进行。当模型不仅可以分割图像，还能区分不同类型的对象时，这被称为语义分割。自动驾驶汽车使用语义分割来识别附近的对象：行人、停车标志、道路、其他汽车等。另一个应用领域是在医学（放射学）中，其中模型可以被训练以识别超声图像中的恶性肿瘤。还有更多的例子。'
- en: This article assumes you’re familiar with the basic concepts of image segmentation,
    and with optimization algorithms such as [simulated annealing](https://en.wikipedia.org/wiki/Simulated_annealing).
    To keep the total size reasonable, no code is quoted in the article — but see
    [my GitHub repository](https://github.com/FlorinAndrei/segmentation_click_train),
    also linked in the final section, for all the project’s code. I’ve also placed
    links to code in relevant places throughout the text. [This is the main notebook
    with the code used to generate images for this article](https://github.com/FlorinAndrei/segmentation_click_train/blob/main/article.ipynb).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 本文假设您对图像分割的基本概念以及[模拟退火](https://en.wikipedia.org/wiki/Simulated_annealing)等优化算法有所了解。为了保持总大小合理，文章中没有引用代码——但请参阅[我的GitHub仓库](https://github.com/FlorinAndrei/segmentation_click_train)，也在最后部分提供了链接，获取项目的所有代码。我还在文本中的相关位置放置了代码链接。[这是生成本文图像所使用的主要笔记本](https://github.com/FlorinAndrei/segmentation_click_train/blob/main/article.ipynb)。
- en: The Goals of the Project
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 项目的目标
- en: 'A bit of background first:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 首先提供一些背景信息：
- en: In December 2022 I’ve completed the final semester of my MS in Data Science
    study at the University of Wisconsin-La Crosse. [My capstone project](https://github.com/FlorinAndrei/datascience_capstone_project),
    under the supervision of Dr. Jeff Baggett of UWLAX, aimed to build semantic segmentation
    models that could detect tissue lesions in breast ultrasound images. Some of these
    lesions are malignant, and it’s important to have good diagnostic tools to catch
    the disease in its early stages. Segmentation models, pre-trained on large, generic
    datasets such as ImageNet, can be fine-tuned on medical ultrasound imaging data.
    By feeding images from an ultrasound scanner to such a model, we get predictions
    from the model indicating whether a lesion is present in the scanned area, where
    it is located, what is its shape, and optionally hints about its nature (malignant
    or benign).
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在2022年12月，我完成了威斯康星大学拉克罗斯分校（University of Wisconsin-La Crosse）数据科学硕士学位的最后一个学期。[我的毕业项目](https://github.com/FlorinAndrei/datascience_capstone_project)在UWLAX的Jeff
    Baggett博士的监督下，旨在构建能够检测乳腺超声图像中组织病变的语义分割模型。这些病变中有些是恶性的，因此拥有良好的诊断工具以在早期阶段发现疾病非常重要。预先在大规模通用数据集（如ImageNet）上训练的分割模型，可以在医学超声图像数据上进行微调。通过将超声扫描仪的图像输入到这样的模型中，我们可以得到模型的预测，指示扫描区域内是否存在病变，病变的位置、形状以及可选的性质提示（恶性或良性）。
- en: 'Here’s an example of an image segmentation model performing a prediction on
    an ultrasound image taken from the [Dataset of Breast Ultrasound Images](https://www.sciencedirect.com/science/article/pii/S2352340919312181):'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是一个图像分割模型对来自[乳腺超声图像数据集](https://www.sciencedirect.com/science/article/pii/S2352340919312181)的超声图像进行预测的示例：
- en: '![](../Images/5d425a8e0cfb06b241ce07217a779b17.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5d425a8e0cfb06b241ce07217a779b17.png)'
- en: 'source: Dataset of Breast Ultrasound Images'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 来源：乳腺超声图像数据集
- en: The frame on the left is the ultrasound image from the dataset; it contains
    what appears to be a lesion that may or may not be malignant (the dark area).
    The frame in the center is the ground-truth label, also part of the dataset; a
    human expert has drawn a contour around the region of interest (lesion); such
    labels are used to train the model, and measure its performance after training.
    The frame on the right is my model’s prediction, which in this case is close to
    the ground-truth label. In this case, the model is not set to distinguish between
    malignant and benign lesions, they are all shown in yellow tones here.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 左侧的框是数据集中超声图像；它包含一个可能是恶性或良性的病变（黑暗区域）。中间的框是地面真实标签，同样是数据集的一部分；一位人类专家在感兴趣区域（病变）周围绘制了轮廓；这些标签用于训练模型，并在训练后评估其性能。右侧的框是我的模型预测，在这种情况下接近地面真实标签。在此情况下，模型未设置区分恶性和良性病变，它们都以黄色调显示。
- en: 'Building on top of the capstone described above, this project (also part of
    [the CADBUSI project at UWLAX](https://datascienceuwl.github.io/CADBUSI/) just
    like the capstone) starts with this key observation:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 基于上述描述的顶点项目，本项目（同样是[UWLAX的CADBUSI项目](https://datascienceuwl.github.io/CADBUSI/)的一部分，如同顶点项目）从这个关键观察开始：
- en: Medical imaging is somewhat different from other segmentation applications,
    in that the user (radiologist) looking at the model’s output has significant expertise
    in the field. The user is not completely passive, like in most other applications.
    And the question is not whether the model outperforms the human operator in how
    well it identifies a lesion, but it’s rather how to combine what the model can
    do, and what the user knows, in order to get better results overall.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 医学影像与其他分割应用有所不同，因为观察模型输出的用户（放射科医师）在该领域具有显著的专业知识。用户并不像大多数其他应用那样完全被动。问题不在于模型在识别病变方面是否优于人类操作员，而在于如何将模型的能力与用户的知识结合起来，以获得更好的整体结果。
- en: The user may agree with the model’s prediction in every way. Or there may be
    disagreement between the model and the user. Also, the user may possess knowledge
    about the patient that the model does not have. It would be useful if the user
    could provide hints or feedback to the model, extra input data as far as the model
    is concerned, in order to arrive at a higher quality prediction that combines
    the strengths of both model and user.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 用户可能在各个方面都同意模型的预测。或者模型和用户之间可能存在分歧。此外，用户可能拥有模型所没有的患者知识。如果用户能够向模型提供提示或反馈作为额外输入数据，将有助于得到更高质量的预测，结合模型和用户的优点。
- en: Additionally, the user should be able to provide feedback to the model in a
    simple way — e.g. by clicking the image with their mouse to highlight important
    areas. The coordinates of the user-generated mouse clicks become additional input
    to the model, which should then adjust its predictions accordingly.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，用户应该能够以简单的方式向模型提供反馈——例如，通过点击图像以突出显示重要区域。用户生成的鼠标点击的坐标成为模型的额外输入，模型应根据这些坐标调整其预测。
- en: If you’re building the model from scratch, you can design the input any way
    you want, to include various kinds of data. But in that case you need to do the
    full pre-training cycle (e.g. using ImageNet), which takes significant compute
    and time resources. If you use an off-the-shelf model, pre-trained on ImageNet,
    that saves a lot of time and effort, but it may seem like there is no room in
    the model’s input for the user feedback — these models are designed to take an
    image as the input and nothing else.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你从头开始构建模型，你可以设计任何你想要的输入，以包含各种数据。但在这种情况下，你需要进行完整的预训练周期（例如使用ImageNet），这需要大量的计算和时间资源。如果你使用一个在ImageNet上预训练的现成模型，这可以节省大量时间和精力，但模型的输入中可能没有为用户反馈留出空间——这些模型设计为仅接收图像作为输入。
- en: 'Unless, of course, you could identify redundancies in the input of an off-the-shelf
    model. That would mean there are redundant channels in the input that could be
    re-purposed to provide user feedback to the model, in addition to the usual imaging
    data. This series of articles will describe how to:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 除非你能够识别出现成模型输入中的冗余部分。这意味着输入中存在可以重新利用的冗余通道，以便向模型提供用户反馈，除了常规的影像数据。这一系列文章将描述如何：
- en: identify input redundancies in off-the-shelf image segmentation models
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 识别现成图像分割模型中的输入冗余。
- en: use redundant input channels to provide user feedback
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用冗余的输入通道来提供用户反馈。
- en: train the models to recognize the user feedback correctly
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练模型正确识别用户反馈
- en: automate the whole process as much as possible
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 尽可能地自动化整个过程
- en: When Models Are Wrong
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 当模型出错时
- en: 'Let’s consider this case:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑这种情况：
- en: '![](../Images/df010196dc9b67941aaa0ad4819a8673.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/df010196dc9b67941aaa0ad4819a8673.png)'
- en: 'source: Dataset of Breast Ultrasound Images'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '来源: 乳腺超声图像数据集'
- en: There seems to be a region of interest (RoI) on the left-hand side of the image
    — a small, dark, oval shape. This is shown in the label as a yellow area. But
    the model’s prediction is the null set — there are no yellow pixels in the prediction
    frame. The model does not appear to believe there is an RoI in this image. We,
    playing the role of the human expert here, disagree.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 图像左侧似乎有一个感兴趣区域（RoI）——一个小的、暗的、椭圆形的区域。这在标签中显示为黄色区域。但模型的预测结果为空集——预测框中没有黄色像素。模型似乎不认为图像中存在RoI。我们在这里扮演人类专家的角色，不同意这个观点。
- en: 'Or this case:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 或者这种情况：
- en: '![](../Images/1adc58e59d3d60a789e21cb2180237e1.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1adc58e59d3d60a789e21cb2180237e1.png)'
- en: 'source: Dataset of Breast Ultrasound Images'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '来源: 乳腺超声图像数据集'
- en: The human expert (the author of the label) believes there is only one true RoI
    in the image. The model, however, identifies two separate RoIs.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 人类专家（标签的作者）认为图像中只有一个真正的感兴趣区域（RoI）。然而，模型识别出了两个独立的RoI。
- en: In cases like these, where the predictions are quite marginal, and the human
    expert may have reason to disagree with the model’s predictions, it would be useful
    to allow feedback from the user to steer or guide the model’s predictions based
    on information or knowledge that the user has but the model doesn’t.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些情况下，当预测相当边缘化，而人类专家可能有理由不同意模型的预测时，允许用户反馈来引导或调整模型的预测会很有用，这样可以基于用户拥有但模型没有的信息或知识。
- en: Ideally, the user should be able to provide feedback via a very simple method,
    such as clicking various areas in the image — the information provided by the
    click coordinates should then be taken into account by the model to adjust its
    predictions. The click coordinates become part of the model input. This could
    be accomplished in a number of different ways.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 理想情况下，用户应该能够通过非常简单的方法提供反馈，例如点击图像中的各个区域——点击坐标提供的信息应被模型考虑，以调整其预测。点击坐标成为模型输入的一部分。这可以通过多种不同的方式实现。
- en: 'The PseudoClick paper by Liu et al. (2022) describes a model architecture where
    the clicks are provided to the model via separate input layers: the model has
    an input for the actual image, and a different input for the clicks. If you build
    your model from scratch, you can design it any way you like, and you could take
    suggestions from the PseudoClick architecture.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: Liu等人（2022）的《PseudoClick》论文描述了一种模型架构，其中点击通过单独的输入层提供给模型：模型有一个用于实际图像的输入和一个用于点击的不同输入。如果你从头开始构建你的模型，你可以按自己喜欢的方式设计它，并且可以借鉴PseudoClick架构的建议。
- en: But if you use an off-the-shelf model, you have to use the existing input. This
    is described in the next section.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 但如果你使用现成的模型，你必须使用现有的输入。这将在下一节中描述。
- en: Using the Color Channels to Provide Feedback
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用颜色通道提供反馈
- en: 'If you use an off-the-shelf vision model, chances are it was built to work
    with color images — the input to the model is actually three identical layers,
    one for each color channel: Red, Green, and Blue. If the input is black-and-white,
    which is the case for ultrasound images, the same information (pure luminance)
    is distributed the same way across all color channels. In other words, having
    three separate channels for the same information seems redundant.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你使用现成的视觉模型，可能它是为了处理彩色图像而构建的——模型的输入实际上是三个相同的层，每个层对应一个颜色通道：红色、绿色和蓝色。如果输入是黑白图像，如超声图像的情况，那么相同的信息（纯亮度）在所有颜色通道中分布的方式是相同的。换句话说，为了相同的信息有三个独立的通道似乎是多余的。
- en: Would the model work the same if only one color channel was used for monochrome
    images? Let’s say we zero out two color channels (R and G), and we keep the image
    information only in the B channel.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 如果只使用一个颜色通道处理单色图像，模型是否会表现相同？假设我们将两个颜色通道（R和G）归零，只保留B通道中的图像信息。
- en: '![](../Images/a37d1dfe1a2c2533e2117f76cce104c7.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a37d1dfe1a2c2533e2117f76cce104c7.png)'
- en: 'source: Dataset of Breast Ultrasound Images'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '来源: 乳腺超声图像数据集'
- en: Testing this idea with a pre-trained image segmentation model such as the SegFormer
    MiT-b3 ([available on the HuggingFace repository](https://huggingface.co/nvidia/mit-b3),
    pre-trained on ImageNet), it is immediately obvious that the model performance
    measured with the usual image segmentation metrics ([IoU](https://en.wikipedia.org/wiki/Jaccard_index),
    [Dice](https://en.wikipedia.org/wiki/S%C3%B8rensen%E2%80%93Dice_coefficient))
    does not change. The model works essentially the same. The redundancy from the
    color channels does not help, nor does it harm, when doing predictions on monochrome
    images.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 使用预训练的图像分割模型（例如 SegFormer MiT-b3（[可在 HuggingFace 仓库中找到](https://huggingface.co/nvidia/mit-b3)，在
    ImageNet 上预训练））测试这个想法，很明显，用通常的图像分割指标（[IoU](https://en.wikipedia.org/wiki/Jaccard_index)，[Dice](https://en.wikipedia.org/wiki/S%C3%B8rensen%E2%80%93Dice_coefficient)）测量的模型性能没有变化。模型的工作方式基本相同。在对单色图像进行预测时，颜色通道的冗余既没有帮助也没有伤害。
- en: That means we could keep the image data only in the B channel, and use the R
    and G channels for the additional input — the user-generated clicks. Not just
    that, but we have two separate channels, R and G, which could provide different
    kinds of input to the model.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着我们可以仅在B通道中保留图像数据，而将R和G通道用于额外的输入——用户生成的点击。不仅如此，我们还有两个独立的通道，R和G，这可以为模型提供不同类型的输入。
- en: 'That’s exactly what we need: one kind of click should be “activating” or “positive”,
    telling the model *“this is a region of interest, focus here”*, while the other
    kind should be “inhibitory” or “negative”, telling the model *“nothing is happening
    here, avoid this area”*.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 这正是我们需要的：一种点击应该是“激活”或“正面”的，告诉模型*“这是一个感兴趣的区域，集中注意力”*，而另一种点击应该是“抑制”或“负面”的，告诉模型*“这里没有活动，避免这个区域”*。
- en: The question is, what is the best way to place the clicks in the training data
    to make the models responsive to user input? This is described in the next section.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 问题是，如何在训练数据中放置点击，以使模型对用户输入作出响应？这将在下一部分中描述。
- en: True Positives, False Positives, False Negatives
  id: totrans-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 真阳性、假阳性、假阴性
- en: The prediction from a segmentation model is an area in the image where the pixels
    are marked in some way — e.g. by a non-zero value. When the predicted area matches
    closely the ground-truth label for that image, we say the model performs well.
    The predicted pixels that match the label are called true positives (TP).
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 分割模型的预测是图像中像素以某种方式标记的区域——例如，标记为非零值。当预测区域与该图像的真实标签紧密匹配时，我们说模型表现良好。与标签匹配的预测像素称为真阳性（TP）。
- en: 'Where the model makes non-zero predictions but the pixels in the label are
    zero, those pixels are false positives (FP). Where the pixels in the label are
    non-zero, but the model’s predictions are zero, those are false negatives (FN).
    Here’s an example:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在模型做出非零预测但标签中的像素为零的地方，这些像素被称为假阳性（FP）。在标签中的像素为非零而模型预测为零的地方，这些被称为假阴性（FN）。以下是一个示例：
- en: '![](../Images/775921ac2983817bdef629c4ebaf270d.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/775921ac2983817bdef629c4ebaf270d.png)'
- en: 'source: Dataset of Breast Ultrasound Images'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 来源：乳腺超声图像数据集
- en: 'The left-hand frame is the label. The center frame is the model’s prediction.
    In the right-hand frame, we’ve marked the true positives (TP) with white: the
    prediction pixels match the label pixels. The false positives (FP) are the non—zero
    prediction pixels that are zero in the label, and are marked green. The false
    negatives (FN) are the zero-value prediction pixels where there are non-zero pixels
    in the label, and are marked red.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 左侧框架是标签。中间框架是模型的预测。在右侧框架中，我们用白色标记了真阳性（TP）：预测像素与标签像素匹配。假阳性（FP）是那些预测值为非零但标签中为零的像素，用绿色标记。假阴性（FN）是那些预测值为零但标签中为非零的像素，用红色标记。
- en: If we knew the image areas where the models tend to be wrong and make false
    predictions (FP, FN), we could add clicks to the original dataset, marking the
    FP and FN areas. Since we’ve moved all the image information to the Blue channel,
    we could use the Red and Green channels for these clicks.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们知道模型容易出错并产生错误预测（FP，FN）的图像区域，我们可以向原始数据集中添加标记，标记这些FP和FN区域。由于我们已经将所有图像信息转移到了蓝色通道，我们可以利用红色和绿色通道来标记这些区域。
- en: 'For example, we could mark the False Positive areas with clicks in the Red
    channel. These, we hope, will become “inhibitory” or “negative” clicks that will
    steer the model away from making predictions there. The False Negative areas could
    be marked with clicks in the Green channel, which will become “activating” or
    “positive” clicks and guide the model to focus more on those areas. Examples:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，我们可以在红色通道中标记 FP 区域的点击。我们希望这些点击能成为“抑制性”或“负面”点击，指导模型避免在此处进行预测。FN 区域可以在绿色通道中标记点击，这些点击将成为“激活性”或“正面”点击，引导模型更多地关注这些区域。示例：
- en: '![](../Images/5e1d8d17f4481965a6f75aad3f46bb87.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5e1d8d17f4481965a6f75aad3f46bb87.png)'
- en: 'source: Dataset of Breast Ultrasound Images'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 来源：乳腺超声图像数据集
- en: '![](../Images/8a8ae7bf47de6111503eb31dd631d71e.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8a8ae7bf47de6111503eb31dd631d71e.png)'
- en: 'source: Dataset of Breast Ultrasound Images'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 来源：乳腺超声图像数据集
- en: In the images shown above, we’ve placed Red channel clicks (negative clicks)
    in the False Positive areas (where the model predicts an RoI that doesn’t exist),
    and we’ve placed Green channel clicks (positive clicks) in the False Negative
    areas (where the model predicts nothing but there is an actual RoI). For good
    measure, we place a few more Green channel clicks in the True Positive areas,
    to “anchor” the prediction and make sure the TP areas remain stable.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在上面显示的图像中，我们在错误预测的区域（模型预测存在 RoI 但实际上不存在）放置了红色通道点击（负点击），并在假阴性区域（模型预测什么都没有但实际上有
    RoI）放置了绿色通道点击（正点击）。为了保险起见，在真阳性区域我们还放置了几个绿色通道点击，以“锚定”预测并确保 TP 区域保持稳定。
- en: The complete training procedure using the clicks is described below.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 使用点击完成的完整训练过程如下所述。
- en: How to Train Models With Clicks
  id: totrans-64
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何使用点击训练模型
- en: 'These are the main steps involved in training models to become responsive to
    user input:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是训练模型以响应用户输入的主要步骤：
- en: decide on a particular off-the-shelf image segmentation model, such as SegFormer
    MiT-b3 pre-trained with ImageNet
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 决定采用特定的现成图像分割模型，例如预先训练的 SegFormer MiT-b3 模型（使用 ImageNet 进行预训练）
- en: process all your monochrome images such that the image data only exists in the
    B channel; the R and G channels will be made blank
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理所有单色图像，使得图像数据仅存在于 B 通道中；将 R 和 G 通道置空
- en: split the image dataset into 5 folds; fine-tune a model for each fold; this
    will create 5 models, each fine-tuned for a different fold in the dataset; let’s
    call these **the baseline models**
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将图像数据集分成 5 折；对每个折进行微调，这将创建 5 个模型，每个模型针对数据集中的不同折进行微调；我们称这些为**基线模型**
- en: use each one of the 5 baseline models to make predictions on the images it has
    not seen in training; this will generate predictions for all images in the dataset
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用每个基线模型对其未在训练中见过的图像进行预测；这将为数据集中的所有图像生成预测结果
- en: for each prediction, determine the TP, FP, FN areas; cover the FP, FN, and optionally
    TP areas with R and G clicks as described above; small areas receive one click
    each; large areas receive multiple clicks; for now, assume that the click coordinates
    are generated manually (more on this later)
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于每个预测，确定 TP、FP、FN 区域；使用上述描述的 R 和 G 点击覆盖 FP、FN 和可选的 TP 区域；小区域每个接收一个点击；大区域接收多个点击；暂时假设点击坐标手动生成（稍后详述）
- en: embed the clicks into the R and G channels in the dataset, but leave the B channel
    untouched; each click will be a 3x3 pixels area in the R or G channels where we
    set the pixel values to the maximum value for that channel (e.g. 255 for the uint8
    dtype)
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将点击嵌入数据集中的 R 和 G 通道中，但保留 B 通道不变；每个点击将成为 R 或 G 通道中的一个 3x3 像素区域，其中我们将像素值设为该通道的最大值（例如
    uint8 类型的 255）
- en: using the same 5 folds, train 5 new models on the dataset with the clicks added
    to the R and G channels; let’s call these **the click-trained models**
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用相同的 5 折，对数据集进行训练并在 R 和 G 通道中添加点击训练 5 个新模型；我们称这些为**点击训练模型**
- en: In other words, we train the **baseline models** to “forecast” where the models
    tend to be wrong, we add clicks as needed to the “wrong” areas, then train new
    models (**click-trained models**) with the clicks added to the dataset. We hope
    that the click-trained models will become responsive to clicks provided via the
    R and G channels. [The full code is shown here](https://github.com/FlorinAndrei/segmentation_click_train/blob/main/train_models.ipynb).
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，我们训练**基线模型**来“预测”模型倾向于错误的地方，我们根据需要在“错误”区域添加点击，然后使用添加了点击的数据集训练新模型（**点击训练模型**）。我们希望通过
    R 和 G 通道提供的点击，点击训练模型能够作出响应。[完整的代码在这里显示](https://github.com/FlorinAndrei/segmentation_click_train/blob/main/train_models.ipynb)。
- en: 'For clarity, this is what the images will look like, after all processing,
    with the clicks added:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 为了清楚起见，处理后加上点击的图像将如下所示：
- en: '![](../Images/cea79304fcb931c1efe9288e8105ef2a.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cea79304fcb931c1efe9288e8105ef2a.png)'
- en: 'source: Dataset of Breast Ultrasound Images'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 来源：乳腺超声图像数据集
- en: '![](../Images/5cf078da733a15cbe82ffbd083419b39.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5cf078da733a15cbe82ffbd083419b39.png)'
- en: 'source: Dataset of Breast Ultrasound Images'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 来源：乳腺超声图像数据集
- en: The left-hand frame is the ground-truth label we’re trying to match. The center
    frame is the prediction from the baseline models. The right-hand frame shows the
    processed images used to train the click-trained models — all image data has been
    moved to the B channel, and clicks have been added to the R and G channels as
    needed to modify predictions.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 左侧框架是我们尝试匹配的真实标签。中间框架是基线模型的预测。右侧框架显示了用于训练点击训练模型的处理图像——所有图像数据已转移到 B 通道，点击已根据需要添加到
    R 和 G 通道，以修改预测。
- en: You don’t need to actually modify the dataset to add the R and G clicks. You
    could simply register the click coordinates, and modify the dataloader to apply
    the clicks dynamically when the models are trained. This variant is far more flexible
    if you need to re-generate click coordinates for whatever reason.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 你不需要实际修改数据集来添加 R 和 G 点击。你可以简单地注册点击坐标，并修改数据加载器，以便在训练模型时动态应用点击。如果你需要重新生成点击坐标，这种变体要灵活得多。
- en: Also, you could take a hint from image augmentation techniques, and apply the
    clicks in training with some probability. The clicks are not always added to the
    input, but only in some fraction of cases, randomly. I’ve used a probability of
    0.9 with good results. The idea is to not make the models over-dependent on clicks.
    Fine-tuning this parameter may require further exploration.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，你可以借鉴图像增强技术，以一定概率在训练中应用点击。点击并不总是添加到输入中，而只是随机地在某些情况下添加。我使用了 0.9 的概率，效果很好。这个想法是避免让模型过度依赖点击。微调这个参数可能需要进一步探索。
- en: Results
  id: totrans-82
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结果
- en: Does it work?
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 它有效吗？
- en: 'It sure does. Here is a click-trained model making predictions, then responding
    to user feedback in real time:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 确实如此。这里是一个经过点击训练的模型进行预测，然后实时响应用户反馈的例子：
- en: We ask the model to make predictions, and it covers the two dark areas in the
    upper half of the image. We disagree with the model — we think the area on the
    left is not a region of interest, so we place an inhibitory (red) click on it.
    We also place an activating (green) click on the area to the right. Now the model’s
    predictions follow the additional information that we have.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 我们要求模型进行预测，它覆盖了图像上半部分的两个暗区。我们不同意模型的预测——我们认为左侧区域不是感兴趣区域，因此我们在上面放置了一个抑制（红色）点击。我们还在右侧区域放置了一个激活（绿色）点击。现在模型的预测跟随我们提供的额外信息。
- en: 'It should be noted that a single click, about 3x3 pixels in size, influences
    the model’s predictions in an area a few hundreds of pixels in diameter. Also,
    the model takes into account both the presence of the click, and the features
    visible in the image: placing a click in an RoI makes the model fill that whole
    region with a prediction mask, following the contour visible in the image.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 应该注意的是，单个点击（约 3x3 像素）会影响模型在直径几百像素区域内的预测。此外，模型会考虑点击的存在以及图像中可见的特征：在 RoI 中放置一个点击会使模型用预测掩膜填充整个区域，遵循图像中可见的轮廓。
- en: There are cases where the model follows the user feedback quite easily — this
    is where there is high ambiguity / low confidence for the predictions in the model
    output. There are other cases where the model’s predictions will resist being
    “evicted” from an RoI by a negative click — this is when there is low ambiguity
    / high confidence in the model’s own output.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 模型有时会很容易遵循用户反馈——这是在模型输出预测存在高歧义/低置信度的情况下。还有其他情况，模型的预测会抵抗被负点击“驱逐”——这是在模型自身输出具有低歧义/高置信度时。
- en: Scalability
  id: totrans-88
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 可扩展性
- en: The main problem with the technique described so far is scalability. We’ve assumed
    that the click coordinates are generated manually. In other words, a human operator
    needs to literally sift through all images, compare ground-truth labels with predictions,
    decide where to place the clicks, and how many clicks to place, and record all
    click coordinates.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止描述的技术的主要问题是可扩展性。我们假设点击坐标是手动生成的。换句话说，需要人工操作员逐一筛选所有图像，比对真实标签与预测，决定点击位置及数量，并记录所有点击坐标。
- en: This obviously doesn’t scale. It is already tedious and time-consuming, but
    not impossible, to generate a single set of clicks for one dataset containing
    hundreds of images. If the dataset contains thousands of images or more, or especially
    if the set of clicks needs to be re-generated as the baseline models change, the
    task becomes impossible. Some kind of automation is needed.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，这种方法无法扩展。为包含数百张图像的数据集生成一组点击已经很繁琐且耗时，但还不是不可能的。如果数据集包含数千张图像，或者特别是如果在基准模型更改时需要重新生成点击集，则任务变得不可能。需要某种形式的自动化。
- en: '[That will be the topic of Part 2 of this series](/train-image-segmentation-models-to-accept-user-feedback-via-voronoi-tiling-part-2-1f02eebbddb9).
    We will show how to automate the creation of click coordinates, to the point where
    the training process could run completely unsupervised. [Part 2](/train-image-segmentation-models-to-accept-user-feedback-via-voronoi-tiling-part-2-1f02eebbddb9)
    will describe an algorithm that generates the clicks in a way that is very similar
    to the decisions that a human operator would make.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '[这将是本系列第二部分的主题](/train-image-segmentation-models-to-accept-user-feedback-via-voronoi-tiling-part-2-1f02eebbddb9)。我们将展示如何自动化创建点击坐标，以便训练过程可以完全无监督地运行。[第二部分](/train-image-segmentation-models-to-accept-user-feedback-via-voronoi-tiling-part-2-1f02eebbddb9)将描述一种生成点击的算法，这种算法与人工操作员做出的决策非常相似。'
- en: Links, Citations, Comments
  id: totrans-92
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 链接、引用、评论
- en: 'This project is an extension of my capstone project from the final semester
    of my MS in Data Science studies: [https://github.com/FlorinAndrei/datascience_capstone_project](https://github.com/FlorinAndrei/datascience_capstone_project)'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 这个项目是我在数据科学硕士课程最后一个学期的毕业设计项目的扩展：[https://github.com/FlorinAndrei/datascience_capstone_project](https://github.com/FlorinAndrei/datascience_capstone_project)
- en: Both the capstone and this work were done within the Computer Aided Diagnosis
    for Breast Ultrasound Imagery (CADBUSI) project at the University of Wisconsin-La
    Crosse, under the supervision of Dr. Jeff Baggett. [https://datascienceuwl.github.io/CADBUSI/](https://datascienceuwl.github.io/CADBUSI/)
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 这个项目和我的毕业设计都在威斯康星大学拉克罗斯分校的计算机辅助乳腺超声影像（CADBUSI）项目中完成，由杰夫·巴格特博士监督。[https://datascienceuwl.github.io/CADBUSI/](https://datascienceuwl.github.io/CADBUSI/)
- en: 'The GitHub repository with code for this article: [https://github.com/FlorinAndrei/segmentation_click_train](https://github.com/FlorinAndrei/segmentation_click_train)'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 本文代码的GitHub仓库：[https://github.com/FlorinAndrei/segmentation_click_train](https://github.com/FlorinAndrei/segmentation_click_train)
- en: 'All ultrasound images used in this article are part of the Dataset of Breast
    Ultrasound Images, available under the CC BY 4.0 license. Citation link:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 本文中使用的所有超声图像均属于乳腺超声图像数据集，按CC BY 4.0许可证提供。引用链接：
- en: Al-Dhabyani, W., Gomaa, M., Khaled, H., & Fahmy, A. (2019). Dataset of Breast
    Ultrasound Images. *ResearchGate*. Retrieved May 1, 2023 from [https://www.sciencedirect.com/science/article/pii/S2352340919312181](https://www.sciencedirect.com/science/article/pii/S2352340919312181)
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: Al-Dhabyani, W., Gomaa, M., Khaled, H., & Fahmy, A. (2019). 乳腺超声图像数据集。*ResearchGate*。检索于2023年5月1日，[https://www.sciencedirect.com/science/article/pii/S2352340919312181](https://www.sciencedirect.com/science/article/pii/S2352340919312181)
- en: 'Other links, citations and comments:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 其他链接、引用和评论：
- en: 'Liu, Q., Zheng, M., Planche, B., Karanam, S., Chen, T., Niethammer, M., & Wu,
    Z. (2022). PseudoClick: Interactive Image Segmentation with Click Imitation. *arXiv.org*.
    Retrieved May 1, 2023, from [https://arxiv.org/abs/2207.05282](https://arxiv.org/abs/2207.05282)'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 'Liu, Q., Zheng, M., Planche, B., Karanam, S., Chen, T., Niethammer, M., & Wu,
    Z. (2022). PseudoClick: 通过点击模仿进行交互式图像分割。*arXiv.org*。检索于2023年5月1日，[https://arxiv.org/abs/2207.05282](https://arxiv.org/abs/2207.05282)'
- en: 'Xie, E., Wang, W., Yu, Z., Anandkumar, A., Alvarez, J. M., & Luo. P. (2021).
    SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers.
    arXiv.org. Retrieved May 1, 2023, from [https://arxiv.org/abs/2105.15203](https://arxiv.org/abs/2105.15203)'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 'Xie, E., Wang, W., Yu, Z., Anandkumar, A., Alvarez, J. M., & Luo, P. (2021).
    SegFormer: 使用变换器进行语义分割的简单高效设计。arXiv.org。检索于2023年5月1日，[https://arxiv.org/abs/2105.15203](https://arxiv.org/abs/2105.15203)'
- en: 'Pretrained SegFormer models at HuggingFace: [https://huggingface.co/docs/transformers/model_doc/segformer](https://huggingface.co/docs/transformers/model_doc/segformer)'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: HuggingFace上的预训练SegFormer模型：[https://huggingface.co/docs/transformers/model_doc/segformer](https://huggingface.co/docs/transformers/model_doc/segformer)
- en: Images in this article that are not part of the Dataset of Breast Ultrasound
    Images are created by the author.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 本文中不属于乳腺超声图像数据集的图像由作者创建。
