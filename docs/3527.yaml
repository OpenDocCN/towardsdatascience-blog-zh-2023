- en: Bias, Toxicity, and Jailbreaking Large Language Models (LLMs)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/bias-toxicity-and-jailbreaking-large-language-models-llms-37cd71a3048f?source=collection_archive---------5-----------------------#2023-11-28](https://towardsdatascience.com/bias-toxicity-and-jailbreaking-large-language-models-llms-37cd71a3048f?source=collection_archive---------5-----------------------#2023-11-28)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: A review of recent research on concerning characteristics of LLMs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://rachel-draelos.medium.com/?source=post_page-----37cd71a3048f--------------------------------)[![Rachel
    Draelos, MD, PhD](../Images/edc30d41f9fea7e57dcf0c44caf68165.png)](https://rachel-draelos.medium.com/?source=post_page-----37cd71a3048f--------------------------------)[](https://towardsdatascience.com/?source=post_page-----37cd71a3048f--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----37cd71a3048f--------------------------------)
    [Rachel Draelos, MD, PhD](https://rachel-draelos.medium.com/?source=post_page-----37cd71a3048f--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F209c0f742bcf&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbias-toxicity-and-jailbreaking-large-language-models-llms-37cd71a3048f&user=Rachel+Draelos%2C+MD%2C+PhD&userId=209c0f742bcf&source=post_page-209c0f742bcf----37cd71a3048f---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----37cd71a3048f--------------------------------)
    ·17 min read·Nov 28, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F37cd71a3048f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbias-toxicity-and-jailbreaking-large-language-models-llms-37cd71a3048f&user=Rachel+Draelos%2C+MD%2C+PhD&userId=209c0f742bcf&source=-----37cd71a3048f---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F37cd71a3048f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbias-toxicity-and-jailbreaking-large-language-models-llms-37cd71a3048f&source=-----37cd71a3048f---------------------bookmark_footer-----------)![](../Images/7ca68df720b68138e8d971d351c51b49.png)'
  prefs: []
  type: TYPE_NORMAL
- en: The featured image is derived from the [Galton box video from Wikimedia Commons](https://en.wikipedia.org/wiki/File:Galton_box.webm)
    (Creative Commons Attribution-Share Alike 4.0 International license).
  prefs: []
  type: TYPE_NORMAL
- en: 'CONTENT WARNING: This post contains examples of biased, toxic text generated
    by LLMs.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This post provides a deep dive into recent research on bias, toxicity, and
    jailbreaking of large language models (LLMs), especially ChatGPT and GPT-4\. I’ll
    discuss the ethical guidelines companies are currently using in LLM development
    and the approaches they use to try to safeguard against generation of undesirable
    content. Then I’ll review recent research papers studying toxic content generation,
    jailbreaking, and bias from multiple angles: gender, race, medicine, politics,
    the workplace, and fiction.'
  prefs: []
  type: TYPE_NORMAL
- en: Bias refers to prejudice in favor or or against a specific group, person, or
    thing, while toxicity refers to disrespectful, vulgar, rude, or harm-promoting
    content. LLMs are biased and have the capacity to generate toxic content because
    they are trained on vast quantities of Internet data, which unfortunately represents
    both the good and bad sides of humanity, including all of our biases and toxicity.
    Thankfully, developers of LLMs like OpenAI and Google have taken steps to reduce
    the chances of LLMs producing overtly biased or toxic content. However, as we
    will see, that doesn’t mean the models are…
  prefs: []
  type: TYPE_NORMAL
