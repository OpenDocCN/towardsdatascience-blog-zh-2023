- en: t-SNE from Scratch (ft. NumPy)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/t-sne-from-scratch-ft-numpy-172ee2a61df7?source=collection_archive---------2-----------------------#2023-04-14](https://towardsdatascience.com/t-sne-from-scratch-ft-numpy-172ee2a61df7?source=collection_archive---------2-----------------------#2023-04-14)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/4d9ef097294cb21259a550a26efce328.png)'
  prefs: []
  type: TYPE_IMG
- en: Cover Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: Acquire a deep understanding of the inner workings of t-SNE via implementation
    from scratch in python
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@jakepenzak?source=post_page-----172ee2a61df7--------------------------------)[![Jacob
    Pieniazek](../Images/2d9c6295d39fcaaec4e62f11c359cb29.png)](https://medium.com/@jakepenzak?source=post_page-----172ee2a61df7--------------------------------)[](https://towardsdatascience.com/?source=post_page-----172ee2a61df7--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----172ee2a61df7--------------------------------)
    [Jacob Pieniazek](https://medium.com/@jakepenzak?source=post_page-----172ee2a61df7--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F6f0948d99b1c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ft-sne-from-scratch-ft-numpy-172ee2a61df7&user=Jacob+Pieniazek&userId=6f0948d99b1c&source=post_page-6f0948d99b1c----172ee2a61df7---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----172ee2a61df7--------------------------------)
    ·17 min read·Apr 14, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F172ee2a61df7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ft-sne-from-scratch-ft-numpy-172ee2a61df7&user=Jacob+Pieniazek&userId=6f0948d99b1c&source=-----172ee2a61df7---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F172ee2a61df7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ft-sne-from-scratch-ft-numpy-172ee2a61df7&source=-----172ee2a61df7---------------------bookmark_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: I have found that one of the best ways to *truly* understanding any statistical
    algorithm or methodology is to manually implement it yourself. On the flip side,
    coding these algorithms can sometimes be time consuming and a real pain, and when
    somebody else has already done it, why would I want to spend my time doing it
    — seems inefficient, no? Both are fair points, and I am not here to make an argument
    for one over the other.
  prefs: []
  type: TYPE_NORMAL
- en: This article is designed for readers who are interested in understanding t-SNE
    via translation of the mathematics in the [original paper](https://jmlr.org/papers/volume9/vandermaaten08a/vandermaaten08a.pdf)
    — by Laurens van der Maaten & Geoffrey Hinton — into python code implementation.[1]
    I find these sort of exercises to be quite illuminating into the inner workings
    of statistical algorithms/models and really test your underlying understanding
    and assumptions regarding these algorithms/models. You are almost guaranteed to
    walk away with a better understanding then you had before. At the very minimum,
    successful implementation is always very satisfying!
  prefs: []
  type: TYPE_NORMAL
- en: 'This article will be accessible to individuals with any level of exposure of
    t-SNE. However, note a few things this post definitely is **not**:'
  prefs: []
  type: TYPE_NORMAL
- en: A *strictly* conceptual introduction and exploration of t-SNE, as there are
    plenty of other great resources out there that do this; nevertheless, I will be
    doing my best to connect the mathematical equations to their intuitive/conceptual
    counterparts at each stage of implementation.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A *comprehensive* discussion into the applications & pros/cons of t-SNE, as
    well as direct comparisons of t-SNE to other dimensionality reduction techniques.
    I will, however, briefly touch on these topics throughout this article, but will
    by no means cover this in-depth.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Without further ado, let’s start with a *brief* introduction into t-SNE.
  prefs: []
  type: TYPE_NORMAL
- en: A Brief Introduction to t-SNE
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*t-distributed stochastic neighbor embedding (*t-SNE) is a dimensionality reduction
    tool that is primarily used in datasets with a large dimensional feature space
    and enables one to visualize the data down, or project it, into a lower dimensional
    space (usually 2-D). It is especially useful for visualizing non-linearly separable
    data wherein linear methods such as [Principal Component Analysis](https://en.m.wikipedia.org/wiki/Principal_component_analysis)
    (PCA) would fail. Generalizing linear frameworks of dimensionality reduction (such
    as PCA) into non-linear approaches (such as t-SNE) is also known as [Manifold
    Learning](https://en.m.wikipedia.org/wiki/Nonlinear_dimensionality_reduction).
    These methods can be extremely useful for visualizing and understanding the underlying
    structure of a high dimensional, non-linear data set, and can be great for disentangling
    and grouping together observations that are similar in the high-dimensional space.
    For more information on t-SNE and other manifold learning techniques, the [scikit-learn
    documentation](https://scikit-learn.org/stable/modules/manifold.html) is a great
    resource. Additionally, to read about some cool areas t-SNE has seen applications,
    the [Wikipedia page](https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding#cite_note-3)
    highlights some of these areas with references to the work.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s start with breaking down the name *t-distributed stochastic neighbor
    embedding* into its components*.* t-SNE is an extension on stochastic neighbor
    embedding (SNE) presented 6 years earlier in this [paper](https://cs.nyu.edu/~roweis/papers/sne_final.pdf)
    by Geoffrey Hinton & Sam Roweis. So let’s start there. The *stochastic* part of
    the name comes from the fact that the objective function is not convex and thus
    different results can arise from different initializations. The *neighbor embedding*
    highlights the nature of the algorithm — optimally mapping the points in the original
    high-dimensional space into the corresponding low-dimensional space while best
    preserving the “neighborhood” structure of the points. SNE is comprised of the
    following (simplified) steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Obtain the Similarity Matrix between Points in the Original Space:* Compute
    the conditional probabilities for each datapoint *j* relative to each datapoint
    *i*. These conditional probabilities are calculated in the original high-dimensional
    space using a Gaussian centered at *i* and take on the following interpretation:
    the probability that *i* would pick *j* as its neighbor in the original space.This
    creates a matrix that represents similarities between the points.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Initialization:* Choose random starting points in the lower-dimensional space
    (say, 2-D) for each datapoint in the original space and compute new conditional
    probabilities similarly as above in this new space.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Mapping:* Iteratively improve upon the points in the lower-dimensional space
    until the [Kullback-Leibler](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence)
    divergences between all the conditional probabilities is minimized. Essentially
    we are minimizing the differences in the probabilities between the similarity
    matrices of the two spaces so as to ensure the similarities are best preserved
    in the mapping of the original high-dimensional dataset to the low-dimensional
    dataset.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 't-SNE improves upon SNE in two primary ways:'
  prefs: []
  type: TYPE_NORMAL
- en: It minimizes the [Kullback-Leibler](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence)
    divergences between the *joint probabilities* rather than the conditional probabilities.
    The authors refer to this as “symmetric SNE” b/c their approach ensures that the
    joint probabilities *p_ij* = *p_ji.* **This results in a much better behaved cost
    function that is easier to optimize.**
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'It computes the similarities between points using a [Student-t distribution](https://en.wikipedia.org/wiki/Student%27s_t-distribution)
    w/ one degree of freedom (also a [Cauchy Distribution](https://en.wikipedia.org/wiki/Cauchy_distribution))
    rather than a Gaussian in the *low-dimensional* space (step 2 above). Here we
    can see where the “t” in t-SNE is coming from. **This improvement helps to alleviate
    the “crowding problem” highlighted by the authors and to further improve the optimization
    problem.** This “crowding problem” can be envisioned as such: Imagine we have
    a 10-D space, the amount of space available in 2-D will not be sufficient to accurately
    capture those moderately dissimilar points compared to the amount of space for
    nearby points relative to the amount of space available in a 10-D space. More
    simply, just envision taking a 3-D space and projecting it down to 2-D, the 3-D
    space will have much more overall space to model the similarities relative to
    the projection down into 2-D. The Student-t distribution helps alleviate this
    problem by having heavier tails than the normal distribution. See the [original
    paper](https://jmlr.org/papers/volume9/vandermaaten08a/vandermaaten08a.pdf) for
    a much more in-depth treatment of this problem.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'If this did not all track immediately, that is okay! I am hoping when we implement
    this in code, the pieces will all fall in to place. The main takeaway is this:
    **t-SNE models similarities between datapoints in the high-dimensional space using
    joint probabilities of “datapoints choosing others as its neighbor”, and then
    tries to find the best mapping of these points down into the low-dimensional space,
    while best preserving the original high-dimensional similarities.**'
  prefs: []
  type: TYPE_NORMAL
- en: Implementation from Scratch
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s now move on to understanding t-SNE via implementing the original version
    of the algorithm as presented in the [paper](https://lvdmaaten.github.io/publications/papers/JMLR_2008.pdf)
    by Laurens van der Maaten & Geoffrey Hinton. We will first start with implementing
    algorithm 1 below step-by-step, which will cover 95% of the main algorithm. There
    are two additional enhancements the authors note: 1) Early Exaggeration & 2) Adaptive
    Learning Rates. We will only discuss adding in the early exaggeration as that
    is most conducive in aiding the interpretation of the actual algorithms inner
    workings, as the adaptive learning rate is focused on improving the rates of convergence.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2e3f7799e417a127b3257121722fa4d7.png)'
  prefs: []
  type: TYPE_IMG
- en: Algorithm 1 (from [paper](https://lvdmaaten.github.io/publications/papers/JMLR_2008.pdf))
  prefs: []
  type: TYPE_NORMAL
- en: '**1\. Inputs & Outputs**'
  prefs: []
  type: TYPE_NORMAL
- en: Following the original [paper](https://lvdmaaten.github.io/publications/papers/JMLR_2008.pdf),
    we will be using the publicly available MNIST [dataset](https://www.openml.org/search?type=data&status=active&id=554)
    from OpenML with images of handwritten digits from 0–9.[2] We will also randomly
    sample 1000 images from the dataset & reduce the dimensionality of the dataset
    using Principal Component Analysis (PCA) and keep 30 components. These are both
    to improve computational time of the algorithm, as the code here is not optimized
    for speed, but rather for interpretability & learning.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'This will be our *X* dataset with each *row* being an image and each *column*
    being a feature, or principal component in this case (i.e. linear combinations
    of the original pixels):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6dd2af5bc94d63503a5bf3c96b53c64b.png)'
  prefs: []
  type: TYPE_IMG
- en: Sample of 1000 from MNIST Dataset with first 30 principal components
  prefs: []
  type: TYPE_NORMAL
- en: We also will need to specify the cost function parameters — perplexity — and
    the optimization parameters — iterations, learning rate, & momentum. We will hold
    off on these for now and address them as they appear at each stage.
  prefs: []
  type: TYPE_NORMAL
- en: 'In terms of output, recall that we seek a the low-dimensional mapping of the
    original dataset *X.* We will be mapping the original space into a 2 dimensional
    space throughout this example. Thus, our new output will be the 1000 images now
    represented in a 2 dimensional space rather than the original 30 dimensional space:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/dc89f27e6f8c53c7539ca276e6d0ff4b.png)'
  prefs: []
  type: TYPE_IMG
- en: Desired Output in 2-D Space
  prefs: []
  type: TYPE_NORMAL
- en: '**2\. Compute Affinities/Similarities of *X* in the Original Space**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have our inputs, the first step is to compute the pairwise similarities
    in the original high dimensional space. That is, for each image *i* we compute
    the probability that *i* would pick image *j* as its neighbor in the original
    space for each *j*. These probabilities are calculated via a normal distribution
    centered around each point and then are normalized to sum up to 1\. Mathematically,
    we have:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e4da47d9af18e17d9a3356bd293c0585.png)'
  prefs: []
  type: TYPE_IMG
- en: Eq. (1) — High Dimensional Affinity
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that, in our case with *n = 1000*, these computations will result in a
    *1000* x *1000* matrix of similarity scores. Note we set *p = 0* whenever *i =
    j* b/c we are modeling pairwise similarities. However, you may notice that we
    have not mentioned how σ is determined. This value is determined for each observation
    *i* via a grid searchbased on the user-specified desired [perplexity](https://en.wikipedia.org/wiki/Perplexity)
    of the distributions. We will talk about this immediately below, but let’s first
    look at how we would code eq. (1) above:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Now before we look at the results of this code, let’s discuss how we determine
    the values of σ via the grid_search() function. Given a specified value of perplexity
    (which in this context can be loosely thought about as the number of nearest neighbors
    for each point), we do a grid search over a range of values of σ such that the
    following equation is as close to equality as possible for each *i*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5f487e931a57b04a170b15731b946439.png)'
  prefs: []
  type: TYPE_IMG
- en: Perplexity
  prefs: []
  type: TYPE_NORMAL
- en: where H(P) is the Shannon [entropy](https://en.wikipedia.org/wiki/Entropy_(information_theory))
    of P.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/59b1bbe330ddaf7cdc88d57200ed76e9.png)'
  prefs: []
  type: TYPE_IMG
- en: Shannon Entropy of P
  prefs: []
  type: TYPE_NORMAL
- en: 'In our case, we will set perplexity = 10 and set the search space to be defined
    by [0.01 * standard deviation of the norms for the difference between images *i*
    and *j*, 5 * standard deviation of the norms for the difference between images
    *i* and *j*] divided into 200 equal steps. Knowing this, we can define our grid_search()
    function as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Given these functions, we can compute the affinity matrix via `p_ij = get_original_pairwise_affinities(X)`
    where we obtain the following matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/15c579861c711dad1aaecc25972b5c4e.png)'
  prefs: []
  type: TYPE_IMG
- en: Affinity Matrix of Conditional Probabilities in Original High Dimensional Space
  prefs: []
  type: TYPE_NORMAL
- en: 'Note, the diagonal elements are set to ε ≈ 0 by construction (whenever *i =
    j*). Recall that a key extension of the t-SNE algorithm is to compute the joint
    probabilities rather than the conditional probabilities. This is computed simply
    as follow:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1985cc2d53d64bd1f1913a4845acf957.png)'
  prefs: []
  type: TYPE_IMG
- en: Converting Conditional Probabilities to Joint Probabilities
  prefs: []
  type: TYPE_NORMAL
- en: 'Thus, we can define a new function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Feeding in `p_ij` from above, we have `p_ij_symmetric = get_symmetric_p_ij(p_ij)`
    where we obtain the following *symmetric* affinities matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/06c5403b356c0c8d8e7d821d85fb8c82.png)'
  prefs: []
  type: TYPE_IMG
- en: Symmetric Affinity Matrix of Joint Probabilities in Original High Dimensional
    Space
  prefs: []
  type: TYPE_NORMAL
- en: Now we have completed the first main step in t-SNE! We computed the symmetric
    affinity matrix in the original high-dimensional space. Before we dive right into
    the optimization stage, we will discuss the main components of the optimization
    problem in the next two steps and then combine them into our for loop.
  prefs: []
  type: TYPE_NORMAL
- en: '**3\. Sample Initial Solution & Compute Low Dimensional Affinity Matrix**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we want to sample a random initial solution in the lower dimensional space
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'where calling `y0 = initialization(X)` we obtain a random starting solution:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/dcdd43442d43ea88a02a0b8f811a7100.png)'
  prefs: []
  type: TYPE_IMG
- en: Initial Random Solution in 2-D
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we want to compute the affinity matrix in this lower dimensional space.
    However, recall that we do this utilizing a student-t distribution w/ 1 degree
    of freedom:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/73872a630d696cbefd02c6945aafd94a.png)'
  prefs: []
  type: TYPE_IMG
- en: Eq. (4) — Low Dimensional Affinity
  prefs: []
  type: TYPE_NORMAL
- en: 'Again, we set *q = 0* whenever *i = j*. Note this equation differs from eq.
    (1) in that the denominator is over all *i* and thus symmetric by construction.
    Putting this into code, we obtain:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Here we are seeking a *1000* x *1000* affinity matrix but now in the lower
    dimensional space. Calling `q_ij = get_low_dimensional_affinities(y0)` we obtain:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e51d9873343526bd4ece743d437dd250.png)'
  prefs: []
  type: TYPE_IMG
- en: Symmetric Affinity Matrix of Joint Probabilities in New Low Dimensional Space
  prefs: []
  type: TYPE_NORMAL
- en: '**4\. Compute Gradient of the Cost Function**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Recall, our cost function is the Kullback-Leibler divergence between joint
    probability distributions in the high dimensional space and low dimensional space:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4cb5562aab6cfcef0a783763d828dbad.png)'
  prefs: []
  type: TYPE_IMG
- en: Kullback-Leibler divergence between joint probability distributions
  prefs: []
  type: TYPE_NORMAL
- en: 'Intuitively, we want to minimize the difference in the affinity matrices `p_ij`
    and `q_ij` thereby best preserving the “neighborhood” structure of the original
    space. The optimization problem is solved using gradient descent, but first let’s
    look at computing the gradient for the cost function above. The authors derive
    (see appendix A of the [paper](https://lvdmaaten.github.io/publications/papers/JMLR_2008.pdf))
    the gradient of the cost function as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/30cc398d3c3e508cf2cba4714373e815.png)'
  prefs: []
  type: TYPE_IMG
- en: Gradient of Cost Function (Eq. 5, but from appendix)
  prefs: []
  type: TYPE_NORMAL
- en: 'In python, we have:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Feeding in the relevant arguments, we obtain the gradient at `y0` via `gradient
    = get_gradient(p_ij_symmetric,q_ij,y0)` with the corresponding output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c9256b70c406c44ed02b0e937ede1b64.png)'
  prefs: []
  type: TYPE_IMG
- en: Gradient of Cost Function at Initial Solution (y0)
  prefs: []
  type: TYPE_NORMAL
- en: Now, we have all the pieces in order to solve the optimization problem!
  prefs: []
  type: TYPE_NORMAL
- en: '**5\. Iterate & Optimize the Low-Dimensional Mapping**'
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to update our low-dimensional mapping, we use [gradient descent with
    momentum](https://en.wikipedia.org/wiki/Gradient_descent) as outlined by the authors:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/be5237cff646cf09c85ace0b11217c67.png)'
  prefs: []
  type: TYPE_IMG
- en: Update Rule (Gradient Descent w/ Momentum)
  prefs: []
  type: TYPE_NORMAL
- en: where *η* is our [learning rate](https://en.wikipedia.org/wiki/Learning_rate)
    and *α(t)* is our momentum term as a function of time. The learning rate controls
    the step size at each iteration and the momentum term allows the optimization
    algorithm to gain inertia in the smooth direction of the search space, while not
    being bogged down by the noisy parts of the gradient. We will set *η=200* for
    our example and will fix *α(t)=0.5* if *t < 250* and *α(t)=0.8* otherwise. We
    have all the components necessary above to compute to the update rule, thus we
    can now run our optimization over a set number of iterations *T* (we will set
    *T=1000*).
  prefs: []
  type: TYPE_NORMAL
- en: Before we set up for iteration scheme, let’s first introduce the enhancement
    the authors refer to as “early exaggeration”. This term is a constant that scales
    the original matrix of affinities `p_ij`. What this does is it places more emphasis
    on modeling the very similar points (high values in `p_ij` from the original space)
    in the new space early on and thus forming “clusters” of highly similar points.
    The early exaggeration is placed on at the beginning of the iteration scheme (*T<250*)
    and then turned off otherwise. Early exaggeration will be set to 4 in our case.
    We will see this in action in our visual below following implementation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, putting all of the pieces together for the algorithm, we have the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Calling `solution, Y = tSNE(X)` we obtain the following output :'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/548265c99338dbfcfa90a14fc1767c8b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'where `solution` is the final 2-D mapping and `Y` is our mapped 2-D values
    at each step of the iteration. Plotting the evolution of `Y` where `Y[-1]` is
    our final 2-D mapping, we obtain (note how the algorithm behaves with early exaggeration
    on and off):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/32a2288b18c398e80e34977388109036.png)'
  prefs: []
  type: TYPE_IMG
- en: Evolution of 2-D Mapping in t-SNE Algorithm
  prefs: []
  type: TYPE_NORMAL
- en: I recommend playing around with different values of the parameters (i.e., perplexity,
    learning rate, early exaggeration, etc.) to see how the solution differs (See
    the [original paper](https://lvdmaaten.github.io/publications/papers/JMLR_2008.pdf)
    and the [scikit-learn documentation](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html)
    for guides on using these parameters).
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: And there you have it, we have coded t-SNE from scratch! I hope you have found
    this exercise to be illuminating into the inner workings of t-SNE and, at the
    very minimum, satisfying. Note that this implementation is not intended to be
    optimized for speed, but rather for understanding. Additions to the t-SNE algorithm
    have been implemented to improve computational speed and performance, such as
    [variants of the Barnes-Hut algorithm](https://lvdmaaten.github.io/publications/papers/JMLR_2014.pdf)
    (tree-based approaches), using PCA as the initialization of the embedding, or
    using additional gradient descent extensions such as adaptive learning rates.
    The implementation in [scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html)
    makes use of many of these enhancements.
  prefs: []
  type: TYPE_NORMAL
- en: As always, I hope you have enjoyed reading this as much as I enjoyed writing
    it.
  prefs: []
  type: TYPE_NORMAL
- en: Resources
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] van der Maaten, L.J.P.; Hinton, G.E. Visualizing High-Dimensional Data
    Using t-SNE. Journal of Machine Learning Research 9:2579–2605, 2008.'
  prefs: []
  type: TYPE_NORMAL
- en: '[2] LeCun *et al.* (1999): The MNIST Dataset Of Handwritten Digits (Images)
    License: CC BY-SA 3.0'
  prefs: []
  type: TYPE_NORMAL
- en: '*Access all the code via this GitHub Repo:* [https://github.com/jakepenzak/Blog-Posts](https://github.com/jakepenzak/Blog-Posts)'
  prefs: []
  type: TYPE_NORMAL
- en: '*I appreciate you reading my post! My posts on Medium seek to explore real-world
    and theoretical applications utilizing* ***econometric*** *and* ***statistical/machine
    learning*** *techniques. Additionally, I seek to provide posts on the theoretical
    underpinnings of various methodologies via theory and simulations. Most importantly,
    I write to learn and help others learn! I hope to make complex topics slightly
    more accessible to all. If you enjoyed this post, please consider* [***following
    me on Medium***](https://medium.com/@jakepenzak)*!*'
  prefs: []
  type: TYPE_NORMAL
