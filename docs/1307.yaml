- en: t-SNE from Scratch (ft. NumPy)
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从头实现 t-SNE（配合 NumPy）
- en: 原文：[https://towardsdatascience.com/t-sne-from-scratch-ft-numpy-172ee2a61df7?source=collection_archive---------2-----------------------#2023-04-14](https://towardsdatascience.com/t-sne-from-scratch-ft-numpy-172ee2a61df7?source=collection_archive---------2-----------------------#2023-04-14)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/t-sne-from-scratch-ft-numpy-172ee2a61df7?source=collection_archive---------2-----------------------#2023-04-14](https://towardsdatascience.com/t-sne-from-scratch-ft-numpy-172ee2a61df7?source=collection_archive---------2-----------------------#2023-04-14)
- en: '![](../Images/4d9ef097294cb21259a550a26efce328.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4d9ef097294cb21259a550a26efce328.png)'
- en: Cover Image by Author
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 封面图片由作者提供
- en: Acquire a deep understanding of the inner workings of t-SNE via implementation
    from scratch in python
  id: totrans-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 通过从头实现 t-SNE 并使用 Python，深入理解其内部工作原理
- en: '[](https://medium.com/@jakepenzak?source=post_page-----172ee2a61df7--------------------------------)[![Jacob
    Pieniazek](../Images/2d9c6295d39fcaaec4e62f11c359cb29.png)](https://medium.com/@jakepenzak?source=post_page-----172ee2a61df7--------------------------------)[](https://towardsdatascience.com/?source=post_page-----172ee2a61df7--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----172ee2a61df7--------------------------------)
    [Jacob Pieniazek](https://medium.com/@jakepenzak?source=post_page-----172ee2a61df7--------------------------------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@jakepenzak?source=post_page-----172ee2a61df7--------------------------------)[![Jacob
    Pieniazek](../Images/2d9c6295d39fcaaec4e62f11c359cb29.png)](https://medium.com/@jakepenzak?source=post_page-----172ee2a61df7--------------------------------)[](https://towardsdatascience.com/?source=post_page-----172ee2a61df7--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----172ee2a61df7--------------------------------)
    [Jacob Pieniazek](https://medium.com/@jakepenzak?source=post_page-----172ee2a61df7--------------------------------)'
- en: ·
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F6f0948d99b1c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ft-sne-from-scratch-ft-numpy-172ee2a61df7&user=Jacob+Pieniazek&userId=6f0948d99b1c&source=post_page-6f0948d99b1c----172ee2a61df7---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----172ee2a61df7--------------------------------)
    ·17 min read·Apr 14, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F172ee2a61df7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ft-sne-from-scratch-ft-numpy-172ee2a61df7&user=Jacob+Pieniazek&userId=6f0948d99b1c&source=-----172ee2a61df7---------------------clap_footer-----------)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F6f0948d99b1c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ft-sne-from-scratch-ft-numpy-172ee2a61df7&user=Jacob+Pieniazek&userId=6f0948d99b1c&source=post_page-6f0948d99b1c----172ee2a61df7---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----172ee2a61df7--------------------------------)
    ·17分钟阅读·2023年4月14日 [](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F172ee2a61df7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ft-sne-from-scratch-ft-numpy-172ee2a61df7&user=Jacob+Pieniazek&userId=6f0948d99b1c&source=-----172ee2a61df7---------------------clap_footer-----------)'
- en: --
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F172ee2a61df7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ft-sne-from-scratch-ft-numpy-172ee2a61df7&source=-----172ee2a61df7---------------------bookmark_footer-----------)'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F172ee2a61df7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ft-sne-from-scratch-ft-numpy-172ee2a61df7&source=-----172ee2a61df7---------------------bookmark_footer-----------)'
- en: I have found that one of the best ways to *truly* understanding any statistical
    algorithm or methodology is to manually implement it yourself. On the flip side,
    coding these algorithms can sometimes be time consuming and a real pain, and when
    somebody else has already done it, why would I want to spend my time doing it
    — seems inefficient, no? Both are fair points, and I am not here to make an argument
    for one over the other.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 我发现，*真正*理解任何统计算法或方法的最佳方式之一就是亲自实现它。另一方面，编写这些算法有时会很耗时且非常麻烦，如果别人已经完成了，为什么我还要花时间去做呢——这似乎不太高效，不是吗？这两个观点都很公平，我并不是要为其中一个观点辩护。
- en: This article is designed for readers who are interested in understanding t-SNE
    via translation of the mathematics in the [original paper](https://jmlr.org/papers/volume9/vandermaaten08a/vandermaaten08a.pdf)
    — by Laurens van der Maaten & Geoffrey Hinton — into python code implementation.[1]
    I find these sort of exercises to be quite illuminating into the inner workings
    of statistical algorithms/models and really test your underlying understanding
    and assumptions regarding these algorithms/models. You are almost guaranteed to
    walk away with a better understanding then you had before. At the very minimum,
    successful implementation is always very satisfying!
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: 'This article will be accessible to individuals with any level of exposure of
    t-SNE. However, note a few things this post definitely is **not**:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: A *strictly* conceptual introduction and exploration of t-SNE, as there are
    plenty of other great resources out there that do this; nevertheless, I will be
    doing my best to connect the mathematical equations to their intuitive/conceptual
    counterparts at each stage of implementation.
  id: totrans-13
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A *comprehensive* discussion into the applications & pros/cons of t-SNE, as
    well as direct comparisons of t-SNE to other dimensionality reduction techniques.
    I will, however, briefly touch on these topics throughout this article, but will
    by no means cover this in-depth.
  id: totrans-14
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Without further ado, let’s start with a *brief* introduction into t-SNE.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: A Brief Introduction to t-SNE
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*t-distributed stochastic neighbor embedding (*t-SNE) is a dimensionality reduction
    tool that is primarily used in datasets with a large dimensional feature space
    and enables one to visualize the data down, or project it, into a lower dimensional
    space (usually 2-D). It is especially useful for visualizing non-linearly separable
    data wherein linear methods such as [Principal Component Analysis](https://en.m.wikipedia.org/wiki/Principal_component_analysis)
    (PCA) would fail. Generalizing linear frameworks of dimensionality reduction (such
    as PCA) into non-linear approaches (such as t-SNE) is also known as [Manifold
    Learning](https://en.m.wikipedia.org/wiki/Nonlinear_dimensionality_reduction).
    These methods can be extremely useful for visualizing and understanding the underlying
    structure of a high dimensional, non-linear data set, and can be great for disentangling
    and grouping together observations that are similar in the high-dimensional space.
    For more information on t-SNE and other manifold learning techniques, the [scikit-learn
    documentation](https://scikit-learn.org/stable/modules/manifold.html) is a great
    resource. Additionally, to read about some cool areas t-SNE has seen applications,
    the [Wikipedia page](https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding#cite_note-3)
    highlights some of these areas with references to the work.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s start with breaking down the name *t-distributed stochastic neighbor
    embedding* into its components*.* t-SNE is an extension on stochastic neighbor
    embedding (SNE) presented 6 years earlier in this [paper](https://cs.nyu.edu/~roweis/papers/sne_final.pdf)
    by Geoffrey Hinton & Sam Roweis. So let’s start there. The *stochastic* part of
    the name comes from the fact that the objective function is not convex and thus
    different results can arise from different initializations. The *neighbor embedding*
    highlights the nature of the algorithm — optimally mapping the points in the original
    high-dimensional space into the corresponding low-dimensional space while best
    preserving the “neighborhood” structure of the points. SNE is comprised of the
    following (simplified) steps:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: '*Obtain the Similarity Matrix between Points in the Original Space:* Compute
    the conditional probabilities for each datapoint *j* relative to each datapoint
    *i*. These conditional probabilities are calculated in the original high-dimensional
    space using a Gaussian centered at *i* and take on the following interpretation:
    the probability that *i* would pick *j* as its neighbor in the original space.This
    creates a matrix that represents similarities between the points.'
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Initialization:* Choose random starting points in the lower-dimensional space
    (say, 2-D) for each datapoint in the original space and compute new conditional
    probabilities similarly as above in this new space.'
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Mapping:* Iteratively improve upon the points in the lower-dimensional space
    until the [Kullback-Leibler](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence)
    divergences between all the conditional probabilities is minimized. Essentially
    we are minimizing the differences in the probabilities between the similarity
    matrices of the two spaces so as to ensure the similarities are best preserved
    in the mapping of the original high-dimensional dataset to the low-dimensional
    dataset.'
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 't-SNE improves upon SNE in two primary ways:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: It minimizes the [Kullback-Leibler](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence)
    divergences between the *joint probabilities* rather than the conditional probabilities.
    The authors refer to this as “symmetric SNE” b/c their approach ensures that the
    joint probabilities *p_ij* = *p_ji.* **This results in a much better behaved cost
    function that is easier to optimize.**
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'It computes the similarities between points using a [Student-t distribution](https://en.wikipedia.org/wiki/Student%27s_t-distribution)
    w/ one degree of freedom (also a [Cauchy Distribution](https://en.wikipedia.org/wiki/Cauchy_distribution))
    rather than a Gaussian in the *low-dimensional* space (step 2 above). Here we
    can see where the “t” in t-SNE is coming from. **This improvement helps to alleviate
    the “crowding problem” highlighted by the authors and to further improve the optimization
    problem.** This “crowding problem” can be envisioned as such: Imagine we have
    a 10-D space, the amount of space available in 2-D will not be sufficient to accurately
    capture those moderately dissimilar points compared to the amount of space for
    nearby points relative to the amount of space available in a 10-D space. More
    simply, just envision taking a 3-D space and projecting it down to 2-D, the 3-D
    space will have much more overall space to model the similarities relative to
    the projection down into 2-D. The Student-t distribution helps alleviate this
    problem by having heavier tails than the normal distribution. See the [original
    paper](https://jmlr.org/papers/volume9/vandermaaten08a/vandermaaten08a.pdf) for
    a much more in-depth treatment of this problem.'
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它使用具有一个自由度的[Student-t 分布](https://en.wikipedia.org/wiki/Student%27s_t-distribution)（也就是[柯西分布](https://en.wikipedia.org/wiki/Cauchy_distribution)）来计算点之间的相似性，而不是在*低维*空间中使用高斯分布（上面的第
    2 步）。在这里我们可以看到 t-SNE 中的“t”来自哪里。**这一改进有助于缓解作者所强调的“拥挤问题”，并进一步改善优化问题。** “拥挤问题”可以这样理解：假设我们有一个
    10 维空间，那么在 2 维空间中可用的空间将不足以准确捕捉那些适度不相似的点，而与 10 维空间中相邻点所占用的空间相比，2 维空间的空间远远不够。更简单地说，只需设想将
    3 维空间投影到 2 维空间，3 维空间将有更多的整体空间来建模相似性，相对于投影到 2 维的空间。Student-t 分布通过具有比正态分布更重的尾部来帮助缓解这个问题。有关这个问题的更深入的讨论，请参见[原始论文](https://jmlr.org/papers/volume9/vandermaaten08a/vandermaaten08a.pdf)。
- en: 'If this did not all track immediately, that is okay! I am hoping when we implement
    this in code, the pieces will all fall in to place. The main takeaway is this:
    **t-SNE models similarities between datapoints in the high-dimensional space using
    joint probabilities of “datapoints choosing others as its neighbor”, and then
    tries to find the best mapping of these points down into the low-dimensional space,
    while best preserving the original high-dimensional similarities.**'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 如果这些内容没有立即跟上，那也没关系！我希望当我们在代码中实现这些时，所有部分都会迎刃而解。主要的要点是：**t-SNE 在高维空间中通过“数据点选择其他点作为邻居”的联合概率来建模数据点之间的相似性，然后尝试找到这些点映射到低维空间的最佳方式，同时尽可能保留原始高维相似性。**
- en: Implementation from Scratch
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从头开始的实现
- en: 'Let’s now move on to understanding t-SNE via implementing the original version
    of the algorithm as presented in the [paper](https://lvdmaaten.github.io/publications/papers/JMLR_2008.pdf)
    by Laurens van der Maaten & Geoffrey Hinton. We will first start with implementing
    algorithm 1 below step-by-step, which will cover 95% of the main algorithm. There
    are two additional enhancements the authors note: 1) Early Exaggeration & 2) Adaptive
    Learning Rates. We will only discuss adding in the early exaggeration as that
    is most conducive in aiding the interpretation of the actual algorithms inner
    workings, as the adaptive learning rate is focused on improving the rates of convergence.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们继续了解 t-SNE，方法是实现 Laurens van der Maaten 和 Geoffrey Hinton 在[论文](https://lvdmaaten.github.io/publications/papers/JMLR_2008.pdf)中提出的算法原版。我们将首先逐步实现下面的算法
    1，这将涵盖主算法的 95%。作者还提到了两个额外的改进：1) 早期夸张和 2) 自适应学习率。我们将仅讨论添加早期夸张，因为这有助于解释实际算法的内部工作原理，而自适应学习率则侧重于提高收敛速度。
- en: '![](../Images/2e3f7799e417a127b3257121722fa4d7.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2e3f7799e417a127b3257121722fa4d7.png)'
- en: Algorithm 1 (from [paper](https://lvdmaaten.github.io/publications/papers/JMLR_2008.pdf))
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 1（见[论文](https://lvdmaaten.github.io/publications/papers/JMLR_2008.pdf)）
- en: '**1\. Inputs & Outputs**'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '**1\. 输入和输出**'
- en: Following the original [paper](https://lvdmaaten.github.io/publications/papers/JMLR_2008.pdf),
    we will be using the publicly available MNIST [dataset](https://www.openml.org/search?type=data&status=active&id=554)
    from OpenML with images of handwritten digits from 0–9.[2] We will also randomly
    sample 1000 images from the dataset & reduce the dimensionality of the dataset
    using Principal Component Analysis (PCA) and keep 30 components. These are both
    to improve computational time of the algorithm, as the code here is not optimized
    for speed, but rather for interpretability & learning.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'This will be our *X* dataset with each *row* being an image and each *column*
    being a feature, or principal component in this case (i.e. linear combinations
    of the original pixels):'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6dd2af5bc94d63503a5bf3c96b53c64b.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
- en: Sample of 1000 from MNIST Dataset with first 30 principal components
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: We also will need to specify the cost function parameters — perplexity — and
    the optimization parameters — iterations, learning rate, & momentum. We will hold
    off on these for now and address them as they appear at each stage.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: 'In terms of output, recall that we seek a the low-dimensional mapping of the
    original dataset *X.* We will be mapping the original space into a 2 dimensional
    space throughout this example. Thus, our new output will be the 1000 images now
    represented in a 2 dimensional space rather than the original 30 dimensional space:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/dc89f27e6f8c53c7539ca276e6d0ff4b.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
- en: Desired Output in 2-D Space
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: '**2\. Compute Affinities/Similarities of *X* in the Original Space**'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have our inputs, the first step is to compute the pairwise similarities
    in the original high dimensional space. That is, for each image *i* we compute
    the probability that *i* would pick image *j* as its neighbor in the original
    space for each *j*. These probabilities are calculated via a normal distribution
    centered around each point and then are normalized to sum up to 1\. Mathematically,
    we have:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e4da47d9af18e17d9a3356bd293c0585.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
- en: Eq. (1) — High Dimensional Affinity
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that, in our case with *n = 1000*, these computations will result in a
    *1000* x *1000* matrix of similarity scores. Note we set *p = 0* whenever *i =
    j* b/c we are modeling pairwise similarities. However, you may notice that we
    have not mentioned how σ is determined. This value is determined for each observation
    *i* via a grid searchbased on the user-specified desired [perplexity](https://en.wikipedia.org/wiki/Perplexity)
    of the distributions. We will talk about this immediately below, but let’s first
    look at how we would code eq. (1) above:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Now before we look at the results of this code, let’s discuss how we determine
    the values of σ via the grid_search() function. Given a specified value of perplexity
    (which in this context can be loosely thought about as the number of nearest neighbors
    for each point), we do a grid search over a range of values of σ such that the
    following equation is as close to equality as possible for each *i*:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们查看这段代码的结果之前，让我们讨论一下如何通过 grid_search() 函数确定 σ 的值。给定一个指定的 perplexity 值（在这种情况下可以大致理解为每个点的最近邻数量），我们对一系列
    σ 值进行网格搜索，以便使以下方程对于每个 *i* 尽可能接近等式：
- en: '![](../Images/5f487e931a57b04a170b15731b946439.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5f487e931a57b04a170b15731b946439.png)'
- en: Perplexity
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: Perplexity
- en: where H(P) is the Shannon [entropy](https://en.wikipedia.org/wiki/Entropy_(information_theory))
    of P.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 H(P) 是 P 的香农 [熵](https://en.wikipedia.org/wiki/Entropy_(information_theory))。
- en: '![](../Images/59b1bbe330ddaf7cdc88d57200ed76e9.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/59b1bbe330ddaf7cdc88d57200ed76e9.png)'
- en: Shannon Entropy of P
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: P 的香农熵
- en: 'In our case, we will set perplexity = 10 and set the search space to be defined
    by [0.01 * standard deviation of the norms for the difference between images *i*
    and *j*, 5 * standard deviation of the norms for the difference between images
    *i* and *j*] divided into 200 equal steps. Knowing this, we can define our grid_search()
    function as follows:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的案例中，我们将 perplexity 设置为 10，并将搜索空间定义为 [0.01 * 图像 *i* 和 *j* 之间差异的范数的标准差，5 *
    图像 *i* 和 *j* 之间差异的范数的标准差]，分成 200 个相等的步长。知道这一点后，我们可以按如下方式定义我们的 grid_search() 函数：
- en: '[PRE2]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Given these functions, we can compute the affinity matrix via `p_ij = get_original_pairwise_affinities(X)`
    where we obtain the following matrix:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这些函数，我们可以通过`p_ij = get_original_pairwise_affinities(X)`计算亲和矩阵，从而得到以下矩阵：
- en: '![](../Images/15c579861c711dad1aaecc25972b5c4e.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/15c579861c711dad1aaecc25972b5c4e.png)'
- en: Affinity Matrix of Conditional Probabilities in Original High Dimensional Space
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 原始高维空间中条件概率的亲和矩阵
- en: 'Note, the diagonal elements are set to ε ≈ 0 by construction (whenever *i =
    j*). Recall that a key extension of the t-SNE algorithm is to compute the joint
    probabilities rather than the conditional probabilities. This is computed simply
    as follow:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，主对角线元素按构造设置为 ε ≈ 0（每当 *i = j* 时）。请记住，t-SNE 算法的一个关键扩展是计算联合概率而不是条件概率。这可以简单地按如下方式计算：
- en: '![](../Images/1985cc2d53d64bd1f1913a4845acf957.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1985cc2d53d64bd1f1913a4845acf957.png)'
- en: Converting Conditional Probabilities to Joint Probabilities
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 将条件概率转换为联合概率
- en: 'Thus, we can define a new function:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们可以定义一个新函数：
- en: '[PRE3]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Feeding in `p_ij` from above, we have `p_ij_symmetric = get_symmetric_p_ij(p_ij)`
    where we obtain the following *symmetric* affinities matrix:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 将上面的`p_ij`代入，我们得到`p_ij_symmetric = get_symmetric_p_ij(p_ij)`，从而获得以下*symmetric*亲和矩阵：
- en: '![](../Images/06c5403b356c0c8d8e7d821d85fb8c82.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/06c5403b356c0c8d8e7d821d85fb8c82.png)'
- en: Symmetric Affinity Matrix of Joint Probabilities in Original High Dimensional
    Space
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 原始高维空间中联合概率的对称亲和矩阵
- en: Now we have completed the first main step in t-SNE! We computed the symmetric
    affinity matrix in the original high-dimensional space. Before we dive right into
    the optimization stage, we will discuss the main components of the optimization
    problem in the next two steps and then combine them into our for loop.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经完成了 t-SNE 中的第一个主要步骤！我们计算了原始高维空间中的对称亲和矩阵。在我们深入优化阶段之前，我们将在接下来的两个步骤中讨论优化问题的主要组件，然后将它们结合到我们的
    for 循环中。
- en: '**3\. Sample Initial Solution & Compute Low Dimensional Affinity Matrix**'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '**3\. 样本初始解决方案及计算低维亲和矩阵**'
- en: 'Now we want to sample a random initial solution in the lower dimensional space
    as follows:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们想在低维空间中随机抽样一个初始解决方案，如下所示：
- en: '[PRE4]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'where calling `y0 = initialization(X)` we obtain a random starting solution:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 其中调用 `y0 = initialization(X)` 我们得到一个随机的起始解决方案：
- en: '![](../Images/dcdd43442d43ea88a02a0b8f811a7100.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/dcdd43442d43ea88a02a0b8f811a7100.png)'
- en: Initial Random Solution in 2-D
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 2-D 初始随机解决方案
- en: 'Now, we want to compute the affinity matrix in this lower dimensional space.
    However, recall that we do this utilizing a student-t distribution w/ 1 degree
    of freedom:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们想在这个低维空间中计算亲和矩阵。然而，请记住，我们是利用具有 1 个自由度的学生-t 分布来完成的：
- en: '![](../Images/73872a630d696cbefd02c6945aafd94a.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/73872a630d696cbefd02c6945aafd94a.png)'
- en: Eq. (4) — Low Dimensional Affinity
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 方程 (4) — 低维亲和
- en: 'Again, we set *q = 0* whenever *i = j*. Note this equation differs from eq.
    (1) in that the denominator is over all *i* and thus symmetric by construction.
    Putting this into code, we obtain:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 同样地，我们设置 *q = 0* 当 *i = j*。注意这个方程与公式 (1) 的不同之处在于分母涉及所有 *i*，因此按照构造是对称的。将其转化为代码，我们得到：
- en: '[PRE5]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Here we are seeking a *1000* x *1000* affinity matrix but now in the lower
    dimensional space. Calling `q_ij = get_low_dimensional_affinities(y0)` we obtain:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 这里我们正在寻找一个 *1000* x *1000* 的亲和矩阵，但现在是在低维空间中。调用 `q_ij = get_low_dimensional_affinities(y0)`
    我们得到：
- en: '![](../Images/e51d9873343526bd4ece743d437dd250.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e51d9873343526bd4ece743d437dd250.png)'
- en: Symmetric Affinity Matrix of Joint Probabilities in New Low Dimensional Space
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 新低维空间中联合概率的对称亲和矩阵
- en: '**4\. Compute Gradient of the Cost Function**'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '**4. 计算成本函数的梯度**'
- en: 'Recall, our cost function is the Kullback-Leibler divergence between joint
    probability distributions in the high dimensional space and low dimensional space:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 回顾一下，我们的成本函数是高维空间和低维空间中联合概率分布的 Kullback-Leibler 散度：
- en: '![](../Images/4cb5562aab6cfcef0a783763d828dbad.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4cb5562aab6cfcef0a783763d828dbad.png)'
- en: Kullback-Leibler divergence between joint probability distributions
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 联合概率分布的 Kullback-Leibler 散度
- en: 'Intuitively, we want to minimize the difference in the affinity matrices `p_ij`
    and `q_ij` thereby best preserving the “neighborhood” structure of the original
    space. The optimization problem is solved using gradient descent, but first let’s
    look at computing the gradient for the cost function above. The authors derive
    (see appendix A of the [paper](https://lvdmaaten.github.io/publications/papers/JMLR_2008.pdf))
    the gradient of the cost function as follows:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 直观上，我们希望最小化亲和矩阵 `p_ij` 和 `q_ij` 之间的差异，从而最好地保留原始空间的“邻域”结构。使用梯度下降法来解决优化问题，但首先让我们看看如何计算上面成本函数的梯度。作者推导了成本函数的梯度（见
    [论文](https://lvdmaaten.github.io/publications/papers/JMLR_2008.pdf) 的附录 A）如下：
- en: '![](../Images/30cc398d3c3e508cf2cba4714373e815.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/30cc398d3c3e508cf2cba4714373e815.png)'
- en: Gradient of Cost Function (Eq. 5, but from appendix)
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 成本函数的梯度（公式 5，但来自附录）
- en: 'In python, we have:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Python 中，我们有：
- en: '[PRE6]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Feeding in the relevant arguments, we obtain the gradient at `y0` via `gradient
    = get_gradient(p_ij_symmetric,q_ij,y0)` with the corresponding output:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 输入相关参数，我们通过 `gradient = get_gradient(p_ij_symmetric,q_ij,y0)` 得到在 `y0` 处的梯度及相应输出：
- en: '![](../Images/c9256b70c406c44ed02b0e937ede1b64.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c9256b70c406c44ed02b0e937ede1b64.png)'
- en: Gradient of Cost Function at Initial Solution (y0)
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 初始解（y0）下成本函数的梯度
- en: Now, we have all the pieces in order to solve the optimization problem!
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们已经准备好解决优化问题的所有部分！
- en: '**5\. Iterate & Optimize the Low-Dimensional Mapping**'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '**5. 迭代与优化低维映射**'
- en: 'In order to update our low-dimensional mapping, we use [gradient descent with
    momentum](https://en.wikipedia.org/wiki/Gradient_descent) as outlined by the authors:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更新我们的低维映射，我们使用 [带动量的梯度下降](https://en.wikipedia.org/wiki/Gradient_descent)，正如作者所述：
- en: '![](../Images/be5237cff646cf09c85ace0b11217c67.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/be5237cff646cf09c85ace0b11217c67.png)'
- en: Update Rule (Gradient Descent w/ Momentum)
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 更新规则（带动量的梯度下降）
- en: where *η* is our [learning rate](https://en.wikipedia.org/wiki/Learning_rate)
    and *α(t)* is our momentum term as a function of time. The learning rate controls
    the step size at each iteration and the momentum term allows the optimization
    algorithm to gain inertia in the smooth direction of the search space, while not
    being bogged down by the noisy parts of the gradient. We will set *η=200* for
    our example and will fix *α(t)=0.5* if *t < 250* and *α(t)=0.8* otherwise. We
    have all the components necessary above to compute to the update rule, thus we
    can now run our optimization over a set number of iterations *T* (we will set
    *T=1000*).
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *η* 是我们的 [学习率](https://en.wikipedia.org/wiki/Learning_rate)，*α(t)* 是我们随时间变化的动量项。学习率控制每次迭代的步长，而动量项使优化算法在搜索空间的平滑方向上获得惯性，同时不被梯度的噪声部分所困扰。我们将例子中的
    *η=200*，并且如果 *t < 250* 时将 *α(t)=0.5*，否则将 *α(t)=0.8*。以上是计算更新规则所需的所有组件，因此我们可以在设定的迭代次数
    *T* 上进行优化（我们将 *T=1000*）。
- en: Before we set up for iteration scheme, let’s first introduce the enhancement
    the authors refer to as “early exaggeration”. This term is a constant that scales
    the original matrix of affinities `p_ij`. What this does is it places more emphasis
    on modeling the very similar points (high values in `p_ij` from the original space)
    in the new space early on and thus forming “clusters” of highly similar points.
    The early exaggeration is placed on at the beginning of the iteration scheme (*T<250*)
    and then turned off otherwise. Early exaggeration will be set to 4 in our case.
    We will see this in action in our visual below following implementation.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在设置迭代方案之前，首先介绍作者所称的“早期夸张”增强。这一术语是一个常数，用于缩放原始的亲和度矩阵 `p_ij`。这将更多地强调在新空间中早期建模非常相似的点（原始空间中
    `p_ij` 的高值），从而形成高度相似点的“簇”。早期夸张在迭代方案的开始阶段 (*T<250*) 中开启，然后关闭。在我们的情况下，早期夸张将设置为 4。我们将在下面的可视化中看到它的实际效果。
- en: 'Now, putting all of the pieces together for the algorithm, we have the following:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，将所有算法部分结合起来，我们得到了如下内容：
- en: '[PRE7]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Calling `solution, Y = tSNE(X)` we obtain the following output :'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 调用 `solution, Y = tSNE(X)` 我们得到以下输出：
- en: '![](../Images/548265c99338dbfcfa90a14fc1767c8b.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/548265c99338dbfcfa90a14fc1767c8b.png)'
- en: 'where `solution` is the final 2-D mapping and `Y` is our mapped 2-D values
    at each step of the iteration. Plotting the evolution of `Y` where `Y[-1]` is
    our final 2-D mapping, we obtain (note how the algorithm behaves with early exaggeration
    on and off):'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 `solution` 是最终的 2-D 映射，`Y` 是我们在每次迭代步骤中的 2-D 映射值。绘制 `Y` 的演变，其中 `Y[-1]` 是我们的最终
    2-D 映射，我们得到（注意算法在早期夸张开启和关闭时的表现）：
- en: '![](../Images/32a2288b18c398e80e34977388109036.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/32a2288b18c398e80e34977388109036.png)'
- en: Evolution of 2-D Mapping in t-SNE Algorithm
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: t-SNE 算法中的 2-D 映射演变
- en: I recommend playing around with different values of the parameters (i.e., perplexity,
    learning rate, early exaggeration, etc.) to see how the solution differs (See
    the [original paper](https://lvdmaaten.github.io/publications/papers/JMLR_2008.pdf)
    and the [scikit-learn documentation](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html)
    for guides on using these parameters).
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 我建议尝试不同的参数值（如困惑度、学习率、早期夸张等），看看解决方案如何变化（参见[原始论文](https://lvdmaaten.github.io/publications/papers/JMLR_2008.pdf)和[scikit-learn
    文档](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html)获取使用这些参数的指南）。
- en: Conclusion
  id: totrans-107
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结论
- en: And there you have it, we have coded t-SNE from scratch! I hope you have found
    this exercise to be illuminating into the inner workings of t-SNE and, at the
    very minimum, satisfying. Note that this implementation is not intended to be
    optimized for speed, but rather for understanding. Additions to the t-SNE algorithm
    have been implemented to improve computational speed and performance, such as
    [variants of the Barnes-Hut algorithm](https://lvdmaaten.github.io/publications/papers/JMLR_2014.pdf)
    (tree-based approaches), using PCA as the initialization of the embedding, or
    using additional gradient descent extensions such as adaptive learning rates.
    The implementation in [scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html)
    makes use of many of these enhancements.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样，我们从零开始实现了 t-SNE！我希望你发现这个练习对 t-SNE 的内部工作有启发，至少是令人满意的。请注意，这个实现并不旨在优化速度，而是为了理解。t-SNE
    算法的改进包括提高计算速度和性能，例如[Barnes-Hut 算法的变体](https://lvdmaaten.github.io/publications/papers/JMLR_2014.pdf)（基于树的方法）、使用
    PCA 作为嵌入的初始化，或使用如自适应学习率等额外的梯度下降扩展。[scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html)中的实现采用了许多这些增强功能。
- en: As always, I hope you have enjoyed reading this as much as I enjoyed writing
    it.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 一如既往，我希望你阅读这篇文章的乐趣与我写作时的乐趣一样。
- en: Resources
  id: totrans-110
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 资源
- en: '[1] van der Maaten, L.J.P.; Hinton, G.E. Visualizing High-Dimensional Data
    Using t-SNE. Journal of Machine Learning Research 9:2579–2605, 2008.'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] van der Maaten, L.J.P.; Hinton, G.E. 使用 t-SNE 可视化高维数据。《机器学习研究期刊》9:2579–2605,
    2008。'
- en: '[2] LeCun *et al.* (1999): The MNIST Dataset Of Handwritten Digits (Images)
    License: CC BY-SA 3.0'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] LeCun *et al.* (1999)：手写数字 (图像) 的 MNIST 数据集 许可证：CC BY-SA 3.0'
- en: '*Access all the code via this GitHub Repo:* [https://github.com/jakepenzak/Blog-Posts](https://github.com/jakepenzak/Blog-Posts)'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '*通过这个 GitHub 仓库访问所有代码：* [https://github.com/jakepenzak/Blog-Posts](https://github.com/jakepenzak/Blog-Posts)'
- en: '*I appreciate you reading my post! My posts on Medium seek to explore real-world
    and theoretical applications utilizing* ***econometric*** *and* ***statistical/machine
    learning*** *techniques. Additionally, I seek to provide posts on the theoretical
    underpinnings of various methodologies via theory and simulations. Most importantly,
    I write to learn and help others learn! I hope to make complex topics slightly
    more accessible to all. If you enjoyed this post, please consider* [***following
    me on Medium***](https://medium.com/@jakepenzak)*!*'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '*感谢你阅读我的帖子！我在 Medium 上的帖子旨在探讨利用* ***计量经济学*** *和* ***统计学/机器学习*** *技术的实际和理论应用。此外，我还希望通过理论和模拟提供有关各种方法论的理论基础的帖子。最重要的是，我写作是为了学习并帮助他人学习！我希望使复杂的主题对大家稍微更加易懂。如果你喜欢这篇帖子，请考虑*
    [***在 Medium 上关注我***](https://medium.com/@jakepenzak)*！*'
