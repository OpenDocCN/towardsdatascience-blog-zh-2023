- en: Keeping Robots from Going Off the Ethical Rails
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ÂéüÊñáÔºö[https://towardsdatascience.com/keeping-robots-from-going-off-the-ethical-rails-7dc089b53917?source=collection_archive---------16-----------------------#2023-04-13](https://towardsdatascience.com/keeping-robots-from-going-off-the-ethical-rails-7dc089b53917?source=collection_archive---------16-----------------------#2023-04-13)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/fd2fba06a7d547e47de2ec841a83e141.png)'
  prefs: []
  type: TYPE_IMG
- en: Image created by author using Dall-E 2
  prefs: []
  type: TYPE_NORMAL
- en: The key to building transparent AI software systems
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://statistician-in-stilettos.medium.com/?source=post_page-----7dc089b53917--------------------------------)[![Claire
    Longo](../Images/5a04940feeba1412688b4f38ec1fe974.png)](https://statistician-in-stilettos.medium.com/?source=post_page-----7dc089b53917--------------------------------)[](https://towardsdatascience.com/?source=post_page-----7dc089b53917--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----7dc089b53917--------------------------------)
    [Claire Longo](https://statistician-in-stilettos.medium.com/?source=post_page-----7dc089b53917--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ¬∑
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F1f6936fe85bb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fkeeping-robots-from-going-off-the-ethical-rails-7dc089b53917&user=Claire+Longo&userId=1f6936fe85bb&source=post_page-1f6936fe85bb----7dc089b53917---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----7dc089b53917--------------------------------)
    ¬∑8 min read¬∑Apr 13, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F7dc089b53917&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fkeeping-robots-from-going-off-the-ethical-rails-7dc089b53917&user=Claire+Longo&userId=1f6936fe85bb&source=-----7dc089b53917---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F7dc089b53917&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fkeeping-robots-from-going-off-the-ethical-rails-7dc089b53917&source=-----7dc089b53917---------------------bookmark_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: An AI algorithm is a software system that has the ability to automate or perform
    tasks that typically require human intelligence. These systems often include more
    than just a trained model. They can also include explicit algorithm functionality,
    such as business rules, that integrate the model‚Äôs output into the larger AI system
    to complete an end-to-end task.
  prefs: []
  type: TYPE_NORMAL
- en: To properly implement ethical AI systems, the software used to deploy the models
    must include the ability to measure and mitigate the live algorithm behavior from
    end-to-end. By intentionally building in methods to audit AI, we can ensure a
    good robot doesn‚Äôt go off the rails ‚Äî and if it does, we‚Äôll have the tools to
    course correct it.
  prefs: []
  type: TYPE_NORMAL
- en: Ethical AI Software Infrastructure
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So what does it take to build ethics into an AI‚Äôs software infrastructure? How
    can we approach preemptively designing these systems to ensure we will have the
    ability to audit AI models for bias?
  prefs: []
  type: TYPE_NORMAL
- en: A truly auditable AI system should have enough transparency that users and creators
    can answer the questions ‚Äú*what data went into the model, what predictions came
    out, and what adjustments were made to it down the road before the output was
    used?‚Äù* If bias or quality issues are detected, the levers built into an ethical
    AI system can be used to mitigate any bias or correct quality the issues that
    arise.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/fcab682676a67de763bf36884ed18a98.png)'
  prefs: []
  type: TYPE_IMG
- en: An auditable AI system design (diagram by author)
  prefs: []
  type: TYPE_NORMAL
- en: This proposed system design is a generalized infrastructure that could be adapted
    for many business use-cases relying on live AI.
  prefs: []
  type: TYPE_NORMAL
- en: 'This design provides core functionality that ensures the model operates in
    a responsible, ethical, and unbiased manner. It includes:'
  prefs: []
  type: TYPE_NORMAL
- en: A **data collection system** with coverage for all system-generated data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A **metric and monitoring system** to track model performance and bias in live
    models to provide both data and model ML Observability..
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Multiple levers to pull to mitigate any potential bias:** a targeted model
    retraining loop for updating the model with better data compelements a rules engine
    for programming in explicit logic such as model overrides or bias mitigation and
    a human-in-the-loop quality check system.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data Collection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The data required to completely audit an AI system is extensive. It includes:'
  prefs: []
  type: TYPE_NORMAL
- en: Features (the model input)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Predictions (the model output)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SHAP or some representation of feature importance to provide pointwise prediction
    explainability. This allows us to trace why a specific prediction was made.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Any business logic or rules applied to augment the prediction
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Any human decisions applied to augment the prediction
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Isolated demographic data that can be linked to predictions for monitoring bias
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: üí°Notice the protected demographic data is kept isolated from the system in this
    design. This data is only used to measure model performance to detect bias. It
    should never be intermingled with model inputs as that could cause bias to be
    encoded into the model. Some core attributes to consider appear below.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/715d1a6a48a16ba71fa4c64b4fb96f36.png)'
  prefs: []
  type: TYPE_IMG
- en: Diagram by author
  prefs: []
  type: TYPE_NORMAL
- en: Bias Measurement and Monitoring
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Once that data is collected, metrics and visualizations can be used to quantify
    and monitor for bias trends in the live system. Standard fairness and bias metrics
    to consider are
  prefs: []
  type: TYPE_NORMAL
- en: 'Recall Parity: measures how ‚Äúsensitive‚Äù the model is for one group compared
    to another, or the model‚Äôs ability to predict true positives correctly.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'False Positive Rate Parity: measures whether a model incorrectly predicts the
    positive class for the sensitive group as compared to the base group.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Disparate Impact](https://arize.com/blog-course/fairness-bias-metrics/#what-are-the-prevailing-model-fairness-metrics):
    a quantitative measure of the adverse treatment of protected classes'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A model may appear to perform well on average, but digging deeper it‚Äôs possible
    to look beyond the average model performance and isolate performance across demographic
    groups. Tacking the accuracy per group gives visibility into how fair the model
    is serving the population as a whole.
  prefs: []
  type: TYPE_NORMAL
- en: Bias Mitigation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The **rules engine**, also known as a rule-based system or an expert system,
    is a type of algorithm that uses a predefined set of rules to make decisions or
    solve problems. These rules, often represented as IF-THEN statements, capture
    the domain-specific knowledge and expertise in a structured and organized manner.
    In real life AI applications, the model predictions are often fed into a rules
    engine, where business decisions are made around how to use the prediction or
    how to augment it. If bias is detected, new rules can be encoded to override it.
  prefs: []
  type: TYPE_NORMAL
- en: Many AI systems include an automated pipeline for collecting new data and retraining
    the model on the new data to update it with the freshest information. This keeps
    the model healthy and performant. This same retraining loop can be used to remove
    bias from models. The retraining data can be collected in a targeted way to focus
    on providing more data or better examples for the model to learn from on areas
    where it is failing.
  prefs: []
  type: TYPE_NORMAL
- en: The **human in the loop** component provides the ability to quality check the
    AI‚Äôs output before it is used. This functionality also needs to be part of the
    software design so the human decision data can be collected, especially in use-cases
    that cannot or should not be fully automated. AI is often most effective as a
    human assistant, not a complete task automation tool. Building in infrastructure
    to support human interaction and decision making allows us humans to override
    bias or harmful patterns when they are detected.
  prefs: []
  type: TYPE_NORMAL
- en: Wait, What Is Bias in AI?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So how can an AI algorithm be biased, unfair, or unethical? The bias is often
    not hard coded. It‚Äôs not explicitly written by a software engineer or data scientist.
    Instead, the algorithms learn automatically from scanning large datasets. These
    AI models work to learn patterns from a large set of data, and encode those patterns
    mathematically. Those mathematical patterns are then saved as a ‚Äúmodel‚Äù and used
    to make inferences on new data. Under this paradigm, these models can learn harmful
    and unfair patterns simply if these patterns exist in the data provided to it.
    They will then perpetuate these harmful patterns by relying on them when making
    predictions. With data available widely on the internet, even old patterns and
    historical biases could get encoded into these models if we‚Äôre not careful.
  prefs: []
  type: TYPE_NORMAL
- en: So when thinking about patterns that get encoded into AI algorithms, there are
    two kinds of patterns to consider. Explicit and implicit patterns. Explicit patterns
    are rules hard coded in. These patterns are a purposeful choice by an organization,
    and typically represented as IF-THEN statements in code. Implicit patterns are
    learned by the model from data provided to it.
  prefs: []
  type: TYPE_NORMAL
- en: AI Development Lifecycle
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To add further context to this discussion, it is helpful to break down the
    **AI project lifecycle** to understand where bias or quality issues could be introduced.
    There are two core phases of the AI project lifecycle to focus on: *Research and
    Development (R&D)*, and *Operationalization*.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/74817d7554b18b8942c9869169d3015c.png)'
  prefs: []
  type: TYPE_IMG
- en: Diagram by author
  prefs: []
  type: TYPE_NORMAL
- en: R&D Phase
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/4f0990d971d52626538eb2b886107e9e.png)'
  prefs: []
  type: TYPE_IMG
- en: Image created by author using Dall-E 2
  prefs: []
  type: TYPE_NORMAL
- en: In the R&D phase, scientists or researchers work to create the model. They collect
    raw data, transform the data into meaningful model features, experiment and test
    various modeling approaches and parameters, and evaluate the model‚Äôs performance
    based on its ability to optimize a specific outcome of choice. During these steps,
    there are many considerations these model creators will take to prevent and test
    for bias in their model. They work to collect unbiased data and carefully measure
    model performance across protected demographics to ensure fairness. However, bias
    can still creep in. So we‚Äôll want to be able to monitor and track this model‚Äôs
    behavior once it‚Äôs live.
  prefs: []
  type: TYPE_NORMAL
- en: Operationalization Phase
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/264f54138a0dcdd82639ea77d4bb8134.png)'
  prefs: []
  type: TYPE_IMG
- en: Image created by author using Dall-E 2
  prefs: []
  type: TYPE_NORMAL
- en: Once the model is created and we have our good robot, the project will move
    into the next phase ‚Äî where said good robot is operationalized. That‚Äôs where the
    engineers come in. This phase of development focuses on transforming the chosen
    model into a practical and functional system that produces predictions from the
    model that an end-user can access and rely on and use live. This process involves
    constructing software systems that integrate the model‚Äôs core functionality and
    adhere to best practices for production code, ensuring the system is scalable
    and maintainable. This is whereethical AI by design comes in. This system can
    be designed to not only produce predictions from the model, but to also safeguard
    these algorithms if designed with ethical AI in mind.
  prefs: []
  type: TYPE_NORMAL
- en: An [auditable AI system](https://arize.com/blog-course/transparent-ethical-ai-software-systems/)
    collects data from the model itself as well as the patterns around explicit decisions
    regarding how the model‚Äôs output is used. This allows for a holistic view of the
    algorithm‚Äôs behavior and facilitates monitoring the system as a whole.
  prefs: []
  type: TYPE_NORMAL
- en: This algorithmic transparency should be at the forefront of the design of any
    AI system from the beginning of the design. This means that the ability to measure
    and mitigate bias needs to be baked into the software, and not an afterthought.
    If an algorithm with potential bias is deployed without these mechanisms in place,
    it will be difficult to detect and correct any harmful patterns.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It‚Äôs likely many AI systems will need to be audited at some point, whether it‚Äôs
    for bias, or simply interpretability or quality. It‚Äôs possible to get ahead of
    this by thoughtfully designing infrastructure that provides enough visibility
    into the data, and opportunity for algorithm improvements. Let‚Äôs keep those robots
    from going off the rails.
  prefs: []
  type: TYPE_NORMAL
- en: 'For deeper reading on this subject, here are a few of my favorite books and
    resources on ethical AI:'
  prefs: []
  type: TYPE_NORMAL
- en: '[A Summary and Review of the Ethical Algorithm](https://hackernoon.com/a-summary-and-review-of-the-ethical-algorithm)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Invisible Women: Data Bias In a World Designed for Men](https://carolinecriadoperez.com/book/invisible-women/)
    by Caroline Criado Perez'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Weapons of Math Destruction](https://www.penguinrandomhouse.com/books/241363/weapons-of-math-destruction-by-cathy-oneil/)
    by Cathy O‚ÄôNeil'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Ethical Machines: Your Concise Guide To Totally Unbiased, Transparent and
    Respectful AI](https://www.reidblackman.com/ethical-machines/) by Reid Blackman'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
