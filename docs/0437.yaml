- en: 'Emojis Aid Social Media Sentiment Analysis: Stop Cleaning Them Out!'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ÂéüÊñáÔºö[https://towardsdatascience.com/emojis-aid-social-media-sentiment-analysis-stop-cleaning-them-out-bb32a1e5fc8e?source=collection_archive---------5-----------------------#2023-01-31](https://towardsdatascience.com/emojis-aid-social-media-sentiment-analysis-stop-cleaning-them-out-bb32a1e5fc8e?source=collection_archive---------5-----------------------#2023-01-31)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Leverage emojis in social media sentiment analysis to improve accuracy.
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@bc3088?source=post_page-----bb32a1e5fc8e--------------------------------)[![Bale
    Chen](../Images/de6276419f1f93de346387a5ea2cc5b8.png)](https://medium.com/@bc3088?source=post_page-----bb32a1e5fc8e--------------------------------)[](https://towardsdatascience.com/?source=post_page-----bb32a1e5fc8e--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----bb32a1e5fc8e--------------------------------)
    [Bale Chen](https://medium.com/@bc3088?source=post_page-----bb32a1e5fc8e--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ¬∑
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F8695cd8317da&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Femojis-aid-social-media-sentiment-analysis-stop-cleaning-them-out-bb32a1e5fc8e&user=Bale+Chen&userId=8695cd8317da&source=post_page-8695cd8317da----bb32a1e5fc8e---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----bb32a1e5fc8e--------------------------------)
    ¬∑14 min read¬∑Jan 31, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fbb32a1e5fc8e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Femojis-aid-social-media-sentiment-analysis-stop-cleaning-them-out-bb32a1e5fc8e&user=Bale+Chen&userId=8695cd8317da&source=-----bb32a1e5fc8e---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fbb32a1e5fc8e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Femojis-aid-social-media-sentiment-analysis-stop-cleaning-them-out-bb32a1e5fc8e&source=-----bb32a1e5fc8e---------------------bookmark_footer-----------)![](../Images/37923fbf0be08bd4aef8544f5266e515.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Photo by [Denis Cherkashin](https://unsplash.com/@denic?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    on [Unsplash](https://unsplash.com/s/photos/emojis?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
  prefs: []
  type: TYPE_NORMAL
- en: '**TL;DR:**'
  prefs: []
  type: TYPE_NORMAL
- en: Including emojis in the social media sentiment analysis would robustly improve
    the sentiment classification accuracy no matter what model you use or how you
    incorporate emojis in the loop
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: More than half of the popular BERT-based encoders don‚Äôt support emojis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Twitter-RoBERTa encoder performs the best in sentiment analysis and coordinates
    well with emojis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Instead of cleaning emojis out, converting them to their textual description
    can help boost sentiment classification accuracy and handle the out-of-vocabulary
    issue.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As social media has become an essential part of people‚Äôs lives, the content
    that people share on the Internet is highly valuable to many parties. Many modern
    natural language processing (NLP) techniques were deployed to understand the general
    public‚Äôs social media posts. Sentiment Analysis is one of the most popular and
    critical NLP topics that focuses on analyzing opinions, sentiments, emotions,
    or attitudes toward entities in written texts computationally [[1](#1a95)]. Social
    media sentiment analysis (SMSA) is thus a field of understanding and learning
    representations for the sentiments expressed in short social media posts.
  prefs: []
  type: TYPE_NORMAL
- en: Another important feature of this project is the cute little in-text graphics
    ‚Äî emojisüòÑ. These graphical symbols have increasingly gained ground in social media
    communications. According to [Emojipedia‚Äôs statistics](https://emojipedia.org/stats/)
    in 2021, a famous emoji reference site, over one-fifth of the tweets now contains
    emojis (21.54%), while over half of the comments on Instagram include emojis.
    Emojis are handy and concise ways to express emotions and convey meanings, which
    may explain their great popularity.
  prefs: []
  type: TYPE_NORMAL
- en: However ubiquitous emojis are in network communications, they are not favored
    by the field of NLP and SMSA. In the stage of preprocessing data, emojis are usually
    removed alongside other unstructured information like URLs, stop words, unique
    characters, and pictures [[2](#c7e8)]. While some researchers have started to
    study the potential of including emojis in SMSA in recent years, it remains a
    niche approach and awaits further research. This project aims to **examine the
    emoji-compatibility of trending BERT encoders** and **explore different methods
    of incorporating emojis in SMSA to improve accuracy**.
  prefs: []
  type: TYPE_NORMAL
- en: Table of Contents
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**1** [**Background Knowledge**](#776f)'
  prefs: []
  type: TYPE_NORMAL
- en: 1.1 [What is SMSA exactly?](#4843)
  prefs: []
  type: TYPE_NORMAL
- en: 1.2 [Development of Sentiment Analysis Methodologies](#d8db)
  prefs: []
  type: TYPE_NORMAL
- en: 2[**Experiment**](#cfe0)2.1 [Model design](#40f1)
  prefs: []
  type: TYPE_NORMAL
- en: 2.2 [Lesson learned in Data Preparation Stage (A Sad Story)](#2f88)
  prefs: []
  type: TYPE_NORMAL
- en: 2.3 [Emoji-compatibility Test of the BERT family](#666a)
  prefs: []
  type: TYPE_NORMAL
- en: 2.4 [Experimenting Methods to Preprocess Emojis](#bc42)
  prefs: []
  type: TYPE_NORMAL
- en: '**3** [**Results Discussion**](#df89) **4** [**Conclusion**](#fa8d)[**Acknowledgments**](#5d30)[**Reference**](#e8ad)'
  prefs: []
  type: TYPE_NORMAL
- en: 1 Background Knowledge
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 1.1 What is SMSA exactly?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Here is some background knowledge about SMSA you might want to know before
    looking into the actual experiment. No technical background/math is required so
    far. Let me first explain the intuition of the most typical SMSA task:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/08d91363c13fa841f5deb5f015f9ded1.png)'
  prefs: []
  type: TYPE_IMG
- en: SMSA Example
  prefs: []
  type: TYPE_NORMAL
- en: As the picture above shows, given a social media post, the model (represented
    by the gray robot) will output the prediction of its sentiment label. In this
    example, the model responds that this post is 57.60% likely to express positive
    sentiment, 12.38% likely to be negative, and 30.02% likely to be neutral. Some
    studies classify posts in a binary way, i.e. positive/negative, but others consider
    ‚Äúneutral‚Äù as an option as well. This project follows the latter.
  prefs: []
  type: TYPE_NORMAL
- en: 1.2 Development of Sentiment Analysis Methodologies
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To my best knowledge, the first quantitative approach to studying social media
    sentiment is using the **lexicon-based method**. The model has a predefined lexicon
    that maps each token to a sentiment score. So, given a sentence, the model consults
    the lexicon, aggregates the sentiment scores of each word, and outputs the overall
    sentiment score. Very intuitive, right?
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a25d24f3eec39ee769107336e506031b.png)'
  prefs: []
  type: TYPE_IMG
- en: Basic Diagram of a Lexicon-based Sentiment Analysis Model
  prefs: []
  type: TYPE_NORMAL
- en: '[SentiWordNet](https://github.com/aesuli/SentiWordNet) and [VADER](https://github.com/cjhutto/vaderSentiment)
    are the two paradigms of this kind that have been favored by both the industry
    and academia.'
  prefs: []
  type: TYPE_NORMAL
- en: With the development of **machine learning**, classifiers like SVM, Random Forests,
    Multi-layer Perceptron, etc., gained ground in sentiment analysis. However, textual
    input isn‚Äôt valid for those models, so those classifiers are compounded with **word
    embedding models** to perform sentiment analysis tasks. Word embedding models
    convert words into numerical vectors that machines could play with. [Google‚Äôs
    word2vec](https://arxiv.org/abs/1301.3781) embedding model was a great breakthrough
    in representation learning for textual data, followed by [GloVe by Pennington
    et al.](https://nlp.stanford.edu/projects/glove/) and [fasttext by Facebook](https://fasttext.cc/).
  prefs: []
  type: TYPE_NORMAL
- en: Due to the sequential nature of natural language and the immense popularity
    of Deep Learning, **Recurrent Neural Network (RNN)** then becomes ‚Äúthe popular
    kid.‚Äù RNN decodes, or ‚Äúreads‚Äù, the sequence of word embeddings in order, preserving
    the sequential structure in the loop, which lexicon-based models and traditional
    machine learning models didn‚Äôt achieve.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c4ff03168aa444fe5c62fe3842d61634.png)'
  prefs: []
  type: TYPE_IMG
- en: A Typical Workflow of SMSA Nowadays
  prefs: []
  type: TYPE_NORMAL
- en: The evolved workflow is explained in the diagram above. Word embeddings are
    passed into an RNN model that outputs the last hidden state(s) (If you don‚Äôt know
    what the last hidden state is, it‚Äôs intuitively the ‚Äúsummary‚Äù composed by the
    RNN after ‚Äúreading‚Äù all the text.) Lastly, we use a feed-forward fully connected
    neural network to map the high-dimensional hidden state to a sentiment label.
  prefs: []
  type: TYPE_NORMAL
- en: We are almost there! The last piece of the puzzle is the **Transformer models**.
    Even if you haven‚Äôt learned NLP, you still might have heard about ‚ÄúAttention is
    All You Need‚Äù [[3](#1c64)]. In this paper, they proposed the self-attention technique
    and developed the Transformer Model.These models are so powerful that it transcends
    the previous models in almost every subtask of NLP. If you are not familiar with
    Transformer models, I strongly recommend you read [this introductory article](/transformers-141e32e69591)
    by Giuliano Giacaglia.
  prefs: []
  type: TYPE_NORMAL
- en: Both industry and academia have started to use the pretrained Transformer models
    on a large scale due to their unbeatable performance. Thanks to the [Hugging Face](http://huggingface.co/models)
    *transformer* package, developers can now easily import and deploy those large
    pretrained models. BERT, aka. Bidirectional Encoder Representations for Transformer,
    is the most famous transformer-based encoder model that learns excellent representations
    for text. Later on, RoBERTa, BERTweet, DeBERTa, etc., were developed based on
    BERT.
  prefs: []
  type: TYPE_NORMAL
- en: 2 Experiments
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'With all those background knowledge, we can now dive into the experiments and
    programming parts! If you do feel not confident with the mechanism of NLP, I recommend
    you to skip the technical details or go read some introductory blogs about NLP
    on Towards Data Science. Let‚Äôs clarify our experiment objectives first. We want
    to know:'
  prefs: []
  type: TYPE_NORMAL
- en: how compatible the currently trending pretrained BERT-based models are with
    emoji data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: how the performance would be influenced if we incorporate emojis in the SMSA
    process.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: what exactly we should do in the data processing stage to include the emojis.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 2.1 Model Design
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Our model follows the aforementioned neural network paradigm that consists
    of a pretrained BERT-based encoder, a Bi-LSTM layer, and a feedforward fully connected
    network. The diagram is shown below:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1fe39eea86b8886152593fa59c468784.png)'
  prefs: []
  type: TYPE_IMG
- en: Model diagram
  prefs: []
  type: TYPE_NORMAL
- en: To be clear, a preprocessed tweet is first passed through the pretrained encoder
    and becomes a sequence of representational vectors. Then, the representational
    vectors are passed through the Bi-LSTM layer. The two last hidden states of the
    two directions of LSTM will be processed by the feedforward layer to output the
    final prediction of the tweet‚Äôs sentiment.
  prefs: []
  type: TYPE_NORMAL
- en: We alter the encoder models and emoji preprocessing methods to observe the varying
    performance. The Bi-LSTM and feedforward layers are configured in the same way
    for all experiments in order to control variables. In the training process, we
    only train the Bi-LSTM and feed-forward layers. The parameters of pretrained encoder
    models are frozen.
  prefs: []
  type: TYPE_NORMAL
- en: The PyTorch implementation of this model and other technical details can be
    found in my [GitHub Repo](https://github.com/BaleChen/emoji-setiment-analysis).
  prefs: []
  type: TYPE_NORMAL
- en: 2.2 Lesson Learnt in Data Preparation Stage
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Data availability is every data science researcher‚Äôs pain in the neck. At first,
    I wanted to find a benchmark Twitter sentiment analysis dataset where I can compare
    the results with the previous models, but I encountered the following setbacks:'
  prefs: []
  type: TYPE_NORMAL
- en: Most datasets only have ‚Äútweet ID‚Äù as a query key to find the original content.
    To access the original tweet with the IDs, I need to have **Twitter API access**.
    My professor mentor and I both tried to apply for one, but neither of us was approved
    (We still don‚Äôt know why).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Well‚Ä¶another problem is that **a large portion of the tweets already perished**!
    This means either they were deleted by the author or by the Twitter server for
    some reason.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Even though there are few datasets that directly store tweet content, those
    stored in ***.csv or *.tsv formats are unable to preserve emojis**. Namely, the
    original tweets have emojis, but the compiled dataset that I downloaded from the
    web completely lost all emojis.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: So, whenever you want to conduct Twitter sentiment analysis, make sure you first
    validate the dataset if the dataset store tweets by their Tweet ID, which require
    you to spend extra effort to retrieve the original text. Tweets can easily perish
    if the dataset is from years ago. Also, don‚Äôt expect too much on applying for
    Twitter API. My mentor, who is an assistant professor at a prestigious American
    university, can‚Äôt even meet their requirement (for some unknown reason). Lastly,
    to preserve the emojis, don‚Äôt ever save them in csv or tsv format. Pickle, xlsx,
    or json can be your good choices.
  prefs: []
  type: TYPE_NORMAL
- en: Anyways, to find a dataset that retains emojis, has sentiment labels, and is
    of desirable size was extremely hard for me. Eventually, I found this Novak et
    al‚Äôs [dataset](https://figshare.com/articles/dataset/Emoji_Sentiment_Ranking/1600931)
    satisfies all criteria.
  prefs: []
  type: TYPE_NORMAL
- en: 2.3 Emoji-compatibility Test of the BERT family
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Before implementing the BERT-based encoders, we need to know whether they are
    compatible with emojis, i.e. whether they can produce unique representations for
    emoji tokens. More specifically, before passing the tweet into an encoder, it
    will first be ***tokenized*** by a model tokenizer that is unique to the encoder
    (e.g. RoBERTa-base uses the RoBERTa-base tokenizer, while BERT-base uses the BERT-base
    tokenizer). What the tokenizer does is splitting the long strings of textual input
    into individual word tokens that are in the vocabulary (shown in the graph below).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/46fd0a58138ad2eeca109b73da2a8f1b.png)'
  prefs: []
  type: TYPE_IMG
- en: spaCy‚Äôs rule-based Tokenizer ([source](https://github.com/explosion/spaCy))
  prefs: []
  type: TYPE_NORMAL
- en: In our case, if emojis are not in the tokenizer vocabulary, then they will all
    be tokenized into an unknown token (e.g. ‚Äú<UNK>‚Äù). Encoder models will thus produce
    the same vector representation for all those unknown tokens, in which case cleaning
    or not cleaning out the emojis will technically not make any difference in the
    model performance.
  prefs: []
  type: TYPE_NORMAL
- en: I chose the following list of common BERT-based encoders.
  prefs: []
  type: TYPE_NORMAL
- en: ALBERT-base-v2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: BERT-base, BERT-large
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: BERTweet-base, BERTweet-large
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: DeBERTa-base, DeBERTa-large
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: DistilBERT
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: RoBERTa-base, RoBERTa-large
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Twitter-RoBERTa
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: XLMRoBERTa-base, XLMRoBERTa-large
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The test can be easily done using the HuggingFace [*transformers*](https://pypi.org/project/transformers/)
    package and the [*emoji*](https://pypi.org/project/emoji/) package. We first import
    them:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: AutoTokenizer is a very useful function where you can use the name of the model
    to load the corresponding tokenizer, like the following one-line code where I
    import the BERT-base tokenizer.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Then, we use the *emoji* package to obtain the full list of emojis and use the
    encode and decode function to detect compatibility.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 2.4 Experimenting Methods to Preprocess Emojis
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We came up with 5 ways of data preprocessing methods to make use of the emoji
    information as opposed to removing emojis (rm) from the original tweets.
  prefs: []
  type: TYPE_NORMAL
- en: '**Directly encode (dir)** Use the pretrained encoder models that support emojis
    to directly vectorize the emojis. In this way, emojis are treated as normal word
    tokens. This is the most straightforward method.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Replacing emojis with descriptions (emoji2desc)** The pretrained encoders
    are not specifically trained to create representations for emojis. Rather, they
    are trained on a vast amount of text. We conjecture that encoders might have better
    representations for words than emojis, so converting emojis to their official
    description might help better extract the semantic information. For example, ‚ÄúI
    love animals üòç‚Äù will become ‚ÄúI love animals smiling face with heart-eyes.‚Äù The
    python realization is shown below (using the *emoji* package):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '**Concatenate emojis (concat-emoji)** Essentially, we reposition the emojis
    to the end of the sentence and perform directly encode method. Since emojis don‚Äôt
    belong to the grammatical structure of the sentences, we want to know if repositioning
    them would help better distinguish the textual and emoji information. For example,
    ‚ÄúThe cold weather is killing meüßä. Don‚Äôt wanna work any longerüò°üò≠. ‚Äù becomes ‚ÄúThe
    cold weather is killing me. Don‚Äôt wanna work any longer. üßäüò°üò≠‚Äù'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '**Concatenate description (concat-desc)** Besides, we also tested replacing
    those repositioned emojis with their textual descriptions.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '**Meta-feature (meta)** Instead of treating emojis as part of the sentence,
    we can also regard them as high-level features. We use the Emoji Sentiment Ranking
    [[4](#de51)] lexicon to get the positivity, neutrality, negativity, and sentiment
    score features. Then, we concatenate those features with the emoji vector representations,
    which form the emoji meta-feature vector of the tweet. This vector harbors the
    emoji sentiment information of the tweet. Pure text will be as usual passed through
    the encoder and BiLSTM layer, then the meta-feature vector will be concatenated
    with the last hidden states from the BiLSTM layer to be the input of the feedforward
    layers. This process is essentially isolating the emojis from the sentence and
    treating them as meta-data of a tweet.'
  prefs: []
  type: TYPE_NORMAL
- en: '**3 Results Discussion**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With all those technical designs, we finally arrive at the results part. First,
    let‚Äôs look at the emoji-compatibility of those commonBERT-based encoder models.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/bae1c8a4f2a8a36d4375f2199ed2ccb7.png)'
  prefs: []
  type: TYPE_IMG
- en: Emoji-compatibility of BERT-based emoji models
  prefs: []
  type: TYPE_NORMAL
- en: '**More than half of those models can‚Äôt recognize all emojis! RoBERTa** (both
    base and large versions), **DeBERTa** (both base and large versions), **BERTweet-large**,
    and **Twitter-RoBERTa** support all emojis. However, common encoders like **BERT**
    (both base and large versions), **DistilBERT**, and **ALBERT** nearly do not support
    any emoji.'
  prefs: []
  type: TYPE_NORMAL
- en: Now, let‚Äôs compare the model performance with different emoji-compatible encoders
    and different methods to incorporate emojis. The percentage in the following graph
    indicates the sentiment classification accuracy. Each cell represents the accuracy
    of an encoder model with a certain preprocessing method.
  prefs: []
  type: TYPE_NORMAL
- en: (Note that *emoji2vec* is a baseline model that is developed in 2015\. It‚Äôs
    not BERT-based but it‚Äôs a predefined emoji-embedding model that can also produce
    vector representation for emojis. It can be seen as an extension of Google‚Äôs Word2vec
    model)
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9603acca512c29461bcc59bc9041739b.png)'
  prefs: []
  type: TYPE_IMG
- en: Sentiment Classification Accuracy
  prefs: []
  type: TYPE_NORMAL
- en: To compare different methods to incorporate emojis into the SMSA process, we
    also show the accuracy across different methods with confidence intervals.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/564a0d218182070e34434676c064f680.png)'
  prefs: []
  type: TYPE_IMG
- en: The average accuracy of each preprocessing method (with confidence interval)
  prefs: []
  type: TYPE_NORMAL
- en: One of the most significant insights is that **including emojis, no matter how
    you include them, enhances the performance of** SMSA **models.** Removing the
    emojis lowers the accuracy by 1.202% on average. For methods that include emojis,
    the overlapping confidence intervals indicate a relatively blurry distinction.
    There‚Äôs no ‚Äúgenerally best‚Äù method to utilize emojis.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9e8a528a73b2c060276b63a66e7b3861.png)'
  prefs: []
  type: TYPE_IMG
- en: The average accuracy of each encoder model (with confidence interval)
  prefs: []
  type: TYPE_NORMAL
- en: For comparison among all encoder models, the results are shown in the bar chart
    above. The confidence interval is also annotated on the top of the bar chart.
    Small confidence intervals imply **high statistical confidence in the ranking**.
    **Twitter-RoBERTa performed the best** across all models, which is very likely
    caused by the **training domain**. ***emoji2vec***, which was developed in 2015
    and prior to the boom of transformer models, holds **relatively poor representations
    of emojis** under the standards of this time.
  prefs: []
  type: TYPE_NORMAL
- en: Now that no ‚Äúgenerally best‚Äù method is found, we probe into how different models
    would benefit differently from various preprocessing methods. The following graph
    depicts the percentage improvement of using a certain preprocessing method compared
    with removing emojis at the beginning.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e41ec99904cff81302c9a289b4d89a1f.png)'
  prefs: []
  type: TYPE_IMG
- en: Percentage improvement heatmap
  prefs: []
  type: TYPE_NORMAL
- en: Firstly, all the improvement indices are positive, which strongly justifies
    the usefulness of emojis in SMSA. Including emojis in the data would improve the
    SMSA model‚Äôs performance.
  prefs: []
  type: TYPE_NORMAL
- en: '**Generally for BERT-based models, directly encoding emojis seems to be a sufficient
    and sometimes the best method.** Surprisingly, the most straightforward methods
    work just as well as the complicated ones, if not better.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Poor emoji representation learning models might benefit more from converting
    emojis to textual descriptions.** Maximal and minimal improvement both appear
    on the emoji2vec model. It‚Äôs likely that emoji2vec has relatively worse vector
    representations of emojis, but converting emojis to their textual descriptions
    would help capture the emotional meanings of a social media post.'
  prefs: []
  type: TYPE_NORMAL
- en: '**RoBERTa-large displayed an unexpectedly small improvement regardless of preprocessing
    methods**, indicating that it doesn‚Äôt benefit as much from the emojis as other
    BERT-based models. This result might be explained by the fact that RoBERTa-large‚Äôs
    architecture might be more suitable for learning representations for pure text
    than for emojis, but it still awaits a more rigorous justification.'
  prefs: []
  type: TYPE_NORMAL
- en: 4 Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: From this project, the key takeaway is that **including emojis in the loop of
    SMSA would improve the sentiment classification accuracy no matter what model
    or preprocessing method you use. So, THINK TWICE about cleaning them out when
    you face a social media sentiment analysis task!**
  prefs: []
  type: TYPE_NORMAL
- en: The best model to handle SMSA tasks and coordinate with emojis is the [Twitter-RoBERTa](https://huggingface.co/cardiffnlp/twitter-roberta-base-sentiment)
    encoder! Please use it if you are dealing with Twitter data and analyzing tweet
    sentiment.
  prefs: []
  type: TYPE_NORMAL
- en: Regarding how to incorporate the emojis specifically, the methods didn‚Äôt show
    a significant difference, so a straightforward way ‚Äî directly treating the emojis
    as regular word tokens ‚Äî would do the job perfectly. Yet, considering that half
    of the common BERT-based encoders in our study don‚Äôt support emojis, we recommend
    using the [emoji2desc](#ff68) method. That means converting emojis to their official
    textual description using [a simple line of code](#57b7) I mentioned before, which
    can easily handle the out-of-vocabulary emoji tokens.
  prefs: []
  type: TYPE_NORMAL
- en: If you are using traditional word embeddings like *word2vec* and you also don‚Äôt
    want to waste the cute emojis, consider using the [emoji2desc](#ff68) or [concat-emoji](#afb8)
    method instead of using *emoji2vec* model.
  prefs: []
  type: TYPE_NORMAL
- en: Hope our project can guide SMSA researchers and industry workers on how to include
    emojis in the process. More importantly, this project offers a new perspective
    on improving SMSA accuracy. Diving into the technical bits is not necessarily
    the only way to make progress, and for example, these simple but powerful emojis
    can help as well.
  prefs: []
  type: TYPE_NORMAL
- en: Scripts, an academic report, and more can be found in my [GitHub Repo](https://github.com/BaleChen/emoji-setiment-analysis).
  prefs: []
  type: TYPE_NORMAL
- en: Regarding images in the post, all unless otherwise noted are by the author.
  prefs: []
  type: TYPE_NORMAL
- en: '**Acknowledgments**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: I would like to extend my warmest gratitude to my research supervisor and mentor
    Professor [Mathieu Lauri√®re](https://mlauriere.github.io/). He provides me with
    insightful advice and guides me through this summer research. It is my great honor
    and pleasure to finish this study with him and receive his email greeting on my
    birthday.
  prefs: []
  type: TYPE_NORMAL
- en: This work was also supported in part through the NYU IT High Performance Computing
    resources, services, and staff expertise.
  prefs: []
  type: TYPE_NORMAL
- en: Besides, I genuinely appreciate NYU and NYU Shanghai for offering me the [DURF](https://shanghai.nyu.edu/academics/undergraduate-research)
    research opportunity.
  prefs: []
  type: TYPE_NORMAL
- en: Thanks to all my friends and family who helped me throughout this summer. The
    research would not have been possible without any of you.
  prefs: []
  type: TYPE_NORMAL
- en: Reference
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[1] Liu, B. [Sentiment Analysis: Mining Opinions, Sentiments, and Emotions.](https://doi.org/10.1017/CBO9781139084789)
    (2015), Cambridge University Press, Cambridge.'
  prefs: []
  type: TYPE_NORMAL
- en: '[2] Chakriswaran, P., Vincent, D. R., Srinivasan, K., Sharma, V., Chang, C.-Y.,
    and Reina, D. G. [Emotion AI-Driven Sentiment Analysis: A Survey, Future Research
    Directions, and Open Issues.](https://doi.org/10.3390/app9245462) (2019), Applied
    Sciences.'
  prefs: []
  type: TYPE_NORMAL
- en: '[3] Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez,
    A., Kaiser, u., & Polosukhin, I. [Attention is All you Need.](https://papers.nips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf)
    (2017), In *Advances in Neural Information Processing Systems*. Curran Associates,
    Inc.'
  prefs: []
  type: TYPE_NORMAL
- en: '[4] Kralj Novak, P., Smailoviƒá, J., Sluban, B., & Mozetiƒç, I. [Sentiment of
    Emojis.](https://doi.org/10.1371/journal.pone.0144296) (2015), *PLOS ONE*, *10*(12),
    e0144296.'
  prefs: []
  type: TYPE_NORMAL
