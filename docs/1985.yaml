- en: 'How GPT works: A Metaphoric Explanation of Key, Value, Query in Attention,
    using a Tale of Potion'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: GPT 的工作原理：使用一个药水的故事对注意力中的键、值、查询进行隐喻性的解释
- en: 原文：[https://towardsdatascience.com/how-gpt-works-a-metaphoric-explanation-of-key-value-query-in-attention-using-a-tale-of-potion-8c66ace1f470?source=collection_archive---------0-----------------------#2023-06-17](https://towardsdatascience.com/how-gpt-works-a-metaphoric-explanation-of-key-value-query-in-attention-using-a-tale-of-potion-8c66ace1f470?source=collection_archive---------0-----------------------#2023-06-17)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/how-gpt-works-a-metaphoric-explanation-of-key-value-query-in-attention-using-a-tale-of-potion-8c66ace1f470?source=collection_archive---------0-----------------------#2023-06-17](https://towardsdatascience.com/how-gpt-works-a-metaphoric-explanation-of-key-value-query-in-attention-using-a-tale-of-potion-8c66ace1f470?source=collection_archive---------0-----------------------#2023-06-17)
- en: '![](../Images/110a36140729edc095c7c2d5b8ae1bcb.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/110a36140729edc095c7c2d5b8ae1bcb.png)'
- en: 'Source: Generated by Midjourney.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 来源：由 Midjourney 生成。
- en: '[](https://medium.com/@lilipads93?source=post_page-----8c66ace1f470--------------------------------)[![Lili
    Jiang](../Images/ca3c10589bf964a7c29e70f6cdd12244.png)](https://medium.com/@lilipads93?source=post_page-----8c66ace1f470--------------------------------)[](https://towardsdatascience.com/?source=post_page-----8c66ace1f470--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----8c66ace1f470--------------------------------)
    [Lili Jiang](https://medium.com/@lilipads93?source=post_page-----8c66ace1f470--------------------------------)'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@lilipads93?source=post_page-----8c66ace1f470--------------------------------)[![Lili
    Jiang](../Images/ca3c10589bf964a7c29e70f6cdd12244.png)](https://medium.com/@lilipads93?source=post_page-----8c66ace1f470--------------------------------)[](https://towardsdatascience.com/?source=post_page-----8c66ace1f470--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----8c66ace1f470--------------------------------)
    [Lili Jiang](https://medium.com/@lilipads93?source=post_page-----8c66ace1f470--------------------------------)'
- en: ·
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F3cc31a4b9430&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-gpt-works-a-metaphoric-explanation-of-key-value-query-in-attention-using-a-tale-of-potion-8c66ace1f470&user=Lili+Jiang&userId=3cc31a4b9430&source=post_page-3cc31a4b9430----8c66ace1f470---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----8c66ace1f470--------------------------------)
    ·10 min read·Jun 17, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F8c66ace1f470&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-gpt-works-a-metaphoric-explanation-of-key-value-query-in-attention-using-a-tale-of-potion-8c66ace1f470&user=Lili+Jiang&userId=3cc31a4b9430&source=-----8c66ace1f470---------------------clap_footer-----------)'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F3cc31a4b9430&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-gpt-works-a-metaphoric-explanation-of-key-value-query-in-attention-using-a-tale-of-potion-8c66ace1f470&user=Lili+Jiang&userId=3cc31a4b9430&source=post_page-3cc31a4b9430----8c66ace1f470---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----8c66ace1f470--------------------------------)
    ·10 分钟阅读·2023年6月17日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F8c66ace1f470&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-gpt-works-a-metaphoric-explanation-of-key-value-query-in-attention-using-a-tale-of-potion-8c66ace1f470&user=Lili+Jiang&userId=3cc31a4b9430&source=-----8c66ace1f470---------------------clap_footer-----------)'
- en: --
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F8c66ace1f470&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-gpt-works-a-metaphoric-explanation-of-key-value-query-in-attention-using-a-tale-of-potion-8c66ace1f470&source=-----8c66ace1f470---------------------bookmark_footer-----------)'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F8c66ace1f470&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-gpt-works-a-metaphoric-explanation-of-key-value-query-in-attention-using-a-tale-of-potion-8c66ace1f470&source=-----8c66ace1f470---------------------bookmark_footer-----------)'
- en: '**!Update: a 10-min video version of this post is now** [**available**](https://www.youtube.com/watch?v=Mhy7I4E6eXs&ab_channel=ArchermanCapital)**!**'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '**！更新：该帖子现在有一个 10 分钟的视频版本** [**可用**](https://www.youtube.com/watch?v=Mhy7I4E6eXs&ab_channel=ArchermanCapital)**！**'
- en: The backbone of ChatGPT is the GPT model, which is built using the **Transformer**
    architecture. The backbone of Transformer is the **Attention** mechanism. The
    hardest concept to grok in Attention for many is **Key, Value, and Query**. In
    this post, I will use an analogy of potion to internalize these concepts. Even
    if you already understand the maths of transformer mechanically, I hope by the
    end of this post, you can develop a more intuitive understanding of the inner
    workings of GPT from end to end.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: ChatGPT的核心是GPT模型，它是使用**变换器**架构构建的。变换器的核心是**注意力**机制。对许多人来说，理解注意力中最难的概念是**键、值和查询**。在这篇文章中，我将用药水的类比来帮助理解这些概念。即使你已经机械地理解了变换器的数学部分，我希望通过这篇文章，你能从头到尾更直观地理解GPT的内部工作原理。
- en: This explanation requires no maths background. For the technically inclined,
    I add more technical explanations in […]. You can also safely skip notes in [brackets]
    and side notes in quote blocks like this one. Throughout my writing, I make up
    some human-readable interpretation of the intermediary states of the transformer
    model to aid the explanation, but GPT doesn’t think exactly like that.
  id: totrans-11
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 这个解释不需要数学背景。对于技术性强的读者，我在[…]中添加了更多的技术解释。你也可以安全地跳过[方括号]中的注释和像这样在引号块中的附注。在我的写作过程中，我编造了一些可读的人类解释，用于说明变换器模型的中间状态，以帮助解释，但GPT并不完全是这样思考的。
- en: ''
  id: totrans-12
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[When I talk about “attention”, I exclusively mean “self-attention”, as that
    is what’s behind GPT. But the same analogy explains the general concept of “attention”
    just as well.]'
  id: totrans-13
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '[当我谈论“注意力”时，我专指“自注意力”，因为这正是GPT背后的机制。但同样的类比也可以很好地解释“注意力”的一般概念。]'
- en: The Set Up
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 设置
- en: 'GPT can spew out paragraphs of coherent content, because it does one task superbly
    well: “Given a text, what word comes next?” Let’s role-play GPT: *“Sarah lies
    still on the bed, feeling ____”.* Can you fill in the blank?'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: GPT可以输出连贯的段落内容，因为它非常擅长执行一个任务：“给定一个文本，下一个词是什么？”让我们角色扮演GPT：*“Sarah躺在床上，感觉____”*。你能填上这个空白吗？
- en: One reasonable answer, among many, is *“tired”*. In the rest of the post, I
    will unpack how GPT arrives at this answer. (For fun, I put this prompt in ChatGPT
    and it wrote a short [story](https://chat.openai.com/share/169f2702-3811-4388-b3d4-67064903f4b2)
    out of it.)
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 其中一个合理的答案是*“疲倦”*。在接下来的内容中，我将详细说明GPT是如何得出这个答案的。（为了好玩，我将这个提示放入ChatGPT中，它从中写了一个简短的[故事](https://chat.openai.com/share/169f2702-3811-4388-b3d4-67064903f4b2)）。
- en: 'The Analogy: (Key, Value, Query), or (Tag, Potion, Recipe)'
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 类比：（键、值、查询），或（标签、药水、配方）
- en: 'You feed the above prompt to GPT. In GPT, each word is equipped with three
    things: Key, Value, Query, whose values are learned from devouring the entire
    internet of texts during the training of the GPT model. It’s the interaction among
    these three ingredients that allows GPT to make sense of a word in the context
    of a text. So what do they do, really?'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 你将上述提示输入GPT。在GPT中，每个词都配备了三样东西：键（Key）、值（Value）、查询（Query），这些值是在GPT模型训练期间从整个互联网的文本中学习到的。正是这三种成分的相互作用使得GPT能够在文本的上下文中理解一个词。那么它们究竟是做什么的呢？
- en: '![](../Images/facf02f8c1e0c648188d2ae03f38ed57.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/facf02f8c1e0c648188d2ae03f38ed57.png)'
- en: 'Source: created by the author.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 来源：由作者创建。
- en: 'Let’s set up our analogy of alchemy. **For each word**, we have:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们设立炼金术的类比。**对于每一个词**，我们有：
- en: '**A potion** (aka “value”): The potion contains rich information about the
    word. For illustrative purpose, imagine the potion of the word *“lies”* contains
    information like *“tired; dishonesty; can have a positive connotation if it’s
    a white lie; …”*. The word *“lies”* can take on multiple meanings, e.g. “tell
    lies” (associated with dishonesty) or, “lies down” (associated with tired). You
    can only tell the real meaning in the context of a text. Right now, the potion
    contains information for both meanings, because it doesn’t have the context of
    a text.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**药水**（即“值”）：药水包含了关于词的丰富信息。为了说明目的，假设词*“lies”*的药水包含了如*“疲倦；不诚实；如果是善意的谎言可能有积极的含义；……”*的信息。词*“lies”*可以有多重含义，例如“说谎”（与不诚实相关）或“躺下”（与疲倦相关）。只有在文本的上下文中，你才能真正了解其含义。目前，药水包含了两种含义的信息，因为它没有文本的上下文。'
- en: '**An alchemist’s recipe** (aka “query”): The alchemist of a given word, e.g.
    *“lies”*, goes over all the nearby words. He finds a few of those words relevant
    to his own word *“lies”*, and he is tasked with filling an empty flask with potions
    of those words. The alchemist has a recipe, listing specific criteria that identifies
    what potions he should pay *attention* to.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**炼金术师的配方**（也称为“查询”）：给定单词的炼金术师，例如*“谎言”*，会查看所有附近的单词。他找到了一些与自己单词*“谎言”*相关的单词，他的任务是用这些单词的药水填满一个空药瓶。炼金术师有一个配方，列出了确定他应该关注哪些药水的具体标准。'
- en: '**A tag** (aka “key”): each potion (value) comes with a tag (key). If the tag
    (key) matches well with the alchemist’s recipe (query), the alchemist will pay
    attention to this potion.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**标签**（也称为“关键字”）：每种药水（值）都有一个标签（关键字）。如果标签（关键字）与炼金术师的配方（查询）匹配得很好，炼金术师将会关注这种药水。'
- en: 'Attention: the Alchemist’s Potion Mixology'
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 注意：炼金术师的药水调配
- en: '![](../Images/9f7049af7c85eaa5446c9fd306fc66f9.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9f7049af7c85eaa5446c9fd306fc66f9.png)'
- en: 'The potions with their tags. Source: created by the author.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 带标签的药水。来源：作者创作。
- en: In the first step (attention), the alchemists of all words each go out on their
    own quests to fill their flasks with potions from relevant words.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一步（注意），所有单词的炼金术师各自出发，去从相关单词中获取药水填满他们的药瓶。
- en: 'Let’s take the alchemist of the word *“lies”* for example. He knows from previous
    experience — after being pre-trained on the entire internet of texts — that words
    that help interpret *“lies”* in a sentence are usually of the form: *“some flat
    surfaces, words related to dishonesty, words related to resting”*. He writes down
    these criteria in his recipe (query) and looks for tags (key) on the potions of
    other words. If the tag is very similar to the criteria, he will pour a lot of
    that potion into his flask; if the tag is not similar, he will pour little or
    none of that potion.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 以单词*“谎言”*的炼金术师为例。他知道根据之前的经验——经过对整个互联网文本的预训练——有助于解释句子中*“谎言”*的单词通常是：*“一些平面表面、与不诚实相关的词、与休息相关的词”*。他将这些标准写在他的配方（查询）中，并寻找其他单词药水上的标签（关键字）。如果标签与标准非常相似，他会将大量这种药水倒入他的药瓶中；如果标签不相似，他则会倒入少量或不倒入。
- en: So he finds the tag for *“bed”* says “*a flat piece of furniture*”. That’s similar
    to *“some flat surfaces”* in his recipe! He pours the potion for *“bed”* in his
    flask. The potion (value) for *“bed”* contains information like “*tired, restful,
    sleepy, sick*”.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 所以他发现*“床”*的标签上写着“*一个平面的家具*”。这与他配方中的*“一些平面表面”*类似！他将*“床”*的药水倒入药瓶中。*“床”*的药水（值）包含的信息有“*疲惫、安静、昏昏欲睡、生病*”。
- en: The alchemist for the word *“lies”* continues the search. He finds the tag for
    the word “*still*” says “*related to resting”* (among other connotations of the
    word *“still”*). That’s related to his criteria *“restful”*, so he pours in part
    of the potion from *“still”*, which contains information like *“restful, silent,
    stationary”.*
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 单词*“谎言”*的炼金术师继续搜索。他发现*“still”*的标签上写着“*与休息相关*”（以及*“still”*的其他含义）。这与他的标准*“休息”*相关，因此他将一部分*“still”*的药水倒入药瓶中，包含的信息有*“休息、安静、静止”*。
- en: He looks at the tag of *“on”, “Sarah”, “the”, “feeling”* and doesn’t find them
    relevant. So he doesn’t pour any of their potions into his flask.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 他查看了*“on”，“Sarah”，“the”，“feeling”*的标签，发现它们不相关。因此，他没有将这些药水倒入他的药瓶。
- en: Remember, he needs to check his own potion too. The tag of his own potion *“lies”*
    says *“a verb related to resting”,* which matches his recipe*.* So he pours some
    of his own potion into the flask as well, which contains information like “*tired;
    dishonest; can have a positive connotation if it’s a white lie; …”.*
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，他也需要检查自己的药水。自己的药水*“谎言”*的标签上写着*“与休息相关的动词”*，这与他的配方*“休息”*相匹配。因此，他也将自己药水的一部分倒入药瓶中，包含的信息有“*疲惫；不诚实；如果是善意的谎言，可能有积极的含义；……*”。
- en: By the end of his quest to check words in the text, his flask is full.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在他完成检查文本中的单词任务时，他的药瓶已经满了。
- en: '![](../Images/dfc11bbf52b9f698260d0b6eaff15f95.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/dfc11bbf52b9f698260d0b6eaff15f95.png)'
- en: 'Source: created by the author.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 来源：作者创作。
- en: Unlike the original potion for *“lies”*, this mixed potion now takes into account
    the context of this very specific sentence. Namely, it has a lot of elements of
    *“tired, exhausted”* and only a pinch of “*dishonest*”.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 与原始的*“谎言”*药水不同，这种混合药水现在考虑了这个特定句子的上下文。也就是说，它包含了很多*“疲惫，精疲力竭”*的元素，只有一点点“*不诚实*”。
- en: 'In this quest, **the alchemist knows to pay attention to the right words, and
    combines the value of those relevant words**. **This is a metaphoric step for
    “attention”.** We’ve just explained the most important equation for Transformer,
    the underlying architecture of GPT:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个任务中，**炼金术师知道如何关注正确的词，并结合那些相关词的值**。**这是“注意力”的隐喻步骤。**我们刚刚解释了Transformer的最重要方程，这是GPT的底层架构：
- en: '![](../Images/c71cb3f0e0cc58d3bc9b346807c787ba.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c71cb3f0e0cc58d3bc9b346807c787ba.png)'
- en: 'Q is Query; K is Key; V is Value. Source: [Attention is All You Need](https://arxiv.org/pdf/1706.03762.pdf)'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: Q 是查询；K 是键；V 是值。来源：[Attention is All You Need](https://arxiv.org/pdf/1706.03762.pdf)
- en: 'Advanced notes:'
  id: totrans-41
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 高级笔记：
- en: ''
  id: totrans-42
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 1\. Each alchemist looks at every bottle, including their own [Q·K.transpose()].
  id: totrans-43
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 1\. 每位炼金术师查看每个瓶子，包括他们自己的 [Q·K.transpose()]。
- en: ''
  id: totrans-44
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 2\. The alchemist can match his recipe (query) with the tag (key) quickly and
    make a fast decision. [The similarity between query and key is determined by a
    dot product, which is a fast operation.] Additionally, all alchemists do their
    quests in parallel, which also helps speed things up. [Q·K.transpose() is a matrix
    multiplication, which is parallelizable. Speed is a winning feature of Transformer,
    compared to its predecessor Recurrent Neural Network that computes sequentially.]
  id: totrans-45
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 2\. 炼金术师可以快速将他的配方（查询）与标签（键）匹配，并做出快速决定。[查询和键之间的相似性通过点积确定，这是一项快速操作。] 此外，所有炼金术师的任务都是并行进行的，这也有助于加快速度。[Q·K.transpose()
    是矩阵乘法，可以并行处理。与按顺序计算的前身递归神经网络相比，速度是Transformer的优势特性。]
- en: ''
  id: totrans-46
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 3\. The alchemist is picky. He only selects the top few potions, instead of
    mixing in a bit of everything. [We use softmax to collapse Q·K.transpose(). Softmax
    will pull the inputs into more extreme values, and collapse many inputs to near-zero.]
  id: totrans-47
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 3\. 炼金术师很挑剔。他只选择最好的几种药水，而不是混合所有药水。[我们使用softmax来压缩Q·K.transpose()。softmax将输入值拉向更极端的值，并将许多输入压缩到接近零。]
- en: ''
  id: totrans-48
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 4\. At this stage, the alchemist does not take into account the ordering of
    words. Whether it’s “Sarah lies still on the bed, feeling” or “still bed the Sarah
    feeling on lies”, the filled flask (output of attention) will be the same. [In
    the absence of “positional encoding”, Attention(Q, K, V) is independent of word
    positions.]
  id: totrans-49
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 4\. 在这个阶段，炼金术师不考虑单词的顺序。无论是“Sarah lies still on the bed, feeling”还是“still bed
    the Sarah feeling on lies”，填满的烧瓶（注意力的输出）将是相同的。[在没有“位置编码”的情况下，Attention(Q, K, V)
    与单词位置无关。]
- en: ''
  id: totrans-50
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 5\. The flask always returns 100% filled, no more, no less. [The softmax is
    normalized to 1.]
  id: totrans-51
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 5\. 烧瓶总是返回100%满，没有多也没有少。[softmax被归一化为1。]
- en: ''
  id: totrans-52
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 6\. The alchemist’s recipe and the potions’ tags must speak the same language.
    [The Query and Key must be of the same dimension to be able to dot product together
    to communicate. The Value can take on a different dimension if you wish.]
  id: totrans-53
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 6\. 炼金术师的配方和药水的标签必须使用相同的语言。[查询和键必须具有相同的维度才能进行点积运算以进行通信。值可以采用不同的维度，如果你愿意的话。]
- en: ''
  id: totrans-54
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 7\. The technically astute readers may point out we didn’t do **masking**. I
    don’t want to clutter the analogy with too many details but I will explain it
    here. In self-attention, each word can only see the previous words. So in the
    sentence *“Sarah lies still on the bed, feeling”*, *“lies”* only sees *“Sarah”;
    “still”* only sees *“Sarah”, “lies”.* The alchemist of *“still”* can’t reach into
    the potions of *“on”, “the”,* *“bed”* and *“feeling”*.
  id: totrans-55
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 7\. 技术精明的读者可能会指出我们没有进行**掩蔽**。我不想用太多细节来混淆类比，但我会在这里解释。在自注意力中，每个词只能看到前面的词。因此，在句子*“Sarah
    lies still on the bed, feeling”*中，*“lies”*只看到*“Sarah”；“still”*只看到*“Sarah”和“lies”。*
    *“still”*的炼金术师不能接触到*“on”、“the”、“bed”*和*“feeling”*的药水。
- en: 'Feed Forward: Chemistry on the Mixed Potions'
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 'Feed Forward: 混合药水中的化学反应'
- en: Up till this point, the alchemist simply pours the potion from other bottles.
    In other words, he pours the potion of *“lies”* — “*tired; dishonest;…” —* as
    a uniform mixture into the flask; he can’t distill out the *“tired”* part and
    discard the *“dishonest”* part just yet. [Attention is simply summing the different
    V’s together, weighted by the softmax.]
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 直到此时，炼金术师只是将其他瓶子的药水倒入烧瓶中。换句话说，他将*“谎言”*——“*疲倦；不诚实；…”*——作为均匀混合物倒入烧瓶中；他还不能将*“疲倦”*部分提取出来并丢弃*“不诚实”*部分。[注意力只是将不同的V加在一起，经过softmax加权。]
- en: '![](../Images/57dea32ee1485643d85681ea3508552a.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/57dea32ee1485643d85681ea3508552a.png)'
- en: 'Source: created by the author.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 来源：作者创建。
- en: Now comes the real chemistry (feed forward). The alchemist mixes everything
    together and does some synthesis. He notices interactions between words like *“sleepy”*
    and*“restful”*, etc. He also notices that *“dishonesty”* is only mentioned in
    one potion. He knows from past experiences how to make some ingredients interact
    with each other and how discard the one-off ones. [The feed forward layer is a
    linear (and then non-linear) transformation of the Value. Feed forward layer is
    the building block of neural networks. You can think of it as the “thinking” step
    in Transformer, while the earlier mixology step is simply “collecting”.]
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 现在进入真正的化学过程（前馈）。炼金术师将一切混合在一起并进行合成。他注意到词语之间的互动，例如*“sleepy”*和*“restful”*等。他还发现*“dishonesty”*只在一个药水中提到。他根据以往的经验知道如何让一些成分相互作用，以及如何丢弃那些一次性的成分。[前馈层是值的线性（然后是非线性）变换。前馈层是神经网络的构建块。你可以将其视为Transformer中的“思考”步骤，而早期的混合步骤只是“收集”。]
- en: The resulting potion after his processing becomes much more useful for the task
    of predicting the next word. Intuitively, it represents some richer properties
    about this word in the context of its sentence, in contrast with the starting
    potion (value) that is out of context.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 经过处理后的药水变得更适合预测下一个词。直观上，它在句子上下文中表现出该词的一些更丰富的特性，而与起始药水（值）相比，后者是脱离上下文的。
- en: 'The Final Linear and Softmax Layer: the Assembly of Alchemists'
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 最终线性和softmax层：炼金术师的集会
- en: How do we get from here to the final output, which is to predict that the next
    word after *“Sarah lies still on the bed, feeling ___”* is *“tired”*?
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何从这里得到最终输出，即预测*“Sarah lies still on the bed, feeling ___”*之后的下一个词是*“tired”*？
- en: So far, each alchemist has been working independently, only tending to his own
    word. Now all the alchemists of different words assemble and stack their flasks
    in the original word order and present them to the final linear and softmax layer
    of the Transformer. What do I mean by this? Here, we must depart from the metaphor.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，每个炼金术师一直独立工作，只关注自己的词汇。现在，所有不同词汇的炼金术师汇集在一起，将他们的药水瓶按照原始词序排列，并呈现给Transformer的最终线性和softmax层。这是什么意思呢？这里，我们必须离开比喻。
- en: This final linear layer synthesizes information *across* different words. Based
    on pre-trained data, one plausible learning is that the immediate previous word
    is important to predict the next word. As an example, the linear layer might heavily
    focus on the last flask (“feeling”’s flask).
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 这个最终线性层综合了不同词汇的信息。基于预训练数据，一个可能的学习是，紧接前一个词对预测下一个词是重要的。例如，线性层可能会重点关注最后一个药水瓶（“feeling”药水瓶）。
- en: Then combined with the softmax layer, this step assigns every single word in
    our vocabulary a probability for how likely this is the next word after *“Sarah
    lies on the bed, feeling…”*. For example, non-English words will receive probabilities
    close to 0\. Words like *“tired”, “sleepy”, “exhausted”* will receive high probabilities.
    We then pick the top winner as the final answer.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 然后结合softmax层，这一步为词汇表中的每个词分配一个概率，表示在*“Sarah lies on the bed, feeling…”*之后这个词的可能性。例如，非英语词汇的概率接近0。像*“tired”、
    “sleepy”、 “exhausted”*这样的词将获得高概率。然后我们选择概率最高的词作为最终答案。
- en: '![](../Images/cdd3c4c2a3d6118a0562c52d4b07a032.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cdd3c4c2a3d6118a0562c52d4b07a032.png)'
- en: 'Source: created by the author.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 来源：作者创建。
- en: Recap
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 总结
- en: Now you’ve built a minimalist GPT!
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经构建了一个极简主义的GPT！
- en: To recap, for each word in the attention step, you determine which words (including
    self) each word should pay attention to, based on how well that word’s query (recipe)
    matches the other word’s key (tag). You mix together those words’ values (potions)
    proportional to the attention that word pays to them. You process this mixture
    to do some “thinking” (feed forward). Once each word is processed, you then combine
    the mixtures from all the other words to do more “thinking” (linear layer) and
    make the final prediction of what the next word should be.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 总结一下，在注意力步骤中，你确定每个词应该关注哪些词（包括自身），基于该词的查询（配方）与其他词的键（标签）的匹配程度。你将这些词的值（药水）按该词对它们的注意力进行混合。你处理这种混合物以进行一些“思考”（前馈）。每个词处理后，你将所有其他词的混合物组合在一起，以进行更多的“思考”（线性层）并做出最终预测，确定下一个词应该是什么。
- en: '![](../Images/269793bc336270de37f8b5bc333a5b7c.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/269793bc336270de37f8b5bc333a5b7c.png)'
- en: 'Source: created by the author.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 来源：作者创建。
- en: 'Side note: the language “decoder” is a vestige from the original [paper](https://arxiv.org/pdf/1706.03762.pdf),
    as Transformer was first used for machine translation tasks. You “encode” the
    source language into embeddings, and “decode” from the embeddings to the target
    language.'
  id: totrans-74
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 旁注：“解码器”这一术语是原始[论文](https://arxiv.org/pdf/1706.03762.pdf)中的残余，因为 Transformer
    最初用于机器翻译任务。你“编码”源语言到嵌入中，然后“解码”从嵌入到目标语言。
- en: This is a good stopping point.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个很好的停顿点。
- en: 'If you are eager to learn more, we will go over two more variants on top of
    the minimalist architecture above: Multi-Head Attention and repeated blocks.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你渴望了解更多，我们将在上述简约架构的基础上介绍另外两种变体：多头注意力和重复块。
- en: 'Multi-Head Attention: Many Sets of Alchemists'
  id: totrans-77
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 多头注意力：许多炼金术士的集合
- en: So far, every word only has one alchemist’s recipe, one tag, and one potion.
    [For each word, each query, value, key is a single vector, not a matrix.] But
    we can get better results if we equip each word with a few sets of recipe, tag,
    potion. For [reference](https://www.cs.ubc.ca/~amuham01/LING530/papers/radford2018improving.pdf),
    GPT uses 12 sets (aka 12 “attention heads”) per word. Maybe for each word, the
    alchemist from the first set specializes in analyzing sentiments, the alchemist
    from the second set specializes in resolving references (what does “it” refer
    to), etc.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，每个词只有一个炼金术士的配方、一个标签和一种药水。[对于每个词，每个查询、值、键是一个单一向量，而不是一个矩阵。] 但是，如果我们为每个词配备几组配方、标签、药水，我们可以获得更好的结果。对于[参考](https://www.cs.ubc.ca/~amuham01/LING530/papers/radford2018improving.pdf)，GPT
    每个词使用 12 组（即 12 个“注意力头”）。也许对于每个词，第一组的炼金术士专注于分析情感，第二组的炼金术士专注于解析引用（“它”指的是什么），等等。
- en: 'Advanced note: The group of sentiment alchemists have only studied the sentiment
    potions; they wouldn’t know how to handle potions from the other sets, nor will
    they ever touch those. [V, K, Q from the same attention head are jointly trained.
    V, K, Q from different attention heads don’t interact in the Multi-Head Attention
    step.]'
  id: totrans-79
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 高级说明：情感炼金术士组只研究情感药水；他们不会知道如何处理其他集合的药水，也不会触及那些药水。[来自同一注意力头的 V、K、Q 是联合训练的。来自不同注意力头的
    V、K、Q 在多头注意力步骤中不进行交互。]
- en: '![](../Images/e4be1b05ac623d54f374fe9d73c4dceb.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e4be1b05ac623d54f374fe9d73c4dceb.png)'
- en: 'Source: created by the author.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 来源：由作者创建。
- en: The 12 alchemists of each word present their specialized, filled flasks together
    [concatenate output of different attention heads]. As a group, they do a giant
    chemical reaction using all these flasks and present one resulting potion [feed
    forward, aka linear layer].
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 每个词的 12 位炼金术士将他们的专门化、填充的瓶子一起[连接不同注意力头的输出]。作为一个群体，他们利用所有这些瓶子进行一次巨大的化学反应，并呈现出一种最终的药水[前馈，即线性层]。
- en: 'Advanced note: Just like before, within the decoder block, flasks of different
    words don’t get mixed together. [The feed forward step is position-wise, meaning
    it applies to each word independently.]'
  id: totrans-83
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 高级说明：就像以前一样，在解码器块中，不同词的瓶子不会混合在一起。[前馈步骤是按位置进行的，这意味着它对每个词独立应用。]
- en: This metaphor explains the following equation and diagram from the original
    paper.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 这个比喻解释了原论文中的以下方程式和图表。
- en: '![](../Images/07ae2467242bd67f099056786d4ec0ef.png)![](../Images/3df6411e659d67b76ded2bbb2e636db1.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/07ae2467242bd67f099056786d4ec0ef.png)![](../Images/3df6411e659d67b76ded2bbb2e636db1.png)'
- en: 'Source: [Attention is All You Need](https://arxiv.org/pdf/1706.03762.pdf).'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 来源：[Attention is All You Need](https://arxiv.org/pdf/1706.03762.pdf)。
- en: 'Stacking the Blocks: And … Repeat!'
  id: totrans-87
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 堆叠块：并且……重复！
- en: '![](../Images/f2cc8bbf14bf81e0342760fcf18f476f.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f2cc8bbf14bf81e0342760fcf18f476f.png)'
- en: 'Source: created by the author.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 来源：由作者创建。
- en: 'For better results, we repeat the decoder block N times. For [reference](https://www.cs.ubc.ca/~amuham01/LING530/papers/radford2018improving.pdf),
    GPT repeats 12 times. Intuitively, you want to intersperse the act of collecting
    potions from other relevant words (attention) and synthesizing those potions on
    your own to get meaning out of it (feed forward): collect, synthesize; collect,
    synthesize…'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 为了获得更好的效果，我们将解码器块重复 N 次。对于[参考](https://www.cs.ubc.ca/~amuham01/LING530/papers/radford2018improving.pdf)，GPT
    重复 12 次。直观上，你会希望在收集其他相关词汇的药水（注意力）和自己合成这些药水以获得意义（前馈）之间进行交替：收集，合成；收集，合成……
- en: Now you can know alchemy… I mean…GPT!
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你可以了解炼金术……我的意思是……GPT！
