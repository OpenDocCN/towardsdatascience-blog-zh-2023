- en: 'The Art of Prompt Design: Use Clear Syntax'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/the-art-of-prompt-design-use-clear-syntax-4fc846c1ebd5?source=collection_archive---------0-----------------------#2023-05-02](https://towardsdatascience.com/the-art-of-prompt-design-use-clear-syntax-4fc846c1ebd5?source=collection_archive---------0-----------------------#2023-05-02)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Explore how clear syntax can enable you to communicate intent to language models,
    and also help ensure that outputs are easy to parse
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@scottmlundberg?source=post_page-----4fc846c1ebd5--------------------------------)[![Scott
    Lundberg](../Images/99f1c984f0aaabfe4e348a92fa50a1ee.png)](https://medium.com/@scottmlundberg?source=post_page-----4fc846c1ebd5--------------------------------)[](https://towardsdatascience.com/?source=post_page-----4fc846c1ebd5--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----4fc846c1ebd5--------------------------------)
    [Scott Lundberg](https://medium.com/@scottmlundberg?source=post_page-----4fc846c1ebd5--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F3a739af9ef3a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-art-of-prompt-design-use-clear-syntax-4fc846c1ebd5&user=Scott+Lundberg&userId=3a739af9ef3a&source=post_page-3a739af9ef3a----4fc846c1ebd5---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----4fc846c1ebd5--------------------------------)
    ·9 min read·May 2, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F4fc846c1ebd5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-art-of-prompt-design-use-clear-syntax-4fc846c1ebd5&user=Scott+Lundberg&userId=3a739af9ef3a&source=-----4fc846c1ebd5---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4fc846c1ebd5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-art-of-prompt-design-use-clear-syntax-4fc846c1ebd5&source=-----4fc846c1ebd5---------------------bookmark_footer-----------)![](../Images/acca20e7fd6c31390e54b99cbc5ca055.png)'
  prefs: []
  type: TYPE_NORMAL
- en: All images were generated by Scott and Marco.
  prefs: []
  type: TYPE_NORMAL
- en: This is the first installment of a series on how to use `[guidance](https://github.com/guidance-ai/guidance)`
    to control large language models (LLMs), written jointly with [Marco Tulio Ribeiro](https://medium.com/@marcotcr).
    We’ll start from the basics and work our way up to more advanced topics.
  prefs: []
  type: TYPE_NORMAL
- en: In this post, we’ll show that having **clear syntax** enables you to communicate
    your intent to the LLM, and also ensure that outputs are easy to parse (like JSON
    that is guaranteed to be valid). For the sake of clarity and reproducibility we’ll
    start with an open source Mistral 7B model without fine tuning. Then, we will
    show how the same ideas apply to fine-tuned models like ChatGPT / GPT-4\. All
    the code below is [available in a notebook](https://github.com/guidance-ai/guidance/blob/main/notebooks/art_of_prompt_design/use_clear_syntax.ipynb)
    for you to reproduce if you like.
  prefs: []
  type: TYPE_NORMAL
- en: '**Clear syntax helps with parsing the output**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The first, and most obvious benefit of using clear syntax is that it makes
    it easier to parse the output of the LLM. Even if the LLM is able to generate
    a correct output, it may be difficult to programatically extract the desired information
    from the output. For example, consider the following Guidance prompt (where `gen()`
    is a `guidance` command to generate text from the LLM):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/0548bcc5abcc476471a6c9524df32715.png)'
  prefs: []
  type: TYPE_IMG
- en: Output as it appears in a notebook.
  prefs: []
  type: TYPE_NORMAL
- en: 'While the answer is readable, the output *format* is arbitrary (i.e. we don’t
    know it in advance), and thus hard to parse programatically. For example here
    is another run of a similar prompt where the output format is very different:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/688808521a984ae3605665ffeebaf7d3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Enforcing clear syntax in your prompts can help reduce the problem of arbitrary
    output formats. There are a couple ways you can do this:'
  prefs: []
  type: TYPE_NORMAL
- en: 1\. Giving structure hints to the LLM inside a standard prompt (perhaps even
    using few-shot examples).
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Using `guidance` (or some other package) that enforces a specific output
    format.
  prefs: []
  type: TYPE_NORMAL
- en: These are not mutually exclusive. Let’s see an example of each approach.
  prefs: []
  type: TYPE_NORMAL
- en: '**Traditional prompt with structure hints**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Here is an example of a traditional prompt that uses structure hints to encourage
    the use of a specific output format. The prompt is designed to generate a list
    of 5 items that is easy to parse. Note that in comparison to the previous prompt,
    we have written this prompt in such a way that it has committed the LLM to a specific
    clear syntax (numbers followed by a quoted string). This makes it much easier
    to parse the output after generation.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/fe034d617f6cacdc6e02272c0d8a869c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Note that the LLM follows the syntax correctly, but does not stop after generating
    5 items. We can fix this by creating a clear stopping criteria, e.g. asking for
    6 items and stopping when we see the start of the sixth item (so we end up with
    five):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/a5a63e9d54cf5c5824316e5e74fc7fec.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Enforcing syntax with a guidance program**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Rather than using *hints*, a Guidance program *enforces* a specific output format,
    inserting the tokens that are part of the structure rather than getting the LLM
    to generate them.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, this is what we would do if we wanted to enforce a numbered list
    as a format:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/24ec248c9af9283ac431a453774896cc.png)'
  prefs: []
  type: TYPE_IMG
- en: In the above prompt the `lm2 = lm + …` command saves the new model state that
    results from adding a string to the starting `lm` state into the variable `lm2`.
    The `for` loop then iteratively updates `lm2` by adding a mixture of strings and
    generated sequences. Note that the structure (the numbers, and quotes) are *not*
    generated by the LLM.
  prefs: []
  type: TYPE_NORMAL
- en: 'Output parsing is done automatically by the `guidance` program, so we don’t
    need to worry about it. In this case, the `commands` variable will be the list
    of generated command names:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/3a5fc333af8e650f1669402eaf801e78.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Forcing valid JSON syntax:** Using `guidance` we can create any syntax we
    want with absolute confidence that what we generate will exactly follow the format
    we specify. This is particularly useful for things like JSON:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/c685b4d1950771ec7bca47d112ff533e.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Guidance acceleration:** Another benefit of `guidance` programs is speed
    — incremental generation is actually faster than a single generation of the entire
    list, because the LLM does not have to generate the syntax tokens for the list
    itself, only the actual command names (this makes more of a difference when the
    output structure is richer).'
  prefs: []
  type: TYPE_NORMAL
- en: If you are using a model endpoint that does not support such [acceleration](https://github.com/guidance-ai/guidance/blob/main/notebooks/guidance_acceleration.ipynb)
    (e.g. OpenAI models), then many incremental API calls will slow you down, so `guidance`
    uses a single running stream (see details below when we demo chat models).
  prefs: []
  type: TYPE_NORMAL
- en: '**Clear syntax gives the user more power**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Getting stuck in a low-diversity rut is a common failure mode of LLMs, which
    can happen even if we use a relatively high temperature:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/7d040cda0ddafe6e5afb630705db5aad.png)'
  prefs: []
  type: TYPE_IMG
- en: 'When generating a list of items previous items in the list influence future
    items. This can lead to unhelpful biases or trends in what gets generated. One
    possible fix to this problem is asking for parallel completions (so that prior
    generated commands do not influence the next command generation):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/09878da99e9a21ee5f36152eabbaa0dd.png)'
  prefs: []
  type: TYPE_IMG
- en: We still get some repetition, but much less than before. Also, since clear structure
    gives us outputs that are easy to parse and manipulate, we can easily take the
    output, remove duplicates, and use them in the next step of our program.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is an example program that takes the listed commands, picks one, and does
    further operations on it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/d4cc838a677258ddc73f16a75de1e3d5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We introduced one import control method in the above program: the `regex` parameter
    for generation. The command `gen(''coolness'', regex=''[0–9]+'')` uses a regular
    expression to enforce a certain syntax on the output (i.e. forcing the output
    to match an arbitrary regular experession). In this case we force the coolness
    score to be a whole number (note that generation stops once the model has completed
    generation of the pattern and starts to generate something else).'
  prefs: []
  type: TYPE_NORMAL
- en: '**Combining clear syntax with model-specific structure like chat**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: All the examples above used a base model without any later fine-tuning. But
    if the model you are using has fine tuning, it is important to combine clear syntax
    with the structure that has been tuned into the model.
  prefs: []
  type: TYPE_NORMAL
- en: For example, chat models have been fine tuned to expect several “role” tags
    in the prompt. We can leverage these tags to further enhance the structure of
    our programs/prompts.
  prefs: []
  type: TYPE_NORMAL
- en: The following example adapts the above prompt for use with a chat based model.
    `guidance` has special role tags (like `user()`), which allow you to mark out
    various roles and get them automatically translated into the right special tokens
    or API calls for the LLM you are using. This helps make prompts easier to read
    and makes them more general across different chat models.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/3e1ab452e5003faf896e165d9858da32.png)'
  prefs: []
  type: TYPE_IMG
- en: Output as it appears in a notebook.
  prefs: []
  type: TYPE_NORMAL
- en: '**Using API-restricted models**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When we have control over generation, we can guide the output at any step of
    the process. But some model endpoints (e.g. OpenAI’s ChatGPT) currently have a
    much more limited API, e.g. we can’t control what happens inside each `role` block.
  prefs: []
  type: TYPE_NORMAL
- en: 'While this limits the user’s power, we can still use a subset of syntax hints,
    and enforce the structure outside of the role blocks:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/a93792841ec7832e19c368d6e6d9a79a.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Summary**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Whenever you are building a prompt to control a model it is important to consider
    not only the content of the prompt, but also the `syntax`.
  prefs: []
  type: TYPE_NORMAL
- en: Clear syntax makes it easier to parse the output, helps the LLM produce output
    that matches your intent, and lets you write complex multi-step programs.
  prefs: []
  type: TYPE_NORMAL
- en: While even a trivial example (listing common OS commands) benefits from clear
    syntax, most tasks are much more complex, and benefit even more. We hope this
    post gives you some ideas on how to use clear syntax to improve your prompts.
  prefs: []
  type: TYPE_NORMAL
- en: Also, make sure to check out `[guidance](https://github.com/guidance-ai/guidance)`.
    You certainly don’t need it to write prompts with clear syntax, but we think it
    makes it *much easier* to do so.
  prefs: []
  type: TYPE_NORMAL
