- en: Learn to Unlearn Machines
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/learn-to-unlearn-machines-6b0843dfc40f?source=collection_archive---------5-----------------------#2023-11-23](https://towardsdatascience.com/learn-to-unlearn-machines-6b0843dfc40f?source=collection_archive---------5-----------------------#2023-11-23)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: A data-driven approach to machine unlearning of generative language models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@suxodolskaya?source=post_page-----6b0843dfc40f--------------------------------)[![Evgeniya
    Sukhodolskaya](../Images/2d7cd9aa6b106fefa2ae598a4255ec10.png)](https://medium.com/@suxodolskaya?source=post_page-----6b0843dfc40f--------------------------------)[](https://towardsdatascience.com/?source=post_page-----6b0843dfc40f--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----6b0843dfc40f--------------------------------)
    [Evgeniya Sukhodolskaya](https://medium.com/@suxodolskaya?source=post_page-----6b0843dfc40f--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fab8927d88a52&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flearn-to-unlearn-machines-6b0843dfc40f&user=Evgeniya+Sukhodolskaya&userId=ab8927d88a52&source=post_page-ab8927d88a52----6b0843dfc40f---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----6b0843dfc40f--------------------------------)
    ·7 min read·Nov 23, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F6b0843dfc40f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flearn-to-unlearn-machines-6b0843dfc40f&user=Evgeniya+Sukhodolskaya&userId=ab8927d88a52&source=-----6b0843dfc40f---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F6b0843dfc40f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flearn-to-unlearn-machines-6b0843dfc40f&source=-----6b0843dfc40f---------------------bookmark_footer-----------)![](../Images/54886bda691ab0a30577746d91efa2f7.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Image generated with [DALLE 3](https://chat.openai.com/)
  prefs: []
  type: TYPE_NORMAL
- en: In today’s tech landscape, you’d be hard pressed to find someone who hasn’t
    heard of machine learning. Over the last decade the research field has been so
    trendy that even those outside the industry are now familiar with terms such as
    Artificial Intelligence (AI), Neural Networks (NNs), and Machine Learning (ML).
  prefs: []
  type: TYPE_NORMAL
- en: However, when it comes to *machine unlearning*, it seems the legal industry
    has heard more about it than the tech community. The recent boom in Large Language
    Models (LLMs), which in the fast-paced world of IT feels like a decade even if
    it’s only been 1–2 years, has unearthed hundreds of unresolved ethical and legal
    issues related to AI development. Novelists [are suing](https://www.hollywoodreporter.com/business/business-news/authors-sue-meta-openai-class-action-1235588711/)
    OpenAI for using their texts to train GPT models without consent. Twitter is [abuzz
    with critical comments from](https://twitter.com/mattdeitke/status/1638608472525897728)
    artists who believe their works were used in violation of copyright laws. [Complying
    with “the right to be forgotten” has become extremely challenging.](https://arxiv.org/pdf/2307.03941.pdf)
  prefs: []
  type: TYPE_NORMAL
- en: Much like [AI alignment](/what-is-ai-alignment-2bbbe4633c7f), machine unlearning
    appears to be an overlooked field, given the limited open-sourced solutions available.
    I believe that machine unlearning exploration should be encouraged and popularized,
    especially considering that the current laws and ethical norms surrounding AI
    usage are underdeveloped and severely lack mechanisms for data protection. In
    this article, I would like to suggest some practical improvements to [one of the
    first applied unlearning techniques for generative language models.](https://browse.arxiv.org/pdf/2310.02238.pdf)
  prefs: []
  type: TYPE_NORMAL
- en: Machine Unlearning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The term “machine unlearning” or “machine forgetting” means exactly what it
    sounds like: it includes techniques designed to erase requested information from
    a machine learning model’s “knowledge storage”. However, it’s far from intuitive
    when you need to consider actual methods to achieve this efficiently in terms
    of time, computational resources, and model performance on the “not unlearned”
    data. An obvious solution is to retrain models from scratch using the initial
    dataset while excluding the “forget set” — but this would be an extremely impractical
    approach to deep neural network unlearning.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7f0bb7fabe14e41fa2c8ccdd811a2f6c.png)'
  prefs: []
  type: TYPE_IMG
- en: “Machine Unlearning Framework” from “[Survey of Machine Learning](https://arxiv.org/pdf/2209.02299.pdf)”
  prefs: []
  type: TYPE_NORMAL
- en: 'The core research findings in the field of machine unlearning are concisely
    compiled in “[A Survey of Machine Unlearning](https://arxiv.org/pdf/2209.02299.pdf)”.
    Another article that covers the basics with accessible explanations is “[Machine
    unlearning: The duty of forgetting](/machine-unlearning-the-duty-of-forgetting-3666e5b9f6e5)”.
    While I personally recommend these resources, you can find a multitude of other
    quality research materials on the subject. Yet in terms of practical applications,
    there remains much to be done.'
  prefs: []
  type: TYPE_NORMAL
- en: A promising initiative that might shift this field from theoretical exploration
    to practical application is the [NeurIPS 2023 Machine Unlearning challenge](https://www.kaggle.com/competitions/neurips-2023-machine-unlearning/overview).
    Here, participants compete to create an unlearning algorithm for the ResNet18
    Convolutional Neural Network.
  prefs: []
  type: TYPE_NORMAL
- en: Machine Unlearning of Generative Language Models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Considering the widespread accessibility and promotion of generative language
    models to the vast majority of internet users, there’s a critical need for unlearning
    mechanisms. One of the first successful techniques was not so long ago published
    in an open source; you can find the details in “[Who’s Harry Potter? Approximate
    Unlearning in LLMs](https://browse.arxiv.org/pdf/2310.02238.pdf)” by Ronen Eldan
    and Mark Russinovich.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2540c12c081ec5d6d2265013c3160122.png)'
  prefs: []
  type: TYPE_IMG
- en: Image generated with [StableDiffusion](https://stablediffusionweb.com/#demo)
  prefs: []
  type: TYPE_NORMAL
- en: The authors use a data augmentation approach for machine unlearning on the [Llama
    2 7b chat model](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) released
    this summer by Meta. The chosen unlearning target, also known as the “forget set”,
    is the Harry Potter saga (ingenious, these muggles!), which is a perfect example
    of machine unlearning due to the possible violation of copyright law. They show
    that with just one GPU hour of fine-tuning, the resulting model is unable to recall
    most of the Harry Potter-related content, while its performance on common benchmarks
    remains almost unaffected.
  prefs: []
  type: TYPE_NORMAL
- en: Approach overview
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The main goal of the approach is to make Llama 2 7b forget the linkage between
    entities from a defined forget set (*“*Harry” <is friends with> *“Hermione”*)
    by giving the model plausible generic alternatives (“Harry” <is friends with>
    *“Sally”*). To provide these alternatives as target labels in a **fine-tuning
    dataset**, idiosyncratic terms from the “domain to be forgotten” should be highly
    penalized during the generation of targets. Such penalization could be achieved
    by combining in equation (1) logits generated by a **reinforced model** on the
    original input — Harry Potter books — and by a **baseline model** on a **generic
    translation** of the original input.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/fafe018e676d86fdbf62bed5da44b3ac.png)'
  prefs: []
  type: TYPE_IMG
- en: Equation (1) from “[Who’s Harry Potter? Approximate Unlearning in LLMs](https://browse.arxiv.org/pdf/2310.02238.pdf)”
  prefs: []
  type: TYPE_NORMAL
- en: The **reinforced model** is Llama 2 7b fine-tuned additionally on Harry Potter
    novels. The **baseline model** is untuned Llama 2 7b. To shift the **baseline
    model**’s output distribution away from the Harry Potter theme, the authors replace
    idiosyncratic terms in the original input with generic ones so the model generates
    a next word based on a context unrelated to the Harry Potter saga. To automate
    such replacements, the authors introduce a dictionary of **anchor terms** — terms
    specific to “Harry Potter” — mapped onto **generic translations***.* The dictionary
    is fully gathered by [GPT-4](https://openai.com/research/gpt-4).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3a8e75eb642c69556705cbd281f2de30.png)'
  prefs: []
  type: TYPE_IMG
- en: '{‘Anchor Terms’: ‘Generic translations’} from “[Who’s Harry Potter? Approximate
    Unlearning in LLMs](https://browse.arxiv.org/pdf/2310.02238.pdf)”'
  prefs: []
  type: TYPE_NORMAL
- en: The resulting **fine-tuning dataset** consists of tokenized blocks of text from
    Harry Potter books in a one-to-one mapping to target labels , which are tokens
    corresponding to the maximal entries of the *v_generic* from the equation (1).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0a7c818dcbbe86c7e511a14891c32c9c.png)'
  prefs: []
  type: TYPE_IMG
- en: A piece of the finetuning dataset from “[Who’s Harry Potter? Approximate Unlearning
    in LLMs](https://browse.arxiv.org/pdf/2310.02238.pdf)”
  prefs: []
  type: TYPE_NORMAL
- en: 'To summarize, the authors describe four steps in the unlearning process:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7eaa5a59eda23eecd3516e5f77e8031f.png)'
  prefs: []
  type: TYPE_IMG
- en: Machine Unlearning Algorithm from “[Who’s Harry Potter? Approximate Unlearning
    in LLMs](https://browse.arxiv.org/pdf/2310.02238.pdf)”
  prefs: []
  type: TYPE_NORMAL
- en: 'Leveraging the Approach: Key Challenges'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The results of the data augmentation approach are promising, encouraging further
    application in similar tasks. Yet, the authors left some room for improvement
    in several application stages.
  prefs: []
  type: TYPE_NORMAL
- en: '**Dependency on GPT-4’s existing knowledge:** The algorithm to some extent
    depends on GPT-4’s prior understanding of the Harry Potter series to generate
    generic translations. While the model is expected to have extensive knowledge
    of the Harry Potter realm, a reassessment by fans of the series could provide
    invaluable insights.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Challenges with idiosyncratic terms:** Penalizing all unique terms related
    to the series poses an issue. For instance, replacing every instance of ‘Harry’
    with a common name like ‘John’ disrupts the model’s grasp of natural language,
    leading to sentences like, “Harry went up to him and said, ‘Hi, my name is *John*’.”
    To address this, the authors employ the following strategy:'
  prefs: []
  type: TYPE_NORMAL
- en: Excluding repeated instances of anchored terms from contributing to the loss
    function beyond their initial occurrence.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lowering the likelihood of logits connected to translations of terms that have
    appeared before.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: However, this strategy also affects the model’s general language comprehension.
    A plausible alternative useful for the fine-tuning dataset would be, for example,
    “Harry went up to him and said, ‘Hi, my name is *Harold*’.”
  prefs: []
  type: TYPE_NORMAL
- en: '**Evaluation techniques:** The team utilized GPT-4 for an initial evaluation,
    comprising 300 Harry Potter prompt completions, and further analysis of completions.
    Nonetheless, they acknowledged its limitations in accuracy, opting for manual
    inspections of the results for more thorough verification in their final training.
    The authors have not provided insights on how to set up such a manual inspection.'
  prefs: []
  type: TYPE_NORMAL
- en: Overcoming the Challenges
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A more effective way to address the key challenges would be a hybrid approach
    that combines human insight and Large Language Models (LLMs).
  prefs: []
  type: TYPE_NORMAL
- en: In order to harness the collective strengths of human intuition and large language
    models, I have designed three crowdsourcing project interfaces that facilitate
    collaborative labeling using LLMs and the crowd. Each interface designed for human
    labeling is tailored to a challenge listed above.
  prefs: []
  type: TYPE_NORMAL
- en: '**Dependency on GPT-4’s existing knowledge:**'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8802ee2afad515c63f253e1687081cd9.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: Use the *Named Entity Recognition (NER)* to correct GPT-4 NER choices for a
    dictionary of anchor terms. As input, provide the text and GPT-4’s selection of
    terms (you can ask the model to return positions in the text directly), and instruct
    the crowd to correct and complement the selected entities.
  prefs: []
  type: TYPE_NORMAL
- en: '**Challenges with Idiosyncratic Terms:**'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4bee77ece5822db0885119a02587d7dc.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: With the help of a baseline model, check on linguistic correctness prompts with
    completions done by the baseline model on a generic translation of the original
    input. All examples where the baseline model is unsure of an answer (the probability
    of output tokens is below a certain threshold, chosen by you empirically) should
    be sent to a crowdsourcing project with the interface shown on the image.
  prefs: []
  type: TYPE_NORMAL
- en: '**Evaluation Techniques:**'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/788dbb4ef5f519fe8453843c4ae946e4.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: Manual inspection of the evaluation done by GPT-4 can be designed like in the
    image above.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The authors highlight that, unlike the fictional world of Harry Potter, non-fiction
    areas may not have the same abundance of unique terms, which might make the data
    augmentation approach based on achor terms not applicable. However, if the data
    augmentation techniques outlined in this article fit your project, consider integrating
    the suggested improvements and introducing further your own tweaks. Together,
    we can advance the field of machine unlearning!
  prefs: []
  type: TYPE_NORMAL
