["```py\nmodels:\n- type: main\n\tengine: openai\n\tmodel: text-davinci-003\n```", "```py\nins_assistant\n└── config\n```", "```py\ninstructions:\n  - type: general\n    content: |\n      You are an AI assistant that supports employees at an insurance company's customer support center.\n```", "```py\ndefine user ask off topic\n  \"How's the weather today?\"\n  \"Can you recommend a good restaurant nearby?\"\n  \"What's your opinion on the latest political news?\"\n  \"How do I cook spaghetti?\"\n  \"What are the best tourist attractions in Paris?\"\n\ndefine bot explain cant off topic\n  \"I cannot answer to your question because I'm programmed to assist only with insurance-related questions.\"\n\ndefine flow\n  user ask off topic\n  bot explain cant off topic\n```", "```py\nimport os\nfrom langchain.chat_models import AzureChatOpenAI\nfrom nemoguardrails import LLMRails, RailsConfig\n\n# Reading environment variables\nazure_openai_key = os.environ.get(\"AZURE_OPENAI_KEY\")\nazure_openai_model = os.environ.get(\"AZURE_OPENAI_MODEL\")\nazure_openai_endpoint = os.environ.get(\"AZURE_OPENAI_ENDPOINT\")\n\n# Define LLM and parameters to pass to the guardrails configuration\nchat_model = AzureChatOpenAI(\n    openai_api_type=\"azure\",\n    openai_api_version=\"2023-03-15-preview\",\n    openai_api_key=azure_openai_key,\n    deployment_name=azure_openai_model,\n    openai_api_base=azure_openai_endpoint\n)\n\n# Load configuration\nconfig = RailsConfig.from_path(\"./config\")\n\n# Configuration of LLMs is passed\napp = LLMRails(config=config, llm=chat_model)\n\n# sample user input\nnew_message = app.generate(messages=[{\n    \"role\": \"user\",\n    \"content\": \"What's the latest fashion trend?\"\n}])\n\nprint(f\"new_message: {new_message}\")\n```", "```py\npython cli_chat.py\n```"]