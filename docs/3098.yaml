- en: Neural Basis Models for Interpretability
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/neural-basis-models-for-interpretability-fd04ac958ff2?source=collection_archive---------8-----------------------#2023-10-11](https://towardsdatascience.com/neural-basis-models-for-interpretability-fd04ac958ff2?source=collection_archive---------8-----------------------#2023-10-11)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Unpacking the new interpretable model proposed by Meta AI
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@upadhyan?source=post_page-----fd04ac958ff2--------------------------------)[![Nakul
    Upadhya](../Images/336cb21272e9b1f098177adbde50e92e.png)](https://medium.com/@upadhyan?source=post_page-----fd04ac958ff2--------------------------------)[](https://towardsdatascience.com/?source=post_page-----fd04ac958ff2--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----fd04ac958ff2--------------------------------)
    [Nakul Upadhya](https://medium.com/@upadhyan?source=post_page-----fd04ac958ff2--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F4d9dddc62a80&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fneural-basis-models-for-interpretability-fd04ac958ff2&user=Nakul+Upadhya&userId=4d9dddc62a80&source=post_page-4d9dddc62a80----fd04ac958ff2---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----fd04ac958ff2--------------------------------)
    ·6 min read·Oct 11, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Ffd04ac958ff2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fneural-basis-models-for-interpretability-fd04ac958ff2&user=Nakul+Upadhya&userId=4d9dddc62a80&source=-----fd04ac958ff2---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ffd04ac958ff2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fneural-basis-models-for-interpretability-fd04ac958ff2&source=-----fd04ac958ff2---------------------bookmark_footer-----------)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: The widespread application of Machine Learning and Artificial Intelligence across
    various domains brings about heightened challenges regarding risks and ethical
    assessments. As seen in case studies like the [criminal recidivism model reported
    on by ProPublica,](https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing)
    machine learning algorithms can be incredibly biased and, as a result, robust
    explainability mechanisms are needed to ensure trust and safety when these models
    are deployed in high-stakes areas.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: So, how do we balance interpretability with accuracy and model expressivity?
    Well, Meta AI researchers have proposed a new approach they dubbed [Neural Basis
    Models (NBMs)[1]](https://proceedings.neurips.cc/paper_files/paper/2022/hash/37da88965c016dca016514df0e420c72-Abstract-Conference.html),
    a sub-family of generalized additive models that achieve state-of-the-art (SOTA)
    performance on benchmark datasets while retaining glass-box interpretability.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，我们如何在可解释性与准确性和模型表达能力之间取得平衡呢？好吧，Meta AI 研究人员提出了一种新方法，他们称之为[神经基础模型 (NBMs)[1]](https://proceedings.neurips.cc/paper_files/paper/2022/hash/37da88965c016dca016514df0e420c72-Abstract-Conference.html)，这是广义加性模型的一个子系列，在基准数据集上实现了最先进的
    (SOTA) 性能，同时保留了玻璃箱的可解释性。
- en: In this article, I aim to explain the NBM and what makes it a beneficial model.
    As usual, I encourage everyone to read the original paper.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我旨在解释 NBM 及其成为有益模型的原因。像往常一样，我鼓励大家阅读原始论文。
- en: If you’re interested in interpretable machine learning and other aspects of
    ethical AI, consider checking out some of my other articles and following me!
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你对可解释的机器学习和其他伦理 AI 方面感兴趣，可以查看我的其他文章并关注我！
- en: '![Nakul Upadhya](../Images/e62aa67aa11cd0f9bcd1132257fc3773.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![纳库尔·乌帕德亚](../Images/e62aa67aa11cd0f9bcd1132257fc3773.png)'
- en: '[Nakul Upadhya](https://medium.com/@upadhyan?source=post_page-----fd04ac958ff2--------------------------------)'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '[纳库尔·乌帕德亚](https://medium.com/@upadhyan?source=post_page-----fd04ac958ff2--------------------------------)'
- en: Interpretable and Ethical AI
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 可解释和伦理 AI
- en: '[View list](https://medium.com/@upadhyan/list/interpretable-and-ethical-ai-f6ee1f0b476d?source=post_page-----fd04ac958ff2--------------------------------)5
    stories![](../Images/3718151c0f72303f3d1c71f54229bc98.png)![](../Images/eddb4279ebae7fc1ba79cf6dcc6ebd5a.png)![](../Images/7def8e23dad656929857f2a488b1f547.png)'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '[查看列表](https://medium.com/@upadhyan/list/interpretable-and-ethical-ai-f6ee1f0b476d?source=post_page-----fd04ac958ff2--------------------------------)5个故事![](../Images/3718151c0f72303f3d1c71f54229bc98.png)![](../Images/eddb4279ebae7fc1ba79cf6dcc6ebd5a.png)![](../Images/7def8e23dad656929857f2a488b1f547.png)'
- en: 'Background: GAMs'
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 背景：GAMs
