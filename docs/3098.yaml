- en: Neural Basis Models for Interpretability
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/neural-basis-models-for-interpretability-fd04ac958ff2?source=collection_archive---------8-----------------------#2023-10-11](https://towardsdatascience.com/neural-basis-models-for-interpretability-fd04ac958ff2?source=collection_archive---------8-----------------------#2023-10-11)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Unpacking the new interpretable model proposed by Meta AI
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@upadhyan?source=post_page-----fd04ac958ff2--------------------------------)[![Nakul
    Upadhya](../Images/336cb21272e9b1f098177adbde50e92e.png)](https://medium.com/@upadhyan?source=post_page-----fd04ac958ff2--------------------------------)[](https://towardsdatascience.com/?source=post_page-----fd04ac958ff2--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----fd04ac958ff2--------------------------------)
    [Nakul Upadhya](https://medium.com/@upadhyan?source=post_page-----fd04ac958ff2--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F4d9dddc62a80&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fneural-basis-models-for-interpretability-fd04ac958ff2&user=Nakul+Upadhya&userId=4d9dddc62a80&source=post_page-4d9dddc62a80----fd04ac958ff2---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----fd04ac958ff2--------------------------------)
    ·6 min read·Oct 11, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Ffd04ac958ff2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fneural-basis-models-for-interpretability-fd04ac958ff2&user=Nakul+Upadhya&userId=4d9dddc62a80&source=-----fd04ac958ff2---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ffd04ac958ff2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fneural-basis-models-for-interpretability-fd04ac958ff2&source=-----fd04ac958ff2---------------------bookmark_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: The widespread application of Machine Learning and Artificial Intelligence across
    various domains brings about heightened challenges regarding risks and ethical
    assessments. As seen in case studies like the [criminal recidivism model reported
    on by ProPublica,](https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing)
    machine learning algorithms can be incredibly biased and, as a result, robust
    explainability mechanisms are needed to ensure trust and safety when these models
    are deployed in high-stakes areas.
  prefs: []
  type: TYPE_NORMAL
- en: So, how do we balance interpretability with accuracy and model expressivity?
    Well, Meta AI researchers have proposed a new approach they dubbed [Neural Basis
    Models (NBMs)[1]](https://proceedings.neurips.cc/paper_files/paper/2022/hash/37da88965c016dca016514df0e420c72-Abstract-Conference.html),
    a sub-family of generalized additive models that achieve state-of-the-art (SOTA)
    performance on benchmark datasets while retaining glass-box interpretability.
  prefs: []
  type: TYPE_NORMAL
- en: In this article, I aim to explain the NBM and what makes it a beneficial model.
    As usual, I encourage everyone to read the original paper.
  prefs: []
  type: TYPE_NORMAL
- en: If you’re interested in interpretable machine learning and other aspects of
    ethical AI, consider checking out some of my other articles and following me!
  prefs: []
  type: TYPE_NORMAL
- en: '![Nakul Upadhya](../Images/e62aa67aa11cd0f9bcd1132257fc3773.png)'
  prefs: []
  type: TYPE_IMG
- en: '[Nakul Upadhya](https://medium.com/@upadhyan?source=post_page-----fd04ac958ff2--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: Interpretable and Ethical AI
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[View list](https://medium.com/@upadhyan/list/interpretable-and-ethical-ai-f6ee1f0b476d?source=post_page-----fd04ac958ff2--------------------------------)5
    stories![](../Images/3718151c0f72303f3d1c71f54229bc98.png)![](../Images/eddb4279ebae7fc1ba79cf6dcc6ebd5a.png)![](../Images/7def8e23dad656929857f2a488b1f547.png)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Background: GAMs'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
