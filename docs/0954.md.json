["```py\ntimesteps = tf.range(1, 1000, 1000 // num_steps)\nalphas, alphas_prev = self._get_initial_alphas(timesteps)\nprogbar = keras.utils.Progbar(len(timesteps))\niteration = 0\nfor index, timestep in list(enumerate(timesteps))[::-1]:\n    latent_prev = latent  # Set aside the previous latent vector\n    t_emb = self._get_timestep_embedding(timestep, batch_size)\n    unconditional_latent = self.diffusion_model.predict_on_batch(\n        [latent, t_emb, unconditional_context]\n    )\n    latent = self.diffusion_model.predict_on_batch(\n        [latent, t_emb, context]\n    )\n    latent = unconditional_latent + unconditional_guidance_scale * (\n        latent - unconditional_latent\n    )\n    a_t, a_prev = alphas[index], alphas_prev[index]\n    pred_x0 = (latent_prev - math.sqrt(1 - a_t) * latent) / math.sqrt(\n        a_t\n    )\n    latent = (\n        latent * math.sqrt(1.0 - a_prev) + math.sqrt(a_prev) * pred_x0\n    )\n    iteration += 1\n    progbar.update(iteration)\n```", "```py\nlatent = self.diffusion_reverse_loop(\n    latent,\n    context=context, \n    unconditional_context=unconditional_context, \n    batch_size=batch_size, \n    unconditional_guidance_scale=unconditional_guidance_scale,\n    num_steps=num_steps,\n)\n```", "```py\n@tf.function\ndef diffusion_reverse_loop(self, latent, context, unconditional_context, batch_size, unconditional_guidance_scale, num_steps):\n\n  index = num_steps -1\n  cond = tf.math.greater(index, -1)\n  timesteps = tf.range(1, 1000, 1000 // num_steps)\n  alphas, alphas_prev = self._get_initial_alphas(timesteps)\n  iter_partial_fn = functools.partial(\n      self._diffusion_reverse_iter, \n      timesteps=timesteps, \n      alphas=alphas, \n      alphas_prev=alphas_prev, \n      context=context, \n      unconditional_context=unconditional_context, \n      batch_size=batch_size, \n      unconditional_guidance_scale=unconditional_guidance_scale, \n      num_steps=num_steps\n  )\n\n  latent, index = tf.while_loop(cond=lambda _, i: tf.math.greater(i, -1), body=iter_partial_fn, loop_vars=[latent, index])\n\n  return latent \n\n@tf.function\ndef _diffusion_reverse_iter(self, latent_prev, index, timesteps,  alphas, alphas_prev, context, unconditional_context, batch_size, unconditional_guidance_scale, num_steps):\n\n  t_emb = self._get_timestep_embedding(timesteps[index], batch_size)\n\n  combined_latent = self.diffusion_model(\n            [\n                tf.concat([latent_prev, latent_prev],axis=0), \n                tf.concat([t_emb, t_emb], axis=0), \n                tf.concat([context, unconditional_context], axis=0)\n            ], training=False\n        )\n  latent, unconditional_latent = tf.split(combined_latent, 2, axis=0)\n  latent = unconditional_latent + unconditional_guidance_scale * (\n        latent - unconditional_latent\n  )\n  a_t, a_prev = alphas[index], alphas_prev[index]\n  pred_x0 = (latent_prev - tf.math.sqrt(1 - a_t) * latent) / tf.math.sqrt(a_t)\n  latent = latent * tf.math.sqrt(1.0 - a_prev) + tf.math.sqrt(a_prev) * pred_x0\n  index -= 1\n\n  return latent, index\n```", "```py\nclass StableDiffusionTFModel(tf.keras.models.Model):\n\n  def __init__(self):\n    super().__init__()\n    self.image_width = self.image_height = 384\n    self.model = StableDiffusionNoTokenizer(img_width=self.image_width, img_height=self.image_height, encoded_text_length=None, jit_compile=True)\n    # This forces the model download its components\n    # self.image_encoder is only required for in-painting - we will ignore this functionality in this excercise\n    self.text_encoder = self.model.text_encoder\n    self.diffusion_model = self.model.diffusion_model\n    self.decoder = self.model.decoder\n\n    self.default_num_steps = tf.constant(40) \n    self.default_batch_size = tf.constant(2)\n\n    # These negative prompt tokens are borrowed from the original stable diffusion model\n    self.default_negative_prompt_tokens = tf.constant(\n        [\n            49406, 8159, 267, 83, 3299, 267, 21101, 8893, 3500, 267, 21101, \n            8893, 4804, 267, 21101, 8893, 1710, 267, 620, 539, 6481, 267, \n            38626, 267, 12598, 943, 267, 4231, 34886, 267, 4231, 7072, 267, \n            4231, 5706, 267, 1518, 15630, 267, 561, 6528, 267, 3417, 268, \n            3272, 267, 1774, 620, 539, 6481, 267, 21977, 267, 2103, 794, \n            267, 2103, 15376, 267, 38013, 267, 4160, 267, 2505, 2110, 267, \n            782, 23257, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407\n        ], dtype=tf.int32\n    )\n\n  def call(self, inputs):\n\n    encoded_text = self.text_encoder([inputs[\"tokens\"], self.model._get_pos_ids()], training=False)\n\n    images = self.model.generate_image(\n        encoded_text, \n        negative_prompt_tokens=inputs.get(\"negative_prompt_tokens\", self.default_negative_prompt_tokens),\n        num_steps=inputs.get(\"num_steps\", self.default_num_steps), \n        batch_size=inputs.get(\"batch_size\", self.default_batch_size)\n    )\n    return images\n\nmodel = StableDiffusionTFModel()\n```", "```py\n# Tokenizing the prompts\ntokenizer = SimpleTokenizer()\n\ndef generate_tokens(tokenizer, prompt, MAX_PROMPT_LENGTH):\n\n  inputs = tokenizer.encode(prompt)\n  if len(inputs) > MAX_PROMPT_LENGTH:\n      raise ValueError(\n          f\"Prompt is too long (should be <= {MAX_PROMPT_LENGTH} tokens)\"\n      )\n  phrase = tf.concat([inputs, ([49407] * (MAX_PROMPT_LENGTH - len(inputs)))], axis=0)\n  return phrase\n\ntokens = generate_tokens(tokenizer, \"a ferrari car with wings\", MAX_PROMPT_LENGTH)\n\n# Invoking the model\nall_images = []\nnum_steps = 30\ntokens = generate_tokens(tokenizer, \"a castle in Norway overlooking a glacier, landscape, surrounded by fairies fighting trolls, sunset, high quality\", MAX_PROMPT_LENGTH)\nneg_tokens = generate_tokens(tokenizer, \"ugly, tiling, poorly drawn hands, poorly drawn feet, poorly drawn face, out of frame, mutation, mutated, extra limbs, extra legs, extra arms, disfigured, deformed, cross-eye, body out of frame, blurry, bad art, bad anatomy, blurred, text, watermark, grainy\", MAX_PROMPT_LENGTH)\nimages = model({\n    \"tokens\": tokens, \n    \"negative_prompt_tokens\": neg_tokens,\n    \"num_steps\": tf.constant(num_steps), \n    \"batch_size\": tf.constant(1)\n})\n```", "```py\n!gsutil -m cp -r  ./stable_diffusion_model gs://<project>-bucket/\n```", "```py\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: stable-diffusion\n  template:\n    metadata:\n      labels:\n        app: stable-diffusion\n    spec:\n      containers:\n      - name: tf-serving\n        image: \"tensorflow/serving:2.11.0\"\n        args:\n        - \"--model_name=$(MODEL_NAME)\"\n        - \"--model_base_path=$(MODEL_PATH)\"\n        - \"--rest_api_timeout_in_ms=720000\"\n        envFrom:\n        - configMapRef:\n            name: tfserving-configs\n        imagePullPolicy: IfNotPresent\n        readinessProbe:\n          httpGet:\n            path: \"/v1/models/stable-diffusion\"\n            port: 8501\n            scheme: HTTP\n          initialDelaySeconds: 30\n          periodSeconds: 15\n          failureThreshold: 10\n        ports:\n        - name: http\n          containerPort: 8501\n          protocol: TCP\n        - name: grpc\n          containerPort: 8500\n          protocol: TCP\n        resources:\n          requests:\n            cpu: \"3\"\n            memory: \"12Gi\"\n```", "```py\nmetadata:\n  name: stable-diffusion\n  namespace: default\n  labels:\n    app: stable-diffusion\nspec:\n  type: LoadBalancer\n  ports:\n  - port: 8500\n    protocol: TCP\n    name: tf-serving-grpc\n  - port: 8501\n    protocol: TCP\n    name: tf-serving-http\n  selector:\n    app: stable-diffusion\n```", "```py\nkubectl autoscale deployment stable-diffusion --cpu-percent=60 --min=1 --max=2\n```", "```py\n gcloud container clusters get-credentials sd-cluster --zone us-central1-c && \\\nkubectl apply -f tf-serving/configmap.yaml && \\\nkubectl apply -f tf-serving/deployment.yaml && \\\nkubectl autoscale deployment stable-diffusion --cpu-percent=60 --min=1 --max=2 && \\\nkubectl apply -f tf-serving/service.yaml\n```", "```py\ndef predict_rest(json_data, url):\n    json_response = requests.post(url, data=json_data)\n    response = json.loads(json_response.text)\n    if \"predictions\" not in response:\n      print(response)\n    rest_outputs = np.array(response[\"predictions\"])\n    return rest_outputs\n\nurl = f\"http://{stable_diffusion_service_ip}:8501/v1/models/stable-diffusion:predict\"\n\ntokens_list = [\n    generate_tokens(tokenizer, \"A wine glass made from lego bricks, rainbow colored liquid being poured into it, hyper realistic, high detail\", MAX_PROMPT_LENGTH).numpy().tolist(),\n    generate_tokens(tokenizer, \"A staircase made from color pencils, hyper realistic, high detail\", MAX_PROMPT_LENGTH).numpy().tolist(),\n    generate_tokens(tokenizer, \"A ferrari car in the space astronaut driving it, futuristic, hyper realistic, high detail\", MAX_PROMPT_LENGTH).numpy().tolist(),\n    generate_tokens(tokenizer, \"a dragon covered with weapons fighting an army, fire, explosions, hyper realistic, high detail\", MAX_PROMPT_LENGTH).numpy().tolist(),\n    generate_tokens(tokenizer, \"A sawing girl in a boat, hyper realistic, high detail\", MAX_PROMPT_LENGTH).numpy().tolist(),\n\n]\nnegative_tokens = generate_tokens(tokenizer, \"ugly, tiling, poorly drawn hands, poorly drawn feet, poorly drawn face, out of frame, mutation, mutated, extra limbs, extra legs, extra arms, disfigured, deformed, cross-eye, body out of frame, blurry, bad art, bad anatomy, blurred, text, watermark, grainy\", MAX_PROMPT_LENGTH).numpy().tolist()\n\nall_images = []\nall_data = []\nfor tokens, negative_tokens in zip(tokens_list, [negative_tokens for _ in range(5)]):\n    all_data.append(generate_json_data(tokens, negative_tokens))\n\nall_images = [predict_rest(data, url) for data in all_data]\n```", "```py\ndef predict_rest(input_data, url):\n    json_data, sleep_time = input_data[\"data\"], input_data[\"sleep_time\"]\n\n    # We add a delay to simulate real world user requests\n    time.sleep(sleep_time)\n    print(\"Making a request\")\n    t1 = time.perf_counter()\n    json_response = requests.post(url, data=json_data)\n    response = json.loads(json_response.text)\n    result = np.array([])\n    try: \n        result = np.array(response[\"predictions\"])\n    except KeyError:\n        print(f\"Couldn't complete the request {response}\")\n    finally:\n        t2 = time.perf_counter() \n        print(f\"It took {t2-t1}s to complete a single request\")\n        return result\n\nt1 = time.perf_counter()\n\nwith concurrent.futures.ThreadPoolExecutor(max_workers=3) as executor:\n    all_images_gen = executor.map(\n        functools.partial(predict_rest, url=url), \n        [{\"data\": data, \"sleep_time\": min(i*20, 60)} for i, data in enumerate(all_data)]\n    )\n    all_images = [img for img in all_images_gen]\n\nt2 = time.perf_counter()    \nprint(f\"It took {t2-t1}s to complete {n_requests} requests\")\n```", "```py\nkubectl exec --stdin --tty <container name> -- /bin/bash\n```", "```py\ntail -n 10  /proc/7/fd/2\n```"]