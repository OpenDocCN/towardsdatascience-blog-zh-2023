- en: The Surprising Behavior of Data in Higher Dimensions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/the-surprising-behavior-of-data-in-higher-dimensions-1c49bca9bbee?source=collection_archive---------1-----------------------#2023-12-15](https://towardsdatascience.com/the-surprising-behavior-of-data-in-higher-dimensions-1c49bca9bbee?source=collection_archive---------1-----------------------#2023-12-15)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'A journey into the surprising world of high-dimensional data: the blessings
    and the challenges'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@salih.salih?source=post_page-----1c49bca9bbee--------------------------------)[![Salih
    Salih](../Images/220f3c5363989d94c5593eca7ff72c67.png)](https://medium.com/@salih.salih?source=post_page-----1c49bca9bbee--------------------------------)[](https://towardsdatascience.com/?source=post_page-----1c49bca9bbee--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----1c49bca9bbee--------------------------------)
    [Salih Salih](https://medium.com/@salih.salih?source=post_page-----1c49bca9bbee--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F2037cbb08e24&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-surprising-behavior-of-data-in-higher-dimensions-1c49bca9bbee&user=Salih+Salih&userId=2037cbb08e24&source=post_page-2037cbb08e24----1c49bca9bbee---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----1c49bca9bbee--------------------------------)
    ·9 min read·Dec 15, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F1c49bca9bbee&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-surprising-behavior-of-data-in-higher-dimensions-1c49bca9bbee&user=Salih+Salih&userId=2037cbb08e24&source=-----1c49bca9bbee---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F1c49bca9bbee&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-surprising-behavior-of-data-in-higher-dimensions-1c49bca9bbee&source=-----1c49bca9bbee---------------------bookmark_footer-----------)![](../Images/840a964ed305fca552069803b26a6992.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Photo by Guillermo Ferla on Unsplash
  prefs: []
  type: TYPE_NORMAL
- en: Richard Feynman, the renowned physicist, once said, “I can safely say that nobody
    understands quantum mechanics.” In his interview titled “[Fun to Imagine with
    Richard Feynman](https://youtu.be/P1ww1IXRfTA)”, he touched on the strange behavior
    of things at the atomic and subatomic particles level, noting how they often defy
    our common sense. Interestingly, we can notice a similar behavior at the level
    of higher-dimensional data. It’s not exactly like quantum mechanics, but there’s
    a similar element of surprise and beauty — mixed with a few/a lot of challenges
    — when we transition from lower to higher dimensions.
  prefs: []
  type: TYPE_NORMAL
- en: In this and future articles, I want to provide some insights into this fascinating
    topic. My goal is to pique the interest and encourage learning about the world
    of higher-dimensional data, especially by those who are unfamiliar with it.
  prefs: []
  type: TYPE_NORMAL
- en: High-dimensional data, or data in higher dimensions, in the context of data
    analysis and ML, generally refers to datasets that have a large number of variables,
    features, or attributes. Each of those represents a different “dimension” in our
    data.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: To begin, let’s examine some basic examples that highlight the distinctions
    that arise when we go from lower-dimensional spaces to higher-dimensional ones.
  prefs: []
  type: TYPE_NORMAL
- en: '**Volume Concentration in High-Dimensional Spaces**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: First, let’s explore the concept of volume concentration in high-dimensional
    spaces. Consider generating random points within a hypercube whose sides range
    from 0 to 1\. How likely is it that these points fall in the middle region of
    this hypercube as its dimensions increase?
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ac35323659769ce4dfd110eb6a1ead96.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: In the image above, let’s assume x is a small value, such as 0.1\. We aim to
    determine how the probability of a point randomly falling in this middle region
    (not on the edge) varies with increasing dimensions.
  prefs: []
  type: TYPE_NORMAL
- en: One-Dimensional Space (Line)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Think of a line segment from 0 to 1\. The middle part is between 0.1 and 0.9,
    The chance of a random point landing here is simply the length of this middle
    segment over the total length, which is 0.8.
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Two-Dimensional Space (Square)
  prefs: []
  type: TYPE_NORMAL
- en: Now, envision a square where each side ranges from 0 to 1\. The middle region
    is a smaller square with each side from 0.1 to 0.9\. The probability calculation
    involves comparing the area of this smaller square to the total area, giving us
    a probability of 0.64.
  prefs: []
  type: TYPE_NORMAL
- en: 3\. Three-Dimensional Space (Cube)
  prefs: []
  type: TYPE_NORMAL
- en: For a cube with each edge measuring 1, the middle region is a smaller cube with
    each edge from 0.1 to 0.9\. Here, the probability is the volume of this smaller
    cube divided by the total volume, resulting in 0.512.
  prefs: []
  type: TYPE_NORMAL
- en: 4\. Higher Dimensions (Hypercube)
  prefs: []
  type: TYPE_NORMAL
- en: In a hypercube of n dimensions, the ‘volume’ of the middle region shrinks drastically
    as dimensions increase. For instance, in 4D, the probability is 0.4096; in 5D,
    it’s 0.32768; and in 10D, it drops to approximately 0.10737.
  prefs: []
  type: TYPE_NORMAL
- en: The generalization of this idea starts with considering the edge to be of a
    small distance x, as shown in the image above. For a line, the probability of
    a point falling in the middle region is 1–2x. For a square, it’s (1–2x)*(1–2x),
    as a point must fall in the middle of both dimensions.
  prefs: []
  type: TYPE_NORMAL
- en: This pattern continues in n dimensions, where the probability of falling in
    the middle region is (1–2x)^n, becoming very small in higher dimensions.
  prefs: []
  type: TYPE_NORMAL
- en: Note that here, we simplify by considering the length of each side as 1.
  prefs: []
  type: TYPE_NORMAL
- en: Inscribing a Hypersphere within a Hypercube
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To further illustrate the concept of volume concentration, I performed a simple
    simulation using python, where we inscribe a hypersphere within a hypercube, and
    then compare the ratio of the volume of the hypersphere to the hypercube as the
    dimensions increase.
  prefs: []
  type: TYPE_NORMAL
- en: What’s a Hypercube Anyway?
  prefs: []
  type: TYPE_NORMAL
- en: Picture a square. Now, puff it out into a cube. That’s the jump from 2D to 3D.
    Now, take a leap of imagination into the fourth dimension and beyond — that’s
    where hypercubes come in. A hypercube is essentially a cube extended into higher
    dimensions. It’s a shape with equal sides, and in our simulation, we’re considering
    hypercubes with side lengths of 2\. The formula for its volume? just 2^*n(2 to
    the power n)* for an n-dimensional hypercube.
  prefs: []
  type: TYPE_NORMAL
- en: And a Hypersphere?
  prefs: []
  type: TYPE_NORMAL
- en: 'A hypersphere, the higher-dimensional equivalent of a sphere, emerges when
    you extend a 2D circle into 3D (forming a sphere) and then continue into higher
    dimensions. The catch? Its volume isn’t as straightforward to calculate. It involves
    pi (yes, the famous 3.14159…) and the gamma function, which is like a factorial
    on steroids. In a nutshell, the volume of a hypersphere with a radius of 1 in
    an `n`-dimensional space is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/89f7d0941b221726d702e3860120668a.png)'
  prefs: []
  type: TYPE_IMG
- en: The Gamma function Γ(*n*) extends the factorial function to real and complex
    numbers. For positive integers *n*, Γ(n)=(n−1)!, and for non-integer values, it
    is computed numerically.
  prefs: []
  type: TYPE_NORMAL
- en: 'To calculate this ratio using python we can use the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the above code is the following graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9f47efd1d6a45ed1a5ddf11a16155e22.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: We can clearly see that as we go to higher dimensions the ratio decreases rapidly,
    leaving the most volume concentrated in the corners of the hypercube.
  prefs: []
  type: TYPE_NORMAL
- en: These examples demonstrate that in higher dimensions, the volume of the middle
    region becomes a progressively smaller fraction of the total volume, highlighting
    the counterintuitive nature of high-dimensional spaces.
  prefs: []
  type: TYPE_NORMAL
- en: 'Q: What are some of the implications of this volume concentration phenomenon
    on the performance of ML algorithms?'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**The Paper and DVD Experiment**'
  prefs: []
  type: TYPE_NORMAL
- en: Consider the experiment where you try to fit a DVD through a paper piece with
    a square hole. Initially, it seems impossible as the square’s diagonal is smaller
    than the DVD’s diameter. However, folding the paper allows the DVD to pass through.
  prefs: []
  type: TYPE_NORMAL
- en: The folding of the paper, a small yet effective adjustment of spatial dimensions,
    holds the key to the puzzle. An intriguing analogy for comprehending the complexity
    of higher-dimensional landscapes can be found in this experiment.
  prefs: []
  type: TYPE_NORMAL
- en: When the paper is first laid out, it forms a two-dimensional plane. The square
    slot seems too narrow to let the DVD through because of its set dimensions.
  prefs: []
  type: TYPE_NORMAL
- en: This hypothetical situation is consistent with our daily experiences in a three-dimensional
    environment, where length, width, and height are the units of measurement for
    size and distance. But the instant we begin to fold the paper, we add another
    dimension. The hole and the DVD’s spatial connection are completely altered by
    this folding action.
  prefs: []
  type: TYPE_NORMAL
- en: In this new three-dimensional setting, the concept of distance, which was so
    inflexible and clear-cut in two dimensions, becomes more flexible and less intuitive.
    The paper is folded, which effectively modifies the angles generated by the paper’s
    edges and the distances between points surrounding the hole.
  prefs: []
  type: TYPE_NORMAL
- en: The hole in this new three-dimensional form can fit the DVD, demonstrating how
    the inclusion of a third dimension can make an apparently hopeless task in a two-dimensional
    space achievable.
  prefs: []
  type: TYPE_NORMAL
- en: The mathematics underlying this experiment is explained in full in an intriguing
    study by Weiwei Lin et al.
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://ms.copernicus.org/articles/12/933/2021/?source=post_page-----1c49bca9bbee--------------------------------)
    [## A toy-inspired kirigami pattern and its kinematic performance by applying
    mechanisms and machine…'
  prefs: []
  type: TYPE_NORMAL
- en: Abstract. Origami that can form various shapes by setting simple creases on
    the paper and folding along these creases…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: ms.copernicus.org](https://ms.copernicus.org/articles/12/933/2021/?source=post_page-----1c49bca9bbee--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: 'You can also watch this beautiful video by “The Action Lab” that demonstrates
    the idea intuitively:'
  prefs: []
  type: TYPE_NORMAL
- en: A round Disk Through a smaller Square Hole
  prefs: []
  type: TYPE_NORMAL
- en: This shift in perspective has significant implications, especially in the fields
    of mathematics, physics, and machine learning. This idea is reflected in machine
    learning methods like Support Vector Machines (SVMs).
  prefs: []
  type: TYPE_NORMAL
- en: '**SVM and the Kernel Trick**'
  prefs: []
  type: TYPE_NORMAL
- en: The kernel trick in Support Vector Machines (SVMs) shows a similar idea. In
    SVMs, we often encounter data that isn’t linearly separable. The kernel trick
    overcomes this by transforming the data into a higher-dimensional space, akin
    to how folding the paper changed its spatial properties. (In reality, SVMs don’t
    actually transform data into higher dimensions, as this is computationally expensive.
    Instead, they compute relationships between data points as if they were in a higher
    dimension using the kernel trick).
  prefs: []
  type: TYPE_NORMAL
- en: In simpler terms, SVMs normally find a separating line (or hyperplane) in lower
    dimensions. But with non-linear data, this isn’t possible. The kernel trick, like
    folding the paper, adds dimensions, making it easier to find a hyperplane that
    does the job.
  prefs: []
  type: TYPE_NORMAL
- en: The kernel trick doesn’t just shift dimensions; it also simplifies complex problems.
    It’s really a great example of how higher-dimensional thinking can provide solutions
    to problems that seem impossible in lower dimensions.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The result of the above code is the following graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/02f118496f56adc20796162d6dad12b6.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: It’s clear that the data, initially non-linearly separable on the left, becomes
    separable in 2D. This transformation, as shown in the graph to the right, effectively
    solves our problem. Isn’t this amazing?
  prefs: []
  type: TYPE_NORMAL
- en: '**Wrapping Up**'
  prefs: []
  type: TYPE_NORMAL
- en: In this article, we have explored some ideas about the world of high-dimensional
    data. We have shown how entering higher dimensions can greatly alter our viewpoints
    and methods of approaching problems, starting with volume concentration and continuing
    with the real-life example of the paper and DVD experiment and, lastly, the kernel
    trick in SVMs.
  prefs: []
  type: TYPE_NORMAL
- en: In the upcoming article, we will discuss the “curse of dimensionality,” which
    refers to the difficulties and complications involved in navigating high-dimensional
    spaces. We’ll examine how this impacts machine learning and data analysis, as
    well as strategies for mitigating its effects.
  prefs: []
  type: TYPE_NORMAL
- en: Thank you for making it this far! I really appreciate your time in reading this,
    and I hope you found the topic engaging. Please feel free to share any suggestions
    or possible edits for future articles!
  prefs: []
  type: TYPE_NORMAL
