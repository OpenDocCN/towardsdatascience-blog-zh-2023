["```py\nsk-4f3a9b8e7c4f4c8f8f3a9b8e7c4f4c8f-UsH4C3vE64\n```", "```py\n# Install OpenAI & tiktoken packages to use the embeddings model as well as the chat completion model\n!pip install openai tiktoken\n# Install the langchain package to facilitate a most of the functionality in our solution, from processing documents to enabling \"chat\" using LLM\n!pip install langchain\n# Install ChromaDB - an in-memory vector database package - to save the \"knowledge\" relied on by our solution to answer questions\n!pip install chromadb\n# Install HTML to text package to transform webpage content to a more human readable format\n!pip install html2text\n# Install gradio to create a basic UI for our solution\n!pip install gradio\n```", "```py\n# Import packages needed to enable different functionality for the solution\nfrom langchain.document_loaders import AsyncHtmlLoader # To load website content into a document\nfrom langchain.text_splitter import MarkdownHeaderTextSplitter # To document into smaller chunks by document headings \nfrom langchain.document_transformers import Html2TextTransformer # To converrt HTML to Markdown text\nfrom langchain.chat_models import ChatOpenAI # To use OpenAI's LLM\nfrom langchain.prompts import PromptTemplate # To formulate instructions / prompts\nfrom langchain.chains import RetrievalQA, ConversationalRetrievalChain # For RAG\nfrom langchain.memory import ConversationTokenBufferMemory # To maintain chat history\nfrom langchain.embeddings.openai import OpenAIEmbeddings # To convert text to numerical representation\nfrom langchain.vectorstores import Chroma # To interact with vector database\nimport pandas as pd, gradio as gr # To show data as tables, and to build UI respectively\nimport chromadb, json, textwrap # Vector database, converting json to text, and prettify printing respectively\nfrom chromadb.utils import embedding_functions # Setting up embedding function, following protocol required by Chroma\n```", "```py\n# Add your OpenAI API Key to a variable\n# Saving the key in a variable like so is bad practice. It should be loaded into environment variables and loaded from there, but this is okay for a quick demo\nOPENAI_API_KEY='sk-4f3a9b8e7c4f4c8f8f3a9b8e7c4f4c8f-UsH4C3vE64' # Fake Key - use your own real key here\n```", "```py\nurl = \"https://ninadsohoni.github.io/booklist/\" # Feel free to use any other website here, but note that some code might need to be edited to display contents properly\n\n# Load HTML from URL and transform to a more readable text format\ndocs = Html2TextTransformer().transform_documents(AsyncHtmlLoader(url).load())\n\n# Let's take a quick peek again to see what do we have now\nprint(\"\\n\\nIncluded metadata:\\n\", textwrap.fill(json.dumps(docs[0].metadata), width=100), \"\\n\\n\")\nprint(\"Page content loaded:\")\nprint('...', textwrap.fill(docs[0].page_content[2500:3000], width=100, replace_whitespace=False), '...')\n```", "```py\n# Now we split the entire content of the website into smaller chunks\n# Each book review will get its own chunk since we are splitting by headings\n# The LangChain splitter used here will also create a set of metadata from headings and associate it with the text in each chunk\nheaders_to_split_on = [ (\"#\", \"Header 1\"), (\"##\", \"Header 2\"),\n    (\"###\", \"Header 3\"), (\"####\", \"Header 4\"), (\"#####\", \"Header 5\") ]\nsplitter = MarkdownHeaderTextSplitter(headers_to_split_on = headers_to_split_on)\nchunks = splitter.split_text(docs[0].page_content)\n\nprint(f\"{len(chunks)} smaller chunks generated from original document\")\n\n# Let's look at one of the chunks\nprint(\"\\nLooking at a sample chunk:\")\nprint(\"Included metadata:\\n\", textwrap.fill(json.dumps(chunks[5].metadata), width=100), \"\\n\\n\")\nprint(\"Page content loaded:\")\nprint(textwrap.fill(chunks[5].page_content[:500], width=100, drop_whitespace=False), '...')\n```", "```py\n# We will get embeddings for each chunk (and subsequently questions) using an embeddings model from OpenAI\nopenai_embedding_func = embedding_functions.OpenAIEmbeddingFunction(api_key=OPENAI_API_KEY)\n\n# Initialize vector DB and create a collection\npersistent_chroma_client = chromadb.PersistentClient()\ncollection = persistent_chroma_client.get_or_create_collection(\"my_rag_demo_collection\", embedding_function=openai_embedding_func)\ncur_max_id = collection.count() # To not overwrite existing data\n\n# Let's add data to our collection in the vector DB\ncollection.add(\n    ids=[str(t) for t in range(cur_max_id+1, cur_max_id+len(chunks)+1)],\n    documents=[t.page_content for t in chunks],\n    metadatas=[None if len(t.metadata) == 0 else t.metadata for t in chunks]\n    )\n\nprint(f\"{collection.count()} documents in vector DB\")\n#  25 documents in vector DB\n\n# Optional: We will write a scrappy helper function to print data slightly better -\n#  it limits the length embeddings shown on the screen (since these are over a 1,000 long numbers).\n#  Also shows a subset of text from documents as well as metadatas fields\ndef render_vectorDB_content(chromadb_collection):\n    vectordb_data = pd.DataFrame(chromadb_collection.get(include=[\"embeddings\", \"metadatas\", \"documents\"]))\n    return pd.DataFrame({'IDs': [str(t) if len(str(t)) <= 10 else str(t)[:10] + '...'for t in vectordb_data.ids],\n                         'Embeddings': [str(t)[:27] + '...' for t in vectordb_data.embeddings],\n                         'Documents': [str(t) if len(str(t)) <= 300 else str(t)[:300] + '...' for t in vectordb_data.documents],\n                         'Metadatas': ['' if not t else json.dumps(t) if len(json.dumps(t))  <= 90 else '...' + json.dumps(t)[-90:] for t in vectordb_data.metadatas]\n                        })\n\n# Let's take a look at what is in the vector DB using our helper function. We will look at the first 4 chunks\nrender_vectorDB_content(collection)[:4]\n```", "```py\n# Here we are linking to the previously created an instance of the ChromaDB database using a LangChain ChromaDB client\nvectordb = Chroma(client=persistent_chroma_client, collection_name=\"my_rag_demo_collection\", \n                  embedding_function=OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)\n                  )\n\n# Optional - We will define another scrappy helper function to print data slightly better\ndef render_source_documents(source_documents):\n    return pd.DataFrame({'#': range(1, len(source_documents) + 1), \n                         'Documents': [t.page_content if len(t.page_content) <= 300 else t.page_content[:300] + '...' for t in source_documents],\n                         'Metadatas': ['' if not t else '...' + json.dumps(t.metadata, indent=2)[-88:] for t in source_documents]\n                         })\n\n# Here's where we are compiling the question\nquestion = \"Can you recommend a few detective novels?\"\n\n# Running the search against the vector DB based on our question\nrelevant_chunks = vectordb.similarity_search(question)\n\n# Printing results\nprint(f\"Top {len(relevant_chunks)} search results\")\nrender_source_documents(relevant_chunks)\n```", "```py\n# Let's try filtering and accessing records that match a specific metadata filter. This can probably be transformed into a pre-filter if needed\npd.DataFrame(vectordb.get(where = {'Header 2': 'Finance'}))\n```", "```py\n# Let's select the language model behind free version of ChatGPT: GPT-3.5-turbo\nllm = ChatOpenAI(model_name = 'gpt-3.5-turbo', temperature = 0, openai_api_key = OPENAI_API_KEY)\n\n# Let's build a prompt. This is what actually gets sent to the ChatGPT LLM, with the context from our vector database and question injected into the prompt\ntemplate = \"\"\"Use the following pieces of context to answer the question at the end. \nIf you don't know the answer, just say that the available information is not sufficient to answer the question. \nDon't try to make up an answer. Keep the answer as concise as possible, limited to five sentences.\n{context}\nQuestion: {question}\nHelpful Answer:\"\"\"\nQA_CHAIN_PROMPT = PromptTemplate.from_template(template)\n\n# Define a Retrieval QA chain, which will take the question, get the relevant context from the vectorDB, and pass both to the language model for a response\nqa_chain = RetrievalQA.from_chain_type(llm, \n                                       retriever=vectordb.as_retriever(),\n                                       return_source_documents=True,\n                                       chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT}\n                                       )\n```", "```py\n# Let's ask a question and run our question-answering chain\nquestion = \"Can you recommend a few detective novels?\"\n\nresult = qa_chain({\"query\": question})\n\n# Let's look at the result\nresult[\"result\"]\n```", "```py\n# Let's look at the source documents used as context by the LLM\n# We will use our helper function from before to limit the size of the information shown\nrender_source_documents(result[\"source_documents\"])\n```", "```py\n# Let's create a memory object to track chat history. This will start accumulating human messages and AI responses.\n# Here a \"token\" based memory is used to restrict the length of chat history to what can be passed into the selected LLM. \n# Generally, the maximum token length configured will depend on the LLM. Assuming we are using the 4K version of the LLM, \n#  we will set the token maximum to 3K, to allow some room for the question prompt.\n#  The LLM parameter is to make LangChain aware of the tokenization scheme of the selected LLM. \nmemory = ConversationTokenBufferMemory(memory_key=\"chat_history\", return_messages=True, input_key=\"question\", output_key=\"answer\", max_token_limit=3000, llm=llm)\n\n# While LangChain includes a default prompt to generate a standalone question based on the users' latest question and\n# any context from the conversation up to that point, we will extend the default prompt with additional instructions.\nstandalone_question_generator_template = \"\"\"Given the following conversation and a follow up question, \nrephrase the follow up question to be a standalone question, in its original language.\nBe as explicit as possible in formulating the standalone question, and \ninclude any context necessary to clarify the standalone question.\n\nConversation:\n{chat_history}\nFollow Up Question: {question}\nStandalone question:\"\"\"\nupdated_condense_question_prompt = PromptTemplate.from_template(standalone_question_generator_template)\n\n# Let's rebuild the final prompt (again, optional since LangChain uses a default prompt, though it might be a little different)\nfinal_response_synthesizer_template = \"\"\"Use the following pieces of context to answer the question at the end. \nIf you don't know the answer, just say that the available information is not sufficient to answer the question. \nDon't try to make up an answer. Keep the answer as concise as possible, limited to five sentences.\n{context}\nQuestion: {question}\nHelpful Answer:\"\"\"\ncustom_final_prompt = PromptTemplate.from_template(final_response_synthesizer_template)\n\nqa = ConversationalRetrievalChain.from_llm(\n    llm=llm, \n    retriever=vectordb.as_retriever(), \n    memory=memory,\n    return_source_documents=True,\n    return_generated_question=True,\n    condense_question_prompt= updated_condense_question_prompt,\n    combine_docs_chain_kwargs={\"prompt\": custom_final_prompt})\n\n# Let's again ask the question we previously asked the retrieval QA chain\nquery = \"Can you recommend a few detective novels?\"\nresult = qa({\"question\": query})\nprint(textwrap.fill(result['answer'], width=100))\n```", "```py\nquery = \"Tell me more about the second book\"\nresult = qa({\"question\": query})\nprint(textwrap.fill(result['answer'], width=100))\n```", "```py\n# Let's look at chat history upto this point\nresult['chat_history']\n```", "```py\n# Let's print the other parts of the results\nprint(\"Here is the standalone question generated by the LLM based on chat history:\")\nprint(textwrap.fill(result['generated_question'], width=100 ))\nprint(\"\\nHere are the source documents the model referenced:\")\ndisplay(render_source_documents(result['source_documents']))\nprint(textwrap.fill(f\"\\nGenerated Answer: {result['answer']}\", width=100, replace_whitespace=False) )\n```", "```py\n# Initial setup - Install necessary software in code environment\n!pip install openai tiktoken langchain chromadb html2text gradio   # Uncomment by removing '#' at the beginning if you're starting here and haven't yet installed anything\n\n# Import packages needed to enable different functionality for the solution\nfrom langchain.document_loaders import AsyncHtmlLoader # To load website content into a document\nfrom langchain.text_splitter import MarkdownHeaderTextSplitter # To document into smaller chunks by document headings \nfrom langchain.document_transformers import Html2TextTransformer # To converrt HTML to Markdown text\nfrom langchain.chat_models import ChatOpenAI # To use OpenAI's LLM\nfrom langchain.prompts import PromptTemplate # To formulate instructions / prompts\nfrom langchain.chains import RetrievalQA, ConversationalRetrievalChain # For RAG\nfrom langchain.memory import ConversationTokenBufferMemory # To maintain chat history\nfrom langchain.embeddings.openai import OpenAIEmbeddings # To convert text to numerical representation\nfrom langchain.vectorstores import Chroma # To interact with vector database\nimport pandas as pd, gradio as gr # To show data as tables, and to build UI respectively\nimport chromadb, json, textwrap # Vector database, converting json to text, and prettify printing respectively\nfrom chromadb.utils import embedding_functions # Setting up embedding function, following protocol required by Chroma\n\n# Add the OpenAI API Key to a variable\n# Saving the key in a variable like so is bad practice. It should be loaded into environment variables and loaded from there, but this is okay for a quick demo\nOPENAI_API_KEY='sk-4f3a9b8e7c4f4c8f8f3a9b8e7c4f4c8f-UsH4C3vE64' # Fake Key - use your own real key here\n```", "```py\n# To be run at the end to terminate the demo chatbot\ndemo.close() # To end the chat session and terminate the shared demo\n\n# Retrieve and delete the vector DB collection created for the chatbot\nvectordb = Chroma(client=persistent_chroma_client, collection_name=\"my_rag_demo_collection\", embedding_function=openai_embedding_func_for_langchain)\nvectordb.delete_collection()\n```", "```py\n# Initiate OpenAI embedding functions. There are two because the function protocol is different when passing the function to Chroma DB directly vs using it with Chroma DB via LangChain\nopenai_embedding_func_for_langchain = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)\nopenai_embedding_func_for_chroma = embedding_functions.OpenAIEmbeddingFunction(api_key=OPENAI_API_KEY)\n\n# Initiate the LangChain chat model object using the GPT 3.5 turbo model\nllm = ChatOpenAI(model_name='gpt-3.5-turbo', temperature=0, openai_api_key=OPENAI_API_KEY)\n\n# Initialize vector DB and create a collection\npersistent_chroma_client = chromadb.PersistentClient()\ncollection = persistent_chroma_client.get_or_create_collection(\"my_rag_demo_collection\", embedding_function=openai_embedding_func_for_chroma)\n\n# Function to load website content into vector DB\ndef load_content_from_url(url):\n  # Load HTML from URL and transform to a more readable text format\n  docs = Html2TextTransformer().transform_documents(AsyncHtmlLoader(url).load())\n  # Split docs by section\n  headers_to_split_on = [ (\"#\", \"Header 1\"), (\"##\", \"Header 2\"), (\"###\", \"Header 3\"), (\"####\", \"Header 4\"), (\"#####\", \"Header 5\") ]\n  chunks = MarkdownHeaderTextSplitter(headers_to_split_on = headers_to_split_on).split_text(docs[0].page_content)\n  # Here we are linking to the previously created an instance of the ChromaDB database\n  vectordb_collection = persistent_chroma_client.get_or_create_collection(\"my_rag_demo_collection\", embedding_function=openai_embedding_func_for_chroma)\n  # Let's add data to the vector DB; specifically to our collection in the vector DB\n  cur_max_id = vectordb_collection.count()\n  vectordb_collection.add(ids=[str(t) for t in range(cur_max_id+1, cur_max_id+len(chunks)+1)],\n                           documents=[t.page_content for t in chunks],\n                           metadatas=[None if len(t.metadata) == 0 else t.metadata for t in chunks]\n                           )\n  # Alert user that content is loaded and ready to be queried\n  gr.Info(f\"Website content loaded. Vector DB now has {vectordb_collection.count()} chunks\")\n  return\n\n# Define the UI and the chat function\nwith gr.Blocks() as demo:\n\n  # Function to chat with language model using documents for context\n  def predict(message, history):\n    # Here we are linking to the previously created an instance of the ChromaDB database using a LangChain ChromaDB client\n    langchain_chroma = Chroma(client=persistent_chroma_client, collection_name=\"my_rag_demo_collection\", embedding_function=openai_embedding_func_for_langchain)\n    # Convert to langchain chat history format - list of tuples rather than list of lists\n    langchain_history_format = []\n    for human, ai in history:\n      langchain_history_format.append((human, ai))\n    # We are now defining the ConversationalRetrieval chain, starting with the prompts used\n    standalone_question_generator_template = \"\"\"Given the following conversation and a follow up question, \n    rephrase the follow up question to be a standalone question, in its original language. Be as explicit as possible \n    in formulating the standalone question, and include any context necessary to clarify the standalone question.\n\n    Conversation: {chat_history}\n    Follow Up Question: {question}\n    Standalone question:\"\"\"\n    updated_condense_question_prompt = PromptTemplate.from_template(standalone_question_generator_template)\n    # Let's rebuild the final prompt (again, optional since LangChain uses a default prompt, though it might be a little different)\n    final_response_synthesizer_template = \"\"\"Use the following pieces of context to answer the question at the end. \n    If you don't know the answer, just say that the available information is not sufficient to answer the question. \n    Don't try to make up an answer. Keep the answer as concise as possible, limited to five sentences.\n    {context}\n    Question: {question}\n    Helpful Answer:\"\"\"\n    custom_final_prompt = PromptTemplate.from_template(final_response_synthesizer_template)\n    # Define the chain\n    qa_chain = ConversationalRetrievalChain.from_llm(llm, retriever=langchain_chroma.as_retriever(), \n                                                     return_source_documents=True, return_generated_question=True, \n                                                     condense_question_prompt= updated_condense_question_prompt,\n                                                     combine_docs_chain_kwargs={\"prompt\": custom_final_prompt})\n    # Execute the chain\n    gpt_response = qa_chain({\"question\": message, \"chat_history\": langchain_history_format})\n    # Add human message and LLM response to chat history\n    langchain_history_format.append((message, gpt_response['answer']))\n    return gpt_response['answer'] \n\n  gr.Markdown(\n      \"\"\"\n      # Chat with Websites\n      ### Enter URL to extract content from website and start question-answering using this Chatbot\n      \"\"\"\n  )\n  with gr.Row():\n    url_text = gr.Textbox(show_label=False, placeholder='Website URL to load content', scale = 5)\n    url_submit = gr.Button(value=\"Load\", scale = 1)\n    url_submit.click(fn=load_content_from_url, inputs=url_text)\n\n  with gr.Row():\n    gr.ChatInterface(fn=predict)\n\ndemo.launch(debug=True)\n```"]