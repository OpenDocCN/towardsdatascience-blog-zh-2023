- en: The Perfect Way to Smooth Your Noisy Data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/the-perfect-way-to-smooth-your-noisy-data-4f3fe6b44440?source=collection_archive---------0-----------------------#2023-10-25](https://towardsdatascience.com/the-perfect-way-to-smooth-your-noisy-data-4f3fe6b44440?source=collection_archive---------0-----------------------#2023-10-25)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Insanely fast and reliable smoothing and interpolation with the Whittaker-Eilers
    method.
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@anbowell?source=post_page-----4f3fe6b44440--------------------------------)[![Andrew
    Bowell](../Images/a23bade2986dd9ce01f9056d0a9b108f.png)](https://medium.com/@anbowell?source=post_page-----4f3fe6b44440--------------------------------)[](https://towardsdatascience.com/?source=post_page-----4f3fe6b44440--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----4f3fe6b44440--------------------------------)
    [Andrew Bowell](https://medium.com/@anbowell?source=post_page-----4f3fe6b44440--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F6096004462d1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-perfect-way-to-smooth-your-noisy-data-4f3fe6b44440&user=Andrew+Bowell&userId=6096004462d1&source=post_page-6096004462d1----4f3fe6b44440---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----4f3fe6b44440--------------------------------)
    ·13 min read·Oct 25, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F4f3fe6b44440&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-perfect-way-to-smooth-your-noisy-data-4f3fe6b44440&user=Andrew+Bowell&userId=6096004462d1&source=-----4f3fe6b44440---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4f3fe6b44440&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-perfect-way-to-smooth-your-noisy-data-4f3fe6b44440&source=-----4f3fe6b44440---------------------bookmark_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: Real-world data is never clean. Whether you’re carrying out a survey, measuring
    rainfall or receiving GPS signals from space, noisy data is ever present. Dealing
    with such data is the main part of a data scientist’s job. It’s not all glamorous
    machine learning models and AI — it’s cleaning data in an attempt to extract as
    much meaningful information as possible. If you’re currently looking at a graph
    that has way too many squiggles to be useful. Well, I have the solution you’re
    looking for.
  prefs: []
  type: TYPE_NORMAL
- en: '**Whittaker-Eilers Smoothing**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Whittaker-Eilers smoother [1] was introduced to me when I was working in
    Earth Observation. Doing pixel-wise analysis on thousands of high-resolution satellite
    images requires insanely fast algorithms to sanitise the data. On any given day
    it might be cloudy, a few pixels may be obscured by smoke, or the sensor may have
    an artifact. The list can go on and on. What you’re left with is terabytes of
    rather noisy, gappy time-series data that needs to be smoothed and interpolated
    — and this is where the Whittaker smoother thrives.
  prefs: []
  type: TYPE_NORMAL
- en: Smoothing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s start with a graph showing the [global temperature anomaly](https://climate.nasa.gov/vital-signs/global-temperature/)
    between 1880 and 2022 [2]. In orange is the measured data and in green is the
    same data smoothed using the Whittaker-Eilers method.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d1296ce02811f68f3bd0185873ba9fab.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1) Whittaker-Eilers smoothed global temperature anomaly
  prefs: []
  type: TYPE_NORMAL
- en: As you can see the raw time-series data is rather noisy. The conclusions we
    want to extract are not about year-to-year fluctuations but the general trend
    of the data over the past century. Smoothing the data offers a straight forward
    way to make the trend stand out and even better, it only takes 4 lines of code
    to run.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: If you’re not into Python, it’s only 4 lines of Rust code too.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '*Disclaimer, this is a package I’ve written — Python:* `[*pip install whittaker-eilers*](https://pypi.org/project/whittaker-eilers/)`
    *or Rust:* `[*cargo add whittaker-eilers*](https://crates.io/crates/whittaker-eilers)`'
  prefs: []
  type: TYPE_NORMAL
- en: Interpolation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now lets introduce some gaps into our data. Instead of having data that’s been
    uniformly collected each year, lets reduce it to every other year and add a couple
    of long gaps.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c38abba605bf5b6f47fc78bb8a6399d4.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2) Whittaker-Eilers smoothed and interpolated global temperature anomaly
  prefs: []
  type: TYPE_NORMAL
- en: The Whittaker handles it effortlessly. All you need to do is assign a weight
    to each measurement. When interpolating, a measurement that exists is given a
    weight of 1 and a measurement you wish to obtain a value for is filled in with
    a dummy value, say -999, and assigned a weight of 0\. The Whittaker handles the
    rest. It can also be used to accordingly weight measurements based on their uncertainty
    and can once again be deployed in a few lines of Python.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: As well as gaps in the data, the Whittaker smoother can easily deal with unevenly
    spaced measurements. If you’re not bothered about interpolation and just want
    the smoothed values for the measurements you have, just provide an `x_input` containing
    when or where your measurements were taken. For example, `[1880, 1885, 1886, 1900]`.
  prefs: []
  type: TYPE_NORMAL
- en: Configuration
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: With a single parameter, λ (lambda), you have continuous control over the smoothness
    of the data. The larger the lambda, the smoother the data.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d72e117eba62b994bc53354eefbf5c1b.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3) Whittaker-Eilers smoothed data for multiple values of lambda
  prefs: []
  type: TYPE_NORMAL
- en: The order of the smoother, **d,** is also controllable. The higher the order,
    the more adjacent elements the Whittaker will take into account when calculating
    how smooth the time-series should be. The core take away from this is that interpolated
    data will be a polynomial of order 2**d** and extrapolated data will be of order
    **d**; though, we’ll dive into the mathematics of this a little later.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0d5949c31df6a60e6ab7152ba0f486af.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4) Whittaker-Eilers smoothed data for multiple orders
  prefs: []
  type: TYPE_NORMAL
- en: Comparison with other methods
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To demonstrate how good the Whittaker-Eilers method is, let’s compare it against
    a few different techniques:'
  prefs: []
  type: TYPE_NORMAL
- en: Gaussian kernel smoothing (also known as an RBF kernel)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Savitzky-Golay filter
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Local Regression (LOWESS)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The first is a kernel smoother, which essentially amounts to a fancy weighted
    average of neighbouring points. The second, the Savitzky-Golay filter is what
    Eilers’ original 2003 paper was positioning itself against and is very widely
    used. It smooths by carrying out a least squares fit of a [polynomial to successive
    subsets of adjacent data](https://en.wikipedia.org/wiki/Savitzky%E2%80%93Golay_filter#/media/File:Lissage_sg3_anim.gif).
    And finally, local regression which was the method of choice by NASA to [smooth
    the temperature anomaly](https://climate.nasa.gov/vital-signs/global-temperature/)
    data seen above. This method performs iterative weighted linear fits on successive
    subsets of data, re-weighting the points based on the residuals as it goes.
  prefs: []
  type: TYPE_NORMAL
- en: I’ve opted not to include any methods aimed at real-time smoothing as they necessarily
    introduce a lag into your signal by only assessing past data. If you’re interested
    in such methods be sure to check out moving averages, exponential smoothers, and
    Kalman filters.
  prefs: []
  type: TYPE_NORMAL
- en: Smoothing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: So let’s take a look at temperature anomaly time-series again, but this time
    smoothed with the additional methods.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b4f59976a77fdc629623b6d378c47a9b.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5) Comparison of different smoothing techniques
  prefs: []
  type: TYPE_NORMAL
- en: As you can see there’s not much variation — at least once you’ve got your parameters
    tuned. Let’s try an example that can tease out some differences as well as be
    benchmarked for performance. We’ll generate 4 basic sine waves between 0 and 2**π**
    and add some Gaussian noise to them. With each signal the sample size will increase,
    starting with 40 and finishing with 10,000\. Already challenges are presented
    for the the Savitzky-Golay, Gaussian Kernel, and LOWESS methods. They expect a
    window length — some length of data that should be included in each successive
    subset to perform the fit/average on. If you’re increasing the number of measurements
    in the same space of time, you’ll need to make sure your window length varies
    with your overall data length to obtain optimal smoothing for each dataset. Below,
    each method roughly takes into account one tenth of the overall data for each
    window.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/74094e41628bd15b83776679f907e774.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6) Smoothed sine waves with different measurement sampling frequencies
  prefs: []
  type: TYPE_NORMAL
- en: The Whittaker however does not need such dynamic parameter tuning. You can provide
    it with the measurement position x, set λ once, and you’ll be given smooth data
    every time. This knowledge of measurement position additionally enables it to
    handle unequally spaced data. LOWESS can also run on such data, but the other
    two methods require it to be equally spaced. Overall, I find the Whittaker the
    easiest to use.
  prefs: []
  type: TYPE_NORMAL
- en: Another benefit of the Whittaker smoother is its adaptiveness to boundary conditions.
    At the edges of data, the Savitzky-Golay filter changes behavior as it can’t fit
    a polynomial past the end of the dataset without padding or mirroring it. Another
    option is to let it fit a polynomial to the final window and just use those results.
    The Gaussian kernel smoother struggles even more as no future measurements enter
    the average and it starts to exhibit a large bias from previous values, which
    can be clearly seen in the graph below.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6db4987af0f2c6b64de8f26d795ca3da.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7) Boundaries of smoothed sine waves with different measurement sampling
    frequencies
  prefs: []
  type: TYPE_NORMAL
- en: Interpolation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A key feature of the Whittaker is its in-built ability to interpolate data.
    But how good is it compared to these other methods? The question isn’t clear-cut.
    Savitzky-Golay smoothing can interpolate, but only for gaps in data smaller than
    it’s window size and the same is true for LOWESS smoothing. Gaussian kernel smoothing
    doesn’t even have the ability to interpolate at all. The traditional solution
    to this problem is to apply linear interpolation to your data first and then smooth
    it. So we’ll apply this method to the other three techniques and compare the results
    against the Whittaker.
  prefs: []
  type: TYPE_NORMAL
- en: Each method will be compared against it’s own smoothed baseline taken from the
    graph at the start of this section (Figure 5). I removed every other point and
    introduced two large gaps, creating a dataset identical to the one seen in the
    interpolation example at the start of the article (Figure 2). For the baseline
    and interpolation runs the parameters were kept the same.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f5e1e5ef7eb86664ecdf16748a9ec525.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8) Comparison of techniques for used in conjunction with linear interpolation
  prefs: []
  type: TYPE_NORMAL
- en: With linear interpolation filling in gaps, the methods perform well across the
    board. By calculating the [Root Mean Squared Error (RSME)](https://en.wikipedia.org/wiki/Root-mean-square_deviation)
    between the smoothed data without gaps and the smoothed data with gaps we get
    the following results.
  prefs: []
  type: TYPE_NORMAL
- en: 'Linear Interpolation + Savitzky-Golay: **0.0245 °C**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Whittaker : **0.0271 °C**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Linear Interpolation + Gaussian kernel: **0.0275 °C**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Linear Interpolation + LOWESS: **0.0299 °C**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Savitzky-Golay method with linear interpolation gets the closest to the
    original smoothed data followed by the Whittaker, and there’s not much in it!
  prefs: []
  type: TYPE_NORMAL
- en: I’d just quickly like to mention that I've performed the interpolation benchmark
    this way, against their own smoothed baselines, to avoid tuning parameters. I
    could have used the sine wave with added noise, removed some data and tried to
    smooth it back to the original signal but this would have given me a headache
    trying to find the optimal parameters for each method.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Benchmarking
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: So lets revisit the sine wave data to generate some benchmarks of just how fast
    these methods are. I chose the most popular implementations in Python for each
    method. Savitzky-Golay and Gaussian kernel filters were implemented using `SciPy`,
    LOWESS was implemented from `statsmodels`, and the Whittaker from my Rust based
    Python package. The graph below shows how long each method took to smooth the
    sine wave with varying data lengths. The times reported are the sum of how long
    it took to smooth each dataset 50 times.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6daa7c9ce94297cef542bddf851af352.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9) Time taken to smooth time-series of different lengths 50 times
  prefs: []
  type: TYPE_NORMAL
- en: The quickest method by far is the Whittaker. It can smooth 50 time-series each
    100,000 data points in length in under a second, 10 times faster than a Gaussian
    filter and 100 times faster than a Savitzky-Golay filter. The slowest was LOWESS
    even though it was configured not to iteratively re-weight each linear regression
    (an expensive operation). It’s worth noting that these methods can be sped up
    by adapting the window lengths, but then you’ll be sacrificing the smoothness
    of your data. This is a really great property of the Whittaker — its computation
    time increases linearly with data length (O(n)) and you never have to worry about
    window size. Furthermore, if you have gaps in your data you’ll be interpolating
    without any cost in speed whereas the other methods require some form of pre-processing!
  prefs: []
  type: TYPE_NORMAL
- en: The Mathematics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now we’ve covered the top-line stuff, let’s dive into the maths behind the Whittaker-Eilers
    smoother and see why it’s such an elegant solution for noisy data [2] [3].
  prefs: []
  type: TYPE_NORMAL
- en: Imagine your noisy data **y.** There exists some series **z** which you believe
    to be of optimal smoothness for your **y**. The smoother **z** becomes, the larger
    the residuals between itself and the original data **y**. The Whittaker-Eilers
    method finds the optimal balance between these residuals and the smoothness of
    the data. The residuals are calculated as the standard sum of squared differences,
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8689699a8486d7d83d8a9eb1a32ddf6a.png)'
  prefs: []
  type: TYPE_IMG
- en: A metric for how smooth the data is can then be computed using the sum of squared
    differences between adjacent measurements,
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4577bb5dcf8a66ac33cb31b5b3c27ed9.png)'
  prefs: []
  type: TYPE_IMG
- en: '**S** and **R** are the two properties we need to balance. But we also want
    to give the user control over where the right balance is, and we do this by introducing
    **λ** to scale the smoothness.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/65f970a952bc4a14590b646c90dce207.png)'
  prefs: []
  type: TYPE_IMG
- en: Now our goal becomes finding the series **z** that minimizes **Q** as this is
    where both the smoothness metric and residuals are at their minimum. Let’s expand
    Equation 3 and attempt to solve for **z.**
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5f62ce5b5dac8ddeade69bfea01ca3a7.png)![](../Images/cc189978f50f6cfd752171e2cb54e5d9.png)'
  prefs: []
  type: TYPE_IMG
- en: At this point it’s ideal to replace our summations with vectors,
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1fef8446ca8e3971c9412acca041e82c.png)'
  prefs: []
  type: TYPE_IMG
- en: We can then use a clever trick to represent **Δz** as a matrix and vector,
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/70b7496fe87f0c35ea62f140309be3ce.png)![](../Images/cdf8b06f94532c8301a236330f3bfb5f.png)'
  prefs: []
  type: TYPE_IMG
- en: where m is the length of the data. If you matrix **D** against a vector, you’ll
    see it gives you the differences between adjacent elements — exactly what we want.
    We’re now left with a [least squares problem](https://en.wikipedia.org/wiki/Least_squares#Solving_the_least_squares_problem).
    To find the minimum of **Q** we set its gradient to 0,
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/11416b50efba7938a53cd66943c5f2a4.png)![](../Images/ff8d5b8019bfd95f995faae1a2f495e6.png)'
  prefs: []
  type: TYPE_IMG
- en: where **I** is the identity matrix (from factorizing z, a vector). We know **I**,
    **D**, **λ** and **y**, so we’re left with a simple linear equation,
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e32184910ba27f895efe585fcc7a7533.png)'
  prefs: []
  type: TYPE_IMG
- en: which can be solved with any of your favourite matrix decompositions to achieve
    the smoothed data series **z**.
  prefs: []
  type: TYPE_NORMAL
- en: Interpolation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The above solution only accounts for evenly spaced data where all measurements
    are available. What about if you want interpolation? Well, you’ll need to apply
    weights to each of your measurements.
  prefs: []
  type: TYPE_NORMAL
- en: It’s as simple as revisiting Equation 1 and applying a weight to each residual
    and representing it as a diagonal matrix,
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3d9229e9adbf042f6d22834c82a435e5.png)'
  prefs: []
  type: TYPE_IMG
- en: and then carrying out the same calculations as before,
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/79d94c70d787869001ca6ea1e4c24db5.png)![](../Images/7aa4434ed3c1c72c9b7fb905f892cc0a.png)![](../Images/a48f1b41104fc7ea929b836b0447d8a7.png)'
  prefs: []
  type: TYPE_IMG
- en: Once again, this can be solved with a simple matrix decomposition, returning
    smoothed and interpolated data. All that needs to be done beforehand is to fill
    **y** with dummy values when an interpolated value is needed, such as -999, and
    set the weight of those measurements to 0 and watch the magic happen. Exactly
    how the data is interpolated depends upon the filter’s order.
  prefs: []
  type: TYPE_NORMAL
- en: Filter Order
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The order of the Whittaker-Eilers smoother is something I touched upon in the
    configuration section. Now we have a mathematical framework for describing the
    smoother, it may make more sense. When creating **R,** our measure of smoothness,
    we first opted for “first-order” differences. We can quite easily take a second
    order difference where instead of penalizing our smoother based on adjacent data
    points, we can penalize it based on the change in first order differences, just
    like calculating a derivative.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b2df783e00d7d520509a790a27fd8ae1.png)'
  prefs: []
  type: TYPE_IMG
- en: This can then be expanded to third, forth, and fifth order differences and so
    on. It’s normally denoted as **d** and it’s not too tricky to implement as all
    that changes is the matrix **D** like so,
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f6119d1f5c3ebff5dbf203c64b0617a9.png)'
  prefs: []
  type: TYPE_IMG
- en: such that when it is multiplied with **z**,it expands into Equation 17\. A simple
    function can be implemented to generate this matrix given a generic **d**.
  prefs: []
  type: TYPE_NORMAL
- en: Sparse Matrices
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This can and has been implemented with sparse matrices as recommended by Eilers
    [1]. The matrices **I** and **D** are very sparsely populated and hugely benefit
    in terms of memory and computation if stored as sparse matrices. All of the maths
    presented above can be easily handled by sparse matrix packages, including Cholesky
    decompositions (and others). If not implemented with sparse matrices the algorithm
    can be incredibly slow for longer time-series, much slower than the other methods
    I compared it with.
  prefs: []
  type: TYPE_NORMAL
- en: Wrapping-up & Further Reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This is an awesome algorithm and I can’t believe it isn’t utilized more. Weighted
    smoothing and interpolation wrapped up into fast, efficient matrix operations.
    What’s not to love?
  prefs: []
  type: TYPE_NORMAL
- en: I’ve included the Python scripts I used to carry out benchmarking and interpolation
    tests in the repo for the [whittaker-eilers](https://github.com/AnBowell/whittaker-eilers/tree/main/whittaker-eilers-py/examples)
    package. There’s also lots of examples showing you how to get started in Python
    or Rust as well as tests against Eilers’ original MATLAB algorithms [1]. But if
    you don’t care for that level of verbosity,
  prefs: []
  type: TYPE_NORMAL
- en: 'Python: `[*pip install whittaker-eilers*](https://pypi.org/project/whittaker-eilers/)`
    *or Rust:* `[*cargo add whittaker-eilers*](https://crates.io/crates/whittaker-eilers)`'
  prefs: []
  type: TYPE_NORMAL
- en: Even though this was a long post, I haven’t been able to cover everything here.
    Eilers’ 2003 paper also covers the mathematics behind smoothing unevenly spaced
    data and how cross-validation can be used to find an optimal λ. I’d recommend
    checking it out if you want to learn more about the maths behind the algorithm.
    I’d also suggest “Applied Optimum Signal Processing” by Sophocles J. Orfanidis
    as it offers an in-depth mathematical guide to all things signal processing. Thanks
    for reading! Be sure to check this post and others out on my [personal site](https://www.anbowell.com/blog_home).
  prefs: []
  type: TYPE_NORMAL
- en: '*I’ve since written a follow up post explaining how to best tune the Whittaker!*'
  prefs: []
  type: TYPE_NORMAL
- en: '[](/how-to-tune-the-perfect-smoother-bcc5a67660b1?source=post_page-----4f3fe6b44440--------------------------------)
    [## How to Tune the Perfect Smoother'
  prefs: []
  type: TYPE_NORMAL
- en: Get the most out of your data with Whittaker-Eilers smoothing and leave-one-out
    cross validation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/how-to-tune-the-perfect-smoother-bcc5a67660b1?source=post_page-----4f3fe6b44440--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: '*All images within this article have been produced by the author.*'
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] Paul H. C. Eilers, *A Perfect Smoother*, Analytical Chemistry **2003**
    *75* (14), 3631-3636, DOI: 10.1021/ac034173t'
  prefs: []
  type: TYPE_NORMAL
- en: '[2] NASA/GISS, *Global Temperature,* NASA’s Goddard Institute for Space Studies
    (GISS). URL: [https://climate.nasa.gov/vital-signs/global-temperature/](https://climate.nasa.gov/vital-signs/global-temperature/)'
  prefs: []
  type: TYPE_NORMAL
- en: '[3] Sophocles J. Orfanidi, *Applied Optimum Signal Processing,* Rutgers University,
    URL: [http://www.ece.rutgers.edu/~orfanidi/aosp](http://www.ece.rutgers.edu/~orfanidi/aosp)'
  prefs: []
  type: TYPE_NORMAL
