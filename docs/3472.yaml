- en: Supercharge Training of Your Deep Learning Models
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用超级收敛加速你的深度学习模型训练
- en: 原文：[https://towardsdatascience.com/supercharge-training-of-your-deep-learning-models-7168ff81a042?source=collection_archive---------7-----------------------#2023-11-22](https://towardsdatascience.com/supercharge-training-of-your-deep-learning-models-7168ff81a042?source=collection_archive---------7-----------------------#2023-11-22)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/supercharge-training-of-your-deep-learning-models-7168ff81a042?source=collection_archive---------7-----------------------#2023-11-22](https://towardsdatascience.com/supercharge-training-of-your-deep-learning-models-7168ff81a042?source=collection_archive---------7-----------------------#2023-11-22)
- en: Super convergence with one-cycle learning rates
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用单周期学习率实现超级收敛
- en: '[](https://medium.com/@Rghv_Bali?source=post_page-----7168ff81a042--------------------------------)[![Raghav
    Bali](../Images/49fea68f38f59d0bc39dab484b55684f.png)](https://medium.com/@Rghv_Bali?source=post_page-----7168ff81a042--------------------------------)[](https://towardsdatascience.com/?source=post_page-----7168ff81a042--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----7168ff81a042--------------------------------)
    [Raghav Bali](https://medium.com/@Rghv_Bali?source=post_page-----7168ff81a042--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@Rghv_Bali?source=post_page-----7168ff81a042--------------------------------)[![Raghav
    Bali](../Images/49fea68f38f59d0bc39dab484b55684f.png)](https://medium.com/@Rghv_Bali?source=post_page-----7168ff81a042--------------------------------)[](https://towardsdatascience.com/?source=post_page-----7168ff81a042--------------------------------)[![走向数据科学](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----7168ff81a042--------------------------------)
    [Raghav Bali](https://medium.com/@Rghv_Bali?source=post_page-----7168ff81a042--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fdff4008c1908&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsupercharge-training-of-your-deep-learning-models-7168ff81a042&user=Raghav+Bali&userId=dff4008c1908&source=post_page-dff4008c1908----7168ff81a042---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----7168ff81a042--------------------------------)
    ·7 min read·Nov 22, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F7168ff81a042&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsupercharge-training-of-your-deep-learning-models-7168ff81a042&user=Raghav+Bali&userId=dff4008c1908&source=-----7168ff81a042---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fdff4008c1908&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsupercharge-training-of-your-deep-learning-models-7168ff81a042&user=Raghav+Bali&userId=dff4008c1908&source=post_page-dff4008c1908----7168ff81a042---------------------post_header-----------)
    发表在[《走向数据科学》](https://towardsdatascience.com/?source=post_page-----7168ff81a042--------------------------------)
    ·7 分钟阅读·2023 年 11 月 22 日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F7168ff81a042&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsupercharge-training-of-your-deep-learning-models-7168ff81a042&user=Raghav+Bali&userId=dff4008c1908&source=-----7168ff81a042---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F7168ff81a042&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsupercharge-training-of-your-deep-learning-models-7168ff81a042&source=-----7168ff81a042---------------------bookmark_footer-----------)![](../Images/f3668520565b60e31f7dacbe8189e9fa.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F7168ff81a042&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsupercharge-training-of-your-deep-learning-models-7168ff81a042&source=-----7168ff81a042---------------------bookmark_footer-----------)![](../Images/f3668520565b60e31f7dacbe8189e9fa.png)'
- en: Photo by [Philip Swinburn](https://unsplash.com/@pjswinburn?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 照片由[Philip Swinburn](https://unsplash.com/@pjswinburn?utm_source=medium&utm_medium=referral)拍摄，来源于[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: Have you come across scenarios when it is easy to get an initial burst in accuracy
    but once you reach 90%, you have to push really really hard to squeeze out any
    improvement in performance? Does your model take too long to train?
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 你是否遇到这样的情况：一开始提高准确率很容易，但一旦达到90%，就必须非常努力才能进一步提高性能？你的模型训练时间太长吗？
- en: In this article, we will look at an interesting technique to supercharge your
    training setup and get that extra bit of performance you have been looking for
    and train faster. Essentially, we will work towards dynamically changing the learning
    rate over epochs using a policy called the ***One-Cycle Learning Rate***.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们将探讨一种有趣的技术，以加速你的训练设置，并获得你一直寻求的额外性能，提高训练速度。本质上，我们将致力于通过一种称为***一周期学习率***的策略，在训练过程中动态调整学习率。
- en: Originally mentioned in a paper by Leslie Smith, the **one-cycle learning rate
    schedule**[[1](https://arxiv.org/abs/1803.09820)], [[2](https://arxiv.org/abs/1708.07120)]
    focuses on a unique strategy to dynamically update the learning rate during the
    training process. Sounds like a mouthful of terms, don’t worry, let’s first start
    with a typical training setup and then we will gradually understand how we can
    improve results using one-cycle learning rate.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 原本在Leslie Smith的论文中提到的**一周期学习率计划**[[1](https://arxiv.org/abs/1803.09820)], [[2](https://arxiv.org/abs/1708.07120)]，专注于一种独特的策略，在训练过程中动态更新学习率。听起来术语很多，别担心，让我们先从一个典型的训练设置开始，然后逐渐理解如何通过一周期学习率来改进结果。
- en: Training a Image Classifier
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练图像分类器
- en: As we are working towards learning a neat trick (cycle-rate) to improve model
    performance, why not do it while enjoying the classic ***rock-paper-scissors***
    game.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们致力于学习一种提高模型性能的巧妙技巧（周期率）时，何不在享受经典的***石头剪子布***游戏时进行呢。
- en: '![](../Images/69028e843abe371779ee844df84ddd0b.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/69028e843abe371779ee844df84ddd0b.png)'
- en: Photo by [Markus Spiske](https://unsplash.com/@markusspiske?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 由[Markus Spiske](https://unsplash.com/@markusspiske?utm_source=medium&utm_medium=referral)拍摄于[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: Problem Statement
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题陈述
- en: '*The game of rock-paper-scissors is a classic child’s game involving two players
    using hand gestures (for rock, paper or scissors) competing to over-power their
    opponent. For instance, the rock gesture wins over scissors but the paper gesture
    wins over the rock. Interesting, isn’t it?*'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '*石头剪子布是一个经典的儿童游戏，涉及两个玩家使用手势（石头、纸或剪刀）进行竞争，以压倒对手。例如，石头手势战胜剪刀，但纸手势战胜石头。有趣吧？*'
- en: Our objective here, is to train an image classification model which can detect
    one of the three gestures. We can then leverage such a trained model to develop
    an end-to-end game. For the purpose of this article, we will limit the scope towards
    training a classifier itself, the end-to-end game complete with a deployable model
    is for another article probably.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的目标是训练一个图像分类模型，能够检测三种手势之一。然后，我们可以利用这样的训练模型开发一个端到端的游戏。为了本文的目的，我们将把范围限制在训练分类器本身，端到端的游戏和可部署的模型是另一个可能的文章主题。
- en: The Dataset
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据集
- en: 'We are lucky that we already have a labelled dataset which we can leverage
    to train a classification model to great effect. The dataset is hosted on [TensorFlow
    dataset](https://www.tensorflow.org/datasets/catalog/rock_paper_scissors) catalog
    made available by [Laurence Moroney](https://laurencemoroney.com/datasets.html#rock-paper-scissors-dataset)
    (CC BY 2.0). It has the following attributes:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 我们很幸运已经有一个带标签的数据集，我们可以利用这个数据集有效地训练分类模型。数据集托管在[TensorFlow数据集](https://www.tensorflow.org/datasets/catalog/rock_paper_scissors)目录中，由[劳伦斯·莫罗尼](https://laurencemoroney.com/datasets.html#rock-paper-scissors-dataset)（CC
    BY 2.0）提供。数据集具有以下属性：
- en: 'Number of data points: 2800'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据点数量：2800
- en: 'Number of classes : 3'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 类别数量：3
- en: 'Available train-test split: Yes'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可用的训练-测试拆分：是
- en: 'Dataset size: 220 MiB'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据集大小：220 MiB
- en: TensorFlow provides a nice and clean API to access such datasets, the following
    snippet allows us to download the train and validation splits
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow提供了一个干净的API来访问这些数据集，以下代码片段允许我们下载训练和验证拆分
- en: '[PRE0]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The following are a few sample images from this dataset itself:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是该数据集的一些样本图像：
- en: '![](../Images/fa32543b74881bdc796de9ca4f8bcc04.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fa32543b74881bdc796de9ca4f8bcc04.png)'
- en: Figure:Sample data points in the Rock Paper Scissors Dataset
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 图：石头剪子布数据集中的样本数据点
- en: Learning Rate
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 学习率
- en: Learning Rate is one of the key hyper-parameters which can make or break a setup
    yet it is one which is typically overlooked. The reason it is overlooked is because
    most libraries/packages come with good enough defaults to begin with. But these
    defaults can take you only so far.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 学习率是一个关键的超参数，它可能决定设置的成败，但通常被忽视。忽视的原因是，大多数库/包提供了足够好的默认值，但这些默认值只能带你走到一定程度。
- en: Getting the correct learning rate for a bespoke use case such as ours is very
    important. It is a tricky trade-off to find the optimal value. Go too slow (or
    small) with the learning rate and your model will hardly learn anything. Go too
    fast (or large) and it will overshoot the ever so mysterious minima all neural
    networks aim to find. The same is depicted in the below illustration for a better
    understanding.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 对于像我们这样的定制使用案例，获得正确的学习率非常重要。找到最佳值是一项棘手的权衡。学习率设置得过慢（或过小），你的模型几乎无法学到任何东西。设置得过快（或过大），它将超越所有神经网络旨在找到的神秘最小值。下图展示了这一点，以便更好地理解。
- en: '![](../Images/c7f45ab8d5b0617894d110992ebd4064.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c7f45ab8d5b0617894d110992ebd4064.png)'
- en: 'Figure:Impact of Learning Rate towards Model’s ability to learn the objective
    (minima). Souce: Author'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 图示：学习率对模型学习目标（最小值）能力的影响。来源：作者
- en: '**Gradient Descent & Optimizers**'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '**梯度下降与优化器**'
- en: Gradient descent is the standard way to train/optimise neural networks. It works
    by minimizing the objective function by updating the parameters of the network
    in the opposite direction of the gradient. Without going into much details, it
    helps in travelling downhill along the slope of the objective function. A detailed
    introduction to gradient descent is available [here](https://cs231n.github.io/optimization-1/)
    for reference.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度下降是训练/优化神经网络的标准方法。它通过更新网络参数，使其朝着梯度的反方向前进，从而最小化目标函数。不深入细节，它有助于沿着目标函数的坡度向下行进。有关梯度下降的详细介绍，请参考[这里](https://cs231n.github.io/optimization-1/)。
- en: 'The deep learning community has come a long way since the initial models were
    trained with vanilla gradient descent. Over the years a number of improvements
    have helped train faster and avoid obvious pitfalls. Briefly, some of the notable
    and most popular ones are:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习社区自最初模型使用基础梯度下降进行训练以来已经取得了长足的进步。多年来，许多改进帮助更快地训练并避免明显的陷阱。简要地说，一些显著和最受欢迎的方法有：
- en: '**AdaGrad** [Adaptive Gradient algorithm](https://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf)
    is an optimization algorithm which adapts the learning rates of individual parameters
    based on their historical gradients, allowing for larger updates for infrequent
    parameters and smaller updates for frequent ones. It is designed to handle sparse
    data efficiently. It is well-suited when dealing with sparse data.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '**AdaGrad** [自适应梯度算法](https://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf)
    是一种优化算法，根据各个参数的历史梯度调整学习率，从而对不频繁的参数进行较大更新，对频繁的参数进行较小更新。它旨在有效处理稀疏数据，特别适合处理稀疏数据。'
- en: '**RMSProp** [Root Mean Square Propagation](https://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf)
    optimises the learning by adjusting the learning rates for each parameter individually.
    It addresses the diminishing learning rates problem in AdaGrad by using a moving
    average of squared gradients. This helps adaptively scale the learning rates based
    on recent gradient magnitudes.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '**RMSProp** [均方根传播](https://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf)
    通过对每个参数单独调整学习率来优化学习。它通过使用平方梯度的移动平均值来解决 AdaGrad 中学习率递减的问题。这有助于根据最近的梯度大小自适应地缩放学习率。'
- en: '**ADAM** [Adaptive Moment Estimation](https://arxiv.org/pdf/1412.6980.pdf)
    is an optimization algorithm that combines ideas from both RMSProp and momentum
    methods. It maintains exponentially decaying averages of past gradients and squared
    gradients, using them to adaptively update parameters. ADAM is known for its efficiency
    and effectiveness in training deep neural networks.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '**ADAM** [自适应矩估计](https://arxiv.org/pdf/1412.6980.pdf) 是一种优化算法，结合了 RMSProp
    和动量方法的思想。它保持过去梯度和平方梯度的指数衰减平均值，利用这些值自适应地更新参数。ADAM 以其高效性和有效性在训练深度神经网络中而闻名。'
- en: One-Cycle Learning Rate and Super Convergence
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: One-Cycle Learning Rate 和超收敛
- en: 'One-Cycle Learning Rate is a simple two-step process to improvise upon the
    learning rate and the momentum as the training progresses. It works as follows:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: One-Cycle Learning Rate 是一个简单的两步过程，用于在训练过程中改进学习率和动量。其工作原理如下：
- en: '**Step 1**: We start by ramping up the learning rate initially from a lower
    to a higher value in a linear incremental fashion for a few epochs'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**第 1 步**：我们从较低的学习率开始，在几个时期内以线性递增的方式逐步提高到较高的值'
- en: '**Step 2**: We maintain the highest value of learning rate for a few epochs'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**第 2 步**：我们在几个时期内维持学习率的最高值'
- en: '**Step 3**: We then go back to a lower learning rate decaying over time'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**第 3 步**：然后我们回到一个较低的学习率，并随着时间的推移逐渐衰减'
- en: During these three steps, the momentum is updated in the exact opposite direction,
    i.e. when the learning rate goes up, the momentum goes down and vice-versa.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在这三个步骤中，动量在完全相反的方向上进行更新，即当学习率上升时，动量下降，反之亦然。
- en: '**One-Cycle Learning Rate in Action**'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '**一周期学习率的实际应用**'
- en: Let us first work our way through a simple implementation for one-cycle learning
    rate and then use it for training our model. We will leverage a ready-to-use implementation
    for the one-cycle LR schedule from [Martin Gorner’s 2019 talk at *TensorFlow World*](https://docs.google.com/presentation/d/e/2PACX-1vRqvlSpX5CVRC2oQ_e_nRNahOSPoDVL6I36kdjuPR_4y_tCPb-_k98Du1QXBwx4sBvVrzsCPulmuPn8/pub?slide=id.g50ba8fd3eb_0_0)
    as depicted in listing 2.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将通过一个简单的1周期学习率实现进行操作，然后用它来训练我们的模型。我们将利用[Martin Gorner 2019年在*TensorFlow
    World*](https://docs.google.com/presentation/d/e/2PACX-1vRqvlSpX5CVRC2oQ_e_nRNahOSPoDVL6I36kdjuPR_4y_tCPb-_k98Du1QXBwx4sBvVrzsCPulmuPn8/pub?slide=id.g50ba8fd3eb_0_0)的演讲中现成的1周期LR计划实现，如清单2所示。
- en: '[PRE1]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: We execute this function(*see* *listing 2)* for a fixed number of epochs to
    showcase how the learning rate changes as per the two steps we discussed earlier.
    Here we start with an initial learning rate of **1e-3** and ramp it up to **2e-3**
    in the first few epochs. It is then reduced again back to **1e-3** over the course
    of the remaining epochs. This dynamic learning rate curve is depicted with a sample
    run of 24 epochs in following figure.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 我们执行这个函数（*见* *清单2*）来展示学习率如何根据我们之前讨论的两个步骤进行变化。这里我们从**1e-3**的初始学习率开始，并在前几个epoch中将其提升至**2e-3**。然后在剩余的epoch中将其再次降低至**1e-3**。这种动态学习率曲线在以下24个epoch的样本运行中得以展示。
- en: '![](../Images/160baa91c3ce99fdf821cce2bbfb6683.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/160baa91c3ce99fdf821cce2bbfb6683.png)'
- en: 'One-cycle learning rate policy over 24 epochs. Learning rate is ramped up linearly
    , followed by a slow decay over remaining epochs. Image Source: Author'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 24个epoch的1周期学习率策略。学习率线性上升，然后在剩余的epoch中缓慢衰减。图像来源：作者
- en: We will now put our one-cycle learning rate scheduler to the test by applying
    it when using a MobileNetV2 model as a feature extractor while training a classification
    head for our current case of rock-paper-scissors. We will then be comparing it
    against a simple CNN as well as MobileNetV2+classification head with standard
    Adam optimiser. The complete notebook is available for reference on [github](https://github.com/raghavbali/python_notebooks/blob/master/supercharge_series/supercharge_learning_lr.ipynb).
    For a quick overview, the following snippet outlines how we use TensorFlow callbacks
    to plug-in our 1-cycle-rate utility.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将通过在使用MobileNetV2模型作为特征提取器，并为当前的石头剪子布分类任务训练一个分类头时，测试我们的1周期学习率调度器。然后我们将其与简单的CNN以及使用标准Adam优化器的MobileNetV2+分类头进行比较。完整的笔记本可以在[github](https://github.com/raghavbali/python_notebooks/blob/master/supercharge_series/supercharge_learning_lr.ipynb)上找到参考。以下片段快速概述了我们如何使用TensorFlow回调来插入我们的1周期学习率工具。
- en: '[PRE2]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: We train all 3 models for 24 epochs with a batch size of 64\. The following
    figure showcases the impact of 1 cycle learning rate. It is able to assist our
    model to achieve convergence in just 5 epochs as compared to the other two models.
    The super-convergence phenomenon is visible for validation dataset as well.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我们用批量大小为64的情况下训练了所有3个模型24个epoch。下图展示了1周期学习率的影响。与其他两个模型相比，它能帮助我们的模型在仅5个epoch内实现收敛。超收敛现象在验证数据集上也可见。
- en: '![](../Images/f97ad1bd91456177a4c935ba16b42d0a.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f97ad1bd91456177a4c935ba16b42d0a.png)'
- en: MobileNetV2 with 1 cycle learning rate (mobileNetV2_lr) outperforms MobileNetV2
    and simple CNN architectures by achieving converge is just 5 epochs
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 使用1周期学习率（mobileNetV2_lr）的MobileNetV2在5个epoch内即可收敛，表现优于MobileNetV2和简单CNN架构。
- en: We reach consistent values of validation accuracies ranging between 90–92% within
    10 epochs which is so far the best we have seen across all our models. On evaluation
    the model performance on the test dataset also depicts the same story, i.e. MobileNetV2_lr
    outperforms the other two very easily.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在10个epoch内达到了90–92%的验证准确率，这在所有模型中是迄今为止表现最好的。模型在测试数据集上的表现也显示了同样的情况，即MobileNetV2_lr轻松超越了其他两个模型。
- en: '[PRE3]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Conclusion
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: Overcoming the plateau in model performance beyond 90% accuracy and optimizing
    training time can be achieved through the implementation of the One-Cycle Learning
    Rate. This technique, introduced by Leslie Smith and team, dynamically adjusts
    the learning rate during training, offering a strategic approach to supercharging
    the model performance. By adopting this method, you can efficiently navigate the
    complexities of training setups and unlock the potential for faster and more effective
    deep learning models. Embrace the power of One-Cycle Learning Rate to elevate
    your training experience and achieve superior results!
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 克服模型性能超过90%准确率的瓶颈并优化训练时间，可以通过实施**One-Cycle Learning Rate**来实现。这一技术由Leslie Smith及其团队提出，在训练过程中动态调整学习率，提供了一种战略性方法以增强模型性能。通过采用这一方法，你可以有效地应对训练设置的复杂性，并发掘更快、更有效的深度学习模型的潜力。拥抱**One-Cycle
    Learning Rate**的力量，提高你的训练体验，实现卓越的结果！
