- en: Quantisation and co. Reducing inference times on LLMs by 80%
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/quantisation-and-co-reducing-inference-times-on-llms-by-80-671db9349bdb?source=collection_archive---------2-----------------------#2023-10-27](https://towardsdatascience.com/quantisation-and-co-reducing-inference-times-on-llms-by-80-671db9349bdb?source=collection_archive---------2-----------------------#2023-10-27)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[](https://medium.com/@christopher_karg?source=post_page-----671db9349bdb--------------------------------)[![Christopher
    Karg](../Images/9d163d59e0c3167732f55d497caf9db2.png)](https://medium.com/@christopher_karg?source=post_page-----671db9349bdb--------------------------------)[](https://towardsdatascience.com/?source=post_page-----671db9349bdb--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----671db9349bdb--------------------------------)
    [Christopher Karg](https://medium.com/@christopher_karg?source=post_page-----671db9349bdb--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F5fbda6d16c39&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fquantisation-and-co-reducing-inference-times-on-llms-by-80-671db9349bdb&user=Christopher+Karg&userId=5fbda6d16c39&source=post_page-5fbda6d16c39----671db9349bdb---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----671db9349bdb--------------------------------)
    ·12 min read·Oct 27, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F671db9349bdb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fquantisation-and-co-reducing-inference-times-on-llms-by-80-671db9349bdb&user=Christopher+Karg&userId=5fbda6d16c39&source=-----671db9349bdb---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F671db9349bdb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fquantisation-and-co-reducing-inference-times-on-llms-by-80-671db9349bdb&source=-----671db9349bdb---------------------bookmark_footer-----------)![](../Images/c44c0f469d07bf09ab4e359dd48265fe.png)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Source: [https://www.pexels.com/photo/cropland-in-autumn-18684338/](https://www.pexels.com/photo/cropland-in-autumn-18684338/)'
  prefs: []
  type: TYPE_NORMAL
- en: Quantisation is a technique used for a range of different algorithms but has
    gained prevalence with the fairly recent influx of Large Language Models (LLMs).
    In this article, I aim to provide information on the quantisation of LLMs and
    the impact this technique can have on running these models locally. I’ll cover
    a different strategy outside quantisation that can further reduce computational
    requirements of running these models. I’ll go on to explain why these techniques
    may be of interest to you and will show you some benchmarks with code examples
    as to how effective these techniques are. I also briefly cover hardware requirements/recommendations
    and the modern tools available to you for achieving your LLM goals on your machine.
    In a later article I plan to provide step-by-step instructions and code for fine-tuning
    your own LLM so keep an eye out for that.
  prefs: []
  type: TYPE_NORMAL
- en: TL;DR — by quantising our LLM and changing the tensor *dtype*, we are able to
    run inference on an LLM with 2x the parameters whilst also reducing *Wall time*
    by 80%.
  prefs: []
  type: TYPE_NORMAL
- en: As always, if you wish to discuss anything I cover here please [reach out](http://www.linkedin.com/in/-christopherkarg).
  prefs: []
  type: TYPE_NORMAL
- en: All opinions in this article are my own. This article is not sponsored.
  prefs: []
  type: TYPE_NORMAL
- en: What is quantisation (of LLMs)?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
