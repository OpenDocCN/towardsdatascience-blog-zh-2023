- en: 'Tiny Audio Diffusion: Waveform Diffusion That Doesn’t Require Cloud Computing'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/tiny-audio-diffusion-ddc19e90af9b?source=collection_archive---------3-----------------------#2023-06-29](https://towardsdatascience.com/tiny-audio-diffusion-ddc19e90af9b?source=collection_archive---------3-----------------------#2023-06-29)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/cb135c405e7b88027dfc7c37b2d82f30.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
- en: Generated with Stable Diffusion
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: Exploring how to train models and generate sounds with audio waveform diffusion
    on a consumer laptop and GPU with less than 2GB VRAM
  id: totrans-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@crlandschoot?source=post_page-----ddc19e90af9b--------------------------------)[![Christopher
    Landschoot](../Images/99a2569f5a6a3a99fd1f72553aa3d634.png)](https://medium.com/@crlandschoot?source=post_page-----ddc19e90af9b--------------------------------)[](https://towardsdatascience.com/?source=post_page-----ddc19e90af9b--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----ddc19e90af9b--------------------------------)
    [Christopher Landschoot](https://medium.com/@crlandschoot?source=post_page-----ddc19e90af9b--------------------------------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: ·
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fb64548f914a5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftiny-audio-diffusion-ddc19e90af9b&user=Christopher+Landschoot&userId=b64548f914a5&source=post_page-b64548f914a5----ddc19e90af9b---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----ddc19e90af9b--------------------------------)
    ·11 min read·Jun 29, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fddc19e90af9b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftiny-audio-diffusion-ddc19e90af9b&user=Christopher+Landschoot&userId=b64548f914a5&source=-----ddc19e90af9b---------------------clap_footer-----------)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: --
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fddc19e90af9b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftiny-audio-diffusion-ddc19e90af9b&source=-----ddc19e90af9b---------------------bookmark_footer-----------)'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: Background
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Diffusion models are all the rage currently, especially since [Stable Diffusion](https://stability.ai/blog/stable-diffusion-public-release)
    took the world by storm this past summer. Since then, countless variations and
    new diffusion models have been published in a wide variety of contexts. And while
    the stunning visuals have stolen the spotlight, there has been significant development
    with diffusion related to generative audio.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: Fueled by diffusion and other methods, generative music has seen many recent
    triumphs, as new models are published all the time. OpenAI wowed the world with
    the capabilities of [Jukebox](https://openai.com/research/jukebox) when it was
    released in 2020\. But Google said “Hold my model” when it produced the remarkable
    [MusicLM](https://google-research.github.io/seanet/musiclm/examples/) at the beginning
    of this year. Meta was not far behind when they released and open-sourced [MusicGen](https://ai.honu.io/papers/musicgen/)
    this past month. But large institutions are not the only ones joining in, as there
    have been very interesting contributions from independent researchers such as
    [Riffusion](https://www.riffusion.com/) (Forsgren & Martiros) and [Moûsai](https://arxiv.org/abs/2301.11757)
    (Schneider, et al.). On top of these, numerous other models have been released
    within the last few years, all having their benefits and drawbacks.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在扩散和其他方法的推动下，生成音乐取得了许多最近的胜利，因为新的模型不断发布。OpenAI在2020年发布的[Jukebox](https://openai.com/research/jukebox)让世界惊叹不已。然而，当Google在今年年初推出了卓越的[MusicLM](https://google-research.github.io/seanet/musiclm/examples/)时，却让人们惊呼“让我来展示我的模型”。Meta也紧随其后，上个月发布并开源了[MusicGen](https://ai.honu.io/papers/musicgen/)。但大型机构并不是唯一参与者，独立研究者如[Riffusion](https://www.riffusion.com/)（Forsgren
    & Martiros）和[Moûsai](https://arxiv.org/abs/2301.11757)（Schneider等）也做出了非常有趣的贡献。除此之外，过去几年还发布了许多其他模型，每个模型都有其优缺点。
- en: Diffusion models have captivated so many due to their remarkable creative ability;
    something that many other genres of machine learning (ML) lack. Most ML models
    are trained to perform a task and their success can be measured by being correct
    vs incorrect. But when we enter the realm of art and music, how can a model be
    optimized to what might be considered best? It could of course learn to reproduce
    famous art or music, but without novelty, there is no point. So how can this problem
    be solved — to inject creativity into a machine that only knows 1s and 0s? Diffusion
    is one method that offers an elegant solution to this quandary.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 扩散模型由于其非凡的创造力吸引了众多关注；这是许多其他机器学习（ML）领域所缺乏的。大多数ML模型被训练来执行特定任务，其成功可以通过正确与错误来衡量。但当我们进入艺术和音乐的领域时，如何优化模型以达到可能被认为是最佳的效果？当然，它可以学习重现著名的艺术作品或音乐，但没有新颖性是没有意义的。那么如何解决这个问题——将创造力注入到只懂得1和0的机器中呢？扩散是一种为这个难题提供优雅解决方案的方法。
- en: Diffusion — From 10,000 feet
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 扩散 — 从10,000英尺的视角
- en: At its core, diffusion in ML is simply the process of adding or removing noise
    from a signal (think of static from an old TV). *Forward diffusion* adds noise
    to a signal and *reverse diffusion* removes noise. The process we are most familiar
    with is the reverse diffusion process, where the model takes in noise and then
    “denoises” it into something that humans recognize (art, music, speech, etc.).
    This process can be manipulated in a myriad of ways to serve different purposes.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 从本质上讲，ML中的扩散只是从信号中添加或移除噪声的过程（想象一下旧电视的静态噪声）。*正向扩散*向信号添加噪声，而*反向扩散*则移除噪声。我们最熟悉的过程是反向扩散过程，在这个过程中，模型接收噪声，然后将其“去噪”成人类能够识别的东西（艺术、音乐、语音等）。这个过程可以通过多种方式进行操作，以服务于不同的目的。
- en: The “creativity” in diffusion comes from the random noise that initiates the
    denoising process. If you are providing the model with a different starting point
    every time to denoise into some form of art or music, this simulates creativity
    as the outputs will always be unique.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 扩散中的“创造力”来自于启动去噪过程的随机噪声。如果你每次都为模型提供不同的起点，以去噪成某种形式的艺术或音乐，这就模拟了创造力，因为输出结果总是独一无二的。
- en: '![](../Images/d390fdb920543ffc647552be467be026.png)![](../Images/da2a9802ace0b07cb4c850129bfa1f1e.png)![](../Images/69ab8f107bc959b1047d3b3e17dc73a3.png)![](../Images/6cfa650cde2b0f07f713f37780def651.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d390fdb920543ffc647552be467be026.png)![](../Images/da2a9802ace0b07cb4c850129bfa1f1e.png)![](../Images/69ab8f107bc959b1047d3b3e17dc73a3.png)![](../Images/6cfa650cde2b0f07f713f37780def651.png)'
- en: Images Generated with Stable Diffusion
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 使用稳定扩散生成的图像
- en: The method of teaching a model to perform this denoising process may actually
    be a bit counter-intuitive from an initial thought. The model actually learns
    to denoise a signal by doing the exact opposite — adding noise to a clean signal
    over and over again until only noise remains. The idea is that if the model can
    learn how to predict the noise added to a signal at each step, then it can also
    predict the noise removed at each step for the reverse process. The critical element
    to make this possible is that the noise being added/removed needs to be of a defined
    probabilistic distribution (typically Gaussian) so that the noising/denoising
    steps are predictable and repeatable.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: There is far more detail that goes into this process, but this should provide
    a sound conceptual understanding of what is happening under the hood. If you are
    interested in learning more about diffusion models (mathematical formulations,
    scheduling, latent space, etc.), I recommend reading this [blog post](https://www.assemblyai.com/blog/diffusion-models-for-machine-learning-introduction/)
    by AssemblyAI and these papers ([DDPM](https://arxiv.org/abs/2006.11239), [Improving
    DDPM](https://arxiv.org/abs/2102.09672), [DDIM](https://arxiv.org/abs/2010.02502),
    [Stable Diffusion](https://arxiv.org/abs/2112.10752)).
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: Tiny Audio Diffusion
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Understanding Audio for Machine Learning
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: My interest in diffusion stems from the potential that it has shown with generative
    audio. Traditionally, to train ML algorithms, audio was converted into a spectrogram,
    which is basically a heatmap of sound energy over time. This was because a spectrogram
    representation was similar to an image, which computers are exceptional at working
    with, and it was a significant reduction in data size compared to a raw waveform.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6658a340cb4dbb4904b280d3dd4bd78c.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
- en: Example Spectrogram of a Vocalist
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: However, with this transformation come some tradeoffs, including a reduction
    of resolution and a loss of *phase* information. The phase of an audio signal
    represents the position of multiple waveforms relative to one another. This can
    be demonstrated in the difference between a sine and a cosine function. They represent
    the same exact signal regarding amplitude, the only difference is a 90° (π/2 radians)
    phase shift between the two. For a more in-depth explanation of phase, check out
    this [video](https://www.youtube.com/watch?v=0XdJTipTnjM&t=318s) by [Akash Murthy](https://www.youtube.com/@akashmurthy).
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a7b7cb84b9dd5d4946d07ff57a005a8c.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
- en: 90° phase shift between *sin* and cos
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: Phase is a perpetually challenging concept to grasp, even for those who work
    in audio, but it plays a critical role in creating the timbral qualities of sound.
    Suffice it to say that it should not be discarded so easily. Phase information
    can also technically be represented in spectrogram form (the complex portion of
    the transform), just like magnitude. However, the result is noisy and visually
    appears random, making it challenging for a model to learn any useful information
    from it. Because of this drawback, there has been recent interest in refraining
    from transforming audio into spectrograms and rather leaving it as a raw waveform
    to train models. While this brings its own set of challenges, both the amplitude
    and phase information are contained within the single signal of a waveform, providing
    a model with a more holistic picture of sound to learn from.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a041d57a2a78ae32cc291324b9302d9b.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
- en: Example Waveform of a Vocalist
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: This is a key piece of my interest in waveform diffusion, and it has shown promise
    in yielding high-quality results for generative audio. Waveforms, however, are
    very dense signals requiring a significant amount of data to represent the range
    of frequencies humans can hear. For example, the music industry standard sampling
    rate is 44.1kHz, which means that 44,100 samples are required to represent just
    1 second of mono audio. Now double that for stereo playback. Because of this,
    most waveform diffusion models (that don’t leverage latent diffusion or other
    compression methods) require high GPU capacity (usually at least 16GB+ VRAM) to
    store all of the information while being trained.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: Motivation
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Many people do not have access to high-powered, high-capacity GPUs, or do not
    want to pay the fee to rent cloud GPUs for personal projects. Finding myself in
    this position, but still wanting to explore waveform diffusion models, I decided
    to develop a waveform diffusion system that could run on my meager local hardware.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: Hardware Setup
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: I was equipped with an HP Spectre laptop from 2017 with an 8th Gen i7 processor
    and GeForce MX150 graphics card with 2GB VRAM — not what you would call a powerhouse
    for training ML models. My goal was to be able to create a model that could train
    and produce high-quality (44.1kHz) stereo outputs on this system.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: Model Architecture
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: I leveraged Archinet’s [audio-diffusion-pytorch](https://github.com/archinetai/audio-diffusion-pytorch)
    library to build this model — thank you to [Flavio Schneider](https://github.com/flavioschneider)
    for his help working with this library that he largely built.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: Attention U-Net
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The base model architecture consists of a U-Net with attention blocks which
    is standard for modern diffusion models. A U-Net is a neural network that was
    originally developed for image (2D) segmentation but has been adapted to audio
    (1D) for our uses with waveform diffusion. The U-Net architecture gets its name
    from its U-shaped design.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/342f2c88bc2f14bea027a44026df8a50.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
- en: 'U-Net (Source: [*U-Net: Convolutional Networks for Biomedical Image Segmentation
    (Ronneberger, et. al)*](https://arxiv.org/abs/1505.04597v1)*)*'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: Very similar to an autoencoder, consisting of an encoder and a decoder, a U-Net
    also contains skip connections at each level of the network. These skip connections
    are direct connections between corresponding layers of the encoder and decoder,
    facilitating the transfer of fine-grained details from the encoder to the decoder.
    The encoder is responsible for capturing the important features of the input signal,
    while the decoder is responsible for generating the new audio sample. The encoder
    gradually reduces the resolution of the input audio, extracting features at different
    levels of abstraction. The decoder then takes these features and upsamples them,
    gradually increasing the resolution to generate the final audio sample.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1711cafeee3a27e21db179a32bc624f7.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
- en: 'Attention U-Net (Source: [Attention U-Net: Learning Where to Look for the Pancreas
    (](https://arxiv.org/abs/1804.03999v3)[Oktay, et al.)](https://arxiv.org/search/cs?searchtype=author&query=Oktay%2C+O))'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: This U-Net also contains self-attention blocks at the lower levels which help
    maintain the temporal consistency of the output. It is critical for the audio
    to be downsampled sufficiently to maintain efficiency for sampling during the
    diffusion process as well as avoid overloading the attention blocks. The model
    leverages [V-Diffusion](https://arxiv.org/abs/2202.00512) which is a diffusion
    technique inspired by [DDIM](https://arxiv.org/abs/2010.02502v4) sampling.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: To avoid running out of GPU VRAM, the length of the data that the base model
    was to be trained on needed to be short. Because of this, I decided to train one-shot
    drum samples due to their inherently short context lengths. After many iterations,
    the base model length was determined to be 32,768 samples @ 44.1kHz in stereo,
    which results in approximately 0.75 seconds. This may seem particularly short,
    but it is plenty of time for most drum samples.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: Transforms
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To downsample the audio enough for the attention blocks, several pre-processing
    transforms were attempted. The hope was that if the audio data could be downsampled
    without losing significant information prior to training the model, then the number
    of nodes (neurons) and layers could be maximized without increasing the GPU memory
    load.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: The first transform attempted was a version of “patching”. Originally proposed
    for [images](https://arxiv.org/abs/2207.04316v1), this process was adapted to
    audio for our purposes. The input audio sample is grouped by sequential time steps
    into chunks that are then transposed into channels. This process could then be
    reversed at the output of the U-Net to un-chunk the audio back to its full length.
    The un-chunking process created aliasing issues, however, resulting in undesirable
    high frequency artifacts in the generated audio.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: The second transform attempted, proposed by [Schneider](https://arxiv.org/abs/2301.13267),
    is called a “Learned Transform” which consists of single convolutional blocks
    with large kernel sizes and strides at the start and end of the U-Net. Multiple
    kernel sizes and strides were attempted (16, 32, 64) coupled with accompanying
    model variations to appropriately downsample the audio. Again, however, this resulted
    in aliasing issues in the generated audio, though not as prevalent as the patching
    transform.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: Because of this, I decided that the model architecture would need to be adjusted
    to accommodate the raw audio with no pre-processing transforms to produce sufficient
    quality outputs.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: This required extending the number of layers within the U-Net to avoid downsampling
    too quickly and losing important features along the way. After multiple iterations,
    the best architecture resulted in downsampling by only 2 at each layer. While
    this required a reduction in the number of nodes per layer, it ultimately produced
    the best results. Detailed information about the exact number of U-Net levels,
    layers, nodes, attention features, etc. can be found in the [configuration file](https://github.com/crlandsc/tiny-audio-diffusion/blob/main/exp/drum_diffusion.yaml)
    in the [tiny-audio-diffusion](https://github.com/crlandsc/tiny-audio-diffusion)
    repository on GitHub.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: '**Conclusion**'
  id: totrans-54
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Pre-Trained Models
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: I trained 4 separate unconditional models to produce kicks, snare drums, hi-hats,
    and percussion (all drum sounds). The datasets used for training were small free
    one-shot samples that I had collected for my music production workflows (all open-source).
    Larger, more varied datasets would improve the quality and diversity of each model’s
    generated outputs. The models were trained for a various number of steps and epochs
    depending on the size of each dataset.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: Pre-trained models are available for download on [Hugging Face](https://huggingface.co/crlandsc).
    See the training progress and output samples logged at [Weights & Biases](https://wandb.ai/crlandsc/unconditional-drum-diffusion?workspace=user-crlandsc).
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: Results
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Overall, the quality of the output is quite high in spite of the reduced size
    of the models. However, there is still some slight high frequency “hiss” remaining,
    which is likely due to the limited size of the model. This can be seen in the
    small amount of noise remaining in the waveforms below. Most samples generated
    are crisp, maintaining transients and broadband timbral characteristics. Sometimes
    the models add extra noise toward the end of the sample, and this is likely a
    cost of the limit of layers and nodes of the model.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: Listen to some output samples from the models [here](https://github.com/crlandsc/tiny-audio-diffusion/tree/main/samples).
    Example outputs from each model are shown below.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a245362428366e2fbeed7762e42cd95f.png)![](../Images/4904fb3abcd46cc221d80e835efb218a.png)![](../Images/fc39d4bcbd5ee7c8edf2c314b011e0e5.png)![](../Images/91ce2a034476c2b4afcf0ef16e6c2abd.png)![](../Images/8fb1eb610bebfa2703540c9a8d89086d.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a245362428366e2fbeed7762e42cd95f.png)![](../Images/4904fb3abcd46cc221d80e835efb218a.png)![](../Images/fc39d4bcbd5ee7c8edf2c314b011e0e5.png)![](../Images/91ce2a034476c2b4afcf0ef16e6c2abd.png)![](../Images/8fb1eb610bebfa2703540c9a8d89086d.png)'
- en: Discussion
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 讨论
- en: Along with exploring waveform diffusion models on my local hardware, an important
    goal for this project was to be able to share that same opportunity with others.
    I wanted to offer an easy entry point for those with limited resources that were
    looking to experiment with audio waveform diffusion. Because of this, I structured
    the project repository to offer step-by-step instructions on how to train or fine-tune
    your own models as well as generate new samples from the [Inference.ipynb](https://github.com/crlandsc/tiny-audio-diffusion/blob/main/Inference.ipynb)
    notebook.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 除了在本地硬件上探索波形扩散模型外，这个项目的重要目标之一是能够将这一机会分享给其他人。我希望为那些资源有限但希望尝试音频波形扩散的人提供一个简单的入口。因此，我将项目库结构化，提供了如何训练或微调自己的模型以及如何从[Inference.ipynb](https://github.com/crlandsc/tiny-audio-diffusion/blob/main/Inference.ipynb)笔记本生成新样本的逐步说明。
- en: In addition, I recorded a [Tutorial Video](https://youtu.be/m6Eh2srtTro) that
    walks through setting up an Anaconda environment and demonstrates ways to generate
    unique samples with the pre-trained models.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我录制了一个[Tutorial Video](https://youtu.be/m6Eh2srtTro)，展示了如何设置Anaconda环境，并演示了使用预训练模型生成独特样本的方法。
- en: It is an exciting time for generative audio, especially with diffusion. I have
    learned an immense amount through building this project and have further expanded
    my optimism about what is to come in audio AI. I hope that this project can be
    of use to others looking to explore the world of audio AI as well.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 这是生成音频特别是扩散领域的激动人心的时刻。我在构建这个项目的过程中学到了很多，也进一步扩展了我对音频AI未来的乐观态度。我希望这个项目能对那些想探索音频AI世界的人有所帮助。
- en: '*All images, unless otherwise noted, are by the author.*'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '*所有图片，除非另有说明，均由作者提供。*'
- en: '**tiny-audio-diffusion code found here:** [**https://github.com/crlandsc/tiny-
    audio-diffusion**](https://github.com/crlandsc/tiny-audio-diffusion)'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '**tiny-audio-diffusion 代码可以在这里找到：** [**https://github.com/crlandsc/tiny-audio-diffusion**](https://github.com/crlandsc/tiny-audio-diffusion)'
- en: 'Tutorial video on setting up your environment to generate samples with tiny-audio-diffusion:
    [https://youtu.be/m6Eh2srtTro](https://youtu.be/m6Eh2srtTro)'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 设置环境以使用tiny-audio-diffusion生成样本的教程视频：[https://youtu.be/m6Eh2srtTro](https://youtu.be/m6Eh2srtTro)
- en: I am an audio scientist with a focus on AI/ML and spatial audio as well as a
    lifelong musician. If you are interested in more audio AI applications, see my
    recent article on [Music Demixing](https://medium.com/rock-nheavy/the-music-demixing-ai-revolution-9d1528c6ef7c).
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 我是一名专注于AI/ML和空间音频的音频科学家，同时也是一名终身音乐家。如果你对更多音频AI应用感兴趣，请参阅我最近的文章：[音乐去混响](https://medium.com/rock-nheavy/the-music-demixing-ai-revolution-9d1528c6ef7c)。
- en: 'Find me on [LinkedIn](https://www.linkedin.com/in/christopher-landschoot/)
    & [GitHub](https://github.com/crlandsc) and keep up to date with my current work
    and research here: [www.chrislandschoot.com](https://www.chrislandschoot.com/)'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在[LinkedIn](https://www.linkedin.com/in/christopher-landschoot/)和[GitHub](https://github.com/crlandsc)上找到我，并通过这里的[www.chrislandschoot.com](https://www.chrislandschoot.com/)跟进我当前的工作和研究。
- en: Find my music on [Spotify](https://open.spotify.com/artist/2i6noWJnJQPXPsudoiJuMS?si=fvLOxUPqTAWKB894WXMz5Q),
    [Apple Music](https://music.apple.com/us/artist/after-august/259370281), [YouTube](https://www.youtube.com/AfterAugust),
    [SoundCloud](https://soundcloud.com/after-august), and other streaming platforms
    as [After August](https://www.instagram.com/the_after_august/).
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在[Spotify](https://open.spotify.com/artist/2i6noWJnJQPXPsudoiJuMS?si=fvLOxUPqTAWKB894WXMz5Q)、[Apple
    Music](https://music.apple.com/us/artist/after-august/259370281)、[YouTube](https://www.youtube.com/AfterAugust)、[SoundCloud](https://soundcloud.com/after-august)以及其他流媒体平台上以[After
    August](https://www.instagram.com/the_after_august/)的名字找到我的音乐。
