- en: How to Build ML Applications on the AWS Cloud with Kubernetes and oneAPI
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/how-to-build-distributed-ml-applications-on-the-aws-cloud-with-kubernetes-and-oneapi-81535012d136?source=collection_archive---------9-----------------------#2023-03-17](https://towardsdatascience.com/how-to-build-distributed-ml-applications-on-the-aws-cloud-with-kubernetes-and-oneapi-81535012d136?source=collection_archive---------9-----------------------#2023-03-17)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/57caeb13d291c9b760f62f46ef459972.png)'
  prefs: []
  type: TYPE_IMG
- en: '[Image Source](https://www.freepik.com/free-photo/business-network-background-connecting-dots-technology-design_21629783.htm#query=distributed&position=1&from_view=search&track=sph)'
  prefs: []
  type: TYPE_NORMAL
- en: Learn the basics of Kubernetes and Intel AI Analytics Toolkit for building distributed
    ML Apps
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://eduand-alvarez.medium.com/?source=post_page-----81535012d136--------------------------------)[![Eduardo
    Alvarez](../Images/afa0ad855c8ec2e977ebbe60dc3e77a4.png)](https://eduand-alvarez.medium.com/?source=post_page-----81535012d136--------------------------------)[](https://towardsdatascience.com/?source=post_page-----81535012d136--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----81535012d136--------------------------------)
    [Eduardo Alvarez](https://eduand-alvarez.medium.com/?source=post_page-----81535012d136--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fe49cc416a8ef&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-build-distributed-ml-applications-on-the-aws-cloud-with-kubernetes-and-oneapi-81535012d136&user=Eduardo+Alvarez&userId=e49cc416a8ef&source=post_page-e49cc416a8ef----81535012d136---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----81535012d136--------------------------------)
    ·12 min read·Mar 17, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F81535012d136&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-build-distributed-ml-applications-on-the-aws-cloud-with-kubernetes-and-oneapi-81535012d136&user=Eduardo+Alvarez&userId=e49cc416a8ef&source=-----81535012d136---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F81535012d136&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-build-distributed-ml-applications-on-the-aws-cloud-with-kubernetes-and-oneapi-81535012d136&source=-----81535012d136---------------------bookmark_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: Building and deploying high-performance AI applications can be a challenging
    task that requires a significant amount of computing resources and expertise.
    Fortunately, modern technologies such as Kubernetes, Docker, and the Intel AI
    Analytics Toolkit ([AI Kit](https://www.intel.com/content/www/us/en/developer/tools/oneapi/ai-analytics-toolkit.html))
    make it easier to develop and deploy AI applications optimized for performance
    and scalability. Moreover, by using cloud services like Amazon Web Services (AWS),
    developers can further streamline the process and take advantage of the flexible
    and scalable infrastructure provided by the cloud.
  prefs: []
  type: TYPE_NORMAL
- en: In this article, we will explore how to use Kubernetes, Docker, and the Intel
    AI Analytics Toolkit to build and deploy AI applications on the AWS cloud. Specifically,
    we will focus on one of the first Intel Cloud Optimization Modules, which serves
    as a template with codified Intel accelerations covering various AI workloads.
    We will also introduce the AWS services that we will use in the process, including
    Amazon Elastic Kubernetes Service (EKS), Amazon Elastic Container Registry (ECR),
    Amazon Elastic Compute Cloud (EC2), and Elastic Load Balancer (ELB).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/313ad6f84a198846fa6e738f16598c53.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1\. This architecture is designed for AI production scenarios where many
    discrete models must be trained with low-moderate compute requirements. — Image
    by Author
  prefs: []
  type: TYPE_NORMAL
- en: The sample application that we will deploy focuses on Loan Default prediction,
    a common problem in the finance industry. We will use the daal4Py library to accelerate
    the inference of an XGBoost Classifier, enabling us to achieve high performance
    while reducing the time required to train and deploy the model.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3fe30f2483e071b9ca73b42b2cc0e884.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2\. A simplified API to Intel(R) oneAPI Data Analytics Library that allows
    for fast usage of the framework suited for Data Scientists or Machine Learning
    users. Built to help provide an abstraction to Intel(R) oneAPI Data Analytics
    Library for either direct usage or integration into one’s framework. — [Image
    Source](https://pypi.org/project/daal4py/)
  prefs: []
  type: TYPE_NORMAL
- en: By the end of this article, readers will have a basic understanding of how to
    build and deploy performant AI applications on the AWS cloud using Kubernetes,
    Docker, and the Intel AI Analytics Toolkit. Additionally, they will have a practical
    example of how to leverage these technologies to accelerate the inference of a
    loan default prediction model.
  prefs: []
  type: TYPE_NORMAL
- en: '*You can find all of the source code for this tutorial in our public* [*GitHub
    Repository*](https://github.com/intel/kubernetes-intel-aws-high-availability-training)*.*'
  prefs: []
  type: TYPE_NORMAL
- en: Get your Development Environment Ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Install the AWS CLI —** The AWS CLI (Command Line Interface) tool is a command-line
    tool for managing various Amazon Web Services (AWS) resources and services.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Configure AWS Credentials using `aws configure` — learn more about setting credentials
    with aws cli [here](https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-quickstart.html).
  prefs: []
  type: TYPE_NORMAL
- en: '**Install eksctl** — eksctl is a command-line tool for creating, managing,
    and operating Kubernetes clusters on EKS.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '**Install aws-iam-configurator** — AWS IAM Authenticator is a command-line
    tool that enables users to authenticate with their Kubernetes clusters on EKS
    using their AWS IAM credentials.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '**Install kubectl** — Kubectl is a command-line tool for interacting with Kubernetes
    clusters. It allows users to deploy, inspect, and manage applications and services
    running on a Kubernetes cluster and perform various administrative tasks such
    as scaling, updating, and deleting resources.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '**Our Loan Default Prediction Application**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The application we will be deploying is based on the [Loan Default Risk Prediction
    AI Reference Kit](https://www.intel.com/content/www/us/en/developer/articles/reference-kit/loan-default-risk-prediction.html).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/26dd148be5e5ce8c62896e13d43767b4.png)'
  prefs: []
  type: TYPE_IMG
- en: '[Image Source](https://www.freepik.com/free-photo/loan-buy-house-pressure-buy-house_28492444.htm#query=loan&position=32&from_view=search&track=sph)'
  prefs: []
  type: TYPE_NORMAL
- en: 'We refactored the code from this reference solution to be more modular in support
    of our three main APIs:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Data processing** — This endpoint preprocess data and stores it in a data
    lake or another structured format. This codebase also handles the expansion of
    the dataset for benchmarking purposes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model Trainin**g — This endpoint trains an XGBoost Classifier and converts
    it to an inference-optimized daal4py format.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Inference** — This endpoint receives a payload with raw data and returns
    the loan default classification of each sample.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The directory tree below outlines the codebase’s various scripts, assets, and
    configuration files. The majority of the ML application code is in the [app/ folder](https://github.com/intel/kubernetes-intel-aws-high-availability-training/tree/main/app).
    This folder contains [loan_default](https://github.com/intel/kubernetes-intel-aws-high-availability-training/tree/main/app/loan_default)
    and [utils](https://github.com/intel/kubernetes-intel-aws-high-availability-training/tree/main/app/utils)
    packages — the loan_default package contains the server-side python modules that
    support our three main APIs. The [server.py](https://github.com/intel/kubernetes-intel-aws-high-availability-training/blob/main/app/server.py)
    script contains the FastAPI endpoint configurations, payload data models, and
    commands to start a uvicorn server.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: A deep dive into the code base is beyond the scope of this tutorial. However,
    it is worth pointing out where we leverage the daal4py to improve our inference
    performance. Inside [model.py](https://github.com/intel/kubernetes-intel-aws-high-availability-training/blob/main/app/loan_default/model.py),
    you’ll find the “train” method, which handles model training and conversion to
    daal4py format using the `d4p.get_gbt_model_from_xgboost()` function.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: In the original reference kit’s performance testing, this simple conversion
    resulted in an ~4.44x boost in performance (Figure 3).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/23ea5fadc048d805783e044d341349a6.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3\. For batch inference of size 1M, Intel® v1.4.2 offers up to a 1.34x
    speedup over stock XGBoost v0.81 and with Intel® oneDAL, up to a 4.44x speedup.
    — Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: Configuring and Launching Elastic Kubernetes Service Clusters
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Elastic Kubernetes Service is a fully managed service that makes it easy to
    deploy, manage, and scale containerized applications using Kubernetes on Amazon
    Web Services (AWS). It eliminates the need to install, operate, and scale Kubernetes
    clusters on your own infrastructure.
  prefs: []
  type: TYPE_NORMAL
- en: To launch our EKS cluster, we must first create our [cluster configuration file](https://github.com/intel/kubernetes-intel-aws-high-availability-training/blob/main/kubernetes/cluster.yaml).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'We can configure the name and region of our cluster deployment, as well as
    the version of EKS that we want to run, in our “metadata” section. Most importantly,
    we can configure basic requirements for we compute resources in the “managedNodeGroups”
    section:'
  prefs: []
  type: TYPE_NORMAL
- en: desiredCapacity — the number of nodes to scale to when your stack is created.
    In this tutorial, we will set this to 3.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: instanceType — the instance type for your nodes. This tutorial uses an **m6i.large**
    instance, a 3rd Generation Xeon (2vCPU and 8GiB). Once openly available, we recommend
    trying out the [**r7iz** instance family](https://aws.amazon.com/ec2/instance-types/r7iz/)
    to take advantage of the [Intel Advanced Matrix Extension (AMX)](https://www.intel.com/content/www/us/en/products/docs/accelerator-engines/advanced-matrix-extensions/overview.html)
    — a dedicated accelerator for deep learning workloads inside of [Intel 4th Generation
    Xeon CPUs](https://www.intel.com/content/www/us/en/newsroom/news/4th-gen-xeon-scalable-processors-max-series-cpus-gpus.html).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We execute `eksctl create cluster -f cluster.yaml` to create the Cloud Formation
    stack and provision all relevant resources. With the current configurations, this
    process should take 10 to 15 minutes. You should see a log similar to Figure 4.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ef8f259c476450da8e86b3cafdef8a77.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4\. Cloud formation log for EKS cluster provision workflow — Image by
    Author
  prefs: []
  type: TYPE_NORMAL
- en: You should run a quick test to ensure your cluster has been provisioned properly.
    Run `eksctl get cluster` to get the name of your available cluster(s), and `eksctl
    get nodegroup --cluster <cluster name>` to check on your cluster’s node group.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up all of the Kubernetes Application Resources
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s dig into launching your Kubernetes application. This process entails creating
    a namespace, a deployment manifest, and a Kubernetes service. All of these files
    are available in the [tutorial’s codebase](https://github.com/intel/kubernetes-intel-aws-high-availability-training).
  prefs: []
  type: TYPE_NORMAL
- en: '**Before moving on to this part of the tutorial, please:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[Create a docker image using the Dockerfile in the application codebase and
    push it to the Elastic Container Registry on AWS.](https://medium.com/@eduand-alvarez/creating-an-ecr-registry-and-pushing-a-docker-image-93e372e74ff7)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Create and configure your kubernetes service account to grant your application
    proper access to S3 resources.](https://eduand-alvarez.medium.com/how-to-assign-aws-service-permissions-to-kubernetes-resources-cb1e0257ca22?sk=6a2c485933741deacd4f39a46e90bf22)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/dfc1669dbd2ec96a24847ac7ca712465.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: A Kubernetes namespace is a virtual cluster that divides and isolates resources
    within a physical cluster. Let’s create a namespace called “loan-default-app”
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Now, let’s configure our Kubernetes [deployment manifest](https://github.com/intel/kubernetes-intel-aws-high-availability-training/blob/main/kubernetes/deployment.yaml).
    A Kubernetes deployment is a Kubernetes resource that allows you to declaratively
    manage a set of replica pods for a given application, ensuring that the desired
    number of replicas are running and available at all times while enabling features
    such as scaling, rolling updates, and rollbacks. It also provides an abstraction
    layer over the pods, allowing you to define your application’s desired state without
    worrying about the underlying infrastructure.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The Kubernetes deployment manifest (deployment.yaml) above defines the following:'
  prefs: []
  type: TYPE_NORMAL
- en: 'kind: Deployment — The type of Kubernetes resource'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'name: “eks-loan-default-app” — The name of our deployment'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'namespace: “loan-default-app” — The namespace that this deployment should be
    assigned to'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'app: “loan-default” — The name we assign our application'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'replicas: 3 — the number of desired copies of a pod that should be created
    and maintained at all times.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'serviceAccountName: “loan-default-service-account” — make sure this matches
    the service account you created earlier.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'topologySpreadConstraints: — helps define how pods should be distributed across
    your cluster. The current configuration will maintain an equal distribution of
    pods across available nodes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'containers: name/image — where you provide the URI for your application container
    image and assign the image a name.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Run `kubectl apply -f deployment.yaml` to create your Kubernetes deployment.
  prefs: []
  type: TYPE_NORMAL
- en: Now let’s configure our Kubernetes [service](https://github.com/intel/kubernetes-intel-aws-high-availability-training/blob/main/kubernetes/service.yaml).
    A Kubernetes service is an abstraction layer that provides a stable IP address
    and DNS name for a set of pods running the same application, enabling clients
    to access the application without needing to know the specific IP addresses of
    individual pods. It also provides a way to load-balance traffic between multiple
    replicas of the application and can be used to define ingress rules for external
    access.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The Kubernetes service manifest (service.yaml) above defines the following:'
  prefs: []
  type: TYPE_NORMAL
- en: 'kind: Service — the type of Kubernetes resource.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'name: “loan-default-service” — The name of our deployment.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'namespace: “loan-default-app” — The namespace that this Service should be assigned
    to.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'port: 8080 — The port where the service will listen to.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'targetPort: 5000 — The port the service will communicate with on the pods.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'app: “loan-default” — The name we assigned to our application'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'type: “LoadBalancer” — The type of service we selected.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Run `kubectl apply -f service.yaml` to create your Kubernetes service.
  prefs: []
  type: TYPE_NORMAL
- en: This will automatically launch an Elastic Load Balancer — a cloud service that
    distributes incoming network traffic across multiple targets, such as EC2 instances,
    containers, and IP addresses, to improve application availability and fault tolerance.
    We can use the ELB’s public DNS to make requests to our API endpoints from anywhere
    in the world.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are a few tips before moving on:'
  prefs: []
  type: TYPE_NORMAL
- en: Run `kubectl get all -n loan-default-app` to get a full overview of the Kubernetes
    resources you have provisioned. You should see your pods, services, and replica
    groups.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Run `kubectl -n loan-default-app describe pod <pod-id>` to get a detailed description
    of your pod.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you need to diagnose a specific pod’s behavior, you can start a bash shell
    inside your pod by running `kubectl exec -it <pod-id> -n loan-default-app -- bash`
    — type exit and hit enter to exit the shell.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Testing our Loan Default Prediction Kubernetes Application
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now that all of our infrastructure is in place, we can set up the data component
    of our application and test our endpoints.
  prefs: []
  type: TYPE_NORMAL
- en: We will begin by [downloading the dataset from Kaggle](https://www.kaggle.com/datasets/laotse/credit-risk-dataset).
    The dataset used for this demo is a set of 32581 simulated loans. It has 11 features,
    including customer and loan characteristics and one label, which is the outcome
    of the loan. Once we have the .csv file in our working directory, we can create
    an S3 bucket and upload are Kaggle dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '**Making HTTP Requests to our API Endpoints**'
  prefs: []
  type: TYPE_NORMAL
- en: We will be using Curl to make HTTP requests to our server. Curl allows you to
    send HTTP requests by providing a command-line interface where you can specify
    the URL, request method, headers, and data. It then handles the low-level details
    of establishing a connection, sending the request, and receiving the response,
    making it easy to automate HTTP interactions.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will start by sending a request to our data processing endpoint. This will
    create test/train files and save our preprocessing pipeline as a .sav file to
    S3\. The body of the requests requires the following parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: 'bucket: name of S3 bucket'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'key: path where your raw data is saved in S3'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'size: total samples you want to process'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'backend: options include “local” or “s3” — the codebase supports running the
    entire app locally for debugging purposes. When using the “s3” backend, the “local_path”
    and “target_path” parameters can be set to “None”.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: You can navigate to your S3 bucket in the AWS console to verify that all files
    have been properly generated (Figure 5).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/647e97e4bee81c8ff3fa920c62c4d823.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5\. S3 bucket with the outputs generated by our /data endpoint — Image
    by Author
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we are ready to train our XGBoost Classifier model. We will make a request
    to our /train endpoint, which trains our model, converts it to daal4py format,
    and saves it to S3\. The body of the requests requires the following parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: 'bucket: name of S3 bucket'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'data_key: folder path that contains processed data created by our data processing
    API'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'model_key: folder where we want to store our trained model'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'model_name: the name that we want to give our trained model'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'backend: options include “local” or “s3” — the codebase supports running the
    entire app locally for debugging purposes. When using the “s3” backend, the “local_model_path”
    and “local_data_path” parameters can be set to “None.”'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: You can navigate to your S3 bucket in the AWS console to verify that your model
    file has been created (Figure 6).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9b48629d4ddaa730c5736dfabcf9c4c8.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6\. S3 bucket with the outputs generated by our /train endpoint — Image
    by Author
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have a trained daal4py optimized XGBoost Classifier, we can make
    inference requests to our API. The /predict endpoint will return a binary classification
    of True for high default likelihood and False for low default likelihood. The
    response also includes the probability generated by the classifier. In the codebase,
    we have set anything above a 50% probability to be labeled as a high default likelihood.
    This can be adjusted to return more discretized labels like low, medium, and high
    default likelihood. The body of the requests requires the following parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: 'bucket: name of S3 bucket'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'model_name: the name of the trained model is S3'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'data_key: folder path that contains .sav processing pipeline file (should be
    the same as your processed data folder)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'model_key: folder where your trained model was saved in S3'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'sample: your model inputs as a list of dictionaries'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'backend: options include “local” or “s3” — the codebase supports running the
    entire app locally for debugging purposes. When using the “s3” backend, the “local_model_path”
    and “preprocessor_path” parameters can be set to “None”.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: You can expect a response from the server fairly quickly (Figure 7).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/110911bd912e88f92de812e1a5e3c18f.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7\. Payload and response from the /predict endpoint — Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: You can find all of the source code for this tutorial in our public [GitHub
    Repository](https://github.com/intel/kubernetes-intel-aws-high-availability-training).
    Feel free to leave a comment or [message me on LinkedIn](https://www.linkedin.com/in/eduandalv/)
    if you have any questions.
  prefs: []
  type: TYPE_NORMAL
- en: Summary and Discussion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this tutorial, we have demonstrated how to build a Kubernetes application
    on the AWS cloud based on a high-availability solution architecture. We have highlighted
    the use of Intel Xeon processors and AI Kit components to improve performance
    while enabling scale with Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: We encourage readers to watch for upcoming workshops and future Intel Cloud
    Optimization Modules (ICOMs), as leveraging the Intel optimizations in these modules
    can qualify their applications for an “Accelerated by Intel” badge.
  prefs: []
  type: TYPE_NORMAL
- en: Our goal with ICOMs is to help developers enhance the performance and scalability
    of their applications with intel software and hardware. With the increasing demand
    for high-performance cloud applications, it is crucial for developers to stay
    informed and utilize the latest technologies and tools available to them.
  prefs: []
  type: TYPE_NORMAL
- en: '***Don’t forget to follow*** [***my profile for more articles***](https://eduand-alvarez.medium.com/)
    ***like this!***'
  prefs: []
  type: TYPE_NORMAL
