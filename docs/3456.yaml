- en: 'MusicGen Reimagined: Meta’s Under-the-Radar Advances in AI Music'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: MusicGen 重新构想：Meta 在 AI 音乐领域的低调进展
- en: 原文：[https://towardsdatascience.com/musicgen-reimagined-metas-under-the-radar-advances-in-ai-music-36c1adfd13b7?source=collection_archive---------6-----------------------#2023-11-21](https://towardsdatascience.com/musicgen-reimagined-metas-under-the-radar-advances-in-ai-music-36c1adfd13b7?source=collection_archive---------6-----------------------#2023-11-21)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/musicgen-reimagined-metas-under-the-radar-advances-in-ai-music-36c1adfd13b7?source=collection_archive---------6-----------------------#2023-11-21](https://towardsdatascience.com/musicgen-reimagined-metas-under-the-radar-advances-in-ai-music-36c1adfd13b7?source=collection_archive---------6-----------------------#2023-11-21)
- en: Exploring the overlooked but remarkable progress of MusicGen
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 探索被忽视但非凡的 MusicGen 进展
- en: '[](https://medium.com/@maxhilsdorf?source=post_page-----36c1adfd13b7--------------------------------)[![Max
    Hilsdorf](../Images/01da76c553e43d5ed6b6849bdbfd00da.png)](https://medium.com/@maxhilsdorf?source=post_page-----36c1adfd13b7--------------------------------)[](https://towardsdatascience.com/?source=post_page-----36c1adfd13b7--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----36c1adfd13b7--------------------------------)
    [Max Hilsdorf](https://medium.com/@maxhilsdorf?source=post_page-----36c1adfd13b7--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@maxhilsdorf?source=post_page-----36c1adfd13b7--------------------------------)[![Max
    Hilsdorf](../Images/01da76c553e43d5ed6b6849bdbfd00da.png)](https://medium.com/@maxhilsdorf?source=post_page-----36c1adfd13b7--------------------------------)[](https://towardsdatascience.com/?source=post_page-----36c1adfd13b7--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----36c1adfd13b7--------------------------------)
    [Max Hilsdorf](https://medium.com/@maxhilsdorf?source=post_page-----36c1adfd13b7--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fd0c085a74ae8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmusicgen-reimagined-metas-under-the-radar-advances-in-ai-music-36c1adfd13b7&user=Max+Hilsdorf&userId=d0c085a74ae8&source=post_page-d0c085a74ae8----36c1adfd13b7---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----36c1adfd13b7--------------------------------)
    ·7 min read·Nov 21, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F36c1adfd13b7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmusicgen-reimagined-metas-under-the-radar-advances-in-ai-music-36c1adfd13b7&user=Max+Hilsdorf&userId=d0c085a74ae8&source=-----36c1adfd13b7---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fd0c085a74ae8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmusicgen-reimagined-metas-under-the-radar-advances-in-ai-music-36c1adfd13b7&user=Max+Hilsdorf&userId=d0c085a74ae8&source=post_page-d0c085a74ae8----36c1adfd13b7---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----36c1adfd13b7--------------------------------)
    · 7分钟阅读 · 2023年11月21日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F36c1adfd13b7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmusicgen-reimagined-metas-under-the-radar-advances-in-ai-music-36c1adfd13b7&user=Max+Hilsdorf&userId=d0c085a74ae8&source=-----36c1adfd13b7---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F36c1adfd13b7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmusicgen-reimagined-metas-under-the-radar-advances-in-ai-music-36c1adfd13b7&source=-----36c1adfd13b7---------------------bookmark_footer-----------)![](../Images/c3484466a64a8a80c37a2d3e3344d68c.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F36c1adfd13b7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmusicgen-reimagined-metas-under-the-radar-advances-in-ai-music-36c1adfd13b7&source=-----36c1adfd13b7---------------------bookmark_footer-----------)![](../Images/c3484466a64a8a80c37a2d3e3344d68c.png)'
- en: An image symbolizing how Music AI products can elevate music-making for everyone.
    Image generated through a conversation with ChatGPT and DALL-E-3.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 一张象征 Music AI 产品如何提升音乐创作的图像。图像通过与 ChatGPT 和 DALL-E-3 的对话生成。
- en: How it started…
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 事情是怎么开始的……
- en: 'In February 2023, Google made waves with their generative music AI *MusicLM*.
    At that point, two things became clear:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在2023年2月，谷歌推出了其生成音乐 AI *MusicLM*，引起了轰动。那时，两个事实变得很清楚：
- en: '**2023 would be the breakthrough year for AI-based music generation**'
  id: totrans-11
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**2023年将成为基于 AI 的音乐生成突破年**'
- en: '**A new model would overshadow MusicLM in no time**'
  id: totrans-12
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**一个新模型很快就会超越 MusicLM**'
- en: Many anticipated that the next breakthrough model would be ten times the size
    of MusicLM in terms of model parameters and training data. It would also raise
    the same ethical issues, including restricted access to the source code and the
    use of copyrighted training material.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 许多人预期下一个突破性模型在模型参数和训练数据方面将是 MusicLM 的十倍。这也会引发相同的伦理问题，包括对源代码的限制访问以及使用受版权保护的训练材料。
- en: Today, we know that only half of this was true.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 今天，我们知道这些说法只有一半是正确的。
- en: Released in June 2023, Meta’s MusicGen model brought some massive improvements,
    including…
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 发布于 2023 年 6 月，Meta 的 MusicGen 模型带来了一些巨大的改进，包括…
- en: Higher quality music output (24kHz → 32kHz)
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更高质量的音乐输出（24kHz → 32kHz）
- en: More natural-sounding instruments
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更自然的乐器声音
- en: The option to condition the generation on any melody (I wrote a [blog post](https://medium.com/towards-data-science/how-metas-ai-generates-music-based-on-a-reference-melody-de34acd783)
    about this)
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 可以根据任何旋律进行生成的选项（我写了一篇关于此的 [博客文章](https://medium.com/towards-data-science/how-metas-ai-generates-music-based-on-a-reference-melody-de34acd783)）
- en: …all while using **less** training data, open-sourcing the code and model weights,
    and using only commercially licensed training material.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: …同时使用**更少**的训练数据，开源代码和模型权重，仅使用商业授权的训练材料。
- en: Six months later, the hype has slowly subsided. However, Meta’s research team
    *FAIR* has continued publishing papers and updating the code to incrementally
    improve MusicGen.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 六个月后，炒作逐渐平息。然而，Meta 的研究团队*FAIR*继续发表论文并更新代码，以逐步改进 MusicGen。
- en: … how it’s going
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: …当前的进展
- en: 'Since its release, Meta has upgraded MusicGen in two key ways:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 自发布以来，Meta 在两个关键方面升级了 MusicGen：
- en: Higher quality generation using multi-band diffusion
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用多频段扩散进行更高质量的生成
- en: More lively outputs thanks to stereo generation
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 由于立体声生成，输出更加生动
- en: 'While this may sound like two small improvements, it makes a big difference.
    Listen for yourself! Here is a 10-second piece generated with the original MusicGen
    model (3.3B parameters):'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这听起来像是两个小改进，但差别很大。自己听听吧！以下是使用原始 MusicGen 模型（3.3B 参数）生成的 10 秒钟片段：
- en: Generated track taken from the official MusicGen [demo page](https://ai.honu.io/papers/musicgen/).
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 从官方 MusicGen [演示页面](https://ai.honu.io/papers/musicgen/) 提取的生成曲目。
- en: 'The prompt used was:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 使用的提示是：
- en: earthy tones, environmentally conscious, ukulele-infused, harmonic, breezy,
    easygoing, organic instrumentation, gentle grooves
  id: totrans-28
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 具有自然质感、环保意识、尤克里里融入、和谐、轻快、随意、自然乐器、柔和的节奏
- en: 'Now, here is an example of the output MusicGen can produce six months later
    based on the same prompt:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，这里有一个基于相同提示生成的 MusicGen 输出示例，展示了六个月后的效果：
- en: Generated track created with MusicGen 3.3B stereo by the author.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 MusicGen 3.3B 立体声生成的曲目由作者创作。
- en: If you are listening through smartphone speakers, the difference might not be
    very noticeable. On other devices, you should be able to hear that the overall
    sound is much clearer and natural and that the stereo sound makes the composition
    more lively and exciting.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你通过智能手机扬声器收听，可能不会很明显。使用其他设备时，你应该能够听出整体声音更加清晰自然，立体声使得作品更加生动和令人兴奋。
- en: In this blog post, I want to showcase these improvements, explain why they matter
    and how they work, and provide some example generations.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇博客文章中，我想展示这些改进，解释它们的重要性和工作原理，并提供一些示例生成。
- en: Multi-Band Diffusion — What Does That Do?
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 多频段扩散——这有什么用？
- en: To understand what multi-band diffusion is and why it makes a difference, let
    us look at how the original MusicGen model **[1]** produced its outputs.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解多频段扩散是什么及其重要性，让我们看看原始 MusicGen 模型 **[1]** 是如何生成其输出的。
- en: 30 seconds of audio at a sample rate of 34kHz are represented in a computer
    with almost 1 million numbers. Generating something like that sample-by-sample
    is comparable to generating 10 full novels with ChatGPT.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 34kHz 采样率的 30 秒音频在计算机中表示为近 100 万个数字。逐样生成这样的音频相当于使用 ChatGPT 生成 10 部完整的小说。
- en: Instead, Meta relies on neural audio compression techniques. Their compression
    model, EnCodec **[2]**, can compress music from 34kHz to roughly 0.05kHz, all
    while maintaining the relevant information to reconstruct it to the original sample
    rate. EnCodec consists of an encoder, which compresses the audio, and a decoder,
    which recreates the original sounds (*Figure 1*).
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，Meta 依赖于神经音频压缩技术。他们的压缩模型 EnCodec **[2]** 可以将音乐从 34kHz 压缩到大约 0.05kHz，同时保持重建到原始采样率所需的相关信息。EnCodec
    包括一个编码器，用于压缩音频，以及一个解码器，用于重建原始声音（*图 1*）。
- en: '![](../Images/9a4432d91696820637aea7862ec61b7c.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9a4432d91696820637aea7862ec61b7c.png)'
- en: 'Figure 1 — Encodec: Meta’s neural audio compression model. Image by author.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 图1 — Encodec：Meta的神经音频压缩模型。图片作者提供。
- en: Now back to MusicGen. Instead of generating music at full sample rate, it generates
    it at 0.05kHz and lets EnCodec “reconstruct” it, resulting in high-fidelity outputs
    at minimal computing time & cost (*Figure 2*).
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 现在回到MusicGen。它不是以全采样率生成音乐，而是以0.05kHz生成，并让EnCodec“重建”，从而在最小的计算时间和成本下实现高保真输出（*图2*）。
- en: '![](../Images/98f254e3ca2879f08d9c6f73760b1225.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/98f254e3ca2879f08d9c6f73760b1225.png)'
- en: 'Figure 2 — MusicGen: A user prompt (text) is converted to an encoded audio
    signal which is then decoded to produce the final result. Image by author.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 图2 — MusicGen：用户提示（文本）被转换为编码音频信号，然后解码以产生最终结果。图片作者提供。
- en: While EnCodec is an impressive technology, its compression is not lossless.
    There are noticeable artifacts in the reconstructed audio compared to the original.
    Listen for yourselves!
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管EnCodec是一项令人印象深刻的技术，但其压缩并非无损。与原始音频相比，重建后的音频中存在明显的伪影。请亲自聆听！
- en: '**Original Audio**'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '**原始音频**'
- en: EnCodec music example taken from the official [EnCodec demo page](https://ai.honu.io/papers/encodec/samples.html).
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: EnCodec音乐示例取自官方的[EnCodec演示页面](https://ai.honu.io/papers/encodec/samples.html)。
- en: '**Reconstructed Audio**'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '**重建音频**'
- en: EnCodec music example taken from the official [EnCodec demo page](https://ai.honu.io/papers/encodec/samples.html).
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: EnCodec音乐示例取自官方的[EnCodec演示页面](https://ai.honu.io/papers/encodec/samples.html)。
- en: As MusicGen fully relies on EnCodec, it is a major bottleneck for the quality
    of the generated music. That is why Meta decided to work on improving EnCodec’s
    decoder part. In August 2023, they had developed an updated decoder for EnCodec
    leveraging multi-band diffusion **[3]**.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 由于MusicGen完全依赖于EnCodec，因此它是生成音乐质量的主要瓶颈。这就是为什么Meta决定改进EnCodec的解码部分。2023年8月，他们开发了一个更新的解码器，利用了多频带扩散**[3]**。
- en: One problem Meta saw with EnCodec’s original decoder was that it tended to generate
    low frequencies first and higher frequencies after. Unfortunately, this meant
    that any errors/artifacts in the low frequencies would distort the high frequencies
    as well, drastically decreasing the output quality.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: Meta发现EnCodec的原始解码器有一个问题，就是它倾向于先生成低频，然后生成高频。不幸的是，这意味着低频中的任何错误/伪影也会扭曲高频，从而大幅降低输出质量。
- en: 'Multi-band diffusion addresses this problem by generating different sections
    of the frequency spectrum independently before combining them. The researchers
    found that this procedure significantly improved the generated outputs. The differences
    are clearly noticeable from my perspective. Listen to the same track with the
    original EnCodec decoder and the multi-band diffusion decoder:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 多频带扩散通过在组合之前独立生成频谱的不同部分来解决这个问题。研究人员发现，这一过程显著改善了生成的输出。从我的角度来看，差异非常明显。请用原始EnCodec解码器和多频带扩散解码器聆听相同的曲目：
- en: '**Original Decoder**'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '**原始解码器**'
- en: Generated track taken from the Multi-Band Diffusion [demo page](https://ai.honu.io/papers/mbd/).
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 生成的曲目取自多频带扩散[演示页面](https://ai.honu.io/papers/mbd/)。
- en: '**Multi-Band Diffusion Decoder**'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '**多频带扩散解码器**'
- en: Generated track taken from the Multi-Band Diffusion [demo page](https://ai.honu.io/papers/mbd/).
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 生成的曲目取自多频带扩散[演示页面](https://ai.honu.io/papers/mbd/)。
- en: One of the core issues of current text-to-music systems is that there is always
    an unnatural quality to the sounds it produces, especially for acoustical instruments.
    Multi-band diffusion makes the output sound much cleaner and more natural and
    takes MusicGen to a new level.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 当前文本到音乐系统的核心问题之一是其生成的声音总是有一种不自然的质量，尤其是对于声学乐器。多频带扩散使输出的声音更加干净自然，将MusicGen提升到了一个新的水平。
- en: Why is Stereo Sound so Significant?
  id: totrans-55
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为什么立体声如此重要？
- en: Up until now, most generative music models have been producing mono sound. This
    means MusicGen does not place any sounds or instruments on the left or right side,
    resulting in a less lively and exciting mix. The reason why stereo sound has been
    mostly overlooked so far is that generating stereo is not a trivial task.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，大多数生成音乐模型生成的是单声道声音。这意味着MusicGen不会将声音或乐器放置在左侧或右侧，从而导致混音较少生动和兴奋。立体声被大多忽视的原因是生成立体声并非一项简单的任务。
- en: As musicians, when we produce stereo signals, we have access to the individual
    instrument tracks in our mix and we can place them wherever we want. MusicGen
    does not generate all instruments separately but instead produces one combined
    audio signal. Without access to these instrument sources, creating stereo sound
    is hard. Unfortunately, splitting an audio signal into its individual sources
    is a tough problem (I’ve published a [blog post](https://medium.com/towards-data-science/ai-music-source-separation-how-it-works-and-why-it-is-so-hard-187852e54752)
    about that) and the tech is still not 100% ready.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 作为音乐家，当我们制作立体声信号时，我们可以访问混音中的各个乐器轨道，并将它们放置在我们想要的位置。MusicGen 并不是分别生成所有乐器，而是生成一个合成的音频信号。在没有这些乐器源的情况下，创建立体声音效是困难的。不幸的是，将音频信号拆分成各个源是一个棘手的问题（我已经发布了一篇
    [博客文章](https://medium.com/towards-data-science/ai-music-source-separation-how-it-works-and-why-it-is-so-hard-187852e54752)
    讨论了这个问题），而且技术仍然未完全成熟。
- en: Therefore, Meta decided to incorporate stereo generation directly into the MusicGen
    model. Using a new dataset consisting of stereo music, they trained MusicGen to
    produce stereo outputs. The researchers claim that generating stereo has no additional
    computing costs compared to mono.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，Meta 决定将立体声生成直接集成到 MusicGen 模型中。他们使用包含立体声音乐的新数据集，训练 MusicGen 生成立体声输出。研究人员声称，生成立体声与单声道相比没有额外的计算成本。
- en: 'Although I feel that the stereo procedure is not very clearly described in
    the paper, my understanding it works like this (*Figure 3*): MusicGen has learned
    to generate two compressed audio signals (left and right channel) instead of one
    mono signal. These compressed signals must then be decoded separately before they
    are combined to build the final stereo output. The reason this process does not
    take twice as long is that MusicGen can now produce two compressed audio signals
    at approximately the same time it previously took for one signal.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我觉得论文中对立体声过程的描述并不十分清楚，但我的理解是这样的（*图 3*）：MusicGen 已经学会生成两个压缩音频信号（左声道和右声道），而不是一个单声道信号。这些压缩信号必须分别解码，然后再结合起来生成最终的立体声输出。这个过程不会花费两倍的时间，因为
    MusicGen 现在可以在大约生成一个信号的时间内同时生成两个压缩音频信号。
- en: '![](../Images/48157fe5a80cc2bfa76eab5aed542464.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/48157fe5a80cc2bfa76eab5aed542464.png)'
- en: Figure 3 — MusicGen stereo update. Note that the process was not sufficiently
    documented in the paper for me to be 100% sure about this. Take it as an educated
    guess. Image by author.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3 — MusicGen 立体声更新。请注意，由于论文中对该过程的记录不够充分，我不能 100% 确定这一点。请将其视为一个有根据的猜测。图片由作者提供。
- en: 'Being able to produce convincing stereo sound really sets MusicGen apart from
    other state-of-the-art models like MusicLM or Stable Audio. From my perspective,
    this “little” addition makes a huge difference in the liveliness of the generated
    music. Listen for yourselves (might be hard to hear on smartphone speakers):'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 能够生成令人信服的立体声效果确实使 MusicGen 从其他先进模型如 MusicLM 或 Stable Audio 中脱颖而出。从我的角度来看，这个“微小”的增加在生成音乐的生动性方面产生了巨大的差异。自己听听吧（在智能手机扬声器上可能很难听到）：
- en: '**Mono**'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '**单声道**'
- en: '**Stereo**'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '**立体声**'
- en: Conclusion
  id: totrans-65
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: MusicGen was impressive from the day it was released. However, since then, Meta’s
    FAIR team has been continually improving their product, enabling higher quality
    results that sound more authentic. When it comes to text-to-music models generating
    audio signals (not MIDI etc.), MusicGen is ahead of its competitors from my perspective
    (as of November 2023).
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 自发布之日起，MusicGen 就给人留下了深刻的印象。然而，从那时起，Meta 的 FAIR 团队不断改进他们的产品，提供了更高质量、更逼真的效果。在生成音频信号（而非
    MIDI 等）的文本到音乐模型方面，我认为 MusicGen 在竞争对手中处于领先地位（截至 2023 年 11 月）。
- en: Further, since MusicGen and all its related products (EnCodec, AudioGen) are
    open-source, they constitute an incredible source of inspiration and a go-to framework
    for aspiring AI audio engineers. If we look at the improvements MusicGen has made
    in only 6 months, I can only imagine that 2024 will be an exciting year.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，由于 MusicGen 及其相关产品（EnCodec、AudioGen）都是开源的，它们成为了令人难以置信的灵感来源和有志于成为 AI 音频工程师的首选框架。如果我们看看
    MusicGen 在短短 6 个月内取得的进步，我只能想象 2024 年将是一个激动人心的年份。
- en: Another important point is that with their transparent approach, Meta is also
    doing foundational work for developers who want to integrate this technology into
    software for musicians. Generating samples, brainstorming musical ideas, or changing
    the genre of your existing work — these are some of the exciting applications
    we are already starting to see. With a sufficient level of transparency, we can
    make sure we are building a future where AI makes creating music more exciting
    instead of being only a threat to human musicianship.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个重要的点是，Meta通过其透明的方法，也为希望将这项技术集成到音乐软件中的开发人员做出了基础工作。生成样本、头脑风暴音乐创意或者改变现有作品的风格
    —— 这些是我们已经开始看到的一些令人兴奋的应用。通过足够的透明度，我们可以确保我们正在构建一个未来，使得人工智能不仅仅是对人类音乐能力的威胁，而是让音乐创作变得更加令人兴奋的未来。
- en: 'Note: While MusicGen is open-source, the pre-trained models may not be used
    commercially! Visit the audiocraft [GitHub repository](https://github.com/facebookresearch/audiocraft)
    for more detailed information on the intended use for all its components.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：虽然MusicGen是开源的，但预训练模型可能不允许商业使用！访问audiocraft的[GitHub仓库](https://github.com/facebookresearch/audiocraft)以获取有关其所有组件预期使用的更详细信息。
- en: '**References**'
  id: totrans-70
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**参考文献**'
- en: '**[1] Copet et al. (2023)**. Simple and Controllable Music Generation. [https://arxiv.org/pdf/2306.05284.pdf](https://arxiv.org/pdf/2306.05284.pdf)'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '**[1] Copet等人（2023）**。简单可控的音乐生成。[https://arxiv.org/pdf/2306.05284.pdf](https://arxiv.org/pdf/2306.05284.pdf)'
- en: '**[2] Défossez et al. (2022)**. High Fidelity Neural Audio Compression. [https://arxiv.org/pdf/2210.13438.pdf](https://arxiv.org/pdf/2210.13438.pdf)'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '**[2] Défossez等人（2022）**。高保真神经音频压缩。[https://arxiv.org/pdf/2210.13438.pdf](https://arxiv.org/pdf/2210.13438.pdf)'
- en: '**[3] Roman et al. (2023)**. From Discrete Tokens to High-Fidelity Audio Using
    Multi-Band Diffusion. [https://arxiv.org/abs/2308.02560](https://arxiv.org/abs/2308.02560)'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '**[3] Roman等人（2023）**。从离散标记到高保真音频使用多频带扩散。[https://arxiv.org/abs/2308.02560](https://arxiv.org/abs/2308.02560)'
- en: About Me
  id: totrans-74
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 关于我
- en: 'Hi there! I’m a musicologist and a data scientist, sharing my thoughts on current
    topics in AI & music. Here is some of my previous work related to this article:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 嗨！我是一名音乐学家和数据科学家，分享我对人工智能和音乐当前主题的见解。这里是我之前与本文相关的一些工作：
- en: '**How Meta’s AI Generates Music Based on a Reference Melody**: [https://medium.com/towards-data-science/how-metas-ai-generates-music-based-on-a-reference-melody-de34acd783](https://medium.com/towards-data-science/how-metas-ai-generates-music-based-on-a-reference-melody-de34acd783)'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Meta的AI如何基于参考旋律生成音乐**：[https://medium.com/towards-data-science/how-metas-ai-generates-music-based-on-a-reference-melody-de34acd783](https://medium.com/towards-data-science/how-metas-ai-generates-music-based-on-a-reference-melody-de34acd783)'
- en: '**MusicLM: Has Google Solved AI Music Generation?**: [https://medium.com/towards-data-science/musiclm-has-google-solved-ai-music-generation-c6859e76bc3c](https://medium.com/towards-data-science/musiclm-has-google-solved-ai-music-generation-c6859e76bc3c)'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**MusicLM：Google解决了AI音乐生成问题吗？**：[https://medium.com/towards-data-science/musiclm-has-google-solved-ai-music-generation-c6859e76bc3c](https://medium.com/towards-data-science/musiclm-has-google-solved-ai-music-generation-c6859e76bc3c)'
- en: '**AI Music Source Separation: How it Works and Why it is so Hard**: [https://medium.com/towards-data-science/ai-music-source-separation-how-it-works-and-why-it-is-so-hard-187852e54752](https://medium.com/towards-data-science/ai-music-source-separation-how-it-works-and-why-it-is-so-hard-187852e54752)'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**AI音乐源分离：工作原理及其难点**：[https://medium.com/towards-data-science/ai-music-source-separation-how-it-works-and-why-it-is-so-hard-187852e54752](https://medium.com/towards-data-science/ai-music-source-separation-how-it-works-and-why-it-is-so-hard-187852e54752)'
- en: Find me on [Medium](https://medium.com/@maxhilsdorf) and [Linkedin](https://www.linkedin.com/in/max-hilsdorf/)!
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在[Medium](https://medium.com/@maxhilsdorf)和[Linkedin](https://www.linkedin.com/in/max-hilsdorf/)找到我！
