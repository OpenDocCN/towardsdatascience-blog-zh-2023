- en: 'MusicGen Reimagined: Meta’s Under-the-Radar Advances in AI Music'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/musicgen-reimagined-metas-under-the-radar-advances-in-ai-music-36c1adfd13b7?source=collection_archive---------6-----------------------#2023-11-21](https://towardsdatascience.com/musicgen-reimagined-metas-under-the-radar-advances-in-ai-music-36c1adfd13b7?source=collection_archive---------6-----------------------#2023-11-21)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Exploring the overlooked but remarkable progress of MusicGen
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@maxhilsdorf?source=post_page-----36c1adfd13b7--------------------------------)[![Max
    Hilsdorf](../Images/01da76c553e43d5ed6b6849bdbfd00da.png)](https://medium.com/@maxhilsdorf?source=post_page-----36c1adfd13b7--------------------------------)[](https://towardsdatascience.com/?source=post_page-----36c1adfd13b7--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----36c1adfd13b7--------------------------------)
    [Max Hilsdorf](https://medium.com/@maxhilsdorf?source=post_page-----36c1adfd13b7--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fd0c085a74ae8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmusicgen-reimagined-metas-under-the-radar-advances-in-ai-music-36c1adfd13b7&user=Max+Hilsdorf&userId=d0c085a74ae8&source=post_page-d0c085a74ae8----36c1adfd13b7---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----36c1adfd13b7--------------------------------)
    ·7 min read·Nov 21, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F36c1adfd13b7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmusicgen-reimagined-metas-under-the-radar-advances-in-ai-music-36c1adfd13b7&user=Max+Hilsdorf&userId=d0c085a74ae8&source=-----36c1adfd13b7---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F36c1adfd13b7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmusicgen-reimagined-metas-under-the-radar-advances-in-ai-music-36c1adfd13b7&source=-----36c1adfd13b7---------------------bookmark_footer-----------)![](../Images/c3484466a64a8a80c37a2d3e3344d68c.png)'
  prefs: []
  type: TYPE_NORMAL
- en: An image symbolizing how Music AI products can elevate music-making for everyone.
    Image generated through a conversation with ChatGPT and DALL-E-3.
  prefs: []
  type: TYPE_NORMAL
- en: How it started…
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In February 2023, Google made waves with their generative music AI *MusicLM*.
    At that point, two things became clear:'
  prefs: []
  type: TYPE_NORMAL
- en: '**2023 would be the breakthrough year for AI-based music generation**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**A new model would overshadow MusicLM in no time**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Many anticipated that the next breakthrough model would be ten times the size
    of MusicLM in terms of model parameters and training data. It would also raise
    the same ethical issues, including restricted access to the source code and the
    use of copyrighted training material.
  prefs: []
  type: TYPE_NORMAL
- en: Today, we know that only half of this was true.
  prefs: []
  type: TYPE_NORMAL
- en: Released in June 2023, Meta’s MusicGen model brought some massive improvements,
    including…
  prefs: []
  type: TYPE_NORMAL
- en: Higher quality music output (24kHz → 32kHz)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: More natural-sounding instruments
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The option to condition the generation on any melody (I wrote a [blog post](https://medium.com/towards-data-science/how-metas-ai-generates-music-based-on-a-reference-melody-de34acd783)
    about this)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: …all while using **less** training data, open-sourcing the code and model weights,
    and using only commercially licensed training material.
  prefs: []
  type: TYPE_NORMAL
- en: Six months later, the hype has slowly subsided. However, Meta’s research team
    *FAIR* has continued publishing papers and updating the code to incrementally
    improve MusicGen.
  prefs: []
  type: TYPE_NORMAL
- en: … how it’s going
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Since its release, Meta has upgraded MusicGen in two key ways:'
  prefs: []
  type: TYPE_NORMAL
- en: Higher quality generation using multi-band diffusion
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: More lively outputs thanks to stereo generation
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'While this may sound like two small improvements, it makes a big difference.
    Listen for yourself! Here is a 10-second piece generated with the original MusicGen
    model (3.3B parameters):'
  prefs: []
  type: TYPE_NORMAL
- en: Generated track taken from the official MusicGen [demo page](https://ai.honu.io/papers/musicgen/).
  prefs: []
  type: TYPE_NORMAL
- en: 'The prompt used was:'
  prefs: []
  type: TYPE_NORMAL
- en: earthy tones, environmentally conscious, ukulele-infused, harmonic, breezy,
    easygoing, organic instrumentation, gentle grooves
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Now, here is an example of the output MusicGen can produce six months later
    based on the same prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: Generated track created with MusicGen 3.3B stereo by the author.
  prefs: []
  type: TYPE_NORMAL
- en: If you are listening through smartphone speakers, the difference might not be
    very noticeable. On other devices, you should be able to hear that the overall
    sound is much clearer and natural and that the stereo sound makes the composition
    more lively and exciting.
  prefs: []
  type: TYPE_NORMAL
- en: In this blog post, I want to showcase these improvements, explain why they matter
    and how they work, and provide some example generations.
  prefs: []
  type: TYPE_NORMAL
- en: Multi-Band Diffusion — What Does That Do?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To understand what multi-band diffusion is and why it makes a difference, let
    us look at how the original MusicGen model **[1]** produced its outputs.
  prefs: []
  type: TYPE_NORMAL
- en: 30 seconds of audio at a sample rate of 34kHz are represented in a computer
    with almost 1 million numbers. Generating something like that sample-by-sample
    is comparable to generating 10 full novels with ChatGPT.
  prefs: []
  type: TYPE_NORMAL
- en: Instead, Meta relies on neural audio compression techniques. Their compression
    model, EnCodec **[2]**, can compress music from 34kHz to roughly 0.05kHz, all
    while maintaining the relevant information to reconstruct it to the original sample
    rate. EnCodec consists of an encoder, which compresses the audio, and a decoder,
    which recreates the original sounds (*Figure 1*).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9a4432d91696820637aea7862ec61b7c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1 — Encodec: Meta’s neural audio compression model. Image by author.'
  prefs: []
  type: TYPE_NORMAL
- en: Now back to MusicGen. Instead of generating music at full sample rate, it generates
    it at 0.05kHz and lets EnCodec “reconstruct” it, resulting in high-fidelity outputs
    at minimal computing time & cost (*Figure 2*).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/98f254e3ca2879f08d9c6f73760b1225.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2 — MusicGen: A user prompt (text) is converted to an encoded audio
    signal which is then decoded to produce the final result. Image by author.'
  prefs: []
  type: TYPE_NORMAL
- en: While EnCodec is an impressive technology, its compression is not lossless.
    There are noticeable artifacts in the reconstructed audio compared to the original.
    Listen for yourselves!
  prefs: []
  type: TYPE_NORMAL
- en: '**Original Audio**'
  prefs: []
  type: TYPE_NORMAL
- en: EnCodec music example taken from the official [EnCodec demo page](https://ai.honu.io/papers/encodec/samples.html).
  prefs: []
  type: TYPE_NORMAL
- en: '**Reconstructed Audio**'
  prefs: []
  type: TYPE_NORMAL
- en: EnCodec music example taken from the official [EnCodec demo page](https://ai.honu.io/papers/encodec/samples.html).
  prefs: []
  type: TYPE_NORMAL
- en: As MusicGen fully relies on EnCodec, it is a major bottleneck for the quality
    of the generated music. That is why Meta decided to work on improving EnCodec’s
    decoder part. In August 2023, they had developed an updated decoder for EnCodec
    leveraging multi-band diffusion **[3]**.
  prefs: []
  type: TYPE_NORMAL
- en: One problem Meta saw with EnCodec’s original decoder was that it tended to generate
    low frequencies first and higher frequencies after. Unfortunately, this meant
    that any errors/artifacts in the low frequencies would distort the high frequencies
    as well, drastically decreasing the output quality.
  prefs: []
  type: TYPE_NORMAL
- en: 'Multi-band diffusion addresses this problem by generating different sections
    of the frequency spectrum independently before combining them. The researchers
    found that this procedure significantly improved the generated outputs. The differences
    are clearly noticeable from my perspective. Listen to the same track with the
    original EnCodec decoder and the multi-band diffusion decoder:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Original Decoder**'
  prefs: []
  type: TYPE_NORMAL
- en: Generated track taken from the Multi-Band Diffusion [demo page](https://ai.honu.io/papers/mbd/).
  prefs: []
  type: TYPE_NORMAL
- en: '**Multi-Band Diffusion Decoder**'
  prefs: []
  type: TYPE_NORMAL
- en: Generated track taken from the Multi-Band Diffusion [demo page](https://ai.honu.io/papers/mbd/).
  prefs: []
  type: TYPE_NORMAL
- en: One of the core issues of current text-to-music systems is that there is always
    an unnatural quality to the sounds it produces, especially for acoustical instruments.
    Multi-band diffusion makes the output sound much cleaner and more natural and
    takes MusicGen to a new level.
  prefs: []
  type: TYPE_NORMAL
- en: Why is Stereo Sound so Significant?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Up until now, most generative music models have been producing mono sound. This
    means MusicGen does not place any sounds or instruments on the left or right side,
    resulting in a less lively and exciting mix. The reason why stereo sound has been
    mostly overlooked so far is that generating stereo is not a trivial task.
  prefs: []
  type: TYPE_NORMAL
- en: As musicians, when we produce stereo signals, we have access to the individual
    instrument tracks in our mix and we can place them wherever we want. MusicGen
    does not generate all instruments separately but instead produces one combined
    audio signal. Without access to these instrument sources, creating stereo sound
    is hard. Unfortunately, splitting an audio signal into its individual sources
    is a tough problem (I’ve published a [blog post](https://medium.com/towards-data-science/ai-music-source-separation-how-it-works-and-why-it-is-so-hard-187852e54752)
    about that) and the tech is still not 100% ready.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, Meta decided to incorporate stereo generation directly into the MusicGen
    model. Using a new dataset consisting of stereo music, they trained MusicGen to
    produce stereo outputs. The researchers claim that generating stereo has no additional
    computing costs compared to mono.
  prefs: []
  type: TYPE_NORMAL
- en: 'Although I feel that the stereo procedure is not very clearly described in
    the paper, my understanding it works like this (*Figure 3*): MusicGen has learned
    to generate two compressed audio signals (left and right channel) instead of one
    mono signal. These compressed signals must then be decoded separately before they
    are combined to build the final stereo output. The reason this process does not
    take twice as long is that MusicGen can now produce two compressed audio signals
    at approximately the same time it previously took for one signal.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/48157fe5a80cc2bfa76eab5aed542464.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3 — MusicGen stereo update. Note that the process was not sufficiently
    documented in the paper for me to be 100% sure about this. Take it as an educated
    guess. Image by author.
  prefs: []
  type: TYPE_NORMAL
- en: 'Being able to produce convincing stereo sound really sets MusicGen apart from
    other state-of-the-art models like MusicLM or Stable Audio. From my perspective,
    this “little” addition makes a huge difference in the liveliness of the generated
    music. Listen for yourselves (might be hard to hear on smartphone speakers):'
  prefs: []
  type: TYPE_NORMAL
- en: '**Mono**'
  prefs: []
  type: TYPE_NORMAL
- en: '**Stereo**'
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: MusicGen was impressive from the day it was released. However, since then, Meta’s
    FAIR team has been continually improving their product, enabling higher quality
    results that sound more authentic. When it comes to text-to-music models generating
    audio signals (not MIDI etc.), MusicGen is ahead of its competitors from my perspective
    (as of November 2023).
  prefs: []
  type: TYPE_NORMAL
- en: Further, since MusicGen and all its related products (EnCodec, AudioGen) are
    open-source, they constitute an incredible source of inspiration and a go-to framework
    for aspiring AI audio engineers. If we look at the improvements MusicGen has made
    in only 6 months, I can only imagine that 2024 will be an exciting year.
  prefs: []
  type: TYPE_NORMAL
- en: Another important point is that with their transparent approach, Meta is also
    doing foundational work for developers who want to integrate this technology into
    software for musicians. Generating samples, brainstorming musical ideas, or changing
    the genre of your existing work — these are some of the exciting applications
    we are already starting to see. With a sufficient level of transparency, we can
    make sure we are building a future where AI makes creating music more exciting
    instead of being only a threat to human musicianship.
  prefs: []
  type: TYPE_NORMAL
- en: 'Note: While MusicGen is open-source, the pre-trained models may not be used
    commercially! Visit the audiocraft [GitHub repository](https://github.com/facebookresearch/audiocraft)
    for more detailed information on the intended use for all its components.'
  prefs: []
  type: TYPE_NORMAL
- en: '**References**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**[1] Copet et al. (2023)**. Simple and Controllable Music Generation. [https://arxiv.org/pdf/2306.05284.pdf](https://arxiv.org/pdf/2306.05284.pdf)'
  prefs: []
  type: TYPE_NORMAL
- en: '**[2] Défossez et al. (2022)**. High Fidelity Neural Audio Compression. [https://arxiv.org/pdf/2210.13438.pdf](https://arxiv.org/pdf/2210.13438.pdf)'
  prefs: []
  type: TYPE_NORMAL
- en: '**[3] Roman et al. (2023)**. From Discrete Tokens to High-Fidelity Audio Using
    Multi-Band Diffusion. [https://arxiv.org/abs/2308.02560](https://arxiv.org/abs/2308.02560)'
  prefs: []
  type: TYPE_NORMAL
- en: About Me
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Hi there! I’m a musicologist and a data scientist, sharing my thoughts on current
    topics in AI & music. Here is some of my previous work related to this article:'
  prefs: []
  type: TYPE_NORMAL
- en: '**How Meta’s AI Generates Music Based on a Reference Melody**: [https://medium.com/towards-data-science/how-metas-ai-generates-music-based-on-a-reference-melody-de34acd783](https://medium.com/towards-data-science/how-metas-ai-generates-music-based-on-a-reference-melody-de34acd783)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**MusicLM: Has Google Solved AI Music Generation?**: [https://medium.com/towards-data-science/musiclm-has-google-solved-ai-music-generation-c6859e76bc3c](https://medium.com/towards-data-science/musiclm-has-google-solved-ai-music-generation-c6859e76bc3c)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**AI Music Source Separation: How it Works and Why it is so Hard**: [https://medium.com/towards-data-science/ai-music-source-separation-how-it-works-and-why-it-is-so-hard-187852e54752](https://medium.com/towards-data-science/ai-music-source-separation-how-it-works-and-why-it-is-so-hard-187852e54752)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Find me on [Medium](https://medium.com/@maxhilsdorf) and [Linkedin](https://www.linkedin.com/in/max-hilsdorf/)!
  prefs: []
  type: TYPE_NORMAL
