- en: 'MusicGen Reimagined: Meta’s Under-the-Radar Advances in AI Music'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: MusicGen 重新构想：Meta 在 AI 音乐领域的低调进展
- en: 原文：[https://towardsdatascience.com/musicgen-reimagined-metas-under-the-radar-advances-in-ai-music-36c1adfd13b7?source=collection_archive---------6-----------------------#2023-11-21](https://towardsdatascience.com/musicgen-reimagined-metas-under-the-radar-advances-in-ai-music-36c1adfd13b7?source=collection_archive---------6-----------------------#2023-11-21)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/musicgen-reimagined-metas-under-the-radar-advances-in-ai-music-36c1adfd13b7?source=collection_archive---------6-----------------------#2023-11-21](https://towardsdatascience.com/musicgen-reimagined-metas-under-the-radar-advances-in-ai-music-36c1adfd13b7?source=collection_archive---------6-----------------------#2023-11-21)
- en: Exploring the overlooked but remarkable progress of MusicGen
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 探索被忽视但非凡的 MusicGen 进展
- en: '[](https://medium.com/@maxhilsdorf?source=post_page-----36c1adfd13b7--------------------------------)[![Max
    Hilsdorf](../Images/01da76c553e43d5ed6b6849bdbfd00da.png)](https://medium.com/@maxhilsdorf?source=post_page-----36c1adfd13b7--------------------------------)[](https://towardsdatascience.com/?source=post_page-----36c1adfd13b7--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----36c1adfd13b7--------------------------------)
    [Max Hilsdorf](https://medium.com/@maxhilsdorf?source=post_page-----36c1adfd13b7--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@maxhilsdorf?source=post_page-----36c1adfd13b7--------------------------------)[![Max
    Hilsdorf](../Images/01da76c553e43d5ed6b6849bdbfd00da.png)](https://medium.com/@maxhilsdorf?source=post_page-----36c1adfd13b7--------------------------------)[](https://towardsdatascience.com/?source=post_page-----36c1adfd13b7--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----36c1adfd13b7--------------------------------)
    [Max Hilsdorf](https://medium.com/@maxhilsdorf?source=post_page-----36c1adfd13b7--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fd0c085a74ae8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmusicgen-reimagined-metas-under-the-radar-advances-in-ai-music-36c1adfd13b7&user=Max+Hilsdorf&userId=d0c085a74ae8&source=post_page-d0c085a74ae8----36c1adfd13b7---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----36c1adfd13b7--------------------------------)
    ·7 min read·Nov 21, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F36c1adfd13b7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmusicgen-reimagined-metas-under-the-radar-advances-in-ai-music-36c1adfd13b7&user=Max+Hilsdorf&userId=d0c085a74ae8&source=-----36c1adfd13b7---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fd0c085a74ae8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmusicgen-reimagined-metas-under-the-radar-advances-in-ai-music-36c1adfd13b7&user=Max+Hilsdorf&userId=d0c085a74ae8&source=post_page-d0c085a74ae8----36c1adfd13b7---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----36c1adfd13b7--------------------------------)
    · 7分钟阅读 · 2023年11月21日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F36c1adfd13b7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmusicgen-reimagined-metas-under-the-radar-advances-in-ai-music-36c1adfd13b7&user=Max+Hilsdorf&userId=d0c085a74ae8&source=-----36c1adfd13b7---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F36c1adfd13b7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmusicgen-reimagined-metas-under-the-radar-advances-in-ai-music-36c1adfd13b7&source=-----36c1adfd13b7---------------------bookmark_footer-----------)![](../Images/c3484466a64a8a80c37a2d3e3344d68c.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F36c1adfd13b7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmusicgen-reimagined-metas-under-the-radar-advances-in-ai-music-36c1adfd13b7&source=-----36c1adfd13b7---------------------bookmark_footer-----------)![](../Images/c3484466a64a8a80c37a2d3e3344d68c.png)'
- en: An image symbolizing how Music AI products can elevate music-making for everyone.
    Image generated through a conversation with ChatGPT and DALL-E-3.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 一张象征 Music AI 产品如何提升音乐创作的图像。图像通过与 ChatGPT 和 DALL-E-3 的对话生成。
- en: How it started…
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 事情是怎么开始的……
- en: 'In February 2023, Google made waves with their generative music AI *MusicLM*.
    At that point, two things became clear:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在2023年2月，谷歌推出了其生成音乐 AI *MusicLM*，引起了轰动。那时，两个事实变得很清楚：
- en: '**2023 would be the breakthrough year for AI-based music generation**'
  id: totrans-11
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**2023年将成为基于 AI 的音乐生成突破年**'
- en: '**A new model would overshadow MusicLM in no time**'
  id: totrans-12
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**一个新模型很快就会超越 MusicLM**'
- en: Many anticipated that the next breakthrough model would be ten times the size
    of MusicLM in terms of model parameters and training data. It would also raise
    the same ethical issues, including restricted access to the source code and the
    use of copyrighted training material.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: Today, we know that only half of this was true.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: Released in June 2023, Meta’s MusicGen model brought some massive improvements,
    including…
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: Higher quality music output (24kHz → 32kHz)
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: More natural-sounding instruments
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The option to condition the generation on any melody (I wrote a [blog post](https://medium.com/towards-data-science/how-metas-ai-generates-music-based-on-a-reference-melody-de34acd783)
    about this)
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: …all while using **less** training data, open-sourcing the code and model weights,
    and using only commercially licensed training material.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: Six months later, the hype has slowly subsided. However, Meta’s research team
    *FAIR* has continued publishing papers and updating the code to incrementally
    improve MusicGen.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: … how it’s going
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Since its release, Meta has upgraded MusicGen in two key ways:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: Higher quality generation using multi-band diffusion
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: More lively outputs thanks to stereo generation
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'While this may sound like two small improvements, it makes a big difference.
    Listen for yourself! Here is a 10-second piece generated with the original MusicGen
    model (3.3B parameters):'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: Generated track taken from the official MusicGen [demo page](https://ai.honu.io/papers/musicgen/).
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: 'The prompt used was:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: earthy tones, environmentally conscious, ukulele-infused, harmonic, breezy,
    easygoing, organic instrumentation, gentle grooves
  id: totrans-28
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Now, here is an example of the output MusicGen can produce six months later
    based on the same prompt:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: Generated track created with MusicGen 3.3B stereo by the author.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: If you are listening through smartphone speakers, the difference might not be
    very noticeable. On other devices, you should be able to hear that the overall
    sound is much clearer and natural and that the stereo sound makes the composition
    more lively and exciting.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: In this blog post, I want to showcase these improvements, explain why they matter
    and how they work, and provide some example generations.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: Multi-Band Diffusion — What Does That Do?
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To understand what multi-band diffusion is and why it makes a difference, let
    us look at how the original MusicGen model **[1]** produced its outputs.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: 30 seconds of audio at a sample rate of 34kHz are represented in a computer
    with almost 1 million numbers. Generating something like that sample-by-sample
    is comparable to generating 10 full novels with ChatGPT.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: Instead, Meta relies on neural audio compression techniques. Their compression
    model, EnCodec **[2]**, can compress music from 34kHz to roughly 0.05kHz, all
    while maintaining the relevant information to reconstruct it to the original sample
    rate. EnCodec consists of an encoder, which compresses the audio, and a decoder,
    which recreates the original sounds (*Figure 1*).
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9a4432d91696820637aea7862ec61b7c.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1 — Encodec: Meta’s neural audio compression model. Image by author.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: Now back to MusicGen. Instead of generating music at full sample rate, it generates
    it at 0.05kHz and lets EnCodec “reconstruct” it, resulting in high-fidelity outputs
    at minimal computing time & cost (*Figure 2*).
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/98f254e3ca2879f08d9c6f73760b1225.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2 — MusicGen: A user prompt (text) is converted to an encoded audio
    signal which is then decoded to produce the final result. Image by author.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: While EnCodec is an impressive technology, its compression is not lossless.
    There are noticeable artifacts in the reconstructed audio compared to the original.
    Listen for yourselves!
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: '**Original Audio**'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: EnCodec music example taken from the official [EnCodec demo page](https://ai.honu.io/papers/encodec/samples.html).
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: '**Reconstructed Audio**'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: EnCodec music example taken from the official [EnCodec demo page](https://ai.honu.io/papers/encodec/samples.html).
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: As MusicGen fully relies on EnCodec, it is a major bottleneck for the quality
    of the generated music. That is why Meta decided to work on improving EnCodec’s
    decoder part. In August 2023, they had developed an updated decoder for EnCodec
    leveraging multi-band diffusion **[3]**.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: One problem Meta saw with EnCodec’s original decoder was that it tended to generate
    low frequencies first and higher frequencies after. Unfortunately, this meant
    that any errors/artifacts in the low frequencies would distort the high frequencies
    as well, drastically decreasing the output quality.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: 'Multi-band diffusion addresses this problem by generating different sections
    of the frequency spectrum independently before combining them. The researchers
    found that this procedure significantly improved the generated outputs. The differences
    are clearly noticeable from my perspective. Listen to the same track with the
    original EnCodec decoder and the multi-band diffusion decoder:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: '**Original Decoder**'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: Generated track taken from the Multi-Band Diffusion [demo page](https://ai.honu.io/papers/mbd/).
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: '**Multi-Band Diffusion Decoder**'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: Generated track taken from the Multi-Band Diffusion [demo page](https://ai.honu.io/papers/mbd/).
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: One of the core issues of current text-to-music systems is that there is always
    an unnatural quality to the sounds it produces, especially for acoustical instruments.
    Multi-band diffusion makes the output sound much cleaner and more natural and
    takes MusicGen to a new level.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: Why is Stereo Sound so Significant?
  id: totrans-55
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Up until now, most generative music models have been producing mono sound. This
    means MusicGen does not place any sounds or instruments on the left or right side,
    resulting in a less lively and exciting mix. The reason why stereo sound has been
    mostly overlooked so far is that generating stereo is not a trivial task.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: As musicians, when we produce stereo signals, we have access to the individual
    instrument tracks in our mix and we can place them wherever we want. MusicGen
    does not generate all instruments separately but instead produces one combined
    audio signal. Without access to these instrument sources, creating stereo sound
    is hard. Unfortunately, splitting an audio signal into its individual sources
    is a tough problem (I’ve published a [blog post](https://medium.com/towards-data-science/ai-music-source-separation-how-it-works-and-why-it-is-so-hard-187852e54752)
    about that) and the tech is still not 100% ready.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, Meta decided to incorporate stereo generation directly into the MusicGen
    model. Using a new dataset consisting of stereo music, they trained MusicGen to
    produce stereo outputs. The researchers claim that generating stereo has no additional
    computing costs compared to mono.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: 'Although I feel that the stereo procedure is not very clearly described in
    the paper, my understanding it works like this (*Figure 3*): MusicGen has learned
    to generate two compressed audio signals (left and right channel) instead of one
    mono signal. These compressed signals must then be decoded separately before they
    are combined to build the final stereo output. The reason this process does not
    take twice as long is that MusicGen can now produce two compressed audio signals
    at approximately the same time it previously took for one signal.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/48157fe5a80cc2bfa76eab5aed542464.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
- en: Figure 3 — MusicGen stereo update. Note that the process was not sufficiently
    documented in the paper for me to be 100% sure about this. Take it as an educated
    guess. Image by author.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: 'Being able to produce convincing stereo sound really sets MusicGen apart from
    other state-of-the-art models like MusicLM or Stable Audio. From my perspective,
    this “little” addition makes a huge difference in the liveliness of the generated
    music. Listen for yourselves (might be hard to hear on smartphone speakers):'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: '**Mono**'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: '**Stereo**'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  id: totrans-65
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: MusicGen was impressive from the day it was released. However, since then, Meta’s
    FAIR team has been continually improving their product, enabling higher quality
    results that sound more authentic. When it comes to text-to-music models generating
    audio signals (not MIDI etc.), MusicGen is ahead of its competitors from my perspective
    (as of November 2023).
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
- en: Further, since MusicGen and all its related products (EnCodec, AudioGen) are
    open-source, they constitute an incredible source of inspiration and a go-to framework
    for aspiring AI audio engineers. If we look at the improvements MusicGen has made
    in only 6 months, I can only imagine that 2024 will be an exciting year.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: Another important point is that with their transparent approach, Meta is also
    doing foundational work for developers who want to integrate this technology into
    software for musicians. Generating samples, brainstorming musical ideas, or changing
    the genre of your existing work — these are some of the exciting applications
    we are already starting to see. With a sufficient level of transparency, we can
    make sure we are building a future where AI makes creating music more exciting
    instead of being only a threat to human musicianship.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个重要的点是，Meta通过其透明的方法，也为希望将这项技术集成到音乐软件中的开发人员做出了基础工作。生成样本、头脑风暴音乐创意或者改变现有作品的风格
    —— 这些是我们已经开始看到的一些令人兴奋的应用。通过足够的透明度，我们可以确保我们正在构建一个未来，使得人工智能不仅仅是对人类音乐能力的威胁，而是让音乐创作变得更加令人兴奋的未来。
- en: 'Note: While MusicGen is open-source, the pre-trained models may not be used
    commercially! Visit the audiocraft [GitHub repository](https://github.com/facebookresearch/audiocraft)
    for more detailed information on the intended use for all its components.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：虽然MusicGen是开源的，但预训练模型可能不允许商业使用！访问audiocraft的[GitHub仓库](https://github.com/facebookresearch/audiocraft)以获取有关其所有组件预期使用的更详细信息。
- en: '**References**'
  id: totrans-70
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**参考文献**'
- en: '**[1] Copet et al. (2023)**. Simple and Controllable Music Generation. [https://arxiv.org/pdf/2306.05284.pdf](https://arxiv.org/pdf/2306.05284.pdf)'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '**[1] Copet等人（2023）**。简单可控的音乐生成。[https://arxiv.org/pdf/2306.05284.pdf](https://arxiv.org/pdf/2306.05284.pdf)'
- en: '**[2] Défossez et al. (2022)**. High Fidelity Neural Audio Compression. [https://arxiv.org/pdf/2210.13438.pdf](https://arxiv.org/pdf/2210.13438.pdf)'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '**[2] Défossez等人（2022）**。高保真神经音频压缩。[https://arxiv.org/pdf/2210.13438.pdf](https://arxiv.org/pdf/2210.13438.pdf)'
- en: '**[3] Roman et al. (2023)**. From Discrete Tokens to High-Fidelity Audio Using
    Multi-Band Diffusion. [https://arxiv.org/abs/2308.02560](https://arxiv.org/abs/2308.02560)'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '**[3] Roman等人（2023）**。从离散标记到高保真音频使用多频带扩散。[https://arxiv.org/abs/2308.02560](https://arxiv.org/abs/2308.02560)'
- en: About Me
  id: totrans-74
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 关于我
- en: 'Hi there! I’m a musicologist and a data scientist, sharing my thoughts on current
    topics in AI & music. Here is some of my previous work related to this article:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 嗨！我是一名音乐学家和数据科学家，分享我对人工智能和音乐当前主题的见解。这里是我之前与本文相关的一些工作：
- en: '**How Meta’s AI Generates Music Based on a Reference Melody**: [https://medium.com/towards-data-science/how-metas-ai-generates-music-based-on-a-reference-melody-de34acd783](https://medium.com/towards-data-science/how-metas-ai-generates-music-based-on-a-reference-melody-de34acd783)'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Meta的AI如何基于参考旋律生成音乐**：[https://medium.com/towards-data-science/how-metas-ai-generates-music-based-on-a-reference-melody-de34acd783](https://medium.com/towards-data-science/how-metas-ai-generates-music-based-on-a-reference-melody-de34acd783)'
- en: '**MusicLM: Has Google Solved AI Music Generation?**: [https://medium.com/towards-data-science/musiclm-has-google-solved-ai-music-generation-c6859e76bc3c](https://medium.com/towards-data-science/musiclm-has-google-solved-ai-music-generation-c6859e76bc3c)'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**MusicLM：Google解决了AI音乐生成问题吗？**：[https://medium.com/towards-data-science/musiclm-has-google-solved-ai-music-generation-c6859e76bc3c](https://medium.com/towards-data-science/musiclm-has-google-solved-ai-music-generation-c6859e76bc3c)'
- en: '**AI Music Source Separation: How it Works and Why it is so Hard**: [https://medium.com/towards-data-science/ai-music-source-separation-how-it-works-and-why-it-is-so-hard-187852e54752](https://medium.com/towards-data-science/ai-music-source-separation-how-it-works-and-why-it-is-so-hard-187852e54752)'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**AI音乐源分离：工作原理及其难点**：[https://medium.com/towards-data-science/ai-music-source-separation-how-it-works-and-why-it-is-so-hard-187852e54752](https://medium.com/towards-data-science/ai-music-source-separation-how-it-works-and-why-it-is-so-hard-187852e54752)'
- en: Find me on [Medium](https://medium.com/@maxhilsdorf) and [Linkedin](https://www.linkedin.com/in/max-hilsdorf/)!
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在[Medium](https://medium.com/@maxhilsdorf)和[Linkedin](https://www.linkedin.com/in/max-hilsdorf/)找到我！
