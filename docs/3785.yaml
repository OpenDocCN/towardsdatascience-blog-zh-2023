- en: 3 Music AI Breakthroughs to Expect in 2024
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/3-music-ai-breakthroughs-to-expect-in-2024-2d945ae6b5fd?source=collection_archive---------2-----------------------#2023-12-30](https://towardsdatascience.com/3-music-ai-breakthroughs-to-expect-in-2024-2d945ae6b5fd?source=collection_archive---------2-----------------------#2023-12-30)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 2024 could be the tipping point of Music AI
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@maxhilsdorf?source=post_page-----2d945ae6b5fd--------------------------------)[![Max
    Hilsdorf](../Images/01da76c553e43d5ed6b6849bdbfd00da.png)](https://medium.com/@maxhilsdorf?source=post_page-----2d945ae6b5fd--------------------------------)[](https://towardsdatascience.com/?source=post_page-----2d945ae6b5fd--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----2d945ae6b5fd--------------------------------)
    [Max Hilsdorf](https://medium.com/@maxhilsdorf?source=post_page-----2d945ae6b5fd--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fd0c085a74ae8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F3-music-ai-breakthroughs-to-expect-in-2024-2d945ae6b5fd&user=Max+Hilsdorf&userId=d0c085a74ae8&source=post_page-d0c085a74ae8----2d945ae6b5fd---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----2d945ae6b5fd--------------------------------)
    ·11 min read·Dec 30, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F2d945ae6b5fd&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F3-music-ai-breakthroughs-to-expect-in-2024-2d945ae6b5fd&user=Max+Hilsdorf&userId=d0c085a74ae8&source=-----2d945ae6b5fd---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F2d945ae6b5fd&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F3-music-ai-breakthroughs-to-expect-in-2024-2d945ae6b5fd&source=-----2d945ae6b5fd---------------------bookmark_footer-----------)![](../Images/1af7a374365e9166ebbd91aeafabe31c.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Image Generated with DALL-E 3.
  prefs: []
  type: TYPE_NORMAL
- en: '**Recap: How 2023 Changed Music AI**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'From my perspective, 2023 was **the most exciting year for Music AI** in the
    history of the field. Here are only some of the breakthroughs we got to experience
    in this amazing year:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Text-to-Music generation** has crossed Uncanny Valley (e.g. [MusicLM](https://google-research.github.io/seanet/musiclm/examples/))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Open-Source **melody-conditioned music generation** released (e.g. [MusicGen](https://musicgen.com/))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: First **prompt-based music search** products launched (e.g. [Cyanite](https://cyanite.ai/))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Open-source **chatbots with audio understanding**/generation capabilities were
    made available (e.g. [AudioGPT](https://github.com/AIGC-Audio/AudioGPT))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Open-source **music description AI** was released (e.g. [Doh et al., 2023](https://arxiv.org/abs/2307.16372))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Prompt-based source separation** piloted (e.g. [Liu et al., 2023](https://arxiv.org/abs/2308.05037))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: …
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'From text-to-music generation to full-text music search, 2023 was rich with
    breakthroughs. These advancements are **just the tip of the iceberg**, showcasing
    the potential that lies within Music AI. However, even with all these exciting
    developments, the field is still noticeably lagging behind its bigger brother
    Speech AI, or even its cousins NLP & Computer Vision. This gap is observable (or
    audible) in two key ways:'
  prefs: []
  type: TYPE_NORMAL
- en: '**1.** **The tech is not mature enough**. Whether it is music generation, text-based
    search, or neural embeddings: Everything we have in music AI today has been available
    in the text & image domain for at least 1–3 years. The field needs more funding,
    time, and brain power.'
  prefs: []
  type: TYPE_NORMAL
- en: '**2.** **Lack of convincing and popular commercial products**. After the potential
    of music AI became clear, a whole bunch of startups formed to start working on
    commercial products. However, as these products are being developed and tested,
    musicians and businesses eagerly await their chance to integrate AI tech into
    their workflows.'
  prefs: []
  type: TYPE_NORMAL
- en: After the technological success of music AI in 2023, however, I am optimistic
    that researchers & companies will be working hard to make progress in both of
    these dimensions. In this post, I want to highlight three specific developments
    I hope to see in 2024 and explain why they would be important. With these expected
    advancements, **2024 stands on the brink of revolutionizing how we interact with
    music through AI**.
  prefs: []
  type: TYPE_NORMAL
- en: '**1\. Flexible and Natural Source Separation**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/8c0180f5a1e606558e9e933cde71196c.png)'
  prefs: []
  type: TYPE_IMG
- en: Source separation visualized. Image taken from this [blog post](https://medium.com/towards-data-science/ai-music-source-separation-how-it-works-and-why-it-is-so-hard-187852e54752)
    by the author.
  prefs: []
  type: TYPE_NORMAL
- en: What is Source Separation?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Music source separation is the task of splitting a fully produced piece of music
    into its original instrument sources (e.g. vocals, rhythm, keys). If you have
    never heard about source separation, I have written a full [blog post](https://medium.com/towards-data-science/ai-music-source-separation-how-it-works-and-why-it-is-so-hard-187852e54752)
    about how it works and why it is such a challenging technological problem.
  prefs: []
  type: TYPE_NORMAL
- en: The first big breakthrough in source separation happened in 2019 when Deezer
    released [Spleeter](https://github.com/deezer/spleeter) as an open-source tool.
    Since this technological leap, the field has experienced rather steady, **small
    steps of improvement**. Still, if you compare the original Spleeter to modern
    open-source tools like Meta’s [DEMUCS](https://github.com/facebookresearch/demucs)
    or commercial solutions like [LALAL.ai](https://www.lalal.ai/), it feels like
    a night and day difference. So, after years of slow, incremental progress, why
    would I expect source separation to blow up in 2024?
  prefs: []
  type: TYPE_NORMAL
- en: Why Should We Expect Breakthroughs in Source Separation?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Firstly, source separation is a **keystone technology for other music AI problems**.
    Having a fast, flexible, and natural-sounding source separation tool could bring
    music classification, tagging, or data augmentation to the next level. Many researchers
    & companies are carefully observing advancements in source separation, ready to
    act when the next breakthrough occurs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Secondly, **different kinds of breakthroughs** would move the field forward.
    The most obvious one is an increase in separation quality. While we will surely
    see advancements in this regard, I do not expect a major leap here (happy to be
    proven wrong). Still, aside from output quality, source separation algorithms
    have two other problems:'
  prefs: []
  type: TYPE_NORMAL
- en: '**1\. Speed:** Source separation often runs on large generative neural networks.
    For individual tracks, this might be fine. However, for larger workloads that
    you would encounter in commercial applications, the speed is usually still too
    slow — especially if source separation is performed during inference.'
  prefs: []
  type: TYPE_NORMAL
- en: '**2\. Flexibility:** In general, source separation tools offer a fixed set
    of stems (e.g. “vocals”, “drums”, “bass”, “other”). Traditionally, there is no
    way to perform customized source separation tailored to the user''s needs, as
    that would require training a whole new neural network on this task.'
  prefs: []
  type: TYPE_NORMAL
- en: Many interesting applications emerge once source separation is fast enough to
    perform during inference (i.e. before every single model prediction). For example,
    I have written about the potential of using source separation for [making black-box
    music AI explainable](/making-music-tagging-ai-explainable-through-source-separation-2d9493547a7e).
    I would argue that there is **significant commercial interest** in speed optimization
    which might drive a breakthrough next year.
  prefs: []
  type: TYPE_NORMAL
- en: Further, the limited flexibility of current-gen source separation AI makes it
    unusable for various use cases, even though the potential is there, in principle.
    In a paper called [Separate Anything You Describe](https://arxiv.org/abs/2308.05037),
    researchers introduced a **prompt-based source separation** system, this year.
    Imagine typing “give me the main synth in the second verse, but without the delay
    effect” into a text box, and out comes your desired source audio. That’s the potential
    we are looking at.
  prefs: []
  type: TYPE_NORMAL
- en: 'Summary: Source Separation'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In summary, music source separation is likely to make big strides in 2024 due
    to its importance in music AI and ongoing improvements in speed and flexibility.
    New developments, like prompt-based systems, are making it more user-friendly
    and adaptable to different needs. All this promises a wider use in the industry,
    which could motivate research breakthroughs in the field.
  prefs: []
  type: TYPE_NORMAL
- en: '**2\. General-Purpose Music Embeddings**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/47b8a9da8c078044f181c43752a3b6f4.png)'
  prefs: []
  type: TYPE_IMG
- en: Image generated with DALL-E 3.
  prefs: []
  type: TYPE_NORMAL
- en: Embeddings in Natural Language Processing (NLP)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To understand what music embeddings are and why they matter, let us look at
    the field of Natural Language Processing (NLP), where this term originates from.
    Before the advent of embeddings in NLP, the field primarily relied on simpler,
    statistics-based methods for understanding text. For instance, in a simple bag-of-words
    (BoW) approach, you would simply count how often each word in a vocabulary occurs
    in a text. This makes BoW **no more useful than a simple word cloud**.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/769978b4f394ddefecb223445cd6b2d0.png)'
  prefs: []
  type: TYPE_IMG
- en: An example of a simple word cloud. Image by Author.
  prefs: []
  type: TYPE_NORMAL
- en: The introduction of embeddings significantly changed the landscape of NLP. Embeddings
    are mathematical representations of words (or phrases) where the semantic similarity
    between words is reflected in the distance between vectors in this embedding space.
    **Simply put**, the meaning of words, sentences, or entire books can be crunched
    into a bunch of numbers. Oftentimes, 100 to 1000 numbers per word/text are already
    enough to capture its meaning, mathematically.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/622aa45a9a1d11d3db042d1460cc449f.png)'
  prefs: []
  type: TYPE_IMG
- en: Word2Vec (10k) embeddings visualized with t-SNE on the [Tensorflow Embedding
    Projector](https://projector.tensorflow.org/). The top 5 most similar words to
    “violin” are highlighted. Screenshot by Author.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the figure above, you can see 10,000 words represented in a 3-dimensional
    chart, based on their numerical embeddings. Because these embeddings capture each
    word’s meaning, we can simply look for the closest embeddings in the chart to
    find similar terms. This way, we can easily identify the 5 most similar terms
    to “violin”: “cello”, “concerto”, “piano”, “sonata”, and “clarinet”.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Key advantages of embeddings:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Contextual Understanding:** Unlike earlier methods, embeddings are context-sensitive.
    This means the same word can have different embeddings based on its usage in different
    sentences, granting a more nuanced understanding of language.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Semantic Similarity:** Words with similar meanings are often close together
    in the embedding space, which makes embeddings predestined for retrieval tasks
    found in music search engines or recommender systems.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Pre-Trained Models:** With models like BERT, embeddings are learned from
    large corpora of text and can be fine-tuned for specific tasks, significantly
    reducing the need for task-specific data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Embeddings for Music
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Because embeddings are nothing more than numbers, **everything can be crunched
    into a meaningful embedding**, in principle. An example is given in the following
    figure, where different music genres are visualized in a two-dimensional space,
    according to their similarity.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/93c81ab4f59c412f3106c0e17b528fd7.png)'
  prefs: []
  type: TYPE_IMG
- en: Music genre embeddings visualized in a 2-dimensional space on [Every Noise at
    Once](https://everynoise.com/). Screenshot by Author.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, while embeddings have been successfully used in industry and academia
    for more than 5 years, we still have **no widely adopted domain-specific embedding
    models for music**. Clearly, there is a lot of economic potential in leveraging
    embeddings for music. Here are a few use cases for embeddings that could be instantly
    implemented at minimal development effort, given access to high-quality music
    embeddings:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Music Similarity Search**: Search any music database for similar tracks to
    a given reference track.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Text-to-Music Search**: Search through a music database with natural language,
    instead of using pre-defined tags.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Efficient Machine Learning**: Embedding-based models often require 10–100
    times less training data than traditional approaches based on spectrograms or
    similar audio representations.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In 2023, we already made a lot of progress toward open-source high-quality music
    embedding models. For instance, [Microsoft](https://github.com/microsoft/CLAP)
    and [LAION](https://github.com/LAION-AI/CLAP) both released separately trained
    CLAP models (a specific type of embedding model) for the general audio domain.
    However, these models were mostly trained on speech and environmental sounds,
    making them **less effective for music**. Later, both Microsoft and LAION released
    music-specific versions of their CLAP models that were solely trained on music
    data. [M-A-P](https://m-a-p.ai/#) has also released multiple impressive music-specific
    embedding models this year.
  prefs: []
  type: TYPE_NORMAL
- en: My impression after testing all these models is that we are getting closer and
    closer, but have not even achieved what text-embeddings could do 3 years ago.
    In my estimation, **the primary bottleneck remains data**. We can assume that
    all major players like Google, Apple, Meta, Spotify, etc. are already using music
    embedding models effectively, as they have access to gigantic amounts of music
    data. However, the open-source community has not quite been able to catch up and
    provide a convincing model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Summary: General-Purpose Music Embeddings'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Embeddings are a promising technology, making retrieval tasks more accurate
    and enabling machine learning when data is scarce. Unfortunately, a breakthrough
    domain-specific embedding model for music is yet to be released. My hope and suspicion
    is that open-source initiatives or even big players committed to open-source releases
    (like Meta) will solve this problem in 2024\. We are already close and once we
    reach a certain level of embedding quality, every company will be adopting embedding-based
    music tech to create much more value in a much shorter time.
  prefs: []
  type: TYPE_NORMAL
- en: '**3\.** Bridging the Gap Between Technology and Practical Application'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/c671d1a630565116a82998dd67eb9e8c.png)'
  prefs: []
  type: TYPE_IMG
- en: Image generated with DALL-E 3.
  prefs: []
  type: TYPE_NORMAL
- en: 2023 was a weird year… On the one hand, AI has become the biggest buzzword in
    tech, and use cases for ChatGPT, Midjourney, etc. are easy to find for almost
    any end user and business. On the other hand, only a few actual finalized products
    have been launched and widely adopted. Of course, Drake can now sing “My Heart
    Will Go On”, but no business case has been constructed around this tech, so far.
    And yes, AI can now generate vocal samples for beat producers. However, in reality,
    some composers are making the effort to fine-tune their own AI models for the
    **lack of attractive commercial solutions**.
  prefs: []
  type: TYPE_NORMAL
- en: 'In that light, the biggest breakthrough for Music AI might not be a fancy research
    innovation. Instead, it might be a leap in the maturity of AI-based products and
    services that serve the needs of businesses or end-users. Along this path, there
    are still plenty of challenges to solve for anyone wanting to build Music AI products:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Understanding the Music Industry’s or End-User’s Needs**: The tech itself
    is often quite use-case-agnostic. Finding out how the tech can serve real needs
    is a key challenge.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Turning Fancy Demos into Robust Products**: Today, a data scientist can build
    a chatbot prototype or even a music generation tool in a day. However, turning
    a fun demo into a useful, secure, and mature product is demanding and time-consuming.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Navigating Intellectual Property & Licensing Concerns**: Ethical and legal
    considerations are leaving companies and users hesitant to provide or adopt AI-based
    products.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Securing Funding/Investment and First Income Streams**: In 2023, countless
    Music AI startups have been founded. A strong vision and a clear business case
    will be mandatory to secure funding and enable product development.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Marketing and User Adoption**: Even the greatest innovative products can
    easily go unnoticed, these days. End-users and businesses are swarmed with reports
    and promises about the future of AI, making it challenging to reach your target
    audience.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'As an example, let us look a bit closer at how AI already impacts music production
    through new plugins for digital audio workstations (DAW). In a recent [blog post](https://blog.native-instruments.com/ai-powered-plugins/),
    Native Instruments presents 10 new AI-power plugins. To showcase what is already
    possible, let us look at [“Emergent Drums 2” by Audialab](https://audialab.com/?ref=sergei18&gad_source=1&gclid=Cj0KCQiA1rSsBhDHARIsANB4EJZIqsQGOspW7hEmO_ybIio7oN06fqzjCom6R0pCpFdnr5DMH1PZADQaAvL7EALw_wcB).
    Emergent Drums allows musicians to **design their drum samples from scratch with
    generative AI**. The plugin is nicely integrated into the DAW and functions as
    a fully-fledged drum machine plugin. Have a look at it yourselves:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Demo video: “Emergent Drums” by Audialab.'
  prefs: []
  type: TYPE_NORMAL
- en: Zooming out again, the potential applications for Music AI are vast, ranging
    from music production to education or marketing & distribution. Leveraging the
    immense technological potential of AI to provide real value in these domains will
    be a key challenge to solve in the upcoming year.
  prefs: []
  type: TYPE_NORMAL
- en: 'Summary: From Research to Products'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 2023 was a landmark year for Music AI, setting the stage for what’s next. The
    real game-changer for 2024? It is not just about the tech — it is about making
    it work for real people, in real scenarios. Expect to see Music AI stepping out
    of the lab and into our lives, influencing everything from how we create to how
    we consume music.
  prefs: []
  type: TYPE_NORMAL
- en: Hello, 2024.
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 2023 has set the technological groundwork and created public awareness for AI
    and its possibilities. That is why, in my estimation, 2024 might be the best year
    to get started developing Music AI products. Of course, **many of these products
    will fail** and some of AI’s promises will fizzle out eventually. Looking at the
    well-known Garter Hype Cycle in the figure below, we should remind ourselves that
    this is normal and to be expected.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/042a0e664663c00fd7b16768e6b7cfd3.png)'
  prefs: []
  type: TYPE_IMG
- en: Gartner Hype Cycle. Image by author.
  prefs: []
  type: TYPE_NORMAL
- en: Nobody can tell with certainty where we currently are on this hype cycle (if
    someone can, please let me know). Still, with all the groundwork and public awareness
    created this year, 2024 has the potential to become **the** historical landmark
    year for AI-based music technology. I am more than excited to see what the next
    year will bring.
  prefs: []
  type: TYPE_NORMAL
- en: '**What a great time to be a musician!**'
  prefs: []
  type: TYPE_NORMAL
- en: About Me
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'I’m a musicologist and a data scientist, sharing my thoughts on current topics
    in AI & music. Here is some of my previous work related to this article:'
  prefs: []
  type: TYPE_NORMAL
- en: '**How Meta’s AI Generates Music Based on a Reference Melody**: [https://medium.com/towards-data-science/how-metas-ai-generates-music-based-on-a-reference-melody-de34acd783](https://medium.com/towards-data-science/how-metas-ai-generates-music-based-on-a-reference-melody-de34acd783)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**MusicLM: Has Google Solved AI Music Generation?**: [https://medium.com/towards-data-science/musiclm-has-google-solved-ai-music-generation-c6859e76bc3c](https://medium.com/towards-data-science/musiclm-has-google-solved-ai-music-generation-c6859e76bc3c)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**AI Music Source Separation: How it Works and Why it is so Hard**: [https://medium.com/towards-data-science/ai-music-source-separation-how-it-works-and-why-it-is-so-hard-187852e54752](https://medium.com/towards-data-science/ai-music-source-separation-how-it-works-and-why-it-is-so-hard-187852e54752)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Find me on [Medium](https://medium.com/@maxhilsdorf) and [Linkedin](https://www.linkedin.com/in/max-hilsdorf/)!
  prefs: []
  type: TYPE_NORMAL
