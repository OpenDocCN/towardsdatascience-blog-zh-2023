- en: Temporal Graph Learning in 2023
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/temporal-graph-learning-in-2023-d28d1640dbf2?source=collection_archive---------1-----------------------#2023-01-16](https://towardsdatascience.com/temporal-graph-learning-in-2023-d28d1640dbf2?source=collection_archive---------1-----------------------#2023-01-16)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The story so far
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@shenyanghuang1996?source=post_page-----d28d1640dbf2--------------------------------)[![Shenyang(Andy)
    Huang](../Images/ab63c37868db97b19480d536388930c5.png)](https://medium.com/@shenyanghuang1996?source=post_page-----d28d1640dbf2--------------------------------)[](https://towardsdatascience.com/?source=post_page-----d28d1640dbf2--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----d28d1640dbf2--------------------------------)
    [Shenyang(Andy) Huang](https://medium.com/@shenyanghuang1996?source=post_page-----d28d1640dbf2--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F8aa224c5cedd&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftemporal-graph-learning-in-2023-d28d1640dbf2&user=Shenyang%28Andy%29+Huang&userId=8aa224c5cedd&source=post_page-8aa224c5cedd----d28d1640dbf2---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----d28d1640dbf2--------------------------------)
    ·15 min read·Jan 16, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fd28d1640dbf2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftemporal-graph-learning-in-2023-d28d1640dbf2&user=Shenyang%28Andy%29+Huang&userId=8aa224c5cedd&source=-----d28d1640dbf2---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fd28d1640dbf2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftemporal-graph-learning-in-2023-d28d1640dbf2&source=-----d28d1640dbf2---------------------bookmark_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: Real world networks such as social, traffic and citation networks often evolve
    over time and the field of Temporal Graph Learning (TGL) aims to extract, learn
    and predict from these evolving networks. Recently, TGL has gained increasing
    attention from the ML community, with a surge in the number of papers and the
    [first workshop](https://sites.google.com/view/tglworkshop2022/home) in this area
    held last year at NeurIPS 2022!
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8318f9cf45be7274d6407d27d3808232.png)'
  prefs: []
  type: TYPE_IMG
- en: Evolutions in a temporal graph. Image by authors.
  prefs: []
  type: TYPE_NORMAL
- en: '*This post was co-authored with* [*Emanuele Rossi*](https://www.emanuelerossi.co.uk/),
    [*Michael Galkin*](https://migalkin.github.io/) *and* [*Kellin Pelrine*](https://scholar.google.com/citations?user=_s2HT_0AAAAJ&hl=en).
    *Thanks to* [*Farimah Poursafaei*](https://scholar.google.ca/citations?user=gZ7HEsMAAAAJ&hl=en)
    *for the helpful feedback.*'
  prefs: []
  type: TYPE_NORMAL
- en: In this blog post, we present major progress in TGL until 2022 and discuss promising
    future directions. Note that we use the term “dynamic graph” and “temporal graph”
    interchangeably. If you want to learn or start a project in TGL, this article
    would be a good reference and starting point.
  prefs: []
  type: TYPE_NORMAL
- en: Please share with us in the comment section any other advances you are excited
    about.
  prefs: []
  type: TYPE_NORMAL
- en: '**Table of Contents:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[Introduction to Temporal Graph Learning](#a31b)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Expressiveness of Temporal Graph Networks](#ba74)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Rethinking Evaluation in Temporal Graphs](#1305)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Temporal Knowledge Graphs](#8b9a)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Libraries and Datasets](#e4ec)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Disease Modeling with Temporal Graphs](#f7ab)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Anomaly Detection in Temporal Graphs](#c801)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Detecting Misinformation on Temporal Graphs](#6386)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Joining Temporal Graph Learning Community](#5628)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Introduction to Temporal Graph Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we provide a brief overview of some well-known TGL methods
    in the literature. There exist two main broad classes of methods for learning
    on Continuous-Time Dynamic Graphs (CTDGs): Temporal Graph Networks and Walk Aggregating
    methods. For more details on the formulation of CTDGs, see this survey by [Kazemi
    et al.](https://arxiv.org/pdf/1905.11485.pdf)'
  prefs: []
  type: TYPE_NORMAL
- en: Temporal Graph Networks ([TGNs](https://arxiv.org/abs/2006.10637)) generalize
    Message Passing Neural Networks ([MPNNs](https://dl.acm.org/doi/10.5555/3305381.3305512))
    to temporal graphs. They do so by introducing a node memory which represents the
    state of the node at a given time, acting as a compressed representation of the
    node’s past interactions. Every time two nodes are involved in an interaction,
    they send messages to each other which are then used to update their memories.
    When computing the embedding of a node, an additional graph aggregation is performed
    over the temporal neighbors of the node, using both the original node features
    and memory at that point in time. Below we show a diagram of the computation of
    TGN.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c18e9bd0bc197d1d47dc82e3e91f864d.png)'
  prefs: []
  type: TYPE_IMG
- en: Computations of TGN on a batch of training edges.
  prefs: []
  type: TYPE_NORMAL
- en: 'Image source: [Rossi et al.](https://arxiv.org/abs/2006.10637)'
  prefs: []
  type: TYPE_NORMAL
- en: TGNs are a general framework that generalizes previous models such as Joint
    Dynamic User-Item Embeddings ([JODIE](https://snap.stanford.edu/jodie/)) and Temporal
    Graph Attention ([TGAT](https://openreview.net/forum?id=rJeW1yHYwH)) as specific
    cases. A more comprehensive introduction to TGNs can be found in the below blog
    post by one of the authors.
  prefs: []
  type: TYPE_NORMAL
- en: '[](/temporal-graph-networks-ab8f327f2efe?source=post_page-----d28d1640dbf2--------------------------------)
    [## Temporal Graph Networks'
  prefs: []
  type: TYPE_NORMAL
- en: A new neural network architecture for dynamic graphs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/temporal-graph-networks-ab8f327f2efe?source=post_page-----d28d1640dbf2--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: Walk Aggregating methods such as Causal Anonymous Walks ([CAW](http://snap.stanford.edu/caw/))
    instead rely on (temporal) random walks. In particular, to predict the existence
    of a link *(u, v)* at time *t*, CAW first extracts multiple random walks starting
    from *u* and *v* such that the timestamps of edges in a walk can only be monotonically
    decreasing. The walks are first anonymized by replacing each node identifier with
    a count vector of how many times that node appears at each of the possible positions
    in the walks. Each walk is then encoded using an RNN, and the encodings are then
    aggregated by using self-attention or taking a simple average.
  prefs: []
  type: TYPE_NORMAL
- en: Expressiveness of Temporal Graph Networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There is a large body of work studying the expressive power of GNNs operating
    on static graphs. [Xu et al. 2019](https://openreview.net/pdf?id=ryGs6iA5Km) first
    characterized the discriminative power of Graph Neural Networks (GNNs) by connecting
    them to the Weisfeiler-Lehman (WL) graph isomorphism test and showing that many
    GNNs are no more powerful than the 1-WL test. Subsequent more expressive models
    such as [subgraph GNNs](/using-subgraphs-for-more-expressive-gnns-8d06418d5ab),
    [graph transformers](https://openreview.net/pdf?id=lMMaNf6oxKM%5C) and [higher
    order GNNs](https://arxiv.org/pdf/1810.02244.pdf) are designed to be more expressive
    than 1-WL test (below is link to [Michael Bronstein](https://michael-bronstein.medium.com/)’s
    excellent blog post on how to go beyond WL test).
  prefs: []
  type: TYPE_NORMAL
- en: '[](/graph-neural-networks-beyond-weisfeiler-lehman-and-vanilla-message-passing-bc8605fa59a?source=post_page-----d28d1640dbf2--------------------------------)
    [## Graph Neural Networks beyond Weisfeiler-Lehman and vanilla Message Passing'
  prefs: []
  type: TYPE_NORMAL
- en: Physics-inspired continuous learning models on graphs allow to overcome the
    limitations of traditional GNNs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/graph-neural-networks-beyond-weisfeiler-lehman-and-vanilla-message-passing-bc8605fa59a?source=post_page-----d28d1640dbf2--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: Until this year, there has been little work on understanding the expressive
    power of TGL methods. The first effort to bridge this gap was by [Ribeiro et al.](https://proceedings.mlr.press/v162/gao22e.html)
    where the key idea is to categorize existing TGL methods into *time-and-graph*
    and *time-then-graph* frameworks.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1adc4f01c0bb270cb1e3a66645cc95ae.png)'
  prefs: []
  type: TYPE_IMG
- en: Converting a TG into time-then-graph representation.
  prefs: []
  type: TYPE_NORMAL
- en: 'image source: [Ribeiro et al.](https://arxiv.org/abs/2103.07016)'
  prefs: []
  type: TYPE_NORMAL
- en: 1️). In *time-and-graph*, GNNs are used to generate node embeddings on the snapshot
    graph at each time thus forming a sequence of node embeddings.
  prefs: []
  type: TYPE_NORMAL
- en: 2️). In *time-then-graph*, each edge in the TG is converted to a time series
    which indicates at which time the edge exists, therefore collapsing the temporal
    edges into edge features in a static graph.
  prefs: []
  type: TYPE_NORMAL
- en: It was shown that a *time-then-graph* representation can be constructed from
    any given *time-and-graph* representation thus proving *time-then-graph* is at
    least as expressive as *time-and-graph.* With the static representation in *time-then-graph*,
    we can directly use the WL-test expressiveness framework from the static graph
    for TGL methods. In this way, *time-then-graph* is more expressive than *time-and-graph*
    as long as a 1-WL GNN is used as the backbone model.
  prefs: []
  type: TYPE_NORMAL
- en: '[Souza et al.](https://openreview.net/pdf?id=MwSXgQSxL5s) also aims to establish
    the 1-WL expressiveness framework for TGL methods. Notably, they view a CTDG as
    a sequence of time-stamped multi-graphs where the multi-graph *G(t)* at a given
    time *t* is obtained by sequentially applying all events prior to *t.* A multi-graph
    here means there can be multiple edges between two nodes in the graph and the
    edge attribute is the timestamp information.'
  prefs: []
  type: TYPE_NORMAL
- en: Now, the Temporal WL test can be defined by applying the WL test on the multigraphs
    constructed from CTDGs. Therefore, more expressive TGN methods must be injective
    on its temporal neighborhood (i.e. hashing two different multi-set nodes into
    different colors), called injective MP-TGNs. [Souza et al.](https://openreview.net/pdf?id=MwSXgQSxL5s)
    also analyzed walk based TGNs such as [CAW](http://snap.stanford.edu/caw/) and
    show that MP-TGNs and CAW are not more expressive than each other (as seen above).
    Their proposed PINT method combines benefits from both categories of methods thus
    being the most expressive. The example below shows two temporal graphs that MP-TGNs
    are unable to distinguish. The colors are node labels and the edge has timestamps
    starting from *t₁*.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/fa101bc3babee6d1c6350da5ff64f895.png)'
  prefs: []
  type: TYPE_IMG
- en: Examples of temporal graphs for which MP-TGNs are unable to distinguish graph
    structures such as diameter, girth, and number of cycles.
  prefs: []
  type: TYPE_NORMAL
- en: 'Image source: [Souza et al.](https://arxiv.org/abs/2209.15059)'
  prefs: []
  type: TYPE_NORMAL
- en: Rethinking Evaluation in Temporal Graphs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To a large extent, the evaluation procedure in TGL is relatively under-explored
    and heavily influenced by static graph learning. For example, evaluation on the
    link prediction task on dynamic graphs (or dynamic link prediction) often involves:
    1). *fixed train, test split*, 2). *random negative edge sampling* and 3). *small
    datasets from similar domains*. Such evaluation protocols often lead to result
    tables where reported metrics are already around 95+% and it’s really hard to
    distinguish whether new models bring any benefit or just rehash existing methods
    again.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8fd48489b3f4fb80c50944eaa712ab7a.png)'
  prefs: []
  type: TYPE_IMG
- en: A typical temporal link prediction result table reporting Average Precision
    (AP). Are we really making any progress when even baselines yield 98%?
  prefs: []
  type: TYPE_NORMAL
- en: 'Image source: [Souza et al.](https://arxiv.org/abs/2209.15059)'
  prefs: []
  type: TYPE_NORMAL
- en: '[You et al.](https://arxiv.org/abs/2208.07239) discussed the limitations of
    current TGL methods in model design, evaluation settings and training strategies
    for Discrete Time Dynamic Graphs (DTDGs). They argue that the evolving nature
    of data and models are not accounted for. In standard evaluation, all time points
    are split chronologically into a training set, an evaluation set and then a test
    set. The split is fixed for a given dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: They pointed out that such a fixed split means only edges from the chosen test
    period would be evaluated thus long term behavior potentially spanning training,
    validation and test period would not be correctly evaluated. In addition, many
    TGL methods are stale at test time, meaning that the model representation is not
    updated with information during evaluation. Consider an example transaction graph,
    if information on the prior day is available, the user would likely want to update
    the model with such information to achieve the best possible performance. Therefore,
    a live-update evaluation is proposed where models are finetuned with newly observed
    data, utilizing historical information and predicting future links.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3177c3500825244ea4dc8109926d7259.png)'
  prefs: []
  type: TYPE_IMG
- en: Grey / red bars indicate the amount of recurring / novel edges respectively
    in the Wikipedia / MOOC dataset. Many edges recur over time in temporal graphs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Image source: [*Poursafaei et al.*](https://arxiv.org/abs/2207.10128)'
  prefs: []
  type: TYPE_NORMAL
- en: '[Recent work](https://openreview.net/forum?id=1GVpwr2Tfdg) by two of the authors
    examines which negative edges to sample for evaluation of CTDG methods and introduces
    more datasets from diverse domains. When evaluating dynamic link prediction, negative
    edges are often randomly sampled from any node pair. However, many edges in a
    temporal graph recur over time (as seen in the figure above). Considering the
    sparsity of real world graphs, the majority of node pairs are unlikely to form
    an edge. Therefore, *random* negative edges can be seen as *easy* negative edges.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c1a1e4a8ab7287729ba5feeb337d1efa.png)'
  prefs: []
  type: TYPE_IMG
- en: Avg. Performance of TGL methods. Using more difficult negative edges significantly
    impacts model performance. The simple baseline EdgeBank also works surprisingly
    well.
  prefs: []
  type: TYPE_NORMAL
- en: 'Image source: [*Poursafaei et al.*](https://arxiv.org/abs/2207.10128)'
  prefs: []
  type: TYPE_NORMAL
- en: Now, what can be considered as *hard* negative edges? First, we introduce the
    *historical* negative edges which are edges that appeared in the training set
    but are absent in the current test step. We also define *inductive* negative edges
    as test edges which occurred previously in the test set but are not present at
    the current step. Lastly, we propose a baseline EdgeBank relying solely on memorizing
    past edges (essentially a hashtable of seen edges). In the plot above, we see
    that by changing the negative edges for evaluation, the average performance of
    existing TGL methods reduces significantly in the *historical* and *inductive*
    setting when compared to the *standard* setting. EdgeBank is also a surprisingly
    strong baseline for the *standard* setting. For details, see the blog below from
    one of the authors.
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/@shenyanghuang1996/towards-better-link-prediction-in-dynamic-graphs-cdb8bb1e24e9?source=post_page-----d28d1640dbf2--------------------------------)
    [## Towards Better Link Prediction in Dynamic Graphs'
  prefs: []
  type: TYPE_NORMAL
- en: companion blog post for Towards Better Evaluation for Dynamic Link Prediction,
    to appear in NeurIPS 2022 Dataset and…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: medium.com](https://medium.com/@shenyanghuang1996/towards-better-link-prediction-in-dynamic-graphs-cdb8bb1e24e9?source=post_page-----d28d1640dbf2--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: Temporal Knowledge Graphs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the realm of Knowledge Graphs (KGs), temporal setup is slightly different
    from the homogeneous world, i.e., timestamped graph snapshots are not that common.
    Instead, some (or all) triples have an accompanying (start time, end time) pair
    of attributes denoting the time frame when a given fact was true. Triples thus
    become *quintuples,* or, in Wikidata, time attributes become [*qualifiers*](https://www.wikidata.org/wiki/Help:Qualifiers)
    of a more general [*statement*](https://www.wikidata.org/wiki/Help:Statements)(main
    triple + multiple key-value qualifiers), and statements form so-called [hyper-relational
    KGs](/representation-learning-on-rdf-and-lpg-knowledge-graphs-6a92f2660241)*.*
  prefs: []
  type: TYPE_NORMAL
- en: For example, `(President of the French republic, office holder, Nicolas Sarkozy,
    2007, 2012)` is a quintuple describing the time period where Nicolas Sarkozy was
    the President of the French republic. Alternatively, there can be only one timestamp
    per triple (forming quadruples). The most common prediction task is scoring head/tail
    prediction given the time attributes, e.g. , `(President of the French Republic,
    office holder, **???**, 2007, 2012)` — this can be considered as a particular
    case of the hyper-relational link prediction where qualifiers are only dateTime
    literals. A classic example of temporal KG completion model is [**TNTComplex**](https://arxiv.org/pdf/2004.04926.pdf)
    (ICLR 2020).
  prefs: []
  type: TYPE_NORMAL
- en: '[Krause et al.](https://arxiv.org/pdf/2207.09964.pdf) has taken the first effort
    towards bridging the gap between temporal KGs and homogeneous graphs. In this
    work, the authors propose a framework to formalize various temporal aspects in
    KGs. Namely, they define **temporal** KGs as local extensions, i.e., graphs that
    have timestamps on the edges, and **dynamic** KGs as global extensions, i.e.,
    graphs that change topology over time by adding or removing nodes and edges. Even
    more, there can exits combinations of those basic types, e.g., a temporal and
    dynamic KG would be called **incremental**. We hope this work would bring a bit
    more order and clarity to the hectic literature on temporal KGs and the community
    would stick to the nice taxonomy. Next step: finalize a proper evaluation protocol
    for those graph types.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b33b6420eb72ee2d292b5b038008968f.png)'
  prefs: []
  type: TYPE_IMG
- en: Temporal and Dynamic KGs (and their combinations).
  prefs: []
  type: TYPE_NORMAL
- en: 'Image source: [Krause et al.](https://arxiv.org/abs/2207.09964)'
  prefs: []
  type: TYPE_NORMAL
- en: '[Wang et al.](https://openreview.net/pdf?id=1LmgISIDZJ) addressed the task
    of few-shot link prediction over temporal + dynamic graphs where the edges have
    timestamps **and** new nodes might appear at later timesteps (*Incremental* *graphs*
    as to the above classification by Krause et al.). The few-shot scenario makes
    the task even more challenging — we only have access to a limited number of training
    and inference points (usually, <5) to reason about the queries link. Here, the
    authors propose **MetaTKGR**, a meta-learning based approach that builds representations
    of new nodes by aggregating features of existing nodes within a certain *delta
    t* temporal neighborhood. The scalar difference between timestamps is vectorized
    via the Fourier transform.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9800b8a8cd1f35415b12f3ca6d500cb5.png)'
  prefs: []
  type: TYPE_IMG
- en: Components of MetaTKGR.
  prefs: []
  type: TYPE_NORMAL
- en: 'Image source: [Wang et al.](https://arxiv.org/abs/2210.08654)'
  prefs: []
  type: TYPE_NORMAL
- en: Libraries and Datasets
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The lack of large datasets and challenging tasks has been holding back research
    on Temporal Graph Learning in the past few years. Luckily, new datasets from diverse
    domains are emerging. For example, [Poursafaei et al.](https://openreview.net/forum?id=1GVpwr2Tfdg)
    introduced six new publicly available TG datasets from the transportation, politics,
    economics and proximity domains. However, the field is still lacking a consistent
    effort to standardize benchmarks and evaluation to a high quality, what [OGB](https://ogb.stanford.edu/)
    did for static graphs. We hope that in 2023, we can see more standardized TG benchmarks
    with a focus on real applications.
  prefs: []
  type: TYPE_NORMAL
- en: Regarding libraries, a well-known one is [Pytorch Geometric Temporal](https://pytorch-geometric-temporal.readthedocs.io/en/latest/),
    an extension of [Pytorch Geometric](https://pytorch-geometric.readthedocs.io/en/latest/)
    for temporal graphs. However, Pytorch-Geometric Temporal seems to only feature
    discrete-time methods and datasets. A library which also includes continuous-time
    methods would be a great added value for the community. Recently, [Zhou et al.](https://www.vldb.org/pvldb/vol15/p1572-zhou.pdf)
    introduced TGL, a unified framework for large-scale offline Temporal Graph Neural
    Network training. In particular, on a 4-GPU machine, TGL can train one epoch of
    more than one billion temporal edges within 1–10 hours.
  prefs: []
  type: TYPE_NORMAL
- en: We list links to various TGL libraries and datasets below.
  prefs: []
  type: TYPE_NORMAL
- en: 13 processed TG datasets accessible via “[pip install dgb](https://complexdata.ml/docs/proj-tg/dgb/start)”
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Pytorch Geometric Temporal](https://pytorch-geometric-temporal.readthedocs.io/en/latest/notes/installation.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[TGL library](https://github.com/amazon-science/tgl)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Chartalist Dynamic Blockchain Transaction Network](https://github.com/cakcora/chartalist)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Temporal knowledge graph forecasting benchmark](https://github.com/nec-research/TKG-Forecasting-Evaluation)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Disease Modeling with Temporal Graphs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the recent COVID-19 pandemic, epidemic modeling is instrumental for understanding
    the spread of the disease as well as designing corresponding intervention strategies.
    Human contact networks are in fact temporal graphs. By combining contact graphs
    with classical compartment based models such as [SEIR](https://www.canada.ca/en/public-health/services/reports-publications/canada-communicable-disease-report-ccdr/monthly-issue/2020-46/issue-6-june-4-2020/predictive-modelling-covid-19-canada.html)
    and [SIR](http://networksciencebook.com/chapter/10#epidemic), we can more accurately
    forecast COVID-19 infection curve and go beyond the homogenous mixing assumption
    (all individuals are equally likely to be in contact with each other).
  prefs: []
  type: TYPE_NORMAL
- en: '[Chang et al.](https://www.nature.com/articles/s41586-020-2923-3) derived a
    temporal mobility network from cell phone data and mapped the hourly movement
    of 98 million people from census block groups (CBGs) to specific points of interest
    (POIs) in the US. By combining the hourly contact network with SEIR models on
    the CBG level, they are able to accurately fit real infection trajectory. In particular,
    the model shows that some ‘superspreader’ POIs such as restaurants and fitness
    centers account for a large majority of the infections. Also, differences in mobility
    between racial and socioeconomic groups lead to different infection rates among
    these groups. This work showcased the real world potential of utilizing large
    scale temporal graphs for disease forecasting and informing policies on intervention
    strategies.'
  prefs: []
  type: TYPE_NORMAL
- en: Besides human contact networks, dynamic transportation networks also play an
    important role in the spread of COVID-19\. In [recent work](https://appliednetsci.springeropen.com/articles/10.1007/s41109-021-00378-3)
    by one of the authors, we incorporated daily flight networks into the SEIR model
    to estimate imported COVID-19 cases. By incorporating flight networks, it is possible
    for early detection of outbreaks and forecast the impact of travel restrictions.
    See the [blog post](https://mila.quebec/en/article/flight-seir-incorporating-flight-data-to-improve-epidemiological-modelling-and-disease-outbreak-prevention/)
    by one of the authors for more details.
  prefs: []
  type: TYPE_NORMAL
- en: Despite the empirical success of temporal-graph-based disease models, it is
    also important to answer questions such as “*how does the contact network structure
    impact the spread of disease?*” and “*what is the best way to modify the contact
    pattern such that the spread of COVID-19 can be slowed or prevented?*” [Holme
    et al.](https://journals.aps.org/pre/abstract/10.1103/PhysRevE.94.022305) compared
    the difference in outbreak characteristics between using temporal, static and
    fully-connected networks on eight network datasets and examined various network
    structures affecting the spread of the disease. They showed that converting temporal
    networks into static ones can lead to severe under- or over-estimation of both
    the outbreak size and extinction time of the disease.
  prefs: []
  type: TYPE_NORMAL
- en: What are the next steps for TGL on epidemic modeling?
  prefs: []
  type: TYPE_NORMAL
- en: 1️). First, forecasting the entire contact or mobility network snapshot for
    the immediate future is a crucial challenge. With the predicted structure, we
    can apply network based SEIR models to estimate the infection curve.
  prefs: []
  type: TYPE_NORMAL
- en: 2️). Second, defining and understanding the impact of interaction patterns on
    the contact network is crucial for policy making and interpretability. Analyzing
    the interplay between graph structures and the infection curve can help us identify
    the most effective intervention strategies.
  prefs: []
  type: TYPE_NORMAL
- en: Anomaly Detection in Temporal Graphs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Anomaly detection is a fundamental task in analyzing temporal graphs which identifies
    entities that deviate significantly from the rest. For example, fraud detection
    can be modeled as detecting abnormal edges in a transaction network and traffic
    accident identification can be seen as detecting anomalous events in a traffic
    network.
  prefs: []
  type: TYPE_NORMAL
- en: There is growing interest in utilizing the representation power of temporal
    graph networks for anomaly detection. [Cai et al.](https://arxiv.org/abs/2005.07427)
    designed an end-to-end structural temporal Graph Neural Network model for detecting
    anomalous edges, called **StrGNN**. An enclosing subgraph, a k-hop subgraph centered
    around an edge, is first extracted based on the edge of interest to reduce computational
    complexity. A Graph Convolutional Neural Network ([GCN](https://openreview.net/forum?id=SJU4ayYgl))
    is then used to generate structural embedding from the subgraph. Gated Recurrent
    Units ([GRUs](https://aclanthology.org/D14-1179/)) are then used to capture temporal
    information. One of the challenges of anomaly detection is the lack of labeled
    examples. Therefore, [Cai et al.](https://arxiv.org/abs/2005.07427) proposed to
    generate “context-dependent” negative edges by replacing one of the nodes in a
    normal edge and training the model with these negative edges.
  prefs: []
  type: TYPE_NORMAL
- en: When comparing with unsupervised, non-GNN based anomaly detection methods such
    as [**SEDANSPOT**](https://dhivyaeswaran.github.io/papers/icdm18-sedanspot.pdf)
    and [**AnomRank**](https://www.cs.cmu.edu/~christos/PUBLICATIONS/kdd20-ANRank.pdf),
    GNN based methods can easily incorporate any given attribute and has the potential
    to achieve stronger performance. However, there are two significant challenges
    for GNN based approaches.
  prefs: []
  type: TYPE_NORMAL
- en: 1). First, how to scale to dynamic graphs with millions of edges and nodes?
    This is an open question for both the GNN module in extracting the graph features
    but also the temporal module such as GRUs and transformers in processing long
    term information.
  prefs: []
  type: TYPE_NORMAL
- en: 2️). Second, how to produce accurate explanations for the detected anomalies?
    In real applications, detected anomalies are often verified and then potentially
    resulting in punitive measures for those detected entities. GNN explainability
    on dynamic graphs remains an open challenge.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2f03649d8d6032495851a120e3981981.png)'
  prefs: []
  type: TYPE_IMG
- en: LAD detects 2013 as a change point in the Canadian MP voting network due to
    abnormal amounts of edges between political parties.
  prefs: []
  type: TYPE_NORMAL
- en: 'Image source: [Huang et al.](https://arxiv.org/abs/2007.01229)'
  prefs: []
  type: TYPE_NORMAL
- en: The task of *change point detection* aims to detect time points in a dynamic
    graph where the graph structure or distribution deviates significantly from what
    was observed before. This change can be attributed to external events (such as
    traffic disruption and COVID-19 related flight restrictions) or simply natural
    evolution of the dynamic graph. [Recent work](https://arxiv.org/abs/2007.01229)
    by one of the authors utilized the eigenvalues of the Laplacian matrix of each
    graph snapshot to embed the graph structure while applying sliding windows to
    compare the changes in graph structure in the long and short term. In the above,
    the proposed Laplacian Anomaly Detection (**LAD**) method detects a change in
    the Canadian Member of Parliament (MP) voting network due to increased edges between
    political parties. This coincides with Justin Trudeau being selected as the Liberal
    party leader in 2013.
  prefs: []
  type: TYPE_NORMAL
- en: Detecting Misinformation on Temporal Graphs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Misinformation** spreads in different patterns and rates when compared to
    true information ([Vosoughi et al.](https://www.science.org/doi/10.1126/science.aap9559)).
    There has been considerable research studying these network patterns in a static
    graph while dynamic graph based methods are underexplored ([Song et al.](https://www.sciencedirect.com/science/article/abs/pii/S0306457321001965)).
    However, in the past year an increased amount of TGL methods were employed for
    misinformation detection and understanding. For instance, [Zhang et al.](https://link.springer.com/chapter/10.1007/978-3-030-72240-1_48)
    developed a method based on Temporal Point Processes while Dynamic GCN ([**DynGCN**](https://journals.plos.org/plosone/article?id=10.1371%2Fjournal.pone.0256039))
    and [**DGNF**](https://www.sciencedirect.com/science/article/pii/S0925231222009158)
    are dynamic GNN based methods.'
  prefs: []
  type: TYPE_NORMAL
- en: The illustration below shows the architecture of DynGCN. They construct graph
    snapshots with even spacing in time, feed each through GCN layers, and then combine
    the representations and learn the snapshots’ evolution patterns using attention.
    This is a relatively simpler approach to leverage temporal information compared
    to some methods discussed above like [TGN](https://arxiv.org/abs/2006.10637) or
    [CAW](http://snap.stanford.edu/caw/), but nonetheless gives better performance
    than previous state-of-the-art for misinformation detection on the datasets that
    the authors examined.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a9486bf6050dd1d05cadf94488b24b72.png)'
  prefs: []
  type: TYPE_IMG
- en: DynGCN processes individual graph snapshots using GCN layers with shared weights,
    then combines the representations over time with an attention mechanism.
  prefs: []
  type: TYPE_NORMAL
- en: 'Image source: [Choi et al.](https://journals.plos.org/plosone/article?id=10.1371%2Fjournal.pone.0256039)'
  prefs: []
  type: TYPE_NORMAL
- en: Dynamic interaction patterns are shown to be quite informative for misinformation
    detection ([Plepi et al.](https://aclanthology.org/2022.textgraphs-1.10/)). With
    significant recent advances in TGL methods, we can expect novel state-of-the-art
    misinformation detection methods that incorporate dynamic graphs.
  prefs: []
  type: TYPE_NORMAL
- en: Joining Temporal Graph Learning Community
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 2022 has seen increased attention in TGL from the ML community. The first ever
    [TGL workshop](https://sites.google.com/view/tglworkshop2022/home) was held at
    NeurIPS 2022\. The recordings of the talks and panel will be available soon on
    the [NeurIPS virtual site](https://neurips.cc/virtual/2022/workshop/49999). The
    accepted papers are available on the [workshop website](https://sites.google.com/view/tglworkshop2022/home).
    Keep an eye out for announcements of new iterations of the TGL workshop there
    and join the workshop slack (up-to-date link in the website) to engage with the
    community. This year, we are also planning a TGL reading group, if you would like
    to share your work or be involved in co-organizing the reading group, please email
    [shenyang.huang@mail.mcgill.ca](mailto:shenyang.huang@mail.mcgill.ca)
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/16687a5a16fb0ec79714aab91d36d2a0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Image Source: Logo of the [NeurIPS 2022 Temporal Graph Learning Workshop](https://sites.google.com/view/tglworkshop2022/home).
    Image by authors.'
  prefs: []
  type: TYPE_NORMAL
