- en: From the Perceptron to Adaline
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/from-the-perceptron-to-adaline-1730e33d41c5?source=collection_archive---------6-----------------------#2023-11-28](https://towardsdatascience.com/from-the-perceptron-to-adaline-1730e33d41c5?source=collection_archive---------6-----------------------#2023-11-28)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Setting the foundations right
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@cretanpan?source=post_page-----1730e33d41c5--------------------------------)[![Pan
    Cretan](../Images/8b3fbab70c0e61f7ca516d2f54b646e5.png)](https://medium.com/@cretanpan?source=post_page-----1730e33d41c5--------------------------------)[](https://towardsdatascience.com/?source=post_page-----1730e33d41c5--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----1730e33d41c5--------------------------------)
    [Pan Cretan](https://medium.com/@cretanpan?source=post_page-----1730e33d41c5--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fff990ba57425&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffrom-the-perceptron-to-adaline-1730e33d41c5&user=Pan+Cretan&userId=ff990ba57425&source=post_page-ff990ba57425----1730e33d41c5---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----1730e33d41c5--------------------------------)
    ·11 min read·Nov 28, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F1730e33d41c5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffrom-the-perceptron-to-adaline-1730e33d41c5&user=Pan+Cretan&userId=ff990ba57425&source=-----1730e33d41c5---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F1730e33d41c5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffrom-the-perceptron-to-adaline-1730e33d41c5&source=-----1730e33d41c5---------------------bookmark_footer-----------)![](../Images/8ac80c2c95ab7e4f13888daeeb72fccc.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Photo by [Einar Storsul](https://unsplash.com/@einarstorsul?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In a previous article I tried to explain the most basic binary classifier that
    has likely ever existed, [Rosenblatt’s perceptron](https://medium.com/towards-data-science/classification-with-rosenblatts-perceptron-e7f49e3af562).
    Understanding this algorithm has educational value and can serve as a good introduction
    in elementary machine learning courses. It is an algorithm that can be coded from
    scratch in a single afternoon and can spark interest, a sense of achievement and
    motivation to delve into more complex topics. Still, as an algorithm it leaves
    much to be desired because convergence is only guaranteed when the classes are
    linearly separable that is often not the case.
  prefs: []
  type: TYPE_NORMAL
- en: In this article we will continue the journey on mastering classification concepts.
    A natural evolution from the Rosenblatt’s perceptron is the **ada**ptive **li**near
    **ne**uron classifier, or adaline as it is colloquially known. Moving from the
    perceptron to adaline is not a big leap. We simply need to change the step activation
    function to a linear one. This small change leads to a continuous loss function
    that can be robustly minimised. This allows us to introduce many useful concepts
    in machine learning, such as vectorisation and optimisation methods.
  prefs: []
  type: TYPE_NORMAL
- en: In future articles we will also cover further subtle changes of the activation
    and loss functions that will take us from adaline to logistic regression, that
    is already a useful algorithm in daily practice. All of the above algorithms are
    essentially single layer neural networks and can be readily extended to multilayer
    ones. In this sense, this article takes the reader a step further through this
    evolution and builds the foundations to tackle more advanced concepts.
  prefs: []
  type: TYPE_NORMAL
- en: We will need some formulas. I used the online [LaTeX equation editor](https://latexeditor.lagrida.com/)
    to develop the LaTeX code for the equation and then the chrome plugin [Maths Equations
    Anywhere](https://chromewebstore.google.com/detail/math-equations-anywhere/fkioioejambaepmmpepneigdadjpfamh)
    to render the equation into an image. The only downside of this approach is that
    the LaTeX code is not stored in case you need to render it again. For this purpose
    I provide the list of equations at the end of this article. If you are not familiar
    with LaTex this may have its own educational value. Getting the notation right
    is part of the journey in machine learning.
  prefs: []
  type: TYPE_NORMAL
- en: Adaptive linear neuron classifier (adaline)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'So what is the adaline algorithm? Adaline is a binary classifier as the perceptron.
    A prediction is made by using a set of input values for the features [x₁, .. ,
    xₘ] where m is the number of features. The input values are multiplied with the
    weights [w₁, .. , wₘ] and the bias is added to obtain the net input z = w₁x₁ +
    .. + wₘxₘ + b. The net input is passed to the linear activation function σ(z)
    that is then used to make a prediction using a step function as with the perceptron:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4de8a2c1883431481a490f75dae9cbc9.png)'
  prefs: []
  type: TYPE_IMG
- en: A key difference with the perceptron is that the linear activation function
    is used for learning the weights, whilst the step function is only used for making
    the prediction at the end. This sounds like a small thing, but it is of significant
    importance. The linear activation function is differentiable whilst the step function
    is not! The threshold 0.5 above is not written in stone. By adjusting the threshold
    we can regulate the precision and recall according to our use case, i.e. based
    on what is the cost of false positives and false negatives.
  prefs: []
  type: TYPE_NORMAL
- en: In the case of adaline the linear activation function is simply the identity,
    i.e. σ(z) = z. The objective function (also known as loss function) that needs
    to be minimised in the training process is
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/951717d16569b60b106264072ed64894.png)'
  prefs: []
  type: TYPE_IMG
- en: where **w** are the weights
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/950c38d1374475f49563667d652a7fab.png)'
  prefs: []
  type: TYPE_IMG
- en: and b is the bias. The summation is over all of the examples in the training
    set. In some implementations the loss function also includes a 1/2 coefficient
    for convenience. This cancels out once we take the gradients of the loss function
    with respect to the weights and bias and, as we will see below, has no effect
    other than scaling the learning rate by a factor of 2\. In this article we do
    not use the 1/2 coefficient.
  prefs: []
  type: TYPE_NORMAL
- en: For each example, we compute the square difference between the calculated outcome
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0428f21b171a5009335f18e58547e0d9.png)'
  prefs: []
  type: TYPE_IMG
- en: and the true class label. Note that the input vector is understood to be a matrix
    with shape (1, m), i.e. as we will see later is one row of our feature matrix
    **x** with shape (n, m).
  prefs: []
  type: TYPE_NORMAL
- en: The training is nothing else than an optimisation problem. We need to adjust
    the weights and bias so that the loss function is minimised. As with any minimisation
    problem we need to compute the gradients of the objective function with respect
    to the independent variables that in our case will be the weights and the bias.
    The partial derivative of the loss function with regard to the weight wⱼ is
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/36280ce22e0f3219e9f107deaafaac5d.png)'
  prefs: []
  type: TYPE_IMG
- en: The last row introduces important matrix notation. The feature matrix **x**
    has shape (n, m) and we take the transpose of its column j, i.e. a matrix with
    shape (1, n). The true class labels **y** is a matrix with shape (n, 1). The net
    output of all samples **z** is also a matrix with shape (n, 1), that does not
    change after the activation that is understood to apply to each of its elements.
    The final result of the above formula is a scalar. Can you guess how we could
    express the gradients with respect to all weights using the matrix notation?
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b8f325ce6ba59bbc67f22bce6525a850.png)'
  prefs: []
  type: TYPE_IMG
- en: where the transpose of the feature matrix has shape (m, n). The end result of
    this operation is a matrix with shape (m, 1). This notation is important. Instead
    of using loops, we will be using exactly this matrix multiplication using [numpy](https://numpy.org/).
    In the era of neural networks and GPUs, the ability to apply vectorization is
    essential!
  prefs: []
  type: TYPE_NORMAL
- en: What about the gradient of the loss function with respect to the bias?
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b9670bba511407e32efb41e492f9c4e8.png)'
  prefs: []
  type: TYPE_IMG
- en: where the overbar denotes the mean of the vector under it. Once more, computing
    the mean with numpy is a vectorised operation, i.e. summation does not need to
    be implemented using a loop.
  prefs: []
  type: TYPE_NORMAL
- en: Once we have the gradients we can employ the gradient descent optimisation method
    to minimise the loss. The weights and bias terms are iteratively updated using
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/769ddba3bcab91dcc4cc72ee5b19191d.png)'
  prefs: []
  type: TYPE_IMG
- en: where η is a suitable chosen learning rate. Too small values can delay convergence,
    whilst too high values can prevent convergence altogether. Some experimentation
    is needed, as is generally the case with the parameters of machine learning algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: In the above implementation we assume that the weights and bias are updated
    based on all examples at once. This is known as full batch gradient descent and
    is one extreme. The other extreme is to update the weights and bias after each
    training example, that is known as stochastic gradient descent (SGD). In reality
    there is also some middle ground, known as mini batch gradient descent, where
    the weights and bias are updated based on a subset of the examples. Convergence
    is typically reached faster in this way, i.e. we do not need to run as many iterations
    over the whole training set, whilst vectorisation is still (at least partially)
    possible. If the training set is very large (or the model is very complex as is
    nowadays the case with the transformers in NLP) full batch gradient descent may
    simply be not an option.
  prefs: []
  type: TYPE_NORMAL
- en: Alternative formulation and closed form solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Before we proceed with the implementation of adaline in Python, we will make
    a quick digression. We could absorb the bias b in the weight vector as
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6ff96cb3272df542dfd4908785bed347.png)'
  prefs: []
  type: TYPE_IMG
- en: in which case the net output for all samples in the training set becomes
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/074d30b29d129705bfdffc2f75f2605e.png)'
  prefs: []
  type: TYPE_IMG
- en: meaning that the feature matrix has been prepended with a column filled with
    1, leading to a shape (n, m+1). The gradient with regard to the combined weights
    set becomes
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/514ef3e1d6948525ce50b24bcc970cb7.png)'
  prefs: []
  type: TYPE_IMG
- en: In principle we could derive a closed form solution given that at the minimum
    all gradients will be zero
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/04f9d0536e724cc5365846649870a4cc.png)'
  prefs: []
  type: TYPE_IMG
- en: In reality the inverse of the matrix in the above equation may not exist because
    of singularities or it cannot be computed sufficiently accurately. Hence, such
    closed form solution is not used in practice neither in machine learning nor in
    numerical methods in general. Still, it is useful to appreciate that adaline resembles
    linear regression and as such it has a closed form solution.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing adaline in Python
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Our implementation will use mini batch gradient descent. However, the implementation
    is flexible and allows optimising the loss function using both stochastic gradient
    descent and full batch gradient descent as the two extremes. We will examine the
    convergence behaviour by varying the batch size.
  prefs: []
  type: TYPE_NORMAL
- en: We implement adaline using a class that exposes a fit and a predict function
    in the usual [scikit-learn](https://scikit-learn.org/stable/) API style.
  prefs: []
  type: TYPE_NORMAL
- en: Upon initialisation the adaline classifier sets the batch size for the mini
    batch gradient descent. If batch size is set to None, the whole training set is
    used (full batch gradient descent), otherwise the training set is used in batches
    (mini batch gradient descent). If the batch size is one we essentially revert
    to stochastic gradient descent. The training set is shuffled before each pass
    through the training set to avoid repetitive cycles, but this only has an effect
    if mini batch gradient descent is used. The essence of the algorithm is in the
    `_update_weights_bias` function that carries out a full pass through the training
    set and returns the corresponding loss. This function applies the gradient descent
    with the analytically computed gradients using the derivations as in the previous
    section. Note the use of the numpy `matmul` and `dot` functions that avoid the
    use of explicit loops. If the batch_size is set to None then there are no loops
    whatsoever and the implementation is fully vectorised.
  prefs: []
  type: TYPE_NORMAL
- en: Using adaline in practice
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We make the necessary imports and create a synthetic dataset as in the earlier
    perceptron [article](/classification-with-rosenblatts-perceptron-e7f49e3af562)
  prefs: []
  type: TYPE_NORMAL
- en: that produces
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5dee3d70e7c16dba4c0a25bf0a52e432.png)'
  prefs: []
  type: TYPE_IMG
- en: Scatterplot of the two classes in the synthetic dataset. Image by the Author.
  prefs: []
  type: TYPE_NORMAL
- en: The only difference with the earlier article is that we tweaked the gaussian
    means and covariances so that the classes are not linearly separable as we would
    expect adaline to overcome this. Moreover, the two independent variables have
    on purpose different scales to discuss the importance of feature scaling.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s try to fit a first model and visualise convergence. Prior to fitting we
    normalise the features so that they both have zero mean and unit standard deviation
  prefs: []
  type: TYPE_NORMAL
- en: This produces the convergence plot
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/98546345ebcf7fad86898a6205443025.png)'
  prefs: []
  type: TYPE_IMG
- en: Adaline convergence. Image by the Author.
  prefs: []
  type: TYPE_NORMAL
- en: Adaline slowly converges, but the loss function does not become zero. In order
    to verify the successful training we visualise the decision boundary using the
    same approach as in the earlier article
  prefs: []
  type: TYPE_NORMAL
- en: that produces
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c30747fdfbb6ca87ec43952aaff19d3b.png)'
  prefs: []
  type: TYPE_IMG
- en: Decision boundary of the fitted adaline model. Image by the Author.
  prefs: []
  type: TYPE_NORMAL
- en: There are some misclassified points given that the two classes in the training
    set were not linearly separable and we used a linear decision boundary. Still,
    the algorithm converged nicely. The solution is deterministic. With sufficient
    number of passes through the training set we obtain numerically equal weights
    and bias, regardless of their initial values.
  prefs: []
  type: TYPE_NORMAL
- en: '**Mini batch vs. full batch gradient descent**'
  prefs: []
  type: TYPE_NORMAL
- en: The above numerical experiment used full batch gradient descent that partially
    explains the slow convergence. We will use the same dataset and random state as
    before, but this time we will fit the adaline classifier using different batch
    sizes, ranging from 20 to 400 that is the number of examples in our training set.
  prefs: []
  type: TYPE_NORMAL
- en: that produces
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f400fc967309960fb862c808c1c04073.png)'
  prefs: []
  type: TYPE_IMG
- en: Effect of batch size on convergence (using a 0.001 learning rate). Image by
    the Author.
  prefs: []
  type: TYPE_NORMAL
- en: We can clearly see that the smaller the batch size the faster the convergence,
    but there are also some oscillations. These oscillation may destabilise the convergence
    with larger learning rates. If we double the learning rate to 0.002 this becomes
    evident
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/44a8a9c76b5e33ccf0783f8963a9265c.png)'
  prefs: []
  type: TYPE_IMG
- en: Effect of batch size on convergence (using a 0.002 learning rate). Image by
    the Author.
  prefs: []
  type: TYPE_NORMAL
- en: Increasing the learning rate further will eventually prevent convergence with
    the smaller batch sizes. With even larger learning rates even the full batch gradient
    descent will fail to converge as we would overshoot the global minimum.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Adaline is a significant improvement over the perceptron. The weights and bias
    are obtained via the minimisation of a continuous loss function that in addition
    is convex (and hence does not have local minima). With a sufficient small learning
    rate the algorithm converges even if the classes are not linearly separable. When
    using gradient descent in any of its variants the convergence rate is affected
    by the scaling of the features. In this article we used simple standardisation
    that shifts the mean of every feature to become zero, whilst the spread is adjusted
    to unit variance. In this way it is possible to select a learning rate that works
    well for all weights and bias, meaning that the global minimum can be obtained
    in fewer epochs.
  prefs: []
  type: TYPE_NORMAL
- en: Obtaining a good understanding on how to build a binary classifier using vectorisation
    is key before delving into more complex topics, such as support vector machines
    and multilayer neural networks. In daily practice, one would use [scikit-learn](https://scikit-learn.org/stable/)
    that offers advanced classification algorithms that allow for nonlinear decision
    boundaries, whilst supporting efficient and systematic hyper parameter tuning
    and cross validation. However, building simple binary classifiers from scratch
    offers a deep understanding, increases confidence and gives a sense of ownership.
    Although building everything from scratch is of course not realistic, deeply understanding
    the simpler algorithms provides the necessary skills and insights so that more
    advanced algorithms included in off-the-shelf libraries feel less opaque.
  prefs: []
  type: TYPE_NORMAL
- en: LaTeX code of equations used in the article
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The equations used in the article can be found in the gist below, in case you
    would like to render them again.
  prefs: []
  type: TYPE_NORMAL
