- en: A Complete Guide to Effectively Scale your Data Pipelines and Data Products
    with Contract Testing and dbt
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/how-to-scale-your-data-pipelines-and-data-products-with-dbt-and-contract-testing-10c92ea9a443?source=collection_archive---------2-----------------------#2023-10-25](https://towardsdatascience.com/how-to-scale-your-data-pipelines-and-data-products-with-dbt-and-contract-testing-10c92ea9a443?source=collection_archive---------2-----------------------#2023-10-25)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: All you need to know to start implementing contract tests with dbt
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@pablo.porto?source=post_page-----10c92ea9a443--------------------------------)[![Pablo
    Porto](../Images/acfca713c40ae7f2b86756fb39402c60.png)](https://medium.com/@pablo.porto?source=post_page-----10c92ea9a443--------------------------------)[](https://towardsdatascience.com/?source=post_page-----10c92ea9a443--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----10c92ea9a443--------------------------------)
    [Pablo Porto](https://medium.com/@pablo.porto?source=post_page-----10c92ea9a443--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fa6f9bc7e34a6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-scale-your-data-pipelines-and-data-products-with-dbt-and-contract-testing-10c92ea9a443&user=Pablo+Porto&userId=a6f9bc7e34a6&source=post_page-a6f9bc7e34a6----10c92ea9a443---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----10c92ea9a443--------------------------------)
    ·11 min read·Oct 25, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F10c92ea9a443&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-scale-your-data-pipelines-and-data-products-with-dbt-and-contract-testing-10c92ea9a443&user=Pablo+Porto&userId=a6f9bc7e34a6&source=-----10c92ea9a443---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F10c92ea9a443&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-scale-your-data-pipelines-and-data-products-with-dbt-and-contract-testing-10c92ea9a443&source=-----10c92ea9a443---------------------bookmark_footer-----------)![](../Images/7762e018a2f9b67dff6d9432dcb7e350.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Photo by [Jonas Gerg](https://unsplash.com/@walamana?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: Let me tell you a story about data management systems and scale that will probably
    resonate with you if you are a data or analytics engineer trying to do your best
    work in 2023.
  prefs: []
  type: TYPE_NORMAL
- en: Not too long ago, almost all data architectures and data team structures followed
    [a centralized approach](https://www.getdbt.com/data-teams/centralized-vs-decentralized#the-centralized-model).
    As a data or analytics engineer, you knew where to find all the transformation
    logic and models because they were all in the same codebase. You probably work
    closely with the colleague who builds the data pipeline that you were consuming.
    There was only one data team, two at most.
  prefs: []
  type: TYPE_NORMAL
- en: This approach was effective for small organizations and startups with limited
    data sources and use cases. It also worked for large enterprises not fully focused
    on extracting value from data at scale. However, as organizations prioritized
    being data-driven, there was an increased need for more machine learning, analytics,
    and business intelligence data use cases.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/aaa21e48e65eaf597be05bb80aa6c87a.png)'
  prefs: []
  type: TYPE_IMG
- en: Centralized data architecture developed and maintained by one team
  prefs: []
  type: TYPE_NORMAL
- en: The proliferation of use cases and data sources increased the complexity of
    managing data and the amount of people needed to create and maintain data systems.
    To meet these needs, the latest version of your company data strategy may have
    moved towards decentralization. This includes forming decentralized data teams
    and the adoption of decentralized data architectures like [Data Mesh](https://martinfowler.com/articles/data-mesh-principles.html).
  prefs: []
  type: TYPE_NORMAL
- en: Decentralization allows organizations to scale data management but brings new
    challenges in ensuring the coordination of different components, such as data
    products and pipelines developed and managed by various teams.
  prefs: []
  type: TYPE_NORMAL
- en: In this type of architecture and organisation structures, it often becomes unclear
    who is accountable for each component resulting in issues and blame shifting.
    The number of integration points between teams also increases and maintaining
    working interfaces between the different components becomes harder.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8f43f4b098d08d37d8223b1a4c3947ca.png)'
  prefs: []
  type: TYPE_IMG
- en: Decentralized data architecture with multiple teams and multiple components
  prefs: []
  type: TYPE_NORMAL
- en: '**If you can relate to this situation, you are not alone. Your organization
    may be undergoing the decentralization of data.** To navigate this transition,
    we can learn from successful implementations of decentralization and distributed
    architectures like microservices in the operational world. How did they do it?
    How did they manage to provide reliable systems at that scale? Well, they **leveraged
    modern testing techniques.**'
  prefs: []
  type: TYPE_NORMAL
- en: “For years, software engineering has successfully embraced the concept of small
    units of work performed by ‘two-pizza teams’. Each team owns its own component
    of a larger system. Teams integrate with one another through well-defined, versioned
    interfaces. Sadly, data has yet to catch up with software. Monolithic data architecture
    is still the norm — even though there are clear drawbacks.” — [dbt labs](https://www.getdbt.com/blog/what-is-data-mesh-the-definition-and-importance-of-data-mesh)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'In this article, I will introduce one of those techniques: contract testing.
    I will show **how you can use** [**dbt**](https://www.getdbt.com/product/what-is-dbt)
    **to create simple contract tests for your upstream sources and your dbt models’
    public interfaces**. This type of test will keep you sane as your dbt apps become
    more complex and decentralized.'
  prefs: []
  type: TYPE_NORMAL
- en: But… what is a contract test?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When a distributed system starts growing into multiple components developed
    by several teams, the first approach teams may try to follow to verify that the
    system is behaving as expected is **implementing end-to-end tests that exercise
    the system as a whole**.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/670bc60845697f0967a6cf9b588cd5ba.png)'
  prefs: []
  type: TYPE_IMG
- en: End to end test scope focused on verifying the system as a whole
  prefs: []
  type: TYPE_NORMAL
- en: End-to-end tests often become very hard to work with due to their complexity,
    slow feedback, and how hard they are to maintain and orchestrate.
  prefs: []
  type: TYPE_NORMAL
- en: That was the case in the operational world when implementing microservices at
    scale. When testing the system as a whole is not an option, engineering teams
    started implementing different approaches, like contract testing.
  prefs: []
  type: TYPE_NORMAL
- en: '***“An integration contract test is a test at the boundary of an external service
    verifying that it meets the contract expected by a consuming service.”* —** [Toby
    Clemson](https://martinfowler.com/articles/microservice-testing/#testing-contract-introduction)'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Teams can still keep a small set of end-to-end tests, but they move down the
    testing pyramid by verifying the system with faster and more reliable tests like
    contract, component, and unit tests.
  prefs: []
  type: TYPE_NORMAL
- en: The trade-offs of different test types are often visualized with a testing pyramid.
    I mentioned this concept in my previous article about [implementing unit testing
    for dbt models](/improving-the-code-quality-of-your-dbt-models-with-unit-tests-and-tdd-203ed0be791e).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e35ca1e5c8d8700590bc1bb6c3370145.png)'
  prefs: []
  type: TYPE_IMG
- en: '[Typical test pyramid](https://martinfowler.com/articles/practical-test-pyramid.html)
    for operational systems'
  prefs: []
  type: TYPE_NORMAL
- en: 'If we apply the same concept to data management systems, contract tests for
    dbt apps can be implemented to verify the behavior of two types of interfaces:'
  prefs: []
  type: TYPE_NORMAL
- en: The upstream sources.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The public interfaces, like marts and output ports.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/9473d0a15f28704bca895b443739e3e9.png)'
  prefs: []
  type: TYPE_IMG
- en: Contract tests scope
  prefs: []
  type: TYPE_NORMAL
- en: Benefits of contract testing for data systems
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As we have seen, data architectures are becoming more complex and decentralised
    as it once happened with operational services. As this type of system continues
    to scale, the ability to run maintainable and effective end-to-end test suites
    diminishes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Contract testing becomes a powerful ally in managing different situations by
    providing several advantages:'
  prefs: []
  type: TYPE_NORMAL
- en: Reducing the number of end to end tests needed to verify the system’s behavior.
    Leading to faster feedback and lower maintainability costs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Manage the complexity of having separate data teams working in the same code
    base by providing clear expectations between team public interfaces.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Surface integration issues between components in lower environments before they
    reach production.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Better defined and documented interfaces between the different data pipelines
    or data products.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Contract tests vs data quality test
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You may be thinking, but… the contract test concept sounds like the quality
    test we are already running in our data pipelines.
  prefs: []
  type: TYPE_NORMAL
- en: That is a fair observation as there is a blurred line between the contract tests
    and data quality tests scopes. I like to think of contract tests as a subset of
    quality tests as part of a modern data testing strategy.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b2eea1a8d22d90b40c4bc9a962306481.png)'
  prefs: []
  type: TYPE_IMG
- en: Contract tests can be considered a subset of quality tests
  prefs: []
  type: TYPE_NORMAL
- en: The difference is that **a contract test looks at the schema and constraints
    whereas the data quality test looks at the actual data and its characteristics**.
    Let’s look at some examples.
  prefs: []
  type: TYPE_NORMAL
- en: '**Contract tests scope:**'
  prefs: []
  type: TYPE_NORMAL
- en: Checking column types.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Checking expected constraints at the schema level like primary and foreign keys,
    not null columns.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Checking accepted values for a given column.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Checking valid ranges for a given column.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Quality tests scope:**'
  prefs: []
  type: TYPE_NORMAL
- en: Assessing completeness, e.g. percentage of not nulls in a column.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Assessing uniqueness, e.g. number of rows that are not unique.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Assessing consistency, e.g. all user identifiers in the source are included
    in the output.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing our first contract test
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Okay, enough theory, let’s get into action with a simple example. We have a
    dbt app called **health-insights** that takes weight and height data from upstream
    data sources and calculates the body mass index metric.
  prefs: []
  type: TYPE_NORMAL
- en: Our colleagues from the amazing backend team are in charge of producing the
    weight and height data we need to build our health-insights app. They work in
    a different team which is a bit busy and stressed. Sometimes they fail to notify
    us of schema changes. To test against these changes in upstream interfaces, we
    decided to create our first source contract test.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9ec94b8abce79c7e5d79a3c97fc152e0.png)'
  prefs: []
  type: TYPE_IMG
- en: The system architecture of our example
  prefs: []
  type: TYPE_NORMAL
- en: First, we need to add two new dbt packages, [dbt-expectations](https://github.com/calogica/dbt-expectations)
    and dbt-utils, that will allow us to make assertions on the schema of our sources
    and the accepted values.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Testing the data sources
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s start by defining a contract test for our first source. We pull data from
    **raw_height**, a table that contains height information from the users of the
    gym app.
  prefs: []
  type: TYPE_NORMAL
- en: We agree with our data producers that we will receive the height measurement,
    the units for the measurements, and the user ID. We agree on the data types and
    that only ‘cm’ and ‘inches’ are supported as units. With all this, we can define
    our first contract in the dbt source YAML file.
  prefs: []
  type: TYPE_NORMAL
- en: The building blocks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Looking at the previous test, we can see several of the dbt-unit-testing macros
    in use:'
  prefs: []
  type: TYPE_NORMAL
- en: '**dbt_expectations.expect_column_values_to_be_of_type:**This assertion allows
    us to define the expected column data type.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**accepted_values:**This assertion allows us to define a list of the accepeted
    values for a specific column.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**dbt_utils.accepted_range:** This assertion allows us to define a numerical
    range for a given column. In the example, we expected the column’s value not to
    be less than 0.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**not null:** Finally, built-in assertions like ‘not null’ allow us to define
    column constraints.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Using these building blocks, we added several tests to define the contract
    expectations described above. Notice also how we have tagged the tests as “contract-test-source”.
    This tag allows us to run all contract tests in isolation, both locally, and as
    we will see later, in the CI/CD pipeline:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Implementing contract tests for marts and output ports
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have seen how quickly we can create contract tests for the sources of our
    dbt app, but what about the public interfaces of our data pipeline or data product?
  prefs: []
  type: TYPE_NORMAL
- en: As data producers, we want to make sure we are producing data according to the
    expectations of our data consumers so we can satisfy the contract we have with
    them and [make our data pipeline or data product trustworthy and reliable](https://www.thoughtworks.com/en-es/insights/blog/data-strategy/building-an-amazon-com-for-your-data-products).
  prefs: []
  type: TYPE_NORMAL
- en: A simple way to ensure that we are meeting our obligations to our data consumers
    is to add contract testing for our public interfaces.
  prefs: []
  type: TYPE_NORMAL
- en: Dbt [recently released a new feature](https://docs.getdbt.com/docs/collaborate/govern/model-contracts)
    for SQL models, **model contracts**, that allows to define the contract for a
    dbt model. While building your model, **dbt will verify that your model’s transformation
    will produce a dataset matching up with its contract, or it will fail to build.**
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s see it in action. Our mart, **body_mass_indexes**, produces a BMI metric
    from the weight and height measure data we get from our sources. The contract
    with our provider establishes the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Data types for each column.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: User IDs cannot be null
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: User IDs are always greater than 0
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let’s define the contract of the body_mass_indexes model using dbt model contracts:'
  prefs: []
  type: TYPE_NORMAL
- en: The building blocks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Looking at the previous model specification file, we can see several metadata
    that allow us to define the contract.
  prefs: []
  type: TYPE_NORMAL
- en: '**contract.enforced:**This configuration tells dbt that we want to enforce
    the contract every time the model is run.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**data_type:**This assertion allows us to define the column type we are expecting
    to produce once the model runs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**constraints:** Finally, the constraints block gives us the chance to define
    useful constraints like that a column cannot be null, set primary keys, and custom
    expressions. In the example above we defined a constraint to tell dbt that the
    user_id must always be greater than 0\. You can see all the available constraints
    [here](https://docs.getdbt.com/reference/resource-properties/constraints).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Source contract tests vs dbt model contracts
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A difference between the contract tests we defined for our sources and the ones
    defined for our marts or output ports is **when the contracts are verified and
    enforced.**
  prefs: []
  type: TYPE_NORMAL
- en: Dbt enforces [model contracts](https://docs.getdbt.com/docs/collaborate/govern/model-contracts)
    when the model is being generated by ‘dbt run’, whereas contracts based on [dbt
    tests](https://docs.getdbt.com/docs/build/tests) are enforced when the dbt tests
    run.
  prefs: []
  type: TYPE_NORMAL
- en: If one of the model contracts is not satisfied, you will see an error when you
    execute ‘dbt run’ with specific details on the failure. You can see an example
    in the following dbt run console output.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Running the contract tests in the pipeline
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Until now we have a test suite of powerful contract tests, but how and when
    do we run them?
  prefs: []
  type: TYPE_NORMAL
- en: We can run contract tests in two types of pipelines.
  prefs: []
  type: TYPE_NORMAL
- en: '**CI/CD pipelines**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data pipelines**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For example, you can execute the source contract tests on a schedule in a **CI/CD
    pipeline** targeting the data sources available in lower environments like test
    or staging. You can set the pipeline to fail every time the contract is not met.
  prefs: []
  type: TYPE_NORMAL
- en: These failures provides valuable information about contract-breaking changes
    introduced by other teams before these changes reach production.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e8652099a68c59a64267a0a9cf807fa2.png)'
  prefs: []
  type: TYPE_IMG
- en: '[Example of a dbt CI/CD pipeline in Github Actions](https://github.com/portovep/dbt-unit-testing-examples/actions/workflows/cd-pipeline.yml)'
  prefs: []
  type: TYPE_NORMAL
- en: You can also run your output port/mart contract tests each time you deploy a
    new change via the CI/CD pipeline. As dbt model contracts are checked every time
    the model is built, you tell dbt to enforce the contract so that if the new model
    changes introduce a breaking change in the contract, your team will get notified
    before your data consumers get impacted.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, you can also run your source and output port/mart contract tests in
    **your data pipelines in production**. Running contract tests in production can
    help your team understand if a data pipeline failed because one of the upstream
    dependencies broke the contract or because the data you are producing is not meeting
    the contract with your downstream consumers.
  prefs: []
  type: TYPE_NORMAL
- en: Additional tips to get started
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Start small, testing the integration points that are more brittle and prone
    to failure.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apply [the tolerant reader pattern](https://martinfowler.com/bliki/TolerantReader.html)
    when implementing contract tests. Only assert on the data you need.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tweak the behavior of the contract test based on your needs, you can configure
    [**the severity attribute**](https://docs.getdbt.com/reference/resource-configs/severity)to
    make them fail loudly or just throw a warning.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Integrate these types of tests [with modern data observability tools like Montecarlo](https://docs.getmontecarlo.com/docs/dbt-failures-are-monte-carlo-incidents)
    so they are part of your incident management process.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Leverage dbt contract tests even if your data systems are not developed using
    dbt**. You can still define source contract tests in dbt and execute them against
    tables or files created with other frameworks or plain SQL.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Considering more advanced contract testing techniques like [consumer-driven
    contracts](https://www.martinfowler.com/articles/consumerDrivenContracts.html)
    could make it easier to implement contract testing in specific contexts.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have seen how testing strategies for data systems can also benefit from testing
    techniques like contract testing as these systems become more decentralized and
    grow in complexity.
  prefs: []
  type: TYPE_NORMAL
- en: 'We also saw how to start implementing contract tests leveraging dbt built-in
    features and additional dbt packages. We apply these type of test to two integration
    points: upstream data sources and data marts/output ports.'
  prefs: []
  type: TYPE_NORMAL
- en: I hope this article gives you and your team all the tools and tips to **start
    implementing contract tests as your data systems scale to fulfill new data use
    cases**. If you are curious, you can check the source code of the example dbt
    application in [this Github repo](https://github.com/portovep/dbt-testing-examples).
  prefs: []
  type: TYPE_NORMAL
- en: Are you ready to give it a go and start your contract testing journey? I would
    love to hear your thoughts and experience in the comments.
  prefs: []
  type: TYPE_NORMAL
- en: '*This article is part of series of articles I am writing on* [*testing data
    pipelines and data products.*](https://medium.com/@pablo.porto/list/testing-data-products-and-data-pipelines-with-dbt-52eaff0e92dd)'
  prefs: []
  type: TYPE_NORMAL
- en: '*I am always looking forward to meeting new people. If you want to connect,
    you can find me on* [*Linkedin*](https://www.linkedin.com/in/pabloportoveloso/)*,*
    [*Github*](https://github.com/portovep)*,* [*Instagram*](https://www.instagram.com/porto.vga/),
    [*Substack*](https://pabsgarage.substack.com/), *or on* [*my personal website.*](https://pabloporto.me)'
  prefs: []
  type: TYPE_NORMAL
- en: '*Thanks to my Thoughtworks colleagues Arne, Manisha and David for taking the
    time to review early versions of this article. Thanks to the maintainers of the*
    [*dbt-expectations package*](https://github.com/calogica/dbt-expectations) *for
    their great work.*'
  prefs: []
  type: TYPE_NORMAL
- en: '*All images unless otherwise noted are by the author.*'
  prefs: []
  type: TYPE_NORMAL
