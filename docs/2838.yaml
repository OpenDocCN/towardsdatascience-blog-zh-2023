- en: 'Reinforcement Learning: an Easy Introduction to Value Iteration'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ÂéüÊñáÔºö[https://towardsdatascience.com/reinforcement-learning-an-easy-introduction-to-value-iteration-e4cfe0731fd5?source=collection_archive---------0-----------------------#2023-09-10](https://towardsdatascience.com/reinforcement-learning-an-easy-introduction-to-value-iteration-e4cfe0731fd5?source=collection_archive---------0-----------------------#2023-09-10)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Learn the fundamentals of RL and how to apply Value Iteration to a simple example
    problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@carlbettosi?source=post_page-----e4cfe0731fd5--------------------------------)[![Carl
    Bettosi](../Images/19c640d8e96fa39583a3c998764aa2b8.png)](https://medium.com/@carlbettosi?source=post_page-----e4cfe0731fd5--------------------------------)[](https://towardsdatascience.com/?source=post_page-----e4cfe0731fd5--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----e4cfe0731fd5--------------------------------)
    [Carl Bettosi](https://medium.com/@carlbettosi?source=post_page-----e4cfe0731fd5--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ¬∑
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fabe6f5e189c8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-an-easy-introduction-to-value-iteration-e4cfe0731fd5&user=Carl+Bettosi&userId=abe6f5e189c8&source=post_page-abe6f5e189c8----e4cfe0731fd5---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----e4cfe0731fd5--------------------------------)
    ¬∑15 min read¬∑Sep 10, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fe4cfe0731fd5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-an-easy-introduction-to-value-iteration-e4cfe0731fd5&user=Carl+Bettosi&userId=abe6f5e189c8&source=-----e4cfe0731fd5---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fe4cfe0731fd5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-an-easy-introduction-to-value-iteration-e4cfe0731fd5&source=-----e4cfe0731fd5---------------------bookmark_footer-----------)![](../Images/291df786927604981f62437fda3c5b4f.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Value Iteration (VI) is typically one of the first algorithms introduced on
    the Reinforcement Learning (RL) learning pathway. The underlying specifics of
    the algorithm introduce some of the most fundamental aspects of RL and, hence,
    it is important to master VI before progressing to more complex RL algorithms.
    Yet, this can be tricky to get your head around.
  prefs: []
  type: TYPE_NORMAL
- en: This article is designed to be an easy-to-understand introduction to VI, which
    will assume the reader to be new to the field of RL. Let‚Äôs get started.
  prefs: []
  type: TYPE_NORMAL
- en: Already know the basics of RL? ‚Üí [**Skip to how to use Value Iteration**](#dab6).
  prefs: []
  type: TYPE_NORMAL
- en: The basics of RL
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let‚Äôs start with a textbook definition, then I‚Äôll break it down using an easy
    example.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/bdfd099792f5ee2310c43662522084b7.png)'
  prefs: []
  type: TYPE_IMG
- en: Overview of the reinforcement learning training process
  prefs: []
  type: TYPE_NORMAL
- en: RL is one of the three key machine learning paradigms beside supervised and
    unsupervised learning. RL is not a singular algorithm, but rather a framework
    which encompasses a range of techniques and approaches for teaching agents to
    learn and make decisions in their environments.
  prefs: []
  type: TYPE_NORMAL
- en: In RL, an agent interacts with an environment by taking various actions. The
    agent is rewarded when those actions lead to desired states and punished when
    they don‚Äôt. The agent‚Äôs objective is to learn a strategy, called a policy, that
    guides its actions to maximize the reward it accumulates over time. This trial-and-error
    process refines the agent's behavioral policy, allowing it to take optimal or
    near-optimal behaviors in its environment.
  prefs: []
  type: TYPE_NORMAL
- en: '*The book An ‚ÄúIntroduction to Reinforcement Learning‚Äù by Richard S. Sutton
    and Andrew G. Barto is considered the best in the field for those wishing to gain
    a solid understanding of RL.. and it‚Äôs* [*available for free*](http://incompleteideas.net/book/the-book-2nd.html)*!*'
  prefs: []
  type: TYPE_NORMAL
- en: Let‚Äôs define an example problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/25fc75950f08dcf8fbb9ceba748c9fa4.png)'
  prefs: []
  type: TYPE_IMG
- en: Possible states for the game of golf
  prefs: []
  type: TYPE_NORMAL
- en: This image depicts the game of golf in its simplest form. We will use this as
    golf has a clearly defined goal - get the ball in the hole.
  prefs: []
  type: TYPE_NORMAL
- en: 'In our example, the golf ball can be in one of three positions: *on the fairway*;
    *on the green*; or *in the hole*. We start on the fairway and aim to get closer
    to the hole with each shot, with the hole sitting on the green.'
  prefs: []
  type: TYPE_NORMAL
- en: In RL, each of these positions is referred to as a ***state*** of the ***environment.***
    You can think of the state as being a snapshot of the current environment (the
    golf course), which also records the ball position.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2938c430e74f76f0d14cc0001e1ae266.png)'
  prefs: []
  type: TYPE_IMG
- en: Possible actions in the game of golf
  prefs: []
  type: TYPE_NORMAL
- en: 'In our game, the ***agent*** takes shots of hitting the ball, starting at the
    *ball on fairway* state. The ***agent*** simply refers to the entity that is in
    control of taking ***actions***. Our game has three available actions: *hit to
    fairway*; *hit to green*; and *hit in hole*.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/110c6f6d02f7fb59ee2b805e836a368b.png)'
  prefs: []
  type: TYPE_IMG
- en: Transition probabilities in the game of golf
  prefs: []
  type: TYPE_NORMAL
- en: Of course, when you take a shot, the ball may not go where you want it to. Therefore,
    we introduce a ***transition function*** linking actions to states with some probability
    weighting.
  prefs: []
  type: TYPE_NORMAL
- en: For example, we may miss the green when we take a shot from the fairway and
    end up still on the fairway. Written in an RL context, if we are in the *ball
    on fairway* state and take the action *hit to green*, there is a 90% probability
    we will enter the *ball on green* state but a 10% probability we will re-enter
    the *ball on fairway* state.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d9a6e396a04c36b4d5f972989603693b.png)'
  prefs: []
  type: TYPE_IMG
- en: A reward of 10 for getting the ball in the hole
  prefs: []
  type: TYPE_NORMAL
- en: Every time the agent takes an action, we refer to this as a ***step*** through
    the environment. Based upon the action just taken, the agent observes the new
    state it ends up in as well as a ***reward***. A reward function is an incentive
    mechanism for pushing the agent in the right direction. In other words, we design
    the reward function to shape the desired behavior of our agent. In our simplified
    golf example, we provide a reward of 10 for getting the ball in the hole.
  prefs: []
  type: TYPE_NORMAL
- en: '*The design of environment dynamics (the transition and reward functions) is
    not a trivial task. If the environment does not represent the problem you are
    attempting to solve, the agent will learn a policy reflecting a correct solution
    for an incorrect problem. We will not cover such design elements here, but it‚Äôs
    worth noting.*'
  prefs: []
  type: TYPE_NORMAL
- en: Markov Decision Processes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To represent a problem in a way the agent understands, we must formalise it
    as a Markov Decision Process (MDP).
  prefs: []
  type: TYPE_NORMAL
- en: A MDP is a mathematical model which describes our problem in a structured way.
    It represents the agent's interactions with the environment as a sequential decision-making
    process (i.e., one action after another).
  prefs: []
  type: TYPE_NORMAL
- en: 'It consists of the environment dynamics (I‚Äôm going to add some math notation
    here to shorten things):'
  prefs: []
  type: TYPE_NORMAL
- en: a finite set of states **s ‚àà S**.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: a finite set of actions ***a* ‚àà *A***.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: a transition function ***T*(*s*‚Ä≤‚à£*s*,*a*)** returning the probability of reaching
    state ***s*‚Ä≤**, given the current state ***s***, the current action ***a***.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: a reward function ***R*(*s*,*a*,*s*‚Ä≤)** returning a scalar reward based on reaching
    next state ***s*‚Ä≤**, after being in state ***s***, and taking action ***a***.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Note that if there is some uncertainty or randomness involved in the transitions
    between states (i.e. taking the same action in the same state twice may lead to
    different outcomes), we refer to this as a* ***stochastic*** *MDP. We could also
    create a* ***deterministic*** *MDP, where transitions and rewards are entirely
    predictable. This means when an agent takes an action in a specific state, there
    is a one-to-one correspondence between the action and the resulting next state
    and the reward.*'
  prefs: []
  type: TYPE_NORMAL
- en: Visualised as an MDP, our golf problem looks pretty much the same as the images
    depicted earlier. We will use S = {s1, s2, s3} for shorthand.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d2778219029b8237cd1d52e14e59fb54.png)'
  prefs: []
  type: TYPE_IMG
- en: Our golf example problem written as an MDP
  prefs: []
  type: TYPE_NORMAL
- en: '*The use of an MDP assumes that what‚Äôs going to happen next in the environment
    only depends on what‚Äôs happening right now ‚Äî the current state and action ‚Äî and
    not on what happened before. This is called the* **Markov property***, and it‚Äôs
    important in RL as it reduces computational complexity. I‚Äôll explain this more
    later.*'
  prefs: []
  type: TYPE_NORMAL
- en: What is Value Iteration?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Value Iteration (VI) is an algorithm used to solve RL problems like the golf
    example mentioned above, where we have full knowledge of all components of the
    MDP. It works by iteratively improving its estimate of the ‚Äòvalue‚Äô of being in
    each state. It does this by considering the immediate rewards and expected future
    rewards when taking different available actions. These values are tracked using
    a value table, which updates at each step. Eventually, this sequence of improvements
    converges, yielding an optimal policy of state ‚Üí action mappings that the agent
    can follow to make the best decisions in the given environment.
  prefs: []
  type: TYPE_NORMAL
- en: VI leverages the concept of *dynamic programming*, where solving a big problem
    is broken down into smaller subproblems. To achieve this in VI, the *Bellman equation*
    is used to guide the process of iteratively updating value estimates for each
    state, providing a recursive relationship that expresses the value of a state
    in terms of the values of its neighbouring states.
  prefs: []
  type: TYPE_NORMAL
- en: This won‚Äôt make much sense now. The easiest way to learn VI is to break it down
    step-by-step, so let‚Äôs do that.
  prefs: []
  type: TYPE_NORMAL
- en: How does the Value Iteration algorithm work?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The image below depicts the steps of the algorithm. Don‚Äôt be put off, it's easier
    than it appears.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b6f032fc68d508cb4d115e760c583e68.png)'
  prefs: []
  type: TYPE_IMG
- en: The Value Iteration algorithm
  prefs: []
  type: TYPE_NORMAL
- en: Firstly, we need to define some parameters for our training.
  prefs: []
  type: TYPE_NORMAL
- en: Theta **Œ∏** represents a threshold for convergence. Once we reach **Œ∏**, we
    can terminate the training loop and generate the policy. It‚Äôs essentially just
    a way to ensure the policy we create is accurate enough. If we stop training too
    early, we may not learn the best actions to take.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gamma **Œ≥** represents the *discount factor*. This is a value which determines
    how much our agent values future rewards compared to immediate rewards. A higher
    discount factor (closer to 1) indicates that the agent values long-term rewards
    more, while a lower discount factor (closer to 0) places greater emphasis on immediate
    rewards.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*To understand the discount factor better, consider an RL agent playing chess.
    Let‚Äôs say you have the opportunity to capture your opponent‚Äôs queen in the next
    move, which would yield a significant immediate reward. However, you also notice
    that by sacrificing a less valuable piece now, you could set up a future advantage
    that might lead to checkmate and an even bigger reward later. The discount factor
    helps you balance this decision.*'
  prefs: []
  type: TYPE_NORMAL
- en: '**(1) Initialisation:** Now we have the parameters defined, we want to initialise
    our *value function* ***V(s)*** for all states in ***S***. This typically means
    we set all values to 0 (or some other arbitrary constant) for every state. Think
    of the value function as a table that tracks a value for each state, updating
    frequently.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/fb25e69630ea42956f30577387e322b3.png)'
  prefs: []
  type: TYPE_IMG
- en: An initialised value table
  prefs: []
  type: TYPE_NORMAL
- en: '**(2) Outer loop:** Now everything is set up, we can start the iterative process
    of updating our values. We begin in the outer loop, which repeats until the convergence
    criteria are met (until Œî < Œ∏).'
  prefs: []
  type: TYPE_NORMAL
- en: At each pass of the outer loop, we begin by setting Œî = 0\. Delta **Œî** is used
    to represent the change in value estimates across all states, and the algorithm
    continues iterating until this change Œîfalls below the specified threshold Œ∏.
  prefs: []
  type: TYPE_NORMAL
- en: '**(3) Inner loop:** For every state *s* in *S,* we:'
  prefs: []
  type: TYPE_NORMAL
- en: set a variable ***v*** to the current value of that state ***V(s)***, remember
    - this is fetched from our value table (so on the first pass, ***v*** = ***V(s)***
    = 0)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: perform the bellman equation to update ***V(s)***
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: update **Œî** (we‚Äôll come back to this)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**The Bellman Equation**'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ac783b3215bf86884f63526f4edb24b5.png)'
  prefs: []
  type: TYPE_IMG
- en: This line of the algorithm is the most important. It requires that we update
    the value of the current state we are looking at in the loop. This value is calculated
    by considering all available actions from that specific state (a 1-step look ahead).
    When we take each of those possible actions it will present us with a set of possible
    next states ***s*‚Ä≤** and respective rewards ***r.***
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8076df201e89c49f337967581486f369.png)'
  prefs: []
  type: TYPE_IMG
- en: 'So, for each of those next states ***s*‚Ä≤** and respective rewards ***r***,
    we perform ***p(s‚Ä≤, r|s, a)[r + Œ≥V(s‚Ä≤)].*** Let''s break this up:'
  prefs: []
  type: TYPE_NORMAL
- en: '***p(s‚Ä≤, r|s, a)*** the probability of being in state ***s***, taking action
    ***a***, and ending up in next state ***s*‚Ä≤**(this is just our transition function)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '***[r + Œ≥V(s‚Ä≤)]*** the reward ***r*** of ending up in next state ***s*‚Ä≤** (we
    get that from our reward function) *+* our discount **Œ≥** * by the value of that
    next state ***s*‚Ä≤** (we get that from our value table)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We then multiply these two parts ***p(s‚Ä≤, r|s, a) * [r + Œ≥V(s‚Ä≤)]***
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Remember, this calculation is just for **one** next state ***s‚Ä≤*** (the third
    level of the tree), we need to repeat this for each possible next state ***s‚Ä≤***
    after taking ***a.***
  prefs: []
  type: TYPE_NORMAL
- en: Once we have done this, we sum all the results we just got ***Œ£‚Çõ‚Ä≤, ·µ£ p(s‚Ä≤, r|s,
    a) * [r + Œ≥V(s‚Ä≤)].*** We then repeat this for each action ***a*** (the second
    level in the tree).
  prefs: []
  type: TYPE_NORMAL
- en: Once these steps are complete, we will have a value associated with each possible
    action ***a*** from the current state we are looking at in the inner loop ***s.***
    Wechoose the highest using ***max‚Çê*** and set this equal to our new value for
    that state ***V(s)‚Üêmax‚Çê Œ£‚Çõ‚Ä≤, ·µ£ p(s‚Ä≤, r|s, a) * [r + Œ≥V(s‚Ä≤)].***
  prefs: []
  type: TYPE_NORMAL
- en: Remember, this process covered only one state ***s*** (the first level in the
    tree)
  prefs: []
  type: TYPE_NORMAL
- en: '*If we were programming this, it would be 3* ***for*** *loops for each level
    in the tree:*'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a949262be65f616b0d6fbd68d7ef07d3.png)'
  prefs: []
  type: TYPE_IMG
- en: '**(3) Inner loop (continued):** Before moving to the next pass in the inner
    loop, we perform a comparison between the current value of ***Œî*** and the difference
    between the previous value of this state ***v*** and the new value for this state
    we just calculated ***V(s)***. We update ***Œî*** to the larger of these two: ***Œî
    ‚Üê max(Œî,| v - V(s)|)***. This helps us track how close we are to convergence.'
  prefs: []
  type: TYPE_NORMAL
- en: Ok, this process completes one pass of the inner loop. **We perform step (3)
    for each s in S** before breaking out of the inner loop and performing a check
    on the convergence condition ***Œî < Œ∏.*** If this condition is met, we break out
    the outer loop, if not, we go back to step (2).
  prefs: []
  type: TYPE_NORMAL
- en: '**(4) Policy extraction:** By this time, we have likely performed multiple
    passes through the outer loop until we have converged. This means our value table
    will be updated to represent the final value of each state (in other words, ‚Äòhow
    good it is to be in each state‚Äô). We can now extract a policy from this.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ea3f5ebe2aa38c2fc3d57beabedaeae4.png)'
  prefs: []
  type: TYPE_IMG
- en: Remember, the policy **œÄ** is essentially a mapping from **states ‚Üí actions**,
    and for each state, it selects the action that maximizes the expected return.
    To calculate this, we perform the exact same process as before, but instead of
    getting a value for state ***s*** using ***max‚Çê***, we get the action ***a***
    that gives us the best value using ***argmax‚Çê***.
  prefs: []
  type: TYPE_NORMAL
- en: And that‚Äôs it!
  prefs: []
  type: TYPE_NORMAL
- en: '***Policy Iteration*** *is another dynamic programming algorithm. It is similar
    to VI except it alternates between improving the policy by making it greedy with
    respect to the current value function and evaluating the policy‚Äôs performance
    until convergence, often requiring fewer iterations but more computation per iteration.*'
  prefs: []
  type: TYPE_NORMAL
- en: Solving the example using Value Iteration
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: VI should make even more sense once we complete an example problem, so let‚Äôs
    get back to our golf MDP. We have formalised this as an MDP but currently, the
    agent doesn‚Äôt know the best strategy when playing golf, so let‚Äôs solve the golf
    MDP using VI.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3bc4524300a1072607e002076d7f28bd.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We‚Äôll start by defining our model parameters using fairly standard values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'We will then initialise our value table to 0 for states in ***S***:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now start in the outer loop:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'And three passes of the inner loop for each state in ***S***:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'This gives us the following update to our value table:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '*We don‚Äôt need to worry about* ***s2*** *as this is a terminal state, meaning
    no actions are possible here.*'
  prefs: []
  type: TYPE_NORMAL
- en: 'We now break out the inner loop and continue the outer loop, performing a convergence
    check on:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Since we have not converged, we do a second iteration of the outer loop:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'And another 3 passes of the inner loop, using the updated value table:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'At the end of the second iteration, our values are:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Check convergence once again:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Still no convergence, so we continue the same process as above until Œî < Œ∏.
    I won‚Äôt show all the calculations, the above two are enough to understand the
    process.
  prefs: []
  type: TYPE_NORMAL
- en: 'After 6 iterations, our policy converges. This is our values and convergence
    rate as they change over each iteration:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we can extract our policy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Our final policy is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: So, when our agent is in the ***Ball on fairway*** state (s0), the best action
    is to ***hit to green***. This seems pretty obvious since that is the only available
    action. However, in ***s1***, where there are two possible actions, our policy
    has learned to ***hit in hole***. We can now give this learned policy to other
    agents who want to play golf!
  prefs: []
  type: TYPE_NORMAL
- en: And there you have it! We have just solved a very simple RL problem using Value
    Iteration.
  prefs: []
  type: TYPE_NORMAL
- en: Limitations to dynamic programming
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: It‚Äôs important to note that Value Iteration, along with other Dynamic Programming
    algorithms, has its limitations. Firstly, it assumes that we have complete knowledge
    of the dynamics of the MDP (we call this ***model-based RL***). However, this
    is rarely the case in real-world problems, for example, we may not know the transition
    probabilities. For problems where this is the case, we need to use other approaches
    such as *Q-learning* (***model-free RL***).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f60ad4bf1fac52b752748a38f8f6dc1f.png)'
  prefs: []
  type: TYPE_IMG
- en: Curse of dimensionality in chess
  prefs: []
  type: TYPE_NORMAL
- en: Secondly, for bigger problems, as the number of states and actions increases,
    the size of the value table grows exponentially (think about trying to define
    all the possible states of chess). This results in the ‚Äò*curse of dimensionality*‚Äô
    problem, where the computational and memory requirements escalate rapidly, making
    it challenging to apply DP to high-dimensional problems.
  prefs: []
  type: TYPE_NORMAL
- en: Nevertheless, VI is great to learn as it introduces some of the key foundational
    concepts of RL which form the basis of more complex algorithms that you may go
    on to learn.
  prefs: []
  type: TYPE_NORMAL
- en: Thanks for reading!
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: I hope this article has provided an easy-to-understand introduction to reinforcement
    learning, and Value Iteration specifically.
  prefs: []
  type: TYPE_NORMAL
- en: '*If you learnt something new here, please give this a üëè and follow!*'
  prefs: []
  type: TYPE_NORMAL
- en: Unless otherwise stated, all images are created by the author.
  prefs: []
  type: TYPE_NORMAL
