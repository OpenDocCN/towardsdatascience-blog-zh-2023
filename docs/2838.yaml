- en: 'Reinforcement Learning: an Easy Introduction to Value Iteration'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å¼ºåŒ–å­¦ä¹ ï¼šä»·å€¼è¿­ä»£çš„ç®€å•ä»‹ç»
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/reinforcement-learning-an-easy-introduction-to-value-iteration-e4cfe0731fd5?source=collection_archive---------0-----------------------#2023-09-10](https://towardsdatascience.com/reinforcement-learning-an-easy-introduction-to-value-iteration-e4cfe0731fd5?source=collection_archive---------0-----------------------#2023-09-10)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://towardsdatascience.com/reinforcement-learning-an-easy-introduction-to-value-iteration-e4cfe0731fd5?source=collection_archive---------0-----------------------#2023-09-10](https://towardsdatascience.com/reinforcement-learning-an-easy-introduction-to-value-iteration-e4cfe0731fd5?source=collection_archive---------0-----------------------#2023-09-10)
- en: Learn the fundamentals of RL and how to apply Value Iteration to a simple example
    problem
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å­¦ä¹ å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„åŸºç¡€çŸ¥è¯†ä»¥åŠå¦‚ä½•å°†ä»·å€¼è¿­ä»£åº”ç”¨äºä¸€ä¸ªç®€å•çš„ç¤ºä¾‹é—®é¢˜ã€‚
- en: '[](https://medium.com/@carlbettosi?source=post_page-----e4cfe0731fd5--------------------------------)[![Carl
    Bettosi](../Images/19c640d8e96fa39583a3c998764aa2b8.png)](https://medium.com/@carlbettosi?source=post_page-----e4cfe0731fd5--------------------------------)[](https://towardsdatascience.com/?source=post_page-----e4cfe0731fd5--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----e4cfe0731fd5--------------------------------)
    [Carl Bettosi](https://medium.com/@carlbettosi?source=post_page-----e4cfe0731fd5--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@carlbettosi?source=post_page-----e4cfe0731fd5--------------------------------)[![Carl
    Bettosi](../Images/19c640d8e96fa39583a3c998764aa2b8.png)](https://medium.com/@carlbettosi?source=post_page-----e4cfe0731fd5--------------------------------)[](https://towardsdatascience.com/?source=post_page-----e4cfe0731fd5--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----e4cfe0731fd5--------------------------------)
    [Carl Bettosi](https://medium.com/@carlbettosi?source=post_page-----e4cfe0731fd5--------------------------------)'
- en: Â·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Â·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fabe6f5e189c8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-an-easy-introduction-to-value-iteration-e4cfe0731fd5&user=Carl+Bettosi&userId=abe6f5e189c8&source=post_page-abe6f5e189c8----e4cfe0731fd5---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----e4cfe0731fd5--------------------------------)
    Â·15 min readÂ·Sep 10, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fe4cfe0731fd5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-an-easy-introduction-to-value-iteration-e4cfe0731fd5&user=Carl+Bettosi&userId=abe6f5e189c8&source=-----e4cfe0731fd5---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[å…³æ³¨](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fabe6f5e189c8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-an-easy-introduction-to-value-iteration-e4cfe0731fd5&user=Carl+Bettosi&userId=abe6f5e189c8&source=post_page-abe6f5e189c8----e4cfe0731fd5---------------------post_header-----------)
    å‘è¡¨åœ¨ [Towards Data Science](https://towardsdatascience.com/?source=post_page-----e4cfe0731fd5--------------------------------)
    Â· 15 åˆ†é’Ÿé˜…è¯» Â· 2023å¹´9æœˆ10æ—¥[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fe4cfe0731fd5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-an-easy-introduction-to-value-iteration-e4cfe0731fd5&user=Carl+Bettosi&userId=abe6f5e189c8&source=-----e4cfe0731fd5---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fe4cfe0731fd5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-an-easy-introduction-to-value-iteration-e4cfe0731fd5&source=-----e4cfe0731fd5---------------------bookmark_footer-----------)![](../Images/291df786927604981f62437fda3c5b4f.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fe4cfe0731fd5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-an-easy-introduction-to-value-iteration-e4cfe0731fd5&source=-----e4cfe0731fd5---------------------bookmark_footer-----------)![](../Images/291df786927604981f62437fda3c5b4f.png)'
- en: Value Iteration (VI) is typically one of the first algorithms introduced on
    the Reinforcement Learning (RL) learning pathway. The underlying specifics of
    the algorithm introduce some of the most fundamental aspects of RL and, hence,
    it is important to master VI before progressing to more complex RL algorithms.
    Yet, this can be tricky to get your head around.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: ä»·å€¼è¿­ä»£ï¼ˆVIï¼‰é€šå¸¸æ˜¯å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å­¦ä¹ è·¯å¾„ä¸­é¦–å…ˆå¼•å…¥çš„ç®—æ³•ä¹‹ä¸€ã€‚è¯¥ç®—æ³•çš„åŸºæœ¬ç»†èŠ‚ä»‹ç»äº†RLçš„ä¸€äº›æœ€åŸºæœ¬çš„æ–¹é¢ï¼Œå› æ­¤ï¼Œåœ¨è¿›é˜¶åˆ°æ›´å¤æ‚çš„RLç®—æ³•ä¹‹å‰ï¼ŒæŒæ¡VIæ˜¯å¾ˆé‡è¦çš„ã€‚ç„¶è€Œï¼Œè¿™å¯èƒ½æœ‰äº›éš¾ä»¥ç†è§£ã€‚
- en: This article is designed to be an easy-to-understand introduction to VI, which
    will assume the reader to be new to the field of RL. Letâ€™s get started.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬æ–‡æ—¨åœ¨æˆä¸ºä¸€ä¸ªæ˜“äºç†è§£çš„ä»·å€¼è¿­ä»£ï¼ˆVIï¼‰ä»‹ç»ï¼Œå‡è®¾è¯»è€…å¯¹å¼ºåŒ–å­¦ä¹ é¢†åŸŸæ˜¯æ–°çš„ã€‚è®©æˆ‘ä»¬å¼€å§‹å§ã€‚
- en: Already know the basics of RL? â†’ [**Skip to how to use Value Iteration**](#dab6).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: å·²ç»äº†è§£äº†RLçš„åŸºç¡€çŸ¥è¯†ï¼Ÿâ†’ [**è·³åˆ°å¦‚ä½•ä½¿ç”¨ä»·å€¼è¿­ä»£**](#dab6)ã€‚
- en: The basics of RL
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å¼ºåŒ–å­¦ä¹ çš„åŸºç¡€
- en: Letâ€™s start with a textbook definition, then Iâ€™ll break it down using an easy
    example.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬ä»æ•™ç§‘ä¹¦å®šä¹‰å¼€å§‹ï¼Œç„¶åç”¨ä¸€ä¸ªç®€å•çš„ä¾‹å­æ¥è§£é‡Šã€‚
- en: '![](../Images/bdfd099792f5ee2310c43662522084b7.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bdfd099792f5ee2310c43662522084b7.png)'
- en: Overview of the reinforcement learning training process
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: å¼ºåŒ–å­¦ä¹ è®­ç»ƒè¿‡ç¨‹æ¦‚è¿°
- en: RL is one of the three key machine learning paradigms beside supervised and
    unsupervised learning. RL is not a singular algorithm, but rather a framework
    which encompasses a range of techniques and approaches for teaching agents to
    learn and make decisions in their environments.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ˜¯é™¤äº†ç›‘ç£å­¦ä¹ å’Œæ— ç›‘ç£å­¦ä¹ ä¹‹å¤–çš„ä¸‰å¤§ä¸»è¦æœºå™¨å­¦ä¹ èŒƒå¼ä¹‹ä¸€ã€‚RLä¸æ˜¯å•ä¸€çš„ç®—æ³•ï¼Œè€Œæ˜¯ä¸€ä¸ªæ¡†æ¶ï¼Œæ¶µç›–äº†ä¸€ç³»åˆ—æŠ€æœ¯å’Œæ–¹æ³•ï¼Œç”¨äºæ•™å¯¼ä»£ç†åœ¨å…¶ç¯å¢ƒä¸­å­¦ä¹ å’Œåšå‡ºå†³ç­–ã€‚
- en: In RL, an agent interacts with an environment by taking various actions. The
    agent is rewarded when those actions lead to desired states and punished when
    they donâ€™t. The agentâ€™s objective is to learn a strategy, called a policy, that
    guides its actions to maximize the reward it accumulates over time. This trial-and-error
    process refines the agent's behavioral policy, allowing it to take optimal or
    near-optimal behaviors in its environment.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨RLä¸­ï¼Œä»£ç†é€šè¿‡é‡‡å–å„ç§åŠ¨ä½œä¸ç¯å¢ƒäº’åŠ¨ã€‚å½“è¿™äº›åŠ¨ä½œå¯¼è‡´æœŸæœ›çš„çŠ¶æ€æ—¶ï¼Œä»£ç†ä¼šè·å¾—å¥–åŠ±ï¼›å½“åŠ¨ä½œæœªè¾¾åˆ°æœŸæœ›çš„çŠ¶æ€æ—¶ï¼Œä»£ç†ä¼šå—åˆ°æƒ©ç½šã€‚ä»£ç†çš„ç›®æ ‡æ˜¯å­¦ä¹ ä¸€ä¸ªç­–ç•¥ï¼Œç§°ä¸ºç­–ç•¥ï¼ˆpolicyï¼‰ï¼Œè¯¥ç­–ç•¥æŒ‡å¯¼å…¶è¡ŒåŠ¨ä»¥æœ€å¤§åŒ–å…¶éšç€æ—¶é—´ç§¯ç´¯çš„å¥–åŠ±ã€‚è¿™ä¸ªè¯•é”™è¿‡ç¨‹ç²¾ç‚¼äº†ä»£ç†çš„è¡Œä¸ºç­–ç•¥ï¼Œä½¿å…¶åœ¨ç¯å¢ƒä¸­é‡‡å–æœ€ä¼˜æˆ–æ¥è¿‘æœ€ä¼˜çš„è¡Œä¸ºã€‚
- en: '*The book An â€œIntroduction to Reinforcement Learningâ€ by Richard S. Sutton
    and Andrew G. Barto is considered the best in the field for those wishing to gain
    a solid understanding of RL.. and itâ€™s* [*available for free*](http://incompleteideas.net/book/the-book-2nd.html)*!*'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '*ç†æŸ¥å¾·Â·SÂ·è¨é¡¿å’Œå®‰å¾·é²Â·GÂ·å·´æ‰˜çš„ã€Šå¼ºåŒ–å­¦ä¹ å¯¼è®ºã€‹ä¸€ä¹¦è¢«è®¤ä¸ºæ˜¯è¯¥é¢†åŸŸçš„æœ€ä½³è¯»ç‰©ä¹‹ä¸€ï¼Œå¯¹äºé‚£äº›å¸Œæœ›æ·±å…¥ç†è§£RLçš„äººæ¥è¯´..è€Œä¸”å®ƒæ˜¯* [*å…è´¹æä¾›çš„*](http://incompleteideas.net/book/the-book-2nd.html)*ï¼*'
- en: Letâ€™s define an example problem
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬å®šä¹‰ä¸€ä¸ªç¤ºä¾‹é—®é¢˜
- en: '![](../Images/25fc75950f08dcf8fbb9ceba748c9fa4.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/25fc75950f08dcf8fbb9ceba748c9fa4.png)'
- en: Possible states for the game of golf
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: é«˜å°”å¤«æ¸¸æˆä¸­çš„å¯èƒ½çŠ¶æ€
- en: This image depicts the game of golf in its simplest form. We will use this as
    golf has a clearly defined goal - get the ball in the hole.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å¼ å›¾ç‰‡ä»¥æœ€ç®€å•çš„å½¢å¼æç»˜äº†é«˜å°”å¤«æ¸¸æˆã€‚æˆ‘ä»¬å°†ä½¿ç”¨è¿™ä¸ªä¾‹å­ï¼Œå› ä¸ºé«˜å°”å¤«æœ‰ä¸€ä¸ªæ˜ç¡®çš„ç›®æ ‡â€”â€”æŠŠçƒæ‰“è¿›æ´é‡Œã€‚
- en: 'In our example, the golf ball can be in one of three positions: *on the fairway*;
    *on the green*; or *in the hole*. We start on the fairway and aim to get closer
    to the hole with each shot, with the hole sitting on the green.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æˆ‘ä»¬çš„ç¤ºä¾‹ä¸­ï¼Œé«˜å°”å¤«çƒå¯ä»¥å¤„äºä¸‰ç§ä½ç½®ä¹‹ä¸€ï¼š*åœ¨çƒé“ä¸Š*ï¼›*åœ¨æœå²­ä¸Š*ï¼›æˆ–*åœ¨æ´é‡Œ*ã€‚æˆ‘ä»¬ä»çƒé“å¼€å§‹ï¼Œç›®æ ‡æ˜¯æ¯æ¬¡å‡»çƒéƒ½é è¿‘æ´ï¼Œæ´ä½äºæœå²­ä¸Šã€‚
- en: In RL, each of these positions is referred to as a ***state*** of the ***environment.***
    You can think of the state as being a snapshot of the current environment (the
    golf course), which also records the ball position.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä¸­ï¼Œè¿™äº›ä½ç½®ä¸­çš„æ¯ä¸€ä¸ªè¢«ç§°ä¸º***çŠ¶æ€***æˆ–***ç¯å¢ƒçŠ¶æ€***ã€‚ä½ å¯ä»¥æŠŠçŠ¶æ€çœ‹ä½œæ˜¯å½“å‰ç¯å¢ƒï¼ˆé«˜å°”å¤«çƒåœºï¼‰çš„å¿«ç…§ï¼ŒåŒæ—¶è®°å½•çƒçš„ä½ç½®ã€‚
- en: '![](../Images/2938c430e74f76f0d14cc0001e1ae266.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2938c430e74f76f0d14cc0001e1ae266.png)'
- en: Possible actions in the game of golf
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: é«˜å°”å¤«çƒæ¸¸æˆä¸­çš„å¯èƒ½åŠ¨ä½œ
- en: 'In our game, the ***agent*** takes shots of hitting the ball, starting at the
    *ball on fairway* state. The ***agent*** simply refers to the entity that is in
    control of taking ***actions***. Our game has three available actions: *hit to
    fairway*; *hit to green*; and *hit in hole*.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æˆ‘ä»¬çš„æ¸¸æˆä¸­ï¼Œ***ä»£ç†***è¿›è¡Œå‡»çƒåŠ¨ä½œï¼Œå¼€å§‹æ—¶å¤„äº*çƒåœ¨çƒé“ä¸Š*çŠ¶æ€ã€‚***ä»£ç†***ä»…æŒ‡æ§åˆ¶è¿›è¡Œ***åŠ¨ä½œ***çš„å®ä½“ã€‚æˆ‘ä»¬çš„æ¸¸æˆæœ‰ä¸‰ç§å¯ç”¨çš„åŠ¨ä½œï¼š*å‡»çƒåˆ°çƒé“*ï¼›*å‡»çƒåˆ°æœå²­*ï¼›ä»¥åŠ*å‡»çƒè¿›æ´*ã€‚
- en: '![](../Images/110c6f6d02f7fb59ee2b805e836a368b.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/110c6f6d02f7fb59ee2b805e836a368b.png)'
- en: Transition probabilities in the game of golf
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: é«˜å°”å¤«æ¸¸æˆä¸­çš„è½¬ç§»æ¦‚ç‡
- en: Of course, when you take a shot, the ball may not go where you want it to. Therefore,
    we introduce a ***transition function*** linking actions to states with some probability
    weighting.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: å½“ç„¶ï¼Œå½“ä½ å‡»çƒæ—¶ï¼Œçƒå¯èƒ½ä¸ä¼šè½åœ¨ä½ æƒ³è¦çš„ä½ç½®ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ª***è½¬ç§»å‡½æ•°***ï¼Œé€šè¿‡ä¸€äº›æ¦‚ç‡æƒé‡å°†åŠ¨ä½œä¸çŠ¶æ€å…³è”èµ·æ¥ã€‚
- en: For example, we may miss the green when we take a shot from the fairway and
    end up still on the fairway. Written in an RL context, if we are in the *ball
    on fairway* state and take the action *hit to green*, there is a 90% probability
    we will enter the *ball on green* state but a 10% probability we will re-enter
    the *ball on fairway* state.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: ä¾‹å¦‚ï¼Œå½“æˆ‘ä»¬ä»çƒé“ä¸Šå‡»çƒæ—¶ï¼Œå¯èƒ½ä¼šåç¦»æœå²­ï¼Œä»ç„¶åœç•™åœ¨çƒé“ä¸Šã€‚ç”¨RLçš„æœ¯è¯­æ¥è¯´ï¼Œå¦‚æœæˆ‘ä»¬å¤„äº*çƒåœ¨çƒé“ä¸Š*çŠ¶æ€ï¼Œå¹¶é‡‡å–*å‡»çƒåˆ°æœå²­*çš„åŠ¨ä½œï¼Œåˆ™æœ‰90%çš„æ¦‚ç‡è¿›å…¥*çƒåœ¨æœå²­ä¸Š*çŠ¶æ€ï¼Œä½†ä¹Ÿæœ‰10%çš„æ¦‚ç‡é‡æ–°è¿›å…¥*çƒåœ¨çƒé“ä¸Š*çŠ¶æ€ã€‚
- en: '![](../Images/d9a6e396a04c36b4d5f972989603693b.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d9a6e396a04c36b4d5f972989603693b.png)'
- en: A reward of 10 for getting the ball in the hole
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: å°†çƒå‡»å…¥æ´ä¸­çš„å¥–åŠ±ä¸º 10
- en: Every time the agent takes an action, we refer to this as a ***step*** through
    the environment. Based upon the action just taken, the agent observes the new
    state it ends up in as well as a ***reward***. A reward function is an incentive
    mechanism for pushing the agent in the right direction. In other words, we design
    the reward function to shape the desired behavior of our agent. In our simplified
    golf example, we provide a reward of 10 for getting the ball in the hole.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: æ¯å½“ä»£ç†æ‰§è¡Œä¸€ä¸ªåŠ¨ä½œæ—¶ï¼Œæˆ‘ä»¬ç§°ä¹‹ä¸ºåœ¨ç¯å¢ƒä¸­çš„ ***æ­¥***ã€‚æ ¹æ®åˆšåˆšé‡‡å–çš„åŠ¨ä½œï¼Œä»£ç†è§‚å¯Ÿåˆ°å®ƒæ‰€å¤„çš„æ–°çŠ¶æ€ä»¥åŠä¸€ä¸ª ***å¥–åŠ±***ã€‚å¥–åŠ±å‡½æ•°æ˜¯ä¸€ä¸ªæ¿€åŠ±æœºåˆ¶ï¼Œç”¨äºå¼•å¯¼ä»£ç†æœæ­£ç¡®æ–¹å‘å‰è¿›ã€‚æ¢å¥è¯è¯´ï¼Œæˆ‘ä»¬è®¾è®¡å¥–åŠ±å‡½æ•°ä»¥å¡‘é€ ä»£ç†çš„æœŸæœ›è¡Œä¸ºã€‚åœ¨æˆ‘ä»¬ç®€åŒ–çš„é«˜å°”å¤«ç¤ºä¾‹ä¸­ï¼Œæˆ‘ä»¬ä¸ºå°†çƒå‡»å…¥æ´ä¸­çš„è¡Œä¸ºæä¾›
    10 çš„å¥–åŠ±ã€‚
- en: '*The design of environment dynamics (the transition and reward functions) is
    not a trivial task. If the environment does not represent the problem you are
    attempting to solve, the agent will learn a policy reflecting a correct solution
    for an incorrect problem. We will not cover such design elements here, but itâ€™s
    worth noting.*'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '*ç¯å¢ƒåŠ¨æ€ï¼ˆè½¬æ¢å’Œå¥–åŠ±å‡½æ•°ï¼‰çš„è®¾è®¡ä¸æ˜¯ä¸€é¡¹ç®€å•çš„ä»»åŠ¡ã€‚å¦‚æœç¯å¢ƒæœªèƒ½ä»£è¡¨ä½ è¯•å›¾è§£å†³çš„é—®é¢˜ï¼Œä»£ç†å°†å­¦ä¹ åˆ°ä¸€ä¸ªåæ˜ ä¸æ­£ç¡®é—®é¢˜çš„æ­£ç¡®è§£å†³æ–¹æ¡ˆçš„ç­–ç•¥ã€‚æˆ‘ä»¬åœ¨è¿™é‡Œä¸è®¨è®ºè¿™äº›è®¾è®¡å…ƒç´ ï¼Œä½†å€¼å¾—æ³¨æ„ã€‚*'
- en: Markov Decision Processes
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹
- en: To represent a problem in a way the agent understands, we must formalise it
    as a Markov Decision Process (MDP).
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†ä»¥ä»£ç†èƒ½å¤Ÿç†è§£çš„æ–¹å¼è¡¨ç¤ºé—®é¢˜ï¼Œæˆ‘ä»¬å¿…é¡»å°†å…¶å½¢å¼åŒ–ä¸ºé©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼ˆMDPï¼‰ã€‚
- en: A MDP is a mathematical model which describes our problem in a structured way.
    It represents the agent's interactions with the environment as a sequential decision-making
    process (i.e., one action after another).
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: MDP æ˜¯ä¸€ç§æ•°å­¦æ¨¡å‹ï¼Œå®ƒä»¥ç»“æ„åŒ–çš„æ–¹å¼æè¿°äº†æˆ‘ä»¬çš„é—®é¢˜ã€‚å®ƒå°†ä»£ç†ä¸ç¯å¢ƒçš„äº’åŠ¨è¡¨ç¤ºä¸ºä¸€ä¸ªé¡ºåºå†³ç­–è¿‡ç¨‹ï¼ˆå³ï¼Œä¸€ä¸ªæ¥ä¸€ä¸ªçš„åŠ¨ä½œï¼‰ã€‚
- en: 'It consists of the environment dynamics (Iâ€™m going to add some math notation
    here to shorten things):'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: å®ƒç”±ç¯å¢ƒåŠ¨æ€ç»„æˆï¼ˆæˆ‘å°†æ·»åŠ ä¸€äº›æ•°å­¦ç¬¦å·ä»¥ç®€åŒ–è¯´æ˜ï¼‰ï¼š
- en: a finite set of states **s âˆˆ S**.
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªæœ‰é™çš„çŠ¶æ€é›†åˆ **s âˆˆ S**ã€‚
- en: a finite set of actions ***a* âˆˆ *A***.
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªæœ‰é™çš„åŠ¨ä½œé›†åˆ ***a* âˆˆ *A***ã€‚
- en: a transition function ***T*(*s*â€²âˆ£*s*,*a*)** returning the probability of reaching
    state ***s*â€²**, given the current state ***s***, the current action ***a***.
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªè½¬æ¢å‡½æ•° ***T*(*s*â€²âˆ£*s*,*a*)** è¿”å›åœ¨å½“å‰çŠ¶æ€ ***s*** å’Œå½“å‰åŠ¨ä½œ ***a*** ä¸‹è¾¾åˆ°çŠ¶æ€ ***s*â€²** çš„æ¦‚ç‡ã€‚
- en: a reward function ***R*(*s*,*a*,*s*â€²)** returning a scalar reward based on reaching
    next state ***s*â€²**, after being in state ***s***, and taking action ***a***.
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªå¥–åŠ±å‡½æ•° ***R*(*s*,*a*,*s*â€²)** åŸºäºä»çŠ¶æ€ ***s*** åˆ°è¾¾ä¸‹ä¸€ä¸ªçŠ¶æ€ ***s*â€²** çš„æƒ…å†µè¿”å›ä¸€ä¸ªæ ‡é‡å¥–åŠ±ï¼Œå¹¶è€ƒè™‘é‡‡å–åŠ¨ä½œ
    ***a***ã€‚
- en: '*Note that if there is some uncertainty or randomness involved in the transitions
    between states (i.e. taking the same action in the same state twice may lead to
    different outcomes), we refer to this as a* ***stochastic*** *MDP. We could also
    create a* ***deterministic*** *MDP, where transitions and rewards are entirely
    predictable. This means when an agent takes an action in a specific state, there
    is a one-to-one correspondence between the action and the resulting next state
    and the reward.*'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '*æ³¨æ„ï¼Œå¦‚æœçŠ¶æ€ä¹‹é—´çš„è½¬æ¢æ¶‰åŠæŸäº›ä¸ç¡®å®šæ€§æˆ–éšæœºæ€§ï¼ˆå³åœ¨ç›¸åŒçŠ¶æ€ä¸‹ä¸¤æ¬¡é‡‡å–ç›¸åŒåŠ¨ä½œå¯èƒ½å¯¼è‡´ä¸åŒç»“æœï¼‰ï¼Œæˆ‘ä»¬ç§°ä¹‹ä¸º* ***éšæœº*** *MDPã€‚æˆ‘ä»¬ä¹Ÿå¯ä»¥åˆ›å»ºä¸€ä¸ª*
    ***ç¡®å®šæ€§*** *MDPï¼Œå…¶ä¸­è½¬æ¢å’Œå¥–åŠ±æ˜¯å®Œå…¨å¯é¢„æµ‹çš„ã€‚è¿™æ„å‘³ç€å½“ä»£ç†åœ¨ç‰¹å®šçŠ¶æ€ä¸‹é‡‡å–ä¸€ä¸ªåŠ¨ä½œæ—¶ï¼ŒåŠ¨ä½œä¸ç»“æœçŠ¶æ€åŠå¥–åŠ±ä¹‹é—´æ˜¯ä¸€ä¸€å¯¹åº”çš„ã€‚*'
- en: Visualised as an MDP, our golf problem looks pretty much the same as the images
    depicted earlier. We will use S = {s1, s2, s3} for shorthand.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥ MDP å½¢å¼å¯è§†åŒ–ï¼Œæˆ‘ä»¬çš„é«˜å°”å¤«é—®é¢˜çœ‹èµ·æ¥ä¸ä¹‹å‰æè¿°çš„å›¾åƒå‡ ä¹ç›¸åŒã€‚æˆ‘ä»¬å°†ä½¿ç”¨ S = {s1, s2, s3} ä½œä¸ºç®€å†™ã€‚
- en: '![](../Images/d2778219029b8237cd1d52e14e59fb54.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d2778219029b8237cd1d52e14e59fb54.png)'
- en: Our golf example problem written as an MDP
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬çš„é«˜å°”å¤«ç¤ºä¾‹é—®é¢˜ä»¥ MDP å½¢å¼å‘ˆç°
- en: '*The use of an MDP assumes that whatâ€™s going to happen next in the environment
    only depends on whatâ€™s happening right now â€” the current state and action â€” and
    not on what happened before. This is called the* **Markov property***, and itâ€™s
    important in RL as it reduces computational complexity. Iâ€™ll explain this more
    later.*'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '*MDP çš„ä½¿ç”¨å‡è®¾ç¯å¢ƒä¸­æ¥ä¸‹æ¥ä¼šå‘ç”Ÿçš„äº‹æƒ…åªä¾èµ–äºç°åœ¨çš„çŠ¶æ€å’ŒåŠ¨ä½œï¼Œè€Œä¸ä¾èµ–äºä¹‹å‰å‘ç”Ÿçš„äº‹æƒ…ã€‚è¿™è¢«ç§°ä¸º* **é©¬å°”å¯å¤«æ€§è´¨** *ï¼Œå®ƒåœ¨å¼ºåŒ–å­¦ä¹ ä¸­å¾ˆé‡è¦ï¼Œå› ä¸ºå®ƒé™ä½äº†è®¡ç®—å¤æ‚æ€§ã€‚æˆ‘ç¨åä¼šè¯¦ç»†è§£é‡Šè¿™ä¸€ç‚¹ã€‚*'
- en: What is Value Iteration?
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä»€ä¹ˆæ˜¯ä»·å€¼è¿­ä»£ï¼Ÿ
- en: Value Iteration (VI) is an algorithm used to solve RL problems like the golf
    example mentioned above, where we have full knowledge of all components of the
    MDP. It works by iteratively improving its estimate of the â€˜valueâ€™ of being in
    each state. It does this by considering the immediate rewards and expected future
    rewards when taking different available actions. These values are tracked using
    a value table, which updates at each step. Eventually, this sequence of improvements
    converges, yielding an optimal policy of state â†’ action mappings that the agent
    can follow to make the best decisions in the given environment.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: å€¼è¿­ä»£ï¼ˆVIï¼‰æ˜¯ä¸€ç§ç”¨äºè§£å†³ç±»ä¼¼äºä¸Šè¿°é«˜å°”å¤«çƒä¾‹å­ä¸­çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰é—®é¢˜çš„ç®—æ³•ï¼Œå…¶ä¸­æˆ‘ä»¬å¯¹MDPçš„æ‰€æœ‰ç»„ä»¶æœ‰å®Œå…¨çš„äº†è§£ã€‚å®ƒé€šè¿‡è¿­ä»£æ”¹è¿›å¯¹æ¯ä¸ªçŠ¶æ€â€œä»·å€¼â€çš„ä¼°è®¡æ¥å®ç°ã€‚å®ƒé€šè¿‡è€ƒè™‘ä¸åŒå¯ç”¨åŠ¨ä½œæ—¶çš„å³æ—¶å¥–åŠ±å’ŒæœŸæœ›çš„æœªæ¥å¥–åŠ±æ¥å®Œæˆè¿™é¡¹å·¥ä½œã€‚è¿™äº›å€¼é€šè¿‡ä¸€ä¸ªå€¼è¡¨è¿›è¡Œè·Ÿè¸ªï¼Œè¯¥è¡¨åœ¨æ¯ä¸€æ­¥éƒ½ä¼šæ›´æ–°ã€‚æœ€ç»ˆï¼Œè¿™ä¸€ç³»åˆ—çš„æ”¹è¿›å°†ä¼šæ”¶æ•›ï¼Œäº§ç”Ÿä¸€ä¸ªçŠ¶æ€
    â†’ åŠ¨ä½œæ˜ å°„çš„æœ€ä¼˜ç­–ç•¥ï¼Œä½¿ä»£ç†å¯ä»¥åœ¨ç»™å®šç¯å¢ƒä¸­åšå‡ºæœ€ä½³å†³ç­–ã€‚
- en: VI leverages the concept of *dynamic programming*, where solving a big problem
    is broken down into smaller subproblems. To achieve this in VI, the *Bellman equation*
    is used to guide the process of iteratively updating value estimates for each
    state, providing a recursive relationship that expresses the value of a state
    in terms of the values of its neighbouring states.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: VI åˆ©ç”¨ *åŠ¨æ€è§„åˆ’* çš„æ¦‚å¿µï¼Œå…¶ä¸­å°†ä¸€ä¸ªå¤§é—®é¢˜åˆ†è§£æˆè¾ƒå°çš„å­é—®é¢˜æ¥è§£å†³ã€‚ä¸ºäº†åœ¨VIä¸­å®ç°è¿™ä¸€ç‚¹ï¼Œä½¿ç”¨ *è´å°”æ›¼æ–¹ç¨‹* æ¥æŒ‡å¯¼è¿­ä»£æ›´æ–°æ¯ä¸ªçŠ¶æ€çš„ä»·å€¼ä¼°è®¡çš„è¿‡ç¨‹ï¼Œæä¾›ä¸€ä¸ªé€’å½’å…³ç³»ï¼Œç”¨äºè¡¨ç¤ºçŠ¶æ€çš„ä»·å€¼ä¸å…¶é‚»è¿‘çŠ¶æ€çš„ä»·å€¼ä¹‹é—´çš„å…³ç³»ã€‚
- en: This wonâ€™t make much sense now. The easiest way to learn VI is to break it down
    step-by-step, so letâ€™s do that.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨è¿™å¯èƒ½ä¸å¤ªå®¹æ˜“ç†è§£ã€‚å­¦ä¹ VIçš„æœ€ç®€å•æ–¹æ³•æ˜¯é€æ­¥åˆ†è§£ï¼Œæˆ‘ä»¬å°±è¿™æ ·åšå§ã€‚
- en: How does the Value Iteration algorithm work?
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å€¼è¿­ä»£ç®—æ³•æ˜¯å¦‚ä½•å·¥ä½œçš„ï¼Ÿ
- en: The image below depicts the steps of the algorithm. Donâ€™t be put off, it's easier
    than it appears.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸‹å›¾å±•ç¤ºäº†ç®—æ³•çš„æ­¥éª¤ã€‚ä¸è¦è¢«å“åˆ°ï¼Œå®ƒæ¯”çœ‹èµ·æ¥æ›´ç®€å•ã€‚
- en: '![](../Images/b6f032fc68d508cb4d115e760c583e68.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b6f032fc68d508cb4d115e760c583e68.png)'
- en: The Value Iteration algorithm
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: å€¼è¿­ä»£ç®—æ³•
- en: Firstly, we need to define some parameters for our training.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: é¦–å…ˆï¼Œæˆ‘ä»¬éœ€è¦ä¸ºæˆ‘ä»¬çš„è®­ç»ƒå®šä¹‰ä¸€äº›å‚æ•°ã€‚
- en: Theta **Î¸** represents a threshold for convergence. Once we reach **Î¸**, we
    can terminate the training loop and generate the policy. Itâ€™s essentially just
    a way to ensure the policy we create is accurate enough. If we stop training too
    early, we may not learn the best actions to take.
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Theta **Î¸** ä»£è¡¨æ”¶æ•›çš„é˜ˆå€¼ã€‚ä¸€æ—¦æˆ‘ä»¬è¾¾åˆ° **Î¸**ï¼Œå°±å¯ä»¥ç»ˆæ­¢è®­ç»ƒå¾ªç¯å¹¶ç”Ÿæˆç­–ç•¥ã€‚å®ƒæœ¬è´¨ä¸Šåªæ˜¯ä¸€ç§ç¡®ä¿æˆ‘ä»¬åˆ›å»ºçš„ç­–ç•¥è¶³å¤Ÿå‡†ç¡®çš„æ–¹æ³•ã€‚å¦‚æœæˆ‘ä»¬è¿‡æ—©åœæ­¢è®­ç»ƒï¼Œå¯èƒ½æ— æ³•å­¦ä¹ åˆ°æœ€ä½³çš„è¡ŒåŠ¨ã€‚
- en: Gamma **Î³** represents the *discount factor*. This is a value which determines
    how much our agent values future rewards compared to immediate rewards. A higher
    discount factor (closer to 1) indicates that the agent values long-term rewards
    more, while a lower discount factor (closer to 0) places greater emphasis on immediate
    rewards.
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gamma **Î³** ä»£è¡¨ *æŠ˜æ‰£å› å­*ã€‚è¿™æ˜¯ä¸€ä¸ªå€¼ï¼Œå†³å®šäº†æˆ‘ä»¬çš„ä»£ç†å¯¹æœªæ¥å¥–åŠ±çš„é‡è§†ç¨‹åº¦ä¸å¯¹å³æ—¶å¥–åŠ±çš„é‡è§†ç¨‹åº¦ç›¸æ¯”ã€‚è¾ƒé«˜çš„æŠ˜æ‰£å› å­ï¼ˆæ¥è¿‘1ï¼‰è¡¨ç¤ºä»£ç†æ›´é‡è§†é•¿æœŸå¥–åŠ±ï¼Œè€Œè¾ƒä½çš„æŠ˜æ‰£å› å­ï¼ˆæ¥è¿‘0ï¼‰åˆ™æ›´å¼ºè°ƒå³æ—¶å¥–åŠ±ã€‚
- en: '*To understand the discount factor better, consider an RL agent playing chess.
    Letâ€™s say you have the opportunity to capture your opponentâ€™s queen in the next
    move, which would yield a significant immediate reward. However, you also notice
    that by sacrificing a less valuable piece now, you could set up a future advantage
    that might lead to checkmate and an even bigger reward later. The discount factor
    helps you balance this decision.*'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '*ä¸ºäº†æ›´å¥½åœ°ç†è§£æŠ˜æ‰£å› å­ï¼Œè€ƒè™‘ä¸€ä¸ªç©å›½é™…è±¡æ£‹çš„RLä»£ç†ã€‚å‡è®¾ä½ æœ‰æœºä¼šåœ¨ä¸‹ä¸€æ­¥æ•è·å¯¹æ‰‹çš„çš‡åï¼Œè¿™å°†å¸¦æ¥æ˜¾è‘—çš„å³æ—¶å¥–åŠ±ã€‚ç„¶è€Œï¼Œä½ è¿˜æ³¨æ„åˆ°ï¼Œé€šè¿‡ç°åœ¨ç‰ºç‰²ä¸€ä¸ªä¸å¤ªé‡è¦çš„æ£‹å­ï¼Œä½ å¯ä»¥ä¸ºæœªæ¥çš„ä¼˜åŠ¿é“ºå¹³é“è·¯ï¼Œå¯èƒ½ä¼šå¯¼è‡´å°†æ­»å¹¶è·å¾—æ›´å¤§çš„å¥–åŠ±ã€‚æŠ˜æ‰£å› å­å¸®åŠ©ä½ å¹³è¡¡è¿™ä¸ªå†³å®šã€‚*'
- en: '**(1) Initialisation:** Now we have the parameters defined, we want to initialise
    our *value function* ***V(s)*** for all states in ***S***. This typically means
    we set all values to 0 (or some other arbitrary constant) for every state. Think
    of the value function as a table that tracks a value for each state, updating
    frequently.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '**(1) åˆå§‹åŒ–ï¼š** ç°åœ¨æˆ‘ä»¬å·²ç»å®šä¹‰äº†å‚æ•°ï¼Œæˆ‘ä»¬æƒ³è¦åˆå§‹åŒ–æˆ‘ä»¬æ‰€æœ‰çŠ¶æ€çš„ *ä»·å€¼å‡½æ•°* ***V(s)***ã€‚è¿™é€šå¸¸æ„å‘³ç€æˆ‘ä»¬å°†æ¯ä¸ªçŠ¶æ€çš„æ‰€æœ‰å€¼è®¾ç½®ä¸º0ï¼ˆæˆ–å…¶ä»–ä»»æ„å¸¸æ•°ï¼‰ã€‚å¯ä»¥æŠŠä»·å€¼å‡½æ•°æƒ³è±¡æˆä¸€ä¸ªè¡¨æ ¼ï¼Œè·Ÿè¸ªæ¯ä¸ªçŠ¶æ€çš„å€¼ï¼Œå¹¶é¢‘ç¹æ›´æ–°ã€‚'
- en: '![](../Images/fb25e69630ea42956f30577387e322b3.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fb25e69630ea42956f30577387e322b3.png)'
- en: An initialised value table
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: åˆå§‹åŒ–çš„å€¼è¡¨
- en: '**(2) Outer loop:** Now everything is set up, we can start the iterative process
    of updating our values. We begin in the outer loop, which repeats until the convergence
    criteria are met (until Î” < Î¸).'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '**(2) å¤–å±‚å¾ªç¯ï¼š** ç°åœ¨ä¸€åˆ‡éƒ½å‡†å¤‡å¥½äº†ï¼Œæˆ‘ä»¬å¯ä»¥å¼€å§‹æ›´æ–°å€¼çš„è¿­ä»£è¿‡ç¨‹ã€‚æˆ‘ä»¬ä»å¤–å±‚å¾ªç¯å¼€å§‹ï¼Œå®ƒä¼šé‡å¤ç›´åˆ°æ”¶æ•›æ ‡å‡†æ»¡è¶³ï¼ˆç›´åˆ° Î” < Î¸ï¼‰ã€‚'
- en: At each pass of the outer loop, we begin by setting Î” = 0\. Delta **Î”** is used
    to represent the change in value estimates across all states, and the algorithm
    continues iterating until this change Î”falls below the specified threshold Î¸.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ¯æ¬¡å¤–å±‚å¾ªç¯ä¸­ï¼Œæˆ‘ä»¬é¦–å…ˆè®¾ç½®Î” = 0ã€‚**Î”** ç”¨æ¥è¡¨ç¤ºæ‰€æœ‰çŠ¶æ€çš„ä»·å€¼ä¼°è®¡å˜åŒ–ï¼Œç®—æ³•ç»§ç»­è¿­ä»£ï¼Œç›´åˆ°è¿™ä¸€å˜åŒ–Î”ä½äºæŒ‡å®šçš„é˜ˆå€¼Î¸ã€‚
- en: '**(3) Inner loop:** For every state *s* in *S,* we:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '**(3) å†…å±‚å¾ªç¯ï¼š** å¯¹äºæ¯ä¸ªçŠ¶æ€ *s* åœ¨ *S* ä¸­ï¼Œæˆ‘ä»¬ï¼š'
- en: set a variable ***v*** to the current value of that state ***V(s)***, remember
    - this is fetched from our value table (so on the first pass, ***v*** = ***V(s)***
    = 0)
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å°†å˜é‡ ***v*** è®¾ç½®ä¸ºå½“å‰çŠ¶æ€çš„ä»·å€¼ ***V(s)***ï¼Œè®°ä½ - è¿™æ˜¯ä»æˆ‘ä»¬çš„ä»·å€¼è¡¨ä¸­è·å–çš„ï¼ˆæ‰€ä»¥åœ¨ç¬¬ä¸€æ¬¡éå†æ—¶ï¼Œ***v*** = ***V(s)***
    = 0ï¼‰
- en: perform the bellman equation to update ***V(s)***
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ‰§è¡Œè´å°”æ›¼æ–¹ç¨‹æ¥æ›´æ–° ***V(s)***
- en: update **Î”** (weâ€™ll come back to this)
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ›´æ–° **Î”**ï¼ˆæˆ‘ä»¬ç¨åä¼šå›æ¥è®¨è®ºï¼‰
- en: '**The Bellman Equation**'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '**è´å°”æ›¼æ–¹ç¨‹**'
- en: '![](../Images/ac783b3215bf86884f63526f4edb24b5.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ac783b3215bf86884f63526f4edb24b5.png)'
- en: This line of the algorithm is the most important. It requires that we update
    the value of the current state we are looking at in the loop. This value is calculated
    by considering all available actions from that specific state (a 1-step look ahead).
    When we take each of those possible actions it will present us with a set of possible
    next states ***s*â€²** and respective rewards ***r.***
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: ç®—æ³•çš„è¿™ä¸€è¡Œæ˜¯æœ€é‡è¦çš„ã€‚å®ƒè¦æ±‚æˆ‘ä»¬æ›´æ–°å½“å‰å¾ªç¯ä¸­æ‰€çœ‹çš„çŠ¶æ€çš„ä»·å€¼ã€‚è¿™ä¸ªä»·å€¼æ˜¯é€šè¿‡è€ƒè™‘ä»é‚£ä¸ªç‰¹å®šçŠ¶æ€æ‰€æœ‰å¯ç”¨çš„åŠ¨ä½œï¼ˆå‰ç»1æ­¥ï¼‰æ¥è®¡ç®—çš„ã€‚å½“æˆ‘ä»¬é‡‡å–è¿™äº›å¯èƒ½çš„åŠ¨ä½œæ—¶ï¼Œå®ƒä¼šç»™æˆ‘ä»¬ä¸€ç»„å¯èƒ½çš„ä¸‹ä¸€ä¸ªçŠ¶æ€
    ***s*â€²** å’Œç›¸åº”çš„å¥–åŠ± ***r.***
- en: '![](../Images/8076df201e89c49f337967581486f369.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8076df201e89c49f337967581486f369.png)'
- en: 'So, for each of those next states ***s*â€²** and respective rewards ***r***,
    we perform ***p(sâ€², r|s, a)[r + Î³V(sâ€²)].*** Let''s break this up:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥ï¼Œå¯¹äºæ¯ä¸€ä¸ªä¸‹ä¸€ä¸ªçŠ¶æ€ ***s*â€²** å’Œç›¸åº”çš„å¥–åŠ± ***r***ï¼Œæˆ‘ä»¬æ‰§è¡Œ ***p(sâ€², r|s, a)[r + Î³V(sâ€²)].***
    è®©æˆ‘ä»¬åˆ†è§£ä¸€ä¸‹ï¼š
- en: '***p(sâ€², r|s, a)*** the probability of being in state ***s***, taking action
    ***a***, and ending up in next state ***s*â€²**(this is just our transition function)'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***p(sâ€², r|s, a)*** åœ¨çŠ¶æ€ ***s*** ä¸­ï¼Œé‡‡å–åŠ¨ä½œ ***a*** å¹¶æœ€ç»ˆåˆ°è¾¾ä¸‹ä¸€ä¸ªçŠ¶æ€ ***s*â€²** çš„æ¦‚ç‡ï¼ˆè¿™åªæ˜¯æˆ‘ä»¬çš„è½¬ç§»å‡½æ•°ï¼‰'
- en: '***[r + Î³V(sâ€²)]*** the reward ***r*** of ending up in next state ***s*â€²** (we
    get that from our reward function) *+* our discount **Î³** * by the value of that
    next state ***s*â€²** (we get that from our value table)'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***[r + Î³V(sâ€²)]*** ä¸‹ä¸€ä¸ªçŠ¶æ€ ***s*â€²** çš„å¥–åŠ± ***r***ï¼ˆæˆ‘ä»¬ä»å¥–åŠ±å‡½æ•°ä¸­å¾—åˆ°è¿™ä¸ªå€¼ï¼‰ *+* æˆ‘ä»¬çš„æŠ˜æ‰£ **Î³**
    * ä¹˜ä»¥ä¸‹ä¸€ä¸ªçŠ¶æ€çš„ä»·å€¼ ***s*â€²**ï¼ˆæˆ‘ä»¬ä»ä»·å€¼è¡¨ä¸­å¾—åˆ°è¿™ä¸ªå€¼ï¼‰'
- en: We then multiply these two parts ***p(sâ€², r|s, a) * [r + Î³V(sâ€²)]***
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç„¶åæˆ‘ä»¬å°†è¿™ä¸¤ä¸ªéƒ¨åˆ† ***p(sâ€², r|s, a) * [r + Î³V(sâ€²)]*** ç›¸ä¹˜
- en: Remember, this calculation is just for **one** next state ***sâ€²*** (the third
    level of the tree), we need to repeat this for each possible next state ***sâ€²***
    after taking ***a.***
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: è®°ä½ï¼Œè¿™ä¸ªè®¡ç®—åªæ˜¯é’ˆå¯¹**ä¸€ä¸ª**ä¸‹ä¸€ä¸ªçŠ¶æ€ ***sâ€²***ï¼ˆæ ‘çš„ç¬¬ä¸‰å±‚ï¼‰ï¼Œæˆ‘ä»¬éœ€è¦å¯¹æ¯ä¸€ä¸ªå¯èƒ½çš„ä¸‹ä¸€ä¸ªçŠ¶æ€ ***sâ€²*** åœ¨é‡‡å– ***a.***
    åé‡å¤è¿™ä¸ªè¿‡ç¨‹ã€‚
- en: Once we have done this, we sum all the results we just got ***Î£â‚›â€², áµ£ p(sâ€², r|s,
    a) * [r + Î³V(sâ€²)].*** We then repeat this for each action ***a*** (the second
    level in the tree).
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€æ—¦å®Œæˆè¿™ä¸€æ­¥ï¼Œæˆ‘ä»¬å°†æ‰€æœ‰åˆšè·å¾—çš„ç»“æœ ***Î£â‚›â€², áµ£ p(sâ€², r|s, a) * [r + Î³V(sâ€²)].*** æ±‡æ€»èµ·æ¥ã€‚ç„¶åæˆ‘ä»¬å¯¹æ¯ä¸ªåŠ¨ä½œ
    ***a*** è¿›è¡Œé‡å¤ï¼ˆæ ‘ä¸­çš„ç¬¬äºŒå±‚ï¼‰ã€‚
- en: Once these steps are complete, we will have a value associated with each possible
    action ***a*** from the current state we are looking at in the inner loop ***s.***
    Wechoose the highest using ***maxâ‚*** and set this equal to our new value for
    that state ***V(s)â†maxâ‚ Î£â‚›â€², áµ£ p(sâ€², r|s, a) * [r + Î³V(sâ€²)].***
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€æ—¦å®Œæˆè¿™äº›æ­¥éª¤ï¼Œæˆ‘ä»¬å°†ä¸ºå½“å‰çŠ¶æ€ ***s*** åœ¨å†…å±‚å¾ªç¯ä¸­çœ‹åˆ°çš„æ¯ä¸ªå¯èƒ½çš„åŠ¨ä½œ ***a*** å…³è”ä¸€ä¸ªå€¼ã€‚æˆ‘ä»¬ä½¿ç”¨ ***maxâ‚*** é€‰æ‹©æœ€é«˜çš„ï¼Œå¹¶å°†å…¶è®¾ç½®ä¸ºè¯¥çŠ¶æ€çš„æ–°å€¼
    ***V(s)â†maxâ‚ Î£â‚›â€², áµ£ p(sâ€², r|s, a) * [r + Î³V(sâ€²)].***
- en: Remember, this process covered only one state ***s*** (the first level in the
    tree)
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: è®°ä½ï¼Œè¿™ä¸ªè¿‡ç¨‹åªæ¶µç›–äº†ä¸€ä¸ªçŠ¶æ€ ***s***ï¼ˆæ ‘çš„ç¬¬ä¸€å±‚ï¼‰
- en: '*If we were programming this, it would be 3* ***for*** *loops for each level
    in the tree:*'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '*å¦‚æœæˆ‘ä»¬åœ¨ç¼–ç¨‹ï¼Œè¿™å°†æ˜¯æ ‘ä¸­æ¯ä¸€å±‚çš„3* ***for*** *å¾ªç¯ï¼š*'
- en: '![](../Images/a949262be65f616b0d6fbd68d7ef07d3.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a949262be65f616b0d6fbd68d7ef07d3.png)'
- en: '**(3) Inner loop (continued):** Before moving to the next pass in the inner
    loop, we perform a comparison between the current value of ***Î”*** and the difference
    between the previous value of this state ***v*** and the new value for this state
    we just calculated ***V(s)***. We update ***Î”*** to the larger of these two: ***Î”
    â† max(Î”,| v - V(s)|)***. This helps us track how close we are to convergence.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '**(3) å†…å¾ªç¯ï¼ˆç»§ç»­ï¼‰ï¼š** åœ¨ç§»åŠ¨åˆ°å†…å¾ªç¯çš„ä¸‹ä¸€æ¬¡è¿­ä»£ä¹‹å‰ï¼Œæˆ‘ä»¬å¯¹å½“å‰***Î”***çš„å€¼å’Œè¯¥çŠ¶æ€***v***çš„å‰ä¸€ä¸ªå€¼ä¸åˆšè®¡ç®—çš„çŠ¶æ€æ–°å€¼***V(s)***ä¹‹é—´çš„å·®å¼‚è¿›è¡Œæ¯”è¾ƒã€‚æˆ‘ä»¬å°†***Î”***æ›´æ–°ä¸ºè¿™ä¸¤è€…ä¸­çš„è¾ƒå¤§è€…ï¼š***Î”
    â† max(Î”,| v - V(s)|)***ã€‚è¿™æœ‰åŠ©äºæˆ‘ä»¬è·Ÿè¸ªç¦»æ”¶æ•›çš„è·ç¦»ã€‚'
- en: Ok, this process completes one pass of the inner loop. **We perform step (3)
    for each s in S** before breaking out of the inner loop and performing a check
    on the convergence condition ***Î” < Î¸.*** If this condition is met, we break out
    the outer loop, if not, we go back to step (2).
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: å¥½çš„ï¼Œè¿™ä¸ªè¿‡ç¨‹å®Œæˆäº†ä¸€æ¬¡å†…å¾ªç¯çš„æ“ä½œã€‚ **æˆ‘ä»¬å¯¹Sä¸­çš„æ¯ä¸ªsæ‰§è¡Œç¬¬ï¼ˆ3ï¼‰æ­¥**ï¼Œç„¶åå†é€€å‡ºå†…å¾ªç¯å¹¶å¯¹æ”¶æ•›æ¡ä»¶***Î” < Î¸***è¿›è¡Œæ£€æŸ¥ã€‚å¦‚æœæ»¡è¶³æ­¤æ¡ä»¶ï¼Œæˆ‘ä»¬é€€å‡ºå¤–å¾ªç¯ï¼Œå¦åˆ™è¿”å›ç¬¬ï¼ˆ2ï¼‰æ­¥ã€‚
- en: '**(4) Policy extraction:** By this time, we have likely performed multiple
    passes through the outer loop until we have converged. This means our value table
    will be updated to represent the final value of each state (in other words, â€˜how
    good it is to be in each stateâ€™). We can now extract a policy from this.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '**(4) ç­–ç•¥æå–ï¼š** åˆ°ç›®å‰ä¸ºæ­¢ï¼Œæˆ‘ä»¬å¯èƒ½å·²ç»è¿›è¡Œäº†å¤šæ¬¡å¤–å¾ªç¯ï¼Œç›´åˆ°æ”¶æ•›ã€‚è¿™æ„å‘³ç€æˆ‘ä»¬çš„å€¼è¡¨å°†è¢«æ›´æ–°ä¸ºè¡¨ç¤ºæ¯ä¸ªçŠ¶æ€çš„æœ€ç»ˆå€¼ï¼ˆæ¢å¥è¯è¯´ï¼Œâ€˜å¤„äºæ¯ä¸ªçŠ¶æ€çš„å¥½å¤„â€™ï¼‰ã€‚æˆ‘ä»¬ç°åœ¨å¯ä»¥ä»ä¸­æå–ç­–ç•¥ã€‚'
- en: '![](../Images/ea3f5ebe2aa38c2fc3d57beabedaeae4.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ea3f5ebe2aa38c2fc3d57beabedaeae4.png)'
- en: Remember, the policy **Ï€** is essentially a mapping from **states â†’ actions**,
    and for each state, it selects the action that maximizes the expected return.
    To calculate this, we perform the exact same process as before, but instead of
    getting a value for state ***s*** using ***maxâ‚***, we get the action ***a***
    that gives us the best value using ***argmaxâ‚***.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: è®°ä½ï¼Œç­–ç•¥**Ï€**æœ¬è´¨ä¸Šæ˜¯**çŠ¶æ€ â†’ åŠ¨ä½œ**çš„æ˜ å°„ï¼Œå¯¹äºæ¯ä¸ªçŠ¶æ€ï¼Œå®ƒé€‰æ‹©æœ€å¤§åŒ–æœŸæœ›å›æŠ¥çš„åŠ¨ä½œã€‚è¦è®¡ç®—è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬æ‰§è¡Œä¸ä¹‹å‰å®Œå…¨ç›¸åŒçš„è¿‡ç¨‹ï¼Œä½†ä¸æ˜¯ä½¿ç”¨***maxâ‚***è·å¾—çŠ¶æ€***s***çš„å€¼ï¼Œè€Œæ˜¯ä½¿ç”¨***argmaxâ‚***è·å¾—ç»™æˆ‘ä»¬æœ€ä½³å€¼çš„åŠ¨ä½œ***a***ã€‚
- en: And thatâ€™s it!
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: å°±è¿™æ ·ï¼
- en: '***Policy Iteration*** *is another dynamic programming algorithm. It is similar
    to VI except it alternates between improving the policy by making it greedy with
    respect to the current value function and evaluating the policyâ€™s performance
    until convergence, often requiring fewer iterations but more computation per iteration.*'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '***ç­–ç•¥è¿­ä»£*** *æ˜¯å¦ä¸€ç§åŠ¨æ€è§„åˆ’ç®—æ³•ã€‚å®ƒç±»ä¼¼äºVIï¼Œåªæ˜¯å®ƒåœ¨é€šè¿‡ä½¿ç­–ç•¥ç›¸å¯¹äºå½“å‰å€¼å‡½æ•°å˜å¾—è´ªå©ªæ¥æ”¹è¿›ç­–ç•¥å’Œè¯„ä¼°ç­–ç•¥çš„è¡¨ç°ç›´åˆ°æ”¶æ•›ä¹‹é—´äº¤æ›¿è¿›è¡Œï¼Œé€šå¸¸éœ€è¦è¾ƒå°‘çš„è¿­ä»£ä½†æ¯æ¬¡è¿­ä»£éœ€è¦æ›´å¤šè®¡ç®—ã€‚*'
- en: Solving the example using Value Iteration
  id: totrans-90
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä½¿ç”¨å€¼è¿­ä»£è§£å†³ç¤ºä¾‹
- en: VI should make even more sense once we complete an example problem, so letâ€™s
    get back to our golf MDP. We have formalised this as an MDP but currently, the
    agent doesnâ€™t know the best strategy when playing golf, so letâ€™s solve the golf
    MDP using VI.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€æ—¦å®Œæˆç¤ºä¾‹é—®é¢˜ï¼ŒVIåº”è¯¥æ›´æœ‰æ„ä¹‰ï¼Œæ‰€ä»¥è®©æˆ‘ä»¬å›åˆ°æˆ‘ä»¬çš„é«˜å°”å¤«MDPã€‚æˆ‘ä»¬å·²å°†å…¶å½¢å¼åŒ–ä¸ºMDPï¼Œä½†ç›®å‰ï¼Œä»£ç†ä¸çŸ¥é“æ‰“é«˜å°”å¤«çš„æœ€ä½³ç­–ç•¥ï¼Œå› æ­¤è®©æˆ‘ä»¬ä½¿ç”¨VIè§£å†³é«˜å°”å¤«MDPã€‚
- en: '![](../Images/3bc4524300a1072607e002076d7f28bd.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3bc4524300a1072607e002076d7f28bd.png)'
- en: 'Weâ€™ll start by defining our model parameters using fairly standard values:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†å¼€å§‹ä½¿ç”¨ç›¸å½“æ ‡å‡†çš„å€¼æ¥å®šä¹‰æˆ‘ä»¬çš„æ¨¡å‹å‚æ•°ï¼š
- en: '[PRE0]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'We will then initialise our value table to 0 for states in ***S***:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åæˆ‘ä»¬å°†å¯¹***S***ä¸­çš„çŠ¶æ€åˆå§‹åŒ–æˆ‘ä»¬çš„å€¼è¡¨ä¸º0ï¼š
- en: '[PRE1]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'We can now start in the outer loop:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ç°åœ¨å¯ä»¥å¼€å§‹å¤–å¾ªç¯ï¼š
- en: '[PRE2]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'And three passes of the inner loop for each state in ***S***:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äº***S***ä¸­çš„æ¯ä¸ªçŠ¶æ€ï¼Œè¿›è¡Œä¸‰æ¬¡å†…å¾ªç¯ï¼š
- en: '[PRE3]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'This gives us the following update to our value table:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ºæˆ‘ä»¬çš„å€¼è¡¨æä¾›äº†ä»¥ä¸‹æ›´æ–°ï¼š
- en: '[PRE4]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '*We donâ€™t need to worry about* ***s2*** *as this is a terminal state, meaning
    no actions are possible here.*'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '*æˆ‘ä»¬ä¸éœ€è¦æ‹…å¿ƒ* ***s2*** *ï¼Œå› ä¸ºè¿™æ˜¯ä¸€ä¸ªç»ˆæ­¢çŠ¶æ€ï¼Œæ„å‘³ç€è¿™é‡Œä¸å¯èƒ½è¿›è¡Œä»»ä½•æ“ä½œã€‚*'
- en: 'We now break out the inner loop and continue the outer loop, performing a convergence
    check on:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ç°åœ¨åˆ†ç¦»å†…å¾ªç¯ï¼Œç»§ç»­å¤–å¾ªç¯ï¼Œå¯¹ä»¥ä¸‹å†…å®¹è¿›è¡Œæ”¶æ•›æ£€æŸ¥ï¼š
- en: '[PRE5]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Since we have not converged, we do a second iteration of the outer loop:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±äºå°šæœªæ”¶æ•›ï¼Œæˆ‘ä»¬è¿›è¡Œå¤–å¾ªç¯çš„ç¬¬äºŒæ¬¡è¿­ä»£ï¼š
- en: '[PRE6]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'And another 3 passes of the inner loop, using the updated value table:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: å¦è¿›è¡Œ3æ¬¡å†…å¾ªç¯ï¼Œä½¿ç”¨æ›´æ–°åçš„å€¼è¡¨ï¼š
- en: '[PRE7]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'At the end of the second iteration, our values are:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ç¬¬äºŒæ¬¡è¿­ä»£ç»“æŸæ—¶ï¼Œæˆ‘ä»¬çš„å€¼æ˜¯ï¼š
- en: '[PRE8]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Check convergence once again:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: å†æ¬¡æ£€æŸ¥æ”¶æ•›ï¼š
- en: '[PRE9]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Still no convergence, so we continue the same process as above until Î” < Î¸.
    I wonâ€™t show all the calculations, the above two are enough to understand the
    process.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: ä»æœªæ”¶æ•›ï¼Œæ‰€ä»¥æˆ‘ä»¬ç»§ç»­ä¸Šè¿°è¿‡ç¨‹ï¼Œç›´åˆ° Î” < Î¸ã€‚æˆ‘ä¸ä¼šå±•ç¤ºæ‰€æœ‰è®¡ç®—ï¼Œä»¥ä¸Šä¸¤ä¸ªæ­¥éª¤è¶³ä»¥ç†è§£è¿‡ç¨‹ã€‚
- en: 'After 6 iterations, our policy converges. This is our values and convergence
    rate as they change over each iteration:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: ç»è¿‡6æ¬¡è¿­ä»£ï¼Œæˆ‘ä»¬çš„ç­–ç•¥å·²ç»æ”¶æ•›ã€‚è¿™æ˜¯æˆ‘ä»¬çš„ä»·å€¼å’Œæ”¶æ•›ç‡åœ¨æ¯æ¬¡è¿­ä»£ä¸­çš„å˜åŒ–æƒ…å†µï¼š
- en: '[PRE10]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Now we can extract our policy:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬å¯ä»¥æå–æˆ‘ä»¬çš„ç­–ç•¥ï¼š
- en: '[PRE11]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Our final policy is:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬çš„æœ€ç»ˆç­–ç•¥æ˜¯ï¼š
- en: '[PRE12]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: So, when our agent is in the ***Ball on fairway*** state (s0), the best action
    is to ***hit to green***. This seems pretty obvious since that is the only available
    action. However, in ***s1***, where there are two possible actions, our policy
    has learned to ***hit in hole***. We can now give this learned policy to other
    agents who want to play golf!
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥ï¼Œå½“æˆ‘ä»¬çš„æ™ºèƒ½ä½“å¤„äº ***çƒåœ¨çƒé“ä¸Š*** çŠ¶æ€ï¼ˆs0ï¼‰æ—¶ï¼Œæœ€ä½³è¡ŒåŠ¨æ˜¯ ***å‡»çƒåˆ°æœå²­***ã€‚è¿™ä¼¼ä¹å¾ˆæ˜æ˜¾ï¼Œå› ä¸ºè¿™æ˜¯å”¯ä¸€å¯ç”¨çš„è¡ŒåŠ¨ã€‚ç„¶è€Œï¼Œåœ¨ ***s1***
    ä¸­ï¼Œé‚£é‡Œæœ‰ä¸¤ä¸ªå¯èƒ½çš„è¡ŒåŠ¨ï¼Œæˆ‘ä»¬çš„ç­–ç•¥å·²ç»å­¦ä¼šäº† ***æ‰“å…¥æ´ä¸­***ã€‚æˆ‘ä»¬ç°åœ¨å¯ä»¥å°†è¿™ä¸ªå­¦åˆ°çš„ç­–ç•¥æä¾›ç»™å…¶ä»–æƒ³æ‰“é«˜å°”å¤«çš„æ™ºèƒ½ä½“ï¼
- en: And there you have it! We have just solved a very simple RL problem using Value
    Iteration.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: å°±æ˜¯è¿™æ ·ï¼æˆ‘ä»¬åˆšåˆšä½¿ç”¨ä»·å€¼è¿­ä»£è§£å†³äº†ä¸€ä¸ªéå¸¸ç®€å•çš„ RL é—®é¢˜ã€‚
- en: Limitations to dynamic programming
  id: totrans-123
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: åŠ¨æ€ç¼–ç¨‹çš„å±€é™æ€§
- en: Itâ€™s important to note that Value Iteration, along with other Dynamic Programming
    algorithms, has its limitations. Firstly, it assumes that we have complete knowledge
    of the dynamics of the MDP (we call this ***model-based RL***). However, this
    is rarely the case in real-world problems, for example, we may not know the transition
    probabilities. For problems where this is the case, we need to use other approaches
    such as *Q-learning* (***model-free RL***).
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œä»·å€¼è¿­ä»£ä»¥åŠå…¶ä»–åŠ¨æ€ç¼–ç¨‹ç®—æ³•éƒ½æœ‰å…¶å±€é™æ€§ã€‚é¦–å…ˆï¼Œå®ƒå‡è®¾æˆ‘ä»¬å¯¹ MDP çš„åŠ¨æ€æœ‰å®Œå…¨çš„äº†è§£ï¼ˆæˆ‘ä»¬ç§°ä¹‹ä¸º***åŸºäºæ¨¡å‹çš„ RL***ï¼‰ã€‚ç„¶è€Œï¼Œåœ¨å®é™…é—®é¢˜ä¸­è¿™ç§æƒ…å†µå¾ˆå°‘è§ï¼Œä¾‹å¦‚ï¼Œæˆ‘ä»¬å¯èƒ½ä¸çŸ¥é“è½¬ç§»æ¦‚ç‡ã€‚å¯¹äºè¿™ç§æƒ…å†µï¼Œæˆ‘ä»¬éœ€è¦ä½¿ç”¨å…¶ä»–æ–¹æ³•ï¼Œå¦‚*Q-learning*ï¼ˆ***æ— æ¨¡å‹
    RL***ï¼‰ã€‚
- en: '![](../Images/f60ad4bf1fac52b752748a38f8f6dc1f.png)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f60ad4bf1fac52b752748a38f8f6dc1f.png)'
- en: Curse of dimensionality in chess
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: å›½é™…è±¡æ£‹ä¸­çš„ç»´åº¦è¯…å’’
- en: Secondly, for bigger problems, as the number of states and actions increases,
    the size of the value table grows exponentially (think about trying to define
    all the possible states of chess). This results in the â€˜*curse of dimensionality*â€™
    problem, where the computational and memory requirements escalate rapidly, making
    it challenging to apply DP to high-dimensional problems.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶æ¬¡ï¼Œå¯¹äºæ›´å¤§çš„é—®é¢˜ï¼Œéšç€çŠ¶æ€å’ŒåŠ¨ä½œæ•°é‡çš„å¢åŠ ï¼Œä»·å€¼è¡¨çš„å¤§å°ä¼šå‘ˆæŒ‡æ•°å¢é•¿ï¼ˆæƒ³è±¡ä¸€ä¸‹å°è¯•å®šä¹‰æ‰€æœ‰å¯èƒ½çš„å›½é™…è±¡æ£‹çŠ¶æ€ï¼‰ã€‚è¿™å¯¼è‡´äº†â€˜*ç»´åº¦è¯…å’’*â€™é—®é¢˜ï¼Œå…¶ä¸­è®¡ç®—å’Œå†…å­˜éœ€æ±‚è¿…é€Ÿå¢åŠ ï¼Œä½¿å¾—å°†
    DP åº”ç”¨äºé«˜ç»´é—®é¢˜å˜å¾—å…·æœ‰æŒ‘æˆ˜æ€§ã€‚
- en: Nevertheless, VI is great to learn as it introduces some of the key foundational
    concepts of RL which form the basis of more complex algorithms that you may go
    on to learn.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶è€Œï¼ŒVI å­¦ä¹ æ˜¯å¾ˆæ£’çš„ï¼Œå› ä¸ºå®ƒä»‹ç»äº†ä¸€äº› RL çš„å…³é”®åŸºç¡€æ¦‚å¿µï¼Œè¿™äº›æ¦‚å¿µæ„æˆäº†ä½ å¯èƒ½ä¼šç»§ç»­å­¦ä¹ çš„æ›´å¤æ‚ç®—æ³•çš„åŸºç¡€ã€‚
- en: Thanks for reading!
  id: totrans-129
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ„Ÿè°¢é˜…è¯»ï¼
- en: I hope this article has provided an easy-to-understand introduction to reinforcement
    learning, and Value Iteration specifically.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘å¸Œæœ›è¿™ç¯‡æ–‡ç« æä¾›äº†ä¸€ä¸ªæ˜“äºç†è§£çš„å¼ºåŒ–å­¦ä¹ ä»‹ç»ï¼Œç‰¹åˆ«æ˜¯ä»·å€¼è¿­ä»£ã€‚
- en: '*If you learnt something new here, please give this a ğŸ‘ and follow!*'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '*å¦‚æœä½ åœ¨è¿™é‡Œå­¦åˆ°äº†æ–°çš„çŸ¥è¯†ï¼Œè¯·ç‚¹èµğŸ‘å¹¶å…³æ³¨ï¼*'
- en: Unless otherwise stated, all images are created by the author.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: é™¤éå¦æœ‰è¯´æ˜ï¼Œæ‰€æœ‰å›¾ç‰‡å‡ç”±ä½œè€…åˆ›ä½œã€‚
