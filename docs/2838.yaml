- en: 'Reinforcement Learning: an Easy Introduction to Value Iteration'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/reinforcement-learning-an-easy-introduction-to-value-iteration-e4cfe0731fd5?source=collection_archive---------0-----------------------#2023-09-10](https://towardsdatascience.com/reinforcement-learning-an-easy-introduction-to-value-iteration-e4cfe0731fd5?source=collection_archive---------0-----------------------#2023-09-10)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Learn the fundamentals of RL and how to apply Value Iteration to a simple example
    problem
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@carlbettosi?source=post_page-----e4cfe0731fd5--------------------------------)[![Carl
    Bettosi](../Images/19c640d8e96fa39583a3c998764aa2b8.png)](https://medium.com/@carlbettosi?source=post_page-----e4cfe0731fd5--------------------------------)[](https://towardsdatascience.com/?source=post_page-----e4cfe0731fd5--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----e4cfe0731fd5--------------------------------)
    [Carl Bettosi](https://medium.com/@carlbettosi?source=post_page-----e4cfe0731fd5--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fabe6f5e189c8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-an-easy-introduction-to-value-iteration-e4cfe0731fd5&user=Carl+Bettosi&userId=abe6f5e189c8&source=post_page-abe6f5e189c8----e4cfe0731fd5---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----e4cfe0731fd5--------------------------------)
    ·15 min read·Sep 10, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fe4cfe0731fd5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-an-easy-introduction-to-value-iteration-e4cfe0731fd5&user=Carl+Bettosi&userId=abe6f5e189c8&source=-----e4cfe0731fd5---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fe4cfe0731fd5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-an-easy-introduction-to-value-iteration-e4cfe0731fd5&source=-----e4cfe0731fd5---------------------bookmark_footer-----------)![](../Images/291df786927604981f62437fda3c5b4f.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: Value Iteration (VI) is typically one of the first algorithms introduced on
    the Reinforcement Learning (RL) learning pathway. The underlying specifics of
    the algorithm introduce some of the most fundamental aspects of RL and, hence,
    it is important to master VI before progressing to more complex RL algorithms.
    Yet, this can be tricky to get your head around.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: This article is designed to be an easy-to-understand introduction to VI, which
    will assume the reader to be new to the field of RL. Let’s get started.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: Already know the basics of RL? → [**Skip to how to use Value Iteration**](#dab6).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: The basics of RL
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s start with a textbook definition, then I’ll break it down using an easy
    example.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/bdfd099792f5ee2310c43662522084b7.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
- en: Overview of the reinforcement learning training process
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: RL is one of the three key machine learning paradigms beside supervised and
    unsupervised learning. RL is not a singular algorithm, but rather a framework
    which encompasses a range of techniques and approaches for teaching agents to
    learn and make decisions in their environments.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: In RL, an agent interacts with an environment by taking various actions. The
    agent is rewarded when those actions lead to desired states and punished when
    they don’t. The agent’s objective is to learn a strategy, called a policy, that
    guides its actions to maximize the reward it accumulates over time. This trial-and-error
    process refines the agent's behavioral policy, allowing it to take optimal or
    near-optimal behaviors in its environment.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: '*The book An “Introduction to Reinforcement Learning” by Richard S. Sutton
    and Andrew G. Barto is considered the best in the field for those wishing to gain
    a solid understanding of RL.. and it’s* [*available for free*](http://incompleteideas.net/book/the-book-2nd.html)*!*'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: Let’s define an example problem
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/25fc75950f08dcf8fbb9ceba748c9fa4.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
- en: Possible states for the game of golf
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: This image depicts the game of golf in its simplest form. We will use this as
    golf has a clearly defined goal - get the ball in the hole.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: 'In our example, the golf ball can be in one of three positions: *on the fairway*;
    *on the green*; or *in the hole*. We start on the fairway and aim to get closer
    to the hole with each shot, with the hole sitting on the green.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: In RL, each of these positions is referred to as a ***state*** of the ***environment.***
    You can think of the state as being a snapshot of the current environment (the
    golf course), which also records the ball position.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2938c430e74f76f0d14cc0001e1ae266.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
- en: Possible actions in the game of golf
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: 'In our game, the ***agent*** takes shots of hitting the ball, starting at the
    *ball on fairway* state. The ***agent*** simply refers to the entity that is in
    control of taking ***actions***. Our game has three available actions: *hit to
    fairway*; *hit to green*; and *hit in hole*.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/110c6f6d02f7fb59ee2b805e836a368b.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
- en: Transition probabilities in the game of golf
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: Of course, when you take a shot, the ball may not go where you want it to. Therefore,
    we introduce a ***transition function*** linking actions to states with some probability
    weighting.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: For example, we may miss the green when we take a shot from the fairway and
    end up still on the fairway. Written in an RL context, if we are in the *ball
    on fairway* state and take the action *hit to green*, there is a 90% probability
    we will enter the *ball on green* state but a 10% probability we will re-enter
    the *ball on fairway* state.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d9a6e396a04c36b4d5f972989603693b.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d9a6e396a04c36b4d5f972989603693b.png)'
- en: A reward of 10 for getting the ball in the hole
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 将球击入洞中的奖励为 10
- en: Every time the agent takes an action, we refer to this as a ***step*** through
    the environment. Based upon the action just taken, the agent observes the new
    state it ends up in as well as a ***reward***. A reward function is an incentive
    mechanism for pushing the agent in the right direction. In other words, we design
    the reward function to shape the desired behavior of our agent. In our simplified
    golf example, we provide a reward of 10 for getting the ball in the hole.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 每当代理执行一个动作时，我们称之为在环境中的 ***步***。根据刚刚采取的动作，代理观察到它所处的新状态以及一个 ***奖励***。奖励函数是一个激励机制，用于引导代理朝正确方向前进。换句话说，我们设计奖励函数以塑造代理的期望行为。在我们简化的高尔夫示例中，我们为将球击入洞中的行为提供
    10 的奖励。
- en: '*The design of environment dynamics (the transition and reward functions) is
    not a trivial task. If the environment does not represent the problem you are
    attempting to solve, the agent will learn a policy reflecting a correct solution
    for an incorrect problem. We will not cover such design elements here, but it’s
    worth noting.*'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '*环境动态（转换和奖励函数）的设计不是一项简单的任务。如果环境未能代表你试图解决的问题，代理将学习到一个反映不正确问题的正确解决方案的策略。我们在这里不讨论这些设计元素，但值得注意。*'
- en: Markov Decision Processes
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 马尔可夫决策过程
- en: To represent a problem in a way the agent understands, we must formalise it
    as a Markov Decision Process (MDP).
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 为了以代理能够理解的方式表示问题，我们必须将其形式化为马尔可夫决策过程（MDP）。
- en: A MDP is a mathematical model which describes our problem in a structured way.
    It represents the agent's interactions with the environment as a sequential decision-making
    process (i.e., one action after another).
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: MDP 是一种数学模型，它以结构化的方式描述了我们的问题。它将代理与环境的互动表示为一个顺序决策过程（即，一个接一个的动作）。
- en: 'It consists of the environment dynamics (I’m going to add some math notation
    here to shorten things):'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 它由环境动态组成（我将添加一些数学符号以简化说明）：
- en: a finite set of states **s ∈ S**.
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个有限的状态集合 **s ∈ S**。
- en: a finite set of actions ***a* ∈ *A***.
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个有限的动作集合 ***a* ∈ *A***。
- en: a transition function ***T*(*s*′∣*s*,*a*)** returning the probability of reaching
    state ***s*′**, given the current state ***s***, the current action ***a***.
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个转换函数 ***T*(*s*′∣*s*,*a*)** 返回在当前状态 ***s*** 和当前动作 ***a*** 下达到状态 ***s*′** 的概率。
- en: a reward function ***R*(*s*,*a*,*s*′)** returning a scalar reward based on reaching
    next state ***s*′**, after being in state ***s***, and taking action ***a***.
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个奖励函数 ***R*(*s*,*a*,*s*′)** 基于从状态 ***s*** 到达下一个状态 ***s*′** 的情况返回一个标量奖励，并考虑采取动作
    ***a***。
- en: '*Note that if there is some uncertainty or randomness involved in the transitions
    between states (i.e. taking the same action in the same state twice may lead to
    different outcomes), we refer to this as a* ***stochastic*** *MDP. We could also
    create a* ***deterministic*** *MDP, where transitions and rewards are entirely
    predictable. This means when an agent takes an action in a specific state, there
    is a one-to-one correspondence between the action and the resulting next state
    and the reward.*'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '*注意，如果状态之间的转换涉及某些不确定性或随机性（即在相同状态下两次采取相同动作可能导致不同结果），我们称之为* ***随机*** *MDP。我们也可以创建一个*
    ***确定性*** *MDP，其中转换和奖励是完全可预测的。这意味着当代理在特定状态下采取一个动作时，动作与结果状态及奖励之间是一一对应的。*'
- en: Visualised as an MDP, our golf problem looks pretty much the same as the images
    depicted earlier. We will use S = {s1, s2, s3} for shorthand.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 以 MDP 形式可视化，我们的高尔夫问题看起来与之前描述的图像几乎相同。我们将使用 S = {s1, s2, s3} 作为简写。
- en: '![](../Images/d2778219029b8237cd1d52e14e59fb54.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d2778219029b8237cd1d52e14e59fb54.png)'
- en: Our golf example problem written as an MDP
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的高尔夫示例问题以 MDP 形式呈现
- en: '*The use of an MDP assumes that what’s going to happen next in the environment
    only depends on what’s happening right now — the current state and action — and
    not on what happened before. This is called the* **Markov property***, and it’s
    important in RL as it reduces computational complexity. I’ll explain this more
    later.*'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '*MDP 的使用假设环境中接下来会发生的事情只依赖于现在的状态和动作，而不依赖于之前发生的事情。这被称为* **马尔可夫性质** *，它在强化学习中很重要，因为它降低了计算复杂性。我稍后会详细解释这一点。*'
- en: What is Value Iteration?
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 什么是价值迭代？
- en: Value Iteration (VI) is an algorithm used to solve RL problems like the golf
    example mentioned above, where we have full knowledge of all components of the
    MDP. It works by iteratively improving its estimate of the ‘value’ of being in
    each state. It does this by considering the immediate rewards and expected future
    rewards when taking different available actions. These values are tracked using
    a value table, which updates at each step. Eventually, this sequence of improvements
    converges, yielding an optimal policy of state → action mappings that the agent
    can follow to make the best decisions in the given environment.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: VI leverages the concept of *dynamic programming*, where solving a big problem
    is broken down into smaller subproblems. To achieve this in VI, the *Bellman equation*
    is used to guide the process of iteratively updating value estimates for each
    state, providing a recursive relationship that expresses the value of a state
    in terms of the values of its neighbouring states.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: This won’t make much sense now. The easiest way to learn VI is to break it down
    step-by-step, so let’s do that.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: How does the Value Iteration algorithm work?
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The image below depicts the steps of the algorithm. Don’t be put off, it's easier
    than it appears.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b6f032fc68d508cb4d115e760c583e68.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
- en: The Value Iteration algorithm
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: Firstly, we need to define some parameters for our training.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: Theta **θ** represents a threshold for convergence. Once we reach **θ**, we
    can terminate the training loop and generate the policy. It’s essentially just
    a way to ensure the policy we create is accurate enough. If we stop training too
    early, we may not learn the best actions to take.
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gamma **γ** represents the *discount factor*. This is a value which determines
    how much our agent values future rewards compared to immediate rewards. A higher
    discount factor (closer to 1) indicates that the agent values long-term rewards
    more, while a lower discount factor (closer to 0) places greater emphasis on immediate
    rewards.
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*To understand the discount factor better, consider an RL agent playing chess.
    Let’s say you have the opportunity to capture your opponent’s queen in the next
    move, which would yield a significant immediate reward. However, you also notice
    that by sacrificing a less valuable piece now, you could set up a future advantage
    that might lead to checkmate and an even bigger reward later. The discount factor
    helps you balance this decision.*'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: '**(1) Initialisation:** Now we have the parameters defined, we want to initialise
    our *value function* ***V(s)*** for all states in ***S***. This typically means
    we set all values to 0 (or some other arbitrary constant) for every state. Think
    of the value function as a table that tracks a value for each state, updating
    frequently.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/fb25e69630ea42956f30577387e322b3.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
- en: An initialised value table
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: '**(2) Outer loop:** Now everything is set up, we can start the iterative process
    of updating our values. We begin in the outer loop, which repeats until the convergence
    criteria are met (until Δ < θ).'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '**(2) 外层循环：** 现在一切都准备好了，我们可以开始更新值的迭代过程。我们从外层循环开始，它会重复直到收敛标准满足（直到 Δ < θ）。'
- en: At each pass of the outer loop, we begin by setting Δ = 0\. Delta **Δ** is used
    to represent the change in value estimates across all states, and the algorithm
    continues iterating until this change Δfalls below the specified threshold θ.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在每次外层循环中，我们首先设置Δ = 0。**Δ** 用来表示所有状态的价值估计变化，算法继续迭代，直到这一变化Δ低于指定的阈值θ。
- en: '**(3) Inner loop:** For every state *s* in *S,* we:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '**(3) 内层循环：** 对于每个状态 *s* 在 *S* 中，我们：'
- en: set a variable ***v*** to the current value of that state ***V(s)***, remember
    - this is fetched from our value table (so on the first pass, ***v*** = ***V(s)***
    = 0)
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将变量 ***v*** 设置为当前状态的价值 ***V(s)***，记住 - 这是从我们的价值表中获取的（所以在第一次遍历时，***v*** = ***V(s)***
    = 0）
- en: perform the bellman equation to update ***V(s)***
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 执行贝尔曼方程来更新 ***V(s)***
- en: update **Δ** (we’ll come back to this)
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更新 **Δ**（我们稍后会回来讨论）
- en: '**The Bellman Equation**'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '**贝尔曼方程**'
- en: '![](../Images/ac783b3215bf86884f63526f4edb24b5.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ac783b3215bf86884f63526f4edb24b5.png)'
- en: This line of the algorithm is the most important. It requires that we update
    the value of the current state we are looking at in the loop. This value is calculated
    by considering all available actions from that specific state (a 1-step look ahead).
    When we take each of those possible actions it will present us with a set of possible
    next states ***s*′** and respective rewards ***r.***
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 算法的这一行是最重要的。它要求我们更新当前循环中所看的状态的价值。这个价值是通过考虑从那个特定状态所有可用的动作（前瞻1步）来计算的。当我们采取这些可能的动作时，它会给我们一组可能的下一个状态
    ***s*′** 和相应的奖励 ***r.***
- en: '![](../Images/8076df201e89c49f337967581486f369.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8076df201e89c49f337967581486f369.png)'
- en: 'So, for each of those next states ***s*′** and respective rewards ***r***,
    we perform ***p(s′, r|s, a)[r + γV(s′)].*** Let''s break this up:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，对于每一个下一个状态 ***s*′** 和相应的奖励 ***r***，我们执行 ***p(s′, r|s, a)[r + γV(s′)].***
    让我们分解一下：
- en: '***p(s′, r|s, a)*** the probability of being in state ***s***, taking action
    ***a***, and ending up in next state ***s*′**(this is just our transition function)'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***p(s′, r|s, a)*** 在状态 ***s*** 中，采取动作 ***a*** 并最终到达下一个状态 ***s*′** 的概率（这只是我们的转移函数）'
- en: '***[r + γV(s′)]*** the reward ***r*** of ending up in next state ***s*′** (we
    get that from our reward function) *+* our discount **γ** * by the value of that
    next state ***s*′** (we get that from our value table)'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***[r + γV(s′)]*** 下一个状态 ***s*′** 的奖励 ***r***（我们从奖励函数中得到这个值） *+* 我们的折扣 **γ**
    * 乘以下一个状态的价值 ***s*′**（我们从价值表中得到这个值）'
- en: We then multiply these two parts ***p(s′, r|s, a) * [r + γV(s′)]***
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后我们将这两个部分 ***p(s′, r|s, a) * [r + γV(s′)]*** 相乘
- en: Remember, this calculation is just for **one** next state ***s′*** (the third
    level of the tree), we need to repeat this for each possible next state ***s′***
    after taking ***a.***
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，这个计算只是针对**一个**下一个状态 ***s′***（树的第三层），我们需要对每一个可能的下一个状态 ***s′*** 在采取 ***a.***
    后重复这个过程。
- en: Once we have done this, we sum all the results we just got ***Σₛ′, ᵣ p(s′, r|s,
    a) * [r + γV(s′)].*** We then repeat this for each action ***a*** (the second
    level in the tree).
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦完成这一步，我们将所有刚获得的结果 ***Σₛ′, ᵣ p(s′, r|s, a) * [r + γV(s′)].*** 汇总起来。然后我们对每个动作
    ***a*** 进行重复（树中的第二层）。
- en: Once these steps are complete, we will have a value associated with each possible
    action ***a*** from the current state we are looking at in the inner loop ***s.***
    Wechoose the highest using ***maxₐ*** and set this equal to our new value for
    that state ***V(s)←maxₐ Σₛ′, ᵣ p(s′, r|s, a) * [r + γV(s′)].***
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦完成这些步骤，我们将为当前状态 ***s*** 在内层循环中看到的每个可能的动作 ***a*** 关联一个值。我们使用 ***maxₐ*** 选择最高的，并将其设置为该状态的新值
    ***V(s)←maxₐ Σₛ′, ᵣ p(s′, r|s, a) * [r + γV(s′)].***
- en: Remember, this process covered only one state ***s*** (the first level in the
    tree)
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，这个过程只涵盖了一个状态 ***s***（树的第一层）
- en: '*If we were programming this, it would be 3* ***for*** *loops for each level
    in the tree:*'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '*如果我们在编程，这将是树中每一层的3* ***for*** *循环：*'
- en: '![](../Images/a949262be65f616b0d6fbd68d7ef07d3.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a949262be65f616b0d6fbd68d7ef07d3.png)'
- en: '**(3) Inner loop (continued):** Before moving to the next pass in the inner
    loop, we perform a comparison between the current value of ***Δ*** and the difference
    between the previous value of this state ***v*** and the new value for this state
    we just calculated ***V(s)***. We update ***Δ*** to the larger of these two: ***Δ
    ← max(Δ,| v - V(s)|)***. This helps us track how close we are to convergence.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '**(3) 内循环（继续）：** 在移动到内循环的下一次迭代之前，我们对当前***Δ***的值和该状态***v***的前一个值与刚计算的状态新值***V(s)***之间的差异进行比较。我们将***Δ***更新为这两者中的较大者：***Δ
    ← max(Δ,| v - V(s)|)***。这有助于我们跟踪离收敛的距离。'
- en: Ok, this process completes one pass of the inner loop. **We perform step (3)
    for each s in S** before breaking out of the inner loop and performing a check
    on the convergence condition ***Δ < θ.*** If this condition is met, we break out
    the outer loop, if not, we go back to step (2).
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，这个过程完成了一次内循环的操作。 **我们对S中的每个s执行第（3）步**，然后再退出内循环并对收敛条件***Δ < θ***进行检查。如果满足此条件，我们退出外循环，否则返回第（2）步。
- en: '**(4) Policy extraction:** By this time, we have likely performed multiple
    passes through the outer loop until we have converged. This means our value table
    will be updated to represent the final value of each state (in other words, ‘how
    good it is to be in each state’). We can now extract a policy from this.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '**(4) 策略提取：** 到目前为止，我们可能已经进行了多次外循环，直到收敛。这意味着我们的值表将被更新为表示每个状态的最终值（换句话说，‘处于每个状态的好处’）。我们现在可以从中提取策略。'
- en: '![](../Images/ea3f5ebe2aa38c2fc3d57beabedaeae4.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ea3f5ebe2aa38c2fc3d57beabedaeae4.png)'
- en: Remember, the policy **π** is essentially a mapping from **states → actions**,
    and for each state, it selects the action that maximizes the expected return.
    To calculate this, we perform the exact same process as before, but instead of
    getting a value for state ***s*** using ***maxₐ***, we get the action ***a***
    that gives us the best value using ***argmaxₐ***.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，策略**π**本质上是**状态 → 动作**的映射，对于每个状态，它选择最大化期望回报的动作。要计算这一点，我们执行与之前完全相同的过程，但不是使用***maxₐ***获得状态***s***的值，而是使用***argmaxₐ***获得给我们最佳值的动作***a***。
- en: And that’s it!
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样！
- en: '***Policy Iteration*** *is another dynamic programming algorithm. It is similar
    to VI except it alternates between improving the policy by making it greedy with
    respect to the current value function and evaluating the policy’s performance
    until convergence, often requiring fewer iterations but more computation per iteration.*'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '***策略迭代*** *是另一种动态规划算法。它类似于VI，只是它在通过使策略相对于当前值函数变得贪婪来改进策略和评估策略的表现直到收敛之间交替进行，通常需要较少的迭代但每次迭代需要更多计算。*'
- en: Solving the example using Value Iteration
  id: totrans-90
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用值迭代解决示例
- en: VI should make even more sense once we complete an example problem, so let’s
    get back to our golf MDP. We have formalised this as an MDP but currently, the
    agent doesn’t know the best strategy when playing golf, so let’s solve the golf
    MDP using VI.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦完成示例问题，VI应该更有意义，所以让我们回到我们的高尔夫MDP。我们已将其形式化为MDP，但目前，代理不知道打高尔夫的最佳策略，因此让我们使用VI解决高尔夫MDP。
- en: '![](../Images/3bc4524300a1072607e002076d7f28bd.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3bc4524300a1072607e002076d7f28bd.png)'
- en: 'We’ll start by defining our model parameters using fairly standard values:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将开始使用相当标准的值来定义我们的模型参数：
- en: '[PRE0]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'We will then initialise our value table to 0 for states in ***S***:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们将对***S***中的状态初始化我们的值表为0：
- en: '[PRE1]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'We can now start in the outer loop:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以开始外循环：
- en: '[PRE2]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'And three passes of the inner loop for each state in ***S***:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 对于***S***中的每个状态，进行三次内循环：
- en: '[PRE3]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'This gives us the following update to our value table:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 这为我们的值表提供了以下更新：
- en: '[PRE4]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '*We don’t need to worry about* ***s2*** *as this is a terminal state, meaning
    no actions are possible here.*'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '*我们不需要担心* ***s2*** *，因为这是一个终止状态，意味着这里不可能进行任何操作。*'
- en: 'We now break out the inner loop and continue the outer loop, performing a convergence
    check on:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在分离内循环，继续外循环，对以下内容进行收敛检查：
- en: '[PRE5]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Since we have not converged, we do a second iteration of the outer loop:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 由于尚未收敛，我们进行外循环的第二次迭代：
- en: '[PRE6]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'And another 3 passes of the inner loop, using the updated value table:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 另进行3次内循环，使用更新后的值表：
- en: '[PRE7]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'At the end of the second iteration, our values are:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 在第二次迭代结束时，我们的值是：
- en: '[PRE8]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Check convergence once again:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 再次检查收敛：
- en: '[PRE9]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Still no convergence, so we continue the same process as above until Δ < θ.
    I won’t show all the calculations, the above two are enough to understand the
    process.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 仍未收敛，所以我们继续上述过程，直到 Δ < θ。我不会展示所有计算，以上两个步骤足以理解过程。
- en: 'After 6 iterations, our policy converges. This is our values and convergence
    rate as they change over each iteration:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Now we can extract our policy:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Our final policy is:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: So, when our agent is in the ***Ball on fairway*** state (s0), the best action
    is to ***hit to green***. This seems pretty obvious since that is the only available
    action. However, in ***s1***, where there are two possible actions, our policy
    has learned to ***hit in hole***. We can now give this learned policy to other
    agents who want to play golf!
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: And there you have it! We have just solved a very simple RL problem using Value
    Iteration.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: Limitations to dynamic programming
  id: totrans-123
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: It’s important to note that Value Iteration, along with other Dynamic Programming
    algorithms, has its limitations. Firstly, it assumes that we have complete knowledge
    of the dynamics of the MDP (we call this ***model-based RL***). However, this
    is rarely the case in real-world problems, for example, we may not know the transition
    probabilities. For problems where this is the case, we need to use other approaches
    such as *Q-learning* (***model-free RL***).
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f60ad4bf1fac52b752748a38f8f6dc1f.png)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
- en: Curse of dimensionality in chess
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: Secondly, for bigger problems, as the number of states and actions increases,
    the size of the value table grows exponentially (think about trying to define
    all the possible states of chess). This results in the ‘*curse of dimensionality*’
    problem, where the computational and memory requirements escalate rapidly, making
    it challenging to apply DP to high-dimensional problems.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: Nevertheless, VI is great to learn as it introduces some of the key foundational
    concepts of RL which form the basis of more complex algorithms that you may go
    on to learn.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: Thanks for reading!
  id: totrans-129
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: I hope this article has provided an easy-to-understand introduction to reinforcement
    learning, and Value Iteration specifically.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: '*If you learnt something new here, please give this a 👏 and follow!*'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: Unless otherwise stated, all images are created by the author.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
