- en: Exploring Feature Extraction with CNNs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/exploring-feature-extraction-with-cnns-345125cefc9a?source=collection_archive---------0-----------------------#2023-11-25](https://towardsdatascience.com/exploring-feature-extraction-with-cnns-345125cefc9a?source=collection_archive---------0-----------------------#2023-11-25)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Using a Convolutional Neural Network to check specialization in feature extraction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@rodrigopesilva?source=post_page-----345125cefc9a--------------------------------)[![Rodrigo
    Silva](../Images/d260f05ed9887c5072e0590db1481be2.png)](https://medium.com/@rodrigopesilva?source=post_page-----345125cefc9a--------------------------------)[](https://towardsdatascience.com/?source=post_page-----345125cefc9a--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----345125cefc9a--------------------------------)
    [Rodrigo Silva](https://medium.com/@rodrigopesilva?source=post_page-----345125cefc9a--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F222e82adf972&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fexploring-feature-extraction-with-cnns-345125cefc9a&user=Rodrigo+Silva&userId=222e82adf972&source=post_page-222e82adf972----345125cefc9a---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----345125cefc9a--------------------------------)
    ·8 min read·Nov 25, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F345125cefc9a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fexploring-feature-extraction-with-cnns-345125cefc9a&user=Rodrigo+Silva&userId=222e82adf972&source=-----345125cefc9a---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F345125cefc9a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fexploring-feature-extraction-with-cnns-345125cefc9a&source=-----345125cefc9a---------------------bookmark_footer-----------)![](../Images/a922079bb33abb945ff04ccc0fd86159.png)![](../Images/b37cab9a98460c70520e728500584f40.png)'
  prefs: []
  type: TYPE_NORMAL
- en: (**Left**) Feature extraction performed over the image of a lion using vgg19
    CNN architecture (image by author). (**Right**) Original picture of the lion (public
    domain, availabe at [Pexels](https://www.pexels.com/pt-br/foto/africa-africano-animais-selvagens-animal-41315/)).
  prefs: []
  type: TYPE_NORMAL
- en: Convolutional Neural Networks are today’s building blocks for image classification
    tasks using machine learning. However, another very useful task they perform before
    classification is to extract relevant features from an image. Feature extraction
    is the way CNNs recognize key patterns of an image in order to classify it. This
    article will show an example of how to perform feature extractions using TensorFlow
    and the Keras functional API. But first, in order to formalize these CNN concepts,
    we need to talk first about pixel space.
  prefs: []
  type: TYPE_NORMAL
- en: Background
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Pixel space
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Pixel space is exactly what the name suggests: it is the space where the image
    gets converted into a matrix of values, where each value corresponds to an individual
    pixel. Therefore, the original image that we see, when fed into the CNN, gets
    converted into a matrix of numbers. In grayscale images, these numbers typically
    range from 0 (black) to 255 (white), and values in-between are shades of gray.
    In this article, all images have been normalized, that is, every pixel has been
    divided by 255 so its value lies in the interval [0, 1].'
  prefs: []
  type: TYPE_NORMAL
- en: CNN and pixel space
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: What a CNN does with the image in pixel representation is to apply filters and
    to process it in order to extract relevant pixels to make the final “decision”,
    which is to put that image within a class. For example, in the image at the top
    of the page, that CNN paid a lot of attention to the lion’s mouth, tongue, eyes
    (and strong contours in general), and these features are further extracted as
    we go deeper into the neural network. Therefore, it suffices to say that, the
    more specialized a CNN is in terms of classification, the more professional it
    is in recognizing key features of an image.
  prefs: []
  type: TYPE_NORMAL
- en: The goal
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'With that being said, the goal is simple: to see the level of specialization
    of a CNN when it comes to feature extraction.'
  prefs: []
  type: TYPE_NORMAL
- en: The Method
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To do this, I trained two CNNs with the same architecture, but with different
    training sizes: one with 50K images (this is the *benchmark*, the smart one),
    and the other with 10K images (this is the *dummy* one). After that, I sliced
    up the layers of the CNN to check what the algorithm is seeing and the sense it
    makes of the image fed into it.'
  prefs: []
  type: TYPE_NORMAL
- en: Dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The dataset used for this project was the widely used [cifar10](https://keras.io/api/datasets/cifar10/)
    image dataset [1], a public domain dataset, which is a 60K image base divided
    into 10 classes, of which 10K images are used as hold-out validation set. The
    images are 32x32 pixels in size, and they are RGB-colored, which means 3 color
    channels.
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to prevent data leak, I kept one image to use as a test image in feature
    recognition, therefore this image was not used in either of the trainings. I present
    you our guinea pig: the frog.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7848cfb5764c67c4872f6e6b80a5455d.png)'
  prefs: []
  type: TYPE_IMG
- en: The frog.
  prefs: []
  type: TYPE_NORMAL
- en: Implementation in TensorFlow
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The implementation is shown in the code snippet below. To properly slice the
    layers of the CNN it is necessary to use the Keras functional API in TensorFlow
    instead of the Sequential API. It works as a cascade, where the next layer is
    called over the last one.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The specs of the architecture are shown below in Fig. 1.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d4c391b63dc5594ae120d213fc2c0e44.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig. 1: Summary of the CNN used. Image by author.'
  prefs: []
  type: TYPE_NORMAL
- en: The optimiser used is Adam, the loss function was categorical cross-entropy,
    and the metric used for evaluation was simply accuracy, since the dataset is perfectly
    balanced.
  prefs: []
  type: TYPE_NORMAL
- en: Slicing the CNNs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now we can slice some strategical layers of the two CNNs in order to check
    the processing level of the images. The code implementation is shown below:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'What is happening here is as follows: the first line access each layer of the
    model and the second line returns the input layer of the whole CNN. Then in the
    third line we make a list showing the outputs of each layer, and finally, we create
    a new model, whose outputs are the outputs of the layers. This way we can take
    a look at what is happening in-between layers.'
  prefs: []
  type: TYPE_NORMAL
- en: A very similar code was written to access the layers of our dummy model, so
    it will be omitted here. Now let’s proceed to look at the images of our frog,
    processed within different layers of our CNNs.
  prefs: []
  type: TYPE_NORMAL
- en: First convolutional layer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '***Dummy***'
  prefs: []
  type: TYPE_NORMAL
- en: Fig. 2 shows the images of the 16 filters of the convolutional layer (conv2d_1).
    We can see that the images are not super processed, and there is a lot of redundancy.
    One could argue that this is only the first convolutional layer, which accounts
    for the fact that the processing isn’t so heavy, and that’s a fair observation.
    In order to tackle this, we shall look at the first layer of the benchmark.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2f0315aa8278c50a79032051a500ceb6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig. 2: First convolutional layer of the dummy classifier. Image by author.'
  prefs: []
  type: TYPE_NORMAL
- en: '***Benchmark***'
  prefs: []
  type: TYPE_NORMAL
- en: 'The benchmark classifier shows a much more processed image, to the point where
    most of these images are not recognizable anymore. Remember: this is only the
    first convolutional layer.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/dbf27ae108e871c0d24006df6b3ac155.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig. 3: First convolutional layer of the benchmark classifier. Image by author.'
  prefs: []
  type: TYPE_NORMAL
- en: Last convolutional layer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '***Dummy***'
  prefs: []
  type: TYPE_NORMAL
- en: As expected the image is not recognizable anymore, since we have gone through
    6 convolutional layers at this point, and 2 pooling layers, which explains the
    lower dimensions of the images. Let’s see what the last layer of the benchmark
    looks like.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/64db99c5f1fb96533bab7541f60a7917.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig. 4: Last convolutional layer of the dummy classifier. Image by author.'
  prefs: []
  type: TYPE_NORMAL
- en: '***Benchmark***'
  prefs: []
  type: TYPE_NORMAL
- en: This is even more processed, to the point where the majority of pixels are black,
    which shows that the important features were selected, and the rest of the image
    is basically thrown away.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/69c94f6b4d741d94c41d91738863d0c6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig. 5: Last convolutional layer of benchmark classifier. Image by author.'
  prefs: []
  type: TYPE_NORMAL
- en: How this ties with Information?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We can see that the processing degrees are very different for the same slice
    of the network. Qualitative analysis indicates that the benchmark model is more
    aggressive when it comes to extracting useful information from the input. This
    is particularly evident from the first convolutional layer comparison: the frog
    image output is much less distorted and much more recognizable on the dummy than
    it is on the benchmark model.'
  prefs: []
  type: TYPE_NORMAL
- en: This suggests that the benchmark is more efficient at throwing away elements
    of the image that are not useful when predicting the class, while the dummy classifier,
    uncertain of how to proceed, considers more features. We can see in Fig. 6 that
    the benchmark (in blue) throws away more color pixels than the dummy model (in
    red), which shows a longer tail in its distribution of colors.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8edcd5cc82e055a8f94bdc9f5b6ec445.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig. 6: Probability distribution for the pixels in the last layer. We can see
    that the benchmark´s pixels (in blue) are more squeezed towards zero, while the
    dummy mode´sl pixels (in red) show a longer tail.'
  prefs: []
  type: TYPE_NORMAL
- en: If we take a look at the pixel distribution of our original frog image, we have
    Fig. 7, which shows a much more symmetric distribution, centered more or less
    around 0.4.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7d09e89780f1d6bda77dec70a9a27b49.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig. 7: Color distribution of our original frog image. Image by author.'
  prefs: []
  type: TYPE_NORMAL
- en: From an information theory standpoint, the differences in the probability distributions
    from the original image and the resulting images after the convolutional layers
    represent a massive information gain.
  prefs: []
  type: TYPE_NORMAL
- en: Looking at Fig. 6 and comparing it with Fig. 7, we are much more certain of
    which pixels we are going to find in the former than in the latter. Hence, there
    is a gain of information. This is a very brief and qualitative exploration of
    Information Theory and opens up a doorway to a vast area. For more information
    about Information (pun intended), see this [post](https://medium.com/towards-data-science/neural-network-via-information-68af7f49b978).
  prefs: []
  type: TYPE_NORMAL
- en: And finally, a way to look at the uncertainty in the answer of the classifiers
    is to look at the probability distribution over the classes. This is the output
    of the sofmax function, at the end of our CNN. Fig. 8(left) shows that the benchmark
    is much more certain of the class, with a distribution peaked on the frog class;
    while Fig. 8(right) shows a confused dummy classifier, with the highest probability
    in the wrong class.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5393621906d0a791da424f130adb204a.png)![](../Images/4e5c10e6837620ee0aa0e9c463fcd0f6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig. 8: Probability distributions assigned by each classifier. Image by author.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This analysis shows an expected outcome: bigger training sets result in better
    feature capturing algorithms.'
  prefs: []
  type: TYPE_NORMAL
- en: The notebook for this project is available [here](https://github.com/rodrigops123/ML_projects/blob/main/Feature_extraction.ipynb).
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[1] Krizhevsky, Alex, and Geoffrey Hinton. [Learning multiple layers of features
    from tiny images](https://www.cs.toronto.edu/~kriz/learning-features-2009-TR.pdf).
    (2009): 7.'
  prefs: []
  type: TYPE_NORMAL
- en: '[2] M. Jogin, et al., [Feature Extraction using Convolution Neural Networks
    (CNN) and Deep Learning](https://ieeexplore.ieee.org/document/9012507) (2018),
    IEEE International Conference on Recent Trends in Electronics, Information & Communication
    Technology (RTEICT)'
  prefs: []
  type: TYPE_NORMAL
- en: '[3] K. Simonyan, A. Zisserman, [Very Deep Convolution Networks for Large-Scale
    Image Recognition](https://arxiv.org/abs/1409.1556) (2015), Published as a conference
    paper at ICLR 2015'
  prefs: []
  type: TYPE_NORMAL
- en: '[4] Z. Matthew and F. Rob. [Visualizing and Understanding Convolutional Networks](https://arxiv.org/abs/1311.2901)
    (2013), European Conference on Computer Vision'
  prefs: []
  type: TYPE_NORMAL
- en: '[5] B. Jyostna and N. Veeranjaneyulu, [Feature Extraction and Classification
    Using Deep Convolutional Neural Networks](https://journals.riverpublishers.com/index.php/JCSANDM/article/view/5341)
    (2018), Journal of Cyber Security and Mobility'
  prefs: []
  type: TYPE_NORMAL
