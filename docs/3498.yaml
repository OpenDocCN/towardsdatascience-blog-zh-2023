- en: Exploring Feature Extraction with CNNs
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索 CNNs 中的特征提取
- en: 原文：[https://towardsdatascience.com/exploring-feature-extraction-with-cnns-345125cefc9a?source=collection_archive---------0-----------------------#2023-11-25](https://towardsdatascience.com/exploring-feature-extraction-with-cnns-345125cefc9a?source=collection_archive---------0-----------------------#2023-11-25)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/exploring-feature-extraction-with-cnns-345125cefc9a?source=collection_archive---------0-----------------------#2023-11-25](https://towardsdatascience.com/exploring-feature-extraction-with-cnns-345125cefc9a?source=collection_archive---------0-----------------------#2023-11-25)
- en: Using a Convolutional Neural Network to check specialization in feature extraction
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用卷积神经网络检查特征提取中的专业化
- en: '[](https://medium.com/@rodrigopesilva?source=post_page-----345125cefc9a--------------------------------)[![Rodrigo
    Silva](../Images/d260f05ed9887c5072e0590db1481be2.png)](https://medium.com/@rodrigopesilva?source=post_page-----345125cefc9a--------------------------------)[](https://towardsdatascience.com/?source=post_page-----345125cefc9a--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----345125cefc9a--------------------------------)
    [Rodrigo Silva](https://medium.com/@rodrigopesilva?source=post_page-----345125cefc9a--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@rodrigopesilva?source=post_page-----345125cefc9a--------------------------------)[![Rodrigo
    Silva](../Images/d260f05ed9887c5072e0590db1481be2.png)](https://medium.com/@rodrigopesilva?source=post_page-----345125cefc9a--------------------------------)[](https://towardsdatascience.com/?source=post_page-----345125cefc9a--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----345125cefc9a--------------------------------)
    [Rodrigo Silva](https://medium.com/@rodrigopesilva?source=post_page-----345125cefc9a--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F222e82adf972&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fexploring-feature-extraction-with-cnns-345125cefc9a&user=Rodrigo+Silva&userId=222e82adf972&source=post_page-222e82adf972----345125cefc9a---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----345125cefc9a--------------------------------)
    ·8 min read·Nov 25, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F345125cefc9a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fexploring-feature-extraction-with-cnns-345125cefc9a&user=Rodrigo+Silva&userId=222e82adf972&source=-----345125cefc9a---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F222e82adf972&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fexploring-feature-extraction-with-cnns-345125cefc9a&user=Rodrigo+Silva&userId=222e82adf972&source=post_page-222e82adf972----345125cefc9a---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----345125cefc9a--------------------------------)
    ·8 min read·2023年11月25日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F345125cefc9a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fexploring-feature-extraction-with-cnns-345125cefc9a&user=Rodrigo+Silva&userId=222e82adf972&source=-----345125cefc9a---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F345125cefc9a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fexploring-feature-extraction-with-cnns-345125cefc9a&source=-----345125cefc9a---------------------bookmark_footer-----------)![](../Images/a922079bb33abb945ff04ccc0fd86159.png)![](../Images/b37cab9a98460c70520e728500584f40.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F345125cefc9a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fexploring-feature-extraction-with-cnns-345125cefc9a&source=-----345125cefc9a---------------------bookmark_footer-----------)![](../Images/a922079bb33abb945ff04ccc0fd86159.png)![](../Images/b37cab9a98460c70520e728500584f40.png)'
- en: (**Left**) Feature extraction performed over the image of a lion using vgg19
    CNN architecture (image by author). (**Right**) Original picture of the lion (public
    domain, availabe at [Pexels](https://www.pexels.com/pt-br/foto/africa-africano-animais-selvagens-animal-41315/)).
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: (**左**) 使用 vgg19 CNN 架构对狮子图像进行的特征提取（图像由作者提供）。 (**右**) 狮子的原始照片（公共领域，来源于 [Pexels](https://www.pexels.com/pt-br/foto/africa-africano-animais-selvagens-animal-41315/)）。
- en: Convolutional Neural Networks are today’s building blocks for image classification
    tasks using machine learning. However, another very useful task they perform before
    classification is to extract relevant features from an image. Feature extraction
    is the way CNNs recognize key patterns of an image in order to classify it. This
    article will show an example of how to perform feature extractions using TensorFlow
    and the Keras functional API. But first, in order to formalize these CNN concepts,
    we need to talk first about pixel space.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积神经网络（Convolutional Neural Networks，CNN）是当今利用机器学习进行图像分类任务的基石。然而，在分类之前，它们执行的另一个非常有用的任务是从图像中提取相关特征。特征提取是CNN识别图像关键模式的方式，以便对其进行分类。本文将展示如何使用TensorFlow和Keras函数API执行特征提取的示例。但首先，为了正式化这些CNN概念，我们需要先讨论像素空间。
- en: Background
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 背景
- en: Pixel space
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 像素空间
- en: 'Pixel space is exactly what the name suggests: it is the space where the image
    gets converted into a matrix of values, where each value corresponds to an individual
    pixel. Therefore, the original image that we see, when fed into the CNN, gets
    converted into a matrix of numbers. In grayscale images, these numbers typically
    range from 0 (black) to 255 (white), and values in-between are shades of gray.
    In this article, all images have been normalized, that is, every pixel has been
    divided by 255 so its value lies in the interval [0, 1].'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 像素空间正是其名字所暗示的：它是将图像转换为数值矩阵的空间，其中每个值对应一个单独的像素。因此，我们看到的原始图像，在输入CNN时，被转换为一个数字矩阵。在灰度图像中，这些数字通常范围从0（黑色）到255（白色），中间值是灰度。在本文中，所有图像都已经过归一化处理，即每个像素都被255除以，使其值位于[0,
    1]区间内。
- en: CNN and pixel space
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: CNN和像素空间
- en: What a CNN does with the image in pixel representation is to apply filters and
    to process it in order to extract relevant pixels to make the final “decision”,
    which is to put that image within a class. For example, in the image at the top
    of the page, that CNN paid a lot of attention to the lion’s mouth, tongue, eyes
    (and strong contours in general), and these features are further extracted as
    we go deeper into the neural network. Therefore, it suffices to say that, the
    more specialized a CNN is in terms of classification, the more professional it
    is in recognizing key features of an image.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: CNN在像素表示中对图像所做的事情是应用滤波器并处理它，以提取相关像素，从而做出最终的“决策”，即将该图像归入某个类别。例如，在页面顶部的图像中，该CNN特别关注狮子的嘴、舌头、眼睛（以及总体上的强轮廓），随着我们深入神经网络，这些特征进一步被提取。因此，可以说，CNN在分类方面越专业，它在识别图像关键特征方面就越好。
- en: The goal
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 目标
- en: 'With that being said, the goal is simple: to see the level of specialization
    of a CNN when it comes to feature extraction.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 说到这一点，目标很简单：看看CNN在进行特征提取时的专业程度。
- en: The Method
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 方法
- en: 'To do this, I trained two CNNs with the same architecture, but with different
    training sizes: one with 50K images (this is the *benchmark*, the smart one),
    and the other with 10K images (this is the *dummy* one). After that, I sliced
    up the layers of the CNN to check what the algorithm is seeing and the sense it
    makes of the image fed into it.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，我使用了相同架构但训练大小不同的两个CNN：一个使用了50K张图像（这是*基准*，智能的那一个），另一个使用了10K张图像（这是*虚拟的*）。之后，我切割了CNN的层，以检查算法看到的图像及其理解。
- en: Dataset
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据集
- en: The dataset used for this project was the widely used [cifar10](https://keras.io/api/datasets/cifar10/)
    image dataset [1], a public domain dataset, which is a 60K image base divided
    into 10 classes, of which 10K images are used as hold-out validation set. The
    images are 32x32 pixels in size, and they are RGB-colored, which means 3 color
    channels.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 本项目使用的数据集是广泛使用的[cifar10](https://keras.io/api/datasets/cifar10/)图像数据集[1]，这是一个公共领域数据集，包含60K张图像，分为10个类别，其中10K张图像用作保留验证集。这些图像大小为32x32像素，是RGB彩色图像，即有3个颜色通道。
- en: 'In order to prevent data leak, I kept one image to use as a test image in feature
    recognition, therefore this image was not used in either of the trainings. I present
    you our guinea pig: the frog.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 为了防止数据泄漏，我保留了一张图像作为特征识别的测试图像，因此该图像未在任何训练中使用。现在，我向大家介绍我们的小白鼠：青蛙。
- en: '![](../Images/7848cfb5764c67c4872f6e6b80a5455d.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7848cfb5764c67c4872f6e6b80a5455d.png)'
- en: The frog.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 青蛙。
- en: Implementation in TensorFlow
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在TensorFlow中的实现
- en: The implementation is shown in the code snippet below. To properly slice the
    layers of the CNN it is necessary to use the Keras functional API in TensorFlow
    instead of the Sequential API. It works as a cascade, where the next layer is
    called over the last one.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 实现代码见下方代码片段。为了正确切片CNN的层，必须使用TensorFlow中的Keras功能API而不是顺序API。它像级联一样工作，下一层调用在上一层之上。
- en: '[PRE0]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The specs of the architecture are shown below in Fig. 1.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 架构规格如下图1所示。
- en: '![](../Images/d4c391b63dc5594ae120d213fc2c0e44.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d4c391b63dc5594ae120d213fc2c0e44.png)'
- en: 'Fig. 1: Summary of the CNN used. Image by author.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：使用的CNN概述。图像由作者提供。
- en: The optimiser used is Adam, the loss function was categorical cross-entropy,
    and the metric used for evaluation was simply accuracy, since the dataset is perfectly
    balanced.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 使用的优化器是Adam，损失函数是类别交叉熵，用于评估的指标仅仅是准确度，因为数据集完全平衡。
- en: Slicing the CNNs
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 切片CNN
- en: 'Now we can slice some strategical layers of the two CNNs in order to check
    the processing level of the images. The code implementation is shown below:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以切片两个CNN的一些关键层，以检查图像的处理水平。代码实现如下：
- en: '[PRE1]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'What is happening here is as follows: the first line access each layer of the
    model and the second line returns the input layer of the whole CNN. Then in the
    third line we make a list showing the outputs of each layer, and finally, we create
    a new model, whose outputs are the outputs of the layers. This way we can take
    a look at what is happening in-between layers.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 这里发生的情况如下：第一行访问模型的每一层，第二行返回整个CNN的输入层。然后在第三行，我们列出每一层的输出，最后我们创建一个新模型，其输出是这些层的输出。这样我们可以查看层之间发生了什么。
- en: A very similar code was written to access the layers of our dummy model, so
    it will be omitted here. Now let’s proceed to look at the images of our frog,
    processed within different layers of our CNNs.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 编写了非常相似的代码来访问我们虚拟模型的层，因此在此省略。现在让我们继续查看在不同层处理后的青蛙图像。
- en: First convolutional layer
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第一层卷积层
- en: '***Dummy***'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '***虚拟***'
- en: Fig. 2 shows the images of the 16 filters of the convolutional layer (conv2d_1).
    We can see that the images are not super processed, and there is a lot of redundancy.
    One could argue that this is only the first convolutional layer, which accounts
    for the fact that the processing isn’t so heavy, and that’s a fair observation.
    In order to tackle this, we shall look at the first layer of the benchmark.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 图2显示了卷积层（conv2d_1）的16个滤波器的图像。我们可以看到这些图像并未被过度处理，而且有很多冗余。有人可能会认为这只是第一层卷积层，这也是处理不那么复杂的原因，这确实是一个合理的观察。为了解决这个问题，我们将查看基准测试的第一层。
- en: '![](../Images/2f0315aa8278c50a79032051a500ceb6.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2f0315aa8278c50a79032051a500ceb6.png)'
- en: 'Fig. 2: First convolutional layer of the dummy classifier. Image by author.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：虚拟分类器的第一层卷积层。图像由作者提供。
- en: '***Benchmark***'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '***基准测试***'
- en: 'The benchmark classifier shows a much more processed image, to the point where
    most of these images are not recognizable anymore. Remember: this is only the
    first convolutional layer.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 基准测试分类器显示了一个经过处理的图像，处理得更加彻底，以至于这些图像大部分已经无法识别。请记住：这只是第一层卷积层。
- en: '![](../Images/dbf27ae108e871c0d24006df6b3ac155.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/dbf27ae108e871c0d24006df6b3ac155.png)'
- en: 'Fig. 3: First convolutional layer of the benchmark classifier. Image by author.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：基准测试分类器的第一层卷积层。图像由作者提供。
- en: Last convolutional layer
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 最后一层卷积层
- en: '***Dummy***'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '***虚拟***'
- en: As expected the image is not recognizable anymore, since we have gone through
    6 convolutional layers at this point, and 2 pooling layers, which explains the
    lower dimensions of the images. Let’s see what the last layer of the benchmark
    looks like.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 正如预期的那样，图像现在已经无法识别，因为我们已经经过了6层卷积层和2层池化层，这解释了图像尺寸的减小。让我们看看基准测试的最后一层是什么样的。
- en: '![](../Images/64db99c5f1fb96533bab7541f60a7917.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/64db99c5f1fb96533bab7541f60a7917.png)'
- en: 'Fig. 4: Last convolutional layer of the dummy classifier. Image by author.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：虚拟分类器的最后一层卷积层。图像由作者提供。
- en: '***Benchmark***'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '***基准测试***'
- en: This is even more processed, to the point where the majority of pixels are black,
    which shows that the important features were selected, and the rest of the image
    is basically thrown away.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 这已经处理得更为彻底，大部分像素变成了黑色，这表明重要的特征已被选出，其余的图像基本上被丢弃了。
- en: '![](../Images/69c94f6b4d741d94c41d91738863d0c6.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/69c94f6b4d741d94c41d91738863d0c6.png)'
- en: 'Fig. 5: Last convolutional layer of benchmark classifier. Image by author.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：基准测试分类器的最后一层卷积层。图像由作者提供。
- en: How this ties with Information?
  id: totrans-54
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 这与信息有何关联？
- en: 'We can see that the processing degrees are very different for the same slice
    of the network. Qualitative analysis indicates that the benchmark model is more
    aggressive when it comes to extracting useful information from the input. This
    is particularly evident from the first convolutional layer comparison: the frog
    image output is much less distorted and much more recognizable on the dummy than
    it is on the benchmark model.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，对于网络的相同切片，处理程度差异很大。定性分析表明，基准模型在从输入中提取有用信息时更加激进。这在第一次卷积层比较中尤为明显：青蛙图像的输出在虚拟模型上扭曲较少，更加可识别，而在基准模型上则较为模糊。
- en: This suggests that the benchmark is more efficient at throwing away elements
    of the image that are not useful when predicting the class, while the dummy classifier,
    uncertain of how to proceed, considers more features. We can see in Fig. 6 that
    the benchmark (in blue) throws away more color pixels than the dummy model (in
    red), which shows a longer tail in its distribution of colors.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 这表明基准在预测类别时，能够更有效地丢弃图像中无用的元素，而虚拟分类器则对如何进行处理不确定，因此考虑了更多的特征。从图 6 中我们可以看到，基准（蓝色）丢弃的颜色像素比虚拟模型（红色）更多，后者在其颜色分布中显示出更长的尾部。
- en: '![](../Images/8edcd5cc82e055a8f94bdc9f5b6ec445.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8edcd5cc82e055a8f94bdc9f5b6ec445.png)'
- en: 'Fig. 6: Probability distribution for the pixels in the last layer. We can see
    that the benchmark´s pixels (in blue) are more squeezed towards zero, while the
    dummy mode´sl pixels (in red) show a longer tail.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6：最后一层的像素概率分布。我们可以看到基准的像素（蓝色）更集中在零附近，而虚拟模型的像素（红色）显示出更长的尾部。
- en: If we take a look at the pixel distribution of our original frog image, we have
    Fig. 7, which shows a much more symmetric distribution, centered more or less
    around 0.4.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们查看原始青蛙图像的像素分布，图 7 显示了一个更对称的分布，大致集中在0.4左右。
- en: '![](../Images/7d09e89780f1d6bda77dec70a9a27b49.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7d09e89780f1d6bda77dec70a9a27b49.png)'
- en: 'Fig. 7: Color distribution of our original frog image. Image by author.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7：我们原始青蛙图像的颜色分布。图像由作者提供。
- en: From an information theory standpoint, the differences in the probability distributions
    from the original image and the resulting images after the convolutional layers
    represent a massive information gain.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 从信息理论的角度来看，原始图像和卷积层后得到的图像之间概率分布的差异代表了巨大的信息增益。
- en: Looking at Fig. 6 and comparing it with Fig. 7, we are much more certain of
    which pixels we are going to find in the former than in the latter. Hence, there
    is a gain of information. This is a very brief and qualitative exploration of
    Information Theory and opens up a doorway to a vast area. For more information
    about Information (pun intended), see this [post](https://medium.com/towards-data-science/neural-network-via-information-68af7f49b978).
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 从图 6 和图 7 比较来看，我们对前者中将要找到哪些像素比对后者中的像素更为确定。因此，信息获得有所增加。这是对信息理论的非常简要和定性探讨，并为广阔的领域开启了一扇门。有关信息（带有双关语）的更多信息，请参见[这篇文章](https://medium.com/towards-data-science/neural-network-via-information-68af7f49b978)。
- en: And finally, a way to look at the uncertainty in the answer of the classifiers
    is to look at the probability distribution over the classes. This is the output
    of the sofmax function, at the end of our CNN. Fig. 8(left) shows that the benchmark
    is much more certain of the class, with a distribution peaked on the frog class;
    while Fig. 8(right) shows a confused dummy classifier, with the highest probability
    in the wrong class.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，查看分类器答案的不确定性的一种方法是观察类别的概率分布。这是我们CNN末尾的softmax函数的输出。图 8（左）显示基准对类别的确定性更高，分布在青蛙类别上峰值；而图
    8（右）显示了一个困惑的虚拟分类器，最高概率出现在错误的类别上。
- en: '![](../Images/5393621906d0a791da424f130adb204a.png)![](../Images/4e5c10e6837620ee0aa0e9c463fcd0f6.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5393621906d0a791da424f130adb204a.png)![](../Images/4e5c10e6837620ee0aa0e9c463fcd0f6.png)'
- en: 'Fig. 8: Probability distributions assigned by each classifier. Image by author.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8：每个分类器分配的概率分布。图像由作者提供。
- en: 'This analysis shows an expected outcome: bigger training sets result in better
    feature capturing algorithms.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 这一分析展示了预期的结果：更大的训练集导致更好的特征捕捉算法。
- en: The notebook for this project is available [here](https://github.com/rodrigops123/ML_projects/blob/main/Feature_extraction.ipynb).
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 本项目的笔记本可在[这里](https://github.com/rodrigops123/ML_projects/blob/main/Feature_extraction.ipynb)找到。
- en: References
  id: totrans-69
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] Krizhevsky, Alex, and Geoffrey Hinton. [Learning multiple layers of features
    from tiny images](https://www.cs.toronto.edu/~kriz/learning-features-2009-TR.pdf).
    (2009): 7.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] Krizhevsky, Alex 和 Geoffrey Hinton。[从小图像中学习多个特征层](https://www.cs.toronto.edu/~kriz/learning-features-2009-TR.pdf)。（2009年）：7。'
- en: '[2] M. Jogin, et al., [Feature Extraction using Convolution Neural Networks
    (CNN) and Deep Learning](https://ieeexplore.ieee.org/document/9012507) (2018),
    IEEE International Conference on Recent Trends in Electronics, Information & Communication
    Technology (RTEICT)'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] M. Jogin 等人，[使用卷积神经网络（CNN）和深度学习的特征提取](https://ieeexplore.ieee.org/document/9012507)（2018年），IEEE国际电子信息与通信技术近期趋势会议（RTEICT）'
- en: '[3] K. Simonyan, A. Zisserman, [Very Deep Convolution Networks for Large-Scale
    Image Recognition](https://arxiv.org/abs/1409.1556) (2015), Published as a conference
    paper at ICLR 2015'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] K. Simonyan 和 A. Zisserman，[用于大规模图像识别的深度卷积网络](https://arxiv.org/abs/1409.1556)（2015年），作为会议论文发表在ICLR
    2015'
- en: '[4] Z. Matthew and F. Rob. [Visualizing and Understanding Convolutional Networks](https://arxiv.org/abs/1311.2901)
    (2013), European Conference on Computer Vision'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] Z. Matthew 和 F. Rob。[可视化与理解卷积网络](https://arxiv.org/abs/1311.2901)（2013年），欧洲计算机视觉大会'
- en: '[5] B. Jyostna and N. Veeranjaneyulu, [Feature Extraction and Classification
    Using Deep Convolutional Neural Networks](https://journals.riverpublishers.com/index.php/JCSANDM/article/view/5341)
    (2018), Journal of Cyber Security and Mobility'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] B. Jyostna 和 N. Veeranjaneyulu，[使用深度卷积神经网络的特征提取与分类](https://journals.riverpublishers.com/index.php/JCSANDM/article/view/5341)（2018年），《网络安全与移动性期刊》'
