# ç°ä»£æ•°æ®å·¥ç¨‹

> åŸæ–‡ï¼š[`towardsdatascience.com/modern-data-engineering-e202776fb9a9`](https://towardsdatascience.com/modern-data-engineering-e202776fb9a9)

## å¹³å°ç‰¹å®šå·¥å…·å’Œé«˜çº§æŠ€æœ¯

[](https://mshakhomirov.medium.com/?source=post_page-----e202776fb9a9--------------------------------)![ğŸ’¡Mike Shakhomirov](https://mshakhomirov.medium.com/?source=post_page-----e202776fb9a9--------------------------------)[](https://towardsdatascience.com/?source=post_page-----e202776fb9a9--------------------------------)![Towards Data Science](https://towardsdatascience.com/?source=post_page-----e202776fb9a9--------------------------------) [ğŸ’¡Mike Shakhomirov](https://mshakhomirov.medium.com/?source=post_page-----e202776fb9a9--------------------------------)

Â·å‘å¸ƒäº [Towards Data Science](https://towardsdatascience.com/?source=post_page-----e202776fb9a9--------------------------------) Â·é˜…è¯»æ—¶é—´ 12 åˆ†é’ŸÂ·2023 å¹´ 11 æœˆ 4 æ—¥

--

![](img/c4dcaf0e3ae7f465c2b949ddc1ac7edf.png)

å›¾ç‰‡ç”± [Christopher Burns](https://unsplash.com/@christopher__burns?utm_source=medium&utm_medium=referral) æä¾›ï¼Œæ¥æºäº [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)

ç°ä»£æ•°æ®ç”Ÿæ€ç³»ç»Ÿä¸æ–­æ¼”å˜ï¼Œæ–°æ•°æ®å·¥å…·æ—¶ä¸æ—¶å‡ºç°ã€‚åœ¨è¿™ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘æƒ³è°ˆè°ˆå½±å“æ•°æ®å·¥ç¨‹å¸ˆçš„å…³é”®å› ç´ ã€‚æˆ‘ä»¬å°†è®¨è®ºå¦‚ä½•åˆ©ç”¨è¿™äº›çŸ¥è¯†æ¥æ¨åŠ¨å…ˆè¿›çš„åˆ†æç®¡é“å’Œå“è¶Šçš„è¿è¥ã€‚

## æˆ‘æƒ³è®¨è®ºä¸€äº›æµè¡Œçš„æ•°æ®å·¥ç¨‹é—®é¢˜ï¼š

+   ç°ä»£æ•°æ®å·¥ç¨‹ï¼ˆDEï¼‰ã€‚å®ƒæ˜¯ä»€ä¹ˆï¼Ÿ

+   ä½ çš„æ•°æ®å·¥ç¨‹ï¼ˆDEï¼‰æ˜¯å¦è¶³å¤Ÿå¥½ï¼Œèƒ½ä¸ºå…ˆè¿›çš„æ•°æ®ç®¡é“å’Œå•†ä¸šæ™ºèƒ½ï¼ˆBIï¼‰æä¾›æ”¯æŒï¼Ÿ

+   ä½ çš„æ•°æ®ç®¡é“æ˜¯å¦é«˜æ•ˆï¼Ÿ

+   ä»æŠ€æœ¯è§’åº¦æ¥çœ‹ï¼Œå®ç°å“è¶Šè¿è¥éœ€è¦ä»€ä¹ˆï¼Ÿ

åœ¨åæœˆä»½ï¼Œæˆ‘å†™äº†å…³äºæ•°æ®å·¥ç¨‹å¸ˆçš„å´›èµ·ã€è§’è‰²ã€æŒ‘æˆ˜ã€èŒè´£ã€æ—¥å¸¸å·¥ä½œä»¥åŠå¦‚ä½•åœ¨è¿™ä¸ªé¢†åŸŸå–å¾—æˆåŠŸã€‚æ•°æ®å·¥ç¨‹é¢†åŸŸåœ¨ä¸æ–­å˜åŒ–ï¼Œä½†ä¸»è¦è¶‹åŠ¿ä¼¼ä¹ä¿æŒä¸å˜ã€‚

[](/how-to-become-a-data-engineer-c0319cb226c2?source=post_page-----e202776fb9a9--------------------------------) ## å¦‚ä½•æˆä¸ºæ•°æ®å·¥ç¨‹å¸ˆ

### 2024 å¹´åˆå­¦è€…çš„æ·å¾„

towardsdatascience.com

ä½œä¸ºä¸€åæ•°æ®å·¥ç¨‹å¸ˆï¼Œæˆ‘å‡ ä¹æ¯å¤©éƒ½éœ€è¦è®¾è®¡é«˜æ•ˆçš„æ•°æ®æµç¨‹ã€‚å› æ­¤ï¼Œè¿™é‡Œæœ‰ä¸€äº›éœ€è¦è€ƒè™‘çš„äº‹é¡¹ï¼Œå¯ä»¥å¸®åŠ©æˆ‘ä»¬è§£ç­”è¿™äº›é—®é¢˜ã€‚

## ç°ä»£æ•°æ®å·¥ç¨‹è¶‹åŠ¿

+   ETL ä¸ ELT

+   ç®€åŒ–çš„æ•°æ®è¿æ¥å™¨å’Œ API é›†æˆ

+   ETL æ¡†æ¶çš„çˆ†ç‚¸æ€§å¢é•¿

+   æ•°æ®åŸºç¡€è®¾æ–½å³ä»£ç 

+   æ•°æ®ç½‘æ ¼ä¸å»ä¸­å¿ƒåŒ–æ•°æ®ç®¡ç†

+   åˆ©ç”¨ AI è¿›è¡Œå•†ä¸šæ™ºèƒ½ç®¡é“çš„æ°‘ä¸»åŒ–

+   å…³æ³¨æ•°æ®ç´ å…»

## ELT ä¸ ETL

å—æ¬¢è¿çš„ SQL æ•°æ®è½¬æ¢å·¥å…·ï¼Œå¦‚**Dataform**å’Œ**DBT**ï¼Œå¯¹ ELT æ–¹æ³•çš„æ™®åŠåšå‡ºäº†é‡è¦è´¡çŒ®[1]ã€‚åœ¨å­˜å‚¨æ•°æ®çš„åœ°æ–¹æ‰§è¡Œæ‰€éœ€çš„æ•°æ®è½¬æ¢ï¼Œå¦‚æ¸…æ´—ã€ä¸°å¯Œå’Œæå–ï¼Œæ˜¾å¾—éå¸¸åˆç†ã€‚é€šå¸¸ï¼Œè¿™æ˜¯ä¸€ç§ä½äºæˆ‘ä»¬åŸºç¡€è®¾æ–½ä¸­å¿ƒçš„æ•°æ®ä»“åº“è§£å†³æ–¹æ¡ˆï¼ˆDWHï¼‰ã€‚äº‘å¹³å°é¢†å¯¼è€…ä½¿ DWHï¼ˆSnowflakeã€BigQueryã€Redshiftã€Fireboltï¼‰çš„åŸºç¡€è®¾æ–½ç®¡ç†å˜å¾—éå¸¸ç®€å•ï¼Œåœ¨è®¸å¤šåœºæ™¯ä¸­ï¼Œå®ƒä»¬åœ¨æˆæœ¬æ•ˆç›Šå’Œé€Ÿåº¦æ–¹é¢å°†ä¼˜äºä¸“é—¨çš„å†…éƒ¨åŸºç¡€è®¾æ–½ç®¡ç†å›¢é˜Ÿã€‚

![](img/d4fbc3d51212c640c74e34cc21f6baff.png)

æ•°æ®ä»“åº“ç¤ºä¾‹ã€‚ä½œè€…æä¾›çš„å›¾ç‰‡

å®ƒä¹Ÿå¯èƒ½æ˜¯ä¸­å¿ƒçš„æ•°æ®æ¹–ï¼Œè¿™å–å†³äºæˆ‘ä»¬çš„æ•°æ®å¹³å°ç±»å‹å’Œä½¿ç”¨çš„å·¥å…·ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼ŒSQL åœ¨è®¸å¤šæƒ…å†µä¸‹ä¸å†æ˜¯ä¸€ä¸ªé€‰é¡¹ï¼Œä½¿å¾—é‚£äº›ä¸ç†Ÿæ‚‰ç¼–ç¨‹çš„ç”¨æˆ·éš¾ä»¥æŸ¥è¯¢æ•°æ®ã€‚åƒ Databricksã€Tabular å’Œ Galaxy è¿™æ ·çš„å·¥å…·è¯•å›¾è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæ„Ÿè§‰å®ƒç¡®å®æ˜¯æœªæ¥çš„å‘å±•æ–¹å‘ã€‚ç¡®å®ï¼Œæ•°æ®æ¹–å¯ä»¥å­˜å‚¨æ‰€æœ‰ç±»å‹çš„æ•°æ®ï¼ŒåŒ…æ‹¬éç»“æ„åŒ–æ•°æ®ï¼Œæˆ‘ä»¬ä»ç„¶éœ€è¦èƒ½å¤Ÿåˆ†æè¿™äº›æ•°æ®é›†ã€‚

![](img/121483bb3ae93ecd7fd90f7d796c3cb0.png)

æ•°æ®æ¹–ç¤ºä¾‹ã€‚ä½œè€…æä¾›çš„å›¾ç‰‡ã€‚

> æƒ³è±¡ä¸€ä¸‹å…·æœ‰äº‹åŠ¡ä¸€è‡´æ€§çš„ æ•°æ®æ¹–è¡¨å’Œæ—¶é—´ç‚¹å¿«ç…§éš”ç¦»ã€‚

æˆ‘ä¹‹å‰åœ¨å…³äº Apache Iceberg è¡¨æ ¼å¼çš„æ•…äº‹ä¸­å†™è¿‡è¿™æ–¹é¢çš„å†…å®¹[2]ã€‚

[](/introduction-to-apache-iceberg-tables-a791f1758009?source=post_page-----e202776fb9a9--------------------------------) ## Apache Iceberg è¡¨ä»‹ç»

### é€‰æ‹© Apache Iceberg ä½œä¸ºæ•°æ®æ¹–çš„å‡ ä¸ªæœ‰åŠ›ç†ç”±

towardsdatascience.com

## ç®€åŒ–çš„æ•°æ®é›†æˆ

åƒ**Fivetran**å’Œ**Stitch**è¿™æ ·çš„æ‰˜ç®¡è§£å†³æ–¹æ¡ˆè¢«åˆ›å»ºä»¥è½»æ¾ç®¡ç†ç¬¬ä¸‰æ–¹ API é›†æˆã€‚å¦‚ä»Šï¼Œè®¸å¤šå…¬å¸é€‰æ‹©è¿™ç§æ–¹æ³•æ¥ç®€åŒ–ä¸å¤–éƒ¨æ•°æ®æºçš„äº’åŠ¨ã€‚è¿™å°†æ˜¯æ•°æ®åˆ†æå›¢é˜Ÿä¸­ä¸ç†Ÿæ‚‰ç¼–ç çš„äººå‘˜çš„æ­£ç¡®é€‰æ‹©ã€‚

> ç¡®å®ï¼Œå¦‚æœæ•°æ®è¿æ¥å™¨å·²ç»å­˜åœ¨å¹¶ä¸”åœ¨äº‘ä¸­ç®¡ç†ï¼Œæˆ‘ä»¬ä¸ºä»€ä¹ˆè¿˜è¦ä»å¤´å¼€å§‹æ„å»ºå‘¢ï¼Ÿ
> 
> ä¸è¿‡è¿™ç§æ–¹æ³•çš„ç¼ºç‚¹æ˜¯å…¶å®šä»·æ¨¡å‹ã€‚

å¾ˆå¤šæ—¶å€™å®ƒæ˜¯åŸºäºè¡Œçš„ï¼Œå¹¶ä¸”åœ¨ä¼ä¸šçº§æ•°æ®æ‘„å–ï¼ˆå³å¤§æ•°æ®ç®¡é“ï¼‰ä¸Šå¯èƒ½å˜å¾—ç›¸å½“æ˜‚è´µã€‚è¿™å°±æ˜¯å¼€æºæ›¿ä»£æ–¹æ¡ˆå‘æŒ¥ä½œç”¨çš„åœ°æ–¹ã€‚åƒ**Airbyte**å’Œ**Meltano**è¿™æ ·çš„æ¡†æ¶å¯èƒ½æ˜¯éƒ¨ç½²æ•°æ®æº**é›†æˆ**å¾®æœåŠ¡çš„ç®€å•å¿«æ·è§£å†³æ–¹æ¡ˆã€‚

å¦‚æœä½ æ²¡æœ‰æ—¶é—´å­¦ä¹ æ–°çš„ ETL æ¡†æ¶ï¼Œä½ å¯ä»¥è‡ªå·±åˆ›å»ºä¸€ä¸ªç®€å•çš„æ•°æ®è¿æ¥å™¨ã€‚å¦‚æœä½ æ‡‚ä¸€ç‚¹ Pythonï¼Œè¿™å°†æ˜¯ä¸€ä¸ªå¾®ä¸è¶³é“çš„ä»»åŠ¡ã€‚åœ¨æˆ‘ä¹‹å‰çš„ä¸€ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘å†™äº†å¦‚ä½•è½»æ¾åˆ›å»ºä¸€ä¸ªä» NASA API æ‹‰å–æ•°æ®çš„å¾®æœåŠ¡[3]ï¼š

[](/python-for-data-engineers-f3d5db59b6dd?source=post_page-----e202776fb9a9--------------------------------) ## æ•°æ®å·¥ç¨‹å¸ˆçš„ Python

### åˆå­¦è€…çš„é«˜çº§ ETL æŠ€æœ¯

towardsdatascience.com

è€ƒè™‘`app.py`ä¸­çš„è¿™æ®µä»£ç 

```py
import requests
session = requests.Session()

url="https://api.nasa.gov/neo/rest/v1/feed"
apiKey="your_api_key"
requestParams = {
    'api_key': apiKey,
    'start_date': '2023-04-20',
    'end_date': '2023-04-21'
}
response = session.get(url, params = requestParams, stream=True)
print(response.status_code)
```

å®ƒå¯ä»¥éƒ¨ç½²åœ¨ä»»ä½•äº‘æœåŠ¡å•†å¹³å°ä¸Šï¼Œå¹¶æŒ‰æ‰€éœ€é¢‘ç‡è°ƒåº¦è¿è¡Œã€‚ä½¿ç”¨ç±»ä¼¼**Terraform**çš„å·¥å…·æ¥éƒ¨ç½²æˆ‘ä»¬çš„æ•°æ®ç®¡é“åº”ç”¨ç¨‹åºå§‹ç»ˆæ˜¯ä¸€ä¸ªå¥½çš„å®è·µã€‚

## ETL æ¡†æ¶çš„çˆ†ç‚¸æ€§å¢é•¿

æˆ‘ä»¬å¯ä»¥è§è¯å„ç§ ETL æ¡†æ¶åœ¨æ•°æ®æå–å’Œè½¬æ¢ä¸­çš„â€œå¯’æ­¦çºªå¤§çˆ†å‘â€ã€‚è®¸å¤šæ¡†æ¶éƒ½æ˜¯å¼€æºçš„ï¼Œå¹¶ä¸”åŸºäº Pythonï¼Œè¿™å¹¶ä¸ä»¤äººæƒŠè®¶ã€‚

**Luigi** [8]å°±æ˜¯å…¶ä¸­ä¹‹ä¸€ï¼Œå®ƒå¸®åŠ©åˆ›å»º ETL ç®¡é“ã€‚å®ƒç”± Spotify åˆ›å»ºï¼Œç”¨äºç®¡ç†å¤§è§„æ¨¡æ•°æ®å¤„ç†å·¥ä½œè´Ÿè½½ã€‚å®ƒå…·æœ‰å‘½ä»¤è¡Œç•Œé¢å’Œå‡ºè‰²çš„å¯è§†åŒ–åŠŸèƒ½ã€‚ç„¶è€Œï¼Œå³ä½¿æ˜¯åŸºæœ¬çš„ ETL ç®¡é“ä¹Ÿéœ€è¦ä¸€å®šçš„ Python ç¼–ç¨‹æŠ€èƒ½ã€‚ä»æˆ‘çš„ç»éªŒæ¥çœ‹ï¼Œå®ƒé€‚åˆä¸¥æ ¼ä¸”ç›´æ¥çš„ç®¡é“ã€‚æˆ‘å‘ç°ä½¿ç”¨ Luigi å®ç°å¤æ‚çš„åˆ†æ”¯é€»è¾‘ç‰¹åˆ«å›°éš¾ï¼Œä½†å®ƒåœ¨è®¸å¤šåœºæ™¯ä¸­è¡¨ç°è‰¯å¥½ã€‚

**Python ETL (PETL) [9]**æ˜¯æœ€å¹¿æ³›ä½¿ç”¨çš„å¼€æº ETL æ¡†æ¶ä¹‹ä¸€ï¼Œç”¨äºç®€å•çš„æ•°æ®è½¬æ¢ã€‚åœ¨å¤„ç†è¡¨æ ¼ã€ä»å¤–éƒ¨æ•°æ®æºæå–æ•°æ®ä»¥åŠå¯¹æ•°æ®æ‰§è¡ŒåŸºæœ¬ ETL æ—¶ï¼Œå®ƒéå¸¸å®è´µã€‚åœ¨è®¸å¤šæ–¹é¢ï¼Œå®ƒä¸**Pandas**ç±»ä¼¼ï¼Œä½†åè€…åœ¨åå°å…·æœ‰æ›´å¤šçš„åˆ†æåŠŸèƒ½ã€‚PETL éå¸¸é€‚åˆèšåˆå’Œè¡Œçº§ ETLã€‚

**Bonobo** [10]æ˜¯å¦ä¸€ä¸ªå¼€æºçš„è½»é‡çº§æ•°æ®å¤„ç†å·¥å…·ï¼Œéå¸¸é€‚åˆå¿«é€Ÿå¼€å‘ã€è‡ªåŠ¨åŒ–å’Œæ‰¹å¤„ç†æ•°æ®ç®¡é“çš„å¹¶è¡Œæ‰§è¡Œã€‚æˆ‘å–œæ¬¢å®ƒçš„ä¸€ç‚¹æ˜¯ï¼Œå®ƒè®©å¤„ç†å„ç§æ•°æ®æ–‡ä»¶æ ¼å¼å˜å¾—éå¸¸ç®€å•ï¼Œä¾‹å¦‚ SQLã€XMLã€XLSã€CSV å’Œ JSONã€‚å¯¹äºé‚£äº› Python çŸ¥è¯†æœ‰é™çš„äººæ¥è¯´ï¼Œå®ƒå°†æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„å·¥å…·ã€‚åœ¨å…¶ä»–å¥½å¤„ä¸­ï¼Œæˆ‘å–œæ¬¢å®ƒå¯¹åŠå¤æ‚æ•°æ®æ¨¡å¼çš„è‰¯å¥½æ”¯æŒã€‚å®ƒéå¸¸é€‚åˆç®€å•çš„ ETLï¼Œå¹¶ä¸”å¯ä»¥åœ¨ Docker å®¹å™¨ä¸­è¿è¡Œï¼ˆå®ƒæœ‰ä¸€ä¸ª Docker æ‰©å±•ï¼‰ã€‚

**Pandas**åœ¨æ•°æ®é¢†åŸŸä¸­ç»å¯¹æ˜¯ä¸€ä¸ªå·¨å¤´ï¼Œåœ¨è¿™ä¸ªæ•…äº‹ä¸­æ²¡æœ‰å¿…è¦è¯¦ç»†ä»‹ç»å®ƒçš„èƒ½åŠ›ã€‚å€¼å¾—ä¸€æçš„æ˜¯ï¼Œå®ƒçš„æ•°æ®æ¡†æ¶è½¬æ¢å·²è¢«çº³å…¥è®¸å¤šç°ä»£æ•°æ®ä»“åº“çš„åŸºæœ¬æ•°æ®åŠ è½½æ–¹æ³•ä¹‹ä¸€ã€‚è€ƒè™‘å°†æ•°æ®åŠ è½½åˆ° BigQuery æ•°æ®ä»“åº“è§£å†³æ–¹æ¡ˆä¸­çš„ç¤ºä¾‹ï¼š

```py
from google.cloud import bigquery
from google.oauth2 import service_account
...
# Authenticate BigQuery client:
service_acount_str = config.get('BigQuery') # Use config
credentials = service_account.Credentials.from_service_account_info(service_acount_str)
client = bigquery.Client(credentials=credentials, project=credentials.project_id)

...
def load_table_from_dataframe(table_schema, table_name, dataset_id):
    #! source data file format must be outer array JSON:
    """
    [
    {"id":"1"},
    {"id":"2"}
    ]
    """
    blob = """
            [
    {"id":"1","first_name":"John","last_name":"Doe","dob":"1968-01-22","addresses":[{"status":"current","address":"123 First Avenue","city":"Seattle","state":"WA","zip":"11111","numberOfYears":"1"},{"status":"previous","address":"456 Main Street","city":"Portland","state":"OR","zip":"22222","numberOfYears":"5"}]},
    {"id":"2","first_name":"John","last_name":"Doe","dob":"1968-01-22","addresses":[{"status":"current","address":"123 First Avenue","city":"Seattle","state":"WA","zip":"11111","numberOfYears":"1"},{"status":"previous","address":"456 Main Street","city":"Portland","state":"OR","zip":"22222","numberOfYears":"5"}]}
    ]
    """
    body = json.loads(blob)
    print(pandas.__version__)

    table_id = client.dataset(dataset_id).table(table_name)
    job_config = bigquery.LoadJobConfig()
    schema = create_schema_from_yaml(table_schema) 
    job_config.schema = schema

    df = pandas.DataFrame(
    body,
    # In the loaded table, the column order reflects the order of the
    # columns in the DataFrame.
    columns=["id", "first_name","last_name","dob","addresses"],

    )
    df['addresses'] = df.addresses.astype(str)
    df = df[['id','first_name','last_name','dob','addresses']]

    print(df)

    load_job = client.load_table_from_dataframe(
        df,
        table_id,
        job_config=job_config,
    )

    load_job.result()
    print("Job finished.")
```

ä¾‹å¦‚ï¼Œ**Apache Airflow**å¹¶ä¸æ˜¯ä¸€ä¸ª ETL å·¥å…·ï¼Œä½†å®ƒæœ‰åŠ©äºå°†æˆ‘ä»¬çš„ ETL ç®¡é“ç»„ç»‡æˆä¾èµ–å…³ç³»å›¾ï¼ˆDAGsï¼‰çš„å¯è§†åŒ–ï¼Œä»¥æè¿°ä»»åŠ¡ä¹‹é—´çš„å…³ç³»ã€‚å…¸å‹çš„ Airflow æ¶æ„åŒ…æ‹¬åŸºäºå…ƒæ•°æ®çš„è°ƒåº¦å™¨ã€æ‰§è¡Œå™¨ã€å·¥ä½œèŠ‚ç‚¹å’Œä»»åŠ¡ã€‚

ä¾‹å¦‚ï¼Œæˆ‘ä»¬å¯ä»¥åœ¨å°†æ•°æ®å¯¼å‡ºåˆ°äº‘å­˜å‚¨ï¼ˆbq_export_opï¼‰åè¿è¡Œ ml_engine_training_opï¼Œå¹¶ä½¿æ­¤å·¥ä½œæµæ¯æ—¥æˆ–æ¯å‘¨è¿è¡Œã€‚

![](img/137358971d4273ca60da3554c13fd586.png)

ä½¿ç”¨ Airflow è®­ç»ƒ ML æ¨¡å‹ã€‚å›¾åƒç”±ä½œè€…æä¾›ã€‚

è¯·è€ƒè™‘ä¸‹é¢çš„è¿™ä¸ªä¾‹å­ã€‚

> å®ƒåˆ›å»ºäº†ä¸€ä¸ªç®€å•çš„æ•°æ®ç®¡é“å›¾ï¼Œå°†æ•°æ®å¯¼å‡ºåˆ°äº‘å­˜å‚¨æ¡¶ä¸­ï¼Œç„¶åä½¿ç”¨ MLEngineTrainingOperator è®­ç»ƒ ML æ¨¡å‹ã€‚

```py
"""DAG definition for recommendation_bespoke model training."""

import airflow
from airflow import DAG
from airflow.contrib.operators.bigquery_operator import BigQueryOperator
from airflow.contrib.operators.bigquery_to_gcs import BigQueryToCloudStorageOperator
from airflow.hooks.base_hook import BaseHook
from airflow.operators.app_engine_admin_plugin import AppEngineVersionOperator
from airflow.operators.ml_engine_plugin import MLEngineTrainingOperator

import datetime

def _get_project_id():
  """Get project ID from default GCP connection."""

  extras = BaseHook.get_connection('google_cloud_default').extra_dejson
  key = 'extra__google_cloud_platform__project'
  if key in extras:
    project_id = extras[key]
  else:
    raise ('Must configure project_id in google_cloud_default '
           'connection from Airflow Console')
  return project_id

PROJECT_ID = _get_project_id()

# Data set constants, used in BigQuery tasks.  You can change these
# to conform to your data.
DATASET = 'staging' #'analytics'
TABLE_NAME = 'recommendation_bespoke'

# GCS bucket names and region, can also be changed.
BUCKET = 'gs://rec_wals_eu'
REGION = 'us-central1' #'europe-west2' #'us-east1'
JOB_DIR = BUCKET + '/jobs'

default_args = {
    'owner': 'airflow',
    'depends_on_past': False,
    'start_date': airflow.utils.dates.days_ago(2),
    'email': ['mike.shakhomirov@gmail.com'],
    'email_on_failure': True,
    'email_on_retry': False,
    'retries': 5,
    'retry_delay': datetime.timedelta(minutes=5)
}

# Default schedule interval using cronjob syntax - can be customized here
# or in the Airflow console.
schedule_interval = '00 21 * * *'

dag = DAG('recommendations_training_v6', default_args=default_args,
          schedule_interval=schedule_interval)

dag.doc_md = __doc__

#
#
# Task Definition
#
#

# BigQuery training data export to GCS

training_file = BUCKET + '/data/recommendations_small.csv' # just a few records for staging

t1 = BigQueryToCloudStorageOperator(
    task_id='bq_export_op',
    source_project_dataset_table='%s.recommendation_bespoke' % DATASET,
    destination_cloud_storage_uris=[training_file],
    export_format='CSV',
    dag=dag
)

# ML Engine training job
training_file = BUCKET + '/data/recommendations_small.csv'
job_id = 'recserve_{0}'.format(datetime.datetime.now().strftime('%Y%m%d%H%M'))
job_dir = BUCKET + '/jobs/' + job_id
output_dir = BUCKET
delimiter=','
data_type='user_groups'
master_image_uri='gcr.io/my-project/recommendation_bespoke_container:tf_rec_latest'

training_args = ['--job-dir', job_dir,
                 '--train-file', training_file,
                 '--output-dir', output_dir,
                 '--data-type', data_type]

master_config = {"imageUri": master_image_uri,}

t3 = MLEngineTrainingOperator(
    task_id='ml_engine_training_op',
    project_id=PROJECT_ID,
    job_id=job_id,
    training_args=training_args,
    region=REGION,
    scale_tier='CUSTOM',
    master_type='complex_model_m_gpu',
    master_config=master_config,
    dag=dag
)

t3.set_upstream(t1)
```

**Bubbles** [11] æ˜¯å¦ä¸€ä¸ªç”¨äº Python ä¸–ç•Œä¸­ ETL çš„å¼€æºå·¥å…·ã€‚å®ƒéå¸¸é€‚åˆå¿«é€Ÿå¼€å‘ï¼Œæˆ‘å–œæ¬¢å®ƒå¦‚ä½•ä½¿ç”¨å…ƒæ•°æ®æ¥æè¿°æ•°æ®ç®¡é“ã€‚Bubbles çš„åˆ›å»ºè€…ç§°å…¶ä¸ºâ€œæŠ½è±¡æ¡†æ¶â€ï¼Œå¹¶è¡¨ç¤ºå®ƒå¯ä»¥ä»è®¸å¤šå…¶ä»–ç¼–ç¨‹è¯­è¨€ä¸­ä½¿ç”¨ï¼Œè€Œä¸ä»…ä»…æ˜¯ Pythonã€‚

è¿˜æœ‰è®¸å¤šå…¶ä»–å·¥å…·å…·æœ‰æ›´å…·ä½“çš„åº”ç”¨ï¼Œä¾‹å¦‚ä»ç½‘é¡µä¸­æå–æ•°æ®ï¼ˆPyQueryã€BeautifulSoup ç­‰ï¼‰å’Œå¹¶è¡Œæ•°æ®å¤„ç†ã€‚è¿™å¯ä»¥æ˜¯å¦ä¸€ä¸ªè¯é¢˜ï¼Œä½†æˆ‘ä¹‹å‰å†™è¿‡ä¸€äº›ç›¸å…³å†…å®¹ï¼Œä¾‹å¦‚ `joblib` åº“ [12]ã€‚

## æ•°æ®åŸºç¡€è®¾æ–½å³ä»£ç 

**åŸºç¡€è®¾æ–½å³ä»£ç **ï¼ˆIaCï¼‰æ˜¯ä¸€ç§æµè¡Œä¸”éå¸¸å®ç”¨çš„æ–¹æ³•ï¼Œç”¨äº**ç®¡ç†æ•°æ®å¹³å°èµ„æº**ã€‚å³ä½¿æ˜¯æ•°æ®æ–¹é¢ï¼Œç›®å‰è¿™å‡ ä¹å·²ç»æˆä¸ºæ ‡å‡†ï¼Œè€Œä¸”åœ¨ç®€å†ä¸Šæåˆ°ä½ ç†Ÿæ‚‰ DevOps æ ‡å‡†ç»å¯¹ä¼šæ˜¾å¾—å¾ˆæ£’ã€‚ä½¿ç”¨åƒ Terraformï¼ˆå¹³å°æ— å…³ï¼‰å’Œ CloudFormation è¿™æ ·çš„å·¥å…·ï¼Œæˆ‘ä»¬å¯ä»¥è½»æ¾åœ°å°†æˆ‘ä»¬çš„å¼€å‘å·¥ä½œå’Œéƒ¨ç½²ï¼ˆè¿ç»´ï¼‰è¿›è¡Œé›†æˆã€‚

é€šå¸¸ï¼Œæˆ‘ä»¬å¸Œæœ›ä¸ºæ•°æ®ç®¡é“è®¾ç½®æµ‹è¯•ç¯å¢ƒå’Œç”Ÿäº§ç¯å¢ƒã€‚è¿™æœ‰åŠ©äºæµ‹è¯•æˆ‘ä»¬çš„ç®¡é“ï¼Œå¹¶ä¿ƒè¿›å›¢é˜Ÿä¹‹é—´çš„åä½œã€‚

è¯·æŸ¥çœ‹ä¸‹é¢çš„å›¾è¡¨ã€‚å®ƒè§£é‡Šäº†æ•°æ®ç¯å¢ƒçš„å·¥ä½œåŸç†ã€‚

![](img/75b7d0ef1c9e75fbc698ea6d609ab4c1.png)

æ•°æ®ç¯å¢ƒã€‚å›¾åƒç”±ä½œè€…æä¾›ã€‚

æˆ‘ä»¬ç»å¸¸éœ€è¦ä¸€ä¸ªé¢å¤–çš„æ²™ç®±æ¥è¿›è¡Œæµ‹è¯•ï¼Œæˆ–åœ¨ ETL æœåŠ¡è§¦å‘ CI/CD å·¥ä½œæµæ—¶è¿è¡Œæ•°æ®è½¬æ¢å•å…ƒæµ‹è¯•ã€‚

æˆ‘ä¹‹å‰åœ¨è¿™é‡Œå†™è¿‡ç›¸å…³å†…å®¹ï¼š

[](https://levelup.gitconnected.com/infrastructure-as-code-for-beginners-a4e36c805316?source=post_page-----e202776fb9a9--------------------------------) [## åˆå­¦è€…çš„åŸºç¡€è®¾æ–½å³ä»£ç 

### åƒä¸“ä¸šäººå£«ä¸€æ ·ä½¿ç”¨è¿™äº›æ¨¡æ¿éƒ¨ç½²æ•°æ®ç®¡é“

levelup.gitconnected.com](https://levelup.gitconnected.com/infrastructure-as-code-for-beginners-a4e36c805316?source=post_page-----e202776fb9a9--------------------------------)

> ä½¿ç”¨ AWS CloudFormation æ¨¡æ¿æ–‡ä»¶ï¼Œæˆ‘ä»¬å¯ä»¥æè¿°æ‰€éœ€çš„èµ„æºåŠå…¶ä¾èµ–å…³ç³»ï¼Œè¿™æ ·æˆ‘ä»¬å¯ä»¥å°†å®ƒä»¬ä½œä¸ºä¸€ä¸ªæ•´ä½“å †æ ˆä¸€èµ·å¯åŠ¨å’Œé…ç½®ã€‚

å¦‚æœä½ æ˜¯ä¸€ä½**æ•°æ®ä¸“å®¶**ï¼Œè¿™ç§æ–¹æ³•è‚¯å®šä¼šå¸®åŠ©ä½ æ›´å¥½åœ°å¤„ç†ä¸åŒçš„æ•°æ®ç¯å¢ƒï¼Œå¹¶æ›´å¿«é€Ÿã€æ›´ä¸€è‡´åœ°å¤åˆ¶æ•°æ®å¹³å°èµ„æºè€Œä¸å‡ºé”™ã€‚

é—®é¢˜åœ¨äºè®¸å¤šæ•°æ®ä»ä¸šè€…å¯¹ IaC å¹¶ä¸ç†Ÿæ‚‰ï¼Œè¿™åœ¨å¼€å‘è¿‡ç¨‹ä¸­ä¼šäº§ç”Ÿå¾ˆå¤šé”™è¯¯ã€‚

## æ•°æ®ç½‘æ ¼å’Œå»ä¸­å¿ƒåŒ–æ•°æ®ç®¡ç†

æ•°æ®ç©ºé—´åœ¨è¿‡å»åå¹´é‡Œå‘ç”Ÿäº†æ˜¾è‘—å˜åŒ–ï¼Œç°åœ¨æˆ‘ä»¬æœ‰å¾ˆå¤šæ•°æ®å·¥å…·å’Œæ¡†æ¶ã€‚**æ•°æ®ç½‘æ ¼**å®šä¹‰äº†è¿™æ ·ä¸€ç§çŠ¶æ€ï¼šæˆ‘ä»¬æ‹¥æœ‰ä¸åŒçš„æ•°æ®é¢†åŸŸï¼ˆå…¬å¸éƒ¨é—¨ï¼‰ï¼Œæ¯ä¸ªé¢†åŸŸéƒ½æœ‰è‡ªå·±çš„å›¢é˜Ÿå’Œå…±äº«çš„æ•°æ®èµ„æºã€‚æ¯ä¸ªå›¢é˜Ÿéƒ½æœ‰è‡ªå·±çš„ç›®æ ‡ã€å…³é”®ç»©æ•ˆæŒ‡æ ‡ï¼ˆKPIï¼‰ã€æ•°æ®è§’è‰²å’ŒèŒè´£ã€‚

> é•¿æœŸä»¥æ¥ï¼Œæ•°æ®å®˜åƒšä¸»ä¹‰ä¸€ç›´æ˜¯è®¸å¤šå…¬å¸çš„çœŸæ­£ç—›ç‚¹ã€‚

è¿™ç§æ•°æ®å¹³å°ç±»å‹[4]å¯èƒ½çœ‹èµ·æ¥æœ‰äº›æ··ä¹±ï¼Œä½†å®ƒæ—¨åœ¨æˆä¸ºä¸€ä¸ªæˆåŠŸä¸”é«˜æ•ˆçš„é€‰æ‹©ï¼Œå°¤å…¶é€‚ç”¨äºå»ä¸­å¿ƒåŒ–çš„å…¬å¸ï¼Œåœ¨è¿™ç§ç»“æ„ä¸‹ï¼Œä¸åŒçš„å›¢é˜Ÿå¯ä»¥è‡ªè¡Œè®¿é—®è·¨é¢†åŸŸçš„æ•°æ®é›†å¹¶æ‰§è¡Œåˆ†ææˆ– ETL ä»»åŠ¡ã€‚

çš„ç¡®ï¼Œå¦‚æœä½ æ˜¯æ•°æ®åˆ†æå¸ˆè€Œä¸ç†Ÿæ‚‰ Sparkï¼ŒSnowflake å¯èƒ½æ˜¯ä½ æœ€å–œæ¬¢çš„æ•°æ®ä»“åº“è§£å†³æ–¹æ¡ˆã€‚ç„¶è€Œï¼Œå½“ä½ å¯èƒ½å¸Œæœ›åœ¨æ²¡æœ‰æ•°æ®å·¥ç¨‹å¸®åŠ©çš„æƒ…å†µä¸‹è¯»å–æ•°æ®æ¹–æ•°æ®æ—¶ï¼Œé€šå¸¸è¿™ä¼šæ˜¯ä¸€ä¸ªå¾®ä¸è¶³é“çš„é—®é¢˜ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œå¤§é‡çš„æ•°æ®é›†å…ƒæ•°æ®è®°å½•å¯èƒ½éå¸¸æœ‰ç”¨ï¼Œè¿™ä¹Ÿæ˜¯æ•°æ®ç½‘æ ¼å¦‚æ­¤æˆåŠŸçš„åŸå› ã€‚

è¿™ä½¿å¾—ç”¨æˆ·äº†è§£æ•°æ®åŠå…¶æ¥æºï¼Œå¹¶äº†è§£å…¶ä»–å›¢é˜Ÿå¦‚ä½•å……åˆ†åˆ©ç”¨ä»–ä»¬ä¹‹å‰ä¸çŸ¥é“çš„æ•°æ®é›†ã€‚

æœ‰æ—¶æ•°æ®é›†å’Œæ•°æ®æºè¿æ¥å˜å¾—éå¸¸å¤æ‚ï¼Œå› æ­¤å§‹ç»ˆä¿æŒä¸€ä¸ªåŒ…å«å…ƒæ•°æ®å’Œæ•°æ®é›†æè¿°çš„å•ä¸€çœŸå®æ•°æ®ä»“åº“æ˜¯ä¸€ä¸ªå¥½ä¹ æƒ¯ã€‚

åœ¨æˆ‘ä¹‹å‰çš„ä¸€ç¯‡æ•…äº‹[5]ä¸­ï¼Œæˆ‘å†™åˆ°äº† SQL ä½œä¸ºå›¢é˜Ÿå’Œæ•°æ®çš„ç»Ÿä¸€æŸ¥è¯¢è¯­è¨€çš„ä½œç”¨ã€‚ç¡®å®ï¼Œå®ƒå…·æœ‰åˆ†ææ€§ã€è‡ªæè¿°æ€§ï¼Œç”šè‡³å¯ä»¥æ˜¯åŠ¨æ€çš„ï¼Œè¿™ä½¿å¾—å®ƒæˆä¸ºæ‰€æœ‰æ•°æ®ç”¨æˆ·çš„å®Œç¾å·¥å…·ã€‚

> **è¿™å¾€å¾€å˜æˆä¸€å›¢ç³Ÿ**

è¿™ä¸€äº‹å®ä½¿å¾—åŸºäº SQL çš„æ¨¡æ¿å¼•æ“å¦‚ DBTã€Jinja å’Œ Dataform éå¸¸å—æ¬¢è¿ã€‚**è¯•æƒ³ä¸€ä¸‹ï¼Œä½ æ‹¥æœ‰ä¸€ä¸ªç±»ä¼¼ SQL çš„å¹³å°ï¼Œå…¶ä¸­æ‰€æœ‰æ•°æ®é›†åŠå…¶è½¬æ¢éƒ½è¢«å½»åº•æè¿°å’Œå®šä¹‰[6]ã€‚**

![](img/0baeff3ede9d729968e3c27de50bdcaf.png)

Dataform çš„ä¾èµ–å›¾å’Œå…ƒæ•°æ®ã€‚å›¾ç‰‡æ¥æºï¼šä½œè€…ã€‚

ç†è§£æ•°æ®å›¢é˜Ÿå¦‚ä½•ä¸æ•°æ®æºå’Œæ¨¡å¼å…³è”å¯èƒ½æ˜¯ä¸€ä¸ªå¾ˆå¤§çš„æŒ‘æˆ˜ã€‚é€šå¸¸ï¼Œè¿™äº›éƒ½çº ç¼ åœ¨æ•°æ®é›†ä¾èµ–å…³ç³»å’Œ ETL è½¬æ¢çš„å¤æ‚ç½‘ç»œä¸­ã€‚

æ•°æ®å·¥ç¨‹åœ¨æŒ‡å¯¼ã€æå‡æ•°æ®ç´ å…»ä»¥åŠé€šè¿‡æœ€å…ˆè¿›çš„æ•°æ®å¤„ç†æŠ€æœ¯å’Œæœ€ä½³å®è·µæ¥èµ‹èƒ½å…¬å¸å…¶ä»–éƒ¨é—¨æ–¹é¢å‘æŒ¥äº†å…³é”®ä½œç”¨ã€‚

## ä½¿ç”¨ AI è¿›è¡Œå•†ä¸šæ™ºèƒ½ç®¡é“çš„æ°‘ä¸»åŒ–

æ”¹å–„æ•°æ®å¯è®¿é—®æ€§ä¸€ç›´æ˜¯æ•°æ®é¢†åŸŸçš„çƒ­é—¨è¯é¢˜ï¼Œä½†æœ‰è¶£çš„æ˜¯ï¼Œæ•´ä¸ªæ•°æ®ç®¡é“è®¾è®¡è¿‡ç¨‹æ­£å˜å¾—è¶Šæ¥è¶Šå®¹æ˜“è¢«é‚£äº›ä¹‹å‰ä¸ç†Ÿæ‚‰æ•°æ®çš„å›¢é˜Ÿæ‰€æ¥è§¦ã€‚ç°åœ¨ï¼Œå‡ ä¹æ¯ä¸ªéƒ¨é—¨éƒ½å¯ä»¥åˆ©ç”¨å†…ç½®çš„ AI èƒ½åŠ›ï¼Œåœ¨æ•°æ®ä¸Šåˆ›å»ºå¤æ‚çš„å•†ä¸šæ™ºèƒ½è½¬æ¢ã€‚

> ä»–ä»¬æ‰€éœ€è¦çš„åªæ˜¯ç”¨è‡ªå·±çš„è¯æè¿°ä»–ä»¬åœ¨å•†ä¸šæ™ºèƒ½æ–¹é¢çš„éœ€æ±‚ã€‚

ä¾‹å¦‚ï¼Œåƒ**Thoughspot**è¿™æ ·çš„ BI å·¥å…·ä½¿ç”¨å…·æœ‰ç›´è§‚â€œGoogle é£æ ¼æœç´¢ç•Œé¢â€çš„ AI [7] æ¥ä»å­˜å‚¨åœ¨ä»»ä½•ç°ä»£æ•°æ®ä»“åº“è§£å†³æ–¹æ¡ˆï¼ˆå¦‚ Google Big Queryã€Redshiftã€Snowflake æˆ– Databricksï¼‰ä¸­çš„æ•°æ®ä¸­è·å¾—æ´å¯Ÿã€‚

ç°ä»£æ•°æ®æ ˆåŒ…æ‹¬å¸®åŠ©æ•°æ®å»ºæ¨¡å’Œå¯è§†åŒ–çš„ BI å·¥å…·ã€‚è®¸å¤šå·¥å…·å·²ç»å…·å¤‡äº†è¿™äº›å†…ç½®çš„ AI åŠŸèƒ½ï¼Œä»¥æ ¹æ®ç”¨æˆ·è¡Œä¸ºæ›´å¿«åœ°è·å¾—æ•°æ®æ´å¯Ÿã€‚

æˆ‘ç›¸ä¿¡å°† GPT ä¸ BI é›†æˆæ˜¯ä¸€é¡¹ç›¸å½“ç®€å•çš„ä»»åŠ¡ã€‚åœ¨æ¥ä¸‹æ¥çš„å‡ å¹´ä¸­ï¼Œæˆ‘ä»¬å°†çœ‹åˆ°è®¸å¤šæ–°äº§å“ä½¿ç”¨è¿™é¡¹æŠ€æœ¯ã€‚

> GPT å¯ä»¥é¢„å¤„ç†æ–‡æœ¬æ•°æ®ï¼Œä»¥ç”Ÿæˆç†è§£æ‚¨çš„æ„å›¾å¹¶å›ç­”æ‚¨çš„é—®é¢˜çš„ SQL æŸ¥è¯¢ã€‚

## ç»“è®º

åœ¨è¿™ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘è¯•å›¾ç»™å‡ºä¸€ä¸ªå…³äºå½“å‰æ•°æ®å·¥ç¨‹è§’è‰²æ‰€é¢ä¸´çš„ä¸»è¦æ•°æ®è¶‹åŠ¿çš„**é«˜çº§æ¦‚è¿°**ã€‚æ•°æ®ç½‘æ ¼å’Œå¸¦æœ‰ä¾èµ–å…³ç³»å›¾çš„æ¨¡æ¿åŒ– SQL ä½¿æ•°æ®ç´ å…»çš„æ™®åŠæˆä¸ºå¯èƒ½ï¼Œä»è€Œä½¿æ•´ä¸ªåˆ†æè¿‡ç¨‹æ°‘ä¸»åŒ–ã€‚å…ˆè¿›çš„æ•°æ®ç®¡é“ä»¥åŠå¤æ‚çš„ ETL æŠ€æœ¯å’Œè½¬æ¢ç°åœ¨å¯¹ç»„ç»‡ä¸­çš„æ¯ä¸ªäººéƒ½æ˜¯é€æ˜çš„ã€‚æ•°æ®ç®¡é“æ­£å˜å¾—è¶Šæ¥è¶Šæ˜“äºå…¶ä»–å›¢é˜Ÿè®¿é—®ï¼Œä»–ä»¬ä¸éœ€è¦äº†è§£ç¼–ç¨‹å°±èƒ½å­¦ä¹ å’Œç†è§£ ETL çš„å¤æ‚æ€§ã€‚æ•°æ®ç½‘æ ¼å’Œå…ƒæ•°æ®æœ‰åŠ©äºè§£å†³è¿™ä¸ªé—®é¢˜ã€‚ä»æˆ‘çš„ç»éªŒæ¥çœ‹ï¼Œæˆ‘å‘ç°è¶Šæ¥è¶Šå¤šçš„äººå­¦ä¹  SQL ä»¥å‚ä¸è½¬å‹å±‚ã€‚è¯ç”Ÿäºâ€œé«˜çº§æ•°æ®åˆ†æâ€æ—¶ä»£çš„å…¬å¸äº«æœ‰è½»æ¾è®¿é—®äº‘ä¾›åº”å•†äº§å“åŠå…¶æ‰˜ç®¡æœåŠ¡çš„å¥¢ä¾ˆã€‚å®ƒç¡®å®æœ‰åŠ©äºè·å¾—æ‰€éœ€çš„æ•°æ®æŠ€èƒ½å¹¶åŠ ä»¥æå‡ï¼Œä»¥è·å¾—ç«äº‰ä¼˜åŠ¿ã€‚

## æ¨èé˜…è¯»

[1] [`medium.com/towards-data-science/data-pipeline-design-patterns-100afa4b93e3`](https://medium.com/towards-data-science/data-pipeline-design-patterns-100afa4b93e3)

[2] `towardsdatascience.com/introduction-to-apache-iceberg-tables-a791f1758009`

[3] `towardsdatascience.com/python-for-data-engineers-f3d5db59b6dd`

[4] [`medium.com/towards-data-science/data-platform-architecture-types-f255ac6e0b7`](https://medium.com/towards-data-science/data-platform-architecture-types-f255ac6e0b7)

[5] [`medium.com/towards-data-science/advanced-sql-techniques-for-beginners-211851a28488`](https://medium.com/towards-data-science/advanced-sql-techniques-for-beginners-211851a28488)

[6] [`medium.com/towards-data-science/easy-way-to-create-live-and-staging-environments-for-your-data-e4f03eb73365`](https://medium.com/towards-data-science/easy-way-to-create-live-and-staging-environments-for-your-data-e4f03eb73365)

[7] [`docs.thoughtspot.com/cloud/latest/search-sage`](https://docs.thoughtspot.com/cloud/latest/search-sage)

[8] [`github.com/spotify/luigi`](https://github.com/spotify/luigi)

[9] [`petl.readthedocs.io/en/stable/`](https://petl.readthedocs.io/en/stable/)

[10] [`www.bonobo-project.org`](https://www.bonobo-project.org)

[11] [`bubbles.databrewery.org/`](http://bubbles.databrewery.org/)

[12] [`medium.com/towards-data-science/how-to-become-a-data-engineer-c0319cb226c2`](https://medium.com/towards-data-science/how-to-become-a-data-engineer-c0319cb226c2)
