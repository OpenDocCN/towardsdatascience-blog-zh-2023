- en: 'Parameter-Efficient Fine-Tuning (PEFT) for LLMs: A Comprehensive Introduction'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/parameter-efficient-fine-tuning-peft-for-llms-a-comprehensive-introduction-e52d03117f95?source=collection_archive---------1-----------------------#2023-08-22](https://towardsdatascience.com/parameter-efficient-fine-tuning-peft-for-llms-a-comprehensive-introduction-e52d03117f95?source=collection_archive---------1-----------------------#2023-08-22)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: A conceptual survey of PEFT methods used by Hugging Face, Google’s Vertex AI,
    and eventually OpenAI
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@smsmith714?source=post_page-----e52d03117f95--------------------------------)[![Sean
    Smith](../Images/611395d113b10ec4bbfaf781301139c7.png)](https://medium.com/@smsmith714?source=post_page-----e52d03117f95--------------------------------)[](https://towardsdatascience.com/?source=post_page-----e52d03117f95--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----e52d03117f95--------------------------------)
    [Sean Smith](https://medium.com/@smsmith714?source=post_page-----e52d03117f95--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F6957f6523097&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fparameter-efficient-fine-tuning-peft-for-llms-a-comprehensive-introduction-e52d03117f95&user=Sean+Smith&userId=6957f6523097&source=post_page-6957f6523097----e52d03117f95---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----e52d03117f95--------------------------------)
    ·19 min read·Aug 22, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fe52d03117f95&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fparameter-efficient-fine-tuning-peft-for-llms-a-comprehensive-introduction-e52d03117f95&user=Sean+Smith&userId=6957f6523097&source=-----e52d03117f95---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fe52d03117f95&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fparameter-efficient-fine-tuning-peft-for-llms-a-comprehensive-introduction-e52d03117f95&source=-----e52d03117f95---------------------bookmark_footer-----------)![](../Images/425a0569c2f36a07f176e6742754e25f.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Image created by DALL-E. A Sunday Afternoon on the Island of La Grande Jatte
    but everyone is a humanoid.
  prefs: []
  type: TYPE_NORMAL
- en: Large Language Models (LLMs) are quite large by name. These models usually have
    anywhere from 7 to 70 billion parameters. To load a 70 billion parameter model
    in full precision would require 280 GB of GPU memory! To train that model you
    would update billions of tokens over millions or billions of documents. The computation
    required is substantial for updating those parameters. The self-supervised training
    of these models is expensive, [costing companies up to $100 million](https://www.wired.com/story/openai-ceo-sam-altman-the-age-of-giant-ai-models-is-already-over/).
  prefs: []
  type: TYPE_NORMAL
- en: For the rest of us, there is significant interest in adapting our data to these
    models. With our limited datasets (in comparison) and lacking computing power,
    how do we create models that can improve on the major players at a fraction of
    the cost?
  prefs: []
  type: TYPE_NORMAL
- en: This is where the research field of Parameter-Efficient Fine-Tuning (PEFT) comes
    into play. Through various techniques, which we will soon explore in detail, we
    can augment small sections of these models so they are better suited to the tasks
    we aim to complete.
  prefs: []
  type: TYPE_NORMAL
- en: After reading this article, you will conceptually grasp each PEFT technique
    applied in Hugging Face and…
  prefs: []
  type: TYPE_NORMAL
