- en: Calling All Functions
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/calling-all-functions-a421434ae0a4?source=collection_archive---------7-----------------------#2023-12-07](https://towardsdatascience.com/calling-all-functions-a421434ae0a4?source=collection_archive---------7-----------------------#2023-12-07)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/eba8ad88a5fb2ef4e5bb4bfbe563ef50.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
- en: Image created by author using Dall-E
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: Benchmarking OpenAI function calling and explanations
  id: totrans-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@amber.roberts?source=post_page-----a421434ae0a4--------------------------------)[![Amber
    Roberts](../Images/ee686891eeedca7f33a63e147cc2c086.png)](https://medium.com/@amber.roberts?source=post_page-----a421434ae0a4--------------------------------)[](https://towardsdatascience.com/?source=post_page-----a421434ae0a4--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----a421434ae0a4--------------------------------)
    [Amber Roberts](https://medium.com/@amber.roberts?source=post_page-----a421434ae0a4--------------------------------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: ·
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ffef13eb0f830&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcalling-all-functions-a421434ae0a4&user=Amber+Roberts&userId=fef13eb0f830&source=post_page-fef13eb0f830----a421434ae0a4---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----a421434ae0a4--------------------------------)
    ·9 min read·Dec 7, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fa421434ae0a4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcalling-all-functions-a421434ae0a4&user=Amber+Roberts&userId=fef13eb0f830&source=-----a421434ae0a4---------------------clap_footer-----------)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: --
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fa421434ae0a4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcalling-all-functions-a421434ae0a4&source=-----a421434ae0a4---------------------bookmark_footer-----------)'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: '*Thanks to* [*Roger Yang*](https://www.linkedin.com/in/roger-y-a35595114/)
    *for his contributions to this piece*'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: Observability in third-party large language models (LLMs) is largely approached
    with benchmarking and evaluations since models like Anthropic’s Claude, OpenAI’s
    GPT models, and Google’s PaLM 2 are proprietary. In this blog post, we benchmark
    OpenAI’s GPT models with function calling and explanations against various performance
    metrics. We are specifically interested in how the GPT models and OpenAI features
    perform on correctly classifying hallucinated and relevant responses. The results
    below show the trade-offs between speed and performance for different LLM application
    systems, as well as a discussion on how these results with explanations can be
    used for data labeling, LLM assisted evaluation, and quality checks. The experimental
    framework we used is provided below so that practitioners can iterate and improve
    on the default classification template.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 对第三方大型语言模型（LLMs）的可观察性主要通过基准测试和评估来实现，因为像Anthropic的Claude、OpenAI的GPT模型和Google的PaLM
    2这样的模型是专有的。在这篇博客文章中，我们将OpenAI的GPT模型的函数调用和解释与各种性能指标进行基准测试。我们特别关注GPT模型和OpenAI功能在正确分类幻觉和相关响应方面的表现。下面的结果展示了不同LLM应用系统在速度和性能之间的权衡，以及如何利用这些带有解释的结果进行数据标注、LLM辅助评估和质量检查。我们使用的实验框架也在下面提供，以便从业者可以在默认分类模板上进行迭代和改进。
- en: OpenAI Function Calling and Explanations
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: OpenAI函数调用和解释
- en: With OpenAI function calling you are able to describe functions to various GPT
    models, which can then be used to output a JSON object containing arguments to
    call those functions. Function calling is essentially acting as a tool or agent
    for recording a response in a given format in order to reliably connect OpenAI
    GPT model capabilities with external tools and APIs. In regards to explanations,
    since it can be hard to understand in many cases why an LLM responds in a specific
    way, these are designed to prompt the LLM to justify whether or not an output
    is correct. You can then affix an output label (‘relevant’ or ‘irrelevant’) and
    an explanation from the LLM. Below is an example of a ‘relevant’ evaluation with
    an explanation to debug LLM responses.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 使用OpenAI函数调用，你可以描述函数给各种GPT模型，这些函数可以用来输出包含调用这些函数的参数的JSON对象。函数调用本质上作为一种工具或代理，用于以给定格式记录响应，以可靠地将OpenAI
    GPT模型的能力与外部工具和API连接。关于解释，由于许多情况下很难理解LLM为何以特定方式响应，这些解释旨在促使LLM解释输出是否正确。然后，你可以附上一个输出标签（‘相关’或‘无关’）和LLM的解释。下面是一个带有解释的‘相关’评估示例，用于调试LLM响应。
- en: '![](../Images/16b573487f1f3fa16913ea72627dd754.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/16b573487f1f3fa16913ea72627dd754.png)'
- en: Image by author
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供
- en: 'In the Results and Tradeoffs section below, comparison tables are provided
    for two use cases: predicting relevance and predicting hallucinations (these are
    Table 1 and Table 2, respectively). Each table compares the performance of GPT-4-turbo,
    GPT-4, GPT-3.5 and GPT-3.5-turbo across a variety of classification metrics (accuracy,
    precision, recall and [F1 score](https://arize.com/blog-course/f1-score/)) under
    specific prompt instructions and LLM attributes.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的“结果与权衡”部分，提供了两个用例的比较表格：预测相关性和预测幻觉（分别为表1和表2）。每个表格比较了GPT-4-turbo、GPT-4、GPT-3.5和GPT-3.5-turbo在特定提示指令和LLM属性下的各种分类指标（准确性、精确度、召回率和[F1分数](https://arize.com/blog-course/f1-score/)）的表现。
- en: 'The four foundational models tested are evaluated with those metrics (in addition
    to median processing time) in the cases:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 测试的四个基础模型在这些情况下使用上述指标（除了中位处理时间）进行评估。
- en: Without_function_calling & without_explanations
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 无_function_calling & 无_explanations
- en: With_function_calling & without_explanations
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有_function_calling & 无_explanations
- en: With_function_calling & with_explanations
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有_function_calling & 有_explanations
- en: Without_function_calling & with_explanations
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 无_function_calling & 有_explanations
- en: The prompt template in the LLMs is therefore unchanged (i.e the ordinary prompt
    completion) for examples **without_function_calling & without_explanations**.
    Samples **with_function_calling & without_explanations** ask the LLM to put its
    answer in a JSON object that only accepts enum as input and samples **with_function_calling
    & with_explanations** ask the LLM to provide explanation alongside its answer
    in the same JSON object (see [Google colab notebook](https://colab.research.google.com/github/Arize-ai/phoenix/blob/benchmarking-function-calling-and-explanations/tutorials/internal/benchmarking_function_calling_and_explanations.ipynb)).
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，LLM中的提示模板保持不变（即普通的提示完成），例如**without_function_calling & without_explanations**。具有功能调用但无解释的样本要求LLM将其答案放在仅接受枚举作为输入的JSON对象中，而具有功能调用和解释的样本要求LLM在同一个JSON对象中提供解释（详见[Google
    colab notebook](https://colab.research.google.com/github/Arize-ai/phoenix/blob/benchmarking-function-calling-and-explanations/tutorials/internal/benchmarking_function_calling_and_explanations.ipynb))。
- en: Benchmarked Dataset and Evaluation Metrics
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基准数据集和评估指标
- en: 'Correctly identifying hallucinations and relevant responses in LLM outputs
    are two of the largest pain points for our customers who are currently implementing
    LLM applications. [Optimizing LLM evaluation systems](https://arize.com/blog-course/llm-evaluation-the-definitive-guide/)
    for hallucinations means correctly identifying all hallucinated responses, while
    keeping track of factual outputs. For use cases that leverage search and recommendation
    as part of their user experience, the most important factors related to user satisfaction
    are speed and relevance of results. In order to evaluate the performance of a
    LLM system for relevant and irrelevant outputs you should be aware of key metrics:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 正确识别LLM输出中的幻觉和相关响应是我们客户当前实施LLM应用中最大的两个痛点。[优化LLM评估系统](https://arize.com/blog-course/llm-evaluation-the-definitive-guide/)对于幻觉意味着正确识别所有虚构响应，同时跟踪事实输出。对于利用搜索和推荐作为其用户体验的一部分的用例，与用户满意度相关的最重要因素是结果的速度和相关性。为了评估LLM系统在相关和无关输出上的性能，您应该了解关键指标：
- en: '*Precision and Recall*: How relevant is the information retrieved?'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*精确度和召回率*：检索到的信息有多相关？'
- en: '*Accuracy*: How contextually accurate are the responses?'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*准确性*：回应在语境上的准确度有多高？'
- en: '*Latency*: How long does the system take to provide the response?'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*延迟*：系统提供响应需要多长时间？'
- en: '*User Feedback*: How have the relevance and response time for the result impacted
    the user’s experience?'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*用户反馈*：结果的相关性和响应时间如何影响用户体验？'
- en: Results and Trade-offs
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结果与权衡
- en: 'Here are the results for benchmarking LLM systems for irrelevant and relevant
    predictions:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是关于基准测试LLM系统在无关和相关预测上的结果：
- en: '![](../Images/c960caad6033db8f8681ca5d59cfdbc6.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c960caad6033db8f8681ca5d59cfdbc6.png)'
- en: Table 1 by author
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 表1 作者提供
- en: 'Here are the results for benchmarking LLM systems for hallucination and factual
    predictions:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是基准测试LLM系统在幻觉和事实预测上的结果：
- en: '![](../Images/6c0d3c56fd9c82c9fa530037a2cfa2b7.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6c0d3c56fd9c82c9fa530037a2cfa2b7.png)'
- en: Table 2 by author
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 表2 作者提供
- en: Before going into the analysis of the results, you are able to reproduce these
    results for yourself in this [Google colab notebook](https://colab.research.google.com/github/Arize-ai/phoenix/blob/benchmarking-function-calling-and-explanations/tutorials/internal/benchmarking_function_calling_and_explanations.ipynb).
    Note that ordinarily you wouldn’t be able to recreate the numbers in these tables
    exactly, because of the non-deterministic nature of LLMs, but for this notebook
    we have added a seed to the sampling so it’ll be the same every time. Also, stratified
    sampling has been added so the binary categories are exactly 50/50\. **Be aware
    that there is a computational cost associated with running this notebook with
    your OpenAI API keys.** The default number of samples has been set to 2, but you
    can change the number to 100 if you wish to replicate the results from this blog
    post.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在分析结果之前，你可以在这个[Google colab notebook](https://colab.research.google.com/github/Arize-ai/phoenix/blob/benchmarking-function-calling-and-explanations/tutorials/internal/benchmarking_function_calling_and_explanations.ipynb)中自行复现这些结果。请注意，通常情况下，由于LLM的非确定性特性，你无法完全复现这些表中的数字，但对于这个notebook，我们已经添加了一个种子以保证每次采样结果都相同。此外，我们还添加了分层采样，使得二进制类别确实是完全50/50。**请注意，运行此notebook需要消耗计算资源与您的OpenAI
    API密钥相关。** 默认采样数量设置为2，但如果您希望复制这篇博文中的结果，可以将数字更改为100。
- en: Medium Processing Time
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 中等处理时间
- en: For clarity, these comparisons (using 100 samples) were run on Google Colab
    with a standard OpenAI API account and key. So while the latency values are unlikely
    to be exact when run on a different setup, the slowest and fastest models will
    be reproduced.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, using explanations in your evaluations is likely to take anywhere
    from 3–20x longer to compile (this is independent of function calling).
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: For model predictive ability on relevance overall
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: '*Latency*: GPT-3.5-instruct > GPT-3.5-turbo > GPT-4-turbo > GPT-4'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For model predictive ability on hallucinations
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: 'Latency: GPT-3.5-instruct > GPT-3.5-turbo ~ GPT-4-turbo > GPT-4'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GPT models with function calling tend to have a slightly higher latency than
    LLMs without function calling, but take this with a grain of salt because there
    are a few caveats. First, the latency is extracted from HTTP headers returned
    to us by OpenAI, so depending on your account and your method of making these
    requests, the latency values can shift since they were calculated by OpenAI internally.
    Function calling trade-offs also depend on your use case. For example, without
    function calling you would need to specify exactly how you would need your output
    structured by providing examples and a detailed description. However, if your
    use case is [structured data extraction](https://arize.com/blog-course/structured-data-extraction-openai-function-calling/?utm_campaign=Q323+Content&utm_source=Content)
    then it is simplest to work directly with the OpenAI function calling API.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: Overall, LLMs with function calling perform on par with LLMs that do not leverage
    function calling and instead use the ordinary prompt completion. Whether you decide
    to use the OpenAI function calling API over prompt engineering should depend on
    your use case and complexity of your outputs.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: GPT Model Performance Comparisons
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For model predictive ability on relevance overall:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: 'Performance: GPT-4 ~ GPT-4-turbo ~ GPT-3.5-turbo >>> GPT-3.5-instruct'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For model predictive ability on hallucinations:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: 'Performance: GPT-4 ~ GPT-4-turbo > GPT-3.5-turbo > GPT-3.5-instruct'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Interestingly, in both use cases, using explanations does not always improve
    performance. More on this below.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: Evaluation Metrics
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If you are deciding which LLM to predict relevance, you want to use either GPT-4,
    GPT-4-turbo or GPT-3.5-turbo.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: GPT-4-turbo is precisely identifying when an output is relevant, but is sacrificing
    on recalling all 50 examples, in fact recall is no better than a coin flip even
    when using explanations.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: GPT-3.5-turbo suffers from the same trade-off, while having lower latency and
    lower accuracy. From these results GPT-4 has the highest F1 scores (harmonic mean
    of precision and recall) and overall best performance, while running comparable
    times to GPT4-turbo.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: GPT-3.5-instruct and predicts everything to be relevant and therefore is not
    a viable LLM for predicting relevance. Interestingly, when using explanations
    the predictive performance improves drastically, although it still underperforms
    the other LLMs. Also GPT-3.5-instruct cannot use the OpenAI function calling API
    and is likely [to be deprecated in early 2024](https://platform.openai.com/docs/deprecations).
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-3.5-instruct 预测所有内容都是相关的，因此不是一个可行的预测相关性的 LLM。有趣的是，当使用解释时，预测性能显著提高，尽管它仍然低于其他
    LLM。同时，GPT-3.5-instruct 无法使用 OpenAI 函数调用 API，并且可能会在 [2024 年初被弃用](https://platform.openai.com/docs/deprecations)。
- en: If you are deciding which LLM to predict hallucinations, you want to use either
    GPT-4, GPT-4-turbo or GPT-3.5-turbo.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你正在决定选择哪个 LLM 来预测虚假信息，你可以使用 GPT-4、GPT-4-turbo 或 GPT-3.5-turbo。
- en: The results show GPT-4 correctly identifying hallucinated and factual outputs
    more often (~3% of the time more) across precision, accuracy, recall and F1 than
    GPT-4-turbo.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 结果显示，在精确度、准确性、召回率和 F1 分数方面，GPT-4 在识别虚假和事实输出的正确率上比 GPT-4-turbo 高出约 3%。
- en: While both GPT-4 and GPT-4-turbo perform slightly higher than GPT-3.5-turbo
    (*note a higher number of samples should be used before concluding that the small
    margin isn’t noise*), it might be worth working with GPT-3.5-turbo if you are
    planning to use explanations.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管 GPT-4 和 GPT-4-turbo 的表现略高于 GPT-3.5-turbo（*请注意，得出小差距不代表噪声的结论时应使用更多样本*），如果你计划使用解释，可能值得使用
    GPT-3.5-turbo。
- en: Explanations for predicting hallucinated and factual returned at a rate greater
    than three times faster for GPT-3.5-turbo than they did for both GPT-4 and GPT-4-turbo,
    however the recall did suffer for both GPT-3.5 models when compared to the recall
    of the GPT-4 models when predicting hallucinations correctly.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 用于预测虚假和事实的解释在 GPT-3.5-turbo 上的返回速度比 GPT-4 和 GPT-4-turbo 快三倍以上，然而，当比较 GPT-4 模型预测虚假信息的召回率时，两个
    GPT-3.5 模型的召回率有所下降。
- en: Discussion & Implications
  id: totrans-61
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 讨论与影响
- en: 'When deciding on which LLM to use for your application, there is a series of
    experiments and iterations required to make that decision. Similarly, benchmarking
    and experimentation is also required when deciding on whether or not a LLM should
    be used as an evaluator. Essentially these are the two main methods of benchmarking
    LLMs: LLM model evaluation (evaluating foundation models) and LLM system evaluation
    through observability.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在决定使用哪个 LLM 进行应用时，需要进行一系列实验和迭代。同样，在决定是否将 LLM 用作评估器时，也需要进行基准测试和实验。基本上，这两种方法是基准
    LLM 的主要方法：LLM 模型评估（评估基础模型）和通过可观察性进行 LLM 系统评估。
- en: '![](../Images/7156337ea757257f7cd51b751dc47b39.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7156337ea757257f7cd51b751dc47b39.png)'
- en: Image by author | Evaluating two different prompt templates on a single foundational
    model. We are testing the same dataset across the two templates and seeing how
    their metrics like precision and recall stack up.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供 | 在单一基础模型上评估两种不同的提示模板。我们正在测试相同的数据集，并观察它们的精确度和召回率等指标如何对比。
- en: Ultimately, when deciding if an LLM will make a good performance evaluator for
    your use case, you need to consider the latency of your system in addition to
    the performance of relevant prediction metrics. Throughout this post we summarize
    how these models perform out of the box– without the addition of techniques to
    increase speed and performance. Recall that out-of-the-box all of these LLMs use
    zero-shot templates, so no examples were added to the LLM prompt template to improve
    the model outputs. Since these numbers act as a baseline, teams are able to improve
    the LLM system performance with prompt engineering, prompt templates (and stored
    libraries), agents, fine-tuning, as well as with search and retrieval applications
    like RAG and HyDE.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，当决定一个 LLM 是否适合你的用例作为性能评估器时，你需要考虑系统的延迟以及相关预测指标的性能。在本文中，我们总结了这些模型的开箱即用表现——没有增加提高速度和性能的技术。请记住，开箱即用时，这些
    LLM 都使用零样本模板，因此没有将示例添加到 LLM 提示模板中以改善模型输出。由于这些数据作为基准，团队可以通过提示工程、提示模板（和存储库）、代理、微调以及像
    RAG 和 HyDE 这样的搜索和检索应用来提高 LLM 系统性能。
- en: 'Potential Future Work: Explanations for Data Labeling'
  id: totrans-66
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 潜在的未来工作：数据标注的解释
- en: Through this benchmarking, we found some interesting examples where providing
    an explanation changes the predicted label. The example below predicts “relevant”
    when not using an explanation and even has a “relevant” label for the ground truth.
    Since even “golden datasets” can have mislabeling (especially for more subjective
    tasks), a well justified explanation from a LLM could be enough to edit the ground
    truth label. This can be thought of as a LLM assisted evaluation or quality check.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: Below is one example from the wiki dataset for relevance, note column ‘D’ is
    the ground truth label provided by the dataset, column ‘E’ is the predicted label
    without function calling and without explanation, while column ‘F’ is the predicted
    label created (without function calling) with the explanation in column ‘G.’ Therefore
    column ‘E’ and columns ‘F’ & ‘G’ are responses from two separate LLM calls. F&G
    were generated together from the same call as seen from Figure 1 below.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f794b0059301154c29e8a9b7bdd9919c.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1 (by author): Screenshot of script (code provided in colab). Here,
    the label and explanation are returned together but we call for the explanation
    to be provided first (see prompt).'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: The table below shows an example when we have a ground truth label = ‘relevant’,
    a LLM predicted label without function calling = ‘relevant’ , but then we have
    the label change to ‘irrelevant’ when the LLM is asked to provide an explanation
    first. Like several similar examples we encountered, the LLM makes a valid argument
    for labeling the retrieved answer to the user’s question as ‘irrelevant.’ While
    many of us think about the Roman Empire often, we can agree that the multi-paragraph
    response to “how long did the roman empire last?” is not a concise nor relevant
    enough response to elicit positive feedback from the end user. There are many
    possibilities for LLM assisted evaluations, including cost and time savings to
    companies that need data labeled. This as well as the visibility that explanations
    provide, along with LLMs returning their references (documents used in the prediction),
    are major advancements for the LLM observability space.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/be2961b71844d8b67ab3d7ddf8c4cae6.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
- en: Example from author
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  id: totrans-74
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Hopefully [this piece](https://arize.com/blog/calling-all-functions-benchmarking-openai-function-calling-and-explanations/)
    provides a good start for teams looking to better understand and balance the tradeoffs
    between performance and speed with new LLM application systems. As always, the
    generative AI and LLMOps space are evolving rapidly so it will be interesting
    to watch how these findings and the space change over time.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: Questions? Please reach out to me here or on [LinkedIn](https://www.linkedin.com/in/amber-roberts42/),
    [X](https://twitter.com/astronomeramber), or [Slack](https://join.slack.com/t/arize-ai/shared_invite/zt-26zg4u3lw-OjUNoLvKQ2Yv53EfvxW6Kg)!
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
