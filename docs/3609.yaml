- en: Calling All Functions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/calling-all-functions-a421434ae0a4?source=collection_archive---------7-----------------------#2023-12-07](https://towardsdatascience.com/calling-all-functions-a421434ae0a4?source=collection_archive---------7-----------------------#2023-12-07)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/eba8ad88a5fb2ef4e5bb4bfbe563ef50.png)'
  prefs: []
  type: TYPE_IMG
- en: Image created by author using Dall-E
  prefs: []
  type: TYPE_NORMAL
- en: Benchmarking OpenAI function calling and explanations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@amber.roberts?source=post_page-----a421434ae0a4--------------------------------)[![Amber
    Roberts](../Images/ee686891eeedca7f33a63e147cc2c086.png)](https://medium.com/@amber.roberts?source=post_page-----a421434ae0a4--------------------------------)[](https://towardsdatascience.com/?source=post_page-----a421434ae0a4--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----a421434ae0a4--------------------------------)
    [Amber Roberts](https://medium.com/@amber.roberts?source=post_page-----a421434ae0a4--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ffef13eb0f830&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcalling-all-functions-a421434ae0a4&user=Amber+Roberts&userId=fef13eb0f830&source=post_page-fef13eb0f830----a421434ae0a4---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----a421434ae0a4--------------------------------)
    ·9 min read·Dec 7, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fa421434ae0a4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcalling-all-functions-a421434ae0a4&user=Amber+Roberts&userId=fef13eb0f830&source=-----a421434ae0a4---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fa421434ae0a4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcalling-all-functions-a421434ae0a4&source=-----a421434ae0a4---------------------bookmark_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: '*Thanks to* [*Roger Yang*](https://www.linkedin.com/in/roger-y-a35595114/)
    *for his contributions to this piece*'
  prefs: []
  type: TYPE_NORMAL
- en: Observability in third-party large language models (LLMs) is largely approached
    with benchmarking and evaluations since models like Anthropic’s Claude, OpenAI’s
    GPT models, and Google’s PaLM 2 are proprietary. In this blog post, we benchmark
    OpenAI’s GPT models with function calling and explanations against various performance
    metrics. We are specifically interested in how the GPT models and OpenAI features
    perform on correctly classifying hallucinated and relevant responses. The results
    below show the trade-offs between speed and performance for different LLM application
    systems, as well as a discussion on how these results with explanations can be
    used for data labeling, LLM assisted evaluation, and quality checks. The experimental
    framework we used is provided below so that practitioners can iterate and improve
    on the default classification template.
  prefs: []
  type: TYPE_NORMAL
- en: OpenAI Function Calling and Explanations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With OpenAI function calling you are able to describe functions to various GPT
    models, which can then be used to output a JSON object containing arguments to
    call those functions. Function calling is essentially acting as a tool or agent
    for recording a response in a given format in order to reliably connect OpenAI
    GPT model capabilities with external tools and APIs. In regards to explanations,
    since it can be hard to understand in many cases why an LLM responds in a specific
    way, these are designed to prompt the LLM to justify whether or not an output
    is correct. You can then affix an output label (‘relevant’ or ‘irrelevant’) and
    an explanation from the LLM. Below is an example of a ‘relevant’ evaluation with
    an explanation to debug LLM responses.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/16b573487f1f3fa16913ea72627dd754.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: 'In the Results and Tradeoffs section below, comparison tables are provided
    for two use cases: predicting relevance and predicting hallucinations (these are
    Table 1 and Table 2, respectively). Each table compares the performance of GPT-4-turbo,
    GPT-4, GPT-3.5 and GPT-3.5-turbo across a variety of classification metrics (accuracy,
    precision, recall and [F1 score](https://arize.com/blog-course/f1-score/)) under
    specific prompt instructions and LLM attributes.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The four foundational models tested are evaluated with those metrics (in addition
    to median processing time) in the cases:'
  prefs: []
  type: TYPE_NORMAL
- en: Without_function_calling & without_explanations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With_function_calling & without_explanations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With_function_calling & with_explanations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Without_function_calling & with_explanations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The prompt template in the LLMs is therefore unchanged (i.e the ordinary prompt
    completion) for examples **without_function_calling & without_explanations**.
    Samples **with_function_calling & without_explanations** ask the LLM to put its
    answer in a JSON object that only accepts enum as input and samples **with_function_calling
    & with_explanations** ask the LLM to provide explanation alongside its answer
    in the same JSON object (see [Google colab notebook](https://colab.research.google.com/github/Arize-ai/phoenix/blob/benchmarking-function-calling-and-explanations/tutorials/internal/benchmarking_function_calling_and_explanations.ipynb)).
  prefs: []
  type: TYPE_NORMAL
- en: Benchmarked Dataset and Evaluation Metrics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Correctly identifying hallucinations and relevant responses in LLM outputs
    are two of the largest pain points for our customers who are currently implementing
    LLM applications. [Optimizing LLM evaluation systems](https://arize.com/blog-course/llm-evaluation-the-definitive-guide/)
    for hallucinations means correctly identifying all hallucinated responses, while
    keeping track of factual outputs. For use cases that leverage search and recommendation
    as part of their user experience, the most important factors related to user satisfaction
    are speed and relevance of results. In order to evaluate the performance of a
    LLM system for relevant and irrelevant outputs you should be aware of key metrics:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Precision and Recall*: How relevant is the information retrieved?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Accuracy*: How contextually accurate are the responses?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Latency*: How long does the system take to provide the response?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*User Feedback*: How have the relevance and response time for the result impacted
    the user’s experience?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Results and Trade-offs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Here are the results for benchmarking LLM systems for irrelevant and relevant
    predictions:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c960caad6033db8f8681ca5d59cfdbc6.png)'
  prefs: []
  type: TYPE_IMG
- en: Table 1 by author
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are the results for benchmarking LLM systems for hallucination and factual
    predictions:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6c0d3c56fd9c82c9fa530037a2cfa2b7.png)'
  prefs: []
  type: TYPE_IMG
- en: Table 2 by author
  prefs: []
  type: TYPE_NORMAL
- en: Before going into the analysis of the results, you are able to reproduce these
    results for yourself in this [Google colab notebook](https://colab.research.google.com/github/Arize-ai/phoenix/blob/benchmarking-function-calling-and-explanations/tutorials/internal/benchmarking_function_calling_and_explanations.ipynb).
    Note that ordinarily you wouldn’t be able to recreate the numbers in these tables
    exactly, because of the non-deterministic nature of LLMs, but for this notebook
    we have added a seed to the sampling so it’ll be the same every time. Also, stratified
    sampling has been added so the binary categories are exactly 50/50\. **Be aware
    that there is a computational cost associated with running this notebook with
    your OpenAI API keys.** The default number of samples has been set to 2, but you
    can change the number to 100 if you wish to replicate the results from this blog
    post.
  prefs: []
  type: TYPE_NORMAL
- en: Medium Processing Time
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For clarity, these comparisons (using 100 samples) were run on Google Colab
    with a standard OpenAI API account and key. So while the latency values are unlikely
    to be exact when run on a different setup, the slowest and fastest models will
    be reproduced.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, using explanations in your evaluations is likely to take anywhere
    from 3–20x longer to compile (this is independent of function calling).
  prefs: []
  type: TYPE_NORMAL
- en: For model predictive ability on relevance overall
  prefs: []
  type: TYPE_NORMAL
- en: '*Latency*: GPT-3.5-instruct > GPT-3.5-turbo > GPT-4-turbo > GPT-4'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For model predictive ability on hallucinations
  prefs: []
  type: TYPE_NORMAL
- en: 'Latency: GPT-3.5-instruct > GPT-3.5-turbo ~ GPT-4-turbo > GPT-4'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GPT models with function calling tend to have a slightly higher latency than
    LLMs without function calling, but take this with a grain of salt because there
    are a few caveats. First, the latency is extracted from HTTP headers returned
    to us by OpenAI, so depending on your account and your method of making these
    requests, the latency values can shift since they were calculated by OpenAI internally.
    Function calling trade-offs also depend on your use case. For example, without
    function calling you would need to specify exactly how you would need your output
    structured by providing examples and a detailed description. However, if your
    use case is [structured data extraction](https://arize.com/blog-course/structured-data-extraction-openai-function-calling/?utm_campaign=Q323+Content&utm_source=Content)
    then it is simplest to work directly with the OpenAI function calling API.
  prefs: []
  type: TYPE_NORMAL
- en: Overall, LLMs with function calling perform on par with LLMs that do not leverage
    function calling and instead use the ordinary prompt completion. Whether you decide
    to use the OpenAI function calling API over prompt engineering should depend on
    your use case and complexity of your outputs.
  prefs: []
  type: TYPE_NORMAL
- en: GPT Model Performance Comparisons
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For model predictive ability on relevance overall:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Performance: GPT-4 ~ GPT-4-turbo ~ GPT-3.5-turbo >>> GPT-3.5-instruct'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For model predictive ability on hallucinations:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Performance: GPT-4 ~ GPT-4-turbo > GPT-3.5-turbo > GPT-3.5-instruct'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Interestingly, in both use cases, using explanations does not always improve
    performance. More on this below.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluation Metrics
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If you are deciding which LLM to predict relevance, you want to use either GPT-4,
    GPT-4-turbo or GPT-3.5-turbo.
  prefs: []
  type: TYPE_NORMAL
- en: GPT-4-turbo is precisely identifying when an output is relevant, but is sacrificing
    on recalling all 50 examples, in fact recall is no better than a coin flip even
    when using explanations.
  prefs: []
  type: TYPE_NORMAL
- en: GPT-3.5-turbo suffers from the same trade-off, while having lower latency and
    lower accuracy. From these results GPT-4 has the highest F1 scores (harmonic mean
    of precision and recall) and overall best performance, while running comparable
    times to GPT4-turbo.
  prefs: []
  type: TYPE_NORMAL
- en: GPT-3.5-instruct and predicts everything to be relevant and therefore is not
    a viable LLM for predicting relevance. Interestingly, when using explanations
    the predictive performance improves drastically, although it still underperforms
    the other LLMs. Also GPT-3.5-instruct cannot use the OpenAI function calling API
    and is likely [to be deprecated in early 2024](https://platform.openai.com/docs/deprecations).
  prefs: []
  type: TYPE_NORMAL
- en: If you are deciding which LLM to predict hallucinations, you want to use either
    GPT-4, GPT-4-turbo or GPT-3.5-turbo.
  prefs: []
  type: TYPE_NORMAL
- en: The results show GPT-4 correctly identifying hallucinated and factual outputs
    more often (~3% of the time more) across precision, accuracy, recall and F1 than
    GPT-4-turbo.
  prefs: []
  type: TYPE_NORMAL
- en: While both GPT-4 and GPT-4-turbo perform slightly higher than GPT-3.5-turbo
    (*note a higher number of samples should be used before concluding that the small
    margin isn’t noise*), it might be worth working with GPT-3.5-turbo if you are
    planning to use explanations.
  prefs: []
  type: TYPE_NORMAL
- en: Explanations for predicting hallucinated and factual returned at a rate greater
    than three times faster for GPT-3.5-turbo than they did for both GPT-4 and GPT-4-turbo,
    however the recall did suffer for both GPT-3.5 models when compared to the recall
    of the GPT-4 models when predicting hallucinations correctly.
  prefs: []
  type: TYPE_NORMAL
- en: Discussion & Implications
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'When deciding on which LLM to use for your application, there is a series of
    experiments and iterations required to make that decision. Similarly, benchmarking
    and experimentation is also required when deciding on whether or not a LLM should
    be used as an evaluator. Essentially these are the two main methods of benchmarking
    LLMs: LLM model evaluation (evaluating foundation models) and LLM system evaluation
    through observability.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7156337ea757257f7cd51b751dc47b39.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author | Evaluating two different prompt templates on a single foundational
    model. We are testing the same dataset across the two templates and seeing how
    their metrics like precision and recall stack up.
  prefs: []
  type: TYPE_NORMAL
- en: Ultimately, when deciding if an LLM will make a good performance evaluator for
    your use case, you need to consider the latency of your system in addition to
    the performance of relevant prediction metrics. Throughout this post we summarize
    how these models perform out of the box– without the addition of techniques to
    increase speed and performance. Recall that out-of-the-box all of these LLMs use
    zero-shot templates, so no examples were added to the LLM prompt template to improve
    the model outputs. Since these numbers act as a baseline, teams are able to improve
    the LLM system performance with prompt engineering, prompt templates (and stored
    libraries), agents, fine-tuning, as well as with search and retrieval applications
    like RAG and HyDE.
  prefs: []
  type: TYPE_NORMAL
- en: 'Potential Future Work: Explanations for Data Labeling'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Through this benchmarking, we found some interesting examples where providing
    an explanation changes the predicted label. The example below predicts “relevant”
    when not using an explanation and even has a “relevant” label for the ground truth.
    Since even “golden datasets” can have mislabeling (especially for more subjective
    tasks), a well justified explanation from a LLM could be enough to edit the ground
    truth label. This can be thought of as a LLM assisted evaluation or quality check.
  prefs: []
  type: TYPE_NORMAL
- en: Below is one example from the wiki dataset for relevance, note column ‘D’ is
    the ground truth label provided by the dataset, column ‘E’ is the predicted label
    without function calling and without explanation, while column ‘F’ is the predicted
    label created (without function calling) with the explanation in column ‘G.’ Therefore
    column ‘E’ and columns ‘F’ & ‘G’ are responses from two separate LLM calls. F&G
    were generated together from the same call as seen from Figure 1 below.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f794b0059301154c29e8a9b7bdd9919c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1 (by author): Screenshot of script (code provided in colab). Here,
    the label and explanation are returned together but we call for the explanation
    to be provided first (see prompt).'
  prefs: []
  type: TYPE_NORMAL
- en: The table below shows an example when we have a ground truth label = ‘relevant’,
    a LLM predicted label without function calling = ‘relevant’ , but then we have
    the label change to ‘irrelevant’ when the LLM is asked to provide an explanation
    first. Like several similar examples we encountered, the LLM makes a valid argument
    for labeling the retrieved answer to the user’s question as ‘irrelevant.’ While
    many of us think about the Roman Empire often, we can agree that the multi-paragraph
    response to “how long did the roman empire last?” is not a concise nor relevant
    enough response to elicit positive feedback from the end user. There are many
    possibilities for LLM assisted evaluations, including cost and time savings to
    companies that need data labeled. This as well as the visibility that explanations
    provide, along with LLMs returning their references (documents used in the prediction),
    are major advancements for the LLM observability space.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/be2961b71844d8b67ab3d7ddf8c4cae6.png)'
  prefs: []
  type: TYPE_IMG
- en: Example from author
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Hopefully [this piece](https://arize.com/blog/calling-all-functions-benchmarking-openai-function-calling-and-explanations/)
    provides a good start for teams looking to better understand and balance the tradeoffs
    between performance and speed with new LLM application systems. As always, the
    generative AI and LLMOps space are evolving rapidly so it will be interesting
    to watch how these findings and the space change over time.
  prefs: []
  type: TYPE_NORMAL
- en: Questions? Please reach out to me here or on [LinkedIn](https://www.linkedin.com/in/amber-roberts42/),
    [X](https://twitter.com/astronomeramber), or [Slack](https://join.slack.com/t/arize-ai/shared_invite/zt-26zg4u3lw-OjUNoLvKQ2Yv53EfvxW6Kg)!
  prefs: []
  type: TYPE_NORMAL
