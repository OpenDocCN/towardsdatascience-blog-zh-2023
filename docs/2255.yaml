- en: Learning Transformers Code First Part 2 — GPT Up Close and Personal
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/learning-transformers-code-first-part-2-gpt-up-close-and-personal-1635b52ae0d7?source=collection_archive---------11-----------------------#2023-07-13](https://towardsdatascience.com/learning-transformers-code-first-part-2-gpt-up-close-and-personal-1635b52ae0d7?source=collection_archive---------11-----------------------#2023-07-13)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Digging into Generative Pre-Trained Transformers via nanoGPT
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@oaguy1?source=post_page-----1635b52ae0d7--------------------------------)[![Lily
    Hughes-Robinson](../Images/b610721a40e274e7fb81418395314ae3.png)](https://medium.com/@oaguy1?source=post_page-----1635b52ae0d7--------------------------------)[](https://towardsdatascience.com/?source=post_page-----1635b52ae0d7--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----1635b52ae0d7--------------------------------)
    [Lily Hughes-Robinson](https://medium.com/@oaguy1?source=post_page-----1635b52ae0d7--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F5389e25ca1bb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flearning-transformers-code-first-part-2-gpt-up-close-and-personal-1635b52ae0d7&user=Lily+Hughes-Robinson&userId=5389e25ca1bb&source=post_page-5389e25ca1bb----1635b52ae0d7---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----1635b52ae0d7--------------------------------)
    ·13 min read·Jul 13, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F1635b52ae0d7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flearning-transformers-code-first-part-2-gpt-up-close-and-personal-1635b52ae0d7&user=Lily+Hughes-Robinson&userId=5389e25ca1bb&source=-----1635b52ae0d7---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F1635b52ae0d7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flearning-transformers-code-first-part-2-gpt-up-close-and-personal-1635b52ae0d7&source=-----1635b52ae0d7---------------------bookmark_footer-----------)![](../Images/f90478c1b96dad6cd2667bfcf0da1f03.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Photo by [Luca Onniboni](https://unsplash.com/it/@lucaonniboni?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: Welcome to the second part of my project, where I delve into the intricacies
    of transformer and GPT-based models using the [TinyStories dataset](https://huggingface.co/datasets/roneneldan/TinyStories)
    and [nanoGPT](https://github.com/karpathy/nanoGPT/tree/master) all trained on
    an aging gaming laptop. In the first part, I prepared the dataset for input into
    a character-level generative model. You can find a link to part one below.
  prefs: []
  type: TYPE_NORMAL
- en: '[](/nanogpt-learning-transformers-code-first-part-1-f2044cf5bca0?source=post_page-----1635b52ae0d7--------------------------------)
    [## Learning Transformers Code First Part 1'
  prefs: []
  type: TYPE_NORMAL
- en: Part 1 of a new series where I endeavor to learn transformers code first using
    nanoGPT
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/nanogpt-learning-transformers-code-first-part-1-f2044cf5bca0?source=post_page-----1635b52ae0d7--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: In this article, I aim to dissect the GPT model, its components, and its implementation
    in nanoGPT. I selected nanoGPT due to its straightforward Python implementation
    of a GPT model, which is approximately 300 lines long, and its similarly digestible
    training script. With the necessary background knowledge, one could quickly comprehend
    GPT models from simply reading the source code. To be frank, I lacked this understanding
    when I first examined the code. Some of the material still eludes me. However,
    I hope that with all I’ve learned, this explanation will provide a starting point
    for those wishing to gain an intuitive…
  prefs: []
  type: TYPE_NORMAL
