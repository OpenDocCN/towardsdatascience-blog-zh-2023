- en: 'Optimization or Architecture: How to Hack Kalman Filtering'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 优化还是架构：如何破解卡尔曼滤波
- en: 原文：[https://towardsdatascience.com/optimization-or-architecture-how-to-hack-kalman-filtering-c0a21ac7d756?source=collection_archive---------3-----------------------#2023-12-26](https://towardsdatascience.com/optimization-or-architecture-how-to-hack-kalman-filtering-c0a21ac7d756?source=collection_archive---------3-----------------------#2023-12-26)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/optimization-or-architecture-how-to-hack-kalman-filtering-c0a21ac7d756?source=collection_archive---------3-----------------------#2023-12-26](https://towardsdatascience.com/optimization-or-architecture-how-to-hack-kalman-filtering-c0a21ac7d756?source=collection_archive---------3-----------------------#2023-12-26)
- en: Why neural networks may seem better than the KF even when they are not — and
    how to both fix this and improve your KF itself
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为什么即使在神经网络不如卡尔曼滤波器时，它们可能看起来更好——以及如何修复这个问题并改进你的卡尔曼滤波器
- en: '[](https://idogreenberg90.medium.com/?source=post_page-----c0a21ac7d756--------------------------------)[![Ido
    Greenberg](../Images/887c83b2f367133a6eab98b2bb4716ad.png)](https://idogreenberg90.medium.com/?source=post_page-----c0a21ac7d756--------------------------------)[](https://towardsdatascience.com/?source=post_page-----c0a21ac7d756--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----c0a21ac7d756--------------------------------)
    [Ido Greenberg](https://idogreenberg90.medium.com/?source=post_page-----c0a21ac7d756--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://idogreenberg90.medium.com/?source=post_page-----c0a21ac7d756--------------------------------)[![Ido
    Greenberg](../Images/887c83b2f367133a6eab98b2bb4716ad.png)](https://idogreenberg90.medium.com/?source=post_page-----c0a21ac7d756--------------------------------)[](https://towardsdatascience.com/?source=post_page-----c0a21ac7d756--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----c0a21ac7d756--------------------------------)
    [Ido Greenberg](https://idogreenberg90.medium.com/?source=post_page-----c0a21ac7d756--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fe3afd2470982&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Foptimization-or-architecture-how-to-hack-kalman-filtering-c0a21ac7d756&user=Ido+Greenberg&userId=e3afd2470982&source=post_page-e3afd2470982----c0a21ac7d756---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----c0a21ac7d756--------------------------------)
    ·6 min read·Dec 26, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fc0a21ac7d756&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Foptimization-or-architecture-how-to-hack-kalman-filtering-c0a21ac7d756&user=Ido+Greenberg&userId=e3afd2470982&source=-----c0a21ac7d756---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fe3afd2470982&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Foptimization-or-architecture-how-to-hack-kalman-filtering-c0a21ac7d756&user=Ido+Greenberg&userId=e3afd2470982&source=post_page-e3afd2470982----c0a21ac7d756---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----c0a21ac7d756--------------------------------)
    ·6分钟阅读·2023年12月26日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fc0a21ac7d756&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Foptimization-or-architecture-how-to-hack-kalman-filtering-c0a21ac7d756&user=Ido+Greenberg&userId=e3afd2470982&source=-----c0a21ac7d756---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc0a21ac7d756&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Foptimization-or-architecture-how-to-hack-kalman-filtering-c0a21ac7d756&source=-----c0a21ac7d756---------------------bookmark_footer-----------)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc0a21ac7d756&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Foptimization-or-architecture-how-to-hack-kalman-filtering-c0a21ac7d756&source=-----c0a21ac7d756---------------------bookmark_footer-----------)'
- en: This post introduces our [recent paper](https://arxiv.org/abs/2310.00675) from
    NeurIPS 2023\. Code is available on [PyPI](https://pypi.org/project/Optimized-Kalman-Filter/).
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 本文介绍了我们在 NeurIPS 2023 上发表的 [近期论文](https://arxiv.org/abs/2310.00675)。代码可在 [PyPI](https://pypi.org/project/Optimized-Kalman-Filter/)
    上获取。
- en: Background
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 背景
- en: The Kalman Filter (KF) is a celebrated method for sequential forecasting and
    control since 1960\. While many new methods were introduced in the last decades,
    the KF’s simple design makes it a practical, robust and competitive method to
    this day. The [original paper](https://asmedigitalcollection.asme.org/fluidsengineering/article-abstract/82/1/35/397706/A-New-Approach-to-Linear-Filtering-and-Prediction)
    from 1960 has [12K citations](https://scholar.google.com/scholar?as_ylo=2019&hl=en&as_sdt=2005&sciodt=0%2C5&cites=5225957811069312144&scipsc=)
    in the last 5 years alone. Its broad applications include [navigation](https://ardupilot.org/dev/docs/extended-kalman-filter.html),
    [medical treatment](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4029942/), [marketing
    analysis](https://medium.com/@bqnnguyen/kalman-filter-and-its-application-in-marketing-analytics-d568def19679),
    [deep learning](https://arxiv.org/abs/1901.07860) and even [getting to the moon](https://www.lancaster.ac.uk/stor-i-student-sites/jack-trainer/how-nasa-used-the-kalman-filter-in-the-apollo-program/).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 卡尔曼滤波器 (KF) 自1960年以来一直是一种备受推崇的序列预测和控制方法。尽管在过去几十年中出现了许多新方法，但 KF 的简单设计使其至今仍然是一种实用、稳健且具有竞争力的方法。1960年的[原始论文](https://asmedigitalcollection.asme.org/fluidsengineering/article-abstract/82/1/35/397706/A-New-Approach-to-Linear-Filtering-and-Prediction)在过去5年内已有[12K次引用](https://scholar.google.com/scholar?as_ylo=2019&hl=en&as_sdt=2005&sciodt=0%2C5&cites=5225957811069312144&scipsc=)。其广泛的应用包括[导航](https://ardupilot.org/dev/docs/extended-kalman-filter.html)、[医疗治疗](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4029942/)、[市场分析](https://medium.com/@bqnnguyen/kalman-filter-and-its-application-in-marketing-analytics-d568def19679)、[深度学习](https://arxiv.org/abs/1901.07860)
    甚至[登月](https://www.lancaster.ac.uk/stor-i-student-sites/jack-trainer/how-nasa-used-the-kalman-filter-in-the-apollo-program/)。
- en: '![](../Images/6101f1c1b4507de6519d5e1eff447d57.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6101f1c1b4507de6519d5e1eff447d57.png)'
- en: Illustration of the KF on Apollo mission (image by author)
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: KF 在阿波罗任务中的示意图（图源：作者）
- en: Technically, the KF predicts the state *x* of a system (e.g. spaceship location)
    from a sequence of noisy observations (e.g. radar/camera). It estimates a distribution
    over the state (e.g. location estimate + uncertainty). Every time-step, it predicts
    the next state according to the dynamics model *F*, and increases the uncertainty
    according to the dynamics noise ***Q***. Every observation, it updates the state
    and its uncertainty according to the new observation *z* and its noise ***R***.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 从技术上讲，KF 从一系列噪声观测（例如雷达/相机）中预测系统的状态 *x*（例如航天器位置）。它估计状态的分布（例如位置估计 + 不确定性）。每个时间步，它根据动态模型
    *F* 预测下一个状态，并根据动态噪声 ***Q*** 增加不确定性。每个观测，它根据新的观测 *z* 及其噪声 ***R*** 更新状态及其不确定性。
- en: '![](../Images/bcbed22dbdc7e8ed4df2368fd38073b1.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bcbed22dbdc7e8ed4df2368fd38073b1.png)'
- en: An illustration of a single step of the KF (image by author)
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: KF 的单步操作示意图（图源：作者）
- en: Kalman Filter or a Neural Network?
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 卡尔曼滤波器还是神经网络？
- en: The KF prediction model *F* is linear, which is somewhat restrictive. So we
    built a fancy neural network on top of the KF. This gave us better prediction
    accuracy than the standard KF! Good for us!
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: KF 的预测模型 *F* 是线性的，这有些限制。因此，我们在 KF 的基础上构建了一个高端神经网络。这使我们比标准 KF 获得了更好的预测准确性！这对我们来说真是好事！
- en: Finalizing our experiments for a paper, we conducted some ablation tests. In
    one of them, we *removed the neural network completely*, and just *optimized the
    internal KF parameters* *Q* and *R*. Imagine the look at my face when this optimized
    KF outperformed not only the standard KF, but also my fancy network! The same
    KF model exactly, with the same 60-yo linear architecture, becomes superior just
    by changing the values of its noise parameters.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 为了最终确定我们的论文实验，我们进行了几项消融测试。在其中一个测试中，我们*完全移除了神经网络*，仅仅*优化了内部 KF 参数* *Q* 和 *R*。当这个优化后的
    KF 不仅超越了标准 KF，还超越了我那高端的网络时，你可以想象我脸上的表情！同样的 KF 模型，完全相同的60年线性架构，仅通过更改噪声参数的值就变得优越了。
- en: '![](../Images/b656bd7fb939d4f32614e8c64a4c9c73.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b656bd7fb939d4f32614e8c64a4c9c73.png)'
- en: Prediction errors of the KF, the Neural KF (NKF), and the Optimized KF (OKF)
    (image from our paper)
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: KF、神经 KF (NKF) 和优化 KF (OKF) 的预测误差（图源：我们的论文）
- en: 'The KF beating a neural network is interesting but *anecdotal* to our problem.
    More important is the *methodological* insight: before the extended tests, we
    were about to declare the network as superior to the KF — just because we didn’t
    compare the two properly.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: KF 击败神经网络虽然有趣，但对我们的问题来说却是*轶事性的*。更重要的是*方法论*的见解：在扩展测试之前，我们差点就宣称网络优于 KF——仅仅因为我们没有正确地比较这两者。
- en: '**Message 1: To make sure that your neural network is actually better than
    the KF, optimize the KF just as nicely as you do for the network.**'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '**信息 1：为了确保你的神经网络确实比KF更好，就像优化网络一样优化KF。**'
- en: '**Remark — does this mean that the KF is better than neural networks?** We
    certainly make no such general claim. Our claim is about the methodology — that
    both models should be optimized similarly if you’d like to compare them. Having
    said that, we do demonstrate *anecdotally* that the KF can be better in the Doppler
    radar problem, despite the non-linearity in the problem. In fact, this was so
    hard for me to accept, that I lost a bet about my neural KF, along with many weeks
    of hyperparameter optimization and other tricks.'
  id: totrans-23
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**备注——这是否意味着KF比神经网络更好？** 我们确实没有做出这样的普遍性声明。我们的声明是关于方法论的——如果你想比较这两种模型，它们都应当类似地优化。话虽如此，我们确实*从经验上*展示了在多普勒雷达问题中，尽管存在非线性，KF可能表现更好。事实上，这让我难以接受，以至于我输了关于我的神经KF的赌注，花费了许多周的超参数优化和其他技巧。'
- en: Optimizing the Kalman Filter
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 优化卡尔曼滤波器
- en: 'When comparing two architectures, optimize them similarly. Sounds somewhat
    trivial, doesn’t it? As it happens, this flaw was not unique to our research:
    in the literature of non-linear filtering, the linear KF (or its extension EKF)
    is usually used as a baseline for comparison, but is rarely optimized. And there
    is actually a reason for that: the standard KF parameters are “known” to already
    yield optimal predictions, so why bother optimizing further?'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在比较两个架构时，类似地优化它们。这听起来有些微不足道，对吧？实际上，这个缺陷并非我们研究中的独特问题：在非线性滤波文献中，线性KF（或其扩展EKF）通常作为比较的基准，但很少被优化。事实上，这背后有一个原因：标准KF参数“被认为”已经能够提供最优预测，那么何必再进一步优化呢？
- en: '![](../Images/70b7f1594bce4d25619337cda490b4b3.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/70b7f1594bce4d25619337cda490b4b3.png)'
- en: The standard closed-form equation for the KF parameters *Q* and R. We focus
    on the settings where offline data is available with both states {x} and observations
    {z}, hence the covariances can be calculated directly. Other methods to determine
    Q and R are typically intended for other settings, e.g. without data of {x}.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: KF参数*Q*和R的标准封闭形式方程。我们关注的是离线数据同时提供状态{x}和观测{z}的设置，因此协方差可以直接计算。其他确定Q和R的方法通常用于其他设置，例如没有{x}数据的情况。
- en: 'Unfortunately, the optimality of the closed-form equations does not hold in
    practice, as it relies on a set of quite strong assumptions, which rarely hold
    in the real world. In fact, in the simple, classic, low-dimensional problem of
    a Doppler radar, we found no less than 4 violations of the assumptions. In some
    cases, the violation is tricky to even notice: for example, we simulated iid observation
    noise — but in spherical coordinates. Once transforming to Cartesian coordinates,
    the noise is no longer iid!'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，封闭形式方程的最优性在实践中并不成立，因为它依赖于一组相当强的假设，而这些假设在现实世界中很少成立。事实上，在简单的经典低维多普勒雷达问题中，我们发现了不少于4个假设的违背。在某些情况下，这种违背甚至很难察觉：例如，我们模拟了独立同分布的观测噪声——但使用的是球坐标系。一旦转换到笛卡尔坐标系，噪声就不再是独立同分布的了！
- en: '![](../Images/1bfdc17cbb11accee8c01189bf3ec11c.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1bfdc17cbb11accee8c01189bf3ec11c.png)'
- en: 'The violation of 4 KF assumptions in the Doppler radar problem: non-linear
    dynamics; non-linear observations; inaccurate initialized distribution; and non-iid
    observation noise. (image by author)'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 多普勒雷达问题中4个KF假设的违背：非线性动力学；非线性观测；不准确的初始化分布；以及非独立同分布的观测噪声。（图片由作者提供）
- en: '**Message 2: Do not trust the KF assumptions, and thus avoid the closed-form
    covariance estimation. Instead, optimize the parameters wrt your loss — just as
    with any other prediction model.**'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '**信息 2：不要相信KF的假设，因此避免使用封闭形式的协方差估计。相反，按照你的损失函数优化参数——就像任何其他预测模型一样。**'
- en: In other words, in the real world, noise covariance estimation is no longer
    a proxy to optimize the prediction errors. This discrepancy between the objectives
    creates surprising anomalies. In one experiment, we replace noise estimation with
    an *oracle* KF that knows the exact noise in the system. This oracle is still
    inferior to the Optimized KF — since accurate noise estimation is not the desired
    objective, but rather accurate state prediction. In another experiment, the KF
    *deteriorates* when it is fed with more data, since it effectively pursues a different
    objective than the MSE!
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，在现实世界中，噪声协方差估计不再是优化预测误差的代理。这种目标之间的差异会导致意外的异常。在一次实验中，我们用一个*oracle* KF替代噪声估计，该*oracle*知道系统中的准确噪声。这个*oracle*仍然不如优化后的KF，因为准确的噪声估计不是期望的目标，而是准确的状态预测。在另一次实验中，当KF接收到更多数据时，其表现会*恶化*，因为它实际上追求的是与均方误差（MSE）不同的目标！
- en: '![](../Images/3a39e8212534b7ac9b919976d0c46e15.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3a39e8212534b7ac9b919976d0c46e15.png)'
- en: Test errors vs. train data size. The standard KF is not only inferior to the
    Optimized KF, but also deteriorates with the data, since its parameters are not
    set to optimize the desired objective. (image from our paper)
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 测试误差与训练数据大小的关系。标准KF不仅劣于优化后的KF，而且随着数据的增加而*恶化*，因为其参数并未设置以优化期望的目标。（图像摘自我们的论文）
- en: So how to optimize the KF?
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 那么，如何优化KF呢？
- en: 'Behind the standard noise-estimation method for KF tuning, stands the view
    of the KF parameters as representatives of the noise. This view is beneficial
    in some contexts. However, as discussed above, for the sake of optimization, we
    should “forget” about this role of the KF parameters, and just treat them as model
    parameters, whose objective is loss minimization. This alternative view also tells
    us *how* to optimize: just like any sequential prediction model, such as RNN!
    Given the data, we just make predictions, calculate loss, backpropagate gradients,
    update the model, and repeat.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在标准噪声估计方法用于KF调整的背后，是将KF参数视为噪声的代表。这种观点在某些情况下是有益的。然而，如上所述，为了优化，我们应该“忘记”KF参数的这一角色，只将它们视为模型参数，其目标是损失最小化。这种替代观点也告诉我们*如何*进行优化：就像任何序列预测模型，例如RNN！给定数据，我们只需进行预测，计算损失，反向传播梯度，更新模型，并重复进行。
- en: The main difference from RNN, is that the parameters *Q*,*R* come in the form
    of a covariance matrix, so they should remain symmetric and positive definite.
    To handle this, we use the Cholesky decomposition to write *Q=LL**, and optimize
    the entries of *L*. This guarantees that *Q* remains positive definite regardless
    of the optimization updates. This trick is used for both *Q*,*R*.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 与RNN的主要区别在于，参数*Q*和*R*以协方差矩阵的形式出现，因此它们应该保持对称且正定。为此，我们使用Cholesky分解将*Q*写成*LL**的形式，并优化*L*的条目。这确保了*Q*在优化更新后仍然保持正定。这个技巧同时适用于*Q*和*R*。
- en: '![](../Images/5ffae49da232000912445ea2a510e593.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5ffae49da232000912445ea2a510e593.png)'
- en: Pseudocode of the OKF training procedure (from our paper)
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: OKF训练过程的伪代码（摘自我们的论文）
- en: This optimization procedure was found fast and stable in all of our experiments,
    as the number of parameters was several orders of magnitude beneath typical neural
    networks. And while the training is easy to implement yourself, you may also use
    our PyPI package, as demonstrated in [this](https://github.com/ido90/Optimized-Kalman-Filter/blob/master/example.ipynb)
    example :)
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们所有的实验中，这种优化过程被发现既快速又稳定，因为参数数量比典型神经网络少几个数量级。虽然训练过程容易自己实现，但你也可以使用我们的PyPI包，如[这个](https://github.com/ido90/Optimized-Kalman-Filter/blob/master/example.ipynb)示例所示
    :)
- en: Summary
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: As summarized in the diagram below, our main message is that the KF assumptions
    cannot be trusted, and thus we should optimize the KF directly — whether we use
    it as our primary prediction model, or as a reference for comparison with a new
    method.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 如下图所示，我们的主要信息是KF假设不可靠，因此我们应该直接优化KF——无论我们是将其作为主要预测模型，还是作为与新方法比较的参考。
- en: Our simple training procedure is available in PyPI. More importantly, since
    our architecture remains identical to the original KF, any system using KF (or
    Extended-KF) can be easily upgraded to OKF just by re-learning the parameters
    — without adding a single line of code in inference time.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的简单训练过程可以在PyPI上找到。更重要的是，由于我们的架构与原始KF保持一致，任何使用KF（或扩展KF）的系统都可以通过重新学习参数轻松升级为OKF——无需在推理时增加任何代码行。
- en: '![](../Images/298fa1f9a83fa1a5598cb391a7dcc433.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/298fa1f9a83fa1a5598cb391a7dcc433.png)'
- en: Summary of our scope and contribution. Since the KF assumptions are often violated,
    the KF must be optimized directly. (image from our paper)
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的范围和贡献总结。由于 KF 假设经常被违反，因此必须直接优化 KF。 （图片来自我们的论文）
