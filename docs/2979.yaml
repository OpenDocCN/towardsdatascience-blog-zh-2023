- en: Run Llama 2 70B on Your GPU with ExLlamaV2
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在你的 GPU 上运行 Llama 2 70B 使用 ExLlamaV2
- en: 原文：[https://towardsdatascience.com/run-llama-2-70b-on-your-gpu-with-exllamav2-588141a88598?source=collection_archive---------0-----------------------#2023-09-29](https://towardsdatascience.com/run-llama-2-70b-on-your-gpu-with-exllamav2-588141a88598?source=collection_archive---------0-----------------------#2023-09-29)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/run-llama-2-70b-on-your-gpu-with-exllamav2-588141a88598?source=collection_archive---------0-----------------------#2023-09-29](https://towardsdatascience.com/run-llama-2-70b-on-your-gpu-with-exllamav2-588141a88598?source=collection_archive---------0-----------------------#2023-09-29)
- en: Finding the optimal mixed-precision quantization for your hardware
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 找到适合你硬件的最佳混合精度量化方法
- en: '[](https://medium.com/@bnjmn_marie?source=post_page-----588141a88598--------------------------------)[![Benjamin
    Marie](../Images/3ea1ad230cb1e67610418a8e36a5e5dd.png)](https://medium.com/@bnjmn_marie?source=post_page-----588141a88598--------------------------------)[](https://towardsdatascience.com/?source=post_page-----588141a88598--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----588141a88598--------------------------------)
    [Benjamin Marie](https://medium.com/@bnjmn_marie?source=post_page-----588141a88598--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@bnjmn_marie?source=post_page-----588141a88598--------------------------------)[![Benjamin
    Marie](../Images/3ea1ad230cb1e67610418a8e36a5e5dd.png)](https://medium.com/@bnjmn_marie?source=post_page-----588141a88598--------------------------------)[](https://towardsdatascience.com/?source=post_page-----588141a88598--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----588141a88598--------------------------------)
    [Benjamin Marie](https://medium.com/@bnjmn_marie?source=post_page-----588141a88598--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fad2a414578b3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frun-llama-2-70b-on-your-gpu-with-exllamav2-588141a88598&user=Benjamin+Marie&userId=ad2a414578b3&source=post_page-ad2a414578b3----588141a88598---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----588141a88598--------------------------------)
    ·7 min read·Sep 29, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F588141a88598&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frun-llama-2-70b-on-your-gpu-with-exllamav2-588141a88598&user=Benjamin+Marie&userId=ad2a414578b3&source=-----588141a88598---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fad2a414578b3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frun-llama-2-70b-on-your-gpu-with-exllamav2-588141a88598&user=Benjamin+Marie&userId=ad2a414578b3&source=post_page-ad2a414578b3----588141a88598---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----588141a88598--------------------------------)
    ·7 min read·2023年9月29日 [](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F588141a88598&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frun-llama-2-70b-on-your-gpu-with-exllamav2-588141a88598&user=Benjamin+Marie&userId=ad2a414578b3&source=-----588141a88598---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F588141a88598&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frun-llama-2-70b-on-your-gpu-with-exllamav2-588141a88598&source=-----588141a88598---------------------bookmark_footer-----------)![](../Images/53f04c131fbf146ebbb153e73ad4d27a.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F588141a88598&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frun-llama-2-70b-on-your-gpu-with-exllamav2-588141a88598&source=-----588141a88598---------------------bookmark_footer-----------)![](../Images/53f04c131fbf146ebbb153e73ad4d27a.png)'
- en: Image by the author — Made with an illustration from [Pixabay](https://pixabay.com/vectors/llama-alpaca-animal-mammal-zoo-297668/)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供 — 制作自 [Pixabay](https://pixabay.com/vectors/llama-alpaca-animal-mammal-zoo-297668/)
- en: The largest and best model of the Llama 2 family has 70 billion parameters.
    One fp16 parameter weighs 2 bytes. Loading Llama 2 70B requires 140 GB of memory
    (70 billion * 2 bytes).
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: Llama 2 系列中最大且最优秀的模型拥有 700 亿个参数。一个 fp16 参数占用 2 字节。加载 Llama 2 70B 需要 140 GB 的内存（70
    亿 * 2 字节）。
- en: In a previous article, I showed how you can run a 180-billion-parameter model,
    Falcon 180B, on 100 GB of CPU RAM thanks to quantization.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的一篇文章中，我展示了如何通过量化技术在 100 GB 的 CPU RAM 上运行一个 1800 亿参数的模型 Falcon 180B。
- en: '[](https://newsletter.kaitchup.com/p/falcon-180b-can-it-run-on-your-computer?source=post_page-----588141a88598--------------------------------)
    [## Falcon 180B: Can It Run on Your Computer?'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://newsletter.kaitchup.com/p/falcon-180b-can-it-run-on-your-computer?source=post_page-----588141a88598--------------------------------)
    [## Falcon 180B: 能在你的电脑上运行吗？'
- en: Yes, if you have enough CPU RAM
  id: totrans-12
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 是的，如果你有足够的 CPU RAM
- en: newsletter.kaitchup.com](https://newsletter.kaitchup.com/p/falcon-180b-can-it-run-on-your-computer?source=post_page-----588141a88598--------------------------------)
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '[newsletter.kaitchup.com](https://newsletter.kaitchup.com/p/falcon-180b-can-it-run-on-your-computer?source=post_page-----588141a88598--------------------------------)'
- en: Llama 2 70B is substantially smaller than Falcon 180B.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: Llama 2 70B比Falcon 180B小得多。
- en: '*Can it entirely fit into a single consumer GPU?*'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '*它能完全适配到单个消费级GPU中吗？*'
- en: This is challenging. A high-end consumer GPU, such as the NVIDIA RTX 3090 or
    4090, has 24 GB of VRAM. If we quantize Llama 2 70B to 4-bit precision, we still
    need 35 GB of memory (70 billion * 0.5 bytes). The model could fit into 2 consumer
    GPUs.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 这具有挑战性。一款高端消费级GPU，如NVIDIA RTX 3090或4090，具有24 GB的VRAM。如果我们将Llama 2 70B量化到4位精度，我们仍然需要35
    GB的内存（70亿 * 0.5字节）。该模型可以适配到2个消费级GPU中。
- en: With GPTQ quantization, we can further reduce the precision to 3-bit without
    losing much in the performance of the model. A 3-bit parameter weighs 0.375 bytes
    in memory. Llama 2 70B quantized to 3-bit would still weigh 26.25 GB. It doesn’t
    fit into one consumer GPU.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 通过GPTQ量化，我们可以进一步将精度降低到3位，而不会在模型性能上损失太多。一个3位参数在内存中占用0.375字节。Llama 2 70B量化到3位仍将占用26.25
    GB。这无法适配到一个消费级GPU中。
- en: '[](https://newsletter.kaitchup.com/p/the-best-quantization-methods-to?source=post_page-----588141a88598--------------------------------)
    [## The Best Quantization Methods to Run Llama 3.1 on Your GPU'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://newsletter.kaitchup.com/p/the-best-quantization-methods-to?source=post_page-----588141a88598--------------------------------)
    [## 运行Llama 3.1的最佳量化方法'
- en: Benchmarking inference throughput, accuracy, and memory consumption of AQLM,
    bitsandbytes, AWQ, GPTQ, and AutoRound
  id: totrans-19
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 基准测试AQLM、bitsandbytes、AWQ、GPTQ和AutoRound的推理吞吐量、准确性和内存消耗
- en: newsletter.kaitchup.com](https://newsletter.kaitchup.com/p/the-best-quantization-methods-to?source=post_page-----588141a88598--------------------------------)
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '[newsletter.kaitchup.com](https://newsletter.kaitchup.com/p/the-best-quantization-methods-to?source=post_page-----588141a88598--------------------------------)'
- en: We could reduce the precision to 2-bit. It would fit into 24 GB of VRAM but
    then the performance of the model would also significantly drop according to [previous
    studies on 2-bit quantization](https://arxiv.org/abs/2210.17323).
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将精度降低到2位。这将适配24 GB的VRAM，但根据[之前对2位量化的研究](https://arxiv.org/abs/2210.17323)，模型的性能也会显著下降。
- en: To avoid losing too much in the performance of the model, we could quantize
    important layers, or parts, of the model to a higher precision and the less important
    parts to a lower precision. The model would be quantized with mixed precision.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 为了避免在模型性能上损失过多，我们可以将模型的重要层或部分量化为更高的精度，将不太重要的部分量化为更低的精度。模型将以混合精度进行量化。
- en: '[ExLlamaV2](https://github.com/turboderp/exllamav2) (MIT license) implements
    mixed-precision quantization.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '[ExLlamaV2](https://github.com/turboderp/exllamav2)（MIT许可证）实现了混合精度量化。'
- en: In this article, I show how to use ExLlamaV2 to quantize models with mixed precision.
    More particularly, we will see how to quantize Llama 2 70B to an **average precision**
    lower than 3-bit.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我展示了如何使用ExLlamaV2进行混合精度量化模型。更具体地，我们将看到如何将Llama 2 70B量化到**低于3位的平均精度**。
- en: 'I implemented a notebook demonstrating and benchmarking mixed-precision quantization
    of Llama 2 with ExLlamaV2\. It is available here:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 我实现了一个演示并基准测试Llama 2混合精度量化的笔记本。可以在这里获取：
- en: '[Get the notebook (#18)](https://newsletter.kaitchup.com/p/notebooks)'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '[获取笔记本（#18）](https://newsletter.kaitchup.com/p/notebooks)'
- en: Quantization of Llama 2 with Mixed Precision
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Llama 2的混合精度量化
- en: Requirements
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 要求
- en: To quantize models with mixed precision and run them, we need to install ExLlamaV2.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 要进行混合精度量化并运行模型，我们需要安装ExLlamaV2。
- en: 'Install it from source:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 从源代码安装：
- en: '[PRE0]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: We aim to run models on consumer GPUs.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的目标是让模型在消费级GPU上运行。
- en: 'Llama 2 70B: We target 24 GB of VRAM. NVIDIA RTX3090/4090 GPUs would work.
    If you use Google Colab, you cannot run it on the free Google Colab. Only the
    A100 of Google Colab PRO has enough VRAM.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Llama 2 70B：我们目标是24 GB的VRAM。NVIDIA RTX3090/4090 GPU将适用。如果你使用Google Colab，则不能在免费的Google
    Colab上运行。只有Google Colab PRO的A100具有足够的VRAM。
- en: 'Llama 2 13B: We target 12 GB of VRAM. Many GPUs with at least 12 GB of VRAM
    are available. RTX3060/3080/4060/4080 are some of them. It can run on the free
    Google Colab with the T4 GPU.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Llama 2 13B：我们目标是12 GB的VRAM。许多至少有12 GB VRAM的GPU可用，如RTX3060/3080/4060/4080等。它可以在免费的Google
    Colab上使用T4 GPU运行。
- en: How to quantize with mixed precision using ExLlamaV2
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何使用ExLlamaV2进行混合精度量化
- en: The quantization algorithm used by ExLlamaV2 is similar to GPTQ. But instead
    of choosing one precision type, ExLlamaV2 tries different precision types for
    each layer while measuring quantization errors. All the tries and associated error
    rates are saved. Then, given a target precision provided by the user, the ExLlamaV2
    algorithm will quantize the model by choosing for each layer’s module the quantization
    precision that leads, **on average,** to the target precision with the lowest
    error rate.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: ExLlamaV2 使用的量化算法类似于 GPTQ。但不同于选择一种精度类型，ExLlamaV2 尝试了每一层的不同精度类型，同时测量量化误差。所有尝试和相关的误差率都被保存。然后，给定用户提供的目标精度，ExLlamaV2
    算法会为每层模块选择能**平均**达到目标精度的量化精度，且误差率最低。
- en: 'During quantization, ExLlamaV2 outputs all the tries:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在量化过程中，ExLlamaV2 输出所有尝试：
- en: '*Quantization tries for the 10th layer’s up_proj module of Llama 2 13B*'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '*Llama 2 13B 第 10 层 up_proj 模块的量化尝试*'
- en: '[PRE1]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: We can see that the error rate decreases as the quantization precision (bpw,
    i.e., bits per weight) increases, as expected.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，随着量化精度（bpw，即每重量的位）增加，误差率如预期那样降低。
- en: 'Quantization with ExLlamaV2 is as simple as running the convert.py script:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 ExLlamaV2 进行量化就像运行 convert.py 脚本一样简单：
- en: '*Note: convert.py is in the root directory of ExLlamaV2*'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '*注意：convert.py 在 ExLlamaV2 的根目录下*'
- en: '[PRE2]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: ExLlamaV2 doesn’t support Hugging Face libraries. It expects the model and the
    calibration dataset to be stored locally.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: ExLlamaV2 不支持 Hugging Face 库。它期望模型和校准数据集存储在本地。
- en: 'The script’s main arguments are the following:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 脚本的主要参数如下：
- en: '*input model (-i)*: A local directory that contains the model in the “safetensors”
    format.'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*输入模型 (-i)*：一个包含以“safetensors”格式存储模型的本地目录。'
- en: '*dataset used for calibration (-c)*: We need a dataset for calibrating the
    quantization. It must be stored locally in the “parquet” format.'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*用于校准的 dataset (-c)*：我们需要一个数据集来进行量化校准。它必须以“parquet”格式存储在本地。'
- en: '*output directory (-cf)*: The local directory in which the quantized model
    will be saved.'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*输出目录 (-cf)*：量化模型将被保存的本地目录。'
- en: '*Target precision of the quantization (-b)*: The model will be quantized with
    a mixed precision which will be on average the targeted precision. Here, I chose
    to target a 3-bit precision.'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*量化的目标精度 (-b)*：模型将以混合精度进行量化，平均精度将是目标精度。在这里，我选择了 3 位精度。'
- en: This quantization took 2 hours and 5 minutes. I used Google Colab PRO with the
    T4 GPU and high CPU RAM. It didn’t consume more than 5 GB of VRAM during the entire
    process, but there was a peak consumption of 20 GB of CPU RAM.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 此量化过程花费了 2 小时 5 分钟。我使用了 Google Colab PRO 的 T4 GPU 和高 CPU RAM。在整个过程中，它的 VRAM
    消耗没有超过 5 GB，但 CPU RAM 有高达 20 GB 的峰值消耗。
- en: 'The T4 is quite slow. The quantization time could be reduced with Google Colab
    V100 or an RTX GPU. *Note: It’s unclear to me how much the GPU is used during
    quantization. It might be that the CPU speed has more impact on the quantization
    time than the GPU.*'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: T4 的速度相当慢。使用 Google Colab V100 或 RTX GPU 可以减少量化时间。*注意：我不清楚量化过程中 GPU 的使用情况。可能
    CPU 的速度对量化时间的影响大于 GPU。*
- en: To quantize Llama 2 70B, you can do the same.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 要量化 Llama 2 70B，你可以做同样的操作。
- en: '*What precision should we target so that the quantized Llama 2 70B would fit
    into 24 GB of VRAM?*'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '*我们应该针对什么精度，以便量化后的 Llama 2 70B 能适应 24 GB 的 VRAM？*'
- en: Here is the method you can apply to decide on the precision of a model given
    your hardware.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 这是你可以应用的方法，以决定根据你的硬件选择模型的精度。
- en: Let’s say we have 24 GB of VRAM. We should also always expect some memory overhead
    for inference. So let’s target a quantized model size of 22 GB.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有 24 GB 的 VRAM。我们还应该总是预期一些推理的内存开销。因此，我们的目标量化模型大小为 22 GB。
- en: 'First, we need to convert 22 GB into bits:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要将 22 GB 转换为位：
- en: 22 GB = 2.2e+10 bytes = 1.76e+11 bits (since 1 byte = 8 bits)
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 22 GB = 2.2e+10 bytes = 1.76e+11 bits（因为1字节 = 8位）
- en: We have 1.76e+11 bits (*b*) available. Llama 2 70B has 7e+10 parameters (*p*)
    to be quantized. We target a precision that I denote *bpw*.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有 1.76e+11 位（*b*）可用。Llama 2 70B 有 7e+10 个参数（*p*）需要量化。我们目标的精度是我称之为 *bpw* 的精度。
- en: bpw = b/p
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: bpw = b/p
- en: bpw = 176 000 000 000 / 70 000 000 000 = 2.51
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: bpw = 176 000 000 000 / 70 000 000 000 = 2.51
- en: So we can afford an average precision of 2.51 bits per parameter.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们可以承受每个参数 2.51 位的平均精度。
- en: I round it to 2.5 bits.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 我将其四舍五入到 2.5 位。
- en: 'To quantize Llama 2 70B to an average precision of 2.5 bits, we run:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 要将 Llama 2 70B 量化为平均 2.5 位精度，我们运行：
- en: '[PRE3]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: This quantization is also feasible on consumer hardware with a 24 GB GPU. It
    can take up to 15 hours. If you want to use Google Colab for this one, note that
    you will have to store the original model outside of Google Colab's hard drive
    since it is too small when using the A100 GPU.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 这种量化在配备24 GB GPU的消费级硬件上也是可行的。可能需要长达15小时。如果你打算使用Google Colab进行此操作，请注意，由于A100
    GPU的存储空间过小，你必须将原始模型存储在Google Colab硬盘之外。
- en: Running Llama 2 70B on Your GPU with ExLlamaV2
  id: totrans-66
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在你的GPU上运行Llama 2 70B，使用ExLlamaV2
- en: ExLlamaV2 provides all you need to run models quantized with mixed precision.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: ExLlamaV2提供了运行混合精度量化模型所需的一切。
- en: There is a chat.py script that will run the model as a chatbot for interactive
    use. You can also simply test the model with test_inference.py. This is what we
    will do to check the model speed and memory consumption.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 有一个chat.py脚本，可以将模型作为聊天机器人进行交互使用。你也可以简单地使用test_inference.py测试模型。这是我们将用来检查模型速度和内存消耗的方法。
- en: 'For testing Llama 2 70B quantized with 2.5 bpw, we run:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 为测试量化为2.5 bpw的Llama 2 70B，我们运行：
- en: '[PRE4]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '*Note: “-p” is the testing prompt.*'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '*注意：“-p”是测试提示。*'
- en: 'It should take several minutes (8 minutes on an A100 GPU). ExLlamaV2 uses “[torch.compile](https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html)”.
    According to PyTorch documentation:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 这应该需要几分钟（在A100 GPU上约8分钟）。ExLlamaV2使用“[torch.compile](https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html)”。根据PyTorch文档：
- en: '*torch.compile makes PyTorch code run faster by JIT-compiling PyTorch code
    into optimized kernels, all while requiring minimal code changes.*'
  id: totrans-73
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*torch.compile通过将PyTorch代码即时编译成优化的内核来加速PyTorch代码的运行，同时需要最少的代码更改。*'
- en: This compilation is time-consuming but cached.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 这个编译过程比较耗时，但会被缓存。
- en: If you run test_inference.py, again it should take only 30 seconds.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你运行test_inference.py，通常应该只需30秒。
- en: The model itself weighs exactly 22.15 GB. During my inference experiments, it
    occupied exactly 24 GB. It barely fits on our consumer GPU.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 模型本身的重量正好是22.15 GB。在我的推理实验中，它正好占用了24 GB。它几乎适用于我们的消费级GPU。
- en: '*Why it doesn’t only consume 22.15 GB?*'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '*为什么它不仅仅消耗22.15 GB？*'
- en: The model in memory actually occupies 22.15 GB but the inference itself also
    consumes additional memory. For instance, we have to encode the prompt and store
    it in memory. Also, if you set a higher max sequence length or do batch decoding,
    inference will consume more memory.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 内存中的模型实际占用22.15 GB，但推理本身也会消耗额外的内存。例如，我们必须对提示进行编码并将其存储在内存中。此外，如果你设置了更高的最大序列长度或进行批量解码，推理将消耗更多内存。
- en: I used the A100 of Google Colab for this experiment. If you use a GPU with 24
    GB, you will likely get a CUDA out-of-memory error during inference, especially
    if you also use the GPU to run your OS graphical user interface (e.g., Ubuntu
    Desktop consumes around 1.5 GB of VRAM).
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 我在这个实验中使用了Google Colab的A100。如果你使用24 GB的GPU，你可能会在推理过程中遇到CUDA内存不足错误，尤其是当你还使用GPU运行操作系统图形用户界面（例如，Ubuntu桌面大约消耗1.5
    GB的显存）时。
- en: To give you some margin, targeting a lower bpw. 2.4 or even 2.3 would leave
    several GB of VRAM available for inference.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 为了给你一些余地，目标设置较低的bpw。2.4甚至2.3会留下几GB的显存供推理使用。
- en: ExLlamaV2 models are also extremely fast. I observed a generation speed between
    15 and 30 tokens/second. To give you a point of comparison, when I benchmarked
    Llama 2 7B quantized to 4-bit with GPTQ, a model 10 times smaller, I obtained
    a speed of around 28 tokens/sec using Hugging Face transformers for generation.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: ExLlamaV2模型也非常快速。我观察到生成速度在15到30个token/秒之间。为了给你一个比较点，当我用GPTQ将Llama 2 7B量化为4-bit，一个小10倍的模型时，使用Hugging
    Face transformers进行生成时的速度约为28 tokens/sec。
- en: '[](https://newsletter.kaitchup.com/p/gptq-or-bitsandbytes-which-quantization?source=post_page-----588141a88598--------------------------------)
    [## GPTQ or bitsandbytes: Which Quantization Method to Use for LLMs - Examples
    with Llama 2'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://newsletter.kaitchup.com/p/gptq-or-bitsandbytes-which-quantization?source=post_page-----588141a88598--------------------------------)
    [## GPTQ还是bitsandbytes：LLM使用哪种量化方法 - 以Llama 2为例]'
- en: Large language model quantization for affordable fine-tuning and inference on
    your computer
  id: totrans-83
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 适合在你的计算机上进行经济实惠的微调和推理的大型语言模型量化
- en: newsletter.kaitchup.com](https://newsletter.kaitchup.com/p/gptq-or-bitsandbytes-which-quantization?source=post_page-----588141a88598--------------------------------)
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: newsletter.kaitchup.com](https://newsletter.kaitchup.com/p/gptq-or-bitsandbytes-which-quantization?source=post_page-----588141a88598--------------------------------)
- en: Conclusion
  id: totrans-85
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: Quantization to mixed-precision is intuitive. We aggressively lower the precision
    of the model where it has less impact.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 混合精度量化是直观的。我们在模型的影响较小的地方大幅降低精度。
- en: Running huge models such as Llama 2 70B is possible on a single consumer GPU.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在单个消费级 GPU 上运行大型模型如 Llama 2 70B 是可能的。
- en: Be sure to evaluate your models quantized with different target precisions.
    While larger models are easier to quantize without much performance loss, there
    is always a precision under which the quantized model will become worse than models,
    not quantized, but with fewer parameters, e.g., Llama 2 70B 2-bit could be significantly
    worse than Llama 2 7B 4-bit while still being bigger.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 一定要评估你在不同目标精度下量化的模型。虽然较大的模型在量化时性能损失较少，但总有一种精度下，量化模型的表现会比未量化但参数较少的模型差，例如，Llama
    2 70B 2-bit 可能会显著比 Llama 2 7B 4-bit 表现更差，即使前者更大。
