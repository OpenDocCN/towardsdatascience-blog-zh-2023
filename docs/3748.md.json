["```py\n# Column rename and cast with SQL\ndf = df.selectExpr([f\"{c}::int as {c}_abc\" for c in df.columns])\n\n# Column rename and cast with native spark\nfor c in df.columns:\n    df = df.withColumn(f\"{c}_abc\", F.col(c).cast(\"int\")).drop(c)\n```", "```py\n# Window functions with SQL\ndf.withColumn(\"running_total\", expr(\n  \"sum(value) over (order by id rows between unbounded preceding and current row)\"\n))\n\n# Window functions with native spark\nwindowSpec = Window.orderBy(\"id\").rowsBetween(Window.unboundedPreceding, Window.currentRow)\ndf_with_running_total_native = df.withColumn(\"running_total\", F.sum(\"value\").over(windowSpec))\n```", "```py\n# Read all parquet files in the directory (and subdirectories)\ndf = spark.read.load(\n  \"examples/src/main/resources/dir1\",\n  format=\"parquet\", \n  pathGlobFilter=\"*.parquet\"\n)\n```", "```py\nimport functools\nfrom pyspark.sql import DataFrame\n\npaths = get_file_paths()\n\n# BAD: For loop\nfor path in paths:\n  df = spark.read.load(path)\n  df = fancy_transformations(df)\n  df.write.mode(\"append\").saveAsTable(\"xyz\")\n\n# GOOD: functools.reduce\nlazily_evaluated_reads = [spark.read.load(path) for path in paths]\nlazily_evaluted_transforms = [fancy_transformations(df) for df in lazily_evaluated_reads]\nunioned_df = functools.reduce(DataFrame.union, lazily_evaluted_transforms)\nunioned_df.write.mode(\"append\").saveAsTable(\"xyz\")\n```"]