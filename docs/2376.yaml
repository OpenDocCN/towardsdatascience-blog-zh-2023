- en: Chain of Thought Prompting for LLMs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/chain-of-thought-prompting-for-llms-33c963eead38?source=collection_archive---------4-----------------------#2023-07-24](https://towardsdatascience.com/chain-of-thought-prompting-for-llms-33c963eead38?source=collection_archive---------4-----------------------#2023-07-24)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: A practical and simple approach for “reasoning” with LLMs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://wolfecameron.medium.com/?source=post_page-----33c963eead38--------------------------------)[![Cameron
    R. Wolfe, Ph.D.](../Images/52bb88d7cf1105501be2fae5ccbe7a03.png)](https://wolfecameron.medium.com/?source=post_page-----33c963eead38--------------------------------)[](https://towardsdatascience.com/?source=post_page-----33c963eead38--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----33c963eead38--------------------------------)
    [Cameron R. Wolfe, Ph.D.](https://wolfecameron.medium.com/?source=post_page-----33c963eead38--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F28aa6026c553&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fchain-of-thought-prompting-for-llms-33c963eead38&user=Cameron+R.+Wolfe%2C+Ph.D.&userId=28aa6026c553&source=post_page-28aa6026c553----33c963eead38---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----33c963eead38--------------------------------)
    ·16 min read·Jul 24, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F33c963eead38&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fchain-of-thought-prompting-for-llms-33c963eead38&user=Cameron+R.+Wolfe%2C+Ph.D.&userId=28aa6026c553&source=-----33c963eead38---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F33c963eead38&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fchain-of-thought-prompting-for-llms-33c963eead38&source=-----33c963eead38---------------------bookmark_footer-----------)![](../Images/29d7dcc5bce6b373e4e07c854001e445.png)'
  prefs: []
  type: TYPE_NORMAL
- en: (Photo by [Matthew Lancaster](https://unsplash.com/@matthewelancaster?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    on [Unsplash](https://unsplash.com/s/photos/chain?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText))
  prefs: []
  type: TYPE_NORMAL
- en: The success of large language models (LLMs) stems from our ability to pre-train
    (using a [language modeling objective](https://cameronrwolfe.substack.com/i/85568430/language-modeling))
    [decoder-only transformer](https://twitter.com/cwolferesearch/status/1640446111348555776?s=20)
    models across massive textual corpora. Given that we pre-train sufficiently large
    models, LLMs are incredibly capable [few-shot learners](https://cameronrwolfe.substack.com/i/88082618/language-models-are-few-shot-learners).
    In other words, this means that we can solve a variety of different problems (e.g.,
    translation, sentence classification, summarization, etc.) by just formulating
    a textual prompt (potentially containing a few example of correct output) and
    having the LLM generate the correct answer.
  prefs: []
  type: TYPE_NORMAL
- en: Despite the power of LLMs, there are some problems that these models consistently
    struggle to solve. In particular, reasoning problems (e.g., arithmetic or commonsense
    reasoning) are notoriously difficult. Initial attempts to solve this issue explored
    fine-tuning LLMs and task-specific verification modules over a supervised dataset
    of solutions and explanations of various reasoning problems [3, 4]. However, recent
    work has found that few-shot learning can be leveraged for an easier solution.
  prefs: []
  type: TYPE_NORMAL
- en: “The goal of this paper is to endow language models with the ability to generate
    a chain of thought — a coherent series of intermediate reasoning steps that lead
    to the final answer for a…
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
