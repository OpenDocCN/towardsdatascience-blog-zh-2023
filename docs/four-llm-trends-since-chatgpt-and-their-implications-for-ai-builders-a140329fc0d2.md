# 自ChatGPT以来的四个LLM趋势及其对AI构建者的影响

> 原文：[https://towardsdatascience.com/four-llm-trends-since-chatgpt-and-their-implications-for-ai-builders-a140329fc0d2?source=collection_archive---------2-----------------------#2023-05-29](https://towardsdatascience.com/four-llm-trends-since-chatgpt-and-their-implications-for-ai-builders-a140329fc0d2?source=collection_archive---------2-----------------------#2023-05-29)

[](https://medium.com/@janna.lipenkova_52659?source=post_page-----a140329fc0d2--------------------------------)[![Dr. Janna Lipenkova](../Images/112fe9a8c5936869243f2a43fde6dfee.png)](https://medium.com/@janna.lipenkova_52659?source=post_page-----a140329fc0d2--------------------------------)[](https://towardsdatascience.com/?source=post_page-----a140329fc0d2--------------------------------)[![Towards Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----a140329fc0d2--------------------------------) [Dr. Janna Lipenkova](https://medium.com/@janna.lipenkova_52659?source=post_page-----a140329fc0d2--------------------------------)

·

[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ff215f8e427a2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffour-llm-trends-since-chatgpt-and-their-implications-for-ai-builders-a140329fc0d2&user=Dr.+Janna+Lipenkova&userId=f215f8e427a2&source=post_page-f215f8e427a2----a140329fc0d2---------------------post_header-----------) 发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----a140329fc0d2--------------------------------) ·15分钟阅读·2023年5月29日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fa140329fc0d2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffour-llm-trends-since-chatgpt-and-their-implications-for-ai-builders-a140329fc0d2&user=Dr.+Janna+Lipenkova&userId=f215f8e427a2&source=-----a140329fc0d2---------------------clap_footer-----------)

--

[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fa140329fc0d2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffour-llm-trends-since-chatgpt-and-their-implications-for-ai-builders-a140329fc0d2&source=-----a140329fc0d2---------------------bookmark_footer-----------)![](../Images/50eaefd7c0e07824bb0e6a17a5a93792.png)

表1：截至2023年5月的热门LLM（按提及数量排序）。提及数量、趋势和下游任务的适用性是从超过50万份与AI相关的在线文档中计算得出的，这些文档包括商业媒体、普通新闻、AI博客和科学出版物。任务适用性通过语义嵌入和模型与NLP任务之间的潜在关联强度进行计算。

2022年10月，我发布了一篇[关于特定NLP用例的LLM选择的文章](/choosing-the-right-language-model-for-your-nlp-use-case-1288ef3c4929)，涉及对话、翻译和摘要等内容。从那时起，AI已经取得了巨大的进步，在这篇文章中，我们将回顾过去几个月的一些趋势以及它们对AI构建者的影响。具体而言，我们将涵盖自回归模型的任务选择、商业和开源LLMs之间不断变化的权衡，以及LLM的集成和生产中故障的缓解。

# 1\. 生成AI推动了自回归模型，而自动编码模型则在等待它们的时机。

对于许多AI公司来说，ChatGPT似乎已成为**终极**竞争者。在早期推介我的分析初创公司时，我经常面临挑战：“如果谷歌（Facebook、阿里巴巴、Yandex等）突然出现并做同样的事情，你会怎么做？”现在，当前的问题是：“为什么你不能用ChatGPT来做这个？”

简短的回答是：ChatGPT在很多方面表现出色，但远未覆盖AI的全部领域。目前的热潮主要集中在**生成**AI上——而不是分析型AI，或者其相对较新的分支——合成AI[1]。这对LLMs意味着什么？正如我在[上一篇文章](https://medium.com/towards-data-science/choosing-the-right-language-model-for-your-nlp-use-case-1288ef3c4929)中描述的，LLMs可以通过三种目标进行预训练——自回归、自动编码和序列到序列（参见表1，列“预训练目标”）。通常，一个模型会以这些目标中的一个进行预训练，但也有例外——例如，UniLM[2]在所有三种目标上进行了预训练。近几个月来使AI流行的有趣生成任务包括对话、问答和内容生成——这些任务中，模型确实学习“生成”下一个标记、句子等。这些任务最适合由自回归模型执行，包括GPT家族以及大多数近期的开源模型，如MPT-7B、OPT和Pythia。自动编码模型更适合信息提取、蒸馏和其他分析任务，虽然它们处于背景中——但不要忘记，初次突破LLM的2018年是通过BERT这款自动编码模型实现的。虽然这对现代AI来说可能感觉像是石器时代，但自动编码模型对于许多B2B用例特别相关，其中重点是提炼出针对特定业务任务的简明见解。我们确实可能会看到围绕自动编码的另一波浪潮，以及一代新的LLMs，在提取和综合信息以进行分析方面表现卓越。

对于构建者来说，这意味着流行的自回归模型可以用于所有内容生成的任务——内容越长，效果越好。然而，对于分析任务，你应该仔细评估所使用的自回归大型语言模型（LLM）是否能输出令人满意的结果，并考虑使用自动编码模型或更传统的自然语言处理方法。

# 2\. 开源与商业公司的竞争，推动了LLM效率和扩展性的创新。

在过去几个月中，关于开源与商业AI之间的复杂关系有很多讨论。短期内，开源社区无法在需要大量数据和/或计算资源的竞赛中跟上。然而，从长远来看，即使是像谷歌和OpenAI这样的公司也感受到开源的威胁。[3] 这种紧张关系促使双方继续发展，最终取得的进展逐渐汇聚成富有成果的协同效应。开源社区注重节俭，即通过更少的资源提高LLM的效率。这不仅使LLM对更广泛的用户群体更具负担能力——即AI民主化——而且从环境角度来看也更具可持续性。LLM变得更高效的主要维度有三个：

+   **更少的计算和内存**：例如，FlashAttention [4] 允许减少GPU上的读写次数，相比于标准的注意力算法，从而实现更快且内存高效的微调。

+   **更少的参数**：在标准微调中，所有模型权重都会被重新训练——然而，在大多数情况下，只有一小部分权重影响模型在微调数据上的表现。参数高效微调（PEFT）识别这一子集并“冻结”其他权重，这可以大幅减少资源使用，同时实现更稳定的模型表现。

+   **更少的训练数据**：数据质量比数据规模更具可扩展性[3]——你的训练数据越集中和精心策划，优化性能所需的数据量就越少。其中一种最成功的方法是指令微调。在训练过程中，LLM会被提供任务特定的指令，这些指令反映了它在推理过程中最终会如何被提示。缩小训练范围可以从更少的数据中更快地学习。指令微调已经被应用了一段时间，例如在T0、FLAN、InstructGPT中——最终，它也是ChatGPT所基于的方法。

另一方面，目前，“生成性人工智能的控制权掌握在那些能够负担得起训练和大规模部署模型的少数人手中”。[5] 商业产品的规模正在迅速膨胀——无论是模型规模、数据规模还是训练时间——在输出质量方面明显优于开源模型。技术上这里没有太多可以报告的内容——而更多的担忧在于治理和监管。因此，“一个关键风险是像GPT这样的强大LLM可能只朝着符合这些公司商业目标的方向发展。”[5]

这两端将如何交汇——它们会交汇吗？一方面，任何能够减少资源消耗的技巧最终都可以通过投入更多资源来扩大规模。另一方面，LLM的训练遵循幂律，这意味着随着模型规模、数据集规模和训练时间的增加，学习曲线会变得平缓。[6] 你可以将其理解为人类教育的类比——在人类历史的长河中，学校教育时间有所增加，但普通人的智力和博学是否也随之增加？

学习曲线平缓的积极之处在于它在对人工智能“变得比人类更强大和聪明”的担忧中带来的缓解。但请做好准备——LLM领域充满了惊喜，其中最不可预测的之一是**涌现**。[7] 涌现是指系统中的定量变化导致行为上的定性变化——总结为“量变引起质变”，或者简单地说“更多即不同”。[8] 在训练的某个阶段，LLM似乎会获得一些新的、意想不到的能力，这些能力不在原始训练范围内。目前，这些能力表现为新的语言技能——例如，模型突然学会了总结或翻译，而不仅仅是生成文本。无法预测何时会发生这种情况，以及这些新能力的性质和范围是什么。因此，虽然涌现现象对研究人员和未来学家来说很吸引人，但在商业背景下仍然远未提供稳健的价值。

随着越来越多的方法被开发出来以提高LLM微调和推理的效率，围绕开源LLM物理操作的资源瓶颈似乎正在放松。由于商业LLM的高使用成本和限制配额，越来越多的公司考虑部署自己的LLM。然而，开发和维护成本依然存在，大多数描述的优化也需要扩展的技术技能来操作模型及其部署的硬件。选择开源还是商业LLM是一个战略性决策，应在仔细探索包括成本（包括开发、运营和使用成本）、可用性、灵活性和性能在内的一系列权衡后做出。一个常见的建议是先使用大型商业LLM来快速验证最终产品的商业价值，然后再“切换”到开源LLM。然而，这种过渡可能会很艰难，甚至不现实，因为LLM在擅长的任务上有很大差异。存在着开源模型无法满足你已开发应用需求的风险，或者你需要做大量修改以缓解相关权衡。最后，对于那些在LLM上构建各种功能的公司，最先进的设置是多LLM架构，这允许利用不同LLM的优势。

# 3\. LLM正在通过插件、代理和框架实现操作。

LLM训练中的重大挑战已基本解决，另一项工作重点是将LLM集成到现实世界的产品中。除了提供提高开发者便利性的现成组件外，这些创新还帮助克服现有LLM的限制，并通过推理和使用非语言数据等附加能力来丰富它们。[9] 基本思想是，尽管LLM在模拟人类语言能力方面已经很出色，但它们仍需置于更广泛的计算“认知”背景中，以进行更复杂的推理和执行。这种认知包括推理、行动和观察环境等多种不同能力。目前，这通过插件和代理来近似实现，这些插件和代理可以通过LangChain、LlamaIndex和AutoGPT等模块化LLM框架进行组合。

## 3.1 插件提供对外部数据和功能的访问

预训练的LLM在利用数据时存在显著的实际限制：一方面，数据很快就会过时——例如，尽管GPT-4于2023年发布，其数据却截止于2021年。另一方面，大多数现实世界的应用需要对LLM中的知识进行一些定制。考虑构建一个允许你创建个性化营销内容的应用——你可以向LLM提供关于你的产品和具体用户的信息，结果会更好。插件使这一点成为可能——你的程序可以从外部源（如客户电子邮件和通话记录）获取数据，并将这些数据插入到提示中，以生成个性化的、受控的输出。

## 3.2 代理人言行一致

语言与可操作性紧密相关。我们的交流意图常常围绕行动展开，例如当我们要求别人做某事或拒绝以某种方式行动时。计算机程序也是如此，它们可以被视为执行特定操作的函数集合，当满足特定条件时会阻止这些操作等。基于LLM的代理人将这两个世界结合在一起。这些代理人的指令不是用编程语言硬编码的，而是由LLM以推理链的形式自由生成，这些推理链引导实现给定目标。每个代理都有一组插件，并可以根据推理链的需要进行调整——例如，它可以结合一个用于检索特定信息的搜索引擎和一个用于随后对这些信息进行计算的计算器。代理人的概念在强化学习中存在已久——然而，到今天为止，强化学习仍发生在相对封闭和安全的环境中。凭借LLM的广泛常识，代理人不仅可以进入“广阔的世界”，还可以发挥无尽的组合潜力：每个代理可以执行多种任务以实现其目标，多个代理可以互相互动和合作。[10] 此外，代理人从与世界的互动中学习，建立的记忆比LLM的纯语言记忆更接近人类的多模态记忆。

## 3.3 框架提供了LLM集成的便捷接口

在过去几个月里，我们见证了一系列基于LLM的新框架，如LangChain、AutoGPT和LlamaIndex。这些框架允许将插件和代理集成到复杂的生成和操作链中，以实现包括多步骤推理和执行的复杂过程。开发人员现在可以专注于高效的提示工程和快速应用原型设计。[11] 目前，使用这些框架时仍然有很多硬编码的工作——但逐渐地，它们可能会向更全面和灵活的系统发展，比如Yann LeCun提出的JEPA架构。[12]

这些新组件和框架对构建者有何影响？一方面，它们通过增强外部数据和能力提升了 LLM 的潜力。框架与方便的商业 LLM 结合，将应用原型制作缩短到了几天。但 LLM 框架的兴起也对 LLM 层有影响。它现在隐藏在额外的抽象层之后，任何抽象层都需要更高的意识和纪律，以可持续的方式利用。首先，在生产开发时，仍需要一个结构化的过程来评估和选择适合当前任务的 LLM。目前，许多公司在假设 OpenAI 提供的最新模型最合适的情况下跳过了这个过程。其次，LLM 的选择应该与期望的代理行为协调：期望的行为越复杂和灵活，LLM 应该表现得越好，以确保它能在广泛的选项空间中选择正确的行动。[13] 最后，在操作中，MLOps 管道应该确保模型不会偏离变化的数据分布和用户偏好。

# 4\. LLM 的语言接口为人机交互带来了新的挑战。

随着提示技术的进步，使用 AI 来做炫酷和创造性的事情对于非技术人员变得越来越可行。不再需要成为程序员——只需使用语言，我们的自然沟通媒介，来告诉机器该做什么。然而，在所有围绕快速原型制作和 LLM 实验的热潮和兴奋中，我们仍然会发现“用 LLM 做些炫酷的东西很容易，但要让它们达到生产级的准备却非常困难。”[14] 在生产环境中，LLM 会出现幻觉，对不完美的提示设计很敏感，并且在治理、安全和与期望结果的一致性方面提出了许多问题。我们最喜欢 LLM 的地方——它开放的输入和输出空间——也使得在部署到生产环境之前，更难以测试潜在的故障。

## 4.1 幻觉和沉默的失败

如果你曾经构建过 AI 产品，你会知道最终用户通常对 AI 故障非常敏感。用户倾向于“负面偏见”：即使你的系统整体准确性很高，那些偶尔但不可避免的错误情况也会被放大镜审视。对于 LLM 来说，情况有所不同。就像任何其他复杂的 AI 系统一样，LLM 也会失败——但它们以沉默的方式失败。即使它们没有一个好的回应，它们仍然会生成某些东西，并以高度自信的方式展示出来，欺骗我们相信和接受它们，并让我们在后续过程中陷入尴尬的境地。想象一下，一个由 LLM 生成指令的多步骤代理——第一步的错误将级联到所有后续任务中，并破坏代理的整个行动序列。

大型语言模型（LLMs）面临的最大质量问题之一是幻觉，指的是生成在语义上或语法上看似合理但事实上却不正确的文本。早在诺姆·乔姆斯基通过他著名的句子“[无色的绿色思想愤怒地睡觉](https://en.wikipedia.org/wiki/Colorless_green_ideas_sleep_furiously)”中，就指出了一个句子从语言学角度看可能完美无缺，但对人类而言却完全毫无意义的观点。但对于LLMs而言情况不同，它们缺乏人类所拥有的非语言知识，因此无法将语言与基础世界的现实相结合。虽然我们能立即发现乔姆斯基句子中的问题，但一旦进入我们专业领域之外的更专业领域，验证LLM输出的准确性会变得相当繁琐。对于长篇内容以及没有真实依据的交互，如预测和开放式的科学或哲学问题，未被发现的幻觉风险尤其高。

对于幻觉有多种处理方法。从统计学的角度来看，我们可以期待随着语言模型的学习越来越多，幻觉现象会减少。但在商业环境中，这种“解决方案”的增量性和不确定的时间线使得它相当不可靠。另一种方法基于神经符号AI。通过结合统计语言生成和确定性世界知识的力量，我们或许能够减少幻觉和隐性失败，最终使大规模生产中的大型语言模型（LLM）更加稳健。例如，ChatGPT通过集成Wolfram Alpha这一庞大的结构化知识数据库来兑现这一承诺。

## 4.2 提示的挑战

表面上，提示提供的自然语言界面似乎缩小了 AI 专家和外行之间的差距——毕竟，我们都知道至少一种语言并用它进行沟通，那么为何不在 LLM 中做同样的事？但提示是一门精细的工艺。成功的提示不仅需要强大的语言直觉，还需要对 LLM 的学习和工作原理有深入了解。而且，设计成功的提示过程是高度迭代的，需要系统的实验。如论文 [Why Johnny can’t prompt](https://dl.acm.org/doi/abs/10.1145/3544548.3581388) 所示，人类很难保持这种严格性。一方面，我们常常被根植于人际互动经验中的期望所影响。与人交谈不同于与 LLM 交谈——当我们互相交流时，我们的输入是通过丰富的情境背景传递的，这使我们能够中和人类语言中的不精确性和模糊性。LLM 只接收到语言信息，因此宽容度要小得多。另一方面，采用系统化的方法进行提示工程很困难，因此我们很快陷入机会主义的试错中，难以构建一个可扩展且一致的提示系统。

解决这些挑战需要教育提示工程师和用户了解学习过程和 LLM 的失败模式，并保持对界面中可能出现的错误的意识。应该清楚，LLM 的输出总是具有不确定性的。例如，可以通过模型校准得出的置信度分数在用户界面中实现这一点。[15] 对于提示工程，我们目前看到 LLMOps 的兴起，这是 MLOps 的一个子类别，允许通过提示模板、版本控制、优化等管理提示生命周期。最后，在一致性方面，微调胜过少量学习，因为它消除了即兴提示的“人为因素”并丰富了 LLM 的内在知识。只要在你的设置中有可能，你应该考虑在积累了足够的训练数据后从提示切换到微调。

# 结论

随着新模型、性能优化和集成每天都在出现，LLM 的深度正在不断加深。对公司而言，保持差异化、关注近期发展和新风险以及偏向实际操作实验而非热点是重要的——许多与 LLM 相关的权衡和问题只有在实际使用中才会显现。本文探讨了近期的发展及其对 LLM 构建的影响：

+   **大多数当前的 LLM 是自回归的，在生成任务中表现出色**。它们在分析任务中可能不可靠，此时应优先考虑自编码 LLM 或其他 NLP 技术。

+   **开源LLM和商业LLM之间存在显著差异，切换LLM可能比预期的更困难**。仔细考虑权衡，评估可能的发展路径（从开源开始，随后切换到商业LLM），并考虑如果你产品的不同特性依赖于LLM时，是否考虑多LLM设置。

+   **框架提供了一个便捷的接口来构建LLM，但不要低估LLM层的重要性** — LLM应经过实验和细致挑选的过程，然后通过完整的MLOps周期，以确保稳健、持续优化的操作，并减轻诸如模型漂移等问题。

+   **构建者应主动管理人类因素**。LLM已经征服了语言，这是一个最初仅人类能够接触的认知领域。作为人类，我们很快忘记LLM仍然是“机器”，并未能如对待机器一样操作它们。对于用户和员工，考虑如何提高他们的意识，并教育他们正确操作和使用LLM。

# 参考文献

[1] 安德森·霍洛维茨。2023年。 [对于B2B生成AI应用程序，少即是多吗](https://a16z.com/2023/03/30/b2b-generative-ai-synthai/)？

[2] 李栋等。2019年。统一语言模型预训练用于自然语言理解和生成。在第33届国际神经信息处理系统会议论文集中，第13063–13075页。

[3] 信息日报。2023年。 [谷歌研究员：公司在AI领域没有“护城河”](https://www.theinformation.com/briefings/google-researcher-memo-company-has-no-moat-in-ai)。

[4] 特里·道等。2022年。 [FlashAttention：具有IO感知的快速且内存高效的精确注意力](https://arxiv.org/abs/2205.14135)。

[5] EE Times。2023年。 [开源LLM能解决AI民主化问题吗](https://www.eetimes.com/can-open-source-llms-solve-ais-democratization-problem/)？

[6] 贾雷德·卡普兰等。2023年。 [神经语言模型的扩展定律](https://arxiv.org/abs/2001.08361)。

[7] 杰森·魏等。2023年。 [大型语言模型的涌现能力](https://arxiv.org/abs/2206.07682)。

[8] 菲利普·安德森。1972年。更多即不同。在《科学》，第177卷，第4047期，第393–396页。

[9] 贾娜·利朋科娃。2023年。 [克服大型语言模型的局限性](https://medium.com/towards-data-science/overcoming-the-limitations-of-large-language-models-9d4e92ad9823)。

[10] 俊尚·朴等。2023年。 [生成代理：人类行为的互动模拟](https://arxiv.org/pdf/2304.03442.pdf)

[11] 哈佛大学。2023年。GPT-4 — [它是如何工作的，我如何用它构建应用程序？ — CS50技术讲座](https://www.youtube.com/live/vw-KWfKwvTQ)。

[12] 扬·勒昆。2022年。 [走向自主机器智能的路径](https://openreview.net/pdf?id=BZ5a1r-kVsf)。

[13] 杰瑞·刘。2023年。 [更笨的LLM代理需要更多的约束和更好的工具](https://medium.com/llamaindex-blog/dumber-llm-agents-need-more-constraints-and-better-tools-17a524c59e12)。

[14] Chip Huyen. 2023\. [构建生产环境中的LLM应用](https://huyenchip.com/2023/04/11/llm-engineering.html)。

[15] Stephanie Lin 等. 2022\. [教模型用语言表达它们的不确定性](https://arxiv.org/abs/2205.14334)。
