["```py\nno_links_section = re.sub(r\"<[^>]+>_?\",\"\", section)\n```", "```py\n.. _brain-embeddings-visualization:\n\nVisualizing embeddings\n______________________\n\nThe FiftyOne Brain provides a powerful\n:meth:`compute_visualization() <fiftyone.brain.compute_visualization>` method\nthat you can use to generate low-dimensional representations of the samples\nand/or individual objects in your datasets.\n\nThese representations can be visualized natively in the App's\n:ref:`Embeddings panel <app-embeddings-panel>`, where you can interactively\nselect points of interest and view the corresponding samples/labels of interest\nin the :ref:`Samples panel <app-samples-panel>`, and vice versa.\n\n.. image:: /images/brain/brain-mnist.png\n   :alt: mnist\n   :align: center\n\nThere are two primary components to an embedding visualization: the method used\nto generate the embeddings, and the dimensionality reduction method used to\ncompute a low-dimensional representation of the embeddings.\n\nEmbedding methods\n-----------------\n\nThe `embeddings` and `model` parameters of\n:meth:`compute_visualization() <fiftyone.brain.compute_visualization>`\nsupport a variety of ways to generate embeddings for your data:\n```", "```py\n.. list-table::\n\n   * - :meth:`match() <fiftyone.core.collections.SampleCollection.match>`\n   * - :meth:`match_frames() <fiftyone.core.collections.SampleCollection.match_frames>`\n   * - :meth:`match_labels() <fiftyone.core.collections.SampleCollection.match_labels>`\n   * - :meth:`match_tags() <fiftyone.core.collections.SampleCollection.match_tags>`\n```", "```py\n+-----------------------------------------+-----------------------------------------------------------------------+\n| Operation                               | Command                                                               |\n+=========================================+=======================================================================+\n| Filepath starts with \"/Users\"           |  .. code-block::                                                      |\n|                                         |                                                                       |\n|                                         |     ds.match(F(\"filepath\").starts_with(\"/Users\"))                     |\n+-----------------------------------------+-----------------------------------------------------------------------+\n| Filepath ends with \"10.jpg\" or \"10.png\" |  .. code-block::                                                      |\n|                                         |                                                                       |\n|                                         |     ds.match(F(\"filepath\").ends_with((\"10.jpg\", \"10.png\"))            |\n+-----------------------------------------+-----------------------------------------------------------------------+\n| Label contains string \"be\"              |  .. code-block::                                                      |\n|                                         |                                                                       |\n|                                         |     ds.filter_labels(                                                 |\n|                                         |         \"predictions\",                                                |\n|                                         |         F(\"label\").contains_str(\"be\"),                                |\n|                                         |     )                                                                 |\n+-----------------------------------------+-----------------------------------------------------------------------+\n| Filepath contains \"088\" and is JPEG     |  .. code-block::                                                      |\n|                                         |                                                                       |\n|                                         |     ds.match(F(\"filepath\").re_match(\"088*.jpg\"))                      |\n+-----------------------------------------+-----------------------------------------------------------------------+\n```", "```py\nimport json\nifile = \"my_notebook.ipynb\"\nwith open(ifile, \"r\") as f:\n    contents = f.read()\ncontents = json.loads(contents)[\"cells\"]\ncontents = [(\" \".join(c[\"source\"]), c['cell_type'] for c in contents]\n```", "```py` ```", "```py`before and after. This also made it easy to split into text and code.\n2.  **Still contained anchors:** unlike raw RST, this Markdown included section heading anchors, as the implicit anchors had already been generated. This way, I could link not just to the page containing the result, but to the specific section or subsection of that page.\n3.  **Standardization**: Markdown provided a mostly uniform formatting for the initial RST and Jupyter documents, allowing us to give their content consistent treatment in the vector search application.\n\n## Note on LangChain\n\nSome of you may know about the open source library [LangChain](https://python.langchain.com/en/latest/index.html) for building applications with LLMs, and may be wondering why I didn’t just use LangChain’s [Document Loaders](https://python.langchain.com/en/latest/modules/indexes/document_loaders.html) and [Text Splitters](https://python.langchain.com/en/latest/modules/indexes/text_splitters.html). The answer: I needed more control!\n\n# Processing the documents\n\nOnce the documents had been converted to Markdown, I proceeded to clean the contents and split them into smaller segments.\n\n## Cleaning\n\nCleaning most consisting in removing unnecessary elements, including:\n\n*   Headers and footers\n*   Table row and column scaffolding — e.g. the `|`’s in `|select()| select_by()|`\n*   Extra newlines\n*   Links\n*   Images\n*   Unicode characters\n*   Bolding — i.e. `**text**` → `text`\n\nI also removed escape characters that were escaping from characters which have special meaning in our docs: `_` and `*`. The former is used in many method names, and the latter, as usual, is used in multiplication, regex patterns, and many other places:\n\n```", "```py\n\n## Splitting documents into semantic blocks\n\nWith the contents of our docs cleaned, I proceeded to split the docs into bite-sized blocks.\n\nFirst, I split each document into sections. At first glance, it seems like this can be done by finding any line that starts with a `#` character. In my application, I did not differentiate between h1, h2, h3, and so on (`#` , `##` , `###`), so checking the first character is sufficient. However, this logic gets us in trouble when we realize that `#` is also employed to allow comments in Python code.\n\nTo bypass this problem, I split the document into text blocks and code blocks:\n\n```", "```py')\ntext = text_and_code[::2]\ncode = text_and_code[1::2]\n```", "```py\ndef extract_title_and_anchor(header):\n    header = \" \".join(header.split(\" \")[1:])\n    title = header.split(\"[\")[0]\n    anchor = header.split(\"(\")[1].split(\" \")[0]\n    return title, anchor\n```", "```py\nexport OPENAI_API_KEY=\"<MY_API_KEY>\"\n```", "```py\npip install openai\n```", "```py\nMODEL = \"text-embedding-ada-002\"\n\ndef embed_text(text):\n    response = openai.Embedding.create(\n        input=text,\n        model=MODEL\n    )\n    embeddings = response['data'][0]['embedding']\n    return embeddings\n```", "```py\ndocker pull qdrant/qdrant\ndocker run -d -p 6333:6333 qdrant/qdrant\n```", "```py\npip install qdrant-client\n```", "```py\nimport qdrant_client as qc\nimport qdrant_client.http.models as qmodels\n\nclient = qc.QdrantClient(url=\"localhost\")\nMETRIC = qmodels.Distance.DOT\nDIMENSION = 1536\nCOLLECTION_NAME = \"fiftyone_docs\"\n\ndef create_index():\n    client.recreate_collection(\n    collection_name=COLLECTION_NAME,\n    vectors_config = qmodels.VectorParams(\n            size=DIMENSION,\n            distance=METRIC,\n        )\n    )\n```", "```py\nimport uuid\ndef create_subsection_vector(\n    subsection_content,\n    section_anchor,\n    page_url,\n    doc_type\n    ):\n\n    vector = embed_text(subsection_content)\n    id = str(uuid.uuid1().int)[:32]\n    payload = {\n        \"text\": subsection_content,\n        \"url\": page_url,\n        \"section_anchor\": section_anchor,\n        \"doc_type\": doc_type,\n        \"block_type\": block_type\n    }\n    return id, vector, payload\n```", "```py\ndef add_doc_to_index(subsections, page_url, doc_type, block_type):\n    ids = []\n    vectors = []\n    payloads = []\n\n    for section_anchor, section_content in subsections.items():\n        for subsection in section_content:\n            id, vector, payload = create_subsection_vector(\n                subsection,\n                section_anchor,\n                page_url,\n                doc_type,\n                block_type\n            )\n            ids.append(id)\n            vectors.append(vector)\n            payloads.append(payload)\n\n    ## Add vectors to collection\n    client.upsert(\n        collection_name=COLLECTION_NAME,\n        points=qmodels.Batch(\n            ids = ids,\n            vectors=vectors,\n            payloads=payloads\n        ),\n    )\n```", "```py\ndef _generate_query_filter(query, doc_types, block_types):\n    \"\"\"Generates a filter for the query.\n    Args:\n        query: A string containing the query.\n        doc_types: A list of document types to search.\n        block_types: A list of block types to search.\n    Returns:\n        A filter for the query.\n    \"\"\"\n    doc_types = _parse_doc_types(doc_types)\n    block_types = _parse_block_types(block_types)\n\n    _filter = models.Filter(\n        must=[\n            models.Filter(\n                should= [\n                    models.FieldCondition(\n                        key=\"doc_type\",\n                        match=models.MatchValue(value=dt),\n                    )\n                for dt in doc_types\n                ],\n\n            ),\n            models.Filter(\n                should= [\n                    models.FieldCondition(\n                        key=\"block_type\",\n                        match=models.MatchValue(value=bt),\n                    )\n                for bt in block_types\n                ]  \n            )\n        ]\n    )\n\n    return _filter\n```", "```py\ndef query_index(query, top_k=10, doc_types=None, block_types=None):\n    vector = embed_text(query)\n    _filter = _generate_query_filter(query, doc_types, block_types)\n\n    results = CLIENT.search(\n        collection_name=COLLECTION_NAME,\n        query_vector=vector,\n        query_filter=_filter,\n        limit=top_k,\n        with_payload=True,\n        search_params=_search_params,\n    )\n\n    results = [\n        (\n            f\"{res.payload['url']}#{res.payload['section_anchor']}\",\n            res.payload[\"text\"],\n            res.score,\n        )\n        for res in results\n    ]\n\n    return results\n```", "```py\nfrom fiftyone.docs_search import FiftyOneDocsSearch\nfosearch = FiftyOneDocsSearch(open_url=False, top_k=3, score=True)\n```", "```py\nfosearch(“How to load a dataset”)\n```", "```py\nfiftyone-docs-search query \"<my-query>\" <args \n```", "```py\nalias fosearch='fiftyone-docs-search query'\n```", "```py\nfosearch \"<my-query>\" args\n```"]