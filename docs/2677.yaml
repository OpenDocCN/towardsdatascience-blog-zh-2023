- en: An Accessible Derivation of Linear Regression
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/an-accessible-derivation-of-linear-regression-69fa6aefbd93?source=collection_archive---------8-----------------------#2023-08-23](https://towardsdatascience.com/an-accessible-derivation-of-linear-regression-69fa6aefbd93?source=collection_archive---------8-----------------------#2023-08-23)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The math behind the model, from additive assumptions to pseudoinverse matrices
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://william-caicedo.medium.com/?source=post_page-----69fa6aefbd93--------------------------------)[![William
    Caicedo-Torres, PhD](../Images/06123561570e6a998e8d1c47808fc6e2.png)](https://william-caicedo.medium.com/?source=post_page-----69fa6aefbd93--------------------------------)[](https://towardsdatascience.com/?source=post_page-----69fa6aefbd93--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----69fa6aefbd93--------------------------------)
    [William Caicedo-Torres, PhD](https://william-caicedo.medium.com/?source=post_page-----69fa6aefbd93--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F7a925ed40bb4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fan-accessible-derivation-of-linear-regression-69fa6aefbd93&user=William+Caicedo-Torres%2C+PhD&userId=7a925ed40bb4&source=post_page-7a925ed40bb4----69fa6aefbd93---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----69fa6aefbd93--------------------------------)
    ·9 min read·Aug 23, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F69fa6aefbd93&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fan-accessible-derivation-of-linear-regression-69fa6aefbd93&user=William+Caicedo-Torres%2C+PhD&userId=7a925ed40bb4&source=-----69fa6aefbd93---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F69fa6aefbd93&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fan-accessible-derivation-of-linear-regression-69fa6aefbd93&source=-----69fa6aefbd93---------------------bookmark_footer-----------)![](../Images/e5a0163271c749e55718fb68f4f0f088.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: Photo by [Saad Ahmad](https://unsplash.com/@saadahmad_umn?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: '***Technical disclaimer****: It is possible to derive a model without normality
    assumptions. We’ll go down this route because it’s straightforward enough to understand
    and by assuming normality of the model’s output, we can reason about the uncertainty
    of our predictions.*'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: This post is intended for people who are already aware of what linear regression
    is (and maybe have used it once or twice) and want a more principled understanding
    of the math behind it.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: 'Some background in basic probability (probability distributions, joint probability,
    mutually exclusive events), linear algebra, and stats is probably required to
    make the most of what follows. Without further ado, here we go:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: 'The machine learning world is full of amazing connections: the exponential
    family, regularization and prior beliefs, KNN and SVMs, Maximum Likelihood and
    Information Theory — it’s all connected! (I love [Dark](https://www.themoviedb.org/tv/70523-dark)).
    This time we’ll discuss how to derive another one of the members of the exponential
    family: the Linear Regression model, and in the process we’ll see that the Mean
    Squared Error loss is theoretically well motivated. As with any regression model,
    we’ll be able to use it to predict numerical, continuous targets. It’s a simple
    yet powerful model that happens to be one of the workhorses of statistical inference
    and experimental design. However we will be concerned only with its usage as a
    predictive tool. No pesky inference (and God forbid, causal) stuff here.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: 'Alright, let us begin. We want to predict something based on something else.
    We’ll call the ***predicted*** thing y and the ***something******else*** x. As
    a concrete example, I offer the following toy situation: You are a credit analyst
    working in a bank and you’re interested in automatically finding out the right
    credit limit for a bank customer. You also happen to have a dataset pertaining
    to past clients and what credit limit (the ***predicted*** thing) was approved
    for them, together with some of their features such as demographic info, past
    credit performance, income, etc. (the ***something else***).'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: 'So we have a great idea and write down a model that explains the credit limit
    in terms of those features available to you, with the model’s main assumption
    being that each feature contributes something to the observed output in an additive
    manner. Since the credit stuff was just a motivating (and contrived) example,
    let’s go back to our pure math world of spherical cows, with our model turning
    into something like this:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/48fafb4f29622acec9fdb9e608c789db.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
- en: 'We still have the predicted stuff (y) and the something else we use to predict
    it (x). We concede that some sort of noise is unavoidable (be it by virtue of
    imperfect measuring or our own blindness) and the best we can do is to assume
    that the model behind the data we observe is stochastic. The consequence of this
    is that we might see slightly different outputs for the same input, so instead
    of neat point estimates we are “stuck” with a probability distribution over the
    outputs (y) conditioned on the inputs (x):'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/524edb479fcf4dda6233d7a024390d13.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
- en: Every data point in y is replaced by a little bell curve, whose mean lies in
    the observed values of y, and has some variance which we don’t care about at the
    moment. Then our little model will take the place of the distribution mean.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: Assuming all those bell curves are actually normal distributions and their means
    (data points in y) are independent from each other, the (joint) probability of
    observing the dataset is
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ab47a367efd762543f8daa2199a634c8.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
- en: 'Logarithms and some algebra to the rescue:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b3e255bf5869558a0fb27f75dcec3f5d.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
- en: 'Logarithms are cool, aren’t they? Logs transform multiplication into sum, division
    into subtraction, and powers into multiplication. Quite handy from both algebraic
    and numerical standpoints. Getting rid of constant stuff, which is irrelevant
    in this case, we arrive to the following maximum likelihood problem:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9e917197b615317260addf6c89c1dd5d.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
- en: Well, that’s the same as
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c32bb978408caf4f0cae950f0144b883.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
- en: The expression we are about to minimize is something very close to the famous
    **Mean Square Error** loss. In fact, for optimization purposes they’re equivalent.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: So what now? This minimization problem can be solved exactly using derivatives.
    We’ll take advantage of the fact that the loss is quadratic, which means convex,
    which means one global minima; allowing us to take its derivative, set it to zero
    and solve for theta. Doing this we’ll find the value of the parameters theta that
    makes the derivative of the loss zero. And why? because it is precisely at the
    point where the derivative is zero, that the loss is at its minimum.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: 'To make everything somewhat simpler, let’s express the loss in vector notation:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7e9f4d49f8aaa407c62dc74d2ccc4667.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
- en: Here, X is an *NxM* matrix representing our whole dataset of N examples and
    M features and y is a vector containing the expected responses per training example.
    Taking the derivative and setting it to zero we get
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/287c5728c06bbd5fbe61a029794a0c3f.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
- en: There you have it, the solution to the optimization problem we have cast our
    original machine learning problem into. If you go ahead and plug those parameter
    values into your model, you’ll have a trained ML model ready to be evaluated using
    some holdout dataset (or maybe through cross-validation).
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: If you think that final expression looks an awful lot like the solution of a
    linear system,
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e60393dc618d09b9412c625b72840dde.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
- en: it’s because it does. The extra stuff comes from the fact that for our problem
    to be equivalent to a vanilla linear system, we’d need an equal number of features
    and training examples so we can invert X. Since that’s seldom the case we can
    only hope for a “best fit” solution — in some sense of best — resorting to the
    Moore-Penrose Pseudoinverse of X, which is a generalization of the good ol’ inverse
    matrix. The associated [*wikipedia*](https://en.wikipedia.org/wiki/Moore–Penrose_inverse#Definition)
    entry makes for a fun reading.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: A Small Incursion into Regularization
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'From now on I’ll assume you may have heard the term “regularization” somewhere
    and maybe you know what it means. In any case I’ll tell you how I understand it:
    regularization is anything you do to a model that makes its out-of-sample performance
    better at the expense of a worse in-sample performance. Why is this even a thing?
    because in the end, out of sample (i.e. real life) performance is what matters
    — no one is gonna care how low your training error is, or how amazing your AUC
    is on the training set unless your model performs comparatively well in production.
    And we have found that if we are not careful we will run into something called
    overfitting, where your model memorizes the training set all the way down to the
    noise, becoming potentially useless for out of sample predictions. We also know
    that there is a trade-off between model complexity and out of sample performance
    (Vapnik’s statistical learning theory) and regularization can be seen as an attempt
    to keep model complexity under control in the hope of achieving better generalization
    abilities (here generalization is another name for out of sample performance).'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: 'Regularization can take many forms, but for the purposes of this article we’ll
    only discuss one approach that is often used for linear regression models. Remember
    I said that with regularization we are trying to keep complexity under control,
    right? Well, unwanted complexity can creep in mainly via two ways: 1\. too many
    input features and 2\. large values for their corresponding theta values. It turns
    out that something called L2 regularization can help with both. Remember our loss?'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/89c6726caace2f5db0163487734d7b21.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
- en: 'we were only concerned to minimize the squared differences between predicted
    and observed values of y, without regard for anything else. Such myopic focus
    could be a problem because you could in principle game the system and keep adding
    in features till the error is minuscule, even if those extra features have nothing
    to do with the signal we are trying to learn. If we had perfect knowledge about
    the things that are relevant for our problem, we wouldn’t include useless features!
    (to be fair we wouldn’t be using machine learning on the first place but i digress).
    To compensate for our ignorance and mitigate the risk of introducing said unwanted
    complexity, we’ll add an extra term to the loss:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0060ad42cb1aafd84f30553538c2a73d.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0060ad42cb1aafd84f30553538c2a73d.png)'
- en: 'What does that do, you might ask. I’ll tell you — we’ve just introduced an
    incentive to keep the magnitude of the parameters as close to zero as possible.
    In other words we want to keep complexity at a minimum. Now instead of having
    just one objective (the minimization of prediction errors), we’ve got one more
    (minimization of complexity), and the solution for the overall problem will have
    to balance both. We do have a say in how that balance looks like, through that
    shiny new lambda coefficient: the larger it is, the more importance we give to
    the low complexity target.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 这有什么作用，你可能会问。我告诉你——我们刚刚引入了一个激励，以使参数的幅度尽可能接近零。换句话说，我们希望保持复杂性在最小化。现在，我们不再只有一个目标（最小化预测误差），我们多了一个（最小化复杂性），解决整体问题的方案将需要平衡这两个目标。通过那个闪亮的新lambda系数，我们对这种平衡有一定的发言权：它越大，我们就越重视低复杂性的目标。
- en: That’s a lot of talk. Let’s see what the extra term does to the optimal theta.
    The new loss in vectorized form is
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 说了这么多。让我们看看额外的项对最优theta的影响。新的向量化形式的损失是
- en: '![](../Images/9b7bcdcf0721cc6a4002890b98aa5a45.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9b7bcdcf0721cc6a4002890b98aa5a45.png)'
- en: 'taking its derivative, setting it to zero and solving for theta:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 计算其导数，将其设置为零并求解theta：
- en: '![](../Images/63ad7a2d5ae456ee7c203fe6acc9a825.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/63ad7a2d5ae456ee7c203fe6acc9a825.png)'
- en: 'It turns out that the new solution is not too different from the previous one.
    We’ve got an extra diagonal matrix which is going to take care of any rank issues
    that would prevent us from inverting the XTX matrix, by adding some “jitter” into
    the mix. That illuminates another way in which unnecessary complexity might be
    introduced: collinear features, that is, features that are linear transformations
    (scaled and shifted versions) of other features. As far as linear regression cares,
    two collinear features are the same feature, count as only one column, and the
    extra feature doesn’t add any new information to help train our model; rendering
    the XTX matrix rank-deficient, i.e. impossible to invert. With regularization,
    in every row of XTX, a different column is affected by lambda, the jitter — stopping
    any possible collinearity and making it full-rank. We see that adding random crap
    as predictors could actually ruin the whole learning process. Thankfully regularization
    is here to save the day (please don’t misconstrue my praise of regularization
    as an invitation to be careless when selecting your predictors — it’s not).'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 事实证明，新解决方案与之前的方案并没有太大区别。我们多了一个额外的对角矩阵，它将处理任何可能导致我们无法反转XTX矩阵的秩问题，通过在混合中加入一些“抖动”。这揭示了另一种可能引入不必要复杂性的方式：共线特征，即线性变换（缩放和移动版本）的特征。就线性回归而言，两个共线特征实际上是同一个特征，只算作一列，额外的特征不会为训练模型提供任何新的信息；这使得XTX矩阵的秩不足，即无法反转。通过正则化，在XTX的每一行中，lambda（抖动）会影响不同的列——从而阻止任何可能的共线性，并使其变为满秩。我们看到，添加随机变量作为预测器实际上可能破坏整个学习过程。幸运的是，正则化可以拯救这一切（请不要误解我对正则化的赞美为在选择预测器时可以粗心大意——绝不是这样）。
- en: So there you have it, a regularized linear regression model ready to use. Where
    do we go from here? well, you could expand on the idea that regularization encodes
    a prior belief about the parameter space, revealing its connection with Bayesian
    inference. Or you could double down on its connection with Support Vector Machines
    and eventually with non-parametric models like KNN. Also, you could ponder about
    the fact that all those fancy gradients from linear and logistic regression look
    suspiciously similar. Rest assured I’ll be touching on those subjects in a future
    post!
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，就这样，一个正则化的线性回归模型准备好了。接下来我们怎么做？嗯，你可以扩展正则化编码了关于参数空间的先验信念的想法，揭示其与贝叶斯推断的联系。或者你可以深入研究其与支持向量机的联系，并最终与像KNN这样的非参数模型的联系。同时，你还可以思考线性和逻辑回归中的那些华丽梯度看起来非常相似的事实。请放心，我将在未来的文章中触及这些主题！
