- en: How to Build a 5-Layer Data Stack
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/how-to-build-a-5-layer-data-stack-508ed09711f2?source=collection_archive---------4-----------------------#2023-07-21](https://towardsdatascience.com/how-to-build-a-5-layer-data-stack-508ed09711f2?source=collection_archive---------4-----------------------#2023-07-21)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Spinning up a data platform doesn’t have to be complicated. Here are the 5 must-have
    layers to drive data product adoption at scale.
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://barrmoses.medium.com/?source=post_page-----508ed09711f2--------------------------------)[![Barr
    Moses](../Images/4c74558ee692a85196d5a55ac1920718.png)](https://barrmoses.medium.com/?source=post_page-----508ed09711f2--------------------------------)[](https://towardsdatascience.com/?source=post_page-----508ed09711f2--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----508ed09711f2--------------------------------)
    [Barr Moses](https://barrmoses.medium.com/?source=post_page-----508ed09711f2--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F2818bac48708&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-build-a-5-layer-data-stack-508ed09711f2&user=Barr+Moses&userId=2818bac48708&source=post_page-2818bac48708----508ed09711f2---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----508ed09711f2--------------------------------)
    ·10 min read·Jul 21, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F508ed09711f2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-build-a-5-layer-data-stack-508ed09711f2&user=Barr+Moses&userId=2818bac48708&source=-----508ed09711f2---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F508ed09711f2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-build-a-5-layer-data-stack-508ed09711f2&source=-----508ed09711f2---------------------bookmark_footer-----------)![](../Images/3ddb77b6e57a66c3a3d179f5c4a36344.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Image courtesy of author. We hope it doesn’t make your eyes water.
  prefs: []
  type: TYPE_NORMAL
- en: Like bean dip and [ogres](https://www.youtube.com/watch?v=aJQmVZSAqlc), layers
    are the building blocks of the modern data stack.
  prefs: []
  type: TYPE_NORMAL
- en: Its powerful selection of tooling components combine to create a single synchronized
    and extensible data platform with each layer serving a unique function of the
    data pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: Unlike ogres, however, the cloud data platform isn’t a fairy tale. New tooling
    and integrations are created almost daily in an effort to augment and elevate
    it.
  prefs: []
  type: TYPE_NORMAL
- en: So, with infinitely expanding integrations and the opportunity to add new layers
    for every feature and function of your data motion, the question arises — where
    do you start? Or to put it a different way, how do you deliver a data platform
    that drives real value for stakeholders without building a platform that’s either
    too complex to manage or too expensive to justify?
  prefs: []
  type: TYPE_NORMAL
- en: For small data teams building their first cloud-native platforms and teams making
    the jump from on-prem for the first time, it’s essential to bias those layers
    that will have the most immediate impact on business outcomes.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this article, we’ll present you with the Five Layer Data Stack — a model
    for platform development consisting of five critical tools that will not only
    allow you to maximize impact but empower you to grow with the needs of your organization.
    Those tools include:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Cloud storage and compute](#storage)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Data transformation](#transform)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Business intelligence](#bi)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Data observability](#observability)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[And orchestration](#orchestration)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: And we won’t mention ogres or bean dip again.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s dive into it. (The content, not the bean dip. Okay, that’s really the
    last time).
  prefs: []
  type: TYPE_NORMAL
- en: Cloud storage and compute
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Whether you’re stacking data tools or pancakes, you always build from the bottom
    up. Like any good stack, an appropriate foundation is critical to ensuring the
    structural and functional integrity of your data platform.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before you can model the data for your stakeholders, you need a place to collect
    and store it. The first layer of your stack will generally fall into one of three
    categories: a data warehouse solution like [Snowflake](https://www.snowflake.com/en/)
    that handles predominantly structured data; a data lake that focuses on larger
    volumes of unstructured data; and a hybrid solution like [Databricks’](https://www.databricks.com/)
    Lakehouse that combines elements of both.'
  prefs: []
  type: TYPE_NORMAL
- en: However, this won’t simply be where you store your data — it’s also the power
    to activate it. In the cloud data stack, your storage solution is the primary
    source of compute power for the other layers of your platform. (Read more on when
    it makes sense to couple or decouple your storage and compute via my colleague
    Shane).
  prefs: []
  type: TYPE_NORMAL
- en: Now, I could get into the merits of the warehouse, the lake, the lakehouse,
    and everything in between, but that’s not really what’s important here. What *is*
    important is that you select a solution that meets both the current and future
    needs of your platform at a resource cost that’s amenable to your finance team.
    It will also dictate what tools and solutions you’ll be able to connect in the
    future to fine-tune your data stack for new use cases.
  prefs: []
  type: TYPE_NORMAL
- en: What specific storage and compute solution you need will depend entirely on
    your business needs and use-case, but our recommendation is to choose something
    common — Snowflake, Databricks, BigQuery, etc. — that’s well supported, well-integrated,
    and easy to scale.
  prefs: []
  type: TYPE_NORMAL
- en: Open-source is always a tempting solution, but unless you’ve reached a level
    of scale that actually necessitates it, it can present some major challenges for
    scaling at the storage and compute level. Take our word for it, choosing a managed
    storage and compute solution at the outset will save you a lot of headache — and
    likely a painful migration — down the line.
  prefs: []
  type: TYPE_NORMAL
- en: Data transformation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Okay, so your data needs to *live* in the cloud. Makes sense. What else does
    your data platform need? Let’s look at layer two of the Five Layer Data Stack
    — transformation.
  prefs: []
  type: TYPE_NORMAL
- en: When data is first ingested, it comes in all sorts of fun shapes and sizes.
    Different formats. Different structures. Different values. In simple terms, data
    transformation refers to the process of converting all that data from a variety
    of disparate formats into something consistent and useful for modeling.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8b520dd1d07e0a5421c6037421eb1467.png)'
  prefs: []
  type: TYPE_IMG
- en: Image courtesy of author.
  prefs: []
  type: TYPE_NORMAL
- en: Traditionally, transformation was a manual process, requiring data engineers
    to hard-code each pipeline by hand within a CLI.
  prefs: []
  type: TYPE_NORMAL
- en: Recently, however, cloud transformation tools have begun to democratize the
    data modeling process. In an effort to make data pipelines more accessible for
    practitioners, automated data pipeline tools like [dbt Labs](https://www.getdbt.com/),
    [Preql,](https://www.preql.com/) and [Dataform](https://dataform.co/) allow users
    to create effective models without writing any code at all.
  prefs: []
  type: TYPE_NORMAL
- en: Tools like dbt rely on what’s known as “modular SQL” to build pipelines from
    common, pre-written, and optimized plug-and-play blocks of SQL code.
  prefs: []
  type: TYPE_NORMAL
- en: As you begin your cloud data journey, you’ll quickly discover new ways to model
    the data and provide value to data consumers. You’ll field new dashboard requests
    from finance and marketing. You’ll find new sources that need to be introduced
    to existing models. The opportunities will come fast and furious.
  prefs: []
  type: TYPE_NORMAL
- en: Like many layers of the data stack, coding your own transforms *can* work on
    a small scale. Unfortunately, as you begin to grow, manually coding transforms
    will quickly become a bottleneck to your data platform’s success. Investing in
    out-of-the-box operationalized tooling is often necessary to remaining competitive
    and continuing to provide new value across domains.
  prefs: []
  type: TYPE_NORMAL
- en: But, it’s not just *writing* your transforms that gets cumbersome. Even if you
    could code enough transforms to cover your scaling use-cases, what happens if
    those transforms break? Fixing one broken model is probably no big deal, but fixing
    100 is a pipe dream (pun obviously intended).
  prefs: []
  type: TYPE_NORMAL
- en: '**Improved time-to-value for scaling organizations**'
  prefs: []
  type: TYPE_NORMAL
- en: Transformation tools like dbt make creating and managing complex models faster
    and more reliable for expanding engineering and practitioner teams. Unlike manual
    SQL coding which is generally limited to data engineers, dbt’s modular SQL makes
    it possible for anyone familiar with SQL to create their own data pipelines. This
    means faster time to value for busy teams, reduced engineering drain, and, in
    some cases, a reduced demand on expertise to drive your platform forward.
  prefs: []
  type: TYPE_NORMAL
- en: '**Flexibility to experiment with transformation sequencing**'
  prefs: []
  type: TYPE_NORMAL
- en: An automated cloud transformation layer also allows for data transforms to take
    place at different stages of the pipeline, offering the flexibility to experiment
    with ETL, ELT, and everything in between as your platform evolves.
  prefs: []
  type: TYPE_NORMAL
- en: '**Enables self-service capabilities**'
  prefs: []
  type: TYPE_NORMAL
- en: Finally, an operationalized transform tool will pave the road for a fully self-service
    architecture in the future — *should you choose to travel it*.
  prefs: []
  type: TYPE_NORMAL
- en: Business Intelligence (BI)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If transformation is layer two, then business intelligence has to be layer three.
  prefs: []
  type: TYPE_NORMAL
- en: Business intelligence in the context of data platform tooling refers to the
    analytical capabilities we present to end-users to fulfill a given use-case. While
    our data may feed some external products, business intelligence functions are
    the primary data product for most teams.
  prefs: []
  type: TYPE_NORMAL
- en: While business intelligence tools like [Looker](https://cloud.google.com/looker),
    [Tableau](https://www.googleadservices.com/pagead/aclk?sa=L&ai=DChcSEwjJlfyxmJmAAxVdnloFHW-3BUIYABAAGgJ2dQ&ae=2&ohost=www.google.com&cid=CAESa-D2Rs6vVh-kCNsved-huFcyDclkcTJR6ZAoI9EsckHNUhehVWQW04-ahLIfZaJ_LSTych2ysfMN-EkaDcvPmQgbnEqg6lGb6FMD2ZFB-QZCicvPMy7nI_9DAOe3k6CWesySU5QVhgww9P7-&sig=AOD64_2Pa9CECELwzsfEN-KiOVWDQD-QQA&q=&adurl=&ved=2ahUKEwii0_SxmJmAAxVYQzABHeRaBScQ0Qx6BAgJEAE&nis=2&dct=1),
    and a variety of [open-source tools](https://www.montecarlodata.com/blog-open-source-bi-tools/)
    can vary wildly in complexity, ease of use, and feature-sets, what these tools
    always share is an ability to help data consumers uncover insights through *visualization*.
  prefs: []
  type: TYPE_NORMAL
- en: This one’s gonna be pretty self-explanatory because while everything else in
    your stack is a means to an end, business intelligence is often the end itself.
  prefs: []
  type: TYPE_NORMAL
- en: Business intelligence is generally the consumable product at the heart of a
    data stack, and it’s an essential value driver for any cloud data platform. As
    your company’s appetite to create and consume data grows, the need to access that
    data quickly and easily will grow right along with it.
  prefs: []
  type: TYPE_NORMAL
- en: Business intelligence tooling is what makes it possible for your stakeholders
    to derive value from your data platform. Without a way to activate and consume
    the data, there would be no need for a cloud data platform at all — no matter
    how many layers it had.
  prefs: []
  type: TYPE_NORMAL
- en: Data observability
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The average data engineering team spends roughly [two days per week firefighting
    bad data](https://www.montecarlodata.com/blog-2022-data-quality-survey/). In fact,
    according to a recent survey by Gartner, bad data costs organizations an average
    of $12.9 million per year. To mitigate all that financial risk and protect the
    integrity of your platform, you need layer four: data observability.'
  prefs: []
  type: TYPE_NORMAL
- en: Before data observability, one of the most common ways to discover data quality
    issues was through manual SQL tests. Open source data testing tools like Great
    Expectations and dbt enabled data engineers to validate their organization’s assumptions
    about the data and write logic to prevent the issue from working its way downstream.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/09e175dfaa54a868d9ea9ff15597088c.png)'
  prefs: []
  type: TYPE_IMG
- en: Image courtesy of author.
  prefs: []
  type: TYPE_NORMAL
- en: Data observability platforms use machine learning instead of manual coding to
    automatically generate quality checks for things like freshness, volume, schema,
    and null rates across all your production tables. In addition to comprehensive
    quality coverage, a good data observability solution will also generate both table
    and column-level lineage to help teams quickly identify where a break happened
    and what’s been impacted based on upstream and downstream dependencies.
  prefs: []
  type: TYPE_NORMAL
- en: The value of your data platform — and by extension its products — is inextricably
    tied to the quality of the data that feeds it. Garbage in, garbage out. (Or nothing
    out if you’ve got a broken ingestion job.) To have reliable, actionable, and useful
    data products, the underlying data has to be *trustworthy*. If you can’t trust
    the data, you can’t trust the data product.
  prefs: []
  type: TYPE_NORMAL
- en: Unfortunately, as your data grows, your data quality issues will grow right
    along with it. The more complex your platform, the more sources you ingest, the
    more teams you support — the more quality incidents you’re likely to have. And
    as teams increasingly [leverage data to power AI models and ML use cases](https://www.montecarlodata.com/blog-generative-ai-data-engineering/),
    the need to ensure its trust and reliability grows exponentially.
  prefs: []
  type: TYPE_NORMAL
- en: While data testing can provide some quality coverage, its function is limited
    to known issues and specific tables. And because each check manual test needs
    to be coded by hand, scalability is only proportionate to your available engineering
    resources. Data observability, on the other hand, provides plug-and-play coverage
    across every table automatically, so you’ll be alerted to any data quality incident
    — known or unknown — before it impacts downstream consumers. And as your platform
    and your data scale, your quality coverage will scale along with it.
  prefs: []
  type: TYPE_NORMAL
- en: Plus, data observability provides end-to-end lineage down to the BI layer, which
    makes it possible to actually root cause and resolve quality incidents. That can
    mean hours of time recovered for your data team. While manual testing may be able
    to catch a portion of quality incidents, it’s useless to help you resolve them.
    That’s even more alarming when you realize that [time-to-resolution has nearly
    doubled for data teams year-over-year](https://www.montecarlodata.com/blog-data-quality-survey).
  prefs: []
  type: TYPE_NORMAL
- en: Unlike data testing which is reactionary by nature, data observability provides
    proactive visibility into [known *and* unknown](/5-most-important-things-to-include-in-a-modern-data-quality-framework-34695711c992)
    issues with a real-time record of your pipeline lineage to position your data
    platform for growth — all without sacrificing your team’s time or resources.
  prefs: []
  type: TYPE_NORMAL
- en: Data orchestration
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When you’re extracting and processing data for analytics, the order of operation
    matters. As we’ve seen already, your data doesn’t simply exist within the storage
    layer of your data stack. It’s ingested from one source, housed in another, then
    ferried somewhere else to be transformed and visualized.
  prefs: []
  type: TYPE_NORMAL
- en: In the broadest terms, data orchestration is the configuration of multiple tasks
    (some may be automated) into a single end-to-end process. It triggers when and
    how critical jobs will be activated to ensure data flows predictably through your
    platform at the right time, in the right sequence, and at the appropriate velocity
    to maintain production standards. (Kind of like a conveyor belt for your data
    products.)
  prefs: []
  type: TYPE_NORMAL
- en: Unlike storage or transformation, pipelines don’t require orchestration to be
    considered functional — at least not at a foundational level. However, once data
    platforms scale beyond a certain point, managing jobs will quickly become unwieldy
    by in-house standards.
  prefs: []
  type: TYPE_NORMAL
- en: When you’re extracting and processing a small amount of data, scheduling jobs
    requires only a small amount of effort. But when you’re extracting and processing
    very large amounts of data from multiple sources and for countless use cases,
    scheduling those jobs requires a very large amount of effort — an inhuman amount
    of effort.
  prefs: []
  type: TYPE_NORMAL
- en: The reason that orchestration is a functional necessity of the 5 Layer Data
    Stack — if not a literal one — is due to the inherent lack of scalability in hand-coded
    pipelines. Much like transformation and data quality, engineering resources become
    the limiting principle for scheduling and managing pipelines.
  prefs: []
  type: TYPE_NORMAL
- en: The beauty of so much of the modern data stack is that it enables tools and
    integrations that remove engineering bottlenecks, freeing up engineers to provide
    new value to their organizations. These are the tools that justify themselves.
    That’s exactly what orchestration does as well.
  prefs: []
  type: TYPE_NORMAL
- en: And as your organization grows and silos naturally begin to develop across your
    data, having an orchestration layer in place will position your data team to maintain
    control of your data sources and continue to provide value across domains.
  prefs: []
  type: TYPE_NORMAL
- en: Some of the most popular solutions for data orchestration include [Apache Airflow](https://airflow.apache.org/),
    [Dagster](https://dagster.io/), and relative newcomer [Prefect](https://www.prefect.io/).
  prefs: []
  type: TYPE_NORMAL
- en: The most important part? Building for impact and scale
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Of course, five isn’t *the* magic number. A great data stack might have six
    layers, seven layers, or 57 layers. And many of those potential layers — like
    [governance](https://www.montecarlodata.com/blog-why-data-governance-matters-best-practices-and-how-to-build-a-strategy/),
    [contracts](https://www.montecarlodata.com/blog-data-contracts-explained/), and
    even some additional testing — can be quite useful depending on the stage of your
    organization and its platform.
  prefs: []
  type: TYPE_NORMAL
- en: However, when you’re just getting started, you don’t have the resources, the
    time, or even the requisite use cases to boil the [Mariana Trench](https://en.wikipedia.org/wiki/Mariana_Trench)
    of platform tooling available to the modern data stack. More than that, each new
    layer will introduce new complexities, new challenges, and new costs that will
    need to be justified. Instead, focus on what matters most to realize the potential
    of your data and drive company growth in the near term.
  prefs: []
  type: TYPE_NORMAL
- en: Each of the layers mentioned above — storage, transformation, BI, data observability,
    and orchestration — provides an essential function of any fully operational modern
    data stack that maximizes impact and provides the immediate scalability you’ll
    need to rapidly grow your platform, your use cases, and your team in the future.
  prefs: []
  type: TYPE_NORMAL
- en: If you’re a data leader who’s just getting started on their data journey and
    you want to deliver a lean data platform that limits costs without sacrificing
    power, the 5-Layer Data Stack is the one to beat.
  prefs: []
  type: TYPE_NORMAL
- en: '**Disagree? Reach out to** [**Barr**](https://www.linkedin.com/in/barrmoses/)
    **on LinkedIn with any comments, questions, or bean dip recipes.**'
  prefs: []
  type: TYPE_NORMAL
