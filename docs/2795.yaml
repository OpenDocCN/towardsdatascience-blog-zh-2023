- en: 'Language Models and Friends: Gorilla, HuggingGPT, TaskMatrix, and More'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/language-models-and-friends-gorilla-hugginggpt-taskmatrix-and-more-b88c1200afd3?source=collection_archive---------9-----------------------#2023-09-04](https://towardsdatascience.com/language-models-and-friends-gorilla-hugginggpt-taskmatrix-and-more-b88c1200afd3?source=collection_archive---------9-----------------------#2023-09-04)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: What happens when we give LLMs access to thousands of deep learning models?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://wolfecameron.medium.com/?source=post_page-----b88c1200afd3--------------------------------)[![Cameron
    R. Wolfe, Ph.D.](../Images/52bb88d7cf1105501be2fae5ccbe7a03.png)](https://wolfecameron.medium.com/?source=post_page-----b88c1200afd3--------------------------------)[](https://towardsdatascience.com/?source=post_page-----b88c1200afd3--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----b88c1200afd3--------------------------------)
    [Cameron R. Wolfe, Ph.D.](https://wolfecameron.medium.com/?source=post_page-----b88c1200afd3--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F28aa6026c553&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flanguage-models-and-friends-gorilla-hugginggpt-taskmatrix-and-more-b88c1200afd3&user=Cameron+R.+Wolfe%2C+Ph.D.&userId=28aa6026c553&source=post_page-28aa6026c553----b88c1200afd3---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----b88c1200afd3--------------------------------)
    ·18 min read·Sep 4, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fb88c1200afd3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flanguage-models-and-friends-gorilla-hugginggpt-taskmatrix-and-more-b88c1200afd3&user=Cameron+R.+Wolfe%2C+Ph.D.&userId=28aa6026c553&source=-----b88c1200afd3---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb88c1200afd3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flanguage-models-and-friends-gorilla-hugginggpt-taskmatrix-and-more-b88c1200afd3&source=-----b88c1200afd3---------------------bookmark_footer-----------)![](../Images/6d46238c8cde54e36f1961a4a4509706.png)'
  prefs: []
  type: TYPE_NORMAL
- en: (Photo by [Mike Arney](https://unsplash.com/@mikearney?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    on [Unsplash](https://unsplash.com/photos/rJ5vHo8gr2U?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText))
  prefs: []
  type: TYPE_NORMAL
- en: Recently, we have witnessed a rise of foundation models to popularity within
    deep learning research. Pre-trained large language models (LLMs) have led to a
    new paradigm, in which a single model can be used — with surprising success —
    to solve many different problems. Despite the popularity of generic LLMs, however,
    fine-tuning models in a task-specific manner tends to outperform approaches that
    leverage foundation models. Put simply, *specialized models are still very hard
    to beat*! With this being said, we might start to wonder whether the powers of
    foundation models and specialized deep learning models can be combined. Within
    this overview, we will study recent research that integrates LLMs with other,
    specialized deep learning models by learning to call their associated APIs. The
    resulting framework uses the language model as a centralized controller that forms
    a plan for solving a complex, AI-related tasks and delegates specialized portions
    of the solution process to more appropriate models.
  prefs: []
  type: TYPE_NORMAL
- en: “By providing only the model descriptions, HuggingGPT can continuously and conveniently
    integrate diverse expert models from AI communities, without altering any structure
    or prompt…
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
