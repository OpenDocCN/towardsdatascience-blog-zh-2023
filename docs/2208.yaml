- en: Async for LangChain and LLMs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/async-calls-for-chains-with-langchain-3818c16062ed?source=collection_archive---------0-----------------------#2023-07-10](https://towardsdatascience.com/async-calls-for-chains-with-langchain-3818c16062ed?source=collection_archive---------0-----------------------#2023-07-10)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: How to make LangChain chains work with Async calls to LLMs, speeding up the
    time it takes to run a sequential long chain
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://gabrielcassimiro17.medium.com/?source=post_page-----3818c16062ed--------------------------------)[![Gabriel
    Cassimiro](../Images/2cf8a09a706236059c46c7f0f20d4365.png)](https://gabrielcassimiro17.medium.com/?source=post_page-----3818c16062ed--------------------------------)[](https://towardsdatascience.com/?source=post_page-----3818c16062ed--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----3818c16062ed--------------------------------)
    [Gabriel Cassimiro](https://gabrielcassimiro17.medium.com/?source=post_page-----3818c16062ed--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F3692fb93d7e5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fasync-calls-for-chains-with-langchain-3818c16062ed&user=Gabriel+Cassimiro&userId=3692fb93d7e5&source=post_page-3692fb93d7e5----3818c16062ed---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----3818c16062ed--------------------------------)
    ·6 min read·Jul 10, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F3818c16062ed&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fasync-calls-for-chains-with-langchain-3818c16062ed&user=Gabriel+Cassimiro&userId=3692fb93d7e5&source=-----3818c16062ed---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F3818c16062ed&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fasync-calls-for-chains-with-langchain-3818c16062ed&source=-----3818c16062ed---------------------bookmark_footer-----------)![](../Images/3d661ccf3f4d03c45b31cfc182d4ecd0.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Image by[hp koch](https://unsplash.com/@iggii?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    on [Unsplash](https://unsplash.com/pt-br/fotografias/2OuTr9_VaUg?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
  prefs: []
  type: TYPE_NORMAL
- en: In this article, I will cover how to use asynchronous calls to LLMs for long
    workflows using LangChain. We will go through an example with the full code and
    compare Sequential execution with the Async calls.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the overview of the content. If you’d like you can jump to the section
    of your interest:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Basics: What is LangChain'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How to run a Synchronous chain with LangChain
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How to run a single Asynchronous chain with LangChain
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Real-world tips for long workflows with Async Chains.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: So let’s start!
  prefs: []
  type: TYPE_NORMAL
- en: 'Basics: What is Langchain'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: LangChain is a framework for developing applications powered by language models.
    That is the official definition of LangChain. This framework was created recently
    and is already used as the industry standard for building tools powered by LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: It is open-source and well-maintained, with new features being released in a
    very fast time frame.
  prefs: []
  type: TYPE_NORMAL
- en: The official documentation can be found [here](https://python.langchain.com/docs/get_started/introduction.html)
    and the GitHub repository [here](https://github.com/hwchase17/langchain).
  prefs: []
  type: TYPE_NORMAL
- en: One downside that we have in this library is that since the features are new
    we cannot use Chat GPT to help effectively to build new code. So this means that
    we have to work in the “Ancient” way of reading documentation, forums, and tutorials.
  prefs: []
  type: TYPE_NORMAL
- en: The documentation for LangChain.is really good however there are not a lot of
    examples of some specific things.
  prefs: []
  type: TYPE_NORMAL
- en: I ran into this problem with Async for long chains.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are the main resources I used to learn more about the framework:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Deep Learning AI course: [LangChain Chat with your data](https://www.deeplearning.ai/short-courses/langchain-chat-with-your-data/);'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Official Documentation](https://python.langchain.com/docs/get_started/introduction.html);'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Youtube channel](https://www.youtube.com/watch?v=_v_fgW2SkkQ&list=PLqZXAkvF1bPNQER9mLmDbntNfSpzdDIU5).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: (ps. They are all free)
  prefs: []
  type: TYPE_NORMAL
- en: How to run a Synchronous chain with LangChain
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'So let me set up the problem I had: I have a data frame with a lot of rows
    and for each of those rows I need to run multiple prompts (chains) to an LLM and
    return the result to my data frame.'
  prefs: []
  type: TYPE_NORMAL
- en: When you have multiple rows, let’s say 10K, running 3 prompts for each and each
    response (if the server is not overloaded) taking about 3–5 seconds you end up
    waiting for days for the workflow to be completed.
  prefs: []
  type: TYPE_NORMAL
- en: Bellow I am going to show the main steps and code to build a synchronous chain
    and time it on a subset of data.
  prefs: []
  type: TYPE_NORMAL
- en: For this example, I am going to use the dataset [Wine Reviews](https://www.kaggle.com/datasets/zynicide/wine-reviews),
    [license](https://creativecommons.org/licenses/by-nc-sa/4.0/). The goal here is
    to extract some information from the written reviews.
  prefs: []
  type: TYPE_NORMAL
- en: I want to extract a Summary of the review, the main sentiment, and the top 5
    characteristics of each wine.
  prefs: []
  type: TYPE_NORMAL
- en: For that, I created two chains, one for the summary and sentiment and another
    that takes the summary as input to extract the characteristics.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the code to run it:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Run time (10 examples):'
  prefs: []
  type: TYPE_NORMAL
- en: Summary Chain (Sequential) executed in 22.59 seconds.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Characteristics Chain (Sequential) executed in 22.85 seconds.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: If you want to understand more about the components I am using I really recommend
    watching the [Deep Learning AI Course](https://www.deeplearning.ai/short-courses/langchain-chat-with-your-data/).
  prefs: []
  type: TYPE_NORMAL
- en: The main takeaways from this code are the building blocks for a chain, how to
    run it in a sequential way, and the time it took to finish this loop. It is important
    to remember that it was about 45 seconds for 10 examples and the full dataset
    contains 130K rows. So the Async implementation is the New Hope to run this in
    a reasonable time.
  prefs: []
  type: TYPE_NORMAL
- en: So with the problem set up and the baseline established, let's see how we can
    optimize this code to run much faster.
  prefs: []
  type: TYPE_NORMAL
- en: How to run a single Asynchronous chain with LangChain
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So for this, we are going to use a resource called Asynchronous calls. To explain
    this, first I will explain briefly what the code is doing and where the time is
    taking too long.
  prefs: []
  type: TYPE_NORMAL
- en: In our example, we go through each row of the data frame, extract some information
    from the rows, add them to our prompt, and call the GPT API to get a response.
    After the response, we just parse it and add it back to the data frame.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e446a0030466fef8a134fcceada0501a.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: The main bottleneck here is when we call the GPT API because our computer has
    to wait idly for the response from that API (about 3 seconds). The rest of the
    steps are fast and can still be optimized but that is not the focus of this article.
  prefs: []
  type: TYPE_NORMAL
- en: So instead of waiting Idly for the response, what if we sent all the calls to
    the API at the same time? This way we would only have to wait for a single response
    and then process them. This is called Asynchronous calls to the API.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e0b1422650d5840a248989d6ca91aedb.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: This way we do the pre-process and post-process sequentially but the calls to
    the API do not have to wait for the last response to come back before sending
    the next one.
  prefs: []
  type: TYPE_NORMAL
- en: 'So here is the code for the Async chains:'
  prefs: []
  type: TYPE_NORMAL
- en: In this code, we use the Python syntax of async and await. LangChain also gives
    us the code to run the chain async, with the arun() function. So in the beginning
    we first process each row sequentially (can be optimized) and create multiple
    “tasks” that will await the response from the API in parallel and then we process
    the response to the final desired format sequentially (can also be optimized).
  prefs: []
  type: TYPE_NORMAL
- en: 'Run time (10 examples):'
  prefs: []
  type: TYPE_NORMAL
- en: Summary Chain (Async) executed in 3.35 seconds.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Characteristics Chain (Async) executed in 2.49 seconds.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Compared to the sequential:'
  prefs: []
  type: TYPE_NORMAL
- en: Summary Chain (Sequential) executed in 22.59 seconds.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Characteristics Chain (Sequential) executed in 22.85 seconds.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: We can see almost a 10x improvement in the run time. So for big workloads, I
    highly recommend using this method. Also my code is full of for loops that can
    also be optimized further to improve performance.
  prefs: []
  type: TYPE_NORMAL
- en: The full code to this tutorial can be found in this [Github Repo](https://github.com/gabrielcassimiro17/async-langchain).
  prefs: []
  type: TYPE_NORMAL
- en: Real-world tips for long workflows with Async Chains.
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When I had to run this, I ran into some limitations and a few roadblocks, that
    I want to share with you.
  prefs: []
  type: TYPE_NORMAL
- en: Notebooks are not Async Friendly
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When running async calls on Jupyter Notebooks you may encounter some issues.
    However, just ask Chat GPT and it can probably help you out with that. The code
    I built is to run big workloads in a .py file, so it may need some changes to
    run in a notebook.
  prefs: []
  type: TYPE_NORMAL
- en: Too many output keys
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The First one was that my chain had multiple keys as outputs and at the time
    the arun() only accepted chains that had one key as the output. So to fix this
    I had to break my chain into two separate ones.
  prefs: []
  type: TYPE_NORMAL
- en: Not all chains can be async
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: I had a logic of using a vector database for examples and comparisons in my
    prompt and that required that the examples were sequentially compared and added
    to the database. This rendered unfeasible the use of async for this link in the
    full chain.
  prefs: []
  type: TYPE_NORMAL
- en: Lack of content
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For this specific matter, the best content I could find was the [official documentation
    for async](https://python.langchain.com/docs/modules/chains/how_to/async_chain)
    and build from there to my use case. So if you run it and find new things out
    share it with the world!
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: LangChain is a very powerful tool to create LLM-based applications. I highly
    recommend learning this framework and doing the courses cited above.
  prefs: []
  type: TYPE_NORMAL
- en: For the specific topic of running chains, for high workloads we saw the potential
    improvement that Async calls have, so my recommendation is to take the time to
    understand what the code is doing and have a boilerplate class (such as the one
    provided in my code) and run it Asynchronously!
  prefs: []
  type: TYPE_NORMAL
- en: For small workloads or applications that require only one call to an API it
    is not necessary to do it async, but if you have a boilerplate class just add
    a sync function so you can easily use one or the other.
  prefs: []
  type: TYPE_NORMAL
- en: Thanks for reading.
  prefs: []
  type: TYPE_NORMAL
- en: The full code can be found [here](https://github.com/gabrielcassimiro17/async-langchain).
  prefs: []
  type: TYPE_NORMAL
- en: 'If you like the content and want to support me, you can buy me a coffee:'
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://www.buymeacoffee.com/cassimiro?source=post_page-----3818c16062ed--------------------------------)
    [## Gabriel Cassimiro is a Data Scientist sharing free content to the community'
  prefs: []
  type: TYPE_NORMAL
- en: Hey 👋 I just created a page here. You can now buy me a coffee!
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: www.buymeacoffee.com](https://www.buymeacoffee.com/cassimiro?source=post_page-----3818c16062ed--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are a few other articles you might be interested in:'
  prefs: []
  type: TYPE_NORMAL
- en: '[](/solving-unity-environment-with-deep-reinforcement-learning-836dc181ee3b?source=post_page-----3818c16062ed--------------------------------)
    [## Solving Unity Environment with Deep Reinforcement Learning'
  prefs: []
  type: TYPE_NORMAL
- en: End to End Project with code of a PyTorch implementation of Deep Reinforcement
    Learning Agent.
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/solving-unity-environment-with-deep-reinforcement-learning-836dc181ee3b?source=post_page-----3818c16062ed--------------------------------)
    [](/object-detection-with-tensorflow-model-and-opencv-d839f3e42849?source=post_page-----3818c16062ed--------------------------------)
    [## Object detection with Tensorflow model and OpenCV
  prefs: []
  type: TYPE_NORMAL
- en: Using a trained model to identify objects on static images and live video
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/object-detection-with-tensorflow-model-and-opencv-d839f3e42849?source=post_page-----3818c16062ed--------------------------------)
  prefs: []
  type: TYPE_NORMAL
