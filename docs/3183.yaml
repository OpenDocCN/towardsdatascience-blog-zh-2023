- en: Understanding and Mitigating LLM Hallucinations
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解和减轻LLM幻觉
- en: 原文：[https://towardsdatascience.com/understanding-and-mitigating-llm-hallucinations-be88d31c4200?source=collection_archive---------1-----------------------#2023-10-23](https://towardsdatascience.com/understanding-and-mitigating-llm-hallucinations-be88d31c4200?source=collection_archive---------1-----------------------#2023-10-23)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/understanding-and-mitigating-llm-hallucinations-be88d31c4200?source=collection_archive---------1-----------------------#2023-10-23](https://towardsdatascience.com/understanding-and-mitigating-llm-hallucinations-be88d31c4200?source=collection_archive---------1-----------------------#2023-10-23)
- en: LLM hallucination detection challenges and a possible solution presented in
    a prominent research paper
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: LLM 幻觉检测挑战及其在一篇重要研究论文中提出的可能解决方案。
- en: '[](https://felipe-p-adachi.medium.com/?source=post_page-----be88d31c4200--------------------------------)[![Felipe
    de Pontes Adachi](../Images/58c9544ae85f43548c5e5b56fda31bb4.png)](https://felipe-p-adachi.medium.com/?source=post_page-----be88d31c4200--------------------------------)[](https://towardsdatascience.com/?source=post_page-----be88d31c4200--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----be88d31c4200--------------------------------)
    [Felipe de Pontes Adachi](https://felipe-p-adachi.medium.com/?source=post_page-----be88d31c4200--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://felipe-p-adachi.medium.com/?source=post_page-----be88d31c4200--------------------------------)[![Felipe
    de Pontes Adachi](../Images/58c9544ae85f43548c5e5b56fda31bb4.png)](https://felipe-p-adachi.medium.com/?source=post_page-----be88d31c4200--------------------------------)[](https://towardsdatascience.com/?source=post_page-----be88d31c4200--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----be88d31c4200--------------------------------)
    [Felipe de Pontes Adachi](https://felipe-p-adachi.medium.com/?source=post_page-----be88d31c4200--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fa038269245d5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-and-mitigating-llm-hallucinations-be88d31c4200&user=Felipe+de+Pontes+Adachi&userId=a038269245d5&source=post_page-a038269245d5----be88d31c4200---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----be88d31c4200--------------------------------)
    ·8 min read·Oct 23, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fbe88d31c4200&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-and-mitigating-llm-hallucinations-be88d31c4200&user=Felipe+de+Pontes+Adachi&userId=a038269245d5&source=-----be88d31c4200---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fa038269245d5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-and-mitigating-llm-hallucinations-be88d31c4200&user=Felipe+de+Pontes+Adachi&userId=a038269245d5&source=post_page-a038269245d5----be88d31c4200---------------------post_header-----------)
    发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----be88d31c4200--------------------------------)
    ·8 min read·Oct 23, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fbe88d31c4200&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-and-mitigating-llm-hallucinations-be88d31c4200&user=Felipe+de+Pontes+Adachi&userId=a038269245d5&source=-----be88d31c4200---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fbe88d31c4200&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-and-mitigating-llm-hallucinations-be88d31c4200&source=-----be88d31c4200---------------------bookmark_footer-----------)![](../Images/ada358f0bc7233d554233f71e47aa0b8.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fbe88d31c4200&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-and-mitigating-llm-hallucinations-be88d31c4200&source=-----be88d31c4200---------------------bookmark_footer-----------)![](../Images/ada358f0bc7233d554233f71e47aa0b8.png)'
- en: '[Image by Enrique from Pixabay](https://pixabay.com/photos/forest-person-surreal-poisoning-7772371/)'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '[由 Enrique 提供的图片](https://pixabay.com/photos/forest-person-surreal-poisoning-7772371/)'
- en: Recently, large language models (LLMs) have shown impressive and increasing
    capabilities, including generating highly fluent and convincing responses to user
    prompts. However, LLMs are known for their ability to generate non-factual or
    nonsensical statements, more commonly known as “hallucinations.” This characteristic
    can undermine trust in many scenarios where factuality is required, such as summarization
    tasks, generative question answering, and dialogue generations.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，大型语言模型（LLMs）展示了令人印象深刻且不断增强的能力，包括对用户提示生成高度流畅和令人信服的响应。然而，LLMs 以生成非事实性或荒谬陈述而闻名，这种特性通常称为“幻觉”。这种特征可能会在许多需要事实性的场景中损害信任，如总结任务、生成式问答和对话生成。
- en: Detecting hallucinations has always been challenging among humans, which remains
    true in the context of LLMs. This is especially challenging, considering we usually
    don’t have access to ground truth context for consistency checks. Additional information
    on the LLM’s generations, like the output probability distributions, can help
    with this task. Still, it is often the case where this type of information is
    unavailable, making the task even more difficult.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 检测幻觉在人类中一直是一个挑战，在 LLM 的背景下同样如此。这尤其具有挑战性，因为我们通常无法获取用于一致性检查的真实背景信息。有关 LLM 生成的附加信息，如输出概率分布，可以帮助完成这一任务。然而，这类信息往往不可用，使得任务更加困难。
- en: 'Hallucination detection has yet to be solved and is an active area of research.
    In this blog post, we’ll present the task in general and its challenges and one
    possible approach published in the research paper [**SELFCHECKGPT: Zero-Resource
    Black-Box Hallucination Detection for Generative Large Language Models**](https://arxiv.org/pdf/2303.08896.pdf?ref=content.whylabs.tds)[1].
    We will illustrate some of the approaches presented in the paper with real examples,
    pointing out some pros and cons of each method. You can review the examples yourself
    by going to this [Google Colab Notebook](https://colab.research.google.com/drive/1ftgiASR3TeMaRTQ-cTd81iEoh2j0UP16?usp=sharing&ref=content.whylabs.ai).'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '幻觉检测尚未解决，仍是一个活跃的研究领域。在这篇博客文章中，我们将一般介绍任务及其挑战，并介绍在研究论文 [**SELFCHECKGPT: Zero-Resource
    Black-Box Hallucination Detection for Generative Large Language Models**](https://arxiv.org/pdf/2303.08896.pdf?ref=content.whylabs.tds)[1]
    中提出的一种可能的方法。我们将用实际例子说明论文中提出的一些方法，并指出每种方法的一些优缺点。你可以通过访问 [Google Colab Notebook](https://colab.research.google.com/drive/1ftgiASR3TeMaRTQ-cTd81iEoh2j0UP16?usp=sharing&ref=content.whylabs.ai)
    来查看这些示例。'
- en: '*💡* ***Update****: Inspired by the research done in this blog post, we released
    a new feature in* [***LangKit***](https://github.com/whylabs/langkit)*. The* ***response_hallucination***
    *module will automatically calculate consistency scores to help you gain insights
    on the presence of hallucinated responses in your LLM. You can check it out in*
    [*this example notebook*](https://github.com/whylabs/langkit/blob/main/langkit/examples/Response_Consistency.ipynb)'
  id: totrans-12
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*💡* ***更新****：受本博客文章研究的启发，我们在* [***LangKit***](https://github.com/whylabs/langkit)*中发布了一个新功能。*
    ***response_hallucination*** *模块将自动计算一致性分数，帮助你了解 LLM 中幻觉响应的存在。你可以在* [*这个示例笔记本*](https://github.com/whylabs/langkit/blob/main/langkit/examples/Response_Consistency.ipynb)
    *中查看它。*'
- en: 'This blog will cover:'
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 本博客将涵盖：
- en: What Is LLM Hallucination
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 什么是 LLM 幻觉
- en: 'The Approach: SelfCheckGPT'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 方法：SelfCheckGPT
- en: Consistency Check
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一致性检查
- en: 1\. BERTScore
  id: totrans-17
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 1\. BERTScore
- en: 2\. Natural Language Inference
  id: totrans-18
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 2\. 自然语言推理
- en: 3\. LLM Prompt
  id: totrans-19
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 3\. LLM 提示
- en: Experiments
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实验
- en: Conclusion
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 结论
- en: References
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 参考文献
- en: What Is LLM Hallucination
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是 LLM 幻觉
- en: 'In natural language generation, hallucination can be defined as “generated
    content that is nonsensical or unfaithful to the provided source content”[2].
    The source content can represent different functions for different tasks. For
    example, for summarization tasks, the source is the input text being summarized,
    while for generative question-answering tasks, the source is considered the world
    knowledge[2]. Let’s focus on the question-answering task and take a look at one
    example of a user question and the answer given by OpenAI’s GPT model (text-davinci-003):'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在自然语言生成中，幻觉可以定义为“生成的内容是无意义的或不忠于提供的源内容”[2]。源内容可以代表不同任务的不同功能。例如，对于摘要任务，源内容是被摘要的输入文本，而对于生成式问答任务，源内容被认为是世界知识[2]。我们来关注问答任务，并以用户问题和
    OpenAI 的 GPT 模型（text-davinci-003）给出的答案为例：
- en: 'User Prompt: **Who was Philip Hayworth?**'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 用户提示：**菲利普·海沃斯是谁？**
- en: 'LLM Response: **Philip Hayworth was an English barrister and politician who
    served as Member of Parliament for Thetford from 1859 to 1868.**'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: LLM 响应：**菲利普·海沃斯是英国的一名律师和政治家，曾于1859年至1868年担任特特福德的国会议员。**
- en: 'At first sight, that looks like a coherent response, but after a while, we
    might ask ourselves a very reasonable question: is it true?'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 初看起来，这似乎是一个连贯的回答，但过一段时间，我们可能会问一个非常合理的问题：这是真的吗？
- en: We don’t have much information to answer our new question, though. We don’t
    have a context with proven facts we can use to verify the statement. LLMs are
    typically trained under a massive corpora of texts, and it can be hard to verify
    if the statement is grounded in a particular source of information. A quick Google
    search does not show any hits for a person matching the response. What exactly
    constitutes a hallucination is still not well established, but this example could
    be understandably considered a hallucination because the subject with this name,
    location, and employment may not have existed at all.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 不过，我们并没有足够的信息来回答这个新问题。我们没有可以用来验证陈述的经过验证的事实的背景。LLM通常在大量文本中进行训练，因此很难验证陈述是否基于特定的信息来源。快速的Google搜索没有显示出匹配回答的人的任何信息。什么确切构成幻觉仍然没有很好地建立，但这个例子可以被理解为幻觉，因为这个名字、地点和职业的主题可能根本不存在。
- en: 'Let’s try this again with another question:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们再试一次另一个问题：
- en: 'User Prompt: **Who is Bill Gates?**'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 用户提示：**比尔·盖茨是谁？**
- en: 'LLM Response: **Bill Gates is a technology entrepreneur, investor, and philanthropist.
    He is the co-founder of Microsoft Corporation and is one of the wealthiest people
    in the world.**'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: LLM回应：**比尔·盖茨是技术企业家、投资者和慈善家。他是微软公司的共同创始人，也是世界上最富有的人之一。**
- en: Ok, these statements are much easier to verify and are supported by a quick
    check on Wikipedia, so this is unlikely to be a hallucination. During training,
    it is very likely that the LLM has seen a lot of statements about Bill Gates,
    so the following tokens after “Bill Gates is _” will likely be generated with
    high confidence. On the other hand, the LLM might not be sure about which words
    to use after “Philip Hayworth is _”. This insight allows us to link uncertainty
    with factuality, as factual sentences will likely contain tokens predicted with
    a higher probability when compared to hallucinated sentences. However, we might
    not have the output probability distribution at hand for a good number of cases.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，这些陈述更容易验证，并且通过快速检查维基百科得到了支持，所以这不太可能是幻觉。在训练过程中，LLM很可能见过很多关于比尔·盖茨的陈述，因此“比尔·盖茨是_”之后的令牌很可能会以较高的信心生成。另一方面，LLM可能对“Philip
    Hayworth 是_”之后使用哪些词不太确定。这一见解使我们能够将不确定性与真实性联系起来，因为事实句子通常会包含预测概率较高的令牌，而幻觉句子则不然。然而，对于许多案例，我们可能没有手头的输出概率分布。
- en: The example and content of the current session was based on the original paper
    [1], and we will continue to explore the paper’s approach in the following sections.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 本次会议的示例和内容基于原始论文[1]，我们将在接下来的章节中继续探索论文的方法。
- en: 'The Approach: SelfCheckGPT'
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 方法：SelfCheckGPT
- en: 'Throughout the last section, we considered two important considerations for
    our approach: access to an external context and access to the LLM’s output probability
    distribution. When a method does not require an external context or database to
    perform the consistency check, we can call it a **zero-resource** method. Similarly,
    when a method requires only the LLM’s generated text, it can be called a **black-box**
    method.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们考虑了我们方法的两个重要因素：访问外部背景和访问LLM的输出概率分布。当一种方法不需要外部背景或数据库来进行一致性检查时，我们可以称其为**零资源**方法。类似地，当一种方法只需要LLM生成的文本时，可以称之为**黑箱**方法。
- en: The approach we want to talk about in this blog post is a zero-resource black-box
    hallucination detection method and is based on the premise that **sampled responses
    to the same prompt will likely diverge and contradict each other for hallucinated
    facts, and will likely be similar and consistent with each other for factual statements**.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这篇博客文章中要讨论的方法是一种零资源黑箱幻觉检测方法，基于这样一个前提：**对相同提示的采样回答对于幻觉事实可能会出现分歧和矛盾，而对于事实陈述则可能会相似和一致**。
- en: 'Let’s revisit the previous examples. To apply the detection method, we need
    more samples, so let’s ask the LLM the same question three more times:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们重新审视之前的例子。为了应用检测方法，我们需要更多的样本，所以让我们再向LLM提出三个相同的问题：
- en: '![](../Images/6a353b00255ca73e9f5e6b8a22b0adbe.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6a353b00255ca73e9f5e6b8a22b0adbe.png)'
- en: Table by author
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的表格
- en: Indeed, the answers contradict each other — at times, Philip Hayworth is a British
    politician, and in other samples, he is an Australian engineer or an American
    lawyer, who all lived and acted in different periods.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 确实，答案相互矛盾——有时，Philip Hayworth 是一位英国政治家，而在其他样本中，他是澳大利亚工程师或美国律师，他们生活和行动于不同的时期。
- en: 'Let’s compare with the Bill Gates example:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们以比尔·盖茨的例子进行比较：
- en: '![](../Images/3c9d9bc9e5e6b6c8712ed29f0cf3eb18.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3c9d9bc9e5e6b6c8712ed29f0cf3eb18.png)'
- en: Table by author
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 表格作者提供
- en: We can observe that the occupations, organizations, and traits assigned to Bill
    Gates are consistent across samples, with equal or semantically similar terms
    being used.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以观察到，比尔·盖茨分配的职业、组织和特征在样本之间是一致的，使用了相等或语义相似的术语。
- en: Consistency Check
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 一致性检查
- en: Now that we have multiple samples, the final step is to perform a consistency
    check — a way to determine whether the answers agree with each other. This can
    be done in a number of ways, so let’s explore some approaches presented in the
    paper. Feel free to execute the code yourself by checking this [Google Colab Notebook](https://colab.research.google.com/drive/1ftgiASR3TeMaRTQ-cTd81iEoh2j0UP16?usp=sharing&ref=content.whylabs.tds).
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了多个样本，最后一步是进行一致性检查——确定答案是否彼此一致。这可以通过多种方式完成，所以让我们探索一下论文中提出的一些方法。你可以通过查看这个
    [Google Colab Notebook](https://colab.research.google.com/drive/1ftgiASR3TeMaRTQ-cTd81iEoh2j0UP16?usp=sharing&ref=content.whylabs.tds)
    自行执行代码。
- en: BERTScore
  id: totrans-47
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: BERTScore
- en: An intuitive approach to perform this check is by measuring the semantic similarity
    between the samples, and BERTScore[3] is one way to do that. BERTScore computes
    a similarity score for each token in the candidate sentence with each token in
    the reference sentence to calculate a similarity score between the sentences.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 执行此检查的一种直观方法是测量样本之间的语义相似度，而 BERTScore[3] 是一种实现方式。BERTScore 为候选句子中的每个词与参考句子中的每个词计算相似度分数，以计算句子之间的相似度分数。
- en: In the context of SelfCheckGPT, the score is calculated per sentence. Each sentence
    of the original answer will be scored against each sentence of a given sample
    to find the most similar sentence. These maximum similarity scores will be averaged
    across all samples, resulting in a final hallucination score for each sentence
    in the original answer. The final score needs to tend towards 1 for dissimilar
    sentences and 0 for similar sentences, so we need to subtract the similarity score
    from 1.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在 SelfCheckGPT 的背景下，分数是逐句计算的。原始答案的每个句子将与给定样本的每个句子进行评分，以找到最相似的句子。这些最大相似度分数将在所有样本中进行平均，从而为原始答案中的每个句子得到最终的幻觉分数。最终分数需要趋近于
    1（表示不相似的句子）和 0（表示相似的句子），因此我们需要从 1 中减去相似度分数。
- en: 'Let’s show how this works with the first sentence of our original answer being
    checked against the first sample:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们展示如何用原始答案的第一个句子与第一个样本进行检查：
- en: '![](../Images/00483d188ef30cfd8e58a5c812887998.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/00483d188ef30cfd8e58a5c812887998.png)'
- en: Image by author
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 图片作者提供
- en: The maximum score for the first sample is 0.69\. Repeating the process for the
    two remaining samples and assuming the other maximum scores were 0.72 and 0.72,
    our final score for this sentence would be **1 — (0.69+0.72+0.72)/3 = 0.29**.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个样本的最高分是 0.69。重复对剩余两个样本的处理，并假设其他最高分为 0.72 和 0.72，那么我们对该句子的最终分数将是 **1 — (0.69+0.72+0.72)/3
    = 0.29**。
- en: Using semantic similarity to verify consistency is an intuitive approach. Other
    encoders can be used for embedding representations, so it’s also an approach that
    can be further explored.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 使用语义相似度来验证一致性是一种直观的方法。其他编码器也可以用于嵌入表示，因此这也是一种可以进一步探索的方法。
- en: Natural Language Inference
  id: totrans-55
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自然语言推理
- en: Natural language inference is the task of determining entailment, that is, whether
    a hypothesis is true, false, or undetermined based on a premise[4]. In our case,
    each sample is used as the premise and each sentence of the original answer is
    used as our hypothesis. The scores across samples are averaged for each sentence
    to obtain the final score. The entailment is performed with a Deberta model fine-tuned
    to the Multi-NLI dataset[5]. We’ll use the normalized prediction probability instead
    of the actual classes, such as “entailment” or “contradiction,” to compute the
    scores.[6]
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 自然语言推理是确定蕴涵的任务，即根据前提[4]判断一个假设是否为真、假或未确定。在我们的案例中，每个样本用作前提，每个原始答案的句子用作我们的假设。通过对每个句子的样本分数进行平均，得到最终分数。蕴涵通过对
    Multi-NLI 数据集[5] 进行微调的 Deberta 模型来执行。我们将使用归一化预测概率来计算分数，而不是实际类别，如“蕴涵”或“矛盾”。[6]
- en: The entailment task is closer to our goal of consistency checking, so we can
    expect that a model fine-tuned for that purpose will perform well. The author
    also publicly shared [the model on HuggingFace](https://huggingface.co/potsawee/deberta-v3-large-mnli?ref=content.whylabs.tds),
    and other NLI models are publicly available, making this approach very accessible.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 蕴涵任务更接近我们的一致性检查目标，因此我们可以期待为此目的微调的模型会表现良好。作者还在 [HuggingFace](https://huggingface.co/potsawee/deberta-v3-large-mnli?ref=content.whylabs.tds)
    上公开分享了该模型，其他 NLI 模型也公开可用，使得这种方法非常容易获取。
- en: LLM Prompt
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LLM Prompt
- en: 'Considering we already use LLMs to generate the answers and samples, we might
    as well use an LLM to perform the consistency check. We can query the LLM for
    a consistency check for each original sentence and each sample as our context.
    The image below, taken from the original paper’s repository, illustrates how this
    is done:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到我们已经使用 LLM 来生成答案和样本，我们不妨使用 LLM 来执行一致性检查。我们可以对每个原始句子和每个样本进行一致性检查，将 LLM 作为我们的上下文。下面的图片，来自原始论文的仓库，说明了如何进行这个操作：
- en: '![](../Images/d0c3428d1c9cb81ab76b5d1a1b75ec9b.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d0c3428d1c9cb81ab76b5d1a1b75ec9b.png)'
- en: '**SELFCHECKGPT WITH LLM PROMPT. FROM:** [**HTTPS://GITHUB.COM/POTSAWEE/SELFCHECKGPT/TREE/MAIN**](https://github.com/potsawee/selfcheckgpt/tree/main?ref=content.whylabs.ai)'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '**SELFCHECKGPT WITH LLM PROMPT. 来源：** [**HTTPS://GITHUB.COM/POTSAWEE/SELFCHECKGPT/TREE/MAIN**](https://github.com/potsawee/selfcheckgpt/tree/main?ref=content.whylabs.ai)'
- en: The final score can be computed by assigning 1 to “No”, 0 to “Yes”, 0.5 for
    N/A, and averaging the values across samples.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 最终得分可以通过将“否”赋值为 1，“是”赋值为 0，“不适用”赋值为 0.5，并对样本的值进行平均来计算。
- en: Unlike the other two approaches, this one incurs extra calls to the LLM of your
    choice, meaning additional latency and, possibly, additional costs. On the other
    hand, we can leverage the LLM’s capabilities to help us perform this check.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 与其他两种方法不同，这种方法需要额外调用你选择的 LLM，这意味着额外的延迟和可能的额外成本。另一方面，我们可以利用 LLM 的能力来帮助我们进行检查。
- en: Experiments
  id: totrans-64
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实验
- en: Let’s see what we get as results for the two examples we’ve been discussing
    for each of the three approaches.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看在三种方法中讨论的两个示例的结果如何。
- en: '![](../Images/bbb277288b2562da66001aba60dd5e1d.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bbb277288b2562da66001aba60dd5e1d.png)'
- en: Table by author
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的表格
- en: These values are solely meant to illustrate the method. With only three sentences,
    it’s not supposed to be a means to compare and determine which approach is best.
    For that purpose, the original paper shares the experimental results on the paper’s
    repository [here](https://github.com/potsawee/selfcheckgpt/tree/main?ref=content.whylabs.ai#experimental-results),
    which includes additional versions that weren’t discussed in this blog post. I
    won’t go into the details of the results, but by all three metrics (NonFact, Factual,
    and Ranking), the LLM-Prompt is the best-performing version, closely followed
    by the NLI version. The BERTScore version looks to be considerably worse than
    the remaining two. Our simple examples seem to follow along the lines of the shared
    results.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 这些值仅用于说明方法。只有三个句子的情况下，它不应该用来比较和确定哪种方法最佳。为此，原始论文在论文的仓库中分享了实验结果 [这里](https://github.com/potsawee/selfcheckgpt/tree/main?ref=content.whylabs.ai#experimental-results)，包括了在这篇博客中未讨论的附加版本。我不会详细讨论结果，但根据所有三个指标（NonFact、Factual
    和 Ranking），LLM-Prompt 是表现最好的版本，其次是 NLI 版本。BERTScore 版本明显比剩余两个版本要差。我们的简单示例似乎符合共享结果的方向。
- en: Conclusion
  id: totrans-69
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: We hope this blog post helped explain the hallucination problem and provides
    one possible solution for hallucination detection. This is a relatively new problem,
    and it’s good to see that efforts are being made towards solving it.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望这篇博客文章有助于解释幻觉问题，并提供一种可能的幻觉检测解决方案。这是一个相对较新的问题，很高兴看到已经有努力在解决它。
- en: 'The discussed approach has the advantage of not requiring external context
    (zero-resource) and also not requiring the LLM’s output probability distribution
    (black-box). However, this comes with a cost: in addition to the original response,
    we need to generate extra samples to perform the consistency check, increasing
    latency and cost. The consistency check will also require additional computation
    and language models for encoding the responses into embeddings, performing textual
    entailment, or querying the LLM, depending on the chosen method.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 讨论的方法具有不需要外部上下文（零资源）和不需要LLM的输出概率分布（黑箱）的优点。然而，这也带来了成本：除了原始响应外，我们还需要生成额外的样本来执行一致性检查，从而增加了延迟和成本。一致性检查还需要额外的计算和语言模型来将响应编码为嵌入，进行文本蕴含，或查询LLM，这取决于所选的方法。
- en: References
  id: totrans-72
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] — Manakul, Potsawee, Adian Liusie, and Mark JF Gales. “Selfcheckgpt: Zero-resource
    black-box hallucination detection for generative large language models.” arXiv
    preprint arXiv:2303.08896 (2023).'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] — Manakul, Potsawee, Adian Liusie, 和 Mark JF Gales。“Selfcheckgpt：用于生成大型语言模型的零资源黑箱幻觉检测。”
    arXiv预印本 arXiv:2303.08896 (2023)。'
- en: '[2] — JI, Ziwei et al. Survey of hallucination in natural language generation.
    **ACM Computing Surveys**, v. 55, n. 12, p. 1–38, 2023.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] — JI, Ziwei 等人。《自然语言生成中的幻觉调查》。**ACM计算调查**，第55卷，第12期，页码1–38，2023年。'
- en: '[3] — ZHANG, Tianyi et al. Bertscore: Evaluating text generation with bert.
    **arXiv preprint arXiv:1904.09675**, 2019.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] — ZHANG, Tianyi 等人。Bertscore：使用bert评估文本生成。**arXiv预印本 arXiv:1904.09675**，2019年。'
- en: '[4] — [https://nlpprogress.com/english/natural_language_inference.html](https://nlpprogress.com/english/natural_language_inference.html?ref=content.whylabs.ai)'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] — [https://nlpprogress.com/english/natural_language_inference.html](https://nlpprogress.com/english/natural_language_inference.html?ref=content.whylabs.ai)'
- en: '[5] — Williams, A., Nangia, N., & Bowman, S. R. (2017). A broad-coverage challenge
    corpus for sentence understanding through inference. arXiv preprint arXiv:1704.05426.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] — Williams, A., Nangia, N., & Bowman, S. R. (2017)。用于通过推理理解句子的广泛覆盖挑战语料库。arXiv预印本
    arXiv:1704.05426。'
- en: '[6] — [https://github.com/potsawee/selfcheckgpt/tree/main#selfcheckgpt-usage-nli](https://github.com/potsawee/selfcheckgpt/tree/main#selfcheckgpt-usage-nli)'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '[6] — [https://github.com/potsawee/selfcheckgpt/tree/main#selfcheckgpt-usage-nli](https://github.com/potsawee/selfcheckgpt/tree/main#selfcheckgpt-usage-nli)'
