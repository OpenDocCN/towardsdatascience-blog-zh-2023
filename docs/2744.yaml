- en: A Beginner’s Guide to LLM Fine-Tuning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LLM微调初学者指南
- en: 原文：[https://towardsdatascience.com/a-beginners-guide-to-llm-fine-tuning-4bae7d4da672?source=collection_archive---------1-----------------------#2023-08-30](https://towardsdatascience.com/a-beginners-guide-to-llm-fine-tuning-4bae7d4da672?source=collection_archive---------1-----------------------#2023-08-30)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/a-beginners-guide-to-llm-fine-tuning-4bae7d4da672?source=collection_archive---------1-----------------------#2023-08-30](https://towardsdatascience.com/a-beginners-guide-to-llm-fine-tuning-4bae7d4da672?source=collection_archive---------1-----------------------#2023-08-30)
- en: How to fine-tune Llama and other LLMs with one tool
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何使用一种工具微调Llama和其他LLMs
- en: '[](https://medium.com/@mlabonne?source=post_page-----4bae7d4da672--------------------------------)[![Maxime
    Labonne](../Images/a7efdd305e3cc77d5509bbb1076d57d8.png)](https://medium.com/@mlabonne?source=post_page-----4bae7d4da672--------------------------------)[](https://towardsdatascience.com/?source=post_page-----4bae7d4da672--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----4bae7d4da672--------------------------------)
    [Maxime Labonne](https://medium.com/@mlabonne?source=post_page-----4bae7d4da672--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@mlabonne?source=post_page-----4bae7d4da672--------------------------------)[![Maxime
    Labonne](../Images/a7efdd305e3cc77d5509bbb1076d57d8.png)](https://medium.com/@mlabonne?source=post_page-----4bae7d4da672--------------------------------)[](https://towardsdatascience.com/?source=post_page-----4bae7d4da672--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----4bae7d4da672--------------------------------)
    [Maxime Labonne](https://medium.com/@mlabonne?source=post_page-----4bae7d4da672--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fdc89da634938&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-beginners-guide-to-llm-fine-tuning-4bae7d4da672&user=Maxime+Labonne&userId=dc89da634938&source=post_page-dc89da634938----4bae7d4da672---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----4bae7d4da672--------------------------------)
    ·8 min read·Aug 30, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F4bae7d4da672&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-beginners-guide-to-llm-fine-tuning-4bae7d4da672&user=Maxime+Labonne&userId=dc89da634938&source=-----4bae7d4da672---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fdc89da634938&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-beginners-guide-to-llm-fine-tuning-4bae7d4da672&user=Maxime+Labonne&userId=dc89da634938&source=post_page-dc89da634938----4bae7d4da672---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----4bae7d4da672--------------------------------)
    ·8分钟阅读·2023年8月30日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F4bae7d4da672&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-beginners-guide-to-llm-fine-tuning-4bae7d4da672&user=Maxime+Labonne&userId=dc89da634938&source=-----4bae7d4da672---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4bae7d4da672&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-beginners-guide-to-llm-fine-tuning-4bae7d4da672&source=-----4bae7d4da672---------------------bookmark_footer-----------)![](../Images/3cd56f68c14e07ab9ae3eb624bd064ed.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[无](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4bae7d4da672&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-beginners-guide-to-llm-fine-tuning-4bae7d4da672&source=-----4bae7d4da672---------------------bookmark_footer-----------)![](../Images/3cd56f68c14e07ab9ae3eb624bd064ed.png)'
- en: Image by author
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供
- en: The growing interest in Large Language Models (LLMs) has led to a surge in **tools
    and wrappers designed to streamline their training process**.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 对大型语言模型（LLMs）的兴趣日益增长，导致了**旨在简化其训练过程的工具和封装的激增**。
- en: Popular options include [FastChat](https://github.com/lm-sys/FastChat) from
    LMSYS (used to train [Vicuna](https://huggingface.co/lmsys/vicuna-13b-v1.5)) and
    Hugging Face’s [transformers](https://github.com/huggingface/transformers)/[trl](https://github.com/huggingface/trl)
    libraries (used in [my previous article](/fine-tune-your-own-llama-2-model-in-a-colab-notebook-df9823a04a32)).
    In addition, each big LLM project, like [WizardLM](https://github.com/nlpxucan/WizardLM/tree/main),
    tends to have its own training script, inspired by the original [Alpaca](https://github.com/tatsu-lab/stanford_alpaca)
    implementation.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 流行的选择包括 LMSYS 的[FastChat](https://github.com/lm-sys/FastChat)（用于训练[Vicuna](https://huggingface.co/lmsys/vicuna-13b-v1.5)）和
    Hugging Face 的[transformers](https://github.com/huggingface/transformers)/[trl](https://github.com/huggingface/trl)库（用于[我之前的文章](/fine-tune-your-own-llama-2-model-in-a-colab-notebook-df9823a04a32)）。此外，每个大型
    LLM 项目，如[WizardLM](https://github.com/nlpxucan/WizardLM/tree/main)，往往都有自己的训练脚本，灵感来自原始的[Alpaca](https://github.com/tatsu-lab/stanford_alpaca)实现。
- en: In this article, we will use [**Axolotl**](https://github.com/OpenAccess-AI-Collective/axolotl),
    a tool created by the OpenAccess AI Collective. We will use it to fine-tune a
    [**Code Llama 7b**](https://github.com/OpenAccess-AI-Collective/axolotl/blob/main/examples/llama-2/qlora.yml)
    model on an evol-instruct dataset comprised of 1,000 samples of Python code.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们将使用由 OpenAccess AI Collective 创建的[**Axolotl**](https://github.com/OpenAccess-AI-Collective/axolotl)工具。我们将利用它在一个包含
    1,000 个 Python 代码样本的 evol-instruct 数据集上微调一个[**Code Llama 7b**](https://github.com/OpenAccess-AI-Collective/axolotl/blob/main/examples/llama-2/qlora.yml)模型。
- en: 🤔 Why Axolotl?
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 🤔 为什么选择 Axolotl？
- en: 'The main appeal of Axolotl is that it provides a one-stop solution, which includes
    numerous features, model architectures, and an active community. Here’s a quick
    list of my favorite things about it:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: Axolotl 的主要吸引力在于它提供了一站式解决方案，包括众多功能、模型架构和一个活跃的社区。以下是我最喜欢的一些特点：
- en: '**Configuration**: All parameters used to train an LLM are neatly stored in
    a yaml config file. This makes it convenient for sharing and reproducing models.
    You can see an example for Llama 2 [here](https://github.com/OpenAccess-AI-Collective/axolotl/tree/main/examples/llama-2).'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**配置**：用于训练 LLM 的所有参数都整齐地存储在一个 yaml 配置文件中。这使得共享和重现模型变得非常方便。你可以查看 Llama 2 的示例[在这里](https://github.com/OpenAccess-AI-Collective/axolotl/tree/main/examples/llama-2)。'
- en: '**Dataset Flexibility**: Axolotl allows the specification of multiple datasets
    with varied prompt formats such as…'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据集灵活性**：Axolotl 允许指定多个数据集，并支持各种提示格式，例如……'
