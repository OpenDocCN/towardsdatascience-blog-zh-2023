- en: 1.5 Years of Spark Knowledge in 8 Tips
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 1.5 年的 Spark 知识总结为 8 个技巧
- en: 原文：[https://towardsdatascience.com/1-5-years-of-spark-knowledge-in-8-tips-f003c4743083?source=collection_archive---------0-----------------------#2023-12-24](https://towardsdatascience.com/1-5-years-of-spark-knowledge-in-8-tips-f003c4743083?source=collection_archive---------0-----------------------#2023-12-24)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/1-5-years-of-spark-knowledge-in-8-tips-f003c4743083?source=collection_archive---------0-----------------------#2023-12-24](https://towardsdatascience.com/1-5-years-of-spark-knowledge-in-8-tips-f003c4743083?source=collection_archive---------0-----------------------#2023-12-24)
- en: My learnings from Databricks customer engagements
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我在 Databricks 客户合作中的学习经验
- en: '[](https://michaelberk.medium.com/?source=post_page-----f003c4743083--------------------------------)[![Michael
    Berk](../Images/c79c07ed3973cad1305a1d970aaea0b5.png)](https://michaelberk.medium.com/?source=post_page-----f003c4743083--------------------------------)[](https://towardsdatascience.com/?source=post_page-----f003c4743083--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----f003c4743083--------------------------------)
    [Michael Berk](https://michaelberk.medium.com/?source=post_page-----f003c4743083--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://michaelberk.medium.com/?source=post_page-----f003c4743083--------------------------------)[![Michael
    Berk](../Images/c79c07ed3973cad1305a1d970aaea0b5.png)](https://michaelberk.medium.com/?source=post_page-----f003c4743083--------------------------------)[](https://towardsdatascience.com/?source=post_page-----f003c4743083--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----f003c4743083--------------------------------)
    [Michael Berk](https://michaelberk.medium.com/?source=post_page-----f003c4743083--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fe101cd051ea3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F1-5-years-of-spark-knowledge-in-8-tips-f003c4743083&user=Michael+Berk&userId=e101cd051ea3&source=post_page-e101cd051ea3----f003c4743083---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----f003c4743083--------------------------------)
    ·8 min read·Dec 24, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Ff003c4743083&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F1-5-years-of-spark-knowledge-in-8-tips-f003c4743083&user=Michael+Berk&userId=e101cd051ea3&source=-----f003c4743083---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fe101cd051ea3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F1-5-years-of-spark-knowledge-in-8-tips-f003c4743083&user=Michael+Berk&userId=e101cd051ea3&source=post_page-e101cd051ea3----f003c4743083---------------------post_header-----------)
    发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----f003c4743083--------------------------------)
    · 8 分钟阅读 · 2023 年 12 月 24 日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Ff003c4743083&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F1-5-years-of-spark-knowledge-in-8-tips-f003c4743083&user=Michael+Berk&userId=e101cd051ea3&source=-----f003c4743083---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff003c4743083&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F1-5-years-of-spark-knowledge-in-8-tips-f003c4743083&source=-----f003c4743083---------------------bookmark_footer-----------)![](../Images/a363f6cea7c86a50b902af7da7137544.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff003c4743083&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F1-5-years-of-spark-knowledge-in-8-tips-f003c4743083&source=-----f003c4743083---------------------bookmark_footer-----------)![](../Images/a363f6cea7c86a50b902af7da7137544.png)'
- en: 'Figure 1: a technical diagram of how to write apache spark. Image by author.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：如何编写 Apache Spark 的技术图示。图片由作者提供。
- en: At Databricks, I help large retail organizations deploy and scale data and machine
    learning pipelines. Here are the 8 most important [spark](https://spark.apache.org/)
    tips/tricks I’ve learned in the field.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Databricks，我帮助大型零售组织部署和扩展数据及机器学习管道。以下是我在实际工作中学到的 8 个最重要的 [spark](https://spark.apache.org/)
    技巧/窍门。
- en: Throughout this post, we assume a general working knowledge of spark and it’s
    structure, but this post should be accessible to all levels.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们假设读者对 Spark 及其结构有一般的工作知识，但本文应对所有层次的读者都适用。
- en: Let’s dive in!
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们深入探讨一下吧！
- en: 0 — Quick Review
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 0 — 快速回顾
- en: Quickly, let’s review what spark does…
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 快速回顾一下 Spark 的功能…
- en: Spark is a big data processing engine. It takes python/java/scala/R/SQL and
    converts that code into a highly optimized set of transformations.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 是一个大数据处理引擎。它接受 Python/Java/Scala/R/SQL 并将这些代码转换为一组高度优化的转换操作。
- en: '![](../Images/06c30062d226c82299f50052ff6b6abf.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/06c30062d226c82299f50052ff6b6abf.png)'
- en: 'Figure 2: spark driver and worker configuration. Image by author.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：Spark 驱动程序和工作节点配置。图像作者提供。
- en: At it’s lowest level, spark creates tasks, which are **parallelizable transformations
    on data partitions**. These tasks are then distributed across from a driver node
    to worker nodes, which are responsible for leveraging their CPU cores to complete
    the transformations. By distributing tasks to potentially many workers, spark
    allows us to horizontally scale and thereby support complex data pipelines that
    would be impossible on a single machine.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在最低层次上，Spark 创建任务，这些任务是 **对数据分区的可并行变换**。这些任务从驱动节点分配到工作节点，工作节点负责利用它们的 CPU 核心来完成变换。通过将任务分配给可能许多的工作节点，Spark
    允许我们进行水平扩展，从而支持在单台机器上不可能实现的复杂数据管道。
- en: Ok, hopefully not all of that was new information. Either way, in the following
    sections we’ll slow down a bit. These tips should help both novices and intermediates
    at spark.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧，希望这些信息不是全新的。不管怎样，在接下来的部分，我们会慢下来一点。这些提示应该对 Spark 的初学者和中级用户都有帮助。
- en: 1 — Spark is a Grocery Store
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 1 — Spark 就是一个杂货店
- en: 'Spark is complex. To help both you and potentially others understand its structure,
    let’s leverage an impressively good analogy borrowed from [queueing theory](https://en.wikipedia.org/wiki/Queueing_theory):
    **spark is a grocery store.**'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 是复杂的。为了帮助你和其他人理解其结构，我们借用 [排队理论](https://en.wikipedia.org/wiki/Queueing_theory)
    中一个非常好的类比：**spark 就像是一个杂货店**。
- en: When thinking about the distributed computing component of spark, there are
    three main components….
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 当考虑 Spark 的分布式计算组件时，有三个主要组件……
- en: '**Data partitions:** subsets of rows of our data. In our grocery store, they
    are **groceries.**'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据分区：** 我们数据的行子集。在我们的杂货店中，它们是 **杂货**。'
- en: '**Spark tasks:** low-level transformations performed on a data partition. In
    our grocery store, they are **customers**.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Spark 任务：** 在数据分区上执行的低级变换。在我们的杂货店中，它们是 **顾客**。'
- en: '**Cores:** the part of your processor(s) that do work in parallel. In our grocery
    store, they are **cashiers**.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**核心：** 你处理器中并行工作的部分。在我们的杂货店中，它们是 **收银员**。'
- en: That’s it!
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样！
- en: Now, let’s leverage these concepts to talk through some fundamentals of spark.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们利用这些概念来讨论一些 Spark 的基础知识。
- en: '![](../Images/7c684a57bb5357a02622754e4786b018.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7c684a57bb5357a02622754e4786b018.png)'
- en: 'Figure 3: illustration of the cashier analog, specifically for data skew. Image
    by author.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：收银员类比的插图，特别是针对数据倾斜。图像作者提供。
- en: As show in figure 3, our cashiers (cores) can only process one customer (task)
    at a time. Furthermore, some customers have a lot of groceries (partition row
    count), as shown by the first customer at cashier 2\. From these simple observations…
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 如图 3 所示，我们的收银员（核心）一次只能处理一个顾客（任务）。此外，一些顾客有很多杂货（分区行数），如图中收银员 2 处的第一个顾客所示。从这些简单的观察……
- en: The more cashiers (cores), the more customers (tasks) you can process in parallel.
    This is [horizontal/vertical scaling](https://www.geeksforgeeks.org/horizontal-and-vertical-scaling-in-databases/#).
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 收银员（核心）越多，你可以并行处理的顾客（任务）就越多。这是 [水平/垂直扩展](https://www.geeksforgeeks.org/horizontal-and-vertical-scaling-in-databases/#)。
- en: If you don’t have enough customers (tasks) to saturate your cashiers (cores),
    you’ll be paying for the cashier to sit there. This relates to [autoscaling](https://www.databricks.com/blog/2018/05/02/introducing-databricks-optimized-auto-scaling.html),
    cluster sizing, and partition sizing.
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你没有足够的顾客（任务）来充分利用你的收银员（核心），你就会支付收银员的空闲时间。这与 [自动扩展](https://www.databricks.com/blog/2018/05/02/introducing-databricks-optimized-auto-scaling.html)、集群大小和分区大小有关。
- en: If customers (tasks) have very different amounts of groceries (partition row
    counts), you’ll see uneven utilization of your cashiers. This is [**data skew**](/data-skew-in-pyspark-783d529a9dd7).
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果顾客（任务）的杂货（分区行数）差异很大，你会看到收银员的利用率不均。这是 [**数据倾斜**](/data-skew-in-pyspark-783d529a9dd7)。
- en: The better your cashiers (cores), the faster they can process a single customer
    (task). This relates to upgrading your processor.
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你的收银员（核心）越好，他们处理单个顾客（任务）的速度就越快。这涉及到升级你的处理器。
- en: etc.
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 等等。
- en: Given the analogy comes from queueing theory, a field directly related to distributed
    computing, it’s quite powerful!
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这个类比来源于直接与分布式计算相关的排队理论，它的解释能力非常强！
- en: Use this analogy to debug, communicate, and develop spark.
  id: totrans-36
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 使用这个类比来调试、沟通和开发 Spark。
- en: 2— Collect Data to Memory Once
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 2— 将数据一次性收集到内存中
- en: The most common mistake for spark novices is misunderstanding lazy evaluation.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 初学者最常见的错误是误解了延迟计算。
- en: '[Lazy evaluation](https://medium.com/@think-data/mastering-lazy-evaluation-a-must-know-for-pyspark-pros-ac855202495e)
    means that no data transformations will be performed until you invoke a collection
    to memory. Examples of methods that invoke a collection include but are not limited
    to…'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '[懒惰求值](https://medium.com/@think-data/mastering-lazy-evaluation-a-must-know-for-pyspark-pros-ac855202495e)
    意味着在调用内存集合之前不会执行任何数据转换。调用集合的方法示例包括但不限于……'
- en: '[.collect()](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.collect.html):
    bring the DataFrame into memory as a python list.'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[.collect()](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.collect.html)：将
    DataFrame 载入内存作为 Python 列表。'
- en: '[.show()](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.show.html):
    print the first `n` rows of your DataFrame.'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[.show()](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.show.html)：打印
    DataFrame 的前 `n` 行。'
- en: '[.count()](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.count.html):
    get the number of rows of your DataFrame.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[.count()](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.count.html)：获取
    DataFrame 的行数。'
- en: '[.first()](https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.sql.functions.first.html):
    get the first row of your DataFrame.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[.first()](https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.sql.functions.first.html)：获取
    DataFrame 的第一行。'
- en: The single most common incorrect collection method is leveraging `.count()`
    throughout a program. Every time you invoke a collection, all upstream transformations
    will be recomputed from scratch, so if you have 5 invocations of `.count()`, your
    program will asymptotically run 5x as long.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 最常见的错误集合方法是程序中普遍使用 `.count()`。每次你调用集合时，所有上游的转换都会从头开始重新计算，因此如果你有 5 次 `.count()`
    调用，你的程序将呈渐近地运行 5 倍时间。
- en: Spark is lazily evaluated! Pipelines should have a single flow from source(s)
    to target(s).
  id: totrans-45
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Spark 是懒惰求值的！管道应该有一个从源头到目标的单一流动。
- en: 3— Meet the SLA then Stop
  id: totrans-46
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 3— 达到服务级别协议后停止
- en: A surprisingly common issue that’s come up when working with large organizations
    is they lose sight of the big picture and thereby optimize pipelines in an inefficient
    manner.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在与大型组织合作时，一个出乎意料的常见问题是他们忽视了大局，从而以低效的方式优化管道。
- en: Here’s how pipelines should be optimized for the majority of use cases…
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 这是大多数用例中管道优化的方法……
- en: '**Ask if we need to do the project.** Put simply, think about what you’re actually
    getting from optimizing a pipeline. If you expect to improve runtime by 20% and
    the pipeline costs $100 to run, should you invest your extremely expensive data
    engineer’s salary to save $20 per run? Maybe. Maybe not.'
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**问问我们是否需要做这个项目。** 简而言之，考虑一下优化管道实际上能带来什么。如果你期望运行时间提高20%，而管道的运行成本是$100，那么你是否应该投入你极其昂贵的数据工程师的薪资以节省每次运行的$20？也许吧，也许不是。'
- en: '**Look for low hanging fruit in the code.** After agreeing to do the project,
    check if the code has obvious flaws. Examples are misuse of lazy evaluation, unnecessary
    transformations, and incorrect ordering of transformations.'
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**寻找代码中的低悬果实。** 在同意做这个项目后，检查代码是否有明显的缺陷。示例包括懒惰求值的误用、不必要的转换和转换顺序不正确。'
- en: '**Get the job running under the SLA by leveraging compute.** After checking
    that the code is relatively efficient, just throw compute at the problem so you
    can 1) meet the SLA and, 2) gather statistics from the spark UI.'
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**利用计算来使任务在服务级别协议下运行。** 在检查代码相对高效后，直接将计算资源投入问题中，以便你可以 1) 达到服务级别协议，并且 2) 从 Spark
    UI 收集统计数据。'
- en: '**Stop.** If you’re properly saturating your compute and cost isn’t egregious,
    do some last minute compute improvements then stop. Your time is valuable. Don’t
    waste it saving dollars when you could be creating thousands of dollars elsewhere.'
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**停止。** 如果你已经充分利用了计算资源，并且成本不算过高，做一些最后的计算改进后就停下来。你的时间是宝贵的。不要浪费时间去节省美元，而是在其他地方创造数千美元。'
- en: '**Deep dive.** Finally, if you really need to deep dive because cost is unacceptable,
    then roll up your sleeves and optimize data, code, and compute.'
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**深入挖掘。** 最后，如果你真的需要深入挖掘，因为成本不可接受，那么就卷起袖子，优化数据、代码和计算。'
- en: The beauty of this framework is that 1–4 only require cursory knowledge of spark
    and are very quick to execute; sometimes you can collect information on steps
    1–4 during a 30 minute call. The framework also ensures that we’ll stop as soon
    as we are *good enough*. Finally, if step 5 is needed, we can delegate that to
    those on the team who are strongest at spark.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 这个框架的美妙之处在于，1–4只需要对Spark有基本了解，并且执行非常快速；有时你可以在30分钟的电话会议中收集1–4的相关信息。该框架还确保我们会在“足够好”时立即停止。最后，如果需要第5步，我们可以将其委托给团队中最强的Spark专家。
- en: By finding all the ways to avoid over-optimizing a pipeline, you’re saving precious
    developer hours.
  id: totrans-55
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 通过找出所有避免过度优化管道的方法，你可以节省宝贵的开发时间。
- en: 4 — Disk Spill
  id: totrans-56
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 4—磁盘溢出
- en: Disk spill is the single most common reason that spark jobs run slow.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 磁盘溢出是Spark作业运行缓慢的最常见原因。
- en: It’s a very simple concept. Spark is designed to leverage in-memory processing.
    If you don’t have enough memory, spark will try to write the extra data to disk
    to prevent your process from crashing. This is called disk spill.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个非常简单的概念。Spark设计用于利用内存处理。如果内存不足，Spark将尝试将额外的数据写入磁盘以防止进程崩溃。这称为磁盘溢出。
- en: '![](../Images/8965e74adda45122e1a8357d1745c172.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8965e74adda45122e1a8357d1745c172.png)'
- en: 'Figure 4: screen shot of the spark UI highlighting disk spill. Image by author.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：突出显示磁盘溢出的Spark UI屏幕截图。图片由作者提供。
- en: Writing to and reading from disk is slow, so it should be avoided. If you want
    to learn how to identify and mitigate spill, follow [this tutorial](https://selectfrom.dev/spark-performance-tuning-spill-7318363e18cb).
    However, some very common and simple methods to mitigate spill are…
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 写入和读取磁盘速度较慢，因此应尽量避免。如果你想了解如何识别和减轻溢出，请参考[这个教程](https://selectfrom.dev/spark-performance-tuning-spill-7318363e18cb)。然而，一些非常常见且简单的方法来减轻溢出是…
- en: Process less data per task, which can be achieved by changing the partition
    count via [spark.shuffle.partitions](https://spark.apache.org/docs/latest/sql-performance-tuning.html#other-configuration-options)
    or [repartition](https://spark.apache.org/docs/3.1.3/api/python/reference/api/pyspark.sql.DataFrame.repartition.html).
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 每个任务处理更少的数据，这可以通过更改分区数量来实现，方法是通过[spark.shuffle.partitions](https://spark.apache.org/docs/latest/sql-performance-tuning.html#other-configuration-options)或[repartition](https://spark.apache.org/docs/3.1.3/api/python/reference/api/pyspark.sql.DataFrame.repartition.html)。
- en: Increase the RAM to core ratio in your compute.
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 增加计算中的RAM与核心的比率。
- en: If you want your job to run optimally, prevent spill.
  id: totrans-64
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 如果你希望你的作业运行得更佳，请防止溢出。
- en: 5— Use SQL Syntax
  id: totrans-65
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 5—使用SQL语法
- en: Whether you’re using scala, java, python, SQL, or R, spark will always leverage
    the same transformations under the hood. So, use the the right language for your
    task.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 无论你使用Scala、Java、Python、SQL还是R，Spark在底层总是利用相同的转换。因此，选择适合你任务的语言。
- en: 'SQL is the least verbose “language” out of all supported spark languages for
    many operations! More tangibly:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 对于许多操作，SQL是支持的Spark语言中最简洁的“语言”！更具体地说：
- en: If you’re adding or modifying a column, use [selectExpr](https://spark.apache.org/docs/3.1.3/api/python/reference/api/pyspark.sql.DataFrame.selectExpr.html)
    or [expr](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.expr.html),
    especially paired with Python’s [f-strings](https://realpython.com/python-f-strings/).
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你在添加或修改列，请使用[selectExpr](https://spark.apache.org/docs/3.1.3/api/python/reference/api/pyspark.sql.DataFrame.selectExpr.html)或[expr](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.expr.html)，尤其是与Python的[f-strings](https://realpython.com/python-f-strings/)配合使用。
- en: If you need complex SQL, create temp views then use [spark.sql()](https://spark.apache.org/docs/latest/api/sql/index.html).
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你需要复杂的SQL，创建临时视图然后使用[spark.sql()](https://spark.apache.org/docs/latest/api/sql/index.html)。
- en: Here are two quick examples…
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有两个快速示例…
- en: '[PRE0]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '[PRE1]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Use SQL.
  id: totrans-73
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 使用SQL。
- en: 6— Glob Filters
  id: totrans-74
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 6—全局过滤器
- en: Do you need to read a bunch of data files stored in a complex directory? If
    so, use spark’s extremely powerful [read options](https://spark.apache.org/docs/latest/sql-data-sources-generic-options.html#generic-file-source-options).
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要读取存储在复杂目录中的一堆数据文件吗？如果是这样，请使用Spark极其强大的[读取选项](https://spark.apache.org/docs/latest/sql-data-sources-generic-options.html#generic-file-source-options)。
- en: The first time I encountered this problem, I rewrote [os.walk](https://www.tutorialspoint.com/python/os_walk.htm)
    to work with my cloud provider where data was stored. I very proudly showed this
    method to my project partner who simply said, “let me share my screen,” and proceeded
    to introduce me to glob filters.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 我第一次遇到这个问题时，我重写了[os.walk](https://www.tutorialspoint.com/python/os_walk.htm)以与数据存储在的云提供商一起使用。我非常自豪地向我的项目伙伴展示了这个方法，他只是说，“让我分享我的屏幕，”然后向我介绍了全局过滤器。
- en: '[PRE2]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: When I applied the glob filter shown above instead of my custom os.walk, the
    ingestion operation was over 10x faster.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 当我应用上面显示的glob过滤器，而不是我自定义的os.walk时，数据摄取操作的速度提高了超过10倍。
- en: Spark has powerful parameters. Check if your desired functionality exists before
    building bespoke implementations.
  id: totrans-79
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Spark有强大的参数。在构建定制实现之前，请检查是否存在所需的功能。
- en: 7 — Use Reduce with DataFrame.Union
  id: totrans-80
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 7 — 使用Reduce与DataFrame.Union
- en: Loops are almost always detrimental to spark performance. Here’s why…
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 循环几乎总是对spark性能有害。原因如下…
- en: Spark has two core phases — planning and execution. In the planning phase, spark
    creates a directed acyclical graph (DAG) which indicates how your specified transformations
    will be carried out. The planning phase is relatively expensive and can sometimes
    take several seconds, so you want to invoke it as infrequently as possible.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: Spark有两个核心阶段 — 规划和执行。在规划阶段，spark创建一个有向无环图（DAG），指示如何执行指定的转换。规划阶段相对昂贵，有时可能需要几秒钟，因此你希望尽可能少地调用它。
- en: Let’s discuss a use case where you must iterate through many DataFrames, perform
    expensive transformations, then append them to a table.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们讨论一个必须遍历许多DataFrame、执行昂贵转换，然后将它们追加到表中的用例。
- en: First, there is native support for nearly all iterative use cases, specifically
    [pandas UDFs](https://stackoverflow.com/questions/58170261/how-to-use-pandas-udf-in-class),
    window functions, and joins. But, if you truly do need a loop, here’s how you
    invoke a single planning phase and thereby get all transformations in a single
    DAG.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，几乎所有的迭代使用案例都得到了本地支持，特别是[ pandas UDFs](https://stackoverflow.com/questions/58170261/how-to-use-pandas-udf-in-class)、窗口函数和连接。但如果你确实需要一个循环，下面是如何调用一个单一的规划阶段，从而在一个DAG中完成所有变换。
- en: '[PRE3]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The first solution uses a for loop to iterate over paths, do fancy transformations,
    then append to our delta table of interest. In the second, we store a list of
    lazily evaluated DataFrames, apply transformations over them, then reduce them
    via a union, performing a single spark plan and write.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个解决方案使用for循环遍历路径，进行复杂转换，然后追加到我们感兴趣的delta表中。在第二种方法中，我们存储一个惰性计算的DataFrame列表，对它们进行转换，然后通过合并进行简化，执行一个spark计划并写入。
- en: We can actually see the difference in architecture on the backend via the Spark
    UI…
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 我们实际上可以通过Spark UI看到后端架构的差异…
- en: '![](../Images/d94967682a2201899b5b7897edb06847.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d94967682a2201899b5b7897edb06847.png)'
- en: 'Figure 5: spark DAG for for loop vs. functools.reduce. Image by author.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：for循环与functools.reduce的spark DAG。图片由作者提供。
- en: In figure 5, the DAG on the left corresponding to the for loop will have 10
    stages. However, the DAG on the right corresponding to `functools.reduce` will
    have a single stage and thereby can be processed more easily in parallel.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在图5中，左侧对应于for循环的DAG将有10个阶段。然而，右侧对应于`functools.reduce`的DAG将只有一个阶段，因此可以更容易地进行并行处理。
- en: For a simple use case of reading 400 unique delta tables then appending to a
    delta table, this method was 6x faster than a for loop.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 对于读取400个唯一delta表并追加到一个delta表的简单用例，这种方法比for循环快6倍。
- en: Get creative to create a single spark DAG.
  id: totrans-92
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 创造性地创建一个单一的spark DAG。
- en: 8 — Use ChatGPT
  id: totrans-93
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 8 — 使用ChatGPT
- en: This is not about hype.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 这不是关于炒作。
- en: Spark is a well-establish and thereby well-documented piece of software. LLMs,
    specifically GPT-4, are really good at distilling complex information into digestible
    and concise explanations. Since the release of GPT-4, I have not done a complex
    spark project where I didn’t heavily rely on GPT-4.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: Spark是一个成熟且有充分文档的软件。LLMs，特别是GPT-4，擅长将复杂信息提炼成易于理解和简明的解释。自从GPT-4发布以来，我没有做过一个复杂的spark项目而没有依赖GPT-4。
- en: '![](../Images/ac29f4bd334512d8294617a5a8e6fe9e.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ac29f4bd334512d8294617a5a8e6fe9e.png)'
- en: 'Figure 6: example of GPT-4 output on impacting data partition size in spark.
    Image by author.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：GPT-4对影响spark中数据分区大小的示例输出。图片由作者提供。
- en: However, stating the (hopefully) obvious, be careful with LLMs. Anything you
    send to a closed source model can become training data for the parent organization
    — make sure you don’t send anything sensitive. Also, please validate that the
    output from GPT is legit.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，显而易见的是，使用LLMs时要小心。你发送到封闭源模型的任何内容都可能成为母公司组织的训练数据 — 确保你不发送任何敏感信息。此外，请验证GPT的输出是否可靠。
- en: When used properly, LLMs are game-changing for spark learning and development.
    It’s worth $20/month.
  id: totrans-99
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 如果使用得当，LLMs在spark学习和开发中具有颠覆性作用。每月$20是值得的。
