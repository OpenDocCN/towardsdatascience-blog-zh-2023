- en: 1.5 Years of Spark Knowledge in 8 Tips
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/1-5-years-of-spark-knowledge-in-8-tips-f003c4743083?source=collection_archive---------0-----------------------#2023-12-24](https://towardsdatascience.com/1-5-years-of-spark-knowledge-in-8-tips-f003c4743083?source=collection_archive---------0-----------------------#2023-12-24)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: My learnings from Databricks customer engagements
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://michaelberk.medium.com/?source=post_page-----f003c4743083--------------------------------)[![Michael
    Berk](../Images/c79c07ed3973cad1305a1d970aaea0b5.png)](https://michaelberk.medium.com/?source=post_page-----f003c4743083--------------------------------)[](https://towardsdatascience.com/?source=post_page-----f003c4743083--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----f003c4743083--------------------------------)
    [Michael Berk](https://michaelberk.medium.com/?source=post_page-----f003c4743083--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fe101cd051ea3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F1-5-years-of-spark-knowledge-in-8-tips-f003c4743083&user=Michael+Berk&userId=e101cd051ea3&source=post_page-e101cd051ea3----f003c4743083---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----f003c4743083--------------------------------)
    ·8 min read·Dec 24, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Ff003c4743083&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F1-5-years-of-spark-knowledge-in-8-tips-f003c4743083&user=Michael+Berk&userId=e101cd051ea3&source=-----f003c4743083---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff003c4743083&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F1-5-years-of-spark-knowledge-in-8-tips-f003c4743083&source=-----f003c4743083---------------------bookmark_footer-----------)![](../Images/a363f6cea7c86a50b902af7da7137544.png)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 1: a technical diagram of how to write apache spark. Image by author.'
  prefs: []
  type: TYPE_NORMAL
- en: At Databricks, I help large retail organizations deploy and scale data and machine
    learning pipelines. Here are the 8 most important [spark](https://spark.apache.org/)
    tips/tricks I’ve learned in the field.
  prefs: []
  type: TYPE_NORMAL
- en: Throughout this post, we assume a general working knowledge of spark and it’s
    structure, but this post should be accessible to all levels.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s dive in!
  prefs: []
  type: TYPE_NORMAL
- en: 0 — Quick Review
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Quickly, let’s review what spark does…
  prefs: []
  type: TYPE_NORMAL
- en: Spark is a big data processing engine. It takes python/java/scala/R/SQL and
    converts that code into a highly optimized set of transformations.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/06c30062d226c82299f50052ff6b6abf.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: spark driver and worker configuration. Image by author.'
  prefs: []
  type: TYPE_NORMAL
- en: At it’s lowest level, spark creates tasks, which are **parallelizable transformations
    on data partitions**. These tasks are then distributed across from a driver node
    to worker nodes, which are responsible for leveraging their CPU cores to complete
    the transformations. By distributing tasks to potentially many workers, spark
    allows us to horizontally scale and thereby support complex data pipelines that
    would be impossible on a single machine.
  prefs: []
  type: TYPE_NORMAL
- en: Ok, hopefully not all of that was new information. Either way, in the following
    sections we’ll slow down a bit. These tips should help both novices and intermediates
    at spark.
  prefs: []
  type: TYPE_NORMAL
- en: 1 — Spark is a Grocery Store
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Spark is complex. To help both you and potentially others understand its structure,
    let’s leverage an impressively good analogy borrowed from [queueing theory](https://en.wikipedia.org/wiki/Queueing_theory):
    **spark is a grocery store.**'
  prefs: []
  type: TYPE_NORMAL
- en: When thinking about the distributed computing component of spark, there are
    three main components….
  prefs: []
  type: TYPE_NORMAL
- en: '**Data partitions:** subsets of rows of our data. In our grocery store, they
    are **groceries.**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Spark tasks:** low-level transformations performed on a data partition. In
    our grocery store, they are **customers**.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cores:** the part of your processor(s) that do work in parallel. In our grocery
    store, they are **cashiers**.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: That’s it!
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s leverage these concepts to talk through some fundamentals of spark.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7c684a57bb5357a02622754e4786b018.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: illustration of the cashier analog, specifically for data skew. Image
    by author.'
  prefs: []
  type: TYPE_NORMAL
- en: As show in figure 3, our cashiers (cores) can only process one customer (task)
    at a time. Furthermore, some customers have a lot of groceries (partition row
    count), as shown by the first customer at cashier 2\. From these simple observations…
  prefs: []
  type: TYPE_NORMAL
- en: The more cashiers (cores), the more customers (tasks) you can process in parallel.
    This is [horizontal/vertical scaling](https://www.geeksforgeeks.org/horizontal-and-vertical-scaling-in-databases/#).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you don’t have enough customers (tasks) to saturate your cashiers (cores),
    you’ll be paying for the cashier to sit there. This relates to [autoscaling](https://www.databricks.com/blog/2018/05/02/introducing-databricks-optimized-auto-scaling.html),
    cluster sizing, and partition sizing.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If customers (tasks) have very different amounts of groceries (partition row
    counts), you’ll see uneven utilization of your cashiers. This is [**data skew**](/data-skew-in-pyspark-783d529a9dd7).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The better your cashiers (cores), the faster they can process a single customer
    (task). This relates to upgrading your processor.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: etc.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Given the analogy comes from queueing theory, a field directly related to distributed
    computing, it’s quite powerful!
  prefs: []
  type: TYPE_NORMAL
- en: Use this analogy to debug, communicate, and develop spark.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 2— Collect Data to Memory Once
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The most common mistake for spark novices is misunderstanding lazy evaluation.
  prefs: []
  type: TYPE_NORMAL
- en: '[Lazy evaluation](https://medium.com/@think-data/mastering-lazy-evaluation-a-must-know-for-pyspark-pros-ac855202495e)
    means that no data transformations will be performed until you invoke a collection
    to memory. Examples of methods that invoke a collection include but are not limited
    to…'
  prefs: []
  type: TYPE_NORMAL
- en: '[.collect()](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.collect.html):
    bring the DataFrame into memory as a python list.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[.show()](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.show.html):
    print the first `n` rows of your DataFrame.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[.count()](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.count.html):
    get the number of rows of your DataFrame.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[.first()](https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.sql.functions.first.html):
    get the first row of your DataFrame.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The single most common incorrect collection method is leveraging `.count()`
    throughout a program. Every time you invoke a collection, all upstream transformations
    will be recomputed from scratch, so if you have 5 invocations of `.count()`, your
    program will asymptotically run 5x as long.
  prefs: []
  type: TYPE_NORMAL
- en: Spark is lazily evaluated! Pipelines should have a single flow from source(s)
    to target(s).
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 3— Meet the SLA then Stop
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A surprisingly common issue that’s come up when working with large organizations
    is they lose sight of the big picture and thereby optimize pipelines in an inefficient
    manner.
  prefs: []
  type: TYPE_NORMAL
- en: Here’s how pipelines should be optimized for the majority of use cases…
  prefs: []
  type: TYPE_NORMAL
- en: '**Ask if we need to do the project.** Put simply, think about what you’re actually
    getting from optimizing a pipeline. If you expect to improve runtime by 20% and
    the pipeline costs $100 to run, should you invest your extremely expensive data
    engineer’s salary to save $20 per run? Maybe. Maybe not.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Look for low hanging fruit in the code.** After agreeing to do the project,
    check if the code has obvious flaws. Examples are misuse of lazy evaluation, unnecessary
    transformations, and incorrect ordering of transformations.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Get the job running under the SLA by leveraging compute.** After checking
    that the code is relatively efficient, just throw compute at the problem so you
    can 1) meet the SLA and, 2) gather statistics from the spark UI.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Stop.** If you’re properly saturating your compute and cost isn’t egregious,
    do some last minute compute improvements then stop. Your time is valuable. Don’t
    waste it saving dollars when you could be creating thousands of dollars elsewhere.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Deep dive.** Finally, if you really need to deep dive because cost is unacceptable,
    then roll up your sleeves and optimize data, code, and compute.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The beauty of this framework is that 1–4 only require cursory knowledge of spark
    and are very quick to execute; sometimes you can collect information on steps
    1–4 during a 30 minute call. The framework also ensures that we’ll stop as soon
    as we are *good enough*. Finally, if step 5 is needed, we can delegate that to
    those on the team who are strongest at spark.
  prefs: []
  type: TYPE_NORMAL
- en: By finding all the ways to avoid over-optimizing a pipeline, you’re saving precious
    developer hours.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 4 — Disk Spill
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Disk spill is the single most common reason that spark jobs run slow.
  prefs: []
  type: TYPE_NORMAL
- en: It’s a very simple concept. Spark is designed to leverage in-memory processing.
    If you don’t have enough memory, spark will try to write the extra data to disk
    to prevent your process from crashing. This is called disk spill.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8965e74adda45122e1a8357d1745c172.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: screen shot of the spark UI highlighting disk spill. Image by author.'
  prefs: []
  type: TYPE_NORMAL
- en: Writing to and reading from disk is slow, so it should be avoided. If you want
    to learn how to identify and mitigate spill, follow [this tutorial](https://selectfrom.dev/spark-performance-tuning-spill-7318363e18cb).
    However, some very common and simple methods to mitigate spill are…
  prefs: []
  type: TYPE_NORMAL
- en: Process less data per task, which can be achieved by changing the partition
    count via [spark.shuffle.partitions](https://spark.apache.org/docs/latest/sql-performance-tuning.html#other-configuration-options)
    or [repartition](https://spark.apache.org/docs/3.1.3/api/python/reference/api/pyspark.sql.DataFrame.repartition.html).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Increase the RAM to core ratio in your compute.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If you want your job to run optimally, prevent spill.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 5— Use SQL Syntax
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Whether you’re using scala, java, python, SQL, or R, spark will always leverage
    the same transformations under the hood. So, use the the right language for your
    task.
  prefs: []
  type: TYPE_NORMAL
- en: 'SQL is the least verbose “language” out of all supported spark languages for
    many operations! More tangibly:'
  prefs: []
  type: TYPE_NORMAL
- en: If you’re adding or modifying a column, use [selectExpr](https://spark.apache.org/docs/3.1.3/api/python/reference/api/pyspark.sql.DataFrame.selectExpr.html)
    or [expr](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.expr.html),
    especially paired with Python’s [f-strings](https://realpython.com/python-f-strings/).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you need complex SQL, create temp views then use [spark.sql()](https://spark.apache.org/docs/latest/api/sql/index.html).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Here are two quick examples…
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Use SQL.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 6— Glob Filters
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Do you need to read a bunch of data files stored in a complex directory? If
    so, use spark’s extremely powerful [read options](https://spark.apache.org/docs/latest/sql-data-sources-generic-options.html#generic-file-source-options).
  prefs: []
  type: TYPE_NORMAL
- en: The first time I encountered this problem, I rewrote [os.walk](https://www.tutorialspoint.com/python/os_walk.htm)
    to work with my cloud provider where data was stored. I very proudly showed this
    method to my project partner who simply said, “let me share my screen,” and proceeded
    to introduce me to glob filters.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: When I applied the glob filter shown above instead of my custom os.walk, the
    ingestion operation was over 10x faster.
  prefs: []
  type: TYPE_NORMAL
- en: Spark has powerful parameters. Check if your desired functionality exists before
    building bespoke implementations.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 7 — Use Reduce with DataFrame.Union
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Loops are almost always detrimental to spark performance. Here’s why…
  prefs: []
  type: TYPE_NORMAL
- en: Spark has two core phases — planning and execution. In the planning phase, spark
    creates a directed acyclical graph (DAG) which indicates how your specified transformations
    will be carried out. The planning phase is relatively expensive and can sometimes
    take several seconds, so you want to invoke it as infrequently as possible.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s discuss a use case where you must iterate through many DataFrames, perform
    expensive transformations, then append them to a table.
  prefs: []
  type: TYPE_NORMAL
- en: First, there is native support for nearly all iterative use cases, specifically
    [pandas UDFs](https://stackoverflow.com/questions/58170261/how-to-use-pandas-udf-in-class),
    window functions, and joins. But, if you truly do need a loop, here’s how you
    invoke a single planning phase and thereby get all transformations in a single
    DAG.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: The first solution uses a for loop to iterate over paths, do fancy transformations,
    then append to our delta table of interest. In the second, we store a list of
    lazily evaluated DataFrames, apply transformations over them, then reduce them
    via a union, performing a single spark plan and write.
  prefs: []
  type: TYPE_NORMAL
- en: We can actually see the difference in architecture on the backend via the Spark
    UI…
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d94967682a2201899b5b7897edb06847.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: spark DAG for for loop vs. functools.reduce. Image by author.'
  prefs: []
  type: TYPE_NORMAL
- en: In figure 5, the DAG on the left corresponding to the for loop will have 10
    stages. However, the DAG on the right corresponding to `functools.reduce` will
    have a single stage and thereby can be processed more easily in parallel.
  prefs: []
  type: TYPE_NORMAL
- en: For a simple use case of reading 400 unique delta tables then appending to a
    delta table, this method was 6x faster than a for loop.
  prefs: []
  type: TYPE_NORMAL
- en: Get creative to create a single spark DAG.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 8 — Use ChatGPT
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This is not about hype.
  prefs: []
  type: TYPE_NORMAL
- en: Spark is a well-establish and thereby well-documented piece of software. LLMs,
    specifically GPT-4, are really good at distilling complex information into digestible
    and concise explanations. Since the release of GPT-4, I have not done a complex
    spark project where I didn’t heavily rely on GPT-4.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ac29f4bd334512d8294617a5a8e6fe9e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: example of GPT-4 output on impacting data partition size in spark.
    Image by author.'
  prefs: []
  type: TYPE_NORMAL
- en: However, stating the (hopefully) obvious, be careful with LLMs. Anything you
    send to a closed source model can become training data for the parent organization
    — make sure you don’t send anything sensitive. Also, please validate that the
    output from GPT is legit.
  prefs: []
  type: TYPE_NORMAL
- en: When used properly, LLMs are game-changing for spark learning and development.
    It’s worth $20/month.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
