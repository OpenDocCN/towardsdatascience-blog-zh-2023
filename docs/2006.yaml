- en: 'PatchTST: A Breakthrough in Time Series Forecasting'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/patchtst-a-breakthrough-in-time-series-forecasting-e02d48869ccc?source=collection_archive---------0-----------------------#2023-06-20](https://towardsdatascience.com/patchtst-a-breakthrough-in-time-series-forecasting-e02d48869ccc?source=collection_archive---------0-----------------------#2023-06-20)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: From theory to practice, understand the PatchTST algorithm and apply it in Python
    alongside N-BEATS and N-HiTS
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@marcopeixeiro?source=post_page-----e02d48869ccc--------------------------------)[![Marco
    Peixeiro](../Images/7cf0a81d87281d35ff47f51e3026a3e9.png)](https://medium.com/@marcopeixeiro?source=post_page-----e02d48869ccc--------------------------------)[](https://towardsdatascience.com/?source=post_page-----e02d48869ccc--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----e02d48869ccc--------------------------------)
    [Marco Peixeiro](https://medium.com/@marcopeixeiro?source=post_page-----e02d48869ccc--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F741c1c8fcfbd&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpatchtst-a-breakthrough-in-time-series-forecasting-e02d48869ccc&user=Marco+Peixeiro&userId=741c1c8fcfbd&source=post_page-741c1c8fcfbd----e02d48869ccc---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----e02d48869ccc--------------------------------)
    ·10 min read·Jun 20, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fe02d48869ccc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpatchtst-a-breakthrough-in-time-series-forecasting-e02d48869ccc&user=Marco+Peixeiro&userId=741c1c8fcfbd&source=-----e02d48869ccc---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fe02d48869ccc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpatchtst-a-breakthrough-in-time-series-forecasting-e02d48869ccc&source=-----e02d48869ccc---------------------bookmark_footer-----------)![](../Images/3a14de30ec6761d39da8521d597c09d3.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Photo by [Ray Hennessy](https://unsplash.com/@rayhennessy?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: Transformer-based models have been successfully applied in many fields like
    natural language processing (think BERT or GPT models) and computer vision to
    name a few.
  prefs: []
  type: TYPE_NORMAL
- en: However, when it comes to time series, state-of-the-art results have mostly
    been achieved by MLP models (multilayer perceptron) such as [N-BEATS](https://medium.com/towards-data-science/the-easiest-way-to-forecast-time-series-using-n-beats-d778fcc2ba60)
    and [N-HiTS](/all-about-n-hits-the-latest-breakthrough-in-time-series-forecasting-a8ddcb27b0d5).
    A recent paper even shows that simple linear models outperform complex transformer-based
    forecasting models on many benchmark datasets (see [Zheng et al., 2022](https://arxiv.org/pdf/2205.13504.pdf)).
  prefs: []
  type: TYPE_NORMAL
- en: 'Still, a new transformer-based model has been proposed that achieves state-of-the-art
    results for long-term forecasting tasks: **PatchTST**.'
  prefs: []
  type: TYPE_NORMAL
- en: 'PatchTST stands for patch time series transformer, and it was first proposed
    in March 2023 by Nie, Nguyen et al in their paper: [A Time Series is Worth 64
    Words: Long-Term Forecasting with Transformers](https://arxiv.org/pdf/2211.14730.pdf).
    Their proposed method achieved state-of-the-art results when compared to other
    transformer-based models.'
  prefs: []
  type: TYPE_NORMAL
- en: In this article, we first explore the inner workings of PatchTST, using intuition
    and no equations. Then, we apply the model in a forecasting project and compare…
  prefs: []
  type: TYPE_NORMAL
