["```py\nLLM_model_evals != LLM_System_evals\n```", "```py\nYou are comparing a reference text to a question and trying to determine if the reference text contains information relevant to answering the question. Here is the data:\n    [BEGIN DATA]\n    ************\n    [Question]: {query}\n    ************\n    [Reference text]: {reference}\n    [END DATA]\nCompare the Question above to the Reference text. You must determine whether the Reference text\ncontains information that can answer the Question. Please focus on whether the very specific\nquestion can be answered by the information in the Reference text.\nYour response must be single word, either \"relevant\" or \"irrelevant\",\nand should not contain any text or characters aside from that word.\n\"irrelevant\" means that the reference text does not contain an answer to the Question.\n\"relevant\" means the reference text contains an answer to the Question.\n```", "```py\nfrom phoenix.experimental.evals import (\n   RAG_RELEVANCY_PROMPT_TEMPLATE_STR,\n   RAG_RELEVANCY_PROMPT_RAILS_MAP,\n   OpenAIModel,\n   download_benchmark_dataset,\n   llm_eval_binary,\n)\nfrom sklearn.metrics import precision_recall_fscore_support\n```", "```py\n# Download a \"golden dataset\" built into Phoenix\nbenchmark_dataset = download_benchmark_dataset(\n   task=\"binary-relevance-classification\", dataset_name=\"wiki_qa-train\"\n)\n# For the sake of speed, we'll just sample 100 examples in a repeatable way\nbenchmark_dataset = benchmark_dataset.sample(100, random_state=2023)\nbenchmark_dataset = benchmark_dataset.rename(\n   columns={\n       \"query_text\": \"query\",\n       \"document_text\": \"reference\",\n   },\n)\n# Match the label between our dataset and what the eval will generate\ny_true = benchmark_dataset[\"relevant\"].map({True: \"relevant\", False: \"irrelevant\"})\n```", "```py\n# Any general purpose LLM should work here, but it is best practice to keep the temperature at 0\nmodel = OpenAIModel(\n   model_name=\"gpt-4\",\n   temperature=0.0,\n)\n# Rails will define our output classes\nrails = list(RAG_RELEVANCY_PROMPT_RAILS_MAP.values())\n\nbenchmark_dataset[\"eval_relevance\"] = \\\n   llm_eval_binary(benchmark_dataset,\n                   model,\n                   RAG_RELEVANCY_PROMPT_TEMPLATE_STR,\n                   rails)\ny_pred = benchmark_dataset[\"eval_relevance\"]\n\n# Calculate evaluation metrics\nprecision, recall, f1, support = precision_recall_fscore_support(y_true, y_pred)\n```"]