["```py\nimport jax\n\n# Numpy API with hardware acceleration and automatic differentiation\nfrom jax import numpy as jnp\n\n# Low level operators\nfrom jax import lax\n\n# API for working with pseudorandom number generators\nfrom jax import random\n```", "```py\n# Random seed to make our experiment replicable \nSEED = 42\n\n# Number of visitors we want to simulate\nNUM_VISITS = 10000\n\n# Expected click rates for the five variants with the\nCLICK_RATES = [0.042, 0.03, 0.035, 0.038, 0.045]\n```", "```py\ndef visit(state, timestep, click_rates, policy_fn, update_fn):\n    \"\"\"\n    Simulates a user visit.\n    \"\"\"\n\n    # Unpacking the environment state into\n    # the agent's parameters and the random number generator\n    params, rng = state\n\n    # Splitting the random number generator\n    next_rng, policy_rng, user_rng = random.split(rng, num=3)\n\n    # Selecting the variant to show the user, based on\n    # the given policy, the agent's parameters, and the current timestep\n    variant = policy_fn(params, timestep, policy_rng)\n\n    # Randomly simulating the user click, based on\n    # the variant's click rate\n    clicked = random.uniform(user_rng) < click_rates[variant]\n\n    # Calculating the agent's updated parameters, based on\n    # the current parameters, the selected variant,\n    # and whether or not the user clicked\n    next_params = update_fn(params, variant, clicked)\n\n    # Returning the updated experiment state (params and rng) and\n    # whether or not the user clicked\n    return (next_params, next_rng), clicked\n```", "```py\nstate -> timestep -> (state, Bool)\n```", "```py\n# Initialising the state\ns0 = ...\n\n# Simulating the time steps 0, 1, and 2\ns1, r0 = visit(s0, 0)\ns2, r1 = visit(s1, 1)\ns3, r2 = visit(s2, 2)\n```", "```py\ndef action_value_init(num_variants):\n    \"\"\"\n    Returns the initial action values\n    \"\"\"\n\n    return {\n        'n': jnp.ones(num_variants, dtype=jnp.int32),\n        'q': jnp.ones(num_variants, dtype=jnp.float32)\n    }\n\ndef action_value_update(params, variant, clicked):\n    \"\"\"\n    Calculates the updated action values\n    \"\"\"\n\n    # Reading n and q parameters of the selected variant\n    n, q = params['n'][variant], params['q'][variant]\n\n    # Converting the boolean clicked variable to a float value\n    r = clicked.astype(jnp.float32)\n\n    return {\n        # Incrementing the counter of the taken action by one\n        'n': params['n'].at[variant].add(1),\n\n        # Incrementally updating the action-value estimate\n        'q': params['q'].at[variant].add((r - q) / n)\n    }\n```", "```py\ndef epsilon_greedy_policy(params, timestep, rng, epsilon):\n    \"\"\"\n    Randomly selects either the variant with highest action-value,\n    or an arbitrary variant.\n    \"\"\"\n\n    # Selecting a random variant\n    def explore(q, rng):\n        return random.choice(rng, jnp.arange(len(q)))\n\n    # Selecting the variant with the highest action-value estimate\n    def exploit(q, rng):\n        return jnp.argmax(q)\n\n    # Splitting the random number generator \n    uniform_rng, choice_rng = random.split(rng)\n\n    # Deciding randomly whether to explore or to exploit\n    return lax.cond(\n        random.uniform(uniform_rng) < epsilon,\n        explore,\n        exploit,\n        params['q'],\n        choice_rng\n    )\n```", "```py\ndef boltzmann_policy(params, timestep, rng, tau):\n    \"\"\"\n    Randomly selects a variant proportional to the current action-values\n    \"\"\"\n\n    return random.choice(\n        rng,\n        jnp.arange(len(params['q'])),\n        # Turning the action-value estimates into a probability distribution\n        # by applying the softmax function controlled by tau\n        p=jax.nn.softmax(params['q'] / tau)\n    )\n```", "```py\ndef upper_confidence_bound_policy(params, timestep, rng, confidence):\n    \"\"\"\n    Selects the variant with highest action-value plus upper confidence bound\n    \"\"\"\n\n    # Read n and q parameters\n    n, q = params['n'], params['q']\n\n    # Calculating each variant's upper confidence bound\n    # and selecting the variant with the highest value\n    return jnp.argmax(q + confidence * jnp.sqrt(jnp.log(timestep) / n))\n```", "```py\ndef beta_init(num_variants):\n    \"\"\"\n    Returns the initial hyperparameters of the beta distribution\n    \"\"\"\n\n    return {\n        'a': jnp.ones(num_variants, dtype=jnp.int32),\n        'b': jnp.ones(num_variants, dtype=jnp.int32)\n    }\n\ndef beta_update(params, variant, clicked):\n    \"\"\"\n    Calculates the updated hyperparameters of the beta distribution\n    \"\"\"\n\n    # Incrementing alpha by one\n    def increment_alpha(a, b):\n        return {'a': a.at[variant].add(1), 'b': b}\n\n    # Incrementing beta by one\n    def increment_beta(a, b):\n        return {'b': b.at[variant].add(1), 'a': a}\n\n    # Incrementing either alpha or beta\n    # depending on whether or not the user clicked\n    return lax.cond(\n        clicked,\n        increment_alpha,\n        increment_beta,\n        params['a'],\n        params['b']\n    )\n```", "```py\ndef thompson_policy(params, timestep, rng):\n    \"\"\"\n    Randomly sampling click rates for all variants\n    and selecting the variant with the highest sample\n    \"\"\"\n\n    return jnp.argmax(random.beta(rng, params['a'], params['b']))\n```", "```py\nfrom functools import partial\nfrom matplotlib import pyplot as plt\n```", "```py\ndef evaluate(policy_fn, init_fn, update_fn):\n    \"\"\"\n    Simulating the environment for NUM_VISITS users\n    while accumulating the click history\n    \"\"\"\n\n    return lax.scan(\n        # Compiling the visit function using just-in-time (JIT) compilation\n        # for better performance\n        jax.jit(\n            # Partially applying the visit function by fixing\n            # the click_rates, policy_fn, and update_fn parameters \n            partial(\n                visit,\n                click_rates=jnp.array(CLICK_RATES),\n                policy_fn=jax.jit(policy_fn),\n                update_fn=jax.jit(update_fn)\n            )\n        ),\n\n        # Initialising the experiment state using\n        # init_fn and a new PRNG key\n        (init_fn(len(CLICK_RATES)), random.PRNGKey(SEED)),\n\n        # Setting the number of steps in our environment\n        jnp.arange(1, NUM_VISITS + 1)\n    )\n```", "```py\ndef regret(history):\n    \"\"\"\n    Calculates the regret for every action in the environment history\n    \"\"\"\n\n    # Calculating regret with regard to picking the optimal (0.045) variant\n    def fn(acc, reward):\n        n, v = acc[0] + 1, acc[1] + reward\n        return (n, v), 0.045 - (v / n)\n\n    # Calculating regret values over entire history\n    _, result = lax.scan(\n        jax.jit(fn),\n        (jnp.array(0), jnp.array(0)),\n        history\n    )\n\n    return result\n```", "```py\n# Epsilon greedy policy\n(epsilon_greedy_params, _), epsilon_greedy_history = evaluate(\n    policy_fn=partial(epsilon_greedy_policy, epsilon=0.1),\n    init_fn=action_value_init,\n    update_fn=action_value_update\n)\n\n# Boltzmann policy\n(boltzmann_params, _), boltzmann_history = evaluate(\n    policy_fn=partial(boltzmann_policy, tau=1.0),\n    init_fn=action_value_init,\n    update_fn=action_value_update\n)\n\n# Upper confidence bound policy\n(ucb_params, _), ucb_history = evaluate(\n    policy_fn=partial(upper_confidence_bound_policy, confidence=2),\n    init_fn=action_value_init,\n    update_fn=action_value_update\n)\n\n# Thompson sampling policy\n(ts_params, _), ts_history = evaluate(\n    policy_fn=thompson_policy,\n    init_fn=beta_init,\n    update_fn=beta_update\n)\n\n# Visualisation\nfig, ax = plt.subplots(figsize=(16, 8))\n\nx = jnp.arange(1, NUM_VISITS + 1)\n\nax.set_xlabel('Number of visits')\nax.set_ylabel('Regret')\n\nax.plot(x, jnp.repeat(jnp.mean(jnp.array(CLICK_RATES)), NUM_VISITS), label='A/B Testing')\nax.plot(x, regret(epsilon_greedy_history), label='Espilon Greedy Policy')\nax.plot(x, regret(boltzmann_history), label='Boltzmann Policy')\nax.plot(x, regret(ucb_history), label='UCB Policy')\nax.plot(x, regret(ts_history), label='TS Policy')\n\nplt.legend()\nplt.show()\n```", "```py\n|      Name      | Original |   V1   |   V2   |   V3   |   V4   |  RMSE  |\n|----------------|----------|--------|--------|--------|--------|--------|\n| Ground truth   |   0.0420 | 0.0300 | 0.0350 | 0.0380 | 0.0450 | 0.0000 |\n| Epsilon greedy |   0.0420 | 0.0367 | 0.0378 | 0.0256 | 0.0375 | 0.0072 |\n| Boltzmann      |   0.0397 | 0.0291 | 0.0383 | 0.0346 | 0.0449 | 0.0024 |\n| UCB            |   0.0399 | 0.0259 | 0.0247 | 0.0390 | 0.0518 | 0.0060 |\n| TS             |   0.0390 | 0.0425 | 0.0370 | 0.0393 | 0.0441 | 0.0059 |\n```", "```py\n|      Name      | Original |  V1  |  V2  |  V3  |  V4  |\n|----------------|----------|------|------|------|------|\n| Epsilon greedy |     8334 |  409 |  291 |  352 |  614 |\n| Boltzmann      |     1991 | 1998 | 2012 | 2024 | 1984 |\n| UCB            |     2079 | 1701 | 1661 | 2051 | 2508 |\n| TS             |     1901 |  963 | 1324 |  735 | 5077 |\n```"]