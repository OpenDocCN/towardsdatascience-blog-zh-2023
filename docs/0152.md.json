["```py\nimport sympy as sm\nimport numpy as np\n\n# Define symbols & objective function\nx, y = sm.symbols('x y')\nGamma = [x,y]\nobjective = 100*(y-x**2)**2 + (1-x)**2\n\ndef get_gradient(\n    function: sm.core.expr.Expr,\n    symbols: list[sm.core.symbol.Symbol],\n) -> np.ndarray:\n    \"\"\"\n    Calculate the gradient of a function.\n\n    Args:\n        function (sm.core.expr.Expr): The function to calculate the gradient of.\n        symbols (list[sm.core.symbol.Symbol]): The symbols representing the variables in the function.\n\n    Returns:\n        numpy.ndarray: The gradient of the function.\n    \"\"\"\n    d1 = {}\n    gradient = np.array([])\n\n    for i in symbols:\n        d1[i] = sm.diff(function, i, 1)\n        gradient = np.append(gradient, d1[i])\n\n    return gradient\n\ndef get_hessian(\n    function: sm.core.expr.Expr,\n    symbols: list[sm.core.symbol.Symbol],\n    x0: dict[sm.core.symbol.Symbol, float],\n) -> np.ndarray:\n    \"\"\"\n    Calculate the Hessian matrix of a function.\n\n    Args:\n    function (sm.core.expr.Expr): The function for which the Hessian matrix is calculated.\n    symbols (list[sm.core.symbol.Symbol]): The list of symbols used in the function.\n\n    Returns:\n    numpy.ndarray: The Hessian matrix of the function.\n    \"\"\"\n    d2 = {}\n    hessian = np.array([])\n\n    for i in symbols:\n        for j in symbols:\n            d2[f\"{i}{j}\"] = sm.diff(function, i, j)\n            hessian = np.append(hessian, d2[f\"{i}{j}\"])\n\n    hessian = np.array(np.array_split(hessian, len(symbols)))\n\n    return hessian\n```", "```py\nimport sympy as sm\nimport numpy as np\n\ndef get_gradient(\n    function: sm.core.expr.Expr,\n    symbols: list[sm.core.symbol.Symbol],\n    x0: dict[sm.core.symbol.Symbol, float], # Add x0 as argument\n) -> np.ndarray:\n    \"\"\"\n    Calculate the gradient of a function at a given point.\n\n    Args:\n        function (sm.core.expr.Expr): The function to calculate the gradient of.\n        symbols (list[sm.core.symbol.Symbol]): The symbols representing the variables in the function.\n        x0 (dict[sm.core.symbol.Symbol, float]): The point at which to calculate the gradient.\n\n    Returns:\n        numpy.ndarray: The gradient of the function at the given point.\n    \"\"\"\n    d1 = {}\n    gradient = np.array([])\n\n    for i in symbols:\n        d1[i] = sm.diff(function, i, 1).evalf(subs=x0) # add evalf method\n        gradient = np.append(gradient, d1[i])\n\n    return gradient.astype(np.float64) # Change data type to float\n\ndef get_hessian(\n    function: sm.core.expr.Expr,\n    symbols: list[sm.core.symbol.Symbol],\n    x0: dict[sm.core.symbol.Symbol, float],\n) -> np.ndarray:\n    \"\"\"\n    Calculate the Hessian matrix of a function at a given point.\n\n    Args:\n    function (sm.core.expr.Expr): The function for which the Hessian matrix is calculated.\n    symbols (list[sm.core.symbol.Symbol]): The list of symbols used in the function.\n    x0 (dict[sm.core.symbol.Symbol, float]): The point at which the Hessian matrix is evaluated.\n\n    Returns:\n    numpy.ndarray: The Hessian matrix of the function at the given point.\n    \"\"\"\n    d2 = {}\n    hessian = np.array([])\n\n    for i in symbols:\n        for j in symbols:\n            d2[f\"{i}{j}\"] = sm.diff(function, i, j).evalf(subs=x0)\n            hessian = np.append(hessian, d2[f\"{i}{j}\"])\n\n    hessian = np.array(np.array_split(hessian, len(symbols)))\n\n    return hessian.astype(np.float64)\n```", "```py\nimport sympy as sm\nimport numpy as np\n\ndef newton_method(\n    function: sm.core.expr.Expr,\n    symbols: list[sm.core.symbol.Symbol],\n    x0: dict[sm.core.symbol.Symbol, float],\n    iterations: int = 100,\n) -> dict[sm.core.symbol.Symbol, float] or None:\n    \"\"\"\n    Perform Newton's method to find the solution to the optimization problem.\n\n    Args:\n        function (sm.core.expr.Expr): The objective function to be optimized.\n        symbols (list[sm.core.symbol.Symbol]): The symbols used in the objective function.\n        x0 (dict[sm.core.symbol.Symbol, float]): The initial values for the symbols.\n        iterations (int, optional): The maximum number of iterations. Defaults to 100.\n\n    Returns:\n        dict[sm.core.symbol.Symbol, float] or None: The solution to the optimization problem, or None if no solution is found.\n    \"\"\"\n\n    x_star = {}\n    x_star[0] = np.array(list(x0.values()))\n\n    print(f\"Starting Values: {x_star[0]}\")\n\n    for i in range(iterations):\n\n        gradient = get_gradient(function, symbols, dict(zip(x0.keys(), x_star[i])))\n        hessian = get_hessian(function, symbols, dict(zip(x0.keys(), x_star[i])))\n\n        x_star[i + 1] = x_star[i].T - np.linalg.inv(hessian) @ gradient.T\n\n        if np.linalg.norm(x_star[i + 1] - x_star[i]) < 10e-5:\n            solution = dict(zip(x0.keys(), x_star[i + 1]))\n            print(f\"\\nConvergence Achieved ({i+1} iterations): Solution = {solution}\")\n            break\n        else:\n            solution = None\n\n        print(f\"Step {i+1}: {x_star[i+1]}\")\n\n    return solution\n```", "```py\nimport sympy as sm\nimport numpy as np\n\ndef gradient_descent(\n    function: sm.core.expr.Expr,\n    symbols: list[sm.core.symbol.Symbol],\n    x0: dict[sm.core.symbol.Symbol, float],\n    learning_rate: float = 0.1,\n    iterations: int = 100,\n) -> dict[sm.core.symbol.Symbol, float] or None:\n    \"\"\"\n    Performs gradient descent optimization to find the minimum of a given function.\n\n    Args:\n        function (sm.core.expr.Expr): The function to be optimized.\n        symbols (list[sm.core.symbol.Symbol]): The symbols used in the function.\n        x0 (dict[sm.core.symbol.Symbol, float]): The initial values for the symbols.\n        learning_rate (float, optional): The learning rate for the optimization. Defaults to 0.1.\n        iterations (int, optional): The maximum number of iterations. Defaults to 100.\n\n    Returns:\n        dict[sm.core.symbol.Symbol, float] or None: The solution found by the optimization, or None if no solution is found.\n    \"\"\"\n    x_star = {}\n    x_star[0] = np.array(list(x0.values()))\n\n    print(f\"Starting Values: {x_star[0]}\")\n\n    for i in range(iterations):\n\n        gradient = get_gradient(function, symbols, dict(zip(x0.keys(), x_star[i])))\n\n        x_star[i + 1] = x_star[i].T - learning_rate * gradient.T\n\n        if np.linalg.norm(x_star[i + 1] - x_star[i]) < 10e-5:\n            solution = dict(zip(x0.keys(), x_star[i + 1]))\n            print(f\"\\nConvergence Achieved ({i+1} iterations): Solution = {solution}\")\n            break\n        else:\n            solution = None\n\n        print(f\"Step {i+1}: {x_star[i+1]}\")\n\n    return solution\n```"]