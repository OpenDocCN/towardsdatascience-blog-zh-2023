["```py\nclass WordPredictionLSTMModel(nn.Module):\n    def __init__(self, num_embed, embed_dim, pad_idx, lstm_hidden_dim, lstm_num_layers, output_dim, dropout):\n        super().__init__()\n        self.vocab_size = num_embed\n        self.embed = nn.Embedding(num_embed, embed_dim, pad_idx)\n        self.lstm = nn.LSTM(embed_dim, lstm_hidden_dim, lstm_num_layers, batch_first=True, dropout=dropout)\n        self.fc = nn.Sequential(\n            nn.Linear(lstm_hidden_dim, lstm_hidden_dim * 4),\n            nn.LayerNorm(lstm_hidden_dim * 4),\n            nn.LeakyReLU(),\n            nn.Dropout(p=dropout),\n\n            nn.Linear(lstm_hidden_dim * 4, output_dim),\n        )\n    #\n\n    def forward(self, x):\n        x = self.embed(x)\n        x, _ = self.lstm(x)\n        x = self.fc(x)\n        x = x.permute(0, 2, 1)\n        return x\n    #\n#\n```", "```py\n=================================================================\nLayer (type:depth-idx) Param #\n=================================================================\nWordPredictionLSTMModel - \n├─Embedding: 1–1 3,379,200\n├─LSTM: 1–2 4,087,200\n├─Sequential: 1–3 - \n│ └─Linear: 2–1 2,474,328\n│ └─LayerNorm: 2–2 6,288\n│ └─LeakyReLU: 2–3 - \n│ └─Dropout: 2–4 - \n│ └─Linear: 2–5 20,757,000\n=================================================================\nTotal params: 30,704,016\nTrainable params: 30,704,016\nNon-trainable params: 0\n=================================================================\n```", "```py\n[I think] [I've] = 0.00087\n[I think] [over] = 0.00051\n[I think] [ice] = 0.00001\n[I think] [Oct] = 0.00000\n```", "```py\n def get_completion_probability(self, input, completion, tok):\n      self.model.eval()\n      ids = tok.encode(input).ids\n      ids = torch.tensor(ids, device=self.device).unsqueeze(0)\n      completion_ids = torch.tensor(tok.encode(completion).ids, device=self.device).unsqueeze(0)\n      probs = []\n      for i in range(completion_ids.size(1)):\n          y = self.model(ids)\n          y = y[0,:,-1].softmax(dim=0)\n          # prob is the probability of this completion.\n          prob = y[completion_ids[0,i]]\n          probs.append(prob)\n          ids = torch.cat([ids, completion_ids[:,i:i+1]], dim=1)\n      #\n      return torch.tensor(probs)\n  #\n```", "```py\n[That ice-cream looks] [really] = 0.00709\n[That ice-cream looks] [delicious] = 0.00264\n[That ice-cream looks] [absolutely] = 0.00122\n[That ice-cream looks] [real] = 0.00031\n[That ice-cream looks] [fish] = 0.00004\n[That ice-cream looks] [paper] = 0.00001\n[That ice-cream looks] [atrocious] = 0.00000\n\n[Since we're heading] [toward] = 0.01052\n[Since we're heading] [away] = 0.00344\n[Since we're heading] [against] = 0.00035\n[Since we're heading] [both] = 0.00009\n[Since we're heading] [death] = 0.00000\n[Since we're heading] [bubble] = 0.00000\n[Since we're heading] [birth] = 0.00000\n\n[Did I make] [a] = 0.22704\n[Did I make] [the] = 0.06622\n[Did I make] [good] = 0.00190\n[Did I make] [food] = 0.00020\n[Did I make] [color] = 0.00007\n[Did I make] [house] = 0.00006\n[Did I make] [colour] = 0.00002\n[Did I make] [pencil] = 0.00001\n[Did I make] [flower] = 0.00000\n\n[We want a candidate] [with] = 0.03209\n[We want a candidate] [that] = 0.02145\n[We want a candidate] [experience] = 0.00097\n[We want a candidate] [which] = 0.00094\n[We want a candidate] [more] = 0.00010\n[We want a candidate] [less] = 0.00007\n[We want a candidate] [school] = 0.00003\n\n[This is the definitive guide to the] [the] = 0.00089\n[This is the definitive guide to the] [complete] = 0.00047\n[This is the definitive guide to the] [sentence] = 0.00006\n[This is the definitive guide to the] [rapper] = 0.00001\n[This is the definitive guide to the] [illustrated] = 0.00001\n[This is the definitive guide to the] [extravagant] = 0.00000\n[This is the definitive guide to the] [wrapper] = 0.00000\n[This is the definitive guide to the] [miniscule] = 0.00000\n\n[Please can you] [check] = 0.00502\n[Please can you] [confirm] = 0.00488\n[Please can you] [cease] = 0.00002\n[Please can you] [cradle] = 0.00000\n[Please can you] [laptop] = 0.00000\n[Please can you] [envelope] = 0.00000\n[Please can you] [options] = 0.00000\n[Please can you] [cordon] = 0.00000\n[Please can you] [corolla] = 0.00000\n\n[I think] [I've] = 0.00087\n[I think] [over] = 0.00051\n[I think] [ice] = 0.00001\n[I think] [Oct] = 0.00000\n\n[Please] [can] = 0.00428\n[Please] [cab] = 0.00000\n\n[I've scheduled this] [meeting] = 0.00077\n[I've scheduled this] [messing] = 0.00000\n```", "```py\nimport math\n\ndef generate_src_mask(sz, device):\n    return torch.triu(torch.full((sz, sz), True, device=device), diagonal=1)\n#\n\nclass PositionalEmbedding(nn.Module):\n    def __init__(self, sequence_length, embed_dim):\n        super().__init__()\n        self.sqrt_embed_dim = math.sqrt(embed_dim)\n        self.pos_embed = nn.Parameter(torch.empty((1, sequence_length, embed_dim)))\n        nn.init.uniform_(self.pos_embed, -1.0, 1.0)\n    #\n\n    def forward(self, x):\n        return x * self.sqrt_embed_dim + self.pos_embed[:,:x.size(1)]\n    #\n#\n\nclass WordPredictionTransformerModel(nn.Module):\n    def __init__(self, sequence_length, num_embed, embed_dim, pad_idx, num_heads, num_layers, output_dim, dropout, norm_first, activation):\n        super().__init__()\n        self.vocab_size = num_embed\n        self.sequence_length = sequence_length\n        self.embed_dim = embed_dim\n        self.sqrt_embed_dim = math.sqrt(embed_dim)\n        self.embed = nn.Sequential(\n            nn.Embedding(num_embed, embed_dim, pad_idx),\n            PositionalEmbedding(sequence_length, embed_dim),\n            nn.LayerNorm(embed_dim),\n            nn.Dropout(p=0.1),\n        )\n\n        encoder_layer = nn.TransformerEncoderLayer(\n            d_model=embed_dim, nhead=num_heads, dropout=dropout, batch_first=True, norm_first=norm_first, activation=activation,\n        )\n        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n        self.fc = nn.Sequential(\n            nn.Linear(embed_dim, embed_dim * 4),\n            nn.LayerNorm(embed_dim * 4),\n            nn.LeakyReLU(),\n            nn.Dropout(p=dropout),\n\n            nn.Linear(embed_dim * 4, output_dim),\n        )\n    #\n\n    def forward(self, x):\n        src_attention_mask = generate_src_mask(x.size(1), x.device)\n        x = self.embed(x)\n        x = self.encoder(x, is_causal=True, mask=src_attention_mask)\n        x = self.fc(x)\n        x = x.permute(0, 2, 1)\n        return x\n    #\n#\n```"]