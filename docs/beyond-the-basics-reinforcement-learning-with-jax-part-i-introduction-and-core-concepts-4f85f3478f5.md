# 基础知识之外：使用 Jax 的强化学习 — 第一部分：介绍和核心概念

> 原文：[https://towardsdatascience.com/beyond-the-basics-reinforcement-learning-with-jax-part-i-introduction-and-core-concepts-4f85f3478f5?source=collection_archive---------11-----------------------#2023-05-02](https://towardsdatascience.com/beyond-the-basics-reinforcement-learning-with-jax-part-i-introduction-and-core-concepts-4f85f3478f5?source=collection_archive---------11-----------------------#2023-05-02)

## 强化学习基础概述：马尔可夫决策过程、策略和价值函数

[](https://medium.com/@Lando-L?source=post_page-----4f85f3478f5--------------------------------)[![Lando L](../Images/9675667c061daf1d9ac527b49b7a8a9f.png)](https://medium.com/@Lando-L?source=post_page-----4f85f3478f5--------------------------------)[](https://towardsdatascience.com/?source=post_page-----4f85f3478f5--------------------------------)[![Towards Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----4f85f3478f5--------------------------------) [Lando L](https://medium.com/@Lando-L?source=post_page-----4f85f3478f5--------------------------------)

·

[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F395c5e41bd1e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbeyond-the-basics-reinforcement-learning-with-jax-part-i-introduction-and-core-concepts-4f85f3478f5&user=Lando+L&userId=395c5e41bd1e&source=post_page-395c5e41bd1e----4f85f3478f5---------------------post_header-----------) 发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----4f85f3478f5--------------------------------) ·12 分钟阅读·2023年5月2日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F4f85f3478f5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbeyond-the-basics-reinforcement-learning-with-jax-part-i-introduction-and-core-concepts-4f85f3478f5&user=Lando+L&userId=395c5e41bd1e&source=-----4f85f3478f5---------------------clap_footer-----------)

--

[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4f85f3478f5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbeyond-the-basics-reinforcement-learning-with-jax-part-i-introduction-and-core-concepts-4f85f3478f5&source=-----4f85f3478f5---------------------bookmark_footer-----------)![](../Images/37e98d1c74438a090d291747bfb68fff.png)

图表由作者使用 Miro 创建。

在这一系列博客文章中，我很高兴与您分享我对强化学习（RL）范式的热情。我的使命是提供对 RL 的深入探索，将理论知识与实际例子相结合，以解决现实世界中的问题。

# 课程详情

## 这个课程适合谁？

本课程适合任何希望学习RL的人。建议你具有一定的Python编程经验，并且熟悉Jupyter Notebooks的使用。此外，处理本课程第二部分的章节时，拥有基本的神经网络知识会更有帮助。如果你对神经网络没有先前知识，你仍然可以将其视为黑箱函数逼近器来使用；不过，我建议在继续学习后面的章节之前，花时间多了解一下神经网络。

## 为什么我应该关心强化学习？

RL是一个极具激动人心的领域，近年来研究增长呈指数级。尽管取得了一些成功，但弥合学术界与工业界之间的差距并不像其他机器学习领域那样直接。RL常被视为商业应用中过于不切实际，但我在这里打破这种观念，并为你提供RL方法的实用知识，以增强你的问题解决工具箱。

## 我会学到什么？

这是系列博客文章中的第一篇，你将会：

+   学会识别强化学习（RL）的应用场景，并将你的现实世界情境建模以适应RL范式

+   熟悉各种RL算法，了解它们的优缺点

+   理解探索与利用的原则，并了解如何平衡它们以实现最佳性能

+   分析和解读RL实验的结果

这里是我们将要涵盖的技术的预览：

+   多臂老虎机问题

+   动态规划

+   蒙特卡洛方法

+   时序差分学习

+   深度Q网络

+   策略梯度方法

+   Actor-Critic方法

# 介绍

人工智能领域的一些重大突破灵感来源于自然，而RL范式也不例外。这个简单却强大的概念最接近我们人类学习的方式，并且可以被看作是我们对人工通用智能所期待的一个重要组成部分：通过试错学习。这种学习方法教会我们因果关系以及我们的行为如何影响环境。它教会我们我们的行为如何对我们产生害处或益处，并使我们能够制定策略以实现我们的长期目标。

## 什么是RL？

RL范式是一种强大而多用途的机器学习方法，能够让决策者通过与环境的互动来学习。它借鉴了广泛的思想和方法来寻找最大化数值奖励的最佳策略。RL具有与其他科学和工程学科的长期联系，研究已非常成熟。然而，尽管学术上取得了丰富的成功，RL在商业领域的实际应用仍然很少。最著名的RL应用例子是计算机在象棋、围棋等游戏以及Atari和Starcraft等游戏中实现超人水平的表现。然而，近年来我们看到越来越多的行业采用RL方法。

## 现在如何使用它？

尽管RL的商业应用水平较低，但在以下领域有一些令人兴奋的应用：

+   健康：动态治疗方案；自动化诊断；药物发现

+   财务：交易；动态定价；风险管理

+   交通：自适应交通控制；自动驾驶

+   推荐：网页搜索；新闻推荐；产品推荐

+   自然语言处理：文本摘要；问答系统；机器翻译；对话生成

# 有时候，少即是多

了解RL（强化学习）使用案例的好方法是考虑一个挑战示例。假设我们在帮助朋友学习一种乐器。每天早晨，我们的朋友告诉我们他们的动机如何，以及他们在昨天的练习中学到了多少，并询问我们该如何继续。由于我们不清楚的原因，我们的朋友只有有限的学习选择：休息一天、练习一小时或练习三小时。

在观察了我们朋友的进展后，我们注意到了一些有趣的特征：

1.  似乎我们朋友的进展与他们练习的小时数直接相关。

1.  一致的练习使我们的朋友进步更快。

1.  我们的朋友不擅长长时间的练习。每当我们指导他们学习三小时，第二天他们总是感到疲倦，不愿继续。

根据我们的观察，我们使用状态机符号创建了一个图表来建模他们的学习进展。

![](../Images/dbc51d737a1771bfab31fcad7d871349.png)

图表由作者使用Miro创建。

让我们再次讨论基于模型的发现：

1.  我们的朋友有三种不同的情绪状态：中性、激励和失去动力。

1.  在任何一天，他们可以选择练习零、一或三小时，除非他们感到失去动力——在这种情况下，零小时（或不学习）是唯一的选择。

1.  我们朋友的心情是具有预测性的：在中性状态下，练习一个小时会让他们在第二天感到有动力，而练习三个小时则会使他们感到失去动力，不练习则会保持在中性状态。相反，在有动力的状态下，一个小时的练习会维持我们朋友的动力，而三个小时的练习会使他们感到失去动力，不练习则会使他们感到中性。最后，在失去动力的状态下，我们的朋友会完全停止学习，结果是第二天他们会感到中性。

1.  他们的进展受他们的心情和练习时间的影响很大：他们越有动力，投入的时间越多，他们学习和成长的速度就越快。

为什么我们要这样构建我们的发现？因为这有助于我们使用一个称为有限*马尔可夫决策过程*（MDP）的数学框架来建模我们的挑战。这种方法帮助我们更好地理解问题以及如何最好地解决它。

## 马尔可夫决策过程

有限MDP提供了一个有用的框架来建模强化学习问题，使我们能够抽象化具体问题，并将其表述为可以用强化学习算法解决的问题。通过这样做，我们能够将从一个问题中获得的知识转移到另一个问题，而不必对每个问题逐一进行理论分析。这有助于我们简化解决复杂强化学习问题的过程。正式地，有限MDP是由一个四元组定义的控制过程：

![](../Images/935296186b805287707a7ddd8fa813c7.png)

四元组（*S*，*A*，*P*，*R*）定义了四个不同的组件，每个组件描述了系统的一个特定方面。*S*和*A*分别定义了*状态*和*动作*的集合。而*P*表示*转移函数*，R表示*奖励函数*。在我们的例子中，我们将朋友的心情定义为状态集合*S*，他们的练习选择定义为动作集合*A*。转移函数*P*，通过图中的箭头可视化，展示了朋友的心情如何根据他们的学习量而变化。此外，奖励函数*R*用于衡量朋友的进步，这受到他们的心情和练习选择的影响。

## 策略和价值函数

给定MDP，我们现在可以为我们的朋友制定策略。借鉴我们最喜欢的烹饪播客的智慧，我们被提醒要掌握烹饪艺术，必须建立每天练习一点的习惯。受到这一思想的启发，我们为朋友制定了一项策略，提倡坚持练习的计划：每天练习一个小时。在强化学习理论中，策略被称为*策略*或*策略函数*，定义为从状态集合到在该状态下每个可能行动的概率的映射。正式地，策略*π*是给定状态*s*的行动*a*的概率分布。

![](../Images/dd4ec9c01c2be4d54a70293707f71b1e.png)

为了遵循“每天练习一点点”的理念，我们建立了一个策略，在中立和有动力状态下都有100%的概率进行一小时的练习。然而，在沮丧状态下，我们100%跳过练习，因为这是唯一可用的动作。这个例子展示了策略可以是确定性的，而不是返回所有可用动作的完整概率分布，而是返回一个只有一个动作的退化分布，该动作被完全执行。

![](../Images/40749867c48e26c42795da9e81079938.png)

尽管我们非常信任自己喜欢的烹饪播客，我们仍然希望通过跟随我们的策略来了解朋友的进展。在强化学习的术语中，我们称之为评估我们的策略，这由*价值函数*定义。为了获得初步印象，让我们计算一下朋友在跟随我们策略的十天内获得了多少知识。假设他们在开始练习时感到中立，他们将在第一天获得一个知识单元，之后每天增加两个知识单元，总计为19个单元。相反，如果我们的朋友在第一天已经有了动力，他们将获得20个知识单元；如果他们开始时感到沮丧，他们将仅获得17个知识单元。

尽管这个计算最开始看起来有点随意，但实际上我们可以从中学到一些东西。首先，我们直观地找到了为我们的策略分配数值的方法。其次，我们观察到这个数值依赖于朋友开始时的情绪。也就是说，让我们看看价值函数的正式定义。状态 *s* 的价值函数 *v* 被定义为代理在状态 *s* 开始并随后遵循策略 *π* 所获得的期望*折现回报*。我们将 *v* 称为策略 *π* 的*状态价值函数*。

![](../Images/09a1f63ff6a20d4515654c15496accf9.png)

我们将状态价值函数定义为在状态 *s* 开始时折现回报 *G* 的期望值 *E*。事实证明，我们的第一个方法实际上与实际定义相差不远。唯一的区别在于，我们的计算基于在固定天数内的知识增益总和，而不是更为客观的期望折现回报 *G*。在强化学习理论中，折现回报被定义为折现未来奖励的总和：

![](../Images/8eb810e2a60104bae0a7ef9e3f20fd40.png)

其中 *R* 表示在时间步 *t* 的奖励，乘以用小写 gamma 表示的 *折扣率*。折扣率在零到一的区间内，决定了我们对未来奖励赋予多少价值。为了更好地理解折扣率对奖励总和的影响，我们可以考虑将 gamma 设为零或一的特殊情况。将 gamma 设为零时，我们只考虑即时奖励，忽略任何未来奖励，这意味着折扣回报仅等于时间步 *t+1* 的奖励 *R*。相反，当 gamma 设为一时，我们对未来奖励赋予其全部价值，因此折扣回报将等于所有未来奖励的总和。

具备了价值函数和折扣回报的概念后，我们现在可以正确评估我们的策略。首先，我们需要为我们的例子选择一个合适的折扣率。我们必须排除零作为可能的候选，因为它不会考虑我们感兴趣的知识生成的长期价值。折扣率为一也应避免，因为我们的例子没有自然的最终状态；因此，任何包含定期练习工具的策略，无论多么无效，只要时间足够，都将产生无限的知识。因此，选择折扣率为一将使我们对让朋友每天练习还是每年练习无动于衷。在排除了零和一的特殊情况后，我们必须选择介于两者之间的合适折扣率。折扣率越小，未来奖励的价值越低，反之亦然。对于我们的例子，我们将折扣率设为0.9，并计算每种情绪的折扣回报。我们再次从动机状态开始。我们不再只考虑接下来的十天，而是计算所有折扣未来奖励的总和，结果为20个知识单位。计算过程如下¹：

![](../Images/12d147f86d7d6e2ec13559bc20df7157.png)

注意，通过引入小于一的折扣率，无限数量的未来奖励的总和仍然是常数。接下来我们希望分析的是中立状态。在这种状态下，我们的代理选择练习一个小时，获得一个知识单位，然后过渡到动机状态。这极大地简化了计算过程，因为我们已经知道动机状态的价值。

![](../Images/2ef4b747b82d1870aa680ffda3c53bd6.png)

作为最终步骤，我们还可以计算失去动机状态的价值函数。这个过程类似于中立状态，结果为略高于17的值。

![](../Images/023893a2814a64745684f576f69e687f.png)

通过检查我们策略中所有状态的状态价值函数，我们可以推断出动机状态是最有奖励的，这就是为什么我们应该指示我们的朋友尽快到达这个状态并保持在那里。

我鼓励你考虑开发替代的策略函数，并使用状态价值函数来评估它们。虽然其中一些可能在短期内更为成功，但从长期来看，它们生成的知识单元不会像我们提出的理论那样多²。如果你想深入了解 MDP、策略和价值函数背后的数学，我强烈推荐理查德·S·萨顿和安德鲁·G·巴托的《强化学习——导论》。另外，我建议你查看 YouTube 上的“David Silver 的 RL 课程”。

# 接下来是什么？

如果我们的朋友不对音乐感兴趣，而是请求我们帮助他们构建一辆自动驾驶汽车，或者我们的主管指示我们的团队开发一个改进的推荐系统怎么办？不幸的是，发现我们例子中的最优策略对其他强化学习问题帮助不大。因此，我们需要设计能够解决任何有限 MDP² 的算法。

在接下来的博客文章中，你将探索如何将各种强化学习算法应用于实际例子。我们将从表格解法方法开始，这些是最简单的强化学习算法形式，适用于解决具有小状态和动作空间的 MDP，例如我们例子中的情况。然后我们将深入深度学习，以解决具有任意大状态和动作空间的更复杂的强化学习问题，其中表格方法不再适用。这些近似解法方法将成为本课程第二部分的重点。最后，为了结束课程，我们将介绍一些领域内最具创新性的论文，对每篇论文进行全面分析，并提供实际例子来说明理论。

[1] 在我们的例子中，状态价值函数仅等于折扣回报。然而，如果环境或策略是非确定性的，那么我们需要对所有折扣回报进行求和，每个回报都按其可能性加权。

[2] 证明所提出的策略在长期内确实是最优的超出了这篇博客文章的范围。然而，我鼓励有兴趣的人将其作为练习来寻找证明。我建议从定义一个假设的最优策略开始，然后尝试证明所提出的策略的价值函数在环境的每个状态下都大于或等于最优策略的价值函数。

[3] 并不是所有的强化学习问题都符合数学上理想化的有限马尔可夫决策过程（MDP）形式。许多有趣的强化学习问题需要一种放宽的表述。尽管如此，为有限 MDP 开发的算法往往在不完全符合严格表述的强化学习问题中也能表现良好。
