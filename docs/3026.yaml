- en: Hierarchical Transformers — part 1
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 层次化变换器 — 第 1 部分
- en: 原文：[https://towardsdatascience.com/hierarchical-transformers-54f6d59fa8fc?source=collection_archive---------5-----------------------#2023-10-04](https://towardsdatascience.com/hierarchical-transformers-54f6d59fa8fc?source=collection_archive---------5-----------------------#2023-10-04)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/hierarchical-transformers-54f6d59fa8fc?source=collection_archive---------5-----------------------#2023-10-04](https://towardsdatascience.com/hierarchical-transformers-54f6d59fa8fc?source=collection_archive---------5-----------------------#2023-10-04)
- en: More efficient language models
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 更高效的语言模型
- en: '[](https://medium.com/@mina.ghashami?source=post_page-----54f6d59fa8fc--------------------------------)[![Mina
    Ghashami](../Images/745f53b94f5667a485299b49913c7a21.png)](https://medium.com/@mina.ghashami?source=post_page-----54f6d59fa8fc--------------------------------)[](https://towardsdatascience.com/?source=post_page-----54f6d59fa8fc--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----54f6d59fa8fc--------------------------------)
    [Mina Ghashami](https://medium.com/@mina.ghashami?source=post_page-----54f6d59fa8fc--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@mina.ghashami?source=post_page-----54f6d59fa8fc--------------------------------)[![Mina
    Ghashami](../Images/745f53b94f5667a485299b49913c7a21.png)](https://medium.com/@mina.ghashami?source=post_page-----54f6d59fa8fc--------------------------------)[](https://towardsdatascience.com/?source=post_page-----54f6d59fa8fc--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----54f6d59fa8fc--------------------------------)
    [Mina Ghashami](https://medium.com/@mina.ghashami?source=post_page-----54f6d59fa8fc--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fc99ed9ed7b9a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhierarchical-transformers-54f6d59fa8fc&user=Mina+Ghashami&userId=c99ed9ed7b9a&source=post_page-c99ed9ed7b9a----54f6d59fa8fc---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----54f6d59fa8fc--------------------------------)
    ·9 min read·Oct 4, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F54f6d59fa8fc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhierarchical-transformers-54f6d59fa8fc&user=Mina+Ghashami&userId=c99ed9ed7b9a&source=-----54f6d59fa8fc---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fc99ed9ed7b9a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhierarchical-transformers-54f6d59fa8fc&user=Mina+Ghashami&userId=c99ed9ed7b9a&source=post_page-c99ed9ed7b9a----54f6d59fa8fc---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----54f6d59fa8fc--------------------------------)
    · 9 分钟阅读 · 2023 年 10 月 4 日 [](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F54f6d59fa8fc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhierarchical-transformers-54f6d59fa8fc&user=Mina+Ghashami&userId=c99ed9ed7b9a&source=-----54f6d59fa8fc---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F54f6d59fa8fc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhierarchical-transformers-54f6d59fa8fc&source=-----54f6d59fa8fc---------------------bookmark_footer-----------)![](../Images/1b3d6e9ce3c3e6c0dec3c8326bd0faf9.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F54f6d59fa8fc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhierarchical-transformers-54f6d59fa8fc&source=-----54f6d59fa8fc---------------------bookmark_footer-----------)![](../Images/1b3d6e9ce3c3e6c0dec3c8326bd0faf9.png)'
- en: Image from [unsplash.com](https://unsplash.com/photos/x22UAIdif_k)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来自 [unsplash.com](https://unsplash.com/photos/x22UAIdif_k)
- en: 'In this article, we look at hierarchical transformers: what they are, how they
    work, how they differ from standard Transformers and what are their benefits.
    Let’s get started.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们将探讨层次化变换器：它们是什么，如何工作，它们与标准变换器有何不同以及它们的好处。让我们开始吧。
- en: What Are Hierarchical Transformers
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是层次化变换器
- en: The “hierarchical Transformer” refers to a transformer architecture that operates
    on multiple scales or resolutions of the input sequence.
  id: totrans-11
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “层次化变换器”指的是一种在输入序列的多个尺度或分辨率上操作的变换器架构。
- en: '**Why do we need hierarchical transformers?**'
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**我们为什么需要层次化变换器？**'
- en: Standard Transformers as amazing that they are are very time consuming. The
    attention mechanism inside Transformers takes O(n²) to run on an input sequence
    of n tokens. This means Transformers are not practical for long sequences. One
    way to address this inefficiencies, is to have hierarchical transformers. Is it
    the only way? no! another way is to have improve efficiency of attention mechanism.
    But this is a topic for another day.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管标准的 Transformers 非常惊艳，但它们却非常耗时。 Transformers 内部的注意力机制在处理 n 个 token 的输入序列时需要
    O(n²) 的时间。这意味着 Transformers 对于长序列并不实用。解决这种低效的方法之一是采用层次化的 Transformers。这是唯一的方法吗？不是！另一种方法是提高注意力机制的效率。但这将是另一个话题。
- en: How does hierarchy in Transformers help?
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 层次结构在 Transformers 中如何发挥作用？
- en: Hierarchical Transformers enable the model to operate on different levels of
    the input e.g. words, sentences, paragraphs etc. This matches how humans process
    text too. This forces attention over different hierarchies to model relationships
    between…
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 层次化 Transformers 使模型能够在输入的不同层次上操作，例如：词语、句子、段落等。这也符合人类处理文本的方式。这迫使注意力机制在不同的层次之间建模关系……
