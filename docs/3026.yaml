- en: Hierarchical Transformers — part 1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/hierarchical-transformers-54f6d59fa8fc?source=collection_archive---------5-----------------------#2023-10-04](https://towardsdatascience.com/hierarchical-transformers-54f6d59fa8fc?source=collection_archive---------5-----------------------#2023-10-04)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: More efficient language models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@mina.ghashami?source=post_page-----54f6d59fa8fc--------------------------------)[![Mina
    Ghashami](../Images/745f53b94f5667a485299b49913c7a21.png)](https://medium.com/@mina.ghashami?source=post_page-----54f6d59fa8fc--------------------------------)[](https://towardsdatascience.com/?source=post_page-----54f6d59fa8fc--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----54f6d59fa8fc--------------------------------)
    [Mina Ghashami](https://medium.com/@mina.ghashami?source=post_page-----54f6d59fa8fc--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fc99ed9ed7b9a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhierarchical-transformers-54f6d59fa8fc&user=Mina+Ghashami&userId=c99ed9ed7b9a&source=post_page-c99ed9ed7b9a----54f6d59fa8fc---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----54f6d59fa8fc--------------------------------)
    ·9 min read·Oct 4, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F54f6d59fa8fc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhierarchical-transformers-54f6d59fa8fc&user=Mina+Ghashami&userId=c99ed9ed7b9a&source=-----54f6d59fa8fc---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F54f6d59fa8fc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhierarchical-transformers-54f6d59fa8fc&source=-----54f6d59fa8fc---------------------bookmark_footer-----------)![](../Images/1b3d6e9ce3c3e6c0dec3c8326bd0faf9.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Image from [unsplash.com](https://unsplash.com/photos/x22UAIdif_k)
  prefs: []
  type: TYPE_NORMAL
- en: 'In this article, we look at hierarchical transformers: what they are, how they
    work, how they differ from standard Transformers and what are their benefits.
    Let’s get started.'
  prefs: []
  type: TYPE_NORMAL
- en: What Are Hierarchical Transformers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The “hierarchical Transformer” refers to a transformer architecture that operates
    on multiple scales or resolutions of the input sequence.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**Why do we need hierarchical transformers?**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Standard Transformers as amazing that they are are very time consuming. The
    attention mechanism inside Transformers takes O(n²) to run on an input sequence
    of n tokens. This means Transformers are not practical for long sequences. One
    way to address this inefficiencies, is to have hierarchical transformers. Is it
    the only way? no! another way is to have improve efficiency of attention mechanism.
    But this is a topic for another day.
  prefs: []
  type: TYPE_NORMAL
- en: How does hierarchy in Transformers help?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Hierarchical Transformers enable the model to operate on different levels of
    the input e.g. words, sentences, paragraphs etc. This matches how humans process
    text too. This forces attention over different hierarchies to model relationships
    between…
  prefs: []
  type: TYPE_NORMAL
