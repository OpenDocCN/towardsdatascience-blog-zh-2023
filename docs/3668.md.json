["```py\nMixtralForCausalLM(\n  (model): MixtralModel(\n    (embed_tokens): Embedding(32000, 4096)\n    (layers): ModuleList(\n      (0-31): 32 x MixtralDecoderLayer(\n        (self_attn): MixtralAttention(\n          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n          (rotary_emb): MixtralRotaryEmbedding()\n        )\n **(block_sparse_moe): MixtralSparseMoeBlock(\n          (gate): Linear4bit(in_features=4096, out_features=8, bias=False)\n          (experts): ModuleList(\n            (0-7): 8 x MixtralBLockSparseTop2MLP(\n              (w1): Linear4bit(in_features=4096, out_features=14336, bias=False)\n              (w2): Linear4bit(in_features=14336, out_features=4096, bias=False)\n              (w3): Linear4bit(in_features=4096, out_features=14336, bias=False)\n              (act_fn): SiLU()\n            )\n          )**\n        )\n        (input_layernorm): MixtralRMSNorm()\n        (post_attention_layernorm): MixtralRMSNorm()\n      )\n    )\n    (norm): MixtralRMSNorm()\n  )\n  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n)\n```", "```py\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_id = \"mistralai/Mixtral-8x7B-v0.1\"\ntokenizer = AutoTokenizer.from_pretrained(model_id)\n\nmodel = AutoModelForCausalLM.from_pretrained(model_id, load_in_4bit=True, use_flash_attention_2=True)\n\ntext = \"The mistral is\"\ninputs = tokenizer(text, return_tensors=\"pt\").to(0)\n\noutputs = model.generate(**inputs, max_new_tokens=200)\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))\n```", "```py\nmodel_name = \"mistralai/Mixtral-8x7B-v0.1\"\n#Tokenizer\ntokenizer = AutoTokenizer.from_pretrained(model_name, add_eos_token=True, use_fast=True)\ntokenizer.pad_token = tokenizer.unk_token\ntokenizer.pad_token_id =  tokenizer.unk_token_id\ntokenizer.padding_side = 'left'\n\ncompute_dtype = getattr(torch, \"float16\")\nbnb_config = BitsAndBytesConfig(\n        load_in_4bit=True,\n        bnb_4bit_quant_type=\"nf4\",\n        bnb_4bit_compute_dtype=compute_dtype,\n        bnb_4bit_use_double_quant=True,\n)\nmodel = AutoModelForCausalLM.from_pretrained(\n          model_name, quantization_config=bnb_config, device_map={\"\": 0}\n)\nmodel = prepare_model_for_kbit_training(model)\n```"]