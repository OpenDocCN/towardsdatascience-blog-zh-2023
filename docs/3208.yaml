- en: 'Mistral 7B: Recipes for Fine-tuning and Quantization on Your Computer'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Mistral 7B：在您的计算机上进行微调和量化的配方
- en: 原文：[https://towardsdatascience.com/mistral-7b-recipes-for-fine-tuning-and-quantization-on-your-computer-631401583f77?source=collection_archive---------0-----------------------#2023-10-26](https://towardsdatascience.com/mistral-7b-recipes-for-fine-tuning-and-quantization-on-your-computer-631401583f77?source=collection_archive---------0-----------------------#2023-10-26)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/mistral-7b-recipes-for-fine-tuning-and-quantization-on-your-computer-631401583f77?source=collection_archive---------0-----------------------#2023-10-26](https://towardsdatascience.com/mistral-7b-recipes-for-fine-tuning-and-quantization-on-your-computer-631401583f77?source=collection_archive---------0-----------------------#2023-10-26)
- en: Cheap supervised fine-tuning with an impressive LLM
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 便宜的监督微调与令人印象深刻的LLM
- en: '[](https://medium.com/@bnjmn_marie?source=post_page-----631401583f77--------------------------------)[![Benjamin
    Marie](../Images/3ea1ad230cb1e67610418a8e36a5e5dd.png)](https://medium.com/@bnjmn_marie?source=post_page-----631401583f77--------------------------------)[](https://towardsdatascience.com/?source=post_page-----631401583f77--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----631401583f77--------------------------------)
    [Benjamin Marie](https://medium.com/@bnjmn_marie?source=post_page-----631401583f77--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@bnjmn_marie?source=post_page-----631401583f77--------------------------------)[![Benjamin
    Marie](../Images/3ea1ad230cb1e67610418a8e36a5e5dd.png)](https://medium.com/@bnjmn_marie?source=post_page-----631401583f77--------------------------------)[](https://towardsdatascience.com/?source=post_page-----631401583f77--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----631401583f77--------------------------------)
    [Benjamin Marie](https://medium.com/@bnjmn_marie?source=post_page-----631401583f77--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fad2a414578b3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmistral-7b-recipes-for-fine-tuning-and-quantization-on-your-computer-631401583f77&user=Benjamin+Marie&userId=ad2a414578b3&source=post_page-ad2a414578b3----631401583f77---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----631401583f77--------------------------------)
    ·9 min read·Oct 26, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F631401583f77&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmistral-7b-recipes-for-fine-tuning-and-quantization-on-your-computer-631401583f77&user=Benjamin+Marie&userId=ad2a414578b3&source=-----631401583f77---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fad2a414578b3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmistral-7b-recipes-for-fine-tuning-and-quantization-on-your-computer-631401583f77&user=Benjamin+Marie&userId=ad2a414578b3&source=post_page-ad2a414578b3----631401583f77---------------------post_header-----------)
    发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----631401583f77--------------------------------)
    · 9分钟阅读 · 2023年10月26日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F631401583f77&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmistral-7b-recipes-for-fine-tuning-and-quantization-on-your-computer-631401583f77&user=Benjamin+Marie&userId=ad2a414578b3&source=-----631401583f77---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F631401583f77&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmistral-7b-recipes-for-fine-tuning-and-quantization-on-your-computer-631401583f77&source=-----631401583f77---------------------bookmark_footer-----------)![](../Images/bfa5a4ac457b91e1272b5d1cd9f44188.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F631401583f77&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmistral-7b-recipes-for-fine-tuning-and-quantization-on-your-computer-631401583f77&source=-----631401583f77---------------------bookmark_footer-----------)![](../Images/bfa5a4ac457b91e1272b5d1cd9f44188.png)'
- en: The mistral is a wind blowing in the northern Mediterranean Sea — Illustration
    from [Pixabay](https://pixabay.com/illustrations/wind-girl-tree-long-hair-4054954/)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: mistral 是一种吹拂在北部地中海的风 —— 图片来自 [Pixabay](https://pixabay.com/illustrations/wind-girl-tree-long-hair-4054954/)
- en: Mistral 7B is a very popular large language model (LLM) created by Mistral AI.
    It outperforms all [the other pre-trained LLMs of similar size](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard)
    and is even better than larger LLMs such as Llama 2 13B.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: Mistral 7B 是由 Mistral AI 创建的非常受欢迎的大型语言模型（LLM）。它超越了所有 [其他相似规模的预训练LLM](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard)，甚至优于更大的LLM，如
    Llama 2 13B。
- en: It is also very well optimized for fast decoding, especially for long contexts,
    thanks to the use of a sliding window to compute attention and grouped-query attention
    (GQA). You can find more details in [the arXiv paper presenting Mistral 7B](https://arxiv.org/abs/2310.06825)
    and in [this excellent article](https://medium.com/gitconnected/mistral-7b-a-new-wind-blowing-other-language-models-b74d7bfe137e)
    by [Salvatore Raieli](https://medium.com/u/f1a08d9452cd?source=post_page-----631401583f77--------------------------------).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 它也非常优化了快速解码，特别是对于长上下文，这要归功于使用滑动窗口计算注意力和分组查询注意力（GQA）。你可以在[介绍Mistral 7B的arXiv论文](https://arxiv.org/abs/2310.06825)和[Salvatore
    Raieli](https://medium.com/u/f1a08d9452cd?source=post_page-----631401583f77--------------------------------)的[这篇精彩文章](https://medium.com/gitconnected/mistral-7b-a-new-wind-blowing-other-language-models-b74d7bfe137e)中找到更多细节。
- en: Mistral 7B is good but small enough to be exploited with affordable hardware.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: Mistral 7B表现良好，但足够小，可以在经济实惠的硬件上利用。
- en: In this article, I will show you how to fine-tune Mistral 7B with QLoRA. We
    will use the dataset “ultrachat” that I modified for this article. Ultrachat was
    used by Hugging Face to create [Zephyr 7B](https://huggingface.co/HuggingFaceH4/zephyr-7b-alpha).
    We will also see how to quantize Mistral7B with AutoGPTQ.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我将展示如何使用QLoRA微调Mistral 7B。我们将使用我为本文修改的“ultrachat”数据集。Ultrachat被Hugging
    Face用来创建[Zephyr 7B](https://huggingface.co/HuggingFaceH4/zephyr-7b-alpha)。我们还将看看如何使用AutoGPTQ对Mistral7B进行量化。
- en: 'I wrote notebooks implementing all the sections. You can find them here:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 我编写了实现所有部分的笔记本。你可以在这里找到它们：
- en: '[Get the notebooks (#22, #23)](https://kaitchup.substack.com/p/notebooks)'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '[获取笔记本（#22，#23）](https://kaitchup.substack.com/p/notebooks)'
- en: Supervised Fine-Tuning of Mistral 7B with TRL
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用TRL对Mistral 7B进行监督微调
- en: Mistral 7B is a 7 billion parameter model. You roughly need 15 GB of VRAM to
    load it on a GPU. Then, full fine-tuning with batches will consume even more VRAM.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: Mistral 7B是一个拥有70亿参数的模型。你大约需要15 GB的VRAM来加载它到GPU上。然后，使用批量进行全面微调将消耗更多的VRAM。
- en: An alternative to standard full fine-tuning is to fine-tune with QLoRA. QLoRA
    fine-tunes LoRA adapters on top of a frozen quantized model. In previous articles,
    [I have used it to fine-tune Llama 2 7B](https://kaitchup.substack.com/p/fine-tune-llama-2-on-your-computer).
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 标准全面微调的替代方案是使用QLoRA进行微调。QLoRA在冻结的量化模型上微调LoRA适配器。在之前的文章中，[我已经使用它来微调Llama 2 7B](https://kaitchup.substack.com/p/fine-tune-llama-2-on-your-computer)。
- en: Since QLoRA quantizes to 4-bit (NF4), we approximately divide by 4 the memory
    consumption for loading the model, i.e., Mistral 7B quantized with NF4 consumes
    around 4 GB of VRAM. If you have a GPU with 12 GB of VRAM, it leaves a lot of
    space for increasing batch size and targeting more modules with LoRA.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 由于QLoRA将量化到4位（NF4），我们大约将加载模型的内存消耗减少了4倍，即，使用NF4量化的Mistral 7B消耗约4 GB的VRAM。如果你有一块12
    GB VRAM的GPU，这会留出大量空间来增加批量大小并使用LoRA针对更多模块。
- en: In other words, you can fine-tune Mistral 7B for free, on your machine if you
    have enough VRAM, or with the free instance of Google Colab which is equipped
    with a T4 GPU.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，如果你的机器有足够的VRAM，或者使用配备T4 GPU的Google Colab免费实例，你可以免费微调Mistral 7B。
- en: 'To fine-tune with QLoRA, you will need to install the following libraries:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用QLoRA进行微调，你需要安装以下库：
- en: '[PRE0]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Then, import the following:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，导入以下内容：
- en: '[PRE1]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Next, we load the tokenizer and configure it. As with Llama 2, we need to define
    a padding token. As usual, I chose the UNK token to pad the training examples.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们加载并配置标记器。与Llama 2一样，我们需要定义一个填充标记。像往常一样，我选择了UNK标记来填充训练示例。
- en: '*Note: Mistral 7B is fully open. You don’t need to be connected to Hugging
    Face or to sign a license agreement before downloading the model and the tokenizer.*'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '*注意：Mistral 7B完全开放。你无需连接到Hugging Face或在下载模型和标记器之前签署许可协议。*'
- en: '[PRE2]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: For fine-tuning, I made a custom version of [ultrachat](https://huggingface.co/datasets/stingning/ultrachat)
    (MIT license). Ultrachat contains 774k dialogues in the JSON format. This is way
    too many for a cheap fine-tuning and the format is not optimal for fine-tuning
    with TRL on consumer hardware. *Note:* [*TRL*](https://huggingface.co/docs/trl/index)
    *is a library developed by Hugging Face that simplifies fine-tuning of instruct
    LLMs.*
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 对于微调，我制作了一个自定义版本的[ultrachat](https://huggingface.co/datasets/stingning/ultrachat)（MIT许可证）。Ultrachat包含774k个JSON格式的对话。这对便宜的微调来说太多了，而且该格式对于在消费级硬件上使用TRL微调并不理想。*注意：*
    [*TRL*](https://huggingface.co/docs/trl/index) *是Hugging Face开发的一个库，它简化了对指令LLMs的微调。*
- en: 'I randomly subsampled ultrachat to only keep 100k dialogues. Then, I flattened
    the dialogues. For instance, in the original dataset, one example is formatted
    like this:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Once flattened, we have everything into one single sequence of tokens with
    “### Human” and “### Assistant” tags:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'This is the same format as [timdettmers/openassistant-guanaco](https://huggingface.co/datasets/timdettmers/openassistant-guanaco).
    This format can be directly used by TRL. You can find this version of ultrachat
    here:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: '[kaitchup/ultrachat-100k-flattened](https://huggingface.co/datasets/kaitchup/ultrachat-100k-flattened)'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To load the dataset, we do:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Now, let’s load the model and prepare it for QLoRA:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Replace “float16” with “bfloat16“ if your GPU supports it. It will make the
    training more stable.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: Next, we define the configuration of LoRA.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: I usually set up r=lora_alpha=16 since I read the Platypus paper.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://kaitchup.substack.com/p/platypus-dataset-curation-and-adapters?source=post_page-----631401583f77--------------------------------)
    [## Platypus: Dataset Curation and Adapters for Better Large Language Models'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: Achieve low-cost state-of-the-art on your target tasks
  id: totrans-43
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: kaitchup.substack.com](https://kaitchup.substack.com/p/platypus-dataset-curation-and-adapters?source=post_page-----631401583f77--------------------------------)
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: I target several modules. Here, I chose all the “proj” modules but there are
    more. You can find them all by running “print(model)”. Note that adding more modules
    may improve the performance but it also adds trainable parameters that will consume
    more VRAM.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: 'For training, I chose the following hyperparameters:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: I trained for only 100 steps. Since the examples are very long, training for
    one epoch would take more than 200 hours using a T4 GPU. If you use a V100 or
    an RTX 40xx, you may reduce it to 100 hours.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: I recommend at least 2,000 training steps to get a reasonably good model. It
    should take around two days of training with a T4 or one day with a more recent
    GPU.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: I also commented with “#” the hyperparameters related to the evaluation which
    is costly. Uncomment them to log the validation loss.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: 'Start the training with:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Then, once training is done, test the trained adapter by running:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'All this code is implemented in [this notebook #22](https://kaitchup.substack.com/p/notebooks).'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: Quantization of Mistral 7B with AutoGPTQ and bitsandbytes NF4
  id: totrans-56
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With bitsandbytes NF4
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To keep memory consumption low, we want to run quantized models.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: 'If you have fine-tuned your own model with QLoRA and would like to quantize
    it, your best option is to load and quantize Mistral 7B with bitsandbytes nf4
    as we did for QLoRA. Then, load your fine-tuned adapter on top of it. This is
    what we did in the last code sample in the previous section:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: I have explored several other options, for instance merging the adapter to the
    base model, but none of them are optimal.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://kaitchup.substack.com/p/lora-adapters-when-a-naive-merge?source=post_page-----631401583f77--------------------------------)
    [## LoRA Adapters: When a Naive Merge Leads to Poor Performance'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://kaitchup.substack.com/p/lora-adapters-when-a-naive-merge?source=post_page-----631401583f77--------------------------------)
    [## LoRA Adapters: When a Naive Merge Leads to Poor Performance'
- en: The case of LoRA adapters fine-tuned with QLoRA
  id: totrans-63
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: LoRA适配器微调的情况，使用QLoRA
- en: kaitchup.substack.com](https://kaitchup.substack.com/p/lora-adapters-when-a-naive-merge?source=post_page-----631401583f77--------------------------------)
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: kaitchup.substack.com](https://kaitchup.substack.com/p/lora-adapters-when-a-naive-merge?source=post_page-----631401583f77--------------------------------)
- en: We could have used [QA-LoRA to fine-tune a quantization-aware LoRA](https://kaitchup.substack.com/p/qa-lora-quantization-aware-fine-tuning)
    but the framework to do that doesn’t support yet Mistral 7B.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 我们本可以使用[QA-LoRA对量化感知的LoRA进行微调](https://kaitchup.substack.com/p/qa-lora-quantization-aware-fine-tuning)，但该框架目前尚不支持Mistral
    7B。
- en: 'Mistral AI also released an instruct version of Mistral 7B. You can get it
    here instead of training your instruct Mistral 7B:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: Mistral AI还发布了Mistral 7B的指令版。你可以在这里获取，而不是训练自己的指令Mistral 7B：
- en: '[mistralai/Mistral-7B-Instruct-v0.1](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1)'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[mistralai/Mistral-7B-Instruct-v0.1](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1)'
- en: I use this model for the following quantization examples.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 我使用这个模型进行以下量化示例。
- en: 'To load and quantize the model with bitsandbytes, you first need to install
    the following libraries:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 要用bitsandbytes加载并量化模型，你首先需要安装以下库：
- en: '[PRE12]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Then, run:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，运行：
- en: '[PRE13]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'For testing generation, you can use the generate function I defined in the
    previous section with the right prompt format used by Mistral Instruct:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 对于生成测试，你可以使用我在前面部分定义的`generate`函数，配合Mistral Instruct使用的正确提示格式：
- en: '[PRE14]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: With AutoGPTQ
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用AutoGPTQ
- en: bitsandbytes nf4 is fast to quantize but slow for inference. Currently, we can’t
    serialize nf4 models.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: bitsandbytes nf4量化速度快，但推理速度慢。目前，我们无法序列化nf4模型。
- en: AutoGPTQ is much slower for quantization but you only need to do it once. It’s
    also [much faster than nf4 for decoding](https://kaitchup.substack.com/p/gptq-or-bitsandbytes-which-quantization)
    but keep in mind that models quantized with [AutoGPTQ (INT4) are slightly worse
    than models quantized with nf4](https://kaitchup.substack.com/p/quantize-and-fine-tune-llms-with).
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: AutoGPTQ的量化速度较慢，但你只需做一次。它也比[nf4解码速度快](https://kaitchup.substack.com/p/gptq-or-bitsandbytes-which-quantization)，但请注意，用[AutoGPTQ
    (INT4) 量化的模型略逊色于用nf4量化的模型](https://kaitchup.substack.com/p/quantize-and-fine-tune-llms-with)。
- en: 'We need the following libraries:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要以下库：
- en: '[PRE15]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Then, import the following package:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，导入以下包：
- en: '[PRE16]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Then, the model will be quantized when we load it with “AutoModelForCausalLM.from_pretrained”.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，当我们用“AutoModelForCausalLM.from_pretrained”加载模型时，它会被量化。
- en: '[PRE17]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Quantization is very costly in memory. You will need more than 24 GB of VRAM,
    e.g., the A100 of Google Colab Pro would work. I recommend saving the model once
    it’s loaded to make sure you won’t have to do it again.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 量化非常消耗内存。你需要超过24 GB的VRAM，例如，Google Colab Pro的A100会有效。我建议在加载模型后保存它，以确保你不会再需要重复操作。
- en: 'All this quantization code is also available in my [notebook #23](https://kaitchup.substack.com/p/notebooks).'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '所有这些量化代码也可以在我的[笔记本 #23](https://kaitchup.substack.com/p/notebooks)中找到。'
- en: 'Note that TheBloke proposes Mistral Instruct 7B quantized with AutoGPTQ on
    the Hugging Face Hub:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，TheBloke在Hugging Face Hub上提议了使用AutoGPTQ量化的Mistral Instruct 7B：
- en: '[TheBloke/Mistral-7B-Instruct-v0.1-GPTQ](https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GPTQ)'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[TheBloke/Mistral-7B-Instruct-v0.1-GPTQ](https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GPTQ)'
- en: Conclusion
  id: totrans-88
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: Mistral 7B is an impressive pre-trained LLM. You can easily fine-tune it on
    your computer with QLoRA. However, fine-tuning remains very time-consuming. As
    we saw, you may need to run it for a hundred hours or more.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: Mistral 7B是一个令人印象深刻的预训练LLM。你可以很容易地在电脑上用QLoRA进行微调。然而，微调依然非常耗时。正如我们所见，你可能需要运行几百小时或更长时间。
- en: If you don’t need to fine-tune Mistral 7B on your data, Mistral Instruct is
    a good alternative proposed by Mistral AI. To run it on consumer hardware, you
    can quantize it with bitsandbytes nf4 or GPTQ.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你不需要在你的数据上微调Mistral 7B，Mistral AI提出的Mistral Instruct是一个不错的替代方案。要在消费级硬件上运行它，你可以使用bitsandbytes
    nf4或GPTQ进行量化。
- en: 'To support my work, consider subscribing to my newsletter:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 支持我的工作，请考虑订阅我的新闻通讯：
- en: '[](https://kaitchup.substack.com/?source=post_page-----631401583f77--------------------------------)
    [## The Kaitchup - AI on a Budget | Benjamin Marie, PhD | Substack'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://kaitchup.substack.com/?source=post_page-----631401583f77--------------------------------)
    [## Kaitchup - AI on a Budget | Benjamin Marie, PhD | Substack'
- en: Weekly news, tips, and tutorials on fine-tuning, running, and serving large
    language models on your computer. Each…
  id: totrans-93
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 每周提供关于在你的计算机上微调、运行和服务大型语言模型的新闻、技巧和教程。每一个……
- en: kaitchup.substack.com](https://kaitchup.substack.com/?source=post_page-----631401583f77--------------------------------)
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '[kaitchup.substack.com](https://kaitchup.substack.com/?source=post_page-----631401583f77--------------------------------)'
