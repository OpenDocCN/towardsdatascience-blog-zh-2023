- en: Kaiming He Initialization in Neural Networks — Math Proof
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/kaiming-he-initialization-in-neural-networks-math-proof-73b9a0d845c4?source=collection_archive---------7-----------------------#2023-02-15](https://towardsdatascience.com/kaiming-he-initialization-in-neural-networks-math-proof-73b9a0d845c4?source=collection_archive---------7-----------------------#2023-02-15)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Deriving optimal initial variance of weight matrices in neural network layers
    with ReLU activation function
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@EsterHlav?source=post_page-----73b9a0d845c4--------------------------------)[![Ester
    Hlav](../Images/5fe679b93c42d568d0d5b331a8bf92b9.png)](https://medium.com/@EsterHlav?source=post_page-----73b9a0d845c4--------------------------------)[](https://towardsdatascience.com/?source=post_page-----73b9a0d845c4--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----73b9a0d845c4--------------------------------)
    [Ester Hlav](https://medium.com/@EsterHlav?source=post_page-----73b9a0d845c4--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F7476ea235ae9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fkaiming-he-initialization-in-neural-networks-math-proof-73b9a0d845c4&user=Ester+Hlav&userId=7476ea235ae9&source=post_page-7476ea235ae9----73b9a0d845c4---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----73b9a0d845c4--------------------------------)
    ·10 min read·Feb 15, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F73b9a0d845c4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fkaiming-he-initialization-in-neural-networks-math-proof-73b9a0d845c4&user=Ester+Hlav&userId=7476ea235ae9&source=-----73b9a0d845c4---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F73b9a0d845c4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fkaiming-he-initialization-in-neural-networks-math-proof-73b9a0d845c4&source=-----73b9a0d845c4---------------------bookmark_footer-----------)![](../Images/0e072a98caece16e7471a537515610cc.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Initialization techniques are one of the prerequisites for successfully training
    a deep learning architecture. Traditionally, weight initialization methods need
    to be compatible with the choice of an activation function as a mismatch can potentially
    affect training negatively.
  prefs: []
  type: TYPE_NORMAL
- en: ReLU is one of the most commonly used activation functions in deep learning.
    Its properties make it a very convenient choice for scaling to large neural networks.
    On one hand, it is inexpensive to calculate the derivative during backpropagation
    because it is a linear function with a step-function derivative. On the other
    hand, ReLU helps reduce feature correlation as it is a non-negative activation
    function, *i.e.* features can only contribute positively to subsequent layers.
    It is a prevalent choice in convolutional architectures where the input dimension
    is large and neural networks tend to be very deep.
  prefs: []
  type: TYPE_NORMAL
- en: 'In *“Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet
    Classification”* ⁽*¹* ⁾ by He *et al.* (2015), the authors present a methodology
    to optimally initialize neural network layers using a ReLU activation function.
    This…'
  prefs: []
  type: TYPE_NORMAL
