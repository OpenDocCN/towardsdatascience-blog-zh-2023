- en: CLIP Model and The Importance of Multimodal Embeddings
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/clip-model-and-the-importance-of-multimodal-embeddings-1c8f6b13bf72?source=collection_archive---------1-----------------------#2023-12-11](https://towardsdatascience.com/clip-model-and-the-importance-of-multimodal-embeddings-1c8f6b13bf72?source=collection_archive---------1-----------------------#2023-12-11)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[](https://medium.com/@faheemrustamy?source=post_page-----1c8f6b13bf72--------------------------------)[![Fahim
    Rustamy, PhD](../Images/949c8654bd91d03124d0ba95182d8558.png)](https://medium.com/@faheemrustamy?source=post_page-----1c8f6b13bf72--------------------------------)[](https://towardsdatascience.com/?source=post_page-----1c8f6b13bf72--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----1c8f6b13bf72--------------------------------)
    [Fahim Rustamy, PhD](https://medium.com/@faheemrustamy?source=post_page-----1c8f6b13bf72--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F931fc8afcda1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fclip-model-and-the-importance-of-multimodal-embeddings-1c8f6b13bf72&user=Fahim+Rustamy%2C+PhD&userId=931fc8afcda1&source=post_page-931fc8afcda1----1c8f6b13bf72---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----1c8f6b13bf72--------------------------------)
    ·10 min read·Dec 11, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F1c8f6b13bf72&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fclip-model-and-the-importance-of-multimodal-embeddings-1c8f6b13bf72&user=Fahim+Rustamy%2C+PhD&userId=931fc8afcda1&source=-----1c8f6b13bf72---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F1c8f6b13bf72&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fclip-model-and-the-importance-of-multimodal-embeddings-1c8f6b13bf72&source=-----1c8f6b13bf72---------------------bookmark_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: CLIP, which stands for Contrastive Language-Image Pretraining, is a deep learning
    model developed by OpenAI in 2021\. CLIP’s embeddings for images and text share
    the same space, enabling direct comparisons between the two modalities. This is
    accomplished by training the model to bring related images and texts closer together
    while pushing unrelated ones apart. This article will explain how CLIP works and
    guide you through an example to train CLIP model using the flikker and COCO datasets.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can find the code in this GitHub repo:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/RustamyF/clip-multimodal-ml](https://github.com/RustamyF/clip-multimodal-ml)'
  prefs: []
  type: TYPE_NORMAL
- en: '**CLIP’s Applications**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Some applications of CLIP include:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Image Classification and Retrieval: CLIP can be used for image classification
    tasks by associating images with natural language descriptions. It allows for
    more versatile and flexible image retrieval systems where users can search for
    images using textual queries.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Content Moderation: CLIP can be used to moderate content on online platforms
    by analyzing images and accompanying text to identify and filter out inappropriate
    or harmful content.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The original CLIP model aimed to unite image and text modalities within a shared
    embedding space. This concept, along with its techniques, extends beyond images
    and text to embrace other modalities. Netflix, in [this blog post](https://netflixtechblog.com/building-in-video-search-936766f0017c),
    trained a model by combining video and text modalities in the common embedding
    space to enhance search within video applications. [Contrastive Language-Audio
    Pretraining (CLAP)](https://arxiv.org/abs/2206.04769) is another model that integrates
    text and audio modalities within the same embedding space, making it valuable
    for improving search functionalities within audio applications.
  prefs: []
  type: TYPE_NORMAL
- en: The underlying technology for CLIP is extremely simple but very powerful, opening
    the door for many multi-model machine learning techniques. Meta AI recently released
    [**ImageBind**](https://imagebind.metademolab.com/), which learns a joint embedding
    across six modalities — images, text, audio, depth, thermal, and IMU data. CLIP,
    the first large-scale AI model that accepts two modalities, is a prerequisite
    to understanding ImageBind and other multi-modality AI systems.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1a5691a4c507c3342a3efbd4537d7ee5.png)'
  prefs: []
  type: TYPE_IMG
- en: Imagebind from META AI accepts six different modalities as input (Taken from
    [ImageBind's official GitHub page](https://github.com/facebookresearch/ImageBind)).
  prefs: []
  type: TYPE_NORMAL
- en: '**What is CLIP**'
  prefs: []
  type: TYPE_NORMAL
- en: CLIP is designed to predict which N × N potential (image, text) pairings within
    the batch are actual matches. To achieve this, CLIP establishes a multi-modal
    embedding space through the joint training of an image encoder and text encoder.
    **The CLIP loss aims to maximize the cosine similarity between the image and text
    embeddings for the N genuine pairs in the batch while minimizing the cosine similarity
    for the N² − N incorrect pairings.** The optimization process involves using a
    symmetric cross-entropy loss function that operates on these similarity scores.
    The following presents pseudocode (taken from the original paper) outlining the
    core implementation of CLIP.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Here’s a step-by-step description of each line in the pseudo code and its implementation
    using PyTorch:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Model Architecture:**'
  prefs: []
  type: TYPE_NORMAL
- en: 'ClIP uses two separate architectures as the backbone for encoding vision and
    text datasets:'
  prefs: []
  type: TYPE_NORMAL
- en: '`image_encoder`: Represents the neural network architecture (e.g., ResNet or
    Vision Transformer) responsible for encoding images.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`text_encoder`: Represents the neural network architecture (e.g., CBOW, BERT,
    or Text Transformer) responsible for encoding textual information.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The original CLIP model was trained from scratch without initializing the image
    encoder and the text encoder with pre-trained weights due to the large volume
    of the dataset (400 million image-text pairs) that they used to train their CLIP
    model. In the example in this blog post, we’ll do things a bit differently. We’ll
    start with pre-trained weights from resnet (for images) and distilbert (for text)
    models to initialize these parts.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5c97c5a7d2daac2d6759947f8927d4e1.png)'
  prefs: []
  type: TYPE_IMG
- en: Architecture of CLIP model (taken from the original paper)
  prefs: []
  type: TYPE_NORMAL
- en: '**Input Data:**'
  prefs: []
  type: TYPE_NORMAL
- en: 'The model takes a batch of n pairs of images and texts as input where:'
  prefs: []
  type: TYPE_NORMAL
- en: '`I[n, h, w, c]`: Represents a minibatch of aligned images, where `n` is the
    batch size, `h` is the image height, `w` is the image width, and `c` is the number
    of channels.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`T[n, l]`: Represents a minibatch of aligned texts, where `n` is the batch
    size, and `l` is the length of the textual sequence.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/565a0701a6c33df60c9fb469afa20d20.png)'
  prefs: []
  type: TYPE_IMG
- en: One batch of image and caption pairs for a batch size of 128
  prefs: []
  type: TYPE_NORMAL
- en: '**Feature Extraction:**'
  prefs: []
  type: TYPE_NORMAL
- en: '`I_f = image_encoder(I)`: Extracts feature representations (`I_f`) from the
    image encoder. The shape of `I_f` is `[n, d_i]`, where `d_i` is the dimensionality
    of the image features.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`T_f = text_encoder(T)`: Extracts feature representations (`T_f`) from the
    text encoder. The shape of `T_f` is `[n, d_t]`, where `d_t` is the dimensionality
    of the text features.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '**Learned Projections:**'
  prefs: []
  type: TYPE_NORMAL
- en: '`W_i[d_i, d_e]`: Represents the learned projection matrix for mapping image
    features (`I_f`) to an embedding space (`I_e`). The shape of `W_i` is `[d_i, d_e]`,
    where `d_e` is the desired dimensionality of the joint embedding space.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`W_t[d_t, d_e]`: Represents the learned projection matrix for mapping text
    features (`T_f`) to the same embedding space (`T_e`). The shape of `W_t` is `[d_t,
    d_e]`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The projection operation can be coded using a neural network with two linear
    layers, whose weights are the **learned projection matrix**. In most cases, the
    projection weights are the only weights with active gradients that can be trained
    on new datasets. Additionally, the projection layer plays a crucial role in aligning
    the dimensions of image and text embeddings, ensuring that they have the same
    size.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '**Embedding and Normalization:**'
  prefs: []
  type: TYPE_NORMAL
- en: '`I_e = l2_normalize(np.dot(I_f, W_i), axis=1)`: Embeds and normalizes image
    features in the joint embedding space (`I_e`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`T_e = l2_normalize(np.dot(T_f, W_t), axis=1)`: Embeds and normalizes text
    features in the joint embedding space (`T_e`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The code below illustrates the sequential processing of image and text data.
    Initially, the data undergoes processing through the base encoder, followed by
    the projection layer. finally, normalized embeddings are generated for both modalities
    and returned.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '**Cosine Similarities:**'
  prefs: []
  type: TYPE_NORMAL
- en: '`logits = np.dot(I_e, T_e.T) * np.exp(t)`: Computes pairwise cosine similarities
    between image and text embeddings, scaled by a learned temperature parameter `t`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this example, we interchangeably use similarity with logits in the same way
    that was used in the original paper. We will not include the temperature parameter
    `t` in this blog post.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '**Symmetric Loss Function:**'
  prefs: []
  type: TYPE_NORMAL
- en: CLIP uses contrastive loss (first introduced in [Representation Learning with
    Contrastive Predictive Coding](https://arxiv.org/pdf/1807.03748.pdf)) to bring
    related images and texts closer together while pushing unrelated ones apart.
  prefs: []
  type: TYPE_NORMAL
- en: '`labels = np.arange(n)`: Generates labels representing the indices of the batch.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`loss_i = cross_entropy_loss(logits, labels, axis=0)`: Computes the cross-entropy
    loss along the image axis.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`loss_t = cross_entropy_loss(logits, labels, axis=1)`: Computes the cross-entropy
    loss along the text axis.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`loss = (loss_i + loss_t)/2`: Computes the symmetric average of the image and
    text losses.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '**Final Custom CLIP Model**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Combing all the different pieces together, the final custom CLIP model looks
    like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '**Example**'
  prefs: []
  type: TYPE_NORMAL
- en: This example demonstrates the process of creating image caption datasets and
    training a custom CLIP model. The aim is to train a vision encoder and a text
    encoder jointly to project the representation of images and their captions into
    the same embedding space, such that the caption embeddings are located near the
    embeddings of the images they describe. The code for this project is in my [GitHub
    repository](https://github.com/RustamyF/clip-multimodal-ml).
  prefs: []
  type: TYPE_NORMAL
- en: '**Dataset and Dataloader**'
  prefs: []
  type: TYPE_NORMAL
- en: Our custom CLIP model will be trained using the [flickr30k dataset](https://huggingface.co/datasets/nlphuji/flickr30k).
    This dataset comprises more than 31,000 images, each with a minimum of 5 independent
    human-generated captions. We will use two captions for each image in this example
    to have a total of 62,000 image and text pairs for training. Although traditionally
    employed for image captioning tasks, we intend to adapt the image-caption pairs
    to train our dual encoder model specifically for image search purposes. The [GitHub
    repository](https://github.com/RustamyF/clip-multimodal-ml) also includes the
    code to train the model on the MS-COCO dataset with 164,000 image and text pairs.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Key model constants include`embed_dim` for learned representations, `transformer_embed_dim`
    for transformer layer features, and `max_len` for text input length. The chosen
    `text_model` is “distilbert-base-multilingual-cased.” Training spans 3`epochs`
    with a`batch_size` of 128, which are the constants that will feed into the model
    building and training.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: The DataLoader is set up for efficient iteration during training, providing
    organized access to image-caption pairs.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Here is an example of an image caption pair in one of the batches in the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/4069befa5671d85cbb02e7f5ed477411.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, we initiate our CustomModel and send it to the device (CPU or GPU). Additionally,
    we specify the parameters to be optimized throughout the training process. Given
    that we have fixed the base layer for both text and image encoders, only the parameters
    associated with the projection layer will undergo training on the new dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '**Model training**'
  prefs: []
  type: TYPE_NORMAL
- en: The training was performed with a Tesla T4 (g4dn-xlarge) GPU machine for 3 training
    epochs. The [Jupyter Notebook](https://github.com/RustamyF/clip-multimodal-ml/blob/main/notebooks/flicker30kclip_model.ipynb)
    is available in the project’s [GitHub repository](https://github.com/RustamyF/clip-multimodal-ml)
    and contains the code for the training loop.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: The following are the results of training loops for each epoch using the flicker30k
    dataset. For additional details, please refer to [this notebook](https://github.com/RustamyF/clip-multimodal-ml/blob/main/notebooks/flicker30kclip_model.ipynb).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Here are the results from the training loops for each epoch using the COCO2017
    dataset. The model exhibits faster convergence on the COCO dataset, attributed
    to the availability of over 160,000 image-text pairs, in contrast to the 62,000
    image pairs in the flickr30k dataset. For additional details, please refer to
    [this notebook](https://github.com/RustamyF/clip-multimodal-ml/blob/main/notebooks/coco2017clip_model.ipynb).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '**Conclusion**'
  prefs: []
  type: TYPE_NORMAL
- en: In conclusion, this blog post has explored the CLIP model, uncovering its potential
    for wide-ranging applications. As we understand the applications of CLIP, it becomes
    evident that its impact spans far beyond initial expectations, paving the way
    for innovative solutions across diverse fields. CLIP was the first successful
    model that bridged the gap between different modalities and opened avenues for
    cross-disciplinary innovations.
  prefs: []
  type: TYPE_NORMAL
