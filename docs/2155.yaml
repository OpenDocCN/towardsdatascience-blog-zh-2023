- en: 'Implementing math in deep learning papers into efficient PyTorch code: SimCLR
    Contrastive Loss'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将深度学习论文中的数学公式转化为高效的 PyTorch 代码：SimCLR 对比损失
- en: 原文：[https://towardsdatascience.com/implementing-math-in-deep-learning-papers-into-efficient-pytorch-code-simclr-contrastive-loss-be94e1f63473?source=collection_archive---------5-----------------------#2023-07-05](https://towardsdatascience.com/implementing-math-in-deep-learning-papers-into-efficient-pytorch-code-simclr-contrastive-loss-be94e1f63473?source=collection_archive---------5-----------------------#2023-07-05)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/implementing-math-in-deep-learning-papers-into-efficient-pytorch-code-simclr-contrastive-loss-be94e1f63473?source=collection_archive---------5-----------------------#2023-07-05](https://towardsdatascience.com/implementing-math-in-deep-learning-papers-into-efficient-pytorch-code-simclr-contrastive-loss-be94e1f63473?source=collection_archive---------5-----------------------#2023-07-05)
- en: Learning to implement advanced math formulas in deep learning papers into performant
    PyTorch code in 3 steps.
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 学习如何将深度学习论文中的高级数学公式转化为高效的 PyTorch 代码，共分为三步。
- en: '[](https://medium.com/@moein.shariatnia?source=post_page-----be94e1f63473--------------------------------)[![Moein
    Shariatnia](../Images/fe92e30312ac3e0536c52ca7c0dfe30c.png)](https://medium.com/@moein.shariatnia?source=post_page-----be94e1f63473--------------------------------)[](https://towardsdatascience.com/?source=post_page-----be94e1f63473--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----be94e1f63473--------------------------------)
    [Moein Shariatnia](https://medium.com/@moein.shariatnia?source=post_page-----be94e1f63473--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@moein.shariatnia?source=post_page-----be94e1f63473--------------------------------)[![Moein
    Shariatnia](../Images/fe92e30312ac3e0536c52ca7c0dfe30c.png)](https://medium.com/@moein.shariatnia?source=post_page-----be94e1f63473--------------------------------)[](https://towardsdatascience.com/?source=post_page-----be94e1f63473--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----be94e1f63473--------------------------------)
    [Moein Shariatnia](https://medium.com/@moein.shariatnia?source=post_page-----be94e1f63473--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F72d1b463ba60&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimplementing-math-in-deep-learning-papers-into-efficient-pytorch-code-simclr-contrastive-loss-be94e1f63473&user=Moein+Shariatnia&userId=72d1b463ba60&source=post_page-72d1b463ba60----be94e1f63473---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----be94e1f63473--------------------------------)
    ·7 min read·Jul 5, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fbe94e1f63473&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimplementing-math-in-deep-learning-papers-into-efficient-pytorch-code-simclr-contrastive-loss-be94e1f63473&user=Moein+Shariatnia&userId=72d1b463ba60&source=-----be94e1f63473---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F72d1b463ba60&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimplementing-math-in-deep-learning-papers-into-efficient-pytorch-code-simclr-contrastive-loss-be94e1f63473&user=Moein+Shariatnia&userId=72d1b463ba60&source=post_page-72d1b463ba60----be94e1f63473---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----be94e1f63473--------------------------------)
    ·7分钟阅读·2023年7月5日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fbe94e1f63473&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimplementing-math-in-deep-learning-papers-into-efficient-pytorch-code-simclr-contrastive-loss-be94e1f63473&user=Moein+Shariatnia&userId=72d1b463ba60&source=-----be94e1f63473---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fbe94e1f63473&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimplementing-math-in-deep-learning-papers-into-efficient-pytorch-code-simclr-contrastive-loss-be94e1f63473&source=-----be94e1f63473---------------------bookmark_footer-----------)![](../Images/06f593f36f53b88fe931cba617c07f43.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fbe94e1f63473&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimplementing-math-in-deep-learning-papers-into-efficient-pytorch-code-simclr-contrastive-loss-be94e1f63473&source=-----be94e1f63473---------------------bookmark_footer-----------)![](../Images/06f593f36f53b88fe931cba617c07f43.png)'
- en: Photo by [Jeswin Thomas](https://unsplash.com/@jeswinthomas?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 摄影：[Jeswin Thomas](https://unsplash.com/@jeswinthomas?utm_source=medium&utm_medium=referral)
    由 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral) 提供
- en: Introduction
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 介绍
- en: One of the best ways to deepen your understanding of the math behind deep learning
    models and loss functions, and also a great way to improve your PyTorch skills
    is to get used to implementing deep learning papers all by yourself.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 加深对深度学习模型和损失函数背后数学理解的最佳方法之一，也是提高 PyTorch 技能的好方法，是自己动手实现深度学习论文。
- en: Books and blog posts could help you get started in coding and learning the basics
    in ML / DL, but after studying a few of them and getting good at the routine tasks
    in the field, you will soon realize that you are on your own in the learning journey
    and you’ll find most of the resources online as boring and too shallow. However,
    I believe that if you can study new deep learning papers as they get published
    and understand the required pieces of math in it (not necessarily all the mathematical
    proofs behind authors’ theories), and, you are a capable coder who can implement
    them into efficient code, nothing can stop you from staying up to date in the
    field and learning new ideas.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 书籍和博客帖子可以帮助你入门编程和学习机器学习/深度学习的基础知识，但在学习了几个相关资源并掌握了领域中的常规任务后，你会很快意识到在学习的过程中你只能依靠自己，并且你会发现大多数在线资源都很枯燥且过于浅薄。然而，我相信，如果你能在新深度学习论文发表时学习，并理解其中所需的数学部分（不一定要理解作者理论背后的所有数学证明），同时，你又是一个能够将其实现为高效代码的能干程序员，那么没有什么能阻止你在该领域保持最新并学习新思想。
- en: Contrastive Loss implementation
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 对比损失的实现
- en: 'I’ll introduce my routine and the steps I follow to implement math in deep
    learning papers using a not trivial example: The **Contrastive Loss** in the [SimCLR
    paper](https://arxiv.org/pdf/2002.05709.pdf).'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 我将介绍我的常规方法和实现数学在深度学习论文中的步骤，使用一个不简单的例子：在[SimCLR 论文](https://arxiv.org/pdf/2002.05709.pdf)中的**对比损失**。
- en: 'Here’s the mathematical formulation of the loss:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 这是损失的数学公式：
- en: '![](../Images/9d0967b5aeec60112b7981fabd5f7074.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9d0967b5aeec60112b7981fabd5f7074.png)'
- en: Contrastive (NT-Xent) loss from the SimCLR paper | from [https://arxiv.org/pdf/2002.05709.pdf](https://arxiv.org/pdf/2002.05709.pdf)
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: SimCLR 论文中的对比（NT-Xent）损失 | 来源于 [https://arxiv.org/pdf/2002.05709.pdf](https://arxiv.org/pdf/2002.05709.pdf)
- en: I agree that the mere look of the formula could be daunting! and you might be
    thinking that there must be lot’s of ready PyTorch implementations on GitHub,
    so let’s use them :) and Yes, you’re right. There are dozens of implementations
    online. However, I think this is a good example for practicing this skill and
    could serve as a good starting point.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 我同意公式的外观可能会让人感到畏惧！你可能会想，GitHub 上一定有很多现成的 PyTorch 实现，所以我们就使用它们吧 :) 是的，你说得对。网上确实有很多实现。然而，我认为这是一个练习这种技能的好例子，也可以作为一个很好的起点。
- en: Steps to implement math in code
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将数学实现到代码中的步骤
- en: 'My routine in implementing the math in papers into efficient PyTorch code is
    as follows:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 我将数学实现到 PyTorch 高效代码中的常规方法如下：
- en: Understand the math, explain it in simple terms
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 理解数学，将其用简单的术语解释
- en: Implement an initial version using simple Python **“for” loops**, no fancy matrix
    multiplications for now
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用简单的 Python **“for” 循环** 实现一个初始版本，现在不进行复杂的矩阵乘法
- en: Convert your code into **efficient** **matrix-friendly** PyTorch code
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将你的代码转换成**高效** **矩阵友好的** PyTorch 代码
- en: OK, let’s get straight to the first step.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，让我们直接进入第一步。
- en: 'Step 1: Understanding the math and explaining it in simple terms'
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 步骤 1：理解数学并用简单的术语解释
- en: I’m assuming that you have a basic knowledge of linear algebra and are familiar
    with mathematical notations. If you’re not, you can use [this tool](https://detexify.kirelabs.org/classify.html)
    to know what each of these symbols are and what they do in math, simply by drawing
    the symbol. You can also check this awesome [Wikipedia page](https://en.wikipedia.org/wiki/Glossary_of_mathematical_symbols)
    where most of the notations are described. These are the opportunities in where
    you learn new stuff, in searching and reading what is needed at the time you need
    it. I believe it’s a more efficient way of learning, instead of starting with
    a math textbook from scratch and putting it away after a few days :)
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 我假设你具备基本的线性代数知识并熟悉数学符号。如果没有，你可以使用[这个工具](https://detexify.kirelabs.org/classify.html)来了解这些符号的含义和功能，只需绘制符号即可。你还可以查看这个很棒的[维基百科页面](https://en.wikipedia.org/wiki/Glossary_of_mathematical_symbols)，其中描述了大多数符号。这些都是你在需要时学习新知识的机会。我认为这是一种更高效的学习方式，而不是从头开始阅读数学教科书，几天后就放在一边
    :)
- en: Back to our business. As the paragraph above the formula adds more context,
    in the SimCLR learning strategy you start with N images and transform each of
    them 2 times to get augmented views of those images (2*N images now). Then, you
    pass these 2 * N images through a model to get embedding vectors for each of them.
    Now, you want to make the embedding vectors of the 2 augmented views of the same
    image (a positive pair) closer in the embedding space (and do the same for all
    the other positive pairs). One way to measure how similar (close, in the same
    direction) two vectors are, is by using **Cosine Similarity** which is defined
    as sim(u, v) (look up the definition in the image above).
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 回到我们的主题。正如公式上方的段落增加了更多的背景，在SimCLR学习策略中，你从N张图像开始，将每张图像转换2次以获得这些图像的增强视图（现在有2*N张图像）。然后，你将这些2
    * N张图像通过一个模型，得到每张图像的嵌入向量。现在，你希望使同一图像的2个增强视图（一个正样本对）的嵌入向量在嵌入空间中更接近（对所有其他正样本对也做同样的处理）。一种测量两个向量相似度（接近，相同方向）的方法是使用**余弦相似度**，它被定义为sim(u,
    v)（请参见上图的定义）。
- en: 'In simple terms, what the formula is describing is that for each item in our
    batch, which is the embedding of one of the augmented views of an image, (Remember:
    the batch contains all the embeddings of the augmented views of different images
    → if starting w/ N images, the batch has a size of 2*N), we first find the embedding
    of the other augmented view of that image to make a positive pair. Then, we calculate
    the cosine similarity of these two embeddings and exponentiate it (the **numerator**
    of the formula). Then, we calculate the exponentiate of the cosine similarity
    of all the other pairs we can build with our first embedding vector with which
    we started (except for the pair with itself, this is what that 1[k!=i] means in
    the formula), and we sum them up to build the **denominator**. We can now divide
    the numerator by denominator and take the natural Log of that and flip the sign!
    Now, we have the loss of the first item in our batch. We need to just repeat the
    same process for all the other items in the batch and then take the average to
    be able to call *.backward()* method of PyTorch to calculate the gradients.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，公式描述的是，对于我们批次中的每个项目，即图像的一个增强视图的嵌入，（记住：批次包含不同图像的所有增强视图的嵌入→如果从N张图像开始，批次大小为2*N），我们首先找到该图像的另一个增强视图的嵌入以形成一个正样本对。然后，我们计算这两个嵌入的余弦相似度并对其进行指数运算（公式的**分子**）。接着，我们计算与我们开始时的第一个嵌入向量构建的所有其他对的余弦相似度的指数运算（除了与自身的对，这就是公式中的1[k!=i]的含义），并将它们相加以构建**分母**。现在，我们可以将分子除以分母，取自然对数并翻转符号！现在，我们得到了批次中第一个项目的损失。我们只需对批次中的所有其他项目重复相同的过程，然后取平均值，以便调用PyTorch的*.backward()*方法来计算梯度。
- en: 'Step 2: Implementing it using simple Python code, with naive “for” loops!'
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第2步：使用简单的Python代码实现，采用幼稚的“for”循环！
- en: Simple Pythonic implementation, using slow “for” loops
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 使用慢速“for”循环的简单Python实现
- en: 'Let’s go over the code. Let’s say we have two images: A and B. The variable
    aug_views_1 holds the embeddings (each with size 3) of one augmented view of these
    two images (A1 and B1), same as aug_views_2 (A2 and B2); so, the first item in
    both matrixes are related to image A and the second of item of the both is related
    to image B. We concatenate the two matrixes into the *projections* matrix (which
    has 4 vectors in it: A1, B1, A2, B2).'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一下代码。假设我们有两张图像：A和B。变量aug_views_1保存了这两张图像的一个增强视图（A1和B1）的嵌入（每个大小为3），与aug_views_2（A2和B2）相同；因此，两个矩阵中的第一个项目与图像A相关，第二个项目与图像B相关。我们将这两个矩阵拼接成*projections*矩阵（其中包含4个向量：A1，B1，A2，B2）。
- en: To keep the relation of the vectors in the projections matrix, we define *pos_pairs*
    dictionary to store which two items are related in the concatenated matrix. (soon
    I’ll explain the *F.normalize()* thing!)
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 为了保持投影矩阵中向量的关系，我们定义了*pos_pairs*字典来存储在拼接矩阵中哪些两个项目是相关的。（稍后我会解释*F.normalize()*的事！）
- en: As you see in the next lines of code, I’m going over the items in the projections
    matrix in a ***for*** loop, I find the related vector of that using our dictionary
    and then I calculate the cosine similarity. You might wonder why you do not divide
    by the size of the vectors, as the cosine similarity formula suggests. The point
    is that before starting the loop, using the F.normalize function, I’m normalizing
    all the vectors in our projection matrix to have the size of 1\. So, there’s no
    need to divide by the size in the line where we’re calculating the cosine similarity.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你在接下来的代码行中看到的，我在一个***for*** 循环中遍历投影矩阵中的项，使用我们的字典找到相关向量，然后计算余弦相似度。你可能会想为什么不按照余弦相似度公式除以向量的大小。关键是，在开始循环之前，使用
    F.normalize 函数，我将投影矩阵中的所有向量标准化为大小为 1。因此，在计算余弦相似度的那一行不需要除以大小。
- en: After building our numerator, I’m finding all the other indexes of vectors in
    the batch (except for the same index *i*), to calculate the cosine similarities
    consisting the denominator. Finally, I’m calculating the loss by dividing the
    numerator by denominator and applying the log function and flipping the sign.
    Make sure to play with the code to understand what is happening in each line.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在构建好分子后，我会找到批次中所有其他向量的索引（除了相同的索引 *i*），以计算包含分母的余弦相似度。最后，我通过将分子除以分母，并应用对数函数和翻转符号来计算损失。确保玩转代码以理解每一行的作用。
- en: 'Step 3: Converting it into efficient matrix-friendly PyTorch code'
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第 3 步：将其转换为高效的矩阵友好的 PyTorch 代码
- en: The problem with the previous python implementation is that **it’s too slow**
    to be used in our training pipeline; we need to get rid of the slow “for” loops
    and convert it into matrix multiplications and array manipulations in order to
    leverage the parallelization power.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的 Python 实现的问题是**太慢**，无法用于我们的训练流程；我们需要摆脱缓慢的“for”循环，并将其转换为矩阵乘法和数组操作，以利用并行化的优势。
- en: PyTorch implementation
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch 实现
- en: Let’s see what’s happening in this code snippet. This time, I’ve introduced
    the *labels_1* and *labels_2* tensors to encode the arbitrary classes to which
    these images belong, as we need a way to encode the relationship of A1, A2 and
    B1, B2 images. It does not matter if you choose labels 0 and 1 (as I did) or say
    5 and 8.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看这个代码片段发生了什么。这一次，我引入了 *labels_1* 和 *labels_2* 张量来编码这些图像所属的任意类别，因为我们需要一种方法来编码
    A1、A2 和 B1、B2 图像之间的关系。你选择标签 0 和 1（就像我做的）还是 5 和 8 都无所谓。
- en: After concatenating both the embeddings and labels, we start by creating a *sim_matrix*
    containing the cosine similarity of **all the possible pairs**.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在连接了所有的嵌入和标签后，我们首先创建一个包含**所有可能配对**的余弦相似度的 *sim_matrix*。
- en: '![](../Images/471741345f7ffa01e5487623bfae95f4.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/471741345f7ffa01e5487623bfae95f4.png)'
- en: 'How the sim_matrix looks like: the green cells contain our positive pairs,
    the orange cells are the pairs which need to be ignored in the denominator | Visualization
    by the author'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: sim_matrix 的样子：绿色单元格包含我们的正样本对，橙色单元格是需要在分母中忽略的配对 | 作者提供的可视化
- en: 'The visualization above is all you need :) to understand how the code’s working
    and why we’re doing the steps in there. Considering the first row of the sim_matrix,
    we can calculate the loss for the first item in the batch (A1) as follows: we
    need to divide A1A2 (exponentiated) by the sum of A1B1, A1A2, and A1B2 (each exponentiated
    first) and keep the result in the first item of a tensor storing all the losses.
    So, we need to first make a mask to find the green cells in the visualization
    above. The two lines in the code defining the variable *mask* do exactly this.
    The numerator is calculated by multiplying our sim_matrix by the mask we just
    created, and then summing the items of each row (after masking, there will be
    only one non-zero item in each row; i.e. the green cells). For calculating the
    denominator, we need to sum over each row, ignoring the orange cells on the diagonal.
    To do so, we’ll use the *.diag()* method of PyTorch tensors. The rest is self-explanatory!'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的可视化图是你理解代码如何工作的全部所需 :) 以及为什么我们要进行其中的步骤。考虑到 sim_matrix 的第一行，我们可以按如下方式计算批次中第一个项目
    (A1) 的损失：我们需要将 A1A2（指数化）除以 A1B1、A1A2 和 A1B2（每个都先指数化）的总和，并将结果保存在存储所有损失的张量的第一个项目中。因此，我们需要首先制作一个掩码，以找到上面可视化图中的绿色单元格。代码中定义变量*mask*的两行正是做这件事。分子是通过将我们的
    sim_matrix 乘以刚创建的掩码来计算的，然后对每行的项目求和（掩码后，每行将只有一个非零项目；即绿色单元格）。为了计算分母，我们需要在每行上求和，忽略对角线上的橙色单元格。为此，我们将使用
    PyTorch 张量的*.diag()*方法。其余部分不言自明！
- en: 'Bonus: Using AI assistants (ChatGPT, Copilot, …) to implement the formula'
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 额外内容：使用 AI 助手（ChatGPT、Copilot 等）来实现公式
- en: We’ve great tools in our disposal to help us understand and implement the math
    in deep learning papers. For example, you can ask ChatGPT (or other similar tools)
    to implement the code in PyTorch after giving it the formula from the paper. In
    my experience, ChatGPT can be most helpful and provide the best final answers
    in less trail and errors if you can get yourself somehow to the *pythonic-for-loop*
    implementation step. Give that naive implementation to ChatGPT and ask it to convert
    it into efficient PyTorch code which uses matrix multiplications and tensor manipulations
    only; you’ll be surprised by the answer :)
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有很棒的工具可以帮助我们理解和实现深度学习论文中的数学。例如，你可以在给出论文中的公式后，要求 ChatGPT（或其他类似工具）用 PyTorch
    实现代码。根据我的经验，如果你能在*python-for-loop*实现步骤中找到自己，ChatGPT 最能提供最好的最终答案，并减少试错次数。把那个初步实现交给
    ChatGPT，要求它将其转换为仅使用矩阵乘法和张量操作的高效 PyTorch 代码；你会对答案感到惊讶 :)
- en: Further Reading
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: I encourage you to check out the following two great implementations of the
    same idea to learn how you can extend this implementation into considering more
    nuanced situations, like in **supervised contrastive learning** setting.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 我鼓励你查看以下两个相同理念的优秀实现，以了解如何将这一实现扩展到更微妙的情况中，比如在**监督对比学习**设置中。
- en: '[Supervised Contrastive Loss, by Guillaume Erhard](https://github.com/GuillaumeErhard/Supervised_contrastive_loss_pytorch/blob/main/loss/spc.py)'
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[监督对比损失，由 Guillaume Erhard 编写](https://github.com/GuillaumeErhard/Supervised_contrastive_loss_pytorch/blob/main/loss/spc.py)'
- en: '[SupContrast, by Yonglong Tian](https://github.com/HobbitLong/SupContrast/blob/master/losses.py)'
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[SupContrast，由 Yonglong Tian 编写](https://github.com/HobbitLong/SupContrast/blob/master/losses.py)'
- en: About Me
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 关于我
- en: I’m Moein Shariatnia, a machine learning developer and medical student, focusing
    on using deep learning solutions for medical imaging applications. My research
    is mostly on investigating the generalizability of deep models under various circumstances.
    Feel free to reach out to me, via Email, Twitter, or LinkedIn.
  id: totrans-49
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 我是 Moein Shariatnia，一名机器学习开发者和医学学生，专注于使用深度学习解决方案进行医学影像应用。我的研究主要集中在研究深度模型在各种情况下的泛化能力。欢迎通过电子邮件、Twitter
    或 LinkedIn 与我联系。
