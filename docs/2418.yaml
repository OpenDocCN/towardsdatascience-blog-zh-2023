- en: Elegant prompt versioning and LLM model configuration with spacy-llm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/elegant-prompt-versioning-and-llm-model-configuration-with-spacy-llm-126b836daad1?source=collection_archive---------3-----------------------#2023-07-26](https://towardsdatascience.com/elegant-prompt-versioning-and-llm-model-configuration-with-spacy-llm-126b836daad1?source=collection_archive---------3-----------------------#2023-07-26)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Using spacy-llm to simplify prompt management and create tasks for data extraction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@dehhmesquita?source=post_page-----126b836daad1--------------------------------)[![Déborah
    Mesquita](../Images/3b77b7eb569e24f2679875429173daf1.png)](https://medium.com/@dehhmesquita?source=post_page-----126b836daad1--------------------------------)[](https://towardsdatascience.com/?source=post_page-----126b836daad1--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----126b836daad1--------------------------------)
    [Déborah Mesquita](https://medium.com/@dehhmesquita?source=post_page-----126b836daad1--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fdd9e06a0a640&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Felegant-prompt-versioning-and-llm-model-configuration-with-spacy-llm-126b836daad1&user=D%C3%A9borah+Mesquita&userId=dd9e06a0a640&source=post_page-dd9e06a0a640----126b836daad1---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----126b836daad1--------------------------------)
    ·5 min read·Jul 26, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F126b836daad1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Felegant-prompt-versioning-and-llm-model-configuration-with-spacy-llm-126b836daad1&user=D%C3%A9borah+Mesquita&userId=dd9e06a0a640&source=-----126b836daad1---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F126b836daad1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Felegant-prompt-versioning-and-llm-model-configuration-with-spacy-llm-126b836daad1&source=-----126b836daad1---------------------bookmark_footer-----------)![](../Images/64fa1ffb2743af291cb93acc5f61d321.png)'
  prefs: []
  type: TYPE_NORMAL
- en: A [tidy desk](https://unsplash.com/pt-br/fotografias/zCQsBI7ZltQ), how you code
    will look like if you use spacy-llm haha
  prefs: []
  type: TYPE_NORMAL
- en: Managing prompts and handling OpenAI request failures can be a challenging task.
    Fortunately, spaCy released [spacy-llm](https://github.com/explosion/spacy-llm),
    a powerful tool that simplifies prompt management and eliminates the need to create
    a custom solution from scratch.
  prefs: []
  type: TYPE_NORMAL
- en: In this article, you will learn how to leverage spacy-llm to create a task that
    extracts data from text using a prompt. We will dive into the basics of spacy
    and explore some of the features of spacy-llm.
  prefs: []
  type: TYPE_NORMAL
- en: spaCy and spacy-llm 101
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: spaCy is a library for advanced NLP in Python and Cython. When dealing with
    text data, several processing steps are typically required, such as tokenization
    and POS tagging. In order to execute these steps, spaCy provides the `nlp` method,
    which invokes a processing pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: spaCy v3.0 introduces `config.cfg`, a file where we can include detailed settings
    of these pipelines.
  prefs: []
  type: TYPE_NORMAL
- en: '`config.cfg` uses [confection](https://github.com/explosion/confection), a
    config system which allows the creation of arbitrary object trees. For instance,
    confection parsers the following `config.cfg`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'into:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Each pipeline use components, and spacy-llm stores the pipeline components
    into registries using [catalogue](https://github.com/explosion/catalogue). This
    library, also from Explosion, introduces function registries that allow for efficient
    management of the components. A `llm`component is defined into [two main settings](https://github.com/explosion/spacy-llm/tree/a3ee82eae366d101d37afd055606c9ead0186251#-api):'
  prefs: []
  type: TYPE_NORMAL
- en: A [**task**](https://github.com/explosion/spacy-llm/tree/a3ee82eae366d101d37afd055606c9ead0186251#tasks),
    defining the prompt to send to the LLM as well as the functionality to parse the
    resulting response
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A [**model**](https://github.com/explosion/spacy-llm/tree/a3ee82eae366d101d37afd055606c9ead0186251#models),
    defining the model and how to connect to it
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To include a component that uses a LLM in our pipeline, we need to follow a
    few steps. First, we need to create a task and register it into the registry.
    Next, we can use a model to execute the prompt and retrieve the responses. Now
    it’s time to do all that so we can run the pipeline
  prefs: []
  type: TYPE_NORMAL
- en: Creating a task to extract data from text
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will use quotes from [https://dummyjson.com/](https://dummyjson.com/quotes)
    and create a task to extract the context from every quote. We will create the
    prompt, register the task and finally create the config file.
  prefs: []
  type: TYPE_NORMAL
- en: '**1\. The prompt**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'spacy-llm uses Jinja templates to define the instructions and examples. The
    `{{ text }}` will be replaced by the quote we will provide. This is our prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '**2\. The task class**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now let’s create the class for the task. The class should implement two functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '`**generate_prompts(docs: Iterable[Doc]) -> Iterable[str]**`: a function that
    takes in a list of spaCy `[Doc](https://spacy.io/api/doc)` objects and transforms
    them into a list of prompts'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`**parse_responses(docs: Iterable[Doc], responses: Iterable[str]) -> Iterable[Doc]**`:
    a function for parsing the LLM''s outputs into spaCy `[Doc](https://spacy.io/api/doc)`
    objects'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`**generate_prompts**` will use our Jinja template and `**parse_responses**`
    will add the attribute context to our Doc. This is the `QuoteContextExtractTask`
    class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we just need to add the task to the spacy-llm `llm_tasks` register:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '**3\. The config.cfg file**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We’ll use the GPT-3.5 model from OpenAI. spacy-llm has a model for that so
    we just need to make sure the secret key is available as an environmental variable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'To build the `nlp` method that runs the pipeline we’ll use the `assemble` method
    from spacy-llm. This methods reads from a `.cfg` file. The file should reference
    the GPT-3.5 model (it’s already in he registry) and the task we’ve created:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '**4\. Running the pipeline**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now we just need to put everything together and run the code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'And run:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: If you want to change the prompt, just create another Jinja file and create
    a `my_namespace.QuoteContextExtractTask.v2` task the same way we’ve created the
    first one. If you want to change the temperature, just change the parameter on
    the `config.cfg` file. Nice, right?
  prefs: []
  type: TYPE_NORMAL
- en: Final thoughts
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The ability to handle OpenAI REST requests and its straightforward approach
    to storing and versioning prompts are my favorite things about spacy-llm. Additionally,
    the library offers a Cache for caching prompts and responses per document, a method
    for providing examples for few-shot prompts, and a logging feature, among other
    things.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can take a look at the entire code from today here: [https://github.com/dmesquita/spacy-llm-elegant-prompt-versioning](https://github.com/dmesquita/spacy-llm-elegant-prompt-versioning).'
  prefs: []
  type: TYPE_NORMAL
- en: As always, thank you for reading!
  prefs: []
  type: TYPE_NORMAL
