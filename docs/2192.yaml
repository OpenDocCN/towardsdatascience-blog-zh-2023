- en: Matrix and Vector Operations in Logistic Regression
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/matrix-and-vector-operations-in-logistic-regression-e35714c4810f?source=collection_archive---------8-----------------------#2023-07-07](https://towardsdatascience.com/matrix-and-vector-operations-in-logistic-regression-e35714c4810f?source=collection_archive---------8-----------------------#2023-07-07)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Vectorized Logistic Regression
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://murali-kashaboina.medium.com/?source=post_page-----e35714c4810f--------------------------------)[![Murali
    Kashaboina](../Images/ff1118f3c317dab87fe4b625a614fb93.png)](https://murali-kashaboina.medium.com/?source=post_page-----e35714c4810f--------------------------------)[](https://towardsdatascience.com/?source=post_page-----e35714c4810f--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----e35714c4810f--------------------------------)
    [Murali Kashaboina](https://murali-kashaboina.medium.com/?source=post_page-----e35714c4810f--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F2d02cbc7d153&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmatrix-and-vector-operations-in-logistic-regression-e35714c4810f&user=Murali+Kashaboina&userId=2d02cbc7d153&source=post_page-2d02cbc7d153----e35714c4810f---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----e35714c4810f--------------------------------)
    ·10 min read·Jul 7, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fe35714c4810f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmatrix-and-vector-operations-in-logistic-regression-e35714c4810f&user=Murali+Kashaboina&userId=2d02cbc7d153&source=-----e35714c4810f---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fe35714c4810f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmatrix-and-vector-operations-in-logistic-regression-e35714c4810f&source=-----e35714c4810f---------------------bookmark_footer-----------)![](../Images/28c90d52b5f07dc1c15a0dd6b6832cf0.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: Photo by [fabio](https://unsplash.com/@fabioha?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: The underlying math behind any Artificial Neural Network (ANN) algorithm can
    be overwhelming to understand. Moreover, the matrix and vector operations used
    to represent feed-forward and back-propagation computations during batch training
    of the model can add to the comprehension overload. While succinct matrix and
    vector notations make sense, peeling through such notations down to subtle working
    details of such matrix operations would bring more clarity. I realized that the
    best way to understand such subtle details is to consider a bare minimum network
    model. I could not find a better algorithm than Logistic Regression to explore
    what goes under the hood because it has all the bells and whistles of ANN, such
    as multidimensional inputs, the network weights, the bias, forward propagation
    operations, activations that apply non-linear function, loss function, and gradients-based
    back-propagation. My intent for this blog is to share my notes and findings of
    the matrix and vector operations that are core to Logistic Regression model.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: Brief Synopsis of Logistic Regression
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Despite its name, Logistic Regression is a classification algorithm and not
    a regression algorithm. Typically it is used for binary classification to predict
    the probability of an instance belonging to one of two classes, for example, predicting
    if an email is spam or not. As such, in Logistic Regression, the dependent or
    target variable is considered a categorical variable. For example, an email being
    spam is represented as 1 and not spam as 0\. The primary goal of the Logistic
    Regression model is to establish a relationship between the input variables (features)
    and the probability of the target variable. For example, given the characteristics
    of an email as a set of input features, a Logistic Regression model would find
    a relationship between such features and the probability of the email being spam.
    If ‘Y’ represents the output class, such as an email being spam, ‘X’ represents
    the input features, the probability can be designated as π = Pr( Y = 1 | X, βi),
    where βi represents the logistic regression parameters that include model weights
    ‘*wi*’ and a bias parameter ‘b’. Effectively, a Logistic Regression predicts the
    probability of Y = 1 given the input features and the model parameters. Specifically,
    the probability π is modeled as an S-Shaped logistic function called the Sigmoid
    function, given by π = e^z/(1 + e^z) or equivalently by π = 1/(1 + e^-z), where
    z = βi . X. The sigmoid function allows for a smooth curve bounded between 0 and
    1, making it suitable for estimating probabilities. Essentially, a Logistic Regression
    model applies the sigmoid function on a linear combination of the input features
    to predict a probability between 0 and 1\. A common approach to determining an
    instance’s output class is thresholding the predicted probability. For example,
    if the predicted probability is greater than or equal to 0.5, the instance is
    classified as belonging to class 1; otherwise, it is classified as class 0.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管名字里有“回归”，逻辑回归实际上是一种分类算法，而不是回归算法。它通常用于二分类任务，以预测某个实例属于两个类别之一的概率，例如，预测一封电子邮件是否是垃圾邮件。因此，在逻辑回归中，因变量或目标变量被视为分类变量。例如，垃圾邮件用1表示，而非垃圾邮件用0表示。逻辑回归模型的主要目标是建立输入变量（特征）与目标变量概率之间的关系。例如，给定一封电子邮件的特征作为输入特征集合，逻辑回归模型会找到这些特征与电子邮件是垃圾邮件的概率之间的关系。如果‘Y’表示输出类别，比如电子邮件是垃圾邮件，‘X’表示输入特征，则概率可以表示为
    π = Pr( Y = 1 | X, βi)，其中 βi 表示包括模型权重‘*wi*’和偏置参数‘b’在内的逻辑回归参数。实际上，逻辑回归预测给定输入特征和模型参数下
    Y = 1 的概率。具体来说，概率 π 被建模为一个 S 形的逻辑函数，称为 Sigmoid 函数，公式为 π = e^z/(1 + e^z) 或等效地 π
    = 1/(1 + e^-z)，其中 z = βi . X。Sigmoid 函数允许在0和1之间平滑曲线，非常适合于估计概率。本质上，逻辑回归模型在输入特征的线性组合上应用
    Sigmoid 函数，以预测0和1之间的概率。确定实例输出类别的常见方法是对预测概率进行阈值处理。例如，如果预测概率大于或等于0.5，则该实例被分类为类别1；否则，分类为类别0。
- en: '![](../Images/b54db4b515dc708bb21e7fd64b96dcfb.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b54db4b515dc708bb21e7fd64b96dcfb.png)'
- en: A Logistic Regression Model Schematic — Image Created by the Author
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑回归模型示意图 — 由作者创建
- en: 'A Logistic Regression model is trained by fitting the model to the training
    data and then minimizing a loss function to adjust the model parameters. A loss
    function estimates the difference between the predicted and actual probabilities
    of the output class. The most common loss function used in training a Logistic
    Regression model is the Log Loss function, also known as Binary Cross Entropy
    Loss function. The formula for the Log Loss function is as follows:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑回归模型通过将模型拟合到训练数据上来训练，然后通过最小化损失函数来调整模型参数。损失函数估计预测概率与实际概率之间的差异。用于训练逻辑回归模型的最常见损失函数是对数损失函数，也称为二元交叉熵损失函数。对数损失函数的公式如下：
- en: L = — ( y * ln(p) + (1 — y) * ln(1 — p) )
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: L = — ( y * ln(p) + (1 — y) * ln(1 — p) )
- en: 'Where:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 其中：
- en: L represents the Log Loss.
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: L 代表对数损失。
- en: y is the ground-truth binary label (0 or 1).
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: y 是实际的二元标签（0 或 1）。
- en: p is the predicted probability of the output class.
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: p 是输出类别的预测概率。
- en: A Logistic Regression model adjusts its parameters by minimizing the loss function
    using techniques such as gradient descent. Given a batch of input features and
    their ground-truth class labels, training of the model is carried out in several
    iterations, called epochs. In each epoch, the model carries forward propagation
    operations to estimate losses and backward propagation operations to minimize
    the loss function and adjust the parameters. All such operations in an epoch employ
    matrix and vector computations as illustrated in the next sections.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: Matrix and Vector Notations
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '***Please note that*** ***I used LaTeX scripts to create the mathematical equations
    and matrix/vector representations embedded as images in this blog***. If anyone
    is interested in the LaTeX scripts, don’t hesitate to contact me; I will be happy
    to share.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: As shown in the schematic diagram above, a binary Logistic Regression classifier
    is used as an example to keep the illustrations simple. As shown below, a matrix
    X represents the ‘m’ number of input instances. Each input instance comprises
    an ’n’ number of features and is represented as a column, an input features vector,
    within the matrix X, making it a (n x m) sized matrix. The super-script (i) represents
    the ordinal number of the input vector in the matrix X. The sub-script ‘j’ represents
    the ordinal index of the feature within an input vector. The matrix Y of size
    (1 x m) captures the ground-truth labels corresponding to each input vector in
    the matrix X. The model weights are represented by a column vector W of size (n
    x 1) comprising ’n’ weight parameters corresponding to each feature in the input
    vector. While there is only one bias parameter ‘b’, for illustrating matrix/vector
    operations, a matrix B of size (1 x m) comprising ‘m’ number of the same bias
    b parameter is considered.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8dc491fa91410a44f21b226d5071c1a1.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
- en: Forward Propagation
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The first step in the forward propagation operation is to compute a linear
    combination of model parameters and input features. The notation for such matrix
    operation is shown below where a new matrix Z is evaluated:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2b585602eac790182831465b1353ad1a.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
- en: 'Note the use of the transpose of weight matrix W. The above operation in the
    matrix expanded representation is as follows:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/996d0df64ffcda4a7f145cded84df8be.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
- en: 'The above matrix operation results in the computation of matrix Z of size (1
    x m) as shown below:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/64c31281cd6dc7a89b44d97669bedcd8.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
- en: The next step is to derive activations by applying the sigmoid function on the
    computed linear combinations for each input as shown in the following matrix operation.
    This results in an activation matrix A of size (1 x m).
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a632cfef5c85e0507f38fc6664521332.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
- en: Backward Propagation
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Backward propagation or back-propagation is a technique to compute the contributions
    of each parameter to the overall error or loss caused by incorrect predictions
    at the end of each epoch. The individual loss contributions are evaluated by computing
    the gradients of the loss function with respect to (w.r.t) each model parameter.
    A gradient or derivative of a function is the rate of change or the slope of that
    function w.r.t a parameter considering other parameters as constants. When evaluated
    for a specific parameter value or point, the sign of the gradient indicates in
    which direction the function increases, and the gradient magnitude indicates the
    steepness of the slope. The log loss function as shown below is a bowl-shaped
    convex function with one global minimum point. As such, in most cases, the gradient
    of the log loss function w.r.t a parameter points in the opposite direction to
    the global minima. Once gradients are evaluated, each parameter value is updated
    using the parameter’s gradient, typically by using a technique called gradient
    descent.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2b6c87bb5f875c07583c4520046798d4.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
- en: The gradient for each parameter is computed using the chain rule. The chain
    rule enables the computation of derivatives of functions that are composed of
    other functions. In the case of Logistic Regression, the log loss L is a function
    of activation ‘a’ and ground-truth label ‘y’, while ‘a’ itself is a sigmoid function
    of ‘z’ and ‘z’ is a linear function of weights ‘w’ and bias ‘b’ implying that
    the loss function L is a function composed of other functions as shown below.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d18cc74dc6680d666b87836fe816a93e.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
- en: 'Using the chain rule of partial derivatives, the gradients of weight and bias
    parameters can be computed as follows:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/08b68c624cd2e55cc919baa9ae1285c5.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
- en: '**Derivation of Gradients for Single Input Instance**'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: Before we review the matrix and vector representations that come into play as
    part of updating the parameters in one shot, we will first derive the gradients
    using a single input instance to understand the basis for such representations
    better.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: Assuming that ‘a’ and ‘z’ represent computed values for a single input instance
    with the ground-truth label ‘y’, the gradient of the loss function w.r.t ‘a’ can
    be derived as follows. Note that this gradient is the first quantity required
    to evaluate the chain rule to derive parameter gradients later.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1a5ef1c9ec58b70cf69d43b665b045d9.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
- en: 'Given the gradient of loss function w.r.t ‘a’, the gradient of loss function
    w.r.t ‘z’ can be derived using the following chain rule:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c933a583975437fa2b718414c6adb48d.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
- en: 'The above chain rule implies that the gradient of ‘a’ w.r.t ‘z’ must also be
    derived. Note that ‘a’ is computed by applying the sigmoid function on ‘z’. Therefore,
    the gradient of ‘a’ w.r.t ‘z’ can be derived by using the sigmoid function expression
    as follows:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/745ffa7d390b9b867fbf99a305118d0f.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
- en: 'The above derivation is expressed in terms of ‘e’, and it appears that additional
    computations are needed to evaluate the gradient of ‘a’ w.r.t ‘z’. We know that
    ‘a’ gets computed as part of forward propagation. Therefore to eliminate any additional
    computations, the above derivative can be fully expressed in terms of ‘a’ instead
    as follows:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/79d9fdac00d680ce8c2d30a24e4c63f4.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
- en: 'Plugging in the above terms expressed in ‘a’, the gradient of ‘a’ w.r.t ‘z’
    is as follows:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/947d2e4cb99ed35cab7c6745a9881945.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
- en: 'Now that we have the gradient of loss function w.r.t ‘a’ and the gradient of
    ‘a’ w.r.t ‘z’, the gradient of loss function w.r.t ‘z’ can be evaluated as follows:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/88780a8cdf6de1acae26aac7d8bcbc8a.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
- en: 'We came a long way in evaluating the gradient of loss function w.r.t ‘z’. We
    still need to evaluate the gradients of loss function w.r.t model parameters.
    We know that ‘z’ is a linear combination of model parameters and features of an
    input instance ‘x’ as shown below:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d9e3562dd7682af65faee75cc4ba3fae.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
- en: 'Using the chain rule the gradient of loss function w.r.t weight parameter ‘wi’
    gets evaluated as shown below:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ca91815d745cb0ec89622252206cdcd5.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
- en: 'Similarly, the gradient of the loss function w.r.t ‘b’ gets evaluated as follows:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d534432ca71074eafd4a8c363b73ae58.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
- en: '**Matrix and Vector Representation of Parameter Updates using Gradients**'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we understand gradient formulas for model parameters derived using
    a single input instance, we can represent the formulas in matrix and vector forms
    accounting for the entire training batch. We will first vectorize gradients of
    the loss function w.r.t ‘z’ given by the following expression:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/25adb16335677d0af11dd642241580c4.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
- en: 'The vector form of the above for the entire ‘m’ number of instances is:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7a29beb97558bb18cb4f9c528acd4f05.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
- en: 'Similarly, the gradients of the loss function w.r.t each weight ‘wi’ can be
    vectorized. The gradient of the loss function w.r.t weight ‘wi’ for a single instance
    is given by:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/edffa69ddf2006375955e7bb552351cb.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
- en: 'The vector form of the above for all weights across all ‘m’ input instances
    is evaluated as the mean of ‘m’ gradients as follows:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d1e8cd1878a049bd2fe28fa8e606fa7b.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
- en: 'Similarly, the resultant gradient of loss function w.r.t ‘b’ across all ‘m’
    input instances is computed as a mean of the individual instance gradients as
    follows:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b56b93f539efe842c6c8d05ddea7b2d7.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
- en: Given the model weights gradient vector and the overall gradient for bias, the
    model parameters get updated as follows. The parameter updates as shown below
    are based on the technique called gradient descent where a learning rate is used.
    A learning rate is a hyper-parameter used in optimization techniques such as gradient
    descent to control the step size of adjustments made at each epoch to the model
    parameters based on computed gradients. Effectively, a learning rate acts as a
    scaling factor, influencing the speed and convergence of the optimization algorithm.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 给定模型权重梯度向量和偏置的整体梯度，模型参数将按以下方式更新。如下所示的参数更新基于称为梯度下降的技术，其中使用了学习率。学习率是优化技术（如梯度下降）中使用的超参数，用于控制每次迭代时对模型参数进行调整的步长。有效地说，学习率充当缩放因子，影响优化算法的速度和收敛性。
- en: '![](../Images/e9a533630ad1f2321ee5231b4b956290.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e9a533630ad1f2321ee5231b4b956290.png)'
- en: Conclusion
  id: totrans-74
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结论
- en: As evident from the matrix and vector representations illustrated in this blog,
    Logistic Regression enables a bare minimum network model to understand the subtle
    working details of such matrix and vector operations. Most machine-learning libraries
    encapsulate such nitty-gritty mathematical details but instead expose well-defined
    programming interfaces at a higher level, such as forward or backward propagation.
    While understanding all such subtle details may not be required to develop models
    using such libraries, such details do shed light on the mathematical intuitions
    behind such algorithms. However, such understanding will certainly help carry
    forward the underlying mathematical intuitions to other models such as ANN, Recurrent
    Neural Networks (RNN), Convolutional Neural Networks (CNN), and Generative Adversarial
    Networks (GAN).
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 从本博客中说明的矩阵和向量表示可以看出，逻辑回归使得一个基本的网络模型能够理解这些矩阵和向量操作的细微工作细节。大多数机器学习库封装了这些琐碎的数学细节，但却在更高层次上暴露了定义良好的编程接口，如前向传播或反向传播。虽然理解所有这些细微的细节可能不是使用这些库开发模型的必要条件，但这些细节确实揭示了这些算法背后的数学直觉。然而，这种理解肯定有助于将底层数学直觉应用到其他模型，如人工神经网络（ANN）、递归神经网络（RNN）、卷积神经网络（CNN）和生成对抗网络（GAN）。
