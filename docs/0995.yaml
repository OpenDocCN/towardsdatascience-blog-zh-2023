- en: Embracing Automated Retraining
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/embracing-automated-retraining-780ed49f9985?source=collection_archive---------4-----------------------#2023-03-17](https://towardsdatascience.com/embracing-automated-retraining-780ed49f9985?source=collection_archive---------4-----------------------#2023-03-17)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/3a1dab9fab5bcfe38d0a7a6f09c22394.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: How to move away from retraining at a set cadence (or not at all) in favor of
    a dynamic approach
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://statistician-in-stilettos.medium.com/?source=post_page-----780ed49f9985--------------------------------)[![Claire
    Longo](../Images/5a04940feeba1412688b4f38ec1fe974.png)](https://statistician-in-stilettos.medium.com/?source=post_page-----780ed49f9985--------------------------------)[](https://towardsdatascience.com/?source=post_page-----780ed49f9985--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----780ed49f9985--------------------------------)
    [Claire Longo](https://statistician-in-stilettos.medium.com/?source=post_page-----780ed49f9985--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F1f6936fe85bb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fembracing-automated-retraining-780ed49f9985&user=Claire+Longo&userId=1f6936fe85bb&source=post_page-1f6936fe85bb----780ed49f9985---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----780ed49f9985--------------------------------)
    ·7 min read·Mar 17, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F780ed49f9985&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fembracing-automated-retraining-780ed49f9985&user=Claire+Longo&userId=1f6936fe85bb&source=-----780ed49f9985---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F780ed49f9985&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fembracing-automated-retraining-780ed49f9985&source=-----780ed49f9985---------------------bookmark_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: '*This piece was co-authored by* [*Trevor LaViale*](https://www.linkedin.com/in/trevor-laviale/)'
  prefs: []
  type: TYPE_NORMAL
- en: While the industry has invested a lot in processes and techniques for knowing
    when to deploy a model into production, there is arguably less collective knowledge
    on the equally important task of knowing when to retrain a model. In truth, knowing
    when to retrain a model is hard due to factors like delays in feedback or labels
    for live predictions. In practice, many models are in production with no retraining
    at all, use manual retraining methods, or are retraining without optimizing or
    studying the cadence.
  prefs: []
  type: TYPE_NORMAL
- en: This post is written to help data scientists and machine learning engineering
    teams embrace automated retraining.
  prefs: []
  type: TYPE_NORMAL
- en: Approaches for Retraining
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There are two core approaches to automated model retraining:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Fixed:** Retraining a set cadence (e.g., daily, weekly, monthly)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Dynamic:** Ad-hoc triggered retraining based on model performance metrics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: While the fixed approach is straightforward to implement, there are some drawbacks.
    Compute costs can be higher than necessary, and the frequent retraining can lead
    to inconsistencies from one model to another, while infrequent retraining schedules
    can lead to a stale model.
  prefs: []
  type: TYPE_NORMAL
- en: The dynamic approach can prevent models from going stale, and optimize the compute
    cost. While there are numerous approaches to retraining, here are some recommended
    best practices for dynamic model retraining that will keep models healthier and
    performant.
  prefs: []
  type: TYPE_NORMAL
- en: Generalized Retraining Architecture
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There is a suite of various tools that can be used to create a model retraining
    system. This diagram shows how an ML observability platform might integrate into
    a generalized flow.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are a wealth of tutorials for specific tooling. Here are a few:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Automated model retraining with Eventbridge](https://docs.arize.com/arize/monitors/alerts-and-integrations/amazon-eventbridge#leverage-arizes-drift-monitoring-capabilities-to-automate-ml-training-workflows)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Automated model retraining with Airflow](https://docs.arize.com/arize/monitors/alerts-and-integrations/airflow-retrain)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Pachyderm example for FinTech use-case](https://www.pachyderm.com/blog/leaping-the-gap-fintech/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For those ready to skip ahead, you can also take it a step further with [Etsy’s
    take](/building-a-lil-stateful-ml-application-for-online-learning-66624d62afae)
    on stateful model retraining.
  prefs: []
  type: TYPE_NORMAL
- en: Retraining Strategy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Automating the retraining of a live machine learning model can be a complex
    task, but there are some best practices that can help guide the design.
  prefs: []
  type: TYPE_NORMAL
- en: Metrics To Trigger Retraining
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The metrics used to trigger retraining will depend on the specific model and
    use-case. Each metric will need a threshold set. The threshold will be used to
    trigger retraining when the performance of the model falls below the threshold.
    This is where monitors can come into play. When a performance monitor fires in
    a model monitoring platform, you can then programmatically query the performance
    and drift metrics to evaluate whether retraining is needed.
  prefs: []
  type: TYPE_NORMAL
- en: 'Ideal metrics to trigger model retraining:'
  prefs: []
  type: TYPE_NORMAL
- en: Prediction (score or label) drift
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Performance metric degradation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Performance metric degradation for specific segments/cohorts.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Feature drift
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Embeddings drift
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Drift is the measure of the distance between two distributions. It is a meaningful
    metric for triggering model retraining because it indicates how much your production
    data has shifted from a baseline. Statistical drift can be measured with various
    [drift metrics](https://arize.com/blog-course/drift/).
  prefs: []
  type: TYPE_NORMAL
- en: The baseline dataset used to calculate drift can be derived from either the
    training dataset, or a window of production data.
  prefs: []
  type: TYPE_NORMAL
- en: Ensuring the New Model Is Working
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The new model will need to be tested or validated before promoting it to production
    to replace the old one. There are a few recommended approaches here:'
  prefs: []
  type: TYPE_NORMAL
- en: Human review
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Automated metric checks in CI/CD pipeline
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Strategy for Promoting the New Model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The strategy for promoting the new model will depend on the impact that the
    model has on the business. In some cases, it may be appropriate to automatically
    replace the old model with the new model. But in other cases, the new model may
    need to be A/B test live before replacing the old model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Some strategies for live model testing to consider are:'
  prefs: []
  type: TYPE_NORMAL
- en: '***Champion vs. Challenger*** — serve production traffic to both models but
    only use the prediction/response from the existing model (champion) in the application.
    The data from the challenger model is stored for analysis but not used.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '***A/B testing*** — split production traffic to both models for a fixed experimentation
    period. Compare key metrics at the end of the experiment and decide which model
    to promote.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '***Canary deployment*** — start by redirecting a small percentage of production
    traffic to the new model. Since it’s in a production path, this helps to catch
    real issues with the new model but limits the impact to a small percentage of
    users. Ramp up the traffic to the new model until the new model receives 100%
    of the traffic.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Retraining Feedback Loop Data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Once we identify that the model needs to be retained, the next step is to choose
    the right dataset to retrain with. Here are some recommendations to ensure the
    new training data will improve the models performance:'
  prefs: []
  type: TYPE_NORMAL
- en: If the model performs well overall, but is failing to meet optimal performance
    criteria on some segments, such as specific feature values or demographics, the
    new training dataset should contain extra data points for these lower performing
    segments. A simple upsampling strategy can be used to create a new training dataset
    that targets these low performing segments
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the model is trained on a small timeslice, the training dataset may not accurately
    capture and represent all possible patterns that will appear in the live production
    data. To prevent this, avoid training the model on recent data alone. Instead,
    use a large sample of historical data, and augment this with the latest data to
    add additional patterns for the model to learn from.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If your model architecture follows the transfer learning design, new data can
    simply be added to the model during retraining, without losing the patterns that
    the model has already learned from previous training data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dashboards from a model monitoring platform (i.e. Arize — full disclosure:
    I work for Arize) are great for tracking and comparing model live performance
    during these tests. Whether the model is tested as a shadow deploy, live A/B test,
    or simply an offline comparison, these dashboards offer a simple way to view a
    side by side model comparison. The dashboards can also easily be shared with others
    to demonstrate model performance improvements to stakeholders.'
  prefs: []
  type: TYPE_NORMAL
- en: Quantifying ROI
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Overall, it’s important to have a clear understanding of your business requirements
    and the problem you are trying to solve when determining the best approach for
    automating the retraining of a live machine learning model. It’s also important
    to continuously monitor the performance of the model and make adjustments to the
    retraining cadence and metrics as needed.
  prefs: []
  type: TYPE_NORMAL
- en: '**Measuring Cost impact:**'
  prefs: []
  type: TYPE_NORMAL
- en: Although it is challenging to calculate direct ROI for some tasks in AI, the
    value of optimized model retraining is simple, tangible, and possible to calculate
    directly. The compute and storage costs for model training jobs are often already
    tracked as part of cloud compute costs. Often, the business impact of a model
    can be calculated as well.
  prefs: []
  type: TYPE_NORMAL
- en: When optimizing retraining, we are considering both the retraining costs, and
    the impact of model performance to the business (“AI ROI”). We can weigh this
    cost against each other to justify the cost of model retraining.
  prefs: []
  type: TYPE_NORMAL
- en: Here, we propose a weekly cost calculation, although this calculator can be
    adapted to a different cadence such as daily or monthly depending on the model’s
    purpose and maintenance needs.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0ef37ed0567c1e990f412a04d8cac744.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: '**Consider scenario 1, a case where the model is retraining too frequently.**'
  prefs: []
  type: TYPE_NORMAL
- en: '*My model costs $200 to retrain. I train my model 1x per day. This model maintained
    a steady average weekly accuracy of 85%. I set up a pipeline to automatically
    retrain based on prediction score drift greater than 0.25 PSI and accuracy. Based
    on the new rule, my model starts retraining only twice a week, and maintains that
    accuracy of 85%.*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Comparison of weekly maintenance costs:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Old model maintenance cost: 7*$200 = $1400'
  prefs: []
  type: TYPE_NORMAL
- en: New model maintenance cost 2*$200= $400
  prefs: []
  type: TYPE_NORMAL
- en: That’s a x% reduction in model maintenance costs. Although this is a simple
    contrived example, the magnitude of cost savings can be on this scale.
  prefs: []
  type: TYPE_NORMAL
- en: '**Consider scenario 2, a case where the model is not retrained enough.**'
  prefs: []
  type: TYPE_NORMAL
- en: '*My model costs $200 to train. I train my model once per week. This model maintained
    a steady average weekly Accuracy of 65%. I set up a pipeline to automatically
    retrain based on prediction score drift greater than 0.25 PSI. Based on the new
    rule, my model retrains twice a week, and has achieved a better Accuracy of 85%.*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Comparison of weekly maintenance costs:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Old model maintenance cost: 1*$200 = $200 for 65% accuracy'
  prefs: []
  type: TYPE_NORMAL
- en: 'New model maintenance cost: 2*$200= $400 for 85% accuracy'
  prefs: []
  type: TYPE_NORMAL
- en: So for a higher price, better model performance has been achieved. This can
    be justified and profitable if the AI ROI values are higher than the retraining
    costs. Lack of frequent retraining could have been leaving money on the table.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Transitioning from model retraining at fixed intervals to automated model retraining
    triggered by model performance offers numerous benefits for organizations, from
    lower compute costs at a time when [cloud costs are increasing](https://www.wsj.com/articles/technology-chiefs-seek-help-wrangling-cloud-costs-61ba0b50)
    to better AI ROI from to improved model performance. Hopefully this blog provides
    a template for teams to take action.
  prefs: []
  type: TYPE_NORMAL
