["```py\nexample = \"You are not a chatbot.\"\n```", "```py\nexample = [\"▁You\", \"▁are\", \"▁not\", \"▁a\". \"▁chat\", \"bot\", \".\"]\n```", "```py\nexample = [887, 526, 451, 263, 13563, 7451, 29889]\n```", "```py\nexample = [1, 887, 526, 451, 263, 13563, 7451, 29889]\n```", "```py\n#We have as many values as tokens.\nattention_mask = [1, 1, 1, 1, 1, 1, 1, 1]\n```", "```py\n{'input_ids': tensor([[1, 887, 526, 451, 263, 13563, 7451, 29889]]), \n 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1]])}\n```", "```py\n{'input_ids': tensor([[1, 887, 526, 451, 263, 13563, 7451, 29889],\n                      [1, 887, 526, 451, 263, 13563, 7451, 29889]]), \n 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1],\n                           [1, 1, 1, 1, 1, 1, 1, 1]])}\n```", "```py\nexample = \"You are not.\"\n```", "```py\nexample = [1, 887, 526, 451, 29889]\nattention_mask = [1, 1, 1, 1, 1]\n```", "```py\n{'input_ids': tensor([[1, 887, 526, 451, 263, 13563, 7451, 29889],\n                      [1, 887, 526, 451, 263, 13563, 7451, 29889],\n                      [1, 887, 526, 451, 29889]]), \n 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1],\n                           [1, 1, 1, 1, 1, 1, 1, 1],\n                           [1, 1, 1, 1, 1]])}\n```", "```py\nexample = \"You are not. [PAD] [PAD] [PAD]\"\n```", "```py\nexample = [1, 887, 526, 451, 29889, 32000, 32000, 32000]\n```", "```py\nattention_mask = [1, 1, 1, 1, 1, 0, 0, 0]\n```", "```py\n{'input_ids': tensor([[1, 887, 526, 451, 263, 13563, 7451, 29889],\n                      [1, 887, 526, 451, 263, 13563, 7451, 29889],\n                      [1, 887, 526, 451, 29889, 32000, 32000, 32000]]), \n 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1],\n                           [1, 1, 1, 1, 1, 1, 1, 1],\n                           [1, 1, 1, 1, 1, 0, 0, 0]])}\n```", "```py\n{'input_ids': tensor([[1, 887, 526, 451, 263, 13563, 7451, 29889],\n                      [1, 887, 526, 451, 263, 13563, 7451, 29889],\n                      [32000, 32000, 32000, 1, 887, 526, 451, 29889]]), \n 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1],\n                           [1, 1, 1, 1, 1, 1, 1, 1],\n                           [0, 0, 0, 1, 1, 1, 1, 1]])}\n```", "```py\n{'input_ids': tensor([[1, 887, 526, 451, 263, 13563, 7451, 29889],\n                      [1, 887, 526, 451, 263, 13563, 7451, 29889],\n                      [1, 887, 526, 451, 29889, 2, 2, 2]]), \n 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1],\n                           [1, 1, 1, 1, 1, 1, 1, 1],\n                           [1, 1, 1, 1, 1, 0, 0, 0]])}\n```", "```py\npip install transformers\n```", "```py\nfrom transformers import AutoTokenizer\n\n#Replace the following with your own Hugging Face access token.\naccess_token = \"hf_token\"\n\n#The model we want to quantize\npretrained_model_dir = \"meta-llama/Llama-2-7b-hf\"\ntokenizer = AutoTokenizer.from_pretrained(pretrained_model_dir, use_fast=True,  use_auth_token=access_token)\n```", "```py\nprompt1 = \"You are not a chatbot.\"\nprompt2 = \"You are not.\"\n```", "```py\nprompts = [prompt1, prompt1]\ninput = tokenizer(prompts, return_tensors=\"pt\");\nprint(input)\n```", "```py\n{'input_ids': tensor([[    1,   887,   526,   451,   263, 13563,  7451, 29889],\n        [    1,   887,   526,   451,   263, 13563,  7451, 29889]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1],\n        [1, 1, 1, 1, 1, 1, 1, 1]])}\n```", "```py\nprompts = [prompt1, prompt1, prompt2]\ninput = tokenizer(prompts, return_tensors=\"pt\");\nprint(input)\n```", "```py\nValueError: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n```", "```py\ntokenizer.padding_side = \"left\"\ntokenizer.pad_token = tokenizer.unk_token\ninput = tokenizer(prompts, padding='max_length', max_length=20, return_tensors=\"pt\");\nprint(input)\n```", "```py\n{'input_ids': tensor([[    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     1,   887,   526,   451,   263, 13563,  7451, 29889],\n        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     1,   887,   526,   451,   263, 13563,  7451, 29889],\n        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     1,   887,   526,   451, 29889]]), 'attention_mask': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1],\n        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1],\n        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1]])}\n```", "```py\ntokenizer.add_special_tokens({'pad_token': '[PAD]'})\ninput = tokenizer(prompts, padding='max_length', max_length=20, return_tensors=\"pt\");\nprint(input)\n```", "```py\n{'input_ids': tensor([[32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000,\n         32000, 32000,     1,   887,   526,   451,   263, 13563,  7451, 29889],\n        [32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000,\n         32000, 32000,     1,   887,   526,   451,   263, 13563,  7451, 29889],\n        [32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000,\n         32000, 32000, 32000, 32000, 32000,     1,   887,   526,   451, 29889]]), 'attention_mask': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1],\n        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1],\n        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1]])}\n```"]