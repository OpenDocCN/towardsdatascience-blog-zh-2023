- en: How to Validate OpenAI GPT Model Performance with Text Summarization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/how-to-validate-openai-gpt-model-performance-with-text-summarization-298978fea764?source=collection_archive---------6-----------------------#2023-04-04](https://towardsdatascience.com/how-to-validate-openai-gpt-model-performance-with-text-summarization-298978fea764?source=collection_archive---------6-----------------------#2023-04-04)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Part 1 of a study on generative AI usage and testing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://markopolocheno.medium.com/?source=post_page-----298978fea764--------------------------------)[![Mark
    Chen](../Images/2d51d4e7ab451b55733a018a3d10a0a7.png)](https://markopolocheno.medium.com/?source=post_page-----298978fea764--------------------------------)[](https://towardsdatascience.com/?source=post_page-----298978fea764--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----298978fea764--------------------------------)
    [Mark Chen](https://markopolocheno.medium.com/?source=post_page-----298978fea764--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F377682c0f342&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-validate-openai-gpt-model-performance-with-text-summarization-298978fea764&user=Mark+Chen&userId=377682c0f342&source=post_page-377682c0f342----298978fea764---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----298978fea764--------------------------------)
    ·9 min read·Apr 4, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F298978fea764&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-validate-openai-gpt-model-performance-with-text-summarization-298978fea764&user=Mark+Chen&userId=377682c0f342&source=-----298978fea764---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F298978fea764&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-validate-openai-gpt-model-performance-with-text-summarization-298978fea764&source=-----298978fea764---------------------bookmark_footer-----------)![](../Images/3d19cbe8fc67655d0ad5525170d590da.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Photo by [Patrick Tomasso](https://unsplash.com/@impatrickt?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: Regardless of your occupation or age, you’ve heard about OpenAI’s generative
    pre-trained transformer (GPT) technology on LinkedIn, YouTube, or in the news.
    These powerful artificial intelligence models/chatbots can seemingly handle any
    task, from creating poems to solving leetcode problems to coherently summarizing
    long articles of text.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/30787bc6542582b06ffd10096457ba8c.png)'
  prefs: []
  type: TYPE_IMG
- en: Screenshot of [OpenAI’s GPT Playground](https://platform.openai.com/playground)
    Summarizing Jupiter Notes, taken by Author
  prefs: []
  type: TYPE_NORMAL
- en: The promising applications of GPT models seem endless within the expanding NLP
    industry. But with ever-increasing model sizes, it is crucial for teams that are
    building large language models (LLMs) to **understand every model’s performance
    and behaviors**.Since AI, like GPT, is a growing subject in ethics, developers
    should ensure that their models are fair, accountable, and explainable. However,
    doing proper testing on artificial general intelligence across many different
    contexts is tedious, expensive, and time-consuming.
  prefs: []
  type: TYPE_NORMAL
- en: From the perspective of a machine learning engineer at [Kolena](https://www.kolena.io/),
    this article offers an extensive guide to using GPT models and compares theirperformance
    for the [abstractive text summarization](https://paperswithcode.com/task/abstractive-text-summarization#:~:text=Abstractive%20Text%20Summarization%20is%20the,appear%20in%20the%20source%20text.)
    task. With this actively researched NLP problem, we will be able to **review model
    behavior, performance differences, ROI**, and so much more.
  prefs: []
  type: TYPE_NORMAL
- en: By the end of this article, you will learn that GPT-3.5’s Turbo model gives
    a 22% higher BERT-F1 score with a 15% lower failure rate at 4.8x the cost and
    4.5x the average inference time in comparison to GPT-3’s Ada model for abstractive
    text summarization.
  prefs: []
  type: TYPE_NORMAL
- en: Using GPT Effectively
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Suppose you want to use GPT for fast solutions in NLP applications, like translating
    text or explaining code. Where do you start? Fortunately, there are only three
    main steps in using GPT for any unique task:'
  prefs: []
  type: TYPE_NORMAL
- en: Picking the right model
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Creating an appropriate [prompt](https://platform.openai.com/docs/guides/completion/prompt-design)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Using [GPT’s API](https://platform.openai.com/docs/api-reference/completions/create)
    for responses (our code is at the end of this article)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Prior to picking a model, we must first consider a few things: How well does
    each model work? Which one gives the best ROI? Which one generally performs the
    best? Which one performs the best on your data?'
  prefs: []
  type: TYPE_NORMAL
- en: 'To narrow down the logistics in choosing a GPT model, we use the [CNN-DailyMail](https://paperswithcode.com/dataset/cnn-daily-mail-1)
    text summarization dataset to benchmark and compare the performance of five [**GPT
    models**](https://platform.openai.com/docs/models/gpt-3)**: Ada, Babbage, Curie,
    Davinci, and Turbo**. The test split of the dataset contains 11,490 news articles
    and their respective summaries.'
  prefs: []
  type: TYPE_NORMAL
- en: 'For step two, we generate new summaries with each model using a consistent
    prompt in the following format:'
  prefs: []
  type: TYPE_NORMAL
- en: '*“Professionally summarize this news article like a reporter with about {word_count_limit}
    to {word_count_limit+50} words:\n {full_text}”*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: In practice, it takes some experimentation to refine a prompt that will give
    subjectively optimal results. By using the same prompt, we can accurately compare
    model behaviors with one less variable in how each model differs.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d77b3f0f4dacf41d0af9a3b69ae09c7a.png)'
  prefs: []
  type: TYPE_IMG
- en: In this particular article, we focus on step one, which is picking the right
    model.
  prefs: []
  type: TYPE_NORMAL
- en: Validating GPT Model Performance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let’s get acquainted with the GPT models of interest, which come from the [GPT-3](https://platform.openai.com/docs/models/gpt-3)
    and [GPT-3.5](https://platform.openai.com/docs/models/gpt-3-5) series. Each model
    has a token limit defining the maximum size of the combined input and output,
    so if, for example, your prompt for the Turbo model contains 2,000 tokens, the
    maximum output you will receive is 2,096 tokens. For English text, 75 words typically
    tokenizes into roughly 100 tokens.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e5bbc5007d1d6b97dc1487ec2767c673.png)'
  prefs: []
  type: TYPE_IMG
- en: We’re currently on the waitlist for [GPT-4](https://platform.openai.com/docs/models/gpt-4)
    access, so we’ll include those models in the future. For now, the main difference
    between GPT-4 and GPT-3.5 is not significant for basic tasks, but GPT-4 offers
    a much larger limit for tokens at a much higher price point compared to Davinci.
  prefs: []
  type: TYPE_NORMAL
- en: Performance Metrics of Abstractive Text Summarization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As we all know, metrics help us measure performance. The tables below highlight
    the standard and custom metrics we use to evaluate models on their text summarization
    performance:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/119111b34611f75076a75019912c94c0.png)'
  prefs: []
  type: TYPE_IMG
- en: '*We calculate BLEU scores with [SacreBLEU](https://huggingface.co/spaces/evaluate-metric/sacrebleu)
    and BERT scores with Microsoft’s [deberta-xlarge-mnli](https://huggingface.co/microsoft/deberta-xlarge-mnli)
    model.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3171ffddf0fda1b31e10bfaa94d80d75.png)'
  prefs: []
  type: TYPE_IMG
- en: 'ROUGE and BLEU measure similarity with word matchings in the ground truths
    and inferences, while BERT scores consider semantic similarity. The higher the
    value, the closer the similarity:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2e553be150410d278ef069348edc5459.png)'
  prefs: []
  type: TYPE_IMG
- en: Results with Standard Metrics
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: After we generate new summaries (inferences) per article on each model, we can
    compare model performance across any type of metric with the ground truths. Let’s
    look into the summary comparisons and metric plots, ignoring Babbage for more
    readability.
  prefs: []
  type: TYPE_NORMAL
- en: '**ROUGE_L and BLEU**‍'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following example, the original 350-word news article has this summary:'
  prefs: []
  type: TYPE_NORMAL
- en: '*A new report from Suncorp Bank found Australians spent $20 billion on technology
    in the past year. Men spent twice as much as women on computers, digital accessories,
    mobile apps, and streaming services. Families with children at home spend 50 per
    cent more to stay digitally than singles, couples without children and empty nesters.
    One third of households don’t budget for technology or wildly underestimate how
    much they will spend.*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'We get the following ROUGE_L, BLEU, and generated summaries with Davinci and
    Ada:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c3504e652170776af801823853e54562.png)'
  prefs: []
  type: TYPE_IMG
- en: You’ll notice that by reading the generated summaries, Davinci does a coherent
    job of summarizing the content of a larger text. Ada, however, does not provide
    a summary of the same quality, and the lower values of ROUGE_L and BLEU reflect
    that lower quality of output.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/45808dfcdc595a9f4deb97d5a7466913.png)'
  prefs: []
  type: TYPE_IMG
- en: Distribution of ROUGE_L
  prefs: []
  type: TYPE_NORMAL
- en: When we examine the distributions of ROUGE_L and BLEU for each model, we see
    that Ada has lower metric values, and Turbo has the highest metric values. Davinci
    falls just behind Turbo in terms of these metrics. As GPT models **increase in
    size**, we see a general **increase in ROUGE and BLEU scores,** too. The greater
    the value for these metrics, the greater the number of words from the ground truth
    summary exist in the generated texts. In addition, these **larger models produce
    a more informative summary with fewer grammatical issues**.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e9fd69ab2b5deccbe2f2537027173e24.png)'
  prefs: []
  type: TYPE_IMG
- en: Distribution of BLEU
  prefs: []
  type: TYPE_NORMAL
- en: ‍**BERT_F1**‍
  prefs: []
  type: TYPE_NORMAL
- en: 'For BERT scores, the same trend is consistent: larger models have better performance
    in matching key words and semantic meaning from the provided summary. This is
    evident in how the distribution for larger models shifts to the right, in the
    direction of higher F1 scores.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3844e97e53995f55fc429e5f482a0166.png)'
  prefs: []
  type: TYPE_IMG
- en: Distribution of BERT_F1
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/616b44804e4e036d6dd7aa0ea8eccc64.png)'
  prefs: []
  type: TYPE_IMG
- en: BERT_F1 vs word_count
  prefs: []
  type: TYPE_NORMAL
- en: From the plot above, we see that bigger models maintain their performance better
    than smaller models as text size grows. The larger models remain consistently
    performant across a wide range of text lengths while the smaller models fluctuate
    in performance as texts grow longer.
  prefs: []
  type: TYPE_NORMAL
- en: Results with Custom Metrics
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s check our custom metrics to see if there’s any reason not to use Turbo
    or Davinci.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0684aa06a3d1ddacd2e24130db3e5aaf.png)'
  prefs: []
  type: TYPE_IMG
- en: Distribution of API Request Costs
  prefs: []
  type: TYPE_NORMAL
- en: From the models’ cost distributions, we learn that **Davinci is far more expensive**
    than any other model. Although Davinci and Turbo perform at similar levels, **Davinci
    costs around ten times the cost of Turbo**.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/eea86402e480a9f7dff0afdd07bef8a1.png)'
  prefs: []
  type: TYPE_IMG
- en: Distribution of inf_to_gt_word_count
  prefs: []
  type: TYPE_NORMAL
- en: In the figure above, there is a drastic difference in the number of words generated
    for the same ground truth. Turbo and Davinci consistently provide a summary that
    is twice the ground truth summary length, whereas **other models are very inconsistent**.
    Specifically, some generated summaries from the smaller models are much shorter
    and some are more than four times as long! Keep in mind that we prompted each
    model with the same request and word count targetper article, but **certain models
    adhered to that restriction** while others completely ignored it.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5a1ca507ffc3155e4c62d565beac0d6d.png)'
  prefs: []
  type: TYPE_IMG
- en: The variance in summary length is a problem for users as this imbalance indicates
    potential issues with the model or poor performance. In the example above, Curie
    repeats “number of charitable causes in the past, most notably his work with St.
    Jude Children’s Research Hospital” at least twice. In comparison to Turbo, **Curie’s
    summary is redundant and suboptimal** while costing the **same price within a
    tenth of a cent**. Within that small difference, we should note that the cost
    in generating this particular summary with Curie is double the cost of Turbo since
    the number of tokens contained in the output was extremely high.
  prefs: []
  type: TYPE_NORMAL
- en: Analysis of Results
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: After running model evaluations for an hour on Kolena, we can outline and summarize
    each model’s performance and characteristics as shown below.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1fd40e6d32caa34c54642e8622a5f42c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We now understand that the larger the model size:'
  prefs: []
  type: TYPE_NORMAL
- en: The more semantically similar the provided and generated summaries are
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The more expensive it is to compute, with the exception of Turbo
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The lower the number of empty summaries
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The slower it is to generate a summary
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **more consistently the model behaves**
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ultimately, the **Turbo model is the top-performing model** offered in the GPT-3/3.5
    series, providing the most consistent text similarity scores, all while also being
    very cost-effective.
  prefs: []
  type: TYPE_NORMAL
- en: Notes for Further Research
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Interestingly, given a text to summarize, **some models simply refuse to generate
    output**, even though the prompt is within the token limit. Turbo failed on none
    of the articles, which is a great achievement. However, this might be because
    Turbo is not as responsive in flagging sensitive content or puts less emphasis
    in making such considerations. Ada might be less performant, but we should ask
    OpenAI if it refuses to generate summaries out of ethical consideration or technical
    limitations. Below is a sample of the **top sixteen news articles by BERT_F1 where
    Ada failed** to provide any summary, but Turbo produced decent summaries. It does
    seem like Ada is less lenient in producing summaries with sensitive content:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/bc14c077db7059eb345df4c3adeb8071.png)'
  prefs: []
  type: TYPE_IMG
- en: Articles Where Ada Fails While Turbo Performs Well — From Kolena
  prefs: []
  type: TYPE_NORMAL
- en: 'The ground truth summaries from the dataset are **not necessarily ideal in
    content or length**. However, we assume ground truth summaries are ideal for the
    purpose of straightforward performance computations, so model evaluation metrics
    might indicate that a great model is actually underperforming, even though it
    produces perfectly valid and detailed summaries. Perhaps **some generated summaries
    are even better than their ground truth counterpart**, as shown below:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ee7e65ec15f1cf870d4c9f76fdd00de7.png)'
  prefs: []
  type: TYPE_IMG
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The world of NLP is rapidly advancing with the introduction of LLMs like GPT.
    As such models become larger, more complex, and more expensive, it is crucial
    for developers and users alike to understand their expected performance levels
    for specific use cases.
  prefs: []
  type: TYPE_NORMAL
- en: ‍
  prefs: []
  type: TYPE_NORMAL
- en: ‍**Different models may better fit your business requirements**, depending on
    your problem, expectations, and available resources. There is much to consider
    when picking a single GPT model for your NLP tasks. In the quickly advancing era
    of LLMs, hopefully the findings outlined in this article give a new perspective
    on the differences among OpenAI’s models.
  prefs: []
  type: TYPE_NORMAL
- en: ‍
  prefs: []
  type: TYPE_NORMAL
- en: Stay tuned for [more posts](https://www.kolena.io/blog) in the future where
    we may cover prompt engineering, GPT-4 performance, or differences in model behavior
    by types of content as well!
  prefs: []
  type: TYPE_NORMAL
- en: ‍
  prefs: []
  type: TYPE_NORMAL
- en: As promised earlier in this article, our code for reference and all five models’
    summaries for every example within this article are all on this [page](https://docs.google.com/document/d/1FrbfHgEJL90Nnwr3mr3De4gJJtntnN2ZsB98f_9-mZg/edit?usp=sharing).
    You can learn more about OpenAI’s API or models in [OpenAI’s documentation](https://platform.openai.com/docs/introduction).
  prefs: []
  type: TYPE_NORMAL
- en: All images of plots are screenshots taken from [Kolena](https://www.kolena.io/)
    unless otherwise indicated. Note that similar plots can be manually generated
    in common frameworks such as mathplotlib.
  prefs: []
  type: TYPE_NORMAL
