- en: 'HashGNN: Deep Dive into Neo4j GDSâ€™s New Node Embedding Algorithm'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 'HashGNN: æ·±å…¥æ¢è®¨ Neo4j GDS çš„æ–°èŠ‚ç‚¹åµŒå…¥ç®—æ³•'
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/hashgnn-deep-dive-into-neo4j-gdss-new-node-embedding-algorithm-5ce9c3029a5c?source=collection_archive---------7-----------------------#2023-08-10](https://towardsdatascience.com/hashgnn-deep-dive-into-neo4j-gdss-new-node-embedding-algorithm-5ce9c3029a5c?source=collection_archive---------7-----------------------#2023-08-10)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://towardsdatascience.com/hashgnn-deep-dive-into-neo4j-gdss-new-node-embedding-algorithm-5ce9c3029a5c?source=collection_archive---------7-----------------------#2023-08-10](https://towardsdatascience.com/hashgnn-deep-dive-into-neo4j-gdss-new-node-embedding-algorithm-5ce9c3029a5c?source=collection_archive---------7-----------------------#2023-08-10)
- en: In this article, we will explore alongside a small example how HashGNN hashes
    graph nodes into an embedding space.
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: åœ¨è¿™ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬å°†é€šè¿‡ä¸€ä¸ªå°ç¤ºä¾‹æ¥æ¢è®¨ HashGNN å¦‚ä½•å°†å›¾èŠ‚ç‚¹å“ˆå¸Œåˆ°åµŒå…¥ç©ºé—´ã€‚
- en: '[](https://medium.com/@philipp.brunenberg?source=post_page-----5ce9c3029a5c--------------------------------)[![Philipp
    Brunenberg](../Images/b2384bcc51966f669f84c949a33ebfcc.png)](https://medium.com/@philipp.brunenberg?source=post_page-----5ce9c3029a5c--------------------------------)[](https://towardsdatascience.com/?source=post_page-----5ce9c3029a5c--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----5ce9c3029a5c--------------------------------)
    [Philipp Brunenberg](https://medium.com/@philipp.brunenberg?source=post_page-----5ce9c3029a5c--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@philipp.brunenberg?source=post_page-----5ce9c3029a5c--------------------------------)[![Philipp
    Brunenberg](../Images/b2384bcc51966f669f84c949a33ebfcc.png)](https://medium.com/@philipp.brunenberg?source=post_page-----5ce9c3029a5c--------------------------------)[](https://towardsdatascience.com/?source=post_page-----5ce9c3029a5c--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----5ce9c3029a5c--------------------------------)
    [Philipp Brunenberg](https://medium.com/@philipp.brunenberg?source=post_page-----5ce9c3029a5c--------------------------------)'
- en: Â·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Â·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F6edbaee01d5d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhashgnn-deep-dive-into-neo4j-gdss-new-node-embedding-algorithm-5ce9c3029a5c&user=Philipp+Brunenberg&userId=6edbaee01d5d&source=post_page-6edbaee01d5d----5ce9c3029a5c---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----5ce9c3029a5c--------------------------------)
    Â·9 min readÂ·Aug 10, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F5ce9c3029a5c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhashgnn-deep-dive-into-neo4j-gdss-new-node-embedding-algorithm-5ce9c3029a5c&user=Philipp+Brunenberg&userId=6edbaee01d5d&source=-----5ce9c3029a5c---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[å…³æ³¨](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F6edbaee01d5d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhashgnn-deep-dive-into-neo4j-gdss-new-node-embedding-algorithm-5ce9c3029a5c&user=Philipp+Brunenberg&userId=6edbaee01d5d&source=post_page-6edbaee01d5d----5ce9c3029a5c---------------------post_header-----------)
    å‘è¡¨åœ¨ [Towards Data Science](https://towardsdatascience.com/?source=post_page-----5ce9c3029a5c--------------------------------)
    Â·9åˆ†é’Ÿé˜…è¯»Â·2023å¹´8æœˆ10æ—¥[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F5ce9c3029a5c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhashgnn-deep-dive-into-neo4j-gdss-new-node-embedding-algorithm-5ce9c3029a5c&user=Philipp+Brunenberg&userId=6edbaee01d5d&source=-----5ce9c3029a5c---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F5ce9c3029a5c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhashgnn-deep-dive-into-neo4j-gdss-new-node-embedding-algorithm-5ce9c3029a5c&source=-----5ce9c3029a5c---------------------bookmark_footer-----------)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F5ce9c3029a5c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhashgnn-deep-dive-into-neo4j-gdss-new-node-embedding-algorithm-5ce9c3029a5c&source=-----5ce9c3029a5c---------------------bookmark_footer-----------)'
- en: If you prefer watching a video on this, you can do so [here](https://youtu.be/fccFuyjNEcM).
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä½ æ›´å–œæ¬¢è§‚çœ‹è§†é¢‘ï¼Œä½ å¯ä»¥[ç‚¹å‡»è¿™é‡Œ](https://youtu.be/fccFuyjNEcM)ã€‚
- en: HashGG (#GNN) is a node embedding technique, which employs concepts of Message
    Passing Neural Networks (MPNN) to capture high-order proximity and node properties.
    It significantly speeds-up calculation in comparison to traditional Neural Networks
    by utilizing an approximation technique called MinHashing. Therefore, it is a
    hash-based approach and introduces a trade-off between efficiency and accuracy.
    In this article, we will understand what all of that means and will explore along
    a small example how the algorithms works.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: HashGGï¼ˆ#GNNï¼‰æ˜¯ä¸€ç§èŠ‚ç‚¹åµŒå…¥æŠ€æœ¯ï¼Œå®ƒé‡‡ç”¨äº†æ¶ˆæ¯ä¼ é€’ç¥ç»ç½‘ç»œï¼ˆMPNNï¼‰çš„æ¦‚å¿µæ¥æ•æ‰é«˜é˜¶é‚»è¿‘æ€§å’ŒèŠ‚ç‚¹å±æ€§ã€‚é€šè¿‡åˆ©ç”¨ä¸€ç§ç§°ä¸ºMinHashingçš„è¿‘ä¼¼æŠ€æœ¯ï¼Œå®ƒæ˜¾è‘—åŠ å¿«äº†è®¡ç®—é€Ÿåº¦ï¼Œç›¸æ¯”äºä¼ ç»Ÿç¥ç»ç½‘ç»œã€‚å› æ­¤ï¼Œå®ƒæ˜¯ä¸€ç§åŸºäºå“ˆå¸Œçš„æ–¹æ³•ï¼Œåœ¨æ•ˆç‡å’Œå‡†ç¡®æ€§ä¹‹é—´å¼•å…¥äº†æƒè¡¡ã€‚æœ¬æ–‡å°†æ·±å…¥ç†è§£è¿™äº›å†…å®¹ï¼Œå¹¶é€šè¿‡ä¸€ä¸ªå°ç¤ºä¾‹æ¢ç´¢ç®—æ³•çš„å·¥ä½œåŸç†ã€‚
- en: 'Node Embeddings: Nodes with similar contexts should be close in the embedding
    space'
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: èŠ‚ç‚¹åµŒå…¥ï¼šå…·æœ‰ç›¸ä¼¼ä¸Šä¸‹æ–‡çš„èŠ‚ç‚¹åœ¨åµŒå…¥ç©ºé—´ä¸­åº”å½“æ¥è¿‘
- en: 'Many graph machine learning use-cases like link prediction and node classification
    require the calculation of similarities of nodes. In a graph context, these similarities
    are most expressive when they capture (i) the neighborhood (i.e. the graph structure)
    and (ii) the properties of the node to be embedded. Node embedding algorithms
    project nodes into a low-dimensional embedding space â€” i.e. they assign each node
    a numerical vector. These vectors â€” the embeddings â€” can be used for further numerical
    predictive analysis (e.g. machine learning algorithms). Embedding algorithms optimize
    for the metric: Nodes with a similar graph context (neighborhood) and/or properties
    should be mapped close in the embedding space. Graph embedding algorithms usually
    employ two fundamental steps: (i) Define a mechanism to sample the context of
    nodes (Random walk in node2vec, k-fold transition matrix in FastRP), and (ii)
    subsequently reduce the dimensionality while preserving pairwise distances (SGD
    in node2vec, random projections in FastRP).'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: è®¸å¤šå›¾æœºå™¨å­¦ä¹ ç”¨ä¾‹ï¼Œå¦‚é“¾æ¥é¢„æµ‹å’ŒèŠ‚ç‚¹åˆ†ç±»ï¼Œéƒ½éœ€è¦è®¡ç®—èŠ‚ç‚¹çš„ç›¸ä¼¼åº¦ã€‚åœ¨å›¾çš„èƒŒæ™¯ä¸‹ï¼Œå½“è¿™äº›ç›¸ä¼¼åº¦æ•æ‰åˆ°ï¼ˆiï¼‰é‚»åŸŸï¼ˆå³å›¾ç»“æ„ï¼‰å’Œï¼ˆiiï¼‰å¾…åµŒå…¥èŠ‚ç‚¹çš„å±æ€§æ—¶ï¼Œæ‰æœ€å…·è¡¨ç°åŠ›ã€‚èŠ‚ç‚¹åµŒå…¥ç®—æ³•å°†èŠ‚ç‚¹æŠ•å°„åˆ°ä½ç»´åµŒå…¥ç©ºé—´ä¸­â€”â€”å³å®ƒä»¬ä¸ºæ¯ä¸ªèŠ‚ç‚¹åˆ†é…ä¸€ä¸ªæ•°å€¼å‘é‡ã€‚è¿™äº›å‘é‡â€”â€”å³åµŒå…¥â€”â€”å¯ä»¥ç”¨äºè¿›ä¸€æ­¥çš„æ•°å€¼é¢„æµ‹åˆ†æï¼ˆä¾‹å¦‚ï¼Œæœºå™¨å­¦ä¹ ç®—æ³•ï¼‰ã€‚åµŒå…¥ç®—æ³•ä¼˜åŒ–çš„æŒ‡æ ‡æ˜¯ï¼šå…·æœ‰ç›¸ä¼¼å›¾ä¸Šä¸‹æ–‡ï¼ˆé‚»åŸŸï¼‰å’Œ/æˆ–å±æ€§çš„èŠ‚ç‚¹åº”å½“åœ¨åµŒå…¥ç©ºé—´ä¸­è¢«æ˜ å°„å¾—æ¥è¿‘ã€‚å›¾åµŒå…¥ç®—æ³•é€šå¸¸é‡‡ç”¨ä¸¤ä¸ªåŸºæœ¬æ­¥éª¤ï¼šï¼ˆiï¼‰å®šä¹‰ä¸€ç§æœºåˆ¶æ¥é‡‡æ ·èŠ‚ç‚¹çš„ä¸Šä¸‹æ–‡ï¼ˆnode2vecä¸­çš„éšæœºæ¸¸èµ°ï¼ŒFastRPä¸­çš„k-foldè½¬ç§»çŸ©é˜µï¼‰ï¼Œå’Œï¼ˆiiï¼‰éšååœ¨ä¿ç•™æˆå¯¹è·ç¦»çš„åŒæ—¶å‡å°‘ç»´åº¦ï¼ˆnode2vecä¸­çš„SGDï¼ŒFastRPä¸­çš„éšæœºæŠ•å½±ï¼‰ã€‚
- en: 'HashGNN: Circumvent training a Neural Network'
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: HashGNNï¼šç»•è¿‡è®­ç»ƒç¥ç»ç½‘ç»œ
- en: The clue about HashGNN is that it does not require us to train a Neural Network
    based on a loss function, as we would have to in a traditional Message Passing
    Neural Network. As node embedding algorithms optimize for â€œsimilar nodes should
    be close in the embedding spaceâ€, the evaluation of the loss involves calculating
    the true similarity of a node pair. This is then used as feedback to the training,
    how accurate the predictions are, and to adjust the weights accordingly. Often,
    the cosine similarity is used as similarity measure.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: å…³äºHashGNNçš„çº¿ç´¢æ˜¯ï¼Œå®ƒä¸è¦æ±‚æˆ‘ä»¬åŸºäºæŸå¤±å‡½æ•°è®­ç»ƒç¥ç»ç½‘ç»œï¼Œå°±åƒæˆ‘ä»¬åœ¨ä¼ ç»Ÿçš„æ¶ˆæ¯ä¼ é€’ç¥ç»ç½‘ç»œä¸­éœ€è¦åšçš„é‚£æ ·ã€‚ç”±äºèŠ‚ç‚¹åµŒå…¥ç®—æ³•ä¼˜åŒ–çš„æ˜¯â€œç›¸ä¼¼çš„èŠ‚ç‚¹åº”è¯¥åœ¨åµŒå…¥ç©ºé—´ä¸­æ¥è¿‘â€ï¼ŒæŸå¤±çš„è¯„ä¼°æ¶‰åŠè®¡ç®—èŠ‚ç‚¹å¯¹çš„çœŸå®ç›¸ä¼¼åº¦ã€‚è¿™è¢«ç”¨ä½œè®­ç»ƒçš„åé¦ˆï¼Œæ¥è¯„ä¼°é¢„æµ‹çš„å‡†ç¡®æ€§ï¼Œå¹¶ç›¸åº”åœ°è°ƒæ•´æƒé‡ã€‚é€šå¸¸ï¼Œä½™å¼¦ç›¸ä¼¼åº¦è¢«ç”¨ä½œç›¸ä¼¼åº¦åº¦é‡ã€‚
- en: HashGNN circumvents the model training and actually does not employ a Neural
    Network altogether. Instead of training weight matrices and defining a loss function,
    it uses a randomized hashing scheme, which hashes node vectors to the same signature
    with the probability of their similarity, meaning that we can embed nodes without
    the necessity of comparing nodes directly against each other (i.e. no need to
    calculate a cosine similarity). This hashing technique is known as MinHashing
    and was originally defined to approximate the similarity of two sets without comparing
    them. As sets are encoded as binary vectors, HashGNN requires a binary node representation.
    In order to understand how this can be used to embed nodes of a general graph,
    several techniques are required. Letâ€™s have a look.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: HashGNNè§„é¿äº†æ¨¡å‹è®­ç»ƒï¼Œå®é™…ä¸Šå®Œå…¨ä¸ä½¿ç”¨ç¥ç»ç½‘ç»œã€‚å®ƒä¸è®­ç»ƒæƒé‡çŸ©é˜µæˆ–å®šä¹‰æŸå¤±å‡½æ•°ï¼Œè€Œæ˜¯ä½¿ç”¨éšæœºå“ˆå¸Œæ–¹æ¡ˆï¼Œå°†èŠ‚ç‚¹å‘é‡å“ˆå¸Œåˆ°å…·æœ‰ç›¸ä¼¼åº¦çš„ç›¸åŒç­¾åï¼Œè¿™æ„å‘³ç€æˆ‘ä»¬å¯ä»¥åµŒå…¥èŠ‚ç‚¹ï¼Œè€Œä¸éœ€è¦ç›´æ¥æ¯”è¾ƒèŠ‚ç‚¹ï¼ˆå³æ— éœ€è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦ï¼‰ã€‚è¿™ç§å“ˆå¸ŒæŠ€æœ¯ç§°ä¸ºMinHashingï¼Œæœ€åˆå®šä¹‰ä¸ºåœ¨ä¸æ¯”è¾ƒé›†åˆçš„æƒ…å†µä¸‹è¿‘ä¼¼ä¸¤ä¸ªé›†åˆçš„ç›¸ä¼¼åº¦ã€‚ç”±äºé›†åˆè¢«ç¼–ç ä¸ºäºŒè¿›åˆ¶å‘é‡ï¼ŒHashGNNéœ€è¦äºŒè¿›åˆ¶èŠ‚ç‚¹è¡¨ç¤ºã€‚ä¸ºäº†ç†è§£å¦‚ä½•å°†å…¶ç”¨äºåµŒå…¥ä¸€èˆ¬å›¾çš„èŠ‚ç‚¹ï¼Œéœ€è¦å‡ ç§æŠ€æœ¯ã€‚è®©æˆ‘ä»¬æ¥çœ‹ä¸€çœ‹ã€‚
- en: MinHashing
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: MinHashing
- en: 'First of all, letâ€™s talk about MinHashing. MinHashing is a locality-sensitive
    hashing technique to approximate the Jaccard similarity of two sets. The Jaccard
    similarity measures the overlap (intersection) of two sets by dividing the intersection
    size by the number of the unique elements present (union) in the two sets. It
    is defined on sets, which are encoded as binary vectors: Each element in the universe
    (the set of all elements) is assigned a unique row index. If a particular set
    contains an element, this is represented as value 1 in the respective row of the
    setâ€™s vector. The MinHashing algorithm hashes each setâ€™s binary vector independently
    and uses `K` hash functions to generate a `K`-dimensional signature for it. The
    intuitive explanation of MinHashing is to randomly select a non-zero element `K`
    times, by choosing the one with the smallest hash value. This will yield the signature
    vector for the input set. The interesting part is, that if we do this for two
    sets without comparing them, they will hash to the same signature with the probability
    of their Jaccard similarity (if `K` is large enough). In other words: The probability
    converges against the Jaccard similarity.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: é¦–å…ˆï¼Œè®©æˆ‘ä»¬è®¨è®ºä¸€ä¸‹MinHashingã€‚MinHashingæ˜¯ä¸€ç§å±€éƒ¨æ•æ„Ÿå“ˆå¸ŒæŠ€æœ¯ï¼Œç”¨äºè¿‘ä¼¼ä¸¤ä¸ªé›†åˆçš„Jaccardç›¸ä¼¼åº¦ã€‚Jaccardç›¸ä¼¼åº¦é€šè¿‡å°†äº¤é›†å¤§å°é™¤ä»¥ä¸¤ä¸ªé›†åˆä¸­å­˜åœ¨çš„å”¯ä¸€å…ƒç´ æ•°é‡ï¼ˆå¹¶é›†ï¼‰æ¥åº¦é‡ä¸¤ä¸ªé›†åˆçš„é‡å ï¼ˆäº¤é›†ï¼‰ã€‚å®ƒå®šä¹‰åœ¨ç¼–ç ä¸ºäºŒè¿›åˆ¶å‘é‡çš„é›†åˆä¸Šï¼šå®‡å®™ä¸­çš„æ¯ä¸ªå…ƒç´ ï¼ˆæ‰€æœ‰å…ƒç´ çš„é›†åˆï¼‰éƒ½åˆ†é…ä¸€ä¸ªå”¯ä¸€çš„è¡Œç´¢å¼•ã€‚å¦‚æœç‰¹å®šé›†åˆåŒ…å«ä¸€ä¸ªå…ƒç´ ï¼Œåˆ™åœ¨é›†åˆå‘é‡çš„ç›¸åº”è¡Œä¸­è¡¨ç¤ºä¸ºå€¼1ã€‚MinHashingç®—æ³•ç‹¬ç«‹åœ°å“ˆå¸Œæ¯ä¸ªé›†åˆçš„äºŒè¿›åˆ¶å‘é‡ï¼Œå¹¶ä½¿ç”¨`K`ä¸ªå“ˆå¸Œå‡½æ•°ç”Ÿæˆ`K`ç»´ç­¾åã€‚MinHashingçš„ç›´è§‚è§£é‡Šæ˜¯éšæœºé€‰æ‹©éé›¶å…ƒç´ `K`æ¬¡ï¼Œé€‰æ‹©å“ˆå¸Œå€¼æœ€å°çš„é‚£ä¸ªã€‚è¿™å°†äº§ç”Ÿè¾“å…¥é›†åˆçš„ç­¾åå‘é‡ã€‚æœ‰è¶£çš„æ˜¯ï¼Œå¦‚æœæˆ‘ä»¬å¯¹ä¸¤ä¸ªé›†åˆè¿›è¡Œè¿™ç§æ“ä½œè€Œä¸è¿›è¡Œæ¯”è¾ƒï¼Œå®ƒä»¬å°†ä»¥å…¶Jaccardç›¸ä¼¼åº¦çš„æ¦‚ç‡å“ˆå¸Œåˆ°ç›¸åŒçš„ç­¾åï¼ˆå¦‚æœ`K`è¶³å¤Ÿå¤§ï¼‰ã€‚æ¢å¥è¯è¯´ï¼šæ¦‚ç‡è¶‹è¿‘äºJaccardç›¸ä¼¼åº¦ã€‚
- en: '![](../Images/92b7a735435b2b2a6c645fd5ac4fbc0a.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/92b7a735435b2b2a6c645fd5ac4fbc0a.png)'
- en: The Jaccard similarity measures the similarity of two sets. Generally, sets
    can also be encoded as binary vectors. Image by author.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: Jaccardç›¸ä¼¼åº¦åº¦é‡ä¸¤ä¸ªé›†åˆçš„ç›¸ä¼¼åº¦ã€‚é€šå¸¸ï¼Œé›†åˆä¹Ÿå¯ä»¥ç¼–ç ä¸ºäºŒè¿›åˆ¶å‘é‡ã€‚å›¾åƒæ¥æºï¼šä½œè€…ã€‚
- en: 'In the illustration: Example sets `s1` and `s2` are represented as binary vectors.
    We can easily compute the Jaccard similarity by comparing the two and counting
    the rows where both vectors have a 1\. These are quite simple operations, but
    the complexity resides in the pair-wise comparison of vectors if we have many
    vectors.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ’å›¾ä¸­ï¼šç¤ºä¾‹é›†åˆ`s1`å’Œ`s2`è¢«è¡¨ç¤ºä¸ºäºŒè¿›åˆ¶å‘é‡ã€‚æˆ‘ä»¬å¯ä»¥é€šè¿‡æ¯”è¾ƒè¿™ä¸¤ä¸ªå‘é‡å¹¶è®¡ç®—ä¸¤ä¸ªå‘é‡éƒ½ä¸º1çš„è¡Œæ•°ï¼Œè½»æ¾è®¡ç®—Jaccardç›¸ä¼¼åº¦ã€‚è¿™äº›æ“ä½œç›¸å½“ç®€å•ï¼Œä½†å¤æ‚æ€§åœ¨äºå½“æˆ‘ä»¬æœ‰å¤šä¸ªå‘é‡æ—¶ï¼Œå‘é‡ä¹‹é—´çš„æˆå¯¹æ¯”è¾ƒã€‚
- en: '![](../Images/ca3602e2959515ed684910fab2596254.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ca3602e2959515ed684910fab2596254.png)'
- en: The MinHashing algorithm generates k permutations of the set features and selects
    the feature with the smallest hashing value to create the minHash signature vector.
    Image by author.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: MinHashingç®—æ³•ç”Ÿæˆé›†åˆç‰¹å¾çš„kä¸ªæ’åˆ—ï¼Œå¹¶é€‰æ‹©å…·æœ‰æœ€å°å“ˆå¸Œå€¼çš„ç‰¹å¾ä»¥åˆ›å»ºminHashç­¾åå‘é‡ã€‚å›¾åƒæ¥æºï¼šä½œè€…ã€‚
- en: Our universe `U` has size 6 and we choose `K` (the number of hash functions)
    to be 3\. We can easily generate new hash functions by using a simple formula
    and boundaries for `a`, `b` and `c`. Now, what we actually do is to hash the indices
    of our vectors (`1-6`), each relating to a single element in our universe, with
    each of our hash functions. This will give us 3 random permutations of the indices
    and therefore elements in our universe. Subsequently, we can take our set vectors
    `s1` and `s2` and use them as masks on our permuted features. For each permutation
    and set vector, we select the index of the smallest hash value, which is present
    in the set. This will yield two 3-dimensional vectors, one for each set, which
    is called the MinHash signature of the set.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬çš„å®‡å®™ `U` å¤§å°ä¸º 6ï¼Œæˆ‘ä»¬é€‰æ‹© `K`ï¼ˆå“ˆå¸Œå‡½æ•°çš„æ•°é‡ï¼‰ä¸º 3ã€‚æˆ‘ä»¬å¯ä»¥é€šè¿‡ä½¿ç”¨ç®€å•çš„å…¬å¼å’Œ `a`ã€`b` å’Œ `c` çš„è¾¹ç•Œæ¥è½»æ¾ç”Ÿæˆæ–°çš„å“ˆå¸Œå‡½æ•°ã€‚ç°åœ¨ï¼Œæˆ‘ä»¬å®é™…ä¸Šåšçš„æ˜¯ä½¿ç”¨æ¯ä¸ªå“ˆå¸Œå‡½æ•°å¯¹æˆ‘ä»¬å‘é‡çš„ç´¢å¼•ï¼ˆ`1-6`ï¼‰è¿›è¡Œå“ˆå¸Œï¼Œæ¯ä¸ªç´¢å¼•ä¸æˆ‘ä»¬å®‡å®™ä¸­çš„ä¸€ä¸ªå•ä¸€å…ƒç´ ç›¸å…³è”ã€‚è¿™å°†ä¸ºæˆ‘ä»¬æä¾›
    3 ä¸ªéšæœºæ’åˆ—çš„ç´¢å¼•ï¼Œä»è€Œå¾—åˆ°æˆ‘ä»¬å®‡å®™ä¸­çš„å…ƒç´ ã€‚éšåï¼Œæˆ‘ä»¬å¯ä»¥å°†æˆ‘ä»¬çš„é›†åˆå‘é‡ `s1` å’Œ `s2` ç”¨ä½œæˆ‘ä»¬æ’åˆ—ç‰¹å¾çš„æ©ç ã€‚å¯¹äºæ¯ä¸ªæ’åˆ—å’Œé›†åˆå‘é‡ï¼Œæˆ‘ä»¬é€‰æ‹©é›†åˆä¸­æœ€å°å“ˆå¸Œå€¼çš„ç´¢å¼•ã€‚è¿™å°†ç”Ÿæˆä¸¤ä¸ª
    3 ç»´å‘é‡ï¼Œæ¯ä¸ªé›†åˆä¸€ä¸ªï¼Œè¿™å°±æ˜¯é›†åˆçš„ MinHash ç­¾åã€‚
- en: MinHashing simply selects random features from the input sets, and we need the
    hash functions only as a means to reproduce this randomness equally on all input
    sets. We have to use the same set of hash functions on all vectors, so that the
    signature values of two input sets collide if both have the selected element present.
    The signature values will collide with the probability of the setsâ€™ Jaccard similarities.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: MinHashing ä»…ä»…ä»è¾“å…¥é›†é€‰æ‹©éšæœºç‰¹å¾ï¼Œæˆ‘ä»¬åªéœ€ä½¿ç”¨å“ˆå¸Œå‡½æ•°æ¥åœ¨æ‰€æœ‰è¾“å…¥é›†ä¸Šå‡ç­‰åœ°å†ç°è¿™ç§éšæœºæ€§ã€‚æˆ‘ä»¬å¿…é¡»åœ¨æ‰€æœ‰å‘é‡ä¸Šä½¿ç”¨ç›¸åŒçš„å“ˆå¸Œå‡½æ•°ï¼Œä»¥ä¾¿ä¸¤ä¸ªè¾“å…¥é›†çš„ç­¾åå€¼åœ¨ä¸¤ä¸ªé›†åˆéƒ½åŒ…å«æ‰€é€‰å…ƒç´ æ—¶å‘ç”Ÿå†²çªã€‚ç­¾åå€¼å°†ä»¥é›†åˆçš„
    Jaccard ç›¸ä¼¼åº¦çš„æ¦‚ç‡å‘ç”Ÿå†²çªã€‚
- en: WLKNN (Weisfeiler-Lehman Kernel Neural Network)
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: WLKNNï¼ˆWeisfeiler-Lehman æ ¸å¿ƒç¥ç»ç½‘ç»œï¼‰
- en: HashGNN uses a message passing scheme as defined in Weisfeiler-Lehman Kernel
    Neural Networks (WLKNN) to capture high-order graph structure and node properties.
    It defines the context sampling strategy as mentioned earlier for HashGNN. The
    WLK runs in `T` iterations. In each iteration, it generates a new node vector
    for each node by combining the node's current vector with the vectors of all directly
    connected neighbors. Therefore, it is considered to pass messages (node vectors)
    along the edges to neighboring nodes.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: HashGNN ä½¿ç”¨å¦‚ Weisfeiler-Lehman æ ¸å¿ƒç¥ç»ç½‘ç»œï¼ˆWLKNNï¼‰ä¸­å®šä¹‰çš„æ¶ˆæ¯ä¼ é€’æ–¹æ¡ˆæ¥æ•æ‰é«˜é˜¶å›¾ç»“æ„å’ŒèŠ‚ç‚¹å±æ€§ã€‚å®ƒå®šä¹‰äº†ä¹‹å‰æåˆ°çš„
    HashGNN ä¸Šä¸‹æ–‡é‡‡æ ·ç­–ç•¥ã€‚WLK åœ¨ `T` æ¬¡è¿­ä»£ä¸­è¿è¡Œã€‚åœ¨æ¯æ¬¡è¿­ä»£ä¸­ï¼Œå®ƒé€šè¿‡å°†èŠ‚ç‚¹çš„å½“å‰å‘é‡ä¸æ‰€æœ‰ç›´æ¥è¿æ¥é‚»å±…çš„å‘é‡ç»„åˆï¼Œä¸ºæ¯ä¸ªèŠ‚ç‚¹ç”Ÿæˆä¸€ä¸ªæ–°çš„èŠ‚ç‚¹å‘é‡ã€‚å› æ­¤ï¼Œå®ƒè¢«è®¤ä¸ºæ˜¯æ²¿è¾¹å°†æ¶ˆæ¯ï¼ˆèŠ‚ç‚¹å‘é‡ï¼‰ä¼ é€’ç»™ç›¸é‚»èŠ‚ç‚¹ã€‚
- en: '![](../Images/6d8c869a768d2965965db28608e63dcd.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6d8c869a768d2965965db28608e63dcd.png)'
- en: The WLK runs in T iterations. In each iteration it passes node information along
    the edges to the neighboring nodes. Image by author.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: WLK åœ¨ T æ¬¡è¿­ä»£ä¸­è¿è¡Œã€‚åœ¨æ¯æ¬¡è¿­ä»£ä¸­ï¼Œå®ƒå°†èŠ‚ç‚¹ä¿¡æ¯æ²¿è¾¹ä¼ é€’ç»™ç›¸é‚»èŠ‚ç‚¹ã€‚å›¾ç‰‡æ¥æºäºä½œè€…ã€‚
- en: After `T` iterations, each node contains information of nodes at `T` hops distance
    (high-order). The calculation of the new node vector in iteration `t` essentially
    aggregates all neighbor messages (from iteration `t-1`) into a single neighbor
    message and then combines it with the node vector of the previous iteration. Additionally,
    the WLKNN employs three neural networks (weight matrices and activation function);
    (i) for the aggregated neighbor vector, (ii) for the node vector and (iii) for
    the combination of the two. It is a notable feature of the WLK that the calculation
    in iteration `t` is only dependent on the result of iteration `t-1`. Therefore,
    it can be considered a Markov chain.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ `T` æ¬¡è¿­ä»£ä¹‹åï¼Œæ¯ä¸ªèŠ‚ç‚¹åŒ…å« `T` è·³è·ç¦»ï¼ˆé«˜é˜¶ï¼‰çš„èŠ‚ç‚¹ä¿¡æ¯ã€‚è¿­ä»£ `t` ä¸­çš„æ–°èŠ‚ç‚¹å‘é‡çš„è®¡ç®—æœ¬è´¨ä¸Šæ˜¯å°†æ‰€æœ‰é‚»å±…æ¶ˆæ¯ï¼ˆæ¥è‡ªè¿­ä»£ `t-1`ï¼‰èšåˆä¸ºå•ä¸€é‚»å±…æ¶ˆæ¯ï¼Œç„¶åä¸å‰ä¸€è¿­ä»£çš„èŠ‚ç‚¹å‘é‡ç»„åˆã€‚æ­¤å¤–ï¼ŒWLKNN
    é‡‡ç”¨ä¸‰ä¸ªç¥ç»ç½‘ç»œï¼ˆæƒé‡çŸ©é˜µå’Œæ¿€æ´»å‡½æ•°ï¼‰ï¼›ï¼ˆiï¼‰ç”¨äºèšåˆé‚»å±…å‘é‡ï¼Œï¼ˆiiï¼‰ç”¨äºèŠ‚ç‚¹å‘é‡ï¼Œä»¥åŠï¼ˆiiiï¼‰ç”¨äºä¸¤è€…çš„ç»„åˆã€‚WLK çš„ä¸€ä¸ªæ˜¾è‘—ç‰¹å¾æ˜¯è¿­ä»£ `t` ä¸­çš„è®¡ç®—ä»…ä¾èµ–äºè¿­ä»£
    `t-1` çš„ç»“æœã€‚å› æ­¤ï¼Œå®ƒå¯ä»¥è¢«è§†ä¸ºä¸€ä¸ªé©¬å°”å¯å¤«é“¾ã€‚
- en: '![](../Images/fd153c49ff0f6c9c916dadd15064c065.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fd153c49ff0f6c9c916dadd15064c065.png)'
- en: 'WLK: In each iteration each node vector gets updated with information from
    neighboring nodes. Therefore, after t iterations, each node contains information
    from nodes with t-hop distance. Image by author.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: WLKï¼šåœ¨æ¯æ¬¡è¿­ä»£ä¸­ï¼Œæ¯ä¸ªèŠ‚ç‚¹å‘é‡éƒ½ä¼šæ›´æ–°æ¥è‡ªç›¸é‚»èŠ‚ç‚¹çš„ä¿¡æ¯ã€‚å› æ­¤ï¼Œåœ¨ t æ¬¡è¿­ä»£ä¹‹åï¼Œæ¯ä¸ªèŠ‚ç‚¹åŒ…å«æ¥è‡ª t è·³è·ç¦»èŠ‚ç‚¹çš„ä¿¡æ¯ã€‚å›¾ç‰‡æ¥æºäºä½œè€…ã€‚
- en: HashGNN by example
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: HashGNN ç¤ºä¾‹
- en: Letâ€™s explore how HashGNN combines the two approaches to embed graph vectors
    efficiently to embedding vectors. Just like the WLKNN, the HashGNN algorithm runs
    in `T` iterations, calculating a new node vector for every node by aggregating
    the neighbor vectors and its own node vector from the previous iteration. However,
    instead of training three weight matrices, it uses three hashing schemes for locality-sensitive
    hashing. Each of the hashing schemes consists of `K` randomly constructed hashing
    functions to draw `K` random features from a binary vector.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬æ¢è®¨ä¸€ä¸‹ HashGNN å¦‚ä½•ç»“åˆè¿™ä¸¤ç§æ–¹æ³•é«˜æ•ˆåœ°å°†å›¾å‘é‡åµŒå…¥åˆ°åµŒå…¥å‘é‡ä¸­ã€‚ä¸ WLKNN ç±»ä¼¼ï¼ŒHashGNN ç®—æ³•åœ¨ `T` æ¬¡è¿­ä»£ä¸­è¿è¡Œï¼Œé€šè¿‡èšåˆé‚»å±…å‘é‡å’Œå‰ä¸€è¿­ä»£çš„è‡ªèº«èŠ‚ç‚¹å‘é‡ï¼Œä¸ºæ¯ä¸ªèŠ‚ç‚¹è®¡ç®—ä¸€ä¸ªæ–°çš„èŠ‚ç‚¹å‘é‡ã€‚ç„¶è€Œï¼Œå®ƒä¸æ˜¯è®­ç»ƒä¸‰ä¸ªæƒé‡çŸ©é˜µï¼Œè€Œæ˜¯ä½¿ç”¨ä¸‰ç§å“ˆå¸Œæ–¹æ¡ˆè¿›è¡Œå±€éƒ¨æ•æ„Ÿå“ˆå¸Œã€‚æ¯ç§å“ˆå¸Œæ–¹æ¡ˆåŒ…å«
    `K` ä¸ªéšæœºæ„é€ çš„å“ˆå¸Œå‡½æ•°ï¼Œä»äºŒè¿›åˆ¶å‘é‡ä¸­æå– `K` ä¸ªéšæœºç‰¹å¾ã€‚
- en: '![](../Images/2518fbe2829144f4daf4ba000e1e1fa8.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2518fbe2829144f4daf4ba000e1e1fa8.png)'
- en: 'The HashGNN Algorithm: We initialize the node vectors with their binary feature
    vectors. Image by author.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: HashGNN ç®—æ³•ï¼šæˆ‘ä»¬ç”¨å…¶äºŒè¿›åˆ¶ç‰¹å¾å‘é‡åˆå§‹åŒ–èŠ‚ç‚¹å‘é‡ã€‚å›¾ç‰‡ç”±ä½œè€…æä¾›ã€‚
- en: 'In each iteration we perform the following steps:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ¯æ¬¡è¿­ä»£ä¸­ï¼Œæˆ‘ä»¬æ‰§è¡Œä»¥ä¸‹æ­¥éª¤ï¼š
- en: '**Step 1: Calculate the nodeâ€™s signature vector:** Min-hash (randomly select
    `K` features) the node vector from the previous iteration using hashing scheme
    3\. In the first iteration the nodes are initialized with their binary feature
    vectors (we''ll talk about how to binarize nodes later). The resulting signature
    (or message) vector is the one which is passed along the edges to all neighbors.
    Therefore, we have to do this for all nodes first in every iteration.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '**ç¬¬ 1 æ­¥ï¼šè®¡ç®—èŠ‚ç‚¹çš„ç­¾åå‘é‡ï¼š** ä½¿ç”¨å“ˆå¸Œæ–¹æ¡ˆ 3 å¯¹æ¥è‡ªå‰ä¸€è¿­ä»£çš„èŠ‚ç‚¹å‘é‡è¿›è¡Œæœ€å°å“ˆå¸Œï¼ˆéšæœºé€‰æ‹© `K` ä¸ªç‰¹å¾ï¼‰ã€‚åœ¨ç¬¬ä¸€æ¬¡è¿­ä»£ä¸­ï¼ŒèŠ‚ç‚¹ç”¨å…¶äºŒè¿›åˆ¶ç‰¹å¾å‘é‡åˆå§‹åŒ–ï¼ˆç¨åæˆ‘ä»¬å°†è®¨è®ºå¦‚ä½•å¯¹èŠ‚ç‚¹è¿›è¡ŒäºŒå€¼åŒ–ï¼‰ã€‚å¾—åˆ°çš„ç­¾åï¼ˆæˆ–æ¶ˆæ¯ï¼‰å‘é‡æ˜¯æ²¿ç€è¾¹ä¼ é€’ç»™æ‰€æœ‰é‚»å±…çš„ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¿…é¡»åœ¨æ¯æ¬¡è¿­ä»£ä¸­é¦–å…ˆå¯¹æ‰€æœ‰èŠ‚ç‚¹æ‰§è¡Œæ­¤æ“ä½œã€‚'
- en: '![](../Images/d6cb39108a5ad1faa4c451660f264125.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d6cb39108a5ad1faa4c451660f264125.png)'
- en: 'HashGNN Step 1: in each iteration is to calculate the message vector for each
    node by using hashing scheme 3\. Image by author.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: HashGNN ç¬¬ 1 æ­¥ï¼šåœ¨æ¯æ¬¡è¿­ä»£ä¸­ï¼Œé€šè¿‡ä½¿ç”¨å“ˆå¸Œæ–¹æ¡ˆ 3 è®¡ç®—æ¯ä¸ªèŠ‚ç‚¹çš„æ¶ˆæ¯å‘é‡ã€‚å›¾ç‰‡ç”±ä½œè€…æä¾›ã€‚
- en: '**Step 2: Construct the neighbor vector:** In each node, weâ€™ll receive the
    signature vectors from all directly connected neighbors and aggregate them into
    a single binary vector. Subsequently, we use hashing scheme 2 to select `K` random
    features from the aggregated neighbor vector and call the result the neighbor
    vector.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '**ç¬¬ 2 æ­¥ï¼šæ„å»ºé‚»å±…å‘é‡ï¼š** åœ¨æ¯ä¸ªèŠ‚ç‚¹ä¸­ï¼Œæˆ‘ä»¬å°†æ¥æ”¶æ¥è‡ªæ‰€æœ‰ç›´æ¥è¿æ¥é‚»å±…çš„ç­¾åå‘é‡ï¼Œå¹¶å°†å®ƒä»¬èšåˆæˆä¸€ä¸ªäºŒè¿›åˆ¶å‘é‡ã€‚éšåï¼Œæˆ‘ä»¬ä½¿ç”¨å“ˆå¸Œæ–¹æ¡ˆ 2
    ä»èšåˆçš„é‚»å±…å‘é‡ä¸­é€‰æ‹© `K` ä¸ªéšæœºç‰¹å¾ï¼Œå¹¶å°†ç»“æœç§°ä¸ºé‚»å±…å‘é‡ã€‚'
- en: '![](../Images/773b590d410d722cdbdb6f9c82944cf1.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/773b590d410d722cdbdb6f9c82944cf1.png)'
- en: 'HashGNN Step 2: We collect the message vectors from all neighbors and aggregate
    them. Image by author.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: HashGNN ç¬¬ 2 æ­¥ï¼šæˆ‘ä»¬æ”¶é›†æ‰€æœ‰é‚»å±…çš„æ¶ˆæ¯å‘é‡å¹¶å°†å…¶èšåˆã€‚å›¾ç‰‡ç”±ä½œè€…æä¾›ã€‚
- en: '**Step 3: Combine node and neighbor vector into new node vector:** Finally,
    we use hashing scheme 1 to randomly select `K` features from the node vector of
    the previous iteration and combine the result with the neighbor vector. The resulting
    vector is the new node vector which is the starting point for the next iteration.
    Note, that this is not the same as in step 1: There, we apply hashing scheme 3
    on the node vector to construct the message/signature vector.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '**ç¬¬ 3 æ­¥ï¼šå°†èŠ‚ç‚¹å‘é‡å’Œé‚»å±…å‘é‡åˆå¹¶æˆæ–°çš„èŠ‚ç‚¹å‘é‡ï¼š** æœ€åï¼Œæˆ‘ä»¬ä½¿ç”¨å“ˆå¸Œæ–¹æ¡ˆ 1 ä»å‰ä¸€è¿­ä»£çš„èŠ‚ç‚¹å‘é‡ä¸­éšæœºé€‰æ‹© `K` ä¸ªç‰¹å¾ï¼Œå¹¶å°†ç»“æœä¸é‚»å±…å‘é‡ç»“åˆã€‚å¾—åˆ°çš„å‘é‡æ˜¯æ–°çš„èŠ‚ç‚¹å‘é‡ï¼Œå®ƒæ˜¯ä¸‹ä¸€æ¬¡è¿­ä»£çš„èµ·ç‚¹ã€‚æ³¨æ„ï¼Œè¿™ä¸ç¬¬
    1 æ­¥ä¸åŒï¼šåœ¨ç¬¬ 1 æ­¥ä¸­ï¼Œæˆ‘ä»¬å¯¹èŠ‚ç‚¹å‘é‡åº”ç”¨å“ˆå¸Œæ–¹æ¡ˆ 3 æ¥æ„å»ºæ¶ˆæ¯/ç­¾åå‘é‡ã€‚'
- en: '![](../Images/a3caa1e99ca7dc8ef64304c84234ba7d.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a3caa1e99ca7dc8ef64304c84234ba7d.png)'
- en: 'HashGNN Step 3: We combine the min-hashed node vector with the aggregated neighbor
    vector to arrive at the resulting node vector for this iteration. Image by author.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: HashGNN ç¬¬ 3 æ­¥ï¼šæˆ‘ä»¬å°†æœ€å°å“ˆå¸Œçš„èŠ‚ç‚¹å‘é‡ä¸èšåˆçš„é‚»å±…å‘é‡ç»“åˆï¼Œä»¥å¾—åˆ°è¯¥è¿­ä»£çš„ç»“æœèŠ‚ç‚¹å‘é‡ã€‚å›¾ç‰‡ç”±ä½œè€…æä¾›ã€‚
- en: As we can see, the resulting (new) node vector has influence of its own node
    features (3 & 5), as well as its neighborsâ€™ features (2 & 5). After iteration
    1 the node vector will capture information from neighbors with 1 hop distance.
    However, as we use this as input for iteration 2, it will already be influenced
    by features 2 hops away.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: ä»å›¾ä¸­æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œå¾—åˆ°çš„ï¼ˆæ–°ï¼‰èŠ‚ç‚¹å‘é‡å—è‡ªèº«èŠ‚ç‚¹ç‰¹å¾ï¼ˆ3 å’Œ 5ï¼‰ä»¥åŠå…¶é‚»å±…ç‰¹å¾ï¼ˆ2 å’Œ 5ï¼‰çš„å½±å“ã€‚ç»è¿‡ç¬¬ä¸€æ¬¡è¿­ä»£åï¼ŒèŠ‚ç‚¹å‘é‡å°†æ•æ‰åˆ°æ¥è‡ªè·ç¦» 1
    è·³çš„é‚»å±…çš„ä¿¡æ¯ã€‚ç„¶è€Œï¼Œå½“æˆ‘ä»¬å°†å…¶ç”¨ä½œç¬¬äºŒæ¬¡è¿­ä»£çš„è¾“å…¥æ—¶ï¼Œå®ƒå·²ç»å—åˆ°è·ç¦» 2 è·³ç‰¹å¾çš„å½±å“ã€‚
- en: '![](../Images/208f648a73632a7df517c06f669af84a.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/208f648a73632a7df517c06f669af84a.png)'
- en: After the first iteration, the new node vector has influence from its own features
    and the neighboring nodeâ€™s features. Image by author.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ç¬¬ä¸€æ¬¡è¿­ä»£åï¼Œæ–°çš„èŠ‚ç‚¹å‘é‡å—è‡ªèº«ç‰¹å¾å’Œé‚»è¿‘èŠ‚ç‚¹ç‰¹å¾çš„å½±å“ã€‚å›¾ç‰‡ç”±ä½œè€…æä¾›ã€‚
- en: Neo4j GDS generalizations
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Neo4j GDS æ³›åŒ–
- en: HashGNN was implemented by the Neo4j GDS (Graph Data Science Library) and adds
    some useful generalizations to the algorithm.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: HashGNN æ˜¯ç”± Neo4j GDSï¼ˆå›¾æ•°æ®ç§‘å­¦åº“ï¼‰å®ç°çš„ï¼Œå¹¶å¯¹ç®—æ³•è¿›è¡Œäº†æœ‰ç”¨çš„æ³›åŒ–ã€‚
- en: 'One important auxiliary step in the GDS is feature binarization. MinHashing
    and therefore HashGNN requires binary vectors as input. Neo4j offers an additional
    preparation step to transform real-valued node vectors into binary feature vectors.
    They utilize a technique called hyperplane rounding. The algorithm works as follows:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: GDS ä¸­ä¸€ä¸ªé‡è¦çš„è¾…åŠ©æ­¥éª¤æ˜¯ç‰¹å¾äºŒå€¼åŒ–ã€‚MinHashing å’Œå› æ­¤ HashGNN éœ€è¦äºŒè¿›åˆ¶å‘é‡ä½œä¸ºè¾“å…¥ã€‚Neo4j æä¾›äº†ä¸€ä¸ªé¢å¤–çš„å‡†å¤‡æ­¥éª¤ï¼Œå°†å®å€¼èŠ‚ç‚¹å‘é‡è½¬æ¢ä¸ºäºŒè¿›åˆ¶ç‰¹å¾å‘é‡ã€‚ä»–ä»¬ä½¿ç”¨äº†ä¸€ç§ç§°ä¸ºè¶…å¹³é¢å–æ•´çš„æŠ€æœ¯ã€‚ç®—æ³•çš„å·¥ä½œæµç¨‹å¦‚ä¸‹ï¼š
- en: '**Step 1: Define node features:** Define the (real-valued) node properties
    to be used as node features. This is done using the parameter `featureProperties`.
    We will call this the node input vector `f`.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '**æ­¥éª¤ 1ï¼šå®šä¹‰èŠ‚ç‚¹ç‰¹å¾ï¼š** å®šä¹‰ç”¨äºèŠ‚ç‚¹ç‰¹å¾çš„ï¼ˆå®å€¼ï¼‰èŠ‚ç‚¹å±æ€§ã€‚è¿™æ˜¯é€šè¿‡å‚æ•°`featureProperties`å®Œæˆçš„ã€‚æˆ‘ä»¬å°†å…¶ç§°ä¸ºèŠ‚ç‚¹è¾“å…¥å‘é‡`f`ã€‚'
- en: '**Step 2: Construct random binary classifiers:** Define one hyperplane for
    each target dimension. The number of resulting dimensions is controlled by the
    parameter `dimensions`. A hyperplane is a high-dimensional plane and, as long
    as it resides in the origin, can be described solely by its normal vector `n`.
    The `n` vector is orthogonal to the plane''s surface and therefore describes its
    orientation. In our case the `n` vector needs to be of the same dimensionality
    as the node input vectors (`dim(f) = dim(n)`). To construct a hyperplane, we simply
    draw `dim(f)` times from a Gaussian distribution.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '**æ­¥éª¤ 2ï¼šæ„å»ºéšæœºäºŒè¿›åˆ¶åˆ†ç±»å™¨ï¼š** ä¸ºæ¯ä¸ªç›®æ ‡ç»´åº¦å®šä¹‰ä¸€ä¸ªè¶…å¹³é¢ã€‚ç»“æœç»´åº¦çš„æ•°é‡ç”±å‚æ•°`dimensions`æ§åˆ¶ã€‚è¶…å¹³é¢æ˜¯ä¸€ä¸ªé«˜ç»´å¹³é¢ï¼Œåªè¦å®ƒä½äºåŸç‚¹ï¼Œå°±å¯ä»¥ä»…é€šè¿‡å…¶æ³•å‘é‡`n`æ¥æè¿°ã€‚`n`å‘é‡å‚ç›´äºå¹³é¢çš„è¡¨é¢ï¼Œå› æ­¤æè¿°äº†å…¶æ–¹å‘ã€‚åœ¨æˆ‘ä»¬çš„æƒ…å†µä¸‹ï¼Œ`n`å‘é‡éœ€è¦ä¸èŠ‚ç‚¹è¾“å…¥å‘é‡å…·æœ‰ç›¸åŒçš„ç»´åº¦ï¼ˆ`dim(f)
    = dim(n)`ï¼‰ã€‚ä¸ºäº†æ„å»ºä¸€ä¸ªè¶…å¹³é¢ï¼Œæˆ‘ä»¬ç®€å•åœ°ä»é«˜æ–¯åˆ†å¸ƒä¸­æŠ½å–`dim(f)`æ¬¡ã€‚'
- en: '![](../Images/d2f1bc2713cd84e395263a60dd11230e.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d2f1bc2713cd84e395263a60dd11230e.png)'
- en: 'Binarization of features: We use hyperplane rounding to construct binary features
    from real-valued input vectors. We use one random Gaussian classifier for each
    target dimension. Image by author.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: ç‰¹å¾äºŒå€¼åŒ–ï¼šæˆ‘ä»¬ä½¿ç”¨è¶…å¹³é¢å–æ•´å°†å®å€¼è¾“å…¥å‘é‡æ„é€ ä¸ºäºŒè¿›åˆ¶ç‰¹å¾ã€‚æˆ‘ä»¬ä¸ºæ¯ä¸ªç›®æ ‡ç»´åº¦ä½¿ç”¨ä¸€ä¸ªéšæœºé«˜æ–¯åˆ†ç±»å™¨ã€‚å›¾ç‰‡ç”±ä½œè€…æä¾›ã€‚
- en: '**Step 3: Classify node vectors:** Calculate the dot-product of the node input
    vector and each hyperplane vector, which yields the angle between the hyperplane
    and the input vector. Using a `threshold` parameter, we can decide whether the
    input vector is above (1) or below (0) the hyperplane and assign the respective
    value to the resulting binary feature vector. This is exactly what also happens
    in a binary classification â€” with the only difference that we do not iteratively
    optimize our hyperplane, but rather use random Gaussian classifiers.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '**æ­¥éª¤ 3ï¼šåˆ†ç±»èŠ‚ç‚¹å‘é‡ï¼š** è®¡ç®—èŠ‚ç‚¹è¾“å…¥å‘é‡å’Œæ¯ä¸ªè¶…å¹³é¢å‘é‡çš„ç‚¹ç§¯ï¼Œä»è€Œå¾—åˆ°è¶…å¹³é¢å’Œè¾“å…¥å‘é‡ä¹‹é—´çš„è§’åº¦ã€‚ä½¿ç”¨`threshold`å‚æ•°ï¼Œæˆ‘ä»¬å¯ä»¥å†³å®šè¾“å…¥å‘é‡æ˜¯é«˜äºï¼ˆ1ï¼‰è¿˜æ˜¯ä½äºï¼ˆ0ï¼‰è¶…å¹³é¢ï¼Œå¹¶å°†ç›¸åº”çš„å€¼åˆ†é…ç»™ç»“æœçš„äºŒè¿›åˆ¶ç‰¹å¾å‘é‡ã€‚è¿™ä¸äºŒå…ƒåˆ†ç±»ä¸­çš„è¿‡ç¨‹å®Œå…¨ä¸€è‡´â€”â€”å”¯ä¸€çš„ä¸åŒæ˜¯æˆ‘ä»¬ä¸è¿­ä»£ä¼˜åŒ–è¶…å¹³é¢ï¼Œè€Œæ˜¯ä½¿ç”¨éšæœºé«˜æ–¯åˆ†ç±»å™¨ã€‚'
- en: '![](../Images/60c9c061a86addd96ac456bd37d3013f.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/60c9c061a86addd96ac456bd37d3013f.png)'
- en: Using n hyperplanes leads to n-dimensional binary node signatures. Image by
    author.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ n ä¸ªè¶…å¹³é¢ä¼šå¯¼è‡´ n ç»´çš„äºŒè¿›åˆ¶èŠ‚ç‚¹ç­¾åã€‚å›¾ç‰‡ç”±ä½œè€…æä¾›ã€‚
- en: In essence, we draw one random Gaussian classifier for each target dimension
    and set a threshold parameter. Then, we classify our input vectors for each target
    dimension and obtain a `d`-dimensional binary vector which we use as input for
    HashGNN.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬è´¨ä¸Šï¼Œæˆ‘ä»¬ä¸ºæ¯ä¸ªç›®æ ‡ç»´åº¦ç»˜åˆ¶ä¸€ä¸ªéšæœºé«˜æ–¯åˆ†ç±»å™¨ï¼Œå¹¶è®¾ç½®ä¸€ä¸ªé˜ˆå€¼å‚æ•°ã€‚ç„¶åï¼Œæˆ‘ä»¬å¯¹æ¯ä¸ªç›®æ ‡ç»´åº¦çš„è¾“å…¥å‘é‡è¿›è¡Œåˆ†ç±»ï¼Œå¹¶å¾—åˆ°ä¸€ä¸ª`d`ç»´çš„äºŒè¿›åˆ¶å‘é‡ï¼Œè¯¥å‘é‡å°†ä½œä¸ºHashGNNçš„è¾“å…¥ã€‚
- en: Conclusion
  id: totrans-59
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç»“è®º
- en: HashGNN uses locality-sensitive hashing to embed node vectors into an embedding
    space. By using this technique, it circumvents the computationally intensive,
    iterative training of a Neural Network (or other optimization), as well as direct
    node comparisons. The authors of the paper report a 2â€“4 orders of magnitude faster
    runtime than learning based algorithms like SEAL and P-GNN, while still producing
    highly comparable (in some cases even better) accuracy.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: HashGNN ä½¿ç”¨å±€éƒ¨æ•æ„Ÿå“ˆå¸Œå°†èŠ‚ç‚¹å‘é‡åµŒå…¥åˆ°åµŒå…¥ç©ºé—´ä¸­ã€‚é€šè¿‡ä½¿ç”¨è¿™ç§æŠ€æœ¯ï¼Œå®ƒç»•è¿‡äº†è®¡ç®—å¯†é›†å‹çš„ç¥ç»ç½‘ç»œï¼ˆæˆ–å…¶ä»–ä¼˜åŒ–ï¼‰çš„è¿­ä»£è®­ç»ƒä»¥åŠç›´æ¥çš„èŠ‚ç‚¹æ¯”è¾ƒã€‚è®ºæ–‡çš„ä½œè€…æŠ¥å‘Šç§°ï¼Œä¸åŸºäºå­¦ä¹ çš„ç®—æ³•å¦‚
    SEAL å’Œ P-GNN ç›¸æ¯”ï¼Œå…¶è¿è¡Œæ—¶é—´å¿« 2 åˆ° 4 ä¸ªæ•°é‡çº§ï¼ŒåŒæ—¶ä»èƒ½äº§ç”Ÿé«˜åº¦å¯æ¯”ï¼ˆåœ¨æŸäº›æƒ…å†µä¸‹ç”šè‡³æ›´å¥½ï¼‰çš„å‡†ç¡®æ€§ã€‚
- en: '![](../Images/57a5e45e97bc200333c9ccaf5309e684.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/57a5e45e97bc200333c9ccaf5309e684.png)'
- en: 'HashGNN is 2â€“4 orders of magnitudes faster than learning-based algorithms,
    while delivering comparable results. Image taken from: [https://arxiv.org/abs/2105.14280.](https://arxiv.org/abs/2105.14280.)'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: HashGNN æ¯”åŸºäºå­¦ä¹ çš„ç®—æ³•å¿« 2 åˆ° 4 ä¸ªæ•°é‡çº§ï¼ŒåŒæ—¶æä¾›äº†å¯æ¯”çš„ç»“æœã€‚å›¾åƒæ¥æºï¼š[https://arxiv.org/abs/2105.14280.](https://arxiv.org/abs/2105.14280.)
- en: HashGNN is implemented in the Neo4j GDS (Graph Data Science Library) and can
    therefore be used out-of-the-box on your Neo4j graph. In the next post, I will
    go into practical details about how to use it, and what to keep in mind.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: HashGNN åœ¨ Neo4j GDSï¼ˆå›¾æ•°æ®ç§‘å­¦åº“ï¼‰ä¸­å®ç°ï¼Œå› æ­¤å¯ä»¥ç›´æ¥åœ¨ä½ çš„ Neo4j å›¾ä¸Šä½¿ç”¨ã€‚åœ¨ä¸‹ä¸€ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘å°†è¯¦ç»†è®²è§£å¦‚ä½•ä½¿ç”¨å®ƒä»¥åŠéœ€è¦æ³¨æ„çš„äº‹é¡¹ã€‚
- en: Thanks for stopping by and see you next time. ğŸš€
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: æ„Ÿè°¢ä½ çš„å…‰ä¸´ï¼Œä¸‹æ¬¡è§ã€‚ğŸš€
- en: References
  id: totrans-65
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å‚è€ƒæ–‡çŒ®
- en: '[Original Paper: Hashing-Accelerated Graph Neural Networks for Link Prediction](https://doi.org/10.48550/arXiv.2105.14280)'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[åŸå§‹è®ºæ–‡ï¼šå“ˆå¸ŒåŠ é€Ÿçš„å›¾ç¥ç»ç½‘ç»œç”¨äºé“¾è·¯é¢„æµ‹](https://doi.org/10.48550/arXiv.2105.14280)'
- en: '[Neo4j GDS Documentation: HashGNN](https://neo4j.com/docs/graph-data-science/current/machine-learning/node-embeddings/hashgnn/)'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Neo4j GDS æ–‡æ¡£ï¼šHashGNN](https://neo4j.com/docs/graph-data-science/current/machine-learning/node-embeddings/hashgnn/)'
