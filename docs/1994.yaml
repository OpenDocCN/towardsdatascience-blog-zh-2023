- en: 'AI-Driven Insights: Leveraging LangChain and Pinecone with GPT-4'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/ai-driven-insights-for-product-managers-vol-1-leveraging-langchain-and-pinecone-with-gpt-4-7755485019f9?source=collection_archive---------6-----------------------#2023-06-19](https://towardsdatascience.com/ai-driven-insights-for-product-managers-vol-1-leveraging-langchain-and-pinecone-with-gpt-4-7755485019f9?source=collection_archive---------6-----------------------#2023-06-19)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Empowering Next-Gen Product Managers
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://elengabrielyan.medium.com/?source=post_page-----7755485019f9--------------------------------)[![Elen
    Gabrielyan](../Images/a28280d051c33841e870f57101a731b2.png)](https://elengabrielyan.medium.com/?source=post_page-----7755485019f9--------------------------------)[](https://towardsdatascience.com/?source=post_page-----7755485019f9--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----7755485019f9--------------------------------)
    [Elen Gabrielyan](https://elengabrielyan.medium.com/?source=post_page-----7755485019f9--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F9f456c2bb76&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fai-driven-insights-for-product-managers-vol-1-leveraging-langchain-and-pinecone-with-gpt-4-7755485019f9&user=Elen+Gabrielyan&userId=9f456c2bb76&source=post_page-9f456c2bb76----7755485019f9---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----7755485019f9--------------------------------)
    ·10 min read·Jun 19, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F7755485019f9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fai-driven-insights-for-product-managers-vol-1-leveraging-langchain-and-pinecone-with-gpt-4-7755485019f9&user=Elen+Gabrielyan&userId=9f456c2bb76&source=-----7755485019f9---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F7755485019f9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fai-driven-insights-for-product-managers-vol-1-leveraging-langchain-and-pinecone-with-gpt-4-7755485019f9&source=-----7755485019f9---------------------bookmark_footer-----------)![](../Images/fb2dd3b8f691b939ee14e08d2c00b81e.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: Working effectively with qualitative data is one of the most important skills
    a product manager can have; collecting data, analyzing it and communicating it
    in an efficient way, by coming up with actionable and valuable insights.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: You can get qualitative data from many places — user interviews, competitor
    feedback, or comments from people using your product. Depending on what you’re
    trying to achieve, you might analyze this data straight away or save it up for
    later. Sometimes, you might only need a few user interviews to confirm a hypothesis.
    Other times, you could need feedback from a thousand users to spot trends or test
    ideas. So, your approach to analyzing this data can change depending on the situation.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: With Large Language Models like GPT-4, and AI tools such as LangChain and Pinecone,
    we can handle various situations and lots of data more effectively. In this guide,
    I’ll share my experience with these tools. My goal is to show product managers
    and anyone else who works with qualitative data how to use these AI tools to get
    more useful insights from their data.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: What will you find in this guide-style article?
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: I’ll start by introducing you to these AI tools, and some current limitations
    of Large Language Models (LLMs).
  id: totrans-12
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: I’ll discuss different ways you can make the most of these tools for real-life
    use cases.
  id: totrans-13
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Using user feedback analysis as an example, I’ll provide code snippets and examples
    to show you how these tools work in practice.
  id: totrans-14
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Please note: To use tools like GPT-4, LangChain, and Pinecone, you need to
    be comfortable with data and have some basic coding skills. It’s also important
    to understand your customers and be able to turn data insights into real actions.
    Knowledge of AI and machine learning is a plus, but not a must.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: 'Understanding AI Tools: why you might need LangChain, and Pinecone'
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Assuming you’re already familiar with GPT-4, it’s important to grasp some concepts
    as we discuss tools that work with LLMs. One major challenge with current LLMs
    like GPT-4 is their ‘context window’ — this is how much information they can process
    and remember at one time.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: Currently, there are two versions of GPT-4\. The standard one has an 8k token
    context, while the extended version has a 32k context window. To give you an idea,
    a 32k token is about 24,000 words, which is roughly equivalent to 48 pages of
    text. But bear in mind, the 32k version isn’t available to everyone, even if you
    have access to GPT-4.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: Also, OpenAI announced recently about a new ChatGPT model, called gpt-3.5-turbo-16k,
    which offers 4 times the context length of gpt-3.5-turbo. When working with insight
    analysis I would suggest working with gpt-4, as it has better reasoning than GPT-3.5\.
    But you can play around and see what works on your use case.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: Why am I mentioning this?
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When dealing with insight analysis, a big challenge comes up if you have a lot
    of data, or you’re interested in more than just one prompt. Let’s say you have
    one user interview. You want to dig deeper and get more insights from it, using
    GPT-4\. In this case, you can just take the interview transcript and give it to
    ChatGPT, choosing GPT-4\. You might have to split the text once, but that’s it.
    You don’t need any other fancy tools for this. So you would actually need these
    fancy, new tools when working with a lot of qualitative data. Let’s understand
    what those tools are, then we will move to some specific use cases and examples.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理洞察分析时，如果你有大量数据，或者对不止一个提示感兴趣，那么会遇到一个大挑战。假设你有一个用户访谈。你想更深入地挖掘并从中获取更多的洞察，使用 GPT-4。在这种情况下，你可以直接拿访谈记录并交给
    ChatGPT，选择 GPT-4。你可能需要分割一次文本，但仅此而已。你不需要其他复杂的工具。因此，实际上当处理大量定性数据时，你才会需要这些复杂的新工具。让我们了解这些工具是什么，然后我们将转向一些具体的使用案例和示例。
- en: So what is LangChain?
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 那么 LangChain 是什么？
- en: LangChain is a framework that revolves around LLMs and offers various functionalities
    like chatbots, Generative Question-Answering (GQA), and summarization. Its versatility
    lies in the ability to connect different components together, including prompt
    templates, LLMs, agents, and memory systems.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: LangChain 是一个围绕 LLM 的框架，提供了各种功能，如聊天机器人、生成式问答（GQA）和总结。它的多功能性在于将不同组件连接在一起，包括提示模板、LLMs、代理和记忆系统。
- en: Prompt templates are pre-made prompts for different situations, while LLMs process
    and generate responses. Agents help make decisions based on the LLM’s output,
    and memory systems store information for later use.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 提示模板是针对不同情况预制的提示，而 LLMs 处理和生成响应。代理帮助根据 LLM 的输出做出决策，记忆系统则存储信息以供后续使用。
- en: In this article I will share some capabilities of it in my examples.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我将分享一些我例子中的功能。
- en: '![](../Images/3da1aa9ce4639b4a93a09d1e73526dca.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3da1aa9ce4639b4a93a09d1e73526dca.png)'
- en: '*High-level Overview of LangChain Modules*'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '*LangChain 模块的高级概述*'
- en: What is Pinecone?
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Pinecone 是什么？
- en: Pinecone.ai is a powerful tool designed to simplify the management of high-dimensional
    data representations known as vectors.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: Pinecone.ai 是一个强大的工具，旨在简化高维数据表示（即向量）的管理。
- en: Vectors are particularly useful when dealing with a lot of text data, like when
    you’re trying to extract information from it. Consider a situation where you’re
    analyzing feedback and you want to find out various details about a product. This
    kind of deep insight gathering wouldn’t be possible with just keyword searches
    like “great”, “improve”, or “i suggest”, as you might miss out on a lot of context.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 向量在处理大量文本数据时特别有用，比如当你试图从中提取信息时。考虑一个你在分析反馈并想要了解产品各种细节的情况。仅靠“great”、“improve”或“i
    suggest”等关键词搜索无法实现这种深度洞察，因为你可能会错过很多上下文。
- en: Now, I won’t delve into the technical aspects of text vectorization (which could
    be word-based, sentence-based, etc.). The key thing you need to understand is
    that words get converted into numbers through machine learning models, and these
    numbers are then stored in arrays.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我不会深入探讨文本向量化的技术细节（这可以是基于词的、基于句子的等）。你需要理解的关键点是，单词通过机器学习模型转换成数字，这些数字然后存储在数组中。
- en: 'Let’s take an example:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们举个例子：
- en: 'The word “seafood” might be translated into a series of numbers like this:
    [1.2, -0.2, 7.0, 19.9, 3.1, …, 10.2].'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 单词 “seafood” 可能被转换为一系列数字，如：[1.2, -0.2, 7.0, 19.9, 3.1, …, 10.2]。
- en: 'When I search for another word, that word also gets transformed into a number
    series (or vector). If our machine learning model is doing its job properly, words
    that have a similar context to “seafood” should have a number series that’s close
    to the series for “seafood”. Here’s an example:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 当我搜索另一个词时，该词也会被转换为一系列数字（或向量）。如果我们的机器学习模型正常工作，与“seafood”有类似上下文的单词，其数字系列应该接近于“seafood”的系列。这里有一个例子：
- en: '“shrimp” might be translated as: [1.1, -0.3, 7.1, 19.8, 3.0, …, 10.5], where
    the numbers are close to numbers “seafood” has.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: “shrimp” 可能被转换为：[1.1, -0.3, 7.1, 19.8, 3.0, …, 10.5]，其中这些数字接近于“seafood”拥有的数字。
- en: With Pinecone.ai, you can efficiently store and search these vectors, enabling
    quick and accurate similarity comparisons.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Pinecone.ai，你可以高效地存储和搜索这些向量，实现快速而准确的相似性比较。
- en: By using its capabilities, you can organize and index vectors derived from LLM
    models, opening the door to deeper insights and the discovery of meaningful patterns
    within extensive datasets.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 通过利用其功能，你可以组织和索引来自 LLM 模型的向量，从而开启更深层次的见解，并在广泛的数据集中发现有意义的模式。
- en: In simpler words, Pinecone.ai allows you to store the vector representations
    of your qualitative data in a convenient way. You can easily search through these
    vectors and apply LLM models to extract valuable insights from them. It simplifies
    the process of managing your data and deriving meaningful information from it.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，Pinecone.ai 允许你以便捷的方式存储你的定性数据的向量表示。你可以轻松地在这些向量中进行搜索，并应用 LLM 模型从中提取有价值的见解。它简化了管理数据和从中获取有意义信息的过程。
- en: '![](../Images/81c46b6619a12375dab3bb4390791059.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/81c46b6619a12375dab3bb4390791059.png)'
- en: Representation of Vector Databases
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 向量数据库的表示
- en: When would you actually need tools like LangChain and Pinecone?
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 你什么时候实际需要像 LangChain 和 Pinecone 这样的工具？
- en: 'Short answer: when you are working with a lot of qualitative data.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 简短回答：当你处理大量的定性数据时。
- en: 'Let me share some use cases from my experience to give you an idea:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 让我分享一些我的使用案例，以给你一个了解：
- en: Imagine you have thousands of written feedback entries from your product channels.
    You want to identify patterns in the data and track how the feedback has evolved
    over time.
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 想象一下你有来自产品渠道的数千条书面反馈。你想识别数据中的模式，并跟踪反馈随时间的演变。
- en: Suppose you have reviews in different languages and you want to translate them
    in your preferred language, and then extract insights.
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 假设你有不同语言的评论，你想将它们翻译成你偏好的语言，然后提取见解。
- en: You aim to conduct competitive analysis by analyzing customer reviews, feedback,
    and sentiment regarding your competitors’ products.
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你打算通过分析客户评论、反馈和对竞争对手产品的情感来进行竞争分析。
- en: Your company conducts surveys or user studies, generating a significant volume
    of qualitative responses. Extracting meaningful insights, uncovering trends, and
    informing product or service improvements are your goals.
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你的公司进行调查或用户研究，生成大量的定性反馈。提取有意义的见解，发现趋势，并为产品或服务改进提供信息是你的目标。
- en: These are just a few examples of situations where tools like LangChain and Pinecone
    can be invaluable for product managers working with qualitative data.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 这些只是 LangChain 和 Pinecone 等工具在处理定性数据的产品经理工作中的一些宝贵应用示例。
- en: 'Example Project: Feedback analysis'
  id: totrans-49
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 示例项目：反馈分析
- en: As a product manager, my job involves improving our meeting notes and transcription
    features. To do this, we listen to what our users say about them.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 作为产品经理，我的工作包括改进我们的会议记录和转录功能。为此，我们听取用户对这些功能的评价。
- en: 'For our [meeting notes feature](https://krisp.ai/ai-meeting-assistant/), users
    give us a score between 1 and 5 for quality, tell us which template they used,
    and also send us their comments. Here is the flow:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的[会议记录功能](https://krisp.ai/ai-meeting-assistant/)，用户给我们质量打分1到5分，告诉我们他们使用了哪个模板，并发送他们的评论。流程如下：
- en: '![](../Images/e82324a307d9a09ba751a071fe1c457a.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e82324a307d9a09ba751a071fe1c457a.png)'
- en: 'In this project, I looked closely at two things: what users said about our
    feature and which templates they used. I ended up dealing with a huge amount of
    data — over 20,000 words, which turned into more than 38,000 “tokens” (or pieces
    of data) when I used a special tool to break it down. That’s so much data that
    it’s more than what some advanced models can handle all at once!'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个项目中，我密切关注了两件事：用户对我们功能的反馈和他们使用了哪些模板。我处理了大量的数据——超过 20,000 字，当我使用一个特殊工具将其拆分时，变成了超过
    38,000 个“令牌”（或数据片段）。这数据量之大，超出了某些高级模型一次性处理的能力！
- en: 'To help me analyze this extensive data, I turned to two advanced tools: LangChain
    and Pinecone, supplemented with GPT-4\. With these in our arsenal, let’s delve
    deeper into the project and see what these high-tech tools enabled us to do.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 为了帮助我分析这些大量的数据，我转向了两个高级工具：LangChain 和 Pinecone，并辅以 GPT-4。借助这些工具，让我们更深入地探讨这个项目，看看这些高科技工具使我们能够做些什么。
- en: 'This project’s primary objective was extracting insights from the gathered
    data, which required:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 这个项目的主要目标是从收集的数据中提取见解，这需要：
- en: The ability to create specific queries related to our dataset.
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建与数据集相关的特定查询的能力。
- en: The use of LLMs for handling vast information volumes.
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 LLMs 处理大量信息。
- en: First, I’ll give you an overview of how I carried out the project. After that,
    I’ll share some examples of the code I used.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我会给你一个项目实施的概述。之后，我会分享一些我使用的代码示例。
- en: We start with a collection of text files. Each file contains user feedback paired
    with the name of the template they used. You can process this data to fit your
    needs — I had to do some post-processing for my project. Remember, your files
    and data might be different, so feel free to tweak the information for your own
    project.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从一组文本文件开始。每个文件包含用户反馈和他们使用的模板名称。你可以处理这些数据以适应你的需求——我在我的项目中需要做一些后处理。记住，你的文件和数据可能不同，所以可以根据自己的项目进行调整。
- en: 'For example you want to understand users’ feedback on meeting notes structure:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，你想了解用户对会议记录结构的反馈：
- en: '[PRE0]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Here’s a high-level diagram showcasing the process flow when utilizing LLM
    and Pinecone. You ask GPT-4 a question, or what we call a ‘query’. Meanwhile,
    Pinecone, our library full of all feedback, provides the context to your query,
    when you send the question itself to it (“embed query”). Together, they help us
    make sense of our data efficiently:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是一个高层次的图示，展示了在使用LLM和Pinecone时的流程。你向GPT-4提问，或者我们称之为“查询”。与此同时，Pinecone，我们的反馈库，提供了你的查询的上下文，当你将问题本身发送给它（“嵌入查询”）时。它们一起帮助我们高效地理解数据：
- en: '![](../Images/cc2c3092924645f8cbe8d8a3a06ac72d.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cc2c3092924645f8cbe8d8a3a06ac72d.png)'
- en: 'Below is a more simplified version of the diagram:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是图示的简化版本：
- en: '![](../Images/dbae8365b9ef674d83a32d23103964c8.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/dbae8365b9ef674d83a32d23103964c8.png)'
- en: Let’s do it! In this script, we set up a pipeline to analyze user feedback data
    using OpenAI’s GPT-4, Pinecone, and LangChain. Essentially, it imports necessary
    libraries, sets the path to the feedback data, and establishes the OpenAI API
    key for processing this data.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始吧！在这个脚本中，我们设置了一个管道来使用OpenAI的GPT-4、Pinecone和LangChain分析用户反馈数据。本质上，它导入了必要的库，设置了反馈数据的路径，并建立了处理数据的OpenAI
    API密钥。
- en: '[PRE1]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Then we define and call a function load_docs() that loads user feedback documents
    from a specified directory using LangChain’s DirectoryLoader. It then counts and
    displays the total number of loaded documents.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们定义并调用一个函数`load_docs()`，该函数使用LangChain的DirectoryLoader从指定目录加载用户反馈文档。然后，它会统计并显示加载的文档总数。
- en: '[PRE2]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Next define and execute the split_docs() function, which divides the loaded
    documents into smaller chunks of a specific size and overlap using LangChain’s
    RecursiveCharacterTextSplitter. It then counts and prints the total number of
    resulting chunks.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来定义并执行`split_docs()`函数，该函数使用LangChain的RecursiveCharacterTextSplitter将加载的文档分割成特定大小和重叠的小块。然后，它会统计并打印出结果块的总数。
- en: '[PRE3]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'To work with Pinecone, which is basically a vector database we need to get
    embeddings out of our docs, that’s why we should introduce a function for that.
    There are many ways to do it, but let’s go with OpenAI’s embedding function:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用Pinecone，这基本上是一个向量数据库，我们需要从文档中获取嵌入，因此我们应该为此引入一个函数。有很多方法可以做到这一点，但我们使用OpenAI的嵌入函数：
- en: '[PRE4]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: For storing those vectors into Pinecone, you need to create an account there
    and create an index as well. That’s quite straightforward to do. Then you will
    get an API key, environment name and the index name from there.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 要将这些向量存储到Pinecone中，你需要在那里创建一个帐户并创建一个索引。这个过程非常简单。然后你将从那里获得API密钥、环境名称和索引名称。
- en: '[PRE5]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The next step is to be able to find answers. It’s like finding the closest points
    to your question in a field of possible answers, giving us the most relevant results.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是能够找到答案。这就像在可能的答案领域中找到离你的问题最近的点，从而给出最相关的结果。
- en: '[PRE6]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: In this code, we set up a question-answering system using OpenAI’s GPT-4 model
    and LangChain. The get_answer() function takes a question as input, finds similar
    documents, and uses the question-answering chain to generate an answer.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在这段代码中，我们使用OpenAI的GPT-4模型和LangChain设置了一个问答系统。`get_answer()`函数接受一个问题作为输入，查找类似的文档，并使用问答链生成一个答案。
- en: '[PRE7]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: We got to the question! Or questions. You can ask as many questions as you wish.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 我们来到了问题！或者问题们。你可以问任意多的问题。
- en: '[PRE8]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Implementing Retrieval Q&A Chain:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 实现检索问答链：
- en: To implement the retrieval question-answering system, we use the RetrievalQA
    class from LangChain. It uses an OpenAI LLM to answer questions and relies on
    a “stuff” chain type. The retriever is connected to a previously created index
    and is stored in the ‘qa’ variable. For better understanding, you can learn more
    about [retrieval techniques.](https://blog.langchain.dev/retrieval/)
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: So we got the response, let’s present the content stored in the response variable
    in a visually appealing format using Markdown text. It makes the displayed text
    more organized and easier to read.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '![](../Images/354be8b9180c8499f71421e758707b06.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
- en: Example output
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: Go ahead and experiment both with input files and queries to get the best out
    of this approach and tools.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  id: totrans-90
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In short, GPT-4, LangChain, and Pinecone make it easy to handle big chunks of
    qualitative data. They help us dig into this data and find valuable insights,
    guiding better decisions. This article gave a sneak peek into their use, but there’s
    a lot more they can do.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: As these tools continue to advance and become more common, learning to use them
    now will give you a significant advantage in the future. So, keep exploring and
    learning about these tools because they are shaping the present and the future
    of data analysis.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: Stay tuned for more ways to explore these handy tools in the future!
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: '*All images, unless otherwise noted, are by the author*'
  id: totrans-94
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: References
  id: totrans-95
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[LangChain Documentation](https://api.python.langchain.com/en/latest/)'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: '[Pinecone Documentation](https://docs.pinecone.io/docs/overview)'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: '[LangChain for LLM Application Development short course](https://learn.deeplearning.ai/langchain/lesson/1/introduction)'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
