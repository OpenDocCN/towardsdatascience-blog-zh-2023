- en: 'K-means Clustering: An Introductory Guide and Practical Application'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/k-means-clustering-an-introductory-guide-and-practical-application-dce70bfa4249?source=collection_archive---------9-----------------------#2023-01-23](https://towardsdatascience.com/k-means-clustering-an-introductory-guide-and-practical-application-dce70bfa4249?source=collection_archive---------9-----------------------#2023-01-23)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[](https://medium.com/@kurt.klingensmith?source=post_page-----dce70bfa4249--------------------------------)[![Kurt
    Klingensmith](../Images/2249e99f12d10f81598c754b1aaf76cc.png)](https://medium.com/@kurt.klingensmith?source=post_page-----dce70bfa4249--------------------------------)[](https://towardsdatascience.com/?source=post_page-----dce70bfa4249--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----dce70bfa4249--------------------------------)
    [Kurt Klingensmith](https://medium.com/@kurt.klingensmith?source=post_page-----dce70bfa4249--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fbaf16815de65&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fk-means-clustering-an-introductory-guide-and-practical-application-dce70bfa4249&user=Kurt+Klingensmith&userId=baf16815de65&source=post_page-baf16815de65----dce70bfa4249---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----dce70bfa4249--------------------------------)
    ·10 min read·Jan 23, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fdce70bfa4249&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fk-means-clustering-an-introductory-guide-and-practical-application-dce70bfa4249&user=Kurt+Klingensmith&userId=baf16815de65&source=-----dce70bfa4249---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fdce70bfa4249&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fk-means-clustering-an-introductory-guide-and-practical-application-dce70bfa4249&source=-----dce70bfa4249---------------------bookmark_footer-----------)![](../Images/cbfe2141e10479a0bee80aa89d178bef.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Cars of varying engine types, sizes, and weights. Photograph by author.
  prefs: []
  type: TYPE_NORMAL
- en: Using clustering algorithms such as K-means is one of the most popular starting
    points for machine learning. K-means clustering is an unsupervised machine learning
    technique that sorts similar data into groups, or clusters. Data within a specific
    cluster bears a higher degree of commonality amongst observations within the cluster
    than it does with observations outside of the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: The K in K-means represents the user-defined *k*-number of clusters. K-means
    clustering works by attempting to find the best cluster centroid positions within
    the data for *k-*number of clusters, ensuring data within the cluster is closer
    in distance to the given centroid than it is to any other centroid. Ideally, the
    resulting clusters maximize similarity amongst the data within each unique cluster.
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that various methods for clustering exist; this article will focus on
    one of the most popular techniques: K-means.'
  prefs: []
  type: TYPE_NORMAL
- en: '**This guide consists of two parts:**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A K-means clustering introduction using generated data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: An application of K-means clustering to an automotive dataset.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Code:'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: All code is [available at the github page linked here](https://github.com/kurtklingensmith/KMeansClustering)**.**
    Feel free to download the notebook (click CODE and Download Zip) and run it alongside
    this article!
  prefs: []
  type: TYPE_NORMAL
- en: 1\. K-means Clustering Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For this guide, we will use the [scikit-learn](https://scikit-learn.org/stable/)
    libraries [1]:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'To demonstrate K-means clustering, we first need data. Conveniently, the sklearn
    library includes the ability to [generate data blobs](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_blobs.html)
    [2]. The code is rather simple:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The parameters of the make_blobs() function allow the user to specify the number
    of centers (which could correlate to potential cluster centroids) and how messy
    the “blobs” are (cluster_std, which adjusts the cluster’s standard deviation).
    The above code generates the blobs; the below code gets it into a dataframe and
    a plotly scatter plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Here’s the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/906ac1b0ddc8eb615f621159644161ae.png)'
  prefs: []
  type: TYPE_IMG
- en: Screenshot by author.
  prefs: []
  type: TYPE_NORMAL
- en: Due to picking a low “cluster_std” value in the make_blobs() function, the resulting
    graph has three very clearly defined data blobs that should be easy work for a
    K-means clustering algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: How Many Clusters?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The K in K-means is the number of clusters, a user-defined figure. For a given
    dataset, there is typically an optimal number of clusters. In the generated data
    seen above, it’s probably three.
  prefs: []
  type: TYPE_NORMAL
- en: To mathematically determine the optimal number of clusters, use the “Elbow Method.”
    This method calculates the within-cluster sum of squares (WCSS) for various values
    of *k*, with lower values generally being better. The WCSS represents the sum
    of the squared distances of each data point from a cluster’s centroid. The Elbow
    Method plots the WCSS as a result of adding additional clusters; eventually, an
    “elbow” appears as WCSS drops diminish with the addition of new clusters. This
    reveals the optimal cluster amount.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code generates an Elbow Chart for the above data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'When plotted, this yields:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/d7298ba26f86ccc1fb44e0c890840eca.png)'
  prefs: []
  type: TYPE_IMG
- en: Screenshot by author.
  prefs: []
  type: TYPE_NORMAL
- en: Note how the plot of WCSS has a sharp “elbow” at 3 clusters. This implies 3
    is the optimal cluster choice, as the WCSS value decreased sharply with the addition
    of clusters up to three. Adding clusters beyond 3 sees only minimal gains in WCSS
    reduction. Thus, the optimal cluster value is *k* = 3.
  prefs: []
  type: TYPE_NORMAL
- en: Generating Clusters
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The next step is to run the K-means clustering algorithm. In the below code,
    the line kmeans = KMeans(3) is where the value for *k* is input:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The result is a labeled dataframe with a “Cluster” column:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d3ea0a5f10d0373005e1e87b05b5e8a6.png)'
  prefs: []
  type: TYPE_IMG
- en: Screenshot by author.
  prefs: []
  type: TYPE_NORMAL
- en: 'Plotting this yields:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/bf6d46d3c7bc52287ee247dc92bd5372.png)'
  prefs: []
  type: TYPE_IMG
- en: Screenshot by author.
  prefs: []
  type: TYPE_NORMAL
- en: The K-means method has successfully clustered the data into three distinct clusters.
    Now let’s see what happens with more realistic data.
  prefs: []
  type: TYPE_NORMAL
- en: 2\. K-means Clustering in Automotive Data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Python Library Seaborn provides various datasets, including one on automobile
    fuel efficiency from cars built during the oil crisis era. For the purpose of
    aiding in the learning of clustering, we will filter the dataset for 8 and 4-cylinder-engined
    cars. This represents the largest and smallest engines typically available during
    that time period.
  prefs: []
  type: TYPE_NORMAL
- en: Fortunately, this could be an example of a real world analysis scenario. In
    the 1970’s, rapid increases in fuel prices made 8-cylinder cars less desirable;
    as a result, 4-cylinder cars became increasingly common, but how do 4 and 8-cylinder-engined
    cars truly behave regarding fuel consumption?
  prefs: []
  type: TYPE_NORMAL
- en: Specifically, we will explore 4 and 8-cylinder cars with regards to their *weight*
    and *fuel efficiency* measured in miles per gallon (MPG).
  prefs: []
  type: TYPE_NORMAL
- en: 'Preparing the data is straightfoward:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The dataset looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2997f7cac78fb3d4fe037099ef3bc3eb.png)'
  prefs: []
  type: TYPE_IMG
- en: Screenshot by author.
  prefs: []
  type: TYPE_NORMAL
- en: 'Visualizing the weight and MPG of the cars yields the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/8b65254a0c286a353220d23e4eaaaa10.png)'
  prefs: []
  type: TYPE_IMG
- en: Screenshot by author.
  prefs: []
  type: TYPE_NORMAL
- en: Scaling the Data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Note how the x-axis represents weight, while the y-axis represents MPG. An increase
    of 10 pounds is not as significant as an increase in 10 MPG. This could impact
    the clustering results.
  prefs: []
  type: TYPE_NORMAL
- en: There are various options available to combat this issue; Jeff Hale [has an
    excellent article linked here](/scale-standardize-or-normalize-with-scikit-learn-6ccc7d176a02)
    providing a technical overview of the various methods and use cases for scaling,
    standardizing, and normalizing [1]. For this exercise, we will use Sci-Kit Learn’s
    StandardScaler() function.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'This returns:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/17fd6413a6a9755569920c8ce52f6c9f.png)'
  prefs: []
  type: TYPE_IMG
- en: This two-column dataframe is now ready to pass through the Elbow Method.
  prefs: []
  type: TYPE_NORMAL
- en: How Many Clusters?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As in section 1, we follow the Elbow Method with the same code to return the
    following chart:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/a230b8f3b371d374be369d31af942c31.png)'
  prefs: []
  type: TYPE_IMG
- en: Screenshot by author.
  prefs: []
  type: TYPE_NORMAL
- en: Two clusters appears to be the elbow. However, the jump from 2 to 3 clusters
    isn’t as flat as it was in the data blobs example of section 1.
  prefs: []
  type: TYPE_NORMAL
- en: '**Note:** The elbow method may not always provide the clearest results, necessitating
    trying a few values for *k* in the region that appears to be the elbow. While
    we are going with 2 clusters, it might be worth trying 3\. For more advanced analysis
    on the topic, Satyam Kumar provides an [alternative to the Elbow Method, referred
    to as the Silhouette Method, in this Towards Data Science article](/silhouette-method-better-than-elbow-method-to-find-optimal-clusters-378d62ff6891)
    [4].'
  prefs: []
  type: TYPE_NORMAL
- en: Generating Clusters
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The code to generate clusters remains similar to the earlier example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'This gets the dataframe back to where we started, but with one addition — a
    column identifying the Cluster for each observation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/06bbcac8532cf712f93c47d08da6431b.png)'
  prefs: []
  type: TYPE_IMG
- en: Screenshot by author.
  prefs: []
  type: TYPE_NORMAL
- en: 'Plotting this reveals the clusters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/24c319cf86fe9a5cd87dccc76d8d1808.png)'
  prefs: []
  type: TYPE_IMG
- en: Screenshot by author.
  prefs: []
  type: TYPE_NORMAL
- en: Further Analysis
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Remember that K-means clustering attempts to group data based on similarities
    within the data, with those similarities determined by distance to a cluster centroid.
    Having the cluster value added to the original dataframe allows additional analysis
    on the original dataset. For example, the above cluster visualization shows a
    split between the clusters around 3000 pounds and about 20 MPG.
  prefs: []
  type: TYPE_NORMAL
- en: 'Additional visualizations may yield more insights. Consider the following strip
    plots ([code available at linked Git page](https://github.com/kurtklingensmith/KMeansClustering)):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6cb232be802e906e802415e355cda643.png)'
  prefs: []
  type: TYPE_IMG
- en: Screenshot by author.
  prefs: []
  type: TYPE_NORMAL
- en: First, K-means clustering managed to sort vehicles based only on weight and
    MPG into clusters that almost perfectly align with the cylinder counts. One cluster
    is 100% 4-cylinder cars, while the other is almost all 8-cylinder.
  prefs: []
  type: TYPE_NORMAL
- en: Second, the mostly 8-cylinder cluster has four cars with 4-cylinder engines.
    Despite having smaller engines, these cars appear to have MPG performance closer
    to larger, 8-cylinder cars. The 4-cylinder cars in cluster zero are all close
    to 3000 pounds in weight while performing at or below 20 MPG.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, there appears to be some 8-cylinder cars that achieve an MPG score
    closer to 4-cylinder cars. An additional chart reveals these examples are considered
    outliers ([code available at Git page](https://github.com/kurtklingensmith/KMeansClustering)):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d92f0ba49a072dec3bfad8c9b7968a7a.png)'
  prefs: []
  type: TYPE_IMG
- en: Screenshot by author.
  prefs: []
  type: TYPE_NORMAL
- en: This is an example of how clustering can help understand data while guiding
    follow-on analysis and data exploration. An engineer may find it worth analyzing
    why certain 4-cylinder cars seem to perform poorly for efficiency and get clustered
    with 8-cylinder cars.
  prefs: []
  type: TYPE_NORMAL
- en: '**For additional practice and learning:** [take the full notebook at the linked
    Git page](https://github.com/kurtklingensmith/KMeansClustering), re-load the seaborn
    MPG dataset, but do not filter for 4 and 8-cylinder cars. You will find the Elbow
    Method reveals an ambiguous divide between 2 versus 3 clusters — both may be worth
    trying. Or, attempt the notebook [using one of the other seaborn datasets](/seaborn-essentials-for-data-visualization-in-python-291aa117583b)
    or other features within the MPG dataset [5].'
  prefs: []
  type: TYPE_NORMAL
- en: '**Is K-means clustering the best technique for this data?**'
  prefs: []
  type: TYPE_NORMAL
- en: Recall that for the example with blobs, the K-means Elbow Method had a very
    clear optimal point and the resultant clustering analysis easily identified the
    distinct blobs. K-means tends to perform better when the data is more spherical
    in nature, as was the case with the data blobs. Alternative methods may work better
    with complicated data and perhaps even the automotive data used in this example.
    For further reading on some alternative clustering methods, Victor Roman provides
    a great [overview in this Towards Data Science article](/unsupervised-machine-learning-clustering-analysis-d40f2b34ae7e)
    [6].
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: K-means clustering is a powerful machine learning tool that enables identifying
    similarities within data. The technique can provide insights or enhance data understanding
    in a way that guides further analysis questions and improves data visualization.
    This article and code provide a guide on K-means clustering, but there are other
    clustering techniques available, some of which may be more appropriate given the
    type of data being analyzed. But even if K-means is not the most appropriate method
    for the given data, K-means clustering is an excellent method to know and a great
    spot to start getting familiarized with machine learning. Furthermore, K-means
    clustering can serve as a baseline for comparison to other clustering methods,
    meaning it may still prove useful even if it ends up not being the ideal clustering
    algorithm for a given dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'References:'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] Scikit learn, [scikit-learn: machine learning in Python](https://scikit-learn.org/stable/)
    (2023).'
  prefs: []
  type: TYPE_NORMAL
- en: '[2] Scikit learn, [sklearn.datasets.make_blobs](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_blobs.html)
    (2023).'
  prefs: []
  type: TYPE_NORMAL
- en: '[3] J. Hale, [Scale, Standardize, or Normalize with Scikit-Learn](/scale-standardize-or-normalize-with-scikit-learn-6ccc7d176a02)
    (2019), Towards Data Science.'
  prefs: []
  type: TYPE_NORMAL
- en: '[4] S. Kumar, [Silhouette Method — Better than Elbow Method to Find Optimal
    Clusters](/silhouette-method-better-than-elbow-method-to-find-optimal-clusters-378d62ff6891)
    (2020), Towards Data Science.'
  prefs: []
  type: TYPE_NORMAL
- en: '[5] M. Alam, [Seaborn essentials for data visualization in Python](/seaborn-essentials-for-data-visualization-in-python-291aa117583b)
    (2020), Towards Data Science.'
  prefs: []
  type: TYPE_NORMAL
- en: '[6] V. Roman, [Machine Learning: Clustering Analysis](/unsupervised-machine-learning-clustering-analysis-d40f2b34ae7e)
    (2019), Towards Data Science.'
  prefs: []
  type: TYPE_NORMAL
