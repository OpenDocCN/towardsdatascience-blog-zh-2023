- en: 'When to Run Code on CPU and Not GPU: Typical Cases'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/when-cpu-is-faster-than-gpu-typical-cases-bc7c64ee3c66?source=collection_archive---------2-----------------------#2023-05-21](https://towardsdatascience.com/when-cpu-is-faster-than-gpu-typical-cases-bc7c64ee3c66?source=collection_archive---------2-----------------------#2023-05-21)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: How to choose the hardware to optimize the computation for your use case
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://robertkwiatkowski01.medium.com/?source=post_page-----bc7c64ee3c66--------------------------------)[![Robert
    Kwiatkowski](../Images/94ec06b3647aef0b65fce2dd97972318.png)](https://robertkwiatkowski01.medium.com/?source=post_page-----bc7c64ee3c66--------------------------------)[](https://towardsdatascience.com/?source=post_page-----bc7c64ee3c66--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----bc7c64ee3c66--------------------------------)
    [Robert Kwiatkowski](https://robertkwiatkowski01.medium.com/?source=post_page-----bc7c64ee3c66--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F9f55d2ee5cad&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhen-cpu-is-faster-than-gpu-typical-cases-bc7c64ee3c66&user=Robert+Kwiatkowski&userId=9f55d2ee5cad&source=post_page-9f55d2ee5cad----bc7c64ee3c66---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----bc7c64ee3c66--------------------------------)
    ·6 min read·May 21, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fbc7c64ee3c66&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhen-cpu-is-faster-than-gpu-typical-cases-bc7c64ee3c66&user=Robert+Kwiatkowski&userId=9f55d2ee5cad&source=-----bc7c64ee3c66---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fbc7c64ee3c66&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhen-cpu-is-faster-than-gpu-typical-cases-bc7c64ee3c66&source=-----bc7c64ee3c66---------------------bookmark_footer-----------)![](../Images/e802259c9d5e0dbc6d15c2285b8ea5bd.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: Photo by [Francesco Vantini](https://unsplash.com/@brostvarta?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: With rapidly advancing technologies like Artificial Intelligence (AI), Machine
    Learning (ML), Internet of Things (IoT), Virtual Reality (VR) and sophisticated
    numerical simulations, the demand for computational power is reaching unprecedented
    heights. In most real-life cases, it’s not only the computational power that counts
    but also factors like the hardware size and power consumption.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: 'When you’re designing a system depending on business and technical requirements,
    you can choose from various computational components like:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 当你根据业务和技术要求设计系统时，可以从各种计算组件中进行选择，例如：
- en: Integrated Circuits (ICs)
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 集成电路（ICs）
- en: Microcontrollers (MCs)
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 微控制器（MCs）
- en: Central Processing Units (CPU)
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 中央处理单元（CPU）
- en: Graphics Processing Units (GPU)
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图形处理单元（GPU）
- en: Specialized chips like Tensor Processing Units (TPU).
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 专用芯片，如张量处理单元（TPU）。
- en: 'Although this is an ever-evolving landscape of computing, two integral components
    have revolutionized the way we process data and execute complex tasks. These are
    the Central Processing Unit (CPU) and the Graphics Processing Unit (GPU). Both
    computing powerhouses are instrumental in propelling advancements in various fields:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这是一个不断发展的计算领域，但有两个关键组件彻底改变了我们处理数据和执行复杂任务的方式。这些组件是中央处理单元（CPU）和图形处理单元（GPU）。这两大计算强者在推动各个领域的进步方面起着至关重要的作用：
- en: Artificial intelligence (e.g. ChatGPT)
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 人工智能（例如ChatGPT）
- en: Scientific simulations (e.g. Finite Element Methods, CFD)
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 科学模拟（例如有限元方法、计算流体力学）
- en: Gaming
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 游戏
- en: Visual effects
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 视觉效果
- en: Therefore, understanding the unique capabilities and performance characteristics
    of CPUs and GPUs is crucial to harness their full potential and optimizing the
    entire system to the business requirements.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，理解CPU和GPU的独特能力和性能特征对于充分发挥它们的潜力以及优化整个系统以满足业务需求至关重要。
- en: 'When talking about reducing the cost of running computations, you should consider
    the following aspects:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 当谈到降低计算运行成本时，你应该考虑以下方面：
- en: Hardware Cost
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 硬件成本
- en: Power Consumption
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 功耗
- en: Performance Efficiency
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 性能效率
- en: Maintenance and Upgrades
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 维护和升级
- en: Regarding purchase cost, CPUs are more affordable than medium- and high-end
    GPUs. However, GPUs consume more energy than CPUs due to their higher core counts
    and memory bandwidth. So if you design a simple IoT device powered from a 5V battery,
    you’ll probably focus on the power consumption and low hardware cost — then CPU
    (or even IC or MC) would be the best choice.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 关于购买成本，CPU比中高端GPU更实惠。然而，由于GPU具有更高的核心数量和内存带宽，它们的能耗比CPU高。因此，如果你设计的是一个由5V电池供电的简单物联网设备，你可能会关注功耗和低硬件成本——那么CPU（甚至IC或MC）将是最佳选择。
- en: Also, there is an option of using public cloud resources from, e.g. Google,
    Amazon, or Azure to pay only for the usage time. It is the best choice if you
    design a web service or your hardware requires high computational power but is
    restricted by size or a need for remote access and control. However, in mass-production
    devices like smartphones or smartwatches, hardware cost and power consumption
    still play a decisive role.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 另外，还有一个选项是使用来自例如Google、Amazon或Azure等公共云资源，只为使用时间付费。如果你设计的是一个Web服务，或者你的硬件需要高计算能力但受限于尺寸或需要远程访问和控制，那么这将是最佳选择。然而，在像智能手机或智能手表这样的量产设备中，硬件成本和功耗仍然发挥着决定性作用。
- en: 'Now, when considering computational costs, the total expense usually is a function
    of the time required to complete the task end-to-end. A good example is a training
    of a neural network (NN). It is a very computationally intensive task itself.
    From the end-to-end perspective, there are additional steps like data preparation
    and experiments tracking (for the hyperparameters optimization). It creates an
    additional overhead. However, in the phase of development, the training is still
    the bottleneck, so most popular ML frameworks (e.g. pytorch, Keras) support GPU
    calculations to tackle this problem. It is a classical case for utilizing the
    capabilities of GPU — the training of NN is ideal for massive parallelization.
    It is due to its underlying implementation. However, the inference itself (after
    the model is trained) can be done often on the CPU or even Microcontroller. The
    bottleneck after training may be then on the data preparation side (memory side)
    or I/O operations. For both CPUs are often more suitable. That’s why there are
    even dedicated microprocessors for such kind of tasks (e.g. Intel Atom® Processors
    x7000E). So finally, we came here to two different optimal solutions, depending
    on the environment: development (GPU) and production (CPU).'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: As you see, while GPUs excel in heavy parallel processing, there are several
    situations where CPUs outperform them from the end-to-end perspective. It depends
    on the nature of the algorithms and business requirements. If you’re a software
    developer or a system designer/architect, knowing these situations is crucial
    to deliver the optimal solution.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: In this article, we will explore some of such scenarios.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: '**Single-threaded recursive algorithms**'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: There are algorithms that per design are not a subject of parallelization —
    recursive algorithms. In recursion, the current value depends on the previous
    values — one simple but clear example is the algorithm to calculate the Fibonacci
    number. An exemplary implementation is below. It is impossible in this case to
    break the chain of calculations and run them in parallel.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: Another example of such algorithm is a recursive calculation of a factorial
    (see below).
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: '**Memory-Intensive Tasks**'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: There are tasks where the memory access time is a bottleneck, not computations
    themselves. CPUs usually have larger cache sizes (fast memory access element)
    than GPUs and have faster memory subsystems which allow them to excel at manipulating
    frequently accessed data. A simple example can be an element-wise addition of
    large arrays.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: However, in many cases, popular frameworks (like Pytorch) will perform such
    calculations on GPU faster by moving the objects to the GPU’s memory and parallelizing
    operations under the hood.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: We can create a process where we initialize arrays in RAM and move them to the
    GPU for calculations. This additional overhead of transferring data causes end-to-end
    processing time to be longer than when running it directly on the CPU.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: That’s when we usually use so-called CUDA-enabled arrays — in this case, using
    Pytorch. You must only make sure that your GPU can handle this size of data. To
    give you an overview — typical, popular GPUs have a memory size of 2–6GB VRAM,
    while the high-end ones have up to 24GB VRAM (GeForce RTX 4090).
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: '**Other Non-parallelizable Algorithms**'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: 'There is a group of algorithms that are not recursive but still cannot be parallelized.
    Some examples are:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: Gradient Descent — used in optimization tasks and machine learning
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hash-chaining — used in cryptography
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The Gradient Descent cannot be parallelized in its vanilla form, because it
    is a sequential algorithm. Every iteration (called a step) depends on the results
    of the previous one. There are, however, some studies on how to implement this
    algorithm in a parallel manner. To learn more check:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: '[HOGWILD!: A Lock-Free Approach to Parallelizing Stochastic Gradient Descent](https://arxiv.org/abs/1106.5730)'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Parallel Stochastic Gradient Descent with Sound Combiners](https://arxiv.org/pdf/1705.08030.pdf)'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'An example of the Hash-chaining algorithm you can find here: [https://www.geeksforgeeks.org/c-program-hashing-chaining/](https://www.geeksforgeeks.org/c-program-hashing-chaining/)'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: '**Small tasks**'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: Another case when CPUs are a better choice is when the data size is very small.
    In such situations, the overhead of transferring data between the RAM and GPU
    memory (VRAM) can outweigh the benefit of GPU parallelism. This is because of
    the very fast access to the CPU cache. It was mentioned previously in a section
    related to memory-intensive tasks.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: Also, some tasks are simply too small and although the calculations can be run
    in parallel, the benefit to the end user is not visible. In such cases running
    on GPU generates only the additional hardware-related costs.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: 'That’s why in IoT, GPUs are not commonly used. Typical IoT tasks are:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: to capture some sensor data and send them over
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: to activate other devices (lights, alarms, motors, etc.) after detecting a signal
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: However, in this field GPUs are still used in so-called edge-computing tasks.
    These are the situations when you have to acquire and process data directly at
    its source instead of sending them over the Internet for heavy processing. A good
    example is [iFACTORY](https://www.bmwgroup.com/en/news/general/2023/BMWGroupIT)
    developed by BMW.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: '**Task with small level of parallelization**'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: There are numerous use cases where you have to run the code in parallel but
    due to the speed of CPU it is enough to parallelize the process using multi-core
    CPU. GPU excel in situations where you need a massive parallelization (hundreds
    or thousands of parallel operations). In cases where you find that, e.g. 4x or
    6x speed up is enough you can reduce costs by running the code on CPU, each process
    on different core. Nowadays, manufacturers of CPU offer them with between 2 and
    18 cores (e.g. Intel Core i9–9980XE Extreme Edition Processor).
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: '**Summary**'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: 'Overall, the rule of thumb when choosing between CPU and GPU is to answer these
    main questions:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 总体来说，选择CPU和GPU的经验法则是回答以下主要问题：
- en: Can a CPU handle the entire task within the required time?
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: CPU是否能够在要求的时间内完成整个任务？
- en: Can my code be parallelized?
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我的代码可以并行处理吗？
- en: Can I fit all the data on a GPU? If not does it introduce a heave overhead?
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我能将所有数据放入GPU吗？如果不能，这是否会引入较大的开销？
- en: To answer these questions, its crucial to understand well both how your algorithms
    work and what are the business requirements now and how can they change in the
    future.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 要回答这些问题，关键在于深入理解你的算法是如何工作的，以及当前的业务需求是什么，这些需求未来可能会如何变化。
