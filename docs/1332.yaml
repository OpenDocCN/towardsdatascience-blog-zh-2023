- en: Integrating Neo4j into the LangChain ecosystem
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/integrating-neo4j-into-the-langchain-ecosystem-df0e988344d2?source=collection_archive---------1-----------------------#2023-04-17](https://towardsdatascience.com/integrating-neo4j-into-the-langchain-ecosystem-df0e988344d2?source=collection_archive---------1-----------------------#2023-04-17)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Learn how to develop a LangChain agent that has multiple ways of interacting
    with the Neo4j database
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://bratanic-tomaz.medium.com/?source=post_page-----df0e988344d2--------------------------------)[![Tomaz
    Bratanic](../Images/d5821aa70918fcb3fc1ff0013497b3d5.png)](https://bratanic-tomaz.medium.com/?source=post_page-----df0e988344d2--------------------------------)[](https://towardsdatascience.com/?source=post_page-----df0e988344d2--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----df0e988344d2--------------------------------)
    [Tomaz Bratanic](https://bratanic-tomaz.medium.com/?source=post_page-----df0e988344d2--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F57f13c0ea39a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintegrating-neo4j-into-the-langchain-ecosystem-df0e988344d2&user=Tomaz+Bratanic&userId=57f13c0ea39a&source=post_page-57f13c0ea39a----df0e988344d2---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----df0e988344d2--------------------------------)
    ·15 min read·Apr 17, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fdf0e988344d2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintegrating-neo4j-into-the-langchain-ecosystem-df0e988344d2&user=Tomaz+Bratanic&userId=57f13c0ea39a&source=-----df0e988344d2---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fdf0e988344d2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintegrating-neo4j-into-the-langchain-ecosystem-df0e988344d2&source=-----df0e988344d2---------------------bookmark_footer-----------)![](../Images/79e7c6ccc5293a96177b339b15d4db39.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Photo by [Alex Knight](https://unsplash.com/@agk42?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: '*Update: The so-called Cypher Search, where the LLM generates a Cypher statement
    to query the Neo4j database, has been integrated directly to the LangChain library.
    Learn more* [*here*](/langchain-has-added-cypher-search-cb9d821120d5)'
  prefs: []
  type: TYPE_NORMAL
- en: '*2nd update: Vector search is now supported directly by vector index in Neo4j,
    so I have changed the vector search code to use the new index introduced in 5.11*'
  prefs: []
  type: TYPE_NORMAL
- en: ChatGPT inspired the world and started a new AI revolution. However, it seems
    that the latest trend is supplying ChatGPT with external information to increase
    its accuracy and give it the ability to answer questions where the answers are
    not present in public datasets. Another trend around large language models (LLMs)
    is to turn them into agents, where they have an ability to interact with their
    environment through various API calls or other integrations.
  prefs: []
  type: TYPE_NORMAL
- en: Since enhancing LLMs is relatively new, there aren’t a lot of open-source libraries
    yet. However, it seems that the go-to library for building applications around
    LLMs like ChatGPT is called [LangChain](https://python.langchain.com/en/latest/index.html).
    The library provides the ability to enhance an LLM by giving it access to various
    tools and external data sources. Not only can it improve its responses by accessing
    external data, but it can also act as an agent and manipulate its environment
    through external endpoints.
  prefs: []
  type: TYPE_NORMAL
- en: I randomly stumbled upon a LangChain project by [Ibis Prevedello](https://medium.com/u/fd610570f1c7?source=post_page-----df0e988344d2--------------------------------)
    that uses graph search to enhance the LLMs by providing additional external context.
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://github.com/ibiscp/LLM-IMDB?source=post_page-----df0e988344d2--------------------------------)
    [## GitHub - ibiscp/LLM-IMDB: Proof of concept app using LangChain and LLMs to
    retrieve information…'
  prefs: []
  type: TYPE_NORMAL
- en: Proof of concept app using LangChain and LLMs to retrieve information from graphs,
    built with the IMDB dataset - GitHub…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: github.com](https://github.com/ibiscp/LLM-IMDB?source=post_page-----df0e988344d2--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: The project by Ibis uses [NetworkX library](https://networkx.org/) to store
    the graph information. I really liked his approach and how easy it was to integrate
    graph search into the LangChain ecosystem. Therefore, I have decided to develop
    a project that would integrate [Neo4j](https://neo4j.com/), a graph database,
    into the LangChain ecosystem.
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://github.com/tomasonjo/langchain2neo4j?source=post_page-----df0e988344d2--------------------------------)
    [## GitHub - tomasonjo/langchain2neo4j: Integrating Neo4j database into langchain
    ecosystem'
  prefs: []
  type: TYPE_NORMAL
- en: The Langchain2Neo4j is a proof of concept application of how to integrate Neo4j
    into the Langchain ecosystem. This…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: github.com](https://github.com/tomasonjo/langchain2neo4j?source=post_page-----df0e988344d2--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: 'After two weeks of coding, the project now allows a LangChain agent to interact
    with Neo4j in three different modes:'
  prefs: []
  type: TYPE_NORMAL
- en: Generating Cypher statements to query the database
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Full-text keyword search of relevant entities
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vector similarity search
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this blog post, I will walk you through the reasoning and implementation
    of each approach I developed.
  prefs: []
  type: TYPE_NORMAL
- en: Environment setup
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: First, we will configure the Neo4j environment. We will use the dataset available
    as the [recommendations project](https://sandbox.neo4j.com/?usecase=recommendations)
    in the Neo4j sandbox. The easiest solution is simply to create a Neo4j Sandbox
    instance by following [this link](https://sandbox.neo4j.com/?usecase=recommendations).
    However, if you would prefer a local instance of Neo4j, you can also restore a
    [database dump that is available on GitHub](https://github.com/neo4j-graph-examples/recommendations/tree/main/data).
    The dataset is part of the [MovieLens datasets](https://grouplens.org/datasets/movielens/)
    [1], specifically the small version.
  prefs: []
  type: TYPE_NORMAL
- en: After the Neo4j database is instantiated, we should have a graph with the following
    schema populated.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7c83401d58a5c1a58e6cdc6d5294af0b.png)'
  prefs: []
  type: TYPE_IMG
- en: Graph schema. Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, you need to clone the langchain2neo4j repository by executing the following
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: In the next step, you need to create an `.env` file and populate the neo4j and
    OpenAI credentials as shown in the `.env.example` file.
  prefs: []
  type: TYPE_NORMAL
- en: 'Lastly, you need to create a full-text index in Neo4j and import movie title
    embeddings by running:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: If you are a Windows user, the `seed_db` script probably won’t work. In that
    case, I have prepared a Jupyter notebook that can help you seed the database as
    an alternative to the shell script.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s jump to the LangChain integration.
  prefs: []
  type: TYPE_NORMAL
- en: LangChain agent
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As far as I have seen, the most common data flow of using a LangChain agent
    to answer a user question is the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7f905a41e80ab8cd9127f4d3ce7b1e7c.png)'
  prefs: []
  type: TYPE_IMG
- en: LangChain agent flow. Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: The agent data flow is initiated when it receives input from a user. The agent
    then sends a request to an LLM model that includes the user question along with
    the agent prompt, which is a set of instructions in a natural language the agent
    should follow. In turn, the LLM responds with further instructions to the agent.
    Most often, the first response is to use any available tools to gain additional
    information from external sources. However, tools are not limited to read-only
    operations. For example, you could use them to update a database. After the tool
    returns additional context, another call is made to an LLM that includes the newly
    gained information. The LLM now has the option to produce a final answer that
    is returned to a user, or it can decide it needs to perform more actions through
    its available tools.
  prefs: []
  type: TYPE_NORMAL
- en: A LangChain agent uses LLMs for its reasoning. Therefore, the first step is
    to define which model to use. At the moment, the langchain2neo4j project supports
    only OpenAI’s chat completion models, specifically GPT-3.5-turbo, and GPT-4 models.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: I haven’t yet explored other LLMs besides OpenAI’s. However, with LangChain,
    it should be easy, as it has integration with more than ten other LLMs. I didn’t
    know that that many existed.
  prefs: []
  type: TYPE_NORMAL
- en: '[## Integrations'
  prefs: []
  type: TYPE_NORMAL
- en: Edit description
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: python.langchain.com](https://python.langchain.com/en/latest/modules/models/llms/integrations.html?source=post_page-----df0e988344d2--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we need to add a conversational memory with the following line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'LangChain support [multiple types of agents](https://python.langchain.com/en/latest/modules/agents/agents.html).
    For example, some agents can use the memory component, while others cannot. Since
    the object was to build a chatbot, I chose the [Conversation Agent (for Chat Models)
    agent](https://python.langchain.com/en/latest/modules/agents/agents/examples/chat_conversation_agent.html)
    type. What is interesting about the LangChain library is that half the code is
    written in Python, while the other half is prompt engineering. We can explore
    the [prompts that the conversational agent uses](https://github.com/hwchase17/langchain/blob/master/langchain/agents/conversational_chat/prompt.py).
    For example, the agents has some basic instructions it must follow:'
  prefs: []
  type: TYPE_NORMAL
- en: Assistant is a large language model trained by OpenAI. Assistant is designed
    to be able to assist with a wide range of tasks, from answering simple questions
    to providing in-depth explanations and discussions on a wide range of topics.
    As a language model, Assistant is able to generate human-like text based on the
    input it receives, allowing it to engage in natural-sounding conversations and
    provide responses that are coherent and relevant to the topic at hand. Assistant
    is constantly learning and improving, and its capabilities are constantly evolving.
    It is able to process and understand large amounts of text, and can use this knowledge
    to provide accurate and informative responses to a wide range of questions. Additionally,
    Assistant is able to generate its own text based on the input it receives, allowing
    it to engage in discussions and provide explanations and descriptions on a wide
    range of topics. Overall, Assistant is a powerful system that can help with a
    wide range of tasks and provide valuable insights and information on a wide range
    of topics. Whether you need help with a specific question or just want to have
    a conversation about a particular topic, Assistant is here to assist.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Additionally, the agent has instructions to use any of the specified tools if
    needed.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Interestingly, the prompt states that the assistant can ask the user to look
    up additional information using tools. However, the user is not a human but an
    application built on top of the LangChain library. Therefore, the entire process
    of finding further information is done automatically without any human in the
    loop. Of course, we can change the prompts if needed. The prompt also includes
    the format the LLMs should use to communicate with the agent.
  prefs: []
  type: TYPE_NORMAL
- en: '*Note that the agent prompt doesn’t include that the agent shouldn’t answer
    a question if the answer is not provided in the context returned by tools.*'
  prefs: []
  type: TYPE_NORMAL
- en: Now, all we have to do is to define the available tools. As mentioned, I have
    prepared three methods of interacting with Neo4j database.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: The description of a tool is used to specify the capabilities of the tool as
    well as to inform the agent when to use it. Additionally, we need to specify the
    format of the input a tool expects. For example, both the Cypher and vector search
    expect a full question as an input, while the keyword search expects a list of
    relevant movies as input.
  prefs: []
  type: TYPE_NORMAL
- en: LangChain is quite different from what I am used to in coding. It uses prompts
    to instruct the LLMs to do the work for you instead of coding it yourself. For
    example, the keyword search instructs the ChatGPT to extract relevant movies and
    use that as input. I spent 2 hours debugging the tool input format before realizing
    I could specify it using natural language, and the LLM will handle the rest.
  prefs: []
  type: TYPE_NORMAL
- en: Remember how I mentioned that the agent doesn’t have instructions that it shouldn’t
    answer questions where the information is not provided in the context? Let’s examine
    the following dialogue.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/379650f0d2d96d1a4aefa9d50b1d3db7.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: The LLM decided that based on the tool descriptions, it cannot use any of them
    to retrieve relevant context. However, the LLM knows a lot by default, and since
    the agent has no constraints that it should only rely on external sources, the
    LLM can form the answer independently. We would need to change the agent prompt
    if we wanted to enforce different behavior.
  prefs: []
  type: TYPE_NORMAL
- en: Generating Cypher statements
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: I have already developed a chatbot interacting with a Neo4j database by generating
    Cypher statements using the OpenAI’s [conversational models like the GPT-3.5-turbo
    and GPT-4](https://medium.com/neo4j/context-aware-knowledge-graph-chatbot-with-gpt-4-and-neo4j-d3a99e8ae21e).
    Therefore, I could borrow most of the ideas to implement a tool that allows the
    LangChain agent to retrieve information from the Neo4j database by constructing
    Cypher statements.
  prefs: []
  type: TYPE_NORMAL
- en: The older models like text-davinci-003 and GPT-3.5-turbo work better as a few-shot
    Cypher generator, where we provide a couple of Cypher examples that the model
    can use to generate new Cypher statements. However, it seems the GPT-4 works well
    when we only present the graph schema. Consequently, since graph schema can be
    extracted with a Cypher query, the GPT-4 can be theoretically used on any graph
    schema without any manual work required by a human.
  prefs: []
  type: TYPE_NORMAL
- en: I won’t walk you through what LangChain does under the hood. We will just look
    at the function that gets executed when the LangChain agents decides to interact
    with the Neo4j database using Cypher statements.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: The Cypher generating tool gets the question along with the chat history as
    input. The input to the LLM is then combined by using the **system** message,
    **chat history**, and the current question. I have prepared the following **system**
    message prompt for the Cypher generating tool.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Prompt engineering feels more like art than science. In this example, we provide
    the LLM with a couple of Cypher statement examples and let it generate Cypher
    statements based on that information. Additionally, we place a couple of constraints,
    like allowing it to construct only Cypher statements that could be inferred from
    training examples. Additionally, we don’t let the model apologize or explain its
    thoughts (however, GPT-3.5-turbo won’t listen to that instructions). Finally,
    if the question lacks context, we allow the model to respond with that information
    instead of forcing it to generate Cypher statements.
  prefs: []
  type: TYPE_NORMAL
- en: After the LLM construct a Cypher statements, we simply use it to query a Neo4j
    database, and return the results to the Agent. Here is an example flow.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d1b1003ba12bf55aa2646189c90598a5.png)'
  prefs: []
  type: TYPE_IMG
- en: Flow of the agent using a Cypher generating tool. Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: When a user inputs their question, it gets sent to an LLM along with the agent
    prompt. In this example, the LLM responds that it needs to use the **Cypher search**
    tool. The Cypher search tool constructs a Cypher statement and uses it to query
    Neo4j. The results of the query are then passed back to the agent. Next, the agent
    sends another request to an LLM along with the new context. As the context contains
    the needed information to construct an answer, the LLM forms the final answer
    and instructs the agent to return it to the user.
  prefs: []
  type: TYPE_NORMAL
- en: Of course, we can now ask follow up questions.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/dd80db68f71b28fd477cb57d6e86231a.png)'
  prefs: []
  type: TYPE_IMG
- en: Follow up question. Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: Since the agent has memory, it is aware of who is the second actor and, therefore,
    can pass the information along to the Cypher search tool to construct appropriate
    Cypher statements.
  prefs: []
  type: TYPE_NORMAL
- en: Keyword search of relevant triples
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: I got the idea for keyword search from existing knowledge graph index implementations
    in both [LangChain](https://python.langchain.com/en/latest/modules/chains/index_examples/graph_qa.html?highlight=graph)
    and [GPT-index libraries](https://gpt-index.readthedocs.io/en/latest/reference/indices/kg_query.html).
    Both implementations are fairly similar. They ask an LLM to extract relevant entities
    from a question and search the graph for any triples that contain those entities.
    So I figured we could do something similar with Neo4j. However, while we could
    search for entities with a simple **MATCH** statement, I have decided that using
    Neo4j’s full-text index would be better. After relevant entities are found using
    the full-text index, we return the triples and hope the relevant information to
    answer the question is there.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Remember, the agent has instructions to parse out relevant movie titles already
    and use that as input to the Keyword search tool. Therefore, we don’t have to
    deal with that. However, since multiple entities could exist in the question,
    we must construct appropriate Lucene query parameters as the full-text index is
    based on Lucene. Then, we simply query the full-text index and return hopefully
    relevant triples. The Cypher statement we use is the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: So, we take the top five relevant entities returned by the full-text index.
    Next, we generate triples by traversing to their neighbors. I have specifically
    excluded the **RATED** relationships from being traversed because they contain
    irrelevant information. I haven’t explored it, but I have a good feeling we could
    also instruct the LLM to provide a list of relevant relationships to be investigated
    along with the appropriate entities, which would make our keyword search more
    focused. The keyword search can be initiated by explicitly instructing the agent.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/56fabe719ae3f80dcaaab4b30fd7d113.png)'
  prefs: []
  type: TYPE_IMG
- en: Keyword search flow. Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: The LLM is instructed to use the **keyword search** tool. Additionally, the
    agent is told to provide the keywords search a list of relevant entities as input,
    which is only **Pokemon** in this example. The Lucene parameter is then used to
    query Neo4j. This approach casts a broader net and hopes the extracted triples
    contain relevant information. For example, the retrieved context includes information
    on the genre of Pokemon, which is irrelevant. Still, it also has information about
    who acted in the movie, which allows the agent to answer the user’s question.
  prefs: []
  type: TYPE_NORMAL
- en: '*As mentioned, we could instruct the LLM to produce a list of relevant relationship
    types along with appropriate entities, which could help the agent retrieve more
    relevant information.*'
  prefs: []
  type: TYPE_NORMAL
- en: Vector similarity search
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The vector similarity search is the last mode to interact with a Neo4j database
    we will examine. Vector search is trendy at the moment. For example, LangChain
    offers [integrations with more than ten vector databases](https://python.langchain.com/en/latest/modules/indexes/vectorstores.html).
    Additionally, Neo4j has [added a vector index in their version 5.11](https://neo4j.com/blog/vector-search-deeper-insights/),
    which we will be using in this example. The idea behind vector similarity search
    is to embed a question into embedding space and find relevant documents based
    on the similarity of the embeddings of the question and documents. We only need
    to be careful to use the same embedding model to produce the vector representation
    of documents and the question. I have used the OpenAI’s embeddings in the vector
    search implementation.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'So, the first thing we do is embed the question. Next, we use the embedding
    to find relevant movies in the database. Usually, the vector databases return
    the text of a relevant document. However, we are dealing with a graph database.
    Therefore, I have decided to produce relevant information using the triple structure.
    The Cypher statement used is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: The Cypher statement is similar to the keyword search example. The only difference
    is that we use vector index instead of a full-text index to identify relevant
    movies.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/670b64f57b90aa96243df90b31c1dc60.png)'
  prefs: []
  type: TYPE_IMG
- en: Vector search flow. Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: When an agent is instructed to use the **vector search** tool, the first step
    is to embed the question as a vector. The OpenAI’s embedding model produces vector
    representations with a dimension of 1536\. So, the next step is to use the constructed
    vector and search for relevant information in the database using the vector index.
    Again, since we are dealing with a graph database, I have decided to return the
    information to the agent in the form of a triple.
  prefs: []
  type: TYPE_NORMAL
- en: What is interesting about vector search is that even though we instructed the
    agent to search for the **Lord of the Ring** movies, the vector similarity search
    also returned information about the **Hobbit** movies. It looks like that Lord
    of the Ring and Hobbit movies are close in the embedded space, which is understandable.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: It looks like chatbots and generative agents that can access external tools
    and information are the next wave that follows the original ChatGPT hype. Having
    the ability to provide additional context to an LLM can greatly improve its results.
    Additionally, the agent’s tools are not restricted to read-only operations, which
    means they can update a database or even make orders on Amazon. For the most part,
    it seems that the LangChain library is the primary library at the moment to be
    used to implement generative agents. When you start using LangChain, you might
    need a bit of a shift in the coding process, as you need to combine LLM prompts
    with code to complete tasks. For example, messages between LLMs and tools can
    be shaped and reshaped with natural language instructions as prompts instead of
    Python code. I hope this project will help you implement the capabilities of a
    graph database like Neo4j into your LangChain project.
  prefs: []
  type: TYPE_NORMAL
- en: As always, the code is available on [GitHub](https://github.com/tomasonjo/langchain2neo4j).
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] *F. Maxwell Harper and Joseph A. Konstan. 2015\. The MovieLens Datasets:
    History and Context. ACM Transactions on Interactive Intelligent Systems (TiiS)
    5, 4: 19:1–19:19\.* [*https://doi.org/10.1145/2827872*](https://doi.org/10.1145/2827872)'
  prefs: []
  type: TYPE_NORMAL
