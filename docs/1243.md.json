["```py\n>>> dataset.get_data()[0]\n\n         attr1     attr2     attr3     attr4     attr5     attr6     attr7     attr8  ...   attr293   attr294  Beach  Sunset  FallFoliage  Field  Mountain  Urban\n0     0.646467  0.666435  0.685047  0.699053  0.652746  0.407864  0.150309  0.535193  ...  0.014025  0.029709      1       0            0      0         1      0\n1     0.770156  0.767255  0.761053  0.745630  0.742231  0.688086  0.708416  0.757351  ...  0.082672  0.036320      1       0            0      0         0      1\n2     0.793984  0.772096  0.761820  0.762213  0.740569  0.734361  0.722677  0.849128  ...  0.112506  0.083924      1       0            0      0         0      0\n3     0.938563  0.949260  0.955621  0.966743  0.968649  0.869619  0.696925  0.953460  ...  0.049780  0.090959      1       0            0      0         0      0\n4     0.512130  0.524684  0.520020  0.504467  0.471209  0.417654  0.364292  0.562266  ...  0.164270  0.184290      1       0            0      0         0      0\n...        ...       ...       ...       ...       ...       ...       ...       ...  ...       ...       ...    ...     ...          ...    ...       ...    ...\n2402  0.875782  0.901653  0.926227  0.721366  0.795826  0.867642  0.794125  0.899067  ...  0.254413  0.134350      0       0            0      0         0      1\n2403  0.657706  0.669877  0.692338  0.713920  0.727374  0.750354  0.684372  0.718770  ...  0.048747  0.041638      0       0            0      0         0      1\n2404  0.952281  0.944987  0.905556  0.836604  0.875916  0.957034  0.953938  0.967956  ...  0.017547  0.019734      0       0            0      0         0      1\n2405  0.883990  0.899004  0.901019  0.904298  0.846402  0.858145  0.851362  0.852472  ...  0.226332  0.223070      0       0            0      0         0      1\n2406  0.974915  0.866425  0.818144  0.936140  0.938583  0.935087  0.930597  1.000000  ...  0.025059  0.004033      0       0            0      0         0      1\n\n[2407 rows x 300 columns]\n```", "```py\nimport numpy as np\nimport openml\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import cross_val_score\nimport plotly.express as px\nfrom plotly.subplots import make_subplots\nimport dimod\nfrom dwave.system import LeapHybridCQMSampler\n\ndataset = openml.datasets.get_dataset(312)\n\nX, y, categorical_indicator, attribute_names = dataset.get_data(\n    target=dataset.default_target_attribute, dataset_format=\"dataframe\"\n)\nX = X.astype(float)\ny = y.values.astype(int)\n\nclf = RandomForestClassifier()\n```", "```py\ndef evaluate_model(m, X, y, indices=None):\n    if not indices is None:\n        X_filtered = X.iloc[:, indices]\n    else:\n        X_filtered = X\n    acc = np.mean(cross_val_score(clf, X_filtered, y, cv=5))\n    return acc\n\ndef show_relevance_redundancy(X, y, indices=None, title=\"\"):\n    if not indices is None:\n        X = X.iloc[:, indices].copy()\n    y = y\n    fig = make_subplots(\n        rows=1,\n        cols=2,\n        column_widths=[0.68, 0.32],\n        column_titles=[\"relevance\", \"redundancy\"],\n    )\n    trace_rel = px.bar(np.array([abs(np.corrcoef(x, y)[0, 1]) for x in X.values.T]))\n    trace_red = px.imshow(abs(np.corrcoef(X.values, rowvar=False)))\n    fig.add_trace(trace_rel.data[0], row=1, col=1)\n    fig.add_trace(trace_red.data[0], row=1, col=2)\n    fig.update_layout(width=1200, height=480, title=title)\n    fig.show()\n\nshow_relevance_redundancy(\n    X,\n    y,\n    None,\n    f\"full dataset: acc={evaluate_model(clf, X, y, None):.4f}\",\n)\n```", "```py\nk = 30\n# Pearson correlation\ncorrelation_matrix = abs(np.corrcoef(np.hstack((X, y[:, np.newaxis])), rowvar=False))\n# fix the alpha parameter from the Milne paper\n# to account for the numerous quadratic terms that are possible\nbeta = 0.5\nalpha = 2 * beta * (k - 1) / (1 - beta + 2 * beta * (k - 1))\n# generate weights for linear and quadratic terms, per Milne algorithm\nRxy = correlation_matrix[:, -1]\nQ = correlation_matrix[:-1, :-1] * (1 - alpha)\nnp.fill_diagonal(Q, -Rxy * alpha)\n# create binary quadratic model from the linear and quadratic weights\nbqm = dimod.BinaryQuadraticModel(Q, \"BINARY\")\n# create constrained quadratic model\ncqm = dimod.ConstrainedQuadraticModel()\n# the objective function of the CQM is the same as BQM\ncqm.set_objective(bqm)\n# constraint: limit the number of features to k\ncqm.add_constraint_from_iterable(\n    ((i, 1) for i in range(len(cqm.variables))), \"==\", rhs=k\n)\n# the sampler that will be used is the hybrid sampler in the DWave cloud\nsampler = LeapHybridCQMSampler()\n# solve the problem\nsampleset = sampler.sample_cqm(cqm)\n```", "```py\n# postprocess results\nfeasible = sampleset.filter(lambda s: s.is_feasible)\nif feasible:\n    best = feasible.first\nelse:\n    assert len(cqm.constraints) == 1\n    best = sorted(\n        sampleset.data(),\n        key=lambda x: (list(cqm.violations(x.sample).values())[0], x.energy),\n    )[0]\nassert list(best.sample.keys()) == sorted(best.sample.keys())\nis_selected = np.array([bool(val) for val in best.sample.values()])\nfeatures = np.array([i for i, val in enumerate(is_selected) if val])\nbest_energy = best.energy\n```", "```py\nshow_relevance_redundancy(\n    X,\n    y,\n    features,\n    f\"explicit optimization: acc={evaluate_model(clf, X, y, features):.4f}\",\n)\n```", "```py\nX_new = SelectFromQuadraticModel(num_features=30, alpha=0.5).fit_transform(X, y)\n```", "```py\nX_new_df = pd.DataFrame(data=X_new, columns=list(range(X_new.shape[1])))\n\nshow_relevance_redundancy(\n    X_new_df,\n    y,\n    None,\n    f\"plugin optimization: acc={evaluate_model(clf, X_new_df, y, None):.4f}\",\n)\n```"]