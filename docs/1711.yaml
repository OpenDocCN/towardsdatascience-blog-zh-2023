- en: A/B Optimization with Policy Gradient Reinforcement Learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用策略梯度强化学习进行A/B优化
- en: 原文：[https://towardsdatascience.com/a-b-optimization-with-policy-gradient-reinforcement-learning-b4a3527f849?source=collection_archive---------13-----------------------#2023-05-23](https://towardsdatascience.com/a-b-optimization-with-policy-gradient-reinforcement-learning-b4a3527f849?source=collection_archive---------13-----------------------#2023-05-23)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/a-b-optimization-with-policy-gradient-reinforcement-learning-b4a3527f849?source=collection_archive---------13-----------------------#2023-05-23](https://towardsdatascience.com/a-b-optimization-with-policy-gradient-reinforcement-learning-b4a3527f849?source=collection_archive---------13-----------------------#2023-05-23)
- en: A step by step visual explanation of the Policy Gradient method
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 关于策略梯度方法的逐步视觉解释
- en: '[](https://medium.com/@berndebenhoch?source=post_page-----b4a3527f849--------------------------------)[![Dr.
    Bernd Ebenhoch](../Images/77e5adff208c8d6771bffa633948e949.png)](https://medium.com/@berndebenhoch?source=post_page-----b4a3527f849--------------------------------)[](https://towardsdatascience.com/?source=post_page-----b4a3527f849--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----b4a3527f849--------------------------------)
    [Dr. Bernd Ebenhoch](https://medium.com/@berndebenhoch?source=post_page-----b4a3527f849--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@berndebenhoch?source=post_page-----b4a3527f849--------------------------------)[![Dr.
    Bernd Ebenhoch](../Images/77e5adff208c8d6771bffa633948e949.png)](https://medium.com/@berndebenhoch?source=post_page-----b4a3527f849--------------------------------)[](https://towardsdatascience.com/?source=post_page-----b4a3527f849--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----b4a3527f849--------------------------------)
    [Dr. Bernd Ebenhoch](https://medium.com/@berndebenhoch?source=post_page-----b4a3527f849--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F94ac20c7acbb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-b-optimization-with-policy-gradient-reinforcement-learning-b4a3527f849&user=Dr.+Bernd+Ebenhoch&userId=94ac20c7acbb&source=post_page-94ac20c7acbb----b4a3527f849---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----b4a3527f849--------------------------------)
    ·10 min read·May 23, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fb4a3527f849&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-b-optimization-with-policy-gradient-reinforcement-learning-b4a3527f849&user=Dr.+Bernd+Ebenhoch&userId=94ac20c7acbb&source=-----b4a3527f849---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F94ac20c7acbb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-b-optimization-with-policy-gradient-reinforcement-learning-b4a3527f849&user=Dr.+Bernd+Ebenhoch&userId=94ac20c7acbb&source=post_page-94ac20c7acbb----b4a3527f849---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----b4a3527f849--------------------------------)
    ·10分钟阅读·2023年5月23日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fb4a3527f849&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-b-optimization-with-policy-gradient-reinforcement-learning-b4a3527f849&user=Dr.+Bernd+Ebenhoch&userId=94ac20c7acbb&source=-----b4a3527f849---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb4a3527f849&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-b-optimization-with-policy-gradient-reinforcement-learning-b4a3527f849&source=-----b4a3527f849---------------------bookmark_footer-----------)![](../Images/35554205db5a22f7406d75391dbc6dfa.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb4a3527f849&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-b-optimization-with-policy-gradient-reinforcement-learning-b4a3527f849&source=-----b4a3527f849---------------------bookmark_footer-----------)![](../Images/35554205db5a22f7406d75391dbc6dfa.png)'
- en: Choose wisely! (Image created by the author with the help of Stable Diffusion
    Online)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 明智选择！（图像由作者借助Stable Diffusion Online创建）
- en: In this article, we will explore how policy gradient reinforcement learning
    can be used for A/B optimization. It is a simple demo to observe the policy gradient
    method, where we will deeply understand the underlying mechanism and visualize
    the learning process step by step.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们将探讨如何将策略梯度强化学习应用于A/B优化。这是一个简单的演示，以观察策略梯度方法，在这里我们将深入理解其基础机制，并逐步可视化学习过程。
- en: Introduction
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: Reinforcement learning is a fundamental concept in machine learning, along with
    supervised, self-supervised and unsupervised learning. In reinforcement learning,
    an agent tries to find the optimal set of actions within an environment to maximize
    a reward. Reinforcement learning has become widely known as a method that can
    beat the best players at Go and chess when combined with neural networks as highly
    flexible agents.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习是机器学习中的一个基本概念，与监督学习、自监督学习和无监督学习一样。在强化学习中，智能体试图在环境中找到最佳的一组动作以最大化奖励。强化学习因其与神经网络结合后作为高度灵活的智能体能够击败围棋和象棋的顶级玩家而广为人知。
- en: The neural network used as the agent learns to optimize the policy step by step
    by maximizing the reward obtained. Several strategies have been developed to update
    the parameters of the neural network, such as policy gradient, q-learning or actor
    critic learning. The policy gradient method is the closest to backpropagation,
    which is commonly used for supervised and self-supervised learning of neural networks.
    However, in reinforcement learning, we do not directly evaluate each action as
    we would in supervised learning, but rather we try to maximize the total reward
    and let the neural network decide the individual actions to take. The actions
    are chosen from a probability distribution, which provides a high degree of flexibility
    for exploration. At the beginning of the optimization, the actions are chosen
    randomly and the agent explores different policies. Over time, some actions prove
    to be more useful than others and the probability distribution collapses to unambiguous
    decisions. Unlike other reinforcement learning methods, the user does not have
    to control this balance between exploration and exploitation, the optimal balance
    is found by the gradient policy method itself.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 用作智能体的神经网络通过最大化获得的奖励逐步优化策略。已经开发出几种更新神经网络参数的策略，如策略梯度、Q学习或演员评论学习。策略梯度方法最接近于反向传播，这在神经网络的监督学习和自监督学习中常常使用。然而，在强化学习中，我们不会像在监督学习中那样直接评估每个动作，而是尝试最大化总奖励，并让神经网络决定采取的具体动作。这些动作从概率分布中选择，这为探索提供了高度的灵活性。在优化初期，动作是随机选择的，智能体探索不同的策略。随着时间的推移，一些动作证明比其他动作更有用，概率分布会收敛到明确的决策上。与其他强化学习方法不同，用户不需要控制探索与利用之间的平衡，最佳平衡由梯度策略方法本身找到。
- en: 'Often, the optimal policy to maximize the reward is achieved by a sequence
    of actions, where each action leads to a new state of the environment. However,
    the gradient policy method can also be used to find the optimal action that statistically
    gives the highest reward. Such a situation is often found when performing A/B
    optimization, which is a very common technique for choosing the better of two
    options. For example, in marketing, A/B tests are used to select the advertising
    scheme that leads to higher sales. Which ad would you rather click on? Option
    A: “*Get the most out of your data:**I am a professional Data Scientist — I can
    help you analyze your data*” or option B “*Struggling with your data?**Professional
    Data Analyst has free capacity to help you automate your data analysis*”?'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，为了最大化奖励，最佳策略是通过一系列动作实现的，每个动作都会导致环境的新状态。然而，梯度策略方法也可以用来找到统计上奖励最高的最佳动作。这种情况通常出现在进行A/B优化时，这是一种选择两个选项中较好者的常见技术。例如，在营销中，A/B测试用于选择能够带来更高销售的广告方案。你更愿意点击哪个广告？选项A：“*充分利用你的数据：**我是一名专业的数据科学家——我可以帮助你分析数据*”还是选项B：“*数据处理遇到困难？**专业数据分析师有空帮助你自动化数据分析*”？
- en: '![](../Images/1a6c94c9aa9bae1487c62f741ccd8230.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1a6c94c9aa9bae1487c62f741ccd8230.png)'
- en: Two ad creative options. Which one would you rather click? (Image created by
    the author)
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 两种广告创意选项。你更愿意点击哪一个？（图片由作者创作）
- en: The difficulty with A/B optimization is that the click rate is variable. For
    example, after seeing the ad on a website, each user may have different preferences,
    be in different moods, and therefore react differently. Because of this variability,
    we need statistical techniques to choose the better ad scheme. Common methods
    for comparing options A and B are hypothesis tests, such as t-tests. To perform
    a t-test, the two potential versions of the ad must be displayed for a period
    of time to collect click rates from users. In order to get a significant evaluation
    of the preferred ad scheme, a fairly long period of exploration is required, with
    the disadvantage of a potential loss of revenue since during exploration the better
    and worse ads are randomly displayed equally often. It is advantageous to maximize
    the click rate by showing the better ad more often as soon as possible. By performing
    the A/B optimization with the gradient policy method, the agent will first randomly
    explore both variants A and B, but will receive higher rewards for the ad that
    leads to higher click rates, so the agent will quickly learn to show the better
    ad to users more often and maximize the click rate and revenue.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: A/B优化的难点在于点击率是变量。例如，在网站上看到广告后，每个用户可能有不同的偏好、处于不同的心情，因此反应也会不同。由于这种变异性，我们需要统计技术来选择更好的广告方案。比较A和B选项的常见方法是假设检验，如t检验。要进行t检验，必须展示广告的两个潜在版本一段时间，以从用户那里收集点击率。为了获得对首选广告方案的显著评估，需要相当长时间的探索，这会带来潜在的收入损失，因为在探索期间，好的和差的广告会随机展示得一样频繁。尽快通过更频繁地展示更好的广告来最大化点击率是有利的。通过使用梯度策略方法进行A/B优化，代理首先会随机探索A和B两个变体，但对点击率更高的广告给予更高的奖励，因此代理会迅速学会更频繁地向用户展示更好的广告，从而最大化点击率和收入。
- en: Example
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 示例
- en: In our example, we have two ad creative options, where we assume that option
    A has a click probability of 30% and option B has a click probability of 40%.
    We run an ad campaign with 1000 ad impressions. If we perform exploration only
    and show both options equally often, we can expect an average click rate of 35%
    and an average of 350 clicks in total. If we knew that B would be clicked on more
    often, we would show only B and get an average of 400 clicks. However, if we were
    unlucky and chose to display only A, we would get an average of only 300 clicks.
    With the policy gradient method, which we will explore in more detail in a moment,
    we can achieve an average of 391 clicks, clearly showing that quickly applying
    the learned policy leads to a number of clicks that is almost as high as if we
    had chosen the better option B in the first place.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的例子中，我们有两个广告创意选项，假设选项A的点击概率为30%，选项B的点击概率为40%。我们运行一个包含1000次广告展示的广告活动。如果我们仅进行探索，并且两个选项展示的频率相同，我们可以期望平均点击率为35%，总点击数为350次。如果我们知道B的点击率会更高，我们只会展示B并获得平均400次点击。然而，如果我们运气不好，选择只展示A，我们将获得平均仅300次点击。使用政策梯度方法，我们可以实现平均391次点击，这清楚地表明，快速应用学习到的策略可以获得几乎与首次选择更好的B选项一样多的点击数。
- en: How it works — step by step
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何运作——逐步说明
- en: We run the A/B optimization with a gradient policy method on a small neural
    network using the TensorFlow library. First, we need some imports.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用TensorFlow库在一个小型神经网络上运行A/B优化，采用梯度策略方法。首先，我们需要进行一些导入。
- en: '[PRE0]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The neural network contains only one layer with a single neuron that decides
    which ad to show. Since we have no information about the user’s preference, location,
    time, or anything else, the decision is based on a zero input to the neural network
    and we do not need the non-linearity that we would achieve with large neural networks.
    Training is achieved by adjusting the bias of this neuron.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络仅包含一层单个神经元，该神经元决定展示哪个广告。由于我们没有关于用户偏好、位置、时间或其他任何信息，因此决策基于对神经网络的零输入，我们不需要大神经网络所实现的非线性。训练是通过调整该神经元的偏置来实现的。
- en: '[PRE1]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: A function is used to select the action, either displaying option A or option
    B, using the neural network. The function is decorated with tf.function(), which
    creates a static computation graph, allowing it to run much faster than in eager
    mode. Using TensorFlow’s GradientTape function, we collect gradients during the
    ad selection process. Each time a user enters the website, the neural network
    produces an output that is treated as the probability of selecting ad variant
    A or variant B to be presented to the user. Since we only have one neuron with
    sigmoid activation, the output is a single number between 0 and 1\. If the output
    is 0.5, there is a 50% chance that ad B will be displayed and a 50% chance that
    ad A will be displayed. If the output is 0.8, there is an 80% chance that ad B
    will be displayed and the remaining 20% chance that ad A will be displayed. The
    action is selected by comparing the output of the neural network to a uniformly
    distributed random number between 0 and 1\. If the random number is smaller than
    the output, the action is True (1) and ad B is selected; if the random number
    is greater than the output, the action is False (0) and ad A is selected. The
    loss value measures the difference between the output of the neural network and
    the selected action using binary_crossentropy_loss. Then we create the gradients
    of the loss with respect to the model parameters.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 使用神经网络的函数来选择动作，显示选项A或选项B。该函数装饰为tf.function()，这会创建一个静态计算图，使其运行速度比在急切模式下快得多。使用TensorFlow的GradientTape函数，我们在广告选择过程中收集梯度。每当用户进入网站时，神经网络会产生一个输出，这个输出被视为选择展示给用户的广告变体A或变体B的概率。由于我们只有一个带有sigmoid激活的神经元，输出是介于0和1之间的单一数字。如果输出为0.5，则广告B和广告A的展示机会各为50%。如果输出为0.8，则广告B的展示机会为80%，广告A的展示机会为20%。通过将神经网络的输出与介于0和1之间的均匀分布的随机数进行比较来选择动作。如果随机数小于输出，则动作为True（1），选择广告B；如果随机数大于输出，则动作为False（0），选择广告A。损失值使用binary_crossentropy_loss来衡量神经网络输出与所选动作之间的差异。然后我们创建损失相对于模型参数的梯度。
- en: '[PRE2]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: We run the training over 1000 ad impressions. In each steps, the ad is presented
    once and a new user has the chance to click the ad. To evaluate the learning process,
    we count the total number of clicks after this period. The learning rate is defined
    as 0.5\. We will discuss the influence of the learning rate on the total number
    of clicks later.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在1000次广告展示中进行训练。在每一步中，广告被展示一次，新的用户有机会点击广告。为了评估学习过程，我们计算这一阶段的点击总数。学习率定义为0.5。我们将稍后讨论学习率对点击总数的影响。
- en: '[PRE3]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Now, let us run the ad campaign. The neural network will improve its prediction
    over time. Through reinforcement learning, training and application occur simultaneously.
    In practice, the selected ad is now displayed on the website and we have to wait
    and see if the user clicks on the ad or leaves the website without clicking it.
    In the code, we simply simulate whether a user clicks or not. As described above,
    ad A has a 30% chance of being clicked, while ad B has a 40% chance of being clicked.
    The click can be treated directly as a reward to train the neural network. The
    reward is used to modify the gradients. If the user clicked the ad, the gradient
    of that action is left unchanged, but if the user did not click the ad, the gradient
    is reversed. Finally, gradient descent updates the parameters of the neural network
    by assigning new weight and bias values to the neural network.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们运行广告活动。神经网络将随着时间的推移改进其预测。通过强化学习，训练和应用同时进行。在实践中，所选广告现在会显示在网站上，我们需要等待并查看用户是否点击了广告或离开了网站而没有点击。在代码中，我们只是模拟用户是否点击。正如上面所述，广告A有30%的点击机会，而广告B有40%的点击机会。点击可以直接作为奖励来训练神经网络。奖励用于修改梯度。如果用户点击了广告，该动作的梯度保持不变；但如果用户没有点击广告，梯度则会被反转。最后，梯度下降通过为神经网络分配新的权重和偏置值来更新神经网络的参数。
- en: '[PRE4]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The following figure summarizes the evolution of the learning process.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 下图总结了学习过程的演变。
- en: '![](../Images/f1a0921652fa858c716b182754398ae5.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f1a0921652fa858c716b182754398ae5.png)'
- en: Evolution of the learning process of an A/B optimization with policy gradient
    reinforcement learning. (Image created by the author)
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 使用策略梯度强化学习进行A/B优化的学习过程演变。（图片由作者创建）
- en: In total, the campaign of 1000 ad impressions shown in the previous figure has
    resulted in a total of 393 clicks, which is quite close to 400, the number of
    clicks we would expect if we selected only the better ad B. We first review the
    learning process by observing all graphs at initial step = 1\. We observe that
    the neural network output starts at 0.5, resulting in a random selection of the
    ad with 50% probability for both ad B and ad A. The binary_crossentropy_loss measures
    the difference between the model output and the action taken. Since the action
    is either 0 or 1, the initial loss value is the negative logarithm of the model
    output 0.5, which is approximately 0.7\. Since we only have a single neuron in
    our neural network, the gradients contain two scalar values for the weight and
    bias of that neuron. If ad A is selected, the gradient of the bias is a positive
    number, and if ad B is selected, the gradient of the bias is a negative number.
    The gradient of the weight parameter is always zero because the input to the neural
    network is zero. The reward is highly random because there is only a 30% - 40%
    chance that the ad will be clicked. If the ad is clicked, we get a reward and
    the gradient is unchanged, otherwise we reverse the gradient. The adjusted gradients
    are multiplied by the learning rate and subtracted from the initial parameters
    of the neural network. We can see that the bias value starts at zero and becomes
    more negative when positive adjusted gradients are applied and more positive when
    negative adjusted gradients are applied.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在前图显示的1000次广告展示的活动中，总共得到了393次点击，这与我们如果仅选择更好的广告 B 预期的400次点击非常接近。我们首先通过观察所有图表在初始步骤
    = 1 来回顾学习过程。我们观察到神经网络输出从0.5开始，导致广告 B 和广告 A 的选择概率均为50%。binary_crossentropy_loss
    测量模型输出与实际采取的行动之间的差异。由于行动要么是0，要么是1，初始损失值为模型输出0.5的负对数，大约为0.7。由于我们在神经网络中只有一个神经元，梯度包含该神经元的权重和偏差的两个标量值。如果选择了广告
    A，偏差的梯度是正数；如果选择了广告 B，偏差的梯度是负数。权重参数的梯度始终为零，因为神经网络的输入为零。奖励是高度随机的，因为广告点击的概率只有30%-40%。如果广告被点击，我们得到奖励，梯度保持不变，否则我们会反转梯度。调整后的梯度乘以学习率并从神经网络的初始参数中减去。我们可以看到，偏差值从零开始，当应用正的调整梯度时变得更负，而当应用负的调整梯度时变得更正。
- en: During the ad campaign, the output of the neural network tends towards one,
    increasing the chance that ad B will be selected. However, even if the model output
    is already close to one, there is still a small chance to display ad A. As the
    model output approaches one, there is a small loss value if action B is selected
    and we obtain small negative gradients, but in the rarer cases that ad A is chosen,
    a larger loss value is obtained, visible as occasional peaks and as large positive
    gradients. After the rewards are collected, it is observed that some of these
    positive peaks are reversed in the adjusted gradients, as these actions did not
    lead to clicks. As ad B has a higher click probability, the small negative adjusted
    gradients are applied more often then the positive gradients stemming from clicks
    on ad A. Thus, the bias value of the model increases by small steps and on the
    rare occasions that ad A is selected and clicked, the bias value decreases. The
    output of the model is provided by the sigmoid function applied to the model bias
    value.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在广告活动期间，神经网络的输出趋向于一，从而增加了选择广告 B 的机会。然而，即使模型输出已经接近一，广告 A 仍有小概率被展示。当模型输出接近一时，如果选择了动作
    B，会有小的损失值和小的负梯度，但在选择广告 A 的少数情况下，会得到较大的损失值，表现为偶尔的峰值和较大的正梯度。在收集奖励后，观察到一些正峰值在调整梯度中被反转，因为这些动作没有带来点击。由于广告
    B 的点击概率较高，因此小的负调整梯度比广告 A 点击产生的正梯度更频繁地应用。因此，模型的偏差值逐步增加，而在稀有的情况下，如果广告 A 被选择并点击，偏差值会减少。模型的输出由应用于模型偏差值的
    sigmoid 函数提供。
- en: Influence of the learning rate
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 学习率的影响
- en: In this demo, we observed, that a neural network can learn to select the better
    of two options and apply that option more frequently to maximize rewards. On average
    391 clicks will be obtained in this setting, where ad A has a click probability
    of 30% and ad B has a click probability of 40%. In practice, these probabilities
    would be much lower, and the difference between them might be smaller, making
    it harder for the neural network to explore the better option. The policy gradient
    method has the advantage of automatically adjusting the balance between exploration
    and exploitation. However, this balance is affected by the learning rate. Higher
    learning rates would result in a shorter exploration phase and faster application
    of the learned policy, as shown in the following graph, where the learning rate
    is increased from 0.01 to 10\. The model output, averaged over 100 individual
    ad campagins, increases more rapidly with increasing learning rate up to a learning
    rate of 1\. However, at higher learning rates, there is a risk of adapting to
    the wrong action, which may appear better only during the short exploration period.
    At high learning rates the model output is adjusting too quickly, leading to unstable
    decisions.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在本演示中，我们观察到神经网络可以学习选择两个选项中的较优者，并更频繁地应用该选项以最大化奖励。在这种设置下，平均将获得391次点击，其中广告A的点击概率为30%，广告B的点击概率为40%。在实际操作中，这些概率会低得多，而且它们之间的差异可能会更小，使得神经网络更难探索更好的选项。策略梯度方法的优点在于自动调整探索与利用之间的平衡。然而，这一平衡会受到学习率的影响。较高的学习率将导致较短的探索阶段和更快地应用所学策略，如下图所示，学习率从0.01增加到10\.
    模型输出在100个单独广告活动中平均随着学习率的增加而更快地增长，直到学习率为1\. 然而，在较高的学习率下，存在适应错误行动的风险，这种行动可能仅在短暂的探索期内显得更好。在高学习率下，模型输出调整得过快，导致决策不稳定。
- en: '![](../Images/d0e047c6bdf059cac4ab0fde02115ae7.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d0e047c6bdf059cac4ab0fde02115ae7.png)'
- en: Influence of the learning rate on the output of the neural network. (Image created
    by the author)
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 学习率对神经网络输出的影响。（图像由作者创建）
- en: Thus, there is an optimal learning rate to choose, which may be difficult to
    find in practice since nothing is known about the click probabilities beforehand.
    Varying the learning rate from 0.01 to 10.0 shows that a broad maximum for the
    sum of clicks is obtained for learning rates between 0.1 and 2.0\. Higher learning
    rates clearly increase the standard deviation, demonstrating the instability of
    the learning process, also leading to a reduction of the average sum of clicks.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，存在一个最佳学习率需要选择，这在实践中可能很难找到，因为事先不知道点击概率。将学习率从0.01变化到10.0可以看到，在0.1到2.0之间的学习率可以获得点击总数的广泛最大值。较高的学习率明显增加了标准偏差，显示了学习过程的不稳定性，同时也导致了点击总数的减少。
- en: '![](../Images/136e9b184c0c9cd87c41d7894115256a.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/136e9b184c0c9cd87c41d7894115256a.png)'
- en: Influence of the learning rate on the total sum of clicks obtained during the
    ad campaign. (Image created by the author)
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 学习率对广告活动期间获得的总点击数的影响。（图像由作者创建）
- en: Summary
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: This demo shows how reinforcement learning can be used for A/B optimization.
    It is a simple example to illustrate the underlying process of the policy gradient
    method. We have learned how the neural network updates its parameters based on
    adjusted gradients depending on whether the selected ad was clicked. Applying
    the learned policy quickly maximizes the click rate. However, choosing the optimal
    learning rate can be difficult in practice.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 本演示展示了如何将强化学习用于A/B优化。它是一个简单的例子，用于说明策略梯度方法的基本过程。我们已经学习了神经网络如何根据调整后的梯度更新其参数，具体取决于所选择的广告是否被点击。应用所学策略可以迅速最大化点击率。然而，选择最佳学习率在实际操作中可能比较困难。
- en: 'You can find the complete code and a streamlit demo on huggingface.co: [https://huggingface.co/spaces/Bernd-Ebenhoch/AB_optimization](https://huggingface.co/spaces/Bernd-Ebenhoch/AB_optimization)'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在 huggingface.co 上找到完整的代码和一个 streamlit 演示：[https://huggingface.co/spaces/Bernd-Ebenhoch/AB_optimization](https://huggingface.co/spaces/Bernd-Ebenhoch/AB_optimization)
