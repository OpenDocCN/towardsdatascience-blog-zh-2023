["```py\npip install openai streamlit streamlit-chat\n```", "```py\nif 'messages' not in st.session_state:\n    st.session_state['messages'] = [\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}\n    ]\n\ndef generate_response(prompt):\n    st.session_state['messages'].append({\"role\": \"user\", \"content\": prompt})\n\n    completion = openai.ChatCompletion.create(\n        model=model,\n        messages=st.session_state['messages']\n    )\n    response = completion.choices[0].message.content\n    st.session_state['messages'].append({\"role\": \"assistant\", \"content\": response})\n```", "```py\nfrom streamlit_chat import message\n\nif st.session_state['generated']:\n    with response_container:\n        for i in range(len(st.session_state['generated'])):\n            message(st.session_state[\"past\"][i], is_user=True, key=str(i) + '_user')\n            message(st.session_state[\"generated\"][i], key=str(i))\n```", "```py\ntotal_tokens = completion.usage.total_tokens\nprompt_tokens = completion.usage.prompt_tokens\ncompletion_tokens = completion.usage.completion_tokens\n\nif model_name == \"GPT-3.5\":\n    cost = total_tokens * 0.002 / 1000\nelse:\n    cost = (prompt_tokens * 0.03 + completion_tokens * 0.06) / 1000\n\nst.write(\n    f\"Model used: {st.session_state['model_name'][i]}; Number of tokens: {st.session_state['total_tokens'][i]}; Cost: ${st.session_state['cost'][i]:.5f}\")\n```", "```py\nstreamlit run app.py\n```"]