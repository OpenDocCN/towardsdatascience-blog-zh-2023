- en: Pre-Training Context is All You Need
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 预训练上下文才是你所需要的
- en: 原文：[https://towardsdatascience.com/pre-training-context-is-all-you-need-f457ffa8a358?source=collection_archive---------3-----------------------#2023-11-27](https://towardsdatascience.com/pre-training-context-is-all-you-need-f457ffa8a358?source=collection_archive---------3-----------------------#2023-11-27)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/pre-training-context-is-all-you-need-f457ffa8a358?source=collection_archive---------3-----------------------#2023-11-27](https://towardsdatascience.com/pre-training-context-is-all-you-need-f457ffa8a358?source=collection_archive---------3-----------------------#2023-11-27)
- en: The driving force behind modern transformer models stems to a large extent from
    its pertaining data, allowing for strong in-context learning capabilities.
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 现代变换器模型的驱动力在很大程度上源于其相关数据，这使得强大的上下文学习能力成为可能。
- en: '[](https://medium.com/@benjamin.thuerer?source=post_page-----f457ffa8a358--------------------------------)[![Benjamin
    Thürer](../Images/b4c49698c7270c592bf992fc47f75765.png)](https://medium.com/@benjamin.thuerer?source=post_page-----f457ffa8a358--------------------------------)[](https://towardsdatascience.com/?source=post_page-----f457ffa8a358--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----f457ffa8a358--------------------------------)
    [Benjamin Thürer](https://medium.com/@benjamin.thuerer?source=post_page-----f457ffa8a358--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@benjamin.thuerer?source=post_page-----f457ffa8a358--------------------------------)[![Benjamin
    Thürer](../Images/b4c49698c7270c592bf992fc47f75765.png)](https://medium.com/@benjamin.thuerer?source=post_page-----f457ffa8a358--------------------------------)[](https://towardsdatascience.com/?source=post_page-----f457ffa8a358--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----f457ffa8a358--------------------------------)
    [Benjamin Thürer](https://medium.com/@benjamin.thuerer?source=post_page-----f457ffa8a358--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fcd27eb9661fd&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpre-training-context-is-all-you-need-f457ffa8a358&user=Benjamin+Th%C3%BCrer&userId=cd27eb9661fd&source=post_page-cd27eb9661fd----f457ffa8a358---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----f457ffa8a358--------------------------------)
    ·6 min read·Nov 27, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Ff457ffa8a358&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpre-training-context-is-all-you-need-f457ffa8a358&user=Benjamin+Th%C3%BCrer&userId=cd27eb9661fd&source=-----f457ffa8a358---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fcd27eb9661fd&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpre-training-context-is-all-you-need-f457ffa8a358&user=Benjamin+Th%C3%BCrer&userId=cd27eb9661fd&source=post_page-cd27eb9661fd----f457ffa8a358---------------------post_header-----------)
    发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----f457ffa8a358--------------------------------)
    ·6 min read·2023年11月27日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Ff457ffa8a358&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpre-training-context-is-all-you-need-f457ffa8a358&user=Benjamin+Th%C3%BCrer&userId=cd27eb9661fd&source=-----f457ffa8a358---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff457ffa8a358&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpre-training-context-is-all-you-need-f457ffa8a358&source=-----f457ffa8a358---------------------bookmark_footer-----------)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff457ffa8a358&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpre-training-context-is-all-you-need-f457ffa8a358&source=-----f457ffa8a358---------------------bookmark_footer-----------)'
- en: Generative Artificial Intelligence and its popular transformer models are advertised
    everywhere these days and new models are being released every hour (see [the inflation
    of AI](https://medium.com/towards-data-science/the-inflation-of-ai-is-more-always-better-8ea1be75e0aa)).
    In this rapidly evolving field of AI, the possibilities of values these models
    could bring seem to be endless. Large Language Models (LLM) like [chatGPT](http://chat.openai.com)
    already made it into every Engineers' pile of resources, writers use them to support
    their articles, and designers create the first visuals or seek inspiration from
    the outcome of computer vision models.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 生成型人工智能及其流行的变换器模型如今随处可见，每小时都有新模型发布（见 [AI的膨胀](https://medium.com/towards-data-science/the-inflation-of-ai-is-more-always-better-8ea1be75e0aa)）。在这个快速发展的人工智能领域，这些模型可能带来的价值似乎是无穷无尽的。像
    [chatGPT](http://chat.openai.com) 这样的语言模型已经成为每位工程师的资源库的一部分，作家用它们来支持他们的文章，设计师则利用计算机视觉模型的结果来创作首批视觉作品或寻求灵感。
- en: If it is not magic, what really powers these impressive transformer models?
  id: totrans-9
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 如果这不是魔法，那么究竟是什么让这些令人印象深刻的变换器模型运转？
- en: However, even though the achievements and usefulness are great and generative
    AI enhances productivity, it is important to recall that modern Machine Learning
    models (like LLMs or VisionTransformers) are not performing any magic at all (similar
    to the fact that ML, or statistical models in general, never have been magical).
    Even though the remarkable abilities of models might be perceived as *magic-like*
    and some experts in the field even talk about things like *hallucinations* of
    models, still, the foundation of every model is just math and statistical probabilities
    (sometimes complex, but still…
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，尽管这些成就和实用性非常高，生成式 AI 提高了生产力，但重要的是要记住，现代机器学习模型（如 LLMs 或 VisionTransformers）根本没有施展任何魔法（类似于
    ML 或统计模型总体上从未有过魔法）。即便模型的显著能力可能被认为是*类似魔法*的，一些领域的专家甚至谈论模型的*幻觉*，但每个模型的基础依然只是数学和统计概率（有时复杂，但仍然…
