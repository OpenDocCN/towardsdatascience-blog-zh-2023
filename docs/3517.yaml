- en: Pre-Training Context is All You Need
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/pre-training-context-is-all-you-need-f457ffa8a358?source=collection_archive---------3-----------------------#2023-11-27](https://towardsdatascience.com/pre-training-context-is-all-you-need-f457ffa8a358?source=collection_archive---------3-----------------------#2023-11-27)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The driving force behind modern transformer models stems to a large extent from
    its pertaining data, allowing for strong in-context learning capabilities.
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@benjamin.thuerer?source=post_page-----f457ffa8a358--------------------------------)[![Benjamin
    Thürer](../Images/b4c49698c7270c592bf992fc47f75765.png)](https://medium.com/@benjamin.thuerer?source=post_page-----f457ffa8a358--------------------------------)[](https://towardsdatascience.com/?source=post_page-----f457ffa8a358--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----f457ffa8a358--------------------------------)
    [Benjamin Thürer](https://medium.com/@benjamin.thuerer?source=post_page-----f457ffa8a358--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fcd27eb9661fd&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpre-training-context-is-all-you-need-f457ffa8a358&user=Benjamin+Th%C3%BCrer&userId=cd27eb9661fd&source=post_page-cd27eb9661fd----f457ffa8a358---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----f457ffa8a358--------------------------------)
    ·6 min read·Nov 27, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Ff457ffa8a358&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpre-training-context-is-all-you-need-f457ffa8a358&user=Benjamin+Th%C3%BCrer&userId=cd27eb9661fd&source=-----f457ffa8a358---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff457ffa8a358&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpre-training-context-is-all-you-need-f457ffa8a358&source=-----f457ffa8a358---------------------bookmark_footer-----------)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: Generative Artificial Intelligence and its popular transformer models are advertised
    everywhere these days and new models are being released every hour (see [the inflation
    of AI](https://medium.com/towards-data-science/the-inflation-of-ai-is-more-always-better-8ea1be75e0aa)).
    In this rapidly evolving field of AI, the possibilities of values these models
    could bring seem to be endless. Large Language Models (LLM) like [chatGPT](http://chat.openai.com)
    already made it into every Engineers' pile of resources, writers use them to support
    their articles, and designers create the first visuals or seek inspiration from
    the outcome of computer vision models.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: If it is not magic, what really powers these impressive transformer models?
  id: totrans-9
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 如果这不是魔法，那么究竟是什么让这些令人印象深刻的变换器模型运转？
- en: However, even though the achievements and usefulness are great and generative
    AI enhances productivity, it is important to recall that modern Machine Learning
    models (like LLMs or VisionTransformers) are not performing any magic at all (similar
    to the fact that ML, or statistical models in general, never have been magical).
    Even though the remarkable abilities of models might be perceived as *magic-like*
    and some experts in the field even talk about things like *hallucinations* of
    models, still, the foundation of every model is just math and statistical probabilities
    (sometimes complex, but still…
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，尽管这些成就和实用性非常高，生成式 AI 提高了生产力，但重要的是要记住，现代机器学习模型（如 LLMs 或 VisionTransformers）根本没有施展任何魔法（类似于
    ML 或统计模型总体上从未有过魔法）。即便模型的显著能力可能被认为是*类似魔法*的，一些领域的专家甚至谈论模型的*幻觉*，但每个模型的基础依然只是数学和统计概率（有时复杂，但仍然…
