- en: Pre-Training Context is All You Need
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/pre-training-context-is-all-you-need-f457ffa8a358?source=collection_archive---------3-----------------------#2023-11-27](https://towardsdatascience.com/pre-training-context-is-all-you-need-f457ffa8a358?source=collection_archive---------3-----------------------#2023-11-27)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The driving force behind modern transformer models stems to a large extent from
    its pertaining data, allowing for strong in-context learning capabilities.
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@benjamin.thuerer?source=post_page-----f457ffa8a358--------------------------------)[![Benjamin
    Thürer](../Images/b4c49698c7270c592bf992fc47f75765.png)](https://medium.com/@benjamin.thuerer?source=post_page-----f457ffa8a358--------------------------------)[](https://towardsdatascience.com/?source=post_page-----f457ffa8a358--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----f457ffa8a358--------------------------------)
    [Benjamin Thürer](https://medium.com/@benjamin.thuerer?source=post_page-----f457ffa8a358--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fcd27eb9661fd&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpre-training-context-is-all-you-need-f457ffa8a358&user=Benjamin+Th%C3%BCrer&userId=cd27eb9661fd&source=post_page-cd27eb9661fd----f457ffa8a358---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----f457ffa8a358--------------------------------)
    ·6 min read·Nov 27, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Ff457ffa8a358&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpre-training-context-is-all-you-need-f457ffa8a358&user=Benjamin+Th%C3%BCrer&userId=cd27eb9661fd&source=-----f457ffa8a358---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff457ffa8a358&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpre-training-context-is-all-you-need-f457ffa8a358&source=-----f457ffa8a358---------------------bookmark_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: Generative Artificial Intelligence and its popular transformer models are advertised
    everywhere these days and new models are being released every hour (see [the inflation
    of AI](https://medium.com/towards-data-science/the-inflation-of-ai-is-more-always-better-8ea1be75e0aa)).
    In this rapidly evolving field of AI, the possibilities of values these models
    could bring seem to be endless. Large Language Models (LLM) like [chatGPT](http://chat.openai.com)
    already made it into every Engineers' pile of resources, writers use them to support
    their articles, and designers create the first visuals or seek inspiration from
    the outcome of computer vision models.
  prefs: []
  type: TYPE_NORMAL
- en: If it is not magic, what really powers these impressive transformer models?
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: However, even though the achievements and usefulness are great and generative
    AI enhances productivity, it is important to recall that modern Machine Learning
    models (like LLMs or VisionTransformers) are not performing any magic at all (similar
    to the fact that ML, or statistical models in general, never have been magical).
    Even though the remarkable abilities of models might be perceived as *magic-like*
    and some experts in the field even talk about things like *hallucinations* of
    models, still, the foundation of every model is just math and statistical probabilities
    (sometimes complex, but still…
  prefs: []
  type: TYPE_NORMAL
