- en: 'Constructing a Decision Tree Classifier: A Comprehensive Guide to Building
    Decision Tree Models from Scratch'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 《构建决策树分类器：从头构建决策树模型的全面指南》
- en: 原文：[https://towardsdatascience.com/constructing-a-decision-tree-classifier-a-comprehensive-guide-to-building-decision-tree-models-2e59959db22d?source=collection_archive---------13-----------------------#2023-03-29](https://towardsdatascience.com/constructing-a-decision-tree-classifier-a-comprehensive-guide-to-building-decision-tree-models-2e59959db22d?source=collection_archive---------13-----------------------#2023-03-29)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/constructing-a-decision-tree-classifier-a-comprehensive-guide-to-building-decision-tree-models-2e59959db22d?source=collection_archive---------13-----------------------#2023-03-29](https://towardsdatascience.com/constructing-a-decision-tree-classifier-a-comprehensive-guide-to-building-decision-tree-models-2e59959db22d?source=collection_archive---------13-----------------------#2023-03-29)
- en: Gain insight into the fundamental processes involved in building a decision
    tree classifier from the ground
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 了解从头构建决策树分类器所涉及的基本过程
- en: '[](https://suhas-maddali007.medium.com/?source=post_page-----2e59959db22d--------------------------------)[![Suhas
    Maddali](../Images/933f27eab8ba9ee1f06ed2f24746d788.png)](https://suhas-maddali007.medium.com/?source=post_page-----2e59959db22d--------------------------------)[](https://towardsdatascience.com/?source=post_page-----2e59959db22d--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----2e59959db22d--------------------------------)
    [Suhas Maddali](https://suhas-maddali007.medium.com/?source=post_page-----2e59959db22d--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://suhas-maddali007.medium.com/?source=post_page-----2e59959db22d--------------------------------)[![Suhas
    Maddali](../Images/933f27eab8ba9ee1f06ed2f24746d788.png)](https://suhas-maddali007.medium.com/?source=post_page-----2e59959db22d--------------------------------)[](https://towardsdatascience.com/?source=post_page-----2e59959db22d--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----2e59959db22d--------------------------------)
    [苏哈斯·马达利](https://suhas-maddali007.medium.com/?source=post_page-----2e59959db22d--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F2a74f90399ae&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fconstructing-a-decision-tree-classifier-a-comprehensive-guide-to-building-decision-tree-models-2e59959db22d&user=Suhas+Maddali&userId=2a74f90399ae&source=post_page-2a74f90399ae----2e59959db22d---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----2e59959db22d--------------------------------)
    ·7 min read·Mar 29, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F2e59959db22d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fconstructing-a-decision-tree-classifier-a-comprehensive-guide-to-building-decision-tree-models-2e59959db22d&user=Suhas+Maddali&userId=2a74f90399ae&source=-----2e59959db22d---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F2a74f90399ae&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fconstructing-a-decision-tree-classifier-a-comprehensive-guide-to-building-decision-tree-models-2e59959db22d&user=Suhas+Maddali&userId=2a74f90399ae&source=post_page-2a74f90399ae----2e59959db22d---------------------post_header-----------)
    发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----2e59959db22d--------------------------------)
    ·7分钟阅读·2023年3月29日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F2e59959db22d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fconstructing-a-decision-tree-classifier-a-comprehensive-guide-to-building-decision-tree-models-2e59959db22d&user=Suhas+Maddali&userId=2a74f90399ae&source=-----2e59959db22d---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F2e59959db22d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fconstructing-a-decision-tree-classifier-a-comprehensive-guide-to-building-decision-tree-models-2e59959db22d&source=-----2e59959db22d---------------------bookmark_footer-----------)![](../Images/20a9591784f2a489089a025aa9089eb8.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F2e59959db22d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fconstructing-a-decision-tree-classifier-a-comprehensive-guide-to-building-decision-tree-models-2e59959db22d&source=-----2e59959db22d---------------------bookmark_footer-----------)![](../Images/20a9591784f2a489089a025aa9089eb8.png)'
- en: Photo by [Jeroen den Otter](https://unsplash.com/@jeroendenotter?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 照片由 [Jeroen den Otter](https://unsplash.com/@jeroendenotter?utm_source=medium&utm_medium=referral)
    提供，来自 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: '**Decision trees** serve various purposes in machine learning, including classification,
    regression, feature selection, anomaly detection, and reinforcement learning.
    They operate using straightforward **if-else** statements until the tree’s depth
    is reached. Grasping certain key concepts is crucial to fully comprehend the inner
    workings of a decision tree.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '**决策树**在机器学习中有多种用途，包括分类、回归、特征选择、异常检测和强化学习。它们通过简单的**if-else**语句进行操作，直到达到树的深度。理解某些关键概念对于全面理解决策树的内部工作原理至关重要。'
- en: Two critical concepts to grasp when exploring decision trees are **entropy**
    and **information gain**. Entropy quantifies the impurity within a set of training
    examples. A training set containing only one class exhibits an entropy of 0, while
    a set with an equal distribution of examples from all classes has an entropy of
    1\. Information gain, conversely, represents the decrease in entropy or impurity
    achieved by dividing the training examples into subsets based on a specific attribute.
    A strong comprehension of these concepts is valuable for understanding the inner
    mechanics of decision trees.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 探索决策树时需要掌握两个关键概念：**熵**和**信息增益**。熵量化了训练样本集中的杂质。一个只包含一个类别的训练集的熵为0，而一个包含所有类别样本均匀分布的集合的熵为1。信息增益则表示通过根据特定属性将训练样本划分为子集所获得的熵或杂质的减少。对这些概念的深入理解对于理解决策树的内部机制非常有价值。
- en: We will develop a **decision tree class** and define essential attributes required
    for making predictions. As mentioned earlier, entropy and information gain are
    calculated for each feature before deciding on which attribute to split. In the
    training phase, nodes are divided, and these values are considered during the
    inference phase for making predictions. We will examine how this is accomplished
    by going through the code segments.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将开发一个**决策树类**并定义进行预测所需的基本属性。如前所述，熵和信息增益是在决定哪个属性进行拆分之前计算的。在训练阶段，节点被拆分，这些值在推断阶段用于进行预测。我们将通过查看代码片段来检查这是如何完成的。
- en: Code Implementation of Decision Tree Classifier
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 决策树分类器的代码实现
- en: The initial step involves creating a decision tree class, incorporating methods
    and attributes in subsequent code segments. This article primarily emphasizes
    constructing decision tree classifiers from the ground up to facilitate a clear
    comprehension of complex models’ inner mechanisms. Here are some considerations
    to keep in mind when developing a decision tree classifier.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 初步步骤包括创建一个决策树类，在随后的代码段中添加方法和属性。本文主要强调从头开始构建决策树分类器，以便清晰理解复杂模型的内部机制。以下是开发决策树分类器时需要考虑的一些事项。
- en: '**Defining a Decision Tree Class**'
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**定义决策树类**'
- en: In this code segment, we define a decision tree class with a **constructor**
    that accepts values for max_depth, min_samples_split, and min_samples_leaf. The
    **max_depth** attribute denotes the maximum depth at which the algorithm can cease
    node splitting. The **min_samples_split** attribute considers the minimum number
    of samples required for node splitting. The **min_samples_leaf** attribute specifies
    the total number of samples in the leaf nodes, beyond which the algorithm is restricted
    from further division. These hyperparameters, along with others not mentioned,
    will be utilized later in the code when we define additional methods for various
    functionalities.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个代码段中，我们定义了一个决策树类，具有一个**构造函数**，接受max_depth、min_samples_split和min_samples_leaf的值。**max_depth**属性表示算法可以停止节点拆分的最大深度。**min_samples_split**属性考虑节点拆分所需的最小样本数量。**min_samples_leaf**属性指定叶节点中的样本总数，超过此数量算法将限制进一步的分裂。这些超参数以及其他未提及的参数将在稍后定义各种功能的方法时使用。
- en: '**Entropy**'
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**熵**'
- en: This concept pertains to the **uncertainty** or **impurity** present in the
    data. It is employed to identify the optimal split for each node by calculating
    the overall information gain achieved through the split.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 这个概念涉及数据中存在的**不确定性**或**杂质**。它用于通过计算通过拆分所获得的整体信息增益来识别每个节点的最佳拆分点。
- en: This code computes the overall entropy based on the count of samples for each
    category in the output samples. It is important to note that the output variable
    may have more than two categories (multi-class), making this model applicable
    for multi-class classification as well. Next, we will incorporate a method for
    calculating **information gain,** which aids the model in splitting examples based
    on this value. The following code snippet outlines the sequence of steps executed.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码根据输出样本中每个类别的样本计数计算整体熵。需要注意的是，输出变量可能有两个以上类别（多类别），使得该模型同样适用于多类别分类。接下来，我们将引入一种计算**信息增益**的方法，该方法帮助模型根据此值拆分示例。以下代码片段概述了执行的步骤顺序。
- en: '**Information Gain**'
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**信息增益**'
- en: A **threshold** is defined below, which divides the data into left and right
    nodes. This process is carried out for all feature indexes to identify the best
    fit. Subsequently, the resulting entropy from the split is recorded, and the difference
    is returned as the total information gain resulting from the split for a specific
    feature. The final step involves creating a **split_node** function that executes
    the splitting operation for all features based on the information gain derived
    from the split.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 下定义了一个**阈值**，它将数据分为左右两个节点。这个过程对所有特征索引进行，以识别最佳拟合。随后，记录拆分后得到的熵差异，并返回作为特定特征拆分的总信息增益。最后一步涉及创建一个**split_node**函数，该函数基于从拆分中得出的信息增益对所有特征执行拆分操作。
- en: '**Split Node**'
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**拆分节点**'
- en: We initiated the process by defining key hyperparameters such as `max_depth`and
    `min_samples_leaf`. These factors play a crucial role in the `split_node` method
    as they determine if further splitting should occur. For instance, when the tree
    reaches its maximum depth or when the minimum number of samples is met, data splitting
    ceases.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过定义关键超参数如`max_depth`和`min_samples_leaf`来启动这一过程。这些因素在`split_node`方法中发挥着至关重要的作用，因为它们决定是否应进一步拆分。例如，当树达到最大深度或满足最小样本数时，数据拆分将停止。
- en: Once the minimum samples and maximum tree depth conditions are satisfied, the
    next step involves **identifying** the feature that offers the highest information
    gain from the split. To achieve this, we iterate through all features, calculating
    the total entropy and information gain resulting from the split based on each
    feature. Ultimately, the feature yielding the maximum information gain serves
    as a reference for dividing the data into left and right nodes. This process continues
    until the tree’s depth is reached and the minimum number of samples are accounted
    for during the split.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦满足最小样本数和最大树深度条件，下一步是**识别**从拆分中提供最高信息增益的特征。为此，我们遍历所有特征，计算每个特征拆分所产生的总熵和信息增益。最终，提供最大信息增益的特征作为将数据分为左右节点的参考。这一过程持续进行，直到树的深度达到并在拆分过程中考虑到最小样本数。
- en: '**Fitting the Model**'
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**模型拟合**'
- en: Moving forward, we employ the previously defined methods to fit our model. The
    `split_node` function is **instrumental** in computing the entropy and information
    gain derived from partitioning the data into two subsets based on different features.
    As a result, the tree attains its maximum depth, allowing the model to acquire
    a feature representation that streamlines the inference process.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们利用之前定义的方法来拟合我们的模型。`split_node`函数在计算通过不同特征将数据划分为两个子集所产生的熵和信息增益方面**至关重要**。结果是，树达到了其最大深度，使得模型获得了优化的特征表示，从而简化了推断过程。
- en: The `split_node` function accepts a set of attributes, including input data,
    output, and depth, which is a **hyperparameter**. The function traverses the decision
    tree based on its initial training with the training data, identifying the optimal
    set of conditions for splitting. As the tree is traversed, factors such as depth,
    minimum number of samples, and minimum number of leaves play a role in determining
    the final prediction.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '`split_node`函数接受一组属性，包括输入数据、输出和深度，这是一个**超参数**。该函数基于初始训练数据遍历决策树，识别拆分的最佳条件。随着树的遍历，深度、最小样本数和最小叶子数等因素在确定最终预测时发挥作用。'
- en: Once the decision tree is constructed with the appropriate hyperparameters,
    it can be employed to make predictions for unseen or test data points. In the
    following sections, we will explore how the model handles predictions for new
    data, utilizing the **well-structured** decision tree generated by the `split_node`
    function.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦使用适当的超参数构建了决策树，就可以用它来对未见或测试数据点进行预测。在接下来的章节中，我们将探讨模型如何利用由`split_node`函数生成的**结构良好的**决策树处理新数据的预测。
- en: Defining Predict Function
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 定义预测函数
- en: We are going to define the `predict` function that accepts the **input** and
    makes predictions for every instance. Based on the threshold value that was defined
    earlier to make the split, the model would **traverse** through the tree until
    the outcome is obtained for the test set. Finally, predictions are returned in
    the form of arrays to the users.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将定义`predict`函数，该函数接受**输入**并对每个实例做出预测。基于先前定义的阈值进行拆分，模型会**遍历**树，直到为测试集获得结果。最后，预测以数组形式返回给用户。
- en: This `predict` method serves as a **decision-making** function for a decision
    tree classifier. It starts by initializing an empty list, `y_pred`, to store the
    predicted class labels for a given set of input values. The algorithm then iterates
    over each input example, setting the current node to the decision tree's root.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 这个`predict`方法作为决策树分类器的**决策功能**。它开始时初始化一个空列表`y_pred`，以存储给定输入值集的预测类别标签。然后，算法遍历每个输入示例，将当前节点设置为决策树的根节点。
- en: As the algorithm navigates the tree, it encounters dictionary-based nodes containing
    crucial information about each feature. This information helps the algorithm decide
    whether to **traverse** towards the left or right child node, depending on the
    feature value and the specified threshold. The traversal process continues until
    a leaf node is reached.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 当算法遍历树时，它会遇到包含每个特征关键信息的基于字典的节点。这些信息帮助算法决定是**向左**还是**向右**子节点移动，取决于特征值和指定的阈值。遍历过程继续，直到到达叶子节点。
- en: Upon reaching a leaf node, the predicted class label is appended to the `y_pred`
    list. This procedure is repeated for every input example, generating a list of
    predictions. Finally, the list of predictions is converted into a **NumPy** array,
    providing the predicted class labels for each test data point in the input.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 达到叶子节点后，预测的类别标签会被追加到`y_pred`列表中。这个过程对每个输入示例重复，生成一个预测列表。最后，预测列表会被转换成**NumPy**数组，为每个测试数据点提供预测的类别标签。
- en: Visualization
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 可视化
- en: In this subsection, we will examine the output of a decision tree regressor
    model applied to a dataset for estimating **AirBnb** housing prices. It is important
    to note that analogous plots can be generated for various cases, with the **tree’s
    depth** and other **hyperparameters** indicating the complexity of the decision
    tree.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在本小节中，我们将检验应用于估计**AirBnb**房价的数据集的决策树回归模型的输出。需要注意的是，可以为各种情况生成类似的图表，其中**树的深度**和其他**超参数**表示决策树的复杂性。
- en: In this section, we emphasize the **interpretability** of machine learning (ML)
    models. With the burgeoning demand for ML across various industries, it is essential
    to not overlook the importance of model interpretability. Rather than treating
    these models as black boxes, it is vital to develop tools and techniques that
    unravel their inner workings and elucidate the rationale behind their predictions.
    By doing so, we foster trust in ML algorithms and ensure responsible integration
    into a wide range of applications.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们强调**机器学习（ML）模型的可解释性**。随着各行各业对机器学习需求的激增，不能忽视模型可解释性的重要性。与其将这些模型视为黑箱，不如开发工具和技术来揭示它们的内部工作原理，并阐明其预测背后的理由。通过这样做，我们培养对机器学习算法的信任，并确保它们在各种应用中的负责任整合。
- en: 'Note: The dataset was taken from [New York City Airbnb Open Data | Kaggle](https://www.kaggle.com/datasets/dgomonov/new-york-city-airbnb-open-data)
    under [Creative Commons — CC0 1.0 Universal](https://creativecommons.org/publicdomain/zero/1.0/)
    License'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：数据集取自[纽约市Airbnb开放数据 | Kaggle](https://www.kaggle.com/datasets/dgomonov/new-york-city-airbnb-open-data)，根据[创作共用
    — CC0 1.0 全球](https://creativecommons.org/publicdomain/zero/1.0/)许可协议。
- en: '![](../Images/c6f9fdfacf6db3c21bae23f5cac23422.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c6f9fdfacf6db3c21bae23f5cac23422.png)'
- en: Decision Tree Regressor (Image by Author)
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树回归模型（图片由作者提供）
- en: Decision tree regressors and classifiers are renowned for their **interpretability**,
    offering valuable insights into the **rationale** behind their predictions. This
    clarity fosters trust and confidence in model predictions by aligning them with
    domain knowledge and enhancing our understanding. Moreover, it enables opportunities
    for debugging and addressing ethical and legal considerations.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树回归器和分类器因其**可解释性**而闻名，提供了对其预测背后**理由**的宝贵洞察。这种清晰度通过与领域知识对齐来增强对模型预测的信任和信心，并提高我们的理解。此外，它还提供了调试的机会，并处理伦理和法律问题。
- en: After conducting hyperparameter tuning and optimization, the optimal tree depth
    for the **AirBnb** home price prediction problem was determined to be **2**. Utilizing
    this depth and visualizing the results, features such as the Woodside neighborhood,
    longitude, and Midland Beach neighborhood emerged as the most significant factors
    in predicting AirBnb housing prices.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在进行超参数调整和优化后，**AirBnb** 房价预测问题的最佳树深度被确定为 **2**。利用这一深度并可视化结果时，Woodside 邻里、经度和Midland
    Beach 邻里等特征被发现是预测 AirBnb 房价最重要的因素。
- en: Conclusion
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结论
- en: Upon completing this article, you should possess a **robust** understanding
    of decision tree model mechanics. Gaining insights into the model’s implementation
    from the ground up can prove invaluable, particularly when employing **scikit-learn**
    models and their hyperparameters. Additionally, you can customize the model by
    adjusting the threshold or other hyperparameters to enhance performance. Thank
    you for investing your time in reading this article.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 完成本文后，你应该对决策树模型的机制有一个**坚实**的理解。从头到尾了解模型的实现尤为重要，特别是在使用**scikit-learn**模型及其超参数时。此外，你可以通过调整阈值或其他超参数来定制模型，从而提高性能。感谢你花时间阅读这篇文章。
- en: '*Below are the ways where you could contact me or take a look at my work.*'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '*以下是你可以联系我或查看我工作的方式。*'
- en: '***GitHub:***[*suhasmaddali (Suhas Maddali ) (github.com)*](https://github.com/suhasmaddali)'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '***GitHub:***[*suhasmaddali (Suhas Maddali ) (github.com)*](https://github.com/suhasmaddali)'
- en: '***YouTube:***[*https://www.youtube.com/channel/UCymdyoyJBC_i7QVfbrIs-4Q*](https://www.youtube.com/channel/UCymdyoyJBC_i7QVfbrIs-4Q)'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '***YouTube:***[*https://www.youtube.com/channel/UCymdyoyJBC_i7QVfbrIs-4Q*](https://www.youtube.com/channel/UCymdyoyJBC_i7QVfbrIs-4Q)'
- en: '***LinkedIn:***[*(1) Suhas Maddali, Northeastern University, Data Science |
    LinkedIn*](https://www.linkedin.com/in/suhas-maddali/)'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '***LinkedIn:***[*(1) Suhas Maddali, Northeastern University, Data Science |
    LinkedIn*](https://www.linkedin.com/in/suhas-maddali/)'
- en: '***Medium:*** [*Suhas Maddali — Medium*](https://suhas-maddali007.medium.com/)'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '***Medium:*** [*Suhas Maddali — Medium*](https://suhas-maddali007.medium.com/)'
- en: '***Kaggle:***[*Suhas Maddali | Contributor | Kaggle*](https://www.kaggle.com/suhasmaddali007)'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '***Kaggle:***[*Suhas Maddali | Contributor | Kaggle*](https://www.kaggle.com/suhasmaddali007)'
