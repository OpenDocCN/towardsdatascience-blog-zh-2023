# 梯度下降算法及其背后的直觉

> 原文：[https://towardsdatascience.com/the-gradient-descent-algorithm-4d54e0d446cd?source=collection_archive---------6-----------------------#2023-08-19](https://towardsdatascience.com/the-gradient-descent-algorithm-4d54e0d446cd?source=collection_archive---------6-----------------------#2023-08-19)

## 对梯度下降方法的技术描述，并配有算法运行的图示

[](https://medium.com/@fyi.oamd?source=post_page-----4d54e0d446cd--------------------------------)[![Antonieta Mastrogiuseppe](../Images/3b9e70a54fcb887f5ccee6d305085675.png)](https://medium.com/@fyi.oamd?source=post_page-----4d54e0d446cd--------------------------------)[](https://towardsdatascience.com/?source=post_page-----4d54e0d446cd--------------------------------)[![Towards Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----4d54e0d446cd--------------------------------) [Antonieta Mastrogiuseppe](https://medium.com/@fyi.oamd?source=post_page-----4d54e0d446cd--------------------------------)

·

[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fa8ee237975ec&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-gradient-descent-algorithm-4d54e0d446cd&user=Antonieta+Mastrogiuseppe&userId=a8ee237975ec&source=post_page-a8ee237975ec----4d54e0d446cd---------------------post_header-----------) 发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----4d54e0d446cd--------------------------------) ·9 分钟阅读·2023年8月19日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F4d54e0d446cd&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-gradient-descent-algorithm-4d54e0d446cd&user=Antonieta+Mastrogiuseppe&userId=a8ee237975ec&source=-----4d54e0d446cd---------------------clap_footer-----------)

--

[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4d54e0d446cd&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-gradient-descent-algorithm-4d54e0d446cd&source=-----4d54e0d446cd---------------------bookmark_footer-----------)![](../Images/6e6a647dfadc374fcf9425097033183a.png)

亚瑟·叔本华的名言：“一旦你越过山丘，你会开始加速”。照片由作者拍摄。

1.  **引入一些关键定义**

在优化方法中，尤其是在一阶算法类型中，您肯定听说过一种被称为梯度下降的方法。它是一阶优化类型，因为它需要一阶导数，即梯度。通过优化，梯度下降旨在最小化“实际”输出和模型预测输出之间的差异，这种差异由目标函数（即成本函数）测量。梯度或斜率被定义为在该函数（弯曲或直线）在给定点处绘制的线的方向。迭代地，梯度下降旨在在不同点对成本函数进行微分，从而推导出这些点上方向变化的程度，并朝向最陡下降的方向，即局部最小值。正如其名字所示，*梯度* 用作寻找局部最小值的*下降*方向，在那里优化的成本函数的参数值被最小化，从而达到最低值。

梯度下降主要用于（包括但不限于）训练机器学习模型和深度学习模型，后者基于神经网络结构类型。从线性回归和逻辑回归到神经网络，梯度下降旨在计算函数的最佳参数值。在其最简单的形式中，梯度下降旨在通过推导独立变量的参数的最佳值来最小化下面线性回归的误差项。这是，

y = β0 + β1 * X1 + … βk * Xk + Ɛ

其中，

*y* 是因变量

*k* 是独立变量的数量

*X* 是独立变量

β 参数

*Ɛ* 是误差项成分

在更复杂的形式中，梯度下降最常被定义为训练深度学习模型时的优化器，特别是在编译阶段。深度学习基于一个互连的网络进行不断的学习和改进，这个网络被称为神经网络。神经网络受到人脑的启发，是由人工神经元（称为节点）组成的高度复杂的网络。在最上层，节点在处理和分析来自前一层节点的数据，并将其传递到下一层节点方面扮演着重要角色。在神经网络中，权重，即优化的参数，是节点之间的联系。它们连接了输入/特征和隐藏层，因此它们表示了特定特征在预测最终输出时的重要性。找到单个权重的最佳值取决于许多权重的值。而这种优化是同时进行的，在深度神经网络中，即使是数百万个权重也可能会大大增加。在这里，梯度下降在涉及大量计算时表现得非常高效，这些计算基于神经网络的三个主要层：1）输入层 2）隐藏层 和 3）输出层。

有大量文献详细阐述和扩展了诸如深度学习方法及估计函数参数值的方法，进而扩展了梯度下降和普通最小二乘法（OLS）之间的差异，例如在线性回归的情况下。由于这不是本文的重点，读者可以进一步调查和扩展，以便更好地理解这些方法论。

**2\. 计算时间！**

为了更好地理解梯度下降，我们需要扩展对可微分函数的定义。一个函数，明确地说是 *ƒ(x)*，当其导数在该函数的曲线上的任意点都可以定义时，该函数就是可微分的。这就是说，对于函数 *ƒ(x)* 的定义域中的 *所有* 点。这里，有两个概念支持这个定义：一阶导数和二阶导数。一阶导数公式定义如下：

![](../Images/dbe88c79581bd5d59b2871fba3e17a34.png)

严格来说，函数的一阶导数，记作 *ƒ’(x)* 或 *df(x)/dx*，是函数 *ƒ(x)* 在给定 x 点的斜率。如果斜率为正（负），则表明函数在增加（减少），以及增加（减少）的幅度。正斜率表示随着 x 值的增加，函数 *ƒ(x)* 也增加。相反，负斜率表示随着 x 值的增加，*ƒ(x)* 减少。二阶导数是函数 *ƒ(x)* 的导数的导数。记作 *ƒ’’(x)* 或 *d2f(x)/dx2*，二阶导数指示了函数 *ƒ(x)* 的形状，即该函数是凹的还是凸的。从数学上讲（这一点很重要！！！），二阶导数可以区分相对最大值和相对最小值。

其中，

如果 *ƒ’’(x)* > 0，则 *ƒ(x)* 在 x = *a* 处是凸的。

如果 *ƒ’(a)* = 0，则 *a* 是一个临界点，因此是相对最小值。

![](../Images/92b0ff98218d34de751a3104e0fd8502.png)

图由作者绘制

其中，

如果 *ƒ’’(x)* < 0，则 *ƒ(x)* 在 x = *a* 处是凹的。

如果 *ƒ’(a)* = 0，则 *a* 是一个临界点，因此是相对最大值。

![](../Images/15d54f3b0ef54d9df8b2ef80dba5c37b.png)

图由作者绘制

如果二阶导数等于零，则可能有两种情况：1) 函数 *ƒ(x)* 处于转折点，即拐点，在该点函数从凹变为凸，或反之；2) 在该点函数未定义（即，不连续）。对于前者：

如果 *ƒ’’(x)* = 0，则 *ƒ(x)* 在 x = *2* 处是拐点。

![](../Images/705d5e1f36ea883f6348237a83a1dea8.png)

图由作者绘制

上述内容集中于一个具有单一自变量的函数，即一元函数 y = *ƒ(x)*。在现实世界中，人们会研究和建模多变量函数，其中研究的变量受到多个因素的影响，这就是两个或更多自变量 y = ƒ(x, z)。为了测量自变量 x 对因变量 y 的影响，并保持 z 不变，需要计算函数关于 x 的偏导数。因此，偏导数计算了因每个输入的变化引起的成本函数的变化率。梯度下降迭代地计算这些成本函数的变化，并在每一步更新这些函数参数的值，直到达到使参数值优化、从而最小化成本函数的最小点。

**3\. 梯度下降算法的实际应用**

斜率的绝对值越大，我们可以迈出的步伐就越大，或者我们可以继续朝着最陡下降的方向迈步，即局部最小值。随着我们接近最低/最小点，斜率减小，因此可以迈出更小的步伐，直到到达斜率为零（0）的平坦表面，即*ƒ’(x)* = 0，这是下图中红色箭头指向的βi的最低值。这就是曲线的局部最小值，函数参数的最优值也在此处得出。

![](../Images/45f0e3a2e12d36989c395262e5df3fda.png)

作者绘制的图表

因此，如果一个函数是*严格*凸的（凹的），则只有一个临界点。现在，也有可能存在多个局部最小值的情况。在这种情况下，搜索的是函数能够达到的最低值，这被称为全局最小值。

![](../Images/83c582728280d36bd59544b797ffe5a8.png)

作者绘制的图表

以下两个关键问题出现：

1) 应该朝哪个方向迈步？

2) 步长应该有多大？

让我们回顾一下到目前为止的内容。梯度下降是一种算法，在模型的训练阶段，通过在每一步向最陡下降的方向（即局部最小值）前进时，对函数的每个输入取其偏导数，从而迭代地调整和优化函数参数的值。如果导数为正，函数值在增加。因此，应该朝相反方向迈步。梯度指示了应该迈步的方向。如果梯度很大，即斜率的绝对值很大，则应该朝着局部最小值迈较大的步伐。实际上，梯度下降在每次迭代中朝着局部最小值方向采取越来越小的步伐，如上图中的蓝色箭头所示。

步长的大小与学习率有关。这决定了算法学习/移动到最陡下降方向的速度。在最高梯度下，即斜率的绝对值最大时，算法学习最快。当接近局部最小值时，步长会变小。因此，学习率作为超参数在尝试不同值后设置，以便成本函数在迭代中减少。如果学习率过大，可能会错过局部最小值。学习率过小可能导致权重更新较小，使模型没有显著改善。如果学习率过小，可能需要时间才能收敛。收敛是指成本函数不再减少。因此，成本函数是算法性能的指标。在多变量函数的世界中，这表示为：

![](../Images/7967ef48d2b7ebc592bb70eace9eb39e.png)

其中，

*df/d*β 表示成本函数对参数 β 的偏导数。

*m* 数据点数量。

*yi* 是第 i 个数据点的实际依赖/目标变量值。

*ŷi* 是模型预测的第 i 个数据点的依赖/目标变量值。

*xi* 表示与数据点相关的第 i 个输入。

![](../Images/650f42910dde7c968ff22971e7f731d7.png)

其中，

*▽f* 表示函数 *f(x)* 对参数 β 的梯度向量。

*df/d*βk 表示函数 *f(x)* 对第 k 个参数 β 的偏导数。

![](../Images/a47daebf7247b61566e975b9730e8eef.png)

其中，

新的 β 表示第 i 个参数 β 的当前值。

旧的 β 表示第 i 个参数 β 的更新值。

*n* 是 *学习率：采取步长的长度！*

*▽f* 是指向函数 *f(x)* 在参数 β 变化方向上的最陡下降方向的梯度向量，以最小化 *f(x)*。

**4\. 梯度下降的局限性**

梯度下降的一个局限性与上述提到的标准之一有关，即函数必须在其定义域的每一点都可微分。当情况不是这样，算法找到一个未定义的点（即不连续的点）时，算法会失败。

另一个局限性与步长的大小有关，即学习率 (*n*)，即朝着最陡下降方向采取的步伐。如果步长过大，可能会错过局部最小值，甚至可能无法收敛。如果步长过小，则需要更长的时间才能收敛。如果输入数量很大，这种情况会更加严重。

最终，梯度下降可能永远无法找到全局最小值。算法无法区分局部最小值和全局最小值。当算法在寻找局部最小值时，一旦收敛就会停止。局部最小值会把算法困在局部最小值所在的谷底，阻止步伐足够大以便退出。

**5\. 结论**

总结来说，梯度下降是：

1) 一种迭代的、一阶优化算法类型。

2) 在每次迭代中，对可微函数的参数进行更新，并最小化成本函数。

3) 因此，收敛于局部最小值。

基于梯度下降法的局限性，有动机探索不同的、更先进的梯度下降方法，甚至其他类型的优化算法，如二阶方法。然而，这超出了本文的范围，因此我会把它留作我下一篇文章的主题 😊

感谢阅读！

请添加任何你认为能增强该主题知识的评论！
