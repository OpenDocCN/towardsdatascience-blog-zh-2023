- en: Spoken language recognition on Mozilla Common Voice — Part I.
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/spoken-language-recognition-on-mozilla-common-voice-part-i-3f5400bbbcd8?source=collection_archive---------9-----------------------#2023-08-02](https://towardsdatascience.com/spoken-language-recognition-on-mozilla-common-voice-part-i-3f5400bbbcd8?source=collection_archive---------9-----------------------#2023-08-02)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[](https://medium.com/@sergeyvilov?source=post_page-----3f5400bbbcd8--------------------------------)[![Sergey
    Vilov](../Images/42efe223e2aa575250e050cf3660cf20.png)](https://medium.com/@sergeyvilov?source=post_page-----3f5400bbbcd8--------------------------------)[](https://towardsdatascience.com/?source=post_page-----3f5400bbbcd8--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----3f5400bbbcd8--------------------------------)
    [Sergey Vilov](https://medium.com/@sergeyvilov?source=post_page-----3f5400bbbcd8--------------------------------)'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: ·
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F33297faf768d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fspoken-language-recognition-on-mozilla-common-voice-part-i-3f5400bbbcd8&user=Sergey+Vilov&userId=33297faf768d&source=post_page-33297faf768d----3f5400bbbcd8---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----3f5400bbbcd8--------------------------------)
    ·6 min read·Aug 2, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F3f5400bbbcd8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fspoken-language-recognition-on-mozilla-common-voice-part-i-3f5400bbbcd8&user=Sergey+Vilov&userId=33297faf768d&source=-----3f5400bbbcd8---------------------clap_footer-----------)'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F3f5400bbbcd8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fspoken-language-recognition-on-mozilla-common-voice-part-i-3f5400bbbcd8&source=-----3f5400bbbcd8---------------------bookmark_footer-----------)![](../Images/9b9afdc8d66a02cc489220c16c907be9.png)'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: Photo by [Sebastian Unrau](https://unsplash.com/@sebastian_unrau?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: One of the most challenging AI tasks is identifying the speaker’s language for
    the purpose of subsequent speech-to-text conversion. This problem might arise,
    for example, when people living in the same household and speaking different languages
    use the same voice-control device such as a garage lock or a smart home system.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: In this series of articles, we will try to maximize spoken language recognition
    accuracy using the [Mozilla Common Voice](https://commonvoice.mozilla.org/en)
    (MCV) Dataset. In particular, we will compare several neural network models trained
    to distinguish between German, English, Spanish, French, and Russian.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一系列文章中，我们将尝试通过使用[Mozilla Common Voice](https://commonvoice.mozilla.org/en)（MCV）数据集来最大化口语语言识别的准确性。特别是，我们将比较几种神经网络模型，这些模型被训练用来区分德语、英语、西班牙语、法语和俄语。
- en: In this first part we shall discuss data selection, preprocessing, and embedding.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一部分中，我们将讨论数据选择、预处理和嵌入。
- en: '**Data selection**'
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**数据选择**'
- en: MCV is by far the largest publicly available voice dataset, comprising short
    records (average duration = 5.3s) in as many as 112 languages.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: MCV迄今为止是最大的公开语音数据集，包括多达112种语言的短录音（平均时长 = 5.3秒）。
- en: 'For our language recognition task, we choose 5 languages: German, English,
    Spanish, French, and Russian. For German, English, Spanish, and French, we consider
    only accents labelled in MCV as *Deutschland Deutsch*, *United States English*,
    *España*, and *Français de France* correspondingly. For each language, we select
    a subset of adult records among [validated](https://github.com/common-voice/cv-dataset)
    samples.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的语言识别任务，我们选择了5种语言：德语、英语、西班牙语、法语和俄语。对于德语、英语、西班牙语和法语，我们仅考虑MCV中标注的口音，即*Deutschland
    Deutsch*、*United States English*、*España*和*Français de France*。对于每种语言，我们从[验证过的](https://github.com/common-voice/cv-dataset)样本中选择一部分成人记录。
- en: We used a train/val/test split with 40K/5K/5K audio clips per language. To get
    objective evaluation, we ensured that speakers (*client_id*) did not overlap between
    the three sets. When splitting data, we first filled the test and validation sets
    with records from poorly represented speakers, then we allocated the remaining
    data to the train set. This improved speakers diversity in the val/test sets and
    led to a more objective estimation of the generalization error. To avoid that
    a single speaker dominates in the train set, we limited the maximal number of
    records per *client_id* to 2000\. On average, we got 26 records per speaker. We
    also made sure that the number of female records matches the number of male records.
    Finally, we upsampled the train set if the resulting number of records was below
    40K. The final distribution of counts is depicted in the figure below.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用了40K/5K/5K的训练/验证/测试划分。为了获得客观评价，我们确保三组之间的说话者（*client_id*）不重叠。在数据拆分时，我们首先将测试和验证集填充来自表现不佳的说话者的记录，然后将剩余的数据分配到训练集中。这提高了验证/测试集中说话者的多样性，并导致了对泛化误差的更客观估计。为了避免单一说话者在训练集中占主导地位，我们将每个*client_id*的记录最大数量限制为2000个。平均来说，我们得到了每个说话者26个记录。我们还确保女性记录的数量与男性记录的数量匹配。最后，如果最终记录数量低于40K，我们对训练集进行了上采样。最终的记录分布如下面的图所示。
- en: '![](../Images/a58b234bfa3d4766c0df4267235357b7.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a58b234bfa3d4766c0df4267235357b7.png)'
- en: Class distribution in the train set (image by the author).
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 训练集中的类别分布（图像由作者提供）。
- en: The resulting dataframe with indicated split is available [here](https://github.com/sergeyvilov/MCV-spoken-language-recognition/tree/master).
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 带有指示拆分的最终数据框架可以在[这里](https://github.com/sergeyvilov/MCV-spoken-language-recognition/tree/master)找到。
- en: '**Data Preprocessing**'
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**数据预处理**'
- en: 'All MCV audio files are provided in the .mp3 format. Although .mp3 is great
    for compact storage of music, it is not widely supported by audio processing libraries,
    such as librosa in python. So, we first need to convert all files to .wav. In
    addition, the original MCV sampling rate is 44kHz. This implies the maximal encoded
    frequency of 22kHz (according to the [Nyquist theorem](https://en.wikipedia.org/wiki/Nyquist%E2%80%93Shannon_sampling_theorem)).
    This would be an overkill for a spoken language recognition task: in English,
    for example, most phonemes do not exceed 3kHz in conversational speech. So, we
    can also lower the sampling rate downto 16kHz. This will not only reduce the file
    size, but also accelerate generation of embeddings.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 所有MCV音频文件都以.mp3格式提供。虽然.mp3非常适合音乐的紧凑存储，但它在音频处理库（如python中的librosa）中支持有限。因此，我们首先需要将所有文件转换为.wav格式。此外，原始MCV采样率为44kHz。这意味着最大编码频率为22kHz（根据[奈奎斯特定理](https://en.wikipedia.org/wiki/Nyquist%E2%80%93Shannon_sampling_theorem)）。对于口语语言识别任务来说，这样的频率有些过高：例如，在英语中，大多数音素在会话语音中不会超过3kHz。因此，我们也可以将采样率降低到16kHz。这不仅会减少文件大小，还会加快嵌入生成的速度。
- en: 'Both operations can be executed within one command using [ffmpeg](https://github.com/FFmpeg/FFmpeg):'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个操作可以通过[ffmpeg](https://github.com/FFmpeg/FFmpeg)的一条命令来执行：
- en: '[PRE0]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '**Feature engineering**'
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**特征工程**'
- en: 'Relevant information is usually extracted from audio clips by computing embeddings.
    We will consider four more or less common embeddings for speech recognition /
    spoken language recognition tasks: mel spectrogram, MFCC, RASTA-PLP, and GFCC.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 相关信息通常通过计算嵌入从音频片段中提取。我们将考虑四种或多或少常见的用于语音识别/口语语言识别任务的嵌入：梅尔频谱图、MFCC、RASTA-PLP和GFCC。
- en: '**Mel Spectrogram**'
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**梅尔频谱图**'
- en: The principles of mel spectrograms have widely been discussed [on Medium](https://medium.com/analytics-vidhya/understanding-the-mel-spectrogram-fca2afa2ce53).
    An awesome step-by-step tutorial on mel spectrograms and MFCC can also be found
    [here](http://practicalcryptography.com/miscellaneous/machine-learning/guide-mel-frequency-cepstral-coefficients-mfccs/#eqn2).
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 梅尔频谱图的原理已广泛讨论 [在Medium](https://medium.com/analytics-vidhya/understanding-the-mel-spectrogram-fca2afa2ce53)上。关于梅尔频谱图和MFCC的精彩逐步教程也可以在[此处](http://practicalcryptography.com/miscellaneous/machine-learning/guide-mel-frequency-cepstral-coefficients-mfccs/#eqn2)找到。
- en: To obtain the mel spectrogram, the input signal is first subject to pre-emphasis
    filtering. Then, Fourier Transform is performed on sliding windows consecutively
    applied to the obtained waveform. After that, the frequency scale is transformed
    to the mel scale which is linear with respect to human perception of intervals.
    Finally, a filter bank of overlapping triangular filters is [applied](https://jonathan-hui.medium.com/speech-recognition-feature-extraction-mfcc-plp-5455f5a69dd9)
    to the power spectrum on the mel scale to mimic how the human ear perceives sound.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 为了获得梅尔频谱图，首先对输入信号进行预加重滤波。然后，对滑动窗口应用于获得的波形进行连续的傅里叶变换。之后，频率尺度被转换为梅尔尺度，这与人类对间隔的感知是线性的。最后，应用一组重叠的三角滤波器的[滤波器组](https://jonathan-hui.medium.com/speech-recognition-feature-extraction-mfcc-plp-5455f5a69dd9)到梅尔尺度上的功率谱，以模拟人耳对声音的感知。
- en: '**MFCC**'
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**MFCC**'
- en: Mel coefficients are highly correlated, which might be undesirable for some
    machine learning algorithms (for example, it is more convenient to have a diagonal
    covariance matrix for Gaussian Mixture Models). [To decorrelate](https://dsp.stackexchange.com/questions/15938/is-this-a-correct-interpretation-of-the-dct-step-in-mfcc-calculation/15945#15945)
    mel filter banks, mel-frequency cepstral coefficients (MFCC) are obtained by computing
    Discrete Cosine Transformation (DCT) on log filterbank energies. Only a few first
    MFCC are usually used. Exact steps are outlined [here](https://haythamfayek.com/2016/04/21/speech-processing-for-machine-learning.html).
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 梅尔系数高度相关，这可能对一些机器学习算法不利（例如，高斯混合模型更方便使用对角协方差矩阵）。 [去相关](https://dsp.stackexchange.com/questions/15938/is-this-a-correct-interpretation-of-the-dct-step-in-mfcc-calculation/15945#15945)梅尔滤波器组，梅尔频率倒谱系数（MFCC）通过对对数滤波器组能量进行离散余弦变换（DCT）获得。通常只使用前几个MFCC。确切步骤详见[此处](https://haythamfayek.com/2016/04/21/speech-processing-for-machine-learning.html)。
- en: '**RASTA-PLP**'
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**RASTA-PLP**'
- en: Perceptual Linear Prediction (PLP) (Hermansky and Hynek, 1990) are another way
    to compute embeddings for music clips.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 知觉线性预测（PLP）（Hermansky和Hynek，1990）是计算音乐片段嵌入的另一种方法。
- en: Differences between PLP and MFCC lie in the filter-banks, the equal-loudness
    pre-emphasis, the intensity-to-loudness conversion and in the application of linear
    prediction (Hönig et al, 2005).
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: PLP和MFCC之间的差异在于滤波器组、等响预加重、强度到响度的转换以及线性预测的应用（Hönig等，2005）。
- en: '![](../Images/641298716a4efc46073768344ec401b1.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/641298716a4efc46073768344ec401b1.png)'
- en: Overview of PLP and MFCC techniques (from Hönig et al, 2005)
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: PLP和MFCC技术概述（来自Hönig等，2005）
- en: PLP were reported (Woodland et al., 1996) to be more robust than MFCC when there
    is acoustic mismatch between training and test data.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 有报告称，PLP（Woodland等，1996）在训练和测试数据之间存在声学不匹配时，比MFCC更具鲁棒性。
- en: Compared to PLP, RASTA-PLP (Hermansky et al., 1991) performs additional filtering
    in the logarithmic spectral domain, which makes the method more robust to linear
    spectral distortions introduced by communication channels.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 与PLP相比，RASTA-PLP（Hermansky等，1991）在对数频谱域中执行额外的滤波，这使得该方法对通信通道引入的线性频谱失真更为鲁棒。
- en: '**GFCC**'
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**GFCC**'
- en: Gammatone frequency cepstral coefficients (GFCC) were reported to be less noise-sensitive
    than MFCC (Zhao, 2012; Shao, 2007). Compared to MFCC, Gammatone filters are computed
    on the equivalent rectangular bandwidth scale (instead of the mel scale) and the
    cubic root operation (instead of logarithm) is applied prior to computing DCT.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 有报告称，伽玛音调频谱系数（GFCC）比MFCC对噪声的敏感性更低（Zhao，2012；Shao，2007）。与MFCC相比，伽玛音调滤波器是在等效矩形带宽尺度上计算的（而不是梅尔尺度），并且在计算DCT之前应用了立方根操作（而不是对数）。
- en: 'The figure below shows an example signal alongside with its different embeddings:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 下图显示了一个示例信号及其不同的嵌入：
- en: '![](../Images/522eb0418bc3963b2e626237e0f7523b.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/522eb0418bc3963b2e626237e0f7523b.png)'
- en: Example audio file and its embeddings (image by the author).
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 示例音频文件及其嵌入（图片由作者提供）。
- en: '**Comparing embeddings**'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '**比较嵌入**'
- en: To choose the most efficient embedding, we trained the Attention LSTM network
    from De Andrade et al., 2018\. For time reasons, we trained the neural network
    only on 5K clips.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 为了选择最有效的嵌入，我们训练了De Andrade等人（2018年）提出的注意力LSTM网络。由于时间原因，我们只训练了5000个剪辑。
- en: The figure below compares the validation accuracy for all embeddings.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 下图比较了所有嵌入的验证准确性。
- en: '![](../Images/436e3bb20bee57dd01c008b7f3450441.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/436e3bb20bee57dd01c008b7f3450441.png)'
- en: Performance of different embeddings on the 5K dataset (image by the author).
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 不同嵌入在5000个数据集上的表现（图片由作者提供）。
- en: So, mel spectrograms with the first 13 filter banks perform closely to RASTA-PLP
    with *model_order*=13.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，前13个滤波器组的mel频谱图的表现接近于*model_order*=13的RASTA-PLP。
- en: It is interesting to note that mel spectrograms outperform MFCC. This fits previous
    claims (see [here](https://haythamfayek.com/2016/04/21/speech-processing-for-machine-learning.html),
    and [here](https://stackoverflow.com/questions/60439741/why-do-mel-filterbank-energies-outperform-mfccs-for-speech-commands-recognition))
    that mel spectrograms are a better choice for neural network classifiers.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，mel频谱图的表现优于MFCC。这符合之前的说法（见[这里](https://haythamfayek.com/2016/04/21/speech-processing-for-machine-learning.html)，以及[这里](https://stackoverflow.com/questions/60439741/why-do-mel-filterbank-energies-outperform-mfccs-for-speech-commands-recognition)），即mel频谱图是神经网络分类器的更好选择。
- en: One other observation is that the performance usually drops for higher number
    of coefficients. This can be due to overfitting since high order coefficients
    often [represent](https://jonathan-hui.medium.com/speech-recognition-feature-extraction-mfcc-plp-5455f5a69dd9)
    speaker-related features that are not generalizable to the test set where different
    speakers are chosen.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个观察是，通常系数数量越多，性能会下降。这可能是由于过拟合，因为高阶系数通常[代表](https://jonathan-hui.medium.com/speech-recognition-feature-extraction-mfcc-plp-5455f5a69dd9)与说话者相关的特征，这些特征在测试集中（不同说话者被选择）不可泛化。
- en: Due to time constraints, we did not test any combinations of embeddings, although
    it was previously [observed](/from-mfccs-xor-gfccs-to-mfccs-and-gfccs-urban-sounds-classification-case-study-a087ac007901)
    that they may provide superior accuracy.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 由于时间限制，我们没有测试任何嵌入组合，尽管之前有[观察](https://from-mfccs-xor-gfccs-to-mfccs-and-gfccs-urban-sounds-classification-case-study-a087ac007901)到它们可能提供更好的准确性。
- en: Since mel spectrograms are much faster to compute than RASTA-PLP, we will use
    these embeddings in further experiments.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 由于mel频谱图的计算速度远快于RASTA-PLP，我们将在进一步的实验中使用这些嵌入。
- en: '**In Part II we will run several neural network models and chose the one that
    classifies languages the best.**'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '**在第二部分，我们将运行几个神经网络模型，并选择分类效果最佳的模型。**'
- en: '**References**'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '**参考文献**'
- en: De Andrade, Douglas Coimbra, et al. “A neural attention model for speech command
    recognition.” *arXiv preprint arXiv:1808.08929* (2018).
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: De Andrade, Douglas Coimbra, 等. “用于语音命令识别的神经注意力模型。” *arXiv预印本arXiv:1808.08929*（2018年）。
- en: 'Hermansky, Hynek. “Perceptual linear predictive (PLP) analysis of speech.”
    *the Journal of the Acoustical Society of America* 87.4 (1990): 1738–1752.'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hermansky, Hynek. “语音的感知线性预测（PLP）分析。” *美国声学学会期刊* 87.4（1990年）：1738–1752。
- en: Hönig, Florian, et al. “Revising perceptual linear prediction (PLP).” *Ninth
    European Conference on Speech Communication and Technology*. 2005.
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hönig, Florian, 等. “修订感知线性预测（PLP）。” *第九届欧洲语音通信与技术会议*。2005年。
- en: Hermansky, Hynek, et al. “RASTA-PLP speech analysis.” *Proc. IEEE Int’l Conf.
    Acoustics, speech and signal processing*. Vol. 1\. 1991.
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hermansky, Hynek, 等. “RASTA-PLP语音分析。” *IEEE国际声学、语音与信号处理会议论文集*。第1卷。1991年。
- en: Shao, Yang, Soundararajan Srinivasan, and DeLiang Wang. “Incorporating auditory
    feature uncertainties in robust speaker identification.” *2007 IEEE International
    Conference on Acoustics, Speech and Signal Processing-ICASSP’07*. Vol. 4\. IEEE,
    2007.
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shao, Yang, Soundararajan Srinivasan, 和 DeLiang Wang. “在鲁棒说话人识别中引入听觉特征不确定性。”
    *2007年IEEE国际声学、语音与信号处理会议-ICASSP’07*。第4卷。IEEE，2007年。
- en: Woodland, Philip C., Mark John Francis Gales, and David Pye. “Improving environmental
    robustness in large vocabulary speech recognition.” *1996 IEEE International Conference
    on Acoustics, Speech, and Signal Processing Conference Proceedings*. Vol. 1\.
    IEEE, 1996.
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Woodland, Philip C., Mark John Francis Gales, 和 David Pye. “在大词汇量语音识别中提高环境鲁棒性。”
    *1996 IEEE 国际声学、语音与信号处理会议论文集*。第 1 卷\. IEEE, 1996.
- en: 'Zhao, Xiaojia, Yang Shao, and DeLiang Wang. “CASA-based robust speaker identification.”
    *IEEE Transactions on Audio, Speech, and Language Processing* 20.5 (2012): 1608–1616.'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhao, Xiaojia, Yang Shao, 和 DeLiang Wang. “基于 CASA 的鲁棒说话人识别。” *IEEE 音频、语音与语言处理汇刊*
    20.5 (2012): 1608–1616.'
