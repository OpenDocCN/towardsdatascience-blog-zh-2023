- en: 'Similarity Search, Part 7: LSH Compositions'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/similarity-search-part-7-lsh-compositions-1b2ae8239aca?source=collection_archive---------6-----------------------#2023-07-24](https://towardsdatascience.com/similarity-search-part-7-lsh-compositions-1b2ae8239aca?source=collection_archive---------6-----------------------#2023-07-24)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Dive into combinations of LSH functions to guarantee a more reliable search
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@slavahead?source=post_page-----1b2ae8239aca--------------------------------)[![Vyacheslav
    Efimov](../Images/db4b02e75d257063e8e9d3f1f75d9d6d.png)](https://medium.com/@slavahead?source=post_page-----1b2ae8239aca--------------------------------)[](https://towardsdatascience.com/?source=post_page-----1b2ae8239aca--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----1b2ae8239aca--------------------------------)
    [Vyacheslav Efimov](https://medium.com/@slavahead?source=post_page-----1b2ae8239aca--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fc8a0ca9d85d8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsimilarity-search-part-7-lsh-compositions-1b2ae8239aca&user=Vyacheslav+Efimov&userId=c8a0ca9d85d8&source=post_page-c8a0ca9d85d8----1b2ae8239aca---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----1b2ae8239aca--------------------------------)
    ·11 min read·Jul 24, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F1b2ae8239aca&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsimilarity-search-part-7-lsh-compositions-1b2ae8239aca&user=Vyacheslav+Efimov&userId=c8a0ca9d85d8&source=-----1b2ae8239aca---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F1b2ae8239aca&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsimilarity-search-part-7-lsh-compositions-1b2ae8239aca&source=-----1b2ae8239aca---------------------bookmark_footer-----------)![](../Images/0f691f4da9f0e51df324f06b23ba96d4.png)'
  prefs: []
  type: TYPE_NORMAL
- en: S**imilarity search** is a problem where given a query the goal is to find the
    most similar documents to it among all the database documents.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In data science, similarity search often appears in the NLP domain, search engines
    or recommender systems where the most relevant documents or items need to be retrieved
    for a query. There exists a large variety of different ways to improve search
    performance in massive volumes of data.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the last two parts of this article series, we digged into LSH — an algorithm
    *transforming input vectors to lower-dimensional hash values while preserving
    information about their similarity*. In particular, we have already looked at
    two algorithms that were suitable for different distance metrics:'
  prefs: []
  type: TYPE_NORMAL
- en: '[](/similarity-search-part-5-locality-sensitive-hashing-lsh-76ae4b388203?source=post_page-----1b2ae8239aca--------------------------------)
    [## Similarity Search, Part 5: Locality Sensitive Hashing (LSH)'
  prefs: []
  type: TYPE_NORMAL
- en: Explore how similarity information can be incorporated into hash function
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/similarity-search-part-5-locality-sensitive-hashing-lsh-76ae4b388203?source=post_page-----1b2ae8239aca--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: Classic LSH algorithm constructs signatures that reflect the information about
    **Jaccard index** of vectors.
  prefs: []
  type: TYPE_NORMAL
- en: '[](/similarity-search-part-6-random-projections-with-lsh-forest-f2e9b31dcc47?source=post_page-----1b2ae8239aca--------------------------------)
    [## Similarity Search, Part 6: Random Projections with LSH Forest'
  prefs: []
  type: TYPE_NORMAL
- en: Understand how to hash data and reflect its similarity by constructing random
    hyperplanes
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/similarity-search-part-6-random-projections-with-lsh-forest-f2e9b31dcc47?source=post_page-----1b2ae8239aca--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: The method of random projections builds a forest of hyperplanes preserving **cosine
    similarity** of vectors.
  prefs: []
  type: TYPE_NORMAL
- en: In fact, LSH algorithms exist for other distance metrics as well. Though every
    method has its own unique parts, there are a lot of common concepts and formulas
    which appear in each of them. To facilitate the learning process of new methods
    in the future, we are going to focus more on the theory and provide several essential
    definitions and theorems which often appear in advanced LSH literature. By the
    end of the article, we will be able to construct more complex LSH schemes by simply
    combining the basic ones as LEGO building blocks.
  prefs: []
  type: TYPE_NORMAL
- en: As a bonus, at the end, we will have a look at how **Euclidean distance** can
    be incorporated into LSH.
  prefs: []
  type: TYPE_NORMAL
- en: '*Note*. As the main prerequisite, it is expected that you are already familiar
    with parts 5 and 6 of this article series. If not, it is highly recommended to
    read them first.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Note*. [Cosine distance](https://en.wikipedia.org/wiki/Cosine_similarity)
    is formally defined in the range [0, 2]. For simplicity, we will map it to the
    interval [0, 1] where 0 and 1 indicate the lowest and the highest possible similarity
    respectively.'
  prefs: []
  type: TYPE_NORMAL
- en: Formal LSH definition
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Given a distance metric d, H is called a (d₁, d₂, p₁, p₂)-sensitive LSH function
    if for randomly chosen objects x and y, the following conditions are satisfied:'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*If d(x, y) ≤ d₁, then p(H(x) = H(y)) ≥ p₁, i.e. H(x) = H(y) with probability
    at least p₁.*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*If d(x, y) ≥ d₂, then p(H(x) = H(y)) ≤ p₂, i.e. H(x) = H(y) with probability
    at most p₂.*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let us understand what these statements mean. When two vectors are similar,
    they have a low distance between them. Basically, the first statement makes sure
    the probability of hashing them to the same bucket is above a certain threshold.
    This way, some *false negatives* are eliminated: if the distance between two vectors
    is larger than *d₁*, then the probability of them hashing to the same bucket is
    always less than *p₁*. Inversely, the second statement controls *false positives*:
    if two vectors are not similar and the distance between them is larger than *d₂*,
    then they have an upper probability *p₂* threshold of appearing in the same bucket.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Given the statement above, we normally want the following statements in the
    system to be satisfied:'
  prefs: []
  type: TYPE_NORMAL
- en: '*p₁* should be as close to 1 as possible to reduce the number of *false negatives*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*p₂* should be as close to 0 as possible to reduce the number of *false positives*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The gap between *d₁* and *d₂* should be as low as possible to reduce the interval
    where probabilistic estimations on data cannot be made.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/ec88a6b38658c10a66d7281cb27f5dca.png)'
  prefs: []
  type: TYPE_IMG
- en: The diagram on the left shows a typical curve demonstrating relationships between
    LSH parameters for (d₁, d₂, p₁, p₂) notation. A curve on the right demonstrates
    an ideal scenario where there is not gap between thresholds *d₁* and *d₂*.
  prefs: []
  type: TYPE_NORMAL
- en: Sometimes the statement above is introduced by using the term of similarity
    *s* instead of distance *d:*
  prefs: []
  type: TYPE_NORMAL
- en: 'Given a similarity metric s, H is called a (s₁, s₂, p₁, p₂)-sensitive LSH function
    if for randomly chosen objects x and y, the following conditions are satisfied:'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*If s(x, y) ≥ s₁, then p(H(x) = H(y)) ≥ p₁, i.e. H(x) = H(y) with probability
    at least p₁.*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*If s(x, y) ≤ s₂, then p(H(x) = H(y)) ≤ p₂, i.e. H(x) = H(y) with probability
    at most p₂.*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/4a2926a1c78be5ec5d31262022adf59d.png)'
  prefs: []
  type: TYPE_IMG
- en: The diagram on the left shows a typical curve demonstrating relationships between
    LSH parameters for (*s₁, s₂, p₁, p₂*) notation. A curve on the right demonstrates
    an ideal scenario where there is not gap between thresholds *s₁* and *s₂*.
  prefs: []
  type: TYPE_NORMAL
- en: '*Note*. In this article, both notations *(d₁, d₂, p₁, p₂)* and *(s₁, s₂, p₁,
    p₂)* will be used. Based on the letters used in notations in the text, it should
    be clear whether distance *d* or similarity *s* is implied. By default, the notation
    *(d₁, d₂, p₁, p₂)* is used.'
  prefs: []
  type: TYPE_NORMAL
- en: LSH example
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To make things more clear, let us prove the following statement:'
  prefs: []
  type: TYPE_NORMAL
- en: 'If the distance metric *s* is Jaccard index, then *H* is a *(0.6, 0.6, 0.4,
    0.4)-*sensitive LSH function. Basically, the equivalent statements have to be
    proved:'
  prefs: []
  type: TYPE_NORMAL
- en: '*If d(x, y) ≤ 0.6, then p(H(x) = H(y)) ≥ 0.4*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*If d(x, y) ≥ 0.6, then p(H(x) = H(y)) ≤ 0.4*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: From the 5-th part of this article series, we know that *the probability of
    getting equal hash values for two binary vectors equals the Jaccard similarity*.
    Therefore, if two vectors are similar by at least 40%, then it is guaranteed that
    the probability of getting equal hash values is also at least 40%. In the meantime,
    a Jaccard similarity of at least 40% is equivalent to a Jaccard index at most
    of 60%. As a result, the first statement is proved. The analogous reflections
    can be done for the second statement.
  prefs: []
  type: TYPE_NORMAL
- en: 'This example can be generalised into the theorem:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Theorem**. If d is Jaccard index, then H is a (d₁, d₂, 1 — d₁, 1 — d₂)-family
    of LSH functions.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Similarly, based on the results obtained from part 6, it is possible to prove
    another theorem:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Theorem**. If s is cosine similarity (between -1 and 1), then H is a (s₁,
    s₂, 1 — arccos(s₁) / 180, 1 — arccos(d₂) / 180)-family of LSH functions.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Combining LSH functions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let us refer to useful concepts we learned in previous parts on LSH:'
  prefs: []
  type: TYPE_NORMAL
- en: Getting back to part 5 on minhashing, every vector was split into several bands
    each containing a set of rows. In order for a pair of vectors to be considered
    candidates, there had to exist **at least one** band where **all** of the vector
    rows were equal.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Regarding to part 6 on random projections, two vectors were considered candidates
    only if there existed **at least one** tree where **all** of the random projections
    did not separate the initial vectors.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'As we can notice, these two approaches have a similar paradigm under the hood.
    Both of them consider a pair of vectors as candidates only if at **least one time**
    out of *n* configurations vectors have the same hash values **all** *k* times.
    With the boolean algebra notation, it can be written like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d40f11e2ede06bf7da03a7e5fee71439.png)'
  prefs: []
  type: TYPE_IMG
- en: Based on this example, let us introduce logical operators *OR* and *AND* which
    allow aggregating a set of hash functions. Then we will estimate how they affect
    the output probability of two vectors being candidates and the rate of *false
    negative* and *false positive* errors.
  prefs: []
  type: TYPE_NORMAL
- en: AND operator
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Given n independent LSH functions H₁, H₂, … Hₙ, the **AND** operator considers
    two vectors as a candidate pair only if **all** of n corresponding hash values
    of both vectors are equal. Otherwise, the vectors are not considered as candidates.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: If hash values of two highly different vectors are aggregated by the *AND* operator,
    then the probability of them being candidates decreases with the increase of the
    number of used hash functions. Therefore, the number of *false positive* decreases.
  prefs: []
  type: TYPE_NORMAL
- en: At the same time, two similar vectors can by chance result in a pair of different
    hash values. Because of this, such vectors will not be considered similar by the
    algorithm. This aspect results in a higher rate of *false negatives*.
  prefs: []
  type: TYPE_NORMAL
- en: '**Theorem**. Consider r independent (s₁, s₂, p₁, p₂)-sensitive LSH functions.
    Combining these r LSH functions with the AND operator results in a new LSH function
    with parameters as'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/df8a221eb298604e45bafb780e33e5bc.png)'
  prefs: []
  type: TYPE_IMG
- en: It is easy to prove this statement by using the probability formula of several
    independent events which multiplies the probabilities of all events to estimate
    the probability that all events will occur.
  prefs: []
  type: TYPE_NORMAL
- en: OR operator
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Given n independent LSH functions H₁, H₂, … Hₙ, the **OR** operator considers
    two vectors as a candidate pair only if **at least one** of n corresponding hash
    values of both vectors are equal. Otherwise, the vectors are not considered as
    candidates.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Inversely to the *AND* operator, the *OR* operator increases the probability
    of any two vectors of being candidates. For any pair of vectors, it is sufficient
    at least a single equality of corresponding hash values. For that reason, the
    OR aggregation decreases the number of *false negatives* and increases *false
    positives*.
  prefs: []
  type: TYPE_NORMAL
- en: '**Theorem**. Consider *b* independent (d₁, d₂, p₁, p₂)-family LSH functions.
    Combining these *b* LSH functions with the AND operator results in a new LSH function
    with parameters as'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/1ce7752b4e0f6fc439ab573ee37d82ee.png)'
  prefs: []
  type: TYPE_IMG
- en: We will not prove this theorem because the resulting analogous probability formula
    was obtained and explained in part 5 of this article series.
  prefs: []
  type: TYPE_NORMAL
- en: Composition
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Having *AND* and *OR* operations, it is possible to combine them together in
    various manners to better control the *false positive* and *false negative* rates.
    Let us imagine having *r* LSH functions used by the *AND* combinator and *b* LSH
    functions used by the *OR* combinator. Two different compositions can be constructed
    by using these basic combinators:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/566766fb92be31d3a6da63a1ca54065c.png)'
  prefs: []
  type: TYPE_IMG
- en: AND-OR and OR-AND are two types of compositions that can be built by using AND
    and OR operators.
  prefs: []
  type: TYPE_NORMAL
- en: Algorithms described in two previous articles used the *AND-OR* composition.
    In fact, nothing prevents us from building more complex compositions based on
    *AND* and *OR* operations.
  prefs: []
  type: TYPE_NORMAL
- en: Composition example
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let us study an example to figure out how combinations of *AND* and *OR* can
    significantly improve performance. Assume an *OR-AND* combination with parameters
    *b = 4* and *r = 8*. Based on the corresponding formula above, we can estimate
    how the initial probability of two vectors being candidates transforms after the
    composition:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6ac71b2ad637b0298c2af56db0b16615.png)'
  prefs: []
  type: TYPE_IMG
- en: Probability changes by applying an OR-AND composition with parameters b = 4
    and r = 8\. The first row shows initial probabilities while the second row shows
    the transformed ones.
  prefs: []
  type: TYPE_NORMAL
- en: For instance, if for a certain value of similarity between two vectors, a single
    LSH function hashes them to the same bucket in 40% of cases, then after the *OR-AND*
    composition they will be hashed in 32.9% of cases.
  prefs: []
  type: TYPE_NORMAL
- en: To understand what is so special about compositions, consider a *(0.4, 1.7,
    0.8, 0.2)*-sensitive LSH function. After the *OR-AND* transformation, the LSH
    function transforms into *(0.4, 1.7, 0.0148, 0.987)*-sensitive format.
  prefs: []
  type: TYPE_NORMAL
- en: In essence, if initially two vectors were very similar and had a distance of
    less than 0.4, then they would be considered candidates in 80% of cases. However,
    with the composition applied, they are now candidates in 98.7% of scenarios resulting
    in much fewer *false negative* errors!
  prefs: []
  type: TYPE_NORMAL
- en: Analogously, if two vectors are very different from each other and have a distance
    greater than 1.7, then they are now considered candidates only in 1.48% of cases
    (compared to 20% before). This way, the frequency of *false positive* errors is
    reduced by 13.5 times! This is a massive improvement!
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f501c884ce08598c3bb4a6d0817888f2.png)'
  prefs: []
  type: TYPE_IMG
- en: Curves showing how initial probabilities are transformed after different compositions
  prefs: []
  type: TYPE_NORMAL
- en: In general, by having a *(d₁, d₂, p₁, p₂)*-sensitive LSH function, it is possible
    to transform it to a *(d₁, d₂, p’₁, p’₂)* format where *p’₁* is close to 1 and
    *p’₂* is close to 0\. Getting more closer to 1 and 0 usually requires more compositions
    to be used.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: LSH for other distance metrics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have already studied in-depth LSH schemes which are used to preserve information
    about Jaccard index and cosine distance. The natural question which arises is
    whether it is possible to use LSH for other distance metrics. Unfortunately, for
    most of the metrics, there is no corresponding LSH algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: Nevertheless, LSH schema exists for Euclidean distance — one of the most commonly
    used metrics in machine learning. As it is used frequently, we are going to study
    how to get hash values for Euclidean distance. With the theoretical notations
    introduced above, we will prove an important LSH property for this metric.
  prefs: []
  type: TYPE_NORMAL
- en: LSH for Euclidean distance
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The mechanism of hashing of points in Euclidean space includes projecting them
    on a random line. Algorithm assumes that
  prefs: []
  type: TYPE_NORMAL
- en: If two points are relatively close to each other, then their projections should
    also be close.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If two points are far from each other, then their projections should be also
    far.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To measure how close two projections are, a line can be divided into several
    equal segments (buckets) of size *a* each. Each line segment corresponds to a
    certain hash value. If two points project to the same line segment, then they
    have the same hash value. Otherwise, hash values are different.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1925eca7139b0afb0a6aa0b098f84d98.png)'
  prefs: []
  type: TYPE_IMG
- en: Projecting points on a random line
  prefs: []
  type: TYPE_NORMAL
- en: Though the method might seem robust at first, it can still project points which
    are far from each other to the same segment. This happens especially when a line
    connecting two points is almost perpendicular to the initial projection line.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/46de10be3c85955f16d8f6593be25466.png)'
  prefs: []
  type: TYPE_IMG
- en: Despite both points being relatively far from each other, there is still chance
    that they will be hashed into the same bucket.
  prefs: []
  type: TYPE_NORMAL
- en: In order to decrease the error rate, it is highly recommended to use compositions
    of random projection lines, as discussed above.
  prefs: []
  type: TYPE_NORMAL
- en: It is geometrically possible to prove that if *a* is a length of a single line
    segment in Euclidean space, then *H* is *(a / 2, 2a, ½, ⅓)*-sensitive LSH function.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we accumulated knowledge on general LSH notation which helped
    us to formally introduce composition operations allowing us significantly reduce
    error rates. It is worth noting that LSH exists only for a small part of machine
    learning metrics but at least for the most popular ones which are Euclidean distance,
    cosine distance and Jaccard index. When dealing with another metric measuring
    similarity between vectors, it is recommended to choose another similarity search
    approach.
  prefs: []
  type: TYPE_NORMAL
- en: For reference, formal proofs of the statements introduced in this article can
    be found in [these notes](https://web.lums.edu.pk/~imdad/pdfs/CS5312_Notes/CS5312_Notes-14-LSH.pdf).
  prefs: []
  type: TYPE_NORMAL
- en: Resources
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[Locality Sensitive Hashing | Lecture Notes for Big Data Analytics | Nimrah
    Mustafa](https://web.lums.edu.pk/~imdad/pdfs/CS5312_Notes/CS5312_Notes-14-LSH.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Cosine distance | Wikipedia](https://en.wikipedia.org/wiki/Cosine_similarity)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*All images unless otherwise noted are by the author.*'
  prefs: []
  type: TYPE_NORMAL
