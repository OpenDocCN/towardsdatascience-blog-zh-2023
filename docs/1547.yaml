- en: Understanding LLM Hallucinations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/llm-hallucinations-ec831dcd7786?source=collection_archive---------4-----------------------#2023-05-08](https://towardsdatascience.com/llm-hallucinations-ec831dcd7786?source=collection_archive---------4-----------------------#2023-05-08)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Opinion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: How LLMs can make stuff up and what to do about it
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://franklyai.medium.com/?source=post_page-----ec831dcd7786--------------------------------)[![Frank
    Neugebauer](../Images/0da70d082d0f9c7ad8ccf574ed215df2.png)](https://franklyai.medium.com/?source=post_page-----ec831dcd7786--------------------------------)[](https://towardsdatascience.com/?source=post_page-----ec831dcd7786--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----ec831dcd7786--------------------------------)
    [Frank Neugebauer](https://franklyai.medium.com/?source=post_page-----ec831dcd7786--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F29e3b5503cd1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fllm-hallucinations-ec831dcd7786&user=Frank+Neugebauer&userId=29e3b5503cd1&source=post_page-29e3b5503cd1----ec831dcd7786---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----ec831dcd7786--------------------------------)
    ·6 min read·May 8, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fec831dcd7786&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fllm-hallucinations-ec831dcd7786&user=Frank+Neugebauer&userId=29e3b5503cd1&source=-----ec831dcd7786---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fec831dcd7786&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fllm-hallucinations-ec831dcd7786&source=-----ec831dcd7786---------------------bookmark_footer-----------)![](../Images/95f952aadd9b08c63fbd537fcf27100e.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Photo by [Ahmad Dirini](https://unsplash.com/@ahmadirini?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: Main Objectives
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Working with large language models is not without risks including responses
    based on what’s called a LLM “hallucination.” Hallucinations can be a serious
    problem for LLMs because they can lead to the spread of misinformation, expose
    confidential information, and create unrealistic expectations about what LLMs
    can do. Understanding hallucinations and being critical of the information that
    they generate helps explain and mitigate problems such hallucinations can cause.
  prefs: []
  type: TYPE_NORMAL
- en: What’s a LLM Hallucination?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: LLMs are a type of artificial intelligence (AI) that are trained on massive
    datasets of text and code. They can generate text, translate languages, write
    different kinds of creative content, and answer questions in informative ways.
    However, LLMs are also prone to “hallucinating,” which means that they can generate
    text that is factually incorrect or nonsensical. As has been spoken about regularly,
    “LLMs can be confidently full of sh**.” Such hallucinations happen because LLMs
    are trained on data that is often incomplete or contradictory. As a result, they
    may learn to associate certain words or phrases with certain concepts, even if
    those associations are…
  prefs: []
  type: TYPE_NORMAL
