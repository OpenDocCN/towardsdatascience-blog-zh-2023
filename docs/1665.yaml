- en: 'Similarity Search, Part 3: Blending Inverted File Index and Product Quantization'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 相似性搜索，第3部分：结合倒排文件索引和产品量化
- en: 原文：[https://towardsdatascience.com/similarity-search-blending-inverted-file-index-and-product-quantization-a8e508c765fa?source=collection_archive---------1-----------------------#2023-05-19](https://towardsdatascience.com/similarity-search-blending-inverted-file-index-and-product-quantization-a8e508c765fa?source=collection_archive---------1-----------------------#2023-05-19)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/similarity-search-blending-inverted-file-index-and-product-quantization-a8e508c765fa?source=collection_archive---------1-----------------------#2023-05-19](https://towardsdatascience.com/similarity-search-blending-inverted-file-index-and-product-quantization-a8e508c765fa?source=collection_archive---------1-----------------------#2023-05-19)
- en: Discover how to combine two basic similarity search indexes to get the advantages
    of both
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 了解如何结合两种基本的相似性搜索索引，以获得两者的优势
- en: '[](https://medium.com/@slavahead?source=post_page-----a8e508c765fa--------------------------------)[![Vyacheslav
    Efimov](../Images/db4b02e75d257063e8e9d3f1f75d9d6d.png)](https://medium.com/@slavahead?source=post_page-----a8e508c765fa--------------------------------)[](https://towardsdatascience.com/?source=post_page-----a8e508c765fa--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----a8e508c765fa--------------------------------)
    [Vyacheslav Efimov](https://medium.com/@slavahead?source=post_page-----a8e508c765fa--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@slavahead?source=post_page-----a8e508c765fa--------------------------------)[![Vyacheslav
    Efimov](../Images/db4b02e75d257063e8e9d3f1f75d9d6d.png)](https://medium.com/@slavahead?source=post_page-----a8e508c765fa--------------------------------)[](https://towardsdatascience.com/?source=post_page-----a8e508c765fa--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----a8e508c765fa--------------------------------)
    [Vyacheslav Efimov](https://medium.com/@slavahead?source=post_page-----a8e508c765fa--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fc8a0ca9d85d8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsimilarity-search-blending-inverted-file-index-and-product-quantization-a8e508c765fa&user=Vyacheslav+Efimov&userId=c8a0ca9d85d8&source=post_page-c8a0ca9d85d8----a8e508c765fa---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----a8e508c765fa--------------------------------)
    ·8 min read·May 19, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fa8e508c765fa&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsimilarity-search-blending-inverted-file-index-and-product-quantization-a8e508c765fa&user=Vyacheslav+Efimov&userId=c8a0ca9d85d8&source=-----a8e508c765fa---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fc8a0ca9d85d8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsimilarity-search-blending-inverted-file-index-and-product-quantization-a8e508c765fa&user=Vyacheslav+Efimov&userId=c8a0ca9d85d8&source=post_page-c8a0ca9d85d8----a8e508c765fa---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----a8e508c765fa--------------------------------)
    ·8分钟阅读·2023年5月19日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fa8e508c765fa&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsimilarity-search-blending-inverted-file-index-and-product-quantization-a8e508c765fa&user=Vyacheslav+Efimov&userId=c8a0ca9d85d8&source=-----a8e508c765fa---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fa8e508c765fa&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsimilarity-search-blending-inverted-file-index-and-product-quantization-a8e508c765fa&source=-----a8e508c765fa---------------------bookmark_footer-----------)![](../Images/8768896b9afee7af342394c9d1259b58.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fa8e508c765fa&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsimilarity-search-blending-inverted-file-index-and-product-quantization-a8e508c765fa&source=-----a8e508c765fa---------------------bookmark_footer-----------)![](../Images/8768896b9afee7af342394c9d1259b58.png)'
- en: S**imilarity search** is a problem where given a query the goal is to find the
    most similar documents to it among all the database documents.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '**相似性搜索** 是一个问题，其中给定一个查询的目标是找到数据库中与之最相似的文档。'
- en: Introduction
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 引言
- en: In data science, similarity search often appears in the NLP domain, search engines
    or recommender systems where the most relevant documents or items need to be retrieved
    for a query. There exists a large variety of different ways to improve search
    performance in massive volumes of data.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据科学中，相似性搜索常见于自然语言处理（NLP）领域、搜索引擎或推荐系统中，这些系统需要为查询检索出最相关的文档或项目。在海量数据中提升搜索性能的方法有很多种。
- en: 'In the first two parts of this series we have discussed two fundamental algorithms
    in information retrieval: **inverted file index** and **product quantization**.
    Both of them optimize search performance but focus on different aspects: the first
    one accelerates the search speed while the latter compresses vectors to a smaller,
    memory-efficient representation.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: '[](/similarity-search-knn-inverted-file-index-7cab80cc0e79?source=post_page-----a8e508c765fa--------------------------------)
    [## Similarity Search, Part 1: kNN & Inverted File Index'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: Similarity search is a popular problem where given a query Q we need to find
    the most similar documents to it among all…
  id: totrans-13
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'towardsdatascience.com](/similarity-search-knn-inverted-file-index-7cab80cc0e79?source=post_page-----a8e508c765fa--------------------------------)
    [](https://medium.com/@slavahead/similarity-search-product-quantization-b2a1a6397701?source=post_page-----a8e508c765fa--------------------------------)
    [## Similarity Search, Part 2: Product Quantization'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: In the first part of this article series, we looked at kNN and inverted file
    index structure for performing similarity…
  id: totrans-15
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: medium.com](https://medium.com/@slavahead/similarity-search-product-quantization-b2a1a6397701?source=post_page-----a8e508c765fa--------------------------------)
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: Since both algorithm focus on different aspects, the question that naturally
    arises is whether it is possible to merge these two algorithms into a new one
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: In this article, we will combine the advantages of both approaches to produce
    a fast and memory-efficient algorithm. For information, most of the discussed
    ideas will be based on this [paper](https://lear.inrialpes.fr/pubs/2011/JDS11/jegou_searching_with_quantization.pdf).
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: Before diving into details, it is necessary to understand what residual vectors
    are and get a simple intuition on their useful properties. We will use them later
    while designing an algorithm.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: Residual vectors
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Imagine a clustering algorithm was executed and it produced several clusters.
    Each cluster has a centroid and points associated with it. **Residual** is an
    offset of a point (vector) from its centroid. Basically, to find a residual for
    a particular vector, the vector has to be subtracted from its centroid.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: If the cluster was built by the k-means algorithm, then the cluster centroid
    is the mean of all points belonging to that cluster. Thus, finding a residual
    from any point would be equivalent to subtracting the mean of a cluster from it.
    By subtracting the mean value from all points belonging to a particular cluster,
    the points would be centered around 0.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7070915590dccde2b899bc0a636574f5.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
- en: An original cluster of points is shown on the left. Then the cluster centroid
    is subtracted from all cluster points. The resulting residual vectors are shown
    on the right.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: 'We can observe a useful fact:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: Replacing original vectors with their residuals does not change their relative
    position to each other.
  id: totrans-26
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: That is, the distance between vectors stays always the same. Let us simply look
    at two equations below.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/46d8743cac599b38e4087207b97f6a19.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
- en: Subtracting the mean does not change the relative distance
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: The first equation is the formula of euclidean distance between a pair of vectors.
    In the second equation, the cluster mean value is subtracted from both vectors.
    We can see that the mean term simply cancels out — the whole expression becomes
    identical to euclidean distance in the first equation!
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: We proved this statement by using the formula for the L2 metric (euclidean distance).
    It is important to remember that this rule may not work for other metrics.
  id: totrans-31
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: So, if for a given query the goal is to find the nearest neighbour, it is possible
    to just subtract the cluster mean from the query and proceed to the normal search
    procedure among residuals.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/db343cbb826a8da69214579119ce24eb.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
- en: Subtracting a mean from a query does not change its relative position to other
    vectors
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: Right now let us look at another example in the figure below with two clusters
    where residuals for vectors of each cluster are calculated separately.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: '*Subtracting the mean of the corresponding centroid from each vector of a cluster
    will center all the dataset vectors around 0*.'
  id: totrans-36
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: That is a useful observation that will be used in the future. Additionally,
    for a given query, we can calculate query residuals to all the clusters. The query
    residuals allow us to calculate distances to the original residuals of the cluster.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1b183b38434931323d2192408830c3cb.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
- en: After subtracting mean values from each cluster, all the points become centered
    around 0\. Relative position from query and query residuals to other points of
    corresponding clusters does not change.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: Training
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Having taken into considerations useful observations in the previous section,
    we can start designing the algorithm.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: Given a vector database, an inverted file index is constructed that divides
    the set of vectors into *n* Voronoi partitions and thus reduces the search scope
    during the inference.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: Inside each Voronoi partition, the coordinates of the centroid are subtracted
    from each vector. As a result, vectors from all the partitions become closer to
    each other and centered around 0\. At this moment, there is no need the original
    vectors as we store their residuals instead.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: After that, the product quantization algorithm is run on vectors from all the
    partitions.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: '*Important aspect*: the product quantization is **not** executed for each partition
    separately — that would be inefficient because the number of partitions usually
    tends to be high which will could result in a lot of memory needed to store all
    the codebooks. Instead, **the algorithm is executed** **for all the residuals
    from all partitions simultaneously**.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: Effectively, now each subspace contains subvectors from different Voronoi partitions.
    Then, for each subspace, a clustering algorithm is performed and *k* clusters
    with their centroids are built, as usual.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，现在每个子空间包含来自不同 Voronoi 分区的子向量。然后，对于每个子空间，执行一个聚类算法，构建出如常规的 *k* 个簇及其中心点。
- en: '![](../Images/d790a9adfcd54f936d2deaab13c2390d.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d790a9adfcd54f936d2deaab13c2390d.png)'
- en: Training process
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 训练过程
- en: '*What was the importance of replacing vectors with their residuals?* If vectors
    were not replaced with their residuals, then each subspace would contain more
    various subvectors (because subspaces would store subvectors from different non-intersecting
    Voronoi partitions which could have been very far from each other in space). Now
    vectors (residuals) from different partitions overlap with each other. Since now
    there is less variety in each subspace, it takes fewer reproduction values to
    effectively represent vectors. In other words:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '*替换向量为其残差的重要性是什么？* 如果向量没有被其残差替换，那么每个子空间将包含更多的各种子向量（因为子空间将存储来自不同不相交的 Voronoi
    分区的子向量，而这些子向量可能在空间中相距很远）。现在来自不同分区的向量（残差）彼此重叠。由于现在每个子空间的变化更小，因此表示向量所需的重现值也更少。换句话说：'
- en: With the same length of PQ codes used before, vectors can be represented more
    accurately because they have less variance.
  id: totrans-50
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 使用之前相同长度的 PQ 代码，向量可以更准确地表示，因为它们的方差更小。
- en: Inference
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 推断
- en: For a given query, the *k* nearest centroids of Voronoi partitions are found.
    All the points inside these regions are considered as candidates. Since the original
    vectors were initially replaced by their residuals in each Voronoi region, the
    residual of the query vector also needs to be calculated. In this case, the query
    residual needs to be calculated separately for every Voronoi partition (because
    every region has different centroids). Only residuals from chosen Voronoi partitions
    are going to the candidates.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 对于给定的查询，找到 Voronoi 分区的 *k* 个最近中心点。所有这些区域内的点都被视为候选点。由于原始向量在每个 Voronoi 区域中最初被其残差所替代，查询向量的残差也需要被计算。在这种情况下，查询残差需要为每个
    Voronoi 分区单独计算（因为每个区域有不同的中心点）。只有来自所选 Voronoi 分区的残差才会成为候选点。
- en: The query residual is then split into subvectors. As in the original product
    quantization algorithm, for each subspace, the distance matrix *d* containing
    distances from subspace centroids to the query subvector is computed. It is essential
    to keep in mind that the query residual is different for each Voronoi partition.
    Basically it means that the distance matrix *d* needs to be computed separately
    for each of the query residuals. That is the computation price for the desired
    optimization.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 查询残差随后被拆分为子向量。与原始的产品量化算法相同，对于每个子空间，计算包含从子空间中心点到查询子向量的距离的距离矩阵 *d*。必须记住，查询残差在每个
    Voronoi 分区中都是不同的。这基本上意味着距离矩阵 *d* 需要为每个查询残差单独计算。这是所需优化的计算代价。
- en: Finally, partial distances are summed up as it was previously done in the product
    quantization algorithm.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，部分距离被汇总，就像在产品量化算法中之前所做的那样。
- en: Sorting results
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 排序结果
- en: After all the distances are calculated, the *k* nearest neighbors need to be
    selected. To do it efficiently, authors propose maintaining a [MaxHeap](https://medium.com/@slavahead/heapify-with-heap-sort-5df23b5764c1)
    data structure. It has a limited capacity of *k* and at each step, it stores *k*
    current smallest distances. Whenever a new distance is calculated, its value is
    added to the MaxHeap only if the computed value is less than the largest value
    in the MaxHeap. After calculating all the distances, the answer to the query is
    already stored in the MaxHeap. The advantage of using the MaxHeap is its fast
    building time which is *O(n)*.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在计算所有距离后，需要选择 *k* 个最近邻点。为了高效完成这一过程，作者建议使用一个 [MaxHeap](https://medium.com/@slavahead/heapify-with-heap-sort-5df23b5764c1)
    数据结构。它的容量有限为 *k*，并在每一步中存储 *k* 个当前最小的距离。每当计算出一个新距离时，只有当计算出的值小于 MaxHeap 中的最大值时，该值才会被添加到
    MaxHeap 中。计算完所有距离后，查询的答案已经存储在 MaxHeap 中。使用 MaxHeap 的优点是其构建时间很快，为 *O(n)*。
- en: '![](../Images/e743b5800768527c67c5281736ef73a8.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e743b5800768527c67c5281736ef73a8.png)'
- en: Inference process
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 推断过程
- en: Performance
  id: totrans-59
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 性能
- en: The algorithm takes advantage of both inverted file index and product quantization.
    Depending on the number of Voronoi partitions probe during the inference, the
    same number of subvector-to-centroid matrices *d* needs to be computed and stored
    in the memory. This might look like a downside but comparing it to overall advantages,
    it is a pretty good trade-off.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 该算法利用了倒排文件索引和产品量化。根据推理过程中 Voronoi 分区的数量，相同数量的子向量到质心矩阵 *d* 需要计算并存储在内存中。这可能看起来像是一个缺点，但与整体优势相比，这是一个相当好的折衷。
- en: '![](../Images/d6b859afba6649a67a22658427d08a91.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d6b859afba6649a67a22658427d08a91.png)'
- en: The algorithm inherits a good search speed from inverted file index and compression
    efficiency from product quantization
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 该算法从倒排文件索引继承了良好的搜索速度，从产品量化继承了压缩效率。
- en: Faiss implementation
  id: totrans-63
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Faiss 实现
- en: '[**Faiss**](https://github.com/facebookresearch/faiss) (Facebook AI Search
    Similarity) is a Python library written in C++ used for optimised similarity search.
    This library presents different types of indexes which are data structures used
    to efficiently store the data and perform queries.'
  id: totrans-64
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '[**Faiss**](https://github.com/facebookresearch/faiss)（Facebook AI 搜索相似性）是一个用
    C++ 编写的 Python 库，用于优化相似性搜索。该库提供了不同类型的索引，这些数据结构用于高效地存储数据和执行查询。'
- en: Based on the information from the [Faiss documentation](https://faiss.ai), we
    will see how inverted file and product quantization indexes can be combined together
    to form a new index.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 根据 [Faiss 文档](https://faiss.ai) 的信息，我们将了解如何将倒排文件和产品量化索引组合在一起形成新的索引。
- en: 'Faiss implements the described algorithm in the *IndexIVFPQ* class which accepts
    the following arguments:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: Faiss 在 *IndexIVFPQ* 类中实现了上述算法，该类接受以下参数：
- en: '**quantizer**: specifies how distance between vectors is computed.'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**quantizer**：指定计算向量之间距离的方式。'
- en: '**d**: data dimensionality.'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**d**：数据维度。'
- en: '**nlist**: number of Voronoi partitions.'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**nlist**：Voronoi 分区的数量。'
- en: '**M**: number of subspaces.'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**M**：子空间的数量。'
- en: '**nbits**: number of bits it takes to encode a single cluster ID. This means
    that the number of total clusters in a single subspace will be equal to *k = 2^nbits*.'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**nbits**：编码单个簇 ID 所需的位数。这意味着单个子空间中的簇总数将等于 *k = 2^nbits*。'
- en: Additionally, it is possible to adjust the **nprobe** attribute which specifies
    how many Voronoi partitions must be used for the search of candidates during inference.
    Changing this parameter does not require retraining.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，可以调整 **nprobe** 属性，该属性指定在推理过程中用于搜索候选项的 Voronoi 分区数量。更改此参数无需重新训练。
- en: '![](../Images/6725c5f1ff01d708620078a43c2de613.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6725c5f1ff01d708620078a43c2de613.png)'
- en: Faiss implementation of IndexIVFPQ
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: Faiss 的 IndexIVFPQ 实现
- en: Memory required to store a single vector is the same as in the original product
    quantization method except now we add 8 more bytes to store information about
    the vector in the inverted file index.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 存储单个向量所需的内存与原始产品量化方法相同，只是现在我们增加了 8 个字节，用于在倒排文件索引中存储关于向量的信息。
- en: '![](../Images/3c990b125612a4c97a448acbe3bd99a9.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3c990b125612a4c97a448acbe3bd99a9.png)'
- en: Conclusion
  id: totrans-77
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: Using the knowledge from the previous article parts, we have walked through
    the implementation of a state-of-the-art algorithm that achieves high memory compression
    and accelerated search speed. This algorithm is widely used in information retrieval
    systems when dealing with massive volumes of data.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 利用之前文章部分的知识，我们探讨了一个先进算法的实现，该算法实现了高效的内存压缩和加速的搜索速度。该算法在处理大量数据时广泛用于信息检索系统。
- en: Resources
  id: totrans-79
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 资源
- en: '[Product quantization for the nearest search](https://lear.inrialpes.fr/pubs/2011/JDS11/jegou_searching_with_quantization.pdf)'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[最近邻搜索的产品量化](https://lear.inrialpes.fr/pubs/2011/JDS11/jegou_searching_with_quantization.pdf)'
- en: '[Faiss documentation](https://faiss.ai)'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Faiss 文档](https://faiss.ai)'
- en: '[Faiss repository](https://github.com/facebookresearch/faiss)'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Faiss 仓库](https://github.com/facebookresearch/faiss)'
- en: '[Summary of Faiss indexes](https://github.com/facebookresearch/faiss/wiki/Faiss-indexes)'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Faiss 索引汇总](https://github.com/facebookresearch/faiss/wiki/Faiss-indexes)'
- en: '*All images unless otherwise noted are by the author.*'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '*除非另有说明，所有图片均由作者提供。*'
