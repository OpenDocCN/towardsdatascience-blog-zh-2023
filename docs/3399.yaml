- en: The Rise of Vision Transformers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/the-rise-of-vision-transformers-f623c980419f?source=collection_archive---------10-----------------------#2023-11-15](https://towardsdatascience.com/the-rise-of-vision-transformers-f623c980419f?source=collection_archive---------10-----------------------#2023-11-15)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Is the era of ResNet coming to an end?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://natecibik.medium.com/?source=post_page-----f623c980419f--------------------------------)[![Nate
    Cibik](../Images/008c22b715ddf4f1d0f9970142edc09f.png)](https://natecibik.medium.com/?source=post_page-----f623c980419f--------------------------------)[](https://towardsdatascience.com/?source=post_page-----f623c980419f--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----f623c980419f--------------------------------)
    [Nate Cibik](https://natecibik.medium.com/?source=post_page-----f623c980419f--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F82bf2304955e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-rise-of-vision-transformers-f623c980419f&user=Nate+Cibik&userId=82bf2304955e&source=post_page-82bf2304955e----f623c980419f---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----f623c980419f--------------------------------)
    ·9 min read·Nov 15, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Ff623c980419f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-rise-of-vision-transformers-f623c980419f&user=Nate+Cibik&userId=82bf2304955e&source=-----f623c980419f---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff623c980419f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-rise-of-vision-transformers-f623c980419f&source=-----f623c980419f---------------------bookmark_footer-----------)![](../Images/1fa5c6fd2b774522a9b88d6bc5e03e21.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Image by author.
  prefs: []
  type: TYPE_NORMAL
- en: Over the last decade, computer vision has gone from a promising area of research
    to a cornerstone of the modern technological revolution. Having once turned heads
    with the accurate classification of small pixelated images, these technologies
    can now generate high-resolution images like the one above from thin air from
    nothing but a short description. Such stunning capabilities are currently the
    domain of immense model sizes, but not all models that affect our everyday lives
    are large.
  prefs: []
  type: TYPE_NORMAL
- en: Another line of developments in the past decade of computer vision has been
    the simultaneous gains in performance and parameter efficiency of smaller models,
    with their compact memory footprints and higher frame rates enabling developments
    in areas like smartphone photography, augmented reality, and robotics. Pixelwise
    semantic segmentation and depth estimation, along with detection and tracking
    of objects and their 3D poses, can now be performed in real time on increasingly
    large image sizes, helping autonomous vehicles to plan their daily maneuvers through
    traffic.
  prefs: []
  type: TYPE_NORMAL
- en: This article discusses an important development in backbone selection for these
    smaller vision architectures. As we will see, there is a new family of models
    with performance powerful and lightweight enough to warrant a paradigm shift,
    and allow the perseverant stalwarts of the past era to retire.
  prefs: []
  type: TYPE_NORMAL
- en: The Convolutional Era
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Since [AlexNet](https://papers.nips.cc/paper_files/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html)
    shattered the ImageNet challenge in 2012 and [ignited the world of deep learning
    research](/what-alexnet-brought-to-the-world-of-deep-learning-46c7974b46fc), Convolutional
    Neural Networks ([CNNs](https://ieeexplore.ieee.org/document/6795724)) have dominated
    the computer vision landscape. In particular, the [ResNet](https://arxiv.org/abs/1512.03385)
    model famously solved the [vanishing gradient problem](https://en.wikipedia.org/wiki/Vanishing_gradient_problem)
    and improved parameter efficiency in 2015 with the introduction of skip (“residual”)
    connections, subsequently gaining near ubiquitous use as a backbone for various
    downstream tasks thanks to its strong knowledge transferability, supplanting [VGGNet](https://arxiv.org/abs/1409.1556)
    and becoming the most popular backbone architecture of the time.
  prefs: []
  type: TYPE_NORMAL
- en: And there it has largely remained for nearly eight years, an epic lifespan in
    the rapid-fire world of computer vision research. But now, there are challengers
    approaching the throne. Following the meteoric rise of transformers in Natural
    Language Processing (NLP) that started in 2017 with “[Attention is All You Need](https://arxiv.org/abs/1706.03762)”
    and grew into the LLM phenomenon seen today, Vision Transformer ([ViT](https://arxiv.org/abs/2010.11929))
    was the first to show in late October 2020 that a pure transformer architecture
    could achieve state-of-the-art performance in computer vision tasks, although
    requiring many more training epochs, resources, and data to do so.
  prefs: []
  type: TYPE_NORMAL
- en: It turns out that the design of CNNs gives them inductive biases such as translation
    equivariance (via weight sharing) and locality (via small filter sizes) [that
    make them inherently effective on the structure of image data](https://arxiv.org/abs/2305.08404).
    Transformers can learn these biases from scratch with enough training, but this
    originally meant that transformers were far less data efficient than CNNs. On
    the other hand, while CNNs come standard with biases that aid their ability to
    learn and operate on image data, they lack the capability for global dependency
    modeling across individual layers offered by the self-attention in transformers,
    and this cannot be learned with any amount of data.
  prefs: []
  type: TYPE_NORMAL
- en: A study titled “[Are Transformers More Robust Than CNNs?](https://arxiv.org/abs/2111.05464)”
    found that while the two architectures are equally robust to adversarial attacks
    when trained with the same augmentation strategies, transformers demonstrated
    better generalization to out-of-distribution (OOD) samples in comparison to larger
    ResNet models trained on any strategy, and the ablation studies indicated that
    the advantage is driven by the self-attention mechanism.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9aa5224fdbf902311b9354ca75f6b714.png)'
  prefs: []
  type: TYPE_IMG
- en: '[DeiT](https://arxiv.org/abs/2012.12877) consistently outperforms ResNet on
    OOD samples with fewer parameters. Graphic courtesy of [https://arxiv.org/pdf/2111.05464.pdf](https://arxiv.org/pdf/2111.05464.pdf)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Thus, the attractive qualities of CNNs that make them data efficient when learning
    image features also seemed to herald their downfall, since their limited receptive
    fields prevent them from modeling global context, a nontrivial weakness. In the
    years preceding the ViT publication, several attempts were made to address the
    long-range dependency issue in CNNs: dilated (aka “atrous”) convolution was explored
    in models like [DeepLab](https://arxiv.org/abs/1412.7062) to increase receptive
    field, [Squeeze-and-Excitation](https://arxiv.org/abs/1709.01507), [Non-Local
    Neural Networks](https://arxiv.org/abs/1711.07971), and [Attention Augmented Convolutional
    Neural Networks](https://arxiv.org/abs/1904.09925) demonstrate methods to introduce
    attention into convolutional operations. Bottleneck Transformers ([BoTNet](https://arxiv.org/abs/2101.11605))
    saw notable performance gain at larger image resolutions by replacing the spatial
    convolution bottleneck blocks in the last 3 layers of ResNet with multi-head self-attention
    blocks, but larger gains were yet to come.'
  prefs: []
  type: TYPE_NORMAL
- en: Vision Transformers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Before transformers could hope to usurp the throne, they also had architectural
    weaknesses to overcome. The ViT paper was as helpful in demonstrating that a pure
    transformer architecture could achieve state-of-the-art performance in computer
    vision tasks as it was in illuminating the challenges posed by this application,
    including the aforementioned data inefficiency, untenable quadratic complexity
    of self-attention in relation to image size, and the lack of multi-scale features
    that are useful in dense prediction tasks like semantic segmentation and monocular
    depth estimation.
  prefs: []
  type: TYPE_NORMAL
- en: “While these initial results are encouraging, many challenges remain. One is
    to apply ViT to other computer vision tasks, such as detection and segmentation.”
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: — Call to action from [ViT](https://arxiv.org/abs/2010.11929) paper conclusion.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The research community clearly read the 2020 ViT paper like a TODO list, and
    2021 was a busy year. First, Data-efficient Image Transformer ([DeiT](https://arxiv.org/abs/2012.12877))
    tackled the data efficiency problem by demonstrating that augmentation methods
    and knowledge distillation from teacher models were effective in training ViT
    models well past their previous performance benchmarks in fewer epochs using only
    the ImageNet dataset, eliminating the need for massive external datasets, and
    bringing convolution-free vision models into the realm of widespread adoptability.
    Interestingly, it seemed that using CNNs for the teacher models worked significantly
    better than the pre-trained DeiTs, possibly because this process teaches their
    structural biases to the student. Vision transformers were now an official threat
    to the status quo in terms of data efficiency, but they were still lacking multiscale
    features.
  prefs: []
  type: TYPE_NORMAL
- en: “Therefore, considering our results, where image transformers are on par with
    convnets already, we believe that they will rapidly become a method of choice
    considering their lower memory footprint for a given accuracy.”
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: — Prophesying from the [DeiT](https://arxiv.org/abs/2012.12877) paper conclusion.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'In late February, a paper titled “[Pyramid Vision Transformer: A Versatile
    Backbone for Dense Prediction without Convolutions](https://arxiv.org/abs/2102.12122)”
    addressed both the lack of multiscale features and self-attention complexity through
    the use of convolutions. [Swin Transformer](https://arxiv.org/abs/2103.14030)
    infuses the structural biases of CNNs, generates multiscale feature maps, and
    reduces self-attention complexity by attending over shifting local windows, but
    like PVT, still relies on positional encoding in the patches. Convolutional Vision
    Transformer ([CvT](https://arxiv.org/abs/2103.15808)) then changed the game, removing
    the need for positional encodings by generating overlapping patch embeddings via
    zero-padded 2D convolution, exploiting a property of CNNs that enables them to
    [encode positional information from zero-padding](https://arxiv.org/abs/2001.08248).'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/710c61b596dd2c8108adf4530164d8d7.png)'
  prefs: []
  type: TYPE_IMG
- en: This table from the [CvT](https://arxiv.org/abs/2103.15808) paper is a helpful
    summary of the work in vision transformers up to their publication. Interestingly,
    a look at the [PVT code](https://github.com/whai362/PVT/blob/57e2dfaa5a46f9050d76f306a4fcd9a7c061f520/segmentation/pvt.py#L51)
    reveals that “spatial reduction” is just a euphemism for 2D convolution.
  prefs: []
  type: TYPE_NORMAL
- en: Then, a new version of the PVT ([PVTv2](https://arxiv.org/abs/2106.13797)) was
    published simultaneously with [Segformer](https://arxiv.org/abs/2105.15203), the
    latter of which used the former as a backbone to achieve state-of-the-art semantic
    segmentation performance with impressive parameter efficiency using a lightweight
    all-MLP decoder. Global-Local Path Network ([GLPN](https://arxiv.org/abs/2201.07436))
    then used this backbone to achieve competitive results in monocular depth estimation
    (another dense prediction task).
  prefs: []
  type: TYPE_NORMAL
- en: 'To understand what a breakthrough the PVTv2 really is as a backbone, let’s
    look at some benchmarks, starting with a chart from the Segformer paper:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/690ebeb8bac39642e1298b1f0e59c804.png)'
  prefs: []
  type: TYPE_IMG
- en: Graphic from [Segformer](https://arxiv.org/abs/2105.15203) paper demonstrating
    superior performance and efficiency on the semantic segmentation task against
    contemporaneous models.
  prefs: []
  type: TYPE_NORMAL
- en: The above graphic clearly shows superior performance and parameter efficiency
    compared to contemporaneous hierarchical transformers and ResNet-backed architectures
    (see the [Segformer](https://arxiv.org/abs/2105.15203) paper for more detailed
    comparisons).
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we can review an example of PVTv2 backing a more complex architecture.
    [Deformable DETR](https://arxiv.org/abs/2010.04159) was a strong improvement on
    the Detection Transformer ([DETR](https://arxiv.org/abs/2005.12872)), with far
    more computational and data efficiency thanks to the proposed deformable attention,
    and trainable in 10x fewer epochs. [Panoptic Segformer](https://arxiv.org/abs/2109.03814)
    was a natural extension on top of Deformable DETR for panoptic segmentation that
    was released later in 2021, and there is one comparison chart from this paper
    that is particularly informative to our discussion:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9ab1dab3df89f0a9d2a80923bbf3dc59.png)'
  prefs: []
  type: TYPE_IMG
- en: Chart from [Panoptic Segformer](https://arxiv.org/abs/2109.03814) paper. Note
    the two best performing models at the bottom built with Swin-L and PVTv2 backbones,
    where the latter has comparable performance with half the parameters.
  prefs: []
  type: TYPE_NORMAL
- en: Regrettably, the authors did not benchmark any sizes of PVTv2 backbone other
    than the largest (B5) version, but we can still glean some important insights
    from this example. First, we see that although the Swin-L backbone was technically
    the most performant of the larger models, the PVTv2-B5 had nearly identical performance
    with less than half the parameters and FLOPS. Further, (although the PVTv2-B3
    would be a better comparison) we can see that the PVTv2-B5 is significantly more
    performant than the ResNet101 backbone.
  prefs: []
  type: TYPE_NORMAL
- en: The success of these models suggests that the Mix Transformer (MiT) encoding
    layers in the PVTv2 are capable of producing extremely descriptive features with
    an efficient number of parameters, thanks to adopting a “best of both worlds”
    approach. By incorporating convolutional operations into transformer layers to
    make self-attention more efficient, create multiscale features, and infuse inductive
    biases beneficial to the efficient learning of image data, these hierarchical
    transformer models may be the next household name in computer vision model development.
  prefs: []
  type: TYPE_NORMAL
- en: The Winner
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/317414f790b180d03885017d303d110e.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author, inspired by [this image](https://images.app.goo.gl/mX4NGhZK3XGHHKe77).
  prefs: []
  type: TYPE_NORMAL
- en: And so, it appears that the answer is not a fight to the death between CNNs
    and Transformers (see the many [overindulgent](/deep-learning-no-lstms-are-not-dead-20217553b87a)
    [eulogies](/the-fall-of-rnn-lstm-2d1594c74ce0) for LSTMs), but rather something
    a bit more romantic. Not only does the adoption of 2D convolutions in hierarchical
    transformers like CvT and PVTv2 conveniently create multiscale features, reduce
    the complexity of self-attention, and simplify architecture by alleviating the
    need for positional encoding, but these models also employ residual connections,
    another inherited trait of their progenitors. The complementary strengths of transformers
    and CNNs have been brought together in viable offspring.
  prefs: []
  type: TYPE_NORMAL
- en: So is the era of ResNet over? It would certainly seem so, although any paper
    will surely need to include this indefatigable backbone for comparison for some
    time to come. It is important to remember, however, that there are no losers here,
    just a new generation of powerful and transferable feature extractors for all
    to enjoy, if they know where to look. Parameter efficient models like PVTv2 democratize
    research of more complex architectures by offering powerful feature extraction
    with a small memory footprint, and deserve to be added to the list of standard
    backbones for benchmarking new architectures.
  prefs: []
  type: TYPE_NORMAL
- en: Future Work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This article has focused on how the cross-pollination of convolutional operations
    and self-attention has given us the evolution of hierarchical feature transformers.
    These models have shown dominant performance and parameter efficiency at small
    scales, making them ideal feature extraction backbones (especially in parameter-constrained
    environments). However, there is a lack of exploration into whether the efficiencies
    and inductive biases that these models capitalize on at smaller scales can transfer
    to large-scale success and threaten the dominance of pure ViTs at much higher
    parameter counts.
  prefs: []
  type: TYPE_NORMAL
- en: 'Large Multimodal Models (LMMS) like Large Language and Visual Assistant ([LLaVA](https://arxiv.org/abs/2304.08485))
    and other applications that require a natural language understanding of visual
    data rely on Contrastive Language–Image Pretraining ([CLIP](https://arxiv.org/abs/2103.00020))
    embeddings generated from ViT-L features, and therefore inherit the strengths
    and weaknesses of ViT. If research into scaling hierarchical transformers shows
    that their benefits, such as multiscale features that enhance fine-grained understanding,
    enable them to to achieve better or similar performance with greater parameter
    efficiency than ViT-L, it would have widespread and immediate practical impact
    on anything using CLIP: LMMs, robotics, assistive technologies, augmented/virtual
    reality, content moderation, education, research, and many more applications affecting
    society and industry could be improved and made more efficient, lowering the barrier
    for development and deployment of these technologies.'
  prefs: []
  type: TYPE_NORMAL
