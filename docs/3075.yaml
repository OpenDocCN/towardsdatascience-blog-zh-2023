- en: Matrix Multiplication on GPU
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: GPU ä¸Šçš„çŸ©é˜µä¹˜æ³•
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/matrix-multiplication-on-the-gpu-e920e50207a8?source=collection_archive---------1-----------------------#2023-10-09](https://towardsdatascience.com/matrix-multiplication-on-the-gpu-e920e50207a8?source=collection_archive---------1-----------------------#2023-10-09)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://towardsdatascience.com/matrix-multiplication-on-the-gpu-e920e50207a8?source=collection_archive---------1-----------------------#2023-10-09](https://towardsdatascience.com/matrix-multiplication-on-the-gpu-e920e50207a8?source=collection_archive---------1-----------------------#2023-10-09)
- en: How to achieve state-of-the-art matrix multiplication performance in CUDA.
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å¦‚ä½•åœ¨ CUDA ä¸­å®ç°æœ€å…ˆè¿›çš„çŸ©é˜µä¹˜æ³•æ€§èƒ½ã€‚
- en: '[](https://medium.com/@andylolu24?source=post_page-----e920e50207a8--------------------------------)[![Andy
    Lo](../Images/07ba4bfa6792696a82c251f5e9d5b9ac.png)](https://medium.com/@andylolu24?source=post_page-----e920e50207a8--------------------------------)[](https://towardsdatascience.com/?source=post_page-----e920e50207a8--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----e920e50207a8--------------------------------)
    [Andy Lo](https://medium.com/@andylolu24?source=post_page-----e920e50207a8--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@andylolu24?source=post_page-----e920e50207a8--------------------------------)[![Andy
    Lo](../Images/07ba4bfa6792696a82c251f5e9d5b9ac.png)](https://medium.com/@andylolu24?source=post_page-----e920e50207a8--------------------------------)[](https://towardsdatascience.com/?source=post_page-----e920e50207a8--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----e920e50207a8--------------------------------)
    [Andy Lo](https://medium.com/@andylolu24?source=post_page-----e920e50207a8--------------------------------)'
- en: Â·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Â·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F9b10f678560b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmatrix-multiplication-on-the-gpu-e920e50207a8&user=Andy+Lo&userId=9b10f678560b&source=post_page-9b10f678560b----e920e50207a8---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----e920e50207a8--------------------------------)
    Â·10 min readÂ·Oct 9, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fe920e50207a8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmatrix-multiplication-on-the-gpu-e920e50207a8&user=Andy+Lo&userId=9b10f678560b&source=-----e920e50207a8---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[å…³æ³¨](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F9b10f678560b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmatrix-multiplication-on-the-gpu-e920e50207a8&user=Andy+Lo&userId=9b10f678560b&source=post_page-9b10f678560b----e920e50207a8---------------------post_header-----------)
    å‘è¡¨åœ¨ [Towards Data Science](https://towardsdatascience.com/?source=post_page-----e920e50207a8--------------------------------)
    Â·10 åˆ†é’Ÿé˜…è¯»Â·2023 å¹´ 10 æœˆ 9 æ—¥[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fe920e50207a8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmatrix-multiplication-on-the-gpu-e920e50207a8&user=Andy+Lo&userId=9b10f678560b&source=-----e920e50207a8---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fe920e50207a8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmatrix-multiplication-on-the-gpu-e920e50207a8&source=-----e920e50207a8---------------------bookmark_footer-----------)![](../Images/52104d461a859da63d47995f461249b9.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fe920e50207a8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmatrix-multiplication-on-the-gpu-e920e50207a8&source=-----e920e50207a8---------------------bookmark_footer-----------)![](../Images/52104d461a859da63d47995f461249b9.png)'
- en: â€œA minimalist art taking inspiration from Matrix Multiplication, in the style
    of vaporwaveâ€ by DALLE-2
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: â€œä»çŸ©é˜µä¹˜æ³•ä¸­æ±²å–çµæ„Ÿçš„æç®€è‰ºæœ¯ï¼Œé£æ ¼ä¸º vaporwaveâ€ â€”â€” DALLE-2
- en: 'This blog came from a sudden realisation of how little I knew about how matrix
    multiplication works on the GPU. Having done so many ML projects, I feel like
    I ought to understand how the most important operation in ML works: What is this
    â€œTensor Coreâ€ thing? Why does everyone say â€œ*data movement is the bottleneck*â€?
    How fast can GPUs actually go?'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ç¯‡åšå®¢æºäºæˆ‘çªç„¶æ„è¯†åˆ°è‡ªå·±å¯¹çŸ©é˜µä¹˜æ³•åœ¨ GPU ä¸Šå¦‚ä½•è¿ä½œçŸ¥ä¹‹ç”šå°‘ã€‚åšäº†è¿™ä¹ˆå¤šæœºå™¨å­¦ä¹ é¡¹ç›®ï¼Œæˆ‘è§‰å¾—æˆ‘åº”è¯¥äº†è§£è¿™ä¸ªåœ¨æœºå™¨å­¦ä¹ ä¸­æœ€é‡è¦çš„æ“ä½œæ˜¯å¦‚ä½•å·¥ä½œçš„ï¼šä»€ä¹ˆæ˜¯â€œå¼ é‡æ ¸å¿ƒâ€ï¼Ÿä¸ºä»€ä¹ˆæ¯ä¸ªäººéƒ½è¯´â€œ*æ•°æ®ç§»åŠ¨æ˜¯ç“¶é¢ˆ*â€ï¼ŸGPU
    å®é™…ä¸Šèƒ½æœ‰å¤šå¿«ï¼Ÿ
- en: To answer these questions, I decided that I must go out of my PyTorch bubble
    and **venture into the abyss of CUDA**. I wrote this blog to document all that
    I have learnt, and hopefully anyone reading this wouldnâ€™t have to go through the
    pain of digging through CUDA docs/code as I did.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†å›ç­”è¿™äº›é—®é¢˜ï¼Œæˆ‘å†³å®šå¿…é¡»èµ°å‡ºæˆ‘çš„ PyTorch é¢†åŸŸï¼Œ**æ·±å…¥æ¢ç´¢ CUDA çš„æ·±æ¸Š**ã€‚æˆ‘å†™äº†è¿™ç¯‡åšå®¢æ¥è®°å½•æˆ‘æ‰€å­¦åˆ°çš„ä¸€åˆ‡ï¼Œå¸Œæœ›è¯»åˆ°è¿™ç¯‡æ–‡ç« çš„ä»»ä½•äººéƒ½ä¸å¿…åƒæˆ‘ä¸€æ ·ç»å†æŒ–æ˜
    CUDA æ–‡æ¡£/ä»£ç çš„ç—›è‹¦ã€‚
- en: If there is anything that Iâ€™ve learnt in this journey, it is **concurrent matrix
    multiplication is HARD**. Efficient matrix multiplication heavily depends on the
    specific hardware you are using and the problem size you are trying to solve.
    There is no one-size-fits-all solution.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæˆ‘åœ¨è¿™æ®µæ—…ç¨‹ä¸­å­¦åˆ°äº†ä»€ä¹ˆï¼Œé‚£å°±æ˜¯**å¹¶å‘çŸ©é˜µä¹˜æ³•æ˜¯å›°éš¾çš„**ã€‚é«˜æ•ˆçš„çŸ©é˜µä¹˜æ³•åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šä¾èµ–äºä½ ä½¿ç”¨çš„å…·ä½“ç¡¬ä»¶å’Œä½ å°è¯•è§£å†³çš„é—®é¢˜è§„æ¨¡ã€‚æ²¡æœ‰ä¸€åˆ€åˆ‡çš„è§£å†³æ–¹æ¡ˆã€‚
- en: Enough nagging, letâ€™s dig in!
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: å¤Ÿäº†ï¼Œè®©æˆ‘ä»¬æ·±å…¥äº†è§£å§ï¼
- en: Recap on GPU architecture
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å›é¡¾GPUæ¶æ„
- en: Letâ€™s remind ourselves how (NVIDIA) GPUs work. A GPU achieves parallelism by
    running many **threads**. Each thread is executed on a single CUDA core, though
    at a given time, only a subset of the threads are active so there can be many
    more threads than CUDA cores available. Each thread, no matter it is active or
    not, has its own set of **registers**.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬å›é¡¾ä¸€ä¸‹ï¼ˆNVIDIAï¼‰GPUçš„å·¥ä½œåŸç†ã€‚GPUé€šè¿‡è¿è¡Œè®¸å¤š**çº¿ç¨‹**æ¥å®ç°å¹¶è¡Œå¤„ç†ã€‚æ¯ä¸ªçº¿ç¨‹åœ¨ä¸€ä¸ªCUDAæ ¸å¿ƒä¸Šæ‰§è¡Œï¼Œä½†åœ¨æŸä¸€æ—¶åˆ»ï¼Œåªæœ‰ä¸€éƒ¨åˆ†çº¿ç¨‹æ˜¯æ´»åŠ¨çš„ï¼Œå› æ­¤å¯èƒ½æœ‰æ¯”å¯ç”¨çš„CUDAæ ¸å¿ƒæ›´å¤šçš„çº¿ç¨‹ã€‚æ¯ä¸ªçº¿ç¨‹ï¼Œæ— è®ºæ˜¯å¦æ´»åŠ¨ï¼Œéƒ½æœ‰è‡ªå·±çš„**å¯„å­˜å™¨**ã€‚
- en: A group of 32 threads is known as a **warp**. All threads in a warp must execute
    together (or be inactive together). In most cases, there are a lot more inactive
    warps than active warps, and the **warp scheduler** is responsible for choosing
    which warps to execute at a given time. This allows the GPU to hide the latency
    of memory accesses by scheduling other warps to execute while a warp is waiting
    for data.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ç»„32ä¸ªçº¿ç¨‹ç§°ä¸º**warp**ã€‚warpä¸­çš„æ‰€æœ‰çº¿ç¨‹å¿…é¡»ä¸€èµ·æ‰§è¡Œï¼ˆæˆ–ä¸€èµ·å¤„äºéæ´»åŠ¨çŠ¶æ€ï¼‰ã€‚åœ¨å¤§å¤šæ•°æƒ…å†µä¸‹ï¼Œéæ´»åŠ¨warpçš„æ•°é‡è¿œå¤šäºæ´»åŠ¨warpï¼Œè€Œ**warpè°ƒåº¦å™¨**è´Ÿè´£é€‰æ‹©åœ¨ç‰¹å®šæ—¶é—´æ‰§è¡Œå“ªäº›warpã€‚è¿™ä½¿å¾—GPUèƒ½å¤Ÿé€šè¿‡è°ƒåº¦å…¶ä»–warpåœ¨warpç­‰å¾…æ•°æ®æ—¶æ‰§è¡Œï¼Œä»è€Œéšè—å†…å­˜è®¿é—®çš„å»¶è¿Ÿã€‚
- en: A group of warps is known as a **threadblock**. All warps in a threadblock are
    executed in the same **Streaming Multiprocessor** (SM). Each threadblock has its
    own **shared memory** that can be accessed by all threads in the threadblock.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ç»„warpç§°ä¸º**çº¿ç¨‹å—**ã€‚æ‰€æœ‰çº¿ç¨‹å—ä¸­çš„warpåœ¨åŒä¸€ä¸ª**æµå¤„ç†å™¨**ï¼ˆSMï¼‰ä¸­æ‰§è¡Œã€‚æ¯ä¸ªçº¿ç¨‹å—æœ‰è‡ªå·±çš„**å…±äº«å†…å­˜**ï¼Œæ‰€æœ‰çº¿ç¨‹å—ä¸­çš„çº¿ç¨‹éƒ½å¯ä»¥è®¿é—®ã€‚
- en: '**Note: Newer architectures**'
  id: totrans-17
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**æ³¨æ„ï¼šè¾ƒæ–°çš„æ¶æ„**'
- en: ''
  id: totrans-18
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: From Volta architecture onwards, each thread also has its own program counter
    and call stack etc. This means that each thread in a warp can execute different
    instructions at the same time.
  id: totrans-19
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ä»Voltaæ¶æ„å¼€å§‹ï¼Œæ¯ä¸ªçº¿ç¨‹ä¹Ÿæœ‰è‡ªå·±çš„ç¨‹åºè®¡æ•°å™¨å’Œè°ƒç”¨æ ˆç­‰ã€‚è¿™æ„å‘³ç€warpä¸­çš„æ¯ä¸ªçº¿ç¨‹å¯ä»¥åŒæ—¶æ‰§è¡Œä¸åŒçš„æŒ‡ä»¤ã€‚
- en: ''
  id: totrans-20
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The Volta architecture also introduced **Tensor Cores** that are specialised
    to solve matrix multiplications of specific sizes. Each active warp have access
    to one Tensor Core.
  id: totrans-21
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Voltaæ¶æ„è¿˜å¼•å…¥äº†**Tensor Cores**ï¼Œè¿™äº›æ ¸å¿ƒä¸“é—¨ç”¨äºè§£å†³ç‰¹å®šå¤§å°çš„çŸ©é˜µä¹˜æ³•ã€‚æ¯ä¸ªæ´»åŠ¨warpå¯ä»¥è®¿é—®ä¸€ä¸ªTensor Coreã€‚
- en: ''
  id: totrans-22
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: In the newest Hopper architecture, there is a concept of **threadblock clusters**
    that represents a group of threadblocks. It gives the user more fine-grained control
    over the scheduling of threadblocks and allows the shared memory of one threadblock
    to be access by other threadblocks in the same cluster.
  id: totrans-23
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åœ¨æœ€æ–°çš„Hopperæ¶æ„ä¸­ï¼Œå¼•å…¥äº†**çº¿ç¨‹å—é›†ç¾¤**çš„æ¦‚å¿µï¼Œå®ƒè¡¨ç¤ºä¸€ç»„çº¿ç¨‹å—ã€‚å®ƒä½¿ç”¨æˆ·èƒ½å¤Ÿæ›´ç»†ç²’åº¦åœ°æ§åˆ¶çº¿ç¨‹å—çš„è°ƒåº¦ï¼Œå¹¶å…è®¸ä¸€ä¸ªçº¿ç¨‹å—çš„å…±äº«å†…å­˜åœ¨åŒä¸€é›†ç¾¤ä¸­çš„å…¶ä»–çº¿ç¨‹å—è®¿é—®ã€‚
- en: Parallelising matrix multiplication
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å¹¶è¡ŒåŒ–çŸ©é˜µä¹˜æ³•
- en: 'Suppose we want to compute:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: å‡è®¾æˆ‘ä»¬æƒ³è®¡ç®—ï¼š
- en: '![](../Images/4428aab079a1a6c5d6c3f9e39a458d3b.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4428aab079a1a6c5d6c3f9e39a458d3b.png)'
- en: We say that the problem size is (*M*, *N*, *K*) in this case. To parallelise
    this operation, we can split *A* and *B* into smaller matrices, matrix multiply
    them individually, and concatenate the results to form *C*.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬è¯´åœ¨è¿™ç§æƒ…å†µä¸‹é—®é¢˜çš„è§„æ¨¡æ˜¯(*M*, *N*, *K*)ã€‚ä¸ºäº†å¹¶è¡ŒåŒ–è¿™ä¸ªæ“ä½œï¼Œæˆ‘ä»¬å¯ä»¥å°†*A*å’Œ*B*æ‹†åˆ†æˆæ›´å°çš„çŸ©é˜µï¼Œåˆ†åˆ«è¿›è¡ŒçŸ©é˜µä¹˜æ³•ï¼Œç„¶åå°†ç»“æœè¿æ¥èµ·æ¥å½¢æˆ*C*ã€‚
- en: 'Specifically, we can partition *A* row-wise (i.e., *M* into chunks of size
    *Mâ€™*) and *B* column-wise (i.e., *N* into chunks of size *Nâ€™*) to give:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å¯ä»¥æŒ‰è¡Œåˆ†å‰²*A*ï¼ˆå³ï¼Œå°†*M*åˆ†æˆå¤§å°ä¸º*Mâ€™*çš„å—ï¼‰å’ŒæŒ‰åˆ—åˆ†å‰²*B*ï¼ˆå³ï¼Œå°†*N*åˆ†æˆå¤§å°ä¸º*Nâ€™*çš„å—ï¼‰ï¼Œå¾—åˆ°ï¼š
- en: '![](../Images/1aebc2d7255e497596700febd57338a9.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1aebc2d7255e497596700febd57338a9.png)'
- en: We can see that each sub-matrix in *C* are independent of each other, so we
    can easily parallelise the computation of each sub-matrix.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥çœ‹åˆ°*C*ä¸­çš„æ¯ä¸ªå­çŸ©é˜µå½¼æ­¤ç‹¬ç«‹ï¼Œå› æ­¤æˆ‘ä»¬å¯ä»¥è½»æ¾åœ°å¹¶è¡Œè®¡ç®—æ¯ä¸ªå­çŸ©é˜µã€‚
- en: 'In practice, *K* might be too large to directly load into memory and compute
    on. Instead, a typical implementation will also split *K* into chunks of size
    *Kâ€™*, iterate over each chunk, and accumulate (by summing) over the partial results.
    This is known as serial-*K* reduction. (As opposed to *parallel-K reduction*,
    see section below). Mathematically, this looks like:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: å®é™…ä¸Šï¼Œ*K*å¯èƒ½è¿‡å¤§ï¼Œæ— æ³•ç›´æ¥åŠ è½½åˆ°å†…å­˜ä¸­è¿›è¡Œè®¡ç®—ã€‚ç›¸åï¼Œå…¸å‹çš„å®ç°ä¹Ÿä¼šå°†*K*åˆ†å‰²æˆå¤§å°ä¸º*Kâ€™*çš„å—ï¼Œè¿­ä»£æ¯ä¸ªå—ï¼Œå¹¶å¯¹éƒ¨åˆ†ç»“æœè¿›è¡Œç´¯åŠ ï¼ˆæ±‚å’Œï¼‰ã€‚è¿™è¢«ç§°ä¸ºä¸²è¡Œ-*K*å½’çº¦ã€‚ï¼ˆä¸*parallel-K
    reduction*ç›¸å¯¹ï¼Œè§ä¸‹èŠ‚ï¼‰ã€‚ä»æ•°å­¦ä¸Šçœ‹ï¼Œè¿™æ ·è¡¨ç¤ºï¼š
- en: '![](../Images/2a1425f0cb0de36141cd782c6612f755.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2a1425f0cb0de36141cd782c6612f755.png)'
- en: '**Note: Padding**'
  id: totrans-33
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**æ³¨æ„ï¼šPadding**'
- en: ''
  id: totrans-34
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: At any point where the problem size is not divisible by the partition size,
    we need to add **padding**. This is typically done implicitly when we load the
    partitioned inputs (ğ´áµ¢,â‚– and ğµâ‚–,â±¼) into lower-level memory where we ensure the
    loaded partition (of size Mâ€™Ã—Kâ€™ for ğ´áµ¢,â‚– and Kâ€™Ã—Nâ€™ for ğµâ‚–,â±¼) is always â€œfullâ€
    by adding zeros. Special care needs to be taken when writing the results back
    to global memory to avoid out-of-bounds errors.
  id: totrans-35
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åœ¨ä»»ä½•é—®é¢˜å¤§å°ä¸èƒ½è¢«åˆ†åŒºå¤§å°æ•´é™¤çš„æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬éœ€è¦æ·»åŠ **padding**ã€‚è¿™é€šå¸¸åœ¨æˆ‘ä»¬å°†åˆ†åŒºè¾“å…¥ï¼ˆğ´áµ¢,â‚– å’Œ ğµâ‚–,â±¼ï¼‰åŠ è½½åˆ°ä½çº§å†…å­˜æ—¶éšå¼å®Œæˆï¼Œæˆ‘ä»¬é€šè¿‡æ·»åŠ é›¶ç¡®ä¿åŠ è½½çš„åˆ†åŒºï¼ˆğ´áµ¢,â‚–çš„å¤§å°ä¸ºMâ€™Ã—Kâ€™ï¼Œğµâ‚–,â±¼çš„å¤§å°ä¸ºKâ€™Ã—Nâ€™ï¼‰æ€»æ˜¯â€œæ»¡â€çš„ã€‚åœ¨å°†ç»“æœå†™å›å…¨å±€å†…å­˜æ—¶éœ€è¦ç‰¹åˆ«å°å¿ƒï¼Œä»¥é¿å…è¶Šç•Œé”™è¯¯ã€‚
- en: 'On a high level, **three nested partitions** happen to parallelise matrix multiplication
    on the GPU:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: ä»é«˜å±‚æ¬¡çœ‹ï¼Œ**ä¸‰å±‚åµŒå¥—åˆ†åŒº**ç”¨äºåœ¨GPUä¸Šå¹¶è¡ŒåŒ–çŸ©é˜µä¹˜æ³•ï¼š
- en: 1\. The first partition happens on the **threadblock** level. Each threadblock
    is responsible for computing *Cáµ¢,â±¼ = Aáµ¢ Bâ±¼.*
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 1. ç¬¬ä¸€æ¬¡åˆ†åŒºå‘ç”Ÿåœ¨**threadblock**çº§åˆ«ã€‚æ¯ä¸ªçº¿ç¨‹å—è´Ÿè´£è®¡ç®—*Cáµ¢,â±¼ = Aáµ¢ Bâ±¼*ã€‚
- en: 2\. The second partition happens on the **warp** level. The threadblock-level
    problem *Cáµ¢,â±¼* is further partitioned such that each warp is responsible for computing
    *Cáµ¢,â±¼â½áµâ¿â¾ = Aáµ¢â½áµâ¾ Bâ±¼â½â¿â¾*.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 2. ç¬¬äºŒæ¬¡åˆ†åŒºå‘ç”Ÿåœ¨**warp**çº§åˆ«ã€‚çº¿ç¨‹å—çº§åˆ«çš„é—®é¢˜*Cáµ¢,â±¼* è¿›ä¸€æ­¥è¢«åˆ†åŒºï¼Œæ¯ä¸ªwarpè´Ÿè´£è®¡ç®—*Cáµ¢,â±¼â½áµâ¿â¾ = Aáµ¢â½áµâ¾ Bâ±¼â½â¿â¾*ã€‚
- en: '3\. The third partition happens on the **instruction** level. Some instructions
    expect inputs of particular sizes. For example, second-generation Tensor Cores
    operate on problems of size (16, 8, 8) for *fp16*, whereas a direct implementation
    on CUDA cores by scalar multiplication would simply operate on size (1, 1, 1).
    The warp-level problem is thus even further partitioned such that each chunk has
    a suitable size for the instruction: *Cáµ¢,â±¼â½áµâ¿â¾â½áµƒáµ‡â¾ = Aáµ¢â½áµâ¾â½áµƒâ¾ Bâ±¼â½â¿â¾â½áµ‡â¾.*'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 3. ç¬¬ä¸‰æ¬¡åˆ†åŒºå‘ç”Ÿåœ¨**instruction**çº§åˆ«ã€‚æœ‰äº›æŒ‡ä»¤éœ€è¦ç‰¹å®šå¤§å°çš„è¾“å…¥ã€‚ä¾‹å¦‚ï¼Œç¬¬äºŒä»£Tensor Coresæ“ä½œå¤§å°ä¸º(16, 8, 8)çš„*fp16*é—®é¢˜ï¼Œè€Œåœ¨CUDAæ ¸å¿ƒä¸Šé€šè¿‡æ ‡é‡ä¹˜æ³•ç›´æ¥å®ç°åˆ™ä»…æ“ä½œå¤§å°ä¸º(1,
    1, 1)çš„é—®é¢˜ã€‚å› æ­¤ï¼Œwarpçº§åˆ«çš„é—®é¢˜è¢«è¿›ä¸€æ­¥åˆ†åŒºï¼Œä½¿å¾—æ¯ä¸ªå—æœ‰é€‚åˆæŒ‡ä»¤çš„å¤§å°ï¼š*Cáµ¢,â±¼â½áµâ¿â¾â½áµƒáµ‡â¾ = Aáµ¢â½áµâ¾â½áµƒâ¾ Bâ±¼â½â¿â¾â½áµ‡â¾*ã€‚
- en: Thereâ€™s a good reason why we need three partition levels, as we will see in
    the next section.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬éœ€è¦ä¸‰ä¸ªåˆ†åŒºçº§åˆ«æ˜¯æœ‰å……åˆ†ç†ç”±çš„ï¼Œæ­£å¦‚æˆ‘ä»¬åœ¨ä¸‹ä¸€èŠ‚ä¸­å°†çœ‹åˆ°çš„ã€‚
- en: Data redundancy
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ•°æ®å†—ä½™
- en: Matrix multiplication can easily become memory-bound if we naively re-fetch
    data from global memory to registers every time we perform a computation. The
    key observation is that many of the sub-inputs *Aáµ¢* and *Bâ±¼* are reused across
    different sub-matrix multiplications. For example, *Aáµ¢* is required for *Cáµ¢,â‚
    , Cáµ¢,â‚‚* , ... and *Bâ±¼* is required for *C*â‚*,â±¼* , *C*â‚‚*,â±¼* , â€¦ . We can get the
    best throughput if we can minimise redundant data movement and reuse the loaded
    data as much as possible.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: çŸ©é˜µä¹˜æ³•å¦‚æœæˆ‘ä»¬æ¯æ¬¡è®¡ç®—æ—¶éƒ½ä»å…¨å±€å†…å­˜é‡æ–°è·å–æ•°æ®ï¼Œå¾ˆå®¹æ˜“å˜æˆå†…å­˜ç“¶é¢ˆã€‚å…³é”®è§‚å¯Ÿæ˜¯ï¼Œè®¸å¤šå­è¾“å…¥*Aáµ¢*å’Œ*Bâ±¼*åœ¨ä¸åŒçš„å­çŸ©é˜µä¹˜æ³•ä¸­è¢«é‡å¤ä½¿ç”¨ã€‚ä¾‹å¦‚ï¼Œ*Aáµ¢*éœ€è¦ç”¨äº*Cáµ¢,â‚
    , Cáµ¢,â‚‚* , ... å’Œ*Bâ±¼*éœ€è¦ç”¨äº*C*â‚*,â±¼* , *C*â‚‚*,â±¼* , â€¦ ã€‚å¦‚æœæˆ‘ä»¬èƒ½æœ€å°åŒ–å†—ä½™æ•°æ®ç§»åŠ¨å¹¶å°½å¯èƒ½å¤šåœ°é‡ç”¨åŠ è½½çš„æ•°æ®ï¼Œå°±èƒ½è·å¾—æœ€ä½³çš„ååé‡ã€‚
- en: 'In CUDA, there are three types of user-accessible memory:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨CUDAä¸­ï¼Œæœ‰ä¸‰ç§ç”¨æˆ·å¯è®¿é—®çš„å†…å­˜ç±»å‹ï¼š
- en: '![](../Images/765cbf3a96054ed41752bf1224c377c3.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/765cbf3a96054ed41752bf1224c377c3.png)'
- en: 'Hereâ€™s a high-level view of how each memory type is utilised:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸‹é¢æ˜¯æ¯ç§å†…å­˜ç±»å‹å¦‚ä½•ä½¿ç”¨çš„é«˜çº§è§†å›¾ï¼š
- en: Each threadblock will first load its required inputs **from global memory into
    shared memory**. Subsequent accesses to those data would thus be served by the
    shared memory instead of by the slower global memory.
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æ¯ä¸ªçº¿ç¨‹å—å°†é¦–å…ˆ**ä»å…¨å±€å†…å­˜åŠ è½½å…¶æ‰€éœ€è¾“å…¥åˆ°å…±äº«å†…å­˜ä¸­**ã€‚ä¹‹åå¯¹è¿™äº›æ•°æ®çš„è®¿é—®å°†ç”±å…±äº«å†…å­˜æä¾›ï¼Œè€Œä¸æ˜¯è¾ƒæ…¢çš„å…¨å±€å†…å­˜ã€‚
- en: Within each threadblock, each warp will first load its required inputs **from
    shared memory into registers**. Subsequent accesses to those data would be served
    by the fast registers directly.
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: åœ¨æ¯ä¸ªçº¿ç¨‹å—ä¸­ï¼Œæ¯ä¸ªwarpå°†é¦–å…ˆ**ä»å…±äº«å†…å­˜åŠ è½½å…¶æ‰€éœ€è¾“å…¥åˆ°å¯„å­˜å™¨ä¸­**ã€‚éšåå¯¹è¿™äº›æ•°æ®çš„è®¿é—®å°†ç›´æ¥ç”±å¿«é€Ÿå¯„å­˜å™¨æä¾›ã€‚
- en: Diving into the details
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ·±å…¥ç»†èŠ‚
- en: Threadblock level
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: çº¿ç¨‹å—çº§åˆ«
- en: 'On the threadblock level, the problem is partitioned into sub-problems of size
    (*Mâ€™*, *Nâ€™*, *Kâ€™*). Thus, each threadblock is responsible for computing a fragment
    of *C*, denoted as:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨çº¿ç¨‹å—çº§åˆ«ï¼Œé—®é¢˜è¢«åˆ’åˆ†ä¸ºå¤§å°ä¸º (*Mâ€™*, *Nâ€™*, *Kâ€™*) çš„å­é—®é¢˜ã€‚å› æ­¤ï¼Œæ¯ä¸ªçº¿ç¨‹å—è´Ÿè´£è®¡ç®— *C* çš„ä¸€ä¸ªç‰‡æ®µï¼Œè®°ä½œï¼š
- en: '![](../Images/a08a624cc545b2b965a70ec6b47cd807.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a08a624cc545b2b965a70ec6b47cd807.png)'
- en: Redundant data movement is minimised by loading the sub-inputs *Aáµ¢,â‚–* and *Bâ‚–,â±¼*
    into shared memory. When we are done with computing *Aáµ¢,â‚– Bâ‚–,â±¼*, the next chunk
    (*Aáµ¢,â‚–â‚Šâ‚* and *Bâ‚–â‚Šâ‚,â±¼*) will be loaded.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: é€šè¿‡å°†å­è¾“å…¥ *Aáµ¢,â‚–* å’Œ *Bâ‚–,â±¼* åŠ è½½åˆ°å…±äº«å†…å­˜ä¸­æ¥æœ€å°åŒ–å†—ä½™çš„æ•°æ®ç§»åŠ¨ã€‚å½“æˆ‘ä»¬å®Œæˆ *Aáµ¢,â‚– Bâ‚–,â±¼* çš„è®¡ç®—åï¼Œä¸‹ä¸€ä¸ªå— (*Aáµ¢,â‚–â‚Šâ‚*
    å’Œ *Bâ‚–â‚Šâ‚,â±¼*) å°†è¢«åŠ è½½ã€‚
- en: Warp level
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: warpçº§åˆ«
- en: 'On the warp level, the sub-problem is further partitioned into sub-sub-problems
    of size (*Mâ€™â€™, Nâ€™â€™, Kâ€™â€™*). Thus, each *warp* is responsible for computing a fragment
    of *Cáµ¢,â±¼,* denoted as *Cáµ¢,â±¼â½áµ â¿â¾*:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨warpçº§åˆ«ï¼Œå­é—®é¢˜è¿›ä¸€æ­¥åˆ’åˆ†ä¸ºå¤§å°ä¸º (*Mâ€™â€™, Nâ€™â€™, Kâ€™â€™*) çš„å­å­é—®é¢˜ã€‚å› æ­¤ï¼Œæ¯ä¸ª *warp* è´Ÿè´£è®¡ç®— *Cáµ¢,â±¼,* è®°ä½œ *Cáµ¢,â±¼â½áµ
    â¿â¾*ï¼š
- en: '![](../Images/f37fcc11c765303007fa4ce7af23df12.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f37fcc11c765303007fa4ce7af23df12.png)'
- en: Redundant data movement is minimised by loading the sub-sub-inputs *Aáµ¢,â‚–â½áµ Ë¡â¾
    and Bâ‚–,â±¼â½Ë¡ â¿â¾* into **registers**. Any accesses to *Aáµ¢,â‚–â½áµ Ë¡â¾* and *Bâ‚–,â±¼â½Ë¡ â¿â¾*
    *within* a warp will then be served by the fast registers.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: é€šè¿‡å°†å­å­è¾“å…¥ *Aáµ¢,â‚–â½áµ Ë¡â¾* å’Œ *Bâ‚–,â±¼â½Ë¡ â¿â¾* åŠ è½½åˆ°**å¯„å­˜å™¨**ä¸­æ¥æœ€å°åŒ–å†—ä½™çš„æ•°æ®ç§»åŠ¨ã€‚ä»»ä½•å¯¹ *Aáµ¢,â‚–â½áµ Ë¡â¾* å’Œ *Bâ‚–,â±¼â½Ë¡
    â¿â¾* çš„è®¿é—®*åœ¨*warpå†…å°†ç”±å¿«é€Ÿå¯„å­˜å™¨æä¾›æœåŠ¡ã€‚
- en: '**Note: Distributing data across registers**'
  id: totrans-57
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**æ³¨æ„ï¼šåœ¨å¯„å­˜å™¨ä¹‹é—´åˆ†é…æ•°æ®**'
- en: ''
  id: totrans-58
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: It is worth noting that registers are **thread-level only**. This means that
    inputs in a register cannot be accessed by other threads in a warp. The exact
    way of how Aáµ¢,â‚–â½áµ Ë¡â¾and Bâ‚–,â±¼â½Ë¡ â¿â¾ are partitioned into the registers of each thread
    depends on the specific instruction used. The NVIDIA docs on [Warp Level Matrix
    Multiply-Accumulate Instructions](https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#warp-level-matrix-instructions)
    gives a detailed description for each instruction.
  id: totrans-59
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œå¯„å­˜å™¨æ˜¯**çº¿ç¨‹çº§çš„**ã€‚è¿™æ„å‘³ç€å¯„å­˜å™¨ä¸­çš„è¾“å…¥ä¸èƒ½è¢«warpä¸­çš„å…¶ä»–çº¿ç¨‹è®¿é—®ã€‚å¦‚ä½•å°† Aáµ¢,â‚–â½áµ Ë¡â¾ å’Œ Bâ‚–,â±¼â½Ë¡ â¿â¾ åˆ†é…åˆ°æ¯ä¸ªçº¿ç¨‹çš„å¯„å­˜å™¨ä¸­ï¼Œå–å†³äºä½¿ç”¨çš„å…·ä½“æŒ‡ä»¤ã€‚NVIDIA
    æ–‡æ¡£ä¸­çš„ [Warp Level Matrix Multiply-Accumulate Instructions](https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#warp-level-matrix-instructions)
    å¯¹æ¯æ¡æŒ‡ä»¤è¿›è¡Œäº†è¯¦ç»†æè¿°ã€‚
- en: Tensor core level
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å¼ é‡æ ¸å¿ƒçº§åˆ«
- en: 'To actually perform the matrix multiplication, we use the **Tensor Cores**
    on the GPU. My GPU (RTX 2060) has the second generation Tensor Cores, which are
    specialised to solve fp16 problems of size (*Mâ€™â€™â€™, Nâ€™â€™â€™, Kâ€™â€™â€™*) = (16, 8, 8).
    Thus, we even further partition *Cáµ¢,â±¼â½áµ â¿â¾* to fit the size expected by the instruction:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†å®é™…æ‰§è¡ŒçŸ©é˜µä¹˜æ³•ï¼Œæˆ‘ä»¬ä½¿ç”¨ GPU ä¸Šçš„**å¼ é‡æ ¸å¿ƒ**ã€‚æˆ‘çš„ GPU (RTX 2060) å…·æœ‰ç¬¬äºŒä»£å¼ é‡æ ¸å¿ƒï¼Œä¸“é—¨è§£å†³å¤§å°ä¸º (*Mâ€™â€™â€™, Nâ€™â€™â€™,
    Kâ€™â€™â€™*) = (16, 8, 8) çš„ fp16 é—®é¢˜ã€‚å› æ­¤ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥å°† *Cáµ¢,â±¼â½áµ â¿â¾* åˆ’åˆ†ä¸ºç¬¦åˆæŒ‡ä»¤é¢„æœŸå¤§å°çš„ç‰‡æ®µï¼š
- en: '![](../Images/26652d7248aa29b590f21fc6cbf47814.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/26652d7248aa29b590f21fc6cbf47814.png)'
- en: Here, all the inputs are already in the registers and thus the data movement
    overhead is minimal.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™é‡Œï¼Œæ‰€æœ‰è¾“å…¥å·²ç»åœ¨å¯„å­˜å™¨ä¸­ï¼Œå› æ­¤æ•°æ®ç§»åŠ¨å¼€é”€æœ€å°ã€‚
- en: '**Note: Tensor Cores**'
  id: totrans-64
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**æ³¨æ„ï¼šå¼ é‡æ ¸å¿ƒ**'
- en: ''
  id: totrans-65
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Tensor Core operations are **warp-level instructions**, meaning that all the
    threads in a warp need to execute the Tensor Core instruction at the same time,
    collaboratively preparing the data to be consumed by **one** Tensor Core.
  id: totrans-66
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: å¼ é‡æ ¸å¿ƒæ“ä½œæ˜¯**warpçº§æŒ‡ä»¤**ï¼Œæ„å‘³ç€warpä¸­çš„æ‰€æœ‰çº¿ç¨‹éœ€è¦åŒæ—¶æ‰§è¡Œå¼ é‡æ ¸å¿ƒæŒ‡ä»¤ï¼ŒååŒå‡†å¤‡è¦è¢«**ä¸€ä¸ª**å¼ é‡æ ¸å¿ƒæ¶ˆè´¹çš„æ•°æ®ã€‚
- en: Choosing the partition sizes
  id: totrans-67
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: é€‰æ‹©åˆ†åŒºå¤§å°
- en: So, given that we want to minimise data movement, we should just choose a partition
    size as large as possible to utilise all of the shared memory and registers, *right?*
    Well, not quite.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥ï¼Œé‰´äºæˆ‘ä»¬å¸Œæœ›æœ€å°åŒ–æ•°æ®ç§»åŠ¨ï¼Œæˆ‘ä»¬åº”è¯¥é€‰æ‹©å°½å¯èƒ½å¤§çš„åˆ†åŒºå¤§å°æ¥åˆ©ç”¨æ‰€æœ‰çš„å…±äº«å†…å­˜å’Œå¯„å­˜å™¨ï¼Œ*å¯¹å—ï¼Ÿ* å…¶å®å¹¶ä¸æ˜¯è¿™æ ·ã€‚
- en: Threadblock partition size
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: çº¿ç¨‹å—åˆ†åŒºå¤§å°
- en: 'Asymptotically, as the problem size increases, yes, we do want to use as much
    shared memory and registers as possible. However, for small problem sizes, we
    might run into two problems:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: ä»æ¸è¿‘çš„è§’åº¦æ¥çœ‹ï¼Œéšç€é—®é¢˜å¤§å°çš„å¢åŠ ï¼Œæ˜¯çš„ï¼Œæˆ‘ä»¬ç¡®å®å¸Œæœ›å°½å¯èƒ½ä½¿ç”¨æ›´å¤šçš„å…±äº«å†…å­˜å’Œå¯„å­˜å™¨ã€‚ç„¶è€Œï¼Œå¯¹äºå°é—®é¢˜å¤§å°ï¼Œæˆ‘ä»¬å¯èƒ½ä¼šé‡åˆ°ä¸¤ä¸ªé—®é¢˜ï¼š
- en: Have a large partition size means that we will have fewer threadblocks. Since
    each threadblock can only execute on one SM, this might mean that we cannot utilise
    all the SMs.
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å¤§çš„åˆ†åŒºå¤§å°æ„å‘³ç€æˆ‘ä»¬å°†æœ‰æ›´å°‘çš„çº¿ç¨‹å—ã€‚ç”±äºæ¯ä¸ªçº¿ç¨‹å—åªèƒ½åœ¨ä¸€ä¸ªSMä¸Šæ‰§è¡Œï¼Œè¿™å¯èƒ½æ„å‘³ç€æˆ‘ä»¬ä¸èƒ½åˆ©ç”¨æ‰€æœ‰çš„SMã€‚
- en: For problem sizes that are not divisible by the partition size, we will have
    to add more padding to the inputs. This lowers the efficiency as less computation
    will be done on meaningful inputs.
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å¯¹äºä¸èƒ½è¢«åˆ†åŒºå¤§å°æ•´é™¤çš„é—®é¢˜å¤§å°ï¼Œæˆ‘ä»¬éœ€è¦ä¸ºè¾“å…¥æ·»åŠ æ›´å¤šçš„å¡«å……ã€‚è¿™ä¼šé™ä½æ•ˆç‡ï¼Œå› ä¸ºå¯¹æœ‰æ„ä¹‰çš„è¾“å…¥è®¡ç®—è¾ƒå°‘ã€‚
- en: A typical implementation might use a partition size of (*Mâ€™, Nâ€™, Kâ€™*) = (128,
    256, 32).
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
- en: Warp partition size
  id: totrans-74
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In general, having a large warp partition size means there will be less redundant
    data movement, but at the cost of having fewer warps. Having too few warps means
    that we will not be able to hide the latency of memory accesses (because we might
    run out of other warps to schedule while the current warp is waiting for data).
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: A typical implementation might use a partition size of (*Mâ€™â€™, Nâ€™â€™, Kâ€™â€™*) = (64,
    64, 32).
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: Instruction partition size
  id: totrans-77
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This is completely determined by what instructions your GPU supports. For my
    RTX 2060, the ptx instruction for fp16 Tensor Core matrix multiplication (with
    fp16 accumulation) is `mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16`, which
    expects inputs of size (16, 8, 8).
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: Even more optimisations
  id: totrans-79
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The above techniques can get us close to the theoretical peak performance of
    the GPU when the problem size is large. However, for smaller problem sizes, they
    are not as efficient. There are two common techniques to further improve the performance
    of matrix multiplication: **parallel-K reduction** and **software pipelining**.'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
- en: Parallel-K reduction
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In cases where *M* and *N* are small, we might only have a few threadblocks.
    For example, if (*Mâ€™, Nâ€™*) = (128, 256) and the original problem size has *M*
    â‰¤ 128 and *N* â‰¤ 256, we will only have one threadblock, and so we are only utilising
    a fraction of the GPUâ€™s compute power! (For example, my RTX 2060 has 30 SMs, so
    to maximise utilisation we want at least 30 threadblocks.)
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: 'In cases where *K* is large (even though *M* and *N* are small), we can leverage
    more parallelism by doing **parallel-*K* reduction**. Recall that in serial-*K*
    reduction, each threadblock iterates over the following sum:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c3f0c3cbaa4b6a6680fab8fd428b2f57.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
- en: and accumulates the intermediate results into *Cáµ¢,â±¼.* In parallel-*K* reduction,
    we instead assign each threadblock to only compute *one element of the sum* (i.e.
    *Aáµ¢,â‚– Bâ‚–,â±¼*). This allows us to increase the number of threadblocks by a factor
    of *K/Kâ€™*, thus utilising more SMs.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: The caveat is that now, we need to *allocate more memory* to store the results
    from each threadblock, and *invoke a second kernel* to perform a final reduction
    over the partial results to get *Cáµ¢,â±¼*.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
- en: Software pipelining
  id: totrans-87
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Normally, CUDA hides the latency of memory accesses by scheduling other warps
    to execute while a warp is waiting for data. This requires us to have enough warps
    to mask the latency.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: However, the number of warps is typically relatively small when doing GEMM.
    This is because the number of warps is limited by â€œThe number of available registers
    per threadblock divided by the number of registers required per warpâ€. For matrix
    multiplication, we use a lot of registers per warp to maximise data reuse. As
    a result, we might not have enough warps to mask the latency.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: â€œThe accumulator elements typically occupy at least half a threadâ€™s total register
    budgetâ€. â€” CUTLASS Docs
  id: totrans-90
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: â€œç´¯åŠ å™¨å…ƒç´ é€šå¸¸å ç”¨è‡³å°‘ä¸€åŠçš„çº¿ç¨‹æ€»å¯„å­˜å™¨é¢„ç®—ã€‚â€ â€” CUTLASSæ–‡æ¡£
- en: 'To mitigate this effect, we can use **software pipelining**. In essence, we
    can (manually) pre-load the inputs for the next iteration of the loop asynchronously
    using special instructions. While the inputs are being loaded, we can continue
    to compute on the current iteration. It is summarised by the following diagram:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†ç¼“è§£è¿™ä¸€æ•ˆæœï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨**è½¯ä»¶æµæ°´çº¿**ã€‚æœ¬è´¨ä¸Šï¼Œæˆ‘ä»¬å¯ä»¥ï¼ˆæ‰‹åŠ¨ï¼‰ä½¿ç”¨ç‰¹æ®ŠæŒ‡ä»¤å¼‚æ­¥é¢„åŠ è½½ä¸‹ä¸€ä¸ªè¿­ä»£çš„è¾“å…¥ã€‚åœ¨è¾“å…¥è¢«åŠ è½½çš„åŒæ—¶ï¼Œæˆ‘ä»¬å¯ä»¥ç»§ç»­åœ¨å½“å‰è¿­ä»£ä¸Šè¿›è¡Œè®¡ç®—ã€‚å…¶æ€»ç»“å¦‚ä¸‹å›¾æ‰€ç¤ºï¼š
- en: '![](../Images/d065965816235f5f63c80e4664e724f2.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d065965816235f5f63c80e4664e724f2.png)'
- en: Software Pipelining from [CUTLASS](https://github.com/NVIDIA/cutlass/blob/main/media/docs/efficient_gemm.md)
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥è‡ª[CUTLASS](https://github.com/NVIDIA/cutlass/blob/main/media/docs/efficient_gemm.md)çš„è½¯ä»¶ä¸‹è½½æµæ°´çº¿
- en: 'This is made possible by the fact that the GPU is like any modern CPU: it can
    pipeline memory accesses and arithmetic operations as long as there is no data
    dependency between them. This is known as **instruction-level parallelism**.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å¾—ç›ŠäºGPUçš„ç‰¹æ€§ï¼šå®ƒåƒä»»ä½•ç°ä»£CPUä¸€æ ·ï¼Œå¯ä»¥åœ¨æ²¡æœ‰æ•°æ®ä¾èµ–å…³ç³»çš„æƒ…å†µä¸‹æµæ°´åŒ–å†…å­˜è®¿é—®å’Œç®—æœ¯æ“ä½œã€‚è¿™è¢«ç§°ä¸º**æŒ‡ä»¤çº§å¹¶è¡Œ**ã€‚
- en: Matrix multiplication in action
  id: totrans-95
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: çŸ©é˜µä¹˜æ³•çš„å®é™…åº”ç”¨
- en: 'If you want to see how all these concepts come together in a real implementation,
    check out my implementation of training MNIST from scratch with CUDA. There, I
    trained a multi-layer perceptron on MNIST using CUDA, achieving **6x speedup over
    optimised PyTorch** for hidden size of 128:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä½ æƒ³äº†è§£è¿™äº›æ¦‚å¿µå¦‚ä½•åœ¨å®é™…å®ç°ä¸­ç»“åˆèµ·æ¥ï¼Œå¯ä»¥æŸ¥çœ‹æˆ‘ç”¨CUDAä»é›¶å¼€å§‹è®­ç»ƒMNISTçš„å®ç°ã€‚åœ¨é‚£é‡Œï¼Œæˆ‘ä½¿ç”¨CUDAè®­ç»ƒäº†ä¸€ä¸ªå¤šå±‚æ„ŸçŸ¥å™¨ï¼Œå¹¶åœ¨éšè—å±‚å¤§å°ä¸º128æ—¶å®ç°äº†**æ¯”ä¼˜åŒ–åçš„PyTorchå¿«6å€**ï¼š
- en: '[](https://github.com/andylolu2/cuda-mnist?source=post_page-----e920e50207a8--------------------------------)
    [## GitHub - andylolu2/cuda-mnist'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://github.com/andylolu2/cuda-mnist?source=post_page-----e920e50207a8--------------------------------)
    [## GitHub - andylolu2/cuda-mnist'
- en: Contribute to andylolu2/cuda-mnist development by creating an account on GitHub.
  id: totrans-98
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: é€šè¿‡åœ¨GitHubä¸Šåˆ›å»ºè´¦æˆ·æ¥å‚ä¸andylolu2/cuda-mnistçš„å¼€å‘ã€‚
- en: github.com](https://github.com/andylolu2/cuda-mnist?source=post_page-----e920e50207a8--------------------------------)
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '[github.com](https://github.com/andylolu2/cuda-mnist?source=post_page-----e920e50207a8--------------------------------)'
- en: References
  id: totrans-100
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å‚è€ƒèµ„æ–™
- en: 1\. [CUTLASS docs](https://github.com/NVIDIA/cutlass/blob/main/media/docs/)
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 1\. [CUTLASSæ–‡æ¡£](https://github.com/NVIDIA/cutlass/blob/main/media/docs/)
- en: 2\. [CUDA docs](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html)
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 2\. [CUDAæ–‡æ¡£](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html)
- en: 3\. [CUTLASS examples](https://github.com/NVIDIA/cutlass/tree/main/examples)
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 3\. [CUTLASSç¤ºä¾‹](https://github.com/NVIDIA/cutlass/tree/main/examples)
