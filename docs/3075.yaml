- en: Matrix Multiplication on GPU
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: GPU 上的矩阵乘法
- en: 原文：[https://towardsdatascience.com/matrix-multiplication-on-the-gpu-e920e50207a8?source=collection_archive---------1-----------------------#2023-10-09](https://towardsdatascience.com/matrix-multiplication-on-the-gpu-e920e50207a8?source=collection_archive---------1-----------------------#2023-10-09)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/matrix-multiplication-on-the-gpu-e920e50207a8?source=collection_archive---------1-----------------------#2023-10-09](https://towardsdatascience.com/matrix-multiplication-on-the-gpu-e920e50207a8?source=collection_archive---------1-----------------------#2023-10-09)
- en: How to achieve state-of-the-art matrix multiplication performance in CUDA.
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何在 CUDA 中实现最先进的矩阵乘法性能。
- en: '[](https://medium.com/@andylolu24?source=post_page-----e920e50207a8--------------------------------)[![Andy
    Lo](../Images/07ba4bfa6792696a82c251f5e9d5b9ac.png)](https://medium.com/@andylolu24?source=post_page-----e920e50207a8--------------------------------)[](https://towardsdatascience.com/?source=post_page-----e920e50207a8--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----e920e50207a8--------------------------------)
    [Andy Lo](https://medium.com/@andylolu24?source=post_page-----e920e50207a8--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@andylolu24?source=post_page-----e920e50207a8--------------------------------)[![Andy
    Lo](../Images/07ba4bfa6792696a82c251f5e9d5b9ac.png)](https://medium.com/@andylolu24?source=post_page-----e920e50207a8--------------------------------)[](https://towardsdatascience.com/?source=post_page-----e920e50207a8--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----e920e50207a8--------------------------------)
    [Andy Lo](https://medium.com/@andylolu24?source=post_page-----e920e50207a8--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F9b10f678560b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmatrix-multiplication-on-the-gpu-e920e50207a8&user=Andy+Lo&userId=9b10f678560b&source=post_page-9b10f678560b----e920e50207a8---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----e920e50207a8--------------------------------)
    ·10 min read·Oct 9, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fe920e50207a8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmatrix-multiplication-on-the-gpu-e920e50207a8&user=Andy+Lo&userId=9b10f678560b&source=-----e920e50207a8---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F9b10f678560b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmatrix-multiplication-on-the-gpu-e920e50207a8&user=Andy+Lo&userId=9b10f678560b&source=post_page-9b10f678560b----e920e50207a8---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----e920e50207a8--------------------------------)
    ·10 分钟阅读·2023 年 10 月 9 日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fe920e50207a8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmatrix-multiplication-on-the-gpu-e920e50207a8&user=Andy+Lo&userId=9b10f678560b&source=-----e920e50207a8---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fe920e50207a8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmatrix-multiplication-on-the-gpu-e920e50207a8&source=-----e920e50207a8---------------------bookmark_footer-----------)![](../Images/52104d461a859da63d47995f461249b9.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fe920e50207a8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmatrix-multiplication-on-the-gpu-e920e50207a8&source=-----e920e50207a8---------------------bookmark_footer-----------)![](../Images/52104d461a859da63d47995f461249b9.png)'
- en: “A minimalist art taking inspiration from Matrix Multiplication, in the style
    of vaporwave” by DALLE-2
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: “从矩阵乘法中汲取灵感的极简艺术，风格为 vaporwave” —— DALLE-2
- en: 'This blog came from a sudden realisation of how little I knew about how matrix
    multiplication works on the GPU. Having done so many ML projects, I feel like
    I ought to understand how the most important operation in ML works: What is this
    “Tensor Core” thing? Why does everyone say “*data movement is the bottleneck*”?
    How fast can GPUs actually go?'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 这篇博客源于我突然意识到自己对矩阵乘法在 GPU 上如何运作知之甚少。做了这么多机器学习项目，我觉得我应该了解这个在机器学习中最重要的操作是如何工作的：什么是“张量核心”？为什么每个人都说“*数据移动是瓶颈*”？GPU
    实际上能有多快？
- en: To answer these questions, I decided that I must go out of my PyTorch bubble
    and **venture into the abyss of CUDA**. I wrote this blog to document all that
    I have learnt, and hopefully anyone reading this wouldn’t have to go through the
    pain of digging through CUDA docs/code as I did.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 为了回答这些问题，我决定必须走出我的 PyTorch 领域，**深入探索 CUDA 的深渊**。我写了这篇博客来记录我所学到的一切，希望读到这篇文章的任何人都不必像我一样经历挖掘
    CUDA 文档/代码的痛苦。
- en: If there is anything that I’ve learnt in this journey, it is **concurrent matrix
    multiplication is HARD**. Efficient matrix multiplication heavily depends on the
    specific hardware you are using and the problem size you are trying to solve.
    There is no one-size-fits-all solution.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我在这段旅程中学到了什么，那就是**并发矩阵乘法是困难的**。高效的矩阵乘法在很大程度上依赖于你使用的具体硬件和你尝试解决的问题规模。没有一刀切的解决方案。
- en: Enough nagging, let’s dig in!
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 够了，让我们深入了解吧！
- en: Recap on GPU architecture
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 回顾GPU架构
- en: Let’s remind ourselves how (NVIDIA) GPUs work. A GPU achieves parallelism by
    running many **threads**. Each thread is executed on a single CUDA core, though
    at a given time, only a subset of the threads are active so there can be many
    more threads than CUDA cores available. Each thread, no matter it is active or
    not, has its own set of **registers**.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回顾一下（NVIDIA）GPU的工作原理。GPU通过运行许多**线程**来实现并行处理。每个线程在一个CUDA核心上执行，但在某一时刻，只有一部分线程是活动的，因此可能有比可用的CUDA核心更多的线程。每个线程，无论是否活动，都有自己的**寄存器**。
- en: A group of 32 threads is known as a **warp**. All threads in a warp must execute
    together (or be inactive together). In most cases, there are a lot more inactive
    warps than active warps, and the **warp scheduler** is responsible for choosing
    which warps to execute at a given time. This allows the GPU to hide the latency
    of memory accesses by scheduling other warps to execute while a warp is waiting
    for data.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 一组32个线程称为**warp**。warp中的所有线程必须一起执行（或一起处于非活动状态）。在大多数情况下，非活动warp的数量远多于活动warp，而**warp调度器**负责选择在特定时间执行哪些warp。这使得GPU能够通过调度其他warp在warp等待数据时执行，从而隐藏内存访问的延迟。
- en: A group of warps is known as a **threadblock**. All warps in a threadblock are
    executed in the same **Streaming Multiprocessor** (SM). Each threadblock has its
    own **shared memory** that can be accessed by all threads in the threadblock.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 一组warp称为**线程块**。所有线程块中的warp在同一个**流处理器**（SM）中执行。每个线程块有自己的**共享内存**，所有线程块中的线程都可以访问。
- en: '**Note: Newer architectures**'
  id: totrans-17
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**注意：较新的架构**'
- en: ''
  id: totrans-18
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: From Volta architecture onwards, each thread also has its own program counter
    and call stack etc. This means that each thread in a warp can execute different
    instructions at the same time.
  id: totrans-19
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 从Volta架构开始，每个线程也有自己的程序计数器和调用栈等。这意味着warp中的每个线程可以同时执行不同的指令。
- en: ''
  id: totrans-20
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The Volta architecture also introduced **Tensor Cores** that are specialised
    to solve matrix multiplications of specific sizes. Each active warp have access
    to one Tensor Core.
  id: totrans-21
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Volta架构还引入了**Tensor Cores**，这些核心专门用于解决特定大小的矩阵乘法。每个活动warp可以访问一个Tensor Core。
- en: ''
  id: totrans-22
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: In the newest Hopper architecture, there is a concept of **threadblock clusters**
    that represents a group of threadblocks. It gives the user more fine-grained control
    over the scheduling of threadblocks and allows the shared memory of one threadblock
    to be access by other threadblocks in the same cluster.
  id: totrans-23
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 在最新的Hopper架构中，引入了**线程块集群**的概念，它表示一组线程块。它使用户能够更细粒度地控制线程块的调度，并允许一个线程块的共享内存在同一集群中的其他线程块访问。
- en: Parallelising matrix multiplication
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 并行化矩阵乘法
- en: 'Suppose we want to compute:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们想计算：
- en: '![](../Images/4428aab079a1a6c5d6c3f9e39a458d3b.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4428aab079a1a6c5d6c3f9e39a458d3b.png)'
- en: We say that the problem size is (*M*, *N*, *K*) in this case. To parallelise
    this operation, we can split *A* and *B* into smaller matrices, matrix multiply
    them individually, and concatenate the results to form *C*.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 我们说在这种情况下问题的规模是(*M*, *N*, *K*)。为了并行化这个操作，我们可以将*A*和*B*拆分成更小的矩阵，分别进行矩阵乘法，然后将结果连接起来形成*C*。
- en: 'Specifically, we can partition *A* row-wise (i.e., *M* into chunks of size
    *M’*) and *B* column-wise (i.e., *N* into chunks of size *N’*) to give:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，我们可以按行分割*A*（即，将*M*分成大小为*M’*的块）和按列分割*B*（即，将*N*分成大小为*N’*的块），得到：
- en: '![](../Images/1aebc2d7255e497596700febd57338a9.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1aebc2d7255e497596700febd57338a9.png)'
- en: We can see that each sub-matrix in *C* are independent of each other, so we
    can easily parallelise the computation of each sub-matrix.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到*C*中的每个子矩阵彼此独立，因此我们可以轻松地并行计算每个子矩阵。
- en: 'In practice, *K* might be too large to directly load into memory and compute
    on. Instead, a typical implementation will also split *K* into chunks of size
    *K’*, iterate over each chunk, and accumulate (by summing) over the partial results.
    This is known as serial-*K* reduction. (As opposed to *parallel-K reduction*,
    see section below). Mathematically, this looks like:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，*K*可能过大，无法直接加载到内存中进行计算。相反，典型的实现也会将*K*分割成大小为*K’*的块，迭代每个块，并对部分结果进行累加（求和）。这被称为串行-*K*归约。（与*parallel-K
    reduction*相对，见下节）。从数学上看，这样表示：
- en: '![](../Images/2a1425f0cb0de36141cd782c6612f755.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2a1425f0cb0de36141cd782c6612f755.png)'
- en: '**Note: Padding**'
  id: totrans-33
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**注意：Padding**'
- en: ''
  id: totrans-34
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: At any point where the problem size is not divisible by the partition size,
    we need to add **padding**. This is typically done implicitly when we load the
    partitioned inputs (𝐴ᵢ,ₖ and 𝐵ₖ,ⱼ) into lower-level memory where we ensure the
    loaded partition (of size M’×K’ for 𝐴ᵢ,ₖ and K’×N’ for 𝐵ₖ,ⱼ) is always “full”
    by adding zeros. Special care needs to be taken when writing the results back
    to global memory to avoid out-of-bounds errors.
  id: totrans-35
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 在任何问题大小不能被分区大小整除的情况下，我们需要添加**padding**。这通常在我们将分区输入（𝐴ᵢ,ₖ 和 𝐵ₖ,ⱼ）加载到低级内存时隐式完成，我们通过添加零确保加载的分区（𝐴ᵢ,ₖ的大小为M’×K’，𝐵ₖ,ⱼ的大小为K’×N’）总是“满”的。在将结果写回全局内存时需要特别小心，以避免越界错误。
- en: 'On a high level, **three nested partitions** happen to parallelise matrix multiplication
    on the GPU:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 从高层次看，**三层嵌套分区**用于在GPU上并行化矩阵乘法：
- en: 1\. The first partition happens on the **threadblock** level. Each threadblock
    is responsible for computing *Cᵢ,ⱼ = Aᵢ Bⱼ.*
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 1. 第一次分区发生在**threadblock**级别。每个线程块负责计算*Cᵢ,ⱼ = Aᵢ Bⱼ*。
- en: 2\. The second partition happens on the **warp** level. The threadblock-level
    problem *Cᵢ,ⱼ* is further partitioned such that each warp is responsible for computing
    *Cᵢ,ⱼ⁽ᵐⁿ⁾ = Aᵢ⁽ᵐ⁾ Bⱼ⁽ⁿ⁾*.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 2. 第二次分区发生在**warp**级别。线程块级别的问题*Cᵢ,ⱼ* 进一步被分区，每个warp负责计算*Cᵢ,ⱼ⁽ᵐⁿ⁾ = Aᵢ⁽ᵐ⁾ Bⱼ⁽ⁿ⁾*。
- en: '3\. The third partition happens on the **instruction** level. Some instructions
    expect inputs of particular sizes. For example, second-generation Tensor Cores
    operate on problems of size (16, 8, 8) for *fp16*, whereas a direct implementation
    on CUDA cores by scalar multiplication would simply operate on size (1, 1, 1).
    The warp-level problem is thus even further partitioned such that each chunk has
    a suitable size for the instruction: *Cᵢ,ⱼ⁽ᵐⁿ⁾⁽ᵃᵇ⁾ = Aᵢ⁽ᵐ⁾⁽ᵃ⁾ Bⱼ⁽ⁿ⁾⁽ᵇ⁾.*'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 3. 第三次分区发生在**instruction**级别。有些指令需要特定大小的输入。例如，第二代Tensor Cores操作大小为(16, 8, 8)的*fp16*问题，而在CUDA核心上通过标量乘法直接实现则仅操作大小为(1,
    1, 1)的问题。因此，warp级别的问题被进一步分区，使得每个块有适合指令的大小：*Cᵢ,ⱼ⁽ᵐⁿ⁾⁽ᵃᵇ⁾ = Aᵢ⁽ᵐ⁾⁽ᵃ⁾ Bⱼ⁽ⁿ⁾⁽ᵇ⁾*。
- en: There’s a good reason why we need three partition levels, as we will see in
    the next section.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要三个分区级别是有充分理由的，正如我们在下一节中将看到的。
- en: Data redundancy
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据冗余
- en: Matrix multiplication can easily become memory-bound if we naively re-fetch
    data from global memory to registers every time we perform a computation. The
    key observation is that many of the sub-inputs *Aᵢ* and *Bⱼ* are reused across
    different sub-matrix multiplications. For example, *Aᵢ* is required for *Cᵢ,₁
    , Cᵢ,₂* , ... and *Bⱼ* is required for *C*₁*,ⱼ* , *C*₂*,ⱼ* , … . We can get the
    best throughput if we can minimise redundant data movement and reuse the loaded
    data as much as possible.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 矩阵乘法如果我们每次计算时都从全局内存重新获取数据，很容易变成内存瓶颈。关键观察是，许多子输入*Aᵢ*和*Bⱼ*在不同的子矩阵乘法中被重复使用。例如，*Aᵢ*需要用于*Cᵢ,₁
    , Cᵢ,₂* , ... 和*Bⱼ*需要用于*C*₁*,ⱼ* , *C*₂*,ⱼ* , … 。如果我们能最小化冗余数据移动并尽可能多地重用加载的数据，就能获得最佳的吞吐量。
- en: 'In CUDA, there are three types of user-accessible memory:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在CUDA中，有三种用户可访问的内存类型：
- en: '![](../Images/765cbf3a96054ed41752bf1224c377c3.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/765cbf3a96054ed41752bf1224c377c3.png)'
- en: 'Here’s a high-level view of how each memory type is utilised:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是每种内存类型如何使用的高级视图：
- en: Each threadblock will first load its required inputs **from global memory into
    shared memory**. Subsequent accesses to those data would thus be served by the
    shared memory instead of by the slower global memory.
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 每个线程块将首先**从全局内存加载其所需输入到共享内存中**。之后对这些数据的访问将由共享内存提供，而不是较慢的全局内存。
- en: Within each threadblock, each warp will first load its required inputs **from
    shared memory into registers**. Subsequent accesses to those data would be served
    by the fast registers directly.
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在每个线程块中，每个warp将首先**从共享内存加载其所需输入到寄存器中**。随后对这些数据的访问将直接由快速寄存器提供。
- en: Diving into the details
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深入细节
- en: Threadblock level
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 线程块级别
- en: 'On the threadblock level, the problem is partitioned into sub-problems of size
    (*M’*, *N’*, *K’*). Thus, each threadblock is responsible for computing a fragment
    of *C*, denoted as:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在线程块级别，问题被划分为大小为 (*M’*, *N’*, *K’*) 的子问题。因此，每个线程块负责计算 *C* 的一个片段，记作：
- en: '![](../Images/a08a624cc545b2b965a70ec6b47cd807.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a08a624cc545b2b965a70ec6b47cd807.png)'
- en: Redundant data movement is minimised by loading the sub-inputs *Aᵢ,ₖ* and *Bₖ,ⱼ*
    into shared memory. When we are done with computing *Aᵢ,ₖ Bₖ,ⱼ*, the next chunk
    (*Aᵢ,ₖ₊₁* and *Bₖ₊₁,ⱼ*) will be loaded.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将子输入 *Aᵢ,ₖ* 和 *Bₖ,ⱼ* 加载到共享内存中来最小化冗余的数据移动。当我们完成 *Aᵢ,ₖ Bₖ,ⱼ* 的计算后，下一个块 (*Aᵢ,ₖ₊₁*
    和 *Bₖ₊₁,ⱼ*) 将被加载。
- en: Warp level
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: warp级别
- en: 'On the warp level, the sub-problem is further partitioned into sub-sub-problems
    of size (*M’’, N’’, K’’*). Thus, each *warp* is responsible for computing a fragment
    of *Cᵢ,ⱼ,* denoted as *Cᵢ,ⱼ⁽ᵐ ⁿ⁾*:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在warp级别，子问题进一步划分为大小为 (*M’’, N’’, K’’*) 的子子问题。因此，每个 *warp* 负责计算 *Cᵢ,ⱼ,* 记作 *Cᵢ,ⱼ⁽ᵐ
    ⁿ⁾*：
- en: '![](../Images/f37fcc11c765303007fa4ce7af23df12.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f37fcc11c765303007fa4ce7af23df12.png)'
- en: Redundant data movement is minimised by loading the sub-sub-inputs *Aᵢ,ₖ⁽ᵐ ˡ⁾
    and Bₖ,ⱼ⁽ˡ ⁿ⁾* into **registers**. Any accesses to *Aᵢ,ₖ⁽ᵐ ˡ⁾* and *Bₖ,ⱼ⁽ˡ ⁿ⁾*
    *within* a warp will then be served by the fast registers.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将子子输入 *Aᵢ,ₖ⁽ᵐ ˡ⁾* 和 *Bₖ,ⱼ⁽ˡ ⁿ⁾* 加载到**寄存器**中来最小化冗余的数据移动。任何对 *Aᵢ,ₖ⁽ᵐ ˡ⁾* 和 *Bₖ,ⱼ⁽ˡ
    ⁿ⁾* 的访问*在*warp内将由快速寄存器提供服务。
- en: '**Note: Distributing data across registers**'
  id: totrans-57
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**注意：在寄存器之间分配数据**'
- en: ''
  id: totrans-58
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: It is worth noting that registers are **thread-level only**. This means that
    inputs in a register cannot be accessed by other threads in a warp. The exact
    way of how Aᵢ,ₖ⁽ᵐ ˡ⁾and Bₖ,ⱼ⁽ˡ ⁿ⁾ are partitioned into the registers of each thread
    depends on the specific instruction used. The NVIDIA docs on [Warp Level Matrix
    Multiply-Accumulate Instructions](https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#warp-level-matrix-instructions)
    gives a detailed description for each instruction.
  id: totrans-59
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 值得注意的是，寄存器是**线程级的**。这意味着寄存器中的输入不能被warp中的其他线程访问。如何将 Aᵢ,ₖ⁽ᵐ ˡ⁾ 和 Bₖ,ⱼ⁽ˡ ⁿ⁾ 分配到每个线程的寄存器中，取决于使用的具体指令。NVIDIA
    文档中的 [Warp Level Matrix Multiply-Accumulate Instructions](https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#warp-level-matrix-instructions)
    对每条指令进行了详细描述。
- en: Tensor core level
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 张量核心级别
- en: 'To actually perform the matrix multiplication, we use the **Tensor Cores**
    on the GPU. My GPU (RTX 2060) has the second generation Tensor Cores, which are
    specialised to solve fp16 problems of size (*M’’’, N’’’, K’’’*) = (16, 8, 8).
    Thus, we even further partition *Cᵢ,ⱼ⁽ᵐ ⁿ⁾* to fit the size expected by the instruction:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实际执行矩阵乘法，我们使用 GPU 上的**张量核心**。我的 GPU (RTX 2060) 具有第二代张量核心，专门解决大小为 (*M’’’, N’’’,
    K’’’*) = (16, 8, 8) 的 fp16 问题。因此，我们进一步将 *Cᵢ,ⱼ⁽ᵐ ⁿ⁾* 划分为符合指令预期大小的片段：
- en: '![](../Images/26652d7248aa29b590f21fc6cbf47814.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/26652d7248aa29b590f21fc6cbf47814.png)'
- en: Here, all the inputs are already in the registers and thus the data movement
    overhead is minimal.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，所有输入已经在寄存器中，因此数据移动开销最小。
- en: '**Note: Tensor Cores**'
  id: totrans-64
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**注意：张量核心**'
- en: ''
  id: totrans-65
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Tensor Core operations are **warp-level instructions**, meaning that all the
    threads in a warp need to execute the Tensor Core instruction at the same time,
    collaboratively preparing the data to be consumed by **one** Tensor Core.
  id: totrans-66
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 张量核心操作是**warp级指令**，意味着warp中的所有线程需要同时执行张量核心指令，协同准备要被**一个**张量核心消费的数据。
- en: Choosing the partition sizes
  id: totrans-67
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 选择分区大小
- en: So, given that we want to minimise data movement, we should just choose a partition
    size as large as possible to utilise all of the shared memory and registers, *right?*
    Well, not quite.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，鉴于我们希望最小化数据移动，我们应该选择尽可能大的分区大小来利用所有的共享内存和寄存器，*对吗？* 其实并不是这样。
- en: Threadblock partition size
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 线程块分区大小
- en: 'Asymptotically, as the problem size increases, yes, we do want to use as much
    shared memory and registers as possible. However, for small problem sizes, we
    might run into two problems:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 从渐近的角度来看，随着问题大小的增加，是的，我们确实希望尽可能使用更多的共享内存和寄存器。然而，对于小问题大小，我们可能会遇到两个问题：
- en: Have a large partition size means that we will have fewer threadblocks. Since
    each threadblock can only execute on one SM, this might mean that we cannot utilise
    all the SMs.
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 大的分区大小意味着我们将有更少的线程块。由于每个线程块只能在一个SM上执行，这可能意味着我们不能利用所有的SM。
- en: For problem sizes that are not divisible by the partition size, we will have
    to add more padding to the inputs. This lowers the efficiency as less computation
    will be done on meaningful inputs.
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于不能被分区大小整除的问题大小，我们需要为输入添加更多的填充。这会降低效率，因为对有意义的输入计算较少。
- en: A typical implementation might use a partition size of (*M’, N’, K’*) = (128,
    256, 32).
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
- en: Warp partition size
  id: totrans-74
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In general, having a large warp partition size means there will be less redundant
    data movement, but at the cost of having fewer warps. Having too few warps means
    that we will not be able to hide the latency of memory accesses (because we might
    run out of other warps to schedule while the current warp is waiting for data).
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: A typical implementation might use a partition size of (*M’’, N’’, K’’*) = (64,
    64, 32).
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: Instruction partition size
  id: totrans-77
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This is completely determined by what instructions your GPU supports. For my
    RTX 2060, the ptx instruction for fp16 Tensor Core matrix multiplication (with
    fp16 accumulation) is `mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16`, which
    expects inputs of size (16, 8, 8).
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: Even more optimisations
  id: totrans-79
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The above techniques can get us close to the theoretical peak performance of
    the GPU when the problem size is large. However, for smaller problem sizes, they
    are not as efficient. There are two common techniques to further improve the performance
    of matrix multiplication: **parallel-K reduction** and **software pipelining**.'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
- en: Parallel-K reduction
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In cases where *M* and *N* are small, we might only have a few threadblocks.
    For example, if (*M’, N’*) = (128, 256) and the original problem size has *M*
    ≤ 128 and *N* ≤ 256, we will only have one threadblock, and so we are only utilising
    a fraction of the GPU’s compute power! (For example, my RTX 2060 has 30 SMs, so
    to maximise utilisation we want at least 30 threadblocks.)
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: 'In cases where *K* is large (even though *M* and *N* are small), we can leverage
    more parallelism by doing **parallel-*K* reduction**. Recall that in serial-*K*
    reduction, each threadblock iterates over the following sum:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c3f0c3cbaa4b6a6680fab8fd428b2f57.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
- en: and accumulates the intermediate results into *Cᵢ,ⱼ.* In parallel-*K* reduction,
    we instead assign each threadblock to only compute *one element of the sum* (i.e.
    *Aᵢ,ₖ Bₖ,ⱼ*). This allows us to increase the number of threadblocks by a factor
    of *K/K’*, thus utilising more SMs.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: The caveat is that now, we need to *allocate more memory* to store the results
    from each threadblock, and *invoke a second kernel* to perform a final reduction
    over the partial results to get *Cᵢ,ⱼ*.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
- en: Software pipelining
  id: totrans-87
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Normally, CUDA hides the latency of memory accesses by scheduling other warps
    to execute while a warp is waiting for data. This requires us to have enough warps
    to mask the latency.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: However, the number of warps is typically relatively small when doing GEMM.
    This is because the number of warps is limited by “The number of available registers
    per threadblock divided by the number of registers required per warp”. For matrix
    multiplication, we use a lot of registers per warp to maximise data reuse. As
    a result, we might not have enough warps to mask the latency.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: “The accumulator elements typically occupy at least half a thread’s total register
    budget”. — CUTLASS Docs
  id: totrans-90
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “累加器元素通常占用至少一半的线程总寄存器预算。” — CUTLASS文档
- en: 'To mitigate this effect, we can use **software pipelining**. In essence, we
    can (manually) pre-load the inputs for the next iteration of the loop asynchronously
    using special instructions. While the inputs are being loaded, we can continue
    to compute on the current iteration. It is summarised by the following diagram:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 为了缓解这一效果，我们可以使用**软件流水线**。本质上，我们可以（手动）使用特殊指令异步预加载下一个迭代的输入。在输入被加载的同时，我们可以继续在当前迭代上进行计算。其总结如下图所示：
- en: '![](../Images/d065965816235f5f63c80e4664e724f2.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d065965816235f5f63c80e4664e724f2.png)'
- en: Software Pipelining from [CUTLASS](https://github.com/NVIDIA/cutlass/blob/main/media/docs/efficient_gemm.md)
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 来自[CUTLASS](https://github.com/NVIDIA/cutlass/blob/main/media/docs/efficient_gemm.md)的软件下载流水线
- en: 'This is made possible by the fact that the GPU is like any modern CPU: it can
    pipeline memory accesses and arithmetic operations as long as there is no data
    dependency between them. This is known as **instruction-level parallelism**.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 这得益于GPU的特性：它像任何现代CPU一样，可以在没有数据依赖关系的情况下流水化内存访问和算术操作。这被称为**指令级并行**。
- en: Matrix multiplication in action
  id: totrans-95
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 矩阵乘法的实际应用
- en: 'If you want to see how all these concepts come together in a real implementation,
    check out my implementation of training MNIST from scratch with CUDA. There, I
    trained a multi-layer perceptron on MNIST using CUDA, achieving **6x speedup over
    optimised PyTorch** for hidden size of 128:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想了解这些概念如何在实际实现中结合起来，可以查看我用CUDA从零开始训练MNIST的实现。在那里，我使用CUDA训练了一个多层感知器，并在隐藏层大小为128时实现了**比优化后的PyTorch快6倍**：
- en: '[](https://github.com/andylolu2/cuda-mnist?source=post_page-----e920e50207a8--------------------------------)
    [## GitHub - andylolu2/cuda-mnist'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://github.com/andylolu2/cuda-mnist?source=post_page-----e920e50207a8--------------------------------)
    [## GitHub - andylolu2/cuda-mnist'
- en: Contribute to andylolu2/cuda-mnist development by creating an account on GitHub.
  id: totrans-98
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 通过在GitHub上创建账户来参与andylolu2/cuda-mnist的开发。
- en: github.com](https://github.com/andylolu2/cuda-mnist?source=post_page-----e920e50207a8--------------------------------)
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '[github.com](https://github.com/andylolu2/cuda-mnist?source=post_page-----e920e50207a8--------------------------------)'
- en: References
  id: totrans-100
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考资料
- en: 1\. [CUTLASS docs](https://github.com/NVIDIA/cutlass/blob/main/media/docs/)
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 1\. [CUTLASS文档](https://github.com/NVIDIA/cutlass/blob/main/media/docs/)
- en: 2\. [CUDA docs](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html)
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 2\. [CUDA文档](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html)
- en: 3\. [CUTLASS examples](https://github.com/NVIDIA/cutlass/tree/main/examples)
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 3\. [CUTLASS示例](https://github.com/NVIDIA/cutlass/tree/main/examples)
