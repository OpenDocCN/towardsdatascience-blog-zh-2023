# 前向传递中的学习与推理：新框架

> 原文：[https://towardsdatascience.com/learning-and-inference-in-a-forward-pass-the-new-framework-dc1356399002?source=collection_archive---------1-----------------------#2023-01-20](https://towardsdatascience.com/learning-and-inference-in-a-forward-pass-the-new-framework-dc1356399002?source=collection_archive---------1-----------------------#2023-01-20)

## 学习与推理统一为一个连续、异步和并行的过程

[](https://amassivek.medium.com/?source=post_page-----dc1356399002--------------------------------)[![Adam Kohan](../Images/3a8ed2e6fd192a14b76ff5ca2616fcf1.png)](https://amassivek.medium.com/?source=post_page-----dc1356399002--------------------------------)[](https://towardsdatascience.com/?source=post_page-----dc1356399002--------------------------------)[![Towards Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----dc1356399002--------------------------------) [Adam Kohan](https://amassivek.medium.com/?source=post_page-----dc1356399002--------------------------------)

·

[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F8275b40d8f6b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flearning-and-inference-in-a-forward-pass-the-new-framework-dc1356399002&user=Adam+Kohan&userId=8275b40d8f6b&source=post_page-8275b40d8f6b----dc1356399002---------------------post_header-----------) 发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----dc1356399002--------------------------------) ·18分钟阅读·2023年1月20日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fdc1356399002&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flearning-and-inference-in-a-forward-pass-the-new-framework-dc1356399002&user=Adam+Kohan&userId=8275b40d8f6b&source=-----dc1356399002---------------------clap_footer-----------)

--

[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fdc1356399002&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flearning-and-inference-in-a-forward-pass-the-new-framework-dc1356399002&source=-----dc1356399002---------------------bookmark_footer-----------)![](../Images/e992fb971fe17fd5af8af3572ad5acff.png)

**在三层网络上进行前向传递中的学习和推理。** 推理和学习同时进行。学习重用与推理相同的前向路径。综合来看，这些信号传播特性将学习和推理统一为一个连续、异步和并行的过程——从而解除学习的限制。这与之前关于学习的观点大相径庭，尤其是在监督设置和反向传播下。

在这篇文章中，我介绍了一个前向传播的推理和学习框架，称为信号传播框架。这是一个只使用前向传播来学习任何类型数据和任何类型网络的框架。我展示了它在离散网络、连续网络和尖峰网络中都能良好地工作，而无需修改网络架构。换句话说，用于推理的网络版本与用于学习的网络版本相同。相比之下，反向传播和以前的工作对于训练版本的网络有额外的结构和算法元素，这些被称为学习约束。

信号传播是一种约束最少的学习方法，且具有比反向传播以前的替代方法更好的性能、效率和兼容性。它也比反向传播具有更好的效率和兼容性。这个框架在[https://arxiv.org/abs/2204.01723](https://arxiv.org/abs/2204.01723)（2022年）和[https://ieeexplore.ieee.org/document/10027559](https://ieeexplore.ieee.org/document/10027559)中介绍。前向学习的起源在我的工作[https://arxiv.org/abs/1808.03357](https://arxiv.org/abs/1808.03357)（2018年）。

我开发了一个库来实现对任何模型的前向传播学习。该库的[快速开始](https://github.com/amassivek/signalpropagation#2-quick-start)指南可以帮助你在现有模型上实现这个库。还有[cifar-10](https://github.com/amassivek/signalpropagation/tree/main/examples)的[示例](https://github.com/amassivek/signalpropagation/tree/main/examples)实验，也作为教程使用。

[](https://github.com/amassivek/signalpropagation?source=post_page-----dc1356399002--------------------------------) [## GitHub - amassivek/signalpropagation: 前向传播学习与推理库，适用于神经网络...

### 信号传播：一个统一学习和推理的前向传播框架，一个用于训练的Python包...

[github.com](https://github.com/amassivek/signalpropagation?source=post_page-----dc1356399002--------------------------------)

这篇文章是一个关于前向传播学习的简明教程。在教程结束时，你将理解这个概念，并知道如何在你的工作中应用这种学习方式。教程提供了适合初学者的解释，以及适合专家的详细步骤。

目录

1.  引言

    1.1\. 以前的学习方法

    1.2\. 一种新的学习框架

    1.3\. 学习约束的问题

1.  学习的两个要素

1.  前向传播中的学习

    3.1\. 学习方法

    3.2\. 学习步骤

    3.3\. 完整程序概述

    3.4\. 尖峰网络

1.  前向学习的工作

    4.1 错误前向传播

    4.2\. 前向传播

1.  阅读材料

1.  附录：信用分配阅读

    6.1\. 空间信用分配

    6.2\. 时间信用分配

# 1\. 引言

## 1.1\. 以前的学习方法

学习是使人工神经网络工作的活跃成分。反向传播被认为是表现最佳的学习算法，为人工神经网络的成功提供了动力。然而，它是一个高度约束的学习算法。这些约束被视为其高性能所必需的。普遍接受的观点是，减少这些约束中的任何一个都会降低性能。然而，由于这些相同的约束，反向传播在效率和兼容性方面存在问题。它在时间、内存和能量方面效率低下，与生物学习模型、神经形态芯片和边缘设备的兼容性低。因此，人们可能会考虑通过减少不同的约束子集来解决这个问题，以提高效率和兼容性，同时尽量不降低性能。

例如，训练网络的反向传播有两个约束： (1) 反馈权重与前馈权重对称；以及 (2) 每个神经元都需要这些反馈权重。推理网络从不使用反馈权重，这就是我们将它们称为学习约束的原因。这些约束的子集包括：不添加任何反馈权重、只在五层网络中的一两层添加反馈权重、反馈权重不对称，或这些的任意组合。这意味着约束可以部分或完全地添加或移除，从而形成约束子集进行减少。可以不断尝试减少这些约束的不同子集，以提高效率和兼容性，并希望不会对性能产生重大影响。

以前的反向传播替代学习算法曾尝试放松约束，但没有成功。它们减少学习中的约束子集以提高效率和兼容性。它们保留其他约束，期望保持与保留所有约束（即反向传播）相似的性能。因此，这意味着学习约束存在一个光谱，从高度约束的反向传播到没有约束的信号传播，即我在这里介绍的框架。

## 1.2\. 一种新的学习框架

现在，我展示了与以往工作的不同。这里展示的结果支持最少约束的学习方法——信号传播——在性能、效率和兼容性上优于反向传播的替代方法，这些替代方法选择性地减少学习的约束。这包括一些成熟且有重大影响的方法，如随机反馈对齐、直接反馈对齐和局部学习（所有这些方法都不依赖反向传播）。这是对从神经科学到计算机科学等领域学习的迷人洞察。它惠及生物学习（例如在大脑中）到人工学习（例如在神经网络、硬件、神经形态芯片中）的领域。

信号传播也显著影响了学习算法未来研究的方向，其中反向传播是比较的标准。在学习约束的范围中，与高度约束的反向传播相反，信号传播是最少约束的方法，用于比较和作为开发学习算法的起点。仅有反向传播作为最佳比较，学习算法没有起始点，只有最终目标。现在，我引入信号传播作为学习算法评估其效率、兼容性和性能的新基准。

## 1.3\. 学习约束的问题

反向传播下的约束是什么？

为什么这是个问题？

反向传播下的学习约束与大脑中的学习难以调和。下面，我提供了主要的约束：

+   在网络中完成完整的前向传播后，才可以顺序地在反向传播中提供反馈。

+   训练网络需要为每个神经元添加全面的反馈连接。

+   学习和推理有两种不同的计算。换句话说，反馈算法是一种与前馈活动分开的计算类型。

+   反馈权重需要与前馈权重对称。

这些约束还妨碍了在硬件上高效实施学习算法，原因如下：

+   权重对称性与非双向的基本计算单元不兼容。

+   非本地权重和误差信息的传输需要特殊的通信通道。

这些学习约束禁止在学习过程中进行计算并行化，并增加了内存和计算量，原因如下：

+   前向传播需要在后向传播开始之前完成（时间，顺序）

+   隐藏层的激活需要在前向传播期间存储，以便于后向传播（内存）

+   反向传播需要特殊的反馈连接（结构）

+   参数在前向传播的反向顺序中更新（时间，同步）

# 2\. 学习的两个要素

神经网络中的学习是如何进行的？

简短回答：空间和时间信用分配

数据主要有两种形式：单个输入和多个连接的输入，这些输入按顺序或时间上连接。狗的图像是一个单个输入，因为网络仅基于该图像进行预测。在这种情况下，网络被提供一个单一图像以预测该图像是狗还是海龟。

一只海龟行走的视频是多个连接的输入，因为视频由多个图像组成，网络在看到所有这些图像后进行预测。在这种情况下，网络被提供多个图像以预测海龟是行走还是隐藏。

反向传播 (BP) 用于单个输入；时间反向传播 (BPT) 用于多个连接的输入。

BP 提供的学习包括：

+   每个神经元（空间信用分配）

BPT 提供的学习包括：

+   每个神经元（空间信用分配）

+   多个连接的输入（时间信用分配）

为每个神经元提供学习被称为空间信用分配问题。空间信用分配指的是网络中神经元的布置，例如组织成神经元层。例如，在一个五层网络中，反向传播学习信号从第五层依次传递到第一层神经元。在第3节中，我将展示信号传播学习信号是如何从第一层传递到第五层的，与推断过程相同。

为多个连接的输入提供学习被称为时间信用分配问题。时间信用分配指的是在多个连接输入中移动。例如，视频中的每一帧图像都被输入到网络中，产生来自相同神经元的新响应。每个神经元的响应是特定于每张图像/输入的。因此，反向传播学习信号在这些神经元响应中传递，从视频中最后一张图像的神经元响应开始到第一张图像的神经元响应。在第3节中，会清楚地看到信号传播学习信号从第一张图像的神经元响应传递到最后一张图像的神经元响应，与推断过程相同。

请注意，时间信用分配的内在问题是空间信用分配。时间信用分配将学习信号通过视频中每张图像。对于每张图像，空间信用分配将学习信号传递到每个神经元。信号传播优雅地通过解决内在问题来解决外在问题——前向传播，通过构建推断网络，遍历两个问题。

BP 进行空间信用分配。BPT 将 BP 扩展到同时进行空间和时间信用分配。（有关空间和时间信用分配的完整阅读，请参见第6节。）

# 3. 前向传播中的学习

## 信号传播框架 (SP)

我在这里介绍的是一种前向传递学习和推理的框架，称为信号传播（SP）。这是一个令人满意的解决方案，用于时间和空间的信用分配。SP是一种约束最少的学习方法，其性能、效率和兼容性优于以前的反向传播替代方案。它还具有比反向传播更好的效率和兼容性。SP提供了一个合理的效率和兼容性性能折中。这特别吸引人，因为它兼容基于目标的深度学习（例如监督学习和强化学习），适用于新硬件和长期存在的生物模型，而以前的工作则不然。（一般来说，反向传播是表现最好的算法。）

SP在学习发生时没有约束，包括：

+   仅前向传递，没有反向传递

+   无反馈连接或对称权重

+   只有一种用于学习和推理的计算类型。

+   在前向传递过程中随输入传播的学习信号

+   在神经元/层被前向传递到达后，更新参数

一个有趣的见解是，SP为大脑中没有错误反馈连接的神经元如何接收全局学习信号提供了解释。

因此，信号传播是：

+   与大脑和硬件中的学习模型兼容。

+   学习更高效，时间和内存消耗较少，无需额外结构。

+   一种低复杂度的学习算法。

## 3.1\. 如何在前向传递中学习？

信号传播将目标视为额外的输入（见下图）。通过这种方法，SP将目标前向传递通过网络，就像它是一个输入一样。

![](../Images/2fc9566edfdeaaced4dec79e4c0081b5.png)

将目标视为输入。来自Upsplash的动物图像。

SP在网络中向前移动（见下图），将目标和输入越来越靠近，从第一层（左上）一直到最后一层（右下）。注意到在最后一步/层时，狗的图像接近其目标[1,0,0]，而青蛙的图像接近其目标[0,1,0]。然而，狗的图像和目标与青蛙的图像和目标之间仍有较大距离。这一操作发生在每层神经元的表示空间中。例如，第1层的神经元接收狗的图像（输入x）和狗的标签（目标c），分别输出激活h_1_dog和t_1_dog。青蛙的情况也一样，产生h_1_frog和t_1_frog。在这些神经元的激活空间中，SP训练网络使输入及其目标更加接近，同时与其他输入及其相应目标保持距离。

![](../Images/765308f6a617b0aa1e14d91ef71e4ce6.png)

层层推进，将目标及其相应输入逐渐靠近，但与其他输入和目标保持距离。来自Upsplash的动物图像。

## 3.2\. 前向学习的步骤

以下是一个示例三层网络的整体图。每层都有自己的损失函数，用于更新网络中的权重。因此，SP 执行损失函数并在目标和标签到达某层时立即更新权重。由于 SP 将目标和输入一起（交替）输入，层/神经元权重会立即更新。对于空间信用分配，SP 在输入从第一层到达最后一层之前更新权重。对于时间信用分配，SP 为每个时间步长的多个连接输入（例如视频中的图像）提供学习信号，而无需等待最后一个输入被送入网络。

![](../Images/408fa98d5fb0686bbb6ebefd8c7b1d04.png)

这是一个三层网络。学习和推理的前向传播将分三步进行。每层有自己的损失，共有三种损失。输入是 x，目标是 c，二者通过网络的前端输入。

![](../Images/8abf9748100a86c52fd3dbb8833882ff.png)

在前向传播中，学习和推理的整体算法。推理和学习阶段并行运行，每层的权重会立即更新。注：对于图示网络（左），N = 3，即层数。为了清晰起见，省略了偏置（b 和 d）。损失 L（例如梯度、赫布式）和优化器（例如 SGD、Momentum、ADAM）有很多选择。输出函数 `output()`，y，在下面的步骤 4 中详细说明。

接下来，我们将逐步、逐层进行学习和推理（即产生答案/预测），以前向传播的方式进行。请注意，在下面的指南中，目标和输入被批量拼接到前向传播中，便于跟随。

## 步骤 1) 层 1

![](../Images/7d10bca4be17a50bf2b58d5a2e817a74.png)![](../Images/c5178544f49038d50668cc0f2e3c017e.png)![](../Images/cd996664a9780955ea512fae60991367.png)

## 步骤 2) 层 2

![](../Images/fdd99710ddb891abd5c6129c6bac2056.png)![](../Images/6b1d70d93fec9d8b8b3654bdfa77ac49.png)![](../Images/de57bb3c94560e8f574f96d461fbad42.png)

## 步骤 3) 层 3

![](../Images/f3e714c4560db6e591ea05daaa8a88a8.png)

## 步骤 4) 预测

在输出层，有三种选择来输出预测结果。第一种和第二种选项提供了更多的灵活性，并自然地从使用前向传播的训练过程中跟随。第一种选项是取一个 h_3 作为一个类别，并与每个 t_3 进行比较。例如，SP 输入一张狗的图像并得到 h_3_dog，然后输入所有类别的标签并得到 t_3_i = { t_3_dog, t_3_frog 和 t_3_horse}，最后它将 h_3_dog 与每个 t_3_i 进行比较；最接近的 t_3_i 即为正确的类别。

第二个选项是第一个选项的自适应版本。它是自适应的，因为SP不再将h_3_dog与每个t_3_i进行比较，而是寻找最近的t_3_i子集。例如，我们维护一个树结构，其中t_3_frog在树中比t_3_horse更接近t_3_dog。因此，我们首先将h_3_dog与t_3_frog进行比较，然后与t_3_dog进行比较，并停止。我们不会与t_3_horse进行比较，因为它距离太远，不在我们最近的t_3_i子集中。

第三个选项：经典且直观的选择是训练一个预测输出层。这个选项在回归和生成任务中也更直接。例如，一个分类层，每个类别有一个输出。因此，第3层将是一个分类层。注意，在推断过程中t_3不再使用。此外，注意到t_3_i等同于一个分类层。要看到这一点，只需将t_3_i连接在一起，形成一个分类（预测）层的权重矩阵，与h_3（例如h_3_dog，h_3_horse，…）一起使用。这意味着第三个选项是第一个选项的特例，并且可以是第二个选项的特例。

![](../Images/e817c4b00d5cb79479ae00064f78ec2d.png)![](../Images/6147fb06425c5c7e2ad95452f08e3b8b.png)![](../Images/73ca5ea2bb547d45298033cc3ee6b0fb.png)

## 3.3\. **完整过程概述**

![](../Images/e992fb971fe17fd5af8af3572ad5acff.png)![](../Images/408fa98d5fb0686bbb6ebefd8c7b1d04.png)

## 3.4\. **尖峰网络**

尖峰神经网络类似于生物神经网络。它们被用于大脑学习模型中，也用于神经形态芯片。尖峰神经网络在学习中存在两个问题。首先，反向传播下的学习约束与大脑学习难以协调，这阻碍了在硬件上高效实现学习算法（如上所述）。其次，训练尖峰网络会导致死亡神经元问题（见下文）。

下面提供了一个参考图。这些网络中的神经元通过激活（尖峰）来响应输入，以将信息传递给另一个神经元，或者什么都不做（左上图）。通常，这些网络存在一个问题，即神经元从不激活，这意味着它们从不尖峰（左下图）。因此，无论输入是什么，神经元的响应始终是无反应。这被称为死亡神经元问题。

解决这个问题的最流行方法是使用替代函数来替代神经元的脉冲行为。网络仅在学习期间使用替代品，即当学习信号发送到神经元时。替代函数（蓝色）即使在神经元不脉冲时也为神经元提供值（右上图）。因此，即使神经元没有脉冲向另一个神经元传递信息，它仍能学习（右下图）。这有助于防止神经元的“死亡”。然而，替代品在硬件学习中，如神经形态芯片中难以实现。此外，替代品不适合大脑中的学习模型。

信号传播提供了两种与大脑和硬件学习模型兼容的解决方案。

![](../Images/6063981352c8703c09e2a56117084270.png)

以下是学习信号（标记为红色）通过一个脉冲神经元（标记为S）、穿过电压或膜电位（U），以更新权重（W）的可视化。左侧是带有“死神经元”问题的反向传播。左二是带有替代函数（f）的反向传播。反向传播的学习信号是全局的（L_G），来自网络的最后一层；虚线框表示上层神经元/层。

右侧的其他图像展示了信号传播（SP）提供的两种解决方案。首先，SP也可以使用替代品，但学习信号不经过脉冲方程（S）。相反，学习信号在脉冲方程（S）之前，直接附加到替代函数（f）上。因此，SP与大脑中的学习更兼容，例如在生物神经元的多室模型中。其次，SP可以仅使用电压或膜电位（U）进行学习。在这种情况下，学习信号直接附加到U上。这不需要替代品或对神经元的更改。因此，SP与硬件中的学习兼容。

![](../Images/2d78d2946f5f5fef77b8eb64e13e90ed.png)

# 4\. 前向学习的研究工作

关于前向学习的工作列表 - 使用前向传播进行学习。工作按日期排序。

一个社区维护的仓库网页用于记录前向学习方法，位于 [https://amassivek.github.io/sigprop](https://amassivek.github.io/sigprop) 。代码库可在 [https://github.com/amassivek/signalpropagation](https://github.com/amassivek/signalpropagation) 获取。

## 4.1\. 误差前向传播算法（2018）

误差前向传播算法是信号传播框架在前向传播中的实现（如下图）。在信号传播下，S是上下文c的变换，对于监督学习来说，c是目标。

在误差前向传播中，S是从输出到网络前端的误差投影，如下图所示。

![](../Images/5e88aec10f836b620d931ced3c41ca94.png)

错误前向传播算法。来自 [MNIST 数据集](http://yann.lecun.com/exdb/mnist/) 的图像 7。

错误前向传播：重用前馈连接在深度学习中传播错误

[https://arxiv.org/abs/1808.03357](https://arxiv.org/abs/1808.03357)

## 4.2\. 前向前向算法 (2022)

前向前向算法是信号传播框架在前向传递中的实现（见下图）。在信号传播下，S 是上下文 c 的变换，对于监督学习来说，这是目标。

在前向前向中，S 是目标 c 和输入 x 的连接，如下图所示。

![](../Images/dde3a141fe9cfec6e48aa88d8123d3a9.png)

前向前向算法。来自 [MNIST 数据集](http://yann.lecun.com/exdb/mnist/) 的图像 7。

前向前向算法

[https://www.cs.toronto.edu/~hinton/FFA13.pdf](https://www.cs.toronto.edu/~hinton/FFA13.pdf)

# 5\. 阅读材料

信号传播：前向传递中的学习与推理框架

[https://arxiv.org/abs/2204.01723](https://arxiv.org/abs/2204.01723) (2022)

前向前向算法

[https://www.cs.toronto.edu/~hinton/FFA13.pdf](https://www.cs.toronto.edu/~hinton/FFA13.pdf) (2022)

错误前向传播：重用前馈连接在深度学习中传播错误

[https://arxiv.org/abs/1808.03357](https://arxiv.org/abs/1808.03357) (2018)

## 5.1 其他材料

一份关于空间和时间信贷分配的良好指南。我参考了它来帮助编写“附录：信贷分配阅读”。

使用深度学习的经验训练尖峰神经网络

[https://arxiv.org/abs/2109.12894](https://arxiv.org/abs/2109.12894) (2021)

社区维护了一个代码库网页来记录前向学习方法，位于 [https://amassivek.github.io/sigprop](https://amassivek.github.io/sigprop)。

代码库可以在 [https://github.com/amassivek/signalpropagation](https://github.com/amassivek/signalpropagation) 上获取。

感谢：Alexandra Marmarinos 的编辑工作和指导。

# 6\. 附录：信贷分配阅读

## 6.1\. 空间信贷分配

空间信贷分配的问题是：学习信号如何到达每一个神经元？

在下图的左侧，是一个三层网络。一般来说，学习分为两个阶段：推理阶段和学习阶段。在第一个阶段，称为推理阶段，输入从第一层传递到最后一层。由于输入通过网络前向传递，因此推理阶段发生在网络的“前向传递”过程中。在第二阶段，称为学习阶段，学习信号（标记为红色）需要到达网络中的每一个神经元。

不同的学习算法在学习阶段有不同的解决方案。在反向传播中，学习信号通过网络向后传播，因此学习阶段发生在通过网络的“反向传递”中。正如我们将在信号传播中看到的，学习也可以在前向传递中进行。

广泛来说，学习阶段有两种方法。第一种方法计算全局学习信号（左中图），然后将此学习信号发送到每个神经元。第二种方法在每个神经元（或层）计算局部学习信号（右图）。第一种方法的问题在于需要以精确的方式协调将此信号发送到每个神经元。这在时间、内存和兼容性上都很昂贵。第二种方法没有遇到这个问题，但性能较差。

![](../Images/a386c471774bfbe111f7dbc3daa182a2.png)

## 6.2\. 时间信用分配

信贷分配的时间局部性问题是：全局学习信号如何到达多个连接的输入（即每个时间步）？

单张图像只需要学习信号到达每个神经元。然而，视频是一系列连接的图像。因此，学习信号需要通过多个连接的输入（即时间）进行传播，从视频中的最后一张图像一直到视频中的第一张图像。这一概念适用于任何序列或时间序列数据。那么，全局学习信号如何到达每个时间步？有两种流行的方法来回答这个问题：时间上的反向传播和前向模式微分。

## 6.2.1\. 时间上的反向传播（BPT）

对上述问题的主要回答如下，并分为两个阶段。首先，将构成视频的所有图像逐一输入网络。这是推断阶段，在这个阶段，多个连接的输入通过网络向前传递（前向传递）。其次，从最后一张图像开始向回传播学习信号直到第一张图像。这是学习阶段，在这个阶段，学习信号在多个连接的输入（即时间）中向后传播；因此，称为时间上的反向传播。

## 第一步：推断

在下图中，BPT将构成视频的每一张图像X[i]（例如乌龟走路的图像）输入网络。BPT从第1张图像X[0]（第一张图像的左下角）开始，这是时间步长1（时间显示在图的顶部）。接着，BPT输入图像X[1]，这是时间步长2。最后，我们以时间步长3的最后一张图像X[2]结束——这个演示用于非常短的视频或GIF。每次BPT将图像输入网络时，请注意网络中的中间层将每张图像通过时间连接到下一张图像。

![](../Images/2a6b9a826f6d1959705d165fccd79c2f.png)

## 第二步：通过时间进行学习

BPT将学习信号（用红色标记）从图像（时间）中向后传播，形成了乌龟行走的视频。学习信号是从损失函数（图中的右上方）形成的。它的传播方向与我们输入图像X[i]的方向相反。首先计算时间3的图像X[2]的梯度/更新，然后是时间2的图像X[1]，最后是时间1的图像X[0]。这就是为什么它被称为时间上的反向传播。再次注意，网络中的中间层将来自最后一张图像X[2]的学习信号连接到第一张图像X[0]。

![](../Images/aff1d694a7b664ae2b3b6b304d537f76.png)

## 6.2.2\. 正向模式微分（FMD）

在FMD下，推断（第1步）和学习（第2步）阶段的行为是类似的。因此，FMD将第1步（推断）和第2步（学习）一起进行（交替）。怎么做？在第2步中，FMD将学习信号向前传播通过图像（时间），这与第1步中的推断过程非常相似。因此，学习信号不再需要从视频中的最后一张图像X[3]返回到第一张图像X[0]。结果是：FMD的学习信号从X[0]开始，而不需要等待X[3]。

为什么选择FMD而不是BPT？上面我讨论了在反向传播（backpropagation）下学习的限制以及它在效率和兼容性方面存在的问题。FMD尝试提高效率。特别是，BPT会在学习之前将所有构成视频的图像输入到网络中。而FMD则不会，因此在时间上比BPT更高效。然而，FMD在成本上明显高于BPT，特别是在内存和计算方面。注意，FMD解决了时间上的问题。然而，它并没有解决反向传播下空间信用分配的学习限制，这在FMD中也存在。

![](../Images/003081cfcb5e5ffd6935788b92da960c.png)![](../Images/bfbbf05ce62d1c63fdd6f92040c8683b.png)![](../Images/06c556b25d352b89599c82116278443b.png)

除非另有说明，否则所有图片均由作者提供。
