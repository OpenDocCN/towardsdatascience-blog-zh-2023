["```py\n#Variation 1: Do not fit transform both training and testing dataset!\nss = StandardScaler()\nX_train_scaled = ss.fit_transform(X_train)\nX_test_scaled = ss.fit_transform(X_test)\n\n#Variation 2: Remember to transform your training dataset!\nX_train_scaled = ss.fit(X_train)\nX_test_scaled = ss.transform(X_test)\n```", "```py\n#Previously an oversight, correction contributed by @Scott Lyden\n\nimport numpy as np\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.feature_selection import SequentialFeatureSelector\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.pipeline import Pipeline\n\nX, y = load_breast_cancer(return_X_y=True, as_frame=True)\n\n#Without Pipeline\nselect = SequentialFeatureSelector(RandomForestClassifier(n_estimators=100, n_jobs=-1), \n                                   n_features_to_select=8, \n                                   direction='forward').fit(X,y)\nX_selected = select.transform(X)\nlogreg = LogisticRegression()\nnp.mean(cross_val_score(estimator=logreg, \n                        X=X_selected, \n                        y=y, \n                        n_jobs=-1))\n\n#With Pipeline\npipe = Pipeline([(\"select\", SequentialFeatureSelector(RandomForestClassifier(n_estimators=100, n_jobs=-1), \n                                                      n_features_to_select=8, \n                                                      direction='forward',\n                                                      n_jobs=-1)),\n                 (\"log\", LogisticRegression())])\nnp.mean(cross_val_score(estimator=pipe, X=X, y=y))\n```", "```py\nfrom sklearn.compose import ColumnTransformer\n\nohe_categorical_features = ['a', 'b', 'c']\nohe_categorical_transformer = Pipeline(steps=[\n    ('ohe', OneHotEncoder(handle_unknown='ignore', sparse_output=False, drop='first'))\n])\n\norde_categorical_features = ['d', 'e', 'f']\norde_categorical_transformer = Pipeline(steps=[\n    ('orde', OrdinalEncoder(dtype='float'))\n])\n\ncol_trans = ColumnTransformer(\n    transformers=[\n        ('ohe_categorical_features', ohe_categorical_transformer, ohe_categorical_features),\n        ('orde_categorical_features', orde_categorical_transformer, orde_categorical_features),\n    ], \n    remainder='passthrough', \n    n_jobs=-1,\n)\n```", "```py\nimputer = KNNImputer(n_neighbors=5)\nfeature_select = SequentialFeatureSelector(RandomForestClassifier(n_estimators=100), n_features_to_select=8, direction='forward')\nlog_reg = LogisticRegression()\npipe = Pipeline([(\"imputer\", imputer),\n                 (\"select\", feature_select),\n                 (\"log\", log_reg)])\n```", "```py\nfrom sklearn.pipeline import make_pipeline\nimputer = KNNImputer(n_neighbors=5)\nfeature_select = SequentialFeatureSelector(RandomForestClassifier(n_estimators=100), n_features_to_select=8, direction='forward')\nlog_reg = LogisticRegression()\nmake_pipeline(imputer, feature_select, log_reg)\n```", "```py\nfrom scipy.stats import boxcox\nfrom sklearn.preprocessing import FunctionTransformer\nfrom sklearn.compose import ColumnTransformer\n\nboxcox_features = ['x1', 'x2']\nboxcox_transformer = Pipeline(steps=[\n    ('boxcox', FunctionTransformer(lambda x: boxcox(x)[0])\n])\n\ncol_trans = ColumnTransformer(\n    transformers=[\n        ('boxcox_features', boxcox_transformer, boxcox_features),\n        ...\n    ], \n    remainder='passthrough', \n    n_jobs=-1,\n)\n```", "```py\ndef outlier_thresholds(df: pd.DataFrame, \n                       col: str, \n                       q1: float = 0.05, \n                       q3: float = 0.95):\n    #1.5 as multiplier is a rule of thumb. Generally, the higher the multiplier,\n    #the outlier threshold is set farther from the third quartile, allowing fewer data points to be classified as outliers\n\n    return (df[col].quantile(q1) - 1.5 * (df[col].quantile(q3) - df[col].quantile(q1)),\n            df[col].quantile(q3) + 1.5 * (df[col].quantile(q3) - df[col].quantile(q1)))\n\ndef delete_potential_outlier_list(df: pd.DataFrame,\n                                  cols: list) -> pd.DataFrame:\n\n    for item in cols:\n        low, high = outlier_thresholds(df, col)\n        df.loc[(df[col]>high) | (df[col]<low),col] = np.nan\n    return df\n\nclass OutlierRemove(BaseEstimator, TransformerMixin):\n\n    def __init__(self, outlierlist):\n        self.outlierlist = outlierlist\n\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self,X,y=None):\n        return delete_potential_outlier_list(X, self.outlierlist)\n```", "```py\npipe = Pipeline([(\"remove_outlier\", OutlierRemove([\"a\", \"b\", \"c\"])),\n                 (\"imputer\", imputer),\n                 (\"select\", feature_select),\n                 (\"log\", log_reg)])\n```", "```py\nfrom sklearn.pipeline import FeatureUnion\n\nstandard_numerical_features = ['x1', 'x2']\nstandard_numerical_transformer = Pipeline(steps=[\n    ('remove_outlier', OutlierTrans(standard_numerical_features)),\n    ('scale', StandardScaler())\n])\n\nohe_categorical_features = ['x3', 'x4']\nohe_categorical_transformer = Pipeline(steps=[\n    ('ohe', OneHotEncoder(handle_unknown='ignore', sparse_output=False, drop='first'))\n])\n\nfeature_union = FeatureUnion(\n    transformers=[\n        ('standard_numerical_features', standard_numerical_transformer),\n        ('ohe_categorical_features', ohe_categorical_transformer),\n    ], \n    n_jobs=-1,\n)\n\npipeline = Pipeline([\n        ('feature_union', feature_union),\n        ('model', RandomForestClassifier())\n])\n\npipeline.fit(X_train, y_train)\n```", "```py\nimport pandas as pd\n\ndf = (pd\n      .read_csv('../../dataset/bank_marketing/bank-11k.csv', sep=',')\n      .rename(columns={'y': 'deposit'})\n      .pipe(lambda df_: df_.assign(deposit=np.where(df_.deposit == \"no\", 0, 1)))\n     )\n```", "```py\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(df.drop(columns=['deposit']),\n                                                    df[['deposit']].values.ravel(),\n                                                    test_size=0.2,\n                                                    random_state=42)\n```", "```py\nfrom sklearn.base import BaseEstimator, TransformerMixin\n\n#Custom class #1: switch between classifiers\nclass ClfSwitcher(BaseEstimator):\n\n    #By default, run XGBClassifier\n    def __init__(self, estimator = XGBClassifier()):\n        self.estimator = estimator\n\n    def fit(self, X, y=None, **kwargs):\n        self.estimator.fit(X, y)\n        return self\n\n    def predict(self, X, y=None):\n        return self.estimator.predict(X)\n\n    def predict_proba(self, X):\n        return self.estimator.predict_proba(X)\n\n    def score(self, X, y):\n        return self.estimator.score(X, y)\n\n#Custom class 2: remove outliers\ndef outlier_thresholds(df: pd.DataFrame, \n                       col: str, \n                       q1: float = 0.05, \n                       q3: float = 0.95):\n\n    return (df[col].quantile(q1) - 1.5 * (df[col].quantile(q3) - df[col].quantile(q1)),\n            df[col].quantile(q3) + 1.5 * (df[col].quantile(q3) - df[col].quantile(q1)))\n\ndef delete_potential_outlier_list(df: pd.DataFrame,\n                                  cols: list) -> pd.DataFrame:\n\n    for item in cols:\n        low, high = outlier_thresholds(df, col)\n        df.loc[(df[col]>high) | (df[col]<low),col] = np.nan\n    return df\n\nclass OutlierTrans(BaseEstimator, TransformerMixin):\n\n    def __init__(self, outlierlist):\n        self.outlierlist = outlierlist\n\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self,X,y=None):\n        return delete_potential_outlier_list(X, self.outlierlist)\n\n#Custom class #3: add new columns, drop column, and modify data types\nclass TweakBankMarketing(BaseEstimator, TransformerMixin):\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X, y=None):\n        return (X\n                .assign(pdays_cat=lambda df_: np.where(df_.pdays < 0, \"no contact\", \"contacted\"),\n                        previous_cat=lambda df_: np.where(df_.previous == 0, \"no contact\", \"contacted\"),\n                        job=lambda df_: np.where(df_.job == \"unknown\", np.nan, df_.job),\n                        education=lambda df_: np.where(df_.education == \"unknown\", np.nan, df_.education),\n                        contact=lambda df_:np.where(df_.contact == \"unknown\", np.nan, df_.contact),\n                        poutcome=lambda df_: np.where(df_.poutcome == \"other\", np.nan, df_.contact),\n                       ) #add new predictors\n                .drop(columns=['duration']) #drop predictor due to data leakage\n                .astype({'age': 'int8', \n                         'balance': 'int32', \n                         'day': 'category', \n                         'campaign': 'int8', \n                         'pdays': 'int16', \n                         'previous': 'int16',})\n                .pipe(lambda df_: df_.astype({column: 'category' for column in (df_.select_dtypes(\"object\").columns.tolist())})) #convert data type from object to category\n               ) \n```", "```py\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, OrdinalEncoder, OneHotEncoder,\nfrom sklearn.impute import KNNImputer\n\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\n\nstandard_numerical_features = ['age', 'campaign', 'pdays', 'previous'] #drop pdays\nstandard_numerical_transformer = Pipeline(steps=[\n    ('remove_outlier', OutlierTrans(standard_numerical_features)),\n    ('scale', StandardScaler())\n])\n\nminmax_numerical_features = ['balance']\nminmax_numerical_transformer = Pipeline(steps=[\n    ('remove_outlier', OutlierTrans(minmax_numerical_features)),\n    ('scale', MinMaxScaler())\n])\n\nohe_categorical_features = ['job', 'marital', 'default', 'housing', 'loan', 'contact', 'poutcome', 'pdays_cat', 'previous_cat']\nohe_categorical_transformer = Pipeline(steps=[\n    ('ohe', OneHotEncoder(handle_unknown='ignore', sparse_output=False, drop='first'))\n])\n\norde_categorical_features = ['education', 'day', 'month']\norde_categorical_transformer = Pipeline(steps=[\n    ('orde', OrdinalEncoder(dtype='float'))\n])\n\ncol_trans = ColumnTransformer(\n    transformers=[\n        ('standard_numerical_features', standard_numerical_transformer, standard_numerical_features),\n        ('minmax_numerical_features', minmax_numerical_transformer, minmax_numerical_features),\n        ('ohe_categorical_features', ohe_categorical_transformer, ohe_categorical_features),\n        ('orde_categorical_features', orde_categorical_transformer, orde_categorical_features),\n    ], \n    remainder='passthrough', \n    verbose=0, \n    verbose_feature_names_out=False, \n    n_jobs=-1,)\n\npipeline = Pipeline(steps = [\n    ('tweak_bank_marketing', TweakBankMarketing()),\n    ('col_trans', col_trans),\n    ('imputer', KNNImputer(n_neighbors=5)),\n    ('clf', ClfSwitcher()),\n])\npipeline\n```", "```py\n#We define all the hyperparameters for 4 classifiers so that we can easily switch from one to another\nparams_grid = [\n    {'clf__estimator': [SGDClassifier()],\n     'clf__estimator__penalty': ('l2', 'elasticnet', 'l1'),\n     'clf__estimator__max_iter': [500],\n     'clf__estimator__tol': [1e-4],\n     'clf__estimator__loss': ['hinge', 'log_loss', 'modified_huber'],\n    },\n\n    {'clf__estimator': [LogisticRegression()],\n     'clf__estimator__C': [0.01, 0.1, 1, 10, 100],\n     'clf__estimator__max_iter': [1000]\n    },\n\n    {'clf__estimator': [RandomForestClassifier(n_estimators=100)], \n     'clf__estimator__max_features': [3,4,5,6,7],\n     'clf__estimator__max_depth': [3,4,5]\n    },\n\n    {'clf__estimator': [XGBClassifier()], \n     'clf__estimator__max_depth': [4,5,6],\n     'clf__estimator__learning_rate': [0.01, 0.1],\n     'clf__estimator__n_estimators': [80, 100],\n     'clf__estimator__booster': ['gbtree'],\n     'clf__estimator__gamma': [7, 25, 100],\n     'clf__estimator__subsample': [0.3, 0.6],\n     'clf__estimator__colsample_bytree': [0.5, 0.7],\n     'clf__estimator__colsample_bylevel': [0.5, 0.7],\n     'clf__estimator__eval_metric': ['auc']\n    },\n]\n```", "```py\nfrom sklearn.model_selection import GridSearchCV\n\n%%time\ngrid = GridSearchCV(pipeline, params_grid, cv=5, n_jobs=-1, return_train_score=False, verbose=0)\ngrid.fit(X_train, y_train)\n```", "```py\nprint(f'Best params: {grid.best_params_}')\nprint(f'Best CV score: {grid.best_score_}')\nprint(f'Validation-set score: {grid.score(X_test, y_test)}')\n\nprint(f'Accuracy score: {accuracy_score(y_test, grid.predict(X_test))}')\nprint(f'Precision score: {precision_score(y_test, grid.predict(X_test))}')\nprint(f'Recall score: {recall_score(y_test, grid.predict(X_test))}')\nprint(f'ROC-AUC score: {roc_auc_score(y_test, grid.predict(X_test))}')\n```", "```py\nfpr, tpr, thresholds = skmet.roc_curve(y_test, grid.predict(X_test))\nroc_auc = skmet.auc(fpr, tpr)\ndisplay = skmet.RocCurveDisplay(fpr=fpr, \n                                tpr=tpr, \n                                roc_auc=roc_auc,\n                                estimator_name='XGBoost Classifier')\ndisplay.plot();\n```"]