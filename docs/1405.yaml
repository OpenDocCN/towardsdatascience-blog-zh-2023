- en: Fine-tuning an LLM model with H2O LLM Studio to generate Cypher statements
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 H2O LLM Studio 对 LLM 模型进行微调以生成 Cypher 语句
- en: 原文：[https://towardsdatascience.com/fine-tuning-an-llm-model-with-h2o-llm-studio-to-generate-cypher-statements-3f34822ad5?source=collection_archive---------1-----------------------#2023-04-24](https://towardsdatascience.com/fine-tuning-an-llm-model-with-h2o-llm-studio-to-generate-cypher-statements-3f34822ad5?source=collection_archive---------1-----------------------#2023-04-24)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/fine-tuning-an-llm-model-with-h2o-llm-studio-to-generate-cypher-statements-3f34822ad5?source=collection_archive---------1-----------------------#2023-04-24](https://towardsdatascience.com/fine-tuning-an-llm-model-with-h2o-llm-studio-to-generate-cypher-statements-3f34822ad5?source=collection_archive---------1-----------------------#2023-04-24)
- en: Avoid depending on external and ever changing APIs for your knowledge graph
    based chatbot
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 避免依赖外部和不断变化的 API 来构建你的知识图谱聊天机器人
- en: '[](https://bratanic-tomaz.medium.com/?source=post_page-----3f34822ad5--------------------------------)[![Tomaz
    Bratanic](../Images/d5821aa70918fcb3fc1ff0013497b3d5.png)](https://bratanic-tomaz.medium.com/?source=post_page-----3f34822ad5--------------------------------)[](https://towardsdatascience.com/?source=post_page-----3f34822ad5--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----3f34822ad5--------------------------------)
    [Tomaz Bratanic](https://bratanic-tomaz.medium.com/?source=post_page-----3f34822ad5--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://bratanic-tomaz.medium.com/?source=post_page-----3f34822ad5--------------------------------)[![Tomaz
    Bratanic](../Images/d5821aa70918fcb3fc1ff0013497b3d5.png)](https://bratanic-tomaz.medium.com/?source=post_page-----3f34822ad5--------------------------------)[](https://towardsdatascience.com/?source=post_page-----3f34822ad5--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----3f34822ad5--------------------------------)
    [Tomaz Bratanic](https://bratanic-tomaz.medium.com/?source=post_page-----3f34822ad5--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F57f13c0ea39a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffine-tuning-an-llm-model-with-h2o-llm-studio-to-generate-cypher-statements-3f34822ad5&user=Tomaz+Bratanic&userId=57f13c0ea39a&source=post_page-57f13c0ea39a----3f34822ad5---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----3f34822ad5--------------------------------)
    ·8 min read·Apr 24, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F3f34822ad5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffine-tuning-an-llm-model-with-h2o-llm-studio-to-generate-cypher-statements-3f34822ad5&user=Tomaz+Bratanic&userId=57f13c0ea39a&source=-----3f34822ad5---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F57f13c0ea39a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffine-tuning-an-llm-model-with-h2o-llm-studio-to-generate-cypher-statements-3f34822ad5&user=Tomaz+Bratanic&userId=57f13c0ea39a&source=post_page-57f13c0ea39a----3f34822ad5---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----3f34822ad5--------------------------------)
    ·8 分钟阅读·2023年4月24日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F3f34822ad5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffine-tuning-an-llm-model-with-h2o-llm-studio-to-generate-cypher-statements-3f34822ad5&user=Tomaz+Bratanic&userId=57f13c0ea39a&source=-----3f34822ad5---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F3f34822ad5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffine-tuning-an-llm-model-with-h2o-llm-studio-to-generate-cypher-statements-3f34822ad5&source=-----3f34822ad5---------------------bookmark_footer-----------)![](../Images/edfa946a7b182f6f657c887e009d825f.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F3f34822ad5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffine-tuning-an-llm-model-with-h2o-llm-studio-to-generate-cypher-statements-3f34822ad5&source=-----3f34822ad5---------------------bookmark_footer-----------)![](../Images/edfa946a7b182f6f657c887e009d825f.png)'
- en: Photo by [Mike Hindle](https://unsplash.com/@mikehindle?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 照片由 [Mike Hindle](https://unsplash.com/@mikehindle?utm_source=medium&utm_medium=referral)
    拍摄，来自 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: Large language models like ChatGPT have a knowledge cutoff date beyond which
    they are not aware of any events that happened later. Instead of fine-tuning models
    with later information, the trend is to provide additional external context to
    LLM at query time. I have written a couple of blog posts on implementing a [context-aware
    knowledge graph-based bot](https://medium.com/neo4j/context-aware-knowledge-graph-chatbot-with-gpt-4-and-neo4j-d3a99e8ae21e)
    to a [bot that can read through the company’s resources to answer questions](/implementing-a-sales-support-agent-with-langchain-63c4761193e7).
    However, I have used OpenAI’s large language models in all of the examples so
    far
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 像 ChatGPT 这样的大型语言模型有一个知识截止日期，之后它们不了解任何发生的事件。与其用更晚的信息微调模型，不如在查询时为 LLM 提供额外的外部上下文。我已经撰写了一些博客文章，介绍了实现
    [基于知识图谱的上下文感知聊天机器人](https://medium.com/neo4j/context-aware-knowledge-graph-chatbot-with-gpt-4-and-neo4j-d3a99e8ae21e)
    和 [一个能够阅读公司资源以回答问题的聊天机器人](/implementing-a-sales-support-agent-with-langchain-63c4761193e7)。然而，到目前为止，我在所有示例中使用的都是
    OpenAI 的大型语言模型。
- en: While OpenAI’s official position is that they don’t use users’ data to improve
    their models, there are stories like how [Samsung employees leaked top secret
    data by inputting it into ChatGPT](https://mashable.com/article/samsung-chatgpt-leak-details).
    If I were dealing with top-secret, proprietary information, I would stay on the
    safe side and not share that information with OpenAI. Luckily, new open-source
    LLM models are popping up every day.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管 OpenAI 官方立场是他们不使用用户数据来改进他们的模型，但有像 [三星员工通过输入 ChatGPT 泄露机密数据](https://mashable.com/article/samsung-chatgpt-leak-details)
    这样的故事。如果我处理的是绝密的专有信息，我会保持谨慎，不与 OpenAI 分享这些信息。幸运的是，新开源的 LLM 模型每天都在涌现。
- en: I have tested many open-source LLM models on their ability to generate Cypher
    statements. Some of them have a basic understanding of Cypher syntax. However,
    I haven’t found any models reliably generating Cypher statements based on provided
    examples or graph schema. So, the only solution was to fine-tune an open-sourced
    LLM model to generate Cypher statements reliably.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 我测试了许多开源 LLM 模型，以评估它们生成 Cypher 语句的能力。它们中的一些对 Cypher 语法有基本了解。然而，我还没有找到任何模型能够根据提供的示例或图谱模式可靠地生成
    Cypher 语句。因此，唯一的解决方案是微调一个开源 LLM 模型，以可靠地生成 Cypher 语句。
- en: I have never fine-tuned any NLP model, let alone an LLM. Therefore, I had to
    find a simple way to get started without first obtaining a Ph.D. in machine learning.
    Luckily, I stumbled upon H2O’s LLM Studio tool, released just a couple of days
    ago, which provides a graphical interface for fine-tuning LLM models. I was delighted
    to discover that fine-tuning an LLM no longer required me to write any code or
    long bash commands. With just a few mouse clicks, I would be able to complete
    the task.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 我从未微调过任何 NLP 模型，更不用说 LLM 了。因此，我必须找到一种简单的方法来入门，而无需首先获得机器学习博士学位。幸运的是，我偶然发现了 H2O
    最近发布的 LLM Studio 工具，它提供了一个图形界面用于微调 LLM 模型。我很高兴地发现，微调 LLM 不再需要我编写任何代码或长命令。只需几次鼠标点击，我就能完成任务。
- en: '[](https://github.com/h2oai/h2o-llmstudio?source=post_page-----3f34822ad5--------------------------------)
    [## GitHub - h2oai/h2o-llmstudio: H2O LLM Studio - a framework and no-code GUI
    for fine-tuning LLMs'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://github.com/h2oai/h2o-llmstudio?source=post_page-----3f34822ad5--------------------------------)
    [## GitHub - h2oai/h2o-llmstudio: H2O LLM Studio - 一个用于微调 LLM 的框架和无代码 GUI'
- en: Welcome to H2O LLM Studio, a framework and no-code GUI designed for fine-tuning
    state-of-the-art large language models…
  id: totrans-14
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 欢迎来到 H2O LLM Studio，一个用于微调最先进的大型语言模型的框架和无代码 GUI…
- en: github.com](https://github.com/h2oai/h2o-llmstudio?source=post_page-----3f34822ad5--------------------------------)
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: github.com](https://github.com/h2oai/h2o-llmstudio?source=post_page-----3f34822ad5--------------------------------)
- en: All the code of this blog post is [available on GitHub](https://github.com/tomasonjo/blogs/tree/master/h20_llm).
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 本博客文章中的所有代码均 [在 GitHub 上可用](https://github.com/tomasonjo/blogs/tree/master/h20_llm)。
- en: Preparing a training dataset
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备训练数据集
- en: First, I had to learn how the training dataset should be structured. I examined
    their [tutorial notebook](https://colab.research.google.com/drive/1-OYccyTvmfa3r7cAquw8sioFFPJcn4R9?usp=sharing)
    and discovered that the tool could handle training data provided as a CSV file,
    where the first column includes user prompts, and the second column contains desired
    LLM responses.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我必须学习训练数据集的结构应如何安排。我查看了他们的 [教程笔记本](https://colab.research.google.com/drive/1-OYccyTvmfa3r7cAquw8sioFFPJcn4R9?usp=sharing)，发现该工具可以处理以
    CSV 文件提供的训练数据，其中第一列包括用户提示，第二列包含期望的 LLM 响应。
- en: 'Ok, that’s easy enough. Now I just had to produce the training examples. I
    decided that 200 is a good number of training examples. However, I am way too
    lazy to write 200 Cypher statements manually. Therefore, I employed GPT-4 to do
    the job for me. The code can be found here:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，这很简单。现在我只需生成训练示例。我决定 200 个训练示例是一个好数字。然而，我实在懒得手动编写 200 个 Cypher 语句。因此，我使用了
    GPT-4 来完成这项工作。代码可以在这里找到：
- en: '[](https://github.com/tomasonjo/blogs/blob/master/h20_llm/LLM_train_dataset.ipynb?source=post_page-----3f34822ad5--------------------------------)
    [## blogs/LLM_train_dataset.ipynb at master · tomasonjo/blogs'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://github.com/tomasonjo/blogs/blob/master/h20_llm/LLM_train_dataset.ipynb?source=post_page-----3f34822ad5--------------------------------)
    [## blogs/LLM_train_dataset.ipynb at master · tomasonjo/blogs'
- en: You can't perform that action at this time. You signed in with another tab or
    window. You signed out in another tab or…
  id: totrans-21
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 目前无法执行该操作。您在另一个标签或窗口中登录。您在另一个标签或窗口中退出了…
- en: github.com](https://github.com/tomasonjo/blogs/blob/master/h20_llm/LLM_train_dataset.ipynb?source=post_page-----3f34822ad5--------------------------------)
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: github.com](https://github.com/tomasonjo/blogs/blob/master/h20_llm/LLM_train_dataset.ipynb?source=post_page-----3f34822ad5--------------------------------)
- en: The movie recommendation dataset is baked into GPT-4, so it can generate good
    enough examples. However, some examples are slightly off and don’t fit the graph
    schema. So, if I were fine-tuning an LLM for commercial use, I would use GPT-4
    to generate Cypher statements and then walk through manually to validate them.
    Additionally, I would want to ensure that the validation set contains no examples
    from the training set.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 电影推荐数据集已嵌入到 GPT-4 中，因此它可以生成足够好的示例。然而，一些示例略有偏差，不符合图表模式。因此，如果我为商业用途微调 LLM，我会使用
    GPT-4 生成 Cypher 语句，然后手动检查以验证它们。此外，我还要确保验证集不包含训练集中的示例。
- en: I have also tested if a prefix “Create a Cypher statement for the following
    question” is needed for instructions. It seems that some models like **EleutherAI/pythia-12b-deduped**
    need the prefix, otherwise they fail miserably. On the other hand, **facebook/opt-13b**
    did a solid job even without the prefix.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 我还测试了是否需要“为以下问题创建一个 Cypher 语句”作为指令的前缀。似乎一些模型如 **EleutherAI/pythia-12b-deduped**
    需要这个前缀，否则效果会非常差。另一方面，**facebook/opt-13b** 即使没有前缀也表现得很好。
- en: '![](../Images/210ed6317452a1b90552f269ebb58efd.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/210ed6317452a1b90552f269ebb58efd.png)'
- en: Models trained without or with a prefix in instructions. Image by the author.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 没有前缀或有前缀的指令训练的模型。图片由作者提供。
- en: To be able to compare all models with the same dataset, I used a dataset that
    adds a prefix “Create a Cypher statement for the following question:” to the instructions
    section of the dataset.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 为了能够使用相同的数据集比较所有模型，我使用了一个数据集，该数据集在数据集的指令部分添加了前缀“为以下问题创建一个 Cypher 语句：”。
- en: H2O LLM Studio installation
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: H2O LLM Studio 安装
- en: H2O LLM Studio can be installed in two simple steps. In the first step, we have
    to install Python 3.10 environment if it is missing. The steps to install Python
    3.10 are described in their GitHub repository.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: H2O LLM Studio 可以通过两个简单步骤进行安装。第一步是安装 Python 3.10 环境，如果它缺失的话。安装 Python 3.10 的步骤描述在他们的
    GitHub 仓库中。
- en: '[](https://github.com/h2oai/h2o-llmstudio?source=post_page-----3f34822ad5--------------------------------)
    [## GitHub - h2oai/h2o-llmstudio: H2O LLM Studio - a framework and no-code GUI
    for fine-tuning LLMs'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://github.com/h2oai/h2o-llmstudio?source=post_page-----3f34822ad5--------------------------------)
    [## GitHub - h2oai/h2o-llmstudio: H2O LLM Studio - 用于微调 LLM 的框架和无代码 GUI'
- en: Welcome to H2O LLM Studio, a framework and no-code GUI designed for fine-tuning
    state-of-the-art large language models…
  id: totrans-31
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 欢迎使用 H2O LLM Studio，一个为微调最先进大型语言模型设计的框架和无代码 GUI…
- en: github.com](https://github.com/h2oai/h2o-llmstudio?source=post_page-----3f34822ad5--------------------------------)
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: github.com](https://github.com/h2oai/h2o-llmstudio?source=post_page-----3f34822ad5--------------------------------)
- en: After we ensure a Python 3.10 environment, we simply clone the repository and
    install dependencies with the `make install` command. After the installation,
    we can run the LLM studio with the `make wave` command. Now we can open the graphical
    interface in your favourite browser by opening the `localhost:10101` website.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在确保有 Python 3.10 环境后，我们只需克隆仓库并使用 `make install` 命令安装依赖项。安装完成后，我们可以使用 `make wave`
    命令运行 LLM studio。现在可以通过在浏览器中打开 `localhost:10101` 网站来打开图形界面。
- en: '![](../Images/10c03010733405249c9df00bf2234c6d.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/10c03010733405249c9df00bf2234c6d.png)'
- en: H2O LLM Studio home page. Image by the author.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: H2O LLM Studio 首页。图片由作者提供。
- en: Import dataset
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 导入数据集
- en: First, we have to import the dataset to be used to fine-tune an LLM. You can
    [download the one I used](https://github.com/tomasonjo/blog-datasets/tree/main/llm)
    if you don’t want to create your dataset. Note that it is not curated, and some
    examples do not fit the movie recommendation graph schema. However, it is a great
    start to getting to know the tool. We can import CSV files using the drag&drop
    interface.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们必须导入用于微调LLM的数据集。如果你不想创建自己的数据集，可以[下载我使用的那个](https://github.com/tomasonjo/blog-datasets/tree/main/llm)。请注意，它不是经过整理的，一些示例不符合电影推荐图表模式。然而，这是了解工具的一个很好的起点。我们可以使用拖放界面导入CSV文件。
- en: '![](../Images/148360e49f058b89064dc1a24c9b77b7.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/148360e49f058b89064dc1a24c9b77b7.png)'
- en: Upload CSV interface. Image by the author.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 上传CSV界面。图片由作者提供。
- en: It is a bit counter-intuitive, but we have to upload the training and validation
    sets separately. Let’s say we first upload the training set. Then, when we upload
    the validation set, we have to use the merge datasets option so that we have both
    the training and validation sets in the same dataset.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 这有点反直觉，但我们必须分别上传训练集和验证集。假设我们首先上传训练集。然后，当我们上传验证集时，我们必须使用合并数据集选项，以便在同一个数据集中包含训练集和验证集。
- en: '![](../Images/095a31992411b7a048574ee11da90542.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/095a31992411b7a048574ee11da90542.png)'
- en: Imported dataset with both train and validation dataframes. Image by the author.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 导入的数据集包含训练和验证数据框。图片由作者提供。
- en: The final dataset should have both training and validation dataframes present.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 最终的数据集应该包含训练和验证的数据框。
- en: '*I’ve learned you can also upload a ZIP file with both training and validation
    sets to avoid having to separately upload files.*'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '*我了解到，你还可以上传一个包含训练集和验证集的ZIP文件，以避免分别上传文件。*'
- en: Create experiment
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建实验
- en: 'Now that everything is ready, we can go ahead and fine-tune an LLM model. If
    we click on the **Create Experiment** tab, we will be presented with fine-tuning
    options. The most important setting to choose are the **dataset** used for training,
    the **LLM backbone**, and I have also increased the **epochs** count in my experiments.
    I have left the other parameters default as I have no idea what they do. We can
    choose from 13 LLM models:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 现在一切准备就绪，我们可以继续微调LLM模型。如果我们点击**创建实验**选项卡，就会看到微调选项。最重要的设置包括用于训练的**数据集**、**LLM骨干**，我在实验中也增加了**epochs**的数量。其他参数我保持默认，因为我不清楚它们的作用。我们可以从13个LLM模型中进行选择：
- en: '![](../Images/4e919257ce68a365da6ca6d7f5702b22.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4e919257ce68a365da6ca6d7f5702b22.png)'
- en: Available LLM models. Image by the author.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 可用的LLM模型。图片由作者提供。
- en: Note that the higher the parameter count, the more GPU RAM we require for finetuning
    and inference. For example, I ran out of memory using a 40GB GPU when trying to
    finetune an LLM model with 20B parameters. On the other hand, we expect that the
    higher the parameter count of an LLM, the better the results. I would say that
    we require about 5GB of GPU RAM for smaller LLMs like pythia-1b and up to 40GB
    GPU for opt-13b models. Once we set the desired parameters, we can run the experiment
    with a single click. For the most part, the finetuning process was relatively
    fast using an Nvidia A100 40GB.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，参数数量越高，我们需要的GPU RAM就越多用于微调和推理。例如，我在使用40GB GPU微调一个具有20B参数的LLM模型时内存不足。另一方面，我们期望LLM的参数数量越高，结果会更好。我认为，对于像pythia-1b这样的小型LLM，我们需要大约5GB的GPU
    RAM，而对于opt-13b模型则需要最多40GB的GPU。设置好所需参数后，我们可以通过单击一次来运行实验。在大多数情况下，使用Nvidia A100 40GB进行微调的过程相对较快。
- en: '![](../Images/7391b335e14c3f77a12fa4b650506023.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7391b335e14c3f77a12fa4b650506023.png)'
- en: Experiments page. Image by the author.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 实验页面。图片由作者提供。
- en: Most models were trained in less than 30 minutes using 15 epochs. The nice thing
    about the LLM Studio is that it produces a dashboard to inspect the training results.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数模型在使用15个epochs的情况下，训练时间不到30分钟。LLM Studio的一个优点是它会生成一个仪表板来检查训练结果。
- en: '![](../Images/c7bbcf06db55e7fb64c93fa6370c0772.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c7bbcf06db55e7fb64c93fa6370c0772.png)'
- en: LLM finetuning metrics. Image by the author.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: LLM微调指标。图片由作者提供。
- en: Not only that, but we can also chat with the model in the graphical interface.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 不仅如此，我们还可以在图形界面中与模型进行聊天。
- en: '![](../Images/31caa0c1c969de4e725d602f38655f90.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/31caa0c1c969de4e725d602f38655f90.png)'
- en: Chat interface in the LLM Studio. Image by the author.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: LLM Studio中的聊天界面。图片由作者提供。
- en: Export models to HuggingFace repository
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将模型导出到HuggingFace仓库
- en: It’s as if the H2O LLM Studio wasn’t cool enough, it also allows to export finetuned
    models to HuggingFace with a single click.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 就在H2O LLM Studio已经够酷的情况下，它还允许通过单击将微调过的模型导出到HuggingFace。
- en: '![](../Images/34697e1772359ac572b2e68d402ceaa9.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/34697e1772359ac572b2e68d402ceaa9.png)'
- en: Export models to HuggingFace. Image by the author.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 将模型导出到HuggingFace。图片由作者提供。
- en: The ability to export a model to the HuggingFace repository with a single click
    allows us to use the model anywhere in our workflows as easily as possible. I
    have exported a small finetuned pythia-1b model that can run in Google Colab to
    demonstrate how to use it with the transformers library.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 能够通过单击将模型导出到HuggingFace库，使我们可以在工作流中尽可能轻松地使用该模型。我已经导出了一种可以在Google Colab中运行的小型微调pythia-1b模型，以演示如何与transformers库一起使用。
- en: '[PRE0]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The LLM Studio uses a special `<|endoftext|>`character that must be added to
    the end of the user prompt in order for the model to work correctly. Therefore,
    we must do the same when using the finetuned model with the transformers library.
    Other than that, there is nothing really that needs to be done. We can now use
    the model to generate Cypher statements.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: LLM Studio使用一个特殊的`<|endoftext|>`字符，该字符必须添加到用户提示的末尾，以确保模型正确运行。因此，在使用调整过的模型时，我们也必须在transformers库中执行相同操作。除此之外，实际上没有其他需要做的事情。我们现在可以使用模型生成Cypher语句。
- en: '[PRE1]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: I deliberately showed one valid and one invalid Cypher statement generated to
    show that the smaller models might be good enough for demos, where the prompts
    can be predefined. On the other hand, you probably wouldn’t want to use them in
    production. However, using bigger models comes with a price. For example, to run
    models with 12B parameters, we need at least 24 GB GPU, while the 20B parameter
    models require GPUs with 48 GB.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 我故意展示了一个有效和一个无效的Cypher语句，以说明较小的模型可能足够用于演示，其中提示可以预定义。另一方面，你可能不愿意在生产中使用它们。然而，使用更大的模型会有代价。例如，要运行具有12B参数的模型，我们至少需要24
    GB的GPU，而20B参数模型则需要48 GB的GPU。
- en: Summary
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: Finetuning open-source LLMs allows us to break free of the OpenAI dependency.
    Although GPT-4 works better, especially in a conversational setting where follow-up
    questions could be asked, we can still keep our top-secret data to ourselves.
    I tested multiple models while writing this blog post, except for 20B models,
    due to GPU memory issues. I can confidently say that you could finetune a model
    to generate Cypher statements good enough for a production setting. One thing
    to note is that follow-up questions, where the model has to rely on previous dialogue
    to understand the context of the question, don’t seem to be functioning at the
    moment. Therefore, we are limited to single-step queries, where we need to provide
    the whole context in a single prompt. However, since the development of open-source
    LLMs is exploding, I am excited about what’s to come next.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 微调开源LLMs使我们能够摆脱对OpenAI的依赖。虽然GPT-4表现更好，特别是在需要提问的对话环境中，但我们仍然可以将我们的绝密数据保留给自己。在写这篇博客文章时，我测试了多个模型，除了由于GPU内存问题而没有测试20B模型。我可以自信地说，你可以微调一个模型以生成足够好的Cypher语句用于生产设置。需要注意的一点是，涉及到模型需要依赖前面对话来理解问题背景的后续问题，目前似乎无法正常工作。因此，我们被限制在单步查询中，需要在单个提示中提供完整的上下文。然而，随着开源LLMs的发展迅猛，我对未来的发展充满期待。
- en: Till then, try out the [H2O LLM Studio](https://github.com/h2oai/h2o-llmstudio)
    if you want to finetune an LLM to fit your personal or company’s needs with only
    a few mouse clicks.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 直到那时，如果你想通过仅需几次点击就能调整一个LLM以适应个人或公司的需求，可以试试[H2O LLM Studio](https://github.com/h2oai/h2o-llmstudio)。
