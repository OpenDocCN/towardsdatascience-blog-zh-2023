- en: Poems, Flowers, and Dragons at EMNLP 2022
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/poems-flowers-and-dragons-at-emnlp-2022-e83dbb0e91db?source=collection_archive---------13-----------------------#2023-01-02](https://towardsdatascience.com/poems-flowers-and-dragons-at-emnlp-2022-e83dbb0e91db?source=collection_archive---------13-----------------------#2023-01-02)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[](https://medium.com/@phoenixilya?source=post_page-----e83dbb0e91db--------------------------------)[![Ilya
    Gusev](../Images/f6b6cee7d3bd208c0731b784ea4fc0c6.png)](https://medium.com/@phoenixilya?source=post_page-----e83dbb0e91db--------------------------------)[](https://towardsdatascience.com/?source=post_page-----e83dbb0e91db--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----e83dbb0e91db--------------------------------)
    [Ilya Gusev](https://medium.com/@phoenixilya?source=post_page-----e83dbb0e91db--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F99b43d56e4b6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpoems-flowers-and-dragons-at-emnlp-2022-e83dbb0e91db&user=Ilya+Gusev&userId=99b43d56e4b6&source=post_page-99b43d56e4b6----e83dbb0e91db---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----e83dbb0e91db--------------------------------)
    ·8 min read·Jan 2, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fe83dbb0e91db&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpoems-flowers-and-dragons-at-emnlp-2022-e83dbb0e91db&user=Ilya+Gusev&userId=99b43d56e4b6&source=-----e83dbb0e91db---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fe83dbb0e91db&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpoems-flowers-and-dragons-at-emnlp-2022-e83dbb0e91db&source=-----e83dbb0e91db---------------------bookmark_footer-----------)![](../Images/bfd6c85917e82ed35248b178b1138ab4.png)'
  prefs: []
  type: TYPE_NORMAL
- en: “poems, flowers, dungeons and dragons united, digital art — ar 3:2 — v 4”, Midjourney
  prefs: []
  type: TYPE_NORMAL
- en: The EMNLP conference is a highly regarded event in the field of natural language
    processing, where researchers come together to share and discuss the latest findings
    in the field. This year’s conference took place from December 7th to December
    11th in Abu Dhabi. Of the many papers presented at the conference, I wanted to
    highlight three that stood out to me. These papers may not necessarily be the
    most practical or well-known, but I believe they are worth mentioning. Two papers
    were presented as posters, while the third was a full talk. My favorite of the
    three is PoeLM.
  prefs: []
  type: TYPE_NORMAL
- en: 'PoeLM: A Meter- and Rhyme-Controllable Language Model for Unsupervised Poetry
    Generation'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Paper**: [Ormazabal et al., 2022](https://arxiv.org/abs/2205.12206)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Organizations**: University of the Basque Country, Meta AI, University of
    Copenhagen'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Code**: [https://github.com/aitorormazabal/poetry_generation](https://github.com/aitorormazabal/poetry_generation),
    though there is only dataset creation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Main idea**: Generating Spanish and Basque formal verse poems through control
    codes with a language model trained on non-poetic texts.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Motivation**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Can modern language models write poems? Of course, they can. You can quickly
    test it with [ChatGPT](https://chat.openai.com/chat). The challenges arise when
    trying to impose specific constraints, such as a fixed number of syllables or
    a specific rhyme or rhythm scheme.
  prefs: []
  type: TYPE_NORMAL
- en: How can we force language models to generate formal verse poems? One way is
    to modify the decoding algorithm, which is complicated with modern language models
    as they operate with sub-words, which are neither words nor syllables. This paper
    describes another way to do it. For this to work, you will need a regular text
    corpus and a system capable of analyzing syllables and rhymes.
  prefs: []
  type: TYPE_NORMAL
- en: Training a language model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/2b9e1894de81d349cd9dfe3d2174d3d8.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure from the paper, a proposed method.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is what you need to do:'
  prefs: []
  type: TYPE_NORMAL
- en: Get a regular, non-poetic corpus, and split it into phrases.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Group the text in blocks of N phrases, where N is randomly sampled.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Augment groups with structure descriptors (=prefixes) to include the number
    of syllables and rhyme endings for each phrase.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Train a classic transformer language model with structure descriptors treated
    as ordinary tokens.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](../Images/4706bb3972c9984ed6593fe904d5aa7a.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure from the paper. A formal verse poem and its associated structure descriptor.
  prefs: []
  type: TYPE_NORMAL
- en: A structure descriptor from the figure above is
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: This descriptor means four lines; each has 11 syllables; the first and last
    lines end with “echo”, and lines 2 and 3 end with “ura”. The model will learn
    how to use these codes, as generating texts using such hints is easier than without
    them.
  prefs: []
  type: TYPE_NORMAL
- en: Generation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Choose a rhyming scheme and number of syllables.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Generate a structure descriptor. Authors do it from the given scheme by sampling
    each rhyming sound independently from the training corpus’s five most common rhyme
    sounds.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Provide the first line of a poem (optionally)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Generate a lot of poem candidates using the trained language model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Filter all candidates that do not fit the rhyming scheme or contain an incorrect
    number of syllables.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Re-rank remaining candidates by general fluency using the trained language model
    without a structure descriptor and output the one with the highest score.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How well does it work?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/86ed9e5dfa2c81e1ca4e2e1f0a143066.png)'
  prefs: []
  type: TYPE_IMG
- en: Table from the paper. Percentage of times that system S1 is ranked ahead of
    S2 in the human evaluation.
  prefs: []
  type: TYPE_NORMAL
- en: The filtering rate from step 5 is 30.9% for Spanish poems and 23.4% for Basque
    poems. 37.3% of humans prefer automatic poems over those written by renowned poets
    comparing poems with the same first line.
  prefs: []
  type: TYPE_NORMAL
- en: Can you do the same in your language?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A reliable syllabication and rhyme detection process are necessary to use the
    described algorithm. While such programs may already exist for some languages,
    other languages may have more complex features, such as rhythm, that need to be
    considered. The structure descriptors can be modified in these cases to include
    additional components.
  prefs: []
  type: TYPE_NORMAL
- en: Why is it important to me?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Six years ago, Daniil Anastasyev and I developed a system for the Russian poem
    generation, [rupo](https://github.com/IlyaGusev/rupo). It was an LSTM-based language
    model with some unique features: it predicted texts from right to left, separately
    using normal forms of words and their grammatical features, and it was based on
    finite-state acceptors. Since then, natural language processing technologies have
    advanced significantly, making it likely easier to create a similar system today.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Draw Me a Flower: Processing and Grounding Abstraction in Natural Language'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Paper**: [Lachmy et al., 2022](https://arxiv.org/abs/2106.14321)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Organizations**: Bar-Ilan University, AI2'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Code**: [https://github.com/OnlpLab/Hexagons](https://github.com/OnlpLab/Hexagons),
    but there are no baselines yet, only the dataset itself.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Main idea**: Creating a benchmark for grounded abstractions in natural language
    with instruction-based pattern drawing on a hexagonal grid.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/caa75894bd38530ef82f6b8da0b6c514.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure from the paper, levels of abstraction in natural language
  prefs: []
  type: TYPE_NORMAL
- en: '**Motivation**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We know large language models [can’t count correctly](https://arxiv.org/pdf/2210.17517.pdf)
    or perform [back-of-the-envelope calculations](https://aclanthology.org/2021.emnlp-main.582.pdf).
    Even [a simple spatial reasoning](https://github.com/google/BIG-bench/tree/main/bigbench/benchmark_tasks/navigate)
    task is a problem ([chain-of-thought](https://arxiv.org/pdf/2210.09261.pdf) helps,
    though). But what about abstraction? When you command your hypothetical AI assistant,
    “order three pizzas, one BBQ, one Pepperoni, and one Margherita, first two large,
    the last medium, at 5 pm”, it should be able to understand you. It’s not only
    about ellipsis but also conditions, iterations, functional decomposition, recursion,
    and other mechanisms.
  prefs: []
  type: TYPE_NORMAL
- en: To measure the extent to which a model can grasp abstract concepts, we can ground
    it in [various](https://en.wikipedia.org/wiki/SHRDLU) [virtual](https://aclanthology.org/2020.acl-main.232/)
    [worlds](https://arxiv.org/abs/2106.00188). In this case, the authors used a hexagonal
    board with 10x18 tiles and eight colors as the basis for grounding abstractions.
  prefs: []
  type: TYPE_NORMAL
- en: '**Dataset**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The dataset for this study was gathered through crowd-sourcing efforts. While
    the authors provided the starting images, crowd workers also contributed by drawing
    additional patterns. The annotation process was divided into two phases: in the
    first phase, a group of annotators wrote instructions based on the images, and
    in the second phase, another group attempted to recreate the images based on the
    instructions. Any discrepancies or disagreements were resolved through manual
    inspection. The resulting dataset has 175 unique images, 620 instruction sets,
    and 4177 instruction steps.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c01560bdcb5e62f69aea21d43adf3bd4.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure from the paper, a gallery sample.
  prefs: []
  type: TYPE_NORMAL
- en: '**Experiments**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Two types of models were tested: classification and generation-based. DeBERTa
    was used for the classification to predict every tile’s state. For the generation,
    T5 was used to generate a set of actions. The models were tested under various
    settings that varied in terms of the amount of history and current board information
    available to them: no history, one previous step, full history, predicted board,
    and oracle board. The results indicate that the models performed significantly
    worse than humans and could only handle the most basic abstractions, even with
    access to an oracle board and full history.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1ed33a694e851e0b5e806278d9e5196c.png)'
  prefs: []
  type: TYPE_IMG
- en: Table from the paper. Results for both types of models on the test set, actions-based
    metrics.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e62af6758b619f747658f1966e1f66d4.png)'
  prefs: []
  type: TYPE_IMG
- en: Table from the paper. Dataset evaluation, human performance.
  prefs: []
  type: TYPE_NORMAL
- en: '**Why is it important?**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: It is a great visual representation of how challenging this problem is for natural
    language models. This benchmark makes it possible to identify which abstraction
    mechanisms are lacking in these models quickly. I suspect code-based models [would](https://arxiv.org/pdf/2210.09261.pdf)
    perform better in this task and am interested in testing this hypothesis.
  prefs: []
  type: TYPE_NORMAL
- en: Dungeons and Dragons as a Dialog Challenge for Artificial Intelligence
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Paper**: [Callison-Burch et al., 2022](https://arxiv.org/pdf/2210.07109.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Organizations**: University of Pennsylvania, Google Research'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Code**: not yet released, should be [here](https://www.cis.upenn.edu/~ccb/dnd-data.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Main idea**: Creating a challenge for dialogue systems based on D&D conversations,
    where the tasks are to generate the next conversational turn in the game and predict
    the state of the game, given the dialogue history.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/252309de0d7d010f49ed25dc0dcd03f6.png)'
  prefs: []
  type: TYPE_IMG
- en: “robots playing D&D, digital art, futuristic — ar 3:2 — v 4”, Midjourney
  prefs: []
  type: TYPE_NORMAL
- en: '**Motivation**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*Dungeons & Dragons is a fantasy tabletop role-playing game. Characters embark
    upon adventures within a fantasy setting. A Dungeon Master serves as the game’s
    referee and storyteller while maintaining the setting in which the adventures
    occur, and playing the role of the game world’s inhabitants, also referred to
    as non-player characters (NPCs). The characters form a party and interact with
    the setting’s inhabitants and each other. Together they solve dilemmas, engage
    in battles, explore, and gather treasure and knowledge. In the process, the characters
    earn experience points to rise in levels and become increasingly powerful over
    a series of separate gaming sessions. —* [*Wikipedia*](https://en.wikipedia.org/wiki/Dungeons_%26_Dragons)'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Many natural language processing datasets are highly specialized, focusing on
    a specific task. Dungeons and Dragons (D&D) is a human activity that requires
    a high level of language comprehension from all participants. It involves a range
    of skills such as text generation, knowledge base lookup, multi-party dialogue,
    goal setting, common sense reasoning, intent detection, state tracking, and question
    answering, making it an ideal testbed for evaluating the capabilities of NLP models.
  prefs: []
  type: TYPE_NORMAL
- en: Other applications of AI for D&D include [character photo creation](https://www.reddit.com/r/StableDiffusion/comments/yskhce/my_new_dd_model_trained_for_30000_steps_on_2500/)
    and, of course, the famous [AI Dungeon](https://play.aidungeon.io/main/home).
  prefs: []
  type: TYPE_NORMAL
- en: '**Dataset**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/dfa1d9b3291c840b852879194e0115af.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure from the paper. Example of 3 turns in the D&D Beyond play-by-post forum.
  prefs: []
  type: TYPE_NORMAL
- en: Authors scraped Play-By-Post data from the D&D Beyond web forum, where people
    play by taking turns posting on the forum to describe their moves. It isn’t the
    only possible source for D&D sessions. For instance, [the CRD3 dataset](https://aclanthology.org/2020.acl-main.459.pdf)
    used transcripts from the Critical Role show.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/cce369b1c375586b380c1aafc21f022b.png)'
  prefs: []
  type: TYPE_IMG
- en: Table from the paper, dataset statistics.
  prefs: []
  type: TYPE_NORMAL
- en: Rule-based heuristics were used to extract game state information from texts
    using regular expressions and NER. In addition, a CNN classifier for texts was
    used in cases where heuristics failed to extract anything. The dataset includes
    not only in-character texts but also out-of-character posts.
  prefs: []
  type: TYPE_NORMAL
- en: '**Experiments**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'LaMDA, Google’s large language model similar to GPT-3, was used to tackle two
    tasks: game state tracking and response generation. The authors experimented with
    various fine-tuning variations of the model, including using states from the current
    or previous turns as control features. To evaluate the model’s performance, six
    professional raters interested in the fantasy genre and prior experience with
    D&D, including three who had served as Dungeon Masters, were recruited for a manual
    assessment.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d5bc8d152a0a423b146e26757acfe3aa.png)'
  prefs: []
  type: TYPE_IMG
- en: Table from the paper. Average human evaluators’ scores for systems and human-written
    gold responses.
  prefs: []
  type: TYPE_NORMAL
- en: The evaluation results show that domain adaptation is beneficial, but the impact
    of control features could be clearer. However, these features enable the model
    to take on specific roles within the game, which could make it a valuable substitute
    for a Dungeon Master or a player in actual D&D games.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1ef0592e0de0d6e335543d4953d69d5e.png)'
  prefs: []
  type: TYPE_IMG
- en: Table from the paper. Average accuracy for GST compared to a majority class
    baseline.
  prefs: []
  type: TYPE_NORMAL
- en: The results for the game state tracking task could have been better. The model
    was fed all previous dialog turns and their corresponding state variables, as
    well as the text of the current turn, and was expected to output the correct state
    variables for the current turn. The joint accuracy for the model was 58%. These
    results suggest that the use of a large language model alone is not sufficient
    for this task and that further modifications may be necessary to improve performance.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In conclusion, the research and findings discussed above highlight the ongoing
    challenges and areas for improvement. It is essential to consider the value of
    non-mainstream papers, as they may offer unique insights and approaches that could
    be overlooked in a rush to keep up with more widely recognized works.
  prefs: []
  type: TYPE_NORMAL
