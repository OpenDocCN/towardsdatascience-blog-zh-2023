["```py\nimport os\nimport json\nimport joblib\nimport torch\nfrom PIL import Image\nimport numpy as np\nimport io\nimport boto3\nfrom enum import Enum\nfrom urllib.parse import urlsplit\nfrom omegaconf import OmegaConf\nfrom anomalib.data.utils import read_image, InputNormalizationMethod, get_transforms\nfrom anomalib.models.ai_vad.torch_model import AiVadModel\n\ndevice = \"cuda\"\n\nclass PredictMode(Enum):\n    frame = 1\n    batch = 2\n    clip = 3\n\ndef model_fn(model_dir):\n    \"\"\"\n    This function is the first to get executed upon a prediction request,\n    it loads the model from the disk and returns the model object which will be used later for inference.\n    \"\"\"\n\n    # Load the config file\n    config = OmegaConf.load(os.path.join(model_dir, \"ai_vad_config.yaml\"))\n    config_model = config.model\n\n    # Load the model\n    model = AiVadModel(\n            box_score_thresh=config_model.box_score_thresh,\n            persons_only=config_model.persons_only,\n            min_bbox_area=config_model.min_bbox_area,\n            max_bbox_overlap=config_model.max_bbox_overlap,\n            enable_foreground_detections=config_model.enable_foreground_detections,\n            foreground_kernel_size=config_model.foreground_kernel_size,\n            foreground_binary_threshold=config_model.foreground_binary_threshold,\n            n_velocity_bins=config_model.n_velocity_bins,\n            use_velocity_features=config_model.use_velocity_features,\n            use_pose_features=config_model.use_pose_features,\n            use_deep_features=config_model.use_deep_features,\n            n_components_velocity=config_model.n_components_velocity,\n            n_neighbors_pose=config_model.n_neighbors_pose,\n            n_neighbors_deep=config_model.n_neighbors_deep,\n        )\n\n    # Load the model weights\n    model.load_state_dict(torch.load(os.path.join(model_dir, \"ai_vad_weights.pth\"), map_location=device), strict=False)\n\n    # Load the memory banks\n    velocity_estimator_memory_bank, pose_estimator_memory_bank, appearance_estimator_memory_bank = joblib.load(os.path.join(model_dir, \"ai_vad_banks.joblib\")) \n    if velocity_estimator_memory_bank is not None:\n        model.density_estimator.velocity_estimator.memory_bank = velocity_estimator_memory_bank\n    if pose_estimator_memory_bank is not None:\n        model.density_estimator.pose_estimator.memory_bank = pose_estimator_memory_bank\n    if appearance_estimator_memory_bank is not None:\n        model.density_estimator.appearance_estimator.memory_bank = appearance_estimator_memory_bank\n    model.density_estimator.fit()\n\n    # Move the entire model to device\n    model = model.to(device)\n\n    # get the transforms\n    transform_config = config.dataset.transform_config.eval if \"transform_config\" in config.dataset.keys() else None\n    image_size = (config.dataset.image_size[0], config.dataset.image_size[1])\n    center_crop = config.dataset.get(\"center_crop\")\n    center_crop = tuple(center_crop) if center_crop is not None else None\n    normalization = InputNormalizationMethod(config.dataset.normalization)\n    transform = get_transforms(config=transform_config, image_size=image_size, center_crop=center_crop, normalization=normalization)\n\n    return model, transform\n\ndef input_fn(request_body, request_content_type):\n    \"\"\"\n    The request_body is passed in by SageMaker and the content type is passed in \n    via an HTTP header by the client (or caller).\n    \"\"\"\n\n    print(\"input_fn-----------------------\")\n\n    if request_content_type in (\"application/x-image\", \"image/x-image\"):\n        image = Image.open(io.BytesIO(request_body)).convert(\"RGB\")\n        numpy_array = np.array(image)\n        print(\"numpy_array.shape\", numpy_array.shape)\n        print(\"input_fn-----------------------\")\n        return [numpy_array], PredictMode.frame\n\n    elif request_content_type == \"application/json\":\n        request_body_json = json.loads(request_body)\n\n        s3_uris = request_body_json.get(\"images\", [])\n\n        if len(s3_uris) == 0:\n            raise ValueError(f\"Images is a required key and should contain at least a list of one S3 URI\")\n\n        s3 = boto3.client(\"s3\")\n        frame_paths = []\n        for s3_uri in s3_uris:\n            parsed_url = urlsplit(s3_uri)\n            bucket_name = parsed_url.netloc\n            object_key = parsed_url.path.lstrip('/')\n            local_frame_path = f\"/tmp/{s3_uri.replace('/', '_')}\"\n            # Download the frame from S3\n            s3.download_file(bucket_name, object_key, local_frame_path)\n            frame_paths.append(local_frame_path)\n\n        frames = np.stack([torch.Tensor(read_image(frame_path)) for frame_path in frame_paths], axis=0)\n\n        predict_mode = PredictMode.clip if request_body_json.get(\"clip\", False) else PredictMode.batch\n\n        print(\"frames.shape\", frames.shape)\n        print(\"predict_mode\", predict_mode)\n        print(\"input_fn-----------------------\")\n\n        return frames, predict_mode\n\n    # If the request_content_type is not as expected, raise an exception\n    raise ValueError(f\"Content type {request_content_type} is not supported\")\n\ndef predict_fn(input_data, model):\n    \"\"\"\n    This function takes in the input data and the model returned by the model_fn\n    It gets executed after the model_fn and its output is returned as the API response.\n    \"\"\"\n\n    print(\"predict_fn-----------------------\")\n\n    model, transform = model\n\n    frames, predict_mode = input_data\n\n    processed_data = {}\n    processed_data[\"image\"] = [transform(image=frame)[\"image\"] for frame in frames]\n    processed_data[\"image\"] = torch.stack(processed_data[\"image\"])\n\n    image = processed_data[\"image\"].to(device)\n\n    # Add one more dimension for a batch size of one clip\n    if predict_mode == PredictMode.clip:\n        image = image.unsqueeze(0)\n\n    print(\"image.shape\", image.shape)\n\n    model.eval()\n\n    with torch.no_grad():\n        boxes, anomaly_scores, image_scores = model(image)\n\n    print(\"boxes_len\", [len(b) for b in boxes])\n\n    processed_data[\"pred_boxes\"] = [box.int() for box in boxes]\n    processed_data[\"box_scores\"] = [score.to(device) for score in anomaly_scores]\n    processed_data[\"pred_scores\"] = torch.Tensor(image_scores).to(device)\n\n    print(\"predict_fn-----------------------\")\n\n    return processed_data\n\ndef output_fn(prediction, accept):\n    \"\"\"\n    Post-processing function for model predictions. It gets executed after the predict_fn.\n    \"\"\"\n\n    print(\"output_fn-----------------------\")\n\n    # Check if accept type is JSON\n    if accept != \"application/json\":\n        raise ValueError(f\"Accept type {accept} is not supported\")\n\n    # Convert PyTorch Tensors to lists so they can be JSON serializable\n    for key in prediction:\n        # If torch.Tensor convert it to list\n        if isinstance(prediction[key], torch.Tensor):\n            prediction[key] = prediction[key].tolist()\n        # If list, convert every tensor in the list\n        elif isinstance(prediction[key], list):\n            prediction[key] = [tensor.tolist() if isinstance(tensor, torch.Tensor) else tensor for tensor in prediction[key]]\n\n    print(\"output_fn-----------------------\")\n\n    return json.dumps(prediction), accept\n```", "```py\nimport json\nfrom inference import model_fn, predict_fn, input_fn, output_fn\n\nresponse, accept = output_fn(\n    predict_fn(\n        input_fn(payload, \"application/x-image\"),\n        model_fn(\"../\")\n    ),\n    \"application/json\"\n)\njson.loads(response).keys()\n```", "```py\nimport torch\nimport joblib\nimport shutil\n\ncheckpoint = \"results/ai_vad/ucsd/run/weights/lightning/model.ckpt\"\nconfig_path = \"results/ai_vad/ucsd/run/config.yaml\"\n\nmodel_weights = torch.load(checkpoint)\nmodel_state_dict = model_weights[\"state_dict\"]\n\ntorch.save(model_state_dict, \"../ai_vad_weights.pth\")\n\nvelocity_estimator_memory_bank = None\npose_estimator_memory_bank = None\nappearance_estimator_memory_bank = None\nif \"velocity_estimator_memory_bank\" in model_weights:\n    velocity_estimator_memory_bank = model_weights[\"velocity_estimator_memory_bank\"]\nif \"pose_estimator_memory_bank\" in model_weights:\n    pose_estimator_memory_bank = model_weights[\"pose_estimator_memory_bank\"]\nif \"appearance_estimator_memory_bank\" in model_weights:\n    appearance_estimator_memory_bank = model_weights[\"appearance_estimator_memory_bank\"]\nbanks = (velocity_estimator_memory_bank, pose_estimator_memory_bank, appearance_estimator_memory_bank)\n\njoblib.dump(banks, \"../ai_vad_banks.joblib\")\n\nshutil.copyfile(config_path, \"../ai_vad_config.yaml\")\n```", "```py\ntar -czvf ../ai_vad_model.tar.gz -C ../ ai_vad_weights.pth ai_vad_banks.joblib ai_vad_config.yaml inference.py\n```", "```py\nimport boto3\nfrom datetime import datetime\n\ncurrent_datetime = datetime.now().strftime('%Y-%m-%d-%H-%M-%S')\n\ns3 = boto3.resource('s3')\ns3.meta.client.upload_file(\"../ai_vad_model.tar.gz\", \"ai-vad\", f\"{current_datetime}/ai_vad_model.tar.gz\")\n```", "```py\nREGION=<my_aws_region>\nACCOUNT=<my_aws_account>\n\n# Authenticate Docker to an Amazon ECR registry\naws ecr get-login-password --region $REGION | docker login --username AWS --password-stdin <docker_registry_url>.dkr.ecr.$REGION.amazonaws.com\n\n# Loging to your private Amazon ECR registry\naws ecr get-login-password --region $REGION | docker login --username AWS --password-stdin $ACCOUNT.dkr.ecr.$REGION.amazonaws.com\n```", "```py\n# Use the SageMaker PyTorch image as the base image\nFROM <docker_registry_url>.dkr.ecr.<my_aws_region>.amazonaws.com/pytorch-inference:2.0.0-gpu-py310\n\n# Install the additional dependency\nRUN pip install \"git+https://github.com/hairozen/anomalib.git@ai-vad-inference-improvements\"\n```", "```py\ndocker build -t ai-vad-image .\n```", "```py\n# Create the AWS ECR repository\naws ecr create-repository --repository-name ai-vad-image\n\n# Tag the image\ndocker tag ai-vad-image:latest $ACCOUNT.dkr.ecr.$REGION.amazonaws.com/ai-vad-image:latest\n\n# Push the tagged image to the AWS ECR repository\ndocker push $ACCOUNT.dkr.ecr.$REGION.amazonaws.com/ai-vad-image:latest\n```", "```py\nimport boto3\nimport sagemaker\n\nsagemaker_client = boto3.client(service_name=\"sagemaker\")\nrole = sagemaker.get_execution_role()\n\nmodel_name = f\"ai-vad-model-{current_datetime}\"\n\nprimary_container = {\n    \"Image\": f\"{my_aws_account}.dkr.ecr.{my_aws_region}.amazonaws.com/ai-vad-image:latest\",\n    \"ModelDataUrl\": f\"s3://ai-vad/{current_datetime}/ai_vad_model.tar.gz\"\n}\n\ncreate_model_response = sagemaker_client.create_model(\n    ModelName=model_name,\n    ExecutionRoleArn=role,\n    PrimaryContainer=primary_container)\n```", "```py\nendpoint_config_name = f\"ai-vad-model-config-{current_datetime}\"\n\nsagemaker_client.create_endpoint_config(\n    EndpointConfigName=endpoint_config_name,\n    ProductionVariants=[{\n        \"InstanceType\": \"ml.g5.xlarge\",\n        \"InitialVariantWeight\": 1,\n        \"InitialInstanceCount\": 1,\n        \"ModelName\": model_name,\n        \"VariantName\": \"AllTraffic\"}])\n```", "```py\nendpoint_name = f\"ai-vad-model-endpoint-{current_datetime}\"\n\nsagemaker_client.create_endpoint(\n    EndpointName=endpoint_name,\n    EndpointConfigName=endpoint_config_name)\n```", "```py\nresponse = sagemaker_client.describe_endpoint(EndpointName=endpoint_name)\nresponse[\"EndpointStatus\"]\n```", "```py\nwith open(file_name, \"rb\") as f:\n    payload = f.read()\n\npredictor = sagemaker.predictor.Predictor(endpoint_name=endpoint_name)\npredictor.serializer = DataSerializer(content_type=\"image/x-image\")\npredictor.predict(payload)\n```", "```py\nwith open(file_name, \"rb\") as f:\n    payload = f.read()\n\nsagemaker_runtime = boto3.client(\"runtime.sagemaker\")\nresponse = sagemaker_runtime.invoke_endpoint(\n    EndpointName=endpoint_name,\n    ContentType=\"image/x-image\",\n    Body=payload\n)\n\nresponse = json.loads(response[\"Body\"].read().decode())\n```"]