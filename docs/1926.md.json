["```py\nx = torch.randn(8, 2)\n```", "```py\nxcs = F.cosine_similarity(x[None,:,:], x[:,None,:], dim=-1)\n```", "```py\neye = torch.eye(8)\n```", "```py\neye = eye.bool()\n```", "```py\ny = xcs.clone()\n```", "```py\ny[eye] = float(\"-inf\")\n```", "```py\ntarget = torch.arange(8)\ntarget[0::2] += 1\ntarget[1::2] -= 1\n```", "```py\nloss = F.cross_entropy(y / temperature, target, reduction=\"mean\")\n```", "```py\ndef nt_xent_loss(x, temperature):\n  assert len(x.size()) == 2\n\n  # Cosine similarity\n  xcs = F.cosine_similarity(x[None,:,:], x[:,None,:], dim=-1)\n  xcs[torch.eye(x.size(0)).bool()] = float(\"-inf\")\n\n  # Ground truth labels\n  target = torch.arange(8)\n  target[0::2] += 1\n  target[1::2] -= 1\n\n  # Standard cross-entropy loss\n  return F.cross_entropy(xcs / temperature, target, reduction=\"mean\")\n```", "```py\nx = torch.randn(8, 2)\n```", "```py\nxcs = F.cosine_similarity(x[None,:,:], x[:,None,:], dim=-1)\n```", "```py\neye = torch.eye(8).bool()\n```", "```py\ny = xcs.clone()\n```", "```py\ny[eye] = float(\"inf\")\n```", "```py\ntarget = torch.zeros(8, 8)\npos_indices = torch.tensor([\n  (0, 0), (0, 2), (0, 4),\n  (1, 4), (1, 6), (1, 1),\n  (2, 3),\n  (3, 7),\n  (4, 3),\n  (7, 6),\n])\n# Add indexes of the principal diagonal as positive indexes.\n# This will be useful since we will use the BCELoss in PyTorch,\n# which will expect a value for the elements on the principal\n# diagonal as well.\npos_indices = torch.cat([pos_indices, torch.arange(8).reshape(8, 1).expand(-1, 2)], dim=0)\n# Set the values in the target vector to 1.\ntarget[pos_indices[:,0], pos_indices[:,1]] = 1\n```", "```py\ntemperature = 0.1\nloss = F.binary_cross_entropy((y / temperature).sigmoid(), target, reduction=\"none\")\n```", "```py\ntarget_pos = target.bool()\ntarget_neg = ~target_pos\n# loss_pos and loss_neg below contain non-zero values only for those elements\n# that are positive pairs and negative pairs respectively.\nloss_pos = torch.zeros(x.size(0), x.size(0)).masked_scatter(target_pos, loss[target_pos])\nloss_neg = torch.zeros(x.size(0), x.size(0)).masked_scatter(target_neg, loss[target_neg])\n```", "```py\n# loss_pos and loss_neg now contain the sum of positive and negative pair losses\n# as computed relative to the i'th input.\nloss_pos = loss_pos.sum(dim=1)\nloss_neg = loss_neg.sum(dim=1)\n```", "```py\n# num_pos and num_neg below contain the number of positive and negative pairs\n# computed relative to the i'th input. In an actual setting, this number should\n# be the same for every input element, but we let it vary here for maximum\n# flexibility.\nnum_pos = target.sum(dim=1)\nnum_neg = target.size(0) - num_pos\n```", "```py\ndef nt_bxent_loss(x, pos_indices, temperature):\n    assert len(x.size()) == 2\n\n    # Add indexes of the principal diagonal elements to pos_indices\n    pos_indices = torch.cat([\n        pos_indices,\n        torch.arange(x.size(0)).reshape(x.size(0), 1).expand(-1, 2),\n    ], dim=0)\n\n    # Ground truth labels\n    target = torch.zeros(x.size(0), x.size(0))\n    target[pos_indices[:,0], pos_indices[:,1]] = 1.0\n\n    # Cosine similarity\n    xcs = F.cosine_similarity(x[None,:,:], x[:,None,:], dim=-1)\n    # Set logit of diagonal element to \"inf\" signifying complete\n    # correlation. sigmoid(inf) = 1.0 so this will work out nicely\n    # when computing the Binary cross-entropy Loss.\n    xcs[torch.eye(x.size(0)).bool()] = float(\"inf\")\n\n    # Standard binary cross-entropy loss. We use binary_cross_entropy() here and not\n    # binary_cross_entropy_with_logits() because of\n    # https://github.com/pytorch/pytorch/issues/102894\n    # The method *_with_logits() uses the log-sum-exp-trick, which causes inf and -inf values\n    # to result in a NaN result.\n    loss = F.binary_cross_entropy((xcs / temperature).sigmoid(), target, reduction=\"none\")\n\n    target_pos = target.bool()\n    target_neg = ~target_pos\n\n    loss_pos = torch.zeros(x.size(0), x.size(0)).masked_scatter(target_pos, loss[target_pos])\n    loss_neg = torch.zeros(x.size(0), x.size(0)).masked_scatter(target_neg, loss[target_neg])\n    loss_pos = loss_pos.sum(dim=1)\n    loss_neg = loss_neg.sum(dim=1)\n    num_pos = target.sum(dim=1)\n    num_neg = x.size(0) - num_pos\n\n    return ((loss_pos / num_pos) + (loss_neg / num_neg)).mean()\n\npos_indices = torch.tensor([\n    (0, 0), (0, 2), (0, 4),\n    (1, 4), (1, 6), (1, 1),\n    (2, 3),\n    (3, 7),\n    (4, 3),\n    (7, 6),\n])\nfor t in (0.01, 0.1, 1.0, 10.0, 20.0):\n    print(f\"Temperature: {t:5.2f}, Loss: {nt_bxent_loss(x, pos_indices, temperature=t)}\")\n```"]