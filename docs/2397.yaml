- en: 'Survival of the Fittest: Compact Generative AI Models Are the Future for Cost-Effective
    AI at Scale'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 适者生存：紧凑型生成式 AI 模型是规模化成本效益 AI 的未来
- en: 原文：[https://towardsdatascience.com/survival-of-the-fittest-compact-generative-ai-models-are-the-future-for-cost-effective-ai-at-scale-6bbdc138f618?source=collection_archive---------6-----------------------#2023-07-25](https://towardsdatascience.com/survival-of-the-fittest-compact-generative-ai-models-are-the-future-for-cost-effective-ai-at-scale-6bbdc138f618?source=collection_archive---------6-----------------------#2023-07-25)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/survival-of-the-fittest-compact-generative-ai-models-are-the-future-for-cost-effective-ai-at-scale-6bbdc138f618?source=collection_archive---------6-----------------------#2023-07-25](https://towardsdatascience.com/survival-of-the-fittest-compact-generative-ai-models-are-the-future-for-cost-effective-ai-at-scale-6bbdc138f618?source=collection_archive---------6-----------------------#2023-07-25)
- en: '![](../Images/66769e37c3188dde2545179de38baef1.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/66769e37c3188dde2545179de38baef1.png)'
- en: 'Image credit: Adobe Stock.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：Adobe Stock。
- en: The case for nimble, targeted, retrieval-based models as the best solution for
    generative AI applications deployed at scale.
  id: totrans-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 支持灵活、针对性强的检索式模型作为规模化部署生成式 AI 应用的最佳解决方案。
- en: '[](https://gadi-singer.medium.com/?source=post_page-----6bbdc138f618--------------------------------)[![Gadi
    Singer](../Images/293941f11306a6e2100c2375ccb1a85a.png)](https://gadi-singer.medium.com/?source=post_page-----6bbdc138f618--------------------------------)[](https://towardsdatascience.com/?source=post_page-----6bbdc138f618--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----6bbdc138f618--------------------------------)
    [Gadi Singer](https://gadi-singer.medium.com/?source=post_page-----6bbdc138f618--------------------------------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://gadi-singer.medium.com/?source=post_page-----6bbdc138f618--------------------------------)[![Gadi
    Singer](../Images/293941f11306a6e2100c2375ccb1a85a.png)](https://gadi-singer.medium.com/?source=post_page-----6bbdc138f618--------------------------------)[](https://towardsdatascience.com/?source=post_page-----6bbdc138f618--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----6bbdc138f618--------------------------------)
    [Gadi Singer](https://gadi-singer.medium.com/?source=post_page-----6bbdc138f618--------------------------------)'
- en: ·
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F51de1f48d0b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsurvival-of-the-fittest-compact-generative-ai-models-are-the-future-for-cost-effective-ai-at-scale-6bbdc138f618&user=Gadi+Singer&userId=51de1f48d0b&source=post_page-51de1f48d0b----6bbdc138f618---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----6bbdc138f618--------------------------------)
    ·18 min read·Jul 25, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F6bbdc138f618&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsurvival-of-the-fittest-compact-generative-ai-models-are-the-future-for-cost-effective-ai-at-scale-6bbdc138f618&user=Gadi+Singer&userId=51de1f48d0b&source=-----6bbdc138f618---------------------clap_footer-----------)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F51de1f48d0b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsurvival-of-the-fittest-compact-generative-ai-models-are-the-future-for-cost-effective-ai-at-scale-6bbdc138f618&user=Gadi+Singer&userId=51de1f48d0b&source=post_page-51de1f48d0b----6bbdc138f618---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----6bbdc138f618--------------------------------)
    ·18分钟阅读·2023年7月25日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F6bbdc138f618&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsurvival-of-the-fittest-compact-generative-ai-models-are-the-future-for-cost-effective-ai-at-scale-6bbdc138f618&user=Gadi+Singer&userId=51de1f48d0b&source=-----6bbdc138f618---------------------clap_footer-----------)'
- en: --
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F6bbdc138f618&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsurvival-of-the-fittest-compact-generative-ai-models-are-the-future-for-cost-effective-ai-at-scale-6bbdc138f618&source=-----6bbdc138f618---------------------bookmark_footer-----------)'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F6bbdc138f618&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsurvival-of-the-fittest-compact-generative-ai-models-are-the-future-for-cost-effective-ai-at-scale-6bbdc138f618&source=-----6bbdc138f618---------------------bookmark_footer-----------)'
- en: After a decade of rapid growth in artificial intelligence (AI) model complexity
    and compute, 2023 marks a shift in focus to efficiency and the broad application
    of generative AI (GenAI). As a result, a new crop of models with less than 15
    billion parameters, referred to as nimble AI, can closely match the capabilities
    of ChatGPT-style giant models containing more than 100B parameters, especially
    when targeted for particular domains. While GenAI is already being deployed throughout
    industries for a wide range of business usages, the use of compact, yet highly
    intelligent models, is rising. In the near future, I expect there will be a small
    number of giant modes and a giant number of small, more nimble AI models embedded
    in countless applications.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在人工智能 (AI) 模型复杂性和计算能力快速增长了十年之后，2023 年标志着对效率和生成 AI (GenAI) 广泛应用的关注转变。因此，一批参数少于
    150 亿的新型模型被称为灵活 AI，可以在特定领域中接近 ChatGPT 风格的大型模型（参数超过 1000 亿）的能力。尽管 GenAI 已经在各行业广泛应用于各种商业用途，但紧凑且智能的模型使用率正在上升。在不久的将来，我预计将会有少量的大型模型和大量的小型、更灵活的
    AI 模型嵌入到无数应用中。
- en: While there has been great progress with larger models, bigger is certainly
    not better when it comes to training and environmental costs. [TrendForce](https://www.trendforce.com/presscenter/news/20230301-11584.html)
    estimates that ChatGPT training alone for GPT-4 reportedly costs more than $100
    million, while nimble model pre-training costs are orders-of-magnitude lower (for
    example, quoted as [approximately $200,000](https://www.mosaicml.com/blog/mpt-7b)
    for MosaicML’s MPT-7B). Most of the compute costs occur during continuous inference
    execution, but this follows a similar challenge for larger models including expensive
    compute. Furthermore, giant models hosted on third-party environments raise security
    and privacy challenges. Nimble models are substantially cheaper to run and provide
    a host of additional benefits such as adaptability, hardware flexibility, integrability
    within larger applications, security and privacy, explainability, and more (see
    Figure 1). The perception that smaller models don’t perform as well as larger
    models is also changing. Smaller, targeted models are not less intelligent — they
    can provide equivalent or superior performance for business, consumer, and scientific
    domains, increasing their value while decreasing time and cost investment.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管较大的模型取得了巨大进展，但在训练和环境成本方面，大型模型未必更好。[TrendForce](https://www.trendforce.com/presscenter/news/20230301-11584.html)
    估计，仅 GPT-4 的 ChatGPT 训练费用就超过 1 亿美元，而灵活模型的预训练成本则低得多（例如，MosaicML 的 MPT-7B 的预训练费用约为
    [20 万美元](https://www.mosaicml.com/blog/mpt-7b)）。大部分计算成本发生在持续的推断执行过程中，但这与大型模型面临的类似挑战有关，包括高昂的计算费用。此外，托管在第三方环境中的大型模型会带来安全性和隐私问题。灵活模型的运行成本大大降低，并提供了额外的好处，如适应性、硬件灵活性、在更大应用中的集成性、安全性和隐私、可解释性等（见图
    1）。对较小模型表现不如大型模型的看法也在改变。较小的、针对性的模型并不缺乏智能——它们可以在商业、消费和科学领域提供等效或更优的性能，增加了其价值，同时减少了时间和成本投资。
- en: A growing number of these nimble models roughly match the performance of ChatGPT-3.5-level
    giant models and continue to rapidly improve in performance and scope. And, when
    nimble models are equipped with on-the-fly retrieval of curated domain-specific
    private data and targeted retrieval of web content based on a query, they become
    more accurate and more cost-effective than giant models that memorize a wide-ranging
    data set.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 越来越多的这些灵活模型大致匹配了 ChatGPT-3.5 级别的大型模型的性能，并且在性能和范围上持续快速提升。而且，当灵活模型配备了即时检索策划的特定领域私人数据和基于查询的网络内容有针对性的检索时，它们比记忆广泛数据集的大型模型更加准确且更具成本效益。
- en: '![](../Images/7f8528a11c60b20b547c0dccb0f1d6b0.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7f8528a11c60b20b547c0dccb0f1d6b0.png)'
- en: '*Figure 1\. Benefits of nimble GenAI models. Image credit: Intel Labs.*'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 1\. 灵活 GenAI 模型的好处。图片来源：Intel Labs。*'
- en: As nimble open source GenAI models step forward to drive the fast progression
    of the field, this “iPhone moment,” when a revolutionary technology becomes mainstream,
    is being challenged by an “Android revolution” as a strong community of researchers
    and developers build on each other’s open source efforts to create increasingly
    more capable nimble models.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 随着灵活的开源 GenAI 模型不断推进领域的快速发展，这一“iPhone 时刻”——当一种革命性技术变得主流——正受到“Android 革命”的挑战，因为一个强大的研究和开发社区在彼此的开源努力基础上进行构建，创造出越来越强大的灵活模型。
- en: '**Think, Do, Know: Nimble Models with Targeted Domains Can Perform Like Giant
    Models**'
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**思考、执行、了解：目标领域的灵活模型可以像巨型模型一样表现**'
- en: '![](../Images/c855b1209fdd0be813e213c2c2f11820.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c855b1209fdd0be813e213c2c2f11820.png)'
- en: '*Figure 2\. Generative AI classes of competencies. Image credit: Intel Labs.*'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 2\. 生成型人工智能能力分类。图像来源：Intel Labs。*'
- en: 'To gain more understanding of when and how a smaller model can deliver highly
    competitive results for generative AI, it is important to observe that both nimble
    and giant GenAI models need three classes of competencies to perform:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 要更深入地了解何时以及如何让较小的模型在生成型人工智能中提供高度竞争的结果，重要的是观察到，无论是灵活的还是巨型的生成型人工智能模型，都需要三类能力才能表现出色：
- en: '**Cognitive abilities to think:** Including language comprehension, summarization,
    reasoning, planning, learning from experience, long-form articulation and interactive
    dialog.'
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**认知能力以思考：** 包括语言理解、总结、推理、规划、从经验中学习、长篇阐述和互动对话。'
- en: '**Functional skills to do:** For example — reading text in the wild, reading
    charts/graphs, visual recognition, programming (coding and debug), image generation
    and speech.'
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**功能技能以执行：** 例如——在自然环境中阅读文本、阅读图表/图形、视觉识别、编程（编码和调试）、图像生成和语音。'
- en: '**Information (memorized or retrieved) to know:** Web content, including social
    media, news, research, and other general content, and/or curated domain-specific
    content such as medical, financial and enterprise data.'
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**信息（记忆或检索）以了解：** 网页内容，包括社交媒体、新闻、研究和其他一般内容，和/或策划的领域特定内容，如医学、金融和企业数据。'
- en: '**Cognitive abilities to think.** Based on its cognitive abilities, the model
    can “think” and understand, summarize, synthesize, reason, and compose language
    and other symbolic representations. Both nimble and giant models can perform well
    in these cognitive tasks and it is not clear that those core capabilities require
    massive model sizes. For example, nimble models like [Microsoft Research’s Orca](https://arxiv.org/pdf/2306.02707.pdf)
    are showing understanding, logic, and reasoning skills that already match or surpass
    those of ChatGPT on multiple benchmarks. Furthermore, Orca also demonstrates that
    reasoning skills can be distilled from larger models used as teachers. However,
    the current benchmarks used to evaluate cognitive skills of models are still rudimentary.
    Further research and benchmarking are required to validate that nimble models
    can be pre-trained or fine-tuned to fully match the “thinking” strength of giant
    models.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '**思考的认知能力。** 基于其认知能力，模型可以“思考”并理解、总结、综合、推理和构建语言及其他符号表示。无论是灵活模型还是巨型模型，在这些认知任务中表现良好，并且这些核心能力是否需要庞大的模型规模尚不清楚。例如，像[微软研究的
    Orca](https://arxiv.org/pdf/2306.02707.pdf)这样的灵活模型已经在多个基准测试中展示了与 ChatGPT 相匹配或超越的理解、逻辑和推理技能。此外，Orca
    还表明，推理技能可以从作为教师的大型模型中提炼出来。然而，目前用于评估模型认知技能的基准仍然很初级。需要进一步的研究和基准测试来验证灵活模型是否可以通过预训练或微调来完全匹配巨型模型的“思考”能力。'
- en: '**Functional skills to do.** Larger models are likely to have more functional
    skills and information given their general focus as all-in-one models. However,
    for most business usages, there is a particular range of functional skills needed
    for any application being deployed. A model used in a business application should
    have flexibility and headroom for growth and variation of use, but it rarely needs
    an unbounded set of functional skills. GPT-4 can generate text, code and images
    in multiple languages, but speaking hundreds of languages doesn’t necessarily
    mean that those giant models has inherently more underlying cognitive competencies
    — it primarily gives the model added functional skills to “do” more. Furthermore,
    functionally specialized engines will be linked to GenAI models and used when
    that functionality is needed — like [adding mathematical “Wolfram superpowers”
    to ChatGPT](https://writings.stephenwolfram.com/2023/03/chatgpt-gets-its-wolfram-superpowers/)
    modularly could provide best-in-class functionality without burdening the model
    with unnecessary scale. For example, [GPT-4 is deploying plugins](https://openai.com/blog/chatgpt-plugins)
    that are essentially utilizing smaller models for add-on functions. It’s also
    [rumored that GPT-4](https://the-decoder.com/gpt-4-architecture-datasets-costs-and-more-leaked/)
    model itself is a collection of multiple giant (less than 100B parameters) “mixture
    of experts” models trained on different data and task distributions rather than
    one monolithic dense model like GPT-3.5\. To get the best combination of capabilities
    and model efficiencies, it is likely that future multi-functional models might
    employ smaller, more focused mixture of experts models that are each smaller than
    15B parameters.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '**执行功能技能。** 较大的模型由于其作为全能模型的一般关注，可能具有更多的功能技能和信息。然而，对于大多数业务用途来说，每个应用程序所需的功能技能有特定的范围。用于业务应用的模型应具备灵活性和扩展性，但通常不需要无限的功能技能。GPT-4可以生成多种语言的文本、代码和图像，但说几百种语言并不一定意味着这些巨型模型本质上具有更多的认知能力——它主要是给模型增加了更多的“执行”功能技能。此外，功能专用引擎将与GenAI模型关联，并在需要该功能时使用——例如，[将数学“Wolfram超能力”添加到ChatGPT](https://writings.stephenwolfram.com/2023/03/chatgpt-gets-its-wolfram-superpowers/)
    模块化可以提供最佳的功能，而不会给模型带来不必要的规模负担。例如，[GPT-4正在部署插件](https://openai.com/blog/chatgpt-plugins)，这些插件实质上利用了较小的模型来提供附加功能。此外，[据传GPT-4](https://the-decoder.com/gpt-4-architecture-datasets-costs-and-more-leaked/)
    模型本身是由多个巨型（少于100B参数）“专家混合”模型组成，这些模型在不同的数据和任务分布上进行训练，而不是像GPT-3.5那样的单一密集模型。为了获得最佳的能力和模型效率组合，未来的多功能模型可能会使用每个小于15B参数的更小、更专注的专家混合模型。'
- en: '![](../Images/b36f9a6c61ec3238384c5dd081395944.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b36f9a6c61ec3238384c5dd081395944.png)'
- en: '*Figure 3\. Retrieval-based, functionally extended models can offer a broad
    scope of functionality and relevant information, largely independent of model
    size. Image credit: Intel Labs.*'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '*图3\. 基于检索的功能扩展模型可以提供广泛的功能和相关信息，与模型大小基本无关。图片来源：Intel Labs。*'
- en: '**Information (memorized or retrieved) to know.** Giant models “know” more
    by memorizing vast amounts of data within parametric memory, but it doesn’t necessarily
    make them smarter. They are just more generally knowledgeable than smaller models.
    Giant models have high value in zero-shot environments for new use cases, offering
    a general consumer base when there’s no need for targeting, and acting as a teacher
    model when distilling and fine-tuning nimble models like Orca. However, targeted
    nimble models can be trained and/or fine-tuned for particular domains, providing
    sharper skills for the capabilities needed.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '**需要了解的信息（记忆的或检索的）。** 巨型模型通过在参数记忆中记忆大量数据来“知道”更多内容，但这不一定使它们更聪明。它们只是比小模型更具一般知识。在零-shot环境下，巨型模型具有较高的价值，对于新用例提供了通用消费者基础，当不需要进行目标定位时，以及在提炼和微调灵活模型（如Orca）时作为教师模型。然而，目标明确的灵活模型可以为特定领域进行训练和/或微调，从而提供所需能力的更锐利技能。'
- en: '![](../Images/388a8d9b8e2297ec3d32e156e397897b.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/388a8d9b8e2297ec3d32e156e397897b.png)'
- en: '*Figure 4\. Value of retrieval in allowing small models to match much larger
    models (using the* [*Contriever*](https://arxiv.org/abs/2112.09118) *retrieval
    method). Image credit: Intel Labs based on the work of* [*Mallen et al*](https://doi.org/10.48550/arxiv.2212.10511)*.*'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '*图4\. 检索在允许小模型匹配更大模型的价值（使用* [*Contriever*](https://arxiv.org/abs/2112.09118)
    *检索方法）。图片来源：Intel Labs，基于* [*Mallen et al*](https://doi.org/10.48550/arxiv.2212.10511)*的工作。*'
- en: For example, a model that is targeted for programming can focus on a different
    set of capabilities than a healthcare AI system. Furthermore, by using retrieval
    over a curated set of internal and external data, the accuracy and timeliness
    of the model can be greatly improved. A [recent study](https://arxiv.org/abs/2212.10511)
    showed that on the [PopQA benchmark](https://paperswithcode.com/dataset/popqa),
    models as small as 1.3B parameters with retrieval can perform as well as a model
    more than hundred times their size at 175B parameters (see Figure 4). In that
    sense, the relevant knowledge of a targeted system with high-quality indexed accessible
    data may be much more extensive than an all-in-one general-purpose system. This
    may be more important for the majority of enterprise applications that require
    use-case or application-specific data — and in many instances, local knowledge
    instead of vast general knowledge. This is where the value of nimble models will
    be realized moving forward.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，一个针对编程的模型可以专注于与医疗 AI 系统不同的能力集。此外，通过使用针对内部和外部数据的检索，模型的准确性和时效性可以大大提高。[最近的一项研究](https://arxiv.org/abs/2212.10511)
    显示，在[PopQA 基准](https://paperswithcode.com/dataset/popqa)上，参数只有 1.3B 的模型通过检索可以与参数高达
    175B 的模型表现相当（见图 4）。在这种意义上，具有高质量索引的可访问数据的针对性系统的相关知识可能远比全能的通用系统更广泛。这对于需要用例或应用程序特定数据的多数企业应用程序来说可能更为重要——在许多情况下，还需要本地知识而不是广泛的通用知识。这就是灵活模型未来将显现其价值的地方。
- en: '**Three Aspects Contributing to the Explosive Growth in Nimble Models**'
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**三大因素推动灵活模型的爆炸性增长**'
- en: 'There are three aspects to consider when assessing the benefits and value of
    nimble models:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 评估灵活模型的好处和价值时需要考虑三个方面：
- en: High efficiency at modest model sizes.
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 中等模型尺寸的高效率。
- en: Licensing as open source or proprietary.
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 开源或专有的许可。
- en: Model specialization as general purpose or targeted including retrieval.
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 模型的专门化包括通用或针对性检索。
- en: In terms of size, nimble general-purpose models, such as [Meta’s LLaMA-7B and
    -13B](https://ai.facebook.com/blog/large-language-model-llama-meta-ai/) or [Technology
    Innovation Institute’s Falcon 7B](https://falconllm.tii.ae/) open source models,
    and proprietary models such as [MosaicML’s MPT-7B](https://www.mosaicml.com/blog/mpt-7b),
    [Microsoft Research’s Orca-13B](https://arxiv.org/pdf/2306.02707.pdf) and [Saleforce
    AI Research’s XGen-7B](https://blog.salesforceairesearch.com/xgen/) are improving
    in rapid succession (see Figure 6). Having a choice of high-performance, smaller
    models has significant implications for the cost of operation as well as the choice
    of compute environments. ChatGPT’s 175B parameter model and the [estimated 1.8
    trillion parameters for GPT-4](https://the-decoder.com/gpt-4-architecture-datasets-costs-and-more-leaked/)
    require a massive installation of accelerators such as GPUs with enough compute
    power to handle the training and fine-tuning workload. In contrast, nimble models
    can generally run inference on any choice of hardware, anywhere from a single
    socket CPU, through entry-level GPUs, and up to the largest acceleration racks.
    The definition of nimble AI has been currently set to 15B parameters empirically
    based on the outstanding results of models sized at 13B parameters or smaller.
    Overall, nimble models offer a more cost-effective and scalable approach to handling
    new use cases (see the section on advantages and disadvantages of nimble models).
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在尺寸方面，灵活的通用模型，如[Meta 的 LLaMA-7B 和 -13B](https://ai.facebook.com/blog/large-language-model-llama-meta-ai/)
    或 [技术创新研究所的 Falcon 7B](https://falconllm.tii.ae/) 开源模型，以及专有模型如[MosaicML 的 MPT-7B](https://www.mosaicml.com/blog/mpt-7b)、[微软研究院的
    Orca-13B](https://arxiv.org/pdf/2306.02707.pdf) 和 [Salesforce AI Research 的 XGen-7B](https://blog.salesforceairesearch.com/xgen/)
    正在迅速改进（见图 6）。选择高性能的小型模型对操作成本以及计算环境的选择有重大影响。ChatGPT 的 175B 参数模型和 [GPT-4 的估计 1.8
    万亿参数](https://the-decoder.com/gpt-4-architecture-datasets-costs-and-more-leaked/)
    需要大量的加速器安装，如具有足够计算能力的 GPU 以处理训练和微调工作负载。相比之下，灵活的模型通常可以在任何选择的硬件上运行推理，从单插槽 CPU，到入门级
    GPU，再到最大加速机架。灵活 AI 的定义目前已基于 13B 参数或更小模型的出色结果经验性地设定为 15B 参数。总体而言，灵活模型提供了一种更具成本效益和可扩展性的方法来处理新的用例（见灵活模型的优缺点部分）。
- en: The second aspect of open source licensing allows universities and companies
    to iterate on one another’s models, driving a boom of creative innovations. Open
    source models allow for the incredible progress of small model capabilities as
    demonstrated in Figure 5.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 开源许可的第二个方面允许大学和公司互相迭代模型，从而推动了创造性创新的蓬勃发展。开源模型允许小型模型能力的惊人进步，如图5所示。
- en: '![](../Images/7e5a777ec4963b416f03098c6e1907e3.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7e5a777ec4963b416f03098c6e1907e3.png)'
- en: '*Figure 5\. Nimble open source non-commercial and commercial GenAI models blast
    off during the first half of 2023\. Image credit: Intel Labs.*'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '*图5\. 灵活的开源非商业和商业GenAI模型在2023年上半年迅速崛起。图片来源：英特尔实验室。*'
- en: 'There are multiple examples from early 2023 of general nimble generative AI
    models starting with [LLaMA from Meta](https://ai.facebook.com/blog/large-language-model-llama-meta-ai/),
    which has models with 7B, 13B, 33B, and 65B parameters. The following models in
    the 7B and 13B parameters range were created by fine-tuning LLaMA: [Alpaca](https://crfm.stanford.edu/2023/03/13/alpaca.html)
    from Stanford University, [Koala](https://bair.berkeley.edu/blog/2023/04/03/koala/)
    from Berkeley AI Research, and [Vicuna](https://lmsys.org/blog/2023-03-30-vicuna/)
    created by researchers from UC Berkeley, Carnegie Mellon University, Stanford,
    UC San Diego, and MBZUAI. Recently, Microsoft Research published a [paper on the
    not-yet-released Orca](https://arxiv.org/pdf/2306.02707.pdf), a 13B parameter
    LLaMA-based model that imitates the reasoning process of giant models with impressive
    results prior to targeting or fine-tuning to a particular domain.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 从2023年初开始，有多个例子显示了通用灵活生成性AI模型的出现，比如[Meta的LLaMA](https://ai.facebook.com/blog/large-language-model-llama-meta-ai/)，该模型包括7B、13B、33B和65B参数。以下在7B和13B参数范围内的模型是通过微调LLaMA创建的：斯坦福大学的[Alpaca](https://crfm.stanford.edu/2023/03/13/alpaca.html)，伯克利人工智能研究所的[Koala](https://bair.berkeley.edu/blog/2023/04/03/koala/)，以及由加州大学伯克利分校、卡内基梅隆大学、斯坦福大学、加州大学圣地亚哥分校和MBZUAI的研究人员创建的[Vicuna](https://lmsys.org/blog/2023-03-30-vicuna/)。最近，微软研究院发布了一篇关于尚未发布的Orca的[论文](https://arxiv.org/pdf/2306.02707.pdf)，这是一个基于LLaMA的13B参数模型，模拟了大型模型的推理过程，并在针对特定领域进行微调之前取得了令人印象深刻的结果。
- en: '![](../Images/5cd0a740c3b11633423e2d728c972970.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5cd0a740c3b11633423e2d728c972970.png)'
- en: '*Figure 6\. A comparison of open source chatbots’ relative response quality
    evaluated by GPT-4 using the Vicuna evaluation set. Image credit:* [*Microsoft
    Research*](https://arxiv.org/pdf/2306.02707.pdf)*.*'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '*图6\. 使用Vicuna评估集，通过GPT-4评估的开源聊天机器人的相对响应质量比较。图片来源：* [*微软研究院*](https://arxiv.org/pdf/2306.02707.pdf)*。*'
- en: Vicuna could be a good proxy for recent open source nimble models that were
    derived from LLaMA as the base model. Vicuna-13B is a chatbot created by a university
    collaboration that has been “developed to [address the lack of training and architecture
    details](https://pub.towardsai.net/meet-vicuna-the-latest-metas-llama-model-that-matches-chatgpt-performance-e23b2fc67e6b)
    in existing models such as ChatGPT.” After being fine-tuned on user-shared conversations
    from ShareGPT, the response quality of Vicuna is [more than 90%](https://lmsys.org/blog/2023-03-30-vicuna/)
    compared to ChatGPT and Google Bard when using GPT-4 as a judge. However, these
    early open source models are not available for commercial use. [MosaicML’s MPT-7B](https://www.mosaicml.com/blog/mpt-7b)
    and [Technology Innovation Institute’s Falcon 7B](https://falconllm.tii.ae/) commercially
    usable, open source models are reportedly equal in quality to LLaMA-7B.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: Vicuna可以作为近期从LLaMA衍生出的开源灵活模型的一个良好代表。Vicuna-13B是一个由大学合作创建的聊天机器人，旨在“[解决现有模型如ChatGPT中训练和架构细节的缺乏](https://pub.towardsai.net/meet-vicuna-the-latest-metas-llama-model-that-matches-chatgpt-performance-e23b2fc67e6b)。”在ShareGPT上进行用户共享对话微调后，Vicuna的响应质量相比于ChatGPT和Google
    Bard的GPT-4判定结果[超过90%](https://lmsys.org/blog/2023-03-30-vicuna/)。然而，这些早期的开源模型尚不可用于商业用途。[MosaicML的MPT-7B](https://www.mosaicml.com/blog/mpt-7b)和[技术创新研究所的Falcon
    7B](https://falconllm.tii.ae/)为商业可用的开源模型，其质量 reportedly 与LLaMA-7B相当。
- en: '![](../Images/345cd5b3469a20a638a64d36bf21ac6d.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/345cd5b3469a20a638a64d36bf21ac6d.png)'
- en: '*Figure 7\. Orca-13B performs as well as ChatGPT on* BIG-bench Hard’s *complex
    zero-shot reasoning tasks. Image credit:* [*Microsoft Research*](https://arxiv.org/pdf/2306.02707.pdf)*.*'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '*图7\. Orca-13B在* BIG-bench Hard的*复杂零样本推理任务上表现与ChatGPT相当。图片来源：* [*微软研究院*](https://arxiv.org/pdf/2306.02707.pdf)*。*'
- en: Orca “surpasses conventional instruction-tuned models such as Vicuna-13B by
    more than 100% in complex zero-shot reasoning benchmarks like [Big-Bench Hard](https://github.com/suzgunmirac/BIG-Bench-Hard)
    (BBH). It reaches parity with ChatGPT-3.5 on the BBH benchmark,” [according to
    researchers](https://arxiv.org/pdf/2306.02707.pdf). Orca-13B’s top performance
    over other general models reinforces the notion that the large size of giant models
    may result from brute-force early models. The scale of giant foundation models
    can be important for some smaller models like Orca-13B to distill knowledge and
    methods, but size is not necessarily required for inference — even for the general
    case. A word of caution — a full evaluation of cognitive capabilities, functional
    skills, and knowledge memorization of the model will only be possible when it
    is broadly deployed and exercised.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: Orca 在复杂的零-shot推理基准测试中，如[Big-Bench Hard](https://github.com/suzgunmirac/BIG-Bench-Hard)（BBH），**超越**了传统的指令调优模型，如
    Vicuna-13B，超过了100%。在 BBH 基准测试中，它与 ChatGPT-3.5 达到了相同的水平，”[根据研究人员](https://arxiv.org/pdf/2306.02707.pdf)的说法。**Orca-13B**
    在其他通用模型中的顶级表现强化了这样的观点，即巨型模型的巨大规模可能来源于早期模型的暴力破解。巨型基础模型的规模对一些较小的模型，如 Orca-13B，用于提炼知识和方法可能很重要，但大小不一定是推理所必需的——即使对于一般情况也是如此。需要注意的是——对模型的认知能力、功能技能和知识记忆的全面评估，只有在广泛部署和应用时才有可能。
- en: As of the writing of this blog, [Meta released their Llama 2 model](https://about.fb.com/news/2023/07/llama-2/)
    with 7B, 13B and 70B parameters. Arriving just four months after the first generation,
    the model offers meaningful improvements. In the [comparison chart](https://ai.meta.com/llama/),
    a nimble Llama 2 13B achieves similar results to larger models from the previous
    LLaMA generation as well as to MPT-30B and Falcon 40B. Llama 2 is open source
    and free for research and commercial use. It was introduced in tight partnership
    with Microsoft as well as quite a few other partners, including Intel. Meta’s
    commitment to open source models and its broad collaboration will surely give
    an additional boost to the rapid cross-industry/academia improvement cycles we
    are seeing for such models.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 截至撰写本博客时，[Meta 发布了他们的 Llama 2 模型](https://about.fb.com/news/2023/07/llama-2/)，包含
    7B、13B 和 70B 参数。该模型在首代发布四个月后问世，提供了显著的改进。在[对比图表](https://ai.meta.com/llama/)中，灵活的
    Llama 2 13B 达到了与前一代 LLaMA 更大模型以及 MPT-30B 和 Falcon 40B 相似的结果。Llama 2 是开源的，可用于研究和商业用途。它在与微软及包括英特尔在内的多个合作伙伴的紧密合作下推出。Meta
    对开源模型的承诺及其广泛的合作将无疑为我们看到的这种模型的跨行业/学术快速改进周期提供额外的推动力。
- en: 'The third aspect of nimble models has to do with specialization. Many of the
    newly-introduced nimble models are general purpose — like LLaMA, Vicuna and Orca.
    General nimble models may rely solely on their parametric memory, using low-cost
    updates through fine-tune methods including [LoRA: Low-Rank Adaptation of Large
    Language Models](https://arxiv.org/abs/2106.09685) as well as [retrieval-augmented
    generation](https://proceedings.neurips.cc/paper/2020/hash/6b493230205f780e1bc26945df7481e5-Abstract.html),
    which pulls relevant knowledge from a curated corpora on-the-fly during inference
    time. Retrieval-augmented solutions are being established and continuously enhanced
    with GenAI frameworks like [LangChain](https://python.langchain.com/docs/get_started/introduction.html)
    and [Haystack](https://www.haystackteam.com/core/knowledge). These [frameworks
    allow easy and flexible integration](https://mantiumai.com/blog/how-haystack-and-langchain-are-empowering-large-language-models/)
    of indexing and effectively accessing large corpora for semantics-based retrieval.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '灵活模型的第三个方面涉及专业化。许多新推出的灵活模型都是通用的——例如 LLaMA、Vicuna 和 Orca。通用灵活模型可能完全依赖于它们的参数记忆，通过细调方法进行低成本更新，包括[LoRA:
    大型语言模型的低秩适应](https://arxiv.org/abs/2106.09685)以及[检索增强生成](https://proceedings.neurips.cc/paper/2020/hash/6b493230205f780e1bc26945df7481e5-Abstract.html)，在推理时实时从策划的语料库中提取相关知识。检索增强的解决方案正在建立并不断通过
    GenAI 框架如[LangChain](https://python.langchain.com/docs/get_started/introduction.html)和[Haystack](https://www.haystackteam.com/core/knowledge)进行增强。这些[框架允许轻松灵活的集成](https://mantiumai.com/blog/how-haystack-and-langchain-are-empowering-large-language-models/)索引和有效访问大型语料库以进行基于语义的检索。'
- en: Most business users prefer targeted models that are tuned for their particular
    domain of interest. These targeted models also tend to be retrieval-based to utilize
    all key information assets. For example, healthcare users may want to automate
    patient communications.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数商业用户更倾向于针对其特定领域的定制模型。这些针对性模型通常也是基于检索的，以利用所有关键的信息资产。例如，医疗保健用户可能希望自动化患者沟通。
- en: 'Targeted models use two methods:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 针对性模型使用两种方法：
- en: Specialization of the model itself to the tasks and type of data required for
    the targeted use cases. This could be done in multiple ways, including pre-training
    a model on specific domain knowledge (like how [phi-1](https://arxiv.org/abs/2306.11644)
    pre-trained on textbook-quality data from the web), fine-tuning a general purpose
    base model of the same size (like how [Clinical Camel](https://arxiv.org/abs/2305.12031)
    fine-tuned LLaMA-13B), or distilling and learning from a giant model into a student
    nimble model (like how [Orca](https://arxiv.org/pdf/2306.02707.pdf) learned to
    imitate the reasoning process of GPT-4, including explanation traces, step-by-step
    thought processes, and other complex instructions).
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对模型本身进行专业化以满足目标使用案例所需的任务和数据类型。这可以通过多种方式实现，包括在特定领域知识上对模型进行预训练（如[phi-1](https://arxiv.org/abs/2306.11644)在来自网络的教科书质量数据上进行预训练）、对相同规模的通用基础模型进行微调（如[Clinical
    Camel](https://arxiv.org/abs/2305.12031)微调了LLaMA-13B），或将巨型模型的知识提炼并学习到学生型灵活模型中（如[Orca](https://arxiv.org/pdf/2306.02707.pdf)学习模仿GPT-4的推理过程，包括解释痕迹、逐步思维过程和其他复杂指令）。
- en: Curating and indexing relevant data for on-the-fly retrieval, which could be
    a large volume, but still within the scope/space of the targeted use case. Models
    can retrieve public web and private consumer or enterprise content that is continuously
    updated. Users determine which sources to index, allowing the choice of high-quality
    resources from the web plus more complete resources like an individual’s private
    data or a company’s enterprise data. While retrieval is now integrated into both
    giant and nimble systems, it plays a crucial role in smaller models as it provides
    all the necessary information for the model performance. It also allows for businesses
    to make all their private and local information available to a nimble model running
    within their compute environment.
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为即时检索策划和索引相关数据，这可能是大量的，但仍在目标使用案例的范围/空间内。模型可以检索持续更新的公共网络和私有消费者或企业内容。用户确定索引哪些来源，从而选择来自网络的高质量资源以及更完整的资源，如个人的私人数据或公司的企业数据。虽然检索现在已集成到巨型和灵活系统中，但在小型模型中发挥着关键作用，因为它提供了模型性能所需的所有必要信息。它还允许企业将其所有私有和本地信息提供给在其计算环境中运行的灵活模型。
- en: '**Nimble Generative AI Model Advantages and Disadvantages**'
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**灵活生成性AI模型的优点和缺点**'
- en: In the future, the size of compact models might drift up to 20B or 25B parameters,
    but still stay far below the 100B parameters scope. There is also a variety of
    models of intermediate sizes like MPT-30B, Falcon 40B and Llama 2 70B. While they
    are expected to perform better than smaller models on zero-shot, I would not expect
    them to perform materially better for any defined set of functionalities than
    nimble, targeted, retrieval-based models.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在未来，紧凑模型的规模可能会增加到20B或25B参数，但仍远低于100B参数范围。也有许多中等规模的模型，如MPT-30B、Falcon 40B和Llama
    2 70B。虽然这些模型在零样本任务上预计表现会比小模型更好，但我不认为它们在任何特定功能集上的表现会显著优于灵活的、针对性的、基于检索的模型。
- en: 'When compared with giant models, there are many advantages of nimble models,
    which are further enhanced when the model is targeted and retrieval-based. These
    benefits include:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 与巨型模型相比，灵活模型有许多优点，尤其是当模型是针对性的和基于检索的时，这些优点会得到进一步增强。这些好处包括：
- en: '**Sustainable and lower cost models:** Models with substantially lower costs
    for [training and inference compute](https://www.semianalysis.com/p/google-we-have-no-moat-and-neither).
    Inference run-time compute costs might be the determining factor for viability
    of business-oriented models integrated into 24x7 usages, and the much-decreased
    environmental impact is also significant when taken in aggregate across broad
    deployments. Finally, with their sustainable, specific, and functionally oriented
    systems, nimble models are not attempting to address ambitious goals of artificial
    general intelligence (AGI) and are therefore less involved in the public and regulatory
    debate related to the latter.'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可持续且成本较低的模型：** 模型在[训练和推理计算](https://www.semianalysis.com/p/google-we-have-no-moat-and-neither)上具有显著较低的成本。推理运行时计算成本可能是24x7使用的商业导向模型可行性的决定因素，并且在广泛部署中整体环境影响的大幅减少也很显著。最后，凭借其可持续、特定和功能导向的系统，灵活模型并不试图解决人工通用智能（AGI）的雄心勃勃的目标，因此在与后者相关的公共和监管辩论中参与较少。'
- en: '**Faster fine-tune iterations:** Smaller models can be fine-tuned in a few
    hours (or less), adding new information or functionality to the model through
    [adaptation methods like LoRA](https://arxiv.org/pdf/2106.09685.pdf), which are
    highly effective in nimble models. This enables more frequent improvement cycles,
    keeping the model continuously up to date with its usage needs.'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**更快的微调迭代：** 较小的模型可以在几小时（或更短时间）内完成微调，通过[类似LoRA的适应方法](https://arxiv.org/pdf/2106.09685.pdf)向模型添加新信息或功能，这在灵活模型中非常有效。这使得改进周期更频繁，使模型始终与其使用需求保持同步。'
- en: '**Retrieval-based model benefits:** Retrieval systems refactor knowledge, referencing
    most of the information from the direct sources rather than the parametric memory
    of the model. This improves the following:'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基于检索的模型优点：** 检索系统重新组织知识，从直接来源引用大部分信息，而不是模型的参数记忆。这改善了以下方面：'
- en: – **Explainability:** Retrieval models use source attribution, providing provenance
    or the ability to track back to the source of information to provide credibility.
  id: totrans-59
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: – **可解释性：** 检索模型使用源属性，提供来源或能够追溯到信息来源的能力，以提供可信度。
- en: – **Timeliness:** Once an up-to-date source is indexed, it is immediately available
    for use by the model without any need for training or fine-tuning. That allows
    for continuously adding or updating relevant information in near real-time.
  id: totrans-60
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: – **时效性：** 一旦索引了最新的来源，模型可以立即使用，无需任何训练或微调。这允许在接近实时的情况下不断添加或更新相关信息。
- en: – **Scope of data:** The information indexed for per-need retrieval can be very
    broad and detailed. When focused on its target domains, the model can cover a
    huge scope and depth of private and public data. It may include more volume and
    details in its target space than a giant foundation model training dataset.
  id: totrans-61
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: – **数据范围：** 为按需检索编制的信息可以非常广泛和详细。当集中在目标领域时，模型可以覆盖大量的私有和公共数据的范围和深度。它可能在其目标空间中包含比巨型基础模型训练数据集更多的量和细节。
- en: – **Accuracy:** Direct access to data in its original form, detail, and context
    can reduce hallucinations and data approximations. It can provide reliable and
    complete answers as long as they are in the retrieval space. With smaller models,
    there is also less conflict between traceable curated information retrieved per-need,
    and memorized information (as in giant models) that might be dated, partial and
    not attributed to sources.
  id: totrans-62
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: – **准确性：** 直接访问数据的原始形式、细节和上下文可以减少幻觉和数据近似。只要在检索范围内，就可以提供可靠和完整的答案。使用较小的模型时，按需检索的可追溯策划信息和（如巨型模型中）可能过时、部分且未标注来源的记忆信息之间的冲突也更少。
- en: '**Choice of hardware:** Inference of nimble models can be done practically
    on any hardware, including ubiquitous solutions that might already be part of
    the compute setting. For example, Meta’s Llama 2 nimble models (7B and 13B parameters)
    are [running well on Intel’s data center products](https://www.intel.com/content/www/us/en/developer/articles/news/llama2.html)
    including Xeon, Gaudi2 and Intel Data Center GPU Max Series.'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**硬件选择：** 灵活模型的推理可以在任何硬件上实际完成，包括可能已经是计算设置一部分的普遍解决方案。例如，Meta的Llama 2灵活模型（7B和13B参数）[在英特尔的数据中心产品](https://www.intel.com/content/www/us/en/developer/articles/news/llama2.html)上运行良好，包括Xeon、Gaudi2和Intel数据中心GPU
    Max系列。'
- en: '**Integration, security and privacy:** Today’s ChatGPT and other giant GenAI
    models are independent models that usually run on large accelerator installations
    on a third-party platform and are accessed through interfaces. Nimble AI models
    can run as an engine that is embedded within a larger business application and
    can be fully integrated into the local compute environment. This has major implications
    for security and privacy because there is no need for exchange/exposure of information
    with third-party models and compute environments, and all security mechanisms
    of the broader application can be applied to the GenAI engine.'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**集成、安全和隐私：** 今天的ChatGPT和其他巨型GenAI模型是独立模型，通常在第三方平台上的大型加速器设施上运行，并通过接口访问。灵活的AI模型可以作为嵌入到更大业务应用程序中的引擎运行，并且可以完全集成到本地计算环境中。这对于安全和隐私有重大影响，因为不需要与第三方模型和计算环境交换/暴露信息，而且更广泛应用的所有安全机制都可以应用于GenAI引擎。'
- en: '**Optimization and model reduction:** Optimization and model reduction techniques
    such as quantization, which reduces computational demands by converting input
    values to smaller output values, [have shown strong initial results](https://huggingface.co/blog/generative-ai-models-on-intel-cpu)
    on nimble models with increasing power efficiency.'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**优化和模型缩减：** 优化和模型缩减技术，如量化，通过将输入值转换为较小的输出值来减少计算需求，在增加功率效率的灵活模型上已经显示出了强大的初步结果。'
- en: 'Some challenges of nimble models are still worth mentioning:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '**检索模型需要对所有源数据进行索引：** 模型在推断期间通过索引映射获取所需信息，但存在错过信息源的风险，使其对模型不可用。为了确保来源可追溯性、可解释性和其他属性，针对检索型模型不应依赖于存储在参数记忆中的详细信息，而应主要依赖于在需要时可用于提取的索引信息。'
- en: '**Reduced range of tasks:** General-purpose giant models have outstanding versatility
    and especially excel in zero-shot new usages that were not considered earlier.
    The breadth and scope that can be achieved with nimble systems is still under
    evaluation but seems to be improving with recent models. Targeted models assume
    that the range of tasks is known and defined during pre-training and/or fine-tuning,
    so the reduction in scope should not impact any relevant capabilities. Targeted
    models are not single task, but rather a family of related capabilities. This
    can lead to fragmentation as a result of task- or business-specific nimble models.'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**任务范围减少：** 通用巨型模型具有出色的多功能性，特别擅长于未曾考虑的零-shot新用途。灵活系统能够实现的广度和范围目前仍在评估中，但似乎随着最近的模型而有所改善。目标模型假定任务范围在预训练和/或微调期间已知和定义，因此范围的缩减不应影响任何相关能力。目标模型不是单一任务，而是一组相关能力。这可能会导致由于任务或业务特定的灵活模型而产生的碎片化。'
- en: '**May be improved with few-shot fine-tuning:** For a model to address a targeted
    space effectively, fine-tuning is not always required, but can aid the effectiveness
    of AI by adjusting the model to the tasks and information needed for the application.
    Modern techniques enable this process to be done with a small number of examples
    and without the need for deep data science expertise.'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可能通过少量样本的精细调整来改进：** 为了有效地解决目标空间的模型，不总是需要进行精细调整，但可以通过调整模型以适应应用程序所需的任务和信息来提高AI的效果。现代技术使得这一过程可以用少量示例完成，而无需深入的数据科学专业知识。'
- en: '**Retrieval models need indexing of all source data:** Models pull in needed
    information during inference through index mapping, but there is risk of missing
    an information source, making it unavailable to the model. To ensure provenance,
    explainability and other properties, targeted retrieval-based models should not
    rely on detailed information stored in the parametric memory, and instead rely
    mainly on indexed information that is available for extraction when needed.'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[在Intel CPU上的生成AI模型已经显示了强大的初步结果](https://huggingface.co/blog/generative-ai-models-on-intel-cpu)。'
- en: '**Summary**'
  id: totrans-70
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**摘要**'
- en: The major leap in generative AI is enabling new capabilities such as AI agents
    conversing in plain language, the summarization and generation of compelling text,
    image creation, utilization of the context of previous iterations and much more.
    This blog introduces the term “nimble AI” and makes the case for why it will be
    the predominant method in deploying GenAI at scale. Simply put, nimble AI models
    are faster to run, quicker to refresh through continuous fine-tuning, and more
    amenable to rapid technology improvement cycles through the collective innovation
    of the open source community.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 生成AI的重大飞跃使得新的能力成为可能，例如AI代理以自然语言交谈、引人入胜的文本总结和生成、图像创作、利用先前迭代的上下文等。本文介绍了“灵活AI”这一术语，并阐述了为什么它将成为大规模部署生成AI的主要方法。简单来说，灵活AI模型运行更快，通过持续微调更新更迅速，并且通过开源社区的集体创新，更容易进行快速技术改进。
- en: As demonstrated through multiple examples, the outstanding performance that
    emerged through the evolution of the largest models shows that nimble models do
    not require the same massive heft as the giant models. Once the underlying cognitive
    abilities have been mastered, the required functionality tuned and the data made
    available per-need, nimble models provide the highest value for the business world.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 通过多个示例展示，随着最大模型的进化，表现出的卓越性能表明，灵活模型不需要像巨型模型那样的庞大体积。一旦掌握了基本认知能力，调整了所需功能，并根据需要提供数据，灵活模型为商业世界提供了最高的价值。
- en: That said, nimble models will not render giant models extinct. Giant models
    are still expected to perform better in a zero-shot, out-of-the-box setting. These
    large models also might be used as the source (teacher model) for distillation
    into smaller, nimble models. While giant models have a huge volume of additional
    memorized information to address any potential use, and are equipped with multiple
    skills, this generality is not expected to be required for most GenAI applications.
    Instead, the ability to fine-tune a model to the information and skills relevant
    for the domain, plus the ability to retrieve recent information from curated local
    and global sources, would be a much better value proposition for many applications.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管如此，灵活模型不会使巨型模型灭绝。巨型模型在零-shot、开箱即用的设置中仍然有更好的表现。这些大型模型也可能被用作蒸馏成较小灵活模型的来源（教师模型）。虽然巨型模型拥有大量额外的记忆信息来处理任何潜在的用途，并且配备了多种技能，但这种通用性不被期望在大多数生成AI应用中需要。相反，将模型微调到与领域相关的信息和技能，并能够从策划的本地和全球来源中检索最新信息，将为许多应用提供更好的价值主张。
- en: 'Viewing nimble, targeted AI models as modules that can be incorporated into
    any existing application offers a very compelling value proposition including:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 将灵活、针对性的AI模型视为可以集成到任何现有应用中的模块，提供了非常有吸引力的价值主张，包括：
- en: Requires a fraction of the cost for deployment and operation.
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 部署和操作成本仅为其一小部分。
- en: Adaptable to tasks and private/enterprise data.
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 适应任务和私人/企业数据。
- en: Updatable overnight, and can run on any hardware from CPU, to GPU or accelerators.
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每晚更新，并且可以在从CPU到GPU或加速器的任何硬件上运行。
- en: Integrated into current compute environment and application.
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 集成到当前计算环境和应用中。
- en: Runs on premise or in a private cloud.
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 运行在本地或私有云中。
- en: Benefits from all security and privacy settings.
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 受益于所有的安全和隐私设置。
- en: Higher accuracy and explainability.
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更高的准确性和可解释性。
- en: More environmentally responsible while providing a similar level of generative
    AI capabilities.
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在提供类似生成AI能力的同时，更具环保责任感。
- en: Impressive progress on a small number of giant models will continue. However,
    the industry will most likely need just a few dozen or so general-purpose nimble
    base models, which can then be used to build countless targeted versions. I foresee
    a near-term future in which a broad scaling of advanced GenAI will permeate all
    industries, mostly by integrating nimble, targeted secure intelligence modules
    as their engines of growth.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在少量巨型模型上取得的令人印象深刻的进展将继续。然而，行业最可能需要的只是几十个通用的灵活基础模型，这些模型可以用来构建无数的目标版本。我预见到不久的将来，广泛扩展的高级生成AI将渗透到所有行业，主要通过将灵活、针对性的安全智能模块作为增长引擎。
- en: '**References**'
  id: totrans-84
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**参考资料**'
- en: Tseng, P. K. (2023, March 1). TrendForce Says with Cloud Companies Initiating
    AI Arms Race, GPU Demand from ChatGPT Could Reach 30,000 Chips as It Readies for
    Commercialization*. TrendForce.* [https://www.trendforce.com/presscenter/news/20230301-11584.html](https://www.trendforce.com/presscenter/news/20230301-11584.html)
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Tseng, P. K. (2023年3月1日). TrendForce 表示，由于云公司发起 AI 军备竞赛，ChatGPT 对 GPU 的需求可能达到
    30,000 芯片，为商业化做好准备。*TrendForce*。 [https://www.trendforce.com/presscenter/news/20230301-11584.html](https://www.trendforce.com/presscenter/news/20230301-11584.html)
- en: 'Introducing MPT-7B: A New Standard for Open Source, Commercially Usable LLMs.
    (2023, May 5). [https://www.mosaicml.com/blog/mpt-7b](https://www.mosaicml.com/blog/mpt-7b)'
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 介绍 MPT-7B：开源、商业可用 LLM 的新标准。 (2023年5月5日)。 [https://www.mosaicml.com/blog/mpt-7b](https://www.mosaicml.com/blog/mpt-7b)
- en: 'Mukherjee, S., Mitra, A., Jawahar, G., Agarwal, S., Palangi, H., &and Awadallah,
    A. (2023). Orca: Progressive Learning from Complex Explanation Traces of GPT-4\.
    *arXiv (Cornell University)*. [https://doi.org/10.48550/arxiv.2306.02707](https://doi.org/10.48550/arxiv.2306.02707)'
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Mukherjee, S., Mitra, A., Jawahar, G., Agarwal, S., Palangi, H., & Awadallah,
    A. (2023). Orca：从 GPT-4 的复杂解释轨迹中逐步学习。*arXiv (康奈尔大学)*。 [https://doi.org/10.48550/arxiv.2306.02707](https://doi.org/10.48550/arxiv.2306.02707)
- en: Wolfram, S. (2023, March 23). ChatGPT Gets Its “Wolfram Superpowers”!. *Stephen
    Wolfram Writings*. [https://writings.stephenwolfram.com/2023/03/chatgpt-gets-its-wolfram-superpowers/](https://writings.stephenwolfram.com/2023/03/chatgpt-gets-its-wolfram-superpowers/)
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Wolfram, S. (2023年3月23日). ChatGPT 获得了“Wolfram 超能力”！*Stephen Wolfram Writings*。
    [https://writings.stephenwolfram.com/2023/03/chatgpt-gets-its-wolfram-superpowers/](https://writings.stephenwolfram.com/2023/03/chatgpt-gets-its-wolfram-superpowers/)
- en: Schreiner, M. (2023, July 11). GPT-4 architecture, datasets, costs and more
    leaked. *THE DECODER*. [https://the-decoder.com/gpt-4-architecture-datasets-costs-and-more-leaked/](https://the-decoder.com/gpt-4-architecture-datasets-costs-and-more-leaked/)
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Schreiner, M. (2023年7月11日). GPT-4 架构、数据集、成本等泄露。*THE DECODER*。 [https://the-decoder.com/gpt-4-architecture-datasets-costs-and-more-leaked/](https://the-decoder.com/gpt-4-architecture-datasets-costs-and-more-leaked/)
- en: '*ChatGPT plugins*. (n.d.). [https://openai.com/blog/chatgpt-plugins](https://openai.com/blog/chatgpt-plugins)'
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*ChatGPT 插件*。 (无日期)。 [https://openai.com/blog/chatgpt-plugins](https://openai.com/blog/chatgpt-plugins)'
- en: Izacard, G., Caron, M., Hosseini, L., Riedel, S., Bojanowski, P., Joulin, A.,
    & Grave, E. (2021). Unsupervised Dense Information Retrieval with Contrastive
    Learning. *arXiv (Cornell University)*. [https://doi.org/10.48550/arxiv.2112.09118](https://doi.org/10.48550/arxiv.2112.09118)
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Izacard, G., Caron, M., Hosseini, L., Riedel, S., Bojanowski, P., Joulin, A.,
    & Grave, E. (2021). 使用对比学习进行无监督密集信息检索。*arXiv (康奈尔大学)*。 [https://doi.org/10.48550/arxiv.2112.09118](https://doi.org/10.48550/arxiv.2112.09118)
- en: 'Mallen, A., Asai, A., Zhong, V., Das, R., Hajishirzi, H., and Khashabi, D.
    (2022). When not to Trust Language Models: Investigating Effectiveness of Parametric
    and Non-Parametric Memories. *arXiv (Cornell University)*. [https://doi.org/10.48550/arxiv.2212.10511](https://doi.org/10.48550/arxiv.2212.10511)'
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Mallen, A., Asai, A., Zhong, V., Das, R., Hajishirzi, H., 和 Khashabi, D. (2022).
    何时不信任语言模型：调查参数化和非参数化记忆的有效性。*arXiv (康奈尔大学)*。 [https://doi.org/10.48550/arxiv.2212.10511](https://doi.org/10.48550/arxiv.2212.10511)
- en: '*Papers with Code — PopQA Dataset*. (n.d.). [https://paperswithcode.com/dataset/popqa](https://paperswithcode.com/dataset/popqa)'
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*Papers with Code — PopQA 数据集*。 (无日期)。 [https://paperswithcode.com/dataset/popqa](https://paperswithcode.com/dataset/popqa)'
- en: 'Introducing LLaMA: A foundational, 65-billion-parameter large language model.
    (2023, February 24). [https://ai.facebook.com/blog/large-language-model-llama-meta-ai/](https://ai.facebook.com/blog/large-language-model-llama-meta-ai/)'
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 介绍 LLaMA：一个基础的 65 亿参数的大型语言模型。 (2023年2月24日)。 [https://ai.facebook.com/blog/large-language-model-llama-meta-ai/](https://ai.facebook.com/blog/large-language-model-llama-meta-ai/)
- en: Introducing Falcon LLM. (n.d.). [https://falconllm.tii.ae/](https://falconllm.tii.ae/)
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 介绍 Falcon LLM。 (无日期)。 [https://falconllm.tii.ae/](https://falconllm.tii.ae/)
- en: 'Nijkamp, E., Hayashi, H., Xie, T., Xia, C., Pang, B., Meng, R., Kryscinski,
    W., Tu, L., Bhat, M., Yavuz, S., Xing, C., Vig, J., Murakhovs’ka, L., Wu, C. S.,
    Zhou, Y., Joty, S. R., Xiong, C., and Savarese, S. (2023). Long Sequence Modeling
    with XGen: A 7B LLM Trained on 8K Input Sequence Length. *Salesforce AI Research*.
    [https://blog.salesforceairesearch.com/xgen/](https://blog.salesforceairesearch.com/xgen/)'
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Nijkamp, E., Hayashi, H., Xie, T., Xia, C., Pang, B., Meng, R., Kryscinski,
    W., Tu, L., Bhat, M., Yavuz, S., Xing, C., Vig, J., Murakhovs’ka, L., Wu, C. S.,
    Zhou, Y., Joty, S. R., Xiong, C., 和 Savarese, S. (2023). 使用 XGen 进行长序列建模：一个在 8K
    输入序列长度上训练的 7B LLM。*Salesforce AI Research*。 [https://blog.salesforceairesearch.com/xgen/](https://blog.salesforceairesearch.com/xgen/)
- en: 'Hu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L.,
    and Chen, W. (2021, June 17). LoRA: Low-Rank Adaptation of Large Language Models.
    *arXiv (Cornell University)*. [https://doi.org/10.48550/arXiv.2106.09685](https://doi.org/10.48550/arXiv.2106.09685)'
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Hu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L.,
    和Chen, W. (2021年6月17日). LoRA：大型语言模型的低秩适应。*arXiv (康奈尔大学)*. [https://doi.org/10.48550/arXiv.2106.09685](https://doi.org/10.48550/arXiv.2106.09685)
- en: Lewis, P., Perez, E., Piktus, A., Petroni, F., Karpukhin, V., Goyal, N., Küttler,
    H., Lewis, M., Yih, W., Rocktäschel, T., Riedel, S., and Kiela, D. (2020). Retrieval-Augmented
    Generation for Knowledge-Intensive NLP Tasks. *NeurIPS 2020*. [https://proceedings.neurips.cc/paper/2020/hash/6b493230205f780e1bc26945df7481e5-Abstract.html](https://proceedings.neurips.cc/paper/2020/hash/6b493230205f780e1bc26945df7481e5-Abstract.html)
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Lewis, P., Perez, E., Piktus, A., Petroni, F., Karpukhin, V., Goyal, N., Küttler,
    H., Lewis, M., Yih, W., Rocktäschel, T., Riedel, S., 和Kiela, D. (2020). 针对知识密集型NLP任务的检索增强生成。*NeurIPS
    2020*. [https://proceedings.neurips.cc/paper/2020/hash/6b493230205f780e1bc26945df7481e5-Abstract.html](https://proceedings.neurips.cc/paper/2020/hash/6b493230205f780e1bc26945df7481e5-Abstract.html)
- en: Introduction LangChain. (n.d.). [https://python.langchain.com/docs/get_started/introduction.html](https://python.langchain.com/docs/get_started/introduction.html)
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Introduction LangChain. (无日期). [https://python.langchain.com/docs/get_started/introduction.html](https://python.langchain.com/docs/get_started/introduction.html)
- en: Haystack. (n.d.). [https://www.haystackteam.com/core/knowledge](https://www.haystackteam.com/core/knowledge)
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Haystack. (无日期). [https://www.haystackteam.com/core/knowledge](https://www.haystackteam.com/core/knowledge)
- en: Mantium. (2023). How Haystack and LangChain are Empowering Large Language Models.
    *Mantium*. [https://mantiumai.com/blog/how-haystack-and-langchain-are-empowering-large-language-models/](https://mantiumai.com/blog/how-haystack-and-langchain-are-empowering-large-language-models/)
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Mantium. (2023). Haystack和LangChain如何赋能大型语言模型。*Mantium*. [https://mantiumai.com/blog/how-haystack-and-langchain-are-empowering-large-language-models/](https://mantiumai.com/blog/how-haystack-and-langchain-are-empowering-large-language-models/)
- en: 'Taori, R., Gulrajani, I., Zhang, T., Dubois, Y., Li, X., Guestrin, C., Liang,
    P., and Hashimoto, T. B. (2023, March 13). Alpaca: A Strong, Replicable Instruction-Following
    Model. *Stanford University CRFM*. [https://crfm.stanford.edu/2023/03/13/alpaca.html](https://crfm.stanford.edu/2023/03/13/alpaca.html)'
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Taori, R., Gulrajani, I., Zhang, T., Dubois, Y., Li, X., Guestrin, C., Liang,
    P., 和Hashimoto, T. B. (2023年3月13日). Alpaca：一个强大且可复制的指令跟随模型。*斯坦福大学CRFM*. [https://crfm.stanford.edu/2023/03/13/alpaca.html](https://crfm.stanford.edu/2023/03/13/alpaca.html)
- en: 'Geng, X., Gudibande, A., Liu, H., Wallace, E., Abbeel, P., Levine, S. and Song,
    D. (2023, April 3). Koala: A Dialogue Model for Academic Research. *Berkeley Artificial
    Intelligence Research Blog.* [https://bair.berkeley.edu/blog/2023/04/03/koala/](https://bair.berkeley.edu/blog/2023/04/03/koala/)'
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Geng, X., Gudibande, A., Liu, H., Wallace, E., Abbeel, P., Levine, S. 和 Song,
    D. (2023年4月3日). Koala：一个用于学术研究的对话模型。*伯克利人工智能研究博客*. [https://bair.berkeley.edu/blog/2023/04/03/koala/](https://bair.berkeley.edu/blog/2023/04/03/koala/)
- en: 'Chiang, W. L., Li, Z., Lin, Z., Sheng, Y., Wu, Z., Zhang, H., Zheng, L., Zhuang,
    S., Zhuang, Y., Gonzalez, J. E., Stoica, I., and Xing, E. P. (2023, March 30).
    Vicuna: An Open Source Chatbot Impressing GPT-4 with 90%* ChatGPT Quality. *LMSYS
    Org*. [https://lmsys.org/blog/2023-03-30-vicuna/](https://lmsys.org/blog/2023-03-30-vicuna/)'
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Chiang, W. L., Li, Z., Lin, Z., Sheng, Y., Wu, Z., Zhang, H., Zheng, L., Zhuang,
    S., Zhuang, Y., Gonzalez, J. E., Stoica, I., 和Xing, E. P. (2023年3月30日). Vicuna：一个令人印象深刻的开源聊天机器人，媲美GPT-4，*质量达到90%*的ChatGPT。*LMSYS
    Org*. [https://lmsys.org/blog/2023-03-30-vicuna/](https://lmsys.org/blog/2023-03-30-vicuna/)
- en: 'Rodriguez, J. (2023, April 5). Meet Vicuna: The Latest Meta’s Llama Model that
    Matches ChatGPT Performance. *Medium*. [https://pub.towardsai.net/meet-vicuna-the-latest-metas-llama-model-that-matches-chatgpt-performance-e23b2fc67e6b](https://pub.towardsai.net/meet-vicuna-the-latest-metas-llama-model-that-matches-chatgpt-performance-e23b2fc67e6b)'
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Rodriguez, J. (2023年4月5日). 认识Vicuna：最新的Meta的Llama模型，与ChatGPT性能相当。*Medium*. [https://pub.towardsai.net/meet-vicuna-the-latest-metas-llama-model-that-matches-chatgpt-performance-e23b2fc67e6b](https://pub.towardsai.net/meet-vicuna-the-latest-metas-llama-model-that-matches-chatgpt-performance-e23b2fc67e6b)
- en: '*Papers with Code — BIG-bench Dataset*. (n.d.). [https://paperswithcode.com/dataset/big-bench](https://paperswithcode.com/dataset/big-bench)'
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*Papers with Code — BIG-bench 数据集*. (无日期). [https://paperswithcode.com/dataset/big-bench](https://paperswithcode.com/dataset/big-bench)'
- en: Meta. (2023, July 18). Meta and Microsoft introduce the next generation of Llama.
    Meta. [https://about.fb.com/news/2023/07/llama-2/](https://about.fb.com/news/2023/07/llama-2/)
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Meta. (2023年7月18日). Meta和微软推出下一代Llama。Meta. [https://about.fb.com/news/2023/07/llama-2/](https://about.fb.com/news/2023/07/llama-2/)
- en: Meta AI. (n.d.). Introducing Llama 2\. [https://ai.meta.com/llama/](https://ai.meta.com/llama/)
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Meta AI. (无日期). 介绍Llama 2。 [https://ai.meta.com/llama/](https://ai.meta.com/llama/)
- en: Gunasekar, S., Zhang, Y., Aneja, J., Mendes, C. C. T., Allie, D. G., Gopi, S.,
    Javaheripi, M., Kauffmann, P., Gustavo, D. R., Saarikivi, O., Salim, A., Shah,
    S., Behl, H. S., Wang, X., Bubeck, S., Eldan, R., Kalai, A. T., Lee, Y. T., and
    Li, Y. (2023). Textbooks Are All You Need. *arXiv (Cornell University)*. [https://doi.org/10.48550/arxiv.2306.11644](https://doi.org/10.48550/arxiv.2306.11644)
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Gunasekar, S., Zhang, Y., Aneja, J., Mendes, C. C. T., Allie, D. G., Gopi, S.,
    Javaheripi, M., Kauffmann, P., Gustavo, D. R., Saarikivi, O., Salim, A., Shah,
    S., Behl, H. S., Wang, X., Bubeck, S., Eldan, R., Kalai, A. T., Lee, Y. T., 和
    Li, Y. (2023). 教科书就是你所需的一切。*arXiv (康奈尔大学)*。 [https://doi.org/10.48550/arxiv.2306.11644](https://doi.org/10.48550/arxiv.2306.11644)
- en: 'Toma, A., Lawler, P. R., Ba, J., Krishnan, R. G., Rubin, B. B., and Wang, B.
    (2023). Clinical Camel: An Open-Source Expert-Level Medical Language Model with
    Dialogue-Based Knowledge Encoding. *arXiv (Cornell University)*. [https://doi.org/10.48550/arxiv.2305.12031](https://doi.org/10.48550/arxiv.2305.12031)'
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Toma, A., Lawler, P. R., Ba, J., Krishnan, R. G., Rubin, B. B., 和 Wang, B.
    (2023). Clinical Camel: 一个开源的专家级医疗语言模型，具有基于对话的知识编码。*arXiv (康奈尔大学)*。 [https://doi.org/10.48550/arxiv.2305.12031](https://doi.org/10.48550/arxiv.2305.12031)'
- en: Patel, D., and Ahmad, A. (2023, May 4). Google “We Have No Moat, And Neither
    Does OpenAI.” SemiAnalysis. [https://www.semianalysis.com/p/google-we-have-no-moat-and-neither](https://www.semianalysis.com/p/google-we-have-no-moat-and-neither)
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Patel, D., 和 Ahmad, A. (2023年5月4日). Google “我们没有护城河，OpenAI 也没有。” SemiAnalysis。
    [https://www.semianalysis.com/p/google-we-have-no-moat-and-neither](https://www.semianalysis.com/p/google-we-have-no-moat-and-neither)
- en: '*Accelerate Llama 2 with Intel AI Hardware and Software Optimizations*. (n.d.).
    Intel. [https://www.intel.com/content/www/us/en/developer/articles/news/llama2.html](https://www.intel.com/content/www/us/en/developer/articles/news/llama2.html)'
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*加速 Llama 2 通过英特尔 AI 硬件和软件优化*。（无日期）。英特尔。 [https://www.intel.com/content/www/us/en/developer/articles/news/llama2.html](https://www.intel.com/content/www/us/en/developer/articles/news/llama2.html)'
- en: '*Smaller is better: Q8-Chat, an efficient generative AI experience on Xeon*.
    (n.d.). Hugging Face. [https://huggingface.co/blog/generative-ai-models-on-intel-cpu](https://huggingface.co/blog/generative-ai-models-on-intel-cpu)'
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*更小更好：Q8-Chat，在 Xeon 上的高效生成 AI 体验*。（无日期）。Hugging Face。 [https://huggingface.co/blog/generative-ai-models-on-intel-cpu](https://huggingface.co/blog/generative-ai-models-on-intel-cpu)'
