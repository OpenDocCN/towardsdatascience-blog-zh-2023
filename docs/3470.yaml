- en: 'Courage to learn ML: Demystifying L1 & L2 Regularization (part 1)'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å‹‡æ•¢å­¦ä¹ æœºå™¨å­¦ä¹ ï¼šæ­ç¤º L1 å’Œ L2 æ­£åˆ™åŒ–ï¼ˆç¬¬ 1 éƒ¨åˆ†ï¼‰
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/understanding-l1-l2-regularization-part-1-9c7affe6f920?source=collection_archive---------5-----------------------#2023-11-22](https://towardsdatascience.com/understanding-l1-l2-regularization-part-1-9c7affe6f920?source=collection_archive---------5-----------------------#2023-11-22)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://towardsdatascience.com/understanding-l1-l2-regularization-part-1-9c7affe6f920?source=collection_archive---------5-----------------------#2023-11-22](https://towardsdatascience.com/understanding-l1-l2-regularization-part-1-9c7affe6f920?source=collection_archive---------5-----------------------#2023-11-22)
- en: Comprehend the underlying purpose of L1 and L2 regularization
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç†è§£ L1 å’Œ L2 æ­£åˆ™åŒ–çš„åŸºæœ¬ç›®çš„
- en: '[](https://amyma101.medium.com/?source=post_page-----9c7affe6f920--------------------------------)[![Amy
    Ma](../Images/2edf55456a1f92724535a1441fa2bef5.png)](https://amyma101.medium.com/?source=post_page-----9c7affe6f920--------------------------------)[](https://towardsdatascience.com/?source=post_page-----9c7affe6f920--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----9c7affe6f920--------------------------------)
    [Amy Ma](https://amyma101.medium.com/?source=post_page-----9c7affe6f920--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://amyma101.medium.com/?source=post_page-----9c7affe6f920--------------------------------)[![è‰¾ç±³Â·é©¬](../Images/2edf55456a1f92724535a1441fa2bef5.png)](https://amyma101.medium.com/?source=post_page-----9c7affe6f920--------------------------------)[](https://towardsdatascience.com/?source=post_page-----9c7affe6f920--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----9c7affe6f920--------------------------------)
    [è‰¾ç±³Â·é©¬](https://amyma101.medium.com/?source=post_page-----9c7affe6f920--------------------------------)'
- en: Â·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Â·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fd6d8df787b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-l1-l2-regularization-part-1-9c7affe6f920&user=Amy+Ma&userId=d6d8df787b&source=post_page-d6d8df787b----9c7affe6f920---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----9c7affe6f920--------------------------------)
    Â·6 min readÂ·Nov 22, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F9c7affe6f920&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-l1-l2-regularization-part-1-9c7affe6f920&user=Amy+Ma&userId=d6d8df787b&source=-----9c7affe6f920---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[å…³æ³¨](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fd6d8df787b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-l1-l2-regularization-part-1-9c7affe6f920&user=Amy+Ma&userId=d6d8df787b&source=post_page-d6d8df787b----9c7affe6f920---------------------post_header-----------)
    å‘è¡¨åœ¨ [Towards Data Science](https://towardsdatascience.com/?source=post_page-----9c7affe6f920--------------------------------)
    Â· 6 åˆ†é’Ÿé˜…è¯» Â· 2023å¹´11æœˆ22æ—¥[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F9c7affe6f920&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-l1-l2-regularization-part-1-9c7affe6f920&user=Amy+Ma&userId=d6d8df787b&source=-----9c7affe6f920---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F9c7affe6f920&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-l1-l2-regularization-part-1-9c7affe6f920&source=-----9c7affe6f920---------------------bookmark_footer-----------)![](../Images/96d31baeaebab0907c5ebf3211eec9a3.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F9c7affe6f920&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-l1-l2-regularization-part-1-9c7affe6f920&source=-----9c7affe6f920---------------------bookmark_footer-----------)![](../Images/96d31baeaebab0907c5ebf3211eec9a3.png)'
- en: Photo by [Holly Mandarich](https://unsplash.com/@hollymandarich?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ç‰‡ç”± [éœè‰Â·æ›¼è¾¾é‡Œå¥‡](https://unsplash.com/@hollymandarich?utm_source=medium&utm_medium=referral)
    æä¾›ï¼Œæ¥è‡ª [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: Welcome to the â€˜[Courage to learn MLâ€™](/towardsdatascience.com/tagged/courage-to-learn-ml),
    where we kick off with an exploration of L1 and L2 regularization. This series
    aims to simplify complex machine learning concepts, presenting them as a relaxed
    and informative dialogue, much like the engaging style of â€œ[The Courage to Be
    Disliked](https://www.goodreads.com/book/show/43306206-the-courage-to-be-disliked),â€
    but with a focus on ML.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: æ¬¢è¿æ¥åˆ°â€œ[å‹‡æ•¢å­¦ä¹ æœºå™¨å­¦ä¹ ](towardsdatascience.com/tagged/courage-to-learn-ml)â€ï¼Œæˆ‘ä»¬å°†ä»æ¢ç´¢ L1
    å’Œ L2 æ­£åˆ™åŒ–å¼€å§‹ã€‚æœ¬ç³»åˆ—æ—¨åœ¨ç®€åŒ–å¤æ‚çš„æœºå™¨å­¦ä¹ æ¦‚å¿µï¼Œä»¥è½»æ¾å’Œä¿¡æ¯ä¸°å¯Œçš„å¯¹è¯å‘ˆç°ï¼Œç±»ä¼¼äº â€œ[å‹‡æ•¢åšè‡ªå·±](https://www.goodreads.com/book/show/43306206-the-courage-to-be-disliked)â€
    çš„é£æ ¼ï¼Œä½†èšç„¦äºæœºå™¨å­¦ä¹ ã€‚
- en: These Q&A sessions are a reflection of my own learning path, which Iâ€™m excited
    to share with you. Think of this as a blog chronicling my journey into the depths
    of machine learning. Your interactions â€” likes, comments, and follows â€” go beyond
    just supporting; theyâ€™re the motivation that fuels the continuation of this series
    and my sharing process.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›é—®ç­”ç¯èŠ‚åæ˜ äº†æˆ‘ä¸ªäººçš„å­¦ä¹ å†ç¨‹ï¼Œæˆ‘å¾ˆé«˜å…´ä¸ä½ åˆ†äº«ã€‚æŠŠå®ƒçœ‹ä½œæ˜¯ä¸€ä¸ªè®°å½•æˆ‘æ·±å…¥æœºå™¨å­¦ä¹ çš„åšå®¢ã€‚ä½ çš„äº’åŠ¨â€”â€”ç‚¹èµã€è¯„è®ºå’Œå…³æ³¨â€”â€”ä¸ä»…ä»…æ˜¯æ”¯æŒï¼Œå®ƒä»¬è¿˜æ˜¯æ¨åŠ¨è¿™ä¸€ç³»åˆ—å†…å®¹ç»§ç»­ä¸‹å»å’Œæˆ‘åˆ†äº«è¿‡ç¨‹çš„åŠ¨åŠ›ã€‚
- en: Todayâ€™s discussion goes beyond merely reviewing the formulas and properties
    of L1 and L2 regularization. Weâ€™re delving into the core reasons why these methods
    are used in machine learning. If youâ€™re seeking to truly understand these concepts,
    youâ€™re in the right place for some enlightening insights!
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: ä»Šå¤©çš„è®¨è®ºä¸ä»…ä»…æ˜¯å›é¡¾ L1 å’Œ L2 æ­£åˆ™åŒ–çš„å…¬å¼å’Œæ€§è´¨ã€‚æˆ‘ä»¬å°†æ·±å…¥æ¢è®¨ä¸ºä»€ä¹ˆåœ¨æœºå™¨å­¦ä¹ ä¸­ä½¿ç”¨è¿™äº›æ–¹æ³•çš„æ ¸å¿ƒåŸå› ã€‚å¦‚æœä½ å¸Œæœ›çœŸæ­£ç†è§£è¿™äº›æ¦‚å¿µï¼Œä½ æ¥å¯¹åœ°æ–¹äº†ï¼Œè¿™é‡Œæœ‰ä¸€äº›å¯å‘æ€§çš„è§è§£ï¼
- en: 'In this post, weâ€™ll be answering the following questions:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬å°†å›ç­”ä»¥ä¸‹é—®é¢˜ï¼š
- en: What is regularization? Why we need it?
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä»€ä¹ˆæ˜¯æ­£åˆ™åŒ–ï¼Ÿæˆ‘ä»¬ä¸ºä»€ä¹ˆéœ€è¦å®ƒï¼Ÿ
- en: What is L1, L2 regularization?
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä»€ä¹ˆæ˜¯ L1 å’Œ L2 æ­£åˆ™åŒ–ï¼Ÿ
- en: Why do we prefer smaller coefficients over large ones? How do large coefficients
    equate to increased model complexity?
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¸ºä»€ä¹ˆæˆ‘ä»¬æ›´å€¾å‘äºä½¿ç”¨å°ç³»æ•°è€Œä¸æ˜¯å¤§ç³»æ•°ï¼Ÿå¤§ç³»æ•°å¦‚ä½•å¯¼è‡´æ¨¡å‹å¤æ‚åº¦å¢åŠ ï¼Ÿ
- en: Why there are multiple combinations of weights and biases in a neural network?
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¸ºä»€ä¹ˆç¥ç»ç½‘ç»œä¸­å­˜åœ¨å¤šç§æƒé‡å’Œåç½®ç»„åˆï¼Ÿ
- en: Why arenâ€™t bias terms penalized in L1 and L2 regularization?
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¸ºä»€ä¹ˆåœ¨ L1 å’Œ L2 æ­£åˆ™åŒ–ä¸­åç½®é¡¹ä¸å—æƒ©ç½šï¼Ÿ
- en: What is regularization? Why we need it?
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ä»€ä¹ˆæ˜¯æ­£åˆ™åŒ–ï¼Ÿæˆ‘ä»¬ä¸ºä»€ä¹ˆéœ€è¦å®ƒï¼Ÿ
- en: Regularization is a cornerstone technique in machine learning, designed to **prevent
    models from overfitting**. Overfitting occurs when a model, often too complex,
    doesnâ€™t just learn from the underlying patterns (signals) in the training data,
    but also picks up and amplifies the noise. This results in a model that performs
    well on training data but poorly on unseen data.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: æ­£åˆ™åŒ–æ˜¯æœºå™¨å­¦ä¹ ä¸­çš„ä¸€ä¸ªåŸºç¡€æŠ€æœ¯ï¼Œæ—¨åœ¨**é˜²æ­¢æ¨¡å‹è¿‡æ‹Ÿåˆ**ã€‚è¿‡æ‹Ÿåˆå‘ç”Ÿåœ¨ä¸€ä¸ªæ¨¡å‹ï¼Œé€šå¸¸æ˜¯è¿‡äºå¤æ‚ï¼Œä¸ä»…ä»è®­ç»ƒæ•°æ®ä¸­çš„æ½œåœ¨æ¨¡å¼ï¼ˆä¿¡å·ï¼‰ä¸­å­¦ä¹ ï¼Œè¿˜ä¼šæ•æ‰å¹¶æ”¾å¤§å™ªå£°ã€‚è¿™ä¼šå¯¼è‡´æ¨¡å‹åœ¨è®­ç»ƒæ•°æ®ä¸Šè¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨æœªè§è¿‡çš„æ•°æ®ä¸Šè¡¨ç°è¾ƒå·®ã€‚
- en: '**What is L1, L2 regularization?**'
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**ä»€ä¹ˆæ˜¯ L1 å’Œ L2 æ­£åˆ™åŒ–ï¼Ÿ**'
- en: There are multiple ways to prevent overfitting. L1, L2 regularization is mainly
    addresses overfitting by adding a penalty term on coefficients to the modelâ€™s
    loss function. This penalty discourages the model from assigning too much importance
    to any single feature (represented by large coefficients), thereby simplifying
    the model. In essence, regularization keeps the model balanced and focused on
    the true signal, enhancing its ability to generalize to unseen data.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: æœ‰å¤šç§æ–¹æ³•å¯ä»¥é˜²æ­¢è¿‡æ‹Ÿåˆã€‚L1 å’Œ L2 æ­£åˆ™åŒ–ä¸»è¦é€šè¿‡åœ¨æ¨¡å‹çš„æŸå¤±å‡½æ•°ä¸Šæ·»åŠ æƒ©ç½šé¡¹æ¥è§£å†³è¿‡æ‹Ÿåˆé—®é¢˜ã€‚è¿™ç§æƒ©ç½šé¡¹é˜»æ­¢æ¨¡å‹å¯¹ä»»ä½•å•ä¸€ç‰¹å¾ï¼ˆç”±å¤§ç³»æ•°è¡¨ç¤ºï¼‰èµ‹äºˆè¿‡å¤šçš„é‡è¦æ€§ï¼Œä»è€Œç®€åŒ–æ¨¡å‹ã€‚æœ¬è´¨ä¸Šï¼Œæ­£åˆ™åŒ–ä¿æŒæ¨¡å‹å¹³è¡¡ï¼Œä¸“æ³¨äºçœŸæ­£çš„ä¿¡å·ï¼Œæé«˜å…¶å¯¹æœªè§æ•°æ®çš„æ³›åŒ–èƒ½åŠ›ã€‚
- en: Wait, why exactly do we impose a penalty on large weights in our models? How
    do large coefficients equate to increased model complexity?
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç­‰ç­‰ï¼Œæˆ‘ä»¬ä¸ºä»€ä¹ˆè¦å¯¹æ¨¡å‹ä¸­çš„å¤§æƒé‡æ–½åŠ æƒ©ç½šï¼Ÿå¤§ç³»æ•°å¦‚ä½•å¯¼è‡´æ¨¡å‹å¤æ‚åº¦å¢åŠ ï¼Ÿ
- en: While there are many combinations that can minimize the loss function, not all
    are equally good for generalization. Large coefficients tend to amplify both the
    useful information (signal) and the unwanted noise in the data. This amplification
    makes the model sensitive to small changes in the input, leading it to overemphasize
    noise. As a result, it cannot perform well on new, unseen data.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: è™½ç„¶æœ‰å¾ˆå¤šç»„åˆå¯ä»¥æœ€å°åŒ–æŸå¤±å‡½æ•°ï¼Œä½†å¹¶éæ‰€æœ‰çš„ç»„åˆéƒ½å¯¹æ³›åŒ–æ•ˆæœåŒæ ·è‰¯å¥½ã€‚å¤§ç³»æ•°å¾€å¾€ä¼šæ”¾å¤§æ•°æ®ä¸­çš„æœ‰ç”¨ä¿¡æ¯ï¼ˆä¿¡å·ï¼‰å’Œä¸å¿…è¦çš„å™ªå£°ã€‚è¿™ç§æ”¾å¤§ä½¿å¾—æ¨¡å‹å¯¹è¾“å…¥ä¸­çš„å°å˜åŒ–éå¸¸æ•æ„Ÿï¼Œå¯¼è‡´å®ƒè¿‡åº¦å¼ºè°ƒå™ªå£°ã€‚å› æ­¤ï¼Œæ¨¡å‹åœ¨æ–°æ•°æ®ä¸Šçš„è¡¨ç°ä¸ä½³ã€‚
- en: Smaller coefficients, on the other hand, help the model to focus on the more
    significant, broader patterns in the data, reducing its sensitivity to minor fluctuations.
    This approach promotes a better balance, allowing the model to generalize more
    effectively.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: ç›¸åï¼Œå°ç³»æ•°æœ‰åŠ©äºæ¨¡å‹ä¸“æ³¨äºæ•°æ®ä¸­æ›´é‡è¦ã€æ›´å¹¿æ³›çš„æ¨¡å¼ï¼Œä»è€Œå‡å°‘å¯¹ç»†å¾®æ³¢åŠ¨çš„æ•æ„Ÿæ€§ã€‚è¿™ç§æ–¹æ³•ä¿ƒè¿›äº†æ›´å¥½çš„å¹³è¡¡ï¼Œä½¿æ¨¡å‹èƒ½æ›´æœ‰æ•ˆåœ°æ³›åŒ–ã€‚
- en: Consider an example where a neural network is trained to predict a catâ€™s weight.
    If one model has a coefficient of 10 and another a vastly larger one of 1000,
    their outputs for the next layer would be drastically different â€” 300 and 30000,
    respectively. The model with the larger coefficient is more prone to making extreme
    predictions. In cases where 30lbs is an outlier (which is quite unusual for a
    cat!), the second model with the larger coefficient would yield significantly
    less accurate results. This example illustrates the importance of moderating coefficients
    to avoid exaggerated responses to outliers in the data.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸¾ä¸ªä¾‹å­ï¼Œå‡è®¾ä¸€ä¸ªç¥ç»ç½‘ç»œè¢«è®­ç»ƒç”¨æ¥é¢„æµ‹çŒ«çš„ä½“é‡ã€‚å¦‚æœä¸€ä¸ªæ¨¡å‹çš„ç³»æ•°ä¸º10ï¼Œè€Œå¦ä¸€ä¸ªæ¨¡å‹çš„ç³»æ•°å¤§å¾—å¤šä¸º1000ï¼Œé‚£ä¹ˆå®ƒä»¬å¯¹ä¸‹ä¸€å±‚çš„è¾“å‡ºå°†å¤§ç›¸å¾„åº­â€”â€”åˆ†åˆ«ä¸º300å’Œ30000ã€‚ç³»æ•°è¾ƒå¤§çš„æ¨¡å‹æ›´å®¹æ˜“åšå‡ºæç«¯é¢„æµ‹ã€‚åœ¨30ç£…ä½œä¸ºå¼‚å¸¸å€¼ï¼ˆå¯¹çŒ«æ¥è¯´éå¸¸ä¸å¯»å¸¸ï¼ï¼‰çš„æƒ…å†µä¸‹ï¼Œç¬¬äºŒä¸ªç³»æ•°è¾ƒå¤§çš„æ¨¡å‹ä¼šäº§ç”Ÿæ˜æ˜¾ä¸å‡†ç¡®çš„ç»“æœã€‚è¿™ä¸ªä¾‹å­è¯´æ˜äº†è°ƒèŠ‚ç³»æ•°çš„é‡è¦æ€§ï¼Œä»¥é¿å…å¯¹æ•°æ®ä¸­çš„å¼‚å¸¸å€¼åšå‡ºå¤¸å¼ çš„ååº”ã€‚
- en: '**Could you elaborate on why there are multiple combinations of weights and
    biases in a neural network?**'
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**ä½ èƒ½è¯¦ç»†è§£é‡Šä¸€ä¸‹ä¸ºä»€ä¹ˆç¥ç»ç½‘ç»œä¸­æœ‰å¤šä¸ªæƒé‡å’Œåç½®ç»„åˆå—ï¼Ÿ**'
- en: 'Imagine navigating the complex terrain of a neural networkâ€™s loss function,
    where your mission is to find the lowest point, or a â€˜minimaâ€™. Hereâ€™s what you
    might encounter:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: æƒ³è±¡ä¸€ä¸‹åœ¨ç¥ç»ç½‘ç»œæŸå¤±å‡½æ•°çš„å¤æ‚åœ°å½¢ä¸­å¯¼èˆªï¼Œä½ çš„ä»»åŠ¡æ˜¯æ‰¾åˆ°æœ€ä½ç‚¹ï¼Œå³â€˜æœ€å°å€¼â€™ã€‚ä½ å¯èƒ½ä¼šé‡åˆ°ä»¥ä¸‹æƒ…å†µï¼š
- en: '![](../Images/fdca2f0a2e22f36e1a31b47f951957ab.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fdca2f0a2e22f36e1a31b47f951957ab.png)'
- en: Photo by [Tamas Tuzes-Katai](https://unsplash.com/@tamas_tuzeskatai?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: ç…§ç‰‡ç”±[Tamas Tuzes-Katai](https://unsplash.com/@tamas_tuzeskatai?utm_source=medium&utm_medium=referral)æ‹æ‘„ï¼Œå‘å¸ƒåœ¨[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)ä¸Šã€‚
- en: '**A Landscape of Multiple Destinations**: As you traverse this landscape, youâ€™ll
    notice itâ€™s filled with various local minima, much like a non-convex terrain with
    many dips and valleys. This is because the loss function of a neural network with
    multiple hidden layers, is inherently non-convex. Each local minima represents
    a different combination of weights and biases, offering multiple potential solutions.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**å¤šé‡ç›®çš„åœ°çš„é£æ™¯**ï¼šå½“ä½ ç©¿è¶Šè¿™ç‰‡é£æ™¯æ—¶ï¼Œä½ ä¼šå‘ç°å®ƒå……æ»¡äº†å„ç§å±€éƒ¨æœ€å°å€¼ï¼Œå°±åƒä¸€ä¸ªå…·æœ‰è®¸å¤šå‡¹é™·å’Œå±±è°·çš„éå‡¸åœ°å½¢ã€‚è¿™æ˜¯å› ä¸ºå…·æœ‰å¤šä¸ªéšè—å±‚çš„ç¥ç»ç½‘ç»œçš„æŸå¤±å‡½æ•°æœ¬è´¨ä¸Šæ˜¯éå‡¸çš„ã€‚æ¯ä¸ªå±€éƒ¨æœ€å°å€¼ä»£è¡¨äº†ä¸åŒçš„æƒé‡å’Œåç½®ç»„åˆï¼Œæä¾›äº†å¤šç§æ½œåœ¨çš„è§£å†³æ–¹æ¡ˆã€‚'
- en: '**Various Routes to the Same Destination**: The networkâ€™s non-linear activation
    functions enable it to form intricate patterns, approximating the real underlying
    function of the data. With several layers of these functions, there are numerous
    ways to represent the same truth, each way characterized by a distinct set of
    weights and biases. This is the redundancy in network design.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**å¤šæ¡è·¯å¾„åˆ°è¾¾åŒä¸€ç›®çš„åœ°**ï¼šç½‘ç»œçš„éçº¿æ€§æ¿€æ´»å‡½æ•°ä½¿å…¶èƒ½å¤Ÿå½¢æˆå¤æ‚çš„æ¨¡å¼ï¼Œè¿‘ä¼¼æ•°æ®çš„å®é™…åŸºç¡€å‡½æ•°ã€‚é€šè¿‡å¤šä¸ªè¿™äº›å‡½æ•°çš„å±‚ï¼Œæœ‰è®¸å¤šæ–¹å¼æ¥è¡¨ç¤ºåŒä¸€çœŸç›¸ï¼Œæ¯ç§æ–¹å¼ç”±ä¸€ç»„ç‹¬ç‰¹çš„æƒé‡å’Œåç½®ç‰¹å¾ã€‚è¿™å°±æ˜¯ç½‘ç»œè®¾è®¡ä¸­çš„å†—ä½™ã€‚'
- en: '**Flexibility in Sequence**: Imagine altering the sequence of your journey,
    like swapping the order of biking and taking a bus, yet still arriving at the
    same destination. Relating this to a neural network with two hidden layers: if
    you double the weights and biases in the first layer and then halve them in the
    second layer, the final output remains unchanged. (Note this flexibility, however,
    mainly applies to activation functions with some linear characteristics, like
    ReLU, but not to others like sigmoid or tanh). This phenomenon is referred to
    as â€˜scale symmetryâ€™ in neural networks.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**åºåˆ—çš„çµæ´»æ€§**ï¼šæƒ³è±¡ä¸€ä¸‹æ”¹å˜ä½ çš„æ—…ç¨‹é¡ºåºï¼Œæ¯”å¦‚äº¤æ¢éª‘è½¦å’Œä¹˜å…¬äº¤è½¦çš„é¡ºåºï¼Œä½†ä»ç„¶åˆ°è¾¾åŒä¸€ç›®çš„åœ°ã€‚ä¸å…·æœ‰ä¸¤ä¸ªéšè—å±‚çš„ç¥ç»ç½‘ç»œç›¸å…³ï¼šå¦‚æœä½ åœ¨ç¬¬ä¸€å±‚ä¸­å°†æƒé‡å’Œåç½®åŠ å€ï¼Œç„¶ååœ¨ç¬¬äºŒå±‚ä¸­å°†å®ƒä»¬å‡åŠï¼Œæœ€ç»ˆè¾“å‡ºä¿æŒä¸å˜ã€‚ï¼ˆè¯·æ³¨æ„ï¼Œè¿™ç§çµæ´»æ€§ä¸»è¦é€‚ç”¨äºå…·æœ‰æŸäº›çº¿æ€§ç‰¹å¾çš„æ¿€æ´»å‡½æ•°ï¼Œå¦‚ReLUï¼Œè€Œä¸é€‚ç”¨äºsigmoidæˆ–tanhç­‰å…¶ä»–å‡½æ•°ï¼‰ã€‚è¿™ä¸€ç°è±¡è¢«ç§°ä¸ºç¥ç»ç½‘ç»œä¸­çš„â€˜å°ºåº¦å¯¹ç§°â€™ã€‚'
- en: Iâ€™ve been reading about L1 and L2 regularization and observed that the penalty
    terms mainly focus on weights rather than biases. But why is that? Arenâ€™t biases
    also coefficients that could be penalized?
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æˆ‘ä¸€ç›´åœ¨é˜…è¯»L1å’ŒL2æ­£åˆ™åŒ–ï¼Œå¹¶è§‚å¯Ÿåˆ°æƒ©ç½šé¡¹ä¸»è¦é›†ä¸­åœ¨æƒé‡ä¸Šï¼Œè€Œä¸æ˜¯åç½®ä¸Šã€‚ä½†è¿™æ˜¯ä¸ºä»€ä¹ˆå‘¢ï¼Ÿéš¾é“åç½®ä¸æ˜¯ä¹Ÿå¯ä»¥è¢«æƒ©ç½šçš„ç³»æ•°å—ï¼Ÿ
- en: '![](../Images/be415f749b622557c43be559af9f0c16.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/be415f749b622557c43be559af9f0c16.png)'
- en: Source â€” [http://laid.delanover.com/difference-between-l1-and-l2-regularization-implementation-and-visualization-in-tensorflow/](http://laid.delanover.com/difference-between-l1-and-l2-regularization-implementation-and-visualization-in-tensorflow/)
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥æº â€” [http://laid.delanover.com/difference-between-l1-and-l2-regularization-implementation-and-visualization-in-tensorflow/](http://laid.delanover.com/difference-between-l1-and-l2-regularization-implementation-and-visualization-in-tensorflow/)
- en: In brief, the primary objective of regularization techniques like L1 and L2
    is to predominantly prevent overfitting by regulating the magnitude of the modelâ€™s
    weights (personally, I think thatâ€™s why we call them regularizations). Conversely,
    biases have a relatively modest impact on model complexity, which typically renders
    the need to penalize them unnecessary.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: ç®€è€Œè¨€ä¹‹ï¼Œåƒ L1 å’Œ L2 è¿™æ ·çš„æ­£åˆ™åŒ–æŠ€æœ¯çš„ä¸»è¦ç›®æ ‡æ˜¯é€šè¿‡è°ƒèŠ‚æ¨¡å‹æƒé‡çš„å¤§å°æ¥é˜²æ­¢è¿‡æ‹Ÿåˆï¼ˆä¸ªäººè®¤ä¸ºè¿™å°±æ˜¯æˆ‘ä»¬ç§°å…¶ä¸ºæ­£åˆ™åŒ–çš„åŸå› ï¼‰ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œåå·®å¯¹æ¨¡å‹å¤æ‚æ€§çš„å½±å“ç›¸å¯¹è¾ƒå°ï¼Œå› æ­¤é€šå¸¸ä¸éœ€è¦å¯¹å…¶æ–½åŠ æƒ©ç½šã€‚
- en: To understand better, letâ€™s look at what weights and biases do. Weights determine
    the importance of each feature in the model, affecting its complexity and the
    shape of its decision boundary in high-dimensional space. Think of them as the
    knobs that tweak the shape of the modelâ€™s decision-making process in a high-dimensional
    space, influencing how complex the model becomes.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†æ›´å¥½åœ°ç†è§£ï¼Œè®©æˆ‘ä»¬æ¥çœ‹ä¸€ä¸‹æƒé‡å’Œåå·®çš„ä½œç”¨ã€‚æƒé‡å†³å®šäº†æ¨¡å‹ä¸­æ¯ä¸ªç‰¹å¾çš„é‡è¦æ€§ï¼Œå½±å“æ¨¡å‹çš„å¤æ‚æ€§ä»¥åŠåœ¨é«˜ç»´ç©ºé—´ä¸­çš„å†³ç­–è¾¹ç•Œçš„å½¢çŠ¶ã€‚å¯ä»¥æŠŠå®ƒä»¬çœ‹ä½œæ˜¯è°ƒæ•´æ¨¡å‹å†³ç­–è¿‡ç¨‹å½¢çŠ¶çš„æ—‹é’®ï¼Œå½±å“æ¨¡å‹çš„å¤æ‚ç¨‹åº¦ã€‚
- en: Biases, however, serve a different purpose. They act like the intercept in a
    linear function, shifting the modelâ€™s output independently of the input features.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶è€Œï¼Œåå·®å…·æœ‰ä¸åŒçš„ä½œç”¨ã€‚å®ƒä»¬åƒçº¿æ€§å‡½æ•°ä¸­çš„æˆªè·ä¸€æ ·ï¼Œç‹¬ç«‹äºè¾“å…¥ç‰¹å¾æ¥è°ƒæ•´æ¨¡å‹çš„è¾“å‡ºã€‚
- en: 'Hereâ€™s the key takeaway: Overfitting occurs mainly because of the intricate
    interplay between features, and these interactions are primarily handled by weights.
    To tackle this, we apply penalties to weights, adjusting how much importance each
    feature carries and how much information the model extracts from them. This, in
    turn, reshapes the modelâ€™s landscape and, as a result, its complexity.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œæ˜¯å…³é”®ç‚¹ï¼šè¿‡æ‹Ÿåˆä¸»è¦å‘ç”Ÿåœ¨ç‰¹å¾ä¹‹é—´çš„å¤æ‚ç›¸äº’ä½œç”¨ä¸­ï¼Œè€Œè¿™äº›ç›¸äº’ä½œç”¨ä¸»è¦ç”±æƒé‡å¤„ç†ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¯¹æƒé‡æ–½åŠ æƒ©ç½šï¼Œè°ƒæ•´æ¯ä¸ªç‰¹å¾çš„é‡è¦æ€§ä»¥åŠæ¨¡å‹ä»ä¸­æå–çš„ä¿¡æ¯é‡ã€‚è¿™åè¿‡æ¥ä¼šé‡å¡‘æ¨¡å‹çš„æ ¼å±€ï¼Œä»è€Œæ”¹å˜å…¶å¤æ‚æ€§ã€‚
- en: In contrast, biases donâ€™t significantly contribute to the modelâ€™s complexity.
    Moreover, they can adapt as weights change, reducing the need for separate bias
    penalties.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: ç›¸æ¯”ä¹‹ä¸‹ï¼Œåå·®å¯¹æ¨¡å‹å¤æ‚æ€§çš„å½±å“ä¸å¤§ã€‚æ­¤å¤–ï¼Œåå·®å¯ä»¥éšç€æƒé‡çš„å˜åŒ–è€Œè°ƒæ•´ï¼Œä»è€Œå‡å°‘äº†å¯¹å•ç‹¬åå·®æƒ©ç½šçš„éœ€æ±‚ã€‚
- en: Now that youâ€™ve gained insight into the existence of multiple sets of weights
    and biases and the preference for smaller ones, weâ€™re ready to dive deeper.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ä½ å·²ç»å¯¹æƒé‡å’Œåå·®çš„å¤šä¸ªé›†åˆä»¥åŠåå¥½è¾ƒå°æƒé‡çš„åŸå› æœ‰äº†äº†è§£ï¼Œæˆ‘ä»¬å¯ä»¥æ·±å…¥æ¢è®¨ã€‚
- en: Join me in [the second part](https://medium.com/@yujing-ma45/courage-to-learn-ml-unraveling-l1-l2-regularization-part-2-1bb171e43b35)
    of the series, Iâ€™ll unravel the layers behind L1 and L2 regularization, offering
    an intuitive understanding with Lagrange multipliers (donâ€™t worry about the name,
    itâ€™s a straightforward concept ğŸ˜ƒ)
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: åŠ å…¥æˆ‘åœ¨ [ç³»åˆ—çš„ç¬¬äºŒéƒ¨åˆ†](https://medium.com/@yujing-ma45/courage-to-learn-ml-unraveling-l1-l2-regularization-part-2-1bb171e43b35)
    ä¸­ï¼Œæˆ‘å°†æ­ç¤º L1 å’Œ L2 æ­£åˆ™åŒ–èƒŒåçš„å±‚æ¬¡ï¼Œå¹¶é€šè¿‡æ‹‰æ ¼æœ—æ—¥ä¹˜å­æä¾›ç›´è§‚çš„ç†è§£ï¼ˆä¸ç”¨æ‹…å¿ƒåå­—ï¼Œè¿™ä¸ªæ¦‚å¿µå¾ˆç®€å• ğŸ˜ƒï¼‰
- en: See you there!
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ°æ—¶è§ï¼
- en: If you liked the article, you can find me on [LinkedIn](https://www.linkedin.com/in/amyma101/),
    and please donâ€™t hesitate to connect or reach out with your questions and suggestions!
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä½ å–œæ¬¢è¿™ç¯‡æ–‡ç« ï¼Œä½ å¯ä»¥åœ¨ [LinkedIn](https://www.linkedin.com/in/amyma101/) ä¸Šæ‰¾åˆ°æˆ‘ï¼Œéšæ—¶æ¬¢è¿ä½ è”ç³»æˆ‘æˆ–è€…æå‡ºé—®é¢˜å’Œå»ºè®®ï¼
