- en: 'Courage to learn ML: Demystifying L1 & L2 Regularization (part 1)'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 勇敢学习机器学习：揭示 L1 和 L2 正则化（第 1 部分）
- en: 原文：[https://towardsdatascience.com/understanding-l1-l2-regularization-part-1-9c7affe6f920?source=collection_archive---------5-----------------------#2023-11-22](https://towardsdatascience.com/understanding-l1-l2-regularization-part-1-9c7affe6f920?source=collection_archive---------5-----------------------#2023-11-22)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/understanding-l1-l2-regularization-part-1-9c7affe6f920?source=collection_archive---------5-----------------------#2023-11-22](https://towardsdatascience.com/understanding-l1-l2-regularization-part-1-9c7affe6f920?source=collection_archive---------5-----------------------#2023-11-22)
- en: Comprehend the underlying purpose of L1 and L2 regularization
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解 L1 和 L2 正则化的基本目的
- en: '[](https://amyma101.medium.com/?source=post_page-----9c7affe6f920--------------------------------)[![Amy
    Ma](../Images/2edf55456a1f92724535a1441fa2bef5.png)](https://amyma101.medium.com/?source=post_page-----9c7affe6f920--------------------------------)[](https://towardsdatascience.com/?source=post_page-----9c7affe6f920--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----9c7affe6f920--------------------------------)
    [Amy Ma](https://amyma101.medium.com/?source=post_page-----9c7affe6f920--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://amyma101.medium.com/?source=post_page-----9c7affe6f920--------------------------------)[![艾米·马](../Images/2edf55456a1f92724535a1441fa2bef5.png)](https://amyma101.medium.com/?source=post_page-----9c7affe6f920--------------------------------)[](https://towardsdatascience.com/?source=post_page-----9c7affe6f920--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----9c7affe6f920--------------------------------)
    [艾米·马](https://amyma101.medium.com/?source=post_page-----9c7affe6f920--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fd6d8df787b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-l1-l2-regularization-part-1-9c7affe6f920&user=Amy+Ma&userId=d6d8df787b&source=post_page-d6d8df787b----9c7affe6f920---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----9c7affe6f920--------------------------------)
    ·6 min read·Nov 22, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F9c7affe6f920&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-l1-l2-regularization-part-1-9c7affe6f920&user=Amy+Ma&userId=d6d8df787b&source=-----9c7affe6f920---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fd6d8df787b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-l1-l2-regularization-part-1-9c7affe6f920&user=Amy+Ma&userId=d6d8df787b&source=post_page-d6d8df787b----9c7affe6f920---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----9c7affe6f920--------------------------------)
    · 6 分钟阅读 · 2023年11月22日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F9c7affe6f920&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-l1-l2-regularization-part-1-9c7affe6f920&user=Amy+Ma&userId=d6d8df787b&source=-----9c7affe6f920---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F9c7affe6f920&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-l1-l2-regularization-part-1-9c7affe6f920&source=-----9c7affe6f920---------------------bookmark_footer-----------)![](../Images/96d31baeaebab0907c5ebf3211eec9a3.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F9c7affe6f920&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-l1-l2-regularization-part-1-9c7affe6f920&source=-----9c7affe6f920---------------------bookmark_footer-----------)![](../Images/96d31baeaebab0907c5ebf3211eec9a3.png)'
- en: Photo by [Holly Mandarich](https://unsplash.com/@hollymandarich?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由 [霍莉·曼达里奇](https://unsplash.com/@hollymandarich?utm_source=medium&utm_medium=referral)
    提供，来自 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: Welcome to the ‘[Courage to learn ML’](/towardsdatascience.com/tagged/courage-to-learn-ml),
    where we kick off with an exploration of L1 and L2 regularization. This series
    aims to simplify complex machine learning concepts, presenting them as a relaxed
    and informative dialogue, much like the engaging style of “[The Courage to Be
    Disliked](https://www.goodreads.com/book/show/43306206-the-courage-to-be-disliked),”
    but with a focus on ML.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 欢迎来到“[勇敢学习机器学习](towardsdatascience.com/tagged/courage-to-learn-ml)”，我们将从探索 L1
    和 L2 正则化开始。本系列旨在简化复杂的机器学习概念，以轻松和信息丰富的对话呈现，类似于 “[勇敢做自己](https://www.goodreads.com/book/show/43306206-the-courage-to-be-disliked)”
    的风格，但聚焦于机器学习。
- en: These Q&A sessions are a reflection of my own learning path, which I’m excited
    to share with you. Think of this as a blog chronicling my journey into the depths
    of machine learning. Your interactions — likes, comments, and follows — go beyond
    just supporting; they’re the motivation that fuels the continuation of this series
    and my sharing process.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 这些问答环节反映了我个人的学习历程，我很高兴与你分享。把它看作是一个记录我深入机器学习的博客。你的互动——点赞、评论和关注——不仅仅是支持，它们还是推动这一系列内容继续下去和我分享过程的动力。
- en: Today’s discussion goes beyond merely reviewing the formulas and properties
    of L1 and L2 regularization. We’re delving into the core reasons why these methods
    are used in machine learning. If you’re seeking to truly understand these concepts,
    you’re in the right place for some enlightening insights!
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 今天的讨论不仅仅是回顾 L1 和 L2 正则化的公式和性质。我们将深入探讨为什么在机器学习中使用这些方法的核心原因。如果你希望真正理解这些概念，你来对地方了，这里有一些启发性的见解！
- en: 'In this post, we’ll be answering the following questions:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们将回答以下问题：
- en: What is regularization? Why we need it?
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 什么是正则化？我们为什么需要它？
- en: What is L1, L2 regularization?
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 什么是 L1 和 L2 正则化？
- en: Why do we prefer smaller coefficients over large ones? How do large coefficients
    equate to increased model complexity?
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为什么我们更倾向于使用小系数而不是大系数？大系数如何导致模型复杂度增加？
- en: Why there are multiple combinations of weights and biases in a neural network?
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为什么神经网络中存在多种权重和偏置组合？
- en: Why aren’t bias terms penalized in L1 and L2 regularization?
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为什么在 L1 和 L2 正则化中偏置项不受惩罚？
- en: What is regularization? Why we need it?
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是正则化？我们为什么需要它？
- en: Regularization is a cornerstone technique in machine learning, designed to **prevent
    models from overfitting**. Overfitting occurs when a model, often too complex,
    doesn’t just learn from the underlying patterns (signals) in the training data,
    but also picks up and amplifies the noise. This results in a model that performs
    well on training data but poorly on unseen data.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 正则化是机器学习中的一个基础技术，旨在**防止模型过拟合**。过拟合发生在一个模型，通常是过于复杂，不仅从训练数据中的潜在模式（信号）中学习，还会捕捉并放大噪声。这会导致模型在训练数据上表现良好，但在未见过的数据上表现较差。
- en: '**What is L1, L2 regularization?**'
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**什么是 L1 和 L2 正则化？**'
- en: There are multiple ways to prevent overfitting. L1, L2 regularization is mainly
    addresses overfitting by adding a penalty term on coefficients to the model’s
    loss function. This penalty discourages the model from assigning too much importance
    to any single feature (represented by large coefficients), thereby simplifying
    the model. In essence, regularization keeps the model balanced and focused on
    the true signal, enhancing its ability to generalize to unseen data.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 有多种方法可以防止过拟合。L1 和 L2 正则化主要通过在模型的损失函数上添加惩罚项来解决过拟合问题。这种惩罚项阻止模型对任何单一特征（由大系数表示）赋予过多的重要性，从而简化模型。本质上，正则化保持模型平衡，专注于真正的信号，提高其对未见数据的泛化能力。
- en: Wait, why exactly do we impose a penalty on large weights in our models? How
    do large coefficients equate to increased model complexity?
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 等等，我们为什么要对模型中的大权重施加惩罚？大系数如何导致模型复杂度增加？
- en: While there are many combinations that can minimize the loss function, not all
    are equally good for generalization. Large coefficients tend to amplify both the
    useful information (signal) and the unwanted noise in the data. This amplification
    makes the model sensitive to small changes in the input, leading it to overemphasize
    noise. As a result, it cannot perform well on new, unseen data.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然有很多组合可以最小化损失函数，但并非所有的组合都对泛化效果同样良好。大系数往往会放大数据中的有用信息（信号）和不必要的噪声。这种放大使得模型对输入中的小变化非常敏感，导致它过度强调噪声。因此，模型在新数据上的表现不佳。
- en: Smaller coefficients, on the other hand, help the model to focus on the more
    significant, broader patterns in the data, reducing its sensitivity to minor fluctuations.
    This approach promotes a better balance, allowing the model to generalize more
    effectively.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，小系数有助于模型专注于数据中更重要、更广泛的模式，从而减少对细微波动的敏感性。这种方法促进了更好的平衡，使模型能更有效地泛化。
- en: Consider an example where a neural network is trained to predict a cat’s weight.
    If one model has a coefficient of 10 and another a vastly larger one of 1000,
    their outputs for the next layer would be drastically different — 300 and 30000,
    respectively. The model with the larger coefficient is more prone to making extreme
    predictions. In cases where 30lbs is an outlier (which is quite unusual for a
    cat!), the second model with the larger coefficient would yield significantly
    less accurate results. This example illustrates the importance of moderating coefficients
    to avoid exaggerated responses to outliers in the data.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 举个例子，假设一个神经网络被训练用来预测猫的体重。如果一个模型的系数为10，而另一个模型的系数大得多为1000，那么它们对下一层的输出将大相径庭——分别为300和30000。系数较大的模型更容易做出极端预测。在30磅作为异常值（对猫来说非常不寻常！）的情况下，第二个系数较大的模型会产生明显不准确的结果。这个例子说明了调节系数的重要性，以避免对数据中的异常值做出夸张的反应。
- en: '**Could you elaborate on why there are multiple combinations of weights and
    biases in a neural network?**'
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**你能详细解释一下为什么神经网络中有多个权重和偏置组合吗？**'
- en: 'Imagine navigating the complex terrain of a neural network’s loss function,
    where your mission is to find the lowest point, or a ‘minima’. Here’s what you
    might encounter:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下在神经网络损失函数的复杂地形中导航，你的任务是找到最低点，即‘最小值’。你可能会遇到以下情况：
- en: '![](../Images/fdca2f0a2e22f36e1a31b47f951957ab.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fdca2f0a2e22f36e1a31b47f951957ab.png)'
- en: Photo by [Tamas Tuzes-Katai](https://unsplash.com/@tamas_tuzeskatai?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 照片由[Tamas Tuzes-Katai](https://unsplash.com/@tamas_tuzeskatai?utm_source=medium&utm_medium=referral)拍摄，发布在[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)上。
- en: '**A Landscape of Multiple Destinations**: As you traverse this landscape, you’ll
    notice it’s filled with various local minima, much like a non-convex terrain with
    many dips and valleys. This is because the loss function of a neural network with
    multiple hidden layers, is inherently non-convex. Each local minima represents
    a different combination of weights and biases, offering multiple potential solutions.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**多重目的地的风景**：当你穿越这片风景时，你会发现它充满了各种局部最小值，就像一个具有许多凹陷和山谷的非凸地形。这是因为具有多个隐藏层的神经网络的损失函数本质上是非凸的。每个局部最小值代表了不同的权重和偏置组合，提供了多种潜在的解决方案。'
- en: '**Various Routes to the Same Destination**: The network’s non-linear activation
    functions enable it to form intricate patterns, approximating the real underlying
    function of the data. With several layers of these functions, there are numerous
    ways to represent the same truth, each way characterized by a distinct set of
    weights and biases. This is the redundancy in network design.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**多条路径到达同一目的地**：网络的非线性激活函数使其能够形成复杂的模式，近似数据的实际基础函数。通过多个这些函数的层，有许多方式来表示同一真相，每种方式由一组独特的权重和偏置特征。这就是网络设计中的冗余。'
- en: '**Flexibility in Sequence**: Imagine altering the sequence of your journey,
    like swapping the order of biking and taking a bus, yet still arriving at the
    same destination. Relating this to a neural network with two hidden layers: if
    you double the weights and biases in the first layer and then halve them in the
    second layer, the final output remains unchanged. (Note this flexibility, however,
    mainly applies to activation functions with some linear characteristics, like
    ReLU, but not to others like sigmoid or tanh). This phenomenon is referred to
    as ‘scale symmetry’ in neural networks.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**序列的灵活性**：想象一下改变你的旅程顺序，比如交换骑车和乘公交车的顺序，但仍然到达同一目的地。与具有两个隐藏层的神经网络相关：如果你在第一层中将权重和偏置加倍，然后在第二层中将它们减半，最终输出保持不变。（请注意，这种灵活性主要适用于具有某些线性特征的激活函数，如ReLU，而不适用于sigmoid或tanh等其他函数）。这一现象被称为神经网络中的‘尺度对称’。'
- en: I’ve been reading about L1 and L2 regularization and observed that the penalty
    terms mainly focus on weights rather than biases. But why is that? Aren’t biases
    also coefficients that could be penalized?
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 我一直在阅读L1和L2正则化，并观察到惩罚项主要集中在权重上，而不是偏置上。但这是为什么呢？难道偏置不是也可以被惩罚的系数吗？
- en: '![](../Images/be415f749b622557c43be559af9f0c16.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/be415f749b622557c43be559af9f0c16.png)'
- en: Source — [http://laid.delanover.com/difference-between-l1-and-l2-regularization-implementation-and-visualization-in-tensorflow/](http://laid.delanover.com/difference-between-l1-and-l2-regularization-implementation-and-visualization-in-tensorflow/)
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 来源 — [http://laid.delanover.com/difference-between-l1-and-l2-regularization-implementation-and-visualization-in-tensorflow/](http://laid.delanover.com/difference-between-l1-and-l2-regularization-implementation-and-visualization-in-tensorflow/)
- en: In brief, the primary objective of regularization techniques like L1 and L2
    is to predominantly prevent overfitting by regulating the magnitude of the model’s
    weights (personally, I think that’s why we call them regularizations). Conversely,
    biases have a relatively modest impact on model complexity, which typically renders
    the need to penalize them unnecessary.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，像 L1 和 L2 这样的正则化技术的主要目标是通过调节模型权重的大小来防止过拟合（个人认为这就是我们称其为正则化的原因）。相比之下，偏差对模型复杂性的影响相对较小，因此通常不需要对其施加惩罚。
- en: To understand better, let’s look at what weights and biases do. Weights determine
    the importance of each feature in the model, affecting its complexity and the
    shape of its decision boundary in high-dimensional space. Think of them as the
    knobs that tweak the shape of the model’s decision-making process in a high-dimensional
    space, influencing how complex the model becomes.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解，让我们来看一下权重和偏差的作用。权重决定了模型中每个特征的重要性，影响模型的复杂性以及在高维空间中的决策边界的形状。可以把它们看作是调整模型决策过程形状的旋钮，影响模型的复杂程度。
- en: Biases, however, serve a different purpose. They act like the intercept in a
    linear function, shifting the model’s output independently of the input features.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，偏差具有不同的作用。它们像线性函数中的截距一样，独立于输入特征来调整模型的输出。
- en: 'Here’s the key takeaway: Overfitting occurs mainly because of the intricate
    interplay between features, and these interactions are primarily handled by weights.
    To tackle this, we apply penalties to weights, adjusting how much importance each
    feature carries and how much information the model extracts from them. This, in
    turn, reshapes the model’s landscape and, as a result, its complexity.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是关键点：过拟合主要发生在特征之间的复杂相互作用中，而这些相互作用主要由权重处理。为了解决这个问题，我们对权重施加惩罚，调整每个特征的重要性以及模型从中提取的信息量。这反过来会重塑模型的格局，从而改变其复杂性。
- en: In contrast, biases don’t significantly contribute to the model’s complexity.
    Moreover, they can adapt as weights change, reducing the need for separate bias
    penalties.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，偏差对模型复杂性的影响不大。此外，偏差可以随着权重的变化而调整，从而减少了对单独偏差惩罚的需求。
- en: Now that you’ve gained insight into the existence of multiple sets of weights
    and biases and the preference for smaller ones, we’re ready to dive deeper.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经对权重和偏差的多个集合以及偏好较小权重的原因有了了解，我们可以深入探讨。
- en: Join me in [the second part](https://medium.com/@yujing-ma45/courage-to-learn-ml-unraveling-l1-l2-regularization-part-2-1bb171e43b35)
    of the series, I’ll unravel the layers behind L1 and L2 regularization, offering
    an intuitive understanding with Lagrange multipliers (don’t worry about the name,
    it’s a straightforward concept 😃)
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 加入我在 [系列的第二部分](https://medium.com/@yujing-ma45/courage-to-learn-ml-unraveling-l1-l2-regularization-part-2-1bb171e43b35)
    中，我将揭示 L1 和 L2 正则化背后的层次，并通过拉格朗日乘子提供直观的理解（不用担心名字，这个概念很简单 😃）
- en: See you there!
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 到时见！
- en: If you liked the article, you can find me on [LinkedIn](https://www.linkedin.com/in/amyma101/),
    and please don’t hesitate to connect or reach out with your questions and suggestions!
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你喜欢这篇文章，你可以在 [LinkedIn](https://www.linkedin.com/in/amyma101/) 上找到我，随时欢迎你联系我或者提出问题和建议！
