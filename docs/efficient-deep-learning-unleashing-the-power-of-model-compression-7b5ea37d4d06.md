# é«˜æ•ˆæ·±åº¦å­¦ä¹ ï¼šé‡Šæ”¾æ¨¡å‹å‹ç¼©çš„åŠ›é‡

> åŸæ–‡ï¼š[`towardsdatascience.com/efficient-deep-learning-unleashing-the-power-of-model-compression-7b5ea37d4d06`](https://towardsdatascience.com/efficient-deep-learning-unleashing-the-power-of-model-compression-7b5ea37d4d06)

![](img/e3f104a88962263b5a88baa0f644362f.png)

å›¾ç‰‡æ¥æºï¼šä½œè€…

## åŠ é€Ÿç”Ÿäº§ä¸­çš„æ¨¡å‹æ¨ç†é€Ÿåº¦

[](https://medium.com/@marcellopoliti?source=post_page-----7b5ea37d4d06--------------------------------)![Marcello Politi](https://medium.com/@marcellopoliti?source=post_page-----7b5ea37d4d06--------------------------------)[](https://towardsdatascience.com/?source=post_page-----7b5ea37d4d06--------------------------------)![Towards Data Science](https://towardsdatascience.com/?source=post_page-----7b5ea37d4d06--------------------------------) [Marcello Politi](https://medium.com/@marcellopoliti?source=post_page-----7b5ea37d4d06--------------------------------)

Â·å‘è¡¨äº[Towards Data Science](https://towardsdatascience.com/?source=post_page-----7b5ea37d4d06--------------------------------) Â·9 åˆ†é’Ÿé˜…è¯»Â·2023 å¹´ 9 æœˆ 3 æ—¥

--

## ä»‹ç»

å½“æœºå™¨å­¦ä¹ æ¨¡å‹æŠ•å…¥ç”Ÿäº§æ—¶ï¼Œé€šå¸¸ä¼šæœ‰ä¸€äº›åœ¨æ¨¡å‹åŸå‹é˜¶æ®µæœªè€ƒè™‘çš„è¦æ±‚ã€‚ä¾‹å¦‚ï¼Œç”Ÿäº§ä¸­çš„æ¨¡å‹å¿…é¡»å¤„ç†æ¥è‡ªä¸åŒç”¨æˆ·çš„å¤§é‡è¯·æ±‚ã€‚å› æ­¤ï¼Œä½ éœ€è¦ä¼˜åŒ–ä¾‹å¦‚å»¶è¿Ÿå’Œ/æˆ–ååé‡ã€‚

+   **å»¶è¿Ÿ**ï¼šæ˜¯å®Œæˆä»»åŠ¡æ‰€éœ€çš„æ—¶é—´ï¼Œä¾‹å¦‚ç‚¹å‡»é“¾æ¥ååŠ è½½ç½‘é¡µæ‰€éœ€çš„æ—¶é—´ã€‚è¿™æ˜¯ä»å¼€å§‹æŸé¡¹å·¥ä½œåˆ°çœ‹åˆ°ç»“æœçš„ç­‰å¾…æ—¶é—´ã€‚

+   **ååé‡**ï¼šæ˜¯ç³»ç»Ÿåœ¨ä¸€å®šæ—¶é—´å†…èƒ½å¤Ÿå¤„ç†çš„è¯·æ±‚æ•°é‡ã€‚

è¿™æ„å‘³ç€æœºå™¨å­¦ä¹ æ¨¡å‹å¿…é¡»éå¸¸å¿«é€Ÿåœ°è¿›è¡Œé¢„æµ‹ï¼Œä¸ºæ­¤æœ‰å„ç§æŠ€æœ¯å¯ä»¥æé«˜æ¨¡å‹æ¨ç†çš„é€Ÿåº¦ï¼Œè®©æˆ‘ä»¬åœ¨æœ¬æ–‡ä¸­æ¢è®¨å…¶ä¸­æœ€é‡è¦çš„å‡ ç§ã€‚

# æ¨¡å‹å‹ç¼©

æœ‰ä¸€äº›æŠ€æœ¯æ—¨åœ¨ä½¿**æ¨¡å‹æ›´å°**ï¼Œè¿™å°±æ˜¯ä¸ºä»€ä¹ˆå®ƒä»¬è¢«ç§°ä¸º**æ¨¡å‹å‹ç¼©**æŠ€æœ¯ï¼Œè€Œå…¶ä»–æŠ€æœ¯åˆ™ä¸“æ³¨äºä½¿æ¨¡å‹**æ¨ç†æ›´å¿«**ï¼Œå› æ­¤å±äº**æ¨¡å‹ä¼˜åŒ–**é¢†åŸŸã€‚

ä½†é€šå¸¸ä½¿æ¨¡å‹æ›´å°ä¹Ÿæœ‰åŠ©äºæ¨ç†é€Ÿåº¦ï¼Œå› æ­¤è¿™ä¸¤ä¸ªç ”ç©¶é¢†åŸŸä¹‹é—´çš„ç•Œé™éå¸¸æ¨¡ç³Šã€‚

## ä½ç§©åˆ†è§£

è¿™æ˜¯æˆ‘ä»¬çœ‹åˆ°çš„ç¬¬ä¸€ç§æ–¹æ³•ï¼Œå®é™…ä¸Šè¿™æ–¹é¢çš„ç ”ç©¶å¾ˆå¤šï¼Œæœ€è¿‘æœ‰è®¸å¤šç›¸å…³è®ºæ–‡å‘è¡¨ã€‚

**åŸºæœ¬æ€æƒ³æ˜¯å°†ç¥ç»ç½‘ç»œä¸­çš„çŸ©é˜µ**ï¼ˆä»£è¡¨ç½‘ç»œå±‚çš„çŸ©é˜µï¼‰**æ›¿æ¢ä¸ºå…·æœ‰è¾ƒä½ç»´åº¦çš„çŸ©é˜µ**ï¼Œå°½ç®¡æ›´å‡†ç¡®çš„è¯´æ³•æ˜¯å¼ é‡ï¼Œå› ä¸ºæˆ‘ä»¬å¸¸å¸¸ä¼šæœ‰è¶…è¿‡ 2 ç»´çš„çŸ©é˜µã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œæˆ‘ä»¬å°†æ‹¥æœ‰æ›´å°‘çš„ç½‘ç»œå‚æ•°å’Œæ›´å¿«çš„æ¨ç†é€Ÿåº¦ã€‚

ä¸€ä¸ªç®€å•çš„ä¾‹å­æ˜¯åœ¨ CNN ç½‘ç»œä¸­ç”¨ 1x1 å·ç§¯æ›¿ä»£ 3x3 å·ç§¯ã€‚è¿™æ ·çš„æŠ€æœ¯è¢«[ SqueezeNet](https://arxiv.org/abs/1602.07360)ç­‰ç½‘ç»œä½¿ç”¨ã€‚

æœ€è¿‘ï¼Œç±»ä¼¼çš„æ€æƒ³è¢«åº”ç”¨äºå…¶ä»–ç›®çš„ï¼Œä¾‹å¦‚åœ¨èµ„æºæœ‰é™çš„æƒ…å†µä¸‹å…è®¸å¯¹å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œå¾®è°ƒã€‚

åœ¨å¯¹é¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œå¾®è°ƒä»¥ç”¨äºä¸‹æ¸¸ä»»åŠ¡æ—¶ï¼Œä»éœ€å¯¹é¢„è®­ç»ƒæ¨¡å‹çš„æ‰€æœ‰å‚æ•°è¿›è¡Œè®­ç»ƒï¼Œè¿™å¯èƒ½ä¼šéå¸¸æ˜‚è´µã€‚

æ‰€ä»¥ï¼Œåä¸ºâ€œ[å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹çš„ä½ç§©é€‚åº”](https://arxiv.org/abs/2106.09685)â€æˆ– LoRA çš„æ–¹æ³•çš„æƒ³æ³•æ˜¯å°†åŸå§‹æ¨¡å‹ä¸­çš„çŸ©é˜µæ›¿æ¢ä¸ºä¸€å¯¹å¯¹å°ºå¯¸è¾ƒå°çš„çŸ©é˜µï¼ˆä½¿ç”¨çŸ©é˜µåˆ†è§£ï¼‰ã€‚è¿™æ ·ï¼Œåªæœ‰è¿™äº›æ–°çš„çŸ©é˜µéœ€è¦é‡æ–°è®­ç»ƒï¼Œä»¥ä¾¿å°†é¢„è®­ç»ƒæ¨¡å‹è°ƒæ•´åˆ°æ›´å¤šçš„ä¸‹æ¸¸ä»»åŠ¡ä¸­ã€‚

![](img/b21d32443e3691de45e54500a439b131.png)

LoRA ä¸­çš„çŸ©é˜µåˆ†è§£ï¼ˆæ¥æºï¼š https://arxiv.org/pdf/2106.09685.pdfï¼‰

ç°åœ¨è®©æˆ‘ä»¬çœ‹çœ‹å¦‚ä½•ä½¿ç”¨ Hugging Face ğŸ¤— çš„[PEFT](https://huggingface.co/docs/peft/index)åº“æ¥å®ç°å¾®è°ƒã€‚

å‡è®¾æˆ‘ä»¬æƒ³ä½¿ç”¨ LoRA å¯¹`[bigscience/mt0-large](https://huggingface.co/bigscience/mt0-large)`è¿›è¡Œå¾®è°ƒã€‚æˆ‘ä»¬å¿…é¡»é¦–å…ˆå¤„ç†å¯¼å…¥æˆ‘ä»¬éœ€è¦çš„å†…å®¹ã€‚

```py
!pip install peft
!pip install transformers
```

```py
 from transformers import AutoModelForSeq2SeqLM
  from peft import get_peft_model, LoraConfig, TaskType

  model_name_or_path = "bigscience/mt0-large"
  tokenizer_name_or_path = "bigscience/mt0-large"
```

ä¸‹ä¸€æ­¥å°†æ˜¯åˆ›å»ºä¸€ä¸ª LoRA é…ç½®ï¼Œä»¥ä¾¿åœ¨å¾®è°ƒè¿‡ç¨‹ä¸­åº”ç”¨ã€‚

```py
peft_config = LoraConfig(
    task_type=TaskType.SEQ_2_SEQ_LM, inference_mode=False, r=8, lora_alpha=32, lora_dropout=0.1
)
```

æˆ‘ä»¬ç°åœ¨ä½¿ç”¨ Transformers åº“çš„åŸºç¡€æ¨¡å‹å’Œæˆ‘ä»¬ä¸º LoRA åˆ›å»ºçš„é…ç½®å¯¹è±¡æ¥å®ä¾‹åŒ–æ¨¡å‹ã€‚

```py
model = AutoModelForSeq2SeqLM.from_pretrained(model_name_or_path)
model = get_peft_model(model, peft_config)
model.print_trainable_parameters()
```

## çŸ¥è¯†è’¸é¦

è¿™æ˜¯å¦ä¸€ç§æ–¹æ³•ï¼Œå®ƒå…è®¸æˆ‘ä»¬å°†ä¸€ä¸ªâ€œå°å‹â€ä¸”å› æ­¤é€Ÿåº¦æ›´å¿«çš„æ¨¡å‹æŠ•å…¥ç”Ÿäº§ã€‚

è¿™ä¸ªæ–¹æ³•çš„æ ¸å¿ƒæ€æƒ³æ˜¯æœ‰ä¸€ä¸ª**ç§°ä¸ºæ•™å¸ˆçš„å¤§å‹æ¨¡å‹**å’Œä¸€ä¸ª**ç§°ä¸ºå­¦ç”Ÿçš„å°å‹æ¨¡å‹**ï¼Œæˆ‘ä»¬å°†**åˆ©ç”¨æ•™å¸ˆçš„çŸ¥è¯†æ¥æ•™å­¦ç”Ÿå¦‚ä½•è¿›è¡Œé¢„æµ‹**ã€‚è¿™æ ·æˆ‘ä»¬å°±å¯ä»¥åªå°†å­¦ç”Ÿæ¨¡å‹æŠ•å…¥ç”Ÿäº§ã€‚

![](img/e3f862323f2d9b3c00b3c9ae076faab1.png)

çŸ¥è¯†è’¸é¦ï¼ˆæ¥æºï¼š[`www.analyticsvidhya.com/blog/2022/01/knowledge-distillation-theory-and-end-to-end-case-study/`](https://www.analyticsvidhya.com/blog/2022/01/knowledge-distillation-theory-and-end-to-end-case-study/)ï¼‰

ä»¥è¿™ç§æ–¹å¼å¼€å‘çš„ç»å…¸æ¨¡å‹ç¤ºä¾‹æ˜¯[DistillBERT](https://huggingface.co/docs/transformers/model_doc/distilbert)ï¼Œå®ƒæ˜¯[ BERT](https://arxiv.org/abs/1810.04805)çš„å­¦ç”Ÿæ¨¡å‹ã€‚DistilBERT æ¯” BERT å° 40%ï¼Œä½†ä¿ç•™äº† 97%çš„è¯­è¨€ç†è§£èƒ½åŠ›ï¼Œå¹¶ä¸”åœ¨æ¨ç†æ—¶å¿« 60%ã€‚

è¿™ç§æ–¹æ³•çš„ä¸€ä¸ªç¼ºç‚¹æ˜¯ï¼Œä½ ä»ç„¶éœ€è¦æ‹¥æœ‰å¤§å‹æ•™å¸ˆæ¨¡å‹ä»¥è®­ç»ƒå­¦ç”Ÿï¼Œè€Œä½ å¯èƒ½æ²¡æœ‰èµ„æºè®­ç»ƒåƒæ•™å¸ˆé‚£æ ·çš„æ¨¡å‹ã€‚

è®©æˆ‘ä»¬çœ‹ä¸€ä¸ª Python ä¸­çš„çŸ¥è¯†è’¸é¦çš„ç®€å•ç¤ºä¾‹ã€‚ä¸€ä¸ªå…³é”®æ¦‚å¿µæ˜¯[KL æ•£åº¦](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence)ï¼Œè¿™æ˜¯ä¸€ä¸ªç†è§£ä¸¤ä¸ªåˆ†å¸ƒä¹‹é—´å·®å¼‚çš„æ•°å­¦æ¦‚å¿µï¼Œå®é™…ä¸Šåœ¨æˆ‘ä»¬çš„æ¡ˆä¾‹ä¸­ï¼Œæˆ‘ä»¬æƒ³è¦äº†è§£ä¸¤ä¸ªæ¨¡å‹çš„é¢„æµ‹ä¹‹é—´çš„å·®å¼‚ï¼Œå› æ­¤è®­ç»ƒçš„æŸå¤±å‡½æ•°å°†åŸºäºè¿™ä¸ªæ•°å­¦æ¦‚å¿µã€‚

```py
import tensorflow as tf
from tensorflow.keras import layers, models
from tensorflow.keras.datasets import mnist
from tensorflow.keras.utils import to_categorical
import numpy as np

# Load the MNIST dataset
(train_images, train_labels), (test_images, test_labels) = mnist.load_data()

# Preprocess the data
train_images = train_images.reshape((60000, 28, 28, 1)).astype('float32') / 255
test_images = test_images.reshape((10000, 28, 28, 1)).astype('float32') / 255
train_labels = to_categorical(train_labels)
test_labels = to_categorical(test_labels)

# Define the teacher model (a larger model)
teacher_model = models.Sequential([
    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),
    layers.MaxPooling2D((2, 2)),
    layers.Conv2D(64, (3, 3), activation='relu'),
    layers.MaxPooling2D((2, 2)),
    layers.Conv2D(64, (3, 3), activation='relu'),
    layers.Flatten(),
    layers.Dense(64, activation='relu'),
    layers.Dense(10, activation='softmax')
])

teacher_model.compile(optimizer='adam',
                      loss='categorical_crossentropy',
                      metrics=['accuracy'])

# Train the teacher model
teacher_model.fit(train_images, train_labels, epochs=5, batch_size=64, validation_split=0.2)

# Define the student model (a smaller model)
student_model = models.Sequential([
    layers.Flatten(input_shape=(28, 28, 1)),
    layers.Dense(64, activation='relu'),
    layers.Dense(10, activation='softmax')
])

student_model.compile(optimizer='adam',
                      loss='categorical_crossentropy',
                      metrics=['accuracy'])

# Knowledge distillation step: Transfer knowledge from the teacher to the student
def distillation_loss(y_true, y_pred):
    alpha = 0.1  # Temperature parameter (adjust as needed)
    return tf.keras.losses.KLDivergence()(tf.nn.softmax(y_true / alpha, axis=1),
                                           tf.nn.softmax(y_pred / alpha, axis=1))

# Train the student model using knowledge distillation
student_model.fit(train_images, train_labels, epochs=10, batch_size=64,
                  validation_split=0.2, loss=distillation_loss)

# Evaluate the student model
test_loss, test_acc = student_model.evaluate(test_images, test_labels)
print(f'Test accuracy: {test_acc * 100:.2f}%')
```

## å‰ªæ

å‰ªææ˜¯ä¸€ç§æ¨¡å‹å‹ç¼©æ–¹æ³•ï¼Œæˆ‘åœ¨ç ”ç©¶ç”Ÿè®ºæ–‡ä¸­ç ”ç©¶è¿‡è¿™ä¸ªæ–¹æ³•ï¼Œå®é™…ä¸Šæˆ‘ä¹‹å‰å·²ç»å‘å¸ƒäº†ä¸€ç¯‡å…³äºå¦‚ä½•åœ¨ Julia ä¸­å®ç°å‰ªæçš„æ–‡ç« ï¼šJulia ä¸­çš„è¿­ä»£å‰ªææ–¹æ³•ã€‚

å‰ªæè¯ç”Ÿæ˜¯ä¸ºäº†åº”å¯¹å†³ç­–æ ‘ä¸­çš„è¿‡æ‹Ÿåˆï¼Œå®é™…ä¸Šï¼Œå‰ªå»äº†åˆ†æ”¯ä»¥å‡å°‘æ ‘çš„æ·±åº¦ã€‚è¿™ä¸ªæ¦‚å¿µåæ¥è¢«åº”ç”¨äºç¥ç»ç½‘ç»œä¸­ï¼Œåœ¨å…¶ä¸­ç½‘ç»œä¸­çš„è¾¹å’Œ/æˆ–èŠ‚ç‚¹è¢«ç§»é™¤ï¼ˆå–å†³äºæ˜¯è¿›è¡Œéç»“æ„åŒ–å‰ªæè¿˜æ˜¯ç»“æ„åŒ–å‰ªæï¼‰ã€‚

![](img/a5512b117100950116c5961541a85284.png)

ç¥ç»ç½‘ç»œå‰ªæï¼ˆæ¥æºï¼š`towardsdatascience.com/pruning-neural-networks-1bb3ab5791f9`ï¼‰

å¦‚æœæˆ‘ä»¬ä»ç½‘ç»œä¸­ç§»é™¤æ•´ä¸ªèŠ‚ç‚¹ï¼Œè¡¨ç¤ºå±‚çš„çŸ©é˜µä¼šå˜å¾—æ›´å°ï¼Œæ¨¡å‹ä¹Ÿä¼šå˜å¾—æ›´å¿«ã€‚

ç›¸åï¼Œå¦‚æœæˆ‘ä»¬ç§»é™¤å•ä¸ªè¾¹ï¼ŒçŸ©é˜µçš„å¤§å°å°†ä¿æŒä¸å˜ï¼Œä½†æˆ‘ä»¬ä¼šåœ¨ä¸è¢«ç§»é™¤çš„è¾¹å¯¹åº”çš„ä½ç½®æ”¾ç½®é›¶ï¼Œå› æ­¤æˆ‘ä»¬å°†æ‹¥æœ‰éå¸¸ç¨€ç–çš„çŸ©é˜µã€‚å› æ­¤ï¼Œåœ¨éç»“æ„åŒ–å‰ªæä¸­ï¼Œä¼˜åŠ¿ä¸åœ¨äºæé«˜é€Ÿåº¦ï¼Œè€Œåœ¨äºèŠ‚çœå†…å­˜ï¼Œå› ä¸ºåœ¨å†…å­˜ä¸­ä¿å­˜ç¨€ç–çŸ©é˜µå ç”¨çš„ç©ºé—´æ¯”ä¿å­˜å¯†é›†çŸ©é˜µå°‘å¾—å¤šã€‚

ä½†æˆ‘ä»¬è¦å‰ªæçš„èŠ‚ç‚¹æˆ–è¾¹æ˜¯ä»€ä¹ˆå‘¢ï¼Ÿæœ€ä¸å¿…è¦çš„é‚£äº›â€¦â€¦å…³äºè¿™ä¸€ç‚¹æœ‰å¾ˆå¤šç ”ç©¶ï¼Œæˆ‘ç‰¹åˆ«æƒ³ç»™ä½ æ¨èä¸¤ç¯‡è®ºæ–‡ï¼š

+   [Optimal Brain Damage](https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=17c0a7de3c17d31f79589d245852b57d083d386e)

+   [Optimal Brain Surgeon å’Œé€šç”¨ç½‘ç»œå‰ªæ](https://ieeexplore.ieee.org/document/298572)

è®©æˆ‘ä»¬çœ‹ä¸€ä¸ªç®€å•çš„ Python è„šæœ¬ï¼Œäº†è§£å¦‚ä½•ä¸ºä¸€ä¸ªç®€å•çš„ MNIST æ¨¡å‹å®ç°å‰ªæã€‚

```py
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.datasets import mnist
from tensorflow.keras.utils import to_categorical
from tensorflow_model_optimization.sparsity import keras as sparsity
import numpy as np

# Load the MNIST dataset
(train_images, train_labels), (test_images, test_labels) = mnist.load_data()

# Preprocess the data
train_images = train_images.reshape((60000, 28, 28, 1)).astype('float32') / 255
test_images = test_images.reshape((10000, 28, 28, 1)).astype('float32') / 255
train_labels = to_categorical(train_labels)
test_labels = to_categorical(test_labels)

# Create a simple neural network model
def create_model():
    model = Sequential([
        tf.keras.layers.Flatten(input_shape=(28, 28, 1)),
        tf.keras.layers.Dense(128, activation='relu'),
        tf.keras.layers.Dropout(0.2),
        tf.keras.layers.Dense(64, activation='relu'),
        tf.keras.layers.Dropout(0.2),
        tf.keras.layers.Dense(10, activation='softmax')
    ])
    return model

# Create and compile the original model
model = create_model()
model.compile(optimizer='adam',
              loss='categorical_crossentropy',
              metrics=['accuracy'])

# Train the original model
model.fit(train_images, train_labels, epochs=5, batch_size=64, validation_split=0.2)

# Prune the model
# Specify the pruning parameters
pruning_params = {
    'pruning_schedule': sparsity.PolynomialDecay(initial_sparsity=0.50,
                                                 final_sparsity=0.90,
                                                 begin_step=0,
                                                 end_step=2000,
                                                 frequency=100)
}

# Create a pruned model
pruned_model = sparsity.prune_low_magnitude(create_model(), **pruning_params)

# Compile the pruned model
pruned_model.compile(optimizer='adam',
                     loss='categorical_crossentropy',
                     metrics=['accuracy'])

# Train the pruned model (fine-tuning)
pruned_model.fit(train_images, train_labels, epochs=2, batch_size=64, validation_split=0.2)

# Strip pruning wrappers to create a smaller and faster model
final_model = sparsity.strip_pruning(pruned_model)

# Evaluate the final pruned model
test_loss, test_acc = final_model.evaluate(test_images, test_labels)
print(f'Test accuracy after pruning: {test_acc * 100:.2f}%')
```

## é‡åŒ–

æˆ‘ä¸è®¤ä¸ºæˆ‘è¯´é”™äº†ï¼Œé‡åŒ–å¯èƒ½æ˜¯ç›®å‰ä½¿ç”¨æœ€å¹¿æ³›çš„å‹ç¼©æŠ€æœ¯ã€‚å†ä¸€æ¬¡ï¼ŒåŸºæœ¬æ¦‚å¿µå¾ˆç®€å•ã€‚**æˆ‘ä»¬é€šå¸¸ç”¨ 32 ä½æµ®ç‚¹æ•°è¡¨ç¤ºç¥ç»ç½‘ç»œçš„å‚æ•°ã€‚ä½†å¦‚æœæˆ‘ä»¬ä½¿ç”¨æ›´å°‘çš„ä½æ•°å‘¢ï¼Ÿæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ 16 ä½ã€8 ä½ã€4 ä½ï¼Œç”šè‡³ 1 ä½ï¼Œç”šè‡³å¯ä»¥æ‹¥æœ‰äºŒè¿›åˆ¶ç½‘ç»œï¼**

è¿™æ„å‘³ç€ä»€ä¹ˆï¼Ÿé€šè¿‡ä½¿ç”¨è¾ƒä½ç²¾åº¦çš„æ•°å­—ï¼Œæ¨¡å‹çš„ä½“ç§¯ä¼šæ›´å°ï¼Œä½†ä¹Ÿä¼šå¤±å»ç²¾åº¦ï¼Œç»“æœä¼šæ¯”åŸå§‹æ¨¡å‹æ›´ä¸ºè¿‘ä¼¼ã€‚è¿™æ˜¯ä¸€ç§åœ¨éœ€è¦å°†æ¨¡å‹éƒ¨ç½²åˆ°è¾¹ç¼˜è®¾å¤‡ä¸Šæ—¶å¸¸ç”¨çš„æŠ€æœ¯ï¼Œç‰¹åˆ«æ˜¯æ™ºèƒ½æ‰‹æœºç­‰ç‰¹å®šç¡¬ä»¶ä¸Šï¼Œå› ä¸ºå®ƒèƒ½å¤§å¹…ç¼©å°ç½‘ç»œçš„å°ºå¯¸ã€‚è®¸å¤šæ¡†æ¶å…è®¸è½»æ¾åº”ç”¨é‡åŒ–ï¼Œä¾‹å¦‚ [TensorFlow Lite](https://www.tensorflow.org/lite)ã€[PyTorch](https://pytorch.org/) æˆ– [TensorRT](https://developer.nvidia.com/tensorrt)ã€‚

é‡åŒ–å¯ä»¥åœ¨è®­ç»ƒå‰åº”ç”¨ï¼Œå³æˆ‘ä»¬ç›´æ¥æˆªæ–­å‚æ•°åªèƒ½å–ç‰¹å®šèŒƒå›´å†…çš„å€¼ï¼Œæˆ–è€…åœ¨è®­ç»ƒååº”ç”¨ï¼Œå³åœ¨ç»“æŸæ—¶å¯¹å‚æ•°çš„å€¼è¿›è¡Œå››èˆäº”å…¥ã€‚

åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å¿«é€Ÿå±•ç¤ºäº†å¦‚ä½•åœ¨ Python ä¸­åº”ç”¨é‡åŒ–ã€‚

```py
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Flatten, Dropout
from tensorflow.keras.datasets import mnist
from tensorflow.keras.utils import to_categorical
import numpy as np

# Load the MNIST dataset
(train_images, train_labels), (test_images, test_labels) = mnist.load_data()

# Preprocess the data
train_images = train_images.reshape((60000, 28, 28, 1)).astype('float32') / 255
test_images = test_images.reshape((10000, 28, 28, 1)).astype('float32') / 255
train_labels = to_categorical(train_labels)
test_labels = to_categorical(test_labels)

# Create a simple neural network model
def create_model():
    model = Sequential([
        Flatten(input_shape=(28, 28, 1)),
        Dense(128, activation='relu'),
        Dropout(0.2),
        Dense(64, activation='relu'),
        Dropout(0.2),
        Dense(10, activation='softmax')
    ])
    return model

# Create and compile the original model
model = create_model()
model.compile(optimizer='adam',
              loss='categorical_crossentropy',
              metrics=['accuracy'])

# Train the original model
model.fit(train_images, train_labels, epochs=5, batch_size=64, validation_split=0.2)

# Quantize the model to 8-bit integers
converter = tf.lite.TFLiteConverter.from_keras_model(model)
converter.optimizations = [tf.lite.Optimize.DEFAULT]
quantized_model = converter.convert()

# Save the quantized model to a file
with open('quantized_model.tflite', 'wb') as f:
    f.write(quantized_model)

# Load the quantized model for inference
interpreter = tf.lite.Interpreter(model_path='quantized_model.tflite')
interpreter.allocate_tensors()

# Evaluate the quantized model
test_loss, test_acc = 0.0, 0.0
for i in range(len(test_images)):
    input_data = np.array([test_images[i]], dtype=np.float32)
    interpreter.set_tensor(interpreter.get_input_details()[0]['index'], input_data)
    interpreter.invoke()
    output_data = interpreter.get_tensor(interpreter.get_output_details()[0]['index'])
    test_loss += tf.keras.losses.categorical_crossentropy(test_labels[i], output_data).numpy()
    test_acc += np.argmax(test_labels[i]) == np.argmax(output_data)

test_loss /= len(test_images)
test_acc /= len(test_images)

print(f'Test accuracy after quantization: {test_acc * 100:.2f}%')
```

# ç»“è®º

åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æ¢è®¨äº†å‡ ç§æ¨¡å‹å‹ç¼©æ–¹æ³•ï¼Œä»¥åŠ å¿«æ¨¡å‹æ¨ç†é˜¶æ®µï¼Œè¿™å¯¹äºç”Ÿäº§ç¯å¢ƒä¸­çš„æ¨¡å‹å¯èƒ½æ˜¯ä¸€ä¸ªå…³é”®è¦æ±‚ã€‚ç‰¹åˆ«åœ°ï¼Œæˆ‘ä»¬å…³æ³¨äº†ä½ç§©åˆ†è§£ã€çŸ¥è¯†è’¸é¦ã€å‰ªæå’Œé‡åŒ–ï¼Œè§£é‡Šäº†åŸºæœ¬æ¦‚å¿µï¼Œå¹¶å±•ç¤ºäº† Python ä¸­çš„ç®€å•å®ç°ã€‚

æ¨¡å‹å‹ç¼©å¯¹äºåœ¨èµ„æºæœ‰é™ï¼ˆå¦‚ RAMã€GPU ç­‰ï¼‰çš„ç‰¹å®šç¡¬ä»¶ä¸Šéƒ¨ç½²æ¨¡å‹å°¤å…¶æœ‰ç”¨ï¼Œä¾‹å¦‚æ™ºèƒ½æ‰‹æœºã€‚

æˆ‘éå¸¸çƒ­è¡·çš„ä¸€ä¸ªåº”ç”¨æ¡ˆä¾‹æ˜¯ä½¿ç”¨æ¨¡å‹å‹ç¼©æ¥åœ¨å«æ˜Ÿå’Œèˆªå¤©å™¨ä¸Šéƒ¨ç½²æ¨¡å‹ï¼Œè¿™åœ¨åœ°çƒè§‚æµ‹é¢†åŸŸå°¤å…¶æœ‰ç”¨ï¼Œä¾‹å¦‚ä½¿å«æ˜Ÿèƒ½å¤Ÿè‡ªä¸»è¯†åˆ«éœ€è¦ä¸¢å¼ƒçš„æ•°æ®æˆ–å›¾åƒï¼Œä»¥é¿å…åœ¨æ•°æ®ä¼ è¾“åˆ°åœ°é¢æ®µè¿›è¡Œåˆ†ææ—¶äº§ç”Ÿè¿‡å¤šæµé‡ã€‚å¸Œæœ›è¿™ç¯‡æ–‡ç« å¯¹ä½ æ›´å¥½åœ°ç†è§£è¿™ä¸ªè¯é¢˜æœ‰æ‰€å¸®åŠ©ã€‚

å¦‚æœä½ å–œæ¬¢è¿™ç¯‡æ–‡ç« ï¼Œè¯·åœ¨ Medium ä¸Šå…³æ³¨æˆ‘ï¼ğŸ˜„

ğŸ’¼ [Linkedin](https://www.linkedin.com/in/marcello-politi/) ï¸| ğŸ¦ [Twitter](https://twitter.com/_March08_) | [ğŸ’»](https://emojiterra.com/laptop-computer/) [ç½‘ç«™](https://marcello-politi.super.site/)
