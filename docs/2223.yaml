- en: 'Mixed Effects Machine Learning for High-Cardinality Categorical Variables —
    Part II: GPBoost library'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/mixed-effects-machine-learning-for-high-cardinality-categorical-variables-part-ii-gpboost-3bdd9ef74492?source=collection_archive---------6-----------------------#2023-07-11](https://towardsdatascience.com/mixed-effects-machine-learning-for-high-cardinality-categorical-variables-part-ii-gpboost-3bdd9ef74492?source=collection_archive---------6-----------------------#2023-07-11)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: A demo of GPBoost in Python & R using real-world data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@fabsig?source=post_page-----3bdd9ef74492--------------------------------)[![Fabio
    Sigrist](../Images/f7bc2adc17255ae1efd0886a19ec202c.png)](https://medium.com/@fabsig?source=post_page-----3bdd9ef74492--------------------------------)[](https://towardsdatascience.com/?source=post_page-----3bdd9ef74492--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----3bdd9ef74492--------------------------------)
    [Fabio Sigrist](https://medium.com/@fabsig?source=post_page-----3bdd9ef74492--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fb5b503a0c329&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmixed-effects-machine-learning-for-high-cardinality-categorical-variables-part-ii-gpboost-3bdd9ef74492&user=Fabio+Sigrist&userId=b5b503a0c329&source=post_page-b5b503a0c329----3bdd9ef74492---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----3bdd9ef74492--------------------------------)
    ·8 min read·Jul 11, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F3bdd9ef74492&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmixed-effects-machine-learning-for-high-cardinality-categorical-variables-part-ii-gpboost-3bdd9ef74492&user=Fabio+Sigrist&userId=b5b503a0c329&source=-----3bdd9ef74492---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F3bdd9ef74492&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmixed-effects-machine-learning-for-high-cardinality-categorical-variables-part-ii-gpboost-3bdd9ef74492&source=-----3bdd9ef74492---------------------bookmark_footer-----------)![](../Images/e5dd3c4f7bc91e17c444088bc24348ba.png)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Illustration of high-cardinality categorical data**: box plots and raw data
    (red points) of the response variable for different levels of a categorical variable
    — Image by author'
  prefs: []
  type: TYPE_NORMAL
- en: High-cardinality categorical variables are variables for which the number of
    different levels is large relative to the sample size of a data set. In [Part
    I](https://medium.com/towards-data-science/mixed-effects-machine-learning-for-high-cardinality-categorical-variables-part-i-an-empirical-df0a43dce059)
    of this series, we did an empirical comparison of different machine learning methods
    and found that random effects are an effective tool for handling high-cardinality
    categorical variables with the [GPBoost algorithm](https://www.jmlr.org/papers/v23/20-322.html)
    [Sigrist, 2022, 2023] having the highest prediction accuracy. In this article,
    we demonstrate how the GPBoost algorithm, which combines tree-boosting with random
    effects, can be applied with the Python and R packages of the `[GPBoost](https://github.com/fabsig/GPBoost)`
    [library](https://github.com/fabsig/GPBoost). `GPBoost` version 1.2.1 is used
    in this demo.
  prefs: []
  type: TYPE_NORMAL
- en: Table of contents
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: ∘ [1 Introduction](#c2dc)
  prefs: []
  type: TYPE_NORMAL
- en: '∘ [2 Data: description, loading, and sample split](#c3cf)'
  prefs: []
  type: TYPE_NORMAL
- en: ∘ [3 Training a GPBoost model](#a119)
  prefs: []
  type: TYPE_NORMAL
- en: ∘ [4 Choosing tuning parameter](#d562)
  prefs: []
  type: TYPE_NORMAL
- en: ∘ [5 Prediction](#9914)
  prefs: []
  type: TYPE_NORMAL
- en: ∘ [6 Interpretation](#7784)
  prefs: []
  type: TYPE_NORMAL
- en: ∘ [7 Further modeling options](#6d73)
  prefs: []
  type: TYPE_NORMAL
- en: · · [7.1 Interaction between categorical variables and other predictor variables](#b005)
  prefs: []
  type: TYPE_NORMAL
- en: · · [7.2 (Generalized) linear mixed effects models](#49d6)
  prefs: []
  type: TYPE_NORMAL
- en: ∘ [8 Conclusion and references](#4ebe)
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Applying a GPBoost model involves the following main steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Define a `GPModel` in which one specifies the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '— A random effects model: grouped random effects via `group_data` and/or Gaussian
    processes via `gp_coords`'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: — The `likelihood` *(= distribution of the response variable conditional on
    fixed and random effects)*
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Create a `Dataset` containing the response variable (`label`) and fixed effects
    predictor variables (`data`)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Choose tuning parameters, e.g., using the function `gpb.grid.search.tune.parameters`
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Train the model
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Make predictions and/or interpret the trained model
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the following, we go through these points step-by-step.
  prefs: []
  type: TYPE_NORMAL
- en: '2 Data: description, loading, and sample split'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The data used in this demo is the wages data which was also analyzed in [Sigrist
    (2022)](https://www.jmlr.org/papers/v23/20-322.html). It can be downloaded from
    [here](https://raw.githubusercontent.com/fabsig/Compare_ML_HighCardinality_Categorical_Variables/master/data/wages.csv.gz).
    The data contains 28’013 samples and one high-cardinality categorical variable
    (person ID = `idcode`) with 4’711 different levels. The response variable is the
    logarithmic real wage (`ln_wage`), and the data includes several predictor variables
    such as age, total work experience, job tenure in years, date, and others.
  prefs: []
  type: TYPE_NORMAL
- en: In the code below, we first load the data and randomly partition the data into
    80% training data and 20% test data. The latter is left aside and will be used
    later for measuring prediction accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: '***Python***'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '***R***'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 3 Training a GPBoost model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The code below shows how to train a GPBoost model. We first define a random
    effects model (`gp_model`), a `Dataset` (`data_bst`), tuning parameters (`params`
    and `nrounds` = number of trees / boosting iteration), and then run the GPBoost
    algorithm by calling the `gpb.train` function. Note that we use tuning parameters
    that have been selected beforehand (see below).
  prefs: []
  type: TYPE_NORMAL
- en: '***Python***'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '***R***'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'For this data, there is only one high-cardinality categorical variable. If
    there are multiple categorical variables that should be modeled using random effects,
    this can be specified in the `gp_model` as shown in the following code. *Note:
    computations can become slower when having multiple random effects compared to
    only one. Also, it can be faster to use* `*nelder_mead*` *instead of* `*gradient_descent*`
    *(=default) as optimizer when having multiple random effects. This can be set
    as follows before calling* `*gpb.train*`*.*'
  prefs: []
  type: TYPE_NORMAL
- en: '***Python***'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '***R***'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 4 Choosing tuning parameter
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'It is important that tuning parameters are appropriately chosen for boosting.
    There are no universal default values and every data set will likely need different
    tuning parameters. Below, we show how tuning parameters can be chosen using the
    `gpb.grid.search.tune.parameters` function. We use a deterministic grid search
    with the parameter combinations shown below in the code. Note that we tune the
    `max_depth` parameter and thus set the `num_leaves` to a large value. We randomly
    split the training data into 80% inner training data 20% validation data and use
    the mean squared error (`mse`) as a prediction accuracy measure on the validation
    data. Alternatively, one can also use, e.g., the test negative log-likelihood
    (`test_neg_log_likelihood` = default) which also takes prediction uncertainty
    into account. *Note: depending on the data set and the grid size, this can take
    some time (a few minutes on my laptop for this data set). Instead of a deterministic
    grid search as below, one can also do a random grid search (see* `*num_try_random*`*)
    or another approach such as Bayesian optimization to speed things up.*'
  prefs: []
  type: TYPE_NORMAL
- en: '***Python***'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '***R***'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 5 Prediction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Predictions can be obtained via the `predict` function. We need to provide both
    test predictor variables (`data`) for the tree ensemble and test categorical variables
    (`group_data_pred`) for the random effects model. Below, we make predictions on
    the left-out test data and calculate the test mean squared error (MSE).
  prefs: []
  type: TYPE_NORMAL
- en: '***Python***'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '***R***'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 6 Interpretation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Information on the estimated random effects model can be obtained as follows.
  prefs: []
  type: TYPE_NORMAL
- en: '***Python***'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '***R***'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'There are several tools for understanding the form of the fixed effects tree
    ensemble function supported by the `GPBoost` library:'
  prefs: []
  type: TYPE_NORMAL
- en: SHAP values
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SHAP & partial dependence plots (one and two-dimensional)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SHAP force plots
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Interaction statistics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Split-based variable importances
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Below we look at SHAP values and a SHAP dependence plot.
  prefs: []
  type: TYPE_NORMAL
- en: '***Python***'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/74fb408e7c6fe5a23df8be4bf03aafaa.png)![](../Images/8966198e2b3bcfc1ad60eb8283de210b.png)'
  prefs: []
  type: TYPE_IMG
- en: SHAP values and SHAP dependence plot — Image by author
  prefs: []
  type: TYPE_NORMAL
- en: '***R***'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 7 Further modeling options
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 7.1 Interaction between categorical variables and other predictor variables
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In the above model, there is no interaction between the random effects and
    the fixed effects predictor variables, i.e., between the high-cardinality categorical
    variable and the other predictor variables. Such interaction can be modeled by
    additionally including the categorical variable in the tree ensemble. The following
    code shows how this can be done. Below, we also calculate the test MSE of this
    model. The test MSE is slightly smaller compared to a model without such interaction.
    *Note: the tuning parameters used below have been chosen beforehand, but for the
    sake of brevity, we omit this code.*'
  prefs: []
  type: TYPE_NORMAL
- en: '***Python***'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '***R***'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 7.2 (Generalized) linear mixed effects models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The `GPBoost` library also supports (generalized) linear mixed effects models
    (GLMMs). In a GLMM, one assumes that the fixed effects function F() is a linear
    function instead of a tree ensemble. The code below shows how this can be done.
    Note that one needs to manually add a column of 1s to include an intercept. With
    the option `params={"std_dev": True}` / `params = list(std_dev=TRUE)`, one obtains
    standard deviations and p-values in the `summary` function.'
  prefs: []
  type: TYPE_NORMAL
- en: '***Python***'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '***R***'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 8 Conclusion and references
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have demonstrated how tree-boosting combined with random effects can be applied
    with the Python and R packages of the `[GPBoost](https://github.com/fabsig/GPBoost)`
    [library](https://github.com/fabsig/GPBoost). There are several more features
    of the `GPBoost` library that we did not show here. You may want to have a look
    at the [Python examples](https://github.com/fabsig/GPBoost/tree/master/examples/python-guide)
    and [R examples](https://github.com/fabsig/GPBoost/tree/master/R-package/demo).
    In [Part III](https://medium.com/towards-data-science/mixed-effects-machine-learning-for-longitudinal-panel-data-with-gpboost-part-iii-523bb38effc)
    of this series, we will show how longitudinal data (aka panel data) can be modeled
    with the `GPBoost` library.
  prefs: []
  type: TYPE_NORMAL
- en: F. Sigrist. Gaussian Process Boosting. The Journal of Machine Learning Research,
    23(1):10565–10610, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: F. Sigrist. Latent Gaussian Model Boosting. IEEE Transactions on Pattern Analysis
    and Machine Intelligence, 45(2):1894–1905, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://github.com/fabsig/GPBoost](https://github.com/fabsig/GPBoost)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
