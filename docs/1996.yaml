- en: 'PaLM: Efficiently Training Massive Language Models'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/palm-efficiently-training-massive-language-models-b82d6cc1582?source=collection_archive---------8-----------------------#2023-06-19](https://towardsdatascience.com/palm-efficiently-training-massive-language-models-b82d6cc1582?source=collection_archive---------8-----------------------#2023-06-19)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Unprecedented size, efficiency, and performance for LLMs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://wolfecameron.medium.com/?source=post_page-----b82d6cc1582--------------------------------)[![Cameron
    R. Wolfe, Ph.D.](../Images/52bb88d7cf1105501be2fae5ccbe7a03.png)](https://wolfecameron.medium.com/?source=post_page-----b82d6cc1582--------------------------------)[](https://towardsdatascience.com/?source=post_page-----b82d6cc1582--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----b82d6cc1582--------------------------------)
    [Cameron R. Wolfe, Ph.D.](https://wolfecameron.medium.com/?source=post_page-----b82d6cc1582--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F28aa6026c553&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpalm-efficiently-training-massive-language-models-b82d6cc1582&user=Cameron+R.+Wolfe%2C+Ph.D.&userId=28aa6026c553&source=post_page-28aa6026c553----b82d6cc1582---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----b82d6cc1582--------------------------------)
    ·17 min read·Jun 19, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fb82d6cc1582&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpalm-efficiently-training-massive-language-models-b82d6cc1582&user=Cameron+R.+Wolfe%2C+Ph.D.&userId=28aa6026c553&source=-----b82d6cc1582---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb82d6cc1582&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpalm-efficiently-training-massive-language-models-b82d6cc1582&source=-----b82d6cc1582---------------------bookmark_footer-----------)![](../Images/451e0862c498cfad7a6e861d29efbd84.png)'
  prefs: []
  type: TYPE_NORMAL
- en: (Photo by [Corey Agopian](https://unsplash.com/@corey_lyfe?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    on [Unsplash](https://unsplash.com/images/nature/palm-tree?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText))
  prefs: []
  type: TYPE_NORMAL
- en: In recent years, large, deep neural networks have become the definitive architecture
    of choice for solving most language understanding and generation tasks. Initially,
    models were proposed, such as [BERT](https://cameronrwolfe.substack.com/p/language-understanding-with-bert)
    [2] and T5 [3], that used a [two-part training methodology](https://cameronrwolfe.substack.com/i/76273144/training-bert)
    of pre-training (with [self-supervised “infilling” objectives](https://cameronrwolfe.substack.com/i/76273144/self-supervised-learning))
    over a large corpus of text, then fine-tuning on a target dataset; see below.
    Despite the utility of these techniques, recent work on large language models
    (LLMs) has shown that large, autoregressive (decoder-only) transformer models
    are incredibly capable at few-shot learning, achieving impressive performance
    with minimal adaptation to downstream tasks.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/dc4d51598c9de0703ecde8cfa28d78cd.png)'
  prefs: []
  type: TYPE_IMG
- en: (from [4])
  prefs: []
  type: TYPE_NORMAL
- en: The few-shot learning capabilities of LLMs were first demonstrated by [GPT-3](https://cameronrwolfe.substack.com/p/language-model-scaling-laws-and-gpt)
    [4], a 175 billion parameter LLM. To perform few-shot prediction, the model is
    pre-trained (using a basic [language modeling objective](https://cameronrwolfe.substack.com/i/85568430/language-modeling))
    over a massive corpus of text, then provided task descriptions and a handful of
    examples of how a task should be solved; see above. Further analysis of LLMs indicated
    that model performance improves…
  prefs: []
  type: TYPE_NORMAL
