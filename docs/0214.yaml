- en: 'Specialized LLMs: ChatGPT, LaMDA, Galactica, Codex, Sparrow, and More'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 专业化的LLM：ChatGPT、LaMDA、Galactica、Codex、Sparrow等
- en: 原文：[https://towardsdatascience.com/specialized-llms-chatgpt-lamda-galactica-codex-sparrow-and-more-ccccdd9f666f?source=collection_archive---------1-----------------------#2023-01-13](https://towardsdatascience.com/specialized-llms-chatgpt-lamda-galactica-codex-sparrow-and-more-ccccdd9f666f?source=collection_archive---------1-----------------------#2023-01-13)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/specialized-llms-chatgpt-lamda-galactica-codex-sparrow-and-more-ccccdd9f666f?source=collection_archive---------1-----------------------#2023-01-13](https://towardsdatascience.com/specialized-llms-chatgpt-lamda-galactica-codex-sparrow-and-more-ccccdd9f666f?source=collection_archive---------1-----------------------#2023-01-13)
- en: Simple techniques for creating better, domain-specific LLMs
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建更佳领域特定LLM的简单技巧
- en: '[](https://wolfecameron.medium.com/?source=post_page-----ccccdd9f666f--------------------------------)[![Cameron
    R. Wolfe, Ph.D.](../Images/52bb88d7cf1105501be2fae5ccbe7a03.png)](https://wolfecameron.medium.com/?source=post_page-----ccccdd9f666f--------------------------------)[](https://towardsdatascience.com/?source=post_page-----ccccdd9f666f--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----ccccdd9f666f--------------------------------)
    [Cameron R. Wolfe, Ph.D.](https://wolfecameron.medium.com/?source=post_page-----ccccdd9f666f--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://wolfecameron.medium.com/?source=post_page-----ccccdd9f666f--------------------------------)[![Cameron
    R. Wolfe, Ph.D.](../Images/52bb88d7cf1105501be2fae5ccbe7a03.png)](https://wolfecameron.medium.com/?source=post_page-----ccccdd9f666f--------------------------------)[](https://towardsdatascience.com/?source=post_page-----ccccdd9f666f--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----ccccdd9f666f--------------------------------)
    [Cameron R. Wolfe, Ph.D.](https://wolfecameron.medium.com/?source=post_page-----ccccdd9f666f--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F28aa6026c553&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fspecialized-llms-chatgpt-lamda-galactica-codex-sparrow-and-more-ccccdd9f666f&user=Cameron+R.+Wolfe%2C+Ph.D.&userId=28aa6026c553&source=post_page-28aa6026c553----ccccdd9f666f---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----ccccdd9f666f--------------------------------)
    ·30 min read·Jan 13, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fccccdd9f666f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fspecialized-llms-chatgpt-lamda-galactica-codex-sparrow-and-more-ccccdd9f666f&user=Cameron+R.+Wolfe%2C+Ph.D.&userId=28aa6026c553&source=-----ccccdd9f666f---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F28aa6026c553&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fspecialized-llms-chatgpt-lamda-galactica-codex-sparrow-and-more-ccccdd9f666f&user=Cameron+R.+Wolfe%2C+Ph.D.&userId=28aa6026c553&source=post_page-28aa6026c553----ccccdd9f666f---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----ccccdd9f666f--------------------------------)
    ·30分钟阅读·2023年1月13日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fccccdd9f666f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fspecialized-llms-chatgpt-lamda-galactica-codex-sparrow-and-more-ccccdd9f666f&user=Cameron+R.+Wolfe%2C+Ph.D.&userId=28aa6026c553&source=-----ccccdd9f666f---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fccccdd9f666f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fspecialized-llms-chatgpt-lamda-galactica-codex-sparrow-and-more-ccccdd9f666f&source=-----ccccdd9f666f---------------------bookmark_footer-----------)![](../Images/1469bf9ac8a88095d227844718d069f4.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fccccdd9f666f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fspecialized-llms-chatgpt-lamda-galactica-codex-sparrow-and-more-ccccdd9f666f&source=-----ccccdd9f666f---------------------bookmark_footer-----------)![](../Images/1469bf9ac8a88095d227844718d069f4.png)'
- en: (Photo by [NASA](https://unsplash.com/es/@nasa?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    on [Unsplash](https://unsplash.com/images/nature/space?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText))
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: (照片由 [NASA](https://unsplash.com/es/@nasa?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    提供，来源于 [Unsplash](https://unsplash.com/images/nature/space?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText))
- en: Large language models (LLMs) are incredibly-useful, task-agnostic foundation
    models. But, *how much can we actually accomplish with a generic model?* These
    models are adept at solving common natural language benchmarks that we see within
    the deep learning literature. But, using LLMs practically usually requires that
    the model be taught new behavior that is relevant to a particular application.
    Within this overview, we will explore methods of specializing and improving LLMs
    for a variety of use cases.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）是非常有用的任务无关基础模型。但，*我们实际能用一个通用模型完成多少工作？*这些模型擅长解决我们在深度学习文献中看到的常见自然语言基准问题。然而，实际使用LLMs通常需要对模型进行新的行为训练，使其与特定应用相关。在本综述中，我们将探讨为各种用例专业化和改进LLMs的方法。
- en: We can modify the behavior of LLMs by using techniques like domain-specific
    pre-training, model alignment, and supervised fine-tuning. These methods can be
    used to eliminate known limitations of LLMs (e.g., generating incorrect/biased
    info), modify LLM behavior to better suit our needs, or even inject specialized
    knowledge into an LLM such that it becomes a domain expert.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过使用领域特定的预训练、模型对齐和监督微调等技术来修改大语言模型（LLMs）的行为。这些方法可以用来消除LLMs的已知局限性（例如，生成错误/偏见信息），调整LLM行为以更好地满足我们的需求，甚至将专门的知识注入LLM，使其成为某个领域的专家。
- en: 'The concept of creating specialized LLMs for particular applications has been
    heavily explored in recent literature. Though many different methodologies exist,
    they share a common theme: making LLMs more practically viable and useful. Though
    the definition of “useful” is highly variable across applications and human…'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 为特定应用创建专业化LLMs的概念在近期文献中得到了广泛探讨。尽管存在多种不同的方法，但它们有一个共同的主题：使LLMs在实际应用中更具可行性和实用性。尽管“实用”的定义在不同应用和人类之间高度可变…
