- en: 'Specialized LLMs: ChatGPT, LaMDA, Galactica, Codex, Sparrow, and More'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/specialized-llms-chatgpt-lamda-galactica-codex-sparrow-and-more-ccccdd9f666f?source=collection_archive---------1-----------------------#2023-01-13](https://towardsdatascience.com/specialized-llms-chatgpt-lamda-galactica-codex-sparrow-and-more-ccccdd9f666f?source=collection_archive---------1-----------------------#2023-01-13)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Simple techniques for creating better, domain-specific LLMs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://wolfecameron.medium.com/?source=post_page-----ccccdd9f666f--------------------------------)[![Cameron
    R. Wolfe, Ph.D.](../Images/52bb88d7cf1105501be2fae5ccbe7a03.png)](https://wolfecameron.medium.com/?source=post_page-----ccccdd9f666f--------------------------------)[](https://towardsdatascience.com/?source=post_page-----ccccdd9f666f--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----ccccdd9f666f--------------------------------)
    [Cameron R. Wolfe, Ph.D.](https://wolfecameron.medium.com/?source=post_page-----ccccdd9f666f--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F28aa6026c553&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fspecialized-llms-chatgpt-lamda-galactica-codex-sparrow-and-more-ccccdd9f666f&user=Cameron+R.+Wolfe%2C+Ph.D.&userId=28aa6026c553&source=post_page-28aa6026c553----ccccdd9f666f---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----ccccdd9f666f--------------------------------)
    ·30 min read·Jan 13, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fccccdd9f666f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fspecialized-llms-chatgpt-lamda-galactica-codex-sparrow-and-more-ccccdd9f666f&user=Cameron+R.+Wolfe%2C+Ph.D.&userId=28aa6026c553&source=-----ccccdd9f666f---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fccccdd9f666f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fspecialized-llms-chatgpt-lamda-galactica-codex-sparrow-and-more-ccccdd9f666f&source=-----ccccdd9f666f---------------------bookmark_footer-----------)![](../Images/1469bf9ac8a88095d227844718d069f4.png)'
  prefs: []
  type: TYPE_NORMAL
- en: (Photo by [NASA](https://unsplash.com/es/@nasa?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    on [Unsplash](https://unsplash.com/images/nature/space?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText))
  prefs: []
  type: TYPE_NORMAL
- en: Large language models (LLMs) are incredibly-useful, task-agnostic foundation
    models. But, *how much can we actually accomplish with a generic model?* These
    models are adept at solving common natural language benchmarks that we see within
    the deep learning literature. But, using LLMs practically usually requires that
    the model be taught new behavior that is relevant to a particular application.
    Within this overview, we will explore methods of specializing and improving LLMs
    for a variety of use cases.
  prefs: []
  type: TYPE_NORMAL
- en: We can modify the behavior of LLMs by using techniques like domain-specific
    pre-training, model alignment, and supervised fine-tuning. These methods can be
    used to eliminate known limitations of LLMs (e.g., generating incorrect/biased
    info), modify LLM behavior to better suit our needs, or even inject specialized
    knowledge into an LLM such that it becomes a domain expert.
  prefs: []
  type: TYPE_NORMAL
- en: 'The concept of creating specialized LLMs for particular applications has been
    heavily explored in recent literature. Though many different methodologies exist,
    they share a common theme: making LLMs more practically viable and useful. Though
    the definition of “useful” is highly variable across applications and human…'
  prefs: []
  type: TYPE_NORMAL
