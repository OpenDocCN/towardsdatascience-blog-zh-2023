- en: So How Fair Is Your AI, Exactly?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/so-how-fair-is-your-ai-exactly-83f8defcf449?source=collection_archive---------14-----------------------#2023-02-24](https://towardsdatascience.com/so-how-fair-is-your-ai-exactly-83f8defcf449?source=collection_archive---------14-----------------------#2023-02-24)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: About the challenge of getting the fairness objective right
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@boris-ruf?source=post_page-----83f8defcf449--------------------------------)[![Boris
    Ruf](../Images/96dc4fc2f32add89fef6911195590cd8.png)](https://medium.com/@boris-ruf?source=post_page-----83f8defcf449--------------------------------)[](https://towardsdatascience.com/?source=post_page-----83f8defcf449--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----83f8defcf449--------------------------------)
    [Boris Ruf](https://medium.com/@boris-ruf?source=post_page-----83f8defcf449--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fed341456850c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fso-how-fair-is-your-ai-exactly-83f8defcf449&user=Boris+Ruf&userId=ed341456850c&source=post_page-ed341456850c----83f8defcf449---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----83f8defcf449--------------------------------)
    ·6 min read·Feb 24, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F83f8defcf449&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fso-how-fair-is-your-ai-exactly-83f8defcf449&user=Boris+Ruf&userId=ed341456850c&source=-----83f8defcf449---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F83f8defcf449&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fso-how-fair-is-your-ai-exactly-83f8defcf449&source=-----83f8defcf449---------------------bookmark_footer-----------)![](../Images/ca243d1259ab5acfdc6fee99022d439e.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Photo by [Pawel Czerwinski](https://unsplash.com/@pawel_czerwinski?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: '*The use of artificial intelligence (AI) has given rise to new ethical and
    legal challenges. In* [*my previous article*](/to-guarantee-impartial-ai-decisions-lady-justice-needs-to-blink-2992167b2591)
    *I illustrated why removing the sensitive information from the training data does
    not promote fairness, but rather the opposite. This article is about identifying
    the most appropriate fairness definition for an AI application. The bespoken tool
    has been originally presented in a* [*research paper*](https://arxiv.org/abs/2102.08453)
    *I co-published on the topic.*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Collecting and analyzing data through AI has become standard, and machine learning
    is improving business performances in many areas today. However, numerous cases
    of AI machine bias have been exposed in recent years, and new examples emerge
    on a regular basis. This is one of the main pitfalls of AI: if the training data
    include any kind of bias, the algorithm will incorporate and enforce it — potentially
    harming sensitive subgroups defined by gender, religion, ethnicity, or age.'
  prefs: []
  type: TYPE_NORMAL
- en: The challenge of defining ‘fairness’
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Current approaches to mitigate bias are mostly technical and focus on adjusting
    algorithms or data to satisfy some kind of fairness. However, multiple, conflicting
    notions of fairness exist and unfortunately there is no universally accepted definition.
    The most appropriate fairness metric always depends on the context of application.
    In practice, identifying the fairness objective is complicated because mapping
    ethical principles to metrics is not a straightforward process. Still, to obtain
    sustainable solutions for fairer AI, attention must be focused on this problem
    since the best technical mitigation method will fail when the implemented fairness
    objective is not aligned with the stakeholders’ expectations.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/28159840fee300fdcecb547d87477a8a.png)'
  prefs: []
  type: TYPE_IMG
- en: Available fairness metrics for classification tasks. Image by author.
  prefs: []
  type: TYPE_NORMAL
- en: Not on the same page
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In many scenarios, this decision is not a trivial one, as has become obvious
    in the heated debate over the COMPAS algorithm. This one had been developed by
    the company Northpointe to generate an independent, data-derived “risk score”
    for several forms of recidivism. Such an algorithm is used in the criminal justice
    sector in the US to support the judge with particular decisions such as granting
    of bail or parole. The score is of informative character and the final decision
    is still up to the judge.
  prefs: []
  type: TYPE_NORMAL
- en: In May 2016, the investigative journalism website ProPublica focused attention
    on possible racial biases in the COMPAS algorithm. Its main argument was based
    on analysis of the data which showed that the results were biased. In particular,
    the [false positive rate](https://en.wikipedia.org/wiki/False_positive_rate) for
    people who were black was significantly higher compared to people who were white.
    As a result, black people were disproportionately often falsely attributed a high
    risk of recidivism. Northpointe, on the other hand, responded to the accusations
    by arguing that the algorithm effectively achieved [predictive value parity](https://en.wikipedia.org/wiki/Positive_and_negative_predictive_values)
    for both groups. In a nutshell, this ensured that risk scores corresponded to
    probabilities of reoffending, irrespective of any skin colour.
  prefs: []
  type: TYPE_NORMAL
- en: 'From an objective point of view, it can be stated that both parties make valid
    and reasonable observations of the data. However, the huge controversy revealed
    that it is absolutely critical to precisely define and disclose the selected fairness
    objective for an application. And this decision usually involves arbitration and
    compromise. In the given scenario, for example, the two fairness objectives could
    only be mutually satisfied if one of the following conditions was met: Either
    the [base rates](https://en.wikipedia.org/wiki/Base_rate) of the sensitive subgroups
    are exactly identical, or the outcome classes are perfectly separable which would
    allow for creating an ideal classifier that achieves perfect accuracy. Unfortunately,
    both requirements are very unlikely in real world scenarios.'
  prefs: []
  type: TYPE_NORMAL
- en: Navigating metrics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Surprisingly, relatively little research has been conducted on how to streamline
    the fairness selection process for AI applications in practice. To overcome this
    challenge, we have developed the [Fairness Compass](https://axa-rev-research.github.io/fairness-compass.html),
    an experimental tool which seeks to structure the complex landscape of fairness
    metrics. Based on a set of concrete questions regarding the nature of their data,
    beliefs in its correctness, fairness policies, and whether the focus should be
    more on specificity or sensitivity of the model, the Fairness Compass guides AI
    practitioners towards the most suitable option for a given scenario. Formalizing
    this selection process and turning it into a straightforward procedure helps clear
    a hurdle for implementing responsible AI in the real world. Moreover, recording
    the reasoning behind the decisions can serve as documentation for internal purposes
    and as means of communication to increase transparency, fostering trust in the
    technology.
  prefs: []
  type: TYPE_NORMAL
- en: The Fairness Compass has been published as open source project on [GitHub](https://github.com/axa-rev-research/fairness-compass).
    It was nominated for the [Gartner Eye on Innovation award](https://www.gartner.com/en/about/awards/eye-on-innovation)
    and has been included in the [AI Fairness Global Library](https://www.aifairnesslibrary.com)
    by the World Economic Forum.
  prefs: []
  type: TYPE_NORMAL
- en: Sample application
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To illustrate the concept, let’s take a sample scenario in the human resources
    context. As sensitive subgroups, we consider men and women. The question to be
    answered is which definition of fairness would be most appropriate when it comes
    to assessing fairness in employee promotion decisions. Please note that this is
    just a fictional thought experiment, and depending on the context, other answers
    with different results may apply. The purpose of the Fairness Compass is to support
    well informed decision making based on the defined requirements for a given scenario.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/da24c0e28fa61f79a322054e92f858c4.png)'
  prefs: []
  type: TYPE_IMG
- en: B. Ruf and M. Detyniecki, “[Towards the Right Kind of Fairness in AI](https://axa-rev-research.github.io/static/AXA_FairnessCompass-English.pdf)”,
    ECML/PKDD 2021 (Industry Track)
  prefs: []
  type: TYPE_NORMAL
- en: 'In the animation above, the Fairness Compass is represented as a decision tree
    with three different types of nodes: The diamonds symbolize decision points; the
    white boxes stand for actions and the grey boxes with round corners are the fairness
    definitions. The arrows which connect the nodes represent the possible choices.'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s kick-off the process. The first question is about existing policies which
    may influence the decision. Fairness objectives can go beyond equal treatment
    of different groups or similar individuals. If the target is to bridge prevailing
    inequalities by boosting underprivileged groups, affirmative actions or quotas
    can be valid measures. Such a goal may stem from law, regulation, or internal
    organizational guidelines. This approach rules out any possible causality between
    the sensitive attribute and the outcome. If the data tells a different story in
    terms of varying base rates across the subgroups, this is a strong commitment
    which leads to subordinating the algorithm’s accuracy to the policy’s overarching
    goal. For example, many universities aim to improve diversity by accepting more
    students from disadvantaged backgrounds. Such admission policies acknowledge an
    equally high academic potential of students from sensitive subgroups and considers
    their possibly lower level of education rather an injustice in society than a
    personal shortcoming.​​​​​​​
  prefs: []
  type: TYPE_NORMAL
- en: For our sample scenario, we conclude that no such affirmative action policy
    is in place for promotion decisions. Therefore, we choose “No” and document the
    reasoning behind our choice. Now, we continue with the next question and repeat
    the procedure until we reach a leaf node which contains the recommended fairness
    definition for the defined use case.
  prefs: []
  type: TYPE_NORMAL
- en: Hence, following such a formalised process can significantly contribute to identifying
    and explaining the optimal fairness metric for a particular AI application.
  prefs: []
  type: TYPE_NORMAL
- en: So what
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In practice, a lot of different definitions of fairness exist. Because some
    are mutually exclusive, it is necessary to settle for one of them. Making the
    choice is not trivial since the best decision always depends on the context of
    application and trade-offs are often unavoidable. Therefore, it is crucial to
    select the fairness objective of an AI application with great care and also communicate
    it to internal and external stakeholders. Transparency about the reasoning behind
    this decision is a key factor towards a sustainable implemention of fairer AI.
  prefs: []
  type: TYPE_NORMAL
- en: '*Many thanks to Antoine Pietri for his valuable support in writing this post.
    In* [*the following article*](https://medium.com/just-tech-it-now/3-strategies-to-fight-discrimination-in-ai-applications-2ce78e7fae65)*,
    I will outline how to actively mitigate biases in AI applications.*'
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: N. Mehrabi, F. Morstatter, et al. (2021). [A Survey on Bias and Fairness in
    Machine Learning](https://arxiv.org/abs/1908.09635). ACM Computing Surveys (CSUR)
    54, 6 (2021), 1–35.
  prefs: []
  type: TYPE_NORMAL
- en: J. Angwin, J. Larson, et al. (2016). [Machine bias](https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing).
    Ethics of Data and Analytics, Auerbach Publications, 254–264.
  prefs: []
  type: TYPE_NORMAL
- en: 'W. Dieterich, C. Mendoza, et al. (2016). [COMPAS risk scales: Demonstrating
    accuracy equity and predictive parity](https://go.volarisgroup.com/rs/430-MBX-989/images/ProPublica_Commentary_Final_070616.pdf).
    Northpointe Inc.'
  prefs: []
  type: TYPE_NORMAL
- en: 'S. Corbett-Davies & S. Goel (2018). [The Measure and Mismeasure of Fairness:
    A Critical Review of Fair Machine Learning](https://arxiv.org/abs/1808.00023).
    *arXiv:1808.00023*.'
  prefs: []
  type: TYPE_NORMAL
- en: 'P. Saleiro, B. Kuester, et al. (2018). [Aequitas: A bias and fairness audit
    toolkit](https://arxiv.org/pdf/1811.05577.pdf). *arXiv:1811.05577*.'
  prefs: []
  type: TYPE_NORMAL
- en: K. Makhlouf, S. Zhioua, et al. (2021). [On the Applicability of Machine Learning
    Fairness Notions](https://dl.acm.org/doi/10.1145/3468507.3468511). ACM SIGKDD
    Explorations Newsletter. 23, 1, 14–23.
  prefs: []
  type: TYPE_NORMAL
- en: B. Ruf & M. Detyniecki (2021). [Towards the Right Kind of Fairness in AI](https://arxiv.org/abs/2102.08453).
    ECML/PKDD 2021 Industry Track.
  prefs: []
  type: TYPE_NORMAL
