- en: So How Fair Is Your AI, Exactly?
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/so-how-fair-is-your-ai-exactly-83f8defcf449?source=collection_archive---------14-----------------------#2023-02-24](https://towardsdatascience.com/so-how-fair-is-your-ai-exactly-83f8defcf449?source=collection_archive---------14-----------------------#2023-02-24)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: About the challenge of getting the fairness objective right
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@boris-ruf?source=post_page-----83f8defcf449--------------------------------)[![Boris
    Ruf](../Images/96dc4fc2f32add89fef6911195590cd8.png)](https://medium.com/@boris-ruf?source=post_page-----83f8defcf449--------------------------------)[](https://towardsdatascience.com/?source=post_page-----83f8defcf449--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----83f8defcf449--------------------------------)
    [Boris Ruf](https://medium.com/@boris-ruf?source=post_page-----83f8defcf449--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fed341456850c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fso-how-fair-is-your-ai-exactly-83f8defcf449&user=Boris+Ruf&userId=ed341456850c&source=post_page-ed341456850c----83f8defcf449---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----83f8defcf449--------------------------------)
    ·6 min read·Feb 24, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F83f8defcf449&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fso-how-fair-is-your-ai-exactly-83f8defcf449&user=Boris+Ruf&userId=ed341456850c&source=-----83f8defcf449---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F83f8defcf449&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fso-how-fair-is-your-ai-exactly-83f8defcf449&source=-----83f8defcf449---------------------bookmark_footer-----------)![](../Images/ca243d1259ab5acfdc6fee99022d439e.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: Photo by [Pawel Czerwinski](https://unsplash.com/@pawel_czerwinski?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: '*The use of artificial intelligence (AI) has given rise to new ethical and
    legal challenges. In* [*my previous article*](/to-guarantee-impartial-ai-decisions-lady-justice-needs-to-blink-2992167b2591)
    *I illustrated why removing the sensitive information from the training data does
    not promote fairness, but rather the opposite. This article is about identifying
    the most appropriate fairness definition for an AI application. The bespoken tool
    has been originally presented in a* [*research paper*](https://arxiv.org/abs/2102.08453)
    *I co-published on the topic.*'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: 'Collecting and analyzing data through AI has become standard, and machine learning
    is improving business performances in many areas today. However, numerous cases
    of AI machine bias have been exposed in recent years, and new examples emerge
    on a regular basis. This is one of the main pitfalls of AI: if the training data
    include any kind of bias, the algorithm will incorporate and enforce it — potentially
    harming sensitive subgroups defined by gender, religion, ethnicity, or age.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: The challenge of defining ‘fairness’
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Current approaches to mitigate bias are mostly technical and focus on adjusting
    algorithms or data to satisfy some kind of fairness. However, multiple, conflicting
    notions of fairness exist and unfortunately there is no universally accepted definition.
    The most appropriate fairness metric always depends on the context of application.
    In practice, identifying the fairness objective is complicated because mapping
    ethical principles to metrics is not a straightforward process. Still, to obtain
    sustainable solutions for fairer AI, attention must be focused on this problem
    since the best technical mitigation method will fail when the implemented fairness
    objective is not aligned with the stakeholders’ expectations.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/28159840fee300fdcecb547d87477a8a.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
- en: Available fairness metrics for classification tasks. Image by author.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: Not on the same page
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In many scenarios, this decision is not a trivial one, as has become obvious
    in the heated debate over the COMPAS algorithm. This one had been developed by
    the company Northpointe to generate an independent, data-derived “risk score”
    for several forms of recidivism. Such an algorithm is used in the criminal justice
    sector in the US to support the judge with particular decisions such as granting
    of bail or parole. The score is of informative character and the final decision
    is still up to the judge.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: In May 2016, the investigative journalism website ProPublica focused attention
    on possible racial biases in the COMPAS algorithm. Its main argument was based
    on analysis of the data which showed that the results were biased. In particular,
    the [false positive rate](https://en.wikipedia.org/wiki/False_positive_rate) for
    people who were black was significantly higher compared to people who were white.
    As a result, black people were disproportionately often falsely attributed a high
    risk of recidivism. Northpointe, on the other hand, responded to the accusations
    by arguing that the algorithm effectively achieved [predictive value parity](https://en.wikipedia.org/wiki/Positive_and_negative_predictive_values)
    for both groups. In a nutshell, this ensured that risk scores corresponded to
    probabilities of reoffending, irrespective of any skin colour.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: 'From an objective point of view, it can be stated that both parties make valid
    and reasonable observations of the data. However, the huge controversy revealed
    that it is absolutely critical to precisely define and disclose the selected fairness
    objective for an application. And this decision usually involves arbitration and
    compromise. In the given scenario, for example, the two fairness objectives could
    only be mutually satisfied if one of the following conditions was met: Either
    the [base rates](https://en.wikipedia.org/wiki/Base_rate) of the sensitive subgroups
    are exactly identical, or the outcome classes are perfectly separable which would
    allow for creating an ideal classifier that achieves perfect accuracy. Unfortunately,
    both requirements are very unlikely in real world scenarios.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 从客观的角度来看，可以说双方对数据做出了有效且合理的观察。然而，巨大的争议揭示了精确定义和披露应用程序所选公平目标是绝对关键的。而这个决定通常涉及仲裁和妥协。在给定的场景中，例如，只有在满足以下条件之一时，两个公平目标才能互相满足：要么敏感子群体的[基本比率](https://en.wikipedia.org/wiki/Base_rate)完全相同，要么结果类别完全可分，从而能够创建一个实现完美准确性的理想分类器。不幸的是，这两个要求在现实世界中都非常不可能满足。
- en: Navigating metrics
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 导航指标
- en: Surprisingly, relatively little research has been conducted on how to streamline
    the fairness selection process for AI applications in practice. To overcome this
    challenge, we have developed the [Fairness Compass](https://axa-rev-research.github.io/fairness-compass.html),
    an experimental tool which seeks to structure the complex landscape of fairness
    metrics. Based on a set of concrete questions regarding the nature of their data,
    beliefs in its correctness, fairness policies, and whether the focus should be
    more on specificity or sensitivity of the model, the Fairness Compass guides AI
    practitioners towards the most suitable option for a given scenario. Formalizing
    this selection process and turning it into a straightforward procedure helps clear
    a hurdle for implementing responsible AI in the real world. Moreover, recording
    the reasoning behind the decisions can serve as documentation for internal purposes
    and as means of communication to increase transparency, fostering trust in the
    technology.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 令人惊讶的是，关于如何简化实际应用中的公平选择过程的研究相对较少。为了解决这个挑战，我们开发了[Fairness Compass](https://axa-rev-research.github.io/fairness-compass.html)，这是一个实验工具，旨在结构化复杂的公平指标领域。基于一系列关于数据性质、对其正确性的信念、公平政策以及模型应关注的特异性或灵敏度的问题，Fairness
    Compass 引导 AI 从业者找到最适合给定场景的选项。将这一选择过程形式化并转化为简单的程序，有助于消除在现实世界中实施负责任的 AI 的障碍。此外，记录决策背后的理由可以作为内部文档，并作为沟通手段来增加透明度，促进对技术的信任。
- en: The Fairness Compass has been published as open source project on [GitHub](https://github.com/axa-rev-research/fairness-compass).
    It was nominated for the [Gartner Eye on Innovation award](https://www.gartner.com/en/about/awards/eye-on-innovation)
    and has been included in the [AI Fairness Global Library](https://www.aifairnesslibrary.com)
    by the World Economic Forum.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: Fairness Compass 已作为开源项目发布在[GitHub](https://github.com/axa-rev-research/fairness-compass)上。它曾获得[Gartner
    Eye on Innovation award](https://www.gartner.com/en/about/awards/eye-on-innovation)提名，并被世界经济论坛纳入[AI
    Fairness Global Library](https://www.aifairnesslibrary.com)。
- en: Sample application
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 示例应用程序
- en: To illustrate the concept, let’s take a sample scenario in the human resources
    context. As sensitive subgroups, we consider men and women. The question to be
    answered is which definition of fairness would be most appropriate when it comes
    to assessing fairness in employee promotion decisions. Please note that this is
    just a fictional thought experiment, and depending on the context, other answers
    with different results may apply. The purpose of the Fairness Compass is to support
    well informed decision making based on the defined requirements for a given scenario.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明这个概念，让我们以人力资源领域的一个示例场景为例。作为敏感子群体，我们考虑男性和女性。需要回答的问题是，在评估员工晋升决策的公平性时，哪种公平定义最为合适。请注意，这只是一个虚构的思维实验，根据具体情况，其他答案可能会有不同的结果。Fairness
    Compass 的目的是基于给定场景的定义要求，支持做出明智的决策。
- en: '![](../Images/da24c0e28fa61f79a322054e92f858c4.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/da24c0e28fa61f79a322054e92f858c4.png)'
- en: B. Ruf and M. Detyniecki, “[Towards the Right Kind of Fairness in AI](https://axa-rev-research.github.io/static/AXA_FairnessCompass-English.pdf)”,
    ECML/PKDD 2021 (Industry Track)
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: 'In the animation above, the Fairness Compass is represented as a decision tree
    with three different types of nodes: The diamonds symbolize decision points; the
    white boxes stand for actions and the grey boxes with round corners are the fairness
    definitions. The arrows which connect the nodes represent the possible choices.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: Let’s kick-off the process. The first question is about existing policies which
    may influence the decision. Fairness objectives can go beyond equal treatment
    of different groups or similar individuals. If the target is to bridge prevailing
    inequalities by boosting underprivileged groups, affirmative actions or quotas
    can be valid measures. Such a goal may stem from law, regulation, or internal
    organizational guidelines. This approach rules out any possible causality between
    the sensitive attribute and the outcome. If the data tells a different story in
    terms of varying base rates across the subgroups, this is a strong commitment
    which leads to subordinating the algorithm’s accuracy to the policy’s overarching
    goal. For example, many universities aim to improve diversity by accepting more
    students from disadvantaged backgrounds. Such admission policies acknowledge an
    equally high academic potential of students from sensitive subgroups and considers
    their possibly lower level of education rather an injustice in society than a
    personal shortcoming.​​​​​​​
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: For our sample scenario, we conclude that no such affirmative action policy
    is in place for promotion decisions. Therefore, we choose “No” and document the
    reasoning behind our choice. Now, we continue with the next question and repeat
    the procedure until we reach a leaf node which contains the recommended fairness
    definition for the defined use case.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: Hence, following such a formalised process can significantly contribute to identifying
    and explaining the optimal fairness metric for a particular AI application.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: So what
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In practice, a lot of different definitions of fairness exist. Because some
    are mutually exclusive, it is necessary to settle for one of them. Making the
    choice is not trivial since the best decision always depends on the context of
    application and trade-offs are often unavoidable. Therefore, it is crucial to
    select the fairness objective of an AI application with great care and also communicate
    it to internal and external stakeholders. Transparency about the reasoning behind
    this decision is a key factor towards a sustainable implemention of fairer AI.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: '*Many thanks to Antoine Pietri for his valuable support in writing this post.
    In* [*the following article*](https://medium.com/just-tech-it-now/3-strategies-to-fight-discrimination-in-ai-applications-2ce78e7fae65)*,
    I will outline how to actively mitigate biases in AI applications.*'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: References
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: N. Mehrabi, F. Morstatter, et al. (2021). [A Survey on Bias and Fairness in
    Machine Learning](https://arxiv.org/abs/1908.09635). ACM Computing Surveys (CSUR)
    54, 6 (2021), 1–35.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: N. Mehrabi, F. Morstatter 等 (2021). [机器学习中的偏见与公平性调查](https://arxiv.org/abs/1908.09635)。ACM
    计算调查 (CSUR) 54, 6 (2021), 1–35。
- en: J. Angwin, J. Larson, et al. (2016). [Machine bias](https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing).
    Ethics of Data and Analytics, Auerbach Publications, 254–264.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: J. Angwin, J. Larson 等 (2016). [机器偏见](https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing)。数据与分析伦理，Auerbach
    出版社，254–264。
- en: 'W. Dieterich, C. Mendoza, et al. (2016). [COMPAS risk scales: Demonstrating
    accuracy equity and predictive parity](https://go.volarisgroup.com/rs/430-MBX-989/images/ProPublica_Commentary_Final_070616.pdf).
    Northpointe Inc.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: W. Dieterich, C. Mendoza 等 (2016). [COMPAS 风险量表：展示准确性、公平性和预测一致性](https://go.volarisgroup.com/rs/430-MBX-989/images/ProPublica_Commentary_Final_070616.pdf)。Northpointe
    Inc.
- en: 'S. Corbett-Davies & S. Goel (2018). [The Measure and Mismeasure of Fairness:
    A Critical Review of Fair Machine Learning](https://arxiv.org/abs/1808.00023).
    *arXiv:1808.00023*.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: S. Corbett-Davies & S. Goel (2018). [公平性的测量与误测量：对公平机器学习的批判性回顾](https://arxiv.org/abs/1808.00023)。*arXiv:1808.00023*。
- en: 'P. Saleiro, B. Kuester, et al. (2018). [Aequitas: A bias and fairness audit
    toolkit](https://arxiv.org/pdf/1811.05577.pdf). *arXiv:1811.05577*.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: P. Saleiro, B. Kuester 等 (2018). [Aequitas：一个偏见与公平性审计工具包](https://arxiv.org/pdf/1811.05577.pdf)。*arXiv:1811.05577*。
- en: K. Makhlouf, S. Zhioua, et al. (2021). [On the Applicability of Machine Learning
    Fairness Notions](https://dl.acm.org/doi/10.1145/3468507.3468511). ACM SIGKDD
    Explorations Newsletter. 23, 1, 14–23.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: K. Makhlouf, S. Zhioua 等 (2021). [关于机器学习公平性概念的适用性](https://dl.acm.org/doi/10.1145/3468507.3468511)。ACM
    SIGKDD 探索新闻通讯。23, 1, 14–23。
- en: B. Ruf & M. Detyniecki (2021). [Towards the Right Kind of Fairness in AI](https://arxiv.org/abs/2102.08453).
    ECML/PKDD 2021 Industry Track.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: B. Ruf & M. Detyniecki (2021). [迈向正确的人工智能公平性](https://arxiv.org/abs/2102.08453)。ECML/PKDD
    2021 行业专场。
