- en: Easily Implement Multiclass SVM From Scratch in Python
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/implement-multiclass-svm-from-scratch-in-python-b141e43dc084?source=collection_archive---------2-----------------------#2023-11-04](https://towardsdatascience.com/implement-multiclass-svm-from-scratch-in-python-b141e43dc084?source=collection_archive---------2-----------------------#2023-11-04)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Comes with a Free Deep Overview on SVMs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://essamwissam.medium.com/?source=post_page-----b141e43dc084--------------------------------)[![Essam
    Wisam](../Images/6320ce88ba2e5d56d70ce3e0f97ceb1d.png)](https://essamwissam.medium.com/?source=post_page-----b141e43dc084--------------------------------)[](https://towardsdatascience.com/?source=post_page-----b141e43dc084--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----b141e43dc084--------------------------------)
    [Essam Wisam](https://essamwissam.medium.com/?source=post_page-----b141e43dc084--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fccb82b9f3b87&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimplement-multiclass-svm-from-scratch-in-python-b141e43dc084&user=Essam+Wisam&userId=ccb82b9f3b87&source=post_page-ccb82b9f3b87----b141e43dc084---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----b141e43dc084--------------------------------)
    ·14 min read·Nov 4, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fb141e43dc084&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimplement-multiclass-svm-from-scratch-in-python-b141e43dc084&user=Essam+Wisam&userId=ccb82b9f3b87&source=-----b141e43dc084---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb141e43dc084&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimplement-multiclass-svm-from-scratch-in-python-b141e43dc084&source=-----b141e43dc084---------------------bookmark_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: In this story, we shall implement the support vector machine learning algorithm
    in its general soft-margin and kernelized form. We will start by providing a brief
    overview of SVM and its training and inference equations, then subsequently translate
    these into code to develop the SVM model. Afterwards, we extend our implementation
    to handle multiclass scenarios and conclude by testing our model using Sci-kit
    Learn.
  prefs: []
  type: TYPE_NORMAL
- en: 'Thus, by the end of this story:'
  prefs: []
  type: TYPE_NORMAL
- en: You will gain a clear perspective of various important SVM concepts.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You will be able to implement, with genuine comprehension, the SVM model from
    scratch for the binary and multiclass cases.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/6a4535e70a07e8f4eb155ced0108351c.png)'
  prefs: []
  type: TYPE_IMG
- en: Beautiful Van Gogh painting for Two Stars and a Line Between them like Starry
    Night— Generated by author using DALLE 2
  prefs: []
  type: TYPE_NORMAL
- en: Table of Content
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: · [Brief Overview](#af4c)
  prefs: []
  type: TYPE_NORMAL
- en: ∘ [Hard Margin SVM](#018a)
  prefs: []
  type: TYPE_NORMAL
- en: ∘ [Soft Margin SVM](#0953)
  prefs: []
  type: TYPE_NORMAL
- en: ∘ [Kernel Soft Margin SVM](#a603)
  prefs: []
  type: TYPE_NORMAL
- en: · [Implementation](#48da)
  prefs: []
  type: TYPE_NORMAL
- en: ∘ [Basic Imports](#25b3)
  prefs: []
  type: TYPE_NORMAL
- en: ∘ [Defining Kernels and SVM Hyperparameters](#9c9f)
  prefs: []
  type: TYPE_NORMAL
- en: ∘ [Define the Predict Method](#ce06)
  prefs: []
  type: TYPE_NORMAL
- en: ∘ [Define the Predict Method](#3067)
  prefs: []
  type: TYPE_NORMAL
- en: ∘ [Test the Implementation](#a728)
  prefs: []
  type: TYPE_NORMAL
- en: ∘ [Generalizing Fit to Multiclass](#19b6)
  prefs: []
  type: TYPE_NORMAL
- en: ∘ [Generalizing Predict to Multiclass](#ec98)
  prefs: []
  type: TYPE_NORMAL
- en: ∘ [Testing the Implementation](#6d1e)
  prefs: []
  type: TYPE_NORMAL
- en: Brief Overview
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Hard Margin SVM
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The goal in SVM is to fit the hyperplane that would attain the maximum margin
    (distance from the closest points from in the two classes). It can be shown, and
    is intuitive that such hyperplane (A) has better generalization properties and
    is more robust to noise than a hyperplane that doesn’t maximize the margin (B).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/737a87973433b07c15a0387f14999fa6.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure by [Ennepetaler86](https://commons.wikimedia.org/wiki/User:Ennepetaler86)
    on [Wikimedia](https://commons.wikimedia.org/wiki/File:Svm_intro.svg). [CC BY-SA
    3.0 Unported](https://creativecommons.org/licenses/by-sa/3.0/deed.en).
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to achieve this, SVM finds the hyperplane’s *W* and b by solving the
    following optimization problem:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/32796612256ef92f355b7ddfd38d5449.png)'
  prefs: []
  type: TYPE_IMG
- en: 'It attempts to find *W,b* that maximizes the distance to the closest point
    and classifies everything correctly (as in the constraint where y takes ±1). This
    can be shown to be equivalent to the following optimization problem:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1895a0f3d251c878035d72174f328005.png)'
  prefs: []
  type: TYPE_IMG
- en: For which one can write the equivalent [dual](https://medium.com/p/de09f645b068)
    optimization problem
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/005e90809d22718fd8291ae9b8ccd40a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The solution to this yields a Lagrange multiplier for each point in the dataset
    which we assume to have size *m: (α****₁****, α₂, …, α_N)*. The objective function
    is clearly quadratic in *α* and the constraints are linear which means that this
    can be easily solved with [quadratic programming](https://en.wikipedia.org/wiki/Quadratic_programming).
    Once the solution found, it follows from the derivation of the dual that:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4482d9a60a50d728e3753aeb4fcf2c08.png)'
  prefs: []
  type: TYPE_IMG
- en: '*(xₛ,yₛ)* is any point with *α>0*'
  prefs: []
  type: TYPE_NORMAL
- en: Notice that only points with *α>0* define the hyperplane (contribute to the
    sum). Those are called support vectors.
  prefs: []
  type: TYPE_NORMAL
- en: 'And thereby, the prediction equation, that when given a new example *x,* returns
    its prediction *y=±1* is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/be855d245c929e03b7b9c07efc96430e.png)'
  prefs: []
  type: TYPE_IMG
- en: Involves plugging then doing some algebraic simplification
  prefs: []
  type: TYPE_NORMAL
- en: This basic form of SVM is called hard margin SVM because the optimization problem
    it solves (as defined above) enforces that all points in training must be classified
    correctly. In practical scenarios, there may be some noise that prevents or limits
    the existence of a hyperplane that perfectly separates the data in which case
    the optimization problem would return no or a poor solution.
  prefs: []
  type: TYPE_NORMAL
- en: Soft Margin SVM
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/a76e2673690129189779c4d61cd0ec0f.png)'
  prefs: []
  type: TYPE_IMG
- en: Fit by Soft Margin SVM by [Mangat et al](https://www.researchgate.net/publication/366243093_High-Frequency_Trading_with_Machine_Learning_Algorithms_and_Limit_Order_Book_Data)
    on [Research Gate](https://www.researchgate.net/figure/Support-vector-classification-for-the-non-separable-case-Points-corresponding-to-x-1-and_fig2_366243093).
    [CC BY-SA 4.0 International](https://creativecommons.org/licenses/by/4.0/)
  prefs: []
  type: TYPE_NORMAL
- en: 'To generalize hard margin SVM, soft margin SVM adapts the optimization problem
    by introducing a C constant (user given hyperparamer) that controls how “hard”
    it should be. In particular, it modifies the primal optimization problem to become
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/dcca387cb13de3cd5905b5325539708b.png)'
  prefs: []
  type: TYPE_IMG
- en: modifications in blue
  prefs: []
  type: TYPE_NORMAL
- en: which allows each point to make some violation ϵₙ (e.g., be in the wrong side
    of the hyperplane) but still aims to reduce them overall by weighting their sum
    in the objective function by C. It becomes equivalent to hard margin as C approaches
    infinity (generally before it does). Meanwhile, a smaller C would allow more violations
    (in return for a wider margin; i.e., smaller *wᵗw*).
  prefs: []
  type: TYPE_NORMAL
- en: Quite surprisingly, it can be shown that the equivalent dual problem only changes
    by constraining *α* for each point to be *≤C.*
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4e0378a5399a659e15e095b16648da47.png)'
  prefs: []
  type: TYPE_IMG
- en: Since violations are allowed, support vectors (points with *α>0)* are no longer
    all on the margin’s edge. It can be shown that any support vector that has committed
    a violation will have *α=C* and that non-support vectors (*α=0)* cannot commit
    violations. We call support vectors that potentially committed violations (*α=C*)
    “non-margin support vectors” and other pure ones (that have not committed violations;
    lie on the edge) “margin support vectors” (0<*α<C*).
  prefs: []
  type: TYPE_NORMAL
- en: 'It can be shown that the inference equation doesn’t change:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/be855d245c929e03b7b9c07efc96430e.png)'
  prefs: []
  type: TYPE_IMG
- en: However, now *(xₛ,yₛ)* must now be a support vector that has not committed violations
    because the equation assumes it’s on the margin’s edge (previously, any support
    vector could be used).
  prefs: []
  type: TYPE_NORMAL
- en: Kernel Soft Margin SVM
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Soft Margin SVM extends the Hard Margin SVM to handle noise, but frequently,
    data is not separable by a hyperplane due to factors beyond noise, such as being
    naturally nonlinear. Soft Margin SVM can be used in cases like these, but then
    the optimal solution will probably involve a hyperplane that permits much more
    errors than is tolerable in reality.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2ef99eb747b1d860fffc880654080914.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure by [Machine Learner](https://commons.wikimedia.org/w/index.php?title=User%3AMachine_Learner&action=edit&redlink=1)
    on [Wikimedia](https://commons.wikimedia.org/wiki/File:Nonlinear_SVM_example_illustration.svg).
    [CC BY-SA 4.0 International](https://creativecommons.org/licenses/by/4.0/)
  prefs: []
  type: TYPE_NORMAL
- en: Kernel Soft Margin SVM generalizes Soft Margin SVM to deal with situations where
    the data is naturally nonlinear. For instance, in the example shown on the left
    there is no linear hyperplane that Soft Margin SVM can find, regardless to the
    setting of C, that would decently separate the data.
  prefs: []
  type: TYPE_NORMAL
- en: 'It is possible, however, to map each point *x* in the dataset to a higher dimension
    via some transform function *z=Φ(x)*to make the data more linear (or perfectly
    linear) in the new higher dimensional space. This is equivalent to replacing *x*
    with *z* in the dual to get:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1386d4ace7586b2a1daf2f084989ac78.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In reality, especially when *Φ* transforms into a very high-dimensional space,
    computing *z* can take a very long time. This is solved by the kernel trick which
    replaces the *z*ᵗ*z* with an **equivalent** computation of a mathematical function
    (called kernel function) and which is much faster (e.g., an algebraic simplification
    of *z*ᵗ*z).* For instance, here are some popular kernel functions (each of which
    corresponds to some transformation *Φ* to a higher dimensional space):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/bf4c4cc694cdc99432a46577cc381b04.png)'
  prefs: []
  type: TYPE_IMG
- en: Degree of polynomial (Q) and RBF γ are hyperparameters (decided by the user)
  prefs: []
  type: TYPE_NORMAL
- en: 'With this, the dual optimization problem becomes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8efaaa3293103ac2d3719eb572b25de6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'and intuitively, the inference equation becomes (after algebraic manipulation):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6cdc91661bd6a7a3600c83566e74ed4d.png)'
  prefs: []
  type: TYPE_IMG
- en: A full derivation of all the equations above assuming that you have the [mathematical
    background](https://medium.com/p/de09f645b068) can be found [here](https://drive.google.com/file/d/1f_VRZ_SICOnciEoLr-Nig4cr2fLczE_y/view?usp=sharing).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b9d94720b23e6f3ccc0e570e0db6320a.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Scott Graham](https://unsplash.com/@homajob?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: Implementation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For the implementation we will use
  prefs: []
  type: TYPE_NORMAL
- en: Basic Imports
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We start by importing some basic libraries:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Defining Kernels and SVM Hyperparameters
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We start by defining the three kernels using their respective functions
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/bf4c4cc694cdc99432a46577cc381b04.png)'
  prefs: []
  type: TYPE_IMG
- en: Degree of polynomial (Q) and RBF γ are hyperparameters (decided by the user)
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: For consistency with other kernels, the linear kernel takes an extra useless
    hyperparameter. As obvious, `kernel_funs` takes a string for the kernel and returns
    the corresponding kernel function.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now let’s carry on by defining the constructor:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: The SVM has three main hyperparameters, the kernel (we store the string given
    and the corresponding kernel function), the regularization parameter C and the
    kernel hyperparameter (to be passed to the kernel function); it represents Q for
    the polynomial kernel and γ for the RBF kernel.
  prefs: []
  type: TYPE_NORMAL
- en: Define the Fit Method
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To extend this class with `fit` and `predict` functions in separate cells,
    we will define the following function and use it as a decorator later:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Recall that fitting the SVM corresponds to finding the support vector *α* for
    each point by solving the dual optimization problem:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8e481f884c10d6dc6e79e6d1a2d629e6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let ***α*** be a variable column vector*(α****₁*** *α₂ … α_N)*ᵗ and let y be
    a constant column vector for the labels *(y****₁*** *y₂ … y_N)*ᵗ and let *K* be
    a constant matrix where *K[n,m]* computes the kernel at *(xₙ, xₘ)*. Recall the
    following index-based equivalences for the dot product, outer product and quadratic
    form respectively:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c2f6f972e0384b38163c88cab37a80c1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'to be able to write the dual optimization problem in matrix form as follow:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ebff3482f7b778cab6f88c0dd476e20b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Knowing that this is a quadratic program as we hinted earlier, we can read
    over Quadratic Programming in CVXOPT’s documentation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/284be9ba0c0ee37d6eaea4c509310e0f.png)'
  prefs: []
  type: TYPE_IMG
- en: From [CVXOPT](https://cvxopt.org/userguide/coneprog.html) Documentation. [GNU
    General Public License](http://www.gnu.org/licenses/gpl-3.0.html)
  prefs: []
  type: TYPE_NORMAL
- en: The square brackets tell you that you can call this with *(P,q)* only or *(P,q,G,h)*
    or *(P, q, G, h, A, b)* and so on (anything not given will be set by a default
    value such as 1).
  prefs: []
  type: TYPE_NORMAL
- en: 'To know the values for *(P, q, G, h, A, b)* in our case, we make the following
    comparison:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/72e0c1e764846319423f64a053c20afb.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let’s the comparison easier by rewriting the first as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0b55408509fa24c6b148c5be93f13f9d.png)'
  prefs: []
  type: TYPE_IMG
- en: Notice that we changed the max to min by multiplying the function by -1
  prefs: []
  type: TYPE_NORMAL
- en: It’s now obvious that (note that *0≤****α*** is equivalent to *-α≤0):*
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3d0d0333f4492e1ba5846108e1255f36.png)'
  prefs: []
  type: TYPE_IMG
- en: 'By this, we can write the following fit function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: We ensure that this is a binary problem and that the binary labels are set as
    assumed by SVM (±1) and that *y* is a column vector with dimensions *(N,1)*. Then
    we solve the optimization problem to find *(α****₁*** *α₂ … α_N)*ᵗ.
  prefs: []
  type: TYPE_NORMAL
- en: We use *(α****₁*** *α₂ … α_N)*ᵗ to get an array of flags that is 1 at any index
    corresponding to a support vector so that we can later apply the prediction equation
    by only summing over support vectors and an index for a margin support vector
    for *(xₛ,yₛ).* Notice that in the checks we do assume that non-support vectors
    may not have *α=0* exactly, if it’s *α≤1e-3* then this is approximately zero (we
    know CVXOPT results may not be ultimately precise). Likewise, we assume that non-margin
    support vectors may not have *α=C* exactly.
  prefs: []
  type: TYPE_NORMAL
- en: Define the Predict Method
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Recall the prediction equation is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6cdc91661bd6a7a3600c83566e74ed4d.png)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: That’s it. We can also implement an `evaluate` method to compute the accuracy
    (used in fit above).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Test the Implementation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: You can change the dataset and hyperparameter to further ensure that they are
    the same. Ideally, do so by comparing decision functions rather than accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: Generalizing Fit to Multiclass
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: To generalize the model to multiclass, over *k* classes. We train a binary SVM
    classifier for each class present where we loop on each class and relabel points
    belonging to it into +1 and points from all other classes into -1.
  prefs: []
  type: TYPE_NORMAL
- en: The result from training is *k* classifiers when given *k* classes where the
    *ith* classifier was trained on the data with the *ith* class being labeled as
    +1 and all others being labeled as -1.
  prefs: []
  type: TYPE_NORMAL
- en: Generalizing Predict to Multiclass
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Then to perform prediction on a new example, we choose the class for which the
    corresponding classifier is most confident (has the highest score)
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Testing the Implementation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Plotting the decision regions for each further leads to the following plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/63a5bb19e547c15ef319422d7c37cd60.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure by the author
  prefs: []
  type: TYPE_NORMAL
- en: Beware that, although Sci-kit Learn’s SVM supports OVR by default (without an
    explicit call as shown above), that potentially also has further optimizations
    specific to SVM.
  prefs: []
  type: TYPE_NORMAL
- en: Full Code
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/50a2723007014b05afd3968090843780.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Nathan Van Egmond](https://unsplash.com/@thevanegmond?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: In summary, we implemented the support vector machine (SVM) learning algorithm,
    covering its general soft-margin and kernelized form. We provided an overview
    of SVM, developed the model in code, extended it for multiclass scenarios, and
    validated our implementation using Sci-kit Learn.
  prefs: []
  type: TYPE_NORMAL
- en: Hope you find what you have learnt from this story useful for your work. Till
    next time, au revoir.
  prefs: []
  type: TYPE_NORMAL
- en: '**Resources:**'
  prefs: []
  type: TYPE_NORMAL
- en: 'The code is mostly adaptations over the one present here ([MIT License](https://opensource.org/license/mit/)):'
  prefs: []
  type: TYPE_NORMAL
- en: Persson, Aladdin. “[SVM from Scratch — Machine Learning Python (Support Vector
    Machine](https://github.com/aladdinpersson/Machine-Learning-Collection/blob/master/ML/algorithms/svm/svm.py)).”
    [YouTube](https://www.youtube.com/watch?v=gBTtR0bs-1k).
  prefs: []
  type: TYPE_NORMAL
