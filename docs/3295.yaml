- en: Hidden Markov Models Explained with a Real Life Example and Python code
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 隐藏马尔可夫模型：通过实际例子和 Python 代码进行解释
- en: 原文：[https://towardsdatascience.com/hidden-markov-models-explained-with-a-real-life-example-and-python-code-2df2a7956d65?source=collection_archive---------0-----------------------#2023-11-05](https://towardsdatascience.com/hidden-markov-models-explained-with-a-real-life-example-and-python-code-2df2a7956d65?source=collection_archive---------0-----------------------#2023-11-05)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/hidden-markov-models-explained-with-a-real-life-example-and-python-code-2df2a7956d65?source=collection_archive---------0-----------------------#2023-11-05](https://towardsdatascience.com/hidden-markov-models-explained-with-a-real-life-example-and-python-code-2df2a7956d65?source=collection_archive---------0-----------------------#2023-11-05)
- en: '[](https://carolinabento.medium.com/?source=post_page-----2df2a7956d65--------------------------------)[![Carolina
    Bento](../Images/9585232979bf7c2dbad05934f0735d89.png)](https://carolinabento.medium.com/?source=post_page-----2df2a7956d65--------------------------------)[](https://towardsdatascience.com/?source=post_page-----2df2a7956d65--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----2df2a7956d65--------------------------------)
    [Carolina Bento](https://carolinabento.medium.com/?source=post_page-----2df2a7956d65--------------------------------)'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://carolinabento.medium.com/?source=post_page-----2df2a7956d65--------------------------------)[![Carolina
    Bento](../Images/9585232979bf7c2dbad05934f0735d89.png)](https://carolinabento.medium.com/?source=post_page-----2df2a7956d65--------------------------------)[](https://towardsdatascience.com/?source=post_page-----2df2a7956d65--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----2df2a7956d65--------------------------------)
    [Carolina Bento](https://carolinabento.medium.com/?source=post_page-----2df2a7956d65--------------------------------)'
- en: ·
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fe960c0367546&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhidden-markov-models-explained-with-a-real-life-example-and-python-code-2df2a7956d65&user=Carolina+Bento&userId=e960c0367546&source=post_page-e960c0367546----2df2a7956d65---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----2df2a7956d65--------------------------------)
    ·14 min read·Nov 5, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F2df2a7956d65&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhidden-markov-models-explained-with-a-real-life-example-and-python-code-2df2a7956d65&user=Carolina+Bento&userId=e960c0367546&source=-----2df2a7956d65---------------------clap_footer-----------)'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fe960c0367546&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhidden-markov-models-explained-with-a-real-life-example-and-python-code-2df2a7956d65&user=Carolina+Bento&userId=e960c0367546&source=post_page-e960c0367546----2df2a7956d65---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----2df2a7956d65--------------------------------)
    ·14分钟阅读·2023年11月5日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F2df2a7956d65&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhidden-markov-models-explained-with-a-real-life-example-and-python-code-2df2a7956d65&user=Carolina+Bento&userId=e960c0367546&source=-----2df2a7956d65---------------------clap_footer-----------)'
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F2df2a7956d65&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhidden-markov-models-explained-with-a-real-life-example-and-python-code-2df2a7956d65&source=-----2df2a7956d65---------------------bookmark_footer-----------)![](../Images/418afba8ee3df1346601a1d1e5c1623f.png)'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F2df2a7956d65&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhidden-markov-models-explained-with-a-real-life-example-and-python-code-2df2a7956d65&source=-----2df2a7956d65---------------------bookmark_footer-----------)![](../Images/418afba8ee3df1346601a1d1e5c1623f.png)'
- en: Image by Author
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片
- en: '*Hidden Markov Models are probabilistic models used to solve real life problems
    ranging from something everyone thinks about at least once a week — how is the
    weather going to be like tomorrow?*[1] — *to hard molecular biology problems,
    such as predicting peptide binders to the human MHC class II molecule*[2].'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '*隐藏马尔可夫模型是用于解决现实生活中的问题的概率模型，问题范围从每个人每周至少考虑一次的简单问题——明天的天气会如何？*[1] — *到复杂的分子生物学问题，例如预测肽与人类MHC
    II类分子的结合位点*[2]。'
- en: '*Hidden Markov Models are close relatives of Markov Chains, but their hidden
    states make them a unique tool to use when you’re interested in determining the
    probability of a sequence of random variables.*'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '*隐藏马尔可夫模型是马尔可夫链的近亲，但其隐藏状态使其成为确定随机变量序列概率时的独特工具。*'
- en: '*In this article we’ll breakdown Hidden Markov Models into all its different
    components and see, step by step with both the Math and Python code, which emotional
    states led to your dog’s results in a training exam. We’ll be using the Viterbi
    Algorithm to determine the probability of observing a specific sequence of observations,
    and how you can use the Forward Algorithm to determine the likelihood of an observed
    sequence, when you’re given a sequence of hidden states.*'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '*在本文中，我们将拆解隐马尔可夫模型的所有不同组件，并一步步展示数学和Python代码，看看哪些情感状态导致了你狗狗在训练考试中的结果。我们将使用维特比算法来确定观察到特定观测序列的概率，并展示如何使用前向算法来确定观察到序列的可能性，当你给出一个隐藏状态序列时。*'
- en: The real world is full of phenomena for which we can see the final outcome,
    but can’t actually observe the underlying factors that generated those outcomes.
    One example is predicting the weather, determining if it’s going to be rainy or
    sunny tomorrow, based on past weather observations and the observed probabilities
    of the different weather outcomes.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 现实世界充满了这样的现象：我们可以看到最终结果，但无法实际观察生成这些结果的潜在因素。例如，根据过去的天气观测和不同天气结果的观察概率来预测天气，确定明天是雨天还是晴天。
- en: Although driven by factors we can’t observe, with an **Hidden Markov Model**
    it’s possible to model these phenomena as probabilistic systems.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管受我们无法观察的因素驱动，利用**隐马尔可夫模型**可以将这些现象建模为概率系统。
- en: Markov Models with Hidden States
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 隐藏状态的马尔可夫模型
- en: '[Hidden Markov Models](https://en.wikipedia.org/wiki/Hidden_Markov_model),
    known as HMM for short, are statistical models that work as a sequence of labeling
    problems. These are the types of problems that describe the evolution of observable
    events, which themselves, are dependent on internal factors that can’t be directly
    observed — they are **hidden**[3].'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '[隐马尔可夫模型](https://en.wikipedia.org/wiki/Hidden_Markov_model)，简称HMM，是一种统计模型，作为一系列标记问题进行工作。这些问题描述了可观察事件的演变，而这些事件本身依赖于无法直接观察的内部因素——它们是**隐藏的**[3]。'
- en: An Hidden Markov Model is made of two distinct [stochastic processes](https://en.wikipedia.org/wiki/Stochastic_process),
    meaning those are processes that can be defined as sequences of [random variables](https://en.wikipedia.org/wiki/Random_variable)
    — variables that depend on random events.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 隐马尔可夫模型由两个不同的[随机过程](https://en.wikipedia.org/wiki/Stochastic_process)组成，这些过程可以定义为[随机变量](https://en.wikipedia.org/wiki/Random_variable)的序列——这些变量依赖于随机事件。
- en: There’s an **invisible process** and an **observable process**.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 存在**隐形过程**和**可观察过程**。
- en: The **invisible process** is a [Markov Chain](/markov-models-and-markov-chains-explained-in-real-life-probabilistic-workout-routine-65e47b5c9a73),
    like chaining together multiple **hidden states** that are traversed over time
    in order to reach an outcome. This is a probabilistic process because all the
    parameters of the Markov Chain, as well as the score of each sequence, are in
    fact probabilities[4].
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '**隐形过程**是一个[马尔可夫链](/markov-models-and-markov-chains-explained-in-real-life-probabilistic-workout-routine-65e47b5c9a73)，如同将多个**隐藏状态**串联在一起，随着时间的推移以达到某个结果。这是一个概率过程，因为马尔可夫链的所有参数以及每个序列的得分实际上都是概率[4]。'
- en: Hidden Markov Models describe the evolution of observable events, which themselves,
    are dependent on internal factors that can’t be directly observed — they are **hidden**[3]
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 隐马尔可夫模型描述了可观察事件的演变，这些事件本身依赖于无法直接观察的内部因素——它们是**隐藏的**[3]。
- en: Just like any other [Markov Chain](/markov-models-and-markov-chains-explained-in-real-life-probabilistic-workout-routine-65e47b5c9a73),
    in order to know which state you’re going next, the only thing that matters is
    where you are now — in which state of the Markov Chain you’re currently in. None
    of the previous *history* of states you’ve been in the past matters to understand
    where you’re going next.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 就像其他[马尔可夫链](/markov-models-and-markov-chains-explained-in-real-life-probabilistic-workout-routine-65e47b5c9a73)一样，为了知道你接下来会进入哪个状态，唯一重要的因素是你现在的位置——即你目前处于马尔可夫链的哪个状态。你过去的*历史*状态对于理解你接下来要去的地方并没有意义。
- en: This kind of *short-term* memory is one of the key characteristics of HMMs and
    it’s called the **Markov Assumption**, indicating that the probability of reaching
    the next state is only dependent on the probability of the current state.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 这种*短期*记忆是HMM的关键特征之一，称为**马尔可夫假设**，表示到达下一个状态的概率仅依赖于当前状态的概率。
- en: '![](../Images/097a3c7f00c78d7bb4e6a10be3600758.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/097a3c7f00c78d7bb4e6a10be3600758.png)'
- en: Markov Assumption. (Image by Author)
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 马尔可夫假设。（作者提供的图像）
- en: The other key characteristic of an HMM, is that it also assumes that each observation
    is only dependent on the state that produced it therefore, being completely independent
    from any other state in the chain[5].
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: HMM 的另一个关键特征是，它还假设每次观察仅依赖于产生该观察的状态，因此完全独立于链中的其他状态[5]。
- en: The **Markov Assumption** states that the probability of reaching the next state
    is only dependent on the probability of the current state.
  id: totrans-24
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**马尔可夫假设**表明，达到下一个状态的概率仅依赖于当前状态的概率。'
- en: This is all great background information on HMM but, what classes of problems
    are they actually used in?
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 这些都是关于 HMM 的极好背景信息，但它们实际上用于哪些问题类别？
- en: 'HMMs help model the behavior of phenomena. Besides modeling and allowing to
    run simulations, you can also ask different types of questions those phenomena:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: HMM 帮助建模现象的行为。除了建模和允许进行模拟，你还可以提出关于这些现象的不同类型的问题：
- en: '**Likelihood** or **Scoring**, as in, determining the probability of observing
    a sequence'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**似然性**或**评分**，即确定观察到序列的概率'
- en: '**Decoding** the best sequence of states that generated a specific observation'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**解码**生成特定观察的最佳状态序列'
- en: '**Learning** the parameters of the HMM that led to observing a given sequence,
    that traversed a specific set of states.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**学习** HMM 参数，这些参数用于观察到特定序列，该序列遍历了特定的状态集合。'
- en: Let's see this in practice!
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看实际操作吧！
- en: Dog Training Performance as an HMM
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 作为 HMM 的狗训练表现
- en: Today you’re not as worried about the weather forecast, what’s on your mind
    is that your dog is possibly graduating from their training lessons. After all
    the time, effort and dog treats involved, all you want is for them to succeed.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 今天你不太担心天气预报，思考的重点是你的狗是否可能从训练课程中毕业。经过所有的时间、精力和狗狗零食，你只希望它们能成功。
- en: During dog training sessions, your four-legged friend is expected to do a few
    actions or tricks, so the trainer can *observe* and grade their performance. After
    combining the scores of three trials, they’ll determine if your dog graduates
    or needs additional training.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在狗狗训练过程中，你的四脚朋友会做一些动作或把戏，教练可以*观察*并评分它们的表现。通过合并三次试验的分数，他们会判断你的狗是否毕业或是否需要额外的训练。
- en: The trainer only sees the outcome, but there are several factors involved that
    can’t be directly observed such as, if your dog is tired, happy, if they don’t
    like the trainer at all or the other dogs around them.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 教练只看到结果，但涉及到的几个因素是无法直接观察的，例如你的狗是否疲倦、快乐、是否不喜欢教练或周围的其他狗。
- en: None of these are directly observed, unless there’s undoubtably a specific action
    your dog does only when they feel a certain way. Would be great if they could
    express how they feel in words, maybe in the future!
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 这些状态都无法直接观察，除非你的狗在特定情绪下做出明显的行动。如果它们能用语言表达自己的感受，那就太好了，也许未来能实现！
- en: With Hidden Markov Models fresh in your mind, this looks like the perfect opportunity
    to try to predict how your dog was feeling during the exam. They might get a certain
    score because they were feeling tired, maybe they were hungry, or they were annoyed
    at the trainer.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在脑海中还新鲜的隐马尔可夫模型，这看起来是预测你狗在考试期间感觉如何的绝佳机会。它们可能因为感到疲倦、饥饿或对教练感到烦恼而获得某个分数。
- en: Your dog has been taking lessons for a while and, based on data collected during
    that training, you have all the building blocks needed to build a Hidden Markov
    Model.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 你的狗已经接受了一段时间的训练，基于训练过程中收集的数据，你拥有了建立隐马尔可夫模型所需的所有基础构件。
- en: 'In order to build a HMM that models the performance of your dog in the training
    evaluation you need:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 为了建立一个建模你狗在训练评估中表现的 HMM，你需要：
- en: Hidden States
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 隐藏状态
- en: Transition Matrix
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 转移矩阵
- en: Sequence of Observations
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 观察序列
- en: Observation Likelihood Matrix
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 观察似然矩阵
- en: Initial Probability Distribution
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 初始概率分布
- en: '**Hidden States** arethose non-observable factors that influence the observation
    sequence. You’ll only consider if your dog is Tired or Happy.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '**隐藏状态**是那些影响观察序列的不可观察因素。你只会考虑你的狗是疲倦还是快乐。'
- en: '![](../Images/787e1db3f5cad33f98426995562f4dc4.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/787e1db3f5cad33f98426995562f4dc4.png)'
- en: '*Different hidden states in the HMM. (Image by Author)*'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '*HMM 中的不同隐藏状态。（作者提供的图像）*'
- en: Knowing your dog very well, the non-observable factors that can impact their
    exam performance are simply being tired or happy.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 了解你的狗后，非可观察因素可能影响它们的考试表现，仅仅是疲劳或快乐。
- en: Next you need to know what’s the probability of going from one state to another,
    which is captured in a **Transition Matrix**. This matrix must also be [**row
    stochastic**](https://en.wikipedia.org/wiki/Stochastic_matrix) meaning that the
    probabilities from one state to any other state in the chain, each row in the
    matrix, must sum to one.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来你需要知道从一个状态转移到另一个状态的概率，这些信息被记录在**转移矩阵**中。这个矩阵也必须是[**行随机矩阵**](https://en.wikipedia.org/wiki/Stochastic_matrix)，意味着链中从一个状态到任何其他状态的概率，每一行的总和必须为一。
- en: '![](../Images/33188ec7cf3bd8bc24208d2287b6f5fa.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/33188ec7cf3bd8bc24208d2287b6f5fa.png)'
- en: '*Transition Matrix: represents the probability of moving from one state to
    another.* (Image by Author)'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '*转移矩阵：表示从一个状态转移到另一个状态的概率。*（图像来源：作者）'
- en: Regardless of what type of problem you’re solving for, you always need a **Sequence
    of Observations.** Each observation representing the result of traversing the
    Markov Chain. Each observation is drawn from a specific vocabulary.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 不论你解决的是什么类型的问题，你总是需要一个**观测序列**。每个观测值表示在马尔可夫链中遍历的结果。每个观测值都是从特定的词汇表中抽取的。
- en: '![](../Images/303710a9d5933e8f0a24d35346e02be6.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/303710a9d5933e8f0a24d35346e02be6.png)'
- en: '*Vocabulary (Image by Author)*'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '*词汇表（图像来源：作者）*'
- en: In the case of your dog’s exam you observe the score they get after each trial,
    which can be *Fail*, *OK* or *Perfect*. These are all the possible *terms* in
    the observation vocabulary.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在你狗的考试中，你会观察到每次试验后的得分，这可能是*失败*、*OK*或*完美*。这些都是观测词汇表中的所有可能*术语*。
- en: You also need the **Observation Likelihood Matrix**, which is the probability
    of an observation being generated from a specific state.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 你还需要**观测似然矩阵**，即从特定状态生成观测值的概率。
- en: '![](../Images/374897edecaebef5b8b473b802176c83.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/374897edecaebef5b8b473b802176c83.png)'
- en: Observation Likelihood Matrix. (Image by Author)
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 观测似然矩阵。（图像来源：作者）
- en: Finally, there’s the **Initial Probability Distribution**. This is the probability
    that the Markov Chain will start in each specific hidden state.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 最后是**初始概率分布**。这是马尔可夫链在每个特定隐藏状态下开始的概率。
- en: There can also be some states will never be the starting state in the Markov
    Chain. In these situations, their initial probability is zero. And just like the
    probabilities in the Transition Matrix, these sum of all initial probabilities
    must add up to one.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在马尔可夫链中，有些状态可能永远不会成为起始状态。在这些情况下，它们的初始概率为零。就像转移矩阵中的概率一样，这些初始概率的总和也必须加起来为一。
- en: '![](../Images/50d55e6acc0101b554e7719829e25c82.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/50d55e6acc0101b554e7719829e25c82.png)'
- en: Initial Probabilities (Image by Author)
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 初始概率（图像来源：作者）
- en: The Initial Probability Distribution, along with the Transition Matrix and the
    Observation Likelihood, make up the **parameters of an HMM**. These are the probabilities
    you’re figuring out if you have a sequence of observations and hidden states,
    and attempt to *learn* which specific HMM could have generated them.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 初始概率分布，加上转移矩阵和观测似然矩阵，组成了**隐马尔可夫模型的参数**。这些是你在拥有观测序列和隐藏状态的情况下，需要弄清楚的概率，并尝试*学习*哪个特定的HMM可能生成了这些数据。
- en: Putting all of these pieces together, this is what the Hidden Markov model that
    represents your dog’s performance on the training exam looks like
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 将这些部分组合在一起，这就是表示你狗在训练考试中表现的隐马尔可夫模型的样子。
- en: '![](../Images/1a03ebaab0ee07dd8844ca64513e59b4.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1a03ebaab0ee07dd8844ca64513e59b4.png)'
- en: Hidden states and the transition probabilities between them. (Image by Autor)
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 隐藏状态以及它们之间的转移概率。（图像来源：作者）
- en: During the exam, your dog will perform three trials, and graduate only if they
    don’t *Fail* in two of those trials.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在考试期间，你的狗将进行三次试验，只有在这三次试验中不*失败*两次，才能毕业。
- en: At the end of the day, if your dog needs more training, you’ll care for them
    all the same. The big question circling your mind is *How are they feeling during
    the exam.*
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 到头来，如果你的狗需要更多训练，你仍然会照顾它。你脑海中的大问题是*它们在考试期间感觉如何*。
- en: Imagining a scenario where they graduate with a score of *OK — Fail — Perfect*
    exactly in this order, what sequence of emotional states will they be in? Will
    they be mostly tired, or hungry throughout, or maybe a mix of both?
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一个场景，如果它们以*OK — Fail — Perfect*的顺序毕业，它们将处于什么样的情感状态？它们会大多数时间感到疲惫，还是饥饿，或者两者的混合？
- en: This type of problem falls right under the category of *Decoding* problems that
    HMMs can be applied to. In this case, you’re interested figuring out what’s the
    best sequence of states that generated a specific sequence of observations, *OK
    — Fail — Perfect.*
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 这种问题正好属于HMM可以应用的*解码*问题。在这种情况下，你希望找出生成特定观察序列*OK — Fail — Perfect*的最佳状态序列。
- en: The problem of decoding the sequence of states that generated a given sequence
    of observations leverages the **Viterbi Algorithm**. However, is worth doing a
    short detour and take a peek into how you could calculate the probability of a
    given observation sequence — a Likelihood task — using the **Forward Algorithm**.
    This will set the stage to better understanding how the Viterbi Algorithm works.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 解码生成给定观察序列的状态序列的问题利用了**维特比算法**。然而，值得稍微绕个弯子，了解一下如何使用**前向算法**计算给定观察序列的概率——这是一项似然任务。这将为更好地理解维特比算法的工作原理奠定基础。
- en: The Forward Algorithm
  id: totrans-71
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 前向算法
- en: If you were modeling this problem like a regular [Markov Chain](https://medium.com/p/65e47b5c9a73),
    and wanted to calculate the likelihood of observing the sequence of outcomes *OK,
    Fail, Perfect* you’d traverse the chain by landing in each specific state that
    generates the desired outcome. At each step you would take the conditional probability
    of observing the current outcome given that you’ve observed the previous outcome
    and multiply that probability by the transition probability of going from one
    state to the other.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你将这个问题建模为常规的[马尔可夫链](https://medium.com/p/65e47b5c9a73)，并希望计算观察序列*OK, Fail,
    Perfect*的似然，你需要通过每个特定的状态生成期望的结果。在每一步，你会根据前一个观察结果的条件概率来观察当前结果，并将该概率乘以从一个状态到另一个状态的转移概率。
- en: The big difference is that, in a regular Markov Chain, all states are well known
    and observable. Not in an Hidden Markov Model! In an Hidden Markov Model you observe
    a sequence of outcomes, not knowing which specific sequence of hidden states had
    to be traversed in order to observe that.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 大的区别在于，在常规的马尔可夫链中，所有状态都是已知和可观察的。但在隐马尔可夫模型中却不是这样！在隐马尔可夫模型中，你观察到的是一个结果序列，而不知道为了观察到这个序列需要经过哪个特定的隐藏状态序列。
- en: The big difference is that, in a regular Markov Chain, all states are well known
    and observable. Not in an Hidden Markov Model!
  id: totrans-74
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 大的区别在于，在常规的马尔可夫链中，所有状态都是已知和可观察的。但在隐马尔可夫模型中却不是这样！
- en: At this point you might be thinking, *Well I can simply traverse all possible
    paths and eventually have a rule to pick between equivalent paths.* The mathematical
    definition for this approach looks something like this
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 此时你可能会想，*好吧，我可以简单地遍历所有可能的路径，然后最终有一个规则来选择等效路径。* 这种方法的数学定义如下所示。
- en: '![](../Images/33c668175694c5cdd6158a95bfffe1f3.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/33c668175694c5cdd6158a95bfffe1f3.png)'
- en: '*Calculating the probability of observing a sequence of outcomes, traversing
    every hidden state sequence possible. (Image by Author)*'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '*计算观察结果序列的概率，遍历所有可能的隐藏状态序列。（图片来自作者）*'
- en: That’s one strategy for sure! You’d have to calculate the probability of observing
    the sequence *OK, Fail, Perfect* for every single combination of hidden states
    that could ever generate that sequence.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 这肯定是一种策略！你需要计算观察到序列*OK, Fail, Perfect*的概率，考虑所有可能生成该序列的隐藏状态组合。
- en: When you have a small enough number of hidden states and sequence of observed
    outcomes, it's possible to do that calculation within a reasonable time.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 当隐藏状态和观察结果序列的数量足够小时，可以在合理的时间内完成计算。
- en: '![](../Images/97983f02f7cfe479a82bfea5e9e494d7.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/97983f02f7cfe479a82bfea5e9e494d7.png)'
- en: '*Outline of the possible paths in your HMM (Image by Author)*'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '*HMM中可能路径的概述（图片来自作者）*'
- en: Thankfully, the Hidden Markov model you just defined is relatively simple, with
    3 observed outcomes and 2 hidden states.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 值得庆幸的是，你刚定义的隐马尔可夫模型相对简单，包含3个观察结果和2个隐藏状态。
- en: For an observed sequence of length L outcomes, on a HMM with M hidden states,
    you have “M to the power L” possible states which in your case, means *2 to the
    power of 3*, i.e., 8 possible paths for the sequence *OK — Fail — Perfect,* involving
    an exponential computational complexity of O(M^L L), described in [Big O-Notation](https://en.wikipedia.org/wiki/Big_O_notation).
    As the complexity of the model increases, the number of paths you need to take
    into account grows exponentially.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 对于长度为L的观察序列，在具有M个隐藏状态的HMM中，你有“M的L次方”种可能的状态，在你的例子中，这意味着*2的3次方*，即8条可能的路径来生成序列*OK
    — Fail — Perfect*，涉及到的计算复杂度为O(M^L L)，在[大O符号](https://en.wikipedia.org/wiki/Big_O_notation)中描述。随着模型复杂度的增加，你需要考虑的路径数量呈指数级增长。
- en: As the complexity of the model increases, the number of paths you need to take
    into account grows exponentially.
  id: totrans-84
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 随着模型复杂度的增加，你需要考虑的路径数量呈指数级增长。
- en: This is where the **Forward Algorithm** shines.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是**前向算法**的亮点所在。
- en: The Forward Algorithm calculates the probability of a new symbol in the observed
    sequence, without the need to calculate the probabilities of all possible paths
    that form that sequence [3].
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 前向算法计算观察序列中新符号的概率，而无需计算形成该序列的所有可能路径的概率[3]。
- en: Instead of computing the probabilities of all possible paths that form that
    sequence the algorithm defines the **forward variable** and calculates its value
    [recursively](https://en.wikipedia.org/wiki/Recursion).
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 算法定义了**前向变量**，而不是计算所有形成该序列的可能路径的概率，前向变量的值是通过[递归](https://en.wikipedia.org/wiki/Recursion)计算的。
- en: '![](../Images/cd803b5f94fc52d8c67669787bf43c05.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cd803b5f94fc52d8c67669787bf43c05.png)'
- en: How the forward variable is calculated recursively. (Image by Author)
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 前向变量是如何递归计算的。（图片来源：作者）
- en: The fact that it uses recursion, is the key reason why this algorithm is faster
    than calculating all the probabilities of possible paths. In fact, it can calculate
    the probability of observing the sequence *x* in only “L times M squared” computations,
    instead of “M to the power of L times L”.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 它使用递归是该算法比计算所有可能路径的概率更快的关键原因。实际上，它可以在仅需“L乘以M平方”的计算中计算观察到序列*x*的概率，而不是“M的L次方乘以L”。
- en: In your case, with 2 hidden states and a sequence of 3 observed outcomes, it’s
    the difference between calculating the probabilities O(MˆL L) = 2³x3 *=* 8x3 *=*
    24 times, as opposed to O(L Mˆ2)*=*3*2²=3x4 = 12 times.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在你的例子中，使用2个隐藏状态和3个观察结果的序列，计算概率的次数为O(MˆL L) = 2³x3 *=* 8x3 *=* 24次，而不是O(L Mˆ2)*=*3*2²=3x4
    = 12次。
- en: This reduction in the number of calculations is achieved by [Dynamic Programming](https://en.wikipedia.org/wiki/Dynamic_programming),
    a programming technique that uses an auxiliary data structures to store intermediate
    information, therefore making sure the same calculations are not done multiple
    times.
  id: totrans-92
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 这种减少计算次数的方式是通过[动态规划](https://en.wikipedia.org/wiki/Dynamic_programming)实现的，这是一种编程技术，使用辅助数据结构存储中间信息，从而确保不会多次进行相同的计算。
- en: Every time the algorithm is about to calculate a new probability it checks if
    it has already computed it, and if so, it can easily access that value in the
    intermediate data structure. Otherwise, the probability is calculated and the
    value is stored.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 每次算法要计算新概率时，它会检查是否已经计算过，如果是，它可以轻松地在中间数据结构中访问该值。否则，计算概率并存储该值。
- en: Let’s get back to your decoding problem, using the Viterbi Algorithm.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 回到你的解码问题，使用维特比算法。
- en: The Viterbi Algorithm
  id: totrans-95
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 维特比算法
- en: 'Thinking in *pseudo code*, If you were to brute force your way into decoding
    the sequence of hidden states that generate a specific observation sequence, all
    you needed to do was:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 以*伪代码*的思维方式，如果你要用暴力破解的方法解码生成特定观察序列的隐藏状态序列，你只需：
- en: generate all possible permutations of paths that lead to the desired observation
    sequence
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生成所有可能的路径排列，以达到期望的观察序列
- en: use the Forward Algorithm to calculate the likelihood of each observation sequence,
    for each possible sequence of hidden states
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用前向算法计算每个观察序列的可能性，对于每个可能的隐藏状态序列
- en: pick the sequence of hidden states with highest probability
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择概率最高的隐藏状态序列
- en: '![](../Images/844da6164710e90f629645ea79e948a8.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/844da6164710e90f629645ea79e948a8.png)'
- en: All possible hidden state sequences that generate the observation sequence *OK
    — Fail — Perfect* (Image by Author)
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 所有可能的隐藏状态序列生成观察序列*OK — Fail — Perfect*（图片来源：作者）
- en: For your specific HMM, there are 8 possible paths that lead to an outcome of
    *OK — Fail — Perfect*. Add just one more observation, and you’ll have double the
    amount of possible sequences of hidden states! Similarly to what was described
    for the Forward Algorithm, you easily end up with an exponentially complex algorithm
    and hit performance ceiling.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 对于你的特定HMM，有8条可能的路径可以得到*OK — Fail — Perfect*的结果。再添加一个观察值，你将会有双倍数量的隐藏状态序列！与前向算法所描述的类似，你很容易会遇到指数级复杂度的算法，并达到性能瓶颈。
- en: The Viterbi Algorithm, gives you a hand with that.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: Viterbi算法可以帮助你解决这个问题。
- en: When the sequence of hidden states in the HMM is traversed, at each step, the
    probability *vt(j)* is the probability that the HMM is in the hidden state *j*
    after seeing the observation and is being traversed through the most probable
    state that lead to *j*.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 当HMM中的隐藏状态序列被遍历时，在每一步，概率*vt(j)*是HMM在观察后处于隐藏状态*j*的概率，并且是通过最可能的状态到达*j*的。
- en: '![](../Images/287242d38d5a258ccf9cba1292bb79aa.png)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/287242d38d5a258ccf9cba1292bb79aa.png)'
- en: Viterbi path to hidden state *j* on time step *t.* (Image by Author)
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: Viterbi路径到隐藏状态*j*在时间步*t*。（图像来源：作者）
- en: The key to decoding the sequence of hidden states that generate a specific observation
    sequence, is this concept of the **most probable path**. Also called the **Viterbi
    path,** the most probable path, is the path that has highest likelihood, from
    all the paths that can lead to any given hidden state.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 解码生成特定观察序列的隐藏状态序列的关键是**最可能路径**的概念。也称为**Viterbi路径**，最可能路径是从所有可能的路径中，具有最高概率的路径。
- en: The key to decoding the sequence of hidden states that generate a specific observation
    sequence, is to use the Viterbi path. The most probable path that leads to any
    given hidden state.
  id: totrans-108
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 解码生成特定观察序列的隐藏状态序列的关键是使用Viterbi路径。最可能的路径通向任何给定的隐藏状态。
- en: You can draw a parallel between the Forward Algorithm and the Viterbi Algorithm.
    Where the Forward Algorithm sums all probabilities to obtain the likelihood of
    reaching a certain state taking into account all the paths that lead there, the
    Viterbi algorithm doesn’t want to explore all possibilities. It focuses on the
    most probable path that leads to any given state.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以将前向算法和Viterbi算法进行对比。前向算法通过将所有概率相加来获得达到某个状态的可能性，考虑所有到达该状态的路径，而Viterbi算法则不想探索所有可能性。它专注于通向任何给定状态的最可能路径。
- en: Going back to the task of decoding the sequence of hidden states that lead to
    the scores of OK — Fail — Perfect in their exam, *running* the **Viterbi Algorithm**
    by hand would look like this
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 回到解码隐藏状态序列的任务，以获得考试中OK — Fail — Perfect的分数，*手动*运行**Viterbi算法**将会像这样：
- en: '![](../Images/786b06523dc656786c27591176108ea4.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/786b06523dc656786c27591176108ea4.png)'
- en: Viterbi paths and decoded sequence. (Image by Author)
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: Viterbi路径和解码序列。（图像来源：作者）
- en: Another unique characteristic of the Viterbi algorithm is that it must have
    a way to keep track of all the paths that led to any given hidden state, in order
    to compare their probabilities. To do that it keeps track of **backpointers**
    to each hidden state, using an auxiliary data structure typical of dynamic programming
    algorithms. That way it can easily access the probability of any viterbi path
    traversed in the past.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: Viterbi算法的另一个独特特点是它必须有一种方法来跟踪所有到达任何给定隐藏状态的路径，以便比较它们的概率。为此，它使用动态规划算法的典型辅助数据结构来跟踪**回溯指针**到每个隐藏状态。这样，它可以轻松访问过去遍历的任何Viterbi路径的概率。
- en: '**Backpointers are the key to figure out the most probable path that leads
    to an observation sequence.**'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '**回溯指针是确定通向观察序列的最可能路径的关键。**'
- en: In the example of your dogs’ exam, when you calculate the Viterbi paths *v3(Happy)*
    and *v3(Tired)*, you pick the path with highest probability and start going backwards,
    i.e., backtracking, through all the paths that led to where you are.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 在你狗的考试示例中，当你计算Viterbi路径*v3(Happy)*和*v3(Tired)*时，你选择概率最高的路径，然后开始向后遍历，即回溯，通过所有到达你所在位置的路径。
- en: The Viterbi Algorithm in Python
  id: totrans-116
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Python中的Viterbi算法
- en: Doing all of this by hand is time consuming and error prone. Miss one significant
    digit and you might have to start from scratch and re-check all your probabilities!
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 手动完成这些工作既费时又容易出错。错过一个重要的数字，你可能需要从头开始并重新检查所有的概率！
- en: The good news is that you can leverage software libraries like [hmmlearn](https://hmmlearn.readthedocs.io/en/stable/tutorial.html),
    and with a few lines of code you can decode the sequence of hidden states that
    lead to your dog graduating with *OK — Fail — Perfect* in the trials, exactly
    in this order.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 好消息是，你可以利用像 [hmmlearn](https://hmmlearn.readthedocs.io/en/stable/tutorial.html)
    这样的软件库，只需几行代码就可以解码隐藏状态的序列，使你的狗在试验中获得*OK — Fail — Perfect*，并且按照这个顺序。
- en: '[PRE0]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: In a few seconds you get an output that matches results the calculations you
    did by hand, much fast and with much less room for error.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 几秒钟内你就能得到一个与手工计算结果匹配的输出，速度更快，且错误空间更小。
- en: '![](../Images/41b354a93fb104c925d85c22edcd5966.png)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/41b354a93fb104c925d85c22edcd5966.png)'
- en: Output of running the code above. (Image by Author)
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 运行上述代码的输出结果。（图片由作者提供）
- en: Conclusion
  id: totrans-123
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: What’s fascinating about Hidden Markov Models is how this statistical tool created
    in the mid 1960’s [6] is so powerful and applicable to real world problems in
    such distinct areas, from weather forecasting to finding the next word in a sentence.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 隐马尔可夫模型令人着迷之处在于，这一统计工具诞生于20世纪60年代中期[6]，却如此强大，并且在从天气预报到找出句子中的下一个词等不同领域中应用广泛。
- en: In this article, you had the chance to learn about the different components
    of an HMM, how they can be applied to different types of tasks, and spotting the
    similarities between the Forward Algorithm and Viterbi Algorithm. Two very similar
    algorithms that use dynamic programming to deal with the exponential number of
    calculations involved.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，你有机会了解HMM的不同组件，它们如何应用于不同类型的任务，以及如何发现前向算法和维特比算法之间的相似性。这两个非常相似的算法使用动态规划来处理涉及的指数级计算。
- en: Either doing the calculations by hand or plugging in the parameters into TensorFlow
    code, hope you enjoyed diving deep into the world of HMMs.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 无论是手工计算还是将参数输入到TensorFlow代码中，希望你享受深入探索HMM世界的过程。
- en: '*Thank you for reading!*'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '*感谢阅读！*'
- en: References
  id: totrans-128
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'D. Khiatani and U. Ghose, “Weather forecasting using Hidden Markov Model,”
    2017 International Conference on Computing and Communication Technologies for
    Smart Nation (IC3TSN), Gurgaon, India, 2017, pp. 220–225, doi: 10.1109/IC3TSN.2017.8284480.'
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'D. Khiatani 和 U. Ghose，“使用隐马尔可夫模型进行天气预报”，2017年国际智能国家计算与通信技术会议（IC3TSN），印度古尔冈，2017年，第220–225页，doi:
    10.1109/IC3TSN.2017.8284480。'
- en: 'Noguchi H, Kato R, Hanai T, Matsubara Y, Honda H, Brusic V, Kobayashi T. Hidden
    Markov model-based prediction of antigenic peptides that interact with MHC class
    II molecules. J Biosci Bioeng. 2002;94(3):264–70\. doi: 10.1263/jbb.94.264\. PMID:
    16233301.'
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Noguchi H, Kato R, Hanai T, Matsubara Y, Honda H, Brusic V, Kobayashi T. 基于隐马尔可夫模型的抗原肽预测，这些抗原肽与MHC
    II类分子相互作用。《生物科学与生物工程杂志》。2002；94(3)：264–70。doi: 10.1263/jbb.94.264。PMID: 16233301。'
- en: 'Yoon BJ. [Hidden Markov Models and their Applications in Biological Sequence
    Analysis](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2766791/). Curr Genomics.
    2009 Sep;10(6):402–15\. doi: 10.2174/138920209789177575\. PMID: 20190955; PMCID:
    PMC2766791.'
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Yoon BJ. [隐马尔可夫模型及其在生物序列分析中的应用](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2766791/)。《当前基因组学》。2009年9月；10(6)：402–15。doi:
    10.2174/138920209789177575。PMID: 20190955；PMCID: PMC2766791。'
- en: Eddy, S. [What is a hidden Markov model?](https://www.nature.com/articles/nbt1004-1315).
    *Nat Biotechnol* **22**, 1315–1316 (2004). [https://doi.org/10.1038/nbt1004-1315](https://doi.org/10.1038/nbt1004-1315)
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Eddy, S. [什么是隐马尔可夫模型？](https://www.nature.com/articles/nbt1004-1315)。*自然生物技术*
    **22**，1315–1316（2004）。[https://doi.org/10.1038/nbt1004-1315](https://doi.org/10.1038/nbt1004-1315)
- en: 'Jurafsky, Dan and Martin, James H.. *Speech and language processing : an introduction
    to natural language processing, computational linguistics, and speech recognition*.
    Upper Saddle River, N.J.: Pearson Prentice Hall, 2009.'
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Jurafsky, Dan 和 Martin, James H.，《*语音与语言处理：自然语言处理、计算语言学和语音识别导论*》。上萨德尔河，新泽西州：Pearson
    Prentice Hall，2009。
- en: 'Baum, Leonard E., and Ted Petrie. “[Statistical Inference for Probabilistic
    Functions of Finite State Markov Chains.](https://projecteuclid.org/journals/annals-of-mathematical-statistics/volume-37/issue-6/Statistical-Inference-for-Probabilistic-Functions-of-Finite-State-Markov-Chains/10.1214/aoms/1177699147.full)”
    *The Annals of Mathematical Statistics* 37, no. 6 (1966): 1554–63.'
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Baum, Leonard E., 和 Ted Petrie。“[有限状态马尔可夫链的概率函数的统计推断。](https://projecteuclid.org/journals/annals-of-mathematical-statistics/volume-37/issue-6/Statistical-Inference-for-Probabilistic-Functions-of-Finite-State-Markov-Chains/10.1214/aoms/1177699147.full)”
    *《数学统计年鉴》* 37, no. 6 (1966): 1554–63。'
