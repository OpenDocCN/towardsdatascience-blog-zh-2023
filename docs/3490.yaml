- en: Two-Tower Networks and Negative Sampling in Recommender Systems
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 推荐系统中的双塔网络和负采样
- en: 原文：[https://towardsdatascience.com/two-tower-networks-and-negative-sampling-in-recommender-systems-fdc88411601b?source=collection_archive---------0-----------------------#2023-11-24](https://towardsdatascience.com/two-tower-networks-and-negative-sampling-in-recommender-systems-fdc88411601b?source=collection_archive---------0-----------------------#2023-11-24)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/two-tower-networks-and-negative-sampling-in-recommender-systems-fdc88411601b?source=collection_archive---------0-----------------------#2023-11-24](https://towardsdatascience.com/two-tower-networks-and-negative-sampling-in-recommender-systems-fdc88411601b?source=collection_archive---------0-----------------------#2023-11-24)
- en: Understand the key elements that power advanced recommendation engines
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 了解推动高级推荐引擎的关键元素
- en: '[](https://roizner.medium.com/?source=post_page-----fdc88411601b--------------------------------)[![Michael
    Roizner](../Images/bcb68ee626ea57234b62e512ed4b383b.png)](https://roizner.medium.com/?source=post_page-----fdc88411601b--------------------------------)[](https://towardsdatascience.com/?source=post_page-----fdc88411601b--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----fdc88411601b--------------------------------)
    [Michael Roizner](https://roizner.medium.com/?source=post_page-----fdc88411601b--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://roizner.medium.com/?source=post_page-----fdc88411601b--------------------------------)[![Michael
    Roizner](../Images/bcb68ee626ea57234b62e512ed4b383b.png)](https://roizner.medium.com/?source=post_page-----fdc88411601b--------------------------------)[](https://towardsdatascience.com/?source=post_page-----fdc88411601b--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----fdc88411601b--------------------------------)
    [Michael Roizner](https://roizner.medium.com/?source=post_page-----fdc88411601b--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F1bee5af37d8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftwo-tower-networks-and-negative-sampling-in-recommender-systems-fdc88411601b&user=Michael+Roizner&userId=1bee5af37d8&source=post_page-1bee5af37d8----fdc88411601b---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----fdc88411601b--------------------------------)
    ·7 min read·Nov 24, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Ffdc88411601b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftwo-tower-networks-and-negative-sampling-in-recommender-systems-fdc88411601b&user=Michael+Roizner&userId=1bee5af37d8&source=-----fdc88411601b---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F1bee5af37d8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftwo-tower-networks-and-negative-sampling-in-recommender-systems-fdc88411601b&user=Michael+Roizner&userId=1bee5af37d8&source=post_page-1bee5af37d8----fdc88411601b---------------------post_header-----------)
    发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----fdc88411601b--------------------------------)
    ·7分钟阅读·2023年11月24日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Ffdc88411601b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftwo-tower-networks-and-negative-sampling-in-recommender-systems-fdc88411601b&user=Michael+Roizner&userId=1bee5af37d8&source=-----fdc88411601b---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ffdc88411601b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftwo-tower-networks-and-negative-sampling-in-recommender-systems-fdc88411601b&source=-----fdc88411601b---------------------bookmark_footer-----------)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ffdc88411601b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftwo-tower-networks-and-negative-sampling-in-recommender-systems-fdc88411601b&source=-----fdc88411601b---------------------bookmark_footer-----------)'
- en: 'One of the most important types of models in recommendation systems at present
    is the two-tower neural networks. They are structured as follows: one part of
    the neural network (tower) processes all the information about the query (user,
    context), while the other tower processes information about the object. The outputs
    of these towers are embeddings, which are then multiplied (dot product or cosine,
    as we already discussed [here](https://roizner.medium.com/dot-product-or-cosine-55cd5b22a87c)).
    One of the first mentions of the application of such networks for recommendations
    can be found in a very good [paper](https://dl.acm.org/doi/10.1145/2959100.2959190)
    about YouTube. By the way, I would now call this article a classic and most suitable
    for entering the field of recommendations.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 目前推荐系统中最重要的模型之一是双塔神经网络。它们的结构如下：神经网络的一个部分（塔）处理关于查询（用户、上下文）的所有信息，而另一个塔处理关于对象的信息。这些塔的输出是嵌入，然后将这些嵌入相乘（点积或余弦，如我们已经在[这里](https://roizner.medium.com/dot-product-or-cosine-55cd5b22a87c)讨论过）。双塔网络应用于推荐的最早提及之一可以在关于
    YouTube 的一篇非常好的[论文](https://dl.acm.org/doi/10.1145/2959100.2959190)中找到。顺便提一下，我现在会将这篇文章称为经典且最适合进入推荐领域的文章。
- en: '![](../Images/4ba8e24dde42029e01b58a473e90365f.png)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4ba8e24dde42029e01b58a473e90365f.png)'
- en: From the paper [Deep Neural Networks for YouTube Recommendations](https://dl.acm.org/doi/10.1145/2959100.2959190)
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 来自论文 [YouTube 推荐的深度神经网络](https://dl.acm.org/doi/10.1145/2959100.2959190)
- en: What characterizes such networks? They are very similar to matrix factorization,
    which is actually a special case, taking only *user_id* and *item_id* as input.
    However, if we compare them with arbitrary networks, the restriction on late crossing
    (not allowing inputs from different towers to fuse until the very end) makes two-tower
    networks extremely efficient in application. To build recommendations for a single
    user, we need to calculate the query tower once, and then multiply this embedding
    with the embeddings of documents, which are usually pre-calculated. This process
    is very fast. Moreover, these pre-calculated document embeddings can be organized
    into an [ANN](https://en.wikipedia.org/wiki/Nearest_neighbor_search) index (e.g.,
    [HNSW](https://github.com/nmslib/hnswlib)) to quickly find good candidates without
    having to go through the entire database.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 这种网络的特点是什么？它们与矩阵分解非常相似，实际上矩阵分解是一种特殊情况，仅以 *user_id* 和 *item_id* 作为输入。然而，如果我们将它们与任意网络进行比较，限制晚期交叉（不允许来自不同塔的输入在最后阶段之前融合）使得双塔网络在应用中极为高效。为了为单个用户构建推荐，我们只需要计算一次查询塔，然后将该嵌入与通常预先计算的文档嵌入相乘。这个过程非常快速。此外，这些预先计算的文档嵌入可以组织成一个
    [ANN](https://en.wikipedia.org/wiki/Nearest_neighbor_search) 索引（例如，[HNSW](https://github.com/nmslib/hnswlib)），以便快速找到好的候选项，而无需遍历整个数据库。
- en: '[](/similarity-search-part-4-hierarchical-navigable-small-world-hnsw-2aad4fe87d37?source=post_page-----fdc88411601b--------------------------------)
    [## Similarity Search, Part 4: Hierarchical Navigable Small World (HNSW)'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/similarity-search-part-4-hierarchical-navigable-small-world-hnsw-2aad4fe87d37?source=post_page-----fdc88411601b--------------------------------)
    [## 相似性搜索，第 4 部分：分层可导航小世界（HNSW）'
- en: Hierarchical Navigable Small World (HNSW) is a state-of-the-art algorithm used
    for an approximate search of nearest…
  id: totrans-13
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 分层可导航小世界（HNSW）是一种用于近似最近邻搜索的最先进算法...
- en: towardsdatascience.com](/similarity-search-part-4-hierarchical-navigable-small-world-hnsw-2aad4fe87d37?source=post_page-----fdc88411601b--------------------------------)
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/similarity-search-part-4-hierarchical-navigable-small-world-hnsw-2aad4fe87d37?source=post_page-----fdc88411601b--------------------------------)
- en: We can achieve even more efficiency by computing the user part not for every
    query, but asynchronously, with some regularity. However, this means sacrificing
    the consideration of real-time history and context.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过以某种规律异步计算用户部分而不是对每个查询进行计算来实现更高的效率。然而，这意味着需要牺牲对实时历史和上下文的考虑。
- en: The towers themselves can be quite sophisticated. For example, in the user part,
    we can use the self-attention mechanism to process history, which results in a
    transformer for personalization. But what is the cost of introducing the restriction
    on late crossing? Naturally, it affects quality. In the same attention mechanism,
    we cannot use the items that we currently wish to recommend. Ideally, we would
    like to focus on similar items in the user’s history. Therefore, networks with
    early crossing are typically used in the later stages of ranking, when there are
    only a few dozen or hundred candidates left, while those with late crossing (two-tower)
    are used, conversely, in the early stages and for candidate generation.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: '*(However, there is a purely theoretical argument that any reasonable ranking
    of documents for different queries can be encoded through embeddings of sufficient
    dimensionality. Moreover, decoders in NLP actually operate on the same principle,
    only recalculating the query tower for each token.)*'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: Loss Functions and Negative Sampling
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A particular point of interest is the loss function used to train two-tower
    networks. In principle, they can be trained with any loss function, targeting
    various outcomes and even having multiple different ones for different heads (with
    different embeddings in each tower). However, an interesting variant is training
    with a softmax loss on in-batch negatives. For each query-document pair in the
    dataset, the other documents in the same mini-batch are used as negatives in combination
    with the same query in the softmax loss. This method is a highly effective form
    of hard negative mining.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: But it’s important to consider the probabilistic interpretation of such a loss
    function, which is not always well-understood. In a trained network,
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/714c36ba7b5f07a6f592cab38b6e9156.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
- en: The exponent of the score is proportional not to the a priori probability of
    the document given the query, but to the PMI ([Pointwise Mutual Information](https://en.wikipedia.org/wiki/Pointwise_mutual_information)),
    specific to the query. More popular documents will not necessarily be recommended
    more often by such a model, because during training, they proportionally appear
    more often as negatives. Using the score as a feature can be beneficial, but for
    final ranking and candidate generation, this can lead to very specific, yet poor-quality
    documents.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: Google, in a [paper](https://research.google/pubs/pub48840/), suggested combating
    this issue with logQ correction during training. We, on the other hand, typically
    addressed this at the application stage, not during training, by simply multiplying
    by the a priori probability of the document P(*d*). However, we never compared
    these approaches, which would indeed be an interesting comparison.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: 'Implicit Regularization: Bridging ALS and Modern Neural Networks'
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There is a collaborative filtering algorithm known as Implicit ALS (IALS).
    I have already [mentioned it before](https://roizner.medium.com/leveraging-external-embeddings-in-recommender-systems-a-practical-guide-e3768cc574e1).
    In the pre-neural network era, it was arguably one of the most popular algorithms.
    Its distinguishing feature is the effective ‘mining’ of negatives: all user-object
    pairs without a history of interaction are treated as negatives (though with less
    weight than actual interactions). Moreover, unlike in actual mining, these negatives
    are not sampled but are used in their entirety in every iteration. This approach
    is known as implicit regularization.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: How is this possible? Given reasonable task sizes (number of users and objects),
    there should be so many negatives that even listing them would take longer than
    the entire training process. The beauty of the algorithm lies in the fact that,
    by using the MSE loss and the method of least squares, it’s possible to pre-calculate
    certain elements separately for all users and separately for all objects before
    each full iteration, and this is sufficient for performing implicit regularization.
    This way, the algorithm avoids a quadratic size. (For more details, I refer you
    to [one of my favorite papers from that time](https://dl.acm.org/doi/10.1145/1864708.1864726)).
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: A couple of years ago, I pondered whether it’s possible to combine this wonderful
    idea of implicit regularization with the more advanced technology of two-tower
    neural networks. It’s a complex question, as there’s stochastic optimization instead
    of full batch, and there’s a reluctance to revert back to MSE loss (at least for
    the entire task; specifically for regularization, it might be okay) as it tends
    to yield inferior results.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: I thought long and hard, and finally came up with a solution! For a couple of
    weeks, I was exhilarated, eagerly anticipating how we would try this out in place
    of in-batch negatives.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: And then, of course (as often happens in such cases), I read in a paper that
    everything had already been thought of three years earlier. Again, it was [Google](https://arxiv.org/abs/1807.07187).
    Later, in that same [paper about logQ correction](https://research.google/pubs/pub48840/),
    they demonstrated that softmax loss with in-batch negatives works better than
    implicit regularization.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: That’s how we were able to save time and not test this idea 🙂
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: Do We Really Need Negative Sampling for Recommendation Models?
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: After all, we have real instances of recommendation impressions, and if a user
    did not interact with them, these can be used as strong negatives. (This does
    not consider the case where the recommendation service itself has not yet been
    launched and there are no impressions yet.)
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: 'The answer to this question is not so trivial; it depends on how exactly we
    intend to apply the trained model: for final ranking, for candidate generation,
    or simply as features for input into another model.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: 'What happens when we train the model only on actual impressions? A rather strong
    selection bias occurs, and the model learns to distinguish well only those documents
    that were shown in that particular context. On documents (or more precisely, query-document
    pairs) that were not shown, the model will perform much worse: it might overpredict
    for some documents and underpredict for others. Certainly, this effect can be
    mitigated by applying exploration in ranking, but most often this is only a partial
    solution.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们仅在实际展示上训练模型时，会发生什么？会出现相当强的选择偏差，模型只学会在特定上下文中区分那些文档。对于未展示的文档（或者更准确地说，查询-文档对），模型的表现会差很多：它可能会对一些文档进行过度预测，对其他文档进行低估。当然，这种效果可以通过在排名中应用探索来缓解，但通常这只是部分解决方案。
- en: If a candidate generator is trained in this way, it may produce a plethora of
    documents in response to a query, documents it has never seen in such a context
    and for which it overestimates predictions. Among these documents, there’s often
    complete trash. If the final ranking model is good enough, it will filter out
    these documents and not show them to the user. However, we still waste the candidate
    quota on them unnecessarily (and there may not be any suitable documents left
    at all). Therefore, candidate generators should be trained in a way that they
    understand most of the document base is of poor quality and should not be recommended
    (nominated as candidates). Negative sampling is a good method for this.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 如果候选生成器以这种方式训练，它可能会针对一个查询生成大量文档，这些文档在这样的上下文中它从未见过，并且其预测被高估。在这些文档中，常常会有完全无用的内容。如果最终排序模型足够好，它会过滤掉这些文档，并不会展示给用户。然而，我们仍然不必要地浪费候选配额在这些文档上（而且可能根本没有合适的文档）。因此，候选生成器应以一种理解大部分文档库质量较差并且不应被推荐（提名为候选）的方式进行训练。负采样是一个好的方法。
- en: 'Models for final ranking are very similar to candidate generation in this regard,
    but there is an important distinction: they learn from their mistakes. When the
    model makes an error by overestimating predictions for certain documents, these
    documents are shown to users and then may be included in the next training dataset.
    We can retrain the model on this new dataset and roll it out to users again. New
    false positives will emerge. The dataset collection and retraining process can
    be repeated, resulting in a kind of active learning. In practice, just a few iterations
    of retraining are sufficient for the process to converge and for the model to
    stop recommending nonsense. Of course, the harm from random recommendations must
    be weighed, and sometimes it’s worth taking extra precautions. But overall, negative
    sampling is not needed here. On the contrary, it can impair exploration, causing
    the system to remain in a local optimum.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在这方面，最终排序模型与候选生成非常相似，但有一个重要的区别：它们从错误中学习。当模型通过对某些文档的预测过高而出错时，这些文档会展示给用户，并可能被纳入下一个训练数据集。我们可以在这个新数据集上重新训练模型，并再次推出给用户。新的假阳性会出现。数据集收集和重新训练过程可以重复，从而形成一种主动学习。实际上，只需几次重新训练迭代即可使过程收敛，并使模型停止推荐无用内容。当然，必须权衡随机推荐的危害，有时值得采取额外的预防措施。但总体而言，这里不需要负采样。相反，它可能会损害探索，使系统停留在局部最优。
- en: If the model is used for features as input into another model, then the same
    logic applies, but the harm from overestimating predictions for random candidate
    documents is even less significant because other features can help adjust the
    final prediction. (If a document doesn’t even make it to the candidate list, we
    won’t compute features for it.)
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 如果模型用于将特征作为输入传递给另一个模型，那么相同的逻辑适用，但对随机候选文档的预测过高的伤害更不显著，因为其他特征可以帮助调整最终预测。（如果文档甚至没有进入候选列表，我们不会为其计算特征。）
- en: At one point, we directly tested and found that as features, standard ALS works
    better than IALS, but it should not be used for candidate generation.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 曾经我们直接测试发现，作为特征的标准ALS比IALS表现更好，但不应用于候选生成。
- en: In summary, our exploration emphasized the effectiveness of two-tower networks
    in ranking, examined the significance of loss functions and negative sampling
    in model accuracy, bridged the gap with classical collaborative filtering through
    implicit regularization, and debated the essential role of negative sampling in
    recommendation systems. This discussion underscores the evolving complexity and
    sophistication of recommendation system technologies.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 总结而言，我们的探索强调了双塔网络在排序中的有效性，研究了损失函数和负采样在模型准确性中的重要性，通过隐式正则化弥合了与经典协同过滤的差距，并讨论了负采样在推荐系统中的核心作用。此次讨论突显了推荐系统技术的不断演变的复杂性和精密性。
