- en: 'Random Forests in 2023: Modern Extensions of a Powerful Method'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/random-forests-in-2023-modern-extensions-of-a-powerful-method-b62debaf1d62?source=collection_archive---------9-----------------------#2023-11-07](https://towardsdatascience.com/random-forests-in-2023-modern-extensions-of-a-powerful-method-b62debaf1d62?source=collection_archive---------9-----------------------#2023-11-07)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Random Forests came a long way
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@jeffrey_85949?source=post_page-----b62debaf1d62--------------------------------)[![Jeffrey
    Näf](../Images/0ce6db85501192cdebeeb910eb81a688.png)](https://medium.com/@jeffrey_85949?source=post_page-----b62debaf1d62--------------------------------)[](https://towardsdatascience.com/?source=post_page-----b62debaf1d62--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----b62debaf1d62--------------------------------)
    [Jeffrey Näf](https://medium.com/@jeffrey_85949?source=post_page-----b62debaf1d62--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fca780798011a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frandom-forests-in-2023-modern-extensions-of-a-powerful-method-b62debaf1d62&user=Jeffrey+N%C3%A4f&userId=ca780798011a&source=post_page-ca780798011a----b62debaf1d62---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----b62debaf1d62--------------------------------)
    ·12 min read·Nov 7, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fb62debaf1d62&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frandom-forests-in-2023-modern-extensions-of-a-powerful-method-b62debaf1d62&user=Jeffrey+N%C3%A4f&userId=ca780798011a&source=-----b62debaf1d62---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb62debaf1d62&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frandom-forests-in-2023-modern-extensions-of-a-powerful-method-b62debaf1d62&source=-----b62debaf1d62---------------------bookmark_footer-----------)![](../Images/11d2719adb78db10fac42a7884ac2918.png)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Features of modern Random Forest methods. Source: Author.'
  prefs: []
  type: TYPE_NORMAL
- en: In terms of Machine Learning timelines, Random Forests (RFs), introduced in
    the seminal paper of Breimann ([1]), are ancient. Despite their age, they keep
    impressing with their performance and are a topic of active research. The goal
    of this article is to highlight what a versatile toolbox Random Forest methods
    have become, focussing on **Generalized Random Forest (GRF)** and **Distributional
    Random Forest (DRF)**.
  prefs: []
  type: TYPE_NORMAL
- en: In short, the main idea underlying both methods is that the weights implicitly
    produced by RF can be used to estimate targets other than the conditional expectation.
    The idea of GRF is to use a Random Forest with a splitting criterion that is adapted
    to the target one has in mind (e.g., conditional mean, conditional quantiles,
    or the conditional treatment effect). The idea of DRF is to adapt the splitting
    criterion such that the whole conditional distribution can be estimated. From
    this object, many different targets can then be derived in a second step. In fact,
    I mostly talk about DRF in this article, as I am more familiar with this method
    and it is somewhat more streamlined (only one forest has to be fitted for a wide
    range of targets). However, all the advantages, indicated in the figure above,
    also apply to GRF and in fact, the DRF package in R is built upon the [professional
    implementation of GRF](https://grf-labs.github.io/grf/). Moreover, the fact that
    the splitting criterion of GRF forests is adapted to the target means it can have
    better performance than DRF. This is particularly true for binary *Y*, where probability_forests()
    should be used. So, even though I talk mostly about DRF, GRF should be kept in
    mind throughout this article.
  prefs: []
  type: TYPE_NORMAL
- en: 'The goal of this article is to provide an overview with links to deeper reading
    in the corresponding sections. We will go through each of the points in the above
    figure clock-wise, reference the corresponding articles, and highlight it with
    a little example. I first quickly summarize the most important links to further
    reading below:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Versatility/Performance: [Medium Article](/drf-a-random-forest-for-almost-everything-625fa5c3bcb8)
    and Original Papers ([DRF](https://jmlr.org/papers/v23/21-0585.html)/[GRF](https://projecteuclid.org/journals/annals-of-statistics/volume-47/issue-2/Generalized-random-forests/10.1214/18-AOS1709.full))'
  prefs: []
  type: TYPE_NORMAL
- en: 'Missing Values Incorporated: [Medium Article](/random-forests-and-missing-values-3daaea103db0)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Uncertainty Measures: [Medium Article](/inference-for-distributional-random-forests-64610bbb3927)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Variable Importance: [Medium Article](https://medium.com/towards-data-science/variable-importance-in-random-forests-20c6690e44e0)'
  prefs: []
  type: TYPE_NORMAL
- en: The Example
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We take *X_1, X_2, X_4, …, X_10* independently uniform between (-1,1) and create
    dependence between *X_1* and *X_3* by taking *X_3=X_1 + uniform error*. Then we
    simulate *Y* as
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/db8b95d3d53203e739c5509eb1aa992e.png)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Notice that with the function ampute from the [mice package](https://cran.r-project.org/web/packages/mice/index.html),
    we put **Missing not at Random (MAR)** missing values on ***X*** to highlight
    the ability of GRF/DRF to deal with missing values. Moreover, in the above process
    only *X_1* and *X_2* are relevant for predicting *Y*, all other variables are
    “noise” variables. Such a “sparse” setting might actually be common in real-life
    datasets.
  prefs: []
  type: TYPE_NORMAL
- en: 'We now choose a test point for this example that we will use throughout:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Versatility
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'DRF estimates the conditional distribution *P_{Y|****X****=****x****}*in the
    form of simple weights:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/206d5127ad181f1e1458ee40d1784285.png)'
  prefs: []
  type: TYPE_IMG
- en: From these weights, a wide range of targets can be calculated, or they can be
    used to simulate from the conditional distribution. A good reference for its versatility
    is the original [research article](https://jmlr.org/papers/v23/21-0585.html),
    where a lot of examples were used, as well as the corresponding [medium article](/drf-a-random-forest-for-almost-everything-625fa5c3bcb8).
  prefs: []
  type: TYPE_NORMAL
- en: 'In the example, we first simulate from this distribution:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/707d8f363302b0a531f4172989c20161.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Histogram of the simulated conditional distribution overlaid with the true
    density (in red). Source: Author.'
  prefs: []
  type: TYPE_NORMAL
- en: The plot shows the approximate draws from the conditional distribution overlaid
    with the true density in red. We now use this to estimate the **conditional expectation**
    and the **conditional (0.05, 0.95) quantiles** at ***x:***
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/b011f4a2998015c8a6f07661ea241c2c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Histogram of the simulated conditional distribution overlaid with the true
    density (in red). Additionally, the estimated conditional expectation and the
    conditional (0.05, 0.95) quantiles are in blue, with true values in red. Source:
    Author.'
  prefs: []
  type: TYPE_NORMAL
- en: Likewise, many targets can be calculated with GRF, only in this case for each
    of those two targets a different forest would need to be fit. In particular, *regression_forest()*
    for the conditional expectation and *quantile_forest()* for the quantiles.
  prefs: []
  type: TYPE_NORMAL
- en: What GRF cannot do is deal with multivariate targets, which is possible with
    DRF as well.
  prefs: []
  type: TYPE_NORMAL
- en: Performance
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Despite all the work on powerful new (nonparametric) methods, such as neural
    nets, tree-based methods are consistently able to beat competitors on tabular
    data. See e.g., this [fascinating paper](https://arxiv.org/abs/2207.08815), or
    this older paper on the [strength of RF in classification](https://dl.acm.org/doi/pdf/10.5555/2627435.2697065).
  prefs: []
  type: TYPE_NORMAL
- en: To be fair, with parameter tuning, [boosted tree methods](/an-overview-of-boosting-methods-catboost-xgboost-adaboost-lightboost-histogram-based-gradient-407447633ac1),
    such as XGboost, often take the lead, at least when it comes to classical prediction
    (which corresponds to conditional expectation estimation). Nonetheless, the robust
    performance RF methods tend to have without any tuning is remarkable. Moreover,
    there has also been work on improving the performance of Random Forests, for example,
    the [hedged Random Forest approach](https://arxiv.org/pdf/2308.15384.pdf).
  prefs: []
  type: TYPE_NORMAL
- en: Missing Values Incorporated
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '“Missing incorporated in attributes criterion” (MIA) from [this paper](https://www.sciencedirect.com/science/article/abs/pii/S0167865508000305)
    is a very simple but very powerful idea that allows tree-based methods to handle
    missing data. This was implemented in the GRF R package and so it is also available
    in DRF. The details are also explained in this [medium article](/random-forests-and-missing-values-3daaea103db0).
    As simple as the concept is, it works remarkably well in practice: In the above
    example, DRF had no trouble handling substantial MAR missingness in the training
    data ***X*** (!)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Uncertainty Measures**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As a statistician I don’t just want point estimates (even of a distribution),
    but also a measure of **estimation uncertainty** of my parameters (even if the
    “parameter” is my whole distribution). Turns out that a simple additional subsampling
    baked into DRF/GRF allows for a principled uncertainty quantification for large
    sample sizes. The theory behind this in the case of DRF is derived in this [research
    article](https://arxiv.org/abs/2302.05761), but I also explain it in this [medium
    article](/inference-for-distributional-random-forests-64610bbb3927). GRF has all
    the theory in the [original paper](https://projecteuclid.org/journals/annals-of-statistics/volume-47/issue-2/Generalized-random-forests/10.1214/18-AOS1709.full).
  prefs: []
  type: TYPE_NORMAL
- en: 'We adapt this for the above example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/bb22e3661f626b01f57210c507fef1c2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Histogram of the simulated conditional distribution overlaid with the true
    density (in red). Additionally, the estimated conditional expectation and the
    conditional (0.05, 0.95) quantiles are in blue, with true values in red. Moreover,
    the dashed red lines are the confidence intervals for the estimates as calculated
    by DRF. Source: Author.'
  prefs: []
  type: TYPE_NORMAL
- en: As can be seen from the above code, we essentially have *B* subtrees that can
    be used to calculate the measure each time. From these *B* samples of mean and
    quantiles, we can then calculate variances and use a normal approximation to obtain
    (asymptotic) confidence intervals seen in the dashed line in the Figure. *Again,
    all of this can be done despite the missing values in* ***X***(!)
  prefs: []
  type: TYPE_NORMAL
- en: '**Variable Importance**'
  prefs: []
  type: TYPE_NORMAL
- en: 'A final important aspect of Random Forests is efficiently calculated variable
    importance measures. While traditional measures are somewhat ad hoc, for the traditional
    RF and for DRF, there are now principled measures available, as explained in this
    [medium article](https://medium.com/towards-data-science/variable-importance-in-random-forests-20c6690e44e0).
    For RF, the Sobol-MDA method reliably identifies the variables most important
    for conditional expectation estimation, whereas for DRF, the MMD-MDA identifies
    the variables most important for the estimation of the distribution overall. As
    discussed in the article, using the idea of *projected Random Forests*, these
    measures can be very efficiently implemented. We demonstrate this in the example
    with a less efficient implementation of the MMD variable importance measure:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Here both *X_1* and *X_2* are correctly identified as being the most relevant
    variable when trying to estimate the distribution. Remarkably, despite the dependence
    of *X_3* and *X_1*, the measure correctly quantifies that *X_3* is not important
    for the prediction of the distribution of *Y*. This is something that the original
    MDA measure of Random Forests tends to do wrong, as demonstrated in the medium
    article. Moreover, notice again that the missing values in ***X*** are no problem
    here.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: GRF/DRF and also the traditional Random Forest should not be missing in the
    toolbox of any data scientist. While methods like XGboost can have a better performance
    in traditional prediction, the many strengths of modern RF-based approaches render
    them an incredibly versatile tool.
  prefs: []
  type: TYPE_NORMAL
- en: Of course, one should keep in mind that these methods are still fully nonparametric,
    and a lot of data points are needed for the fit to make sense. This is in particularly
    true for the uncertainty quantification, which is only valid asymptotically, i.e.
    for “large” samples.
  prefs: []
  type: TYPE_NORMAL
- en: Literature
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] Breiman, L. (2001). Random forests. Machine learning, 45(1):5–32.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Appendix: Code'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
