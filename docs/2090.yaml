- en: 'TaatikNet: Sequence-to-Sequence Learning for Hebrew Transliteration'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 'TaatikNet: 序列到序列学习用于希伯来文音译'
- en: 原文：[https://towardsdatascience.com/taatiknet-sequence-to-sequence-learning-for-hebrew-transliteration-4c9175a90c23?source=collection_archive---------5-----------------------#2023-06-28](https://towardsdatascience.com/taatiknet-sequence-to-sequence-learning-for-hebrew-transliteration-4c9175a90c23?source=collection_archive---------5-----------------------#2023-06-28)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/taatiknet-sequence-to-sequence-learning-for-hebrew-transliteration-4c9175a90c23?source=collection_archive---------5-----------------------#2023-06-28](https://towardsdatascience.com/taatiknet-sequence-to-sequence-learning-for-hebrew-transliteration-4c9175a90c23?source=collection_archive---------5-----------------------#2023-06-28)
- en: 'A simple demonstration of character-level seq2seq learning applied to a complex
    task: converting between Hebrew text and Latin transliteration'
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 一个简单的示例，展示了字符级别的 seq2seq 学习应用于复杂任务：在希伯来文文本和拉丁文音译之间的转换
- en: '[](https://medium.com/@morrisalper?source=post_page-----4c9175a90c23--------------------------------)[![Morris
    Alper](../Images/3c21362e6bbb077d066f625a46c8f56c.png)](https://medium.com/@morrisalper?source=post_page-----4c9175a90c23--------------------------------)[](https://towardsdatascience.com/?source=post_page-----4c9175a90c23--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----4c9175a90c23--------------------------------)
    [Morris Alper](https://medium.com/@morrisalper?source=post_page-----4c9175a90c23--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@morrisalper?source=post_page-----4c9175a90c23--------------------------------)[![Morris
    Alper](../Images/3c21362e6bbb077d066f625a46c8f56c.png)](https://medium.com/@morrisalper?source=post_page-----4c9175a90c23--------------------------------)[](https://towardsdatascience.com/?source=post_page-----4c9175a90c23--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----4c9175a90c23--------------------------------)
    [莫里斯·阿尔珀](https://medium.com/@morrisalper?source=post_page-----4c9175a90c23--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F3d69bf87dbed&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftaatiknet-sequence-to-sequence-learning-for-hebrew-transliteration-4c9175a90c23&user=Morris+Alper&userId=3d69bf87dbed&source=post_page-3d69bf87dbed----4c9175a90c23---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----4c9175a90c23--------------------------------)
    ·10 min read·Jun 28, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F4c9175a90c23&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftaatiknet-sequence-to-sequence-learning-for-hebrew-transliteration-4c9175a90c23&user=Morris+Alper&userId=3d69bf87dbed&source=-----4c9175a90c23---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F3d69bf87dbed&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftaatiknet-sequence-to-sequence-learning-for-hebrew-transliteration-4c9175a90c23&user=Morris+Alper&userId=3d69bf87dbed&source=post_page-3d69bf87dbed----4c9175a90c23---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----4c9175a90c23--------------------------------)
    ·10分钟阅读·2023年6月28日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F4c9175a90c23&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftaatiknet-sequence-to-sequence-learning-for-hebrew-transliteration-4c9175a90c23&user=Morris+Alper&userId=3d69bf87dbed&source=-----4c9175a90c23---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4c9175a90c23&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftaatiknet-sequence-to-sequence-learning-for-hebrew-transliteration-4c9175a90c23&source=-----4c9175a90c23---------------------bookmark_footer-----------)![](../Images/1ae0bda614ea7219880780a53035490d.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4c9175a90c23&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftaatiknet-sequence-to-sequence-learning-for-hebrew-transliteration-4c9175a90c23&source=-----4c9175a90c23---------------------bookmark_footer-----------)![](../Images/1ae0bda614ea7219880780a53035490d.png)'
- en: How can we use deep learning to convert between strings without getting “boggled”?
    ([Image](https://commons.wikimedia.org/wiki/File:Boggle_%282041108449%29.jpg)
    by Andrew Malone, [CC BY 2.0)](https://creativecommons.org/licenses/by/2.0/deed.en)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何利用深度学习在不被“困惑”的情况下进行字符串转换？（[图片](https://commons.wikimedia.org/wiki/File:Boggle_%282041108449%29.jpg)
    by Andrew Malone, [CC BY 2.0](https://creativecommons.org/licenses/by/2.0/deed.en)）
- en: '*This article describes TaatikNet and how to easily implement seq2seq models.
    For code and documentation, see the* [*TaatikNet GitHub repo*](https://github.com/morrisalp/taatiknet)*.
    For an interactive demo, see* [*TaatikNet on HF Spaces*](https://huggingface.co/spaces/malper/taatiknet)*.*'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '*本文描述了 TaatikNet 以及如何轻松实现 seq2seq 模型。有关代码和文档，请参见* [*TaatikNet GitHub 仓库*](https://github.com/morrisalp/taatiknet)*。要查看交互式演示，请参见*
    [*TaatikNet 在 HF Spaces 上*](https://huggingface.co/spaces/malper/taatiknet)*。*'
- en: Introduction
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: 'Many tasks of interest in NLP involve converting between texts in different
    styles, languages, or formats:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 许多自然语言处理中的有趣任务涉及在不同风格、语言或格式的文本之间转换：
- en: '**Machine translation** (e.g. English to German)'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**机器翻译**（例如，从英语到德语）'
- en: '**Text summarization** **and paraphrasing** (e.g. long text to short text)'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**文本摘要** **和释义**（例如，将长文本缩短为短文本）'
- en: '**Spelling correction**'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**拼写纠正**'
- en: '**Abstractive question answering** (input: context and question, output: text
    of answer)'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**抽象问题回答**（输入：上下文和问题，输出：答案文本）'
- en: Such tasks are known collectively as **Sequence-to-Sequence (Seq2seq) Learning**.
    In all of these tasks, the input and desired output are strings, which may be
    of different lengths and which are usually not in one-to-one correspondence with
    each other.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 这些任务统称为**序列到序列（Seq2seq）学习**。在所有这些任务中，输入和期望的输出都是字符串，这些字符串可能具有不同的长度，并且通常彼此之间没有一一对应关系。
- en: Suppose you have a dataset of paired examples (e.g. lists of sentences and their
    translations, many examples of misspelled and corrected texts, etc.). Nowadays,
    it is fairly easy to train a neural network on these as long as there is enough
    data so that the model may learn to generalize to new inputs. Let’s take a look
    at how to train seq2seq models with minimal effort, using PyTorch and the Hugging
    Face transformers library.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你有一个配对示例的数据集（例如，句子及其翻译的列表，拼写错误和修正文本的多个示例等）。如今，只要数据足够多，使得模型能够学习到对新输入的泛化，训练神经网络变得相当容易。让我们看看如何使用
    PyTorch 和 Hugging Face transformers 库以最小的努力训练 seq2seq 模型。
- en: 'We’ll focus on a particularly interesting use case: learning to **convert between
    Hebrew text and Latin transliteration.** We’ll give an overview of this task below,
    but the ideas and code presented here are useful beyond this particular case —
    this tutorial should be useful for **anyone who wants to perform seq2seq learning
    from a dataset of examples**.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将重点关注一个特别有趣的用例：学习**在希伯来文本和拉丁音译之间转换**。我们将在下文中概述这一任务，但这里提出的想法和代码对超出这一特定案例的应用也是有用的——本教程对**任何希望从示例数据集中执行
    seq2seq 学习的人**都应该有帮助。
- en: 'Our Task: Hebrew Transliteration'
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 我们的任务：希伯来音译
- en: 'In order to demonstrate seq2seq learning with an interesting and fairly novel
    use case, we apply it to **transliteration**. In general, transliteration refers
    to converting between different scripts. While English is written with the Latin
    script (“ABC…”), the world’s languages use many different writing systems, as
    illustrated below:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 为了展示具有趣味性和相当新颖的应用案例，我们将其应用于**音译**。一般来说，音译是指在不同书写系统之间转换。虽然英语使用拉丁字母书写（“ABC…”），但世界上的语言使用许多不同的书写系统，如下所示：
- en: '![](../Images/102989c28f1b328fa40affc11b9136c9.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/102989c28f1b328fa40affc11b9136c9.png)'
- en: Some of the many writing systems of the world. ([Image](https://commons.wikimedia.org/wiki/File:World_alphabets_%26_writing_systems.svg)
    by Nickshanks, [CC-BY-SA-3](https://creativecommons.org/licenses/by-sa/3.0/))
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 世界上的一些书写系统。（[图片](https://commons.wikimedia.org/wiki/File:World_alphabets_%26_writing_systems.svg)
    by Nickshanks, [CC-BY-SA-3](https://creativecommons.org/licenses/by-sa/3.0/))
- en: 'What if we want to use the Latin alphabet to write out a word from a language
    originally written in a different script? This challenge is illustrated by the
    many ways to write the name of the Jewish holiday of Hanukkah. The current introduction
    to [its Wikipedia article](https://en.wikipedia.org/wiki/Hanukkah) reads:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想使用拉丁字母来书写一个原本用不同书写系统书写的语言中的单词怎么办？这一挑战通过许多书写犹太节日汉ukkah 名称的方式得到体现。目前的介绍在[维基百科文章](https://en.wikipedia.org/wiki/Hanukkah)中读取如下：
- en: '**Hanukkah** ([/ˈhɑːnəkə/](https://en.wikipedia.org/wiki/Help:IPA/English);
    [Hebrew](https://en.wikipedia.org/wiki/Hebrew_language): חֲנֻכָּה‎, [Modern](https://en.wikipedia.org/wiki/Modern_Hebrew):
    Ḥanukka, [Tiberian](https://en.wikipedia.org/wiki/Tiberian_vocalization): Ḥănukkā)
    is a [Jewish festival](https://en.wikipedia.org/wiki/Jewish_holidays) commemorating
    the recovery of [Jerusalem](https://en.wikipedia.org/wiki/Jerusalem) and subsequent
    rededication of the [Second Temple](https://en.wikipedia.org/wiki/Second_Temple)
    at the beginning of the [Maccabean Revolt](https://en.wikipedia.org/wiki/Maccabean_Revolt)
    against the [Seleucid Empire](https://en.wikipedia.org/wiki/Seleucid_Empire) in
    the 2nd century BCE.'
  id: totrans-24
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The Hebrew word חֲנֻכָּה‎ may be transliterated in Latin script as *Hanukkah*,
    *Chanukah, Chanukkah, Ḥanukka,* or one of many other variants. In Hebrew as well
    as in many other writing systems, there are various conventions and ambiguities
    that make transliteration complex and not a simple one-to-one mapping between
    characters.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: In the case of Hebrew, it is largely possible to transliterate text with nikkud
    (vowel signs) into Latin characters using a complex set of rules, though there
    are various edge cases that make this deceptively complex. Furthermore, attempting
    to transliterate text without vowel signs or to perform the reverse mapping (e.g.
    *Chanukah* → חֲנֻכָּה) is much more difficult since there are many possible valid
    outputs.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: Thankfully, with deep learning applied to existing data, we can make great headway
    on solving this problem with only a minimal amount of code. Let’s see how we can
    **train a seq2seq model — TaatikNet — to learn how to convert between Hebrew text
    and Latin transliteration on its own**. We note that this is a **character-level
    task** since it involves reasoning on the correlations between different characters
    in Hebrew text and transliterations. We will discuss the significance of this
    in more detail below.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: As an aside, you may have heard of [UNIKUD](/unikud-adding-vowels-to-hebrew-text-with-deep-learning-powered-by-dagshub-56d238e22d3f),
    our model for adding vowel points to unvocalized Hebrew text. There are some similarities
    between these tasks, but the key difference is that UNIKUD performed character-level
    classification, where for each character we learned whether to insert one or more
    vowel symbols adjacent to it. By contrast, in our case the input and output texts
    may not exactly correspond in length or order due to the complex nature of transliteration,
    which is why we use seq2seq learning here (and not just per-character classification).
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: Data collection
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As with most machine learning tasks, we are fortunate if we can collect many
    examples of inputs and desired outputs of our model, so that we may train it using
    **supervised learning.**
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: 'For many tasks regarding words and phrases, a great resource is [Wiktionary](https://www.wiktionary.org/)
    and its multilingual counterparts — think Wikipedia meets dictionary. In particular,
    the [Hebrew Wiktionary (ויקימילון)](https://he.wiktionary.org/) contains entries
    with structured grammatical information as shown below:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 对于许多与单词和短语相关的任务，一个很好的资源是[Wiktionary](https://www.wiktionary.org/)及其多语言对照——可以将其想象为维基百科与词典的结合。特别是，[希伯来语Wiktionary
    (ויקימילון)](https://he.wiktionary.org/)包含了结构化的语法信息条目，如下所示：
- en: '![](../Images/0ce163ee79b25159ef63f0c717ca3ab9.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0ce163ee79b25159ef63f0c717ca3ab9.png)'
- en: Grammatical information from the Hebrew Wiktionary article [עגבניה](https://he.wiktionary.org/wiki/%D7%A2%D7%92%D7%91%D7%A0%D7%99%D7%94)
    (tomato).
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 来自希伯来语Wiktionary文章[עגבניה](https://he.wiktionary.org/wiki/%D7%A2%D7%92%D7%91%D7%A0%D7%99%D7%94)（西红柿）的语法信息。
- en: In particular, this includes Latin transliteration (*agvani****ya***, where
    the bold indicates stress). Along with section titles containing nikkud (vowel
    characters), this gives us the (freely-licensed) data that we need to train our
    model.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 特别是，这包括拉丁音译（*agvani****ya***，其中粗体表示重音）。连同包含nikkud（元音字符）的章节标题，这为我们提供了训练模型所需的（自由许可）数据。
- en: In order to create a dataset, we scrape these items using the Wikimedia REST
    API ([example here](https://gist.github.com/morrisalp/8fb89b43d79e2ea2190b18441e83d5eb)).
    Please note that original texts in Wiktionary entries have permissive licenses
    for derivative works (CC and GNU licenses, [details here](https://en.wiktionary.org/wiki/Wiktionary:Copyrights))
    and require share-alike licensing (TaatikNet license [here](https://github.com/morrisalp/taatiknet/blob/main/LICENSE));
    in general, if you perform data scraping make sure that you are using permissively
    licensed data, scraping appropriately, and using the correct license for your
    derivative work.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 为了创建数据集，我们使用Wikimedia REST API抓取这些条目（[示例见此](https://gist.github.com/morrisalp/8fb89b43d79e2ea2190b18441e83d5eb)）。请注意，Wiktionary条目的原始文本具有宽松的衍生作品许可（CC和GNU许可，[详情见此](https://en.wiktionary.org/wiki/Wiktionary:Copyrights)），并要求共享相同许可（TaatikNet许可[见此](https://github.com/morrisalp/taatiknet/blob/main/LICENSE)）；通常情况下，如果你执行数据抓取，请确保使用具有宽松许可的数据，适当抓取，并使用正确的衍生作品许可。
- en: 'We perform various preprocessing steps on this data, including:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对这些数据执行各种预处理步骤，包括：
- en: Removing Wiki markup and metadata
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 去除Wiki标记和元数据
- en: Replacing bolding to represent stress with acute accents (e.g. *agvani****ya***
    → *agvaniyá)*
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用尖音符号代替粗体表示重音（例如 *agvani****ya*** → *agvaniyá)*。
- en: '[Unicode NFC normalization](https://unicode.org/reports/tr15/) to unify identically
    appearing glyphs such as בּ (U+05D1 Hebrew Letter Bet + U+05BC Hebrew Point Dagesh
    or Mapiq) and בּ (U+FB31 Hebrew Letter Bet with Dagesh). You can compare these
    yourself by copy-pasting them into the [Show Unicode Character tool](https://qaz.wtf/u/show.cgi).
    We also unify similarly appearing punctuation marks such as Hebrew geresh (׳)
    and apostrophe (‘).'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Unicode NFC规范化](https://unicode.org/reports/tr15/) 用于统一相同出现的字形，例如 בּ（U+05D1
    希伯来字母 Bet + U+05BC 希伯来点 Dagesh 或 Mapiq）和 בּ（U+FB31 希伯来字母 Bet 带 Dagesh）。你可以通过将它们粘贴到[Show
    Unicode Character工具](https://qaz.wtf/u/show.cgi)中自行比较。我们还统一了类似的标点符号，如希伯来语 geresh（׳）和撇号（‘）。'
- en: Splitting multiple-word expressions into individual words.
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将多词表达拆分为单个词。
- en: 'After data scraping and preprocessing, we are left with nearly 15k word-transliteration
    pairs ([csv file available here](https://github.com/morrisalp/taatiknet/blob/main/data/he_transliterations.csv)).
    A few examples are shown below:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 数据抓取和预处理后，我们得到近15k对单词-音译对（[csv文件见此](https://github.com/morrisalp/taatiknet/blob/main/data/he_transliterations.csv)）。以下是几个示例：
- en: '![](../Images/f78fcd61ff3823076b9648f752661681.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f78fcd61ff3823076b9648f752661681.png)'
- en: A few examples of items from our dataset. Note that the Hebrew with nikkud (vowel
    points) is the second column, but appears first due to right-to-left text rendering
    issues.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 我们数据集中的几个项目示例。请注意，带有nikkud（元音点）的希伯来语在第二列，但由于从右到左的文本渲染问题，它首先出现。
- en: The transliterations are by no means consistent or error-free; for example,
    stress is inconsistently and often incorrectly marked, and various spelling conventions
    are used (e.g. ח may correspond to *h, kh,* or *ch*). Rather than attempting to
    clean these, we will simply feed them directly to the model and have it make sense
    of them by itself.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 音译绝非一致或无误；例如，重音标记不一致且经常错误标记，且使用了各种拼写规则（例如 ח 可能对应于 *h, kh,* 或 *ch*）。我们不会尝试清理这些，而是将其直接输入模型，让模型自行理解。
- en: Training
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练
- en: Now that we have our dataset, let’s get to the “meat” of our project — training
    a seq2seq model on our data. We call the final model **TaatikNet** after the Hebrew
    word תעתיק *taatik* meaning “transliteration”. We will describe TaatikNet’s training
    on a high level here, but you are highly recommended to peruse the [annotated
    training notebook](https://github.com/morrisalp/taatiknet/blob/main/training.ipynb).
    The training code itself is quite short and instructive.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了数据集，让我们进入项目的“核心”——在我们的数据上训练 seq2seq 模型。我们将最终模型命名为 **TaatikNet**，取自希伯来语单词
    תעתיק *taatik*，意为“音译”。我们将在这里高层次地描述 TaatikNet 的训练，但强烈建议你阅读[注释过的训练笔记本](https://github.com/morrisalp/taatiknet/blob/main/training.ipynb)。训练代码本身非常简短且具有指导性。
- en: To achieve state-of-the-art results on NLP tasks, a common paradigm is to take
    a pretrained transformer neural network and apply transfer learning by continuing
    to fine-tune it on a task-specific dataset. For seq2seq tasks, the most natural
    choice of base model is an [encoder-decoder (enc-dec) model](https://huggingface.co/docs/transformers/model_doc/encoder-decoder).
    Common enc-dec models such as T5 and BART are excellent for common seq2seq tasks
    like text summarization, but because they tokenize text (split it into subword
    tokens, roughly words or chunks of words) these are less appropriate for our task
    which requires reasoning on the level of individual characters. For this reason,
    we use the tokenizer-free ByT5 enc-dec model ([paper](https://arxiv.org/abs/2105.13626),
    [HF model page](https://huggingface.co/docs/transformers/model_doc/byt5)), which
    performs calculations on the level of individual bytes (roughly characters, but
    see [Joel Spolsky’s excellent post on Unicode and character sets](https://www.joelonsoftware.com/2003/10/08/the-absolute-minimum-every-software-developer-absolutely-positively-must-know-about-unicode-and-character-sets-no-excuses/)
    for a better understanding of how Unicode glyphs map to bytes).
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 要在自然语言处理（NLP）任务上实现最先进的结果，一种常见的方法是使用预训练的变换器神经网络，并通过继续在任务特定数据集上进行微调来应用迁移学习。对于
    seq2seq 任务，最自然的基模型选择是一个[编码器-解码器（enc-dec）模型](https://huggingface.co/docs/transformers/model_doc/encoder-decoder)。像
    T5 和 BART 这样的常见 enc-dec 模型非常适合常见的 seq2seq 任务，如文本摘要，但由于它们对文本进行标记化（将其拆分为子词标记，大致是词或词块），因此不太适合我们的任务，因为我们需要在单个字符级别上进行推理。为此，我们使用无标记化的
    ByT5 enc-dec 模型（[论文](https://arxiv.org/abs/2105.13626)，[HF 模型页面](https://huggingface.co/docs/transformers/model_doc/byt5)），该模型在单个字节级别上进行计算（大致为字符，但请参阅[Joel
    Spolsky 对 Unicode 和字符集的优秀文章](https://www.joelonsoftware.com/2003/10/08/the-absolute-minimum-every-software-developer-absolutely-positively-must-know-about-unicode-and-character-sets-no-excuses/)以更好地理解
    Unicode 字形如何映射到字节）。
- en: 'We first create a PyTorch Dataset object to encapsulate our training data.
    We could simply wrap the data from our dataset csv file with no modifications,
    but we add some random augmentations to make the model’s training procedure more
    interesting:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先创建一个 PyTorch Dataset 对象来封装我们的训练数据。我们可以简单地将数据集 CSV 文件中的数据包装起来而不做任何修改，但我们添加了一些随机增强，使模型的训练过程更加有趣：
- en: '[PRE0]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: This augmentation teaches TaatikNet to accept either Hebrew script or Latin
    script as input and to calculate the corresponding matching output. We also randomly
    drop vowel signs or accents to train the model to be robust to their absence.
    In general, random augmentation is a nice trick when you would like your network
    to learn to handle various types of inputs without calculating all possible inputs
    and outputs from your dataset ahead of time.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 这种增强方法教会 TaatikNet 接受希伯来文字或拉丁文字作为输入，并计算相应的匹配输出。我们还会随机丢弃元音符号或重音，以训练模型对其缺失具有鲁棒性。一般来说，随机增强是一种很好的技巧，当你希望网络学会处理各种类型的输入，而不必事先计算数据集中所有可能的输入和输出时。
- en: 'We load the base model with the Hugging Face pipeline API using a single line
    of code:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用一行代码通过 Hugging Face pipeline API 加载基础模型：
- en: '[PRE1]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'After handling data collation and setting hyperparameters (number of epochs,
    batch size, learning rate) we train our model on our dataset and print out selected
    results after each epoch. The training loop is standard PyTorch, apart from the
    `evaluate(…)` function which we define elsewhere and which prints out the model’s
    current predictions on various inputs:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理数据整合和设置超参数（如训练轮次、批量大小、学习率）后，我们在数据集上训练模型，并在每轮训练后打印出选定的结果。训练循环是标准的 PyTorch，除了
    `evaluate(…)` 函数外，该函数在其他地方定义，并打印出模型在各种输入上的当前预测：
- en: '[PRE2]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Compare some results from early epochs and at the end of training:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 比较早期轮次和训练结束时的一些结果：
- en: '[PRE3]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Before training the model outputs gibberish, as expected. During training we
    see that the model first learns how to construct valid-looking Hebrew and transliterations,
    but takes longer to learn the connection between them. It also takes longer to
    learn rare items such as ג׳ (*gimel* + *geresh*) corresponding to *j*.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练之前，模型输出的是无意义的字符，这在预期之中。在训练过程中，我们看到模型首先学会如何构造有效的希伯来语和音译，但花费更长时间来学习它们之间的联系。它也需要更长时间来学习诸如ג׳（*gimel*
    + *geresh*）对应于 *j* 的稀有项目。
- en: 'A caveat: We did not attempt to optimize the training procedure; the hyperparameters
    were chosen rather arbitrarily, and we did not set aside validation or test sets
    for rigorous evaluation. The purpose of this was only to provide a simple example
    of seq2seq training and a proof of concept of learning transliterations; however,
    hyperparameter tuning and rigorous evaluation would be a promising direction for
    future work along with the points mentioned in the limitations section below.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 一个警告：我们没有尝试优化训练过程；超参数的选择相当随意，也没有为严格评估预留验证集或测试集。此目的仅为提供一个seq2seq训练的简单示例和音译学习的概念验证；然而，超参数调优和严格评估将是未来工作的一个有前途的方向，结合下文限制部分提到的要点。
- en: Results
  id: totrans-59
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结果
- en: A few examples are shown below, demonstrating conversion between Hebrew text
    (with or without vowels) and Latin transliteration, in both directions. You may
    try playing with TaatikNet yourself at the [interactive demo on HF Spaces](https://huggingface.co/spaces/malper/taatiknet).
    Note that it uses beam search (5 beams) for decoding and inference is run on each
    word separately.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 下面显示了一些示例，演示了希伯来文本（有或没有元音）与拉丁音译之间的双向转换。你可以在[HF Spaces上的互动演示](https://huggingface.co/spaces/malper/taatiknet)中尝试自己使用TaatikNet。注意，它使用束搜索（5束）进行解码，推理是对每个单词单独进行的。
- en: '![](../Images/47a83d23e139337310a9b5dab24aacc5.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/47a83d23e139337310a9b5dab24aacc5.png)'
- en: Sample inputs and outputs of TaatikNet from [the interactive demo](https://huggingface.co/spaces/malper/taatiknet).
    Multiple outputs are generated using beam search decoding (5 beams).
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: TaatikNet的输入和输出示例见[互动演示](https://huggingface.co/spaces/malper/taatiknet)。使用束搜索解码（5束）生成多个输出。
- en: '![](../Images/7b1fd23bfc4ffe356ccf0b1981454607.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7b1fd23bfc4ffe356ccf0b1981454607.png)'
- en: Sample output on a longer text. Inference is run on each word separately. Note
    the successful transliteration of challenging cases such as שבעיניו (the final
    yud is not pronounced), חוכמה (kamatz gadol), כאלה (penultimate stress).
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 更长文本的示例输出。推理是对每个单词单独进行的。注意挑战性案例如שבעיניו（最后的yud不发音）、חוכמה（kamatz gadol）、כאלה（倒数第二个音节重音）的成功音译。
- en: Limitations and Further Directions
  id: totrans-65
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 限制与进一步方向
- en: 'For the sake of simplicity we implemented TaatikNet as a minimal seq2seq model
    without extensive tuning. However, if you are interested in improving results
    on conversion between Hebrew text and transliteration, there are many promising
    directions for future work:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简化，我们将TaatikNet实现为一个最小的seq2seq模型，没有进行广泛的调优。然而，如果你对改进希伯来文文本与音译之间的转换结果感兴趣，有许多有前景的未来工作方向：
- en: TaatikNet only tries to guess the appropriate spelling (in Hebrew or Latin transliteration)
    based on letter or sound correspondences. However, you might want to convert from
    transliteration to valid Hebrew text given the context (e.g. *zot dugma* → זאת
    דוגמא rather than the incorrectly spelled *זות דוגמע). Possible ways to accomplish
    this could include retrieval augmented generation (accessing a dictionary) or
    training on pairs of Hebrew sentences and their Latin transliterations in order
    to learn contextual cues.
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TaatikNet仅尝试根据字母或声音对应关系猜测适当的拼写（无论是希伯来文还是拉丁音译）。然而，根据上下文，你可能希望将音译转换为有效的希伯来文本（例如
    *zot dugma* → זאת דוגמא，而不是拼写错误的 *זות דוגמע*）。实现这一点的可能方法包括检索增强生成（访问词典）或在希伯来语句子及其拉丁音译的配对上进行训练，以学习上下文提示。
- en: Unusually formed inputs may cause TaatikNet’s decoding to get stuck in a loop,
    e.g. *drapapap* → דְּרַפָּפָּאפָּאפָּאפָּאפָּאפָּאפָּאפָּאפָּאפָּאפָּאפָּאפָּאפָּאפָּאפָּאפָּאפָּאפָּאפָּאפָּאפָּאפָּ.
    This might be handled by augmentation during training, more diverse training data,
    or using cycle consistency in training or decoding.
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 形式不寻常的输入可能会导致TaatikNet的解码陷入循环，例如 *drapapap* → דְּרַפָּפָּאפָּאפָּאפָּאפָּאפָּאפָּאפָּאפָּאפָּאפָּאפָּאפָּאפָּאפָּאפָּאפָּאפָּאפָּאפָּאפָּאפָּאפָּאפָּ.
    这可能通过训练中的数据增强、更丰富的训练数据或在训练或解码中使用循环一致性来解决。
- en: TaatikNet may not handle some conventions that are quite rare in its training
    data. For example, it often does not properly handle ז׳ (*zayin+geresh*) which
    indicates the rare foreign sound *zh*. This might indicate underfitting or that
    it would be helpful to use sample weights during training to emphasize difficult
    examples
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TaatikNet可能无法处理其训练数据中一些相当稀有的惯例。例如，它常常无法正确处理ז׳（*zayin+geresh*），这表示稀有的外来音*zh*。这可能表明过拟合，或者在训练时使用样本权重以强调困难示例可能会有所帮助。
- en: The ease of seq2seq training comes at the cost of interpretability and robustness
    — we might like to know exactly how TaatikNet makes its decisions, and ensure
    that they are applied consistently. An interesting possible extension would be
    distilling its knowledge into a set of rule-based conditions (e.g. if character
    X is seen in context Y, then write Z). Perhaps recent code-pretrained LLMs could
    be helpful for this.
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: seq2seq训练的便利性是以解释性和鲁棒性为代价的——我们可能希望确切了解TaatikNet如何做出决策，并确保这些决策的一致性。一个有趣的可能扩展是将其知识提炼成一组基于规则的条件（例如，如果在上下文Y中看到字符X，则写Z）。最近的代码预训练LLM可能对这个有所帮助。
- en: We do not handle [“full spelling” and “defective spelling” (כתיב מלא / חסר)](https://en.wikipedia.org/wiki/Ktiv_hasar_niqqud),
    whereby Hebrew words are spelled slightly differently when written with or without
    vowel signs. Ideally, the model would be trained on “full” spellings without vowels
    and “defective” spellings with vowels. See [UNIKUD](/unikud-adding-vowels-to-hebrew-text-with-deep-learning-powered-by-dagshub-56d238e22d3f)
    for one approach to handling these spellings in models trained on Hebrew text.
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们没有处理[“完整拼写”和“缺陷拼写”（כתיב מלא / חסר）](https://en.wikipedia.org/wiki/Ktiv_hasar_niqqud)，即希伯来词在有无元音符号的情况下拼写略有不同。理想情况下，模型应在无元音的“完整”拼写和有元音的“缺陷”拼写上进行训练。请参阅[UNIKUD](/unikud-adding-vowels-to-hebrew-text-with-deep-learning-powered-by-dagshub-56d238e22d3f)了解处理这些拼写的一种方法。
- en: If you try these or other ideas and find that they lead to an improvement, I
    would be very interested in hearing from you, and crediting you here — feel free
    to reach out via my contact info below this article.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你尝试这些或其他想法并发现它们带来了改进，我非常乐意听到你的反馈，并在此处给你致谢——可以通过本文下方的联系信息与我联系。
- en: Conclusion
  id: totrans-73
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: We have seen that it is quite easy to train a seq2seq model with supervised
    learning — teaching it to generalize from a large set of paired examples. In our
    case, we used a character-level model (TaatikNet, fine-tuned from the base ByT5
    model), but nearly the same procedure and code could be used for a more standard
    seq2seq task such as machine translation.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到，用监督学习训练一个seq2seq模型是相当简单的——教它从大量配对示例中进行归纳。在我们的案例中，我们使用了一个字符级模型（TaatikNet，从基础ByT5模型微调而来），但几乎可以使用相同的过程和代码来处理更标准的seq2seq任务，如机器翻译。
- en: I hope you have learned as much from this tutorial as I did from putting it
    together! Feel free to contact me with any questions, comments, or suggestions;
    my contact information may be found at my website, linked below.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 我希望你从本教程中学到的东西与我整理它时学到的一样多！如有任何问题、意见或建议，请随时与我联系；我的联系信息可以在下面链接的我的网站上找到。
- en: '**Morris Alper, MSc** is a PhD student at Tel Aviv University researching multimodal
    learning (NLP, Computer Vision, and other modalities). Please see his webpage
    for more information and contact info: [https://morrisalp.github.io/](https://morrisalp.github.io/)'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '**莫里斯·阿尔珀，硕士** 是特拉维夫大学的博士生，研究多模态学习（NLP、计算机视觉及其他模态）。有关更多信息和联系信息，请访问他的网页：[https://morrisalp.github.io/](https://morrisalp.github.io/)'
