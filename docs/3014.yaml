- en: Mastering Language Models
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 掌握语言模型
- en: 原文：[https://towardsdatascience.com/mastering-language-models-32e1d891511a?source=collection_archive---------5-----------------------#2023-10-03](https://towardsdatascience.com/mastering-language-models-32e1d891511a?source=collection_archive---------5-----------------------#2023-10-03)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/mastering-language-models-32e1d891511a?source=collection_archive---------5-----------------------#2023-10-03](https://towardsdatascience.com/mastering-language-models-32e1d891511a?source=collection_archive---------5-----------------------#2023-10-03)
- en: Navigating the quality-diversity tradeoff with temperature, top-p, top-k, and
    more
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 通过温度、top-p、top-k等来平衡质量与多样性
- en: '[](https://medium.com/@samuel.montgomery59?source=post_page-----32e1d891511a--------------------------------)[![Samuel
    Montgomery](../Images/52f12c797b53706ad1039459238ece44.png)](https://medium.com/@samuel.montgomery59?source=post_page-----32e1d891511a--------------------------------)[](https://towardsdatascience.com/?source=post_page-----32e1d891511a--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----32e1d891511a--------------------------------)
    [Samuel Montgomery](https://medium.com/@samuel.montgomery59?source=post_page-----32e1d891511a--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@samuel.montgomery59?source=post_page-----32e1d891511a--------------------------------)[![Samuel
    Montgomery](../Images/52f12c797b53706ad1039459238ece44.png)](https://medium.com/@samuel.montgomery59?source=post_page-----32e1d891511a--------------------------------)[](https://towardsdatascience.com/?source=post_page-----32e1d891511a--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----32e1d891511a--------------------------------)
    [Samuel Montgomery](https://medium.com/@samuel.montgomery59?source=post_page-----32e1d891511a--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fbf5cf7332a65&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmastering-language-models-32e1d891511a&user=Samuel+Montgomery&userId=bf5cf7332a65&source=post_page-bf5cf7332a65----32e1d891511a---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----32e1d891511a--------------------------------)
    ·12 min read·Oct 3, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F32e1d891511a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmastering-language-models-32e1d891511a&user=Samuel+Montgomery&userId=bf5cf7332a65&source=-----32e1d891511a---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fbf5cf7332a65&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmastering-language-models-32e1d891511a&user=Samuel+Montgomery&userId=bf5cf7332a65&source=post_page-bf5cf7332a65----32e1d891511a---------------------post_header-----------)
    发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----32e1d891511a--------------------------------)
    · 12 min read · 2023年10月3日 [](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F32e1d891511a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmastering-language-models-32e1d891511a&user=Samuel+Montgomery&userId=bf5cf7332a65&source=-----32e1d891511a---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F32e1d891511a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmastering-language-models-32e1d891511a&source=-----32e1d891511a---------------------bookmark_footer-----------)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F32e1d891511a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmastering-language-models-32e1d891511a&source=-----32e1d891511a---------------------bookmark_footer-----------)'
- en: If you have ever used a language model through a playground or an API, you may
    have been asked to choose some input parameters. For many of us, the meaning of
    these parameters (and the right way to use them) is less than totally clear.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你曾经通过操控台或API使用过语言模型，你可能被要求选择一些输入参数。对于我们许多人来说，这些参数的意义（以及正确的使用方法）可能并不十分清楚。
- en: '![](../Images/b9bdc5e91439bacb53807788c292484d.png)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b9bdc5e91439bacb53807788c292484d.png)'
- en: A screenshot showing parameter selection in the SillyTavern interface. Image
    by the author.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 一张显示 SillyTavern 界面中参数选择的截图。图片由作者提供。
- en: This article will teach you how to use these parameters to control hallucinations,
    inject creativity into your model’s outputs, and make other fine-grained adjustments
    to optimize behavior. Much like prompt engineering, input parameter tuning can
    get your model running at 110%.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 这篇文章将教你如何使用这些参数来控制幻觉，注入创造力到模型输出中，并进行其他细微的调整以优化行为。就像提示工程一样，输入参数调整可以让你的模型达到110%的性能。
- en: By the end of this article, you’ll be an expert on five essential input parameters
    — temperature, top-p, top-k, frequency penalty, and presence penalty. You’ll also
    learn how each of these parameters helps us navigate the quality-diversity tradeoff.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文结束时，你将成为五个重要输入参数的专家——温度、top-p、top-k、频率惩罚和存在惩罚。你还将了解这些参数如何帮助我们在质量和多样性之间进行权衡。
- en: So, grab a coffee, and let’s get started!
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，拿一杯咖啡，我们开始吧！
- en: Table of Contents
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 目录
- en: · [Background](#5876)
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: · [背景](#5876)
- en: · [Quality, Diversity, and Temperature](#1164)
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: · [质量、多样性与温度](#1164)
- en: · [Top-k and Top-p](#a476)
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: · [Top-k 和 Top-p](#a476)
- en: · [Frequency and Presence Penalties](#e517)
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: · [频率和存在惩罚](#e517)
- en: · [The Parameter-Tuning Cheat Sheet](#c9f4)
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: · [参数调整备忘单](#c9f4)
- en: · [Wrapping up](#7610)
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: · [总结](#7610)
- en: Background
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 背景
- en: Before we get around to selecting our input parameters, we will need to go over
    some background information. Let’s talk about how these models choose the words
    that they generate.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始选择输入参数之前，我们需要了解一些背景信息。让我们来谈谈这些模型是如何选择生成的单词的。
- en: 'To read a document, a language model breaks it down into a sequence of tokens.
    A token is just a small chunk of text that the model can easily understand: It
    could be a word, a syllable, or a character. For example, “Megaputer Intelligence
    Inc.” might be broken down into five tokens: [“Mega”, “puter ”, “Intelligence”,
    “Inc”, “.”].'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 要读取文档，语言模型将其分解为一系列标记。标记只是模型可以轻松理解的小块文本：它可以是一个词、一个音节或一个字符。例如，“Megaputer Intelligence
    Inc.”可能会被分解为五个标记：[“Mega”，“puter”，“Intelligence”，“Inc”，“.”]。
- en: Most language models we are familiar with operate by repeatedly generating the
    next token in a sequence. Each time the model wants to generate another token,
    it re-reads the entire sequence and predicts the token that should come next.
    This strategy is known as *autoregressive* generation.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 我们熟悉的大多数语言模型通过反复生成序列中的下一个标记来操作。每当模型想要生成另一个标记时，它会重新读取整个序列，并预测下一个应该出现的标记。这种策略被称为*自回归*生成。
- en: '![](../Images/ff9f86565a19ba6f422e13157cd7b039.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ff9f86565a19ba6f422e13157cd7b039.png)'
- en: Autoregressive generation of tokens by a language model. GIF by [Echo Lu](https://www.linkedin.com/in/echoxlu/),
    containing a modification of an image by [Annie Surla](https://developer.nvidia.com/blog/author/anniesurla/)
    from [NVIDIA](https://developer.nvidia.com/blog/how-to-get-better-outputs-from-your-large-language-model/).
    Modified with permission from owner.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 语言模型的自回归标记生成。GIF由[Echo Lu](https://www.linkedin.com/in/echoxlu/)提供，包含由[Annie
    Surla](https://developer.nvidia.com/blog/author/anniesurla/)修改的[NVIDIA](https://developer.nvidia.com/blog/how-to-get-better-outputs-from-your-large-language-model/)的图像。经版权所有者许可修改。
- en: 'This explains why ChatGPT prints the words out one at a time: It is streaming
    the words to you as it writes them.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 这解释了为什么ChatGPT一次输出一个单词：它在写字的时候把单词流式传输给你。
- en: To choose the next token in the sequence, a language model first assigns a likelihood
    score to each token in its vocabulary. A token gets a high likelihood score if
    it is a good continuation of the text and a low likelihood score if it is a poor
    continuation of the text, as assessed by the model.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 要选择序列中的下一个标记，语言模型首先为其词汇表中的每个标记分配一个可能性分数。如果标记是文本的良好延续，它会获得一个高可能性分数；如果标记是文本的较差延续，它会获得一个低可能性分数，模型会进行评估。
- en: '![](../Images/40490cd101aa2e34bdc072b88614741d.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/40490cd101aa2e34bdc072b88614741d.png)'
- en: A language model assigns likelihood scores to predict the next token in the
    sequence. Original image by [Annie Surla](https://developer.nvidia.com/blog/author/anniesurla/)
    from [NVIDIA](https://developer.nvidia.com/blog/how-to-get-better-outputs-from-your-large-language-model/),
    modified by [Echo Lu](https://www.linkedin.com/in/echoxlu/) with permission from
    owner.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 语言模型分配可能性分数以预测序列中的下一个标记。原始图像由[Annie Surla](https://developer.nvidia.com/blog/author/anniesurla/)提供，来自[NVIDIA](https://developer.nvidia.com/blog/how-to-get-better-outputs-from-your-large-language-model/)，经[Echo
    Lu](https://www.linkedin.com/in/echoxlu/)修改，经过版权所有者许可。
- en: After the likelihood scores are assigned, a token is chosen using a token sampling
    scheme that takes the likelihood scores into account. The token sampling scheme
    may incorporate some randomness so that the language model does not answer the
    same question in the same way every time. This randomness can be a nice feature
    in chatbots or other applications.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在分配了可能性分数之后，通过考虑可能性分数的标记采样方案来选择标记。标记采样方案可能会包含一些随机性，以便语言模型不会每次以相同的方式回答相同的问题。这种随机性在聊天机器人或其他应用中可能是一个很好的特性。
- en: '*TLDR: Language models break down text into tokens, predict the next token
    in the sequence, and mix in some randomness. Repeat as needed to generate language.*'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: Quality, Diversity, and Temperature
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: But why would we ever want to pick the second-best token, the third-best token,
    or any other token besides the best, for that matter? Wouldn’t we want to pick
    the best token (the one with the highest likelihood score) every time? Often,
    we do. But if we picked the best answer every time, we would get the same answer
    every time. If we want a diverse range of answers, we may have to give up some
    quality to get it. This sacrifice of quality for diversity is called a quality-diversity
    tradeoff.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: With this in mind, *temperature* tells the machine how to navigate the quality-diversity
    tradeoff. Low temperatures mean more quality, while high temperatures mean more
    diversity. When the temperature is set to zero, the model always samples the token
    with the highest likelihood score, resulting in zero diversity between queries,
    but ensuring that we always pick the highest quality continuation as assessed
    by the model.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: Quite often, we will want to set the temperature to zero. As a rule, you should
    always choose temperature zero for any prompt that you will pass to the model
    only once, as this is most likely to get you a good answer. In my job as a data
    analyst, I set the temperature to zero for entity extraction, fact extraction,
    sentiment analysis, and most other standard tasks.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: At higher temperatures, we see more garbage and hallucinations, less coherence,
    and lower quality of responses on average, but also more creativity and diversity
    of answers. We recommend that you should *only* use non-zero temperatures when
    you want to ask the same question twice and get two different answers.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/72a0df19da4637d3f2f8b985de25958c.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
- en: Higher temperatures bring diversity, creativity, and multiplicity of answers
    but also add garbage, incoherence, and hallucinations. Image by [Echo Lu](https://www.linkedin.com/in/echoxlu/).
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: 'And why would we ever want two different answers to the same prompt? In some
    cases, having many answers to one prompt can be useful. For example, there is
    a technique in which we generate multiple answers to a prompt and keep only the
    best, which often produces better results than a single query at temperature zero.
    Another use case is synthetic data generation: We want many diverse synthetic
    data points, not just one data point that’s really good. We may discuss these
    use cases (and others) in later articles, but more often than not, we want only
    one answer per prompt. **When in doubt, choose temperature zero!**'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: It is important to note that while temperature zero should *in theory* produce
    the same answer every time, this may not be true in practice! This is because
    the GPUs the model is running on can be prone to small miscalculations, such as
    rounding errors. These errors introduce a low level of randomness into the calculations,
    even at temperature zero. Since changing one token in a text can significantly
    alter its meaning, a single error may cause a cascade of different token choices
    later in the text, resulting in an almost totally different output. But rest assured
    that this usually has a negligible impact on quality. We only mention it so that
    you’re not surprised when you get some randomness at temperatures zero.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，虽然理论上温度为零应该每次产生相同的答案，但实际情况可能并非如此！这是因为模型运行的 GPU 可能会出现小的计算误差，比如四舍五入错误。这些错误会在计算中引入微小的随机性，即使在温度为零时也是如此。由于在文本中改变一个词可能会显著改变其含义，因此一个错误可能会引起后续文本中不同词汇的连锁反应，从而导致几乎完全不同的输出。但请放心，这通常对质量的影响微乎其微。我们提到这一点是为了让你在温度为零时遇到一些随机性时不会感到惊讶。
- en: There are many more ways than temperature alone to navigate the quality-diversity
    tradeoff. In the next section, we will discuss some modifications to the temperature
    sampling technique. But if you are content with using temperature zero, feel free
    to skip it for now. You may rest soundly knowing that your choice of these parameters
    at temperature zero will not affect your answer.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 除了温度，还有很多其他方法可以在质量和多样性之间取得平衡。在下一节中，我们将讨论一些对温度采样技术的修改。但如果你对使用温度为零感到满意，可以暂时跳过这部分。你可以放心，你选择的这些参数在温度为零时不会影响你的答案。
- en: '*TLDR: Temperature increases diversity but decreases quality by adding randomness
    to the model’s outputs.*'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '*总结：温度增加了多样性，但通过将随机性添加到模型输出中而降低了质量。*'
- en: Top-k and Top-p
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Top-k 和 Top-p
- en: 'One common way to tweak our token-sampling formula is called top-k sampling.
    Top-k sampling is a lot like ordinary temperature sampling, except that the lowest
    likelihood tokens are excluded from being picked: Only the “top k” best choices
    are considered, which is where we get the name. The advantage of this method is
    that it stops us from picking truly bad tokens.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 调整我们的词汇采样公式的一种常见方法叫做 top-k 采样。Top-k 采样与普通的温度采样类似，只是排除了最低概率的词汇：只有“前 k”个最佳选择会被考虑，这也是名字的由来。这个方法的优势在于它防止我们选择真正糟糕的词汇。
- en: Let’s suppose, for example, that we are trying to make a completion for “The
    sun rises in the…” Then, without top-k sampling, the model considers every token
    in its vocabulary as a possible continuation of the sequence. Then there is some
    non-zero chance that it will write something ridiculous like “The sun rises in
    the refrigerator.” With top-k sampling, the model filters out these truly bad
    picks and only considers the k best options. By clipping off the long tail, we
    lose a little diversity, but our quality shoots way up.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 比如说，我们试图为“太阳在……升起”生成一个续写。如果不使用 top-k 采样，模型会考虑词汇表中的每一个词作为序列的可能延续。这样可能会有非零的几率生成诸如“太阳在冰箱里升起”这样荒谬的内容。使用
    top-k 采样，模型会过滤掉这些真正糟糕的选择，只考虑前 k 个最佳选项。通过剪掉长尾，我们会失去一点多样性，但质量会大幅提升。
- en: '![](../Images/9d2e8cbeca38c147e31be7b9657736f9.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9d2e8cbeca38c147e31be7b9657736f9.png)'
- en: Top-k sampling improves quality by keeping only the k best candidate tokens
    and throwing out the rest. Image by [Echo Lu](https://www.linkedin.com/in/echoxlu/).
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: Top-k 采样通过仅保留 k 个最佳候选词并丢弃其余词来提高质量。图片来源于[Echo Lu](https://www.linkedin.com/in/echoxlu/)。
- en: 'Top-k sampling is a way to have your cake and eat it too: It gets you the diversity
    you need at a smaller cost to quality than with temperature alone. Since this
    technique is so wildly effective, it has inspired many variants.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: Top-k 采样是一种“既要蛋糕又要吃蛋糕”的方法：它以比单独使用温度更小的成本获得所需的多样性。由于这一技术效果显著，它激发了许多变体。
- en: One common variant of top-k sampling is called top-p sampling, which is also
    known as nucleus sampling. Top-p sampling is a lot like top-k, except that it
    uses likelihood scores instead of token ranks to determine where it clips the
    tail. More specifically, it only considers those top-ranked tokens whose combined
    likelihood exceeds the threshold p, throwing out the rest.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: Top-k 采样的一种常见变体叫做 top-p 采样，也称为核采样。Top-p 采样与 top-k 非常相似，只是它使用概率得分而不是词汇排名来确定剪切尾部的标准。更具体地说，它只考虑那些排名前列的、其组合概率超过阈值
    p 的词汇，丢弃其余词汇。
- en: The power of top-p sampling compared to top-k sampling becomes evident when
    there are many poor or mediocre continuations. Suppose, for example, that there
    are only a handful of good picks for the next token, and there are dozens that
    just vaguely make sense. If we were using top-k sampling with k=25, we would be
    considering many poor continuations. In contrast, if we used top-p sampling to
    filter out the bottom 10% of the probability distribution, we might only consider
    those good tokens while filtering out the rest.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 当存在许多质量低劣或平庸的延续时，与top-k抽样相比，top-p抽样的优势变得明显。例如，假设下一个标记只有几个好的选择，而有数十个模糊合理的选择。如果我们使用k=25的top-k抽样，我们将考虑许多质量低劣的延续。相反，如果我们使用top-p抽样来过滤掉概率分布的底部10%，我们可能只会考虑那些好的标记，同时过滤掉其余的标记。
- en: In practice, top-p sampling tends to give better results compared to top-k sampling.
    By focusing on the cumulative likelihood, it adapts to the context of the input
    and provides a more flexible cut-off. So, in conclusion, top-p and top-k sampling
    can both be used at non-zero temperatures to capture diversity at a lower quality
    cost, but top-p sampling usually does it better.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，与top-k抽样相比，top-p抽样通常能够产生更好的结果。通过关注累积概率，它能够适应输入的上下文并提供更灵活的截断。因此，总之，top-p和top-k抽样都可以在非零温度下使用，以在较低的质量成本下捕捉多样性，但通常top-p抽样做得更好。
- en: 'Tip: For both of these settings, lower value = more filtering. At zero, they
    will filter out all but the top-ranked token, which has the same effect as setting
    the temperature to zero. So please use these parameters, be aware that setting
    them too low will give up all of your diversity.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 提示：对于这两个设置，较低的值=更多过滤。当值为零时，它们将过滤掉除排名第一的标记以外的所有标记，这与将温度设置为零具有相同效果。因此，请使用这些参数时，请注意不要将它们设置得太低，否则会损失所有的多样性。
- en: '*TLDR: Top-k and top-p increase quality at only a small cost to diversity.
    They achieve this by removing the worst token choices before random sampling.*'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '*TLDR：Top-k和top-p在只付出较小代价以增加质量的情况下提高质量。它们通过在随机抽样之前移除最差的标记选择来实现这一点。*'
- en: Frequency and Presence Penalties
  id: totrans-55
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 频率和存在惩罚
- en: 'We have just two more parameters to discuss before we start to wrap things
    up: The frequency and presence penalties. These parameters are — big surprise—
    yet another way to navigate the quality-diversity tradeoff. But while the temperature
    parameter achieves diversity by adding randomness to the token sampling procedure,
    the frequency and presence penalties add diversity by penalizing the reuse of
    tokens that have already occurred in the text. This makes the sampling of old
    and overused tokens less likely, influencing the model to make more novel token
    choices.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始总结之前，我们只需讨论另外两个参数：频率和存在惩罚。这些参数——大惊小怪——是导航质量多样性权衡的又一种方式。虽然温度参数通过在标记抽样过程中添加随机性来实现多样性，频率和存在惩罚则通过对已经在文本中出现过的标记施加惩罚来增加多样性。这使得旧的和过度使用的标记的抽样变得不太可能，影响模型进行更新颖的标记选择。
- en: The frequency penalty adds a penalty to a token for each time it has occurred
    in the text. This discourages repeated use of the same tokens/words/phrases and
    also has the side effect of causing the model to discuss more diverse subject
    matter and change topics more often. On the other hand, the presence penalty is
    a flat penalty that is applied if a token has already occurred in the text. This
    causes the model to introduce more new tokens/words/phrases, which causes it to
    discuss more diverse subject matter and change topics more often without significantly
    discouraging the repetition of frequently used words.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 频率惩罚为每次标记在文本中出现都添加一定的惩罚。这样做可以减少重复使用相同的标记/词语/短语，同时也会导致模型讨论更多样化的主题并更频繁地更换话题。另一方面，存在惩罚是一种固定的惩罚，如果一个标记已经在文本中出现过，则会应用该惩罚。这使得模型引入更多新的标记/词语/短语，导致它讨论更多样化的主题并更频繁地更换话题，而且不会显著地抑制经常使用的词语的重复出现。
- en: Much like temperature, the frequency and presence penalties lead us away from
    the “best” possible answer and toward a more creative one. But instead of doing
    this with randomness, they add targeted penalties that are carefully calculated
    to inject diversity into the answer. On some of those rare tasks requiring a non-zero
    temperature (when you require many answers to the same prompt), you might also
    consider adding a small frequency or presence penalty to the mix to boost creativity.
    But for prompts having just one right answer that you want to find in just one
    try, your odds of success are highest when you set all of these parameters to
    zero.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于温度，频率和存在惩罚使我们远离“最佳”答案，趋向于更具创意的答案。但不同的是，它们不是通过随机性实现的，而是通过精心计算的有针对性的惩罚来注入多样性。在一些少见的需要非零温度的任务中（当你需要对同一提示获得多个答案时），你也可以考虑加入小幅的频率或存在惩罚，以提升创意。但对于那些只希望找到唯一正确答案的提示，在一次尝试中设置所有这些参数为零时，你成功的机会最高。
- en: 'As a rule, when there is one right answer, and you are asking just one time,
    you should set the frequency and presence penalties to zero. But what if there
    are many right answers, such as in text summarization? In this case, you have
    a little discretion. If you find a model’s outputs boring, uncreative, repetitive,
    or limited in scope, judicious application of the frequency or presence penalties
    could be a good way to spice things up. But our final suggestion for these parameters
    is the same as for temperature: **When in doubt, choose zero!**'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，当只有一个正确答案，并且你只问一次时，应该将频率和存在惩罚设置为零。但如果有多个正确答案，比如在文本摘要中呢？在这种情况下，你有一点自由裁量权。如果你发现模型的输出无聊、缺乏创意、重复或范围有限，合理应用频率或存在惩罚可能是个不错的方法来增加趣味。但对于这些参数，我们的最终建议和温度参数一样：**当有疑问时，选择零！**
- en: We should note that while temperature and frequency/presence penalties both
    add diversity to the model’s responses, the kind of diversity that they add is
    not the same. The frequency/presence penalties increase the diversity *within
    a single response.* This means that a response will have more distinct words,
    phrases, topics, and subject matters than it would have without these penalties.
    But when you pass the same prompt twice, you are not more likely to get two different
    answers. This is in contrast with temperature, which increases diversity *between
    responses:* At higher temperatures, you will get a more diverse range of answers
    when passing the same prompt to the model many times.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该注意，虽然温度和频率/存在惩罚都能增加模型响应的多样性，但它们增加的多样性种类并不相同。频率/存在惩罚增加了*单次响应内的多样性*。这意味着，一个响应将具有比没有这些惩罚时更多的不同词汇、短语、主题和学科。但当你两次输入相同的提示时，不会更容易得到两个不同的答案。这与温度形成对比，温度增加了*响应之间的多样性*：在较高的温度下，当多次将相同提示输入模型时，你会得到更为多样化的回答。
- en: I like to refer to this distinction as within-response diversity vs. between-response
    diversity. The temperature parameter adds both within-response AND between-response
    diversity, while the frequency/presence penalties add only within-response diversity.
    So, when we need diversity, our choice of parameters should depend on the kind
    of diversity we need.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 我喜欢将这种区分称为响应内多样性与响应间多样性。温度参数增加了响应内和响应间的多样性，而频率/存在惩罚仅增加响应内的多样性。因此，当我们需要多样性时，我们对参数的选择应取决于我们需要的多样性种类。
- en: '*TLDR: The frequency and presence penalties increase the diversity of subject
    matters discussed by a model and make it change topics more often. The frequency
    penalty also increases diversity of word choice by reducing the repetition of
    words and phrases.*'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '*总结：频率和存在惩罚增加了模型讨论的主题多样性，并使其更频繁地更换话题。频率惩罚还通过减少词汇和短语的重复来增加词汇选择的多样性。*'
- en: The Parameter-Tuning Cheat Sheet
  id: totrans-63
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参数调整备忘单
- en: This section is intended as a practical guide for choosing your model’s input
    parameters. We first provide some hard-and-fast rules for deciding which values
    to set to zero. Then, we give some tips to help you find the right values for
    your non-zero parameters.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 本节旨在作为选择模型输入参数的实用指南。我们首先提供一些明确的规则来决定哪些值应设置为零。然后，我们给出一些建议，帮助你找到适合非零参数的正确值。
- en: I strongly encourage you to use this cheat sheet when choosing your input parameters.
    Go ahead and bookmark this page now so you don’t lose it!
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 我强烈建议你在选择输入参数时使用这份备忘单。现在就去收藏此页面，以免丢失！
- en: 'Rules for setting parameters to zero:'
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将参数设置为零的规则：
- en: 'Temperature:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 温度：
- en: 'For a **single answer** per prompt: Zero.'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个提示**一个答案**：零。
- en: 'For **many answers** per prompt: Non-zero.'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个提示**多个答案**：非零。
- en: 'Frequency and Presence Penalties:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 频率和存在惩罚：
- en: When there is **one correct answer:** Zero.
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当有**一个正确答案**时：零。
- en: When there are **many correct answers:** Optional.
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当有**多个正确答案**时：可选。
- en: 'Top-p/Top-k:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: Top-p/Top-k：
- en: 'With **zero temperature**: The output is not affected.'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在**零温度**下：输出不受影响。
- en: 'With **non-zero temperature**: Non-zero.'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在**非零温度**下：非零。
- en: If your language model has additional parameters not listed here, it is always
    okay to leave them at their default values.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你的语言模型有其他未列出的参数，保持其默认值也是可以的。
- en: 'Tips for tuning the non-zero parameters:'
  id: totrans-77
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 调整非零参数的提示：
- en: Make a list of those parameters that should have non-zero values, and then go
    to a playground and fiddle around with some test prompts to see what works. ***But
    if the rules above say to leave a parameter at zero, leave it at zero!***
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 列出应该具有非零值的那些参数，然后去试验场中尝试一些测试提示以查看效果。***但如果上述规则要求将参数保持在零，则保持在零！***
- en: 'Tuning temperature/top-p/top-k:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 调整温度/top-p/top-k：
- en: For more diversity/randomness, increase the temperature.
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了更多的多样性/随机性，增加温度。
- en: With non-zero temperatures, start with a top-p around 0.95 (or top-k around
    250) and lower it as needed.
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在非零温度下，从 top-p 约 0.95（或 top-k 约 250）开始，根据需要逐渐降低。
- en: 'Troubleshooting:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 故障排除：
- en: If there is too much nonsense, garbage, or hallucination, decrease temperature
    and/or decrease top-p/top-k.
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果出现过多的废话、垃圾或幻觉，降低温度和/或减少 top-p/top-k。
- en: If the temperature is high and diversity is low, increase top-p/top-k.
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果温度高而多样性低，增加 top-p/top-k。
- en: 'Tip: While some interfaces allow you to use top-p and top-k at the same time,
    we prefer to keep things simple by choosing one or the other. Top-k is easier
    to use and understand, but top-p is often more effective.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 提示：虽然一些界面允许你同时使用 top-p 和 top-k，但我们倾向于选择其中之一以保持简单。Top-k 更易于使用和理解，但 top-p 通常更有效。
- en: 'Tuning frequency penalty and presence penalty:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 调整频率惩罚和存在惩罚：
- en: For more diverse topics and subject matters, increase the presence penalty.
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了更多的多样化主题和内容，增加存在惩罚。
- en: For more diverse and less repetitive language, increase the frequency penalty.
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了更丰富且不重复的语言，增加频率惩罚。
- en: 'Troubleshooting:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 故障排除：
- en: If the outputs seem scattered and change topics too quickly, decrease the presence
    penalty.
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果输出显得零散且主题变化过快，减少存在惩罚。
- en: If there are too many new and unusual words, or if the presence penalty is set
    to zero and you still get too many topic changes, decrease the frequency penalty.
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果有太多新词和不寻常的词，或者存在惩罚设置为零但仍然有太多主题变化，减少频率惩罚。
- en: '*TLDR: You can use this section as a cheat sheet for tuning language models.
    You are* ***definitely*** *going to**forget these rules, so bookmark this page
    and use it later as a reference.*'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '*TLDR：你可以将此部分作为调整语言模型的备忘单。你* ***肯定*** *会**忘记这些规则，因此请收藏此页面，并在以后作为参考使用。*'
- en: Wrapping up
  id: totrans-93
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: While there are limitless ways to define a token sampling strategy, the parameters
    we’ve discussed here — temperature, top-k, top-p, frequency penalty, and presence
    penalty — are among the most commonly used. These are the parameters that you
    can expect to find in models like Claude, Llama, and the GPT series. In this article,
    we have shown that all of these parameters are really just here to help us navigate
    the quality-diversity tradeoff.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然定义令牌采样策略的方法无穷无尽，但我们讨论过的参数——温度、top-k、top-p、频率惩罚和存在惩罚——是最常用的参数。这些参数是你可以在 Claude、Llama
    和 GPT 系列等模型中找到的。本文展示了所有这些参数实际上只是帮助我们导航质量与多样性权衡的工具。
- en: 'Before we go, there is one last input parameter to mention: maximum token length.
    The maximum token length is just the cutoff where the model stops printing its
    answer, even if it isn’t finished. After that complex discussion, we hope this
    one is self-explanatory. 🙂'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们结束之前，还有一个最后的输入参数需要提及：最大令牌长度。最大令牌长度就是模型停止打印答案的截止点，即使答案未完成。在复杂的讨论之后，我们希望这个参数是不言自明的。🙂
- en: As we move further in this series, we’ll do more deep dives into topics such
    as prompt engineering, choosing the right language model for your use case, and
    more! I will also show some real-world use cases from my work as a data analysis
    consultant at Megaputer Intelligence. Stay tuned for more insights, and happy
    modeling!
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 随着我们在这一系列中的深入，我们将更深入地探讨诸如提示工程、为你的使用案例选择合适的语言模型等主题！我还会展示一些来自我在Megaputer Intelligence担任数据分析顾问时的实际应用案例。敬请期待更多见解，祝建模愉快！
- en: '*TLDR: When in doubt, set the temperature, frequency penalty, and presence
    penalty to zero. If that doesn’t work for you, reference the cheat sheet above.*'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '*TLDR: 如果有疑问，将温度、频率惩罚和存在惩罚设置为零。如果这样做对你不起作用，请参考上面的备忘单。*'
