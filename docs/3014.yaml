- en: Mastering Language Models
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æŒæ¡è¯­è¨€æ¨¡å‹
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/mastering-language-models-32e1d891511a?source=collection_archive---------5-----------------------#2023-10-03](https://towardsdatascience.com/mastering-language-models-32e1d891511a?source=collection_archive---------5-----------------------#2023-10-03)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://towardsdatascience.com/mastering-language-models-32e1d891511a?source=collection_archive---------5-----------------------#2023-10-03](https://towardsdatascience.com/mastering-language-models-32e1d891511a?source=collection_archive---------5-----------------------#2023-10-03)
- en: Navigating the quality-diversity tradeoff with temperature, top-p, top-k, and
    more
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: é€šè¿‡æ¸©åº¦ã€top-pã€top-kç­‰æ¥å¹³è¡¡è´¨é‡ä¸å¤šæ ·æ€§
- en: '[](https://medium.com/@samuel.montgomery59?source=post_page-----32e1d891511a--------------------------------)[![Samuel
    Montgomery](../Images/52f12c797b53706ad1039459238ece44.png)](https://medium.com/@samuel.montgomery59?source=post_page-----32e1d891511a--------------------------------)[](https://towardsdatascience.com/?source=post_page-----32e1d891511a--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----32e1d891511a--------------------------------)
    [Samuel Montgomery](https://medium.com/@samuel.montgomery59?source=post_page-----32e1d891511a--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@samuel.montgomery59?source=post_page-----32e1d891511a--------------------------------)[![Samuel
    Montgomery](../Images/52f12c797b53706ad1039459238ece44.png)](https://medium.com/@samuel.montgomery59?source=post_page-----32e1d891511a--------------------------------)[](https://towardsdatascience.com/?source=post_page-----32e1d891511a--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----32e1d891511a--------------------------------)
    [Samuel Montgomery](https://medium.com/@samuel.montgomery59?source=post_page-----32e1d891511a--------------------------------)'
- en: Â·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Â·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fbf5cf7332a65&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmastering-language-models-32e1d891511a&user=Samuel+Montgomery&userId=bf5cf7332a65&source=post_page-bf5cf7332a65----32e1d891511a---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----32e1d891511a--------------------------------)
    Â·12 min readÂ·Oct 3, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F32e1d891511a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmastering-language-models-32e1d891511a&user=Samuel+Montgomery&userId=bf5cf7332a65&source=-----32e1d891511a---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[å…³æ³¨](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fbf5cf7332a65&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmastering-language-models-32e1d891511a&user=Samuel+Montgomery&userId=bf5cf7332a65&source=post_page-bf5cf7332a65----32e1d891511a---------------------post_header-----------)
    å‘è¡¨äº [Towards Data Science](https://towardsdatascience.com/?source=post_page-----32e1d891511a--------------------------------)
    Â· 12 min read Â· 2023å¹´10æœˆ3æ—¥ [](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F32e1d891511a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmastering-language-models-32e1d891511a&user=Samuel+Montgomery&userId=bf5cf7332a65&source=-----32e1d891511a---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F32e1d891511a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmastering-language-models-32e1d891511a&source=-----32e1d891511a---------------------bookmark_footer-----------)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F32e1d891511a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmastering-language-models-32e1d891511a&source=-----32e1d891511a---------------------bookmark_footer-----------)'
- en: If you have ever used a language model through a playground or an API, you may
    have been asked to choose some input parameters. For many of us, the meaning of
    these parameters (and the right way to use them) is less than totally clear.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä½ æ›¾ç»é€šè¿‡æ“æ§å°æˆ–APIä½¿ç”¨è¿‡è¯­è¨€æ¨¡å‹ï¼Œä½ å¯èƒ½è¢«è¦æ±‚é€‰æ‹©ä¸€äº›è¾“å…¥å‚æ•°ã€‚å¯¹äºæˆ‘ä»¬è®¸å¤šäººæ¥è¯´ï¼Œè¿™äº›å‚æ•°çš„æ„ä¹‰ï¼ˆä»¥åŠæ­£ç¡®çš„ä½¿ç”¨æ–¹æ³•ï¼‰å¯èƒ½å¹¶ä¸ååˆ†æ¸…æ¥šã€‚
- en: '![](../Images/b9bdc5e91439bacb53807788c292484d.png)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b9bdc5e91439bacb53807788c292484d.png)'
- en: A screenshot showing parameter selection in the SillyTavern interface. Image
    by the author.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€å¼ æ˜¾ç¤º SillyTavern ç•Œé¢ä¸­å‚æ•°é€‰æ‹©çš„æˆªå›¾ã€‚å›¾ç‰‡ç”±ä½œè€…æä¾›ã€‚
- en: This article will teach you how to use these parameters to control hallucinations,
    inject creativity into your modelâ€™s outputs, and make other fine-grained adjustments
    to optimize behavior. Much like prompt engineering, input parameter tuning can
    get your model running at 110%.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ç¯‡æ–‡ç« å°†æ•™ä½ å¦‚ä½•ä½¿ç”¨è¿™äº›å‚æ•°æ¥æ§åˆ¶å¹»è§‰ï¼Œæ³¨å…¥åˆ›é€ åŠ›åˆ°æ¨¡å‹è¾“å‡ºä¸­ï¼Œå¹¶è¿›è¡Œå…¶ä»–ç»†å¾®çš„è°ƒæ•´ä»¥ä¼˜åŒ–è¡Œä¸ºã€‚å°±åƒæç¤ºå·¥ç¨‹ä¸€æ ·ï¼Œè¾“å…¥å‚æ•°è°ƒæ•´å¯ä»¥è®©ä½ çš„æ¨¡å‹è¾¾åˆ°110%çš„æ€§èƒ½ã€‚
- en: By the end of this article, youâ€™ll be an expert on five essential input parameters
    â€” temperature, top-p, top-k, frequency penalty, and presence penalty. Youâ€™ll also
    learn how each of these parameters helps us navigate the quality-diversity tradeoff.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬æ–‡ç»“æŸæ—¶ï¼Œä½ å°†æˆä¸ºäº”ä¸ªé‡è¦è¾“å…¥å‚æ•°çš„ä¸“å®¶â€”â€”æ¸©åº¦ã€top-pã€top-kã€é¢‘ç‡æƒ©ç½šå’Œå­˜åœ¨æƒ©ç½šã€‚ä½ è¿˜å°†äº†è§£è¿™äº›å‚æ•°å¦‚ä½•å¸®åŠ©æˆ‘ä»¬åœ¨è´¨é‡å’Œå¤šæ ·æ€§ä¹‹é—´è¿›è¡Œæƒè¡¡ã€‚
- en: So, grab a coffee, and letâ€™s get started!
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥ï¼Œæ‹¿ä¸€æ¯å’–å•¡ï¼Œæˆ‘ä»¬å¼€å§‹å§ï¼
- en: Table of Contents
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç›®å½•
- en: Â· [Background](#5876)
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: Â· [èƒŒæ™¯](#5876)
- en: Â· [Quality, Diversity, and Temperature](#1164)
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: Â· [è´¨é‡ã€å¤šæ ·æ€§ä¸æ¸©åº¦](#1164)
- en: Â· [Top-k and Top-p](#a476)
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: Â· [Top-k å’Œ Top-p](#a476)
- en: Â· [Frequency and Presence Penalties](#e517)
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: Â· [é¢‘ç‡å’Œå­˜åœ¨æƒ©ç½š](#e517)
- en: Â· [The Parameter-Tuning Cheat Sheet](#c9f4)
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: Â· [å‚æ•°è°ƒæ•´å¤‡å¿˜å•](#c9f4)
- en: Â· [Wrapping up](#7610)
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: Â· [æ€»ç»“](#7610)
- en: Background
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: èƒŒæ™¯
- en: Before we get around to selecting our input parameters, we will need to go over
    some background information. Letâ€™s talk about how these models choose the words
    that they generate.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æˆ‘ä»¬å¼€å§‹é€‰æ‹©è¾“å…¥å‚æ•°ä¹‹å‰ï¼Œæˆ‘ä»¬éœ€è¦äº†è§£ä¸€äº›èƒŒæ™¯ä¿¡æ¯ã€‚è®©æˆ‘ä»¬æ¥è°ˆè°ˆè¿™äº›æ¨¡å‹æ˜¯å¦‚ä½•é€‰æ‹©ç”Ÿæˆçš„å•è¯çš„ã€‚
- en: 'To read a document, a language model breaks it down into a sequence of tokens.
    A token is just a small chunk of text that the model can easily understand: It
    could be a word, a syllable, or a character. For example, â€œMegaputer Intelligence
    Inc.â€ might be broken down into five tokens: [â€œMegaâ€, â€œputer â€, â€œIntelligenceâ€,
    â€œIncâ€, â€œ.â€].'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: è¦è¯»å–æ–‡æ¡£ï¼Œè¯­è¨€æ¨¡å‹å°†å…¶åˆ†è§£ä¸ºä¸€ç³»åˆ—æ ‡è®°ã€‚æ ‡è®°åªæ˜¯æ¨¡å‹å¯ä»¥è½»æ¾ç†è§£çš„å°å—æ–‡æœ¬ï¼šå®ƒå¯ä»¥æ˜¯ä¸€ä¸ªè¯ã€ä¸€ä¸ªéŸ³èŠ‚æˆ–ä¸€ä¸ªå­—ç¬¦ã€‚ä¾‹å¦‚ï¼Œâ€œMegaputer Intelligence
    Inc.â€å¯èƒ½ä¼šè¢«åˆ†è§£ä¸ºäº”ä¸ªæ ‡è®°ï¼š[â€œMegaâ€ï¼Œâ€œputerâ€ï¼Œâ€œIntelligenceâ€ï¼Œâ€œIncâ€ï¼Œâ€œ.â€]ã€‚
- en: Most language models we are familiar with operate by repeatedly generating the
    next token in a sequence. Each time the model wants to generate another token,
    it re-reads the entire sequence and predicts the token that should come next.
    This strategy is known as *autoregressive* generation.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ç†Ÿæ‚‰çš„å¤§å¤šæ•°è¯­è¨€æ¨¡å‹é€šè¿‡åå¤ç”Ÿæˆåºåˆ—ä¸­çš„ä¸‹ä¸€ä¸ªæ ‡è®°æ¥æ“ä½œã€‚æ¯å½“æ¨¡å‹æƒ³è¦ç”Ÿæˆå¦ä¸€ä¸ªæ ‡è®°æ—¶ï¼Œå®ƒä¼šé‡æ–°è¯»å–æ•´ä¸ªåºåˆ—ï¼Œå¹¶é¢„æµ‹ä¸‹ä¸€ä¸ªåº”è¯¥å‡ºç°çš„æ ‡è®°ã€‚è¿™ç§ç­–ç•¥è¢«ç§°ä¸º*è‡ªå›å½’*ç”Ÿæˆã€‚
- en: '![](../Images/ff9f86565a19ba6f422e13157cd7b039.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ff9f86565a19ba6f422e13157cd7b039.png)'
- en: Autoregressive generation of tokens by a language model. GIF by [Echo Lu](https://www.linkedin.com/in/echoxlu/),
    containing a modification of an image by [Annie Surla](https://developer.nvidia.com/blog/author/anniesurla/)
    from [NVIDIA](https://developer.nvidia.com/blog/how-to-get-better-outputs-from-your-large-language-model/).
    Modified with permission from owner.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: è¯­è¨€æ¨¡å‹çš„è‡ªå›å½’æ ‡è®°ç”Ÿæˆã€‚GIFç”±[Echo Lu](https://www.linkedin.com/in/echoxlu/)æä¾›ï¼ŒåŒ…å«ç”±[Annie
    Surla](https://developer.nvidia.com/blog/author/anniesurla/)ä¿®æ”¹çš„[NVIDIA](https://developer.nvidia.com/blog/how-to-get-better-outputs-from-your-large-language-model/)çš„å›¾åƒã€‚ç»ç‰ˆæƒæ‰€æœ‰è€…è®¸å¯ä¿®æ”¹ã€‚
- en: 'This explains why ChatGPT prints the words out one at a time: It is streaming
    the words to you as it writes them.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™è§£é‡Šäº†ä¸ºä»€ä¹ˆChatGPTä¸€æ¬¡è¾“å‡ºä¸€ä¸ªå•è¯ï¼šå®ƒåœ¨å†™å­—çš„æ—¶å€™æŠŠå•è¯æµå¼ä¼ è¾“ç»™ä½ ã€‚
- en: To choose the next token in the sequence, a language model first assigns a likelihood
    score to each token in its vocabulary. A token gets a high likelihood score if
    it is a good continuation of the text and a low likelihood score if it is a poor
    continuation of the text, as assessed by the model.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: è¦é€‰æ‹©åºåˆ—ä¸­çš„ä¸‹ä¸€ä¸ªæ ‡è®°ï¼Œè¯­è¨€æ¨¡å‹é¦–å…ˆä¸ºå…¶è¯æ±‡è¡¨ä¸­çš„æ¯ä¸ªæ ‡è®°åˆ†é…ä¸€ä¸ªå¯èƒ½æ€§åˆ†æ•°ã€‚å¦‚æœæ ‡è®°æ˜¯æ–‡æœ¬çš„è‰¯å¥½å»¶ç»­ï¼Œå®ƒä¼šè·å¾—ä¸€ä¸ªé«˜å¯èƒ½æ€§åˆ†æ•°ï¼›å¦‚æœæ ‡è®°æ˜¯æ–‡æœ¬çš„è¾ƒå·®å»¶ç»­ï¼Œå®ƒä¼šè·å¾—ä¸€ä¸ªä½å¯èƒ½æ€§åˆ†æ•°ï¼Œæ¨¡å‹ä¼šè¿›è¡Œè¯„ä¼°ã€‚
- en: '![](../Images/40490cd101aa2e34bdc072b88614741d.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/40490cd101aa2e34bdc072b88614741d.png)'
- en: A language model assigns likelihood scores to predict the next token in the
    sequence. Original image by [Annie Surla](https://developer.nvidia.com/blog/author/anniesurla/)
    from [NVIDIA](https://developer.nvidia.com/blog/how-to-get-better-outputs-from-your-large-language-model/),
    modified by [Echo Lu](https://www.linkedin.com/in/echoxlu/) with permission from
    owner.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: è¯­è¨€æ¨¡å‹åˆ†é…å¯èƒ½æ€§åˆ†æ•°ä»¥é¢„æµ‹åºåˆ—ä¸­çš„ä¸‹ä¸€ä¸ªæ ‡è®°ã€‚åŸå§‹å›¾åƒç”±[Annie Surla](https://developer.nvidia.com/blog/author/anniesurla/)æä¾›ï¼Œæ¥è‡ª[NVIDIA](https://developer.nvidia.com/blog/how-to-get-better-outputs-from-your-large-language-model/)ï¼Œç»[Echo
    Lu](https://www.linkedin.com/in/echoxlu/)ä¿®æ”¹ï¼Œç»è¿‡ç‰ˆæƒæ‰€æœ‰è€…è®¸å¯ã€‚
- en: After the likelihood scores are assigned, a token is chosen using a token sampling
    scheme that takes the likelihood scores into account. The token sampling scheme
    may incorporate some randomness so that the language model does not answer the
    same question in the same way every time. This randomness can be a nice feature
    in chatbots or other applications.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨åˆ†é…äº†å¯èƒ½æ€§åˆ†æ•°ä¹‹åï¼Œé€šè¿‡è€ƒè™‘å¯èƒ½æ€§åˆ†æ•°çš„æ ‡è®°é‡‡æ ·æ–¹æ¡ˆæ¥é€‰æ‹©æ ‡è®°ã€‚æ ‡è®°é‡‡æ ·æ–¹æ¡ˆå¯èƒ½ä¼šåŒ…å«ä¸€äº›éšæœºæ€§ï¼Œä»¥ä¾¿è¯­è¨€æ¨¡å‹ä¸ä¼šæ¯æ¬¡ä»¥ç›¸åŒçš„æ–¹å¼å›ç­”ç›¸åŒçš„é—®é¢˜ã€‚è¿™ç§éšæœºæ€§åœ¨èŠå¤©æœºå™¨äººæˆ–å…¶ä»–åº”ç”¨ä¸­å¯èƒ½æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„ç‰¹æ€§ã€‚
- en: '*TLDR: Language models break down text into tokens, predict the next token
    in the sequence, and mix in some randomness. Repeat as needed to generate language.*'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: Quality, Diversity, and Temperature
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: But why would we ever want to pick the second-best token, the third-best token,
    or any other token besides the best, for that matter? Wouldnâ€™t we want to pick
    the best token (the one with the highest likelihood score) every time? Often,
    we do. But if we picked the best answer every time, we would get the same answer
    every time. If we want a diverse range of answers, we may have to give up some
    quality to get it. This sacrifice of quality for diversity is called a quality-diversity
    tradeoff.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: With this in mind, *temperature* tells the machine how to navigate the quality-diversity
    tradeoff. Low temperatures mean more quality, while high temperatures mean more
    diversity. When the temperature is set to zero, the model always samples the token
    with the highest likelihood score, resulting in zero diversity between queries,
    but ensuring that we always pick the highest quality continuation as assessed
    by the model.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: Quite often, we will want to set the temperature to zero. As a rule, you should
    always choose temperature zero for any prompt that you will pass to the model
    only once, as this is most likely to get you a good answer. In my job as a data
    analyst, I set the temperature to zero for entity extraction, fact extraction,
    sentiment analysis, and most other standard tasks.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: At higher temperatures, we see more garbage and hallucinations, less coherence,
    and lower quality of responses on average, but also more creativity and diversity
    of answers. We recommend that you should *only* use non-zero temperatures when
    you want to ask the same question twice and get two different answers.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/72a0df19da4637d3f2f8b985de25958c.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
- en: Higher temperatures bring diversity, creativity, and multiplicity of answers
    but also add garbage, incoherence, and hallucinations. Image by [Echo Lu](https://www.linkedin.com/in/echoxlu/).
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: 'And why would we ever want two different answers to the same prompt? In some
    cases, having many answers to one prompt can be useful. For example, there is
    a technique in which we generate multiple answers to a prompt and keep only the
    best, which often produces better results than a single query at temperature zero.
    Another use case is synthetic data generation: We want many diverse synthetic
    data points, not just one data point thatâ€™s really good. We may discuss these
    use cases (and others) in later articles, but more often than not, we want only
    one answer per prompt. **When in doubt, choose temperature zero!**'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: It is important to note that while temperature zero should *in theory* produce
    the same answer every time, this may not be true in practice! This is because
    the GPUs the model is running on can be prone to small miscalculations, such as
    rounding errors. These errors introduce a low level of randomness into the calculations,
    even at temperature zero. Since changing one token in a text can significantly
    alter its meaning, a single error may cause a cascade of different token choices
    later in the text, resulting in an almost totally different output. But rest assured
    that this usually has a negligible impact on quality. We only mention it so that
    youâ€™re not surprised when you get some randomness at temperatures zero.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œè™½ç„¶ç†è®ºä¸Šæ¸©åº¦ä¸ºé›¶åº”è¯¥æ¯æ¬¡äº§ç”Ÿç›¸åŒçš„ç­”æ¡ˆï¼Œä½†å®é™…æƒ…å†µå¯èƒ½å¹¶éå¦‚æ­¤ï¼è¿™æ˜¯å› ä¸ºæ¨¡å‹è¿è¡Œçš„ GPU å¯èƒ½ä¼šå‡ºç°å°çš„è®¡ç®—è¯¯å·®ï¼Œæ¯”å¦‚å››èˆäº”å…¥é”™è¯¯ã€‚è¿™äº›é”™è¯¯ä¼šåœ¨è®¡ç®—ä¸­å¼•å…¥å¾®å°çš„éšæœºæ€§ï¼Œå³ä½¿åœ¨æ¸©åº¦ä¸ºé›¶æ—¶ä¹Ÿæ˜¯å¦‚æ­¤ã€‚ç”±äºåœ¨æ–‡æœ¬ä¸­æ”¹å˜ä¸€ä¸ªè¯å¯èƒ½ä¼šæ˜¾è‘—æ”¹å˜å…¶å«ä¹‰ï¼Œå› æ­¤ä¸€ä¸ªé”™è¯¯å¯èƒ½ä¼šå¼•èµ·åç»­æ–‡æœ¬ä¸­ä¸åŒè¯æ±‡çš„è¿é”ååº”ï¼Œä»è€Œå¯¼è‡´å‡ ä¹å®Œå…¨ä¸åŒçš„è¾“å‡ºã€‚ä½†è¯·æ”¾å¿ƒï¼Œè¿™é€šå¸¸å¯¹è´¨é‡çš„å½±å“å¾®ä¹å…¶å¾®ã€‚æˆ‘ä»¬æåˆ°è¿™ä¸€ç‚¹æ˜¯ä¸ºäº†è®©ä½ åœ¨æ¸©åº¦ä¸ºé›¶æ—¶é‡åˆ°ä¸€äº›éšæœºæ€§æ—¶ä¸ä¼šæ„Ÿåˆ°æƒŠè®¶ã€‚
- en: There are many more ways than temperature alone to navigate the quality-diversity
    tradeoff. In the next section, we will discuss some modifications to the temperature
    sampling technique. But if you are content with using temperature zero, feel free
    to skip it for now. You may rest soundly knowing that your choice of these parameters
    at temperature zero will not affect your answer.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: é™¤äº†æ¸©åº¦ï¼Œè¿˜æœ‰å¾ˆå¤šå…¶ä»–æ–¹æ³•å¯ä»¥åœ¨è´¨é‡å’Œå¤šæ ·æ€§ä¹‹é—´å–å¾—å¹³è¡¡ã€‚åœ¨ä¸‹ä¸€èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†è®¨è®ºä¸€äº›å¯¹æ¸©åº¦é‡‡æ ·æŠ€æœ¯çš„ä¿®æ”¹ã€‚ä½†å¦‚æœä½ å¯¹ä½¿ç”¨æ¸©åº¦ä¸ºé›¶æ„Ÿåˆ°æ»¡æ„ï¼Œå¯ä»¥æš‚æ—¶è·³è¿‡è¿™éƒ¨åˆ†ã€‚ä½ å¯ä»¥æ”¾å¿ƒï¼Œä½ é€‰æ‹©çš„è¿™äº›å‚æ•°åœ¨æ¸©åº¦ä¸ºé›¶æ—¶ä¸ä¼šå½±å“ä½ çš„ç­”æ¡ˆã€‚
- en: '*TLDR: Temperature increases diversity but decreases quality by adding randomness
    to the modelâ€™s outputs.*'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '*æ€»ç»“ï¼šæ¸©åº¦å¢åŠ äº†å¤šæ ·æ€§ï¼Œä½†é€šè¿‡å°†éšæœºæ€§æ·»åŠ åˆ°æ¨¡å‹è¾“å‡ºä¸­è€Œé™ä½äº†è´¨é‡ã€‚*'
- en: Top-k and Top-p
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Top-k å’Œ Top-p
- en: 'One common way to tweak our token-sampling formula is called top-k sampling.
    Top-k sampling is a lot like ordinary temperature sampling, except that the lowest
    likelihood tokens are excluded from being picked: Only the â€œtop kâ€ best choices
    are considered, which is where we get the name. The advantage of this method is
    that it stops us from picking truly bad tokens.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: è°ƒæ•´æˆ‘ä»¬çš„è¯æ±‡é‡‡æ ·å…¬å¼çš„ä¸€ç§å¸¸è§æ–¹æ³•å«åš top-k é‡‡æ ·ã€‚Top-k é‡‡æ ·ä¸æ™®é€šçš„æ¸©åº¦é‡‡æ ·ç±»ä¼¼ï¼Œåªæ˜¯æ’é™¤äº†æœ€ä½æ¦‚ç‡çš„è¯æ±‡ï¼šåªæœ‰â€œå‰ kâ€ä¸ªæœ€ä½³é€‰æ‹©ä¼šè¢«è€ƒè™‘ï¼Œè¿™ä¹Ÿæ˜¯åå­—çš„ç”±æ¥ã€‚è¿™ä¸ªæ–¹æ³•çš„ä¼˜åŠ¿åœ¨äºå®ƒé˜²æ­¢æˆ‘ä»¬é€‰æ‹©çœŸæ­£ç³Ÿç³•çš„è¯æ±‡ã€‚
- en: Letâ€™s suppose, for example, that we are trying to make a completion for â€œThe
    sun rises in theâ€¦â€ Then, without top-k sampling, the model considers every token
    in its vocabulary as a possible continuation of the sequence. Then there is some
    non-zero chance that it will write something ridiculous like â€œThe sun rises in
    the refrigerator.â€ With top-k sampling, the model filters out these truly bad
    picks and only considers the k best options. By clipping off the long tail, we
    lose a little diversity, but our quality shoots way up.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: æ¯”å¦‚è¯´ï¼Œæˆ‘ä»¬è¯•å›¾ä¸ºâ€œå¤ªé˜³åœ¨â€¦â€¦å‡èµ·â€ç”Ÿæˆä¸€ä¸ªç»­å†™ã€‚å¦‚æœä¸ä½¿ç”¨ top-k é‡‡æ ·ï¼Œæ¨¡å‹ä¼šè€ƒè™‘è¯æ±‡è¡¨ä¸­çš„æ¯ä¸€ä¸ªè¯ä½œä¸ºåºåˆ—çš„å¯èƒ½å»¶ç»­ã€‚è¿™æ ·å¯èƒ½ä¼šæœ‰éé›¶çš„å‡ ç‡ç”Ÿæˆè¯¸å¦‚â€œå¤ªé˜³åœ¨å†°ç®±é‡Œå‡èµ·â€è¿™æ ·è’è°¬çš„å†…å®¹ã€‚ä½¿ç”¨
    top-k é‡‡æ ·ï¼Œæ¨¡å‹ä¼šè¿‡æ»¤æ‰è¿™äº›çœŸæ­£ç³Ÿç³•çš„é€‰æ‹©ï¼Œåªè€ƒè™‘å‰ k ä¸ªæœ€ä½³é€‰é¡¹ã€‚é€šè¿‡å‰ªæ‰é•¿å°¾ï¼Œæˆ‘ä»¬ä¼šå¤±å»ä¸€ç‚¹å¤šæ ·æ€§ï¼Œä½†è´¨é‡ä¼šå¤§å¹…æå‡ã€‚
- en: '![](../Images/9d2e8cbeca38c147e31be7b9657736f9.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9d2e8cbeca38c147e31be7b9657736f9.png)'
- en: Top-k sampling improves quality by keeping only the k best candidate tokens
    and throwing out the rest. Image by [Echo Lu](https://www.linkedin.com/in/echoxlu/).
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: Top-k é‡‡æ ·é€šè¿‡ä»…ä¿ç•™ k ä¸ªæœ€ä½³å€™é€‰è¯å¹¶ä¸¢å¼ƒå…¶ä½™è¯æ¥æé«˜è´¨é‡ã€‚å›¾ç‰‡æ¥æºäº[Echo Lu](https://www.linkedin.com/in/echoxlu/)ã€‚
- en: 'Top-k sampling is a way to have your cake and eat it too: It gets you the diversity
    you need at a smaller cost to quality than with temperature alone. Since this
    technique is so wildly effective, it has inspired many variants.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: Top-k é‡‡æ ·æ˜¯ä¸€ç§â€œæ—¢è¦è›‹ç³•åˆè¦åƒè›‹ç³•â€çš„æ–¹æ³•ï¼šå®ƒä»¥æ¯”å•ç‹¬ä½¿ç”¨æ¸©åº¦æ›´å°çš„æˆæœ¬è·å¾—æ‰€éœ€çš„å¤šæ ·æ€§ã€‚ç”±äºè¿™ä¸€æŠ€æœ¯æ•ˆæœæ˜¾è‘—ï¼Œå®ƒæ¿€å‘äº†è®¸å¤šå˜ä½“ã€‚
- en: One common variant of top-k sampling is called top-p sampling, which is also
    known as nucleus sampling. Top-p sampling is a lot like top-k, except that it
    uses likelihood scores instead of token ranks to determine where it clips the
    tail. More specifically, it only considers those top-ranked tokens whose combined
    likelihood exceeds the threshold p, throwing out the rest.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: Top-k é‡‡æ ·çš„ä¸€ç§å¸¸è§å˜ä½“å«åš top-p é‡‡æ ·ï¼Œä¹Ÿç§°ä¸ºæ ¸é‡‡æ ·ã€‚Top-p é‡‡æ ·ä¸ top-k éå¸¸ç›¸ä¼¼ï¼Œåªæ˜¯å®ƒä½¿ç”¨æ¦‚ç‡å¾—åˆ†è€Œä¸æ˜¯è¯æ±‡æ’åæ¥ç¡®å®šå‰ªåˆ‡å°¾éƒ¨çš„æ ‡å‡†ã€‚æ›´å…·ä½“åœ°è¯´ï¼Œå®ƒåªè€ƒè™‘é‚£äº›æ’åå‰åˆ—çš„ã€å…¶ç»„åˆæ¦‚ç‡è¶…è¿‡é˜ˆå€¼
    p çš„è¯æ±‡ï¼Œä¸¢å¼ƒå…¶ä½™è¯æ±‡ã€‚
- en: The power of top-p sampling compared to top-k sampling becomes evident when
    there are many poor or mediocre continuations. Suppose, for example, that there
    are only a handful of good picks for the next token, and there are dozens that
    just vaguely make sense. If we were using top-k sampling with k=25, we would be
    considering many poor continuations. In contrast, if we used top-p sampling to
    filter out the bottom 10% of the probability distribution, we might only consider
    those good tokens while filtering out the rest.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: å½“å­˜åœ¨è®¸å¤šè´¨é‡ä½åŠ£æˆ–å¹³åº¸çš„å»¶ç»­æ—¶ï¼Œä¸top-kæŠ½æ ·ç›¸æ¯”ï¼Œtop-pæŠ½æ ·çš„ä¼˜åŠ¿å˜å¾—æ˜æ˜¾ã€‚ä¾‹å¦‚ï¼Œå‡è®¾ä¸‹ä¸€ä¸ªæ ‡è®°åªæœ‰å‡ ä¸ªå¥½çš„é€‰æ‹©ï¼Œè€Œæœ‰æ•°åä¸ªæ¨¡ç³Šåˆç†çš„é€‰æ‹©ã€‚å¦‚æœæˆ‘ä»¬ä½¿ç”¨k=25çš„top-kæŠ½æ ·ï¼Œæˆ‘ä»¬å°†è€ƒè™‘è®¸å¤šè´¨é‡ä½åŠ£çš„å»¶ç»­ã€‚ç›¸åï¼Œå¦‚æœæˆ‘ä»¬ä½¿ç”¨top-pæŠ½æ ·æ¥è¿‡æ»¤æ‰æ¦‚ç‡åˆ†å¸ƒçš„åº•éƒ¨10%ï¼Œæˆ‘ä»¬å¯èƒ½åªä¼šè€ƒè™‘é‚£äº›å¥½çš„æ ‡è®°ï¼ŒåŒæ—¶è¿‡æ»¤æ‰å…¶ä½™çš„æ ‡è®°ã€‚
- en: In practice, top-p sampling tends to give better results compared to top-k sampling.
    By focusing on the cumulative likelihood, it adapts to the context of the input
    and provides a more flexible cut-off. So, in conclusion, top-p and top-k sampling
    can both be used at non-zero temperatures to capture diversity at a lower quality
    cost, but top-p sampling usually does it better.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å®è·µä¸­ï¼Œä¸top-kæŠ½æ ·ç›¸æ¯”ï¼Œtop-pæŠ½æ ·é€šå¸¸èƒ½å¤Ÿäº§ç”Ÿæ›´å¥½çš„ç»“æœã€‚é€šè¿‡å…³æ³¨ç´¯ç§¯æ¦‚ç‡ï¼Œå®ƒèƒ½å¤Ÿé€‚åº”è¾“å…¥çš„ä¸Šä¸‹æ–‡å¹¶æä¾›æ›´çµæ´»çš„æˆªæ–­ã€‚å› æ­¤ï¼Œæ€»ä¹‹ï¼Œtop-på’Œtop-kæŠ½æ ·éƒ½å¯ä»¥åœ¨éé›¶æ¸©åº¦ä¸‹ä½¿ç”¨ï¼Œä»¥åœ¨è¾ƒä½çš„è´¨é‡æˆæœ¬ä¸‹æ•æ‰å¤šæ ·æ€§ï¼Œä½†é€šå¸¸top-pæŠ½æ ·åšå¾—æ›´å¥½ã€‚
- en: 'Tip: For both of these settings, lower value = more filtering. At zero, they
    will filter out all but the top-ranked token, which has the same effect as setting
    the temperature to zero. So please use these parameters, be aware that setting
    them too low will give up all of your diversity.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: æç¤ºï¼šå¯¹äºè¿™ä¸¤ä¸ªè®¾ç½®ï¼Œè¾ƒä½çš„å€¼=æ›´å¤šè¿‡æ»¤ã€‚å½“å€¼ä¸ºé›¶æ—¶ï¼Œå®ƒä»¬å°†è¿‡æ»¤æ‰é™¤æ’åç¬¬ä¸€çš„æ ‡è®°ä»¥å¤–çš„æ‰€æœ‰æ ‡è®°ï¼Œè¿™ä¸å°†æ¸©åº¦è®¾ç½®ä¸ºé›¶å…·æœ‰ç›¸åŒæ•ˆæœã€‚å› æ­¤ï¼Œè¯·ä½¿ç”¨è¿™äº›å‚æ•°æ—¶ï¼Œè¯·æ³¨æ„ä¸è¦å°†å®ƒä»¬è®¾ç½®å¾—å¤ªä½ï¼Œå¦åˆ™ä¼šæŸå¤±æ‰€æœ‰çš„å¤šæ ·æ€§ã€‚
- en: '*TLDR: Top-k and top-p increase quality at only a small cost to diversity.
    They achieve this by removing the worst token choices before random sampling.*'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '*TLDRï¼šTop-kå’Œtop-påœ¨åªä»˜å‡ºè¾ƒå°ä»£ä»·ä»¥å¢åŠ è´¨é‡çš„æƒ…å†µä¸‹æé«˜è´¨é‡ã€‚å®ƒä»¬é€šè¿‡åœ¨éšæœºæŠ½æ ·ä¹‹å‰ç§»é™¤æœ€å·®çš„æ ‡è®°é€‰æ‹©æ¥å®ç°è¿™ä¸€ç‚¹ã€‚*'
- en: Frequency and Presence Penalties
  id: totrans-55
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: é¢‘ç‡å’Œå­˜åœ¨æƒ©ç½š
- en: 'We have just two more parameters to discuss before we start to wrap things
    up: The frequency and presence penalties. These parameters are â€” big surpriseâ€”
    yet another way to navigate the quality-diversity tradeoff. But while the temperature
    parameter achieves diversity by adding randomness to the token sampling procedure,
    the frequency and presence penalties add diversity by penalizing the reuse of
    tokens that have already occurred in the text. This makes the sampling of old
    and overused tokens less likely, influencing the model to make more novel token
    choices.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æˆ‘ä»¬å¼€å§‹æ€»ç»“ä¹‹å‰ï¼Œæˆ‘ä»¬åªéœ€è®¨è®ºå¦å¤–ä¸¤ä¸ªå‚æ•°ï¼šé¢‘ç‡å’Œå­˜åœ¨æƒ©ç½šã€‚è¿™äº›å‚æ•°â€”â€”å¤§æƒŠå°æ€ªâ€”â€”æ˜¯å¯¼èˆªè´¨é‡å¤šæ ·æ€§æƒè¡¡çš„åˆä¸€ç§æ–¹å¼ã€‚è™½ç„¶æ¸©åº¦å‚æ•°é€šè¿‡åœ¨æ ‡è®°æŠ½æ ·è¿‡ç¨‹ä¸­æ·»åŠ éšæœºæ€§æ¥å®ç°å¤šæ ·æ€§ï¼Œé¢‘ç‡å’Œå­˜åœ¨æƒ©ç½šåˆ™é€šè¿‡å¯¹å·²ç»åœ¨æ–‡æœ¬ä¸­å‡ºç°è¿‡çš„æ ‡è®°æ–½åŠ æƒ©ç½šæ¥å¢åŠ å¤šæ ·æ€§ã€‚è¿™ä½¿å¾—æ—§çš„å’Œè¿‡åº¦ä½¿ç”¨çš„æ ‡è®°çš„æŠ½æ ·å˜å¾—ä¸å¤ªå¯èƒ½ï¼Œå½±å“æ¨¡å‹è¿›è¡Œæ›´æ–°é¢–çš„æ ‡è®°é€‰æ‹©ã€‚
- en: The frequency penalty adds a penalty to a token for each time it has occurred
    in the text. This discourages repeated use of the same tokens/words/phrases and
    also has the side effect of causing the model to discuss more diverse subject
    matter and change topics more often. On the other hand, the presence penalty is
    a flat penalty that is applied if a token has already occurred in the text. This
    causes the model to introduce more new tokens/words/phrases, which causes it to
    discuss more diverse subject matter and change topics more often without significantly
    discouraging the repetition of frequently used words.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: é¢‘ç‡æƒ©ç½šä¸ºæ¯æ¬¡æ ‡è®°åœ¨æ–‡æœ¬ä¸­å‡ºç°éƒ½æ·»åŠ ä¸€å®šçš„æƒ©ç½šã€‚è¿™æ ·åšå¯ä»¥å‡å°‘é‡å¤ä½¿ç”¨ç›¸åŒçš„æ ‡è®°/è¯è¯­/çŸ­è¯­ï¼ŒåŒæ—¶ä¹Ÿä¼šå¯¼è‡´æ¨¡å‹è®¨è®ºæ›´å¤šæ ·åŒ–çš„ä¸»é¢˜å¹¶æ›´é¢‘ç¹åœ°æ›´æ¢è¯é¢˜ã€‚å¦ä¸€æ–¹é¢ï¼Œå­˜åœ¨æƒ©ç½šæ˜¯ä¸€ç§å›ºå®šçš„æƒ©ç½šï¼Œå¦‚æœä¸€ä¸ªæ ‡è®°å·²ç»åœ¨æ–‡æœ¬ä¸­å‡ºç°è¿‡ï¼Œåˆ™ä¼šåº”ç”¨è¯¥æƒ©ç½šã€‚è¿™ä½¿å¾—æ¨¡å‹å¼•å…¥æ›´å¤šæ–°çš„æ ‡è®°/è¯è¯­/çŸ­è¯­ï¼Œå¯¼è‡´å®ƒè®¨è®ºæ›´å¤šæ ·åŒ–çš„ä¸»é¢˜å¹¶æ›´é¢‘ç¹åœ°æ›´æ¢è¯é¢˜ï¼Œè€Œä¸”ä¸ä¼šæ˜¾è‘—åœ°æŠ‘åˆ¶ç»å¸¸ä½¿ç”¨çš„è¯è¯­çš„é‡å¤å‡ºç°ã€‚
- en: Much like temperature, the frequency and presence penalties lead us away from
    the â€œbestâ€ possible answer and toward a more creative one. But instead of doing
    this with randomness, they add targeted penalties that are carefully calculated
    to inject diversity into the answer. On some of those rare tasks requiring a non-zero
    temperature (when you require many answers to the same prompt), you might also
    consider adding a small frequency or presence penalty to the mix to boost creativity.
    But for prompts having just one right answer that you want to find in just one
    try, your odds of success are highest when you set all of these parameters to
    zero.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: ç±»ä¼¼äºæ¸©åº¦ï¼Œé¢‘ç‡å’Œå­˜åœ¨æƒ©ç½šä½¿æˆ‘ä»¬è¿œç¦»â€œæœ€ä½³â€ç­”æ¡ˆï¼Œè¶‹å‘äºæ›´å…·åˆ›æ„çš„ç­”æ¡ˆã€‚ä½†ä¸åŒçš„æ˜¯ï¼Œå®ƒä»¬ä¸æ˜¯é€šè¿‡éšæœºæ€§å®ç°çš„ï¼Œè€Œæ˜¯é€šè¿‡ç²¾å¿ƒè®¡ç®—çš„æœ‰é’ˆå¯¹æ€§çš„æƒ©ç½šæ¥æ³¨å…¥å¤šæ ·æ€§ã€‚åœ¨ä¸€äº›å°‘è§çš„éœ€è¦éé›¶æ¸©åº¦çš„ä»»åŠ¡ä¸­ï¼ˆå½“ä½ éœ€è¦å¯¹åŒä¸€æç¤ºè·å¾—å¤šä¸ªç­”æ¡ˆæ—¶ï¼‰ï¼Œä½ ä¹Ÿå¯ä»¥è€ƒè™‘åŠ å…¥å°å¹…çš„é¢‘ç‡æˆ–å­˜åœ¨æƒ©ç½šï¼Œä»¥æå‡åˆ›æ„ã€‚ä½†å¯¹äºé‚£äº›åªå¸Œæœ›æ‰¾åˆ°å”¯ä¸€æ­£ç¡®ç­”æ¡ˆçš„æç¤ºï¼Œåœ¨ä¸€æ¬¡å°è¯•ä¸­è®¾ç½®æ‰€æœ‰è¿™äº›å‚æ•°ä¸ºé›¶æ—¶ï¼Œä½ æˆåŠŸçš„æœºä¼šæœ€é«˜ã€‚
- en: 'As a rule, when there is one right answer, and you are asking just one time,
    you should set the frequency and presence penalties to zero. But what if there
    are many right answers, such as in text summarization? In this case, you have
    a little discretion. If you find a modelâ€™s outputs boring, uncreative, repetitive,
    or limited in scope, judicious application of the frequency or presence penalties
    could be a good way to spice things up. But our final suggestion for these parameters
    is the same as for temperature: **When in doubt, choose zero!**'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: é€šå¸¸ï¼Œå½“åªæœ‰ä¸€ä¸ªæ­£ç¡®ç­”æ¡ˆï¼Œå¹¶ä¸”ä½ åªé—®ä¸€æ¬¡æ—¶ï¼Œåº”è¯¥å°†é¢‘ç‡å’Œå­˜åœ¨æƒ©ç½šè®¾ç½®ä¸ºé›¶ã€‚ä½†å¦‚æœæœ‰å¤šä¸ªæ­£ç¡®ç­”æ¡ˆï¼Œæ¯”å¦‚åœ¨æ–‡æœ¬æ‘˜è¦ä¸­å‘¢ï¼Ÿåœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œä½ æœ‰ä¸€ç‚¹è‡ªç”±è£é‡æƒã€‚å¦‚æœä½ å‘ç°æ¨¡å‹çš„è¾“å‡ºæ— èŠã€ç¼ºä¹åˆ›æ„ã€é‡å¤æˆ–èŒƒå›´æœ‰é™ï¼Œåˆç†åº”ç”¨é¢‘ç‡æˆ–å­˜åœ¨æƒ©ç½šå¯èƒ½æ˜¯ä¸ªä¸é”™çš„æ–¹æ³•æ¥å¢åŠ è¶£å‘³ã€‚ä½†å¯¹äºè¿™äº›å‚æ•°ï¼Œæˆ‘ä»¬çš„æœ€ç»ˆå»ºè®®å’Œæ¸©åº¦å‚æ•°ä¸€æ ·ï¼š**å½“æœ‰ç–‘é—®æ—¶ï¼Œé€‰æ‹©é›¶ï¼**
- en: We should note that while temperature and frequency/presence penalties both
    add diversity to the modelâ€™s responses, the kind of diversity that they add is
    not the same. The frequency/presence penalties increase the diversity *within
    a single response.* This means that a response will have more distinct words,
    phrases, topics, and subject matters than it would have without these penalties.
    But when you pass the same prompt twice, you are not more likely to get two different
    answers. This is in contrast with temperature, which increases diversity *between
    responses:* At higher temperatures, you will get a more diverse range of answers
    when passing the same prompt to the model many times.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬åº”è¯¥æ³¨æ„ï¼Œè™½ç„¶æ¸©åº¦å’Œé¢‘ç‡/å­˜åœ¨æƒ©ç½šéƒ½èƒ½å¢åŠ æ¨¡å‹å“åº”çš„å¤šæ ·æ€§ï¼Œä½†å®ƒä»¬å¢åŠ çš„å¤šæ ·æ€§ç§ç±»å¹¶ä¸ç›¸åŒã€‚é¢‘ç‡/å­˜åœ¨æƒ©ç½šå¢åŠ äº†*å•æ¬¡å“åº”å†…çš„å¤šæ ·æ€§*ã€‚è¿™æ„å‘³ç€ï¼Œä¸€ä¸ªå“åº”å°†å…·æœ‰æ¯”æ²¡æœ‰è¿™äº›æƒ©ç½šæ—¶æ›´å¤šçš„ä¸åŒè¯æ±‡ã€çŸ­è¯­ã€ä¸»é¢˜å’Œå­¦ç§‘ã€‚ä½†å½“ä½ ä¸¤æ¬¡è¾“å…¥ç›¸åŒçš„æç¤ºæ—¶ï¼Œä¸ä¼šæ›´å®¹æ˜“å¾—åˆ°ä¸¤ä¸ªä¸åŒçš„ç­”æ¡ˆã€‚è¿™ä¸æ¸©åº¦å½¢æˆå¯¹æ¯”ï¼Œæ¸©åº¦å¢åŠ äº†*å“åº”ä¹‹é—´çš„å¤šæ ·æ€§*ï¼šåœ¨è¾ƒé«˜çš„æ¸©åº¦ä¸‹ï¼Œå½“å¤šæ¬¡å°†ç›¸åŒæç¤ºè¾“å…¥æ¨¡å‹æ—¶ï¼Œä½ ä¼šå¾—åˆ°æ›´ä¸ºå¤šæ ·åŒ–çš„å›ç­”ã€‚
- en: I like to refer to this distinction as within-response diversity vs. between-response
    diversity. The temperature parameter adds both within-response AND between-response
    diversity, while the frequency/presence penalties add only within-response diversity.
    So, when we need diversity, our choice of parameters should depend on the kind
    of diversity we need.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘å–œæ¬¢å°†è¿™ç§åŒºåˆ†ç§°ä¸ºå“åº”å†…å¤šæ ·æ€§ä¸å“åº”é—´å¤šæ ·æ€§ã€‚æ¸©åº¦å‚æ•°å¢åŠ äº†å“åº”å†…å’Œå“åº”é—´çš„å¤šæ ·æ€§ï¼Œè€Œé¢‘ç‡/å­˜åœ¨æƒ©ç½šä»…å¢åŠ å“åº”å†…çš„å¤šæ ·æ€§ã€‚å› æ­¤ï¼Œå½“æˆ‘ä»¬éœ€è¦å¤šæ ·æ€§æ—¶ï¼Œæˆ‘ä»¬å¯¹å‚æ•°çš„é€‰æ‹©åº”å–å†³äºæˆ‘ä»¬éœ€è¦çš„å¤šæ ·æ€§ç§ç±»ã€‚
- en: '*TLDR: The frequency and presence penalties increase the diversity of subject
    matters discussed by a model and make it change topics more often. The frequency
    penalty also increases diversity of word choice by reducing the repetition of
    words and phrases.*'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '*æ€»ç»“ï¼šé¢‘ç‡å’Œå­˜åœ¨æƒ©ç½šå¢åŠ äº†æ¨¡å‹è®¨è®ºçš„ä¸»é¢˜å¤šæ ·æ€§ï¼Œå¹¶ä½¿å…¶æ›´é¢‘ç¹åœ°æ›´æ¢è¯é¢˜ã€‚é¢‘ç‡æƒ©ç½šè¿˜é€šè¿‡å‡å°‘è¯æ±‡å’ŒçŸ­è¯­çš„é‡å¤æ¥å¢åŠ è¯æ±‡é€‰æ‹©çš„å¤šæ ·æ€§ã€‚*'
- en: The Parameter-Tuning Cheat Sheet
  id: totrans-63
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å‚æ•°è°ƒæ•´å¤‡å¿˜å•
- en: This section is intended as a practical guide for choosing your modelâ€™s input
    parameters. We first provide some hard-and-fast rules for deciding which values
    to set to zero. Then, we give some tips to help you find the right values for
    your non-zero parameters.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬èŠ‚æ—¨åœ¨ä½œä¸ºé€‰æ‹©æ¨¡å‹è¾“å…¥å‚æ•°çš„å®ç”¨æŒ‡å—ã€‚æˆ‘ä»¬é¦–å…ˆæä¾›ä¸€äº›æ˜ç¡®çš„è§„åˆ™æ¥å†³å®šå“ªäº›å€¼åº”è®¾ç½®ä¸ºé›¶ã€‚ç„¶åï¼Œæˆ‘ä»¬ç»™å‡ºä¸€äº›å»ºè®®ï¼Œå¸®åŠ©ä½ æ‰¾åˆ°é€‚åˆéé›¶å‚æ•°çš„æ­£ç¡®å€¼ã€‚
- en: I strongly encourage you to use this cheat sheet when choosing your input parameters.
    Go ahead and bookmark this page now so you donâ€™t lose it!
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘å¼ºçƒˆå»ºè®®ä½ åœ¨é€‰æ‹©è¾“å…¥å‚æ•°æ—¶ä½¿ç”¨è¿™ä»½å¤‡å¿˜å•ã€‚ç°åœ¨å°±å»æ”¶è—æ­¤é¡µé¢ï¼Œä»¥å…ä¸¢å¤±ï¼
- en: 'Rules for setting parameters to zero:'
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å°†å‚æ•°è®¾ç½®ä¸ºé›¶çš„è§„åˆ™ï¼š
- en: 'Temperature:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: æ¸©åº¦ï¼š
- en: 'For a **single answer** per prompt: Zero.'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ¯ä¸ªæç¤º**ä¸€ä¸ªç­”æ¡ˆ**ï¼šé›¶ã€‚
- en: 'For **many answers** per prompt: Non-zero.'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ¯ä¸ªæç¤º**å¤šä¸ªç­”æ¡ˆ**ï¼šéé›¶ã€‚
- en: 'Frequency and Presence Penalties:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: é¢‘ç‡å’Œå­˜åœ¨æƒ©ç½šï¼š
- en: When there is **one correct answer:** Zero.
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å½“æœ‰**ä¸€ä¸ªæ­£ç¡®ç­”æ¡ˆ**æ—¶ï¼šé›¶ã€‚
- en: When there are **many correct answers:** Optional.
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å½“æœ‰**å¤šä¸ªæ­£ç¡®ç­”æ¡ˆ**æ—¶ï¼šå¯é€‰ã€‚
- en: 'Top-p/Top-k:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: Top-p/Top-kï¼š
- en: 'With **zero temperature**: The output is not affected.'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åœ¨**é›¶æ¸©åº¦**ä¸‹ï¼šè¾“å‡ºä¸å—å½±å“ã€‚
- en: 'With **non-zero temperature**: Non-zero.'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åœ¨**éé›¶æ¸©åº¦**ä¸‹ï¼šéé›¶ã€‚
- en: If your language model has additional parameters not listed here, it is always
    okay to leave them at their default values.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä½ çš„è¯­è¨€æ¨¡å‹æœ‰å…¶ä»–æœªåˆ—å‡ºçš„å‚æ•°ï¼Œä¿æŒå…¶é»˜è®¤å€¼ä¹Ÿæ˜¯å¯ä»¥çš„ã€‚
- en: 'Tips for tuning the non-zero parameters:'
  id: totrans-77
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è°ƒæ•´éé›¶å‚æ•°çš„æç¤ºï¼š
- en: Make a list of those parameters that should have non-zero values, and then go
    to a playground and fiddle around with some test prompts to see what works. ***But
    if the rules above say to leave a parameter at zero, leave it at zero!***
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ—å‡ºåº”è¯¥å…·æœ‰éé›¶å€¼çš„é‚£äº›å‚æ•°ï¼Œç„¶åå»è¯•éªŒåœºä¸­å°è¯•ä¸€äº›æµ‹è¯•æç¤ºä»¥æŸ¥çœ‹æ•ˆæœã€‚***ä½†å¦‚æœä¸Šè¿°è§„åˆ™è¦æ±‚å°†å‚æ•°ä¿æŒåœ¨é›¶ï¼Œåˆ™ä¿æŒåœ¨é›¶ï¼***
- en: 'Tuning temperature/top-p/top-k:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: è°ƒæ•´æ¸©åº¦/top-p/top-kï¼š
- en: For more diversity/randomness, increase the temperature.
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ä¸ºäº†æ›´å¤šçš„å¤šæ ·æ€§/éšæœºæ€§ï¼Œå¢åŠ æ¸©åº¦ã€‚
- en: With non-zero temperatures, start with a top-p around 0.95 (or top-k around
    250) and lower it as needed.
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: åœ¨éé›¶æ¸©åº¦ä¸‹ï¼Œä» top-p çº¦ 0.95ï¼ˆæˆ– top-k çº¦ 250ï¼‰å¼€å§‹ï¼Œæ ¹æ®éœ€è¦é€æ¸é™ä½ã€‚
- en: 'Troubleshooting:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: æ•…éšœæ’é™¤ï¼š
- en: If there is too much nonsense, garbage, or hallucination, decrease temperature
    and/or decrease top-p/top-k.
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å¦‚æœå‡ºç°è¿‡å¤šçš„åºŸè¯ã€åƒåœ¾æˆ–å¹»è§‰ï¼Œé™ä½æ¸©åº¦å’Œ/æˆ–å‡å°‘ top-p/top-kã€‚
- en: If the temperature is high and diversity is low, increase top-p/top-k.
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å¦‚æœæ¸©åº¦é«˜è€Œå¤šæ ·æ€§ä½ï¼Œå¢åŠ  top-p/top-kã€‚
- en: 'Tip: While some interfaces allow you to use top-p and top-k at the same time,
    we prefer to keep things simple by choosing one or the other. Top-k is easier
    to use and understand, but top-p is often more effective.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: æç¤ºï¼šè™½ç„¶ä¸€äº›ç•Œé¢å…è®¸ä½ åŒæ—¶ä½¿ç”¨ top-p å’Œ top-kï¼Œä½†æˆ‘ä»¬å€¾å‘äºé€‰æ‹©å…¶ä¸­ä¹‹ä¸€ä»¥ä¿æŒç®€å•ã€‚Top-k æ›´æ˜“äºä½¿ç”¨å’Œç†è§£ï¼Œä½† top-p é€šå¸¸æ›´æœ‰æ•ˆã€‚
- en: 'Tuning frequency penalty and presence penalty:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: è°ƒæ•´é¢‘ç‡æƒ©ç½šå’Œå­˜åœ¨æƒ©ç½šï¼š
- en: For more diverse topics and subject matters, increase the presence penalty.
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ä¸ºäº†æ›´å¤šçš„å¤šæ ·åŒ–ä¸»é¢˜å’Œå†…å®¹ï¼Œå¢åŠ å­˜åœ¨æƒ©ç½šã€‚
- en: For more diverse and less repetitive language, increase the frequency penalty.
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ä¸ºäº†æ›´ä¸°å¯Œä¸”ä¸é‡å¤çš„è¯­è¨€ï¼Œå¢åŠ é¢‘ç‡æƒ©ç½šã€‚
- en: 'Troubleshooting:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: æ•…éšœæ’é™¤ï¼š
- en: If the outputs seem scattered and change topics too quickly, decrease the presence
    penalty.
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å¦‚æœè¾“å‡ºæ˜¾å¾—é›¶æ•£ä¸”ä¸»é¢˜å˜åŒ–è¿‡å¿«ï¼Œå‡å°‘å­˜åœ¨æƒ©ç½šã€‚
- en: If there are too many new and unusual words, or if the presence penalty is set
    to zero and you still get too many topic changes, decrease the frequency penalty.
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å¦‚æœæœ‰å¤ªå¤šæ–°è¯å’Œä¸å¯»å¸¸çš„è¯ï¼Œæˆ–è€…å­˜åœ¨æƒ©ç½šè®¾ç½®ä¸ºé›¶ä½†ä»ç„¶æœ‰å¤ªå¤šä¸»é¢˜å˜åŒ–ï¼Œå‡å°‘é¢‘ç‡æƒ©ç½šã€‚
- en: '*TLDR: You can use this section as a cheat sheet for tuning language models.
    You are* ***definitely*** *going to**forget these rules, so bookmark this page
    and use it later as a reference.*'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '*TLDRï¼šä½ å¯ä»¥å°†æ­¤éƒ¨åˆ†ä½œä¸ºè°ƒæ•´è¯­è¨€æ¨¡å‹çš„å¤‡å¿˜å•ã€‚ä½ * ***è‚¯å®š*** *ä¼š**å¿˜è®°è¿™äº›è§„åˆ™ï¼Œå› æ­¤è¯·æ”¶è—æ­¤é¡µé¢ï¼Œå¹¶åœ¨ä»¥åä½œä¸ºå‚è€ƒä½¿ç”¨ã€‚*'
- en: Wrapping up
  id: totrans-93
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ€»ç»“
- en: While there are limitless ways to define a token sampling strategy, the parameters
    weâ€™ve discussed here â€” temperature, top-k, top-p, frequency penalty, and presence
    penalty â€” are among the most commonly used. These are the parameters that you
    can expect to find in models like Claude, Llama, and the GPT series. In this article,
    we have shown that all of these parameters are really just here to help us navigate
    the quality-diversity tradeoff.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: è™½ç„¶å®šä¹‰ä»¤ç‰Œé‡‡æ ·ç­–ç•¥çš„æ–¹æ³•æ— ç©·æ— å°½ï¼Œä½†æˆ‘ä»¬è®¨è®ºè¿‡çš„å‚æ•°â€”â€”æ¸©åº¦ã€top-kã€top-pã€é¢‘ç‡æƒ©ç½šå’Œå­˜åœ¨æƒ©ç½šâ€”â€”æ˜¯æœ€å¸¸ç”¨çš„å‚æ•°ã€‚è¿™äº›å‚æ•°æ˜¯ä½ å¯ä»¥åœ¨ Claudeã€Llama
    å’Œ GPT ç³»åˆ—ç­‰æ¨¡å‹ä¸­æ‰¾åˆ°çš„ã€‚æœ¬æ–‡å±•ç¤ºäº†æ‰€æœ‰è¿™äº›å‚æ•°å®é™…ä¸Šåªæ˜¯å¸®åŠ©æˆ‘ä»¬å¯¼èˆªè´¨é‡ä¸å¤šæ ·æ€§æƒè¡¡çš„å·¥å…·ã€‚
- en: 'Before we go, there is one last input parameter to mention: maximum token length.
    The maximum token length is just the cutoff where the model stops printing its
    answer, even if it isnâ€™t finished. After that complex discussion, we hope this
    one is self-explanatory. ğŸ™‚'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æˆ‘ä»¬ç»“æŸä¹‹å‰ï¼Œè¿˜æœ‰ä¸€ä¸ªæœ€åçš„è¾“å…¥å‚æ•°éœ€è¦æåŠï¼šæœ€å¤§ä»¤ç‰Œé•¿åº¦ã€‚æœ€å¤§ä»¤ç‰Œé•¿åº¦å°±æ˜¯æ¨¡å‹åœæ­¢æ‰“å°ç­”æ¡ˆçš„æˆªæ­¢ç‚¹ï¼Œå³ä½¿ç­”æ¡ˆæœªå®Œæˆã€‚åœ¨å¤æ‚çš„è®¨è®ºä¹‹åï¼Œæˆ‘ä»¬å¸Œæœ›è¿™ä¸ªå‚æ•°æ˜¯ä¸è¨€è‡ªæ˜çš„ã€‚ğŸ™‚
- en: As we move further in this series, weâ€™ll do more deep dives into topics such
    as prompt engineering, choosing the right language model for your use case, and
    more! I will also show some real-world use cases from my work as a data analysis
    consultant at Megaputer Intelligence. Stay tuned for more insights, and happy
    modeling!
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: éšç€æˆ‘ä»¬åœ¨è¿™ä¸€ç³»åˆ—ä¸­çš„æ·±å…¥ï¼Œæˆ‘ä»¬å°†æ›´æ·±å…¥åœ°æ¢è®¨è¯¸å¦‚æç¤ºå·¥ç¨‹ã€ä¸ºä½ çš„ä½¿ç”¨æ¡ˆä¾‹é€‰æ‹©åˆé€‚çš„è¯­è¨€æ¨¡å‹ç­‰ä¸»é¢˜ï¼æˆ‘è¿˜ä¼šå±•ç¤ºä¸€äº›æ¥è‡ªæˆ‘åœ¨Megaputer Intelligenceæ‹…ä»»æ•°æ®åˆ†æé¡¾é—®æ—¶çš„å®é™…åº”ç”¨æ¡ˆä¾‹ã€‚æ•¬è¯·æœŸå¾…æ›´å¤šè§è§£ï¼Œç¥å»ºæ¨¡æ„‰å¿«ï¼
- en: '*TLDR: When in doubt, set the temperature, frequency penalty, and presence
    penalty to zero. If that doesnâ€™t work for you, reference the cheat sheet above.*'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '*TLDR: å¦‚æœæœ‰ç–‘é—®ï¼Œå°†æ¸©åº¦ã€é¢‘ç‡æƒ©ç½šå’Œå­˜åœ¨æƒ©ç½šè®¾ç½®ä¸ºé›¶ã€‚å¦‚æœè¿™æ ·åšå¯¹ä½ ä¸èµ·ä½œç”¨ï¼Œè¯·å‚è€ƒä¸Šé¢çš„å¤‡å¿˜å•ã€‚*'
