- en: 'Exploring GEMBA: A New LLM-Based Metric for Translation Quality Assessment'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/exploring-gemba-a-new-llm-based-metric-for-translation-quality-assessment-3a3383de6d1f?source=collection_archive---------7-----------------------#2023-09-29](https://towardsdatascience.com/exploring-gemba-a-new-llm-based-metric-for-translation-quality-assessment-3a3383de6d1f?source=collection_archive---------7-----------------------#2023-09-29)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '#GEN-AI RESEARCH PAPERS'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Using LLMs for evaluating translation quality
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://varshitasher.medium.com/?source=post_page-----3a3383de6d1f--------------------------------)[![Dr.
    Varshita Sher](../Images/a3f2e9bf1dc1d8cbe018e54f9341f608.png)](https://varshitasher.medium.com/?source=post_page-----3a3383de6d1f--------------------------------)[](https://towardsdatascience.com/?source=post_page-----3a3383de6d1f--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----3a3383de6d1f--------------------------------)
    [Dr. Varshita Sher](https://varshitasher.medium.com/?source=post_page-----3a3383de6d1f--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ff8ca36def59&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fexploring-gemba-a-new-llm-based-metric-for-translation-quality-assessment-3a3383de6d1f&user=Dr.+Varshita+Sher&userId=f8ca36def59&source=post_page-f8ca36def59----3a3383de6d1f---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----3a3383de6d1f--------------------------------)
    ·9 min read·Sep 29, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F3a3383de6d1f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fexploring-gemba-a-new-llm-based-metric-for-translation-quality-assessment-3a3383de6d1f&user=Dr.+Varshita+Sher&userId=f8ca36def59&source=-----3a3383de6d1f---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F3a3383de6d1f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fexploring-gemba-a-new-llm-based-metric-for-translation-quality-assessment-3a3383de6d1f&source=-----3a3383de6d1f---------------------bookmark_footer-----------)![](../Images/dfe2aa68fdf75319c195035d9fe70462.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Image generated by Author using [DALL.E 2](https://openai.com/product/dall-e-2)
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: I recently read an intriguing [paper from the Microsoft](https://arxiv.org/pdf/2302.14520.pdf)¹
    team (published in May 2023) that caught my attention. The paper delves into the
    world of translation evaluation, shedding light on an innovative metric called
    GEMBA (**G**PT **E**stimation **M**etric **B**ased **A**ssessment). In this blog
    post, we’ll dissect the paper and provide insights into this exciting development.
  prefs: []
  type: TYPE_NORMAL
- en: 'Premise: Exploring the motivation behind the paper'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Research Questions and Hypotheses: Exploring the paper’s primary research question
    and hypotheses.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Metrics for Assessing Translation Quality: Shallow-dive into existing metrics,
    including BLEU, COMET, and METEOR.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Introduce GEMBA: Deep dive into the novel GEMBA metric.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Experiment Details: Insights into the experiments conducted'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Key findings: Highlighting the main results from the paper'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Limitations: Discussing things to look out for before implementing GEMBA in
    production'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 1\. Premise
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: LLMs, although not initially designed for translation tasks, have demonstrated
    impressive precision in this domain. This realization led authors to the exploration
    of using LLMs as evaluation tools for translations. The paper’s central idea is
    quite straightforward- positioning LLMs (Large Language Models) as tools for evaluating
    translations, not just for performing them. The authors propose a new metric called
    GEMBA, which outperforms existing state-of-the-art metrics for translation quality
    assessment.
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Research Question
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*Can LLMs be used for quality assessment of translations?*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 3\. Translation Quality Assessment Metrics
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Before delving into GEMBA, let’s take a quick look at the existing metrics used
    to evaluate the quality of machine-generated translations, such as BLEU, COMET,
    METEOR, etc. These metrics have their own strengths and are suited to different
    use cases, depending on the specific aspects of translation quality that are most
    important for the evaluation.
  prefs: []
  type: TYPE_NORMAL
- en: For instance, BLEU (Bilingual Evaluation Understudy) primarily focuses on n-gram
    precision, which means it measures how many n-word sequences in the machine translation
    overlap with n-word sequences in one or more reference translations. It rewards
    the presence of specific word sequences and does not explicitly consider word
    order, stemming, or synonymy. METEOR (Metric for Evaluation of Translation with
    Explicit ORdering), on the other hand, goes beyond basic n-gram matching and takes
    a more holistic approach to translation evaluation. It considers multiple aspects
    of translation quality, including word stemming, synonymy, word order, exact word
    matches, and even penalties for untranslated words. Similarly, COMET (Content-based
    Machine Translation Evaluation) uses a slightly different approach by focusing
    on content-based evaluation and calculating the semantic similarity between a
    machine translation output and a reference translation using embeddings. In simpler
    words, it evaluates how well the content and meaning of the machine translation
    match the reference, regardless of the specific linguistic variations or word
    choices.
  prefs: []
  type: TYPE_NORMAL
- en: You can learn about other evaluation metrics such as YiSi, chrF, BERTScore,
    etc [here](https://machinetranslate.org/metrics).
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Given the plethora of metrics we discussed just now, you may ask — *Why do we
    need a new metric like GEMBA?* The answer lies in its unique approach — prompting
    LLMs to assess translations based on their own judgment. Unlike traditional metrics,
    GEMBA seeks to align with human assessment of translations by scoring them on
    a scale (say 0 to 100), focusing on both meaning and grammar.
  prefs: []
  type: TYPE_NORMAL
- en: 4\. Introducing GEMBA
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As mentioned previously, GEMBA is a GPT-based metric for the assessment of translation
    quality, which works both with a reference translation and without.
  prefs: []
  type: TYPE_NORMAL
- en: 'In essence, GEMBA is a really well-engineered prompt for the evaluation task
    and consists of:'
  prefs: []
  type: TYPE_NORMAL
- en: prompt variant (from a pre-defined set of four variants)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: source language name, e.g., “Chinese”
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: target language name, e.g., “English”
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: source segments i.e. the sentence that needs to be translated
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: candidate translations i.e. the translated sentence
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[optional] reference translations i.e. the baseline translation that can be
    used as ground truth'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here’s an example of one of the prompt variants: GEMBA-DA (Direct Assessment)'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c2fe2c3dc576ad0398dafd70e9e68bc5.png)'
  prefs: []
  type: TYPE_IMG
- en: GEMBA-DA prompt. Image taken from original paper
  prefs: []
  type: TYPE_NORMAL
- en: '*P.S. In case you are interested in the other three variants, here is detailed
    information on all prompt variants introduced by the paper:*'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b80872de907f5f5ad1a3c2fd7bb50616.png)'
  prefs: []
  type: TYPE_IMG
- en: GEMBA prompt variants. Image taken from original paper
  prefs: []
  type: TYPE_NORMAL
- en: 5\. Experimentation and Evaluation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The authors tested the efficiency of the GEMBA metric using the widely popular
    MQM 2022 dataset (Multidimensional Quality Metrics) as an evaluation set. This
    dataset includes a diverse range of (100K+) sentences from various domains such
    as news, social, e-commerce, etc., and covers three translation directions: English
    to Russian, English to German, and Chinese to English.'
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, to find the best GPT model for implementing GEMBA, the authors
    tested each of the prompt variants with seven models from the GPT family ranging
    from GPT 2 up to the latest GPT-4 model.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/113060dec377b610ab5deb56ace40c0b.png)'
  prefs: []
  type: TYPE_IMG
- en: 7 GPT models used for evaluating GEMBA. Image taken from original paper
  prefs: []
  type: TYPE_NORMAL
- en: Having set the stage for experimentation of the GEMBA metric, the next obvious
    question is —
  prefs: []
  type: TYPE_NORMAL
- en: 'Q: How do we tell if GEMBA is performing better than conventional metrics such
    as BLEU and COMET?'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'A: If GEMBA scores corresponds closely with what a human thinks of the translation,
    we have a winner!'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: To operationalize that answer, there are two metrics (Kendall’s Tau and Accuracy
    (*accuracy*, [Kocmi et al., 2021](https://aclanthology.org/2021.wmt-1.57/))) that
    need to be calculated, depending on whether we are undertaking *segment-level*
    evaluation or *system-level* evaluation. But first, what are they?
  prefs: []
  type: TYPE_NORMAL
- en: '**System level evaluation** assesses the **overall performance** of a machine
    translation system as a whole. It looks at the quality of translations generated
    by the system across a wide range of text or content.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**Segment level evaluation** focuses on assessing the quality of translations
    on a per-segment basis (typically a **sentence** or a smaller unit of text)'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Generally speaking:'
  prefs: []
  type: TYPE_NORMAL
- en: Kendall’s Tau is used for segment-level evaluations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Accuracy is used for system-level evaluations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let’s take a deep dive into their formulas for clarity using simple examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '**A. Kendall’s Tau**'
  prefs: []
  type: TYPE_NORMAL
- en: '**(Kendall’s Tau** tells if there is a correlation between 2 rankings)'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Suppose you have three translations (A, B, and C) of a given sentence, and you
    want to assess the correlation between rankings produced by a metric (e.g., LLM,
    BLEU, METEOR scores) and human judgments of translation quality.
  prefs: []
  type: TYPE_NORMAL
- en: '**Reference** (Human): “The quick brown fox jumps over the lazy dog.”'
  prefs: []
  type: TYPE_NORMAL
- en: '**Translation A**: “The fast brown fox jumps over the lazy dog.”'
  prefs: []
  type: TYPE_NORMAL
- en: '**Translation B**: “The quick red fox jumps over the lazy dog.”'
  prefs: []
  type: TYPE_NORMAL
- en: '**Translation C**: “The lazy dog is jumped over by the quick brown fox.”'
  prefs: []
  type: TYPE_NORMAL
- en: '**Human Ranking:** A > B > C (i.e., they prefer Translation A the most, then
    B, and lastly C)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Metric scores for these translations:** LLM(A) = 0.85'
  prefs: []
  type: TYPE_NORMAL
- en: LLM(B) = 0.75
  prefs: []
  type: TYPE_NORMAL
- en: LLM(C) = 0.60
  prefs: []
  type: TYPE_NORMAL
- en: 'With all information, we can calculate Kendall’s Tau as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9758b4a89fc855185d542b615a69bbd7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Next, let’s calculate concordant and discordant pairs:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Pair 1: (A, B)** Human Ranking: A > B'
  prefs: []
  type: TYPE_NORMAL
- en: 'LLM Scores: LLM(A) = 0.85 > LLM(B) = 0.75'
  prefs: []
  type: TYPE_NORMAL
- en: 'Result: **Concordant pair** (both human and metric prefer A over B).'
  prefs: []
  type: TYPE_NORMAL
- en: '**Pair 2: (A, C)** Human Ranking: A > C'
  prefs: []
  type: TYPE_NORMAL
- en: 'LLM Scores: LLM(A) = 0.85 > LLM(C) = 0.60'
  prefs: []
  type: TYPE_NORMAL
- en: 'Result: **Concordant pair** (both human and metric prefer A over C).'
  prefs: []
  type: TYPE_NORMAL
- en: '**Pair 3: (B, C)** Human Ranking: B > C'
  prefs: []
  type: TYPE_NORMAL
- en: 'LLM Scores: LLM(B) = 0.75 > LLM(C) = 0.60'
  prefs: []
  type: TYPE_NORMAL
- en: 'Result: **Concordant pair** (both human and metric prefer B over C).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Plugging these into the formula we get:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0e0807f463e9c7e7b2433b8c21800542.png)'
  prefs: []
  type: TYPE_IMG
- en: Kendall’s Tau
  prefs: []
  type: TYPE_NORMAL
- en: In other words, *τ* = 1 suggests a perfect agreement between the metric and
    human judgment and is hence a high-quality metric that can be used for automating
    the quality of translations.
  prefs: []
  type: TYPE_NORMAL
- en: '**B. Accuracy**'
  prefs: []
  type: TYPE_NORMAL
- en: Kendall’s Tau assesses the similarity or agreement between rankings, whereas
    accuracy measures the correctness of rankings.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'To illustrate calculations of accuracy, take the same setup as above i.e. Reference
    (Human), Translation A, Translation B, Translation C, Human Ranking but let’s
    update the metric scores a bit so that B is marked as a better translation than
    C according to Bleu:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Metric Scores (BLEU):'
  prefs: []
  type: TYPE_NORMAL
- en: BLEU(A) = 0.80
  prefs: []
  type: TYPE_NORMAL
- en: BLEU(B) = 0.70
  prefs: []
  type: TYPE_NORMAL
- en: BLEU(C) = 0.75
  prefs: []
  type: TYPE_NORMAL
- en: 'With all the information, here’s how to calculate accuracy:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b6c0c4a022e73e9b05d6f5fe78701d82.png)'
  prefs: []
  type: TYPE_IMG
- en: Let’s calculate metric Δ (which is nothing but the difference in metric values
    for a pair of translations) and human Δ (which is the difference in the human
    scores for a pair of translations). If you look at the formula closely, you will
    notice we are not interested in the actual value of the Δ but the sign of the
    Δ. Put simply, a high accuracy can only be achieved if the sign of both Δs are
    the same i.e. human and metric are in alignment about a translation.
  prefs: []
  type: TYPE_NORMAL
- en: '**Pair 1: (A, B)** Metric∆ = BLEU(A) — BLEU(B) = 0.80–0.70 = 0.10'
  prefs: []
  type: TYPE_NORMAL
- en: Human∆ = 1 (A is ranked higher than B)
  prefs: []
  type: TYPE_NORMAL
- en: 'Result: Metric∆ and Human∆ have the same sign (both positive). This is a rank
    agreement.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Pair 2: (A, C)** Metric∆ = BLEU(A) — BLEU(C) = 0.80–0.75 = 0.05'
  prefs: []
  type: TYPE_NORMAL
- en: Human∆ = 2 (A is ranked higher than C)
  prefs: []
  type: TYPE_NORMAL
- en: 'Result: Metric∆ and Human∆ have the same sign (both positive). This is a rank
    agreement.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Pair 3: (B, C)** Metric∆ = BLEU(B) — BLEU(C) = 0.70–0.75 = -0.05'
  prefs: []
  type: TYPE_NORMAL
- en: Human∆ = 1 (B is ranked higher than C)
  prefs: []
  type: TYPE_NORMAL
- en: 'Result: Metric∆ and Human∆ have different signs (metric is negative, human
    is positive). This is a rank disagreement.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Plugging these into the formula we get:'
  prefs: []
  type: TYPE_NORMAL
- en: Accuracy(Bleu) = (2/3)*100 = 67%, meaning the BLEU metric accurately ranks 2
    out of 3 pairs of translations as per human judgment. Whether or not this percentage
    is good enough to automate evaluation with Bleu, I will leave that to the reader’s
    discretion!
  prefs: []
  type: TYPE_NORMAL
- en: '*Note: The examples used to demonstrate the calculation of Kendall’s Tau and
    Accuracy were simplified for demonstration purposes. In reality, the formulas
    get more complicated if ties need to be handled i.e. if a human/metric gives the
    same ranking to two or more translations. You can read more about them* [*here*](https://en.wikipedia.org/wiki/Kendall_rank_correlation_coefficient#Accounting_for_ties)*.*'
  prefs: []
  type: TYPE_NORMAL
- en: '6\. Key Results: System vs. Segment Level Evaluation'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The paper reports that GEMBA excels in system-level evaluations, surpassing
    existing metrics.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/616f4c4e0fe739d69ca0110e1a2625ee.png)'
  prefs: []
  type: TYPE_IMG
- en: Results from system-level evaluations. Image taken from original paper
  prefs: []
  type: TYPE_NORMAL
- en: However, in segment-level evaluations, there is room for improvement. Ties in
    rankings between LLMs and humans at this level may account for this discrepancy
    as Kendall’s Tau penalizes ties. Since the Gemba-DA metric returns a discrete
    value between 0–100, there is a high probability that two translations will obtain
    an equal score.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a91e9b41382d139357bfadceead09239.png)'
  prefs: []
  type: TYPE_IMG
- en: Results from segment-level evaluations (P.S. the first column Accuracy is the
    same one from the previous table). Image taken from original paper
  prefs: []
  type: TYPE_NORMAL
- en: The results also stress the importance of choosing the right LLM for implementing
    GEMBA. Among the seven models from the GPT family tested any model beyond 3.5
    showed promising results. GPT-4 stood out as the top performer, but models like
    Davinci, ChatGPT Turbo, and GPT-3.5 also performed well.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7a2b32be8d7943a777023d7225f40647.png)'
  prefs: []
  type: TYPE_IMG
- en: GEMBA implementation with various models from the GPT family. Image taken from
    original paper
  prefs: []
  type: TYPE_NORMAL
- en: '**7\. Limitations and Considerations**'
  prefs: []
  type: TYPE_NORMAL
- en: The paper highlights certain limitations to address GEMBA’s broader applicability.
  prefs: []
  type: TYPE_NORMAL
- en: There is a need for evaluation of GEMBA with low-resource languages since the
    paper only considers English, Chinese, Russian, and German.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There could be potential data leakage concerns as it is uncertain whether the
    test data was included in Open AI’s training (the secret sauce hasn’t been released
    by Open AI at the time of writing). Having said that, the likelihood is very low
    as GPT models claim to have a knowledge cutoff date of Sep 2021, and the MQM dataset
    was released in Dec 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'There could be occasional invalid responses from LLMs:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ►textual answer instead of score → handled by increasing `temperature` until
    a numeric score is outputted.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: ►“2”, “two”, “**”, “★★”, “two stars”, or “2 stars” → handled in post-processing
    to maintain uniformity.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: ►authors excluded outputs from LLM in the non-English target language ( such
    as 星 or 五) from analysis.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![](../Images/265cb17581d9e1baa8d3129dcbdb9e02.png)'
  prefs: []
  type: TYPE_IMG
- en: Number of invalid responses. Image taken from original paper.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Keeping in mind the ease of implementing GEMBA with a simple prompt, it definitely
    stands out as a groundbreaking metric for translation quality assessment. Its
    alignment with human judgment and adaptability to various LLM models make it a
    compelling addition to the field of NLP and translation evaluation. As we continue
    to explore and refine GEMBA (perhaps with few-shot prompting), it holds promise
    as a valuable tool for ensuring high-quality translations in diverse language
    contexts.
  prefs: []
  type: TYPE_NORMAL
- en: '*[1] Kocmi, T., & Federmann, C. (2023). Large language models are state-of-the-art
    evaluators of translation quality. arXiv preprint arXiv:2302.14520.*'
  prefs: []
  type: TYPE_NORMAL
