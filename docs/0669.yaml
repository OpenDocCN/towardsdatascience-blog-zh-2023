- en: 'Transformer Models 101: Getting Started — Part 1'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/transformer-models-101-getting-started-part-1-b3a77ccfa14d?source=collection_archive---------1-----------------------#2023-02-18](https://towardsdatascience.com/transformer-models-101-getting-started-part-1-b3a77ccfa14d?source=collection_archive---------1-----------------------#2023-02-18)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The complex math behind transformer models, in simple words
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://nandinibansal1811.medium.com/?source=post_page-----b3a77ccfa14d--------------------------------)[![Nandini
    Bansal](../Images/a813ac8be6f89b2e856651fda66ab078.png)](https://nandinibansal1811.medium.com/?source=post_page-----b3a77ccfa14d--------------------------------)[](https://towardsdatascience.com/?source=post_page-----b3a77ccfa14d--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----b3a77ccfa14d--------------------------------)
    [Nandini Bansal](https://nandinibansal1811.medium.com/?source=post_page-----b3a77ccfa14d--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fc41c26af0465&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransformer-models-101-getting-started-part-1-b3a77ccfa14d&user=Nandini+Bansal&userId=c41c26af0465&source=post_page-c41c26af0465----b3a77ccfa14d---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----b3a77ccfa14d--------------------------------)
    ·11 min read·Feb 18, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fb3a77ccfa14d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransformer-models-101-getting-started-part-1-b3a77ccfa14d&user=Nandini+Bansal&userId=c41c26af0465&source=-----b3a77ccfa14d---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb3a77ccfa14d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransformer-models-101-getting-started-part-1-b3a77ccfa14d&source=-----b3a77ccfa14d---------------------bookmark_footer-----------)![](../Images/75e375b7793cc2763bbaa654fb731228.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Image by [Kerttu](https://pixabay.com/users/kerttu-569708/?utm_source=link-attribution&amp%3Butm_medium=referral&amp%3Butm_campaign=image&amp%3Butm_content=1151405)
    from [Pixabay](https://pixabay.com//?utm_source=link-attribution&amp%3Butm_medium=referral&amp%3Butm_campaign=image&amp%3Butm_content=1151405)
  prefs: []
  type: TYPE_NORMAL
- en: It is no secret that transformer architecture was a breakthrough in the field
    of Natural Language Processing (NLP). It overcame the limitation of seq-to-seq
    models like RNNs, etc for being incapable of capturing long-term dependencies
    in text. The transformer architecture turned out to be the foundation stone of
    revolutionary architectures like BERT, GPT, and T5 and their variants. As many
    say, NLP is in the midst of a golden era and it wouldn’t be wrong to say that
    the transformer model is where it all started.
  prefs: []
  type: TYPE_NORMAL
- en: Need for Transformer Architecture
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As said, necessity is the mother of invention. The traditional seq-to-seq models
    were no good when it came to working with long texts. ***Meaning the model tends
    to forget the learnings from the earlier parts of the input sequence as it moves
    to process the latter part of the input sequence***. This loss of information
    is undesirable.
  prefs: []
  type: TYPE_NORMAL
- en: Although gated architectures like LSTMs and GRUs showed some improvement in
    performance for handling long-term dependencies by ***discarding information that
    was useless along the way to remember important information,*** it still wasn’t
    enough. The world needed something more powerful and in 2015, “attentionmechanisms”
    were introduced by [**Bahdanau et al**](https://arxiv.org/abs/1409.0473)**.**
    They were used in combination with RNN/LSTM to mimic human behaviour to focus
    on selective things while ignoring the rest. Bahdanau suggested assigning relative
    importance to each word in a sentence so that model focuses on important words
    and ignores the rest. It emerged to be a massive improvement over encoder-decoder
    models for neural machine translation tasks and soon enough, the application of
    the attention mechanism was rolled out in other tasks as well.
  prefs: []
  type: TYPE_NORMAL
- en: The Era of Transformer Models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The transformer models are entirely based on an attention mechanism which is
    also known as ***“self-attention”***. This architecture was introduced to the
    world in the paper “[**Attention is All You Need**](https://arxiv.org/abs/1706.03762)”
    in 2017\. It consisted of an Encoder-Decoder Architecture.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e1adf6bb56d8cb4d42606b7e2236a8d0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig. Transformer Model Architecture on high-level (Source: Author)'
  prefs: []
  type: TYPE_NORMAL
- en: On a high level,
  prefs: []
  type: TYPE_NORMAL
- en: The ***encoder*** is responsible for accepting the input sentence and converting
    it into a hidden representation with all useless information discarded.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The ***decoder*** accepts this hidden representation and tries to generate the
    target sentence.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this article, we will delve into a detailed breakdown of the Encoder component
    of the Transformer model. In the next article, we shall look at the Decoder component
    in detail. Let’s start!
  prefs: []
  type: TYPE_NORMAL
- en: Encoder of Transformer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The encoder block of the transformer consists of a stack of N encoders that
    work sequentially. The output of one encoder is the input for the next encoder
    and so on. The output of the last encoder is the final representation of the input
    sentence that is fed to the decoder block.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e82a5f3835bb6428ad06e633f02301be.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig. Enoder block with stacked encoders (Source: Author)'
  prefs: []
  type: TYPE_NORMAL
- en: Each encoder block can be further split into two components as shown in the
    figure below.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/701ef09d3f7d1158f0d332eafcbcedb2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig. Components of Encoder Layer (Source: Author)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let us look into each of these components one by one in detail to understand
    how the encoder block is working. The first component in the encoder block is
    ***multi-head attention*** but before we hop into the details, let us first understand
    an underlying concept: ***self-attention***.'
  prefs: []
  type: TYPE_NORMAL
- en: Self-Attention Mechanism
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The first question that might pop up in everyone’s mind: *Are attention and
    self-attention different concepts?* Yes, they are. (Duh!)'
  prefs: []
  type: TYPE_NORMAL
- en: Traditionally, the ***attention mechanisms*** came into existence for the task
    of neural machine translation as discussed in the previous section. So essentially
    the attention mechanism was applied to map the source and target sentence. As
    the seq-to-seq models perform the translation task token by token, the attention
    mechanism helps us to identify which token(s) from the source sentence to ***focus
    more on*** while generating token x for the target sentence. For this, it makes
    use of hidden state representations from encoders and decoders to calculate the
    attention scores and generate context vectors based on these scores as input for
    the decoder. If you wish to learn more about the Attention Mechanism, please hop
    on to [this article](/intuitive-understanding-of-attention-mechanism-in-deep-learning-6c9482aecf4f)
    (Brilliantly explained!).
  prefs: []
  type: TYPE_NORMAL
- en: Coming back to ***self-attention***, the main idea is to calculate the attention
    scores while mapping the source sentence to itself. If you have a sentence like,
  prefs: []
  type: TYPE_NORMAL
- en: '*“The boy did not cross the* ***road*** *because* ***it*** *was too wide.”*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: It is easy for us humans to understand that word “it” refers to “road” in the
    above sentence but how do we make our language model understand this relationship
    as well? That's where ***self-attention*** comes into the picture!
  prefs: []
  type: TYPE_NORMAL
- en: On a high level, every word in the sentence is compared against every other
    word in the sentence to quantify the relationships and understand the context.
    For representational purposes, you can refer to the figure below.
  prefs: []
  type: TYPE_NORMAL
- en: Let us see in detail how this self-attention is calculated (in real).
  prefs: []
  type: TYPE_NORMAL
- en: '*Generate embeddings for the input sentence*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Find embeddings of all the words and convert them into an input matrix. These
    embeddings can be generated via simple tokenisation and one-hot encoding or could
    be generated by embedding algorithms like BERT, etc. The ***dimension of the input
    matrix*** will be equal to the ***sentence length x embedding dimension***. Let
    us call this ***input matrix X*** for future reference.
  prefs: []
  type: TYPE_NORMAL
- en: '*Transform input matrix into Q, K & V*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For calculating self-attention, we need to transform X (input matrix) into
    three new matrices:'
  prefs: []
  type: TYPE_NORMAL
- en: '- Query (Q)'
  prefs: []
  type: TYPE_NORMAL
- en: '- Key (K)'
  prefs: []
  type: TYPE_NORMAL
- en: '- Value (V)'
  prefs: []
  type: TYPE_NORMAL
- en: To calculate these three matrices, we will randomly initialise three weight
    matrices namely ***Wq, Wk, & Wv***. The input matrix X will be multiplied with
    these weight matrices Wq, Wk, & Wv to obtain values for Q, K & V respectively.
    The optimal values for weight matrices will be learned during the process to obtain
    more accurate values for Q, K & V.
  prefs: []
  type: TYPE_NORMAL
- en: '*Calculate the dot product of Q and K-transpose*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: From the figure above, we can imply that qi, ki, and vi represent the values
    of Q, K, and V for the i-th word in the sentence.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/67fc6dc5181747aa4895072c410b31a6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig. Example of dot product of Q and K-transpose (Source: Author)'
  prefs: []
  type: TYPE_NORMAL
- en: The first row of the output matrix will tell you how word1 represented by q1
    is related to the rest of the words in the sentence using dot-product. The higher
    the value of the dot-product, the more related the words are. For intuition of
    why this dot product was calculated, you can understand Q (query) and K (key)
    matrices in terms of information retrieval. So here,
  prefs: []
  type: TYPE_NORMAL
- en: '- Q or Query = Term you are searching for'
  prefs: []
  type: TYPE_NORMAL
- en: '- K or Key = a set of keywords in your search engine against which Q is compared
    and matched.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Scale the dot-product*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As in the previous step, we are calculating the dot-product of two matrices
    i.e. performing a multiplication operation, there are chances that the value might
    explode. To make sure this does not happen and gradients are stabilised, we divide
    the dot product of Q and K-transpose by the square root of the embedding dimension
    (dk).
  prefs: []
  type: TYPE_NORMAL
- en: '*Normalise the values using softmax*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Normalisation using the softmax function will result in values between 0 and
    1\. The cells with high-scaled dot-product will be heightened furthermore whereas
    low values will be reduced making the distinction between matched word pairs clearer.
    The resultant output matrix can be regarded as a *score matrix S*.
  prefs: []
  type: TYPE_NORMAL
- en: '*Calculate the attention matrix Z*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The values matrix or V is multiplied by the score matrix S obtained from the
    previous step to calculate the attention matrix Z.
  prefs: []
  type: TYPE_NORMAL
- en: '*But wait, why multiply?*'
  prefs: []
  type: TYPE_NORMAL
- en: Suppose, Si = [0.9, 0.07, 0.03] is the score matrix value for i-th word from
    a sentence. This vector is multiplied with the V matrix to calculate Zi (attention
    matrix for i-th word).
  prefs: []
  type: TYPE_NORMAL
- en: '***Zi = [0.9 * V1 + 0.07 * V2 + 0.03 * V3]***'
  prefs: []
  type: TYPE_NORMAL
- en: Can we say that for understanding the context of i-th word, we should only focus
    on word1 (i.e. V1) as 90% of the value of attention score is coming from V1? We
    could clearly define the important words where ***more attention*** should be
    paid to understand the context of i-th word.
  prefs: []
  type: TYPE_NORMAL
- en: Hence, we can conclude that the higher the contribution of a word in the Zi
    representation, the more critical and related the words are to one another.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we know how to calculate the self-attention matrix, let us understand
    the concept of the ***multi-head attention mechanism***.
  prefs: []
  type: TYPE_NORMAL
- en: '***Multi-head attention Mechanism***'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: What will happen if your score matrix is biased toward a specific word representation?
    It will mislead your model and the results will not be as accurate as we expect.
    Let us see an example to understand this better.
  prefs: []
  type: TYPE_NORMAL
- en: 'S1: “*All is well*”'
  prefs: []
  type: TYPE_NORMAL
- en: Z(well) = 0.6 * V(all) + 0.0 * v(is) + 0.4 * V(well)
  prefs: []
  type: TYPE_NORMAL
- en: 'S2: “*The dog ate the food because it was hungry*”'
  prefs: []
  type: TYPE_NORMAL
- en: Z(it) = 0.0 * V(the) + 1.0 * V(dog) + 0.0 * V(ate) + …… + 0.0 * V(hungry)
  prefs: []
  type: TYPE_NORMAL
- en: In S1 case, while calculating Z(well), more importance is given to V(all). It
    is even more than V(well) itself. There is no guarantee how accurate this will
    be.
  prefs: []
  type: TYPE_NORMAL
- en: In the S2 case, while calculating Z(it), all the importance is given to V(dog)
    whereas the scores for the rest of the words are 0.0 including V(it) as well.
    This looks acceptable as the “it” word is ambiguous. It makes sense to relate
    it more to another word than the word itself. That was the whole purpose of this
    exercise of calculating self-attention. To handle the context of ambiguous words
    in the input sentences.
  prefs: []
  type: TYPE_NORMAL
- en: In other words, we can say that if the current word is ambiguous then it is
    okay to give more importance to some other word while calculating self-attention
    but in other cases, it can be misleading for the model. So, what do we do now?
  prefs: []
  type: TYPE_NORMAL
- en: '*What if we calculate multiple attention matrices instead of calculating one
    attention matrix and derive the final attention matrix from these?*'
  prefs: []
  type: TYPE_NORMAL
- en: That is precisely what ***multi-head attention*** is all about! We calculate
    multiple versions of attention matrices z1, z2, z3, ….., zm and concatenate them
    to derive the final attention matrix. That way we can be more confident about
    our attention matrix.
  prefs: []
  type: TYPE_NORMAL
- en: Moving on to the next important concept,
  prefs: []
  type: TYPE_NORMAL
- en: Positional Encoding
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In seq-to-seq models, the input sentence is fed word by word to the network
    which allows the model to track the positions of words relative to other words.
  prefs: []
  type: TYPE_NORMAL
- en: But in transformer models, we follow a different approach. Instead of giving
    inputs word by word, they are fed parallel-y which helps in reducing the training
    time and learning long-term dependency. But with this approach, the word order
    is lost. However, to understand the meaning of a sentence correctly, word order
    is extremely important. To overcome this problem, a new matrix called “***positional
    encoding***” (P) is introduced.
  prefs: []
  type: TYPE_NORMAL
- en: This matrix P is sent along with input matrix X to include the information related
    to the word order. For obvious reasons, the dimensions of X and P matrices are
    the same.
  prefs: []
  type: TYPE_NORMAL
- en: To calculate positional encoding, the formula given below is used.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/cd3612f39271e47035921e579b010bfb.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig. Formula to calculate positional encoding (Source: Author)'
  prefs: []
  type: TYPE_NORMAL
- en: In the above formula,
  prefs: []
  type: TYPE_NORMAL
- en: '**pos** = position of the word in the sentence'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**d** = dimension of the word/token embedding'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**i** = represents each dimension in the embedding'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In calculations, d is fixed but pos and i vary. If d=512, then i ∈ [0, 255]
    as we take 2i.
  prefs: []
  type: TYPE_NORMAL
- en: This video covers positional encoding in-depth if you wish to know more about
    it.
  prefs: []
  type: TYPE_NORMAL
- en: '[Visual Guide to Transformer Neural Networks — (Part 1) Position Embeddings](https://www.youtube.com/watch?v=dichIcUZfOw)'
  prefs: []
  type: TYPE_NORMAL
- en: I am using some visuals from the above video to explain this concept in my words.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ea8f94415a5cf0c2c2f9df3d73b615ab.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig. Positional Encoding Vector Representation (Source: Author)'
  prefs: []
  type: TYPE_NORMAL
- en: The above figure shows an example of a positional encoding vector along with
    different variable values.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ab2f78bca8ce05315cae5718b316e410.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig. Positional Encoding Vectors with constant i and d (Source: Author)'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/396c07028afbefbe8e9c4d543febaa9e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig. Positional Encoding Vectors with constant i and d (Source: Author)'
  prefs: []
  type: TYPE_NORMAL
- en: The above figure shows how the values of ***PE(pos, 2i)*** will vary *if i is
    constant and only pos varies*. As we know the ***sinusoidal wave*** is a periodic
    function that tends to repeat itself after a fixed interval. We can see that the
    encoding vectors for pos = 0 and pos = 6 are identical. This is not desirable
    as we would want *different positional encoding vectors for different values of
    pos*.
  prefs: []
  type: TYPE_NORMAL
- en: This can be achieved by *varying the frequency of the sinusoidal wave.*
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8510db69b50d4b69bb7d67905eb17508.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig. Positional Encoding Vectors with varying pos and i (Source: Author)'
  prefs: []
  type: TYPE_NORMAL
- en: As the value of i varies, the frequency of sinusoidal waves also varies resulting
    in different waves and hence, resulting in different values for every positional
    encoding vector. This is exactly what we wanted to achieve.
  prefs: []
  type: TYPE_NORMAL
- en: The positional encoding matrix (P) is added to the input matrix (X) and fed
    to the encoder.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/85aa9a8bf6e14434b531874306780d2e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig. Adding positional encoding to the input embedding (Source: Author)'
  prefs: []
  type: TYPE_NORMAL
- en: The next component of the encoder is the **feedforward network**.
  prefs: []
  type: TYPE_NORMAL
- en: Feedforward Network
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This sublayer in the encoder block is the classic neural network with two dense
    layers and ReLU activations. It accepts input from the multi-head attention layer,
    performs some non-linear transformations on the same and finally generates contextualised
    vectors. The fully-connected layer is responsible for considering each attention
    head and learning relevant information from them. Since the attention vectors
    are independent of each other, they can be passed to the transformer network in
    a parallelised way.
  prefs: []
  type: TYPE_NORMAL
- en: The last and final component of the Encoder block is ***Add & Norm component***.
  prefs: []
  type: TYPE_NORMAL
- en: '**Add & Norm component**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This is a *residual layer* followed by *layer normalisation*. The residual layer
    ensures that no important information related to the input of sub-layers is lost
    in the processing. While the normalisation layer promotes faster model training
    and prevents the values from changing heavily.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8445cba56bcc14616fbd25b49285cc98.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig. Encoder components with Add & Norm layers included (Source: Author)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Within the encoder, there are two add & norm layers:'
  prefs: []
  type: TYPE_NORMAL
- en: connects the input of the multi-head attention sub-layer to its output
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: connects the input of the feedforward network sub-layer to its output
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'With this, we conclude the internal working of the Encoders. To summarize the
    article, let us quickly go over the steps that the encoder uses:'
  prefs: []
  type: TYPE_NORMAL
- en: Generate embeddings or tokenized representations of the input sentence. This
    will be our input matrix X.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generate the positional embeddings to preserve the information related to the
    word order of the input sentence and add it to the input matrix X.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Randomly initialize three matrices: Wq, Wk, & Wvi.e. weights of query, key
    & value. These weights will be updated during the training of the transformer
    model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Multiply the input matrix X with each of Wq, Wk, & Wv to generate Q (query),
    K (key) and V (value) matrices.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Calculate the dot product of Q and K-transpose, scale the product by dividing
    it with the square root of dk or embedding dimension and finally normalize it
    using the softmax function.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Calculate the attention matrix Z by multiplying the V or value matrix with the
    output of the softmax function.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pass this attention matrix to the feedforward network to perform non-linear
    transformations and generate contextualized embeddings.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the next article, we will understand how the Decoder component of the Transformer
    model works.
  prefs: []
  type: TYPE_NORMAL
- en: This would be all for this article. I hope you found it useful. If you did,
    please don’t forget to clap and share it with your friends.
  prefs: []
  type: TYPE_NORMAL
