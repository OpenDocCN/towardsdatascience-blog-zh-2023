- en: Local Light Field Fusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/local-light-field-fusion-14c07ed36117?source=collection_archive---------18-----------------------#2023-04-11](https://towardsdatascience.com/local-light-field-fusion-14c07ed36117?source=collection_archive---------18-----------------------#2023-04-11)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: How to render 3D scenes on a smart phone
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://wolfecameron.medium.com/?source=post_page-----14c07ed36117--------------------------------)[![Cameron
    R. Wolfe, Ph.D.](../Images/52bb88d7cf1105501be2fae5ccbe7a03.png)](https://wolfecameron.medium.com/?source=post_page-----14c07ed36117--------------------------------)[](https://towardsdatascience.com/?source=post_page-----14c07ed36117--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----14c07ed36117--------------------------------)
    [Cameron R. Wolfe, Ph.D.](https://wolfecameron.medium.com/?source=post_page-----14c07ed36117--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F28aa6026c553&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flocal-light-field-fusion-14c07ed36117&user=Cameron+R.+Wolfe%2C+Ph.D.&userId=28aa6026c553&source=post_page-28aa6026c553----14c07ed36117---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----14c07ed36117--------------------------------)
    ·12 min read·Apr 11, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F14c07ed36117&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flocal-light-field-fusion-14c07ed36117&user=Cameron+R.+Wolfe%2C+Ph.D.&userId=28aa6026c553&source=-----14c07ed36117---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F14c07ed36117&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flocal-light-field-fusion-14c07ed36117&source=-----14c07ed36117---------------------bookmark_footer-----------)![](../Images/d1c22270367a41b360ab04e39462e1a2.png)'
  prefs: []
  type: TYPE_NORMAL
- en: (Photo by [Clyde He](https://unsplash.com/@clyde_he?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    on [Unsplash](https://unsplash.com/backgrounds/colors/light?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText))
  prefs: []
  type: TYPE_NORMAL
- en: By now, we should know that deep learning is a great way to represent 3D scenes
    and generate new renderings of these scenes from arbitrary viewpoints. The problem
    with the approaches we have seen so far (e.g., [ONets](https://cameronrwolfe.substack.com/p/shape-reconstruction-with-onets)
    and [SRNs](https://cameronrwolfe.substack.com/p/scene-representation-networks)
    [2, 3], however, is that they require many images of the underlying scene to be
    available to train the model. With this in mind, we might wonder whether it’s
    possible to obtain a deep learning-based scene representation with fewer samples
    of the underlying scene. *How many images do we actually need to train a high-resolution
    scene representation?*
  prefs: []
  type: TYPE_NORMAL
- en: 'This question was addressed and answered by the Local Light Field Fusion (LLFF)
    [1] approach for synthesizing scenes in 3D. An extension of [light field](http://lightfield-forum.com/what-is-the-lightfield/)
    rendering [4], LLFFs generate scene viewpoints by expanding several sets of existing
    views into multi-plane image (MPI) representations, then rendering a new viewpoint
    by blending these representations together. The resulting method:'
  prefs: []
  type: TYPE_NORMAL
- en: Accurately models complex scenes and effects like reflections.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Is theoretically shown to reduce the number samples/images required to produce
    an accurate scene representation.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Plus, *LLFFs are prescriptive*, meaning that the framework can be used to tell
    users how many and what type of images…
  prefs: []
  type: TYPE_NORMAL
