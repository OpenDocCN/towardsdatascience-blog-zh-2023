- en: Local Light Field Fusion
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 本地光场融合
- en: 原文：[https://towardsdatascience.com/local-light-field-fusion-14c07ed36117?source=collection_archive---------18-----------------------#2023-04-11](https://towardsdatascience.com/local-light-field-fusion-14c07ed36117?source=collection_archive---------18-----------------------#2023-04-11)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/local-light-field-fusion-14c07ed36117?source=collection_archive---------18-----------------------#2023-04-11](https://towardsdatascience.com/local-light-field-fusion-14c07ed36117?source=collection_archive---------18-----------------------#2023-04-11)
- en: How to render 3D scenes on a smart phone
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何在智能手机上渲染3D场景
- en: '[](https://wolfecameron.medium.com/?source=post_page-----14c07ed36117--------------------------------)[![Cameron
    R. Wolfe, Ph.D.](../Images/52bb88d7cf1105501be2fae5ccbe7a03.png)](https://wolfecameron.medium.com/?source=post_page-----14c07ed36117--------------------------------)[](https://towardsdatascience.com/?source=post_page-----14c07ed36117--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----14c07ed36117--------------------------------)
    [Cameron R. Wolfe, Ph.D.](https://wolfecameron.medium.com/?source=post_page-----14c07ed36117--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://wolfecameron.medium.com/?source=post_page-----14c07ed36117--------------------------------)[![Cameron
    R. Wolfe, Ph.D.](../Images/52bb88d7cf1105501be2fae5ccbe7a03.png)](https://wolfecameron.medium.com/?source=post_page-----14c07ed36117--------------------------------)[](https://towardsdatascience.com/?source=post_page-----14c07ed36117--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----14c07ed36117--------------------------------)
    [Cameron R. Wolfe, Ph.D.](https://wolfecameron.medium.com/?source=post_page-----14c07ed36117--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F28aa6026c553&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flocal-light-field-fusion-14c07ed36117&user=Cameron+R.+Wolfe%2C+Ph.D.&userId=28aa6026c553&source=post_page-28aa6026c553----14c07ed36117---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----14c07ed36117--------------------------------)
    ·12 min read·Apr 11, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F14c07ed36117&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flocal-light-field-fusion-14c07ed36117&user=Cameron+R.+Wolfe%2C+Ph.D.&userId=28aa6026c553&source=-----14c07ed36117---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F28aa6026c553&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flocal-light-field-fusion-14c07ed36117&user=Cameron+R.+Wolfe%2C+Ph.D.&userId=28aa6026c553&source=post_page-28aa6026c553----14c07ed36117---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----14c07ed36117--------------------------------)
    ·12分钟阅读·2023年4月11日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F14c07ed36117&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flocal-light-field-fusion-14c07ed36117&user=Cameron+R.+Wolfe%2C+Ph.D.&userId=28aa6026c553&source=-----14c07ed36117---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F14c07ed36117&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flocal-light-field-fusion-14c07ed36117&source=-----14c07ed36117---------------------bookmark_footer-----------)![](../Images/d1c22270367a41b360ab04e39462e1a2.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F14c07ed36117&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flocal-light-field-fusion-14c07ed36117&source=-----14c07ed36117---------------------bookmark_footer-----------)![](../Images/d1c22270367a41b360ab04e39462e1a2.png)'
- en: (Photo by [Clyde He](https://unsplash.com/@clyde_he?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    on [Unsplash](https://unsplash.com/backgrounds/colors/light?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText))
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: （照片由 [Clyde He](https://unsplash.com/@clyde_he?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    提供，来源于 [Unsplash](https://unsplash.com/backgrounds/colors/light?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)）
- en: By now, we should know that deep learning is a great way to represent 3D scenes
    and generate new renderings of these scenes from arbitrary viewpoints. The problem
    with the approaches we have seen so far (e.g., [ONets](https://cameronrwolfe.substack.com/p/shape-reconstruction-with-onets)
    and [SRNs](https://cameronrwolfe.substack.com/p/scene-representation-networks)
    [2, 3], however, is that they require many images of the underlying scene to be
    available to train the model. With this in mind, we might wonder whether it’s
    possible to obtain a deep learning-based scene representation with fewer samples
    of the underlying scene. *How many images do we actually need to train a high-resolution
    scene representation?*
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 到现在为止，我们应该知道深度学习是表示3D场景和从任意视点生成这些场景新渲染的好方法。然而，我们迄今为止看到的方法（例如，[ONets](https://cameronrwolfe.substack.com/p/shape-reconstruction-with-onets)和[SRNs](https://cameronrwolfe.substack.com/p/scene-representation-networks)
    [2, 3]）的问题在于，它们需要大量场景图像来训练模型。考虑到这一点，我们可能会想是否有可能使用更少的底层场景样本来获得基于深度学习的场景表示。*我们实际需要多少图像来训练高分辨率的场景表示？*
- en: 'This question was addressed and answered by the Local Light Field Fusion (LLFF)
    [1] approach for synthesizing scenes in 3D. An extension of [light field](http://lightfield-forum.com/what-is-the-lightfield/)
    rendering [4], LLFFs generate scene viewpoints by expanding several sets of existing
    views into multi-plane image (MPI) representations, then rendering a new viewpoint
    by blending these representations together. The resulting method:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 这个问题由局部光场融合（LLFF）[1]方法解决，该方法用于在3D中合成场景。LLFF是[光场](http://lightfield-forum.com/what-is-the-lightfield/)渲染[4]的扩展，LLFF通过将几组现有视图扩展为多平面图像（MPI）表示，生成场景视点，然后通过混合这些表示来渲染新视点。生成的方法：
- en: Accurately models complex scenes and effects like reflections.
  id: totrans-11
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 准确地建模复杂场景和效果，如反射。
- en: Is theoretically shown to reduce the number samples/images required to produce
    an accurate scene representation.
  id: totrans-12
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 理论上，减少了生成准确场景表示所需的样本/图像数量。
- en: Plus, *LLFFs are prescriptive*, meaning that the framework can be used to tell
    users how many and what type of images…
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，*LLFF是规定性的*，这意味着该框架可以用于告知用户需要多少和什么类型的图像……
