- en: 'ReLoRa: Pre-train a Large Language Model on Your GPU'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/relora-pre-train-a-large-language-model-on-your-gpu-d104756f9ddf?source=collection_archive---------3-----------------------#2023-07-20](https://towardsdatascience.com/relora-pre-train-a-large-language-model-on-your-gpu-d104756f9ddf?source=collection_archive---------3-----------------------#2023-07-20)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: LoRa but with multiple resets in a row
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@bnjmn_marie?source=post_page-----d104756f9ddf--------------------------------)[![Benjamin
    Marie](../Images/3ea1ad230cb1e67610418a8e36a5e5dd.png)](https://medium.com/@bnjmn_marie?source=post_page-----d104756f9ddf--------------------------------)[](https://towardsdatascience.com/?source=post_page-----d104756f9ddf--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----d104756f9ddf--------------------------------)
    [Benjamin Marie](https://medium.com/@bnjmn_marie?source=post_page-----d104756f9ddf--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fad2a414578b3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frelora-pre-train-a-large-language-model-on-your-gpu-d104756f9ddf&user=Benjamin+Marie&userId=ad2a414578b3&source=post_page-ad2a414578b3----d104756f9ddf---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----d104756f9ddf--------------------------------)
    ·8 min read·Jul 20, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fd104756f9ddf&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frelora-pre-train-a-large-language-model-on-your-gpu-d104756f9ddf&user=Benjamin+Marie&userId=ad2a414578b3&source=-----d104756f9ddf---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fd104756f9ddf&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frelora-pre-train-a-large-language-model-on-your-gpu-d104756f9ddf&source=-----d104756f9ddf---------------------bookmark_footer-----------)![](../Images/a84220fa206495cacbda94863cf4dfe4.png)'
  prefs: []
  type: TYPE_NORMAL
- en: The ReLoRa framework — Image by the author
  prefs: []
  type: TYPE_NORMAL
- en: In 2021, [Hu et al.](https://arxiv.org/abs/2106.09685) proposed low-rank adapters
    (LoRa) for LLMs. This method significantly reduces the cost of fine-tuning large
    language models (LLMs) by only training a few added parameters (low-rank networks)
    while keeping the LLM’s original parameters (high-rank networks) frozen.
  prefs: []
  type: TYPE_NORMAL
- en: With LoRa, we still need an existing pre-trained model to fine-tune, i.e., it
    can’t pre-train a good LLM from scratch due to the low-rank restrictions. It leaves
    pre-training unaffordable for most individuals and organizations.
  prefs: []
  type: TYPE_NORMAL
- en: To reduce this cost, [Lialin et al. (2023)](https://arxiv.org/pdf/2307.05695.pdf)
    propose ReLoRa. This is a modification of LoRa that allows pre-training LLMs from
    scratch.
  prefs: []
  type: TYPE_NORMAL
- en: In this article, I first explain how ReLoRa works. Then, I analyze and comment
    on the results presented in the scientific paper describing ReLoRa. In the last
    section, I show how to set up and run ReLoRa on your computer.
  prefs: []
  type: TYPE_NORMAL
- en: '*Note about the licenses:* [*The scientific paper published on arXiv*](https://arxiv.org/abs/2307.05695)
    *and describing ReLoRa is distributed under a CC BY 4.0 license.* [*The source
    code of ReLoRa is published on GitHub*](https://github.com/guitaricet/peft_pretraining)
    *and distributed under an Apache 2.0 license allowing commercial use.*'
  prefs: []
  type: TYPE_NORMAL
- en: 'ReLoRa: from low-rank to high-rank networks'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
