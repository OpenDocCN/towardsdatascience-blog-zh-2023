- en: 'From Paper to Pixel: Evaluating the Best Techniques for Digitising Handwritten
    Texts'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从纸张到像素：评估数字化手写文本的最佳技术
- en: 原文：[https://towardsdatascience.com/from-paper-to-pixel-evaluating-the-best-techniques-for-digitising-handwritten-texts-9cc39e7a457d?source=collection_archive---------2-----------------------#2023-09-14](https://towardsdatascience.com/from-paper-to-pixel-evaluating-the-best-techniques-for-digitising-handwritten-texts-9cc39e7a457d?source=collection_archive---------2-----------------------#2023-09-14)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/from-paper-to-pixel-evaluating-the-best-techniques-for-digitising-handwritten-texts-9cc39e7a457d?source=collection_archive---------2-----------------------#2023-09-14](https://towardsdatascience.com/from-paper-to-pixel-evaluating-the-best-techniques-for-digitising-handwritten-texts-9cc39e7a457d?source=collection_archive---------2-----------------------#2023-09-14)
- en: A Comparative Dive into OCR, Transformer Models, and Prompt Engineering-based
    Ensemble Techniques
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 对OCR、变换器模型和基于提示工程的集成技术的比较分析
- en: '[](https://medium.com/@RedjaiSani?source=post_page-----9cc39e7a457d--------------------------------)[![Sohrab
    Sani](../Images/b4a9309f91479873b59d0e182ddd4128.png)](https://medium.com/@RedjaiSani?source=post_page-----9cc39e7a457d--------------------------------)[](https://towardsdatascience.com/?source=post_page-----9cc39e7a457d--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----9cc39e7a457d--------------------------------)
    [Sohrab Sani](https://medium.com/@RedjaiSani?source=post_page-----9cc39e7a457d--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@RedjaiSani?source=post_page-----9cc39e7a457d--------------------------------)[![Sohrab
    Sani](../Images/b4a9309f91479873b59d0e182ddd4128.png)](https://medium.com/@RedjaiSani?source=post_page-----9cc39e7a457d--------------------------------)[](https://towardsdatascience.com/?source=post_page-----9cc39e7a457d--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----9cc39e7a457d--------------------------------)
    [Sohrab Sani](https://medium.com/@RedjaiSani?source=post_page-----9cc39e7a457d--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fc7a4f1e52b82&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffrom-paper-to-pixel-evaluating-the-best-techniques-for-digitising-handwritten-texts-9cc39e7a457d&user=Sohrab+Sani&userId=c7a4f1e52b82&source=post_page-c7a4f1e52b82----9cc39e7a457d---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----9cc39e7a457d--------------------------------)
    ·16 min read·Sep 14, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F9cc39e7a457d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffrom-paper-to-pixel-evaluating-the-best-techniques-for-digitising-handwritten-texts-9cc39e7a457d&user=Sohrab+Sani&userId=c7a4f1e52b82&source=-----9cc39e7a457d---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fc7a4f1e52b82&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffrom-paper-to-pixel-evaluating-the-best-techniques-for-digitising-handwritten-texts-9cc39e7a457d&user=Sohrab+Sani&userId=c7a4f1e52b82&source=post_page-c7a4f1e52b82----9cc39e7a457d---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----9cc39e7a457d--------------------------------)
    ·16分钟阅读·2023年9月14日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F9cc39e7a457d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffrom-paper-to-pixel-evaluating-the-best-techniques-for-digitising-handwritten-texts-9cc39e7a457d&user=Sohrab+Sani&userId=c7a4f1e52b82&source=-----9cc39e7a457d---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F9cc39e7a457d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffrom-paper-to-pixel-evaluating-the-best-techniques-for-digitising-handwritten-texts-9cc39e7a457d&source=-----9cc39e7a457d---------------------bookmark_footer-----------)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F9cc39e7a457d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffrom-paper-to-pixel-evaluating-the-best-techniques-for-digitising-handwritten-texts-9cc39e7a457d&source=-----9cc39e7a457d---------------------bookmark_footer-----------)'
- en: 'By: [Sohrab Sani](https://medium.com/u/c7a4f1e52b82?source=post_page-----9cc39e7a457d--------------------------------)
    and [Diego Capozzi](https://medium.com/u/d2b5153934d4?source=post_page-----9cc39e7a457d--------------------------------)'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '作者: [Sohrab Sani](https://medium.com/u/c7a4f1e52b82?source=post_page-----9cc39e7a457d--------------------------------)
    和 [Diego Capozzi](https://medium.com/u/d2b5153934d4?source=post_page-----9cc39e7a457d--------------------------------)'
- en: 'Organisations have long grappled with the tedious and expensive task of digitising
    historical handwritten documents. Previously, Optical Character Recognition (OCR)
    techniques, such as AWS Textract (TT) [1] and Azure Form Recognizer (FR) [2],
    have led the charge for this. Although these options may be widely available,
    they have many downsides: they’re pricey, require lengthy data processing/cleaning
    and can yield suboptimal accuracies. Recent Deep Learning advancements in image
    segmentation and Natural Language Processing that utilise transformer-based architecture
    have enabled the development of OCR-free techniques, such as the Document Understanding
    Transformer (Donut)[3] model.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 组织长期以来一直在处理数字化历史手写文档这一繁琐且昂贵的任务。之前，光学字符识别（OCR）技术，如 AWS Textract (TT) [1] 和 Azure
    表单识别器 (FR) [2]，在这方面领先。虽然这些选项可能广泛可用，但它们有许多缺点：价格高、需要长时间的数据处理/清洗，并且准确性可能不尽如人意。最近，利用
    Transformer 架构的深度学习进展在图像分割和自然语言处理领域，使得 OCR 自由的技术得以发展，例如 Document Understanding
    Transformer (Donut)[3] 模型。
- en: In this study, we’ll compare OCR and Transformer-based techniques for this digitisation
    process with our custom dataset, which was created from a series of handwritten
    forms. Benchmarking for this relatively simple task is intended to lead towards
    more complex applications on longer, handwritten documents. To increase accuracy,
    we also explored using an ensemble approach by utilising prompt engineering with
    the gpt-3.5-turbo Large Language Model (LLM) to combine the outputs of TT and
    the fine-tuned Donut model.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项研究中，我们将使用自定义数据集比较 OCR 和基于 Transformer 的技术，该数据集由一系列手写表单创建。对这一相对简单任务的基准测试旨在引导向更复杂的应用，特别是在更长的手写文档上。为了提高准确性，我们还探索了利用提示工程结合
    gpt-3.5-turbo 大型语言模型（LLM）的集成方法，以结合 TT 和微调的 Donut 模型的输出。
- en: The code for this work can be viewed in [this](https://github.com/srsani/hvdu)
    GitHub repository. The dataset is available on our Hugging Face repository [here](https://huggingface.co/datasets/ift/handwriting_forms).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 这项工作的代码可以在[这个](https://github.com/srsani/hvdu) GitHub 仓库中查看。数据集可以在我们的 Hugging
    Face 仓库[这里](https://huggingface.co/datasets/ift/handwriting_forms)获得。
- en: '**Table of Contents:**'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '**目录：**'
- en: · [Dataset creation](#d01a)
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: · [数据集创建](#d01a)
- en: · [Methods](#7cc5)
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: · [方法](#7cc5)
- en: ∘ [Azure Form Recognizer (FR)](#7956)
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: ∘ [Azure 表单识别器 (FR)](#7956)
- en: ∘ [AWS Textract (TT)](#3c12)
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: ∘ [AWS Textract (TT)](#3c12)
- en: ∘ [Donut](#59bf)
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: ∘ [Donut](#59bf)
- en: '∘ [Ensemble Method: TT, Donut, GPT](#c3ca)'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: ∘ [集成方法：TT、Donut、GPT](#c3ca)
- en: · [Measurement of Model Performance](#48df)
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: · [模型性能测量](#48df)
- en: ∘ [FLA](#d926)
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: ∘ [FLA](#d926)
- en: ∘ [CBA](#0510)
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: ∘ [CBA](#0510)
- en: ∘ [Coverage](#c673)
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: ∘ [覆盖范围](#c673)
- en: ∘ [Cost](#96fa)
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: ∘ [成本](#96fa)
- en: · [Results](#79c4)
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: · [结果](#79c4)
- en: · [Additional Considerations](#266a)
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: · [其他考虑因素](#266a)
- en: ∘ [Donut model training](#dc66)
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: ∘ [Donut 模型训练](#dc66)
- en: ∘ [Prompt engineering variability](#682d)
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: ∘ [提示工程的变异性](#682d)
- en: · [Conclusion](#0887)
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: · [结论](#0887)
- en: · [Next Steps](#13a4)
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: · [下一步](#13a4)
- en: · [References](#15bb)
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: · [参考文献](#15bb)
- en: · [Acknowledgements](#e00f)
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: · [致谢](#e00f)
- en: Dataset creation
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据集创建
- en: This study created a custom dataset from 2100 handwritten-form images from the
    NIST Special Database 19 dataset [4]. Figure 1 provides a sample image of one
    of these forms. The final collection includes 2099 forms. To curate this dataset,
    we cropped the top section of each NIST form, targeting the DATE, CITY, STATE,
    and ZIP CODE (now referred to as “ZIP”) keys highlighted within the red box [Figure
    1]. This approach launched the benchmarking process with a relatively simple text-extraction
    task, enabling us to then select and manually label the dataset quickly. At the
    time of writing, we are unaware of any publicly available datasets with labelled
    images of handwritten forms that could be used for JSON key-field text extractions.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 本研究从 NIST 特殊数据库 19 数据集[4]的 2100 张手写表单图像中创建了一个自定义数据集。图 1 提供了这些表单之一的样本图像。最终的集合包括
    2099 张表单。为了整理这个数据集，我们裁剪了每个 NIST 表单的顶部部分，目标是红框中高亮的 DATE、CITY、STATE 和 ZIP CODE（现在称为“ZIP”）键[图
    1]。这种方法启动了基准测试过程，进行了一项相对简单的文本提取任务，使我们能够快速选择和手动标记数据集。在撰写时，我们不知道有任何公开的带标记的手写表单图像数据集可以用于
    JSON 键字段文本提取。
- en: '![](../Images/617a9f0dc196f1b3434e2561b8a7f795.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/617a9f0dc196f1b3434e2561b8a7f795.png)'
- en: '**Figure 1.** Example form from the NIST Special Database 19 dataset. The red
    box identifies the cropping process, which selects only the DATE, CITY, STATE,
    and ZIP fields in this form. (Image by the authors)'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 1.** 来自 NIST 特殊数据库 19 数据集的示例表单。红框标识了裁剪过程，该过程仅选择此表单中的 DATE、CITY、STATE 和
    ZIP 字段。（作者提供的图像）'
- en: We manually extracted values for each key from the documents and double-checked
    these for accuracy. In total, 68 forms were discarded for containing at least
    one illegible character. Characters from the forms were recorded exactly as they
    appeared, regardless of spelling errors or formatting inconsistencies.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 我们手动从文档中提取每个键的值，并仔细检查其准确性。总共丢弃了68个表单，因为它们包含至少一个无法辨认的字符。表单中的字符被记录为实际出现的样子，无论拼写错误或格式不一致。
- en: To fine-tune the Donut model on missing data, we added 67 empty forms that would
    enable training for these empty fields. Missing values within the forms are represented
    as a “None” string in the JSON output.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 为了对Donut模型进行微调以处理缺失数据，我们添加了67个空表单，以便为这些空字段进行训练。表单中的缺失值在JSON输出中表示为“None”字符串。
- en: Figure 2a displays a sample form from our dataset, while Figure 2b shares the
    corresponding JSON that is then linked to that form.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 图2a显示了我们数据集中的一个样本表单，而图2b则展示了与该表单相关的JSON。
- en: '![](../Images/6f923942368211ce6bcb195a41d4d2f7.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6f923942368211ce6bcb195a41d4d2f7.png)'
- en: '**Figure 2.** (a) Example image from the dataset; (b) Extracted data in a JSON
    format. (Image by the authors)'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '**图2。** (a) 数据集中的示例图像；(b) JSON格式的提取数据。（图像由作者提供）'
- en: Table 1 provides a breakdown of variability within the dataset for each key.
    From most to least variable, the order is ZIP, CITY, DATE, and STATE. All dates
    were within the year 1989, which may have reduced overall DATE variability. Additionally,
    although there are only 50 US states, STATE variability was increased due to different
    acronyms or case-sensitive spellings that were used for individual state entries.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 表1提供了数据集中每个键的变异性分解。从最具变异性到最少的顺序是ZIP、CITY、DATE和STATE。所有日期均在1989年，这可能降低了整体DATE的变异性。此外，尽管美国只有50个州，但由于不同的首字母缩略词或大小写敏感的拼写，STATE的变异性有所增加。
- en: '![](../Images/56c0e7820a90665b0da77890964b931f.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/56c0e7820a90665b0da77890964b931f.png)'
- en: '**Table 1.** Summary Statistics of the Dataset. (Image by the authors)'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '**表1。** 数据集的总结统计。（图像由作者提供）'
- en: Table 2 summarises character lengths for various attributes of our dataset.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 表2总结了我们数据集中各种属性的字符长度。
- en: '![](../Images/0ed712f724804866090125286a9cda55.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0ed712f724804866090125286a9cda55.png)'
- en: '**Table 2\.** Summary of character length and distribution. (Image by the authors)'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '**表2。** 字符长度和分布的总结。（图像由作者提供）'
- en: The above data shows that CITY entries possessed the longest character length
    while STATE entries had the shortest. The median values for each entry closely
    follow their respective means, indicating a relatively uniform distribution of
    character lengths around the average for each category.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 上述数据表明，CITY条目的字符长度最长，而STATE条目的字符长度最短。每个条目的中位数值紧随其各自的均值，表明各类别的字符长度围绕平均值的分布相对均匀。
- en: 'After annotating the data, we split it into three subsets: training, validation,
    and testing, with respective sample sizes of 1400, 199, and 500\. Here is a [link](https://github.com/srsani/hvdu/blob/main/src/notebooks/3_0_make_train_val_test.ipynb)to
    the notebook that we used for this.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 数据标注后，我们将其分为三个子集：训练集、验证集和测试集，样本量分别为1400、199和500。这里是一个[链接](https://github.com/srsani/hvdu/blob/main/src/notebooks/3_0_make_train_val_test.ipynb)，用于此笔记本。
- en: Methods
  id: totrans-49
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 方法
- en: We will now expand on each method that we tested and link these to relevant
    Python codes which contain more details. The application of methods is first described
    individually, i.e. FR, TT and Donut, and then secondarily, with the TT+GPT+Donut
    ensemble approach.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将详细阐述每种测试方法，并将其与包含更多细节的相关Python代码关联起来。方法的应用首先单独描述，即FR、TT和Donut，然后是TT+GPT+Donut集成方法。
- en: Azure Form Recognizer (FR)
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Azure Form Recognizer (FR)
- en: 'Figure 3 depicts the workflow for extracting handwritten text from our form
    images using Azure FR:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 图3展示了使用Azure FR从表单图像中提取手写文本的工作流程。
- en: '**Store the images:** This could be on a local drive or another solution, such
    as an S3 bucket or Azure Blob Storage.'
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**存储图像：** 可以存储在本地驱动器或其他解决方案中，如S3桶或Azure Blob存储。'
- en: '**Azure SDK**: Python script for loading each image from storage and transferring
    these to the FR API via Azure SDK.'
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**Azure SDK**：用于从存储中加载每个图像并通过Azure SDK将其传输到FR API的Python脚本。'
- en: '**Post-processing**: Using an off-the-shelf method means that the final output
    often needs refining. Here are the 21 extracted keys that require further processing:'
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**后处理**：使用现成的方法意味着最终输出通常需要精细调整。以下是21个需要进一步处理的提取键：'
- en: '[ ‘DATE’, ‘CITY’, ‘STATE’, ‘’DATE’, ‘ZIP’, ‘NAME’, ‘E ZIP’,’·DATE’, ‘.DATE’,
    ‘NAMR’, ‘DATE®’, ‘NAMA’, ‘_ZIP’, ‘.ZIP’, ‘print the following shopsataca i’, ‘-DATE’,
    ‘DATE.’, ‘No.’, ‘NAMN’, ‘STATE\nZIP’]'
  id: totrans-56
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[ ‘DATE’, ‘CITY’, ‘STATE’, ‘’DATE’, ‘ZIP’, ‘NAME’, ‘E ZIP’,’·DATE’, ‘.DATE’,
    ‘NAMR’, ‘DATE®’, ‘NAMA’, ‘_ZIP’, ‘.ZIP’, ‘print the following shopsataca i’, ‘-DATE’,
    ‘DATE.’, ‘No.’, ‘NAMN’, ‘STATE\nZIP’]'
- en: Some keys have extra dots or underscores, which require removal. Due to the
    close positioning of the text within the forms, there are numerous instances where
    extracted values are mistakenly associated with incorrect keys. These issues are
    then addressed to a reasonable extent.
  id: totrans-57
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 一些键有额外的点或下划线，需要去除。由于文本在表单中的位置非常接近，提取的值有许多实例被错误地关联到不正确的键。这些问题随后会得到合理程度的解决。
- en: '**Save the result**: Save the result in a storage space in a pickle format.'
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**保存结果**：将结果以 pickle 格式保存到存储空间中。'
- en: '![](../Images/0dbf5f93c95ac09e1c116ce0ebb89a46.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0dbf5f93c95ac09e1c116ce0ebb89a46.png)'
- en: '**Figure 3.** Visualisation of the Azure FR workflow. (Image by the authors)'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 3.** Azure FR 工作流的可视化。（作者提供的图片）'
- en: '*AWS Textract (TT)*'
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '*AWS Textract (TT)*'
- en: 'Figure 4 depicts the workflow for extracting handwritten text from our form
    images by using AWS TT:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4 描述了使用 AWS TT 从表单图像中提取手写文本的工作流：
- en: '**Store the images:** The images are stored in an S3 bucket.'
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**存储图像：** 图像存储在 S3 桶中。'
- en: '**SageMaker Notebook**: A Notebook instance facilitates interaction with the
    TT API, executes the post-processing cleaning of the script, and saves the outcomes.'
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**SageMaker Notebook**：Notebook 实例帮助与 TT API 进行交互，执行脚本的后处理清理，并保存结果。'
- en: '**TT API:** This is the off-the-shelf OCR-based text extraction API that is
    provided by AWS.'
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**TT API：** 这是由 AWS 提供的现成的基于 OCR 的文本提取 API。'
- en: '**Post-processing**: Using an off-the-shelf method means that the final output
    often needs refining. TT produced a dataset with 68 columns, which is more than
    21 columns from the FR approach. This is mostly due to the detection of additional
    text in the images thought to be fields. These issues are then addressed during
    the rule-based post-processing.'
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**后处理：** 使用现成的方法意味着最终输出通常需要进行精细调整。TT 产生了一个包含 68 列的数据集，而 FR 方法只有 21 列。这主要是由于检测到图像中额外的文本，这些文本被认为是字段。在基于规则的后处理中会解决这些问题。'
- en: '**Save the result**: The refined data is then stored in an S3 bucket by using
    a pickle format.'
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**保存结果**：经过处理的数据会使用 pickle 格式存储在 S3 桶中。'
- en: '![](../Images/0d6ca9f7abbbe60d3d27c0179d7bf501.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0d6ca9f7abbbe60d3d27c0179d7bf501.png)'
- en: '**Figure 4\.** Visualisation of the TT workflow. (Image by the authors)'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 4.** TT 工作流的可视化。（作者提供的图片）'
- en: '*Donut*'
  id: totrans-70
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '*Donut*'
- en: In contrast to the off-the-shelf OCR-based approaches, which are unable to adapt
    to specific data input through custom fields and/or model retraining, this section
    delves into refining the OCR-free approach by using the Donut model, which is
    based on transformer model architecture.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 与不能通过自定义字段和/或模型再训练适应特定数据输入的现成基于 OCR 的方法相比，本节深入探讨了使用基于 Transformer 模型架构的 Donut
    模型来改进无 OCR 方法。
- en: 'First, we fine-tuned the Donut model with our data before applying the model
    to our test images to extract the handwritten text in a JSON format. In order
    to re-train the model efficiently and curb potential overfitting, we employed
    the EarlyStopping module from PyTorch Lightning. With a batch size of 2, the training
    terminated after 14 epochs. Here are more details for the fine-tuning process
    of the Donut model:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们使用我们的数据对 Donut 模型进行了微调，然后将模型应用于我们的测试图像，以 JSON 格式提取手写文本。为了高效地重新训练模型并防止潜在的过拟合，我们使用了
    PyTorch Lightning 的 EarlyStopping 模块。批量大小为 2，训练在 14 个周期后结束。以下是 Donut 模型微调过程的更多细节：
- en: We allocated 1,400 images for training, 199 for validation, and the remaining
    500 for testing.
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们分配了 1,400 张图像用于训练，199 张用于验证，其余 500 张用于测试。
- en: We used a [naver-clova-ix/donut-base](https://huggingface.co/naver-clova-ix/donut-base)
    as our foundation model, which is accessible on Hugging Face.
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们使用了一个 [naver-clova-ix/donut-base](https://huggingface.co/naver-clova-ix/donut-base)
    作为基础模型，它可以在 Hugging Face 上访问。
- en: This model was then fine-tuned using a Quadro P6000 GPU with 24GB memory.
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后使用 24GB 内存的 Quadro P6000 GPU 对模型进行了微调。
- en: The entire training time was approximately 3.5 hours.
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 整个训练时间大约为 3.5 小时。
- en: For more intricate configuration details, refer to the `[train_nist.yaml](https://github.com/srsani/hvdu/blob/main/src/config/train_nist.yaml)`
    in the repository.
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有关更复杂的配置细节，请参考仓库中的 `[train_nist.yaml](https://github.com/srsani/hvdu/blob/main/src/config/train_nist.yaml)`。
- en: This model can also be downloaded from our Hugging Face space [repository](https://huggingface.co/ift/handwriting_forms).
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型也可以从我们的 Hugging Face 空间 [代码库](https://huggingface.co/ift/handwriting_forms)
    下载。
- en: '**Ensemble Method: TT, Donut, GPT**'
  id: totrans-79
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**集成方法：TT、Donut、GPT**'
- en: A variety of ensembling methods were explored, and the combination of TT, Donut
    and GPT performed the best, as explained below.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 探索了各种集成方法，其中 TT、Donut 和 GPT 的组合表现最佳，具体解释如下。
- en: Once the JSON outputs were obtained by the individual application of TT and
    Donut, these were used as inputs to a prompt that was then passed on to GPT. The
    aim was to use GPT to take the information within these JSON inputs, combine it
    with contextual GPT information and create a new/cleaner JSON output with enhanced
    content reliability and accuracy [Table 3]. Figure 5 provides a visual overview
    of this ensembling approach.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦通过单独应用 TT 和 Donut 获得 JSON 输出，这些输出被用作输入，随后传递给 GPT。目的是利用 GPT 将这些 JSON 输入中的信息与上下文
    GPT 信息结合，创建一个新的/更清晰的 JSON 输出，以提高内容的可靠性和准确性 [表 3]。**图 5** 提供了这一集成方法的视觉概述。
- en: '![](../Images/73cb815c9d345287c6b2491ef6900439.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/73cb815c9d345287c6b2491ef6900439.png)'
- en: '**Figure 5.** Visual description of the ensembling method that combines TT,
    Donut and GPT. (Image by the authors)'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 5.** 结合 TT、Donut 和 GPT 的集成方法的视觉描述。（图片由作者提供）'
- en: The creation of the appropriate GPT prompt for this task was iterative and required
    the introduction of ad-hoc rules. The tailoring of the GPT prompt to this task
    — and possibly the dataset — is an aspect of this study that requires exploration,
    as noted in the [**Additional Considerations**](#266a) section.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 为此任务创建适当的 GPT 提示是一个迭代过程，并需要引入临时规则。根据[**附加考虑**](#266a)部分的说明，为此任务 — 以及可能的数据集 —
    定制 GPT 提示是本研究需要探索的一个方面。
- en: '**Measurement of Model Performance**'
  id: totrans-85
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**模型性能测量**'
- en: 'This study measured model performance mainly by using two distinct accuracy
    measures:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 本研究主要通过使用两种不同的准确度测量来评估模型性能：
- en: Field-Level-Accuracy (FLA)
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 字段级准确率（FLA）
- en: Character-Based-Accuracy (CBA)
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于字符的准确度（CBA）
- en: Additional quantities, such as **Coverage** and **Cost**, were also measured
    to provide relevant contextual information. All metrics are described below.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 还测量了其他数量，如**覆盖率**和**成本**，以提供相关的背景信息。所有指标如下所述。
- en: '**FLA**'
  id: totrans-90
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**FLA**'
- en: 'This is a binary measurement: if all of the characters of the keys within the
    predicted JSON match those in the reference JSON, then the FLA is 1; if, however,
    just one character does not match, then the FLA is 0.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个二元测量：如果预测的 JSON 中所有键的字符与参考 JSON 中的字符匹配，则 FLA 为 1；如果有一个字符不匹配，则 FLA 为 0。
- en: 'Consider the examples:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 请考虑以下示例：
- en: '[PRE0]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Comparing JSON1 and JSON2 using FLA results in a score of 0 due to the ZIP mismatch.
    However, comparing JSON1 with itself provides an FLA score of 1.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 FLA 比较 JSON1 和 JSON2 由于 ZIP 不匹配得分为 0。然而，将 JSON1 与自身比较则提供 FLA 得分为 1。
- en: '**CBA**'
  id: totrans-95
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**CBA**'
- en: 'This accuracy measure is computed as follows:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 该准确度测量的计算方法如下：
- en: Determining the Levenshtein edit distance for each corresponding value pair.
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 确定每对对应值的 Levenshtein 编辑距离。
- en: Obtaining a normalised score by summing up all of the distances and dividing
    by each value’s total combined string length.
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过将所有距离相加并除以每个值的总组合字符串长度来获得标准化得分。
- en: Converting this score into a percentage.
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将此得分转换为百分比。
- en: The Levenshtein edit-distance between two strings is the number of changes needed
    to transform one string into another. This involves counting substitutions, insertions,
    or deletions. For example, transforming “marry” into “Murray” would require two
    substitutions and one insertion, resulting in a total of three changes. These
    modifications can be made in various sequences, but at least three actions are
    necessary. For this computation, we employed the edit_distance function from the
    NLTK library.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 两个字符串之间的 Levenshtein 编辑距离是将一个字符串转换为另一个字符串所需的更改次数。这包括计数替换、插入或删除。例如，将“marry”转换为“Murray”需要两个替换和一个插入，共计三个更改。这些修改可以以各种顺序进行，但至少需要三次操作。对于此计算，我们使用了
    NLTK 库中的 edit_distance 函数。
- en: Below is a code snippet illustrating the implementation of the described algorithm.
    This function accepts two JSON inputs and returns with an accuracy percentage.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是一个代码片段，展示了所描述算法的实现。此函数接受两个 JSON 输入，并返回一个准确度百分比。
- en: '[PRE1]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'To better understand the function, let’s see how it performs in the following
    examples:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解该函数，让我们看看它在以下示例中的表现：
- en: '[PRE2]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '`dict_distance(JSON1, JSON1)`: 100% There is no difference between JSON1 and
    JSON1, so we obtain a perfect score of 100%'
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`dict_distance(JSON1, JSON1)`: 100% JSON1 和 JSON1 之间没有差异，因此我们获得了 100% 的完美得分。'
- en: '`dict_distance(JSON1, JSON2)`: 0% Every character in JSON2 would need alteration
    to match JSON1, yielding a 0% score.'
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`dict_distance(JSON1, JSON2)`: 0% JSON2 中的每个字符都需要修改才能匹配 JSON1，从而得出 0% 的得分。'
- en: '`dict_distance(JSON1, JSON3)`: 59% Every character in the STATE and ZIP keys
    of JSON3 must be changed to match JSON1, which results in an accuracy score of
    59%.'
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`dict_distance(JSON1, JSON3)`: 59% JSON3 中的 STATE 和 ZIP 键的每个字符都必须更改以匹配 JSON1，从而得到
    59% 的准确性得分。'
- en: We will now focus on the average value of CBA over the analysed image sample.
    Both of these accuracy measurements are very strict since they measure whether
    all characters and character cases from the examined strings match. FLA is particularly
    conservative due to its binary nature, which blinds it towards partially correct
    cases. Although CBA is less conservative than FLA, it is still considered to be
    somewhat conservative. Additionally, CBA has the ability to identify partially
    correct instances, but it also considers the text case (upper vs. lower), which
    may have differing levels of importance depending on whether the focus is to recover
    the appropriate content of the text or to preserve the exact form of the written
    content. Overall, we decided to use these stringent measurements for a more conservative
    approach since we prioritised text extraction correctness over text semantics.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将重点关注分析图像样本的 CBA 平均值。这些准确性测量非常严格，因为它们衡量了检查字符串中的所有字符及字符大小写是否匹配。由于 FLA 的二元特性，使其在处理部分正确的情况时表现得特别保守。尽管
    CBA 比 FLA 更不保守，但仍被认为是有些保守的。此外，CBA 能够识别部分正确的实例，但它也考虑了文本大小写（大写与小写），这可能根据是否关注于恢复文本的适当内容或保留书写内容的准确形式而具有不同的重要性。总体而言，我们决定使用这些严格的测量方法，以便采取更保守的方法，因为我们优先考虑文本提取的正确性而非文本语义。
- en: '**Coverage**'
  id: totrans-109
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**覆盖率**'
- en: This quantity is defined as the fraction of form images whose fields have all
    been extracted in the output JSON. It is helpful to monitor the overall ability
    to extract all fields from the forms, independent of their correctness. If Coverage
    is very low, it flags that certain fields are systematically being left out of
    the extraction process.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 这个量度定义为字段已全部从输出 JSON 中提取的表单图像的比例。这有助于监测从表单中提取所有字段的整体能力，而不考虑其正确性。如果 Coverage
    非常低，则表示某些字段系统性地被遗漏在提取过程中。
- en: '**Cost**'
  id: totrans-111
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**成本**'
- en: This is a simple estimate of the cost incurred by applying each method to the
    entire test dataset. We have not captured the GPU cost for fine-tuning the Donut
    model.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 这是对将每种方法应用于整个测试数据集所产生的成本的简单估算。我们没有计算微调 Donut 模型的 GPU 成本。
- en: '**Results**'
  id: totrans-113
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**结果**'
- en: We assessed the performance of all methods on the test dataset, which included
    500 samples. The results of this process are summarised in Table 3.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 我们评估了所有方法在测试数据集上的表现，该数据集包含 500 个样本。这一过程的结果汇总在表 3 中。
- en: 'When using FLA, we observe that more traditional OCR-based methods, FR and
    TT, perform similarly with relatively low accuracies (FLA~37%). While not ideal,
    this may be due to FLA’s stringent requirements. Alternatively, when using the
    CBA Total, which is the average CBA value when accounting for all JSON keys together,
    the performances of both TT and FR are far more acceptable, yielding values >
    77%. In particular, TT (CBA Total = 89.34%) outperforms FR by ~15%. This behaviour
    is then preserved when focusing on the values of CBA that are measured for the
    individual form fields, notably in the DATE and CITY categories [Table 3], and
    when measuring the FLA and CBA Totals over the entire sample of 2099 images (TT:
    FLA = 40.06%; CBA Total = 86.64%; FR: FLA = 35,64%; CBA Total = 78.57%). While
    the Cost value for applying these two models is the same, TT is better positioned
    to extract all of the form fields with Coverage values approximately 9% higher
    than the FR ones.'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '使用 FLA 时，我们观察到传统的基于 OCR 的方法 FR 和 TT 表现相似，准确率相对较低（FLA~37%）。虽然不理想，但这可能是由于 FLA
    的严格要求。作为替代，当使用 CBA Total，即考虑所有 JSON 键的平均 CBA 值时，TT 和 FR 的表现远远更可接受，值大于 77%。特别是，TT（CBA
    Total = 89.34%）比 FR 高出约 15%。这种行为在关注单个表单字段的 CBA 值时也得以保持，尤其是在 DATE 和 CITY 类别 [表
    3] 中，并且在测量整个样本 2099 张图像的 FLA 和 CBA Totals 时（TT: FLA = 40.06%；CBA Total = 86.64%；FR:
    FLA = 35.64%；CBA Total = 78.57%）。虽然应用这两种模型的 Cost 值相同，但 TT 更有利于提取所有表单字段，其 Coverage
    值比 FR 高出约 9%。'
- en: '![](../Images/38cc52e9abbcd8c713f05d6fe1ecc6a2.png)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/38cc52e9abbcd8c713f05d6fe1ecc6a2.png)'
- en: '**Table 3.** Performance metric values were calculated over the test dataset.
    CBA Total and CBA key (key= Date, City, State, Zip) are sample average values
    and accounts, respectively, for the JSON keys altogether and individually. (Image
    by the authors)'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '**表 3.** 性能指标值是根据测试数据集计算的。CBA 总体和 CBA 关键（key= 日期、城市、州、邮政编码）分别是样本平均值和 JSON 键的总值和单独值。（图片由作者提供）'
- en: Quantifying the performance of these more traditional OCR-based models provided
    us with a benchmark that we then used to evaluate the advantages of using a purely
    Donut approach versus using one in combination with TT and GPT. We begin this
    by using TT as our benchmark.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 量化这些更传统的基于 OCR 的模型的性能为我们提供了一个基准，我们用这个基准来评估使用纯 Donut 方法与将 Donut 与 TT 和 GPT 结合使用的优缺点。我们从使用
    TT 作为基准开始。
- en: The benefits of utilising this approach are shown through improved metrics from
    the Donut model that was fine-tuned on a sample size of 1400 images and their
    corresponding JSON. Compared to the TT results, this model’s global FLA of 54%
    and CBA Total of 95.23% constitute a 38% and 6% improvement, respectively. The
    most significant increase was seen in the FLA, demonstrating that the model can
    accurately retrieve all form fields for over half of the test sample.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 采用这种方法的好处体现在微调的 Donut 模型上，该模型在 1400 张图像及其对应的 JSON 上进行了微调。与 TT 结果相比，该模型的全球 FLA
    为 54%，CBA 总体为 95.23%，分别提高了 38% 和 6%。最显著的提高体现在 FLA 上，显示出该模型能够准确检索超过一半测试样本中的所有表单字段。
- en: The CBA increase is notable, given the limited number of images used for fine-tuning
    the model. The Donut model shows benefits, as evidenced by the improved overall
    values in Coverage and key-based CBA metrics, which increased by between 2% and
    24%. Coverage achieved 100%, indicating that the model can extract text from all
    form fields, which reduces the post-processing work involved in productionizing
    such a model.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: CBA 的提高是显著的，因为用于微调模型的图像数量有限。Donut 模型显示出其优势，表现在 Coverage 和基于关键的 CBA 指标的总体值有所改善，增加幅度在
    2% 到 24% 之间。Coverage 达到 100%，表明该模型可以从所有表单字段中提取文本，从而减少了将这种模型投入生产所需的后处理工作。
- en: Based on this task and dataset, these results illustrate that using a fine-tuned
    Donut model produces results that are superior to those produced by an OCR model.
    Lastly, ensembling methods were explored to assess if additional improvements
    could continue to be made.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 基于此任务和数据集，这些结果表明，使用经过微调的 Donut 模型产生的结果优于 OCR 模型。最后，还探索了集成方法，以评估是否可以继续进行额外的改进。
- en: The performance of the ensemble of TT and fine-tuned Donut, powered by gpt-3.5-turbo,
    reveals that improvements are possible if specific metrics, such as FLA, are chosen.
    All of the metrics for this model (excluding CBA State and Coverage) show an increase,
    ranging between ~0.2% and ~10%, compared to those for our fine-tuned Donut model.
    The only performance degradation is seen in the CBA State, which decreases by
    ~3% when compared to the value measured for our fine-tuned Donut model. This may
    be owed to the GPT prompt that was used, which can be further fine-tuned to improve
    this metric. Finally, the Coverage value remains unchanged at 100%.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 由 gpt-3.5-turbo 支持的 TT 和经过微调的 Donut 的集成性能表明，如果选择特定的指标（如 FLA），是有可能实现改进的。与我们经过微调的
    Donut 模型相比，该模型的所有指标（不包括 CBA 状态和 Coverage）都有所增加，增加幅度在 ~0.2% 到 ~10% 之间。唯一的性能下降体现在
    CBA 状态上，与我们经过微调的 Donut 模型测得的值相比，下降了 ~3%。这可能归因于使用的 GPT 提示，进一步微调可能会改善这一指标。最后，Coverage
    值保持在 100% 不变。
- en: When compared to the other individual fields, Date extraction (see CBA Date)
    yielded higher efficiency. This was likely due to the limited variability in the
    Date field since all Dates originated in 1989.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 与其他单独字段相比，日期提取（见 CBA 日期）效率更高。这可能是由于日期字段的变异性有限，因为所有日期都来源于 1989 年。
- en: If the performance requirements are considerably conservative, then the 10%
    increase in FLA is significant and may merit the higher cost of building and maintaining
    a more complex infrastructure. This should also consider the source of variability
    introduced by the LLM prompt modification, which is noted in the [**Additional
    Considerations**](#266a)section. However, if the performance requirements are
    less stringent, then the CBA metric improvements yielded by this ensemble method
    may not merit the additional cost and effort.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 如果性能要求非常保守，那么FLA的10%提升是显著的，并可能值得构建和维护更复杂的基础设施的更高成本。这也应考虑到LLM提示词修改引入的变异来源，如[**附加考虑**](#266a)部分所述。然而，如果性能要求不那么严格，那么该集成方法所带来的CBA指标改进可能不值得额外的成本和努力。
- en: Overall, our study shows that while individual OCR-based methods — namely FR
    and TT — have their strengths, the Donut model, fine-tuned on 1400 samples only,
    easily surpasses their accuracy benchmark. Furthermore, ensembling TT and a fine-tuned
    Donut model by a gpt-3.5-turbo prompt further increases accuracy when measured
    by the FLA metric. [**Additional Considerations**](#266a) must also be made concerning
    the fine-tuning process of the Donut model and the GPT prompt, which are now explored
    in the following section.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，我们的研究表明，虽然基于OCR的个别方法——即FR和TT——各有其优点，但仅在1400个样本上微调的甜甜圈模型轻松超越了它们的准确性基准。此外，通过gpt-3.5-turbo提示词对TT和微调后的甜甜圈模型进行集成，进一步提高了通过FLA指标测量的准确性。[**附加考虑**](#266a)也必须考虑甜甜圈模型和GPT提示词的微调过程，接下来将进行探讨。
- en: Additional Considerations
  id: totrans-126
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 附加考虑
- en: '**Donut model training**'
  id: totrans-127
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**甜甜圈模型训练**'
- en: To improve the accuracy of the Donut model, we experimented with three training
    approaches, each aimed at improving inference accuracy while preventing overfitting
    to the training data. Table 4 displays a summary of our results.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 为了提高甜甜圈模型的准确性，我们尝试了三种训练方法，每种方法都旨在提高推理准确性，同时防止过拟合训练数据。表4展示了我们结果的总结。
- en: '![](../Images/212826578131013e96299cb9b2b9cdff.png)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/212826578131013e96299cb9b2b9cdff.png)'
- en: '**Table 4.** Summary of the Donut model fine-tuning. (Image by the authors)'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '**表4.** 甜甜圈模型微调的总结。（作者提供的图像）'
- en: '**1\. The 30-Epoch Training**: We trained the Donut model for 30 epochs using
    a configuration provided in the Donut GitHub repository. This training session
    lasted for approximately 7 hours and resulted in an FLA of 50.0%. The CBA values
    for different categories varied, with CITY achieving a value of 90.55% and ZIP
    achieving 98.01%. However, we noticed that the model started overfitting after
    the 19th epoch when we examined the val_metric.'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '**1. 30-周期训练**：我们使用甜甜圈GitHub仓库提供的配置训练了30个周期的甜甜圈模型。该训练过程持续了约7小时，结果显示FLA为50.0%。不同类别的CBA值有所不同，CITY达到90.55%，ZIP达到98.01%。然而，当我们检查val_metric时注意到模型在第19周期后开始过拟合。'
- en: '**2\. The 19-Epoch Training**: Based on insights gained during the initial
    training, we fine-tuned the model for only 19 epochs. Our results showed a significant
    improvement in FLA, which reached 55.8%. The overall CBA, as well as key-based
    CBAs, showed improved accuracy values. Despite these promising metrics, we detected
    a hint of overfitting, as indicated by the val_metric.'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '**2. 19-周期训练**：根据初步训练获得的见解，我们将模型微调了19个周期。我们的结果显示FLA显著提高，达到55.8%。整体CBA以及基于关键的CBAs显示了准确性值的改善。尽管这些指标很有前景，但我们检测到了过拟合的迹象，如val_metric所示。'
- en: '**3\. The 14-Epoch Training**: To further refine our model and curb potential
    overfitting, we employed the EarlyStopping module from PyTorch Lightning. This
    approach terminated the training after 14 epochs. This resulted in an FLA of 54.0%,
    and CBAs were comparable, if not better, than the 19-epoch training.'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '**3. 14-周期训练**：为了进一步完善我们的模型并抑制潜在的过拟合，我们使用了PyTorch Lightning的EarlyStopping模块。这种方法在14个周期后终止了训练。结果显示FLA为54.0%，CBAs与19周期训练相比相当，甚至更好。'
- en: When comparing the outputs from these three training sessions, although the
    19-epoch training yielded a marginally better FLA, the CBA metrics in the 14-epoch
    training were overall superior. Additionally, the val_metric reinforced our apprehension
    regarding the 19-epoch training, indicating a slight inclination towards overfitting.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 比较这三次训练的输出时，尽管19周期训练的FLA略有改善，但14周期训练的CBA指标整体上更优。此外，val_metric强化了我们对19周期训练的担忧，表明略有过拟合的倾向。
- en: In conclusion, we deduced that the model that was fine-tuned over 14 epochs
    using EarlyStopping was both the most robust and the most cost-efficient.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，我们推断出，经过14个周期使用EarlyStopping微调的模型是最强健且最具成本效益的。
- en: Prompt engineering variability
  id: totrans-136
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 提示工程的变化
- en: We worked on two prompt engineering approaches (ver1 & ver2) to improve data
    extraction efficiency by ensembling a fine-tuned Donut model and our results from
    TT. After training the model for 14 epochs, Prompt ver1 yielded superior results
    with an FLA of 59.6% and higher CBA metrics for all keys [Table 5]. In contrast,
    Prompt ver2 experienced a decline, with its FLA dropping to 54.4%. A detailed
    look at the CBA metrics indicated that accuracy scores for every category in ver2
    were slightly lower when compared to those of ver1, highlighting the significant
    difference this change made.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 我们研究了两种提示工程方法（ver1和ver2），通过将微调的Donut模型与TT结果进行集成，以提高数据提取效率。在训练模型14个周期后，Prompt
    ver1取得了更好的结果，其FLA为59.6%，并且所有键的CBA指标均较高[表5]。相比之下，Prompt ver2出现了下降，其FLA降至54.4%。详细查看CBA指标显示，ver2中每个类别的准确度评分略低于ver1，突显了这一变化的显著差异。
- en: '![](../Images/863e0a6af87a6a4eeedcc748ff5347b3.png)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/863e0a6af87a6a4eeedcc748ff5347b3.png)'
- en: '**Table 5**. Summary of results: extracting handwritten text from forms. (Image
    by the authors)'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '**表5**。结果总结：从表单中提取手写文本。（图片由作者提供）'
- en: During our manual labelling process of the dataset, we utilised the results
    of TT and FR, and developed Prompt ver1 while annotating the text from the forms.
    Despite being intrinsically identical to its predecessor, Prompt ver2 was slightly
    modified. Our primary goal was to refine the prompt by eliminating empty lines
    and redundant spaces that were present in Prompt ver1.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 在对数据集进行手动标注过程中，我们利用了TT和FR的结果，并在对表单文本进行标注时开发了Prompt ver1。尽管Prompt ver2在本质上与其前身相同，但经过了轻微修改。我们的主要目标是通过去除Prompt
    ver1中的空行和冗余空格来改进提示。
- en: In summary, our experimentation highlighted the nuanced impact of seemingly
    minor adjustments. While Prompt ver1 showcased a higher accuracy, the process
    of refining and simplifying it into Prompt ver2, paradoxically, led to a reduction
    in performance across all metrics. This highlights the intricate nature of prompt
    engineering and the need for meticulous testing before finalising a prompt for
    use.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，我们的实验突显了看似微小调整的细微影响。尽管Prompt ver1展示了更高的准确性，但将其精炼和简化为Prompt ver2的过程，悖论地导致了所有指标上的性能下降。这凸显了提示工程的复杂性以及在最终确定提示之前进行细致测试的必要性。
- en: Prompt ver1 is available in [this Notebook](https://github.com/srsani/hvdu/blob/main/src/notebooks/5_1_ensemble_FR_TT_GPT.ipynb),
    and the code for Prompt ver2 can be seen [here](https://github.com/srsani/hvdu/blob/main/src/notebooks/utilities.py).
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: Prompt ver1可在[这个Notebook](https://github.com/srsani/hvdu/blob/main/src/notebooks/5_1_ensemble_FR_TT_GPT.ipynb)中找到，Prompt
    ver2的代码可以在[这里](https://github.com/srsani/hvdu/blob/main/src/notebooks/utilities.py)查看。
- en: Conclusion
  id: totrans-143
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: We created a benchmark dataset for text extraction from images of handwritten
    forms containing four fields (DATE, CITY, STATE, and ZIP). These forms were manually
    annotated into a JSON format. We used this dataset to assess the performances
    of OCR-based models (FR and TT) and a Donut model, which was then fine-tuned using
    our dataset. Lastly, we employed an ensemble model that we built through prompt
    engineering by utilising an LLM (gpt-3.5-turbo) with TT and our fine-tuned Donut
    model outputs.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 我们创建了一个基准数据集，用于从包含四个字段（DATE、CITY、STATE 和 ZIP）的手写表单图像中提取文本。这些表单被手动标注为JSON格式。我们使用此数据集评估了基于OCR的模型（FR和TT）和一个Donut模型，该模型随后使用我们的数据集进行了微调。最后，我们采用了一个通过提示工程构建的集成模型，该模型利用了LLM（gpt-3.5-turbo）与TT和我们微调的Donut模型输出。
- en: We found that TT performed better than FR and used this as a benchmark to evaluate
    prospective improvements that could be generated by a Donut model in isolation
    or in combination with TT and GPT, which is the ensemble approach. As displayed
    by the model performance metrics, this fine-tuned Donut model showed clear accuracy
    improvements that justify its adoption over OCR-based models. The ensemble model
    displayed significant improvement of FLA but comes at a higher cost and therefore,
    can be considered for usage in cases with stricter performance requirements. Despite
    employing the consistent underlying model, gpt-3.5-turbo, we observed notable
    differences in the output JSON form when minor changes in the prompt were made.
    Such unpredictability is a significant drawback when using off-the-shelf LLMs
    in production. We are currently developing a more compact cleaning process based
    on an open-source LLM to address this issue.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 我们发现 TT 的表现优于 FR，并以此作为基准来评估 Donut 模型在单独使用或与 TT 和 GPT 组合使用（即集成方法）时可能产生的改进。正如模型性能指标所显示的那样，这种微调的
    Donut 模型显示出明显的准确性提升，这证明了其优于基于 OCR 的模型。集成模型显著提高了 FLA，但成本较高，因此可以在要求更严格的性能要求的情况下考虑使用。尽管使用了相同的基础模型
    gpt-3.5-turbo，但我们观察到当提示中进行微小更改时，输出 JSON 表单存在显著差异。这种不可预测性是使用现成 LLM 的一个重大缺陷。我们目前正在开发基于开源
    LLM 的更紧凑的清理流程来解决这个问题。
- en: Next Steps
  id: totrans-146
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 下一步
- en: The price column in Table 2 shows that the OpenAI API call was the most expensive
    cognitive service used in this work. Thus, to minimise costs, we are working on
    fine-tuning an LLM for a seq2seq task by utilising methods such as full fine-tuning,
    prompt tuning[5] and QLORA [6].
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 表 2 中的价格列显示，OpenAI API 调用是本研究中最昂贵的认知服务。因此，为了减少成本，我们正在通过利用完全微调、提示微调[5] 和 QLORA
    [6] 等方法来微调 LLM 以进行 seq2seq 任务。
- en: Due to privacy reasons, the name box on the images in the dataset is covered
    by a black rectangle. We are working on updating this by adding random first and
    last names to the dataset, which would increase the data extraction fields from
    four to five.
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于隐私原因，数据集中图像上的名称框被黑色矩形覆盖。我们正在通过向数据集中添加随机的名字和姓氏来更新这一点，这将使数据提取字段从四个增加到五个。
- en: In the future, we plan to increase the complexity of the text-extraction task
    by extending this study to include text extraction of entire forms or other more
    extensive documents.
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 未来，我们计划通过将这项研究扩展到包括整个表单或其他更大文档的文本提取任务，从而增加文本提取任务的复杂性。
- en: Investigate Donut model hyperparameter optimization.
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 调查 Donut 模型超参数优化。
- en: References
  id: totrans-151
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: Amazon Textract, [AWS Textract](https://aws.amazon.com/textract/ocr/)
  id: totrans-152
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Amazon Textract，[AWS Textract](https://aws.amazon.com/textract/ocr/)
- en: Form Recognizer, [Form Recognizer (now Document Intelligence)](https://learn.microsoft.com/en-us/azure/ai-services/document-intelligence/faq?view=doc-intel-3.1.0)
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 表单识别器，[表单识别器（现为文档智能）](https://learn.microsoft.com/en-us/azure/ai-services/document-intelligence/faq?view=doc-intel-3.1.0)
- en: Kim, Geewook and Hong, Teakgyu and Yim, Moonbin and Nam, JeongYeon and Park,
    Jinyoung and Yim, Jinyeong and Hwang, Wonseok and Yun, Sangdoo and Han, Dongyoon
    and Park, Seunghyun, [OCR-free Document Understanding Transformer](https://arxiv.org/abs/2111.15664)
    (2022), European Conference on Computer Vision (ECCV)
  id: totrans-154
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Kim, Geewook 和 Hong, Teakgyu 和 Yim, Moonbin 和 Nam, JeongYeon 和 Park, Jinyoung
    和 Yim, Jinyeong 和 Hwang, Wonseok 和 Yun, Sangdoo 和 Han, Dongyoon 和 Park, Seunghyun，[无
    OCR 文档理解转换器](https://arxiv.org/abs/2111.15664) (2022)，欧洲计算机视觉会议 (ECCV)
- en: 'Grother, P. and Hanaoka, K. (2016) NIST Handwritten Forms and Characters Database
    (NIST Special Database 19). DOI: [http://doi.org/10.18434/T4H01C](http://doi.org/10.18434/T4H01C)'
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Grother, P. 和 Hanaoka, K. (2016) NIST 手写表单和字符数据库 (NIST 特殊数据库 19)。DOI: [http://doi.org/10.18434/T4H01C](http://doi.org/10.18434/T4H01C)'
- en: Brian Lester, Rami Al-Rfou, Noah Constant, [The Power of Scale for Parameter-Efficient
    Prompt Tuning](https://arxiv.org/abs/2104.08691) (2021), arXiv:2104.08691
  id: totrans-156
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Brian Lester, Rami Al-Rfou, Noah Constant，[规模化的参数高效提示微调的力量](https://arxiv.org/abs/2104.08691)
    (2021)，arXiv:2104.08691
- en: 'Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, Luke Zettlemoyer, [QLoRA: Efficient
    Finetuning of Quantized LLMs](https://arxiv.org/abs/2305.14314) (2023), [https://arxiv.org/abs/2305.14314](https://arxiv.org/abs/2305.14314)'
  id: totrans-157
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, Luke Zettlemoyer，[QLoRA: 高效的量化
    LLM 微调](https://arxiv.org/abs/2305.14314) (2023)，[https://arxiv.org/abs/2305.14314](https://arxiv.org/abs/2305.14314)'
- en: Acknowledgements
  id: totrans-158
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 致谢
- en: We would like to thank our colleague, Dr. David Rodrigues, for his continuous
    support and discussions surrounding this project. We would also like to thank
    [Kainos](https://www.kainos.com/) for their support.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 我们要感谢我们的同事**大卫·罗德里格斯**博士，他对这个项目的持续支持和讨论。我们还要感谢[Kainos](https://www.kainos.com/)的支持。
