- en: 'DL Notes: Gradient Descent'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ·±åº¦å­¦ä¹ ç¬”è®°ï¼šæ¢¯åº¦ä¸‹é™
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/gradient-descent-f09f19eb35fb?source=collection_archive---------13-----------------------#2023-11-04](https://towardsdatascience.com/gradient-descent-f09f19eb35fb?source=collection_archive---------13-----------------------#2023-11-04)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://towardsdatascience.com/gradient-descent-f09f19eb35fb?source=collection_archive---------13-----------------------#2023-11-04](https://towardsdatascience.com/gradient-descent-f09f19eb35fb?source=collection_archive---------13-----------------------#2023-11-04)
- en: How Neural Networks â€œLearnâ€
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç¥ç»ç½‘ç»œå¦‚ä½•â€œå­¦ä¹ â€
- en: '[](https://medium.com/@luisdamed?source=post_page-----f09f19eb35fb--------------------------------)[![Luis
    Medina](../Images/d83d326290ae3272f0618d0bd28bd875.png)](https://medium.com/@luisdamed?source=post_page-----f09f19eb35fb--------------------------------)[](https://towardsdatascience.com/?source=post_page-----f09f19eb35fb--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----f09f19eb35fb--------------------------------)
    [Luis Medina](https://medium.com/@luisdamed?source=post_page-----f09f19eb35fb--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@luisdamed?source=post_page-----f09f19eb35fb--------------------------------)[![Luis
    Medina](../Images/d83d326290ae3272f0618d0bd28bd875.png)](https://medium.com/@luisdamed?source=post_page-----f09f19eb35fb--------------------------------)[](https://towardsdatascience.com/?source=post_page-----f09f19eb35fb--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----f09f19eb35fb--------------------------------)
    [Luis Medina](https://medium.com/@luisdamed?source=post_page-----f09f19eb35fb--------------------------------)'
- en: Â·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Â·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F562a027a34f0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgradient-descent-f09f19eb35fb&user=Luis+Medina&userId=562a027a34f0&source=post_page-562a027a34f0----f09f19eb35fb---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----f09f19eb35fb--------------------------------)
    Â·11 min readÂ·Nov 4, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Ff09f19eb35fb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgradient-descent-f09f19eb35fb&user=Luis+Medina&userId=562a027a34f0&source=-----f09f19eb35fb---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[å…³æ³¨](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F562a027a34f0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgradient-descent-f09f19eb35fb&user=Luis+Medina&userId=562a027a34f0&source=post_page-562a027a34f0----f09f19eb35fb---------------------post_header-----------)
    å‘è¡¨åœ¨ [Towards Data Science](https://towardsdatascience.com/?source=post_page-----f09f19eb35fb--------------------------------)
    Â·11åˆ†é’Ÿé˜…è¯»Â·2023å¹´11æœˆ4æ—¥[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Ff09f19eb35fb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgradient-descent-f09f19eb35fb&user=Luis+Medina&userId=562a027a34f0&source=-----f09f19eb35fb---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff09f19eb35fb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgradient-descent-f09f19eb35fb&source=-----f09f19eb35fb---------------------bookmark_footer-----------)![](../Images/c5a5af3b75e58cac86deb3abd95692a0.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff09f19eb35fb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgradient-descent-f09f19eb35fb&source=-----f09f19eb35fb---------------------bookmark_footer-----------)![](../Images/c5a5af3b75e58cac86deb3abd95692a0.png)'
- en: Photo by [Rohit Tandon](https://unsplash.com/@sepoys?utm_source=ghost&utm_medium=referral&utm_campaign=api-credit)
    / [Unsplash](https://unsplash.com/?utm_source=ghost&utm_medium=referral&utm_campaign=api-credit)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ç‰‡ç”± [Rohit Tandon](https://unsplash.com/@sepoys?utm_source=ghost&utm_medium=referral&utm_campaign=api-credit)
    æä¾› / [Unsplash](https://unsplash.com/?utm_source=ghost&utm_medium=referral&utm_campaign=api-credit)
- en: Artificial Neural Networks (ANNs) are [universal function approximators](https://www.youtube.com/watch?v=lkha188L4Gs%3Fref%3Dmakerluis.com).
    They can approximate any complex function if provided with enough data, have a
    proper architecture, and are *trained* for enough time.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: äººå·¥ç¥ç»ç½‘ç»œï¼ˆANNsï¼‰æ˜¯ [é€šç”¨å‡½æ•°é€¼è¿‘å™¨](https://www.youtube.com/watch?v=lkha188L4Gs%3Fref%3Dmakerluis.com)ã€‚åªè¦æä¾›è¶³å¤Ÿçš„æ•°æ®ã€å…·æœ‰é€‚å½“çš„æ¶æ„ï¼Œå¹¶ä¸”ç»è¿‡
    *è¶³å¤Ÿé•¿æ—¶é—´çš„è®­ç»ƒ*ï¼Œå®ƒä»¬å¯ä»¥é€¼è¿‘ä»»ä½•å¤æ‚çš„å‡½æ•°ã€‚
- en: But what does â€œtrainingâ€ a network even mean?
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: é‚£ä¹ˆâ€œè®­ç»ƒâ€ç½‘ç»œåˆ°åº•æ˜¯ä»€ä¹ˆæ„æ€å‘¢ï¼Ÿ
- en: In [a previous post about the feedforward process](https://medium.com/@luisdamed/feedforward-artificial-neural-networks-52bcf96d6ac3),
    I mentioned that training a network means adjusting the value of its weights,
    to obtain a better fit of the function we are trying to approximate.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ [ä¹‹å‰å…³äºå‰é¦ˆè¿‡ç¨‹çš„æ–‡ç« ](https://medium.com/@luisdamed/feedforward-artificial-neural-networks-52bcf96d6ac3)
    ä¸­ï¼Œæˆ‘æåˆ°è®­ç»ƒä¸€ä¸ªç½‘ç»œæ„å‘³ç€è°ƒæ•´å…¶æƒé‡çš„å€¼ï¼Œä»¥è·å¾—å¯¹æˆ‘ä»¬è¯•å›¾é€¼è¿‘çš„å‡½æ•°æ›´å¥½çš„æ‹Ÿåˆã€‚
- en: In this post, Iâ€™m going to describe the algorithm of gradient descent, which
    is used to adjust the weights of an ANN.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘å°†æè¿°æ¢¯åº¦ä¸‹é™ç®—æ³•ï¼Œå®ƒç”¨äºè°ƒæ•´äººå·¥ç¥ç»ç½‘ç»œï¼ˆANNï¼‰çš„æƒé‡ã€‚
- en: Letâ€™s start with the basic concepts.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬ä»åŸºæœ¬æ¦‚å¿µå¼€å§‹ã€‚
- en: Descending from a mountain
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ä»å±±ä¸Šä¸‹é™
- en: Imagine we are at the top of a mountain and need to get to the lowest point
    of a valley next to it.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: æƒ³è±¡ä¸€ä¸‹ï¼Œæˆ‘ä»¬åœ¨å±±é¡¶ä¸Šï¼Œéœ€è¦åˆ°è¾¾æ—è¾¹å±±è°·çš„æœ€ä½ç‚¹ã€‚
- en: We donâ€™t have a map, it is foggy and getting dark, we lost the routes and need
    to get to the bottom quickly. Not a nice scenario, but it shows the â€œboundariesâ€
    of the problem.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬æ²¡æœ‰åœ°å›¾ï¼Œå¤©æ°”é˜´éœ¾ï¼Œå¤©è‰²æ¸æš—ï¼Œæˆ‘ä»¬ä¸¢å¤±äº†è·¯çº¿ï¼Œéœ€è¦å°½å¿«åˆ°è¾¾åº•éƒ¨ã€‚è¿™ä¸æ˜¯ä¸€ä¸ªå¥½åœºæ™¯ï¼Œä½†å®ƒå±•ç¤ºäº†é—®é¢˜çš„â€œè¾¹ç•Œâ€ã€‚
- en: For our safety, letâ€™s suppose there are no steep ridges in the mountain, so
    it is similar to a [differentiable function](https://en.wikipedia.org/wiki/Differentiable_function?ref=makerluis.com).
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†å®‰å…¨èµ·è§ï¼Œå‡è®¾å±±ä¸Šæ²¡æœ‰é™¡å³­çš„å±±è„Šï¼Œå› æ­¤å®ƒç±»ä¼¼äºä¸€ä¸ª[å¯å¾®å‡½æ•°](https://en.wikipedia.org/wiki/Differentiable_function?ref=makerluis.com)ã€‚
- en: '![](../Images/95b5dc5e213b18cdb7bc509d24da0138.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/95b5dc5e213b18cdb7bc509d24da0138.png)'
- en: Descent from the Monviso peak. [A small valley near Oncino, Cuneo](https://www.google.com/maps/@?api=1&map_action=map&basemap=satellite&center=44.659382%2C7.120437&zoom=16&ref=makerluis.com).
    Image by the author.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: ä»è’™ç»´ç´¢å³°ï¼ˆMonviso peakï¼‰ä¸‹å±±ã€‚[é è¿‘ç¿å¥‡è¯ºï¼Œåº“å†…å¥¥çš„å°å±±è°·](https://www.google.com/maps/@?api=1&map_action=map&basemap=satellite&center=44.659382%2C7.120437&zoom=16&ref=makerluis.com)ã€‚å›¾åƒç”±ä½œè€…æä¾›ã€‚
- en: When it gets dark, we canâ€™t see in which direction we are moving. The only way
    we can descend is by taking small steps, and checking whether we are at a lower
    height or not.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: å½“å¤©é»‘æ—¶ï¼Œæˆ‘ä»¬çœ‹ä¸åˆ°æˆ‘ä»¬ç§»åŠ¨çš„æ–¹å‘ã€‚æˆ‘ä»¬å”¯ä¸€èƒ½åšçš„å°±æ˜¯è¿ˆå°æ­¥ï¼Œå¹¶æ£€æŸ¥æˆ‘ä»¬æ˜¯å¦å¤„äºè¾ƒä½çš„é«˜åº¦ã€‚
- en: If we notice we moved up, we go in the opposite direction. If we moved down,
    we continue that way. We repeat the process until, eventually, weâ€™ll reach the
    bottom.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæˆ‘ä»¬æ³¨æ„åˆ°è‡ªå·±å‘ä¸Šç§»åŠ¨äº†ï¼Œæˆ‘ä»¬å°±æœç›¸åçš„æ–¹å‘å‰è¿›ã€‚å¦‚æœæˆ‘ä»¬å‘ä¸‹ç§»åŠ¨ï¼Œæˆ‘ä»¬å°±ç»§ç»­è¿™æ ·åšã€‚æˆ‘ä»¬é‡å¤è¿™ä¸ªè¿‡ç¨‹ï¼Œç›´åˆ°æœ€ç»ˆåˆ°è¾¾åº•éƒ¨ã€‚
- en: As you can see, this is not necessarily the best approach. We might end up in
    a small valley, not at the bottom of the mountain, or we could spend a huge amount
    of time on a plateau.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚ä½ æ‰€è§ï¼Œè¿™ä¸ä¸€å®šæ˜¯æœ€ä½³çš„æ–¹æ³•ã€‚æˆ‘ä»¬å¯èƒ½ä¼šåˆ°è¾¾ä¸€ä¸ªå°å±±è°·ï¼Œè€Œä¸æ˜¯å±±åº•ï¼Œæˆ–è€…å¯èƒ½ä¼šåœ¨ä¸€ä¸ªå¹³åŸä¸ŠèŠ±è´¹å¤§é‡æ—¶é—´ã€‚
- en: This illustrates the basic working principle of gradient descent and also its
    main challenges. Weâ€™ll come back to this example, but letâ€™s see a more formal
    explanation first.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™è¯´æ˜äº†æ¢¯åº¦ä¸‹é™çš„åŸºæœ¬å·¥ä½œåŸç†åŠå…¶ä¸»è¦æŒ‘æˆ˜ã€‚æˆ‘ä»¬ä¼šå›åˆ°è¿™ä¸ªä¾‹å­ï¼Œä½†é¦–å…ˆè®©æˆ‘ä»¬çœ‹çœ‹æ›´æ­£å¼çš„è§£é‡Šã€‚
- en: What is a gradient?
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ä»€ä¹ˆæ˜¯æ¢¯åº¦ï¼Ÿ
- en: A gradient is a representation of the rate of change of a function. It indicates
    the direction of the greatest increase or decrease. Intuitively, that means the
    gradient is zero at a local maximum or a local minimum.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: æ¢¯åº¦æ˜¯å‡½æ•°å˜åŒ–ç‡çš„è¡¨ç¤ºã€‚å®ƒæŒ‡ç¤ºäº†æœ€å¤§å¢å‡æ–¹å‘ã€‚ç›´è§‚åœ°è¯´ï¼Œè¿™æ„å‘³ç€åœ¨å±€éƒ¨æœ€å¤§å€¼æˆ–å±€éƒ¨æœ€å°å€¼å¤„æ¢¯åº¦ä¸ºé›¶ã€‚
- en: For a function that depends on several variables (or coordinate axes), the gradient
    is a vector whose components are the partial derivatives of the function, evaluated
    at a given point. This is denoted with the symbol âˆ‡ (nabla) which represents the
    vector differential operator.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºä¾èµ–å¤šä¸ªå˜é‡ï¼ˆæˆ–åæ ‡è½´ï¼‰çš„å‡½æ•°ï¼Œæ¢¯åº¦æ˜¯ä¸€ä¸ªå‘é‡ï¼Œå…¶åˆ†é‡æ˜¯å‡½æ•°åœ¨ç»™å®šç‚¹çš„åå¯¼æ•°ã€‚è¿™ç”¨ç¬¦å·âˆ‡ï¼ˆnablaï¼‰è¡¨ç¤ºï¼Œä»£è¡¨å‘é‡å¾®åˆ†ç®—å­ã€‚
- en: 'Letâ€™s see this in math notation. Suppose we have an n-dimensional function
    f:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬ç”¨æ•°å­¦ç¬¦å·æ¥çœ‹ä¸€ä¸‹ã€‚å‡è®¾æˆ‘ä»¬æœ‰ä¸€ä¸ªnç»´å‡½æ•°fï¼š
- en: 'The gradient of this function at point p (which is determined by n coordinates),
    is given by:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: è¯¥å‡½æ•°åœ¨ç‚¹pï¼ˆç”±nä¸ªåæ ‡ç¡®å®šï¼‰å¤„çš„æ¢¯åº¦ä¸ºï¼š
- en: Coming back to the example of the mountain, there are areas of the mountain
    where the terrain is steep, like the mountain slopes, and other zones where the
    terrain is almost flat, like a valley or a plateau. Valleys and plateaus represent
    local minima, which are usually critical points.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: å›åˆ°å±±çš„ä¾‹å­ï¼Œå±±ä¸Šæœ‰äº›åœ°æ–¹åœ°åŠ¿é™¡å³­ï¼Œæ¯”å¦‚å±±å¡ï¼Œè¿˜æœ‰äº›åœ°æ–¹åœ°åŠ¿å‡ ä¹å¹³å¦ï¼Œæ¯”å¦‚å±±è°·æˆ–å¹³åŸã€‚å±±è°·å’Œå¹³åŸä»£è¡¨å±€éƒ¨æœ€å°å€¼ï¼Œè¿™é€šå¸¸æ˜¯å…³é”®ç‚¹ã€‚
- en: The gradient descent method
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ¢¯åº¦ä¸‹é™æ–¹æ³•
- en: For many optimization problems, we aim to minimize a loss function to achieve
    the most accurate result.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºè®¸å¤šä¼˜åŒ–é—®é¢˜ï¼Œæˆ‘ä»¬çš„ç›®æ ‡æ˜¯æœ€å°åŒ–æŸå¤±å‡½æ•°ï¼Œä»¥å®ç°æœ€å‡†ç¡®çš„ç»“æœã€‚
- en: 'In Deep Learning and ANNs, the loss functions we use are differentiable: they
    have no discontinuities, being smooth across their whole domain.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ·±åº¦å­¦ä¹ å’Œäººå·¥ç¥ç»ç½‘ç»œï¼ˆANNï¼‰ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨çš„æŸå¤±å‡½æ•°æ˜¯å¯å¾®çš„ï¼šå®ƒä»¬åœ¨æ•´ä¸ªå®šä¹‰åŸŸå†…å¹³æ»‘ä¸”æ— ä¸è¿ç»­æ€§ã€‚
- en: This allows us to use the derivative of the loss function with respect to the
    independent variables as an indication of whether we are moving towards a solution
    (a global minimum).
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä½¿æˆ‘ä»¬èƒ½å¤Ÿä½¿ç”¨ç›¸å¯¹äºè‡ªå˜é‡çš„æŸå¤±å‡½æ•°çš„å¯¼æ•°ä½œä¸ºæ˜¯å¦æ­£åœ¨æœç€è§£å†³æ–¹æ¡ˆï¼ˆå…¨å±€æœ€å°å€¼ï¼‰ç§»åŠ¨çš„æŒ‡ç¤ºã€‚
- en: How large are the steps we take in proportion to the derivative? this is determined
    by a step size parameter, *Î·* (we can call it **learning rate** when we are talking
    about Deep Learning). It will multiply the gradient, scaling it to determine the
    step size.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬åœ¨ä¸å¯¼æ•°æˆæ¯”ä¾‹æ—¶çš„æ­¥é•¿æœ‰å¤šå¤§ï¼Ÿè¿™ç”±æ­¥é•¿å‚æ•°*Î·*å†³å®šï¼ˆå½“æˆ‘ä»¬è°ˆè®ºæ·±åº¦å­¦ä¹ æ—¶ï¼Œæˆ‘ä»¬å¯ä»¥ç§°ä¹‹ä¸º**å­¦ä¹ ç‡**ï¼‰ã€‚å®ƒå°†ä¹˜ä»¥æ¢¯åº¦ï¼Œè°ƒæ•´æ­¥é•¿çš„å¤§å°ã€‚
- en: This way, steeper gradients will produce larger steps. As we approach a local
    minimum, the slope (gradient) will tend to zero.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ ·ä¸€æ¥ï¼Œæ›´é™¡çš„æ¢¯åº¦å°†äº§ç”Ÿæ›´å¤§çš„æ­¥é•¿ã€‚å½“æˆ‘ä»¬æ¥è¿‘å±€éƒ¨æœ€å°å€¼æ—¶ï¼Œæ–œç‡ï¼ˆæ¢¯åº¦ï¼‰å°†è¶‹è¿‘äºé›¶ã€‚
- en: 'Letâ€™s look at the following figure, for example, to illustrate how this works
    when optimizing a 1D function:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬çœ‹ä¸€ä¸‹ä¸‹é¢çš„å›¾ï¼Œä»¥è¯´æ˜åœ¨ä¼˜åŒ–ä¸€ç»´å‡½æ•°æ—¶è¿™å¦‚ä½•å·¥ä½œï¼š
- en: '![](../Images/cdef9ea00750a1e9e7aebad15276301b.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cdef9ea00750a1e9e7aebad15276301b.png)'
- en: Simplified example of gradient descent in a 1D problem. Image by the author.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ç»´é—®é¢˜ä¸­æ¢¯åº¦ä¸‹é™çš„ç®€åŒ–ç¤ºä¾‹ã€‚å›¾ç‰‡ç”±ä½œè€…æä¾›ã€‚
- en: As you can see, we initialize our â€œsearchâ€ for the minima at an arbitrary point
    (Iâ€™ve depicted two examples, A and B). We gradually take steps toward the closest
    minima, making changes in *Î¸* in proportion to the slope.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: æ­£å¦‚ä½ æ‰€çœ‹åˆ°çš„ï¼Œæˆ‘ä»¬ä»ä¸€ä¸ªä»»æ„ç‚¹ï¼ˆæˆ‘ç”»äº†ä¸¤ä¸ªä¾‹å­ï¼ŒAå’ŒBï¼‰åˆå§‹åŒ–æˆ‘ä»¬çš„â€œæœç´¢â€ä»¥å¯»æ‰¾æœ€å°å€¼ã€‚æˆ‘ä»¬é€æ¸å‘æœ€è¿‘çš„æœ€å°å€¼è¿ˆè¿›ï¼Œ*Î¸*çš„å˜åŒ–ä¸æ–œç‡æˆæ¯”ä¾‹ã€‚
- en: 'The illustration represents what the following algorithm does (pseudo-code)
    [1]:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: æ’å›¾è¡¨ç¤ºä»¥ä¸‹ç®—æ³•çš„åŠŸèƒ½ï¼ˆä¼ªä»£ç ï¼‰[1]ï¼š
- en: '[PRE0]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The loss function here is like our example of the mountain when itâ€™s dark and
    we donâ€™t have a map: we donâ€™t know what it looks like. We want to know the value
    of *Î¸* for which ***J*** is minimized, but the optimization algorithm doesnâ€™t
    know what the value of ***J*** would be for all possible inputs *Î¸* .'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œçš„æŸå¤±å‡½æ•°å°±åƒæˆ‘ä»¬åœ¨é»‘æš—ä¸­æ²¡æœ‰åœ°å›¾çš„å±±è„‰ï¼šæˆ‘ä»¬ä¸çŸ¥é“å®ƒçš„æ ·å­ã€‚æˆ‘ä»¬æƒ³çŸ¥é“å“ªä¸ª*Î¸*å€¼èƒ½ä½¿***J***æœ€å°åŒ–ï¼Œä½†ä¼˜åŒ–ç®—æ³•å¹¶ä¸çŸ¥é“***J***åœ¨æ‰€æœ‰å¯èƒ½çš„*Î¸*è¾“å…¥ä¸‹çš„å€¼ã€‚
- en: This is why we initialize our optimization algorithm with any arbitrary value
    of *Î¸*. For instance, points A and B in the figure represent two different initialization
    values.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å°±æ˜¯ä¸ºä»€ä¹ˆæˆ‘ä»¬ç”¨ä»»æ„çš„*Î¸*å€¼åˆå§‹åŒ–æˆ‘ä»¬çš„ä¼˜åŒ–ç®—æ³•ã€‚ä¾‹å¦‚ï¼Œå›¾ä¸­çš„ç‚¹Aå’ŒBè¡¨ç¤ºä¸¤ä¸ªä¸åŒçš„åˆå§‹åŒ–å€¼ã€‚
- en: Potential problems with Gradient Descent
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ¢¯åº¦ä¸‹é™çš„æ½œåœ¨é—®é¢˜
- en: The gradient descent algorithm is effective because it can help us obtain an
    approximate solution for any convex function.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: æ¢¯åº¦ä¸‹é™ç®—æ³•æ˜¯æœ‰æ•ˆçš„ï¼Œå› ä¸ºå®ƒå¯ä»¥å¸®åŠ©æˆ‘ä»¬ä¸ºä»»ä½•å‡¸å‡½æ•°è·å¾—ä¸€ä¸ªè¿‘ä¼¼è§£ã€‚
- en: If the function we are trying to optimize is convex, for any value of *Ïµ* there
    is some step size *Î·* such that gradient descent will converge to *Î¸** within
    *Ïµ* of the true optimal *Î¸.* [1]
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæˆ‘ä»¬å°è¯•ä¼˜åŒ–çš„å‡½æ•°æ˜¯å‡¸çš„ï¼Œå¯¹äºä»»ä½•å€¼*Ïµ*ï¼Œéƒ½æœ‰æŸä¸ªæ­¥é•¿*Î·*ä½¿å¾—æ¢¯åº¦ä¸‹é™å°†åœ¨*Ïµ*å†…æ”¶æ•›åˆ°*Î¸*çš„çœŸå®æœ€ä¼˜å€¼*Î¸*ã€‚ [1]
- en: However, as you might have guessed, this is not perfect. The algorithm might
    converge, but that doesnâ€™t guarantee that weâ€™ll find a global minimum.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶è€Œï¼Œæ­£å¦‚ä½ å¯èƒ½çŒœåˆ°çš„ï¼Œè¿™å¹¶ä¸å®Œç¾ã€‚ç®—æ³•å¯èƒ½ä¼šæ”¶æ•›ï¼Œä½†è¿™å¹¶ä¸ä¿è¯æˆ‘ä»¬ä¼šæ‰¾åˆ°å…¨å±€æœ€å°å€¼ã€‚
- en: 'The main challenges of gradient descent are the following:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: æ¢¯åº¦ä¸‹é™çš„ä¸»è¦æŒ‘æˆ˜å¦‚ä¸‹ï¼š
- en: Arbitrary initialization has an impact on the results
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä»»æ„åˆå§‹åŒ–å¯¹ç»“æœæœ‰å½±å“
- en: Using different initialization values, we could encounter local minima instead
    of global minima.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ä¸åŒçš„åˆå§‹åŒ–å€¼ï¼Œæˆ‘ä»¬å¯èƒ½ä¼šé‡åˆ°å±€éƒ¨æœ€å°å€¼è€Œä¸æ˜¯å…¨å±€æœ€å°å€¼ã€‚
- en: For example, starting at point B instead of point A in the previous figure above.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: ä¾‹å¦‚ï¼Œä»ä¸Šå›¾ä¸­çš„ç‚¹Bå¼€å§‹ï¼Œè€Œä¸æ˜¯ç‚¹Aã€‚
- en: Or, a less evident case, converging to a plateau (vanishing gradient problem)
    as shown by the blue line in the figure below.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ–è€…æ›´ä¸æ˜æ˜¾çš„æƒ…å†µï¼Œä¾‹å¦‚ï¼Œåƒä¸‹å›¾ä¸­çš„è“çº¿æ‰€ç¤ºï¼Œæ”¶æ•›åˆ°ä¸€ä¸ªå¹³å°ï¼ˆæ¢¯åº¦æ¶ˆå¤±é—®é¢˜ï¼‰ã€‚
- en: '![](../Images/e06647b8def86857c479409b11888d71.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e06647b8def86857c479409b11888d71.png)'
- en: Vanilla gradient descent over a saddle surface. The animation shows how different
    initialization points can produce different results. Image by the author.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: é¦™è‰æ¢¯åº¦ä¸‹é™åœ¨éå½¢è¡¨é¢ä¸Šçš„æ•ˆæœã€‚åŠ¨ç”»æ˜¾ç¤ºäº†ä¸åŒåˆå§‹åŒ–ç‚¹å¯èƒ½äº§ç”Ÿä¸åŒç»“æœçš„æƒ…å†µã€‚å›¾ç‰‡ç”±ä½œè€…æä¾›ã€‚
- en: '*ğŸ’¡* If you are curious about how to create this kind of animation, check out
    my article [Creating a Gradient Descent Animation in Python](/creating-a-gradient-descent-animation-in-python-3c4dcd20ca51).'
  id: totrans-55
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*ğŸ’¡* å¦‚æœä½ å¯¹å¦‚ä½•åˆ›å»ºè¿™ç§åŠ¨ç”»æ„Ÿå…´è¶£ï¼Œå¯ä»¥æŸ¥çœ‹æˆ‘çš„æ–‡ç«  [åœ¨ Python ä¸­åˆ›å»ºæ¢¯åº¦ä¸‹é™åŠ¨ç”»](/creating-a-gradient-descent-animation-in-python-3c4dcd20ca51)ã€‚'
- en: Selecting the appropriate step size requires a trade-off between convergence
    speed and stability
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Thereâ€™s an interaction between the step size or learning rate, and the number
    of epochs we should use to achieve accurate results.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: Take for example the results of a parametric experiment, shown below. The images
    are from the online course [A Deep Understanding of Deep Learning](https://www.udemy.com/course/deeplearning_x/?ref=makerluis.com),
    by Mike X Cohen, which I highly recommend to anyone interested in Deep Learning
    and using [PyTorch](https://pytorch.org/?ref=makerluis.com), following a scientific
    approach.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: In this case, Mike showed how to test the results for a gradient descent optimization
    when changing the learning rate and the number of training epochs independently
    (one parameter over time, for a grid of different values). We can see how both
    parameters affect the results for this particular case.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4bd9c40334c3a73736990c791efe9f16.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
- en: Images are from the online course [A Deep Understanding of Deep Learning](https://www.udemy.com/course/deeplearning_x/?ref=makerluis.com),
    by Mike X Cohen.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: The true global minimum of the function is around -1.4\. For smaller learning
    rates, it takes a larger number of training epochs to converge to that result.
    So it would seem like just using a larger learning rate can help us reduce the
    computational time.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: But in practice, this isnâ€™t a simple matter of convergence speed.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: Large step sizes can lead to very slow convergence, prevent the algorithm from
    converging at all (oscillating around the minima forever), or provoke a divergent
    behavior.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: The next figure shows how different learning rates affect the results of the
    optimization, even if we initialize the algorithm in the same location x = 2.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/17adbdaca929c0d64adab695e1b7f420.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
- en: Example of stochastic gradient descent applied to *f*(*x*)=*x^*2 with different
    step sizes. In all cases, the algorithm is initialized at x = 2\. Image by the
    author, with base code adapted from [Saraj Rivalâ€™s notebook](https://github.com/llSourcell/The_evolution_of_gradient_descent/blob/master/GD_vs_SGD.ipynb?ref=makerluis.com).
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: Here, it is evident that having large step sizes improves the convergence speed
    but only up to a certain point.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: Increasing the learning rate by an order of magnitude caused the algorithm to
    get stuck. The results for *Î·* = 1 oscillate between x = 2 and x = -2, and this
    is shown only by the blue horizontal line in the left figure.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: In some cases, a large step size might actually â€œshootâ€ the results to infinity,
    causing a numerical overflow of our program.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, too small step sizes can create a very slow convergence or
    no convergence at all.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: Gradient descent for training a Neural Network
  id: totrans-72
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the context of Deep Learning, the function we are trying to optimize is
    our loss function ***J***. We define our training losses as the average of the
    losses for all our training dataset:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
- en: Where Dtrain is the number of samples in our training dataset.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: Thus, we can implement gradient descent based on the following algorithm, in
    which we compute the gradient of the training losses to perform an update of the
    model weights, and repeat for a number of iterations[2]
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Since we are computing an average of the gradient of the loss function, we have
    a better estimate of it. For this reason, the weights update is more likely to
    be in the direction that improves the model performance.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: The problem with this implementation of gradient descent is its low speed.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: For a toy example with a few points and a simple function it might work well,
    but imagine we are developing an ANN and we have a million data points to train
    it.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
- en: To train the ANN with this algorithm, we would need to compute the outputs of
    the model for each sample of the training data and then average their losses as
    a large batch. Just to do *one* update of the weights. Then repeat it, over and
    over again, until we reach convergence.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
- en: This is called *Batch* Gradient Descent. It makes one (accurate) weight update
    per iteration, but each iteration can take a *long time* because we are repeating
    the model computation n times.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: To overcome this problem, we could use the so-called Stochastic Gradient Descent
    algorithm.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: Stochastic Gradient Descent
  id: totrans-83
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To overcome the slow convergence problem of Batch Gradient Descent, we can perform
    an update of the model weights based on each sample of the training set. The advantage
    is that we donâ€™t need to wait until we have looped through the whole set to perform
    just one update of the weights.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: We can do this by using the Loss function for each individual sample, instead
    of the Training Loss which considers the whole dataset as a batch.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: This is what the Stochastic Gradient Descent Algorithm (SGD) looks like
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Notice that the step size is a function of the training iterations. Thatâ€™s because,
    for the algorithm to converge, Î· must decrease as the number of iterations progresses.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: 'In the [lecture notes of MIT 6.036](https://openlearninglibrary.mit.edu/courses/course-v1:MITx+6.036+1T2019/courseware/Week4/gradient_descent/?activate_block_id=block-v1%3AMITx%2B6.036%2B1T2019%2Btype%40sequential%2Bblock%40gradient_descent%3Fref%3Dmakerluis.com&ref=makerluis.com)
    [1], the following theorem is mentioned:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: Theorem 4.1\. If J is convex, and Î·(t) is a sequence satisfying
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: then SGD converges with probability one to the optimal *Î¸*.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: 'People take different approaches to reduce the value of Î· as the training progresses,
    and this is often called â€œAnnealingâ€:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: Changing the learning rate in proportion to the training epoch (for instance,
    Î·(t) = 1/t ), or setting it to a smaller value once a certain learning epoch has
    been reached. This method offers good results but is unrelated to the model performance.
    This is called â€œlearning rate decayâ€, and is the industry standard to handle this
    problem.
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Multiplying the learning rate by the gradient of the loss function: This method
    is good because itâ€™s adaptive to the problem, but requires careful scaling. This
    is incorporated into RMSprop and Adam variations of gradient descent.'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å°†å­¦ä¹ ç‡ä¹˜ä»¥æŸå¤±å‡½æ•°çš„æ¢¯åº¦ï¼šè¿™ç§æ–¹æ³•å¾ˆå¥½ï¼Œå› ä¸ºå®ƒå¯¹é—®é¢˜æ˜¯è‡ªé€‚åº”çš„ï¼Œä½†éœ€è¦ä»”ç»†è°ƒæ•´ã€‚è¿™å·²ç»è¢«çº³å…¥äº†RMSpropå’ŒAdamæ¢¯åº¦ä¸‹é™çš„å˜ä½“ä¸­ã€‚
- en: 'Multiplying the learning rate by the losses: The advantage is that this is
    also adaptive to the problem, but requires scaling too.'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å°†å­¦ä¹ ç‡ä¹˜ä»¥æŸå¤±ï¼šä¼˜ç‚¹æ˜¯è¿™ç§æ–¹æ³•åŒæ ·å¯¹é—®é¢˜è‡ªé€‚åº”ï¼Œä½†ä¹Ÿéœ€è¦ç¼©æ”¾ã€‚
- en: SGD may perform well after visiting only some of the data. This behavior can
    be useful for relatively large data sets, because we may reduce the amount of
    memory needed, and the total runtime compared to the â€œvanillaâ€ implementation
    of gradient descent.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: SGDåœ¨ä»…è®¿é—®éƒ¨åˆ†æ•°æ®åå¯èƒ½è¡¨ç°è‰¯å¥½ã€‚è¿™ç§è¡Œä¸ºå¯¹ç›¸å¯¹è¾ƒå¤§çš„æ•°æ®é›†å¾ˆæœ‰ç”¨ï¼Œå› ä¸ºæˆ‘ä»¬å¯ä»¥å‡å°‘æ‰€éœ€çš„å†…å­˜é‡ï¼Œå¹¶ä¸”ä¸â€œåŸç”Ÿâ€æ¢¯åº¦ä¸‹é™å®ç°ç›¸æ¯”ï¼Œæ€»è¿è¡Œæ—¶é—´è¾ƒçŸ­ã€‚
- en: We could say that the Batch implementation is slower because it needs to run
    through all samples to perform a single update of the weights.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥è¯´ï¼Œæ‰¹é‡å®ç°è¾ƒæ…¢ï¼Œå› ä¸ºå®ƒéœ€è¦éå†æ‰€æœ‰æ ·æœ¬æ‰èƒ½è¿›è¡Œä¸€æ¬¡æƒé‡æ›´æ–°ã€‚
- en: SGD performs an update at every sample, but the quality of the updates is lower.
    We may have noisy data or a really complex function we are trying to fit with
    our ANN.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: SGDåœ¨æ¯ä¸ªæ ·æœ¬ä¸Šæ‰§è¡Œæ›´æ–°ï¼Œä½†æ›´æ–°çš„è´¨é‡è¾ƒä½ã€‚æˆ‘ä»¬å¯èƒ½æœ‰å™ªå£°æ•°æ®æˆ–ä¸€ä¸ªéå¸¸å¤æ‚çš„å‡½æ•°éœ€è¦ç”¨æˆ‘ä»¬çš„ANNæ‹Ÿåˆã€‚
- en: Using a batch of size Dtrain is slow, but accurate, and using a batch of size
    1 is fast, but less accurate.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨å¤§å°ä¸ºDtrainçš„æ‰¹é‡å¾ˆæ…¢ï¼Œä½†å‡†ç¡®ï¼Œä½¿ç”¨å¤§å°ä¸º1çš„æ‰¹é‡å¾ˆå¿«ï¼Œä½†ä¸å¤Ÿå‡†ç¡®ã€‚
- en: There is a term in between, and itâ€™s called â€œmini-batchâ€ gradient descent.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä¸¤è€…ä¹‹é—´æœ‰ä¸€ä¸ªæœ¯è¯­ï¼Œç§°ä¸ºâ€œmini-batchâ€æ¢¯åº¦ä¸‹é™ã€‚
- en: Mini-Batch Gradient Descent
  id: totrans-101
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Mini-Batchæ¢¯åº¦ä¸‹é™
- en: '![](../Images/e49352109af2b9cbce81357c4eb26785.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e49352109af2b9cbce81357c4eb26785.png)'
- en: Photo by [Sebastian Herrmann](https://unsplash.com/@herrherrmann?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ç‰‡ç”±[Sebastian Herrmann](https://unsplash.com/@herrherrmann?utm_source=medium&utm_medium=referral)æä¾›ï¼Œæ¥æºäº[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: If we split our data into smaller batches of equal size, we could do what Batch
    gradient descent does, but for each of those *mini-batches*.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæˆ‘ä»¬å°†æ•°æ®åˆ†æˆå¤§å°ç›¸ç­‰çš„æ›´å°æ‰¹é‡ï¼Œæˆ‘ä»¬å¯ä»¥åšæ‰¹é‡æ¢¯åº¦ä¸‹é™æ‰€åšçš„äº‹æƒ…ï¼Œä½†é’ˆå¯¹æ¯ä¸ª*mini-batch*ã€‚
- en: Letâ€™s say we divide the data into 100 smaller parts.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: å‡è®¾æˆ‘ä»¬å°†æ•°æ®åˆ†æˆ100ä¸ªæ›´å°çš„éƒ¨åˆ†ã€‚
- en: We go through our data in 100 steps. On each step, we look at the Training Losses
    just for the data contained in the current mini-batch and improve our model parameters.
    We repeat this until we have looked at all samples, and start the cycle again.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†æ•°æ®åˆ†æˆ100æ­¥ã€‚åœ¨æ¯ä¸€æ­¥ä¸­ï¼Œæˆ‘ä»¬ä»…æŸ¥çœ‹å½“å‰mini-batchä¸­æ•°æ®çš„è®­ç»ƒæŸå¤±ï¼Œå¹¶æ”¹è¿›æˆ‘ä»¬çš„æ¨¡å‹å‚æ•°ã€‚æˆ‘ä»¬é‡å¤è¿™ä¸€è¿‡ç¨‹ï¼Œç›´åˆ°æŸ¥çœ‹æ‰€æœ‰æ ·æœ¬ï¼Œç„¶åé‡æ–°å¼€å§‹å¾ªç¯ã€‚
- en: Each cycle is known as an **epoch**. I have used the term more loosely before
    just to refer to the number of iterations during optimization, but the normal
    definition refers to each cycle through the training dataset. For Batch gradient
    descent, an *iteration* is an epoch.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: æ¯ä¸ªå¾ªç¯ç§°ä¸º**å‘¨æœŸ**ã€‚æˆ‘ä¹‹å‰æ›´å®½æ³›åœ°ä½¿ç”¨äº†è¿™ä¸ªæœ¯è¯­æ¥æŒ‡ä»£ä¼˜åŒ–è¿‡ç¨‹ä¸­çš„è¿­ä»£æ¬¡æ•°ï¼Œä½†é€šå¸¸çš„å®šä¹‰æŒ‡çš„æ˜¯æ¯æ¬¡éå†è®­ç»ƒæ•°æ®é›†ã€‚å¯¹äºæ‰¹é‡æ¢¯åº¦ä¸‹é™ï¼Œ*è¿­ä»£*å°±æ˜¯ä¸€ä¸ªå‘¨æœŸã€‚
- en: When we set how many epochs we want to use for training an ANN, we are defining
    how many passes through the training dataset we will make.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: å½“æˆ‘ä»¬è®¾ç½®è®­ç»ƒANNçš„å‘¨æœŸæ•°æ—¶ï¼Œæˆ‘ä»¬æ˜¯åœ¨å®šä¹‰é€šè¿‡è®­ç»ƒæ•°æ®é›†çš„éå†æ¬¡æ•°ã€‚
- en: The advantage of using mini-batches is that we update our model parameters on
    each mini-batch, rather than after we have looked at the whole data set.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨mini-batchçš„ä¼˜ç‚¹æ˜¯æˆ‘ä»¬åœ¨æ¯ä¸ªmini-batchä¸Šæ›´æ–°æ¨¡å‹å‚æ•°ï¼Œè€Œä¸æ˜¯åœ¨æŸ¥çœ‹æ•´ä¸ªæ•°æ®é›†åå†æ›´æ–°ã€‚
- en: 'For Batch gradient descent, the batch size is the total number of samples in
    the training dataset. For mini-batch gradient descent, the mini-batches are usually
    powers of two: 32 samples, 64, 128, 256, and so on.'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºæ‰¹é‡æ¢¯åº¦ä¸‹é™ï¼Œæ‰¹é‡å¤§å°æ˜¯è®­ç»ƒæ•°æ®é›†ä¸­æ ·æœ¬çš„æ€»æ•°ã€‚å¯¹äºmini-batchæ¢¯åº¦ä¸‹é™ï¼Œmini-batché€šå¸¸æ˜¯2çš„å¹‚æ¬¡ï¼š32ä¸ªæ ·æœ¬ã€64ã€128ã€256ç­‰ã€‚
- en: SGD would be an extreme case when the mini-batch size is reduced to a single
    example in the raining dataset.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: å½“mini-batchå¤§å°å‡å°‘åˆ°è®­ç»ƒæ•°æ®é›†ä¸­çš„å•ä¸ªç¤ºä¾‹æ—¶ï¼ŒSGDå°†æ˜¯ä¸€ä¸ªæç«¯æƒ…å†µã€‚
- en: The disadvantage of using a mini-batch gradient in our optimization process
    is that we incorporate a level of variability â€” although less severe than with
    SDG. It is not guaranteed that every step will move us closer to the ideal parameter
    values, but the general direction is still towards the minimum.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨mini-batchæ¢¯åº¦ä¸‹é™çš„ç¼ºç‚¹æ˜¯æˆ‘ä»¬å¼•å…¥äº†ä¸€å®šç¨‹åº¦çš„å˜å¼‚æ€§â€”â€”è™½ç„¶æ¯”éšæœºæ¢¯åº¦ä¸‹é™ï¼ˆSDGï¼‰è½»å¾®ã€‚è¿™å¹¶ä¸ä¿è¯æ¯ä¸€æ­¥éƒ½ä¼šä½¿æˆ‘ä»¬æ›´æ¥è¿‘ç†æƒ³å‚æ•°å€¼ï¼Œä½†æ€»ä½“æ–¹å‘ä»ç„¶æ˜¯å‘æœ€å°å€¼é è¿‘ã€‚
- en: This method is one of the industry standards because by finding the optimal
    batch size, we can choose a compromise between speed and accuracy when dealing
    with very large datasets.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: Thank you for reading! I hope this article was interesting and helped you clear
    some concepts. Iâ€™m also sharing the sources I used when writing this, in case
    you are interested in going through deeper and more formal material.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: In a future post, Iâ€™ll write about [more advanced methods of gradient descent](https://medium.com/towards-data-science/dl-notes-advanced-gradient-descent-4407d84c2515)
    (the ones people use in real applications) and how we actually update the model
    weights during training, using backpropagation, as gradient descent is only one
    part of the full story.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: 'In the meantime, you might be interested in reading my previous article, about
    Feedforward Artificial Neural Networks:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/@luisdamed/feedforward-artificial-neural-networks-52bcf96d6ac3?source=post_page-----f09f19eb35fb--------------------------------)
    [## Feedforward Artificial Neural Networks'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: The basic concept, explained
  id: totrans-118
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: medium.com](https://medium.com/@luisdamed/feedforward-artificial-neural-networks-52bcf96d6ac3?source=post_page-----f09f19eb35fb--------------------------------)
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: References
  id: totrans-120
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[1] [MIT Open Learning Library: 6.036 Introduction to Machine Learning. Chapter
    6: Gradient Descent](https://openlearninglibrary.mit.edu/assets/courseware/v1/d81d9ec0bd142738b069ce601382fdb7/asset-v1:MITx+6.036+1T2019+type@asset+block/notes_chapter_Gradient_Descent.pdf?ref=makerluis.com)'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: '[2] [Standford Online: Artificial Intelligence & Machine Learning 4 â€” Stochastic
    Gradient Descent | Stanford CS221 (2021)](https://www.youtube.com/watch?v=bl2WgBLH0tI%3Fref%3Dmakerluis.com&ref=makerluis.com)'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: '[3] Online course [A Deep Understanding of Deep Learning](https://www.udemy.com/course/deeplearning_x/?ref=makerluis.com),
    by Mike X Cohen ( [sincxpress.com](https://sincxpress.com/?ref=makerluis.com))'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: '[4] [Standford Online: CS231 Convolutional Neural Networks for Visual Recognition](https://cs231n.github.io/neural-networks-3?ref=makerluis.com)'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: '*Originally published at* [*https://www.makerluis.com*](https://www.makerluis.com/gradient-descent/)
    *on November 4, 2023.*'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
