- en: 'DL Notes: Gradient Descent'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习笔记：梯度下降
- en: 原文：[https://towardsdatascience.com/gradient-descent-f09f19eb35fb?source=collection_archive---------13-----------------------#2023-11-04](https://towardsdatascience.com/gradient-descent-f09f19eb35fb?source=collection_archive---------13-----------------------#2023-11-04)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/gradient-descent-f09f19eb35fb?source=collection_archive---------13-----------------------#2023-11-04](https://towardsdatascience.com/gradient-descent-f09f19eb35fb?source=collection_archive---------13-----------------------#2023-11-04)
- en: How Neural Networks “Learn”
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 神经网络如何“学习”
- en: '[](https://medium.com/@luisdamed?source=post_page-----f09f19eb35fb--------------------------------)[![Luis
    Medina](../Images/d83d326290ae3272f0618d0bd28bd875.png)](https://medium.com/@luisdamed?source=post_page-----f09f19eb35fb--------------------------------)[](https://towardsdatascience.com/?source=post_page-----f09f19eb35fb--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----f09f19eb35fb--------------------------------)
    [Luis Medina](https://medium.com/@luisdamed?source=post_page-----f09f19eb35fb--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@luisdamed?source=post_page-----f09f19eb35fb--------------------------------)[![Luis
    Medina](../Images/d83d326290ae3272f0618d0bd28bd875.png)](https://medium.com/@luisdamed?source=post_page-----f09f19eb35fb--------------------------------)[](https://towardsdatascience.com/?source=post_page-----f09f19eb35fb--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----f09f19eb35fb--------------------------------)
    [Luis Medina](https://medium.com/@luisdamed?source=post_page-----f09f19eb35fb--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F562a027a34f0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgradient-descent-f09f19eb35fb&user=Luis+Medina&userId=562a027a34f0&source=post_page-562a027a34f0----f09f19eb35fb---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----f09f19eb35fb--------------------------------)
    ·11 min read·Nov 4, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Ff09f19eb35fb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgradient-descent-f09f19eb35fb&user=Luis+Medina&userId=562a027a34f0&source=-----f09f19eb35fb---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F562a027a34f0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgradient-descent-f09f19eb35fb&user=Luis+Medina&userId=562a027a34f0&source=post_page-562a027a34f0----f09f19eb35fb---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----f09f19eb35fb--------------------------------)
    ·11分钟阅读·2023年11月4日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Ff09f19eb35fb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgradient-descent-f09f19eb35fb&user=Luis+Medina&userId=562a027a34f0&source=-----f09f19eb35fb---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff09f19eb35fb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgradient-descent-f09f19eb35fb&source=-----f09f19eb35fb---------------------bookmark_footer-----------)![](../Images/c5a5af3b75e58cac86deb3abd95692a0.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff09f19eb35fb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgradient-descent-f09f19eb35fb&source=-----f09f19eb35fb---------------------bookmark_footer-----------)![](../Images/c5a5af3b75e58cac86deb3abd95692a0.png)'
- en: Photo by [Rohit Tandon](https://unsplash.com/@sepoys?utm_source=ghost&utm_medium=referral&utm_campaign=api-credit)
    / [Unsplash](https://unsplash.com/?utm_source=ghost&utm_medium=referral&utm_campaign=api-credit)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由 [Rohit Tandon](https://unsplash.com/@sepoys?utm_source=ghost&utm_medium=referral&utm_campaign=api-credit)
    提供 / [Unsplash](https://unsplash.com/?utm_source=ghost&utm_medium=referral&utm_campaign=api-credit)
- en: Artificial Neural Networks (ANNs) are [universal function approximators](https://www.youtube.com/watch?v=lkha188L4Gs%3Fref%3Dmakerluis.com).
    They can approximate any complex function if provided with enough data, have a
    proper architecture, and are *trained* for enough time.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 人工神经网络（ANNs）是 [通用函数逼近器](https://www.youtube.com/watch?v=lkha188L4Gs%3Fref%3Dmakerluis.com)。只要提供足够的数据、具有适当的架构，并且经过
    *足够长时间的训练*，它们可以逼近任何复杂的函数。
- en: But what does “training” a network even mean?
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 那么“训练”网络到底是什么意思呢？
- en: In [a previous post about the feedforward process](https://medium.com/@luisdamed/feedforward-artificial-neural-networks-52bcf96d6ac3),
    I mentioned that training a network means adjusting the value of its weights,
    to obtain a better fit of the function we are trying to approximate.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [之前关于前馈过程的文章](https://medium.com/@luisdamed/feedforward-artificial-neural-networks-52bcf96d6ac3)
    中，我提到训练一个网络意味着调整其权重的值，以获得对我们试图逼近的函数更好的拟合。
- en: In this post, I’m going to describe the algorithm of gradient descent, which
    is used to adjust the weights of an ANN.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我将描述梯度下降算法，它用于调整人工神经网络（ANN）的权重。
- en: Let’s start with the basic concepts.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从基本概念开始。
- en: Descending from a mountain
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从山上下降
- en: Imagine we are at the top of a mountain and need to get to the lowest point
    of a valley next to it.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下，我们在山顶上，需要到达旁边山谷的最低点。
- en: We don’t have a map, it is foggy and getting dark, we lost the routes and need
    to get to the bottom quickly. Not a nice scenario, but it shows the “boundaries”
    of the problem.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 我们没有地图，天气阴霾，天色渐暗，我们丢失了路线，需要尽快到达底部。这不是一个好场景，但它展示了问题的“边界”。
- en: For our safety, let’s suppose there are no steep ridges in the mountain, so
    it is similar to a [differentiable function](https://en.wikipedia.org/wiki/Differentiable_function?ref=makerluis.com).
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 为了安全起见，假设山上没有陡峭的山脊，因此它类似于一个[可微函数](https://en.wikipedia.org/wiki/Differentiable_function?ref=makerluis.com)。
- en: '![](../Images/95b5dc5e213b18cdb7bc509d24da0138.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/95b5dc5e213b18cdb7bc509d24da0138.png)'
- en: Descent from the Monviso peak. [A small valley near Oncino, Cuneo](https://www.google.com/maps/@?api=1&map_action=map&basemap=satellite&center=44.659382%2C7.120437&zoom=16&ref=makerluis.com).
    Image by the author.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 从蒙维索峰（Monviso peak）下山。[靠近翁奇诺，库内奥的小山谷](https://www.google.com/maps/@?api=1&map_action=map&basemap=satellite&center=44.659382%2C7.120437&zoom=16&ref=makerluis.com)。图像由作者提供。
- en: When it gets dark, we can’t see in which direction we are moving. The only way
    we can descend is by taking small steps, and checking whether we are at a lower
    height or not.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 当天黑时，我们看不到我们移动的方向。我们唯一能做的就是迈小步，并检查我们是否处于较低的高度。
- en: If we notice we moved up, we go in the opposite direction. If we moved down,
    we continue that way. We repeat the process until, eventually, we’ll reach the
    bottom.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们注意到自己向上移动了，我们就朝相反的方向前进。如果我们向下移动，我们就继续这样做。我们重复这个过程，直到最终到达底部。
- en: As you can see, this is not necessarily the best approach. We might end up in
    a small valley, not at the bottom of the mountain, or we could spend a huge amount
    of time on a plateau.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，这不一定是最佳的方法。我们可能会到达一个小山谷，而不是山底，或者可能会在一个平原上花费大量时间。
- en: This illustrates the basic working principle of gradient descent and also its
    main challenges. We’ll come back to this example, but let’s see a more formal
    explanation first.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 这说明了梯度下降的基本工作原理及其主要挑战。我们会回到这个例子，但首先让我们看看更正式的解释。
- en: What is a gradient?
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是梯度？
- en: A gradient is a representation of the rate of change of a function. It indicates
    the direction of the greatest increase or decrease. Intuitively, that means the
    gradient is zero at a local maximum or a local minimum.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度是函数变化率的表示。它指示了最大增减方向。直观地说，这意味着在局部最大值或局部最小值处梯度为零。
- en: For a function that depends on several variables (or coordinate axes), the gradient
    is a vector whose components are the partial derivatives of the function, evaluated
    at a given point. This is denoted with the symbol ∇ (nabla) which represents the
    vector differential operator.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 对于依赖多个变量（或坐标轴）的函数，梯度是一个向量，其分量是函数在给定点的偏导数。这用符号∇（nabla）表示，代表向量微分算子。
- en: 'Let’s see this in math notation. Suppose we have an n-dimensional function
    f:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们用数学符号来看一下。假设我们有一个n维函数f：
- en: 'The gradient of this function at point p (which is determined by n coordinates),
    is given by:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 该函数在点p（由n个坐标确定）处的梯度为：
- en: Coming back to the example of the mountain, there are areas of the mountain
    where the terrain is steep, like the mountain slopes, and other zones where the
    terrain is almost flat, like a valley or a plateau. Valleys and plateaus represent
    local minima, which are usually critical points.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 回到山的例子，山上有些地方地势陡峭，比如山坡，还有些地方地势几乎平坦，比如山谷或平原。山谷和平原代表局部最小值，这通常是关键点。
- en: The gradient descent method
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 梯度下降方法
- en: For many optimization problems, we aim to minimize a loss function to achieve
    the most accurate result.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 对于许多优化问题，我们的目标是最小化损失函数，以实现最准确的结果。
- en: 'In Deep Learning and ANNs, the loss functions we use are differentiable: they
    have no discontinuities, being smooth across their whole domain.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度学习和人工神经网络（ANN）中，我们使用的损失函数是可微的：它们在整个定义域内平滑且无不连续性。
- en: This allows us to use the derivative of the loss function with respect to the
    independent variables as an indication of whether we are moving towards a solution
    (a global minimum).
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 这使我们能够使用相对于自变量的损失函数的导数作为是否正在朝着解决方案（全局最小值）移动的指示。
- en: How large are the steps we take in proportion to the derivative? this is determined
    by a step size parameter, *η* (we can call it **learning rate** when we are talking
    about Deep Learning). It will multiply the gradient, scaling it to determine the
    step size.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在与导数成比例时的步长有多大？这由步长参数*η*决定（当我们谈论深度学习时，我们可以称之为**学习率**）。它将乘以梯度，调整步长的大小。
- en: This way, steeper gradients will produce larger steps. As we approach a local
    minimum, the slope (gradient) will tend to zero.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 这样一来，更陡的梯度将产生更大的步长。当我们接近局部最小值时，斜率（梯度）将趋近于零。
- en: 'Let’s look at the following figure, for example, to illustrate how this works
    when optimizing a 1D function:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一下下面的图，以说明在优化一维函数时这如何工作：
- en: '![](../Images/cdef9ea00750a1e9e7aebad15276301b.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cdef9ea00750a1e9e7aebad15276301b.png)'
- en: Simplified example of gradient descent in a 1D problem. Image by the author.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 一维问题中梯度下降的简化示例。图片由作者提供。
- en: As you can see, we initialize our “search” for the minima at an arbitrary point
    (I’ve depicted two examples, A and B). We gradually take steps toward the closest
    minima, making changes in *θ* in proportion to the slope.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所看到的，我们从一个任意点（我画了两个例子，A和B）初始化我们的“搜索”以寻找最小值。我们逐渐向最近的最小值迈进，*θ*的变化与斜率成比例。
- en: 'The illustration represents what the following algorithm does (pseudo-code)
    [1]:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 插图表示以下算法的功能（伪代码）[1]：
- en: '[PRE0]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The loss function here is like our example of the mountain when it’s dark and
    we don’t have a map: we don’t know what it looks like. We want to know the value
    of *θ* for which ***J*** is minimized, but the optimization algorithm doesn’t
    know what the value of ***J*** would be for all possible inputs *θ* .'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的损失函数就像我们在黑暗中没有地图的山脉：我们不知道它的样子。我们想知道哪个*θ*值能使***J***最小化，但优化算法并不知道***J***在所有可能的*θ*输入下的值。
- en: This is why we initialize our optimization algorithm with any arbitrary value
    of *θ*. For instance, points A and B in the figure represent two different initialization
    values.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是为什么我们用任意的*θ*值初始化我们的优化算法。例如，图中的点A和B表示两个不同的初始化值。
- en: Potential problems with Gradient Descent
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 梯度下降的潜在问题
- en: The gradient descent algorithm is effective because it can help us obtain an
    approximate solution for any convex function.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度下降算法是有效的，因为它可以帮助我们为任何凸函数获得一个近似解。
- en: If the function we are trying to optimize is convex, for any value of *ϵ* there
    is some step size *η* such that gradient descent will converge to *θ** within
    *ϵ* of the true optimal *θ.* [1]
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们尝试优化的函数是凸的，对于任何值*ϵ*，都有某个步长*η*使得梯度下降将在*ϵ*内收敛到*θ*的真实最优值*θ*。 [1]
- en: However, as you might have guessed, this is not perfect. The algorithm might
    converge, but that doesn’t guarantee that we’ll find a global minimum.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，正如你可能猜到的，这并不完美。算法可能会收敛，但这并不保证我们会找到全局最小值。
- en: 'The main challenges of gradient descent are the following:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度下降的主要挑战如下：
- en: Arbitrary initialization has an impact on the results
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 任意初始化对结果有影响
- en: Using different initialization values, we could encounter local minima instead
    of global minima.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 使用不同的初始化值，我们可能会遇到局部最小值而不是全局最小值。
- en: For example, starting at point B instead of point A in the previous figure above.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，从上图中的点B开始，而不是点A。
- en: Or, a less evident case, converging to a plateau (vanishing gradient problem)
    as shown by the blue line in the figure below.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 或者更不明显的情况，例如，像下图中的蓝线所示，收敛到一个平台（梯度消失问题）。
- en: '![](../Images/e06647b8def86857c479409b11888d71.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e06647b8def86857c479409b11888d71.png)'
- en: Vanilla gradient descent over a saddle surface. The animation shows how different
    initialization points can produce different results. Image by the author.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 香草梯度下降在鞍形表面上的效果。动画显示了不同初始化点可能产生不同结果的情况。图片由作者提供。
- en: '*💡* If you are curious about how to create this kind of animation, check out
    my article [Creating a Gradient Descent Animation in Python](/creating-a-gradient-descent-animation-in-python-3c4dcd20ca51).'
  id: totrans-55
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*💡* 如果你对如何创建这种动画感兴趣，可以查看我的文章 [在 Python 中创建梯度下降动画](/creating-a-gradient-descent-animation-in-python-3c4dcd20ca51)。'
- en: Selecting the appropriate step size requires a trade-off between convergence
    speed and stability
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There’s an interaction between the step size or learning rate, and the number
    of epochs we should use to achieve accurate results.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: Take for example the results of a parametric experiment, shown below. The images
    are from the online course [A Deep Understanding of Deep Learning](https://www.udemy.com/course/deeplearning_x/?ref=makerluis.com),
    by Mike X Cohen, which I highly recommend to anyone interested in Deep Learning
    and using [PyTorch](https://pytorch.org/?ref=makerluis.com), following a scientific
    approach.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: In this case, Mike showed how to test the results for a gradient descent optimization
    when changing the learning rate and the number of training epochs independently
    (one parameter over time, for a grid of different values). We can see how both
    parameters affect the results for this particular case.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4bd9c40334c3a73736990c791efe9f16.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
- en: Images are from the online course [A Deep Understanding of Deep Learning](https://www.udemy.com/course/deeplearning_x/?ref=makerluis.com),
    by Mike X Cohen.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: The true global minimum of the function is around -1.4\. For smaller learning
    rates, it takes a larger number of training epochs to converge to that result.
    So it would seem like just using a larger learning rate can help us reduce the
    computational time.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: But in practice, this isn’t a simple matter of convergence speed.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: Large step sizes can lead to very slow convergence, prevent the algorithm from
    converging at all (oscillating around the minima forever), or provoke a divergent
    behavior.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: The next figure shows how different learning rates affect the results of the
    optimization, even if we initialize the algorithm in the same location x = 2.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/17adbdaca929c0d64adab695e1b7f420.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
- en: Example of stochastic gradient descent applied to *f*(*x*)=*x^*2 with different
    step sizes. In all cases, the algorithm is initialized at x = 2\. Image by the
    author, with base code adapted from [Saraj Rival’s notebook](https://github.com/llSourcell/The_evolution_of_gradient_descent/blob/master/GD_vs_SGD.ipynb?ref=makerluis.com).
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: Here, it is evident that having large step sizes improves the convergence speed
    but only up to a certain point.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: Increasing the learning rate by an order of magnitude caused the algorithm to
    get stuck. The results for *η* = 1 oscillate between x = 2 and x = -2, and this
    is shown only by the blue horizontal line in the left figure.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: In some cases, a large step size might actually “shoot” the results to infinity,
    causing a numerical overflow of our program.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, too small step sizes can create a very slow convergence or
    no convergence at all.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: Gradient descent for training a Neural Network
  id: totrans-72
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the context of Deep Learning, the function we are trying to optimize is
    our loss function ***J***. We define our training losses as the average of the
    losses for all our training dataset:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
- en: Where Dtrain is the number of samples in our training dataset.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: Thus, we can implement gradient descent based on the following algorithm, in
    which we compute the gradient of the training losses to perform an update of the
    model weights, and repeat for a number of iterations[2]
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Since we are computing an average of the gradient of the loss function, we have
    a better estimate of it. For this reason, the weights update is more likely to
    be in the direction that improves the model performance.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: The problem with this implementation of gradient descent is its low speed.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: For a toy example with a few points and a simple function it might work well,
    but imagine we are developing an ANN and we have a million data points to train
    it.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
- en: To train the ANN with this algorithm, we would need to compute the outputs of
    the model for each sample of the training data and then average their losses as
    a large batch. Just to do *one* update of the weights. Then repeat it, over and
    over again, until we reach convergence.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
- en: This is called *Batch* Gradient Descent. It makes one (accurate) weight update
    per iteration, but each iteration can take a *long time* because we are repeating
    the model computation n times.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: To overcome this problem, we could use the so-called Stochastic Gradient Descent
    algorithm.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: Stochastic Gradient Descent
  id: totrans-83
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To overcome the slow convergence problem of Batch Gradient Descent, we can perform
    an update of the model weights based on each sample of the training set. The advantage
    is that we don’t need to wait until we have looped through the whole set to perform
    just one update of the weights.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: We can do this by using the Loss function for each individual sample, instead
    of the Training Loss which considers the whole dataset as a batch.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: This is what the Stochastic Gradient Descent Algorithm (SGD) looks like
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Notice that the step size is a function of the training iterations. That’s because,
    for the algorithm to converge, η must decrease as the number of iterations progresses.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: 'In the [lecture notes of MIT 6.036](https://openlearninglibrary.mit.edu/courses/course-v1:MITx+6.036+1T2019/courseware/Week4/gradient_descent/?activate_block_id=block-v1%3AMITx%2B6.036%2B1T2019%2Btype%40sequential%2Bblock%40gradient_descent%3Fref%3Dmakerluis.com&ref=makerluis.com)
    [1], the following theorem is mentioned:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: Theorem 4.1\. If J is convex, and η(t) is a sequence satisfying
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: then SGD converges with probability one to the optimal *θ*.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: 'People take different approaches to reduce the value of η as the training progresses,
    and this is often called “Annealing”:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: Changing the learning rate in proportion to the training epoch (for instance,
    η(t) = 1/t ), or setting it to a smaller value once a certain learning epoch has
    been reached. This method offers good results but is unrelated to the model performance.
    This is called “learning rate decay”, and is the industry standard to handle this
    problem.
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Multiplying the learning rate by the gradient of the loss function: This method
    is good because it’s adaptive to the problem, but requires careful scaling. This
    is incorporated into RMSprop and Adam variations of gradient descent.'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将学习率乘以损失函数的梯度：这种方法很好，因为它对问题是自适应的，但需要仔细调整。这已经被纳入了RMSprop和Adam梯度下降的变体中。
- en: 'Multiplying the learning rate by the losses: The advantage is that this is
    also adaptive to the problem, but requires scaling too.'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将学习率乘以损失：优点是这种方法同样对问题自适应，但也需要缩放。
- en: SGD may perform well after visiting only some of the data. This behavior can
    be useful for relatively large data sets, because we may reduce the amount of
    memory needed, and the total runtime compared to the “vanilla” implementation
    of gradient descent.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: SGD在仅访问部分数据后可能表现良好。这种行为对相对较大的数据集很有用，因为我们可以减少所需的内存量，并且与“原生”梯度下降实现相比，总运行时间较短。
- en: We could say that the Batch implementation is slower because it needs to run
    through all samples to perform a single update of the weights.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以说，批量实现较慢，因为它需要遍历所有样本才能进行一次权重更新。
- en: SGD performs an update at every sample, but the quality of the updates is lower.
    We may have noisy data or a really complex function we are trying to fit with
    our ANN.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: SGD在每个样本上执行更新，但更新的质量较低。我们可能有噪声数据或一个非常复杂的函数需要用我们的ANN拟合。
- en: Using a batch of size Dtrain is slow, but accurate, and using a batch of size
    1 is fast, but less accurate.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 使用大小为Dtrain的批量很慢，但准确，使用大小为1的批量很快，但不够准确。
- en: There is a term in between, and it’s called “mini-batch” gradient descent.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 在两者之间有一个术语，称为“mini-batch”梯度下降。
- en: Mini-Batch Gradient Descent
  id: totrans-101
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Mini-Batch梯度下降
- en: '![](../Images/e49352109af2b9cbce81357c4eb26785.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e49352109af2b9cbce81357c4eb26785.png)'
- en: Photo by [Sebastian Herrmann](https://unsplash.com/@herrherrmann?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由[Sebastian Herrmann](https://unsplash.com/@herrherrmann?utm_source=medium&utm_medium=referral)提供，来源于[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: If we split our data into smaller batches of equal size, we could do what Batch
    gradient descent does, but for each of those *mini-batches*.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们将数据分成大小相等的更小批量，我们可以做批量梯度下降所做的事情，但针对每个*mini-batch*。
- en: Let’s say we divide the data into 100 smaller parts.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们将数据分成100个更小的部分。
- en: We go through our data in 100 steps. On each step, we look at the Training Losses
    just for the data contained in the current mini-batch and improve our model parameters.
    We repeat this until we have looked at all samples, and start the cycle again.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将数据分成100步。在每一步中，我们仅查看当前mini-batch中数据的训练损失，并改进我们的模型参数。我们重复这一过程，直到查看所有样本，然后重新开始循环。
- en: Each cycle is known as an **epoch**. I have used the term more loosely before
    just to refer to the number of iterations during optimization, but the normal
    definition refers to each cycle through the training dataset. For Batch gradient
    descent, an *iteration* is an epoch.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 每个循环称为**周期**。我之前更宽泛地使用了这个术语来指代优化过程中的迭代次数，但通常的定义指的是每次遍历训练数据集。对于批量梯度下降，*迭代*就是一个周期。
- en: When we set how many epochs we want to use for training an ANN, we are defining
    how many passes through the training dataset we will make.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们设置训练ANN的周期数时，我们是在定义通过训练数据集的遍历次数。
- en: The advantage of using mini-batches is that we update our model parameters on
    each mini-batch, rather than after we have looked at the whole data set.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 使用mini-batch的优点是我们在每个mini-batch上更新模型参数，而不是在查看整个数据集后再更新。
- en: 'For Batch gradient descent, the batch size is the total number of samples in
    the training dataset. For mini-batch gradient descent, the mini-batches are usually
    powers of two: 32 samples, 64, 128, 256, and so on.'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 对于批量梯度下降，批量大小是训练数据集中样本的总数。对于mini-batch梯度下降，mini-batch通常是2的幂次：32个样本、64、128、256等。
- en: SGD would be an extreme case when the mini-batch size is reduced to a single
    example in the raining dataset.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 当mini-batch大小减少到训练数据集中的单个示例时，SGD将是一个极端情况。
- en: The disadvantage of using a mini-batch gradient in our optimization process
    is that we incorporate a level of variability — although less severe than with
    SDG. It is not guaranteed that every step will move us closer to the ideal parameter
    values, but the general direction is still towards the minimum.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 使用mini-batch梯度下降的缺点是我们引入了一定程度的变异性——虽然比随机梯度下降（SDG）轻微。这并不保证每一步都会使我们更接近理想参数值，但总体方向仍然是向最小值靠近。
- en: This method is one of the industry standards because by finding the optimal
    batch size, we can choose a compromise between speed and accuracy when dealing
    with very large datasets.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: Thank you for reading! I hope this article was interesting and helped you clear
    some concepts. I’m also sharing the sources I used when writing this, in case
    you are interested in going through deeper and more formal material.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: In a future post, I’ll write about [more advanced methods of gradient descent](https://medium.com/towards-data-science/dl-notes-advanced-gradient-descent-4407d84c2515)
    (the ones people use in real applications) and how we actually update the model
    weights during training, using backpropagation, as gradient descent is only one
    part of the full story.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: 'In the meantime, you might be interested in reading my previous article, about
    Feedforward Artificial Neural Networks:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/@luisdamed/feedforward-artificial-neural-networks-52bcf96d6ac3?source=post_page-----f09f19eb35fb--------------------------------)
    [## Feedforward Artificial Neural Networks'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: The basic concept, explained
  id: totrans-118
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: medium.com](https://medium.com/@luisdamed/feedforward-artificial-neural-networks-52bcf96d6ac3?source=post_page-----f09f19eb35fb--------------------------------)
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: References
  id: totrans-120
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[1] [MIT Open Learning Library: 6.036 Introduction to Machine Learning. Chapter
    6: Gradient Descent](https://openlearninglibrary.mit.edu/assets/courseware/v1/d81d9ec0bd142738b069ce601382fdb7/asset-v1:MITx+6.036+1T2019+type@asset+block/notes_chapter_Gradient_Descent.pdf?ref=makerluis.com)'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: '[2] [Standford Online: Artificial Intelligence & Machine Learning 4 — Stochastic
    Gradient Descent | Stanford CS221 (2021)](https://www.youtube.com/watch?v=bl2WgBLH0tI%3Fref%3Dmakerluis.com&ref=makerluis.com)'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: '[3] Online course [A Deep Understanding of Deep Learning](https://www.udemy.com/course/deeplearning_x/?ref=makerluis.com),
    by Mike X Cohen ( [sincxpress.com](https://sincxpress.com/?ref=makerluis.com))'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: '[4] [Standford Online: CS231 Convolutional Neural Networks for Visual Recognition](https://cs231n.github.io/neural-networks-3?ref=makerluis.com)'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: '*Originally published at* [*https://www.makerluis.com*](https://www.makerluis.com/gradient-descent/)
    *on November 4, 2023.*'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
