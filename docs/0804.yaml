- en: How to Understand and Use the Jensen-Shannon Divergence
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何理解和使用Jensen-Shannon散度
- en: 原文：[https://towardsdatascience.com/how-to-understand-and-use-jensen-shannon-divergence-b10e11b03fd6?source=collection_archive---------0-----------------------#2023-03-02](https://towardsdatascience.com/how-to-understand-and-use-jensen-shannon-divergence-b10e11b03fd6?source=collection_archive---------0-----------------------#2023-03-02)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/how-to-understand-and-use-jensen-shannon-divergence-b10e11b03fd6?source=collection_archive---------0-----------------------#2023-03-02](https://towardsdatascience.com/how-to-understand-and-use-jensen-shannon-divergence-b10e11b03fd6?source=collection_archive---------0-----------------------#2023-03-02)
- en: '![](../Images/f6c7a2a138f1455efd358eb0f3ecb001.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f6c7a2a138f1455efd358eb0f3ecb001.png)'
- en: Image by author
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片
- en: A primer on the math, logic, and pragmatic application of JS Divergence — including
    how it is best used in drift monitoring
  id: totrans-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 关于JS散度的数学、逻辑和实际应用的入门 — 包括如何在漂移监控中最佳使用它
- en: '[](https://aparnadhinak.medium.com/?source=post_page-----b10e11b03fd6--------------------------------)[![Aparna
    Dhinakaran](../Images/e431ee69563ecb27c86f3428ba53574c.png)](https://aparnadhinak.medium.com/?source=post_page-----b10e11b03fd6--------------------------------)[](https://towardsdatascience.com/?source=post_page-----b10e11b03fd6--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----b10e11b03fd6--------------------------------)
    [Aparna Dhinakaran](https://aparnadhinak.medium.com/?source=post_page-----b10e11b03fd6--------------------------------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://aparnadhinak.medium.com/?source=post_page-----b10e11b03fd6--------------------------------)[![Aparna
    Dhinakaran](../Images/e431ee69563ecb27c86f3428ba53574c.png)](https://aparnadhinak.medium.com/?source=post_page-----b10e11b03fd6--------------------------------)[](https://towardsdatascience.com/?source=post_page-----b10e11b03fd6--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----b10e11b03fd6--------------------------------)
    [Aparna Dhinakaran](https://aparnadhinak.medium.com/?source=post_page-----b10e11b03fd6--------------------------------)'
- en: ·
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ff32f85889f3a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-understand-and-use-jensen-shannon-divergence-b10e11b03fd6&user=Aparna+Dhinakaran&userId=f32f85889f3a&source=post_page-f32f85889f3a----b10e11b03fd6---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----b10e11b03fd6--------------------------------)
    ·9 min read·Mar 2, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fb10e11b03fd6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-understand-and-use-jensen-shannon-divergence-b10e11b03fd6&user=Aparna+Dhinakaran&userId=f32f85889f3a&source=-----b10e11b03fd6---------------------clap_footer-----------)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ff32f85889f3a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-understand-and-use-jensen-shannon-divergence-b10e11b03fd6&user=Aparna+Dhinakaran&userId=f32f85889f3a&source=post_page-f32f85889f3a----b10e11b03fd6---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----b10e11b03fd6--------------------------------)
    ·9分钟阅读·2023年3月2日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fb10e11b03fd6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-understand-and-use-jensen-shannon-divergence-b10e11b03fd6&user=Aparna+Dhinakaran&userId=f32f85889f3a&source=-----b10e11b03fd6---------------------clap_footer-----------)'
- en: --
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb10e11b03fd6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-understand-and-use-jensen-shannon-divergence-b10e11b03fd6&source=-----b10e11b03fd6---------------------bookmark_footer-----------)'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb10e11b03fd6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-understand-and-use-jensen-shannon-divergence-b10e11b03fd6&source=-----b10e11b03fd6---------------------bookmark_footer-----------)'
- en: '*This piece is co-authored with Jason Lopatecki, CEO and Co-Founder of Arize
    AI*'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '*本文由Arize AI的首席执行官兼联合创始人Jason Lopatecki共同撰写*'
- en: 'In machine learning systems, drift monitoring can be critical to delivering
    quality ML. Some common use cases for drift analysis in production ML systems
    include:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习系统中，漂移监控对于提供高质量的ML至关重要。一些在生产ML系统中漂移分析的常见用例包括：
- en: Detect feature changes between training and production to catch problems ahead
    of performance dips
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 检测训练和生产之间的特征变化，以在性能下降之前发现问题
- en: Detect prediction distribution shifts between two production periods as a proxy
    for performance changes (especially useful in delayed ground truth scenarios)
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 检测两个生产周期之间的预测分布变化，以作为性能变化的代理（特别适用于延迟真实情况）
- en: Use drift as a signal for when to retrain — and how often to retrain
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用漂移作为重新训练的信号——以及重新训练的频率
- en: Catch feature transformation issues or pipeline breaks
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 捕捉特征转换问题或管道中断
- en: Detect default fallback values used erroneously
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 检测错误使用的默认回退值
- en: Find new data to go label
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 查找新的数据以进行标记
- en: Find clusters of new data that are problematic for the model in unstructured
    data
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 查找对模型有问题的新数据集群，特别是在非结构化数据中
- en: Find anomalous clusters of data that are not in the training set
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 查找不在训练集中的异常数据集群
- en: While there is no perfect drift metric, the field has learned a lot over the
    past decade and there are some well-tested approaches and metrics that are useful
    depending on your use case.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然没有完美的漂移度量，但在过去十年里这个领域已经学到了很多，有一些经过良好测试的方法和度量在不同的使用场景中非常有用。
- en: One such measure is **Jensen Shannon divergence** (JS Div). [Also](https://nlp.stanford.edu/fsnlp/)
    [referred](https://dl.acm.org/doi/10.3115/976909.979625) to as **total divergence
    to the average** or **information radius**, JS divergence is a statistical metric
    with a basis in information theory**.** The advantage of JS divergence over other
    metrics like is mostly related to issues with empty probabilities for certain
    events or bins and how these cause issues with Kullback-Leibler divergence ([KL
    Divergence](https://arize.com/blog-course/kl-divergence/)) and population stability
    index (PSI). JS divergence uses a mixture probability as a baseline when comparing
    two distributions. In the discrete versions of PSI and KL divergence, the equations
    blow up when there are 0 probability events.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 一种这样的度量是**詹森-香农散度**（JS Div）。[此外](https://nlp.stanford.edu/fsnlp/) [也](https://dl.acm.org/doi/10.3115/976909.979625)被称为**相对于平均值的总散度**或**信息半径**，JS散度是一个基于信息理论的统计度量**。**
    JS散度相比于其他度量的优势主要与某些事件或箱的空概率问题有关，这些问题会影响Kullback-Leibler散度（[KL散度](https://arize.com/blog-course/kl-divergence/)）和人口稳定性指数（PSI）。JS散度在比较两个分布时使用混合概率作为基线。在PSI和KL散度的离散版本中，当出现0概率事件时，方程会出现爆炸。
- en: This blog post covers what JS divergence is and how it differs from KL divergence,
    how to use JS divergence in drift monitoring, and how mixture distribution resolves
    a common measurement problems.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 本博客文章涵盖了JS散度的定义、它与KL散度的不同、如何在漂移监测中使用JS散度，以及混合分布如何解决常见的测量问题。
- en: JS Divergence Overview
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: JS散度概述
- en: Jensen-Shannon is an asymmetric metric that [measures the relative entropy](http://hanj.cs.illinois.edu/cs412/bk3/KL-divergence.pdf)
    or difference in information represented by two distributions. Closely related
    to KL Divergence, it can be thought of as measuring the distance between two data
    distributions showing how different the two distributions are from each other.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 詹森-香农是一种非对称度量，[测量相对熵](http://hanj.cs.illinois.edu/cs412/bk3/KL-divergence.pdf)或两个分布所表示信息的差异。与KL散度密切相关，它可以被视为测量两个数据分布之间的距离，显示这两个分布之间的差异。
- en: 'The following shows the symmetry with KL Divergence:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了与KL散度的对称性：
- en: '![](../Images/22b530ede09a7031f31c5b481a365440.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/22b530ede09a7031f31c5b481a365440.png)'
- en: Image by author
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供
- en: 'And a discrete form of JS Divergence:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 以及JS散度的离散形式：
- en: '![](../Images/9dd111bba855d65aee45fd3adfde9f22.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9dd111bba855d65aee45fd3adfde9f22.png)'
- en: Image by author
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供
- en: 'Where the mixture distribution is:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 其中混合分布是：
- en: '![](../Images/28a1c4ba276bee31cfbe73b072d8d5f2.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/28a1c4ba276bee31cfbe73b072d8d5f2.png)'
- en: Image by author
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供
- en: For more background, one of the better technical [papers](https://www.mdpi.com/1099-4300/21/5/485)
    on JS Divergence is written by Frank Nielsen of Sony Computer Science Laboratories.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 更多背景信息中，关于JS散度的技术[论文](https://www.mdpi.com/1099-4300/21/5/485)由Sony Computer
    Science Laboratories的Frank Nielsen撰写。
- en: In model monitoring, the discrete form of JS divergence is typically used to
    obtain the discrete distributions by [binning data](https://arize.com/blog-course/data-binning-production/).
    The discrete form of JS and continuous forms converge as the number of samples
    and bins move to infinity. There are optimal [selection approaches](https://stats.stackexchange.com/questions/510699/discrete-kl-divergence-with-decreasing-bin-width)
    to the number of bins to approach the continuous form.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在模型监测中，JS散度的离散形式通常用于通过[数据分箱](https://arize.com/blog-course/data-binning-production/)来获取离散分布。JS的离散形式和连续形式会随着样本和箱数趋于无穷大而收敛。有优化的[选择方法](https://stats.stackexchange.com/questions/510699/discrete-kl-divergence-with-decreasing-bin-width)来选择箱数以接近连续形式。
- en: How Is JS Divergence Used In Drift Monitoring?
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: JS散度如何在漂移监测中使用？
- en: In model monitoring, JS divergence is similar to PSI in that it is used to monitor
    production environments, specifically around feature and prediction data. JS divergence
    is also utilized to ensure that input or output data in production doesn’t drastically
    change from a baseline. The baseline can be a training production window of data
    or a training/validation dataset.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在模型监控中，JS 散度类似于 PSI，因为它用于监控生产环境，特别是特征和预测数据方面。JS 散度还用于确保生产中的输入或输出数据没有从基准数据中发生剧烈变化。基准数据可以是数据的训练生产窗口或训练/验证数据集。
- en: Drift monitoring can be especially useful for teams that receive delayed ground
    truth to compare against production model decisions. Teams rely on changes in
    prediction and feature distributions as a proxy for performance changes.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 漂移监控对于那些收到延迟地面真相以与生产模型决策进行比较的团队尤为有用。团队依赖于预测和特征分布的变化作为性能变化的代理。
- en: JS divergence is typically applied to each feature independently; it is not
    designed as a covariant feature measurement but rather a metric that shows how
    each feature has diverged independently from the baseline values. Although JS
    divergence does uniquely support a multi-distribution mixture approach, it really
    is not designed for comparing completely disparate distributions — it’s not a
    mulit-variate drift measurement.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: JS 散度通常应用于每个特征独立地；它不是作为协方差特征测量的设计，而是显示每个特征如何独立于基准值发生偏离的度量。虽然 JS 散度确实支持多分布混合方法，但它实际上并不适用于比较完全不同的分布——它不是一个多变量漂移测量。
- en: 'The challenge with JS divergence — and also its advantage — is that the comparison
    baseline is a mixture distribution. Think of JS Divergence as occurring in two
    steps:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: JS 散度的挑战 — 也是它的优势 — 是比较基准是一个混合分布。可以将 JS 散度视为两个步骤进行：
- en: '**Step 1:**'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤 1：**'
- en: Create mixture distribution for comparison using the production and baseline
    distributions;
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 使用生产和基准分布创建混合分布进行比较；
- en: '![](../Images/f203a178e7099ee8af66f9d2f0e05999.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f203a178e7099ee8af66f9d2f0e05999.png)'
- en: Image by author
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片
- en: '**Step 2:**'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤 2：**'
- en: Compare production and baseline to mixture.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 比较生产和基准与混合。
- en: '![](../Images/5a881c2453f0edc36e623dcf73c4bc09.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5a881c2453f0edc36e623dcf73c4bc09.png)'
- en: Image by author
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片
- en: The above diagram shows the A distribution, B distribution and mixture distribution.
    The JS Divergence is calculated by comparing the JS distribution to both A & B.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 上图展示了 A 分布、B 分布和混合分布。JS 散度是通过将 JS 分布与 A 和 B 进行比较来计算的。
- en: '*✏️****NOTE****:* sometimes non-practitioners have a somewhat overzealous goal
    of perfecting the mathematics of catching data changes. In practice, it’s important
    to keep in mind that real data changes all the time in production and many models
    extend well to this modified data. The goal of using drift metrics is to have
    a solid, stable and strongly useful metric that enables troubleshooting.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '*✏️****注意****：* 有时非从业者对捕捉数据变化的数学完美目标过于热衷。在实践中，重要的是要记住真实数据在生产中一直在变化，许多模型可以很好地适应这些修改的数据。使用漂移度量的目标是拥有一个可靠、稳定且非常有用的度量，以便进行故障排除。'
- en: JS Divergence Is a Symmetric Metric
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: JS 散度是对称度量
- en: JS divergence is similar to PSI in that it is a symmetric metric. If you swap
    the baseline distribution p(x) and sample distribution q(x), you will get the
    **same** number. This has several advantages compared to KL divergence for troubleshooting
    data model comparisons. There are times where teams want to swap out a comparison
    baseline for a different distribution in a troubleshooting workflow, and having
    a metric where *A / B* is the same as *B / A* can make comparing results much
    easier.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: JS 散度类似于 PSI，因为它是一个对称度量。如果你交换基准分布 p(x) 和样本分布 q(x)，你将得到**相同**的数字。这相比于 KL 散度在故障排除数据模型比较中有几个优势。有时团队希望在故障排除工作流中将比较基准替换为不同的分布，而拥有一个度量使得
    *A / B* 与 *B / A* 相同，可以使比较结果更容易。
- en: '![](../Images/460446b933160356df397264124821b7.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/460446b933160356df397264124821b7.png)'
- en: Image by author
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片
- en: 'JS Divergence Advantage: Handling Zero Bins'
  id: totrans-55
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: JS 散度的优势：处理零 bin
- en: The main advantage of JS divergence is that the mixture distribution allows
    the calculation to handle bin comparisons to 0\. With KL Divergence, if you are
    comparing 0 bins the equation essentially blows up.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: JS 散度的主要优势在于混合分布允许计算处理 bin 比较到 0。对于 KL 散度，如果你比较 0 bin，方程式基本上会爆炸。
- en: '![](../Images/a5f423837cad2e92371f824ea53e1141.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a5f423837cad2e92371f824ea53e1141.png)'
- en: Image by author
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片
- en: 'As you can see in the image above, there are two buckets where one bucket is
    0 in the current time frame and the other has a value. The approach with JS Divergence
    to handle the 0 bucket is to take the two terms in JS Divergence and assume one
    is *0 (0*ln(0) = 0)* as the function is smooth and has a limit as it approaches
    0 and the other has a value:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 如上图所示，有两个桶，其中一个桶在当前时间段内为0，另一个有一个值。处理0桶的JS散度方法是将JS散度中的两个项假设其中一个为*0 (0*ln(0) =
    0)*，因为函数是光滑的，并且在接近0时有一个极限，而另一个有一个值：
- en: '![](../Images/361552a8aeefca01ea9811e52a0fee49.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/361552a8aeefca01ea9811e52a0fee49.png)'
- en: 'Assuming one term is 0, you have for the 0 bin:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 假设一个项为0，你在0桶中得到：
- en: '![](../Images/ec7b89a5476dd2e7719e392f5ca52055.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ec7b89a5476dd2e7719e392f5ca52055.png)'
- en: 'This will not work with KL divergence or PSI as you would divide by 0 in the
    denominator:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 这在KL散度或PSI中不起作用，因为你会在分母中除以0：
- en: '![](../Images/990a2823111543843e66c52a9623f45b.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/990a2823111543843e66c52a9623f45b.png)'
- en: 'In the case where q(x) = 0 you have:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在q(x) = 0的情况下，你有：
- en: '![](../Images/3fd477d5b3709d59cc9e6c7652d566b4.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3fd477d5b3709d59cc9e6c7652d566b4.png)'
- en: '*Advantage: The zero bins are handled naturally without issue*'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '*优势：零桶自然处理，无问题*'
- en: '✏️NOTE: where preferred, it’s also possible to make a modification that allows
    KL Divergence and PSI to be used on distributions with 0 bins.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: ✏️注意：在需要时，也可以进行修改，以允许KL散度和PSI用于0桶的分布。
- en: 'JS Divergence Disadvantage: Mixture Distribution Moves'
  id: totrans-69
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: JS散度的劣势：混合分布移动
- en: The disadvantage of JS divergence actually derives from its advantage, namely
    that the comparison distribution is a “mixture” of both distributions.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: JS散度的劣势实际上源于它的优势，即比较分布是两个分布的“混合”。
- en: In the case of PSI or KL divergence, the baseline comparison distribution is
    static comparison distribution, fixed in every comparison time period. This allows
    you to get a stable metric that means the same thing on every comparison and in
    every period. For example, if you have a PSI value on one day of 0.2 then a week
    later it is 0.2 this implies the entropy difference to the baseline is the same
    on both of these days. That is not necessarily the case with JS divergence.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在PSI或KL散度的情况下，基线比较分布是静态比较分布，在每次比较时间段内固定。这使你能够获得一个稳定的指标，在每次比较和每个时期都具有相同的含义。例如，如果一天的PSI值为0.2，那么一周后仍然是0.2，这意味着这两天与基线的熵差异是相同的。这在JS散度中不一定如此。
- en: In the case of JS divergence, the mixture distribution changes every time you
    run a comparison because the production distribution changes every sample period.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在JS散度的情况下，每次进行比较时混合分布都会改变，因为生产分布在每个样本周期内都会变化。
- en: '![](../Images/644033335603c6bfbd7ba7eb634d59e4.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/644033335603c6bfbd7ba7eb634d59e4.png)'
- en: '*Calculation of Mixture Distribution for Each Monitoring Day (Image by author)*'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '*每个监测日的混合分布计算（图片由作者提供）*'
- en: The chart above shows an example of a mixture distribution calculated for two
    different timeframes. The mixture acts like a slowly moving baseline that smoothly
    connects the baseline at time A with time B by averaging differences.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 上图展示了为两个不同时间框架计算的混合分布的示例。混合分布像是一个缓慢移动的基线，通过平均差异将时间A的基线平滑地连接到时间B。
- en: Differences Between Continuous Numeric and Categorical Features
  id: totrans-76
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 连续数值和分类特征之间的差异
- en: JS divergence can be used to measure differences between numeric distributions
    and categorical distributions.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: JS散度可以用来测量数值分布和分类分布之间的差异。
- en: '![](../Images/1c4e470bdbe494a24cb1759ceac225fa.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1c4e470bdbe494a24cb1759ceac225fa.png)'
- en: Image by author
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供
- en: Numerics
  id: totrans-80
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数值
- en: In the case of numeric distributions, the data is split into bins based on cutoff
    points, bin sizes and bin widths. The binning strategies can be even bins, quintiles
    and complex mixes of strategies that ultimately affect JS divergence (stay tuned
    for a future write-up on binning strategy).
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 对于数值分布，数据基于分割点、桶大小和桶宽度被分成桶。桶化策略可以是均匀桶、五分位数或复杂的混合策略，最终影响JS散度（请关注未来关于桶化策略的文章）。
- en: Categorical
  id: totrans-82
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分类
- en: The monitoring of JS divergence tracks large distributional shifts in the categorical
    datasets. In the case of categorical features, often there is a size where the
    cardinality gets too large for the measure to have much usefulness. The ideal
    size is around 50–100 unique values — as a distribution has higher cardinality,
    the question of how different the two distributions and whether it matters gets
    muddied.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: JS 散度的监控跟踪分类数据集中大的分布变化。对于分类特征，通常有一个大小点，卡尔多性变得过大，导致度量值不再有用。理想的大小在 50–100 个独特值左右——随着分布的卡尔多性增加，两个分布的差异及其重要性变得模糊。
- en: High Cardinality
  id: totrans-84
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 高卡尔多性
- en: 'In the case of high cardinality feature monitoring, out-of-the-box statistical
    distances do not generally work well — instead, it is advisable to use one of
    these options instead:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 对于高卡尔多性特征监控，现成的统计距离通常效果不好——建议使用以下其中一种选项：
- en: '**Embeddings:** In some high cardinality situations, the values being used
    — such as *User ID* or *Content ID* — are already used to create embeddings internally.
    Monitoring embeddings can help.'
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**嵌入：** 在一些高卡尔多性的情况下，使用的值——例如*用户 ID* 或 *内容 ID*——已经用于内部创建嵌入。监控嵌入可以有所帮助。'
- en: '**Pure High Cardinality Categorical:** In other cases, where the model has
    encoded the inputs to a large space, just monitoring the top 50–100 top items
    with JS divergence and all other values as “other” can be useful.'
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**纯高卡尔多性分类变量：** 在其他情况下，当模型对输入进行大规模编码时，仅监控前 50–100 个最重要项的 JS 散度，而将所有其他值视为“其他”，可能会很有用。'
- en: Of course, sometimes what you want to monitor is something very specific — like
    the percent of new values or bins in a period. These are better set up with data
    quality monitors.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，有时你可能想监控的是非常具体的内容——例如某一时期内新值或箱的百分比。此类情况更适合使用数据质量监控工具。
- en: JS Divergence Example
  id: totrans-89
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: JS 散度示例
- en: Here is an example of JS divergence with both numeric and categorical features.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个包含数值和分类特征的 JS 散度示例。
- en: '![](../Images/9d02fb7b9df255591b340a7d770477dd.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9d02fb7b9df255591b340a7d770477dd.png)'
- en: Image by author
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片
- en: 'Imagine you work at a credit card company and have a numeric distribution of
    charge amounts for a fraud model. The model was built with the baseline shown
    in the picture above from training. We can see that the distribution of charges
    has shifted. There are a number of industry standards around thresholds for PSI
    but as one can see the values are very different for JS divergence. The 0.2 standard
    for PSI does not apply to JS divergence. At Arize (full disclosure: I work at
    Arize), we typically look at a moving window of values over a multi-day period
    to set a threshold for each feature.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你在一家信用卡公司工作，负责为欺诈模型制定数值分布。模型是基于上图所示的基线进行训练的。我们可以看到费用分布发生了变化。行业标准在 PSI 阈值方面有许多，但如你所见，JS
    散度的值却非常不同。0.2 的 PSI 标准不适用于 JS 散度。在 Arize（完全披露：我在 Arize 工作），我们通常查看多日周期内的移动窗口值，以为每个特征设置阈值。
- en: The example shows a numeric variable and JS divergence over the distribution.
    In the example above, it’s worth noting that a nine point drop from 12% to 3%
    for bin 95–97 causes a 1.4 movement in JS. This is exactly mirrored by a nine
    point increase from 3% to 12% for bin 105–107\. PSI works in a similar symmetric
    manner to JS. This is not the same for KL divergence. In the case of KL Divergence,
    the 12%->3% causes a larger movement in the number.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 该示例展示了一个数值变量及其分布上的 JS 散度。在上述示例中，值得注意的是，bin 95–97 从 12% 降到 3% 导致 JS 散度移动了 1.4。对于
    bin 105–107 的 3% 到 12% 的九个百分点增加，JS 散度也有类似的变化。PSI 以类似对称的方式工作，这与 KL 散度不同。对于 KL 散度，12%->3%
    会导致数值上更大的变化。
- en: Intuition Behind JS Divergence
  id: totrans-95
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: JS 散度的直觉
- en: It’s important to intrinsically understand some of the logic around the metric
    and changes in the metric based on distribution changes.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 理解一些关于度量和基于分布变化的度量变化的逻辑非常重要。
- en: '![](../Images/253819f9d2f1110915eeb8c68c532215.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/253819f9d2f1110915eeb8c68c532215.png)'
- en: Image by author
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片
- en: The above example shows a move from one categorical bin to another. The predictions
    with “medical” as input on a feature (use of loan proceeds) increase from 2% to
    8%, while the predictions with “vacation” decrease from 23% to 17%.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 上述示例展示了从一个分类 bin 到另一个分类 bin 的迁移。以“医疗”为输入的预测（贷款用途）从 2% 增加到 8%，而以“度假”为输入的预测从 23%
    减少到 17%。
- en: In this example, the component to JS divergence related to “medical” is 0.96
    and is larger than the component for the “vacation” percentage movement of 0.023\.
    This is the opposite of what you get with KL divergence.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，与“医疗”相关的 JS 散度组件是 0.96，远大于“度假”百分比移动的 0.023。这与 KL 散度得到的结果正好相反。
- en: '![](../Images/8f3c0d38c1731ec8ac64dd9643ed071c.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8f3c0d38c1731ec8ac64dd9643ed071c.png)'
- en: Image by author
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供
- en: Conclusion
  id: totrans-103
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: '[JS divergence](https://arize.com/blog-course/jensen-shannon-divergence/) is
    a common way to measure drift. It has some great properties in that it is symmetric
    and handles the 0 bin comparison naturally but also has some drawbacks in the
    moving mixture as a baseline. Depending on your use case, it can be a great choice
    for a drift metric.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '[JS 散度](https://arize.com/blog-course/jensen-shannon-divergence/) 是一种常见的漂移测量方法。它具有对称性，并自然处理
    0 桶比较，但在使用移动混合作为基准时也有一些缺点。根据你的使用场景，它可能是一个很好的漂移度量选择。'
- en: That said, it is not the only choice. One alternative is to use [population
    stability index](https://arize.com/blog-course/population-stability-index-psi/)
    along with an out-of-distribution binning technique to handle zero bins. Leveraging
    an out-of-distribution binning technique can allow teams to both dial in how sensitive
    you want the metric to be out of distribution events and easily compare to a fixed
    baseline distribution (there is no mixture). Stay tuned for additional pieces
    covering this and binning best practices!
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 也就是说，这并不是唯一的选择。另一种替代方法是使用 [人口稳定性指数](https://arize.com/blog-course/population-stability-index-psi/)
    结合分布外分桶技术来处理零桶。利用分布外分桶技术可以让团队调节度量对分布外事件的敏感性，并轻松与固定基准分布进行比较（没有混合）。敬请关注更多关于此主题和分桶最佳实践的后续内容！
