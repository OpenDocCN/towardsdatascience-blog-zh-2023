["```py\nimport pandas as pd\n\nplant = pd.read_csv(\"plant_generated.csv\")\nplant.head()\n```", "```py\n group  time        var1       var2        var3       var4        var5\n0  train     1   83.346705  28.115372  455.898265  12.808663  227.974152\n1  train     2   90.594521  33.497319  462.503195  14.079053  228.173486\n2  train     3  101.275664  30.396332  492.407791  16.832834  250.212025\n3  train     4   90.898109  29.143537  472.162499  16.505277  234.079354\n4  train     5   84.898605  29.459506  467.872180  12.801665  238.440786\n```", "```py\nvariables = plant.columns[2:].tolist()\nprint(variables)\n```", "```py\n['var1', 'var2', 'var3', 'var4', 'var5']\n```", "```py\nplant.groupby([\"group\"]).agg({\"time\": [len, min, max]})\n```", "```py\n time\n       len  min  max\ngroup   \n test   30    1   30\ntrain  100    1  100\n```", "```py\nplant.loc[plant.group==\"train\", variables].corr()\n```", "```py\n var1      var2     var3       var4      var5\nvar1  1.000000  0.536242  0.773872  0.534615  0.727272\nvar2  0.536242  1.000000  0.535442  0.451099  0.632337\nvar3  0.773872  0.535442  1.000000  0.613662  0.709600\nvar4  0.534615  0.451099  0.613662  1.000000  0.571399\nvar5  0.727272  0.632337  0.709600  0.571399  1.000000\n```", "```py\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\n\nX_train = np.array(plant.loc[plant.group==\"train\", variables])\nZ_train = scaler.fit_transform(X_train)\n```", "```py\nu, s, vh = np.linalg.svd(Z_train)\nloadings = np.transpose(vh)\nprint(loadings)\n```", "```py\narray([[ 0.46831293, -0.07447163,  0.50970523,  0.1634115 , -0.69902377],\n       [ 0.40387459,  0.76147727, -0.41265167,  0.29133908, -0.04333374],\n       [ 0.47556921, -0.21952275,  0.30115426,  0.43988217,  0.66441965],\n       [ 0.40644337, -0.58699292, -0.68019685, -0.01720318, -0.16516475],\n       [ 0.47561121,  0.14783576,  0.12867607, -0.83344223,  0.20187888]])\n```", "```py\ntrain_scores = Z_train @ loadings\n```", "```py\nn = Z_train.shape[0]\nvariances = s**2 / (n - 1)\n```", "```py\npd.DataFrame({\n    \"Component\": [*range(1, len(s)+1)],\n    \"Variance\": variances,\n    \"Proportion\": variances/np.sum(variances),\n    \"Cumulative\": np.cumsum(variances/np.sum(variances))\n})\n```", "```py\n Component  Variance  Proportion  Cumulative\n0          1  3.485770    0.690182    0.690182\n1          2  0.573965    0.113645    0.803828\n2          3  0.498244    0.098652    0.902480\n3          4  0.276403    0.054728    0.957208\n4          5  0.216122    0.042792    1.000000\n```", "```py\nn_comp = 2\npca_loadings = loadings[:,:n_comp]\npca_variances = variances[:n_comp]\n```", "```py\nX_test = np.array(plant.loc[plant.group==\"test\", variables])\nZ_test = scaler.transform(X_test)\n```", "```py\ntest_scores = Z_train @ pca_loadings\n```", "```py\ntest_expect = test_scores @ np.transpose(pca_loadings)\n```", "```py\ncontrib_Q = Z_test - test_expect\n```", "```py\npc_scaling = np.diag(1/np.sqrt(pca_variances))\ncontrib_T2 = test_scores @ pc_scaling @ np.transpose(pca_loadings)\n```"]