- en: Time Series Complexity analysis using Entropy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/time-series-complexity-analysis-using-entropy-ec49a4aaff11?source=collection_archive---------2-----------------------#2023-09-04](https://towardsdatascience.com/time-series-complexity-analysis-using-entropy-ec49a4aaff11?source=collection_archive---------2-----------------------#2023-09-04)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Here is a method to understand how complex your time series are, in a few lines
    of code
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://piero-paialunga.medium.com/?source=post_page-----ec49a4aaff11--------------------------------)[![Piero
    Paialunga](../Images/de2185596a49484698733e85114dd1ff.png)](https://piero-paialunga.medium.com/?source=post_page-----ec49a4aaff11--------------------------------)[](https://towardsdatascience.com/?source=post_page-----ec49a4aaff11--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----ec49a4aaff11--------------------------------)
    [Piero Paialunga](https://piero-paialunga.medium.com/?source=post_page-----ec49a4aaff11--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F254e653181d2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftime-series-complexity-analysis-using-entropy-ec49a4aaff11&user=Piero+Paialunga&userId=254e653181d2&source=post_page-254e653181d2----ec49a4aaff11---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----ec49a4aaff11--------------------------------)
    ·8 min read·Sep 4, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fec49a4aaff11&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftime-series-complexity-analysis-using-entropy-ec49a4aaff11&user=Piero+Paialunga&userId=254e653181d2&source=-----ec49a4aaff11---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fec49a4aaff11&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftime-series-complexity-analysis-using-entropy-ec49a4aaff11&source=-----ec49a4aaff11---------------------bookmark_footer-----------)![](../Images/0eeba7c01e80fa5091b16ba778ac2712.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Image by author, generated using Midjourney
  prefs: []
  type: TYPE_NORMAL
- en: 'Every data scientist knows this: **the first step to the solution of a Machine
    Learning problem is the exploration of the data.**'
  prefs: []
  type: TYPE_NORMAL
- en: And it’s not only about understanding which features can help you in solving
    the problem. That is actually something that requires domain knowledge, a lot
    of effort, a lot of asking around and trying to find out. That is a necessary
    step, but in my opinion, is step number **two**.
  prefs: []
  type: TYPE_NORMAL
- en: The first step is in some way, shape, or form, based on the analysis of how
    **complex** your data is. Are they asking you to find fine details and pattern
    in something that is kind of always the same, or the outputs are completely different
    from each other? Do they want you to find the distance between 0.0001 and 0.0002
    or do they want you to find the distance between 0 and 10?
  prefs: []
  type: TYPE_NORMAL
- en: Let me explain myself better.
  prefs: []
  type: TYPE_NORMAL
- en: For example, I am a **signal processing guy**. I studied Fourier Transform,
    Chirplet Transform, Wavelet Transform, Hilbert Transform, Time Series Forecasting,
    Time Series Clustering, 1D CNN, RNN, and a lot of other scary names.
  prefs: []
  type: TYPE_NORMAL
- en: 'A very common problem in the Time Series domain is going from an input (that
    might indeed be another time series) to a time series output. For example:'
  prefs: []
  type: TYPE_NORMAL
- en: '**You have a property of an experimental setup and you want to simulate your
    experiment using Machine Learning:** this is actually my PhD Thesis and it’s called
    **surrogate modelling**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'You have the values of the stock market up to day 300 and you want to predict
    day 301: this is very well known and it is called **time series forecasting**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'You have a signal that is very dirty or noisy and you want to clear it: this
    is called **encoder-decoder signal denoising**, and it is also very well known.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: And in these problems, the first thing that I look at, surprisingly, is the
    **output (not the input) time series**.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s say that I take a random time series in my dataset. Is the time series
    a gentle and smooth combination of sines and cosines? Is it a polynomial function?
    Is it a logarithmic function? Is it a function I can’t even name?
  prefs: []
  type: TYPE_NORMAL
- en: And if I take another random time series, how does it change? Is the task based
    on looking at small changes from an obvious baseline or is the task to identify
    completely different behaviors all across the dataset?
  prefs: []
  type: TYPE_NORMAL
- en: 'In a very single word, we are trying to understand how **complex** our task
    is: we are estimating the **complexity of our time series**. Now the word “complex”
    can mean something different for each one of us.'
  prefs: []
  type: TYPE_NORMAL
- en: When my wife shows me her anatomy lessons I find them extremely complex, but
    for her it’s just another Tuesday :)
  prefs: []
  type: TYPE_NORMAL
- en: 'The good news is that there is a way of describing the complexity in a more
    scientific and unique way: **the concept of Entropy**'
  prefs: []
  type: TYPE_NORMAL
- en: 1\. The Entropy of a 1/0 time series (Theory)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let’s define the entropy starting from a very simple example: a time series
    that can only have values 1 and 0\. I know that it’s not exactly the type of Time
    Series that we are used to treating, but you can imagine it as if every minute
    you step in your room you flip a coin: if it’s head you have measured 1, if it’s
    tail you have measured 0 (or the opposite, I don’t have a specific preference
    of 1 being head, to be honest…)'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/cb8379347bf359f92f493a7842b4630b.png)'
  prefs: []
  type: TYPE_IMG
- en: Image made by author
  prefs: []
  type: TYPE_NORMAL
- en: Now, if you think about it, something is more “complex” when it doesn’t really
    ring a bell in our brain when you don’t understand it fully, or when it doesn't
    really give you a large amount of information.
  prefs: []
  type: TYPE_NORMAL
- en: I’ll stop teasing you, and I will give you the equation of this damn **entropy:**
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8b218137fc7edfc2df29292187068708.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Equation 1**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s break it down:'
  prefs: []
  type: TYPE_NORMAL
- en: '**X** is the domain of our time series, in our case X = {0,1}'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**p(x)** is the probability of verifying the value x that is in X'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Why do we have a logarithm in there? What does it mean? Why there is that minus
    sign?
  prefs: []
  type: TYPE_NORMAL
- en: Let’s learn by example.
  prefs: []
  type: TYPE_NORMAL
- en: Imagine that the probability of X being 0 (tail) is 0 and the probability of
    X being 1 (head) is 1\. This is not even really a time series, as it is always
    1\. What is the value of entropy?
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/06e4963f20d975faeb4e917350149c9e.png)'
  prefs: []
  type: TYPE_IMG
- en: Now, p(x=0)=0, so the first contribution is 0\. p(x=1)=1, but the logarithm
    of 1 is 0\. This means that the second contribution is 0 as well, and the entropy
    is, indeed, 0.
  prefs: []
  type: TYPE_NORMAL
- en: 'What does it mean that the entropy is 0? That the time series is not complex
    at all, and it makes sense because it looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/55d4a47ef4bf39e4b2f6bc61cf5fafe8.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: There is no “complexity” in this time series right? That’s why it’s entropy
    is 0.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s make the same example if we know that p(x=0)=p(x=1)=0.5, that means the
    same exact probability of having 1 and 0 (head or tail)
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5905ccd848b544dccb0a690e4f3bdc38.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: This is definitely more complex, isn’t it?
  prefs: []
  type: TYPE_NORMAL
- en: 'The entropy now becomes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/29d3a39609ec97f96bb540b976a064df.png)'
  prefs: []
  type: TYPE_IMG
- en: That is higher than 0\. This value has no meaning per se, but it is the **highest
    value that you can have**. That means that if you change p(x=0) to be different
    than 0.5 the entropy is lower*.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/896d2a0da2ebbce93c8e67fcdd555c10.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: '*** Note that when you change p(x=0) you also change p(x=1) as p(x=1)=1-p(x=0)**'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Now let’s think about our findings.
  prefs: []
  type: TYPE_NORMAL
- en: 'When the probability is 0, this means that there is **no complexity**, as we
    already know everything: you have one value and one value only.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When the probability is, let’s say 0.0001, this means that the **complexity
    is very little** because it can happen that x=0, but the majority of the time
    x would be equals to 1
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'When the probability is 0.5, now the complexity is **maximum** because you
    have seriously no idea of what’s going to happen next: it can be 1 or 0 with the
    same probability'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is the idea of what is “complex” for us. In a simple 1/0 way, you can retrospectively
    find the probability based on the occurrences, and retrieve the entropy.
  prefs: []
  type: TYPE_NORMAL
- en: 2\. The Entropy of a 1/0 time series (Practice)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In our code, we will use **Python**, and we will also use very basic libraries:'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s write the code to find the same solution but using the probability “retrospectively”,
    or if you want, using their **frequency definition:**
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c58716997e94f56555b9d09cf7481be5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Where:'
  prefs: []
  type: TYPE_NORMAL
- en: '**x** isa value in the domain: in our case, we only have 0 and 1, so x is either
    0 or 1'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**n(x)** is the number of times that we have **x** in our time series'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**N** is the length of our timeseries'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We are going to find p(x=0) and p(x=1) and then use equation 1 above…
  prefs: []
  type: TYPE_NORMAL
- en: 'Fine, I’ll re-paste it for you:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8b218137fc7edfc2df29292187068708.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Equation 1**'
  prefs: []
  type: TYPE_NORMAL
- en: 'In **Python**, you can do that with this very simple code:'
  prefs: []
  type: TYPE_NORMAL
- en: Does it work? Let’s test it!
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s generate a 100 long time series, with a probability=0.5 of having 0:'
  prefs: []
  type: TYPE_NORMAL
- en: Beautiful. So we have our balanced time series. The fact that we have set 0.5
    as a probability doesn’t mean **exactly** 50 and 50 as you can see, so that is
    going to give us some kind of error in estimating the probability. That’s the
    unperfect world we live in :)
  prefs: []
  type: TYPE_NORMAL
- en: 'The equation to compute the theoretical entropy is the following:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s see if the theoretical and real entropy match:'
  prefs: []
  type: TYPE_NORMAL
- en: Beautiful! They do!
  prefs: []
  type: TYPE_NORMAL
- en: 'Now let’s change p_0 and see if they keep matching:'
  prefs: []
  type: TYPE_NORMAL
- en: They match with a very little degree of error, right?
  prefs: []
  type: TYPE_NORMAL
- en: 'And the fun part is that if we do this three times **increasing** the size
    of our time series, the error will be smaller and smaller:'
  prefs: []
  type: TYPE_NORMAL
- en: After size = 10k we basically have 0 differences between real and predicted
    entropy ❤
  prefs: []
  type: TYPE_NORMAL
- en: 3\. The Entropy of a whatever time series
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now, if we still assume that our time series has discrete values (0,1,2,…) we
    can **extend** our definition of entropy into way more than only 2 values of a
    time series.
  prefs: []
  type: TYPE_NORMAL
- en: For example, let’s pick a **three value case**. So our time series can be 0,1,
    or 2.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s create a new probability vector p_0,p_1 and p_2\. To do that we will
    generate 3 random numbers between 0 and 100, and store them in a vector, and then
    divide it by the sum:'
  prefs: []
  type: TYPE_NORMAL
- en: We can apply the same equation (and the same code) as before to find the real
    and predicted entropy.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s extend the definition of the entropy in the real entropy definition:'
  prefs: []
  type: TYPE_NORMAL
- en: 'This also works for only the 0/1 case:'
  prefs: []
  type: TYPE_NORMAL
- en: 'And as we can see the theoretical and predicted entropy match even for the
    three value case:'
  prefs: []
  type: TYPE_NORMAL
- en: 'And to show you that I’m not cheating, we can see that it works for a variety
    of cases. If we change p_vector (and the time series) iteratively, we still see
    that the real and predicted entropy match:'
  prefs: []
  type: TYPE_NORMAL
- en: 4\. Results
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this blog post we:'
  prefs: []
  type: TYPE_NORMAL
- en: Reflected on analyzing the **time series** complexity before applying any machine
    learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reflected on the idea of **entropy** and **disorder** of a time series
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Defined the mathematical equation of **entropy** and explained it by example
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Applied it in practice** for both a 0/1 time series and 0,1,2, time series,
    showing how the theoretical definition matches with our computational approximation'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now, the problem (limitation) with this approach is that sometimes the time
    series can be too **continuous** for this method to work. But don’t panic! There
    is a **continuous entropy** definition that fixes the entropy for a Time Series.
  prefs: []
  type: TYPE_NORMAL
- en: I will treat in the next blogpost!
  prefs: []
  type: TYPE_NORMAL
- en: 5\. Conclusions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'If you liked the article and you want to know more about machine learning,
    or you just want to ask me something, you can:'
  prefs: []
  type: TYPE_NORMAL
- en: A. Follow me on [**Linkedin**](https://www.linkedin.com/in/pieropaialunga/),
    where I publish all my stories
  prefs: []
  type: TYPE_NORMAL
- en: B. Subscribe to my [**newsletter**](https://piero-paialunga.medium.com/subscribe).
    It will keep you updated about new stories and give you the chance to text me
    to receive all the corrections or doubts you may have.
  prefs: []
  type: TYPE_NORMAL
- en: C. Become a [**referred member**](https://piero-paialunga.medium.com/membership),
    so you won’t have any “maximum number of stories for the month” and you can read
    whatever I (and thousands of other Machine Learning and Data Science top writers)
    write about the newest technology available.
  prefs: []
  type: TYPE_NORMAL
