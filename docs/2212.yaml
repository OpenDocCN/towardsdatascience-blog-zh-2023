- en: A fAIry tale of the Inductive Bias
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/a-fairy-tale-of-the-inductive-bias-d418fc61726c?source=collection_archive---------4-----------------------#2023-07-10](https://towardsdatascience.com/a-fairy-tale-of-the-inductive-bias-d418fc61726c?source=collection_archive---------4-----------------------#2023-07-10)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '|INDUCTIVE BIAS| TRANSFORMERS| COMPUTER VISION|'
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Do we need inductive bias? How simple models can reach the performance of complex
    models
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://salvatore-raieli.medium.com/?source=post_page-----d418fc61726c--------------------------------)[![Salvatore
    Raieli](../Images/6bb4520e2df40d20283e7283141b5e06.png)](https://salvatore-raieli.medium.com/?source=post_page-----d418fc61726c--------------------------------)[](https://towardsdatascience.com/?source=post_page-----d418fc61726c--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----d418fc61726c--------------------------------)
    [Salvatore Raieli](https://salvatore-raieli.medium.com/?source=post_page-----d418fc61726c--------------------------------)'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: ·
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ff1a08d9452cd&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-fairy-tale-of-the-inductive-bias-d418fc61726c&user=Salvatore+Raieli&userId=f1a08d9452cd&source=post_page-f1a08d9452cd----d418fc61726c---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----d418fc61726c--------------------------------)
    ·18 min read·Jul 10, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fd418fc61726c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-fairy-tale-of-the-inductive-bias-d418fc61726c&user=Salvatore+Raieli&userId=f1a08d9452cd&source=-----d418fc61726c---------------------clap_footer-----------)'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: --
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fd418fc61726c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-fairy-tale-of-the-inductive-bias-d418fc61726c&source=-----d418fc61726c---------------------bookmark_footer-----------)![](../Images/278a752aca7a7bf5da388e32bb3ac8a8.png)'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: Photo by [Natalia Y.](https://unsplash.com/@foxfox?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: As we have seen in recent years deep learning has had exponential growth both
    in use and in the number of models. What paved the way for this success is perhaps
    the [transfer learning](https://en.wikipedia.org/wiki/Transfer_learning) itself-the
    idea that a model could be trained with a large amount of data and then used for
    a myriad of specific tasks.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: 'In recent years, a paradigm has emerged: [transformer](https://en.wikipedia.org/wiki/Transformer_(machine_learning_model))
    (or otherwise based on this model) is used for NLP applications. While for images,
    [vision transformers](https://en.wikipedia.org/wiki/Vision_transformer) or [convolutional
    networks](https://en.wikipedia.org/wiki/Convolutional_neural_network) are used
    instead.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，出现了一种新范式：用于自然语言处理应用的[transformer](https://en.wikipedia.org/wiki/Transformer_(machine_learning_model))（或基于此模型的其他技术）。而对于图像，则使用[vision
    transformers](https://en.wikipedia.org/wiki/Vision_transformer)或[卷积网络](https://en.wikipedia.org/wiki/Convolutional_neural_network)。
- en: '[](/the-infinite-babel-library-of-llms-90e203b2f6b0?source=post_page-----d418fc61726c--------------------------------)
    [## The Infinite Babel Library of LLMs'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/the-infinite-babel-library-of-llms-90e203b2f6b0?source=post_page-----d418fc61726c--------------------------------)
    [## LLMs的无限巴别图书馆'
- en: 'Open-source, data, and attention: How the future of LLMs will change'
  id: totrans-13
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 开源、数据和关注点：LLM（大语言模型）未来的发展将如何改变。
- en: 'towardsdatascience.com](/the-infinite-babel-library-of-llms-90e203b2f6b0?source=post_page-----d418fc61726c--------------------------------)
    [](/metas-hiera-reduce-complexity-to-increase-accuracy-30f7a147ad0b?source=post_page-----d418fc61726c--------------------------------)
    [## META’s Hiera: reduce complexity to increase accuracy'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/the-infinite-babel-library-of-llms-90e203b2f6b0?source=post_page-----d418fc61726c--------------------------------)
    [](/metas-hiera-reduce-complexity-to-increase-accuracy-30f7a147ad0b?source=post_page-----d418fc61726c--------------------------------)
    [## META的Hiera：减少复杂性以提高准确性
- en: Simplicity allows AI to reach incredible performance and surprising speed
  id: totrans-15
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 简单性使得人工智能能够达到令人难以置信的性能和惊人的速度。
- en: towardsdatascience.com](/metas-hiera-reduce-complexity-to-increase-accuracy-30f7a147ad0b?source=post_page-----d418fc61726c--------------------------------)
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/metas-hiera-reduce-complexity-to-increase-accuracy-30f7a147ad0b?source=post_page-----d418fc61726c--------------------------------)
