- en: Closed AI Models Make Bad Baselines
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 关闭的 AI 模型不适合作为基准
- en: 原文：[https://towardsdatascience.com/closed-ai-models-make-bad-baselines-4bf6e47c9e6a?source=collection_archive---------5-----------------------#2023-04-25](https://towardsdatascience.com/closed-ai-models-make-bad-baselines-4bf6e47c9e6a?source=collection_archive---------5-----------------------#2023-04-25)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/closed-ai-models-make-bad-baselines-4bf6e47c9e6a?source=collection_archive---------5-----------------------#2023-04-25](https://towardsdatascience.com/closed-ai-models-make-bad-baselines-4bf6e47c9e6a?source=collection_archive---------5-----------------------#2023-04-25)
- en: '![](../Images/e43ab302c800b0ec09553bec4f79a2d8.png)[](https://medium.com/@anna.rogers?source=post_page-----4bf6e47c9e6a--------------------------------)[![Anna
    Rogers](../Images/6a6422381ad09cabeff96abe776cc4b5.png)](https://medium.com/@anna.rogers?source=post_page-----4bf6e47c9e6a--------------------------------)[](https://towardsdatascience.com/?source=post_page-----4bf6e47c9e6a--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----4bf6e47c9e6a--------------------------------)
    [Anna Rogers](https://medium.com/@anna.rogers?source=post_page-----4bf6e47c9e6a--------------------------------)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e43ab302c800b0ec09553bec4f79a2d8.png)[](https://medium.com/@anna.rogers?source=post_page-----4bf6e47c9e6a--------------------------------)[![Anna
    Rogers](../Images/6a6422381ad09cabeff96abe776cc4b5.png)](https://medium.com/@anna.rogers?source=post_page-----4bf6e47c9e6a--------------------------------)[](https://towardsdatascience.com/?source=post_page-----4bf6e47c9e6a--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----4bf6e47c9e6a--------------------------------)
    [Anna Rogers](https://medium.com/@anna.rogers?source=post_page-----4bf6e47c9e6a--------------------------------)'
- en: ·
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F201bcd64e17&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fclosed-ai-models-make-bad-baselines-4bf6e47c9e6a&user=Anna+Rogers&userId=201bcd64e17&source=post_page-201bcd64e17----4bf6e47c9e6a---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----4bf6e47c9e6a--------------------------------)
    ·18 min read·Apr 25, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F4bf6e47c9e6a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fclosed-ai-models-make-bad-baselines-4bf6e47c9e6a&user=Anna+Rogers&userId=201bcd64e17&source=-----4bf6e47c9e6a---------------------clap_footer-----------)'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F201bcd64e17&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fclosed-ai-models-make-bad-baselines-4bf6e47c9e6a&user=Anna+Rogers&userId=201bcd64e17&source=post_page-201bcd64e17----4bf6e47c9e6a---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----4bf6e47c9e6a--------------------------------)
    · 18 分钟阅读 · 2023年4月25日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F4bf6e47c9e6a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fclosed-ai-models-make-bad-baselines-4bf6e47c9e6a&user=Anna+Rogers&userId=201bcd64e17&source=-----4bf6e47c9e6a---------------------clap_footer-----------)'
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4bf6e47c9e6a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fclosed-ai-models-make-bad-baselines-4bf6e47c9e6a&source=-----4bf6e47c9e6a---------------------bookmark_footer-----------)'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4bf6e47c9e6a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fclosed-ai-models-make-bad-baselines-4bf6e47c9e6a&source=-----4bf6e47c9e6a---------------------bookmark_footer-----------)'
- en: 'This post was authored by Anna Rogers, with much invaluable help and feedback
    from Niranjan Balasubramanian, Leon Derczynski, Jesse Dodge, Alexander Koller,
    Sasha Luccioni, Maarten Sap, Roy Schwartz, Noah A. Smith, Emma Strubell (listed
    alphabetically) *Header image credit: Sasha Luccioni*'
  id: totrans-7
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 这篇文章的作者是 Anna Rogers，在 Niranjan Balasubramanian、Leon Derczynski、Jesse Dodge、Alexander
    Koller、Sasha Luccioni、Maarten Sap、Roy Schwartz、Noah A. Smith 和 Emma Strubell（按字母顺序排列）的宝贵帮助和反馈下完成。*封面图片来源：Sasha
    Luccioni*
- en: 'What comes below is an attempt to bring together some discussions on the state
    of NLP research post-ChatGPT.¹ We are NLP researchers, and at the absolute minimum
    our job is to preserve the fundamentals of scientific methodology. This post is
    primarily addressed to junior NLP researchers, but is also relevant for other
    members of the community who are wondering how the existence of such models should
    change their next paper. We make the case that as far as research and scientific
    publications are concerned, the “closed” models (as defined below) cannot be meaningfully
    studied, and they should not become a “universal baseline”, the way BERT was for
    some time widely considered to be. The TLDR for this post is a simple proposed
    rule for reviewers and chairs (akin to the [Bender rule](https://thegradient.pub/the-benderrule-on-naming-the-languages-we-study-and-why-it-matters/)
    that requires naming the studied languages):'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 以下内容是尝试汇总ChatGPT之后NLP研究现状的一些讨论。¹ 我们是NLP研究人员，至少我们的工作是维护科学方法的基本原则。这篇文章主要针对初级NLP研究人员，但也对社区的其他成员相关，他们在考虑这种模型的存在应该如何改变他们的下一篇论文。我们认为，就研究和科学出版物而言，“封闭”模型（如下面定义的）不能被有意义地研究，它们不应成为“通用基准”，像BERT曾一度被广泛认为的那样。这篇文章的TLDR是对审稿人和主席的一个简单建议规则（类似于[贝德规则](https://thegradient.pub/the-benderrule-on-naming-the-languages-we-study-and-why-it-matters/)），要求命名所研究的语言）：
- en: '***That which is not open and reasonably reproducible cannot be considered
    a requisite baseline.***'
  id: totrans-9
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '***不可开放且合理可重复的内容不能被视为必要的基准。***'
- en: '*By “open” we mean here that the model is available for download, can be run
    offline (even if it takes non-trivial compute resources), and can be shared with
    other users even if the original provider no longer offers the model for download.
    “Open” models support versioning, and document for each model version what training
    data they used. A model is “closed” if it is not open.*'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '*这里所说的“开放”意味着模型可以下载，可以离线运行（即使需要非平凡的计算资源），并且即使原始提供者不再提供下载，仍可以与其他用户共享。“开放”的模型支持版本控制，并为每个模型版本记录所使用的训练数据。如果模型不是开放的，它就是“封闭”的。*'
- en: '*By “reasonably reproducible” we mean that the creators released enough information
    publicly such that the model can be reproduced with the provided code, data, and
    specified compute resources, with some variation reasonably expected due to hardware/software
    variance, data attrition factors and non-determinism in neural networks. For instance,
    reproducing* [*BLOOM*](https://huggingface.co/bigscience/bloom) *would require
    a super-computer — but at least theoretically it is possible, given the measures
    to open-source the code, collect and document the data. So it is “reasonably reproducible”
    by our definition, even though not everybody could do it.*'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '*这里所说的“合理可重复”意味着创建者公开了足够的信息，使得模型可以通过提供的代码、数据和指定的计算资源来重复，尽管由于硬件/软件差异、数据流失因素和神经网络的非确定性，存在一定的变化。例如，重复*
    [*BLOOM*](https://huggingface.co/bigscience/bloom) *需要超级计算机——但至少从理论上讲，这是可能的，考虑到开源代码、收集和记录数据的措施。因此，根据我们的定义，它是“合理可重复的”，尽管不是每个人都能做到。*'
- en: Relevance != popularity
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 相关性 != 流行度
- en: 'Here’s a question many graduate students in NLP have been asking themselves
    recently:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 许多NLP研究生最近都在问一个问题：
- en: 'This anxiety seems to be due partly to the fact that in our field, **“relevance”
    has been extremely popularity-driven**. For the last decade, there has always
    been a Thing-Everybody-Is-Talking-About: a model or approach that would become
    a yardstick, a baseline that everybody would be wise to have in their papers to
    show that what they’ve done is a meaningful improvement. This can be understood,
    since one of the driving [values](https://dl.acm.org/doi/fullHtml/10.1145/3531146.3533083)
    of the ML community is improving upon past work — otherwise, how would we know
    we are making progress, right? Post-2013 we had word2vec/GloVe, then there was
    a similar craze about BERT. Then GPT-3\. And now — ChatGPT and GPT-4.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 这种焦虑似乎部分源于我们的领域中，**“相关性”一直受到极端流行趋势的驱动**。在过去的十年里，总会有一个大家都在谈论的事物：一个模型或方法，成为一个标尺，一个基准，大家都明智地在他们的论文中展示，以证明他们所做的工作是有意义的改进。这是可以理解的，因为机器学习社区的一个驱动[价值观](https://dl.acm.org/doi/fullHtml/10.1145/3531146.3533083)是改进过去的工作——否则，我们怎么知道自己在取得进展，对吧？2013年之后，我们有了word2vec/GloVe，接着是BERT的类似热潮。然后是GPT-3。现在——ChatGPT和GPT-4。
- en: 'Why does this happen? There are two lines of reasoning behind this:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 这为什么会发生？背后有两种推理：
- en: The-Thing-Everybody-Is-Talking-About is either likely to be truly state-of-the-art
    for whatever I’m doing, or a reasonable baseline, so I better have it in my paper
    and beat it with my model.
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**大家都在谈论的事物**要么是我所做的事情中真正最先进的，要么是一个合理的基准，所以我最好在我的论文中提到它，并用我的模型超越它。'
- en: As an author, my chances of publication depend in part on the reviewers liking
    my work, and hence the safest bet for me is to talk about something that most
    people are likely to be interested in — a.k.a The-Thing-Everybody-Is-Talking-About.
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 作为作者，我的发表机会部分依赖于审稿人对我工作的喜爱，因此对我来说最安全的做法是谈论大多数人可能感兴趣的事物——也就是**大家都在谈论的事物**。
- en: '(b) is actually a self-fulfilling prophecy: the more authors think this way,
    the more papers they write using The-Thing-Everybody-Is-Talking-About, which in
    turn reinforces the reviewers in the belief that that thing is really prerequisite.
    We see this cycle manifested as a mismatch between the beliefs of individual community
    members and their perception of others’ views on what research directions should
    be prioritized (e.g. focus on benchmarks or scale), as documented in the [NLP
    Community Metasurvey](https://nlpsurvey.net/). Though it takes effort, members
    of the research community can push back against that kind of cycle (and we will
    discuss specific [strategies](http://localhost:63342/markdownPreview/775791527/markdown-preview-index-1387425898.html?_ijt=oeg9a3abpl7mtfkhbp8oinaoo1#we-do-have-options)
    for that below). As for (a) — it made sense while The-Thing-Everybody-Is-Talking-About
    was actually something that one could meaningfully compare to.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 实际上是自我实现的预言：作者们越是这样想，他们就会越多地使用**大家都在谈论的事物**，这反过来又强化了审稿人对这一事物确实是必需品的信念。我们看到这种循环表现为个体社区成员的信念与他们对其他人对应优先研究方向（例如，专注于基准测试或规模）的看法之间的差异，正如[NLP社区元调查](https://nlpsurvey.net/)中所记录的那样。尽管这需要付出努力，研究社区的成员可以抵制这种循环（我们将在下面讨论具体的[策略](http://localhost:63342/markdownPreview/775791527/markdown-preview-index-1387425898.html?_ijt=oeg9a3abpl7mtfkhbp8oinaoo1#we-do-have-options)）。至于
    (a) — 当**大家都在谈论的事物**实际上是可以有意义比较的东西时，这是有道理的。
- en: The main point we would like to make is that this kind of reasoning simply no
    longer applies to closed models that do not disclose enough information about
    their architecture, training setup, data, and operations happening at inference
    time. It just doesn’t matter how many people say that they work well. Even without
    going into the dubious ethics of commercial LLMs, with copyright infringement
    lawsuits over [code](https://www.techradar.com/news/microsoft-is-being-sued-over-github-copilot-piracy)
    and [art](https://arstechnica.com/information-technology/2023/01/artists-file-class-action-lawsuit-against-ai-image-generator-companies/)
    already underway, and [unethically sourced labeled data](https://time.com/6247678/openai-chatgpt-kenya-workers/)
    — the basic research methodology demands it. Many, many people are bringing up
    the fact that as researchers, we are now in an impossible position.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 我们想要表达的主要观点是，这种推理方式对于那些未披露足够架构、训练设置、数据和推理时操作信息的封闭模型已经不再适用。即使有多少人说它们效果很好也无关紧要。即使不涉及商业LLM的可疑伦理问题，已经有涉及[代码](https://www.techradar.com/news/microsoft-is-being-sued-over-github-copilot-piracy)和[艺术](https://arstechnica.com/information-technology/2023/01/artists-file-class-action-lawsuit-against-ai-image-generator-companies/)的版权侵权诉讼，以及[不道德来源的标记数据](https://time.com/6247678/openai-chatgpt-kenya-workers/)——基本的研究方法论都要求如此。许多人提出，作为研究人员，我们现在面临着一个不可能的境地。
- en: 'We have very little idea what these models are trained on, or how:'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们对这些模型的训练内容或方式知之甚少：
- en: 'The said black box is constantly changing:'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所谓的黑箱正在不断变化：
- en: 'Both our incoming prompts and outgoing answers may be undergoing unspecified
    edits via unspecified mechanisms. E.g. ChatGPT “self-censors” with content filters
    which people have so much fun bypassing, and has proprietary prompt prefixes:'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们的输入提示和输出答案可能会通过不明确的机制进行不明确的编辑。例如，ChatGPT 通过内容过滤器进行“自我审查”，人们乐于绕过这些过滤器，并且拥有专有的提示前缀：
- en: Yes, these models do seem impressive to many people in practice — but as researchers,
    our job is not to buy into hype. The companies training these models have the
    right to choose to be wholly commercial and therefore not open to independent
    scrutiny — that is expected of for-profit entities whose main purpose is to generate
    profits for their stakeholders. But this necessarily means that they relinquish
    the role of scientific researchers. As Gary Marcus [put](https://garymarcus.substack.com/p/the-sparks-of-agi-or-the-end-of-science?publication_id=888615)
    it,
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 是的，这些模型在实践中确实对许多人来说很令人印象深刻——但作为研究人员，我们的工作不是盲目跟随炒作。训练这些模型的公司有权选择完全商业化，因此不接受独立审查——这是以利润为主要目的的实体的预期。然而，这必然意味着它们放弃了科学研究者的角色。正如
    Gary Marcus [所说](https://garymarcus.substack.com/p/the-sparks-of-agi-or-the-end-of-science?publication_id=888615)，
- en: '*I don’t expect Coca Cola to present its secret formula. But nor do I plan
    to give them scientific credibility for alleged advances that we know nothing
    about.*'
  id: totrans-24
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*我不期望可口可乐公布其秘密配方。但我也不打算为他们声称的进展提供科学可信度，这些进展我们一无所知。*'
- en: Why closed models as requisite baselines would break NLP research narratives
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为什么封闭模型作为必要基准会破坏 NLP 研究叙事
- en: To make things more concrete, let us consider a few frequent “research narratives”
    in NLP papers, and how they would be affected by using such “closed” models as
    baselines. We will use GPT-4 as a running example of a “closed” model that was
    released with [almost no technical details](https://virtualizationreview.com/articles/2023/03/15/gpt-4-details.aspx),
    despite being the subject of a 100-page report singing its praises, but the same
    points apply to other such models.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使事情更具体，让我们考虑一些 NLP 论文中常见的“研究叙事”，以及使用这种“封闭”模型作为基准时它们将如何受到影响。我们将以 GPT-4 作为一个“封闭”模型的实例，尽管它发布时几乎没有
    [技术细节](https://virtualizationreview.com/articles/2023/03/15/gpt-4-details.aspx)，但它的确得到了
    100 页的报告称赞，但这些观点同样适用于其他类似模型。
- en: '“We propose a machine learning model that improves on the state-of-the-art”:'
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: “我们提出了一种在技术上超越现有水平的机器学习模型”：
- en: To make the claim that our algorithm improves over whatever it is that a commercial
    model is doing, we need to at least know that we are doing something qualitatively
    different. If we are proposing some modification of a currently-popular approach
    (e.g., Transformers), without documentation, we simply cannot exclude that the
    “closed” model might be doing something similar.
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 要声称我们的算法比商业模型所做的任何事情更优秀，我们至少需要知道我们在做一些质的不同的事情。如果我们提议对当前流行的方法（例如，Transformers）进行某些修改，而没有文档，我们根本不能排除“封闭”模型可能在做类似的事情。
- en: Even if we believe that we are doing something qualitatively different, we still
    need to be able to claim that any improvements are due to our proposed modification
    and not model size, the type and amount of data, important hyperparameters, “lucky”
    random seed, etc. Since we don’t have any of this information for the closed “baseline”,
    we cannot meaningfully compare our model to it.
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 即使我们相信我们正在做一些质的不同的事情，我们仍然需要能够声称任何改进都是由于我们提出的修改，而不是模型大小、数据的类型和数量、重要的超参数、“幸运”的随机种子等。由于我们没有这些信息，无法对封闭“基准”进行有意义的比较。
- en: And even if we ignore all the above factors — to make a fair comparison with
    these models on some performance metric, we have to at least know that neither
    of our models has observed the test data. Which, for the “closed” model, we also
    don’t know. Even OpenAI itself was initially concerned about [test data contamination
    with GPT-3](https://proceedings.neurips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html),
    which could not possibly have improved — especially after the whole world has
    obligingly tested ChatGPT for months. And it [hasn’t improved](/the-decontaminated-evaluation-of-gpt-4-38a27fc45c30).
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 即使我们忽略所有上述因素——为了在某些性能指标上与这些模型进行公平比较，我们至少要知道我们的模型没有观察到测试数据。对于“封闭”模型，我们同样不知道。即使是
    OpenAI 自身最初也对 [GPT-3 的测试数据污染](https://proceedings.neurips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html)
    感到担忧，尤其是在全世界都乐意测试 ChatGPT 几个月之后，这种情况是不可能改善的。而且它 [没有改善](/the-decontaminated-evaluation-of-gpt-4-38a27fc45c30)。
- en: The only thing that we as model developers can learn from the existence of GPT-4,
    is that this is the kind of performance that can be obtained with some unspecified
    combination of current methods and data. An upper bound or existence proof, which
    seems higher than existing alternatives. **Upper bounds are important, and could
    serve as a source of motivation for our work, but they cannot be used as a point
    of comparison.**
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 我们作为模型开发者从GPT-4的存在中能学到的唯一一点是，这是一种通过某种未指定的当前方法和数据组合可以获得的性能。这是一个上限或存在证明，看起来高于现有的替代方案。**上限很重要，可以作为我们工作的动力来源，但不能用作比较的依据。**
- en: '“We propose a new challenging task/benchmark/metric”:'
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: “我们提出了一个新的挑战性任务/基准/指标”：
- en: Constructing good evaluation data is very hard and expensive work, and it makes
    sense to invest in it when we believe that it can be used as a public benchmark
    to measure progress in NLP models, at least for a few months. Examples of such
    benchmarks that have driven NLP research in the past include [SQuAD](https://rajpurkar.github.io/SQuAD-explorer/),
    [GLUE](https://gluebenchmark.com/) and [BigBench](https://github.com/google/BIG-bench).
    But public benchmarks can only work if the test data remains hidden (and even
    then [eventually people evaluate too many times](https://laurenoakdenrayner.com/2019/09/19/ai-competitions-dont-produce-useful-models/)
    and start to implicitly overfit). This is obviously incompatible with the scenario
    where the developer of the popular “closed” models, only accessible via API, keeps
    our submitted data and may use it for training. And unless the models explicitly
    describe *and share* their training data, we have no way of auditing this.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 构建良好的评估数据是一项非常艰难且昂贵的工作，当我们相信这些数据可以作为公开基准来衡量NLP模型的进展时，投资其中是有意义的，至少可以维持几个月。过去推动NLP研究的基准示例包括[SQuAD](https://rajpurkar.github.io/SQuAD-explorer/)、[GLUE](https://gluebenchmark.com/)和[BigBench](https://github.com/google/BIG-bench)。但是，公开基准只有在测试数据保持隐藏的情况下才能有效（即使如此，[最终人们还是会评估太多次](https://laurenoakdenrayner.com/2019/09/19/ai-competitions-dont-produce-useful-models/)并开始隐性过拟合）。这显然与流行的“封闭”模型开发者的场景不兼容，该开发者通过API访问模型，保留我们的提交数据并可能用于训练。除非模型明确描述*并分享*其训练数据，否则我们无法进行审计。
- en: This means that our efforts will be basically single-use as far as the models
    by that developer are concerned. The next iteration will likely “ace” it (but
    not for the right reasons).
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着就开发者的模型而言，我们的努力基本上是一次性的。下一次迭代可能会“表现出色”（但原因不一定正确）。
- en: 'Let us consider OpenAI policies in this respect:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑一下OpenAI在这方面的政策：
- en: ChatGPT [by default keeps your data and may use it for training](https://help.openai.com/en/articles/7039943-data-usage-for-consumer-services-faq).
    It is said to provide an [opt-out](https://help.openai.com/en/articles/7039943-data-usage-for-consumer-services-faq)
    of data collection.
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ChatGPT [默认会保留你的数据并可能用于训练](https://help.openai.com/en/articles/7039943-data-usage-for-consumer-services-faq)。据说提供了[退出数据收集的选项](https://help.openai.com/en/articles/7039943-data-usage-for-consumer-services-faq)。
- en: 'The [OpenAI API policy](https://openai.com/policies/api-data-usage-policies)
    was updated on March 1 2023, and currently states that by default data is not
    retained and not used for training. Whatever was submitted before this date, can
    be used, so we can safely assume that much if not all of existing public benchmark
    data has been submitted to GPT-3 since 2020, including the labels or “gold” answers
    — at least those that were used as few-shot prompts. Interestingly, OpenAI then
    uses the contamination as a reason to exclude some evaluations but not others:
    the GPT4 tech report says that they did not evaluate on BIG-bench because of data
    contamination (in [v.3](https://arxiv.org/abs/2303.08774v3) of the report it’s
    footnote 5 on p.6), although they do present their results for 100% contaminated
    GRE writing exams (Table 9).'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[OpenAI API政策](https://openai.com/policies/api-data-usage-policies) 于2023年3月1日更新，目前规定默认情况下数据不会被保留或用于训练。任何在此日期之前提交的数据都可以使用，因此我们可以安全地假设，自2020年以来，现有的大部分甚至全部公共基准数据已经提交给GPT-3，包括标签或“黄金”答案——至少那些被用作少量示例提示的内容。有趣的是，OpenAI随后使用数据污染作为排除某些评估的理由，但不排除其他评估：GPT-4技术报告称由于数据污染未对BIG-bench进行评估（在[版本3](https://arxiv.org/abs/2303.08774v3)的报告中是第6页的脚注5），尽管他们确实展示了100%污染的GRE写作考试的结果（表9）。'
- en: 'The overall problem is that opt-outs and even opt-ins are not sufficient in
    the case of something meant to be a public benchmark: as dataset creators, the
    future of our work might be affected not only by our own use of our data — but
    also by anybody else using it! It takes just one other researcher who wasn’t careful
    to opt-out, or wasn’t able to — and our data is “poisoned” with respect to future
    models by that developer. Even if only some few-shot examples are submitted, they
    might be used to somehow auto-augment similar user prompts. Last but not least,
    if we make our data public, the model developers themselves could also proactively
    add it to the training data, looking to improve their model. If the labels or
    the “gold” answers are not public for an important benchmark, it would be worthwhile
    for them to create some similar data.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 总体问题在于，选择退出甚至选择加入在作为公共基准的数据集的情况下是不足够的：作为数据集创建者，我们工作的未来可能不仅会受到我们自己使用数据的影响，还会受到其他人使用数据的影响！只需要一个没有小心选择退出的研究者，或无法选择退出的研究者，我们的数据就会被那个开发者“污染”对未来模型的影响。即使只提交了一些少量样本，它们也可能被用来以某种方式自动增强类似的用户提示。最后但同样重要的是，如果我们将数据公开，模型开发者自己也可能主动将其加入训练数据，以期改善他们的模型。如果标签或“黄金”答案对于一个重要基准数据集不公开，他们可能会创建一些类似的数据。
- en: It’s unclear yet how to solve this problem. Perhaps there will soon appear some
    special version of robots.txt that both prohibits use for AI training, *and* requires
    that any resharing of this data keeps the same flag. And, hopefully, the large
    companies will eventually be required to comply, and be subject to audits. **In
    the short-term, it seems like the only option is to simply not trust or produce
    benchmark results for models where test-train overlap analysis cannot be performed.**
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 目前还不清楚如何解决这个问题。也许很快会出现某种特殊版本的robots.txt，它不仅禁止用于AI训练，*还*要求任何重新分享这些数据的行为保留相同的标记。并且，希望大型公司最终会被要求遵守，并接受审计。**在短期内，唯一的选择似乎是简单地不信任或不产生无法进行测试-训练重叠分析的模型的基准结果。**
- en: '“We show that model X does/doesn’t do Y: (model analysis and interpretability)'
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: “我们展示了模型X是否做Y：（模型分析和可解释性）”
- en: Since we only have access to GPT-4 via the API, we can only probe model outputs.
    If the plan is to use existing probing datasets or construct new ones, we have
    the same resource problem described above (the previously used probing datasets
    might have been trained on, the previously used techniques could have been optimized
    for, the new work will be single-use, and still have the train-test overlap problem
    to an unknown extent).
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们只能通过API访问GPT-4，我们只能探查模型输出。如果计划使用现有的探查数据集或构建新的数据集，我们将面临上述相同的资源问题（之前使用的探查数据集可能已经被训练，之前使用的技术可能已经被优化，新工作将是一次性的，并且仍然存在未确定程度的训练测试重叠问题）。
- en: Furthermore, at least some of these models seem to intentionally not produce
    identical outputs when queried with the same probe and settings (perhaps via random
    seeds or different versions of the model being used in parallel). In this case,
    whatever results we get may already be different for someone else, which puts
    our basic conclusions at risk. This could include, for instance, the reviewers
    of the paper, who will rightfully conclude that what our report may not be true.
    Moreover, if the developer keeps tweaking the model as we go, then by the time
    we finish writing the paper, the model could change (perhaps even based on our
    own data). Which would also make our work not only obsolete before it is even
    reviewed, but also incorrect.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，至少有些模型似乎故意在使用相同的探针和设置时不产生相同的输出（可能通过随机种子或不同版本的模型并行使用）。在这种情况下，我们获得的结果可能对其他人已经不同，这使我们的基本结论面临风险。这可能包括论文的审稿人，他们可能会合理地得出我们的报告可能不真实的结论。而且，如果开发者在我们写论文的过程中不断调整模型，那么当我们完成论文时，模型可能会发生变化（甚至基于我们自己的数据）。这不仅会使我们的工作在审查前就已经过时，还可能不正确。
- en: This issue might be addressed by “freezing” given versions of the model and
    committing to keep them available to researchers, but there is hardly any incentive[²]
    for for-profit companies to do so. For instance, some popular models including
    Codex/code-davinci-002 have already been [deprecated](https://platform.openai.com/docs/models/gpt-3).
    We also have no public information about what changes lead or do not lead to a
    new version number (and it is likely that at least the filters are updated continually,
    as users are trying to break the model).
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 这个问题可以通过“冻结”模型的给定版本并承诺保持其对研究人员的可用性来解决，但对于盈利公司来说几乎没有任何激励[²]去这样做。例如，一些流行的模型包括
    Codex/code-davinci-002 已经被[弃用](https://platform.openai.com/docs/models/gpt-3)。我们也没有公开的信息说明哪些变化会导致或不会导致新的版本号（而且很可能至少过滤器会不断更新，因为用户在尝试突破模型）。
- en: 'Last but not least, consider the effect of showing that model X does/doesn’t
    do Y:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 最后但同样重要的是，考虑展示模型 X 是否做/不做 Y 的影响：
- en: '*“Model does Y”*: without test-train overlap guarantees this is not necessarily
    a statement about the model. For example, ChatGPT was reported to be able to play
    chess ([badly](https://medium.com/@ivanreznikov/how-good-is-chatgpt-at-playing-chess-spoiler-youll-be-impressed-35b2d3ac024a)).
    That looks unexpected of something that you consider a language model, but if
    you knew that it has seen a lot of chess data — it is hardly newsworthy that a
    language model can predict a plausible-looking sequence of moves. Basically, instead
    of discovering properties of a language model (which could be a research finding),
    we’re discovering that the internet dump it was trained on contained some chess
    data.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*“模型做 Y”*：没有测试和训练重叠的保证，这不一定是对模型的声明。例如，ChatGPT 被报告能够下棋（[表现不好](https://medium.com/@ivanreznikov/how-good-is-chatgpt-at-playing-chess-spoiler-youll-be-impressed-35b2d3ac024a)）。这对于你认为是语言模型的东西来看似乎有些意外，但如果你知道它看过大量的棋局数据——那么语言模型能预测出看起来合理的棋步序列几乎不值得惊讶。基本上，我们发现的不是语言模型的属性（这可能是一个研究发现），而是我们发现它所训练的互联网数据中包含了一些棋局数据。'
- en: '*“Model doesn’t do Y*”: by collecting cases where the model seems to fail,
    we implicitly help the commercial entity controlling that model to “fix” those
    specific cases, and further blur the line between “emergent” language model properties
    and test cases leaked in training. In fact, GPT-4 was already trained on user
    interactions gathered during the mass testing of ChatGPT, which provided Open
    AI with millions of free examples, including “corrected” responses to prompts
    submitted by users. In the long run, our work would make it harder for the next
    researcher to examine the next “closed” model. What’s even worse, it would decrease
    the number of easy-to-spot errors that might prevent ordinary users from falling
    for the [Eliza effect](https://en.wikipedia.org/wiki/ELIZA_effect), hence increasing
    their trust in these systems (even though they are still fundamentally unreliable).'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*“模型不做 Y”*：通过收集模型似乎失败的案例，我们在隐性地帮助控制该模型的商业实体“修复”这些特定案例，同时进一步模糊了“突现”语言模型属性和训练中泄露的测试案例之间的界限。事实上，GPT-4
    已经在 ChatGPT 的大规模测试中收集了用户交互数据，这为 Open AI 提供了数百万个免费的示例，包括用户提交的提示的“修正”回应。从长远来看，我们的工作会使下一位研究人员更难以检查下一个“封闭”模型。更糟糕的是，这将减少那些可能防止普通用户受到[Eliza
    效应](https://en.wikipedia.org/wiki/ELIZA_effect)影响的明显错误，从而增加他们对这些系统的信任（尽管它们仍然从根本上不可靠）。'
- en: '**In summary, by showing that a closed model X does/doesn’t do Y we would likely
    not contribute to the general understanding of such models, and/or exacerbate
    the evaluation issues.**'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '**总之，通过展示一个封闭模型 X 是否做/不做 Y，我们可能不会对这类模型的总体理解有所贡献，和/或加剧评估问题。**'
- en: '“We show that model X is (un)fair/biased etc”: (AI ethics)'
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: “我们展示模型 X 是（不）公平/有偏见等”：（AI 伦理）
- en: 'Let us say that we somehow showed that the closed model yields some specific
    type of misinformation or misrepresents a given identity group (as it was done
    e.g. for [anti-Muslim bias in GPT-3](https://dl.acm.org/doi/10.1145/3461702.3462624)).
    The most likely outcome for such work is that this specific kind of output will
    be quickly “patched”, perhaps before we even publish the paper. The result is
    that (a) our hard work is short-lived, which may matter for researcher careers,
    (b) we actively helped the company make their model *seem* more ethical, while
    their training data probably didn’t fundamentally change, and hence the model
    probably still encodes the harmful stereotypes that could manifest themselves
    in other ways. Consider how in [Dall-E 2](https://arxiv.org/abs/2211.06323) the
    gender and identity terms were randomly added to make outputs seem more diverse,
    as opposed to showing the default identity groups (read: White Men).'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 比如说，我们以某种方式展示了封闭模型产生了某种特定类型的虚假信息或对某个身份群体的误代表（例如，关于[GPT-3中的反穆斯林偏见](https://dl.acm.org/doi/10.1145/3461702.3462624)）。这种工作的最可能结果是，这种特定类型的输出会被迅速“修补”，可能在我们甚至发布论文之前。结果是（a）我们的辛勤工作短暂，这可能对研究者的职业生涯有影响，（b）我们积极帮助公司使他们的模型*看起来*更具伦理性，而他们的训练数据可能并未根本改变，因此模型可能仍然编码着可能以其他方式显现的有害刻板印象。考虑到在[Dall-E
    2](https://arxiv.org/abs/2211.06323)中，性别和身份术语被随机添加以使输出看起来更具多样性，而不是显示默认的身份群体（即：白人男性）。
- en: 'So, should we just forgo studying “closed” models from the ethics angle? Of
    course not: **independent analysis on commercial systems is strictly necessary.
    But we need to figure out ways to do this without providing companies with free
    data with which to mask the symptoms of the underlying problem.** Here are some
    alternatives that may lean on skillsets that NLP researchers are still developing,
    and perhaps will be strengthened by collaborations with experts in HCI and social
    sciences:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，我们是否应该从伦理角度放弃研究“封闭”模型？当然不是：**对商业系统的独立分析是严格必要的。但我们需要找出在不向公司提供免费数据的情况下进行分析的方法，以免掩盖潜在问题的症状。**
    这里有一些可能依赖于NLP研究人员仍在发展的技能集的替代方案，并可能通过与HCI和社会科学专家的合作得到强化：
- en: User studies on whether people trust the over-simplified chatbot answers, how
    likely they are to verify information, whether students use it in ways that actually
    improves their learning outcomes, and interventions that promote safer use practices.
    This kind of work focuses on the potential effects of these models, given the
    known phenomenon of automation bias, and any negative findings can only be refuted
    with a public user study.
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用户研究是否人们信任过于简化的聊天机器人回答，他们验证信息的可能性，学生是否以实际改善学习成果的方式使用它，以及促进更安全使用实践的干预措施。这类工作关注这些模型的潜在影响，鉴于已知的自动化偏见现象，任何负面发现只有通过公开用户研究才能被反驳。
- en: Discussing and documenting instances of real-world harms, where they can be
    traced to the model (akin to the [Stochastic Parrots](https://dl.acm.org/doi/pdf/10.1145/3442188.3445922)
    paper). Ideally, such cases would require not only a fix, but also public acknowledgment
    and hopefully compensation.
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 讨论和记录实际世界中伤害的实例，这些伤害可以追溯到模型（类似于[随机鹦鹉](https://dl.acm.org/doi/pdf/10.1145/3442188.3445922)论文）。理想情况下，这些案例不仅需要修复，还需要公开承认，并希望得到赔偿。
- en: 'User studies of various demographic cohorts, to see if the system works equally
    well for them in different real-world tasks: something with qualitative evaluation,
    where a fix would require obtaining better training data for that cohort. But
    this kind of work would need to somehow avoid producing too much concrete evidence
    that could be used to simply “patch” the output.'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对各种人口统计群体进行用户研究，查看系统在不同实际任务中是否对他们同样有效：这需要定性评估，修复可能需要为该群体获取更好的训练数据。但这类工作需要在某种程度上避免产生过多具体证据，这些证据可能被用来简单地“修补”输出。
- en: Studies not just of these systems, but on their intended and real impact on
    society. We need a lot of research on system-level issues where a “fix” would
    require changes to the business model and/or the way these systems are presented
    and marketed. An obvious example is the jobs that are too risky to be automated
    with the unreliable, biased, hallucination-prone systems that we currently have.
    For instance, do policy-makers jump on the opportunity to hire fewer teachers,
    and what kinds of schools are more likely to be sent down that path?
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不仅仅是这些系统的研究，还包括它们对社会的预期和实际影响。我们需要大量的关于系统级问题的研究，其中“修复”可能需要对商业模式和/或这些系统的展示和营销方式进行改变。一个明显的例子是那些过于危险而不能用我们当前不可靠、带有偏见、容易产生幻觉的系统来自动化的工作。例如，政策制定者是否会利用机会减少教师数量？哪些类型的学校更可能走上这条路？
- en: '“We develop a more efficient solution than model X”:'
  id: totrans-55
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: “我们开发了一种比模型X更高效的解决方案”：
- en: The reviewers would likely (and rightly) expect us to show that we improve efficiency
    while maintaining a similar level of performance, which means we inherit all the
    above evaluation issues. Also, we likely don’t even have enough details about
    the training of the “baseline”, including its computational costs, the amount
    and source of energy invested in it, etc.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 评审者可能（并且正确地）期望我们在保持类似性能水平的同时提高效率，这意味着我们继承了上述所有评估问题。此外，我们可能连“基线”训练的详细信息，包括其计算成本、投入的能源量和来源等，都不够充分。
- en: We Do Have Options!
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 我们确实有选择！
- en: 'Dear members of the NLP community: the good news is that if you’d like to do…
    you know… actual *research* on language models, you do have open options, and
    more of them will probably be coming, as the cost of training goes down. **Here
    are a few examples of models that come not only with reasonable descriptions of
    their training data, but even tools to query it**:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 亲爱的NLP社区成员：好消息是，如果你想进行……你知道的……实际的*研究*语言模型，你确实有开放的选择，而且随着训练成本的降低，可能会有更多的选择。**以下是一些不仅提供合理训练数据描述，还有查询工具的模型示例**。
- en: '[BLOOM](https://huggingface.co/bigscience/bloom) (multilingual LLM), sizes
    560M-176B: [documentation efforts](https://huggingface.co/spaces/bigscience-data/bigscience-corpus),
    [ROOTS corpus description](https://openreview.net/forum?id=UoEw6KigkUn) , [Training
    data search tool](https://huggingface.co/spaces/bigscience-data/roots-search)'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[BLOOM](https://huggingface.co/bigscience/bloom)（多语言LLM），大小560M-176B: [文档工作](https://huggingface.co/spaces/bigscience-data/bigscience-corpus)，[ROOTS语料库描述](https://openreview.net/forum?id=UoEw6KigkUn)，[训练数据搜索工具](https://huggingface.co/spaces/bigscience-data/roots-search)'
- en: '[GPT-Neo models](https://github.com/EleutherAI/gpt-neo) (mostly-English LLMs),
    sizes 125M-2.7B: [Pile corpus datasheet](https://arxiv.org/abs/2201.07311), [The
    Pile](https://arxiv.org/abs/2101.00027)[The Pile Data Portraits](https://pile.dataportraits.org/)'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[GPT-Neo模型](https://github.com/EleutherAI/gpt-neo)（主要是英文LLM），大小125M-2.7B: [Pile语料库数据表](https://arxiv.org/abs/2201.07311)，[The
    Pile](https://arxiv.org/abs/2101.00027)[The Pile数据肖像](https://pile.dataportraits.org/)'
- en: '[T5](https://huggingface.co/docs/transformers/model_doc/t5) (English LLM),
    sizes 60M-11B: [partial C4 documentation](https://arxiv.org/pdf/2104.08758.pdf),
    [C4 search tool](https://c4-search.apps.allenai.org/)'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[T5](https://huggingface.co/docs/transformers/model_doc/t5)（英文LLM），大小60M-11B:
    [部分C4文档](https://arxiv.org/pdf/2104.08758.pdf)，[C4搜索工具](https://c4-search.apps.allenai.org/)'
- en: '**What about the reviewers who might say “but where’s GPT-4?”** Here’s what
    you can do:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '**那些可能会说“那GPT-4在哪里？”的评审者怎么办？** 你可以这样做：'
- en: Preemptively discuss in your paper why you don’t provide e.g. ChatGPT results
    as a baseline, before your paper is submitted. If necessary, use the arguments
    in this post in your rebuttals to reviewers.
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在你的论文提交之前，预先讨论为什么不提供例如ChatGPT结果作为基线。如果需要，在回复评审者时使用这篇文章中的论点。
- en: Preemptively raise the issue with the chairs of the conference you plan to submit
    to, to ask if they have a policy against such superficial popularity-driven reviews.
    The ACL 2023 [policy](https://2023.aclweb.org/blog/review-acl23/#2-check-for-lazy-thinking)
    didn’t cover this, since the problem became apparent after the submission deadline,
    but it can be extended by future chairs. We will be following any policy discussions
    related to this in ACL conferences; if you have any comments, or if there are
    any major developments and if you’d like us to keep you in the loop — please use
    this [form](https://forms.gle/N2Gakzk8xF6V9aTr5).
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提前向你计划提交的会议主席提出这一问题，询问他们是否有反对这种表面化、受流行趋势驱动的审稿政策。ACL 2023的[政策](https://2023.aclweb.org/blog/review-acl23/#2-check-for-lazy-thinking)并没有涵盖这一点，因为问题在提交截止日期后才显现出来，但未来的主席可以扩展此政策。我们会关注与此相关的ACL会议政策讨论；如果你有任何意见，或有重大进展且希望我们将你纳入讨论——请使用这个[表格](https://forms.gle/N2Gakzk8xF6V9aTr5)。
- en: As a reviewer or chair, if you see someone insisting on closed baselines — side
    with the authors and push back.
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 作为审稿人或主席，如果你看到有人坚持使用封闭的基线——站在作者的一边并提出反对意见。
- en: Discuss these matters openly in your own community; as reviewers, we can continue
    to educate and influence each other to drive our norms to a better place.
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在你的社区中公开讨论这些问题；作为审稿人，我们可以继续教育和影响彼此，将我们的规范推向更好的方向。
- en: Another question outside of the scope of this post, but that could be brought
    up for community discussion in the future, is whether the “closed” models should
    be accepted as regular conference submissions (in direct competition with “open”
    work for conference acceptance and best paper awards) — or perhaps it is time
    to reconsider the role of the “industry” track.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个超出本文范围的问题，但未来可能会引发社区讨论的是，是否应该将“封闭”模型接受为常规会议提交（与“开放”工作直接竞争以获得会议接受和最佳论文奖）——或者是否是时候重新考虑“行业”轨道的角色。
- en: '**Our community is at a turning point, and you can help to direct the new community
    norms to follow science rather than hype — both as an author and as a reviewer**.
    The more people cite and study the best available open solutions, the more we
    incentivize open and transparent research, and the more likely it is that the
    next open solution will be much better. After all, it is our tradition of open
    research that has made our community so successful.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '**我们的社区正处于转折点，你可以帮助引导新的社区规范，遵循科学而不是炒作——无论是作为作者还是审稿人**。引用和研究最佳可用开放解决方案的人越多，我们就越能激励开放和透明的研究，下一步的开放解决方案也更有可能更好。毕竟，正是我们开放研究的传统使我们的社区如此成功。'
- en: 'Addendum: counter-arguments'
  id: totrans-69
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 附录：反对意见
- en: '**Train-test overlap and uninspected training data has always been an issue,
    ever since we started doing transfer learning with word2vec and onwards. Why protest
    now?**'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '**训练-测试重叠和未检查的训练数据一直是一个问题，自我们开始使用word2vec进行迁移学习以来就是如此。为什么现在要抗议呢？**'
- en: People have in fact been raising that issue many times before. Again, even OpenAI
    itself devoted a big chunk of the GPT-3 paper to the issues with benchmark data
    contamination. The fact that an issue is old doesn’t make it a non-issue; it rather
    makes us a field with a decade of methodological debt, which doesn’t make sense
    to just keep accruing.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，人们之前已经多次提出过这个问题。再次，即使是OpenAI自己也在GPT-3论文中花了很大篇幅讨论基准数据污染的问题。问题的陈旧性并不会使它不再成为问题；相反，它使我们成为一个有十年方法学债务的领域，这种情况没有意义，仅仅不断累积。
- en: '**The-Closed-Model-Everybody-Is-Talking-About does seem to work better for
    this task than my model or open alternatives, how can I just ignore it and claim
    state-of-the-art?**'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '**“大家谈论的封闭模型确实在这个任务上表现得比我的模型或开放替代方案更好，我怎么能忽视它并声称自己是最先进的呢？**'
- en: Don’t. “State-of-the-art” claims expire in a few months anyway. Be more specific,
    and just show improvement over the best open solution. Let’s say that in your
    task ChatGPT is clearly, obviously better than open alternatives, based on your
    own small testing with your own examples. What you don’t know is whether this
    is mostly due to some clever model architecture, or some proprietary data. In
    the latter case, your scientific finding would be… that models work best on data
    similar to what they were trained on. Not exactly revolutionary.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 不要。“最先进”的声明通常在几个月内就会过时。请更具体地说明，仅展示相对于最佳公开解决方案的改进。假设在你的任务中，ChatGPT显然、明显优于公开的替代方案，基于你自己用自己示例的小规模测试。你不知道的是，这是否主要由于某种巧妙的模型架构或某些专有数据。在后者的情况下，你的科学发现将是……模型在与其训练数据相似的数据上表现最好。这并不完全具有革命性。
- en: 'Also, ask yourself: are you sure that the impressive behavior you are observing
    is the result of pure generalization? As mentioned above, there is no way to tell
    how similar your test examples are to the training data. And that training data
    could include examples submitted by other researchers working on this topic, examples
    that were not part of any public dataset.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，问问自己：你确定你观察到的令人印象深刻的行为是纯粹的泛化结果吗？如上所述，我们无法判断你的测试示例与训练数据的相似程度。而且那些训练数据可能包括由其他在这个话题上工作的研究者提交的示例，这些示例并不是任何公开数据集的一部分。
- en: '**The-Closed-Model-Everybody-Is-Talking-About does seem to work better for
    this task than my model or open alternatives, how can I just ignore it and not
    build on it?**'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '**封闭模型（The-Closed-Model-Everybody-Is-Talking-About）在这项任务中的表现确实比我的模型或开放的替代方案更好，我怎么能忽视它而不在其基础上进行构建呢？**'
- en: 'That has indeed been the pathway to many, many NLP publications in the past:
    take an existing problem and the newest thing-that-everybody-is-talking-about,
    put them together, show improvement over previous approaches, publish. The problem
    is that with an API-access closed model you do not actually “build” on it; at
    best you formulate new prompts (and hope that they transfer across different model
    versions). If your goal is engineering, if you just need something that works
    — this might be sufficient. But if you are after a scientific contribution to
    machine learning theory or methods — this will necessarily reduce the perceived
    value of your work for the reviewers. And if the claim is that you found some
    new “behavior” that enables your solution, and hasn’t been noticed before — you
    will still need to show that this “behavior” cannot be explained by the training
    data.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 确实，以往许多NLP论文的路径是这样的：拿一个现有的问题和最新的“大家都在谈论的事物”，将它们结合起来，展示相对于以前方法的改进，进行发表。问题在于，对于一个API访问的封闭模型，你实际上并没有“构建”在其上；顶多你是制定新的提示（并希望它们能在不同的模型版本中转移）。如果你的目标是工程，如果你只是需要一个有效的方案——这可能就足够了。但如果你追求的是对机器学习理论或方法的科学贡献——这必然会降低你工作在评审者眼中的价值。如果声称你发现了一些新的“行为”使你的解决方案得以实现，并且之前没有被注意到——你仍然需要证明这种“行为”不能通过训练数据来解释。
- en: '**Whatever we may say, The-Closed-Model-Everybody-Is-Talking-About is on everyone’s
    minds. People are interested in it. If I don’t publish on it, somebody else will
    and get more credit than me.**'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '**无论我们怎么说，大家都在讨论的封闭模型（The-Closed-Model-Everybody-Is-Talking-About）已经成为每个人心中的焦点。人们对此很感兴趣。如果我不发表相关内容，其他人会发布，并且获得比我更多的认可。**'
- en: 'Well, that is a personal choice: what do you want the credit for and who do
    you want recognition from? Publishing on the “hottest” thing might work short-term,
    but, as shown above, if we simply follow the traditional NLP research narratives
    with these models as new requisite baselines in place of BERT, our work will be
    either fundamentally divorced from the basic principles of research methodology,
    or extremely short-lived, or both. Imagine looking at the list of your published
    papers 10 years from now: do you want it to be longer, or containing more things
    that you are proud of long-term?'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧，这个问题是个人选择：你想要什么样的认可，想从谁那里获得认可？在“最热门”的话题上发表可能短期内有效，但正如上面所示，如果我们仅仅跟随这些模型作为BERT的新的必备基准的传统NLP研究叙事，我们的工作将要么从研究方法学的基本原则中根本脱节，要么极其短暂，要么两者兼而有之。想象一下十年后看你的发表论文列表：你希望它更长，还是包含更多你长期以来引以为豪的东西？
- en: Are there other ways to study these models that would not run into these issues?
    We discussed some such ways for ethics-oriented research, perhaps there are other
    options as well.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 有没有其他方式来研究这些模型，不会遇到这些问题？我们讨论过一些针对伦理研究的方法，也许还有其他选择。
- en: '**Can’t we just study very made-up examples that are unlikely to have been
    in training data?**'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '**我们不能仅仅研究那些不太可能出现在训练数据中的虚构示例吗？**'
- en: First of all, if the point is to learn something about what that model does
    with real data — very artificial examples could be handled in some qualitatively
    different way.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，如果目的是了解该模型在真实数据上是如何运作的——某些非常人工的示例可能会以一些定性不同的方式处理。
- en: Second, at this point you need to be sure that you are way more original than
    all those other folks who tested ChatGPT for several months. Especially since
    the data used for RLHF comes from interactions with GPT3 — perhaps even your own!
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，在这一点上，你需要确保你比那些测试ChatGPT几个月的其他人更具原创性。尤其是因为用于RLHF的数据来自与GPT3的互动——可能甚至是你自己的！
- en: Third, you would still need to know what part actually hasn’t been seen. For
    example, ChatGPT was [reported](https://twitter.com/tqbf/status/1598513757805858820?s=20)
    to write a fable about a peanut butter sandwich stuck in a VCR, in King James
    Bible style, and that example got subsequently shared in dozens of media articles.
    This *is* a cool example, but what exactly is it that we believe to be impressive?
    The style transfer, the knowledge that things get stuck in VCRs, the plausible
    instructions? The degree of impressiveness of each one of these depends on what
    was in the training data. And even the impressiveness of the ability to tie these
    things together still depends on what combinations of “skills” were seen in training,
    and whether this is in fact a pure language model behavior, and not some combination
    of pipeline components.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 第三，你仍然需要知道哪些部分实际上未被看到。例如，ChatGPT 被[报道](https://twitter.com/tqbf/status/1598513757805858820?s=20)用King
    James Bible风格写了一则关于粘在VCR里的花生酱三明治的寓言，这个例子随后在数十篇媒体文章中被分享。这*确实*是一个很酷的例子，但我们认为令人印象深刻的到底是什么？风格转移、对物品被卡在VCR里的知识、还是可行的指示？这些中的每一个令人印象深刻的程度取决于训练数据中包含了什么。甚至将这些东西结合在一起的能力的印象深刻程度仍然取决于训练中看到了哪些“技能”的组合，以及这是否实际上是纯语言模型行为，而不是某些管道组件的组合。
- en: 'We tried to reproduce that answer, but accidentally typed “CVR” instead of
    “VCR”. The result was very illuminating. We got generic instructions that could
    have come from something like WikiHow: how to wipe off something sticky off something
    electric. Which, of course, is no good here: a sandwich includes a large piece
    of bread, which you would need to remove by hand rather than by wiping it off.
    But the best part is that the model later “admitted” it had no idea what “CVR”
    was! (Indeed, large language models don’t inherently “know” anything about the
    world). And then, when prompted for “VCR”, apparently the directive to maintain
    consistency within the dialogue overruled whatever it could have said about ‘VCR”…
    so we got the same wrong instructions.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 我们尝试重现那个答案，但不小心输入了“CVR”而不是“VCR”。结果非常有启发性。我们得到了一些通用的指示，这些指示可能来自类似WikiHow的内容：如何擦掉电器上的粘性物质。显然，这在这里没有用处：三明治包括一大块面包，你需要用手而不是擦拭的方式将其移除。但最有趣的是，模型后来“承认”它不知道“CVR”是什么！(实际上，大型语言模型本质上并不“知道”任何关于世界的事情)。然后，当被提示“VCR”时，显然保持对话一致性的指令覆盖了它对‘VCR’可能说的任何内容……所以我们得到了相同的错误指示。
- en: What did go without a hitch is the paraphrasing in the King James style. But
    it’s hard to imagine that paraphrasing was not an intended-and-trained-for “capability”,
    or that this style was not well represented in a large web-based corpus — o ye
    of little faith.Does it work well? Yes. Is it a magical “emergent” property? No.
    Can we develop another paraphrasing system and meaningfully compare it to this
    one? Also no. And this is where it stops being relevant for NLP research. *That
    which is not open and reasonably reproducible cannot be considered a requisite
    baseline.*
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 顺利进行的是King James风格的释义。但很难想象释义不是一个预期的且经过训练的“能力”，或者这种风格在大型基于网络的语料库中没有得到充分代表——哦，你们这些信心不足的人。它效果好吗？是的。它是一个神奇的“涌现”属性吗？不是。我们可以开发另一个释义系统并有意义地将其与这个系统进行比较吗？也不能。这就是它对NLP研究变得不相关的地方。*那种不公开且合理可重复的东西不能被视为必要的基准。*
- en: Notes
  id: totrans-86
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 说明
- en: ¹ The work on this post started a while ago, and has nothing to do with either
    [longtermism](https://futureoflife.org/open-letter/pause-giant-ai-experiments/)
    or the [plea](https://laion.ai/blog/petition/) for democratization of GPU resources.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: ¹ 这篇文章的工作开始于一段时间前，与[长期主义](https://futureoflife.org/open-letter/pause-giant-ai-experiments/)或[GPU资源民主化请求](https://laion.ai/blog/petition/)无关。
- en: ² There are in fact incentives for such companies to close and deprecate previous
    versions of their models, for the sake of (a) reducing attack surface, (b) capping
    technical debt. These are legitimate concerns for commercial entities, but they
    are intrinsically in tension with their models being objects of scientific inquiry.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: ² 实际上，确实有激励措施促使这些公司关闭和弃用其模型的旧版本，以（a）减少攻击面，（b）限制技术债务。这些对于商业实体来说是合理的担忧，但它们与模型作为科学研究对象的本质上存在矛盾。
