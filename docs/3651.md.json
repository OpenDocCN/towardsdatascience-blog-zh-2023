["```py\n#!pip install langchain openai weaviate-client ragas\n```", "```py\nOPENAI_API_KEY=\"<YOUR_OPENAI_API_KEY>\"\n```", "```py\nimport requests\nfrom langchain.document_loaders import TextLoader\nfrom langchain.text_splitter import CharacterTextSplitter\n\nurl = \"https://raw.githubusercontent.com/langchain-ai/langchain/master/docs/docs/modules/state_of_the_union.txt\"\nres = requests.get(url)\nwith open(\"state_of_the_union.txt\", \"w\") as f:\n    f.write(res.text)\n\n# Load the data\nloader = TextLoader('./state_of_the_union.txt')\ndocuments = loader.load()\n\n# Chunk the data\ntext_splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=50)\nchunks = text_splitter.split_documents(documents)\n```", "```py\nfrom langchain.embeddings import OpenAIEmbeddings\nfrom langchain.vectorstores import Weaviate\nimport weaviate\nfrom weaviate.embedded import EmbeddedOptions\nfrom dotenv import load_dotenv,find_dotenv\n\n# Load OpenAI API key from .env file\nload_dotenv(find_dotenv())\n\n# Setup vector database\nclient = weaviate.Client(\n  embedded_options = EmbeddedOptions()\n)\n\n# Populate vector database\nvectorstore = Weaviate.from_documents(\n    client = client,    \n    documents = chunks,\n    embedding = OpenAIEmbeddings(),\n    by_text = False\n)\n\n# Define vectorstore as retriever to enable semantic search\nretriever = vectorstore.as_retriever()\n```", "```py\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.prompts import ChatPromptTemplate\nfrom langchain.schema.runnable import RunnablePassthrough\nfrom langchain.schema.output_parser import StrOutputParser\n\n# Define LLM\nllm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n\n# Define prompt template\ntemplate = \"\"\"You are an assistant for question-answering tasks. \nUse the following pieces of retrieved context to answer the question. \nIf you don't know the answer, just say that you don't know. \nUse two sentences maximum and keep the answer concise.\nQuestion: {question} \nContext: {context} \nAnswer:\n\"\"\"\n\nprompt = ChatPromptTemplate.from_template(template)\n\n# Setup RAG pipeline\nrag_chain = (\n    {\"context\": retriever,  \"question\": RunnablePassthrough()} \n    | prompt \n    | llm\n    | StrOutputParser() \n)\n```", "```py\nfrom datasets import Dataset\n\nquestions = [\"What did the president say about Justice Breyer?\", \n             \"What did the president say about Intel's CEO?\",\n             \"What did the president say about gun violence?\",\n            ]\nground_truths = [[\"The president said that Justice Breyer has dedicated his life to serve the country and thanked him for his service.\"],\n                [\"The president said that Pat Gelsinger is ready to increase Intel's investment to $100 billion.\"],\n                [\"The president asked Congress to pass proven measures to reduce gun violence.\"]]\nanswers = []\ncontexts = []\n\n# Inference\nfor query in questions:\n  answers.append(rag_chain.invoke(query))\n  contexts.append([docs.page_content for docs in retriever.get_relevant_documents(query)])\n\n# To dict\ndata = {\n    \"question\": questions,\n    \"answer\": answers,\n    \"contexts\": contexts,\n    \"ground_truths\": ground_truths\n}\n\n# Convert dict to dataset\ndataset = Dataset.from_dict(data)\n```", "```py\nfrom ragas import evaluate\nfrom ragas.metrics import (\n    faithfulness,\n    answer_relevancy,\n    context_recall,\n    context_precision,\n)\n\nresult = evaluate(\n    dataset = dataset, \n    metrics=[\n        context_precision,\n        context_recall,\n        faithfulness,\n        answer_relevancy,\n    ],\n)\n\ndf = result.to_pandas()\n```"]