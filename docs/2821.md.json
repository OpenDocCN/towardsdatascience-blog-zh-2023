["```py\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_name = \"facebook/opt-350m\"\n\nmodel = AutoModelForCausalLM.from_pretrained(model_name).to(\"cuda\")\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\nprompt = \"Hello, I'm am conscious and\"\ninput_ids = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n\nsample = model.generate(**input_ids, max_length=64)\nprint(tokenizer.decode(sample[0]))\n# Hello, I'm am conscious and I'm a bit of a noob. I'm looking for a good place to start.\n```", "```py\nOPTForCausalLM(\n  (model): OPTModel(\n    (decoder): OPTDecoder(\n      (embed_tokens): Embedding(50272, 512, padding_idx=1)\n      (embed_positions): OPTLearnedPositionalEmbedding(2050, 1024)\n      (project_out): Linear(in_features=1024, out_features=512, bias=False)\n      (project_in): Linear(in_features=512, out_features=1024, bias=False)\n      (layers): ModuleList(\n        (0-23): 24 x OPTDecoderLayer(\n          (self_attn): OPTAttention(\n            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          )\n          (activation_fn): ReLU()\n          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        )\n      )\n    )\n  )\n  (lm_head): Linear(in_features=512, out_features=50272, bias=False)\n)\n```", "```py\nprint(tokenizer.vocab_size)\n# 50265\n```", "```py\nfrom PIL import Image\nimport requests\nfrom transformers import AutoProcessor, AutoModelForCausalLM\n\nmodel_name = \"microsoft/git-base-coco\"\n\nmodel = AutoModelForCausalLM.from_pretrained(model_name)\nprocessor = AutoProcessor.from_pretrained(model_name)\n\n# Downloading and preprocess an image\nurl = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\nimage = Image.open(requests.get(url, stream=True).raw)\npixel_values = processor(images=image, return_tensors=\"pt\").pixel_values\n\n# Preprocessing text\nprompt = \"What is this?\"\ninputs = processor(\n            prompt,\n            image,\n            return_tensors=\"pt\",\n            max_length=64\n        )\n\nsample = model.generate(**inputs, max_length=64)\nprint(processor.tokenizer.decode(sample[0]))\n# two cats sleeping on a couch\n```", "```py\nGitForCausalLM(\n  (git): GitModel(\n    (embeddings): GitEmbeddings(\n      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n      (position_embeddings): Embedding(1024, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (image_encoder): GitVisionModel(\n      (vision_model): GitVisionTransformer(\n        ...\n      )\n    )\n    (encoder): GitEncoder(\n      (layer): ModuleList(\n        (0-5): 6 x GitLayer(\n          ...\n        )\n      )\n    )\n    (visual_projection): GitProjection(\n      (visual_projection): Sequential(\n        (0): Linear(in_features=768, out_features=768, bias=True)\n        (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      )\n    )\n  )\n  (output): Linear(in_features=768, out_features=30522, bias=True)\n)\n```", "```py\nimport torch\n\ndef create_git_attention_mask(\n    tgt: torch.Tensor,\n    memory: torch.Tensor,\n) -> torch.Tensor:\n    num_tgt = tgt.shape[1]\n    num_memory = memory.shape[1]\n\n    # Areas where attention is applied are 0, areas without attention are -inf\n    top_left = torch.zeros((num_memory, num_memory))\n    top_right = torch.full(\n        (num_memory, num_tgt),\n        float(\"-inf\"),\n    )\n    bottom_left = torch.zeros(\n        (num_tgt, num_memory),\n    )\n\n    # Causal Attention Mask\n    bottom_right = torch.triu(torch.ones(tgt.shape[1], tgt.shape[1]), diagonal=1)\n    bottom_right = bottom_right.masked_fill(bottom_right == 1, float(\"-inf\"))\n\n    # Concatenate masks\n    left = torch.cat((top_left, bottom_left), dim=0)\n    right = torch.cat((top_right, bottom_right), dim=0)\n\n    # add axis for multi-head\n    full_attention_mask = torch.cat((left, right), dim=1)[None, None, :]\n\n    return full_attention_mask\n\n# batch_size, sequence, feature_dim\nvisual_feature = torch.rand(1, 3, 128)\ntext_feature = torch.rand(1, 4, 128)\n\nmask = create_git_attention_mask(tgt=text_feature, memory=visual_feature)\nprint(mask)\n\n\"\"\"\ntensor([[[[0., 0., 0., -inf, -inf, -inf, -inf],\n          [0., 0., 0., -inf, -inf, -inf, -inf],\n          [0., 0., 0., -inf, -inf, -inf, -inf],\n          [0., 0., 0., 0., -inf, -inf, -inf],\n          [0., 0., 0., 0., 0., -inf, -inf],\n          [0., 0., 0., 0., 0., 0., -inf],\n          [0., 0., 0., 0., 0., 0., 0.]]]])\n\"\"\"\n```", "```py\nclass GitOPTModel(OPTModel):\n    def __init__(self, config: OPTConfig):\n        super(GitOPTModel, self).__init__(config)\n\n        self.image_encoder = CLIPVisionModel.from_pretrained(config.vision_model_name)\n        self.visual_projection = GitProjection(config)\n```", "```py\nclass GitOPTModel(OPTModel):\n    ...\n\n    def forward(\n        self,\n        input_ids: Optional[torch.Tensor] = None,\n        attention_mask: Optional[torch.Tensor] = None,\n        pixel_values: Optional[torch.Tensor] = None,\n    ) -> BaseModelOutputWithPooling:\n        seq_length = input_shape[1]\n\n        # 1\\. Extract image features using ViT\n        visual_features = self.image_encoder(pixel_values).last_hidden_state\n\n        # 2\\. Convert features extracted by ViT into prompt-like Image Embeddings\n        projected_visual_features = self.visual_projection(visual_features)\n\n        # 3\\. Vectorize the tokens\n        inputs_embeds = self.decoder.embed_tokens(input_ids)\n\n        # 4\\. Obtain Positional Encoding\n        pos_embeds = self.embed_positions(attention_mask, 0)\n\n        # 5\\. Dimension adjustment of Text Embeddings specific to OPT\n        inputs_embeds = self.decoder.project_in(inputs_embeds)\n\n        # 6\\. Text Embeddings + Positional Encoding\n        embedding_output = inputs_embeds + pos_embeds\n\n        # 7\\. Concatenate Image Embeddings and Text Embeddings\n        hidden_states = torch.cat((projected_visual_features, embedding_output), dim=1)\n\n        # 8\\. Create Causal Attention Mask for Text region\n        tgt_mask = self._generate_future_mask(\n            seq_length, embedding_output.dtype, embedding_output.device\n        )\n\n        # 9\\. Create Attention Mask for GIT\n        combined_attention_mask = self.create_attention_mask(\n            tgt=embedding_output,\n            memory=projected_visual_features,\n            tgt_mask=tgt_mask,\n            past_key_values_length=0,\n        )\n\n        # 10\\. Pass through the Decoder layer repeatedly, the main part of the language model\n        for idx, decoder_layer in enumerate(self.decoder.layers):\n            layer_outputs = decoder_layer(\n                hidden_states,\n                attention_mask=combined_attention_mask,\n                output_attentions=output_attentions,\n                use_cache=use_cache,\n            )\n\n            hidden_states = layer_outputs[0]\n\n        # 11\\. Dimension adjustment MLP specific to OPT\n        hidden_states = self.decoder.project_out(hidden_states)\n\n        # 12\\. Align the output interface\n        return BaseModelOutputWithPast(\n            last_hidden_state=hidden_states,\n            past_key_values=next_cache,\n            hidden_states=all_hidden_states,\n            attentions=all_self_attns,\n        )\n```", "```py\nclass GitOPTForCausalLM(OPTForCausalLM):\n    def __init__(\n        self,\n        config,\n    ):\n        super(GitOPTForCausalLM, self).__init__(config)\n        self.model = GitOPTModel(config)\n\n    def forward(\n        ...\n    ) -> CausalLMOutputWithPast:\n\n        outputs = self.model(\n            ...\n        )\n\n        sequence_output = outputs[0]\n        logits = self.lm_head(sequence_output)\n\n        loss = None\n        if labels is not None:\n            # Predict the next word as the task\n            num_image_tokens = self.image_patch_tokens\n            shifted_logits = logits[:, num_image_tokens:-1, :].contiguous()\n            labels = labels[:, 1:].contiguous()\n            loss_fct = CrossEntropyLoss()\n            loss = loss_fct(shifted_logits.view(-1, self.config.vocab_size), labels.view(-1))\n\n        return CausalLMOutputWithPast(\n            loss=loss,\n            logits=logits,\n            ...\n        )\n```", "```py\nfrom transformers import AutoModelForCausalLM\nfrom peft import get_peft_config, get_peft_model, LoraConfig\n\nmodel = AutoModelForCausalLM.from_pretrained('microsoft/git-base')\n\npeft_config = LoraConfig(\n    task_type=\"CAUSAL_LM\",\n    r=8,\n    lora_alpha=32,\n    lora_dropout=0.1,\n    target_modules=[\"v_proj\"]\n)\n\npeft_model = get_peft_model(model, peft_config)\n```", "```py\ntarget_modules = [f\"model.image_encoder.vision_model.encoder.{i}.self_attn.v_proj\" for i in range(len(model.model.decoder))]\n```", "```py\n(self_attn): GitVisionAttention(\n  (k_proj): Linear(in_features=768, out_features=768, bias=True)\n  (v_proj): Linear(\n    in_features=768, out_features=768, bias=True\n    (lora_dropout): ModuleDict(\n      (default): Dropout(p=0.1, inplace=False)\n    )\n    (lora_A): ModuleDict(\n      (default): Linear(in_features=768, out_features=8, bias=False)\n    )\n    (lora_B): ModuleDict(\n      (default): Linear(in_features=8, out_features=768, bias=False)\n    )\n    (lora_embedding_A): ParameterDict()\n    (lora_embedding_B): ParameterDict()\n  )\n  (q_proj): Linear(in_features=768, out_features=768, bias=True)\n  (out_proj): Linear(in_features=768, out_features=768, bias=True)\n)\n```", "```py\nmodel = get_peft_model(model, peft_config)\nmodel.base_model.model.lm_head = model.lm_head\nmodel = model.base_model.model\n```", "```py\nclass SupervisedDataset(Dataset):\n    def __init__(\n        self,\n        vision_model_name: str,\n        model_name: str,\n        loaded_dataset: datasets.GeneratorBasedBuilder,\n        max_length: int = 128,\n    ):\n        super(SupervisedDataset, self).__init__()\n        self.loaded_dataset = loaded_dataset\n        self.max_length = max_length\n\n        self.processor = AutoProcessor.from_pretrained(\"microsoft/git-base\")\n\n        # Setting up the corresponding Processor for each model\n        self.processor.image_processor = CLIPImageProcessor.from_pretrained(vision_model_name)\n        self.processor.tokenizer = AutoTokenizer.from_pretrained(\n            model_name, padding_side=\"right\", use_fast=False\n        )\n\n    def __len__(self) -> int:\n        return len(self.loaded_dataset)\n\n    def __getitem__(self, index) -> dict:\n        # cf: https://huggingface.co/datasets/MMInstruction/M3IT#data-instances\n        row = self.loaded_dataset[index]\n\n        # Creating text input\n        text = f'##Instruction: {row[\"instruction\"]} ##Question: {row[\"inputs\"]} ##Answer: {row[\"outputs\"]}'\n\n        # Loading the image\n        image_base64_str_list = row[\"image_base64_str\"]  # str (base64)\n        img = Image.open(BytesIO(b64decode(image_base64_str_list[0])))\n\n        inputs = self.processor(\n            text,\n            img,\n            return_tensors=\"pt\",\n            max_length=self.max_length,\n            padding=\"max_length\",\n            truncation=True,\n        )\n        # batch size 1 -> unbatch\n        inputs = {k: v[0] for k, v in inputs.items()}\n        inputs[\"labels\"] = inputs[\"input_ids\"]\n        return inputs\n```", "```py\ncoco_datasets = datasets.load_dataset(\"MMInstruction/M3IT\", \"coco\")\ntest_dataset = coco_datasets[\"test\"]\n```", "```py\n# Specifying the parameters to train (training all would increase memory usage)\nfor name, p in model.model.named_parameters():\n    if np.any([k in name for k in keys_finetune]):\n        p.requires_grad = True\n    else:\n        p.requires_grad = False\n```", "```py\nhuggingface-cli login\n```", "```py\ndataset_list = [\n    datasets.load_dataset(\"MMInstruction/M3IT\", i) for i in m3it_name_list\n]\ntrain_dataset = torch.utils.data.ConcatDataset([d[\"train\"] for d in dataset_list])\n```", "```py\ncoco_datasets = datasets.load_dataset(\"MMInstruction/M3IT\", \"coco\")\ntest_dataset = coco_datasets[\"test\"]\nsupervised_test_dataset = SupervisedDataset(model_name, vision_model_name, test_dataset, 256)\n\nids = range(supervised_test_dataset.processor.tokenizer.vocab_size)\nall_ids = torch.tensor([i for i in ids]).cuda()\ntoken_id_to_features = model.model.embed_tokens(all_ids)\n```", "```py\ninputs = supervised_test_dataset[0] # Picking a sample arbitrarily\npixel_values = inputs[\"pixel_values\"]\nout_vit = model.model.image_encoder(pixel_values).last_hidden_state\nout_vit = model.model.visual_projection(out_vit)\n```", "```py\n# Dot product\nnearest_token = out_vit[0] @ token_id_to_features.T\n\n# The index of the maximum value corresponds to the relevant token ID\nvisual_out = nearest_token.argmax(-1).cpu().numpy()\ndecoded_text = supervised_test_dataset.processor.tokenizer.batch_decode(visual_out)\nprint(decoded_text)\n\n\"\"\"\n['otr', 'eg', 'anto', 'rix', 'Nas', ...]\n\"\"\"\n```", "```py\nprint(pd.Series(decoded_text).value_counts())\n\"\"\"\nmess        43\natura       29\nせ           10\nBranch      10\nEnum         9\nbell         9\nworden       7\n...\n\"\"\"\n```", "```py\nn_patches = 14\nIMAGE_HEIGHT = 468\nIMAGE_WIDTH = 640\n\ny_list = np.arange(15, IMAGE_HEIGHT, IMAGE_HEIGHT//n_patches)\nx_list = np.arange(10, IMAGE_WIDTH, IMAGE_WIDTH//n_patches)\n\nplt.figure()\nplt.axis(\"off\")\nplt.imshow(np.array(image), alpha=0.4)\nfor index in np.arange(n_patches ** 2):\n    y_pos = index // n_patches\n    x_pos = index - y_pos * n_patches\n\n    y = y_list[y_pos]\n    x = x_list[x_pos]\n\n    # The first token is the bos token, so it is excluded\n    word = decoded_text[index + 1]\n\n    # For differentiating words by color\n    plt.annotate(word, (x, y), size=7, color=\"blue\")\nplt.show()\nplt.clf()\nplt.close()\n```"]