- en: Domain Adaptation of A Large Language Model
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 大型语言模型的领域适配
- en: 原文：[https://towardsdatascience.com/domain-adaptation-of-a-large-language-model-2692ed59f180?source=collection_archive---------5-----------------------#2023-11-14](https://towardsdatascience.com/domain-adaptation-of-a-large-language-model-2692ed59f180?source=collection_archive---------5-----------------------#2023-11-14)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/domain-adaptation-of-a-large-language-model-2692ed59f180?source=collection_archive---------5-----------------------#2023-11-14](https://towardsdatascience.com/domain-adaptation-of-a-large-language-model-2692ed59f180?source=collection_archive---------5-----------------------#2023-11-14)
- en: Adapt a pre-trained model to a new domain using HuggingFace
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用HuggingFace将预训练模型适配到新领域
- en: '[](https://medium.com/@mina.ghashami?source=post_page-----2692ed59f180--------------------------------)[![Mina
    Ghashami](../Images/745f53b94f5667a485299b49913c7a21.png)](https://medium.com/@mina.ghashami?source=post_page-----2692ed59f180--------------------------------)[](https://towardsdatascience.com/?source=post_page-----2692ed59f180--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----2692ed59f180--------------------------------)
    [Mina Ghashami](https://medium.com/@mina.ghashami?source=post_page-----2692ed59f180--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@mina.ghashami?source=post_page-----2692ed59f180--------------------------------)[![Mina
    Ghashami](../Images/745f53b94f5667a485299b49913c7a21.png)](https://medium.com/@mina.ghashami?source=post_page-----2692ed59f180--------------------------------)[](https://towardsdatascience.com/?source=post_page-----2692ed59f180--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----2692ed59f180--------------------------------)
    [Mina Ghashami](https://medium.com/@mina.ghashami?source=post_page-----2692ed59f180--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fc99ed9ed7b9a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdomain-adaptation-of-a-large-language-model-2692ed59f180&user=Mina+Ghashami&userId=c99ed9ed7b9a&source=post_page-c99ed9ed7b9a----2692ed59f180---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----2692ed59f180--------------------------------)
    ·13 min read·Nov 14, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F2692ed59f180&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdomain-adaptation-of-a-large-language-model-2692ed59f180&user=Mina+Ghashami&userId=c99ed9ed7b9a&source=-----2692ed59f180---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fc99ed9ed7b9a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdomain-adaptation-of-a-large-language-model-2692ed59f180&user=Mina+Ghashami&userId=c99ed9ed7b9a&source=post_page-c99ed9ed7b9a----2692ed59f180---------------------post_header-----------)
    发表在[Towards Data Science](https://towardsdatascience.com/?source=post_page-----2692ed59f180--------------------------------)
    · 13分钟阅读 · 2023年11月14日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F2692ed59f180&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdomain-adaptation-of-a-large-language-model-2692ed59f180&user=Mina+Ghashami&userId=c99ed9ed7b9a&source=-----2692ed59f180---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F2692ed59f180&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdomain-adaptation-of-a-large-language-model-2692ed59f180&source=-----2692ed59f180---------------------bookmark_footer-----------)![](../Images/dcd659ece26bd7d235dacf241d80c574.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F2692ed59f180&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdomain-adaptation-of-a-large-language-model-2692ed59f180&source=-----2692ed59f180---------------------bookmark_footer-----------)![](../Images/dcd659ece26bd7d235dacf241d80c574.png)'
- en: Image from [unsplash](https://unsplash.com/photos/a-pink-flower-in-the-middle-of-a-cactus-TyRP_UH7I_0)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来自[unsplash](https://unsplash.com/photos/a-pink-flower-in-the-middle-of-a-cactus-TyRP_UH7I_0)
- en: Large language models (LLMs) like BERT are usually pre-trained on general domain
    corpora like Wikipedia and BookCorpus. If we apply them to more specialized domains
    like medical, there is often a drop in performance compared to models *adapted*
    for those domains.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs），如BERT，通常是在类似维基百科和BookCorpus的通用领域语料库上进行预训练的。如果将它们应用于医疗等更专业的领域，与*适配*这些领域的模型相比，性能往往会下降。
- en: In this article, we will explore how to adapt a pre-trained LLM like Deberta
    base to medical domain using the HuggingFace Transformers library. Specifically,
    we will cover an effective technique called intermediate pre-training where we
    do further pre-training of the LLM on data from our target domain. This adapts
    the model to the new domain, and improves its performance.
  id: totrans-10
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 在本文中，我们将探讨如何使用HuggingFace Transformers库将预训练的LLM（如Deberta base）适配到医学领域。具体而言，我们将介绍一种称为中间预训练的有效技术，在这种技术中，我们在目标领域的数据上进一步预训练LLM。这使模型适应新领域，并提高其性能。
- en: This is a simple yet effective technique to tune LLMs to your domain and gain
    significant improvements in downstream task performance.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个简单却有效的技术，可以将LLMs调整到你的领域，并在下游任务性能上获得显著提升。
- en: Let’s get started.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始吧。
- en: 'Step 1: The Data'
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第一步：数据
- en: 'First step in any project is to prepare the data. Since our dataset is in medical
    domain, it contains the following fields and many more:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 任何项目的第一步是准备数据。由于我们的数据集属于医学领域，它包含以下字段以及更多：
- en: '![](../Images/153b7422d396eaf71d450853c1da8540.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/153b7422d396eaf71d450853c1da8540.png)'
- en: image by author
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 图片作者
- en: Putting the full list of fields here is impossible, as there are many fields.
    But even this glimpse into the existing fields help us to form the input sequence
    for an LLM.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里列出所有字段是不可能的，因为字段众多。但即使是这些现有字段的简要介绍，也有助于我们为LLM形成输入序列。
- en: First point to keep in mind is that, the input has to be a sequence because
    LLMs read input as text sequences.
  id: totrans-18
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 需要记住的第一点是，输入必须是序列，因为LLMs将输入视为文本序列。
- en: 'To form this into a sequence, we can inject special tags to tell the LLM what
    piece of information is coming next. Consider the following example: `<patient>name:John,
    surname: Doer, patientID:1234, age:34</patient>` , the `<patient>` is a special
    tag that tells LLM that what follows are information about a patient.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '为了将其形成序列，我们可以注入特殊标签，以告知LLM接下来会出现什么信息。考虑以下示例：`<patient>name:John, surname: Doer,
    patientID:1234, age:34</patient>`，`<patient>` 是一个特殊标签，告知LLM接下来的是关于患者的信息。'
- en: 'So we form the input sequence as following:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们将输入序列形成如下：
- en: '![](../Images/15080a76be38bffaa30e1a17f24bd446.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/15080a76be38bffaa30e1a17f24bd446.png)'
- en: Image by author
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 图片作者
- en: 'As you see, we have injected four tags:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，我们注入了四个标签：
- en: '`<patient> </patient>`: to contain information about the patient'
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`<patient> </patient>`：包含关于患者的信息'
- en: '`<hospital> </hospital>`: to contain information regarding the hospital'
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`<hospital> </hospital>`：包含关于医院的信息'
- en: '`<event> </event>` : to contain information regarding the individual events
    the `patient` has done in the `hospital`.'
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`<event> </event>` ：包含关于 `patient` 在 `hospital` 中所做的各个事件的信息。'
- en: '`<visits> </visits>`: this is to enclose all events a patient has had in a
    hospital.'
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`<visits> </visits>`：用于封装患者在医院中经历的所有事件。'
- en: And inside each tag block, we are containing attributes as a *key:value* pair.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个标签块中，我们包含的属性是 *key:value* 对。
- en: Note, for a given patient and hospital, we are sorting events by timestamp and
    concatenating them together. This forms a time-ordered sequence of visits the
    patient has had in the hospital.
  id: totrans-29
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 请注意，对于给定的患者和医院，我们按时间戳排序事件并将它们连接在一起。这形成了患者在医院中经历的按时间排序的访问序列。
- en: The advantage of the special tags is that after training the LLM if I want the
    embedding for a patient, I can get it via retrieving the embedding for `<patient>`
    tag. Similarly, if we want to have an embedding for a patient such that it acts
    as a profile for the patient we can retrieve the embedding for `<visits>` tag,
    as this tag contain all events the patient has had with a hospital.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 特殊标签的优点在于，在训练LLM后，如果我想要一个患者的嵌入，可以通过检索 `<patient>` 标签的嵌入来获取。同样，如果我们想要一个作为患者档案的嵌入，可以检索
    `<visits>` 标签的嵌入，因为这个标签包含了患者在医院中经历的所有事件。
- en: 'Let’s assume our data is stored in s3; where the data schema is only one column
    called “text” and each record is a sequence of above format in “text” column.
    We load the data from s3 using below code:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们的数据存储在s3中；其中数据模式只有一列叫做“text”，每条记录是“text”列中上述格式的序列。我们使用以下代码从s3加载数据：
- en: '[PRE0]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'and the `raw_datasets` looks as following:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 和 `raw_datasets` 看起来如下：
- en: '![](../Images/711fe4d19dac23adebfe6d751f30d809.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/711fe4d19dac23adebfe6d751f30d809.png)'
- en: image by author
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 图片作者
- en: 'Step 2: Coding'
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第二步：编码
- en: 'First, install the requirements via following command:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，通过以下命令安装要求：
- en: '[PRE1]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The requirements.txt file looks as following:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: requirements.txt 文件如下：
- en: '[PRE2]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: To write the code, we have to define model arguments, data arguments and training
    arguments. We then need to define the model and put it in PEFT (parameter efficient)
    setting if we want to train it via PEFT.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 编写代码时，我们需要定义模型参数、数据参数和训练参数。然后我们需要定义模型，并将其置于PEFT（参数高效）设置中，如果我们想通过PEFT进行训练。
- en: First, we define the input arguments for data, for model and for training.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们定义数据、模型和训练的输入参数。
- en: Model Arguments
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型参数
- en: Model arguments are arguments that specify which model/tokenizer we are going
    to train or fine tune. The class below, implements these as a `dataclass` . We
    will get an instance from this later to pass our choices.
  id: totrans-44
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 模型参数是指定我们将要训练或微调的模型/分词器的参数。下面的类将这些实现为`dataclass`。我们稍后将从中获取一个实例来传递我们的选择。
- en: The most important field here is `model_name_or_path` but for completeness we
    keep all arguments.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 这里最重要的字段是`model_name_or_path`，但为了完整性，我们保留所有参数。
- en: '[PRE3]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: PEFT Arguments
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: PEFT参数
- en: Below we have the arguments pertaining the parameter efficient training. This
    uses the lora package for low-rank adaptation.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是与参数高效训练相关的参数。这使用了lora包进行低秩适应。
- en: '[PRE4]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Data Arguments
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据参数
- en: These are arguments pertaining to what data we are going to input our model
    for training and evaluation.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是与我们将要输入模型进行训练和评估的数据相关的参数。
- en: '[PRE5]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Initializing Arguments
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 初始化参数
- en: 'Next, we pass our input arguments in above classes and initialize all arguments:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们在上述类中传递输入参数并初始化所有参数：
- en: '[PRE6]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: As you see, we are loading deberta-base model so we will domain adapt this model
    to the medical domain.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，我们正在加载deberta-base模型，因此我们将把这个模型适应于医学领域。
- en: Data Tokenization
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据分词
- en: In this part we tokenize, collate and group the data.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们进行数据的分词、整理和分组。
- en: The tokenization part loads a pre-trained tokenizer related to our model, and
    adds the special tokens.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 分词部分加载了与我们模型相关的预训练分词器，并添加了特殊标记。
- en: '[PRE7]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'We then load the model and update the embedding layer of the model with number
    of tokens in the vocabulary of the tokenizer:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们加载模型，并使用分词器词汇中的令牌数量更新模型的嵌入层：
- en: '[PRE8]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'We also update the context length of the model in the tokenizer:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还在分词器中更新模型的上下文长度：
- en: '[PRE9]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'We then write the tokenize function and apply it on the dataset. Our data has
    one column called “text”. We tokenize this column and remove it from the output:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们编写分词函数并将其应用于数据集。我们的数据有一列称为“text”。我们对这一列进行分词并从输出中移除它：
- en: '[PRE10]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The `tokenized_datasets` looks as following:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '`tokenized_datasets`如下所示：'
- en: '![](../Images/8cd8e23fa2768b624a2d89b565840132.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8cd8e23fa2768b624a2d89b565840132.png)'
- en: Image by author
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片
- en: If we check the length of the `input_ids` for each record in segment `train`,
    we see that records have `input_ids` of different length.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们检查`train`段中每个记录的`input_ids`的长度，我们会看到记录的`input_ids`长度不同。
- en: '[PRE11]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'and it prints the following long list:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 它打印出以下长列表：
- en: '[PRE12]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: The point is every record has a different sequence length. We can pad them or
    truncate them or group them into sequences of size context length to make sure
    they are of the same size.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 要点是每个记录有不同的序列长度。我们可以填充它们、截断它们或将它们分组为上下文长度的序列，以确保它们大小相同。
- en: '[PRE13]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'now if you repeat the exercise:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，如果你重复这个练习：
- en: '[PRE14]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: it only prints `{512}` , because all sequences are of length 512.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 它只打印`{512}`，因为所有序列的长度都是512。
- en: 'Next we define the data collator:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来我们定义数据整理器：
- en: '[PRE15]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Training The Model
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练模型
- en: 'If we are going to train in parameter efficient mode, we use the lora package
    as following:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们要在参数高效模式下进行训练，我们使用lora包，如下所示：
- en: '[PRE16]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: and then we continue to write the `compute_metrics` function for computing the
    metric of choice. Here we use *accuracy*.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们继续编写用于计算选择指标的`compute_metrics`函数。这里我们使用*准确率*。
- en: '[PRE17]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Note `mask = labels != -100` is to ensure we compute the loss at masked entities.
    For entities which are masked their corresponding label is a positive ID (which
    is the input ID of the original token in that position). For entities which are
    not masked and therefore we don’t want to compute model’s performance on them,
    their corresponding label is set to `-100`.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 注意`mask = labels != -100`是为了确保我们在被掩盖的实体上计算损失。对于被掩盖的实体，其对应的标签是一个正ID（即原始标记在该位置的输入ID）。对于没有被掩盖的实体，因此我们不想计算模型在这些实体上的表现，其对应的标签被设置为`-100`。
- en: Defining `mask = labels != -100` produces `mask` as boolean vector and it is
    True only where entities are masked.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 定义`mask = labels != -100`产生`mask`作为布尔向量，只有在实体被掩盖时它才为True。
- en: '**Logit processing**: Then, we preprocess logits. The following function returns
    the index at which maximum logit was occured. This will be the prediction of the
    model.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '**Logit 处理**：然后，我们对logits进行预处理。以下函数返回最大logit出现的索引。这将是模型的预测。'
- en: '[PRE18]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '**Trainer**:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '**Trainer**：'
- en: This is where we initialize the `trainer` object. We kick off the training via
    `trainer.train()` soon.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们初始化`trainer`对象的地方。我们很快通过`trainer.train()`开始训练。
- en: '[PRE19]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Let’s kick off training via `trainer.train()` and save artifacts:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过`trainer.train()`启动训练并保存工件：
- en: '[PRE20]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'And it prints the following output:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 它打印出以下输出：
- en: '![](../Images/627f970f24d0601cd081d26e49947868.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/627f970f24d0601cd081d26e49947868.png)'
- en: Image by author
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片
- en: 'First of all, note the `trainer.train()` returns an object of type `TrainOutput`
    called `train_result` . This object looks like this:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，注意`trainer.train()`返回一个名为`train_result`的`TrainOutput`类型对象。该对象如下所示：
- en: '[PRE21]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: and note with `metrics = train_result.metrics` we are accessing the *metrics*
    dictionary inside it. We will pass this dictionary later to `trainer.log_metrics()`
    and `trainer.save_metrics()`.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 并且注意使用`metrics = train_result.metrics`我们正在访问*metrics*字典。稍后我们将把这个字典传递给`trainer.log_metrics()`和`trainer.save_metrics()`。
- en: 'The `trainer.log_metrics()`prints out a report as following:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '`trainer.log_metrics()`打印出的报告如下所示：'
- en: '![](../Images/037c6316aef7914599c45733639deb1d.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/037c6316aef7914599c45733639deb1d.png)'
- en: Image by author
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片
- en: 'Second, note that we are saving few things:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，注意我们保存了一些东西：
- en: '`trainer.save_model()`: This saves the model and its tokenizer. We can reload
    it later using `from_pretrained()`.'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`trainer.save_model()`：这将保存模型及其tokenizer。我们可以稍后使用`from_pretrained()`重新加载它。'
- en: '`trainer.save_state()`: This saves the trainer state since `trainer.save_model()`
    does not save the state. This statement creates a *trainer_state.json* file that
    looks as following:'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`trainer.save_state()`：这将保存训练器的状态，因为`trainer.save_model()`不会保存状态。此语句会创建一个*trainer_state.json*文件，如下所示：'
- en: '![](../Images/ef0ac5e2cbb59d6021982a0a3052215b.png)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ef0ac5e2cbb59d6021982a0a3052215b.png)'
- en: Image by author
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片
- en: '`trainer.save_metrics("train", metrics)` : This saves metrics into a json file
    for that train split, e.g. `train_results.json`. This file looks as following:'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`trainer.save_metrics("train", metrics)`：这将把指标保存到该训练分割的json文件中，例如`train_results.json`。这个文件如下所示：'
- en: '![](../Images/858a5c6fea6d1685699160197fcf196a.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/858a5c6fea6d1685699160197fcf196a.png)'
- en: Image by author
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片
- en: Conclusion
  id: totrans-112
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: In this post, we reviewed how to take a pre-trained LLM and adapt it to a new
    domain such as medical, financial etc. We took a pre-trained deberta base model
    from huggingFace and continued pre-training it on medical data. We saved the trained
    model in a directory for customized evaluation.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们回顾了如何将预训练的LLM适应到新的领域，如医学、金融等。我们从huggingFace获取了一个预训练的deberta基础模型，并继续在医学数据上进行再训练。我们将训练好的模型保存在一个目录中以便进行定制化评估。
- en: 'If you have any questions or suggestions, feel free to reach out to me:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你有任何问题或建议，请随时联系我：
- en: 'Email: mina.ghashami@gmail.com'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 'Email: mina.ghashami@gmail.com'
- en: 'LinkedIn: [https://www.linkedin.com/in/minaghashami/](https://www.linkedin.com/in/minaghashami/)'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 'LinkedIn: [https://www.linkedin.com/in/minaghashami/](https://www.linkedin.com/in/minaghashami/)'
