["```py\nimport gpboost as gpb\nimport pandas as pd\nimport numpy as np\n# Load data\ndata = pd.read_csv(\"https://raw.githubusercontent.com/fabsig/Compare_ML_HighCardinality_Categorical_Variables/master/data/wages.csv.gz\")\ndata = data.assign(t_sq = data['t']**2)# Add t^2\n# Partition into training and test data\nn = data.shape[0]\nnp.random.seed(n)\npermute_aux = np.random.permutation(n)\ntrain_idx = permute_aux[0:int(0.8 * n)]\ntest_idx = permute_aux[int(0.8 * n):n]\ndata_train = data.iloc[train_idx]\ndata_test = data.iloc[test_idx]\n# Define fixed effects predictor variables\npred_vars = [col for col in data.columns if col not in ['ln_wage', 'idcode', 't', 't_sq']]\n```", "```py\nlibrary(gpboost)\nlibrary(tidyverse)\n# Load data\ndata <- read_csv(\"https://raw.githubusercontent.com/fabsig/Compare_ML_HighCardinality_Categorical_Variables/master/data/wages.csv.gz\")\ndata[,\"t_sq\"] <- data[,\"t\"]^2 # Add t^2\ndata <- as.matrix(data) # Convert to matrix since the boosting part does not support data.frames\n# Partition into training and test data\nn <- dim(data)[1]\nset.seed(n)\npermute_aux <- sample.int(n, n)\ntrain_idx <- permute_aux[1:as.integer(0.8*n)]\ntest_idx <- permute_aux[(as.integer(0.8*n)+1):n]\ndata_train <- data[train_idx,]\ndata_test <- data[test_idx,]\n# Define fixed effects predictor variables\npred_vars <- colnames(data)[-which(colnames(data) %in% c(\"ln_wage\", \"idcode\", \"t\", \"t_sq\"))]\n```", "```py\ngp_model = gpb.GPModel(group_data=data_train['idcode'], likelihood='gaussian')\ndata_bst = gpb.Dataset(data=data_train[pred_vars], label=data_train['ln_wage'])\n```", "```py\ngp_model <- GPModel(group_data = data_train[,\"idcode\"], likelihood = \"gaussian\")\ndata_bst <- gpb.Dataset(data = data_train[,pred_vars], label = data_train[,\"ln_wage\"])\n```", "```py\npred_vars_FE = ['idcode'] + pred_vars\ndata_bst = gpb.Dataset(data=data_train[pred_vars_FE], categorical_feature=[0],\n                       label=data_train['ln_wage'])\n```", "```py\npred_vars_FE = c(\"idcode\", pred_vars)\ndata_bst <- gpb.Dataset(data = data_train[,pred_vars_FE], categorical_feature = c(1),\n                        label = data_train[,\"ln_wage\"])\n```", "```py\ngp_model = gpb.GPModel(group_data=data_train[['idcode','t']], likelihood='gaussian')\ndata_bst = gpb.Dataset(data=data_train[pred_vars], label=data_train['ln_wage'])\n```", "```py\ngp_model <- GPModel(group_data = data_train[,c(\"idcode\",\"t\")], likelihood = \"gaussian\")\ndata_bst <- gpb.Dataset(data = data_train[,pred_vars], label = data_train[,\"ln_wage\"])\n```", "```py\ngp_model = gpb.GPModel(group_data=data_train['idcode'], likelihood='gaussian',\n                       group_rand_coef_data=data_train[[\"t\",\"t_sq\"]],\n                       ind_effect_group_rand_coef=[1,1])\ndata_bst = gpb.Dataset(data=data_train[pred_vars], label=data_train['ln_wage'])\n```", "```py\ngp_model <- GPModel(group_data = data_train[,\"idcode\"], likelihood = \"gaussian\",\n                    group_rand_coef_data = cbind(data_train[,\"t\"], data_train[,\"t_sq\"]),\n                    ind_effect_group_rand_coef = c(1,1))\ndata_bst <- gpb.Dataset(data = data_train[,pred_vars], label = data_train[,\"ln_wage\"])\n```", "```py\ngp_model = gpb.GPModel(gp_coords=data_train['t'], cov_function='exponential',\n                       cluster_ids=data_train['idcode'], likelihood='gaussian')\ndata_bst = gpb.Dataset(data=data_train[pred_vars], label=data_train['ln_wage'])\n```", "```py\ngp_model <- GPModel(gp_coords = data_train[,\"t\"], cov_function=\"exponential\",\n                    cluster_ids = data_train[,\"idcode\"], likelihood = \"gaussian\")\ndata_bst <- gpb.Dataset(data = data_train[,pred_vars], label = data_train[,\"ln_wage\"])\n```", "```py\ncov_pars = gp_model.get_cov_pars()\nphi_hat = np.exp(-1 / cov_pars['GP_range'][0])\nsigma2_hat = cov_pars['GP_var'][0] * (1\\. - phi_hat ** 2)\nprint(\"Estimated innovation variance and AR(1) coefficient of year effect:\")\nprint([sigma2_hat, phi_hat])\n```", "```py\ncov_pars = gp_model$get_cov_pars()\nphi_hat = exp(-1 / cov_pars[\"GP_range\"])\nsigma2_hat = cov_pars[\"GP_var\"] * (1 - phi_hat^2)\nprint(\"Estimated innovation variance and AR(1) coefficient:\")\nprint(c(sigma2 = sigma2_hat, phi = phi_hat))\n```", "```py\ngp_model = gpb.GPModel(group_data=data_train['idcode'], likelihood='gaussian'\n                        gp_coords=data_train['t'], cov_function='exponential')\ndata_bst = gpb.Dataset(data=data_train[pred_vars], label=data_train['ln_wage'])\n```", "```py\ngp_model <- GPModel(group_data = data_train[,\"idcode\"], likelihood = \"gaussian\",\n                    gp_coords = data_train[,\"t\"], cov_function=\"exponential\")\ndata_bst <- gpb.Dataset(data = data_train[,pred_vars], label = data_train[,\"ln_wage\"])\n```", "```py\ngp_model = gpb.GPModel(group_data=data_train['idcode'], likelihood='gaussian',\n                       group_rand_coef_data=data_train[[\"t\",\"t_sq\"]],\n                       ind_effect_group_rand_coef=[1,1])\ndata_bst = gpb.Dataset(data=data_train[pred_vars], label=data_train['ln_wage'])\nparams = {'learning_rate': 0.01, 'max_depth': 2, 'min_data_in_leaf': 10,\n          'lambda_l2': 10, 'num_leaves': 2**10, 'verbose': 0}\nnrounds = 379\ngpbst = gpb.train(params=params, train_set=data_bst,  gp_model=gp_model,\n                  num_boost_round=nrounds) \ngp_model.summary() # Estimated random effects model\n```", "```py\n## =====================================================\n## Covariance parameters (random effects):\n##                        Param.\n## Error_term             0.0531\n## idcode                 0.0432\n## idcode_rand_coef_t     0.0004\n## idcode_rand_coef_t_sq  0.0000\n## =====================================================\n```", "```py\ngp_model <- GPModel(group_data = data_train[,\"idcode\"], likelihood = \"gaussian\",\n                    group_rand_coef_data = cbind(data_train[,\"t\"], data_train[,\"t_sq\"]),\n                    ind_effect_group_rand_coef = c(1,1))\ndata_bst <- gpb.Dataset(data = data_train[,pred_vars], label = data_train[,\"ln_wage\"])\nparams <- list(learning_rate = 0.01, max_depth = 2, num_leaves = 2^10,\n               min_data_in_leaf = 10, lambda_l2 = 10)\nnrounds <- 379\ngpbst <- gpb.train(data = data_bst, gp_model = gp_model, \n                   nrounds = nrounds, params = params, verbose = 0)\nsummary(gp_model) # Estimated random effects mode\n```", "```py\ngp_model.set_optim_params(params={\"optimizer_cov\": \"nelder_mead\"})\n```", "```py\ngp_model$set_optim_params(params = list(optimizer_cov = \"nelder_mead\"))\n```", "```py\n# Partition training data into inner training data and validation data\nntrain = data_train.shape[0]\nnp.random.seed(ntrain)\npermute_aux = np.random.permutation(ntrain)\ntrain_tune_idx = permute_aux[0:int(0.8 * ntrain)]\nvalid_tune_idx = permute_aux[int(0.8 * ntrain):ntrain]\nfolds = [(train_tune_idx, valid_tune_idx)]\n# Specify parameter grid, gp_model, and gpb.Dataset\nparam_grid = {'learning_rate': [1,0.1,0.01], 'max_depth': [1,2,3,5,10],\n              'min_data_in_leaf': [10,100,1000], 'lambda_l2': [0,1,10]}\nother_params = {'num_leaves': 2**10, 'verbose': 0}\ngp_model = gpb.GPModel(group_data=data_train['idcode'], likelihood='gaussian',\n                       group_rand_coef_data=data_train[[\"t\",\"t_sq\"]],\n                       ind_effect_group_rand_coef=[1,1])\ndata_bst = gpb.Dataset(data=data_train[pred_vars], label=data_train['ln_wage'])\n# Find optimal tuning parameters\nopt_params = gpb.grid_search_tune_parameters(param_grid=param_grid, params=other_params,\n                                             num_try_random=None, folds=folds, seed=1000,\n                                             train_set=data_bst, gp_model=gp_model,\n                                             num_boost_round=1000, early_stopping_rounds=10,\n                                             verbose_eval=1, metric='mse')\nopt_params\n# {'best_params': {'learning_rate': 0.01, 'max_depth': 2, 'min_data_in_leaf': 10, 'lambda_l2': 10}, 'best_iter': 379, 'best_score': 0.08000593485771226}\n```", "```py\n# Partition training data into inner training data and validation data\nntrain <- dim(data_train)[1]\nset.seed(ntrain)\nvalid_tune_idx <- sample.int(ntrain, as.integer(0.2*ntrain))\nfolds <- list(valid_tune_idx)\n# Specify parameter grid, gp_model, and gpb.Dataset\nparam_grid <- list(\"learning_rate\" = c(1,0.1,0.01), \"max_depth\" = c(1,2,3,5,10),\n                   \"min_data_in_leaf\" = c(10,100,1000), \"lambda_l2\" = c(0,1,10))\nother_params <- list(num_leaves = 2^10)\ngp_model <- GPModel(group_data = data_train[,\"idcode\"], likelihood = \"gaussian\",\n                    group_rand_coef_data = cbind(data_train[,\"t\"], data_train[,\"t_sq\"]),\n                    ind_effect_group_rand_coef = c(1,1))\ndata_bst <- gpb.Dataset(data = data_train[,pred_vars], label = data_train[,\"ln_wage\"])\n# Find optimal tuning parameters\nopt_params <- gpb.grid.search.tune.parameters(param_grid = param_grid, params = other_params,\n                                              num_try_random = NULL, folds = folds,\n                                              data = data_bst, gp_model = gp_model,\n                                              nrounds = 1000, early_stopping_rounds = 10,\n                                              verbose_eval = 1, metric = \"mse\")\nopt_params\n```", "```py\npred = gpbst.predict(data=data_test[pred_vars], group_data_pred=data_test['idcode'],\n                     group_rand_coef_data_pred=data_test[[\"t\",\"t_sq\"]],\n                     predict_var=False, pred_latent=False)\ny_pred = pred['response_mean']\nnp.mean((data_test['ln_wage'] - y_pred)**2)\n```", "```py\n## 0.08117682279905969\n```", "```py\npred <- predict(gpbst, data = data_test[,pred_vars], group_data_pred = data_test[,\"idcode\"], \n                group_rand_coef_data_pred = cbind(data_test[,\"t\"], data_test[,\"t_sq\"]),\n                predict_var = FALSE, pred_latent = FALSE)\ny_pred <- pred[[\"response_mean\"]]\nmean((data_test[,\"ln_wage\"] - y_pred)^2)\n```"]