- en: Cooking with Snowflake
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ Snowflake è¿›è¡Œæ•°æ®å¤„ç†
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/cooking-with-snowflake-833a1139ab01?source=collection_archive---------6-----------------------#2023-05-17](https://towardsdatascience.com/cooking-with-snowflake-833a1139ab01?source=collection_archive---------6-----------------------#2023-05-17)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://towardsdatascience.com/cooking-with-snowflake-833a1139ab01?source=collection_archive---------6-----------------------#2023-05-17](https://towardsdatascience.com/cooking-with-snowflake-833a1139ab01?source=collection_archive---------6-----------------------#2023-05-17)
- en: Snowflake optimisation
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Snowflake ä¼˜åŒ–
- en: Simple recipes & instant gratification on your Data Warehouse
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç®€å•çš„é…æ–¹ä¸æ•°æ®ä»“åº“ä¸­çš„å³æ—¶æ»¡è¶³
- en: '[](https://medium.com/@pbd_94?source=post_page-----833a1139ab01--------------------------------)[![Prabodh
    Agarwal](../Images/c4aa2193795fbc56edecbd78172da021.png)](https://medium.com/@pbd_94?source=post_page-----833a1139ab01--------------------------------)[](https://towardsdatascience.com/?source=post_page-----833a1139ab01--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----833a1139ab01--------------------------------)
    [Prabodh Agarwal](https://medium.com/@pbd_94?source=post_page-----833a1139ab01--------------------------------)'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@pbd_94?source=post_page-----833a1139ab01--------------------------------)[![Prabodh
    Agarwal](../Images/c4aa2193795fbc56edecbd78172da021.png)](https://medium.com/@pbd_94?source=post_page-----833a1139ab01--------------------------------)[](https://towardsdatascience.com/?source=post_page-----833a1139ab01--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----833a1139ab01--------------------------------)
    [Prabodh Agarwal](https://medium.com/@pbd_94?source=post_page-----833a1139ab01--------------------------------)'
- en: Â·
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: Â·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F9856a06a88a6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcooking-with-snowflake-833a1139ab01&user=Prabodh+Agarwal&userId=9856a06a88a6&source=post_page-9856a06a88a6----833a1139ab01---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----833a1139ab01--------------------------------)
    Â·10 min readÂ·May 17, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F833a1139ab01&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcooking-with-snowflake-833a1139ab01&user=Prabodh+Agarwal&userId=9856a06a88a6&source=-----833a1139ab01---------------------clap_footer-----------)'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '[å…³æ³¨](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F9856a06a88a6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcooking-with-snowflake-833a1139ab01&user=Prabodh+Agarwal&userId=9856a06a88a6&source=post_page-9856a06a88a6----833a1139ab01---------------------post_header-----------)
    å‘è¡¨åœ¨ [Towards Data Science](https://towardsdatascience.com/?source=post_page-----833a1139ab01--------------------------------)
    Â·10 åˆ†é’Ÿé˜…è¯»Â·2023å¹´5æœˆ17æ—¥[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F833a1139ab01&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcooking-with-snowflake-833a1139ab01&user=Prabodh+Agarwal&userId=9856a06a88a6&source=-----833a1139ab01---------------------clap_footer-----------)'
- en: --
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F833a1139ab01&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcooking-with-snowflake-833a1139ab01&source=-----833a1139ab01---------------------bookmark_footer-----------)![](../Images/73dfb2244fbf2936dcb1412336728421.png)'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F833a1139ab01&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcooking-with-snowflake-833a1139ab01&source=-----833a1139ab01---------------------bookmark_footer-----------)![](../Images/73dfb2244fbf2936dcb1412336728421.png)'
- en: Generated using Midjourney (paid subscription)
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ Midjourney ç”Ÿæˆï¼ˆä»˜è´¹è®¢é˜…ï¼‰
- en: The Snowflake community is rife with information dumps on how to optimize expensive
    queries. We know because we combed through a ton of them. What we present here
    are three tactical ways in which weâ€™ve done this at [Toplyne](https://www.toplyne.io/).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: Snowflake ç¤¾åŒºå……æ–¥ç€æœ‰å…³å¦‚ä½•ä¼˜åŒ–æ˜‚è´µæŸ¥è¯¢çš„ä¿¡æ¯å€¾é”€ã€‚æˆ‘ä»¬çŸ¥é“è¿™ä¸€ç‚¹ï¼Œå› ä¸ºæˆ‘ä»¬ç¿»é˜…äº†å¤§é‡è¿™æ ·çš„ä¿¡æ¯ã€‚æˆ‘ä»¬åœ¨è¿™é‡Œå±•ç¤ºçš„æ˜¯æˆ‘ä»¬åœ¨[Toplyne](https://www.toplyne.io/)ä¸­é‡‡ç”¨çš„ä¸‰ç§æˆ˜æœ¯æ–¹æ³•ã€‚
- en: Toplyneâ€™s business involves extracting real-time insights from real-time data.
    This data is currently sourced from our customersâ€™ Product Analytics, CRM, and
    payments system.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: Toplyne çš„ä¸šåŠ¡æ¶‰åŠä»å®æ—¶æ•°æ®ä¸­æå–å®æ—¶æ´å¯Ÿã€‚è¿™äº›æ•°æ®ç›®å‰æ¥æºäºæˆ‘ä»¬å®¢æˆ·çš„äº§å“åˆ†æã€CRM å’Œæ”¯ä»˜ç³»ç»Ÿã€‚
- en: CRM and payment data volumes are mostly manageable. A product will have a limited
    set of paying customers and marginally more who are tracked in a CRM. However,
    product analytics data is much higher in volume.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: CRM å’Œæ”¯ä»˜æ•°æ®çš„ä½“é‡å¤§å¤šæ˜¯å¯æ§çš„ã€‚ä¸€ä¸ªäº§å“ä¼šæœ‰æœ‰é™çš„ä»˜è´¹å®¢æˆ·ï¼Œä»¥åŠåœ¨ CRM ä¸­è·Ÿè¸ªçš„ç•¥å¤šä¸€äº›çš„å®¢æˆ·ã€‚ç„¶è€Œï¼Œäº§å“åˆ†ææ•°æ®çš„ä½“é‡è¦å¤§å¾—å¤šã€‚
- en: 'Toplyneâ€™s POC (proof-of-concept) and MVP (minimum viable product) were built
    on product analytics data. We knew right from the beginning we needed to use a
    Data Warehousing solution to handle the data. The solution had to pass two clear
    requirements:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: Toplyne çš„POCï¼ˆæ¦‚å¿µéªŒè¯ï¼‰å’ŒMVPï¼ˆæœ€ç®€å¯è¡Œäº§å“ï¼‰æ˜¯åŸºäºäº§å“åˆ†ææ•°æ®æ„å»ºçš„ã€‚æˆ‘ä»¬ä»ä¸€å¼€å§‹å°±çŸ¥é“éœ€è¦ä½¿ç”¨æ•°æ®ä»“åº“è§£å†³æ–¹æ¡ˆæ¥å¤„ç†è¿™äº›æ•°æ®ã€‚è¯¥è§£å†³æ–¹æ¡ˆå¿…é¡»æ»¡è¶³ä¸¤ä¸ªæ˜ç¡®çš„è¦æ±‚ï¼š
- en: It should easily ingest a few 100 gigabytes of data.
  id: totrans-14
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å®ƒåº”è¯¥èƒ½è½»æ¾å¤„ç†å‡ ç™¾GBçš„æ•°æ®ã€‚
- en: It should offer a simple yet concise API to interact with this data.
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å®ƒåº”è¯¥æä¾›ä¸€ä¸ªç®€å•è€Œç®€æ˜çš„ API æ¥ä¸è¿™äº›æ•°æ®äº¤äº’ã€‚
- en: 'We compared 3 solutions: BigQuery, Redshift & Snowflake.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬æ¯”è¾ƒäº†ä¸‰ç§è§£å†³æ–¹æ¡ˆï¼šBigQueryã€Redshift å’Œ Snowflakeã€‚
- en: Post-exploration, Snowflake was a clear choice. The simple reason is its SQL-based
    interface. SQL meant there was no cold start latency for our engineering ops.
    None of the engineers at Toplyne came from a DWH background, still, we found ourselves
    up to speed very quickly.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: æ¢ç´¢åï¼ŒSnowflake æ˜¯æ˜æ˜¾çš„é€‰æ‹©ã€‚ç®€å•çš„åŸå› æ˜¯å®ƒçš„ SQL åŸºç¡€æ¥å£ã€‚SQL æ„å‘³ç€æˆ‘ä»¬çš„å·¥ç¨‹æ“ä½œæ²¡æœ‰å†·å¯åŠ¨å»¶è¿Ÿã€‚Toplyne çš„å·¥ç¨‹å¸ˆæ²¡æœ‰æ¥è‡ª
    DWH èƒŒæ™¯çš„ç»éªŒï¼Œä½†æˆ‘ä»¬å¾ˆå¿«ä¸Šæ‰‹äº†ã€‚
- en: 'The process of interacting with customersâ€™ product analytics data is simple
    as follows:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸å®¢æˆ·äº§å“åˆ†ææ•°æ®äº¤äº’çš„è¿‡ç¨‹ç®€å•å¦‚ä¸‹ï¼š
- en: The product analytics data lands into Snowflake via a connector. (There are
    a lot of over-the-counter as well as native connectors for the same).
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: äº§å“åˆ†ææ•°æ®é€šè¿‡è¿æ¥å™¨è¿›å…¥ Snowflakeã€‚ï¼ˆæœ‰å¾ˆå¤šç°æˆçš„å’Œæœ¬åœ°çš„è¿æ¥å™¨å¯ä»¥ä½¿ç”¨ï¼‰ã€‚
- en: Login to Snowflake and use the in-built worksheets to write SQL. ğŸ‰
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ç™»å½• Snowflake å¹¶ä½¿ç”¨å†…ç½®çš„å·¥ä½œè¡¨ç¼–å†™ SQLã€‚ğŸ‰
- en: This simple 2-step process means that we can get on top of the data that our
    customers share with us in no time.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªç®€å•çš„ 2 æ­¥è¿‡ç¨‹æ„å‘³ç€æˆ‘ä»¬å¯ä»¥è¿…é€ŸæŒæ¡å®¢æˆ·ä¸æˆ‘ä»¬åˆ†äº«çš„æ•°æ®ã€‚
- en: In a short period, we cooked up two algorithms to transform the data we receive
    into a schema that can be trained by our data scientists. The first algorithm
    took care of transforming the product analytics events data. The second took care
    of identifying usersâ€™ profile data. Additional feature engineering algorithms
    are then written on top of this data.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨çŸ­æ—¶é—´å†…ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸¤ä¸ªç®—æ³•ï¼Œå°†æˆ‘ä»¬æ¥æ”¶åˆ°çš„æ•°æ®è½¬æ¢ä¸ºæ•°æ®ç§‘å­¦å®¶å¯ä»¥è®­ç»ƒçš„æ¨¡å¼ã€‚ç¬¬ä¸€ä¸ªç®—æ³•è´Ÿè´£è½¬æ¢äº§å“åˆ†æäº‹ä»¶æ•°æ®ã€‚ç¬¬äºŒä¸ªç®—æ³•è´Ÿè´£è¯†åˆ«ç”¨æˆ·çš„ä¸ªäººèµ„æ–™æ•°æ®ã€‚ç„¶ååœ¨è¿™äº›æ•°æ®ä¸Šç¼–å†™é™„åŠ çš„ç‰¹å¾å·¥ç¨‹ç®—æ³•ã€‚
- en: SQL is a fourth-generation language (4GL) that is relatively easier to learn.
    Combined with a worksheet-based interface that just requires you to have a browser
    tab â€” Snowflake; a scrappy startup could do a lot of data-heavy lifting with minimal
    setup effort.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: SQL æ˜¯ä¸€ç§ç¬¬å››ä»£è¯­è¨€ï¼ˆ4GLï¼‰ï¼Œç›¸å¯¹è¾ƒå®¹æ˜“å­¦ä¹ ã€‚ç»“åˆä¸€ä¸ªåªéœ€æµè§ˆå™¨æ ‡ç­¾çš„åŸºäºå·¥ä½œè¡¨çš„ç•Œé¢â€”â€”Snowflakeï¼›ä¸€ä¸ªåˆåˆ›å…¬å¸å¯ä»¥ä»¥æœ€å°çš„è®¾ç½®å·¥ä½œé‡å®Œæˆå¤§é‡çš„æ•°æ®å¤„ç†ã€‚
- en: We wrote a few SQLs in the worksheet to transform the data and then our data
    scientists could just `SELECT *` the data and write their ML training programs.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬åœ¨å·¥ä½œè¡¨ä¸­ç¼–å†™äº†ä¸€äº› SQL è¯­å¥æ¥è½¬æ¢æ•°æ®ï¼Œç„¶åæˆ‘ä»¬çš„æ•°æ®ç§‘å­¦å®¶å¯ä»¥ç›´æ¥ `SELECT *` æ•°æ®å¹¶ç¼–å†™ä»–ä»¬çš„ ML è®­ç»ƒç¨‹åºã€‚
- en: 'Over time the entire above-mentioned process has scaled up significantly. The
    scaling up has happened in the following aspects:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: éšç€æ—¶é—´çš„æ¨ç§»ï¼Œä¸Šè¿°æ•´ä¸ªè¿‡ç¨‹æ˜¾è‘—æ‰©å±•ã€‚æ‰©å±•å‘ç”Ÿåœ¨ä»¥ä¸‹å‡ ä¸ªæ–¹é¢ï¼š
- en: We have multiple customers, each of whom has their product analytics data in
    multiple platforms viz., Amplitude, Mixpanel, Segment, Clevertap, etc.
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬æœ‰å¤šä¸ªå®¢æˆ·ï¼Œæ¯ä¸ªå®¢æˆ·çš„äº§å“åˆ†ææ•°æ®åˆ†å¸ƒåœ¨å¤šä¸ªå¹³å°ä¸Šï¼Œå¦‚ Amplitudeã€Mixpanelã€Segmentã€Clevertap ç­‰ã€‚
- en: Our teams have written multiple algorithms to crunch the data along different
    axes.
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬çš„å›¢é˜Ÿç¼–å†™äº†å¤šä¸ªç®—æ³•ï¼Œä»¥ä¸åŒçš„ç»´åº¦å¤„ç†æ•°æ®ã€‚
- en: We now integrate CRM as well as payment data. Further, these datasets have their
    own set of ETL algorithms.
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ç°åœ¨é›†æˆäº† CRM å’Œæ”¯ä»˜æ•°æ®ã€‚æ­¤å¤–ï¼Œè¿™äº›æ•°æ®é›†æœ‰è‡ªå·±çš„ä¸€å¥— ETL ç®—æ³•ã€‚
- en: We use Airflow to orchestrate enormous pipelines which have multiple stages.
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ä½¿ç”¨ Airflow ç¼–æ’åºå¤§çš„ç®¡é“ï¼Œè¿™äº›ç®¡é“æœ‰å¤šä¸ªé˜¶æ®µã€‚
- en: Sample architecture diagram of our ETL flow. Snowflake sits at the heart of
    this system.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ ETL æµç¨‹çš„æ ·æœ¬æ¶æ„å›¾ã€‚Snowflake æ˜¯è¯¥ç³»ç»Ÿçš„æ ¸å¿ƒã€‚
- en: '![](../Images/260a503ca0b67035740ce4d76415aaa0.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/260a503ca0b67035740ce4d76415aaa0.png)'
- en: Toplyneâ€™s Data Pipeline architecture
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: Toplyne çš„æ•°æ®ç®¡é“æ¶æ„
- en: Sync source data into Snowflake.
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å°†æºæ•°æ®åŒæ­¥åˆ° Snowflakeã€‚
- en: Use Apache Airflow for ETL orchestration.
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ Apache Airflow è¿›è¡Œ ETL ç¼–æ’ã€‚
- en: Land the transformed data into Snowflake.
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å°†è½¬æ¢åçš„æ•°æ®å­˜å…¥ Snowflakeã€‚
- en: DS/ML/Analysts/Product consumes data from Snowflake for their flows.
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: DS/ML/åˆ†æå¸ˆ/äº§å“ä» Snowflake è·å–æ•°æ®ç”¨äºä»–ä»¬çš„å·¥ä½œæµç¨‹ã€‚
- en: Over the months, there have been multiple changes and major rewrites of different
    components of the system with Snowflake being the only constant.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: å‡ ä¸ªæœˆæ¥ï¼Œç³»ç»Ÿçš„ä¸åŒç»„ä»¶ç»å†äº†å¤šæ¬¡æ›´æ”¹å’Œé‡å¤§é‡å†™ï¼Œè€Œ Snowflake æ˜¯å”¯ä¸€ä¸å˜çš„ã€‚
- en: As we have run and maintained a system, we would like to present a few ideas
    around query optimization in Snowflake. We have a super simple technique that
    has allowed us to extract a lot of performance from the system with minor tweaks
    in your existing queries.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: '**Query optimization**'
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We run a multi-tenant system wherein a single Snowflake instance is responsible
    for the ETL of a lot of customer data. ETL is orchestrated by Airflow.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: We create a warehouse per customer and run all the ETL & feature engineering
    on that warehouse. There are 100s of SQL queries that are fired against a warehouse
    in sequence and/or in parallel during the entire ETL run for the customer. One
    run can last for an hour and there can be multiple runs for the customer in a
    day.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: Essentially, one warehouse size runs all expensive as well as cheap queries.
    So our objective is to keep warehouse size at a minimum. We define minimums by
    defining SLAs for different ETL runs. Then we modify the warehouse size so that
    ETL SLAs can be met at that size. Like any engineering org, we want to keep the
    warehouse size at a bare minimum given the SLA.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: We have dashboards where we monitor query patterns of the most expensive queries.
    These dashboards are at different levels of granularity. We monitor these dashboards
    constantly and keep tweaking the queries. Over time we have identified a few patterns
    in expensive queries and have come up with a playbook on how to minimize the run
    time of these queries. Weâ€™ll present 3 case studies outlining the problem statement
    for the query, how it was originally written, what was the bottleneck in that
    query and what was the optimal solution for the same.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: Window queries
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Scenario
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We track usersâ€™ profile information from product analytics data. Product Analytics
    systems save multiple data points about their users, e.g., location, device, subscription
    status, etc. Some data points change frequently while others do not so much. Given
    the nature of these data, the information is represented as an append-only log
    in a database.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: One of our feature engineering requirements is to capture the usersâ€™ latest
    profile info as of the ETL run.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b758fff47289b6182bb792dbf288cb70.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
- en: The above diagram gives a flowchart of the ETL.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: '**1** is the raw_data from product analytics, **2** is the algorithm that we
    want to apply & **3** is the final result of the ETL.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: 'The SQL query that we have is this:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Bottleneck
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This query is pretty simple to come up with and works great in Snowflake. However,
    the window function in this query is a bottleneck.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: 'Hereâ€™s how the query works:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: create as many logical buckets as there are `user_ids`
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`sort` data in every bucket in descending order'
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: assign `row_numbers` to the arranged data
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`qualify` the first entry in the bucket'
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*discard* the remaining data.'
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Based on the above explanation, we can see that as the data in the table increases,
    the number of buckets and the bucket sizes both will increase. Since we are dealing
    with an append-only dataset, we should be prepared for this eventuality. In Snowflake,
    youâ€™ll notice the size increase trend as **Byte Spillage** in your query profiler.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: Further, we need to understand that based on business requirements, it is expected
    for the number of buckets to increase, but as engineers, we can still keep the
    **size of an individual bucket** to a minimum.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: Optimal solution
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Weâ€™ll come up with a technique to keep the entries being bucketed to a minimum
    by using **CTEs** & an **aggregate function**.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: We convert our `descending sort expression` in-the-window query clause to the
    `max() function` and then join that to our source data to obtain a filter. By
    using this filter, we ensure that the data that would have been *discarded* by
    the `qualify` clause anyways would never be bucketed in the first place. This
    reduces the work performed by the window query drastically. Also, the additional
    cost of using an aggregate function is massively offset by the reduction, so the
    overall query becomes performant.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
- en: Pivot queries
  id: totrans-67
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Scenario
  id: totrans-68
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We use a feature on event data that requires getting a per-user per-event count.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: To obtain this data, we perform a group by query and then transpose this data
    to organize it into columns as shown in the image below.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: '**1** is the raw data & **2** is the output after the transformation.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/23964d6ab2492da9ff62c5fd92a86fd3.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
- en: 'The SQL query that we have is this:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Bottleneck
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Although the sample shows a pivot along 3 elements, our production use case
    generally functions on around a million users for approximately 1000 events.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: The pivot function on this query is the slowest step of the query. So we want
    to replace this logic with a manual pivot query. We generate this query by using
    a combination of `Group By` clause & `Filter clause`.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: Optimal solution 1
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: This query improved performance significantly.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
- en: We then **reduced** the warehouse size to see if the query remains equally performant.
    We observed that the query slowed down significantly and **byte spillage** was
    significant. However, an advantage of byte spillage is that we have more room
    for improvement in the reduced warehouse size.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: Optimal Solution 2
  id: totrans-82
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We rewrote this query as per the **Map-Reduce** framework and observed a significant
    improvement in runtime.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: 'The objective is to perform the above operation on a smaller set of events
    at a time and join all the data together in one go as follows:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Our production system will break up **1000 events** into **10 chunks** of **100
    events** each. This query speeds up significantly as it reduces byte spillage
    to near 0.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
- en: Also, this optimization is quite intuitive to derive once we replace the Pivot
    function with Optimal Solution 1.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
- en: '*Scroll to the bottom to find some accompaniment code for this article.*'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: Aggregate queries
  id: totrans-89
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: SQL spec defines a lot of aggregate functions and Snowflake does a great job
    at this. There is a massive repository of aggregate functions in Snowflake as
    well.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: SQLè§„èŒƒå®šä¹‰äº†è®¸å¤šæ±‡æ€»å‡½æ•°ï¼ŒSnowflakeåœ¨è¿™æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚Snowflakeä¸­ä¹Ÿæœ‰ä¸€ä¸ªåºå¤§çš„æ±‡æ€»å‡½æ•°åº“ã€‚
- en: Different aggregate functions have varying runtimes and in our opinion, every
    aggregate function should be treated on its merit. A strategy for optimizing aggregate
    functions is to first identify aggregate functions to be a bottleneck and then
    motivate yourself that there **might** be an algorithmic solution to your problem
    statement.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸åŒçš„æ±‡æ€»å‡½æ•°æœ‰ä¸åŒçš„è¿è¡Œæ—¶é—´ï¼Œåœ¨æˆ‘ä»¬çœ‹æ¥ï¼Œæ¯ä¸ªæ±‡æ€»å‡½æ•°éƒ½åº”æ ¹æ®å…¶ä¼˜ç‚¹è¿›è¡Œå¤„ç†ã€‚ä¼˜åŒ–æ±‡æ€»å‡½æ•°çš„ç­–ç•¥æ˜¯é¦–å…ˆè¯†åˆ«å‡ºæˆä¸ºç“¶é¢ˆçš„æ±‡æ€»å‡½æ•°ï¼Œç„¶åæ¿€åŠ±è‡ªå·±ç›¸ä¿¡**å¯èƒ½**å­˜åœ¨ç®—æ³•è§£å†³æ–¹æ¡ˆã€‚
- en: We would like to share one case study with you where we identified a query in
    which a suboptimal aggregate function was chosen. We redid the algorithm for the
    solution using a simpler aggregate function thereby getting a far superior performance
    for the same result.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬æƒ³ä¸æ‚¨åˆ†äº«ä¸€ä¸ªæ¡ˆä¾‹ç ”ç©¶ï¼Œåœ¨è¿™ä¸ªæ¡ˆä¾‹ä¸­ï¼Œæˆ‘ä»¬è¯†åˆ«äº†ä¸€ä¸ªæŸ¥è¯¢ï¼Œå…¶ä¸­é€‰æ‹©äº†ä¸€ä¸ªæ¬¡ä¼˜çš„æ±‡æ€»å‡½æ•°ã€‚æˆ‘ä»¬é‡æ–°è®¾è®¡äº†è¯¥è§£å†³æ–¹æ¡ˆçš„ç®—æ³•ï¼Œä½¿ç”¨äº†ä¸€ä¸ªæ›´ç®€å•çš„æ±‡æ€»å‡½æ•°ï¼Œä»è€Œè·å¾—äº†ç›¸åŒç»“æœçš„æ›´ä¼˜æ€§èƒ½ã€‚
- en: Scenario
  id: totrans-93
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æƒ…æ™¯
- en: We have a time series of events that are fired in the product analytics system.
    We need to answer 2 questions from this dataset for one of feature engineering.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬æœ‰ä¸€ä¸ªäº‹ä»¶æ—¶é—´åºåˆ—ï¼Œè¿™äº›äº‹ä»¶åœ¨äº§å“åˆ†æç³»ç»Ÿä¸­è§¦å‘ã€‚æˆ‘ä»¬éœ€è¦ä»è¿™ä¸ªæ•°æ®é›†ä¸­å›ç­”ä¸¤ä¸ªé—®é¢˜ï¼Œä»¥è¿›è¡Œç‰¹å¾å·¥ç¨‹ã€‚
- en: Q1) Identify all data points that are mostly fired multiple times within a second
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: Q1) è¯†åˆ«æ‰€æœ‰åœ¨ä¸€ç§’å†…å¤§å¤šè¢«å¤šæ¬¡è§¦å‘çš„æ•°æ®ç‚¹
- en: Q2) Identify data points that are mostly fired at least an hour apart
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: Q2) è¯†åˆ«å¤§å¤šåœ¨è‡³å°‘ä¸€å°æ—¶é—´éš”å†…è§¦å‘çš„æ•°æ®ç‚¹
- en: '![](../Images/94bc234e71ecdc85f1c613ee208e0eef.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/94bc234e71ecdc85f1c613ee208e0eef.png)'
- en: To answer these questions, we transform input data in `tbl_1` to `tbl_2` using
    a **window query** with the Snowflake **lag function**.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†å›ç­”è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬ä½¿ç”¨Snowflakeçš„**window query**å’Œ**lag function**å°†`tbl_1`ä¸­çš„è¾“å…¥æ•°æ®è½¬æ¢ä¸º`tbl_2`ã€‚
- en: We then write the solution query using the **median function** as follows.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åæˆ‘ä»¬ä½¿ç”¨**median function**ç¼–å†™äº†è§£å†³æ–¹æ¡ˆæŸ¥è¯¢ã€‚
- en: '[PRE5]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Bottleneck
  id: totrans-101
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç“¶é¢ˆ
- en: The `median` function is super slow.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '`median`å‡½æ•°éå¸¸æ…¢ã€‚'
- en: We asked ChatGPT to suggest an optimal solution. It did come up with a solution
    to use a **Percentile** function, but that was equally slow and seemed synonymous
    with the **Median** function itself.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬è®©ChatGPTå»ºè®®ä¸€ä¸ªæœ€ä½³è§£å†³æ–¹æ¡ˆã€‚å®ƒç¡®å®æå‡ºäº†ä½¿ç”¨**Percentile**å‡½æ•°çš„è§£å†³æ–¹æ¡ˆï¼Œä½†è¿™åŒæ ·å¾ˆæ…¢ï¼Œå¹¶ä¸”ä¼¼ä¹ä¸**Median**å‡½æ•°æœ¬èº«ç±»ä¼¼ã€‚
- en: However, ChatGPT did a good job of explaining why it came up with that solution.
    We then came up with a solution by iterating & improving ChatGPTâ€™s solution.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶è€Œï¼ŒChatGPTå¾ˆå¥½åœ°è§£é‡Šäº†ä¸ºä»€ä¹ˆæå‡ºäº†é‚£ä¸ªè§£å†³æ–¹æ¡ˆã€‚ç„¶åæˆ‘ä»¬é€šè¿‡è¿­ä»£å’Œæ”¹è¿›ChatGPTçš„è§£å†³æ–¹æ¡ˆï¼Œæå‡ºäº†ä¸€ä¸ªæ–°çš„è§£å†³æ–¹æ¡ˆã€‚
- en: Optimal solution
  id: totrans-105
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æœ€ä¼˜è§£å†³æ–¹æ¡ˆ
- en: We identified that for our requirement, we can just use count queries. For both
    **Q1)** & **Q2)**, we want the majority of our events to have **sec_diff** & **hour_diff**
    respectively equal to & greater than 0.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ç¡®å®šï¼Œå¯¹äºæˆ‘ä»¬çš„éœ€æ±‚ï¼Œæˆ‘ä»¬åªéœ€ä½¿ç”¨è®¡æ•°æŸ¥è¯¢ã€‚å¯¹äº**Q1)**å’Œ**Q2)**ï¼Œæˆ‘ä»¬å¸Œæœ›å¤§å¤šæ•°äº‹ä»¶çš„**sec_diff**å’Œ**hour_diff**åˆ†åˆ«å¤§äºæˆ–ç­‰äº0ã€‚
- en: '[PRE6]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Lessons learned
  id: totrans-108
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç»éªŒæ•™è®­
- en: We observe our systems constantly and then identify what optimizations require
    urgent analysis and what can be backlogged.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬æŒç»­è§‚å¯Ÿæˆ‘ä»¬çš„ç³»ç»Ÿï¼Œç„¶åç¡®å®šå“ªäº›ä¼˜åŒ–éœ€è¦ç´§æ€¥åˆ†æï¼Œå“ªäº›å¯ä»¥ç§¯å‹ã€‚
- en: Snowflake provides multiple configuration parameters which can be tuned in conjunction
    to obtain performance. The Snowflake community regularly publishes tricks & techniques.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: Snowflakeæä¾›äº†å¤šä¸ªé…ç½®å‚æ•°ï¼Œå¯ä»¥è¿›è¡Œè°ƒä¼˜ä»¥è·å¾—æ€§èƒ½ã€‚Snowflakeç¤¾åŒºå®šæœŸå‘å¸ƒæŠ€å·§å’ŒæŠ€æœ¯ã€‚
- en: Among all this information overload, we need to focus and build a playbook &
    a repo of techniques that works for us and can be applied mindlessly.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™äº›ä¿¡æ¯è¿‡è½½ä¸­ï¼Œæˆ‘ä»¬éœ€è¦ä¸“æ³¨å¹¶å»ºç«‹ä¸€ä¸ªé€‚åˆæˆ‘ä»¬çš„å‰§æœ¬å’ŒæŠ€æœ¯åº“ï¼Œè¿™äº›æŠ€æœ¯å¯ä»¥æ¯«ä¸è´¹åŠ›åœ°åº”ç”¨ã€‚
- en: 'These are the parameters that we use for our purpose:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›æ˜¯æˆ‘ä»¬ç”¨äºç›®çš„çš„å‚æ•°ï¼š
- en: Inspect every node in the query profiler
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æ£€æŸ¥æŸ¥è¯¢åˆ†æå™¨ä¸­çš„æ¯ä¸ªèŠ‚ç‚¹
- en: Do an input v/s output ratio for the node
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å¯¹èŠ‚ç‚¹è¿›è¡Œè¾“å…¥ä¸è¾“å‡ºæ¯”ç‡åˆ†æ
- en: i) Try to bring down this ratio
  id: totrans-115
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: i) å°è¯•é™ä½è¿™ä¸ªæ¯”ç‡
- en: ii) The output will remain constant given a problem statement
  id: totrans-116
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ii) ç»™å®šé—®é¢˜æè¿°æ—¶ï¼Œè¾“å‡ºå°†ä¿æŒä¸å˜
- en: iii) Hence, try to reduce input to the aggregate node
  id: totrans-117
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: iii) å› æ­¤ï¼Œå°½é‡å‡å°‘å¯¹æ±‡æ€»èŠ‚ç‚¹çš„è¾“å…¥
- en: Another way is to constantly measure the disk spillage. Reduce spillage whenever
    possible
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å¦ä¸€ç§æ–¹æ³•æ˜¯æŒç»­æµ‹é‡ç£ç›˜æº¢å‡ºã€‚å°½å¯èƒ½å‡å°‘æº¢å‡º
- en: i) Larger warehouses have low spillage but also cost higher
  id: totrans-119
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: i) è¾ƒå¤§çš„ä»“åº“æœ‰è¾ƒä½çš„æº¢å‡ºï¼Œä½†æˆæœ¬ä¹Ÿè¾ƒé«˜
- en: ii) You get optimization only if you can reduce spillage in the same warehouse
  id: totrans-120
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ii) åªæœ‰åœ¨å‡å°‘åŒä¸€ä»“åº“ä¸­çš„æº¢å‡ºæ—¶ï¼Œä½ æ‰èƒ½è·å¾—ä¼˜åŒ–
- en: '***Reproducing this article as code:*** You can refer [this github link](https://github.com/prabodh1194/sf-query-benchmarks)
    for code related to benchmarking of these queries.'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  id: totrans-122
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The article provides three highly actionable optimization techniques that you
    can apply right away. You might have a suboptimal pattern in your codebase that
    is similar to the case we have presented here. Feel free to use the code examples
    to get an instant resolution. In any other scenario, put on the thinking hat and
    Iâ€™ll be excited to learn about your solutions.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow me](https://medium.com/@pbd_94) for more articles on Snowflake, Data,
    and MLOps.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: P.S. All the images unless noted otherwise are from the author
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
