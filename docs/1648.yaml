- en: Cooking with Snowflake
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: åŽŸæ–‡ï¼š[https://towardsdatascience.com/cooking-with-snowflake-833a1139ab01?source=collection_archive---------6-----------------------#2023-05-17](https://towardsdatascience.com/cooking-with-snowflake-833a1139ab01?source=collection_archive---------6-----------------------#2023-05-17)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Snowflake optimisation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Simple recipes & instant gratification on your Data Warehouse
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@pbd_94?source=post_page-----833a1139ab01--------------------------------)[![Prabodh
    Agarwal](../Images/c4aa2193795fbc56edecbd78172da021.png)](https://medium.com/@pbd_94?source=post_page-----833a1139ab01--------------------------------)[](https://towardsdatascience.com/?source=post_page-----833a1139ab01--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----833a1139ab01--------------------------------)
    [Prabodh Agarwal](https://medium.com/@pbd_94?source=post_page-----833a1139ab01--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: Â·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F9856a06a88a6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcooking-with-snowflake-833a1139ab01&user=Prabodh+Agarwal&userId=9856a06a88a6&source=post_page-9856a06a88a6----833a1139ab01---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----833a1139ab01--------------------------------)
    Â·10 min readÂ·May 17, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F833a1139ab01&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcooking-with-snowflake-833a1139ab01&user=Prabodh+Agarwal&userId=9856a06a88a6&source=-----833a1139ab01---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F833a1139ab01&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcooking-with-snowflake-833a1139ab01&source=-----833a1139ab01---------------------bookmark_footer-----------)![](../Images/73dfb2244fbf2936dcb1412336728421.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Generated using Midjourney (paid subscription)
  prefs: []
  type: TYPE_NORMAL
- en: The Snowflake community is rife with information dumps on how to optimize expensive
    queries. We know because we combed through a ton of them. What we present here
    are three tactical ways in which weâ€™ve done this at [Toplyne](https://www.toplyne.io/).
  prefs: []
  type: TYPE_NORMAL
- en: Toplyneâ€™s business involves extracting real-time insights from real-time data.
    This data is currently sourced from our customersâ€™ Product Analytics, CRM, and
    payments system.
  prefs: []
  type: TYPE_NORMAL
- en: CRM and payment data volumes are mostly manageable. A product will have a limited
    set of paying customers and marginally more who are tracked in a CRM. However,
    product analytics data is much higher in volume.
  prefs: []
  type: TYPE_NORMAL
- en: 'Toplyneâ€™s POC (proof-of-concept) and MVP (minimum viable product) were built
    on product analytics data. We knew right from the beginning we needed to use a
    Data Warehousing solution to handle the data. The solution had to pass two clear
    requirements:'
  prefs: []
  type: TYPE_NORMAL
- en: It should easily ingest a few 100 gigabytes of data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It should offer a simple yet concise API to interact with this data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We compared 3 solutions: BigQuery, Redshift & Snowflake.'
  prefs: []
  type: TYPE_NORMAL
- en: Post-exploration, Snowflake was a clear choice. The simple reason is its SQL-based
    interface. SQL meant there was no cold start latency for our engineering ops.
    None of the engineers at Toplyne came from a DWH background, still, we found ourselves
    up to speed very quickly.
  prefs: []
  type: TYPE_NORMAL
- en: 'The process of interacting with customersâ€™ product analytics data is simple
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: The product analytics data lands into Snowflake via a connector. (There are
    a lot of over-the-counter as well as native connectors for the same).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Login to Snowflake and use the in-built worksheets to write SQL. ðŸŽ‰
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This simple 2-step process means that we can get on top of the data that our
    customers share with us in no time.
  prefs: []
  type: TYPE_NORMAL
- en: In a short period, we cooked up two algorithms to transform the data we receive
    into a schema that can be trained by our data scientists. The first algorithm
    took care of transforming the product analytics events data. The second took care
    of identifying usersâ€™ profile data. Additional feature engineering algorithms
    are then written on top of this data.
  prefs: []
  type: TYPE_NORMAL
- en: SQL is a fourth-generation language (4GL) that is relatively easier to learn.
    Combined with a worksheet-based interface that just requires you to have a browser
    tab â€” Snowflake; a scrappy startup could do a lot of data-heavy lifting with minimal
    setup effort.
  prefs: []
  type: TYPE_NORMAL
- en: We wrote a few SQLs in the worksheet to transform the data and then our data
    scientists could just `SELECT *` the data and write their ML training programs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Over time the entire above-mentioned process has scaled up significantly. The
    scaling up has happened in the following aspects:'
  prefs: []
  type: TYPE_NORMAL
- en: We have multiple customers, each of whom has their product analytics data in
    multiple platforms viz., Amplitude, Mixpanel, Segment, Clevertap, etc.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Our teams have written multiple algorithms to crunch the data along different
    axes.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We now integrate CRM as well as payment data. Further, these datasets have their
    own set of ETL algorithms.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We use Airflow to orchestrate enormous pipelines which have multiple stages.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Sample architecture diagram of our ETL flow. Snowflake sits at the heart of
    this system.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/260a503ca0b67035740ce4d76415aaa0.png)'
  prefs: []
  type: TYPE_IMG
- en: Toplyneâ€™s Data Pipeline architecture
  prefs: []
  type: TYPE_NORMAL
- en: Sync source data into Snowflake.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use Apache Airflow for ETL orchestration.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Land the transformed data into Snowflake.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: DS/ML/Analysts/Product consumes data from Snowflake for their flows.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Over the months, there have been multiple changes and major rewrites of different
    components of the system with Snowflake being the only constant.
  prefs: []
  type: TYPE_NORMAL
- en: As we have run and maintained a system, we would like to present a few ideas
    around query optimization in Snowflake. We have a super simple technique that
    has allowed us to extract a lot of performance from the system with minor tweaks
    in your existing queries.
  prefs: []
  type: TYPE_NORMAL
- en: '**Query optimization**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We run a multi-tenant system wherein a single Snowflake instance is responsible
    for the ETL of a lot of customer data. ETL is orchestrated by Airflow.
  prefs: []
  type: TYPE_NORMAL
- en: We create a warehouse per customer and run all the ETL & feature engineering
    on that warehouse. There are 100s of SQL queries that are fired against a warehouse
    in sequence and/or in parallel during the entire ETL run for the customer. One
    run can last for an hour and there can be multiple runs for the customer in a
    day.
  prefs: []
  type: TYPE_NORMAL
- en: Essentially, one warehouse size runs all expensive as well as cheap queries.
    So our objective is to keep warehouse size at a minimum. We define minimums by
    defining SLAs for different ETL runs. Then we modify the warehouse size so that
    ETL SLAs can be met at that size. Like any engineering org, we want to keep the
    warehouse size at a bare minimum given the SLA.
  prefs: []
  type: TYPE_NORMAL
- en: We have dashboards where we monitor query patterns of the most expensive queries.
    These dashboards are at different levels of granularity. We monitor these dashboards
    constantly and keep tweaking the queries. Over time we have identified a few patterns
    in expensive queries and have come up with a playbook on how to minimize the run
    time of these queries. Weâ€™ll present 3 case studies outlining the problem statement
    for the query, how it was originally written, what was the bottleneck in that
    query and what was the optimal solution for the same.
  prefs: []
  type: TYPE_NORMAL
- en: Window queries
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Scenario
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We track usersâ€™ profile information from product analytics data. Product Analytics
    systems save multiple data points about their users, e.g., location, device, subscription
    status, etc. Some data points change frequently while others do not so much. Given
    the nature of these data, the information is represented as an append-only log
    in a database.
  prefs: []
  type: TYPE_NORMAL
- en: One of our feature engineering requirements is to capture the usersâ€™ latest
    profile info as of the ETL run.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b758fff47289b6182bb792dbf288cb70.png)'
  prefs: []
  type: TYPE_IMG
- en: The above diagram gives a flowchart of the ETL.
  prefs: []
  type: TYPE_NORMAL
- en: '**1** is the raw_data from product analytics, **2** is the algorithm that we
    want to apply & **3** is the final result of the ETL.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The SQL query that we have is this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Bottleneck
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This query is pretty simple to come up with and works great in Snowflake. However,
    the window function in this query is a bottleneck.
  prefs: []
  type: TYPE_NORMAL
- en: 'Hereâ€™s how the query works:'
  prefs: []
  type: TYPE_NORMAL
- en: create as many logical buckets as there are `user_ids`
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`sort` data in every bucket in descending order'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: assign `row_numbers` to the arranged data
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`qualify` the first entry in the bucket'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*discard* the remaining data.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Based on the above explanation, we can see that as the data in the table increases,
    the number of buckets and the bucket sizes both will increase. Since we are dealing
    with an append-only dataset, we should be prepared for this eventuality. In Snowflake,
    youâ€™ll notice the size increase trend as **Byte Spillage** in your query profiler.
  prefs: []
  type: TYPE_NORMAL
- en: Further, we need to understand that based on business requirements, it is expected
    for the number of buckets to increase, but as engineers, we can still keep the
    **size of an individual bucket** to a minimum.
  prefs: []
  type: TYPE_NORMAL
- en: Optimal solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Weâ€™ll come up with a technique to keep the entries being bucketed to a minimum
    by using **CTEs** & an **aggregate function**.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: We convert our `descending sort expression` in-the-window query clause to the
    `max() function` and then join that to our source data to obtain a filter. By
    using this filter, we ensure that the data that would have been *discarded* by
    the `qualify` clause anyways would never be bucketed in the first place. This
    reduces the work performed by the window query drastically. Also, the additional
    cost of using an aggregate function is massively offset by the reduction, so the
    overall query becomes performant.
  prefs: []
  type: TYPE_NORMAL
- en: Pivot queries
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Scenario
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We use a feature on event data that requires getting a per-user per-event count.
  prefs: []
  type: TYPE_NORMAL
- en: To obtain this data, we perform a group by query and then transpose this data
    to organize it into columns as shown in the image below.
  prefs: []
  type: TYPE_NORMAL
- en: '**1** is the raw data & **2** is the output after the transformation.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/23964d6ab2492da9ff62c5fd92a86fd3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The SQL query that we have is this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Bottleneck
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Although the sample shows a pivot along 3 elements, our production use case
    generally functions on around a million users for approximately 1000 events.
  prefs: []
  type: TYPE_NORMAL
- en: The pivot function on this query is the slowest step of the query. So we want
    to replace this logic with a manual pivot query. We generate this query by using
    a combination of `Group By` clause & `Filter clause`.
  prefs: []
  type: TYPE_NORMAL
- en: Optimal solution 1
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: This query improved performance significantly.
  prefs: []
  type: TYPE_NORMAL
- en: We then **reduced** the warehouse size to see if the query remains equally performant.
    We observed that the query slowed down significantly and **byte spillage** was
    significant. However, an advantage of byte spillage is that we have more room
    for improvement in the reduced warehouse size.
  prefs: []
  type: TYPE_NORMAL
- en: Optimal Solution 2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We rewrote this query as per the **Map-Reduce** framework and observed a significant
    improvement in runtime.
  prefs: []
  type: TYPE_NORMAL
- en: 'The objective is to perform the above operation on a smaller set of events
    at a time and join all the data together in one go as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Our production system will break up **1000 events** into **10 chunks** of **100
    events** each. This query speeds up significantly as it reduces byte spillage
    to near 0.
  prefs: []
  type: TYPE_NORMAL
- en: Also, this optimization is quite intuitive to derive once we replace the Pivot
    function with Optimal Solution 1.
  prefs: []
  type: TYPE_NORMAL
- en: '*Scroll to the bottom to find some accompaniment code for this article.*'
  prefs: []
  type: TYPE_NORMAL
- en: Aggregate queries
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: SQL spec defines a lot of aggregate functions and Snowflake does a great job
    at this. There is a massive repository of aggregate functions in Snowflake as
    well.
  prefs: []
  type: TYPE_NORMAL
- en: Different aggregate functions have varying runtimes and in our opinion, every
    aggregate function should be treated on its merit. A strategy for optimizing aggregate
    functions is to first identify aggregate functions to be a bottleneck and then
    motivate yourself that there **might** be an algorithmic solution to your problem
    statement.
  prefs: []
  type: TYPE_NORMAL
- en: We would like to share one case study with you where we identified a query in
    which a suboptimal aggregate function was chosen. We redid the algorithm for the
    solution using a simpler aggregate function thereby getting a far superior performance
    for the same result.
  prefs: []
  type: TYPE_NORMAL
- en: Scenario
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We have a time series of events that are fired in the product analytics system.
    We need to answer 2 questions from this dataset for one of feature engineering.
  prefs: []
  type: TYPE_NORMAL
- en: Q1) Identify all data points that are mostly fired multiple times within a second
  prefs: []
  type: TYPE_NORMAL
- en: Q2) Identify data points that are mostly fired at least an hour apart
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/94bc234e71ecdc85f1c613ee208e0eef.png)'
  prefs: []
  type: TYPE_IMG
- en: To answer these questions, we transform input data in `tbl_1` to `tbl_2` using
    a **window query** with the Snowflake **lag function**.
  prefs: []
  type: TYPE_NORMAL
- en: We then write the solution query using the **median function** as follows.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Bottleneck
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `median` function is super slow.
  prefs: []
  type: TYPE_NORMAL
- en: We asked ChatGPT to suggest an optimal solution. It did come up with a solution
    to use a **Percentile** function, but that was equally slow and seemed synonymous
    with the **Median** function itself.
  prefs: []
  type: TYPE_NORMAL
- en: However, ChatGPT did a good job of explaining why it came up with that solution.
    We then came up with a solution by iterating & improving ChatGPTâ€™s solution.
  prefs: []
  type: TYPE_NORMAL
- en: Optimal solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We identified that for our requirement, we can just use count queries. For both
    **Q1)** & **Q2)**, we want the majority of our events to have **sec_diff** & **hour_diff**
    respectively equal to & greater than 0.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Lessons learned
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We observe our systems constantly and then identify what optimizations require
    urgent analysis and what can be backlogged.
  prefs: []
  type: TYPE_NORMAL
- en: Snowflake provides multiple configuration parameters which can be tuned in conjunction
    to obtain performance. The Snowflake community regularly publishes tricks & techniques.
  prefs: []
  type: TYPE_NORMAL
- en: Among all this information overload, we need to focus and build a playbook &
    a repo of techniques that works for us and can be applied mindlessly.
  prefs: []
  type: TYPE_NORMAL
- en: 'These are the parameters that we use for our purpose:'
  prefs: []
  type: TYPE_NORMAL
- en: Inspect every node in the query profiler
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Do an input v/s output ratio for the node
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: i) Try to bring down this ratio
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: ii) The output will remain constant given a problem statement
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: iii) Hence, try to reduce input to the aggregate node
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Another way is to constantly measure the disk spillage. Reduce spillage whenever
    possible
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: i) Larger warehouses have low spillage but also cost higher
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: ii) You get optimization only if you can reduce spillage in the same warehouse
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '***Reproducing this article as code:*** You can refer [this github link](https://github.com/prabodh1194/sf-query-benchmarks)
    for code related to benchmarking of these queries.'
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The article provides three highly actionable optimization techniques that you
    can apply right away. You might have a suboptimal pattern in your codebase that
    is similar to the case we have presented here. Feel free to use the code examples
    to get an instant resolution. In any other scenario, put on the thinking hat and
    Iâ€™ll be excited to learn about your solutions.
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow me](https://medium.com/@pbd_94) for more articles on Snowflake, Data,
    and MLOps.'
  prefs: []
  type: TYPE_NORMAL
- en: P.S. All the images unless noted otherwise are from the author
  prefs: []
  type: TYPE_NORMAL
