- en: Build and Play! Your Own V&L Model Equipped with LLM!
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 建立并玩耍！你自己的 V&L 模型配备 LLM！
- en: 原文：[https://towardsdatascience.com/build-and-play-your-own-v-l-model-equipped-with-llm-afa773b9249e?source=collection_archive---------6-----------------------#2023-09-07](https://towardsdatascience.com/build-and-play-your-own-v-l-model-equipped-with-llm-afa773b9249e?source=collection_archive---------6-----------------------#2023-09-07)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/build-and-play-your-own-v-l-model-equipped-with-llm-afa773b9249e?source=collection_archive---------6-----------------------#2023-09-07](https://towardsdatascience.com/build-and-play-your-own-v-l-model-equipped-with-llm-afa773b9249e?source=collection_archive---------6-----------------------#2023-09-07)
- en: Developing LLM-integrated GIT vision language models.
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 开发集成 LLM 的 GIT 视觉语言模型。
- en: '[](https://medium.com/@inoichan?source=post_page-----afa773b9249e--------------------------------)[![Yuichi
    Inoue](../Images/d25793aea6ddcdb90ecb21be5031a434.png)](https://medium.com/@inoichan?source=post_page-----afa773b9249e--------------------------------)[](https://towardsdatascience.com/?source=post_page-----afa773b9249e--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----afa773b9249e--------------------------------)
    [Yuichi Inoue](https://medium.com/@inoichan?source=post_page-----afa773b9249e--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@inoichan?source=post_page-----afa773b9249e--------------------------------)[![Yuichi
    Inoue](../Images/d25793aea6ddcdb90ecb21be5031a434.png)](https://medium.com/@inoichan?source=post_page-----afa773b9249e--------------------------------)[](https://towardsdatascience.com/?source=post_page-----afa773b9249e--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----afa773b9249e--------------------------------)
    [Yuichi Inoue](https://medium.com/@inoichan?source=post_page-----afa773b9249e--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ff3eff720c79a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuild-and-play-your-own-v-l-model-equipped-with-llm-afa773b9249e&user=Yuichi+Inoue&userId=f3eff720c79a&source=post_page-f3eff720c79a----afa773b9249e---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----afa773b9249e--------------------------------)
    ·21 min read·Sep 7, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fafa773b9249e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuild-and-play-your-own-v-l-model-equipped-with-llm-afa773b9249e&user=Yuichi+Inoue&userId=f3eff720c79a&source=-----afa773b9249e---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ff3eff720c79a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuild-and-play-your-own-v-l-model-equipped-with-llm-afa773b9249e&user=Yuichi+Inoue&userId=f3eff720c79a&source=post_page-f3eff720c79a----afa773b9249e---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----afa773b9249e--------------------------------)
    ·21 分钟阅读·2023年9月7日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fafa773b9249e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuild-and-play-your-own-v-l-model-equipped-with-llm-afa773b9249e&user=Yuichi+Inoue&userId=f3eff720c79a&source=-----afa773b9249e---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fafa773b9249e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuild-and-play-your-own-v-l-model-equipped-with-llm-afa773b9249e&source=-----afa773b9249e---------------------bookmark_footer-----------)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fafa773b9249e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuild-and-play-your-own-v-l-model-equipped-with-llm-afa773b9249e&source=-----afa773b9249e---------------------bookmark_footer-----------)'
- en: 'Summary of this article:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 本文摘要：
- en: '*Explaining GIT, a Vision Language Model developed by Microsoft.*'
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*解释由微软开发的 GIT 视觉语言模型。*'
- en: '*Replacing GIT’s language model with large language models (LLMs) using PyTorch
    and Hugging Face’s Transformers.*'
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*使用 PyTorch 和 Hugging Face 的 Transformers 替换 GIT 的语言模型为大型语言模型（LLMs）。*'
- en: '*Introducing how to fine-tune GIT-LLM models using LoRA.*'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*介绍如何使用 LoRA 微调 GIT-LLM 模型。*'
- en: '*Testing and discussing the developed models.*'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*测试和讨论开发的模型。*'
- en: '*Investigating if “Image Embeddings” embedded by the Image Encoder of GIT indicate
    specific characters in the same space as “Text Embedding”.*'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*探讨由 GIT 的图像编码器嵌入的“图像嵌入”是否在与“文本嵌入”相同的空间中指示特定字符。*'
- en: Large language models (LLM) are showing their value more and more. Incorporating
    images into LLMs makes them even more useful as vision language models. In this
    article, I will explain the development of a model called GIT-LLM, a simple but
    powerful vision language model. Some parts, like the code explanations, might
    feel a bit tedious, so feel free to jump straight to the results section. I conducted
    various experiments and analyses, so I think you’ll enjoy seeing what I was able
    to achieve.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLM）正展现出越来越多的价值。将图像纳入LLM使其作为视觉语言模型更加有用。在这篇文章中，我将解释一个称为GIT-LLM的模型的开发，这是一种简单但强大的视觉语言模型。某些部分，如代码解释，可能会显得有些繁琐，所以可以直接跳到结果部分。我进行了各种实验和分析，希望你能喜欢我所取得的成果。
- en: The implementation is available publicly, so please give it a try.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 实现已公开发布，所以请试试。
- en: '[](https://github.com/turingmotors/heron?source=post_page-----afa773b9249e--------------------------------)
    [## GitHub - turingmotors/heron'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://github.com/turingmotors/heron?source=post_page-----afa773b9249e--------------------------------)
    [## GitHub - turingmotors/heron'
- en: Contribute to turingmotors/heron development by creating an account on GitHub.
  id: totrans-17
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 通过在GitHub上创建一个账户来为turingmotors/heron的发展做贡献。
- en: github.com](https://github.com/turingmotors/heron?source=post_page-----afa773b9249e--------------------------------)
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: github.com](https://github.com/turingmotors/heron?source=post_page-----afa773b9249e--------------------------------)
- en: Transforming GIT into LLM
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将GIT转化为LLM
- en: Let’s dive into the main topic of this tech blog.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们深入探讨这篇技术博客的主要话题。
- en: What is GIT?
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是GIT？
- en: Generative Image-to-text Transformer, or GIT, is a vision language model proposed
    by Microsoft.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 生成式图像到文本变换器（Generative Image-to-text Transformer），或称GIT，是微软提出的一种视觉语言模型。
- en: 'arXiv: [https://arxiv.org/abs/2205.14100](https://arxiv.org/abs/2205.14100)'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 'arXiv: [https://arxiv.org/abs/2205.14100](https://arxiv.org/abs/2205.14100)'
- en: 'Code: [https://github.com/microsoft/GenerativeImage2Text](https://github.com/microsoft/GenerativeImage2Text)'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 代码： [https://github.com/microsoft/GenerativeImage2Text](https://github.com/microsoft/GenerativeImage2Text)
- en: Its architecture is quite simple. It converts feature vectors extracted from
    an image encoder into vectors that can be treated like text using a projection
    module. These vectors are then input into a language model to produce captions
    for images or to perform Q&A. The model can handle videos in a similar way.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 它的架构相当简单。它将从图像编码器提取的特征向量转换为可以像文本一样处理的向量，使用一个投影模块。这些向量随后输入到语言模型中，以生成图像的标题或进行问答。该模型也可以以类似的方式处理视频。
- en: '![](../Images/e26bc6f23c908cb230eb975a8c6eaac7.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e26bc6f23c908cb230eb975a8c6eaac7.png)'
- en: 'This figure is cited from [“GIT: A Generative Image-to-text Transformer for
    Vision and Language”](https://arxiv.org/abs/2205.14100)'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '该图摘自[“GIT: A Generative Image-to-text Transformer for Vision and Language”](https://arxiv.org/abs/2205.14100)'
- en: Despite its simplicity, if you look at the Leaderboard on “Paper with code”,
    you’ll find that it ranks highly in many tasks.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管它很简单，但如果你查看“Paper with code”的排行榜，你会发现它在许多任务中排名靠前。
- en: '[https://paperswithcode.com/paper/git-a-generative-image-to-text-transformer](https://paperswithcode.com/paper/git-a-generative-image-to-text-transformer)'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://paperswithcode.com/paper/git-a-generative-image-to-text-transformer](https://paperswithcode.com/paper/git-a-generative-image-to-text-transformer)'
- en: Originally, GIT uses strong models like CLIP for its image encoder and trains
    the language model part from scratch. However, in this article, I try to use a
    powerful LLM and fine-tune it. Here, I call the model “GIT-LLM”.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 最初，GIT使用像CLIP这样的强大模型作为其图像编码器，并从头开始训练语言模型部分。然而，在这篇文章中，我尝试使用一个强大的LLM并对其进行微调。在这里，我称该模型为“GIT-LLM”。
- en: Using a LLM with Hugging Face’s Transformers
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Hugging Face的Transformers来实现LLM
- en: I’ll use Hugging Face’s [Transformers](https://huggingface.co/docs/transformers/index)
    library for developping GIT-LLM. Transformers is a Python library for handling
    machine learning models. It offers many state-of-the-art pre-trained models that
    you can immediately run inference on. It also provides tools for training and
    fine-tuning models. I believe that Transformers has contributed significantly
    to the development of recent LLM derivatives. Almost all available LLMs can be
    handled with Transformers, and many multi-modal models derived from them use Transformers
    as their base for development and fine-tuning.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 我将使用Hugging Face的[Transformers](https://huggingface.co/docs/transformers/index)库来开发GIT-LLM。Transformers是一个用于处理机器学习模型的Python库。它提供了许多最先进的预训练模型，你可以立即进行推理。它还提供了训练和微调模型的工具。我相信Transformers在最近的LLM衍生品的发展中做出了重要贡献。几乎所有可用的LLM都可以用Transformers处理，许多从这些LLM衍生出的多模态模型也使用Transformers作为基础进行开发和微调。
- en: Here is the simplest code for using a model of Transformers. You can find it
    easy to try LLMs by useing *AutoModel* and *AutoTokenizer*.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 这是使用Transformers模型的最简单代码。你可以通过使用*AutoModel*和*AutoTokenizer*轻松尝试LLMs。
- en: '[PRE0]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Let’s check out the parameters the OPT model holds. Printing a model created
    by *AutoModelForCausalLM*.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们查看OPT模型所包含的参数。打印由*AutoModelForCausalLM*创建的模型。
- en: '[PRE1]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'It’s quite simple. The input dimension of the initial *embed_tokens* and the
    output dimension of the final *lm_head* is 50,272, which represents the number
    of tokens used in training this model. Let’s verify the size of the tokenizer’s
    vocabulary:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 这非常简单。初始*embed_tokens*的输入维度和最终*lm_head*的输出维度为50,272，表示训练此模型时使用的标记数量。让我们验证一下分词器词汇表的大小：
- en: '[PRE2]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Including special tokens like *bos_token*, *eos_token*, *unk_token*, *sep_token*,
    *pad_token*, *cls_token*, and *mask_token*, it predicts the probability of the
    next word from a total of 50,272 types of tokens.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 包括像*bos_token*、*eos_token*、*unk_token*、*sep_token*、*pad_token*、*cls_token*和*mask_token*这样的特殊标记，它预测了从总共50,272种标记中下一个单词的概率。
- en: 'You can understand how these models are connected by looking at [the implementation](https://github.com/huggingface/transformers/blob/v4.30.0/src/transformers/models/opt/modeling_opt.py).
    A simple diagram would represent the flow as follows:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过查看[实现](https://github.com/huggingface/transformers/blob/v4.30.0/src/transformers/models/opt/modeling_opt.py)来理解这些模型是如何连接的。一个简单的图示将表示如下流程：
- en: '![](../Images/5352c8349ab3b070c14a69644b854f65.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5352c8349ab3b070c14a69644b854f65.png)'
- en: Simplified model architecture of OPT (image made by the author)
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: OPT的简化模型架构（图像由作者制作）
- en: The structure and data flow are quite simple. The 〇〇Model and 〇〇ForCausalLM
    have a similar framework across different language models. The 〇〇Model class mainly
    represents the “Transformer” part of the language model. If, for instance, you
    want to perform tasks like text classification, you’d use only this part. The
    〇〇ForCausalLM class is for text generation, applying a classifier for token count
    to the vectors after processing them with the Transformer. The calculation of
    the loss is also done within the forward method of this class. The *embed_positions*
    denotes positional encoding, which is added to *project_in*.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 结构和数据流非常简单。〇〇Model和〇〇ForCausalLM在不同的语言模型中具有类似的框架。〇〇Model类主要表示语言模型的“Transformer”部分。例如，如果你想执行文本分类任务，你只需使用这一部分。〇〇ForCausalLM类用于文本生成，将分类器应用于处理后转换器的向量中的标记计数。损失计算也是在该类的前向方法中完成的。*embed_positions*表示位置编码，它会被加到*project_in*上。
- en: Using GIT with Transformers
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用GIT与Transformers
- en: I’ll give it a try based on [the official documentation page](https://huggingface.co/docs/transformers/model_doc/git)
    of GIT. As I’ll be processing images as well, I’ll use a Processor that also includes
    a Tokenizer.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 我将根据[GIT的官方文档页面](https://huggingface.co/docs/transformers/model_doc/git)尝试一下。由于我也会处理图像，所以我会使用一个同时包含Tokenizer的Processor。
- en: '[PRE3]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Given that [the input image](http://images.cocodataset.org/val2017/000000039769.jpg)
    produces the output “two cats sleeping on a couch”, it seems to be working well.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 给定[输入图像](http://images.cocodataset.org/val2017/000000039769.jpg)生成的输出为“两只猫在沙发上睡觉”，这表明它的效果很好。
- en: 'Let’s also take a look at the model’s structure:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们也来看看模型的结构：
- en: '[PRE4]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Although it’s a bit lengthy, if you break it down, it’s also quite simple.
    Within GitForCausalLM, there is a GitModel and within that, there are the following
    modules:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然有点长，但如果你拆解开来，它其实也很简单。在GitForCausalLM中，有一个GitModel，内部包含以下模块：
- en: embeddings (GitEmbeddings)
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: embeddings (GitEmbeddings)
- en: image_encoder (GitVisionModel)
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: image_encoder (GitVisionModel)
- en: encoder (GitEncoder)
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: encoder (GitEncoder)
- en: visual_projection (GitProjection)
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: visual_projection (GitProjection)
- en: output (Linear)
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: output (Linear)
- en: The major difference from OPT is the presence of *GitVisionModel* and *GitProjection*,
    which are the exact modules that convert images into prompt-like vectors. While
    the language model uses a Decoder for OPT and an Encoder for GIT, this only signifies
    a difference in how the attention mask is constructed. There may be slight differences
    in the transformer layer, but their functions are essentially the same. GIT uses
    the name Encoder because it uses a unique attention mask that applies attention
    to all features of the image and uses a causal mask for text features.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: Looking at the model’s connections;
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8b563994c34c6404fe9b5c7dcbd6d595.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
- en: Simplified model architecture of GIT ( image made by the author)
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: The image information is treated by the *GitVisionModel* and *GitProjection*
    to match the text’s embeddings. After that, it’s inputted alongside the text’s
    embeddings into the language model’s “Transformer” layers. While there are subtle
    differences, the part related to the language model is almost developed the same
    way.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: GIT’s Attention Mask
  id: totrans-61
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The architectures for the usual language model and the GIT language model are
    almost the same, but the ways of applying attention masks aredifferent.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: For the language model, an attention mask is applied not to look at past tokens
    when predicting future tokens. This is a method called “Causal Attention”, which
    corresponds to the left side of the following figure. The first column token references
    only itself, ensuring no self attention is applied to subsequent words. The second
    column applies self attention up to the second word, with the third word onwards
    becoming 0\. Such masking enables it to be trained to predict the next word effectively.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: 'GIT input has two types of tokens: image tokens and text tokens. Since all
    image tokens are used simultaneously and aren’t used to predict the next token,
    Causal Attention isn’t suitable. On the other hand, Causal Attention is still
    necesarry for text tokens. A mask like the one on the right side of the figure
    is designed to achieve this. For the top three rows of image information, self
    attention is applied with all token information. From the text tokens, moving
    down one column increases the number of words that can be referenced.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2274c8b7bd321af9b8894f9c179b5b3d.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
- en: Difference between the causal attention mask and the Git attention mask (image
    made by the author)
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s also check the code for making a GIT mask. The snippet to create the
    GIT mask is as follows:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: You add the mask to attention weights. Thus, parts where self attention takes
    place are 0, and parts that aren’t included in the attention are -inf. By providing
    this mask forward, only the text part can do causal attention. It’s important
    for vision language models to create and use masks effectively like this.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: Connecting GIT and OPT
  id: totrans-70
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now, let’s connect GIT and OPT. The goal is to create a model as shown in the
    figure.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/15317072b84b09432c4e5fb70e7acb6a.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
- en: Simplified model architecture of GIT-OPT (image made by the author)
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: GIT-OPT的简化模型架构（图由作者制作）
- en: For the general implementation, you can refer to the `[modeling_git.py](https://github.com/huggingface/transformers/blob/main/src/transformers/models/git/modeling_git.py)`.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 对于通用实现，你可以参考`[modeling_git.py](https://github.com/huggingface/transformers/blob/main/src/transformers/models/git/modeling_git.py)`。
- en: The most important part is the *GitOPTModel*. Inside this, a vision encoder
    need to be connected with a LLM. I’ll explain some key components.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 最重要的部分是*GitOPTModel*。在其中，一个视觉编码器需要与LLM连接。我会解释一些关键组件。
- en: '[PRE6]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Inside the *__init__* function, various modules are instantiated. The *super*
    initializes the *OPTModel*. In GIT, it is recommended to use a powerful image
    encoder trained with CLIP, so I have made it compatible with the ViT trained with
    CLIP. The *GitProjection* is taken from the original GIT implementation.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在*__init__*函数内部，实例化了各种模块。*super*初始化了*OPTModel*。在GIT中，推荐使用训练有素的CLIP图像编码器，因此我使其与CLIP训练的ViT兼容。*GitProjection*来自原始GIT实现。
- en: Let’s look inside the forward function. The implementation is based on the forward
    part of the *OPTDecoder*, with added information from the image encoder. Although
    it’s a bit lengthy, I’ve added comments in the code, so please follow each step.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看forward函数内部。实现基于*OPTDecoder*的forward部分，并添加了来自图像编码器的信息。虽然实现有点冗长，但我在代码中添加了注释，请按步骤进行。
- en: '[PRE7]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Although it might look complicated, if you go through each step, you’ll see
    that it follows the flow illustrated in the diagram. The actual code may look
    a bit more complex, but grasping the main process first will make understanding
    the other parts easier. This is pseudocode, so for detailed parts, please refer
    to [the published implementation](https://github.com/Ino-Ichan/GIT-LLM/blob/main/git_llm/git_opt/modeling_git_opt.py#L178).
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然看起来可能很复杂，但如果你逐步了解每个步骤，你会发现它遵循了图示中的流程。实际代码可能看起来有点复杂，但先掌握主要流程将使理解其他部分更容易。这是伪代码，对于详细部分，请参考[发布的实现](https://github.com/Ino-Ichan/GIT-LLM/blob/main/git_llm/git_opt/modeling_git_opt.py#L178)。
- en: Finally, let’s take a brief look at the *GITOPTForCausalLM* part.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们简要看看*GITOPTForCausalLM*部分。
- en: '[PRE8]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The processing inside the model is simple. When *labels* are provided, i.e.,
    during training, the loss calculation is also performed within the forward. In
    *shifted_logits*, tokens were fetched from the first token to the second-to-last
    token of the text tokens. It then calculates the Cross Entropy Loss with the *labels*
    shifted by one word as the correct answer.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 模型内部的处理很简单。当提供*labels*时，即在训练过程中，损失计算也在forward中进行。在*shifted_logits*中，从第一个token到文本tokens的倒数第二个token被提取。然后，它计算与*labels*偏移一个词的Cross
    Entropy Loss作为正确答案。
- en: One thing to note is to name the variable that assigns the *GitOPTModel* in
    the initialization function as *self.model*. If you check [the implementation
    of the parent class *OPTForCausalLM*](https://github.com/huggingface/transformers/blob/v4.31.0/src/transformers/models/opt/modeling_opt.py#L823),
    you'll see that OPT is first placed to *self.model* during the *super* initialization.
    If you change this instance variable name, you will end up holding two OPTs, which
    can strain the memory.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 一点需要注意的是，在初始化函数中分配*GitOPTModel*的变量需要命名为*self.model*。如果你查看[父类*OPTForCausalLM*的实现](https://github.com/huggingface/transformers/blob/v4.31.0/src/transformers/models/opt/modeling_opt.py#L823)，你会看到OPT在*super*初始化期间首先被放置到*self.model*中。如果你更改这个实例变量名，你将最终持有两个OPT，这可能会增加内存负担。
- en: LoRA Extension
  id: totrans-85
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LoRA扩展
- en: In order to fine-tune the LLM effectively, I’ll use a library called Parameter-Efficient
    Fine-Tuning ([PEFT](https://github.com/huggingface/peft)). As it’s developed by
    Hugging Face, it integrates seamlessly with Transfors. While there are various
    methods within PEFT, this time I’m going to do some experiments using a commonly
    seen approach called Low-rank adaptation (LoRA).
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 为了有效地微调LLM，我将使用一个名为Parameter-Efficient Fine-Tuning（[PEFT](https://github.com/huggingface/peft)）的库。由于它由Hugging
    Face开发，它与Transformers无缝集成。虽然PEFT中有各种方法，但这次我将使用一种常见的方法，即低秩适配（LoRA）进行实验。
- en: Models can be applied LoRA in just a few lines if the models support PEFT.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 如果模型支持PEFT，模型可以用LoRA在几行代码中应用。
- en: '[PRE9]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The *target_modules* argument specifies which modules you want to convert to
    LoRA. If a list is provided as *target_modules*, it is implemented to convert
    to LoRA for modules that end with each of the strings. LoRA is applied only to
    “value” (*v_proj*) of the self attention module for simplicity.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '*target_modules* 参数指定了你想要转换为 LoRA 的模块。如果提供了列表作为 *target_modules*，则会将每个字符串结尾的模块转换为
    LoRA。为了简化，LoRA 仅应用于自注意力模块的“value” (*v_proj*)。'
- en: In the model, ViT is used for the image encoder part. Be cautious, as specifying
    it like this, self attention part of ViT might also be applied LoRA. It’s a bit
    tedious, but by specifying down to the part where the key names don’t overlap
    and giving it to *target_modules*, you can avoid this.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在模型中，ViT 用于图像编码部分。请小心，因为这样指定的话，ViT 的自注意力部分可能也会应用 LoRA。这有点繁琐，但通过指定到键名不重叠的部分并将其传递给
    *target_modules*，你可以避免这种情况。
- en: '[PRE10]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The resulting model becomes an instance of the *PeftModelForCausalLM* class.
    It has an instance variable named *base_model* that holds the original model converted
    to LoRA. As an example, I show that LoRA is applied to *v_proj* of the self attention
    in ViT.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 结果模型变成了 *PeftModelForCausalLM* 类的一个实例。它有一个名为 *base_model* 的实例变量，保存了转换为 LoRA
    的原始模型。作为示例，我展示了 LoRA 如何应用于 ViT 的自注意力中的 *v_proj*。
- en: '[PRE11]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Inside the *v_proj* Linear, you'll find added fully connected layers such as
    *lora_A* and *lora_B*. The LoRA-converted *Linear* module is a namesake Linear
    class that inherits from PyTorch's *Linear* and *LoraLayer*. It's a somewhat unique
    module, so those curious about the details should take a look at [the implementation](https://github.com/huggingface/peft/blob/main/src/peft/tuners/lora/layer.py#L153).
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在 *v_proj* 线性层内部，你会发现添加了如 *lora_A* 和 *lora_B* 的全连接层。LoRA转换后的 *Linear* 模块是一个名字相同的
    Linear 类，继承自 PyTorch 的 *Linear* 和 *LoraLayer*。这是一个有些独特的模块，有兴趣了解细节的人可以查看[实现](https://github.com/huggingface/peft/blob/main/src/peft/tuners/lora/layer.py#L153)。
- en: Note that models created with PEFT will not save anything other than the LoRA
    part by default. While there’s a method to save using the *merge_and_unload* method,
    you might want to save all models being saved midway during training with Trainer.
    Overloading the Trainer's *_save_checkpoints* method is one approach, but to avoid
    the hassle, I handled it this time by fetching just the original model part held
    inside the *PeftModel* during the training phase.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，使用 PEFT 创建的模型默认不会保存除 LoRA 部分之外的任何内容。虽然可以通过 *merge_and_unload* 方法保存，但你可能希望在
    Trainer 训练过程中保存所有中途保存的模型。重载 Trainer 的 *_save_checkpoints* 方法是一种方法，但为了避免麻烦，我这次通过在训练阶段仅获取
    *PeftModel* 中原始模型部分来处理。
- en: '[PRE12]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: I believe there are more efficient ways to handle this, so I’m still researching.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 我相信还有更高效的处理方法，所以我仍在研究中。
- en: '**Experimenting with GIT-LLM**'
  id: totrans-98
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**使用 GIT-LLM 进行实验**'
- en: Let’s now conduct some experiments using the model developed so far.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们进行一些使用目前开发的模型的实验。
- en: For details on the training configuration and other setups, please refer to
    [the published implementation](https://github.com/Ino-Ichan/GIT-LLM), as they
    essentially follow the same method.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 关于训练配置和其他设置的详细信息，请参考[已发布的实现](https://github.com/Ino-Ichan/GIT-LLM)，因为它们本质上遵循相同的方法。
- en: 'Dataset: M3IT'
  id: totrans-101
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据集：M3IT
- en: For experiments, I wanted to use a dataset that pairs images with text and is
    easy to integrate. While exploring [the Hugging face’s *Datasets*](https://huggingface.co/datasets),
    I came across M3IT, a multimodal dataset for Instruction Tuning developed by the
    Shanghai AI Lab. Instruction Tuning is a method that yields impressive results
    even with a limited amount of data. It appears that M3IT has re-annotated various
    existing datasets specifically for Instruction Tuning.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 对于实验，我想使用一个将图像与文本配对并且易于集成的数据集。在浏览[Hugging Face 的 *Datasets*](https://huggingface.co/datasets)时，我发现了
    M3IT，这是一个由上海 AI 实验室开发的用于 Instruction Tuning 的多模态数据集。Instruction Tuning 是一种即使在数据量有限的情况下也能产生令人印象深刻结果的方法。看起来
    M3IT 重新标注了各种现有数据集，专门用于 Instruction Tuning。
- en: '[https://huggingface.co/datasets/MMInstruction/M3IT](https://huggingface.co/datasets/MMInstruction/M3IT)'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://huggingface.co/datasets/MMInstruction/M3IT](https://huggingface.co/datasets/MMInstruction/M3IT)'
- en: This dataset is easy to use, so I’ve decided to utilize it for the following
    experiments.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 这个数据集很容易使用，所以我决定在接下来的实验中利用它。
- en: To train using M3IT, it’s necessary to create a custom Pytorch Dataset.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用M3IT进行训练，必须创建一个自定义的Pytorch Dataset。
- en: '[PRE13]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: In the *__init__* function, the image_processor and tokenizer correspond to
    their respective models. The *loaded_dataset* argument passed should be from MMInstruction/M3IT
    datasets.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在 *__init__* 函数中，image_processor 和 tokenizer 分别对应其各自的模型。传递的 *loaded_dataset*
    参数应来自 MMInstruction/M3IT 数据集。
- en: '[PRE14]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: For the COCO Instruction Tuning dataset, the split between training, validation,
    and testing is identical to the original dataset, with 566,747, 25,010, and 25,010
    image-text pairs respectively. Other datasets, such as VQA or Video, can also
    be handled similarly, making it a versatile dataset for validation purposes.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 COCO Instruction Tuning 数据集，训练、验证和测试的划分与原始数据集相同，分别为 566,747、25,010 和 25,010
    对图像-文本对。其他数据集，如 VQA 或 Video，也可以类似处理，使其成为一个多用途的验证数据集。
- en: 'A sample data looks like this:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 示例数据如下：
- en: '![](../Images/357b932eebeffa2ad69452bccfd6b0da.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/357b932eebeffa2ad69452bccfd6b0da.png)'
- en: Image is cited from data in M3IT.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 图像引用自 M3IT 数据。
- en: 'The caption for this picture is as follows:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 该图片的说明如下：
- en: '##Instruction: Write a succinct description of the image, capturing its main
    components, the relationships between them, and any notable details. ##Question:
    ##Answer: A man with a red helmet on a small moped on a dirt road.'
  id: totrans-114
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '##Instruction: 写一个简洁的图像描述，捕捉其主要组成部分、它们之间的关系以及任何显著细节。 ##Question: ##Answer:
    一名戴红色头盔的男子骑在小型摩托车上，行驶在泥土道路上。'
- en: For the COCO dataset, which is for Captions, the Question portion is left blank.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 COCO 数据集，该数据集用于描述，问题部分保持为空。
- en: 'Let’s delve deeper into the processor’s operations. Essentially, it normalizes
    images and tokenizes text. Inputs shorter than *max_length* are also padded. The
    processed data returned by the processor is a dictionary containing:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们*深入探讨*处理器的操作。本质上，它对图像进行归一化并对文本进行分词。短于 *max_length* 的输入也会被填充。处理器返回的数据是一个包含以下内容的字典：
- en: '*input_ids*: An array of tokenized text.'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*input_ids*: 一个分词文本的数组。'
- en: '*attention_mask*: A mask for tokenized text (with Padding being 0).'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*attention_mask*: 用于分词文本的掩码（填充部分为 0）。'
- en: '*pixel_values*: An array of normalized images, also converted to Channel-first.'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*pixel_values*: 归一化图像的数组，也转换为 Channel-first。'
- en: These key names correspond to the arguments for the model’s forward function,
    so they shouldn’t be altered. Finally, *input_ids* aredirectly passed to a key
    named *labels*. The forward function of *GitOPTForCausalLM* calculates the loss
    by predicting the next word shifted by one token.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 这些关键名称对应于模型的前向函数的参数，因此不应更改。最后，*input_ids* 直接传递给名为 *labels* 的关键。*GitOPTForCausalLM*
    的前向函数通过预测下一个词（偏移一个标记）来计算损失。
- en: 'Experiment 1: Determining Fine-tuning Locations'
  id: totrans-121
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实验 1：确定微调位置
- en: In the research papers on GIT models, it was explained that a strong vision
    encoder is utilized and random parameters are adopted for the language model.
    This time, since the goal is to ultimately use a 7B-class language model, a pre-trained
    model will be applied to the language model. The following modules will be examined
    for fine-tuning. The *GIT Projection*, being an initialized module, is always
    included. Some combinations may seem redundant, but they are explored without
    too much concern for this trial.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 在 GIT 模型的研究论文中，解释了使用了强大的视觉编码器，并且语言模型采用了随机参数。这一次，由于目标是最终使用 7B 类语言模型，因此将应用预训练模型。以下模块将用于微调。*GIT
    Projection* 作为一个初始化模块，总是包括在内。一些组合可能看起来冗余，但它们在此试验中被探讨而无需过多担忧。
- en: Modules set for training are given gradients, while the rest are modified to
    not have gradients.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 设置为训练的模块会获得梯度，而其余模块则修改为没有梯度。
- en: '[PRE15]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The Vision Encoder and LLM used for this examination are:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 本次检查所用的 Vision Encoder 和 LLM 是：
- en: openai/clip-vit-base-patch16
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: openai/clip-vit-base-patch16
- en: facebook/opt-350m
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: facebook/opt-350m
- en: Training utilizes COCO dataset and lasts for 5 epochs.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 训练使用 COCO 数据集，持续 5 轮。
- en: 'Here are the target modules trained during each experiment:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是每个实验中训练的目标模块：
- en: '***Proj:*** *GIT Projection. Initialized randomly, so it’s always trained.*'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***Proj:*** *GIT Projection。随机初始化，因此总是进行训练。*'
- en: '***LoRA:*** *Query, Key, and Value of the self attention in the language model
    were applid.*'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***LoRA:*** *语言模型中的自注意力的 Query、Key 和 Value 被应用。*'
- en: '***OPT:*** *All layers were trained.*'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***OPT:*** *所有层都经过训练。*'
- en: '***ViT:*** *All layers were trained.*'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***ViT:*** *所有层都经过训练。*'
- en: '***Head:*** *The final lm_head of OPT was trained.*'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***Head:*** *OPT 的最终 lm_head 已经过训练。*'
- en: '(Note: While LoRA can be applied to ViT, but to avoid making the experiments
    too complicated, it wasn’t included this time.)'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: （注意：虽然 LoRA 可以应用于 ViT，但为了避免使实验过于复杂，这次未包含在内。）
- en: '![](../Images/45eb87ab254a343616727c1da70b4302.png)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/45eb87ab254a343616727c1da70b4302.png)'
- en: This figure shows training loss. Proj, LoRA, OPT, ViT, and Head in the legend
    are the trained modules explained above. (figure made by the author)
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 该图显示了训练损失。图例中的 Proj、LoRA、OPT、ViT 和 Head 是上述训练模块。（图由作者制作）
- en: As shown in the training loss plot, it’s apparent that some groups are not performing
    well. These were the case when OPT is included in the training. Although all experiments
    were conducted under fairly similar conditions, more detailed adjustments, such
    as learning rate, might be necessary when fine-tuning the language model. Results,
    excluding the models where OPT is included in training, will be examined next.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 如训练损失图所示，一些组的表现明显不佳。这些情况发生在 OPT 被包括在训练中时。尽管所有实验在相似的条件下进行，但在微调语言模型时可能需要更详细的调整，如学习率。接下来将检查排除
    OPT 的训练模型的结果。
- en: '![](../Images/74515f8bdfba43737c8cf1dd6be090c1.png)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/74515f8bdfba43737c8cf1dd6be090c1.png)'
- en: This figure shows training loss without full finetuning results. Proj, LoRA,
    OPT, ViT, and Head in the legend are the trained modules explained above. (figure
    made by the author)
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 该图显示了没有完全微调结果的训练损失。图例中的 Proj、LoRA、OPT、ViT 和 Head 是上述训练模块。（图由作者制作）
- en: '![](../Images/1861c8654a743bfba2184aff9fecd01a.png)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1861c8654a743bfba2184aff9fecd01a.png)'
- en: This figure shows validation loss. Proj, LoRA, OPT, ViT, and Head in the legend
    are the trained modules explained above. (figure made by the author)
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 该图显示了验证损失。图例中的 Proj、LoRA、OPT、ViT 和 Head 是上述训练模块。（图由作者制作）
- en: Both training and validation Loss decreased most with the *Projection+LoRA*
    model. Fine-tuning final *Head* layer showed nearly identical outcomes. If ViT
    is also trained, the Loss appears slightly higher and results seem unstable. Even
    when adding LoRA during ViT training, the loss still tends to be high. For fine-tuning
    with this data, it seems using a pre-trained ViT model without updating its parameters
    yields more stable results. The effectiveness of LoRA has been acknowledged in
    various places, and it is evident from this experiment that adding LoRA to the
    LLM improved bothe traininng and validation loss.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 无论是训练还是验证损失，*Projection+LoRA*模型的减少幅度最大。对最终*Head*层进行微调显示出几乎相同的结果。如果 ViT 也被训练，损失值似乎略高，结果也显得不稳定。即使在
    ViT 训练期间添加了 LoRA，损失仍然倾向于较高。对于这个数据的微调，似乎使用一个未更新参数的预训练 ViT 模型会产生更稳定的结果。LoRA 的有效性在多个地方得到了认可，从这个实验中可以明显看出，将
    LoRA 添加到 LLM 中改善了训练和验证损失。
- en: 'Reviewing the inference results on some test data:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 评估一些测试数据的推理结果：
- en: '![](../Images/a675af6f9da2189db8166911ca6bfccf.png)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a675af6f9da2189db8166911ca6bfccf.png)'
- en: Example results of GIT-OPT. Pictures are cited from M3IT dataset, and text results
    were made by the author’s model
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: GIT-OPT 的示例结果。图片引用自 M3IT 数据集，文本结果由作者的模型生成。
- en: When training OPT itself, the results are as poor as the result of loss, making
    the model at a loss for words. Additionally, when training ViT, the output makes
    semantic sense, but describes something entirely different from the given image.
    However, the other results seem to capture the features of the images to some
    extent. For instance, the first image mentions “cat” and “banana”, and the second
    one identifies “traffic sign”. Comparing results with and without LoRA, the latter
    tends to repetitively use similar words, but using LoRA seems to make it slightly
    more natural. Training the *Head* results in intriguing outputs, like using “playing”
    instead of “eating” for the first image. While there are some unnatural elements
    in these results, it can be deduced that the training was successful in capturing
    image features.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 当训练 OPT 本身时，结果与损失结果一样差，使得模型无言以对。此外，训练 ViT 时，输出结果有语义意义，但描述的内容与给定的图像完全不同。然而，其他结果似乎在某种程度上捕捉到了图像的特征。例如，第一张图提到了“猫”和“香蕉”，第二张图识别为“交通标志”。比较有无
    LoRA 的结果，后者倾向于重复使用类似的词汇，但使用 LoRA 似乎使其略微更自然。训练*Head*时得到的输出非常有趣，例如第一张图用“playing”代替“eating”。虽然这些结果中有些元素不自然，但可以推测训练成功捕捉了图像特征。
- en: 'Experiment 2: Comparing Billion-Scale Models'
  id: totrans-148
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实验 2：比较亿级模型
- en: For fine-tuning conditions in earlier experiments, a slightly smaller language
    model, OPT-350m, was used. Now, the intention is to switch the language model
    to a 7B model. Not just settling for OPT, stronger LLMs, LLaMA and MPT, will also
    be introduced.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 对于早期实验中的微调条件，使用了稍小的语言模型OPT-350m。现在的意图是将语言模型切换到7B模型。不仅仅满足于OPT，还将引入更强的LLM，如LLaMA和MPT。
- en: 'Integrating these two models can be done in a similar fashion to OPT. Referring
    to the forward functions of the *LlamaModel* and *MPTModel*, combine the projected
    image vectors with text tokens, and change the mask from *Causal Attention Mask*
    to *GIT’s Attention Mask*. One thing to note: for MPT, the mask isn’t (0, -inf),
    but (False, True). The subsequent processes can be implemented similarly.'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 将这两个模型集成可以按照与OPT类似的方式进行。参考*LlamaModel*和*MPTModel*的前向函数，将投影的图像向量与文本标记结合，并将掩码从*Causal
    Attention Mask*更改为*GIT的Attention Mask*。需要注意的是：对于MPT，掩码不是(0, -inf)，而是(False, True)。随后的过程可以类似地实现。
- en: To use the 7B-class model with OPT, merely change the model name from facebook/opt-350m
    to facebook/opt-6.7b.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用7B级模型与OPT，只需将模型名称从facebook/opt-350m更改为facebook/opt-6.7b。
- en: For LLaMA, with the availability of LLaMA2, that will be the model of choice.
    To use this pre-trained model, approvals from both Meta and Hugging Face are needed.
    An account is necessary for Hugging Face, so make sure to set that up. Approvals
    typically come within a few hours. Afterwards, log into Hugging Face on the terminal
    where training is executed.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 对于LLaMA，考虑到LLaMA2的可用性，它将是首选模型。使用这个预训练模型需要Meta和Hugging Face的批准。需要一个Hugging Face账户，所以确保设置好。批准通常在几小时内完成。之后，登录到执行训练的终端上的Hugging
    Face。
- en: '[PRE16]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: You can log in using the token created in Hugging Face account → Settings →
    Access Token.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用在Hugging Face账户中创建的令牌登录 → 设置 → 访问令牌。
- en: Training parameters remain consistent, using the COCO dataset and lasting for
    3 epochs. Based on results from Experiment 1, the modules set for fine-tuning
    were *Projection + LoRA*.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 训练参数保持一致，使用COCO数据集并持续3个epoch。根据实验1的结果，微调的模块设置为*Projection + LoRA*。
- en: Let’s take a look at the results.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看结果。
- en: '![](../Images/89a4c0d67844b35567a3da3cc56f3f41.png)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/89a4c0d67844b35567a3da3cc56f3f41.png)'
- en: This figure shows training loss (figure made by the author)
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 此图显示了训练损失（图由作者制作）
- en: '![](../Images/4e4594c8d3298607799e0b4d13f9156a.png)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4e4594c8d3298607799e0b4d13f9156a.png)'
- en: This figure shows validation loss (figure made by the author)
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 此图显示了验证损失（图由作者制作）
- en: Reviewing the loss, it’s apparent that the models using LLaMA2 and MPT as LLM
    show a more satisfactory reduction. Let’s also observe the inference results.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 通过查看损失，可以明显看出，使用LLaMA2和MPT作为LLM的模型显示了更令人满意的减少。让我们也观察一下推理结果。
- en: '![](../Images/ad700787927e227b883d10b05f775b25.png)'
  id: totrans-162
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ad700787927e227b883d10b05f775b25.png)'
- en: Example results of GIT-LLMs. Pictures are cited from M3IT dataset, and text
    results were made by the author’s model
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: GIT-LLMs的示例结果。图片引用自M3IT数据集，文本结果由作者的模型生成。
- en: Regarding the first image, for all models, the expressions seem more natural
    compared to OPT-350m. There are no bizarre expressions like “a banana with a banana”,
    highlighting the strength of LLM. For the second image, there’s still some difficulty
    with phrases like “a traffic light” or “a building”. For such complex images,
    there might be a need to consider upgrading the ViT model.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 关于第一张图片，对于所有模型，与OPT-350m相比，表情似乎更自然。没有像“一个香蕉和一个香蕉”这样的奇怪表情，突出了LLM的优势。对于第二张图片，仍然存在像“交通灯”或“建筑物”这样的短语困难。对于这种复杂的图像，可能需要考虑升级ViT模型。
- en: Finally, let’s run inference on images that became popular with GPT-4.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们对在GPT-4中变得流行的图像进行推理。
- en: '![](../Images/f4bac505068c01f309f3c06a5caa8a0c.png)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f4bac505068c01f309f3c06a5caa8a0c.png)'
- en: Example results of GIT-LLMs. A picture is cited from [here](https://www.barnorama.com/wp-content/uploads/2016/12/03-Confusing-Pictures.jpg),
    and text results were made by the author’s models
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: GIT-LLMs的示例结果。图片引用自[这里](https://www.barnorama.com/wp-content/uploads/2016/12/03-Confusing-Pictures.jpg)，文本结果由作者的模型生成。
- en: Although fluent responses were anticipated since LLM is in use, the outcomes
    are quite simple. This might be because the model was trained solely on COCO.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管使用LLM时预期会有流畅的响应，但结果相当简单。这可能是因为模型仅在COCO上进行了训练。
- en: Experiment 3\. Increasing the Data
  id: totrans-169
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实验3：增加数据量
- en: Given the underwhelming results of the previous experiment, it was decided to
    incorporate data other than COCO for training. The M3IT dataset currently in use
    is quite comprehensive, and it can handle a significant amount of data in the
    same format as COCO.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于之前实验的结果不尽如人意，决定在训练中引入COCO以外的数据。当前使用的M3IT数据集相当全面，能够处理与COCO格式相同的大量数据。
- en: '![](../Images/67595c86cfd8ce298c12c80d64cce6aa.png)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/67595c86cfd8ce298c12c80d64cce6aa.png)'
- en: 'This table is cited from Table 3 of “M3IT: A Large-Scale Dataset towards Multi-Modal
    Multilingual Instruction Tuning”'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 该表格引用自“M3IT：面向多模态多语言指令调优的大规模数据集”的表3
- en: It is intended to use data from this source excluding the “Chinese” and “Video”
    categories. Originally, the COCO training dataset contained 566,747 pieces of
    data. By combining it with additional sources, this increased to 1,361,650\. Although
    the size has roughly doubled, the dataset is believed to have become of higher
    quality due to the increased diversity of tasks.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 打算使用来自该来源的数据，但排除“中文”和“视频”类别。最初，COCO训练数据集包含566,747条数据。通过与其他来源结合，总数增加到1,361,650。尽管规模大致翻倍，但由于任务多样性增加，数据集的质量被认为有所提高。
- en: Handling multiple Pytorch datasets can be easily achieved using the *ConcatDataset*.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 使用*ConcatDataset*可以轻松处理多个Pytorch数据集。
- en: '[PRE17]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: The training was conducted for 1 epoch, and the LLaMA2 model was used for fine-tuning
    the *Projection and LoRA*, similarly to Experiment 2.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 训练进行了1轮，并使用LLaMA2模型对*Projection和LoRA*进行了微调，与实验2类似。
- en: As there’s no loss to compare to this time, let’s dive straight into the inference
    results.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这次没有可以比较的损失值，我们直接进入推理结果。
- en: '![](../Images/121d2e67860d5e31dd3621bd2155961e.png)'
  id: totrans-178
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/121d2e67860d5e31dd3621bd2155961e.png)'
- en: Example results of GIT-LLaMA2\. Pictures are cited from M3IT dataset, and text
    results were made by the author’s model
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: GIT-LLaMA2的示例结果。图片来自M3IT数据集，文本结果由作者的模型生成
- en: '![](../Images/2a3ec608bceb6afd8eca13d9590fcf4c.png)'
  id: totrans-180
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2a3ec608bceb6afd8eca13d9590fcf4c.png)'
- en: Example results of GIT-LLaMA2\. Pictures are cited from M3IT dataset, and text
    results were made by the author’s model
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: GIT-LLaMA2的示例结果。图片来自M3IT数据集，文本结果由作者的模型生成
- en: '![](../Images/37f3b927e5e64e8ff16c49504461c639.png)'
  id: totrans-182
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/37f3b927e5e64e8ff16c49504461c639.png)'
- en: Example results of GIT-LLaMA2\. Pictures are cited from M3IT dataset, and text
    results were made by the author’s model
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: GIT-LLaMA2的示例结果。图片来自M3IT数据集，文本结果由作者的模型生成
- en: Along with solving simple problems, the model now handles more complex challenges.
    By adding datasets for tasks more intricate than just captioning, the capabilities
    have expanded significantly. Achieving this level of accuracy with only 1 epoch
    of training was surprising.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 除了解决简单问题外，模型现在还处理更复杂的挑战。通过添加比仅仅是描述更复杂的任务数据集，能力显著扩展。仅用1轮训练就达到这样的准确性令人惊讶。
- en: Let’s test it with the following example image. Given the increased variety
    in the dataset, the way the questions were presented was slightly modified.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们用以下示例图像进行测试。鉴于数据集的多样性增加，问题的呈现方式略有修改。
- en: '![](../Images/f06c8e160a3d026ec3efe7f0224d4be1.png)'
  id: totrans-186
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f06c8e160a3d026ec3efe7f0224d4be1.png)'
- en: Example results of GIT-LLaMA2\. A picture is cited from [here](https://www.barnorama.com/wp-content/uploads/2016/12/03-Confusing-Pictures.jpg),
    and text results were made by the author’s models
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: GIT-LLaMA2的示例结果。一张图片来自[这里](https://www.barnorama.com/wp-content/uploads/2016/12/03-Confusing-Pictures.jpg)，文本结果由作者的模型生成
- en: While the description being “Umbrella” was still wired, it feels like it’s getting
    better. To improve further, there’s a need to increase the number of training
    epochs, add more types or volumes of datasets, and leverage more powerful ViT
    or LLM. Nonetheless, it’s impressive that such a model could be developed in just
    half a day given the computational and data resources.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管“伞状”这一描述仍有些奇怪，但感觉越来越好。为了进一步改进，需要增加训练轮次，添加更多类型或量的数据集，并利用更强大的ViT或LLM。尽管如此，能够在仅半天内开发出这样的模型，考虑到计算和数据资源，确实令人印象深刻。
- en: Bonus Experiment. *Did the Image Turn into Words?*
  id: totrans-189
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 奖励实验。*图像变成文字了吗？*
- en: Let’s take another look at the GIT structure.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 再看一下GIT结构。
- en: '![](../Images/4e2f4f77acb65e4c8a7772a1254074fb.png)'
  id: totrans-191
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4e2f4f77acb65e4c8a7772a1254074fb.png)'
- en: Simplified model architecture of GIT-LLM (image made by the author)
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: GIT-LLM的简化模型架构（图像由作者制作）
- en: As shown in the figure, after feature extraction by the vision encoder, images
    are treated on par with vectorized text through *Visual Projection*. In other
    words, it’s possible that the *Visual Projection* is converting image vectors
    into text vectors. An investigation was conducted to see what the vectors look
    like after the*Visual Projection*.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 如图所示，在视觉编码器进行特征提取后，图像通过*Visual Projection*与向量化的文本平等对待。换句话说，*Visual Projection*
    可能将图像向量转换为文本向量。进行了调查以查看*Visual Projection*之后的向量是什么样的。
- en: While there’s an option to use the *Head* to revert the vector post-projection
    back to text, it was found that even vectors that were vectorized using the *Embedding*
    module could not be reverted back to their original text using this method. Therefore,
    vectors that closely resemble the text vectors before being input into the LLM
    should be assigned as the corresponding word. All tokens registered in the tokenizer
    were vectorized using the Embedding module, and the one with the highest cosine
    similarity was identified as the target word.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然有使用*Head*将投影后的向量还原为文本的选项，但发现即使是使用*Embedding*模块向量化的向量也无法通过这种方法还原为原始文本。因此，应将与输入到
    LLM 之前的文本向量最接近的向量分配为相应的单词。所有在分词器中注册的令牌都使用 Embedding 模块进行向量化，并且选择了余弦相似度最高的词作为目标词。
- en: The image used for this experiment is that of a cat.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 本实验使用的图像是一只猫。
- en: '![](../Images/4b8b2f288c78ac4a91884cca6859233e.png)'
  id: totrans-196
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4b8b2f288c78ac4a91884cca6859233e.png)'
- en: A Picture is cited from M3IT dataset.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 图片摘自 M3IT 数据集。
- en: Now, let’s proceed with the analysis (whole analysis is available [here](https://github.com/Ino-Ichan/GIT-LLM/blob/main/notebooks/show_visual_words.ipynb)).
    First, all the registered tokens are vectorized.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们进行分析（完整分析可在[这里](https://github.com/Ino-Ichan/GIT-LLM/blob/main/notebooks/show_visual_words.ipynb)查看）。首先，对所有注册的令牌进行向量化。
- en: '[PRE18]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Next, the image vectors that would have been converted to words by the ViT and
    *Projection* are extracted.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，将提取本来会被 ViT 和 *Projection* 转换为单词的图像向量。
- en: '[PRE19]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: The dot products of these vectors and the word vectors were calculated, and
    the results with the maximum value were decoded as the relevant token ID.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 计算了这些向量和单词向量的点积，最大值的结果被解码为相关的令牌 ID。
- en: '[PRE20]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: As shown in the printed *decoded_text*, some unfamiliar words have appeared.
    As some words are repeated, they were counted.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 如打印出的*decoded_text*所示，一些不熟悉的单词出现了。由于一些单词重复出现，它们被统计了。
- en: '[PRE21]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: A large number of unfamiliar words seem to have appeared. Depending on the position,
    they might convey meaningful information. Let’s plot the words against the cat
    image.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 似乎出现了大量不熟悉的单词。根据位置，它们可能传达有意义的信息。让我们将这些单词绘制在猫的图像上。
- en: '[PRE22]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '![](../Images/e3abd839ef5d0ca7bef4cfd2ad1ebfc4.png)'
  id: totrans-208
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e3abd839ef5d0ca7bef4cfd2ad1ebfc4.png)'
- en: Image made by the author
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者制作
- en: Words that frequently appear are color-coded. The result seems to suggest that
    they are not simply being projected onto meaningful words. While the word “***Cat***”
    might be superimposed on the cat image, giving it some relevance, its meaning
    remains unclear.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 经常出现的单词用颜色编码。结果似乎表明它们并不仅仅是投射到有意义的单词上。虽然“***Cat***”这个词可能被叠加在猫的图像上，赋予它一定的相关性，但其含义仍不明确。
- en: The inconclusive results in this experiment might be due to forcibly selecting
    a word with a high cosine similarity. At any rate, the approach doesn’t involve
    simply casting words and creating image prompts. Vectors extracted from images
    are converted by *Visual Projection* into vectors in token space, which seem to
    hold some resemblance in meaning, functioning as mysterious prompts. It might
    be best not to delve any deeper into this.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 该实验中不确定的结果可能是由于强行选择了一个余弦相似度高的单词。无论如何，这种方法并不是简单地将单词投射到图像提示上。从图像中提取的向量通过*Visual
    Projection* 转换为令牌空间中的向量，这些向量似乎在意义上有些相似，充当神秘的提示。可能最好不要深入探讨这一点。
- en: Conclusion
  id: totrans-212
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: In this tech blog post, I introduced the method of integrating LLMs into the
    vision language model, GIT. Furthermore, various experiments were conducted using
    the developed models. While there were successes and failures, I would like to
    continue conducting experiments with vision language models to accumulate insights.
    Please consider this article as a reference and feel encouraged to create your
    own vision language models and explore its potential.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇技术博客文章中，我介绍了将 LLM 集成到视觉语言模型 GIT 的方法。此外，还使用开发的模型进行了各种实验。虽然有成功也有失败，但我希望继续进行视觉语言模型的实验，以积累见解。请将本文作为参考，并鼓励你创建自己的视觉语言模型，探索其潜力。
- en: '![](../Images/c0d65bb574d89d207d2ce05bd90c5dad.png)'
  id: totrans-214
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c0d65bb574d89d207d2ce05bd90c5dad.png)'
- en: This is an illustrated image of GIT-LLM created using Stable Diffusion. (Image
    made by the author)
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一张使用 Stable Diffusion 创建的 GIT-LLM 插图。（图片由作者制作）
