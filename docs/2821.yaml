- en: Build and Play! Your Own V&L Model Equipped with LLM!
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/build-and-play-your-own-v-l-model-equipped-with-llm-afa773b9249e?source=collection_archive---------6-----------------------#2023-09-07](https://towardsdatascience.com/build-and-play-your-own-v-l-model-equipped-with-llm-afa773b9249e?source=collection_archive---------6-----------------------#2023-09-07)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Developing LLM-integrated GIT vision language models.
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@inoichan?source=post_page-----afa773b9249e--------------------------------)[![Yuichi
    Inoue](../Images/d25793aea6ddcdb90ecb21be5031a434.png)](https://medium.com/@inoichan?source=post_page-----afa773b9249e--------------------------------)[](https://towardsdatascience.com/?source=post_page-----afa773b9249e--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----afa773b9249e--------------------------------)
    [Yuichi Inoue](https://medium.com/@inoichan?source=post_page-----afa773b9249e--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ff3eff720c79a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuild-and-play-your-own-v-l-model-equipped-with-llm-afa773b9249e&user=Yuichi+Inoue&userId=f3eff720c79a&source=post_page-f3eff720c79a----afa773b9249e---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----afa773b9249e--------------------------------)
    ·21 min read·Sep 7, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fafa773b9249e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuild-and-play-your-own-v-l-model-equipped-with-llm-afa773b9249e&user=Yuichi+Inoue&userId=f3eff720c79a&source=-----afa773b9249e---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fafa773b9249e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuild-and-play-your-own-v-l-model-equipped-with-llm-afa773b9249e&source=-----afa773b9249e---------------------bookmark_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Summary of this article:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Explaining GIT, a Vision Language Model developed by Microsoft.*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Replacing GIT’s language model with large language models (LLMs) using PyTorch
    and Hugging Face’s Transformers.*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Introducing how to fine-tune GIT-LLM models using LoRA.*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Testing and discussing the developed models.*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Investigating if “Image Embeddings” embedded by the Image Encoder of GIT indicate
    specific characters in the same space as “Text Embedding”.*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Large language models (LLM) are showing their value more and more. Incorporating
    images into LLMs makes them even more useful as vision language models. In this
    article, I will explain the development of a model called GIT-LLM, a simple but
    powerful vision language model. Some parts, like the code explanations, might
    feel a bit tedious, so feel free to jump straight to the results section. I conducted
    various experiments and analyses, so I think you’ll enjoy seeing what I was able
    to achieve.
  prefs: []
  type: TYPE_NORMAL
- en: The implementation is available publicly, so please give it a try.
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://github.com/turingmotors/heron?source=post_page-----afa773b9249e--------------------------------)
    [## GitHub - turingmotors/heron'
  prefs: []
  type: TYPE_NORMAL
- en: Contribute to turingmotors/heron development by creating an account on GitHub.
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: github.com](https://github.com/turingmotors/heron?source=post_page-----afa773b9249e--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: Transforming GIT into LLM
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let’s dive into the main topic of this tech blog.
  prefs: []
  type: TYPE_NORMAL
- en: What is GIT?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Generative Image-to-text Transformer, or GIT, is a vision language model proposed
    by Microsoft.
  prefs: []
  type: TYPE_NORMAL
- en: 'arXiv: [https://arxiv.org/abs/2205.14100](https://arxiv.org/abs/2205.14100)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Code: [https://github.com/microsoft/GenerativeImage2Text](https://github.com/microsoft/GenerativeImage2Text)'
  prefs: []
  type: TYPE_NORMAL
- en: Its architecture is quite simple. It converts feature vectors extracted from
    an image encoder into vectors that can be treated like text using a projection
    module. These vectors are then input into a language model to produce captions
    for images or to perform Q&A. The model can handle videos in a similar way.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e26bc6f23c908cb230eb975a8c6eaac7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This figure is cited from [“GIT: A Generative Image-to-text Transformer for
    Vision and Language”](https://arxiv.org/abs/2205.14100)'
  prefs: []
  type: TYPE_NORMAL
- en: Despite its simplicity, if you look at the Leaderboard on “Paper with code”,
    you’ll find that it ranks highly in many tasks.
  prefs: []
  type: TYPE_NORMAL
- en: '[https://paperswithcode.com/paper/git-a-generative-image-to-text-transformer](https://paperswithcode.com/paper/git-a-generative-image-to-text-transformer)'
  prefs: []
  type: TYPE_NORMAL
- en: Originally, GIT uses strong models like CLIP for its image encoder and trains
    the language model part from scratch. However, in this article, I try to use a
    powerful LLM and fine-tune it. Here, I call the model “GIT-LLM”.
  prefs: []
  type: TYPE_NORMAL
- en: Using a LLM with Hugging Face’s Transformers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: I’ll use Hugging Face’s [Transformers](https://huggingface.co/docs/transformers/index)
    library for developping GIT-LLM. Transformers is a Python library for handling
    machine learning models. It offers many state-of-the-art pre-trained models that
    you can immediately run inference on. It also provides tools for training and
    fine-tuning models. I believe that Transformers has contributed significantly
    to the development of recent LLM derivatives. Almost all available LLMs can be
    handled with Transformers, and many multi-modal models derived from them use Transformers
    as their base for development and fine-tuning.
  prefs: []
  type: TYPE_NORMAL
- en: Here is the simplest code for using a model of Transformers. You can find it
    easy to try LLMs by useing *AutoModel* and *AutoTokenizer*.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Let’s check out the parameters the OPT model holds. Printing a model created
    by *AutoModelForCausalLM*.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'It’s quite simple. The input dimension of the initial *embed_tokens* and the
    output dimension of the final *lm_head* is 50,272, which represents the number
    of tokens used in training this model. Let’s verify the size of the tokenizer’s
    vocabulary:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Including special tokens like *bos_token*, *eos_token*, *unk_token*, *sep_token*,
    *pad_token*, *cls_token*, and *mask_token*, it predicts the probability of the
    next word from a total of 50,272 types of tokens.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can understand how these models are connected by looking at [the implementation](https://github.com/huggingface/transformers/blob/v4.30.0/src/transformers/models/opt/modeling_opt.py).
    A simple diagram would represent the flow as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5352c8349ab3b070c14a69644b854f65.png)'
  prefs: []
  type: TYPE_IMG
- en: Simplified model architecture of OPT (image made by the author)
  prefs: []
  type: TYPE_NORMAL
- en: The structure and data flow are quite simple. The 〇〇Model and 〇〇ForCausalLM
    have a similar framework across different language models. The 〇〇Model class mainly
    represents the “Transformer” part of the language model. If, for instance, you
    want to perform tasks like text classification, you’d use only this part. The
    〇〇ForCausalLM class is for text generation, applying a classifier for token count
    to the vectors after processing them with the Transformer. The calculation of
    the loss is also done within the forward method of this class. The *embed_positions*
    denotes positional encoding, which is added to *project_in*.
  prefs: []
  type: TYPE_NORMAL
- en: Using GIT with Transformers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: I’ll give it a try based on [the official documentation page](https://huggingface.co/docs/transformers/model_doc/git)
    of GIT. As I’ll be processing images as well, I’ll use a Processor that also includes
    a Tokenizer.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Given that [the input image](http://images.cocodataset.org/val2017/000000039769.jpg)
    produces the output “two cats sleeping on a couch”, it seems to be working well.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s also take a look at the model’s structure:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Although it’s a bit lengthy, if you break it down, it’s also quite simple.
    Within GitForCausalLM, there is a GitModel and within that, there are the following
    modules:'
  prefs: []
  type: TYPE_NORMAL
- en: embeddings (GitEmbeddings)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: image_encoder (GitVisionModel)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: encoder (GitEncoder)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: visual_projection (GitProjection)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: output (Linear)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The major difference from OPT is the presence of *GitVisionModel* and *GitProjection*,
    which are the exact modules that convert images into prompt-like vectors. While
    the language model uses a Decoder for OPT and an Encoder for GIT, this only signifies
    a difference in how the attention mask is constructed. There may be slight differences
    in the transformer layer, but their functions are essentially the same. GIT uses
    the name Encoder because it uses a unique attention mask that applies attention
    to all features of the image and uses a causal mask for text features.
  prefs: []
  type: TYPE_NORMAL
- en: Looking at the model’s connections;
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8b563994c34c6404fe9b5c7dcbd6d595.png)'
  prefs: []
  type: TYPE_IMG
- en: Simplified model architecture of GIT ( image made by the author)
  prefs: []
  type: TYPE_NORMAL
- en: The image information is treated by the *GitVisionModel* and *GitProjection*
    to match the text’s embeddings. After that, it’s inputted alongside the text’s
    embeddings into the language model’s “Transformer” layers. While there are subtle
    differences, the part related to the language model is almost developed the same
    way.
  prefs: []
  type: TYPE_NORMAL
- en: GIT’s Attention Mask
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The architectures for the usual language model and the GIT language model are
    almost the same, but the ways of applying attention masks aredifferent.
  prefs: []
  type: TYPE_NORMAL
- en: For the language model, an attention mask is applied not to look at past tokens
    when predicting future tokens. This is a method called “Causal Attention”, which
    corresponds to the left side of the following figure. The first column token references
    only itself, ensuring no self attention is applied to subsequent words. The second
    column applies self attention up to the second word, with the third word onwards
    becoming 0\. Such masking enables it to be trained to predict the next word effectively.
  prefs: []
  type: TYPE_NORMAL
- en: 'GIT input has two types of tokens: image tokens and text tokens. Since all
    image tokens are used simultaneously and aren’t used to predict the next token,
    Causal Attention isn’t suitable. On the other hand, Causal Attention is still
    necesarry for text tokens. A mask like the one on the right side of the figure
    is designed to achieve this. For the top three rows of image information, self
    attention is applied with all token information. From the text tokens, moving
    down one column increases the number of words that can be referenced.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2274c8b7bd321af9b8894f9c179b5b3d.png)'
  prefs: []
  type: TYPE_IMG
- en: Difference between the causal attention mask and the Git attention mask (image
    made by the author)
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s also check the code for making a GIT mask. The snippet to create the
    GIT mask is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: You add the mask to attention weights. Thus, parts where self attention takes
    place are 0, and parts that aren’t included in the attention are -inf. By providing
    this mask forward, only the text part can do causal attention. It’s important
    for vision language models to create and use masks effectively like this.
  prefs: []
  type: TYPE_NORMAL
- en: Connecting GIT and OPT
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now, let’s connect GIT and OPT. The goal is to create a model as shown in the
    figure.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/15317072b84b09432c4e5fb70e7acb6a.png)'
  prefs: []
  type: TYPE_IMG
- en: Simplified model architecture of GIT-OPT (image made by the author)
  prefs: []
  type: TYPE_NORMAL
- en: For the general implementation, you can refer to the `[modeling_git.py](https://github.com/huggingface/transformers/blob/main/src/transformers/models/git/modeling_git.py)`.
  prefs: []
  type: TYPE_NORMAL
- en: The most important part is the *GitOPTModel*. Inside this, a vision encoder
    need to be connected with a LLM. I’ll explain some key components.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Inside the *__init__* function, various modules are instantiated. The *super*
    initializes the *OPTModel*. In GIT, it is recommended to use a powerful image
    encoder trained with CLIP, so I have made it compatible with the ViT trained with
    CLIP. The *GitProjection* is taken from the original GIT implementation.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s look inside the forward function. The implementation is based on the forward
    part of the *OPTDecoder*, with added information from the image encoder. Although
    it’s a bit lengthy, I’ve added comments in the code, so please follow each step.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Although it might look complicated, if you go through each step, you’ll see
    that it follows the flow illustrated in the diagram. The actual code may look
    a bit more complex, but grasping the main process first will make understanding
    the other parts easier. This is pseudocode, so for detailed parts, please refer
    to [the published implementation](https://github.com/Ino-Ichan/GIT-LLM/blob/main/git_llm/git_opt/modeling_git_opt.py#L178).
  prefs: []
  type: TYPE_NORMAL
- en: Finally, let’s take a brief look at the *GITOPTForCausalLM* part.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: The processing inside the model is simple. When *labels* are provided, i.e.,
    during training, the loss calculation is also performed within the forward. In
    *shifted_logits*, tokens were fetched from the first token to the second-to-last
    token of the text tokens. It then calculates the Cross Entropy Loss with the *labels*
    shifted by one word as the correct answer.
  prefs: []
  type: TYPE_NORMAL
- en: One thing to note is to name the variable that assigns the *GitOPTModel* in
    the initialization function as *self.model*. If you check [the implementation
    of the parent class *OPTForCausalLM*](https://github.com/huggingface/transformers/blob/v4.31.0/src/transformers/models/opt/modeling_opt.py#L823),
    you'll see that OPT is first placed to *self.model* during the *super* initialization.
    If you change this instance variable name, you will end up holding two OPTs, which
    can strain the memory.
  prefs: []
  type: TYPE_NORMAL
- en: LoRA Extension
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In order to fine-tune the LLM effectively, I’ll use a library called Parameter-Efficient
    Fine-Tuning ([PEFT](https://github.com/huggingface/peft)). As it’s developed by
    Hugging Face, it integrates seamlessly with Transfors. While there are various
    methods within PEFT, this time I’m going to do some experiments using a commonly
    seen approach called Low-rank adaptation (LoRA).
  prefs: []
  type: TYPE_NORMAL
- en: Models can be applied LoRA in just a few lines if the models support PEFT.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: The *target_modules* argument specifies which modules you want to convert to
    LoRA. If a list is provided as *target_modules*, it is implemented to convert
    to LoRA for modules that end with each of the strings. LoRA is applied only to
    “value” (*v_proj*) of the self attention module for simplicity.
  prefs: []
  type: TYPE_NORMAL
- en: In the model, ViT is used for the image encoder part. Be cautious, as specifying
    it like this, self attention part of ViT might also be applied LoRA. It’s a bit
    tedious, but by specifying down to the part where the key names don’t overlap
    and giving it to *target_modules*, you can avoid this.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: The resulting model becomes an instance of the *PeftModelForCausalLM* class.
    It has an instance variable named *base_model* that holds the original model converted
    to LoRA. As an example, I show that LoRA is applied to *v_proj* of the self attention
    in ViT.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Inside the *v_proj* Linear, you'll find added fully connected layers such as
    *lora_A* and *lora_B*. The LoRA-converted *Linear* module is a namesake Linear
    class that inherits from PyTorch's *Linear* and *LoraLayer*. It's a somewhat unique
    module, so those curious about the details should take a look at [the implementation](https://github.com/huggingface/peft/blob/main/src/peft/tuners/lora/layer.py#L153).
  prefs: []
  type: TYPE_NORMAL
- en: Note that models created with PEFT will not save anything other than the LoRA
    part by default. While there’s a method to save using the *merge_and_unload* method,
    you might want to save all models being saved midway during training with Trainer.
    Overloading the Trainer's *_save_checkpoints* method is one approach, but to avoid
    the hassle, I handled it this time by fetching just the original model part held
    inside the *PeftModel* during the training phase.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: I believe there are more efficient ways to handle this, so I’m still researching.
  prefs: []
  type: TYPE_NORMAL
- en: '**Experimenting with GIT-LLM**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let’s now conduct some experiments using the model developed so far.
  prefs: []
  type: TYPE_NORMAL
- en: For details on the training configuration and other setups, please refer to
    [the published implementation](https://github.com/Ino-Ichan/GIT-LLM), as they
    essentially follow the same method.
  prefs: []
  type: TYPE_NORMAL
- en: 'Dataset: M3IT'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For experiments, I wanted to use a dataset that pairs images with text and is
    easy to integrate. While exploring [the Hugging face’s *Datasets*](https://huggingface.co/datasets),
    I came across M3IT, a multimodal dataset for Instruction Tuning developed by the
    Shanghai AI Lab. Instruction Tuning is a method that yields impressive results
    even with a limited amount of data. It appears that M3IT has re-annotated various
    existing datasets specifically for Instruction Tuning.
  prefs: []
  type: TYPE_NORMAL
- en: '[https://huggingface.co/datasets/MMInstruction/M3IT](https://huggingface.co/datasets/MMInstruction/M3IT)'
  prefs: []
  type: TYPE_NORMAL
- en: This dataset is easy to use, so I’ve decided to utilize it for the following
    experiments.
  prefs: []
  type: TYPE_NORMAL
- en: To train using M3IT, it’s necessary to create a custom Pytorch Dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: In the *__init__* function, the image_processor and tokenizer correspond to
    their respective models. The *loaded_dataset* argument passed should be from MMInstruction/M3IT
    datasets.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: For the COCO Instruction Tuning dataset, the split between training, validation,
    and testing is identical to the original dataset, with 566,747, 25,010, and 25,010
    image-text pairs respectively. Other datasets, such as VQA or Video, can also
    be handled similarly, making it a versatile dataset for validation purposes.
  prefs: []
  type: TYPE_NORMAL
- en: 'A sample data looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/357b932eebeffa2ad69452bccfd6b0da.png)'
  prefs: []
  type: TYPE_IMG
- en: Image is cited from data in M3IT.
  prefs: []
  type: TYPE_NORMAL
- en: 'The caption for this picture is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '##Instruction: Write a succinct description of the image, capturing its main
    components, the relationships between them, and any notable details. ##Question:
    ##Answer: A man with a red helmet on a small moped on a dirt road.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: For the COCO dataset, which is for Captions, the Question portion is left blank.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s delve deeper into the processor’s operations. Essentially, it normalizes
    images and tokenizes text. Inputs shorter than *max_length* are also padded. The
    processed data returned by the processor is a dictionary containing:'
  prefs: []
  type: TYPE_NORMAL
- en: '*input_ids*: An array of tokenized text.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*attention_mask*: A mask for tokenized text (with Padding being 0).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*pixel_values*: An array of normalized images, also converted to Channel-first.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These key names correspond to the arguments for the model’s forward function,
    so they shouldn’t be altered. Finally, *input_ids* aredirectly passed to a key
    named *labels*. The forward function of *GitOPTForCausalLM* calculates the loss
    by predicting the next word shifted by one token.
  prefs: []
  type: TYPE_NORMAL
- en: 'Experiment 1: Determining Fine-tuning Locations'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the research papers on GIT models, it was explained that a strong vision
    encoder is utilized and random parameters are adopted for the language model.
    This time, since the goal is to ultimately use a 7B-class language model, a pre-trained
    model will be applied to the language model. The following modules will be examined
    for fine-tuning. The *GIT Projection*, being an initialized module, is always
    included. Some combinations may seem redundant, but they are explored without
    too much concern for this trial.
  prefs: []
  type: TYPE_NORMAL
- en: Modules set for training are given gradients, while the rest are modified to
    not have gradients.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'The Vision Encoder and LLM used for this examination are:'
  prefs: []
  type: TYPE_NORMAL
- en: openai/clip-vit-base-patch16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: facebook/opt-350m
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training utilizes COCO dataset and lasts for 5 epochs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are the target modules trained during each experiment:'
  prefs: []
  type: TYPE_NORMAL
- en: '***Proj:*** *GIT Projection. Initialized randomly, so it’s always trained.*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '***LoRA:*** *Query, Key, and Value of the self attention in the language model
    were applid.*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '***OPT:*** *All layers were trained.*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '***ViT:*** *All layers were trained.*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '***Head:*** *The final lm_head of OPT was trained.*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(Note: While LoRA can be applied to ViT, but to avoid making the experiments
    too complicated, it wasn’t included this time.)'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/45eb87ab254a343616727c1da70b4302.png)'
  prefs: []
  type: TYPE_IMG
- en: This figure shows training loss. Proj, LoRA, OPT, ViT, and Head in the legend
    are the trained modules explained above. (figure made by the author)
  prefs: []
  type: TYPE_NORMAL
- en: As shown in the training loss plot, it’s apparent that some groups are not performing
    well. These were the case when OPT is included in the training. Although all experiments
    were conducted under fairly similar conditions, more detailed adjustments, such
    as learning rate, might be necessary when fine-tuning the language model. Results,
    excluding the models where OPT is included in training, will be examined next.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/74515f8bdfba43737c8cf1dd6be090c1.png)'
  prefs: []
  type: TYPE_IMG
- en: This figure shows training loss without full finetuning results. Proj, LoRA,
    OPT, ViT, and Head in the legend are the trained modules explained above. (figure
    made by the author)
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1861c8654a743bfba2184aff9fecd01a.png)'
  prefs: []
  type: TYPE_IMG
- en: This figure shows validation loss. Proj, LoRA, OPT, ViT, and Head in the legend
    are the trained modules explained above. (figure made by the author)
  prefs: []
  type: TYPE_NORMAL
- en: Both training and validation Loss decreased most with the *Projection+LoRA*
    model. Fine-tuning final *Head* layer showed nearly identical outcomes. If ViT
    is also trained, the Loss appears slightly higher and results seem unstable. Even
    when adding LoRA during ViT training, the loss still tends to be high. For fine-tuning
    with this data, it seems using a pre-trained ViT model without updating its parameters
    yields more stable results. The effectiveness of LoRA has been acknowledged in
    various places, and it is evident from this experiment that adding LoRA to the
    LLM improved bothe traininng and validation loss.
  prefs: []
  type: TYPE_NORMAL
- en: 'Reviewing the inference results on some test data:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a675af6f9da2189db8166911ca6bfccf.png)'
  prefs: []
  type: TYPE_IMG
- en: Example results of GIT-OPT. Pictures are cited from M3IT dataset, and text results
    were made by the author’s model
  prefs: []
  type: TYPE_NORMAL
- en: When training OPT itself, the results are as poor as the result of loss, making
    the model at a loss for words. Additionally, when training ViT, the output makes
    semantic sense, but describes something entirely different from the given image.
    However, the other results seem to capture the features of the images to some
    extent. For instance, the first image mentions “cat” and “banana”, and the second
    one identifies “traffic sign”. Comparing results with and without LoRA, the latter
    tends to repetitively use similar words, but using LoRA seems to make it slightly
    more natural. Training the *Head* results in intriguing outputs, like using “playing”
    instead of “eating” for the first image. While there are some unnatural elements
    in these results, it can be deduced that the training was successful in capturing
    image features.
  prefs: []
  type: TYPE_NORMAL
- en: 'Experiment 2: Comparing Billion-Scale Models'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For fine-tuning conditions in earlier experiments, a slightly smaller language
    model, OPT-350m, was used. Now, the intention is to switch the language model
    to a 7B model. Not just settling for OPT, stronger LLMs, LLaMA and MPT, will also
    be introduced.
  prefs: []
  type: TYPE_NORMAL
- en: 'Integrating these two models can be done in a similar fashion to OPT. Referring
    to the forward functions of the *LlamaModel* and *MPTModel*, combine the projected
    image vectors with text tokens, and change the mask from *Causal Attention Mask*
    to *GIT’s Attention Mask*. One thing to note: for MPT, the mask isn’t (0, -inf),
    but (False, True). The subsequent processes can be implemented similarly.'
  prefs: []
  type: TYPE_NORMAL
- en: To use the 7B-class model with OPT, merely change the model name from facebook/opt-350m
    to facebook/opt-6.7b.
  prefs: []
  type: TYPE_NORMAL
- en: For LLaMA, with the availability of LLaMA2, that will be the model of choice.
    To use this pre-trained model, approvals from both Meta and Hugging Face are needed.
    An account is necessary for Hugging Face, so make sure to set that up. Approvals
    typically come within a few hours. Afterwards, log into Hugging Face on the terminal
    where training is executed.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: You can log in using the token created in Hugging Face account → Settings →
    Access Token.
  prefs: []
  type: TYPE_NORMAL
- en: Training parameters remain consistent, using the COCO dataset and lasting for
    3 epochs. Based on results from Experiment 1, the modules set for fine-tuning
    were *Projection + LoRA*.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s take a look at the results.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/89a4c0d67844b35567a3da3cc56f3f41.png)'
  prefs: []
  type: TYPE_IMG
- en: This figure shows training loss (figure made by the author)
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4e4594c8d3298607799e0b4d13f9156a.png)'
  prefs: []
  type: TYPE_IMG
- en: This figure shows validation loss (figure made by the author)
  prefs: []
  type: TYPE_NORMAL
- en: Reviewing the loss, it’s apparent that the models using LLaMA2 and MPT as LLM
    show a more satisfactory reduction. Let’s also observe the inference results.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ad700787927e227b883d10b05f775b25.png)'
  prefs: []
  type: TYPE_IMG
- en: Example results of GIT-LLMs. Pictures are cited from M3IT dataset, and text
    results were made by the author’s model
  prefs: []
  type: TYPE_NORMAL
- en: Regarding the first image, for all models, the expressions seem more natural
    compared to OPT-350m. There are no bizarre expressions like “a banana with a banana”,
    highlighting the strength of LLM. For the second image, there’s still some difficulty
    with phrases like “a traffic light” or “a building”. For such complex images,
    there might be a need to consider upgrading the ViT model.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, let’s run inference on images that became popular with GPT-4.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f4bac505068c01f309f3c06a5caa8a0c.png)'
  prefs: []
  type: TYPE_IMG
- en: Example results of GIT-LLMs. A picture is cited from [here](https://www.barnorama.com/wp-content/uploads/2016/12/03-Confusing-Pictures.jpg),
    and text results were made by the author’s models
  prefs: []
  type: TYPE_NORMAL
- en: Although fluent responses were anticipated since LLM is in use, the outcomes
    are quite simple. This might be because the model was trained solely on COCO.
  prefs: []
  type: TYPE_NORMAL
- en: Experiment 3\. Increasing the Data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Given the underwhelming results of the previous experiment, it was decided to
    incorporate data other than COCO for training. The M3IT dataset currently in use
    is quite comprehensive, and it can handle a significant amount of data in the
    same format as COCO.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/67595c86cfd8ce298c12c80d64cce6aa.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This table is cited from Table 3 of “M3IT: A Large-Scale Dataset towards Multi-Modal
    Multilingual Instruction Tuning”'
  prefs: []
  type: TYPE_NORMAL
- en: It is intended to use data from this source excluding the “Chinese” and “Video”
    categories. Originally, the COCO training dataset contained 566,747 pieces of
    data. By combining it with additional sources, this increased to 1,361,650\. Although
    the size has roughly doubled, the dataset is believed to have become of higher
    quality due to the increased diversity of tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Handling multiple Pytorch datasets can be easily achieved using the *ConcatDataset*.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: The training was conducted for 1 epoch, and the LLaMA2 model was used for fine-tuning
    the *Projection and LoRA*, similarly to Experiment 2.
  prefs: []
  type: TYPE_NORMAL
- en: As there’s no loss to compare to this time, let’s dive straight into the inference
    results.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/121d2e67860d5e31dd3621bd2155961e.png)'
  prefs: []
  type: TYPE_IMG
- en: Example results of GIT-LLaMA2\. Pictures are cited from M3IT dataset, and text
    results were made by the author’s model
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2a3ec608bceb6afd8eca13d9590fcf4c.png)'
  prefs: []
  type: TYPE_IMG
- en: Example results of GIT-LLaMA2\. Pictures are cited from M3IT dataset, and text
    results were made by the author’s model
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/37f3b927e5e64e8ff16c49504461c639.png)'
  prefs: []
  type: TYPE_IMG
- en: Example results of GIT-LLaMA2\. Pictures are cited from M3IT dataset, and text
    results were made by the author’s model
  prefs: []
  type: TYPE_NORMAL
- en: Along with solving simple problems, the model now handles more complex challenges.
    By adding datasets for tasks more intricate than just captioning, the capabilities
    have expanded significantly. Achieving this level of accuracy with only 1 epoch
    of training was surprising.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s test it with the following example image. Given the increased variety
    in the dataset, the way the questions were presented was slightly modified.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f06c8e160a3d026ec3efe7f0224d4be1.png)'
  prefs: []
  type: TYPE_IMG
- en: Example results of GIT-LLaMA2\. A picture is cited from [here](https://www.barnorama.com/wp-content/uploads/2016/12/03-Confusing-Pictures.jpg),
    and text results were made by the author’s models
  prefs: []
  type: TYPE_NORMAL
- en: While the description being “Umbrella” was still wired, it feels like it’s getting
    better. To improve further, there’s a need to increase the number of training
    epochs, add more types or volumes of datasets, and leverage more powerful ViT
    or LLM. Nonetheless, it’s impressive that such a model could be developed in just
    half a day given the computational and data resources.
  prefs: []
  type: TYPE_NORMAL
- en: Bonus Experiment. *Did the Image Turn into Words?*
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let’s take another look at the GIT structure.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4e2f4f77acb65e4c8a7772a1254074fb.png)'
  prefs: []
  type: TYPE_IMG
- en: Simplified model architecture of GIT-LLM (image made by the author)
  prefs: []
  type: TYPE_NORMAL
- en: As shown in the figure, after feature extraction by the vision encoder, images
    are treated on par with vectorized text through *Visual Projection*. In other
    words, it’s possible that the *Visual Projection* is converting image vectors
    into text vectors. An investigation was conducted to see what the vectors look
    like after the*Visual Projection*.
  prefs: []
  type: TYPE_NORMAL
- en: While there’s an option to use the *Head* to revert the vector post-projection
    back to text, it was found that even vectors that were vectorized using the *Embedding*
    module could not be reverted back to their original text using this method. Therefore,
    vectors that closely resemble the text vectors before being input into the LLM
    should be assigned as the corresponding word. All tokens registered in the tokenizer
    were vectorized using the Embedding module, and the one with the highest cosine
    similarity was identified as the target word.
  prefs: []
  type: TYPE_NORMAL
- en: The image used for this experiment is that of a cat.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4b8b2f288c78ac4a91884cca6859233e.png)'
  prefs: []
  type: TYPE_IMG
- en: A Picture is cited from M3IT dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s proceed with the analysis (whole analysis is available [here](https://github.com/Ino-Ichan/GIT-LLM/blob/main/notebooks/show_visual_words.ipynb)).
    First, all the registered tokens are vectorized.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Next, the image vectors that would have been converted to words by the ViT and
    *Projection* are extracted.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: The dot products of these vectors and the word vectors were calculated, and
    the results with the maximum value were decoded as the relevant token ID.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: As shown in the printed *decoded_text*, some unfamiliar words have appeared.
    As some words are repeated, they were counted.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: A large number of unfamiliar words seem to have appeared. Depending on the position,
    they might convey meaningful information. Let’s plot the words against the cat
    image.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/e3abd839ef5d0ca7bef4cfd2ad1ebfc4.png)'
  prefs: []
  type: TYPE_IMG
- en: Image made by the author
  prefs: []
  type: TYPE_NORMAL
- en: Words that frequently appear are color-coded. The result seems to suggest that
    they are not simply being projected onto meaningful words. While the word “***Cat***”
    might be superimposed on the cat image, giving it some relevance, its meaning
    remains unclear.
  prefs: []
  type: TYPE_NORMAL
- en: The inconclusive results in this experiment might be due to forcibly selecting
    a word with a high cosine similarity. At any rate, the approach doesn’t involve
    simply casting words and creating image prompts. Vectors extracted from images
    are converted by *Visual Projection* into vectors in token space, which seem to
    hold some resemblance in meaning, functioning as mysterious prompts. It might
    be best not to delve any deeper into this.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this tech blog post, I introduced the method of integrating LLMs into the
    vision language model, GIT. Furthermore, various experiments were conducted using
    the developed models. While there were successes and failures, I would like to
    continue conducting experiments with vision language models to accumulate insights.
    Please consider this article as a reference and feel encouraged to create your
    own vision language models and explore its potential.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c0d65bb574d89d207d2ce05bd90c5dad.png)'
  prefs: []
  type: TYPE_IMG
- en: This is an illustrated image of GIT-LLM created using Stable Diffusion. (Image
    made by the author)
  prefs: []
  type: TYPE_NORMAL
