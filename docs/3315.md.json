["```py\n ## Load packages and functions needed\nlibrary(drf)\nlibrary(mice)\nsource(\"drfnew_v2.R\")\n## The function drfnew_v2.R is available below, or on \n## https://github.com/JeffNaef/drfupdate\n\n## Set parameters\nset.seed(10)\nn<-1000\n\n##Simulate Data that experiences both a mean as well as sd shift\n# Simulate from X\nx1 <- runif(n,-1,1)\nx2 <- runif(n,-1,1)\nx3 <- x1+ runif(n,-1,1)\nX0 <- matrix(runif(7*n,-1,1), nrow=n, ncol=7)\nXfull <- cbind(x1,x2, x3, X0)\ncolnames(Xfull)<-paste0(\"X\", 1:10)\n\n# Simulate dependent variable Y\nY <- as.matrix(rnorm(n,mean = 0.8*(x1 > 0), sd = 1 + 1*(x2 > 0)))\n\n##Also add MAR missing values using ampute from the mice package\nX<-ampute(Xfull)$amp\n\nhead(cbind(Y,X))\n\n           Y         X1          X2          X3          X4           X5\n1 -3.0327466 -0.4689827  0.06161759  0.27462737          NA -0.624463079\n2  1.2582824 -0.2557522  0.36972181          NA -0.04100963  0.009518047\n3 -0.8781940  0.1457067 -0.23343321          NA -0.64519687 -0.945426305\n4  3.1595623  0.8164156  0.90997600  0.69184618 -0.20573331 -0.007404298\n5  1.1176545 -0.5966361          NA -1.21276055  0.62845399  0.894703422\n6 -0.4377359  0.7967794 -0.92179989 -0.03863182  0.88271415 -0.237635732\n          X6         X7          X8         X9        X10\n1 -0.9290009  0.5401628  0.39735433 -0.7434697  0.8807558\n2 -0.2885927  0.3805251 -0.09051334 -0.7446170  0.9935311\n3 -0.5022541  0.3009541  0.29424395  0.5554647 -0.5341800\n4  0.7583608 -0.8506881  0.22758566 -0.1596993 -0.7161976\n5 -0.3640422  0.8051613 -0.46714833  0.4318039 -0.8674060\n6 -0.3577590 -0.7341207  0.85504668 -0.6933918  0.4656891\n```", "```py\nx<-matrix(c(0.2, 0.4, runif(8,-1,1)), nrow=1, ncol=10)\nprint(x)\n\n     [,1] [,2]      [,3]      [,4]      [,5]      [,6]    [,7]      [,8]\n[1,]  0.2  0.4 0.7061058 0.8364877 0.2284314 0.7971179 0.78581 0.5310279\n           [,9]     [,10]\n[1,] -0.5067102 0.6918785\n```", "```py\nDRF<-drfCI(X=X, Y=Y, B=50,num.trees=1000, min.node.size = 5)\nDRFpred<-predictdrf(DRF, newdata=x)\n\n## Sample from P_{Y| X=x}\nYxs<-Y[sample(1:n, size=n, replace = T, DRFpred$weights[1,])]\nhist(Yxs, prob=T)\nz<-seq(-6,7,by=0.01)\nd<-dnorm(z, mean=0.8 * (x[1] > 0), sd=(1+(x[2] > 0)))\nlines(z,d, col=\"darkred\"  )\n```", "```py\n# Calculate quantile prediction as weighted quantiles from Y\nqx <- quantile(Yxs, probs = c(0.05,0.95))\n\n# Calculate conditional mean prediction\nmux <- mean(Yxs)\n\n# True quantiles\nq1<-qnorm(0.05, mean=0.8 * (x[1] > 0), sd=(1+(x[2] > 0)))\nq2<-qnorm(0.95, mean=0.8 * (x[1] > 0), sd=(1+(x[2] > 0)))\nmu<-0.8 * (x[1] > 0)\n\nhist(Yxs, prob=T)\nz<-seq(-6,7,by=0.01)\nd<-dnorm(z, mean=0.8 * (x[1] > 0), sd=(1+(x[2] > 0)))\nlines(z,d, col=\"darkred\"  )\nabline(v=q1,col=\"darkred\" )\nabline(v=q2, col=\"darkred\" )\nabline(v=qx[1], col=\"darkblue\")\nabline(v=qx[2], col=\"darkblue\")\nabline(v=mu, col=\"darkred\")\nabline(v=mux, col=\"darkblue\")\n```", "```py\n# Calculate uncertainty\nalpha<-0.05\nB<-length(DRFpred$weightsb)\nqxb<-matrix(NaN, nrow=B, ncol=2)\nmuxb<-matrix(NaN, nrow=B, ncol=1)\nfor (b in 1:B){\nYxsb<-Y[sample(1:n, size=n, replace = T, DRFpred$weightsb[[b]][1,])]\nqxb[b,] <- quantile(Yxsb, probs = c(0.05,0.95))\nmuxb[b] <- mean(Yxsb)\n}\nCI.lower.q1 <- qx[1] - qnorm(1-alpha/2)*sqrt(var(qxb[,1]))\nCI.upper.q1 <- qx[1] + qnorm(1-alpha/2)*sqrt(var(qxb[,1]))\n\nCI.lower.q2 <- qx[2] - qnorm(1-alpha/2)*sqrt(var(qxb[,2]))\nCI.upper.q2 <- qx[2] + qnorm(1-alpha/2)*sqrt(var(qxb[,2]))\n\nCI.lower.mu <- mux - qnorm(1-alpha/2)*sqrt(var(muxb))\nCI.upper.mu <- mux + qnorm(1-alpha/2)*sqrt(var(muxb))\n\nhist(Yxs, prob=T)\nz<-seq(-6,7,by=0.01)\nd<-dnorm(z, mean=0.8 * (x[1] > 0), sd=(1+(x[2] > 0)))\nlines(z,d, col=\"darkred\"  )\nabline(v=q1,col=\"darkred\" )\nabline(v=q2, col=\"darkred\" )\nabline(v=qx[1], col=\"darkblue\")\nabline(v=qx[2], col=\"darkblue\")\nabline(v=mu, col=\"darkred\")\nabline(v=mux, col=\"darkblue\")\nabline(v=CI.lower.q1, col=\"darkblue\", lty=2)\nabline(v=CI.upper.q1, col=\"darkblue\", lty=2)\nabline(v=CI.lower.q2, col=\"darkblue\", lty=2)\nabline(v=CI.upper.q2, col=\"darkblue\", lty=2)\nabline(v=CI.lower.mu, col=\"darkblue\", lty=2)\nabline(v=CI.upper.mu, col=\"darkblue\", lty=2)\n```", "```py\n## Variable importance for conditional Quantile Estimation\n\n## For the conditional quantiles we use a measure that considers the whole distribution,\n## i.e. the MMD based measure of DRF.\nMMDVimp <- compute_drf_vimp(X=X,Y=Y, print=F)\nsort(MMDVimp, decreasing = T)\n\n         X2          X1          X8          X6          X3         X10 \n0.852954299 0.124110913 0.012194176 0.009578300 0.008191663 0.007517931 \n         X9          X7          X5          X4 \n0.006861688 0.006632175 0.005257195 0.002401974 \n```", "```py\nrequire(drf)\nrequire(Matrix)\nrequire(kernlab)\n\ndrfCI <- function(X, Y, B, sampling = \"binomial\", ...) {\n\n  n <- dim(X)[1]\n\n  # compute point estimator and DRF per halfsample\n  # weightsb: B times n matrix of weights\n  DRFlist <- lapply(seq_len(B), function(b) {\n\n    # half-sample index\n    indexb <- if (sampling == \"binomial\") {\n      seq_len(n)[as.logical(rbinom(n, size = 1, prob = 0.5))]\n    } else {\n      sample(seq_len(n), floor(n / 2), replace = FALSE)\n    }\n\n    ## Using normal Bootstrap on the data and refitting DRF\n    DRFb <-\n      drfown(X = X[indexb, , drop = F], Y = Y[indexb, , drop = F], ...)\n\n    return(list(DRF = DRFb, indices = indexb))\n  })\n\n  return(list(DRFlist = DRFlist, X = X, Y = Y))\n}\n\npredictdrf <- function(DRF, newdata, functional = NULL, ...) {\n\n  ##Predict for testpoints in newdata, with B weights for each point, representing\n  ##uncertainty\n\n  ntest <- nrow(x)\n  n <- nrow(DRF$Y)\n\n  weightsb <- lapply(DRF$DRFlist, function(l) {\n\n    weightsbfinal <- Matrix(0, nrow = ntest, ncol = n, sparse = TRUE)\n\n    weightsbfinal[, l$indices] <- predict(l$DRF, x)$weights \n\n    return(weightsbfinal)\n  })\n\n  weightsall <- Reduce(\"+\", weightsb) / length(weightsb)\n\n  if (!is.null(functional)) {\n    stopifnot(\"Not yet implemented for several x\" = ntest == 1)\n\n    thetahatb <- \n      lapply(weightsb, function(w)  \n        functional(weights = w, X = DRF$X, Y = DRF$Y, x = x))\n    thetahatbforvar <- do.call(rbind, thetahatb)\n    thetahat <- functional(weights = weightsall, X = DRF$X, Y = DRF$Y, x = x)\n    thetahat <- matrix(thetahat, nrow = dim(x)[1])\n    var_est <- if (dim(thetahat)[2] > 1) {  \n      a <- sweep(thetahatbforvar, 2, thetahat, FUN = \"-\")\n      crossprod(a, a) / B\n    } else {\n      mean((c(thetahatbforvar) - c(thetahat)) ^ 2) \n    }\n\n    return(list(weights = weightsall, thetahat = thetahat, weightsb = weightsb, \n                var_est = var_est))\n\n  } else {\n    return(list(weights = weightsall, weightsb = weightsb))\n  }\n}\n\ndrfown <-               function(X, Y,\n                                 num.trees = 500,\n                                 splitting.rule = \"FourierMMD\",\n                                 num.features = 10,\n                                 bandwidth = NULL,\n                                 response.scaling = TRUE,\n                                 node.scaling = FALSE,\n                                 sample.weights = NULL,\n                                 sample.fraction = 0.5,\n                                 mtry = min(ceiling(sqrt(ncol(X)) + 20), ncol(X)),\n                                 min.node.size = 15,\n                                 honesty = TRUE,\n                                 honesty.fraction = 0.5,\n                                 honesty.prune.leaves = TRUE,\n                                 alpha = 0.05,\n                                 imbalance.penalty = 0,\n                                 compute.oob.predictions = TRUE,\n                                 num.threads = NULL,\n                                 seed = stats::runif(1, 0, .Machine$integer.max),\n                                 compute.variable.importance = FALSE) {\n\n  # initial checks for X and Y\n  if (is.data.frame(X)) {\n\n    if (is.null(names(X))) {\n      stop(\"the regressor should be named if provided under data.frame format.\")\n    }\n\n    if (any(apply(X, 2, class) %in% c(\"factor\", \"character\"))) {\n      any.factor.or.character <- TRUE\n      X.mat <- as.matrix(fastDummies::dummy_cols(X, remove_selected_columns = TRUE))\n    } else {\n      any.factor.or.character <- FALSE\n      X.mat <- as.matrix(X)\n    }\n\n    mat.col.names.df <- names(X)\n    mat.col.names <- colnames(X.mat)\n  } else {\n    X.mat <- X\n    mat.col.names <- NULL\n    mat.col.names.df <- NULL\n    any.factor.or.character <- FALSE\n  }\n\n  if (is.data.frame(Y)) {\n\n    if (any(apply(Y, 2, class) %in% c(\"factor\", \"character\"))) {\n      stop(\"Y should only contain numeric variables.\")\n    }\n    Y <- as.matrix(Y)\n  }\n\n  if (is.vector(Y)) {\n    Y <- matrix(Y,ncol=1)\n  }\n\n  #validate_X(X.mat)\n\n  if (inherits(X, \"Matrix\") && !(inherits(X, \"dgCMatrix\"))) {\n    stop(\"Currently only sparse data of class 'dgCMatrix' is supported.\")\n  }\n\n  drf:::validate_sample_weights(sample.weights, X.mat)\n  #Y <- validate_observations(Y, X)\n\n  # set legacy GRF parameters\n  clusters <- vector(mode = \"numeric\", length = 0)\n  samples.per.cluster <- 0\n  equalize.cluster.weights <- FALSE\n  ci.group.size <- 1\n\n  num.threads <- drf:::validate_num_threads(num.threads)\n\n  all.tunable.params <- c(\"sample.fraction\", \"mtry\", \"min.node.size\", \"honesty.fraction\",\n                          \"honesty.prune.leaves\", \"alpha\", \"imbalance.penalty\")\n\n  # should we scale or not the data\n  if (response.scaling) {\n    Y.transformed <- scale(Y)\n  } else {\n    Y.transformed <- Y\n  }\n\n  data <- drf:::create_data_matrices(X.mat, outcome = Y.transformed, sample.weights = sample.weights)\n\n  # bandwidth using median heuristic by default\n  if (is.null(bandwidth)) {\n    bandwidth <- drf:::medianHeuristic(Y.transformed)\n  }\n\n  args <- list(num.trees = num.trees,\n               clusters = clusters,\n               samples.per.cluster = samples.per.cluster,\n               sample.fraction = sample.fraction,\n               mtry = mtry,\n               min.node.size = min.node.size,\n               honesty = honesty,\n               honesty.fraction = honesty.fraction,\n               honesty.prune.leaves = honesty.prune.leaves,\n               alpha = alpha,\n               imbalance.penalty = imbalance.penalty,\n               ci.group.size = ci.group.size,\n               compute.oob.predictions = compute.oob.predictions,\n               num.threads = num.threads,\n               seed = seed,\n               num_features = num.features,\n               bandwidth = bandwidth,\n               node_scaling = ifelse(node.scaling, 1, 0))\n\n  if (splitting.rule == \"CART\") {\n    ##forest <- do.call(gini_train, c(data, args))\n    forest <- drf:::do.call.rcpp(drf:::gini_train, c(data, args))\n    ##forest <- do.call(gini_train, c(data, args))\n  } else if (splitting.rule == \"FourierMMD\") {\n    forest <- drf:::do.call.rcpp(drf:::fourier_train, c(data, args))\n  } else {\n    stop(\"splitting rule not available.\")\n  }\n\n  class(forest) <- c(\"drf\")\n  forest[[\"ci.group.size\"]] <- ci.group.size\n  forest[[\"X.orig\"]] <- X.mat\n  forest[[\"is.df.X\"]] <- is.data.frame(X)\n  forest[[\"Y.orig\"]] <- Y\n  forest[[\"sample.weights\"]] <- sample.weights\n  forest[[\"clusters\"]] <- clusters\n  forest[[\"equalize.cluster.weights\"]] <- equalize.cluster.weights\n  forest[[\"tunable.params\"]] <- args[all.tunable.params]\n  forest[[\"mat.col.names\"]] <- mat.col.names\n  forest[[\"mat.col.names.df\"]] <- mat.col.names.df\n  forest[[\"any.factor.or.character\"]] <- any.factor.or.character\n\n  if (compute.variable.importance) {\n    forest[['variable.importance']] <- variableImportance(forest, h = bandwidth)\n  }\n\n  forest\n}\n\n#' Variable importance for Distributional Random Forests\n#'\n#' @param X Matrix with input training data.\n#' @param Y Matrix with output training data.\n#' @param X_test Matrix with input testing data. If NULL, out-of-bag estimates are used.\n#' @param num.trees Number of trees to fit DRF. Default value is 500 trees.\n#' @param silent If FALSE, print variable iteration number, otherwise nothing is print. Default is FALSE.\n#'\n#' @return The list of importance values for all input variables.\n#' @export\n#'\n#' @examples\ncompute_drf_vimp <- function(X, Y, X_test = NULL, num.trees = 500, silent = FALSE){\n\n  # fit initial DRF\n  bandwidth_Y <- drf:::medianHeuristic(Y)\n  k_Y <- rbfdot(sigma = bandwidth_Y)\n  K <- kernelMatrix(k_Y, Y, Y)\n  DRF <- drfown(X, Y, num.trees = num.trees)\n  wall <- predict(DRF, X_test)$weights\n\n  # compute normalization constant\n  wbar <- colMeans(wall)\n  wall_wbar <- sweep(wall, 2, wbar, \"-\")\n  I0 <- as.numeric(sum(diag(wall_wbar %*% K %*% t(wall_wbar))))\n\n  # compute drf importance dropping variables one by one\n  I <- sapply(1:ncol(X), function(j) {\n    if (!silent){print(paste0('Running importance for variable X', j, '...'))}\n    DRFj <- drfown(X = X[, -j, drop=F], Y = Y, num.trees = num.trees) \n    DRFpredj <- predict(DRFj, X_test[, -j])\n    wj <- DRFpredj$weights\n    Ij <- sum(diag((wj - wall) %*% K %*% t(wj - wall)))/I0\n    return(Ij)\n  })\n\n  # compute retraining bias\n  DRF0 <- drfown(X = X, Y = Y, num.trees = num.trees)\n  DRFpred0 = predict(DRF0, X_test)\n  w0 <- DRFpred0$weights\n  vimp0 <- sum(diag((w0 - wall) %*% K %*% t(w0 - wall)))/I0\n\n  # compute final importance (remove bias & truncate negative values)\n  vimp <- sapply(I - vimp0, function(x){max(0,x)})\n\n  names(vimp)<-colnames(X)\n\n  return(vimp)\n\n} \n```"]