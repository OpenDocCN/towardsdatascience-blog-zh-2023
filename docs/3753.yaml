- en: LLM+RAG-Based Question Answering
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于LLM+RAG的问题回答
- en: 原文：[https://towardsdatascience.com/llm-rag-based-question-answering-6a405c8ad38a?source=collection_archive---------0-----------------------#2023-12-25](https://towardsdatascience.com/llm-rag-based-question-answering-6a405c8ad38a?source=collection_archive---------0-----------------------#2023-12-25)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/llm-rag-based-question-answering-6a405c8ad38a?source=collection_archive---------0-----------------------#2023-12-25](https://towardsdatascience.com/llm-rag-based-question-answering-6a405c8ad38a?source=collection_archive---------0-----------------------#2023-12-25)
- en: How to do poorly on Kaggle, and learn about RAG+LLM from it
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何在Kaggle上表现不佳，并从中学习RAG+LLM
- en: '[](https://teemukanstren.medium.com/?source=post_page-----6a405c8ad38a--------------------------------)[![Teemu
    Kanstrén](../Images/8ad278d60d1fa3f794fccb4c61d607ce.png)](https://teemukanstren.medium.com/?source=post_page-----6a405c8ad38a--------------------------------)[](https://towardsdatascience.com/?source=post_page-----6a405c8ad38a--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----6a405c8ad38a--------------------------------)
    [Teemu Kanstrén](https://teemukanstren.medium.com/?source=post_page-----6a405c8ad38a--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://teemukanstren.medium.com/?source=post_page-----6a405c8ad38a--------------------------------)[![Teemu
    Kanstrén](../Images/8ad278d60d1fa3f794fccb4c61d607ce.png)](https://teemukanstren.medium.com/?source=post_page-----6a405c8ad38a--------------------------------)[](https://towardsdatascience.com/?source=post_page-----6a405c8ad38a--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----6a405c8ad38a--------------------------------)
    [Teemu Kanstrén](https://teemukanstren.medium.com/?source=post_page-----6a405c8ad38a--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F9fc0679190dc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fllm-rag-based-question-answering-6a405c8ad38a&user=Teemu+Kanstr%C3%A9n&userId=9fc0679190dc&source=post_page-9fc0679190dc----6a405c8ad38a---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----6a405c8ad38a--------------------------------)
    ·23 min read·Dec 25, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F6a405c8ad38a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fllm-rag-based-question-answering-6a405c8ad38a&user=Teemu+Kanstr%C3%A9n&userId=9fc0679190dc&source=-----6a405c8ad38a---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F9fc0679190dc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fllm-rag-based-question-answering-6a405c8ad38a&user=Teemu+Kanstr%C3%A9n&userId=9fc0679190dc&source=post_page-9fc0679190dc----6a405c8ad38a---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----6a405c8ad38a--------------------------------)
    ·23分钟阅读·2023年12月25日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F6a405c8ad38a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fllm-rag-based-question-answering-6a405c8ad38a&user=Teemu+Kanstr%C3%A9n&userId=9fc0679190dc&source=-----6a405c8ad38a---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F6a405c8ad38a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fllm-rag-based-question-answering-6a405c8ad38a&source=-----6a405c8ad38a---------------------bookmark_footer-----------)![](../Images/f497014da435709dae04a493366a7919.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F6a405c8ad38a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fllm-rag-based-question-answering-6a405c8ad38a&source=-----6a405c8ad38a---------------------bookmark_footer-----------)![](../Images/f497014da435709dae04a493366a7919.png)'
- en: Image generated with ChatGPT+/DALL-E3, asking for an illustrative image for
    an article about RAG.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 图像由ChatGPT+/DALL-E3生成，展示了关于RAG的文章的插图。
- en: Retrieval Augmented Generation (RAG) seems to be quite popular these days. Along
    the wave of Large Language Models (LLM’s), it is one of the popular techniques
    to get LLM’s to perform better on specific tasks such as question answering on
    in-house documents. Some time ago, I played on a [Kaggle competition](https://www.kaggle.com/competitions/kaggle-llm-science-exam)
    that allowed me to try it out and learn a bit better than random experiments on
    my own. Here are a few learnings from that and the following experiments while
    writing this article.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 检索增强生成（RAG）似乎现在相当受欢迎。随着大语言模型（LLM）的兴起，它成为了使LLM在特定任务上表现更好的热门技术之一，比如对内部文档进行问答。前段时间，我参加了一个
    [Kaggle比赛](https://www.kaggle.com/competitions/kaggle-llm-science-exam)，这让我能够尝试它，并比自己随意实验学到更多一些。以下是从这些实验中获得的一些经验教训。
- en: All images, unless otherwise noted, are by the author. Generated with the help
    of ChatGPT+/DALL-E3 (where noted), or taken from my personal Jupyter notebooks.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 除非另有说明，否则所有图片均由作者提供。生成工具为 ChatGPT+/DALL-E3（如有注明），或取自我个人的 Jupyter 笔记本。
- en: RAG Overview
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: RAG 概述
- en: RAG has two main parts, retrieval and generation. In the first part, retrieval
    is used to fetch (chunks of) documents related to the query of interest. Generation
    uses those fetched chunks as added input, called *context*, to the answer generation
    model in the second part. This added context is intended to give the generator
    more up-to-date, hopefully better, information to base its generated answer on
    than just its base training data.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: RAG 有两个主要部分：检索和生成。在第一部分中，检索用于获取与查询相关的（块）文档。生成则使用这些检索到的块作为额外输入，即 *context*，传递给第二部分的答案生成模型。这个附加的上下文旨在为生成器提供比基本训练数据更及时、更好的信息，以生成答案。
- en: Building the RAG Input, or Chunking Text
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建 RAG 输入，或文本分块
- en: LLM’s have a maximum context or sequence window length they can handle, and
    the generated input context for RAG needs to be short enough to fit into this
    sequence window. We want to fit as much relevant information into this context
    as possible, so getting the best “chunks” of text from the potential input documents
    is important. These chunks should optimally be the most relevant ones for generating
    the correct answer to the question posed to the RAG system.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: LLM 的最大上下文或序列窗口长度是它们可以处理的范围，RAG 生成的输入上下文需要足够短以适应这个序列窗口。我们希望将尽可能多的相关信息纳入这个上下文，因此从潜在的输入文档中获取最佳的“块”文本非常重要。这些块应当是生成正确答案所需的最相关的内容。
- en: As a first step, the input text is typically chunked into smaller pieces. A
    basic pre-processing step in RAG is converting these chunks into embeddings using
    a specific embedding model. A typical sequence window for an embedding model is
    512 tokens, which also makes a practical target for chunk size. Once the documents
    are chunked and encoded into embeddings, a similarity search using the embeddings
    can be performed to build the context for generating the answer.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 作为第一步，输入文本通常会被分块成更小的片段。RAG 的一个基本预处理步骤是使用特定的嵌入模型将这些块转换为嵌入。一个典型的嵌入模型的序列窗口为 512
    个 tokens，这也使其成为实际的分块目标。一旦文档被分块并编码为嵌入，就可以使用这些嵌入进行相似性搜索，以构建生成答案的上下文。
- en: 'I have found [Langchain](https://github.com/langchain-ai/langchain) to provide
    useful tools for input loading and chunking. For example, chunking a document
    with Langchain (in this case, using tokenizer for [Flan-T5-Large](https://huggingface.co/google/flan-t5-large)
    model) is as simple as:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 我发现 [Langchain](https://github.com/langchain-ai/langchain) 提供了有用的工具用于输入加载和文本分块。例如，使用
    Langchain 对文档进行分块（在此情况下，使用 [Flan-T5-Large](https://huggingface.co/google/flan-t5-large)
    模型的分词器）是非常简单的：
- en: '[PRE0]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'This produces the following two chunks:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 这将生成以下两个块：
- en: '[PRE1]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'In the above code, *chunk_size* 12 tells LangChain to aim for a maximum of
    12 tokens per chunk. Depending on the text structure, [this may not always be
    100% exact](https://stackoverflow.com/questions/76633836/what-does-langchain-charactertextsplitters-chunk-size-param-even-do).
    However, in my experience it works generally well. Something to keep in mind is
    the difference between tokens vs words. Here is an example of tokenizing the above
    *section_text*:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在上面的代码中，*chunk_size* 12 告诉 LangChain 旨在每个块最多包含 12 个 token。根据文本结构，[这可能并不总是 100%
    精确](https://stackoverflow.com/questions/76633836/what-does-langchain-charactertextsplitters-chunk-size-param-even-do)。然而，根据我的经验，这通常效果很好。需要记住的是
    tokens 和单词之间的区别。下面是对上述 *section_text* 进行分词的一个示例：
- en: '[PRE2]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Resulting output tokens:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 生成的输出 tokens：
- en: '[PRE3]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Most words in the *section_text* form a token on their own, as they are [common
    words in texts](https://huggingface.co/docs/transformers/tokenizer_summary). However,
    for special forms of words, or domain words this can be a bit more complicated.
    For example, here the word “uncharacteristic” becomes three tokens [“ *un*”, “
    *character*”, “ *istic*”]. This is because the model tokenizer knows those 3 partial
    sub-words but not the entire word (“ *uncharacteristic* “). Each model comes with
    its own tokenizer to match these rules in input and model training.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数 *section_text* 中的单词本身形成一个 token，因为它们是[文本中的常见单词](https://huggingface.co/docs/transformers/tokenizer_summary)。然而，对于特殊形式的单词或领域词汇，这可能会更复杂。例如，在这里，“uncharacteristic”
    这个词变成了三个 tokens [“ *un*”， “ *character*”， “ *istic*”]。这是因为模型的分词器知道这三个部分词汇，但不知道整个单词（“
    *uncharacteristic*”）。每个模型都有自己的分词器来匹配输入和模型训练中的这些规则。
- en: In chunking, the [RecursiveCharacterTextSplitter](https://python.langchain.com/docs/modules/data_connection/document_transformers/text_splitters/recursive_text_splitter)
    from Langchain used in above code counts these tokens, and looks for given separators
    to split the text into chunks as requested. Trials with different chunk sizes
    may be useful. In my Kaggle experiment I started with the maximum size for the
    embedding model, which was 512 tokens. Then proceeded to try chunk sizes of 256,
    128, and 64 tokens.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在分块中，来自Langchain的[RecursiveCharacterTextSplitter](https://python.langchain.com/docs/modules/data_connection/document_transformers/text_splitters/recursive_text_splitter)用于上述代码中，计算这些令牌，并寻找给定的分隔符将文本拆分成请求的块。不同块大小的试验可能会有用。在我的Kaggle实验中，我从嵌入模型的最大大小开始，即512个令牌。然后尝试了256、128和64个令牌的块大小。
- en: Example RAG Query
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 示例RAG查询
- en: 'The [Kaggle competition](https://www.kaggle.com/competitions/kaggle-llm-science-exam)
    I mentioned was about multiple-choice question answering based on Wikipedia data.
    The task was to select the correct answer option from the multiple options for
    each question. The obvious approach was to use RAG to find required information
    from a Wikipedia dump, and use it to generate the correct. Here is the first question
    from competition data, and its answer options to illustrate:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 我提到的[Kaggle比赛](https://www.kaggle.com/competitions/kaggle-llm-science-exam)是基于维基百科数据的多项选择题回答。任务是从多个选项中选择每个问题的正确答案。显而易见的方法是使用RAG从维基百科数据中找到所需的信息，并用它来生成正确答案。以下是比赛数据中的第一个问题及其答案选项，用于说明：
- en: '![](../Images/35ff5c28169adc2755e25364da4b542b.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/35ff5c28169adc2755e25364da4b542b.png)'
- en: Example question and answer options A-E.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 示例问题和答案选项A-E。
- en: The multiple-choice questions were an interesting topic to try out RAG. But
    the most common RAG use case is, I believe, answering questions based on source
    documents. Kind of like a chatbot, but typically question answering over domain
    specific or (company) internal documents. I use this basic question answering
    use case to demonstrate RAG in this article.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 多项选择题是尝试RAG的一个有趣话题。但我相信，最常见的RAG用例是根据源文档回答问题。有点像聊天机器人，但通常是针对特定领域或（公司）内部文档的问答。我在本文中使用这个基本的问答用例来展示RAG。
- en: As an example RAG question for this article, I needed something the LLM would
    not know the answer to directly based on its training data alone. I used Wikipedia
    data, and since it is likely used as part of training data for LLM’s, I needed
    a question related to something after the model was trained. The model I used
    for this article was [Zephyr 7B beta](https://huggingface.co/HuggingFaceH4/zephyr-7b-beta),
    trained in early 2023\. Finally, I settled on asking about the [Google Bard AI
    chatbot](https://bard.google.com/). It has had many developments over the past
    year, after the Zephyr training date. I also have a decent knowledge of Bard to
    evaluate the LLM’s answers. Thus I used “*what is google bard?* “ as an example
    question for this article.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 作为本文的RAG示例问题，我需要一个LLM无法仅凭其训练数据直接回答的问题。我使用了维基百科数据，因为它可能是LLM训练数据的一部分，所以我需要一个与模型训练后相关的问题。我为本文使用的模型是[Zephyr
    7B beta](https://huggingface.co/HuggingFaceH4/zephyr-7b-beta)，于2023年初训练完成。最后，我决定询问[Google
    Bard AI聊天机器人](https://bard.google.com/)。它在Zephyr训练日期之后的一年里有很多发展。我对Bard也有一定了解，以评估LLM的答案。因此，我使用“*what
    is google bard?*”作为本文的示例问题。
- en: Embedding Vectors
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 嵌入向量
- en: 'The first phase of retrieval in RAG is based on the embedding vectors, which
    are really just points in a multidimensional space. They look something like this
    (only the first 10 values here):'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: RAG的第一阶段检索基于嵌入向量，这些向量实际上只是多维空间中的点。它们看起来像这样（这里只列出了前10个值）：
- en: '[PRE4]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'These embedding vectors can be used to compare the words/sentences, and their
    relations, against each other. These vectors can be built using embedding models.
    A nice set of those models with various stats per model can be found on the [MTEB
    leaderboard](https://huggingface.co/spaces/mteb/leaderboard). Using one of those
    models is as simple as this:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 这些嵌入向量可以用来比较单词/句子及其相互关系。这些向量可以通过嵌入模型构建。可以在[MTEB排行榜](https://huggingface.co/spaces/mteb/leaderboard)找到各种统计数据的模型集。使用这些模型之一就像这样简单：
- en: '[PRE5]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The model page on HuggingFace typically shows the example code. The above loads
    the model “ [bge-small-en](https://huggingface.co/BAAI/bge-small-en-v1.5) “ from
    local disk. To create the embeddings using this model is just:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: HuggingFace上的模型页面通常显示示例代码。上述代码从本地磁盘加载模型“ [bge-small-en](https://huggingface.co/BAAI/bge-small-en-v1.5)
    ”。使用此模型创建嵌入只是：
- en: '[PRE6]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'In this case, the embedding model is used to encode the given question into
    an embedding vector. The vector is the same as the example above:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，嵌入模型用于将给定的问题编码为嵌入向量。该向量与上面的示例相同：
- en: '[PRE7]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The shape (, 384) tells me *q_embeddings* is a single vector (as opposed to
    embedding a list of multiple texts at once) of length 384 floats. The slice above
    shows the first 10 values out of those 384\. Some models use longer vectors for
    more accurate relations, others, like this one, shorter (here 384). Again, [MTEB
    leaderboard](https://huggingface.co/spaces/mteb/leaderboard) has good examples.
    The small ones require less space and computation, larger ones give some improvements
    in representing the relations between chunks, and sometimes sequence length.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 形状 (, 384) 表示 *q_embeddings* 是一个长度为 384 个浮点数的单一向量（而不是一次嵌入多个文本的列表）。上面的切片显示了这
    384 个值中的前 10 个。某些模型使用更长的向量以获得更准确的关系，而其他模型，如本例所示，则使用较短的向量（此处为 384）。再次，[MTEB 排行榜](https://huggingface.co/spaces/mteb/leaderboard)
    提供了很好的示例。较小的向量需要更少的空间和计算，而较大的向量在表示块之间的关系以及有时的序列长度方面提供了一些改进。
- en: 'For my RAG similarity search, I first needed embeddings for the question. This
    is the *q_embeddings* above. This needed to be compared against embedding vectors
    of all the searched articles (or their chunks). In this case all the chunked Wikipedia
    articles. To build embedding for all of those:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我的 RAG 相似性搜索，我首先需要问题的嵌入。这就是上面的 *q_embeddings*。需要将其与所有被搜索文章（或其块）的嵌入向量进行比较。在这种情况下，所有被分块的维基百科文章。要为所有这些构建嵌入：
- en: '[PRE8]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Here *article_chunks* is a list of all chunks for all articles from the English
    Wikipedia dump. This way they can be batch-encoded.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 这里 *article_chunks* 是来自英文维基百科数据转储的所有文章的所有块的列表。这样它们可以批量编码。
- en: Vector Databases
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 向量数据库
- en: Implementing similarity search over a large set of documents / document chunks
    is not too complicated at a basic level. A common way is to calculate [cosine
    similarity](https://www.geeksforgeeks.org/how-to-calculate-cosine-similarity-in-python/)
    between the query and document vectors, and sort accordingly. However, at large
    scale, this sometimes gets a bit complicated to manage. Vector databases are tools
    that make this management and search easier / more efficient at scale.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在大规模文档/文档块上实现相似性搜索，在基本层面上并不复杂。一个常见的方法是计算查询和文档向量之间的 [余弦相似性](https://www.geeksforgeeks.org/how-to-calculate-cosine-similarity-in-python/)，并进行排序。然而，在大规模时，这有时会变得有些复杂。向量数据库是使这种管理和搜索在规模上变得更简单/更高效的工具。
- en: For example, [Weaviate](https://weaviate.io/) is a vector database that was
    used in [StackOverflow’s AI-based search](https://resources.stackoverflow.co/topic/thought-leadership/stack-overflows-ai-journey-webinar).
    In its latest versions, it can also be used in an [embedded mode](https://weaviate.io/developers/weaviate/installation/embedded),
    which should have made it usable even in a Kaggle notebook. It is also used in
    some [Deeplearning.AI LLM short courses](https://learn.deeplearning.ai/vector-databases-embeddings-applications/lesson/1/introduction),
    so at least seems somewhat popular. Of course, there are many others and it is
    good to make comparisons, this field also evolves fast.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，[Weaviate](https://weaviate.io/) 是一个向量数据库，曾用于 [StackOverflow 的基于 AI 的搜索](https://resources.stackoverflow.co/topic/thought-leadership/stack-overflows-ai-journey-webinar)。在其最新版本中，它也可以以
    [嵌入模式](https://weaviate.io/developers/weaviate/installation/embedded) 使用，这使得它甚至可以在
    Kaggle 笔记本中使用。它也被用于一些 [Deeplearning.AI LLM 短期课程](https://learn.deeplearning.ai/vector-databases-embeddings-applications/lesson/1/introduction)，所以至少似乎有些受欢迎。当然，还有许多其他工具，进行比较是很好的，这个领域也在快速发展。
- en: In my trials, I used [FAISS](https://github.com/facebookresearch/faiss) from
    Facebook/Meta research as the vector database. FAISS is more of a library than
    a client-server database, and was thus simple to use in a Kaggle notebook. And
    it worked quite nicely.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在我的试验中，我使用了来自 Facebook/Meta 研究的 [FAISS](https://github.com/facebookresearch/faiss)
    作为向量数据库。FAISS 更像是一个库，而不是一个客户端-服务器数据库，因此在 Kaggle 笔记本中使用非常简单。它的表现也很不错。
- en: Chunked Data and Embeddings
  id: totrans-49
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分块数据和嵌入
- en: 'Once the chunking and embedding of all the articles was all done, I built a
    Pandas DataFrame with all the relevant information. Here is an example with the
    first 5 chunks of the Wikipedia dump I used, for a document titled *Anarchism*:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦所有文章的分块和嵌入完成后，我构建了一个包含所有相关信息的 Pandas DataFrame。以下是我使用的维基百科数据转储前 5 个块的示例，文档标题为
    *无政府主义*：
- en: '![](../Images/b0cad38abdd821b3e885baf39e3bcafb.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b0cad38abdd821b3e885baf39e3bcafb.png)'
- en: First 5 chunks from the first article in the Wikipedia dump I used.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 我使用的维基百科数据转储中的第一篇文章的前 5 个块。
- en: 'Each row in this table (a Pandas DataFrame) contains data for a single chunk
    after the chunking process. It has 5 columns:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 这个表格中的每一行（一个 Pandas DataFrame）包含块化过程后单个块的数据。它有 5 列：
- en: '*chunk_id*: allows me to map chunk embeddings to the chunk text later.'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*chunk_id*：允许我稍后将块嵌入映射到块文本。'
- en: '*doc_id*: allows mapping the chunks back to their document.'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*doc_id*：允许将块映射回其文档。'
- en: '*doc_title*: for trialing approaches such as adding the doc title to each chunk.'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*doc_title*：用于尝试一些方法，例如将文档标题添加到每个块中。'
- en: '*chunk_title*: article subsection title for the chunk, same purpose as doc_title'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*chunk_title*：块的文章子部分标题，与 *doc_title* 目的相同。'
- en: '*chunk*: the actual chunk text'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*chunk*：实际的块文本'
- en: 'Here are the embeddings for the first five Anarchism chunks, same order as
    the DataFrame above:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是前五个无政府主义块的嵌入，顺序与上面的 DataFrame 相同：
- en: '[PRE9]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Each row is partially only shown here, but illustrates the idea.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 每一行在这里部分展示，但说明了概念。
- en: Seach for Similar Query Embeddings vs Chunk Embeddings
  id: totrans-62
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 搜索相似的查询嵌入与块嵌入
- en: 'Earlier I encoded the query vector for query “ *what is google bard?* “‘, followed
    by encoding all the article chunks. With these two sets of embeddings, the first
    part of RAG search is simple: finding the documents “semantically” closest to
    the query. In practice just calculating a measure such as cosine similarity between
    the query embedding vector and all the chunk vectors, and sorting by the similarity
    score.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 早些时候，我对查询“ *what is google bard?* ”进行了编码，然后编码了所有文章块。通过这两组嵌入，RAG 搜索的第一部分很简单：找到“语义上”最接近查询的文档。实际上，只需计算查询嵌入向量与所有块向量之间的余弦相似度，并按相似度得分排序。
- en: 'Here are the top 10 “semantically” closest chunks to the *q_embeddings*:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是与 *q_embeddings* 语义上最接近的前 10 个块：
- en: '![](../Images/fa786f4b953c673500fa0eeb4ed0bf31.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fa786f4b953c673500fa0eeb4ed0bf31.png)'
- en: Top 10 chunks sorted by their cosine similarity with the question.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 按照与问题的余弦相似度排序的前 10 个块。
- en: Each row in this table (DataFrame) represents a chunk. The *sim_score* here
    is the calculated cosine similarity score, and the rows are sorted from highest
    cosine similarity to lowest. The table shows the top 10 highest *sim_score* rows.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 这个表格中的每一行（DataFrame）表示一个块。这里的 *sim_score* 是计算的余弦相似度得分，行从最高余弦相似度到最低排序。表格显示了前
    10 个最高的 *sim_score* 行。
- en: Re-ranking
  id: totrans-68
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Re-ranking
- en: A pure embeddings based similarity search is very fast and low-cost in terms
    of computation. However, it is not quite as accurate as some other approaches.
    *Re-ranking* is a term used to describe the process of using another model to
    more accurately sort this initial list of top documents, with a more computationally
    expensive model. This model is usually too expensive to run against all documents
    and chunks, but running it on the set of top chunks after the initial similarity
    search is much more feasible. Re-ranking helps to get a better list of final chunks
    to build the input context for the generation part of RAG.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 纯粹基于嵌入的相似性搜索在计算上非常快速且低成本。然而，它不如其他一些方法准确。*Re-ranking* 是一个术语，用于描述使用另一个模型更准确地排序这些初始文档列表的过程，这种模型通常计算成本更高。这个模型通常在所有文档和块上运行的成本太高，但在初始相似性搜索后的前几个块上运行就可行得多。Re-ranking
    帮助获得更好的最终块列表，以便为 RAG 的生成部分建立输入上下文。
- en: 'The same [MTEB leaderboard](https://huggingface.co/spaces/mteb/leaderboard)
    that hosts metrics for the embedding models also has re-ranking scores for many
    models. In this case I used the [bge-reranker-base](https://huggingface.co/BAAI/bge-reranker-base)
    model for re-ranking:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 同样的 [MTEB 排行榜](https://huggingface.co/spaces/mteb/leaderboard) 也托管了许多模型的 re-ranking
    得分。在这种情况下，我使用了 [bge-reranker-base](https://huggingface.co/BAAI/bge-reranker-base)
    模型进行 re-ranking：
- en: '[PRE10]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'After adding *rerank_score* to the chunk DataFrame, and sorting with it:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在将 *rerank_score* 添加到块 DataFrame 并用它进行排序后：
- en: '![](../Images/4fe80155ef3b8e0c33a065708d942c8d.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4fe80155ef3b8e0c33a065708d942c8d.png)'
- en: Top 10 chunks sorted by their re-rank score with the question.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 按照与问题的重新排序得分排序的前 10 个块。
- en: Comparing the two tables above (first sorted by *sim_score* vs now by *rerank_score*),
    there are some clear differences. Sorting by the plain similarity score ( *sim_score*)
    from embeddings, the [Tenor page](https://en.wikipedia.org/wiki/Tenor_(website))
    is the 5th most similar chunk. Since Tenor appears to be a GIF search engine hosted
    by Google, I guess it makes some sense to see its embeddings close to the question
    “ *what is google bard?* “. But it has nothing really to do with Bard itself,
    except that Tenor is a Google product in a similar domain.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 比较上面的两个表格（第一个按*sim_score*排序，现按*rerank_score*排序），可以看到一些明显的差异。按嵌入生成的普通相似性得分（*sim_score*）排序，
    [Tenor页面](https://en.wikipedia.org/wiki/Tenor_(website)) 是第5个最相似的片段。由于Tenor似乎是一个由Google托管的GIF搜索引擎，我想看到它的嵌入与问题“*what
    is google bard?*”接近是有道理的。但它实际上与Bard本身没有什么关系，只是Tenor是一个在类似领域的Google产品。
- en: However, after sorting by the *rerank_score*, the results make much more sense.
    Tenor is gone from the top 10, and only the last two chunks from the top 10 list
    appear to be unrelated. These are about the names “Bard” and “Bård”. Possibly
    because the best source of information on Google Bard appears to be the [page
    on Google Bard](https://en.wikipedia.org/wiki/Bard_(chatbot)), which in the above
    tables is document with id 6026776\. After that I guess RAG runs out of good article
    matches and goes a bit off-road (Bård). Which is also seen in the negative re-rank
    scores for those two last rows/chunks of the table.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在按*rerank_score*排序后，结果更有意义。Tenor从前10名中消失了，前10名列表中的最后两个片段似乎不相关。这些片段关于“Bard”和“Bård”的名字。可能是因为有关Google
    Bard的最佳信息来源似乎是 [Google Bard页面](https://en.wikipedia.org/wiki/Bard_(chatbot))，在上述表格中这是id为6026776的文档。之后，我猜RAG用完了好的文章匹配，并有些偏离了正轨（Bård）。这也可以从表格最后两行/片段的负面重新排序得分中看到。
- en: Typically there would likely be many relevant documents and chunks across those
    documents, not just the 1 document and 8 chunks as above. But in this case this
    limitation helps illustrate the difference in basic embeddings-based similarity
    search and re-ranking, and how re-ranking can positively affect the end result.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 通常会有许多相关文档和文档中的片段，而不仅仅是上面提到的1份文档和8个片段。但是在这种情况下，这种限制有助于说明基于基本嵌入的相似性搜索和重新排序之间的区别，以及重新排序如何积极地影响最终结果。
- en: Building the Context
  id: totrans-78
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建上下文
- en: What do we do once we have collected the top chunks for RAG input? We need to
    build the context for the generator model from these chunks. At its simplest,
    this is just a concatenation of the selected top chunks into a long text sequence.
    The maximum length of this sequence in constrained by the used model. As I used
    the [Zephyr 7B model](https://huggingface.co/HuggingFaceH4/zephyr-7b-beta), I
    used 4096 tokens as the maximum length. The [Zephyr page](https://huggingface.co/HuggingFaceH4/zephyr-7b-beta)
    gives this as a flexible sequence limit (with sliding attention window). Longer
    context seems better, but it appears [this is not always the case](https://www-cs.stanford.edu/~nfliu/papers/lost-in-the-middle.arxiv2023.pdf).
    Better try it.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们收集了RAG输入的顶级片段后，我们该怎么做？我们需要从这些片段中为生成模型构建上下文。最简单的方法就是将选择的顶级片段连接成一个长文本序列。该序列的最大长度受所用模型的限制。由于我使用了
    [Zephyr 7B模型](https://huggingface.co/HuggingFaceH4/zephyr-7b-beta)，所以我将4096个标记作为最大长度。
    [Zephyr页面](https://huggingface.co/HuggingFaceH4/zephyr-7b-beta)将此作为一个灵活的序列限制（带有滑动注意窗口）。更长的上下文似乎更好，但
    [这并不总是如此](https://www-cs.stanford.edu/~nfliu/papers/lost-in-the-middle.arxiv2023.pdf)。最好尝试一下。
- en: 'Here is the base code I used to generate the answer with this context:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是我用来生成具有此上下文的答案的基本代码：
- en: '[PRE11]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: As noted, in this case the context was just a concatenation of the top ranked
    chunks.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，在这种情况下，上下文只是将排名最高的片段连接起来。
- en: Generating the Answer
  id: totrans-83
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 生成答案
- en: 'For comparison, first lets try what the model answers without any added context,
    i.e. based on its training data alone:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 进行比较，首先让我们尝试模型在没有任何额外上下文的情况下的回答，即仅基于其训练数据：
- en: '[PRE12]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'This gives (one of many runs, slight variations but generally similar):'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 这给出了（多次运行之一，略有变化但通常相似）：
- en: '[PRE13]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Generally accurate, but missing much of the latest developments. In comparison,
    lets try with providing the generated context to the question:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 通常来说，虽然准确，但缺乏最新的发展。相比之下，我们可以尝试将生成的上下文提供给问题：
- en: '[PRE14]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The following is an example answer with the top *sim_score* sorted chunks as
    context (includes the Tenor and Bård page chunks):'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一个示例答案，使用了按*sim_score*排序的片段作为上下文（包括Tenor和Bård页面片段）：
- en: '[PRE15]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: This is not a very good answer since it starts talking about completely non-related
    topics here, *Tenor* and *Bård*. Partly because in this case the Tenor chunk is
    included in the context, and chunk order also generally less optimal as it is
    not re-ranked.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 这不是一个很好的答案，因为它开始谈论完全无关的话题，*Tenor*和*Bård*。部分原因是因为在这种情况下，Tenor块被包含在上下文中，块的顺序也通常较差，因为没有重新排序。
- en: 'In comparison, with *rerank_score* sorted context chunks (better chunk ordering
    and Tenor gone):'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，使用*rerank_score*排序的上下文块（更好的块排序和Tenor消失）：
- en: '[PRE16]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Now the unrelated topics are gone and the answer in general is better and more
    to the point.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 现在不相关的主题已经消失，答案总体上更好，更切题。
- en: This highlights that it is not only important to find proper context to give
    to the model, but also to trim out the unrelated context. At least in this case,
    the Zephyr model was not able to directly identify which part of the context was
    relevant, but rather seems to have summarized the it all. Cannot really fault
    the model, as I gave it that context and asked to use it.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 这突出了不仅要找到适当的上下文以提供给模型，而且还要去除无关的上下文。在这种情况下，Zephyr模型似乎无法直接识别哪个部分的上下文是相关的，而是似乎对所有内容进行了总结。不能真正责怪模型，因为我提供了这些上下文并要求它使用这些内容。
- en: Looking at the re-rank scores for the chunks, a general filtering approach based
    on metrics such as negative re-rank scores would have solved this issue also in
    the above case, as the “bad” chunks in this case have a negative re-rank score.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 查看块的重新排序分数，基于诸如负面重新排序分数等指标的一般过滤方法也可以解决上述问题，因为在这种情况下“坏”块具有负面重新排序分数。
- en: Something to note is that Google released a new and much improved *Gemini* family
    of models for Bard, around the time I was writing this article. It is not mentioned
    in the generated answers here since the Wikipedia dumps are generated with a slight
    delay. So as one might imagine, it is important to try to have up-to-date information
    in the context, and to keep it relevant and focused.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，Google在我写这篇文章时发布了一个全新的、显著改进的*Gemini*模型系列。由于维基百科的内容生成有一些延迟，因此这里生成的答案没有提到这个模型。因此，如人们所想，尝试保持上下文的信息是最新的，并保持其相关性和重点是很重要的。
- en: Visual Embedding Check
  id: totrans-99
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 可视化嵌入检查
- en: Embeddings are a great tool, but sometimes it is a bit difficult to really grasp
    how they are working, and what is happening with the similarity search. A basic
    approach is to plot the embeddings against each other to get some insight into
    their relations.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入是一个很好的工具，但有时确实很难真正理解它们是如何工作的，以及相似度搜索发生了什么。一个基本的方法是将嵌入彼此绘制，以获得一些关于它们关系的见解。
- en: 'Building such a visualization is quite simple with [PCA](https://en.wikipedia.org/wiki/Principal_component_analysis)
    and visualization libraries. It involves mapping the embedding vectors to 2 or
    3 dimensions, and plotting the results. Here I map from those 384 dimensions to
    2, and plot the result:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 构建这样的可视化使用[PCA](https://en.wikipedia.org/wiki/Principal_component_analysis)和可视化库是相当简单的。它涉及将嵌入向量映射到2维或3维，并绘制结果。在这里，我将这384维映射到2维，并绘制了结果：
- en: '[PRE17]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'For the top 10 articles in the “ *what is google bard?* “ question, this gives
    the following visualization:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 对于“*what is google bard?*”问题的前10篇文章，这里给出了以下可视化：
- en: '![](../Images/67b91c3adabe96243a9d5e60010483b7.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/67b91c3adabe96243a9d5e60010483b7.png)'
- en: PCA-based 2D plot of question embeddings vs article 1st chunk embeddings.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 基于PCA的2D绘图，比较问题嵌入与文章第一个块嵌入。
- en: In this plot, the red dot is the embedding for the question “ *what is google
    bard?*”. The blue dots are the closest Wikipedia article matches, according to
    *sim_score*.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个图中，红点是问题“*what is google bard?*”的嵌入。蓝点是根据*sim_score*找到的最接近的维基百科文章匹配项。
- en: The [Bard article](https://en.wikipedia.org/wiki/Bard_(chatbot)) is obviously
    the closest one to the question, while the rest are a bit further off. The [Tenor
    article](https://en.wikipedia.org/wiki/Tenor_(website)) seems to be about second
    closest, while the [Bård one](https://en.wikipedia.org/wiki/B%C3%A5rd) is a bit
    further away, possibly due to the loss of information in mapping from 384 dimensions
    to 2\. Due to this, the visualization is not perfectly accurate but helpful for
    quick human overview.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '[Bard文章](https://en.wikipedia.org/wiki/Bard_(chatbot))显然是与问题最接近的，而其他的则稍远一些。[Tenor文章](https://en.wikipedia.org/wiki/Tenor_(website))似乎是第二接近的，而[Bård文章](https://en.wikipedia.org/wiki/B%C3%A5rd)则稍远一些，可能是因为从384维映射到2维时信息的丢失。由于这一点，可视化并不是完全准确的，但对快速人工概览是有帮助的。'
- en: 'The following figure illustrates an actual error finding from my Kaggle code
    using a similar PCA plot. Looking for a bit of insights, I tried a simple question
    about the first article in the Wikipedia dump (“ *Anarchism*”). With the question
    “ *what is the definition of anarchism?* “ . The following is what the PCA visualization
    looked like for the closest articles, the marked outliers are perhaps the most
    interesting part:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了我在Kaggle代码中发现的实际错误，使用了类似的PCA图。为了获取一些见解，我对维基百科转储中的第一篇文章（“ *无政府主义*”）提出了一个简单的问题：“
    *无政府主义的定义是什么？*”。下面是PCA可视化的结果，标记的离群点可能是最有趣的部分：
- en: '![](../Images/200d00bcdabbf0f98d58bdd7cf0f5802.png)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/200d00bcdabbf0f98d58bdd7cf0f5802.png)'
- en: My fail shown in PCA-based 2D plot of Kaggle embeddings for selected top documents.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 我在PCA基于2D图的Kaggle嵌入中显示的失败，针对所选的顶级文档。
- en: The red dot in the bottom left corner is again the question. The cluster of
    blue dots next to it are all related articles about anarchism. And then there
    are the two outlier dots on the top right. I removed the titles from the plot
    to keep it readable. The two outlier articles seemed to have nothing to do with
    the question when looking.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 左下角的红点再次表示问题。旁边的蓝点簇是所有与无政府主义相关的文章。然后右上角有两个离群点。我删除了图表中的标题以保持其可读性。查看时，这两个离群文章似乎与问题无关。
- en: Why is this? As I indexed the articles with various chunk sizes of 512, 256,
    128, and 64, I had some issues in processing all the articles for 256 chunk size,
    and restarted the chunking in the middle. This resulted in some differences in
    indices of some of those embeddings vs the chunk texts I had stored. After noticing
    these strange looking results, I re-calculated the embeddings with the 256 token
    chunk size, and compared the results vs size 512, noted this difference. Too bad
    the competition was done at that time 🙂
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么会这样？由于我使用了512、256、128和64的各种块大小来索引文章，在处理256块大小的所有文章时遇到了一些问题，并在中途重新启动了分块。这导致某些嵌入与我存储的块文本的索引有所不同。在注意到这些奇怪的结果后，我重新计算了256个令牌块大小的嵌入，并将结果与512大小进行比较，注意到这个差异。可惜那时比赛已经结束🙂
- en: More Advanced Context Selection
  id: totrans-113
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 更高级的上下文选择
- en: In the above I discussed chunking the documents and using similarity search
    + re-ranking as a method to find relevant chunks and build a context for the question
    answering. I found sometimes it is also useful to consider how the initial documents
    to chunk are selected vs just the chunks themselves.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 上述内容讨论了将文档分块并使用相似度搜索+重新排序作为找到相关块和构建问题回答上下文的方法。我发现有时也有必要考虑初始文档的选择方式，而不仅仅是块本身。
- en: 'As example methods, the [advanced RAG](https://www.deeplearning.ai/short-courses/building-evaluating-advanced-rag/)
    course on [DeepLearning.AI](https://www.deeplearning.ai/) , presents two approaches:
    sentence windowing, and hierarchical chunk merging. In summary this looks at nearby-chunks
    and if multiple are ranked high by their scores, takes them as a single large
    chunk. The “hierarchy” coming from considering larger and larger chunk combinations
    for joint relevance. Aiming for more cohesive context vs random ordered small
    chunks, giving the generator LLM better input to work with.'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 作为示例方法，[高级RAG](https://www.deeplearning.ai/short-courses/building-evaluating-advanced-rag/)课程在[DeepLearning.AI](https://www.deeplearning.ai/)上介绍了两种方法：句子窗口化和层次块合并。总结来说，这种方法查看附近的块，如果多个块的分数很高，则将它们作为一个大的块。所谓“层次结构”是通过考虑越来越大的块组合来共同相关。旨在提供更连贯的上下文，而不是随机排序的小块，给生成LLM更好的输入。
- en: 'As a simple example of this, here is the re-ranked set of top chunks for my
    above Bard example:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一个简单的示例，这是我上面Bard示例的重新排序的前几个块：
- en: '![](../Images/fa17b950e30b9f3b12c8cb936393317c.png)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fa17b950e30b9f3b12c8cb936393317c.png)'
- en: Top 10 chunks for my Bard example, sorted by rerank_score.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 我在Bard示例中的前10个块，按重新排序分数排序。
- en: The leftmost column here is the index of the chunk. In my generation, I just
    took the top chunks in this sorted order as in the table. If we wanted to make
    the context a bit more coherent, we could sort the final selected chunks by their
    order within a document. If there is a small piece missing between highly ranked
    chunks, adding the missing one (e.g., here chunk id 7) could help in missing gaps,
    similar to the hierarchical merging. This could be something to try as a final
    step for final gains.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 这里最左侧的列是块的索引。在我的生成中，我只是按表中的排序顺序取了顶级块。如果我们想使上下文更连贯，我们可以按文档中的顺序对最终选择的块进行排序。如果在高度排名的块之间有小片段缺失，添加缺失的部分（例如这里的块
    ID 7）可能有助于填补空白，类似于层次合并。这可能是作为最终步骤进行尝试的内容，以获得最终的改进。
- en: In my Kaggle experiments, I performed initial document selection based on the
    first chunk only. In part due to Kaggle’s resource limits, but it appeared to
    have some other advantages as well. Typically, an article’s beginning acts as
    a summary (introduction or abstract). Initial chunk selection from such ranked
    articles may help select chunks with more relevant overall context.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 在我的 Kaggle 实验中，我仅基于第一个块进行初步文档选择。这部分是由于 Kaggle 的资源限制，但它似乎也有一些其他的优势。通常，一篇文章的开头部分充当了总结（引言或摘要）。从这些排名文章中进行初步块选择可能有助于选择具有更相关整体上下文的块。
- en: This is visible in my Bard example above, where both the *rerank_score* and
    *sim_score* are highest for the first chunk of the best article. To try to improve
    this, I also tried using a larger chunk size for this initial document selection,
    to include more of the introduction for better relevance. Then chunked the top
    selected documents with smaller chunk sizes for experimenting on how good the
    context is with each size.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 在我上面的 Bard 示例中可以看到，无论是 *rerank_score* 还是 *sim_score*，对于最佳文章的第一个块都是最高的。为了改进这一点，我还尝试使用更大的块大小进行初始文档选择，以包括更多的引言以提高相关性。然后将顶级选择的文档按较小的块大小进行切分，以实验每种大小的上下文效果。
- en: While I could not run the initial search on all chunks of all documents on Kaggle
    due to resource limitations, I tried it outside of Kaggle. In these trials, I
    noticed that sometimes single chunks of unrelated articles get ranked high, while
    in reality misleading for the answer generation. For example, actor biography
    in a related movie. Initial document relevance selection may help avoid this.
    Unfortunately, I did not have time to study this further with different configurations,
    and good re-ranking may already help.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 由于资源限制，我无法在 Kaggle 上对所有文档的所有块进行初始搜索，但我在 Kaggle 外部尝试了一下。在这些试验中，我发现有时单个不相关的文章块会被排名较高，而实际上对答案生成存在误导。例如，与相关电影的演员传记。初步的文档相关性选择可能有助于避免这种情况。不幸的是，我没有时间用不同的配置进一步研究这个问题，好的重排名可能已经有帮助。
- en: Finally, repeating the same information in multiple chunks in the context is
    not very useful. Top ranking of the chunks does not guarantee that they best complement
    each other, or best chunk diversity. For example, LangChain has a special chunk
    selector for [Maximum Marginal Relevance](https://python.langchain.com/docs/modules/model_io/prompts/example_selectors/mmr).
    It does this by penalizing new chunks by how close they are to the already added
    chunks.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，在上下文中重复相同的信息在多个块中不是很有用。块的最高排名并不保证它们彼此最好地补充，或最佳块多样性。例如，LangChain 为 [最大边际相关性](https://python.langchain.com/docs/modules/model_io/prompts/example_selectors/mmr)
    提供了一个特殊的块选择器。它通过对新块进行惩罚，惩罚依据是它们与已添加块的相似度来实现这一点。
- en: Extending the RAG Query
  id: totrans-124
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 扩展 RAG 查询
- en: 'I used a very simple question / query for my RAG example here (“ *what is google
    bard?*”), and simple is good to illustrate the basic RAG concept. This is a pretty
    short query input considering that the embedding model I used had a 512 token
    maximum sequence length. If I encode this question into tokens using the tokenizer
    for the embedding model ( *bge-small-en*), I get the following tokens:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 我在这里使用了一个非常简单的问题/查询作为我的 RAG 示例（“ *what is google bard?*”），简单的查询有助于说明基本的 RAG
    概念。考虑到我使用的嵌入模型具有 512 令牌的最大序列长度，这个查询输入相当简短。如果我使用嵌入模型的分词器（*bge-small-en*）将这个问题编码成令牌，我会得到以下令牌：
- en: '[PRE18]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Which amounts to a total of 7 tokens. With a maximum sequence length of 512,
    this leaves plenty of room if I want to use a longer query sentence. Sometimes
    this can be useful, especially if the information we want to retrieve is not such
    a simple query, or if the domain is more complex. For a very small query, the
    semantic search may not work best, as noted also in the [Stack Overflows AI Journey
    posting](https://stackoverflow.blog/2023/07/31/ask-like-a-human-implementing-semantic-search-on-stack-overflow/).
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 这总共是 7 个标记。最大序列长度为 512，这为我使用更长的查询句子留出了足够的空间。有时这很有用，特别是当我们想要检索的信息不是简单的查询，或者领域较复杂时。对于非常简单的查询，语义搜索可能效果不好，正如在[Stack
    Overflows AI Journey 文章](https://stackoverflow.blog/2023/07/31/ask-like-a-human-implementing-semantic-search-on-stack-overflow/)中提到的那样。
- en: For example, the Kaggle competition had a set of questions, each with 5 answer
    options to pick from. I initially tried RAG with just the question as the input
    for the embedding model. The search results were not too great, so I tried again
    with the question + all the answer options as the query. This produced much better
    results.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，Kaggle 比赛有一组问题，每个问题有 5 个答案选项可以选择。我最初尝试了仅将问题作为嵌入模型的输入的 RAG。搜索结果并不理想，因此我再次尝试了将问题
    + 所有答案选项作为查询。这产生了更好的结果。
- en: 'As an example, the first question in the training dataset of the competition:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一个例子，比赛训练数据集中的第一个问题：
- en: '[PRE19]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: This is 32 tokens for the *bge-small-en* model. So about 480 still left to fit
    into the maximum 512 token sequence length.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 *bge-small-en* 模型，这需要 32 个标记。因此，最大 512 个标记的序列长度还剩大约 480 个标记可以填充。
- en: 'Here is the first question along with the 5 answer options given for it:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是第一个问题及其给出的 5 个答案选项：
- en: '![](../Images/52216d4403a7ac4827efa1264b76eb88.png)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/52216d4403a7ac4827efa1264b76eb88.png)'
- en: Example question and answer options A-E. Concatenating all these texts formed
    the query.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 示例问题和答案选项 A-E。将所有这些文本合并形成了查询。
- en: Concatenating the question and the given options into one RAG query gives this
    a length 235 tokens, with still more than 50% of embedding model sequence length
    left. In my case, this approach produced much better results. Both from manual
    inspection, and for the competition score. Thus, experimenting with different
    ways to make the RAG query itself more expressive is worth a try.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 将问题和给定的选项合并成一个 RAG 查询，得到的长度为 235 个标记，并且仍然有超过 50% 的嵌入模型序列长度剩余。就我而言，这种方法产生了更好的结果，无论是通过人工检查，还是比赛分数。因此，尝试不同的方式使
    RAG 查询本身更具表现力是值得的。
- en: Hallucinations
  id: totrans-136
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 幻觉
- en: Finally, there is the topic of [hallucinations](https://machinelearningmastery.com/a-gentle-introduction-to-hallucinations-in-large-language-models/),
    where the model produces text that is incorrect or fabricated. The Tenor example
    from my *sim_score* sorting is one kind of an example, even if the generator did
    base it on the actual given context. So better keep the context good I guess :).
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 最后是关于[幻觉](https://machinelearningmastery.com/a-gentle-introduction-to-hallucinations-in-large-language-models/)的话题，即模型生成的文本不正确或虚构。我的
    *sim_score* 排序中的 Tenor 示例就是一个例子，即使生成器确实基于实际的给定上下文。因此，我想最好还是保持上下文良好 :).
- en: To address hallucinations, the chatbots from the big AI companies ( [Google
    Bard](https://bard.google.com/chat), [ChatGPT](https://chat.openai.com/), [Bing
    Chat](https://www.bing.com/chat)) all provide means to link parts of their generated
    answers to verifiable sources. [Bard](https://bard.google.com/chat) has a specific
    “G” button that performs a Google search and highlights parts of the generated
    answer that match the search results. Too bad we do not always have a world-class
    search-engine for our data to help.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决幻觉问题，大型 AI 公司（[Google Bard](https://bard.google.com/chat)，[ChatGPT](https://chat.openai.com/)，[Bing
    Chat](https://www.bing.com/chat)）的聊天机器人都提供了将其生成的回答部分链接到可验证来源的方法。[Bard](https://bard.google.com/chat)
    有一个特定的 “G” 按钮，可以执行 Google 搜索，并突出显示与搜索结果匹配的生成回答部分。可惜我们并不总是拥有世界级的搜索引擎来帮助处理我们的数据。
- en: '[Bing Chat](https://www.bing.com/chat) has a similar approach, highlighting
    parts of the answer and adding a reference to the source websites. [ChatGPT](https://chat.openai.com/)
    has a slightly different approach; I had to explicitly ask it to verify its answer
    and update with latest developments, telling it to use its browser tool. After
    this, it did an internet search and linked to specific websites as sources. The
    source quality seemed to vary quite a bit as in any internet search. Of course,
    for internal documents this type of web search is not possible. However, linking
    to the source should always be possible even internally.'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '[Bing Chat](https://www.bing.com/chat)采用了类似的方法，突出显示答案的部分并添加对源网站的引用。[ChatGPT](https://chat.openai.com/)的方法略有不同；我必须明确要求它验证其答案并更新最新进展，告诉它使用其浏览器工具。之后，它进行了互联网搜索并链接到特定网站作为来源。源的质量似乎有很大的变化，就像任何互联网搜索一样。当然，对于内部文档，这种类型的网络搜索是不可能的。然而，即使在内部，也应该始终可以链接到来源。'
- en: I also asked Bard, ChatGPT+, and Bing for ideas on detecting hallucinations.
    The results included an LLM hallucination [ranking index](https://www.rungalileo.io/blog/hallucination-index),
    including [RAG hallucination](https://www.rungalileo.io/hallucinationindex). When
    tuning LLM’s, it might also help to set the [temperature parameter to zero](https://txt.cohere.com/llm-parameters-best-outputs-language-ai/)
    for the LLM to generate deterministic, most probable output tokens.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 我还询问了Bard、ChatGPT+和Bing关于检测幻觉的想法。结果包括一个LLM幻觉[排名指数](https://www.rungalileo.io/blog/hallucination-index)，以及[RAG幻觉](https://www.rungalileo.io/hallucinationindex)。在调优LLM时，将[温度参数设为零](https://txt.cohere.com/llm-parameters-best-outputs-language-ai/)可能有助于LLM生成确定性的、最可能的输出令牌。
- en: Finally, as this is a very common problem, there seem to be various approaches
    being built to address this challenge a bit better. For example, specific [LLM’s
    to help detect halluciations](https://huggingface.co/blog/dhuynh95/automatic-hallucination-detection)
    seem to be a promising area. I did not have time to try them, but certainly relevant
    in bigger projects.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，由于这是一个非常常见的问题，似乎有各种方法正在被构建以更好地解决这一挑战。例如，特定的[LLM来帮助检测幻觉](https://huggingface.co/blog/dhuynh95/automatic-hallucination-detection)似乎是一个有前途的领域。我没有时间尝试它们，但在更大的项目中肯定是相关的。
- en: Evaluating Results
  id: totrans-142
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评估结果
- en: Besides implementing a working RAG solution, it is also nice to be able to tell
    something about how well it works. In the Kaggle competition this was quite simple.
    I just ran the solution to try to answer the given questions in the training dataset,
    comparing to the correct answers given in the training data. Or submitted the
    model for scoring on the Kaggle competition test set. The better the answer score,
    the better one could call the RAG solution, even if there was more to the score.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 除了实现一个有效的RAG解决方案之外，能够评估它的效果也是很有价值的。在Kaggle比赛中，这相当简单。我只是运行解决方案以尝试回答训练数据集中的给定问题，并将其与训练数据中提供的正确答案进行比较。或者将模型提交到Kaggle比赛测试集进行评分。答案分数越高，RAG解决方案就越好，即使分数背后还有更多内容。
- en: In many cases, a suitable evaluation dataset for domain specific RAG may not
    be available. For this scenario, one might want to start with some generic NLP
    evaluation datasets, such as [this list](https://paperswithcode.com/task/question-answering#:~:text=Popular%20benchmark%20datasets%20for%20evaluation%20question%20answering%20systems%20include%20SQuAD,models%20are%20T5%20and%20XLNet.).
    Tools such as LangChain also come with [support for auto-generating questions
    and answers](https://blog.langchain.dev/auto-eval-of-question-answering-tasks/),
    and evaluating them. In this case, an LLM is used to create example questions
    and answers for a given set of documents, and another LLM is used to evaluate
    whether the RAG can provide the correct answer to these questions. This is perhaps
    better explained in this [tutorial on RAG evaluation with LangChain](https://learn.deeplearning.ai/langchain/lesson/6/evaluation).
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多情况下，可能没有适用于领域特定RAG的合适评估数据集。对于这种情况，可以考虑从一些通用NLP评估数据集开始，例如[这个列表](https://paperswithcode.com/task/question-answering#:~:text=Popular%20benchmark%20datasets%20for%20evaluation%20question%20answering%20systems%20include%20SQuAD,models%20are%20T5%20and%20XLNet.)。像LangChain这样的工具还提供了[自动生成问题和答案](https://blog.langchain.dev/auto-eval-of-question-answering-tasks/)并进行评估的支持。在这种情况下，使用一个LLM为给定文档集创建示例问题和答案，另一个LLM用于评估RAG是否能够提供这些问题的正确答案。也许可以在这个[LangChain的RAG评估教程](https://learn.deeplearning.ai/langchain/lesson/6/evaluation)中更好地解释。
- en: While the generic solutions are likely good to start with, in a real project
    I would try to collect a real dataset of questions and answers from the domain
    experts and the intended users of the RAG solution. As the LLM is typically expected
    to generate a natural language response, this can vary a lot while still being
    correct. For this reason, evaluating if the answer was correct or not is not as
    straightforward as a regular expression or similar pattern matching. Here, I find
    the idea of using another LLM to evaluate whether the given response matches a
    reference response a very useful tool. These models can deal with the text variation
    much better.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然通用解决方案在开始时可能很好，但在实际项目中，我会尝试收集来自领域专家和目标用户的真实问题和答案数据集。由于大型语言模型（LLM）通常被期望生成自然语言响应，这些响应可能在正确的前提下变化很大。因此，评估答案是否正确不像正则表达式或类似的模式匹配那么直接。在这种情况下，我发现使用另一种LLM来评估给定的响应是否匹配参考响应是一个非常有用的工具。这些模型能够更好地处理文本变异。
- en: Conclusions
  id: totrans-146
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: RAG is a very nice tool, and is quite a popular topic these days with the high
    interest in LLM’s in general. While RAG and embeddings have been around for a
    good while, the latest powerful LLM’s and their fast evolution have perhaps made
    them more interesting for many advanced use cases. I expect the field to keep
    evolving at a good pace, and it is sometimes a bit difficult to keep up to date
    on everything. For this, summaries such as reviews on [RAG developments](https://arxiv.org/pdf/2312.10997.pdf)
    can give points to at least keep the main developments in sight.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: RAG 是一个非常不错的工具，随着对LLM的高度关注，它现在也是一个相当热门的话题。虽然RAG和嵌入技术已经存在了很长时间，但最新的强大LLM及其快速演变可能使它们在许多高级应用场景中更具吸引力。我预计这一领域将持续以良好的速度发展，有时很难跟上所有最新动态。为此，像[RAG
    发展综述](https://arxiv.org/pdf/2312.10997.pdf)这样的总结可以提供至少保持主要发展方向的参考。
- en: 'The RAG approach in general is quite simple: find a set of chunks of text similar
    to the given query, concatenate them into a context, and ask the LLM for an answer.
    However, as I tried to show here, there can be various issues to consider in how
    to make this work well and efficiently for different needs. From good context
    retrieval, to ranking and selecting the best results, and finally being able to
    link the results back to actual source documents. And evaluating the resulting
    query contexts and answers. And as [Stack Overflow people noted](https://stackoverflow.blog/2023/07/31/ask-like-a-human-implementing-semantic-search-on-stack-overflow/),
    sometimes the more traditional lexical or hybrid search is very useful as well,
    even if semantic search is cool.'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，RAG 方法相当简单：找到与给定查询类似的一组文本块，将它们拼接成上下文，然后向LLM请求答案。然而，正如我在这里尝试展示的那样，在如何使这一过程对不同需求有效且高效方面，可能会有各种问题需要考虑。从良好的上下文检索，到排名和选择最佳结果，最后能够将结果链接回实际的源文档。还要评估生成的查询上下文和答案。正如[Stack
    Overflow 的人们指出的](https://stackoverflow.blog/2023/07/31/ask-like-a-human-implementing-semantic-search-on-stack-overflow/)，有时更传统的词汇搜索或混合搜索也非常有用，即使语义搜索也很酷。
- en: That’s all for today. RAG on…
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 今天就到这里。RAG继续...
- en: '![](../Images/852104944e2f6138ecb58522d8e91819.png)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/852104944e2f6138ecb58522d8e91819.png)'
- en: ChatGPT+/DALL-E3 vision of what it means to RAG on..
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: ChatGPT+/DALL-E3 对 RAG 的理解..
- en: '*Originally published at* [*http://teemukanstren.com*](https://teemukanstren.com/2023/12/25/llmrag-based-question-answering/)
    *on December 25, 2023.*'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '*最初发布于* [*http://teemukanstren.com*](https://teemukanstren.com/2023/12/25/llmrag-based-question-answering/)
    *于2023年12月25日。*'
