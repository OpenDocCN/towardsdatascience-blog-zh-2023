- en: Improving Hebrew Q&A Models via Intelligent Prompting
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/improving-hebrew-q-a-models-via-prompting-20f5fb5a2f36?source=collection_archive---------8-----------------------#2023-03-17](https://towardsdatascience.com/improving-hebrew-q-a-models-via-prompting-20f5fb5a2f36?source=collection_archive---------8-----------------------#2023-03-17)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[](https://medium.com/@erap129?source=post_page-----20f5fb5a2f36--------------------------------)[![Elad
    Rapaport](../Images/2ad958f92cd8b5735da900ae8f5559f3.png)](https://medium.com/@erap129?source=post_page-----20f5fb5a2f36--------------------------------)[](https://towardsdatascience.com/?source=post_page-----20f5fb5a2f36--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----20f5fb5a2f36--------------------------------)
    [Elad Rapaport](https://medium.com/@erap129?source=post_page-----20f5fb5a2f36--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fd2d1ff8f0490&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimproving-hebrew-q-a-models-via-prompting-20f5fb5a2f36&user=Elad+Rapaport&userId=d2d1ff8f0490&source=post_page-d2d1ff8f0490----20f5fb5a2f36---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----20f5fb5a2f36--------------------------------)
    ·13 min read·Mar 17, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F20f5fb5a2f36&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimproving-hebrew-q-a-models-via-prompting-20f5fb5a2f36&user=Elad+Rapaport&userId=d2d1ff8f0490&source=-----20f5fb5a2f36---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F20f5fb5a2f36&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimproving-hebrew-q-a-models-via-prompting-20f5fb5a2f36&source=-----20f5fb5a2f36---------------------bookmark_footer-----------)![](../Images/460a2dc30eb62d6e636e117e21cf5c2f.png)'
  prefs: []
  type: TYPE_NORMAL
- en: 'DALL-E 2: “A robot dressed in a robe writing biblical sentences in Hebrew.
    Digital art.”'
  prefs: []
  type: TYPE_NORMAL
- en: Hi all,
  prefs: []
  type: TYPE_NORMAL
- en: I am sharing a short project I worked on that involves improving the performance
    of the `text-davinci-003` model by OpenAI using intelligent prompting. I will
    start by saying that this work is inspired by the excellent video tutorials by
    [James Briggs](https://www.youtube.com/@jamesbriggs/featured) — (specifically
    this one — [https://youtu.be/dRUIGgNBvVk](https://youtu.be/dRUIGgNBvVk)) and much
    of the code I used is taken from his examples as well.
  prefs: []
  type: TYPE_NORMAL
- en: 'The outline of this article is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Intro — Unique and Proprietary data in Large Language Models (LLMs)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Problem Statement — Answering Questions in Hebrew
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Solution Suggestion — Prompting & Querying
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Experiment — Question Answering in Hebrew
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Conclusions
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A Google Colab notebook with the complete code is here — [https://colab.research.google.com/drive/1_UqPHGPW1yLf3O_BOySRjf3bWqMe6A4H#scrollTo=4DY7XgilIr-H](https://colab.research.google.com/drive/1_UqPHGPW1yLf3O_BOySRjf3bWqMe6A4H#scrollTo=4DY7XgilIr-H)
  prefs: []
  type: TYPE_NORMAL
- en: Part 1\. Problem Statement — Unique and Proprietary data in Large Language Models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The main official language in Israel is Hebrew, and as an Israeli, I am interested
    in improving the performance of large language models in Hebrew. It is a widely
    known fact that the performance of ChatGPT in Hebrew could be better relative
    to its performance in English, despite recent improvements. In this article, though,
    Hebrew is only an allegory to a source of data that is under-represented in a
    large language model and this article is going to deal with an attempt to assist
    an LLM in yielding useful results which are based on such data.
  prefs: []
  type: TYPE_NORMAL
- en: One option, which was common before the era of very large language models, was
    to perform additional training on the model with the new data/task. When the models
    were at a scale of millions of parameters this option was a viable approach for
    many people and use cases and was achievable using common consumer hardware.
  prefs: []
  type: TYPE_NORMAL
- en: In the past several years there has been an explosion in model sizes for large
    language models, which nowadays reach hundreds of billions of parameters. On the
    other hand, consumer hardware has not seen such a massive efficiency explosion.
    Thus, the option for fine-tuning these models has been taken off the table for
    most users and organizations.
  prefs: []
  type: TYPE_NORMAL
- en: ChatGPT is widely known already and it achieves remarkable performance on many
    tasks and is proving to be an invaluable AI assistant. Yet still, if I want to
    use this technology to assist a person speaking in a rare language, or to assist
    a company with proprietary data which is not available online — chances are that
    ChatGPT will fail for obvious reasons of not seeing the data beforehand. So how
    do we make use of special/proprietary data in these models, without requiring
    millions of dollars for training using special hardware? One option is prompting
    via querying, which we will explore today.
  prefs: []
  type: TYPE_NORMAL
- en: '**Part 2\.** Problem Statement — Answering Questions in Hebrew'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will use the OpenAI API to answer three questions in Hebrew, which were inspired
    by the [SQuAD](https://rajpurkar.github.io/SQuAD-explorer/explore/v2.0/dev/) dataset
    and translated into Hebrew. The following code snippet shows the questions, expected
    answers, and English translations.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '**Disclaimer — the recent version of ChatGPT proves to yield very good answers
    for these queries (in Hebrew). The point of this article is not to “compete” with
    it, but rather to show a solution to a problem that might arise with other models/data.**'
  prefs: []
  type: TYPE_NORMAL
- en: 'We will use the `text-davinci-003` model to answer the questions using this
    code, which will call the OpenAI API:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: We see that `text-davinci-003` is very bad at answering these questions out
    of the box. It gets all three questions wrong and produces some ridiculous answers
    for the second and third questions.
  prefs: []
  type: TYPE_NORMAL
- en: So the question is asked — how do we solve this? How can we get good answers
    from `text-davinci-003` without having to fine-tune it on a bunch of Hebrew data?
  prefs: []
  type: TYPE_NORMAL
- en: Part 3\. Solution Suggestion — Prompting & Querying
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As I wrote in the introduction, one way to add information to the LLM without
    fine-tuning is prompting, which is simply including relevant information in the
    input to the model. But including relevant information for each query can be a
    very tedious and time-consuming task, can we automate this?
  prefs: []
  type: TYPE_NORMAL
- en: Of course, we can! Enter prompting via querying. We will take a fairly small
    dump of Wikipedia in Hebrew, which definitely contains answers to our questions.
    We will split this dump into lines, embed each of these lines using an embedding
    model by OpenAI, and insert these embeddings into a vector database called Pinecone
    DB. Then, for each question we ask the model, we will search the vector database
    for bits of information that are semantically similar to the question and append
    them to the model input as additional information. This procedure is shown in
    figure 1\. Hopefully, the model will be able to make use of the provided context
    and yield correct answers to our questions.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/37163f1f594e22773dcc5dfc74fb4106.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1\. Prompting with querying
  prefs: []
  type: TYPE_NORMAL
- en: Inspired by Jason Briggs, I will show you a prompt template that will help us
    generate intelligible prompts to `text-davinci-003` and hopefully, make it answer
    our three previous questions correctly.
  prefs: []
  type: TYPE_NORMAL
- en: Part 4\. Experiment — Question Answering in Hebrew
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: First things first — we need to populate our vector database with relevant information.
    For this purpose we are going to use a small Hebrew Wikipedia dump which I downloaded
    from here — [https://u.cs.biu.ac.il/~yogo/hebwiki/](https://u.cs.biu.ac.il/~yogo/hebwiki/)
    (available under a Creative Commons Attribution-ShareAlike 4.0 International License).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'For general information, I calculated how much it will cost to embed the whole
    Wikipedia dump using the OpenAI API — not too bad! But still, for our purposes
    we only need specific subjects so we can save some money by choosing more particular
    sentences to embed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'That’s better. We are now ready to populate the vector database:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'We generated a new Pinecone DB index (if it didn’t exist yet). Let’s populate
    it with our data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Great! We have a populated vector DB. Let’s get the top-2 contexts for the
    first question (Which city in Victoria is considered the sporting capital of Australia?):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: We see that the top 2 lines in our vector DB (in terms of cosine similarity
    to the question’s embedding) are not so helpful. The first line is a very general
    sentence about Australia and the second indeed talks about sports in Australia,
    but there is no mention of Melbourne which is the answer we are expecting.
  prefs: []
  type: TYPE_NORMAL
- en: But still, let’s challenge our model and see how it fares now that it has access
    to additional information in Hebrew. Notice that we instructed our model to answer
    “I don’t know” if the answer cannot be deduced from the given context.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: OK, getting a bit better. The first answer is still wrong. In the second and
    third answers, we got “I don’t know” which I perceive as a major improvement over
    getting wrong answers. But still, we want to give current answers, how can we
    improve on this..?
  prefs: []
  type: TYPE_NORMAL
- en: Let’s add some additional subjects to the vector DB, maybe it didn’t have enough
    relevant information from which to pull a relevant context.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Great, we added richer context (you might even say we are cheating a bit — notice
    that we added all sentences that contain the words “Melbourne” and “Charles” amongst
    others, which are the answers to our questions. But it’s just a demonstration!)
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Nice! We have our answers! Although the third answer is still not 100% correct,
    but way better than “I don’t know” and way better than the initial hallucination.
  prefs: []
  type: TYPE_NORMAL
- en: Part 5\. Conclusions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this article, we have seen how to automatically provide relevant context
    to an LLM using a vector database in order to improve the model outputs. In this
    specific case, it might not have been necessary because the performance of ChatGPT
    (which is available as an API) on Hebrew has improved tremendously and it is able
    to answer these questions very well out-of-the-box.
  prefs: []
  type: TYPE_NORMAL
- en: 'Yet still, the point of this article was to introduce this concept, which can
    be generalized for other purposes such as:'
  prefs: []
  type: TYPE_NORMAL
- en: Using an LLM to analyze proprietary data which is not available online
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Adding a form of explainability to the model, by appending the context which
    was used to the output of the user (to prove the credibility of the answer).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: I hope you enjoyed this article and were motivated to try it for yourself. See
    you next time!
  prefs: []
  type: TYPE_NORMAL
- en: Elad
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Hebrew Wikipedia dump — [https://u.cs.biu.ac.il/~yogo/hebwiki/](https://u.cs.biu.ac.il/~yogo/hebwiki/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SQuAD dataset — [https://rajpurkar.github.io/SQuAD-explorer/](https://rajpurkar.github.io/SQuAD-explorer/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Video tutorial by James Briggs — [https://youtu.be/dRUIGgNBvVk](https://youtu.be/dRUIGgNBvVk)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: OpenAI API — [https://openai.com/blog/openai-api](https://openai.com/blog/openai-api)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pinecone DB — [https://www.pinecone.io/](https://www.pinecone.io/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
