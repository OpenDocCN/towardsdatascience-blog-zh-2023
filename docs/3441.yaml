- en: 'ExLlamaV2: The Fastest Library to Run LLMs'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 'ExLlamaV2: è¿è¡ŒLLMsçš„æœ€å¿«åº“'
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/exllamav2-the-fastest-library-to-run-llms-32aeda294d26?source=collection_archive---------0-----------------------#2023-11-20](https://towardsdatascience.com/exllamav2-the-fastest-library-to-run-llms-32aeda294d26?source=collection_archive---------0-----------------------#2023-11-20)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://towardsdatascience.com/exllamav2-the-fastest-library-to-run-llms-32aeda294d26?source=collection_archive---------0-----------------------#2023-11-20](https://towardsdatascience.com/exllamav2-the-fastest-library-to-run-llms-32aeda294d26?source=collection_archive---------0-----------------------#2023-11-20)
- en: Quantize and run EXL2 models
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: é‡åŒ–å¹¶è¿è¡ŒEXL2æ¨¡å‹
- en: '[](https://medium.com/@mlabonne?source=post_page-----32aeda294d26--------------------------------)[![Maxime
    Labonne](../Images/a7efdd305e3cc77d5509bbb1076d57d8.png)](https://medium.com/@mlabonne?source=post_page-----32aeda294d26--------------------------------)[](https://towardsdatascience.com/?source=post_page-----32aeda294d26--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----32aeda294d26--------------------------------)
    [Maxime Labonne](https://medium.com/@mlabonne?source=post_page-----32aeda294d26--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@mlabonne?source=post_page-----32aeda294d26--------------------------------)[![Maxime
    Labonne](../Images/a7efdd305e3cc77d5509bbb1076d57d8.png)](https://medium.com/@mlabonne?source=post_page-----32aeda294d26--------------------------------)[](https://towardsdatascience.com/?source=post_page-----32aeda294d26--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----32aeda294d26--------------------------------)
    [Maxime Labonne](https://medium.com/@mlabonne?source=post_page-----32aeda294d26--------------------------------)'
- en: Â·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Â·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fdc89da634938&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fexllamav2-the-fastest-library-to-run-llms-32aeda294d26&user=Maxime+Labonne&userId=dc89da634938&source=post_page-dc89da634938----32aeda294d26---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----32aeda294d26--------------------------------)
    Â·6 min readÂ·Nov 20, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F32aeda294d26&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fexllamav2-the-fastest-library-to-run-llms-32aeda294d26&user=Maxime+Labonne&userId=dc89da634938&source=-----32aeda294d26---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[å…³æ³¨](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fdc89da634938&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fexllamav2-the-fastest-library-to-run-llms-32aeda294d26&user=Maxime+Labonne&userId=dc89da634938&source=post_page-dc89da634938----32aeda294d26---------------------post_header-----------)
    å‘å¸ƒäº [Towards Data Science](https://towardsdatascience.com/?source=post_page-----32aeda294d26--------------------------------)
    Â· 6åˆ†é’Ÿé˜…è¯» Â· 2023å¹´11æœˆ20æ—¥[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F32aeda294d26&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fexllamav2-the-fastest-library-to-run-llms-32aeda294d26&user=Maxime+Labonne&userId=dc89da634938&source=-----32aeda294d26---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F32aeda294d26&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fexllamav2-the-fastest-library-to-run-llms-32aeda294d26&source=-----32aeda294d26---------------------bookmark_footer-----------)![](../Images/032261661e8c8f24175b7d7766cd2fae.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F32aeda294d26&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fexllamav2-the-fastest-library-to-run-llms-32aeda294d26&source=-----32aeda294d26---------------------bookmark_footer-----------)![](../Images/032261661e8c8f24175b7d7766cd2fae.png)'
- en: Image by author
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ç‰‡ç”±ä½œè€…æä¾›
- en: Quantizing Large Language Models (LLMs) is the most popular approach to reduce
    the size of these models and speed up inference. Among these techniques, GPTQ
    delivers amazing performance on GPUs. Compared to unquantized models, this method
    uses almost 3 times less VRAM while providing a similar level of accuracy and
    faster generation. It became so popular that it has recently been directly integrated
    into the [transformers library](https://huggingface.co/blog/gptq-integration).
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è¿›è¡Œé‡åŒ–æ˜¯å‡å°‘è¿™äº›æ¨¡å‹å¤§å°å’ŒåŠ é€Ÿæ¨ç†çš„æœ€å—æ¬¢è¿çš„æ–¹æ³•ã€‚åœ¨è¿™äº›æŠ€æœ¯ä¸­ï¼ŒGPTQåœ¨GPUä¸Šçš„è¡¨ç°éå¸¸å‡ºè‰²ã€‚ä¸æœªé‡åŒ–æ¨¡å‹ç›¸æ¯”ï¼Œè¿™ç§æ–¹æ³•ä½¿ç”¨çš„æ˜¾å­˜å‡ ä¹å°‘äº†3å€ï¼ŒåŒæ—¶æä¾›äº†ç±»ä¼¼çš„å‡†ç¡®åº¦å’Œæ›´å¿«çš„ç”Ÿæˆé€Ÿåº¦ã€‚å®ƒå˜å¾—å¦‚æ­¤å—æ¬¢è¿ï¼Œä»¥è‡³äºæœ€è¿‘å·²ç»ç›´æ¥é›†æˆåˆ°[transformersåº“](https://huggingface.co/blog/gptq-integration)ä¸­ã€‚
- en: '[**ExLlamaV2**](https://github.com/turboderp/exllamav2) is a library designed
    to squeeze even more performance out of GPTQ. Thanks to new kernels, itâ€™s optimized
    for (blazingly) fast inference. It also introduces a new quantization format,
    EXL2, which brings a lot of flexibility to how weights are stored.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: In this article, we will see how to quantize base models in the EXL2 format
    and how to run them. As usual, the code is available on [GitHub](https://github.com/mlabonne/llm-course/blob/main/Quantize_models_with_ExLlamaV2.ipynb)
    and [Google Colab](https://colab.research.google.com/drive/1yrq4XBlxiA0fALtMoT2dwiACVc77PHou?usp=sharing).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: âš¡ Quantize EXL2 models
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To start our exploration, we need to install the ExLlamaV2 library. In this
    case, we want to be able to use some scripts contained in the repo, which is why
    we will install it from source as follows:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Now that ExLlamaV2 is installed, we need to download the model we want to quantize
    in this format. Letâ€™s use the excellent [zephyr-7B-beta](https://huggingface.co/HuggingFaceH4/zephyr-7b-beta),
    a [Mistral-7B](https://huggingface.co/mistralai/Mistral-7B-v0.1) model fine-tuned
    using Direct Preference Optimization (DPO). It claims to outperform Llama-2 70b
    chat on the MT bench, which is an impressive result for a model that is ten times
    smaller. You can try out the base Zephyr model using [this space](https://huggingface.co/spaces/HuggingFaceH4/zephyr-chat).
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: 'We download zephyr-7B-beta using the following command (this can take a while
    since the model is about 15 GB):'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'GPTQ also requires a **calibration dataset**, which is used to measure the
    impact of the quantization process by comparing the outputs of the base model
    and its quantized version. We will use the [wikitext dataset](https://huggingface.co/datasets/wikitext)
    and directly download the test file as follows:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Once itâ€™s done, we can leverage the `[convert.py](https://github.com/turboderp/exllamav2/blob/master/convert.py)`
    script provided by the ExLlamaV2 library. We''re mostly concerned with four arguments:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: '`-i`: Path of the base model to convert in HF format (FP16).'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`-o`: Path of the working directory with temporary files and final output.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`-c`: Path of the calibration dataset (in Parquet format).'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`-b`: Target average number of bits per weight (bpw). For example, 4.0 bpw
    will give store weights in 4-bit precision.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The complete list of arguments is available [on this page](https://github.com/turboderp/exllamav2/blob/master/doc/convert.md).
    Letâ€™s start the quantization process using the `convert.py` script with the following
    arguments:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Note that you will need a GPU to quantize this model. The official documentation
    specifies that you need approximately 8 GB of VRAM for a 7B model, and 24 GB of
    VRAM for a 70B model. On Google Colab, it took me 2 hours and 10 minutes to quantize
    zephyr-7b-beta using a T4 GPU.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: Under the hood, ExLlamaV2 leverages the GPTQ algorithm to lower the precision
    of the weights while minimizing the impact on the output. You can find more details
    about the GPTQ algorithm [in this article](https://medium.com/towards-data-science/4-bit-quantization-with-gptq-36b0f4f02c34).
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨åº•å±‚ï¼ŒExLlamaV2 åˆ©ç”¨ GPTQ ç®—æ³•é™ä½æƒé‡çš„ç²¾åº¦ï¼ŒåŒæ—¶æœ€å¤§ç¨‹åº¦åœ°å‡å°‘å¯¹è¾“å‡ºçš„å½±å“ã€‚æ‚¨å¯ä»¥åœ¨è¿™ç¯‡[æ–‡ç« ](https://medium.com/towards-data-science/4-bit-quantization-with-gptq-36b0f4f02c34)ä¸­æ‰¾åˆ°æœ‰å…³
    GPTQ ç®—æ³•çš„æ›´å¤šè¯¦ç»†ä¿¡æ¯ã€‚
- en: 'So why are we using the â€œEXL2â€ format instead of the regular GPTQ format? EXL2
    comes with a few new features:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: é‚£ä¹ˆï¼Œæˆ‘ä»¬ä¸ºä»€ä¹ˆä½¿ç”¨â€œEXL2â€æ ¼å¼è€Œä¸æ˜¯å¸¸è§„çš„ GPTQ æ ¼å¼å‘¢ï¼ŸEXL2 å¸¦æ¥äº†ä¸€äº›æ–°åŠŸèƒ½ï¼š
- en: 'It supports **different levels of quantization**: itâ€™s not restricted to 4-bit
    precision and can handle 2, 3, 4, 5, 6, and 8-bit quantization.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å®ƒæ”¯æŒ**ä¸åŒçº§åˆ«çš„é‡åŒ–**ï¼šä¸é™äº 4 ä½ç²¾åº¦ï¼Œå¯ä»¥å¤„ç† 2ã€3ã€4ã€5ã€6 å’Œ 8 ä½çš„é‡åŒ–ã€‚
- en: It can **mix different precisions** within a model and within each layer to
    preserve the most important weights and layers with more bits.
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å®ƒå¯ä»¥**æ··åˆä¸åŒç²¾åº¦**åœ¨æ¨¡å‹å†…éƒ¨å’Œæ¯ä¸ªå±‚å†…ï¼Œä»¥ä¿ç•™æœ€é‡è¦çš„æƒé‡å’Œå…·æœ‰æ›´å¤šæ¯”ç‰¹çš„å±‚ã€‚
- en: ExLlamaV2 uses this additional flexibility during quantization. It tries different
    quantization parameters and measures the error they introduce. On top of trying
    to minimize the error, ExLlamaV2 also has to achieve the target average number
    of bits per weight given as an argument. Thanks to this behavior, we can create
    quantized models with an average number of bits per weight of 3.5 or 4.5 for example.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: ExLlamaV2 åœ¨é‡åŒ–è¿‡ç¨‹ä¸­åˆ©ç”¨äº†è¿™ç§é¢å¤–çš„çµæ´»æ€§ã€‚å®ƒå°è¯•ä¸åŒçš„é‡åŒ–å‚æ•°å¹¶æµ‹é‡å®ƒä»¬å¼•å…¥çš„è¯¯å·®ã€‚é™¤äº†è¯•å›¾æœ€å°åŒ–è¯¯å·®å¤–ï¼ŒExLlamaV2 è¿˜å¿…é¡»è¾¾åˆ°ä½œä¸ºå‚æ•°ç»™å‡ºçš„æ¯ä¸ªæƒé‡å¹³å‡æ¯”ç‰¹æ•°çš„ç›®æ ‡ã€‚ç”±äºè¿™ç§è¡Œä¸ºï¼Œæˆ‘ä»¬å¯ä»¥åˆ›å»ºå¹³å‡æ¯ä¸ªæƒé‡ä¸º
    3.5 æˆ– 4.5 çš„é‡åŒ–æ¨¡å‹ï¼Œä¾‹å¦‚ã€‚
- en: 'The benchmark of different parameters it creates is saved in the `measurement.json`
    file. The following JSON shows the measurement for one layer:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: å®ƒåˆ›å»ºçš„ä¸åŒå‚æ•°çš„åŸºå‡†ä¿å­˜åœ¨`measurement.json`æ–‡ä»¶ä¸­ã€‚ä»¥ä¸‹ JSON æ˜¾ç¤ºäº†ä¸€ä¸ªå±‚çš„æµ‹é‡ï¼š
- en: '[PRE4]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: In this trial, ExLlamaV2 used 5% of 3-bit and 95% of 2-bit precision for an
    average value of 2.188 bpw and a group size of 32\. This introduced a noticeable
    error that is taken into account to select the best parameters.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ­¤è¯•éªŒä¸­ï¼ŒExLlamaV2 ä½¿ç”¨äº† 5% çš„ 3 ä½ç²¾åº¦å’Œ 95% çš„ 2 ä½ç²¾åº¦ï¼Œå¹³å‡å€¼ä¸º 2.188 bpwï¼Œç»„å¤§å°ä¸º 32ã€‚è¿™å¼•å…¥äº†ä¸€ä¸ªæ˜¾è‘—çš„è¯¯å·®ï¼Œè¿™å°†è¢«è€ƒè™‘åœ¨å†…ä»¥é€‰æ‹©æœ€ä½³å‚æ•°ã€‚
- en: ğŸ¦™ Running ExLlamaV2 for Inference
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ğŸ¦™ è¿è¡Œ ExLlamaV2 è¿›è¡Œæ¨ç†
- en: Now that our model is quantized, we want to run it to see how it performs. Before
    that, we need to copy essential config files from the `base_model` directory to
    the new `quant` directory. Basically, we want every file that is not hidden (`.*`)
    or a safetensors file. Additionally, we don't need the `out_tensor` directory
    that was created by ExLlamaV2 during quantization.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬çš„æ¨¡å‹å·²ç»é‡åŒ–ï¼Œæˆ‘ä»¬æƒ³è¦è¿è¡Œå®ƒä»¥æŸ¥çœ‹å…¶æ€§èƒ½ã€‚åœ¨æ­¤ä¹‹å‰ï¼Œæˆ‘ä»¬éœ€è¦å°†`base_model`ç›®å½•ä¸­çš„é‡è¦é…ç½®æ–‡ä»¶å¤åˆ¶åˆ°æ–°çš„`quant`ç›®å½•ä¸­ã€‚åŸºæœ¬ä¸Šï¼Œæˆ‘ä»¬å¸Œæœ›æ¯ä¸ªééšè—æ–‡ä»¶ï¼ˆ`.*`ï¼‰æˆ–
    safetensors æ–‡ä»¶éƒ½åŒ…å«åœ¨å†…ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬ä¸éœ€è¦ ExLlamaV2 åœ¨é‡åŒ–è¿‡ç¨‹ä¸­åˆ›å»ºçš„`out_tensor`ç›®å½•ã€‚
- en: 'In bash, you can implement this as follows:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ bash ä¸­ï¼Œæ‚¨å¯ä»¥è¿™æ ·å®ç°ï¼š
- en: '[PRE5]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Our EXL2 model is ready and we have several options to run it. The most straightforward
    method consists of using the `test_inference.py` script in the ExLlamaV2 repo
    (note that I donâ€™t use a chat template here):'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬çš„ EXL2 æ¨¡å‹å·²å‡†å¤‡å°±ç»ªï¼Œå¹¶æœ‰å‡ ç§è¿è¡Œé€‰é¡¹ã€‚æœ€ç›´æ¥çš„æ–¹æ³•æ˜¯ä½¿ç”¨ ExLlamaV2 ä»“åº“ä¸­çš„`test_inference.py`è„šæœ¬ï¼ˆè¯·æ³¨æ„ï¼Œè¿™é‡Œæˆ‘æ²¡æœ‰ä½¿ç”¨èŠå¤©æ¨¡æ¿ï¼‰ï¼š
- en: '[PRE6]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The generation is very fast (56.44 tokens/second on a T4 GPU), even compared
    to other quantization techniques and tools like GGUF/llama.cpp or GPTQ. You can
    find an in-depth comparison between different solutions in this [excellent article](https://oobabooga.github.io/blog/posts/gptq-awq-exl2-llamacpp/)
    from oobabooga.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: ç”Ÿæˆé€Ÿåº¦éå¸¸å¿«ï¼ˆåœ¨ T4 GPU ä¸Šä¸º 56.44 tokens/secondï¼‰ï¼Œç”šè‡³ä¸å…¶ä»–é‡åŒ–æŠ€æœ¯å’Œå·¥å…·å¦‚ GGUF/llama.cpp æˆ– GPTQ
    ç›¸æ¯”ä¹Ÿæ˜¯å¦‚æ­¤ã€‚æ‚¨å¯ä»¥åœ¨è¿™ç¯‡[ä¼˜ç§€æ–‡ç« ](https://oobabooga.github.io/blog/posts/gptq-awq-exl2-llamacpp/)ä¸­æ‰¾åˆ°ä¸åŒè§£å†³æ–¹æ¡ˆçš„æ·±å…¥æ¯”è¾ƒã€‚
- en: 'In my case, the LLM returned the following output:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æˆ‘çš„æ¡ˆä¾‹ä¸­ï¼ŒLLM è¿”å›äº†ä»¥ä¸‹è¾“å‡ºï¼š
- en: '[PRE7]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Alternatively, you can use a chat version with the `chat.py` script for more
    flexibility:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ–è€…ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨`chat.py`è„šæœ¬è¿›è¡Œæ›´çµæ´»çš„èŠå¤©ç‰ˆæœ¬ï¼š
- en: '[PRE8]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: If youâ€™re planning to use an EXL2 model more regularly, ExLlamaV2 has been integrated
    into several backends like oobaboogaâ€™s [text generation web UI](https://github.com/oobabooga/text-generation-webui).
    Note that it requires **FlashAttention 2** to work as efficiently as possible,
    which requires CUDA 12.1 on Windows at the moment (something you can configure
    during the installation process).
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæ‚¨è®¡åˆ’æ›´ç»å¸¸ä½¿ç”¨ EXL2 æ¨¡å‹ï¼ŒExLlamaV2 å·²é›†æˆåˆ°å‡ ä¸ªåç«¯ä¸­ï¼Œå¦‚ oobabooga çš„[æ–‡æœ¬ç”Ÿæˆ Web UI](https://github.com/oobabooga/text-generation-webui)ã€‚è¯·æ³¨æ„ï¼Œä¸ºäº†å°½å¯èƒ½é«˜æ•ˆåœ°å·¥ä½œï¼Œå®ƒéœ€è¦**FlashAttention
    2**ï¼Œç›®å‰åœ¨ Windows ä¸Šéœ€è¦ CUDA 12.1ï¼ˆæ‚¨å¯ä»¥åœ¨å®‰è£…è¿‡ç¨‹ä¸­è¿›è¡Œé…ç½®ï¼‰ã€‚
- en: Now that we tested the model, weâ€™re ready to upload it to the Hugging Face Hub.
    You can change the name of your repo in the following code snippet and simply
    run it.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬å·²ç»æµ‹è¯•äº†æ¨¡å‹ï¼Œå‡†å¤‡å°†å…¶ä¸Šä¼ åˆ° Hugging Face Hubã€‚ä½ å¯ä»¥åœ¨ä»¥ä¸‹ä»£ç ç‰‡æ®µä¸­æ›´æ”¹ä½ çš„ä»“åº“åç§°ï¼Œç„¶åç®€å•åœ°è¿è¡Œå®ƒã€‚
- en: '[PRE9]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Great, the model can be found on the [Hugging Face Hub](https://huggingface.co/mlabonne/zephyr-7b-beta-5.0bpw-exl2).
    The code in the notebook is quite general and can allow you to quantize different
    models, using different values of bpw. This is ideal for creating models dedicated
    to your hardware.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: å¾ˆå¥½ï¼Œæ¨¡å‹å¯ä»¥åœ¨ [Hugging Face Hub](https://huggingface.co/mlabonne/zephyr-7b-beta-5.0bpw-exl2)
    ä¸Šæ‰¾åˆ°ã€‚ç¬”è®°æœ¬ä¸­çš„ä»£ç ç›¸å½“é€šç”¨ï¼Œå¯ä»¥è®©ä½ é‡åŒ–ä¸åŒçš„æ¨¡å‹ï¼Œä½¿ç”¨ä¸åŒçš„ bpw å€¼ã€‚è¿™å¯¹äºåˆ›å»ºä¸“é—¨é’ˆå¯¹ä½ çš„ç¡¬ä»¶çš„æ¨¡å‹éå¸¸ç†æƒ³ã€‚
- en: Conclusion
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç»“è®º
- en: In this article, we presented ExLlamaV2, a powerful library to quantize LLMs.
    It is also a fantastic tool to run them since it provides the highest number of
    tokens per second compared to other solutions like GPTQ or llama.cpp. We applied
    it to the [zephyr-7B-beta](https://huggingface.co/HuggingFaceH4/zephyr-7b-beta)
    model to create a 5.0 bpw version of it, using the new EXL2 format. After quantization,
    we tested our model to see how it performs. Finally, it was uploaded to the Hugging
    Face Hub and can be found [here](https://huggingface.co/mlabonne/zephyr-7b-beta-5.0bpw-exl2).
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº† ExLlamaV2ï¼Œä¸€ä¸ªå¼ºå¤§çš„åº“ï¼Œç”¨äºé‡åŒ– LLMsã€‚å®ƒä¹Ÿæ˜¯è¿è¡Œ LLMs çš„ç»ä½³å·¥å…·ï¼Œå› ä¸ºå®ƒæä¾›çš„æ¯ç§’ä»¤ç‰Œæ•°é‡ç›¸æ¯”äºå…¶ä»–è§£å†³æ–¹æ¡ˆï¼ˆå¦‚
    GPTQ æˆ– llama.cppï¼‰æ˜¯æœ€é«˜çš„ã€‚æˆ‘ä»¬å°†å…¶åº”ç”¨äº [zephyr-7B-beta](https://huggingface.co/HuggingFaceH4/zephyr-7b-beta)
    æ¨¡å‹ï¼Œä»¥ä½¿ç”¨æ–°çš„ EXL2 æ ¼å¼åˆ›å»º 5.0 bpw ç‰ˆæœ¬ã€‚é‡åŒ–åï¼Œæˆ‘ä»¬æµ‹è¯•äº†æ¨¡å‹çš„è¡¨ç°ã€‚æœ€åï¼Œå®ƒè¢«ä¸Šä¼ åˆ° Hugging Face Hubï¼Œå¯ä»¥åœ¨ [è¿™é‡Œ](https://huggingface.co/mlabonne/zephyr-7b-beta-5.0bpw-exl2)
    æ‰¾åˆ°ã€‚
- en: If youâ€™re interested in more technical content around LLMs, [follow me on Medium](https://medium.com/@mlabonne).
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä½ å¯¹ LLMs çš„æ›´å¤šæŠ€æœ¯å†…å®¹æ„Ÿå…´è¶£ï¼Œ [åœ¨ Medium ä¸Šå…³æ³¨æˆ‘](https://medium.com/@mlabonne)ã€‚
- en: Articles about quantization
  id: totrans-54
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å…³äºé‡åŒ–çš„æ–‡ç« 
- en: '[](/introduction-to-weight-quantization-2494701b9c0c?source=post_page-----32aeda294d26--------------------------------)
    [## Introduction to Weight Quantization'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/introduction-to-weight-quantization-2494701b9c0c?source=post_page-----32aeda294d26--------------------------------)
    [## æƒé‡é‡åŒ–ä»‹ç»'
- en: Reducing the size of Large Language Models with 8-bit quantization
  id: totrans-56
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ 8 ä½é‡åŒ–å‡å°‘å¤§è¯­è¨€æ¨¡å‹çš„å¤§å°
- en: towardsdatascience.com](/introduction-to-weight-quantization-2494701b9c0c?source=post_page-----32aeda294d26--------------------------------)
    [](/4-bit-quantization-with-gptq-36b0f4f02c34?source=post_page-----32aeda294d26--------------------------------)
    [## 4-bit Quantization with GPTQ
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/introduction-to-weight-quantization-2494701b9c0c?source=post_page-----32aeda294d26--------------------------------)
    [](/4-bit-quantization-with-gptq-36b0f4f02c34?source=post_page-----32aeda294d26--------------------------------)
    [## ä½¿ç”¨ GPTQ çš„ 4 ä½é‡åŒ–
- en: Quantize your own LLMs using AutoGPTQ
  id: totrans-58
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ AutoGPTQ é‡åŒ–ä½ è‡ªå·±çš„ LLMs
- en: towardsdatascience.com](/4-bit-quantization-with-gptq-36b0f4f02c34?source=post_page-----32aeda294d26--------------------------------)
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/4-bit-quantization-with-gptq-36b0f4f02c34?source=post_page-----32aeda294d26--------------------------------)
- en: '*Learn more about machine learning and support my work with one click â€” become
    a Medium member here:*'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '*äº†è§£æ›´å¤šå…³äºæœºå™¨å­¦ä¹ çš„å†…å®¹ï¼Œå¹¶é€šè¿‡ä¸€æ¬¡ç‚¹å‡»æ”¯æŒæˆ‘çš„å·¥ä½œâ€”â€”åœ¨è¿™é‡Œæˆä¸º Medium ä¼šå‘˜ï¼š*'
- en: '[](https://medium.com/@mlabonne/membership?source=post_page-----32aeda294d26--------------------------------)
    [## Join Medium with my referral link - Maxime Labonne'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@mlabonne/membership?source=post_page-----32aeda294d26--------------------------------)
    [## é€šè¿‡æˆ‘çš„æ¨èé“¾æ¥åŠ å…¥ Medium - Maxime Labonne'
- en: As a Medium member, a portion of your membership fee goes to writers you read,
    and you get full access to every storyâ€¦
  id: totrans-62
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ä½œä¸º Medium ä¼šå‘˜ï¼Œä½ çš„ä¸€éƒ¨åˆ†ä¼šå‘˜è´¹å°†ç”¨äºæ”¯æŒä½ é˜…è¯»çš„ä½œè€…ï¼Œä½ å°†èƒ½å®Œå…¨è®¿é—®æ¯ä¸€ä¸ªæ•…äº‹â€¦
- en: medium.com](https://medium.com/@mlabonne/membership?source=post_page-----32aeda294d26--------------------------------)
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: medium.com](https://medium.com/@mlabonne/membership?source=post_page-----32aeda294d26--------------------------------)
