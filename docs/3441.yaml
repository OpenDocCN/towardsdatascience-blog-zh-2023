- en: 'ExLlamaV2: The Fastest Library to Run LLMs'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 'ExLlamaV2: 运行LLMs的最快库'
- en: 原文：[https://towardsdatascience.com/exllamav2-the-fastest-library-to-run-llms-32aeda294d26?source=collection_archive---------0-----------------------#2023-11-20](https://towardsdatascience.com/exllamav2-the-fastest-library-to-run-llms-32aeda294d26?source=collection_archive---------0-----------------------#2023-11-20)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/exllamav2-the-fastest-library-to-run-llms-32aeda294d26?source=collection_archive---------0-----------------------#2023-11-20](https://towardsdatascience.com/exllamav2-the-fastest-library-to-run-llms-32aeda294d26?source=collection_archive---------0-----------------------#2023-11-20)
- en: Quantize and run EXL2 models
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 量化并运行EXL2模型
- en: '[](https://medium.com/@mlabonne?source=post_page-----32aeda294d26--------------------------------)[![Maxime
    Labonne](../Images/a7efdd305e3cc77d5509bbb1076d57d8.png)](https://medium.com/@mlabonne?source=post_page-----32aeda294d26--------------------------------)[](https://towardsdatascience.com/?source=post_page-----32aeda294d26--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----32aeda294d26--------------------------------)
    [Maxime Labonne](https://medium.com/@mlabonne?source=post_page-----32aeda294d26--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@mlabonne?source=post_page-----32aeda294d26--------------------------------)[![Maxime
    Labonne](../Images/a7efdd305e3cc77d5509bbb1076d57d8.png)](https://medium.com/@mlabonne?source=post_page-----32aeda294d26--------------------------------)[](https://towardsdatascience.com/?source=post_page-----32aeda294d26--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----32aeda294d26--------------------------------)
    [Maxime Labonne](https://medium.com/@mlabonne?source=post_page-----32aeda294d26--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fdc89da634938&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fexllamav2-the-fastest-library-to-run-llms-32aeda294d26&user=Maxime+Labonne&userId=dc89da634938&source=post_page-dc89da634938----32aeda294d26---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----32aeda294d26--------------------------------)
    ·6 min read·Nov 20, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F32aeda294d26&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fexllamav2-the-fastest-library-to-run-llms-32aeda294d26&user=Maxime+Labonne&userId=dc89da634938&source=-----32aeda294d26---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fdc89da634938&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fexllamav2-the-fastest-library-to-run-llms-32aeda294d26&user=Maxime+Labonne&userId=dc89da634938&source=post_page-dc89da634938----32aeda294d26---------------------post_header-----------)
    发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----32aeda294d26--------------------------------)
    · 6分钟阅读 · 2023年11月20日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F32aeda294d26&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fexllamav2-the-fastest-library-to-run-llms-32aeda294d26&user=Maxime+Labonne&userId=dc89da634938&source=-----32aeda294d26---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F32aeda294d26&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fexllamav2-the-fastest-library-to-run-llms-32aeda294d26&source=-----32aeda294d26---------------------bookmark_footer-----------)![](../Images/032261661e8c8f24175b7d7766cd2fae.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F32aeda294d26&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fexllamav2-the-fastest-library-to-run-llms-32aeda294d26&source=-----32aeda294d26---------------------bookmark_footer-----------)![](../Images/032261661e8c8f24175b7d7766cd2fae.png)'
- en: Image by author
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供
- en: Quantizing Large Language Models (LLMs) is the most popular approach to reduce
    the size of these models and speed up inference. Among these techniques, GPTQ
    delivers amazing performance on GPUs. Compared to unquantized models, this method
    uses almost 3 times less VRAM while providing a similar level of accuracy and
    faster generation. It became so popular that it has recently been directly integrated
    into the [transformers library](https://huggingface.co/blog/gptq-integration).
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 对大语言模型（LLMs）进行量化是减少这些模型大小和加速推理的最受欢迎的方法。在这些技术中，GPTQ在GPU上的表现非常出色。与未量化模型相比，这种方法使用的显存几乎少了3倍，同时提供了类似的准确度和更快的生成速度。它变得如此受欢迎，以至于最近已经直接集成到[transformers库](https://huggingface.co/blog/gptq-integration)中。
- en: '[**ExLlamaV2**](https://github.com/turboderp/exllamav2) is a library designed
    to squeeze even more performance out of GPTQ. Thanks to new kernels, it’s optimized
    for (blazingly) fast inference. It also introduces a new quantization format,
    EXL2, which brings a lot of flexibility to how weights are stored.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: In this article, we will see how to quantize base models in the EXL2 format
    and how to run them. As usual, the code is available on [GitHub](https://github.com/mlabonne/llm-course/blob/main/Quantize_models_with_ExLlamaV2.ipynb)
    and [Google Colab](https://colab.research.google.com/drive/1yrq4XBlxiA0fALtMoT2dwiACVc77PHou?usp=sharing).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: ⚡ Quantize EXL2 models
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To start our exploration, we need to install the ExLlamaV2 library. In this
    case, we want to be able to use some scripts contained in the repo, which is why
    we will install it from source as follows:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Now that ExLlamaV2 is installed, we need to download the model we want to quantize
    in this format. Let’s use the excellent [zephyr-7B-beta](https://huggingface.co/HuggingFaceH4/zephyr-7b-beta),
    a [Mistral-7B](https://huggingface.co/mistralai/Mistral-7B-v0.1) model fine-tuned
    using Direct Preference Optimization (DPO). It claims to outperform Llama-2 70b
    chat on the MT bench, which is an impressive result for a model that is ten times
    smaller. You can try out the base Zephyr model using [this space](https://huggingface.co/spaces/HuggingFaceH4/zephyr-chat).
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: 'We download zephyr-7B-beta using the following command (this can take a while
    since the model is about 15 GB):'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'GPTQ also requires a **calibration dataset**, which is used to measure the
    impact of the quantization process by comparing the outputs of the base model
    and its quantized version. We will use the [wikitext dataset](https://huggingface.co/datasets/wikitext)
    and directly download the test file as follows:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Once it’s done, we can leverage the `[convert.py](https://github.com/turboderp/exllamav2/blob/master/convert.py)`
    script provided by the ExLlamaV2 library. We''re mostly concerned with four arguments:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: '`-i`: Path of the base model to convert in HF format (FP16).'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`-o`: Path of the working directory with temporary files and final output.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`-c`: Path of the calibration dataset (in Parquet format).'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`-b`: Target average number of bits per weight (bpw). For example, 4.0 bpw
    will give store weights in 4-bit precision.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The complete list of arguments is available [on this page](https://github.com/turboderp/exllamav2/blob/master/doc/convert.md).
    Let’s start the quantization process using the `convert.py` script with the following
    arguments:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Note that you will need a GPU to quantize this model. The official documentation
    specifies that you need approximately 8 GB of VRAM for a 7B model, and 24 GB of
    VRAM for a 70B model. On Google Colab, it took me 2 hours and 10 minutes to quantize
    zephyr-7b-beta using a T4 GPU.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: Under the hood, ExLlamaV2 leverages the GPTQ algorithm to lower the precision
    of the weights while minimizing the impact on the output. You can find more details
    about the GPTQ algorithm [in this article](https://medium.com/towards-data-science/4-bit-quantization-with-gptq-36b0f4f02c34).
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在底层，ExLlamaV2 利用 GPTQ 算法降低权重的精度，同时最大程度地减少对输出的影响。您可以在这篇[文章](https://medium.com/towards-data-science/4-bit-quantization-with-gptq-36b0f4f02c34)中找到有关
    GPTQ 算法的更多详细信息。
- en: 'So why are we using the “EXL2” format instead of the regular GPTQ format? EXL2
    comes with a few new features:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，我们为什么使用“EXL2”格式而不是常规的 GPTQ 格式呢？EXL2 带来了一些新功能：
- en: 'It supports **different levels of quantization**: it’s not restricted to 4-bit
    precision and can handle 2, 3, 4, 5, 6, and 8-bit quantization.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它支持**不同级别的量化**：不限于 4 位精度，可以处理 2、3、4、5、6 和 8 位的量化。
- en: It can **mix different precisions** within a model and within each layer to
    preserve the most important weights and layers with more bits.
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它可以**混合不同精度**在模型内部和每个层内，以保留最重要的权重和具有更多比特的层。
- en: ExLlamaV2 uses this additional flexibility during quantization. It tries different
    quantization parameters and measures the error they introduce. On top of trying
    to minimize the error, ExLlamaV2 also has to achieve the target average number
    of bits per weight given as an argument. Thanks to this behavior, we can create
    quantized models with an average number of bits per weight of 3.5 or 4.5 for example.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: ExLlamaV2 在量化过程中利用了这种额外的灵活性。它尝试不同的量化参数并测量它们引入的误差。除了试图最小化误差外，ExLlamaV2 还必须达到作为参数给出的每个权重平均比特数的目标。由于这种行为，我们可以创建平均每个权重为
    3.5 或 4.5 的量化模型，例如。
- en: 'The benchmark of different parameters it creates is saved in the `measurement.json`
    file. The following JSON shows the measurement for one layer:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 它创建的不同参数的基准保存在`measurement.json`文件中。以下 JSON 显示了一个层的测量：
- en: '[PRE4]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: In this trial, ExLlamaV2 used 5% of 3-bit and 95% of 2-bit precision for an
    average value of 2.188 bpw and a group size of 32\. This introduced a noticeable
    error that is taken into account to select the best parameters.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在此试验中，ExLlamaV2 使用了 5% 的 3 位精度和 95% 的 2 位精度，平均值为 2.188 bpw，组大小为 32。这引入了一个显著的误差，这将被考虑在内以选择最佳参数。
- en: 🦙 Running ExLlamaV2 for Inference
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 🦙 运行 ExLlamaV2 进行推理
- en: Now that our model is quantized, we want to run it to see how it performs. Before
    that, we need to copy essential config files from the `base_model` directory to
    the new `quant` directory. Basically, we want every file that is not hidden (`.*`)
    or a safetensors file. Additionally, we don't need the `out_tensor` directory
    that was created by ExLlamaV2 during quantization.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们的模型已经量化，我们想要运行它以查看其性能。在此之前，我们需要将`base_model`目录中的重要配置文件复制到新的`quant`目录中。基本上，我们希望每个非隐藏文件（`.*`）或
    safetensors 文件都包含在内。此外，我们不需要 ExLlamaV2 在量化过程中创建的`out_tensor`目录。
- en: 'In bash, you can implement this as follows:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在 bash 中，您可以这样实现：
- en: '[PRE5]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Our EXL2 model is ready and we have several options to run it. The most straightforward
    method consists of using the `test_inference.py` script in the ExLlamaV2 repo
    (note that I don’t use a chat template here):'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的 EXL2 模型已准备就绪，并有几种运行选项。最直接的方法是使用 ExLlamaV2 仓库中的`test_inference.py`脚本（请注意，这里我没有使用聊天模板）：
- en: '[PRE6]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The generation is very fast (56.44 tokens/second on a T4 GPU), even compared
    to other quantization techniques and tools like GGUF/llama.cpp or GPTQ. You can
    find an in-depth comparison between different solutions in this [excellent article](https://oobabooga.github.io/blog/posts/gptq-awq-exl2-llamacpp/)
    from oobabooga.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 生成速度非常快（在 T4 GPU 上为 56.44 tokens/second），甚至与其他量化技术和工具如 GGUF/llama.cpp 或 GPTQ
    相比也是如此。您可以在这篇[优秀文章](https://oobabooga.github.io/blog/posts/gptq-awq-exl2-llamacpp/)中找到不同解决方案的深入比较。
- en: 'In my case, the LLM returned the following output:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在我的案例中，LLM 返回了以下输出：
- en: '[PRE7]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Alternatively, you can use a chat version with the `chat.py` script for more
    flexibility:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，您可以使用`chat.py`脚本进行更灵活的聊天版本：
- en: '[PRE8]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: If you’re planning to use an EXL2 model more regularly, ExLlamaV2 has been integrated
    into several backends like oobabooga’s [text generation web UI](https://github.com/oobabooga/text-generation-webui).
    Note that it requires **FlashAttention 2** to work as efficiently as possible,
    which requires CUDA 12.1 on Windows at the moment (something you can configure
    during the installation process).
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您计划更经常使用 EXL2 模型，ExLlamaV2 已集成到几个后端中，如 oobabooga 的[文本生成 Web UI](https://github.com/oobabooga/text-generation-webui)。请注意，为了尽可能高效地工作，它需要**FlashAttention
    2**，目前在 Windows 上需要 CUDA 12.1（您可以在安装过程中进行配置）。
- en: Now that we tested the model, we’re ready to upload it to the Hugging Face Hub.
    You can change the name of your repo in the following code snippet and simply
    run it.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经测试了模型，准备将其上传到 Hugging Face Hub。你可以在以下代码片段中更改你的仓库名称，然后简单地运行它。
- en: '[PRE9]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Great, the model can be found on the [Hugging Face Hub](https://huggingface.co/mlabonne/zephyr-7b-beta-5.0bpw-exl2).
    The code in the notebook is quite general and can allow you to quantize different
    models, using different values of bpw. This is ideal for creating models dedicated
    to your hardware.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 很好，模型可以在 [Hugging Face Hub](https://huggingface.co/mlabonne/zephyr-7b-beta-5.0bpw-exl2)
    上找到。笔记本中的代码相当通用，可以让你量化不同的模型，使用不同的 bpw 值。这对于创建专门针对你的硬件的模型非常理想。
- en: Conclusion
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: In this article, we presented ExLlamaV2, a powerful library to quantize LLMs.
    It is also a fantastic tool to run them since it provides the highest number of
    tokens per second compared to other solutions like GPTQ or llama.cpp. We applied
    it to the [zephyr-7B-beta](https://huggingface.co/HuggingFaceH4/zephyr-7b-beta)
    model to create a 5.0 bpw version of it, using the new EXL2 format. After quantization,
    we tested our model to see how it performs. Finally, it was uploaded to the Hugging
    Face Hub and can be found [here](https://huggingface.co/mlabonne/zephyr-7b-beta-5.0bpw-exl2).
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们介绍了 ExLlamaV2，一个强大的库，用于量化 LLMs。它也是运行 LLMs 的绝佳工具，因为它提供的每秒令牌数量相比于其他解决方案（如
    GPTQ 或 llama.cpp）是最高的。我们将其应用于 [zephyr-7B-beta](https://huggingface.co/HuggingFaceH4/zephyr-7b-beta)
    模型，以使用新的 EXL2 格式创建 5.0 bpw 版本。量化后，我们测试了模型的表现。最后，它被上传到 Hugging Face Hub，可以在 [这里](https://huggingface.co/mlabonne/zephyr-7b-beta-5.0bpw-exl2)
    找到。
- en: If you’re interested in more technical content around LLMs, [follow me on Medium](https://medium.com/@mlabonne).
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你对 LLMs 的更多技术内容感兴趣， [在 Medium 上关注我](https://medium.com/@mlabonne)。
- en: Articles about quantization
  id: totrans-54
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 关于量化的文章
- en: '[](/introduction-to-weight-quantization-2494701b9c0c?source=post_page-----32aeda294d26--------------------------------)
    [## Introduction to Weight Quantization'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/introduction-to-weight-quantization-2494701b9c0c?source=post_page-----32aeda294d26--------------------------------)
    [## 权重量化介绍'
- en: Reducing the size of Large Language Models with 8-bit quantization
  id: totrans-56
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用 8 位量化减少大语言模型的大小
- en: towardsdatascience.com](/introduction-to-weight-quantization-2494701b9c0c?source=post_page-----32aeda294d26--------------------------------)
    [](/4-bit-quantization-with-gptq-36b0f4f02c34?source=post_page-----32aeda294d26--------------------------------)
    [## 4-bit Quantization with GPTQ
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/introduction-to-weight-quantization-2494701b9c0c?source=post_page-----32aeda294d26--------------------------------)
    [](/4-bit-quantization-with-gptq-36b0f4f02c34?source=post_page-----32aeda294d26--------------------------------)
    [## 使用 GPTQ 的 4 位量化
- en: Quantize your own LLMs using AutoGPTQ
  id: totrans-58
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用 AutoGPTQ 量化你自己的 LLMs
- en: towardsdatascience.com](/4-bit-quantization-with-gptq-36b0f4f02c34?source=post_page-----32aeda294d26--------------------------------)
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/4-bit-quantization-with-gptq-36b0f4f02c34?source=post_page-----32aeda294d26--------------------------------)
- en: '*Learn more about machine learning and support my work with one click — become
    a Medium member here:*'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '*了解更多关于机器学习的内容，并通过一次点击支持我的工作——在这里成为 Medium 会员：*'
- en: '[](https://medium.com/@mlabonne/membership?source=post_page-----32aeda294d26--------------------------------)
    [## Join Medium with my referral link - Maxime Labonne'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@mlabonne/membership?source=post_page-----32aeda294d26--------------------------------)
    [## 通过我的推荐链接加入 Medium - Maxime Labonne'
- en: As a Medium member, a portion of your membership fee goes to writers you read,
    and you get full access to every story…
  id: totrans-62
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 作为 Medium 会员，你的一部分会员费将用于支持你阅读的作者，你将能完全访问每一个故事…
- en: medium.com](https://medium.com/@mlabonne/membership?source=post_page-----32aeda294d26--------------------------------)
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: medium.com](https://medium.com/@mlabonne/membership?source=post_page-----32aeda294d26--------------------------------)
