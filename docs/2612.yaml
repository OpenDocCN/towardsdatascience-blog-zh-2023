- en: 'Dynamic Pricing with Multi-Armed Bandits: Learning by Doing'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用多臂老虎机进行动态定价：通过实践学习
- en: 原文：[https://towardsdatascience.com/dynamic-pricing-with-multi-armed-bandit-learning-by-doing-3e4550ed02ac?source=collection_archive---------1-----------------------#2023-08-16](https://towardsdatascience.com/dynamic-pricing-with-multi-armed-bandit-learning-by-doing-3e4550ed02ac?source=collection_archive---------1-----------------------#2023-08-16)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/dynamic-pricing-with-multi-armed-bandit-learning-by-doing-3e4550ed02ac?source=collection_archive---------1-----------------------#2023-08-16](https://towardsdatascience.com/dynamic-pricing-with-multi-armed-bandit-learning-by-doing-3e4550ed02ac?source=collection_archive---------1-----------------------#2023-08-16)
- en: Applying Reinforcement Learning strategies to real-world use cases, especially
    in dynamic pricing, can reveal many surprises
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将强化学习策略应用于实际案例，尤其是动态定价中，可以揭示许多惊喜
- en: '[](https://medium.com/@massi.costacurta?source=post_page-----3e4550ed02ac--------------------------------)[![Massimiliano
    Costacurta](../Images/599c3469021c53f116cc67c390db6695.png)](https://medium.com/@massi.costacurta?source=post_page-----3e4550ed02ac--------------------------------)[](https://towardsdatascience.com/?source=post_page-----3e4550ed02ac--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----3e4550ed02ac--------------------------------)
    [Massimiliano Costacurta](https://medium.com/@massi.costacurta?source=post_page-----3e4550ed02ac--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@massi.costacurta?source=post_page-----3e4550ed02ac--------------------------------)[![Massimiliano
    Costacurta](../Images/599c3469021c53f116cc67c390db6695.png)](https://medium.com/@massi.costacurta?source=post_page-----3e4550ed02ac--------------------------------)[](https://towardsdatascience.com/?source=post_page-----3e4550ed02ac--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----3e4550ed02ac--------------------------------)
    [Massimiliano Costacurta](https://medium.com/@massi.costacurta?source=post_page-----3e4550ed02ac--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F233cb43234c3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdynamic-pricing-with-multi-armed-bandit-learning-by-doing-3e4550ed02ac&user=Massimiliano+Costacurta&userId=233cb43234c3&source=post_page-233cb43234c3----3e4550ed02ac---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----3e4550ed02ac--------------------------------)
    ·16 min read·Aug 16, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F3e4550ed02ac&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdynamic-pricing-with-multi-armed-bandit-learning-by-doing-3e4550ed02ac&user=Massimiliano+Costacurta&userId=233cb43234c3&source=-----3e4550ed02ac---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F233cb43234c3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdynamic-pricing-with-multi-armed-bandit-learning-by-doing-3e4550ed02ac&user=Massimiliano+Costacurta&userId=233cb43234c3&source=post_page-233cb43234c3----3e4550ed02ac---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----3e4550ed02ac--------------------------------)
    ·16分钟阅读·2023年8月16日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F3e4550ed02ac&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdynamic-pricing-with-multi-armed-bandit-learning-by-doing-3e4550ed02ac&user=Massimiliano+Costacurta&userId=233cb43234c3&source=-----3e4550ed02ac---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F3e4550ed02ac&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdynamic-pricing-with-multi-armed-bandit-learning-by-doing-3e4550ed02ac&source=-----3e4550ed02ac---------------------bookmark_footer-----------)![](../Images/ee7ee77f9d7a623044975573c7cd42b4.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F3e4550ed02ac&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdynamic-pricing-with-multi-armed-bandit-learning-by-doing-3e4550ed02ac&source=-----3e4550ed02ac---------------------bookmark_footer-----------)![](../Images/ee7ee77f9d7a623044975573c7cd42b4.png)'
- en: Photo by [Markus Spiske](https://unsplash.com/@markusspiske?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 由 [Markus Spiske](https://unsplash.com/@markusspiske?utm_source=medium&utm_medium=referral)
    拍摄，照片来自 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: Dynamic Pricing, Reinforcement Learning and Multi-Armed Bandit
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 动态定价、强化学习与多臂老虎机
- en: 'In the vast world of decision-making problems, one dilemma is particularly
    owned by Reinforcement Learning strategies: exploration versus exploitation. Imagine
    walking into a casino with rows of slot machines (also known as “one-armed bandits”)
    where each machine pays out a different, unknown reward. Do you explore and play
    each machine to discover which one has the highest payout, or do you stick to
    one machine, hoping it’s the jackpot? This metaphorical scenario underpins the
    concept of the Multi-armed Bandit (MAB) problem. The objective is to find a strategy
    that maximizes the rewards over a series of plays. While exploration offers new
    insights, exploitation leverages the information you already possess.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在决策问题的广阔世界中，有一个困境特别属于强化学习策略：探索与利用。想象一下走进一个赌场，那里有一排排的老虎机（也称为“单臂赌博机”），每台机器支付的奖励各不相同且未知。你是探索并玩每台机器以发现哪个机器的回报最高，还是坚持玩一台机器，希望它就是大奖？这个隐喻场景构成了多臂赌博机（MAB）问题的核心概念。目标是找到一个在一系列游戏中最大化奖励的策略。虽然探索提供了新的见解，但利用则是利用你已经拥有的信息。
- en: Now, transpose this principle to dynamic pricing in a retail scenario. Suppose
    you are an e-commerce store owner with a new product. You aren’t certain about
    its optimal selling price. How do you set a price that maximizes your revenue?
    Should you explore different prices to understand customer willingness to pay,
    or should you exploit a price that has been performing well historically? Dynamic
    pricing is essentially a MAB problem in disguise. At each time step, every candidate
    price point can be seen as an “arm” of a slot machine and the revenue generated
    from that price is its “reward.” Another way to see this is that the objective
    of dynamic pricing is to swiftly and accurately measure how a customer base’s
    demand reacts to varying price points. In simpler terms, the aim is to pinpoint
    the demand curve that best mirrors customer behavior.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，将这一原则转移到零售场景中的动态定价上。假设你是一个电子商务商店的老板，拥有一款新产品。你不确定其最佳销售价格。你该如何设定一个最大化收入的价格？你应该探索不同的价格以了解客户的支付意愿，还是应该利用一个在历史上表现良好的价格？动态定价本质上是一个伪装的多臂赌博机问题。在每一个时间点，每一个候选价格点都可以看作是老虎机的一个“臂”，而从该价格生成的收入则是其“奖励”。另一种看法是，动态定价的目标是迅速而准确地测量客户群体对不同价格点的需求反应。简单来说，目标是找出最能反映客户行为的需求曲线。
- en: In this article, we’ll explore four Multi-armed Bandit algorithms to evaluate
    their efficacy against a well-defined (though not straightforward) demand curve.
    We’ll then dissect the primary strengths and limitations of each algorithm and
    delve into the key metrics that are instrumental in gauging their performance.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们将探讨四种多臂赌博机算法，以评估它们在一个明确定义（尽管并不简单）的需求曲线下的效果。随后，我们将分析每种算法的主要优点和局限性，并深入研究评估其性能的关键指标。
- en: Modeling The Demand Curve
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 需求曲线建模
- en: Traditionally, demand curves in economics describe the relationship between
    the price of a product and the quantity of the product that consumers are willing
    to buy. They generally slope downwards, representing the common observation that
    as price rises, demand typically falls, and vice-versa. Think of popular products
    such as smartphones or concert tickets. If prices are lowered, more people tend
    to buy, but if prices skyrocket, even the ardent fans might think twice.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 传统上，经济学中的需求曲线描述了产品价格与消费者愿意购买的产品数量之间的关系。它们通常向下倾斜，表示一个常见的观察，即价格上涨时，需求通常会下降，反之亦然。想想智能手机或演唱会门票这样的热门产品。如果价格降低，更多的人往往会购买，但如果价格飙升，即使是忠实粉丝也可能会重新考虑。
- en: 'Yet in our context, we’ll model the demand curve slightly differently: we’re
    putting price against probability. Why? Because in dynamic pricing scenarios,
    especially digital goods or services, it’s often more meaningful to think in terms
    of the likelihood of a sale at a given price than to speculate on exact quantities.
    In such environments, each pricing attempt can be seen as an exploration of the
    likelihood of success (or purchase), which can be easily modeled as a Bernoulli
    random variable with a probability *p* depending on a given test price.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在我们的背景下，我们将稍微不同地建模需求曲线：我们将价格与概率进行对比。为什么？因为在动态定价场景中，特别是数字商品或服务的情况下，考虑在给定价格下销售的可能性往往比猜测确切数量更有意义。在这样的环境中，每次定价尝试都可以看作是对成功（或购买）可能性的探索，这可以简单地建模为一个伯努利随机变量，其概率*p*取决于给定的测试价格。
- en: 'Here’s where it gets particularly interesting: while intuitively one might
    think the task of our Multi-armed Bandit algorithms is to unearth that ideal price
    where the probability of purchase is highest, it’s not quite so straightforward.
    In fact, our ultimate goal is to maximize the revenue (or the margin). This means
    we’re not searching for the price that gets the most people to click ‘buy’ — we’re
    searching for the price that, when multiplied by its associated purchase probability,
    gives the highest expected return. Imagine setting a high price that fewer people
    buy, but each sale generates significant revenue. On the flip side, a very low
    price might attract more buyers, but the total revenue might still be lower than
    the high price scenario. So, in our context, talking about the ‘demand curve’
    is somewhat unconventional, as our target curve will primarily represent the probability
    of purchase rather than the demand directly.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, getting to the math, let’s start by saying that consumer behavior, especially
    when dealing with price sensitivity, isn’t always linear. A linear model might
    suggest that for every incremental increase in price, there’s a constant decrement
    in demand. In reality, this relationship is often more complex and nonlinear.
    One way to model this behavior is by using logistic functions, which can capture
    this nuanced relationship more effectively. Our chosen model for the demand curve
    is then:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/fe4cc4f9e4698afc79a0ff4ee39a1cc6.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
- en: Here, *a* determines the maximum achievable probability of purchase, while *b*
    modulates the sensitivity of the demand curve against price changes. A higher
    value of *b* means a steeper curve, approaching more rapidly to lower purchase
    probabilities as the price increases.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/53098370c4b84e88030173af66333f01.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
- en: Four examples of demand curves with different combinations of parameters a and
    b
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: For any given price point, we’ll be then able to obtain an associated purchase
    probability, *p*. We can then input *p* into a Bernoulli random variable generator
    to simulate the response of a customer to a particular price proposal. In other
    words, given a price, we can easily emulate our reward function.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we can multiply this function by the price in order to get the expected
    revenue for a given price point:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/11ee2d7e1a22d6a93d5a9092c4bef301.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
- en: Unsurprisingly, this function does not reach its maximum in correspondence with
    the highest probability. Also, the price associated with the maximum does not
    depend on the value of the parameter *a*, while the maximum expected return does.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3676422659c3945a959f3ddd154b8a0a.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
- en: Expected revenue curves with related maxima
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: 'With some recollection from calculus, we can also derive the formula for the
    derivative (you’ll need to use a combination of both the product and the chain
    rule). It’s not exactly a relaxing exercise, but it’s nothing too challenging.
    Here is the analytical expression of the derivative of the expected revenue:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 结合一些微积分知识，我们还可以推导出导数的公式（你需要同时使用乘法法则和链式法则）。这不是一个轻松的练习，但也没有特别困难的地方。这是期望收入的导数的解析表达式：
- en: '![](../Images/6b4aca3305e35203517b213539b0380d.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6b4aca3305e35203517b213539b0380d.png)'
- en: This derivative allows us to find the exact price that maximizes our expected
    revenue curve. In other words, by using this specific formula in tandem with some
    numerical algorithms, we can easily determine the price that sets it to 0\. This,
    in turn, is the price that maximizes the expected revenue.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 这个导数允许我们找到最大化期望收入曲线的确切价格。换句话说，通过使用这个特定的公式以及一些数值算法，我们可以轻松确定将其设为0的价格。反过来，这就是最大化期望收入的价格。
- en: 'And this is exactly what we need, since by fixing the values of *a* and *b*,
    we’ll immediately know the target price that our bandits will have to find. Coding
    this in Python is a matter of a few lines of code:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 这正是我们需要的，因为通过固定*a*和*b*的值，我们将立即知道我们的赌博机需要找到的目标价格。用Python编码这只需几行代码：
- en: For our use case, we’ll set *a* = 2 and *b* = 0.042, which will give us a target
    price of about 30.44, associated with an optimal probability of 0.436 ( → optimal
    average reward is 30.44*0.436=13.26). This price is obviously unknown in general
    and it is exactly the price that our Multi-armed Bandit algorithms will seek.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的用例，我们将设置*a*=2和*b*=0.042，这将给我们一个大约30.44的目标价格，关联的最佳概率为0.436（→最佳平均奖励为30.44*0.436=13.26）。这个价格在一般情况下显然是未知的，正是我们的多臂赌博机算法将要寻找的价格。
- en: Multi-armed Bandit Strategies
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 多臂赌博机策略
- en: 'Now that we’ve identified our objectives, it’s time to explore various strategies
    for testing and analyzing their performance, strengths, and weaknesses. While
    several algorithms exist in MAB literature, when it comes to real-world scenarios,
    four primary strategies (along with their variations) predominantly form the backbone.
    In this section, we’ll provide a brief overview of these strategies. We assume
    the reader has a foundational understanding of them; however, for those interested
    in a more in-depth exploration, references are provided at the end of the article.
    After introducing each algorithm, we’ll also present its Python implementation.
    Although each algorithm possesses its unique set of parameters, they all commonly
    utilize one key input: the `arm_avg_reward` vector. This vector denotes the average
    reward garnered from each arm (or action/price) up to the current time step *t*.
    This critical input guides all the algorithms in making informed decisions about
    the subsequent price setting.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经确定了目标，现在是时候探索各种策略以测试和分析它们的性能、优点和缺点了。虽然在MAB文献中存在几种算法，但在实际应用中，四种主要策略（及其变种）主要构成了基础。在本节中，我们将简要概述这些策略。我们假设读者对这些策略有基本了解；不过，对于那些有兴趣深入研究的人，我们在文章末尾提供了参考文献。在介绍每个算法后，我们还将展示其Python实现。尽管每种算法具有其独特的参数，但它们都普遍使用一个关键输入：`arm_avg_reward`向量。该向量表示当前时间步*t*为止每个臂（或动作/价格）获得的平均奖励。这个关键输入指导所有算法做出有关后续价格设置的明智决策。
- en: 'The algorithms I am going to apply to our dynamic pricing problem are the following:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 我将应用于我们的动态定价问题的算法如下：
- en: '**Greedy**: This strategy is like always going back to the machine that gave
    you the most coins the first few times you played. After trying out each machine
    a bit, it sticks with the one that seemed the best. But there might be a problem.
    What if that machine was just lucky at the start? The Greedy strategy might miss
    out on better options. On the bright side, the code implementation is really simple:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '**贪婪策略**：这一策略就像是每次都回到最初给你最多硬币的机器。尝试过每台机器后，它会坚持最初表现最好的那一台。但可能会有一个问题。如果那台机器只是最初运气好呢？贪婪策略可能会错过更好的选项。幸运的是，代码实现非常简单：'
- en: It’s essential to differentiate the initial scenario (when all rewards are 0)
    from the regular one. Often, you’ll find only the ‘else’ part implemented, which
    indeed works even when all rewards are at 0\. Yet, this approach can lead to a
    bias toward the first element. If you make this oversight, you might end up paying
    that bias, particularly if the optimal reward happens to be tied to the first
    arm (yes, I’ve been there). The Greedy approach is typically the least-performing
    one and we’ll primarily use it as our performance baseline.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 区分初始情境（所有回报均为 0 时）与常规情境是至关重要的。通常，你会发现只实现了‘else’部分，这确实在所有回报为 0 时也能工作。然而，这种方法可能导致对第一个元素的偏见。如果忽视这一点，你可能会因此付出代价，特别是当最优回报恰好与第一个臂相关时（是的，我经历过）。贪婪方法通常表现最差，我们将主要使用它作为性能基准。
- en: '***ϵ*-greedy**: The ε-greedy (epsilon-greedy) algorithm is a modification to
    tackle the main drawback of the greedy approach. It introduces a probability *ε*
    (epsilon), typically a small value, to select a random arm, promoting exploration.
    With a probability 1−*ε*, it chooses the arm with the highest estimated reward,
    favoring exploitation. By balancing between random exploration and exploitation
    of known rewards, the ε-greedy strategy aims to achieve better long-term returns
    compared to purely greedy methods. Again, the implementation is immediate, it’s
    simply an additional ‘if’ on top of the Greedy code.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '***ϵ*-贪婪**：ε-贪婪（epsilon-greedy）算法是对贪婪方法主要缺陷的一种改进。它引入了一个概率 *ε*（epsilon），通常是一个小值，用于选择一个随机的臂，促进探索。以概率
    1−*ε*，它选择回报估计最高的臂，偏向于利用。通过在随机探索和已知回报的利用之间进行平衡，ε-贪婪策略旨在实现比纯粹的贪婪方法更好的长期回报。同样，这种实现是直接的，只需在贪婪代码上添加一个额外的‘if’。'
- en: '**UCB1 (Upper Confidence Bound)**: The UCB1 strategy is like a curious explorer
    trying to find the best restaurant in a new city. While there’s a favorite spot
    they’ve enjoyed, the allure of potentially discovering an even better place grows
    with each passing day. In our context, UCB1 combines the rewards of known price
    points with the uncertainty of those less explored. Mathematically, this balance
    is achieved through a formula: the average reward of a price point plus an “uncertainty
    bonus” based on how long since it was last tried. This bonus is calculated as'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '**UCB1（上置信界）**：UCB1 策略就像一个好奇的探险者，试图在新城市找到最好的餐馆。虽然有一个他们已经喜欢的地方，但每天都会有可能发现更好的地方。在我们的上下文中，UCB1
    将已知价格点的回报与那些较少探索的价格点的不确定性相结合。数学上，这种平衡是通过一个公式实现的：价格点的平均回报加上一个基于距离上次尝试的时间的“未知奖励”奖金。这个奖金计算为'
- en: '![](../Images/78e04b458368a5ab1e4de22f9a30bdf3.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/78e04b458368a5ab1e4de22f9a30bdf3.png)'
- en: and represents the “growing curiosity” about the untried price. The hyperparameter
    *C* controls the balance between exploitation and exploration, with higher values
    of *C* encouraging more exploration of less-sampled arms. By always selecting
    the price with the highest combined value of known reward and curiosity bonus,
    UCB1 ensures a mix of sticking to what’s known and venturing into the unknown,
    aiming to uncover the optimal price point for maximum revenue. I’ll start with
    the by-the-book implementation of this approach, but we’ll soon see that we need
    to tweak it a bit.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 并代表了对未尝试价格的“增长的好奇心”。超参数 *C* 控制利用和探索之间的平衡，*C* 值较高时，鼓励更多探索较少采样的臂。通过始终选择已知回报和好奇心奖金的组合值最高的价格，UCB1
    确保了既坚持已知又探索未知的混合，旨在揭示最大收入的最优价格点。我会从按部就班的实施开始，但我们很快会看到需要对其进行一些调整。
- en: '**Thompson Sampling**: This Bayesian approach addresses the exploration-exploitation
    dilemma by probabilistically selecting arms based on their posterior reward distributions.
    When these rewards adhere to a Bernoulli distribution, representing binary outcomes
    like success/failure, Thompson Sampling (TS) employs the Beta distribution as
    a conjugate prior (see [this table](https://en.wikipedia.org/wiki/Conjugate_prior)
    for reference). Initiating with a non-informative Beta(1,1) prior for every arm,
    the algorithm updates the distribution’s parameters upon observing rewards: a
    success increases the alpha parameter, while a failure augments the beta. During
    each play, TS draws from the current Beta distribution of each arm and opts for
    the one with the top sampled value. This methodology allows TS to dynamically
    adjust based on gathered rewards, adeptly balancing between the exploration of
    uncertain arms and the exploitation of those known to be rewarding. In our specific
    scenario, although the foundational reward function follows a Bernoulli distribution
    (1 for a purchase and 0 for a missed purchase), the actual reward of interest
    is the product of this basic reward and the current price under test. Hence, our
    implementation of TS will need a slight modification (which will also introduce
    some surprises).'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '**汤普森采样**：这种贝叶斯方法通过基于后验奖励分布的概率选择臂来解决探索-利用困境。当这些奖励符合伯努利分布，表示像成功/失败这样的二元结果时，汤普森采样（TS）使用Beta分布作为共轭先验（参见[此表](https://en.wikipedia.org/wiki/Conjugate_prior)）。算法从每个臂开始使用非信息性Beta(1,1)先验，并在观察到奖励后更新分布的参数：成功增加alpha参数，而失败增加beta参数。在每次游戏中，TS从每个臂的当前Beta分布中抽取样本，并选择具有最高抽样值的臂。这种方法使TS能够根据获得的奖励动态调整，巧妙地平衡了对不确定臂的探索和对已知奖励臂的利用。在我们的具体场景中，尽管基础奖励函数遵循伯努利分布（购买为1，错过购买为0），但实际感兴趣的奖励是该基础奖励与当前测试价格的乘积。因此，我们的TS实现将需要稍作修改（这也会带来一些惊喜）。'
- en: 'The change is actually pretty simple: to determine the most promising next
    arm, samples extracted from the posterior estimates are multiplied by their respective
    price points (line 3). This modification ensures decisions are anchored on the
    anticipated average revenue, shifting the focus from the highest purchase probability.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 改动其实很简单：要确定最有前途的下一个臂，提取自后验估计的样本乘以其各自的价格点（第3行）。这一修改确保决策基于预期的平均收入，而非最高购买概率。
- en: How do we evaluate the results?
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 我们如何评估结果？
- en: 'At this point, having gathered all the key ingredients to construct a simulation
    comparing the performance of the four algorithms in our dynamic pricing context,
    we must ask ourselves: what exactly will we be measuring? The metrics we choose
    are pivotal, as they will guide us in the process of both comparing and improving
    the algorithm implementation. In this endeavor, I’m zeroing in on three key indicators:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 此时，在收集了所有关键因素以构建一个比较我们动态定价背景下四种算法性能的仿真后，我们必须问自己：我们究竟要测量什么？我们选择的指标至关重要，因为它们将指导我们在比较和改进算法实现的过程中。在这方面，我关注三个关键指标：
- en: '**Regret**: This metric measures the difference between the reward obtained
    by the chosen action and the reward that would have been obtained by taking the
    best possible action. Mathematically, regret at time *t* is given by: Regret(*t*)=Optimal
    Reward(*t*)−Actual Reward(*t*). Regret, when accumulated over time, provides insight
    into how much we’ve “lost” by not always choosing the best action. It is preferred
    over cumulative reward because it provides a clearer indication of the algorithm’s
    performance relative to the optimal scenario. Ideally, a regret value close to
    0 indicates proximity to optimal decision-making.'
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**遗憾**：该指标衡量所选择行动获得的奖励与采取最佳可能行动所能获得的奖励之间的差异。从数学上讲，时间*t*的遗憾定义为：遗憾(*t*)=最佳奖励(*t*)−实际奖励(*t*)。遗憾在时间上累积，提供了我们没有总是选择最佳行动而“失去”多少的洞察。与累计奖励相比，遗憾更为优选，因为它能更清晰地指示算法相对于最佳情况的表现。理想情况下，接近0的遗憾值表明接近于最佳决策。'
- en: '**Reactivity**: This metric gauges the speed at which an algorithm approaches
    a target average reward. Essentially, it’s a measure of the algorithm’s adaptability
    and learning efficiency. The quicker an algorithm can achieve the desired average
    reward, the more reactive it is, implying a swifter adjustment to the optimal
    price point. In our case the target reward is set at 95% of the optimal average
    reward, which is 13.26\. However, initial steps can exhibit high variability.
    For instance, a lucky early choice might result in a success from a low probability
    arm associated with a high price, quickly achieving the threshold. Due to such
    fluctuations, I’ve opted for a stricter definition of reactivity: the number of
    steps required to attain 95% of the optimal average reward ten times, excluding
    the initial 100 steps.'
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**反应性**：这一指标衡量算法接近目标平均奖励的速度。本质上，它是算法适应性和学习效率的衡量标准。一个算法越快达到期望的平均奖励，它的反应性就越强，意味着更快地调整到最佳价格点。在我们的情况下，目标奖励设定为最佳平均奖励的95%，即13.26。然而，初始步骤可能表现出较高的波动性。例如，一个幸运的早期选择可能会导致从一个低概率的高价格臂中获得成功，迅速达到阈值。由于这些波动，我选择了一个更严格的反应性定义：达到95%最佳平均奖励十次所需的步骤数，排除最初的100步。'
- en: '**Arms Allocation**: This indicates the frequency with which each algorithm
    utilizes the available arms. Presented as a percentage, it reveals the algorithm’s
    propensity to select each arm over time. Ideally, for the most efficient pricing
    strategy, we’d want an algorithm to allocate 100% of its choices to the best-performing
    arm and 0% to the rest. Such an allocation would inherently lead to a regret value
    of 0, denoting optimal performance.'
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**臂分配**：这表示每个算法使用可用臂的频率。以百分比形式呈现，它揭示了算法随时间选择每个臂的倾向。理想情况下，对于最有效的定价策略，我们希望算法将100%的选择分配给表现最好的臂，0%分配给其他臂。这样的分配将固有地导致0的遗憾值，表示最佳性能。'
- en: Running the simulation
  id: totrans-49
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 运行仿真
- en: Evaluating MAB algorithms poses challenges due to the highly stochastic nature
    of their outcomes. This means that because of the inherent randomness in determining
    quantities, the results can greatly vary from one run to another. For a robust
    evaluation, the most effective approach is to execute the target simulation multiple
    times, accumulate the results and metrics from each simulation, and then compute
    the average.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 评估MAB算法具有挑战性，因为其结果具有高度的随机性。这意味着由于确定量的固有随机性，结果在不同运行之间可能大相径庭。为了进行稳健的评估，最有效的方法是多次执行目标仿真，累积每次仿真的结果和指标，然后计算平均值。
- en: The initial step involves creating a function to simulate the decision-making
    process. This function will implement the feedback loop represented in the below
    image.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 初始步骤包括创建一个模拟决策过程的函数。这个函数将实现下图所示的反馈循环。
- en: '![](../Images/3b6378902599171138a32c2348195064.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3b6378902599171138a32c2348195064.png)'
- en: Feedback loop implemented in the simulation function
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在仿真函数中实现的反馈循环
- en: 'This is the implementation of the simulation loop:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 这是仿真循环的实现：
- en: 'The inputs to this function are:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 该函数的输入是：
- en: '`prices`: A list of candidate prices we wish to test (essentially our "arms").'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prices`：我们希望测试的候选价格列表（本质上是我们的“臂”）。'
- en: '`nstep`: The total number of steps in the simulation.'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`nstep`：仿真中的总步骤数。'
- en: '`strategy`: The algorithm we aim to test for making decisions on the next price.'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`strategy`：我们旨在测试的用于决策下一个价格的算法。'
- en: Lastly, we need to write the code for the outer loop. For every target strategy,
    this loop will call `run_simulation` multiple times, collect and aggregate the
    results from each execution, and then display the outcomes.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们需要编写外循环的代码。对于每个目标策略，这个循环将调用`run_simulation`多次，收集并汇总每次执行的结果，然后展示结果。
- en: 'For our analysis, we’ll use the following configuration parameters:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的分析，我们将使用以下配置参数：
- en: '`prices`: Our price candidates → [20, 30, 40, 50, 60]'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prices`：我们的价格候选值 → [20, 30, 40, 50, 60]'
- en: '`nstep`: Number of time steps for every simulation → 10000'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`nstep`：每次仿真的时间步数 → 10000'
- en: '`nepoch`: Number of simulation executions → 1000'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`nepoch`：仿真执行次数 → 1000'
- en: Furthermore, by setting our price candidates, we can promptly obtain the associated
    purchase probabilities, which are (approximately) [0.60, 0.44, 0.31, 0.22, 0.15].
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，通过设置我们的价格候选值，我们可以快速获得相关的购买概率，这些概率是（大约）[0.60, 0.44, 0.31, 0.22, 0.15]。
- en: Results
  id: totrans-65
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结果
- en: 'After running the simulation we are finally able to see some results. Let’s
    start from the plot of the cumulative regret:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在运行模拟之后，我们终于能够看到一些结果。我们从累计遗憾的图表开始：
- en: '![](../Images/38f0d7ed5a3eb34de655707750566df7.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/38f0d7ed5a3eb34de655707750566df7.png)'
- en: From the graph, we can see that TS is the winner in terms of mean cumulative
    regret, but it takes around 7,500 steps to surpass ε-greedy. On the other hand,
    we have a clear loser, which is UCB1\. In its basic configuration, it essentially
    performs on par with the greedy approach (we’ll get back to this later). Let’s
    try to understand the results better by exploring the other available metrics.
    In all four cases, the reactivity shows very large standard deviations, so we’ll
    focus on the median values instead of the means, as they are more resistant to
    outliers.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 从图表中，我们可以看到，在平均累计遗憾方面，TS 是赢家，但它需要大约 7,500 步才能超越 ε-greedy。另一方面，我们有一个明显的失败者，那就是
    UCB1。在其基本配置下，它基本上表现与贪婪方法相当（稍后我们会再讨论）。让我们通过探索其他可用的指标来更好地理解结果。在所有四种情况下，反应性表现出非常大的标准差，因此我们将关注中位数值而非均值，因为它们对离群值更具抵抗力。
- en: '![](../Images/6c7e6a229b568b00c4e47f91f695a5c6.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6c7e6a229b568b00c4e47f91f695a5c6.png)'
- en: 'The initial observation from the plots reveals that while TS surpasses ε-greedy
    in terms of the mean, it slightly lags behind in terms of the median. However,
    its standard deviation is smaller. Particularly interesting is the reactivity
    bar plot, which shows how TS struggles to rapidly achieve a favorable average
    reward. At first, this was counterintuitive to me, but the mechanism behind TS
    in this scenario clarified matters. We previously mentioned that TS estimates
    purchase probabilities. Yet, decisions are made based on the product of these
    probabilities and the prices. Having knowledge of the real probabilities (that,
    as mentioned, are [0.60, 0.44, 0.31, 0.22, 0.15]) allows us to calculate the expected
    rewards TS is actively navigating: [12.06, 13.25, 12.56, 10.90, 8.93]. In essence,
    although the underlying probabilities differ considerably, the expected revenue
    values are relatively close from its perspective, especially in proximity to the
    optimal price. This means TS requires more time to discern the optimal arm. While
    TS remains the top-performing algorithm (and its median eventually drops below
    that of the ε-greedy one if the simulation is prolonged), it demands a longer
    period to identify the best strategy in this context. Below, the arm allocation
    pies show how TS and ε-greedy do pretty well in identifying the best arm (price=30)
    and using it most of the time during the simulation.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 从图表中的初步观察显示，虽然 TS 在均值方面超越了 ε-greedy，但在中位数方面略显滞后。然而，其标准差较小。特别有趣的是反应性条形图，它展示了
    TS 如何努力快速实现有利的平均奖励。起初，这对我来说有些反直觉，但在这个场景中 TS 的机制澄清了问题。我们之前提到 TS 估计购买概率。然而，决策是基于这些概率和价格的乘积。了解真实概率（如前所述，[0.60,
    0.44, 0.31, 0.22, 0.15]）让我们能够计算 TS 正在积极导航的期望奖励：[12.06, 13.25, 12.56, 10.90, 8.93]。本质上，尽管基础概率差异较大，但从
    TS 的角度来看，期望收益值相对接近，尤其是在接近最佳价格时。这意味着 TS 需要更多时间来辨别最佳臂。虽然 TS 仍然是表现最好的算法（如果模拟时间延长，其中位数最终会低于
    ε-greedy 的中位数），但在这种情况下，它需要更长时间来确定最佳策略。下面的臂分配饼图显示 TS 和 ε-greedy 在识别最佳臂（价格=30）并在模拟过程中大部分时间使用它方面做得相当不错。
- en: '![](../Images/0770e428e56f9412f7911d1c7db5aa75.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0770e428e56f9412f7911d1c7db5aa75.png)'
- en: 'Now let’s get back to UCB1\. Regret and reactivity confirm that it’s basically
    acting as a fully exploitative algorithm: quick to get a good level of average
    reward but with big regret and high variability of the outcome. If we look at
    the arm allocations that’s even more clear. UCB1 is only slightly smarter than
    the Greedy approach because it focuses more on the 3 arms with higher expected
    rewards (prices 20, 30, and 40). However, it essentially doesn’t explore at all.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们回到 UCB1。遗憾和反应性确认它基本上作为一个完全利用的算法：快速获得良好的平均奖励水平，但遗憾较大且结果变异性高。如果我们查看臂分配，这一点更为明显。UCB1
    仅比贪婪方法稍微聪明一些，因为它更多地关注具有较高期望奖励的 3 个臂（价格为 20、30 和 40）。然而，它基本上完全没有探索。
- en: Enter hyperparameter tuning. It’s clear that we need to determine the optimal
    value of the weight *C* that balances exploration and exploitation. The first
    step is to modify the UCB1 code.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 进入超参数调优。显然，我们需要确定平衡探索与利用的权重 *C* 的最佳值。第一步是修改 UCB1 代码。
- en: In this updated code, I’ve incorporated the option to normalize the average
    reward before adding the “uncertainty bonus”, which is weighted by the hyperparameter
    *C*. The rationale for this is to allow for a consistent search range for the
    best hyperparameter (say 0.5–1.5). Without this normalization, we could achieve
    similar results, but the search interval would need adjustments based on the range
    of values we’re dealing with each time. I’ll spare you the boredom of finding
    the best *C* value; it can be easily determined through a grid search. It turns
    out that the optimal value is 0.7\. Now, let’s rerun the simulation and examine
    the results.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在这段更新的代码中，我加入了在添加“不确定性奖励”之前标准化平均奖励的选项，这个奖励是由超参数*C*加权的。这样做的原因是为了让最佳超参数的搜索范围保持一致（比如0.5–1.5）。如果没有这个标准化，我们可能会得到类似的结果，但搜索区间需要根据每次处理的值范围进行调整。我会避免让你寻找最佳*C*值的无聊，它可以通过网格搜索轻松确定。事实证明，最佳值是0.7。现在，让我们重新运行模拟并检查结果。
- en: '![](../Images/1c0d6f4f7ab70cd702cb46024b5a8901.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1c0d6f4f7ab70cd702cb46024b5a8901.png)'
- en: That’s quite the plot twist, isn’t it? Now, UCB1 is clearly the best algorithm.
    Even in terms of reactivity, it has only slightly deteriorated compared to the
    previous score.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 这真是个大反转，不是吗？现在，UCB1显然是最好的算法。即使在反应性方面，与之前的得分相比，它也只是略微恶化。
- en: '![](../Images/a4bfe414b0688683ed47a0492273c623.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a4bfe414b0688683ed47a0492273c623.png)'
- en: Additionally, from the perspective of arm allocation, UCB1 is now the undisputed
    leader.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，从臂分配的角度来看，UCB1现在是无可争议的领导者。
- en: '![](../Images/d80c8f229fe5c8e1288b4b104da06de9.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d80c8f229fe5c8e1288b4b104da06de9.png)'
- en: Lessons learned and next steps
  id: totrans-80
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 经验教训和下一步
- en: '**Theory vs. Experience**: Starting with book-based learning is an essential
    first step when delving into new topics. However, the sooner you immerse yourself
    in hands-on experiences, the faster you’ll transform information into knowledge.
    The nuances, subtleties, and corner cases you encounter when applying algorithms
    to real-world use cases will offer insights far beyond any data science book you
    might read.'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**理论与经验**：从书本学习开始是深入新主题的一个必要步骤。然而，你越早投入实际经验，你将越快将信息转化为知识。当你将算法应用到现实世界用例时，遇到的细微差别、复杂性和特殊情况将提供远超你可能阅读的任何数据科学书籍的洞察。'
- en: '**Know Your Metrics and Benchmarks**: If you can’t measure what you’re doing,
    you can’t improve it. Never begin any implementations without understanding the
    metrics you intend to use. Had I only considered regret curves, I might have concluded,
    “UCB1 doesn’t work.” By evaluating other metrics, especially arm allocation, it
    became evident that the algorithm simply wasn’t exploring sufficiently.'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**了解你的指标和基准**：如果你不能衡量你所做的事情，你就不能改进它。在开始任何实现之前，必须了解你打算使用的指标。如果我仅仅考虑了遗憾曲线，我可能会得出“UCB1不起作用”的结论。通过评估其他指标，特别是臂分配，明显发现算法只是没有足够探索。'
- en: '**No One-Size-Fits-All solutions**: While UCB1 emerged as the top choice in
    our analysis, it doesn’t imply it’s the universal solution for your dynamic pricing
    challenge. In this scenario, tuning was straightforward because we knew the optimal
    value we sought. In real life, situations are never so clear-cut. Do you possess
    enough domain knowledge or the means to test and adjust your exploration factor
    for the UCB1 algorithm? Perhaps you’d lean towards a reliably effective option
    like ε-greedy that promises immediate results. Or, you might be managing a bustling
    e-commerce platform, showcasing a product 10000 times per hour, and you’re willing
    to be patient, confident that Thompson Sampling will attain the maximum cumulative
    reward eventually. Yeah, life ain’t easy.'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**没有一刀切的解决方案**：虽然UCB1在我们的分析中脱颖而出，但这并不意味着它是你动态定价挑战的普遍解决方案。在这种情况下，调优相对简单，因为我们知道我们要寻找的最佳值。在现实生活中，情况从未如此明确。你是否具备足够的领域知识或手段来测试和调整UCB1算法的探索因子？也许你会倾向于像ε-贪婪这样的可靠有效选项，承诺立即见效。或者，你可能正在管理一个繁忙的电子商务平台，每小时展示10000次产品，你愿意耐心等待，相信汤普森采样最终会获得最大的累计奖励。是的，生活并不容易。'
- en: Finally, let me say that if this analysis seemed daunting, unfortunately, it
    already represents a very simplified situation. In real-world dynamic pricing,
    prices and purchase probabilities don’t exist in a vacuum — they actually exist
    in ever-changing environments and they’re influenced by various factors. For example,
    it’s highly improbable that purchase probability remains consistent throughout
    the year, across all customer demographics and regions. In other words, to optimize
    pricing decisions, we must consider our customers’ contexts. This consideration
    will be the focal point of my next article, where I’ll delve deeper into the problem
    by integrating customer information and discussing Contextual Bandits. So, stay
    tuned!
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，若这项分析看起来令人生畏，不幸的是，它已经代表了一个非常简化的情况。在现实世界的动态定价中，价格和购买概率并不是在真空中存在的——它们实际上存在于不断变化的环境中，并受到各种因素的影响。例如，购买概率在一年内、不同的客户群体和地区之间保持一致的可能性极低。换句话说，为了优化定价决策，我们必须考虑客户的背景。这一考虑将是我下一篇文章的重点，我将在文中通过整合客户信息和讨论**上下文赌博机**进一步探讨这一问题。所以，请继续关注！
- en: '[Here](https://medium.com/towards-data-science/dynamic-pricing-with-contextual-bandits-learning-by-doing-b88e49f55894)
    is the continuation to this article about Contextual Bandits!'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '[这里](https://medium.com/towards-data-science/dynamic-pricing-with-contextual-bandits-learning-by-doing-b88e49f55894)
    是关于上下文赌博机的文章续篇！'
- en: Code Repository
  id: totrans-86
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 代码库
- en: '[https://github.com/massi82/multi-armed-bandit](https://github.com/massi82/multi-armed-bandit)'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/massi82/multi-armed-bandit](https://github.com/massi82/multi-armed-bandit)'
- en: References
  id: totrans-88
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[https://www.amazon.it/Reinforcement-Learning-Introduction-Richard-Sutton/dp/0262039249](https://www.amazon.it/Reinforcement-Learning-Introduction-Richard-Sutton/dp/0262039249)'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://www.amazon.it/Reinforcement-Learning-Introduction-Richard-Sutton/dp/0262039249](https://www.amazon.it/Reinforcement-Learning-Introduction-Richard-Sutton/dp/0262039249)'
- en: '[https://www.geeksforgeeks.org/epsilon-greedy-algorithm-in-reinforcement-learning/](https://www.geeksforgeeks.org/epsilon-greedy-algorithm-in-reinforcement-learning/)'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://www.geeksforgeeks.org/epsilon-greedy-algorithm-in-reinforcement-learning/](https://www.geeksforgeeks.org/epsilon-greedy-algorithm-in-reinforcement-learning/)'
- en: '[https://towardsdatascience.com/multi-armed-bandits-upper-confidence-bound-algorithms-with-python-code-a977728f0e2d](/multi-armed-bandits-upper-confidence-bound-algorithms-with-python-code-a977728f0e2d)'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://towardsdatascience.com/multi-armed-bandits-upper-confidence-bound-algorithms-with-python-code-a977728f0e2d](/multi-armed-bandits-upper-confidence-bound-algorithms-with-python-code-a977728f0e2d)'
- en: '[https://towardsdatascience.com/thompson-sampling-fc28817eacb8](/thompson-sampling-fc28817eacb8)'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://towardsdatascience.com/thompson-sampling-fc28817eacb8](/thompson-sampling-fc28817eacb8)'
- en: '[https://www.youtube.com/watch?v=e3L4VocZnnQ](https://www.youtube.com/watch?v=e3L4VocZnnQ)'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://www.youtube.com/watch?v=e3L4VocZnnQ](https://www.youtube.com/watch?v=e3L4VocZnnQ)'
- en: '[https://www.youtube.com/watch?v=e3L4VocZnnQ](https://www.youtube.com/watch?v=e3L4VocZnnQ)'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://www.youtube.com/watch?v=e3L4VocZnnQ](https://www.youtube.com/watch?v=e3L4VocZnnQ)'
- en: '[https://www.youtube.com/watch?v=Zgwfw3bzSmQ&t=2s](https://www.youtube.com/watch?v=Zgwfw3bzSmQ&t=2s)'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://www.youtube.com/watch?v=Zgwfw3bzSmQ&t=2s](https://www.youtube.com/watch?v=Zgwfw3bzSmQ&t=2s)'
- en: Did you enjoy this article? If you’re looking for applications of AI, NLP, Machine
    Learning and Data Analytics in solving real-world problems, you’ll likely enjoy
    my other work as well. My goal is to craft actionable articles that show these
    transformative technologies in practical scenarios. If this is also you, follow
    me on Medium to stay informed about my latest pieces!
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 你喜欢这篇文章吗？如果你对AI、自然语言处理、机器学习和数据分析在解决现实问题中的应用感兴趣，你可能也会喜欢我的其他作品。我的目标是撰写可操作的文章，展示这些变革性技术在实际场景中的应用。如果你也是这样的人，可以在Medium上关注我，以了解我最新的文章！
