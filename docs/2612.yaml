- en: 'Dynamic Pricing with Multi-Armed Bandits: Learning by Doing'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/dynamic-pricing-with-multi-armed-bandit-learning-by-doing-3e4550ed02ac?source=collection_archive---------1-----------------------#2023-08-16](https://towardsdatascience.com/dynamic-pricing-with-multi-armed-bandit-learning-by-doing-3e4550ed02ac?source=collection_archive---------1-----------------------#2023-08-16)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Applying Reinforcement Learning strategies to real-world use cases, especially
    in dynamic pricing, can reveal many surprises
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@massi.costacurta?source=post_page-----3e4550ed02ac--------------------------------)[![Massimiliano
    Costacurta](../Images/599c3469021c53f116cc67c390db6695.png)](https://medium.com/@massi.costacurta?source=post_page-----3e4550ed02ac--------------------------------)[](https://towardsdatascience.com/?source=post_page-----3e4550ed02ac--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----3e4550ed02ac--------------------------------)
    [Massimiliano Costacurta](https://medium.com/@massi.costacurta?source=post_page-----3e4550ed02ac--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F233cb43234c3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdynamic-pricing-with-multi-armed-bandit-learning-by-doing-3e4550ed02ac&user=Massimiliano+Costacurta&userId=233cb43234c3&source=post_page-233cb43234c3----3e4550ed02ac---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----3e4550ed02ac--------------------------------)
    ·16 min read·Aug 16, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F3e4550ed02ac&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdynamic-pricing-with-multi-armed-bandit-learning-by-doing-3e4550ed02ac&user=Massimiliano+Costacurta&userId=233cb43234c3&source=-----3e4550ed02ac---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F3e4550ed02ac&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdynamic-pricing-with-multi-armed-bandit-learning-by-doing-3e4550ed02ac&source=-----3e4550ed02ac---------------------bookmark_footer-----------)![](../Images/ee7ee77f9d7a623044975573c7cd42b4.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: Photo by [Markus Spiske](https://unsplash.com/@markusspiske?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: Dynamic Pricing, Reinforcement Learning and Multi-Armed Bandit
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the vast world of decision-making problems, one dilemma is particularly
    owned by Reinforcement Learning strategies: exploration versus exploitation. Imagine
    walking into a casino with rows of slot machines (also known as “one-armed bandits”)
    where each machine pays out a different, unknown reward. Do you explore and play
    each machine to discover which one has the highest payout, or do you stick to
    one machine, hoping it’s the jackpot? This metaphorical scenario underpins the
    concept of the Multi-armed Bandit (MAB) problem. The objective is to find a strategy
    that maximizes the rewards over a series of plays. While exploration offers new
    insights, exploitation leverages the information you already possess.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: Now, transpose this principle to dynamic pricing in a retail scenario. Suppose
    you are an e-commerce store owner with a new product. You aren’t certain about
    its optimal selling price. How do you set a price that maximizes your revenue?
    Should you explore different prices to understand customer willingness to pay,
    or should you exploit a price that has been performing well historically? Dynamic
    pricing is essentially a MAB problem in disguise. At each time step, every candidate
    price point can be seen as an “arm” of a slot machine and the revenue generated
    from that price is its “reward.” Another way to see this is that the objective
    of dynamic pricing is to swiftly and accurately measure how a customer base’s
    demand reacts to varying price points. In simpler terms, the aim is to pinpoint
    the demand curve that best mirrors customer behavior.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: In this article, we’ll explore four Multi-armed Bandit algorithms to evaluate
    their efficacy against a well-defined (though not straightforward) demand curve.
    We’ll then dissect the primary strengths and limitations of each algorithm and
    delve into the key metrics that are instrumental in gauging their performance.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: Modeling The Demand Curve
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Traditionally, demand curves in economics describe the relationship between
    the price of a product and the quantity of the product that consumers are willing
    to buy. They generally slope downwards, representing the common observation that
    as price rises, demand typically falls, and vice-versa. Think of popular products
    such as smartphones or concert tickets. If prices are lowered, more people tend
    to buy, but if prices skyrocket, even the ardent fans might think twice.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: 'Yet in our context, we’ll model the demand curve slightly differently: we’re
    putting price against probability. Why? Because in dynamic pricing scenarios,
    especially digital goods or services, it’s often more meaningful to think in terms
    of the likelihood of a sale at a given price than to speculate on exact quantities.
    In such environments, each pricing attempt can be seen as an exploration of the
    likelihood of success (or purchase), which can be easily modeled as a Bernoulli
    random variable with a probability *p* depending on a given test price.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s where it gets particularly interesting: while intuitively one might
    think the task of our Multi-armed Bandit algorithms is to unearth that ideal price
    where the probability of purchase is highest, it’s not quite so straightforward.
    In fact, our ultimate goal is to maximize the revenue (or the margin). This means
    we’re not searching for the price that gets the most people to click ‘buy’ — we’re
    searching for the price that, when multiplied by its associated purchase probability,
    gives the highest expected return. Imagine setting a high price that fewer people
    buy, but each sale generates significant revenue. On the flip side, a very low
    price might attract more buyers, but the total revenue might still be lower than
    the high price scenario. So, in our context, talking about the ‘demand curve’
    is somewhat unconventional, as our target curve will primarily represent the probability
    of purchase rather than the demand directly.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, getting to the math, let’s start by saying that consumer behavior, especially
    when dealing with price sensitivity, isn’t always linear. A linear model might
    suggest that for every incremental increase in price, there’s a constant decrement
    in demand. In reality, this relationship is often more complex and nonlinear.
    One way to model this behavior is by using logistic functions, which can capture
    this nuanced relationship more effectively. Our chosen model for the demand curve
    is then:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/fe4cc4f9e4698afc79a0ff4ee39a1cc6.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
- en: Here, *a* determines the maximum achievable probability of purchase, while *b*
    modulates the sensitivity of the demand curve against price changes. A higher
    value of *b* means a steeper curve, approaching more rapidly to lower purchase
    probabilities as the price increases.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/53098370c4b84e88030173af66333f01.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
- en: Four examples of demand curves with different combinations of parameters a and
    b
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: For any given price point, we’ll be then able to obtain an associated purchase
    probability, *p*. We can then input *p* into a Bernoulli random variable generator
    to simulate the response of a customer to a particular price proposal. In other
    words, given a price, we can easily emulate our reward function.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we can multiply this function by the price in order to get the expected
    revenue for a given price point:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/11ee2d7e1a22d6a93d5a9092c4bef301.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
- en: Unsurprisingly, this function does not reach its maximum in correspondence with
    the highest probability. Also, the price associated with the maximum does not
    depend on the value of the parameter *a*, while the maximum expected return does.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3676422659c3945a959f3ddd154b8a0a.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
- en: Expected revenue curves with related maxima
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: 'With some recollection from calculus, we can also derive the formula for the
    derivative (you’ll need to use a combination of both the product and the chain
    rule). It’s not exactly a relaxing exercise, but it’s nothing too challenging.
    Here is the analytical expression of the derivative of the expected revenue:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 结合一些微积分知识，我们还可以推导出导数的公式（你需要同时使用乘法法则和链式法则）。这不是一个轻松的练习，但也没有特别困难的地方。这是期望收入的导数的解析表达式：
- en: '![](../Images/6b4aca3305e35203517b213539b0380d.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6b4aca3305e35203517b213539b0380d.png)'
- en: This derivative allows us to find the exact price that maximizes our expected
    revenue curve. In other words, by using this specific formula in tandem with some
    numerical algorithms, we can easily determine the price that sets it to 0\. This,
    in turn, is the price that maximizes the expected revenue.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 这个导数允许我们找到最大化期望收入曲线的确切价格。换句话说，通过使用这个特定的公式以及一些数值算法，我们可以轻松确定将其设为0的价格。反过来，这就是最大化期望收入的价格。
- en: 'And this is exactly what we need, since by fixing the values of *a* and *b*,
    we’ll immediately know the target price that our bandits will have to find. Coding
    this in Python is a matter of a few lines of code:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 这正是我们需要的，因为通过固定*a*和*b*的值，我们将立即知道我们的赌博机需要找到的目标价格。用Python编码这只需几行代码：
- en: For our use case, we’ll set *a* = 2 and *b* = 0.042, which will give us a target
    price of about 30.44, associated with an optimal probability of 0.436 ( → optimal
    average reward is 30.44*0.436=13.26). This price is obviously unknown in general
    and it is exactly the price that our Multi-armed Bandit algorithms will seek.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的用例，我们将设置*a*=2和*b*=0.042，这将给我们一个大约30.44的目标价格，关联的最佳概率为0.436（→最佳平均奖励为30.44*0.436=13.26）。这个价格在一般情况下显然是未知的，正是我们的多臂赌博机算法将要寻找的价格。
- en: Multi-armed Bandit Strategies
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 多臂赌博机策略
- en: 'Now that we’ve identified our objectives, it’s time to explore various strategies
    for testing and analyzing their performance, strengths, and weaknesses. While
    several algorithms exist in MAB literature, when it comes to real-world scenarios,
    four primary strategies (along with their variations) predominantly form the backbone.
    In this section, we’ll provide a brief overview of these strategies. We assume
    the reader has a foundational understanding of them; however, for those interested
    in a more in-depth exploration, references are provided at the end of the article.
    After introducing each algorithm, we’ll also present its Python implementation.
    Although each algorithm possesses its unique set of parameters, they all commonly
    utilize one key input: the `arm_avg_reward` vector. This vector denotes the average
    reward garnered from each arm (or action/price) up to the current time step *t*.
    This critical input guides all the algorithms in making informed decisions about
    the subsequent price setting.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经确定了目标，现在是时候探索各种策略以测试和分析它们的性能、优点和缺点了。虽然在MAB文献中存在几种算法，但在实际应用中，四种主要策略（及其变种）主要构成了基础。在本节中，我们将简要概述这些策略。我们假设读者对这些策略有基本了解；不过，对于那些有兴趣深入研究的人，我们在文章末尾提供了参考文献。在介绍每个算法后，我们还将展示其Python实现。尽管每种算法具有其独特的参数，但它们都普遍使用一个关键输入：`arm_avg_reward`向量。该向量表示当前时间步*t*为止每个臂（或动作/价格）获得的平均奖励。这个关键输入指导所有算法做出有关后续价格设置的明智决策。
- en: 'The algorithms I am going to apply to our dynamic pricing problem are the following:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 我将应用于我们的动态定价问题的算法如下：
- en: '**Greedy**: This strategy is like always going back to the machine that gave
    you the most coins the first few times you played. After trying out each machine
    a bit, it sticks with the one that seemed the best. But there might be a problem.
    What if that machine was just lucky at the start? The Greedy strategy might miss
    out on better options. On the bright side, the code implementation is really simple:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '**贪婪策略**：这一策略就像是每次都回到最初给你最多硬币的机器。尝试过每台机器后，它会坚持最初表现最好的那一台。但可能会有一个问题。如果那台机器只是最初运气好呢？贪婪策略可能会错过更好的选项。幸运的是，代码实现非常简单：'
- en: It’s essential to differentiate the initial scenario (when all rewards are 0)
    from the regular one. Often, you’ll find only the ‘else’ part implemented, which
    indeed works even when all rewards are at 0\. Yet, this approach can lead to a
    bias toward the first element. If you make this oversight, you might end up paying
    that bias, particularly if the optimal reward happens to be tied to the first
    arm (yes, I’ve been there). The Greedy approach is typically the least-performing
    one and we’ll primarily use it as our performance baseline.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 区分初始情境（所有回报均为 0 时）与常规情境是至关重要的。通常，你会发现只实现了‘else’部分，这确实在所有回报为 0 时也能工作。然而，这种方法可能导致对第一个元素的偏见。如果忽视这一点，你可能会因此付出代价，特别是当最优回报恰好与第一个臂相关时（是的，我经历过）。贪婪方法通常表现最差，我们将主要使用它作为性能基准。
- en: '***ϵ*-greedy**: The ε-greedy (epsilon-greedy) algorithm is a modification to
    tackle the main drawback of the greedy approach. It introduces a probability *ε*
    (epsilon), typically a small value, to select a random arm, promoting exploration.
    With a probability 1−*ε*, it chooses the arm with the highest estimated reward,
    favoring exploitation. By balancing between random exploration and exploitation
    of known rewards, the ε-greedy strategy aims to achieve better long-term returns
    compared to purely greedy methods. Again, the implementation is immediate, it’s
    simply an additional ‘if’ on top of the Greedy code.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '***ϵ*-贪婪**：ε-贪婪（epsilon-greedy）算法是对贪婪方法主要缺陷的一种改进。它引入了一个概率 *ε*（epsilon），通常是一个小值，用于选择一个随机的臂，促进探索。以概率
    1−*ε*，它选择回报估计最高的臂，偏向于利用。通过在随机探索和已知回报的利用之间进行平衡，ε-贪婪策略旨在实现比纯粹的贪婪方法更好的长期回报。同样，这种实现是直接的，只需在贪婪代码上添加一个额外的‘if’。'
- en: '**UCB1 (Upper Confidence Bound)**: The UCB1 strategy is like a curious explorer
    trying to find the best restaurant in a new city. While there’s a favorite spot
    they’ve enjoyed, the allure of potentially discovering an even better place grows
    with each passing day. In our context, UCB1 combines the rewards of known price
    points with the uncertainty of those less explored. Mathematically, this balance
    is achieved through a formula: the average reward of a price point plus an “uncertainty
    bonus” based on how long since it was last tried. This bonus is calculated as'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '**UCB1（上置信界）**：UCB1 策略就像一个好奇的探险者，试图在新城市找到最好的餐馆。虽然有一个他们已经喜欢的地方，但每天都会有可能发现更好的地方。在我们的上下文中，UCB1
    将已知价格点的回报与那些较少探索的价格点的不确定性相结合。数学上，这种平衡是通过一个公式实现的：价格点的平均回报加上一个基于距离上次尝试的时间的“未知奖励”奖金。这个奖金计算为'
- en: '![](../Images/78e04b458368a5ab1e4de22f9a30bdf3.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/78e04b458368a5ab1e4de22f9a30bdf3.png)'
- en: and represents the “growing curiosity” about the untried price. The hyperparameter
    *C* controls the balance between exploitation and exploration, with higher values
    of *C* encouraging more exploration of less-sampled arms. By always selecting
    the price with the highest combined value of known reward and curiosity bonus,
    UCB1 ensures a mix of sticking to what’s known and venturing into the unknown,
    aiming to uncover the optimal price point for maximum revenue. I’ll start with
    the by-the-book implementation of this approach, but we’ll soon see that we need
    to tweak it a bit.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 并代表了对未尝试价格的“增长的好奇心”。超参数 *C* 控制利用和探索之间的平衡，*C* 值较高时，鼓励更多探索较少采样的臂。通过始终选择已知回报和好奇心奖金的组合值最高的价格，UCB1
    确保了既坚持已知又探索未知的混合，旨在揭示最大收入的最优价格点。我会从按部就班的实施开始，但我们很快会看到需要对其进行一些调整。
- en: '**Thompson Sampling**: This Bayesian approach addresses the exploration-exploitation
    dilemma by probabilistically selecting arms based on their posterior reward distributions.
    When these rewards adhere to a Bernoulli distribution, representing binary outcomes
    like success/failure, Thompson Sampling (TS) employs the Beta distribution as
    a conjugate prior (see [this table](https://en.wikipedia.org/wiki/Conjugate_prior)
    for reference). Initiating with a non-informative Beta(1,1) prior for every arm,
    the algorithm updates the distribution’s parameters upon observing rewards: a
    success increases the alpha parameter, while a failure augments the beta. During
    each play, TS draws from the current Beta distribution of each arm and opts for
    the one with the top sampled value. This methodology allows TS to dynamically
    adjust based on gathered rewards, adeptly balancing between the exploration of
    uncertain arms and the exploitation of those known to be rewarding. In our specific
    scenario, although the foundational reward function follows a Bernoulli distribution
    (1 for a purchase and 0 for a missed purchase), the actual reward of interest
    is the product of this basic reward and the current price under test. Hence, our
    implementation of TS will need a slight modification (which will also introduce
    some surprises).'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: 'The change is actually pretty simple: to determine the most promising next
    arm, samples extracted from the posterior estimates are multiplied by their respective
    price points (line 3). This modification ensures decisions are anchored on the
    anticipated average revenue, shifting the focus from the highest purchase probability.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: How do we evaluate the results?
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'At this point, having gathered all the key ingredients to construct a simulation
    comparing the performance of the four algorithms in our dynamic pricing context,
    we must ask ourselves: what exactly will we be measuring? The metrics we choose
    are pivotal, as they will guide us in the process of both comparing and improving
    the algorithm implementation. In this endeavor, I’m zeroing in on three key indicators:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: '**Regret**: This metric measures the difference between the reward obtained
    by the chosen action and the reward that would have been obtained by taking the
    best possible action. Mathematically, regret at time *t* is given by: Regret(*t*)=Optimal
    Reward(*t*)−Actual Reward(*t*). Regret, when accumulated over time, provides insight
    into how much we’ve “lost” by not always choosing the best action. It is preferred
    over cumulative reward because it provides a clearer indication of the algorithm’s
    performance relative to the optimal scenario. Ideally, a regret value close to
    0 indicates proximity to optimal decision-making.'
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Reactivity**: This metric gauges the speed at which an algorithm approaches
    a target average reward. Essentially, it’s a measure of the algorithm’s adaptability
    and learning efficiency. The quicker an algorithm can achieve the desired average
    reward, the more reactive it is, implying a swifter adjustment to the optimal
    price point. In our case the target reward is set at 95% of the optimal average
    reward, which is 13.26\. However, initial steps can exhibit high variability.
    For instance, a lucky early choice might result in a success from a low probability
    arm associated with a high price, quickly achieving the threshold. Due to such
    fluctuations, I’ve opted for a stricter definition of reactivity: the number of
    steps required to attain 95% of the optimal average reward ten times, excluding
    the initial 100 steps.'
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Arms Allocation**: This indicates the frequency with which each algorithm
    utilizes the available arms. Presented as a percentage, it reveals the algorithm’s
    propensity to select each arm over time. Ideally, for the most efficient pricing
    strategy, we’d want an algorithm to allocate 100% of its choices to the best-performing
    arm and 0% to the rest. Such an allocation would inherently lead to a regret value
    of 0, denoting optimal performance.'
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Running the simulation
  id: totrans-49
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Evaluating MAB algorithms poses challenges due to the highly stochastic nature
    of their outcomes. This means that because of the inherent randomness in determining
    quantities, the results can greatly vary from one run to another. For a robust
    evaluation, the most effective approach is to execute the target simulation multiple
    times, accumulate the results and metrics from each simulation, and then compute
    the average.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: The initial step involves creating a function to simulate the decision-making
    process. This function will implement the feedback loop represented in the below
    image.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3b6378902599171138a32c2348195064.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
- en: Feedback loop implemented in the simulation function
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: 'This is the implementation of the simulation loop:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: 'The inputs to this function are:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: '`prices`: A list of candidate prices we wish to test (essentially our "arms").'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`nstep`: The total number of steps in the simulation.'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`strategy`: The algorithm we aim to test for making decisions on the next price.'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lastly, we need to write the code for the outer loop. For every target strategy,
    this loop will call `run_simulation` multiple times, collect and aggregate the
    results from each execution, and then display the outcomes.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: 'For our analysis, we’ll use the following configuration parameters:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: '`prices`: Our price candidates → [20, 30, 40, 50, 60]'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`nstep`: Number of time steps for every simulation → 10000'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`nepoch`: Number of simulation executions → 1000'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Furthermore, by setting our price candidates, we can promptly obtain the associated
    purchase probabilities, which are (approximately) [0.60, 0.44, 0.31, 0.22, 0.15].
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: Results
  id: totrans-65
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'After running the simulation we are finally able to see some results. Let’s
    start from the plot of the cumulative regret:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/38f0d7ed5a3eb34de655707750566df7.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
- en: From the graph, we can see that TS is the winner in terms of mean cumulative
    regret, but it takes around 7,500 steps to surpass ε-greedy. On the other hand,
    we have a clear loser, which is UCB1\. In its basic configuration, it essentially
    performs on par with the greedy approach (we’ll get back to this later). Let’s
    try to understand the results better by exploring the other available metrics.
    In all four cases, the reactivity shows very large standard deviations, so we’ll
    focus on the median values instead of the means, as they are more resistant to
    outliers.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6c7e6a229b568b00c4e47f91f695a5c6.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
- en: 'The initial observation from the plots reveals that while TS surpasses ε-greedy
    in terms of the mean, it slightly lags behind in terms of the median. However,
    its standard deviation is smaller. Particularly interesting is the reactivity
    bar plot, which shows how TS struggles to rapidly achieve a favorable average
    reward. At first, this was counterintuitive to me, but the mechanism behind TS
    in this scenario clarified matters. We previously mentioned that TS estimates
    purchase probabilities. Yet, decisions are made based on the product of these
    probabilities and the prices. Having knowledge of the real probabilities (that,
    as mentioned, are [0.60, 0.44, 0.31, 0.22, 0.15]) allows us to calculate the expected
    rewards TS is actively navigating: [12.06, 13.25, 12.56, 10.90, 8.93]. In essence,
    although the underlying probabilities differ considerably, the expected revenue
    values are relatively close from its perspective, especially in proximity to the
    optimal price. This means TS requires more time to discern the optimal arm. While
    TS remains the top-performing algorithm (and its median eventually drops below
    that of the ε-greedy one if the simulation is prolonged), it demands a longer
    period to identify the best strategy in this context. Below, the arm allocation
    pies show how TS and ε-greedy do pretty well in identifying the best arm (price=30)
    and using it most of the time during the simulation.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0770e428e56f9412f7911d1c7db5aa75.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
- en: 'Now let’s get back to UCB1\. Regret and reactivity confirm that it’s basically
    acting as a fully exploitative algorithm: quick to get a good level of average
    reward but with big regret and high variability of the outcome. If we look at
    the arm allocations that’s even more clear. UCB1 is only slightly smarter than
    the Greedy approach because it focuses more on the 3 arms with higher expected
    rewards (prices 20, 30, and 40). However, it essentially doesn’t explore at all.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: Enter hyperparameter tuning. It’s clear that we need to determine the optimal
    value of the weight *C* that balances exploration and exploitation. The first
    step is to modify the UCB1 code.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
- en: In this updated code, I’ve incorporated the option to normalize the average
    reward before adding the “uncertainty bonus”, which is weighted by the hyperparameter
    *C*. The rationale for this is to allow for a consistent search range for the
    best hyperparameter (say 0.5–1.5). Without this normalization, we could achieve
    similar results, but the search interval would need adjustments based on the range
    of values we’re dealing with each time. I’ll spare you the boredom of finding
    the best *C* value; it can be easily determined through a grid search. It turns
    out that the optimal value is 0.7\. Now, let’s rerun the simulation and examine
    the results.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在这段更新的代码中，我加入了在添加“不确定性奖励”之前标准化平均奖励的选项，这个奖励是由超参数*C*加权的。这样做的原因是为了让最佳超参数的搜索范围保持一致（比如0.5–1.5）。如果没有这个标准化，我们可能会得到类似的结果，但搜索区间需要根据每次处理的值范围进行调整。我会避免让你寻找最佳*C*值的无聊，它可以通过网格搜索轻松确定。事实证明，最佳值是0.7。现在，让我们重新运行模拟并检查结果。
- en: '![](../Images/1c0d6f4f7ab70cd702cb46024b5a8901.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1c0d6f4f7ab70cd702cb46024b5a8901.png)'
- en: That’s quite the plot twist, isn’t it? Now, UCB1 is clearly the best algorithm.
    Even in terms of reactivity, it has only slightly deteriorated compared to the
    previous score.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 这真是个大反转，不是吗？现在，UCB1显然是最好的算法。即使在反应性方面，与之前的得分相比，它也只是略微恶化。
- en: '![](../Images/a4bfe414b0688683ed47a0492273c623.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a4bfe414b0688683ed47a0492273c623.png)'
- en: Additionally, from the perspective of arm allocation, UCB1 is now the undisputed
    leader.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，从臂分配的角度来看，UCB1现在是无可争议的领导者。
- en: '![](../Images/d80c8f229fe5c8e1288b4b104da06de9.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d80c8f229fe5c8e1288b4b104da06de9.png)'
- en: Lessons learned and next steps
  id: totrans-80
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 经验教训和下一步
- en: '**Theory vs. Experience**: Starting with book-based learning is an essential
    first step when delving into new topics. However, the sooner you immerse yourself
    in hands-on experiences, the faster you’ll transform information into knowledge.
    The nuances, subtleties, and corner cases you encounter when applying algorithms
    to real-world use cases will offer insights far beyond any data science book you
    might read.'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**理论与经验**：从书本学习开始是深入新主题的一个必要步骤。然而，你越早投入实际经验，你将越快将信息转化为知识。当你将算法应用到现实世界用例时，遇到的细微差别、复杂性和特殊情况将提供远超你可能阅读的任何数据科学书籍的洞察。'
- en: '**Know Your Metrics and Benchmarks**: If you can’t measure what you’re doing,
    you can’t improve it. Never begin any implementations without understanding the
    metrics you intend to use. Had I only considered regret curves, I might have concluded,
    “UCB1 doesn’t work.” By evaluating other metrics, especially arm allocation, it
    became evident that the algorithm simply wasn’t exploring sufficiently.'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**了解你的指标和基准**：如果你不能衡量你所做的事情，你就不能改进它。在开始任何实现之前，必须了解你打算使用的指标。如果我仅仅考虑了遗憾曲线，我可能会得出“UCB1不起作用”的结论。通过评估其他指标，特别是臂分配，明显发现算法只是没有足够探索。'
- en: '**No One-Size-Fits-All solutions**: While UCB1 emerged as the top choice in
    our analysis, it doesn’t imply it’s the universal solution for your dynamic pricing
    challenge. In this scenario, tuning was straightforward because we knew the optimal
    value we sought. In real life, situations are never so clear-cut. Do you possess
    enough domain knowledge or the means to test and adjust your exploration factor
    for the UCB1 algorithm? Perhaps you’d lean towards a reliably effective option
    like ε-greedy that promises immediate results. Or, you might be managing a bustling
    e-commerce platform, showcasing a product 10000 times per hour, and you’re willing
    to be patient, confident that Thompson Sampling will attain the maximum cumulative
    reward eventually. Yeah, life ain’t easy.'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**没有一刀切的解决方案**：虽然UCB1在我们的分析中脱颖而出，但这并不意味着它是你动态定价挑战的普遍解决方案。在这种情况下，调优相对简单，因为我们知道我们要寻找的最佳值。在现实生活中，情况从未如此明确。你是否具备足够的领域知识或手段来测试和调整UCB1算法的探索因子？也许你会倾向于像ε-贪婪这样的可靠有效选项，承诺立即见效。或者，你可能正在管理一个繁忙的电子商务平台，每小时展示10000次产品，你愿意耐心等待，相信汤普森采样最终会获得最大的累计奖励。是的，生活并不容易。'
- en: Finally, let me say that if this analysis seemed daunting, unfortunately, it
    already represents a very simplified situation. In real-world dynamic pricing,
    prices and purchase probabilities don’t exist in a vacuum — they actually exist
    in ever-changing environments and they’re influenced by various factors. For example,
    it’s highly improbable that purchase probability remains consistent throughout
    the year, across all customer demographics and regions. In other words, to optimize
    pricing decisions, we must consider our customers’ contexts. This consideration
    will be the focal point of my next article, where I’ll delve deeper into the problem
    by integrating customer information and discussing Contextual Bandits. So, stay
    tuned!
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，若这项分析看起来令人生畏，不幸的是，它已经代表了一个非常简化的情况。在现实世界的动态定价中，价格和购买概率并不是在真空中存在的——它们实际上存在于不断变化的环境中，并受到各种因素的影响。例如，购买概率在一年内、不同的客户群体和地区之间保持一致的可能性极低。换句话说，为了优化定价决策，我们必须考虑客户的背景。这一考虑将是我下一篇文章的重点，我将在文中通过整合客户信息和讨论**上下文赌博机**进一步探讨这一问题。所以，请继续关注！
- en: '[Here](https://medium.com/towards-data-science/dynamic-pricing-with-contextual-bandits-learning-by-doing-b88e49f55894)
    is the continuation to this article about Contextual Bandits!'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '[这里](https://medium.com/towards-data-science/dynamic-pricing-with-contextual-bandits-learning-by-doing-b88e49f55894)
    是关于上下文赌博机的文章续篇！'
- en: Code Repository
  id: totrans-86
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 代码库
- en: '[https://github.com/massi82/multi-armed-bandit](https://github.com/massi82/multi-armed-bandit)'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/massi82/multi-armed-bandit](https://github.com/massi82/multi-armed-bandit)'
- en: References
  id: totrans-88
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[https://www.amazon.it/Reinforcement-Learning-Introduction-Richard-Sutton/dp/0262039249](https://www.amazon.it/Reinforcement-Learning-Introduction-Richard-Sutton/dp/0262039249)'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://www.amazon.it/Reinforcement-Learning-Introduction-Richard-Sutton/dp/0262039249](https://www.amazon.it/Reinforcement-Learning-Introduction-Richard-Sutton/dp/0262039249)'
- en: '[https://www.geeksforgeeks.org/epsilon-greedy-algorithm-in-reinforcement-learning/](https://www.geeksforgeeks.org/epsilon-greedy-algorithm-in-reinforcement-learning/)'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://www.geeksforgeeks.org/epsilon-greedy-algorithm-in-reinforcement-learning/](https://www.geeksforgeeks.org/epsilon-greedy-algorithm-in-reinforcement-learning/)'
- en: '[https://towardsdatascience.com/multi-armed-bandits-upper-confidence-bound-algorithms-with-python-code-a977728f0e2d](/multi-armed-bandits-upper-confidence-bound-algorithms-with-python-code-a977728f0e2d)'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://towardsdatascience.com/multi-armed-bandits-upper-confidence-bound-algorithms-with-python-code-a977728f0e2d](/multi-armed-bandits-upper-confidence-bound-algorithms-with-python-code-a977728f0e2d)'
- en: '[https://towardsdatascience.com/thompson-sampling-fc28817eacb8](/thompson-sampling-fc28817eacb8)'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://towardsdatascience.com/thompson-sampling-fc28817eacb8](/thompson-sampling-fc28817eacb8)'
- en: '[https://www.youtube.com/watch?v=e3L4VocZnnQ](https://www.youtube.com/watch?v=e3L4VocZnnQ)'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://www.youtube.com/watch?v=e3L4VocZnnQ](https://www.youtube.com/watch?v=e3L4VocZnnQ)'
- en: '[https://www.youtube.com/watch?v=e3L4VocZnnQ](https://www.youtube.com/watch?v=e3L4VocZnnQ)'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://www.youtube.com/watch?v=e3L4VocZnnQ](https://www.youtube.com/watch?v=e3L4VocZnnQ)'
- en: '[https://www.youtube.com/watch?v=Zgwfw3bzSmQ&t=2s](https://www.youtube.com/watch?v=Zgwfw3bzSmQ&t=2s)'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://www.youtube.com/watch?v=Zgwfw3bzSmQ&t=2s](https://www.youtube.com/watch?v=Zgwfw3bzSmQ&t=2s)'
- en: Did you enjoy this article? If you’re looking for applications of AI, NLP, Machine
    Learning and Data Analytics in solving real-world problems, you’ll likely enjoy
    my other work as well. My goal is to craft actionable articles that show these
    transformative technologies in practical scenarios. If this is also you, follow
    me on Medium to stay informed about my latest pieces!
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 你喜欢这篇文章吗？如果你对AI、自然语言处理、机器学习和数据分析在解决现实问题中的应用感兴趣，你可能也会喜欢我的其他作品。我的目标是撰写可操作的文章，展示这些变革性技术在实际场景中的应用。如果你也是这样的人，可以在Medium上关注我，以了解我最新的文章！
