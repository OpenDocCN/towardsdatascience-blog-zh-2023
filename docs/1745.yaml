- en: 'Unsupervised Machine Learning: Explore a List of Models that work without Output
    Labels'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/unsupervised-machine-learning-explore-a-list-of-models-that-work-without-output-labels-89322c2dd47c?source=collection_archive---------8-----------------------#2023-05-25](https://towardsdatascience.com/unsupervised-machine-learning-explore-a-list-of-models-that-work-without-output-labels-89322c2dd47c?source=collection_archive---------8-----------------------#2023-05-25)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Learn the underlying working of various unsupervised machine learning models
    and how they are able to generate predictions without output labels
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://suhas-maddali007.medium.com/?source=post_page-----89322c2dd47c--------------------------------)[![Suhas
    Maddali](../Images/933f27eab8ba9ee1f06ed2f24746d788.png)](https://suhas-maddali007.medium.com/?source=post_page-----89322c2dd47c--------------------------------)[](https://towardsdatascience.com/?source=post_page-----89322c2dd47c--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----89322c2dd47c--------------------------------)
    [Suhas Maddali](https://suhas-maddali007.medium.com/?source=post_page-----89322c2dd47c--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F2a74f90399ae&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funsupervised-machine-learning-explore-a-list-of-models-that-work-without-output-labels-89322c2dd47c&user=Suhas+Maddali&userId=2a74f90399ae&source=post_page-2a74f90399ae----89322c2dd47c---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----89322c2dd47c--------------------------------)
    ·12 min read·May 25, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F89322c2dd47c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funsupervised-machine-learning-explore-a-list-of-models-that-work-without-output-labels-89322c2dd47c&user=Suhas+Maddali&userId=2a74f90399ae&source=-----89322c2dd47c---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F89322c2dd47c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funsupervised-machine-learning-explore-a-list-of-models-that-work-without-output-labels-89322c2dd47c&source=-----89322c2dd47c---------------------bookmark_footer-----------)![](../Images/4c734dd8ce705ae62262520e855084db.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: Photo by [Jacopo Maia](https://unsplash.com/@ja_ma?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: Machine learning and artificial intelligence have gained **substantial** traction
    in the past decade. Numerous companies across various industries have created
    newer, more powerful models. The introduction and implementation of **large language
    models (LLMs)** have significantly increased attention towards the field of artificial
    intelligence. Companies from many sectors are bracing themselves for massive disruption
    in the realm of machine learning and general intelligence.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去的十年中，机器学习和人工智能取得了**巨大的**进展。许多不同行业的公司都创建了更新、更强大的模型。**大型语言模型(LLM)**的引入和实施显著增加了人们对人工智能领域的关注。来自许多行业的公司正准备迎接机器学习和通用智能领域的巨大变革。
- en: While these significant advancements in machine learning motivate people to
    learn the latest technologies in AI, there often lies a problem — a **lack** of
    high-quality, labeled data for successful application in this field. For machine
    learning to operate under supervision, it’s important to have labels for the training
    data. Access to this data can be quite challenging. Thus, acquiring this data
    can be costly and time-consuming as it requires human labor for manual labeling.
    Furthermore, this process can lead to inconsistent labeling issues.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这些机器学习方面的重大进展激励人们学习最新的人工智能技术，但往往存在一个问题——这个领域成功应用的高质量、标记数据的**缺乏**。为了让机器学习在监督下运作，对训练数据需要有标签。获得这些数据可能是相当具有挑战性的。因此，获取这些数据可能会很昂贵且费时，因为需要人工进行手动标记。此外，这个过程可能会导致标签不一致的问题。
- en: So, how can we utilize unlabeled data without having to manually label them?
    This is where unsupervised machine learning comes into play. As the name implies,
    this method suggests that data without output labels can be used during the training
    of unsupervised machine learning models. These models **identify** inherent patterns
    and trends in the data and learn to group them into various categories based on
    a specific set of attributes or features. After grouping them together, one can
    identify commonalities between the groups and use this information to drive business
    in the right direction.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，我们如何利用未标记的数据而不需要手动标记呢？这就是无监督机器学习的用武之地。顾名思义，这种方法认为在无监督机器学习模型的训练过程中可以使用没有输出标签的数据。这些模型会**识别**数据中固有的模式和趋势，并根据一组特定的属性或特征将其分组到不同的类别中。在将它们分组在一起之后，我们可以找出不同组之间的共同点，并利用这些信息将业务引导到正确的方向。
- en: '![](../Images/6ceb06cf1e93dc14a50da6b62ae31d72.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6ceb06cf1e93dc14a50da6b62ae31d72.png)'
- en: Photo by [Lucrezia Carnelos](https://unsplash.com/@ciabattespugnose?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 照片由[Lucrezia Carnelos](https://unsplash.com/@ciabattespugnose?utm_source=medium&utm_medium=referral)在[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)上提供引用。
- en: Consider, for instance, a store that wishes to sell products to a range of customers
    and offer discounts. However, they’re uncertain about who is likely to **transition**
    to their service. If customers don’t respond positively to their advertisements,
    this results in a loss of revenue for the business. In such a case, unsupervised
    machine learning could be employed to perform customer segmentation, grouping
    customers into categories. Consequently, the company can adjust its advertising
    strategy and target specific groups, ensuring successful conversions to purchase
    products. This is merely a simple business application of unsupervised machine
    learning. However, there are countless other applications in fields such as anomaly
    detection, text mining, image recognition, dimensionality reduction, and fraud
    detection.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 举例来说，考虑一个希望向各种类型的客户销售产品并提供折扣的商店。然而，他们不确定谁可能会**转换**成他们的服务对象。如果客户对他们的广告没有积极反应，这将导致企业收入的损失。在这种情况下，可以使用无监督机器学习来执行客户细分，将客户分组成不同的类别。因此，公司可以调整其广告策略并针对特定的群体进行定向，确保成功转化购买产品。这只是无监督机器学习在商业上的简单应用。然而，在异常检测、文本挖掘、图像识别、降维和欺诈检测等领域，还有无数其他的应用。
- en: Having seen the usefulness of unsupervised machine learning, it’s now time to
    delve deeper and explore a variety of these models and their potential use cases
    across different scenarios. It’s important to note that there are always pros
    and cons when using these models, and the best choice largely depends on the specific
    use case and constraints of a company.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 既然已经看到无监督机器学习的有用性，现在是时候更深入地探索这些模型及其在不同场景中的潜在用例。需要注意的是，使用这些模型时总是有优缺点的，最佳选择在很大程度上取决于公司的具体用例和限制。
- en: K-Means Clustering
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: K-means 聚类
- en: K-means clustering is a popular unsupervised machine learning model used in
    various industries for grouping a set of items into different categories. In this
    model, K is a hyperparameter that must be chosen based on either domain knowledge
    or by following standard techniques such as the elbow method or the silhouette
    analysis. Below are the list of steps that are usually followed to group the elements
    in our data into k clusters.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: K-means 聚类是一种流行的无监督机器学习模型，用于将一组项分组到不同的类别中。在此模型中，K是一个超参数，必须根据领域知识或通过标准技术（如肘部法则或轮廓分析）来选择。下面是通常用来将数据中的元素分组到k个簇中的步骤列表。
- en: Randomly initializes a set of data points as centroids based on the value of
    k. If the value of k is 10, for instance, it will randomly assign 10 data points
    and mark them as centroids.
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 随机初始化一组数据点作为质心，基于k的值。例如，如果k的值是10，则会随机选择10个数据点并将其标记为质心。
- en: After this step, the distance from each of the other points and these centroids
    is computed. The points that have the lowest distance between the centroids are
    assigned to that particular centroid. The distance metric can be either Euclidean
    distance, cosine distance or Manhattan distance.
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在此步骤之后，会计算每个数据点与这些质心之间的距离。距离最小的点会被分配到相应的质心。距离度量可以是欧几里得距离、余弦距离或曼哈顿距离。
- en: In this way, all the data points belong to any of the centroids that were randomly
    assigned in step 1.
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这样，所有的数据点都属于在步骤1中随机分配的质心中的一个。
- en: In order to determine a new centroid from each of the clusters, the mean across
    each cluster is taken and this is assigned as a new centroid. This step is followed
    for all the clusters that were formed.
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了从每个簇中确定一个新的质心，需要计算每个簇的均值，并将其指定为新的质心。这个步骤会对所有形成的簇执行。
- en: After finding a new set of centroids for each of the clusters, steps 2 till
    4 are repeated until convergence. In other words, the search for the best clusters
    is stopped as there is not a huge difference between the centroids from previous
    iteration to the present iteration.
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在为每个簇找到新的质心集后，步骤2到步骤4会重复进行，直到收敛。换句话说，寻找最佳簇的过程会停止，因为上一轮迭代的质心与当前迭代的质心之间没有太大差异。
- en: Although this approach is intuitive and there is good potential for it, there
    are some cons which we will discuss below along with the pros.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这种方法直观且具有很好的潜力，但它也有一些缺点，我们将在下文中讨论这些缺点以及优点。
- en: '**Pros**'
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**优点**'
- en: K-means clustering is known for its scalability and its ability to handle large
    datasets.
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: K-means 聚类因其可扩展性和处理大数据集的能力而闻名。
- en: The Interpretability of the algorithm is also good as the process of assigning
    data points with clusters is intuitive.
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 该算法的可解释性也很好，因为将数据点分配到簇的过程是直观的。
- en: Cons
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 缺点
- en: This algorithm is highly sensitive to the initial set of centroids chosen by
    it. On every run, we might get a different set of centroids as initial centroids
    that impact the final clustering output.
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 该算法对初始选择的质心集非常敏感。在每次运行时，我们可能会得到不同的初始质心集，这些初始质心会影响最终的聚类结果。
- en: If there are one or few outliers in the data, they have an impact on the final
    clustering structure with this algorithm.
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果数据中存在一个或少数几个离群点，它们会对该算法的最终聚类结构产生影响。
- en: The optimum number of clusters should be determined by the programmer rather
    than the algorithm finding the right number of clusters. This can sometimes be
    time-consuming.
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最优的簇数量应该由程序员确定，而不是由算法自动寻找。这有时可能会耗时较长。
- en: Hierarchical Clustering
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 层次聚类
- en: One of the cons of using a k-means clustering approach is to determine the total
    number of clusters beforehand when generating clusters. However, this approach
    is often time-consuming and leads to inconsistent results. In the hierarchical
    clustering approach, however, the optimum number of clusters can be determined
    without manual intervention. There are two sub-categories of hierarchical clustering
    that we will be discussing below.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 使用k-means聚类方法的一个缺点是，在生成聚类时需要事先确定聚类的总数。然而，这种方法往往耗时，并且导致结果不一致。然而，在层次聚类方法中，最佳的聚类数目可以在没有人工干预的情况下确定。我们将讨论层次聚类的两个子类别。
- en: '**Agglomerative Clustering:** This method focuses on building clusters one
    at a time in a bottom-up fashion. Initially, each of the data points is assumed
    to be a separate cluster. After this step, the clusters that are closest to each
    other are merged to form a merged cluster. This step is followed until we are
    able to merge into one single final cluster with all the data points. This method
    can help us to determine the hierarchy in which clusters were formed along with
    their similarity with each other. Dendrograms show this tree-like structure that
    shows the way in which all the clusters are formed together.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '**凝聚层次聚类：** 这种方法专注于以自下而上的方式逐个构建聚类。最初，每个数据点被假定为一个单独的聚类。之后，将彼此最接近的聚类合并成一个合并的聚类。此步骤会一直进行，直到我们能够将所有数据点合并为一个最终的单一聚类。该方法可以帮助我们确定聚类形成的层次结构以及它们之间的相似性。树状图显示了这种树状结构，展示了所有聚类的形成方式。'
- en: '**Divisive Clustering:** This method follows a top-down approach where all
    the data points are assigned to a single cluster. We split the data points into
    clusters based on their distance. These steps are followed until we have **n**
    clusters when **n** indicates the total number of data points chosen for clustering.
    Having a large value of n leads to having a higher number of clusters and higher
    computation costs. It is easier to visualize the results with the help of a dendrogram
    that also shows similarities between clusters that are used to divide them.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '**分裂聚类：** 这种方法遵循自上而下的方法，将所有数据点分配到一个单一的聚类中。我们根据数据点之间的距离将其分割成聚类。这些步骤一直进行，直到我们得到**n**个聚类，其中**n**表示选择用于聚类的数据点总数。n的值较大将导致更多的聚类和更高的计算成本。通过树状图可以更容易地可视化结果，该图也显示了用于划分聚类的相似性。'
- en: Pros
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 优点
- en: There is no requirement to specify the number of clusters when using the hierarchical
    clustering approach.
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用层次聚类方法时不需要指定聚类的数量。
- en: This method is more intuitive as there is a hierarchy in the structure, and
    it is easier to interpret.
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这种方法更加直观，因为结构中有一个层次，解释起来更容易。
- en: Running the hierarchical clustering approach multiple times produces deterministic
    results without a change in the clustering pattern.
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 多次运行层次聚类方法会产生确定性的结果，而不会改变聚类模式。
- en: Cons
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 缺点
- en: The time complexity to run this algorithm is quite high. If we have a large
    dataset containing millions of records, it can be better to look for alternative
    algorithms.
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行此算法的时间复杂度相当高。如果我们有一个包含数百万条记录的大型数据集，寻找替代算法可能更好。
- en: This algorithm is also sensitive to outliers in the data which can affect the
    way in which clusters are formed or generated.
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这种算法对数据中的异常值也很敏感，这可能会影响聚类的形成或生成方式。
- en: The choice of the distance metric used to split or merge clusters can have a
    significant impact on the final cluster outputs.
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 用于拆分或合并聚类的距离度量选择可能会对最终的聚类结果产生重大影响。
- en: Density-Based Spatial Clustering of Applications with Noise (DBSCAN)
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基于密度的空间聚类算法（DBSCAN）
- en: This is an unsupervised machine learning approach that consists of the relative
    density of points with respect to each other in order to determine the clusters.
    Below is the list of steps that are followed in the DBSCAN approach.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一种无监督的机器学习方法，通过相对密度来确定聚类。以下是DBSCAN方法中遵循的步骤列表。
- en: '**Choose Initial Parameters:** Choose an arbitrary starting data point that
    has not been visited. Set your parameters: `eps` (epsilon) is the maximum distance
    between two samples for them to be considered as in the same neighborhood, and
    `minPts` is the minimum number of samples in a neighborhood for a data point to
    qualify as a core point.'
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**选择初始参数：** 选择一个尚未访问的任意起始数据点。设置你的参数：`eps`（epsilon）是两个样本之间被认为在同一邻域中的最大距离，`minPts`是邻域内样本的最小数量，以使数据点符合核心点的条件。'
- en: '**Determine Core Points:** For your starting point, calculate the number of
    data points within an `eps` radius to determine its density. If there are at least
    `minPts` within this radius, mark the starting data point as a core point; otherwise,
    mark it as noise (this could be updated later).'
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**确定核心点：** 对于你的起始点，计算在`eps`半径内的数据点数量以确定其密度。如果在此半径内至少有`minPts`，则将起始数据点标记为核心点；否则，标记为噪声（这可能会在以后更新）。'
- en: '**Expand Clusters:** For each core point, if it’s not already assigned to a
    cluster, create a new cluster. Find all points within the distance `eps` of the
    core point (including the core point itself) and assign them to the same cluster.
    If they also have `minPts` within distance `eps`, they are also core points, so
    repeat the process for those points.'
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**扩展簇：** 对于每个核心点，如果它尚未被分配到簇中，则创建一个新簇。找到距离核心点`eps`范围内的所有点（包括核心点本身）并将它们分配到同一个簇。如果这些点在距离`eps`范围内也有`minPts`，它们也是核心点，因此对这些点重复该过程。'
- en: '**Assign Border Points:** If a data point is within the `eps` distance of multiple
    clusters, assign it to the cluster of the first core point encountered.'
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**分配边界点：** 如果一个数据点在多个簇的`eps`距离内，则将其分配给第一个遇到的核心点所在的簇。'
- en: '**Iteration:** Continue the process until all points have been visited, assigned
    to a cluster, or marked as noise. This may require going back to previously marked
    noise points to check if they are in the `eps` radius of a newly found core point.'
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**迭代：** 继续这个过程，直到所有点都被访问、分配到簇中或标记为噪声。这可能需要回到之前标记为噪声的点，检查它们是否在新找到的核心点的`eps`半径内。'
- en: '**End:** The algorithm stops when all points have been marked as either core
    points, border points, or noise, and assigned to clusters appropriately.'
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**结束：** 当所有点都被标记为核心点、边界点或噪声，并且已被适当地分配到簇中时，算法停止。'
- en: Pros
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 优点
- en: There is no need to specify the number of clusters as we did in the k-means
    clustering approach.
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 不需要像k-means聚类方法那样指定簇的数量。
- en: It is more robust to outliers in the data as it works on the concept of core
    and noise points.
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它对数据中的异常值更加鲁棒，因为它基于核心点和噪声点的概念。
- en: It is capable of handling clusters that are arbitrarily shaped as compared to
    the above-discussed algorithms.
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它能够处理比上述讨论的算法更加任意形状的簇。
- en: Cons
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 缺点
- en: It requires careful tuning of `minPoints`and `epsilon`parameter depending on
    the dimensionality of the dataset which can be cumbersome.
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据数据集的维度，需要仔细调整`minPoints`和`epsilon`参数，这可能比较麻烦。
- en: It cannot be used to make predictions on new data samples but instead can only
    cluster the data points that were used during training.
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它不能用于对新数据样本进行预测，而只能对训练期间使用的数据点进行聚类。
- en: Running the DBSCAN model many times might not always lead to similar results
    as it is non-deterministic.
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行DBSCAN模型多次可能不会总是产生相似的结果，因为它是非确定性的。
- en: Gaussian Mixture Models (GMMs)
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 高斯混合模型（GMMs）
- en: This works under the principle of expectation maximization where the data points
    are put into clusters based on their probability scores. In addition, this method
    assumes that the data points are normally distributed. To do this, there is an
    initialization step for the mean, variance, and mixing weights. After this step,
    we follow the expectation maximization approach to determine the cluster for a
    data point. Below is a detailed explanation of the steps taken when using the
    Gaussian mixture models.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 该方法基于期望最大化原理，将数据点根据其概率得分放入簇中。此外，这种方法假设数据点服从正态分布。为此，首先需要初始化均值、方差和混合权重。在此步骤之后，我们按照期望最大化方法确定数据点的簇。以下是使用高斯混合模型时采取的详细步骤解释。
- en: The first step is to initialize the set of mean, variance, and mixing weights.
    This is done randomly or could be done with the help of the k-means clustering
    approach.
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第一步是初始化均值、方差和混合权重的集合。这可以随机完成，也可以借助k-means聚类方法完成。
- en: After this step, we determine the posterior probability of each of the data
    points belonging to any of these k-clusters. This is also known as the expectation
    phase.
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这一步之后，我们确定每个数据点属于这些k个簇中的任何一个的后验概率。这也称为期望阶段。
- en: Once the data points belonging to each of the clusters are determined, their
    combined mean, covariance, and mixing weights are recalculated to maximize the
    expectation.
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦确定了每个簇中的数据点，其组合均值、协方差和混合权重会被重新计算，以最大化期望。
- en: Steps 2 and 3 are repeated until there is no further change in the data points
    belonging to other clusters as compared to the ones determined with the expectation
    maximization approach.
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 步骤2和步骤3会重复进行，直到与期望最大化方法确定的簇相比，数据点在其他簇中的变化不再发生。
- en: Finally, we are left with data points that are clustered together under the
    assumption that each of the data points is normally distributed.
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最终，我们得到的数据点被聚类在一起，假设每个数据点都符合正态分布。
- en: Pros
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 优点
- en: One of the advantages of using this approach is that it is capable of handling
    shapes that are either elliptical or spherical as compared to the k-means clustering
    approach.
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用这种方法的一个优点是，它能够处理比k-means聚类方法更为复杂的椭圆形或球形簇。
- en: It also leads to soft-clustering which can be handy because we get to know the
    confidence score of the model when it assigns data points to each of the clusters.
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这也导致了软聚类，这非常方便，因为我们可以知道模型在将数据点分配到每个簇时的置信度分数。
- en: They have higher flexibility as compared to other models such as k-means as
    they could account for clusters of different shapes and sizes.
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 与其他模型如k-means相比，它们具有更高的灵活性，因为它们可以处理不同形状和大小的簇。
- en: Cons
  id: totrans-70
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 缺点
- en: The algorithms are highly sensitive to the mean and the covariance that is chosen
    initially. Hence, having a poor initialization can have an impact on the performance
    of the models.
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 算法对最初选择的均值和协方差高度敏感。因此，较差的初始化可能会影响模型的性能。
- en: Determining the optimal number of mixture models can be tough and it could be
    done with a trial-and-error method.
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 确定混合模型的最佳数量可能很困难，通常通过试错法完成。
- en: If we have a large dataset, there is a higher computational complexity and the
    models can be slow to converge due to this reason.
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果我们有一个大型数据集，由于这个原因，计算复杂度更高，模型可能会慢速收敛。
- en: Autoencoders
  id: totrans-74
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 自编码器
- en: These models are known to encode the input representation into a lower dimensional
    representation. They do not require output labels but instead, only require the
    input data in the form of features. Once the lower dimensional data is produced,
    it is also used to get the decoding so that the original data is obtained. In
    this way, we are able to reconstruct the original data using the encoded version.
    Note that there can be discrepancies between the decoded output and the original
    input. This error is also known as reconstruction loss which is optimized during
    training. Below is a detailed description of the working of autoencoders.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 这些模型能够将输入表示编码为较低维度的表示。它们不需要输出标签，而是只需要特征形式的输入数据。一旦生成了低维数据，也会用于解码，以获得原始数据。通过这种方式，我们能够使用编码版本重建原始数据。注意，解码输出与原始输入之间可能存在差异。这种错误也称为重建损失，在训练过程中进行优化。下面是自编码器工作原理的详细描述。
- en: By using input features from the data, encoding is performed in order to get
    a low-dimensional representation. Usually, this is selected so that it is better
    able to give a good representation of the data.
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过使用数据中的输入特征，进行编码以获得低维表示。通常，这种选择是为了更好地表示数据。
- en: After this step, the encoded signal is fed to a decoder which is able to reconstruct
    the original input. During this process, there is a possibility that the signal
    is not able to be reconstructed perfectly.
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这一步之后，编码后的信号被送入解码器，解码器能够重建原始输入。在此过程中，信号可能无法被完美重建。
- en: As a result, we use optimization with the reconstruction loss. With this optimization,
    the weights are adjusted so that the output reconstructed data is closely matching
    to that of the input data.
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 因此，我们使用具有重建损失的优化。通过这种优化，调整权重使得重建的数据输出与输入数据紧密匹配。
- en: The loss is reduced with the updates given to each of the weights. As a result,
    we are left with weights that have the required information in order to translate
    a given set of inputs into a lower dimensional representation for easier computation.
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更新每个权重时损失会减少。因此，我们得到的权重包含了将给定输入集转换为低维表示所需的信息，从而便于计算。
- en: Pros
  id: totrans-80
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 优点
- en: They can be better used to determine the anomalies present in the data as compared
    to other methods.
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 与其他方法相比，它们更适合用于确定数据中存在的异常。
- en: They can be used to reduce the dimensionality of the input data which can lead
    to faster computation.
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它们可以用来减少输入数据的维度，从而提高计算速度。
- en: They are better able to reduce the incoming noise from the input data better
    than the other models.
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它们比其他模型更能有效地减少输入数据中的噪声。
- en: Cons
  id: totrans-84
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 缺点
- en: When the input data is huge, it can be computationally expensive to run epochs
    for constructing the encoded representations.
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当输入数据非常庞大时，运行多次训练来构建编码表示可能会非常耗费计算资源。
- en: It can be difficult to interpret the meaning of the encoded representations
    (hidden representation) as they are non-intuitive.
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 解释编码表示（隐藏表示）的含义可能会很困难，因为它们不直观。
- en: Principal Component Analysis (PCA)
  id: totrans-87
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 主成分分析（PCA）
- en: It is a statistical technique that relies on the covariance between the features
    to determine the lower dimensional space. In addition, this algorithm does not
    require output training labels unlike singular value decomposition (SVD) which
    still requires the output labels to determine the best features. In this way,
    we get the final vector that can represent the variance of most of the dataset
    with the help of eigen values and eigen vectors. Below are the list of steps that
    are used in order to perform principal component analysis (PCA).
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一种统计技术，它依赖于特征之间的协方差来确定低维空间。此外，该算法不像奇异值分解（SVD）那样需要输出训练标签来确定最佳特征。在这种方式下，我们得到的最终向量可以通过特征值和特征向量表示数据集的大部分方差。以下是执行主成分分析（PCA）的步骤列表。
- en: The initial step before applying principal component analysis (PCA) is to standardize
    the data. PCA is highly sensitive to the scale of the data. By the nature of machine
    learning, we generally give inputs of different scales. Hence, it is important
    to standardize the data before feeding this information to the model.
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在应用主成分分析（PCA）之前的初始步骤是对数据进行标准化。PCA 对数据的尺度非常敏感。由于机器学习的特性，我们通常输入的数据尺度各异。因此，在将这些信息提供给模型之前，对数据进行标准化是很重要的。
- en: We find the covariance of all the features with respect to each other. In this
    way, we can get a good understanding of how much variation is present in the data
    based on the set of other features.
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们找到所有特征相互之间的协方差。通过这种方式，我们可以很好地理解数据中基于其他特征集的变化程度。
- en: After performing this step, we compute the eigen values and eigen vectors using
    a set of formulas and equations. In order to do this, we should have a covariance
    matrix that highlights important information.
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在执行这一步后，我们使用一组公式和方程来计算特征值和特征向量。为了做到这一点，我们应有一个突出重要信息的协方差矩阵。
- en: After we determine the eigen values and eigen vectors, we should arrange the
    eigen values in descending order to determine the variance explained by each of
    the features with respect to other features. Depending on the number of components
    we want our PCA to decompose, we will set the value for the number of components.
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在确定特征值和特征向量后，我们应按降序排列特征值，以确定每个特征相对于其他特征解释的方差。根据我们希望PCA分解的组件数量，我们将设置组件数量的值。
- en: We will perform matrix multiplication of the original data with the eigen vectors
    to get the transformed features. This represents the reduced set which can be
    computationally less expensive to train and make predictions.
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将对原始数据与特征向量进行矩阵乘法，以获得转换后的特征。这表示减少后的数据集，这样的训练和预测可能计算上更为高效。
- en: Pros
  id: totrans-94
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 优点
- en: Using this approach leads to dimensionality reduction which is useful for scenarios
    where we have a large number of features in our data.
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用这种方法可以实现维度减少，这对于数据中特征数量非常多的场景是有用的。
- en: It can also help reduce dimensions in such a way as to be able to visualize
    these features in the coding cells.
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它还可以帮助以能够在编码单元中可视化这些特征的方式减少维度。
- en: This can reduce overfitting when we have a higher number of features as compared
    to the size of the dataset.
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当特征数量大于数据集的规模时，这可以减少过拟合。
- en: Cons
  id: totrans-98
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 缺点
- en: It is tough to interpret the components that are present due to principal component
    analysis. The original features are easier to interpret as they represent real-world
    data while the principal components cannot be interpreted as features but instead
    as transformations.
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 由于主成分分析，解释存在的组件是困难的。原始特征更容易解释，因为它们代表了现实世界的数据，而主成分不能作为特征来解释，而是作为变换来理解。
- en: It is highly sensitive to scaling and having data of different scales can impact
    the performance of PCA to a large extent.
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于缩放非常敏感，数据的不同尺度会极大地影响PCA的性能。
- en: If there are outliers in the data, it cannot perform well compared to other
    unsupervised machine learning models.
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果数据中存在离群值，它的表现可能不如其他无监督机器学习模型。
- en: Conclusion
  id: totrans-102
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结论
- en: Exploring unsupervised machine learning models is a captivating endeavor. Each
    model, like Principal Component Analysis, carries unique strengths and limitations,
    shaping how we approach and solve problems. The key lies in understanding which
    tool to utilize given the specifics of your data and objectives.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 探索无监督机器学习模型是一项引人入胜的工作。每个模型，如主成分分析，都具有独特的优点和局限性，这影响了我们解决问题的方法。关键在于根据数据和目标的具体情况来选择合适的工具。
- en: By deepening your knowledge of these models, you’re empowering yourself to craft
    effective solutions and make a meaningful impact in the world of data science.
    Despite the challenges, the rewards are significant — the ability to transform
    data into valuable insights and solve complex problems.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 通过深入了解这些模型，你可以为自己制定有效的解决方案并在数据科学领域产生有意义的影响。尽管面临挑战，但回报是显著的——将数据转化为有价值的洞察力并解决复杂问题的能力。
- en: So, continue to learn, question, and innovate. Every step forward contributes
    to shaping the future of technology. Thank you for investing your time and curiosity
    into this important field. The realm of machine learning awaits your contribution.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，继续学习、质疑和创新。每一步进步都有助于塑造技术的未来。感谢你将时间和好奇心投入到这一重要领域。机器学习的领域在等待你的贡献。
- en: '*Below are the ways where you could contact me or take a look at my work.*'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '*以下是你可以联系我或查看我工作的方式。*'
- en: '***GitHub:***[*suhasmaddali (Suhas Maddali ) (github.com)*](https://github.com/suhasmaddali)'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '***GitHub：***[*suhasmaddali (Suhas Maddali ) (github.com)*](https://github.com/suhasmaddali)'
- en: '***YouTube:***[*https://www.youtube.com/channel/UCymdyoyJBC_i7QVfbrIs-4Q*](https://www.youtube.com/channel/UCymdyoyJBC_i7QVfbrIs-4Q)'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '***YouTube：***[*https://www.youtube.com/channel/UCymdyoyJBC_i7QVfbrIs-4Q*](https://www.youtube.com/channel/UCymdyoyJBC_i7QVfbrIs-4Q)'
- en: '***LinkedIn:***[*(1) Suhas Maddali, Northeastern University, Data Science |
    LinkedIn*](https://www.linkedin.com/in/suhas-maddali/)'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '***领英：***[*(1) Suhas Maddali, Northeastern University, Data Science | LinkedIn*](https://www.linkedin.com/in/suhas-maddali/)'
- en: '***Medium:*** [*Suhas Maddali — Medium*](https://suhas-maddali007.medium.com/)'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '***中等：*** [*Suhas Maddali — Medium*](https://suhas-maddali007.medium.com/)'
- en: '***Kaggle:***[*Suhas Maddali | Contributor | Kaggle*](https://www.kaggle.com/suhasmaddali007)'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '***Kaggle：***[*Suhas Maddali | Contributor | Kaggle*](https://www.kaggle.com/suhasmaddali007)'
