- en: First Steps in Machine Learning with Apache Spark
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/first-steps-in-machine-learning-with-apache-spark-672fe31799a3?source=collection_archive---------8-----------------------#2023-01-04](https://towardsdatascience.com/first-steps-in-machine-learning-with-apache-spark-672fe31799a3?source=collection_archive---------8-----------------------#2023-01-04)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Basic concepts and topics of Spark MLlib package
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://joaopedro214.medium.com/?source=post_page-----672fe31799a3--------------------------------)[![João
    Pedro](../Images/64a0e14527be213e5fde0a02439fbfa7.png)](https://joaopedro214.medium.com/?source=post_page-----672fe31799a3--------------------------------)[](https://towardsdatascience.com/?source=post_page-----672fe31799a3--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----672fe31799a3--------------------------------)
    [João Pedro](https://joaopedro214.medium.com/?source=post_page-----672fe31799a3--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fb111eee95c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffirst-steps-in-machine-learning-with-apache-spark-672fe31799a3&user=Jo%C3%A3o+Pedro&userId=b111eee95c&source=post_page-b111eee95c----672fe31799a3---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----672fe31799a3--------------------------------)
    ·11 min read·Jan 4, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F672fe31799a3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffirst-steps-in-machine-learning-with-apache-spark-672fe31799a3&user=Jo%C3%A3o+Pedro&userId=b111eee95c&source=-----672fe31799a3---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F672fe31799a3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffirst-steps-in-machine-learning-with-apache-spark-672fe31799a3&source=-----672fe31799a3---------------------bookmark_footer-----------)![](../Images/b6f9154e2eb16e7cf1f6b67fc5e34280.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Photo by [Element5 Digital](https://unsplash.com/es/@element5digital?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Apache Spark is one of the main tools for data processing and analysis in the
    BigData context. It’s a very complete (and complex) data processing framework,
    with functionalities that can be roughly divided into four groups: SparkSQL &
    DataFrames, the all-purpose data processing needs; Spark Structured Streaming,
    used to handle data-streams; Spark MLlib, for machine learning and data science
    and GraphX, the graph processing API.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/114fdcdf4bf192579d828ab3f5628c35.png)'
  prefs: []
  type: TYPE_IMG
- en: Spark libraries section on the official documentation. Print by Author.
  prefs: []
  type: TYPE_NORMAL
- en: 'I’ve already featured the first two in other posts: [creating an ETL process
    for a Data Warehouse](https://joaopedro214.medium.com/creating-a-simple-etl-pipeline-with-apache-spark-825cc17c8cf6)
    and [integrating Spark and Kafka for stream processing](/a-fast-look-at-spark-structured-streaming-kafka-f0ff64107325).
    Today is the time for the third one — Let’s play with Machine Learning using Spark
    MLlib.'
  prefs: []
  type: TYPE_NORMAL
- en: Machine Learning has a special place in my heart, because it was my entrance
    door to the data science field and, as probably many of yours, I started it with
    the classic [Scikit-Learn](https://scikit-learn.org/stable/) library.
  prefs: []
  type: TYPE_NORMAL
- en: I’ve could write an entire post on why the Scikit-learn library is such a marvelous
    piece of software. It’s beginner-friendly, easy to use, covers most of the machine
    learning cycle, has very well-written documentation, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: But why am I talking about this? If you are like me, used to coding with sklearn,
    keep in mind that the path in Apache Spark is not SO straightforward. It’s not
    hard but has a steeper learning curve.
  prefs: []
  type: TYPE_NORMAL
- en: Along this post, we’ll learn how to make the ‘full machine learning cycle’ of
    data preprocessing, feature engineering, model training, and validation with a
    Hands-On example.
  prefs: []
  type: TYPE_NORMAL
- en: Apache Spark in a Nutshell
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Apache Spark is a distributed memory-based data transformation engine. It is
    geared to operate in distributed environments to parallelize processing between
    machines, achieving high-performance transformations by using its lazy evaluation
    philosophy and query optimizations.
  prefs: []
  type: TYPE_NORMAL
- en: And that’s the main reason to learn such a tool — Performance.
  prefs: []
  type: TYPE_NORMAL
- en: Even with optimizations, the Sklearn package (and other python packages) struggle
    when the dataset gets too big. That’s one of the potential blind spots Spark covers.
    As it scales horizontally, it is easier to increase the computational power to
    train models on BigData.
  prefs: []
  type: TYPE_NORMAL
- en: The problem
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: I’ve chosen the [Avocado Price Dataset](https://www.kaggle.com/datasets/neuromusic/avocado-prices)
    to make this project. Our task in this dataset is to predict the mean avocado
    price given the avocado type, date, the amount of avocado bags available and other
    features. See the [dataset page on Kaggle](https://www.kaggle.com/datasets/neuromusic/avocado-prices)
    for more information.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up the environment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: All you need is docker and docker-compose installed. The code is available on
    [GitHub](https://github.com/jaumpedro214/spark-ml-first-steps/settings).
  prefs: []
  type: TYPE_NORMAL
- en: The architecture described (in the *docker-compose.yaml*) is depicted in the
    image below.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0bc69dd0959c25391ac31ea5829a090b.png)'
  prefs: []
  type: TYPE_IMG
- en: Project’s architecture & Docker Containers. Image by Author.
  prefs: []
  type: TYPE_NORMAL
- en: All the code was developed inside a [jupyter/pyspark-notebook](https://hub.docker.com/r/jupyter/pyspark-notebook)
    container with all the pyspark dependencies already configured.
  prefs: []
  type: TYPE_NORMAL
- en: 'To start the environment, just run:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The implementation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Our objective is to learn how to implement our usual ML pipeline using Spark,
    which covers: Loading data and splitting it into train/test, cleaning the data,
    preprocessing+feature engineering, model definition, hyperparameter-tuning and
    final scorings.'
  prefs: []
  type: TYPE_NORMAL
- en: The following sections will detail how to make each of these steps.
  prefs: []
  type: TYPE_NORMAL
- en: Connect to Spark
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The first thing to do is connect to the Spark cluster, which is quite straightforward.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: It may take a few seconds in the first run.
  prefs: []
  type: TYPE_NORMAL
- en: Loading the data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now it’s time to work with the data. This part still has nothing to do with
    Spark’s MLlib package, just the usual data loading using Spark SQL.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: As Spark is [lazily evaluated](https://stackoverflow.com/questions/38027877/spark-transformation-why-is-it-lazy-and-what-is-the-advantage),
    it’s interesting to cache the dataset in memory to speed up the execution of the
    next steps.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s have a look at the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/41e5e436d69f75fd4547288f7fcc8daa.png)'
  prefs: []
  type: TYPE_IMG
- en: Again, more details about the columns can be found on the original dataset’s
    Kaggle page.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s move on and split the DataFrame into train (75%) and test (25%) set using
    the *randomSplit()* method.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Preprocess data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Before continuing, let’s know the tools we’ll be using. The Spark MLlib package
    has two main types of objects: [Transformers and Estimators](https://spark.apache.org/docs/1.6.0/ml-guide.html).'
  prefs: []
  type: TYPE_NORMAL
- en: A **Transformer** is an object that’s able to transform Dataframes. They receive
    a raw DataFrame and return a processed one. Common transformers include PolynomialExpansion,
    SQLTransformer, and VectorAssembler (very important, discussed later).
  prefs: []
  type: TYPE_NORMAL
- en: '**Estimators**, on the other hand, are objects that need to be fitted/trained
    on the data to generate a Transformer. These include machine learning predictors
    (Linear Regression, Logistic Regression, Decision Trees, etc), dimensionality
    reduction algorithms (PCA, ChiSquare Selector), and also other column transformers
    (StandardScaler, MinMaxScaler, TF-IDF, etc).'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/66e637a5d0dae1f23fd8e1cd6aa7d0f8.png)'
  prefs: []
  type: TYPE_IMG
- en: Transformers and Estimators on Spark MLlib. Image by Author.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s start by using the **SQLTransformer** on our Data. This is a powerful
    transformer, that allows one to select and transform columns using SQL queries.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: The code above selects the *AveragePrice* and *type* columns, transforms the
    numerical columns using the Log function, and creates two new columns extracting
    the *year* (after 2000) and the *month*.
  prefs: []
  type: TYPE_NORMAL
- en: __THIS__ is the default name of the DataFrame currently being transformed.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'The result:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f64d500d4fa54ec5f666bde14553c2e8.png)'
  prefs: []
  type: TYPE_IMG
- en: Scaling is a prevalent practice in data preprocessing. Let’s scale the **month**
    column using the Min-Max scaling technique, putting all values in the [0, 1] interval.
    The MinMaxScaler is of type **estimator** so it needs to be fitted to the data
    before being used to transform it.
  prefs: []
  type: TYPE_NORMAL
- en: Most of the estimators (including all the prediction models) require the input
    columns to be in Vector form. Vector is a special column type used mostly in Spark
    MLlib. It is just what the name suggests, a fixed-size array of numbers.
  prefs: []
  type: TYPE_NORMAL
- en: To join columns into a single Vector column, we use the VectorAssembler Transformer.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/10d7f1182d8aae534012fae3ebcf64a5.png)'
  prefs: []
  type: TYPE_IMG
- en: Vector Assembler in action. Image by Author.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: The image below details the process.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/947a6e4950f72e96ca89f39f97b78dda.png)'
  prefs: []
  type: TYPE_IMG
- en: The process to apply the MinMaxScaler to a column. Image by Author.
  prefs: []
  type: TYPE_NORMAL
- en: 'The result:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4266dc8f0f8d22a0be0135035e63f918.png)'
  prefs: []
  type: TYPE_IMG
- en: With these concepts in mind, is just a matter of knowing the available transformers
    and using them in our pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: For example, the column **type** has two values, “*conventional”* and “*organic*”,
    that need to be mapped into numbers. The transformer responsible for this is the
    StringIndexer.
  prefs: []
  type: TYPE_NORMAL
- en: 'It assigns a numerical value to each category in a column. As the column “type”
    only has two categories, it will be transformed into a column with only two values:
    0 and 1, which is equivalent to applying the one-hot encoding technique.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/4023608a0496a7586241d873e67675f5.png)'
  prefs: []
  type: TYPE_IMG
- en: From now on I’ll summarize what was done.
  prefs: []
  type: TYPE_NORMAL
- en: The numerical features (all columns except the *type_index*) generated were
    assembled in a single Vector called “features_num”, this final vector passes by
    a StandardScaler.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/2f535578bd130098b42226615bc4641b.png)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/0e457d208950ce3e100e5b51d4c2c76b.png)'
  prefs: []
  type: TYPE_IMG
- en: The categorical column “type_index” is then added to the final vector.
  prefs: []
  type: TYPE_NORMAL
- en: The final step is to join all the transformers created into a **pipeline**.
    A pipeline is just an object used to encapsulate a set of transformers and estimators
    to apply them to data sequentially. This helps us to avoid dealing with each intermediate
    transformation step individually (as we’re doing so far).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The final result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Model training
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: That’s the moment we’re all waiting for.
  prefs: []
  type: TYPE_NORMAL
- en: After the long path of data preprocessing, all the features are already in their
    desired final vector form and we’re ready to train the model.
  prefs: []
  type: TYPE_NORMAL
- en: Unfortunately, this part will be very short compared with the previous one ¯\_(ツ)_/¯
  prefs: []
  type: TYPE_NORMAL
- en: 'As mentioned earlier, ML models are just estimators, so the process repeats:
    Instantiate, Fit and Transform.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s train a Linear Regression model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: It's necessary to specify the features column, the target/label column, and
    a name for the prediction column. Just like the other estimators we’ve met, an
    ML model will just add another column to the DataFrame.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/321df60ec602669af3dbf906e9cd169f.png)'
  prefs: []
  type: TYPE_IMG
- en: A Machine Learning model is just an estimator. Image by Author.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'See the result below:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Model evaluation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To measure the model’s performance, we need an **evaluator**. I think its name
    is self-explanatory, it will compute a performance metric between the real labels
    and the model predictions.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/31ce091d0a576d2f5998b40f939d0e27.png)'
  prefs: []
  type: TYPE_IMG
- en: Evaluator in action. Image by Author.
  prefs: []
  type: TYPE_NORMAL
- en: In the cell below, a RegressionEvaluator is instantiated to measure the RMSE
    (*Rooted Mean Squared Error*) between predictions and real values (on train data).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Hyperparameter Tuning with Cross Validation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Hyperparameter Tuning](https://en.wikipedia.org/wiki/Hyperparameter_optimization)
    is one of the last stages in a Machine Learning Pipeline, so our adventure is
    coming to an end.'
  prefs: []
  type: TYPE_NORMAL
- en: In this step, we test several variations of hyperparameters of our model/pipeline
    to pick the best one according to the chosen metric. A common way of doing this
    is using a [cross-validation technique](https://scikit-learn.org/stable/modules/cross_validation.html).
  prefs: []
  type: TYPE_NORMAL
- en: Here, we met the last building blocks of today’s post — The **ParamGridBuilder**
    and the **CrossValidator**.
  prefs: []
  type: TYPE_NORMAL
- en: 'Starting with the ParamGridBuilder: It is the object used to build a hyperparameter
    grid.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: In the code above, several values are specified for the linear regression’s
    *regParam* and *elasticNetParam.* It’s important to note that the original object
    is used to reference the parameters.
  prefs: []
  type: TYPE_NORMAL
- en: The **CrossValidator** then joins everything together (estimator, hyperparameter
    grid, and evaluator) …
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: … and performs a cross-validation for the given number of folds when the method
    *fit*() is called with training data.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: The results are accessed through the fitted Cross Validation object. The code
    below prints the best model’s name and score.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Let’s also see the linear regression’s best parameters.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Evaluate the best model on the test set
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: And all come to this moment. This is the step where we measure the performance
    of the best model on test data.
  prefs: []
  type: TYPE_NORMAL
- en: Fortunately, there is nothing new to learn here, it’s just a matter of applying
    the best model to the test data and passing the result to the evaluator.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: The performance was very similar to the one obtained in the cross-validation
    step.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As ML applications become popular and their requirements become more complex,
    knowledge of a wider variety of tools with different purposes becomes essential.
  prefs: []
  type: TYPE_NORMAL
- en: In this post, we learned a little about how Apache Spark can be used in the
    context of Machine Learning through the Spark MLlib module. With a hands-on project,
    we created a generic ML pipeline, covering the main concepts and basic topics
    of this module.
  prefs: []
  type: TYPE_NORMAL
- en: 'Learning a new tool mainly involves becoming familiar with its vocabulary,
    *i.e.,* understanding the fundamental parts that compose it and how they can be
    used to solve a problem. Therefore, we focused on understanding the basics of
    Spark MLlib: Estimators, Transformers, Evaluators, and Pipelines.'
  prefs: []
  type: TYPE_NORMAL
- en: I hope this brief post helped you understand how Spark can be used in Machine
    Learning applications.
  prefs: []
  type: TYPE_NORMAL
- en: As always, this post just scratches the surface of the topics explored, so I
    strongly encourage further reading, see the references below.
  prefs: []
  type: TYPE_NORMAL
- en: Thank you for reading ;)
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: All the code is available in this [GitHub repository](https://github.com/jaumpedro214/spark-ml-first-steps).
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Data used — [Avocado Prices](https://www.kaggle.com/datasets/neuromusic/avocado-prices)*,*
    [*ODbL v1.0: Open database*](https://opendatacommons.org/licenses/odbl/1-0/)*,
    Kaggle.*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[1] Chambers, B., & Zaharia, M. (2018). *Spark: The definitive guide: Big data
    processing made simple*. “ O’Reilly Media, Inc.”'
  prefs: []
  type: TYPE_NORMAL
- en: '[2] Spark by examples — [https://sparkbyexamples.com/](https://sparkbyexamples.com/)'
  prefs: []
  type: TYPE_NORMAL
- en: '[3] Géron, A. (2022). *Hands-on machine learning with Scikit-Learn, Keras,
    and TensorFlow*. “ O’Reilly Media, Inc.”.'
  prefs: []
  type: TYPE_NORMAL
- en: '[4] Overview: estimators, transformers and pipelines — spark.ml. [Spark Official
    documentation](https://spark.apache.org/docs/1.6.0/ml-guide.html).'
  prefs: []
  type: TYPE_NORMAL
