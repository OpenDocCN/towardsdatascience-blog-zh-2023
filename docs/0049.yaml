- en: First Steps in Machine Learning with Apache Spark
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/first-steps-in-machine-learning-with-apache-spark-672fe31799a3?source=collection_archive---------8-----------------------#2023-01-04](https://towardsdatascience.com/first-steps-in-machine-learning-with-apache-spark-672fe31799a3?source=collection_archive---------8-----------------------#2023-01-04)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Basic concepts and topics of Spark MLlib package
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://joaopedro214.medium.com/?source=post_page-----672fe31799a3--------------------------------)[![João
    Pedro](../Images/64a0e14527be213e5fde0a02439fbfa7.png)](https://joaopedro214.medium.com/?source=post_page-----672fe31799a3--------------------------------)[](https://towardsdatascience.com/?source=post_page-----672fe31799a3--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----672fe31799a3--------------------------------)
    [João Pedro](https://joaopedro214.medium.com/?source=post_page-----672fe31799a3--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fb111eee95c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffirst-steps-in-machine-learning-with-apache-spark-672fe31799a3&user=Jo%C3%A3o+Pedro&userId=b111eee95c&source=post_page-b111eee95c----672fe31799a3---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----672fe31799a3--------------------------------)
    ·11 min read·Jan 4, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F672fe31799a3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffirst-steps-in-machine-learning-with-apache-spark-672fe31799a3&user=Jo%C3%A3o+Pedro&userId=b111eee95c&source=-----672fe31799a3---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F672fe31799a3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffirst-steps-in-machine-learning-with-apache-spark-672fe31799a3&source=-----672fe31799a3---------------------bookmark_footer-----------)![](../Images/b6f9154e2eb16e7cf1f6b67fc5e34280.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: Photo by [Element5 Digital](https://unsplash.com/es/@element5digital?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Apache Spark is one of the main tools for data processing and analysis in the
    BigData context. It’s a very complete (and complex) data processing framework,
    with functionalities that can be roughly divided into four groups: SparkSQL &
    DataFrames, the all-purpose data processing needs; Spark Structured Streaming,
    used to handle data-streams; Spark MLlib, for machine learning and data science
    and GraphX, the graph processing API.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/114fdcdf4bf192579d828ab3f5628c35.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
- en: Spark libraries section on the official documentation. Print by Author.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: 'I’ve already featured the first two in other posts: [creating an ETL process
    for a Data Warehouse](https://joaopedro214.medium.com/creating-a-simple-etl-pipeline-with-apache-spark-825cc17c8cf6)
    and [integrating Spark and Kafka for stream processing](/a-fast-look-at-spark-structured-streaming-kafka-f0ff64107325).
    Today is the time for the third one — Let’s play with Machine Learning using Spark
    MLlib.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: Machine Learning has a special place in my heart, because it was my entrance
    door to the data science field and, as probably many of yours, I started it with
    the classic [Scikit-Learn](https://scikit-learn.org/stable/) library.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: I’ve could write an entire post on why the Scikit-learn library is such a marvelous
    piece of software. It’s beginner-friendly, easy to use, covers most of the machine
    learning cycle, has very well-written documentation, and so on.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: But why am I talking about this? If you are like me, used to coding with sklearn,
    keep in mind that the path in Apache Spark is not SO straightforward. It’s not
    hard but has a steeper learning curve.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: Along this post, we’ll learn how to make the ‘full machine learning cycle’ of
    data preprocessing, feature engineering, model training, and validation with a
    Hands-On example.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: Apache Spark in a Nutshell
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Apache Spark is a distributed memory-based data transformation engine. It is
    geared to operate in distributed environments to parallelize processing between
    machines, achieving high-performance transformations by using its lazy evaluation
    philosophy and query optimizations.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: And that’s the main reason to learn such a tool — Performance.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: Even with optimizations, the Sklearn package (and other python packages) struggle
    when the dataset gets too big. That’s one of the potential blind spots Spark covers.
    As it scales horizontally, it is easier to increase the computational power to
    train models on BigData.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: The problem
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: I’ve chosen the [Avocado Price Dataset](https://www.kaggle.com/datasets/neuromusic/avocado-prices)
    to make this project. Our task in this dataset is to predict the mean avocado
    price given the avocado type, date, the amount of avocado bags available and other
    features. See the [dataset page on Kaggle](https://www.kaggle.com/datasets/neuromusic/avocado-prices)
    for more information.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: Setting up the environment
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: All you need is docker and docker-compose installed. The code is available on
    [GitHub](https://github.com/jaumpedro214/spark-ml-first-steps/settings).
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: The architecture described (in the *docker-compose.yaml*) is depicted in the
    image below.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0bc69dd0959c25391ac31ea5829a090b.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
- en: Project’s architecture & Docker Containers. Image by Author.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: All the code was developed inside a [jupyter/pyspark-notebook](https://hub.docker.com/r/jupyter/pyspark-notebook)
    container with all the pyspark dependencies already configured.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: 'To start the environment, just run:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The implementation
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Our objective is to learn how to implement our usual ML pipeline using Spark,
    which covers: Loading data and splitting it into train/test, cleaning the data,
    preprocessing+feature engineering, model definition, hyperparameter-tuning and
    final scorings.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: The following sections will detail how to make each of these steps.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: Connect to Spark
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The first thing to do is connect to the Spark cluster, which is quite straightforward.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: It may take a few seconds in the first run.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: Loading the data
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now it’s time to work with the data. This part still has nothing to do with
    Spark’s MLlib package, just the usual data loading using Spark SQL.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: As Spark is [lazily evaluated](https://stackoverflow.com/questions/38027877/spark-transformation-why-is-it-lazy-and-what-is-the-advantage),
    it’s interesting to cache the dataset in memory to speed up the execution of the
    next steps.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s have a look at the data:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/41e5e436d69f75fd4547288f7fcc8daa.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
- en: Again, more details about the columns can be found on the original dataset’s
    Kaggle page.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: Let’s move on and split the DataFrame into train (75%) and test (25%) set using
    the *randomSplit()* method.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Preprocess data
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Before continuing, let’s know the tools we’ll be using. The Spark MLlib package
    has two main types of objects: [Transformers and Estimators](https://spark.apache.org/docs/1.6.0/ml-guide.html).'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: A **Transformer** is an object that’s able to transform Dataframes. They receive
    a raw DataFrame and return a processed one. Common transformers include PolynomialExpansion,
    SQLTransformer, and VectorAssembler (very important, discussed later).
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: '**Estimators**, on the other hand, are objects that need to be fitted/trained
    on the data to generate a Transformer. These include machine learning predictors
    (Linear Regression, Logistic Regression, Decision Trees, etc), dimensionality
    reduction algorithms (PCA, ChiSquare Selector), and also other column transformers
    (StandardScaler, MinMaxScaler, TF-IDF, etc).'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/66e637a5d0dae1f23fd8e1cd6aa7d0f8.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
- en: Transformers and Estimators on Spark MLlib. Image by Author.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: Let’s start by using the **SQLTransformer** on our Data. This is a powerful
    transformer, that allows one to select and transform columns using SQL queries.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The code above selects the *AveragePrice* and *type* columns, transforms the
    numerical columns using the Log function, and creates two new columns extracting
    the *year* (after 2000) and the *month*.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: __THIS__ is the default name of the DataFrame currently being transformed.
  id: totrans-57
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'The result:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f64d500d4fa54ec5f666bde14553c2e8.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
- en: Scaling is a prevalent practice in data preprocessing. Let’s scale the **month**
    column using the Min-Max scaling technique, putting all values in the [0, 1] interval.
    The MinMaxScaler is of type **estimator** so it needs to be fitted to the data
    before being used to transform it.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: Most of the estimators (including all the prediction models) require the input
    columns to be in Vector form. Vector is a special column type used mostly in Spark
    MLlib. It is just what the name suggests, a fixed-size array of numbers.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: To join columns into a single Vector column, we use the VectorAssembler Transformer.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/10d7f1182d8aae534012fae3ebcf64a5.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
- en: Vector Assembler in action. Image by Author.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The image below details the process.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/947a6e4950f72e96ca89f39f97b78dda.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
- en: The process to apply the MinMaxScaler to a column. Image by Author.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: 'The result:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4266dc8f0f8d22a0be0135035e63f918.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
- en: With these concepts in mind, is just a matter of knowing the available transformers
    and using them in our pipeline.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: For example, the column **type** has two values, “*conventional”* and “*organic*”,
    that need to be mapped into numbers. The transformer responsible for this is the
    StringIndexer.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: 'It assigns a numerical value to each category in a column. As the column “type”
    only has two categories, it will be transformed into a column with only two values:
    0 and 1, which is equivalent to applying the one-hot encoding technique.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '![](../Images/4023608a0496a7586241d873e67675f5.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
- en: From now on I’ll summarize what was done.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: The numerical features (all columns except the *type_index*) generated were
    assembled in a single Vector called “features_num”, this final vector passes by
    a StandardScaler.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '![](../Images/2f535578bd130098b42226615bc4641b.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
- en: '[PRE8]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '![](../Images/0e457d208950ce3e100e5b51d4c2c76b.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
- en: The categorical column “type_index” is then added to the final vector.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: The final step is to join all the transformers created into a **pipeline**.
    A pipeline is just an object used to encapsulate a set of transformers and estimators
    to apply them to data sequentially. This helps us to avoid dealing with each intermediate
    transformation step individually (as we’re doing so far).
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The final result:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Model training
  id: totrans-87
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: That’s the moment we’re all waiting for.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: After the long path of data preprocessing, all the features are already in their
    desired final vector form and we’re ready to train the model.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: Unfortunately, this part will be very short compared with the previous one ¯\_(ツ)_/¯
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: 'As mentioned earlier, ML models are just estimators, so the process repeats:
    Instantiate, Fit and Transform.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s train a Linear Regression model:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: It's necessary to specify the features column, the target/label column, and
    a name for the prediction column. Just like the other estimators we’ve met, an
    ML model will just add another column to the DataFrame.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/321df60ec602669af3dbf906e9cd169f.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
- en: A Machine Learning model is just an estimator. Image by Author.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'See the result below:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Model evaluation
  id: totrans-100
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To measure the model’s performance, we need an **evaluator**. I think its name
    is self-explanatory, it will compute a performance metric between the real labels
    and the model predictions.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/31ce091d0a576d2f5998b40f939d0e27.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
- en: Evaluator in action. Image by Author.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: In the cell below, a RegressionEvaluator is instantiated to measure the RMSE
    (*Rooted Mean Squared Error*) between predictions and real values (on train data).
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Hyperparameter Tuning with Cross Validation
  id: totrans-106
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Hyperparameter Tuning](https://en.wikipedia.org/wiki/Hyperparameter_optimization)
    is one of the last stages in a Machine Learning Pipeline, so our adventure is
    coming to an end.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: In this step, we test several variations of hyperparameters of our model/pipeline
    to pick the best one according to the chosen metric. A common way of doing this
    is using a [cross-validation technique](https://scikit-learn.org/stable/modules/cross_validation.html).
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: Here, we met the last building blocks of today’s post — The **ParamGridBuilder**
    and the **CrossValidator**.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: 'Starting with the ParamGridBuilder: It is the object used to build a hyperparameter
    grid.'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: In the code above, several values are specified for the linear regression’s
    *regParam* and *elasticNetParam.* It’s important to note that the original object
    is used to reference the parameters.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: The **CrossValidator** then joins everything together (estimator, hyperparameter
    grid, and evaluator) …
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: … and performs a cross-validation for the given number of folds when the method
    *fit*() is called with training data.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: The results are accessed through the fitted Cross Validation object. The code
    below prints the best model’s name and score.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Let’s also see the linear regression’s best parameters.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Output:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Evaluate the best model on the test set
  id: totrans-123
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: And all come to this moment. This is the step where we measure the performance
    of the best model on test data.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: Fortunately, there is nothing new to learn here, it’s just a matter of applying
    the best model to the test data and passing the result to the evaluator.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: The performance was very similar to the one obtained in the cross-validation
    step.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  id: totrans-128
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As ML applications become popular and their requirements become more complex,
    knowledge of a wider variety of tools with different purposes becomes essential.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: In this post, we learned a little about how Apache Spark can be used in the
    context of Machine Learning through the Spark MLlib module. With a hands-on project,
    we created a generic ML pipeline, covering the main concepts and basic topics
    of this module.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: 'Learning a new tool mainly involves becoming familiar with its vocabulary,
    *i.e.,* understanding the fundamental parts that compose it and how they can be
    used to solve a problem. Therefore, we focused on understanding the basics of
    Spark MLlib: Estimators, Transformers, Evaluators, and Pipelines.'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: I hope this brief post helped you understand how Spark can be used in Machine
    Learning applications.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: As always, this post just scratches the surface of the topics explored, so I
    strongly encourage further reading, see the references below.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: Thank you for reading ;)
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: References
  id: totrans-135
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: All the code is available in this [GitHub repository](https://github.com/jaumpedro214/spark-ml-first-steps).
  id: totrans-136
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Data used — [Avocado Prices](https://www.kaggle.com/datasets/neuromusic/avocado-prices)*,*
    [*ODbL v1.0: Open database*](https://opendatacommons.org/licenses/odbl/1-0/)*,
    Kaggle.*'
  id: totrans-137
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 使用的数据 — [鳄梨价格](https://www.kaggle.com/datasets/neuromusic/avocado-prices)*,*
    [*ODbL v1.0：开放数据库*](https://opendatacommons.org/licenses/odbl/1-0/)*, Kaggle.*
- en: '[1] Chambers, B., & Zaharia, M. (2018). *Spark: The definitive guide: Big data
    processing made simple*. “ O’Reilly Media, Inc.”'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] Chambers, B., & Zaharia, M. (2018). *Spark: 终极指南：简化的大数据处理*。“ O’Reilly Media,
    Inc.”'
- en: '[2] Spark by examples — [https://sparkbyexamples.com/](https://sparkbyexamples.com/)'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] Spark 示例 — [https://sparkbyexamples.com/](https://sparkbyexamples.com/)'
- en: '[3] Géron, A. (2022). *Hands-on machine learning with Scikit-Learn, Keras,
    and TensorFlow*. “ O’Reilly Media, Inc.”.'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] Géron, A. (2022). *动手实践机器学习：使用 Scikit-Learn、Keras 和 TensorFlow*。“ O’Reilly
    Media, Inc.”.'
- en: '[4] Overview: estimators, transformers and pipelines — spark.ml. [Spark Official
    documentation](https://spark.apache.org/docs/1.6.0/ml-guide.html).'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] 概述：估算器、转换器和管道 — spark.ml。[Spark 官方文档](https://spark.apache.org/docs/1.6.0/ml-guide.html).'
