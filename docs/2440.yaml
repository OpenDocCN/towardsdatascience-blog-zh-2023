- en: To Use or Not to Use Machine Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/to-use-or-not-to-use-machine-learning-d28185382c14?source=collection_archive---------6-----------------------#2023-07-28](https://towardsdatascience.com/to-use-or-not-to-use-machine-learning-d28185382c14?source=collection_archive---------6-----------------------#2023-07-28)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: How to decide if using ML is a good idea, and how that is changing with GenAI
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://annaviaba.medium.com/?source=post_page-----d28185382c14--------------------------------)[![Anna
    Via](../Images/7e8fe5c1a485a789edad3a6d118bcf45.png)](https://annaviaba.medium.com/?source=post_page-----d28185382c14--------------------------------)[](https://towardsdatascience.com/?source=post_page-----d28185382c14--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----d28185382c14--------------------------------)
    [Anna Via](https://annaviaba.medium.com/?source=post_page-----d28185382c14--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fc1a8933ed8b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fto-use-or-not-to-use-machine-learning-d28185382c14&user=Anna+Via&userId=c1a8933ed8b&source=post_page-c1a8933ed8b----d28185382c14---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----d28185382c14--------------------------------)
    ·8 min read·Jul 28, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fd28185382c14&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fto-use-or-not-to-use-machine-learning-d28185382c14&user=Anna+Via&userId=c1a8933ed8b&source=-----d28185382c14---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fd28185382c14&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fto-use-or-not-to-use-machine-learning-d28185382c14&source=-----d28185382c14---------------------bookmark_footer-----------)![](../Images/8d5c08adb67df49de8a44d9e52766051.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Photo by [Ivan Aleksic](https://unsplash.com/es/@ivalex) on [Unsplash](https://unsplash.com/es/fotos/2RRq1BHPq4E?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
  prefs: []
  type: TYPE_NORMAL
- en: Machine Learning is great at solving certain complex problems, usually involving
    difficult relationships between features and outcomes that cannot be easily hard
    coded as heuristics or if-else statements. However, there are some limitations
    or things to have in mind when deciding if ML is a good solution for a given problem
    at hand. In this post we’ll deep dive into the topic **“to use or not to use ML,”**
    first understanding this for “traditional” ML models, and afterwards discussing
    how this picture is changing with the progress of Generative AI.
  prefs: []
  type: TYPE_NORMAL
- en: 'To clarify some of the points, I’ll use as an example the following initiative:
    *“As a company, I want to know if my clients are satisfied and the main reasons
    for dissatisfaction”*. A “traditional” ML based approach to solve this could be:'
  prefs: []
  type: TYPE_NORMAL
- en: Obtain comments clients write about you (app or play store, twitter or other
    social networks, your website…)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use a sentiment analysis model to classify the comments into positive / neutral
    / negative.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use topic modeling on the predicted “negative sentiment” comments to understand
    what they are about.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/ffc441f8fc3babbbb67a76fdab1f0e13.png)'
  prefs: []
  type: TYPE_IMG
- en: Classification of comments into positive, neutral and negative sentiment (image
    by the author)
  prefs: []
  type: TYPE_NORMAL
- en: Does my data have enough quality and volume?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In supervised ML models, training data is necessary for the model to learn whatever
    it needs to predict (in this example, sentiment from a comment). If data has low
    quality (a lot of typos, missing data, errors…), it will be really hard for the
    model to perform well.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is typically known as the “garbage in, garbage out” problem: if your data
    is garbage, your model and predictions will be garbage too.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Similarly, you need to have enough volume of data for the model to learn the
    different casuistry that impact whatever needs to be predicted. In this example,
    if you only have a case of negative comment label with concepts like “useless”,
    “disappointed” or similar, the model won’t be able to learn that these words usually
    appear when the label is “negative.”
  prefs: []
  type: TYPE_NORMAL
- en: Enough volume of training data should also help ensure you have a good representation
    of the data you will need to perform predictions on. For example, if your training
    data has no representation of a particular geographical area or a particular segment
    of the population, it is more likely the model will fail to perform well for those
    comments at prediction time.
  prefs: []
  type: TYPE_NORMAL
- en: For some use cases, having enough historic data is also relevant, to ensure
    we are able to compute relevant lagging features or labels (e.g. “customer pays
    the credit during the next year or not”).
  prefs: []
  type: TYPE_NORMAL
- en: Are labels clear to define and easy to get?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Again, for traditional supervised ML models, you’ll need a labeled dataset:
    examples for which you know the final outcome of what you want to predict, to
    be able to train your model.'
  prefs: []
  type: TYPE_NORMAL
- en: The definition of the label is key. In this example, our label would be the
    sentiment associated with the comment. We could think we only can have “positive”
    or “negative” comments, and then argue we might have “neutral” comments as well.
    In this case from a given comment, it will usually be clear if the label needs
    to be “positive”, “neutral” or “negative”. But imagine we had the labels “very
    positive”, “positive”, “neutral”, “negative” or “very negative”… For a given comment,
    would it be that easy to decide if it is “positive” or “very positive”? This lack
    of clear definition of the label needs to be avoided, as training with a noisy
    label will make it harder for the model to learn.
  prefs: []
  type: TYPE_NORMAL
- en: Now that the definition of the label is clear, we need to be able to get this
    label for a sufficient and quality set of examples, which will form our training
    data. In our example, we could consider manually tagging a set of comments, be
    it within the company or team, be it externalizing the tagging to professional
    annotators (yes, there are people working full time labelling dataset for ML!).
    Costs and feasibility associated with the obtention of these labels needs to be
    considered.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/55e0ce3919aa814f6b65c54efab18ce4.png)'
  prefs: []
  type: TYPE_IMG
- en: Label for one example, in our case, a comment (image by the author)
  prefs: []
  type: TYPE_NORMAL
- en: Will deployment of the solution be feasible?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To reach final impact, the predictions of the ML model need to be usable. Depending
    on the use case, using the predictions might require specific infrastructures
    (e.g. ML Platform) and experts (e.g. ML Engineers).
  prefs: []
  type: TYPE_NORMAL
- en: 'In our example, as we want to use our model for analytical purposes we could
    run it offline and exploiting the predictions would be quite simple. However,
    if we wanted to automatically respond to a negative comment in the next 5 minutes
    it is published, this would be another story: the model would need to be deployed
    and integrated to make this possible. Overall, it is important to have a clear
    idea of what the requirements to use the predictions will be, to ensure it will
    be feasible with the team and tools available.'
  prefs: []
  type: TYPE_NORMAL
- en: What is at stake?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'ML models will always have a level of error in their predictions. Actually,
    it is a classic in ML to say:'
  prefs: []
  type: TYPE_NORMAL
- en: If the model has no errors, then there is definitely something wrong with the
    data or the model
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: This is important to understand, as if the use case doesn’t allow for these
    errors to happen, then it might not be a good idea to use ML. In our example,
    imagine instead of comments and sentiment, we were using the model to classify
    emails from customers into “pressing charges or not”. It wouldn’t be a good idea
    to have a model that can misclassify an email that is pressing charges against
    the company due to the terrible consequences this might have for the company.
  prefs: []
  type: TYPE_NORMAL
- en: Is using ML ethically correct?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There have been many proven cases of predictive models that discriminated based
    on gender, race and other sensitive personal attributes. Because of this, ML teams
    need to be careful on the data and features they are using for their projects,
    but also on questioning if automating certain types of decision actually makes
    sense from an ethical perspective. You can check my previous [blog post](https://medium.com/glovo-engineering/ml-bias-intro-risks-and-solutions-to-discriminatory-predictive-models-9335b7709818)
    on the topic for further details.
  prefs: []
  type: TYPE_NORMAL
- en: Will I need explainability?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'ML models act somehow as a black box: you input some information, and they
    magically output predictions. The complexity behind the models is what is behind
    this black box, especially if we compare to simpler algorithms from statistics.
    In our example, we might be okay not being able to understand exactly why a comment
    was predicted as “positive” or as “negative”.'
  prefs: []
  type: TYPE_NORMAL
- en: In other use cases, explainability might be a must. For example, in strongly
    regulated sectors like insurances or banks. A bank needs to be able to explain
    why it is granting (or not) a credit to a person even if that decision is based
    on a scoring predictive model.
  prefs: []
  type: TYPE_NORMAL
- en: 'This topic has a strong relationship with the ethics one: if we are not able
    to fully understand the models decisions, it is really hard to know if the model
    has learned to be discriminatory or not.'
  prefs: []
  type: TYPE_NORMAL
- en: Is all this changing with Generative AI?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With the progress on Generative AI, a variety of companies are offering webpages
    and APIs to consume powerful models. How is this changing the limitations and
    considerations I was mentioning before about ML?
  prefs: []
  type: TYPE_NORMAL
- en: '**Data related topics (quality, quantity and labels)**: for use cases that
    can leverage existent GenAI models, this is definitely changing. Huge volumes
    of data are already used to train GenAI models. Quality of the data hasn’t been
    controlled in most of these models, but this seems to compensate with the huge
    volume of data they use. Thanks to these models, it might be the case (again,
    for very specific use cases), that we no longer need training data. This is known
    as zero-shot learning (e.g. *“ask ChatGPT what is the sentiment of a given comment”*)
    and few-shot learning (e.g. *“provide some examples of positive, neutral and negative
    comments to ChatGPT, then ask it to provide the sentiment for a new comment”*).
    A good explanation on this can be found in the [deeplearning.ai newsletter](https://www.deeplearning.ai/the-batch/how-prompting-is-changing-machine-learning-development/).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Deployment feasibility**: for the use cases that can leverage existent GenAI
    models, deployment becomes much easier, as many companies and tools are offering
    easy to use APIs to those powerful models. If those models need to be fine-tuned
    or brought in-house for privacy reasons, then deployment will of course get much
    harder.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/e5b0a50fec9dd97ae0f24a3b96b08a06.png)'
  prefs: []
  type: TYPE_IMG
- en: “Traditional” ML vs leveraging GenAI models (image by the author)
  prefs: []
  type: TYPE_NORMAL
- en: 'Other limitations or considerations are not changing, regardless of leveraging
    GenAI or not:'
  prefs: []
  type: TYPE_NORMAL
- en: '**High stakes:** this will keep being a problem, as GenAI models have a level
    of error in their predictions too. Who hasn’t seen GhatGPT hallucinating or providing
    answers that don’t make sense? What is worse, it is harder to evaluate these models,
    as responses sound always confident regardless of the degree of accuracy they
    have, and evaluation turns subjective (e.g. *“does this response make sense to
    me?”*).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Ethics:** still as important as before. There are proofs GenAI models can
    be biased due to the input data they were used to train with ([link](https://news.mit.edu/2023/large-language-models-are-biased-can-logic-help-save-them-0303)).
    As more companies and functionalities start using these types of models, it is
    important to have the risks this might bring clear.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Explainability:** as GenAI models are bigger and more complex than “traditional”
    ML, explainability on their predictions gets even harder. There is ongoing research
    to understand how this explainability could be achieved, but it is still very
    immature ([link](https://techcrunch.com/2023/05/09/openais-new-tool-attempts-to-explain-language-models-behaviors/)).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wrapping it up
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this blog post we saw the main things to consider when deciding whether **to
    use or not to use ML** and how that is changing with the progress from Generative
    AI models. The main topics discussed were quality and volume of data, label obtention,
    deployment, stakes, ethics and explainability. I hope this summary is useful when
    considering your next ML (or not) initiative!
  prefs: []
  type: TYPE_NORMAL
- en: '**References**'
  prefs: []
  type: TYPE_NORMAL
- en: '[1] [ML bias: intro, risks and solutions to discriminatory predictive models](https://medium.com/glovo-engineering/ml-bias-intro-risks-and-solutions-to-discriminatory-predictive-models-9335b7709818),
    by myself'
  prefs: []
  type: TYPE_NORMAL
- en: '[2] [Beyond Test Sets: how prompting is changing machine learning development](https://www.deeplearning.ai/the-batch/how-prompting-is-changing-machine-learning-development/),
    by deeplearning.ai'
  prefs: []
  type: TYPE_NORMAL
- en: '[3] [Large Language models are biased, can logic help save them?](https://news.mit.edu/2023/large-language-models-are-biased-can-logic-help-save-them-0303),
    by MIT News.'
  prefs: []
  type: TYPE_NORMAL
- en: '[4] [OpenAIs OpenAI’s attempts to explain language models behaviors](https://techcrunch.com/2023/05/09/openais-new-tool-attempts-to-explain-language-models-behaviors/),
    TechCrunch'
  prefs: []
  type: TYPE_NORMAL
