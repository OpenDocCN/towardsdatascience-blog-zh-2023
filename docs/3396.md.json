["```py\nimport openai\n\nopenai.api_key = '<YOUR KEY GOES KEY>'\n\ndef run_llm(query, system_prompt, reference_content):\n\n    llm_query = {\n        \"temperature\": 1.0,\n        \"max_tokens\": 2000,\n        \"top_p\": 0.95,\n        \"frequency_penalty\": 0,\n        \"presence_penalty\": 0,\n    }\n\n    response = openai.ChatCompletion.create(\n        model=model,\n        messages=[  {\n                \"role\":\"system\",\n                \"content\": system_prompt\n            },\n            {\n                \"role\":\"user\",\n                \"content\": query\n            }\n        ],\n        temperature=llm_query['temperature'],\n        max_tokens=llm_query['max_tokens'],\n        top_p=llm_query['top_p'],\n        frequency_penalty=llm_query['frequency_penalty'],\n        presence_penalty=llm_query['presence_penalty'],\n        stop=None\n    )  \n\n    answer = response['choices'][0]['message']['content']\n    return answer\n```", "```py\nimport requests  \nimport os  \nfrom bs4 import BeautifulSoup \nimport re\nimport pandas as pd\nimport PyPDF2 \nimport traceback\nimport json\nimport ast\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nimport tiktoken\n\ndef auto_translate(text):\n    \"\"\"\n    This function automatically detects language and translates to english \n\n    Parameters:\n        text(str): The text to be translated\n\n    Returns:\n        text (str): Translated text if in another language, otherwise \n                    input text\n    \"\"\"\n    try:\n        lang = translator.detect(text)\n        lang = lang.lang\n        print(f\"Language detected: {lang}\")\n        q = translator.translate(text, dest='en')\n        text = q.text\n    except Exception as e:\n        print(\"An exception occurred trying to translate\")\n    return text\n\ndef get_safe_name(name):\n    \"\"\"\n    This function takes a string and returns a version of it that is \n    safe to use as a filename.\n\n    Parameters:\n        name (str): The string to be converted to a safe filename.\n\n    Returns:\n        name (str): The safe filename.\n    \"\"\"\n    name = str(name)\n    name = re.sub(\"[^0-9a-zA-Z]+\", \"_\", name)\n    name = re.sub(r\"_$\",\"\", name)\n    if len(name) == 0:\n        name = 'Unknown' \n    return name\n\ndef download_pdf(url, download_path):  \n    \"\"\"\n    Function to download a PDF from a URL and save locally\n\n    Parameters:\n        url (str): Location of online PDF file\n        download_path (str): Folder where to save PDF\n\n    \"\"\"\n    response = requests.get(url)  \n    with open(download_path, 'wb') as f:  \n        f.write(response.content)  \n\ndef save_text(content, file_path):  \n    \"\"\"\n    Function to save text to local file\n\n    Parameters:\n        content (str): Text to save\n        file_path (str): Folder where to save \n    \"\"\"\n    with open(file_path, 'w') as file:  \n        print(f'Saving {file_path}')\n        file.write(content)  \n\ndef extract_text_from_pdf(pdf_path):  \n    \"\"\"\n    Function to extract text from PDF file\n\n    Parameters:\n        pdf_path (str): Path to PDF file\n\n    Returns:\n        text (str): Text extracted from PDF file\n    \"\"\"\n    print(pdf_path)\n    pdf_reader = PyPDF2.PdfReader(pdf_path)  \n    text = ''  \n    for page_num in range(len(pdf_reader.pages)):  \n        page_obj = pdf_reader.pages[page_num]\n        text += page_obj.extract_text()  \n    return text  \n\ndef get_rw_data(keyword, filter, sort, fields, endpoint, limit=10, \\\n                save_body_to_text=False):  \n    \"\"\"\n    Function to extract data from ReliefWeb API. For API details see:\n\n    https://apidoc.rwlabs.org/?utm_medium=blog&utm_source=reliefweb+website&utm_campaign=api+doc+launching+2016_06\n\n    Parameters:\n        keyword (str): Search string\n        filter (dict): ReliefWeb filter json\n        sort (dict): ReliefWeb sort json\n        fields (list): List of fields to return\n        endpoint (str): API Endpoint, eg reports, disasters\n        limit (int): Maximum records to return\n        save_body_to_text (bool) : Flag to save body to text file, including any PDFs on page\n\n    Returns:\n        all_data (pandas dataframe): Dataframe of data from API\n    \"\"\"\n    query = {  \n        \"appname\": \"myapp\",  \n        \"query\": {  \n            \"value\": keyword\n        },  \n        \"filter\":filter,\n        \"sort\": sort,\n        \"limit\": limit,  \n        \"fields\": fields\n    }  \n    endpoint = f\"{reliefweb_api_url}/{endpoint}?appname=apidoc&query[value]=\"\n    print(f\"Getting {endpoint} ...\")\n\n    all_data =[]\n    response = requests.post(endpoint, json=query)  \n    if response.status_code == 200:  \n        data = response.json()  \n        for article in data[\"data\"]: \n            article_url = article['fields']['url']   \n            try:\n                r = article['fields']\n                print(article_url)\n                article_response = requests.get(article_url)  \n                if save_body_to_text:\n                    soup = BeautifulSoup(article_response.text, 'html.parser')  \n                    main_content = [p.text for p in soup.find_all('p')]  \n                    article_text = ' '.join(main_content)\n                    save_text(article_text, docs_folder + '/{}.txt'.format(get_safe_name(article['fields']['title'])))  \n                    for link in soup.find_all('a'):  \n                        href = link.get('href')  \n                        if href.endswith('.pdf'):  \n                            download_path = os.path.join(docs_folder, href.split('/')[-1])  \n                            if href.startswith('/attachments'):\n                                pdf_url = f'{reliefweb_pdf_url}{href}'\n                            else:\n                                pdf_url = href\n                            download_pdf(pdf_url, download_path)  \n                            print(f\".    Downloaded PDF {download_path} from {pdf_url}\")\n                            article_text = extract_text_from_pdf(download_path)\n                    r['article_text'] = article_text\n                    r['reliefweb_query'] = keyword\n                all_data.append(r)\n            except Exception as e:\n                print(f\"An exception occurred trying to extract {article_url}\")\n                tb_str = ''.join(traceback.format_exception(None, e, e.__traceback__))\n                print(tb_str)\n\n        all_data = pd.DataFrame(all_data)\n        for f in ['disaster','theme']:\n            if f in list(all_data.columns):\n                all_data[f] = all_data[f].astype(str)\n        return all_data  \n    else:  \n        print(f\"Request failed with status {response.status_code} {response.text}\")  \n        return None \n```", "```py\nfilter = {\n    \"operator\": \"AND\",\n    \"conditions\": [\n        {\n            \"field\": \"disaster.status\",\n            \"value\": \"ongoing\"\n        },\n        {\n            \"field\": \"format.name\",\n            \"value\": \"Situation Report\"\n        },\n        {\n            \"field\": \"date.created\",\n            \"value\": {\n                \"from\": \"2023-11-01T00:00:00+00:00\",\n                \"to\": \"2023-11-30T23:59:59+00:00\"\n            }\n        }\n    ]\n}\nsort = [\"date.created:desc\"]\nendpoint = \"reports\"\nfields = {  \n    \"include\": [\"title\", \"body\", \"url\", \"source\", \"date\", \"format\", \"theme\", \n    \"country\", \"status\", \"primary_country\", \"disaster\", \"language\", \"id\"] \n}  \nreliefweb_query = \"\"\narticles = get_rw_data(reliefweb_query, filter, sort, fields, endpoint, 1000, True)\n```", "```py\nfor index, row in articles.iterrows():\n    date = row['date']['created']\n    source = row['source'][0]['name']\n    title = row['title']\n    id = row['id']\n    filename = f'{get_safe_name(title)}__{id}_{get_safe_name(date)}.txt'\n\n    text = f'''\n        title: {title}\n        source: {source}\n        date: {date}\n        id: {id}\n\n        {row['article_text']}\n    '''\n    text = text.encode('utf-8','ignore').decode('utf-8','ignore')\n    print(text)\n\n    save_text(text, docs_folder + '/' + filename) \n```", "```py\n# MISSION\nYou are a Sparse Priming Representation (SPR) writer. An SPR is a \nparticular kind of use of language for advanced NLP, NLU, and NLG \ntasks, particularly useful for the latest generation of Large Language \nModels (LLMs). You will be given information by the USER which you \nare to render as an SPR.\n\n# THEORY\nLLMs are a kind of deep neural network. They have been demonstrated \nto embed knowledge, abilities, and concepts, ranging from reasoning \nto planning, and even to theory of mind. These are called latent abilities and latent content, collectively referred to as latent space. The latent space of an LLM can be activated with the correct series of words as inputs, which will create a useful internal state of the neural network. This is not unlike how the right shorthand cues can prime a human mind to think in a certain way. Like human minds, LLMs are associative, meaning you only need to use the correct associations to \"prime\" another model to think in the same way. The exception are quantitative metrics, which you MUST preserve.\n\n# METHODOLOGY\nRender the input as a distilled list of succinct statements, \nassertions, associations, concepts, analogies, and metaphors. \nThe idea is to capture as much, conceptually, as possible but with \nas few words as possible while preserving all quantitative details. \nWrite it in a way that makes sense to you, as the future audience \nwill be another language model, not a human.\n```", "```py\n- Information comes from World Health Organization. \n- Weekly dengue cases decreasing, 12,634 reported this week. \n- 78 dengue-related deaths this week. \n- Cumulative cases since January 2023: 267,680, including 1,333 deaths. \n- Cases decreasing in all divisions except Sylhet. \n- Increase in dengue cases in Faridpur, Khulna, Rajshahi, Rajbari districts. \n- WHO orientation session for entomologists in Dhaka occurred on 26 October. \n- Overall Case Fatality Rate (CFR) is 0.5%, but 0.62% for this week. \n- Dhaka division reports highest number of cases and deaths. \n- In this week, 13,430 people released from hospital after treatment. \n- Male-to-female death ratio: 43% vs 57%. \n- Most deaths among 16-55 years age group. \n- Four dengue virus types circulating: DENV-1 (2.2%), DENV-2 (68.1%), DENV-3 (25.4%), DENV-4 (0.2%). \n- Dengue prevention & clean-up week launched nationwide from 29 October to 4 November. \n- WHO expert entomologist deployed to Bangladesh for technical support in outbreak response. \n- In the Rohingya camps, weekly dengue cases continue to decline. Total cumulative cases are 12,969 including 17 deaths (CFR: 0.1%).\n```", "```py\n# gpt4 turbo is 128k \nchunk_size = 100000\n\nllm_query = {\n    \"prompt\": \"\",\n    \"temperature\": 1.0,\n    \"max_tokens\": 2000,\n    \"top_p\": 0.95,\n    \"frequency_penalty\": 0,\n    \"presence_penalty\": 0,\n    \"system_prompt\":\"\"\"\n        # MISSION\n        You are a Sparse Priming Representation (SPR) writer. An SPR is a \n        particular kind of use of language for advanced NLP, NLU, and NLG \n        tasks, particularly useful for the latest generation of Large Language \n        Models (LLMs). You will be given information by the USER which you \n        are to render as an SPR.\n\n        # THEORY\n        LLMs are a kind of deep neural network. They have been demonstrated \n        to embed knowledge, abilities, and concepts, ranging from reasoning \n        to planning, and even to theory of mind. These are called latent abilities and latent content, collectively referred to as latent space. The latent space of an LLM can be activated with the correct series of words as inputs, which will create a useful internal state of the neural network. This is not unlike how the right shorthand cues can prime a human mind to think in a certain way. Like human minds, LLMs are associative, meaning you only need to use the correct associations to \"prime\" another model to think in the same way. The exception are quantitative metrics, which you MUST preserve.\n\n        # METHODOLOGY\n        Render the input as a distilled list of succinct statements, \n        assertions, associations, concepts, analogies, and metaphors. \n        The idea is to capture as much, conceptually, as possible but with \n        as few words as possible while preserving all quantitative details. \n        Write it in a way that makes sense to you, as the future audience \n        will be another language model, not a human.\n\n    \"\"\"\n}\n\n# Save texts\nfor index, row in articles.iterrows():\n    date = row['date']['created']\n    source = row['source'][0]['name']\n    report = row['title']\n    id = row['id']\n    text = row['article_text']\n    primary_country = row['primary_country']['name']\n    disaster = ''\n    disaster_types = ''\n    for d in ast.literal_eval(row['disaster']):\n        disaster += f\"{d['name']}; \"\n        for t in d['type']:\n            if 'primary' in t and t['primary'] == True:\n                disaster_types += f\"{t['name']}; \"\n    d = {\n        \"disaster\": disaster,\n        \"date\": date,\n        \"disaster_types\": disaster_types\n    }\n    prefix = \"\"\n    filename = f'{get_safe_name(report)}__{id}_{get_safe_name(date)}.txt'\n    header = f'- report: \"{report}\"\\n- disaster: \"{disaster}\"\\n' + \\\n             f'- disaster_types: \"{disaster_types}\"\\n' + \\\n             f'- primary_country: \"{primary_country}\"\\n- source: \"{source}\"\\n' + \\\n             f'- date: \"{date}\"\\n- id: \"{id}\"\\n'\n\n    text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n        chunk_size=chunk_size, chunk_overlap=100\n    )\n    texts = text_splitter.split_text(text)\n    print(f\"\\n\\n================ {report} =================\\n\")\n    print(primary_country)\n    print(disaster)\n    print(len(texts))\n\n    summarized_text = ''\n\n    for i, t in enumerate(texts):\n        response = openai.ChatCompletion.create(\n        model=model,\n        messages=[  {\n                \"role\":\"system\",\n                \"content\":llm_query['system_prompt']\n            },\n            {\n                \"role\":\"user\",\n                \"content\":t\n            }\n        ],\n        temperature=llm_query['temperature'],\n        max_tokens=llm_query['max_tokens'],\n        top_p=llm_query['top_p'],\n        frequency_penalty=llm_query['frequency_penalty'],\n        presence_penalty=llm_query['presence_penalty'],\n        stop=None)  \n\n        summary = response['choices'][0]['message']['content']\n        summarized_text += \"\\n\" + summary\n\n    summarized_text = auto_translate(summarized_text)\n    summarized_text = header + summarized_text\n\n    summarized_text = summarized_text.split(\"\\n\")\n    summarized_text_prefixed = ''\n    for s in summarized_text:\n        summarized_text_prefixed += f\"{prefix}{s}\\n\"\n\n    print(summarized_text_prefixed)\n    save_text(summarized_text_prefixed, docs_folder2 + '/' + filename)\n```", "```py\nfilter = {\n    \"operator\": \"AND\",\n    \"conditions\": [\n        {\n            \"field\": \"status\",\n            \"value\": \"ongoing\"\n        },\n        {\n            \"field\": \"date.event\",\n            \"value\": {\n                \"from\": \"2020-01-01T00:00:00+00:00\",\n                \"to\": \"2023-11-30T23:59:59+00:00\"\n            }\n        }\n    ]\n}\nsort = [\"date.event:desc\"]\nendpoint = \"disasters\"\nfields = {  \n    \"include\": [\"name\", \"description\", \"date\", \"url\", \"id\",\"status\",\"glide\"] \n} \nreliefweb_query = \"\"\ndisasters = get_rw_data(reliefweb_query, filter, sort, fields, endpoint, 1000, False)\ndisplay(disasters)\ndisasters.to_csv('disasters.csv')\n```", "```py\ndisasters = pd.read_csv('disasters.csv')\nconcatenated_content = \"=========== this section gives a list of DISASTERS =========== \\n\\n \"+ disasters.to_csv() \nconcatenated_content += \"\\n\\n=========== this section provides disater REPORTS for each disaster =========== \"\nfor f in os.listdir(docs_folder2):\n  with open(f\"{docs_folder2}/{f}\", \"r\") as file:\n      file_content = file.read()\n      concatenated_content += f\"\\n\\n----- report: {f} ----- \\n\\n\"\n      concatenated_content += file_content + \"\\n\\n\"\n```", "```py\ndef num_tokens_from_string(string: str, encoding_name: str) -> int:\n    encoding = tiktoken.get_encoding(encoding_name)\n    num_tokens = len(encoding.encode(string))\n    gpt4_token_cost_per_1000 = 0.01\n    cost = (num_tokens/1000.0)*gpt4_token_cost_per_1000\n    return num_tokens, cost\n\ntokens, cost = num_tokens_from_string(concatenated_content,\"cl100k_base\")\n\nOpenAI Tokens: 82001 ($0.82001)\n```", "```py\ndef run_llm(query, reference_content):\n\n    llm_query = {\n        \"temperature\": 1.0,\n        \"max_tokens\": 2000,\n        \"top_p\": 0.95,\n        \"frequency_penalty\": 0,\n        \"presence_penalty\": 0,\n    }\n\n    response = openai.ChatCompletion.create(\n        model=model,\n        messages=[  {\n                \"role\":\"system\",\n                \"content\": f\"\"\"You are a ReliefWeb disasters bot. You \n                provide information on diasters. \n\n                If asked for a list of disasters, just provide the list \n                ONLY using the 'Disasters' section below. \n\n                If asked for details of a disaster, extract that information \n                from 'Situation Reports' section below.   \n\n                {reference_content}\"\"\"\n            },\n            {\n                \"role\":\"user\",\n                \"content\": query\n            }\n        ],\n        temperature=llm_query['temperature'],\n        max_tokens=llm_query['max_tokens'],\n        top_p=llm_query['top_p'],\n        frequency_penalty=llm_query['frequency_penalty'],\n        presence_penalty=llm_query['presence_penalty'],\n        stop=None\n    )  \n\n    answer = response['choices'][0]['message']['content']\n    return answer\n```", "```py\nanswer = run_llm(\"What's the latest status regaring ther Nepalese earthquake \nthis year?\", concatenated_content)\nprint(answer)\n```", "```py\nThe latest status regarding the Nepal earthquake as of November 2023 is as \nfollows:\n\n- Earthquake: Western Nepal, 6.4 magnitude, occurred on 3 November 2023, \nwith an epicenter in Ramidanda, Jajarkot.\n\n- Casualties: 154 deaths, 364+ injuries.\n\n- Displacement: Approximately 250,000 people affected, with significant \nnumbers of homes damaged or destroyed.\n\n- Infrastructure: Over 62,000 houses across 13 districts sustained \ndamage (26,557 fully, 35,455 partially). Several government buildings were also damaged.\n\n- Humanitarian Response: The United Nations and other humanitarian \norganizations have been active in relief efforts, providing medical tents, child-friendly spaces, WASH supplies, and psychosocial support, among other assistance.\n\n- Recovery and Rehabilitation: The recovery and rehabilitation phase is \nin planning, with international reinforcements arriving to aid in coordination and strategy development.\n\n- Education and Health Services: Education has been disrupted for \nabout 80,000 children due to damaged schools, and health services \nhave been impacted, increasing the risk of malnutrition and complicating maternal and newborn care.\n\nPlease note that this is a condensed summary of the available data up until \n10 November 2023, and further updates may follow as the situation evolves \nand more assessments are made.\n```", "```py\nanswer = run_llm(\"How many people died in the Nepal earthquake in 2023 occur?\", \nconcatenated_content)\nprint(answer)\n\nIn the Nepal Earthquake of November 2023, there were 153 fatalities.\n```", "```py\nanswer = run_llm(\"List the organizations providing sitreps for the Nepal \nEarthquake\", concatenated_content)\nprint(answer)\n\nThe organizations providing situation reports (sitreps) for the Nepal Earthquake include:\n\n- UN Office for the Coordination of Humanitarian Affairs (OCHA)\n- Nepal Red Cross Society\n- International Federation of Red Cross And Red Crescent Societies (IFRC)\n- UN Resident Coordinator in Nepal\n- World Health Organization (WHO)\n- UN Children's Fund (UNICEF)\n- UN Country Team in Nepal\n```", "```py\nanswer = run_llm(\"How many displaced people are children?\", \nconcatenated_content)\nprint(answer)\n\nIn the provided data, there are references to children affected by disasters \nin different reports, but the specific number of displaced children is not \ndirectly mentioned in any of the excerpts. It would be necessary to review \nmore detailed reports or data sources that focus specifically on displacement \nfigures for children to determine the exact number affected by displacement.\n```", "```py\nanswer = run_llm(\"What disease risks are there for the Nepal 2023 earthquake?\", \nconcatenated_content)\nprint(answer)\n\nThe disease risks for the Nepal 2023 earthquake, as reported by UNICEF in \ntheir Humanitarian Situation Report No. 2, are primarily focused on \nmalnutrition, particularly among children under five years old, and \npsychosocial stress affecting the affected population. The disruption of \nhealthcare and WASH (Water, Sanitation, and Hygiene) services due to \nthe earthquake has further exacerbated the health risks.\n\nThe key disease risks and health concerns include:\n\n1\\. Malnutrition: The earthquake and its aftermath have disrupted the food \nsupply chain, which can lead to an increased risk of malnutrition among \nvulnerable populations, especially children.\n\n2\\. Maternal and Newborn Health: Pregnant women and newborns might face \nincreased health risks due to the disruption of health care services and \na lack of access to essential medical care and facilities.\n\n3\\. Psychosocial Stress: The trauma caused by the earthquake, the loss of \nloved ones, and the displacement can lead to significant psychosocial \nstress and mental health issues.\n\n4\\. Injuries and Physical Health: With many injured in the aftermath, \nthere is an increased need for medical treatment for physical injuries \nand trauma care. \n\n5\\. Communicable Diseases: Overcrowding in temporary shelters can lead \nto the spread of communicable diseases, particularly in the absence of \nproper sanitation and hygiene facilities.\n\n6\\. Waterborne and Vector-borne Diseases: Lack of clean water and \nsanitation increases the risk of waterborne diseases like diarrhea, \ncholera, and dysentery. There is also a concern for vector-borne \ndiseases such as malaria and dengue fever.\n\n7\\. Non-communicable Diseases: Disruption of routine health services \ncan affect the management and treatment of chronic non-communicable diseases.\n\n8\\. Disruption of Routine Immunization: The potential disruption of \nroutine immunization services can increase the risk of vaccine-preventable \ndiseases.\n\nIn response to these health risks, UNICEF and other health sector partners \nhave been providing health services, medical supplies, and psychosocial \nsupport, as well as ensuring continued access to nutrition for children \nand mothers.\n```"]