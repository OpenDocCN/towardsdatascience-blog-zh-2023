- en: 'Learning and Inference in a Forward Pass: The New Framework'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 前向传递中的学习与推理：新框架
- en: 原文：[https://towardsdatascience.com/learning-and-inference-in-a-forward-pass-the-new-framework-dc1356399002?source=collection_archive---------1-----------------------#2023-01-20](https://towardsdatascience.com/learning-and-inference-in-a-forward-pass-the-new-framework-dc1356399002?source=collection_archive---------1-----------------------#2023-01-20)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/learning-and-inference-in-a-forward-pass-the-new-framework-dc1356399002?source=collection_archive---------1-----------------------#2023-01-20](https://towardsdatascience.com/learning-and-inference-in-a-forward-pass-the-new-framework-dc1356399002?source=collection_archive---------1-----------------------#2023-01-20)
- en: Learning and inference unified into a continuous, asynchronous, and parallel
    process
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 学习与推理统一为一个连续、异步和并行的过程
- en: '[](https://amassivek.medium.com/?source=post_page-----dc1356399002--------------------------------)[![Adam
    Kohan](../Images/3a8ed2e6fd192a14b76ff5ca2616fcf1.png)](https://amassivek.medium.com/?source=post_page-----dc1356399002--------------------------------)[](https://towardsdatascience.com/?source=post_page-----dc1356399002--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----dc1356399002--------------------------------)
    [Adam Kohan](https://amassivek.medium.com/?source=post_page-----dc1356399002--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://amassivek.medium.com/?source=post_page-----dc1356399002--------------------------------)[![Adam
    Kohan](../Images/3a8ed2e6fd192a14b76ff5ca2616fcf1.png)](https://amassivek.medium.com/?source=post_page-----dc1356399002--------------------------------)[](https://towardsdatascience.com/?source=post_page-----dc1356399002--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----dc1356399002--------------------------------)
    [Adam Kohan](https://amassivek.medium.com/?source=post_page-----dc1356399002--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F8275b40d8f6b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flearning-and-inference-in-a-forward-pass-the-new-framework-dc1356399002&user=Adam+Kohan&userId=8275b40d8f6b&source=post_page-8275b40d8f6b----dc1356399002---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----dc1356399002--------------------------------)
    ·18 min read·Jan 20, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fdc1356399002&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flearning-and-inference-in-a-forward-pass-the-new-framework-dc1356399002&user=Adam+Kohan&userId=8275b40d8f6b&source=-----dc1356399002---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F8275b40d8f6b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flearning-and-inference-in-a-forward-pass-the-new-framework-dc1356399002&user=Adam+Kohan&userId=8275b40d8f6b&source=post_page-8275b40d8f6b----dc1356399002---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----dc1356399002--------------------------------)
    ·18分钟阅读·2023年1月20日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fdc1356399002&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flearning-and-inference-in-a-forward-pass-the-new-framework-dc1356399002&user=Adam+Kohan&userId=8275b40d8f6b&source=-----dc1356399002---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fdc1356399002&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flearning-and-inference-in-a-forward-pass-the-new-framework-dc1356399002&source=-----dc1356399002---------------------bookmark_footer-----------)![](../Images/e992fb971fe17fd5af8af3572ad5acff.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fdc1356399002&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flearning-and-inference-in-a-forward-pass-the-new-framework-dc1356399002&source=-----dc1356399002---------------------bookmark_footer-----------)![](../Images/e992fb971fe17fd5af8af3572ad5acff.png)'
- en: '**Learning and inference in a forward pass on a three layer network.** Inference
    and learning take place concurrently. Learning reuses the same forward path as
    inference. Taken together, these features of signal propagation unify learning
    and inference as a continuous, asynchronous, and parallel process — freeing constraints
    on learning. This is a distinct departure from previous perspectives on learning,
    especially in the supervised setting and under backpropagation.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '**在三层网络上进行前向传递中的学习和推理。** 推理和学习同时进行。学习重用与推理相同的前向路径。综合来看，这些信号传播特性将学习和推理统一为一个连续、异步和并行的过程——从而解除学习的限制。这与之前关于学习的观点大相径庭，尤其是在监督设置和反向传播下。'
- en: In this post, I present the framework for inference and learning in a forward
    pass, called the Signal Propagation framework. This is a framework for using only
    forward passes to learn any kind of data and on any kind of network. I demonstrate
    it works well for discrete networks, continuous networks, and spiking networks,
    all without modification to the network architecture. In other words, the version
    of network used for inference is the same as the version used for learning. In
    contrast, backpropagation and previous works have additional structure and algorithm
    elements for the training version of the network than for the inference version
    of the network, which are referred to as learning constraints.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我介绍了一个前向传播的推理和学习框架，称为信号传播框架。这是一个只使用前向传播来学习任何类型数据和任何类型网络的框架。我展示了它在离散网络、连续网络和尖峰网络中都能良好地工作，而无需修改网络架构。换句话说，用于推理的网络版本与用于学习的网络版本相同。相比之下，反向传播和以前的工作对于训练版本的网络有额外的结构和算法元素，这些被称为学习约束。
- en: Signal Propagation is a least constrained method for learning, and yet has better
    performance, efficiency, and compatibility than previous alternatives to backpropagation.
    It also has better efficiency and compatibility than backpropagation. This framework
    is introduced in [https://arxiv.org/abs/2204.01723](https://arxiv.org/abs/2204.01723)
    (2022) and [https://ieeexplore.ieee.org/document/10027559](https://ieeexplore.ieee.org/document/10027559).
    The origin of forward learning is in my work [https://arxiv.org/abs/1808.03357](https://arxiv.org/abs/1808.03357)
    (2018).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 信号传播是一种约束最少的学习方法，且具有比反向传播以前的替代方法更好的性能、效率和兼容性。它也比反向传播具有更好的效率和兼容性。这个框架在[https://arxiv.org/abs/2204.01723](https://arxiv.org/abs/2204.01723)（2022年）和[https://ieeexplore.ieee.org/document/10027559](https://ieeexplore.ieee.org/document/10027559)中介绍。前向学习的起源在我的工作[https://arxiv.org/abs/1808.03357](https://arxiv.org/abs/1808.03357)（2018年）。
- en: I developed a library to implement forward pass learning on any model. There
    is a [quick start](https://github.com/amassivek/signalpropagation#2-quick-start)
    for implementing the library on an existing model. There are [example](https://github.com/amassivek/signalpropagation/tree/main/examples)
    experiments for cifar-10, which also serve as a tutorial.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 我开发了一个库来实现对任何模型的前向传播学习。该库的[快速开始](https://github.com/amassivek/signalpropagation#2-quick-start)指南可以帮助你在现有模型上实现这个库。还有[cifar-10](https://github.com/amassivek/signalpropagation/tree/main/examples)的[示例](https://github.com/amassivek/signalpropagation/tree/main/examples)实验，也作为教程使用。
- en: '[](https://github.com/amassivek/signalpropagation?source=post_page-----dc1356399002--------------------------------)
    [## GitHub - amassivek/signalpropagation: Forward Pass Learning and Inference
    Library, for neural…'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://github.com/amassivek/signalpropagation?source=post_page-----dc1356399002--------------------------------)
    [## GitHub - amassivek/signalpropagation: 前向传播学习与推理库，适用于神经网络...'
- en: Signal Propagation The Framework for Unifying Learning and Inference in a Forward
    Pass A python package for training…
  id: totrans-13
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 信号传播：一个统一学习和推理的前向传播框架，一个用于训练的Python包...
- en: github.com](https://github.com/amassivek/signalpropagation?source=post_page-----dc1356399002--------------------------------)
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '[github.com](https://github.com/amassivek/signalpropagation?source=post_page-----dc1356399002--------------------------------)'
- en: This post is a concise tutorial for learning in a forward pass. By the end of
    the tutorial, you will understand the concept, and know how to apply this form
    of learning in your work. The tutorial provides explanations for beginners, and
    detailed steps for experts.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 这篇文章是一个关于前向传播学习的简明教程。在教程结束时，你将理解这个概念，并知道如何在你的工作中应用这种学习方式。教程提供了适合初学者的解释，以及适合专家的详细步骤。
- en: Table of Contents
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 目录
- en: Introduction
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 引言
- en: 1.1\. Previous Approaches to Learning
  id: totrans-18
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 1.1\. 以前的学习方法
- en: 1.2\. A New Framework for Learning
  id: totrans-19
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 1.2\. 一种新的学习框架
- en: 1.3\. The Problem with Learning Constraints
  id: totrans-20
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 1.3\. 学习约束的问题
- en: The Two Elements of Learning
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 学习的两个要素
- en: Learning in a Forward Pass
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 前向传播中的学习
- en: 3.1\. The Approach to Learn
  id: totrans-23
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 3.1\. 学习方法
- en: 3.2\. The Steps to Learn
  id: totrans-24
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 3.2\. 学习步骤
- en: 3.3\. Overview of Complete Procedure
  id: totrans-25
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 3.3\. 完整程序概述
- en: 3.4\. Spiking Networks
  id: totrans-26
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 3.4\. 尖峰网络
- en: Works on Forward Learning
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 前向学习的工作
- en: 4.1 Error Forward Propagation
  id: totrans-28
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 4.1 错误前向传播
- en: 4.2\. Forward Forward
  id: totrans-29
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 4.2\. 前向传播
- en: Reading Material
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 阅读材料
- en: 'Appendix: Reading on Credit Assignment'
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 附录：信用分配阅读
- en: 6.1\. Spatial Credit Assignment
  id: totrans-32
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 6.1\. 空间信用分配
- en: 6.2\. Temporal Credit Assignment
  id: totrans-33
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 6.2\. 时间信用分配
- en: 1\. Introduction
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 1\. 引言
- en: 1.1\. Previous Approaches to Learning
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.1\. 以前的学习方法
- en: Learning is the active ingredient in making artificial neural networks work.
    Backpropagation is recognized as the best performing learning algorithm, powering
    the success of artificial neural networks. However, it is a highly constrained
    learning algorithm. And, it is these constraints that are seen as necessary for
    its high performance. It is well accepted that reducing even some of these constraints
    lowers performance. However, due to these same constraints, backpropagation has
    problems with efficiency and compatibility. It is not efficient with time, memory,
    and energy. It has low compatibility with biological models of learning, neuromorphic
    chips, and edge devices. So, one may think to address this problem by reducing
    different subsets of constraints in an attempt to increase efficiency and compatibility
    without heavily lowering performance.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 学习是使人工神经网络工作的活跃成分。反向传播被认为是表现最佳的学习算法，为人工神经网络的成功提供了动力。然而，它是一个高度约束的学习算法。这些约束被视为其高性能所必需的。普遍接受的观点是，减少这些约束中的任何一个都会降低性能。然而，由于这些相同的约束，反向传播在效率和兼容性方面存在问题。它在时间、内存和能量方面效率低下，与生物学习模型、神经形态芯片和边缘设备的兼容性低。因此，人们可能会考虑通过减少不同的约束子集来解决这个问题，以提高效率和兼容性，同时尽量不降低性能。
- en: 'For example, two constraints of backpropagation on the training network are:
    (1) the addition of feedback weights that are symmetric with the feedforward weights;
    and (2) the requirement of having these feedback weights for every neuron. The
    inference network never uses the feedback weights, that is why we refer to them
    as learning constraints. Subsets of these constraints include: not adding any
    feedback weights, only adding feedback weights for one or two layers in a five
    layer network, not having the feedback weights be symmetric, or any combination
    of these. This means constraints can be added or removed in part or entirely to
    form subsets of constraints to reduce. One may keep trying to reduce different
    subsets of these constraints, in an attempt to increase efficiency and compatibility,
    and hope to not heavily impact performance.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，训练网络的反向传播有两个约束： (1) 反馈权重与前馈权重对称；以及 (2) 每个神经元都需要这些反馈权重。推理网络从不使用反馈权重，这就是我们将它们称为学习约束的原因。这些约束的子集包括：不添加任何反馈权重、只在五层网络中的一两层添加反馈权重、反馈权重不对称，或这些的任意组合。这意味着约束可以部分或完全地添加或移除，从而形成约束子集进行减少。可以不断尝试减少这些约束的不同子集，以提高效率和兼容性，并希望不会对性能产生重大影响。
- en: Previous alternative learning algorithms to backpropagation have attempted relaxing
    constraints, without success. They reduce subsets of constraints on learning to
    improve efficiency and compatibility. They keep other constraints, with the expectation
    of retaining performance similar to the performance found by keeping all the constraints
    (which is backpropagation). So, this implies there is a spectrum for learning
    constraints, from highly constrained, such as backpropagation, to no constraints,
    such as Signal Propagation, the framework I am introducing here.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 以前的反向传播替代学习算法曾尝试放松约束，但没有成功。它们减少学习中的约束子集以提高效率和兼容性。它们保留其他约束，期望保持与保留所有约束（即反向传播）相似的性能。因此，这意味着学习约束存在一个光谱，从高度约束的反向传播到没有约束的信号传播，即我在这里介绍的框架。
- en: 1.2\. A New Framework for Learning
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.2\. 一种新的学习框架
- en: Now, I demonstrate a shift away from previous works. The results presented here
    provide support that the least constrained learning method, Signal Propagation,
    has better performance, efficiency, and compatibility than alternatives to backpropagation
    that selectively reduce constraints on learning. This includes well established
    and highly impactful methods such as random feedback alignment, direct feedback
    alignment, and local learning (all without backpropagation). This is a fascinating
    insight into learning across fields from neuroscience to computer science. It
    benefits areas from biological learning (e.g. in the brain) to artificial learning
    (e.g. in neural networks, hardware, neuromorphic chips).
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我展示了与以往工作的不同。这里展示的结果支持最少约束的学习方法——信号传播——在性能、效率和兼容性上优于反向传播的替代方法，这些替代方法选择性地减少学习的约束。这包括一些成熟且有重大影响的方法，如随机反馈对齐、直接反馈对齐和局部学习（所有这些方法都不依赖反向传播）。这是对从神经科学到计算机科学等领域学习的迷人洞察。它惠及生物学习（例如在大脑中）到人工学习（例如在神经网络、硬件、神经形态芯片中）的领域。
- en: Signal Propagation also significantly informs the direction of future research
    in learning algorithms where backpropagation is the standard of comparison. On
    the spectrum of learning constraints, contrary to the highly constrained backpropagation,
    Signal Propagation is the least constrained method to compare with and to start
    from for developing learning algorithms. With only backpropagation as a best performing
    comparison, learning algorithms did not have a starting point, only an end goal.
    Now, I am introducing Signal Propagation as the new baseline for learning algorithms
    to assess their efficiency, compatibility, and performance.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 信号传播也显著影响了学习算法未来研究的方向，其中反向传播是比较的标准。在学习约束的范围中，与高度约束的反向传播相反，信号传播是最少约束的方法，用于比较和作为开发学习算法的起点。仅有反向传播作为最佳比较，学习算法没有起始点，只有最终目标。现在，我引入信号传播作为学习算法评估其效率、兼容性和性能的新基准。
- en: 1.3\. The Problem with Learning Constraints
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.3\. 学习约束的问题
- en: What are the constraints found under backpropagation?
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 反向传播下的约束是什么？
- en: Why are they an issue?
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么这是个问题？
- en: 'Learning constraints under backpropagation are difficult to reconcile with
    learning in the brain. Below, I provide the main constraints:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 反向传播下的学习约束与大脑中的学习难以调和。下面，我提供了主要的约束：
- en: A complete forward pass through the network is required before sequentially
    delivering feedback in reverse order during a backward pass.
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在网络中完成完整的前向传播后，才可以顺序地在反向传播中提供反馈。
- en: The training network needs the addition of comprehensive feedback connectivity
    for every neuron.
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练网络需要为每个神经元添加全面的反馈连接。
- en: There are two different computations for learning and for inference. In other
    words, the feedback algorithm is a distinct type of computation, separate from
    feedforward activity.
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习和推理有两种不同的计算。换句话说，反馈算法是一种与前馈活动分开的计算类型。
- en: The feedback weights need to be symmetric with the feedforward weights.
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 反馈权重需要与前馈权重对称。
- en: 'These constraints also hinder efficient implementations of learning algorithms
    on hardware for the following reasons:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 这些约束还妨碍了在硬件上高效实施学习算法，原因如下：
- en: weight symmetry is incompatible with elementary computing units which are not
    bidirectional.
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 权重对称性与非双向的基本计算单元不兼容。
- en: transportation of non local weight and error information requires special communication
    channels.
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 非本地权重和误差信息的传输需要特殊的通信通道。
- en: 'These learning constraints prohibit parallelization of computations during
    learning, and increase memory and compute for the following reasons:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 这些学习约束禁止在学习过程中进行计算并行化，并增加了内存和计算量，原因如下：
- en: The forward pass needs to complete before the backward pass can begin (Time,
    Sequential)
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 前向传播需要在后向传播开始之前完成（时间，顺序）
- en: Activations of hidden layers need to be stored during the forward pass for the
    backward pass (Memory)
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 隐藏层的激活需要在前向传播期间存储，以便于后向传播（内存）
- en: Backward pass requires special feedback connectivity (Structure)
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 反向传播需要特殊的反馈连接（结构）
- en: Parameters are updated in reverse order of the forward pass (Time, Synchronous)
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 参数在前向传播的反向顺序中更新（时间，同步）
- en: 2\. The Two Elements of Learning
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 2\. 学习的两个要素
- en: How does learning function in neural networks?
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络中的学习是如何进行的？
- en: 'The short answer: Spatial and Temporal Credit Assignment'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 简短回答：空间和时间信用分配
- en: 'There are two primary forms of data: individual inputs, and multiple connected
    inputs which are sequentially or temporally connected. An image of a dog is an
    individual input as the network makes a prediction based solely on that image.
    In this case, the network is given a single image to predict if the image is of
    a dog or turtle.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 数据主要有两种形式：单个输入和多个连接的输入，这些输入按顺序或时间上连接。狗的图像是一个单个输入，因为网络仅基于该图像进行预测。在这种情况下，网络被提供一个单一图像以预测该图像是狗还是海龟。
- en: A video of a turtle walking is multiple connected inputs as videos are made
    up of multiple images, and the network makes a prediction after seeing all of
    these images. In this case, the network is given multiple images to predict if
    the turtle is walking or hiding.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 一只海龟行走的视频是多个连接的输入，因为视频由多个图像组成，网络在看到所有这些图像后进行预测。在这种情况下，网络被提供多个图像以预测海龟是行走还是隐藏。
- en: Backpropagation (BP) is used for individual inputs; Backpropagation Through
    Time (BPT) is used for multiple connected inputs.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 反向传播 (BP) 用于单个输入；时间反向传播 (BPT) 用于多个连接的输入。
- en: 'BP provides learning for:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: BP 提供的学习包括：
- en: Every neuron (spatial credit assignment)
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个神经元（空间信用分配）
- en: 'BPT provides learning for:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: BPT 提供的学习包括：
- en: Every neuron (spatial credit assignment)
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个神经元（空间信用分配）
- en: Multiple connected inputs (temporal credit assignment)
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多个连接的输入（时间信用分配）
- en: Providing learning for every neuron is known as the spatial credit assignment
    problem. Spatial credit assignment refers to the placement of neurons in the network,
    such as organized into layers of neurons. For example, in a five layer network,
    the backpropagation learning signal travels from the fifth layer sequentially
    all the way down to the first layer of neurons. In section 3, I will show how
    the signal propagation learning signal travels from the first layer to the fifth
    layer, the same as inference.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 为每个神经元提供学习被称为空间信用分配问题。空间信用分配指的是网络中神经元的布置，例如组织成神经元层。例如，在一个五层网络中，反向传播学习信号从第五层依次传递到第一层神经元。在第3节中，我将展示信号传播学习信号是如何从第一层传递到第五层的，与推断过程相同。
- en: Providing learning for multiple connected inputs is known as the temporal credit
    assignment problem. Temporal credit assignment refers to moving through the multiple
    connected inputs. For example, each image in the video is fed into the network,
    producing a new response from the same neurons. Each neuron response is specific
    to each of the images/inputs. So, the backpropagation learning signal travels
    through each of these neuron responses, starting from the neuron response for
    the last image in the video to the neuron response for the first image. In section
    3, it will become clear that the signal propagation learning signal travels from
    the neuron response to the first image to the neuron response for the last image,
    the same as inference.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 为多个连接的输入提供学习被称为时间信用分配问题。时间信用分配指的是在多个连接输入中移动。例如，视频中的每一帧图像都被输入到网络中，产生来自相同神经元的新响应。每个神经元的响应是特定于每张图像/输入的。因此，反向传播学习信号在这些神经元响应中传递，从视频中最后一张图像的神经元响应开始到第一张图像的神经元响应。在第3节中，会清楚地看到信号传播学习信号从第一张图像的神经元响应传递到最后一张图像的神经元响应，与推断过程相同。
- en: Note, the inner problem of temporal credit assignment is spatial credit assignment.
    Temporal credit assignment takes the learning signal through each of the images
    making up the video. For each image, spatial credit assignment takes the learning
    signal to each neuron. Signal propagation gracefully addresses the outer problem
    by addressing the inner problem — a forward pass, by construction of the inference
    network, traverses through both problems.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，时间信用分配的内在问题是空间信用分配。时间信用分配将学习信号通过视频中每张图像。对于每张图像，空间信用分配将学习信号传递到每个神经元。信号传播优雅地通过解决内在问题来解决外在问题——前向传播，通过构建推断网络，遍历两个问题。
- en: BP does spatial credit assignment. BPT extends BP to do both spatial and temporal
    credit assignment. (Refer to Section 6 for a complete reading on spatial and temporal
    credit assignment.)
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: BP 进行空间信用分配。BPT 将 BP 扩展到同时进行空间和时间信用分配。（有关空间和时间信用分配的完整阅读，请参见第6节。）
- en: 3\. Learning in a Forward Pass
  id: totrans-73
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 3. 前向传播中的学习
- en: The Signal Propagation Framework (SP)
  id: totrans-74
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 信号传播框架 (SP)
- en: I present here, the Framework for Learning and Inference in a Forward Pass,
    called Signal Propagation (SP). It is a satisfyingly straightforward solution
    to temporal and spatial credit assignment. SP is a least constrained method for
    learning, and yet has better performance, efficiency, and compatibility than previous
    alternatives to backpropagation. It also has better efficiency and compatibility
    than backpropagation. SP provides a reasonable performance tradeoff for efficiency
    and compatibility. This is particularly appealing, considering it’s compatibility
    for target based deep learning (e.g. supervised and reinforcement) with new hardware
    and long-standing biological models, whereas previous works are not. (In general,
    backpropagation is the best performing algorithm.)
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 我在这里介绍的是一种前向传递学习和推理的框架，称为信号传播（SP）。这是一个令人满意的解决方案，用于时间和空间的信用分配。SP是一种约束最少的学习方法，其性能、效率和兼容性优于以前的反向传播替代方案。它还具有比反向传播更好的效率和兼容性。SP提供了一个合理的效率和兼容性性能折中。这特别吸引人，因为它兼容基于目标的深度学习（例如监督学习和强化学习），适用于新硬件和长期存在的生物模型，而以前的工作则不然。（一般来说，反向传播是表现最好的算法。）
- en: 'SP is free of constraints for learning to take place, with:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: SP在学习发生时没有约束，包括：
- en: only a forward pass, no backward pass
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 仅前向传递，没有反向传递
- en: no feedback connectivity or symmetric weights
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 无反馈连接或对称权重
- en: only one type of computation for learning and inference.
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 只有一种用于学习和推理的计算类型。
- en: a learning signal which travels with the input in the forward pass
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在前向传递过程中随输入传播的学习信号
- en: updates to parameters once the neuron/layer is reached by the forward pass
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在神经元/层被前向传递到达后，更新参数
- en: An interesting insight, SP provides an explanation for how neurons in the brain
    without error feedback connections receive global learning signals.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 一个有趣的见解是，SP为大脑中没有错误反馈连接的神经元如何接收全局学习信号提供了解释。
- en: 'As a result, Signal Propagation is:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，信号传播是：
- en: Compatible with models of learning in the brain and in hardware.
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与大脑和硬件中的学习模型兼容。
- en: More efficient in learning, with lower time and memory, and no additional structure.
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习更高效，时间和内存消耗较少，无需额外结构。
- en: A low complexity algorithm for learning.
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一种低复杂度的学习算法。
- en: 3.1\. How to Learn in a Forward Pass?
  id: totrans-87
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.1\. 如何在前向传递中学习？
- en: Signal Propagation treats the target as an additional input (figure below).
    With this approach, SP feeds the target forward through the network, as if it
    were an input.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 信号传播将目标视为额外的输入（见下图）。通过这种方法，SP将目标前向传递通过网络，就像它是一个输入一样。
- en: '![](../Images/2fc9566edfdeaaced4dec79e4c0081b5.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2fc9566edfdeaaced4dec79e4c0081b5.png)'
- en: Treat the Target as an Input. Images of animals from Upsplash.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 将目标视为输入。来自Upsplash的动物图像。
- en: SP moves forward through the network (figure below), bringing the target and
    the input closer and closer together, starting from the first layer (top left)
    all the way to the last layer (bottom right). Notice that by the last step/layer,
    the image of the dog is close to its target [1,0,0], and the image of the frog
    is close to its target [0, 1, 0]. However, the image and target of the dog is
    far away from the frog. This operation takes place in the representational space
    of the neurons at each layer. For instance, the neurons at layer 1 take in the
    dog picture (the input x) and dog label (the target c) and output activations
    h_1_dog and t_1_dog, respectively. The same happens for the frog producing h_1_frog
    and t_1_frog. In this activation space of the neurons, SP trains the network to
    bring an input and its target closer together, but farther away from other inputs
    and their respective targets.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: SP在网络中向前移动（见下图），将目标和输入越来越靠近，从第一层（左上）一直到最后一层（右下）。注意到在最后一步/层时，狗的图像接近其目标[1,0,0]，而青蛙的图像接近其目标[0,1,0]。然而，狗的图像和目标与青蛙的图像和目标之间仍有较大距离。这一操作发生在每层神经元的表示空间中。例如，第1层的神经元接收狗的图像（输入x）和狗的标签（目标c），分别输出激活h_1_dog和t_1_dog。青蛙的情况也一样，产生h_1_frog和t_1_frog。在这些神经元的激活空间中，SP训练网络使输入及其目标更加接近，同时与其他输入及其相应目标保持距离。
- en: '![](../Images/765308f6a617b0aa1e14d91ef71e4ce6.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/765308f6a617b0aa1e14d91ef71e4ce6.png)'
- en: Layer by layer, bring the target and its respective input closer together, but
    farther from the other input and target. Images of animals from Upsplash.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 层层推进，将目标及其相应输入逐渐靠近，但与其他输入和目标保持距离。来自Upsplash的动物图像。
- en: 3.2\. The Steps to Forward Learn
  id: totrans-94
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.2\. 前向学习的步骤
- en: Below is the total picture for an example three layer network. Each layer has
    its own loss function, which is used to update weights in the network. So, SP
    executes the loss function and updates the weights as soon as the target and label
    reach a layer. Since SP feeds the target and input together (alternating), layer/neuron
    weights are updated immediately. For spatial credit assignment, SP updates weights
    without waiting for the input to reach the last layer from the first layer. For
    temporal credit assignment, SP provides a learning signal for (each time step)
    each of the multiple connected inputs (e.g. images in a video), without waiting
    for the last input to be fed into the network.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一个示例三层网络的整体图。每层都有自己的损失函数，用于更新网络中的权重。因此，SP 执行损失函数并在目标和标签到达某层时立即更新权重。由于 SP
    将目标和输入一起（交替）输入，层/神经元权重会立即更新。对于空间信用分配，SP 在输入从第一层到达最后一层之前更新权重。对于时间信用分配，SP 为每个时间步长的多个连接输入（例如视频中的图像）提供学习信号，而无需等待最后一个输入被送入网络。
- en: '![](../Images/408fa98d5fb0686bbb6ebefd8c7b1d04.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/408fa98d5fb0686bbb6ebefd8c7b1d04.png)'
- en: This is a three layer network. The forward pass for learning and inference will
    proceed in three steps. Each layer has its own loss; a total of three losses.
    The inputs are x, and the targets are c, both fed in through the front of the
    network.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个三层网络。学习和推理的前向传播将分三步进行。每层有自己的损失，共有三种损失。输入是 x，目标是 c，二者通过网络的前端输入。
- en: '![](../Images/8abf9748100a86c52fd3dbb8833882ff.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8abf9748100a86c52fd3dbb8833882ff.png)'
- en: 'The overall algorithm for learning and inference in a forward pass. The inference
    and learning phases run in parallel, such that each layer’s weights are updated
    immediately. Notes: For the network shown (left), N = 3, the number of layers.
    The biases (b and d) are left out for clarity. There are many choices for a loss
    L (e.g. gradient, Hebbian) and optimizer (e.g. SGD, Momentum, ADAM). The output(),
    y, is detailed in step 4 below.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在前向传播中，学习和推理的整体算法。推理和学习阶段并行运行，每层的权重会立即更新。注：对于图示网络（左），N = 3，即层数。为了清晰起见，省略了偏置（b
    和 d）。损失 L（例如梯度、赫布式）和优化器（例如 SGD、Momentum、ADAM）有很多选择。输出函数 `output()`，y，在下面的步骤 4
    中详细说明。
- en: Below, we will go step by step, layer by layer, doing learning and inference
    (i.e. producing an answer/prediction) in forward passes. Note, in the guide below,
    the target and input are batch concatenated into a forward pass, making it easier
    to follow.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将逐步、逐层进行学习和推理（即产生答案/预测），以前向传播的方式进行。请注意，在下面的指南中，目标和输入被批量拼接到前向传播中，便于跟随。
- en: Step 1) Layer 1
  id: totrans-101
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 步骤 1) 层 1
- en: '![](../Images/7d10bca4be17a50bf2b58d5a2e817a74.png)![](../Images/c5178544f49038d50668cc0f2e3c017e.png)![](../Images/cd996664a9780955ea512fae60991367.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7d10bca4be17a50bf2b58d5a2e817a74.png)![](../Images/c5178544f49038d50668cc0f2e3c017e.png)![](../Images/cd996664a9780955ea512fae60991367.png)'
- en: Step 2) Layer 2
  id: totrans-103
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 步骤 2) 层 2
- en: '![](../Images/fdd99710ddb891abd5c6129c6bac2056.png)![](../Images/6b1d70d93fec9d8b8b3654bdfa77ac49.png)![](../Images/de57bb3c94560e8f574f96d461fbad42.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fdd99710ddb891abd5c6129c6bac2056.png)![](../Images/6b1d70d93fec9d8b8b3654bdfa77ac49.png)![](../Images/de57bb3c94560e8f574f96d461fbad42.png)'
- en: Step 3) Layer 3
  id: totrans-105
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 步骤 3) 层 3
- en: '![](../Images/f3e714c4560db6e591ea05daaa8a88a8.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f3e714c4560db6e591ea05daaa8a88a8.png)'
- en: Step 4) Prediction
  id: totrans-107
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 步骤 4) 预测
- en: At the output layer, there are three choices for outputting a prediction. The
    first and second options provide more flexibility and follow naturally from the
    procedure to train using a forward pass. The first option is to take an h_3 for
    a class and compare it with each t_3 for every class. For example, SP inputs the
    image of a dog and gets h_3_dog , then inputs the labels for all the classes and
    gets t_3_i = { t_3_dog, t_3_frog, and t_3_horse}, finally it compares h_3_dog
    with each of the t_3_i; the closes t_3_i is the correct class.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在输出层，有三种选择来输出预测结果。第一种和第二种选项提供了更多的灵活性，并自然地从使用前向传播的训练过程中跟随。第一种选项是取一个 h_3 作为一个类别，并与每个
    t_3 进行比较。例如，SP 输入一张狗的图像并得到 h_3_dog，然后输入所有类别的标签并得到 t_3_i = { t_3_dog, t_3_frog
    和 t_3_horse}，最后它将 h_3_dog 与每个 t_3_i 进行比较；最接近的 t_3_i 即为正确的类别。
- en: The second option is an adaptive version of the first option. It is adaptive
    since SP no longer compares h_3_dog with every t_3_i, instead finds a subset of
    closest t_3_i. For example, we maintain a tree where t_3_frog is closer in the
    tree to t_3_dog than t_3_horse. So, we first compare h_3_dog to t_3_frog, then
    to t_3_dog, and stop. We never compare with t_3_horse as it is too far away and
    not in our subset of closest t_3_i.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个选项是第一个选项的自适应版本。它是自适应的，因为SP不再将h_3_dog与每个t_3_i进行比较，而是寻找最近的t_3_i子集。例如，我们维护一个树结构，其中t_3_frog在树中比t_3_horse更接近t_3_dog。因此，我们首先将h_3_dog与t_3_frog进行比较，然后与t_3_dog进行比较，并停止。我们不会与t_3_horse进行比较，因为它距离太远，不在我们最近的t_3_i子集中。
- en: 'The third option: the classical and intuitive choice is to train a prediction
    output layer. This option is also more straightforward for regression and generative
    tasks. For example, a classification layer, which has has one output per class.
    So, layer 3 would be a classification layer. Note, that during inference t_3 is
    not longer used. In addition, notice that t_3_i is equivalent to having a classification
    layer. To see this, simply concatenate t_3_i together to form the weight matrix
    of a classification (prediction) layer that is taken with h_3 (e.g. h_3_dog, h_3_horse,
    …). This means that this third option is a special case of the first option, and
    can be a special case of the second option.'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 第三个选项：经典且直观的选择是训练一个预测输出层。这个选项在回归和生成任务中也更直接。例如，一个分类层，每个类别有一个输出。因此，第3层将是一个分类层。注意，在推断过程中t_3不再使用。此外，注意到t_3_i等同于一个分类层。要看到这一点，只需将t_3_i连接在一起，形成一个分类（预测）层的权重矩阵，与h_3（例如h_3_dog，h_3_horse，…）一起使用。这意味着第三个选项是第一个选项的特例，并且可以是第二个选项的特例。
- en: '![](../Images/e817c4b00d5cb79479ae00064f78ec2d.png)![](../Images/6147fb06425c5c7e2ad95452f08e3b8b.png)![](../Images/73ca5ea2bb547d45298033cc3ee6b0fb.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e817c4b00d5cb79479ae00064f78ec2d.png)![](../Images/6147fb06425c5c7e2ad95452f08e3b8b.png)![](../Images/73ca5ea2bb547d45298033cc3ee6b0fb.png)'
- en: 3.3\. Overview of the Complete Procedure
  id: totrans-112
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.3\. **完整过程概述**
- en: '![](../Images/e992fb971fe17fd5af8af3572ad5acff.png)![](../Images/408fa98d5fb0686bbb6ebefd8c7b1d04.png)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e992fb971fe17fd5af8af3572ad5acff.png)![](../Images/408fa98d5fb0686bbb6ebefd8c7b1d04.png)'
- en: 3.4\. Spiking Networks
  id: totrans-114
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.4\. **尖峰网络**
- en: Spiking neural networks are similar to biological neural networks. They are
    used in models of learning in the brain. They are also used for neuromorphic chips.
    There are two problems for learning in spiking neural networks. First, the learning
    constraints under backpropagation are difficult to reconcile with learning in
    the brain, and hinders efficient implementations of learning algorithms on hardware
    (discussed above). Second, training spiking networks results in the dead neuron
    problem (see below).
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 尖峰神经网络类似于生物神经网络。它们被用于大脑学习模型中，也用于神经形态芯片。尖峰神经网络在学习中存在两个问题。首先，反向传播下的学习约束与大脑学习难以协调，这阻碍了在硬件上高效实现学习算法（如上所述）。其次，训练尖峰网络会导致死亡神经元问题（见下文）。
- en: A reference figure is provided below. The neurons in these networks respond
    to inputs by either activating (spiking) to convey information to another neuron
    or by doing nothing (top-left figure). Commonly, these networks have a problem
    where neurons never activate, which means they never spike (bottom-left figure).
    Thereby, regardless of the input, the neurons response is to always do nothing.
    This is called the dead neuron problem.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 下面提供了一个参考图。这些网络中的神经元通过激活（尖峰）来响应输入，以将信息传递给另一个神经元，或者什么都不做（左上图）。通常，这些网络存在一个问题，即神经元从不激活，这意味着它们从不尖峰（左下图）。因此，无论输入是什么，神经元的响应始终是无反应。这被称为死亡神经元问题。
- en: The most popular approach to resolve this problem uses a surrogate function
    to replace the spiking behavior of the neurons. The network uses the surrogate
    only during learning, when the learning signal is sent to the neurons. The surrogate
    function (blue) provides a value for the neuron even when it does not spike (top-right
    figure). So, the neuron learns even when it does not spike to convey information
    to another neuron (bottom-right figure). This helps stop the neuron from dying.
    However, surrogates are difficult to implement for learning in hardware, such
    as neuromorphic chips. Furthermore, surrogates do not fit models of learning in
    the brain.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 解决这个问题的最流行方法是使用替代函数来替代神经元的脉冲行为。网络仅在学习期间使用替代品，即当学习信号发送到神经元时。替代函数（蓝色）即使在神经元不脉冲时也为神经元提供值（右上图）。因此，即使神经元没有脉冲向另一个神经元传递信息，它仍能学习（右下图）。这有助于防止神经元的“死亡”。然而，替代品在硬件学习中，如神经形态芯片中难以实现。此外，替代品不适合大脑中的学习模型。
- en: Signal Propagation provides two solutions that are compatible with models of
    learning in the brain and in hardware.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 信号传播提供了两种与大脑和硬件学习模型兼容的解决方案。
- en: '![](../Images/6063981352c8703c09e2a56117084270.png)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6063981352c8703c09e2a56117084270.png)'
- en: Below is a visualization of the learning signal (colored in red) going through
    a spiking neuron (shown as S), past the voltage or membrane potential (U), to
    update the weights (W). Backpropagation, with the dead neuron problem, is on the
    left. Backpropagation, with a surrogate function (f), is second from the left.
    The learning signal for backpropagation is global (L_G) and comes from the last
    layer of the network; the dotted boxes are upper neurons/layers.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是学习信号（标记为红色）通过一个脉冲神经元（标记为S）、穿过电压或膜电位（U），以更新权重（W）的可视化。左侧是带有“死神经元”问题的反向传播。左二是带有替代函数（f）的反向传播。反向传播的学习信号是全局的（L_G），来自网络的最后一层；虚线框表示上层神经元/层。
- en: The other images on the right show the two solutions Signal Propagation (SP)
    provides. First, SP can use a surrogate as well, but the learning signal does
    not go through the spiking equation (S). Instead, the learning signal is before
    the spiking equation (S), directly attached to the surrogate function (f). As
    a result, SP is more compatible with learning in the brain, such as in a multi
    compartment model of a biological neuron. Second, SP can learn using only the
    voltage or membrane potential (U). In this case, the learning signal is directly
    attached to U. This requires no surrogate or change to the neuron. Thereby, SP
    provides compatibility with learning in hardware.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 右侧的其他图像展示了信号传播（SP）提供的两种解决方案。首先，SP也可以使用替代品，但学习信号不经过脉冲方程（S）。相反，学习信号在脉冲方程（S）之前，直接附加到替代函数（f）上。因此，SP与大脑中的学习更兼容，例如在生物神经元的多室模型中。其次，SP可以仅使用电压或膜电位（U）进行学习。在这种情况下，学习信号直接附加到U上。这不需要替代品或对神经元的更改。因此，SP与硬件中的学习兼容。
- en: '![](../Images/2d78d2946f5f5fef77b8eb64e13e90ed.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2d78d2946f5f5fef77b8eb64e13e90ed.png)'
- en: 4\. Works on Forward Learning
  id: totrans-123
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 4\. 前向学习的研究工作
- en: A list of works on forward learning - using the forward pass for learning. Works
    are ordered by date.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 关于前向学习的工作列表 - 使用前向传播进行学习。工作按日期排序。
- en: A repository webpage is maintained for the community to document Forward Learning
    methods, located here [https://amassivek.github.io/sigprop](https://amassivek.github.io/sigprop)
    . The code library is available at [https://github.com/amassivek/signalpropagation](https://github.com/amassivek/signalpropagation)
    .
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 一个社区维护的仓库网页用于记录前向学习方法，位于 [https://amassivek.github.io/sigprop](https://amassivek.github.io/sigprop)
    。代码库可在 [https://github.com/amassivek/signalpropagation](https://github.com/amassivek/signalpropagation)
    获取。
- en: 4.1\. Error Forward Propagation Algorithm (2018)
  id: totrans-126
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.1\. 误差前向传播算法（2018）
- en: The error forward propagation algorithm is an implementation of the signal propagation
    framework for learning and inference in a forward pass (figure below). Under signal
    propagation, S is the transform of the context c, which for supervised learning
    is the target.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 误差前向传播算法是信号传播框架在前向传播中的实现（如下图）。在信号传播下，S是上下文c的变换，对于监督学习来说，c是目标。
- en: In error forward propagation, S is the projection of the error from the output
    to the front of the network, as shown in the figure below.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 在误差前向传播中，S是从输出到网络前端的误差投影，如下图所示。
- en: '![](../Images/5e88aec10f836b620d931ced3c41ca94.png)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5e88aec10f836b620d931ced3c41ca94.png)'
- en: Error Forward Propagation Algorithm. Image of 7 from [MNIST dataset](http://yann.lecun.com/exdb/mnist/).
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 错误前向传播算法。来自 [MNIST 数据集](http://yann.lecun.com/exdb/mnist/) 的图像 7。
- en: 'Error Forward-Propagation: Reusing Feedforward Connections to Propagate Errors
    in Deep Learning'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 错误前向传播：重用前馈连接在深度学习中传播错误
- en: '[https://arxiv.org/abs/1808.03357](https://arxiv.org/abs/1808.03357)'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://arxiv.org/abs/1808.03357](https://arxiv.org/abs/1808.03357)'
- en: 4.2\. Forward Forward Algorithm (2022)
  id: totrans-133
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.2\. 前向前向算法 (2022)
- en: The forward forward algorithm is an implementation of the signal propagation
    framework for learning and inference in a forward pass (figure below). Under signal
    propagation, S is the transform of the context c, which for supervised learning
    is the target.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 前向前向算法是信号传播框架在前向传递中的实现（见下图）。在信号传播下，S 是上下文 c 的变换，对于监督学习来说，这是目标。
- en: In forward forward, S is a concatenation of the target c with the input x, as
    shown in the figure below.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 在前向前向中，S 是目标 c 和输入 x 的连接，如下图所示。
- en: '![](../Images/dde3a141fe9cfec6e48aa88d8123d3a9.png)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/dde3a141fe9cfec6e48aa88d8123d3a9.png)'
- en: Forward Forward Algorithm. Image of 7 from [MNIST dataset](http://yann.lecun.com/exdb/mnist/).
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 前向前向算法。来自 [MNIST 数据集](http://yann.lecun.com/exdb/mnist/) 的图像 7。
- en: Forward Forward Algorithm
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 前向前向算法
- en: '[https://www.cs.toronto.edu/~hinton/FFA13.pdf](https://www.cs.toronto.edu/~hinton/FFA13.pdf)'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://www.cs.toronto.edu/~hinton/FFA13.pdf](https://www.cs.toronto.edu/~hinton/FFA13.pdf)'
- en: 5\. Reading Material
  id: totrans-140
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 5\. 阅读材料
- en: 'Signal Propagation: The Framework for Learning and Inference In a Forward Pass'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 信号传播：前向传递中的学习与推理框架
- en: '[https://arxiv.org/abs/2204.01723](https://arxiv.org/abs/2204.01723) (2022)'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://arxiv.org/abs/2204.01723](https://arxiv.org/abs/2204.01723) (2022)'
- en: Forward Forward Algorithm
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 前向前向算法
- en: '[https://www.cs.toronto.edu/~hinton/FFA13.pdf](https://www.cs.toronto.edu/~hinton/FFA13.pdf)
    (2022)'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://www.cs.toronto.edu/~hinton/FFA13.pdf](https://www.cs.toronto.edu/~hinton/FFA13.pdf)
    (2022)'
- en: 'Error Forward-Propagation: Reusing Feedforward Connections to Propagate Errors
    in Deep Learning'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 错误前向传播：重用前馈连接在深度学习中传播错误
- en: '[https://arxiv.org/abs/1808.03357](https://arxiv.org/abs/1808.03357) (2018)'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://arxiv.org/abs/1808.03357](https://arxiv.org/abs/1808.03357) (2018)'
- en: 5.1 Other Material
  id: totrans-147
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.1 其他材料
- en: 'A well written guide on spatial and temporal credit assignment. I referenced
    it to help write the “Appendix: Reading on Credit Assignment”.'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 一份关于空间和时间信贷分配的良好指南。我参考了它来帮助编写“附录：信贷分配阅读”。
- en: Training Spiking Neural Networks using lessons from deep learning
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 使用深度学习的经验训练尖峰神经网络
- en: '[https://arxiv.org/abs/2109.12894](https://arxiv.org/abs/2109.12894) (2021)'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://arxiv.org/abs/2109.12894](https://arxiv.org/abs/2109.12894) (2021)'
- en: A repository webpage is maintained for the community to document Forward Learning
    methods, located here [https://amassivek.github.io/sigprop](https://amassivek.github.io/sigprop)
    .
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 社区维护了一个代码库网页来记录前向学习方法，位于 [https://amassivek.github.io/sigprop](https://amassivek.github.io/sigprop)。
- en: The code library is available at [https://github.com/amassivek/signalpropagation](https://github.com/amassivek/signalpropagation).
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 代码库可以在 [https://github.com/amassivek/signalpropagation](https://github.com/amassivek/signalpropagation)
    上获取。
- en: 'With Thanks to: Alexandra Marmarinos for her editing work and guidance.'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 感谢：Alexandra Marmarinos 的编辑工作和指导。
- en: '6\. Appendix: Reading on Credit Assignment'
  id: totrans-154
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 6\. 附录：信贷分配阅读
- en: 6.1\. Spatial Credit Assignment
  id: totrans-155
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.1\. 空间信贷分配
- en: 'Spatial Locality of Credit Assignment is the question: How does the learning
    signal reach every neuron?'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 空间信贷分配的问题是：学习信号如何到达每一个神经元？
- en: 'On the left of the figure below, is a three layer network. In general, learning
    takes place over two phases: the inference phase and the learning phase. In the
    first phase, called the inference phase, the input is fed through the network
    from the first layer up to the last layer. Since the input is fed forward through
    the network, the inference phase takes place during the “forward pass” through
    the network. In the second phase, called the learning phase, the learning signal
    (colored in red) needs to reach every neuron in this network.'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 在下图的左侧，是一个三层网络。一般来说，学习分为两个阶段：推理阶段和学习阶段。在第一个阶段，称为推理阶段，输入从第一层传递到最后一层。由于输入通过网络前向传递，因此推理阶段发生在网络的“前向传递”过程中。在第二阶段，称为学习阶段，学习信号（标记为红色）需要到达网络中的每一个神经元。
- en: Different learning algorithms have different solutions to the learning phase.
    In backpropagation, the learning signal goes backward through the network, so
    the learning phase takes place during the “backward pass” through the network.
    As we will see with Signal Propagation, learning can take place during the forward
    pass as well.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 不同的学习算法在学习阶段有不同的解决方案。在反向传播中，学习信号通过网络向后传播，因此学习阶段发生在通过网络的“反向传递”中。正如我们将在信号传播中看到的，学习也可以在前向传递中进行。
- en: Broadly, there are two approaches to the learning phase. The first approach
    computes a global learning signal (left middle figure) and then sends this learning
    signal to every neuron. The second approach computes a local learning signal (right
    figure) at each neuron (or layer). The first approach has the problem of having
    to coordinate sending this signal to every neuron in a precise way. This is costly
    in time, memory, and compatibility. The second approach does not encounter this
    problem, but has worse performance.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 广泛来说，学习阶段有两种方法。第一种方法计算全局学习信号（左中图），然后将此学习信号发送到每个神经元。第二种方法在每个神经元（或层）计算局部学习信号（右图）。第一种方法的问题在于需要以精确的方式协调将此信号发送到每个神经元。这在时间、内存和兼容性上都很昂贵。第二种方法没有遇到这个问题，但性能较差。
- en: '![](../Images/a386c471774bfbe111f7dbc3daa182a2.png)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a386c471774bfbe111f7dbc3daa182a2.png)'
- en: 6.2\. Temporal Credit Assignment
  id: totrans-161
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.2\. 时间信用分配
- en: 'Temporal Locality of Credit Assignment is the question: How does the global
    learning signal reach multiple connected inputs (aka every time step)?'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 信贷分配的时间局部性问题是：全局学习信号如何到达多个连接的输入（即每个时间步）？
- en: 'A single image requires only that the learning signal reach every neuron. However,
    a video is a series of connected images. So, now the learning signal needs to
    travel through multiple connected inputs (aka time), starting from the last image
    in the video all the way to the first image in the video. This concept applies
    to any sequential or time series data. So, how does the global learning signal
    reach every time step? There are two popular methods to answer this question:
    Backpropagation through time, and forward mode differentiation.'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 单张图像只需要学习信号到达每个神经元。然而，视频是一系列连接的图像。因此，学习信号需要通过多个连接的输入（即时间）进行传播，从视频中的最后一张图像一直到视频中的第一张图像。这一概念适用于任何序列或时间序列数据。那么，全局学习信号如何到达每个时间步？有两种流行的方法来回答这个问题：时间上的反向传播和前向模式微分。
- en: 6.2.1\. Backpropagation Through Time (BPT)
  id: totrans-164
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.2.1\. 时间上的反向传播（BPT）
- en: The primary answer to the question posed above follows, and takes place in two
    phases. First, input all the images that make up the video, one by one, into the
    network. This is the inference phase where the multiple connected inputs are sent
    forward through the network (a forward pass). Second, go backwards from the last
    image to the first image propagating the learning signal. This is the learning
    phase where the learning signal goes backward (a backward pass) through the multiple
    connected inputs (aka time); thus the name backpropagation through time.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 对上述问题的主要回答如下，并分为两个阶段。首先，将构成视频的所有图像逐一输入网络。这是推断阶段，在这个阶段，多个连接的输入通过网络向前传递（前向传递）。其次，从最后一张图像开始向回传播学习信号直到第一张图像。这是学习阶段，在这个阶段，学习信号在多个连接的输入（即时间）中向后传播；因此，称为时间上的反向传播。
- en: 'Step 1: Inference'
  id: totrans-166
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第一步：推断
- en: In the figure below, BPT feeds each image X[i] (e.g. of the turtle walking),
    which makes up the video, through the network. BPT starts with the 1st image X[0]
    (bottom left of the first figure), which is time step 1 (time is shown at the
    top of the figure). Next, BPT feeds in image X[1], which is time step 2\. Finally,
    we end with the last image X[2] at time step 3 — this demonstration is for a very
    short video, or gif. Every time BPT feeds an image to the network notice that
    the middle layer in the network connects each image to the next image through
    time.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 在下图中，BPT将构成视频的每一张图像X[i]（例如乌龟走路的图像）输入网络。BPT从第1张图像X[0]（第一张图像的左下角）开始，这是时间步长1（时间显示在图的顶部）。接着，BPT输入图像X[1]，这是时间步长2。最后，我们以时间步长3的最后一张图像X[2]结束——这个演示用于非常短的视频或GIF。每次BPT将图像输入网络时，请注意网络中的中间层将每张图像通过时间连接到下一张图像。
- en: '![](../Images/2a6b9a826f6d1959705d165fccd79c2f.png)'
  id: totrans-168
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2a6b9a826f6d1959705d165fccd79c2f.png)'
- en: 'Step 2: Learning Backward through time'
  id: totrans-169
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第二步：通过时间进行学习
- en: BPT feeds the learning signal, colored in red, backward through the images (time),
    making up the video of the turtle walking. The learning signal is formed from
    the loss function (top right of figure). It travels in the opposite direction
    of how we fed in the images X[i]. First a gradient/update is calculated for image
    X[2] at time 3, then image X[1] at time 2, and finally image X[0] at time 1\.
    This is why it is called backpropagation through time. Again, notice that the
    middle layer in the network connects the learning signal from the last image X[2]
    to the first image X[0].
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: BPT将学习信号（用红色标记）从图像（时间）中向后传播，形成了乌龟行走的视频。学习信号是从损失函数（图中的右上方）形成的。它的传播方向与我们输入图像X[i]的方向相反。首先计算时间3的图像X[2]的梯度/更新，然后是时间2的图像X[1]，最后是时间1的图像X[0]。这就是为什么它被称为时间上的反向传播。再次注意，网络中的中间层将来自最后一张图像X[2]的学习信号连接到第一张图像X[0]。
- en: '![](../Images/aff1d694a7b664ae2b3b6b304d537f76.png)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/aff1d694a7b664ae2b3b6b304d537f76.png)'
- en: 6.2.2\. Forward Mode Differentiation (FMD)
  id: totrans-172
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.2.2\. 正向模式微分（FMD）
- en: 'Under FMD, the behavior of the inference (step 1) and learning (step 2) phases
    are similar to each other. As a result, FMD does step 1 (inference) and step 2
    (learning) together (alternating). How? In step 2, FMD propagates the learning
    signal forward through the images (time), much the same as inference does in step
    1\. So, the learning signal no longer needs to travel from the last image X[3]
    in the video back to the first X[0]. The result: FMD has a learning signal that
    starts with X[0], instead of having to wait for X[3].'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 在FMD下，推断（第1步）和学习（第2步）阶段的行为是类似的。因此，FMD将第1步（推断）和第2步（学习）一起进行（交替）。怎么做？在第2步中，FMD将学习信号向前传播通过图像（时间），这与第1步中的推断过程非常相似。因此，学习信号不再需要从视频中的最后一张图像X[3]返回到第一张图像X[0]。结果是：FMD的学习信号从X[0]开始，而不需要等待X[3]。
- en: Why FMD vs BPT? Above, I discussed the learning constraints under backpropagation
    and the problems it has with efficiency and compatibility. FMD attempts to improve
    efficiency. Particularly, BPT feeds all of the images, making up the video, into
    the network before learning. FMD does not, so it is more efficient in time than
    BPT. However, FMD is significantly more costly than BPT, particularly in memory
    and computation. Note that FMD addresses time. However, it does not help with
    the learning constraints on spatial credit assignment found under backpropagation,
    which exist in FMD as well.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么选择FMD而不是BPT？上面我讨论了在反向传播（backpropagation）下学习的限制以及它在效率和兼容性方面存在的问题。FMD尝试提高效率。特别是，BPT会在学习之前将所有构成视频的图像输入到网络中。而FMD则不会，因此在时间上比BPT更高效。然而，FMD在成本上明显高于BPT，特别是在内存和计算方面。注意，FMD解决了时间上的问题。然而，它并没有解决反向传播下空间信用分配的学习限制，这在FMD中也存在。
- en: '![](../Images/003081cfcb5e5ffd6935788b92da960c.png)![](../Images/bfbbf05ce62d1c63fdd6f92040c8683b.png)![](../Images/06c556b25d352b89599c82116278443b.png)'
  id: totrans-175
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/003081cfcb5e5ffd6935788b92da960c.png)![](../Images/bfbbf05ce62d1c63fdd6f92040c8683b.png)![](../Images/06c556b25d352b89599c82116278443b.png)'
- en: All images unless otherwise noted are by the author.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 除非另有说明，否则所有图片均由作者提供。
