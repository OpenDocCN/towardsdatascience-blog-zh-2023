- en: Comprehensive Guide to Ranking Evaluation Metrics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/comprehensive-guide-to-ranking-evaluation-metrics-7d10382c1025?source=collection_archive---------0-----------------------#2023-07-29](https://towardsdatascience.com/comprehensive-guide-to-ranking-evaluation-metrics-7d10382c1025?source=collection_archive---------0-----------------------#2023-07-29)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Explore an abundant choice of metrics and find the best one for your problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@slavahead?source=post_page-----7d10382c1025--------------------------------)[![Vyacheslav
    Efimov](../Images/db4b02e75d257063e8e9d3f1f75d9d6d.png)](https://medium.com/@slavahead?source=post_page-----7d10382c1025--------------------------------)[](https://towardsdatascience.com/?source=post_page-----7d10382c1025--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----7d10382c1025--------------------------------)
    [Vyacheslav Efimov](https://medium.com/@slavahead?source=post_page-----7d10382c1025--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fc8a0ca9d85d8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcomprehensive-guide-to-ranking-evaluation-metrics-7d10382c1025&user=Vyacheslav+Efimov&userId=c8a0ca9d85d8&source=post_page-c8a0ca9d85d8----7d10382c1025---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----7d10382c1025--------------------------------)
    ·13 min read·Jul 29, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F7d10382c1025&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcomprehensive-guide-to-ranking-evaluation-metrics-7d10382c1025&user=Vyacheslav+Efimov&userId=c8a0ca9d85d8&source=-----7d10382c1025---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F7d10382c1025&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcomprehensive-guide-to-ranking-evaluation-metrics-7d10382c1025&source=-----7d10382c1025---------------------bookmark_footer-----------)![](../Images/9590e4ba65d5707f16a4ef8cc8b6d58c.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Ranking is a problem in machine learning where the objective is to sort a list
    of documents for an end user in the most suitable way, so the most relevant documents
    appear on top. Ranking appears in several domains of data science, starting from
    recommender systems where an algorithm suggests a set of items for purchase and
    ending up with NLP search engines where by a given query, the system tries to
    return the most relevant search results.
  prefs: []
  type: TYPE_NORMAL
- en: The question which arises naturally is how to estimate the quality of a ranking
    algorithm. As in classical machine learning, there does not exist a single universal
    metric that would be suitable for any type of task. Why? Simply because every
    metric has its own application scope which depends on the nature of a given problem
    and data characteristics.
  prefs: []
  type: TYPE_NORMAL
- en: That is why it is crucial to be aware of all the main metrics to successfully
    tackle any machine learning problem. This is exactly what we are going to do in
    this article.
  prefs: []
  type: TYPE_NORMAL
- en: Nevertheless, before going ahead let us understand why certain popular metrics
    should not be normally used for ranking evaluation. By taking this information
    into consideration, it will be easier to understand the necessity of the existence
    of other, more sophisticated metrics.
  prefs: []
  type: TYPE_NORMAL
- en: '*Note*. The article and used formulas are based on the presentation on [offline
    evaluation from Ilya Markov](https://drive.google.com/drive/folders/19OfEsLME1IR7bGPzVo8Dh31emcPNiaP_).'
  prefs: []
  type: TYPE_NORMAL
- en: Metrics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There are several types of information retrieval metrics that we are going
    to discuss in this article:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ed86835c8e52a6a2461e7378d45bbff6.png)'
  prefs: []
  type: TYPE_IMG
- en: Different types of metrics
  prefs: []
  type: TYPE_NORMAL
- en: Unranked metrics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Imagine a recommender system predicting ratings of movies and showing the most
    relevant films to users. Rating usually represents a positive real number. At
    first sight, a regression metric like *MSE* (*RMSE, MAE*, etc.) seems a reasonable
    choice to evaluate the quality of the system on a hold-out dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '*MSE* takes all the predicted films into consideration and measures the average
    square error between true and predicted labels. However, end users are usually
    interested only in the top results which appear on the first page of a website.
    This indicates that they are not really interested in films with lower ratings
    appearing at the end of the search result which are also equally estimated by
    standard regression metrics.'
  prefs: []
  type: TYPE_NORMAL
- en: A simple example below demonstrates a pair of search results and measures the
    *MSE* value in each of them.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6d98469d260e85bf22d7b32a9823e045.png)'
  prefs: []
  type: TYPE_IMG
- en: Error estimation for both queries shows that MSE is a bad metric for ranking.
    Green documents are relevant while red documents are irrelevant. The list of documents
    is shown in the order of predicted relevance (from left to right).
  prefs: []
  type: TYPE_NORMAL
- en: 'Though the second search result has a lower *MSE*, the user will not be satisfied
    with such a recommendation. By first looking only at non-relevant items, the user
    will have to scroll up all the way down to find the first relevant item. That
    is why from the user experience perspective, the first search result is much better:
    the user is just happy with the top item and proceeds to it while not caring about
    others.'
  prefs: []
  type: TYPE_NORMAL
- en: The same logic goes with classification metrics (*precision*, *recall*) which
    consider all items as well.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/59d4c5db282399ff7a57603488e7dfb9.png)'
  prefs: []
  type: TYPE_IMG
- en: Precision and recall formulas
  prefs: []
  type: TYPE_NORMAL
- en: What do all of described metrics have in common? All of them treat all items
    equally and do not consider any differentiation between high and low-relevant
    results. That is why they are called **unranked**.
  prefs: []
  type: TYPE_NORMAL
- en: 'By having gone through these two similar problematic examples above, the aspect
    we should focus on while designing a ranking metric seems more clear:'
  prefs: []
  type: TYPE_NORMAL
- en: A ranking metric should put more weight on more relevant results while lowering
    or ignoring the less relevant ones.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Ranked metrics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Kendall Tau distance
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Kendall Tau distance](https://en.wikipedia.org/wiki/Kendall_tau_distance#:~:text=The%20Kendall%20tau%20rank%20distance,dissimilar%20the%20two%20lists%20are.)
    is based on the number of rank inversions.'
  prefs: []
  type: TYPE_NORMAL
- en: An **invertion** is a pair of documents (i, j) such as document i having a greater
    relevance than document j, appears after on the search result than j.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Kendall Tau distance calculates all the number of inversions in the ranking.
    The lower the number of inversions, the better the search result is. Though the
    metric might look logical, it still has a downside which is demonstrated in the
    example below.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a684a7b0cdb7d952f4199f8c4bb14aea.png)'
  prefs: []
  type: TYPE_IMG
- en: Despite fewer number of inversions, the second ranking is still worse, from
    the user perspective
  prefs: []
  type: TYPE_NORMAL
- en: It seems like the second search result is better with only 8 inversions versus
    9 in the first one. Similarly to the *MSE* example above, the user is only interested
    in the first relevant result. By going through several non-relevant search results
    in the second case, the user experience will be worse than in the first case.
  prefs: []
  type: TYPE_NORMAL
- en: Precision@k & Recall@k
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Instead of usual *precision* and *recall*, it is possible to consider only at
    a certain number of top recommendations *k*. This way, the metric does not care
    about low-ranked results. Depending on the chosen value of *k*, the corresponding
    metrics are denoted as *precision@k* (*“precision at k”*) and *recall@k* (*“recall
    at k”*) respectively. Their formulas are shown below.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9686d387d426e71d6758a4479effeb6b.png)'
  prefs: []
  type: TYPE_IMG
- en: precision@k and recall@k formulas
  prefs: []
  type: TYPE_NORMAL
- en: Imagine top *k* results are shown to the user where each result can be relevant
    or not. *precision@k* measures the percentage of relevant results among top *k*
    results. At the same time, *recall@k* evaluates the ratio of relevant results
    among top *k* to the total number of relevant items in the whole dataset.
  prefs: []
  type: TYPE_NORMAL
- en: To better understand the calculation process of these metrics, let us refer
    to the example below.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/383ebf3b1628d208bd32aa2141000c4d.png)'
  prefs: []
  type: TYPE_IMG
- en: precision@k and recall@k calculation example. Green documents represent relevant
    items while the red ones correspond to irrelevant ones.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are 7 documents in the system (named from *A* to *G*). Based on its predictions,
    the algorithm chooses *k = 5* documents among them for the user. As we can notice,
    there are 3 relevant documents *(A, C, G)* among top *k = 5* which results in
    *precision@5* being equal to *3 / 5*. At the same time, *recall@5* takes into
    account relevant items in the whole dataset: there are 4 of them *(A, C, F* and
    *G)* making r*ecall@5 = 3 / 4*.'
  prefs: []
  type: TYPE_NORMAL
- en: '*recall@k* always increases with the growth of *k* making this metric not really
    objective in some scenarios. In the edge case where all the items in the system
    are shown to the user, the value of *recall@k* equals 100%. *precision@k* does
    not have the same monotonic property as *recall@k* has as it measures the ranking
    quality in relation to top *k* results, not in relation to the number of relevant
    items in the whole system. Objectivity is one of the reasons *precision@k* is
    usually a preferred metric over *recall@k* in practice.'
  prefs: []
  type: TYPE_NORMAL
- en: AP@k (Average Precision) & MAP@k (Mean Average Precision)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The problem with vanilla *precision@k* is that it does not take into account
    the order of relevant items appearing among retrieved documents. For example,
    if there are 10 retrieved documents with 2 of them being relevant, *precision@10*
    will always be the same despite the location of these 2 documents among 10\. For
    instance, if the relevant items are located in positions *(1, 2)* or *(9, 10)*,
    the metric does differentiate both of these cases resulting in *precision@10*
    being equal to 0.2.
  prefs: []
  type: TYPE_NORMAL
- en: However, in real life, the system should give a higher weight to relevant documents
    ranked on the top rather than on the bottom. This issue is solved by another metric
    called ***average precision*** *(****AP****)*. As a normal *precision*, *AP* takes
    values between 0 and 1.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/bf4a3d8468dd45ddfa6c939bd0450003.png)'
  prefs: []
  type: TYPE_IMG
- en: Average precision formula
  prefs: []
  type: TYPE_NORMAL
- en: '*AP@k* calculates the average value of *precision@i* for all values of *i*
    from 1 to *k* for those of which the *i*-th document is relevant.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f9d8f96c85e626f45184ddd9311bd99c.png)'
  prefs: []
  type: TYPE_IMG
- en: Average precision computed for two queries
  prefs: []
  type: TYPE_NORMAL
- en: 'In the figure above, we can see the same 7 documents. The response to the query
    *Q₁* resulted in *k* = 5 retrieved documents where 3 relevant documents are positioned
    at indexes *(1, 3, 4)*. For each of these positions *i*, *precision@i* is calculated:'
  prefs: []
  type: TYPE_NORMAL
- en: '*precision@1 = 1 / 1*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*precision@3 = 2 / 3*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*precision@4 = 3 / 4*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'All other mismatched indexes *i* are ignored. The final value of *AP@5* is
    computed as an average over the precisions above:'
  prefs: []
  type: TYPE_NORMAL
- en: '*AP@5 = (precision@1 + precision@3 + precision@4) / 3 = 0.81*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For comparison, let us look at the response to another query *Q₂* which also
    contains 3 relevant documents among top *k*. Nevertheless, this time, 2 irrelevant
    documents are located higher in the top (at positions *(1, 3)*) than in the previous
    case which results in lower *AP@5* being equal to 0.53.
  prefs: []
  type: TYPE_NORMAL
- en: 'Sometimes there is a need to evaluate the quality of the algorithm not on a
    single query but on multiple queries. For that purpose, the **mean average precision
    (*MAP*)** is utilised. Is is simply takes the mean of *AP* among several queries
    *Q*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c8b4eaac9c7b671e0c16707a93edc155.png)'
  prefs: []
  type: TYPE_IMG
- en: Mean average precision formula
  prefs: []
  type: TYPE_NORMAL
- en: 'The example below shows how *MAP* is calculated for 3 different queries:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/091d1dcf08fbe3ed72a37866e27c8c28.png)'
  prefs: []
  type: TYPE_IMG
- en: AP and MAP computed for three queries
  prefs: []
  type: TYPE_NORMAL
- en: RR (Reciprocal Rank) & MRR (Mean Reciprocal Rank)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Sometimes users are interested only in the first relevant result. Reciprocal
    rank is a metric which returns a number between 0 and 1 indicating how far from
    the top the first relevant result is located: if the document is located at position
    *k*, then the value of *RR* is *1 / k*.'
  prefs: []
  type: TYPE_NORMAL
- en: Similarly to *AP* and *MAP*, ***mean reciprocal rank (MRR)*** measures the average
    *RR* among several queries.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ffb00e6581390162da75e400448751ea.png)'
  prefs: []
  type: TYPE_IMG
- en: RR and MRR formulas
  prefs: []
  type: TYPE_NORMAL
- en: 'The example below shows how *RR* and *MRR* are computed for 3 queries:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5e02fd3992f114c0a58a885e604f943c.png)'
  prefs: []
  type: TYPE_IMG
- en: RR and MRR computed for three queries
  prefs: []
  type: TYPE_NORMAL
- en: User-oriented metrics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Though ranked metrics consider ranking positions of items thus being a preferable
    choice over the unranked ones, they still have a significant downside: the information
    about user behaviour is not taken into account.'
  prefs: []
  type: TYPE_NORMAL
- en: User-oriented approaches make certain assumptions about user behaviour and based
    on it, produce metrics that suit ranking problems better.
  prefs: []
  type: TYPE_NORMAL
- en: DCG (Discounted Cumulative Gain) & nDCG (Normalized Discounted Cumulative Gain)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The DCG metric usage is based on the following assumption:'
  prefs: []
  type: TYPE_NORMAL
- en: Highly relevant documents are more useful when appearing earlier in a search
    engine result list (have higher ranks) — [Wikipedia](https://en.wikipedia.org/wiki/Discounted_cumulative_gain)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: This assumption naturally represents how users evaluate higher search results,
    compared to those presented lower.
  prefs: []
  type: TYPE_NORMAL
- en: 'In *DCG*, each document is assigned a gain which indicates how relevant a particular
    document is. Given a true relevance *Rᵢ* (real value) for every item, there exist
    several ways to define a gain. One of the most popular is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6a71a47d9c564fc4d55b2a5eed8b59e3.png)'
  prefs: []
  type: TYPE_IMG
- en: Possible gain formula in DCG
  prefs: []
  type: TYPE_NORMAL
- en: 'Basically, the exponent puts a strong emphasis on relevant items. For example,
    if a rating of a movie is assigned an integer between 0 and 5, then each film
    with a corresponding rating will approximatively have double importance, compared
    to a film with the rating reduced by 1:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7ecafe0a114225e95d136332d1942da1.png)'
  prefs: []
  type: TYPE_IMG
- en: Gain function of relevance
  prefs: []
  type: TYPE_NORMAL
- en: 'Apart from it, based on its ranking position, each item receives a discount
    value: the higher the ranking position of an item, the higher the corresponding
    discount is. Discount acts as a penalty by proportionally reducing the item’s
    gain. In practice, the discount is usually chosen as a logarithmic function of
    a ranking index:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3033209d6c6bb2f563ccd481027548c5.png)'
  prefs: []
  type: TYPE_IMG
- en: Discount formula in DCG
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/06a24bf09deacada8b4dde5250ebaad3.png)'
  prefs: []
  type: TYPE_IMG
- en: Discount function of ranking position
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, *DCG@k* is defined as the sum of a gain over a discount for all first
    k retrieved items:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/bd720cb2561e6555c1bf579671d84af7.png)'
  prefs: []
  type: TYPE_IMG
- en: DCG formula in general
  prefs: []
  type: TYPE_NORMAL
- en: 'Replacing *gainᵢ* and *discountᵢ* with the formulas above, the expression takes
    the following form:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3150a812cca07bfde0acb0f987e71614.png)'
  prefs: []
  type: TYPE_IMG
- en: DCG formula
  prefs: []
  type: TYPE_NORMAL
- en: To make *DCG* metric more interpretable, it is usually normalised by the maximum
    possible value of *DCGₘₐₓ* in the case of perfect ranking when all items are correctly
    sorted by their relevance. The resulting metric is called *nDCG* and takes values
    between 0 and 1.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0e294ad92e957a1b27fa26c682754a98.png)'
  prefs: []
  type: TYPE_IMG
- en: nDCG formula
  prefs: []
  type: TYPE_NORMAL
- en: In the figure below, an example of *DCG* and *nDCG* calculation for 5 documents
    is shown.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b12d2758a69b238f31df63242006c4a2.png)'
  prefs: []
  type: TYPE_IMG
- en: DCG and nDCG computed for a set of retrieved documents
  prefs: []
  type: TYPE_NORMAL
- en: RBP (Rank-Biased Precision)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the *RBP* workflow, the user does not have the intention to examine every
    possible item. Instead, he or she sequentially progresses from one document to
    another with probability *p* and with inverse probability *1 — p* terminates the
    search procedure at the current document. Each termination decision is taken independently
    and does not depend on the depth of the search. According to the conducted research,
    such user behaviour has been observed in many experiments. Based on the information
    from [Rank-Biased Precision for Measurement of Retrieval Effectiveness](https://people.eng.unimelb.edu.au/jzobel/fulltext/acmtois08.pdf),
    the workflow can be illustrated in the diagram below.
  prefs: []
  type: TYPE_NORMAL
- en: Parameter p is called **persistence**.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/146a4b079a406eac2678d9934e7b65a1.png)'
  prefs: []
  type: TYPE_IMG
- en: RBP model workflow
  prefs: []
  type: TYPE_NORMAL
- en: 'In this paradigm, the user looks always looks at the *1*-st document, then
    looks at the *2*-nd document with probability *p*, looks at the *3*-rd document
    with probability *p²* and so on. Ultimately, the probability of looking at document
    *i* becomes equal to:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/10d38ca1b4ea3472dfabb76e11f37cc1.png)'
  prefs: []
  type: TYPE_IMG
- en: The user examines document *i* in only when document *i* has just already been
    looked at and the search procedure is immediately terminated with probability
    *1 — p*.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c640e05dcd36f7bc77d2ec22ba5f1d76.png)'
  prefs: []
  type: TYPE_IMG
- en: 'After that, it is possible to estimate the expected number of examined documents.
    Since *0 ≤ p ≤ 1*, the series below is convergent and the expression can be transformed
    into the following format:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/40a3161fcd455c647538a9098324bf44.png)'
  prefs: []
  type: TYPE_IMG
- en: Similarly, given each document’s relevance *Rᵢ*, let us find the expected document
    relevance. Higher values of expected relevance indicate that the user will be
    more satisfied with the document he or she decides to examine.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5af81dfedbb10bd8d323c87ac5f2358e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Finally, *RPB* is computed as the ratio of expected document relevance (utility)
    to the expected number of checked documents:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/fa08fe56d1ca7af9050dcb1af6d863f4.png)'
  prefs: []
  type: TYPE_IMG
- en: '*RPB* formulation makes sure that it takes values between 0 and 1\. Normally,
    relevance scores are of binary type (1 if a document is relevant, 0 otherwise)
    but can take real values between 0 and 1 as well.'
  prefs: []
  type: TYPE_NORMAL
- en: The appropriate value of *p* should be chosen, based on how persistent users
    are in the system. Small values of *p* (less than 0.5) place more emphasis on
    top-ranked documents in the ranking. With bigger values of *p*, the weight on
    first positions is reduced and is distributed across lower positions. Sometimes
    it might be difficult to find out a good value of persistence *p*, so it is better
    to run several experiments and choose *p* which works the best.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/fe23bb4fbc2bd80e997f0bfb6f6f8910.png)'
  prefs: []
  type: TYPE_IMG
- en: ERR (Expected Reciprocal Rank)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As the name suggests, this metric measures the average reciprocal rank across
    many queries.
  prefs: []
  type: TYPE_NORMAL
- en: 'This model is similar to *RPB* but with a little difference: if the current
    item is relevant (*Rᵢ*) for the user, then the search procedure ends. Otherwise,
    if the item is not relevant (*1 — Rᵢ)*, then with probability *p* the user decides
    whether he or she wants to continue the search process. If so, the search proceeds
    to the next item. Otherwise, the users ends the search procedure.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ecc5175b0790f4bf1933993247436107.png)'
  prefs: []
  type: TYPE_IMG
- en: ERR model workflow
  prefs: []
  type: TYPE_NORMAL
- en: According to the presentation on [offline evaluation from Ilya Markov](https://drive.google.com/drive/folders/19OfEsLME1IR7bGPzVo8Dh31emcPNiaP_),
    let us find the formula for *ERR* calculation.
  prefs: []
  type: TYPE_NORMAL
- en: 'First of all, let us calculate the probability that the user looks at document
    i. Basically, it means that all *i — 1* previous documents were not relevant and
    at each iteration, the user proceeded with probability p to the next item:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d5af8cac0fabed1de7d48f4d2f3595f7.png)'
  prefs: []
  type: TYPE_IMG
- en: If a user stops at document *i*, it means that this document has already been
    looked and with probability *Rᵢ*, the user has decided to terminate the search
    procedure. The probability corresponding to this event is actually the same as
    the reciprocal rank equals *1 / i*.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/16825649c5da24f24cc38ae512a23727.png)'
  prefs: []
  type: TYPE_IMG
- en: 'From now, by simply using the formula for the expected value, it is possible
    to estimate the expected reciprocal rank:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3ec4e7e74aea183bb51e98735996012a.png)'
  prefs: []
  type: TYPE_IMG
- en: Parameter p is usually chosen close to 1.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: As in the case of *RBP*, the values of *Rᵢ* can either be binary or real in
    the range from 0 to 1\. An example of *ERR* calculation is demonstrated in the
    figure below for a set of 6 documents.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6c287b27c5ff6aceef544d5f71a87fb7.png)'
  prefs: []
  type: TYPE_IMG
- en: ERR calculation. The best and worst possible rankings are shown on the left
    and on the right respectively. For simplicity, parameter p is chosen as 1.
  prefs: []
  type: TYPE_NORMAL
- en: On the left, all the retrieved documents are sorted in the descending order
    of their relevance resulting in the best possible *ERR*. Contrary to the situation
    on the right, the documents are presented in the ascending order of their relevance
    leading to the worst possible *ERR*.
  prefs: []
  type: TYPE_NORMAL
- en: 'ERR formula assumes that all relevance scores are in the range from 0 to 1\.
    In case when initial relevance scores are given from out of that range, they need
    to be normalised. One of the most popular ways to do it is to exponentially normalise
    them:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4402786be1dbc5e678eb7bb92dc467f5.png)'
  prefs: []
  type: TYPE_IMG
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We have discussed all the main metrics used for quality evaluation in information
    retrieval. User-oriented metrics are used more often because they reflect real
    user behaviour. Additionally, *nDCG*, *BPR* and *ERR* metrics have an advantage
    over other metrics we have looked at so far: they work with multiple relevance
    levels making them more versatile, in comparison to metrics like *AP*, *MAP* or
    *MRR* which are designed only for binary levels of relevance.'
  prefs: []
  type: TYPE_NORMAL
- en: Unfortunately, all of the described metrics are either discontinuous or flat
    making the gradient at problematic points equal to 0 or even not defined. As a
    consequence, it is difficult for most ranking algorithms to optimise these metrics
    directly. However, a lot of research has been elaborated in this area and many
    advanced heuristics have appeared under the hood of the most popular ranking algorithms
    to solve this issue.
  prefs: []
  type: TYPE_NORMAL
- en: Resources
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[Kendall Tau distance | Wikipedia](https://en.wikipedia.org/wiki/Kendall_tau_distance#:~:text=The%20Kendall%20tau%20rank%20distance,dissimilar%20the%20two%20lists%20are.)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Rank-Biased Precision for Measurement of Retrieval Effectiveness](https://people.eng.unimelb.edu.au/jzobel/fulltext/acmtois08.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Discounted Cumulative Gain | Wikipedia](https://en.wikipedia.org/wiki/Discounted_cumulative_gain)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Uncertainty in Rank-Biased Precision](https://staff.cdms.westernsydney.edu.au/~lapark/publications/laparkRBP2016.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Information Retrieval, Offline Evaluation | Ilya Markov](https://drive.google.com/drive/folders/19OfEsLME1IR7bGPzVo8Dh31emcPNiaP_)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*All images unless otherwise noted are by the author.*'
  prefs: []
  type: TYPE_NORMAL
