- en: Towards Stand-Alone Self-Attention in Vision
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/towards-stand-alone-self-attention-in-vision-3d0561c6aee5?source=collection_archive---------8-----------------------#2023-04-28](https://towardsdatascience.com/towards-stand-alone-self-attention-in-vision-3d0561c6aee5?source=collection_archive---------8-----------------------#2023-04-28)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*A deep dive into the application of the transformer architecture and its self-attention
    operation for vision*'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@ju2ez?source=post_page-----3d0561c6aee5--------------------------------)[![Julian
    Hatzky](../Images/9f1ce9a29d215feeb5223e8fd659383e.png)](https://medium.com/@ju2ez?source=post_page-----3d0561c6aee5--------------------------------)[](https://towardsdatascience.com/?source=post_page-----3d0561c6aee5--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----3d0561c6aee5--------------------------------)
    [Julian Hatzky](https://medium.com/@ju2ez?source=post_page-----3d0561c6aee5--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fe24e3594d8a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftowards-stand-alone-self-attention-in-vision-3d0561c6aee5&user=Julian+Hatzky&userId=e24e3594d8a&source=post_page-e24e3594d8a----3d0561c6aee5---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----3d0561c6aee5--------------------------------)
    ·14 min read·Apr 28, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F3d0561c6aee5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftowards-stand-alone-self-attention-in-vision-3d0561c6aee5&user=Julian+Hatzky&userId=e24e3594d8a&source=-----3d0561c6aee5---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F3d0561c6aee5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftowards-stand-alone-self-attention-in-vision-3d0561c6aee5&source=-----3d0561c6aee5---------------------bookmark_footer-----------)![](../Images/b58911b38409853b14032c171382c9bd.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Image created by author using [craiyon AI](https://www.craiyon.com/)
  prefs: []
  type: TYPE_NORMAL
- en: While self-attention is already widely adopted in NLP and significantly contributes
    to the performance of state-of-the-art models (e.g. [2], [3]), more and more work
    is being done to achieve similar results in vision.
  prefs: []
  type: TYPE_NORMAL
- en: Even though, there are hybrid approaches that combine for example CNNs with
    attention [4] or apply linear transformations on patches of the image [5], a pure
    attention-based model is harder to train effectively due to various reasons that
    we will investigate further on.
  prefs: []
  type: TYPE_NORMAL
- en: The *Stand-Alone Self-Attention in Vision Models* [6] paper introduces the idea
    of such a pure attention-based model for vision. In the following, I will give
    an overview of the paper’s ideas and related follow-up work. Further, I assume
    that you are familiar with the [workings of the transformer](https://peterbloem.nl/blog/transformers)
    and have a [basic knowledge of CNNs](https://www.youtube.com/watch?v=2hS_54kgMHs).
    An understanding of [PyTorch](https://pytorch.org/) is also beneficial for the
    coding parts but these can also be safely skipped.
  prefs: []
  type: TYPE_NORMAL
- en: '*If you are on the other hand only interested in the code, feel free to skip
    this article and directly take a look at* [*this annotated colab notebook*](https://colab.research.google.com/drive/1DDjyC3d1R8jgbaP73u6-77FIlnCEN57M?usp=sharing)*.*'
  prefs: []
  type: TYPE_NORMAL
- en: The Case for…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
