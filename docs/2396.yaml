- en: Speech and Natural Language Input for Your Mobile App Using LLMs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/speech-and-natural-language-input-for-your-mobile-app-using-llms-e79e23d3c5fd?source=collection_archive---------5-----------------------#2023-07-25](https://towardsdatascience.com/speech-and-natural-language-input-for-your-mobile-app-using-llms-e79e23d3c5fd?source=collection_archive---------5-----------------------#2023-07-25)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: How to leverage OpenAI GPT-4 Functions to navigate your GUI
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@hgwvandam?source=post_page-----e79e23d3c5fd--------------------------------)[![Hans
    van Dam](../Images/52846b7417b271767597c468edfaec46.png)](https://medium.com/@hgwvandam?source=post_page-----e79e23d3c5fd--------------------------------)[](https://towardsdatascience.com/?source=post_page-----e79e23d3c5fd--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----e79e23d3c5fd--------------------------------)
    [Hans van Dam](https://medium.com/@hgwvandam?source=post_page-----e79e23d3c5fd--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F6ce6c6116a37&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fspeech-and-natural-language-input-for-your-mobile-app-using-llms-e79e23d3c5fd&user=Hans+van+Dam&userId=6ce6c6116a37&source=post_page-6ce6c6116a37----e79e23d3c5fd---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----e79e23d3c5fd--------------------------------)
    ·14 min read·Jul 25, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fe79e23d3c5fd&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fspeech-and-natural-language-input-for-your-mobile-app-using-llms-e79e23d3c5fd&user=Hans+van+Dam&userId=6ce6c6116a37&source=-----e79e23d3c5fd---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fe79e23d3c5fd&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fspeech-and-natural-language-input-for-your-mobile-app-using-llms-e79e23d3c5fd&source=-----e79e23d3c5fd---------------------bookmark_footer-----------)![](../Images/e9ebbfb4cc7e7ae96821d7ba97ed5661.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Photo by [Kelly Sikkema](https://unsplash.com/@kellysikkema?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    on [Unsplash](https://unsplash.com/photos/v9FQR4tbIq8?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A Large Language Model (LLM) is a machine learning system that can effectively
    process natural language. The most advanced LLM available at the moment is GPT-4,
    which powers the paid version of ChatGPT. In this article, you will learn how
    to give your app highly flexible speech interpretation using GPT-4 function calling,
    in full synergy with your app’s Graphical User Interface (GUI). It is intended
    for product owners, UX designers, and mobile developers.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/dd3be11e362fdfc3a7b672287aec81ad.png)'
  prefs: []
  type: TYPE_IMG
- en: Background
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Digital assistants on mobile phones (Android and iOS) have failed to catch on
    for several reasons, among which they are faulty, limited, and often tedious to
    use. LLMs, and now especially OpenAI GPT-4, hold the potential to make a difference
    here, with their ability to more deeply grasp the user’s intention instead of
    trying to coarsely pattern match a spoken expression.
  prefs: []
  type: TYPE_NORMAL
- en: 'Android has Google Assistant’s ‘app actions’, and iOS has SiriKit intents.
    These provide simple templates to register speech requests that your app can handle.
    Google Assistant and Siri have already improved quite a bit over the past few
    years — even more than you probably realize. Their coverage is greatly determined,
    however, by which apps implement support for them. Nevertheless, you can, for
    instance, play your favorite song on Spotify using speech. The natural language
    interpretation of these OS-provided services, however, predates the huge advances
    in this field that LLMs have brought about — so it is time for the next step:
    to harness the power of LLMs to make speech input more reliable and flexible.'
  prefs: []
  type: TYPE_NORMAL
- en: Although we can expect that the operating system services (like Siri and Google
    Assistant) will adapt their strategies soon to take advantage of LLMs, we can
    already enable our apps to use speech without being limited by these services.
    Once you have adopted the concepts in this article, your app will also be ready
    to tap into [new assistants](https://www.axios.com/2023/07/31/google-assistant-artificial-intelligence-news),
    once they become available.
  prefs: []
  type: TYPE_NORMAL
- en: The choice of your LLM (GPT, PaLM, LLama2, MPT, Falcon etc.) does have an impact
    on reliability, but the core principles you will learn here can be applied to
    any of them. We will let the user access the entirety of the app’s functionality
    by saying what they want in a single expression. The LLM maps a natural language
    expression into a function call on the navigation structure and functionality
    of our app. And it need not be a sentence spoken like a robot. **LLM’s interpretation
    powers allow users to speak like a human, using their own words or language; hesitate,
    make mistakes, and correct mistakes**. Where users have rejected voice assistants
    because they often fail to understand what they mean, the flexibility of an LLM
    can make the interaction feel much more natural and reliable, leading to higher
    user adoption.
  prefs: []
  type: TYPE_NORMAL
- en: Why speech input in your app, and why now?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Pros:**'
  prefs: []
  type: TYPE_NORMAL
- en: Navigate to a screen and provide all parameters in one speech expression
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shallow Learning Curve: No need for the user to find where in your app the
    data is or how to operate the GUI'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hands-free
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Complementary and not unconnected (like in a voice user interface or VUI):
    speech and GUI work in harmony.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Accessibility for visually impaired
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now: because interpretation of natural language has risen to a new level through
    LLMs, responses are much more reliable'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cons:**'
  prefs: []
  type: TYPE_NORMAL
- en: Privacy when speaking
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: accuracy/misinterpretations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: still relatively slow
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Knowledge in the head vs. in the world (What can I say?): the user does not
    know what spoken expressions the system understands and has answers to'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Examples of apps that can benefit from speech input include those used for car
    or bicycle driving assistance. In general, users may not want to engage in the
    precision of navigating an app by touch when they cannot easily use their hands,
    for instance, when they are on the move, wearing gloves, or busy working with
    their hands.
  prefs: []
  type: TYPE_NORMAL
- en: Shopping apps may also benefit from this feature, as users can verbalize their
    desires in their own words rather than navigate through shopping screens and set
    filters.
  prefs: []
  type: TYPE_NORMAL
- en: When applying this approach to increase accessibility for visually impaired
    individuals, you might consider adding natural language output and text-to-speech
    features to the mix.
  prefs: []
  type: TYPE_NORMAL
- en: Your app
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The following figure shows the navigation structure of a typical app, exemplified
    by a train trip planner you may be familiar with. At the top, you see the default
    navigation structure for touch navigation. This structure is governed by the Navigation
    Component. All navigation clicks are delegated to the Navigation Component, which
    then executes the navigation action. The bottom depicts how we can tap into this
    structure using speech input.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/20896ca68632952ea9be1a1ee07da5d4.png)'
  prefs: []
  type: TYPE_IMG
- en: speech enabling your app using LLM function calling
  prefs: []
  type: TYPE_NORMAL
- en: 'The users say what they want; then a speech recognizer transforms the speech
    into text. The system constructs a prompt containing this text and sends it to
    the LLM. The LLM responds to the app with data, telling it which screen to activate
    with which parameters. This data object is turned into a deep link and given to
    the navigation component. The navigation component activates the right screen
    with the right parameters: in this example, the ‘Outings’ screen with ‘Amsterdam’
    as a parameter. Please note that this is a simplification. We will elaborate on
    the details below.'
  prefs: []
  type: TYPE_NORMAL
- en: Many modern apps have a centralized navigation component under the hood. Android
    has Jetpack Navigation, Flutter has the Router, and iOS has NavigationStack. Centralized
    navigation components allow deep linking, which is a technique that allows users
    to navigate directly to a specific screen within a mobile application rather than
    going through the app’s main screen or menu. For the concepts in this article
    to work, a navigation component and centralized deep linking are not necessary,
    but they make implementing the concepts easier.
  prefs: []
  type: TYPE_NORMAL
- en: Deep linking involves creating a unique (URI) path that points to a specific
    piece of content or a specific section within an app. Moreover, this path can
    contain parameters that control the states of GUI elements on the screen that
    the deep link points to.
  prefs: []
  type: TYPE_NORMAL
- en: Function calling for your app
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We tell the LLM to map a natural language expression to a navigation function
    call through prompt engineering techniques. The prompt reads something like: ‘Given
    the following function templates with parameters, map the following natural language
    question onto one of these function templates and return it’.'
  prefs: []
  type: TYPE_NORMAL
- en: Most LLMs are capable of this. LangChain has leveraged it effectively through
    Zero Shot ReAct Agents, and the functions to be called are called [Tools](https://python.langchain.com/docs/modules/agents/tools/).
    OpenAI has fine-tuned their GPT-3.5 and GPT-4 models with special versions (currently
    gpt-3.5-turbo-0613 and gpt-4–0613) that are very good at this, and they have made
    specific API entries for this purpose. In this article, we will take the OpenAI
    notation, but the concepts can be applied to any LLM, e.g. using the ReAct mechanism
    mentioned. Moreover, LangChain has a specific agent type (AgentType.OPENAI_FUNCTIONS)
    that translates Tools into OpenAI function templates under the hood. For LLama2
    you will be able to use [llama-api](https://www.llama-api.com/) with the same
    syntax as OpenAI.
  prefs: []
  type: TYPE_NORMAL
- en: 'Function calling for LLMs works as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: You insert a JSON schema of function templates into your prompt along with the
    user’s natural language expression as a user message.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The LLM attempts to map the user’s natural language expression onto one of these
    templates.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The LLM returns the resulting JSON object so your code can make a function call.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In this article, the function definitions are direct mappings of the graphical
    user interface (GUI) of a (mobile) app, where each function corresponds to a screen
    and each parameter to a GUI element on that screen. A natural language expression
    sent to the LLM returns a JSON object containing a function name and its parameters
    that you can use to navigate to the right screen and trigger the right function
    in your view model, such that the right data is fetched. The values of the relevant
    GUI elements on that screen are set according to the parameters.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is illustrated in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b8ae7bc16f62cd81d969e202d45f53c9.png)'
  prefs: []
  type: TYPE_IMG
- en: mapping LLM functions onto your mobile app’s GUI
  prefs: []
  type: TYPE_NORMAL
- en: 'It shows a stripped version of the function templates as added to the prompt
    for the LLM. To see the full-length prompt for the user message: ‘What things
    can I do in Amsterdam?’, [click here (Github Gist](https://gist.github.com/hansvdam/8b9269390e16fa0bf394d7656bec1ea5)).
    It contains a full curl request that you can use from the command line or import
    into Postman. You need to put your own O[penAI-key](https://platform.openai.com/account/api-keys)
    in the placeholder to run it.'
  prefs: []
  type: TYPE_NORMAL
- en: Screens without parameters
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Some screens in your app don’t have any parameters, or at least not the ones
    that the LLM needs to be aware of. To reduce token usage and clutter, we can combine
    a number of these screen triggers in a single function with one parameter: the
    screen to open.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The Criterion as to whether a triggering function needs parameters is whether
    the user has a choice: there is some form of search or navigation going on on
    the screen, i.e. are there any search (like) fields or tabs to choose from?'
  prefs: []
  type: TYPE_NORMAL
- en: If not, then the LLM does not need to know about it, and screen triggering may
    be added to the generic screen triggering function of your app. It is mostly a
    matter of experimentation with the descriptions of the screen purpose. If you
    need a longer description, consider giving it its own function definition to put
    more separate emphasis on its description than the enum of the generic parameter
    does.
  prefs: []
  type: TYPE_NORMAL
- en: 'Prompt instruction guidance and repair:'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the system message of your prompt, you give generic steering information.
    In our example, it can be important for the LLM to know what date and time it
    is now, for instance, if you want to plan a trip for tomorrow. Another important
    thing is to steer its presumptiveness. Often, we would rather have the LLM be
    overconfident than bother the user with its uncertainty. A good system message
    for our example app is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Function parameter descriptions can require quite a bit of tuning. An example
    is the trip_date_time when planning a train trip. A reasonable parameter description
    is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'So if it is now 15:00 and users say they want to leave at 8, they mean 20:00
    unless they specifically mention the time of the day. The above instruction works
    reasonably well for GPT-4\. But in some edge cases, it still fails. We can then
    e.g. add extra parameters to the function template that we can use to make further
    repairs in our own code. For instance, we can add:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: In your app, you will likely find parameters that require post-processing to
    enhance their success ratio.
  prefs: []
  type: TYPE_NORMAL
- en: System requests for clarification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Sometimes, the user’s request lacks information to proceed. There may not be
    a function suitable to handle the user’s request. In that case, the LLM will respond
    in natural language that you can show to the user, e.g. by means of a Toast.
  prefs: []
  type: TYPE_NORMAL
- en: It may also be the case that the LLM does recognize a potential function to
    call, but information is lacking to fill all required function parameters. In
    that case, consider making parameters optional. But if that is not possible, the
    LLM may send a request in natural language for the missing parameters in the language
    of the user. You should show this text to the users, e.g. through a Toast or text-to-speech,
    so they can give the missing information (in speech). For instance, when the user
    says ‘I want to go to Amsterdam’ (and your app has not provided a default or current
    location through the system message) the LLM might respond with ‘I understand
    you want to make a train trip, from where do you want to depart?’.
  prefs: []
  type: TYPE_NORMAL
- en: This brings up the issue of conversational history. I recommend you always include
    the last 4 messages from the user in the prompt so a request for information can
    be spread over multiple turns. To simplify things, omit the system’s responses
    from the history because, in this use case, they tend to do more harm than good.
  prefs: []
  type: TYPE_NORMAL
- en: Speech recognition
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Speech recognition is a crucial part of the transformation from speech to a
    parametrized navigation action in the app. When the quality of interpretation
    is high, bad speech recognition may very well be the weakest link. Mobile phones
    have onboard speech recognition with reasonable quality, but LLM-based speech
    recognition like [Whisper](https://openai.com/research/whisper), Google [Chirp/USM](https://cloud.google.com/speech-to-text/v2/docs/chirp-model),
    Meta [MMS](https://ai.meta.com/blog/multilingual-model-speech-recognition/?utm_source=twitter&utm_medium=organic_social&utm_campaign=blog&utm_content=card),
    or [DeepGram](https://developers.deepgram.com/reference/streaming) tends to lead
    to better results, especially when you can [tune them](https://cloud.google.com/speech-to-text/docs/adaptation-model)
    for your use case.
  prefs: []
  type: TYPE_NORMAL
- en: Architecture
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'It is probably best to store the function definitions on the server, but they
    can also be managed by the app and sent with every request. Both have their pros
    and cons. Having them sent with every request is more flexible, and the alignment
    of functions and screens may be easier to maintain. However, the function templates
    not only contain the function name and parameters but also their descriptions
    that we might want to update quicker than the update flow in the app stores. These
    descriptions are more or less LLM-dependent and crafted for what works. It is
    not unlikely that you want to swap out the LLM for a better or cheaper one or
    even swap dynamically at some point. Having the function templates on the server
    may also have the advantage of maintaining them in one place if your app is native
    on iOS and Android. If you use OpenAI services for both speech recognition and
    natural language processing, the technical big picture of the flow looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ca4debb6ef8e0f616fcb21be2a997df9.png)'
  prefs: []
  type: TYPE_IMG
- en: architecture for speech enabling your mobile app using Whisper and OpenAI function
    calling
  prefs: []
  type: TYPE_NORMAL
- en: The users speak their request; it is recorded into an m4a buffer/file (or mp3
    if you like), which is sent to your server, which relays it to [Whisper](https://openai.com/blog/introducing-chatgpt-and-whisper-apis).
    Whisper responds with the transcription, and your server combines it with your
    system message and function templates into a prompt for the LLM. Your server receives
    back the raw function call JSON, which it then processes into a function call
    JSON object for your app.
  prefs: []
  type: TYPE_NORMAL
- en: From function call to deep link
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To illustrate how a function call translates into a deep link, we take the
    function call response from the initial example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'On different platforms, this is handled quite differently, and over time, many
    different navigation mechanisms have been used and are often still in use. It
    is beyond the scope of this article to go into implementation details, but roughly
    speaking, the platforms in their most recent incarnation can employ deep linking
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'On Android:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'On Flutter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'On iOS, things are a little less standardized, but using NavigationStack:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'And then issuing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'More on deep linking can be found here: [for Android](https://blog.appcircle.io/article/jetpack-compose-navigation-deeplink),
    [for Flutter](https://docs.flutter.dev/cookbook/navigation/navigate-with-arguments),
    [for iOS](https://www.swiftyplace.com/blog/better-navigation-in-swiftui-with-navigation-stack)'
  prefs: []
  type: TYPE_NORMAL
- en: Free text field for apps
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There are two modes of free text input: voice and typing. We’ve mainly talked
    about speech, but a text field for typing input is also an option. Natural language
    is usually quite lengthy, so it may be difficult to compete with GUI interaction.
    However, GPT-4 tends to be quite good at guessing parameters from abbreviations,
    so even very short abbreviated typing can often be interpreted correctly.'
  prefs: []
  type: TYPE_NORMAL
- en: The use of functions with parameters in the prompt often dramatically narrows
    the interpretation context for an LLM. Therefore, it needs very little, and even
    less if you instruct it to be presumptive. This is a new phenomenon that holds
    promise for mobile interaction. In the case of the train station to train station
    planner, the LLM made the following interpretations when used with the exemplary
    prompt structure in this article. You can try it out for yourself using the [prompt
    gist](https://gist.github.com/hansvdam/8b9269390e16fa0bf394d7656bec1ea5) mentioned
    above.
  prefs: []
  type: TYPE_NORMAL
- en: '**Examples:**'
  prefs: []
  type: TYPE_NORMAL
- en: '‘ams utr’: show me a list of train itineraries from Amsterdam Central Station
    to Utrecht Central Station departing from now'
  prefs: []
  type: TYPE_NORMAL
- en: '‘utr ams arr 9’: (Given that it is 13:00 at the moment). Show me a list of
    train itineraries from Utrecht Central Station to Amsterdam Central Station, arriving
    before 21:00'
  prefs: []
  type: TYPE_NORMAL
- en: '**Follow up interaction**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Just like in ChatGPT, you can refine your query if you send a short piece of
    the interaction history along:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Using the history feature, the following also works very well (presume it is
    9:00 in the morning now):'
  prefs: []
  type: TYPE_NORMAL
- en: 'Type: ‘ams utr’ and get the answer as above. Then type ‘arr 7’ in the next
    turn. And yes, it can actually translate that into a trip being planned from Amsterdam
    Central to Utrecht Central, arriving before 19:00.'
  prefs: []
  type: TYPE_NORMAL
- en: I made an example web app about this that you can find a video about [here](https://www.youtube.com/watch?v=XBjiqLD578I).
    The link to the actual app is in the description.
  prefs: []
  type: TYPE_NORMAL
- en: 'Update: a successor to this article incorporating text input can be found [here](https://medium.com/towards-data-science/synergy-of-llm-and-gui-beyond-the-chatbot-c8b0e08c6801)
    and a demo video [here](https://youtu.be/vJy0HI_mH7w).'
  prefs: []
  type: TYPE_NORMAL
- en: Future
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You can expect this deep link structure to handle functions within your app
    to become an integral part of your phone’s OS (Android or iOS). A global assistant
    on the phone will handle speech requests, and apps can expose their functions
    to the OS so that they can be triggered in a deep-linking fashion. This parallels
    how plugins are made available for ChatGPT. Now a coarse form of this is already
    available through the intents in the AndroidManifest and [App Actions](https://developers.google.com/assistant/app)
    on Android and on iOS through [SiriKit intents](https://medium.com/simform-engineering/how-to-integrate-siri-shortcuts-and-design-custom-intents-tutorial-e53285b550cf).
    The amount of control you have over these is limited, and the user has to speak
    like a robot to activate them reliably. Undoubtedly, this will improve over time
    when [LLM-powered assistants](https://www.axios.com/2023/07/31/google-assistant-artificial-intelligence-news)
    take over.
  prefs: []
  type: TYPE_NORMAL
- en: VR and AR (XR) offer great opportunities for speech recognition because the
    user's hands are often engaged in other activities.
  prefs: []
  type: TYPE_NORMAL
- en: It will probably not take long before anyone can run their own high-quality
    LLM. The cost will decrease, and speed will increase rapidly over the next year.
    Soon LoRA LLMs will become available on smartphones, so inference can take place
    on your phone, reducing cost and speed. Also, more and more competition will come,
    both open source like [Llama2](https://ai.meta.com/llama/), and closed source
    like [PaLM](https://developers.generativeai.google/products/palm).
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, the synergy of modalities can be driven further than providing random
    access to the GUI of your entire app. It is the power of LLMs to combine multiple
    sources that hold the promise for better assistance to emerge. Some interesting
    articles: [multimodal dialog](https://masterofcode.com/blog/multimodal-conversation-design-tutorial-part-2-best-practices-use-cases-and-future-outlook),
    [google blog on GUIs and LLMs](https://ai.googleblog.com/2023/05/enabling-conversational-interaction-on.html),
    [interpreting GUI interaction as language](https://pure.tue.nl/ws/files/3655721/200612175.pdf),
    [LLM Powered Assistants](https://nickarner.com/notes/llm-powered-assistants-for-complex-interfaces-february-26-2023/).'
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this article, you learned how to apply function calling to speech enable
    your app. Using the provided [Gist](https://gist.github.com/hansvdam/8b9269390e16fa0bf394d7656bec1ea5)
    as a point of departure, you can experiment in Postman or from the command line
    to get an idea of how powerful function calling is. If you want to run a POC on
    speech enabling your app, I would recommend putting the server bit from the architecture
    section directly into your app. It all boils down to 2 HTTP calls, some prompt
    construction, and implementing microphone recording. Depending on your skill and
    codebase, you will have your POC up and running in several days.
  prefs: []
  type: TYPE_NORMAL
- en: Happy coding!
  prefs: []
  type: TYPE_NORMAL
- en: Follow me on [LinkedIn](https://www.linkedin.com/in/hans-van-dam-71a7866/) or
    [UXX.AI](https://uxx.ai)
  prefs: []
  type: TYPE_NORMAL
- en: '*All images in this article, unless otherwise noted, are by the author.*'
  prefs: []
  type: TYPE_NORMAL
