- en: 'Mixtral-8x7B: Understanding and Running the Sparse Mixture of Experts'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/mixtral-8x7b-understanding-and-running-the-sparse-mixture-of-experts-0e3fc7fde818?source=collection_archive---------2-----------------------#2023-12-15](https://towardsdatascience.com/mixtral-8x7b-understanding-and-running-the-sparse-mixture-of-experts-0e3fc7fde818?source=collection_archive---------2-----------------------#2023-12-15)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: How to efficiently outperform GPT-3.5 and Llama 2 70B
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@bnjmn_marie?source=post_page-----0e3fc7fde818--------------------------------)[![Benjamin
    Marie](../Images/3ea1ad230cb1e67610418a8e36a5e5dd.png)](https://medium.com/@bnjmn_marie?source=post_page-----0e3fc7fde818--------------------------------)[](https://towardsdatascience.com/?source=post_page-----0e3fc7fde818--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----0e3fc7fde818--------------------------------)
    [Benjamin Marie](https://medium.com/@bnjmn_marie?source=post_page-----0e3fc7fde818--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fad2a414578b3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmixtral-8x7b-understanding-and-running-the-sparse-mixture-of-experts-0e3fc7fde818&user=Benjamin+Marie&userId=ad2a414578b3&source=post_page-ad2a414578b3----0e3fc7fde818---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----0e3fc7fde818--------------------------------)
    ·6 min read·Dec 15, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F0e3fc7fde818&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmixtral-8x7b-understanding-and-running-the-sparse-mixture-of-experts-0e3fc7fde818&user=Benjamin+Marie&userId=ad2a414578b3&source=-----0e3fc7fde818---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F0e3fc7fde818&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmixtral-8x7b-understanding-and-running-the-sparse-mixture-of-experts-0e3fc7fde818&source=-----0e3fc7fde818---------------------bookmark_footer-----------)![](../Images/938838a6e7257e33be2de06cb5ee0283.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: Image by [8385](https://pixabay.com/users/8385-8385/?utm_source=link-attribution&utm_medium=referral&utm_campaign=image&utm_content=2147790)
    from [Pixabay](https://pixabay.com//?utm_source=link-attribution&utm_medium=referral&utm_campaign=image&utm_content=2147790)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: Most of the recent large language models (LLMs) use very similar neural architectures.
    For instance, the Falcon, Mistral, and Llama 2 models use a similar combination
    of self-attention and MLP modules.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: 'In contrast, Mistral AI, which also created Mistral 7B, just released a new
    LLM with a significantly different architecture: [Mixtral-8x7B](https://huggingface.co/mistralai/Mixtral-8x7B-v0.1),
    a sparse mixture of 8 expert models.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，Mistral AI（也创造了Mistral 7B）刚刚发布了一种具有显著不同架构的新LLM：[Mixtral-8x7B](https://huggingface.co/mistralai/Mixtral-8x7B-v0.1)，这是一个稀疏混合的8个专家模型。
- en: In total, Mixtral contains 46.7B parameters. Yet, thanks to its architecture,
    Mixtral-8x7B can efficiently run on consumer hardware. Inference with Mixtral-8x7B
    is indeed significantly faster than other models of similar size while outperforming
    them in most tasks.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，Mixtral包含46.7B参数。然而，得益于其架构，Mixtral-8x7B可以高效地在消费者硬件上运行。与其他相似规模的模型相比，Mixtral-8x7B的推理速度显著更快，并且在大多数任务中表现优越。
- en: In this article, I explain what a sparse mixture of experts is and why it is
    faster for inference than a standard model. Then, we will see how to use and fine-tune
    Mixtral-8x7B on consumer hardware.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我解释了什么是稀疏专家混合网络，以及为什么它在推理时比标准模型更快。接着，我们将看到如何在消费者硬件上使用和微调Mixtral-8x7B。
- en: 'I have implemented a notebook demonstrating QLoRA fine-tuning and inference
    with Mixtral-8x7B here:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 我已经在这里实现了一个笔记本，展示了使用Mixtral-8x7B进行QLoRA微调和推理：
- en: '[Get the notebook (#32)](https://colab.research.google.com/drive/1VDa0lIfqiwm16hBlIlEaabGVTNB3dN1A?usp=sharing)'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '[获取笔记本（#32）](https://colab.research.google.com/drive/1VDa0lIfqiwm16hBlIlEaabGVTNB3dN1A?usp=sharing)'
- en: A Sparse Mixture of Experts
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 稀疏专家混合网络
- en: '![](../Images/5ee6f895406c035bc0d9f83f707709ef.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5ee6f895406c035bc0d9f83f707709ef.png)'
- en: Image by the author
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供
- en: A sparse mixture of experts (SMoE) is a type of neural network architecture
    designed to improve the efficiency and scalability of traditional models. The
    concept of a mixture of experts was introduced to allow a model to learn different
    parts of the input space using specialized “expert” sub-networks. In Mixtral,
    there are 8 expert sub-networks.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 稀疏专家混合网络（SMoE）是一种神经网络架构，旨在提高传统模型的效率和可扩展性。专家混合的概念是为了使模型能够通过专门的“专家”子网络学习输入空间的不同部分。在Mixtral中，共有8个专家子网络。
- en: Note that the “8x7B” in the name of the model is slightly misleading. The model
    has a total of 46.7B parameters which is almost 10B parameters less than what
    8x7B parameters would yield. Indeed, Mixtral-8x7b is not a 56B parameter model
    since several modules, such as the ones for self-attention, are shared with the
    8 expert sub-networks.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，模型名称中的“8x7B”略有误导。该模型总共有46.7B参数，比8x7B参数应该有的量少了近10B参数。实际上，Mixtral-8x7B并不是一个56B参数的模型，因为一些模块，比如自注意力模块，与8个专家子网络共享。
- en: 'If you load and print the model with Transformers, the structure of the model
    is easier to understand:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你用Transformers加载并打印模型，模型的结构会更容易理解：
- en: '[PRE0]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: We can see that the MoE blocks are separated from the self-attention blocks.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到MoE模块与自注意力模块是分开的。
- en: Each expert sub-network is responsible for handling a specific region or aspect
    of the input data, and a gating (or router) network determines how much each expert
    contributes to the final prediction. In the case of Mixtral, only 2 experts are
    activated at the same time. Only 13B parameters are used during inference, hence
    the more efficient inference compared to other models of similar size.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 每个专家子网络负责处理输入数据的特定区域或方面，而一个门控（或路由器）网络决定每个专家对最终预测的贡献程度。在Mixtral的情况下，只有2个专家会同时激活。在推理过程中仅使用13B参数，因此相比于其他相似规模的模型，推理更加高效。
- en: 'To sum up, this sparse activation can be beneficial for several reasons:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，这种稀疏激活在几个方面可能会带来好处：
- en: '1\. Computational Efficiency: Activating only a subset of experts reduces the
    computational cost of evaluating the entire expert pool for every input.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 1\. 计算效率：仅激活一部分专家可以降低评估每个输入时整个专家池的计算成本。
- en: '2\. Parameter Efficiency: By allowing only a few experts to be active for a
    given input, the model can allocate its parameters more efficiently.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 2\. 参数效率：通过仅允许少数专家对给定输入进行激活，模型可以更有效地分配其参数。
- en: '3\. Generalization: Sparse activation can encourage the model to learn more
    specialized and fine-grained features for different regions of the input space.
    This can lead to better generalization and performance on diverse inputs. For
    instance, Mistral AI claims that Mixtral has been trained in Italian, German,
    French, Spanish, and also for coding. We can imagine that they may have trained
    some of the 8 experts to be especially better for these languages and tasks.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 3\. 泛化：稀疏激活可以鼓励模型学习输入空间不同区域的更多专门和细化的特征。这可以导致在多样化输入上的更好泛化和性能。例如，Mistral AI 声称
    Mixtral 已在意大利语、德语、法语、西班牙语以及编程方面进行过训练。我们可以想象，他们可能已训练了其中的一些 8 个专家，使其在这些语言和任务上表现得特别好。
- en: While SMoEs are faster than standard models of a similar size, they still occupy
    the same amount of memory. Quantization is one way to reduce the memory consumption.
    Offloading some experts to a slower memory, e.g., the CPU RAM, may also be a good
    alternative when we know in advance which experts will be the most solicited for
    a given task.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 SMoEs 比相似大小的标准模型更快，但它们仍然占用相同的内存量。量化是减少内存消耗的一种方法。将一些专家卸载到较慢的内存，例如 CPU RAM，也可能是一个不错的替代方案，尤其是当我们提前知道哪些专家将最常用于特定任务时。
- en: What We Know about Mixtral
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 关于 Mixtral 的信息
- en: 'Most of the details we know were published in this post by Mistral AI:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 我们知道的大部分细节都发布在 Mistral AI 的这篇文章中：
- en: '[Mixtral of experts](https://mistral.ai/news/mixtral-of-experts/)'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '[Mixtral 专家](https://mistral.ai/news/mixtral-of-experts/)'
- en: Mixtral is a model with 8 sub-networks acting as experts. At each layer and
    for each token, only 2 of these sub-networks are activated. Another set of parameters,
    the router network, decides on which ones to activate.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: Mixtral 是一个具有 8 个子网络的模型，这些子网络作为专家存在。在每一层和每个 token 上，只有 2 个子网络被激活。另一个参数集合，即路由网络，决定激活哪些子网络。
- en: 'As for the training data, Mistral AI doesn''t give much information about it.
    We only know that it is Web data:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 关于训练数据，Mistral AI 并没有给出太多信息。我们只知道它是 Web 数据：
- en: '*Mixtral is pre-trained on data extracted from the open Web*'
  id: totrans-34
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*Mixtral 是在从开放 Web 中提取的数据上进行预训练的*'
- en: I wonder what the “open Web” is.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 我想知道“开放 Web”是什么。
- en: They have evaluated the model on various public benchmarks to demonstrate that
    Mixtral is outperforming Llama 2 70B and GPT-3.5 on most of them.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 他们在各种公共基准测试中评估了模型，以证明 Mixtral 在大多数测试中优于 Llama 2 70B 和 GPT-3.5。
- en: That’s all we know so far.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 目前我们了解的就是这些。
- en: Running Mixtral on Your Computer
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在你的电脑上运行 Mixtral
- en: Mixtral is already supported by Hugging Face Transformers (from version 4.36.0)
    and bitsandbytes. We can quantize the model to fine-tune or run it on consumer
    hardware.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: Mixtral 已由 Hugging Face Transformers（从版本 4.36.0 起）和 bitsandbytes 支持。我们可以对模型进行量化，以便在消费级硬件上进行微调或运行。
- en: It also supports [FlashAttention 2 which helps to reduce the memory consumption](https://kaitchup.substack.com/p/use-flashattention-2-for-faster-fine)
    for inference and fine-tuning with long sequences (up to 32k tokens).
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 它还支持 [FlashAttention 2，这有助于减少推理和长序列（最多 32k tokens）微调的内存消耗](https://kaitchup.substack.com/p/use-flashattention-2-for-faster-fine)。
- en: '[In the model card](https://huggingface.co/mistralai/Mixtral-8x7B-v0.1), Mistral
    AI proposes a code sample to run the model:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '[在模型卡中](https://huggingface.co/mistralai/Mixtral-8x7B-v0.1)，Mistral AI 提供了一个运行模型的代码示例：'
- en: '[PRE1]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: In a Google Colab notebook, it only works with the A100 GPU (you need Google
    Colab Pro). The model has 46.7B parameters. Since the parameters are bfloat16,
    we need 2 bytes of memory per parameter, i.e., almost 100 GB free on your hard
    drive to download the model.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Google Colab 笔记本中，它仅与 A100 GPU 一起工作（你需要 Google Colab Pro）。该模型有 46.7B 个参数。由于参数是
    bfloat16，我们每个参数需要 2 字节的内存，即几乎 100 GB 的硬盘空间来下载模型。
- en: The model is available in the safetensors format. It is divided into 20 shards
    of around 4.95 GB. Once downloaded, it can take up to 7 minutes to load the model
    on the GPU.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型以 safetensors 格式提供。它分为 20 个约 4.95 GB 的分片。下载完成后，加载模型到 GPU 上可能需要最多 7 分钟。
- en: When quantized to 4-bit, the model occupies 23 GB of VRAM. To run it smoothly
    on your computer, you will need at least two GPUs with 16 GB of VRAM each, for
    instance, 2 NVIDIA RTX 4060 16 GB, or even better, 2 NVIDIA RTX 4080 16 GB.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 当量化到 4-bit 时，模型占用 23 GB 的 VRAM。为了在你的电脑上流畅运行，它需要至少两个每个 16 GB VRAM 的 GPU，例如 2
    个 NVIDIA RTX 4060 16 GB，或更好的 2 个 NVIDIA RTX 4080 16 GB。
- en: Fine-tuning Mixtral on Your Computer
  id: totrans-46
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在你的电脑上微调 Mixtral
- en: 'Mixtral also supports QLoRA fine-tuning. Here is a code sample for loading
    the model and preparing it for fine-tuning:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: Mixtral 还支持 QLoRA 微调。以下是加载模型并准备进行微调的代码示例：
- en: '[PRE2]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Or you can simply follow my [previous tutorials fine-tuning Mistral 7B](https://kaitchup.substack.com/p/mistral-7b-recipes-for-fine-tuning).
    It would work the same.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 或者你可以直接参考我的 [之前的 Mistral 7B 微调教程](https://kaitchup.substack.com/p/mistral-7b-recipes-for-fine-tuning)。效果会是一样的。
- en: You could also try to fine-tune it with IPO to make a strong Mixtral chat model.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以尝试通过 IPO 进行微调，来创建一个强大的 Mixtral 聊天模型。
- en: '[](https://kaitchup.substack.com/p/fine-tune-better-chat-models-with?source=post_page-----0e3fc7fde818--------------------------------)
    [## Fine-tune Better Chat Models with Distilled Identity Preference Optimization
    (IPO)'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://kaitchup.substack.com/p/fine-tune-better-chat-models-with?source=post_page-----0e3fc7fde818--------------------------------)
    [## 使用蒸馏身份偏好优化（IPO）微调更好的聊天模型'
- en: Mistral 7B aligned with IPO
  id: totrans-52
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Mistral 7B 与 IPO 对齐
- en: kaitchup.substack.com](https://kaitchup.substack.com/p/fine-tune-better-chat-models-with?source=post_page-----0e3fc7fde818--------------------------------)
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: kaitchup.substack.com](https://kaitchup.substack.com/p/fine-tune-better-chat-models-with?source=post_page-----0e3fc7fde818--------------------------------)
- en: Conclusion
  id: totrans-54
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: The sparse mixture of experts is an efficient model architecture that allows
    faster inference than standard models of similar size. Nonetheless, while Mixtral
    only uses around 1/4 of its parameters at inference time, it still requires to
    have all the parameters loaded in memory.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 稀疏专家混合模型是一种高效的模型架构，它比相似规模的标准模型具有更快的推理速度。尽管 Mixtral 在推理时只使用了大约 1/4 的参数，但仍然需要将所有参数加载到内存中。
- en: One way to reduce the memory footprint of LLM is quantization. For instance,
    we can use bitsandbytes NF4 to quantize Mixtral to 4-bit. It’s then possible to
    run or fine-tune Mixtral on consumer hardware but you’ll need at least two GPUs.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 减少 LLM 内存占用的一种方法是量化。例如，我们可以使用 bitsandbytes 的 NF4 将 Mixtral 量化为 4 位。这样就可以在消费级硬件上运行或微调
    Mixtral，但你至少需要两块 GPU。
- en: Hopefully, Mistral AI will publish a technical paper disclosing more information
    about the method used to train the 8 experts of Mixtral.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 希望 Mistral AI 会发布一篇技术论文，透露有关训练 Mixtral 8 位专家的方法的更多信息。
