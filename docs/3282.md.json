["```py\nimport numpy as np                  # for basic operations over arrays\nfrom scipy.spatial import distance  # to compute the Gaussian kernel\nimport cvxopt                       # to solve the dual opt. problem\nimport copy                         # to copy numpy arrays \n```", "```py\nclass SVM:\n    linear = lambda x, xࠤ , c=0: x @ xࠤ.T\n    polynomial = lambda x, xࠤ , Q=5: (1 + x @ xࠤ.T)**Q\n    rbf = lambda x, xࠤ, γ=10: np.exp(-γ*distance.cdist(x, xࠤ,'sqeuclidean'))\n    kernel_funs = {'linear': linear, 'polynomial': polynomial, 'rbf': rbf}\n```", "```py\nclass SVM:\n    linear = lambda x, xࠤ , c=0: x @ xࠤ.T\n    polynomial = lambda x, xࠤ , Q=5: (1 + x @ xࠤ.T)**Q\n    rbf = lambda x, xࠤ, γ=10: np.exp(-γ*distance.cdist(x, xࠤ,'sqeuclidean'))\n    kernel_funs = {'linear': linear, 'polynomial': polynomial, 'rbf': rbf}\n\n    def __init__(self, kernel='rbf', C=1, k=2):\n        # set the hyperparameters\n        self.kernel_str = kernel\n        self.kernel = SVM.kernel_funs[kernel]\n        self.C = C                  # regularization parameter\n        self.k = k                  # kernel parameter\n\n        # training data and support vectors (set later)\n        self.X, y = None, None\n        self.αs = None\n\n        # for multi-class classification (set later)\n        self.multiclass = False\n        self.clfs = [] \n```", "```py\nSVMClass = lambda func: setattr(SVM, func.__name__, func) or func\n```", "```py\n@SVMClass\ndef fit(self, X, y, eval_train=False):\n    # if more than two unique labels, call the multiclass version\n    if len(np.unique(y)) > 2:\n        self.multiclass = True\n        return self.multi_fit(X, y, eval_train)\n\n    # if labels given in {0,1} change it to {-1,1}\n    if set(np.unique(y)) == {0, 1}: y[y == 0] = -1\n\n    # ensure y is a Nx1 column vector (needed by CVXOPT)\n    self.y = y.reshape(-1, 1).astype(np.double) # Has to be a column vector\n    self.X = X\n    N = X.shape[0]  # Number of points\n\n    # compute the kernel over all possible pairs of (x, x') in the data\n    # by Numpy's vectorization this yields the matrix K\n    self.K = self.kernel(X, X, self.k)\n\n    ### Set up optimization parameters\n    # For 1/2 x^T P x + q^T x\n    P = cvxopt.matrix(self.y @ self.y.T * self.K)\n    q = cvxopt.matrix(-np.ones((N, 1)))\n\n    # For Ax = b\n    A = cvxopt.matrix(self.y.T)\n    b = cvxopt.matrix(np.zeros(1))\n\n    # For Gx <= h\n    G = cvxopt.matrix(np.vstack((-np.identity(N),\n                                 np.identity(N))))\n    h = cvxopt.matrix(np.vstack((np.zeros((N,1)),\n                                 np.ones((N,1)) * self.C)))\n\n    # Solve    \n    cvxopt.solvers.options['show_progress'] = False\n    sol = cvxopt.solvers.qp(P, q, G, h, A, b)\n    self.αs = np.array(sol[\"x\"])            # our solution\n\n    # a Boolean array that flags points which are support vectors\n    self.is_sv = ((self.αs-1e-3 > 0)&(self.αs <= self.C)).squeeze()\n    # an index of some margin support vector\n    self.margin_sv = np.argmax((0 < self.αs-1e-3)&(self.αs < self.C-1e-3))\n\n    if eval_train:  \n      print(f\"Finished training with accuracy{self.evaluate(X, y)}\")\n```", "```py\n@SVMClass\ndef predict(self, X_t):\n    if self.multiclass: return self.multi_predict(X_t)\n    # compute (xₛ, yₛ)\n    xₛ, yₛ = self.X[self.margin_sv, np.newaxis], self.y[self.margin_sv]\n    # find support vectors\n    αs, y, X= self.αs[self.is_sv], self.y[self.is_sv], self.X[self.is_sv]\n    # compute the second term\n    b = yₛ - np.sum(αs * y * self.kernel(X, xₛ, self.k), axis=0)\n    # compute the score\n    score = np.sum(αs * y * self.kernel(X, X_t, self.k), axis=0) + b\n    return np.sign(score).astype(int), score\n```", "```py\n@SVMClass\ndef evaluate(self, X,y):  \n    outputs, _ = self.predict(X)\n    accuracy = np.sum(outputs == y) / len(y)\n    return round(accuracy, 2)\n```", "```py\nfrom sklearn.datasets import make_classification\nimport numpy as np\n\n# Load the dataset\nnp.random.seed(1)\nX, y = make_classification(n_samples=2500, n_features=5, \n                           n_redundant=0, n_informative=5, \n                           n_classes=2,  class_sep=0.3)\n\n# Test Implemented SVM\nsvm = SVM(kernel='rbf', k=1)\nsvm.fit(X, y, eval_train=True)\n\ny_pred, _ = svm.predict(X)\nprint(f\"Accuracy: {np.sum(y==y_pred)/y.shape[0]}\")  #0.9108\n\n# Test with Scikit\nfrom sklearn.svm import SVC\nclf = SVC(kernel='rbf', C=1, gamma=1)\nclf.fit(X, y)\ny_pred = clf.predict(X)\nprint(f\"Accuracy: {sum(y==y_pred)/y.shape[0]}\")    #0.9108\n```", "```py\n@SVMClass\ndef multi_fit(self, X, y, eval_train=False):\n    self.k = len(np.unique(y))      # number of classes\n    # for each pair of classes\n    for i in range(self.k):\n        # get the data for the pair\n        Xs, Ys = X, copy.copy(y)\n        # change the labels to -1 and 1\n        Ys[Ys!=i], Ys[Ys==i] = -1, +1\n        # fit the classifier\n        clf = SVM(kernel=self.kernel_str, C=self.C, k=self.k)\n        clf.fit(Xs, Ys)\n        # save the classifier\n        self.clfs.append(clf)\n    if eval_train:  \n        print(f\"Finished training with accuracy {self.evaluate(X, y)}\")\n```", "```py\n@SVMClass\ndef multi_predict(self, X):\n    # get the predictions from all classifiers\n    N = X.shape[0]\n    preds = np.zeros((N, self.k))\n    for i, clf in enumerate(self.clfs):\n        _, preds[:, i] = clf.predict(X)\n\n    # get the argmax and the corresponding score\n    return np.argmax(preds, axis=1), np.max(preds, axis=1)\n```", "```py\nfrom sklearn.datasets import make_classification\nimport numpy as np\n\n# Load the dataset\nnp.random.seed(1)\nX, y = make_classification(n_samples=500, n_features=2, \n                           n_redundant=0, n_informative=2, \n                           n_classes=4, n_clusters_per_class=1,  \n                           class_sep=0.3)\n\n# Test SVM\nsvm = SVM(kernel='rbf', k=4)\nsvm.fit(X, y, eval_train=True)\n\ny_pred = svm.predict(X)\nprint(f\"Accuracy: {np.sum(y==y_pred)/y.shape[0]}\") # 0.65\n\n# Test with Scikit\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.svm import SVC\n\nclf = OneVsRestClassifier(SVC(kernel='rbf', C=1, gamma=4)).fit(X, y)\ny_pred = clf.predict(X)\nprint(f\"Accuracy: {sum(y==y_pred)/y.shape[0]}\")    # 0.65\n```", "```py\n import numpy as np                  # for basic operations over arrays\nfrom scipy.spatial import distance  # to compute the Gaussian kernel\nimport cvxopt                       # to solve the dual optimization problem\nimport copy                         # to copy numpy arrays \n\nclass SVM:\n    linear = lambda x, xࠤ , c=0: x @ xࠤ .T\n    polynomial = lambda x, xࠤ , Q=5: (1 + x @ xࠤ.T)**Q\n    rbf = lambda x, xࠤ , γ=10: np.exp(-γ * distance.cdist(x, xࠤ,'sqeuclidean'))\n    kernel_funs = {'linear': linear, 'polynomial': polynomial, 'rbf': rbf}\n\n    def __init__(self, kernel='rbf', C=1, k=2):\n        # set the hyperparameters\n        self.kernel_str = kernel\n        self.kernel = SVM.kernel_funs[kernel]\n        self.C = C                  # regularization parameter\n        self.k = k                  # kernel parameter\n\n        # training data and support vectors\n        self.X, y = None, None\n        self.αs = None\n\n        # for multi-class classification\n        self.multiclass = False\n        self.clfs = []                                  \n\n# This is useless here (only for notebook)\nSVMClass = lambda func: setattr(SVM, func.__name__, func) or func\n\n@SVMClass\ndef fit(self, X, y, eval_train=False):\n    if len(np.unique(y)) > 2:\n        self.multiclass = True\n        return self.multi_fit(X, y, eval_train)\n\n    # relabel if needed\n    if set(np.unique(y)) == {0, 1}: y[y == 0] = -1\n    # ensure y has dimensions Nx1\n    self.y = y.reshape(-1, 1).astype(np.double) # Has to be a column vector\n    self.X = X\n    N = X.shape[0]\n\n    # compute the kernel over all possible pairs of (x, x') in the data\n    self.K = self.kernel(X, X, self.k)\n\n    # For 1/2 x^T P x + q^T x\n    P = cvxopt.matrix(self.y @ self.y.T * self.K)\n    q = cvxopt.matrix(-np.ones((N, 1)))\n\n    # For Ax = b\n    A = cvxopt.matrix(self.y.T)\n    b = cvxopt.matrix(np.zeros(1))\n\n    # For Gx <= h\n    G = cvxopt.matrix(np.vstack((-np.identity(N),\n                                 np.identity(N))))\n    h = cvxopt.matrix(np.vstack((np.zeros((N,1)),\n                                 np.ones((N,1)) * self.C)))\n\n    # Solve    \n    cvxopt.solvers.options['show_progress'] = False\n    sol = cvxopt.solvers.qp(P, q, G, h, A, b)\n    self.αs = np.array(sol[\"x\"])\n\n    # Maps into support vectors\n    self.is_sv = ((self.αs > 1e-3) & (self.αs <= self.C)).squeeze()\n    self.margin_sv = np.argmax((1e-3 < self.αs) & (self.αs < self.C - 1e-3))\n\n    if eval_train:  \n      print(f\"Finished training with accuracy {self.evaluate(X, y)}\")\n\n@SVMClass\ndef multi_fit(self, X, y, eval_train=False):\n    self.k = len(np.unique(y))      # number of classes\n    # for each pair of classes\n    for i in range(self.k):\n        # get the data for the pair\n        Xs, Ys = X, copy.copy(y)\n        # change the labels to -1 and 1\n        Ys[Ys!=i], Ys[Ys==i] = -1, +1\n        # fit the classifier\n        clf = SVM(kernel=self.kernel_str, C=self.C, k=self.k)\n        clf.fit(Xs, Ys)\n        # save the classifier\n        self.clfs.append(clf)\n    if eval_train:  \n      print(f\"Finished training with accuracy {self.evaluate(X, y)}\")\n\n@SVMClass\ndef predict(self, X_t):\n    if self.multiclass: return self.multi_predict(X_t)\n    xₛ, yₛ = self.X[self.margin_sv, np.newaxis], self.y[self.margin_sv]\n    αs, y, X= self.αs[self.is_sv], self.y[self.is_sv], self.X[self.is_sv]\n\n    b = yₛ - np.sum(αs * y * self.kernel(X, xₛ, self.k), axis=0)\n    score = np.sum(αs * y * self.kernel(X, X_t, self.k), axis=0) + b\n    return np.sign(score).astype(int), score\n\n@SVMClass\ndef multi_predict(self, X):\n    # get the predictions from all classifiers\n    preds = np.zeros((X.shape[0], self.k))\n    for i, clf in enumerate(self.clfs):\n        _, preds[:, i] = clf.predict(X)\n\n    # get the argmax and the corresponding score\n    return np.argmax(preds, axis=1)\n\n@SVMClass\ndef evaluate(self, X,y):  \n    outputs, _ = self.predict(X)\n    accuracy = np.sum(outputs == y) / len(y)\n    return round(accuracy, 2)\n\nfrom sklearn.datasets import make_classification\nimport numpy as np\n\n# Load the dataset\nnp.random.seed(1)\nX, y = make_classification(n_samples=500, n_features=2, \n          n_redundant=0, n_informative=2, n_classes=4, \n          n_clusters_per_class=1,  class_sep=0.3)\n\n# Test SVM\nsvm = SVM(kernel='rbf', k=4)\nsvm.fit(X, y, eval_train=True)\n\ny_pred = svm.predict(X)\nprint(f\"Accuracy: {np.sum(y==y_pred)/y.shape[0]}\")\n\n# Test with Scikit\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.svm import SVC\n\nclf = OneVsRestClassifier(SVC(kernel='rbf', C=1, gamma=4)).fit(X, y)\ny_pred = clf.predict(X)\nprint(f\"Accuracy: {sum(y==y_pred)/y.shape[0]}\") \n```"]