- en: The Complexities and Challenges of Integrating LLMs into Applications
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将大型语言模型（LLMs）集成到应用中的复杂性与挑战
- en: 原文：[https://towardsdatascience.com/the-complexities-and-challenges-of-integrating-llm-into-applications-913d4461bbe0?source=collection_archive---------0-----------------------#2023-07-08](https://towardsdatascience.com/the-complexities-and-challenges-of-integrating-llm-into-applications-913d4461bbe0?source=collection_archive---------0-----------------------#2023-07-08)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/the-complexities-and-challenges-of-integrating-llm-into-applications-913d4461bbe0?source=collection_archive---------0-----------------------#2023-07-08](https://towardsdatascience.com/the-complexities-and-challenges-of-integrating-llm-into-applications-913d4461bbe0?source=collection_archive---------0-----------------------#2023-07-08)
- en: Planning to integrate some LLM service into your code? Here are some of the
    common challenges you should expect when doing so
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 计划将某些 LLM 服务集成到您的代码中吗？这里有一些在进行集成时可能遇到的常见挑战
- en: '[](https://medium.com/@shahar.davidson?source=post_page-----913d4461bbe0--------------------------------)[![Shahar
    Davidson](../Images/2713dad2fa6610a1105d8dbf8f7dd66a.png)](https://medium.com/@shahar.davidson?source=post_page-----913d4461bbe0--------------------------------)[](https://towardsdatascience.com/?source=post_page-----913d4461bbe0--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----913d4461bbe0--------------------------------)
    [Shahar Davidson](https://medium.com/@shahar.davidson?source=post_page-----913d4461bbe0--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@shahar.davidson?source=post_page-----913d4461bbe0--------------------------------)[![Shahar
    Davidson](../Images/2713dad2fa6610a1105d8dbf8f7dd66a.png)](https://medium.com/@shahar.davidson?source=post_page-----913d4461bbe0--------------------------------)[](https://towardsdatascience.com/?source=post_page-----913d4461bbe0--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----913d4461bbe0--------------------------------)
    [Shahar Davidson](https://medium.com/@shahar.davidson?source=post_page-----913d4461bbe0--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ffa5cf0bcd8ab&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-complexities-and-challenges-of-integrating-llm-into-applications-913d4461bbe0&user=Shahar+Davidson&userId=fa5cf0bcd8ab&source=post_page-fa5cf0bcd8ab----913d4461bbe0---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----913d4461bbe0--------------------------------)
    ·7 min read·Jul 8, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F913d4461bbe0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-complexities-and-challenges-of-integrating-llm-into-applications-913d4461bbe0&user=Shahar+Davidson&userId=fa5cf0bcd8ab&source=-----913d4461bbe0---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ffa5cf0bcd8ab&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-complexities-and-challenges-of-integrating-llm-into-applications-913d4461bbe0&user=Shahar+Davidson&userId=fa5cf0bcd8ab&source=post_page-fa5cf0bcd8ab----913d4461bbe0---------------------post_header-----------)
    发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----913d4461bbe0--------------------------------)
    ·7分钟阅读·2023年7月8日 [](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F913d4461bbe0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-complexities-and-challenges-of-integrating-llm-into-applications-913d4461bbe0&user=Shahar+Davidson&userId=fa5cf0bcd8ab&source=-----913d4461bbe0---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F913d4461bbe0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-complexities-and-challenges-of-integrating-llm-into-applications-913d4461bbe0&source=-----913d4461bbe0---------------------bookmark_footer-----------)![](../Images/0d86770db058ef29cfeef1d0c5f18184.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F913d4461bbe0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-complexities-and-challenges-of-integrating-llm-into-applications-913d4461bbe0&source=-----913d4461bbe0---------------------bookmark_footer-----------)![](../Images/0d86770db058ef29cfeef1d0c5f18184.png)'
- en: Photo by [Christina @ wocintechchat.com](https://unsplash.com/es/@wocintechchat?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由 [Christina @ wocintechchat.com](https://unsplash.com/es/@wocintechchat?utm_source=medium&utm_medium=referral)
    提供，来源于 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: Large Language Models (LLMs) existed before OpenAI’s ChatGPT and GPT API were
    released. But, thanks to OpenAI’s efforts, GPT is now easily accessible to developers
    and non-developers. This launch has undoubtedly played a significant role in the
    recent resurgence of AI.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在OpenAI的ChatGPT和GPT API发布之前，大型语言模型（LLMs）已经存在。但由于OpenAI的努力，GPT现在对开发者和非开发者都易于获取。这次发布无疑在AI的近期复兴中发挥了重要作用。
- en: It is truly remarkable how quickly OpenAI’s GPT API was embraced within just
    six months of its launch. Virtually every SaaS service has incorporated it in
    some way to increase its users’ productivity.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI的GPT API在推出仅六个月内就被迅速接受，确实令人惊叹。几乎所有SaaS服务都以某种方式整合了它，以提高用户的生产力。
- en: However, only those who have completed the design and integration work of such
    APIs, genuinely understand the complexities and new challenges that arise from
    it.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，只有那些完成了此类API设计和集成工作的人，才能真正理解其中的复杂性和新挑战。
- en: Over the last few months, I have implemented several features that utilize OpenAI’s
    GPT API. Throughout this process, I have faced several challenges that seem common
    for anyone utilizing the GPT API or any other LLM API. By listing them out here,
    I hope to help engineering teams properly prepare and design their LLM-based features.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去几个月里，我实现了几个利用OpenAI GPT API的功能。在这个过程中，我遇到了几个似乎对任何使用GPT API或其他LLM API的人都很常见的挑战。通过在这里列出这些挑战，我希望能帮助工程团队正确准备和设计他们的LLM功能。
- en: Let’s take a look at some of the typical obstacles.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一些典型的障碍。
- en: Contextual Memory and Context Limitations
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**上下文记忆与上下文限制**'
- en: This is probably the most common challenge of all. The context for the LLM input
    is limited. Just recently, OpenAI released context support for 16K tokens, and
    in GPT-4 the context limitation can reach 32K, which is a good couple of pages
    (for example if you want the LLM to work on a large document holding a couple
    of pages). But there are many cases where you need more than that, especially
    when working with numerous documents, each having tens of pages (imagine a legal-tech
    company that needs to process tens of legal documents to extract answers using
    LLM).
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能是所有挑战中最常见的。LLM输入的上下文是有限的。就在最近，OpenAI发布了对16K tokens的上下文支持，在GPT-4中，上下文限制可以达到32K，这相当于几页（例如，如果你希望LLM处理一个包含几页的大文档）。但在许多情况下，你需要更多，尤其是当处理许多文档，每个文档有几十页时（比如想象一下一个需要处理数十份法律文档以提取答案的法律技术公司）。
- en: There are different [techniques](https://www.pinecone.io/learn/series/langchain/langchain-conversational-memory)
    to overcome this challenge, and [others](https://huggingface.co/papers/2306.07174)
    are emerging, but this would mean you must implement one or more of these techniques
    yourself. Yet another load of work to implement, test and maintain.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 有不同的[技术](https://www.pinecone.io/learn/series/langchain/langchain-conversational-memory)来克服这个挑战，还有[其他技术](https://huggingface.co/papers/2306.07174)正在出现，但这意味着你必须自己实现这些技术中的一种或多种。又一项需要实现、测试和维护的工作负担。
- en: Data Enrichment
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**数据增强**'
- en: Your LLM-based features likely take some sort of proprietary data as input.
    Whether you are inputting user data as part of the context or using other collected
    data or documents that you store, you need a simple mechanism that will abstract
    the calls of fetching data from the various data sources that you own.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 你的基于LLM的功能可能需要某种专有数据作为输入。无论你是将用户数据作为上下文的一部分，还是使用你存储的其他收集数据或文档，你都需要一个简单的机制来抽象从你拥有的各种数据源中获取数据的调用。
- en: Templating
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**模板化**'
- en: The prompt you submit to the LLM will contain hard-coded text and data from
    other data sources. This means that you will create a static template and dynamically
    fill in the blanks with data that should be part of the prompt in run-time. In
    other words, you will create templates for your prompts and likely have more than
    one.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 你提交给LLM的提示将包含硬编码的文本和来自其他数据源的数据。这意味着你将创建一个静态模板，并在运行时动态填充数据，这些数据应作为提示的一部分。换句话说，你将为你的提示创建模板，并且可能有多个。
- en: This means that you should be using some kind of templating framework because
    you probably don’t want your code to look like a bunch of string concatenations.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着你应该使用某种模板框架，因为你可能不希望你的代码看起来像一堆字符串拼接。
- en: This is not a big challenge but another task that should be considered.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 这不是一个大挑战，但另一个需要考虑的任务。
- en: Testing and Fine-tuning
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**测试与微调**'
- en: Getting the LLM to reach a satisfactory level of accuracy requires a lot of
    testing (sometimes it’s just prompt engineering with a lot of trial and error)
    and fine-tuning based on user feedback.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 使 LLM 达到令人满意的准确度需要大量的测试（有时这仅仅是通过大量的试错进行提示工程）和基于用户反馈的微调。
- en: There are of course also tests that run as part of the CI to assert that all
    integration work properly but that’s not the real challenge.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，还有作为 CI 部分运行的测试，以确保所有集成都正常工作，但这不是实际的挑战。
- en: When I say Testing, I’m talking about running the prompt repeatedly in a sandbox
    to fine-tune the results for accuracy.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 当我说测试时，我是指在沙盒中反复运行提示，以微调结果的准确性。
- en: For testing, you would want a method by which the testing engineer could change
    the templates, enrich them with the required data, and execute the prompt with
    the LLM to test that we’re getting what we wanted. How do you set up such a testing
    framework?
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 对于测试，你会需要一种方法，使测试工程师可以更改模板，用所需的数据丰富它们，并执行提示以测试我们是否得到了想要的结果。你如何设置这样的测试框架？
- en: In addition, we need to constantly fine-tune the LLM model by getting feedback
    from our users regarding the LLM outputs. How do we set up such a process?
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们需要通过获取用户对 LLM 输出的反馈来不断微调 LLM 模型。我们如何设置这样的过程？
- en: Caching
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 缓存
- en: LLM models, such as OpenAI’s GPT, have a parameter to control the randomness
    of answers, allowing the AI to be more creative. Yet if you are handling requests
    on a large scale, you will incur high charges on the API calls, you may hit rate
    limits, and your app performance might degrade. If some inputs to the LLM repeat
    themselves in different calls, you may consider caching the answer. For example,
    you handle 100K’s calls to your LLM-based feature. If all those calls trigger
    an API call to the LLM provider, then costs will be very high. Still, if inputs
    repeat themselves (this can potentially happen when you use templates and feed
    it with specific user fields), there’s a high chance that you can save some of
    the pre-processed LLM output and serve it from the cache.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: LLM 模型，如 OpenAI 的 GPT，有一个参数可以控制答案的随机性，从而让 AI 更具创造力。然而，如果你处理大规模的请求，你将会产生高昂的 API
    调用费用，可能会遇到速率限制，并且你的应用性能可能会下降。如果 LLM 的一些输入在不同调用中重复出现，你可能需要考虑缓存答案。例如，你处理了 100K 次调用你的
    LLM 基础功能。如果所有这些调用都触发对 LLM 提供商的 API 调用，那么成本将非常高。然而，如果输入重复出现（这可能在你使用模板并填入特定用户字段时发生），那么你有很高的机会可以保存一些预处理过的
    LLM 输出，并从缓存中提供服务。
- en: The challenge here is building a caching mechanism for that. It is not hard
    to implement that; it just adds another layer and moving part that needs to be
    maintained and done properly.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的挑战是为此构建一个缓存机制。这并不难实现；它只是增加了另一层需要维护和正确处理的部件。
- en: Security and Compliance
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 安全性和合规性
- en: Security and privacy are perhaps the most challenging aspects of this process
    — how do we ensure that the process created does not cause data leakage and how
    do we ensure that no [PII](https://www.technology.pitt.edu/help-desk/how-to-documents/guide-identifying-personally-identifiable-information-pii)
    is revealed?
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 安全性和隐私可能是这个过程最具挑战性的方面——我们如何确保创建的过程不会导致数据泄露，以及我们如何确保没有[个人身份信息](https://www.technology.pitt.edu/help-desk/how-to-documents/guide-identifying-personally-identifiable-information-pii)被暴露？
- en: In addition, you will need to audit all your actions so that all the actions
    can be examined to ensure that no data leak or privacy policy infringement happened.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，你还需要审计所有操作，以确保所有操作都可以检查，以确保没有数据泄露或隐私政策的违反。
- en: This is a common challenge for any software company that relies on 3rd party
    services, and it needs to be addressed here as well.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 这是任何依赖第三方服务的软件公司常见的挑战，也需要在这里解决。
- en: Observability
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 可观察性
- en: As with any external API you’re using, you must monitor its performance. Are
    there any errors? How long does the processing take? Are we exceeding or about
    to exceed the API’s rate limits or thresholds?
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 与任何你使用的外部 API 一样，你必须监控其性能。是否有错误？处理时间多长？我们是否超过或即将超过 API 的速率限制或阈值？
- en: In addition, you will want to log all calls, not just for security audit purposes
    but also to help you fine-tune your LLM workflow or prompts by grading the outputs.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，你还需要记录所有调用，不仅仅是为了安全审计目的，还为了通过评分输出来帮助你微调 LLM 工作流或提示。
- en: Workflow Management
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 工作流管理
- en: Let’s say we develop a legal-tech software that lawyers use to increase productivity.
    In our example, we have an LLM-based feature that takes a client’s details from
    a CRM system and the general description of the case worked on, and provides an
    answer for the lawyer’s query based on legal precedents.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们开发了一款律师用来提高生产力的法律技术软件。在我们的例子中，我们有一个基于LLM的功能，它从CRM系统中提取客户的详细信息和案件的一般描述，并基于法律先例为律师的查询提供答案。
- en: 'Let’s see what needs to be done to accomplish that:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看完成这些任务需要做些什么：
- en: Look up all the client’s details based on a given client ID.
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据给定的客户ID查找所有客户的详细信息。
- en: Look up all the details of the current case being worked on.
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 查找当前正在处理的案件的所有详细信息。
- en: Extract the relevant info from the current case being worked on using LLM, based
    on the lawyer’s query.
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用LLM根据律师的查询从当前正在处理的案件中提取相关信息。
- en: Combine all the above info onto a predefined question template.
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将上述所有信息组合到一个预定义的问题模板中。
- en: Enrich the context with the numerous legal cases. (recall the Contextual Memory
    challenge)
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 用大量法律案件丰富上下文。（回忆一下上下文记忆挑战）
- en: Have the LLM find the legal precedents that best match the current case, client,
    and lawyer’s query.
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让LLM找到最匹配当前案件、客户和律师查询的法律先例。
- en: Now, imagine that you have 2 or more features with such workflows, and finally
    try to imagine what your code looks like after you implement those workflows.
    I bet that just thinking about the work to be done here makes you move uncomfortably
    in your chair.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，想象一下你有2个或更多这样的功能，最后试着想象在实现这些工作流之后你的代码会是什么样的。我敢打赌，仅仅想到这里的工作就让你在椅子上感到不舒服。
- en: For your code to be maintainable and readable, you will need to implement various
    layers of abstraction and perhaps consider adopting or implementing some sort
    of workflow management framework, if you foresee more workflows in the future.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使你的代码具有可维护性和可读性，你需要实现各种抽象层，并且如果你预见到未来会有更多工作流，可能需要考虑采用或实现某种工作流管理框架。
- en: 'And finally, this example brings us to the next challenge:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，这个例子引出了下一个挑战：
- en: Strong Code Coupling
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 强耦合代码
- en: Now that you are aware of all the above challenges and the complexities that
    arise, you may start seeing that some of the tasks that need to be done should
    not be the developer’s responsibility.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经意识到上述所有挑战及其复杂性，你可能会开始看到一些需要完成的任务不应由开发人员负责。
- en: Specifically, all the tasks related to building workflows, testing, fine-tuning,
    monitoring the outcomes and external API usage can be done by someone more dedicated
    to those tasks and whose expertise is not building software. Let’s call this persona
    the *LLM engineer*.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，所有与构建工作流、测试、微调、监控结果和外部API使用相关的任务可以由更专注于这些任务且专长不在于软件开发的人来完成。我们称这个角色为*LLM工程师*。
- en: There’s no reason why the LLM workflows, testing, fine-tuning, and so on, would
    be placed in the software developer’s responsibility — software developers are
    experts at building software. At the same time, LLM engineers should be experts
    at building and fine-tuning the LLM workflows, not building software.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 没有理由将LLM工作流、测试、微调等工作放在软件开发人员的职责范围内——软件开发人员擅长构建软件。同时，LLM工程师应该擅长构建和微调LLM工作流，而不是开发软件。
- en: But with the current frameworks, the LLM workflow management is coupled into
    the codebase. Whoever is building these workflows needs to have the expertise
    of a software developer and an LLM engineer.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 但目前的框架中，LLM工作流管理与代码库耦合在一起。任何构建这些工作流的人都需要具备软件开发人员和LLM工程师的专业知识。
- en: There are ways to do the decoupling, such as creating a dedicate micro-service
    that handles all workflows, but this is yet another challenge that needs to be
    handled.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 有一些方法可以进行解耦，例如创建一个处理所有工作流的专用微服务，但这又是另一个需要解决的挑战。
- en: These were just some of the challenges.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 这些只是一些挑战。
- en: I didn’t get into how to solve all of these issues, because I’m still trying
    to figure it out myself. I can say however that [**LangChain**](https://python.langchain.com/docs/get_started/introduction.html)seems
    to be the only framework that somehow comes close to solving these issues, far
    from solving all of them, and seems to be in the right direction. As time passes,
    I believe that LLM providers will improve their offerings and provide platforms
    that can address these challenges to a certain extent.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 我没有深入探讨如何解决所有这些问题，因为我自己还在努力弄清楚。不过，我可以说，[**LangChain**](https://python.langchain.com/docs/get_started/introduction.html)似乎是唯一一个在某种程度上接近解决这些问题的框架，虽然距离完全解决还远，但方向似乎是对的。随着时间的推移，我相信LLM提供商会改进他们的产品，并提供能够在一定程度上应对这些挑战的平台。
- en: If you have any additional challenges to share, please do so in the comments
    for the benefit of other readers. Additionally, if you know of any tools that
    can help with these challenges, please share them in the comments as well.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你有任何额外的挑战要分享，请在评论中告诉我们，以便其他读者受益。此外，如果你知道任何可以帮助应对这些挑战的工具，也请在评论中分享。
- en: I hope you found this article insightful! If you did, please show your love
    by adding some applause and following me for more articles about team building,
    software engineering, and technology.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 希望你觉得这篇文章有启发！如果有，请通过鼓掌和关注我来表达你的支持，以便获取更多关于团队建设、软件工程和技术的文章。
