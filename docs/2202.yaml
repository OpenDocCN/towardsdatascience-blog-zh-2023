- en: The Complexities and Challenges of Integrating LLMs into Applications
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/the-complexities-and-challenges-of-integrating-llm-into-applications-913d4461bbe0?source=collection_archive---------0-----------------------#2023-07-08](https://towardsdatascience.com/the-complexities-and-challenges-of-integrating-llm-into-applications-913d4461bbe0?source=collection_archive---------0-----------------------#2023-07-08)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Planning to integrate some LLM service into your code? Here are some of the
    common challenges you should expect when doing so
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@shahar.davidson?source=post_page-----913d4461bbe0--------------------------------)[![Shahar
    Davidson](../Images/2713dad2fa6610a1105d8dbf8f7dd66a.png)](https://medium.com/@shahar.davidson?source=post_page-----913d4461bbe0--------------------------------)[](https://towardsdatascience.com/?source=post_page-----913d4461bbe0--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----913d4461bbe0--------------------------------)
    [Shahar Davidson](https://medium.com/@shahar.davidson?source=post_page-----913d4461bbe0--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ffa5cf0bcd8ab&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-complexities-and-challenges-of-integrating-llm-into-applications-913d4461bbe0&user=Shahar+Davidson&userId=fa5cf0bcd8ab&source=post_page-fa5cf0bcd8ab----913d4461bbe0---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----913d4461bbe0--------------------------------)
    ·7 min read·Jul 8, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F913d4461bbe0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-complexities-and-challenges-of-integrating-llm-into-applications-913d4461bbe0&user=Shahar+Davidson&userId=fa5cf0bcd8ab&source=-----913d4461bbe0---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F913d4461bbe0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-complexities-and-challenges-of-integrating-llm-into-applications-913d4461bbe0&source=-----913d4461bbe0---------------------bookmark_footer-----------)![](../Images/0d86770db058ef29cfeef1d0c5f18184.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Photo by [Christina @ wocintechchat.com](https://unsplash.com/es/@wocintechchat?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: Large Language Models (LLMs) existed before OpenAI’s ChatGPT and GPT API were
    released. But, thanks to OpenAI’s efforts, GPT is now easily accessible to developers
    and non-developers. This launch has undoubtedly played a significant role in the
    recent resurgence of AI.
  prefs: []
  type: TYPE_NORMAL
- en: It is truly remarkable how quickly OpenAI’s GPT API was embraced within just
    six months of its launch. Virtually every SaaS service has incorporated it in
    some way to increase its users’ productivity.
  prefs: []
  type: TYPE_NORMAL
- en: However, only those who have completed the design and integration work of such
    APIs, genuinely understand the complexities and new challenges that arise from
    it.
  prefs: []
  type: TYPE_NORMAL
- en: Over the last few months, I have implemented several features that utilize OpenAI’s
    GPT API. Throughout this process, I have faced several challenges that seem common
    for anyone utilizing the GPT API or any other LLM API. By listing them out here,
    I hope to help engineering teams properly prepare and design their LLM-based features.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s take a look at some of the typical obstacles.
  prefs: []
  type: TYPE_NORMAL
- en: Contextual Memory and Context Limitations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This is probably the most common challenge of all. The context for the LLM input
    is limited. Just recently, OpenAI released context support for 16K tokens, and
    in GPT-4 the context limitation can reach 32K, which is a good couple of pages
    (for example if you want the LLM to work on a large document holding a couple
    of pages). But there are many cases where you need more than that, especially
    when working with numerous documents, each having tens of pages (imagine a legal-tech
    company that needs to process tens of legal documents to extract answers using
    LLM).
  prefs: []
  type: TYPE_NORMAL
- en: There are different [techniques](https://www.pinecone.io/learn/series/langchain/langchain-conversational-memory)
    to overcome this challenge, and [others](https://huggingface.co/papers/2306.07174)
    are emerging, but this would mean you must implement one or more of these techniques
    yourself. Yet another load of work to implement, test and maintain.
  prefs: []
  type: TYPE_NORMAL
- en: Data Enrichment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Your LLM-based features likely take some sort of proprietary data as input.
    Whether you are inputting user data as part of the context or using other collected
    data or documents that you store, you need a simple mechanism that will abstract
    the calls of fetching data from the various data sources that you own.
  prefs: []
  type: TYPE_NORMAL
- en: Templating
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The prompt you submit to the LLM will contain hard-coded text and data from
    other data sources. This means that you will create a static template and dynamically
    fill in the blanks with data that should be part of the prompt in run-time. In
    other words, you will create templates for your prompts and likely have more than
    one.
  prefs: []
  type: TYPE_NORMAL
- en: This means that you should be using some kind of templating framework because
    you probably don’t want your code to look like a bunch of string concatenations.
  prefs: []
  type: TYPE_NORMAL
- en: This is not a big challenge but another task that should be considered.
  prefs: []
  type: TYPE_NORMAL
- en: Testing and Fine-tuning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Getting the LLM to reach a satisfactory level of accuracy requires a lot of
    testing (sometimes it’s just prompt engineering with a lot of trial and error)
    and fine-tuning based on user feedback.
  prefs: []
  type: TYPE_NORMAL
- en: There are of course also tests that run as part of the CI to assert that all
    integration work properly but that’s not the real challenge.
  prefs: []
  type: TYPE_NORMAL
- en: When I say Testing, I’m talking about running the prompt repeatedly in a sandbox
    to fine-tune the results for accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: For testing, you would want a method by which the testing engineer could change
    the templates, enrich them with the required data, and execute the prompt with
    the LLM to test that we’re getting what we wanted. How do you set up such a testing
    framework?
  prefs: []
  type: TYPE_NORMAL
- en: In addition, we need to constantly fine-tune the LLM model by getting feedback
    from our users regarding the LLM outputs. How do we set up such a process?
  prefs: []
  type: TYPE_NORMAL
- en: Caching
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: LLM models, such as OpenAI’s GPT, have a parameter to control the randomness
    of answers, allowing the AI to be more creative. Yet if you are handling requests
    on a large scale, you will incur high charges on the API calls, you may hit rate
    limits, and your app performance might degrade. If some inputs to the LLM repeat
    themselves in different calls, you may consider caching the answer. For example,
    you handle 100K’s calls to your LLM-based feature. If all those calls trigger
    an API call to the LLM provider, then costs will be very high. Still, if inputs
    repeat themselves (this can potentially happen when you use templates and feed
    it with specific user fields), there’s a high chance that you can save some of
    the pre-processed LLM output and serve it from the cache.
  prefs: []
  type: TYPE_NORMAL
- en: The challenge here is building a caching mechanism for that. It is not hard
    to implement that; it just adds another layer and moving part that needs to be
    maintained and done properly.
  prefs: []
  type: TYPE_NORMAL
- en: Security and Compliance
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Security and privacy are perhaps the most challenging aspects of this process
    — how do we ensure that the process created does not cause data leakage and how
    do we ensure that no [PII](https://www.technology.pitt.edu/help-desk/how-to-documents/guide-identifying-personally-identifiable-information-pii)
    is revealed?
  prefs: []
  type: TYPE_NORMAL
- en: In addition, you will need to audit all your actions so that all the actions
    can be examined to ensure that no data leak or privacy policy infringement happened.
  prefs: []
  type: TYPE_NORMAL
- en: This is a common challenge for any software company that relies on 3rd party
    services, and it needs to be addressed here as well.
  prefs: []
  type: TYPE_NORMAL
- en: Observability
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As with any external API you’re using, you must monitor its performance. Are
    there any errors? How long does the processing take? Are we exceeding or about
    to exceed the API’s rate limits or thresholds?
  prefs: []
  type: TYPE_NORMAL
- en: In addition, you will want to log all calls, not just for security audit purposes
    but also to help you fine-tune your LLM workflow or prompts by grading the outputs.
  prefs: []
  type: TYPE_NORMAL
- en: Workflow Management
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s say we develop a legal-tech software that lawyers use to increase productivity.
    In our example, we have an LLM-based feature that takes a client’s details from
    a CRM system and the general description of the case worked on, and provides an
    answer for the lawyer’s query based on legal precedents.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s see what needs to be done to accomplish that:'
  prefs: []
  type: TYPE_NORMAL
- en: Look up all the client’s details based on a given client ID.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Look up all the details of the current case being worked on.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Extract the relevant info from the current case being worked on using LLM, based
    on the lawyer’s query.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Combine all the above info onto a predefined question template.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Enrich the context with the numerous legal cases. (recall the Contextual Memory
    challenge)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Have the LLM find the legal precedents that best match the current case, client,
    and lawyer’s query.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now, imagine that you have 2 or more features with such workflows, and finally
    try to imagine what your code looks like after you implement those workflows.
    I bet that just thinking about the work to be done here makes you move uncomfortably
    in your chair.
  prefs: []
  type: TYPE_NORMAL
- en: For your code to be maintainable and readable, you will need to implement various
    layers of abstraction and perhaps consider adopting or implementing some sort
    of workflow management framework, if you foresee more workflows in the future.
  prefs: []
  type: TYPE_NORMAL
- en: 'And finally, this example brings us to the next challenge:'
  prefs: []
  type: TYPE_NORMAL
- en: Strong Code Coupling
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now that you are aware of all the above challenges and the complexities that
    arise, you may start seeing that some of the tasks that need to be done should
    not be the developer’s responsibility.
  prefs: []
  type: TYPE_NORMAL
- en: Specifically, all the tasks related to building workflows, testing, fine-tuning,
    monitoring the outcomes and external API usage can be done by someone more dedicated
    to those tasks and whose expertise is not building software. Let’s call this persona
    the *LLM engineer*.
  prefs: []
  type: TYPE_NORMAL
- en: There’s no reason why the LLM workflows, testing, fine-tuning, and so on, would
    be placed in the software developer’s responsibility — software developers are
    experts at building software. At the same time, LLM engineers should be experts
    at building and fine-tuning the LLM workflows, not building software.
  prefs: []
  type: TYPE_NORMAL
- en: But with the current frameworks, the LLM workflow management is coupled into
    the codebase. Whoever is building these workflows needs to have the expertise
    of a software developer and an LLM engineer.
  prefs: []
  type: TYPE_NORMAL
- en: There are ways to do the decoupling, such as creating a dedicate micro-service
    that handles all workflows, but this is yet another challenge that needs to be
    handled.
  prefs: []
  type: TYPE_NORMAL
- en: These were just some of the challenges.
  prefs: []
  type: TYPE_NORMAL
- en: I didn’t get into how to solve all of these issues, because I’m still trying
    to figure it out myself. I can say however that [**LangChain**](https://python.langchain.com/docs/get_started/introduction.html)seems
    to be the only framework that somehow comes close to solving these issues, far
    from solving all of them, and seems to be in the right direction. As time passes,
    I believe that LLM providers will improve their offerings and provide platforms
    that can address these challenges to a certain extent.
  prefs: []
  type: TYPE_NORMAL
- en: If you have any additional challenges to share, please do so in the comments
    for the benefit of other readers. Additionally, if you know of any tools that
    can help with these challenges, please share them in the comments as well.
  prefs: []
  type: TYPE_NORMAL
- en: I hope you found this article insightful! If you did, please show your love
    by adding some applause and following me for more articles about team building,
    software engineering, and technology.
  prefs: []
  type: TYPE_NORMAL
