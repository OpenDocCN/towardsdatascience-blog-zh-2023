- en: The Future of the Modern Data Stack in 2023
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/the-future-of-the-modern-data-stack-in-2023-b08c2aed04e2?source=collection_archive---------0-----------------------#2023-01-13](https://towardsdatascience.com/the-future-of-the-modern-data-stack-in-2023-b08c2aed04e2?source=collection_archive---------0-----------------------#2023-01-13)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Featuring 4 new emerging trends and 6 big trends from last year
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://prukalpa.medium.com/?source=post_page-----b08c2aed04e2--------------------------------)[![Prukalpa](../Images/07d7e31131ba88fe8918c2a10ab8c7aa.png)](https://prukalpa.medium.com/?source=post_page-----b08c2aed04e2--------------------------------)[](https://towardsdatascience.com/?source=post_page-----b08c2aed04e2--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----b08c2aed04e2--------------------------------)
    [Prukalpa](https://prukalpa.medium.com/?source=post_page-----b08c2aed04e2--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F38a5ef6ab673&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-future-of-the-modern-data-stack-in-2023-b08c2aed04e2&user=Prukalpa&userId=38a5ef6ab673&source=post_page-38a5ef6ab673----b08c2aed04e2---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----b08c2aed04e2--------------------------------)
    ·19 min read·Jan 13, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fb08c2aed04e2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-future-of-the-modern-data-stack-in-2023-b08c2aed04e2&user=Prukalpa&userId=38a5ef6ab673&source=-----b08c2aed04e2---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb08c2aed04e2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-future-of-the-modern-data-stack-in-2023-b08c2aed04e2&source=-----b08c2aed04e2---------------------bookmark_footer-----------)![](../Images/7bbb93869b193a8a0ff37c3ccb77caf5.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Photo by [Nicholas Cappello](https://unsplash.com/@bash__profile?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    on [Unsplash](https://unsplash.com/photos/Wb63zqJ5gnE?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
  prefs: []
  type: TYPE_NORMAL
- en: '**This article was co-written with Christine Garcia (Director of Content at
    Atlan).**'
  prefs: []
  type: TYPE_NORMAL
- en: In my [annual tradition](/the-future-of-the-modern-data-stack-in-2022-4f4c91bb778f),
    I spent some downtime at the end of 2022 reflecting on what happened in the data
    world. As you can tell by the length of this article, my co-author Christine and
    I had a lot of thoughts.
  prefs: []
  type: TYPE_NORMAL
- en: For the last few years, data has been in hypergrowth mode. The community has
    been stirring up controversy with [hot](https://pedram.substack.com/p/we-need-to-talk-about-dbt)
    [takes](https://benn.substack.com/p/we-all-have-an-audience), debating the latest
    [tech](https://benn.substack.com/p/should-we-be-grateful), [raising](https://ryxcommar.com/2022/11/27/goodbye-data-science/)
    [important](https://www.linkedin.com/pulse/data-teams-break-out-your-bubble-mary-maccarthy/)
    [conversations](https://benn.substack.com/p/the-technical-pay-gap), and duking
    it out on Twitter with [Friday fights](https://roundup.getdbt.com/p/benn-stancil-friday-night-data-fights).
    The data world seemed infinite, and everyone was just trying to keep up as it
    exploded.
  prefs: []
  type: TYPE_NORMAL
- en: Now data is entering a different world. [98% of CEOs](https://www.npr.org/2022/10/13/1128624034/ceos-no-longer-question-if-there-will-be-a-recession-the-question-is-when)
    expect a recession for the next 12–18 months. Companies are preparing for war
    by amping up the pressure, laying people off, cutting budgets, and shifting from
    growth mode to efficiency mode.
  prefs: []
  type: TYPE_NORMAL
- en: So what does this mean for the data world? And, more importantly, for data leaders
    and practitioners? This article breaks down the 10 big trends for 2023 that anyone
    in the data space should know about—4 emerging trends that will be a big deal,
    and 6 existing trends that are poised to grow even further.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/061e7493762d8458ba04924fa5367a06.png)'
  prefs: []
  type: TYPE_IMG
- en: With the recent economic downswing, the tech world is looking into 2023 with
    a new focus on efficiency and cost-cutting. This will lead to four new trends
    related to how modern data stack companies and data teams operate.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/459a58a022e492e4f9ce6e030008dcd5.png)'
  prefs: []
  type: TYPE_IMG
- en: Storage has always been one of the biggest costs for data teams. For example,
    Netflix spent [$9.6 million](https://www.cloudzero.com/blog/netflix-aws) per month
    on AWS data storage. As companies tighten their budgets, they’ll need to take
    a hard look at these bills.
  prefs: []
  type: TYPE_NORMAL
- en: '**Snowflake and Databricks have already been investing in product optimization.
    They’ll likely introduce even more improvements to help customers cut costs this
    year.**'
  prefs: []
  type: TYPE_NORMAL
- en: For example, in its June conference, Snowflake highlighted [product improvements](https://www.snowflake.com/blog/performance-improvements-summit-2022/)
    to speed up queries, reduce compute time, and cut costs. It announced 10% average
    faster compute on AWS, 10–40% faster performance for write-heavy DML workloads,
    and 7–10% lower storage costs from better compression.
  prefs: []
  type: TYPE_NORMAL
- en: At its June conference, Databricks also devoted part of its [keynote](https://humansofdata.atlan.com/2022/07/data-ai-summit-dais-2022-announcements-keynotes/)
    to cost-saving product improvements, such as the launches of [Enzyme](https://www.databricks.com/blog/2022/06/29/delta-live-tables-announces-new-capabilities-and-performance-optimizations.html)
    (an automatic optimizer for ETL pipelines) and [Photon](https://www.databricks.com/blog/2022/08/03/announcing-photon-engine-general-availability-on-the-databricks-lakehouse-platform.html)
    (a query engine with up to 12x better price to performance).
  prefs: []
  type: TYPE_NORMAL
- en: Later in the year, both Snowflake and Databricks [doubled](https://www.snowflake.com/blog/performance-enhancements-making-snowflake-faster-more-efficient/)
    [down](https://docs.databricks.com/optimizations/index.html) by investing further
    in cost optimization features, and more are sure to come next year. Snowflake
    even highlighted cost-cutting as one of its [top data trends for 2023](https://resources.snowflake.com/webinars-thought-leadership/2023-predictions-the-top-data-trends-to-watch-in-the-new-year-2)
    and affirmed its commitment to minimizing cost while increasing performance.
  prefs: []
  type: TYPE_NORMAL
- en: '**In 2023, we’ll also see the growth of tooling from independent companies
    and storage partners to further reduce data costs.**'
  prefs: []
  type: TYPE_NORMAL
- en: '[Dark data](https://metadataweekly.substack.com/p/the-dark-underbelly-of-your-data),
    or data that never actually gets used, is a serious problem for data teams. Up
    to [68% of data](https://www.seagate.com/our-story/rethink-data/) goes unused,
    even though companies are still paying to store it.'
  prefs: []
  type: TYPE_NORMAL
- en: This year will include the growth of cost-management tools like [Bluesky](https://www.getbluesky.io/),
    [CloudZero](https://www.cloudzero.com/), and [Slingshot](https://www.capitalone.com/software/solutions/)
    designed to work with specific data storage systems like Snowflake and Databricks.
  prefs: []
  type: TYPE_NORMAL
- en: Modern data stack partners will also likely introduce compatible optimization
    features, like dbt’s [incremental models](https://docs.getdbt.com/docs/build/incremental-models)
    and [packages](https://docs.getdbt.com/docs/build/packages). dbt Labs and Snowflake
    even wrote an entire [white paper](https://www.snowflake.com/wp-content/uploads/2021/10/Best-Practices-for-Optimizing-Your-dbt-and-Snowflake-Deployment.pdf)
    together on optimizing your data with dbt and Snowflake.
  prefs: []
  type: TYPE_NORMAL
- en: '**Metadata also has a big role to play here.** With a modern metadata platform,
    data teams can use popularity metrics to find unused data assets, column-level
    lineage to see when assets aren’t connected to pipelines, redundancy features
    to delete duplicate data, and more. Much of this can even be automated with [active
    metadata](https://prukalpa.medium.com/what-is-active-metadata-and-why-does-it-matter-add3408c228),
    like automatically optimizing data processing or purging stale data assets.'
  prefs: []
  type: TYPE_NORMAL
- en: For example, a data team we work with reduced their monthly storage costs by
    **$50,000** just by finding and removing an unused BigQuery table. Another team
    deprecated **30,000 unused assets** (or two-thirds of their data estate) by finding
    tables, views, and schemas that weren’t used upstream.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e46d278a0be10c9a2ef32bb8ac3ca5ee.png)'
  prefs: []
  type: TYPE_IMG
- en: In the past few years, data teams have been able to run free with less regulation
    and oversight.
  prefs: []
  type: TYPE_NORMAL
- en: '**Companies have so much belief in the power and value of data that data teams
    haven’t always been required to prove that value.** Instead, they’ve chugged along,
    balancing daily data work with forward-looking tech, process, and culture experiments.
    Optimizing how data people work has always been part of the data discussion, but
    it’s often relegated to more pressing concerns like building a super cool tech
    stack.'
  prefs: []
  type: TYPE_NORMAL
- en: Next year, this will no longer cut it. As budgets tighten, data teams and their
    stacks will get more attention and scrutiny. How much do they cost, and how much
    value are they providing? Data teams will need to focus on performance and efficiency.
  prefs: []
  type: TYPE_NORMAL
- en: '**In 2023, companies will get more serious about measuring data ROI, and data
    team metrics will start becoming mainstream.**'
  prefs: []
  type: TYPE_NORMAL
- en: It’s not easy to measure ROI for a function as fundamental as data, but it’s
    more important than ever that we figure it out.
  prefs: []
  type: TYPE_NORMAL
- en: This year, data teams will start developing proxy metrics to measure their value.
    This may include **usage metrics** like data usage (e.g. DAU, WAU, MAU, and QUA),
    page views or time spent on data assets, and data product adoption; **satisfaction
    metrics** like a d-NPS score for data consumers; and **trust metrics** like data
    downtime and data quality scores.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/20bf1207da4a975180779357a0fd7f10.png)'
  prefs: []
  type: TYPE_IMG
- en: For years, the modern data stack has been growing. And growing. And growing
    some more.
  prefs: []
  type: TYPE_NORMAL
- en: As VCs pumped in millions of dollars in funding, new tools and categories popped
    up every day. But now, with the economic downturn, this growth phase is over.
    VC money has already been drying up — just look at the decrease in funding announcements
    over the last six months.
  prefs: []
  type: TYPE_NORMAL
- en: '**We’ll see fewer data companies and tools launching next year and slower expansion
    for existing companies.** Ultimately, this is probably good for buyers and the
    modern data stack as a whole.'
  prefs: []
  type: TYPE_NORMAL
- en: Yes, hypergrowth mode is fun and exciting, but it’s also chaotic. We used to
    joke that it would suck to be a data buyer right now, with everyone claiming to
    do everything. The result is some truly [wild](https://mattturck.com/data2021/)
    stack diagrams.
  prefs: []
  type: TYPE_NORMAL
- en: This lack of capital will force today’s data companies to focus on what matters
    and ignore the rest. That means fewer “nice to have” features. Fewer splashy pivots.
    Fewer acquisitions that make us wonder “Why did they do that?”
  prefs: []
  type: TYPE_NORMAL
- en: With limited funds, companies will have to focus on what they do best and partner
    with other companies for everything else, rather than trying to tackle every data
    problem in one platform. **This will lead to the creation of the “best-in-class
    modern data stack” in 2023.**
  prefs: []
  type: TYPE_NORMAL
- en: As the chaos calms down and data companies focus on their core USP, the winners
    of each category will start to become clear.
  prefs: []
  type: TYPE_NORMAL
- en: '**These tools will also focus on working even better with each other.** They’ll
    act as launch partners, aligning behind common standards and pushing the modern
    data stack forward. (A couple of examples from last year are Fivetran’s [Metadata
    API](https://www.fivetran.com/blog/governing-data-movement-with-fivetran-metadata-api)
    and dbt’s [Semantic Layer](https://www.getdbt.com/blog/frontiers-of-the-dbt-semantic-layer/),
    where close partners like us built integrations in advance and celebrated the
    launch as much as Fivetran and dbt Labs.)'
  prefs: []
  type: TYPE_NORMAL
- en: These partnerships and consolidation will make it easier for buyers to choose
    tools and get started quickly, a welcome change from how things have been.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/117ac0546b781f58c542ae40d2ce5482.png)'
  prefs: []
  type: TYPE_IMG
- en: Tech companies are facing new pressure to cut costs and increase revenue in
    2023\. One way to do this is by focusing on their core functions, as mentioned
    above. Another way is seeking out new customers.
  prefs: []
  type: TYPE_NORMAL
- en: Guess what the largest untapped source of data customers is today? Enterprise
    companies with legacy, on-premise data systems. To serve these new customers,
    modern data stack companies will have to start supporting legacy tools.
  prefs: []
  type: TYPE_NORMAL
- en: '**In 2023, the modern data stack will start to integrate with Oracle and SAP,
    the two enterprise data behemoths.**'
  prefs: []
  type: TYPE_NORMAL
- en: This may sound controversial, but it’s already begun. The modern data stack
    started reaching into the on-prem, enterprise data world over a year ago.
  prefs: []
  type: TYPE_NORMAL
- en: In October 2021, Fivetran [acquired HVR](https://www.businesswire.com/news/home/20210920005344/en/Fivetran-to-Acquire-HVR-Announces-565-Million-in-Series-D-Funding),
    an enterprise data replication tool. Fivetran said that this would allow it to
    “address the massive market for modernizing analytics for operational data associated
    with ERP systems, Oracle databases, and more”. **This was the first major move
    from a modern data stack company into the enterprise market.**
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3602c10f0f34144d32f21cac263165ca.png)'
  prefs: []
  type: TYPE_IMG
- en: These are six of the big ideas that blew up in the data world last year and
    only promise to get bigger in 2023.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/709abd1a23571c25fb475305a7d530c1.png)'
  prefs: []
  type: TYPE_IMG
- en: This was one of the big trends from [last year’s article](/the-future-of-the-modern-data-stack-in-2022-4f4c91bb778f),
    so it’s not surprising that it’s still a hot topic in the data world. What was
    surprising, though, was how fast the ideas of [active metadata](/what-is-active-metadata-and-why-does-it-matter-add3408c228)
    and [third-generation data catalogs](https://prukalpa.medium.com/data-catalog-3-0-modern-metadata-for-the-modern-data-stack-ec621f593dcf)
    continued to grow.
  prefs: []
  type: TYPE_NORMAL
- en: '**In a major shift from 2021, when these ideas were new and few people were
    talking about them, many companies are now competing to claim the category.**'
  prefs: []
  type: TYPE_NORMAL
- en: Metadata is seen as one of the big gaps in the data world, so even as VC funding
    started to dry up, there were some big raises in the cataloging space last year.
    These included Alation’s [$123M Series E](https://www.crn.com/news/software/data-intelligence-tech-provider-alation-raises-123m-in-latest-funding-round),
    Data.world’s [$50M Series C](https://techcrunch.com/2022/04/05/data-world-raises-50m-to-help-enterprises-organize-and-track-their-data/),
    our [$50M Series B](https://www.businesswire.com/news/home/20220309005103/en/Atlan-Raises-50M-Series-B-at-450M-valuation-to-Build-a-Collaboration-Hub-for-Data-Teams),
    and Castor’s [$23.5M Series A](https://techcrunch.com/2022/06/07/castor-a-data-catalog-startup-nabs-23-5m-to-expand-its-platform/).
  prefs: []
  type: TYPE_NORMAL
- en: The other cause for this growth was analysts, who embraced and amplified the
    ideas of active metadata and modern data catalogs in 2022\. For example, Gartner
    went all in on active metadata at its [annual conference](/key-takeaways-from-gartner-data-analytics-summit-2022-ec908f9599df)
    and G2 released a new “[Active Metadata](https://www.g2.com/articles/active-metadata-management-category-on-g2)”
    category.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/bc2b8ac971e48b9cb374e8ded5aa3914.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Our prediction is that active metadata platforms will replace the “data catalog”
    category in 2023.**'
  prefs: []
  type: TYPE_NORMAL
- en: This actually started last year when Forrester renamed its Wave report on “[Machine
    Learning Data Catalogs](/forrester-changed-the-way-they-think-about-data-catalogs-and-heres-what-you-need-to-know-139e13a5869d)”
    and reversed its rankings. It moved the 2021 Leaders (Alation, IBM, and Collibra)
    to the bottom and middle tiers of its 2022 Wave report, and replaced them with
    a new set of companies (us, Data.world, and Informatica).
  prefs: []
  type: TYPE_NORMAL
- en: 'The “data catalog” is just a single use case of metadata: helping users understand
    their data assets. But that barely scratches the surface of what metadata can
    do.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Activating metadata holds the key to dozens of use cases like observability,
    cost management, remediation, quality, security, programmatic governance, optimized
    pipelines, and more — all of which are already being actively debated in the data
    world. Here are a few [real examples](https://humansofdata.atlan.com/2022/12/introducing-supercharged-automation/):'
  prefs: []
  type: TYPE_NORMAL
- en: '**Eventbridge event-based actions**: Allows data teams to create production-grade,
    event-driven metadata automations, like alerts when ownership changes or auto-tagging
    classifications.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Trident AI**: Uses the power of GPT-3 to automatically create descriptions
    and READMEs for new data assets, based on metadata from earlier assets.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**GitHub integration**: Automatically creates a list of affected data assets
    during each GitHub pull request.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/d78dc5f3d8b4ddb1b223c83109c553f0.png)'
  prefs: []
  type: TYPE_IMG
- en: '[By Josh Wills on Twitter](https://twitter.com/josh_wills/status/1520082864234106881)'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/63840df32407305a262362dd345d4fd8.png)'
  prefs: []
  type: TYPE_IMG
- en: This started in August with Chad Sanderson’s newsletter on [“The Rise of Data
    Contracts”](https://dataproducts.substack.com/p/the-rise-of-data-contracts). He
    later followed this up with a [two-part technical guide](https://dataproducts.substack.com/p/an-engineers-guide-to-data-contracts)
    to data contracts with Adrian Kreuziger. He then spoke about data contracts on
    the [Analytics Engineering Podcast](https://open.spotify.com/episode/65Hs5C3yAJI138ZAGPdHhd)
    — with us! (Shoutout to Chad, Tristan Handy, and Julia Schottenstein for a great
    chat.)
  prefs: []
  type: TYPE_NORMAL
- en: '**The core driver of data contracts is that engineers have no incentive to
    create high-quality data.**'
  prefs: []
  type: TYPE_NORMAL
- en: Because of the modern data stack, the people who create data have been separated
    from the people who consume it. As a result, companies end up with GIGO data systems
    — garbage in, garbage out.
  prefs: []
  type: TYPE_NORMAL
- en: The data contract aims to solve this by creating an agreement between data producers
    and consumers. Data producers commit to producing data that adheres to certain
    rules — e.g. a set data schema, SLAs around accuracy or completeness, and policies
    on how the data can be used and changed. After agreeing on the contract, data
    consumers can create downstream applications with this data, assured that engineers
    won’t unexpectedly change the data and break live data assets.
  prefs: []
  type: TYPE_NORMAL
- en: After Chad Sanderson’s newsletter went live, this conversation blew up. It spread
    across Twitter and Substack, where the data community argued whether data contracts
    were an [important conversation](https://twitter.com/jamesdensmore/status/1572993179665711105),
    [frustratingly vague](https://benn.substack.com/p/data-contracts) or [self-evident](https://twitter.com/josh_wills/status/1580968665737883650),
    [not actually a tech problem](https://twitter.com/sarahmk125/status/1580168014023909376),
    [doomed to fail](https://stkbailey.substack.com/p/data-person-attorney-at-law),
    or [obviously a good idea](https://datacreation.substack.com/p/why-data-contracts-are-obviously).
    People hosted [Twitter fights](https://twitter.com/sarahcat21/status/1562094057534369792),
    created [epic threads](https://twitter.com/sarahmk125/status/1580168014023909376),
    and watched [battle royales](https://www.linkedin.com/video/event/urn:li:ugcPost:6970843266061664256/)
    from a safe distance, popcorn in hand.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/12a794d80a4654c91aaee4c9683b3bc6.png)'
  prefs: []
  type: TYPE_IMG
- en: '**While data contracts are an important issue in their own right, they’re part
    of a larger conversation about how to ensure data quality.**'
  prefs: []
  type: TYPE_NORMAL
- en: It’s no secret that data is often outdated or incomplete or incorrect — the
    data community has been talking about how to fix it for years. First, people said
    that metadata documentation was the solution, then it was data product shipping
    standards. Now the buzzword is data contracts.
  prefs: []
  type: TYPE_NORMAL
- en: This is not to dismiss data contracts, which may be the solution we’ve been
    waiting for. But it seems more likely that data contracts will be subsumed in
    a larger trend around data governance.
  prefs: []
  type: TYPE_NORMAL
- en: '**In 2023, data governance will start shifting “left”, and data standards will
    become a first-class citizen in orchestration tools.**'
  prefs: []
  type: TYPE_NORMAL
- en: For decades, [data governance](https://prukalpa.medium.com/data-governance-has-a-serious-branding-problem-7925b909712b)
    has been an afterthought. It’s often handled by data stewards, not data producers,
    who create documentation long after data is created.
  prefs: []
  type: TYPE_NORMAL
- en: However, we’ve recently seen a shift to move data governance “left”, or closer
    to data producers. This means that whoever creates the data (usually a developer
    or engineer) must create documentation and check the data against pre-defined
    standards before it can go live.
  prefs: []
  type: TYPE_NORMAL
- en: '**Major tools have recently made changes that support this idea, and we expect
    to see even more in the coming year:**'
  prefs: []
  type: TYPE_NORMAL
- en: dbt’s [yaml files](https://docs.getdbt.com/reference/dbt_project.yml) and [Semantic
    Layer](https://www.getdbt.com/product/semantic-layer/), where analytics engineers
    can create READMEs and define metrics while creating a dbt model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Airflow’s [Open Lineage](https://openlineage.io/integration/apache-airflow/),
    which tracks metadata about jobs and datasets as DAGs execute
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fivetran’s [Metadata API](https://www.fivetran.com/blog/governing-data-movement-with-fivetran-metadata-api),
    which provides metadata for data synced by Fivetran connectors
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Atlan’s [GitHub extension](https://humansofdata.atlan.com/2022/12/introducing-supercharged-automation/),
    which creates a list of downstream assets that will be affected by a pull request
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/aa75d019fdcec51ac00a2d9a8069372b.png)'
  prefs: []
  type: TYPE_IMG
- en: Also called a “metrics layer” or “business layer”, the semantic layer is an
    idea that’s been floating around the data world for [decades](https://humansofdata.atlan.com/2022/11/past-present-future-semantic-layer/).
  prefs: []
  type: TYPE_NORMAL
- en: The semantic layer is a literal term — it’s the “layer” in a data architecture
    that uses “semantics” (words) that the business user will understand. Instead
    of raw tables with column names like “A000_CUST_ID_PROD”, data teams build a semantic
    layer and rename that column “Customer”. Semantic layers hide complex code from
    business users while keeping it well-documented and accessible for data teams.
  prefs: []
  type: TYPE_NORMAL
- en: '**In October 2022, dbt Labs made a big splash at its** [**annual conference**](https://coalesce.getdbt.com/)
    **by announcing the new Semantic Layer.**'
  prefs: []
  type: TYPE_NORMAL
- en: 'The core concept behind [dbt’s Semantic Layer](https://www.getdbt.com/blog/frontiers-of-the-dbt-semantic-layer/):
    define things once, use them anywhere. Data producers can now define metrics in
    dbt, then data consumers can query those consistent metrics in downstream tools.
    Regardless of which BI tool they use, analysts and business users can look up
    a stat in the middle of a meeting, confident that their answer will be correct.'
  prefs: []
  type: TYPE_NORMAL
- en: Making metrics part of data transformation intuitively makes sense. Making them
    part of dbt — the dominant transformation tool, which is already well-integrated
    with the modern data stack — is exactly what the semantic layer needed to go from
    idea to reality.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a977fe9ab20d16cc994ac9cce08c3c7c.png)'
  prefs: []
  type: TYPE_IMG
- en: Since dbt’s Semantic Layer launched, progress has been fairly measured — in
    part because this happened less than three months ago, and in part because changing
    the way that people write metrics will take time.
  prefs: []
  type: TYPE_NORMAL
- en: '**In 2023, the first set of Semantic Layer implementations will go live.**'
  prefs: []
  type: TYPE_NORMAL
- en: Many data teams have spent the last couple of months exploring the impact of
    this new technology — experimenting with the Semantic Layer and thinking through
    how to change their metrics frameworks.
  prefs: []
  type: TYPE_NORMAL
- en: This process gets easier as more tools in the modern data stack integrate with
    the Semantic Layer. Seven tools were Semantic Layer–ready at its launch (including
    [us](https://humansofdata.atlan.com/2022/10/dbt-semantic-layer-launch-partner/),
    Hex, Mode, and Thoughtspot). Eight more tools were Metrics Layer–ready, an intermediate
    step to integrating with the Semantic Layer.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/909310ec30563235945daab516a96b28.png)'
  prefs: []
  type: TYPE_IMG
- en: '**In 2022, some of the main players in reverse ETL** (one of [last year’s big
    trends](/the-future-of-the-modern-data-stack-in-2022-4f4c91bb778f)) **sought to
    redefine their category as “data activation”, a new take on the “customer data
    platform”.**'
  prefs: []
  type: TYPE_NORMAL
- en: A CDP combines data from all customer touchpoints (e.g. website, email, social
    media, help center, etc). A company can then segment or analyze that data, build
    customer profiles, and power personalized marketing. For example, they can create
    an automated email with a discount code if someone abandons their cart, or advertise
    to people who have visited a specific page on the website and used the company’s
    live chat.
  prefs: []
  type: TYPE_NORMAL
- en: CDPs are designed around *using* data, rather than simply aggregating and storing
    it. This is where data activation comes in — “activating” data from the warehouse
    to handle CDP functions.
  prefs: []
  type: TYPE_NORMAL
- en: '**Data activation in various forms has been around for a few years. However,
    this idea of data activation as the new CDP took off in 2022.**'
  prefs: []
  type: TYPE_NORMAL
- en: For example, Arpit Choudhury [analyzed the space](https://medium.com/data-beats/data-activation-in-the-modern-data-stack-2dfa53455133)
    in April, Sarah Krasnik [broke down the debate](https://sarahsnewsletter.substack.com/p/reinventing-the-wheel-of-data-activation)
    in July, Priyanka Somrah included it as a [data category](https://thedatasource.substack.com/p/the-data-source-10-modern-data-warehouses)
    in August, and Luke Lin called out data activation in his [2023 data predictions](https://pmdata.substack.com/p/data-predictions-for-2023)
    last month.
  prefs: []
  type: TYPE_NORMAL
- en: 'In part, this trend was caused by marketing from former reverse ETL companies,
    who now brand themselves as data activation products. For example, Hightouch rebranded
    itself with a big splash in April, dropping three blogs on data activation in
    five days:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Data Activation: The Next Step After Analytics](https://hightouch.com/blog/data-activation-the-next-step)
    by Pedram Navid'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Hightouch: The Data Activation Platform](https://hightouch.com/blog/the-data-activation-company)
    by Kashish Gupta'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What is Data Activation?](https://hightouch.com/blog/what-is-data-activation)
    by Luke Kline'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In part, this can also be traced to the larger debate around driving data use
    cases and value, rather than focusing on data infrastructure or stacks. As [Benn
    Stancil](https://benn.substack.com/p/should-we-be-grateful) put it, “Why has data
    technology advanced so much further than value a data team provides?”
  prefs: []
  type: TYPE_NORMAL
- en: In part, this was also an inevitable result of the modern data stack. Stacks
    like Snowflake + Hightouch have the same data and functionality as a CDP, but
    they can be used across a company rather than for only one function.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/133fa25f7e22a22319a9f5126f9fc7bb.png)'
  prefs: []
  type: TYPE_IMG
- en: '**CDPs made sense in the past.** When it was difficult to stand up a data platform,
    having an out-of-the-box, perfectly customized customer data platform for business
    users was a big win.'
  prefs: []
  type: TYPE_NORMAL
- en: Now, though, the world has changed, and companies can set up a data platform
    in under 30 minutes — one that not only has customer data, but also all other
    important company data (e.g. finance, product/users, partners, etc).
  prefs: []
  type: TYPE_NORMAL
- en: At the same time, data work has been consolidating around the modern data stack.
    Salesforce once tried to handle its own analytics (called Einstein Analytics).
    Now it has partnered with Snowflake, and Salesforce data can be piped into Snowflake
    just like any other data source.
  prefs: []
  type: TYPE_NORMAL
- en: The same thing has happened for most SaaS products. While internal analytics
    was once their upsell, they are now realizing that it makes more sense to move
    their data into the existing modern data ecosystem. Instead, their upsell is now
    syncing data to warehouses via APIs.
  prefs: []
  type: TYPE_NORMAL
- en: In this new world, data activation becomes very powerful. **The modern data
    warehouse plus data activation will replace not only the CDP, but also all pre-built,
    specialized SaaS data platforms.**
  prefs: []
  type: TYPE_NORMAL
- en: With the modern data stack, data is now created in specialized SaaS products
    and piped into storage systems like Snowflake, where it is combined with other
    data and transformed in the API layer. Data activation is then crucial for piping
    insights back into the source SaaS systems where business users do their daily
    work.
  prefs: []
  type: TYPE_NORMAL
- en: For example, Snowflake [acquired](https://www.snowflake.com/blog/snowflake-to-acquire-streamlit/)
    Streamlit, which allows people to create pre-built templates and templates on
    top of Snowflake. Rather than developing their own analytics or relying on CDPs,
    tools like Salesforce can now let their customers sync data to Snowflake and use
    a pre-built Salesforce app to analyze the data or do custom actions (like cleaning
    a lead list with Clearbit) with one click. **The result is the customization and
    user-friendliness of a CDP, combined with the power of modern cloud compute.**
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/21fdf550ce642b5f08844d4f9c30632e.png)'
  prefs: []
  type: TYPE_IMG
- en: This idea came from Zhamak Dehghani — first with [two](https://martinfowler.com/articles/data-monolith-to-mesh.html)
    [blogs](https://martinfowler.com/articles/data-mesh-principles.html) in 2019,
    and then with her [O’Reilly book](https://www.oreilly.com/library/view/data-mesh/9781492092384/)
    in 2022.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s the TL;DR from [Data Mesh Learning Community](https://datameshlearning.com/quick-overview/):
    “The shortest summary: treat data as a product, not a by-product. By driving data
    product thinking and applying domain driven design to data, you can unlock significant
    value from your data. Data needs to be owned by those who know it best.”'
  prefs: []
  type: TYPE_NORMAL
- en: '**The data mesh was everywhere in 2021\. In 2022, it started to move from abstract
    idea to reality.**'
  prefs: []
  type: TYPE_NORMAL
- en: The data mesh conversation has shifted from “What is it?” to “How can we implement
    it?” as real user stories grew and the ideas behind the data mesh became less
    abstract and more actionable.
  prefs: []
  type: TYPE_NORMAL
- en: Meanwhile, companies started to brand themselves around the data mesh. So far,
    we’ve seen this with [Starburst](https://www.starburst.io/platform/features/data-products/),
    [Databricks](https://www.databricks.com/blog/2022/10/19/building-data-mesh-based-databricks-lakehouse-part-2.html),
    [Oracle](https://www.oracle.com/integration/what-is-data-mesh/), [Google Cloud](https://services.google.com/fh/files/misc/build-a-modern-distributed-datamesh-with-google-cloud-whitepaper.pdf),
    [Dremio](https://www.dremio.com/blog/enabling-a-data-mesh-with-an-open-lakehouse/),
    [Confluent](https://developer.confluent.io/learn-kafka/data-mesh/intro/), [Denodo](https://www.denodo.com/en/solutions/by-use-case/data-mesh-enabled-data-virtualization),
    [Soda](https://www.soda.io/resources/introducing-a-new-domain-specific-language-for-data-reliability),
    [lakeFS](https://lakefs.io/blog/data-mesh/), and [K2 View](https://www.k2view.com/platform/data-mesh-architecture/),
    among others.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c08156219478b628aa32381d1fa22210.png)'
  prefs: []
  type: TYPE_IMG
- en: Four years after it was created, we’re still in the early phases of the data
    mesh.
  prefs: []
  type: TYPE_NORMAL
- en: '**In 2023, the first wave of data mesh “implementations” will go live, with
    the “data as a product” concept front and center.**'
  prefs: []
  type: TYPE_NORMAL
- en: This year, we’ll start seeing more and more real data mesh architectures — not
    the aspirational diagrams that have been floating around data blogs for years,
    but real architectures from real companies.
  prefs: []
  type: TYPE_NORMAL
- en: 'The data world will also start to converge on a best-in-class reference architecture
    and implementation strategy for the data mesh. This will include the following
    core components:'
  prefs: []
  type: TYPE_NORMAL
- en: Metadata platform that can integrate into developer workflows (e.g. Atlan’s
    [APIs](https://atlan.com/open-api-architecture/) and GitHub [integration](https://humansofdata.atlan.com/2022/12/introducing-supercharged-automation/))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data quality and testing (e.g. [Great Expectations](https://greatexpectations.io/),
    [Monte Carlo](https://www.montecarlodata.com/))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Git-like process for data producers to incorporate testing, metadata management,
    documentation, etc. (e.g. dbt)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All built around the same central data warehouse/lakehouse layer (e.g. Snowflake,
    Databricks)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/fb5e584e7e3c55c611c9d723f4505e12.png)'
  prefs: []
  type: TYPE_IMG
- en: One of our big trends from last year, data observability held its own and continued
    to grow alongside adjacent ideas like data quality and reliability.
  prefs: []
  type: TYPE_NORMAL
- en: Existing companies became even bigger (e.g. [Databand](https://databand.ai/blog/ibm-acquires-databand-to-extend-leadership-in-observability/)’s
    acquisition by IBM in July 2022, Monte Carlo’s $135M [Series B](https://techcrunch.com/2022/05/24/monte-carlo-raises-135m-series-d-at-1-6b-price-showing-that-unicorn-rounds-are-still-a-thing/)),
    new companies went mainstream (e.g. [Kensu](https://www.oreilly.com/library/view/fundamentals-of-data/9781098133283/)
    and [Monte Carlo](https://www.montecarlodata.com/blog-announcing-data-quality-fundamentals-oreillys-pioneering-book-on-data-observability/)
    jumped into thought leadership), and new tools launched every month (e.g. Bigeye’s
    [Metadata Metrics](https://www.datanami.com/2022/05/06/bigeye-introduces-new-metadata-metrics-observability-solution/)).
  prefs: []
  type: TYPE_NORMAL
- en: '**In a notable change, this space also saw significant open-source growth in
    2022\.** Datafold launched an [open-source diff tool](https://venturebeat.com/data-infrastructure/datafold-launches-open-source-diffing-tool-to-perform-data-validation-checks/),
    Acceldata [open-sourced](https://martechseries.com/analytics/data-management-platforms/acceldata-open-sources-data-platform-and-data-observability-libraries/)
    its data platform and data observability libraries, and Soda launched both its
    open-source [Soda Core](https://www.soda.io/press/soda-unveils-soda-core-the-open-source-framework-for-data-quality-and-reliability)
    and enterprise [Soda Cloud](https://www.soda.io/resources/the-ga-of-self-serve-data-quality)
    platforms.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b7c64da8211abcc4bf3fc22f94660b52.png)'
  prefs: []
  type: TYPE_IMG
- en: One of our open questions in [last year’s report](/the-future-of-the-modern-data-stack-in-2022-4f4c91bb778f)
    was where data observability was heading — towards its own category, or merging
    with another category like data reliability or active metadata.
  prefs: []
  type: TYPE_NORMAL
- en: '**Data observability and quality will converge in a larger “data reliability”
    category centered around ensuring high-quality data.**'
  prefs: []
  type: TYPE_NORMAL
- en: This may seem like a big change, but many of the companies in these categories
    have changed names multiple times over the years, such as [Datafold](https://www.datafold.com/)
    going from data diffs to a “data reliability platform”.
  prefs: []
  type: TYPE_NORMAL
- en: As these companies compete to define and own the category, we’ll continue to
    see more confusion in the short term. **However, there are early signs that this
    will start to settle down into one category in the near future.**
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c1588fa949263cb11c011ae252d74d9b.png)'
  prefs: []
  type: TYPE_IMG
- en: It feels interesting to welcome 2023 as data practitioners. While there’s a
    lot of uncertainty looming in the air (uncertainty is the new certainty!), we’re
    also a bit relieved.
  prefs: []
  type: TYPE_NORMAL
- en: '**2021 and 2022 were absurd years in the history of the data stack.**'
  prefs: []
  type: TYPE_NORMAL
- en: The hype was crazy, new tools were launching every day, data people were constantly
    being poached by data startups, and VCs were throwing money at every data practitioner
    who even hinted at building something. The “modern data stack” was finally cool,
    and the data world had all the money and support and acknowledgment it needed.
  prefs: []
  type: TYPE_NORMAL
- en: At Atlan, we started as a [data team](https://humansofdata.atlan.com/2021/05/why-atlan-backstory/)
    ourselves. As people who have been in data for over a decade, this was a wild
    time. Progress is generally made in decades, not years. But in the last three
    years, the modern data stack has grown and matured as much as in the decade before.
  prefs: []
  type: TYPE_NORMAL
- en: It was exciting… yet we ended up asking ourselves [existential questions](https://metadataweekly.substack.com/p/metadata-weekly-20#%C2%A7wtf-happened-at-data-council-and-what-does-this-mean-for-our-community)
    more than once. Is this modern data stack thing real, or is it just hype fueled
    by VC money? Are we living in an echo chamber? Where are the data practitioners
    in this whole thing?
  prefs: []
  type: TYPE_NORMAL
- en: '**While this hype and frenzy led to great tooling, it was ultimately bad for
    the data world.**'
  prefs: []
  type: TYPE_NORMAL
- en: Confronted by a sea of buzzwords and products, data buyers often ended up confused
    and could spend more time trying to get the right stack than actually using it.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s be clear — the goal of the data space is ultimately to help companies
    leverage data. Tools are important for this. But they’re ultimately an enabler,
    not the goal.
  prefs: []
  type: TYPE_NORMAL
- en: '**As this hype starts to die down and the modern data stack starts to stabilize,
    we have the chance to take the tooling progress we’ve made and translate it into
    real business value.**'
  prefs: []
  type: TYPE_NORMAL
- en: We’re at a point where data teams aren’t fighting to set up the right infrastructure.
    With the modern data stack, setting up a data ecosystem is quicker and easier
    than ever. Instead, data teams are fighting to prove their worth and get more
    results out of less time and resources.
  prefs: []
  type: TYPE_NORMAL
- en: Now that companies can’t throw money around, their decisions need to be targeted
    and data-driven. This means that data is more important than ever, and data teams
    are in a unique position to provide real business value. But to make this happen,
    data teams need to finally figure out this “value” question.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we’ve got the modern data stack down, it’s time to figure out the [modern
    data culture stack](/its-time-for-the-modern-data-culture-stack-493036315ed2).
    What does a great data team look like? How should it work with business? How can
    it drive the most impact in the least time?
  prefs: []
  type: TYPE_NORMAL
- en: These are tough questions, and there won’t be any quick fixes. But if the data
    world can crack the secrets to a better data culture, we can finally create dream
    data teams — ones that will not just help their companies survive during the next
    12–18 months, but propel them to new heights in the coming decades.
  prefs: []
  type: TYPE_NORMAL
- en: '**Ready for spicy takes and expert insights on these trends? We assembled a
    panel of superstars (Bob Muglia, Barr Moses, Benn Stancil, Douglas Laney, and
    Tristan Handy) for the first Great Data Debate of 2023\.** [**Watch the recording
    here.**](https://atlan.com/great-data-debate/)'
  prefs: []
  type: TYPE_NORMAL
