# 稳定航向：导航LLM应用程序评估

> 原文：[https://towardsdatascience.com/steady-the-course-navigating-the-evaluation-of-llm-based-applications-8b7a22734fc9?source=collection_archive---------1-----------------------#2023-11-09](https://towardsdatascience.com/steady-the-course-navigating-the-evaluation-of-llm-based-applications-8b7a22734fc9?source=collection_archive---------1-----------------------#2023-11-09)

## 为何评估LLM应用程序至关重要及如何入门

[](https://medium.com/@stijn.goossens?source=post_page-----8b7a22734fc9--------------------------------)[![Stijn Goossens](../Images/df94a9620671e68920db453faed473f2.png)](https://medium.com/@stijn.goossens?source=post_page-----8b7a22734fc9--------------------------------)[](https://towardsdatascience.com/?source=post_page-----8b7a22734fc9--------------------------------)[![Towards Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----8b7a22734fc9--------------------------------) [斯坦·古森斯](https://medium.com/@stijn.goossens?source=post_page-----8b7a22734fc9--------------------------------)

·

[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ffc2031bc0292&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsteady-the-course-navigating-the-evaluation-of-llm-based-applications-8b7a22734fc9&user=Stijn+Goossens&userId=fc2031bc0292&source=post_page-fc2031bc0292----8b7a22734fc9---------------------post_header-----------) 发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----8b7a22734fc9--------------------------------) ·10 分钟阅读·2023 年 11 月 9 日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F8b7a22734fc9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsteady-the-course-navigating-the-evaluation-of-llm-based-applications-8b7a22734fc9&user=Stijn+Goossens&userId=fc2031bc0292&source=-----8b7a22734fc9---------------------clap_footer-----------)

--

[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F8b7a22734fc9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsteady-the-course-navigating-the-evaluation-of-llm-based-applications-8b7a22734fc9&source=-----8b7a22734fc9---------------------bookmark_footer-----------)![](../Images/d37b7688b4451b9257befa1362bb992b.png)

一位膝盖受伤的海盗向他基于LLM的急救助手寻求建议。图像由作者使用DALL·E 3生成。

# 简介

大型语言模型（LLMs）引起了广泛关注，许多人将其融入到他们的应用程序中。例如，回答关系数据库问题的聊天机器人、帮助程序员更高效编写代码的助手，以及代表你采取行动的副驾驶。LLMs 的强大能力使你能够快速取得项目的初步成功。然而，随着你从原型过渡到成熟的 LLM 应用程序，一个强大的评估框架变得至关重要。这样的评估框架帮助你的 LLM 应用程序达到最佳性能，并确保一致和可靠的结果。在本博客文章中，我们将讨论：

1.  评估 LLM 与基于 LLM 的应用程序之间的差异

1.  LLM 应用评估的重要性

1.  LLM 应用评估的挑战

1.  入门

    a. 收集数据和构建测试集

    b. 绩效衡量

1.  LLM 应用评估框架

通过虚构的例子“FirstAidMatey”，一个为海盗提供急救的助手，我们将深入探讨评估技术、挑战和策略的海洋。最后我们将总结关键要点和洞察。让我们开始这段启发性的旅程吧！

# 评估 LLM 与基于 LLM 的应用程序

对 OpenAI 的 GPT-4、Google 的 PaLM 2 和 Anthropic 的 Claude 等个体大型语言模型（LLMs）的评估通常通过基准测试如 [MMLU](https://paperswithcode.com/sota/multi-task-language-understanding-on-mmlu) 来进行。然而，在本博客文章中，我们感兴趣的是评估基于 LLM 的应用程序。这些应用程序由 LLM 提供支持，并包含其他组件，如管理 LLM 调用序列的编排框架。通常，检索增强生成（RAG）用于向 LLM 提供上下文，避免幻觉。简而言之，RAG 需要将上下文文档嵌入到向量存储中，以便从中检索相关片段并与 LLM 共享。与 LLM 相比，基于 LLM 的应用程序（或 LLM 应用）旨在非常好地执行一个或多个特定任务。找到合适的设置通常涉及一些实验和迭代改进。例如，RAG 可以通过多种方式实现。本文讨论的评估框架可以帮助你找到适合你用例的最佳设置。

![](../Images/07db4a149ab5c6ca50cbdd892d2df94d.png)

*在基于 LLM 的应用程序中使用 LLM 可以使其更强大。*

FirstAidMatey是一个基于LLM的应用，帮助海盗处理像*“我的手被绳子缠住，现在肿了，我该怎么办，伙计？”*这样的问题。最简单的形式是Orchestrator由一个单一的提示组成，将用户问题传递给LLM，并要求其提供有帮助的回答。它还可以指示LLM用海盗语言回答，以便更好地理解。作为扩展，可以添加一个包含急救文档的向量存储。根据用户问题，可以检索相关文档并将其包含在提示中，以便LLM可以提供更准确的答案。

# LLM应用评估的重要性

在深入了解如何评估之前，让我们先看看为什么你应该建立一个评估LLM应用的系统。主要目标有三点：

+   **一致性**：确保LLM应用在所有场景下的输出稳定可靠，并在出现回归时发现。例如，当你在特定场景下改善LLM应用表现时，你希望在可能影响其他场景的性能时收到警告。当使用像OpenAI的GPT-4这样的专有模型时，你还需遵循其更新计划。随着新版本的发布，你当前的版本可能会逐渐被弃用。研究表明，[切换到更新的GPT版本并不总是更好](https://arxiv.org/pdf/2307.09009.pdf)。因此，能够评估新版本如何影响LLM应用的表现是很重要的。

+   **洞察**：了解LLM应用表现良好的地方和需要改进的地方。

+   **基准测试**：建立LLM应用的性能标准，衡量实验的效果，并自信地发布新版本。

结果是，你将实现以下成果：

+   **赢得** **用户信任和满意度**，因为你的LLM应用将表现一致。

+   **提升利益相关者信心**，因为你可以展示LLM应用的表现如何，以及新版本如何改进旧版本。

+   **提升你的竞争优势**，因为你可以快速迭代、进行改进并自信地部署新版本。

# LLM应用评估的挑战

阅读了上述好处后，很明显为什么采用基于LLM的应用可能是有利的。但在此之前，我们必须解决以下[两个主要挑战](https://fullstackdeeplearning.com/llm-bootcamp/spring-2023/llmops/)：

+   **缺乏标记数据**：与传统机器学习应用不同，基于LLM的应用不需要标记数据即可开始。LLM可以开箱即用地执行许多任务（如文本分类、摘要、生成等），无需展示具体示例。这很棒，因为我们不必等待数据和标签，但另一方面，这也意味着我们没有数据来检查应用的表现。

+   **多个有效答案**：在LLM应用中，相同的输入通常可以有多个正确答案。例如，聊天机器人可能会提供多个意义相近的回答，或者代码可能会生成具有相同功能但结构不同的版本。

为了应对这些挑战，我们必须定义合适的数据和指标。我们将在下一节中进行说明。

# 开始

## 收集数据并构建测试集

对于评估基于LLM的应用，我们使用一个由**每个包含特定输入和目标的测试用例**组成的测试集。这些测试用例的内容取决于应用的目的。例如，代码生成应用期望将口头指令作为输入，并返回代码。评估期间，将把输入提供给LLM应用，生成的输出可以与参考目标进行比较。以下是FirstAidMatey的一些测试用例：

![](../Images/06e6c8f2dc13357779a2576e60e56140.png)

*一些用于我们急救聊天机器人应用的测试用例。*

在这个例子中，每个测试用例包含一个问题作为输入和一个正确的参考答案作为目标。除此之外，**历史响应**，甚至历史用户反馈也可以添加到测试用例中。这允许你检查较新的回答是否比旧的回答有所改进，以及用户反馈是否得到处理。理想情况下，你的测试集应包括经过验证的参考答案，但如果你希望快速比较新旧模型，使用历史响应和反馈也是一个选项。

由于你通常在没有数据集的情况下开始开发基于LLM的应用，因此**早期开始构建测试集**非常重要。你可以通过添加当前模型失败的那些示例来进行迭代。最初，将是开发人员最频繁地尝试LLM应用，并且是他们根据手动测试添加第一个测试用例。然而，尽早涉及业务或最终用户也很重要，因为他们对相关测试用例有更好的理解。为了启动测试集，你也可以使用LLM根据你的知识库等生成输入-目标对。随着时间的推移，测试集应该涵盖对最终用户最重要的所有主题。因此，收集用户反馈并为表现不佳和被低估的主题添加测试用例是很重要的。

# 评估性能

给定一组测试用例后，我们现在可以将输入传递给 LLM 应用，并将生成的响应与目标进行比较。由于每个输入没有单一的正确答案，我们不能字面上地将响应与参考答案进行比较。响应的措辞可能不同，但意义与参考答案相同。不过，我们可以做的是 [**评估响应的几个属性**](/testing-large-language-models-like-we-test-software-92745d28a359)，作为响应质量的代理。测试的相关属性取决于应用程序和可用数据。以下是 FirstAidMatey 响应的属性列表，以及如何将其转化为指标：

+   **事实一致性**：使用 LLM 来评估生成的答案是否与参考答案在事实上一致。

+   **海盗性**：使用 LLM 来评估答案是否用海盗语言书写。

+   **语义相似性**：计算生成答案与参考答案的嵌入之间的余弦相似度。请注意，这比事实一致性计算要便宜得多，但可能与正确答案的相关性不那么高。

+   **冗长性**：将生成答案的长度除以参考答案的长度。冗长性越高，LLM 应用的表现越啰嗦，这可能不是应用的意图。

+   **延迟**：测量 LLM 应用生成响应所需的时间。这使你能够在更准确但较慢的配置之间做出权衡。

根据使用案例，你可能会有更多/不同的属性。例如，对于一个代码生成的 LLM 应用，一个额外的属性可能是生成代码的语法正确性，这可以通过将代码发送给编译器来测量。下图说明了如何通过使用 LLM 来评估一个属性。

![](../Images/3d0f390839cf36ffc7777a244a952a3f.png)

*通过另一个 LLM 调用评估事实一致性的示意图。*

一旦所有测试的属性都被评估完成，可以计算出每个指标的**平均分数**并与基准/目标性能进行比较。当你尝试不同的配置时，例如 RAG，测试分数会指向最有利的候选项。正确性、冗长性和延迟之间的权衡也会变得清晰。此外，除了指标分数，失败的案例还可以提供有用的改进见解。仔细查看一些失败的测试，它们可能会为进一步改善你的 LLM 应用提供有用的见解。

你可能会想，*“如果LLM不能正确回答问题，我们怎么能相信它来评估这个答案？难道不会对它自己的输出有偏见吗？”* 的确，这听起来有些违背直觉，但这里的主要观点是**评估比生成更容易**。可以考虑创作与评估画作的类比。当创作一幅画时，你需要同时考虑多个属性，如构图、颜色、视角、光影、质感、预期信息等。而评估一幅画则可以逐一关注每个属性；评价一幅完成的作品比从头开始创作一幅画要简单。LLM也是一样：尽管生成一个恰当的响应可能具有挑战性，但批判性地评估现有的响应则更加直接。如果有参考响应，比如对于事实一致性属性，评估会变得更可行。

# LLM应用评估框架

让我们在最后一部分中将所有内容结合起来。下图展示了评估框架的概述，并说明了一个重要的反馈循环：

1.  **LLM应用、属性和测试用例会传递给评估器**，评估器会循环遍历所有测试用例，并将测试输入传递给LLM应用。然后，通过遍历属性并收集结果作为指标来评估生成的输出。

1.  评估结果被存储以供进一步分析。除了指标外，跟踪LLM应用的配置（即使用的LLM和参数、使用的RAG组件和参数、系统提示等）也很重要，这样你可以轻松区分最有前景的实验，并获得进一步改进应用的洞见。你可以将**评估结果和LLM应用配置**存储在自己的数据库中，或者使用像[MLflow](https://github.com/mlflow/mlflow/)这样的工具，它能让你立即访问用户友好的界面。

1.  一旦你对性能感到满意，你可以**发布新版本**的应用，无论是面向内部用户还是外部用户。

1.  在项目的早期阶段，将由开发者进行测试和**收集反馈**。随后，可以从最终用户那里收集反馈，无论是直接的（如点赞/点踩和书面反馈）还是间接的（如对话回合、会话时长、接受的代码建议等）。

1.  通过分析收到的反馈来扩展测试集，并**添加测试用例**以涵盖当前模型处理不充分的情况。

1.  在收到的反馈中发现趋势，并将其转化为**LLM应用改进**。根据情况，你可以改进协调器（例如，创建一个单独LLM调用的链而不是一个单一提示）、检索过程（例如，改进嵌入）或LLM（例如，改变模型、参数或考虑微调）。

![](../Images/14eb8a5ae414efbb5c61d004d6950d80.png)

*LLM 基础应用的评估框架概述。*

关于步骤 1 和 2 的示例，请查看以下 [Jupyter notebook](https://github.com/radix-ai/llm-app-eval/blob/main/src/llm_app_eval/example.ipynb)。该 notebook 说明了本文博客中解释的概念。你可以看到如何定义和发送属性、测试用例以及 LLM 应用程序版本给 Evaluator。除了打印评估结果外，这些结果还会被记录到 MLflow 仪表板中以便进一步分析。绝对值得一看，这将使讨论的话题更加具体。

# 结论

对 LLM 基础应用进行评估是 LLM 应用程序开发的重要部分。本文介绍的评估框架简化了开发过程中实验的比较，确保了一致的性能，并提供了进一步改进的见解。

我们遇到的第一个挑战是缺乏标记数据，可以通过尽早构建测试集并通过添加困难和代表性不足的案例来逐步扩展解决。第二个挑战是存在多个正确答案这一事实，可以通过查看生成输出的不同属性来克服。其中一些属性可以通过简单的公式或规则来衡量，而其他属性则可以通过 LLM 来评估。

最后，我们发现了一个关键的反馈循环，即评估结果和收集的用户反馈推动 LLM 应用程序的性能提升。总之，系统化的评估是指引你的 LLM 应用程序走向成功的指南针！

*作者注：本文的灵感源于我在 Radix 担任解决方案架构师的经验。Radix 是一家比利时人工智能公司，专注于开发创新的机器学习解决方案和应用程序，服务于各种行业。如果你对 LLM 基础应用感兴趣，我鼓励你访问* [*radix.ai*](http://radix.ai) *或直接通过* [*LinkedIn*](https://www.linkedin.com/in/goossensstijn/)*与我联系。*

*除非另有说明，否则所有图片均为作者提供。*
