- en: How to Use Google‚Äôs PaLM 2 API with Python
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ÂéüÊñáÔºö[https://towardsdatascience.com/how-to-use-google-palm-2-api-with-python-373bc564251c?source=collection_archive---------2-----------------------#2023-08-14](https://towardsdatascience.com/how-to-use-google-palm-2-api-with-python-373bc564251c?source=collection_archive---------2-----------------------#2023-08-14)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Customize and integrate Google‚Äôs LLM in your application.
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://eliselandman.medium.com/?source=post_page-----373bc564251c--------------------------------)[![Elise
    Landman](../Images/1cd86aa9df340e430820a48f4d26de5a.png)](https://eliselandman.medium.com/?source=post_page-----373bc564251c--------------------------------)[](https://towardsdatascience.com/?source=post_page-----373bc564251c--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----373bc564251c--------------------------------)
    [Elise Landman](https://eliselandman.medium.com/?source=post_page-----373bc564251c--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ¬∑
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fdbd14e538474&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-use-google-palm-2-api-with-python-373bc564251c&user=Elise+Landman&userId=dbd14e538474&source=post_page-dbd14e538474----373bc564251c---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----373bc564251c--------------------------------)
    ¬∑11 min read¬∑Aug 14, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F373bc564251c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-use-google-palm-2-api-with-python-373bc564251c&user=Elise+Landman&userId=dbd14e538474&source=-----373bc564251c---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F373bc564251c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-use-google-palm-2-api-with-python-373bc564251c&source=-----373bc564251c---------------------bookmark_footer-----------)![](../Images/83e8ff5e480e20067392de2c2d57db3d.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Image by Alexandre Debi√®ve from [Unsplash](https://unsplash.com/photos/FO7JIlwjOtU).
  prefs: []
  type: TYPE_NORMAL
- en: 'Generative AI is all over the place. We see more and more companies investing
    in this **powerful technology** as it becomes increasingly clear how much **potential**
    it has. And as Gartner states: in the near future, [Generative AI] will become
    a competitive advantage and differentiator.'
  prefs: []
  type: TYPE_NORMAL
- en: ‚Äúin the near future, [Generative AI] will become a competitive advantage and
    differentiator.‚Äù
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Unfortunately, developing Generative AI models is not only a complex work of
    engineering, but it is usually quite a pricey project. Luckily, we do not have
    to develop these ourselves ‚Äî we can reuse what has been pre-developed for us:
    with APIs! Therefore, let‚Äôs not wait any longer ‚Äî let‚Äôs jump right into how we
    can leverage Generative AI by **integrating** it into our application.'
  prefs: []
  type: TYPE_NORMAL
- en: 'For this article, we‚Äôll be looking at **Google‚Äôs answer to LLMs**: the **PaLM
    2** API. PaLM 2 is Google‚Äôs newest version of their **Pathways Language Model**,
    a large language model which uses around five times more training data than their
    initial model released in 2022.'
  prefs: []
  type: TYPE_NORMAL
- en: In this article I will be going through **some code examples** and showing you
    how to authenticate to Google Cloud and use, as well as customize the **PaLM 2
    APIs** with Python 3.11.
  prefs: []
  type: TYPE_NORMAL
- en: 1 | Getting Started
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The PaLM 2 APIs can be accessed through **Google Cloud‚Äôs** [**Vertex AI**](https://cloud.google.com/vertex-ai)
    platform. Therefore, before we can make any API calls, we will need to set up
    our Google Cloud account. You can [sign up here](https://console.cloud.google.com/)
    and get [$300 in free credits](https://cloud.google.com/free/docs/free-cloud-features#free-trial)
    to start playing around with the services.
  prefs: []
  type: TYPE_NORMAL
- en: As soon as your account and project are set up, we can go ahead and [create
    a service account](https://cloud.google.com/iam/docs/service-accounts-create)
    which we will use to authenticate to the Vertex AI APIs. We use service accounts,
    because we can ensure **access control** to our Google Cloud resources by giving
    them only specific IAM permissions. For our use case, we will give the service
    account the `Vertex AI User` role. This might be too broad for your use case,
    so I recommend checking the [available access roles](https://cloud.google.com/vertex-ai/docs/general/access-control)
    and choose which one fits your needs.
  prefs: []
  type: TYPE_NORMAL
- en: After having created the service account and given it the right permissions,
    we can go ahead and [generate a service account key](https://cloud.google.com/iam/docs/keys-create-delete).
    Select JSON as the key type and save the file in a safe place.
  prefs: []
  type: TYPE_NORMAL
- en: Great ‚Äî we are ready to get hands-on! üëè
  prefs: []
  type: TYPE_NORMAL
- en: 2 | Authenticate to Google Cloud
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this example, we will authenticate using [OAuth 2.0](https://oauth.net/2/)
    and request an access token with the help of the service account key we generated
    in the previous step.
  prefs: []
  type: TYPE_NORMAL
- en: 'To facilitate this process, we can make use of the `[google-auth](https://pypi.org/project/google-auth/)`
    Python library, as shown in the code sample below:'
  prefs: []
  type: TYPE_NORMAL
- en: Authenticate to Google Cloud using the google-auth Python library.
  prefs: []
  type: TYPE_NORMAL
- en: This code sample uses the service account key file ‚Äúkey.json‚Äù to request and
    generate an access token we can use for the Google Cloud APIs. After having obtained
    our access token, we can start using it to make calls to the Vertex AI PaLM 2
    APIs.
  prefs: []
  type: TYPE_NORMAL
- en: 3 | Calling the PaLM 2 API
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As of today, there are two different PaLM 2 models available in Google Cloud:
    **PaLM 2 for Text** (i. e. `[text-bison](https://cloud.google.com/vertex-ai/docs/generative-ai/text/text-overview)`)
    and **PaLM 2 for Chat** (i. e. `[chat-bison](https://cloud.google.com/vertex-ai/docs/generative-ai/chat/chat-prompts)`).
    The documentation suggests using `text-bison` for text tasks that can be completed
    with one response, and `chat-bison` for text tasks that require more conversational,
    back-and-forth interactions.'
  prefs: []
  type: TYPE_NORMAL
- en: Let‚Äôs start with the `text-bison` model. For these examples, we will be using
    the Python `[requests](https://pypi.org/project/requests/)` library to make the
    API calls. You can also use the [Vertex AI SDK](https://cloud.google.com/python/docs/reference/aiplatform/latest/index.html),
    if you prefer.
  prefs: []
  type: TYPE_NORMAL
- en: 'PaLM 2 for Text: Sentiment Analysis'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The PaLM 2 for Text model can be used for various **text-related tasks**: including
    summarization, answering questions, sentiment analysis, etc. It takes the following
    parameters as input:'
  prefs: []
  type: TYPE_NORMAL
- en: '`prompt`: instructions of the task we want the model to perform.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`temperature`: controls the ‚Äúcreativity‚Äù of the model. If we want our model
    to be more open-ended and creative in its replies, we should increase the temperature.
    If we want it to be more deterministic, the temperature should be lower. Values
    range between 0 and 1.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`maxOutputTokens`: number of tokens to be generated in the output (1 token
    = 4 characters). Values range between 1 and 1024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`topK`: changes the probability of how the model selects tokens for generating
    output. In each token selection step, the `topK` tokens with the highest probabilities
    are sampled and then further filtered with `topP`. The higher the value, the more
    random the responses will be. Values range between 1 and 40.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`topP`: changes the probability of how the model selects tokens for generating
    output. Tokens are selected until the sum of their probabilities equals `topP`.
    The higher the value, the more random the response is. Values range between 0
    and 1.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*For more details on the parameters see* [*this documentation*](https://cloud.google.com/vertex-ai/docs/generative-ai/start/quickstarts/api-quickstart#parameter_definitions)*.*'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this first example, we will perform sentiment analysis on some **sample
    product reviews**:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The instructions (i. e. the prompt) we will give the model need to be clearly
    stating the task we want the model to perform, as well as the output we expect
    it to generate. In our case, we ask it to go through each of the reviews in the
    `sentences` list and **tell us what the sentiment of these is**. We also instruct
    it to provide the output as a Python list.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Lastly, we need to define the **input parameter values**:'
  prefs: []
  type: TYPE_NORMAL
- en: We set the `temperature` to `0` because for this task, we want to avoid the
    model being too creative. With a lower temperature, we configure the the model
    be more likely to output exactly the structure we requested.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We set `256` as `maxOutputTokens`, because it is approximately equivalent to
    200 words and is a good length for our task.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We set `topK` to 40, because this is the model‚Äôs default value.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We set `topK` to 0.95, because this is the model‚Äôs default value.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We can now make the API call, similarly to any other API call we would make
    with the `[requests](https://pypi.org/project/requests/)` library, as shown below:'
  prefs: []
  type: TYPE_NORMAL
- en: Make an API call to the Google Cloud PaLM 2 for Text API.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can get the output of the response with `response.json()[‚Äúpredictions‚Äù][0][‚Äúcontent‚Äù]`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Neat! Now, let‚Äôs do the exact same and try to set the `temperature` parameter
    to `1.0`. This will imply that our model‚Äôs output will become increasingly more
    creative. The output we get looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Indeed, we can see that the output is more creative than before and did not
    properly listen to our prompt mentioning that the ‚Äúoutput should be in a python
    list‚Äù. Therefore, this a great learning that a **good choice of parameter values
    is very important** to obtain the desired output from the model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now you might be asking yourself : ‚Äú*does this mean I cannot be sure that the
    model output is in the correct format?*‚Äù There are various mechanisms you can
    set in place in order to test your model‚Äôs output and check whether it corresponds
    to the format you expected. From understanding and correctly configuring the model‚Äôs
    input parameters, to refining your prompt (i. e. prompt engineering), as well
    as implementing additional static tests that can validate the output‚Äôs structure.
    It is highly recommended to apply these techniques, as Generative AI can make
    mistakes and we want to be sure to tackle them before they happen.'
  prefs: []
  type: TYPE_NORMAL
- en: 'PaLM 2 for Text: Text Generation'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In our next example, let‚Äôs ask PaLM 2 to **generate a welcome text** for new
    customers landing on an e-commerce website. We want to welcome them and offer
    a 20% discount code on their first purchase. For this use case, we want our model
    to come up with something a bit more creative. Therefore, we will set the `temperature`
    to `0.5`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Nice! It even gave us the output in markdown format. Let‚Äôs again try to adjust
    the `temperature` by setting it to `1.0` . The output is the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: We can again see how much more creative our model is compared to the previous
    output. It even came up with an offer expiration date, without us explicitly telling
    it to do so.
  prefs: []
  type: TYPE_NORMAL
- en: 'PaLM 2 for Chat: Conversational Assistant'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In the last example, we will test the PaLM 2 for Chat API which is more focused
    on generating a **conversational experience**. The model we‚Äôll be using is called
    `chat-bison`. It takes the following parameters as input:'
  prefs: []
  type: TYPE_NORMAL
- en: '`messages`: contains the messages and messaging history with the bot.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`context`: defines ‚Äúguidelines‚Äù for the bot‚Äôs behavior. For example: what the
    bot‚Äôs name is, what its role is, vocabulary to include/exclude, etc.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`examples`: sample input and output of how the bot should respond in the conversation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: as well as `temperature`, `maxOutputTokens`, `topK` and `topP` which do the
    same as for the `text-bison` model in the previous section.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*For more details on the parameters and what they exactly do, see* [*this documentation*](https://cloud.google.com/vertex-ai/docs/generative-ai/model-reference/text-chat#generative-ai-text-chat-drest)*.*'
  prefs: []
  type: TYPE_NORMAL
- en: Let‚Äôs create a chatbot which will serve as a **customer support agent** for
    an online gardening store ‚Äú**GardenWorld**‚Äù. The bot should respond to questions
    around plant and flower types, gardening tools, etc. We want the bot to always
    be friendly and welcoming to customers, and it should greet customers with ‚ÄúHoowdy
    gardener! üå±‚Äù as well as motivate customers to sign up to the newsletter to receive
    a 10% discount code on their first purchase.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can define this by setting the `context` and the `examples` parameters as
    shown below:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: We will give our model three examples of what its output should look like. Of
    course, the **more examples** we give, the more the bot can be **customized**
    and the **better** the model will be able to learn what we expect from it. Since
    our use case is very simple, these three examples should give the bot enough information
    to provide us with the right answer structure. We will set the `temperature` to
    `1.0`, `maxOutputTokens` to `256`, `topK` to `40` and `topP` to `0.95`.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let‚Äôs start the conversation by saying ‚Äú**Hi!**‚Äù.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Then we can make the API call:'
  prefs: []
  type: TYPE_NORMAL
- en: Make an API call to the Google Cloud PaLM 2 for Chat API.
  prefs: []
  type: TYPE_NORMAL
- en: 'After a few seconds, we get the following response back:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Nice! Our model did exactly what we asked it to do: it used the correct greeting
    (even including the emoji üå±) and mentioned the 10% discount code.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to continue our conversation with the bot, we will have to update
    the **conversation history** accordingly. We can define two simple functions which
    we can invoke after every API call:'
  prefs: []
  type: TYPE_NORMAL
- en: Functions to update the chatbots message history.
  prefs: []
  type: TYPE_NORMAL
- en: '`update_history()` will add the most recent bot reply to the `messages` list.
    Then we can use `send_message()` to send a new question to the bot ‚Äú**I need a
    nice houseplant that is easy to take care of. What can you recommend?**‚Äù.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now our `messages` variable looks as following, with the most recent message
    at the end of the list:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'We can include the `messages` history in the next API call and the response
    we get back is the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: It suggested us to choose the ‚Äú[Snake Plant](https://www.almanac.com/plant/snake-plants)‚Äù
    which is a houseplant known to be easy to take care of. Let‚Äôs also add this message
    to the `messages` history.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, let‚Äôs ask one last question ‚Äú**Any alternatives you can suggest? Also,
    can I get a discount?**‚Äù. The response we get back is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Great! As we see above, the bot suggested our discount code, and thanks to the
    `messages` history, it remembered the question we asked previously and suggested
    a ‚Äú[ZZ plant](https://www.gardenersworld.com/how-to/grow-plants/how-to-grow-zz-plant-zamioculcas-zamiifolia/)‚Äù
    an alternative plant which is easy to care for.
  prefs: []
  type: TYPE_NORMAL
- en: 'The results show us, how **easily** and **quickly** we can get a **custom chatbot**
    up and running: only by giving three `examples` and a high-level `context` , the
    bot was able to **understand** what we expected from it and correctly return the
    information during the conversation. Imagine we would provide it with hundreds,
    or even thousands of additional `examples` ‚Äî the potential is huge!'
  prefs: []
  type: TYPE_NORMAL
- en: 4 | Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this article we have seen how **easy** it is to make use of the Google Cloud
    **PaLM 2 APIs**, using both the `text-bison` and `chat-bison` models. We have
    seen how we can **authenticate** remotely to our Google Cloud project using the
    Python `[google-auth](https://pypi.org/project/google-auth/)` library and make
    calls to the APIs using the `[requests](https://pypi.org/project/requests/)` library.
    Lastly, we have seen how these APIs can be **customized** by adjusting and playing
    around with the input parameters.
  prefs: []
  type: TYPE_NORMAL
- en: I hope this article was informative for you and gave you some inspiration and
    ideas on how to get started using the PaLM 2 APIs! ü™¥ü§ñ
  prefs: []
  type: TYPE_NORMAL
- en: üîúüîú In the upcoming **part 2 of this article**, we will focus on **Prompt Engineering
    with PaLM 2 APIs** and cover how we can make our input prompts and model parameter
    selections better.
  prefs: []
  type: TYPE_NORMAL
- en: '*Feedback or questions? Feel free to reach out in the comment section!* üí¨'
  prefs: []
  type: TYPE_NORMAL
- en: üìö Keen on learning more? Check out [Generative AI on Google Cloud](https://cloud.google.com/ai/generative-ai)
    or try out the free [Generative AI Learning Path](https://www.cloudskillsboost.google/journeys/118)
    on Google Cloud Skills Boost.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] Gartner, [Gartner Experts Answer the Top Generative AI Questions for Your
    Enterprise](https://www.gartner.com/en/topics/generative-ai), Accessed 13 August
    2023.'
  prefs: []
  type: TYPE_NORMAL
- en: '[2] CB Insights, [The State of Generative AI in 7 Charts](https://www.cbinsights.com/research/generative-ai-funding-top-startups-investors/),
    2 August 2023.'
  prefs: []
  type: TYPE_NORMAL
- en: '[3] Google Cloud, [Overview of Generative AI Support on Vertex AI](https://cloud.google.com/vertex-ai/docs/generative-ai/learn/overview),
    Accessed 13 August 2023.'
  prefs: []
  type: TYPE_NORMAL
