["```py\nfor i in tqdm(range(batch_count)):\n\n    batch = np.load(os.path.join(BATCH_DIRECTORY, file_names[i]))\n\n    # ... (snipped for brevity)\n\n    # transformation of the mesh with the inverse camera extrinsics\n    frame_transformation = np.vstack(np.split(extrinsics_data[i],4))\n    inverse_frame_transformation = np.empty((4, 4))\n    inverse_frame_transformation[:3, :] = np.concatenate((np.linalg.inv(frame_transformation[:3,:3]),  \n                                                          np.expand_dims(-1 * frame_transformation[:3,3],0).T), axi\n    inverse_frame_transformation[3, :] = [0.00, 0.00, 0.00, 1.00]\n    mesh.transform(inverse_frame_transformation)\n\n    # ... (snipped for brevity)\n\n    image = np.transpose(batch['img_1'], (2, 3, 1, 0))[:,:,:,0]    \n    depth = np.transpose(batch['depth'], (2, 3, 1, 0))[:,:,0,0]\n\n    # ... (snipped for brevity)\n\n    # rendering the color and depth buffer of the transformed mesh in the image domain\n    mesh_color_buffer, mesh_depth_buffer = render_mesh_with_depth(np.array(mesh.vertices), \n                                                                  np.array(mesh.vertex_colors), \n                                                                  np.array(mesh.triangles), \n                                                                  depth, intrinsics)\n\n    # depth-aware overlaying of the mesh and the original image\n    combined_frame, combined_depth = combine_frames(image, mesh_color_buffer, depth, mesh_depth_buffer) \n\n    # ... (snipped for brevity)\n```", "```py\ndef render_mesh_with_depth(mesh_vertices, vertex_colors, triangles, depth_frame, intrinsic):\n    vertex_colors = np.asarray(vertex_colors)\n\n    # Initialize depth and color buffers\n    buffer_width, buffer_height = depth_frame.shape[1], depth_frame.shape[0]\n    mesh_depth_buffer = np.ones((buffer_height, buffer_width)) * np.inf\n\n    # Project 3D vertices to 2D image coordinates\n    vertices_homogeneous = np.hstack((mesh_vertices, np.ones((mesh_vertices.shape[0], 1))))\n    camera_coords = vertices_homogeneous.T[:-1,:]\n    projected_vertices = intrinsic @ camera_coords\n    projected_vertices /= projected_vertices[2, :]\n    projected_vertices = projected_vertices[:2, :].T.astype(int)\n    depths = camera_coords[2, :]\n\n    mesh_color_buffer = np.zeros((buffer_height, buffer_width, 3), dtype=np.float32)\n\n    # Loop through each triangle to render it\n    for triangle in triangles:\n        # Get 2D points and depths for the triangle vertices\n        points_2d = np.array([projected_vertices[v] for v in triangle])\n        triangle_depths = [depths[v] for v in triangle]\n        colors = np.array([vertex_colors[v] for v in triangle])\n\n        # Sort the vertices by their y-coordinates for scanline rendering\n        order = np.argsort(points_2d[:, 1])\n        points_2d = points_2d[order]\n        triangle_depths = np.array(triangle_depths)[order]\n        colors = colors[order]\n\n        y_mid = points_2d[1, 1]\n\n        for y in range(points_2d[0, 1], points_2d[2, 1] + 1):\n            if y < 0 or y >= buffer_height:\n                continue\n\n            # Determine start and end x-coordinates for the current scanline\n            if y < y_mid:\n                x_start = interpolate_values(y, points_2d[0, 1], points_2d[1, 1], points_2d[0, 0], points_2d[1, 0])\n                x_end = interpolate_values(y, points_2d[0, 1], points_2d[2, 1], points_2d[0, 0], points_2d[2, 0])\n            else:\n                x_start = interpolate_values(y, points_2d[1, 1], points_2d[2, 1], points_2d[1, 0], points_2d[2, 0])\n                x_end = interpolate_values(y, points_2d[0, 1], points_2d[2, 1], points_2d[0, 0], points_2d[2, 0])\n\n            x_start, x_end = int(x_start), int(x_end)\n\n            # Loop through each pixel in the scanline\n            for x in range(x_start, x_end + 1):\n                if x < 0 or x >= buffer_width:\n                    continue\n\n                # Compute barycentric coordinates for the pixel\n                s, t, u = compute_barycentric_coords(points_2d, x, y)\n\n                # Check if the pixel lies inside the triangle\n                if s >= 0 and t >= 0 and u >= 0:\n                    # Interpolate depth and color for the pixel\n                    depth_interp = s * triangle_depths[0] + t * triangle_depths[1] + u * triangle_depths[2]\n                    color_interp = s * colors[0] + t * colors[1] + u * colors[2]\n\n                    # Update the pixel if it is closer to the camera\n                    if depth_interp < mesh_depth_buffer[y, x]:\n                        mesh_depth_buffer[y, x] = depth_interp\n                        mesh_color_buffer[y, x] = color_interp\n\n    # Convert float colors to uint8\n    mesh_color_buffer = (mesh_color_buffer * 255).astype(np.uint8)\n\n    return mesh_color_buffer, mesh_depth_buffer\n```", "```py\n# Combine the original and mesh-rendered frames based on depth information\ndef combine_frames(original_frame, rendered_mesh_img, original_depth_frame, mesh_depth_buffer):\n    # Create a mask where the mesh is closer than the original depth\n    mesh_mask = mesh_depth_buffer < original_depth_frame\n\n    # Initialize combined frames\n    combined_frame = original_frame.copy()\n    combined_depth = original_depth_frame.copy()\n\n    # Update the combined frames with mesh information where the mask is True\n    combined_frame[mesh_mask] = rendered_mesh_img[mesh_mask]\n    combined_depth[mesh_mask] = mesh_depth_buffer[mesh_mask]\n\n    return combined_frame, combined_depth\n```", "```py\nvideo_name = 'depth_aware_object_insertion_demo.mp4'\nsave_directory = \"depth_aware_object_insertion_demo/\"\nframe_directory = \"depth_aware_object_insertion_demo/\"\nimage_extension = \".png\"\nfps = 15 \n\n# rendering a video of the overlayed frames\nrender_video_from_frames(frame_directory, image_extension, save_directory, video_name, fps)\n```"]