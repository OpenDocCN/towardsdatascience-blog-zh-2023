- en: The power of dbt incremental models for Big Data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/the-power-of-dbt-incremental-models-for-big-data-c8ba821eb078?source=collection_archive---------5-----------------------#2023-02-09](https://towardsdatascience.com/the-power-of-dbt-incremental-models-for-big-data-c8ba821eb078?source=collection_archive---------5-----------------------#2023-02-09)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: An experiment on BigQuery
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@suela.isaj?source=post_page-----c8ba821eb078--------------------------------)[![Suela
    Isaj](../Images/2749bad708a0446216a7e0bb6656026c.png)](https://medium.com/@suela.isaj?source=post_page-----c8ba821eb078--------------------------------)[](https://towardsdatascience.com/?source=post_page-----c8ba821eb078--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----c8ba821eb078--------------------------------)
    [Suela Isaj](https://medium.com/@suela.isaj?source=post_page-----c8ba821eb078--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F6aa4db597456&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-power-of-dbt-incremental-models-for-big-data-c8ba821eb078&user=Suela+Isaj&userId=6aa4db597456&source=post_page-6aa4db597456----c8ba821eb078---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----c8ba821eb078--------------------------------)
    ·5 min read·Feb 9, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fc8ba821eb078&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-power-of-dbt-incremental-models-for-big-data-c8ba821eb078&user=Suela+Isaj&userId=6aa4db597456&source=-----c8ba821eb078---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc8ba821eb078&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-power-of-dbt-incremental-models-for-big-data-c8ba821eb078&source=-----c8ba821eb078---------------------bookmark_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: If you are processing a couple of MB or GB with your dbt model, this is not
    a post for you; you are doing just fine! This post is for those poor souls that
    need to scan terabytes of data in BigQuery to calculate some counts, sums, or
    rolling totals over huge event data on a daily or even at a higher frequency basis.
    In this post, I will go over a technique for enabling a cheap data injestion and
    cheap data consumption for “big data”.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a7ab7fb73c98a2f43f123fd1f58a350e.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Joshua Sortino](https://unsplash.com/@sortino?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: Let’s imagine we have raw data in the granularity of timestamps and we need
    to calculate the totals per customer. Additionally, we would like to have analysis
    on the day and mothly basis because we need aggregated sums on different granularity.
    I will take the example of Twitter likes, where `user_id` is a Twitter user, `post_id`
    is a Twitter post, and `timestamp` is the timestamp when `user_id` liked `post_id`
    . The raw data is stored as a partioned table on day granularity. If your raw
    data is not partitioned, that is already your first improvement, so consider migrating
    the raw data to a partitioned table, following [this](https://www.revisitclass.com/gcp/how-to-add-partition-to-existing-table-in-bigquery/)
    example. Make sure to make the partition filter as required [https://cloud.google.com/bigquery/docs/managing-partitioned-tables#require-filter](https://cloud.google.com/bigquery/docs/managing-partitioned-tables#require-filter)
    to avoid accidents where all raw data is being scanned.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b10e53c96af999ec201f8a59190e72a8.png)'
  prefs: []
  type: TYPE_IMG
- en: Raw log of Twitter post likes
  prefs: []
  type: TYPE_NORMAL
- en: 'And this data is actually around 869.91 GB! So if I want the number of likes
    that each user has performed, my [costly] naive query would be:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: However, the likes on Twitter are *immutable* events, with no chance to be modified.
    This allows me to load this data incrementally. I will go over the raw data to
    load the day, go over the daily aggregated data to load the month, and finally
    through the daily data to update the counts for the user that have changed.
  prefs: []
  type: TYPE_NORMAL
- en: For both incremental models, I will use the static partitioning technique, which
    is proven to be the most performant [https://discourse.getdbt.com/t/benchmarking-incremental-strategies-on-bigquery/981](https://discourse.getdbt.com/t/benchmarking-incremental-strategies-on-bigquery/981).
    This means that I will operate on the partitions and not scan data, so I will
    pick the two last-day partitions (just in case I have late arriving events), aggregate
    them, and add them to my dbt model `twitter_likes_daily`
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '`twitter_likes_daily` is only 55.52 GB and the daily load scans 536.6 MB! Let’s
    create the monthly aggregates, and for that, we can directly work on `twitter_likes_daily`
    instead of the raw `twitter_likes`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: The `twitter_likes_monthly` is 14.32 GB and 1 load scans only 2.5 GB!
  prefs: []
  type: TYPE_NORMAL
- en: Now let’s go for the totals. For this case, we can decide to run a `merge` operation
    to update the totals of only those users that have new likes in the last 2 days.
    The rest of the users have no updates. So for that case, I can pick the users
    with likes from today from `twitter_likes_daily`, calculate their totals in`twitter_likes_monthly`,
    and finally merge them in the table with the totals `twitter_likes_total` . Note
    that the table is `clustered` using the `user_id` because a merge with a cluster
    on the id is more performant that a regular merge [https://discourse.getdbt.com/t/benchmarking-incremental-strategies-on-bigquery/981](https://discourse.getdbt.com/t/benchmarking-incremental-strategies-on-bigquery/981)
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '`twitter_likes_total` is 1.6 GB and scans 16 GB! So from 869.91 GB we ended
    up scanning only 16 GB and producing a bunch of light and cheap analytical tables.'
  prefs: []
  type: TYPE_NORMAL
- en: 'However, the approach of `twitter_likes_total` could also have been a table
    configuration on `dbt` that only scans the `twitter_likes_monthly` . That would
    be like below:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: The above script will scan 14.5 GB so actually lighter than the `merge` procedure!
    What just happened here? It is important to understand that a merge can be less
    effective than a full table recreation. Given that the table that we are scanning
    `twitter_likes_monthly` is not that large and 14.5 GB, a full table recreation
    is more performant than a `merge`. So when the data ends up not being that large,
    a merge would be really over-engineering.
  prefs: []
  type: TYPE_NORMAL
- en: '**Let’s wrap up**'
  prefs: []
  type: TYPE_NORMAL
- en: The scans of raw immutable events can be efficiently treated by using incremental
    models on `dbt`, and specifically, using a static insert+overwrite strategy. The
    improvement can be huge, from TB to a couple of GB daily for the dbt load, but
    most importantly, enabling data analysts to run queries on what would have been
    a costly scan of raw tables to some light, pre-aggregated tables. Finally, while
    appending data to a table might look theoretically cheaper than re-creating the
    table, depending on the use case, sometimes using a `merge` can be less performant
    than a full table recreation, when the source data is a couple of GB.
  prefs: []
  type: TYPE_NORMAL
