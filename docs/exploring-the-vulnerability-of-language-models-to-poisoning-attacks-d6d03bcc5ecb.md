# 探索语言模型对毒化攻击的脆弱性

> 原文：[`towardsdatascience.com/exploring-the-vulnerability-of-language-models-to-poisoning-attacks-d6d03bcc5ecb?source=collection_archive---------5-----------------------#2023-05-10`](https://towardsdatascience.com/exploring-the-vulnerability-of-language-models-to-poisoning-attacks-d6d03bcc5ecb?source=collection_archive---------5-----------------------#2023-05-10)

## 语言模型的优势是否能转化为它们的弱点？

[](https://pandeyparul.medium.com/?source=post_page-----d6d03bcc5ecb--------------------------------)![Parul Pandey](https://pandeyparul.medium.com/?source=post_page-----d6d03bcc5ecb--------------------------------)[](https://towardsdatascience.com/?source=post_page-----d6d03bcc5ecb--------------------------------)![Towards Data Science](https://towardsdatascience.com/?source=post_page-----d6d03bcc5ecb--------------------------------) [Parul Pandey](https://pandeyparul.medium.com/?source=post_page-----d6d03bcc5ecb--------------------------------)

·

[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F7053de462a28&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fexploring-the-vulnerability-of-language-models-to-poisoning-attacks-d6d03bcc5ecb&user=Parul+Pandey&userId=7053de462a28&source=post_page-7053de462a28----d6d03bcc5ecb---------------------post_header-----------) 发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----d6d03bcc5ecb--------------------------------) ·9 分钟阅读·2023 年 5 月 10 日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fd6d03bcc5ecb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fexploring-the-vulnerability-of-language-models-to-poisoning-attacks-d6d03bcc5ecb&user=Parul+Pandey&userId=7053de462a28&source=-----d6d03bcc5ecb---------------------clap_footer-----------)

--

[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fd6d03bcc5ecb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fexploring-the-vulnerability-of-language-models-to-poisoning-attacks-d6d03bcc5ecb&source=-----d6d03bcc5ecb---------------------bookmark_footer-----------)![](img/e2f0cf785260a35cc986628ea62f3e8c.png)

图片由 [FLY:D](https://unsplash.com/@flyd2069?utm_source=medium&utm_medium=referral) 提供，来源于 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)

2016 年，微软经历了一次与其聊天机器人 Tay 相关的[重大事件](https://en.wikipedia.org/wiki/Tay_(chatbot))，突显了数据中毒的潜在危险。Tay 是由微软研究院的一些顶尖人才设计的先进聊天机器人，旨在与 Twitter 上的用户互动并提高对人工智能的认识。不幸的是，在其上线后的 16 小时内，Tay 表现出了极为不当和冒犯的行为，迫使微软将其关闭。

[](https://en.wikipedia.org/wiki/Tay_%28chatbot%29?source=post_page-----d6d03bcc5ecb--------------------------------) [## Tay (chatbot) - Wikipedia

### Tay 是一个人工智能聊天机器人，最初由微软公司于 3 月通过 Twitter 发布。

en.wikipedia.org](https://en.wikipedia.org/wiki/Tay_%28chatbot%29?source=post_page-----d6d03bcc5ecb--------------------------------)

> 那么，究竟发生了什么？

这一事件发生的原因是用户利用了 Tay 的自适应学习系统，故意向其提供种族歧视和露骨内容。这种操控导致聊天机器人将不当内容纳入其训练数据中，从而使 Tay 在互动中生成了冒犯性的输出。
