- en: A Simple Way of Improving Zero-Shot CLIP Performance
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 提高零-shot CLIP 性能的简单方法
- en: 原文：[https://towardsdatascience.com/simple-way-of-improving-zero-shot-clip-performance-4eae474cb447?source=collection_archive---------5-----------------------#2023-11-03](https://towardsdatascience.com/simple-way-of-improving-zero-shot-clip-performance-4eae474cb447?source=collection_archive---------5-----------------------#2023-11-03)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/simple-way-of-improving-zero-shot-clip-performance-4eae474cb447?source=collection_archive---------5-----------------------#2023-11-03](https://towardsdatascience.com/simple-way-of-improving-zero-shot-clip-performance-4eae474cb447?source=collection_archive---------5-----------------------#2023-11-03)
- en: Part 1 — Customized Prompts via Language Models (CuPL)
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第1部分 — 通过语言模型定制提示（CuPL）
- en: '[](https://medium.com/@alexml0123?source=post_page-----4eae474cb447--------------------------------)[![Alexey
    Kravets](../Images/3b31f9b3c73c6c7ca709f845e6f70023.png)](https://medium.com/@alexml0123?source=post_page-----4eae474cb447--------------------------------)[](https://towardsdatascience.com/?source=post_page-----4eae474cb447--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----4eae474cb447--------------------------------)
    [Alexey Kravets](https://medium.com/@alexml0123?source=post_page-----4eae474cb447--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@alexml0123?source=post_page-----4eae474cb447--------------------------------)[![Alexey
    Kravets](../Images/3b31f9b3c73c6c7ca709f845e6f70023.png)](https://medium.com/@alexml0123?source=post_page-----4eae474cb447--------------------------------)[](https://towardsdatascience.com/?source=post_page-----4eae474cb447--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----4eae474cb447--------------------------------)
    [Alexey Kravets](https://medium.com/@alexml0123?source=post_page-----4eae474cb447--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fcf3e4a05b535&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsimple-way-of-improving-zero-shot-clip-performance-4eae474cb447&user=Alexey+Kravets&userId=cf3e4a05b535&source=post_page-cf3e4a05b535----4eae474cb447---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----4eae474cb447--------------------------------)
    ·12 min read·Nov 3, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F4eae474cb447&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsimple-way-of-improving-zero-shot-clip-performance-4eae474cb447&user=Alexey+Kravets&userId=cf3e4a05b535&source=-----4eae474cb447---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fcf3e4a05b535&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsimple-way-of-improving-zero-shot-clip-performance-4eae474cb447&user=Alexey+Kravets&userId=cf3e4a05b535&source=post_page-cf3e4a05b535----4eae474cb447---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----4eae474cb447--------------------------------)
    · 12 min 阅读 · 2023年11月3日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F4eae474cb447&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsimple-way-of-improving-zero-shot-clip-performance-4eae474cb447&user=Alexey+Kravets&userId=cf3e4a05b535&source=-----4eae474cb447---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4eae474cb447&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsimple-way-of-improving-zero-shot-clip-performance-4eae474cb447&source=-----4eae474cb447---------------------bookmark_footer-----------)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4eae474cb447&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsimple-way-of-improving-zero-shot-clip-performance-4eae474cb447&source=-----4eae474cb447---------------------bookmark_footer-----------)'
- en: Unimodal models are designed to work with data from a single mode, which can
    be either text or images. These models specialize in understanding and generating
    content specific to their chosen mode. For example, GPT are excellent at generating
    human-like text. They have been used for tasks like language translation, text
    generation, and answering questions. Convolutional Neural Networks (CNNs) are
    examples of image models that excel at tasks like image classification, object
    detection, and image generation. Currently, many interesting tasks such as Visual
    Question Answering (VQA) and Image-Text retrieval etc. require multimodal capabilities.
    Is it possible to combine both text and image processing? We can! CLIP stands
    out as one of the initial highly successful image-text models, demonstrating proficiency
    in both image recognition and text comprehension.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 单模态模型旨在处理来自单一模态的数据，可能是文本或图像。这些模型专注于理解和生成特定于其选择模态的内容。例如，GPT 擅长生成类人文本，已被用于语言翻译、文本生成和回答问题等任务。卷积神经网络（CNNs）是图像模型的一个例子，在图像分类、物体检测和图像生成等任务中表现出色。目前，许多有趣的任务，如视觉问答（VQA）和图像-文本检索等，要求具备多模态能力。是否可以将文本和图像处理结合起来？可以！CLIP
    是最初成功的图像-文本模型之一，展示了在图像识别和文本理解方面的高超能力。
- en: 'We will divide this article into the following sections:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将把这篇文章分为以下几个部分：
- en: Introduction
  id: totrans-10
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 介绍
- en: Architecture
  id: totrans-11
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 架构
- en: Training process and Contrastive loss
  id: totrans-12
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练过程与对比损失
- en: Zero-shot capability
  id: totrans-13
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 零样本能力
- en: CuPL
  id: totrans-14
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: CuPL
- en: Conclusions
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 结论
- en: Introduction
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: The CLIP model is an impressive zero-shot predictor, enabling predictions on
    tasks it hasn’t explicitly been trained for. As we will see more in detail in
    the next sections, by…
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: CLIP 模型是一个令人印象深刻的零样本预测器，可以在没有明确训练过的任务上进行预测。正如我们在接下来的部分中将详细探讨的那样，通过…
