- en: A Simple Way of Improving Zero-Shot CLIP Performance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/simple-way-of-improving-zero-shot-clip-performance-4eae474cb447?source=collection_archive---------5-----------------------#2023-11-03](https://towardsdatascience.com/simple-way-of-improving-zero-shot-clip-performance-4eae474cb447?source=collection_archive---------5-----------------------#2023-11-03)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Part 1 — Customized Prompts via Language Models (CuPL)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@alexml0123?source=post_page-----4eae474cb447--------------------------------)[![Alexey
    Kravets](../Images/3b31f9b3c73c6c7ca709f845e6f70023.png)](https://medium.com/@alexml0123?source=post_page-----4eae474cb447--------------------------------)[](https://towardsdatascience.com/?source=post_page-----4eae474cb447--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----4eae474cb447--------------------------------)
    [Alexey Kravets](https://medium.com/@alexml0123?source=post_page-----4eae474cb447--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fcf3e4a05b535&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsimple-way-of-improving-zero-shot-clip-performance-4eae474cb447&user=Alexey+Kravets&userId=cf3e4a05b535&source=post_page-cf3e4a05b535----4eae474cb447---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----4eae474cb447--------------------------------)
    ·12 min read·Nov 3, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F4eae474cb447&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsimple-way-of-improving-zero-shot-clip-performance-4eae474cb447&user=Alexey+Kravets&userId=cf3e4a05b535&source=-----4eae474cb447---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4eae474cb447&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsimple-way-of-improving-zero-shot-clip-performance-4eae474cb447&source=-----4eae474cb447---------------------bookmark_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: Unimodal models are designed to work with data from a single mode, which can
    be either text or images. These models specialize in understanding and generating
    content specific to their chosen mode. For example, GPT are excellent at generating
    human-like text. They have been used for tasks like language translation, text
    generation, and answering questions. Convolutional Neural Networks (CNNs) are
    examples of image models that excel at tasks like image classification, object
    detection, and image generation. Currently, many interesting tasks such as Visual
    Question Answering (VQA) and Image-Text retrieval etc. require multimodal capabilities.
    Is it possible to combine both text and image processing? We can! CLIP stands
    out as one of the initial highly successful image-text models, demonstrating proficiency
    in both image recognition and text comprehension.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will divide this article into the following sections:'
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Architecture
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Training process and Contrastive loss
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Zero-shot capability
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: CuPL
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Conclusions
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The CLIP model is an impressive zero-shot predictor, enabling predictions on
    tasks it hasn’t explicitly been trained for. As we will see more in detail in
    the next sections, by…
  prefs: []
  type: TYPE_NORMAL
