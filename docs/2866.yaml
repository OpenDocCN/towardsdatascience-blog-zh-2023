- en: A Quick Guide on Normalization for Your NLP Model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/a-quick-guide-on-normalization-for-your-nlp-model-2dbd7d2d42a7?source=collection_archive---------5-----------------------#2023-09-14](https://towardsdatascience.com/a-quick-guide-on-normalization-for-your-nlp-model-2dbd7d2d42a7?source=collection_archive---------5-----------------------#2023-09-14)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Accelerate your model convergence and stabilize the training process with normalization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@vuphuongthao9611?source=post_page-----2dbd7d2d42a7--------------------------------)[![Thao
    Vu](../Images/9d44a2f199cdc9c29da72d9dc4971561.png)](https://medium.com/@vuphuongthao9611?source=post_page-----2dbd7d2d42a7--------------------------------)[](https://towardsdatascience.com/?source=post_page-----2dbd7d2d42a7--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----2dbd7d2d42a7--------------------------------)
    [Thao Vu](https://medium.com/@vuphuongthao9611?source=post_page-----2dbd7d2d42a7--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fa836aac352ca&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-quick-guide-on-normalization-for-your-nlp-model-2dbd7d2d42a7&user=Thao+Vu&userId=a836aac352ca&source=post_page-a836aac352ca----2dbd7d2d42a7---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----2dbd7d2d42a7--------------------------------)
    ·7 min read·Sep 14, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F2dbd7d2d42a7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-quick-guide-on-normalization-for-your-nlp-model-2dbd7d2d42a7&user=Thao+Vu&userId=a836aac352ca&source=-----2dbd7d2d42a7---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F2dbd7d2d42a7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-quick-guide-on-normalization-for-your-nlp-model-2dbd7d2d42a7&source=-----2dbd7d2d42a7---------------------bookmark_footer-----------)![](../Images/664add61750b752d6992bc44d3645704.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Photo by [Mattia Bericchia](https://unsplash.com/@mattiabericchia?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Efficiently training deep learning models is challenging. The problem becomes
    more difficult with the recent growth of NLP models’ size and architecture complexity.
    To handle billions of parameters, more optimizations are proposed for faster convergence
    and stable training. One of the most remarkable techniques is normalization.
  prefs: []
  type: TYPE_NORMAL
- en: In this article, we will learn about some normalization techniques, how they
    work, and how they can be used for NLP deep models.
  prefs: []
  type: TYPE_NORMAL
- en: Why not BatchNorm?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: BatchNorm [2] is an early normalization technique proposed to solve internal
    covariate shifts.
  prefs: []
  type: TYPE_NORMAL
- en: To explain in simple terms, an internal covariate shift occurs when there is
    a change in the layer’s input data distribution. When the neural networks are
    forced to fit different data distributions, the gradient update changes dramatically
    between batches. Therefore, the models take longer to adjust, learn the correct
    weights and converge. The problem gets worse as the model size grows.
  prefs: []
  type: TYPE_NORMAL
- en: Initial solutions include using a small learning rate (so the impact of data…
  prefs: []
  type: TYPE_NORMAL
