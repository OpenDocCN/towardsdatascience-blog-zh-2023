- en: Introduction to Weight Quantization
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 权重量化简介
- en: 原文：[https://towardsdatascience.com/introduction-to-weight-quantization-2494701b9c0c?source=collection_archive---------0-----------------------#2023-07-07](https://towardsdatascience.com/introduction-to-weight-quantization-2494701b9c0c?source=collection_archive---------0-----------------------#2023-07-07)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/introduction-to-weight-quantization-2494701b9c0c?source=collection_archive---------0-----------------------#2023-07-07](https://towardsdatascience.com/introduction-to-weight-quantization-2494701b9c0c?source=collection_archive---------0-----------------------#2023-07-07)
- en: Reducing the size of Large Language Models with 8-bit quantization
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用8位量化来减少大型语言模型的大小
- en: '[](https://medium.com/@mlabonne?source=post_page-----2494701b9c0c--------------------------------)[![Maxime
    Labonne](../Images/a7efdd305e3cc77d5509bbb1076d57d8.png)](https://medium.com/@mlabonne?source=post_page-----2494701b9c0c--------------------------------)[](https://towardsdatascience.com/?source=post_page-----2494701b9c0c--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----2494701b9c0c--------------------------------)
    [Maxime Labonne](https://medium.com/@mlabonne?source=post_page-----2494701b9c0c--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@mlabonne?source=post_page-----2494701b9c0c--------------------------------)[![Maxime
    Labonne](../Images/a7efdd305e3cc77d5509bbb1076d57d8.png)](https://medium.com/@mlabonne?source=post_page-----2494701b9c0c--------------------------------)[](https://towardsdatascience.com/?source=post_page-----2494701b9c0c--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----2494701b9c0c--------------------------------)
    [Maxime Labonne](https://medium.com/@mlabonne?source=post_page-----2494701b9c0c--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fdc89da634938&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintroduction-to-weight-quantization-2494701b9c0c&user=Maxime+Labonne&userId=dc89da634938&source=post_page-dc89da634938----2494701b9c0c---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----2494701b9c0c--------------------------------)
    ·14 min read·Jul 7, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F2494701b9c0c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintroduction-to-weight-quantization-2494701b9c0c&user=Maxime+Labonne&userId=dc89da634938&source=-----2494701b9c0c---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fdc89da634938&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintroduction-to-weight-quantization-2494701b9c0c&user=Maxime+Labonne&userId=dc89da634938&source=post_page-dc89da634938----2494701b9c0c---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----2494701b9c0c--------------------------------)
    ·14分钟阅读·2023年7月7日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F2494701b9c0c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintroduction-to-weight-quantization-2494701b9c0c&user=Maxime+Labonne&userId=dc89da634938&source=-----2494701b9c0c---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F2494701b9c0c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintroduction-to-weight-quantization-2494701b9c0c&source=-----2494701b9c0c---------------------bookmark_footer-----------)![](../Images/cb9fa85d2c510e5126e6d30fce29eff4.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F2494701b9c0c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintroduction-to-weight-quantization-2494701b9c0c&source=-----2494701b9c0c---------------------bookmark_footer-----------)![](../Images/cb9fa85d2c510e5126e6d30fce29eff4.png)'
- en: Large Language Models (LLMs) are known for their extensive computational requirements.
    Typically, the size of a model is calculated by multiplying the number of parameters
    (**size**) by the precision of these values (**data type**). However, to save
    memory, weights can be stored using lower-precision data types through a process
    known as quantization.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）以其广泛的计算需求而闻名。通常，通过将参数数量（**大小**）乘以这些值的精度（**数据类型**），可以计算模型的大小。然而，为了节省内存，可以通过一种称为量化的过程使用较低精度的数据类型来存储权重。
- en: 'We distinguish two main families of weight quantization techniques in the literature:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在文献中区分了两大类权重量化技术：
- en: '**Post-Training Quantization** (PTQ) is a straightforward technique where the
    weights of an already trained model are converted to lower precision without necessitating
    any retraining. Although easy to implement, PTQ is associated with potential performance
    degradation.'
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**训练后量化**（PTQ）是一种直接的技术，其中已训练好的模型的权重被转换为较低的精度，而不需要进行任何重新训练。尽管实施起来简单，但PTQ可能会导致性能下降。'
- en: '**Quantization-Aware Training** (QAT) incorporates the weight conversion process
    during the pre-training or fine-tuning stage, resulting in enhanced model performance.
    However, QAT is computationally expensive and demands representative training
    data.'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**量化感知训练**（QAT）在预训练或微调阶段纳入权重转换过程，从而提升模型性能。然而，QAT计算成本高且需要代表性的训练数据。'
- en: In this article, we focus on PTQ to reduce the precision of our parameters.
    To get a good intuition, we will apply both naïve and more sophisticated techniques
    to a toy example using a GPT-2 model.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 本文聚焦于PTQ以减少参数的精度。为了获得良好的直觉，我们将应用简单和更复杂的技术于一个使用GPT-2模型的示例。
- en: The entire code is freely available on [Google Colab](https://colab.research.google.com/drive/1DPr4mUQ92Cc-xf4GgAaB6dFcFnWIvqYi?usp=sharing)
    and [GitHub](https://github.com/mlabonne/llm-course/blob/main/Introduction_to_Weight_Quantization.ipynb).
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 完整代码可以在[Google Colab](https://colab.research.google.com/drive/1DPr4mUQ92Cc-xf4GgAaB6dFcFnWIvqYi?usp=sharing)和[GitHub](https://github.com/mlabonne/llm-course/blob/main/Introduction_to_Weight_Quantization.ipynb)上自由获取。
- en: 📚 Background on Floating Point Representation
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 📚 浮点表示的背景
- en: The choice of data type dictates the quantity of computational resources required,
    affecting the speed and efficiency of the model. In deep learning applications,
    balancing precision and computational performance becomes a vital exercise as
    higher precision often implies greater computational demands.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 数据类型的选择决定了所需的计算资源量，影响模型的速度和效率。在深度学习应用中，平衡精度和计算性能成为一个至关重要的任务，因为更高的精度通常意味着更大的计算需求。
- en: 'Among various data types, floating point numbers are predominantly employed
    in deep learning due to their ability to represent a wide range of values with
    high precision. Typically, a floating point number uses *n* bits to store a numerical
    value. These *n* bits are further partitioned into three distinct components:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在各种数据类型中，浮点数由于能够以高精度表示广泛的值范围，因此在深度学习中被广泛使用。通常，浮点数使用*n*位来存储数值。这些*n*位进一步被划分为三个不同的部分：
- en: '**Sign**: The sign bit indicates the positive or negative nature of the number.
    It uses one bit where 0 indicates a positive number and 1 signals a negative number.'
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**符号**：符号位指示数字的正负性质。它使用一个位，其中0表示正数，1表示负数。'
- en: '**Exponent**: The exponent is a segment of bits that represents the power to
    which the base (usually 2 in binary representation) is raised. The exponent can
    also be positive or negative, allowing the number to represent very large or very
    small values.'
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**指数**：指数是一段位表示基数（在二进制表示中通常为2）的幂。指数可以是正数或负数，从而使数字能够表示非常大或非常小的值。'
- en: '**Significand/Mantissa**: The remaining bits are used to store the significand,
    also referred to as the mantissa. This represents the significant digits of the
    number. The precision of the number heavily depends on the length of the significand.'
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**尾数/有效数字**：剩余的位用于存储尾数，也称为有效数字。它表示数字的有效数字。数字的精度严重依赖于尾数的长度。'
- en: 'This design allows floating point numbers to cover a wide range of values with
    varying levels of precision. The formula used for this representation is:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 这种设计允许浮点数以不同精度覆盖广泛的值范围。用于这种表示的公式是：
- en: '![](../Images/2e8ab62ec7cb0706b3d6b99f32abc5fa.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2e8ab62ec7cb0706b3d6b99f32abc5fa.png)'
- en: 'To understand this better, let’s delve into some of the most commonly used
    data types in deep learning: float32 (FP32), float16 (FP16), and bfloat16 (BF16):'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解这一点，让我们深入探讨在深度学习中最常用的数据类型：float32（FP32）、float16（FP16）和bfloat16（BF16）：
- en: '**FP32** uses 32 bits to represent a number: one bit for the sign, eight for
    the exponent, and the remaining 23 for the significand. While it provides a high
    degree of precision, the downside of FP32 is its high computational and memory
    footprint.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**FP32** 使用32位表示一个数字：一个位用于符号，八个位用于指数，其余23个位用于尾数。虽然它提供了较高的精度，但FP32的缺点是其计算和内存开销较大。'
- en: '**FP16** uses 16 bits to store a number: one is used for the sign, five for
    the exponent, and ten for the significand. Although this makes it more memory-efficient
    and accelerates computations, the reduced range and precision can introduce numerical
    instability, potentially impacting model accuracy.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**FP16** 使用16位存储一个数字：一个用于符号，五个位用于指数，十个位用于尾数。虽然这使其在内存上更高效并加速计算，但减少的范围和精度可能会引入数值不稳定性，可能影响模型的准确性。'
- en: '**BF16** is also a 16-bit format but with one bit for the sign, *eight* for
    the exponent, and *seven* for the significand. BF16 expands the representable
    range compared to FP16, thus decreasing underflow and overflow risks. Despite
    a reduction in precision due to fewer significand bits, BF16 typically does not
    significantly impact model performance and is a useful compromise for deep learning
    tasks.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**BF16**也是一种16位格式，但有一个符号位，*八*个位用于指数，*七*个位用于尾数。BF16扩展了表示范围，相较于FP16，减少了下溢和溢出的风险。尽管由于尾数位数减少而精度降低，但BF16通常不会显著影响模型性能，是深度学习任务的一个有用折中方案。'
- en: '![](../Images/7f97ce565ee75a23253ee19eeff714bd.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7f97ce565ee75a23253ee19eeff714bd.png)'
- en: Image by author
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片
- en: In ML jargon, FP32 is often termed “full precision” (4 bytes), while BF16 and
    FP16 are “half-precision” (2 bytes). But could we do even better and store weights
    using a single byte? The answer is the INT8 data type, which consists of an 8-bit
    representation capable of storing 2⁸ = 256 different values. In the next section,
    we’ll see how to convert FP32 weights into an INT8 format.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习术语中，FP32通常被称为“全精度”（4字节），而BF16和FP16则被称为“半精度”（2字节）。但我们是否可以更进一步，用一个字节来存储权重？答案是INT8数据类型，它由一个8位表示组成，能够存储2⁸
    = 256种不同的值。在下一节中，我们将看到如何将FP32权重转换为INT8格式。
- en: 🔰 Naïve 8-bit Quantization
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 🔰 初步的8位量化
- en: 'In this section, we will implement two quantization techniques: a symmetric
    one with **absolute maximum (absmax) quantization** and an asymmetric one with
    **zero-point quantization**. In both cases, the goal is to map an FP32 tensor
    **X** (original weights) to an INT8 tensor **X_quant** (quantized weights).'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们将实现两种量化技术：一种是具有**绝对最大值（absmax）量化**的对称量化，另一种是具有**零点量化**的非对称量化。在这两种情况下，目标是将FP32张量**X**（原始权重）映射到INT8张量**X_quant**（量化权重）。
- en: With **absmax quantization**, the original number is divided by the absolute
    maximum value of the tensor and multiplied by a scaling factor (127) to map inputs
    into the range [-127, 127]. To retrieve the original FP16 values, the INT8 number
    is divided by the quantization factor, acknowledging some loss of precision due
    to rounding.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 使用**absmax量化**，原始数值除以张量的绝对最大值，并乘以缩放因子（127），将输入映射到范围[-127, 127]。要检索原始FP16值，INT8数值除以量化因子，承认由于四舍五入导致的一些精度损失。
- en: '![](../Images/7dfd58e4be8f1412c262188ca9eecb4f.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7dfd58e4be8f1412c262188ca9eecb4f.png)'
- en: 'For instance, let’s say we have an absolution maximum value of 3.2\. A weight
    of 0.1 would be quantized to *round(0.1 × 127/3.2) = 4*. If we want to dequantize
    it, we would get *4 × 3.2/127 = 0.1008*, which implies an error of 0.008\. Here’s
    the corresponding Python implementation:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设我们有一个绝对最大值为3.2的值。一个0.1的权重将被量化为*round(0.1 × 127/3.2) = 4*。如果我们想将其反量化，将得到*4
    × 3.2/127 = 0.1008*，这意味着一个0.008的误差。以下是对应的Python实现：
- en: '[PRE0]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'With **zero-point quantization**, we can consider asymmetric input distributions,
    which is useful when you consider the output of a ReLU function (only positive
    values), for example. The input values are first scaled by the total range of
    values (255) divided by the difference between the maximum and minimum values.
    This distribution is then shifted by the zero-point to map it into the range [-128,
    127] (notice the extra value compared to absmax). First, we calculate the scale
    factor and the zero-point value:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 使用**零点量化**，我们可以考虑非对称输入分布，这在考虑ReLU函数的输出（仅正值）时特别有用。输入值首先按值的总范围（255）除以最大值和最小值之间的差异进行缩放。然后通过零点偏移将这个分布映射到范围[-128,
    127]（注意与absmax相比的额外值）。首先，我们计算缩放因子和零点值：
- en: '![](../Images/8553578f6e6bc10ce524fcf7ec3c4088.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8553578f6e6bc10ce524fcf7ec3c4088.png)'
- en: 'Then, we can use these variables to quantize or dequantize our weights:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以使用这些变量来量化或反量化我们的权重：
- en: '![](../Images/bd1fff48751ed91f96d614a7b1fdced9.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bd1fff48751ed91f96d614a7b1fdced9.png)'
- en: 'Let’s take an example: we have a maximum value of 3.2 and a minimum value of
    -3.0\. We can calculate the scale is *255/(3.2 + 3.0) = 41.13* and the zero-point
    *-round(41.13 × -3.0) - 128 = 123 -128 = -5*, so our previous weight of 0.1 would
    be quantized to *round(41.13 × 0.1 -5) = -1*. This is very different from the
    previous value obtained using absmax (4 vs. -1).'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 例如：我们有一个最大值为3.2和一个最小值为-3.0。我们可以计算出缩放因子为*255/(3.2 + 3.0) = 41.13*，零点为*-round(41.13
    × -3.0) - 128 = 123 -128 = -5*，因此我们之前的权重0.1将被量化为*round(41.13 × 0.1 -5) = -1*。这与之前使用absmax获得的值（4与-1）差异很大。
- en: '![](../Images/773e8d8dbc9ac7c43c8d7f0fbfbe5fda.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/773e8d8dbc9ac7c43c8d7f0fbfbe5fda.png)'
- en: Image by author
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片
- en: 'The Python implementation is quite straightforward:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: Python 实现非常简单：
- en: '[PRE1]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Instead of relying on complete toy examples, we can use these two functions
    on a real model thanks to the `transformers`library.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以利用 `transformers` 库在真实模型上使用这两个函数，而不是依赖完整的玩具示例。
- en: We start by loading the model and tokenizer for GPT-2\. This is a very small
    model we probably don’t want to quantize, but it will be good enough for this
    tutorial. First, we want to observe the model’s size so we can compare it later
    and evaluate the **memory savings** due to 8-bit quantization.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先加载 GPT-2 的模型和分词器。这是一个非常小的模型，我们可能不想对其进行量化，但它对于本教程来说足够了。首先，我们想观察模型的大小，以便稍后进行比较，并评估由于
    8 位量化而节省的**内存**。
- en: '[PRE2]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[PRE3]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[PRE4]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The size of the GPT-2 model is approximately 487MB in FP32\. The next step consists
    of quantizing the weights using zero-point and absmax quantization. In the following
    example, we apply these techniques to the first attention layer of GPT-2 to see
    the results.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-2 模型在 FP32 中的大小约为 487MB。下一步是使用零点和 absmax 量化权重。在以下示例中，我们将这些技术应用于 GPT-2 的第一个注意力层，以查看结果。
- en: '[PRE5]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '[PRE6]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The difference between the original (FP32) and quantized values (INT8) is clear,
    but the difference between absmax and zero-point weights is more subtle. In this
    case, the inputs look shifted by a value of -1\. This suggests that the weight
    distribution in this layer is quite symmetric.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 原始（FP32）值和量化值（INT8）之间的差异很明显，但 absmax 和零点权重之间的差异则更微妙。在这种情况下，输入值看起来偏移了 -1。这表明这一层的权重分布相当对称。
- en: 'We can compare these techniques by quantizing every layer in GPT-2 (linear
    layers, attention layers, etc.) and create two new models: `model_abs` and `model_zp`.
    To be precise, we will actually replace the original weights with ***de***-quantized
    ones. This has two benefits: it allows us to 1/ compare the distribution of our
    weights (same scale) and 2/ actually run the models.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过对 GPT-2 中的每一层（线性层、注意力层等）进行量化，并创建两个新模型：`model_abs` 和 `model_zp`，来比较这些技术。准确地说，我们将实际用***去***量化的权重替换原始权重。这有两个好处：它使我们可以
    1/ 比较权重的分布（相同的尺度），以及 2/ 实际运行这些模型。
- en: Indeed, PyTorch doesn’t allow INT8 matrix multiplication by default. In a real
    scenario, we would dequantize them to run the model (in FP16 for example) but
    store them as INT8\. In the next section, we will use the `[bitsandbytes](https://github.com/TimDettmers/bitsandbytes)`
    library to solve this issue.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 确实，PyTorch 默认不允许 INT8 矩阵乘法。在实际场景中，我们会将其去量化以运行模型（例如 FP16），但以 INT8 存储。在下一部分，我们将使用
    `[bitsandbytes](https://github.com/TimDettmers/bitsandbytes)` 库来解决这个问题。
- en: '[PRE7]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Now that our models have been quantized, we want to check the impact of this
    process. Intuitively, we want to make sure that the quantized weights are **close
    to the original ones**. A visual way to check it is to plot the distribution of
    the dequantized and original weights. If the quantization is lossy, it would drastically
    change the weight distribution.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们的模型已经被量化，我们想检查这一过程的影响。直观上，我们想确保量化的权重**接近原始权重**。一种直观的方法是绘制去量化和原始权重的分布。如果量化有损，它将极大地改变权重分布。
- en: The following figure shows this comparison, where the blue histogram represents
    the original (FP32) weights, and the red one represents the dequantized (from
    INT8) weights. Note that we only display this plot between -2 and 2 because of
    outliers with very high absolute values (more on that later).
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了这种比较，其中蓝色直方图表示原始（FP32）权重，红色直方图表示去量化后的（来自 INT8）权重。注意，我们仅在 -2 和 2 之间显示此图，因为有一些绝对值非常高的离群值（稍后会详细讨论）。
- en: '![](../Images/9e93b5636f8e65de4bb386d68ad67b61.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9e93b5636f8e65de4bb386d68ad67b61.png)'
- en: Both plots are quite similar, with a surprising spike around 0\. This spike
    shows that our quantization is quite lossy since reversing the process doesn’t
    output the original values. This is particularly true for the absmax model, which
    displays both a lower valley and a higher spike around 0.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 两个图表非常相似，都在 0 附近有一个惊人的峰值。这个峰值表明我们的量化相当有损，因为反转过程并没有输出原始值。这在 absmax 模型中尤为明显，该模型在
    0 附近显示了一个更低的谷值和一个更高的峰值。
- en: Let’s compare the performance of the original and quantized models. For this
    purpose, we define a `generate_text()` function to generate 50 tokens with [top-k
    sampling](https://mlabonne.github.io/blog/posts/2023-06-07-Decoding_strategies.html).
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们比较原始模型和量化模型的性能。为此，我们定义了一个 `generate_text()` 函数来生成 50 个标记，并使用 [top-k 采样](https://mlabonne.github.io/blog/posts/2023-06-07-Decoding_strategies.html)。
- en: '[PRE8]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '[PRE9]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Instead of trying to see if one output makes more sense than the others, we
    can quantify it by calculating the **perplexity** of each output. This is a common
    metric used to evaluate language models, which measures the uncertainty of a model
    in predicting the next token in a sequence. In this comparison, we make the common
    assumption that the lower the score, the better the model is. In practice, a sentence
    with a high perplexity could also be correct.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过计算每个输出的**困惑度**来量化它，而不是尝试判断一个输出是否比其他输出更有意义。这是一个常用的评估语言模型的指标，用于测量模型在预测序列中下一个标记时的不确定性。在这个比较中，我们做了一个常见的假设，即分数越低，模型越好。实际上，高困惑度的句子也可能是正确的。
- en: We implement it using a minimal function since it doesn’t need to consider details
    like the length of the context window since our sentences are short.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用一个最小函数来实现它，因为不需要考虑上下文窗口的长度，因为我们的句子较短。
- en: '[PRE10]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '[PRE11]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: We see that the perplexity of the original model is **slightly lower** than
    the two others. A single experiment is not very reliable, but we could repeat
    this process multiple times to see the difference between each model. In theory,
    zero-point quantization should be slightly better than absmax, but is also more
    costly to compute.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 我们发现原始模型的困惑度**略低**于另外两个模型。一次实验并不十分可靠，但我们可以多次重复这个过程，以查看每个模型之间的差异。从理论上讲，零点量化应稍微优于
    absmax，但计算成本也更高。
- en: 'In this example, we applied quantization techniques to entire layers (per-tensor
    basis). However, we could apply it at different granularity levels: from the entire
    model to individual values. Quantizing the entire model in one pass would seriously
    degrade the performance, while quantizing individual values would create a big
    overhead. In practice, we often prefer the **vector-wise quantization**, which
    considers the variability of values in rows and columns inside of the same tensor.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们将量化技术应用于整个层（每张量基础）。然而，我们可以在不同的粒度水平应用它：从整个模型到单个值。一次性量化整个模型会严重降低性能，而量化单个值会产生很大的开销。在实践中，我们通常更喜欢**逐向量量化**，它考虑了同一张量中行和列的值的变异性。
- en: However, even vector-wise quantization doesn’t solve the problem of outlier
    features. Outlier features are extreme values (negative or positive) that appear
    in all transformer layers when the model reach a certain scale (>6.7B parameters).
    This is an issue since a single outlier can reduce the precision for all other
    values. But discarding these outlier features is not an option since it would
    **greatly degrade** the model’s performance.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，即使是逐向量量化也不能解决离群特征的问题。离群特征是出现在所有变换器层中的极端值（负值或正值），当模型达到某个规模（>6.7B 参数）时。这是一个问题，因为一个离群值可以降低所有其他值的精度。但丢弃这些离群特征并不是一个选项，因为这会**大大降低**模型的性能。
- en: 🔢 8-bit Quantization with LLM.int8()
  id: totrans-70
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 🔢 8 位量化与 LLM.int8()
- en: Introduced by [Dettmers et al. (2022)](https://arxiv.org/abs/2208.07339), LLM.int8()
    is a solution to the outlier problem. It relies on a vector-wise (absmax) quantization
    scheme and introduces mixed-precision quantization. This means that outlier features
    are processed in a FP16 format to retain their precision, while the other values
    are processed in an INT8 format. As outliers represent about 0.1% of values, this
    effectively reduces the memory footprint of the LLM by almost 2x.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 由[Dettmers et al. (2022)](https://arxiv.org/abs/2208.07339)引入的LLM.int8() 是解决离群值问题的一个方案。它依赖于逐向量（absmax）量化方案，并引入了混合精度量化。这意味着离群特征以
    FP16 格式处理以保持其精度，而其他值则以 INT8 格式处理。由于离群值约占 0.1% 的值，这有效地将 LLM 的内存占用减少了近 2 倍。
- en: '![](../Images/f7ce7de0c94eb22aaf2c926341e6f2ff.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f7ce7de0c94eb22aaf2c926341e6f2ff.png)'
- en: Image by author
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片
- en: 'LLM.int8() works by conducting matrix multiplication computation in three key
    steps:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: LLM.int8() 通过三个关键步骤进行矩阵乘法计算：
- en: Extract columns from the input hidden states **X** containing outlier features
    using a custom threshold.
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从输入隐藏状态**X**中提取包含离群特征的列，使用自定义阈值。
- en: Perform the matrix multiplication of the outliers using FP16 and the non-outliers
    using INT8 with vector-wise quantization (row-wise for the hidden state **X**
    and column-wise for the weight matrix **W**).
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 FP16 对离群值进行矩阵乘法，并使用 INT8 进行逐向量量化（对隐藏状态**X**进行逐行量化，对权重矩阵**W**进行逐列量化）。
- en: Dequantize the non-outlier results (INT8 to FP16) and add them to the outlier
    results to get the full result in FP16.
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对非离群结果进行反量化（从 INT8 到 FP16），并将其与离群结果相加，以获得完整的 FP16 结果。
- en: '![](../Images/3b4bb0a164b5ebebc1dcaca4b96f34d7.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3b4bb0a164b5ebebc1dcaca4b96f34d7.png)'
- en: Image by author
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片
- en: This approach is necessary because 8-bit precision is limited and can lead to
    substantial errors when quantizing a vector with large values. These errors also
    tend to amplify as they propagate through multiple layers.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法是必要的，因为 8 位精度有限，当量化具有大值的向量时可能导致显著的误差。这些误差也会随着它们在多个层之间传播而放大。
- en: We can easily use this technique thanks to the integration of the `bitsandbytes`
    library into the Hugging Face ecosystem. We just need to specify `load_in_8bit=True`
    when loading the model (it also requires a GPU).
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 `bitsandbytes` 库已集成到 Hugging Face 生态系统中，我们可以轻松使用这项技术。我们只需在加载模型时指定 `load_in_8bit=True`（它还需要一个
    GPU）。
- en: '[PRE12]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '[PRE13]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'With this extra line of code, the model is now almost three times smaller (168MB
    vs. 487MB). We can even compare the distribution of the original and quantized
    weights as we did earlier:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这额外的一行代码，模型现在几乎缩小了三倍（168MB vs. 487MB）。我们甚至可以像之前那样比较原始权重和量化权重的分布：
- en: '![](../Images/b46d4868b6ce8ce99e4746e100fc1525.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b46d4868b6ce8ce99e4746e100fc1525.png)'
- en: In this case, we see spikes around -2, -1, 0, 1, 2, etc. These values correspond
    to the parameters stored in the INT8 format (non-outliers). You can verify it
    by printing the model’s weights using `model_int8.parameters()`.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们看到在 -2，-1，0，1，2 等值周围有尖峰。这些值对应于以 INT8 格式存储的参数（非异常值）。你可以通过使用 `model_int8.parameters()`
    打印模型的权重来验证这一点。
- en: We can also generate text with this quantized model and compare it to the original
    model.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以使用这个量化模型生成文本，并与原始模型进行比较。
- en: '[PRE14]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '[PRE15]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Once again, it is difficult to judge what is the best output, but we can rely
    on the perplexity metric to give us an (approximate) answer.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 再次强调，判断最佳输出是困难的，但我们可以依靠困惑度指标来给出一个（大致的）答案。
- en: '[PRE16]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '[PRE17]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'In this case, the perplexity of the quantized model is twice as low as the
    original one. In general, this is not the case, but it shows that this quantization
    technique is very competitive. In fact, the authors of LLM.int8() show that the
    performance degradation is so low it’s negligible (<1%). However, it has an additional
    cost in terms of computation: LLM.int8() is roughly about 20% slower for large
    models.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，量化模型的困惑度是原始模型的两倍低。一般来说，这种情况并不常见，但它显示了这种量化技术的竞争力。实际上，LLM.int8()的作者展示了性能下降非常小，可以忽略不计（<1%）。然而，它在计算方面有额外的成本：对于大模型，LLM.int8()大约慢了
    20%。
- en: Conclusion
  id: totrans-94
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: 'This article provided an overview of the most popular weight quantization techniques.
    We started by gaining an understanding of floating point representation, before
    introducing two techniques for 8-bit quantization: **absmax** and **zero-point
    quantization**. However, their limitations, particularly when it comes to handling
    outliers, led to **LLM.int8()**, a technique that also preserves the model’s performance.
    This approach underlines the progress being made in the field of weight quantization,
    revealing the importance of properly addressing outliers.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 本文概述了最流行的权重量化技术。我们首先了解了浮点表示，然后介绍了两种 8 位量化技术：**absmax** 和 **零点量化**。然而，它们的局限性，特别是在处理异常值时，导致了
    **LLM.int8()** 技术，该技术还保留了模型的性能。这种方法突显了权重量化领域的进展，揭示了正确处理异常值的重要性。
- en: Looking forward, our next article will explore the GPTQ weight quantization
    technique in depth. This technique, introduced by [Frantar et al.](https://arxiv.org/abs/2210.17323),
    only utilizes 4 bits and represents a significant advancement in the field of
    weight quantization. We will provide a comprehensive guide on how to implement
    GPTQ using the AutoGPTQ library.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 展望未来，我们的下一篇文章将深入探讨 GPTQ 权重量化技术。这项技术由 [Frantar 等人](https://arxiv.org/abs/2210.17323)
    提出，只使用了 4 位，并且代表了权重量化领域的重大进展。我们将提供如何使用 AutoGPTQ 库实现 GPTQ 的全面指南。
- en: If you’re interested in more technical content around LLMs, follow me on Twitter
    [@maximelabonne](https://twitter.com/maximelabonne).
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你对更多关于LLM的技术内容感兴趣，可以在Twitter上关注我 [@maximelabonne](https://twitter.com/maximelabonne)。
- en: References
  id: totrans-98
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'T. Dettmers, M. Lewis, Y. Belkada, and L. Zettlemoyer, [LLM.int8(): 8-bit Matrix
    Multiplication for Transformers at Scale](https://arxiv.org/abs/2208.07339). 2022.'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'T. Dettmers, M. Lewis, Y. Belkada, 和 L. Zettlemoyer，[LLM.int8(): 大规模变压器的 8
    位矩阵乘法](https://arxiv.org/abs/2208.07339)。2022年。'
- en: Y. Beldaka, and T. Dettmers, [A Gentle Introduction to 8-bit Matrix Multiplication](https://huggingface.co/blog/hf-bitsandbytes-integration),
    Hugging Face Blog (2022).
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Y. Beldaka 和 T. Dettmers, [8位矩阵乘法的简明介绍](https://huggingface.co/blog/hf-bitsandbytes-integration),
    Hugging Face Blog (2022)。
- en: A. Gholami, S. Kim, Z. Dong, Z. Yao, M. W. Mahoney, and K. Keutzer, [A Survey
    of Quantization Methods for Efficient Neural Network Inference](https://arxiv.org/abs/2103.13630).
    2021.
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: A. Gholami, S. Kim, Z. Dong, Z. Yao, M. W. Mahoney 和 K. Keutzer, [高效神经网络推理的量化方法综述](https://arxiv.org/abs/2103.13630)。2021年。
- en: 'H. Wu, P. Judd, X. Zhang, M. Isaev, and P. Micikevicius, [Integer Quantization
    for Deep Learning Inference: Principles and Empirical Evaluation](https://arxiv.org/abs/2004.09602).
    2020.'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: H. Wu, P. Judd, X. Zhang, M. Isaev 和 P. Micikevicius, [深度学习推理的整数量化：原理与实证评估](https://arxiv.org/abs/2004.09602)。2020年。
- en: Lilian Weng, [Large Transformer Model Inference Optimization](https://lilianweng.github.io/posts/2023-01-10-inference-optimization/),
    Lil’Log (2023).
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lilian Weng, [大型变压器模型推理优化](https://lilianweng.github.io/posts/2023-01-10-inference-optimization/),
    Lil’Log (2023)。
- en: Kamil Czarnogorski, [Local Large Language Models](https://int8.io/local-large-language-models-beginners-guide/),
    Int8 (2023).
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kamil Czarnogorski, [本地大型语言模型](https://int8.io/local-large-language-models-beginners-guide/),
    Int8 (2023)。
