- en: Introduction to Weight Quantization
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æƒé‡é‡åŒ–ç®€ä»‹
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/introduction-to-weight-quantization-2494701b9c0c?source=collection_archive---------0-----------------------#2023-07-07](https://towardsdatascience.com/introduction-to-weight-quantization-2494701b9c0c?source=collection_archive---------0-----------------------#2023-07-07)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://towardsdatascience.com/introduction-to-weight-quantization-2494701b9c0c?source=collection_archive---------0-----------------------#2023-07-07](https://towardsdatascience.com/introduction-to-weight-quantization-2494701b9c0c?source=collection_archive---------0-----------------------#2023-07-07)
- en: Reducing the size of Large Language Models with 8-bit quantization
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä½¿ç”¨8ä½é‡åŒ–æ¥å‡å°‘å¤§å‹è¯­è¨€æ¨¡å‹çš„å¤§å°
- en: '[](https://medium.com/@mlabonne?source=post_page-----2494701b9c0c--------------------------------)[![Maxime
    Labonne](../Images/a7efdd305e3cc77d5509bbb1076d57d8.png)](https://medium.com/@mlabonne?source=post_page-----2494701b9c0c--------------------------------)[](https://towardsdatascience.com/?source=post_page-----2494701b9c0c--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----2494701b9c0c--------------------------------)
    [Maxime Labonne](https://medium.com/@mlabonne?source=post_page-----2494701b9c0c--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@mlabonne?source=post_page-----2494701b9c0c--------------------------------)[![Maxime
    Labonne](../Images/a7efdd305e3cc77d5509bbb1076d57d8.png)](https://medium.com/@mlabonne?source=post_page-----2494701b9c0c--------------------------------)[](https://towardsdatascience.com/?source=post_page-----2494701b9c0c--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----2494701b9c0c--------------------------------)
    [Maxime Labonne](https://medium.com/@mlabonne?source=post_page-----2494701b9c0c--------------------------------)'
- en: Â·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Â·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fdc89da634938&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintroduction-to-weight-quantization-2494701b9c0c&user=Maxime+Labonne&userId=dc89da634938&source=post_page-dc89da634938----2494701b9c0c---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----2494701b9c0c--------------------------------)
    Â·14 min readÂ·Jul 7, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F2494701b9c0c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintroduction-to-weight-quantization-2494701b9c0c&user=Maxime+Labonne&userId=dc89da634938&source=-----2494701b9c0c---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[å…³æ³¨](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fdc89da634938&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintroduction-to-weight-quantization-2494701b9c0c&user=Maxime+Labonne&userId=dc89da634938&source=post_page-dc89da634938----2494701b9c0c---------------------post_header-----------)
    å‘è¡¨åœ¨ [Towards Data Science](https://towardsdatascience.com/?source=post_page-----2494701b9c0c--------------------------------)
    Â·14åˆ†é’Ÿé˜…è¯»Â·2023å¹´7æœˆ7æ—¥[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F2494701b9c0c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintroduction-to-weight-quantization-2494701b9c0c&user=Maxime+Labonne&userId=dc89da634938&source=-----2494701b9c0c---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F2494701b9c0c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintroduction-to-weight-quantization-2494701b9c0c&source=-----2494701b9c0c---------------------bookmark_footer-----------)![](../Images/cb9fa85d2c510e5126e6d30fce29eff4.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F2494701b9c0c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintroduction-to-weight-quantization-2494701b9c0c&source=-----2494701b9c0c---------------------bookmark_footer-----------)![](../Images/cb9fa85d2c510e5126e6d30fce29eff4.png)'
- en: Large Language Models (LLMs) are known for their extensive computational requirements.
    Typically, the size of a model is calculated by multiplying the number of parameters
    (**size**) by the precision of these values (**data type**). However, to save
    memory, weights can be stored using lower-precision data types through a process
    known as quantization.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä»¥å…¶å¹¿æ³›çš„è®¡ç®—éœ€æ±‚è€Œé—»åã€‚é€šå¸¸ï¼Œé€šè¿‡å°†å‚æ•°æ•°é‡ï¼ˆ**å¤§å°**ï¼‰ä¹˜ä»¥è¿™äº›å€¼çš„ç²¾åº¦ï¼ˆ**æ•°æ®ç±»å‹**ï¼‰ï¼Œå¯ä»¥è®¡ç®—æ¨¡å‹çš„å¤§å°ã€‚ç„¶è€Œï¼Œä¸ºäº†èŠ‚çœå†…å­˜ï¼Œå¯ä»¥é€šè¿‡ä¸€ç§ç§°ä¸ºé‡åŒ–çš„è¿‡ç¨‹ä½¿ç”¨è¾ƒä½ç²¾åº¦çš„æ•°æ®ç±»å‹æ¥å­˜å‚¨æƒé‡ã€‚
- en: 'We distinguish two main families of weight quantization techniques in the literature:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬åœ¨æ–‡çŒ®ä¸­åŒºåˆ†äº†ä¸¤å¤§ç±»æƒé‡é‡åŒ–æŠ€æœ¯ï¼š
- en: '**Post-Training Quantization** (PTQ) is a straightforward technique where the
    weights of an already trained model are converted to lower precision without necessitating
    any retraining. Although easy to implement, PTQ is associated with potential performance
    degradation.'
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**è®­ç»ƒåé‡åŒ–**ï¼ˆPTQï¼‰æ˜¯ä¸€ç§ç›´æ¥çš„æŠ€æœ¯ï¼Œå…¶ä¸­å·²è®­ç»ƒå¥½çš„æ¨¡å‹çš„æƒé‡è¢«è½¬æ¢ä¸ºè¾ƒä½çš„ç²¾åº¦ï¼Œè€Œä¸éœ€è¦è¿›è¡Œä»»ä½•é‡æ–°è®­ç»ƒã€‚å°½ç®¡å®æ–½èµ·æ¥ç®€å•ï¼Œä½†PTQå¯èƒ½ä¼šå¯¼è‡´æ€§èƒ½ä¸‹é™ã€‚'
- en: '**Quantization-Aware Training** (QAT) incorporates the weight conversion process
    during the pre-training or fine-tuning stage, resulting in enhanced model performance.
    However, QAT is computationally expensive and demands representative training
    data.'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**é‡åŒ–æ„ŸçŸ¥è®­ç»ƒ**ï¼ˆQATï¼‰åœ¨é¢„è®­ç»ƒæˆ–å¾®è°ƒé˜¶æ®µçº³å…¥æƒé‡è½¬æ¢è¿‡ç¨‹ï¼Œä»è€Œæå‡æ¨¡å‹æ€§èƒ½ã€‚ç„¶è€Œï¼ŒQATè®¡ç®—æˆæœ¬é«˜ä¸”éœ€è¦ä»£è¡¨æ€§çš„è®­ç»ƒæ•°æ®ã€‚'
- en: In this article, we focus on PTQ to reduce the precision of our parameters.
    To get a good intuition, we will apply both naÃ¯ve and more sophisticated techniques
    to a toy example using a GPT-2 model.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬æ–‡èšç„¦äºPTQä»¥å‡å°‘å‚æ•°çš„ç²¾åº¦ã€‚ä¸ºäº†è·å¾—è‰¯å¥½çš„ç›´è§‰ï¼Œæˆ‘ä»¬å°†åº”ç”¨ç®€å•å’Œæ›´å¤æ‚çš„æŠ€æœ¯äºä¸€ä¸ªä½¿ç”¨GPT-2æ¨¡å‹çš„ç¤ºä¾‹ã€‚
- en: The entire code is freely available on [Google Colab](https://colab.research.google.com/drive/1DPr4mUQ92Cc-xf4GgAaB6dFcFnWIvqYi?usp=sharing)
    and [GitHub](https://github.com/mlabonne/llm-course/blob/main/Introduction_to_Weight_Quantization.ipynb).
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: å®Œæ•´ä»£ç å¯ä»¥åœ¨[Google Colab](https://colab.research.google.com/drive/1DPr4mUQ92Cc-xf4GgAaB6dFcFnWIvqYi?usp=sharing)å’Œ[GitHub](https://github.com/mlabonne/llm-course/blob/main/Introduction_to_Weight_Quantization.ipynb)ä¸Šè‡ªç”±è·å–ã€‚
- en: ğŸ“š Background on Floating Point Representation
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ğŸ“š æµ®ç‚¹è¡¨ç¤ºçš„èƒŒæ™¯
- en: The choice of data type dictates the quantity of computational resources required,
    affecting the speed and efficiency of the model. In deep learning applications,
    balancing precision and computational performance becomes a vital exercise as
    higher precision often implies greater computational demands.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: æ•°æ®ç±»å‹çš„é€‰æ‹©å†³å®šäº†æ‰€éœ€çš„è®¡ç®—èµ„æºé‡ï¼Œå½±å“æ¨¡å‹çš„é€Ÿåº¦å’Œæ•ˆç‡ã€‚åœ¨æ·±åº¦å­¦ä¹ åº”ç”¨ä¸­ï¼Œå¹³è¡¡ç²¾åº¦å’Œè®¡ç®—æ€§èƒ½æˆä¸ºä¸€ä¸ªè‡³å…³é‡è¦çš„ä»»åŠ¡ï¼Œå› ä¸ºæ›´é«˜çš„ç²¾åº¦é€šå¸¸æ„å‘³ç€æ›´å¤§çš„è®¡ç®—éœ€æ±‚ã€‚
- en: 'Among various data types, floating point numbers are predominantly employed
    in deep learning due to their ability to represent a wide range of values with
    high precision. Typically, a floating point number uses *n* bits to store a numerical
    value. These *n* bits are further partitioned into three distinct components:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å„ç§æ•°æ®ç±»å‹ä¸­ï¼Œæµ®ç‚¹æ•°ç”±äºèƒ½å¤Ÿä»¥é«˜ç²¾åº¦è¡¨ç¤ºå¹¿æ³›çš„å€¼èŒƒå›´ï¼Œå› æ­¤åœ¨æ·±åº¦å­¦ä¹ ä¸­è¢«å¹¿æ³›ä½¿ç”¨ã€‚é€šå¸¸ï¼Œæµ®ç‚¹æ•°ä½¿ç”¨*n*ä½æ¥å­˜å‚¨æ•°å€¼ã€‚è¿™äº›*n*ä½è¿›ä¸€æ­¥è¢«åˆ’åˆ†ä¸ºä¸‰ä¸ªä¸åŒçš„éƒ¨åˆ†ï¼š
- en: '**Sign**: The sign bit indicates the positive or negative nature of the number.
    It uses one bit where 0 indicates a positive number and 1 signals a negative number.'
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**ç¬¦å·**ï¼šç¬¦å·ä½æŒ‡ç¤ºæ•°å­—çš„æ­£è´Ÿæ€§è´¨ã€‚å®ƒä½¿ç”¨ä¸€ä¸ªä½ï¼Œå…¶ä¸­0è¡¨ç¤ºæ­£æ•°ï¼Œ1è¡¨ç¤ºè´Ÿæ•°ã€‚'
- en: '**Exponent**: The exponent is a segment of bits that represents the power to
    which the base (usually 2 in binary representation) is raised. The exponent can
    also be positive or negative, allowing the number to represent very large or very
    small values.'
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**æŒ‡æ•°**ï¼šæŒ‡æ•°æ˜¯ä¸€æ®µä½è¡¨ç¤ºåŸºæ•°ï¼ˆåœ¨äºŒè¿›åˆ¶è¡¨ç¤ºä¸­é€šå¸¸ä¸º2ï¼‰çš„å¹‚ã€‚æŒ‡æ•°å¯ä»¥æ˜¯æ­£æ•°æˆ–è´Ÿæ•°ï¼Œä»è€Œä½¿æ•°å­—èƒ½å¤Ÿè¡¨ç¤ºéå¸¸å¤§æˆ–éå¸¸å°çš„å€¼ã€‚'
- en: '**Significand/Mantissa**: The remaining bits are used to store the significand,
    also referred to as the mantissa. This represents the significant digits of the
    number. The precision of the number heavily depends on the length of the significand.'
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**å°¾æ•°/æœ‰æ•ˆæ•°å­—**ï¼šå‰©ä½™çš„ä½ç”¨äºå­˜å‚¨å°¾æ•°ï¼Œä¹Ÿç§°ä¸ºæœ‰æ•ˆæ•°å­—ã€‚å®ƒè¡¨ç¤ºæ•°å­—çš„æœ‰æ•ˆæ•°å­—ã€‚æ•°å­—çš„ç²¾åº¦ä¸¥é‡ä¾èµ–äºå°¾æ•°çš„é•¿åº¦ã€‚'
- en: 'This design allows floating point numbers to cover a wide range of values with
    varying levels of precision. The formula used for this representation is:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ç§è®¾è®¡å…è®¸æµ®ç‚¹æ•°ä»¥ä¸åŒç²¾åº¦è¦†ç›–å¹¿æ³›çš„å€¼èŒƒå›´ã€‚ç”¨äºè¿™ç§è¡¨ç¤ºçš„å…¬å¼æ˜¯ï¼š
- en: '![](../Images/2e8ab62ec7cb0706b3d6b99f32abc5fa.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2e8ab62ec7cb0706b3d6b99f32abc5fa.png)'
- en: 'To understand this better, letâ€™s delve into some of the most commonly used
    data types in deep learning: float32 (FP32), float16 (FP16), and bfloat16 (BF16):'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†æ›´å¥½åœ°ç†è§£è¿™ä¸€ç‚¹ï¼Œè®©æˆ‘ä»¬æ·±å…¥æ¢è®¨åœ¨æ·±åº¦å­¦ä¹ ä¸­æœ€å¸¸ç”¨çš„æ•°æ®ç±»å‹ï¼šfloat32ï¼ˆFP32ï¼‰ã€float16ï¼ˆFP16ï¼‰å’Œbfloat16ï¼ˆBF16ï¼‰ï¼š
- en: '**FP32** uses 32 bits to represent a number: one bit for the sign, eight for
    the exponent, and the remaining 23 for the significand. While it provides a high
    degree of precision, the downside of FP32 is its high computational and memory
    footprint.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**FP32** ä½¿ç”¨32ä½è¡¨ç¤ºä¸€ä¸ªæ•°å­—ï¼šä¸€ä¸ªä½ç”¨äºç¬¦å·ï¼Œå…«ä¸ªä½ç”¨äºæŒ‡æ•°ï¼Œå…¶ä½™23ä¸ªä½ç”¨äºå°¾æ•°ã€‚è™½ç„¶å®ƒæä¾›äº†è¾ƒé«˜çš„ç²¾åº¦ï¼Œä½†FP32çš„ç¼ºç‚¹æ˜¯å…¶è®¡ç®—å’Œå†…å­˜å¼€é”€è¾ƒå¤§ã€‚'
- en: '**FP16** uses 16 bits to store a number: one is used for the sign, five for
    the exponent, and ten for the significand. Although this makes it more memory-efficient
    and accelerates computations, the reduced range and precision can introduce numerical
    instability, potentially impacting model accuracy.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**FP16** ä½¿ç”¨16ä½å­˜å‚¨ä¸€ä¸ªæ•°å­—ï¼šä¸€ä¸ªç”¨äºç¬¦å·ï¼Œäº”ä¸ªä½ç”¨äºæŒ‡æ•°ï¼Œåä¸ªä½ç”¨äºå°¾æ•°ã€‚è™½ç„¶è¿™ä½¿å…¶åœ¨å†…å­˜ä¸Šæ›´é«˜æ•ˆå¹¶åŠ é€Ÿè®¡ç®—ï¼Œä½†å‡å°‘çš„èŒƒå›´å’Œç²¾åº¦å¯èƒ½ä¼šå¼•å…¥æ•°å€¼ä¸ç¨³å®šæ€§ï¼Œå¯èƒ½å½±å“æ¨¡å‹çš„å‡†ç¡®æ€§ã€‚'
- en: '**BF16** is also a 16-bit format but with one bit for the sign, *eight* for
    the exponent, and *seven* for the significand. BF16 expands the representable
    range compared to FP16, thus decreasing underflow and overflow risks. Despite
    a reduction in precision due to fewer significand bits, BF16 typically does not
    significantly impact model performance and is a useful compromise for deep learning
    tasks.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**BF16**ä¹Ÿæ˜¯ä¸€ç§16ä½æ ¼å¼ï¼Œä½†æœ‰ä¸€ä¸ªç¬¦å·ä½ï¼Œ*å…«*ä¸ªä½ç”¨äºæŒ‡æ•°ï¼Œ*ä¸ƒ*ä¸ªä½ç”¨äºå°¾æ•°ã€‚BF16æ‰©å±•äº†è¡¨ç¤ºèŒƒå›´ï¼Œç›¸è¾ƒäºFP16ï¼Œå‡å°‘äº†ä¸‹æº¢å’Œæº¢å‡ºçš„é£é™©ã€‚å°½ç®¡ç”±äºå°¾æ•°ä½æ•°å‡å°‘è€Œç²¾åº¦é™ä½ï¼Œä½†BF16é€šå¸¸ä¸ä¼šæ˜¾è‘—å½±å“æ¨¡å‹æ€§èƒ½ï¼Œæ˜¯æ·±åº¦å­¦ä¹ ä»»åŠ¡çš„ä¸€ä¸ªæœ‰ç”¨æŠ˜ä¸­æ–¹æ¡ˆã€‚'
- en: '![](../Images/7f97ce565ee75a23253ee19eeff714bd.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7f97ce565ee75a23253ee19eeff714bd.png)'
- en: Image by author
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: ä½œè€…æä¾›çš„å›¾ç‰‡
- en: In ML jargon, FP32 is often termed â€œfull precisionâ€ (4 bytes), while BF16 and
    FP16 are â€œhalf-precisionâ€ (2 bytes). But could we do even better and store weights
    using a single byte? The answer is the INT8 data type, which consists of an 8-bit
    representation capable of storing 2â¸ = 256 different values. In the next section,
    weâ€™ll see how to convert FP32 weights into an INT8 format.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœºå™¨å­¦ä¹ æœ¯è¯­ä¸­ï¼ŒFP32é€šå¸¸è¢«ç§°ä¸ºâ€œå…¨ç²¾åº¦â€ï¼ˆ4å­—èŠ‚ï¼‰ï¼Œè€ŒBF16å’ŒFP16åˆ™è¢«ç§°ä¸ºâ€œåŠç²¾åº¦â€ï¼ˆ2å­—èŠ‚ï¼‰ã€‚ä½†æˆ‘ä»¬æ˜¯å¦å¯ä»¥æ›´è¿›ä¸€æ­¥ï¼Œç”¨ä¸€ä¸ªå­—èŠ‚æ¥å­˜å‚¨æƒé‡ï¼Ÿç­”æ¡ˆæ˜¯INT8æ•°æ®ç±»å‹ï¼Œå®ƒç”±ä¸€ä¸ª8ä½è¡¨ç¤ºç»„æˆï¼Œèƒ½å¤Ÿå­˜å‚¨2â¸
    = 256ç§ä¸åŒçš„å€¼ã€‚åœ¨ä¸‹ä¸€èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†çœ‹åˆ°å¦‚ä½•å°†FP32æƒé‡è½¬æ¢ä¸ºINT8æ ¼å¼ã€‚
- en: ğŸ”° NaÃ¯ve 8-bit Quantization
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ğŸ”° åˆæ­¥çš„8ä½é‡åŒ–
- en: 'In this section, we will implement two quantization techniques: a symmetric
    one with **absolute maximum (absmax) quantization** and an asymmetric one with
    **zero-point quantization**. In both cases, the goal is to map an FP32 tensor
    **X** (original weights) to an INT8 tensor **X_quant** (quantized weights).'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ä¸€éƒ¨åˆ†ï¼Œæˆ‘ä»¬å°†å®ç°ä¸¤ç§é‡åŒ–æŠ€æœ¯ï¼šä¸€ç§æ˜¯å…·æœ‰**ç»å¯¹æœ€å¤§å€¼ï¼ˆabsmaxï¼‰é‡åŒ–**çš„å¯¹ç§°é‡åŒ–ï¼Œå¦ä¸€ç§æ˜¯å…·æœ‰**é›¶ç‚¹é‡åŒ–**çš„éå¯¹ç§°é‡åŒ–ã€‚åœ¨è¿™ä¸¤ç§æƒ…å†µä¸‹ï¼Œç›®æ ‡æ˜¯å°†FP32å¼ é‡**X**ï¼ˆåŸå§‹æƒé‡ï¼‰æ˜ å°„åˆ°INT8å¼ é‡**X_quant**ï¼ˆé‡åŒ–æƒé‡ï¼‰ã€‚
- en: With **absmax quantization**, the original number is divided by the absolute
    maximum value of the tensor and multiplied by a scaling factor (127) to map inputs
    into the range [-127, 127]. To retrieve the original FP16 values, the INT8 number
    is divided by the quantization factor, acknowledging some loss of precision due
    to rounding.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨**absmaxé‡åŒ–**ï¼ŒåŸå§‹æ•°å€¼é™¤ä»¥å¼ é‡çš„ç»å¯¹æœ€å¤§å€¼ï¼Œå¹¶ä¹˜ä»¥ç¼©æ”¾å› å­ï¼ˆ127ï¼‰ï¼Œå°†è¾“å…¥æ˜ å°„åˆ°èŒƒå›´[-127, 127]ã€‚è¦æ£€ç´¢åŸå§‹FP16å€¼ï¼ŒINT8æ•°å€¼é™¤ä»¥é‡åŒ–å› å­ï¼Œæ‰¿è®¤ç”±äºå››èˆäº”å…¥å¯¼è‡´çš„ä¸€äº›ç²¾åº¦æŸå¤±ã€‚
- en: '![](../Images/7dfd58e4be8f1412c262188ca9eecb4f.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7dfd58e4be8f1412c262188ca9eecb4f.png)'
- en: 'For instance, letâ€™s say we have an absolution maximum value of 3.2\. A weight
    of 0.1 would be quantized to *round(0.1 Ã— 127/3.2) = 4*. If we want to dequantize
    it, we would get *4 Ã— 3.2/127 = 0.1008*, which implies an error of 0.008\. Hereâ€™s
    the corresponding Python implementation:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: ä¾‹å¦‚ï¼Œå‡è®¾æˆ‘ä»¬æœ‰ä¸€ä¸ªç»å¯¹æœ€å¤§å€¼ä¸º3.2çš„å€¼ã€‚ä¸€ä¸ª0.1çš„æƒé‡å°†è¢«é‡åŒ–ä¸º*round(0.1 Ã— 127/3.2) = 4*ã€‚å¦‚æœæˆ‘ä»¬æƒ³å°†å…¶åé‡åŒ–ï¼Œå°†å¾—åˆ°*4
    Ã— 3.2/127 = 0.1008*ï¼Œè¿™æ„å‘³ç€ä¸€ä¸ª0.008çš„è¯¯å·®ã€‚ä»¥ä¸‹æ˜¯å¯¹åº”çš„Pythonå®ç°ï¼š
- en: '[PRE0]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'With **zero-point quantization**, we can consider asymmetric input distributions,
    which is useful when you consider the output of a ReLU function (only positive
    values), for example. The input values are first scaled by the total range of
    values (255) divided by the difference between the maximum and minimum values.
    This distribution is then shifted by the zero-point to map it into the range [-128,
    127] (notice the extra value compared to absmax). First, we calculate the scale
    factor and the zero-point value:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨**é›¶ç‚¹é‡åŒ–**ï¼Œæˆ‘ä»¬å¯ä»¥è€ƒè™‘éå¯¹ç§°è¾“å…¥åˆ†å¸ƒï¼Œè¿™åœ¨è€ƒè™‘ReLUå‡½æ•°çš„è¾“å‡ºï¼ˆä»…æ­£å€¼ï¼‰æ—¶ç‰¹åˆ«æœ‰ç”¨ã€‚è¾“å…¥å€¼é¦–å…ˆæŒ‰å€¼çš„æ€»èŒƒå›´ï¼ˆ255ï¼‰é™¤ä»¥æœ€å¤§å€¼å’Œæœ€å°å€¼ä¹‹é—´çš„å·®å¼‚è¿›è¡Œç¼©æ”¾ã€‚ç„¶åé€šè¿‡é›¶ç‚¹åç§»å°†è¿™ä¸ªåˆ†å¸ƒæ˜ å°„åˆ°èŒƒå›´[-128,
    127]ï¼ˆæ³¨æ„ä¸absmaxç›¸æ¯”çš„é¢å¤–å€¼ï¼‰ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬è®¡ç®—ç¼©æ”¾å› å­å’Œé›¶ç‚¹å€¼ï¼š
- en: '![](../Images/8553578f6e6bc10ce524fcf7ec3c4088.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8553578f6e6bc10ce524fcf7ec3c4088.png)'
- en: 'Then, we can use these variables to quantize or dequantize our weights:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨è¿™äº›å˜é‡æ¥é‡åŒ–æˆ–åé‡åŒ–æˆ‘ä»¬çš„æƒé‡ï¼š
- en: '![](../Images/bd1fff48751ed91f96d614a7b1fdced9.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bd1fff48751ed91f96d614a7b1fdced9.png)'
- en: 'Letâ€™s take an example: we have a maximum value of 3.2 and a minimum value of
    -3.0\. We can calculate the scale is *255/(3.2 + 3.0) = 41.13* and the zero-point
    *-round(41.13 Ã— -3.0) - 128 = 123 -128 = -5*, so our previous weight of 0.1 would
    be quantized to *round(41.13 Ã— 0.1 -5) = -1*. This is very different from the
    previous value obtained using absmax (4 vs. -1).'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: ä¾‹å¦‚ï¼šæˆ‘ä»¬æœ‰ä¸€ä¸ªæœ€å¤§å€¼ä¸º3.2å’Œä¸€ä¸ªæœ€å°å€¼ä¸º-3.0ã€‚æˆ‘ä»¬å¯ä»¥è®¡ç®—å‡ºç¼©æ”¾å› å­ä¸º*255/(3.2 + 3.0) = 41.13*ï¼Œé›¶ç‚¹ä¸º*-round(41.13
    Ã— -3.0) - 128 = 123 -128 = -5*ï¼Œå› æ­¤æˆ‘ä»¬ä¹‹å‰çš„æƒé‡0.1å°†è¢«é‡åŒ–ä¸º*round(41.13 Ã— 0.1 -5) = -1*ã€‚è¿™ä¸ä¹‹å‰ä½¿ç”¨absmaxè·å¾—çš„å€¼ï¼ˆ4ä¸-1ï¼‰å·®å¼‚å¾ˆå¤§ã€‚
- en: '![](../Images/773e8d8dbc9ac7c43c8d7f0fbfbe5fda.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/773e8d8dbc9ac7c43c8d7f0fbfbe5fda.png)'
- en: Image by author
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: ä½œè€…æä¾›çš„å›¾ç‰‡
- en: 'The Python implementation is quite straightforward:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: Python å®ç°éå¸¸ç®€å•ï¼š
- en: '[PRE1]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Instead of relying on complete toy examples, we can use these two functions
    on a real model thanks to the `transformers`library.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥åˆ©ç”¨ `transformers` åº“åœ¨çœŸå®æ¨¡å‹ä¸Šä½¿ç”¨è¿™ä¸¤ä¸ªå‡½æ•°ï¼Œè€Œä¸æ˜¯ä¾èµ–å®Œæ•´çš„ç©å…·ç¤ºä¾‹ã€‚
- en: We start by loading the model and tokenizer for GPT-2\. This is a very small
    model we probably donâ€™t want to quantize, but it will be good enough for this
    tutorial. First, we want to observe the modelâ€™s size so we can compare it later
    and evaluate the **memory savings** due to 8-bit quantization.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬é¦–å…ˆåŠ è½½ GPT-2 çš„æ¨¡å‹å’Œåˆ†è¯å™¨ã€‚è¿™æ˜¯ä¸€ä¸ªéå¸¸å°çš„æ¨¡å‹ï¼Œæˆ‘ä»¬å¯èƒ½ä¸æƒ³å¯¹å…¶è¿›è¡Œé‡åŒ–ï¼Œä½†å®ƒå¯¹äºæœ¬æ•™ç¨‹æ¥è¯´è¶³å¤Ÿäº†ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬æƒ³è§‚å¯Ÿæ¨¡å‹çš„å¤§å°ï¼Œä»¥ä¾¿ç¨åè¿›è¡Œæ¯”è¾ƒï¼Œå¹¶è¯„ä¼°ç”±äº
    8 ä½é‡åŒ–è€ŒèŠ‚çœçš„**å†…å­˜**ã€‚
- en: '[PRE2]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[PRE3]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[PRE4]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The size of the GPT-2 model is approximately 487MB in FP32\. The next step consists
    of quantizing the weights using zero-point and absmax quantization. In the following
    example, we apply these techniques to the first attention layer of GPT-2 to see
    the results.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-2 æ¨¡å‹åœ¨ FP32 ä¸­çš„å¤§å°çº¦ä¸º 487MBã€‚ä¸‹ä¸€æ­¥æ˜¯ä½¿ç”¨é›¶ç‚¹å’Œ absmax é‡åŒ–æƒé‡ã€‚åœ¨ä»¥ä¸‹ç¤ºä¾‹ä¸­ï¼Œæˆ‘ä»¬å°†è¿™äº›æŠ€æœ¯åº”ç”¨äº GPT-2 çš„ç¬¬ä¸€ä¸ªæ³¨æ„åŠ›å±‚ï¼Œä»¥æŸ¥çœ‹ç»“æœã€‚
- en: '[PRE5]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '[PRE6]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The difference between the original (FP32) and quantized values (INT8) is clear,
    but the difference between absmax and zero-point weights is more subtle. In this
    case, the inputs look shifted by a value of -1\. This suggests that the weight
    distribution in this layer is quite symmetric.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: åŸå§‹ï¼ˆFP32ï¼‰å€¼å’Œé‡åŒ–å€¼ï¼ˆINT8ï¼‰ä¹‹é—´çš„å·®å¼‚å¾ˆæ˜æ˜¾ï¼Œä½† absmax å’Œé›¶ç‚¹æƒé‡ä¹‹é—´çš„å·®å¼‚åˆ™æ›´å¾®å¦™ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œè¾“å…¥å€¼çœ‹èµ·æ¥åç§»äº† -1ã€‚è¿™è¡¨æ˜è¿™ä¸€å±‚çš„æƒé‡åˆ†å¸ƒç›¸å½“å¯¹ç§°ã€‚
- en: 'We can compare these techniques by quantizing every layer in GPT-2 (linear
    layers, attention layers, etc.) and create two new models: `model_abs` and `model_zp`.
    To be precise, we will actually replace the original weights with ***de***-quantized
    ones. This has two benefits: it allows us to 1/ compare the distribution of our
    weights (same scale) and 2/ actually run the models.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥é€šè¿‡å¯¹ GPT-2 ä¸­çš„æ¯ä¸€å±‚ï¼ˆçº¿æ€§å±‚ã€æ³¨æ„åŠ›å±‚ç­‰ï¼‰è¿›è¡Œé‡åŒ–ï¼Œå¹¶åˆ›å»ºä¸¤ä¸ªæ–°æ¨¡å‹ï¼š`model_abs` å’Œ `model_zp`ï¼Œæ¥æ¯”è¾ƒè¿™äº›æŠ€æœ¯ã€‚å‡†ç¡®åœ°è¯´ï¼Œæˆ‘ä»¬å°†å®é™…ç”¨***å»***é‡åŒ–çš„æƒé‡æ›¿æ¢åŸå§‹æƒé‡ã€‚è¿™æœ‰ä¸¤ä¸ªå¥½å¤„ï¼šå®ƒä½¿æˆ‘ä»¬å¯ä»¥
    1/ æ¯”è¾ƒæƒé‡çš„åˆ†å¸ƒï¼ˆç›¸åŒçš„å°ºåº¦ï¼‰ï¼Œä»¥åŠ 2/ å®é™…è¿è¡Œè¿™äº›æ¨¡å‹ã€‚
- en: Indeed, PyTorch doesnâ€™t allow INT8 matrix multiplication by default. In a real
    scenario, we would dequantize them to run the model (in FP16 for example) but
    store them as INT8\. In the next section, we will use the `[bitsandbytes](https://github.com/TimDettmers/bitsandbytes)`
    library to solve this issue.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: ç¡®å®ï¼ŒPyTorch é»˜è®¤ä¸å…è®¸ INT8 çŸ©é˜µä¹˜æ³•ã€‚åœ¨å®é™…åœºæ™¯ä¸­ï¼Œæˆ‘ä»¬ä¼šå°†å…¶å»é‡åŒ–ä»¥è¿è¡Œæ¨¡å‹ï¼ˆä¾‹å¦‚ FP16ï¼‰ï¼Œä½†ä»¥ INT8 å­˜å‚¨ã€‚åœ¨ä¸‹ä¸€éƒ¨åˆ†ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨
    `[bitsandbytes](https://github.com/TimDettmers/bitsandbytes)` åº“æ¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚
- en: '[PRE7]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Now that our models have been quantized, we want to check the impact of this
    process. Intuitively, we want to make sure that the quantized weights are **close
    to the original ones**. A visual way to check it is to plot the distribution of
    the dequantized and original weights. If the quantization is lossy, it would drastically
    change the weight distribution.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬çš„æ¨¡å‹å·²ç»è¢«é‡åŒ–ï¼Œæˆ‘ä»¬æƒ³æ£€æŸ¥è¿™ä¸€è¿‡ç¨‹çš„å½±å“ã€‚ç›´è§‚ä¸Šï¼Œæˆ‘ä»¬æƒ³ç¡®ä¿é‡åŒ–çš„æƒé‡**æ¥è¿‘åŸå§‹æƒé‡**ã€‚ä¸€ç§ç›´è§‚çš„æ–¹æ³•æ˜¯ç»˜åˆ¶å»é‡åŒ–å’ŒåŸå§‹æƒé‡çš„åˆ†å¸ƒã€‚å¦‚æœé‡åŒ–æœ‰æŸï¼Œå®ƒå°†æå¤§åœ°æ”¹å˜æƒé‡åˆ†å¸ƒã€‚
- en: The following figure shows this comparison, where the blue histogram represents
    the original (FP32) weights, and the red one represents the dequantized (from
    INT8) weights. Note that we only display this plot between -2 and 2 because of
    outliers with very high absolute values (more on that later).
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸‹å›¾å±•ç¤ºäº†è¿™ç§æ¯”è¾ƒï¼Œå…¶ä¸­è“è‰²ç›´æ–¹å›¾è¡¨ç¤ºåŸå§‹ï¼ˆFP32ï¼‰æƒé‡ï¼Œçº¢è‰²ç›´æ–¹å›¾è¡¨ç¤ºå»é‡åŒ–åçš„ï¼ˆæ¥è‡ª INT8ï¼‰æƒé‡ã€‚æ³¨æ„ï¼Œæˆ‘ä»¬ä»…åœ¨ -2 å’Œ 2 ä¹‹é—´æ˜¾ç¤ºæ­¤å›¾ï¼Œå› ä¸ºæœ‰ä¸€äº›ç»å¯¹å€¼éå¸¸é«˜çš„ç¦»ç¾¤å€¼ï¼ˆç¨åä¼šè¯¦ç»†è®¨è®ºï¼‰ã€‚
- en: '![](../Images/9e93b5636f8e65de4bb386d68ad67b61.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9e93b5636f8e65de4bb386d68ad67b61.png)'
- en: Both plots are quite similar, with a surprising spike around 0\. This spike
    shows that our quantization is quite lossy since reversing the process doesnâ€™t
    output the original values. This is particularly true for the absmax model, which
    displays both a lower valley and a higher spike around 0.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸¤ä¸ªå›¾è¡¨éå¸¸ç›¸ä¼¼ï¼Œéƒ½åœ¨ 0 é™„è¿‘æœ‰ä¸€ä¸ªæƒŠäººçš„å³°å€¼ã€‚è¿™ä¸ªå³°å€¼è¡¨æ˜æˆ‘ä»¬çš„é‡åŒ–ç›¸å½“æœ‰æŸï¼Œå› ä¸ºåè½¬è¿‡ç¨‹å¹¶æ²¡æœ‰è¾“å‡ºåŸå§‹å€¼ã€‚è¿™åœ¨ absmax æ¨¡å‹ä¸­å°¤ä¸ºæ˜æ˜¾ï¼Œè¯¥æ¨¡å‹åœ¨
    0 é™„è¿‘æ˜¾ç¤ºäº†ä¸€ä¸ªæ›´ä½çš„è°·å€¼å’Œä¸€ä¸ªæ›´é«˜çš„å³°å€¼ã€‚
- en: Letâ€™s compare the performance of the original and quantized models. For this
    purpose, we define a `generate_text()` function to generate 50 tokens with [top-k
    sampling](https://mlabonne.github.io/blog/posts/2023-06-07-Decoding_strategies.html).
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬æ¯”è¾ƒåŸå§‹æ¨¡å‹å’Œé‡åŒ–æ¨¡å‹çš„æ€§èƒ½ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å®šä¹‰äº†ä¸€ä¸ª `generate_text()` å‡½æ•°æ¥ç”Ÿæˆ 50 ä¸ªæ ‡è®°ï¼Œå¹¶ä½¿ç”¨ [top-k é‡‡æ ·](https://mlabonne.github.io/blog/posts/2023-06-07-Decoding_strategies.html)ã€‚
- en: '[PRE8]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '[PRE9]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Instead of trying to see if one output makes more sense than the others, we
    can quantify it by calculating the **perplexity** of each output. This is a common
    metric used to evaluate language models, which measures the uncertainty of a model
    in predicting the next token in a sequence. In this comparison, we make the common
    assumption that the lower the score, the better the model is. In practice, a sentence
    with a high perplexity could also be correct.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥é€šè¿‡è®¡ç®—æ¯ä¸ªè¾“å‡ºçš„**å›°æƒ‘åº¦**æ¥é‡åŒ–å®ƒï¼Œè€Œä¸æ˜¯å°è¯•åˆ¤æ–­ä¸€ä¸ªè¾“å‡ºæ˜¯å¦æ¯”å…¶ä»–è¾“å‡ºæ›´æœ‰æ„ä¹‰ã€‚è¿™æ˜¯ä¸€ä¸ªå¸¸ç”¨çš„è¯„ä¼°è¯­è¨€æ¨¡å‹çš„æŒ‡æ ‡ï¼Œç”¨äºæµ‹é‡æ¨¡å‹åœ¨é¢„æµ‹åºåˆ—ä¸­ä¸‹ä¸€ä¸ªæ ‡è®°æ—¶çš„ä¸ç¡®å®šæ€§ã€‚åœ¨è¿™ä¸ªæ¯”è¾ƒä¸­ï¼Œæˆ‘ä»¬åšäº†ä¸€ä¸ªå¸¸è§çš„å‡è®¾ï¼Œå³åˆ†æ•°è¶Šä½ï¼Œæ¨¡å‹è¶Šå¥½ã€‚å®é™…ä¸Šï¼Œé«˜å›°æƒ‘åº¦çš„å¥å­ä¹Ÿå¯èƒ½æ˜¯æ­£ç¡®çš„ã€‚
- en: We implement it using a minimal function since it doesnâ€™t need to consider details
    like the length of the context window since our sentences are short.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ä½¿ç”¨ä¸€ä¸ªæœ€å°å‡½æ•°æ¥å®ç°å®ƒï¼Œå› ä¸ºä¸éœ€è¦è€ƒè™‘ä¸Šä¸‹æ–‡çª—å£çš„é•¿åº¦ï¼Œå› ä¸ºæˆ‘ä»¬çš„å¥å­è¾ƒçŸ­ã€‚
- en: '[PRE10]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '[PRE11]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: We see that the perplexity of the original model is **slightly lower** than
    the two others. A single experiment is not very reliable, but we could repeat
    this process multiple times to see the difference between each model. In theory,
    zero-point quantization should be slightly better than absmax, but is also more
    costly to compute.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å‘ç°åŸå§‹æ¨¡å‹çš„å›°æƒ‘åº¦**ç•¥ä½**äºå¦å¤–ä¸¤ä¸ªæ¨¡å‹ã€‚ä¸€æ¬¡å®éªŒå¹¶ä¸ååˆ†å¯é ï¼Œä½†æˆ‘ä»¬å¯ä»¥å¤šæ¬¡é‡å¤è¿™ä¸ªè¿‡ç¨‹ï¼Œä»¥æŸ¥çœ‹æ¯ä¸ªæ¨¡å‹ä¹‹é—´çš„å·®å¼‚ã€‚ä»ç†è®ºä¸Šè®²ï¼Œé›¶ç‚¹é‡åŒ–åº”ç¨å¾®ä¼˜äº
    absmaxï¼Œä½†è®¡ç®—æˆæœ¬ä¹Ÿæ›´é«˜ã€‚
- en: 'In this example, we applied quantization techniques to entire layers (per-tensor
    basis). However, we could apply it at different granularity levels: from the entire
    model to individual values. Quantizing the entire model in one pass would seriously
    degrade the performance, while quantizing individual values would create a big
    overhead. In practice, we often prefer the **vector-wise quantization**, which
    considers the variability of values in rows and columns inside of the same tensor.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œæˆ‘ä»¬å°†é‡åŒ–æŠ€æœ¯åº”ç”¨äºæ•´ä¸ªå±‚ï¼ˆæ¯å¼ é‡åŸºç¡€ï¼‰ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬å¯ä»¥åœ¨ä¸åŒçš„ç²’åº¦æ°´å¹³åº”ç”¨å®ƒï¼šä»æ•´ä¸ªæ¨¡å‹åˆ°å•ä¸ªå€¼ã€‚ä¸€æ¬¡æ€§é‡åŒ–æ•´ä¸ªæ¨¡å‹ä¼šä¸¥é‡é™ä½æ€§èƒ½ï¼Œè€Œé‡åŒ–å•ä¸ªå€¼ä¼šäº§ç”Ÿå¾ˆå¤§çš„å¼€é”€ã€‚åœ¨å®è·µä¸­ï¼Œæˆ‘ä»¬é€šå¸¸æ›´å–œæ¬¢**é€å‘é‡é‡åŒ–**ï¼Œå®ƒè€ƒè™‘äº†åŒä¸€å¼ é‡ä¸­è¡Œå’Œåˆ—çš„å€¼çš„å˜å¼‚æ€§ã€‚
- en: However, even vector-wise quantization doesnâ€™t solve the problem of outlier
    features. Outlier features are extreme values (negative or positive) that appear
    in all transformer layers when the model reach a certain scale (>6.7B parameters).
    This is an issue since a single outlier can reduce the precision for all other
    values. But discarding these outlier features is not an option since it would
    **greatly degrade** the modelâ€™s performance.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶è€Œï¼Œå³ä½¿æ˜¯é€å‘é‡é‡åŒ–ä¹Ÿä¸èƒ½è§£å†³ç¦»ç¾¤ç‰¹å¾çš„é—®é¢˜ã€‚ç¦»ç¾¤ç‰¹å¾æ˜¯å‡ºç°åœ¨æ‰€æœ‰å˜æ¢å™¨å±‚ä¸­çš„æç«¯å€¼ï¼ˆè´Ÿå€¼æˆ–æ­£å€¼ï¼‰ï¼Œå½“æ¨¡å‹è¾¾åˆ°æŸä¸ªè§„æ¨¡ï¼ˆ>6.7B å‚æ•°ï¼‰æ—¶ã€‚è¿™æ˜¯ä¸€ä¸ªé—®é¢˜ï¼Œå› ä¸ºä¸€ä¸ªç¦»ç¾¤å€¼å¯ä»¥é™ä½æ‰€æœ‰å…¶ä»–å€¼çš„ç²¾åº¦ã€‚ä½†ä¸¢å¼ƒè¿™äº›ç¦»ç¾¤ç‰¹å¾å¹¶ä¸æ˜¯ä¸€ä¸ªé€‰é¡¹ï¼Œå› ä¸ºè¿™ä¼š**å¤§å¤§é™ä½**æ¨¡å‹çš„æ€§èƒ½ã€‚
- en: ğŸ”¢ 8-bit Quantization with LLM.int8()
  id: totrans-70
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ğŸ”¢ 8 ä½é‡åŒ–ä¸ LLM.int8()
- en: Introduced by [Dettmers et al. (2022)](https://arxiv.org/abs/2208.07339), LLM.int8()
    is a solution to the outlier problem. It relies on a vector-wise (absmax) quantization
    scheme and introduces mixed-precision quantization. This means that outlier features
    are processed in a FP16 format to retain their precision, while the other values
    are processed in an INT8 format. As outliers represent about 0.1% of values, this
    effectively reduces the memory footprint of the LLM by almost 2x.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±[Dettmers et al. (2022)](https://arxiv.org/abs/2208.07339)å¼•å…¥çš„LLM.int8() æ˜¯è§£å†³ç¦»ç¾¤å€¼é—®é¢˜çš„ä¸€ä¸ªæ–¹æ¡ˆã€‚å®ƒä¾èµ–äºé€å‘é‡ï¼ˆabsmaxï¼‰é‡åŒ–æ–¹æ¡ˆï¼Œå¹¶å¼•å…¥äº†æ··åˆç²¾åº¦é‡åŒ–ã€‚è¿™æ„å‘³ç€ç¦»ç¾¤ç‰¹å¾ä»¥
    FP16 æ ¼å¼å¤„ç†ä»¥ä¿æŒå…¶ç²¾åº¦ï¼Œè€Œå…¶ä»–å€¼åˆ™ä»¥ INT8 æ ¼å¼å¤„ç†ã€‚ç”±äºç¦»ç¾¤å€¼çº¦å  0.1% çš„å€¼ï¼Œè¿™æœ‰æ•ˆåœ°å°† LLM çš„å†…å­˜å ç”¨å‡å°‘äº†è¿‘ 2 å€ã€‚
- en: '![](../Images/f7ce7de0c94eb22aaf2c926341e6f2ff.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f7ce7de0c94eb22aaf2c926341e6f2ff.png)'
- en: Image by author
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: ä½œè€…æä¾›çš„å›¾ç‰‡
- en: 'LLM.int8() works by conducting matrix multiplication computation in three key
    steps:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: LLM.int8() é€šè¿‡ä¸‰ä¸ªå…³é”®æ­¥éª¤è¿›è¡ŒçŸ©é˜µä¹˜æ³•è®¡ç®—ï¼š
- en: Extract columns from the input hidden states **X** containing outlier features
    using a custom threshold.
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ä»è¾“å…¥éšè—çŠ¶æ€**X**ä¸­æå–åŒ…å«ç¦»ç¾¤ç‰¹å¾çš„åˆ—ï¼Œä½¿ç”¨è‡ªå®šä¹‰é˜ˆå€¼ã€‚
- en: Perform the matrix multiplication of the outliers using FP16 and the non-outliers
    using INT8 with vector-wise quantization (row-wise for the hidden state **X**
    and column-wise for the weight matrix **W**).
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ FP16 å¯¹ç¦»ç¾¤å€¼è¿›è¡ŒçŸ©é˜µä¹˜æ³•ï¼Œå¹¶ä½¿ç”¨ INT8 è¿›è¡Œé€å‘é‡é‡åŒ–ï¼ˆå¯¹éšè—çŠ¶æ€**X**è¿›è¡Œé€è¡Œé‡åŒ–ï¼Œå¯¹æƒé‡çŸ©é˜µ**W**è¿›è¡Œé€åˆ—é‡åŒ–ï¼‰ã€‚
- en: Dequantize the non-outlier results (INT8 to FP16) and add them to the outlier
    results to get the full result in FP16.
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å¯¹éç¦»ç¾¤ç»“æœè¿›è¡Œåé‡åŒ–ï¼ˆä» INT8 åˆ° FP16ï¼‰ï¼Œå¹¶å°†å…¶ä¸ç¦»ç¾¤ç»“æœç›¸åŠ ï¼Œä»¥è·å¾—å®Œæ•´çš„ FP16 ç»“æœã€‚
- en: '![](../Images/3b4bb0a164b5ebebc1dcaca4b96f34d7.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3b4bb0a164b5ebebc1dcaca4b96f34d7.png)'
- en: Image by author
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: ä½œè€…æä¾›çš„å›¾ç‰‡
- en: This approach is necessary because 8-bit precision is limited and can lead to
    substantial errors when quantizing a vector with large values. These errors also
    tend to amplify as they propagate through multiple layers.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ç§æ–¹æ³•æ˜¯å¿…è¦çš„ï¼Œå› ä¸º 8 ä½ç²¾åº¦æœ‰é™ï¼Œå½“é‡åŒ–å…·æœ‰å¤§å€¼çš„å‘é‡æ—¶å¯èƒ½å¯¼è‡´æ˜¾è‘—çš„è¯¯å·®ã€‚è¿™äº›è¯¯å·®ä¹Ÿä¼šéšç€å®ƒä»¬åœ¨å¤šä¸ªå±‚ä¹‹é—´ä¼ æ’­è€Œæ”¾å¤§ã€‚
- en: We can easily use this technique thanks to the integration of the `bitsandbytes`
    library into the Hugging Face ecosystem. We just need to specify `load_in_8bit=True`
    when loading the model (it also requires a GPU).
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±äº `bitsandbytes` åº“å·²é›†æˆåˆ° Hugging Face ç”Ÿæ€ç³»ç»Ÿä¸­ï¼Œæˆ‘ä»¬å¯ä»¥è½»æ¾ä½¿ç”¨è¿™é¡¹æŠ€æœ¯ã€‚æˆ‘ä»¬åªéœ€åœ¨åŠ è½½æ¨¡å‹æ—¶æŒ‡å®š `load_in_8bit=True`ï¼ˆå®ƒè¿˜éœ€è¦ä¸€ä¸ª
    GPUï¼‰ã€‚
- en: '[PRE12]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '[PRE13]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'With this extra line of code, the model is now almost three times smaller (168MB
    vs. 487MB). We can even compare the distribution of the original and quantized
    weights as we did earlier:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: é€šè¿‡è¿™é¢å¤–çš„ä¸€è¡Œä»£ç ï¼Œæ¨¡å‹ç°åœ¨å‡ ä¹ç¼©å°äº†ä¸‰å€ï¼ˆ168MB vs. 487MBï¼‰ã€‚æˆ‘ä»¬ç”šè‡³å¯ä»¥åƒä¹‹å‰é‚£æ ·æ¯”è¾ƒåŸå§‹æƒé‡å’Œé‡åŒ–æƒé‡çš„åˆ†å¸ƒï¼š
- en: '![](../Images/b46d4868b6ce8ce99e4746e100fc1525.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b46d4868b6ce8ce99e4746e100fc1525.png)'
- en: In this case, we see spikes around -2, -1, 0, 1, 2, etc. These values correspond
    to the parameters stored in the INT8 format (non-outliers). You can verify it
    by printing the modelâ€™s weights using `model_int8.parameters()`.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬çœ‹åˆ°åœ¨ -2ï¼Œ-1ï¼Œ0ï¼Œ1ï¼Œ2 ç­‰å€¼å‘¨å›´æœ‰å°–å³°ã€‚è¿™äº›å€¼å¯¹åº”äºä»¥ INT8 æ ¼å¼å­˜å‚¨çš„å‚æ•°ï¼ˆéå¼‚å¸¸å€¼ï¼‰ã€‚ä½ å¯ä»¥é€šè¿‡ä½¿ç”¨ `model_int8.parameters()`
    æ‰“å°æ¨¡å‹çš„æƒé‡æ¥éªŒè¯è¿™ä¸€ç‚¹ã€‚
- en: We can also generate text with this quantized model and compare it to the original
    model.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬è¿˜å¯ä»¥ä½¿ç”¨è¿™ä¸ªé‡åŒ–æ¨¡å‹ç”Ÿæˆæ–‡æœ¬ï¼Œå¹¶ä¸åŸå§‹æ¨¡å‹è¿›è¡Œæ¯”è¾ƒã€‚
- en: '[PRE14]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '[PRE15]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Once again, it is difficult to judge what is the best output, but we can rely
    on the perplexity metric to give us an (approximate) answer.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: å†æ¬¡å¼ºè°ƒï¼Œåˆ¤æ–­æœ€ä½³è¾“å‡ºæ˜¯å›°éš¾çš„ï¼Œä½†æˆ‘ä»¬å¯ä»¥ä¾é å›°æƒ‘åº¦æŒ‡æ ‡æ¥ç»™å‡ºä¸€ä¸ªï¼ˆå¤§è‡´çš„ï¼‰ç­”æ¡ˆã€‚
- en: '[PRE16]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '[PRE17]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'In this case, the perplexity of the quantized model is twice as low as the
    original one. In general, this is not the case, but it shows that this quantization
    technique is very competitive. In fact, the authors of LLM.int8() show that the
    performance degradation is so low itâ€™s negligible (<1%). However, it has an additional
    cost in terms of computation: LLM.int8() is roughly about 20% slower for large
    models.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œé‡åŒ–æ¨¡å‹çš„å›°æƒ‘åº¦æ˜¯åŸå§‹æ¨¡å‹çš„ä¸¤å€ä½ã€‚ä¸€èˆ¬æ¥è¯´ï¼Œè¿™ç§æƒ…å†µå¹¶ä¸å¸¸è§ï¼Œä½†å®ƒæ˜¾ç¤ºäº†è¿™ç§é‡åŒ–æŠ€æœ¯çš„ç«äº‰åŠ›ã€‚å®é™…ä¸Šï¼ŒLLM.int8()çš„ä½œè€…å±•ç¤ºäº†æ€§èƒ½ä¸‹é™éå¸¸å°ï¼Œå¯ä»¥å¿½ç•¥ä¸è®¡ï¼ˆ<1%ï¼‰ã€‚ç„¶è€Œï¼Œå®ƒåœ¨è®¡ç®—æ–¹é¢æœ‰é¢å¤–çš„æˆæœ¬ï¼šå¯¹äºå¤§æ¨¡å‹ï¼ŒLLM.int8()å¤§çº¦æ…¢äº†
    20%ã€‚
- en: Conclusion
  id: totrans-94
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç»“è®º
- en: 'This article provided an overview of the most popular weight quantization techniques.
    We started by gaining an understanding of floating point representation, before
    introducing two techniques for 8-bit quantization: **absmax** and **zero-point
    quantization**. However, their limitations, particularly when it comes to handling
    outliers, led to **LLM.int8()**, a technique that also preserves the modelâ€™s performance.
    This approach underlines the progress being made in the field of weight quantization,
    revealing the importance of properly addressing outliers.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬æ–‡æ¦‚è¿°äº†æœ€æµè¡Œçš„æƒé‡é‡åŒ–æŠ€æœ¯ã€‚æˆ‘ä»¬é¦–å…ˆäº†è§£äº†æµ®ç‚¹è¡¨ç¤ºï¼Œç„¶åä»‹ç»äº†ä¸¤ç§ 8 ä½é‡åŒ–æŠ€æœ¯ï¼š**absmax** å’Œ **é›¶ç‚¹é‡åŒ–**ã€‚ç„¶è€Œï¼Œå®ƒä»¬çš„å±€é™æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†å¼‚å¸¸å€¼æ—¶ï¼Œå¯¼è‡´äº†
    **LLM.int8()** æŠ€æœ¯ï¼Œè¯¥æŠ€æœ¯è¿˜ä¿ç•™äº†æ¨¡å‹çš„æ€§èƒ½ã€‚è¿™ç§æ–¹æ³•çªæ˜¾äº†æƒé‡é‡åŒ–é¢†åŸŸçš„è¿›å±•ï¼Œæ­ç¤ºäº†æ­£ç¡®å¤„ç†å¼‚å¸¸å€¼çš„é‡è¦æ€§ã€‚
- en: Looking forward, our next article will explore the GPTQ weight quantization
    technique in depth. This technique, introduced by [Frantar et al.](https://arxiv.org/abs/2210.17323),
    only utilizes 4 bits and represents a significant advancement in the field of
    weight quantization. We will provide a comprehensive guide on how to implement
    GPTQ using the AutoGPTQ library.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: å±•æœ›æœªæ¥ï¼Œæˆ‘ä»¬çš„ä¸‹ä¸€ç¯‡æ–‡ç« å°†æ·±å…¥æ¢è®¨ GPTQ æƒé‡é‡åŒ–æŠ€æœ¯ã€‚è¿™é¡¹æŠ€æœ¯ç”± [Frantar ç­‰äºº](https://arxiv.org/abs/2210.17323)
    æå‡ºï¼Œåªä½¿ç”¨äº† 4 ä½ï¼Œå¹¶ä¸”ä»£è¡¨äº†æƒé‡é‡åŒ–é¢†åŸŸçš„é‡å¤§è¿›å±•ã€‚æˆ‘ä»¬å°†æä¾›å¦‚ä½•ä½¿ç”¨ AutoGPTQ åº“å®ç° GPTQ çš„å…¨é¢æŒ‡å—ã€‚
- en: If youâ€™re interested in more technical content around LLMs, follow me on Twitter
    [@maximelabonne](https://twitter.com/maximelabonne).
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä½ å¯¹æ›´å¤šå…³äºLLMçš„æŠ€æœ¯å†…å®¹æ„Ÿå…´è¶£ï¼Œå¯ä»¥åœ¨Twitterä¸Šå…³æ³¨æˆ‘ [@maximelabonne](https://twitter.com/maximelabonne)ã€‚
- en: References
  id: totrans-98
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å‚è€ƒæ–‡çŒ®
- en: 'T. Dettmers, M. Lewis, Y. Belkada, and L. Zettlemoyer, [LLM.int8(): 8-bit Matrix
    Multiplication for Transformers at Scale](https://arxiv.org/abs/2208.07339). 2022.'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'T. Dettmers, M. Lewis, Y. Belkada, å’Œ L. Zettlemoyerï¼Œ[LLM.int8(): å¤§è§„æ¨¡å˜å‹å™¨çš„ 8
    ä½çŸ©é˜µä¹˜æ³•](https://arxiv.org/abs/2208.07339)ã€‚2022å¹´ã€‚'
- en: Y. Beldaka, and T. Dettmers, [A Gentle Introduction to 8-bit Matrix Multiplication](https://huggingface.co/blog/hf-bitsandbytes-integration),
    Hugging Face Blog (2022).
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Y. Beldaka å’Œ T. Dettmers, [8ä½çŸ©é˜µä¹˜æ³•çš„ç®€æ˜ä»‹ç»](https://huggingface.co/blog/hf-bitsandbytes-integration),
    Hugging Face Blog (2022)ã€‚
- en: A. Gholami, S. Kim, Z. Dong, Z. Yao, M. W. Mahoney, and K. Keutzer, [A Survey
    of Quantization Methods for Efficient Neural Network Inference](https://arxiv.org/abs/2103.13630).
    2021.
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: A. Gholami, S. Kim, Z. Dong, Z. Yao, M. W. Mahoney å’Œ K. Keutzer, [é«˜æ•ˆç¥ç»ç½‘ç»œæ¨ç†çš„é‡åŒ–æ–¹æ³•ç»¼è¿°](https://arxiv.org/abs/2103.13630)ã€‚2021å¹´ã€‚
- en: 'H. Wu, P. Judd, X. Zhang, M. Isaev, and P. Micikevicius, [Integer Quantization
    for Deep Learning Inference: Principles and Empirical Evaluation](https://arxiv.org/abs/2004.09602).
    2020.'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: H. Wu, P. Judd, X. Zhang, M. Isaev å’Œ P. Micikevicius, [æ·±åº¦å­¦ä¹ æ¨ç†çš„æ•´æ•°é‡åŒ–ï¼šåŸç†ä¸å®è¯è¯„ä¼°](https://arxiv.org/abs/2004.09602)ã€‚2020å¹´ã€‚
- en: Lilian Weng, [Large Transformer Model Inference Optimization](https://lilianweng.github.io/posts/2023-01-10-inference-optimization/),
    Lilâ€™Log (2023).
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lilian Weng, [å¤§å‹å˜å‹å™¨æ¨¡å‹æ¨ç†ä¼˜åŒ–](https://lilianweng.github.io/posts/2023-01-10-inference-optimization/),
    Lilâ€™Log (2023)ã€‚
- en: Kamil Czarnogorski, [Local Large Language Models](https://int8.io/local-large-language-models-beginners-guide/),
    Int8 (2023).
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kamil Czarnogorski, [æœ¬åœ°å¤§å‹è¯­è¨€æ¨¡å‹](https://int8.io/local-large-language-models-beginners-guide/),
    Int8 (2023)ã€‚
