- en: A Quickstart Guide To Uprooting Model Bias
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/a-quickstart-guide-to-uprooting-model-bias-f4465c8e84bc?source=collection_archive---------12-----------------------#2023-01-19](https://towardsdatascience.com/a-quickstart-guide-to-uprooting-model-bias-f4465c8e84bc?source=collection_archive---------12-----------------------#2023-01-19)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/af28ba3d159eb1a66dd25bd6ce62218b.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://aparnadhinak.medium.com/?source=post_page-----f4465c8e84bc--------------------------------)[![Aparna
    Dhinakaran](../Images/e431ee69563ecb27c86f3428ba53574c.png)](https://aparnadhinak.medium.com/?source=post_page-----f4465c8e84bc--------------------------------)[](https://towardsdatascience.com/?source=post_page-----f4465c8e84bc--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----f4465c8e84bc--------------------------------)
    [Aparna Dhinakaran](https://aparnadhinak.medium.com/?source=post_page-----f4465c8e84bc--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ff32f85889f3a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-quickstart-guide-to-uprooting-model-bias-f4465c8e84bc&user=Aparna+Dhinakaran&userId=f32f85889f3a&source=post_page-f32f85889f3a----f4465c8e84bc---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----f4465c8e84bc--------------------------------)
    ·12 min read·Jan 19, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Ff4465c8e84bc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-quickstart-guide-to-uprooting-model-bias-f4465c8e84bc&user=Aparna+Dhinakaran&userId=f32f85889f3a&source=-----f4465c8e84bc---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff4465c8e84bc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-quickstart-guide-to-uprooting-model-bias-f4465c8e84bc&source=-----f4465c8e84bc---------------------bookmark_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: '*This article is co-authored by Amber Roberts, Machine Learning Engineer at
    Arize AI*'
  prefs: []
  type: TYPE_NORMAL
- en: In today’s world, it is all too common to read about AI acting in discriminatory
    ways. From real estate valuation models that reflect the [continued legacy of
    housing discrimination](https://www.technologyreview.com/2020/10/20/1009452/ai-has-exacerbated-racial-bias-in-housing-could-it-help-eliminate-it-instead/)
    to models used in healthcare that amplify inequities [around access to care and
    health outcomes](https://www.hsph.harvard.edu/news/hsph-in-the-news/study-widely-used-health-care-algorithm-has-racial-bias/),
    examples are unfortunately easy to find. As machine learning (ML) models get more
    complex, the true reach of this issue and its impact on marginalized groups is
    not likely fully known. Fortunately, there are a few simple steps that ML teams
    can take to uproot harmful model biases across the ML lifecycle.
  prefs: []
  type: TYPE_NORMAL
- en: What Is Model Bias?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Model bias is the tendency of a machine learning model to make consistent, systematic
    errors in its predictions. A model will tend to systematically learn the wrong
    signals by not considering all the information contained within the data. Model
    bias may lead an algorithm to miss the relevant relationship between data inputs
    (features) and targeted outputs (predictions). In essence, bias arises when an
    algorithm has insufficient capability in learning the appropriate signal from
    the dataset. For decades, bias in machine learning has been recognized as a potential
    concern, but it remains a complex and challenging issue for machine learning researchers
    and engineers when deploying models into production.
  prefs: []
  type: TYPE_NORMAL
- en: How Does Bias Get Introduced Into a Model?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Biases can be introduced into the machine learning process and reinforced by
    model predictions from a variety of sources. At various phases of the model’s
    development, insufficient data, inconsistent data collecting, and poor data practices
    can all lead to bias in the model’s decisions. While these biases are typically
    unintentional, their existence can have a substantial influence on machine learning
    systems and result in disastrous outcomes — from [employment discrimination](https://www.thomsonreuters.com/en-us/posts/legal/ai-enabled-anti-black-bias/)
    to [misdiagnosis](https://www.who.int/publications/i/item/9789240029200) in healthcare.
    If the machine learning pipeline you’re using contains inherent biases, the model
    will not only learn them but will also exacerbate and maybe even amplify them.
  prefs: []
  type: TYPE_NORMAL
- en: Identifying, assessing, and addressing any potential biases that may impact
    the outcome is a crucial requirement when creating a new machine learning model
    and maintaining it in production. As machine learning practitioners, it is our
    responsibility to inspect, monitor, assess, investigate, and evaluate these systems
    to avoid bias that negatively impacts the effectiveness of the decisions that
    models drive.
  prefs: []
  type: TYPE_NORMAL
- en: Causes of bias in models span both the data and model itself.
  prefs: []
  type: TYPE_NORMAL
- en: Representation Bias (Data)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*Application:* bias introduced by historical data, historical bias, skewed
    sample, tainted examples'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Example:* embeddings trained on news articles exhibit gender-based stereotypes
    in society'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Measurement Bias (Data)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*Application:* proxies, sample size disparity, limited features'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Example:* proxy measurements that predict the likelihood of recidivism lead
    to black defendants getting harsher sentences than white defendants for the same
    crime'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Aggregation Bias (Model)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*Application:* single aggregated model for all populations'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Example:* if you have a single prediction model to predict likelihood for
    a particular disease across minorities, it would likely fail and exhibit bias'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evaluation Bias (Model)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*Application:* benchmarks used for evaluation do not represent the general
    population'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Example:* if you benchmark a housing price prediction model for California
    and then try applying it to the South Carolina housing market, the prices would
    be biased'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Parity Prerequisites
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Defining Protected Attributes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Of course, you can’t quantify harmful model bias without first defining who
    is being protected.
  prefs: []
  type: TYPE_NORMAL
- en: Knowing protected classes under the law is a good first step. Most Americans
    probably know that the U.S. Civil Rights Act of 1964 precludes discrimination
    on the basis of race and sex, for example, but fewer may know that other attributes
    — like genetic information or citizenship — also qualify as protected classes
    under the law and can result in [millions or billions](https://www.justice.gov/opa/pr/justice-department-reaches-settlement-wells-fargo-resulting-more-175-million-relief)
    of dollars in fines when violations occur.
  prefs: []
  type: TYPE_NORMAL
- en: Legal compliance is just a starting point. Many large enterprises also [go beyond](https://purpose.businessroundtable.org/)
    these legal requirements and have additional protected classes or public commitments
    to diversity and equity.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/faee444511a6ac4201789bca31f369ba.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: Defining Fairness
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Once you are clear on protected classes in all relevant jurisdictions, the next
    step is defining what fairness looks like. While this is a [complex topic](https://arxiv.org/pdf/1908.09635.pdf),
    a few fundamentals can help.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/dbb303dc1ceb8828d59d79a998492ee2.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: One primary distinction is the difference between group (equal and proportional)
    fairness and individual fairness.
  prefs: []
  type: TYPE_NORMAL
- en: '**Group fairness** is defined by protected attributes receiving similar treatments
    or outcomes'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Individual fairness** is defined as similar individuals receiving similar
    treatments or outcomes'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For example, imagine a bank that is evaluating 100 mortgage applications. 70
    applications are from men and 30 from women. For group fairness based on equal
    percentages, you would approve 50% of applications for men (35) and 50% for women
    (15). For group fairness based on equal numbers, on the other hand, 50 approvals
    would be spread evenly — 25 for men and 25 for women. Either outcome might be
    considered unfair if there is a higher percentage of loan-worthy individuals in
    either group. One prevailing approach in the industry to mitigate model bias is
    to ensure prediction accuracy is the same for each group — a measure of equal
    opportunity versus accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/320e687084b7c1159388e585af72ea3a.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: What Are the Data Modeling Stages That Are Vulnerable To Bias?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Model fairness [impacts](https://www.oreilly.com/library/view/practical-fairness/9781492075721/)
    the pre-processing, in-processing, and post-processing stages of the data modeling
    pipeline. Fairness interventions, or actions taken to ensure that the model is
    not biased against certain groups, should be implemented at each stage of this
    process.
  prefs: []
  type: TYPE_NORMAL
- en: Pre-Processing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**What is it?** The earliest stage of data processing, during which data is
    translated into inputs for a machine learning model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Why intervene on model fairness at this stage?** Intervening at this earliest
    stage in the ML lifecycle can have a big impact on the data modeling process and
    downstream metrics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**How can you achieve fairness intervention at this stage?** Removing, obscuring,
    obfuscating, renaming or replacing sensitive attributes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Example:** A bank interested in building a model to predict loan defaults
    might sample data to ensure that it includes a representative mix of applicants
    from different races, genders, and geographic locations before removing sensitive
    variables from the data to prevent them from being used in the model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In-Processing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**What is it?** In-processing refers to any action that changes the training
    process of a machine learning model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Why intervene on model fairness at this stage?** If we can’t intervene at
    the earliest stage due to computational constraints or constraints on proprietary
    or licensed data, the next best option for addressing model fairness is in the
    training process. Intervening at this stage can allow teams to keep their training
    data sets raw and unaltered.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**How can you achieve fairness intervention at this stage?** Model regularization
    via adding an additional term to the model’s loss function so no one feature unfairly
    dominates a model’s decisions. You can alsouse an adversarial model to reduce
    the amount of unfair or false information in your model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Example:** A medical provider training a model predicting patient outcomes
    might create an adversary model to use outputs from the target model to predict
    for a protected category for the patient. This is to be sure that a patient’s
    personal information (like, income, race and gender) aren’t predictors of their
    healthcare outcome.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Post-Processing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**What is it?** Takes place after the model has been trained on processed data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Why intervene on model fairness at this stage?** When teams inherit a model
    from a prior team without any knowledge of that model, this may be their earliest
    stage of fairness intervention.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**How can you achieve fairness intervention at this stage?** Equalize decisions
    before the decision is received by the user; third-party auditing tools and bias
    tracing with an ML observability tool can help.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Example:** A broadband provider with a model that predicts customer churn
    wants to ensure it does not discriminate in offering discounts to customers. The
    company implements fairness checks with fairness-specific metrics as part of the
    model deployment process. Bias tracing then monitors model performance on a diverse
    set of customers to ensure that it is not biased against any particular group
    to ensure that outputs remain fair. If there is algorithm bias present, the decisions
    are equalized.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What Are Prevailing Model Fairness Metrics?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are a [wealth of model fairness metrics](http://www.datasciencepublicpolicy.org/our-work/tools-guides/aequitas/)
    that are appropriate depending on your goal. Here are definitions and recommendations
    for prevailing metrics and where to use each.
  prefs: []
  type: TYPE_NORMAL
- en: When deciding on which [fairness metric](https://arize.com/blog-course/fairness-bias-metrics/)
    or metrics to use, you must think about what insights you need to ensure your
    model is not acting discriminately. Teams that care about fairness — especially
    those working in highly regulated industries such as health, lending, insurance,
    and financial services — typically want to see if their models are fair and unbiased
    across sensitive attributes (i.e. race or sex). When a model is biased, teams
    then need to know which group is experiencing the most bias so that action can
    be taken.
  prefs: []
  type: TYPE_NORMAL
- en: To understand the fairness metric value for the period of time you are evaluating,
    many companies use the four-fifths rule. The four-fifths rule is a threshold that
    is [used](https://www.eeoc.gov/laws/guidance/questions-and-answers-clarify-and-provide-common-interpretation-uniform-guidelines)
    by regulatory agencies like the United States Equal Employment Opportunity Commission
    to help in identifying adverse treatment of protected classes. Since an ideal
    parity score is 1, when leveraging the four-fifths rule you typically measure
    whether your fairness metric parity score falls outside of the 0.8–1.25 threshold.
    If the parity score is less than 0.8 or greater than 1.25, algorithmic bias against
    the selected sensitive group may be present in your model.
  prefs: []
  type: TYPE_NORMAL
- en: From the decision tree above, we can see how the fairness metric of choice depends
    on whether your model is addressing disparate representation, equal numbers or
    assistive actions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s look at when and where to use these metrics (*note: FP = False Positive,
    TP = True Positive, FN = False Negative, TN = True Negative*).'
  prefs: []
  type: TYPE_NORMAL
- en: Recall Parity
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Definition:** Measures how “sensitive” the model is for one group compared
    to another, or the model’s ability to predict true positives correctly'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**When to use it:** Recall parity is achieved if the recall in the subgroups
    are close to each other'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**How to calculate it:** *Recall Parity = Recall_sensitive / Recall_base* ;
    Recall = TP / (TP + FN)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: False Positive Rate Parity
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Definition:** Measures whether a model incorrectly predicts the positive
    class for the sensitive group as compared to the base group'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**When to use it:** False positive rate parity is achieved if the false positive
    rates (the ratio between the number of false positives and the total number of
    negatives) in the subgroups are close to each other'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**How to calculate it:** *False Positive Parity = FPR_underprivileged_group
    / FPR_privileged_group*; False Positive Rate = FP / (FP + TN)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Disparate Impact
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Definition:** A quantitative measure of the adverse treatment of protected
    classes'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**When to use it:** Disparate impact, also known as proportional parity, is
    used to check whether the ratio of outcomes for different groups is the same as
    the ratio of their presence in the population'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**How to calculate it:** If a job offer rate for men is 50% and the job offer
    rate for women is 25%, then the ratio of the two rates is 2, indicating disparate
    impact'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Once you have defined fairness in the context of your business problem by consulting
    the fairness tree, you can calculate your parity scores and use the four-fifths
    rule to determine if you need to intervene at the pre-processing, in-processing,
    or post-processing stage in your model development pipeline. For a cheatsheet
    on the type of parity you want to achieve to demonstrate algorithmic neutrality,
    see below.
  prefs: []
  type: TYPE_NORMAL
- en: Type 1 Parity
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Description:** Fairness in both false discovery rate (FDR) parity and false
    positive rate (FPR) parity'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Calculation:** *FDR = FP / (TP+FP)* *; FPR = FP / (TN+FP)*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Type 2 Parity
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Description:** Fairness in both false omission rate (FOR) parity and false
    negative rate (FNR) parity'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Calculation:** *FOR = FN / (TN + FN)* ; *FNR = FN / ( TP + FN)*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Equalized Odds
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Description:** Fairness in both false positive rate (FPR) parity and true
    positive rate (TPR) parity'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Calculation:** *FPR = FP / (TN+FP)* ; *TPR = TP / (TP + FN)*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Supervised Fairness
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Description:** Fairness in both Type 1 and Type 2 parity'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Calculation:** *see above*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Overall Fairness
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Description:** Fairness across all metrics used in a confusion matrix'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Calculation:** *FP, TP, FN, TN*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What Tools Exist To Help Tackle Model Bias?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are a variety of tools built to help tackle algorithmic bias across the
    ML lifecycle.
  prefs: []
  type: TYPE_NORMAL
- en: Model Building & Validation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Most solutions focus on tackling the initial stages of model development, with
    the goal of providing model fairness checks before a model is shipped.
  prefs: []
  type: TYPE_NORMAL
- en: 'Examples of tools:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Aequitas: an open-source bias audit toolkit to do audits of machine learning
    models for discrimination and bias.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Arize AI (**full disclosure**: I am co-founder of Arize!): offers model fairness
    checks and comparisons across a training baseline and production data and root
    cause analysis workflows.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'IBM Fairness 360: an open source toolkit can help you examine, report, and
    mitigate discrimination and bias in machine learning models via an audit.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Google’s PAIR AI: offers several tools useful for specific use cases, including
    a tool for mitigating fairness and bias issues for image datasets supported by
    the TensorFlow Datasets API.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: While some of these tools can be used for aggregated fairness metrics and after-the-fact
    explanations (i.e. model explainability) useful for audits, they are mostly not
    designed for real-time monitoring in production.
  prefs: []
  type: TYPE_NORMAL
- en: Monitoring In Production
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Monitoring fairness metrics in production is important for a simple reason:
    when it comes to deployed AI, it is a matter of when — not if — model bias will
    occur. [Concept drift](https://arize.com/model-drift/), new patterns not seen
    in training, training-serving skew, and outliers challenge even the most advanced
    teams deploying models that perform flawlessly in training and pass the validation
    stage.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are some platforms that offer real-time fairness monitoring in production:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Arize: automatic monitors and fairness checks, with multidimensional comparisons
    to uncover model features and cohorts contributing to algorithmic bias'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'DataRobot: monitors fairness metrics like proportional parity with workflows
    to compare production data tor training'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How Should Teams Resolve Model Bias?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Resolving model bias starts with understanding the data, ensuring teams have
    the right tools, and ensuring organizational governance is set up to ensure fairness.
    Teams need to be aware of the pre-processing, in-processing, and post-processing
    stages of the data modeling pipeline that are valuable to bias – therefore, fairness
    intervention needs to be present in one ( if not more) of these stages. Here are
    a few steps organizations can take to achieve fairness at these stages.
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 1: Make protected class data available to model builders and ML teams
    maintaining models in production**'
  prefs: []
  type: TYPE_NORMAL
- en: According to a [recent survey](https://arize.com/resource/survey-machine-learning-observability-results/),
    79.7% of ML teams report that they “lack access to protected data needed to root
    out bias or ethics issues” at least some of the time, and nearly half (42.1%)
    say this is an issue at least somewhat often. This needs to change. As one researcher
    [puts it](https://fairmlbook.org/classification.html), there is no fairness through
    unawareness.
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 2: Ensure you have tools for visibility into fairness in production,
    ideally before models are shipped**'
  prefs: []
  type: TYPE_NORMAL
- en: Coupling fairness checks at the model-building stage with periodic after-the-fact
    audits is insufficient in a world where model bias can cause real-world harm.
    Continuous monitoring and alerting can help surface blindspots (unknown unknowns)
    that inevitably creep up in the real world and speed up time-to-resolution. When
    model owners and ML engineers maintaining models in production have optimized
    guidance and tools, good things happen.
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 3: Be an internal change agent and act with urgency**'
  prefs: []
  type: TYPE_NORMAL
- en: Addressing model bias is not merely about machine learning. Many challenges
    — such as tradeoffs between fairness and business results or diffused responsibility
    across teams — can only be resolved with multiple teams and executive involvement.
    ML teams are well-positioned to play a key role in building a multi-pronged approach
    that combines purpose-built infrastructure, governance, and dedicated working
    groups for accountability.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Of course, these steps are just the beginning and the industry has a long way
    to go on fairness. Identifying the problem is only half the battle; taking action
    is critical. ML observability and quickly tracing the cause of a model fairness
    issue at a cohort level can help, particularly in knowing when to retrain or revert
    to a prior model (or no model).
  prefs: []
  type: TYPE_NORMAL
- en: Contact Us
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If this blog caught your attention and you’re eager to learn more about [machine
    learning observability](https://arize.com/ml-observability/) and [model monitoring](https://arize.com/model-monitoring/),
    check out our other [blogs](https://arize.com/blog/) and [resources](https://arize.com/resource-hub/)!
    Feel free to [reach out](https://arize.com/contact/) to us with any questions
    or comments, [signup for a free account](https://app.arize.com/auth/join), or
    find our open positions [here](https://arize.com/careers/) if you’re interested
    in joining a fun, rockstar engineering crew to help make models successful in
    production!
  prefs: []
  type: TYPE_NORMAL
