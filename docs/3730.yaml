- en: Berkson's Paradox in Machine Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/berksons-paradox-in-machine-learning-113818ac7657?source=collection_archive---------3-----------------------#2023-12-22](https://towardsdatascience.com/berksons-paradox-in-machine-learning-113818ac7657?source=collection_archive---------3-----------------------#2023-12-22)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Understanding Hidden Biases in Data Analysis
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@ocaelen?source=post_page-----113818ac7657--------------------------------)[![Olivier
    Caelen](../Images/5315295f68999af7c14b456694d19979.png)](https://medium.com/@ocaelen?source=post_page-----113818ac7657--------------------------------)[](https://towardsdatascience.com/?source=post_page-----113818ac7657--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----113818ac7657--------------------------------)
    [Olivier Caelen](https://medium.com/@ocaelen?source=post_page-----113818ac7657--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fd7268030c8a8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fberksons-paradox-in-machine-learning-113818ac7657&user=Olivier+Caelen&userId=d7268030c8a8&source=post_page-d7268030c8a8----113818ac7657---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----113818ac7657--------------------------------)
    ·8 min read·Dec 22, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F113818ac7657&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fberksons-paradox-in-machine-learning-113818ac7657&user=Olivier+Caelen&userId=d7268030c8a8&source=-----113818ac7657---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F113818ac7657&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fberksons-paradox-in-machine-learning-113818ac7657&source=-----113818ac7657---------------------bookmark_footer-----------)![](../Images/1331ec1fae3eb1c3317bf49f07732fa7.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Generated with DALL-E
  prefs: []
  type: TYPE_NORMAL
- en: Sometimes, statistics show surprising things that make us question what we see
    daily. Berkson's Paradox is one example of this. This Paradox is strongly related
    to the sampling bias problem and occurs when we mistakenly think that two things
    are related because we don't see the whole picture. As a machine learning expert,
    you should be familiar with this Paradox because it can significantly impact the
    accuracy of your predictive models by leading to incorrect assumptions about the
    relationship between variables.
  prefs: []
  type: TYPE_NORMAL
- en: Let us start with some examples
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Based on [Berkson](https://en.wikipedia.org/wiki/Joseph_Berkson)'s original
    example, let's imagine a retrospective study conducted in a hospital. In this
    hospital, researchers are studying the risk factors for cholecystitis (a disease
    of the gallbladder), and one of these risks could be diabetes. Because samples
    are drawn from a hospitalized population rather than the general population, there
    is a sampling bias, and this can lead to the mistaken belief that diabetes protects
    against cholecystitis.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Another well-known example comes from [Jordan Ellenberg](https://en.wikipedia.org/wiki/Jordan_Ellenberg).
    In this example, Alex creates a dating pool. This group does not represent all
    men well; we have a sampling bias because she picks either very friendly men,
    attractive, or both. And, in Alex's dating pool, something interesting happens…
    Among the men she dates, it seems like the nicer they are, the less attractive
    they appear, and vice versa. This sampling bias can lead to a mistaken belief
    for Alex that there is a negative association between being friendly and attractive.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's try to formalize a bit the problem
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Suppose we have two independent events, *X* and *Y*. As these events are independent:'
  prefs: []
  type: TYPE_NORMAL
- en: These random events can be, for example, having the disease cholecystitis or
    having diabetes, as in the first example, or being nice or beautiful in the second
    example. Of course, it's important to realize that when I say the two events are
    independent, I'm talking about the entire population!
  prefs: []
  type: TYPE_NORMAL
- en: 'In the previous examples, the sampling bias was always of the same type: there
    were no cases where neither event occurred. In the hospital samples, no patient
    has neither cholecystitis nor diabetes. And in the Alex sample, no man is both
    unfriendly and ugly. We are, therefore, conditioned to the realization of at least
    one of the two events: event *X* has occurred, or event *Y* has occurred, or both.
    To represent this, we can define a new event, *Z,* which is the union of events
    *X* and *Y*.'
  prefs: []
  type: TYPE_NORMAL
- en: 'And now, we can write the following to indicate that we are under the sampling
    bias hypothesis:'
  prefs: []
  type: TYPE_NORMAL
- en: That is the probability that event *X* occurs, knowing that events *X* or *Y*
    (or both)have already been realized. Intuitively, we can feel that this probability
    is higher than *P(X)* … but it is also possible to show it formally.
  prefs: []
  type: TYPE_NORMAL
- en: 'To do that, we know that:'
  prefs: []
  type: TYPE_NORMAL
- en: 'By assuming that it is possible for the two events not to occur at the same
    time (e.g., there are ugly and unfriendly people), the previous statement can
    become a strict inequality; because the set *(X* ∪ *Y)* is not the sample space
    Ω:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, if we divide both sides of this strict inequality by *P(X ∪ Y)* and then
    multiply by *P(X)*, we get:'
  prefs: []
  type: TYPE_NORMAL
- en: where
  prefs: []
  type: TYPE_NORMAL
- en: 'Therefore, we have indeed that the probability under sampling bias *P(X|Z)*
    is higher than *P(X),* in the entire population:'
  prefs: []
  type: TYPE_NORMAL
- en: Okay, fine … But now let us return to our Berkson's Paradox. We have two independent
    random variables, *X* and *Y*, and we want to show that they become dependent
    under the sampling bias *Z* described above.
  prefs: []
  type: TYPE_NORMAL
- en: To do that, let's start with *P(X | Y ∩ Z)*, which is the probability of observing
    event *X*, given that we know event *Y* has already occurred and that we are under-sampling
    bias *Z.* Note that *P(X | Y ∩ Z)* can also be written as *P(X | Y, Z).*
  prefs: []
  type: TYPE_NORMAL
- en: 'As *(Y ∩ Z) = (Y ∩ (X ∪ Y)) = Y,* and as *X* and *Y* are independent variables,
    we have:'
  prefs: []
  type: TYPE_NORMAL
- en: 'And … finally, knowing that *P(X) < P(X | Z)*, we get what we’re looking for:'
  prefs: []
  type: TYPE_NORMAL
- en: This equation shows that under the sampling bias defined by Z, the two initially
    independent events, *X* and *Y,* become dependent (otherwise, we would have had
    equality rather than ">").
  prefs: []
  type: TYPE_NORMAL
- en: Go back to the example of Alex's dating pool, if
  prefs: []
  type: TYPE_NORMAL
- en: Z is the event of being in Alex's dating pool
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: X is the event of selecting a friendly guy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Y is the event of selecting an attractive guy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: then *(X | Z)* is the event that Alex meets a nice man and *(X | Y ∩ Z)* is
    the event that Alex meets a nice man given that he is beautiful. Because of the
    selection process used to build Alex's dating pool, and because of Berkson's Paradox,
    Alex will feel that when she meets good-looking boys, they won't be so nice, whereas
    these could be two independent events if there were taken from the whole population….
  prefs: []
  type: TYPE_NORMAL
- en: Perhaps a numerical example will help to be more concrete
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To illustrate Berkson''s Paradox, we use two dice:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Event X: The first die shows a 6.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Event Y: The second die shows either a one or a 2.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These two events are clearly independent, where *P(X)=1/6* and *P(Y)=1/3*.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's introduce our condition (*Z*), representing the biased sampling by
    excluding all outcomes where the first die is not six and the second is neither
    1 nor 2.
  prefs: []
  type: TYPE_NORMAL
- en: Under our biased sampling condition, we need to calculate the probability that
    the event *X* occurs, given that at least one of the events (*X* or *Y*) has occurred,
    and this is denoted by *P(X|Z)*.
  prefs: []
  type: TYPE_NORMAL
- en: First, we need to determine the probability of Z = (X ∪ Y) … and sorry, but
    from now we'll have to do a bit of calculation… I'll do it for you…. :-)
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we calculate the probability of X given Z:'
  prefs: []
  type: TYPE_NORMAL
- en: To see if there is a dependence between *X* and *Y* under the assumption that
    *Z* occurs, we have to compute *P(X | Y ∩ Z).*
  prefs: []
  type: TYPE_NORMAL
- en: As
  prefs: []
  type: TYPE_NORMAL
- en: We have
  prefs: []
  type: TYPE_NORMAL
- en: 'To demonstrate Berkson''s Paradox, we compare *P(X|Z)* with *P*(*X* ∣ Y ∩ Z)
    and we have:'
  prefs: []
  type: TYPE_NORMAL
- en: '*P(X | Z) = 0.375*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*P(X |Y ∩ Z) ≈ 0.1666…*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We retrieve indeed the property that under Berkson's Paradox, due to the sampling
    bias Z, we have *P(X | Z) > P(X ∣ Y ∩ Z)*.
  prefs: []
  type: TYPE_NORMAL
- en: I personally find it surprising! We had two dice … Two clearly independent random
    events… and we can obtain the impression that dice rolls become dependent through
    a sampling process.
  prefs: []
  type: TYPE_NORMAL
- en: To finish convincing us, let's finish with a bit of simulation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the code below, I will simulate dice rolls with Python.
  prefs: []
  type: TYPE_NORMAL
- en: The following code simulates one million experiments of rolling two dice, where
    for each experiment, it checks if the first dice roll is a 6 (event X) and if
    the second dice roll is a 1 or 2 (event Y). It then stores the results of these
    checks (True or False) in the lists X and Y, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Then, we have to check if these two events are indeed independent. To do that,
    the following code calculates the probability of event X and the conditional probability
    of event X given event Y. It does this by dividing the number of successful outcomes
    by the total number of experiments for each probability.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: As we can see, both probabilities are close; therefore (as expected ;-) ) or
    two dice are independent.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's see what happens when introducing the sampling bias *Z.* The following
    code filters the results of the experiments, keeping only those where either X
    = 1, Y = 1, or both. It stores these filtered results in the lists XZ and YZ.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: And now, let's check if these new variables are still independent.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: We have an inequality (the same values as in the previous section), meaning
    that if Z is true, then having information on Y changes the probabilities for
    X; therefore, they are no longer independent.
  prefs: []
  type: TYPE_NORMAL
- en: What are the implications of this Paradox for experts in machine learning?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: I don't think experts in machine learning pay enough attention to this type
    of bias. When we talk about Berkson's Paradox, we're diving into a critical topic
    for people working in machine learning. This idea is about understanding how we
    can be misled by the data we use. Berkson's Paradox warns us about the danger
    of using biased or one-sided data.
  prefs: []
  type: TYPE_NORMAL
- en: '**Credit Scoring Systems**: In finance, models trained on data featuring applicants
    with either high income or high credit scores, but rarely both, could falsely
    infer a negative correlation between these factors. This risks unfair lending
    practices by favoring certain demographic groups.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Social Media Algorithms**: In social media algorithms, Berkson''s Paradox
    can emerge when training models on extreme user data, like viral content with
    high popularity but low engagement and niche content with deep engagement but
    low popularity. This biased sampling often leads to the false conclusion that
    popularity and engagement depth are negatively correlated. Consequently, algorithms
    may undervalue content that balances moderate popularity and engagement, skewing
    the content recommendation system.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Job Applicant Screening Tools**: Screening models based on applicants with
    either high educational qualifications or extensive experience might incorrectly
    suggest an inverse relationship between these attributes, potentially overlooking
    well-balanced candidates.'
  prefs: []
  type: TYPE_NORMAL
- en: In each scenario, overlooking Berkson's Paradox can result in biased models,
    impacting decision-making and fairness. Machine learning experts must counteract
    this by diversifying data sources and continuously validating models against real-world
    scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In conclusion, Berkson's Paradox is a critical reminder for machine learning
    professionals to scrutinize their data sources and avoid misleading correlations.
    By understanding and accounting for this Paradox, we can build more accurate,
    fair, and practical models that truly reflect the complexities of the real world.
    Remember, the key to robust machine learning lies in sophisticated algorithms
    and the thoughtful, comprehensive collection and analysis of data.
  prefs: []
  type: TYPE_NORMAL
- en: Thanks for reading!
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Please consider following me if you wish to stay up to date with my latest publications
    and increase the visibility of this blog.
  prefs: []
  type: TYPE_NORMAL
