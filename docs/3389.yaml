- en: All you need to know to Develop using Large Language Models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/all-you-need-to-know-to-develop-using-large-language-models-5c45708156bc?source=collection_archive---------0-----------------------#2023-11-15](https://towardsdatascience.com/all-you-need-to-know-to-develop-using-large-language-models-5c45708156bc?source=collection_archive---------0-----------------------#2023-11-15)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/98c22ee1ed98e7ee21069160e94b530b.png)'
  prefs: []
  type: TYPE_IMG
- en: Image generated by Stable Diffusion
  prefs: []
  type: TYPE_NORMAL
- en: Explaining in simple terms the core technologies required to start developing
    LLM-based applications.
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://slgero.medium.com/?source=post_page-----5c45708156bc--------------------------------)[![Sergei
    Savvov](../Images/a653eaeeec954f1a71e6341b424f009a.png)](https://slgero.medium.com/?source=post_page-----5c45708156bc--------------------------------)[](https://towardsdatascience.com/?source=post_page-----5c45708156bc--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----5c45708156bc--------------------------------)
    [Sergei Savvov](https://slgero.medium.com/?source=post_page-----5c45708156bc--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F227c6aaec11a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fall-you-need-to-know-to-develop-using-large-language-models-5c45708156bc&user=Sergei+Savvov&userId=227c6aaec11a&source=post_page-227c6aaec11a----5c45708156bc---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----5c45708156bc--------------------------------)
    ·12 min read·Nov 15, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F5c45708156bc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fall-you-need-to-know-to-develop-using-large-language-models-5c45708156bc&user=Sergei+Savvov&userId=227c6aaec11a&source=-----5c45708156bc---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F5c45708156bc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fall-you-need-to-know-to-develop-using-large-language-models-5c45708156bc&source=-----5c45708156bc---------------------bookmark_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: The purpose of this article is to explain in simple terms the key technologies
    necessary to start developing LLM-based applications. It is intended for software
    developers, data scientists and AI enthusiasts who have a basic understanding
    of machine learning concepts and want to dive deeper. The article also provides
    numerous useful links for further study. It’s going to be interesting!
  prefs: []
  type: TYPE_NORMAL
- en: 1\. Introduction to Large Language Models (LLMs)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'I think you’ve already heard a thousand times about what an LLM is, so I won’t
    overload you with it. All we need to know is: a Large Language Model (LLM) is
    a **LARGE** neural network model that predicts the next token based on the previously
    predicted one. That’s all.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/baef17b325f3a0a91efb69b5f000121a.png)'
  prefs: []
  type: TYPE_IMG
- en: Comparison of the number of parameters of models. **Just look at how big GPT-3
    is.** And nobody knows about GPT-4…
  prefs: []
  type: TYPE_NORMAL
- en: The popularity of LLMs is due to their versatility and effectiveness. They perfectly
    cope with such tasks as translation, summarization, analysis of meanings, etc.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f067bc2b066b128c05a9b99209c1abb0.png)'
  prefs: []
  type: TYPE_IMG
- en: LLMs capabilities
  prefs: []
  type: TYPE_NORMAL
- en: 'Some examples of projects using LLMs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[**Notion AI**](https://www.notion.so/product/ai)— helps improve writing quality,
    generate content, correct spelling and grammar, edit voice and intonation, translate,
    and more.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[**GitHub Copilot**](https://github.com/features/copilot) — improves you code
    by offering autocomplete-style suggestions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[**Dropbox Dash**](https://blog.dropbox.com/topics/product/introducing-AI-powered-tools)—
    provides a natural-language search functionality, and also specifically cites
    which files the answer is derived from.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you want a detailed understanding of how LLMs work, I **highly recommend**
    reading the excellent article “[A Very Gentle Introduction to Large Language Models
    without the Hype](https://medium.com/@mark-riedl/a-very-gentle-introduction-to-large-language-models-without-the-hype-5f67941fa59e)”
    by [Mark Riedl](https://medium.com/u/7247bdeb9655?source=post_page-----5c45708156bc--------------------------------).
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Open Source vs Closed Source Models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'While there are quite a few differences, I highlight the following as the main
    ones:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Privacy** — one of the most important reasons why large companies choose
    self-hosted solutions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Fast prototyping** — great for small startups to quickly test their ideas
    without excessive expenditure.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Quality of generation** — either you fine-tune the model for your specific
    task or use a paid API.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'There is no definitive answer to what is better or worse. I highlighted the
    following points:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/93415feefe3e2027c695a8346773c5bd.png)'
  prefs: []
  type: TYPE_IMG
- en: If you are interested in delving deeper into the details, I suggest you read
    my article “[You don’t need hosted LLMs, do you?](https://medium.com/better-programming/you-dont-need-hosted-llms-do-you-1160b2520526)”.
  prefs: []
  type: TYPE_NORMAL
- en: '**Popular Open Source models**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[LLaMA-2](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) by **Meta**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Falcon](https://huggingface.co/tiiuae/falcon-7b) by **Technology Innovation
    Institute in Abu Dhabi**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Mistral](https://huggingface.co/mistralai/Mistral-7B-v0.1) by **Mistral AI**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Popular Closed Source models**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[GPT-4](https://openai.com/gpt-4) by **OpenAI**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Bard](https://bard.google.com/) by **Google**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Claude](https://www.anthropic.com/) by **Anthropic**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Explore the [LLM Collection](https://www.promptingguide.ai/models/collection)
    to view all models.
  prefs: []
  type: TYPE_NORMAL
- en: 3\. The Art of Prompt Engineering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: I know, I know, many consider it a pseudo-science or just a temporary hype.
    But the truth is, we still don’t fully understand how LLMs work. Why do they sometimes
    provide high-quality responses and other times fabricate facts ([hallucinate](https://medium.com/better-programming/fixing-hallucinations-in-llms-9ff0fd438e33))?
    Or why does adding “let’s think step-by-step” to a prompt suddenly improve the
    quality?
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a54da5dfb878ed04049f2bf61d77a0ee.png)'
  prefs: []
  type: TYPE_IMG
- en: Adding emotional coloring increases the quality on any models. [Source](https://arxiv.org/pdf/2307.11760.pdf)
  prefs: []
  type: TYPE_NORMAL
- en: Due to all this, scientists and enthusiasts can only experiment with different
    prompts, trying to make models perform better.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/78d2c86b54dc16eac939b1047e3f5927.png)'
  prefs: []
  type: TYPE_IMG
- en: Schematic illustrating various approaches to problem-solving with LLMs
  prefs: []
  type: TYPE_NORMAL
- en: 'I won’t bore you with complex prompt chains; instead, I’ll just give a few
    examples that will instantly improve performance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[***“Let’s think step by step”***](https://arxiv.org/pdf/2205.11916.pdf) —
    works great for reasoning or logical tasks..'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[***“Take a deep breath and work on this problem step-by-step“***](https://arxiv.org/pdf/2309.03409.pdf)—
    an improved version of the previous point. It can add a few more percent of quality.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[***“This is very important to my career”***](https://arxiv.org/pdf/2307.11760.pdf)—
    just add it to the end of your prompt and you’ll notice a 5–20% improvement in
    quality.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Also, I’ll share a useful prompt template right away:'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s combine our **X** command and clear thinking to quickly and accurately
    decipher the answer in the step-by-step approach. Provide details and include
    sources in the answer. This is very important to my career.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Where **X** is the industry of the task you are solving, for example, programming.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: I highly recommend spending a few evenings exploring prompt engineering techniques.
    This will not only allow you to better control the model’s behavior but will also
    help improve quality and reduce hallucinations. For this, I recommend reading
    the [**Prompt Engineering Guide.**](https://www.promptingguide.ai/introduction/basics)
  prefs: []
  type: TYPE_NORMAL
- en: 'Useful Links:'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[prompttools](https://github.com/hegelai/prompttools) — prompt testing and
    experimentation, with support for both LLMs (e.g. OpenAI, LLaMA).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[promptfoo](https://github.com/promptfoo/promptfoo) — testing and evaluating
    LLM output quality.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Awesome ChatGPT Prompts](https://github.com/f/awesome-chatgpt-prompts) — A
    collection of prompt examples to be used with the ChatGPT model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '4\. Incorporating New Data: Retrieval Augmented Generation (RAG)'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: RAG is a technique that combines the LLM with external knowledge bases. This
    allows the model to add relevant information or specific data not included in
    the original training set to the model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Despite the intimidating name (sometimes we add the word “reranker” to it),
    it’s actually a pretty old and surprisingly simple technique:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/efe26cbc8b7fd9a77f9e9b7959339d57.png)'
  prefs: []
  type: TYPE_IMG
- en: Schematic illustration of how RAG works
  prefs: []
  type: TYPE_NORMAL
- en: You convert documents into numbers, we call them [**embeddings**](/neural-network-embeddings-explained-4d028e6f0526).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then, you also convert the user’s search query into embeddings using the same
    model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Find the top K closest documents, usually based on [cosine similarity](https://en.wikipedia.org/wiki/Cosine_similarity).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Ask the LLM to generate a response based on these documents.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: When to Use
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Need for Current Information:** When the application requires information
    that is constantly updating, like news articles.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Domain-Specific Applications:** For applications that require specialized
    knowledge outside the LLM’s training data. For example, internal company documents.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When NOT to Use
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**General Conversational Applications:** Where the information needs to be
    general and doesn’t require additional data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Limited Resource Scenarios:** The retrieval component of RAG involves searching
    through large knowledge bases, which can be computationally expensive and slow
    — though still faster and less expensive than fine-tuning.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building an Application with RAG
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A great starting point is using the [LlamaIndex library](https://github.com/run-llama/llama_index).
    It allows you to quickly connect your data to LLMs. For this you only need a few
    lines of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: In real-world applications, things are noticeably more complex. Like in any
    development, you’ll encounter many nuances. For example, the retrieved documents
    might not always be relevant to the question or there might be issues with speed.
    However, even at this stage, you can significantly improve the quality of your
    search system.
  prefs: []
  type: TYPE_NORMAL
- en: What to Read & Useful Links
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Building RAG-based LLM Applications for Production](https://www.anyscale.com/blog/a-comprehensive-guide-for-building-rag-based-llm-applications-part-1)
    — an excellent detailed article about the main components of RAG.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Why Your RAG Is Not Reliable in a Production Environment](/why-your-rag-is-not-reliable-in-a-production-environment-9e6a73b3eddb)
    — a great article by [Ahmed Besbes](https://medium.com/u/adc8ea174c69?source=post_page-----5c45708156bc--------------------------------)
    that explains in clear language the difficulties that can arise when using RAG.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[7 Query Strategies for Navigating Knowledge Graphs With LlamaIndex](https://betterprogramming.pub/7-query-strategies-for-navigating-knowledge-graphs-with-llamaindex-ed551863d416)
    — an informative article from [Wenqi Glantz](https://medium.com/u/ce7cd5b8b74a?source=post_page-----5c45708156bc--------------------------------)
    that takes a detailed and nuanced look at building a RAG pipeline using LlamaIndex.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[OpenAI Retrieval tool](https://platform.openai.com/docs/assistants/tools/knowledge-retrieval)
    — use OpenAI’s RAG if you want a minimum of effort.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 5\. Fine-Tuning Your LLM
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Fine-tuning is the process of continuing the training of a pre-trained LLM
    on a specific dataset. You might ask why we need to train the model further if
    we can already add data using RAG. The simple answer is that only fine-tuning
    can tailor your model to understand a specific domain or define its style. For
    instance, I [created a copy of myself by fine-tuning on personal correspondences](https://medium.com/better-programming/unleash-your-digital-twin-how-fine-tuning-llm-can-create-your-perfect-doppelganger-b5913e7dda2e):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c3a7b21c134390c8c947c4ee28812d55.png)'
  prefs: []
  type: TYPE_IMG
- en: Demo of the fine-tuned model on the author’s correspondences
  prefs: []
  type: TYPE_NORMAL
- en: 'Okay, if I’ve convinced you of its importance, let’s see how it works (**spoiler**
    — it’s not so difficult):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c65f03ad9d92f925b08f8adcde668485.png)'
  prefs: []
  type: TYPE_IMG
- en: '*Classical approach of fine-tuning on domain specific data (all icons from*
    [*flaticon*](http://flaticon.com/)*)*'
  prefs: []
  type: TYPE_NORMAL
- en: Take a trained LLM, sometimes called Base LLM. You can download them from [HuggingFace](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Prepare your training data. You only need to compile instructions and responses.
    [Here’s an example](https://huggingface.co/datasets/databricks/databricks-dolly-15k)
    of such a dataset. You can also [generate synthetic data](https://www.promptingguide.ai/applications/generating)
    using GPT-4.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Choose a suitable fine-tuning method. [LoRA](https://github.com/microsoft/LoRA)
    and [QLoRA](https://github.com/artidoro/qlora) are currently popular.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Fine-tune the model on new data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: When to Use
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Niche Applications:** When the application deals with specialized or unconventional
    topics. For example, legal document applications that need to understand and handle
    legal jargon.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Custom Language Styles:** For applications requiring a specific tone or style.
    For example, creating an [AI character](https://beta.character.ai/) whether it’s
    a celebrity or a character from a book.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When NOT to Use
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Broad Applications:** Where the scope of the application is general and doesn’t
    require specialized knowledge.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Limited Data:** Fine-tuning requires a significant amount of relevant data.
    However, you can always [generate them with another LLM](https://www.confident-ai.com/blog/how-to-generate-synthetic-data-using-llms-part-1).
    For example, the [Alpaca dataset](https://github.com/gururise/AlpacaDataCleaned)
    of 52k LLM-generated instruction-response pairs was used to create the first finetuning
    [Llama v1](https://arxiv.org/abs/2302.13971) model earlier this year.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fine-tune your LLM
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You can find a vast number of articles dedicated to model fine-tuning. Just
    on Medium alone, there are thousands. Therefore, I don’t want to delve too deeply
    into this topic and will show you a high-level library, [Lit-GPT](https://github.com/Lightning-AI/lit-gpt),
    which hides all the magic inside. Yes, it doesn’t allow for much customization
    of the training process, but you can quickly conduct experiments and get initial
    results. You’ll need just a few lines of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'And that’s it! Your training process will start:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b919aa7721ea04f6df5c5783dd882fd0.png)'
  prefs: []
  type: TYPE_IMG
- en: Be aware that the process can take a long time. It takes approximately **10
    hours** and **30 GB** memory to fine-tune Falcon-7B on a single A100 GPU.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Of course, I’ve slightly oversimplified, and we’ve only scratched the surface.
    In reality, the fine-tuning process is much more complex and to get better results,
    you’ll need to understand various adapters, their parameters, and much more. However,
    even after such a simple iteration, you will have a new model that follows your
    instructions.
  prefs: []
  type: TYPE_NORMAL
- en: What to Read & Useful Links
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Create a Clone of Yourself With a Fine-tuned LLM](https://medium.com/better-programming/unleash-your-digital-twin-how-fine-tuning-llm-can-create-your-perfect-doppelganger-b5913e7dda2e)
    — my article where I wrote about collecting datasets, used parameters, and gave
    useful tips on fine-tuning.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Understanding Parameter-Efficient Fine-tuning of Large Language Models](https://lightning.ai/pages/community/article/understanding-llama-adapters/)
    — an excellent tutorial if you want to get into the details of the concept of
    fine-tuning and popular parameter-efficient alternatives.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Fine-tuning LLMs with LoRA and QLoRA: Insights from Hundreds of Experiments](https://lightning.ai/pages/community/lora-insights/)
    — one of my favorite articles for understanding the capabilities of LoRA.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[OpenAI Fine-tuning](https://platform.openai.com/docs/guides/fine-tuning) —
    if you want to fine-tune GPT-3.5 with minimal effort.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 6\. Deploying Your LLM Application
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Sometimes, all we want is to simply push a “deploy” button…
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ccafea187bb4ef867065b18a1781fc9e.png)'
  prefs: []
  type: TYPE_IMG
- en: Fortunately, this is quite feasible. There are a huge number of frameworks that
    specialize in deploying large language models. What makes them so good?
  prefs: []
  type: TYPE_NORMAL
- en: Lots of pre-built wrappers and integrations.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A vast selection of available models.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A multitude of internal optimizations.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rapid prototyping.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Choosing the Right Framework
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The choice of framework for deploying an LLM application depends on various
    factors, including the size of the model, the scalability requirements of the
    application, and the deployment environment. Currently, there isn’t a vast diversity
    of frameworks, so it shouldn’t be too difficult to understand their differences.
    Below, I’ve prepared a cheat sheet for you that will help you quickly get started:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/67ac80e1155c047a0175a5523a329036.png)'
  prefs: []
  type: TYPE_IMG
- en: Also, in my article “[7 Frameworks for Serving LLMs](https://medium.com/better-programming/frameworks-for-serving-llms-60b7f7b23407)”
    I provide a more detailed overview of the existing solutions. I recommend checking
    it out if you’re planning to deploy your model.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2b73a99b6ce220522fcf5cb5db8d26d4.png)'
  prefs: []
  type: TYPE_IMG
- en: Comparison of frameworks for LLMs inference
  prefs: []
  type: TYPE_NORMAL
- en: Example Code for Deployment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s move from theory to practice and try to deploy LLaMA-2 using [Text Generation
    Inference](https://github.com/huggingface/text-generation-inference). And, as
    you might have guessed, you’ll need just a few lines of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: That’s it! You’ve set up a RestAPI service with built-in logging, Prometheus
    endpoint for monitoring, token streaming, and your model is fully optimized. Isn’t
    this magical?
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/89f89d8b815ae7b78448c9441b533a16.png)'
  prefs: []
  type: TYPE_IMG
- en: API Documentation
  prefs: []
  type: TYPE_NORMAL
- en: What to Read & Useful Links
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[7 Frameworks for Serving LLMs](https://medium.com/better-programming/frameworks-for-serving-llms-60b7f7b23407)
    — comprehensive guide into LLMs inference and serving with detailed comparison.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Inference Endpoints](https://huggingface.co/inference-endpoints) — a product
    from HuggingFace that will allow you to deploy any LLMs in a few clicks. A good
    choice when you need rapid prototyping.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 7\. What Remains Behind the Scenes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Even though we’ve covered the main concepts needed for developing LLM-based
    applications, there are still some aspects you’ll likely encounter in the future.
    So, I’d like to leave a few useful links:'
  prefs: []
  type: TYPE_NORMAL
- en: Optimization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When you launch your first model, you inevitably find it’s not as fast as you’d
    like and consumes a lot of resources. If this is your case, you need to understand
    how it can be optimized.
  prefs: []
  type: TYPE_NORMAL
- en: '[7 Ways To Speed Up Inference of Your Hosted LLMs](https://medium.com/better-programming/speed-up-llm-inference-83653aa24c47)
    — techniques to speed up inference of LLMs to increase token generation speed
    and reduce memory consumption.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Optimizing Memory Usage for Training LLMs in PyTorch](https://lightning.ai/pages/community/tutorial/pytorch-memory-vit-llm/)
    — article provides a series of techniques that can reduce memory consumption in
    PyTorch by approximately 20x without sacrificing modeling performance and prediction
    accuracy.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evaluating
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Suppose you have a fine-tuned model. But how can you be sure that its quality
    has improved? What metrics should we use?
  prefs: []
  type: TYPE_NORMAL
- en: '[All about evaluating Large language models](https://explodinggradients.com/all-about-evaluating-large-language-models)
    — a good overview article about benchmarks and metrics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[evals](https://github.com/openai/evals) — the most popular framework for evaluating
    LLMs and LLM systems.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vector Databases
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If you work with RAG, at some point, you’ll move from storing vectors in memory
    to a database. For this, it’s important to understand what’s currently on the
    market and its limitations.
  prefs: []
  type: TYPE_NORMAL
- en: '[All You Need to Know about Vector Databases](/all-you-need-to-know-about-vector-databases-and-how-to-use-them-to-augment-your-llm-apps-596f39adfedb)
    — a step-by-step guide by [Dominik Polzer](https://medium.com/u/3ab8d3143e32?source=post_page-----5c45708156bc--------------------------------)
    to discover and harness the power of vector databases.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Picking a vector database: a comparison and guide for 2023](https://benchmark.vectorview.ai/vectordbs.html)
    — comparison of Pinecone, Weviate, Milvus, Qdrant, Chroma, Elasticsearch and PGvector
    databases.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LLM Agents
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In my opinion, the most promising development in LLMs. If you want multiple
    models to work together, I recommend exploring the following links.
  prefs: []
  type: TYPE_NORMAL
- en: '[A Survey on LLM-based Autonomous Agents](https://github.com/paitesanshi/llm-agent-survey#-more-comprehensive-summarization)
    — this is probably the most comprehensive overview of LLM based agents.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[autogen](https://github.com/microsoft/autogen) — is a framework that enables
    the development of LLM applications using multiple agents that can converse with
    each other to solve tasks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[OpenAgents](https://github.com/xlang-ai/OpenAgents) — an open platform for
    using and hosting language agents in the wild.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reinforcement Learning from Human Feedback (RLHF)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As soon as you allow users access to your model, you start taking responsibility.
    What if it responds rudely? Or reveals bomb-making ingredients? To avoid this,
    check out these articles:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Illustrating Reinforcement Learning from Human Feedback (RLHF)](https://huggingface.co/blog/rlhf)
    — an overview article that details the RLHF technology.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[RL4LMs](https://github.com/allenai/RL4LMs) — a modular RL library to fine-tune
    language models to human preferences.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[TRL](https://github.com/huggingface/trl) — a set of tools to train transformer
    language models with Reinforcement Learning, from the Supervised Fine-tuning step
    (SFT), Reward Modeling step (RM) to the Proximal Policy Optimization (PPO) step.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Despite the hype, which we’re all a bit tired of, LLMs will be with us for a
    long time, and the ability to understand their stack and write simple applications
    can give you a significant boost. I hope I’ve managed to immerse you a bit in
    this area and show you that there is nothing complicated or scary about it.
  prefs: []
  type: TYPE_NORMAL
- en: Thank you for your attention, stay tuned for new articles!
  prefs: []
  type: TYPE_NORMAL
- en: '**Disclaimer**: The information in the article is current as of November 2023,
    but please be aware that changes may occur thereafter.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*Unless otherwise noted, all images are by the author.*'
  prefs: []
  type: TYPE_NORMAL
- en: If you have any questions or suggestions, feel free to connect on [LinkedIn](https://www.linkedin.com/in/sergey-savvov/).
  prefs: []
  type: TYPE_NORMAL
