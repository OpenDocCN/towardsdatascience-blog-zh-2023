- en: When Should You Fine-Tune LLMs?
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 你什么时候应该微调 LLM？
- en: 原文：[https://towardsdatascience.com/when-should-you-fine-tune-llms-2dddc09a404a?source=collection_archive---------1-----------------------#2023-05-15](https://towardsdatascience.com/when-should-you-fine-tune-llms-2dddc09a404a?source=collection_archive---------1-----------------------#2023-05-15)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/when-should-you-fine-tune-llms-2dddc09a404a?source=collection_archive---------1-----------------------#2023-05-15](https://towardsdatascience.com/when-should-you-fine-tune-llms-2dddc09a404a?source=collection_archive---------1-----------------------#2023-05-15)
- en: There has been a flurry of exciting open-source LLMs which can be fine-tuned.
    But how does that compare to just using a closed API?
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 目前有很多令人兴奋的可以微调的开源 LLM。但这与使用一个封闭的 API 相比如何呢？
- en: '[](https://skanda-vivek.medium.com/?source=post_page-----2dddc09a404a--------------------------------)[![Skanda
    Vivek](../Images/9d25bee2fb75176ca7f7ea6eff7d7ab5.png)](https://skanda-vivek.medium.com/?source=post_page-----2dddc09a404a--------------------------------)[](https://towardsdatascience.com/?source=post_page-----2dddc09a404a--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----2dddc09a404a--------------------------------)
    [Skanda Vivek](https://skanda-vivek.medium.com/?source=post_page-----2dddc09a404a--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://skanda-vivek.medium.com/?source=post_page-----2dddc09a404a--------------------------------)[![Skanda
    Vivek](../Images/9d25bee2fb75176ca7f7ea6eff7d7ab5.png)](https://skanda-vivek.medium.com/?source=post_page-----2dddc09a404a--------------------------------)[](https://towardsdatascience.com/?source=post_page-----2dddc09a404a--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----2dddc09a404a--------------------------------)
    [Skanda Vivek](https://skanda-vivek.medium.com/?source=post_page-----2dddc09a404a--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F220d9bbb8014&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhen-should-you-fine-tune-llms-2dddc09a404a&user=Skanda+Vivek&userId=220d9bbb8014&source=post_page-220d9bbb8014----2dddc09a404a---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----2dddc09a404a--------------------------------)
    ·7 min read·May 15, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F2dddc09a404a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhen-should-you-fine-tune-llms-2dddc09a404a&user=Skanda+Vivek&userId=220d9bbb8014&source=-----2dddc09a404a---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F220d9bbb8014&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhen-should-you-fine-tune-llms-2dddc09a404a&user=Skanda+Vivek&userId=220d9bbb8014&source=post_page-220d9bbb8014----2dddc09a404a---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----2dddc09a404a--------------------------------)
    ·7 min read·2023年5月15日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F2dddc09a404a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhen-should-you-fine-tune-llms-2dddc09a404a&user=Skanda+Vivek&userId=220d9bbb8014&source=-----2dddc09a404a---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F2dddc09a404a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhen-should-you-fine-tune-llms-2dddc09a404a&source=-----2dddc09a404a---------------------bookmark_footer-----------)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F2dddc09a404a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhen-should-you-fine-tune-llms-2dddc09a404a&source=-----2dddc09a404a---------------------bookmark_footer-----------)'
- en: I get this question a lot — from folks on LinkedIn asking me questions on how
    to fine-tune open source models like LLaMA, companies trying to figure out the
    business case for selling LLM hosting and deployment solutions, and companies
    trying to capitalize on AI and LLMs applied to their products. But when I ask
    them why they don’t want to use a close-sourced model like ChatGPT — they don’t
    really have an answer. So I decided to write this article as someone who applies
    LLMs to solve business problems every day.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 我经常收到这个问题——无论是在 LinkedIn 上被人询问如何微调开源模型如 LLaMA，还是公司在尝试弄清楚出售 LLM 托管和部署解决方案的商业案例，或者公司希望利用
    AI 和 LLM 应用于其产品。但当我问他们为什么不使用像 ChatGPT 这样的闭源模型时，他们并没有真正的答案。所以我决定以每天应用 LLM 解决业务问题的身份写这篇文章。
- en: The Case For Closed APIs
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 对闭源 API 的论证
- en: Have you tried implementing the ChatGPT API for your use case? Maybe you want
    to summarize documents or answer questions, or just want a chatbot on your website.
    More often than not, you will find that ChatGPT does a pretty good job at multiple
    language tasks.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 你是否尝试过为你的用例实现 ChatGPT API？也许你想要总结文档或回答问题，或者只是想在你的网站上添加一个聊天机器人。更多时候，你会发现 ChatGPT
    在多语言任务上表现得相当出色。
- en: A common perception is that these models are too expensive. But at $0.002/1K
    tokens, I bet you could at least try this out on a few 100 samples and evaluate
    whether LLMs are the way to go or not for your particular application. In fact,
    at thousands of API calls per day or around that range, ChatGPT API works out
    much cheaper than hosting infrastructure for custom open-source models [as I have
    written about in this blog](/llm-economics-chatgpt-vs-open-source-dfc29f69fec1).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 一个常见的看法是这些模型过于昂贵。但以每千个 token 0.002 美元的价格，我敢打赌你至少可以在几百个样本上试用一下，评估 LLM 是否适合你的特定应用。实际上，每天几千次
    API 调用或在这个范围内，ChatGPT API 的成本远低于托管自定义开源模型的基础设施[正如我在这篇博客中写到的](/llm-economics-chatgpt-vs-open-source-dfc29f69fec1)。
