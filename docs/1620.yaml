- en: When Should You Fine-Tune LLMs?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/when-should-you-fine-tune-llms-2dddc09a404a?source=collection_archive---------1-----------------------#2023-05-15](https://towardsdatascience.com/when-should-you-fine-tune-llms-2dddc09a404a?source=collection_archive---------1-----------------------#2023-05-15)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: There has been a flurry of exciting open-source LLMs which can be fine-tuned.
    But how does that compare to just using a closed API?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://skanda-vivek.medium.com/?source=post_page-----2dddc09a404a--------------------------------)[![Skanda
    Vivek](../Images/9d25bee2fb75176ca7f7ea6eff7d7ab5.png)](https://skanda-vivek.medium.com/?source=post_page-----2dddc09a404a--------------------------------)[](https://towardsdatascience.com/?source=post_page-----2dddc09a404a--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----2dddc09a404a--------------------------------)
    [Skanda Vivek](https://skanda-vivek.medium.com/?source=post_page-----2dddc09a404a--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F220d9bbb8014&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhen-should-you-fine-tune-llms-2dddc09a404a&user=Skanda+Vivek&userId=220d9bbb8014&source=post_page-220d9bbb8014----2dddc09a404a---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----2dddc09a404a--------------------------------)
    ·7 min read·May 15, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F2dddc09a404a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhen-should-you-fine-tune-llms-2dddc09a404a&user=Skanda+Vivek&userId=220d9bbb8014&source=-----2dddc09a404a---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F2dddc09a404a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhen-should-you-fine-tune-llms-2dddc09a404a&source=-----2dddc09a404a---------------------bookmark_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: I get this question a lot — from folks on LinkedIn asking me questions on how
    to fine-tune open source models like LLaMA, companies trying to figure out the
    business case for selling LLM hosting and deployment solutions, and companies
    trying to capitalize on AI and LLMs applied to their products. But when I ask
    them why they don’t want to use a close-sourced model like ChatGPT — they don’t
    really have an answer. So I decided to write this article as someone who applies
    LLMs to solve business problems every day.
  prefs: []
  type: TYPE_NORMAL
- en: The Case For Closed APIs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Have you tried implementing the ChatGPT API for your use case? Maybe you want
    to summarize documents or answer questions, or just want a chatbot on your website.
    More often than not, you will find that ChatGPT does a pretty good job at multiple
    language tasks.
  prefs: []
  type: TYPE_NORMAL
- en: A common perception is that these models are too expensive. But at $0.002/1K
    tokens, I bet you could at least try this out on a few 100 samples and evaluate
    whether LLMs are the way to go or not for your particular application. In fact,
    at thousands of API calls per day or around that range, ChatGPT API works out
    much cheaper than hosting infrastructure for custom open-source models [as I have
    written about in this blog](/llm-economics-chatgpt-vs-open-source-dfc29f69fec1).
  prefs: []
  type: TYPE_NORMAL
