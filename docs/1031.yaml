- en: Four Approaches to build on top of Generative AI Foundational Models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/four-approaches-to-build-on-top-of-generative-ai-foundational-models-43c1a64cffd5?source=collection_archive---------1-----------------------#2023-03-21](https://towardsdatascience.com/four-approaches-to-build-on-top-of-generative-ai-foundational-models-43c1a64cffd5?source=collection_archive---------1-----------------------#2023-03-21)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: What works, the pros and cons, and example code for each approach
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://lakshmanok.medium.com/?source=post_page-----43c1a64cffd5--------------------------------)[![Lak
    Lakshmanan](../Images/9faaaf72d600f592cbaf3e9089cbb913.png)](https://lakshmanok.medium.com/?source=post_page-----43c1a64cffd5--------------------------------)[](https://towardsdatascience.com/?source=post_page-----43c1a64cffd5--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----43c1a64cffd5--------------------------------)
    [Lak Lakshmanan](https://lakshmanok.medium.com/?source=post_page-----43c1a64cffd5--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F247b0630b5d6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffour-approaches-to-build-on-top-of-generative-ai-foundational-models-43c1a64cffd5&user=Lak+Lakshmanan&userId=247b0630b5d6&source=post_page-247b0630b5d6----43c1a64cffd5---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----43c1a64cffd5--------------------------------)
    ·11 min read·Mar 21, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F43c1a64cffd5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffour-approaches-to-build-on-top-of-generative-ai-foundational-models-43c1a64cffd5&user=Lak+Lakshmanan&userId=247b0630b5d6&source=-----43c1a64cffd5---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F43c1a64cffd5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffour-approaches-to-build-on-top-of-generative-ai-foundational-models-43c1a64cffd5&source=-----43c1a64cffd5---------------------bookmark_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: '*If some of the terminology I use here is unfamiliar, I encourage you to read
    my* [*earlier article on LLMs*](https://becominghuman.ai/why-large-language-models-like-chatgpt-are-bullshit-artists-c4d5bb850852)
    *first.*'
  prefs: []
  type: TYPE_NORMAL
- en: There *are* teams that are employing ChatGPT or its competitors (Anthropic,
    Google’s Flan T5 or PaLM, Meta’s LLaMA, Cohere, AI21Labs, etc.) for real rather
    for cutesy demos. Unfortunately, informative content about how they are doing
    so is lost amidst marketing hype and technical jargon. Therefore, I see folks
    who are getting started with generative AI take approaches that experts in the
    field will tell you are not going to pan out. This article is my attempt at organizing
    this space and showing you what’s working.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ab78a68eafba35e68323a1f2c22b6a4f.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Sen](https://unsplash.com/es/@sen7?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    on [Unsplash](https://unsplash.com/s/photos/lego?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
  prefs: []
  type: TYPE_NORMAL
- en: The bar to clear
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The problem with many of the cutesy demos and hype-filled posts about generative
    AI is that they [hit the training dataset](https://aisnakeoil.substack.com/p/gpt-4-and-professional-benchmarks)
    — they don’t really tell you how well it will work when applied to the chaos of
    real human users and actually novel input. Typical software is expected to work
    at 99%+ reliability —for example, it was only when speech recognition crossed
    this accuracy bar on phrases that the market for Voice AI took off. Same for automated
    captioning, translation, etc.
  prefs: []
  type: TYPE_NORMAL
- en: 'I see two ways in which teams are addressing this issue in their production
    systems:'
  prefs: []
  type: TYPE_NORMAL
- en: Human users are more forgiving if the UX is in a situation where they already
    expect to correct errors (this seems to be what helps GitHub Copilot) or where
    it is positioned as being interactive and helpful but not ready to use (ChatGPT,
    Bing Chat, etc.)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fully automated applications of generative AI are mostly in the trusted-tester
    stage today, and the jury is out on whether these applications are actually able
    to clear this bar. That said, the results are promising and trending upwards,
    and it’s likely only a matter of time before the bar’s met.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Personally, I have been experimenting with GPT 3.5 Turbo and Google Flan-T5
    with specific production use cases in mind, and learning quite a bit about what
    works and what doesn’t. None of *my* models have crossed the 99% bar. I also haven’t
    yet gotten access to GPT-4 or to Google’s PaLM API at the time of writing (March
    2023). I’m basing this article on my experiments, on published research, and on
    publicly announced projects.
  prefs: []
  type: TYPE_NORMAL
- en: With all uses of generative AI, it is helpful to firmly keep in mind that the
    pretrained models are trained on internet content and can be biased in multiple
    ways. Safeguard against those biases in your application layer.
  prefs: []
  type: TYPE_NORMAL
- en: 'Approach 1: Use the API Directly'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The first approach is the simplest because many users encountered GPT through
    the interactive interface offered by ChatGPT. It seems very intuitive to try out
    various prompts until you get one that generates the output you want. This is
    why you have a lot of LinkedIn influencers publishing ChatGPT prompts that [work
    for sales emails](https://www.linkedin.com/posts/mkosoglow_i-see-sdrs-aes-csms-and-execs-consistently-activity-7024033440933007360-CCmn)
    or whatever.
  prefs: []
  type: TYPE_NORMAL
- en: 'When it comes to automating this workflow, the natural method is to use the
    [REST API endpoint](https://platform.openai.com/docs/api-reference/completions/create)
    of the service and directly invoke it with the final, working prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'However, this approach does not lend itself to operationalization. There are
    several reasons:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Brittleness**. The underlying models keep improving. Sudden changes in the
    deployed models [broke](https://twitter.com/rishdotblog/status/1626273042472271872)
    many production workloads, and people learned from that experience. ML workloads
    are brittle enough already; adding additional points of failure in the form of
    prompts that are fine-tuned to specific models is not wise.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Injection**. It is rare that the instruction and input are plain strings
    as in the example above. Most often, they include variables that are input from
    users. These variables have to be incorporated into the prompts and inputs. And
    as any programmer knows, injection by string concatenation is rife with security
    problems. You put yourself at the mercy of the guardrails placed around the Generative
    AI API when you do this. As when guarding against SQL injection, it’s better to
    use an API that handles variable injection for you.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Multiple prompts**. It is rare that you will be able to get a prompt to work
    in one-shot. More common is to send multiple prompts to the model, and get the
    model to modify its output based on these prompts. These prompts themselves may
    have some human input (such as follow-up inputs) embedded in the workflow. Also
    common is for the prompts to provide a few examples of the desired output (called
    few-shot learning).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A way to resolve all three of these problems is to use langchain.
  prefs: []
  type: TYPE_NORMAL
- en: 'Approach 2: Use langchain'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Langchain is rapidly becoming the library of choice that allows you to invoke
    LLMs from different vendors, handle variable injection, and do few-shot training.
    [Here’s an example of using langchain](https://langchain.readthedocs.io/en/latest/modules/prompts/examples/few_shot_examples.html):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: I strongly recommend using langchain vs. using a vendor’s API directly. Then,
    make sure that everything you do works with at least two APIs or use a LLM checkpoint
    that will not change under you. Either of these approaches will avoid your prompts/code
    being brittle to changes in the underlying LLM. (Here, I’m using API to mean a
    managed LLM endpoint).
  prefs: []
  type: TYPE_NORMAL
- en: Langchain today [supports APIs](https://langchain.readthedocs.io/en/latest/reference/integrations.html)
    from Open AI, Cohere, HuggingFace Hub (and hence Google Flan-T5), etc. and [LLMs
    from](https://langchain.readthedocs.io/en/latest/reference/modules/llms.html)
    AI21, Anthropic, Open AI, HuggingFace Hub, etc.
  prefs: []
  type: TYPE_NORMAL
- en: 'Approach 3: Finetune the Generative AI Chain'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This is the leading-edge approach in that it’s the one I see used by most of
    the sophisticated production applications of generative AI. As just an example
    (no endorsement), [finetuning is how](https://techcrunch.com/2023/03/20/numbers-station-raises-17-5m-to-bring-ai-to-your-data-stack/)
    a startup consisting of Stanford PhDs is approaching standard enterprise use cases
    like SQL generation and record matching.
  prefs: []
  type: TYPE_NORMAL
- en: 'To understand the rationale behind this approach, it helps to know that there
    are four machine learning models that underpin ChatGPT (or its competitors):'
  prefs: []
  type: TYPE_NORMAL
- en: A Large Language Model (LLM) is trained to predict the next word of text given
    the previous words. It does this by learning word associations and patterns on
    a vast corpus of documents. The model is large enough that it learns these patterns
    in different contexts.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A Reinforcement Learning based on Human Feedback Model (RL-HF) is trained by
    showing humans examples of generated text, and asking them to approve text that
    is pleasing to read. The reason this is needed is that an LLM’s output is probabilistic
    — it doesn’t predict a single next word; instead, it predicts a set of words each
    of which has a certain probability of coming next. The RL-HF uses human feedback
    to learn how to choose the continuation that will generate the text that appeals
    to humans.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Instruction Model is a supervised model that is trained by showing prompts (“generate
    a sales email that proposes a demo to the engineering leadership”) and training
    the model on examples of sales emails.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Context Model is trained to carry on a conversation with the user, allowing
    them to craft the output through successive prompts.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In addition, there are guardrails (filters on both the input and output). The
    model declines to answer certain types of queries, and retracts certain answers.
    In practice, these are both machine learning models that are constantly updated.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4f76492aeae2cdeeaa19e4d0fc8f9796.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Step 2: How RL-HF works. Image from [Stiennon et al, 2020](https://proceedings.neurips.cc/paper/2020/file/1f89885d556929e98d3ef9b86448f951-Paper.pdf)'
  prefs: []
  type: TYPE_NORMAL
- en: 'There are open-source generative AI models ([Meta’s LLaMA](https://ai.facebook.com/blog/large-language-model-llama-meta-ai/),
    Google’s [Flan-T5](https://ai.googleblog.com/2021/10/introducing-flan-more-generalizable.html))
    which allow you to pick up at any of the above steps (e.g. use steps 1–2 from
    the released checkpoint, train 3 on your own data, don’t do 4). Note that LLaMA
    does not permit commercial use, and Flan-T5 is a year old (so you are compromising
    on quality). To learn where to break off, it is helpful to understand the cost/benefit
    of each stage:'
  prefs: []
  type: TYPE_NORMAL
- en: If your application uses very different jargon and words, it may be helpful
    to build a LLM from scratch on your own data (i.e., start at step 1). The problem
    is that you may not have enough data and even if you have enough data, the training
    is going to be expensive (on the order of 3–5 million dollars per training run).
    This seems to be what Salesforce has done with the [generative AI they use for
    developers](https://www.salesforce.com/news/press-releases/2023/03/07/einstein-generative-ai/).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The RL-HF model is trained to appeal to a group of testers who may not be subject-matter
    experts, or representative of your own users. If your application requires subject
    matter expertise, you may be better off starting with a LLM and branching off
    from step 2\. The dataset you need for this is much smaller — Stiennon et al 2020
    used 125k documents and presented a pair of outputs for each input document in
    each iteration (see diagram). So, you need human labelers on standby to rate about
    1 million outputs. Assuming that a labeler takes 10 min to rate each pair of documents,
    the cost is that of 250 human-months of labor per training run. I’d estimate $250k
    to $2m depending on location and skillset.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ChatGPT is trained to respond to thousands of different prompts. Your application,
    on the other hand, probably requires only one or two specific ones. It can be
    convenient to train a model such as Google Flan-T5 on your specific instruction
    and input. Such a model can be much smaller (and therefore cheaper to deploy).
    This advantage in serving costs explains why step 3 is the most common point of
    branching off. It’s possible to fine-tune Google Flan-T5 for your specific task
    with about 10k documents using [HuggingFace](https://www.philschmid.de/fine-tune-flan-t5)
    and/or [Keras](https://keras.io/examples/nlp/t5_hf_summarization/). You’d do this
    on your usual ML framework such as Databricks, Sagemaker, or Vertex AI and use
    the same services to deploy the trained model. Because Flan-T5 is a Google model,
    GCP makes training and deployment really easy by providing [pre-built containers](https://medium.com/google-cloud/finetuning-flan-t5-base-and-online-deployment-in-vertex-ai-bf099c3a4a86)
    in Vertex AI. The cost would be perhaps $50 or so.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Theoretically, it’s possible to train a different way to maintain conversational
    context. However, I haven’t seen this in practice. What most people do instead
    is to use a conversational agent framework like Dialogflow that already has a
    LLM built into it, and design a custom chatbot for their application. The infra
    costs are negligible and you don’t need any AI expertise, just domain knowledge.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'It is possible to break off at any of these stages. Limiting my examples to
    publicly published work in medicine:'
  prefs: []
  type: TYPE_NORMAL
- en: 'This [Nature article](https://www.nature.com/articles/s41746-022-00742-2) builds
    a custom 8.9-billion parameter LLM from 90 billion words extracted from medical
    records (i.e., they start from step 1). For comparison, Flan-PaLM used in #3 below
    is [540 billion parameters](https://huggingface.co/google/flan-t5-base) and the
    “small/efficient” PaLM is 62 billion parameters. Obviously, cost is a constraint
    in going much bigger on your custom language model.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This [MIT CSAIL study](https://news.mit.edu/2022/large-language-models-help-decipher-clinical-notes-1201)
    forces the model to closely hew to existing text and also doing instruction fine-tuning
    (i.e., they are starting from step 2).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Deep Mind’s MedPaLM](https://arxiv.org/abs/2212.13138) starts from an instruction-tuned
    variation of PaLM called Flan-PaLM (i.e. it starts after step 3). They report
    that 93% of healthcare professionals rated the AI as being on par with human answers.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: My advice is to choose where to break off based on how different your application
    space is from the generic internet text on which the foundational models are trained.
    Which model should you fine-tune? Currently, Google Flan T5 is the most sophisticated
    fine-tuneable model available and open for commercial use. For non-commercial
    uses, Meta’s LLaMA is the most sophisticated model available.
  prefs: []
  type: TYPE_NORMAL
- en: 'A word of caution though: when you tap into the chain using open-source models,
    the guardrail filters won’t exist, so **you will have to put in toxicity safeguards**.
    One option is to use the [detoxify](https://github.com/unitaryai/detoxify) library.
    Make sure to incorporate toxicity filtering around any API endpoint in production
    — otherwise, you’ll find yourself having to [take it back down](https://www.theregister.com/2023/03/21/stanford_ai_alpaca_taken_offline/).
    API gateways can be a convenient way to ensure that you are doing this for all
    your ML model endpoints.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Approach 4: Simplify the problem'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There are smart approaches to reframe the problem you are solving in such as
    way that you can use a Generative AI model (as in Approach 3) but avoid problems
    with hallucination, etc.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, suppose you want to do question-answering. You could start with
    a powerful LLM and then struggle to “tame” the wild beast to have it not hallucinate.
    A much simpler approach is to reframe the problem. Change the model from one that
    predicts the output text to a model that has three outputs: the URL of a document,
    the starting position within that document, and the length of text. That is what
    Google Search is doing here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e0ebbc9c4057ace3341e972199a34c60.png)'
  prefs: []
  type: TYPE_IMG
- en: Google’s Q&A model predicts a URL, starting position, and length of text. This
    avoids problems with hallucination.
  prefs: []
  type: TYPE_NORMAL
- en: At worst, the model will show you irrelevant text. What it will not do is to
    hallucinate because you don’t allow it to actually predict text.
  prefs: []
  type: TYPE_NORMAL
- en: 'A [Keras sample that follows](https://keras.io/examples/nlp/question_answering/)
    this approach tokenizes the inputs and context (the document that you are finding
    the answer within):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'and then passes the tokens into a Keras regression model whose first layer
    is the Transformer model that takes in these tokens and that outputs the position
    of the answer within the “context” text:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'During inference, you get the predicted locations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: You will note that the sample does not predict the URL — the context is assumed
    to be the result of a typical search query (such as returned by a matching engine
    or vector database), and the sample model only does extraction. However, you can
    build the search also into the model by having a separate layer in Keras.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There are four approaches that I see being used to build production applications
    on top of generative AI foundational models:'
  prefs: []
  type: TYPE_NORMAL
- en: Use the REST API of an all-in model such as GPT-4 for one-shot prompts.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use langchain to abstract away the LLM, input injection, multi-turn conversations,
    and few-shot learning.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finetune on your custom data by tapping into the set of models that comprise
    an end-to-end generative AI model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Reframe the problem into a form that avoids the dangers of generative AI (bias,
    toxicity, hallucination).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Approach #3 is what I see most commonly used by sophisticated teams.'
  prefs: []
  type: TYPE_NORMAL
