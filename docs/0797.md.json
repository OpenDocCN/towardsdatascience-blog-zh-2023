["```py\nclass WaterDataset(Dataset):\n    def __init__(self, X, y):\n\n        # The __init__ method is run once when instantiating the Dataset object\n        self.X = torch.tensor(X)\n        self.y = torch.tensor(np.array(y).astype(float))\n\n    def __len__(self):\n\n        # The __len__ method returns the number of samples in our dataset.\n        return len(self.y)\n\n    def __getitem__(self, idx):\n\n        # The __getitem__ method loads and returns a sample from the dataset\n        # at the given index idx.\n        return self.X[idx], self.y[idx]\n```", "```py\ntrain_dataset = WaterDataset(X_train, y_train)\nval_dataset = WaterDataset(X_val, y_val)\ntest_dataset = WaterDataset(X_test, y_test)\n```", "```py\ntrain_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\nval_dataloader = DataLoader(val_dataset, batch_size=32)\ntest_dataloader = DataLoader(test_dataset, batch_size=32)\n```", "```py\nclass WaterNet(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n        # define 4 linear layers\n        # the first input dimension is the number of features\n        # the output layer is 1 \n        # the other in and output parameters are hyperparamters and can be changed \n        self.fc1 = nn.Linear(in_features=9, out_features=64)\n        self.fc2 = nn.Linear(in_features=64, out_features=32)\n        self.fc3 = nn.Linear(in_features=32, out_features=16)\n        self.fc4 = nn.Linear(in_features=16, out_features=1)\n        self.relu = nn.ReLU()\n        self.dropout = nn.Dropout()\n\n    def forward(self, x):\n\n        # apply the linear layers with a relu activation\n        x = self.fc1(x)\n        x = self.relu(x)\n        x = self.dropout(x)\n        x = self.fc2(x)\n        x = self.relu(x)\n        x = self.fc3(x)\n        x = self.relu(x)\n        x = self.fc4(x)\n\n        return x.squeeze()\n```", "```py\nfor batch in train_dataloader:\n  # apply model\n  y_hat = model(x)\n  # calculate loss\n  loss = loss_function(y_hat, y)\n  # backpropagation\n  loss.backward\n  # update weights\n  optimizer.step()\n```", "```py\ndef train(model, device, train_dataloader, optimizer, criterion, epoch, print_every):\n    '''\n    parameters:\n    model - the model used for training\n    device - the device we work on (cpu or gpu)\n    train_dataloader - the training data wrapped in a dataloader\n    optimizer - the optimizer used for optimizing the parameters\n    criterion - loss function\n    epoch - current epoch\n    print_every - integer after how many batches results should be printed\n    '''\n\n    # create empty list to store the train losses\n    train_loss = []\n    # variable to count correct classified samples\n    correct = 0\n    # variable to count true positive, false positive and false negative samples\n    TP = 0\n    FP = 0\n    FN = 0\n    # create an empty list to store the predictions\n    predictions = []\n\n    # set model to training mode, i.e. the model is used for training\n    # this effects layers like BatchNorm() and Dropout\n    # in our simple example we don't use these layers\n    # for the sake of completeness 'model.train()' is included \n    model.train()\n\n    # loop over batches\n    for batch_idx, (x, y) in enumerate(train_dataloader):\n\n        # set data to device\n        x, y = x.to(device), y.to(device)\n\n        # set optimizer to zero\n        optimizer.zero_grad()\n\n        # apply model \n        y_hat = model(x.float())\n\n        # calculate loss\n        loss = criterion(y_hat, y.float())\n        train_loss.append(loss.item())\n\n        # backpropagation\n        loss.backward()\n\n        # update the weights\n        optimizer.step()\n\n        # print the loss every x batches\n        if batch_idx % print_every == 0:\n            percent = 100\\. * batch_idx / len(train_dataloader)\n            print(f'Train Epoch {epoch} \\\n            [{batch_idx * len(train_dataloader)}/{len(train_dataloader.dataset)} \\\n            ({percent:.0f}%)] \\tLoss: {loss.item():.6f}')\n\n        # calculate some metrics\n\n        # to get the predictions, we need to apply the sigmoid layer\n        # this layer maps the data to the range [0,1]\n        # we set all predictions > 0.5 to 1 and the rest to 0\n        y_pred = torch.sigmoid(y_hat) > 0.5\n        predictions.append(y_pred)\n        correct += (y_pred == y).sum().item()\n        TP += torch.logical_and(y_pred == 1, y == 1).sum()\n        FP += torch.logical_and(y_pred == 1, y == 0).sum()\n        FN += torch.logical_and(y_pred == 0, y == 1).sum()\n\n    # total training loss over all batches\n    train_loss = torch.mean(torch.tensor(train_loss))\n    epoch_accuracy = correct/len(train_dataloader.dataset)\n    # recall = TP/(TP+FN)\n    epoch_recall = TP/(TP+FN)\n    # precision = TP/(TP+FP)\n    epoch_precision = TP/(TP+FP)\n\n    return epoch_accuracy, train_loss, epoch_recall, epoch_precision\n```", "```py\nfor batch in val_dataloader:\n  # apply model\n  y_hat = model(x)\n  # calculate loss\n  loss = loss_function(y_hat, y)\n```", "```py\ndef valid(model, device, val_dataloader, criterion):\n    '''\n    parameters:\n    model - the model used for training\n    device - the device we work on (cpu or gpu)\n    val_dataloader - the validation data wrapped in a dataloader\n    criterion - loss function\n    '''\n\n    # create an empty list to store the loss\n    val_loss = []\n    # variable to count correct classified samples\n    correct = 0\n    # variable to count true positive, false positive and false negative samples\n    TP = 0\n    FP = 0\n    FN = 0\n    # create an empty list to store the predictions\n    predictions = []\n\n    # set model to evaluation mode, i.e. \n    # the model is only used for inference, this has effects on\n    # dropout-layers, which are ignored in this mode and batchnorm-layers, which use running statistics\n    model.eval()\n\n    # disable gradient calculation \n    # this is useful for inference, when we are sure that we will not call Tensor.backward(). \n    # It will reduce memory consumption for computations that would otherwise have requires_grad=True.\n    with torch.no_grad():\n        # loop over batches\n        for x, y in val_dataloader:\n\n            # set data to device\n            x, y = x.to(device), y.to(device)\n\n            # apply model\n            y_hat = model(x.float())\n\n            # append current loss\n            loss = criterion(y_hat, y.float())\n            val_loss.append(loss.item())\n\n            # calculate some metrics\n\n            # to get the predictions, we need to apply the sigmoid layer\n            # this layer maps the data to the range [0,1]\n            # we set all predictions > 0.5 to 1 and the rest to 0\n            y_pred = torch.sigmoid(y_hat) > 0.5 \n            predictions.append(y_pred)\n            correct += (y_pred == y).sum().item()#y_pred.eq(y.view_as(y_pred)).sum().item()\n            TP += torch.logical_and(y_pred == 1, y == 1).sum()\n            FP += torch.logical_and(y_pred == 1, y == 0).sum()\n            FN += torch.logical_and(y_pred == 0, y == 1).sum()\n\n        # total validation loss over all batches\n        val_loss = torch.mean(torch.tensor(val_loss))\n        epoch_accuracy = correct/len(val_dataloader.dataset)\n        # recall = TP/(TP+FN)\n        epoch_recall = TP/(TP+FN)\n        # precision = TP/(TP+FP)\n        epoch_precision = TP/(TP+FP)\n\n        print(f'Validation: Average loss: {val_loss.item():.4f}, \\\n                Accuracy: {epoch_accuracy:.4f} \\\n               ({100\\. * correct/len(val_dataloader.dataset):.0f}%)')\n\n    return predictions, epoch_accuracy, val_loss, epoch_recall, epoch_precision\n```", "```py\n# hyperparamters \nbatch_size = 32\nepochs = 150\nlearning_rate = 1e-3\nprint_every = 200\n\n# set device to GPU, if available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# create datasets\ntrain_dataset = WaterDataset(X_train, y_train)\nvalid_dataset = WaterDataset(X_val, y_val)\n\n# wrap into dataloader\ntrain_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\nval_dataloader = DataLoader(valid_dataset, batch_size=batch_size)\n\n# define the model and move it to the available device\nmodel = WaterNet().to(device)\n\n# define the optimizer\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)#, weight_decay=1e-4)\n\n# define loss\ncriterion = nn.BCEWithLogitsLoss()\n```", "```py\n# create empty lists to store the accuracy and loss per validation epoch \ntrain_epoch_accuracy = []\ntrain_epoch_loss = []\ntrain_epoch_recall = []\ntrain_epoch_precision = []\nval_epoch_accuracy = []\nval_epoch_loss = []\nval_epoch_recall = []\nval_epoch_precision = []\n\n# loop over number of epochs\nfor epoch in range(epochs):\n\n    # train model for each epoch\n    train_accuracy, train_loss, train_recall, train_precision = \\\n        train(model, device, train_dataloader, optimizer, criterion, epoch, print_every)\n    # save training loss and accuracy for each epoch\n    train_epoch_accuracy.append(train_accuracy)\n    train_epoch_loss.append(train_loss)\n    train_epoch_recall.append(train_recall)\n    train_epoch_precision.append(train_precision)\n\n    # validate model for each epoch\n    predictions, val_accuracy, val_loss, val_recall, val_precision = \\\n        valid(model, device, val_dataloader, criterion)\n    # save validation loss and accuracy for each epoch\n    val_epoch_accuracy.append(val_accuracy)\n```", "```py\ntorch.save(model.state_dict(), 'water_model_weights.pth')\n```", "```py\nmodel = WaterNet()\nmodel.load_state_dict(torch.load('water_model_weights.pth'))\nmodel.eval()\n```", "```py\n# create a dataset\ntest_dataset = WaterDataset(X_test, y_test)\n\n# wrap into dataloader\ntest_dataloader = DataLoader(test_dataset, batch_size=batch_size)\n\npredictions, test_accuracy, test_loss, test_recall, test_precision = \\\n        valid(model, device, val_dataloader, criterion)\n```"]