- en: Vision-Based Rep Counting in the Wild
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/vision-based-rep-counting-in-the-wild-cb9a4d1bdb7e?source=collection_archive---------10-----------------------#2023-02-21](https://towardsdatascience.com/vision-based-rep-counting-in-the-wild-cb9a4d1bdb7e?source=collection_archive---------10-----------------------#2023-02-21)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: A review of different approaches to vision-based rep counting
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@aakashagrawal?source=post_page-----cb9a4d1bdb7e--------------------------------)[![Aakash
    Agrawal](../Images/29c88586046f4b51d40cc0336f696cef.png)](https://medium.com/@aakashagrawal?source=post_page-----cb9a4d1bdb7e--------------------------------)[](https://towardsdatascience.com/?source=post_page-----cb9a4d1bdb7e--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----cb9a4d1bdb7e--------------------------------)
    [Aakash Agrawal](https://medium.com/@aakashagrawal?source=post_page-----cb9a4d1bdb7e--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F93ce827b6548&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fvision-based-rep-counting-in-the-wild-cb9a4d1bdb7e&user=Aakash+Agrawal&userId=93ce827b6548&source=post_page-93ce827b6548----cb9a4d1bdb7e---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----cb9a4d1bdb7e--------------------------------)
    ·9 min read·Feb 21, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fcb9a4d1bdb7e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fvision-based-rep-counting-in-the-wild-cb9a4d1bdb7e&user=Aakash+Agrawal&userId=93ce827b6548&source=-----cb9a4d1bdb7e---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fcb9a4d1bdb7e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fvision-based-rep-counting-in-the-wild-cb9a4d1bdb7e&source=-----cb9a4d1bdb7e---------------------bookmark_footer-----------)![](../Images/a982fa9483f349cd3eb00878266278f6.png)'
  prefs: []
  type: TYPE_NORMAL
- en: 'src: photo by [@paragdmehta](https://unsplash.com/@paragdmehta), illustrating
    a repetitive pattern.'
  prefs: []
  type: TYPE_NORMAL
- en: '*In this article, I try to explain my exploration of different vision-based
    repetition counting techniques and discuss their pros and cons. Specifically,
    I highlight five major ways in which computer vision has been employed for rep
    counting.*'
  prefs: []
  type: TYPE_NORMAL
- en: Wearable sensors have been quite popular for reps and set counting. Owing to
    the fact that these sensors are expensive and, in most cases, are only limited
    to tracking a particular body part, lately, a lot of focus has been on using vision-based
    approaches for rep counting.
  prefs: []
  type: TYPE_NORMAL
- en: From countless applications in activity monitoring, sports, and gaming to helping
    gain insight into the number of times a biological event (heartbeat, pulse count,
    etc.) occurs, Rep counting is a problem actively being solved in both academia
    and industry.
  prefs: []
  type: TYPE_NORMAL
- en: '**Keywords:** *Rep Counting, Computer Vision, Pose Estimation.*'
  prefs: []
  type: TYPE_NORMAL
- en: Table of Contents
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '***RepNet: Class Agnostic rep counting in the Wild***'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '***Rule-based exercise rep counting using Pose Estimation***'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '***Exercise rep counting using ideas from Signal Processing***'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '***GymCam***'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '***Rep counting using a DL-based Optical Flow Approach***'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Most of the techniques we discuss in the blog won’t be generic but rather exclusive
    to a specific problem (for example, workouts). Also, for a deeper understanding
    of the technique, please refer to the references provided.
  prefs: []
  type: TYPE_NORMAL
- en: '1\. RepNet: Class Agnostic Rep Counting in the Wild'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*paper:* [Counting Out Time: Class Agnostic Video Repetition Counting in the
    Wild](https://arxiv.org/pdf/2006.15418.pdf) [1]'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2df6017bc6e76280436c625e2a0d26ef.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig: RepNet architecture. src: [https://arxiv.org/pdf/2006.15418.pdf](https://arxiv.org/pdf/2006.15418.pdf)'
  prefs: []
  type: TYPE_NORMAL
- en: One of the most prominent works around Rep Counting has been the **RepNet**,an
    end-to-end deep learning model that can accurately predict counts on a broad range
    of repetitive movements.
  prefs: []
  type: TYPE_NORMAL
- en: 'The RepNet model takes in a video stream as input and predicts two outputs:'
  prefs: []
  type: TYPE_NORMAL
- en: '***Per-frame period length:*** For each frame that is a part of repetitive
    action, we want to know the period length (in time units) of that action.'
  prefs: []
  type: TYPE_NORMAL
- en: '***Per-frame periodicity:*** a score indicating whether the current frame is
    a part of repetition or not.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Some of the key highlights of the RepNet model include: A **Temporal Self-similarity
    Matrix (TSM):**'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7c65b7735474f0ed7e3f153b5d1e4c0d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig: RepNet leverages a Temporal Self-similarity Matrix (TSM). src: [https://arxiv.org/pdf/2006.15418.pdf](https://arxiv.org/pdf/2006.15418.pdf)'
  prefs: []
  type: TYPE_NORMAL
- en: '**TSM** is the highlight of this rep counting technique. It is the information
    bottleneck of the RepNet architecture. This matrix helps relate the frames to
    each other by computing a pairwise similarity function between two embeddings.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: One can also infer (using heuristics) the number of repetitions from these TSMs,
    which makes predictions from the RepNet model interpretable.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Diverse real-world repetition videos ensure these TSMs are quite diverse, and
    hence RepNet has a pool of applications besides just rep counting.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: One of the most impressive things about this rep counting method is that it
    is *class agnostic* (generic) and useful to a wide range of repetitive motions.
    RepNet model is a classical application of popular Transformers in Computer Vision.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/346dd57079b5be82f05679d01f38f351.png)'
  prefs: []
  type: TYPE_IMG
- en: 'src: [https://arxiv.org/pdf/2006.15418.pdf](https://arxiv.org/pdf/2006.15418.pdf)'
  prefs: []
  type: TYPE_NORMAL
- en: However, the model is constrained in the sense that the number of frames in
    the input video has to be limited. This can be attributed to the fact that the
    size of the TSMs is equal to the number of input frames.
  prefs: []
  type: TYPE_NORMAL
- en: The model is quite heavy and complex; hence deploying this on a mobile app or
    any production environment would be quite challenging and might have latency issues.
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Rule-based rep counting using Pose Estimation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*blog:* [Winning Interactivity Using Computer Vision](https://blog.cult.fit/posts/winning-interactivity-using-computer-vision)
    [2]'
  prefs: []
  type: TYPE_NORMAL
- en: This is the most common idea used in industry. A number of health and fitness
    startups have been working on building accurate, lightweight, state-of-the-art
    pose estimation models which can be used to accurately count the reps during exercise
    and provide posture correction feedback, etc.
  prefs: []
  type: TYPE_NORMAL
- en: 'Major Steps involved:'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Given a specific exercise, you first come up with definitions (rules) for states
    in that exercise. There can be multiple states in an exercise. A **squat** exercise,
    for example, can be broken into two states, say a lower state and an upper state.
    During the course of movement, the person doing exercise will shift from one state
    to the other. These state rules can be thought of as representing **activation**
    regions during movement.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'E.g., for a squat, these rules can be (*th* refers to threshold values):'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: During inference, we start by computing the metrics (angles, distances normalized)
    using pose-keypoints from the model in real-time and check whether a particular
    rule gets activated or not, and perform rep counting using the flag.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/6ef09f548a6a452264ff2de80aa44c8c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'src: [Winning Interactivity Using Computer Vision](https://blog.cult.fit/posts/winning-interactivity-using-computer-vision).
    Image by the author.'
  prefs: []
  type: TYPE_NORMAL
- en: 'One of the major upsides of the approach is that rep counting is **fast** and
    **accurate**, and **latency** is very low. However, some major downsides include
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: It is not a generic rep counting.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The pose estimation model is highly sensitive to background noise and hence
    rep counting as well.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '***Scalability Issues:*** writing rules manually is a time-intensive process.
    We also need to test the rules with different variations in angle, orientation,
    posture, etc. Imagine writing rules for 100s of exercises in the corpus.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 3\. Rep Counting using ideas from Signal Processing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*blog:* [Building an Exercise Rep Counter Using Ideas from Signal Processing](https://medium.com/towards-data-science/building-an-exercise-rep-counter-using-ideas-from-signal-processing-fcdf14e76f81)
    [3]'
  prefs: []
  type: TYPE_NORMAL
- en: '**Goal:** *Use Signal Processing ideas like zero-crossing and peak detection
    to make an exercise rep counter.*'
  prefs: []
  type: TYPE_NORMAL
- en: This approach is very similar to rule-based rep counting except for the hassle
    of manually writing the rules for different states during the rep. This approach
    ***semi-automates*** the state calculation approach by inferring a reference line
    (which can be thought of as a state boundary) for a specific movement/exercise
    using a trainer's video and then using the reference line for counting reps of
    any video of that exercise.
  prefs: []
  type: TYPE_NORMAL
- en: Here, we consider exercise as a set of waves of metrics of keypoints. These
    Metrics include angles and distances between a combination of different body keypoints,
    and the keypoints are computed using a pose estimation model (Tensorflow's [**Movenet**](https://www.tensorflow.org/hub/tutorials/movenet)
    pose estimation model).
  prefs: []
  type: TYPE_NORMAL
- en: 'Major Steps involved:'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We first compute metrics (distances and angles) between a combination of keypoints
    using a trainer reference video (as input). These metrics represent a signal temporally.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We filter out all the stationary signals and create a combined signal of the
    non-stationary ones. Then we compute the reference line using the mean of the
    summed-up signal.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: During inference, we start by again computing the metrics on the test user input
    video and compute an overall combined signal in real-time.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We create a fixed-size moving window and check for the intersection of the overall
    signal (from 3) with the reference line (from 2). This intersection gives an indication
    that the rep is complete.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](../Images/8f8451118dd7510832560f3606fe8be2.png)![](../Images/a6de37774f7ccc8281b538bebb04c207.png)'
  prefs: []
  type: TYPE_IMG
- en: Results using the idea of zero-crossing. Image by the Author. [src](/building-an-exercise-rep-counter-using-ideas-from-signal-processing-fcdf14e76f81)
  prefs: []
  type: TYPE_NORMAL
- en: 'This approach is fast, easy to implement, and fairly accurate. However, some
    major downsides include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Rep counting is ***exclusive*** and ***non-generic***.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Highly sensitive to background noise.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '***Scaling issues***: One needs to calculate the zero-crossing line using a
    reference video for any activity (also ensuring the video does not have any noise).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 4\. GymCam
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*paper:* [Detecting, recognizing, and tracking simultaneous exercises in unconstrained
    scenes](https://smashlab.io/publications/gymcam/)[4]'
  prefs: []
  type: TYPE_NORMAL
- en: GymCam is a vision-based system used for automated exercise rep counting and
    tracking. It is based on the assumption that any repetitive motion inside the
    gym is some sort of exercise. Again, here the input to the system is a video stream
    from the camera, and the output is several exercise-related metrics, including
    rep count.
  prefs: []
  type: TYPE_NORMAL
- en: Summary of the Steps Involved
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/fd2b6054cced0a51916651510e7459bc.png)'
  prefs: []
  type: TYPE_IMG
- en: 'GymCam: major steps involved. Image by the author.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Detect all potential motion trajectories in a video using a dense* [***optical
    flow***](https://docs.opencv.org/3.4/d4/dee/tutorial_optical_flow.html) *algorithm.*
    A motion trajectory might be a result of non-exercise activities, too, for example,
    warm-up, users’ gait, roaming here and there, etc.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Detect all* ***exercise*** *motion trajectories in a scene.* How do they do
    so?Firstly, they perform a feature extraction step that involves extracting handcrafted
    features from a 5-sec window of any trajectory. They use an [MLP](https://en.wikipedia.org/wiki/Multilayer_perceptron)-based
    binary classifier model, which takes in the input feature and outputs a probability
    of whether that input trajectory (feature) is an exercise-related activity or
    not.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '***Clustering*** *exercise motion trajectories in space and time.* After clustering,
    an average motion trajectory is generated by combining all trajectories belonging
    to a given cluster. Note here that the number of clusters is pre-defined. These
    average trajectories are then used for exercise rep counting and tracking.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '***Rep Counting*** *and* ***Exercise Recognition:*** Average trajectories are
    then converted into feature vectors, which are then fed to an MLP Regressor and
    an MLP Classifier model to infer rep counts and exercise labels, respectively.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](../Images/6e5d4c861207cef83b2048458baf8e30.png)'
  prefs: []
  type: TYPE_IMG
- en: Rep Counting and exercise recognition from combined trajectories. Image by the
    author.
  prefs: []
  type: TYPE_NORMAL
- en: 'Some of the noteworthy features of this system are: It is an ***end-to-end***
    system that performs rep counting in a real-world setting. ***Optical Flow***
    identifies all movements, and hence it would be sufficient to track the exercise
    and perform rep counting even if the user is **barely** visible.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Issues with this system:'
  prefs: []
  type: TYPE_NORMAL
- en: '***Multiple users overlap*** in a video while doing the exercise. And hence
    it becomes very difficult to figure out the exact boundaries of these users and
    infer the rep counts.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '***Noise Sensitive:*** noisy human behavior such as warming up, rest, user’s
    gait, etc., might exhibit periodicity and hence, can have an undesired contribution
    to the rep count.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Rep counting is not generic: the system is limited to just the exercise rep
    counting.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 5\. DL-based Optical Flow Approach
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*blog:* [Workout Movement Counting App using Deep Learning and Optical Flow
    Algorithm](/how-i-created-the-workout-movement-counting-app-using-deep-learning-and-optical-flow-89f9d2e087ac)
    [5]'
  prefs: []
  type: TYPE_NORMAL
- en: Another interesting idea employing vision to solve rep counting is the Optical
    flow approach.
  prefs: []
  type: TYPE_NORMAL
- en: Major Steps Involved
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*Find color-coded representations of video frames in a repetitive activity
    using a dense optical flow algorithm.* Here, the catch basically lies in the idea
    that different states of a repetitive movement will have different color codings.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](../Images/a46957c2b74135d6fa3782c731f082ba.png)'
  prefs: []
  type: TYPE_IMG
- en: Dense Optical flow encodes downward movement as the green color and upward movement
    as the purple color. Gif by the Author.
  prefs: []
  type: TYPE_NORMAL
- en: For details about the optical flow algorithm, please refer to the **opencv**
    doc [here](https://docs.opencv.org/3.4/d4/dee/tutorial_optical_flow.html) (along
    with the implementation).
  prefs: []
  type: TYPE_NORMAL
- en: '2\. *Dataset Creation*: Next step is to generate a dataset of color-coded images
    and videos and label them with different states of the movement (say up or down).'
  prefs: []
  type: TYPE_NORMAL
- en: 3\. *Model Training:* Next step involves training a vanilla CNN model to perform
    a multiclass classification of the frames. At test time, color-coded frames from
    optical flow are then fed to the model, which predicts one of the movement states
    and also captures the class label. This is basically a color-matching problem
    but via a model, as the model is more robust.
  prefs: []
  type: TYPE_NORMAL
- en: 'The approach is accurate and easily deployable in production. However, the
    cons easily outweigh the pros of the approach:'
  prefs: []
  type: TYPE_NORMAL
- en: Rep counting is ***exclusive*** and ***class-dependent***.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '***Scaling issues***: one needs to annotate the dataset and train a model each
    time a new exercise gets added to the corpus.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '***Orientation Sensitive:*** Same movements in different orientations will
    have different color encodings resulting in a wrong model prediction. This is
    one of the major limitations of the approach.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '***Noise Sensitive***: Any slight noise in the background would change these
    color encodings and hence the model’s prediction.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[1]. Dwibedi, Debidatta and Aytar, Yusuf and Tompson, Jonathan and Sermanet,
    Pierre and Zisserman, Andrew. Counting Out Time: Class Agnostic Video Repetition
    Counting in the Wild. IEEE/CVF Conference on Computer Vision and Pattern Recognition
    (CVPR). DOI: [https://doi.org/10.48550/arxiv.1902.09868](https://doi.org/10.48550/arxiv.1902.09868)'
  prefs: []
  type: TYPE_NORMAL
- en: '[2]. Aakash Agrawal. [Winning Interactivity Using Computer Vision](https://blog.cult.fit/posts/winning-interactivity-using-computer-vision).
    [The .fit way](https://blog.cult.fit/).'
  prefs: []
  type: TYPE_NORMAL
- en: '[3]. Aakash Agrawal. [Building an Exercise Rep Counter Using Ideas from Signal
    Processing](https://medium.com/towards-data-science/building-an-exercise-rep-counter-using-ideas-from-signal-processing-fcdf14e76f81).
    Towards Data Science.'
  prefs: []
  type: TYPE_NORMAL
- en: '[4]. Rushil Khurana, Karan Ahuja, Zac Yu, Jennifer Mankoff, Chris Harrison,
    and Mayank Goel. 2019\. GymCam: Detecting, Recognizing, and Tracking Simultaneous
    Exercises in Unconstrained Scenes. Proc. ACM Interact. Mob. Wearable Ubiquitous
    Technol. 2, 4, Article 185\. DOI: [https://doi.org/10.1145/3287063](https://doi.org/10.1145/3287063)'
  prefs: []
  type: TYPE_NORMAL
- en: '[5]. Art Kulakov. [How I created the Workout Movement Counting App using Deep
    Learning and Optical Flow Algorithm](/how-i-created-the-workout-movement-counting-app-using-deep-learning-and-optical-flow-89f9d2e087ac).
    Towards Data Science.'
  prefs: []
  type: TYPE_NORMAL
- en: I hope you enjoyed exploring a few techniques on vision-based rep counting.
    Most of the ideas are easy to implement and deploy as well. I would like to know
    the feedback of anyone reading this blog. I would be happy to answer any doubts/questions
    on any of the concepts mentioned above. Feedback is greatly welcomed. You can
    reach me via [Linkedin](https://www.linkedin.com/in/akash2016123/).
  prefs: []
  type: TYPE_NORMAL
- en: Thanks for reading!
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
