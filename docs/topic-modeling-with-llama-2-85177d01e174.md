# ä½¿ç”¨ Llama 2 è¿›è¡Œä¸»é¢˜å»ºæ¨¡

> åŸæ–‡ï¼š[`towardsdatascience.com/topic-modeling-with-llama-2-85177d01e174`](https://towardsdatascience.com/topic-modeling-with-llama-2-85177d01e174)

![](img/31f5e2196198d26e5295ae70660d720c.png)

## ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹åˆ›å»ºæ˜“äºè§£é‡Šçš„ä¸»é¢˜

[](https://medium.com/@maartengrootendorst?source=post_page-----85177d01e174--------------------------------)![Maarten Grootendorst](https://medium.com/@maartengrootendorst?source=post_page-----85177d01e174--------------------------------)[](https://towardsdatascience.com/?source=post_page-----85177d01e174--------------------------------)![Towards Data Science](https://towardsdatascience.com/?source=post_page-----85177d01e174--------------------------------) [Maarten Grootendorst](https://medium.com/@maartengrootendorst?source=post_page-----85177d01e174--------------------------------)

Â·å‘è¡¨äº[Towards Data Science](https://towardsdatascience.com/?source=post_page-----85177d01e174--------------------------------) Â·12 åˆ†é’Ÿé˜…è¯»Â·2023 å¹´ 8 æœˆ 22 æ—¥

--

éšç€**Llama 2**çš„å‡ºç°ï¼Œæœ¬åœ°è¿è¡Œå¼ºå¤§çš„ LLM å·²å˜å¾—è¶Šæ¥è¶Šç°å®ã€‚å…¶å‡†ç¡®æ€§æ¥è¿‘ OpenAI çš„ GPT-3.5ï¼Œé€‚ç”¨äºè®¸å¤šç”¨ä¾‹ã€‚

åœ¨è¿™ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬å°†æ¢è®¨å¦‚ä½•ä½¿ç”¨ Llama2 è¿›è¡Œä¸»é¢˜å»ºæ¨¡ï¼Œè€Œæ— éœ€å°†æ¯ä¸€ä¸ªæ–‡æ¡£ä¼ é€’ç»™æ¨¡å‹ã€‚ç›¸åï¼Œæˆ‘ä»¬å°†åˆ©ç”¨[**BERTopic**](https://github.com/MaartenGr/BERTopic)ï¼Œè¿™æ˜¯ä¸€ç§æ¨¡å—åŒ–çš„ä¸»é¢˜å»ºæ¨¡æŠ€æœ¯ï¼Œå¯ä»¥ä½¿ç”¨ä»»ä½• LLM æ¥å¾®è°ƒä¸»é¢˜è¡¨ç¤ºã€‚

BERTopic çš„å·¥ä½œåŸç†éå¸¸ç®€å•ã€‚å®ƒåŒ…æ‹¬ 5 ä¸ªé¡ºåºæ­¥éª¤ï¼š

1.  åµŒå…¥æ–‡æ¡£

1.  é™ä½åµŒå…¥çš„ç»´åº¦

1.  èšç±»å‡å°‘çš„åµŒå…¥

1.  æŒ‰èšç±»å¯¹æ–‡æ¡£è¿›è¡Œåˆ†è¯

1.  æå–æ¯ä¸ªèšç±»çš„æœ€ä½³ä»£è¡¨è¯

![](img/63a385b329d173ae7fc70aa9fa1b4182.png)

BERTopic çš„ 5 ä¸ªä¸»è¦æ­¥éª¤ã€‚

ç„¶è€Œï¼Œéšç€**Llama 2**ç­‰ LLM çš„å…´èµ·ï¼Œæˆ‘ä»¬å¯ä»¥åšå¾—æ¯”æ¯ä¸ªä¸»é¢˜çš„ä¸€å †ç‹¬ç«‹å•è¯æ›´å¥½ã€‚ç›´æ¥å°†æ‰€æœ‰æ–‡æ¡£ä¼ é€’ç»™ Llama 2 å¹¶è®©å…¶åˆ†ææ˜¯ä¸åˆ‡å®é™…çš„ã€‚æˆ‘ä»¬å¯ä»¥ä½¿ç”¨å‘é‡æ•°æ®åº“è¿›è¡Œæœç´¢ï¼Œä½†æˆ‘ä»¬ä¸å®Œå…¨ç¡®å®šè¦æœç´¢å“ªäº›ä¸»é¢˜ã€‚

ç›¸åï¼Œæˆ‘ä»¬å°†åˆ©ç”¨ç”± BERTopic åˆ›å»ºçš„èšç±»å’Œä¸»é¢˜ï¼Œå¹¶è®© Llama 2 å¯¹è¿™äº›ä¿¡æ¯è¿›è¡Œå¾®è°ƒå’Œæç‚¼ï¼Œä»¥è·å¾—æ›´å‡†ç¡®çš„ç»“æœã€‚

è¿™æ˜¯ä¸¤å…¨å…¶ç¾çš„æœ€ä½³æ–¹æ¡ˆï¼Œå³ BERTopic çš„ä¸»é¢˜åˆ›å»ºå’Œ Llama 2 çš„ä¸»é¢˜è¡¨ç¤ºã€‚

![](img/66743d20879491fe94a4075743d520b5.png)

Llama 2 ä½¿æˆ‘ä»¬èƒ½å¤Ÿå¾®è°ƒ BERTopic ç”Ÿæˆçš„ä¸»é¢˜è¡¨ç¤ºã€‚

ç°åœ¨ä»‹ç»éƒ¨åˆ†å·²ç»å®Œæˆï¼Œæˆ‘ä»¬å¼€å§‹å®é™…æ“ä½œæ•™ç¨‹å§ï¼

æˆ‘ä»¬å°†å¼€å§‹å®‰è£…æˆ‘ä»¬å°†åœ¨æ•´ä¸ªç¤ºä¾‹ä¸­ä½¿ç”¨çš„å‡ ä¸ªåŒ…ï¼š

```py
pip install bertopic datasets accelerate bitsandbytes xformers adjustText
```

è¯·æ³¨æ„ï¼Œæ‚¨è‡³å°‘éœ€è¦ä¸€å— T4 GPU æ‰èƒ½è¿è¡Œè¿™ä¸ªç¤ºä¾‹ï¼Œå®ƒå¯ä»¥ä¸å…è´¹çš„ Google Colab å®ä¾‹ä¸€èµ·ä½¿ç”¨ã€‚

ğŸ”¥ **æç¤º**ï¼šä½ ä¹Ÿå¯ä»¥è·Ÿéš [Google Colab Notebook](https://colab.research.google.com/drive/1QCERSMUjqGetGGujdrvv_6_EeoIcd_9M?usp=sharing) ä¸€èµ·æ“ä½œã€‚

# [ğŸ“„](https://emojipedia.org/page-facing-up) æ•°æ®

æˆ‘ä»¬å°†å¯¹å¤§é‡ ArXiv æ‘˜è¦åº”ç”¨ä¸»é¢˜å»ºæ¨¡ã€‚å®ƒä»¬æ˜¯è¿›è¡Œä¸»é¢˜å»ºæ¨¡çš„ç»ä½³æ¥æºï¼Œå› ä¸ºå®ƒä»¬åŒ…å«å„ç§å„æ ·çš„ä¸»é¢˜ï¼Œå¹¶ä¸”é€šå¸¸å†™å¾—å¾ˆå¥½ã€‚

```py
from datasets import load_dataset

dataset = load_dataset("CShorten/ML-ArXiv-Papers")["train"]

# Extract abstracts to train on and corresponding titles
abstracts = dataset["abstract"]
titles = dataset["title"]
```

ä¸ºäº†ç»™ä½ ä¸€ä¸ªæ¦‚å¿µï¼Œä¸‹é¢æ˜¯ä¸€ä¸ªæŠ½è±¡çš„æ ·å­ï¼š

```py
>>> # The abstract of "Attention Is All You Need"
>>> print(abstracts[13894])

"""
The dominant sequence transduction models are based on complex recurrent or
convolutional neural networks in an encoder-decoder configuration. The best
performing models also connect the encoder and decoder through an attention
mechanism. We propose a new simple network architecture, the Transformer, based
solely on attention mechanisms, dispensing with recurrence and convolutions
entirely. Experiments on two machine translation tasks show these models to be
superior in quality while being more parallelizable and requiring significantly
less time to train. Our model achieves 28.4 BLEU on the WMT 2014
English-to-German translation task, improving over the existing best results,
including ensembles by over 2 BLEU. On the WMT 2014 English-to-French
translation task, our model establishes a new single-model state-of-the-art
BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction
of the training costs of the best models from the literature. We show that the
Transformer generalizes well to other tasks by applying it successfully to
English constituency parsing both with large and limited training data.
"""
```

# ğŸ¤— HuggingFace Hub å‡­è¯

åœ¨æˆ‘ä»¬å¯ä»¥ä½¿ç”¨ä¸€äº›æŠ€å·§åŠ è½½ Llama2 ä¹‹å‰ï¼Œæˆ‘ä»¬é¦–å…ˆéœ€è¦æ¥å— Llama2 çš„è®¸å¯åè®®ã€‚æ­¥éª¤å¦‚ä¸‹ï¼š

+   åœ¨ [è¿™é‡Œ](https://huggingface.co/) åˆ›å»ºä¸€ä¸ª HuggingFace è´¦æˆ·

+   ç”³è¯· Llama 2 è®¿é—®æƒé™ [è¿™é‡Œ](https://huggingface.co/meta-llama/Llama-2-13b-chat-hf)

+   åœ¨ [è¿™é‡Œ](https://huggingface.co/settings/tokens) è·å–ä½ çš„ HuggingFace ä»¤ç‰Œ

å®Œæˆè¿™äº›æ­¥éª¤åï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨æˆ‘ä»¬çš„ HuggingFace å‡­è¯ç™»å½•ï¼Œä»¥ä¾¿è¿™ä¸ªç¯å¢ƒçŸ¥é“æˆ‘ä»¬æœ‰æƒé™ä¸‹è½½æˆ‘ä»¬æ„Ÿå…´è¶£çš„ Llama 2 æ¨¡å‹ã€‚

```py
from huggingface_hub import notebook_login
notebook_login()
```

![](img/475af2918fd29d5277c2630d8b6ea2ef.png)

# ğŸ¦™ Llama 2

ç°åœ¨è¿›å…¥è¿™ä¸ªæ•™ç¨‹ä¸­ä¸€ä¸ªæ›´æœ‰è¶£çš„éƒ¨åˆ†â€”â€”å¦‚ä½•åœ¨ T4-GPU ä¸ŠåŠ è½½ Llama 2 æ¨¡å‹ï¼

æˆ‘ä»¬å°†é‡ç‚¹å…³æ³¨ `'meta-llama/Llama-2-13b-chat-hf'` å˜ä½“ã€‚å®ƒè¶³å¤Ÿå¤§ï¼Œèƒ½å¤Ÿæä¾›æœ‰è¶£ä¸”æœ‰ç”¨çš„ç»“æœï¼ŒåŒæ—¶åˆè¶³å¤Ÿå°ï¼Œå¯ä»¥åœ¨æˆ‘ä»¬çš„ç¯å¢ƒä¸­è¿è¡Œã€‚

æˆ‘ä»¬é¦–å…ˆå®šä¹‰æˆ‘ä»¬çš„æ¨¡å‹å¹¶ç¡®è®¤ GPU æ˜¯å¦æ­£ç¡®é€‰æ‹©ã€‚æˆ‘ä»¬æœŸæœ› `device` çš„è¾“å‡ºæ˜¾ç¤ºä¸º Cuda è®¾å¤‡ï¼š

```py
from torch import cuda

model_id = 'meta-llama/Llama-2-13b-chat-hf'
device = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'; print(device)
```

## ä¼˜åŒ–ä¸é‡åŒ–

ä¸ºäº†åŠ è½½æˆ‘ä»¬ 130 äº¿å‚æ•°çš„æ¨¡å‹ï¼Œæˆ‘ä»¬éœ€è¦æ‰§è¡Œä¸€äº›ä¼˜åŒ–æŠ€å·§ã€‚ç”±äºæˆ‘ä»¬æ‹¥æœ‰çš„ VRAM æœ‰é™ä¸”æ²¡æœ‰ A100 GPUï¼Œæˆ‘ä»¬éœ€è¦å¯¹æ¨¡å‹è¿›è¡Œä¸€äº›â€œå‹ç¼©â€ï¼Œä»¥ä¾¿æˆ‘ä»¬å¯ä»¥è¿è¡Œå®ƒã€‚

æˆ‘ä»¬å¯ä»¥ä½¿ç”¨ä¸€äº›æŠ€å·§ï¼Œä½†ä¸»è¦åŸåˆ™æ˜¯ 4 ä½é‡åŒ–ã€‚

è¿™ä¸ªè¿‡ç¨‹å°† 64 ä½è¡¨ç¤ºå‡å°‘åˆ°ä»… 4 ä½ï¼Œä»è€Œå‡å°‘æˆ‘ä»¬éœ€è¦çš„ GPU å†…å­˜ã€‚è¿™æ˜¯ä¸€ç§æœ€è¿‘çš„æŠ€æœ¯ï¼Œä¸”åœ¨é«˜æ•ˆ LLM åŠ è½½å’Œä½¿ç”¨æ–¹é¢ç›¸å½“ä¼˜é›…ã€‚ä½ å¯ä»¥åœ¨ QLoRA è®ºæ–‡ [è¿™é‡Œ](https://arxiv.org/pdf/2305.14314.pdf) å’Œä»¤äººæƒŠå¹çš„ HuggingFace åšå®¢ [è¿™é‡Œ](https://huggingface.co/blog/4bit-transformers-bitsandbytes) ä¸­äº†è§£æ›´å¤šå…³äºè¯¥æ–¹æ³•çš„å†…å®¹ã€‚

```py
from torch import bfloat16
import transformers

# Quantization to load an LLM with less GPU memory
bnb_config = transformers.BitsAndBytesConfig(
    load_in_4bit=True,  # 4-bit quantization
    bnb_4bit_quant_type='nf4',  # Normalized float 4
    bnb_4bit_use_double_quant=True,  # Second quantization after the first
    bnb_4bit_compute_dtype=bfloat16  # Computation type
)
```

æˆ‘ä»¬åˆšåˆšè¿è¡Œçš„è¿™å››ä¸ªå‚æ•°éå¸¸é‡è¦ï¼Œå¹¶å°†è®¸å¤š LLM åº”ç”¨å¸¦ç»™æ¶ˆè´¹è€…ï¼š

+   `**load_in_4bit**` å…è®¸æˆ‘ä»¬ä»¥ 4 ä½ç²¾åº¦åŠ è½½æ¨¡å‹ï¼Œç›¸æ¯”äºåŸå§‹çš„ 32 ä½ç²¾åº¦ï¼Œè¿™å¤§å¤§åŠ å¿«äº†é€Ÿåº¦å¹¶å‡å°‘äº†å†…å­˜ï¼

+   `**bnb_4bit_quant_type**` è¿™æ˜¯ 4 ä½ç²¾åº¦çš„ç±»å‹ã€‚è®ºæ–‡æ¨èä½¿ç”¨æ ‡å‡†åŒ–çš„æµ®ç‚¹ 4 ä½ç²¾åº¦ï¼Œæ‰€ä»¥è¿™å°±æ˜¯æˆ‘ä»¬è¦ä½¿ç”¨çš„ï¼

+   `**bnb_4bit_use_double_quant**` è¿™æ˜¯ä¸€ä¸ªå·§å¦™çš„æŠ€å·§ï¼Œå› ä¸ºå®ƒåœ¨ç¬¬ä¸€æ¬¡é‡åŒ–åæ‰§è¡Œç¬¬äºŒæ¬¡é‡åŒ–ï¼Œè¿›ä¸€æ­¥å‡å°‘æ‰€éœ€çš„ä½æ•°ã€‚

+   `**bnb_4bit_compute_dtype**` è®¡ç®—è¿‡ç¨‹ä¸­ä½¿ç”¨çš„è®¡ç®—ç±»å‹ï¼Œè¿™è¿›ä¸€æ­¥åŠ é€Ÿäº†æ¨¡å‹ã€‚

ä½¿ç”¨è¿™ä¸ªé…ç½®ï¼Œæˆ‘ä»¬å¯ä»¥å¼€å§‹åŠ è½½æ¨¡å‹ä»¥åŠåˆ†è¯å™¨ï¼š

```py
# Llama 2 Tokenizer
tokenizer = transformers.AutoTokenizer.from_pretrained(model_id)

# Llama 2 Model
model = transformers.AutoModelForCausalLM.from_pretrained(
    model_id,
    trust_remote_code=True,
    quantization_config=bnb_config,
    device_map='auto',
)
model.eval()
```

ä½¿ç”¨æ¨¡å‹å’Œåˆ†è¯å™¨ï¼Œæˆ‘ä»¬å°†ç”Ÿæˆä¸€ä¸ª HuggingFace transformers ç®¡é“ï¼Œè¿™æ ·æˆ‘ä»¬å°±å¯ä»¥è½»æ¾ç”Ÿæˆæ–°æ–‡æœ¬ï¼š

```py
# Our text generator
generator = transformers.pipeline(
    model=model, tokenizer=tokenizer,
    task='text-generation',
    temperature=0.1,
    max_new_tokens=500,
    repetition_penalty=1.1
)
```

## æç¤ºå·¥ç¨‹

ä¸ºäº†æ£€æŸ¥æˆ‘ä»¬çš„æ¨¡å‹æ˜¯å¦æ­£ç¡®åŠ è½½ï¼Œè®©æˆ‘ä»¬å°è¯•å‡ ä¸ªæç¤ºã€‚

```py
>>> prompt = "Could you explain to me how 4-bit quantization works as if I am 5?"
>>> res = generator(prompt)
>>> print(res[0]["generated_text"])

"""
Could you explain to me how 4-bit quantization works as if I am 5?
Sure! Imagine you have a big box of crayons. Each crayon represents a different color, like red, blue, green, and so on. Now, imagine that instead of using all the different colors, we only want to use four colors: red, blue, green, and yellow. We can do this by taking all the colors and dividing them into four groups.
Group 1: Red, Blue, Green
Group 2: Yellow
Now, when we want to draw something, we can only choose one color from each group. So, if we want to draw a red apple, we would choose the red crayon from Group 1 and the yellow crayon from Group 2\. This way, we can only use four colors, but we can still make lots of different things!
In the same way, when we use 4-bit quantization, we take all the numbers and divide them into four groups. Each group has a different number of bits, just like our crayon groups had different colors. Then, when we want to represent a number, we can only choose one number from each group. This way, we can represent lots of different numbers using only four bits!
"""
```

è™½ç„¶æˆ‘ä»¬å¯ä»¥ç›´æ¥æç¤ºæ¨¡å‹ï¼Œä½†å®é™…ä¸Šæˆ‘ä»¬éœ€è¦éµå¾ªä¸€ä¸ªæ¨¡æ¿ã€‚æ¨¡æ¿å¦‚ä¸‹æ‰€ç¤ºï¼š

```py
"""
<s>[INST] <<SYS>>

{{ System Prompt }}
<</SYS>>
{{ User Prompt }}
 [/INST]
{{ Model Answer }}
"""
```

è¿™ä¸ªæ¨¡æ¿ç”±ä¸¤ä¸ªä¸»è¦ç»„ä»¶ç»„æˆï¼Œå³`{{ System Prompt }}`å’Œ`{{ User Prompt }}`ï¼š

+   `{{ System Prompt }}`å¸®åŠ©æˆ‘ä»¬åœ¨å¯¹è¯è¿‡ç¨‹ä¸­æŒ‡å¯¼æ¨¡å‹ã€‚ä¾‹å¦‚ï¼Œæˆ‘ä»¬å¯ä»¥è¯´å®ƒæ˜¯ä¸€ä¸ªä¸“é—¨è´Ÿè´£æ ‡è®°ä¸»é¢˜çš„æœ‰ç”¨åŠ©æ‰‹ã€‚

+   `{{ User Prompt }}`æ˜¯æˆ‘ä»¬å‘å®ƒæé—®çš„åœ°æ–¹ã€‚

ä½ å¯èƒ½æ³¨æ„åˆ°`[INST]`æ ‡ç­¾ï¼Œè¿™äº›æ ‡ç­¾ç”¨äºæ ‡è¯†æç¤ºçš„å¼€å§‹å’Œç»“æŸã€‚æˆ‘ä»¬å¯ä»¥ä½¿ç”¨è¿™äº›æ ‡ç­¾æ¥å»ºæ¨¡å¯¹è¯å†å²ï¼Œç¨åæˆ‘ä»¬ä¼šæ›´æ·±å…¥åœ°äº†è§£ã€‚

æ¥ä¸‹æ¥ï¼Œè®©æˆ‘ä»¬çœ‹çœ‹å¦‚ä½•ä½¿ç”¨è¿™ä¸ªæ¨¡æ¿æ¥ä¼˜åŒ– Llama 2 è¿›è¡Œä¸»é¢˜å»ºæ¨¡ã€‚

## æç¤ºæ¨¡æ¿

æˆ‘ä»¬å°†ä¿æŒ`system prompt`ç®€å•æ˜äº†ï¼š

```py
# System prompt describes information given to all conversations
system_prompt = """
<s>[INST] <<SYS>>
You are a helpful, respectful and honest assistant for labeling topics.
<</SYS>>
"""
```

æˆ‘ä»¬å°†å‘Šè¯‰æ¨¡å‹ï¼Œå®ƒåªæ˜¯ä¸€ä¸ªæœ‰ç”¨çš„åŠ©æ‰‹ï¼Œè´Ÿè´£æ ‡è®°ä¸»é¢˜ï¼Œå› ä¸ºè¿™æ˜¯æˆ‘ä»¬çš„ä¸»è¦ç›®æ ‡ã€‚

ç›¸æ¯”ä¹‹ä¸‹ï¼Œæˆ‘ä»¬çš„`user prompt`ä¼šç¨å¾®å¤æ‚ä¸€äº›ã€‚å®ƒå°†åŒ…å«ä¸¤ä¸ªéƒ¨åˆ†ï¼Œä¸€ä¸ª**ç¤ºä¾‹** **æç¤º**å’Œ**ä¸»è¦æç¤º**ã€‚

è®©æˆ‘ä»¬ä»**ç¤ºä¾‹æç¤º**å¼€å§‹ã€‚å¦‚æœä½ ç»™å¤§å¤šæ•° LLM ä¸€ä¸ªç¤ºä¾‹ï¼Œå®ƒä»¬é€šå¸¸èƒ½æ›´å¥½åœ°ç”Ÿæˆå‡†ç¡®çš„å“åº”ã€‚æˆ‘ä»¬å°†å±•ç¤ºä¸€ä¸ªå‡†ç¡®çš„ç¤ºä¾‹ï¼Œè¯´æ˜æˆ‘ä»¬æœŸæœ›çš„è¾“å‡ºç±»å‹ã€‚

```py
# Example prompt demonstrating the output we are looking for
example_prompt = """
I have a topic that contains the following documents:
- Traditional diets in most cultures were primarily plant-based with a little meat on top, but with the rise of industrial style meat production and factory farming, meat has become a staple food.
- Meat, but especially beef, is the word food in terms of emissions.
- Eating meat doesn't make you a bad person, not eating meat doesn't make you a good one.

The topic is described by the following keywords: 'meat, beef, eat, eating, emissions, steak, food, health, processed, chicken'.

Based on the information about the topic above, please create a short label of this topic. Make sure you to only return the label and nothing more.
[/INST] Environmental impacts of eating meat
"""
```

è¿™ä¸ªä¾‹å­åŸºäºä¸€ç³»åˆ—å…³é”®å­—å’Œä¸»è¦å…³äºè‚‰ç±»å½±å“çš„æ–‡æ¡£ï¼Œå¸®åŠ©æ¨¡å‹ç†è§£å®ƒåº”è¯¥ç»™å‡ºä»€ä¹ˆæ ·çš„è¾“å‡ºã€‚æˆ‘ä»¬å‘æ¨¡å‹å±•ç¤ºäº†æˆ‘ä»¬åªæœŸå¾…æ ‡ç­¾ï¼Œè¿™æ ·æˆ‘ä»¬æ›´å®¹æ˜“æå–ã€‚

æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°†åˆ›å»ºä¸€ä¸ªå¯ä»¥åœ¨ BERTopic ä¸­ä½¿ç”¨çš„æ¨¡æ¿ï¼š

```py
# Our main prompt with documents ([DOCUMENTS]) and keywords ([KEYWORDS]) tags
main_prompt = """
[INST]
I have a topic that contains the following documents:
[DOCUMENTS]

The topic is described by the following keywords: '[KEYWORDS]'.

Based on the information about the topic above, please create a short label of this topic. Make sure you to only return the label and nothing more.
[/INST]
"""
```

æœ‰ä¸¤ä¸ª BERTopic ç‰¹å®šçš„æ ‡ç­¾å€¼å¾—å…³æ³¨ï¼Œå³`[DOCUMENTS]`å’Œ`[KEYWORDS]`ï¼š

+   `[DOCUMENTS]`åŒ…å«ä¸ä¸»é¢˜æœ€ç›¸å…³çš„å‰ 5 ä¸ªæ–‡æ¡£

+   `[KEYWORDS]`åŒ…å«é€šè¿‡ c-TF-IDF ç”Ÿæˆçš„ä¸ä¸»é¢˜æœ€ç›¸å…³çš„å‰ 10 ä¸ªå…³é”®å­—

è¿™ä¸ªæ¨¡æ¿å°†æ ¹æ®æ¯ä¸ªä¸»é¢˜è¿›è¡Œå¡«å†™ã€‚æœ€åï¼Œæˆ‘ä»¬å¯ä»¥å°†å…¶åˆå¹¶ä¸ºæˆ‘ä»¬çš„æœ€ç»ˆæç¤ºï¼š

```py
prompt = system_prompt + example_prompt + main_prompt
```

# ğŸ—¨ï¸ BERTopic

åœ¨å¼€å§‹ä¸»é¢˜å»ºæ¨¡ä¹‹å‰ï¼Œæˆ‘ä»¬éœ€è¦å…ˆæ‰§è¡Œä¸¤ä¸ªæ­¥éª¤ï¼š

+   é¢„è®¡ç®—åµŒå…¥

+   å®šä¹‰å­æ¨¡å‹

## å‡†å¤‡åµŒå…¥

é€šè¿‡ä¸ºæ¯ä¸ªæ–‡æ¡£é¢„è®¡ç®—åµŒå…¥ï¼Œæˆ‘ä»¬å¯ä»¥åŠ é€Ÿé¢å¤–çš„æ¢ç´¢æ­¥éª¤ï¼Œå¹¶åœ¨éœ€è¦æ—¶ä½¿ç”¨åµŒå…¥å¿«é€Ÿè¿­ä»£ BERTopic çš„è¶…å‚æ•°ã€‚

ğŸ”¥ **æç¤º**ï¼šä½ å¯ä»¥åœ¨[MTEB æ’è¡Œæ¦œ](https://huggingface.co/spaces/mteb/leaderboard)ä¸Šæ‰¾åˆ°æœ‰å…³èšç±»çš„ä¼˜ç§€åµŒå…¥æ¦‚è¿°ã€‚

```py
from sentence_transformers import SentenceTransformer

# Pre-calculate embeddings
embedding_model = SentenceTransformer("BAAI/bge-small-en")
embeddings = embedding_model.encode(abstracts, show_progress_bar=True)
```

## å­æ¨¡å‹

æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°†å®šä¹‰ BERTopic ä¸­çš„æ‰€æœ‰å­æ¨¡å‹ï¼Œå¹¶å¯¹è¦åˆ›å»ºçš„èšç±»æ•°é‡è¿›è¡Œä¸€äº›å°çš„è°ƒæ•´ï¼Œå¦‚è®¾ç½®éšæœºçŠ¶æ€ç­‰ã€‚

```py
from umap import UMAP
from hdbscan import HDBSCAN

umap_model = UMAP(n_neighbors=15, n_components=5, min_dist=0.0, metric='cosine', random_state=42)
hdbscan_model = HDBSCAN(min_cluster_size=150, metric='euclidean', cluster_selection_method='eom', prediction_data=True)
```

ä½œä¸ºä¸€ä¸ªå°é¢å¤–å†…å®¹ï¼Œæˆ‘ä»¬å°†æŠŠä¹‹å‰åˆ›å»ºçš„åµŒå…¥å‡å°‘åˆ° 2 ç»´ï¼Œä»¥ä¾¿åœ¨åˆ›å»ºä¸»é¢˜æ—¶ç”¨äºå¯è§†åŒ–ã€‚

```py
# Pre-reduce embeddings for visualization purposes
reduced_embeddings = UMAP(n_neighbors=15, n_components=2, min_dist=0.0, metric='cosine', random_state=42).fit_transform(embeddings)
```

## è¡¨ç¤ºæ¨¡å‹

æˆ‘ä»¬å°†ç”¨ Llama 2 æ¥è¡¨ç¤ºè¿™äº›ä¸»é¢˜ï¼Œè¿™æ ·å¯ä»¥è·å¾—ä¸€ä¸ªä¸é”™çš„æ ‡ç­¾ã€‚ä¸è¿‡ï¼Œæˆ‘ä»¬å¯èƒ½è¿˜å¸Œæœ›æœ‰é¢å¤–çš„è¡¨ç¤ºï¼Œä»¥ä»å¤šä¸ªè§’åº¦æŸ¥çœ‹ä¸»é¢˜ã€‚

åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ c-TF-IDF ä½œä¸ºä¸»è¦è¡¨ç¤ºï¼Œå¹¶å°†[KeyBERT](https://maartengr.github.io/BERTopic/getting_started/representation/representation.html#keybertinspired)ã€[MMR](https://maartengr.github.io/BERTopic/getting_started/representation/representation.html#maximalmarginalrelevance)å’Œ[Llama 2](https://maartengr.github.io/BERTopic/getting_started/representation/llm.html)ä½œä¸ºé™„åŠ è¡¨ç¤ºã€‚

```py
from bertopic.representation import KeyBERTInspired, MaximalMarginalRelevance, TextGeneration

# KeyBERT
keybert = KeyBERTInspired()

# MMR
mmr = MaximalMarginalRelevance(diversity=0.3)

# Text generation with Llama 2
llama2 = TextGeneration(generator, prompt=prompt)

# All representation models
representation_model = {
    "KeyBERT": keybert,
    "Llama2": llama2,
    "MMR": mmr,
}
```

# ğŸ”¥ è®­ç»ƒ

ç°åœ¨æˆ‘ä»¬çš„æ¨¡å‹å·²ç»å‡†å¤‡å¥½ï¼Œæˆ‘ä»¬å¯ä»¥å¼€å§‹è®­ç»ƒæˆ‘ä»¬çš„ä¸»é¢˜æ¨¡å‹ï¼æˆ‘ä»¬å°†æ„Ÿå…´è¶£çš„å­æ¨¡å‹æä¾›ç»™ BERTopicï¼Œè¿è¡Œ`.fit_transform`ï¼Œçœ‹çœ‹æˆ‘ä»¬å¾—åˆ°ä»€ä¹ˆæ ·çš„ä¸»é¢˜ã€‚

```py
from bertopic import BERTopic

topic_model = BERTopic(

  # Sub-models
  embedding_model=embedding_model,
  umap_model=umap_model,
  hdbscan_model=hdbscan_model,
  representation_model=representation_model,

  # Hyperparameters
  top_n_words=10,
  verbose=True
)

# Train model
topics, probs = topic_model.fit_transform(abstracts, embeddings)
```

ç°åœ¨æˆ‘ä»¬å·²ç»å®Œæˆäº†æ¨¡å‹è®­ç»ƒï¼Œæ¥çœ‹ä¸€ä¸‹ç”Ÿæˆäº†ä»€ä¹ˆä¸»é¢˜ï¼š

```py
# Show top 3 most frequent topics
topic_model.get_topic_info()[1:4]
```

![](img/d9a0326290f7988d127e851f3d452d37.png)

```py
# Show top 3 least frequent topics
topic_model.get_topic_info()[-3:]
```

![](img/a34443869e8befd4e6af0d8622f045df.png)

æˆ‘ä»¬åˆ›å»ºäº†è¶…è¿‡ 100 ä¸ªä¸»é¢˜ï¼Œå®ƒä»¬ä¼¼ä¹éå¸¸å¤šæ ·åŒ–ã€‚æˆ‘ä»¬å¯ä»¥ä½¿ç”¨ Llama 2 æä¾›çš„æ ‡ç­¾ï¼Œå¹¶å°†å…¶åˆ†é…ç»™æˆ‘ä»¬åˆ›å»ºçš„ä¸»é¢˜ã€‚é€šå¸¸ï¼Œé»˜è®¤çš„ä¸»é¢˜è¡¨ç¤ºæ˜¯ c-TF-IDFï¼Œä½†æˆ‘ä»¬å°†é‡ç‚¹å…³æ³¨ Llama 2 çš„è¡¨ç¤ºã€‚

```py
llama2_labels = [label[0][0].split("\n")[0] for label in topic_model.get_topics(full=True)["Llama2"].values()]
topic_model.set_topic_labels(llama2_labels)
```

# [ğŸ“Š](https://emojigraph.org/bar-chart/) å¯è§†åŒ–

æˆ‘ä»¬å¯ä»¥æ‰‹åŠ¨æµè§ˆæ¯ä¸ªä¸»é¢˜ï¼Œè¿™ä¼šéœ€è¦å¤§é‡å·¥ä½œï¼Œæˆ–è€…æˆ‘ä»¬å¯ä»¥åœ¨ä¸€ä¸ªäº’åŠ¨å›¾è¡¨ä¸­å¯è§†åŒ–å®ƒä»¬ã€‚

BERTopic æœ‰ä¸€ç³»åˆ—[å¯è§†åŒ–å‡½æ•°](https://maartengr.github.io/BERTopic/getting_started/visualization/visualize_documents.html)ä¾›æˆ‘ä»¬ä½¿ç”¨ã€‚ç›®å‰ï¼Œæˆ‘ä»¬å°†ç»§ç»­å¯è§†åŒ–æ–‡æ¡£ã€‚

```py
topic_model.visualize_documents(titles, reduced_embeddings=reduced_embeddings, 
hide_annotations=True, hide_document_hover=False, custom_labels=True)
```

![](img/03b0559c55725b27e2ef199d98605b6e.png)

# [ğŸ–¼ï¸](https://emojipedia.org/framed-picture/)ï¼ˆé¢å¤–å†…å®¹ï¼‰ï¼šé«˜çº§å¯è§†åŒ–

è™½ç„¶æˆ‘ä»¬å¯ä»¥ä½¿ç”¨ BERTopic çš„å†…ç½®å¯è§†åŒ–åŠŸèƒ½ï¼Œä½†æˆ‘ä»¬ä¹Ÿå¯ä»¥åˆ›å»ºä¸€ä¸ªé™æ€å¯è§†åŒ–ï¼Œå¯èƒ½ä¼šæä¾›æ›´å¤šçš„ä¿¡æ¯ã€‚

æˆ‘ä»¬é¦–å…ˆåˆ›å»ºå¿…è¦çš„å˜é‡ï¼Œè¿™äº›å˜é‡åŒ…å«æˆ‘ä»¬å‡å°‘çš„åµŒå…¥å’Œè¡¨ç¤ºï¼š

```py
import itertools
import pandas as pd

# Define colors for the visualization to iterate over
colors = itertools.cycle(['#e6194b', '#3cb44b', '#ffe119', '#4363d8', '#f58231', '#911eb4', '#46f0f0', '#f032e6', '#bcf60c', '#fabebe', '#008080', '#e6beff', '#9a6324', '#fffac8', '#800000', '#aaffc3', '#808000', '#ffd8b1', '#000075', '#808080', '#ffffff', '#000000'])
color_key = {str(topic): next(colors) for topic in set(topic_model.topics_) if topic != -1}

# Prepare dataframe and ignore outliers
df = pd.DataFrame({"x": reduced_embeddings[:, 0], "y": reduced_embeddings[:, 1], "Topic": [str(t) for t in topic_model.topics_]})
df["Length"] = [len(doc) for doc in abstracts]
df = df.loc[df.Topic != "-1"]
df = df.loc[(df.y > -10) & (df.y < 10) & (df.x < 10) & (df.x > -10), :]
df["Topic"] = df["Topic"].astype("category")

# Get centroids of clusters
mean_df = df.groupby("Topic").mean().reset_index()
mean_df.Topic = mean_df.Topic.astype(int)
mean_df = mean_df.sort_values("Topic")
```

æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ Matplotlib å¯è§†åŒ–å‡å°‘åçš„åµŒå…¥ï¼Œå¹¶ä»¥è§†è§‰ä¸Šæ›´ä»¤äººæ„‰æ‚¦çš„æ–¹å¼å¤„ç†æ ‡ç­¾ï¼š

```py
import seaborn as sns
from matplotlib import pyplot as plt
from adjustText import adjust_text
import matplotlib.patheffects as pe
import textwrap

fig = plt.figure(figsize=(20, 20))
sns.scatterplot(data=df, x='x', y='y', c=df['Topic'].map(color_key), alpha=0.4, sizes=(0.4, 10), size="Length")

# Annotate top 50 topics
texts, xs, ys = [], [], []
for row in mean_df.iterrows():
  topic = row[1]["Topic"]
  name = textwrap.fill(topic_model.custom_labels_[int(topic)], 20)
  if int(topic) <= 50:
    xs.append(row[1]["x"])
    ys.append(row[1]["y"])
    texts.append(plt.text(row[1]["x"], row[1]["y"], name, size=10, ha="center", color=color_key[str(int(topic))],
                          path_effects=[pe.withStroke(linewidth=0.5, foreground="black")]
                          ))

# Adjust annotations such that they do not overlap
adjust_text(texts, x=xs, y=ys, time_lim=1, force_text=(0.01, 0.02), force_static=(0.01, 0.02), force_pull=(0.5, 0.5))
plt.axis('off')
plt.legend('', frameon=False)
plt.show()
```

![](img/8fb9e2c99f87f38e3d5f5a824b2a0d65.png)

**æ›´æ–°**ï¼šæˆ‘ä¸Šä¼ äº†ä¸€ä¸ªè§†é¢‘ç‰ˆæœ¬åˆ° YouTubeï¼Œè¯¦ç»†ä»‹ç»äº†å¦‚ä½•ä½¿ç”¨ BERTopic ä¸ Llama 2ï¼š

# æ„Ÿè°¢ä½ çš„é˜…è¯»ï¼

å¦‚æœä½ åƒæˆ‘ä¸€æ ·ï¼Œå¯¹ AI å’Œ/æˆ–å¿ƒç†å­¦å……æ»¡çƒ­æƒ…ï¼Œè¯·éšæ—¶åœ¨[**LinkedIn**](https://www.linkedin.com/in/mgrootendorst/)ä¸ŠåŠ æˆ‘ä¸ºå¥½å‹ï¼Œåœ¨[**Twitter**](https://twitter.com/MaartenGr)ä¸Šå…³æ³¨æˆ‘ï¼Œæˆ–è®¢é˜…æˆ‘çš„[**Newsletter**](http://maartengrootendorst.substack.com/)ã€‚ä½ ä¹Ÿå¯ä»¥åœ¨æˆ‘çš„[**ä¸ªäººç½‘ç«™**](https://maartengrootendorst.com/)ä¸Šæ‰¾åˆ°ä¸€äº›æˆ‘çš„å†…å®¹ã€‚

*æ‰€æœ‰æ²¡æœ‰æ¥æºè¯´æ˜çš„å›¾åƒå‡ç”±ä½œè€…åˆ›ä½œ*
