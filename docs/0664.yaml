- en: Transformer Aided Supply Chain Network Design
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/transformer-aided-supply-chain-network-design-fe45bb846f0f?source=collection_archive---------9-----------------------#2023-02-17](https://towardsdatascience.com/transformer-aided-supply-chain-network-design-fe45bb846f0f?source=collection_archive---------9-----------------------#2023-02-17)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Using transformer to help solve a classic problem in supply chain — facility
    location problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@guanx92?source=post_page-----fe45bb846f0f--------------------------------)[![Guangrui
    Xie](../Images/def9aa637424a88d75a6a3bb103350bc.png)](https://medium.com/@guanx92?source=post_page-----fe45bb846f0f--------------------------------)[](https://towardsdatascience.com/?source=post_page-----fe45bb846f0f--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----fe45bb846f0f--------------------------------)
    [Guangrui Xie](https://medium.com/@guanx92?source=post_page-----fe45bb846f0f--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F495b92f0c66d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransformer-aided-supply-chain-network-design-fe45bb846f0f&user=Guangrui+Xie&userId=495b92f0c66d&source=post_page-495b92f0c66d----fe45bb846f0f---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----fe45bb846f0f--------------------------------)
    ·15 min read·Feb 17, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Ffe45bb846f0f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransformer-aided-supply-chain-network-design-fe45bb846f0f&user=Guangrui+Xie&userId=495b92f0c66d&source=-----fe45bb846f0f---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ffe45bb846f0f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransformer-aided-supply-chain-network-design-fe45bb846f0f&source=-----fe45bb846f0f---------------------bookmark_footer-----------)![](../Images/bd64ec628e3e129b102daba43f9a1307.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Photo by [Mika Baumeister](https://unsplash.com/es/@mbaumi?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: ChatGPT has been a really hot topic recently due to its general intelligence
    to accomplish a broad range of tasks. The core model underneath ChatGPT is transformer,
    which was first proposed for machine translation tasks in the well-known paper
    [*Attention is all you need*](https://arxiv.org/pdf/1706.03762.pdf)by a research
    team at Google. Transformer processes an entire sentence at once using an attention
    mechanism and positional encoding rather than word by word using recursions (like
    recurrent neural networks). This reduces the information loss during recursions,
    hence giving transformer a better capability to learn long-range context dependencies,
    as compared to traditional recurrent neural networks (e.g., LSTM, GRU).
  prefs: []
  type: TYPE_NORMAL
- en: The application of transformer is not limited in language models. Researchers
    have applied it to many other tasks such as time series forecasting, computer
    vision, etc., since it was first proposed. A question I have been thinking about
    is, can transformer be used to help solve practical operations research (OR) problems
    in supply chain area? In this article, I will show an example of using transformer
    to assist in solving a classic OR problem in supply chain area — facility location
    problem.
  prefs: []
  type: TYPE_NORMAL
- en: Facility location problem
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let’s first review the basic setting of this problem. I also briefly discussed
    about this problem in [my first article](https://medium.com/towards-data-science/some-thoughts-on-synergies-between-operations-research-and-machine-learning-921d78ed4bd5)
    on Towards Data Science.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let us assume that a company wants to consider building distribution centers
    (DCs) among *I* candidate sites to ship their finished goods to *J* customers.
    Each site *i* has its associate capacity for storing the finished goods with a
    maximum of *m_i* units of products*.* Buildinga DC oneach site *i* requires a
    fixed construction fee of *f_i.* Shipping each unit of product from site *i* to
    customer *j* toincurs a shipping cost of *c_ij.* Each customer *j* has a demand
    of *d_j* and the demand of all customers must be satisfied. Let binary variable
    *y_i* denote whether we build a DC at site *i* and *x_ij* denote the volume of
    products to be shipped from site *i* to customer *j.* The optimization problem
    with an objective to minimize the total cost can be formulated as below:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/40d846413209efd414da65ab02859c3c.png)'
  prefs: []
  type: TYPE_IMG
- en: Formulation of facility location problem (Image by author)
  prefs: []
  type: TYPE_NORMAL
- en: This is a mixed integer linear programming (MILP) problem, which can be solved
    using any commercial or non-commercial solvers (e.g., CPLEX, GUROBI, SCIP). However,
    when the model gets large (*I* and *J* are large), it could take a long time to
    solve the model. The main complexity of this problem comes from the integer variables
    *y_i’*s, as these variables are the ones that need to be branched on in the branch
    and bound algorithm. If we can find a faster way to determine the values of *y_i’*s,
    that would save a lot of solving time. This is where transformer comes into play.
  prefs: []
  type: TYPE_NORMAL
- en: Transformer aided solution approach
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The basic idea of this solution approach is to train a transformer to predict
    the values of *y_i’*s by learning from the solutions of a large number of instances
    of the facility location problem given by a solver. The problem can be formulated
    as a classification problem as *y_i’*s can only take values of 0 or 1, corresponding
    to two classes. Class 0 means site *i* is not selected for building a DC, and
    class 1 means site *i* is selected. The transformer, once trained, can predict
    which class each *y_i* belongs toall at once for an unseen instance, thus saving
    a lot of time spent on branching on integer variables. The diagram below shows
    the architecture of the transformer adopted here.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3eb9052c67956f25dcd012391d2866d2.png)'
  prefs: []
  type: TYPE_IMG
- en: The architecture of the transformer model adopted in this article (Image by
    author)
  prefs: []
  type: TYPE_NORMAL
- en: As illustrated by the diagram above, the vector representations of each candidate
    DC site and customer (containing relevant features of each candidate DC site and
    customer) are passed into a linear layer. The linear layer produces an embedding
    vector for each candidate DC site and customer. These embeddings are then passed
    through *N* standard encoder blocks as depicted in the paper [*Attention is all
    you need*](https://arxiv.org/pdf/1706.03762.pdf)*.* Within the encoder blocks,
    the multi-head attention mechanism allows each candidate DC site to attend to
    all the customers’ and other candidate DC sites’ information. Then the outputs
    of the encoder blocks are passed through a linear layer + softmax layer as the
    decoding process.
  prefs: []
  type: TYPE_NORMAL
- en: 'A few accommodations I made to the transformer architecture for this particular
    problem are:'
  prefs: []
  type: TYPE_NORMAL
- en: Positional encoding is removed due to the fact that the exact order of the input
    sequence *V_dc1, …, V_cusJ* does not matter to the output from the perspective
    of the optimization problem. In other words, randomly shuffling the the input
    sequence to the transformer shouldn’t affect the solution to the facility location
    problem.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The output layer produces a two-dimensional vector for each element in the input
    sequence *V_dc1, …, V_cusJ*. The two-dimensional vector contains the probabilities
    of this element belonging to class 0 and class 1\. Note that this two-dimensional
    vector is actually invalid for elements *V_cus1, …, V_cusJ* as we cannot build
    a DC on a customer’s site. So when training the transformer, we mask the outputs
    corresponding to *V_cus1, …, V_cusJ* to calculate the loss function, as we don’t
    care about what these outputs are.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In this facility location problem, we assume that the unit shipping cost *c_ij*
    isproportional to the distance between site *i* and customer *j.* We define *V_dci*
    as *[dci_x, dci_y, m_i, f_i, 0],* where *dci_x* and *dci_y* are the coordinates
    of candidate DC site *i,* and *0* indicates that this is a vector representation
    of the features of a candidate DC site; we define *V_cusj* as *[cusj_x, cusj_y,
    d_j, 0, 1],* where *cusj_x* and *cusj_y* are the coordinates of customer *j, 0*
    is the counterpart of *f_i* in *V_dci* meaning there’s no fixed fee incurred at
    a customer*,* and *1* indicates that this is a vector representation of a customer*.*
    Hence each element in the input sequence is a 5-dimensional vector. Through the
    first linear layer, we project each vector into a high dimensional embedding vector
    and feed them into the encoder blocks.
  prefs: []
  type: TYPE_NORMAL
- en: To train the transformer, we first need to build a dataset. To set up the labels
    in the dataset, we solve a large number of instances of the facility location
    problem using an optimization solver (i.e., SCIP) and then record the values of
    *y_i’*s for each instance. The coordinates, *d_j’*s, *m_i’*s, *f_i’*s of each
    instances are used to construct the input sequence for the transformer. The entire
    dataset is split into a training set and a test set.
  prefs: []
  type: TYPE_NORMAL
- en: Once the transformer is trained, it can be used to predict which class each
    candidate DC site *i* belongs to, in other words, whether *y_i* is0or 1\. Then
    we fix each *y_i* to its predicted value, and solve the MILP with the rest of
    the variables *x_ij’*s*.* Note that once the values of *y_i’*s are fixed, the
    facility location problem becomes an LP, which is much easier to solve using the
    optimization solver. The resulting solution may be sub-optimal, but saves a lot
    of solving time for large instances.
  prefs: []
  type: TYPE_NORMAL
- en: 'One caveat of the transformer aided solution approach is that the predicted
    *y_i* values given by the transformer may sometimes result in infeasibility for
    the MILP, due to the fact that the predictions of the transformer are not perfectly
    accurate. The infeasibility is caused by the sum of *m_i * y_i* being smaller
    thanthe sum of *d_j*, meaning there will not be enough supply to satisfy the total
    demand across all the customers if we follow the predicted values of *y_i’*s.
    In such cases, we retain the *y_i* values where *y_i* is 1, and search through
    the rest of the candidate DC sites to make up the difference between the total
    supply and demand. Essentially, we need to solve another optimization problem
    as below:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9f98fb0bfb3931f261f93658268536a6.png)'
  prefs: []
  type: TYPE_IMG
- en: Problem formulation for resolving infeasibility (Image by author)
  prefs: []
  type: TYPE_NORMAL
- en: Here, *I_0* is the set of candidate DC sites of which the predicted *y_i* value
    is 0 and *I_1* is the set of candidate DC sites of which the predicted *y_i* value
    is 1\. This is essentially a knapsack problem which can be solved by an optimization
    solver. However, with large instances, this problem could still consume a lot
    of solving time. Hence, I decided to adopt a heuristic to solve this problem —
    the greedy algorithm for solving fractional knapsack problems. Specifically, we
    calculate the ratio *f_i/m_i* for each candidate DC site in *I_0,* and sort the
    sites in an ascending order based on the ratios. Then we select the sites from
    the first to the last one by one until the constraint is met. After this procedure,
    the *y_i* values of the selected sites in *I_0,* together with all the sites in
    I_1, are set to 1\. The *y_i* values of the rest sites are set to 0\. This configuration
    is then fed into the optimization solver to solve for the rest *x_ij* variables.
  prefs: []
  type: TYPE_NORMAL
- en: Numerical experiments
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: I randomly created 800 instances of the facility location problem with 100 candidate
    DC sites and 200 customers. I used 560 instances of them for training and 240
    instances for testing. The non-commercial solver SCIP was adopted for solving
    the instances. The code for generating the instances is given below.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Then we read in the solutions and create the dataset used for training the transformer.
    Note that here I converted the cartesian coordinate system into polar coordinate
    system when building the dataset, as the latter one leads to higher accuracy for
    the transformer.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Then we define the architecture of the transformer. Here the embedding vectors
    are 256-dimensional, the number of heads in the multi-head attention mechanism
    is 8, and the number of encoder blocks is 3\. I didn’t experiment with many hyper-parameter
    settings as this setting already achieved satisfactory accuracy. More hyper-parameter
    tuning can be done to further increase the accuracy of the transformer.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Then we train and test the transformer with the dataset created previously.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: After training, the transformer achieved approximately 92% accuracy on the test
    set containing 240 instances. Considering the test set is not imbalanced (46.57%
    and 53.43% of the labels in the test set are 0 and 1 respectively) and our transformer
    is not always predicting the same class, 92% accuracy should be a satisfactory
    score.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we test the transformer aided solution approach for the facility location
    problem vs solving the MILP directly using the SCIP solver only. The main purpose
    is to prove that the transformer helps reduce a lot of solving time without a
    significant deterioration in the solution quality.
  prefs: []
  type: TYPE_NORMAL
- en: I created another 100 random unseen instances with *m_i, f_i, d_j* parameters
    following the same distribution as those instances used for training the transformer,
    and applied each solution approach to them. I tested for 3 cases with different
    numbers of candidate DC sites and customers, namely (n_dc=100,n_cus=200), (n_dc=200,n_cus=400),
    (n_dc=400,n_cus=800). The code is given below.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The average solving time (unit: seconds, on a laptop with Intel(R) Core(TM)
    i7–10750H CPU, 6 cores) and objective value of each solution approach for each
    case are reported in the table below.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7e7206c42444311c29c69e97b2353154.png)'
  prefs: []
  type: TYPE_IMG
- en: Test results obtained from the instances with parameters following the same
    distribution as the training set for the transformer (Image by author)
  prefs: []
  type: TYPE_NORMAL
- en: We can see that for all cases, the transformer aided solution approach consumes
    around 2% of the solving time of the SCIP only solution approach, while the objective
    value deteriorates by only around 1%.
  prefs: []
  type: TYPE_NORMAL
- en: So far we have only tested the transformer aided solution approach on instances
    with parameters following the same distribution as the training set. What if we
    apply it to instances with parameters following a different distribution? To test
    this, I generated 100 test instances for (n_dc=100,n_cus=200) with *m_i, f_i,
    d_j* parameters following a normal distribution with double mean and double standard
    deviation, using the code below.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: The results obtained from these instances are reported in the table below.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d6842de075bc800a3a9cc8bf3edba2c8.png)'
  prefs: []
  type: TYPE_IMG
- en: Test results obtained from the instances with parameters following a distribution
    with double mean and standard deviation as compared to the training set for the
    transformer (Image by author)
  prefs: []
  type: TYPE_NORMAL
- en: We see that the transformer aided solution approach consumes around 2% of the
    solving time of the SCIP only solution approach, while the objective value deteriorates
    by around 3%, a little bit more than the previous test, but still acceptable.
  prefs: []
  type: TYPE_NORMAL
- en: We generate one more set of 100 test instances with *m_i, f_i, d_j* parameters
    following a normal distribution with half mean and standard deviation, using the
    code below.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: The results are reported in the table below.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/bd3081cc3722f32bd78bc6d249f68531.png)'
  prefs: []
  type: TYPE_IMG
- en: Test results obtained from the instances with parameters following a distribution
    with half mean and standard deviation as compared to the training set for the
    transformer (Image by author)
  prefs: []
  type: TYPE_NORMAL
- en: We see that the transformer aided solution approach still greatly saves the
    solving time, however, there is also a significant deterioration in solution quality.
    So the transformer is indeed a bit overfitted to the parameter distributions of
    the instances in the training set.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this article, I trained a transformer model to help solve a classic MILP
    in supply chain area — facility location problem. The transformer is able to predict
    the correct values the integer variables should take by learning through the solutions
    of hundreds of instances given by SCIP. We then fix the integer variables in the
    SCIP solver to the predicted values given by the transformer model and solve for
    the rest variables. Numerical experiments show that the transformer aided solution
    approach reduces the solving time significantly with only slight deterioration
    in solution quality, when applied to instances with parameters following the same
    or similar distributions.
  prefs: []
  type: TYPE_NORMAL
- en: Applying machine learning (ML) to help solve MILPs faster is an emerging research
    topic in both ML and OR communities, as mentioned in [my first article.](https://medium.com/towards-data-science/some-thoughts-on-synergies-between-operations-research-and-machine-learning-921d78ed4bd5)
    Existing research ideas include supervised learning (learning from a solver) and
    reinforcement learning (learning to solve MILPs itself by exploring the solution
    space and observing rewards), etc. The core idea of this article belongs to the
    former class (supervised learning), and takes advantage of the attention mechanism
    in the transformer model to learn making decisions for the integer variables.
    Since the transformer model learns from the solver, the solution quality of the
    transformer aided solution approach can never surpass the solver, but we gain
    a huge saving in the solving time.
  prefs: []
  type: TYPE_NORMAL
- en: Using a pre-trained ML model to generate partial solutions for MILPs could be
    a promising idea to expedite the solving process for large instances in commercial
    applications. Here, I selected the facility location problem as an example of
    MILPs to do experiments, but the same idea applies to other problems, such as
    assignment problems. This approach is best suited for the case where we want to
    repeatedly solve large MILP instances with parameters following the same or similar
    distributions, as the pre-trained ML model tends to be overfitted to the parameter
    distributions in the training instances. One way to improve the robustness of
    this approach may be to include more instances with parameters following more
    diverse distributions to reduce overfitting, and this can be something to explore
    in my future articles.
  prefs: []
  type: TYPE_NORMAL
- en: Thanks for reading!
  prefs: []
  type: TYPE_NORMAL
