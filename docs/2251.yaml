- en: Serve Large Language Models from Your Computer with Text Generation Inference
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/serve-large-language-models-from-your-computer-with-text-generation-inference-54f4dd8783a7?source=collection_archive---------7-----------------------#2023-07-13](https://towardsdatascience.com/serve-large-language-models-from-your-computer-with-text-generation-inference-54f4dd8783a7?source=collection_archive---------7-----------------------#2023-07-13)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Examples with the instruct version of Falcon-7B
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@bnjmn_marie?source=post_page-----54f4dd8783a7--------------------------------)[![Benjamin
    Marie](../Images/3ea1ad230cb1e67610418a8e36a5e5dd.png)](https://medium.com/@bnjmn_marie?source=post_page-----54f4dd8783a7--------------------------------)[](https://towardsdatascience.com/?source=post_page-----54f4dd8783a7--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----54f4dd8783a7--------------------------------)
    [Benjamin Marie](https://medium.com/@bnjmn_marie?source=post_page-----54f4dd8783a7--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fad2a414578b3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fserve-large-language-models-from-your-computer-with-text-generation-inference-54f4dd8783a7&user=Benjamin+Marie&userId=ad2a414578b3&source=post_page-ad2a414578b3----54f4dd8783a7---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----54f4dd8783a7--------------------------------)
    ·6 min read·Jul 13, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F54f4dd8783a7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fserve-large-language-models-from-your-computer-with-text-generation-inference-54f4dd8783a7&user=Benjamin+Marie&userId=ad2a414578b3&source=-----54f4dd8783a7---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F54f4dd8783a7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fserve-large-language-models-from-your-computer-with-text-generation-inference-54f4dd8783a7&source=-----54f4dd8783a7---------------------bookmark_footer-----------)![](../Images/41e6b63543c42e19cc0b1e4ee23f95bf.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Photo by [Nana Dua](https://unsplash.com/@nanadua11?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: Running very large language models (LLM) locally, on consumer hardware, is now
    possible thanks to quantization methods such as [QLoRa](/qlora-fine-tune-a-large-language-model-on-your-gpu-27bed5a03e2b)
    and [GPTQ](https://github.com/IST-DASLab/gptq).
  prefs: []
  type: TYPE_NORMAL
- en: Considering how long it takes to load an LLM, we may also want to keep the LLM
    in memory to query it and have the results instantly. If you use LLMs with a standard
    inference pipeline, you must reload the model each time. If the model is very
    large, you may have to wait several minutes for the model to generate an output.
  prefs: []
  type: TYPE_NORMAL
- en: There are various frameworks that can host LLMs on a server (locally or remotely).
    On my blog, I have already presented the [Triton Inference Server which is a very
    optimized framework](https://medium.com/towards-data-science/deploy-your-local-gpt-server-with-triton-a825d528aa5d),
    developed by NVIDIA, to serve multiple LLMs and balance the load across GPUs.
    But if you have only one GPU and if you want to host your model on your computer,
    using a Triton inference may seem unsuitable.
  prefs: []
  type: TYPE_NORMAL
- en: In this article, I present an alternative called Text Generation Inference.
    A more straightforward framework that implements all the minimum features to run
    and serve LLMs on consumer hardware.
  prefs: []
  type: TYPE_NORMAL
- en: After reading this article, you will have on your computer a chat model/LLM
    deployed locally and…
  prefs: []
  type: TYPE_NORMAL
