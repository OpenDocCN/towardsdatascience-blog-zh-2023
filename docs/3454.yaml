- en: A Gentle Introduction to Deep Reinforcement Learning in JAX
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ã€Šæ·±å…¥æµ…å‡º JAX ä¸­çš„æ·±åº¦å¼ºåŒ–å­¦ä¹ ã€‹
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/a-gentle-introduction-to-deep-reinforcement-learning-in-jax-c1e45a179b92?source=collection_archive---------4-----------------------#2023-11-21](https://towardsdatascience.com/a-gentle-introduction-to-deep-reinforcement-learning-in-jax-c1e45a179b92?source=collection_archive---------4-----------------------#2023-11-21)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://towardsdatascience.com/a-gentle-introduction-to-deep-reinforcement-learning-in-jax-c1e45a179b92?source=collection_archive---------4-----------------------#2023-11-21](https://towardsdatascience.com/a-gentle-introduction-to-deep-reinforcement-learning-in-jax-c1e45a179b92?source=collection_archive---------4-----------------------#2023-11-21)
- en: Solving the CartPole environment with DQN in under a second
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: åœ¨ä¸€ç§’é’Ÿå†…ç”¨ DQN è§£å†³ CartPole ç¯å¢ƒ
- en: '[](https://medium.com/@ryanpegoud?source=post_page-----c1e45a179b92--------------------------------)[![Ryan
    PÃ©goud](../Images/9314b76c2be56bda8b73b4badf9e3e4d.png)](https://medium.com/@ryanpegoud?source=post_page-----c1e45a179b92--------------------------------)[](https://towardsdatascience.com/?source=post_page-----c1e45a179b92--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----c1e45a179b92--------------------------------)
    [Ryan PÃ©goud](https://medium.com/@ryanpegoud?source=post_page-----c1e45a179b92--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@ryanpegoud?source=post_page-----c1e45a179b92--------------------------------)[![Ryan
    PÃ©goud](../Images/9314b76c2be56bda8b73b4badf9e3e4d.png)](https://medium.com/@ryanpegoud?source=post_page-----c1e45a179b92--------------------------------)[](https://towardsdatascience.com/?source=post_page-----c1e45a179b92--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----c1e45a179b92--------------------------------)
    [Ryan PÃ©goud](https://medium.com/@ryanpegoud?source=post_page-----c1e45a179b92--------------------------------)'
- en: Â·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Â·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F27fba63b402e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-gentle-introduction-to-deep-reinforcement-learning-in-jax-c1e45a179b92&user=Ryan+P%C3%A9goud&userId=27fba63b402e&source=post_page-27fba63b402e----c1e45a179b92---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----c1e45a179b92--------------------------------)
    Â·10 min readÂ·Nov 21, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fc1e45a179b92&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-gentle-introduction-to-deep-reinforcement-learning-in-jax-c1e45a179b92&user=Ryan+P%C3%A9goud&userId=27fba63b402e&source=-----c1e45a179b92---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[å…³æ³¨](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F27fba63b402e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-gentle-introduction-to-deep-reinforcement-learning-in-jax-c1e45a179b92&user=Ryan+P%C3%A9goud&userId=27fba63b402e&source=post_page-27fba63b402e----c1e45a179b92---------------------post_header-----------)
    å‘è¡¨åœ¨[Towards Data Science](https://towardsdatascience.com/?source=post_page-----c1e45a179b92--------------------------------)
    Â· 10åˆ†é’Ÿé˜…è¯»Â·2023å¹´11æœˆ21æ—¥[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fc1e45a179b92&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-gentle-introduction-to-deep-reinforcement-learning-in-jax-c1e45a179b92&user=Ryan+P%C3%A9goud&userId=27fba63b402e&source=-----c1e45a179b92---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc1e45a179b92&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-gentle-introduction-to-deep-reinforcement-learning-in-jax-c1e45a179b92&source=-----c1e45a179b92---------------------bookmark_footer-----------)![](../Images/af8f701ac536881e802dd7507bdc46f2.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc1e45a179b92&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-gentle-introduction-to-deep-reinforcement-learning-in-jax-c1e45a179b92&source=-----c1e45a179b92---------------------bookmark_footer-----------)![](../Images/af8f701ac536881e802dd7507bdc46f2.png)'
- en: Photo by [Thomas Despeyroux](https://unsplash.com/@thomasdes?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±[Thomas Despeyroux](https://unsplash.com/@thomasdes?utm_source=medium&utm_medium=referral)æ‹æ‘„ï¼Œå‘å¸ƒäº[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: Recent progress in Reinforcement Learning (RL), such as Waymoâ€™s autonomous taxis
    or DeepMindâ€™s superhuman chess-playing agents, complement **classical RL** with
    **Deep Learning** components such as **Neural Networks** and **Gradient Optimization**
    methods.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€è¿‘åœ¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ–¹é¢çš„è¿›å±•ï¼Œä¾‹å¦‚ Waymo çš„è‡ªåŠ¨é©¾é©¶å‡ºç§Ÿè½¦æˆ– DeepMind çš„è¶…äººç±»æ£‹ç±»ä»£ç†ï¼Œç»“åˆäº†**ç»å…¸ RL**å’Œ**æ·±åº¦å­¦ä¹ **ç»„ä»¶ï¼Œå¦‚**ç¥ç»ç½‘ç»œ**å’Œ**æ¢¯åº¦ä¼˜åŒ–**æ–¹æ³•ã€‚
- en: Building on the foundations and coding principles introduced in one of my previous
    stories, weâ€™ll discover and learn to implement **Deep Q-Networks** (**DQN**) and
    **replay buffers** to solve OpenAIâ€™s **CartPole** environment. All of that **in
    under a second** using JAX!
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä¹‹å‰ä»‹ç»çš„åŸºç¡€å’Œç¼–ç åŸåˆ™ä¸Šæ„å»ºï¼Œæˆ‘ä»¬å°†æ¢ç´¢å¹¶å­¦ä¹ å¦‚ä½•ä½¿ç”¨JAXå®ç°**æ·±åº¦Qç½‘ç»œ**ï¼ˆ**DQN**ï¼‰å’Œ**å›æ”¾ç¼“å†²åŒº**æ¥è§£å†³OpenAIçš„**CartPole**ç¯å¢ƒã€‚æ‰€æœ‰è¿™äº›æ“ä½œéƒ½åœ¨**ä¸åˆ°ä¸€ç§’**çš„æ—¶é—´å†…å®Œæˆï¼
- en: 'For an introduction to **JAX**, **vectorized environments**, and **Q-learning**,
    please refer to the content of this story:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äº**JAX**ã€**å‘é‡åŒ–ç¯å¢ƒ**å’Œ**Q-learning**çš„ä»‹ç»ï¼Œè¯·å‚é˜…ä»¥ä¸‹å†…å®¹ï¼š
- en: '[](/vectorize-and-parallelize-rl-environments-with-jax-q-learning-at-the-speed-of-light-49d07373adf5?source=post_page-----c1e45a179b92--------------------------------)
    [## Vectorize and Parallelize RL Environments with JAX: Q-learning at the Speed
    of Lightâš¡'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/vectorize-and-parallelize-rl-environments-with-jax-q-learning-at-the-speed-of-light-49d07373adf5?source=post_page-----c1e45a179b92--------------------------------)
    [## ä½¿ç”¨JAXå‘é‡åŒ–å’Œå¹¶è¡ŒåŒ–RLç¯å¢ƒï¼šQ-learningçš„å…‰é€Ÿâš¡'
- en: Learn to vectorize a GridWorld environment and train 30 Q-learning agents in
    parallel on a CPU, at 1.8 million step perâ€¦
  id: totrans-13
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: å­¦ä¹ å¦‚ä½•åœ¨CPUä¸Šå‘é‡åŒ–GridWorldç¯å¢ƒï¼Œå¹¶åŒæ—¶è®­ç»ƒ30ä¸ªQ-learningä»£ç†ï¼Œæ¯ä¸ªä»£ç†è¿›è¡Œ180ä¸‡æ­¥â€¦
- en: towardsdatascience.com](/vectorize-and-parallelize-rl-environments-with-jax-q-learning-at-the-speed-of-light-49d07373adf5?source=post_page-----c1e45a179b92--------------------------------)
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/vectorize-and-parallelize-rl-environments-with-jax-q-learning-at-the-speed-of-light-49d07373adf5?source=post_page-----c1e45a179b92--------------------------------)
- en: 'Our framework of choice for deep learning will be DeepMindâ€™s **Haiku** library,
    which I recently introduced in the context of Transformers:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬é€‰æ‹©çš„æ·±åº¦å­¦ä¹ æ¡†æ¶æ˜¯DeepMindçš„**Haiku**åº“ï¼Œæˆ‘æœ€è¿‘åœ¨Transformerçš„ä¸Šä¸‹æ–‡ä¸­ä»‹ç»è¿‡ï¼š
- en: '[](/implementing-a-transformer-encoder-from-scratch-with-jax-and-haiku-791d31b4f0dd?source=post_page-----c1e45a179b92--------------------------------)
    [## Implementing a Transformer Encoder from Scratch with JAX and Haiku ğŸ¤–'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/implementing-a-transformer-encoder-from-scratch-with-jax-and-haiku-791d31b4f0dd?source=post_page-----c1e45a179b92--------------------------------)
    [## ä½¿ç”¨JAXå’ŒHaikuä»å¤´å¼€å§‹å®ç°Transformerç¼–ç å™¨ ğŸ¤–'
- en: Understanding the fundamental building blocks of Transformers.
  id: totrans-17
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ç†è§£Transformerçš„åŸºç¡€æ„å»ºæ¨¡å—ã€‚
- en: towardsdatascience.com](/implementing-a-transformer-encoder-from-scratch-with-jax-and-haiku-791d31b4f0dd?source=post_page-----c1e45a179b92--------------------------------)
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/implementing-a-transformer-encoder-from-scratch-with-jax-and-haiku-791d31b4f0dd?source=post_page-----c1e45a179b92--------------------------------)
- en: 'This article will cover the following sections:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬æ–‡å°†æ¶µç›–ä»¥ä¸‹å‡ ä¸ªéƒ¨åˆ†ï¼š
- en: '**Why** do we need Deep RL?'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ä¸ºä»€ä¹ˆ**æˆ‘ä»¬éœ€è¦æ·±åº¦RLï¼Ÿ'
- en: '**Deep Q-Networks,** *theory and practice*'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**æ·±åº¦Qç½‘ç»œ**çš„**ç†è®ºå’Œå®è·µ**'
- en: '**Replay Buffers**'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**å›æ”¾ç¼“å†²åŒº**'
- en: Translating the **CartPole** environment to **JAX**
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å°†**CartPole**ç¯å¢ƒè½¬æ¢ä¸º**JAX**
- en: The **JAX** way to write **efficient training loops**
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**JAX**ç¼–å†™**é«˜æ•ˆè®­ç»ƒå¾ªç¯**çš„æ–¹å¼'
- en: '*As always, all the code presented in this article is available on GitHub:*'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '*å¦‚å¾€å¸¸ä¸€æ ·ï¼Œæœ¬æ–‡ä¸­æä¾›çš„æ‰€æœ‰ä»£ç éƒ½å¯åœ¨GitHubä¸Šæ‰¾åˆ°ï¼š*'
- en: '[](https://github.com/RPegoud/jym?source=post_page-----c1e45a179b92--------------------------------)
    [## GitHub - RPegoud/jym: JAX implementation of RL algorithms and vectorized environments'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://github.com/RPegoud/jym?source=post_page-----c1e45a179b92--------------------------------)
    [## GitHub - RPegoud/jymï¼šJAXå®ç°çš„RLç®—æ³•å’Œå‘é‡åŒ–ç¯å¢ƒ'
- en: 'JAX implementation of RL algorithms and vectorized environments - GitHub -
    RPegoud/jym: JAX implementation of RLâ€¦'
  id: totrans-27
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 'JAXå®ç°çš„RLç®—æ³•å’Œå‘é‡åŒ–ç¯å¢ƒ - GitHub - RPegoud/jym: JAXå®ç°çš„RLâ€¦'
- en: github.com](https://github.com/RPegoud/jym?source=post_page-----c1e45a179b92--------------------------------)
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: github.com](https://github.com/RPegoud/jym?source=post_page-----c1e45a179b92--------------------------------)
- en: '**Why** do we need Deep RL?'
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**ä¸ºä»€ä¹ˆ**æˆ‘ä»¬éœ€è¦æ·±åº¦RLï¼Ÿ'
- en: In previous articles, we introduced [Temporal Difference Learning](https://medium.com/towards-data-science/temporal-difference-learning-and-the-importance-of-exploration-an-illustrated-guide-5f9c3371413a)
    algorithms and in particular [Q-learning](/vectorize-and-parallelize-rl-environments-with-jax-q-learning-at-the-speed-of-light-49d07373adf5).
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä¹‹å‰çš„æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†[æ—¶é—´å·®åˆ†å­¦ä¹ ](https://medium.com/towards-data-science/temporal-difference-learning-and-the-importance-of-exploration-an-illustrated-guide-5f9c3371413a)ç®—æ³•ï¼Œç‰¹åˆ«æ˜¯[Q-learning](/vectorize-and-parallelize-rl-environments-with-jax-q-learning-at-the-speed-of-light-49d07373adf5)ã€‚
- en: Simply put, Q-learning is an **off-policy** algorithm *(the target policy is
    not the policy used for decision-making)* maintaining and updating a **Q-table**,
    an explicit **mapping** of **states** to corresponding **action values**.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: ç®€å•æ¥è¯´ï¼ŒQ-learningæ˜¯ä¸€ç§**ç¦»ç­–ç•¥**ç®—æ³•ï¼ˆ*ç›®æ ‡ç­–ç•¥ä¸ç”¨äºå†³ç­–çš„ç­–ç•¥ä¸åŒ*ï¼‰ï¼Œç”¨äºç»´æŠ¤å’Œæ›´æ–°**Qè¡¨**ï¼Œä¸€ä¸ªæ˜ç¡®çš„**çŠ¶æ€**åˆ°ç›¸åº”**åŠ¨ä½œå€¼**çš„**æ˜ å°„**ã€‚
- en: While Q-learning is a practical solution for environments with discrete action
    spaces and restricted observation spaces, it struggles to scale well to more complex
    environments. Indeed, creating a Q-table requires **defining** the **action**
    and **observation spaces**.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: å°½ç®¡Qå­¦ä¹ æ˜¯ç¦»æ•£è¡ŒåŠ¨ç©ºé—´å’Œå—é™è§‚å¯Ÿç©ºé—´ç¯å¢ƒçš„å®é™…è§£å†³æ–¹æ¡ˆï¼Œä½†åœ¨æ›´å¤æ‚çš„ç¯å¢ƒä¸­å¾ˆéš¾æ‰©å±•ã€‚äº‹å®ä¸Šï¼Œåˆ›å»ºQè¡¨éœ€è¦å®šä¹‰**è¡ŒåŠ¨**å’Œ**è§‚å¯Ÿç©ºé—´**ã€‚
- en: Consider the example of **autonomous driving**, the **observation space** is
    composed of an *infinity of potential configurations* derived from camera feeds
    and other sensory inputs. On the other hand, the **action space** includes a *wide
    spectrum of steering wheel positions* and varying levels of force applied to the
    brake and accelerator.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: è€ƒè™‘**è‡ªåŠ¨é©¾é©¶**çš„ä¾‹å­ï¼Œ**è§‚å¯Ÿç©ºé—´**ç”±æ¥è‡ªæ‘„åƒå¤´å’Œå…¶ä»–æ„ŸçŸ¥è¾“å…¥çš„*æ— é™æ½œåœ¨é…ç½®*ç»„æˆã€‚å¦ä¸€æ–¹é¢ï¼Œ**è¡ŒåŠ¨ç©ºé—´**åŒ…æ‹¬*å¹¿æ³›çš„æ–¹å‘ç›˜ä½ç½®*ä»¥åŠæ–½åŠ åˆ°åˆ¹è½¦å’Œæ²¹é—¨çš„ä¸åŒåŠ›åº¦ã€‚
- en: Even though we could theoretically discretize the action space, the sheer volume
    of possible states and actions leads to an **impractical Q-table** in **real-world
    applications**.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: å°½ç®¡ç†è®ºä¸Šæˆ‘ä»¬å¯ä»¥ç¦»æ•£åŒ–è¡ŒåŠ¨ç©ºé—´ï¼Œä½†å®é™…åº”ç”¨ä¸­å¯èƒ½ä¼šå¯¼è‡´**ä¸åˆ‡å®é™…çš„Qè¡¨**ï¼Œå› ä¸ºå¯èƒ½çš„çŠ¶æ€å’Œè¡ŒåŠ¨æ•°é‡åºå¤§ã€‚
- en: '![](../Images/69029c0c7e9ad43e30b3e37045cbfb99.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/69029c0c7e9ad43e30b3e37045cbfb99.png)'
- en: Photo by [Kirill Tonkikh](https://unsplash.com/@photophotostock?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '[Kirill Tonkikh](https://unsplash.com/@photophotostock?utm_source=medium&utm_medium=referral)çš„ç…§ç‰‡æ¥è‡ª[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)'
- en: Finding optimal actions in large and complex state-action spaces thus requires
    **powerful function approximation algorithms**, which is precisely what **Neural
    Networks** are. In the case of Deep Reinforcement Learning, neural nets are used
    as a **replacement for the Q-table** and provide an efficient solution to the
    *curse of dimensionality* introduced by large state spaces. Furthermore, we do
    not need to explicitly define the observation space.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å¤§å‹å¤æ‚çŠ¶æ€-åŠ¨ä½œç©ºé—´ä¸­å¯»æ‰¾æœ€ä¼˜è¡ŒåŠ¨å› æ­¤éœ€è¦**å¼ºå¤§çš„å‡½æ•°é€¼è¿‘ç®—æ³•**ï¼Œè¿™æ­£æ˜¯**ç¥ç»ç½‘ç»œ**æ‰€æ“…é•¿çš„ã€‚åœ¨æ·±åº¦å¼ºåŒ–å­¦ä¹ ä¸­ï¼Œç¥ç»ç½‘ç»œç”¨ä½œ**Qè¡¨çš„æ›¿ä»£å“**ï¼Œå¹¶ä¸ºå¤§çŠ¶æ€ç©ºé—´å¼•å…¥çš„*ç»´åº¦ç¾éš¾*æä¾›äº†é«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬ä¸éœ€è¦æ˜¾å¼å®šä¹‰è§‚å¯Ÿç©ºé—´ã€‚
- en: Deep Q-Networks & Replay Buffers
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ·±åº¦Qç½‘ç»œä¸é‡æ’­ç¼“å†²åŒº
- en: DQN uses two types of neural networks in parallel, starting with the â€œ***online***â€
    network which is used for **Q-value prediction** and **decision-making**. On the
    other hand, the â€œ***target***â€ network is used to **create stable Q-targets**
    to assess the performance of the online net via the loss function.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: DQNåŒæ—¶ä½¿ç”¨ä¸¤ç§ç±»å‹çš„ç¥ç»ç½‘ç»œï¼Œå¹¶è¡Œè¿›è¡Œï¼Œé¦–å…ˆæ˜¯ç”¨äº**Qå€¼é¢„æµ‹**å’Œ**å†³ç­–**çš„â€œ***åœ¨çº¿***â€ç½‘ç»œã€‚å¦ä¸€æ–¹é¢ï¼Œâ€œ***ç›®æ ‡***â€ç½‘ç»œç”¨äºé€šè¿‡æŸå¤±å‡½æ•°è¯„ä¼°åœ¨çº¿ç½‘ç»œçš„æ€§èƒ½ï¼Œä»¥ç”Ÿæˆ**ç¨³å®šçš„Qç›®æ ‡**ã€‚
- en: 'Similarly to Q-learning, DQN agents are defined by two functions: `act` and
    `update`.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸Qå­¦ä¹ ç±»ä¼¼ï¼ŒDQNä»£ç†ç”±ä¸¤ä¸ªå‡½æ•°å®šä¹‰ï¼š`act`å’Œ`update`ã€‚
- en: Act
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è¡ŒåŠ¨
- en: The `act` function implements an epsilon-greedy policy with respect to Q-values,
    which are estimated by the online neural network. In other words, the agent selects
    the action corresponding to the **maximum predicted Q-value** for a given state,
    with a set probability of acting randomly.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '`act`å‡½æ•°å®ç°äº†å…³äºQå€¼çš„Îµ-è´ªå¿ƒç­–ç•¥ï¼ŒQå€¼ç”±åœ¨çº¿ç¥ç»ç½‘ç»œä¼°è®¡ã€‚æ¢å¥è¯è¯´ï¼Œä»£ç†æ ¹æ®ç»™å®šçŠ¶æ€çš„**æœ€å¤§é¢„æµ‹Qå€¼**é€‰æ‹©åŠ¨ä½œï¼ŒåŒæ—¶ä»¥ä¸€å®šæ¦‚ç‡éšæœºæ‰§è¡ŒåŠ¨ä½œã€‚'
- en: You might remember that Q-learning updates its Q-table **after *every* step**,
    however, in Deep Learning it is common practice to compute updates using **gradient
    descent** on a **batch of inputs**.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨å¯èƒ½è¿˜è®°å¾—Qå­¦ä¹ åœ¨æ¯ä¸€æ­¥ä¹‹åæ›´æ–°å…¶Qè¡¨ï¼Œä½†åœ¨æ·±åº¦å­¦ä¹ ä¸­ï¼Œé€šå¸¸ä½¿ç”¨æ¢¯åº¦ä¸‹é™åœ¨**è¾“å…¥æ‰¹æ¬¡**ä¸Šè®¡ç®—æ›´æ–°ã€‚
- en: For this reason, DQN stores experiences (tuples containing `state, action, reward,
    next_state, done_flag`) in a **replay buffer**. To train the network, weâ€™ll sample
    a batch of experiences from this buffer instead of using only the last experience
    *(more details in the Replay Buffer section)*.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼ŒDQNå°†ç»éªŒï¼ˆåŒ…å«`state, action, reward, next_state, done_flag`çš„å…ƒç»„ï¼‰å­˜å‚¨åœ¨**é‡æ’­ç¼“å†²åŒº**ä¸­ã€‚ä¸ºäº†è®­ç»ƒç½‘ç»œï¼Œæˆ‘ä»¬å°†ä»æ­¤ç¼“å†²åŒºä¸­éšæœºæŠ½å–ä¸€æ‰¹ç»éªŒï¼Œè€Œä¸ä»…ä»…ä½¿ç”¨æœ€åä¸€æ¬¡ç»éªŒï¼ˆæœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è§é‡æ’­ç¼“å†²åŒºéƒ¨åˆ†ï¼‰ã€‚
- en: '![](../Images/7650171ae97fe0479b46e1490c1fdca0.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7650171ae97fe0479b46e1490c1fdca0.png)'
- en: Visual representation of **DQNâ€™s action selection** process (Made by the author)
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: å±•ç¤ºäº†**DQNè¡ŒåŠ¨é€‰æ‹©**è¿‡ç¨‹çš„è§†è§‰è¡¨ç¤ºï¼ˆä½œè€…åˆ¶ä½œï¼‰
- en: 'Hereâ€™s a JAX implementation of the action-selection part of DQN:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œæ˜¯DQNè¡ŒåŠ¨é€‰æ‹©éƒ¨åˆ†çš„JAXå®ç°ï¼š
- en: The only subtlety of this snippet is that the `model` attribute doesnâ€™t contain
    any internal parameters as is usually the case in frameworks such as PyTorch or
    TensorFlow.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: Here, the model is a **function** representing a **forward pass** through our
    architecture, but the ***mutable* weights are stored externally** and passed as
    **arguments**. This explains why we can use `jit` while passing the `self` argument
    as **static *(****the model being stateless as other class attributes)*.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: Update
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The `update` function is responsible for training the network. It computes
    a **mean squared error** (MSE) loss based on the **temporal-difference** (TD)
    **error**:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/449285430b8e3bdfe71e785562543881.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
- en: Mean Squared Error used in DQN
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: In this loss function, ***Î¸*** denotes the **parameters of the online network**,
    and ***Î¸*âˆ’** represents the **parameters of the target network**. The parameters
    of the target network are set on the online networkâ€™s parameters every *N* steps*,*
    similar to a *checkpoint* (*N is a hyperparameter).*
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: This separation of parameters (with *Î¸* for the current Q-values and *Î¸*âˆ’ for
    the target Q-values) is crucial to stabilize training.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: Using the same parameters for both would be similar to aiming at a moving target,
    as **updates to the network** would i**mmediately shift the target values**. By
    **periodically updating** ***Î¸*âˆ’** (i.e. freezing these parameters for a set number
    of steps), we ensure **stable Q-targets** while the online network continues to
    learn.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: Finally, the *(1-done)* term **adjusts the target** for **terminal states**.
    Indeed, when an episode ends (i.e. â€˜doneâ€™ is equal to 1), there is no next state.
    Therefore, the Q-value for the next state is set to 0.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6cc23c97e8034ed5aa9e046e8ca97139.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
- en: Visual representation of **DQNâ€™s parameter update** process (Made by the author)
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: 'Implementing the update function for DQN is slightly more complex, letâ€™s break
    it down:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: First, the `_loss_fn` function implements the squared error described previously
    for a **single experience**.
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Then, `_batch_loss_fn` acts as a wrapper for `_loss_fn` and decorates it with
    `vmap`, applying the loss function to a **batch of experiences**. We then return
    the average error for this batch.
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finally, `update` acts as a final layer to our loss function, computing its
    **gradient** with respect to the online network parameters, the target network
    parameters, and a batch of experiences. We then use **Optax** *(a JAX library
    commonly used for optimization)* to perform an optimizer step and update the online
    parameters.
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Notice that, similarly to the replay buffer, the model and optimizer are **pure
    functions** modifying an **external state**. The following line serves as a good
    illustration of this principle:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: This also explains why we can use a single model for both the online and target
    networks, as the parameters are stored and updated externally.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'For context, the model we use in this article is a *multi-layer perceptron*
    defined as follows:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Replay Buffer
  id: totrans-70
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now let us take a step back and look closer at replay buffers. They are widely
    used in reinforcement learning for a variety of reasons:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: '**Generalization**: By sampling from the replay buffer, we break the correlation
    between consecutive experiences by mixing up their order. This way, we avoid overfitting
    to specific sequences of experiences.'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Diversity**: As the sampling is not limited to recent experiences, we generally
    observe a lower variance in updates and prevent overfitting to the latest experiences.'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Increased sample efficiency**: Each experience can be sampled multiple times
    from the buffer, enabling the model to learn more from individual experiences.'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Finally, we can use several sampling schemes for our replay buffer:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: '**Uniform sampling:** Experiences are sampled uniformly at random. This type
    of sampling is straightforward to implement and allows the model to learn from
    experiences independently from the timestep they were collected.'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Prioritized sampling:** This category includes different algorithms such
    as **Prioritized Experience Replay** (â€œPERâ€, [*Schaul et al. 2015*](https://arxiv.org/abs/1511.05952)*)*
    or **Gradient Experience Replay** (â€œGERâ€, [*Lahire et al., 2022*](https://arxiv.org/abs/2110.01528)*).*
    These methods attempt to prioritize the selection of experiences according to
    some metric related to their â€œ*learning potentialâ€* (the amplitude of the TD error
    for PER and the norm of the experienceâ€™s gradient for GER).'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For the sake of simplicity, weâ€™ll implement a uniform replay buffer in this
    article. However, I plan to cover prioritized sampling extensively in the future.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: As promised, the uniform replay buffer is quite easy to implement, however,
    there are a few complexities related to the use of JAX and functional programming.
    As always, we have to work with **pure functions** that are **devoid of side effects**.
    In other words, we are not allowed to define the buffer as a class instance with
    a variable internal state.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
- en: Instead, we initialize a `buffer_state` dictionary that maps keys to empty arrays
    with predefined shapes, as JAX requires constant-sized arrays when jit-compiling
    code to XLA.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'We will use a `UniformReplayBuffer` class to interact with the buffer state.
    This class has two methods:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: '`add`: Unwraps an experience tuple and maps its components to a specific index.
    `idx = idx % self.buffer_size` ensures that when the buffer is full, adding new
    experiences overwrites older ones.'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`sample`: Samples a sequence of random indexes from the uniform random distribution.
    The sequence length is set by `batch_size` while the range of the indexes is `[0,
    current_buffer_size-1]`. This ensures that we do not sample empty arrays while
    the buffer is not yet full. Finally, we use JAXâ€™s `vmap` in combination with `tree_map`
    to return a batch of experiences.'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Translating the **CartPole** environment to **JAX**
  id: totrans-85
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that our DQN agent is ready for training, weâ€™ll quickly implement a vectorized
    CartPole environment using the same framework as introduced in an [earlier article](/vectorize-and-parallelize-rl-environments-with-jax-q-learning-at-the-speed-of-light-49d07373adf5).
    CartPole is a control environment having a **large continuous observation space,**
    which makes it relevant to test our DQN.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6c7f16ff97e5c92150e3a4ece0e1f843.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
- en: 'Visual representation of the CartPole Environment (credits and documentation:
    [OpenAI Gymnasium](https://gymnasium.farama.org/environments/classic_control/cart_pole/),
    MIT license)'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: 'The process is quite straightforward, we reuse most of [OpenAIâ€™s Gymnasium
    implementation](https://github.com/Farama-Foundation/Gymnasium/blob/main/gymnasium/envs/classic_control/cartpole.py)
    while making sure we use JAX arrays and lax control flow instead of Python or
    Numpy alternatives, for instance:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'For the sake of brevity, the full environment code is available here:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://github.com/RPegoud/jym/blob/main/src/envs/control/cartpole.py?source=post_page-----c1e45a179b92--------------------------------)
    [## jym/src/envs/control/cartpole.py at main Â· RPegoud/jym'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: JAX implementation of RL algorithms and vectorized environments - jym/src/envs/control/cartpole.py
    at main Â·â€¦
  id: totrans-93
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: github.com](https://github.com/RPegoud/jym/blob/main/src/envs/control/cartpole.py?source=post_page-----c1e45a179b92--------------------------------)
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: The **JAX** way to write **efficient training loops**
  id: totrans-95
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The last part of our implementation of DQN is the training loop *(also called
    rollout).* As mentioned in previous articles, we have to respect a specific format
    in order to take advantage of JAXâ€™s speed.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: 'The rollout function might appear daunting at first, but most of its complexity
    is purely syntactic as weâ€™ve already covered most of the building blocks. Hereâ€™s
    a pseudo-code walkthrough:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: We can now run DQN for **20,000 steps** and observe the performances. After
    around 45 episodes, the agent manages to obtain decent performances, balancing
    the pole for more than 100 steps consistently.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
- en: The **green bars** indicate that the agent managed to balance the pole for **more
    than 200 steps**, **solving the environment**. Notably, the agent set its record
    on the **51st episode**, with **393 steps**.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: Performance report for DQN (made by the author)
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: The **20.000 training steps** were executed in **just over a second**, at a
    rate of **15.807 steps per second** *(on a* ***single CPU****)*!
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: These performances hint at JAXâ€™s impressive scaling capabilities, allowing practitioners
    to run large-scale parallelized experiments with minimal hardware requirements.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Weâ€™ll take a closer look at **parallelized rollout procedures** to run **statistically
    significant** experiments and **hyperparameter searches** in a future article!
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: 'In the meantime, feel free to reproduce the experiment and dabble with hyperparameters
    using this notebook:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://github.com/RPegoud/jym/blob/main/notebooks/control/cartpole/dqn_cartpole.ipynb?source=post_page-----c1e45a179b92--------------------------------)
    [## jym/notebooks/control/cartpole/dqn_cartpole.ipynb at main Â· RPegoud/jym'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://github.com/RPegoud/jym/blob/main/notebooks/control/cartpole/dqn_cartpole.ipynb?source=post_page-----c1e45a179b92--------------------------------)
    [## jym/notebooks/control/cartpole/dqn_cartpole.ipynb at main Â· RPegoud/jym'
- en: JAX implementation of RL algorithms and vectorized environments - jym/notebooks/control/cartpole/dqn_cartpole.ipynb
    atâ€¦
  id: totrans-108
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: JAX å¼ºåŒ–å­¦ä¹ ç®—æ³•å’Œå‘é‡åŒ–ç¯å¢ƒçš„å®ç° - jym/notebooks/control/cartpole/dqn_cartpole.ipynb atâ€¦
- en: github.com](https://github.com/RPegoud/jym/blob/main/notebooks/control/cartpole/dqn_cartpole.ipynb?source=post_page-----c1e45a179b92--------------------------------)
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: github.com](https://github.com/RPegoud/jym/blob/main/notebooks/control/cartpole/dqn_cartpole.ipynb?source=post_page-----c1e45a179b92--------------------------------)
- en: Conclusion
  id: totrans-110
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç»“è®º
- en: As always, **thanks for reading this far!** I hope this article provided a decent
    introduction to Deep RL in JAX. Should you have any questions or feedback related
    to the content of this article, make sure to let me know, Iâ€™m always happy to
    have a little chat ;)
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚å¾€å¸¸ä¸€æ ·ï¼Œ**æ„Ÿè°¢æ‚¨è¯»åˆ°è¿™é‡Œï¼**å¸Œæœ›æœ¬æ–‡ä¸ºæ‚¨åœ¨ JAX ä¸­çš„æ·±åº¦å¼ºåŒ–å­¦ä¹ æä¾›äº†ä¸€ä¸ªä¸é”™çš„ä»‹ç»ã€‚å¦‚æœæ‚¨å¯¹æœ¬æ–‡å†…å®¹æœ‰ä»»ä½•é—®é¢˜æˆ–åé¦ˆï¼Œè¯·åŠ¡å¿…å‘Šè¯‰æˆ‘ï¼Œæˆ‘æ€»æ˜¯ä¹æ„èŠä¸€èŠ
    ;)
- en: Until next time ğŸ‘‹
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: ç›´åˆ°ä¸‹æ¬¡è§é¢ ğŸ‘‹
- en: 'Credits:'
  id: totrans-113
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: è‡´è°¢ï¼š
- en: '[Cartpole Gif](https://gymnasium.farama.org/environments/classic_control/cart_pole/),
    OpenAI Gymnasium library, (MIT license)'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Cartpole Gif](https://gymnasium.farama.org/environments/classic_control/cart_pole/)ï¼ŒOpenAI
    Gymnasium åº“ï¼Œï¼ˆMIT è®¸å¯è¯ï¼‰'
