- en: A Gentle Introduction to Deep Reinforcement Learning in JAX
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 《深入浅出 JAX 中的深度强化学习》
- en: 原文：[https://towardsdatascience.com/a-gentle-introduction-to-deep-reinforcement-learning-in-jax-c1e45a179b92?source=collection_archive---------4-----------------------#2023-11-21](https://towardsdatascience.com/a-gentle-introduction-to-deep-reinforcement-learning-in-jax-c1e45a179b92?source=collection_archive---------4-----------------------#2023-11-21)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/a-gentle-introduction-to-deep-reinforcement-learning-in-jax-c1e45a179b92?source=collection_archive---------4-----------------------#2023-11-21](https://towardsdatascience.com/a-gentle-introduction-to-deep-reinforcement-learning-in-jax-c1e45a179b92?source=collection_archive---------4-----------------------#2023-11-21)
- en: Solving the CartPole environment with DQN in under a second
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在一秒钟内用 DQN 解决 CartPole 环境
- en: '[](https://medium.com/@ryanpegoud?source=post_page-----c1e45a179b92--------------------------------)[![Ryan
    Pégoud](../Images/9314b76c2be56bda8b73b4badf9e3e4d.png)](https://medium.com/@ryanpegoud?source=post_page-----c1e45a179b92--------------------------------)[](https://towardsdatascience.com/?source=post_page-----c1e45a179b92--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----c1e45a179b92--------------------------------)
    [Ryan Pégoud](https://medium.com/@ryanpegoud?source=post_page-----c1e45a179b92--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@ryanpegoud?source=post_page-----c1e45a179b92--------------------------------)[![Ryan
    Pégoud](../Images/9314b76c2be56bda8b73b4badf9e3e4d.png)](https://medium.com/@ryanpegoud?source=post_page-----c1e45a179b92--------------------------------)[](https://towardsdatascience.com/?source=post_page-----c1e45a179b92--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----c1e45a179b92--------------------------------)
    [Ryan Pégoud](https://medium.com/@ryanpegoud?source=post_page-----c1e45a179b92--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F27fba63b402e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-gentle-introduction-to-deep-reinforcement-learning-in-jax-c1e45a179b92&user=Ryan+P%C3%A9goud&userId=27fba63b402e&source=post_page-27fba63b402e----c1e45a179b92---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----c1e45a179b92--------------------------------)
    ·10 min read·Nov 21, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fc1e45a179b92&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-gentle-introduction-to-deep-reinforcement-learning-in-jax-c1e45a179b92&user=Ryan+P%C3%A9goud&userId=27fba63b402e&source=-----c1e45a179b92---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F27fba63b402e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-gentle-introduction-to-deep-reinforcement-learning-in-jax-c1e45a179b92&user=Ryan+P%C3%A9goud&userId=27fba63b402e&source=post_page-27fba63b402e----c1e45a179b92---------------------post_header-----------)
    发表在[Towards Data Science](https://towardsdatascience.com/?source=post_page-----c1e45a179b92--------------------------------)
    · 10分钟阅读·2023年11月21日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fc1e45a179b92&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-gentle-introduction-to-deep-reinforcement-learning-in-jax-c1e45a179b92&user=Ryan+P%C3%A9goud&userId=27fba63b402e&source=-----c1e45a179b92---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc1e45a179b92&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-gentle-introduction-to-deep-reinforcement-learning-in-jax-c1e45a179b92&source=-----c1e45a179b92---------------------bookmark_footer-----------)![](../Images/af8f701ac536881e802dd7507bdc46f2.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc1e45a179b92&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-gentle-introduction-to-deep-reinforcement-learning-in-jax-c1e45a179b92&source=-----c1e45a179b92---------------------bookmark_footer-----------)![](../Images/af8f701ac536881e802dd7507bdc46f2.png)'
- en: Photo by [Thomas Despeyroux](https://unsplash.com/@thomasdes?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 由[Thomas Despeyroux](https://unsplash.com/@thomasdes?utm_source=medium&utm_medium=referral)拍摄，发布于[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: Recent progress in Reinforcement Learning (RL), such as Waymo’s autonomous taxis
    or DeepMind’s superhuman chess-playing agents, complement **classical RL** with
    **Deep Learning** components such as **Neural Networks** and **Gradient Optimization**
    methods.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 最近在强化学习（RL）方面的进展，例如 Waymo 的自动驾驶出租车或 DeepMind 的超人类棋类代理，结合了**经典 RL**和**深度学习**组件，如**神经网络**和**梯度优化**方法。
- en: Building on the foundations and coding principles introduced in one of my previous
    stories, we’ll discover and learn to implement **Deep Q-Networks** (**DQN**) and
    **replay buffers** to solve OpenAI’s **CartPole** environment. All of that **in
    under a second** using JAX!
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前介绍的基础和编码原则上构建，我们将探索并学习如何使用JAX实现**深度Q网络**（**DQN**）和**回放缓冲区**来解决OpenAI的**CartPole**环境。所有这些操作都在**不到一秒**的时间内完成！
- en: 'For an introduction to **JAX**, **vectorized environments**, and **Q-learning**,
    please refer to the content of this story:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 对于**JAX**、**向量化环境**和**Q-learning**的介绍，请参阅以下内容：
- en: '[](/vectorize-and-parallelize-rl-environments-with-jax-q-learning-at-the-speed-of-light-49d07373adf5?source=post_page-----c1e45a179b92--------------------------------)
    [## Vectorize and Parallelize RL Environments with JAX: Q-learning at the Speed
    of Light⚡'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/vectorize-and-parallelize-rl-environments-with-jax-q-learning-at-the-speed-of-light-49d07373adf5?source=post_page-----c1e45a179b92--------------------------------)
    [## 使用JAX向量化和并行化RL环境：Q-learning的光速⚡'
- en: Learn to vectorize a GridWorld environment and train 30 Q-learning agents in
    parallel on a CPU, at 1.8 million step per…
  id: totrans-13
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 学习如何在CPU上向量化GridWorld环境，并同时训练30个Q-learning代理，每个代理进行180万步…
- en: towardsdatascience.com](/vectorize-and-parallelize-rl-environments-with-jax-q-learning-at-the-speed-of-light-49d07373adf5?source=post_page-----c1e45a179b92--------------------------------)
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/vectorize-and-parallelize-rl-environments-with-jax-q-learning-at-the-speed-of-light-49d07373adf5?source=post_page-----c1e45a179b92--------------------------------)
- en: 'Our framework of choice for deep learning will be DeepMind’s **Haiku** library,
    which I recently introduced in the context of Transformers:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我们选择的深度学习框架是DeepMind的**Haiku**库，我最近在Transformer的上下文中介绍过：
- en: '[](/implementing-a-transformer-encoder-from-scratch-with-jax-and-haiku-791d31b4f0dd?source=post_page-----c1e45a179b92--------------------------------)
    [## Implementing a Transformer Encoder from Scratch with JAX and Haiku 🤖'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/implementing-a-transformer-encoder-from-scratch-with-jax-and-haiku-791d31b4f0dd?source=post_page-----c1e45a179b92--------------------------------)
    [## 使用JAX和Haiku从头开始实现Transformer编码器 🤖'
- en: Understanding the fundamental building blocks of Transformers.
  id: totrans-17
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 理解Transformer的基础构建模块。
- en: towardsdatascience.com](/implementing-a-transformer-encoder-from-scratch-with-jax-and-haiku-791d31b4f0dd?source=post_page-----c1e45a179b92--------------------------------)
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/implementing-a-transformer-encoder-from-scratch-with-jax-and-haiku-791d31b4f0dd?source=post_page-----c1e45a179b92--------------------------------)
- en: 'This article will cover the following sections:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 本文将涵盖以下几个部分：
- en: '**Why** do we need Deep RL?'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**为什么**我们需要深度RL？'
- en: '**Deep Q-Networks,** *theory and practice*'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**深度Q网络**的**理论和实践**'
- en: '**Replay Buffers**'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**回放缓冲区**'
- en: Translating the **CartPole** environment to **JAX**
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将**CartPole**环境转换为**JAX**
- en: The **JAX** way to write **efficient training loops**
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**JAX**编写**高效训练循环**的方式'
- en: '*As always, all the code presented in this article is available on GitHub:*'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '*如往常一样，本文中提供的所有代码都可在GitHub上找到：*'
- en: '[](https://github.com/RPegoud/jym?source=post_page-----c1e45a179b92--------------------------------)
    [## GitHub - RPegoud/jym: JAX implementation of RL algorithms and vectorized environments'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://github.com/RPegoud/jym?source=post_page-----c1e45a179b92--------------------------------)
    [## GitHub - RPegoud/jym：JAX实现的RL算法和向量化环境'
- en: 'JAX implementation of RL algorithms and vectorized environments - GitHub -
    RPegoud/jym: JAX implementation of RL…'
  id: totrans-27
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 'JAX实现的RL算法和向量化环境 - GitHub - RPegoud/jym: JAX实现的RL…'
- en: github.com](https://github.com/RPegoud/jym?source=post_page-----c1e45a179b92--------------------------------)
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: github.com](https://github.com/RPegoud/jym?source=post_page-----c1e45a179b92--------------------------------)
- en: '**Why** do we need Deep RL?'
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**为什么**我们需要深度RL？'
- en: In previous articles, we introduced [Temporal Difference Learning](https://medium.com/towards-data-science/temporal-difference-learning-and-the-importance-of-exploration-an-illustrated-guide-5f9c3371413a)
    algorithms and in particular [Q-learning](/vectorize-and-parallelize-rl-environments-with-jax-q-learning-at-the-speed-of-light-49d07373adf5).
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的文章中，我们介绍了[时间差分学习](https://medium.com/towards-data-science/temporal-difference-learning-and-the-importance-of-exploration-an-illustrated-guide-5f9c3371413a)算法，特别是[Q-learning](/vectorize-and-parallelize-rl-environments-with-jax-q-learning-at-the-speed-of-light-49d07373adf5)。
- en: Simply put, Q-learning is an **off-policy** algorithm *(the target policy is
    not the policy used for decision-making)* maintaining and updating a **Q-table**,
    an explicit **mapping** of **states** to corresponding **action values**.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 简单来说，Q-learning是一种**离策略**算法（*目标策略与用于决策的策略不同*），用于维护和更新**Q表**，一个明确的**状态**到相应**动作值**的**映射**。
- en: While Q-learning is a practical solution for environments with discrete action
    spaces and restricted observation spaces, it struggles to scale well to more complex
    environments. Indeed, creating a Q-table requires **defining** the **action**
    and **observation spaces**.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管Q学习是离散行动空间和受限观察空间环境的实际解决方案，但在更复杂的环境中很难扩展。事实上，创建Q表需要定义**行动**和**观察空间**。
- en: Consider the example of **autonomous driving**, the **observation space** is
    composed of an *infinity of potential configurations* derived from camera feeds
    and other sensory inputs. On the other hand, the **action space** includes a *wide
    spectrum of steering wheel positions* and varying levels of force applied to the
    brake and accelerator.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑**自动驾驶**的例子，**观察空间**由来自摄像头和其他感知输入的*无限潜在配置*组成。另一方面，**行动空间**包括*广泛的方向盘位置*以及施加到刹车和油门的不同力度。
- en: Even though we could theoretically discretize the action space, the sheer volume
    of possible states and actions leads to an **impractical Q-table** in **real-world
    applications**.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管理论上我们可以离散化行动空间，但实际应用中可能会导致**不切实际的Q表**，因为可能的状态和行动数量庞大。
- en: '![](../Images/69029c0c7e9ad43e30b3e37045cbfb99.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/69029c0c7e9ad43e30b3e37045cbfb99.png)'
- en: Photo by [Kirill Tonkikh](https://unsplash.com/@photophotostock?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '[Kirill Tonkikh](https://unsplash.com/@photophotostock?utm_source=medium&utm_medium=referral)的照片来自[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)'
- en: Finding optimal actions in large and complex state-action spaces thus requires
    **powerful function approximation algorithms**, which is precisely what **Neural
    Networks** are. In the case of Deep Reinforcement Learning, neural nets are used
    as a **replacement for the Q-table** and provide an efficient solution to the
    *curse of dimensionality* introduced by large state spaces. Furthermore, we do
    not need to explicitly define the observation space.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在大型复杂状态-动作空间中寻找最优行动因此需要**强大的函数逼近算法**，这正是**神经网络**所擅长的。在深度强化学习中，神经网络用作**Q表的替代品**，并为大状态空间引入的*维度灾难*提供了高效的解决方案。此外，我们不需要显式定义观察空间。
- en: Deep Q-Networks & Replay Buffers
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度Q网络与重播缓冲区
- en: DQN uses two types of neural networks in parallel, starting with the “***online***”
    network which is used for **Q-value prediction** and **decision-making**. On the
    other hand, the “***target***” network is used to **create stable Q-targets**
    to assess the performance of the online net via the loss function.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: DQN同时使用两种类型的神经网络，并行进行，首先是用于**Q值预测**和**决策**的“***在线***”网络。另一方面，“***目标***”网络用于通过损失函数评估在线网络的性能，以生成**稳定的Q目标**。
- en: 'Similarly to Q-learning, DQN agents are defined by two functions: `act` and
    `update`.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 与Q学习类似，DQN代理由两个函数定义：`act`和`update`。
- en: Act
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 行动
- en: The `act` function implements an epsilon-greedy policy with respect to Q-values,
    which are estimated by the online neural network. In other words, the agent selects
    the action corresponding to the **maximum predicted Q-value** for a given state,
    with a set probability of acting randomly.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '`act`函数实现了关于Q值的ε-贪心策略，Q值由在线神经网络估计。换句话说，代理根据给定状态的**最大预测Q值**选择动作，同时以一定概率随机执行动作。'
- en: You might remember that Q-learning updates its Q-table **after *every* step**,
    however, in Deep Learning it is common practice to compute updates using **gradient
    descent** on a **batch of inputs**.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能还记得Q学习在每一步之后更新其Q表，但在深度学习中，通常使用梯度下降在**输入批次**上计算更新。
- en: For this reason, DQN stores experiences (tuples containing `state, action, reward,
    next_state, done_flag`) in a **replay buffer**. To train the network, we’ll sample
    a batch of experiences from this buffer instead of using only the last experience
    *(more details in the Replay Buffer section)*.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，DQN将经验（包含`state, action, reward, next_state, done_flag`的元组）存储在**重播缓冲区**中。为了训练网络，我们将从此缓冲区中随机抽取一批经验，而不仅仅使用最后一次经验（有关更多详细信息，请参见重播缓冲区部分）。
- en: '![](../Images/7650171ae97fe0479b46e1490c1fdca0.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7650171ae97fe0479b46e1490c1fdca0.png)'
- en: Visual representation of **DQN’s action selection** process (Made by the author)
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 展示了**DQN行动选择**过程的视觉表示（作者制作）
- en: 'Here’s a JAX implementation of the action-selection part of DQN:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是DQN行动选择部分的JAX实现：
- en: The only subtlety of this snippet is that the `model` attribute doesn’t contain
    any internal parameters as is usually the case in frameworks such as PyTorch or
    TensorFlow.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 这个代码片段的唯一细微之处在于 `model` 属性不包含任何内部参数，这与 PyTorch 或 TensorFlow 等框架中通常的情况不同。
- en: Here, the model is a **function** representing a **forward pass** through our
    architecture, but the ***mutable* weights are stored externally** and passed as
    **arguments**. This explains why we can use `jit` while passing the `self` argument
    as **static *(****the model being stateless as other class attributes)*.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，模型是一个 **函数**，表示我们架构中的 **前向传递**，但 ***可变的* 权重是外部存储** 并作为 **参数** 传递。这解释了为什么我们可以在传递
    `self` 参数时使用 `jit`，作为 **静态*（**模型在其他类属性中是无状态的）*。
- en: Update
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 更新
- en: 'The `update` function is responsible for training the network. It computes
    a **mean squared error** (MSE) loss based on the **temporal-difference** (TD)
    **error**:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '`update` 函数负责训练网络。它根据 **时间差**（TD） **误差** 计算 **均方误差**（MSE）损失：'
- en: '![](../Images/449285430b8e3bdfe71e785562543881.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/449285430b8e3bdfe71e785562543881.png)'
- en: Mean Squared Error used in DQN
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: DQN中使用的均方误差
- en: In this loss function, ***θ*** denotes the **parameters of the online network**,
    and ***θ*−** represents the **parameters of the target network**. The parameters
    of the target network are set on the online network’s parameters every *N* steps*,*
    similar to a *checkpoint* (*N is a hyperparameter).*
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个损失函数中，***θ*** 表示 **在线网络的参数**，而 ***θ*−** 代表 **目标网络的参数**。目标网络的参数每隔 *N* 步被设置为在线网络的参数，类似于一个
    *检查点* （*N 是一个超参数*）。
- en: This separation of parameters (with *θ* for the current Q-values and *θ*− for
    the target Q-values) is crucial to stabilize training.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 参数的分离（当前Q值的 *θ* 和目标Q值的 *θ*−）对于稳定训练至关重要。
- en: Using the same parameters for both would be similar to aiming at a moving target,
    as **updates to the network** would i**mmediately shift the target values**. By
    **periodically updating** ***θ*−** (i.e. freezing these parameters for a set number
    of steps), we ensure **stable Q-targets** while the online network continues to
    learn.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 如果对两个网络使用相同的参数，就类似于瞄准一个移动的目标，因为 **对网络的更新** 会 **立即改变目标值**。通过 **定期更新** ***θ*−**（即在设定步数内冻结这些参数），我们确保了
    **稳定的Q目标**，同时在线网络继续学习。
- en: Finally, the *(1-done)* term **adjusts the target** for **terminal states**.
    Indeed, when an episode ends (i.e. ‘done’ is equal to 1), there is no next state.
    Therefore, the Q-value for the next state is set to 0.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，*(1-done)* 项 **调整目标** 用于 **终止状态**。实际上，当一个回合结束时（即 ‘done’ 等于 1），没有下一个状态。因此，下一状态的Q值被设为0。
- en: '![](../Images/6cc23c97e8034ed5aa9e046e8ca97139.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6cc23c97e8034ed5aa9e046e8ca97139.png)'
- en: Visual representation of **DQN’s parameter update** process (Made by the author)
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '**DQN参数更新** 过程的可视化表示（作者制作）'
- en: 'Implementing the update function for DQN is slightly more complex, let’s break
    it down:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 实现DQN的更新函数稍微复杂一些，我们分解一下：
- en: First, the `_loss_fn` function implements the squared error described previously
    for a **single experience**.
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 首先，`_loss_fn` 函数实现了之前描述的用于 **单一经验** 的平方误差。
- en: Then, `_batch_loss_fn` acts as a wrapper for `_loss_fn` and decorates it with
    `vmap`, applying the loss function to a **batch of experiences**. We then return
    the average error for this batch.
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后，`_batch_loss_fn` 作为 `_loss_fn` 的包装器，并通过 `vmap` 装饰它，将损失函数应用于 **一批经验**。然后我们返回这一批的平均误差。
- en: Finally, `update` acts as a final layer to our loss function, computing its
    **gradient** with respect to the online network parameters, the target network
    parameters, and a batch of experiences. We then use **Optax** *(a JAX library
    commonly used for optimization)* to perform an optimizer step and update the online
    parameters.
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，`update` 作为我们损失函数的最终层，计算其 **梯度** 相对于在线网络参数、目标网络参数以及一批经验。然后我们使用 **Optax**
    *（一个常用于优化的JAX库）* 执行优化步骤并更新在线参数。
- en: 'Notice that, similarly to the replay buffer, the model and optimizer are **pure
    functions** modifying an **external state**. The following line serves as a good
    illustration of this principle:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，与回放缓冲区类似，模型和优化器是 **纯函数**，修改 **外部状态**。以下一行很好地说明了这一原则：
- en: '[PRE0]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: This also explains why we can use a single model for both the online and target
    networks, as the parameters are stored and updated externally.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 这也解释了为什么我们可以对在线网络和目标网络使用一个模型，因为参数是外部存储和更新的。
- en: '[PRE1]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'For context, the model we use in this article is a *multi-layer perceptron*
    defined as follows:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 为了提供背景，我们在本文中使用的模型是一个*多层感知机*，定义如下：
- en: '[PRE2]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Replay Buffer
  id: totrans-70
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 重放缓冲区
- en: 'Now let us take a step back and look closer at replay buffers. They are widely
    used in reinforcement learning for a variety of reasons:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们退一步，更详细地看一下重放缓冲区。它们在强化学习中被广泛使用，原因有很多：
- en: '**Generalization**: By sampling from the replay buffer, we break the correlation
    between consecutive experiences by mixing up their order. This way, we avoid overfitting
    to specific sequences of experiences.'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**泛化：** 通过从重放缓冲区采样，我们打破了连续经验之间的相关性，通过混合它们的顺序。这种方式避免了对特定经验序列的过拟合。'
- en: '**Diversity**: As the sampling is not limited to recent experiences, we generally
    observe a lower variance in updates and prevent overfitting to the latest experiences.'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**多样性：** 由于采样不限于最近的经验，我们通常会观察到更新的方差较低，并且避免对最新经验的过拟合。'
- en: '**Increased sample efficiency**: Each experience can be sampled multiple times
    from the buffer, enabling the model to learn more from individual experiences.'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**增加样本效率：** 每个经验可以从缓冲区中被多次采样，使模型能够从个体经验中学习更多。'
- en: 'Finally, we can use several sampling schemes for our replay buffer:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以使用几种采样方案来管理我们的重放缓冲区：
- en: '**Uniform sampling:** Experiences are sampled uniformly at random. This type
    of sampling is straightforward to implement and allows the model to learn from
    experiences independently from the timestep they were collected.'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**均匀采样：** 经验以均匀的随机方式进行采样。这种采样类型易于实现，并允许模型从经验中独立于它们被收集的时间步长中学习。'
- en: '**Prioritized sampling:** This category includes different algorithms such
    as **Prioritized Experience Replay** (“PER”, [*Schaul et al. 2015*](https://arxiv.org/abs/1511.05952)*)*
    or **Gradient Experience Replay** (“GER”, [*Lahire et al., 2022*](https://arxiv.org/abs/2110.01528)*).*
    These methods attempt to prioritize the selection of experiences according to
    some metric related to their “*learning potential”* (the amplitude of the TD error
    for PER and the norm of the experience’s gradient for GER).'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**优先采样：** 这个类别包括不同的算法，如**优先经验重放**（“PER”，[*Schaul等，2015*](https://arxiv.org/abs/1511.05952)）或**梯度经验重放**（“GER”，[*Lahire等，2022*](https://arxiv.org/abs/2110.01528)）。这些方法试图根据与其“*学习潜力*”（PER的TD误差幅度和GER的经验梯度的范数）相关的某些指标优先选择经验。'
- en: For the sake of simplicity, we’ll implement a uniform replay buffer in this
    article. However, I plan to cover prioritized sampling extensively in the future.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简单起见，我们将在本文中实现一个均匀重放缓冲区。然而，我计划在未来详细讨论优先采样。
- en: As promised, the uniform replay buffer is quite easy to implement, however,
    there are a few complexities related to the use of JAX and functional programming.
    As always, we have to work with **pure functions** that are **devoid of side effects**.
    In other words, we are not allowed to define the buffer as a class instance with
    a variable internal state.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 正如承诺的那样，均匀重放缓冲区的实现非常简单，但与JAX和函数式编程的使用相关的一些复杂性需要解决。与往常一样，我们必须使用**纯函数**，这些函数**没有副作用**。换句话说，我们不能将缓冲区定义为具有变量内部状态的类实例。
- en: Instead, we initialize a `buffer_state` dictionary that maps keys to empty arrays
    with predefined shapes, as JAX requires constant-sized arrays when jit-compiling
    code to XLA.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，我们初始化一个`buffer_state`字典，该字典将键映射到具有预定义形状的空数组，因为JAX在对XLA进行JIT编译时要求固定大小的数组。
- en: '[PRE3]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'We will use a `UniformReplayBuffer` class to interact with the buffer state.
    This class has two methods:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用`UniformReplayBuffer`类与缓冲区状态进行交互。这个类有两个方法：
- en: '`add`: Unwraps an experience tuple and maps its components to a specific index.
    `idx = idx % self.buffer_size` ensures that when the buffer is full, adding new
    experiences overwrites older ones.'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`add`：解开一个经验元组，并将其组件映射到特定索引。`idx = idx % self.buffer_size`确保当缓冲区满时，添加的新经验会覆盖旧的经验。'
- en: '`sample`: Samples a sequence of random indexes from the uniform random distribution.
    The sequence length is set by `batch_size` while the range of the indexes is `[0,
    current_buffer_size-1]`. This ensures that we do not sample empty arrays while
    the buffer is not yet full. Finally, we use JAX’s `vmap` in combination with `tree_map`
    to return a batch of experiences.'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sample`：从均匀随机分布中采样一系列随机索引。序列长度由`batch_size`设定，而索引的范围为`[0, current_buffer_size-1]`。这确保了在缓冲区尚未满时不会采样到空数组。最后，我们使用JAX的`vmap`结合`tree_map`返回一批经验。'
- en: Translating the **CartPole** environment to **JAX**
  id: totrans-85
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将**CartPole**环境转换为**JAX**
- en: Now that our DQN agent is ready for training, we’ll quickly implement a vectorized
    CartPole environment using the same framework as introduced in an [earlier article](/vectorize-and-parallelize-rl-environments-with-jax-q-learning-at-the-speed-of-light-49d07373adf5).
    CartPole is a control environment having a **large continuous observation space,**
    which makes it relevant to test our DQN.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们的DQN代理已准备好进行训练，我们将快速实现一个使用与[早期文章](/vectorize-and-parallelize-rl-environments-with-jax-q-learning-at-the-speed-of-light-49d07373adf5)介绍的相同框架的矢量化CartPole环境。
    CartPole是一个具有**大型连续观察空间**的控制环境，这使得测试我们的DQN变得相关。
- en: '![](../Images/6c7f16ff97e5c92150e3a4ece0e1f843.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6c7f16ff97e5c92150e3a4ece0e1f843.png)'
- en: 'Visual representation of the CartPole Environment (credits and documentation:
    [OpenAI Gymnasium](https://gymnasium.farama.org/environments/classic_control/cart_pole/),
    MIT license)'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: CartPole环境的可视化表示（鸣谢和文档：[OpenAI Gymnasium](https://gymnasium.farama.org/environments/classic_control/cart_pole/)，MIT许可证）
- en: 'The process is quite straightforward, we reuse most of [OpenAI’s Gymnasium
    implementation](https://github.com/Farama-Foundation/Gymnasium/blob/main/gymnasium/envs/classic_control/cartpole.py)
    while making sure we use JAX arrays and lax control flow instead of Python or
    Numpy alternatives, for instance:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 这个过程非常简单，我们大部分都重用了[OpenAI的Gymnasium实现](https://github.com/Farama-Foundation/Gymnasium/blob/main/gymnasium/envs/classic_control/cartpole.py)，同时确保我们使用JAX数组和lax控制流，而不是Python或Numpy的替代方案，例如：
- en: '[PRE4]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'For the sake of brevity, the full environment code is available here:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简洁起见，完整的环境代码在此处可用：
- en: '[](https://github.com/RPegoud/jym/blob/main/src/envs/control/cartpole.py?source=post_page-----c1e45a179b92--------------------------------)
    [## jym/src/envs/control/cartpole.py at main · RPegoud/jym'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://github.com/RPegoud/jym/blob/main/src/envs/control/cartpole.py?source=post_page-----c1e45a179b92--------------------------------)
    [## jym/src/envs/control/cartpole.py at main · RPegoud/jym'
- en: JAX implementation of RL algorithms and vectorized environments - jym/src/envs/control/cartpole.py
    at main ·…
  id: totrans-93
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: JAX实现的RL算法和矢量化环境 - jym/src/envs/control/cartpole.py at main ·…
- en: github.com](https://github.com/RPegoud/jym/blob/main/src/envs/control/cartpole.py?source=post_page-----c1e45a179b92--------------------------------)
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '[github.com](https://github.com/RPegoud/jym/blob/main/src/envs/control/cartpole.py?source=post_page-----c1e45a179b92--------------------------------)'
- en: The **JAX** way to write **efficient training loops**
  id: totrans-95
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 以**JAX**的方式编写**高效的训练循环**
- en: The last part of our implementation of DQN is the training loop *(also called
    rollout).* As mentioned in previous articles, we have to respect a specific format
    in order to take advantage of JAX’s speed.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: DQN实现的最后部分是训练循环（也称为推演）。 正如前文所述，为了利用JAX的速度，我们必须遵守特定的格式。
- en: 'The rollout function might appear daunting at first, but most of its complexity
    is purely syntactic as we’ve already covered most of the building blocks. Here’s
    a pseudo-code walkthrough:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 推演函数可能一开始看起来令人生畏，但它大部分的复杂性纯粹是语法上的，因为我们已经涵盖了大多数构建块。 这是一个伪代码演示：
- en: '[PRE5]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: We can now run DQN for **20,000 steps** and observe the performances. After
    around 45 episodes, the agent manages to obtain decent performances, balancing
    the pole for more than 100 steps consistently.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以运行DQN进行**20,000步**并观察其表现。 大约在45集后，代理成功地保持了超过100步的稳定平衡。
- en: The **green bars** indicate that the agent managed to balance the pole for **more
    than 200 steps**, **solving the environment**. Notably, the agent set its record
    on the **51st episode**, with **393 steps**.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '**绿色条**表示代理成功地在**200多步**内平衡了杆，**解决了环境**。 值得注意的是，代理在**第51集**上创下了**393步**的记录。'
- en: Performance report for DQN (made by the author)
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: DQN的性能报告（作者制作）
- en: The **20.000 training steps** were executed in **just over a second**, at a
    rate of **15.807 steps per second** *(on a* ***single CPU****)*!
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '**20,000训练步骤**在**一秒多一点**内执行完毕，速度为**每秒15,807步**（*在单个CPU上*）！'
- en: These performances hint at JAX’s impressive scaling capabilities, allowing practitioners
    to run large-scale parallelized experiments with minimal hardware requirements.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 这些表现提示了JAX令人印象深刻的扩展能力，允许从业者使用最小的硬件要求进行大规模并行实验。
- en: '[PRE6]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: We’ll take a closer look at **parallelized rollout procedures** to run **statistically
    significant** experiments and **hyperparameter searches** in a future article!
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将更详细地看看**并行化的推演过程**，以运行**具有统计显著性**的实验和**超参数搜索**在未来的文章中！
- en: 'In the meantime, feel free to reproduce the experiment and dabble with hyperparameters
    using this notebook:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 与此同时，随时可以使用这本笔记本重现实验并尝试不同的超参数：
- en: '[](https://github.com/RPegoud/jym/blob/main/notebooks/control/cartpole/dqn_cartpole.ipynb?source=post_page-----c1e45a179b92--------------------------------)
    [## jym/notebooks/control/cartpole/dqn_cartpole.ipynb at main · RPegoud/jym'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://github.com/RPegoud/jym/blob/main/notebooks/control/cartpole/dqn_cartpole.ipynb?source=post_page-----c1e45a179b92--------------------------------)
    [## jym/notebooks/control/cartpole/dqn_cartpole.ipynb at main · RPegoud/jym'
- en: JAX implementation of RL algorithms and vectorized environments - jym/notebooks/control/cartpole/dqn_cartpole.ipynb
    at…
  id: totrans-108
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: JAX 强化学习算法和向量化环境的实现 - jym/notebooks/control/cartpole/dqn_cartpole.ipynb at…
- en: github.com](https://github.com/RPegoud/jym/blob/main/notebooks/control/cartpole/dqn_cartpole.ipynb?source=post_page-----c1e45a179b92--------------------------------)
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: github.com](https://github.com/RPegoud/jym/blob/main/notebooks/control/cartpole/dqn_cartpole.ipynb?source=post_page-----c1e45a179b92--------------------------------)
- en: Conclusion
  id: totrans-110
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: As always, **thanks for reading this far!** I hope this article provided a decent
    introduction to Deep RL in JAX. Should you have any questions or feedback related
    to the content of this article, make sure to let me know, I’m always happy to
    have a little chat ;)
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 如往常一样，**感谢您读到这里！**希望本文为您在 JAX 中的深度强化学习提供了一个不错的介绍。如果您对本文内容有任何问题或反馈，请务必告诉我，我总是乐意聊一聊
    ;)
- en: Until next time 👋
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 直到下次见面 👋
- en: 'Credits:'
  id: totrans-113
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 致谢：
- en: '[Cartpole Gif](https://gymnasium.farama.org/environments/classic_control/cart_pole/),
    OpenAI Gymnasium library, (MIT license)'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Cartpole Gif](https://gymnasium.farama.org/environments/classic_control/cart_pole/)，OpenAI
    Gymnasium 库，（MIT 许可证）'
