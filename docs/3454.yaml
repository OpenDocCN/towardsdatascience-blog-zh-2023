- en: A Gentle Introduction to Deep Reinforcement Learning in JAX
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 《深入浅出 JAX 中的深度强化学习》
- en: 原文：[https://towardsdatascience.com/a-gentle-introduction-to-deep-reinforcement-learning-in-jax-c1e45a179b92?source=collection_archive---------4-----------------------#2023-11-21](https://towardsdatascience.com/a-gentle-introduction-to-deep-reinforcement-learning-in-jax-c1e45a179b92?source=collection_archive---------4-----------------------#2023-11-21)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/a-gentle-introduction-to-deep-reinforcement-learning-in-jax-c1e45a179b92?source=collection_archive---------4-----------------------#2023-11-21](https://towardsdatascience.com/a-gentle-introduction-to-deep-reinforcement-learning-in-jax-c1e45a179b92?source=collection_archive---------4-----------------------#2023-11-21)
- en: Solving the CartPole environment with DQN in under a second
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在一秒钟内用 DQN 解决 CartPole 环境
- en: '[](https://medium.com/@ryanpegoud?source=post_page-----c1e45a179b92--------------------------------)[![Ryan
    Pégoud](../Images/9314b76c2be56bda8b73b4badf9e3e4d.png)](https://medium.com/@ryanpegoud?source=post_page-----c1e45a179b92--------------------------------)[](https://towardsdatascience.com/?source=post_page-----c1e45a179b92--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----c1e45a179b92--------------------------------)
    [Ryan Pégoud](https://medium.com/@ryanpegoud?source=post_page-----c1e45a179b92--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@ryanpegoud?source=post_page-----c1e45a179b92--------------------------------)[![Ryan
    Pégoud](../Images/9314b76c2be56bda8b73b4badf9e3e4d.png)](https://medium.com/@ryanpegoud?source=post_page-----c1e45a179b92--------------------------------)[](https://towardsdatascience.com/?source=post_page-----c1e45a179b92--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----c1e45a179b92--------------------------------)
    [Ryan Pégoud](https://medium.com/@ryanpegoud?source=post_page-----c1e45a179b92--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F27fba63b402e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-gentle-introduction-to-deep-reinforcement-learning-in-jax-c1e45a179b92&user=Ryan+P%C3%A9goud&userId=27fba63b402e&source=post_page-27fba63b402e----c1e45a179b92---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----c1e45a179b92--------------------------------)
    ·10 min read·Nov 21, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fc1e45a179b92&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-gentle-introduction-to-deep-reinforcement-learning-in-jax-c1e45a179b92&user=Ryan+P%C3%A9goud&userId=27fba63b402e&source=-----c1e45a179b92---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F27fba63b402e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-gentle-introduction-to-deep-reinforcement-learning-in-jax-c1e45a179b92&user=Ryan+P%C3%A9goud&userId=27fba63b402e&source=post_page-27fba63b402e----c1e45a179b92---------------------post_header-----------)
    发表在[Towards Data Science](https://towardsdatascience.com/?source=post_page-----c1e45a179b92--------------------------------)
    · 10分钟阅读·2023年11月21日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fc1e45a179b92&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-gentle-introduction-to-deep-reinforcement-learning-in-jax-c1e45a179b92&user=Ryan+P%C3%A9goud&userId=27fba63b402e&source=-----c1e45a179b92---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc1e45a179b92&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-gentle-introduction-to-deep-reinforcement-learning-in-jax-c1e45a179b92&source=-----c1e45a179b92---------------------bookmark_footer-----------)![](../Images/af8f701ac536881e802dd7507bdc46f2.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc1e45a179b92&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-gentle-introduction-to-deep-reinforcement-learning-in-jax-c1e45a179b92&source=-----c1e45a179b92---------------------bookmark_footer-----------)![](../Images/af8f701ac536881e802dd7507bdc46f2.png)'
- en: Photo by [Thomas Despeyroux](https://unsplash.com/@thomasdes?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 由[Thomas Despeyroux](https://unsplash.com/@thomasdes?utm_source=medium&utm_medium=referral)拍摄，发布于[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: Recent progress in Reinforcement Learning (RL), such as Waymo’s autonomous taxis
    or DeepMind’s superhuman chess-playing agents, complement **classical RL** with
    **Deep Learning** components such as **Neural Networks** and **Gradient Optimization**
    methods.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 最近在强化学习（RL）方面的进展，例如 Waymo 的自动驾驶出租车或 DeepMind 的超人类棋类代理，结合了**经典 RL**和**深度学习**组件，如**神经网络**和**梯度优化**方法。
- en: Building on the foundations and coding principles introduced in one of my previous
    stories, we’ll discover and learn to implement **Deep Q-Networks** (**DQN**) and
    **replay buffers** to solve OpenAI’s **CartPole** environment. All of that **in
    under a second** using JAX!
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前介绍的基础和编码原则上构建，我们将探索并学习如何使用JAX实现**深度Q网络**（**DQN**）和**回放缓冲区**来解决OpenAI的**CartPole**环境。所有这些操作都在**不到一秒**的时间内完成！
- en: 'For an introduction to **JAX**, **vectorized environments**, and **Q-learning**,
    please refer to the content of this story:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 对于**JAX**、**向量化环境**和**Q-learning**的介绍，请参阅以下内容：
- en: '[](/vectorize-and-parallelize-rl-environments-with-jax-q-learning-at-the-speed-of-light-49d07373adf5?source=post_page-----c1e45a179b92--------------------------------)
    [## Vectorize and Parallelize RL Environments with JAX: Q-learning at the Speed
    of Light⚡'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/vectorize-and-parallelize-rl-environments-with-jax-q-learning-at-the-speed-of-light-49d07373adf5?source=post_page-----c1e45a179b92--------------------------------)
    [## 使用JAX向量化和并行化RL环境：Q-learning的光速⚡'
- en: Learn to vectorize a GridWorld environment and train 30 Q-learning agents in
    parallel on a CPU, at 1.8 million step per…
  id: totrans-13
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 学习如何在CPU上向量化GridWorld环境，并同时训练30个Q-learning代理，每个代理进行180万步…
- en: towardsdatascience.com](/vectorize-and-parallelize-rl-environments-with-jax-q-learning-at-the-speed-of-light-49d07373adf5?source=post_page-----c1e45a179b92--------------------------------)
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/vectorize-and-parallelize-rl-environments-with-jax-q-learning-at-the-speed-of-light-49d07373adf5?source=post_page-----c1e45a179b92--------------------------------)
- en: 'Our framework of choice for deep learning will be DeepMind’s **Haiku** library,
    which I recently introduced in the context of Transformers:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我们选择的深度学习框架是DeepMind的**Haiku**库，我最近在Transformer的上下文中介绍过：
- en: '[](/implementing-a-transformer-encoder-from-scratch-with-jax-and-haiku-791d31b4f0dd?source=post_page-----c1e45a179b92--------------------------------)
    [## Implementing a Transformer Encoder from Scratch with JAX and Haiku 🤖'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/implementing-a-transformer-encoder-from-scratch-with-jax-and-haiku-791d31b4f0dd?source=post_page-----c1e45a179b92--------------------------------)
    [## 使用JAX和Haiku从头开始实现Transformer编码器 🤖'
- en: Understanding the fundamental building blocks of Transformers.
  id: totrans-17
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 理解Transformer的基础构建模块。
- en: towardsdatascience.com](/implementing-a-transformer-encoder-from-scratch-with-jax-and-haiku-791d31b4f0dd?source=post_page-----c1e45a179b92--------------------------------)
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/implementing-a-transformer-encoder-from-scratch-with-jax-and-haiku-791d31b4f0dd?source=post_page-----c1e45a179b92--------------------------------)
- en: 'This article will cover the following sections:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 本文将涵盖以下几个部分：
- en: '**Why** do we need Deep RL?'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**为什么**我们需要深度RL？'
- en: '**Deep Q-Networks,** *theory and practice*'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**深度Q网络**的**理论和实践**'
- en: '**Replay Buffers**'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**回放缓冲区**'
- en: Translating the **CartPole** environment to **JAX**
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将**CartPole**环境转换为**JAX**
- en: The **JAX** way to write **efficient training loops**
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**JAX**编写**高效训练循环**的方式'
- en: '*As always, all the code presented in this article is available on GitHub:*'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '*如往常一样，本文中提供的所有代码都可在GitHub上找到：*'
- en: '[](https://github.com/RPegoud/jym?source=post_page-----c1e45a179b92--------------------------------)
    [## GitHub - RPegoud/jym: JAX implementation of RL algorithms and vectorized environments'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://github.com/RPegoud/jym?source=post_page-----c1e45a179b92--------------------------------)
    [## GitHub - RPegoud/jym：JAX实现的RL算法和向量化环境'
- en: 'JAX implementation of RL algorithms and vectorized environments - GitHub -
    RPegoud/jym: JAX implementation of RL…'
  id: totrans-27
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 'JAX实现的RL算法和向量化环境 - GitHub - RPegoud/jym: JAX实现的RL…'
- en: github.com](https://github.com/RPegoud/jym?source=post_page-----c1e45a179b92--------------------------------)
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: github.com](https://github.com/RPegoud/jym?source=post_page-----c1e45a179b92--------------------------------)
- en: '**Why** do we need Deep RL?'
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**为什么**我们需要深度RL？'
- en: In previous articles, we introduced [Temporal Difference Learning](https://medium.com/towards-data-science/temporal-difference-learning-and-the-importance-of-exploration-an-illustrated-guide-5f9c3371413a)
    algorithms and in particular [Q-learning](/vectorize-and-parallelize-rl-environments-with-jax-q-learning-at-the-speed-of-light-49d07373adf5).
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的文章中，我们介绍了[时间差分学习](https://medium.com/towards-data-science/temporal-difference-learning-and-the-importance-of-exploration-an-illustrated-guide-5f9c3371413a)算法，特别是[Q-learning](/vectorize-and-parallelize-rl-environments-with-jax-q-learning-at-the-speed-of-light-49d07373adf5)。
- en: Simply put, Q-learning is an **off-policy** algorithm *(the target policy is
    not the policy used for decision-making)* maintaining and updating a **Q-table**,
    an explicit **mapping** of **states** to corresponding **action values**.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 简单来说，Q-learning是一种**离策略**算法（*目标策略与用于决策的策略不同*），用于维护和更新**Q表**，一个明确的**状态**到相应**动作值**的**映射**。
- en: While Q-learning is a practical solution for environments with discrete action
    spaces and restricted observation spaces, it struggles to scale well to more complex
    environments. Indeed, creating a Q-table requires **defining** the **action**
    and **observation spaces**.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管Q学习是离散行动空间和受限观察空间环境的实际解决方案，但在更复杂的环境中很难扩展。事实上，创建Q表需要定义**行动**和**观察空间**。
- en: Consider the example of **autonomous driving**, the **observation space** is
    composed of an *infinity of potential configurations* derived from camera feeds
    and other sensory inputs. On the other hand, the **action space** includes a *wide
    spectrum of steering wheel positions* and varying levels of force applied to the
    brake and accelerator.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑**自动驾驶**的例子，**观察空间**由来自摄像头和其他感知输入的*无限潜在配置*组成。另一方面，**行动空间**包括*广泛的方向盘位置*以及施加到刹车和油门的不同力度。
- en: Even though we could theoretically discretize the action space, the sheer volume
    of possible states and actions leads to an **impractical Q-table** in **real-world
    applications**.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管理论上我们可以离散化行动空间，但实际应用中可能会导致**不切实际的Q表**，因为可能的状态和行动数量庞大。
- en: '![](../Images/69029c0c7e9ad43e30b3e37045cbfb99.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/69029c0c7e9ad43e30b3e37045cbfb99.png)'
- en: Photo by [Kirill Tonkikh](https://unsplash.com/@photophotostock?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '[Kirill Tonkikh](https://unsplash.com/@photophotostock?utm_source=medium&utm_medium=referral)的照片来自[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)'
- en: Finding optimal actions in large and complex state-action spaces thus requires
    **powerful function approximation algorithms**, which is precisely what **Neural
    Networks** are. In the case of Deep Reinforcement Learning, neural nets are used
    as a **replacement for the Q-table** and provide an efficient solution to the
    *curse of dimensionality* introduced by large state spaces. Furthermore, we do
    not need to explicitly define the observation space.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在大型复杂状态-动作空间中寻找最优行动因此需要**强大的函数逼近算法**，这正是**神经网络**所擅长的。在深度强化学习中，神经网络用作**Q表的替代品**，并为大状态空间引入的*维度灾难*提供了高效的解决方案。此外，我们不需要显式定义观察空间。
- en: Deep Q-Networks & Replay Buffers
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度Q网络与重播缓冲区
- en: DQN uses two types of neural networks in parallel, starting with the “***online***”
    network which is used for **Q-value prediction** and **decision-making**. On the
    other hand, the “***target***” network is used to **create stable Q-targets**
    to assess the performance of the online net via the loss function.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: DQN同时使用两种类型的神经网络，并行进行，首先是用于**Q值预测**和**决策**的“***在线***”网络。另一方面，“***目标***”网络用于通过损失函数评估在线网络的性能，以生成**稳定的Q目标**。
- en: 'Similarly to Q-learning, DQN agents are defined by two functions: `act` and
    `update`.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 与Q学习类似，DQN代理由两个函数定义：`act`和`update`。
- en: Act
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 行动
- en: The `act` function implements an epsilon-greedy policy with respect to Q-values,
    which are estimated by the online neural network. In other words, the agent selects
    the action corresponding to the **maximum predicted Q-value** for a given state,
    with a set probability of acting randomly.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '`act`函数实现了关于Q值的ε-贪心策略，Q值由在线神经网络估计。换句话说，代理根据给定状态的**最大预测Q值**选择动作，同时以一定概率随机执行动作。'
- en: You might remember that Q-learning updates its Q-table **after *every* step**,
    however, in Deep Learning it is common practice to compute updates using **gradient
    descent** on a **batch of inputs**.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能还记得Q学习在每一步之后更新其Q表，但在深度学习中，通常使用梯度下降在**输入批次**上计算更新。
- en: For this reason, DQN stores experiences (tuples containing `state, action, reward,
    next_state, done_flag`) in a **replay buffer**. To train the network, we’ll sample
    a batch of experiences from this buffer instead of using only the last experience
    *(more details in the Replay Buffer section)*.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，DQN将经验（包含`state, action, reward, next_state, done_flag`的元组）存储在**重播缓冲区**中。为了训练网络，我们将从此缓冲区中随机抽取一批经验，而不仅仅使用最后一次经验（有关更多详细信息，请参见重播缓冲区部分）。
- en: '![](../Images/7650171ae97fe0479b46e1490c1fdca0.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7650171ae97fe0479b46e1490c1fdca0.png)'
- en: Visual representation of **DQN’s action selection** process (Made by the author)
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 展示了**DQN行动选择**过程的视觉表示（作者制作）
- en: 'Here’s a JAX implementation of the action-selection part of DQN:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是DQN行动选择部分的JAX实现：
- en: The only subtlety of this snippet is that the `model` attribute doesn’t contain
    any internal parameters as is usually the case in frameworks such as PyTorch or
    TensorFlow.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: Here, the model is a **function** representing a **forward pass** through our
    architecture, but the ***mutable* weights are stored externally** and passed as
    **arguments**. This explains why we can use `jit` while passing the `self` argument
    as **static *(****the model being stateless as other class attributes)*.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: Update
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The `update` function is responsible for training the network. It computes
    a **mean squared error** (MSE) loss based on the **temporal-difference** (TD)
    **error**:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/449285430b8e3bdfe71e785562543881.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
- en: Mean Squared Error used in DQN
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: In this loss function, ***θ*** denotes the **parameters of the online network**,
    and ***θ*−** represents the **parameters of the target network**. The parameters
    of the target network are set on the online network’s parameters every *N* steps*,*
    similar to a *checkpoint* (*N is a hyperparameter).*
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: This separation of parameters (with *θ* for the current Q-values and *θ*− for
    the target Q-values) is crucial to stabilize training.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: Using the same parameters for both would be similar to aiming at a moving target,
    as **updates to the network** would i**mmediately shift the target values**. By
    **periodically updating** ***θ*−** (i.e. freezing these parameters for a set number
    of steps), we ensure **stable Q-targets** while the online network continues to
    learn.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: Finally, the *(1-done)* term **adjusts the target** for **terminal states**.
    Indeed, when an episode ends (i.e. ‘done’ is equal to 1), there is no next state.
    Therefore, the Q-value for the next state is set to 0.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6cc23c97e8034ed5aa9e046e8ca97139.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
- en: Visual representation of **DQN’s parameter update** process (Made by the author)
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: 'Implementing the update function for DQN is slightly more complex, let’s break
    it down:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: First, the `_loss_fn` function implements the squared error described previously
    for a **single experience**.
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Then, `_batch_loss_fn` acts as a wrapper for `_loss_fn` and decorates it with
    `vmap`, applying the loss function to a **batch of experiences**. We then return
    the average error for this batch.
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finally, `update` acts as a final layer to our loss function, computing its
    **gradient** with respect to the online network parameters, the target network
    parameters, and a batch of experiences. We then use **Optax** *(a JAX library
    commonly used for optimization)* to perform an optimizer step and update the online
    parameters.
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Notice that, similarly to the replay buffer, the model and optimizer are **pure
    functions** modifying an **external state**. The following line serves as a good
    illustration of this principle:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: This also explains why we can use a single model for both the online and target
    networks, as the parameters are stored and updated externally.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'For context, the model we use in this article is a *multi-layer perceptron*
    defined as follows:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Replay Buffer
  id: totrans-70
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now let us take a step back and look closer at replay buffers. They are widely
    used in reinforcement learning for a variety of reasons:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: '**Generalization**: By sampling from the replay buffer, we break the correlation
    between consecutive experiences by mixing up their order. This way, we avoid overfitting
    to specific sequences of experiences.'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Diversity**: As the sampling is not limited to recent experiences, we generally
    observe a lower variance in updates and prevent overfitting to the latest experiences.'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Increased sample efficiency**: Each experience can be sampled multiple times
    from the buffer, enabling the model to learn more from individual experiences.'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Finally, we can use several sampling schemes for our replay buffer:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: '**Uniform sampling:** Experiences are sampled uniformly at random. This type
    of sampling is straightforward to implement and allows the model to learn from
    experiences independently from the timestep they were collected.'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Prioritized sampling:** This category includes different algorithms such
    as **Prioritized Experience Replay** (“PER”, [*Schaul et al. 2015*](https://arxiv.org/abs/1511.05952)*)*
    or **Gradient Experience Replay** (“GER”, [*Lahire et al., 2022*](https://arxiv.org/abs/2110.01528)*).*
    These methods attempt to prioritize the selection of experiences according to
    some metric related to their “*learning potential”* (the amplitude of the TD error
    for PER and the norm of the experience’s gradient for GER).'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For the sake of simplicity, we’ll implement a uniform replay buffer in this
    article. However, I plan to cover prioritized sampling extensively in the future.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: As promised, the uniform replay buffer is quite easy to implement, however,
    there are a few complexities related to the use of JAX and functional programming.
    As always, we have to work with **pure functions** that are **devoid of side effects**.
    In other words, we are not allowed to define the buffer as a class instance with
    a variable internal state.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
- en: Instead, we initialize a `buffer_state` dictionary that maps keys to empty arrays
    with predefined shapes, as JAX requires constant-sized arrays when jit-compiling
    code to XLA.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'We will use a `UniformReplayBuffer` class to interact with the buffer state.
    This class has two methods:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: '`add`: Unwraps an experience tuple and maps its components to a specific index.
    `idx = idx % self.buffer_size` ensures that when the buffer is full, adding new
    experiences overwrites older ones.'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`sample`: Samples a sequence of random indexes from the uniform random distribution.
    The sequence length is set by `batch_size` while the range of the indexes is `[0,
    current_buffer_size-1]`. This ensures that we do not sample empty arrays while
    the buffer is not yet full. Finally, we use JAX’s `vmap` in combination with `tree_map`
    to return a batch of experiences.'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Translating the **CartPole** environment to **JAX**
  id: totrans-85
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that our DQN agent is ready for training, we’ll quickly implement a vectorized
    CartPole environment using the same framework as introduced in an [earlier article](/vectorize-and-parallelize-rl-environments-with-jax-q-learning-at-the-speed-of-light-49d07373adf5).
    CartPole is a control environment having a **large continuous observation space,**
    which makes it relevant to test our DQN.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6c7f16ff97e5c92150e3a4ece0e1f843.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
- en: 'Visual representation of the CartPole Environment (credits and documentation:
    [OpenAI Gymnasium](https://gymnasium.farama.org/environments/classic_control/cart_pole/),
    MIT license)'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: 'The process is quite straightforward, we reuse most of [OpenAI’s Gymnasium
    implementation](https://github.com/Farama-Foundation/Gymnasium/blob/main/gymnasium/envs/classic_control/cartpole.py)
    while making sure we use JAX arrays and lax control flow instead of Python or
    Numpy alternatives, for instance:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'For the sake of brevity, the full environment code is available here:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://github.com/RPegoud/jym/blob/main/src/envs/control/cartpole.py?source=post_page-----c1e45a179b92--------------------------------)
    [## jym/src/envs/control/cartpole.py at main · RPegoud/jym'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: JAX implementation of RL algorithms and vectorized environments - jym/src/envs/control/cartpole.py
    at main ·…
  id: totrans-93
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: github.com](https://github.com/RPegoud/jym/blob/main/src/envs/control/cartpole.py?source=post_page-----c1e45a179b92--------------------------------)
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: The **JAX** way to write **efficient training loops**
  id: totrans-95
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The last part of our implementation of DQN is the training loop *(also called
    rollout).* As mentioned in previous articles, we have to respect a specific format
    in order to take advantage of JAX’s speed.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: 'The rollout function might appear daunting at first, but most of its complexity
    is purely syntactic as we’ve already covered most of the building blocks. Here’s
    a pseudo-code walkthrough:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: We can now run DQN for **20,000 steps** and observe the performances. After
    around 45 episodes, the agent manages to obtain decent performances, balancing
    the pole for more than 100 steps consistently.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
- en: The **green bars** indicate that the agent managed to balance the pole for **more
    than 200 steps**, **solving the environment**. Notably, the agent set its record
    on the **51st episode**, with **393 steps**.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: Performance report for DQN (made by the author)
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: The **20.000 training steps** were executed in **just over a second**, at a
    rate of **15.807 steps per second** *(on a* ***single CPU****)*!
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: These performances hint at JAX’s impressive scaling capabilities, allowing practitioners
    to run large-scale parallelized experiments with minimal hardware requirements.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: We’ll take a closer look at **parallelized rollout procedures** to run **statistically
    significant** experiments and **hyperparameter searches** in a future article!
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: 'In the meantime, feel free to reproduce the experiment and dabble with hyperparameters
    using this notebook:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://github.com/RPegoud/jym/blob/main/notebooks/control/cartpole/dqn_cartpole.ipynb?source=post_page-----c1e45a179b92--------------------------------)
    [## jym/notebooks/control/cartpole/dqn_cartpole.ipynb at main · RPegoud/jym'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://github.com/RPegoud/jym/blob/main/notebooks/control/cartpole/dqn_cartpole.ipynb?source=post_page-----c1e45a179b92--------------------------------)
    [## jym/notebooks/control/cartpole/dqn_cartpole.ipynb at main · RPegoud/jym'
- en: JAX implementation of RL algorithms and vectorized environments - jym/notebooks/control/cartpole/dqn_cartpole.ipynb
    at…
  id: totrans-108
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: JAX 强化学习算法和向量化环境的实现 - jym/notebooks/control/cartpole/dqn_cartpole.ipynb at…
- en: github.com](https://github.com/RPegoud/jym/blob/main/notebooks/control/cartpole/dqn_cartpole.ipynb?source=post_page-----c1e45a179b92--------------------------------)
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: github.com](https://github.com/RPegoud/jym/blob/main/notebooks/control/cartpole/dqn_cartpole.ipynb?source=post_page-----c1e45a179b92--------------------------------)
- en: Conclusion
  id: totrans-110
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: As always, **thanks for reading this far!** I hope this article provided a decent
    introduction to Deep RL in JAX. Should you have any questions or feedback related
    to the content of this article, make sure to let me know, I’m always happy to
    have a little chat ;)
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 如往常一样，**感谢您读到这里！**希望本文为您在 JAX 中的深度强化学习提供了一个不错的介绍。如果您对本文内容有任何问题或反馈，请务必告诉我，我总是乐意聊一聊
    ;)
- en: Until next time 👋
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 直到下次见面 👋
- en: 'Credits:'
  id: totrans-113
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 致谢：
- en: '[Cartpole Gif](https://gymnasium.farama.org/environments/classic_control/cart_pole/),
    OpenAI Gymnasium library, (MIT license)'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Cartpole Gif](https://gymnasium.farama.org/environments/classic_control/cart_pole/)，OpenAI
    Gymnasium 库，（MIT 许可证）'
