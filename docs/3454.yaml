- en: A Gentle Introduction to Deep Reinforcement Learning in JAX
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ÂéüÊñáÔºö[https://towardsdatascience.com/a-gentle-introduction-to-deep-reinforcement-learning-in-jax-c1e45a179b92?source=collection_archive---------4-----------------------#2023-11-21](https://towardsdatascience.com/a-gentle-introduction-to-deep-reinforcement-learning-in-jax-c1e45a179b92?source=collection_archive---------4-----------------------#2023-11-21)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Solving the CartPole environment with DQN in under a second
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@ryanpegoud?source=post_page-----c1e45a179b92--------------------------------)[![Ryan
    P√©goud](../Images/9314b76c2be56bda8b73b4badf9e3e4d.png)](https://medium.com/@ryanpegoud?source=post_page-----c1e45a179b92--------------------------------)[](https://towardsdatascience.com/?source=post_page-----c1e45a179b92--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----c1e45a179b92--------------------------------)
    [Ryan P√©goud](https://medium.com/@ryanpegoud?source=post_page-----c1e45a179b92--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ¬∑
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F27fba63b402e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-gentle-introduction-to-deep-reinforcement-learning-in-jax-c1e45a179b92&user=Ryan+P%C3%A9goud&userId=27fba63b402e&source=post_page-27fba63b402e----c1e45a179b92---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----c1e45a179b92--------------------------------)
    ¬∑10 min read¬∑Nov 21, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fc1e45a179b92&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-gentle-introduction-to-deep-reinforcement-learning-in-jax-c1e45a179b92&user=Ryan+P%C3%A9goud&userId=27fba63b402e&source=-----c1e45a179b92---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc1e45a179b92&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-gentle-introduction-to-deep-reinforcement-learning-in-jax-c1e45a179b92&source=-----c1e45a179b92---------------------bookmark_footer-----------)![](../Images/af8f701ac536881e802dd7507bdc46f2.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Photo by [Thomas Despeyroux](https://unsplash.com/@thomasdes?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: Recent progress in Reinforcement Learning (RL), such as Waymo‚Äôs autonomous taxis
    or DeepMind‚Äôs superhuman chess-playing agents, complement **classical RL** with
    **Deep Learning** components such as **Neural Networks** and **Gradient Optimization**
    methods.
  prefs: []
  type: TYPE_NORMAL
- en: Building on the foundations and coding principles introduced in one of my previous
    stories, we‚Äôll discover and learn to implement **Deep Q-Networks** (**DQN**) and
    **replay buffers** to solve OpenAI‚Äôs **CartPole** environment. All of that **in
    under a second** using JAX!
  prefs: []
  type: TYPE_NORMAL
- en: 'For an introduction to **JAX**, **vectorized environments**, and **Q-learning**,
    please refer to the content of this story:'
  prefs: []
  type: TYPE_NORMAL
- en: '[](/vectorize-and-parallelize-rl-environments-with-jax-q-learning-at-the-speed-of-light-49d07373adf5?source=post_page-----c1e45a179b92--------------------------------)
    [## Vectorize and Parallelize RL Environments with JAX: Q-learning at the Speed
    of Light‚ö°'
  prefs: []
  type: TYPE_NORMAL
- en: Learn to vectorize a GridWorld environment and train 30 Q-learning agents in
    parallel on a CPU, at 1.8 million step per‚Ä¶
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/vectorize-and-parallelize-rl-environments-with-jax-q-learning-at-the-speed-of-light-49d07373adf5?source=post_page-----c1e45a179b92--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: 'Our framework of choice for deep learning will be DeepMind‚Äôs **Haiku** library,
    which I recently introduced in the context of Transformers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[](/implementing-a-transformer-encoder-from-scratch-with-jax-and-haiku-791d31b4f0dd?source=post_page-----c1e45a179b92--------------------------------)
    [## Implementing a Transformer Encoder from Scratch with JAX and Haiku ü§ñ'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the fundamental building blocks of Transformers.
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/implementing-a-transformer-encoder-from-scratch-with-jax-and-haiku-791d31b4f0dd?source=post_page-----c1e45a179b92--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: 'This article will cover the following sections:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Why** do we need Deep RL?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Deep Q-Networks,** *theory and practice*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Replay Buffers**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Translating the **CartPole** environment to **JAX**
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **JAX** way to write **efficient training loops**
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*As always, all the code presented in this article is available on GitHub:*'
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://github.com/RPegoud/jym?source=post_page-----c1e45a179b92--------------------------------)
    [## GitHub - RPegoud/jym: JAX implementation of RL algorithms and vectorized environments'
  prefs: []
  type: TYPE_NORMAL
- en: 'JAX implementation of RL algorithms and vectorized environments - GitHub -
    RPegoud/jym: JAX implementation of RL‚Ä¶'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: github.com](https://github.com/RPegoud/jym?source=post_page-----c1e45a179b92--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: '**Why** do we need Deep RL?'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In previous articles, we introduced [Temporal Difference Learning](https://medium.com/towards-data-science/temporal-difference-learning-and-the-importance-of-exploration-an-illustrated-guide-5f9c3371413a)
    algorithms and in particular [Q-learning](/vectorize-and-parallelize-rl-environments-with-jax-q-learning-at-the-speed-of-light-49d07373adf5).
  prefs: []
  type: TYPE_NORMAL
- en: Simply put, Q-learning is an **off-policy** algorithm *(the target policy is
    not the policy used for decision-making)* maintaining and updating a **Q-table**,
    an explicit **mapping** of **states** to corresponding **action values**.
  prefs: []
  type: TYPE_NORMAL
- en: While Q-learning is a practical solution for environments with discrete action
    spaces and restricted observation spaces, it struggles to scale well to more complex
    environments. Indeed, creating a Q-table requires **defining** the **action**
    and **observation spaces**.
  prefs: []
  type: TYPE_NORMAL
- en: Consider the example of **autonomous driving**, the **observation space** is
    composed of an *infinity of potential configurations* derived from camera feeds
    and other sensory inputs. On the other hand, the **action space** includes a *wide
    spectrum of steering wheel positions* and varying levels of force applied to the
    brake and accelerator.
  prefs: []
  type: TYPE_NORMAL
- en: Even though we could theoretically discretize the action space, the sheer volume
    of possible states and actions leads to an **impractical Q-table** in **real-world
    applications**.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/69029c0c7e9ad43e30b3e37045cbfb99.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Kirill Tonkikh](https://unsplash.com/@photophotostock?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: Finding optimal actions in large and complex state-action spaces thus requires
    **powerful function approximation algorithms**, which is precisely what **Neural
    Networks** are. In the case of Deep Reinforcement Learning, neural nets are used
    as a **replacement for the Q-table** and provide an efficient solution to the
    *curse of dimensionality* introduced by large state spaces. Furthermore, we do
    not need to explicitly define the observation space.
  prefs: []
  type: TYPE_NORMAL
- en: Deep Q-Networks & Replay Buffers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: DQN uses two types of neural networks in parallel, starting with the ‚Äú***online***‚Äù
    network which is used for **Q-value prediction** and **decision-making**. On the
    other hand, the ‚Äú***target***‚Äù network is used to **create stable Q-targets**
    to assess the performance of the online net via the loss function.
  prefs: []
  type: TYPE_NORMAL
- en: 'Similarly to Q-learning, DQN agents are defined by two functions: `act` and
    `update`.'
  prefs: []
  type: TYPE_NORMAL
- en: Act
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `act` function implements an epsilon-greedy policy with respect to Q-values,
    which are estimated by the online neural network. In other words, the agent selects
    the action corresponding to the **maximum predicted Q-value** for a given state,
    with a set probability of acting randomly.
  prefs: []
  type: TYPE_NORMAL
- en: You might remember that Q-learning updates its Q-table **after *every* step**,
    however, in Deep Learning it is common practice to compute updates using **gradient
    descent** on a **batch of inputs**.
  prefs: []
  type: TYPE_NORMAL
- en: For this reason, DQN stores experiences (tuples containing `state, action, reward,
    next_state, done_flag`) in a **replay buffer**. To train the network, we‚Äôll sample
    a batch of experiences from this buffer instead of using only the last experience
    *(more details in the Replay Buffer section)*.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7650171ae97fe0479b46e1490c1fdca0.png)'
  prefs: []
  type: TYPE_IMG
- en: Visual representation of **DQN‚Äôs action selection** process (Made by the author)
  prefs: []
  type: TYPE_NORMAL
- en: 'Here‚Äôs a JAX implementation of the action-selection part of DQN:'
  prefs: []
  type: TYPE_NORMAL
- en: The only subtlety of this snippet is that the `model` attribute doesn‚Äôt contain
    any internal parameters as is usually the case in frameworks such as PyTorch or
    TensorFlow.
  prefs: []
  type: TYPE_NORMAL
- en: Here, the model is a **function** representing a **forward pass** through our
    architecture, but the ***mutable* weights are stored externally** and passed as
    **arguments**. This explains why we can use `jit` while passing the `self` argument
    as **static *(****the model being stateless as other class attributes)*.
  prefs: []
  type: TYPE_NORMAL
- en: Update
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The `update` function is responsible for training the network. It computes
    a **mean squared error** (MSE) loss based on the **temporal-difference** (TD)
    **error**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/449285430b8e3bdfe71e785562543881.png)'
  prefs: []
  type: TYPE_IMG
- en: Mean Squared Error used in DQN
  prefs: []
  type: TYPE_NORMAL
- en: In this loss function, ***Œ∏*** denotes the **parameters of the online network**,
    and ***Œ∏*‚àí** represents the **parameters of the target network**. The parameters
    of the target network are set on the online network‚Äôs parameters every *N* steps*,*
    similar to a *checkpoint* (*N is a hyperparameter).*
  prefs: []
  type: TYPE_NORMAL
- en: This separation of parameters (with *Œ∏* for the current Q-values and *Œ∏*‚àí for
    the target Q-values) is crucial to stabilize training.
  prefs: []
  type: TYPE_NORMAL
- en: Using the same parameters for both would be similar to aiming at a moving target,
    as **updates to the network** would i**mmediately shift the target values**. By
    **periodically updating** ***Œ∏*‚àí** (i.e. freezing these parameters for a set number
    of steps), we ensure **stable Q-targets** while the online network continues to
    learn.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, the *(1-done)* term **adjusts the target** for **terminal states**.
    Indeed, when an episode ends (i.e. ‚Äòdone‚Äô is equal to 1), there is no next state.
    Therefore, the Q-value for the next state is set to 0.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6cc23c97e8034ed5aa9e046e8ca97139.png)'
  prefs: []
  type: TYPE_IMG
- en: Visual representation of **DQN‚Äôs parameter update** process (Made by the author)
  prefs: []
  type: TYPE_NORMAL
- en: 'Implementing the update function for DQN is slightly more complex, let‚Äôs break
    it down:'
  prefs: []
  type: TYPE_NORMAL
- en: First, the `_loss_fn` function implements the squared error described previously
    for a **single experience**.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Then, `_batch_loss_fn` acts as a wrapper for `_loss_fn` and decorates it with
    `vmap`, applying the loss function to a **batch of experiences**. We then return
    the average error for this batch.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finally, `update` acts as a final layer to our loss function, computing its
    **gradient** with respect to the online network parameters, the target network
    parameters, and a batch of experiences. We then use **Optax** *(a JAX library
    commonly used for optimization)* to perform an optimizer step and update the online
    parameters.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Notice that, similarly to the replay buffer, the model and optimizer are **pure
    functions** modifying an **external state**. The following line serves as a good
    illustration of this principle:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: This also explains why we can use a single model for both the online and target
    networks, as the parameters are stored and updated externally.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'For context, the model we use in this article is a *multi-layer perceptron*
    defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Replay Buffer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now let us take a step back and look closer at replay buffers. They are widely
    used in reinforcement learning for a variety of reasons:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Generalization**: By sampling from the replay buffer, we break the correlation
    between consecutive experiences by mixing up their order. This way, we avoid overfitting
    to specific sequences of experiences.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Diversity**: As the sampling is not limited to recent experiences, we generally
    observe a lower variance in updates and prevent overfitting to the latest experiences.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Increased sample efficiency**: Each experience can be sampled multiple times
    from the buffer, enabling the model to learn more from individual experiences.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Finally, we can use several sampling schemes for our replay buffer:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Uniform sampling:** Experiences are sampled uniformly at random. This type
    of sampling is straightforward to implement and allows the model to learn from
    experiences independently from the timestep they were collected.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Prioritized sampling:** This category includes different algorithms such
    as **Prioritized Experience Replay** (‚ÄúPER‚Äù, [*Schaul et al. 2015*](https://arxiv.org/abs/1511.05952)*)*
    or **Gradient Experience Replay** (‚ÄúGER‚Äù, [*Lahire et al., 2022*](https://arxiv.org/abs/2110.01528)*).*
    These methods attempt to prioritize the selection of experiences according to
    some metric related to their ‚Äú*learning potential‚Äù* (the amplitude of the TD error
    for PER and the norm of the experience‚Äôs gradient for GER).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For the sake of simplicity, we‚Äôll implement a uniform replay buffer in this
    article. However, I plan to cover prioritized sampling extensively in the future.
  prefs: []
  type: TYPE_NORMAL
- en: As promised, the uniform replay buffer is quite easy to implement, however,
    there are a few complexities related to the use of JAX and functional programming.
    As always, we have to work with **pure functions** that are **devoid of side effects**.
    In other words, we are not allowed to define the buffer as a class instance with
    a variable internal state.
  prefs: []
  type: TYPE_NORMAL
- en: Instead, we initialize a `buffer_state` dictionary that maps keys to empty arrays
    with predefined shapes, as JAX requires constant-sized arrays when jit-compiling
    code to XLA.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'We will use a `UniformReplayBuffer` class to interact with the buffer state.
    This class has two methods:'
  prefs: []
  type: TYPE_NORMAL
- en: '`add`: Unwraps an experience tuple and maps its components to a specific index.
    `idx = idx % self.buffer_size` ensures that when the buffer is full, adding new
    experiences overwrites older ones.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`sample`: Samples a sequence of random indexes from the uniform random distribution.
    The sequence length is set by `batch_size` while the range of the indexes is `[0,
    current_buffer_size-1]`. This ensures that we do not sample empty arrays while
    the buffer is not yet full. Finally, we use JAX‚Äôs `vmap` in combination with `tree_map`
    to return a batch of experiences.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Translating the **CartPole** environment to **JAX**
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that our DQN agent is ready for training, we‚Äôll quickly implement a vectorized
    CartPole environment using the same framework as introduced in an [earlier article](/vectorize-and-parallelize-rl-environments-with-jax-q-learning-at-the-speed-of-light-49d07373adf5).
    CartPole is a control environment having a **large continuous observation space,**
    which makes it relevant to test our DQN.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6c7f16ff97e5c92150e3a4ece0e1f843.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Visual representation of the CartPole Environment (credits and documentation:
    [OpenAI Gymnasium](https://gymnasium.farama.org/environments/classic_control/cart_pole/),
    MIT license)'
  prefs: []
  type: TYPE_NORMAL
- en: 'The process is quite straightforward, we reuse most of [OpenAI‚Äôs Gymnasium
    implementation](https://github.com/Farama-Foundation/Gymnasium/blob/main/gymnasium/envs/classic_control/cartpole.py)
    while making sure we use JAX arrays and lax control flow instead of Python or
    Numpy alternatives, for instance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'For the sake of brevity, the full environment code is available here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://github.com/RPegoud/jym/blob/main/src/envs/control/cartpole.py?source=post_page-----c1e45a179b92--------------------------------)
    [## jym/src/envs/control/cartpole.py at main ¬∑ RPegoud/jym'
  prefs: []
  type: TYPE_NORMAL
- en: JAX implementation of RL algorithms and vectorized environments - jym/src/envs/control/cartpole.py
    at main ¬∑‚Ä¶
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: github.com](https://github.com/RPegoud/jym/blob/main/src/envs/control/cartpole.py?source=post_page-----c1e45a179b92--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: The **JAX** way to write **efficient training loops**
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The last part of our implementation of DQN is the training loop *(also called
    rollout).* As mentioned in previous articles, we have to respect a specific format
    in order to take advantage of JAX‚Äôs speed.
  prefs: []
  type: TYPE_NORMAL
- en: 'The rollout function might appear daunting at first, but most of its complexity
    is purely syntactic as we‚Äôve already covered most of the building blocks. Here‚Äôs
    a pseudo-code walkthrough:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: We can now run DQN for **20,000 steps** and observe the performances. After
    around 45 episodes, the agent manages to obtain decent performances, balancing
    the pole for more than 100 steps consistently.
  prefs: []
  type: TYPE_NORMAL
- en: The **green bars** indicate that the agent managed to balance the pole for **more
    than 200 steps**, **solving the environment**. Notably, the agent set its record
    on the **51st episode**, with **393 steps**.
  prefs: []
  type: TYPE_NORMAL
- en: Performance report for DQN (made by the author)
  prefs: []
  type: TYPE_NORMAL
- en: The **20.000 training steps** were executed in **just over a second**, at a
    rate of **15.807 steps per second** *(on a* ***single CPU****)*!
  prefs: []
  type: TYPE_NORMAL
- en: These performances hint at JAX‚Äôs impressive scaling capabilities, allowing practitioners
    to run large-scale parallelized experiments with minimal hardware requirements.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: We‚Äôll take a closer look at **parallelized rollout procedures** to run **statistically
    significant** experiments and **hyperparameter searches** in a future article!
  prefs: []
  type: TYPE_NORMAL
- en: 'In the meantime, feel free to reproduce the experiment and dabble with hyperparameters
    using this notebook:'
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://github.com/RPegoud/jym/blob/main/notebooks/control/cartpole/dqn_cartpole.ipynb?source=post_page-----c1e45a179b92--------------------------------)
    [## jym/notebooks/control/cartpole/dqn_cartpole.ipynb at main ¬∑ RPegoud/jym'
  prefs: []
  type: TYPE_NORMAL
- en: JAX implementation of RL algorithms and vectorized environments - jym/notebooks/control/cartpole/dqn_cartpole.ipynb
    at‚Ä¶
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: github.com](https://github.com/RPegoud/jym/blob/main/notebooks/control/cartpole/dqn_cartpole.ipynb?source=post_page-----c1e45a179b92--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As always, **thanks for reading this far!** I hope this article provided a decent
    introduction to Deep RL in JAX. Should you have any questions or feedback related
    to the content of this article, make sure to let me know, I‚Äôm always happy to
    have a little chat ;)
  prefs: []
  type: TYPE_NORMAL
- en: Until next time üëã
  prefs: []
  type: TYPE_NORMAL
- en: 'Credits:'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[Cartpole Gif](https://gymnasium.farama.org/environments/classic_control/cart_pole/),
    OpenAI Gymnasium library, (MIT license)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
