- en: 'Develop Your First AI Agent: Deep Q-Learning'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/develop-your-first-ai-agent-deep-q-learning-375876ee2472?source=collection_archive---------0-----------------------#2023-12-15](https://towardsdatascience.com/develop-your-first-ai-agent-deep-q-learning-375876ee2472?source=collection_archive---------0-----------------------#2023-12-15)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Dive into the world of artificial intelligence — build a deep reinforcement
    learning gym from scratch.
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@heston.cv?source=post_page-----375876ee2472--------------------------------)[![Heston
    Vaughan](../Images/f480ce32e594e4d10258f5929f6bb83c.png)](https://medium.com/@heston.cv?source=post_page-----375876ee2472--------------------------------)[](https://towardsdatascience.com/?source=post_page-----375876ee2472--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----375876ee2472--------------------------------)
    [Heston Vaughan](https://medium.com/@heston.cv?source=post_page-----375876ee2472--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F72a0dba7a030&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdevelop-your-first-ai-agent-deep-q-learning-375876ee2472&user=Heston+Vaughan&userId=72a0dba7a030&source=post_page-72a0dba7a030----375876ee2472---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----375876ee2472--------------------------------)
    ·61 min read·Dec 15, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F375876ee2472&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdevelop-your-first-ai-agent-deep-q-learning-375876ee2472&user=Heston+Vaughan&userId=72a0dba7a030&source=-----375876ee2472---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F375876ee2472&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdevelop-your-first-ai-agent-deep-q-learning-375876ee2472&source=-----375876ee2472---------------------bookmark_footer-----------)![](../Images/1dd9d105bd98aeb6b77070c124fa0391.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Construct your own Deep Reinforcement Learning Gym — Image by author
  prefs: []
  type: TYPE_NORMAL
- en: '**Table of Contents**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*If you already have a grasp of Reinforcement and Deep Q-Learning concepts,
    feel free to jump directly to the step-by-step tutorial. There you’ll have all
    the resources and code necessary to build a Deep Reinforcement Learning gym from
    the ground up, including the environment, agent, and training protocol.*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**Intro**'
  prefs: []
  type: TYPE_NORMAL
- en: '[Why Reinforcement Learning?](#c87e)'
  prefs: []
  type: TYPE_NORMAL
- en: '[What you will gain](#3eed)'
  prefs: []
  type: TYPE_NORMAL
- en: '[What is Reinforcement Learning](#9838)?'
  prefs: []
  type: TYPE_NORMAL
- en: '[Deep Q-Learning](#31ad)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Step-by-Step Tutorial**'
  prefs: []
  type: TYPE_NORMAL
- en: '[1\. Initial Setup](#c0d5)'
  prefs: []
  type: TYPE_NORMAL
- en: '[2\. The Big Picture](#16af)'
  prefs: []
  type: TYPE_NORMAL
- en: '[3\. The Environment: Initial Foundations](#f415)'
  prefs: []
  type: TYPE_NORMAL
- en: '[4\. Implement The Agent: Neural Architecture and Policy](#b396)'
  prefs: []
  type: TYPE_NORMAL
- en: '[5\. Affect The Environment: Finishing Up](#d420)'
  prefs: []
  type: TYPE_NORMAL
- en: '[6\. Learn From Experiences: Experience Replay](#05a1)'
  prefs: []
  type: TYPE_NORMAL
- en: '[7\. Define The Agent’s Learning Process: Fitting The NN](#fa62)'
  prefs: []
  type: TYPE_NORMAL
- en: '[8\. Execute The Training Loop: Putting It All Together](#660f)'
  prefs: []
  type: TYPE_NORMAL
- en: '[9\. Wrapping It Up](#21fd)'
  prefs: []
  type: TYPE_NORMAL
- en: '[10\. Bonus: Optimize State Representation](#d8f9)'
  prefs: []
  type: TYPE_NORMAL
- en: Why Reinforcement Learning?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The recent widespread adoption of advanced AI systems, such as ChatGPT, Bard,
    Midjourney, Stable Diffusion, and many others, has sparked an interest in the
    field of artificial intelligence, machine learning, and neural networks that is
    often left unsatisfied because of the technical nature of implementing such systems.
  prefs: []
  type: TYPE_NORMAL
- en: For those looking to begin their journey into AI (or continue the one they are
    on), building a reinforcement learning gym using Deep Q-Learning is a great start,
    as it does not require advanced knowledge to implement, can be easily expanded
    to solve complex problems, and can give immediate, tangible insight into how artificial
    intelligence becomes “intelligent”.
  prefs: []
  type: TYPE_NORMAL
- en: What you will gain
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Assuming you have a basic understanding of Python, by the end of this introduction
    to deep reinforcement learning, without using high-level reinforcement learning
    frameworks, you will have developed your own gym to train an agent to solve a
    simple problem — move itself from its starting point to the goal!
  prefs: []
  type: TYPE_NORMAL
- en: It’s not very glamorous, but you will have hands-on experience with topics like
    constructing an environment, defining reward structures and basic neural architecture,
    tweaking environmental parameters to observe different learning behaviors, and
    finding a balance between exploration and exploitation in decision-making.
  prefs: []
  type: TYPE_NORMAL
- en: You will then have all of the tools you need to implement your own, more complex
    environments and systems, and be well poised to dive deeper into topics like neural
    networks and advanced optimization strategies in reinforcement learning.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/15debfa447043ce3cc589eb460a4be37.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author using [Gymnasium’s](https://gymnasium.farama.org/) LunarLander-v2
    environment
  prefs: []
  type: TYPE_NORMAL
- en: You will also gain the confidence and understanding needed to effectively utilize
    pre-built tools like the [OpenAI Gym](https://www.gymlibrary.dev/), as each component
    of the system is implemented from scratch and demystified. This allows you to
    seamlessly integrate these powerful resources into your own AI projects.
  prefs: []
  type: TYPE_NORMAL
- en: What is Reinforcement Learning?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Reinforcement Learning (RL) is a sub-field of Machine Learning (ML) that is
    specifically focused on how agents (the entities making decisions) take actions
    in an environment to complete a goal.
  prefs: []
  type: TYPE_NORMAL
- en: 'Its implementations include:'
  prefs: []
  type: TYPE_NORMAL
- en: Games
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Autonomous Vehicles
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Robotics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finance (algorithmic trading)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Natural Language Processing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: and much more..
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The idea of RL is based on the fundamental principles of behavioral psychology
    where an animal or person learns from the consequences of their actions. If an
    action leads to a good outcome, then the agent is rewarded; if it does not, then
    it is punished or no reward is given.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before moving on, it is important to understand some commonly used terms:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Environment**: This is the world — the place where the agent operates. It
    sets the rules, boundaries, and rewards that the agent must navigate.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Agent**: The decision-maker within the environment. The agent takes actions
    based on its understanding of the state it’s in.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**State**: A detailed snapshot of the agent’s current situation in the environment,
    including relevant metrics or sensory information used for decision-making.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Action**: The specific measure the agent takes to interact with the environment,
    such as moving, collecting an item, or initiating an interaction.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Reward**: The feedback given from the environment as a result of the agent’s
    actions, which can be positive, negative, or neutral, guiding the learning process.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**State/Action-Space:** The combination of all possible states the agent can
    encounter and all actions it can take in the environment. This defines the scope
    of decisions and situations the agent must learn to navigate.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Essentially, in each step (turn) of the program the agent receives a state from
    the environment, chooses an action, receives a reward or punishment, and the environment
    is updated or the episode is complete. Information received after each step is
    saved as an “experience” for later training.
  prefs: []
  type: TYPE_NORMAL
- en: For a more concrete example, imagine you are playing chess. The board is the
    ***environment*** and you are the ***agent***. At each step (or turn) you view
    the ***state*** of the board and choose from the action-space, which is the set
    of all possible moves you could make, and pick the ***action*** with the highest
    possible future ***reward***. After the move is made you evaluate whether it was
    a good action or not, and learn to perform better next time.
  prefs: []
  type: TYPE_NORMAL
- en: It may seem like a lot of information at first, but as you build this out yourself
    these terms will come to feel quite natural.
  prefs: []
  type: TYPE_NORMAL
- en: Deep Q-Learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Q-Learning is an algorithm used in ML where the ‘Q’ stands for “Quality”, as
    in the value of actions an agent can take. It works by creating a table of Q-values,
    actions and their associated quality, that estimate the expected future reward
    for taking an action in a given state.
  prefs: []
  type: TYPE_NORMAL
- en: The agent is given the state of the environment, checks the table to see if
    it has encountered it before, and then chooses the action with the highest reward
    value.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/da21ab2772af2e4805ae824b776978f0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Sequential flow of Q-Learning: from state evaluation to reward and Q-Table
    update. — Image by author'
  prefs: []
  type: TYPE_NORMAL
- en: However, Q-Learning has a few drawbacks. Each state and action pair must be
    explored to achieve good results. If the state and action spaces (the set of all
    possible states and actions) are too large, then it is not feasible to store them
    all in a table.
  prefs: []
  type: TYPE_NORMAL
- en: This is where Deep Q-Learning (DQL), an evolution of Q-Learning, comes in. DQL
    utilizes a deep Neural Network (NN) to approximate a Q-value function rather than
    saving them in a table. This allows for handling environments that have high-dimensional
    state-spaces, like image inputs from a camera, which would not be practical for
    traditional Q-Learning.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ff305fc6ef163adf06221b2a862850f0.png)'
  prefs: []
  type: TYPE_IMG
- en: Deep Q-Learning is the intersection of Q-Learning and Deep Neural Networks —
    Image by author
  prefs: []
  type: TYPE_NORMAL
- en: The neural network can then generalize over similar states and actions, choosing
    a desirable move even if it has not been trained on the exact situation, eliminating
    the need for a large table.
  prefs: []
  type: TYPE_NORMAL
- en: '*How the neural network does this is beyond the scope of this tutorial. Thankfully,
    a deep understanding is not needed to implement Deep Q-Learning effectively.*'
  prefs: []
  type: TYPE_NORMAL
- en: Constructing The Reinforcement Learning Gym
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 1\. Initial Setup
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Before we start coding our AI agent, it is recommended that you have a solid
    understanding of Object Oriented Programming (OOP) principles in Python.
  prefs: []
  type: TYPE_NORMAL
- en: If you do not have Python installed already, below is a simple tutorial by [Bhargav
    Bachina](https://medium.com/@bhargavbachina) to get you started. The version I
    will be using is 3.11.6.
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/bb-tutorials-and-thoughts/how-to-install-and-getting-started-with-python-acf369e4cf80?source=post_page-----375876ee2472--------------------------------)
    [## How to Install and Getting Started With Python'
  prefs: []
  type: TYPE_NORMAL
- en: A Beginner’s Guide and for anyone who wants to start learning Python
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: medium.com](https://medium.com/bb-tutorials-and-thoughts/how-to-install-and-getting-started-with-python-acf369e4cf80?source=post_page-----375876ee2472--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: The only dependency you will need is [TensorFlow](https://www.tensorflow.org/),
    an open-source machine learning library by Google that we’ll use to build and
    train our neural network. This can be installed through pip in the terminal. My
    version is 2.14.0.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Or if that doesn’t work:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: You will also need the package [NumPy](https://numpy.org/), but this should
    be included with TensorFlow. If you run into issues there, `pip install numpy`.
  prefs: []
  type: TYPE_NORMAL
- en: It is also recommended that you create a new file for each class, (e.g., environment.py).
    This will keep you from being overwhelmed and ease troubleshooting any errors
    you may run into.
  prefs: []
  type: TYPE_NORMAL
- en: 'For your reference, here is the GitHub repository with the completed code:
    [https://github.com/HestonCV/rl-gym-from-scratch](https://github.com/HestonCV/rl-gym-from-scratch).
    Feel free to clone, explore, and use it as a reference point!'
  prefs: []
  type: TYPE_NORMAL
- en: 2\. The Big Picture
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To really understand the concepts rather than just copying code, it’s crucial
    to get a handle on the different parts we’re going to build and how they fit together.
    This way, each piece will have a place in the bigger picture.
  prefs: []
  type: TYPE_NORMAL
- en: Below is the code for one training loop with 5000 episodes. An episode is essentially
    one complete round of interaction between the agent and the environment, from
    start to finish.
  prefs: []
  type: TYPE_NORMAL
- en: '*This should not be implemented or fully understood at this point. As we build
    out each part, if you want to see how a specific class or method will be used,
    refer back to this.*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Each inner loop is considered one step.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5daaa2788171250a1b86d4aa4857b438.png)'
  prefs: []
  type: TYPE_IMG
- en: Training process through Agent-Environment interaction — Image by author
  prefs: []
  type: TYPE_NORMAL
- en: 'In each step:'
  prefs: []
  type: TYPE_NORMAL
- en: The state is retrieved from the environment.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The agent chooses an action based on this state.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Environment is acted on, returning the reward, resulting state after taking
    the action, and whether the episode is done.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The initial `state`, `action`, `reward`, `next_state`, and `done` are then saved
    into `experience_replay` as a sort of long-term memory (experience).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The agent is then trained on a random sample of these experiences.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: At the end of each episode, or however often you would like, the model weights
    are saved to the models folder. These can later be preloaded to keep from training
    from scratch each time. The environment is then reset at the start of the next
    episode.
  prefs: []
  type: TYPE_NORMAL
- en: This basic structure is pretty much all it takes to create an intelligent agent
    to solve a large variety of problems!
  prefs: []
  type: TYPE_NORMAL
- en: 'As stated in the introduction, our problem for the agent is quite simple: get
    from its initial position in a grid to the designated goal position.'
  prefs: []
  type: TYPE_NORMAL
- en: '3\. The Environment: Initial Foundations'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The most obvious place to start in developing this system is the environment.
  prefs: []
  type: TYPE_NORMAL
- en: 'To have a functioning RL gym, the environment needs to do a few things:'
  prefs: []
  type: TYPE_NORMAL
- en: Maintain the current state of the world.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Keep track of the goal and agent.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Allow the agent to make changes to the world.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Return the state in a form the model can understand.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Render it in a way we can understand to observe the agent.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This will be the place the agent spends its entire life. We will define the
    environment as a simple square matrix/2D array, or a list of lists in Python.
  prefs: []
  type: TYPE_NORMAL
- en: This environment will have a discrete state-space, meaning that the possible
    states the agent can encounter are distinct and countable. Each state is a separate,
    specific condition or scenario in the environment, unlike a continuous state space
    where the states can vary in an infinite, fluid manner — think of chess versus
    controlling a car.
  prefs: []
  type: TYPE_NORMAL
- en: '*DQL is specifically designed for discrete action-spaces (a finite number of
    actions)— this is what we will be focusing on. Other methods are used for continuous
    action-spaces.*'
  prefs: []
  type: TYPE_NORMAL
- en: In the grid, empty space will be represented by 0s, the agent will be represented
    by a 1, and the goal will be represented by a -1\. The size of the environment
    can be whatever you would like, but as the environment grows larger, the set of
    all possible states (state-space) grows exponentially. This can slow training
    time significantly.
  prefs: []
  type: TYPE_NORMAL
- en: 'The grid will look something like this when rendered:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '**Constructing the** `**Environment**` **class and** `**reset**` **method**
    We will begin by implementing the `Environment` class and a way to initialize
    the environment. For now, it will take an integer, `grid_size`, but we will expand
    on this shortly.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: When a new instance is created, `Environment` saves `grid_size` and initializes
    an empty grid.
  prefs: []
  type: TYPE_NORMAL
- en: The `reset` method populates the grid using `np.zeros((self.grid_size, self.grid_size))`
    , which takes a tuple, shape, and outputs a 2D NumPy array of that shape consisting
    only of zeros.
  prefs: []
  type: TYPE_NORMAL
- en: A NumPy array is a grid-like data structure that behaves similar to a list in
    Python, except that it enables us to efficiently store and manipulate numerical
    data. It allows for vectorized operations, meaning that operations are automatically
    applied to all elements in the array without the need for explicit loops.
  prefs: []
  type: TYPE_NORMAL
- en: This makes computations on large datasets much faster and more efficient compared
    to standard Python lists. Not only that, but it is the data structure that our
    agent’s neural network architecture will expect!
  prefs: []
  type: TYPE_NORMAL
- en: Why the name reset? Well, this method will be called to reset the environment
    and will eventually return the initial state of the grid.
  prefs: []
  type: TYPE_NORMAL
- en: '**Adding the agent and goal**'
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will construct the methods for adding the agent and the goal to the
    grid.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: The locations for the agent and the goal will be represented by a tuple (x,
    y). Both methods select random values within the boundaries of the grid and return
    the location. The main difference is that `add_goal` ensures it does not select
    a location already occupied by the agent.
  prefs: []
  type: TYPE_NORMAL
- en: We place the agent and goal at random starting locations to introduce variability
    in each episode, which helps the agent learn to navigate the environment from
    different starting points, rather than memorizing one route.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we will add a method to render the world in the console to enable us
    to see the interactions between the agent and environment.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '`render` does three things: casts the elements of `self.grid` to type int,
    converts it into a Python list, and prints each row.'
  prefs: []
  type: TYPE_NORMAL
- en: The only reason we don’t print each row from the NumPy array directly is simply
    that it just doesn’t look as nice.
  prefs: []
  type: TYPE_NORMAL
- en: '**Tying it all together..**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '*When looking at the locations it may seem there was some error, but they should
    be read as (row, column) from the top left to the bottom right. Also, remember
    that the coordinates are zero indexed.*'
  prefs: []
  type: TYPE_NORMAL
- en: Okay, so the environment is defined. What next?
  prefs: []
  type: TYPE_NORMAL
- en: '**Expanding on** `**reset**`'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s edit the reset method to handle placing the agent and goal for us. While
    we are at it, let’s automate render as well.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Now, when `reset` is called, the agent and goal are added to the grid, their
    initial locations are saved, and if `render_on` is set to true it will render
    the grid.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '**Defining the state of the environment**'
  prefs: []
  type: TYPE_NORMAL
- en: The last method we will implement for now is `get_state`. At first glance it
    seems the state might simply be the grid itself, but the problem with this approach
    is it is not what the neural network will expect.
  prefs: []
  type: TYPE_NORMAL
- en: Neural networks typically need one-dimensional input, not the two-dimensional
    shape that grid currently is represented by. We can fix this by flattening the
    grid using NumPy’s built-in `flatten` method. This will place each row into the
    same array.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'This will transform:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Into:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, it’s not immediately obvious which cells are which, but this
    will be no problem for a deep neural network.
  prefs: []
  type: TYPE_NORMAL
- en: Now we can update `reset` to return the state right after `grid` is populated.
    Nothing else will change.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '**Full code up to this point..**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: You have now successfully implemented the foundation for the environment! Although,
    if you haven’t noticed, we can’t interact with it yet. The agent is stuck in place.
  prefs: []
  type: TYPE_NORMAL
- en: We will return to this problem later after the `Agent` class has been coded
    to provide better context.
  prefs: []
  type: TYPE_NORMAL
- en: 4\. Implement The Agent Neural Architecture and Policy
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As stated previously, the agent is the entity that is given the state of its
    environment, in this case a flattened version of the world grid, and makes a decision
    on what action to take from the action-space.
  prefs: []
  type: TYPE_NORMAL
- en: '*Just to reiterate, the action-space is the set of all possible actions, in
    this scenario the agent can move up, down, left, and right, so the size of the
    action space is 4.*'
  prefs: []
  type: TYPE_NORMAL
- en: '*The state-space is the set of all possible states. This can be a massive number
    depending on the environment and perspective of the agent. In our case, if the
    world is a 5x5 grid there are 600 possible states, but if the world is a 25x25
    grid there are 390,000, wildly increasing the training time.*'
  prefs: []
  type: TYPE_NORMAL
- en: 'For an agent to effectively learn to complete a goal it needs a few things:'
  prefs: []
  type: TYPE_NORMAL
- en: Neural network to approximate the Q-values (estimated total amount of future
    reward for an action) in the case of DQL.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Policy or a strategy that the agent follows to choose an action.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reward signals from the environment to tell an agent how well it is doing.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ability to train on past experiences.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'There are two different policies one can implement:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Greedy Policy**: Choose the action with the highest Q-value in the current
    state.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Epsilon-Greedy Policy**: Choose the action with the highest Q-value in the
    current state, but there is a small chance, epsilon (commonly denoted as ϵ), to
    choose a random action. If epsilon = 0.02 then there is a 2% chance that the action
    will be random.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What we will implement is the **Epsilon-Greedy Policy**.
  prefs: []
  type: TYPE_NORMAL
- en: Why would random actions help the agent learn? Exploration.
  prefs: []
  type: TYPE_NORMAL
- en: When the agent begins, it may learn a suboptimal path to the goal and continue
    to make this choice without ever changing or learning a new route.
  prefs: []
  type: TYPE_NORMAL
- en: Beginning with a large epsilon value and slowly decreasing it allows the agent
    to thoroughly *explore* the environment as it updates its Q-values before *exploiting*
    the learned strategies. The amount we decrease epsilon by over time is called
    epsilon decay, which will make more sense soon.
  prefs: []
  type: TYPE_NORMAL
- en: Like we did with the environment, we will represent the agent with a class.
  prefs: []
  type: TYPE_NORMAL
- en: Now, before we implement the policy, we need a way to get Q-values. This is
    where our agent’s brain — or neural network — comes in.
  prefs: []
  type: TYPE_NORMAL
- en: '**The neural network**'
  prefs: []
  type: TYPE_NORMAL
- en: Without getting too off track here, a neural network is simply a massive function.
    The values go in, get passed to each layer and transformed, and some different
    values pop out at the end. Nothing more than that. The magic comes in when training
    begins.
  prefs: []
  type: TYPE_NORMAL
- en: The idea is to give the NN large amounts of labeled data like, “here is an input,
    and here is what you should output”. It slowly adjusts the values between neurons
    with each training step, attempting to get as close as possible to the given outputs,
    finding patterns within the data, and hopefully helping us predict for inputs
    the network has never seen.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b85cd6191a53eb5638e64adcf5c74f34.png)'
  prefs: []
  type: TYPE_IMG
- en: Transformation of State to Q-Values through a neural network — Image by author
  prefs: []
  type: TYPE_NORMAL
- en: '**The Agent class and defining the neural architecture** For now we will define
    the neural architecture using TensorFlow and focus on the “forward pass” of the
    data.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '*Again, if you are unfamiliar with neural networks, don’t get too caught up
    on this section. While we use activations like ‘relu’ and ‘linear’ in our model,
    a detailed exploration of activation functions is beyond the scope of this article.*'
  prefs: []
  type: TYPE_NORMAL
- en: '*All you really need to know is the model takes in state as input, the values
    are transformed at each layer in the model, and the four Q-values corresponding
    to each action are output.*'
  prefs: []
  type: TYPE_NORMAL
- en: In building our agent’s neural network, we start with an input layer that processes
    the state of the grid, represented as a one-dimensional array of size `grid_size²`.
    This is because we’ve flattened the grid to simplify the input. This layer is
    our input itself and does not need to be defined in our architecture because it
    takes no input.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we have two hidden layers. These are values we don’t see, but as our
    model learns, they are important for getting a closer approximation of the Q-value
    function:'
  prefs: []
  type: TYPE_NORMAL
- en: The first hidden layer has 128 neurons, `Dense(128, activation='relu')`, and
    takes the flattened grid as its input.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The second hidden layer consists of 64 neurons, `Dense(64, activation='relu')`,
    and further processes the information.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, the output layer, `Dense(4, activation='linear')`, comprises 4 neurons,
    corresponding to the four possible actions (up, down, left, right). This layer
    outputs the Q-values — estimates for the future reward of each action.
  prefs: []
  type: TYPE_NORMAL
- en: Typically the more complex problems you have to solve, the more hidden layers
    and neurons you will need. Two hidden layers should be plenty for our simple use-case.
  prefs: []
  type: TYPE_NORMAL
- en: Neurons and layers can and should be experimented with to find a balance between
    speed and results — each adding to the network’s ability to capture and learn
    from the nuances of the data. Like the state-space, the larger the neural network,
    the slower training will be.
  prefs: []
  type: TYPE_NORMAL
- en: '**Greedy Policy** Using this neural network, we are now able to get a Q-value
    prediction, albeit not a very good one yet, and make a decision.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: The TensorFlow neural network architecture requires input, the state, to be
    in batches. This is very useful for when you have a large number of inputs and
    you want a full batch of outputs, but it can be a little confusing when you only
    have one input to predict for.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'We can fix this by using NumPy’s `expand_dims` method, specifying `axis=0`.
    What this does is simply make it a batch of one input. For example the state of
    a grid of size 5x5:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Becomes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'When training the model you will typically use batches of size 32 or more.
    It will look something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Now that we have prepared the input for the model in the correct format, we
    can predict the Q-values for each action and choose the highest one.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: We simply give the model the state and it outputs a batch of predictions. Remember,
    because we are feeding the network a batch of one, it will return a batch of one.
    Additionally, `verbose=0` ensures that the console remains clear of routine debug
    messages every time the predict function is called.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we choose and return the index of the action with the highest value
    using `np.argmax` on the first and only entry in the batch.
  prefs: []
  type: TYPE_NORMAL
- en: In our case, the indices 0, 1, 2, and 3 will be mapped to up, down, left, and
    right respectively.
  prefs: []
  type: TYPE_NORMAL
- en: The Greedy-Policy always picks the action that has the highest reward according
    to the current Q-values, which may not always lead to the best long-term outcomes.
  prefs: []
  type: TYPE_NORMAL
- en: '**Epsilon-Greedy Policy** We have implemented the Greedy-Policy, but what we
    want to have is the Epsilon-Greedy policy. This introduces randomness into the
    agent’s choice to allow for *exploration* of the state-space.'
  prefs: []
  type: TYPE_NORMAL
- en: Just to recap, epsilon is the probability that a random action will be chosen.
    We also want some way to decrease this over time as the agent learns, allowing
    *exploitation*ofits learned policy. As briefly mentioned before, this is called
    epsilon decay.
  prefs: []
  type: TYPE_NORMAL
- en: The epsilon decay value should be set to a decimal number less than 1, which
    is used to progressively reduce the epsilon value after each step the agent takes.
  prefs: []
  type: TYPE_NORMAL
- en: Typically epsilon will start at 1, and epsilon decay will be some value very
    close to 1, like 0.998\. After each step in the training process you multiply
    epsilon by the epsilon decay.
  prefs: []
  type: TYPE_NORMAL
- en: To illustrate this, below is how epsilon will change over the training process.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: As you can see epsilon slowly approaches zero with each step. By step 1000,
    there is a 13.5% chance that a random action will be chosen. Epsilon decay is
    a value that will need to be tweaked based on the state-space. With a large state-space,
    more exploration may be necessary, or a higher epsilon decay.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c6ba59771bc11de889e39a57b19285bb.png)'
  prefs: []
  type: TYPE_IMG
- en: Decay of epsilon over steps — Image by author
  prefs: []
  type: TYPE_NORMAL
- en: Even when the agent is trained well, it is beneficial to keep a small epsilon
    value. We should define a stopping point where epsilon does not get any lower,
    epsilon end. This can be 0.1, 0.01, or even 0.001 depending on the use-case and
    complexity of the task.
  prefs: []
  type: TYPE_NORMAL
- en: In the figure above, you’ll notice epsilon stops decreasing at 0.1, the pre-defined
    epsilon end.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s update our Agent class to incorporate epsilon.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: We’ve given `epsilon`, `epsilon_decay`, and `epsilon_end` default values of
    1, 0.998, and 0.01, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: '*Remember epsilon, and its associated values, are hyper-parameters, parameters
    used to control the learning process. They can and should be experimented with
    to achieve the best result.*'
  prefs: []
  type: TYPE_NORMAL
- en: The method, `get_action`, has been updated to incorporate epsilon. If the random
    value given by `np.random.rand` is less than or equal to `epsilon`, a random action
    is chosen. Otherwise, the process is the same as before.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, if `epsilon` has not reached `epsilon_end`, we update it by multiplying
    by `epsilon_decay` like so — `self.epsilon *= self.epsilon_decay`.
  prefs: []
  type: TYPE_NORMAL
- en: '`**Agent**` **up to this point:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: We have effectively implemented the Epsilon-Greedy Policy, and we are almost
    ready to enable the agent to learn!
  prefs: []
  type: TYPE_NORMAL
- en: '5\. Affect The Environment: Finishing Up'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '`Environment` currently has methods for reseting the grid, adding the agent
    and goal, providing the current state, and printing the grid to the console.'
  prefs: []
  type: TYPE_NORMAL
- en: For the environment to be complete we need to be able to not only allow the
    agent to affect it, but also provide feedback in the form of rewards.
  prefs: []
  type: TYPE_NORMAL
- en: '**Defining the reward structure** Coming up with a good reward structure is
    the main challenge of reinforcement learning. Your problem could be perfectly
    within the capabilities of the model, but if the reward structure is not set up
    correctly it may never learn.'
  prefs: []
  type: TYPE_NORMAL
- en: The goal of the rewards is to encourage specific behavior. In our case we want
    to guide the agent towards the goal cell, defined by -1.
  prefs: []
  type: TYPE_NORMAL
- en: Similar to the layers and neurons in the network, and epsilon and its associated
    values, there can be many right (and many wrong) ways to define the reward structure.
  prefs: []
  type: TYPE_NORMAL
- en: 'The two main types of reward structures:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Sparse**: When rewards are only given in a handful of states.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Dense**: When rewards are common throughout the state-space.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With sparse rewards the agent has very little feedback to lead it. This would
    be like simply giving a set penalty for each step, and if the agent reaches the
    goal you provide one large reward.
  prefs: []
  type: TYPE_NORMAL
- en: The agent can certainly learn to reach the goal, but depending on the size of
    the state-space it can take much longer and may get stuck on a suboptimal strategy.
  prefs: []
  type: TYPE_NORMAL
- en: This is in contrast with dense reward structures, which allow the agent to train
    quicker and behave more predictably.
  prefs: []
  type: TYPE_NORMAL
- en: Dense reward structures either
  prefs: []
  type: TYPE_NORMAL
- en: have more than one goal.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: give hints throughout an episode.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The agent then has more opportunities to learn desired behavior.
  prefs: []
  type: TYPE_NORMAL
- en: For instance, pretend you’re training an agent to use a body to walk, and the
    only reward you give it is if it reaches a goal. The agent may learn to get there
    by simply inching or rolling along the ground, or not even learn at all.
  prefs: []
  type: TYPE_NORMAL
- en: Instead, if you reward the agent for heading towards the goal, staying on its
    feet, putting one foot in front of the other, and standing up straight, you will
    get a much more natural and interesting gait while also improving learning.
  prefs: []
  type: TYPE_NORMAL
- en: '**Allowing the agent to impact the environment** To even have rewards, you
    must allow the agent to interact with its world. Let’s revisit the `Environment`
    class to define this interaction.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: The above code first defines the change in coordinates associated with each
    action value. If the action 0 is chosen, then the coordinates change by (-1, 0).
  prefs: []
  type: TYPE_NORMAL
- en: '*Remember, in this scenario the coordinates are interpreted as (row, column).
    If row lowers by one, the agent moves up one cell, and if column lowers by one,
    the agent moves left one cell.*'
  prefs: []
  type: TYPE_NORMAL
- en: It then calculates the new location based on the move. If the new location is
    valid, `agent_location` is updated. Otherwise, the `agent_location` is left the
    same.
  prefs: []
  type: TYPE_NORMAL
- en: Also, `is_valid_location` simply checks if the new location is within the grid
    boundaries.
  prefs: []
  type: TYPE_NORMAL
- en: That is fairly straight forward, but what are we missing? Feedback!
  prefs: []
  type: TYPE_NORMAL
- en: '**Providing feedback** The environment needs to provide an appropriate reward
    and whether the episode is complete or not.'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s incorporate the `done` flag first to indicate that an episode is finished.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: We’ve set `done` to false by default. If the new `agent_location` is the same
    as `goal_location` then `done` is set to true. Finally, we return this value.
  prefs: []
  type: TYPE_NORMAL
- en: We are ready for our reward structure. First, I will show the implementation
    for the sparse reward structure. This would be satisfactory for a grid of around
    5x5, but we will update it to allow for a larger environment.
  prefs: []
  type: TYPE_NORMAL
- en: '**Sparse rewards** Implementing sparse rewards is quite simple. We primarily
    need to give a reward for landing on the goal.'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s also give a small negative reward for each step that doesn’t land on the
    goal and a larger one for hitting the boundary. This will encourage our agent
    to prioritize the shortest path.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Make sure to initialize `reward` so that it can be accessed after the if blocks.
    Also, check carefully for each case: valid move and achieved goal, valid move
    and did not achieve goal, and invalid move.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Dense rewards** Putting our dense reward system into practice is still quite
    simple, it just involves providing feedback more often.'
  prefs: []
  type: TYPE_NORMAL
- en: What would be a good way to reward the agent to move towards the goal more incrementally?
  prefs: []
  type: TYPE_NORMAL
- en: 'The first way is to return the negative of the Manhattan distance. The Manhattan
    distance is the distance in the row direction, plus the distance in the column
    direction, rather than as the crow flies. Here is what that looks like in code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: So, the number of steps in the row direction plus the number of steps in the
    column direction, negated.
  prefs: []
  type: TYPE_NORMAL
- en: 'The other way we can do this is provide a reward based on the direction the
    agent moves: if it moves away from the goal provide a negative reward and if it
    moves toward it provide a positive reward.'
  prefs: []
  type: TYPE_NORMAL
- en: We can calculate this by subtracting the new Manhattan distance from the previous
    Manhattan distance. It will either be 1 or -1 because the agent can only move
    one cell per step.
  prefs: []
  type: TYPE_NORMAL
- en: In our case it would make most sense to choose the second option. This should
    provide better results because it gives immediate feedback based on that step
    rather than a more general reward.
  prefs: []
  type: TYPE_NORMAL
- en: 'The code for this option:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, if the agent did not get the goal, we calculate `previous_distance`,
    `new_distance`, and then define `reward` as the difference of these.
  prefs: []
  type: TYPE_NORMAL
- en: Depending on the performance it may be appropriate to scale it, or any reward
    in the system. You can do this by simply multiplying by a number (e.g., 0.01,
    2, 100) if it needs to be higher. Their proportions need to effectively guide
    the agent to the goal. For instance, a reward of 1 for moving closer to the goal
    and a reward of 0.1 for the goal itself would not make much sense.
  prefs: []
  type: TYPE_NORMAL
- en: Rewards are proportional. If you scale each positive and negative reward by
    the same factor it should not generally effect training, aside from very large
    or very small values.
  prefs: []
  type: TYPE_NORMAL
- en: In summary, if the agent is 10 steps away from the goal, and it moves to a space
    11 steps away, then `reward` will be -1.
  prefs: []
  type: TYPE_NORMAL
- en: '**Here is the updated** `**move_agent**`**.**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: The reward for achieving the goal and attempting an invalid move should remain
    the same with this structure.
  prefs: []
  type: TYPE_NORMAL
- en: '**Step penalty** There is just one thing we are missing.'
  prefs: []
  type: TYPE_NORMAL
- en: The agent is currently not penalized for how long it takes to reach the goal.
    Our implemented reward structure has many net neutral loops. It could go back
    and forth between two locations forever, and accumulate no penalty. We can fix
    this by subtracting a small value each step, causing the penalty of moving away
    to be greater than the reward for moving closer. This illustration should make
    it much clearer.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3a66e1dec8556ff6db6d5b751268180b.png)'
  prefs: []
  type: TYPE_IMG
- en: Reward paths with and without a step penalty — Image by author
  prefs: []
  type: TYPE_NORMAL
- en: Imagine the agent is starting at the left most node and must make a decision.
    Without a step penalty, it could choose to go forward, then back as many times
    as it wants and its total reward would be 1 before finally moving to the goal.
  prefs: []
  type: TYPE_NORMAL
- en: So mathematically, looping 1000 times and then moving to the goal is just as
    valid as moving straight there.
  prefs: []
  type: TYPE_NORMAL
- en: Try to imagine looping in either case and see how penalty is accumulated (or
    not accumulated).
  prefs: []
  type: TYPE_NORMAL
- en: Let’s implement this.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: That’s it. The agent should now be incentivized to take the shortest path, preventing
    looping behavior.
  prefs: []
  type: TYPE_NORMAL
- en: '**Okay, but what is the point?** At this point you may be thinking it is a
    waste of time to define a reward system and train an agent for a task that could
    be completed with much simpler algorithms.'
  prefs: []
  type: TYPE_NORMAL
- en: And you would be correct.
  prefs: []
  type: TYPE_NORMAL
- en: The reason we are doing this is to learn how to think about guiding your agent
    to its goal. In this case it may seem trivial, but what if the agent’s environment
    included items to pick up, enemies to battle, obstacles to go through, and more?
  prefs: []
  type: TYPE_NORMAL
- en: Or a robot in the real world with dozens of sensors and motors that it needs
    to coordinate in sequence to navigate complex and varied environments?
  prefs: []
  type: TYPE_NORMAL
- en: Designing a system to do these things using traditional programming would be
    quite difficult and most certainly would not behave near as organic or general
    as using RL and a good reward structure to encourage an agent to learn optimal
    strategies.
  prefs: []
  type: TYPE_NORMAL
- en: Reinforcement learning is most useful in applications where defining the exact
    sequence of steps required to complete the task is difficult or impossible due
    to the complexity and variability of the environment. The only thing you need
    for RL to work is to be able to define what is useful behavior and what behavior
    should be discouraged.
  prefs: []
  type: TYPE_NORMAL
- en: '**The final Environment method —** `**step**`**.** With the each component
    of `Environment` in place we can now define the heart of the interaction between
    the agent and the environment.'
  prefs: []
  type: TYPE_NORMAL
- en: Thankfully, it is quite simple.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: '`step` first moves the agent in the environment and records `reward` and `done`.
    Then it gets the state immediately following this interaction, `next_state`. Then
    if `render_on` is set to true the grid is rendered.'
  prefs: []
  type: TYPE_NORMAL
- en: Finally, `step` returns the recorded values, `reward`, `next_state` and `done`.
  prefs: []
  type: TYPE_NORMAL
- en: These will be essential to building the experiences our agent will learn from.
  prefs: []
  type: TYPE_NORMAL
- en: Congratulations! You have officially completed the construction of the environment
    for your DRL gym.
  prefs: []
  type: TYPE_NORMAL
- en: '**Below is the completed** `**Environment**` **class.**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: We have gone through a lot at this point. It may be beneficial to return to
    [the big picture](#16af) at the beginning and reevaluate how each part interacts
    using your new knowledge before moving on.
  prefs: []
  type: TYPE_NORMAL
- en: '6\. Learn From Experiences: Experience Replay'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The agent’s model and policy, along with the environment’s reward structure
    and mechanism for taking steps have all been completed, but we need some way to
    remember the past so that the agent can learn from it.
  prefs: []
  type: TYPE_NORMAL
- en: This can be done by saving the experiences.
  prefs: []
  type: TYPE_NORMAL
- en: 'Each experience consists of a few things:'
  prefs: []
  type: TYPE_NORMAL
- en: '**State**: The state before an action is taken.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Action**: What action was taken in this state.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Reward**: Positive or negative feedback the agent received from the environment
    based on its action.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Next State**: The state immediately following the action, allowing the agent
    to act, not just based on the consequences of the current state, but many states
    in advance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Done**: Indicates the end of an experience, letting the agent know if the
    task has been completed or not. It can be either true or false at each step.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*These terms should not be new to you, but it never hurts to see them again!*'
  prefs: []
  type: TYPE_NORMAL
- en: Each experience is associated with exactly one step from the agent. This will
    provide all of the context needed to train it.
  prefs: []
  type: TYPE_NORMAL
- en: '**The** `**ExperienceReplay**` **class**'
  prefs: []
  type: TYPE_NORMAL
- en: To keep track of and serve these experiences when needed, we will define one
    last class, `ExperienceReplay`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: This class will take `capacity`, an integer value that defines the maximum number
    of experiences we will save at a time, and `batch_size`, an integer value that
    determines how many experiences we sample at a time for training.
  prefs: []
  type: TYPE_NORMAL
- en: '**Batching the experiences** If you remember, the neural network in the `Agent`
    class takes batches of input. While we only used a batch of size one to predict,
    this would be incredibly inefficient for training. Typically, batches of size
    32 or higher are more common.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Batching the input for training does two things:'
  prefs: []
  type: TYPE_NORMAL
- en: Increases efficiency because it allows for parallel processing of multiple data
    points, reducing computational overhead and making better use of GPU or CPU resources.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Helps the model learn more consistently, as it’s learning from a variety of
    examples at once, which can make it better at handling new, unseen data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Memory** The `memory` will be a deque (short for double-ended queue). This
    allows us to add new experiences to the front, and as the max length defined by
    `capacity` is reached, the deque will remove them without having to shift each
    element as you would with a Python list. This can greatly improve speed when `capacity`
    is set to 10,000 or more.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Experience** Each experience will be defined as a `namedtuple`. Although,
    many other data structures would work, this will improve readability as we extract
    each part as needed in training.'
  prefs: []
  type: TYPE_NORMAL
- en: '`**add_experience**` **and** `**sample_batch**` **implementation** Adding a
    new experience and sampling a batch are rather straightforward.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: The method `add_experience` creates a `namedtuple` with each part of an experience,
    `state`, `action`, `reward`, `next_state`, and `done`, and appends it to `memory`.
  prefs: []
  type: TYPE_NORMAL
- en: '`sample_batch` is just as simple. It gets and returns a random sample from
    `memory` of size `batch_size`.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/36c64b8dcf5ffa085576f0b2b77e98e5.png)'
  prefs: []
  type: TYPE_IMG
- en: Experience Replay storing experiences for Agent to batch and learn from — Image
    by author
  prefs: []
  type: TYPE_NORMAL
- en: '**The last method needed —** `**can_provide_sample**` Finally, it would be
    useful to be able to check if `memory` contains enough experiences to provide
    us with a full sample before attempting to get a batch for training.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: '**Completed** `**ExperienceReplay**` **class…**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: With the mechanism for saving each experience and sampling from them in place,
    we can return to the `Agent` class to finally enable learning.
  prefs: []
  type: TYPE_NORMAL
- en: '7\. Define The Agent’s Learning Process: Fitting The NN'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The goal, when training the neural network, is to get the Q-values it produces
    to accurately represent the future reward each choice will provide.
  prefs: []
  type: TYPE_NORMAL
- en: Essentially, we want the network to learn to predict how valuable each decision
    is, considering not just the immediate reward, but also the rewards it could lead
    to in the future.
  prefs: []
  type: TYPE_NORMAL
- en: '**Incorporating future rewards** To achieve this, we incorporate the Q-values
    of the subsequent state into the training process.'
  prefs: []
  type: TYPE_NORMAL
- en: When the agent takes an action and moves to a new state, we look at the Q-values
    in this new state to help inform the value of the previous action. In other words,
    the potential future rewards influence the perceived value of the current choices.
  prefs: []
  type: TYPE_NORMAL
- en: '**The** `**learn**` **method**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: Using the provided batch, `experiences`, we will extract each part using list
    comprehension and the `namedtuple` values we defined earlier in `ExperienceReplay`.
    Then we convert each one into a NumPy array to improve efficiency and to align
    with what the model expects, as explained previously.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we use the model to predict the Q-values of the current state the action
    was taken in and the state immediately following it.
  prefs: []
  type: TYPE_NORMAL
- en: Before continuing with the `learn` method, I need to explain something called
    the discount factor.
  prefs: []
  type: TYPE_NORMAL
- en: '**Discounting future rewards — the role of gamma** Intuitively, we know that
    immediate rewards are generally prioritized when all else is equal. (Would you
    like your paycheck today or next week?)'
  prefs: []
  type: TYPE_NORMAL
- en: Representing this mathematically can seem much less intuitive. When considering
    the future, we don’t want it to be equally important (weighted) as the present.
    By how much we discount the future, or lower its effect on each decision, is defined
    by gamma (commonly denoted by the greek letter γ).
  prefs: []
  type: TYPE_NORMAL
- en: Gamma can be adjusted, with higher values encouraging planning and lower values
    encouraging more short sighted behavior. We will use a default value of 0.99.
  prefs: []
  type: TYPE_NORMAL
- en: '*The discount factor will pretty much always be between 0 and 1\. A discount
    factor greater than 1, prioritizing the future over the present, would introduce
    unstable behavior and has little to no practical applications.*'
  prefs: []
  type: TYPE_NORMAL
- en: '**Implementing gamma and defining the target Q-values** Recall that in the
    context of training a neural network, the process hinges on two key elements:
    the input data we provide and the corresponding outputs we want the network to
    learn to predict.'
  prefs: []
  type: TYPE_NORMAL
- en: We will need to provide the network with some target Q-values that are updated
    based on the reward given by the environment at this specific state and action,
    plus the discounted (by gamma) predicted reward of the best action at the next
    state.
  prefs: []
  type: TYPE_NORMAL
- en: I know that is a lot to take in, but it will be best explained through implementation
    and example.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: We’ve defined the class attribute, `gamma`, with a default value of 0.99.
  prefs: []
  type: TYPE_NORMAL
- en: Then, after getting the prediction for `state` and `next_state` that we implemented
    above, we initialize `target_q_values` to the current Q-values. These will be
    updated in the following loop.
  prefs: []
  type: TYPE_NORMAL
- en: '**Updating** `**target_q_values**` We loop through each `experience` in the
    batch with two cases for updating the values:'
  prefs: []
  type: TYPE_NORMAL
- en: If the episode is `done`, the `target_q_value` for that action is simply the
    reward given because there is no relevant `next_q_value`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Otherwise, the episode is not `done`, and the `target_q_value` for that action
    becomes the reward given, plus the discounted Q-value of the predicted next action
    in `next_q_values`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Update if `done` is true:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'Update if `done` is false:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: The syntax here, `target_q_values[i, actions[i]]`, can seem confusing but it’s
    essentially the Q-value of the i-th experience, for the action `actions[i]`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: '*This is NumPy’s equivalent to* `*[i][actions[i]]*` *in Python lists. Remember
    each action is an index (0 to 3).*'
  prefs: []
  type: TYPE_NORMAL
- en: '**How** `**target_q_values**` **is updated**'
  prefs: []
  type: TYPE_NORMAL
- en: Just to illustrate this more clearly I will show how `target_q_values` more
    closely aligns with the actual rewards given as we train. Remember that we are
    working with a batch. This will be a batch of three with example values for simplicity.
  prefs: []
  type: TYPE_NORMAL
- en: Also, ensure that you understand that the entries in `experiences` are independent.
    Meaning this is not a sequence of steps, but a random sample from a collection
    of individual experiences.
  prefs: []
  type: TYPE_NORMAL
- en: Pretend the values of `actions`, `rewards`, `dones`, `current_q_values`, and
    `next_q_values` are as follows.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: We then copy `current_q_values` into `target_q_values` to be updated.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: Then, for every experience in the batch we can show the associated values.
  prefs: []
  type: TYPE_NORMAL
- en: '*This is not code, but simply an example of the values at each stage. If you
    get lost, be sure to refer back to the initial values to see where each is coming
    from.*'
  prefs: []
  type: TYPE_NORMAL
- en: '**Entry 1**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: Because `dones[i]` is false for this experience we need to consider the `next_q_values`
    and apply `gamma` (0.99).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: Why get the largest of `next_q_values[i]`? Because that would be the next action
    chosen and we want the estimated reward (Q-value).
  prefs: []
  type: TYPE_NORMAL
- en: Then we update the i-th `target_q_values` at the index corresponding to `actions[i]`
    to the reward for this state/action pair plus the discounted reward for the next
    state/action pair.
  prefs: []
  type: TYPE_NORMAL
- en: Here are the target values in this experience after being updated.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, for the current state, choosing 1 (down) is now even more desirable
    because the value is higher and this behavior has been reinforced.
  prefs: []
  type: TYPE_NORMAL
- en: '*It may help to calculate these yourself to really make it clear.*'
  prefs: []
  type: TYPE_NORMAL
- en: '**Entry 2**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: '`dones[i]` is also false here, so we do need to consider the `next_q_values`.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: Again, updating the i-th experience’s `target_q_values` at the index `actions[i]`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: Choosing 2 (left) is now less desirable because the Q-value is lower and this
    behavior is discouraged.
  prefs: []
  type: TYPE_NORMAL
- en: '**Entry 3**'
  prefs: []
  type: TYPE_NORMAL
- en: Finally, the last entry in the batch.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: '`dones[i]` for this entry is true, indicating that the episode is complete
    and there will be no further actions taken. This means we do not consider `next_q_values`
    in our update.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: Notice that we simply set `target_q_values[i, action[i]]` to the value of `rewards[i]`,
    because no more actions will be taken — there is no future to consider.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: Choosing 2 (left) in this and similar states will now be much more desirable.
  prefs: []
  type: TYPE_NORMAL
- en: This is the state where the goal was to the left of the agent, so when that
    action was chosen the full reward was given.
  prefs: []
  type: TYPE_NORMAL
- en: Although it can seem rather confusing, the idea is simply to make updated Q-values
    that accurately represent the rewards given by the environment to provide to the
    neural network. That is what the NN is supposed to approximate.
  prefs: []
  type: TYPE_NORMAL
- en: Try to imagine it in reverse. Because the reward for reaching the goal is substantial,
    it will create a propagation effect throughout the states leading to the one where
    the agent achieves the goal. This is the power of gamma in considering the next
    state and its role in the rippling of reward values backward through the state-space.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1e705737fe068437dec911c5df971487.png)'
  prefs: []
  type: TYPE_IMG
- en: Rippling effect of rewards across the state-space — Image by author
  prefs: []
  type: TYPE_NORMAL
- en: Above is a simplified version of the Q-values and the effect of the discount
    factor, only considering the reward for the goal, not the incremental rewards
    or penalties.
  prefs: []
  type: TYPE_NORMAL
- en: Pick any cell in the grid and move to the highest quality adjacent cell. You
    will see that it always provides an optimal path to the goal.
  prefs: []
  type: TYPE_NORMAL
- en: '*This effect is not immediate. It requires the agent to explore the state and
    action-space to gradually learn and adjust its strategy, building an understanding
    of how different actions lead to varying rewards over time.*'
  prefs: []
  type: TYPE_NORMAL
- en: If the reward structure was carefully crafted, this will slowly guide our agent
    towards taking more advantageous actions.
  prefs: []
  type: TYPE_NORMAL
- en: '**Fitting the neural network** For the `learn` method, the last thing there
    is to do is provide the agent’s neural network with `states` and their associated
    `target_q_values`. TensorFlow will then handle updating the weights to more closely
    predict these values on similar states.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'The only new part is `self.model.fit(states, target_q_values, epochs=1, verbose=0)`.
    `fit` takes two main arguments: the input data and the target values we want.
    In this case, our input is a batch `states` and the target values are the updated
    Q-values for each state.'
  prefs: []
  type: TYPE_NORMAL
- en: '`epochs=1` simply sets the number of times you want the network to try to fit
    to the data. One is enough because we want it to be able to generalize well, not
    to fit to this specific batch. `verbose=0` simply tells TensorFlow not to print
    debug messages like progress bars.'
  prefs: []
  type: TYPE_NORMAL
- en: The `Agent` class is now equipped with the ability to learn from experiences
    but it needs two more simple methods — `save` and `load`.
  prefs: []
  type: TYPE_NORMAL
- en: '**Saving and loading trained models** Saving and loading the model prevents
    us from having to completely retrain every time we need it. We can use the simple
    TensorFlow methods that only take one argument, `file_path`.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: Make a directory called models, or whatever you like, and then you can save
    your trained model at set intervals. These files end in .h5\. So whenever you
    want to save your model you simply call `agent.save(‘models/model_name.h5’)`.
    The same goes for when you want to load one.
  prefs: []
  type: TYPE_NORMAL
- en: '**Full** `**Agent**` **class**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: Each class of your deep reinforcement learning gym is now complete! You have
    successfully coded `Agent,` `Environment`, and `ExperienceReplay`. The only thing
    left is the main training loop.
  prefs: []
  type: TYPE_NORMAL
- en: '8\. Executing The Training Loop: Putting It All Together'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We are at the final stretch of the project! Every piece we have coded, `Agent`,
    `Environment`, and `ExperienceReplay`, needs some way to interact.
  prefs: []
  type: TYPE_NORMAL
- en: This will be the main program where each episode is run and where we define
    our hyper-parameters like `epsilon`.
  prefs: []
  type: TYPE_NORMAL
- en: Although it is fairly simple, I will break up each part as we code it to make
    it more clear.
  prefs: []
  type: TYPE_NORMAL
- en: '**Initialize each part** First, we set `grid_size` and use the classes we have
    made to initialize each instance.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: Now we have each part we need for the main training loop.
  prefs: []
  type: TYPE_NORMAL
- en: '**Episode and step cap** Next, we will define the number of episodes we want
    the training to run, and the max number of steps allowed in each episode.'
  prefs: []
  type: TYPE_NORMAL
- en: Capping the number of steps helps ensure our agent doesn’t get stuck in a loop
    and encourages shorter paths. We will be fairly generous and for a 5x5 we will
    set the max to 200\. This will need to be increased for larger environments.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: '**Episode loop** In each episode we will reset `environment` and save the initial
    `state`. Then we perform each step until either `done` is true or `max_steps`
    is reached. Finally, we save the model. The logic for each step has not been implemented
    quite yet.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: Notice we name the model using `grid_size` because the NN architecture will
    be different for each input size. Trying to load a 5x5 model into a 10x10 architecture
    will throw an error.
  prefs: []
  type: TYPE_NORMAL
- en: '**Step logic** Finally, inside of the step loop we will lay out the interaction
    between each piece as discussed before.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: For every step of the episode, we start by printing the episode and step number
    to give us some information about where we are in training. Additionally, you
    can print `epsilon` to see what percentage of the agent’s actions are random.
    It also helps because if you want to stop for any reason you can restart the agent
    at the same `epsilon` value.
  prefs: []
  type: TYPE_NORMAL
- en: After printing the information, we use the `agent` policy to get `action` from
    this `state` to take a step in `environment`, recording the returned values.
  prefs: []
  type: TYPE_NORMAL
- en: Then we save `state`, `action`, `reward`, `next_state`, and `done` as an experience.
    If `experience_replay` has enough memory we train `agent` on a random batch of
    `experiences`.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we set `state` to `next_state` and check if the episode is `done`.
  prefs: []
  type: TYPE_NORMAL
- en: Once you’ve run at least one episode you’ll have a model saved you can load
    and either continue where you left off or evaluate the performance.
  prefs: []
  type: TYPE_NORMAL
- en: After you initialize `agent` simply use its load method similar to how we saved
    — `agent.load(f’models/model_{grid_size}.h5')`
  prefs: []
  type: TYPE_NORMAL
- en: You can also add a slight delay at each step when you are evaluating the model
    using time — `time.sleep(0.5)`. This causes each step to pause for half a second.
    Make sure you include `import time`.
  prefs: []
  type: TYPE_NORMAL
- en: '**Completed training loop**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: When you need `time.sleep` or `agent.load` you can simply uncomment them.
  prefs: []
  type: TYPE_NORMAL
- en: '**Running the program** Give it a run! You should be able to successfully train
    the agent to complete the goal up to an 8x8 or so grid environment. Any grid size
    much larger than this and the training begins to struggle.'
  prefs: []
  type: TYPE_NORMAL
- en: Try to see how large you can get the environment. You can do a few things such
    as adding layers and neurons to the neural network, changing `epsilon_decay`,
    or giving more time to train. Doing this can solidify your understanding of each
    part.
  prefs: []
  type: TYPE_NORMAL
- en: '*For instance, you may notice* `*epsilon*` *reaches* `*epsilon_end*` *rather
    fast. Don’t be afraid to change the* `*epsilon_decay*` *to values of 0.9998 or
    0.99998 if you would like.*'
  prefs: []
  type: TYPE_NORMAL
- en: As the grid size grows, the state the network is fed gets exponentially larger.
  prefs: []
  type: TYPE_NORMAL
- en: I’ve included a short bonus section at the end to fix this and to demonstrate
    that there are many ways you can represent the environment for the agent.
  prefs: []
  type: TYPE_NORMAL
- en: '**9\. Wrapping It Up**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Congratulations on completing this comprehensive journey through the world of
    Reinforcement and Deep Q-Learning!
  prefs: []
  type: TYPE_NORMAL
- en: Although there is always more to cover, you could walk away having acquired
    important insights and skills.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this guide you:'
  prefs: []
  type: TYPE_NORMAL
- en: Were introduced to the core concepts of reinforcement learning and why it’s
    a crucial area in AI.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Built a simple environment, laying the groundwork for agent interaction and
    learning.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Defined the agent’s Neural Network architecture for use with Deep Q-Learning,
    enabling your agent to make decisions in more complex environments than traditional
    Q-Learning.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understood why exploration is important before exploiting the learned strategy
    and implemented the Epsilon-Greedy policy.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implemented the reward system to guide the agent to the goal and learned the
    differences between sparse and dense rewards.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Designed the experience replay mechanism, allowing the agent to learn from past
    experiences.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gained hands-on experience in fitting the neural network, a critical process
    where the agent improves its performance based on feedback from the environment.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Put all these pieces together in a training loop, witnessing the agent’s learning
    process in action and tweaking it for optimal performance.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By now, you should feel confident in your understanding of Reinforcement Learning
    and Deep Q-Learning. You’ve built a solid foundation, not just in theory but also
    in practical application, by constructing a DRL gym from scratch.
  prefs: []
  type: TYPE_NORMAL
- en: This knowledge equips you to tackle more complex RL problems and paves the way
    for further exploration in this exciting field of AI.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/782a7e8b27260d7f702d6be467400053.png)'
  prefs: []
  type: TYPE_IMG
- en: Agar.io inspired game where agents are encouraged to eat one another to win
    — GIF by author
  prefs: []
  type: TYPE_NORMAL
- en: Above is a grid game inspired by Agar.io where agents are encouraged to grow
    in size, often from eating one another. At each step the environment was plotted
    on a graph using the Python library, [Matplotlib](https://matplotlib.org/). The
    boxes around the agents are their field of view. This is fed to them as their
    state from the environment as a flattened grid, similar to what we’ve done in
    our system.
  prefs: []
  type: TYPE_NORMAL
- en: Games like this, and a myriad of other uses, can be crafted with simple modifications
    to what you have made here.
  prefs: []
  type: TYPE_NORMAL
- en: Remember though, Deep Q-Learning is only suitable for a discrete action-space
    — one that has a finite number of distinct actions. For a continuous action-space,
    like in a physics based environment, you will need to explore other methods in
    the world of DRL.
  prefs: []
  type: TYPE_NORMAL
- en: '10\. Bonus: Optimize State Representation'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Believe it or not, the way we have currently been representing state is not
    the most optimal for this use.
  prefs: []
  type: TYPE_NORMAL
- en: It is actually incredibly inefficient.
  prefs: []
  type: TYPE_NORMAL
- en: For a grid of 100x100 there are 99,990,000 possible states. Not only would the
    model need to be quite large considering the size of the input — 10,000 values,
    it would require a significant volume of training data. Depending on the computational
    resources one has available this could take days or weeks.
  prefs: []
  type: TYPE_NORMAL
- en: Another downfall is flexibility. The model currently is stuck at one grid size.
    If you want to use a different sized grid, you need to train another model completely
    from scratch.
  prefs: []
  type: TYPE_NORMAL
- en: We need a way to represent the state that significantly reduces the state-space
    and translates well to any grid size.
  prefs: []
  type: TYPE_NORMAL
- en: '**The better way** While there are several ways to do this, the simplest, and
    probably most effective, is to use the relative distance from the goal.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Rather than the state for a 5x5 grid looking like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: 'It can be represented with only two values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: Using this method would lower the state-space of a 100x100 grid from 99,990,000
    to 39,601!
  prefs: []
  type: TYPE_NORMAL
- en: Not only that, but it can generalize much better. It simply has to learn that
    moving down is the right choice when the first value is negative, and moving right
    is appropriate when the second value is negative, with the opposite actions applying
    for positive values.
  prefs: []
  type: TYPE_NORMAL
- en: This enables the model to only explore a fraction of the state-space.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ef1ad595d7674550d9fdd21f6dda6934.png)'
  prefs: []
  type: TYPE_IMG
- en: 25x25 heat-map of agent’s decisions at each cell with the goal in the center—GIF
    by author
  prefs: []
  type: TYPE_NORMAL
- en: Above is the progression of a model’s learning, trained on a 25x25 grid. It
    shows the agent’s choice color coded at each cell with the goal in the center.
  prefs: []
  type: TYPE_NORMAL
- en: At first, during the exploration stage, the agent’s strategy is completely off.
    You can see that it chooses to go up when it is above the target, down when it
    is below, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: But in under 10 episodes it learns a strategy that allows it to reach the goal
    in the shortest number of steps from any cell.
  prefs: []
  type: TYPE_NORMAL
- en: This also applies with the goal at any location.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0cb8f50fdea4e5950526f584e02abe93.png)'
  prefs: []
  type: TYPE_IMG
- en: Four 25x25 heat-maps of the model applied to various goal locations — Image
    by author
  prefs: []
  type: TYPE_NORMAL
- en: And finally it generalizes its learning incredibly well.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/30ea0bb6766421c6ed6a30186738e2fb.png)'
  prefs: []
  type: TYPE_IMG
- en: 201x201 heat-map of the 25x25 model’s decisions, showing generalization — Image
    by author
  prefs: []
  type: TYPE_NORMAL
- en: This model has only ever seen a 25x25 grid, but it could use its strategy on
    a far larger environment — 201x201\. With an environment this size there are 1,632,200,400
    agent-goal permutations!
  prefs: []
  type: TYPE_NORMAL
- en: Let’s update our code with this radical improvement.
  prefs: []
  type: TYPE_NORMAL
- en: '**Implementation** There really isn’t much we need to do to get this working,
    thankfully.'
  prefs: []
  type: TYPE_NORMAL
- en: The first thing is update `get_state` in `Environment`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: Rather than a flattened version of the grid, we calculate the distance from
    the target and return it as a NumPy array. The `*` operator simply unpacks the
    tuple into individual components. It will have the same effect as doing this —
    `state = np.array([relative_distance[0], relative_distance[1])`.
  prefs: []
  type: TYPE_NORMAL
- en: Also, in `move_agent` we can update the penalty for hitting the boundary to
    be the same as moving away from the target. This is so that when you change the
    grid size, the agent is not discouraged from moving outside where it was originally
    trained.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: '**Updating the neural architecture** Currently our TensorFlow model looks like
    this. I’ve excluded everything else for simplicity.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: If you remember, our model architecture needs to have a consistent input. In
    this case, the input size relied on `grid_size`.
  prefs: []
  type: TYPE_NORMAL
- en: With our updated state representation, each state will only have two values
    no matter what `grid_size` is. We can update the model to expect this. Also, we
    can remove `self.grid_size` altogether because the `Agent` class no longer relies
    on it.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: The `input_shape` parameter expects a tuple representing the state of the input.
  prefs: []
  type: TYPE_NORMAL
- en: '`(2,)` specifies a one-dimensional array with two values. Looking something
    like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: 'While `(2,1)`, a two-dimensional array for example, specifies two rows and
    one column. Looking something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: Finally, we’ve lowered the number of neurons in our hidden layers to 64 and
    32 respectively. With this simple state representation it’s still probably overkill,
    but should run plenty fast enough.
  prefs: []
  type: TYPE_NORMAL
- en: '*When you start training, try to see how few neurons you need for the model
    to effectively learn. You can even try removing the second layer if you like.*'
  prefs: []
  type: TYPE_NORMAL
- en: '**Fixing the main training loop** The training loop requires very few adjustments.
    Let’s update it to match our changes.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: Because `agent` no longer needs the `grid_size`, we can remove it to prevent
    any errors.
  prefs: []
  type: TYPE_NORMAL
- en: We also no longer have to give the model different names for each `grid_size`,
    since one model now works on any size.
  prefs: []
  type: TYPE_NORMAL
- en: If you’re curious about `ExperienceReplay`, it will remain the same.
  prefs: []
  type: TYPE_NORMAL
- en: '*Please note that there is no one-size-fits-all state representation. In some
    cases it may make sense to provide the full grid like we did, or a subsection
    of it like I’ve done with the multi-agent system in section 9\. The goal is to
    find a balance between simplifying the state-space and providing adequate information
    for the agent to learn.*'
  prefs: []
  type: TYPE_NORMAL
- en: '**Hyper-parameters** Even a simple environment like ours requires adjustments
    of the hyper-parameters. Remember that these are the values we can change that
    effect training.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Each one we have discussed includes:'
  prefs: []
  type: TYPE_NORMAL
- en: '`epsilon`, `epsilon_decay`, `epsilon_end` (exploration/exploitation)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`gamma` (discount factor)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: number of neurons and layers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`batch_size`, `capacity` (experience replay)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`max_steps`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are plenty of others, but there is just one more we will discuss that
    will be critical for learning.
  prefs: []
  type: TYPE_NORMAL
- en: '**Learning rate** The Learning Rate (LR) is a hyper-parameter of the neural
    network model.'
  prefs: []
  type: TYPE_NORMAL
- en: It basically tells the neural network how much to adjust its weights — values
    used for transformation of the input — each time it is fit to the data.
  prefs: []
  type: TYPE_NORMAL
- en: The values of LR typically range from 1 down to 0.0000001, with the most common
    being values like 0.01, 0.001, and 0.0001.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/39e0d56351e0af4323dff78b2edb45c5.png)'
  prefs: []
  type: TYPE_IMG
- en: Sub-optimal learning rate that may never converge on an optimal strategy — Image
    by author
  prefs: []
  type: TYPE_NORMAL
- en: If the learning rate is too low, it might not update the Q-values quickly enough
    to learn an optimal strategy, a process known as convergence. If you notice that
    there seems to be a stagnation in learning, or none at all, this could be a sign
    that the learning rate is not high enough.
  prefs: []
  type: TYPE_NORMAL
- en: '*While these diagrams on learning rate are greatly simplified, they should
    get the basic idea across.*'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a754d683568838fcd6a4d3d60c23945d.png)'
  prefs: []
  type: TYPE_IMG
- en: Sub-optimal learning rate that causes the Q-Values to continue to grow exponentially
    — Image by author
  prefs: []
  type: TYPE_NORMAL
- en: One the other side, a learning rate that is too high can cause your values to
    “explode” or become increasingly large. The adjustments the model makes are too
    great, causing it to diverge — or get worse over time.
  prefs: []
  type: TYPE_NORMAL
- en: '**What is the perfect learning rate?** How long is a piece of string?'
  prefs: []
  type: TYPE_NORMAL
- en: In many cases you just have to use simple trial and error. A good way to determine
    if your learning rate is the issue is to check the output of the model.
  prefs: []
  type: TYPE_NORMAL
- en: This is exactly the issue I was facing when training this model. After switching
    to the simplified state representation, it refused to learn. The agent would actually
    continue to go to the bottom right of the grid after extensively testing each
    hyper-parameter.
  prefs: []
  type: TYPE_NORMAL
- en: It did not make sense to me, so I decided to take a look at the Q-values output
    by the model in the `Agent` `get_action` method.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: This is an example of exploding values.
  prefs: []
  type: TYPE_NORMAL
- en: In TensorFlow the optimizer we are using to adjust the weights, Adam, has a
    default learning rate of 0.001\. For this specific case it happened to be much
    too high.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/34493346947c65275aac5e961813ab08.png)'
  prefs: []
  type: TYPE_IMG
- en: Balanced learning rate, eventually converging to the Optimal Strategy — Image
    by author
  prefs: []
  type: TYPE_NORMAL
- en: After testing various values, a sweet spot seems to be at 0.00001.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s implement this.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: Feel free to adjust this and observe how the Q-values are affected. Also, make
    sure to import Adam.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, you can once again begin training!
  prefs: []
  type: TYPE_NORMAL
- en: '**Heat-map code** Below is the code for plotting your own heat-map as shown
    previously if you are interested.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs: []
  type: TYPE_PRE
- en: Simply import it into your training loop and run it however often you would
    like.
  prefs: []
  type: TYPE_NORMAL
- en: '**Next steps** Once you have effectively trained your model and experimented
    with the hyper-parameters, I encourage you to truly make it your own.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Some ideas for expanding the system:'
  prefs: []
  type: TYPE_NORMAL
- en: Add obstacles between the agent and goal
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Create a more varied environment, possibly with randomly generated rooms and
    pathways
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implement a multi-agent cooperation/competition system — hide and seek
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Create a Pong inspired game
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implement resource management such as a hunger or energy system where the agent
    needs to collect food on the way to the goal
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here is an example that goes beyond our simple grid system:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0537b23b3da6d047d96e6e033df18e5a.png)'
  prefs: []
  type: TYPE_IMG
- en: Flappy Bird inspired game where the agent must avoid the pipes to survive —
    GIF by author
  prefs: []
  type: TYPE_NORMAL
- en: Using [Pygame](https://www.pygame.org/news), a popular Python library for making
    2d games, I constructed a Flappy Bird clone. Then I defined the interactions,
    constraints, and reward structure in our prebuilt `Environment` class.
  prefs: []
  type: TYPE_NORMAL
- en: I represented the state as the current velocity and location of the agent, the
    distance to the closest pipe, and the location of the opening.
  prefs: []
  type: TYPE_NORMAL
- en: For the `Agent` class I simply updated the input size to `(4,)`, added more
    layers to the NN, and updated the network to only output two values — jump or
    not jump.
  prefs: []
  type: TYPE_NORMAL
- en: You can find and run this in the `flappy_bird` directory on the GitHub [repo](https://github.com/HestonCV/rl-gym-from-scratch).
    Make sure to `pip install pygame`.
  prefs: []
  type: TYPE_NORMAL
- en: This shows that what you’ve built is applicable with a variety of environments.
    You can even have the agent explore a 3d environment or perform more abstract
    tasks like stock trading.
  prefs: []
  type: TYPE_NORMAL
- en: While expanding your system don’t be afraid to get creative with your environment,
    state representation, and reward system. Like the agent, we learn best by exploration!
  prefs: []
  type: TYPE_NORMAL
- en: I hope building a DRL gym from scratch has opened your eyes to the beauty of
    AI and has inspired you to dive deeper.
  prefs: []
  type: TYPE_NORMAL
- en: '*This article was inspired by the* [*Neural Networks From Scratch In Python
    Book*](https://nnfs.io/) *and* [*youtube series*](https://www.youtube.com/watch?v=Wo5dMEP_BbI&list=PLQVvvaa0QuDcjD5BAw2DxE6OF2tius3V3)
    *by Harrison Kinsley (sentdex) and Daniel Kukieł. The conversational style and
    from scratch code implementations really solidified my understanding of Neural
    Networks.*'
  prefs: []
  type: TYPE_NORMAL
