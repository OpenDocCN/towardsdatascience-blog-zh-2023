# è¯„ä¼°æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰çš„ 3 æ­¥æ³•

> åŸæ–‡ï¼š[`towardsdatascience.com/a-3-step-approach-to-evaluate-a-retrieval-augmented-generation-rag-5acf2aba86de`](https://towardsdatascience.com/a-3-step-approach-to-evaluate-a-retrieval-augmented-generation-rag-5acf2aba86de)

## åœæ­¢éšæœºé€‰æ‹©ä½ çš„ RAG å‚æ•°

[](https://ahmedbesbes.medium.com/?source=post_page-----5acf2aba86de--------------------------------)![Ahmed Besbes](https://ahmedbesbes.medium.com/?source=post_page-----5acf2aba86de--------------------------------)[](https://towardsdatascience.com/?source=post_page-----5acf2aba86de--------------------------------)![Towards Data Science](https://towardsdatascience.com/?source=post_page-----5acf2aba86de--------------------------------) [Ahmed Besbes](https://ahmedbesbes.medium.com/?source=post_page-----5acf2aba86de--------------------------------)

Â·å‘è¡¨äº[Towards Data Science](https://towardsdatascience.com/?source=post_page-----5acf2aba86de--------------------------------) Â·9 åˆ†é’Ÿé˜…è¯»Â·2023 å¹´ 11 æœˆ 23 æ—¥

--

![](img/3398735846db1df7203b650519c9909b.png)

ç…§ç‰‡ç”±[Adi Goldstein](https://unsplash.com/@adigold1?utm_source=medium&utm_medium=referral)æ‹æ‘„ï¼Œæ¥æºäº[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)

è°ƒæ•´ä½ çš„ RAG ä»¥è·å¾—æœ€ä½³æ€§èƒ½éœ€è¦æ—¶é—´ï¼Œå› ä¸ºè¿™å–å†³äºå„ç§ç›¸äº’å…³è”çš„å‚æ•°ï¼š**å—å¤§å°ã€é‡å ã€æ£€ç´¢çš„å‰ K ä¸ªæ–‡æ¡£ã€åµŒå…¥æ¨¡å‹ã€LLM ç­‰ã€‚**

æœ€ä½³ç»„åˆé€šå¸¸ä¾èµ–äºä½ çš„æ•°æ®å’Œä½¿ç”¨æ¡ˆä¾‹ï¼šä½ ä¸èƒ½ä»…ä»…å¥—ç”¨ä¸Šä¸€ä¸ªé¡¹ç›®ä¸­çš„è®¾ç½®æ¥æœŸæœ›å¾—åˆ°ç›¸åŒçš„ç»“æœã€‚

å¤§å¤šæ•°äººæ²¡æœ‰å¦¥å–„è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œè€Œæ˜¯å‡ ä¹éšæœºåœ°é€‰æ‹©å‚æ•°ã€‚è™½ç„¶æœ‰äº›äººå¯¹æ­¤æ–¹æ³•æ„Ÿåˆ°èˆ’é€‚ï¼Œä½†æˆ‘å†³å®šç”¨æ•°å€¼æ–¹æ³•æ¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚

**è¿™å°±æ˜¯è¯„ä¼°ä½ çš„ RAG çš„åœ°æ–¹ã€‚**

> åœ¨***è¿™ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘å°†å±•ç¤ºä¸€ä¸ªå¿«é€Ÿçš„ 3 æ­¥æ³•ï¼Œä½ å¯ä»¥ç”¨æ¥é«˜æ•ˆã€å¿«é€Ÿåœ°è¯„ä¼°ä½ çš„ RAG åœ¨ä¸¤ä¸ªä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚***

1.  **æ£€ç´¢**

1.  **ç”Ÿæˆ**

é€šè¿‡æŒæ¡è¿™ä¸ªè¯„ä¼°æµç¨‹ï¼Œä½ å¯ä»¥è¿›è¡Œè¿­ä»£ï¼Œæ‰§è¡Œå¤šæ¬¡å®éªŒï¼Œç”¨æŒ‡æ ‡è¿›è¡Œæ¯”è¾ƒï¼Œå¹¶ä¸”å¸Œæœ›æ‰¾åˆ°æœ€ä½³é…ç½®

è®©æˆ‘ä»¬çœ‹çœ‹è¿™ä¸ªå¦‚ä½•è¿ä½œ ğŸ‘‡ã€‚

*é™„æ³¨ï¼šåœ¨æ¯ä¸€èŠ‚ä¸­ï¼Œéƒ½æä¾›äº†ä»£ç ç‰‡æ®µä»¥å¸®åŠ©ä½ å¼€å§‹å®ç°è¿™äº›æƒ³æ³•ã€‚*

> å¦‚æœä½ å¯¹æé«˜æ„å»º ML ç³»ç»Ÿç”Ÿäº§åŠ›çš„å®ç”¨æŠ€å·§æ„Ÿå…´è¶£ï¼Œå¯ä»¥éšæ—¶è®¢é˜…æˆ‘çš„[é€šè®¯](https://thetechbuffet.substack.com/): The Tech Buffetã€‚
> 
> æˆ‘æ¯å‘¨å‘é€ç¼–ç¨‹å’Œç³»ç»Ÿè®¾è®¡çš„è§è§£ï¼Œä»¥å¸®åŠ©ä½ æ›´å¿«åœ°å‘å¸ƒ AI äº§å“ã€‚

# 1 â€” åˆ›å»ºä¸€ä¸ªåˆæˆæ•°æ®é›†

è¯„ä¼° LLM é€šå¸¸éœ€è¦æ‰‹åŠ¨æ ‡æ³¨æµ‹è¯•é›†ã€‚è¿™éœ€è¦æ—¶é—´ã€é¢†åŸŸä¸“ä¸šçŸ¥è¯†ï¼Œå¹¶ä¸”å®¹æ˜“å‡ºé”™ã€‚

å¸Œæœ› LLM èƒ½å¸®åŠ©æˆ‘ä»¬å®Œæˆè¿™é¡¹ä»»åŠ¡ã€‚

ä»ä½ çš„æ•°æ®ä¸­æŠ½å– N ä¸ªå—ã€‚å¯¹äºæ¯ä¸ªå—ï¼ŒæŒ‡ç¤º LLM ç”Ÿæˆ K ä¸ªé—®é¢˜å’Œç­”æ¡ˆçš„å…ƒç»„ã€‚

ç”Ÿæˆå®Œæˆåï¼Œä½ å°†è·å¾—ä¸€ä¸ªåŒ…å« N*K å…ƒç»„çš„æ•°æ®é›†ï¼Œæ¯ä¸ªå…ƒç»„éƒ½æœ‰ï¼ˆé—®é¢˜ï¼Œç­”æ¡ˆï¼Œä¸Šä¸‹æ–‡ï¼‰ã€‚

*é™„æ³¨ï¼šè¿™é‡Œçš„ä¸Šä¸‹æ–‡æ˜¯åŸå§‹å—åŠå…¶å…ƒæ•°æ®*

åœ¨ä»¥ä¸‹ç¤ºä¾‹ä¸­ï¼Œæˆ‘ä»¬å°†è€ƒè™‘ä¸€ä¸ªæåˆ°è‰¾è¨å…‹Â·ç‰›é¡¿çš„æ®µè½ã€‚

> è‰¾è¨å…‹Â·ç‰›é¡¿å› å…¶å…³äºå¼•åŠ›å®šå¾‹çš„ç†è®ºè€Œé—»åï¼Œä½†ä»–çš„ã€Šè‡ªç„¶å“²å­¦çš„æ•°å­¦åŸç†ã€‹ï¼ˆ1686 å¹´ï¼‰
> 
> é€šè¿‡å…¶ä¸‰å¤§è¿åŠ¨å®šå¾‹æå¤§åœ°å½±å“äº†æ¬§æ´²çš„å¯è’™æ—¶ä»£ã€‚ç”Ÿäº 1643 å¹´
> 
> åœ¨è‹±æ ¼å…°çš„ä¼å°”ç´¢æ™®ï¼Œè‰¾è¨å…‹Â·ç‰›é¡¿çˆµå£«å¼€å§‹å‘å±•ä»–çš„ç†è®º
> 
> å…³äºå…‰ã€å¾®ç§¯åˆ†å’Œå¤©ä½“åŠ›å­¦ï¼ŒåŒæ—¶åœ¨å‰‘æ¡¥å¤§å­¦ä¼‘å‡æœŸé—´ã€‚

æˆ‘ä»¬å°†æŒ‡ç¤º LLM ä»ä¸­ç”Ÿæˆä¸‰å¯¹é—®é¢˜å’Œç­”æ¡ˆã€‚

![](img/454300ccb8cb45367792c96f10991ded.png)

å›¾ç‰‡ç”±ä½œè€…æä¾›

è¿™å¯ä»¥ä½¿ç”¨è¿™ä¸ªæç¤ºå®Œæˆã€‚ï¼ˆæ”¹ç¼–è‡ªæˆ‘çš„ä¸€ä¸ªé¡¹ç›®ï¼‰

```py
Create exactly {num_questions} questions using the context and make 
sure each question doesnâ€™t reference
terms like "this study", "this research", or anything thatâ€™s 
not available to the reader.
End each question with a â€˜?â€™ character and then in a newline
write the answer to that question using only 
the context provided.

Separate each question/answer pair by "XXX"

Each question must start with "question:".

Each answer must start with "answer:".

CONTEXT = {context}
```

æˆ‘ä»¬å°†è¿›ä¸€æ­¥äº†è§£è¿™ä¸ªåˆæˆæ•°æ®é›†çš„å±€é™æ€§ï¼Œä½†ç°åœ¨ï¼Œå®ƒæ˜¯è¯„ä¼°æˆ‘ä»¬å®éªŒçš„ä¸€ä¸ªå¿«é€Ÿè§£å†³æ–¹æ¡ˆã€‚

## ä»£ç  ğŸ’»

åœ¨æ¥ä¸‹æ¥çš„éƒ¨åˆ†ä¸­ï¼Œæˆ‘å°†å±•ç¤ºå¦‚ä½•ä½¿ç”¨ LangChain åˆ›å»ºä½ çš„åˆæˆè¯„ä¼°æ•°æ®é›†ã€‚

æˆ‘ä»¬é¦–å…ˆå®šä¹‰æç¤ºå’Œ LLMï¼šæˆ‘æ­£åœ¨ä½¿ç”¨ GCPï¼Œæ‰€ä»¥æˆ‘å°†ä½¿ç”¨ VertexAIã€‚

```py
from random import sample
from langchain.llms import VertexAI
from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain

SYNTHETIC_DATASET_SIZE = 70
NUM_QUESTIONS_TO_GENERATE = 3
sampled_chunks = sample(splits, k=SYNTHETIC_DATASET_SIZE)

prompt = """
Create exactly {num_questions} questions using the context and make sure each question doesn't reference
terms like "this study", "this research", or anything that's not available to the reader.

End each question with a '?' character and then in a newline write the answer to that question using only 
the context provided.
Separate each question/answer pair by "XXX"
Each question must start with "question:".
Each answer must start with "answer:".
CONTEXT = {context}
"""

llm = VertexAI(
    model_name="text-bison",
    max_output_tokens=256,
    temperature=0,
    top_p=0.95,
    top_k=40,
    verbose=True,
)
prompt_template = PromptTemplate(
    template=prompt,
    input_variables=[
        "num_questions",
        "context",
    ],
)
chain = LLMChain(llm=llm, prompt=prompt_template)
```

ç„¶åï¼Œæˆ‘ä»¬ä» BQ åŠ è½½ä¸€äº›æ•°æ®å¹¶å°†å…¶æ‹†åˆ†æˆå—ã€‚å—å°†æ˜¯ç”Ÿæˆé—®é¢˜å’Œç­”æ¡ˆæ‰€éœ€çš„ä¸Šä¸‹æ–‡ã€‚

```py
from langchain.document_loaders.bigquery import BigQueryLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter

def load_data_from_bq(query, content_cols, metadata_cols):
    loader = BigQueryLoader(
        query=query, page_content_columns=content_cols, metadata_columns=metadata_cols
    )
    return loader.load()

query = ... # the SQL query to 
content_cols = ... # the content columns of your table
metadata_cols = ... # the metadata columns of your

loaded_data = load_data_from_bq(
    query,
    content_cols=content_cols,
    metadata_cols=metadata,
)

CHUNK_SIZE = 700
CHUNK_OVERLAP = 100

text_splitter = RecursiveCharacterTextSplitter(
    separators=["."],
    chunk_size=CHUNK_SIZE,
    chunk_overlap=CHUNK_OVERLAP,
)

splits = text_splitter.split_documents(loaded_data)
```

ä¸€æ—¦æ•°æ®åŠ è½½å¹¶æ‹†åˆ†å®Œæ¯•ï¼Œä½ éœ€è¦ä»ä¸­æŠ½å–ä¸€äº›å—ä»¥ç”Ÿæˆé—®é¢˜/ç­”æ¡ˆå¯¹ã€‚

è¿™åœ¨ä»¥ä¸‹å¾ªç¯ä¸­å®Œæˆã€‚

```py
import pandas as pd

SYNTHETIC_DATASET_SIZE = 70
NUM_QUESTIONS_TO_GENERATE = 3
sampled_chunks = sample(splits, k=SYNTHETIC_DATASET_SIZE)

synthetic_dataset = []
for sampled_chunk in tqdm(sampled_chunks):
    prediction = chain.invoke(
        {
            "num_questions": NUM_QUESTIONS_TO_GENERATE,
            "context": sampled_chunk.page_content,
        }
    )
    output = prediction["text"]

    try:
        questions_and_answers = parse_output(output)

        for question_and_answer in questions_and_answers:
            synthetic_dataset.append(
                {
                    **question_and_answer,
                    "context": sampled_chunk.page_content,
                    "source": sampled_chunk.metadata["source"],
                }
            )
    except:
        pass

synthetic_dataset_df = pd.DataFrame(synthetic_dataset)
```

# 2 â€” å¯¹æ¯ä¸ªåˆæˆé—®é¢˜è¿è¡Œä½ çš„ RAG

ä¸€æ—¦åˆæˆæ•°æ®é›†æ„å»ºå®Œæˆï¼Œä½ å¯ä»¥ä½¿ç”¨ä½ çš„ RAG å¯¹æ¯ä¸ªé—®é¢˜è¿›è¡Œé¢„æµ‹ã€‚

è¿™å°†ç”ŸæˆåŸºäºä¸€ç»„æ£€ç´¢æ¥æºçš„ç­”æ¡ˆã€‚

å¯¹äºæ¯ä¸ªé—®é¢˜ï¼Œè¯·ç¡®ä¿ä½ æå–äº†æ£€ç´¢åˆ°çš„æ–‡æ¡£ï¼šè¿™äº›æ–‡æ¡£å°†ç”¨äºè¿›è¡Œè¯„ä¼°ã€‚

![](img/55f0058a3f8a23e00a77ee0b482ea32c.png)

å›¾ç‰‡ç”±ä½œè€…æä¾›

## ä»£ç  ğŸ’»

åœ¨æ¥ä¸‹æ¥çš„éƒ¨åˆ†ä¸­ï¼Œæˆ‘ä»¬å°†çœ‹åˆ°å¦‚ä½•æ„å»º RAG å¹¶å¯¹è¯„ä¼°æ•°æ®è¿›è¡Œé¢„æµ‹ã€‚

æˆ‘ä»¬é¦–å…ˆéœ€è¦åˆ›å»ºä¸€ä¸ªå‘é‡æ•°æ®åº“æ¥ç´¢å¼•è¿™äº›å—ã€‚

```py
from langchain.embeddings import VertexAIEmbeddings
from langchain.vectorstores import Chroma

db = Chroma.from_documents(
    documents=splits,
    embedding=VertexAIEmbeddings(),
    persist_directory="./db",
)
```

LangChain ä¸­é»˜è®¤çš„ RAG å®ç°ä½¿ç”¨äº†ä¸€ä¸ªç‰¹å®šçš„æç¤ºæ¨¡æ¿ã€‚åœ¨æˆ‘ä»¬çš„æ¡ˆä¾‹ä¸­ï¼Œæˆ‘ä»¬å°†ç¨å¾®ä¿®æ”¹å®ƒä»¥é˜²æ­¢äº§ç”Ÿå¹»è§‰ã€‚

```py
RAG_TEMPLATE = """Use the following pieces of context to answer the question at the end. 
If you don't know the answer, just say that you don't know, don't try to make up an answer. 
{context}
Question: {question}
Helpful Answer:"""
```

ç„¶åï¼Œæˆ‘ä»¬å°†æˆ‘ä»¬çš„ RAG å®ç°åŒ…è£…åœ¨ä¸€ä¸ª Python ç±»ä¸­ã€‚

```py
class RAG:
    def __init__(self, vectorstore, rag_template=RAG_TEMPLATE):
        self.vectorstore = vectorstore
        self.rag_prompt = PromptTemplate.from_template(rag_template)
        self.chain = LLMChain(llm=llm, prompt=self.rag_prompt)

    def _format_context(self, docs):
        context = [doc.page_content for doc, score in docs]
        context = "\n---\n".join(context)
        return context

    def _format_source_documents(self, docs):
        source_documents = []
        for doc, score in docs:
            doc.metadata["score"] = score
            source_documents.append(doc)
        return source_documents

    def predict(self, question, k=4, score_threshold=0.6):
        relevant_documents = self.vectorstore.similarity_search_with_relevance_scores(
            query=question,
            k=k,
            score_threshold=score_threshold,
        )
        source_documents = self._format_source_documents(relevant_documents)
        context = self._format_context(relevant_documents)
        answer = self.chain.predict(question=question, context=context)
        output = {
            "question": question,
            "answer": answer,
            "source_documents": source_documents,
        }
        return output

# Create the RAG:

rag = RAG(vectorstore=db)
```

ä¸€æ—¦ RAG æ„å»ºå’Œåˆå§‹åŒ–å®Œæˆï¼Œä½ å¯ä»¥è¿›è¡Œé¢„æµ‹ï¼š

```py
evaluations = []

for i, row in tqdm(synthetic_dataset_df.iterrows(), total=len(synthetic_dataset_df)):
    question = row["question"]
    ground_truth_answer = row["answer"]
    ground_truth_context = row["context"]
    ground_truth_source = row["source"]
    evaluation = {
        "question": question,
        "ground_truth_answer": ground_truth_answer,
        "ground_truth_source": ground_truth_source,
    }

    prediction = rag.predict(question)
    predicted_answer = prediction["answer"]
    predicted_contexts = [
        source_document.page_content
        for source_document in prediction["source_documents"]
    ]
    predicted_sources = [
        source_document.metadata["source"]
        for source_document in prediction["source_documents"]
    ]

    evaluation["predicted_answer"] = predicted_answer
    evaluation["predicted_contexts"] = predicted_contexts
    evaluation["predicted_sources"] = predicted_sources
    evaluation["relevance_scores"] = [
        source_document.metadata["score"]
        for source_document in prediction["source_documents"]
    ]

    evaluations.append(evaluation)

df_evaluations = pd.DataFrame(evaluations)
```

å®Œæˆåï¼Œä½ å°†å¤§è‡´å¾—åˆ°ä»¥ä¸‹ç»“æœï¼š

![](img/e78ceb88f9a94cc94b9ff18afd317e5f.png)

æˆªå›¾ç”±ä½œè€…æä¾›

# 3 â€” è®¡ç®—ä¸¤ä¸ªè¯„ä¼°æŒ‡æ ‡

ç°åœ¨ï¼Œä½ å¯ä»¥å¯¹ä½ ä¸ºæ¯ä¸ªé—®é¢˜åšå‡ºçš„é¢„æµ‹è®¡ç®—ä¸¤ä¸ªè¯„ä¼°æŒ‡æ ‡ã€‚

1.  ä¸€ä¸ª**æ£€ç´¢åˆ†æ•°**ç”¨äºè¯„ä¼°æ£€ç´¢åˆ°çš„æ–‡æ¡£çš„ç›¸å…³æ€§

    è¿™ä¸ªåˆ†æ•°å¯ä»¥æ˜¯äºŒè¿›åˆ¶çš„ï¼ˆæ¯ä¸ªé¢„æµ‹ä¸º 1/0ï¼‰ï¼Œå¹¶ä¸”è¡¨ç¤ºæ¯ä¸ªé—®é¢˜çš„çœŸå®æ¥æºæ˜¯å¦åœ¨æ£€ç´¢åˆ°çš„æ¥æºåˆ—è¡¨ä¸­ã€‚ä½ å¯ä»¥æŠŠè¿™ä¸ªåˆ†æ•°çœ‹ä½œæ˜¯*å¬å›ç‡*ã€‚

1.  ä¸€ä¸ª**è´¨é‡è¯„åˆ†**ç”¨äºè¯„ä¼°ç”Ÿæˆçš„ç­”æ¡ˆï¼Œç»™å®šé—®é¢˜å’ŒçœŸå®ç­”æ¡ˆã€‚å†æ¬¡åœ°ï¼Œå¯ä»¥ä½¿ç”¨ LLM åœ¨æ­¤ä»»åŠ¡ä¸­æä¾› 5 åˆ†è¯„ä»·ã€‚

è¿™æ˜¯æˆ‘ç”¨æ¥è¦æ±‚ LLM è¯„åˆ†ç”Ÿæˆç­”æ¡ˆçš„æç¤ºã€‚

```py
Your job is to rate the quality of a generated answer
given a query  and a reference answer.

QUERY = {query}

GENERATED ANSWER = {generated_answer}

REFERENCE ANSWER = {reference_answer}

Your score has to be between 1 and 5.
You must return your response in a line with only the score.
Do not return answers in any other format.
On a separate line provide your reasoning for the score as well.
```

åœ¨è·å¾—æ¯ä¸ªé—®é¢˜çš„åˆ†æ•°åï¼Œå¯¹æ•°æ®é›†è¿›è¡Œå¹³å‡ä»¥è·å¾—æœ€ç»ˆçš„ä¸¤ä¸ªæŒ‡æ ‡ã€‚

![](img/1149d76870704912ed16f1e5d9124ce3.png)

ä½œè€…å›¾ç‰‡

## ä»£ç  ğŸ’»

åœ¨è¿™ä¸€éƒ¨åˆ†ä¸­ï¼Œæˆ‘ä»¬ä»`df_evaluations`æ•°æ®æ¡†å¼€å§‹å¹¶è®¡ç®—ä¸åŒçš„æŒ‡æ ‡ã€‚

**1 â€” æ£€ç´¢åˆ†æ•°**

æˆ‘ä»¬å¯ä»¥é€šè¿‡è®¡ç®— top_k æŒ‡æ ‡å¼€å§‹ï¼Œè¿™äº›æŒ‡æ ‡è¡¨æ˜çœŸå®æ¥æºæ˜¯å¦åœ¨å‰ k ä¸ªé¢„æµ‹æ¥æºä¸­ã€‚

```py
df_evaluations["top_1"] = df_evaluations.apply(
    lambda row: row["ground_truth_source"] in row["predicted_sources"][:1], axis=1
)
df_evaluations["top_2"] = df_evaluations.apply(
    lambda row: row["ground_truth_source"] in row["predicted_sources"][:2], axis=1
)
df_evaluations["top_3"] = df_evaluations.apply(
    lambda row: row["ground_truth_source"] in row["predicted_sources"][:3], axis=1
)
df_evaluations["top_4"] = df_evaluations.apply(
    lambda row: row["ground_truth_source"] in row["predicted_sources"][:4], axis=1
)
```

å¯¹æ‰€æœ‰æ–‡æ¡£è¿›è¡Œå¹³å‡ï¼Œå°†ç»™æˆ‘ä»¬æ¯ä¸ª k çš„æ£€ç´¢åˆ†æ•°ã€‚

![](img/6cc4be8b748707ba32aca1b64799e006.png)

ä½œè€…æˆªå›¾

> **å¦‚ä½•è§£è¯»è¿™äº›åˆ†æ•°ï¼š**
> 
> ç¤ºä¾‹ï¼šçœŸå®æ¥æºåœ¨å‰å››ä¸ªæ£€ç´¢æ¥æºä¸­çš„æ¦‚ç‡ä¸º 0.37ã€‚

**2 â€” ç”Ÿæˆåˆ†æ•°**

ä¸ºäº†è¯„ä¼°ç”Ÿæˆç­”æ¡ˆçš„è´¨é‡ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ä¸€ä¸ª LLMï¼Œå®ƒå°†æä¾›å…¶è¯„åˆ†å’Œæ¨ç†ã€‚

```py
EVAL_TEMPLATE = """Your job is to rate the quality of a generated answer
given a query  and a reference answer.

QUERY = {query}

GENERATED ANSWER = {generated_answer}

REFERENCE ANSWER = {reference_answer}

Your score has to be between 1 and 5.
You must return your response in a line with only the score.
Do not return answers in any other format.
On a separate line provide your reasoning for the score as well."""

prompt_template_eval = PromptTemplate.from_template(EVAL_TEMPLATE)
llm_eval_chain = LLMChain(llm=llm, prompt=prompt_template_eval)
```

åœ¨è¿™ä¸ªå‡½æ•°ä¸­ï¼Œæˆ‘ä»¬æŒ‡å¯¼è¿™ä¸ª LLM å¹¶è·å–å…¶è¯„åˆ†å’Œæ¨ç†ï¼š

1.  æŸ¥è¯¢

1.  é¢„æµ‹çš„ç­”æ¡ˆ

1.  å‚è€ƒç­”æ¡ˆ

```py
def evaluate_answer(query, generated_answer, reference_answer):
    answer = llm_eval_chain.invoke(
        {
            "query": query,
            "generated_answer": generated_answer,
            "reference_answer": reference_answer,
        }
    )
    answer = answer["text"]

    try:
        rating, reasoning = answer.split("\n")
        rating = rating.strip()
        reasoning = reasoning.strip()

        evaluation = {
            "rating": rating,
            "reasoning": reasoning,
        }
        return evaluation
    except:
        return {
            "rating": None,
            "reasoning": None,
        }
```

ç„¶åï¼Œæˆ‘ä»¬å¯¹è¯„ä¼°æ•°æ®è¿è¡Œè¿™ä¸ªå‡½æ•°ï¼š

```py
answer_quality_evaluations = []

for i, row in tqdm(df_evaluations.iterrows(), total=len(df_evaluations)):
    evaluation = evaluate_answer(
        row["question"],
        row["predicted_answer"],
        row["ground_truth_answer"],
    )
    answer_quality_evaluations.append(evaluation)
```

ç”Ÿæˆè¯„åˆ†åï¼Œæˆ‘ä»¬å¾—åˆ°çš„åˆ†å¸ƒå¦‚ä¸‹ï¼š

![](img/06af084f3a1d30576f291d49bed50d64.png)

ä½œè€…æˆªå›¾

æ ¹æ®è¯„åˆ†è¿‡æ»¤ï¼Œä»¥ä¸‹æ˜¯ä¸€äº› RAG æœªèƒ½ç”Ÿæˆç­”æ¡ˆçš„é—®é¢˜ï¼š

![](img/8d0b1d499bbf9d53c5c50c23c40d7d52.png)

ä½œè€…æˆªå›¾

è¿™é‡Œæ˜¯ä¸€äº›å…¶ä»–æˆåŠŸçš„ä¾‹å­ã€‚

![](img/6c4b4aee3353839f3272efc1501111f6.png)

ä½œè€…æˆªå›¾

åˆ†æè¿™äº›æ•°æ®æœ‰åŠ©äºè¯Šæ–­å’Œè§£å†³ RAG å‡ºç°çš„é”™è¯¯ã€‚

# è¿™ç§è¯„ä¼°æ–¹æ³•çš„å±€é™æ€§

åœ¨ä¸ºåˆ¶è¯è¡Œä¸šæ„å»ºå¤šä¸ª RAG æ—¶ï¼Œæˆ‘åº”ç”¨äº†è¿™ç§è¯„ä¼°æ–¹æ³•ã€‚

è™½ç„¶å®ƒå¿«é€Ÿæä¾›äº†ä¸€ä¸ªç«¯åˆ°ç«¯çš„ç®¡é“å¹¶é¿å…äº†æ‰‹åŠ¨æ ‡è®°æ•°æ®ï¼Œä½†æˆ‘æ³¨æ„åˆ°å®ƒæœ‰ä¸¤ä¸ªä¸»è¦é—®é¢˜ï¼š

+   ä¸€äº›ç”Ÿæˆçš„é—®é¢˜è¿‡äºå…·ä½“ï¼Œå¯¹äºæ²¡æœ‰é˜…è¯»ä¸Šä¸‹æ–‡çš„äººæ¥è¯´æ¯«æ— æ„ä¹‰ã€‚å› æ­¤ï¼Œå®ƒä»¬å¯¹è¯„ä¼°æŒ‡æ ‡é€ æˆäº†å¾ˆå¤§çš„è´Ÿé¢å½±å“ï¼Œå› ä¸ºå®ƒä»¬æ˜¯*ä¸å¯èƒ½*å›ç­”çš„ã€‚

    ä¾‹å¦‚ï¼šâ€œè¿™é¡¹ç ”ç©¶è®¨è®ºçš„ä¸»è¦é—®é¢˜æ˜¯ä»€ä¹ˆï¼Ÿâ€

+   å¦ä¸€æ–¹é¢ï¼Œä¸€äº›å…¶ä»–é—®é¢˜è¿‡äºç®€å•ï¼Œæˆ–è€…åªæ˜¯åŸå§‹ç‰‡æ®µçš„ç®€å•æ”¹è¿°ã€‚

# ç»“è®º

å¦‚æœä½ æƒ³äº†è§£æ›´å¤šå…³äºè¿™ç§è¯„ä¼°æ–¹æ³•ï¼ˆä¹Ÿç§°ä¸ºå†·å¯åŠ¨ï¼‰ï¼Œå¯ä»¥æŸ¥çœ‹è¿™ä¸ª[æŒ‡å—](https://www.anyscale.com/blog/a-comprehensive-guide-for-building-rag-based-llm-applications-part-1#cold-start)ã€‚

å¦‚æœä½ æ˜¯ RAG çš„æ–°æ‰‹å¹¶æƒ³æ·±å…¥äº†è§£ï¼Œä½ ä¹Ÿå¯ä»¥æŸ¥çœ‹æˆ‘ä¹‹å‰çš„ä¸€äº›å¸–å­ä»¥è·å–æ›´å¤šä¿¡æ¯ã€‚

+   [ä¸ºä»€ä¹ˆä½ çš„ RAG åœ¨ç”Ÿäº§ç¯å¢ƒä¸­ä¸å¯é ](https://thetechbuffet.substack.com/p/the-probelms-behind-rag)

+   [è®© RAG å·¥ä½œå¾—æ›´å¥½æ˜¯å¾ˆå›°éš¾çš„ â€” æˆä¸º RAG å¤§å¸ˆçš„ 5 ç¯‡åšå®¢æ–‡ç« ](https://thetechbuffet.substack.com/p/5-curated-rag-blog-posts)

ä½ çŸ¥é“å…¶ä»–åœ¨æ„å»º RAG æ—¶è¢«è¯æ˜æœ‰æ•ˆçš„è¯„ä¼°æ–¹æ³•å—ï¼Ÿè¯·å‘Šè¯‰æˆ‘ã€‚

å¦‚æœä½ ä¹Ÿåœ¨ç ”ç©¶ RAG çš„æœ€ä½³æ€§èƒ½ï¼Œæˆ‘ä¹Ÿå¾ˆæ„¿æ„è®¨è®ºè¿™ä¸ªè¯é¢˜ã€‚

ä»Šå¤©å°±åˆ°è¿™é‡Œã€‚ä¸‹æ¬¡å†è§ï¼ğŸ‘‹
