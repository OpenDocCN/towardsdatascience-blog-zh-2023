- en: Using Fourier Transform of Vector Representations Derived from BERT Embeddings
    for Semantic Closeness Evaluation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/using-fourier-transform-of-vector-representations-derived-from-bert-embeddings-for-semantic-9d91a7d4839c?source=collection_archive---------1-----------------------#2023-01-14](https://towardsdatascience.com/using-fourier-transform-of-vector-representations-derived-from-bert-embeddings-for-semantic-9d91a7d4839c?source=collection_archive---------1-----------------------#2023-01-14)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/7727543fc629cc19f66d6aa3a5236e98.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by Igor Shabalin, with permission
  prefs: []
  type: TYPE_NORMAL
- en: Exploring the mutual influence of words in a sentence by evaluating different
    representations of BERT embeddings
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://jxireal.medium.com/?source=post_page-----9d91a7d4839c--------------------------------)[![Yuli
    Vasiliev](../Images/7a5fbd7fc0d48c87f0163e2ec4622f45.png)](https://jxireal.medium.com/?source=post_page-----9d91a7d4839c--------------------------------)[](https://towardsdatascience.com/?source=post_page-----9d91a7d4839c--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----9d91a7d4839c--------------------------------)
    [Yuli Vasiliev](https://jxireal.medium.com/?source=post_page-----9d91a7d4839c--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F83cfb869ab36&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fusing-fourier-transform-of-vector-representations-derived-from-bert-embeddings-for-semantic-9d91a7d4839c&user=Yuli+Vasiliev&userId=83cfb869ab36&source=post_page-83cfb869ab36----9d91a7d4839c---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----9d91a7d4839c--------------------------------)
    ·5 min read·Jan 14, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F9d91a7d4839c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fusing-fourier-transform-of-vector-representations-derived-from-bert-embeddings-for-semantic-9d91a7d4839c&user=Yuli+Vasiliev&userId=83cfb869ab36&source=-----9d91a7d4839c---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F9d91a7d4839c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fusing-fourier-transform-of-vector-representations-derived-from-bert-embeddings-for-semantic-9d91a7d4839c&source=-----9d91a7d4839c---------------------bookmark_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: BERT embedding is the thing that provides great opportunities when it comes
    to programmatic ways of extracting meaning from text. It seems everything we (as
    well as machines) need to make sense of the text is hidden in those numbers. It’s
    just a matter of properly manipulating those numbers. I discussed this concept
    in my recent post [Discovering Trends in BERT Embeddings of Different Levels for
    the Task of Semantic Context Determining](https://medium.com/towards-data-science/discovering-trends-in-bert-embeddings-of-different-levels-for-the-task-of-semantic-context-268733fdb17e).
    This article continues this discussion, looking at whether the vector representations
    obtained by applying Fourier transform to BERT embeddings can also be useful in
    NLP tasks of meaning extraction.
  prefs: []
  type: TYPE_NORMAL
- en: Hypothesis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As you probably know from physics, the Fourier transform allows us to understand
    the frequencies inside a signal. Does a vector representing a word embedding look
    like a signal? That is, could the knowledge of the frequency domain derived from
    the Fourier transform be any useful when processing BERT embeddings for discovering
    semantic closeness? In simple terms, does the Fourier transform make sense when
    analyzing BERT embeddings? Let’s check it out.
  prefs: []
  type: TYPE_NORMAL
- en: Experiment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The example discussed in the rest of this article assumes you have the model
    defined as discussed in the example in the [previous post](https://medium.com/towards-data-science/discovering-trends-in-bert-embeddings-of-different-levels-for-the-task-of-semantic-context-268733fdb17e).
    We’ll also need the representations derived from the sample sentences as discussed
    in that [previous post](https://medium.com/towards-data-science/discovering-trends-in-bert-embeddings-of-different-levels-for-the-task-of-semantic-context-268733fdb17e).
    Note, however, that this article uses a set of samples that are a bit different
    from those used in the above post. So, before creating the representations, you’ll
    need to define the following samples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, all the sentences have the same set of the direct object modifiers,
    while the direct objects are different in each case. The purpose of our experiment
    is to check out how the semantic closeness of direct objects affects the proximity
    of modifiers.
  prefs: []
  type: TYPE_NORMAL
- en: The process of tokenizing and generating the hidden states for sample sentences
    was covered in the above post. So we won’t cover this process again here.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s focus on the word modifiers (“a/very/good”) of the direct object (“apple/orange/adventure”)
    in each sentence. To use right indexing when obtaining their embeddings, let’s
    first check out the tokens in our sentences:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Since we’re interested in the modifiers of the direct object (in this particular
    example, we have three modifiers: *a, very, good*.), we need the tokens with indexes:
    *3,4,5*. So we need to shift indexing by 3\. Below, we obtain the contextual embedding
    in each sentence for each modifier. We’ll hold modifier embeddings in a list defined
    for each sentence:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Let’s now look at how the semantic closeness of the direct objects in different
    sentences affects the closeness of their respective modifiers.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'In the above output, we can see that the contextual embeddings of Apple and
    Orange modifiers show a high level of closeness. This is quite understandable
    because the direct objects themselves: Apple and Orange are very close.'
  prefs: []
  type: TYPE_NORMAL
- en: 'While the representations derived of the Apple and Adventure modifiers are
    not so close, as can be seen from the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The Orange and Adventure modifiers are not supposed to be that close either:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s now derive more complicated representations from the embeddings provided
    by BERT. To start with, let’s get the initial embeddings for the modifiers in
    each sentence:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Now we can, for example, divide contextual embeddings (generated in the 12th
    Encoder layer) by the corresponding initial embeddings (as it was discussed in
    the previous post), to get some new embedding representations to be used in further
    analysis.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: For analysis purposes, you might also want to create another set of representations,
    calculating just element-wise difference between the contextual and non-contextual
    (initial) embedding.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Before proceeding to evaluate the representations we just created, let’s create
    another set of representations using Fourier transform, so that we can then compare
    all our representations obtained by different methods.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Now we can compare the obtained representations for each pair of sentences per
    modifier.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'The produced output should look as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: In the above experiment, we expect to see a high level of similarity between
    the same modifiers in the first two sentences. In fact, we can see that the difference
    and Fourier transform methods did well on the task.
  prefs: []
  type: TYPE_NORMAL
- en: The purpose of the following experiment is determine the closeness of the modifiers
    whose nouns being modified aren’t that close.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Here’s the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: The above output shows that the difference and log quotient were the best when
    evaluating the closeness of the modifiers whose related nouns aren’t that close.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'The outputs are the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: Once again, we can see that the difference and log quotient turned out to be
    the best when evaluating the closeness of the modifiers whose related nouns aren’t
    that close.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Does the Fourier transform make sense when analyzing BERT embeddings? According
    to the experiment performed in this article, we may conclude that this approach
    can be used effectively along with other methods.
  prefs: []
  type: TYPE_NORMAL
