["```py\n# Install llama.cpp\n!git clone https://github.com/ggerganov/llama.cpp\n!cd llama.cpp && git pull && make clean && LLAMA_CUBLAS=1 make\n!pip install -r llama.cpp/requirements.txt\n```", "```py\nMODEL_ID = \"mlabonne/EvolCodeLlama-7b\"\n\n# Download model\n!git lfs install\n!git clone https://huggingface.co/{MODEL_ID}\n```", "```py\nMODEL_NAME = MODEL_ID.split('/')[-1]\n\n# Convert to fp16\nfp16 = f\"{MODEL_NAME}/{MODEL_NAME.lower()}.fp16.bin\"\n!python llama.cpp/convert.py {MODEL_NAME} --outtype f16 --outfile {fp16}\n```", "```py\nQUANTIZATION_METHODS = [\"q4_k_m\", \"q5_k_m\"]\n\nfor method in QUANTIZATION_METHODS:\n    qtype = f\"{MODEL_NAME}/{MODEL_NAME.lower()}.{method.upper()}.gguf\"\n    !./llama.cpp/quantize {fp16} {qtype} {method}\n```", "```py\nimport os\n\nmodel_list = [file for file in os.listdir(MODEL_NAME) if \"gguf\" in file]\n\nprompt = input(\"Enter your prompt: \")\nchosen_method = input(\"Name of the model (options: \" + \", \".join(model_list) + \"): \")\n\n# Verify the chosen method is in the list\nif chosen_method not in model_list:\n    print(\"Invalid name\")\nelse:\n    qtype = f\"{MODEL_NAME}/{MODEL_NAME.lower()}.{method.upper()}.gguf\"\n    !./llama.cpp/main -m {qtype} -n 128 --color -ngl 35 -p \"{prompt}\"\n```", "```py\ndef fib(n):\n    if n == 0 or n == 1:\n        return n\n    return fib(n - 2) + fib(n - 1)\n\nfor i in range(1, 10):\n    print(fib(i))\n```", "```py\n!pip install -q huggingface_hub\n\nusername = \"mlabonne\"\n\nfrom huggingface_hub import notebook_login, create_repo, HfApi\nnotebook_login()\n```", "```py\n!pip install -q huggingface_hub\nfrom huggingface_hub import create_repo, HfApi\nfrom google.colab import userdata\n\n# Defined in the secrets tab in Google Colab\nhf_token = userdata.get('huggingface')\n\napi = HfApi()\nusername = \"mlabonne\"\n\n# Create empty repo\ncreate_repo(\n    repo_id = f\"{username}/{MODEL_NAME}-GGUF\",\n    repo_type=\"model\",\n    exist_ok=True,\n    token=hf_token\n)\n\n# Upload gguf files\napi.upload_folder(\n    folder_path=MODEL_NAME,\n    repo_id=f\"{username}/{MODEL_NAME}-GGUF\",\n    allow_patterns=f\"*.gguf\",\n    token=hf_token\n)\n```", "```py\n#define QK4_0 32\ntypedef struct {\n    ggml_fp16_t d;          // delta\n    uint8_t qs[QK4_0 / 2];  // nibbles / quants\n} block_q4_0;\n```"]