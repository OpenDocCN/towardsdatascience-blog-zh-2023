- en: Topics per Class Using BERTopic
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/topics-per-class-using-bertopic-252314f2640?source=collection_archive---------0-----------------------#2023-09-09](https://towardsdatascience.com/topics-per-class-using-bertopic-252314f2640?source=collection_archive---------0-----------------------#2023-09-09)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: How to understand the differences in texts by categories
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://miptgirl.medium.com/?source=post_page-----252314f2640--------------------------------)[![Mariya
    Mansurova](../Images/b1dd377b0a1887db900cc5108bca8ea8.png)](https://miptgirl.medium.com/?source=post_page-----252314f2640--------------------------------)[](https://towardsdatascience.com/?source=post_page-----252314f2640--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----252314f2640--------------------------------)
    [Mariya Mansurova](https://miptgirl.medium.com/?source=post_page-----252314f2640--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F15a29a4fc6ad&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftopics-per-class-using-bertopic-252314f2640&user=Mariya+Mansurova&userId=15a29a4fc6ad&source=post_page-15a29a4fc6ad----252314f2640---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----252314f2640--------------------------------)
    ·15 min read·Sep 9, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F252314f2640&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftopics-per-class-using-bertopic-252314f2640&user=Mariya+Mansurova&userId=15a29a4fc6ad&source=-----252314f2640---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F252314f2640&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftopics-per-class-using-bertopic-252314f2640&source=-----252314f2640---------------------bookmark_footer-----------)![](../Images/33e46ed4709dca6e73fabaadff83025f.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Photo by [Fas Khan](https://unsplash.com/@fasbytes?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: 'Nowadays, working in product analytics, we face a lot of free-form texts:'
  prefs: []
  type: TYPE_NORMAL
- en: Users leave comments in AppStore, Google Play or other services;
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Clients reach out to our Customer Support and describe their problems using
    natural language;
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We launch surveys ourselves to get even more feedback, and in most cases, there
    are some free-form questions to get a better understanding.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We have hundreds of thousands of texts. It would take years to read them all
    and get some insights. Luckily, there are a lot of DS tools that could help us
    automate this process. One such tool is Topic Modelling, which I would like to
    discuss today.
  prefs: []
  type: TYPE_NORMAL
- en: Basic Topic Modelling can give you an understanding of the main topics in your
    texts (for example, reviews) and their mixture. But it’s challenging to make decisions
    based on one point. For example, 14.2% of reviews are about too many ads in your
    app. Is it bad or good? Should we look into it? To tell the truth, I have no idea.
  prefs: []
  type: TYPE_NORMAL
- en: But if we try to segment customers, we may learn that this share is 34.8% for
    Android users and 3.2% for iOS. Then, it’s apparent that we need to investigate
    whether we show too many ads on Android or why Android users’ tolerance to ads
    is lower.
  prefs: []
  type: TYPE_NORMAL
- en: That’s why I would like to share not only how to build a topic model but also
    how to compare topics across categories. In the end we will get such insightful
    graphs for each topic.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/50eb38c77d141bb59505642199693fa2.png)'
  prefs: []
  type: TYPE_IMG
- en: Graph by author
  prefs: []
  type: TYPE_NORMAL
- en: Data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The most common real-life cases of free-form texts are some kind of reviews.
    So, let’s use a [dataset](https://archive.ics.uci.edu/dataset/205/opinrank+review+dataset)
    with hotel reviews for this example.
  prefs: []
  type: TYPE_NORMAL
- en: I’ve filtered comments related to several hotel chains in London.
  prefs: []
  type: TYPE_NORMAL
- en: Before starting text analysis, it’s worth getting an overview of our data. In
    total, we have 12 890 reviews on 7 different hotel chains.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/797f31ce6919c232d63770e14410fb19.png)'
  prefs: []
  type: TYPE_IMG
- en: Graph by author
  prefs: []
  type: TYPE_NORMAL
- en: BERTopic
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now we have data and can apply our new fancy tool Topic Modeling to get insights
    from it. As I mentioned in the beginning, we will use Topic Modelling and a powerful
    and easy-to-use `BERTopic` package ([documentation](https://maartengr.github.io/BERTopic/index.html))
    for this text analysis.
  prefs: []
  type: TYPE_NORMAL
- en: You might wonder what Topic Modelling is. It is an unsupervised ML technique
    related to Natural Language Processing. It allows you to find hidden semantic
    patterns in texts (usually called documents) and assign “topics” to them. You
    don’t need to have a list of topics beforehand. The algorithm will define them
    automatically — usually in the form of a bag of the most important words (tokens)
    or N-grams.
  prefs: []
  type: TYPE_NORMAL
- en: '`BERTopic` is a package for Topic Modelling using HuggingFace transformers
    and [class-based TF-IDF](/creating-a-class-based-tf-idf-with-scikit-learn-caea7b15b858).
    `BERTopic` is a highly flexible modular package so that you can tailor it to your
    needs.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a7f3e9f0124fea2827709869b90467dc.png)'
  prefs: []
  type: TYPE_IMG
- en: Image from BERTopic docs ([source](https://github.com/MaartenGr/BERTopic/tree/master/images))
  prefs: []
  type: TYPE_NORMAL
- en: If you want to understand how it works better, I advise you to watch this video
    from the author of the library.
  prefs: []
  type: TYPE_NORMAL
- en: Preprocessing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You can find the full code on [GitHub](https://github.com/miptgirl/miptgirl_medium/tree/main/bertopic_for_hotels).
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: According to [the documentation](https://maartengr.github.io/BERTopic/faq.html#should-i-preprocess-the-data),
    we typically don’t need to preprocess data unless there is a lot of noise, for
    example, HTML tags or other markdowns that don’t add meaning to the documents.
    It’s a significant advantage of `BERTopic` because, for many NLP methods, there
    is a lot of boilerplate to preprocess your data. If you are interested in how
    it could look like, see [this guide](/topic-modelling-f51e5ebfb40a) for Topic
    Modelline using LDA.
  prefs: []
  type: TYPE_NORMAL
- en: You can use `BERTopic` with data in multiple languages specifying `BERTopic(language=
    "multilingual")`. However, from my experience, the model works a bit better with
    texts translated into one language. So, I will translate all comments into English.
  prefs: []
  type: TYPE_NORMAL
- en: For translation, we will use `deep-translator` package (you can install it from
    [PyPI](https://pypi.org/project/deep-translator/)).
  prefs: []
  type: TYPE_NORMAL
- en: Also, it could be interesting to see distribution by languages, for that we
    could use `langdetect` package.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: In our case, 95+% of comments are already in English.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/51b3b12bb732e846c080aab9542fb1fa.png)'
  prefs: []
  type: TYPE_IMG
- en: Graph by author
  prefs: []
  type: TYPE_NORMAL
- en: To understand our data better, let’s look at the distribution of reviews’ length.
    It shows that there are a lot of extremely short (and most likely not meaningful
    comments) — around 5% of reviews are less than 20 symbols.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f90489f6e837c08eacd489bb40e4011a.png)'
  prefs: []
  type: TYPE_IMG
- en: Graph by author
  prefs: []
  type: TYPE_NORMAL
- en: We can look at the most common examples to ensure that there’s not much information
    in such comments.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: So we can filter out all comments shorter than 20 symbols — 556 out of 12 890
    reviews (4.3%). Then, we will analyse only long statements with more context.
    It’s an arbitrary threshold based on examples, you can try a couple of levels
    and see what texts are filtered out.
  prefs: []
  type: TYPE_NORMAL
- en: It’s worth checking whether this filter disproportionally affects some hotels.
    Shares of short comments are pretty close for different categories. So, the data
    looks OK.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/db00af7437b154c1f3ef37efe8257ab4.png)'
  prefs: []
  type: TYPE_IMG
- en: Graph by author
  prefs: []
  type: TYPE_NORMAL
- en: The simpliest topic model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now, it’s time to build our first topic model. Let’s start simple with the most
    basic one to understand how library works, then we will improve it.
  prefs: []
  type: TYPE_NORMAL
- en: We can train a topic model in just a few code lines that could be easily understood
    by anyone who has used at least one ML package before.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: The default model returned 113 topics. We can look at top topics.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/8e73ff2d0dd373322f265f8d0e382908.png)'
  prefs: []
  type: TYPE_IMG
- en: The biggest group is `Topic -1` , which corresponds to outliers. By default,
    `BERTopic` uses `HDBSCAN` for clustering, and it doesn’t force all data points
    to be part of clusters. In our case, 6 356 reviews are outliers (around 49.3%
    of all reviews). It is almost a half of our data, so we will work with this group
    later.
  prefs: []
  type: TYPE_NORMAL
- en: A topic representation is usually a set of most important words specific to
    this topic and not others. So, the best way to understand a topic is to look at
    the main terms (in `BERTopic`, a [class-based TF-IDF](/creating-a-class-based-tf-idf-with-scikit-learn-caea7b15b858)
    score is used to rank the words).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/a960aeccd8b3c5594637dfb3299752d0.png)'
  prefs: []
  type: TYPE_IMG
- en: Graph by author
  prefs: []
  type: TYPE_NORMAL
- en: '`BERTopic` even has [Topics per Class](https://maartengr.github.io/BERTopic/getting_started/topicsperclass/topicsperclass.html)
    representation that can solve our task of understanding the differences in course
    reviews.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/7db8c5297bdcf3ef79213ce4a93cf977.png)'
  prefs: []
  type: TYPE_IMG
- en: Graph by author
  prefs: []
  type: TYPE_NORMAL
- en: If you are wondering how to interpret this graph, you are not alone — I also
    wasn’t able to guess. However, the author kindly supports this package, and there
    are a lot of answers on GitHub. From the [discussion](https://github.com/MaartenGr/BERTopic/issues/446),
    I learned that the current normalisation approach doesn’t show the share of different
    topics for classes. So, it hasn’t completely solved our initial task.
  prefs: []
  type: TYPE_NORMAL
- en: However, we did the first iteration in less than 10 rows of code. It’s fantastic,
    but there’s some room for improvement.
  prefs: []
  type: TYPE_NORMAL
- en: Dealing with the outliers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we saw earlier, almost 50% of data points are considered outliers. It’s quite
    a lot, let’s see what we could do with it.
  prefs: []
  type: TYPE_NORMAL
- en: 'The [documentation](https://maartengr.github.io/BERTopic/getting_started/outlier_reduction/outlier_reduction.html)
    provides four different strategies to deal with the outliers:'
  prefs: []
  type: TYPE_NORMAL
- en: based on topic-document probabilities,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: based on topic distributions,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: based on c-TF-IFD representations,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: based on document and topic embeddings.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can try different strategies and see which one fits your data the best.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Let’s look at examples of outliers. Even though these reviews are relatively
    short, they have multiple topics.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9f9153957e56f274473142d30aca9d79.png)'
  prefs: []
  type: TYPE_IMG
- en: '`BERTopic` uses clustering to define topics. It means that not more than one
    topic is assigned to each document. In most real-life cases, you can have a mixture
    of topics in your texts. We may be unable to assign a topic to the documents because
    they have multiple ones.'
  prefs: []
  type: TYPE_NORMAL
- en: Luckily, there’s a solution for it — use [Topic Distributions](https://maartengr.github.io/BERTopic/getting_started/distribution/distribution.html).
    With such an approach, each document will be split into tokens. Then, we will
    form subsentences (defined by sliding window and stride) and assign a topic for
    each such subsentence.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s try this approach and see whether we will be able to reduce the number
    of outliers without topics.
  prefs: []
  type: TYPE_NORMAL
- en: Improving the topic model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: However, Topic Distributions are based on the fitted topic model, so let’s enhance
    it.
  prefs: []
  type: TYPE_NORMAL
- en: First of all, we can use [CountVectorizer](https://maartengr.github.io/BERTopic/getting_started/vectorizers/vectorizers.html#countvectorizer).
    It defines how a document will be split into tokens. Also, it can help us to get
    rid of meaningless words like `to`, `not` or `the` (there are a lot of such words
    in our first model).
  prefs: []
  type: TYPE_NORMAL
- en: Also, we could improve topics’ representations and even try a couple of different
    models. I used the `KeyBERTInspired` model ([more details](https://maartengr.github.io/BERTopic/getting_started/representation/representation.html#keybertinspired)),
    but you could try other options (for example, [LLMs](https://maartengr.github.io/BERTopic/getting_started/representation/llm.html)).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: I specified `nr_topics = 'auto'` to reduce the number of topics. Then, all topics
    with a similarity over threshold will be merged automatically. With this feature,
    we got 99 topics.
  prefs: []
  type: TYPE_NORMAL
- en: I’ve created a function to get top topics and their shares so we could analyse
    it easier. Let’s look at the new set of topics.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/5d34f53fbcd237badd2812791bbe58b6.png)![](../Images/d0083a5b923beee193a5ff1c7fc2ac5d.png)'
  prefs: []
  type: TYPE_IMG
- en: Graph by author
  prefs: []
  type: TYPE_NORMAL
- en: We can also look at the Interoptic distance map to better understand our clusters,
    for example, which are close to each other. You can also use it to define some
    parent topics and subtopics. It’s called [Hierarchical Topic Modelling](https://maartengr.github.io/BERTopic/getting_started/hierarchicaltopics/hierarchicaltopics.html)
    and you can use other tools for it.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/bb1397be050d6663681d9e0d2b7314a2.png)'
  prefs: []
  type: TYPE_IMG
- en: Graph by author
  prefs: []
  type: TYPE_NORMAL
- en: Another insightful way to better understand your topics is to look at `visualize_documents`
    graph ([documentation](https://maartengr.github.io/BERTopic/getting_started/visualization/visualize_documents.html)).
  prefs: []
  type: TYPE_NORMAL
- en: We can see that the number of topics has reduced significantly. Also, there
    are no meaningless stop words in topics’ representations.
  prefs: []
  type: TYPE_NORMAL
- en: Reducing the number of topics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: However, we still see similar topics in the results. We can investigate and
    merge such topics manually.
  prefs: []
  type: TYPE_NORMAL
- en: For this, we can draw a Similarity matrix. I specified `n_clusters`, and our
    topics were clustered to visualise them better.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/0522d0d57f94d7c879c762ea784c2029.png)'
  prefs: []
  type: TYPE_IMG
- en: Graph by author
  prefs: []
  type: TYPE_NORMAL
- en: There are some pretty close topics. Let’s calculate the pair distances and look
    at the top topics.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: I found guidance on how to get the distance matrix from [GitHub discussions](https://github.com/MaartenGr/BERTopic/issues/292).
  prefs: []
  type: TYPE_NORMAL
- en: We can now see the top pairs of topics by cosine similarity. There are topics
    with close meanings that we could merge.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7a1a307bde2c14ca410fc7e52e446507.png)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '***Attention:*** after merging, all topics’ IDs and representations will be
    recalculated, so it’s worth updating if you use them.'
  prefs: []
  type: TYPE_NORMAL
- en: Now, we’ve improved our initial model and are ready to move on.
  prefs: []
  type: TYPE_NORMAL
- en: With real-life tasks, it’s worth spending more time on merging topics and trying
    different approaches to representation and clustering to get the best results.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The other potential idea is splitting reviews into separate sentences because
    comments are rather long.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Topic Distributions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let’s calculate topics’ and tokens’ distributions. I’ve used a window equal
    to 4 (the author advised using 4–8 tokens) and stride equal 1.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: For example, this comment will be split into subsentences (or sets of four tokens),
    and the closest of existing topics will be assigned to each. Then, these topics
    will be aggregated to calculate probabilities for the whole sentence. You can
    find more details in [the documentation](https://maartengr.github.io/BERTopic/getting_started/distribution/distribution.html).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/191c15ad06267d8710cbba9a45eb3a8c.png)'
  prefs: []
  type: TYPE_IMG
- en: Example shows how split works with basic CountVectorizer, window = 4 and stride
    = 1
  prefs: []
  type: TYPE_NORMAL
- en: Using this data, we can get the probabilities of different topics for each review.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/e980128534ba025d5ffc822c37cddd14.png)'
  prefs: []
  type: TYPE_IMG
- en: Graph by author
  prefs: []
  type: TYPE_NORMAL
- en: We can even see the distribution of terms for each topic and understand why
    we got this result. For our sentence, `best very beautiful`was the main term for
    `Topic 74`, while `location close to`defined a bunch of location-related topics.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/d627cb15f2b1e27ef30296031a7091d7.png)'
  prefs: []
  type: TYPE_IMG
- en: Graph by author
  prefs: []
  type: TYPE_NORMAL
- en: This example also shows that we might have spent more time merging topics because
    there are still pretty similar ones.
  prefs: []
  type: TYPE_NORMAL
- en: Now, we have probabilities for each topic and review. The next task is to select
    a threshold to filter irrelevant topics with too low probability.
  prefs: []
  type: TYPE_NORMAL
- en: We can do it as usual using data. Let’s calculate the distribution of selected
    topics per review for different threshold levels.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/43d349da0086309e4e644c8871cddd43.png)'
  prefs: []
  type: TYPE_IMG
- en: Graph by author
  prefs: []
  type: TYPE_NORMAL
- en: '`threshold = 0.05` looks like a good candidate because, with this level, the
    share of reviews without any topic is still low enough (less than 6%), while the
    percentage of comments with 4+ topics is also not so high.'
  prefs: []
  type: TYPE_NORMAL
- en: This approach has helped us to reduce the number of outliers from 53.4% to 5.8%.
    So, assigning multiple topics could be an effective way to handle outliers.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s calculate the topics for each doc with this threshold.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Comparing distributions by hotels
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now, we have multiple topics mapped to each review and we can compare topics’
    mixtures for different hotel chains.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s find cases when a topic has too high or low share for a particular hotel.
    For that, we will calculate for each pair topic + hotel share of comments related
    to the topic for this hotel vs. all others.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: However, not all differences are significant for us. We can say that the difference
    in topics’ distribution is worth looking at if there are
  prefs: []
  type: TYPE_NORMAL
- en: '**statistical significance** — the difference is not just by chance,'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**practical significance** — the difference is bigger than X% points (I used
    1%).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: We have all the stats for all topics and hotels, and the last step is to create
    a visualisation comparing topic shares by categories.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Then, we can calculate the top topics list and make graphs for them.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Here are a couple of examples of resulting charts. Let’s try to make some conclusions
    based on this data.
  prefs: []
  type: TYPE_NORMAL
- en: We can see that Holiday Inn, Travelodge and Park Inn have better prices and
    value for money compared to Hilton or Park Plaza.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/17c052fd162fc829d727053e3d3d2d6f.png)'
  prefs: []
  type: TYPE_IMG
- en: Graph by author
  prefs: []
  type: TYPE_NORMAL
- en: The other insight is that in Travelodge noise may be a problem.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3bce345923e1bfb3143b0dc6209bbea8.png)'
  prefs: []
  type: TYPE_IMG
- en: Graph by author
  prefs: []
  type: TYPE_NORMAL
- en: It’s a bit challenging for me to interpret this result. I’m not sure what this
    topic is about.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d4134f75b10738b3dc523e98ce730948.png)'
  prefs: []
  type: TYPE_IMG
- en: Graph by author
  prefs: []
  type: TYPE_NORMAL
- en: The best practice for such cases is to look at some examples.
  prefs: []
  type: TYPE_NORMAL
- en: '*We stayed in the East tower where* ***the lifts are under renovation****,
    only one works, but there are signs showing the way to service lifts which can
    be used also.*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*However, the carpet and the furniture could have a* ***refurbishment****.*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*It’s built right over Queensway station. Beware that this tube stop will be
    closed for* ***refurbishing*** *for one year! So you might consider noise levels.*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: So, this topic is about the cases of temporary issues during the hotel stay
    or furniture not in the best condition.
  prefs: []
  type: TYPE_NORMAL
- en: You can find the full code on [GitHub](https://github.com/miptgirl/miptgirl_medium/tree/main/bertopic_for_hotels).
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Today, we’ve done an end-to-end Topic Modelling analysis:'
  prefs: []
  type: TYPE_NORMAL
- en: Build a basic topic model using the BERTopic library.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Then, we’ve handled outliers, so only 5.8% of our reviews don’t have a topic
    assigned.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reduced the number of topics both automatically and manually to have a concise
    list.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Learned how to assign multiple topics to each document because, in most cases,
    your text will have a mixture of topics.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finally, we were able to compare reviews for different courses, create inspiring
    graphs and get some insights.
  prefs: []
  type: TYPE_NORMAL
- en: Thank you a lot for reading this article. I hope it was insightful to you. If
    you have any follow-up questions or comments, please leave them in the comments
    section.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Ganesan, Kavita and Zhai, ChengXiang. (2011). OpinRank Review Dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: UCI Machine Learning Repository.* [*https://doi.org/10.24432/C5QW4W*](https://doi.org/10.24432/C5QW4W.)
  prefs: []
  type: TYPE_NORMAL
- en: If you want to dive deeper into BERTopic
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[Article “Interactive Topic Modelling with BERTopic”](/interactive-topic-modeling-with-bertopic-1ea55e7d73d8)
    by Maarten Grootendorst (*BERTopic* *author*)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Article [“Topic Modelling with BERT”](/topic-modeling-with-bert-779f7db187e6)
    by Maarten Grootendorst
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Paper [“BERTopic: Neural topic modeling with a class-based TF-IDF procedure”](https://arxiv.org/abs/2203.05794)
    by Maarten Grootendorst'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
