- en: Using OpenCLIP for Image Search and Automatic Captioning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/using-openclip-for-image-search-and-automatic-captioning-fa1cbbd48ce4?source=collection_archive---------7-----------------------#2023-03-07](https://towardsdatascience.com/using-openclip-for-image-search-and-automatic-captioning-fa1cbbd48ce4?source=collection_archive---------7-----------------------#2023-03-07)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: How LAION used more data and new ML training techniques to improve image and
    text embeddings for various applications
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://robgon.medium.com/?source=post_page-----fa1cbbd48ce4--------------------------------)[![Robert
    A. Gonsalves](../Images/96b4da0f602a1cd9d1e1d2917868cbee.png)](https://robgon.medium.com/?source=post_page-----fa1cbbd48ce4--------------------------------)[](https://towardsdatascience.com/?source=post_page-----fa1cbbd48ce4--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----fa1cbbd48ce4--------------------------------)
    [Robert A. Gonsalves](https://robgon.medium.com/?source=post_page-----fa1cbbd48ce4--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fc97e6c73c13c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fusing-openclip-for-image-search-and-automatic-captioning-fa1cbbd48ce4&user=Robert+A.+Gonsalves&userId=c97e6c73c13c&source=post_page-c97e6c73c13c----fa1cbbd48ce4---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----fa1cbbd48ce4--------------------------------)
    ·12 min read·Mar 7, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Ffa1cbbd48ce4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fusing-openclip-for-image-search-and-automatic-captioning-fa1cbbd48ce4&user=Robert+A.+Gonsalves&userId=c97e6c73c13c&source=-----fa1cbbd48ce4---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ffa1cbbd48ce4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fusing-openclip-for-image-search-and-automatic-captioning-fa1cbbd48ce4&source=-----fa1cbbd48ce4---------------------bookmark_footer-----------)![](../Images/01279abccfb8391b5c22a466bcb77154.png)'
  prefs: []
  type: TYPE_NORMAL
- en: “**high-tech computer system for finding pictures in a large library,”** I*mage
    created using an AI image creation program,* Midjourney, and edited by the author
  prefs: []
  type: TYPE_NORMAL
- en: I have been using and writing about OpenAI’s CLIP system since it came out in
    2021 [1]. It consists of image and text encoding models that can be used for various
    forms of cross-modal comparison, like using a text query to find the best matching
    image in a library quickly.
  prefs: []
  type: TYPE_NORMAL
- en: In December 2022, an independent group of researchers known as LAION released
    a paper called “Reproducible scaling laws for contrastive language-image learning”
    [2] that describes how they first reimplemented and trained a model similar to
    CLIP and then experimented with improving the system by training with a larger
    dataset and using new ML techniques. They call their new model OpenCLIP.
  prefs: []
  type: TYPE_NORMAL
- en: In this article, I will provide some background info on the original CLIP, describe
    how LAION improved the model, and show some results from my experiments with the
    two systems using images from the [Library of Congress’s Flickr photostream](https://www.flickr.com/photos/library_of_congress/).
    I also implemented a cool technique called “embedding arithmetic” from Meta AI
    to search for photos using both images and text prompts [3].
  prefs: []
  type: TYPE_NORMAL
