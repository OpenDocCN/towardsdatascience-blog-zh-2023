- en: 'Transformers can generate NFL plays : introducing QB-GPT'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Transformers可以生成NFL比赛：介绍QB-GPT
- en: 原文：[https://towardsdatascience.com/transformers-can-generate-nfl-plays-introducing-qb-gpt-2d40f16a03eb?source=collection_archive---------14-----------------------#2023-11-07](https://towardsdatascience.com/transformers-can-generate-nfl-plays-introducing-qb-gpt-2d40f16a03eb?source=collection_archive---------14-----------------------#2023-11-07)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/transformers-can-generate-nfl-plays-introducing-qb-gpt-2d40f16a03eb?source=collection_archive---------14-----------------------#2023-11-07](https://towardsdatascience.com/transformers-can-generate-nfl-plays-introducing-qb-gpt-2d40f16a03eb?source=collection_archive---------14-----------------------#2023-11-07)
- en: Bridging the gap between GenAI and sports analytics
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 弥合GenAI与体育分析之间的差距
- en: '[](https://medium.com/@sam.chaineau?source=post_page-----2d40f16a03eb--------------------------------)[![Samuel
    Chaineau](../Images/53523a7fb804971410841d38a47457c7.png)](https://medium.com/@sam.chaineau?source=post_page-----2d40f16a03eb--------------------------------)[](https://towardsdatascience.com/?source=post_page-----2d40f16a03eb--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----2d40f16a03eb--------------------------------)
    [Samuel Chaineau](https://medium.com/@sam.chaineau?source=post_page-----2d40f16a03eb--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@sam.chaineau?source=post_page-----2d40f16a03eb--------------------------------)[![Samuel
    Chaineau](../Images/53523a7fb804971410841d38a47457c7.png)](https://medium.com/@sam.chaineau?source=post_page-----2d40f16a03eb--------------------------------)[](https://towardsdatascience.com/?source=post_page-----2d40f16a03eb--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----2d40f16a03eb--------------------------------)
    [Samuel Chaineau](https://medium.com/@sam.chaineau?source=post_page-----2d40f16a03eb--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F71edae854a7f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransformers-can-generate-nfl-plays-introducing-qb-gpt-2d40f16a03eb&user=Samuel+Chaineau&userId=71edae854a7f&source=post_page-71edae854a7f----2d40f16a03eb---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----2d40f16a03eb--------------------------------)
    ·10 min read·Nov 7, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F2d40f16a03eb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransformers-can-generate-nfl-plays-introducing-qb-gpt-2d40f16a03eb&user=Samuel+Chaineau&userId=71edae854a7f&source=-----2d40f16a03eb---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F71edae854a7f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransformers-can-generate-nfl-plays-introducing-qb-gpt-2d40f16a03eb&user=Samuel+Chaineau&userId=71edae854a7f&source=post_page-71edae854a7f----2d40f16a03eb---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----2d40f16a03eb--------------------------------)
    · 10分钟阅读 · 2023年11月7日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F2d40f16a03eb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransformers-can-generate-nfl-plays-introducing-qb-gpt-2d40f16a03eb&user=Samuel+Chaineau&userId=71edae854a7f&source=-----2d40f16a03eb---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F2d40f16a03eb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransformers-can-generate-nfl-plays-introducing-qb-gpt-2d40f16a03eb&source=-----2d40f16a03eb---------------------bookmark_footer-----------)![](../Images/c8da7b29154888dc16655b0835a0b189.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F2d40f16a03eb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransformers-can-generate-nfl-plays-introducing-qb-gpt-2d40f16a03eb&source=-----2d40f16a03eb---------------------bookmark_footer-----------)![](../Images/c8da7b29154888dc16655b0835a0b189.png)'
- en: Photo by [Zetong Li](https://unsplash.com/@zetong?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 由 [Zetong Li](https://unsplash.com/@zetong?utm_source=medium&utm_medium=referral)
    摄影，发表于 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: '*Since my first article about* [*StratFormer*](https://medium.com/better-programming/transformers-will-guess-nfl-playbooks-f90e7420835b)*,
    I received a relatively great amount of feedbacks and ideas (so first thank you
    !). This pushed me to deepen my work an try an extra step :* ***building a football
    plays’ generator****. In this article I present* ***QB-GPT****, a model that can
    effectively generate football plays once provided with some elements. A dedicated
    HuggingFace space can be found* [*here*](https://huggingface.co/spaces/samchain/QB-GPT)
    *to play with it. I will later in the month share my work and findings on how
    to better predict NFL plays using this kind of generative models as backbone.
    The industryy is also watching this field as* [*DeepMind Safety Research*](https://medium.com/u/55e08ddea42e?source=post_page-----2d40f16a03eb--------------------------------)
    *is currently doing research on soccer with Liverpool on understanding how players
    move on the field.*'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '*自从我撰写了关于* [*StratFormer*](https://medium.com/better-programming/transformers-will-guess-nfl-playbooks-f90e7420835b)*的第一篇文章后，我收到了相对较多的反馈和想法（所以首先谢谢大家！）。这促使我深入研究，并尝试进一步的步骤：*
    ***构建一个足球战术生成器****。在这篇文章中，我介绍了* ***QB-GPT****，一个能够在提供一些元素后有效生成足球战术的模型。可以在* [*这里*](https://huggingface.co/spaces/samchain/QB-GPT)
    *找到一个专门的 HuggingFace 空间以进行尝试。我将在本月晚些时候分享我关于如何使用这种生成模型作为基础来更好地预测 NFL 战术的工作和发现。行业也在关注这一领域，因为*
    [*DeepMind 安全研究*](https://medium.com/u/55e08ddea42e?source=post_page-----2d40f16a03eb--------------------------------)
    *目前正在与利物浦合作研究足球，了解球员在场上的移动方式。*'
- en: '![](../Images/9191405f2c4641d82bf08476efed5054.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9191405f2c4641d82bf08476efed5054.png)'
- en: Generated trajectory by QB-GPT
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: QB-GPT 生成的轨迹
- en: '![](../Images/b845679a6346a8d6a7cc8c3e4ab33570.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b845679a6346a8d6a7cc8c3e4ab33570.png)'
- en: True trajectory
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 真实轨迹
- en: '[**Stratformer**](https://medium.com/better-programming/transformers-will-guess-nfl-playbooks-f90e7420835b),
    my first idea that I started in Oct 2021, was an encoder-only model which took
    a trajectory as input, tried to complete it and predicted some contextual elements
    associated with it (team, positions and plays). While this model showed interesting
    patterns (such as understanding what makes truly different a RB and a WR), it
    relied on a “a posteriori” look on the play. What could be even more interesting
    is to deeply understand how the set up of players, with some contextual elements,
    effectively affect the paths of the team. In other words, when the teams are facing
    each others at the scrimmage line, what is going to happen ?'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '[**StratFormer**](https://medium.com/better-programming/transformers-will-guess-nfl-playbooks-f90e7420835b)，这是我在
    2021 年 10 月开始的第一个想法，是一个仅编码器模型，它以轨迹作为输入，尝试完成它并预测与之相关的一些上下文元素（如团队、位置和战术）。虽然这个模型显示出有趣的模式（例如理解真正使
    RB 和 WR 区别开来的因素），但它依赖于对战术的“后验”观察。更有趣的可能是深入理解玩家的布置以及一些上下文元素如何实际影响团队的路线。换句话说，当两队在进攻线对峙时，会发生什么？'
- en: By building such an algorithm, we are now able to build a model that “really”
    understands the football game, as it is basically trying to recreate plays from
    few elements. This is the goal of **QB-GPT**. GPT is here because it relies on
    the same concepts of decoding that use any GPT model.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 通过构建这样的算法，我们现在能够建立一个“真正”理解足球比赛的模型，因为它基本上试图从少量元素中重现战术。这就是**QB-GPT**的目标。GPT 在这里的作用是因为它依赖于任何
    GPT 模型使用的解码概念。
- en: This article will cover the model and the few necessary tricks that I had to
    implement to make it “ok”. An attached HuggingFace space is available [here](https://huggingface.co/spaces/samchain/QB-GPT)
    to generate some plays if you wish, I will keep it open a limited time depending
    on the cost. While I recognized it is limited and prone to a range of improvements,
    I think such application deserves to be shared with a broader audience. If interested
    to discuss it or deepen some aspects, my contacts are on the app and at the end
    of the article. Once again, I did this work on my own, with the data I could find
    and the imperfections related to it (If someone working at the NFL/NGS team reads
    this, DMs more than open)
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 这篇文章将介绍模型和我为使其“正常”而必须实现的一些必要技巧。一个附加的 HuggingFace 空间可以在 [这里](https://huggingface.co/spaces/samchain/QB-GPT)
    生成一些战术，如果你愿意，我会根据成本限制开放一段时间。虽然我认识到它有局限性，并且容易改进，但我认为这样的应用值得与更广泛的受众分享。如果有兴趣讨论或深入了解一些方面，我的联系方式在应用程序和文章末尾。再次声明，我是独自完成了这项工作，使用了我能找到的数据及其相关的不完美之处（如果有人在
    NFL/NGS 团队工作看到这篇文章，私信随时欢迎）
- en: '**Part 1 : The data**'
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**第一部分：数据**'
- en: The data required to perform such task is very very hard to find. I rely on
    a specific data type provided only by the **Next Gen Stats (NGS)** tool of the
    NFL, where, for any given play, I can track up to the 22 players present on the
    field. The problem is that this data is not accessible via a classic API request.
    However, since 2019, the NFL is offering a wide range of data competition on Kaggle,
    often provided with datasets from the NGS. Also, some people on GitHub have scrapped
    back in the days the NGS browser. I did not try to do it and only relied on available
    data that had a usable format.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 执行此任务所需的数据非常难以获取。我依赖于 NFL 提供的**Next Gen Stats (NGS)** 工具的特定数据类型，其中，对于任何给定的进攻，我可以跟踪最多
    22 名在场上的球员。问题是，这些数据不能通过经典的 API 请求访问。然而，自 2019 年以来，NFL 在 Kaggle 上提供了各种数据竞赛，通常附带
    NGS 的数据集。此外，一些人在 GitHub 上曾经抓取过 NGS 浏览器。我没有尝试这样做，只依赖于具有可用格式的数据。
- en: 'An extensive time has been spent on merging data between themselves (around
    200 hours). The most important tasks were to:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 合并数据之间花费了大量时间（大约 200 小时）。最重要的任务是：
- en: Find the players on the field using nflVerse
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 nflVerse 查找场上的球员
- en: Associate their positions
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 关联他们的位置
- en: Define the scrimmage line using the play by play data of nflVerse
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 nflVerse 的逐场数据定义进攻线
- en: Normalizing the trajectory by subtracting the original position of the player
    to each instance of the trajectory. Hence each element of the trajectory is a
    distance between time i and time 0.
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过从轨迹的每个实例中减去球员的原始位置来规范化轨迹。因此，轨迹的每个元素是时间 i 和时间 0 之间的距离。
- en: Converting the distances to indices (like building a vocabulary for a BERT)
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将距离转换为索引（类似于为 BERT 构建词汇表）
- en: Apply some sanity checks (for instance removing players that are not on the
    pitch)
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 执行一些合理性检查（例如，移除未在场上的球员）
- en: Convert the data in arrays and dict format for TensorFlow
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将数据转换为适用于 TensorFlow 的数组和字典格式
- en: 'I used **Polars during all the process.** I strongly recommend to any data
    scientist, ML engineer, data engineer or people working with large volume of tabular
    data to quickly add this impressive package into their toolkit. *TLDR: Polars
    is better than pandas on small dataset and better than pypsark on large ones*.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 我在整个过程中使用了**Polars**。我强烈推荐任何数据科学家、机器学习工程师、数据工程师或处理大量表格数据的人员将这个令人印象深刻的包快速添加到他们的工具包中。*TLDR：Polars
    在小数据集上优于 pandas，在大数据集上优于 pypsark*。
- en: All in all, I compiled **47,991 different plays** representing **870,559 different
    players’ trajectories** on the field (18 players monitored per play on average,
    unfortunately never one OL…).
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 总的来说，我汇编了**47,991 个不同的进攻**，代表**870,559 个不同球员的场上轨迹**（平均每场监测 18 名球员，不幸的是从未有 OL…）。
- en: I monitor each player’s position at a 0.2 second frame rate, representing **28,147,112
    positions on the field in total**. I limit the data to the first 10 seconds as
    the trajectories tend to be more and more chaotic after, and hence difficult to
    model from a probabilistic point of view.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 我以 0.2 秒的帧率监控每个球员的位置，总共**28,147,112 个场上位置**。我将数据限制在前 10 秒内，因为轨迹在之后往往变得越来越混乱，因此从概率的角度进行建模较为困难。
- en: My dataset starts in 2018 to 2022 and covers **3,190 unique players.** The data
    is not perfect but gives a good sample and may be enough to assess whether transformers
    are helpful.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 我的数据集从 2018 年到 2022 年，涵盖了**3,190 名独特球员**。数据并不完美，但提供了一个良好的样本，可能足以评估变压器是否有帮助。
- en: 'To summarize, my data sources are :'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 总结一下，我的数据来源是：
- en: The NFL Big Data Bowl of 2021 [link](https://www.kaggle.com/competitions/nfl-big-data-bowl-2021/data)
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NFL 大数据碗 2021 [link](https://www.kaggle.com/competitions/nfl-big-data-bowl-2021/data)
- en: The NFL Big Data Bowl of 2022 [link](https://www.kaggle.com/competitions/nfl-big-data-bowl-2022/data)
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NFL 大数据碗 2022 [link](https://www.kaggle.com/competitions/nfl-big-data-bowl-2022/data)
- en: The NFL Big Data Bowl of 2023 [link](https://www.kaggle.com/competitions/nfl-big-data-bowl-2023/data)
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NFL 大数据碗 2023 [link](https://www.kaggle.com/competitions/nfl-big-data-bowl-2023/data)
- en: Public git repo NGS Highlights [link](https://github.com/asonty/ngs_highlights)
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 公开的 git 仓库 NGS Highlights [link](https://github.com/asonty/ngs_highlights)
- en: 'Here under you have a scheme representing the input input embedding for a single
    player:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是一个表示单个球员输入嵌入的示意图：
- en: '![](../Images/df7540caf17fae50eb5087db156cd23b.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/df7540caf17fae50eb5087db156cd23b.png)'
- en: Then the eleven players are concatenated per play and then truncated by frames
    to always have a number of token equal to 256\. I limited the number of frames
    for one player at 21 (max = 21*11 = 231) to make sure the we have consistently
    the same number of frames per player. Hence, I had to create new trajectories
    starting directly at a given moment of a play as most of my plays have more than
    21 frames. I created a padding step of 12 frames, meaning the trajectory is now
    divided in sub trajectories, each time shifted by 12 frames. This process tends
    to make harder the task of prediction for the frames 12, 24, 36 and 48 as we will
    see later.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 然后将十一名球员按比赛进行连接，然后按帧截断，以始终保持标记数等于 256。为了确保每名球员的帧数始终相同，我将每名球员的帧数限制为 21（最大 = 21*11
    = 231）。因此，我不得不从比赛的某个特定时刻直接创建新的轨迹，因为我的大多数比赛有超过 21 帧。我创建了一个 12 帧的填充步骤，这意味着轨迹现在被分成子轨迹，每次偏移
    12 帧。这个过程往往使得预测第 12、24、36 和 48 帧的任务变得更加困难，正如我们稍后将看到的。
- en: '![](../Images/0e7ae7ecea369218fa4f514c12181dea.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0e7ae7ecea369218fa4f514c12181dea.png)'
- en: Elements can be discussed. For instance the relevance of dicing the field with
    a 1 yard basis or why using a 0.2 second frame rate. I think the model (so its
    training data) is a start and I want to acknowledge that not everything is perfect.
    Feedbacks and opinions are welcome as long as they are relevant.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 元素可以讨论。例如，用 1 码为基础对场地进行划分的相关性，或为何使用 0.2 秒的帧率。我认为模型（即其训练数据）是一个起点，我想承认并非一切都是完美的。反馈和意见都是欢迎的，只要它们是相关的。
- en: 'Part 2 : The model'
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第 2 部分：模型
- en: The model is completely inspired from the OpenAI GPT architecture. It relies
    on a embedding layer adding different contextual elements to the input tokens.
    The embeddings are then fed to a single transformers module using multi-head attention
    with 3 heads. In the Large model, a second transformers module is applied. The
    output is then fed into a dense layer with a “relu” activation. To obtain the
    predictions, we apply a soft-max activation on logits.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型完全受 OpenAI GPT 架构的启发。它依赖于一个嵌入层，将不同的上下文元素添加到输入标记中。然后，将这些嵌入传递到一个使用 3 个头的多头注意力的单个
    Transformer 模块中。在大型模型中，应用了第二个 Transformer 模块。输出随后传递到一个具有“relu”激活的全连接层中。为了获得预测结果，我们对
    logits 应用 soft-max 激活。
- en: 'Two tricks were needed to adapt the architecture and training :'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 为了适应架构和训练，需要两个技巧：
- en: '**Multi-temporal Causal Masks** : In a classic GPT, the embedding’s attention
    at position i can only attend to the tokens from position 0 to i-1\. In our case,
    as I am decoding the team **completely** I need the tokens at time i to attend
    to every tokens available between time 0 and i-1\. Instead of having the so-called
    “lower triangular mask”, you end up with a multi triangular mask.'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**多时间步因果掩码**：在经典 GPT 中，位置 i 处的嵌入只能关注从位置 0 到 i-1 的标记。在我们的案例中，由于我正在**完全**解码团队，我需要时间
    i 处的标记能够关注时间 0 到 i-1 之间的所有标记。与其使用所谓的“下三角掩码”，你最终会得到一个多三角掩码。'
- en: '![](../Images/5af61ad139a1017cd0cefe0aaeb132e0.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5af61ad139a1017cd0cefe0aaeb132e0.png)'
- en: Attention masks
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力掩码
- en: '![](../Images/2236dd3b9ce207f1d2382c1e1305a0f8.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2236dd3b9ce207f1d2382c1e1305a0f8.png)'
- en: Raw attention scores and after attention masks subtractions
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 原始注意力得分和注意力掩码减去后的得分
- en: '![](../Images/7eae47de8f4a51ded66bf92c18af574c.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7eae47de8f4a51ded66bf92c18af574c.png)'
- en: Attention scores with different vmax scale
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 不同 vmax 规模下的注意力得分
- en: '**Pre-Layer Normalization:** Inspired by the work of [Sik-Ho Tsang](https://medium.com/u/aff72a0c1243?source=post_page-----2d40f16a03eb--------------------------------),
    I implemented its proposed Transformers module where normalization is done before
    multi-head attention and before FFN.'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**预层归一化：** 受到 [Sik-Ho Tsang](https://medium.com/u/aff72a0c1243?source=post_page-----2d40f16a03eb--------------------------------)
    研究的启发，我实现了其提出的 Transformer 模块，其中归一化在多头注意力和 FFN 之前完成。'
- en: 'The model does not require a high number of attention layers as the underlying
    patterns to model are quite simple. I trained 4 different models’ architectures
    using the same settings. Results show that embeddings’ dimensions plays a crucial
    roles in the level of accuracy that we can reach while number of attention modules
    does not significantly improve accuracy:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型不需要大量的注意力层，因为要建模的基本模式相当简单。我使用相同的设置训练了 4 种不同模型的架构。结果显示，嵌入维度在我们能达到的准确度水平中发挥了至关重要的作用，而注意力模块的数量对准确度的提升并不显著：
- en: '***Tiny :*** embedding dimension of 64, representing **1,539,452** **parameters**'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***Tiny：*** 嵌入维度为 64，表示**1,539,452** **参数**'
- en: '***Small*** : embedding dimension of 128, representing **3,182,716 parameters**'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***小型***：嵌入维度为 128，代表**3,182,716** 个参数'
- en: '***Medium*** : embedding dimension of 256, representing **6,813,308 parameters**'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***中等***：嵌入维度为 256，代表**6,813,308** 个参数'
- en: '***Large*** : embedding dimension of 128 but with two attention modules, representing
    **7,666,556 parameters**'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***大型***：嵌入维度为 128，但具有两个注意力模块，代表**7,666,556** 个参数'
- en: I think the large one could reach a better performance with an in-depth review
    of the scheduler.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 我认为大模型经过深入审查调度器后可能会获得更好的性能。
- en: (For comparison, GPT3 has 175 billion parameters)
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: （用于比较，GPT3 有 1750 亿个参数）
- en: 'The charts here under show a comparison of accuracy and loss during training
    between the different models:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表展示了不同模型在训练过程中的准确率和损失的比较：
- en: '![](../Images/0352c1ed1fed7fa5a95e05dd4ac38f8b.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0352c1ed1fed7fa5a95e05dd4ac38f8b.png)'
- en: Loss and Accuracy across the four models
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 四个模型的损失和准确率
- en: 'Part 3: The training'
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第三部分：训练
- en: The train set is made of 80% of the trajectories, representing **205,851** examples.
    The test set is made of 20% of the trajectories, representing **51,463** examples.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 训练集由 80% 的轨迹组成，共有**205,851** 个样本。测试集由 20% 的轨迹组成，共有**51,463** 个样本。
- en: Models are trained using a simple callback scheduler on 9 epochs where the learning
    rate start at 1e-3 and ends at 5e-4, with a batch size of 32.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 模型使用简单的回调调度器在 9 个周期中训练，学习率从 1e-3 开始，最终为 5e-4，批量大小为 32。
- en: The loss is a categorical cross-entropy with class weights designed by occurrences’
    thresholds on the train set (integers that occur more often weight less in the
    loss). Labels set at -100 do not count into the loss.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 损失是一个类别交叉熵，具有基于训练集（更常见的整数权重在损失中权重较小）的类别权重。设置为 -100 的标签不计入损失。
- en: 'The metrics used are:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 使用的度量指标包括：
- en: '**Accuracy** : Is the next team movement predicted exactly the same that the
    one labeled ?'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**准确率**：下一步团队动作是否与标记的动作完全相同？'
- en: '**Top 3 accuracy** : Is the next labeled movement in the top 3 predictions
    ?'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**前 3 准确率**：下一步标记的动作是否在前 3 个预测中？'
- en: '**Top 5 accuracy** : Is the next labeled movement in the top 5 predictions
    ?'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**前 5 准确率**：下一步标记的动作是否在前 5 个预测中？'
- en: '![](../Images/9c29b03acaafbc50978382c019d188b5.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9c29b03acaafbc50978382c019d188b5.png)'
- en: Top 3 and top 5 accuracies across the four models
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 四个模型的前 3 和前 5 准确率
- en: Eventually, a RMSE check has been performed on the x, y coordinates associated
    with every movements. This check enables us to monitor that beside not predicting
    accurately, it does not fall to far from the truth (predicting a 1 yard squared
    trajectory might be difficult).
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，对与每个动作相关的 x 和 y 坐标进行了 RMSE 检查。此检查使我们能够监控，除了预测不准确之外，它不会偏离真实情况太远（预测一个 1 码的平方轨迹可能很困难）。
- en: 'Part 4: The results'
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第四部分：结果
- en: Overall, the different models still learn underlying dynamic patterns and perform
    relatively good on my dataset. We can see that increasing embeddings’ dimensions
    brings increases in the accuracy of the models. The task of guessing a 1-yard
    squared is definitely challenging as movements on a football field are not driven
    with such a tiny range.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，不同的模型仍然能够学习潜在的动态模式，并在我的数据集上表现得相对良好。我们可以看到，增加嵌入维度会提高模型的准确率。猜测一个 1 码平方的任务确实具有挑战性，因为足球场上的动作并没有那么小的范围。
- en: 'I decided to slice those predictions in 3 categories : time, play types and
    positions.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 我决定将这些预测分为 3 类：时间、比赛类型和位置。
- en: '![](../Images/1e6997c9489e5ef9220a740a64359825.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1e6997c9489e5ef9220a740a64359825.png)'
- en: Accuracy and RMSE over time (frames)
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 准确率和 RMSE 随时间（帧）的变化
- en: The first 5 frames are relatively easy to predict for the model as often players
    are starting with similar movements. There is a drop in accuracy of 40–50% between
    frame 5 and 10\. This is the moment where plays tend to be different and have
    their own paths. Among the four models, the tiny one struggles the most especially
    at the end of the trajectory. The medium one shows a very good performance even
    on long term (more than 20 frames). The peaks are related to the starts of padded
    trajectories as these are fed in without any past knowledge of the trajectory.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 对模型来说，前 5 帧相对较容易预测，因为球员经常以相似的动作开始。帧 5 到 10 之间的准确率下降了 40-50%。这是动作趋于不同且有各自路径的时刻。在四个模型中，小模型在轨迹末端特别困难。中等模型即使在长期（超过
    20 帧）中也表现得非常好。峰值与填充轨迹的开始相关，因为这些数据在没有过去轨迹知识的情况下输入。
- en: '![](../Images/73ffae52dc35c5994bf1536ff488e289.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/73ffae52dc35c5994bf1536ff488e289.png)'
- en: Accuracy and RMSE over play types
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 比赛类型的准确性和 RMSE
- en: Plays that are relatively static are (without surprise) the easier to predict.
    Runs, passes, kick-offs and punts are (without surprise) the hardest to guess
    as players move a lot more and with possibly chaotic patterns. No significant
    differences among models are seen.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 相对静态的比赛（毫不意外）更容易预测。跑动、传球、开球和踢球（毫不意外）是最难猜测的，因为球员的移动更多，且可能存在混乱的模式。模型之间没有显著差异。
- en: '![](../Images/ff1363859b77dc66ea07fa772feba2d0.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ff1363859b77dc66ea07fa772feba2d0.png)'
- en: Accuracy and RMSE over positions
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 位置的准确性和 RMSE
- en: Position is a very discriminant factor with positions having difference between
    10 to 20% in accuracy. Overall, the positions that move a lot are also the toughest
    to predict. The tiny and small models tend to be less accurate than the rest in
    general.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 位置是一个非常显著的因素，不同位置之间的准确性差异可以达到 10% 到 20%。总体而言，移动较多的位置也是最难预测的。小型和微型模型通常比其他模型的准确性要差。
- en: '**Part 5: What I’ve seen so far by playing with it**'
  id: totrans-85
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**第 5 部分：我通过玩这个模型所见的一切**'
- en: The model’s ideal temperature seems to be between 1.5 and 2.5\. I tested a selection
    of 10, 20 and 50\. The more you push the temperature and selection, the more it
    tends to get crazy and yield strange patterns (players getting off the field,
    changing directions, big gaps between two frames).
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型的理想温度似乎在 1.5 和 2.5 之间。我测试了 10、20 和 50 的选择。温度和选择值越高，模型往往会变得越来越疯狂，产生奇怪的模式（球员离场、改变方向、两个帧之间的巨大间隙）。
- en: However, it seems to give interesting patterns that could be used for simulations
    and play books recognition.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，它似乎能够生成有趣的模式，这些模式可以用于模拟和战术手册识别。
- en: What’s next ?
  id: totrans-88
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 接下来是什么？
- en: This model is raw. It can be used later to explore opponents strategy, define
    new ones and even use it to enhance the player scouting system.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型是原始的。它可以用于后续探索对手策略、定义新的策略，甚至用于提升球员侦察系统。
- en: However, the core idea is to see if this model can effectively predict play
    results effectively aka yard gains. I will soon share my work about a predicting
    framework for NFL plays which is based on findings in QBGPT. By using the generative
    abilities of such model, you can draw a great number of scenarios from which predicting
    plays is an easier task ;)
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，核心思想是看看这个模型是否能有效地预测比赛结果，即码数增益。我将很快分享我关于 NFL 比赛预测框架的工作，该框架基于 QBGPT 的发现。通过利用这种模型的生成能力，你可以从中绘制出大量场景，从而更容易预测比赛
    ;)
- en: In the meantime you can play with QB-GPT at this hugging face space, if you
    liked this article, the work and the findings don’t forget to share and like !
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 与此同时，你可以在这个 hugging face 空间里玩 QB-GPT。如果你喜欢这篇文章、这些工作和发现，别忘了分享和点赞！
- en: (Unless otherwise noted, every images are by the author)
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: （除非另有说明，每张图片均由作者提供）
- en: 'Samuel Chaineau : [Linkedin](https://www.linkedin.com/feed/)'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 'Samuel Chaineau : [Linkedin](https://www.linkedin.com/feed/)'
