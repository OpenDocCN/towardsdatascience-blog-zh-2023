- en: 'Introduction to ICA: Independent Component Analysis'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ÂéüÊñáÔºö[https://towardsdatascience.com/introduction-to-ica-independent-component-analysis-b2c3c4720cd9?source=collection_archive---------1-----------------------#2023-02-14](https://towardsdatascience.com/introduction-to-ica-independent-component-analysis-b2c3c4720cd9?source=collection_archive---------1-----------------------#2023-02-14)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[](https://medium.com/@jonas_dieckmann?source=post_page-----b2c3c4720cd9--------------------------------)[![Jonas
    Dieckmann](../Images/e1f2d236e6bda6ec1e14fd5eaa9d205e.png)](https://medium.com/@jonas_dieckmann?source=post_page-----b2c3c4720cd9--------------------------------)[](https://towardsdatascience.com/?source=post_page-----b2c3c4720cd9--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----b2c3c4720cd9--------------------------------)
    [Jonas Dieckmann](https://medium.com/@jonas_dieckmann?source=post_page-----b2c3c4720cd9--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ¬∑
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F1c8d1cf684f2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintroduction-to-ica-independent-component-analysis-b2c3c4720cd9&user=Jonas+Dieckmann&userId=1c8d1cf684f2&source=post_page-1c8d1cf684f2----b2c3c4720cd9---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----b2c3c4720cd9--------------------------------)
    ¬∑9 min read¬∑Feb 14, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fb2c3c4720cd9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintroduction-to-ica-independent-component-analysis-b2c3c4720cd9&user=Jonas+Dieckmann&userId=1c8d1cf684f2&source=-----b2c3c4720cd9---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb2c3c4720cd9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintroduction-to-ica-independent-component-analysis-b2c3c4720cd9&source=-----b2c3c4720cd9---------------------bookmark_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: Have you ever found yourself in a situation where you were trying to analyze
    a complex and highly correlated data set and felt overwhelmed by the amount of
    information? This is where Independent Component Analysis (ICA) comes in. ICA
    is a powerful technique in the field of data analysis that allows you to separate
    and identify the underlying independent sources in a multivariate data set.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2a1a82db966344023ae9c0c042061047.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by [Unsplash](https://unsplash.com/)
  prefs: []
  type: TYPE_NORMAL
- en: ICA is important because it provides a way to understand the hidden structure
    of a data set, and it can be used in a variety of applications, such as signal
    processing, brain imaging, finance, and many other fields. In addition, ICA can
    help extract the most relevant information from data, providing valuable insights
    that would otherwise be lost in a sea of correlations.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this article, we will delve into #1 the **fundamentals** of ICA by discussing
    what cocktail parties might have to do with it, #2 the **3-step-ICA-algorithm,**
    and #3 how you can **implement** it in your data analysis projects. So, if you‚Äôre
    ready to unlock the full potential of your data, come along and join this journey!'
  prefs: []
  type: TYPE_NORMAL
- en: '#1: Introduction and main idea'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Independent Component Analysis is one of the various unsupervised learning
    algorithms which means that we do not need to supervise the model before we can
    use it. The origin of this method comes from signal processing where we try to
    separate a multivariate signal into additive subcomponents. Let us jump into an
    explanation of the main idea:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f0f9d0211a483891a04c16b475686318.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: Imagine some independent signals or variables. These signals can be represented
    as signal curves, with the first signal located at the top and the second signal
    at the bottom within the above image. As a result of the measurements, we did
    not receive a data set with the signals themselves, but a data set that includes
    measurements of these two signals which are unfortunately mixed into distinct
    linear combinations. The objective of ICA is to recover the original, unknown
    signals by separating the mixed data. The ultimate aim is to reconstruct the data
    such that each dimension is mutually independent.
  prefs: []
  type: TYPE_NORMAL
- en: To make this concept more tangible, the most well-known example of ICA, the
    ‚Äúcocktail party problem,‚Äù will be utilized.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/54cd1499df65b3ef7d8738310921be60.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by [Unsplash](https://unsplash.com/)
  prefs: []
  type: TYPE_NORMAL
- en: The cocktail party problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Imagine attending a cocktail party where multiple individuals are speaking simultaneously,
    making it difficult to follow a single conversation. It is noteworthy that humans
    possess the ability to separate individual voice streams in such situations. Technically,
    this becomes slightly challenging.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ba93641b0f4f8e8e65442a4a27425ff1.png)'
  prefs: []
  type: TYPE_IMG
- en: Cocktail party problem. Image by author.
  prefs: []
  type: TYPE_NORMAL
- en: Suppose we record the conversations of two groups in the party using two microphones.
    This results in two mixed signals, with the first measurement having a greater
    influence of the first group and a lesser influence of the second group, while
    the second measurement has a greater influence of the second group.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/100c7b7d0e9725c28619d75d66280b1b.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: The general framework for this can be represented in vector notation here in
    the grey box. The measurements in **vector X** are actually the signals from **vector
    S** multiplied with some **mixing coefficients**, represented in **matrix A**.
    Since we want to extract the full conversations (original signals), we need to
    solve this for vector S.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9fd17bb461ed1efe23f8d2b4daaee0a4.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: ICA vs. PCA
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You have probably already guessed that ICA is in some way related to principal
    component analysis (PCA). This assumption is not so wrong. The ideas underlying
    both concepts are not that far apart, but they differ in the last stage, which
    we will see later.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let‚Äôs summarise what PCA basically does: Suppose we have two variables that
    appear to be related. By maximizing the variances using the eigenvector and eigenvalues
    of these variables, we can convert them into principal components. In this particular
    example, PCA does a good job of identifying the principal direction of this relationship.'
  prefs: []
  type: TYPE_NORMAL
- en: Let‚Äôs use the previous cocktail example as an example. In a very simple representation,
    we could imagine that the two measurements from microphones one and two have relationships
    that form something like a cross pattern. If we were to apply PCA in this case,
    we would get the wrong results, because PCA fails for data sets with more than
    one main direction.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4a0e702fadaafaae9401042d46036ff4.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: ICA, on the other hand, solves this problem by focusing on independent components
    instead of main components.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b22bed3dc47ddac13b0f3335b61f689e.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: It is important to recall the established conceptual framework. The readings
    obtained from the microphones correspond to the original signals that have been
    multiplied by the mixing matrix A. By rearranging the equation with respect to
    vector S, the only necessary information to determine the original variables is
    matrix A. However, matrix A is unknown.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/de57f86779b1197742e725631d665914.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: Hence, to gain a comprehensive understanding of matrix A and ultimately calculate
    the vector S, it is necessary to undertake inverse operations through a series
    of steps. These sequential inverse operations comprise the three stages of the
    ICA algorithm, which will now be analyzed in greater detail.
  prefs: []
  type: TYPE_NORMAL
- en: '#2: Separation process | the 3-step-ICA-algorithm'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Before proceeding to a practical demonstration in R, it is important to understand
    the three steps of the algorithm. The goal of the algorithm is to perform the
    multiplication of vector X with matrix A. Matrix A is comprised of three constituent
    parts, which are the result of multiplicative interactions between the different
    factors:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/30cb2c0ab173358be8be08b2f7704348.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 1: Find the angle with maximal variance to rotate | estimate U^T**'
  prefs: []
  type: TYPE_NORMAL
- en: The first component of the algorithm involves the use of the matrix U^T, which
    is based on the first angle Theta. The angle Theta can be derived from the primary
    direction of the data, as determined through Principal Component Analysis (PCA).
    This step rotates the graph to the position shown above.
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 2: Find the scaling of the principal components | estimate ‚àë^(-1)**'
  prefs: []
  type: TYPE_NORMAL
- en: The second component involves stretching the figure, which is achieved through
    the Sigma^-1 step. This step employs the variances of sigma 1 and sigma 2 from
    the data, similar to the approach utilized in PCA.
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 3: Independence and kurtosis assumptions for rotation | estimate *V***'
  prefs: []
  type: TYPE_NORMAL
- en: The final component, which distinguishes the current algorithm from PCA, involves
    the rotation of the signals around angle Phi. This step aims to rebuild the original
    dimensions of the signals by utilizing the independence and kurtosis assumptions
    for rotation.
  prefs: []
  type: TYPE_NORMAL
- en: In summary, the algorithm employs measurements and performs rotation around
    theta, stretching through the use of variances sigma 1 and 2, and finally, rotation
    around Phi. The mathematical background of these steps has been summarized in
    the following slide for reference.
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, we can determine the **inverse matrix A using only the two angles
    and the variances of the data**, which is actually all we need to process the
    ICA algorithm. Take the measurements, rotate, and scale them. And finally, we
    rotate them again to get the final dimension.
  prefs: []
  type: TYPE_NORMAL
- en: '#3: Code examples with R using fastICA()'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: I hope you have understood the basic idea of the ICA algorithm so far. It is
    not necessary to understand every single step mathematically, but it is helpful
    to understand the concept behind it. With this knowledge, I would like to work
    out a practical example with you to show the practical application of the ICA
    algorithm using a function called `fastICA` in R.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'We create two random data sets: signal 1 and signal 2 which could be imagined
    as voice signals from our two cocktail groups:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/923b605893a01e22ecb0edd258c14bb3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Screenshots from R-output: original signals. Image by author'
  prefs: []
  type: TYPE_NORMAL
- en: The red curve stands for the first signal and the blue curve for the second.
    The shape does not matter in this case. What you should see is that both signals
    are different from each other. Let‚Äôs mix them now!
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/0851fa431c59d8badfc1b9badbee4908.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Screenshots from R-output: measurements. Image by author'
  prefs: []
  type: TYPE_NORMAL
- en: As you can see above, we simulate two measurements using both signals. Therefore
    the signals within the measurements are not independent anymore. Both mixed signals
    can be imagined as the recordings of our two microphones in the cocktail example.
    We now forget about our two original signals and imagine, that these two measurements
    are the only information we have about this data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Hence we want to separate them to finally get two independent signals:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/21b4eb9e8b0848750906dcb4cb4d16ff.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Screenshots from R-output: independent signals separated again. Image by author'
  prefs: []
  type: TYPE_NORMAL
- en: The result of this algorithm is shown above. The red curve is the estimate of
    signal 1, while the blue curve estimates signal 2\. And ‚Äî no surprise ‚Äî the algorithm
    has estimated almost the original signals, shown here on the right. You may have
    noticed that the red curve matches the expectation perfectly, while the blue curve
    appears to be inverted. This is because the algorithm cannot recover the exact
    amplitude of the source activity. But apart from that, the reconstruction has
    done a really good job here.
  prefs: []
  type: TYPE_NORMAL
- en: Limitation and conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Let‚Äôs start with the bad news:* ICA can only separate linearly mixed sources
    and further, we cannot separate perfectly Gaussian-distributed sources because
    they would kill the third step of our algorithm. While we expect independent sources
    that are mixed within linear combinations, the ICA would find a space where even
    not-independent sources are maximally independent.'
  prefs: []
  type: TYPE_NORMAL
- en: '*But now the good ones:* the ICA algorithm is a powerful method for different
    areas and is easily usable within open source packages for R; Mathlab and other
    systems. There are various examples, where ICA algorithms were used for applications:
    face recognition apps, predictions in the stock market, and many more. It is therefore
    an important and well-respected method in practical usage.'
  prefs: []
  type: TYPE_NORMAL
- en: '**In a nutshell:** We introduced the Independent Component Analysis as an unsupervised
    learning algorithm. The main idea is to separate given measurements of linear
    combinations back into the original signals. This is called *reconstruction* and
    uses the three-step ICA algorithm. The most popular example to visualize the problem
    behind this method is again the cocktail party problem. But enough problems with
    cocktails for now.'
  prefs: []
  type: TYPE_NORMAL
- en: Time for real cocktails parties üçπ
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[](https://medium.com/@jonas_dieckmann?source=post_page-----b2c3c4720cd9--------------------------------)
    [## Jonas Dieckmann - Medium'
  prefs: []
  type: TYPE_NORMAL
- en: Read writing from Jonas Dieckmann on Medium. analytics manager & product owner
    @ philips | passionate and writing about‚Ä¶
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: medium.com](https://medium.com/@jonas_dieckmann?source=post_page-----b2c3c4720cd9--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: I hope you find it useful. Let me know your thoughts! And feel free to connect
    on LinkedIn at [https://www.linkedin.com/in/jonas-dieckmann/](https://www.linkedin.com/in/jonas-dieckmann/)
    and/or to follow me here on medium.
  prefs: []
  type: TYPE_NORMAL
- en: 'See also some of my other articles:'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[](/ensemble-learning-bagging-and-boosting-23f9336d3cb0?source=post_page-----b2c3c4720cd9--------------------------------)
    [## Ensemble learning: Bagging and Boosting'
  prefs: []
  type: TYPE_NORMAL
- en: It‚Äôs time to explore the world of bagging and boosting. With these powerful
    techniques, you can improve the performance‚Ä¶
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/ensemble-learning-bagging-and-boosting-23f9336d3cb0?source=post_page-----b2c3c4720cd9--------------------------------)
    [](/how-to-set-started-with-tensorflow-using-keras-api-and-google-colab-5421e5e4ef56?source=post_page-----b2c3c4720cd9--------------------------------)
    [## How to get started with TensorFlow using Keras API and Google Colab
  prefs: []
  type: TYPE_NORMAL
- en: Step-by-step tutorial to analyze human activity with neuronal networks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/how-to-set-started-with-tensorflow-using-keras-api-and-google-colab-5421e5e4ef56?source=post_page-----b2c3c4720cd9--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[1]: Bell, AJ; Sejnowski, TJ (1997). ‚ÄúThe independent components of natural
    scenes are edge filters‚Äù. Vision Research. 37 (23): 3327‚Äì 3338\. doi:10.1016/s0042‚Äì6989(97)00121‚Äì1\.
    PMC 2882863\. PMID 9425547\. ÔÇß'
  prefs: []
  type: TYPE_NORMAL
- en: '[2]: Back, AD; Weigend, AS (1997). ‚ÄúA first application of independent component
    analysis to extracting structure from stock returns‚Äù. International Journal of
    Neural Systems. 8 (4): 473‚Äì484\. doi:10.1142/s0129065797000458\. PMID 9730022\.
    ÔÇß'
  prefs: []
  type: TYPE_NORMAL
- en: '[3]: Barlett, MS (2001). Face image analysis by unsupervised learning. Boston:
    Kluwer International Series on Engineering and Computer Science. ÔÇß'
  prefs: []
  type: TYPE_NORMAL
- en: '[4]: Comon, Pierre (1994). ‚ÄúIndependent component analysis, A new concept?‚Äù
    Signal Processing, Volume 36, Issue 3,Pages 287‚Äì314, ISSN 0165‚Äì1684, [https://doi.org/10.1016/0165-1684(94)90029-9.](https://doi.org/10.1016/0165-1684(94)90029-9.)'
  prefs: []
  type: TYPE_NORMAL
