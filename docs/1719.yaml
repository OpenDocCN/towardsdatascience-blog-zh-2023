- en: Testing Language Models (and Prompts) Like We Test Software
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 像测试软件一样测试语言模型（和提示）
- en: 原文：[https://towardsdatascience.com/testing-large-language-models-like-we-test-software-92745d28a359?source=collection_archive---------0-----------------------#2023-05-24](https://towardsdatascience.com/testing-large-language-models-like-we-test-software-92745d28a359?source=collection_archive---------0-----------------------#2023-05-24)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/testing-large-language-models-like-we-test-software-92745d28a359?source=collection_archive---------0-----------------------#2023-05-24](https://towardsdatascience.com/testing-large-language-models-like-we-test-software-92745d28a359?source=collection_archive---------0-----------------------#2023-05-24)
- en: '*TL;DR: you should*'
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '*总结：你应该*'
- en: '[](https://medium.com/@marcotcr?source=post_page-----92745d28a359--------------------------------)[![Marco
    Tulio Ribeiro](../Images/beb3909927c9c9575d87a828c9c6b841.png)](https://medium.com/@marcotcr?source=post_page-----92745d28a359--------------------------------)[](https://towardsdatascience.com/?source=post_page-----92745d28a359--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----92745d28a359--------------------------------)
    [Marco Tulio Ribeiro](https://medium.com/@marcotcr?source=post_page-----92745d28a359--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@marcotcr?source=post_page-----92745d28a359--------------------------------)[![Marco
    Tulio Ribeiro](../Images/beb3909927c9c9575d87a828c9c6b841.png)](https://medium.com/@marcotcr?source=post_page-----92745d28a359--------------------------------)[](https://towardsdatascience.com/?source=post_page-----92745d28a359--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----92745d28a359--------------------------------)
    [Marco Tulio Ribeiro](https://medium.com/@marcotcr?source=post_page-----92745d28a359--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F4274f519efce&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftesting-large-language-models-like-we-test-software-92745d28a359&user=Marco+Tulio+Ribeiro&userId=4274f519efce&source=post_page-4274f519efce----92745d28a359---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----92745d28a359--------------------------------)
    ·18 min read·May 24, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F92745d28a359&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftesting-large-language-models-like-we-test-software-92745d28a359&user=Marco+Tulio+Ribeiro&userId=4274f519efce&source=-----92745d28a359---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F4274f519efce&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftesting-large-language-models-like-we-test-software-92745d28a359&user=Marco+Tulio+Ribeiro&userId=4274f519efce&source=post_page-4274f519efce----92745d28a359---------------------post_header-----------)
    发表在[Towards Data Science](https://towardsdatascience.com/?source=post_page-----92745d28a359--------------------------------)
    · 18分钟阅读 · 2023年5月24日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F92745d28a359&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftesting-large-language-models-like-we-test-software-92745d28a359&user=Marco+Tulio+Ribeiro&userId=4274f519efce&source=-----92745d28a359---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F92745d28a359&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftesting-large-language-models-like-we-test-software-92745d28a359&source=-----92745d28a359---------------------bookmark_footer-----------)![](../Images/0971839269724022e19d9d743e6440b6.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F92745d28a359&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftesting-large-language-models-like-we-test-software-92745d28a359&source=-----92745d28a359---------------------bookmark_footer-----------)![](../Images/0971839269724022e19d9d743e6440b6.png)'
- en: Image created by the authors.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者创作。
- en: How can we test applications built with LLMs? In this post we look at the concept
    of testing applications (or prompts) built with language models, in order to better
    understand their capabilities and limitations. We focus entirely on testing in
    this article, but if you are interested in tips for writing better prompts, check
    out our [Art of Prompt Design](https://www.google.com/search?q=Art+of+Prompt+Design)
    series (ongoing).
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何测试使用大型语言模型（LLM）构建的应用程序？在这篇文章中，我们探讨了测试由语言模型构建的应用程序（或提示）的概念，以更好地了解它们的能力和局限性。我们在这篇文章中完全专注于测试，但如果你对编写更好提示的技巧感兴趣，可以查看我们的[提示设计艺术](https://www.google.com/search?q=Art+of+Prompt+Design)系列（持续更新）。
- en: While it is introductory, this post (written jointly with [Scott Lundberg](https://medium.com/@scottmlundberg))
    is based on a fair amount of experience. We’ve been thinking about testing NLP
    models for a while — e.g. in [this paper](https://homes.cs.washington.edu/~marcotcr/acl20_checklist.pdf)
    arguing that we should test NLP models like we test software, or [this paper](https://aclanthology.org/2022.acl-long.230.pdf)
    where we get GPT-3 to help users test their own models. This kind of testing is
    orthogonal to more traditional evaluation focused on benchmarks, or collecting
    human judgments on generated text. Both kinds are important, but we’ll focus on
    testing (as opposed to benchmarking) here, since it tends to be neglected.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这篇文章是引言性质的，但它（与[Scott Lundberg](https://medium.com/@scottmlundberg)共同撰写）基于相当多的经验。我们已经考虑了测试NLP模型有一段时间——例如，在[这篇论文](https://homes.cs.washington.edu/~marcotcr/acl20_checklist.pdf)中，我们主张应该像测试软件一样测试NLP模型，或者在[这篇论文](https://aclanthology.org/2022.acl-long.230.pdf)中，我们让GPT-3帮助用户测试他们自己的模型。这种测试方式与传统的基准测试或收集对生成文本的人类评判的方式是正交的。两种方式都很重要，但我们将在这里专注于测试（而不是基准测试），因为它往往被忽视。
- en: We’ll use ChatGPT as the LLM throughout, but the principles here are general,
    and apply to any LLM (or any NLP model, for that matter). All of our prompts use
    the [guidance](https://github.com/microsoft/guidance) library.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将始终使用ChatGPT作为LLM，但这里的原则是通用的，适用于任何LLM（或者说任何NLP模型）。我们所有的提示都使用[guidance](https://github.com/microsoft/guidance)库。
- en: 'The task: an LLM email-assistant'
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 任务：一个LLM电子邮件助手
- en: 'Testing ChatGPT or another LLM in the abstract is very challenging, since it
    can do so many different things. In this post, we focus on the more tractable
    (but still hard) task of testing a specific tool that *uses* an LLM. In particular,
    we made up a typical LLM-based application: an email-assistant. The idea is that
    a user highlights a segment of an email they received or a draft they are writing,
    and types in a natural language instruction such as `write a response saying no
    politely`, or `please improve the writing`, or `make it more concise`.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在抽象层面测试ChatGPT或其他LLM是非常具有挑战性的，因为它可以做很多不同的事情。在这篇文章中，我们专注于测试一个具体工具的相对易处理（但仍然困难）任务，该工具*使用*了LLM。特别是，我们编造了一个典型的基于LLM的应用：电子邮件助手。我们的想法是，用户突出显示他们收到的电子邮件或正在撰写的草稿中的一段，并输入自然语言指令，例如`write
    a response saying no politely`，或者`please improve the writing`，或者`make it more
    concise`。
- en: 'For example, here is an input `INSTRUCTION, HIGHLIGHTED_TEXT, SOURCE` (source
    indicates whether it''s a received email or draft), and corresponding output:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，这里是一个输入`INSTRUCTION, HIGHLIGHTED_TEXT, SOURCE`（source指示这是收到的电子邮件还是草稿），以及相应的输出：
- en: '[PRE0]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Our first step is to write a simple prompt to execute this task. Note that we
    are not trying to get the best possible prompt for this application, just something
    that allows us to illustrate the testing process.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的第一步是编写一个简单的提示来执行这个任务。请注意，我们不是为了这个应用程序找到最佳提示，而是为了说明测试过程的某种方法。
- en: '[PRE1]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Here is an example of running this prompt on the email above:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 这是在上面的电子邮件上运行这个提示的一个例子：
- en: '![](../Images/1376d98f51e21675acaba56bff3258f9.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1376d98f51e21675acaba56bff3258f9.png)'
- en: 'Let’s try this on making simple edits to a few sample sentences:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试对几个示例句子进行简单的编辑：
- en: '![](../Images/5a8a2605b892e08654570efe1dd131c2.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5a8a2605b892e08654570efe1dd131c2.png)'
- en: Despite being very simple, all of the examples above admit a very large number
    of right answers. How do we test an application like this? Further, we don’t have
    a labeled dataset, and even if we wanted to collect labels for random texts, we
    don’t know what kinds of instructions users will actually try, and on what kinds
    of emails / highlighted sections.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这些示例非常简单，但所有这些示例都有非常大量的正确答案。我们如何测试这样的应用程序？此外，我们没有标注的数据集，即使我们想为随机文本收集标签，我们也不知道用户实际上会尝试什么样的指令，以及在什么样的电子邮件/突出显示的部分上。
- en: We’ll first focus on *how* to test, and then discuss *what* to test.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先关注*如何*测试，然后讨论*测试什么*。
- en: 'How to test: properties'
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何测试：属性
- en: Even if we can’t specify a single right answer to an input, we can specify *properties*
    that any correct output should follow. For example, if the instruction is “Add
    an appropriate emoji”, we can verify properties like `the input only differs from
    the output in the addition of one or more emojis`. Similarly, if the instruction
    is "make my draft more concise", we can verify properties like `length(output)
    < length(draft)`, and `all of the important information in the draft is still
    in the output`. This approach (first explored in [CheckList](https://homes.cs.washington.edu/~marcotcr/acl20_checklist.pdf))
    borrows from **property-based testing** in software engineering and applies it
    to NLP.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: Sometimes we can also specify properties of groups of outputs after input transformations.
    For example, if we perturb an instruction by adding typos or the word ‘please’,
    we expect the output to be roughly the same in terms of content. If we add an
    intensifier to an instruction, such as `make it more concise` -> `make it much
    more concise`, we can expect the output to reflect the change in intensity or
    degree. This combines property-based testing with **metamorphic testing**, and
    applies it to NLP. This type of testing can be handy to check for robustness,
    consistency, and similar properties.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: '**Some properties are easy to evaluate:** The examples in CheckList were mostly
    of classification models, where it’s easy to verify certain properties automatically
    (e.g. `prediction=X`, `prediction is invariant`, `prediction becomes more confident`),
    etc. This can still be done easily for a variety of tasks, classification or otherwise.
    In [another blog post](https://medium.com/@marcotcr/exploring-chatgpt-vs-open-source-models-on-slightly-harder-tasks-aa0395c31610),
    we could check whether models solved quadratic equations correctly, since we knew
    the right answers. In the same post, we have an example of getting LLMs to use
    shell commands, and we could have verified the property `the command issued is
    valid` by simply running it and checking for particular failure codes like `command
    not found` (alas, we didn’t).'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: '**Evaluating harder properties using LLMs:** Many interesting properties are
    hard to evaluate exactly, but can be evaluated with very high accuracy by an LLM.
    It is often easier to evaluate a property of the output than to produce an output
    that matches a set of properties.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: To illustrate this, we write a couple of simple prompts that turn a question
    into a YES-NO classification problem, and then use ChatGPT to evaluate the properties
    (again, we’re not trying to optimize these prompts). Here is one of our prompts
    (the other one is similar, but takes a pair of texts as input). Note that we ask
    for an explanation when the answer is not as expected.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Let’s ask our email assistant to make a few emails more concise, and then use
    this prompt to evaluate the relevant property.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: '**— —'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: INSTRUCTION:** Make it more concise
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: '**TEXT:** Hey Marco,'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: Can you please schedule a meeting for next week?
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 你能请安排下周的会议吗？
- en: We really need to discuss what’s happening with guidance!
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 我们真的需要讨论一下指导的情况！
- en: Thanks,
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 谢谢，
- en: Scott
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 斯科特
- en: '**OUTPUT:** Hi Marco, can we schedule a meeting next week to discuss guidance?
    Thanks, Scott.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '**输出：** 嗨，马尔科，我们可以安排下周开会讨论指导吗？谢谢，斯科特。'
- en: '**TEXT:** Hey Scott,'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '**文本：** 嘿，斯科特，'
- en: I’m sorry man, but you’ll have to do that guidance demo without me… I’m going
    rock climbing with our children tomorrow.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 对不起，兄弟，不过你得自己做那个指导演示了……我明天要去和孩子们攀岩。
- en: Cheers,
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 干杯，
- en: Marco
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 马尔科
- en: '**OUTPUT:** Hey Scott, I can’t do the guidance demo tomorrow. I’m going rock
    climbing with the kids. Cheers, Marco.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '**输出：** 嘿，斯科特，我明天不能做指导演示。我会和孩子们去攀岩。干杯，马尔科。'
- en: — —
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: — —
- en: If we run our property evaluator on these input-output pairs with the question
    `Do the texts have the same meaning?`, it (correctly) judges that both outputs
    have the same meaning as the original emails.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们对这些输入-输出对运行属性评估器，问题是‘文本是否具有相同的意义？’，它（正确地）判断两个输出与原始邮件具有相同的意义。
- en: 'We then change the outputs slightly so as to change the meaning, to see if
    our evaluator identifies the shift and provides good explanations. It does in
    both cases. Here is one of them, with the resulting property evaluation:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 我们然后稍微改变输出，以便改变其意义，看看我们的评估器是否能识别这种变化并提供好的解释。它在这两种情况下都能做到。这是其中之一，以及相应的属性评估：
- en: '![](../Images/f854465a39d1027bf0f50291de0ca206.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f854465a39d1027bf0f50291de0ca206.png)'
- en: '**Property evaluators should be high precision** If we’re using an LLM to evaluate
    a property, we need the LLM to be right when it claims a property is violated
    (high precision). Tests are never exhaustive, and thus a false positive is worse
    than a false negative when testing. If the LLM misses a few violations, it just
    means our test won’t be as exhaustive as it could be. However, if it claims a
    violation when there isn’t one, we won’t be able to trust the test when it matters
    most (when it fails).'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '**属性评估器应该具有高精度**：如果我们使用 LLM 来评估一个属性，我们需要 LLM 在声称属性被违反时是准确的（高精度）。测试永远不会是详尽的，因此测试中的假阳性比假阴性更糟。如果
    LLM 漏掉了一些违反情况，只是意味着我们的测试不会像可能那样详尽。然而，如果它在没有违反的情况下声称存在违反，当测试最重要时（当它失败时），我们将无法信任测试。'
- en: We show a quick example of low precision in [this gist](https://gist.github.com/marcotcr/9ab4ba0f54d9a87f577adf6c36715b92),
    where GPT-4 is used to compare between the outputs of two models solving quadratic
    equations (you can think of this as evaluating the property `model 1 is better
    than model 2`), and GPT-4 cannot reliably select the right model even for an example
    where it can solve the equation correctly. This means that this particular prompt
    would be a bad candidate for testing this property.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在[这个 gist](https://gist.github.com/marcotcr/9ab4ba0f54d9a87f577adf6c36715b92)中展示了一个低精度的快速示例，其中使用了
    GPT-4 来比较两个模型解决二次方程的输出（你可以把这看作是评估属性 `模型 1 比模型 2 更好`），而 GPT-4 甚至在一个它能够正确解决方程的示例中也无法可靠地选择正确的模型。这意味着这个特定的提示将是不适合测试这个属性的。
- en: '**Perception is easier than generation** While it seems reasonable to check
    the output of GPT 3.5 with a *stronger* model (GPT-4), does it make sense to use
    an LLM to judge *its own* output? If it can’t produce an output according to instructions,
    can we reasonably hope it evaluates such properties with high accuracy? While
    it may seem counterintuitive at first, the answer is yes, because perception is
    often easier than generation. Consider the following (non-exhaustive) reasons:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '**感知比生成更容易**：虽然使用更*强大的*模型（GPT-4）来检查 GPT 3.5 的输出似乎很合理，但使用 LLM 来判断*它自己的*输出是否有意义吗？如果它不能按照指令生成输出，我们可以合理地期望它高精度地评估这些属性吗？虽然这起初可能看起来有些违反直觉，但答案是肯定的，因为感知通常比生成更容易。考虑以下（非详尽的）原因：'
- en: '*Generation requires planning*: even if the property we’re evaluating is ‘did
    the model follow the instruction’, evaluating an existing text requires no ‘planning’,
    while generation requires the LLM to produce text that follows the instruction
    step by step (and thus it requires it to somehow ‘plan’ the steps that will lead
    to a right solution from the beginning, or to be able to correct itself if it
    goes down the wrong path *without changing the partial output it already generated*).'
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*生成需要规划*：即使我们评估的属性是‘模型是否遵循指令’，评估现有文本不需要‘规划’，而生成则要求 LLM 逐步产生遵循指令的文本（因此它需要以某种方式‘规划’从一开始就导致正确解决方案的步骤，或者能够在走错路时纠正自己*而不改变已经生成的部分输出*）。'
- en: '*We can perceive one property at a time, but must generate all at once*: many
    instructions require the LLM to balance multiple properties at once, e.g. `make
    it more concise` requires the LLM to balance the property `output is shorter`
    with the property `output contains all the important information` (implicit in
    the instruction). While balancing these may be hard, evaluating them one at a
    time is much easier.'
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*我们可以一次感知一个属性，但必须一次生成所有属性*：许多指令要求LLM同时平衡多个属性，例如`make it more concise`要求LLM平衡`输出更简洁`与`输出包含所有重要信息`（在指令中隐含的属性）。虽然平衡这些可能很困难，但一次评估一个属性要容易得多。'
- en: 'Here is a quick toy example, where ChatGPT can evaluate a property but not
    generate an output that satisfies it:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是一个快速的玩具示例，其中ChatGPT可以评估一个属性但不能生成满足该属性的输出：
- en: '![](../Images/fc5c31dfd89e4b5c0a9cfbb2d4d4ba6f.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fc5c31dfd89e4b5c0a9cfbb2d4d4ba6f.png)'
- en: '‘Unfortunately’ and ‘perhaps’ are adverbs, but ‘Great’ is not. Our property
    evaluator with the question `Does the text start with an adverb?` answers correctly
    on all four examples, flagging only the failure case:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: ‘Unfortunately’和‘perhaps’是副词，但‘Great’不是。我们的属性评估器对于问题`文本是否以副词开头？`在所有四个示例中都回答正确，仅标记了失败的情况：
- en: '![](../Images/d852409caadb4afb21243a485394cc6c.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d852409caadb4afb21243a485394cc6c.png)'
- en: '**Summary**: test properties, use LLM to evaluate them if you can get high
    precision.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '**总结**：测试属性，如果可以得到高精度，则使用LLM进行评估。'
- en: What to test
  id: totrans-59
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 需要测试的内容
- en: Is this section superfluous? Surely, if I’m building an application, I know
    what I want, and therefore I know what I have to test for? Unfortunately, we have
    **never** encountered a situation where this is the case. Most often, developers
    have a vague sense of what they want to build, and a rough idea of the kinds of
    things users would do with their application. Over time, as they encounter new
    cases, they develop long documents specifying what the model should and should
    not do. The *best* developers try to anticipate this as much as possible, but
    it is very hard to do it well, even when you have pilots and early users. Having
    said this, there are big benefits to doing this thinking early. Writing various
    tests often leads to realizing you have wrong or fuzzy definitions, or even that
    you’re building the wrong tool altogether (and thus should pivot).
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 这一部分是否多余？如果我在构建应用程序，我肯定知道我想要什么，因此我知道我需要测试什么吗？不幸的是，我们**从未**遇到过这种情况。大多数情况下，开发人员对自己想构建的东西有一个模糊的概念，并对用户会如何使用他们的应用程序有一个粗略的想法。随着时间的推移，随着遇到新情况，他们会编写长文档来指定模型应该做什么和不应该做什么。*最优秀*的开发人员尽力尽可能提前预见这些，但即使有试点和早期用户，也很难做到这一点。尽管如此，尽早进行这些思考是有很大好处的。编写各种测试通常会导致你意识到你有错误或模糊的定义，甚至你可能在构建错误的工具（因此应该调整方向）。
- en: Thinking carefully about tests means you understand your own tool better, and
    also that you catch bugs early. Here is a rough outline of a testing process,
    which includes figuring out what to test and actually testing it.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 认真考虑测试意味着你更好地理解自己的工具，也能早早发现错误。以下是测试过程的大致轮廓，包括确定要测试的内容和实际进行测试。
- en: Enumerate use cases for your application.
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 列举你应用的用例。
- en: For each use case, try to think of high-level behaviors and properties you can
    test. Write concrete test cases.
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于每个用例，尝试考虑可以测试的高级行为和属性。编写具体的测试用例。
- en: Once you find bugs, drill down and expand them as much as possible (so you can
    understand and fix them).
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦发现错误，深入挖掘并尽可能扩展它们（这样你可以理解并修复它们）。
- en: '**A historical note**: [CheckList](https://homes.cs.washington.edu/~marcotcr/acl20_checklist.pdf)
    assumed use cases were a given, and proposed a set of linguistic capabilities
    (e.g. vocabulary, negation, etc) to help users think about behaviors, properties,
    and testcases (step 2). In hindsight, this was a terrible assumption (as noted
    above, we most often don’t know what use cases to expect ahead of time).'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '**历史备注**：[CheckList](https://homes.cs.washington.edu/~marcotcr/acl20_checklist.pdf)假设用例是已知的，并提出了一套语言能力（例如词汇、否定等）来帮助用户思考行为、属性和测试用例（第2步）。事后看来，这是一个糟糕的假设（如上所述，我们通常无法预先知道用例）。'
- en: If CheckList focused on step 2, [AdaTest](https://aclanthology.org/2022.acl-long.230.pdf)
    focused mostly on step 3, where we showed that GPT-3 with a human in the loop
    was an *amazing* tool for finding and expanding bugs in models. This was a good
    idea, which we now expand by getting the LLM to also help in steps 1 and 2.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 如果CheckList专注于第2步，[AdaTest](https://aclanthology.org/2022.acl-long.230.pdf)主要集中在第3步，我们展示了GPT-3与人工协同工作的*惊人*工具，能够发现并扩展模型中的错误。这是一个好主意，我们现在通过让LLM也帮助完成第1步和第2步来扩展这一点。
- en: '**Recall vs precision**: In contrast to property evaluators (where we want
    high precision), when thinking about *“what to test”* we are interested in *recall*
    (i.e. we want to discover as many use cases, behaviors, tests, etc as possible).
    Since we have a human in the loop in this part of the process, the human can simply
    disregard any LLM suggestions that are not useful (i.e. we don’t need high precision).
    We usually set a higher temperature when using the LLM in this phase.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '**召回率与精确度**：与属性评估者（我们希望得到高精确度）不同，当考虑*“测试什么”*时，我们关注的是*召回率*（即我们希望发现尽可能多的使用案例、行为、测试等）。由于在这个过程中有人工参与，人们可以简单地忽略任何不有用的LLM建议（即我们不需要高精确度）。我们通常在这一阶段使用LLM时设定更高的温度。'
- en: 'The testing process: an example'
  id: totrans-68
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 测试过程：一个示例
- en: 1\. Enumerate use cases
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1\. 列举使用案例
- en: 'Our goal here is to think about the kinds of things users will do with our
    application. This includes both their goals (what they’re trying to do) and the
    kinds of inputs our system may be exposed to. Let’s see if ChatGPT helps us enumerate
    some use cases:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里的目标是考虑用户会如何使用我们的应用程序。这包括他们的目标（他们想做什么）以及我们的系统可能接触到的各种输入。让我们看看ChatGPT是否能帮助我们列举一些使用案例：
- en: '![](../Images/4068674d525983494d0f3c9b1bf86432.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4068674d525983494d0f3c9b1bf86432.png)'
- en: (Output truncated for space reasons)
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: （输出因空间限制被截断）
- en: 'We run the prompt above with `n=3`, getting ChatGPT to list 15 potential use
    cases. Some are pretty good, others are more contrived. We then tell ChatGPT that
    we got all of these use cases from elsewhere, and get it to organize them into
    categories. Here are a few categories it lists (full list in the [notebook](https://github.com/microsoft/guidance/blob/main/notebooks/testing_lms.ipynb)
    accompanying this post):'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 我们用`n=3`运行上述提示，要求ChatGPT列出15个潜在的使用案例。有些相当不错，有些则较为牵强。然后，我们告诉ChatGPT这些使用案例都来自其他地方，并让它将这些案例组织成类别。以下是它列出的一些类别（完整列表请见[笔记本](https://github.com/microsoft/guidance/blob/main/notebooks/testing_lms.ipynb)）：
- en: '[PRE3]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'We don’t want to just take ChatGPT’s summary wholesale, so we reorganize the
    categories it lists and add a couple of ideas of our own (again, [here](https://github.com/microsoft/guidance/blob/main/notebooks/testing_lms.ipynb)).
    Then, we ask ChatGPT to iterate on our work. This is actually a very good pattern:
    **we use the LLM to generate ideas, select and tweak the best ideas, and then
    ask the LLM to generate more ideas based on our selection.**'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不想直接采纳ChatGPT的总结，因此我们重新组织它列出的类别，并添加一些自己的想法（再次参考，[这里](https://github.com/microsoft/guidance/blob/main/notebooks/testing_lms.ipynb)）。然后，我们要求ChatGPT对我们的工作进行迭代。这实际上是一个非常好的模式：**我们使用LLM来生成想法，选择并调整最佳想法，然后要求LLM基于我们的选择生成更多的想法。**
- en: '![](../Images/23e28989562209b607e907223a67e7b8.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/23e28989562209b607e907223a67e7b8.png)'
- en: ChatGPT suggested ‘generating ideas for how to respond to the email’ as a use
    case, which ironically we hadn’t considered (even though we had already listed
    6 broad use cases and were literally using ChagGPT to generate ideas).
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: ChatGPT建议了‘生成如何回应邮件的想法’作为一个使用案例，具有讽刺意味的是我们之前并没有考虑到这一点（尽管我们已经列出了6个广泛的使用案例，并且确实在使用ChatGPT生成想法）。
- en: '**Generating data** We need some concrete data (in our case, emails) to test
    our model on.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '**生成数据** 我们需要一些具体的数据（在我们的案例中是邮件）来测试我们的模型。'
- en: 'We start by simply asking ChatGPT to generate various kinds of emails:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先简单地要求ChatGPT生成各种类型的邮件：
- en: '![](../Images/8c984824593d2c74938ce2f01bde5464.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8c984824593d2c74938ce2f01bde5464.png)'
- en: (Output truncated for space reasons)
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: （输出因空间限制被截断）
- en: ChatGPT writes mostly short emails, but it does cover a variety of situations.
    In addition to changing the prompt above for more diversity, we can also use existing
    datasets. As an example ([see notebook](https://github.com/microsoft/guidance/blob/main/notebooks/testing_lms.ipynb)),
    we load a dataset of Enron emails and take a small subset, such that we have an
    initial set of 60 input emails to work with (30 from ChatGPT and 30 from Enron).
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: ChatGPT主要编写简短的电子邮件，但它涵盖了各种情况。除了改变上述提示以获得更多样化外，我们还可以使用现有的数据集。例如，[见笔记本](https://github.com/microsoft/guidance/blob/main/notebooks/testing_lms.ipynb)，我们加载了一个Enron电子邮件数据集，并取了一个小子集，从而获得了60封输入电子邮件的初始集合（30封来自ChatGPT，30封来自Enron）。
- en: Now that we have a list of use cases and some data to explore them, we can move
    to the next step.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了一些用例和数据来探索它们，我们可以进入下一步。
- en: 2\. Think of behaviors and properties, write tests.
  id: totrans-84
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2\. 想出行为和属性，编写测试。
- en: It is possible (and very useful) to use the same ideation process as above for
    this step (i.e. ask the LLM to generate ideas, select and tweak the best ones,
    and then ask the LLM to generate more ideas based on our selection). However,
    for space reasons we pick a few use cases that are straightforward to test, and
    test just the most basic properties. While one might want to test some use cases
    more exhaustively (e.g. even using CheckList capabilities as in [here](https://gist.github.com/marcotcr/a897fad16f40619af0be693c32f42eda)),
    we’ll only scratch the surface below.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这一步（即要求LLM生成想法，选择和调整最佳想法，然后要求LLM根据我们的选择生成更多想法），使用上述相同的构思过程是可能的（且非常有用）的。然而，由于空间原因，我们选择了一些易于测试的用例，并仅测试最基本的属性。虽然有人可能想要更彻底地测试一些用例（例如，使用CheckList能力，如[这里](https://gist.github.com/marcotcr/a897fad16f40619af0be693c32f42eda)所示），但我们下面只会略作探讨。
- en: '**Use case: responding to an email** We ask our tool to `write a response that
    politely says no` to our 60 input emails. Then, we verify it with the question
    `Is the response a polite way of saying no to the email?`. Notice we could have
    broken the question down into two separate properties if precision was low.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '**用例：回应电子邮件** 我们要求工具`写一个礼貌地说不`的回应给我们的60封输入电子邮件。然后，我们通过问题`回应是否以礼貌的方式拒绝了电子邮件？`来验证它。注意，如果精确度较低，我们本可以将问题分解为两个单独的属性。'
- en: 'Surprisingly, the tool fails 53.3% of the time on this simple instruction.
    Upon inspection, most of the failures have to do with ChatGPT not writing a response
    at all, e.g.:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 令人惊讶的是，这个简单指令的失败率高达53.3%。经过检查，大多数失败与ChatGPT根本没有写回应有关，例如：
- en: '![](../Images/7bcbe52c71ee230703aaa34a6879a5a7.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7bcbe52c71ee230703aaa34a6879a5a7.png)'
- en: While not directly related to its skills in writing full responses, it’s good
    that the test caught this particular failure mode (which we could correct via
    better prompting). It often happens that trying to test a capability reveals a
    problem elsewhere.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这与写完整回应的能力没有直接关系，但测试捕捉到这种特定的失败模式（我们可以通过更好的提示来纠正）是好的。尝试测试某项能力时，往往会暴露出其他地方的问题。
- en: '**Use case: Make a draft more concise** We ask our tool to `Shorten the email
    by removing everything that is unecessary. Make sure not to lose any important
    information`. We then evaluate two properties: (1) whether the text is shorter
    (measured directly by string length), and (2) whether the shortened version loses
    information, through the question `Does the shortened version communicate all
    of the important information in the original email?`'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '**用例：使草稿更简洁** 我们要求工具`通过删除所有不必要的内容来缩短电子邮件。确保不要丢失任何重要信息`。然后我们评估两个属性：（1）文本是否更短（直接通过字符串长度测量），（2）缩短版本是否丢失信息，通过问题`缩短版本是否传达了原始电子邮件中的所有重要信息？`'
- en: 'The first property is almost always met, while the second property has a low
    failure rate of 8.3%, with failures like the following:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个属性几乎总是满足，而第二个属性的失败率较低，为8.3%，失败情况如下：
- en: '![](../Images/24f22402e49b05cb85978343941eedfa.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/24f22402e49b05cb85978343941eedfa.png)'
- en: '**Use case: Extracting action points from an incoming email.** We ask our tool
    to take a received email, and`Extract any action items that I may need to put
    in my TODO list`. Rather than using our existing input emails, we’ll illustrate
    a technique we haven’t talked about yet: **generating inputs that are guaranteed
    to meet a certain property.**'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '**用例：从收到的电子邮件中提取行动点。** 我们要求工具处理收到的电子邮件，并`提取我可能需要放入TODO列表中的任何行动项`。为了说明一种我们尚未讨论过的技术，我们将介绍**生成保证满足特定属性的输入**的方法。'
- en: 'For this use case, we can generate emails with *known* action points, and then
    check if the tool can extract *at least those*. To do so, we take the action item
    `Don''t forget to water the plants` and ask ChatGPT to paraphrase it 10 times.
    We then ask it to generate emails containing one of those paraphrases, like the
    one below:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个用例，我们可以生成带有*已知*行动点的邮件，然后检查工具是否能提取*至少这些*行动点。为此，我们取行动项`不要忘记浇水植物`，并要求ChatGPT对其进行10次意译。然后，我们要求它生成包含这些意译之一的邮件，如下所示：
- en: '![](../Images/048aa0b8f5003aad7ebd8eff4c4dfa84.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/048aa0b8f5003aad7ebd8eff4c4dfa84.png)'
- en: These emails may have additional action items that are not related to watering
    the plants. However, this does not matter at all, as the property we’re going
    to check is whether the tool extracts `watering the plants` as *one* of the action
    items, not whether it is the only one. In other words, our question for the output
    will be `Does it talk about watering the plants?`
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 这些邮件可能包含与浇水植物无关的其他行动项。然而，这一点并不重要，因为我们要检查的属性是工具是否将`浇水植物`提取为*一个*行动项，而不是它是否是唯一的行动项。换句话说，我们对输出的提问将是`是否谈到了浇水植物？`
- en: Our email assistant prompt fails on 4/10 generated emails, saying that “There
    are no action items in the highlighted text”, even though (by design) we know
    there is at least one action point in there. This is a high failure rate for such
    simple examples. Of course, if we were testing for real we would have a variety
    of embedded action items (rather than just this one example), and we would also
    check for other properties (e.g. whether the tool extracts *all* action items,
    whether it extracts *only* action items, etc). However, we’ll now switch gears
    and see an example of metamorphic testing**.**
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的电子邮件助手提示在10封生成的邮件中有4封失败，表示“高亮文本中没有行动项”，尽管（按设计）我们知道其中至少有一个行动点。这对于如此简单的示例来说，失败率很高。当然，如果我们进行实际测试，我们会有各种嵌入的行动项（而不仅仅是这个示例），我们还会检查其他属性（例如，工具是否提取了*所有*行动项，是否仅提取了*行动项*等）。然而，我们现在将转到另一个示例来看变形测试**。**
- en: '**Metamorphic testing: robustness to instruction paraphrasing** Sticking with
    this use case (extracting action items), we go back to our original 60 input emails.
    We will test the tool’s robustness, by paraphrasing the instruction and verifying
    if the output list has the same action items. Note that *we are not testing whether
    the output is correct*, but whether the model is consistent in light of paraphrased
    instructions (which in itself is an important property).'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '**变形测试：对指令意译的鲁棒性** 继续使用此用例（提取行动项），我们回到最初的60封输入邮件。我们将通过意译指令并验证输出列表是否包含相同的行动项来测试工具的鲁棒性。请注意，*我们并不是在测试输出是否正确*，而是模型在面对意译指令时的一致性（这本身就是一个重要的属性）。'
- en: 'For presentation reasons we only paraphrase the original instruction once (in
    practice we would have many paraphrases of different instructions):'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 为了演示目的，我们仅对原始指令进行一次意译（在实际中，我们会有许多不同指令的意译）：
- en: '`Orig: Extract any action items that I may need to put in my TODO list Paraphrase:
    List any action items in the email that I may want to put in a TODO list`.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '`原文：提取我可能需要放入TODO列表中的任何行动项 意译：列出电子邮件中我可能想放入TODO列表中的任何行动项`。'
- en: 'We then verify the property of whether the outputs of these different instructions
    have the same meaning (they should if they have the same bullet points). The failure
    rate is 16.7%, with failures like the following:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们验证这些不同指令的输出是否具有相同的意义（如果它们具有相同的项目符号，应该是这样）。失败率为16.7%，失败示例如下：
- en: '![](../Images/62622a3f398a5f6d49ff8e53f135625a.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/62622a3f398a5f6d49ff8e53f135625a.png)'
- en: Again, our evaluator seems to be working fine on the examples we have. Unfortunately,
    the model has a reasonably high failure rate on this robustness test, extracting
    different action items when we paraphrase the instruction.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，我们的评估工具在我们提供的示例上似乎运行良好。不幸的是，该模型在此鲁棒性测试中有相当高的失败率，当我们对指令进行意译时，会提取出不同的行动项。
- en: 3\. Drill down on discovered bugs
  id: totrans-104
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3\. 深入挖掘发现的错误
- en: 'Let’s go back to our example of making a draft more concise, where we had a
    low error rate (8.3%). We can often find error patterns if we drill down into
    these errors. Here is a very simple prompt to do this, which is a very quick-and-dirty
    emulation of [AdaTest](https://aclanthology.org/2022.acl-long.230.pdf), where
    we optimized the prompt / UI way more (we’re just trying to illustrate the principle
    here):'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回到使草稿更简洁的例子，我们在这里有一个较低的错误率（8.3%）。如果我们深入这些错误，往往可以找到错误模式。这里是一个非常简单的提示，用于实现这一点，这是对[AdaTest](https://aclanthology.org/2022.acl-long.230.pdf)的快速简化模拟，我们对提示/界面进行了更多优化（我们只是试图说明这个原则）：
- en: '[PRE4]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'We run this prompt with the few discovered failures:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用了这个提示，并发现了一些失败的情况：
- en: '![](../Images/5536ead256b00fbb4d4f96a47d675e72.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5536ead256b00fbb4d4f96a47d675e72.png)'
- en: (Output truncated for space reasons)
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: （由于空间原因，输出被截断）
- en: ChatGPT provided a hypothesis for what ties those emails together. Whether that
    hypothesis is right or wrong, we can see how the model does on the new examples
    it generates. Indeed, the failure rate on the same property (`Does the shortened
    version communicate all of the important information in the original email?`)
    is now much higher (23.5%), with similar failures as before.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: ChatGPT 提出了一个假设，解释这些邮件之间的关联。无论这个假设是对是错，我们可以观察模型在生成的新示例中的表现。确实，在相同属性（`缩短版是否传达了原始邮件中的所有重要信息？`）上的失败率现在大大提高了（23.5%），且失败情况与之前相似。
- en: 'It does seem like ChatGPT latched on to kind of a pattern. While we don’t have
    enough data yet to know whether it is a real pattern or not, this illustrates
    the drill-down strategy: **take failures and get a LLM to ‘generate more’**. We
    are very confident that this strategy works, because we have tried it in *a lot*
    of different scenarios, models, and applications (with AdaTest). In real testing,
    we would keep iterating on this process until we found real patterns, would go
    back to the model (or in this case, the prompt) to fix the bugs, and then iterate
    again.'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: ChatGPT 似乎确实抓住了一种模式。虽然我们还没有足够的数据来确定这是否是真正的模式，但这说明了深入挖掘的策略：**接受失败并让LLM‘生成更多’**。我们非常确信这个策略有效，因为我们在*许多*不同的场景、模型和应用（使用AdaTest）中尝试过。在实际测试中，我们会不断迭代这个过程，直到找到真正的模式，再回到模型（或在这种情况下，提示）修复错误，然后再进行迭代。
- en: But now it’s time to wrap up this blog post :)
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 但现在是时候结束这篇博客文章了 :)
- en: Conclusion
  id: totrans-113
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: 'Here is a TL;DR of this whole post (not written by ChatGPT, we promise):'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 这是这篇文章的简要总结（不是由ChatGPT编写的，我们保证）：
- en: '**What we’re saying:** We think it’s a good idea to test LLMs just like we
    test software. Testing does not replace benchmarks, but complements them.'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**我们的观点：** 我们认为像测试软件一样测试LLM是个好主意。测试并不能替代基准测试，但可以补充它们。'
- en: '**How to test:** If you can’t specify a single right answer, and / or you don’t
    have a labeled dataset, specify **properties** of the output or of groups of outputs.
    You can often use the LLM itself to evaluate such properties with high accuracy,
    since *perception is easier than generation*.'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**如何测试：** 如果你不能指定一个正确的答案，和/或你没有标记数据集，请指定输出或输出组的**属性**。你可以使用LLM自身来高精度地评估这些属性，因为*感知比生成更容易*。'
- en: '**What to test:** Get the LLM to help you figure it out. Generate potential
    use cases and potential inputs, and then think of properties you can test. If
    you find bugs, get the LLM to drill down on them to find patterns you can later
    fix.'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**测试什么：** 让LLM帮助你找出答案。生成潜在的用例和输入，然后考虑你可以测试的属性。如果发现错误，让LLM深入分析这些错误，以找出可以后来修复的模式。'
- en: Now, it’s obvious that the process is much less linear and straightforward than
    what we described it here — it is not uncommon that testing a property leads to
    discovering a new use case you hadn’t thought about, and maybe even makes you
    realize you have to redesign your tool in the first place. However, having a stylized
    process is still helpful, and the kinds of techniques we describe here are very
    useful in practice.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，很明显，这个过程远没有我们描述的那样线性和直接——测试一个属性通常会导致发现你未曾想到的新用例，甚至让你意识到你需要重新设计工具。然而，拥有一个风格化的过程仍然是有帮助的，我们在这里描述的技术在实践中非常有用。
- en: '**Is it too much work?** Testing is certainly a laborious process (although
    using LLMs like we did above makes it *much easier*), but consider the alternatives.
    It is really hard to to benchmark generation tasks with multiple right answers.
    and thus we often don’t trust the benchmarks for these tasks. Collecting human
    judgments on the existing model’s output can be even *more* laborious, and does
    not transfer well when you iterate on the model (suddenly your labels are not
    as useful anymore). *Not testing* usually means you don’t really know how your
    model behaves, which is a recipe for disaster. Testing, on the other hand, often
    leads to (1) finding bugs, (2) insight on the task itself, (3) discovering severe
    problems in the specification *early*, which allows for pivoting before its too
    late. On balance, we think *testing is time well spent*.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '**这是否太费劲了？** 测试确实是一个费力的过程（虽然像我们上述使用LLMs的方法使其*轻松许多*），但考虑到其他选择。用多种正确答案来基准生成任务真的很困难，因此我们常常不相信这些任务的基准。收集对现有模型输出的人类判断甚至会更*费力*，并且在模型迭代时转移效果不好（突然之间你的标签不再那么有用）。*不进行测试*通常意味着你不真正知道模型的表现，这是一种灾难的先兆。另一方面，测试通常会导致（1）发现错误，（2）对任务本身的洞察，（3）*及早*发现规范中的严重问题，这样可以在为时已晚之前进行调整。总的来说，我们认为*测试是值得花费时间*的。'
- en: — — — — — — — — — -
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: — — — — — — — — — -
- en: '[Here is a link](https://github.com/microsoft/guidance/blob/main/notebooks/testing_lms.ipynb)
    to the jupyter notebook with code for all the examples above (and more). This
    post was written jointly by Marco Tulio Ribeiro and [Scott Lundberg](https://medium.com/@scottmlundberg)'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '[这里是一个链接](https://github.com/microsoft/guidance/blob/main/notebooks/testing_lms.ipynb)，包含了所有上述示例的代码（还有更多）。这篇文章由Marco
    Tulio Ribeiro和[Scott Lundberg](https://medium.com/@scottmlundberg)共同撰写。'
