- en: Document Topic Extraction with Large Language Models (LLM) and the Latent Dirichlet
    Allocation (LDA) Algorithm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/document-topic-extraction-with-large-language-models-llm-and-the-latent-dirichlet-allocation-e4697e4dae87?source=collection_archive---------0-----------------------#2023-09-13](https://towardsdatascience.com/document-topic-extraction-with-large-language-models-llm-and-the-latent-dirichlet-allocation-e4697e4dae87?source=collection_archive---------0-----------------------#2023-09-13)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: A guide on how to efficiently extract topics from large documents using Large
    Language Models (LLM) and the Latent Dirichlet Allocation (LDA) algorithm.
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@antoniojimzc?source=post_page-----e4697e4dae87--------------------------------)[![Antonio
    Jimenez Caballero](../Images/978c40f181cd13666a00584cb18dc383.png)](https://medium.com/@antoniojimzc?source=post_page-----e4697e4dae87--------------------------------)[](https://towardsdatascience.com/?source=post_page-----e4697e4dae87--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----e4697e4dae87--------------------------------)
    [Antonio Jimenez Caballero](https://medium.com/@antoniojimzc?source=post_page-----e4697e4dae87--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F990fab5876ca&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdocument-topic-extraction-with-large-language-models-llm-and-the-latent-dirichlet-allocation-e4697e4dae87&user=Antonio+Jimenez+Caballero&userId=990fab5876ca&source=post_page-990fab5876ca----e4697e4dae87---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----e4697e4dae87--------------------------------)
    ·8 min read·Sep 13, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fe4697e4dae87&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdocument-topic-extraction-with-large-language-models-llm-and-the-latent-dirichlet-allocation-e4697e4dae87&user=Antonio+Jimenez+Caballero&userId=990fab5876ca&source=-----e4697e4dae87---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fe4697e4dae87&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdocument-topic-extraction-with-large-language-models-llm-and-the-latent-dirichlet-allocation-e4697e4dae87&source=-----e4697e4dae87---------------------bookmark_footer-----------)![](../Images/e5eba3311ca460efacf3087a66f791e0.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Photo by [Henry Be](https://unsplash.com/@henry_be?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    on [Unsplash](https://unsplash.com/photos/lc7xcWebECc?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: I was developing a web application for [chatting with PDF files](https://github.com/a-jimenezc/pdf_chat),
    capable of processing large documents, above 1000 pages. But before starting a
    conversation with the document, I wanted the application to give the user a brief
    summary of the main topics, so it would be easier to start the interaction.
  prefs: []
  type: TYPE_NORMAL
- en: One way to do it is by summarizing the document using [LangChain](https://python.langchain.com/docs/get_started/introduction.html),
    as showed in its [**documentation**](https://python.langchain.com/docs/use_cases/summarization).
    The problem, however, is the high computational cost and, by extension, the monetary
    cost. A thousand-page document contains roughly 250 000 words and each word needs
    to be fed into the LLM. Even more, the results must be further processed, as with
    the map-reduce method. A conservative estimate on the cost using gpt-3.5 Turbo
    with 4k context is above 1$ per document, just for the summary. Even when using
    free resources, such as the [**Unofficial HuggingChat API**](https://github.com/Soulter/hugging-chat-api),
    the sheer number of required API calls would be an abuse. So, I needed a different
    approach.
  prefs: []
  type: TYPE_NORMAL
- en: LDA to the Rescue
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The Latent Dirichlet Allocation algorithm was a natural choice for this task.
    This algorithm takes a set of “documents” (in this context, a “document” refers
    to a piece of text) and returns a list of topics for each “document” along with
    a list of words associated with each topic. What is important for our case is
    the list of words associated with each topic. These lists of words encode the
    content of the file, so they can be fed to the LLM to prompt for a summary. I
    recommend [**this article**](/end-to-end-topic-modeling-in-python-latent-dirichlet-allocation-lda-35ce4ed6b3e0)for
    a detailed explanation of the algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two key considerations to address before we could get a high-quality
    result: selecting the hyperparameters for the LDA algorithm and determining the
    format of the output. The most important hyperparameter to consider is the number
    of topics, as it has the most significant on the final result. As for the format
    of the output, one that worked pretty well is the nested bulleted list. In this
    format, each topic is represented as a bulleted list with subentries that further
    describe the topic. As for why this works, I think that, by using this format,
    the model can focus on extracting content from the lists without the complexity
    of articulating paragraphs with connectors and relationships.'
  prefs: []
  type: TYPE_NORMAL
- en: Implementation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: I implemented the code in [**Google Colab**](https://colab.research.google.com/drive/1_dY4EGaQfz0NIFJlsCc981rNXzmJEK8U?usp=sharing).
    The necessary libraries were gensim for LDA, pypdf for PDF processing, nltk for
    word processing, and LangChain for its promt templates and its interface with
    the OpenAI API.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Next, I defined a utility function, *preprocess*, to assist in processing the
    input text. It removes stop words and short tokens.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The second function, *get_topic_lists_from_pdf*, implements the **LDA** portion
    of the code. I accepts the path to the PDF file, the number of topics, and the
    number of words per topic, and it returns a list. Each element in this list contains
    a list of words associate with each topic. Here, we are considering each page
    from the PDF file to be a “document”.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: The next function, *topics_from_pdf*, invokes the LLM model. As stated earlier,
    the model was prompted to format the output as a nested bulleted list.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: In the previous function, the list of words is converted into a string. Then,
    a prompt is created using the *ChatPromptTemplate* object from LangChain; note
    that the prompt defines the structure for the response. Finally, the function
    calls chatgpt-3.5 Turbo model. The return value is the response given by the LLM
    model.
  prefs: []
  type: TYPE_NORMAL
- en: Now, it’s time to call the functions. We first set the API key. ***T***[***his
    article***](https://www.maisieai.com/help/how-to-get-an-openai-api-key-for-chatgpt)offers
    instructions on how to get one.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Next, we call the *topics_from_pdf* function. I choose the values for the number
    of topics and the number of words per topic. Also, I selected a **public domain**
    book, The Metamorphosis by Franz Kafka, for testing. The document is stored in
    my personal drive and downloaded by using the gdown library.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The result is displayed below:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: The output is pretty decent, and it just took seconds! It correctly extracted
    the main ideas from the book.
  prefs: []
  type: TYPE_NORMAL
- en: 'This approach works with technical books as well. For example, *The Foundations
    of Geometry* by David Hilbert (1899) (also in the public domain):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Combining the LDA algorithm with LLM for large document topic extraction produces
    good results while significantly reducing both cost and processing time. We’ve
    gone from hundreds of API calls to just one and from minutes to seconds.
  prefs: []
  type: TYPE_NORMAL
- en: The quality of the output depends greatly on its format. In this case, a nested
    bulleted list worked just fine. Also, the number of topics and the number of words
    per topic are important for the result’s quality. I recommend trying different
    prompts, number of topics, and number of words per topic to find what works best
    for a given document.
  prefs: []
  type: TYPE_NORMAL
- en: The code could be found in [**this link**](https://colab.research.google.com/drive/1_dY4EGaQfz0NIFJlsCc981rNXzmJEK8U?usp=sharing).
  prefs: []
  type: TYPE_NORMAL
- en: Thank you for reading. Please let me know how it resulted with your documents.
  prefs: []
  type: TYPE_NORMAL
- en: 'LinkedIn: [Antonio Jimenez Caballero](https://www.linkedin.com/in/antonio-jimnzc)'
  prefs: []
  type: TYPE_NORMAL
- en: 'GitHub: [a-jimenezc](https://github.com/a-jimenezc)'
  prefs: []
  type: TYPE_NORMAL
