- en: Document Topic Extraction with Large Language Models (LLM) and the Latent Dirichlet
    Allocation (LDA) Algorithm
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用大型语言模型（LLM）和潜在狄利克雷分配（LDA）算法的文档主题提取
- en: 原文：[https://towardsdatascience.com/document-topic-extraction-with-large-language-models-llm-and-the-latent-dirichlet-allocation-e4697e4dae87?source=collection_archive---------0-----------------------#2023-09-13](https://towardsdatascience.com/document-topic-extraction-with-large-language-models-llm-and-the-latent-dirichlet-allocation-e4697e4dae87?source=collection_archive---------0-----------------------#2023-09-13)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/document-topic-extraction-with-large-language-models-llm-and-the-latent-dirichlet-allocation-e4697e4dae87?source=collection_archive---------0-----------------------#2023-09-13](https://towardsdatascience.com/document-topic-extraction-with-large-language-models-llm-and-the-latent-dirichlet-allocation-e4697e4dae87?source=collection_archive---------0-----------------------#2023-09-13)
- en: A guide on how to efficiently extract topics from large documents using Large
    Language Models (LLM) and the Latent Dirichlet Allocation (LDA) algorithm.
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 一份关于如何高效地从大型文档中提取主题的指南，使用大型语言模型（LLM）和潜在狄利克雷分配（LDA）算法。
- en: '[](https://medium.com/@antoniojimzc?source=post_page-----e4697e4dae87--------------------------------)[![Antonio
    Jimenez Caballero](../Images/978c40f181cd13666a00584cb18dc383.png)](https://medium.com/@antoniojimzc?source=post_page-----e4697e4dae87--------------------------------)[](https://towardsdatascience.com/?source=post_page-----e4697e4dae87--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----e4697e4dae87--------------------------------)
    [Antonio Jimenez Caballero](https://medium.com/@antoniojimzc?source=post_page-----e4697e4dae87--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@antoniojimzc?source=post_page-----e4697e4dae87--------------------------------)[![安东尼奥·吉门内斯·卡巴列罗](../Images/978c40f181cd13666a00584cb18dc383.png)](https://medium.com/@antoniojimzc?source=post_page-----e4697e4dae87--------------------------------)[](https://towardsdatascience.com/?source=post_page-----e4697e4dae87--------------------------------)[![数据科学进展](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----e4697e4dae87--------------------------------)
    [安东尼奥·吉门内斯·卡巴列罗](https://medium.com/@antoniojimzc?source=post_page-----e4697e4dae87--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F990fab5876ca&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdocument-topic-extraction-with-large-language-models-llm-and-the-latent-dirichlet-allocation-e4697e4dae87&user=Antonio+Jimenez+Caballero&userId=990fab5876ca&source=post_page-990fab5876ca----e4697e4dae87---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----e4697e4dae87--------------------------------)
    ·8 min read·Sep 13, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fe4697e4dae87&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdocument-topic-extraction-with-large-language-models-llm-and-the-latent-dirichlet-allocation-e4697e4dae87&user=Antonio+Jimenez+Caballero&userId=990fab5876ca&source=-----e4697e4dae87---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F990fab5876ca&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdocument-topic-extraction-with-large-language-models-llm-and-the-latent-dirichlet-allocation-e4697e4dae87&user=Antonio+Jimenez+Caballero&userId=990fab5876ca&source=post_page-990fab5876ca----e4697e4dae87---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----e4697e4dae87--------------------------------)
    · 8 分钟阅读 · 2023年9月13日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fe4697e4dae87&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdocument-topic-extraction-with-large-language-models-llm-and-the-latent-dirichlet-allocation-e4697e4dae87&user=Antonio+Jimenez+Caballero&userId=990fab5876ca&source=-----e4697e4dae87---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fe4697e4dae87&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdocument-topic-extraction-with-large-language-models-llm-and-the-latent-dirichlet-allocation-e4697e4dae87&source=-----e4697e4dae87---------------------bookmark_footer-----------)![](../Images/e5eba3311ca460efacf3087a66f791e0.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fe4697e4dae87&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdocument-topic-extraction-with-large-language-models-llm-and-the-latent-dirichlet-allocation-e4697e4dae87&source=-----e4697e4dae87---------------------bookmark_footer-----------)![](../Images/e5eba3311ca460efacf3087a66f791e0.png)'
- en: Photo by [Henry Be](https://unsplash.com/@henry_be?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    on [Unsplash](https://unsplash.com/photos/lc7xcWebECc?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 照片由 [亨利·贝](https://unsplash.com/@henry_be?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    提供，刊登于 [Unsplash](https://unsplash.com/photos/lc7xcWebECc?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
- en: Introduction
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 介绍
- en: I was developing a web application for [chatting with PDF files](https://github.com/a-jimenezc/pdf_chat),
    capable of processing large documents, above 1000 pages. But before starting a
    conversation with the document, I wanted the application to give the user a brief
    summary of the main topics, so it would be easier to start the interaction.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 我正在为[与PDF文件聊天](https://github.com/a-jimenezc/pdf_chat)开发一个Web应用程序，能够处理超过1000页的大型文档。但在与文档进行对话之前，我希望应用程序能够给用户提供主要主题的简要摘要，这样开始交互会更容易一些。
- en: One way to do it is by summarizing the document using [LangChain](https://python.langchain.com/docs/get_started/introduction.html),
    as showed in its [**documentation**](https://python.langchain.com/docs/use_cases/summarization).
    The problem, however, is the high computational cost and, by extension, the monetary
    cost. A thousand-page document contains roughly 250 000 words and each word needs
    to be fed into the LLM. Even more, the results must be further processed, as with
    the map-reduce method. A conservative estimate on the cost using gpt-3.5 Turbo
    with 4k context is above 1$ per document, just for the summary. Even when using
    free resources, such as the [**Unofficial HuggingChat API**](https://github.com/Soulter/hugging-chat-api),
    the sheer number of required API calls would be an abuse. So, I needed a different
    approach.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 一种方法是使用[LangChain](https://python.langchain.com/docs/get_started/introduction.html)来总结文档，就像在其[**文档**](https://python.langchain.com/docs/use_cases/summarization)中展示的那样。然而，问题在于高计算成本，以及由此带来的金钱成本。一个千页文档包含大约250,000个单词，每个单词都需要输入到LLM中。此外，结果还必须进一步处理，如使用映射-减少方法。使用4k上下文的gpt-3.5
    Turbo进行摘要的保守成本估算超过1美元每份文档。即使使用[**非官方HuggingChat API**](https://github.com/Soulter/hugging-chat-api)等免费资源，所需的API调用次数也会是一种滥用。因此，我需要另一种方法。
- en: LDA to the Rescue
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: LDA来拯救
- en: The Latent Dirichlet Allocation algorithm was a natural choice for this task.
    This algorithm takes a set of “documents” (in this context, a “document” refers
    to a piece of text) and returns a list of topics for each “document” along with
    a list of words associated with each topic. What is important for our case is
    the list of words associated with each topic. These lists of words encode the
    content of the file, so they can be fed to the LLM to prompt for a summary. I
    recommend [**this article**](/end-to-end-topic-modeling-in-python-latent-dirichlet-allocation-lda-35ce4ed6b3e0)for
    a detailed explanation of the algorithm.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 潜在狄利克雷分配算法对于这个任务是一个自然选择。该算法接受一组“文档”（在这个上下文中，“文档”指的是一段文本）并返回每个“文档”的主题列表，以及与每个主题相关联的单词列表。对于我们的情况来说，重要的是与每个主题相关联的单词列表。这些单词列表编码了文件的内容，因此它们可以被输入到LLM中以提示摘要。我推荐[**这篇文章**](/end-to-end-topic-modeling-in-python-latent-dirichlet-allocation-lda-35ce4ed6b3e0)以获取关于该算法的详细解释。
- en: 'There are two key considerations to address before we could get a high-quality
    result: selecting the hyperparameters for the LDA algorithm and determining the
    format of the output. The most important hyperparameter to consider is the number
    of topics, as it has the most significant on the final result. As for the format
    of the output, one that worked pretty well is the nested bulleted list. In this
    format, each topic is represented as a bulleted list with subentries that further
    describe the topic. As for why this works, I think that, by using this format,
    the model can focus on extracting content from the lists without the complexity
    of articulating paragraphs with connectors and relationships.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在能够获得高质量结果之前，我们需要解决两个关键问题：为LDA算法选择超参数和确定输出格式。最重要的超参数是主题数量，因为它对最终结果有最显著的影响。至于输出格式，一个效果不错的选择是嵌套的项目符号列表。在这种格式中，每个主题都表示为一个带有进一步描述该主题的子条目的项目符号列表。关于为什么这样做有效，我认为通过这种格式，模型可以专注于从列表中提取内容，而不必处理用连接词和关系连接起来的段落的复杂性。
- en: Implementation
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实施
- en: I implemented the code in [**Google Colab**](https://colab.research.google.com/drive/1_dY4EGaQfz0NIFJlsCc981rNXzmJEK8U?usp=sharing).
    The necessary libraries were gensim for LDA, pypdf for PDF processing, nltk for
    word processing, and LangChain for its promt templates and its interface with
    the OpenAI API.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 我在[**Google Colab**](https://colab.research.google.com/drive/1_dY4EGaQfz0NIFJlsCc981rNXzmJEK8U?usp=sharing)中实现了代码。所需的库包括用于LDA的gensim，用于PDF处理的pypdf，用于单词处理的nltk，以及用于其模板和与OpenAI
    API接口的LangChain。
- en: '[PRE0]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Next, I defined a utility function, *preprocess*, to assist in processing the
    input text. It removes stop words and short tokens.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我定义了一个实用函数，*preprocess*，来帮助处理输入文本。它会移除停用词和短词。
- en: '[PRE1]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The second function, *get_topic_lists_from_pdf*, implements the **LDA** portion
    of the code. I accepts the path to the PDF file, the number of topics, and the
    number of words per topic, and it returns a list. Each element in this list contains
    a list of words associate with each topic. Here, we are considering each page
    from the PDF file to be a “document”.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个函数，*get_topic_lists_from_pdf*，实现了**LDA**部分的代码。它接受PDF文件的路径、主题数量和每个主题的字数，并返回一个列表。此列表中的每个元素包含与每个主题相关的单词列表。在这里，我们将PDF文件中的每一页视为一个“文档”。
- en: '[PRE2]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The next function, *topics_from_pdf*, invokes the LLM model. As stated earlier,
    the model was prompted to format the output as a nested bulleted list.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个函数，*topics_from_pdf*，调用了LLM模型。如前所述，模型被提示将输出格式化为嵌套的项目符号列表。
- en: '[PRE3]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: In the previous function, the list of words is converted into a string. Then,
    a prompt is created using the *ChatPromptTemplate* object from LangChain; note
    that the prompt defines the structure for the response. Finally, the function
    calls chatgpt-3.5 Turbo model. The return value is the response given by the LLM
    model.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的函数中，单词列表被转换为字符串。然后，使用LangChain中的*ChatPromptTemplate*对象创建提示；请注意，提示定义了响应的结构。最后，函数调用了chatgpt-3.5
    Turbo模型。返回值是LLM模型提供的响应。
- en: Now, it’s time to call the functions. We first set the API key. ***T***[***his
    article***](https://www.maisieai.com/help/how-to-get-an-openai-api-key-for-chatgpt)offers
    instructions on how to get one.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，是时候调用这些函数了。我们首先设置API密钥。***T***[***这篇文章***](https://www.maisieai.com/help/how-to-get-an-openai-api-key-for-chatgpt)提供了获取API密钥的说明。
- en: '[PRE4]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Next, we call the *topics_from_pdf* function. I choose the values for the number
    of topics and the number of words per topic. Also, I selected a **public domain**
    book, The Metamorphosis by Franz Kafka, for testing. The document is stored in
    my personal drive and downloaded by using the gdown library.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们调用了*topics_from_pdf*函数。我选择了主题数量和每个主题的字数的值。此外，我选择了一本**公有领域**的书籍，《变形记》，作者是弗朗茨·卡夫卡，作为测试。文档存储在我的个人驱动器中，并使用gdown库下载。
- en: '[PRE5]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The result is displayed below:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 结果如下所示：
- en: '[PRE6]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The output is pretty decent, and it just took seconds! It correctly extracted
    the main ideas from the book.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 输出效果相当不错，而且仅用了几秒钟！它正确地提取了书中的主要思想。
- en: 'This approach works with technical books as well. For example, *The Foundations
    of Geometry* by David Hilbert (1899) (also in the public domain):'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法也适用于技术书籍。例如，*The Foundations of Geometry*（《几何学基础》），作者是David Hilbert（1899年）（也在公有领域）：
- en: '[PRE7]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Conclusion
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结论
- en: Combining the LDA algorithm with LLM for large document topic extraction produces
    good results while significantly reducing both cost and processing time. We’ve
    gone from hundreds of API calls to just one and from minutes to seconds.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 将LDA算法与LLM结合用于大文档主题提取可以产生良好的结果，同时显著降低了成本和处理时间。我们从数百次API调用减少到仅一次，从几分钟减少到几秒钟。
- en: The quality of the output depends greatly on its format. In this case, a nested
    bulleted list worked just fine. Also, the number of topics and the number of words
    per topic are important for the result’s quality. I recommend trying different
    prompts, number of topics, and number of words per topic to find what works best
    for a given document.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 输出质量很大程度上取决于其格式。在这种情况下，嵌套的项目符号列表效果很好。此外，主题的数量和每个主题的字数对结果的质量也很重要。我建议尝试不同的提示、主题数量和每个主题的字数，以找到最适合给定文档的配置。
- en: The code could be found in [**this link**](https://colab.research.google.com/drive/1_dY4EGaQfz0NIFJlsCc981rNXzmJEK8U?usp=sharing).
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 代码可以在[**这个链接**](https://colab.research.google.com/drive/1_dY4EGaQfz0NIFJlsCc981rNXzmJEK8U?usp=sharing)中找到。
- en: Thank you for reading. Please let me know how it resulted with your documents.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 感谢阅读。请让我知道它在您的文档中的结果如何。
- en: 'LinkedIn: [Antonio Jimenez Caballero](https://www.linkedin.com/in/antonio-jimnzc)'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 'LinkedIn: [Antonio Jimenez Caballero](https://www.linkedin.com/in/antonio-jimnzc)'
- en: 'GitHub: [a-jimenezc](https://github.com/a-jimenezc)'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 'GitHub: [a-jimenezc](https://github.com/a-jimenezc)'
