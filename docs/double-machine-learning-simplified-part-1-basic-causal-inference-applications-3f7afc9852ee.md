# åŒé‡æœºå™¨å­¦ä¹ ç®€åŒ–ç‰ˆï¼šç¬¬ä¸€éƒ¨åˆ† â€” åŸºæœ¬çš„å› æœæ¨æ–­åº”ç”¨

> åŸæ–‡ï¼š[`towardsdatascience.com/double-machine-learning-simplified-part-1-basic-causal-inference-applications-3f7afc9852ee?source=collection_archive---------2-----------------------#2023-07-12`](https://towardsdatascience.com/double-machine-learning-simplified-part-1-basic-causal-inference-applications-3f7afc9852ee?source=collection_archive---------2-----------------------#2023-07-12)

![](img/da07206f53b52134eaecbb622ac56914.png)

æ‰€æœ‰å›¾ç‰‡ç”±ä½œè€…æä¾›

## å­¦ä¹ å¦‚ä½•åœ¨å› æœæ¨æ–­ä»»åŠ¡ä¸­åˆ©ç”¨ DML

[](https://medium.com/@jakepenzak?source=post_page-----3f7afc9852ee--------------------------------)![Jacob Pieniazek](https://medium.com/@jakepenzak?source=post_page-----3f7afc9852ee--------------------------------)[](https://towardsdatascience.com/?source=post_page-----3f7afc9852ee--------------------------------)![Towards Data Science](https://towardsdatascience.com/?source=post_page-----3f7afc9852ee--------------------------------) [Jacob Pieniazek](https://medium.com/@jakepenzak?source=post_page-----3f7afc9852ee--------------------------------)

Â·

[å…³æ³¨](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F6f0948d99b1c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdouble-machine-learning-simplified-part-1-basic-causal-inference-applications-3f7afc9852ee&user=Jacob+Pieniazek&userId=6f0948d99b1c&source=post_page-6f0948d99b1c----3f7afc9852ee---------------------post_header-----------) å‘è¡¨åœ¨ [Towards Data Science](https://towardsdatascience.com/?source=post_page-----3f7afc9852ee--------------------------------) Â·17 åˆ†é’Ÿé˜…è¯»Â·2023 å¹´ 7 æœˆ 12 æ—¥[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F3f7afc9852ee&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdouble-machine-learning-simplified-part-1-basic-causal-inference-applications-3f7afc9852ee&user=Jacob+Pieniazek&userId=6f0948d99b1c&source=-----3f7afc9852ee---------------------clap_footer-----------)

--

[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F3f7afc9852ee&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdouble-machine-learning-simplified-part-1-basic-causal-inference-applications-3f7afc9852ee&source=-----3f7afc9852ee---------------------bookmark_footer-----------)

> æœ¬æ–‡æ˜¯**2 éƒ¨åˆ†**ç³»åˆ—ä¸­çš„**ç¬¬ä¸€éƒ¨åˆ†**ï¼Œæ—¨åœ¨ç®€åŒ–å’Œæ™®åŠåŒé‡æœºå™¨å­¦ä¹ ã€‚åœ¨ç¬¬ä¸€éƒ¨åˆ†ä¸­ï¼Œæˆ‘ä»¬å°†è¦†ç›–åŒé‡æœºå™¨å­¦ä¹ çš„åŸºç¡€çŸ¥è¯†ï¼Œä»¥åŠä¸¤ä¸ªåŸºæœ¬çš„å› æœæ¨æ–­åº”ç”¨ç¤ºä¾‹ï¼ˆä½¿ç”¨ pythonï¼‰ã€‚ç„¶åï¼Œåœ¨[ç¬¬äºŒéƒ¨åˆ†](https://medium.com/towards-data-science/double-machine-learning-simplified-part-2-extensions-the-cate-99926151cac)ä¸­ï¼Œæˆ‘ä»¬å°†æ‰©å±•è¿™äº›çŸ¥è¯†ï¼Œå°†å› æœæ¨æ–­é—®é¢˜è½¬åŒ–ä¸ºé¢„æµ‹ä»»åŠ¡ï¼Œå…¶ä¸­æˆ‘ä»¬é¢„æµ‹ä¸ªä½“çº§åˆ«çš„å¤„ç†æ•ˆæœï¼Œä»¥è¾…åŠ©å†³ç­–å’Œæ•°æ®é©±åŠ¨çš„ç›®æ ‡å®šä½ã€‚

ç»Ÿè®¡/æœºå™¨å­¦ä¹ ï¼ˆMLï¼‰ä¸å› æœæ¨æ–­/è®¡é‡ç»æµå­¦ï¼ˆCIï¼‰ä»»åŠ¡ä¹‹é—´çš„æ¦‚å¿µæ€§å’Œå®è·µæ€§åŒºåˆ«å·²ç»å­˜åœ¨å¤šå¹´â€”â€”ML æ—¨åœ¨é¢„æµ‹ï¼Œè€Œ CI æ—¨åœ¨æ¨æ–­å¤„ç†æ•ˆæœæˆ–å˜é‡ä¹‹é—´çš„â€œå› æœâ€å…³ç³»ã€‚ç„¶è€Œï¼Œæ•°æ®ç§‘å­¦å®¶ä»ç„¶å¸¸å¸¸ä»è®­ç»ƒçš„æœºå™¨å­¦ä¹ æ¨¡å‹æˆ–å…¶ä»–å¯è§£é‡Šçš„ ML æ–¹æ³•ä¸­å¾—å‡ºå› æœç»“è®ºã€‚å°½ç®¡å¦‚æ­¤ï¼Œä¸šç•Œå’Œè®¸å¤šå­¦æœ¯å­¦ç§‘åœ¨æ¨åŠ¨å› æœå£°æ˜çš„ä¸¥è°¨æ€§æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œè¿™ä¹Ÿåˆºæ¿€äº†å› æœæ¨æ–­é¢†åŸŸçš„å¹¿æ³›è®¨è®ºã€‚åœ¨è¿™ä¸€è¿›å±•ä¸­ï¼Œæˆ‘ä»¬çœ‹åˆ°äº†ä¸€äº›æƒŠäººçš„å·¥ä½œï¼Œå¼€å§‹å¼¥åˆ ML ä¸ CI ä¹‹é—´çš„æ¦‚å¿µå·®è·ï¼Œç‰¹åˆ«æ˜¯ CI å·¥å…·åˆ©ç”¨äº† ML æ–¹æ³•çš„å¼ºå¤§èƒ½åŠ›ã€‚

æœ¬ç³»åˆ—çš„ä¸»è¦åŠ¨æœºæ˜¯ä½¿åŒé‡æœºå™¨å­¦ä¹ ï¼ˆDMLï¼‰çš„ä½¿ç”¨å’Œåº”ç”¨å˜å¾—æ™®åŠï¼Œè¿™ä¸€æ–¹æ³•é¦–æ¬¡ç”± Chernozhukov *ç­‰äºº*åœ¨å…¶å¼€åˆ›æ€§è®ºæ–‡ã€Šç”¨äºå¤„ç†å’Œå› æœå‚æ•°çš„åŒé‡æœºå™¨å­¦ä¹ ã€‹ä¸­ä»‹ç»ï¼Œå¹¶ä½¿æ•°æ®ç§‘å­¦å®¶èƒ½å¤Ÿåœ¨æ—¥å¸¸å› æœæ¨æ–­ä»»åŠ¡ä¸­åˆ©ç”¨ DMLã€‚[1] ä¸ºæ­¤ï¼Œæˆ‘ä»¬å°†é¦–å…ˆæ·±å…¥æ¢è®¨ DML çš„åŸºç¡€çŸ¥è¯†ã€‚å…·ä½“è€Œè¨€ï¼Œæˆ‘ä»¬å°†æ¶µç›–ä¸€äº›æ¦‚å¿µ/ç†è®ºåŸºç¡€ï¼ŒåŒ…æ‹¬å› æœå…³ç³»çš„å›å½’æ¡†æ¶åŠ[Frisch-Waugh-Lovell å®šç†](https://en.wikipedia.org/wiki/Frisch%E2%80%93Waugh%E2%80%93Lovell_theorem)ï¼Œç„¶åæˆ‘ä»¬å°†åˆ©ç”¨è¿™ä¸€æ¡†æ¶æ¥å‘å±• DMLã€‚æœ€åï¼Œæˆ‘ä»¬å°†å±•ç¤ºåŒé‡æœºå™¨å­¦ä¹ çš„ä¸¤ä¸ªæ˜¾è‘—åº”ç”¨ï¼š

1.  *åœ¨æˆ‘ä»¬çš„å¤„ç†è¿‡ç¨‹ä¸­è¶‹å‘äºå¤–ç”Ÿæ€§/CIA/å¯å¿½ç•¥æ€§ï¼ˆå°¤å…¶æ˜¯å½“æˆ‘ä»¬çš„åå˜é‡é›†å…·æœ‰é«˜ç»´åº¦æ—¶ï¼‰*

1.  *åœ¨å®éªŒæ•°æ®ï¼ˆéšæœºå¯¹ç…§è¯•éªŒï¼ˆRCTsï¼‰æˆ– A/B æµ‹è¯•ï¼‰ä¸­æé«˜ç²¾ç¡®åº¦å’Œç»Ÿè®¡åŠŸæ•ˆ*

å¦‚æœè¿™äº›å†…å®¹å¯¹ä½ æ¥è¯´ä»æ„Ÿåˆ°éå¸¸é™Œç”Ÿï¼Œæˆ‘æ¨èæŸ¥çœ‹æˆ‘çš„[ä¸Šä¸€ç¯‡æ–‡ç« ](https://medium.com/towards-data-science/controlling-for-x-9cb51652f7ad)ï¼Œå…¶ä¸­æ¶µç›–äº†å› æœå›å½’æ¡†æ¶å’Œ Frisch-Waugh-Lovell å®šç†ã€‚å°½ç®¡å¦‚æ­¤ï¼Œæˆ‘ä¼šåœ¨ä¸‹é¢æ¶µç›–è¿™äº›ä¸»é¢˜ï¼Œå¹¶å°½åŠ›ç®€åŒ–å¹¶ä½¿å…¶å¯¹æ‰€æœ‰äººéƒ½æ˜“äºç†è§£ã€‚è®©æˆ‘ä»¬é¦–å…ˆå¿«é€Ÿæ¦‚è¿°ä¸€ä¸‹è¿™äº›ç†è®ºåŸºç¡€ï¼

## **å› æœå›å½’æ¡†æ¶ä¸ FWL å®šç†**

ç¡®ç«‹å› æœå…³ç³»çš„é‡‘æ ‡å‡†æ˜¯ RCT æˆ– A/B æµ‹è¯•ï¼Œåœ¨è¿™äº›æµ‹è¯•ä¸­ï¼Œæˆ‘ä»¬éšæœºåˆ†é…ä¸€éƒ¨åˆ†ä¸ªä½“æ¥å—å¤„ç†ï¼Œ***T***ï¼ˆæµ‹è¯•ç»„ï¼‰ï¼Œè€Œå…¶ä»–ä¸ªä½“åˆ™ä¸æ¥å—å¤„ç†ï¼ˆå¯¹ç…§ç»„ï¼‰æˆ–æ¥å—ä¸åŒçš„å¤„ç†ï¼ˆåœ¨â€œA/Bâ€æµ‹è¯•ä¸­ï¼‰ã€‚ä¸ºäº†ä¼°è®¡å¤„ç†å¯¹ç»“æœ***y***çš„å¹³å‡å¤„ç†æ•ˆåº”ï¼ˆATEï¼‰ï¼Œæˆ‘ä»¬å¯ä»¥ä¼°è®¡ä»¥ä¸‹åŒå˜é‡çº¿æ€§å›å½’ï¼š

![](img/87b4dd0faf84275195a1089a75aeeeac.png)

(1)

ç”±äºå¤„ç†æ˜¯éšæœºåˆ†é…çš„ï¼Œæˆ‘ä»¬ç¡®ä¿å¤„ç†æ˜¯å¤–ç”Ÿçš„ï¼›å³ï¼Œç‹¬ç«‹äºè¯¯å·®é¡¹Îµï¼Œå› æ­¤ä¸å­˜åœ¨æˆ‘ä»¬æœªæ§åˆ¶çš„æ··æ‚å˜é‡ï¼ˆå½±å“å¤„ç†å’Œç»“æœçš„å˜é‡ï¼‰â€”â€” cov(T,Îµ)=0ï¼ˆä¾‹å¦‚ï¼Œå‡è®¾è¿åæƒ…å†µä¸‹ï¼Œy = æ”¶å…¥ & T = æ•™è‚²å¹´é™ï¼Œé‚£ä¹ˆæˆ‘ä»¬å¯ä»¥é¢„æœŸä¾‹å¦‚ IQ è¿™æ ·çš„å˜é‡ä¼šåœ¨Îµä¸­æ··æ‚çœŸå®å…³ç³»ï¼‰ã€‚ç”±äºè¿™ç§ç‹¬ç«‹æ€§ï¼Œ***T***ä¸Šçš„ç³»æ•°ä¼°è®¡å…·æœ‰å› æœè§£é‡Šâ€”â€”ATEï¼š

![](img/213cbd04b9c2a4f3c5ffa3c312ca5a9d.png)

(2) ç¦»æ•£å¤„ç†

![](img/3e46a068bc011e05cddc440fe236c909.png)

(3) è¿ç»­å¤„ç†

å½“æˆ‘ä»¬å¤„ç†éå®éªŒæˆ–è§‚å¯Ÿæ•°æ®æ—¶ï¼Œå‡ ä¹æ€»æ˜¯å¤„ç†å˜é‡*ä¸æ˜¯*ä¸Îµç‹¬ç«‹çš„ï¼Œæˆ–è€…æ˜¯å†…ç”Ÿçš„â€”â€”cov(T,Îµ)â‰ 0ï¼Œå¹¶ä¸”å­˜åœ¨æˆ‘ä»¬æœªè€ƒè™‘çš„æ··æ‚å˜é‡ã€‚æ¢å¥è¯è¯´ï¼Œæˆ‘ä»¬ä¸èƒ½å†å°†å¤„ç†ä¸­çš„çœŸå®éšæœºå˜åŒ–ä¸ç»“æœè§£é‡Šåˆ†å¼€ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œç®€å•çš„åŒå˜é‡å›å½’å°†å› [é—æ¼å˜é‡åå€š](https://en.wikipedia.org/wiki/Omitted-variable_bias)è€Œå¯¼è‡´ ATE çš„åå€šä¼°è®¡ï¼ˆÎ²ï¼ˆçœŸå® ATEï¼‰+åå€šï¼‰ã€‚ç„¶è€Œï¼Œå¦‚æœæˆ‘ä»¬èƒ½æ§åˆ¶æ‰€æœ‰å¯èƒ½çš„æ··æ‚å˜é‡ï¼Œ***X***ï¼Œä»¥åŠåœ¨ä½¿ç”¨å‚æ•°æ¨¡å‹æ—¶çš„æ··æ‚å‡½æ•°å½¢å¼ï¼Œæˆ‘ä»¬å¯ä»¥åœ¨æˆ‘ä»¬çš„å¤„ç†ä¸Šå®ç°å¤–ç”Ÿæ€§ï¼Œæˆ–è€…ä¹Ÿç§°ä¸ºæ¡ä»¶ç‹¬ç«‹å‡è®¾ï¼ˆCIAï¼‰ï¼Œæˆ–[å¯å¿½ç•¥æ€§](https://en.wikipedia.org/wiki/Ignorability)ã€‚æ¢å¥è¯è¯´ï¼Œå¤„ç†ä¸­çš„å‰©ä½™å˜åŒ–æ˜¯â€œå¥½åƒæ˜¯éšæœºçš„â€ã€‚å³ï¼Œåœ¨è¯¯å·®é¡¹ä¸­æ²¡æœ‰å‰©ä½™çš„æ··æ‚å› ç´ ï¼Œæˆ–è€…ï¼š

![](img/3a1755f6c8ffa8a77aa93b329115582c.png)

(4)

å¦‚æœå¤–ç”Ÿæ€§æˆç«‹ï¼ˆ***X***ä¹‹å¤–æ²¡æœ‰æ··æ‚å˜é‡ï¼‰ï¼Œé‚£ä¹ˆåœ¨å¤šå…ƒå›å½’ä¸­æ§åˆ¶***X***å…è®¸***T***ä¸Šçš„ç³»æ•°ä¼°è®¡å…·æœ‰ç±»ä¼¼çš„ ATE å› æœè§£é‡Šï¼š

![](img/477994ca9c5169f96e9c1b94ac8a385a.png)

(5)

> **è­¦å‘Šï¼š** æ§åˆ¶æ‰€æœ‰å¯èƒ½çš„åå˜é‡ä¸æ˜¯æœ€ä½³å®è·µï¼Œè€Œæ˜¯æ§åˆ¶é‚£äº›è¢«å‡è®¾/å·²çŸ¥ä¼šå½±å“ç»“æœ***y***å’Œæ„Ÿå…´è¶£çš„å¤„ç†***T***çš„å˜é‡ã€‚è¿™æ˜¯[æ··æ·†å˜é‡](https://en.wikipedia.org/wiki/Confounding)çš„æ¦‚å¿µã€‚ç›¸åï¼Œå¦‚æœ***y***å’Œ***T***éƒ½å½±å“æŸä¸ªå˜é‡ï¼Œæˆ‘ä»¬ä¸å¸Œæœ›æ§åˆ¶è¯¥å˜é‡ï¼Œå› ä¸ºè¿™å¯èƒ½ä¼šå¼•å…¥***y***å’Œ***T***ä¹‹é—´çš„è™šå‡å…³è”ã€‚è¿™æ˜¯[ç¢°æ’å˜é‡](https://en.wikipedia.org/wiki/Collider_(statistics))çš„æ¦‚å¿µã€‚æˆ‘ä»¬å°†åœ¨æœ¬æ–‡åé¢å±•ç¤ºè¿™ä¸€ç‚¹çš„å®é™…ä¾‹å­ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬ä¸å¸Œæœ›åŒ…æ‹¬é‚£äº›æ˜¯æˆ‘ä»¬å¤„ç†å˜é‡çš„ä¸­ä»‹å˜é‡çš„å˜é‡ï¼›å³ï¼Œä¸€ä¸ªå—å¤„ç†å½±å“è¿›è€Œå½±å“ç»“æœçš„åå˜é‡ã€‚åŒ…æ‹¬è¿™ç§ä¸­ä»‹å˜é‡å¯èƒ½ä¼šä¾µèš€æˆ‘ä»¬å¤„ç†æ•ˆåº”çš„ä¼°è®¡ã€‚ç®€è€Œè¨€ä¹‹ï¼Œæˆ‘ä»¬åªå¸Œæœ›åŒ…æ‹¬æ··æ·†å˜é‡ï¼ˆä»¥åŠå¯èƒ½çš„éä¸­ä»‹å’Œéç¢°æ’é¢„æµ‹å˜é‡ä»¥æé«˜ç²¾ç¡®åº¦ï¼›è¿™å°†åœ¨ä¸‹é¢çš„ä¾‹å­ 2 ä¸­è®¨è®ºï¼‰ã€‚

ç„¶è€Œï¼Œåœ¨å®è·µä¸­ï¼Œå¤–ç”Ÿæ€§/CIA/å¯å¿½ç•¥æ€§éå¸¸éš¾ä»¥è·å¾—å’Œè¯æ˜ï¼Œå› ä¸ºæˆ‘ä»¬ä¸å¤ªå¯èƒ½è§‚å¯Ÿåˆ°æ‰€æœ‰çš„æ··æ·†å˜é‡å¹¶æ§åˆ¶è¿™äº›æ··æ·†å˜é‡å¯èƒ½å‡ºç°çš„æ½œåœ¨éçº¿æ€§å…³ç³»ã€‚è¿™æ˜¯ DML çš„ä¸€ä¸ªç‰¹å®šåŠ¨æœºâ€”â€”ç„¶è€Œï¼Œè®©æˆ‘ä»¬é¦–å…ˆè®¨è®º FWL å®šç†ï¼Œå› ä¸ºè¿™ä½¿æˆ‘ä»¬èƒ½å¤Ÿç†è®ºä¸Šå¼€å‘ DMLã€‚

[FWL å®šç†](https://en.wikipedia.org/wiki/Frisch%e2%80%93Waugh%e2%80%93Lovell_theorem)æ˜¯ä¸€ä¸ªé‡è¦çš„è®¡é‡ç»æµå­¦å®šç†ï¼Œå®ƒå…è®¸æˆ‘ä»¬åœ¨ä¸Šè¿°å¤šé‡å›å½’ï¼ˆæ–¹ç¨‹ 5ï¼‰ä¸­åˆ©ç”¨ä»¥ä¸‹ 3 æ­¥ç¨‹åºè·å¾—***ç›¸åŒçš„***ATE å‚æ•°ä¼°è®¡Î²â‚ï¼Œå…³äºå¤„ç†å˜é‡***T***ï¼š

1.  åˆ†åˆ«å¯¹***y***è¿›è¡Œå›å½’***X***ï¼Œå¯¹***T***è¿›è¡Œå›å½’***X***

1.  ä¿å­˜ç¬¬ 1 æ­¥çš„æ®‹å·®â€”â€”ç§°ä¹‹ä¸º***y***å’Œ***T***ã€‚

1.  å¯¹***y***è¿›è¡Œå›å½’***T***

åœ¨å‡† Python ä»£ç ä¸­ï¼Œ

```py
import statsmodels.formula.api as smf

reg_y = smf.ols(formula='y ~ 1 + X', data = df).fit()
reg_T = smf.ols(formula='T ~ 1 + X', data = df).fit()

y_residual = reg_y.resid
T_residual = reg_T.resid

ATE_model = smf.ols(formula='y_residual ~ 1 + T_residual', data = df).fit()
```

ç›´è§‚åœ°è¯´ï¼ŒFWL å®šç†å°†***T***å’Œ***y***ä¸­çš„å˜å¼‚éƒ¨åˆ†ä»æ··æ·†å˜é‡***X***ä¸­åˆ†ç¦»å‡ºæ¥ï¼Œç„¶ååˆ©ç”¨å‰©ä½™çš„å˜å¼‚æ¥è§£é‡Šå…³é”®çš„å…³ç³»ï¼ˆå³ï¼Œ***T***å¦‚ä½•å½±å“***y***ï¼‰ã€‚æ›´å…·ä½“åœ°è¯´ï¼Œå®ƒåˆ©ç”¨***X***çš„ä¸€ç§ç‰¹æ®Šç±»å‹çš„æ­£äº¤æŠ•å½±çŸ©é˜µï¼Œç§°ä¸ºæ¶ˆé™¤çŸ©é˜µæˆ–æ®‹å·®ç”ŸæˆçŸ©é˜µï¼Œæ¥æ®‹å·®åŒ–***T***å’Œ***y***ã€‚æœ‰å…³ FWL ç¨‹åºçš„å®é™…åº”ç”¨ï¼Œè¯·å‚è§æˆ‘ä¹‹å‰çš„å¸–å­ã€‚è¯¥å®šç†å¯¹äºç†è§£ DML è‡³å…³é‡è¦ã€‚

> è¯·æ³¨æ„ï¼Œæˆ‘ï¼ˆæ•…æ„ï¼‰ç•¥è¿‡äº†ä¸€äº›é¢å¤–çš„å› æœæ¨æ–­å‡è®¾ï¼Œä¾‹å¦‚ Positivity/Common Support å’Œ SUTVA/åäº‹å®ä¸€è‡´æ€§ã€‚ä¸€èˆ¬æ¥è¯´ï¼ŒCIA/å¯å¿½ç•¥æ€§å‡è®¾æ˜¯éœ€è¦è¾©æŠ¤çš„æœ€å¸¸è§å‡è®¾ã€‚ç„¶è€Œï¼Œå»ºè®®æ„Ÿå…´è¶£çš„è¯»è€…ç†Ÿæ‚‰é¢å¤–çš„å‡è®¾ã€‚ç®€è¨€ä¹‹ï¼ŒPositivity ç¡®ä¿æˆ‘ä»¬æœ‰ä¸å¤„ç†è¿‡çš„å®¶åº­ç›¸ä¼¼ä¸”å¯æ¯”è¾ƒçš„æœªå¤„ç†å®¶åº­ï¼Œä»¥ä¾¿è¿›è¡Œåäº‹å®ä¼°è®¡ï¼Œè€Œ SUTVA ç¡®ä¿æ²¡æœ‰æº¢å‡º/ç½‘ç»œç±»å‹æ•ˆåº”ï¼ˆä¸€ä¸ªä¸ªä½“çš„å¤„ç†å½±å“å¦ä¸€ä¸ªä¸ªä½“ï¼‰ã€‚

## åŒé‡æœºå™¨å­¦ä¹ â€¦ ç®€åŒ–ç‰ˆï¼

åŒé‡æœºå™¨å­¦ä¹ çš„æ ¸å¿ƒæ˜¯å…è®¸åœ¨ FWL ç¨‹åºçš„æ­¥éª¤ 1) å’Œ 2) ä¸­è¿›è¡Œçš„æ®‹å·®åŒ–/æ­£äº¤åŒ–ä½¿ç”¨ä»»ä½•é«˜åº¦çµæ´»çš„ ML æ¨¡å‹ï¼Œä»è€Œæ„é€ ä¸€ä¸ªéƒ¨åˆ†çº¿æ€§æ¨¡å‹ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡ä»¥ä¸‹æ–¹å¼ä¼°è®¡ ATEï¼š

![](img/ff40e6bdeac9222544390e90e9b8ed1a.png)

(6)

å…¶ä¸­**ğ‘€ğ‘¦**å’Œ***MT***éƒ½æ˜¯ä»»ä½•ç”¨äºé¢„æµ‹***y***å’Œ***T***çš„*ML*æ¨¡å‹ï¼Œç»™å®šæ··æ·†å› ç´ å’Œ/æˆ–æ§åˆ¶å˜é‡***X***ã€‚**ğ‘€ğ‘¦**å’Œ***MT***ä¹Ÿè¢«ç§°ä¸ºâ€œå¹²æ‰°å‡½æ•°â€ï¼Œå› ä¸ºæˆ‘ä»¬æ­£åœ¨æ„é€ å‡½æ•°æ¥éƒ¨åˆ†å»é™¤***y***å’Œ***T***ä¸­ç”± X è§£é‡Šçš„å˜åŒ–ï¼Œè¿™ä¸æ˜¯ä¸»è¦å…³æ³¨çš„å†…å®¹ã€‚ä¸ºäº†é¿å…è¿‡æ‹Ÿåˆå¹¶ç¡®ä¿è¿™ç§æ–¹æ³•çš„ç¨³å¥æ€§ï¼Œæˆ‘ä»¬ä½¿ç”¨*äº¤å‰éªŒè¯é¢„æµ‹*é€šè¿‡æ ·æœ¬å’Œäº¤å‰æ‹Ÿåˆã€‚æˆ‘ç›¸ä¿¡è¿™é‡Œå†æ¬¡çœ‹åˆ°è¿™ç§ç¨‹åºåœ¨å‡† Python ä»£ç ä¸­çš„ç¤ºä¾‹ä¼šå¾ˆæœ‰ç”¨ï¼š

```py
from sklearn.model_selection import cross_val_predict
import statsmodels.formula.api as smf

M_y = *some ML model*
M_T = *some ML model*

y_residual = df[y] - cross_val_predict(M_y, df[X], df[y], cv=3)
T_residual = df[T] - cross_val_predict(M_T, df[X], df[T], cv=3)

ATE_model = smf.ols(formula='y_residual ~ 1 + T_residual', data = df).fit()
```

å…¶ä¸­ *T_residual* ä¸Šçš„ç³»æ•°å°†æ˜¯æˆ‘ä»¬ä¼°è®¡çš„ ATEï¼Œå¹¶ä¸”å›´ç»•æˆ‘ä»¬çš„ä¼°è®¡æœ‰æ¸è¿‘æ­£æ€æ¨æ–­ã€‚å°±è¿™æ ·ï¼

è¿™æ˜¯ DML ä¼°è®¡ ATE èƒŒåçš„è¿‡ç¨‹ã€‚*å®ƒä½¿æˆ‘ä»¬çš„æ··æ·†å»ºæ¨¡å…·æœ‰é«˜åº¦çš„çµæ´»æ€§å’Œéå‚æ•°ç‰¹æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨å­˜åœ¨é«˜ç»´åå˜é‡çš„æƒ…å†µä¸‹ã€‚*

æˆ‘ä¸ä¼šæ·±å…¥æ¢è®¨ä¸ºä»€ä¹ˆè¿™æœ‰æ•ˆçš„æŠ€æœ¯ç»†èŠ‚ï¼Œå¹¶ä¸”æˆ‘ä¼šå°†æ„Ÿå…´è¶£çš„è¯»è€…å¼•å¯¼åˆ°[åŸå§‹è®ºæ–‡](https://arxiv.org/pdf/1608.00060.pdf)å’Œ[EconML æ–‡æ¡£](https://econml.azurewebsites.net/index.html)ã€‚ç„¶è€Œï¼Œç®€è€Œè¨€ä¹‹ï¼ŒDML æ»¡è¶³ä¸€ä¸ªç§°ä¸º Neyman æ­£äº¤æ€§çš„æ¡ä»¶ï¼ˆå³ï¼Œå¹²æ‰°å‡½æ•°åœ¨çœŸå®å€¼é™„è¿‘çš„å°æ‰°åŠ¨å¯¹çŸ©æ¡ä»¶æœ‰äºŒé˜¶æ•ˆåº”ï¼Œå› æ­¤ä¸ä¼šå½±å“æˆ‘ä»¬çš„å…³é”®å‚æ•°ä¼°è®¡ï¼‰ï¼Œè¿™è§£å†³äº†æ­£åˆ™åŒ–åå·®çš„é—®é¢˜ï¼Œå¹¶ä¸”ä¸ DML ä¸­çš„äº¤å‰éªŒè¯ç¨‹åºç»“åˆä½¿ç”¨ï¼Œè§£å†³äº†è¿‡æ‹Ÿåˆåå·®ï¼Œä»è€Œç¡®ä¿äº†è¯¥æ–¹æ³•çš„ç¨³å¥æ€§ã€‚

DML æœ‰ä¸€äº›éå¸¸é…·çš„æ‰©å±•å°†åœ¨ç³»åˆ—çš„[ç¬¬äºŒéƒ¨åˆ†](https://medium.com/towards-data-science/double-machine-learning-simplified-part-2-extensions-the-cate-99926151cac)ä¸­è®¨è®ºï¼Œä½†ç°åœ¨æˆ‘ä»¬å…ˆé€šè¿‡ä¸¤ä¸ªåº”ç”¨æ¥æŸ¥çœ‹å®ƒçš„å®é™…æ•ˆæœã€‚

## DML åº”ç”¨

**åº”ç”¨ 1ï¼š** *åœ¨å¤„ç†éå®éªŒ/è§‚å¯Ÿæ•°æ®æ—¶ï¼Œè¶‹å‘äºå¤–ç”Ÿæ€§/CIA/å¯å¿½ç•¥æ€§*

å›é¡¾ä¸€ä¸‹ï¼Œæˆ‘ä»¬è®¨è®ºäº†åœ¨æ²¡æœ‰éšæœºå®éªŒæ•°æ®çš„æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å¿…é¡»æ§åˆ¶æ‰€æœ‰æ½œåœ¨çš„æ··æ‚å› ç´ ï¼Œä»¥ç¡®ä¿æˆ‘ä»¬è·å¾—æˆ‘ä»¬æ„Ÿå…´è¶£çš„å¤„ç†çš„å¤–ç”Ÿæ€§ã€‚æ¢å¥è¯è¯´ï¼Œå½“æˆ‘ä»¬æ§åˆ¶äº†æ‰€æœ‰æ½œåœ¨çš„æ··æ‚å› ç´ æ—¶ï¼Œæˆ‘ä»¬çš„å¤„ç†æ˜¯â€œå¦‚åŒéšæœºåˆ†é…â€ã€‚è¿™é‡Œä»ç„¶å­˜åœ¨ä¸¤ä¸ªä¸»è¦é—®é¢˜ï¼š

1.  äº†è§£æ‰€æœ‰æ··æ‚å› ç´ å¹¶ä¸”è·å–æ‰€æœ‰è¿™äº›æ··æ‚å› ç´ çš„æ•°æ®æ˜¯å›°éš¾çš„ï¼Œåœ¨æŸäº›æƒ…å†µä¸‹ç”šè‡³æ˜¯ä¸å¯èƒ½çš„ã€‚è§£å†³è¿™ä¸ªé—®é¢˜æ¶‰åŠå¯¹æ•°æ®ç”Ÿæˆè¿‡ç¨‹çš„æ·±å…¥äº†è§£ï¼Œä»”ç»†æ„å»ºå› æœæ¨¡å‹ï¼ˆå³ï¼Œæ„å»ºä¸€ä¸ª[DAG](https://en.wikipedia.org/wiki/Directed_acyclic_graph)çš„åŒæ—¶è¯„ä¼°æ½œåœ¨æ··æ‚å› ç´ å¹¶é¿å…ç¢°æ’å™¨ï¼‰ï¼Œä»¥åŠ/æˆ–è€…åˆ©ç”¨[å‡†å®éªŒ](https://en.wikipedia.org/wiki/Quasi-experiment)è®¾è®¡ã€‚

1.  å¦‚æœæˆ‘ä»¬ç¡®å®å¤„ç†äº†ç¬¬ä¸€ç‚¹ï¼Œæˆ‘ä»¬ä»ç„¶éœ€è¦åœ¨ä½¿ç”¨å‚æ•°æ¨¡å‹ï¼ˆä¾‹å¦‚åœ¨å›å½’æ¡†æ¶ä¸­ï¼‰æ—¶æŒ‡å®šæ··æ‚çš„æ­£ç¡®å‚æ•°å½¢å¼ï¼ŒåŒ…æ‹¬äº¤äº’é¡¹å’Œé«˜é˜¶é¡¹ã€‚ä»…åœ¨å›å½’ä¸­åŒ…å«çº¿æ€§é¡¹å¯èƒ½ä¸è¶³ä»¥æ§åˆ¶æ··æ‚ã€‚è¿™å°±æ˜¯ DML å‘æŒ¥ä½œç”¨çš„åœ°æ–¹ï¼›å®ƒå¯ä»¥ä»¥é«˜åº¦éå‚æ•°çš„æ–¹å¼çµæ´»åœ°éƒ¨åˆ†æ§åˆ¶æ··æ‚ã€‚è¿™ç‰¹åˆ«æœ‰åˆ©äºèŠ‚çœæ•°æ®ç§‘å­¦å®¶ç›´æ¥å»ºæ¨¡æ··æ‚çš„å‡½æ•°å½¢å¼çš„éº»çƒ¦ï¼Œå¹¶ä½¿æ›´å¤šçš„æ³¨æ„åŠ›å¯ä»¥é›†ä¸­åœ¨è¯†åˆ«å’Œæµ‹é‡æ··æ‚å› ç´ ä¸Šã€‚è®©æˆ‘ä»¬çœ‹çœ‹è¿™æ˜¯å¦‚ä½•å·¥ä½œçš„ï¼

å‡è®¾ï¼Œä½œä¸ºä¸€ä¸ªé«˜åº¦ç®€åŒ–çš„ä¾‹å­ï¼Œæˆ‘ä»¬åœ¨ä¸€å®¶ç”µå­å•†åŠ¡å…¬å¸å·¥ä½œï¼Œå¹¶ä¸”æˆ‘ä»¬çš„ä»»åŠ¡æ˜¯ä¼°è®¡ä¸ªäººåœ¨ç½‘ç«™ä¸ŠèŠ±è´¹çš„æ—¶é—´å¯¹ä»–ä»¬åœ¨è¿‡å»ä¸€ä¸ªæœˆçš„è´­ä¹°é‡‘é¢æˆ–é”€å”®é¢çš„å¹³å‡å¤„ç†æ•ˆåº”ï¼ˆATEï¼‰ã€‚ç„¶è€Œï¼Œè¿›ä¸€æ­¥å‡è®¾æˆ‘ä»¬åªæœ‰è§‚å¯Ÿæ•°æ®å¯ä»¥ä½¿ç”¨ï¼Œä½†æˆ‘ä»¬å·²ç»æµ‹é‡äº†æ‰€æœ‰æ½œåœ¨çš„æ··æ‚å› ç´ ï¼ˆé‚£äº›å½±å“ç½‘ç«™åœç•™æ—¶é—´å’Œé”€å”®é¢çš„å˜é‡ï¼‰ã€‚è®©è¿™ä¸ªå› æœè¿‡ç¨‹é€šè¿‡ä»¥ä¸‹çš„æœ‰å‘æ— ç¯å›¾ï¼ˆDAGï¼‰è¿›è¡Œæè¿°ï¼š

![](img/53ff709528289a0762c49af8579e9f7a.png)

å‡è®¾æ•°æ®ç”Ÿæˆè¿‡ç¨‹å¦‚ä¸‹ï¼ˆ*è¯·æ³¨æ„ï¼Œæ‰€æœ‰å€¼å’Œæ•°æ®éƒ½æ˜¯ä¸ºäº†æ¼”ç¤ºç›®çš„è€Œéšæ„é€‰æ‹©å’Œç”Ÿæˆçš„ï¼Œå› æ­¤ä¸ä¸€å®šä»£è¡¨çœŸå®ä¸–ç•Œç›´è§‚çš„å¾ˆå¤§ç¨‹åº¦ï¼Œé™¤éæˆ‘ä»¬å¯¹ ATE çš„ä¼°è®¡*ï¼‰ï¼š

```py
import numpy as np
import pandas as pd

# Sample Size
N = 100_000

# Observed Confounders (Age, Number of Social Media Accounts, & Years Member on Website)
age = np.random.randint(low=18,high=75,size=N)
num_social_media_profiles = np.random.choice([0,1,2,3,4,5,6,7,8,9,10], size = N)
yr_membership = np.random.choice([0,1,2,3,4,5,6,7,8,9,10], size = N)

# Additional Covariates (Arbitrary Z)
Z = np.random.normal(loc=50, scale = 25, size = N)

# Error Terms
Îµ_1 = np.random.normal(loc=20,scale=5,size=N)
Îµ_2 = np.random.normal(loc=40,scale=15,size=N)

# Treatment DGP (T = g(X) + Îµ) - Hrs spent on website in past month
time_on_website = np.maximum(10
                             - 0.01*age 
                             - 0.001*age**2 
                             + num_social_media_profiles 
                             - 0.01 * num_social_media_profiles**2
                             - 0.01*(age * num_social_media_profiles)
                             + 0.2 * yr_membership
                             + 0.001 * yr_membership**2
                             - 0.01 * (age * yr_membership)
                             + 0.2 * (num_social_media_profiles * yr_membership)
                             + 0.01 * (num_social_media_profiles * np.log(age) * age * yr_membership**(1/2))
                             + Îµ_1
                               ,0)

# Outcome DGP (y = f(T,X,Z) + Îµ) - Sales in past month
sales = np.maximum(25
                   +  5 * time_on_website # Simulated ATE = $5
                   - 0.1*age 
                   - 0.001*age**2 
                   + 8 * num_social_media_profiles 
                   - 0.1 * num_social_media_profiles**2
                   - 0.01*(age * num_social_media_profiles)
                   + 2 * yr_membership
                   + 0.1 * yr_membership**2
                   - 0.01 * (age * yr_membership)
                   + 3 * (num_social_media_profiles * yr_membership)
                   + 0.1 * (num_social_media_profiles * np.log(age) * age * yr_membership**(1/2))
                   + 0.5 * Z
                   + Îµ_2
                     ,0)

collider = np.random.normal(loc=100, scale=50, size=N) + 2*sales + 7*time_on_website

df = pd.DataFrame(np.array([sales,time_on_website,age,num_social_media_profiles,yr_membership,Z]).T
                  ,columns=["sales","time_on_website","age","num_social_media_profiles","yr_membership","Z"])
```

æ ¹æ®æ„å»ºï¼Œæˆ‘ä»¬çš„å…´è¶£å¤„ç†ï¼ˆè¿‡å»ä¸€ä¸ªæœˆåœ¨ç½‘ç«™ä¸ŠèŠ±è´¹çš„æ—¶é—´ï¼‰å’Œæˆ‘ä»¬çš„ç»“æœï¼ˆè¿‡å»ä¸€ä¸ªæœˆçš„é”€å”®é¢ï¼‰æœ‰ä»¥ä¸‹æ··æ‚å› ç´ ï¼šå¹´é¾„ã€ç¤¾äº¤åª’ä½“è´¦æˆ·æ•°é‡å’Œç½‘ç«™ä¼šå‘˜å¹´é™ï¼Œè¿™ç§æ··æ‚æ˜¯ä»»æ„éçº¿æ€§çš„ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°æ„å»ºçš„**ATE çœŸå®å€¼æ˜¯ $5**ï¼ˆåœ¨ä¸Šé¢çš„é”€å”® DGP ä¸­è¯´æ˜ï¼‰ã€‚*ä¹Ÿå°±æ˜¯è¯´ï¼Œå¹³å‡è€Œè¨€ï¼Œæ¯å¢åŠ ä¸€å°æ—¶çš„åœ¨ç½‘ç«™ä¸ŠèŠ±è´¹çš„æ—¶é—´ï¼Œä¸ªäººä¼šå¤šèŠ± $5ã€‚* æ³¨æ„ï¼Œæˆ‘ä»¬è¿˜åŒ…æ‹¬äº†ä¸€ä¸ªç¢°æ’å™¨å˜é‡ï¼ˆå—ç½‘ç«™èŠ±è´¹æ—¶é—´å’Œé”€å”®é¢å½±å“çš„å˜é‡ï¼‰ï¼Œè¯¥å˜é‡å°†åœ¨ä¸‹é¢çš„æ¼”ç¤ºä¸­ç”¨äºè¯´æ˜è¿™ç§åå·®å¦‚ä½•å½±å“ ATEã€‚

ä¸ºäº†å±•ç¤º DML çµæ´»éƒ¨åˆ†åŒ–é«˜åº¦éçº¿æ€§æ··æ‚å› ç´ çš„èƒ½åŠ›ï¼Œæˆ‘ä»¬å°†è¿è¡Œä»¥ä¸‹ 4 ä¸ªæ¨¡å‹ï¼š

1.  é”€å”®ï¼ˆyï¼‰å¯¹ç½‘ç«™ä¸ŠèŠ±è´¹çš„æ—¶é—´ï¼ˆTï¼‰è¿›è¡Œå¤©çœŸçš„ OLS å›å½’

1.  é”€å”®ï¼ˆyï¼‰å¯¹ç½‘ç«™ä¸ŠèŠ±è´¹çš„æ—¶é—´ï¼ˆTï¼‰åŠæ‰€æœ‰æ··æ‚å› ç´ çš„çº¿æ€§é¡¹è¿›è¡Œå¤šé‡ OLS å›å½’

1.  ä½¿ç”¨ DML æ®‹å·®åŒ–è¿‡ç¨‹çš„ OLS å›å½’ï¼Œè¯¦è§å…¬å¼ (5)

1.  åŒ…æ‹¬ç¢°æ’å™¨å˜é‡çš„ DML æ®‹å·®åŒ–è¿‡ç¨‹çš„ OLS å›å½’

ä»£ç å¦‚ä¸‹ï¼š

```py
import statsmodels.formula.api as smf
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.model_selection import cross_val_predict

# 1 - Naive OLS
naive_regression = smf.ols(formula=â€™sales ~ 1 + time_on_websiteâ€™,data=df).fit()

# 2 - Multiple OLS
multiple_regression = smf.ols(formula=â€™sales ~ 1 + time_on_website + age + num_social_media_profiles + yr_membershipâ€™,data=df).fit()

# 3 - DML Procedure
M_sales = GradientBoostingRegressor()
M_time_on_website = GradientBoostingRegressor()

df[â€˜residualized_salesâ€™] = df["sales"] - cross_val_predict(M_sales, df[["age","num_social_media_profiles","yr_membership"]], df[â€˜salesâ€™], cv=3)
df[â€˜residualized_time_on_websiteâ€™] = df[â€˜time_on_websiteâ€™] - cross_val_predict(M_time_on_website, df[["age","num_social_media_profiles","yr_membership"]], df[â€˜time_on_websiteâ€™], cv=3)

DML_model = smf.ols(formula=â€™residualized_sales ~ 1 + residualized_time_on_websiteâ€™, data = df).fit()

# 4 - DML Procedure w/ Collider
M_sales = GradientBoostingRegressor()
M_time_on_website = GradientBoostingRegressor()

df[â€˜residualized_salesâ€™] = df["sales"] - cross_val_predict(M_sales, df[["age","num_social_media_profiles","yr_membership","collider"]], df[â€˜salesâ€™], cv=3)
df[â€˜residualized_time_on_websiteâ€™] = df['time_on_website'] - cross_val_predict(M_time_on_website, df[["age","num_social_media_profiles","yr_membership", "collider"]], df['time_on_website'], cv=3)

DML_model_collider = smf.ols(formula='residualized_sales ~ 1 + residualized_time_on_website', data = df).fit()
```

å¯¹åº”çš„ç»“æœï¼ˆè§é™„å½•ä¸­çš„ä»£ç ä»¥åˆ›å»ºæ­¤è¡¨ï¼‰ï¼š

![](img/6d270c529361708fb89a92c20fbb0fb1.png)

å›å¿†ä¸€ä¸‹æˆ‘ä»¬æ¨¡æ‹Ÿçš„ ATE çœŸå®å€¼æ˜¯**$5**ã€‚æ³¨æ„ï¼Œå”¯ä¸€èƒ½å¤Ÿæ•æ‰è¿™ä¸ªå€¼çš„æ¨¡å‹æ˜¯ DML è¿‡ç¨‹ï¼æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œå¤©çœŸçš„æ¨¡å‹åœ¨ä¼°è®¡ä¸­æœ‰æ˜¾è‘—çš„æ­£åå·®ï¼Œè€Œä»…å¯¹æ··æ‚å› ç´ çš„çº¿æ€§é¡¹è¿›è¡Œæ§åˆ¶çš„å¤šé‡å›å½’åˆ™ç¨å¾®å‡å°‘äº†è¿™ç§åå·®ã€‚æ­¤å¤–ï¼Œå¸¦æœ‰ç¢°æ’å™¨çš„ DML è¿‡ç¨‹å±•ç¤ºäº†ä¸€ä¸ªè´Ÿåå·®ï¼›é€šè¿‡åœ¨æˆ‘ä»¬çš„ç¢°æ’å™¨ DGP ä¸­æ±‚è§£é”€å”®é¢ï¼Œå¯ä»¥*æ¾æ•£åœ°*æ¼”ç¤º/è§‚å¯Ÿåˆ°é”€å”®ä¸æˆ‘ä»¬å¤„ç†ä¹‹é—´çš„è¿™ç§è´Ÿç›¸å…³ã€‚

```py
collider = 100 + 2*sales + 7*time_on_website

# Note the negative relationship between sales and time_on_website here
sales = (collider - 100 - 7*time_on_website)/2
```

è¿™äº›ç»“æœå±•ç¤ºäº†ä½¿ç”¨çµæ´»çš„éå‚æ•° ML æ¨¡å‹åœ¨ DML è¿‡ç¨‹ä¸­å»é™¤æ··æ‚çš„æ˜ç¡®èƒ½åŠ›ï¼ç›¸å½“ä»¤äººæ»¡æ„ï¼Œå¯¹å§ï¼Ÿ **DML å»é™¤äº†å¯¹æ··æ‚ DGP çš„æ­£ç¡®å‚æ•°åŒ–è§„æ ¼çš„å¿…è¦æ€§ï¼ˆå‰ææ˜¯æ‰€æœ‰æ··æ‚å› ç´ éƒ½è¢«æ§åˆ¶ï¼‰ï¼**

> ç»†å¿ƒçš„è¯»è€…ä¼šæ³¨æ„åˆ°ï¼Œæˆ‘ä»¬åœ¨é”€å”®çš„ç”Ÿæˆè¿‡ç¨‹ä¸­åŒ…æ‹¬äº†ä»»æ„çš„åå˜é‡ ***Z***ã€‚ç„¶è€Œï¼Œæ³¨æ„åˆ° ***Z*** å¹¶ä¸ä¼šç›´æ¥å½±å“åœ¨ç½‘ç«™ä¸ŠèŠ±è´¹çš„æ—¶é—´ï¼Œå› æ­¤å®ƒä¸ç¬¦åˆæ··æ‚å› ç´ çš„å®šä¹‰ï¼Œå› æ­¤å¯¹ç»“æœæ²¡æœ‰å½±å“ï¼ˆé™¤äº†å¯èƒ½æé«˜ä¼°è®¡çš„ç²¾ç¡®åº¦â€”â€”è§åº”ç”¨ç¨‹åº 2ï¼‰

**åº”ç”¨ç¨‹åº 2ï¼š** *æé«˜å®éªŒæ•°æ®çš„ç²¾ç¡®åº¦å’Œç»Ÿè®¡æ•ˆèƒ½ï¼ˆéšæœºå¯¹ç…§è¯•éªŒ (RCTs) æˆ– A/B æµ‹è¯•ï¼‰*

ä¸€ä¸ªå¸¸è§çš„è¯¯è§£æ˜¯ï¼Œå¦‚æœä¸€ä¸ªå®éªŒæœ‰ä¸€ä¸ª*è¶³å¤Ÿå¤§çš„*æ ·æœ¬é‡ï¼Œå°±å¯ä»¥è·å¾—è¶³å¤Ÿçš„[ç»Ÿè®¡åŠŸæ•ˆ](https://en.wikipedia.org/wiki/Power_of_a_test)æ¥å‡†ç¡®æµ‹é‡æ„Ÿå…´è¶£çš„å¤„ç†ã€‚ç„¶è€Œï¼Œç¡®å®šå®éªŒä¸­çš„ç»Ÿè®¡åŠŸæ•ˆä»¥åŠæœ€ç»ˆ ATE ä¼°è®¡ç²¾åº¦çš„ä¸€ä¸ªå¸¸è¢«å¿½è§†çš„å› ç´ æ˜¯ä½ è¯•å›¾æµ‹é‡çš„ç»“æœçš„å˜å¼‚æ€§ã€‚

ä¾‹å¦‚ï¼Œå‡è®¾æˆ‘ä»¬å¯¹æŸä¸ªç‰¹å®šå¹¿å‘Šå¯¹ä¸ªäººè´­ä¹°é‡‘é¢çš„å½±å“æ„Ÿå…´è¶£ï¼Œå¹¶ä¸”æˆ‘ä»¬é¢„è®¡æ•ˆæœè¾ƒå°ï¼Œä½†å¹¶éå¾®ä¸è¶³é“â€”â€”ä¾‹å¦‚ï¼ŒATE ä¸º$5ã€‚ç„¶è€Œï¼Œå‡è®¾ä¸ªäººé”€å”®é¢çš„æ ‡å‡†å·®éå¸¸å¤§â€¦â€¦å¯èƒ½åœ¨$100 ç”šè‡³$1000 çš„èŒƒå›´å†…ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œç”±äºè¿™ç§é«˜å˜å¼‚æ€§ï¼Œå‡†ç¡®æ•æ‰ ATE å¯èƒ½ä¼šéå¸¸å›°éš¾â€”â€”ä¹Ÿå°±æ˜¯è¯´ï¼Œæˆ‘ä»¬çš„ä¼°è®¡å¯èƒ½ä¼šè·å¾—éå¸¸ä½çš„ç²¾åº¦ï¼ˆå¤§çš„æ ‡å‡†è¯¯å·®ï¼‰ã€‚ç„¶è€Œï¼Œæ•æ‰åˆ°$5 çš„ ATE å¯èƒ½åœ¨ç»æµä¸Šæ˜¯æœ‰æ„ä¹‰çš„ï¼ˆå¦‚æœæˆ‘ä»¬å¯¹ 100,000 æˆ·å®¶åº­è¿›è¡Œå®éªŒï¼Œè¿™å¯èƒ½è¾¾åˆ°$500,000ï¼‰ã€‚è¿™å°±æ˜¯ DML èƒ½å¤Ÿå‘æŒ¥ä½œç”¨çš„åœ°æ–¹ã€‚åœ¨æˆ‘ä»¬å±•ç¤ºå®é™…æ“ä½œä¹‹å‰ï¼Œè®©æˆ‘ä»¬å…ˆæŸ¥çœ‹æ–¹ç¨‹ï¼ˆ1ï¼‰ä¸­ç®€å•å›å½’çš„ ATE ä¼°è®¡çš„æ ‡å‡†è¯¯å·®å…¬å¼ï¼š

![](img/10bfc0c5637eea234212e656f6f7d254.png)

(7)

åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°æˆ‘ä»¬ä¼°è®¡çš„æ ‡å‡†è¯¯å·®ç›´æ¥å—åˆ°æ®‹å·®ï¼ˆÎµï¼‰å¤§å°çš„å½±å“ã€‚é‚£ä¹ˆè¿™å‘Šè¯‰æˆ‘ä»¬ä»€ä¹ˆå‘¢ï¼Ÿå¦‚æœæˆ‘ä»¬çš„å¤„ç†æ˜¯éšæœºåŒ–çš„ï¼Œæˆ‘ä»¬å¯ä»¥åœ¨å¤šé‡æ™®é€šæœ€å°äºŒä¹˜æ³•ï¼ˆOLSï¼‰æˆ–åŒé‡æœºå™¨å­¦ä¹ ï¼ˆDMLï¼‰ç¨‹åºä¸­åŒ…å«åå˜é‡ï¼Œè¿™æ ·åšçš„ç›®çš„æ˜¯å‡å°‘æˆ‘ä»¬ç»“æœçš„å˜å¼‚æ€§ï¼Œè€Œä¸æ˜¯è·å¾—å¤–ç”Ÿæ€§ã€‚æ›´å…·ä½“åœ°è¯´ï¼Œæˆ‘ä»¬å¯ä»¥åŒ…å«é‚£äº›å¯¹ç»“æœæœ‰å¼ºé¢„æµ‹ä½œç”¨çš„å˜é‡ï¼Œä»è€Œå‡å°‘æ®‹å·®ï¼Œå¹¶å› æ­¤é™ä½æˆ‘ä»¬ä¼°è®¡çš„æ ‡å‡†è¯¯å·®ã€‚è®©æˆ‘ä»¬æ¥çœ‹ä¸€ä¸‹è¿™ä¸ªå®é™…åº”ç”¨ã€‚é¦–å…ˆï¼Œå‡è®¾ä»¥ä¸‹æœ‰å‘æ— ç¯å›¾ï¼ˆDAGï¼‰ï¼ˆæ³¨æ„å¤„ç†æ˜¯éšæœºåŒ–çš„ï¼Œæ‰€ä»¥æ²¡æœ‰æ··æ‚å› ç´ ï¼‰ï¼š

![](img/5a4415a300956d8869404a4abecf0e4f.png)

æ­¤å¤–ï¼Œå‡è®¾ä»¥ä¸‹æ•°æ®ç”Ÿæˆè¿‡ç¨‹ï¼ˆDGPï¼‰ï¼š

```py
import numpy as np
import pandas as pd

# Sample Size
N = 100_000

# Observed Confounders (Age, Number of Social Media Accounts, & Years Member on Website)
age = np.random.randint(low=18,high=75,size=N)
num_social_media_profiles = np.random.choice([0,1,2,3,4,5,6,7,8,9,10], size = N)
yr_membership = np.random.choice([0,1,2,3,4,5,6,7,8,9,10], size = N)

# Additional Covariates (Arbitrary Z)
Z = np.random.normal(loc=50, scale = 25, size = N)

# Error Term
Îµ = np.random.normal(loc=40,scale=15,size=N)

# Randomized Treatment (T) - Advertisement Exposure
advertisement_exposure = np.random.choice([0,1],size=N,p=[.5,.5])

# Outcome (y = f(T,X,Z) + Îµ) - Sales in past month
sales = np.maximum(500
                   +  5 * advertisement_exposure # Ground Truth ATE of $5
                   - 10*age 
                   - 0.05*age**2 
                   + 15 * num_social_media_profiles 
                   - 0.01 * num_social_media_profiles**2
                   - 0.5*(age * num_social_media_profiles)
                   + 20 * yr_membership
                   + 0.5 * yr_membership**2
                   - 0.8 * (age * yr_membership)
                   + 5 * (num_social_media_profiles * yr_membership)
                   + 0.8 * (num_social_media_profiles * np.log(age) * age * yr_membership**(1/2))
                   + 15 * Z
                   + 2 * Z**2
                   + Îµ
                     ,0)

df = pd.DataFrame(np.array([sales,advertisement_exposure,age,num_social_media_profiles,yr_membership, Z]).T
                  ,columns=["sales","advertisement_exposure","age","num_social_media_profiles","yr_membership","Z"])
```

åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å†æ¬¡äººå·¥æ¨¡æ‹Ÿ**çœŸå®çš„ ATE ä¸º$5**ã€‚ä¸è¿‡è¿™æ¬¡ï¼Œæˆ‘ä»¬ç”Ÿæˆçš„é”€å”®æ•°æ®å…·æœ‰éå¸¸å¤§çš„æ–¹å·®ï¼Œå› æ­¤éš¾ä»¥æ£€æµ‹åˆ°$5 çš„ ATEã€‚

ä¸ºäº†æ¼”ç¤ºåœ¨ DML ç¨‹åºä¸­åŒ…å«é‚£äº›å¯¹æˆ‘ä»¬çš„ç»“æœæœ‰å¼ºé¢„æµ‹ä½œç”¨çš„åå˜é‡å¦‚ä½•å¤§å¤§æé«˜ ATE ä¼°è®¡çš„ç²¾åº¦ï¼Œæˆ‘ä»¬å°†è¿è¡Œä»¥ä¸‹ä¸‰ä¸ªæ¨¡å‹ï¼š

1.  é”€å”®ï¼ˆyï¼‰å¯¹*éšæœº*å¹¿å‘Šæš´éœ²ï¼ˆTï¼‰çš„ç®€å• OLS

1.  é”€å”®ï¼ˆyï¼‰å¯¹*éšæœº*å¹¿å‘Šæš´éœ²ï¼ˆTï¼‰åŠæ‰€æœ‰é”€å”®é¢„æµ‹å˜é‡çš„çº¿æ€§é¡¹çš„å¤šé‡ OLS

1.  ä½¿ç”¨ DML æ®‹å·®åŒ–ç¨‹åºçš„ OLSï¼Œå¦‚æ–¹ç¨‹ï¼ˆ5ï¼‰æ‰€è¿°

ä»£ç å¦‚ä¸‹ï¼š

```py
import statsmodels.formula.api as smf
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.model_selection import cross_val_predict

# 1 - Naive OLS
naive_regression = smf.ols(formula=â€œsales ~ 1 + advertisement_exposureâ€,data=df).fit()

# 2 - Multiple OLS
multiple_regression = smf.ols(formula=â€œsales ~ 1 + advertisement_exposure + age + num_social_media_profiles + yr_membership + Zâ€,data=df).fit()

# 3 - DML Procedure
M_sales = GradientBoostingRegressor()
M_advertisement_exposure = GradientBoostingClassifier() # Note binary treatment 

df[â€˜residualized_salesâ€™] = df["sales"] - cross_val_predict(M_sales, df[["age","num_social_media_profiles","yr_membership","Z"]], df[â€˜salesâ€™], cv=3)
df['residualized_advertisement_exposure'] = df['advertisement_exposure'] - cross_val_predict(M_advertisement_exposure, df[["age","num_social_media_profiles","yr_membership", "Z"]], df['advertisement_exposure'], cv=3, method = 'predict_proba')[:,0]

DML_model = smf.ols(formula='residualized_sales ~ 1 + residualized_advertisement_exposure', data = df).fit()
```

> æ‚¨å¯èƒ½ä¼šæ³¨æ„åˆ°æˆ‘ä»¬è¿˜åŒ…æ‹¬äº†ç”¨äºé¢„æµ‹å¹¿å‘Šæ›å…‰çš„ ML æ¨¡å‹ã€‚è¿™ä¸»è¦æ˜¯ä¸ºäº†ä¸ DML ç¨‹åºä¿æŒä¸€è‡´ã€‚ ç„¶è€Œï¼Œå› ä¸ºæˆ‘ä»¬çŸ¥é“å¹¿å‘Šæ›å…‰æ˜¯éšæœºçš„ï¼Œè¿™å¹¶éå®Œå…¨å¿…è¦ï¼Œä½†æˆ‘å»ºè®®éªŒè¯æˆ‘ä»¬çš„ç¤ºä¾‹æ¨¡å‹ç¡®å®æ— æ³•å­¦åˆ°ä»»ä½•ä¸œè¥¿ï¼ˆå³ï¼Œåœ¨æˆ‘ä»¬çš„æ¡ˆä¾‹ä¸­ï¼Œå®ƒåº”è¯¥ä¸ºæ‰€æœ‰ä¸ªä½“é¢„æµ‹ ~0.50 çš„æ¦‚ç‡ï¼Œå› æ­¤æ®‹å·®å°†ä¿æŒä¸åˆå§‹å¤„ç†åˆ†é…ç›¸åŒçš„å˜å¼‚ï¼‰ã€‚

è¿™äº›æ¨¡å‹çš„ç›¸åº”ç»“æœï¼ˆè¯·å‚é˜…é™„å½•ä¸­çš„ä»£ç ä»¥åˆ›å»ºæ­¤è¡¨ï¼‰ï¼š

![](img/853d63b779cac7d5151c932063b68296.png)

é¦–å…ˆï¼Œè¯·æ³¨æ„ï¼Œç”±äºå¤„ç†æ˜¯éšæœºåˆ†é…çš„ï¼Œå› æ­¤ä¸Šé¢æ²¡æœ‰å‘ç”ŸçœŸæ­£çš„æ··æ·†ã€‚ (1) å’Œ (2) ä¸­çš„ ATE ä¼°è®¡è¯¯å·®æ˜¯ç”±äºä¸ç²¾ç¡®çš„ä¼°è®¡ï¼ˆè§æ‹¬å·ä¸­çš„å¤§æ ‡å‡†è¯¯å·®ï¼‰ç›´æ¥å¯¼è‡´çš„ã€‚ æ³¨æ„éšç€æˆ‘ä»¬ä» (1) åˆ° (3) è¿ç§»ï¼Œæ ‡å‡†è¯¯å·®å¦‚ä½•å˜å°ï¼ˆç²¾åº¦æé«˜ï¼‰ï¼Œå…¶ä¸­ DML ç¨‹åºå…·æœ‰æœ€ç²¾ç¡®çš„ä¼°è®¡ã€‚ è¯·æ³¨æ„ä¸Šé¢çº¢æ¡†ä¸­çš„â€œæ®‹å·®æ ‡å‡†è¯¯å·®â€è¡Œã€‚ æˆ‘ä»¬å¯ä»¥çœ‹åˆ° DML ç¨‹åºé€šè¿‡ä»æˆ‘ä»¬ç»“æœçš„ ML æ¨¡å‹ï¼ˆé”€å”®ï¼‰ä¸­çš„é¢„æµ‹å› å­ä¸­éƒ¨åˆ†æ¶ˆé™¤å¯å­¦ä¹ çš„å˜å¼‚ï¼Œæ˜¾è‘—å‡å°‘äº† ATE æ¨¡å‹æ®‹å·®çš„å˜å¼‚ã€‚ å†æ¬¡ï¼Œåœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œæˆ‘ä»¬çœ‹åˆ° DML æ˜¯å”¯ä¸€ä¸€ä¸ªèƒ½å¤Ÿè·å¾—çœŸå® ATE çš„æ¨¡å‹ï¼

è¿™äº›ç»“æœå±•ç¤ºäº†åœ¨å®éªŒç¯å¢ƒä¸­ä½¿ç”¨ DML ä»¥æé«˜ç»Ÿè®¡èƒ½åŠ›å’Œ ATE ä¼°è®¡ç²¾åº¦çš„å¥½å¤„ã€‚ å…·ä½“è€Œè¨€ï¼Œè¿™å¯ä»¥åº”ç”¨äº RCT æˆ– A/B æµ‹è¯•ç¯å¢ƒï¼Œå…¶ä¸­ç»“æœçš„å˜å¼‚éå¸¸å¤§å’Œ/æˆ–åœ¨ç²¾ç¡®ä¼°è®¡æ–¹é¢é‡åˆ°å›°éš¾ï¼Œå¹¶ä¸”å¯ä»¥è®¿é—®å¼ºæœ‰åŠ›çš„ç»“æœé¢„æµ‹å› å­ã€‚

## ç»“è®º

å°±è¿™æ ·â€”â€”åŒé‡æœºå™¨å­¦ä¹ ç®€åŒ–ç‰ˆï¼ˆå¸Œæœ›å¦‚æ­¤ï¼‰ï¼ æ„Ÿè°¢æ‚¨èŠ±æ—¶é—´é˜…è¯»æˆ‘çš„æ–‡ç« ã€‚ æˆ‘å¸Œæœ›è¿™ç¯‡æ–‡ç« èƒ½ä¸ºæ‚¨æä¾›å¯¹ DML åŸºç¡€çŸ¥è¯†çš„æ¸…æ™°è€Œç›´è§‚çš„ç†è§£ï¼Œä»¥åŠ**DML æ‰€å…·å¤‡çš„çœŸæ­£åŠ›é‡**ï¼Œä»¥åŠå¦‚ä½•åœ¨æ—¥å¸¸å› æœæ¨æ–­ä»»åŠ¡ä¸­åˆ©ç”¨ DMLã€‚

æ•¬è¯·å…³æ³¨[ç¬¬äºŒéƒ¨åˆ†](https://medium.com/towards-data-science/double-machine-learning-simplified-part-2-extensions-the-cate-99926151cac)ç³»åˆ—ï¼Œæˆ‘ä»¬å°†æ·±å…¥æ¢è®¨ä¸€äº›éå¸¸é…·çš„ DML æ‰©å±•ï¼Œè¿™äº›æ‰©å±•å°†æˆ‘ä»¬çš„å› æœæ¨æ–­é—®é¢˜è½¬åŒ–ä¸ºé¢„æµ‹ä»»åŠ¡ï¼Œæˆ‘ä»¬è¶…è¶Š ATEï¼Œé¢„æµ‹ä¸ªä½“çº§åˆ«çš„å¤„ç†æ•ˆåº”ï¼Œä»¥è¾…åŠ©å†³ç­–å’Œæ•°æ®é©±åŠ¨çš„ç›®æ ‡ã€‚

ä¸€å¦‚æ—¢å¾€ï¼Œæˆ‘å¸Œæœ›ä½ ä»¬è¯»è¿™ç¯‡æ–‡ç« æ—¶å’Œæˆ‘å†™è¿™ç¯‡æ–‡ç« æ—¶ä¸€æ ·æ„‰å¿«ï¼

## é™„å½•

*åˆ›å»ºæ¼‚äº®çš„è¡¨æ ¼ï¼š*

```py
# Example 1
file = open('Example 1.html','w')

order = ['time_on_website','residualized_time_on_website','age','num_social_media_profiles','yr_membership','Intercept']
rename = {'time_on_website':'Treatment: Hours on Website','residualized_time_on_website':'Residualized Treatment: Hours on Website','age':'Age',
          'num_social_media_profiles':"# of Social Media Profiles", "yr_membership":"Years of Membership"}
columns = ['Naive OLS','Multiple OLS','DML','DML w/ Collider']

regtable = Stargazer([naive_regression, multiple_regression, DML_model, DML_model_collider])
regtable.covariate_order(order)
regtable.custom_columns(columns,[1,1,1,1])
regtable.rename_covariates(rename)
regtable.show_degrees_of_freedom(False)
regtable.title('Example 1: Obtaining Exogeneity w/ DML')

file.write(regtable.render_html())
file.close()

# Example 2
file = open('Example 2.html','w')

order = ['advertisement_exposure','residualized_advertisement_exposure','age','num_social_media_profiles','yr_membership','Intercept']
rename = {'advertisement_exposure':'Treatment: Exposure to Advertisement','residualized_advertisement_exposure':'Residualized Treatment: Exposure to Advertisement','age':'Age',
          'num_social_media_profiles':"# of Social Media Profiles", "yr_membership":"Years of Membership"}
columns = ['Naive OLS','Multiple OLS','DML']

regtable = Stargazer([naive_regression, multiple_regression, DML_model])
regtable.covariate_order(order)
regtable.custom_columns(columns,[1,1,1])
regtable.rename_covariates(rename)
regtable.show_degrees_of_freedom(False)
regtable.title('Example 2: Improving Statistical Power in RCT')

file.write(regtable.render_html())
file.close()
```

## èµ„æº

[1] V. Chernozhukov, D. Chetverikov, M. Demirer, E. Duflo, C. Hansen, å’Œ a. W. Newey. åŒé‡æœºå™¨å­¦ä¹ ç”¨äºå¤„ç†å’Œå› æœå‚æ•°ã€‚*ArXiv ç”µå­å°åˆ·å“*ï¼Œ2016 å¹´ 7 æœˆã€‚

*é€šè¿‡è¿™ä¸ª GitHub ä»“åº“è®¿é—®æ‰€æœ‰ä»£ç ï¼š* [`github.com/jakepenzak/Blog-Posts`](https://github.com/jakepenzak/Blog-Posts)

*æ„Ÿè°¢ä½ é˜…è¯»æˆ‘çš„æ–‡ç« ï¼æˆ‘åœ¨ Medium ä¸Šçš„æ–‡ç« æ—¨åœ¨æ¢è®¨åˆ©ç”¨* ***è®¡é‡ç»æµå­¦*** *å’Œ* ***ç»Ÿè®¡å­¦/æœºå™¨å­¦ä¹ *** *æŠ€æœ¯çš„ç°å®ä¸–ç•Œå’Œç†è®ºåº”ç”¨ã€‚æ­¤å¤–ï¼Œæˆ‘è¿˜è‡´åŠ›äºé€šè¿‡ç†è®ºå’Œæ¨¡æ‹Ÿæä¾›æœ‰å…³å„ç§æ–¹æ³•è®ºçš„ç†è®ºåŸºç¡€çš„æ–‡ç« ã€‚æœ€é‡è¦çš„æ˜¯ï¼Œæˆ‘å†™ä½œæ˜¯ä¸ºäº†å­¦ä¹ å’Œå¸®åŠ©ä»–äººå­¦ä¹ ï¼æˆ‘å¸Œæœ›ä½¿å¤æ‚çš„ä¸»é¢˜å¯¹æ‰€æœ‰äººç¨å¾®æ›´å®¹æ˜“ç†è§£ã€‚å¦‚æœä½ å–œæ¬¢è¿™ç¯‡æ–‡ç« ï¼Œè¯·è€ƒè™‘* [***å…³æ³¨æˆ‘åœ¨ Medium ä¸Šçš„è´¦å·***](https://medium.com/@jakepenzak)*ï¼*
