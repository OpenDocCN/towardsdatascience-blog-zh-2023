- en: Data is the Foundation of Language Models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/data-is-the-foundation-of-language-models-52e9f48c07f5?source=collection_archive---------2-----------------------#2023-10-29](https://towardsdatascience.com/data-is-the-foundation-of-language-models-52e9f48c07f5?source=collection_archive---------2-----------------------#2023-10-29)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: How high-quality data impacts every aspect of the LLM training pipeline…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://wolfecameron.medium.com/?source=post_page-----52e9f48c07f5--------------------------------)[![Cameron
    R. Wolfe, Ph.D.](../Images/52bb88d7cf1105501be2fae5ccbe7a03.png)](https://wolfecameron.medium.com/?source=post_page-----52e9f48c07f5--------------------------------)[](https://towardsdatascience.com/?source=post_page-----52e9f48c07f5--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----52e9f48c07f5--------------------------------)
    [Cameron R. Wolfe, Ph.D.](https://wolfecameron.medium.com/?source=post_page-----52e9f48c07f5--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F28aa6026c553&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdata-is-the-foundation-of-language-models-52e9f48c07f5&user=Cameron+R.+Wolfe%2C+Ph.D.&userId=28aa6026c553&source=post_page-28aa6026c553----52e9f48c07f5---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----52e9f48c07f5--------------------------------)
    ·16 min read·Oct 29, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F52e9f48c07f5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdata-is-the-foundation-of-language-models-52e9f48c07f5&user=Cameron+R.+Wolfe%2C+Ph.D.&userId=28aa6026c553&source=-----52e9f48c07f5---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F52e9f48c07f5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdata-is-the-foundation-of-language-models-52e9f48c07f5&source=-----52e9f48c07f5---------------------bookmark_footer-----------)![](../Images/0cdc29d0203d7225c4541733960f9b92.png)'
  prefs: []
  type: TYPE_NORMAL
- en: (Photo by [Joshua Sortino](https://unsplash.com/@sortino?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)
    on [Unsplash](https://unsplash.com/photos/worms-eye-view-photography-of-ceiling-LqKhnDzSF-8?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash))
  prefs: []
  type: TYPE_NORMAL
- en: Large Language Models (LLMs) have been around for quite some time, but only
    recently has their impressive performance warranted significant attention from
    the broader AI community. With this in mind, we might begin to question the origin
    of the current LLM movement. *What was it that actually made recent models so
    impressive compared to their predecessors?* Although some may argue a variety
    of different factors, one especially impactful advancement was the ability to
    perform alignment. In other words, we figured out how to train LLMs to not just
    output the most likely next word, but to output text will satisfy the goals of
    a human, whether it be by following an instruction or retrieving important information.
  prefs: []
  type: TYPE_NORMAL
- en: “We hypothesize that alignment can be a simple process where the model learns
    the style or format for interacting with users, to expose the knowledge and capabilities
    that were already acquired during pretraining” *— from [1]*
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: This overview will study the role and impact of alignment, as well as the interplay
    between alignment and pre-training. Interestingly, these ideas were explored by
    the recent LIMA model [1], which performs alignment by simply fine-tuning a pre-trained
    LLM over a semi-manually…
  prefs: []
  type: TYPE_NORMAL
