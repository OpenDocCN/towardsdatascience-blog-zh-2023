# ä»é›¶å¼€å§‹çš„æœ´ç´ è´å¶æ–¯ä¸ TensorFlow

> åŸæ–‡ï¼š[`towardsdatascience.com/naive-bayes-from-scratch-with-tensorflow-6e04c5a25947`](https://towardsdatascience.com/naive-bayes-from-scratch-with-tensorflow-6e04c5a25947)

## æ¦‚ç‡æ·±åº¦å­¦ä¹ 

[](https://medium.com/@luisroque?source=post_page-----6e04c5a25947--------------------------------)![è·¯æ˜“æ–¯Â·ç½—å…‹](https://medium.com/@luisroque?source=post_page-----6e04c5a25947--------------------------------)[](https://towardsdatascience.com/?source=post_page-----6e04c5a25947--------------------------------)![Towards Data Science](https://towardsdatascience.com/?source=post_page-----6e04c5a25947--------------------------------) [è·¯æ˜“æ–¯Â·ç½—å…‹](https://medium.com/@luisroque?source=post_page-----6e04c5a25947--------------------------------)

Â·å‘è¡¨äº [Towards Data Science](https://towardsdatascience.com/?source=post_page-----6e04c5a25947--------------------------------) Â·10 åˆ†é’Ÿé˜…è¯»Â·2023 å¹´ 1 æœˆ 18 æ—¥

--

# ä»‹ç»

æœ¬æ–‡å±äºâ€œæ¦‚ç‡æ·±åº¦å­¦ä¹ â€ç³»åˆ—ã€‚è¯¥ç³»åˆ—æ¯å‘¨æ¶µç›–æ·±åº¦å­¦ä¹ çš„æ¦‚ç‡æ–¹æ³•ã€‚ä¸»è¦ç›®æ ‡æ˜¯æ‰©å±•æ·±åº¦å­¦ä¹ æ¨¡å‹ä»¥é‡åŒ–ä¸ç¡®å®šæ€§ï¼Œå³äº†è§£å®ƒä»¬ä¸çŸ¥é“çš„å†…å®¹ã€‚

åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å¯¹ä½¿ç”¨è‘¡è„é…’æ ·æœ¬æ•°æ®é›†çš„æœ´ç´ è´å¶æ–¯åˆ†ç±»ç®—æ³•è¿›è¡Œäº†è€ƒå¯Ÿã€‚æœ´ç´ è´å¶æ–¯ç®—æ³•æ˜¯ä¸€ç§åŸºäºè´å¶æ–¯å®šç†çš„æ¦‚ç‡æœºå™¨å­¦ä¹ æŠ€æœ¯ï¼Œå‡è®¾åœ¨ç»™å®šç›®æ ‡æ ‡ç­¾çš„æƒ…å†µä¸‹ç‰¹å¾ä¹‹é—´æ˜¯ç‹¬ç«‹çš„ã€‚ä¸ºäº†ä¾¿äºå¯è§†åŒ–ç±»åˆ«çš„åˆ†ç¦»ï¼Œæˆ‘ä»¬å°†æ¨¡å‹é™åˆ¶ä¸ºä»…ä½¿ç”¨ä¸¤ä¸ªç‰¹å¾ã€‚

æˆ‘ä»¬çš„ç›®æ ‡æ˜¯åŸºäºé€‰å®šçš„ç‰¹å¾å¯¹è‘¡è„é…’æ ·æœ¬è¿›è¡Œåˆ†ç±»ã€‚ä¸ºå®ç°è¿™ä¸€ç›®æ ‡ï¼Œæˆ‘ä»¬é¦–å…ˆæ¢ç´¢æ•°æ®å¹¶é€‰æ‹©æœ‰æ•ˆåŒºåˆ†å„ç±»åˆ«çš„ç‰¹å¾ã€‚ç„¶åï¼Œæˆ‘ä»¬æ„å»ºç±»åˆ«å…ˆéªŒåˆ†å¸ƒå’Œç±»åˆ«æ¡ä»¶å¯†åº¦ï¼Œä»è€Œèƒ½å¤Ÿé¢„æµ‹å…·æœ‰æœ€é«˜æ¦‚ç‡çš„ç±»åˆ«ã€‚è¯¥ç ”ç©¶ä½¿ç”¨çš„æ•°æ®é›†åŒ…å«è‘¡è„é…’çš„å„ç§ç‰¹å¾ï¼Œå¦‚è‰²è°ƒã€é…’ç²¾ã€ç±»é»„é…®ä»¥åŠä¸€ä¸ªç›®æ ‡ç±»åˆ«ï¼Œå¹¶ä¸”æ•°æ®é›†æ¥è‡ª scikit-learn åº“ [1]ã€‚

è¿„ä»Šä¸ºæ­¢å‘è¡¨çš„æ–‡ç« ï¼š

1.  [TensorFlow Probability çš„æ¸©å’Œä»‹ç»ï¼šåˆ†å¸ƒå¯¹è±¡](https://medium.com/towards-data-science/gentle-introduction-to-tensorflow-probability-distribution-objects-1bb6165abee1)

1.  [TensorFlow Probability çš„æ¸©å’Œä»‹ç»ï¼šå¯è®­ç»ƒå‚æ•°](https://medium.com/towards-data-science/gentle-introduction-to-tensorflow-probability-trainable-parameters-5098ea4fed15)

1.  ä»é›¶å¼€å§‹çš„æœ€å¤§ä¼¼ç„¶ä¼°è®¡ï¼Œä½¿ç”¨ TensorFlow Probability

1.  ä»é›¶å¼€å§‹çš„æ¦‚ç‡çº¿æ€§å›å½’ï¼Œä½¿ç”¨ TensorFlow

1.  [Tensorflow ä¸­çš„æ¦‚ç‡æ€§ä¸ç¡®å®šæ€§å›å½’](https://medium.com/towards-data-science/probabilistic-vs-deterministic-regression-with-tensorflow-85ef791beeef)

1.  [é¢‘ç‡å­¦æ´¾ä¸è´å¶æ–¯ç»Ÿè®¡åœ¨ Tensorflow ä¸­çš„åº”ç”¨](https://medium.com/towards-data-science/frequentist-vs-bayesian-statistics-with-tensorflow-fbba2c6c9ae5)

1.  ç¡®å®šæ€§ä¸æ¦‚ç‡æ€§æ·±åº¦å­¦ä¹ 

1.  ä»å¤´å¼€å§‹ä½¿ç”¨ TensorFlow å®ç°æœ´ç´ è´å¶æ–¯

![](img/32c7b8ce4cb661b5ede5f8501c912b4e.png)

å›¾ 1ï¼šä»Šå¤©çš„æ ¼è¨€ï¼šåœ¨è‘¡è„é…’åˆ†ç±»æ–¹é¢è¦ä¿æŒ**å¤©çœŸ**ï¼Ÿ ([source](https://unsplash.com/photos/3uJt73tr4hI))

æˆ‘ä»¬ä½¿ç”¨ TensorFlow å’Œ TensorFlow Probability å¼€å‘æˆ‘ä»¬çš„æ¨¡å‹ã€‚TensorFlow Probability æ˜¯ä¸€ä¸ªå»ºç«‹åœ¨ TensorFlow ä¹‹ä¸Šçš„ Python åº“ã€‚æˆ‘ä»¬å°†ä» TensorFlow Probability ä¸­çš„åŸºæœ¬å¯¹è±¡å¼€å§‹ï¼Œç†è§£å¦‚ä½•æ“ä½œå®ƒä»¬ã€‚æ¥ä¸‹æ¥çš„å‡ å‘¨é‡Œï¼Œæˆ‘ä»¬å°†é€æ­¥å¢åŠ å¤æ‚æ€§ï¼Œå¹¶å°†æˆ‘ä»¬çš„æ¦‚ç‡æ¨¡å‹ä¸ç°ä»£ç¡¬ä»¶ï¼ˆä¾‹å¦‚ GPUï¼‰ä¸Šçš„æ·±åº¦å­¦ä¹ ç»“åˆèµ·æ¥ã€‚

å¦‚å¸¸ï¼Œä»£ç å¯ä»¥åœ¨æˆ‘çš„[GitHub](https://github.com/luisroque/probabilistic_deep_learning_with_TFP)ä¸Šæ‰¾åˆ°ã€‚

# æ¢ç´¢æ€§æ•°æ®

```py
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
%matplotlib inline

import tensorflow as tf
import tensorflow_probability as tfp
tfd = tfp.distributions

from sklearn.metrics import accuracy_score
from sklearn import datasets, model_selection
from sklearn.datasets import load_wine
```

æˆ‘ä»¬çš„ç›®æ ‡æ˜¯è°ƒæŸ¥ä½¿ç”¨æœ´ç´ è´å¶æ–¯ç®—æ³•æ ¹æ®é€‰æ‹©çš„ç‰¹å¾å¯¹è‘¡è„é…’æ ·æœ¬è¿›è¡Œåˆ†ç±»ã€‚ä¸ºäº†å®ç°è¿™ä¸€ç›®æ ‡ï¼Œæˆ‘ä»¬é¦–å…ˆè¿›è¡Œæ•°æ®çš„æ¢ç´¢æ€§åˆ†æã€‚è®©æˆ‘ä»¬å¼€å§‹è¯†åˆ«ä¸¤ä¸ªæœ‰æ•ˆåŒºåˆ†ç›®æ ‡å˜é‡çš„ç‰¹å¾ï¼Œå¹¶åˆ©ç”¨å®ƒä»¬é¢„æµ‹è‘¡è„é…’çš„ç±»åˆ«ã€‚

```py
dataset = load_wine(as_frame=True)
dataset = pd.concat((dataset['data'], dataset['target']), axis=1)
sns.pairplot(dataset[['alcohol','alcalinity_of_ash', 'flavanoids', 'color_intensity', 'hue', 'target']],hue='target');
```

![](img/ce21d60bcffb8b0fc6367373d093b396.png)

å›¾ 2ï¼šåˆ†æè‘¡è„é…’æ•°æ®é›†ä¸­çš„ç‰¹å¾å¯¹ã€‚

é…’ç²¾åº¦å’Œè‰²è°ƒæ˜¯æœ‰æ•ˆåŒºåˆ†ç±»åˆ«çš„ç‰¹å¾ã€‚å› æ­¤ï¼Œè¿™äº›å°±æ˜¯æˆ‘ä»¬å°†ç”¨äºæ„å»ºæœ´ç´ è´å¶æ–¯æ¨¡å‹çš„ä¸¤ä¸ªç‰¹å¾ã€‚

```py
sns.jointplot(x='alcohol',y='hue', hue='target', data=dataset);
```

![](img/f6e044ba304f9de780daf52a77059f79.png)

å›¾ 3ï¼šæŒ‰é…’ç²¾åº¦å’Œè‰²è°ƒåˆ†å¸ƒçš„ç›®æ ‡æ ·æœ¬ã€‚

æˆ‘ä»¬ç°åœ¨å¯ä»¥å°†æ•°æ®åˆ†æˆè®­ç»ƒé›†å’Œæµ‹è¯•é›†ã€‚

```py
data = dataset[['alcohol', 'hue']].to_numpy()
targets = dataset[['target']].to_numpy()

label_colors = ['darkred', 'peachpuff', 'black']
x_train, x_test, y_train, y_test = model_selection.train_test_split(data, targets, test_size=0.2)
```

# æœ´ç´ è´å¶æ–¯åˆ†ç±»å™¨

æœ´ç´ è´å¶æ–¯æ˜¯ä¸€ç§å¹¿æ³›ä½¿ç”¨çš„æ¦‚ç‡æœºå™¨å­¦ä¹ ç®—æ³•ï¼ŒåŸºäºè´å¶æ–¯å®šç†ã€‚å®ƒç‰¹åˆ«é€‚ç”¨äºåˆ†ç±»ä»»åŠ¡ï¼Œå¹¶ä»¥å…¶ç®€å•æ€§å’Œé«˜æ•ˆæ€§è‘—ç§°ã€‚å°½ç®¡åå­—ä¸­æœ‰â€œæœ´ç´ â€ï¼Œä½†ç‰¹å¾ä¹‹é—´çš„â€œæœ´ç´ â€ç‹¬ç«‹æ€§å‡è®¾å¹¶ä¸æ€»æ˜¯é™åˆ¶ï¼Œå¹¶ä¸”åœ¨å®è·µä¸­é€šå¸¸èƒ½å–å¾—è‰¯å¥½ç»“æœã€‚åœ¨è¿™ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬å°†å…¨é¢å›é¡¾æœ´ç´ è´å¶æ–¯ç®—æ³•åŠå…¶å˜ä½“ï¼Œå¹¶ä»åŸºæœ¬åŸç†ä¸Šå®ç°å®ƒã€‚

æˆ‘ä»¬é¦–å…ˆç®€è¦ä»‹ç»è´å¶æ–¯å®šç†ï¼Œè¿™æ˜¯æœ´ç´ è´å¶æ–¯ç®—æ³•çš„åŸºç¡€ã€‚è´å¶æ–¯å®šç†è¡¨æ˜ï¼Œåœ¨ç»™å®šä¸€äº›è¯æ®ï¼ˆEï¼‰çš„æƒ…å†µä¸‹ï¼Œå‡è®¾ï¼ˆHï¼‰çš„æ¦‚ç‡ä¸å‡è®¾çš„å…ˆéªŒæ¦‚ç‡ä¹˜ä»¥è¯æ®åœ¨ç»™å®šå‡è®¾ä¸‹çš„ä¼¼ç„¶æ€§æˆæ­£æ¯”ã€‚æœ´ç´ è´å¶æ–¯ç®—æ³•ä½¿ç”¨è¿™ä¸ªå®šç†é€šè¿‡è®¡ç®—æ¯ä¸ªç±»åˆ«çš„åéªŒæ¦‚ç‡æ¥åˆ†ç±»æ–°å®ä¾‹ï¼Œç„¶åé€‰æ‹©æ¦‚ç‡æœ€é«˜çš„ç±»åˆ«ã€‚

æœ´ç´ è´å¶æ–¯ç®—æ³•çš„åŸºæœ¬åŸç†æ˜¯å‡è®¾ç»™å®šå®ä¾‹çš„ç‰¹å¾åœ¨ç»™å®šç±»åˆ«æ ‡ç­¾çš„æƒ…å†µä¸‹æ˜¯æ¡ä»¶ç‹¬ç«‹çš„ã€‚è¿™ä¸€å‡è®¾ï¼Œä¹Ÿç§°ä¸ºâ€œæœ´ç´ â€å‡è®¾ï¼Œä½¿å¾—ç®—æ³•åœ¨è®¡ç®—ä¸Šæ›´ä¸ºé«˜æ•ˆï¼Œå› ä¸ºå®ƒå‡å°‘äº†éœ€è¦ä¼°è®¡çš„å‚æ•°æ•°é‡ã€‚ç„¶è€Œï¼Œå½“ç‰¹å¾å®é™…ä¸Šå¹¶éç‹¬ç«‹æ—¶ï¼Œè¿™ä¹Ÿå¯èƒ½å¯¼è‡´å‡†ç¡®ç‡ä¸‹é™ã€‚

æœ´ç´ è´å¶æ–¯ç®—æ³•æœ‰å‡ ç§å˜ä½“ï¼Œæ¯ç§éƒ½é€‚ç”¨äºä¸åŒç±»å‹çš„æ•°æ®ã€‚ä¾‹å¦‚ï¼Œé«˜æ–¯æœ´ç´ è´å¶æ–¯ç”¨äºè¿ç»­æ•°æ®ï¼Œè€Œå¤šé¡¹å¼æœ´ç´ è´å¶æ–¯ç”¨äºç¦»æ•£æ•°æ®ã€‚ä¼¯åŠªåˆ©æœ´ç´ è´å¶æ–¯ç”¨äºäºŒå…ƒæ•°æ®ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å°†ä¸“æ³¨äºå®ç°é«˜æ–¯æœ´ç´ è´å¶æ–¯ã€‚

æœ´ç´ è´å¶æ–¯ç®—æ³•å·²è¢«åº”ç”¨äºå¹¿æ³›çš„é¢†åŸŸï¼ŒåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€è®¡ç®—æœºè§†è§‰å’Œç”Ÿç‰©ä¿¡æ¯å­¦ã€‚åœ¨è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸï¼Œå®ƒé€šå¸¸ç”¨äºæ–‡æœ¬åˆ†ç±»ï¼Œä¾‹å¦‚åƒåœ¾é‚®ä»¶æ£€æµ‹å’Œæƒ…æ„Ÿåˆ†æã€‚åœ¨è®¡ç®—æœºè§†è§‰ä¸­ï¼Œå®ƒç”¨äºå›¾åƒåˆ†ç±»å’Œç›®æ ‡æ£€æµ‹ã€‚åœ¨ç”Ÿç‰©ä¿¡æ¯å­¦ä¸­ï¼Œå®ƒç”¨äºè›‹ç™½è´¨åˆ†ç±»å’ŒåŸºå› é¢„æµ‹ã€‚

æ­£å¦‚æˆ‘ä»¬ä¸Šè¿°æ‰€è¿°ï¼Œæœ´ç´ è´å¶æ–¯åˆ†ç±»å™¨åŸºäºè´å¶æ–¯è§„åˆ™ï¼š

![](img/6e4c9aa0f9a2d31017169c48f72d3a12.png)

å…¶ä¸­ *ğ‘‹* æ˜¯è¾“å…¥ç‰¹å¾ï¼Œ*ğ‘Œ* æ˜¯è¾“å‡ºç±»åˆ«ï¼Œ*ğ¾* æ˜¯ç±»åˆ«çš„æ•°é‡ã€‚æ›´å…·ä½“åœ°è¯´ï¼Œ*ğ‘ƒ*(*ğ‘Œ*) è¡¨ç¤ºç±»åˆ«å…ˆéªŒåˆ†å¸ƒï¼Œ*ğ‘ƒ*(*ğ‘‹*|*ğ‘Œ*) æ˜¯è¾“å…¥çš„ç±»åˆ«æ¡ä»¶åˆ†å¸ƒï¼Œè€Œ *ğ‘ƒ*(*ğ‘Œ*|*ğ‘‹*) æ˜¯ç»™å®šè¾“å…¥ç‰¹å¾çš„ç±»åˆ«æ¦‚ç‡ã€‚

ç‹¬ç«‹æ€§å‡è®¾å¤§å¤§ç®€åŒ–äº†ç®—æ³•ï¼Œå› ä¸ºæˆ‘ä»¬ä¸éœ€è¦ä¼°è®¡å®Œæ•´çš„è”åˆåˆ†å¸ƒ *ğ‘ƒ*(*ğ‘‹*|*ğ‘Œ*=*ğ‘¦ğ‘˜*)ã€‚ç›¸åï¼Œç±»åˆ«æ¡ä»¶åˆ†å¸ƒå¯ä»¥å†™ä½œï¼š

![](img/b26ab4071d2731831e6640f443935d02.png)

å…¶ä¸­ *ğ‘“* è¡¨ç¤ºç‰¹å¾çš„æ•°é‡ã€‚

# å…ˆéªŒ

åœ¨æœ´ç´ è´å¶æ–¯ç®—æ³•ä¸­ï¼Œç±»åˆ«å…ˆéªŒåˆ†å¸ƒæ˜¯ä¸€ä¸ªæ¦‚ç‡åˆ†å¸ƒï¼Œæè¿°äº†è®­ç»ƒæ•°æ®ä¸­æ¯ä¸ªç±»åˆ«çš„æ¦‚ç‡ã€‚å®ƒæ˜¯ç®—æ³•çš„ä¸€ä¸ªåŸºæœ¬ç»„æˆéƒ¨åˆ†ï¼Œå› ä¸ºå®ƒç”¨äºè®¡ç®—ç»™å®šä¸€äº›è¯æ®çš„ç±»åˆ«åéªŒæ¦‚ç‡ã€‚

ç±»åˆ«å…ˆéªŒåˆ†å¸ƒå®šä¹‰ä¸ºç»™å®šè®­ç»ƒæ•°æ®ä¸­å®ä¾‹æ€»æ•°çš„ç±»åˆ«æ¦‚ç‡ã€‚å®ƒé€šå¸¸è¡¨ç¤ºä¸º *ğ‘ƒ*(*ğ‘Œ*=*ğ‘¦ğ‘˜*), å…¶ä¸­ *ğ‘˜* æ˜¯ç±»åˆ«æ ‡ç­¾ã€‚ç±»åˆ«å…ˆéªŒåˆ†å¸ƒé€šè¿‡è®­ç»ƒæ•°æ®ä¸­æ¯ä¸ªç±»åˆ«çš„ç›¸å¯¹é¢‘ç‡æ¥ä¼°è®¡ã€‚ä¾‹å¦‚ï¼Œå¦‚æœè®­ç»ƒæ•°æ®ä¸­æœ‰ 100 ä¸ªå®ä¾‹ï¼Œå…¶ä¸­ 60 ä¸ªå±äºç±»åˆ« Aï¼Œé‚£ä¹ˆç±»åˆ« A çš„å…ˆéªŒæ¦‚ç‡ä¼°è®¡ä¸º P(Y=A) = 0.6ã€‚

ç±»åˆ«å…ˆéªŒåˆ†å¸ƒåœ¨æœ´ç´ è´å¶æ–¯ç®—æ³•ä¸­èµ·ç€è‡³å…³é‡è¦çš„ä½œç”¨ï¼Œå› ä¸ºå®ƒç”¨äºè®¡ç®—åœ¨ç»™å®šä¸€äº›è¯æ®çš„æƒ…å†µä¸‹æŸä¸€ç±»åˆ«çš„åéªŒæ¦‚ç‡ã€‚åéªŒæ¦‚ç‡çš„è®¡ç®—æ˜¯å°†ç±»åˆ«å…ˆéªŒå’Œåœ¨ç»™å®šç±»åˆ«ä¸‹è¯æ®çš„ä¼¼ç„¶åº¦ç›¸ä¹˜ï¼Œå¹¶é€šè¿‡è¯æ®çš„è¾¹é™…ä¼¼ç„¶åº¦è¿›è¡Œå½’ä¸€åŒ–ã€‚æ¢å¥è¯è¯´ï¼Œç±»åˆ«å…ˆéªŒåˆ†å¸ƒä½œä¸ºä¸€ä¸ªæƒé‡å› å­ï¼Œè°ƒæ•´ä¼¼ç„¶å‡½æ•°çš„ç›¸å¯¹é‡è¦æ€§ã€‚

ç„¶è€Œï¼Œå¦‚æœç±»åˆ«å…ˆéªŒåˆ†å¸ƒæ˜¯ä»æœ‰åçš„è®­ç»ƒæ•°æ®ä¸­ä¼°è®¡çš„ï¼Œå®ƒå¯èƒ½ä¼šå¯¼è‡´ç®—æ³•æ€§èƒ½ä¸ä½³ï¼Œç‰¹åˆ«æ˜¯å½“æµ‹è¯•æ•°æ®æ¥è‡ªä¸åŒçš„åˆ†å¸ƒæ—¶ã€‚è¿™è¢«ç§°ä¸ºç±»åˆ«ä¸å¹³è¡¡é—®é¢˜ï¼Œå¯ä»¥é€šè¿‡ä½¿ç”¨è¿‡é‡‡æ ·ã€æ¬ é‡‡æ ·æˆ–åˆæˆæ•°æ®ç”Ÿæˆç­‰æŠ€æœ¯æ¥ç¼“è§£ã€‚

ç±»åˆ«å…ˆéªŒåˆ†å¸ƒæ˜¯å±äºç±»åˆ« *ğ‘˜* çš„æ•°æ®ç¤ºä¾‹çš„æ¯”ä¾‹ã€‚æˆ‘ä»¬å¯ä»¥å°†å…¶å†™æˆä»¥ä¸‹å½¢å¼ï¼š

![](img/28dbed717467724e2cbd6b707474dcab.png)

å…¶ä¸­ï¼Œ*ğ‘›* è¡¨ç¤ºç¬¬ *ğ‘›* ä¸ªæ•°æ®é›†ç¤ºä¾‹ï¼Œ*ğ‘* æ˜¯æ•°æ®é›†ä¸­ç¤ºä¾‹çš„æ€»æ•°ï¼Œ*ğ›¿* æ˜¯å…‹ç½—å†…å…‹Î´å‡½æ•°ï¼ˆå½“ç±»åˆ«åŒ¹é…æ—¶è¿”å› 1ï¼Œå¦åˆ™è¿”å› 0ï¼‰ã€‚å®ƒè¿”å›ä¸€ä¸ªä¸ *ğ‘ƒ*(*ğ‘Œ*=*ğ‘¦ğ‘˜*) ç›¸å¯¹åº”çš„åˆ†ç±»åˆ†å¸ƒã€‚

```py
def prior_fn(y):
    n_classes = np.unique(y).shape[0]
    counts = np.zeros(n_classes)
    for c_k in range(n_classes):
        counts[c_k] = np.sum(np.where(y==c_k, 1, 0))
        priors = counts/np.sum(counts)
    dist = tfd.Categorical(probs=priors)
    return dist

prior = prior_fn(y_train)
prior

<tfp.distributions.Categorical 'Categorical' batch_shape=[] event_shape=[] dtype=int32>
```

è®©æˆ‘ä»¬ç»˜åˆ¶æˆ‘ä»¬çš„å…ˆéªŒåˆ†å¸ƒã€‚

```py
plt.bar([0, 1, 2], prior.probs.numpy(), color=label_colors)
plt.xlabel("Class")
plt.ylabel("Prior probability")
plt.title("Class prior distribution")
plt.xticks([0, 1, 2],)
plt.show()
```

![](img/91631c4fe3d458c9e623ebd6cefb3003.png)

å›¾ 4ï¼šæ¯ä¸ªç›®æ ‡ç±»åˆ«çš„å…ˆéªŒæ¦‚ç‡ã€‚

# ä¼¼ç„¶æ€§

åœ¨æœ´ç´ è´å¶æ–¯ç®—æ³•ä¸­ï¼Œç±»åˆ«æ¡ä»¶å¯†åº¦æ˜¯æè¿°ç»™å®šç±»åˆ«æ ‡ç­¾ä¸‹æ¯ä¸ªç‰¹å¾çš„ä¼¼ç„¶æ€§çš„æ¦‚ç‡åˆ†å¸ƒã€‚å®ƒä»¬ç”¨äºè®¡ç®—åœ¨ç»™å®šä¸€äº›è¯æ®çš„æƒ…å†µä¸‹æŸä¸€ç±»åˆ«çš„åéªŒæ¦‚ç‡ï¼Œæ˜¯ç®—æ³•çš„ä¸€ä¸ªåŸºæœ¬ç»„æˆéƒ¨åˆ†ã€‚ç±»åˆ«æ¡ä»¶å¯†åº¦å®šä¹‰ä¸ºç»™å®šç±»åˆ«æ ‡ç­¾ä¸‹æ¯ä¸ªç‰¹å¾çš„æ¦‚ç‡å¯†åº¦å‡½æ•°ï¼ˆpdfï¼‰ã€‚å®ƒä»¬é€šå¸¸è¡¨ç¤ºä¸º *ğ‘ƒ*(*ğ‘‹ğ‘–*|*ğ‘Œ*=*ğ‘¦ğ‘˜*), å…¶ä¸­ *ğ‘‹ğ‘–* æ˜¯ä¸€ä¸ªç‰¹å¾ï¼Œ*ğ‘˜* æ˜¯ç±»åˆ«æ ‡ç­¾ã€‚ç±»åˆ«æ¡ä»¶å¯†åº¦æ˜¯é€šè¿‡å„ç§æŠ€æœ¯ä»è®­ç»ƒæ•°æ®ä¸­ä¼°è®¡å‡ºæ¥çš„ï¼Œå…·ä½“å–å†³äºæ•°æ®çš„ç±»å‹ã€‚ä¾‹å¦‚ï¼Œå¯¹äºè¿ç»­æ•°æ®ï¼Œç±»åˆ«æ¡ä»¶å¯†åº¦å¯ä»¥ä½¿ç”¨é«˜æ–¯åˆ†å¸ƒä¼°è®¡ï¼Œè€Œå¯¹äºç¦»æ•£æ•°æ®ï¼Œå¯ä»¥ä½¿ç”¨å¤šé¡¹å¼åˆ†å¸ƒæˆ–ä¼¯åŠªåˆ©åˆ†å¸ƒè¿›è¡Œä¼°è®¡ã€‚æ­£å¦‚æˆ‘ä»¬ä¹‹å‰æ‰€è¿°ï¼Œåœ¨æˆ‘ä»¬çš„æ¡ˆä¾‹ä¸­ï¼Œæˆ‘ä»¬æœ‰è¿ç»­ç‰¹å¾ï¼Œå› æ­¤æˆ‘ä»¬å°†æ¢ç´¢é«˜æ–¯æ–¹æ³•ã€‚

ç±»åˆ«æ¡ä»¶å¯†åº¦åœ¨æœ´ç´ è´å¶æ–¯ç®—æ³•ä¸­èµ·ç€å…³é”®ä½œç”¨ï¼Œå› ä¸ºå®ƒä»¬ç”¨äºè®¡ç®—ç»™å®šç±»åˆ«æ ‡ç­¾ä¸‹è¯æ®çš„ä¼¼ç„¶æ€§ã€‚è¿™ä¸€ä¼¼ç„¶æ€§æ˜¯é€šè¿‡è¯„ä¼°è¯æ®çš„æ¯ä¸ªç‰¹å¾çš„ç±»åˆ«æ¡ä»¶å¯†åº¦æ¥è®¡ç®—çš„ï¼Œç„¶åå°†å®ƒä»¬ç›¸ä¹˜ã€‚ç±»åˆ«æ¡ä»¶å¯†åº¦ä½œä¸ºä¸€ä¸ªæƒé‡å› å­ï¼Œè°ƒæ•´æ¯ä¸ªç‰¹å¾åœ¨åˆ†ç±»ä»»åŠ¡ä¸­çš„ç›¸å¯¹é‡è¦æ€§ã€‚

ç°åœ¨æ˜¯å®šä¹‰*ğ‘ƒ*(*ğ‘‹*|*ğ‘Œ*)â€”â€”è¾“å…¥çš„ç±»åˆ«æ¡ä»¶åˆ†å¸ƒçš„æ—¶å€™äº†ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬ä½¿ç”¨å•å˜é‡é«˜æ–¯åˆ†å¸ƒï¼ˆè¯·è®°ä½ç‹¬ç«‹æ€§å‡è®¾ï¼‰ï¼š

![](img/8a42de9e92b88c95baceb8575bd21346.png)

å…¶ä¸­*ğœ‡ğ‘–ğ‘˜*å’Œ*ğœğ‘–ğ‘˜*æ˜¯éœ€è¦ä¼°è®¡çš„å‚æ•°ã€‚ä½¿ç”¨æœ€å¤§ä¼¼ç„¶ä¼°è®¡ï¼Œä¼°è®¡å€¼å°±æ˜¯æ¯ä¸ªç±»åˆ«æ ·æœ¬æ•°æ®ç‚¹çš„å‡å€¼å’Œæ–¹å·®ï¼š

![](img/2cce666011e1a0307f40a86c6cf120e7.png)

```py
def class_conditionals_fn(x, y):
    n_classes = np.unique(y).shape[0]
    n_features = x.shape[1]
    counts = np.zeros(n_classes)
    mean_feature_given_class = []
    std_feature_given_class = []
    for c_k in range(n_classes):
        mean_feature_given_class.append(np.mean(x[np.squeeze(y==c_k)], axis=0))
        std_feature_given_class.append(np.std(x[np.squeeze(y==c_k)], axis=0))

    class_cond = tfd.MultivariateNormalDiag(loc = np.asarray(mean_feature_given_class).reshape(n_classes, n_features),
                             scale_diag=np.asarray(std_feature_given_class).reshape(n_classes, n_features))

    return class_cond

class_conditionals = class_conditionals_fn(x_train, y_train)
class_conditionals

<tfp.distributions.MultivariateNormalDiag 'MultivariateNormalDiag' batch_shape=[3] event_shape=[2] dtype=float64>
```

ä¸‹æ–¹çš„ç­‰é«˜çº¿å›¾å±•ç¤ºäº†ç±»åˆ«æ¡ä»¶å¯†åº¦ã€‚è¯·æ³¨æ„æ¯ä¸ªåˆ†å¸ƒçš„ç­‰é«˜çº¿å¦‚ä½•å¯¹åº”äºå…·æœ‰å¯¹è§’åæ–¹å·®çŸ©é˜µçš„é«˜æ–¯åˆ†å¸ƒï¼Œå› ä¸ºæ¨¡å‹å‡è®¾åœ¨ç»™å®šç±»åˆ«çš„æƒ…å†µä¸‹æ¯ä¸ªç‰¹å¾æ˜¯ç‹¬ç«‹çš„ã€‚

```py
def contour_plot(x0_range, x1_range, prob_fn, batch_shape, colors, levels=None, num_points=100):
    x0 = np.linspace(x0_range[0], x0_range[1], num_points)
    x1 = np.linspace(x1_range[0], x1_range[1], num_points)
    X0, X1= np.meshgrid(x0, x1)
    Z = prob_fn(np.expand_dims(np.array([X0.ravel(), X1.ravel()]).T, 1))
    Z = np.array(Z).T.reshape(batch_shape, *X0.shape)
    for batch in np.arange(batch_shape):
        if levels:
            plt.contourf(X0, X1, Z[batch], alpha=0.2, colors=colors, levels=levels)
        else:
            plt.contour(X0, X1, Z[batch], colors=colors[batch], alpha=0.3)

plt.figure(figsize=(10, 6))
plot_data(x_train, y_train, alpha=0.3)
x0_min, x0_max = x_train[:, 0].min()-0.2, x_train[:, 0].max()+0.2
x1_min, x1_max = x_train[:, 1].min()-0.2, x_train[:, 1].max()+0.2
contour_plot((x0_min, x0_max), (x1_min, x1_max), class_conditionals.prob, 3, label_colors)
plt.title("Training set with class-conditional density contours")
plt.show()
```

![](img/7b525c510c0f1d96f30e2b6f52f72add.png)

å›¾ 5ï¼šå¸¦æœ‰ç±»åˆ«æ¡ä»¶å¯†åº¦ç­‰é«˜çº¿çš„è®­ç»ƒé›†ã€‚

åœ¨æ‰§è¡Œä¸Šè¿°è®¡ç®—åï¼Œç®—æ³•çš„æœ€åä¸€æ­¥æ˜¯é¢„æµ‹æ–°çš„æ•°æ®è¾“å…¥*ğ‘‹*Ìƒ :=(*ğ‘‹*Ìƒ 1,â€¦,*ğ‘‹*Ìƒ *ğ‘“*)çš„ç±»åˆ«*ğ‘Œ*Ì‚ã€‚å¯ä»¥é€šè¿‡ä»¥ä¸‹æ–¹å¼å®Œæˆï¼š

![](img/7a121752ff358a9a4e89d09cbecb08f4.png)

```py
def predict_class(prior, class_conditionals, x):
    log_prob_list = []
    for sample in x:
        cond_probs = class_conditionals.log_prob(sample)
        joint_likelihood = tf.add(prior.probs.numpy(), cond_probs)
        norm_factor = tf.math.reduce_logsumexp(joint_likelihood, axis=-1, keepdims=True)
        log_prob = joint_likelihood - norm_factor
        log_prob_list.append(log_prob)
    return np.argmax(np.asarray(log_prob_list), axis=-1)

predictions = predict_class(prior, class_conditionals, x_test)
```

# ç»“æœ

åœ¨è¿™ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬åº”ç”¨äº†æœ´ç´ è´å¶æ–¯ç®—æ³•æ¥æ ¹æ®é€‰å®šçš„ç‰¹å¾å¯¹è‘¡è„é…’æ ·æœ¬è¿›è¡Œåˆ†ç±»ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬ä½¿ç”¨äº†ä¸¤ä¸ªç‰¹å¾ï¼šè‰²è°ƒå’Œé…’ç²¾ï¼Œæ¥é¢„æµ‹è‘¡è„é…’çš„ç±»åˆ«ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨è¿™é¡¹ä»»åŠ¡ä¸­çš„å‡†ç¡®ç‡è¶…è¿‡äº† 91%ã€‚

```py
accuracy = accuracy_score(y_test, predictions)
print("Test accuracy: {:.4f}".format(accuracy))

Test accuracy: 0.9167
```

ä¸ºäº†è¿›ä¸€æ­¥åˆ†ææ¨¡å‹çš„æ€§èƒ½ï¼Œæˆ‘ä»¬è¿˜ç»˜åˆ¶äº†æ¨¡å‹çš„å†³ç­–åŒºåŸŸï¼Œå³åˆ†éš”ä¸åŒç±»åˆ«çš„è¾¹ç•Œã€‚å†³ç­–åŒºåŸŸæœ‰åŠ©äºå¯è§†åŒ–ç®—æ³•æ‰§è¡Œçš„ç±»åˆ«åˆ†ç¦»ã€‚å¦‚æˆ‘ä»¬æ‰€è§ï¼Œæ¨¡å‹èƒ½å¤Ÿç›¸å½“æœ‰æ•ˆåœ°åˆ†éš”æ•°æ®é›†ä¸­çš„ä¸‰ä¸ªç±»åˆ«ã€‚

å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæœ´ç´ è´å¶æ–¯ç®—æ³•å‡è®¾ç‰¹å¾ä¹‹é—´æ˜¯ç‹¬ç«‹çš„ï¼Œè¿™åœ¨å®é™…åœºæ™¯ä¸­å¯èƒ½ä¸æˆç«‹ã€‚ç‰¹å¾ä¹‹é—´çš„ç›¸å…³æ€§å¯ä»¥å¸®åŠ©æé«˜æ¨¡å‹çš„å‡†ç¡®æ€§ã€‚å› æ­¤ï¼Œè€ƒè™‘åœ¨æ¨¡å‹ç‰¹å¾ä¹‹é—´å¼•å…¥ç›¸å…³æ€§å¯èƒ½æœ‰åŠ©äºæå‡æ€§èƒ½ã€‚æ­¤å¤–ï¼Œè¿˜å¯ä»¥è€ƒè™‘å…è®¸ç‰¹å¾ä¹‹é—´ç›¸å…³æ€§çš„å…¶ä»–ç®—æ³•æ¥æ”¹å–„ç»“æœã€‚

```py
plt.figure(figsize=(10, 6))
plot_data(x_train, y_train)
x0_min, x0_max = x_train[:, 0].min()-0.2, x_train[:, 0].max()+0.2
x1_min, x1_max = x_train[:, 1].min()-0.2, x_train[:, 1].max()+0.2
contour_plot((x0_min, x0_max), (x1_min, x1_max), 
             lambda x: predict_class(prior, class_conditionals, x), 
             1, label_colors, levels=[-0.5, 0.5, 1.5, 2.5, 3.5],
             num_points=200)
plt.title("Training set with decision regions")
plt.show()
```

![](img/24462cec3281dc09ff436f5997e1665d.png)

å›¾ 6ï¼šè®­ç»ƒé›†å†³ç­–åŒºåŸŸã€‚

# ç»“è®º

åœ¨è¿™ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨ TensorFlow Probability ä»é›¶å¼€å§‹å®ç°äº†æœ´ç´ è´å¶æ–¯ç®—æ³•ã€‚æˆ‘ä»¬å°†å…¶åº”ç”¨äºä½¿ç”¨è‘¡è„é…’æ ·æœ¬æ•°æ®é›†çš„åˆ†ç±»ä»»åŠ¡ã€‚æˆ‘ä»¬é€‰æ‹©äº†ä¸¤ä¸ªç‰¹å¾ï¼Œè‰²è°ƒå’Œé…’ç²¾ï¼Œæ¥é¢„æµ‹è‘¡è„é…’çš„ç±»åˆ«ï¼Œå¹¶ä¸”è¾¾åˆ°äº†è¶…è¿‡ 91%çš„å‡†ç¡®ç‡ã€‚æˆ‘ä»¬è¿˜å¯è§†åŒ–äº†æ¨¡å‹çš„å†³ç­–åŒºåŸŸï¼Œè¿™æœ‰åŠ©äºç†è§£ç®—æ³•æ‰§è¡Œçš„ç±»åˆ«åˆ†ç¦»ã€‚

è¿™ä¸ªç®€å•çš„ä¾‹å­å±•ç¤ºäº†æœ´ç´ è´å¶æ–¯ç®—æ³•åœ¨åˆ†ç±»ä»»åŠ¡ä¸­çš„ç®€å•æ€§å’Œæœ‰æ•ˆæ€§ã€‚ç„¶è€Œï¼Œæœ´ç´ è´å¶æ–¯ç®—æ³•å‡è®¾ç‰¹å¾ä¹‹é—´æ˜¯ç‹¬ç«‹çš„ï¼Œè¿™åœ¨å®é™…åœºæ™¯ä¸­å¯èƒ½å¹¶ä¸æˆç«‹ã€‚

# å‚è€ƒæ–‡çŒ®å’Œèµ„æ–™

[1] â€” [è‘¡è„é…’æ•°æ®é›†](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_wine.html#sklearn.datasets.load_wine)

[2] â€” [Coursera: æ·±åº¦å­¦ä¹ ä¸“ä¸šè¯¾ç¨‹](https://www.coursera.org/specializations/deep-learning)

[3] â€” [Coursera: TensorFlow 2 æ·±åº¦å­¦ä¹ ](https://www.coursera.org/specializations/tensorflow2-deeplearning) ä¸“ä¸šè¯¾ç¨‹

[4] â€” [TensorFlow æ¦‚ç‡æŒ‡å—ä¸æ•™ç¨‹](https://www.tensorflow.org/probability/overview)

[5] â€” [TensorFlow åšå®¢ä¸­çš„ TensorFlow æ¦‚ç‡å¸–å­](https://blog.tensorflow.org/search?label=TensorFlow+Probability&max-results=20)
