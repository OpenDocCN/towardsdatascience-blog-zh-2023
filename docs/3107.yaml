- en: Activation Functions For Neural Networks & Deep Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/activation-functions-non-linearity-neural-networks-101-ab0036a2e701?source=collection_archive---------8-----------------------#2023-10-12](https://towardsdatascience.com/activation-functions-non-linearity-neural-networks-101-ab0036a2e701?source=collection_archive---------8-----------------------#2023-10-12)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Explaining why neural networks can learn (nearly) anything and everything
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@egorhowell?source=post_page-----ab0036a2e701--------------------------------)[![Egor
    Howell](../Images/1f796e828f1625440467d01dcc3e40cd.png)](https://medium.com/@egorhowell?source=post_page-----ab0036a2e701--------------------------------)[](https://towardsdatascience.com/?source=post_page-----ab0036a2e701--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----ab0036a2e701--------------------------------)
    [Egor Howell](https://medium.com/@egorhowell?source=post_page-----ab0036a2e701--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F1cac491223b2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Factivation-functions-non-linearity-neural-networks-101-ab0036a2e701&user=Egor+Howell&userId=1cac491223b2&source=post_page-1cac491223b2----ab0036a2e701---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----ab0036a2e701--------------------------------)
    ·8 min read·Oct 12, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fab0036a2e701&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Factivation-functions-non-linearity-neural-networks-101-ab0036a2e701&user=Egor+Howell&userId=1cac491223b2&source=-----ab0036a2e701---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fab0036a2e701&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Factivation-functions-non-linearity-neural-networks-101-ab0036a2e701&source=-----ab0036a2e701---------------------bookmark_footer-----------)![](../Images/f20f48d260492d8ecb7fb2bf39ec6862.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Machine learning icons created by Becris — Flatico. [https://www.flaticon.com/free-icons/machine-learning](https://www.flaticon.com/free-icons/machine-learning)
  prefs: []
  type: TYPE_NORMAL
- en: Background
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In my previous article, we introduced the [***multi-layer perceptron***](https://en.wikipedia.org/wiki/Multilayer_perceptron)
    ***(MLP)****,* which is just a set of stacked interconnected [***perceptrons***](https://en.wikipedia.org/wiki/Perceptron).
    I highly recommend you check my previous post if you are unfamiliar with the perceptron
    and MLP as will discuss it quite a bit in this article:'
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://levelup.gitconnected.com/intro-perceptron-architecture-neural-networks-101-2a487062810c?source=post_page-----ab0036a2e701--------------------------------)
    [## Intro, Perceptron & Architecture: Neural Networks 101'
  prefs: []
  type: TYPE_NORMAL
- en: An introduction to Neural Networks and their building blocks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: levelup.gitconnected.com](https://levelup.gitconnected.com/intro-perceptron-architecture-neural-networks-101-2a487062810c?source=post_page-----ab0036a2e701--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: 'An example MLP with two hidden layers is shown below:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/94cdac9897a5e852d9cd6edd425fcba0.png)'
  prefs: []
  type: TYPE_IMG
- en: A basic two-hidden multi-layer perceptron. Diagram by author.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, the problem with the MLP is that it can only fit a [***linear classifier***](https://en.wikipedia.org/wiki/Linear_separability).
    This is because the individual perceptrons have a [***step function***](https://en.wikipedia.org/wiki/Step_function)
    as their [***activation function***](https://en.wikipedia.org/wiki/Activation_function),
    which is linear:'
  prefs: []
  type: TYPE_NORMAL
