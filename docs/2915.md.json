["```py\nimport requests\nfrom io import BytesIO\nimport pandas as pd\n\nurl = \"https://raw.githubusercontent.com/tomonori-masui/entity-resolution/main/data/musicbrainz_200k.csv\"\nres = requests.get(url)\ndf = pd.read_csv(BytesIO(res.content))\n```", "```py\nenglish_cids = df[\n    df.language.str.lower().str.contains(\"^en|^eg\", na=False)\n].CID.unique()\n\ndf = df[df.CID.isin(english_cids)].reset_index(drop=True)\n```", "```py\nfor col in [\"title\", \"artist\", \"album\"]:\n    df[col] = (\n        df[col]\n        .str.lower()\n        .replace(\"[^a-z0-9]\", \" \", regex=True)  # replacing special characters with a space\n        .replace(\" +\", \" \", regex=True)         # removing consecutive spaces\n        .str.strip()                            # removing leading and tailing spaces\n    )\n\ndf.loc[df.number.notna(), \"number\"] = (\n    df[df.number.notna()]\n    .number.replace(\"[^0-9]\", \"\", regex=True)              # removing non-digits\n    .apply(lambda x: str(int(x)) if len(x) > 0 else None)  # removing leading zeros\n)\n```", "```py\nfrom collections import defaultdict\n\ndef standard_blocking(field_values: pd.Series) -> dict[str, list]:\n\n    blocks = defaultdict(list)\n    for idx, key in enumerate(field_values):\n        if key is not None:\n            blocks[key].append(idx)\n\n    return blocks\n```", "```py\nsb_title = standard_blocking(df.title)\nsb_artist = standard_blocking(df.artist)\nsb_album = standard_blocking(df.album)\n```", "```py\nfrom nltk.tokenize import word_tokenize\n\ndef token_blocking(df: pd.DataFrame, stop_words: set) -> dict[str, list]:\n\n    blocks = defaultdict(list)\n\n    for i, row in enumerate(df.itertuples()):\n\n        # concatenate columns and tokenize\n        string = \" \".join([str(value) for value in row if not pd.isna(value)])\n        tokens = set(\n            [word for word in word_tokenize(string) if word not in stop_words]\n        )\n\n        # create blocks\n        for token in tokens:\n            blocks[token].append(i)\n\n    return blocks\n```", "```py\nimport string\nfrom nltk.corpus import stopwords\n\ncolumns = ['title', 'artist', 'album']\nstop_words = set(stopwords.words('english') + list(string.punctuation))\ntoken_blocks = token_blocking(df[columns], stop_words)\n```", "```py\ndef sorted_neighborhood(\n    df: pd.DataFrame, keys: list, window_size: int = 3\n) -> np.ndarray:\n\n    sorted_indices = (\n        df[keys].dropna(how=\"all\").sort_values(keys).index.tolist()\n    )\n    pairs = []\n    for window_end in range(1, len(sorted_indices)):\n        window_start = max(0, window_end - window_size)\n        for i in range(window_start, window_end):\n            pairs.append([sorted_indices[i], sorted_indices[window_end]])\n\n    return np.array(pairs)\n\ncolumns = ['title', 'artist', 'album']\nsn_pairs = sorted_neighborhood(df, columns)\n```", "```py\ndef purge_blocks(\n    blocks: dict[str, list], purging_threshold: int = 1000\n) -> dict[str, list]:\n\n    blocks_purged = {\n        key: indices\n        for key, indices in blocks.items()\n        if len(indices) < purging_threshold and len(indices) > 1\n    }\n\n    return blocks_purged\n\ntoken_blocks = purge_blocks(token_blocks)\nsb_title = purge_blocks(sb_title)\nsb_artist = purge_blocks(sb_artist)\nsb_album = purge_blocks(sb_album)\n```", "```py\nimport itertools\nfrom scipy.sparse import csr_matrix\n\ndef get_pairs_from_blocks(blocks: dict[str, list]) -> list[list]:\n    return [\n        pair\n        for indices in blocks.values()\n        for pair in list(itertools.combinations(indices, 2))\n    ]\n\ndef get_adjacency_matrix_from_pairs(\n    pairs: list[list], matrix_shape: tuple[int, int]\n) -> csr_matrix:\n\n    idx1 = [pair[0] for pair in pairs]\n    idx2 = [pair[1] for pair in pairs]\n    ones = np.ones(len(idx1))\n\n    return csr_matrix(\n        (ones, (idx1, idx2)), shape=matrix_shape, dtype=np.int8\n    )\n\npairs = get_pairs_from_blocks(token_blocks)\nadj_matrix = get_adjacency_matrix_from_pairs(pairs, (len(df), len(df)))\n```", "```py\ndef prune_edges(\n    adj_matrix: csr_matrix,\n    edge_weight_threshold: float,\n) -> csr_matrix:\n\n    adj_matrix_pruned = adj_matrix >= edge_weight_threshold\n\n    return adj_matrix_pruned\n\nadj_matrix = prune_edges(adj_matrix, edge_weight_threshold=2)\n```", "```py\ndef get_pairs_from_adj_matrix(adjacency_matrix: csr_matrix) -> np.ndarray:\n    return np.array(adjacency_matrix.nonzero()).T\n\ntb_pairs = get_pairs_from_adj_matrix(adj_matrix)\n```", "```py\nadj_matrix_list = []\nfor blocks in [sb_title, sb_artist, sb_album]:\n    pairs = get_pairs_from_blocks(blocks)\n    adj_matrix_list.append(\n        get_adjacency_matrix_from_pairs(pairs, (len(df), len(df)))\n    )\n```", "```py\ndef get_union_of_adj_matrices(adj_matrix_list: list) -> csr_matrix:\n\n    adj_matrix = csr_matrix(adj_matrix_list[0].shape)\n    for matrix in adj_matrix_list:\n        adj_matrix += matrix\n\n    return adj_matrix\n\nadj_matrix_union = get_union_of_adj_matrices(adj_matrix_list)\nsb_pairs = get_pairs_from_adj_matrix(adj_matrix_union)\n```", "```py\nfrom sklearn.preprocessing import normalize\nfrom sklearn.feature_extraction.text import CountVectorizer\n\ndef get_field_similarity_scores(\n    df: pd.DataFrame, pairs: np.ndarray, field_config: dict[str, str]\n) -> dict[str, np.ndarray]:\n    \"\"\"\n    Measuring similarity by field. It is either cosine similarity\n    (if sim_type == 'fuzzy') or exact match 0/1 (if sim_type == 'exact'). \n    Attribute's similarity scores are stored in field_score dictionary \n    with the field name as key.\n    \"\"\"\n\n    field_scores = {}\n\n    for field, sim_type in field_config.items():\n        if sim_type == \"fuzzy\":\n            field_scores[field] = cosine_similarities(\n                df[field].fillna(\"\"), pairs\n            )\n        else:\n            field_scores[field] = exact_matches(df[field], pairs)\n\n    return field_scores\n\ndef cosine_similarities(\n    field_values: pd.Series, pairs: np.ndarray\n) -> np.ndarray:\n    \"\"\"\n    Computing cosine similarities on pairs\n    \"\"\"\n\n    token_matrix_1, token_matrix_2 = get_token_matrix_pair(\n        field_values, pairs\n    )\n    cos_sim = cosine_similarities_on_pair_matrices(\n        token_matrix_1, token_matrix_2\n    )\n\n    return cos_sim\n\ndef get_token_matrix_pair(\n    field_values: pd.Series, pairs: np.ndarray,\n) -> tuple[csr_matrix, csr_matrix]:\n    \"\"\"\n    Converting pairs into matrices of token counts (matrix of records \n    by tokens filled with token counts). \n    \"\"\"\n\n    all_idx = np.unique(pairs)\n    vectorizer = CountVectorizer(analyzer=\"char\", ngram_range=(3, 3))\n    vectorizer.fit(field_values.loc[all_idx])\n    token_matrix_1 = vectorizer.transform(field_values.loc[pairs[:, 0]])\n    token_matrix_2 = vectorizer.transform(field_values.loc[pairs[:, 1]])\n\n    return token_matrix_1, token_matrix_2\n\ndef cosine_similarities_on_pair_matrices(\n    token_matrix_1: csr_matrix, token_matrix_2: csr_matrix\n) -> np.ndarray:\n    \"\"\"\n    Computing cosine similarities on pair of token count matrices.\n    It normalizes each record (axis=1) first, then computes dot product\n    for each pair of records.\n    \"\"\"\n\n    token_matrix_1 = normalize(token_matrix_1, axis=1)\n    token_matrix_2 = normalize(token_matrix_2, axis=1)\n    cos_sim = np.asarray(\n        token_matrix_1.multiply(token_matrix_2).sum(axis=1)\n    ).flatten()\n\n    return cos_sim\n\ndef exact_matches(\n    field_values: pd.Series, pairs: np.ndarray\n) -> np.ndarray:\n    \"\"\"\n    Performing exact matches on pairs\n    \"\"\"\n\n    arr1 = field_values.loc[pairs[:, 0]].values\n    arr2 = field_values.loc[pairs[:, 1]].values\n\n    return ((arr1 == arr2) & (~pd.isna(arr1)) & (~pd.isna(arr2))).astype(int)\n\nfield_config = {\n    # <field>: <sim_type>\n    \"title\": \"fuzzy\",\n    \"artist\": \"fuzzy\",\n    \"album\": \"fuzzy\",\n    \"number\": \"exact\",\n}\n\nfield_scores_sb = get_field_similarity_scores(df, sb_pairs, field_config)\n```", "```py\ndef calc_overall_scores(field_scores: dict[str, np.ndarray]) -> np.ndarray:\n    return np.array(list(field_scores.values())).mean(axis=0)\n\ndef find_matches(scores: np.ndarray, threshold: float) -> np.ndarray:\n    return scores >= threshold\n\nscores_sb = calc_overall_scores(field_scores_sb)\nis_matched_sb = find_matches(scores_sb, threshold=0.64)\n```", "```py\nfrom IPython.display import display\nfrom collections import Counter\n\ndef show_results(\n    is_matched_list: list[np.ndarray],\n    blocking_approach_name_list: list[str],\n):\n\n    result = pd.DataFrame(\n        [Counter(is_matched).values() for is_matched in is_matched_list],\n        columns=[\"Unmatch\", \"Match\"],\n    )\n    result[\"Blocking Approach\"] = blocking_approach_name_list\n    result[\"Matching Rate\"] = result.Match / (\n        result.Match + result.Unmatch\n    )\n    result[\"Matching Rate\"] = result[\"Matching Rate\"].map(\"{:.1%}\".format)\n    result[\"Match\"] = result[\"Match\"].map(\"{:,}\".format)\n    result[\"Unmatch\"] = result[\"Unmatch\"].map(\"{:,}\".format)\n\n    display(\n        result[[\"Blocking Approach\", \"Match\", \"Unmatch\", \"Matching Rate\"]]\n    )\n\nis_matched_list = [is_matched_sb, is_matched_tb, is_matched_sn]\nblocking_approach_name_list = [\n    \"Standard Blocking\",\n    \"Token Blocking\",\n    \"Sorted Neighborhood\",\n]\nshow_results(is_matched_list, blocking_approach_name_list)\n```", "```py\ndef get_x_y(\n    field_scores: dict[str, np.ndarray],\n    pairs: np.ndarray,\n    df: pd.DataFrame,\n) -> tuple[pd.DataFrame, np.ndarray]:\n\n    X = pd.DataFrame(field_scores)\n    y = df.loc[pairs[:, 0], \"CID\"].values == df.loc[pairs[:, 1], \"CID\"].values\n\n    return X, y\n\nX, y = get_x_y(field_scores_tb, tb_pairs, df)\n```", "```py\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.5, random_state=42\n)\n\nmodel = LogisticRegression(random_state=0).fit(X_train, y_train)\n```", "```py\nfrom sklearn.metrics import f1_score\n\ny_pred = model.predict(X_test)\nprint(f\"Model f1_score: {f1_score(y_test, y_pred):.3f}\")\n\ny_rule_base = is_matched_tb[X_test.index.values]\nprint(f\"Rule-base f1_score: {f1_score(y_test, y_rule_base):.3f}\")\n```", "```py\nmatched_pairs = tb_pairs[is_matched_tb]\nmatched_scores = scores_tb[is_matched_tb]\n```", "```py\nfrom scipy.sparse.csgraph import connected_components\n\ndef connected_components_from_pairs(\n    pairs: np.ndarray, dim: int\n) -> np.ndarray:\n\n    adjacency_matrix = get_adjacency_matrix_from_pairs(pairs, (dim, dim))\n    _, clusters = connected_components(\n        csgraph=adjacency_matrix, directed=False, return_labels=True\n    )\n\n    return clusters\n\ncc_clusters = connected_components_from_pairs(matched_pairs, len(df))\n```", "```py\ndef sort_pairs(pairs: np.ndarray, scores: np.ndarray) -> np.ndarray:\n    sorted_ids = (-1 * scores).argsort()\n    return pairs[sorted_ids]\n\npairs_sored = sort_pairs(matched_pairs, matched_scores)\n```", "```py\ndef get_center_cluster_pairs(pairs, dim):\n\n    \"\"\"\n    cluster_centers: \n        a list tracking cluster center for each record.\n        indices of the list correspond to the original df indices\n        and the values represent assigned cluster centers' indices\n    center_cluster_pairs: \n        a list of pairs of indices representing center-child pairs\n    merge_cluster_pairs:\n        a list of pairs of merged nodes' indices\n    \"\"\"\n    cluster_centers = [None] * dim\n    center_cluster_pairs = []\n    merge_cluster_pairs = []\n\n    for idx1, idx2 in pairs:\n\n        if (\n            cluster_centers[idx1] is None\n            or cluster_centers[idx1] == idx1\n            or cluster_centers[idx2] is None\n            or cluster_centers[idx2] == idx2\n        ):\n            # if both aren't child, those nodes are merged\n            merge_cluster_pairs.append([idx1, idx2])\n\n        if cluster_centers[idx1] is None and cluster_centers[idx2] is None:\n            # if both weren't seen before, idx1 becomes center and idx2 gets child\n            cluster_centers[idx1] = idx1\n            cluster_centers[idx2] = idx1\n            center_cluster_pairs.append([idx1, idx2])\n        elif cluster_centers[idx2] is None:\n            if cluster_centers[idx1] == idx1:\n                # if idx1 is center, idx2 is assigned to that cluster\n                cluster_centers[idx2] = idx1\n                center_cluster_pairs.append([idx1, idx2])\n            else:\n                # if idx1 is not center, idx2 becomes new center\n                cluster_centers[idx2] = idx2\n        elif cluster_centers[idx1] is None:\n            if cluster_centers[idx2] == idx2:\n                # if idx2 is center, idx1 is assigned to that cluster\n                cluster_centers[idx1] = idx2\n                center_cluster_pairs.append([idx1, idx2])\n            else:\n                # if idx2 is not center, idx1 becomes new center\n                cluster_centers[idx1] = idx1\n\n    return center_cluster_pairs, merge_cluster_pairs\n\ncenter_cluster_pairs, merge_cluster_pairs = get_center_cluster_pairs(pairs_sored, len(df))\nct_clusters = connected_components_from_pairs(center_cluster_pairs, len(df))\nmc_clusters = connected_components_from_pairs(merge_cluster_pairs, len(df))\n```", "```py\nfrom sklearn.metrics.cluster import rand_score, adjusted_rand_score\nfrom IPython.display import display\n\ndef get_stats(labels, clusters):\n\n    stats = []\n    stats.append(f\"{rand_score(labels, clusters):.3f}\")\n    stats.append(f\"{adjusted_rand_score(labels, clusters):.3f}\")\n    clus_dist = pd.Series(clusters).value_counts()\n    stats.append(f\"{len(clus_dist):,}\")\n    stats.append(f\"{clus_dist.mean():.3f}\")\n    stats.append(f\"{clus_dist.min():,}\")\n    stats.append(f\"{clus_dist.max():,}\")\n\n    return stats\n\ndef compare_clusters(cluster_list, cluster_names, labels):\n\n    stats_dict = {}\n    for clusters, name in zip(cluster_list, cluster_names):\n        stats = get_stats(labels, clusters)\n        stats_dict[name] = stats\n\n    display(\n        pd.DataFrame(\n            stats_dict,\n            index=[\n                \"Rand Index\",\n                \"Adjusted Rand Index\",\n                \"Cluster Count\",\n                \"Mean Cluster Size\",\n                \"Min Cluster Size\",\n                \"Max Cluster Size\",\n            ],\n        )\n    )\n\ncluster_list = [cc_clusters, ct_clusters, mc_clusters]\ncluster_names = [\"Connected Components\", \"Center\", \"Merge-Center\"]\ncompare_clusters(cluster_list, cluster_names, df.CID)\n```"]