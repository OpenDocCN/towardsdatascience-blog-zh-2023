- en: Load Testing SageMaker Multi-Model Endpoints
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/load-testing-sagemaker-multi-model-endpoints-f0db7b305770?source=collection_archive---------11-----------------------#2023-02-24](https://towardsdatascience.com/load-testing-sagemaker-multi-model-endpoints-f0db7b305770?source=collection_archive---------11-----------------------#2023-02-24)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Utilize Locust to Distribute Traffic Weight Across Models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://ram-vegiraju.medium.com/?source=post_page-----f0db7b305770--------------------------------)[![Ram
    Vegiraju](../Images/07d9334e905f710d9f3c6187cf69a1a5.png)](https://ram-vegiraju.medium.com/?source=post_page-----f0db7b305770--------------------------------)[](https://towardsdatascience.com/?source=post_page-----f0db7b305770--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----f0db7b305770--------------------------------)
    [Ram Vegiraju](https://ram-vegiraju.medium.com/?source=post_page-----f0db7b305770--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F6e49569edd2b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fload-testing-sagemaker-multi-model-endpoints-f0db7b305770&user=Ram+Vegiraju&userId=6e49569edd2b&source=post_page-6e49569edd2b----f0db7b305770---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----f0db7b305770--------------------------------)
    ·9 min read·Feb 24, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Ff0db7b305770&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fload-testing-sagemaker-multi-model-endpoints-f0db7b305770&user=Ram+Vegiraju&userId=6e49569edd2b&source=-----f0db7b305770---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff0db7b305770&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fload-testing-sagemaker-multi-model-endpoints-f0db7b305770&source=-----f0db7b305770---------------------bookmark_footer-----------)![](../Images/2d73100b5f9f21fc0048bf4d480216c2.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Image from [Unsplash](https://unsplash.com/photos/mTorQ9gFfOg) by Luis Reyes
  prefs: []
  type: TYPE_NORMAL
- en: Productionizing Machine Learning models is a complicated practice. There’s a
    lot of iteration around different model parameters, hardware configurations, traffic
    patterns that you will have to test to try to finalize a production grade deployment.
    [Load testing](https://www.blazemeter.com/blog/performance-testing-vs-load-testing-vs-stress-testing#:~:text=but%20remain%20stable.-,What%20is%20Load%20Testing%3F,systems%20handle%20heavy%20load%20volumes.)
    is an essential software engineering practice, but also crucial to apply in the
    MLOps space to see how performant your model is in a real-world setting.
  prefs: []
  type: TYPE_NORMAL
- en: '**How can we load test?** A simple yet highly effective framework is the Python
    package: [Locust](https://locust.io/). Locust can be used in both a vanilla and
    distributed mode to simulate up to thousands of Transactions Per Second (TPS).
    For today’s blog we will assume basic understanding of this package and cover
    the fundamentals briefly, but for a more general introduction please reference
    this [article](/why-load-testing-is-essential-to-take-your-ml-app-to-production-faab0df1c4e1).'
  prefs: []
  type: TYPE_NORMAL
- en: '**What model/endpoint will we be testing?** [SageMaker Real-Time Inference](https://aws.amazon.com/blogs/machine-learning/part-2-model-hosting-patterns-in-amazon-sagemaker-getting-started-with-deploying-real-time-models-on-sagemaker/)
    is one of the best options for serving your ML models on REST endpoints tailored
    for low latency, high throughput workloads. In this blog we’ll specifically take
    a look at an advanced hosting option known as [**SageMaker Multi-Model Endpoints**](https://aws.amazon.com/blogs/machine-learning/part-3-model-hosting-patterns-in-amazon-sagemaker-run-and-optimize-multi-model-inference-with-amazon-sagemaker-multi-model-endpoints/).
    Here we can host thousands of models behind a singular REST endpoint and specify
    a target model we want to invoke for each API call…'
  prefs: []
  type: TYPE_NORMAL
