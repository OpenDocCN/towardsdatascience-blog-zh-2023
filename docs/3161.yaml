- en: CLIP — Intuitively and Exhaustively Explained
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: CLIP — 直观且详尽的解释
- en: 原文：[https://towardsdatascience.com/clip-intuitively-and-exhaustively-explained-1d02c07dbf40?source=collection_archive---------2-----------------------#2023-10-20](https://towardsdatascience.com/clip-intuitively-and-exhaustively-explained-1d02c07dbf40?source=collection_archive---------2-----------------------#2023-10-20)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/clip-intuitively-and-exhaustively-explained-1d02c07dbf40?source=collection_archive---------2-----------------------#2023-10-20](https://towardsdatascience.com/clip-intuitively-and-exhaustively-explained-1d02c07dbf40?source=collection_archive---------2-----------------------#2023-10-20)
- en: Creating strong image and language representations for general machine learning
    tasks.
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为通用机器学习任务创建强大的图像和语言表示。
- en: '[](https://medium.com/@danielwarfield1?source=post_page-----1d02c07dbf40--------------------------------)[![Daniel
    Warfield](../Images/c1c8b4dd514f6813e08e401401324bca.png)](https://medium.com/@danielwarfield1?source=post_page-----1d02c07dbf40--------------------------------)[](https://towardsdatascience.com/?source=post_page-----1d02c07dbf40--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----1d02c07dbf40--------------------------------)
    [Daniel Warfield](https://medium.com/@danielwarfield1?source=post_page-----1d02c07dbf40--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@danielwarfield1?source=post_page-----1d02c07dbf40--------------------------------)[![Daniel
    Warfield](../Images/c1c8b4dd514f6813e08e401401324bca.png)](https://medium.com/@danielwarfield1?source=post_page-----1d02c07dbf40--------------------------------)[](https://towardsdatascience.com/?source=post_page-----1d02c07dbf40--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----1d02c07dbf40--------------------------------)
    [Daniel Warfield](https://medium.com/@danielwarfield1?source=post_page-----1d02c07dbf40--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fbdc4072cbfdc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fclip-intuitively-and-exhaustively-explained-1d02c07dbf40&user=Daniel+Warfield&userId=bdc4072cbfdc&source=post_page-bdc4072cbfdc----1d02c07dbf40---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----1d02c07dbf40--------------------------------)
    ·17 min read·Oct 20, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F1d02c07dbf40&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fclip-intuitively-and-exhaustively-explained-1d02c07dbf40&user=Daniel+Warfield&userId=bdc4072cbfdc&source=-----1d02c07dbf40---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fbdc4072cbfdc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fclip-intuitively-and-exhaustively-explained-1d02c07dbf40&user=Daniel+Warfield&userId=bdc4072cbfdc&source=post_page-bdc4072cbfdc----1d02c07dbf40---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----1d02c07dbf40--------------------------------)
    ·17分钟阅读·2023年10月20日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F1d02c07dbf40&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fclip-intuitively-and-exhaustively-explained-1d02c07dbf40&user=Daniel+Warfield&userId=bdc4072cbfdc&source=-----1d02c07dbf40---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F1d02c07dbf40&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fclip-intuitively-and-exhaustively-explained-1d02c07dbf40&source=-----1d02c07dbf40---------------------bookmark_footer-----------)![](../Images/bb84737af3ff56dd6f70ef16d6a1e9ce.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F1d02c07dbf40&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fclip-intuitively-and-exhaustively-explained-1d02c07dbf40&source=-----1d02c07dbf40---------------------bookmark_footer-----------)![](../Images/bb84737af3ff56dd6f70ef16d6a1e9ce.png)'
- en: “Contrasting Modes” by Daniel Warfield using MidJourney. All images by the author
    unless otherwise specified.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: “对比模式”由 Daniel Warfield 使用 MidJourney 制作。所有图像由作者提供，除非另有说明。
- en: In this post you’ll learn about “contrastive language-image pre-training” (CLIP),
    A strategy for creating vision and language representations so good they can be
    used to make highly specific and performant classifiers without any training data.
    We’ll go over the theory, how CLIP differs from more conventional methods, then
    we’ll walk through the architecture step by step.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，你将了解“对比语言-图像预训练”（CLIP），这是一种创建视觉和语言表示的方法，效果如此出色，以至于可以用来制作高度特定且高效的分类器，而无需任何训练数据。我们将讨论其理论，CLIP如何不同于传统方法，然后逐步介绍其架构。
- en: '![](../Images/b518de3e7bc648a9fde1be23489a9594.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b518de3e7bc648a9fde1be23489a9594.png)'
- en: CLIP predicting highly specific labels for classification tasks it was never
    directly trained on. [Source](https://arxiv.org/pdf/2103.00020.pdf)
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: CLIP 在从未直接训练过的分类任务中预测高度特定的标签。[来源](https://arxiv.org/pdf/2103.00020.pdf)
- en: '**Who is this useful for?** Anyone interested in computer vision, natural language
    processing (NLP), or multimodal modeling.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '**这对谁有用？** 对计算机视觉、自然语言处理（NLP）或多模态建模感兴趣的任何人。'
- en: '**How advanced is this post?** This post should be approachable to novice data
    scientists. Some of the later sections are a bit more advanced (particularly when
    we dig into the loss function).'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '**这篇文章的难度如何？** 这篇文章对初学者数据科学家应该是易于理解的。部分后续章节稍显高级（特别是当我们深入探讨损失函数时）。'
- en: '**Pre-requisites:** Some cursory knowledge of computer vision and natural language
    processing.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '**前提条件：** 需要具备一些计算机视觉和自然语言处理的基础知识。'
- en: The Typical Image Classifier
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 典型的图像分类器
- en: When training a model to detect if an image is of a cat or a dog, a common approach
    is to present a model with images of…
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练模型以检测图像是猫还是狗时，常见的方法是向模型展示一系列图像…
