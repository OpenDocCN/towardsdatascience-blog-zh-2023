["```py\n- config.py\n- train.py\n- project_config.toml\n```", "```py\n[data]\nvocab_size = 5589\nseq_length = 10\ntest_split = 0.3\ndata_path = \"dataset/\"\ndata_tensors_path = \"data_tensors/\"\n\n[model]\nembedding_dim = 256\nnum_blocks = 5\nnum_heads_in_block = 3\n\n[train]\nnum_epochs = 10\nbatch_size = 32\nlearning_rate = 0.001\ncheckpoint_path = \"auto\"\n```", "```py\n$> pip install toml munch\n```", "```py\nimport toml\nimport munch\n\ndef load_global_config( filepath : str = \"project_config.toml\" ):\n    return munch.munchify( toml.load( filepath ) )\n\ndef save_global_config( new_config , filepath : str = \"project_config.toml\" ):\n    with open( filepath , \"w\" ) as file:\n        toml.dump( new_config , file )\n```", "```py\nfrom config import load_global_config\n\nconfig = load_global_config()\n\nbatch_size = config.train.batch_size\nlr = config.train.learning_rate\n\nif config.train.checkpoint_path == \"auto\":\n    # Make a directory with name as current timestamp\n    pass\n```", "```py\n{'data': {'data_path': 'dataset/',\n          'data_tensors_path': 'data_tensors/',\n          'seq_length': 10,\n          'test_split': 0.3,\n          'vocab_size': 5589},\n 'model': {'embedding_dim': 256, 'num_blocks': 5, 'num_heads_in_block': 3},\n 'train': {'batch_size': 32,\n           'checkpoint_path': 'auto',\n           'learning_rate': 0.001,\n           'num_epochs': 10}}\n```"]