- en: OCR-Free Document Data Extraction with Transformers (2/2)
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 无OCR文档数据提取与变换器（2/2）
- en: 原文：[https://towardsdatascience.com/ocr-free-document-data-extraction-with-transformers-2-2-38ce26f41951?source=collection_archive---------1-----------------------#2023-08-10](https://towardsdatascience.com/ocr-free-document-data-extraction-with-transformers-2-2-38ce26f41951?source=collection_archive---------1-----------------------#2023-08-10)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/ocr-free-document-data-extraction-with-transformers-2-2-38ce26f41951?source=collection_archive---------1-----------------------#2023-08-10](https://towardsdatascience.com/ocr-free-document-data-extraction-with-transformers-2-2-38ce26f41951?source=collection_archive---------1-----------------------#2023-08-10)
- en: Donut versus Pix2Struct on custom data
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Donut与Pix2Struct在自定义数据上的对比
- en: '[](https://toon-beerten.medium.com/?source=post_page-----38ce26f41951--------------------------------)[![Toon
    Beerten](../Images/f169eaa8cefa00f17176955596972d57.png)](https://toon-beerten.medium.com/?source=post_page-----38ce26f41951--------------------------------)[](https://towardsdatascience.com/?source=post_page-----38ce26f41951--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----38ce26f41951--------------------------------)
    [Toon Beerten](https://toon-beerten.medium.com/?source=post_page-----38ce26f41951--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://toon-beerten.medium.com/?source=post_page-----38ce26f41951--------------------------------)[![Toon
    Beerten](../Images/f169eaa8cefa00f17176955596972d57.png)](https://toon-beerten.medium.com/?source=post_page-----38ce26f41951--------------------------------)[](https://towardsdatascience.com/?source=post_page-----38ce26f41951--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----38ce26f41951--------------------------------)
    [Toon Beerten](https://toon-beerten.medium.com/?source=post_page-----38ce26f41951--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F3aef462e13b5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Focr-free-document-data-extraction-with-transformers-2-2-38ce26f41951&user=Toon+Beerten&userId=3aef462e13b5&source=post_page-3aef462e13b5----38ce26f41951---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----38ce26f41951--------------------------------)
    ·7 min read·Aug 10, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F38ce26f41951&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Focr-free-document-data-extraction-with-transformers-2-2-38ce26f41951&user=Toon+Beerten&userId=3aef462e13b5&source=-----38ce26f41951---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F3aef462e13b5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Focr-free-document-data-extraction-with-transformers-2-2-38ce26f41951&user=Toon+Beerten&userId=3aef462e13b5&source=post_page-3aef462e13b5----38ce26f41951---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----38ce26f41951--------------------------------)
    ·7分钟阅读·2023年8月10日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F38ce26f41951&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Focr-free-document-data-extraction-with-transformers-2-2-38ce26f41951&user=Toon+Beerten&userId=3aef462e13b5&source=-----38ce26f41951---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F38ce26f41951&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Focr-free-document-data-extraction-with-transformers-2-2-38ce26f41951&source=-----38ce26f41951---------------------bookmark_footer-----------)![](../Images/45dc7196c8f321f51a04bce1054c5709.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F38ce26f41951&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Focr-free-document-data-extraction-with-transformers-2-2-38ce26f41951&source=-----38ce26f41951---------------------bookmark_footer-----------)![](../Images/45dc7196c8f321f51a04bce1054c5709.png)'
- en: Image by author ([with](https://huggingface.co/spaces/albarji/mixture-of-diffusers))
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供（[使用](https://huggingface.co/spaces/albarji/mixture-of-diffusers)）
- en: How well do these two transformer models understand documents? In this second
    part I will show you how to train them and compare their results for the task
    of key index extraction.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 这两种变换器模型对文档的理解如何？在第二部分中，我将展示如何训练它们并比较它们在关键索引提取任务中的结果。
- en: Finetuning Donut
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 调整Donut模型
- en: So let’s pick up from [part 1](https://medium.com/towards-data-science/ocr-free-document-data-extraction-with-transformers-1-2-b5a826bc2ac3),
    where I explain how to prepare the custom data. I zipped the two folders of the
    dataset and uploaded them into a new huggingface dataset [here](https://huggingface.co/datasets/to-be/ghega_dataset_preprocessed).
    The colab notebook I used can be found [here](https://github.com/Toon-nooT/notebooks/blob/main/Donut_vs_pix2struct_2_Ghega_donut.ipynb).
    It will download the dataset, set up the environment, load the Donut model and
    train it.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 所以让我们从 [第 1 部分](https://medium.com/towards-data-science/ocr-free-document-data-extraction-with-transformers-1-2-b5a826bc2ac3)
    开始，在那里我解释了如何准备自定义数据。我将数据集的两个文件夹打包并上传到一个新的 huggingface 数据集 [这里](https://huggingface.co/datasets/to-be/ghega_dataset_preprocessed)。我使用的
    Colab 笔记本可以在 [这里](https://github.com/Toon-nooT/notebooks/blob/main/Donut_vs_pix2struct_2_Ghega_donut.ipynb)
    找到。它将下载数据集，设置环境，加载 Donut 模型并进行训练。
- en: 'After finetuning for 75 minutes I stopped it when the validation metric (which
    is the edit distance) reached 0.116:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在微调了 75 分钟后，我在验证指标（即编辑距离）达到 0.116 时停止了：
- en: '![](../Images/f3f7633cd3098c341c9cb21ade7ca5bc.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f3f7633cd3098c341c9cb21ade7ca5bc.png)'
- en: Image by author
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图像
- en: 'On field level I get these results for the validation set:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在字段级别，我得到这些验证集结果：
- en: '![](../Images/f960b805faf7ef905d75be3cfe87e98c.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f960b805faf7ef905d75be3cfe87e98c.png)'
- en: Image by author
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图像
- en: When we look at *Doctype*, we see Donut always correctly identifies the docs
    as either a *patent* or a *datasheet*. So we can say that classification reaches
    a 100% accuracy. Also note that even though we have a class *datasheet* it doesn’t
    need this exact word to be on the document to be classifying it as such. It does
    not matter to Donut as it was finetuned to recognize it like that.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们查看*Doctype*时，我们发现 Donut 总是正确地将文档识别为*专利*或*数据表*。因此，我们可以说分类达到了 100% 的准确率。同样需要注意的是，即使我们有一个类别*数据表*，它也不需要文档上出现这个确切的词来进行分类。对于
    Donut 来说，这并不重要，因为它经过微调以这样识别。
- en: 'Other fields score quite OK as well, but it’s hard to say with this graph alone
    what goes on under the hood. I’d like to see where the model goes right and wrong
    in specific cases. So I created a routine in my notebook to generate an HTML-formatted
    report table. For every document in my validation set I have a row entry like
    this:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 其他领域的得分也相当不错，但仅凭这张图表很难了解内部情况。我想看看模型在特定情况下的正确与错误之处。因此，我在我的笔记本中创建了一个例行程序来生成 HTML
    格式的报告表。对于我的验证集中的每个文档，我都有这样的行条目：
- en: '![](../Images/2135bed45f368f3479c204a794a4ecb9.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2135bed45f368f3479c204a794a4ecb9.png)'
- en: Image by author
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图像
- en: 'On the left is the recognized (inferred) data together with its ground truth.
    On the right side is the image. I also used color codes to have a quick overview:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 左侧是识别（推断）数据及其真实值。右侧是图像。我还使用了颜色代码以便快速概览：
- en: '![](../Images/459bea0d48308f7f7011fe4bc18fadbe.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/459bea0d48308f7f7011fe4bc18fadbe.png)'
- en: Image by author
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图像
- en: So ideally, everything should be highlighted green. If you want to see the full
    report for the validation set, you can see it [here](https://neontreebot.be/data/Donut_vs_Pix2Struct/Donut_result_report.html),
    or download this [zip file](http://Donut_vs_pix2struct_validation_results.zip)
    locally.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 理想情况下，一切都应该用绿色突出显示。如果你想查看验证集的完整报告，可以在 [这里](https://neontreebot.be/data/Donut_vs_Pix2Struct/Donut_result_report.html)
    查看，或者本地下载这个 [zip 文件](http://Donut_vs_pix2struct_validation_results.zip)。
- en: With this information we can spot usual OCR errors like *Dczcmbci* instead of
    *December,* or *GL420* instead of *GL420* (0’s and O’s are hard to distinguish),
    that lead to false positives.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这些信息，我们可以发现常见的 OCR 错误，如*Dczcmbci*（应为*December*）或*GL420*（应为*GL420*，0 和 O 难以区分），这些错误会导致假阳性。
- en: 'Let’s focus now on the worst performing field: Voltage. Here are some samples
    of the inferred data, the ground truth and the actual relevant document snippet.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们关注表现最差的字段：电压。以下是推断数据、真实值和实际相关文档片段的一些样本。
- en: '![](../Images/38f1432cc00e4bad8d920228707e2d48.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/38f1432cc00e4bad8d920228707e2d48.png)'
- en: Image by author
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图像
- en: The problem here is that the ground truth is mostly wrong. There is no standard
    of including the unit (Volt or V) or not. Sometimes irrelevant text is taken along,
    sometimes just a (wrong!) number. I can see now why Donut had a hard time with
    this.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 问题在于真实值大多是错误的。是否包括单位（Volt 或 V）没有标准。有时会包含无关文本，有时只是一个（错误的！）数字。我现在明白为什么 Donut 会对此感到困难。
- en: '![](../Images/2c144a76bc7dd36ffe3e1019b18a256b.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2c144a76bc7dd36ffe3e1019b18a256b.png)'
- en: Image by author
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图像
- en: Above are some samples where Donut actually returns the best answer while the
    ground truth is incomplete or wrong.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 上面是一些Donut实际上给出最佳答案的样本，而实际情况是不完整或错误的。
- en: '![](../Images/f3d2be65fbde5bad2154dd824fdac5eb.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f3d2be65fbde5bad2154dd824fdac5eb.png)'
- en: Image by author
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图像
- en: Above is another good example of bad training data confusing Donut. The ‘I’
    letter in the ground truth is an artifact of the OCR reading the vertical line
    in front of the information. Sometimes it’s there, other times not. If you preprocess
    your data to be consequent in this regard, Donut will learn this and adhere to
    this structure.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 上面是另一个糟糕训练数据混淆Donut的好例子。地面真实值中的‘I’字母是OCR读取信息前的垂直线的伪影。有时它存在，有时不存在。如果你对数据进行预处理，使其在这方面一致，Donut将会学习并遵循这种结构。
- en: Finetuning Pix2Struct
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 微调Pix2Struct
- en: Donut’s results are holding up, will Pix2Struct’s as well? The colab notebook
    I used for the training can be found [here](https://github.com/Toon-nooT/notebooks/blob/main/Donut_vs_pix2struct_3_Ghega_Pix2Struct.ipynb).
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: Donut的结果保持稳定，Pix2Struct的呢？我用来训练的Colab笔记本可以在[这里](https://github.com/Toon-nooT/notebooks/blob/main/Donut_vs_pix2struct_3_Ghega_Pix2Struct.ipynb)找到。
- en: After training for 75 minutes I got an edit distance score of 0.197 versus 0.116
    for Donut. It is definitely slower at converging.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 经过75分钟的训练，我得到的编辑距离分数为0.197，而Donut的为0.116。这显然收敛速度较慢。
- en: Another observation is that so far every value that is returned starts with
    a space. This could be an error in the ImageCaptioningDataset class, but I did
    not investigate further into the root cause. I do remove this space when generating
    the validation results though.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个观察结果是，到目前为止，每个返回的值都以一个空格开头。这可能是ImageCaptioningDataset类中的错误，但我没有进一步调查根本原因。不过，我在生成验证结果时会去掉这个空格。
- en: '[PRE0]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'I stopped the finetuning process after 2 hours because the validation metric
    went up again:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在2小时后我停止了微调过程，因为验证指标再次上升：
- en: '![](../Images/9324a3a41e08ae77d58da13021dbfe51.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9324a3a41e08ae77d58da13021dbfe51.png)'
- en: Image by author
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图像
- en: But what does that mean on field level for the validation set?
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 那么这对验证集的字段级别意味着什么呢？
- en: '![](../Images/016b40e227339838bf67744203460909.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/016b40e227339838bf67744203460909.png)'
- en: Image by author
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图像
- en: That looks a lot worse than the results of Donut! If you want to see the full
    HTML report, you can see it [here](https://neontreebot.be/data/Donut_vs_Pix2Struct/Pix2Struct_result_report.html),
    or download this [zip file](http://Donut_vs_pix2struct_validation_results.zip)
    locally.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 这看起来比Donut的结果差得多！如果你想查看完整的HTML报告，可以在[这里](https://neontreebot.be/data/Donut_vs_Pix2Struct/Pix2Struct_result_report.html)查看，或者在本地下载[这个zip文件](http://Donut_vs_pix2struct_validation_results.zip)。
- en: Only the classification between a *datasheet* and a *patent* seems to be quite
    OK (but not as good as Donut). The other fields are just plain bad. Can we deduct
    what’s going on?
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 只有在*数据表*和*专利*之间的分类似乎还不错（但不如Donut）。其他字段则完全不佳。我们能推断发生了什么吗？
- en: For the *patent* docs, I see lots of orange lines which mean that Pix2Struct
    did not return those fields at all.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 对于*专利*文档，我看到很多橙色线条，这意味着Pix2Struct根本没有返回这些字段。
- en: '![](../Images/19611b7234cb2f3788e67025fbb526ca.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/19611b7234cb2f3788e67025fbb526ca.png)'
- en: Image by author
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图像
- en: And even for *patents* where it returns fields, they are completely made up.
    Whereas Donut’s errors stem from picking it from another region on the document
    or having minor OCR mistakes, Pix2Struct is hallucinating here.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 即使在*专利*中返回字段，它们也完全是虚构的。而Donut的错误源于从文档的其他区域提取或有轻微的OCR错误，Pix2Struct在这里则是出现了幻觉。
- en: 'Disappointed by Pix2Struct’s performance, I tried several new training runs
    in hopes of better results:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 对Pix2Struct的表现感到失望，我尝试了几次新的训练以期获得更好的结果：
- en: '![](../Images/d0505d47bb938ff2050c32530298ed09.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d0505d47bb938ff2050c32530298ed09.png)'
- en: Image by author
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图像
- en: 'I tried lowering gradually the accumulate_grad_batches from 8 to 1\. But then
    the learn rate is too high and overshoots. Lowering that to 1e-5 makes the model
    not converge. Other combinations lead to the model collapsing. Even if with some
    specific hyperparameters the validation metric looked quite OK, it was giving
    a lot of incorrect or unparseable lines, like:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 我尝试将accumulate_grad_batches逐渐从8降到1。但这样学习率过高，会导致超调。将其降低到1e-5会使模型无法收敛。其他组合则导致模型崩溃。即使在一些特定的超参数下，验证指标看起来相当不错，但它给出了很多不正确或无法解析的行，例如：
- en: '[PRE1]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: None of these attempts gave me substantial better results, so I left it at that.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 这些尝试都没有给我带来实质性的更好结果，所以我就此停止了。
- en: Until I saw that a cross attention [bug](https://github.com/huggingface/transformers/pull/25200)
    was fixed in the huggingface implementation. So i decided to give it a last try.
    Trained for 2 and a half hour and stopped at a validation metric of 0.1416 .
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 直到我看到huggingface实现中的交叉注意力 [bug](https://github.com/huggingface/transformers/pull/25200)被修复。因此，我决定再试一次。训练了两个半小时，停在0.1416的验证指标上。
- en: '![](../Images/75ea338e70597180b073e5d5230e3d58.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/75ea338e70597180b073e5d5230e3d58.png)'
- en: Image by author
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片
- en: '![](../Images/0f56d46e41ed75b07b8cd2db7e053f11.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0f56d46e41ed75b07b8cd2db7e053f11.png)'
- en: Image by author
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片
- en: This looks definitely better than all previous runs. Looking at the HTML [report](https://neontreebot.be/data/Donut_vs_Pix2Struct/Pix2Struct_result_report_20230804.html),
    it now seems to hallucinate less. Overall it’s still performing worse than Donut.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 这显然比所有之前的结果都要好。查看HTML [报告](https://neontreebot.be/data/Donut_vs_Pix2Struct/Pix2Struct_result_report_20230804.html)，现在似乎幻觉更少。总体来说，它的表现仍不如Donut。
- en: As for reasons why, I have two theories. Firstly, Pix2Struct was mainly trained
    on HTML web page images (predicting what is behind masked image parts) and has
    trouble switching to another domain, namely raw text. Secondly, the dataset used
    was challenging. It contains many OCR errors and non-conformities (such as including
    units, length, minus signs). In my other experiments it really came to light that
    the quality and conformity of the dataset is more important than the quantity.
    In this dataset the data quality is really subpar. Maybe that is why I could not
    replicate the claim in the paper that Pix2Struct exceeds Donuts performance.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 至于原因，我有两个理论。首先，Pix2Struct主要在HTML网页图像上训练（预测掩码图像部分后面的内容），并且在切换到另一个领域，即原始文本时，遇到了困难。其次，使用的数据集非常具有挑战性。它包含了许多OCR错误和不一致（如包含单位、长度、负号）。在我的其他实验中，我真的发现数据集的质量和一致性比数量更重要。在这个数据集中，数据质量真的很差。也许这就是我无法复制论文中声称Pix2Struct超越Donut表现的原因。
- en: Inference speed
  id: totrans-67
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 推断速度
- en: 'How do the two models compare in terms of speed? All trainings were done on
    the same T4 architecture, so the times can be readily compared. We already saw
    that Pix2Struct takes much longer to converge. But what about inference times?
    We can compare the time it took for inferring the validation set:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 这两种模型在速度方面如何比较？所有训练都在相同的T4架构上进行，因此时间可以直接比较。我们已经看到Pix2Struct收敛所需的时间要长得多。那么推断时间呢？我们可以比较推断验证集所需的时间：
- en: '![](../Images/ffe57189a296986fe0d28e53c9caa148.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ffe57189a296986fe0d28e53c9caa148.png)'
- en: Image by author
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片
- en: Donut takes on average 1.3 seconds per document to extract, while Pix2Struct
    more than double.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: Donut每个文档提取的平均时间为1.3秒，而Pix2Struct则超过两倍。
- en: '**Takeaways**'
  id: totrans-72
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**要点**'
- en: The clear winner for me is Donut. In terms of ease-of-use, performance, training
    stability and speed.
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对我来说，最终的赢家是Donut。在易用性、性能、训练稳定性和速度方面。
- en: Pix2Struct is challenging to train because it is very sensitive to training
    hyperparameters. It converges slower and doesn’t reach the results of Donut in
    this dataset. It may proof worthwhile to revisit Pix2Struct with a high(er) quality
    dataset.
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pix2Struct训练具有挑战性，因为它对训练超参数非常敏感。它收敛较慢，并且在这个数据集上没有达到Donut的结果。可能值得重新考虑使用更高质量的数据集来尝试Pix2Struct。
- en: Because the Ghega dataset contains too many inconsistencies, I will refrain
    from using it in further experiments.
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于Ghega数据集包含太多不一致性，我将避免在进一步实验中使用它。
- en: '**Are there any alternative models?**'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '**是否有其他替代模型？**'
- en: '[Dessurt](https://arxiv.org/pdf/2203.16618v3.pdf), which seems to share a similar
    architecture with Donut should perform in the same league.'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Dessurt](https://arxiv.org/pdf/2203.16618v3.pdf)，似乎与Donut有相似的架构，应该表现相当。'
- en: '[DocParser](https://arxiv.org/pdf/2304.12484.pdf), which the paper claims to
    perform even a little better. Unfortunately there is no plan to release this model
    in the future.'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[DocParser](https://arxiv.org/pdf/2304.12484.pdf)，论文称其表现甚至更好。不幸的是，目前没有计划将该模型发布到未来。'
- en: '[mPLUG-DocOwl](https://arxiv.org/abs/2307.02499) will soon be released which
    is yet another OCR-Free LLM for document understanding with promising benchmarks.'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[mPLUG-DocOwl](https://arxiv.org/abs/2307.02499)将很快发布，这是另一个有前景的无OCR LLM文档理解工具。'
- en: 'You may also like:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能还会喜欢：
- en: '[](https://toon-beerten.medium.com/hands-on-document-data-extraction-with-transformer-7130df3b6132?source=post_page-----38ce26f41951--------------------------------)
    [## Hands-on: document data extraction with 🍩 transformer'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://toon-beerten.medium.com/hands-on-document-data-extraction-with-transformer-7130df3b6132?source=post_page-----38ce26f41951--------------------------------)
    [## 实战：使用🍩变压器进行文档数据提取'
- en: My experience using donut transformers model to extract invoice indexes.
  id: totrans-82
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 我使用甜甜圈转换器模型来提取发票索引的经验。
- en: toon-beerten.medium.com](https://toon-beerten.medium.com/hands-on-document-data-extraction-with-transformer-7130df3b6132?source=post_page-----38ce26f41951--------------------------------)
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '[toon-beerten.medium.com](https://toon-beerten.medium.com/hands-on-document-data-extraction-with-transformer-7130df3b6132?source=post_page-----38ce26f41951--------------------------------)'
- en: 'References:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 参考文献：
- en: '[](https://arxiv.org/abs/2210.03347?source=post_page-----38ce26f41951--------------------------------)
    [## Pix2Struct: Screenshot Parsing as Pretraining for Visual Language Understanding'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://arxiv.org/abs/2210.03347?source=post_page-----38ce26f41951--------------------------------)
    [## Pix2Struct: 截图解析作为视觉语言理解的预训练'
- en: Visually-situated language is ubiquitous -- sources range from textbooks with
    diagrams to web pages with images and…
  id: totrans-86
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 视觉定位语言无处不在——来源从带有图表的教科书到带有图像的网页等。
- en: arxiv.org](https://arxiv.org/abs/2210.03347?source=post_page-----38ce26f41951--------------------------------)
    [](https://arxiv.org/abs/2111.15664?source=post_page-----38ce26f41951--------------------------------)
    [## OCR-free Document Understanding Transformer
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '[arxiv.org](https://arxiv.org/abs/2210.03347?source=post_page-----38ce26f41951--------------------------------)
    [](https://arxiv.org/abs/2111.15664?source=post_page-----38ce26f41951--------------------------------)
    [## 无 OCR 文档理解转换器'
- en: Understanding document images (e.g., invoices) is a core but challenging task
    since it requires complex functions such…
  id: totrans-88
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 理解文档图像（例如发票）是一项核心但具有挑战性的任务，因为它需要复杂的功能，比如……
- en: arxiv.org](https://arxiv.org/abs/2111.15664?source=post_page-----38ce26f41951--------------------------------)
    [](https://github.com/Toon-nooT/notebooks/tree/main?source=post_page-----38ce26f41951--------------------------------)
    [## GitHub - Toon-nooT/notebooks
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '[arxiv.org](https://arxiv.org/abs/2111.15664?source=post_page-----38ce26f41951--------------------------------)
    [](https://github.com/Toon-nooT/notebooks/tree/main?source=post_page-----38ce26f41951--------------------------------)
    [## GitHub - Toon-nooT/notebooks'
- en: Contribute to Toon-nooT/notebooks development by creating an account on GitHub.
  id: totrans-90
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 通过在 GitHub 上创建帐户来为 Toon-nooT/notebooks 的开发做贡献。
- en: github.com](https://github.com/Toon-nooT/notebooks/tree/main?source=post_page-----38ce26f41951--------------------------------)
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '[github.com](https://github.com/Toon-nooT/notebooks/tree/main?source=post_page-----38ce26f41951--------------------------------)'
