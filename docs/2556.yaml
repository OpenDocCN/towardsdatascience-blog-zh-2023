- en: OCR-Free Document Data Extraction with Transformers (2/2)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/ocr-free-document-data-extraction-with-transformers-2-2-38ce26f41951?source=collection_archive---------1-----------------------#2023-08-10](https://towardsdatascience.com/ocr-free-document-data-extraction-with-transformers-2-2-38ce26f41951?source=collection_archive---------1-----------------------#2023-08-10)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Donut versus Pix2Struct on custom data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://toon-beerten.medium.com/?source=post_page-----38ce26f41951--------------------------------)[![Toon
    Beerten](../Images/f169eaa8cefa00f17176955596972d57.png)](https://toon-beerten.medium.com/?source=post_page-----38ce26f41951--------------------------------)[](https://towardsdatascience.com/?source=post_page-----38ce26f41951--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----38ce26f41951--------------------------------)
    [Toon Beerten](https://toon-beerten.medium.com/?source=post_page-----38ce26f41951--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F3aef462e13b5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Focr-free-document-data-extraction-with-transformers-2-2-38ce26f41951&user=Toon+Beerten&userId=3aef462e13b5&source=post_page-3aef462e13b5----38ce26f41951---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----38ce26f41951--------------------------------)
    ·7 min read·Aug 10, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F38ce26f41951&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Focr-free-document-data-extraction-with-transformers-2-2-38ce26f41951&user=Toon+Beerten&userId=3aef462e13b5&source=-----38ce26f41951---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F38ce26f41951&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Focr-free-document-data-extraction-with-transformers-2-2-38ce26f41951&source=-----38ce26f41951---------------------bookmark_footer-----------)![](../Images/45dc7196c8f321f51a04bce1054c5709.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Image by author ([with](https://huggingface.co/spaces/albarji/mixture-of-diffusers))
  prefs: []
  type: TYPE_NORMAL
- en: How well do these two transformer models understand documents? In this second
    part I will show you how to train them and compare their results for the task
    of key index extraction.
  prefs: []
  type: TYPE_NORMAL
- en: Finetuning Donut
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So let’s pick up from [part 1](https://medium.com/towards-data-science/ocr-free-document-data-extraction-with-transformers-1-2-b5a826bc2ac3),
    where I explain how to prepare the custom data. I zipped the two folders of the
    dataset and uploaded them into a new huggingface dataset [here](https://huggingface.co/datasets/to-be/ghega_dataset_preprocessed).
    The colab notebook I used can be found [here](https://github.com/Toon-nooT/notebooks/blob/main/Donut_vs_pix2struct_2_Ghega_donut.ipynb).
    It will download the dataset, set up the environment, load the Donut model and
    train it.
  prefs: []
  type: TYPE_NORMAL
- en: 'After finetuning for 75 minutes I stopped it when the validation metric (which
    is the edit distance) reached 0.116:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f3f7633cd3098c341c9cb21ade7ca5bc.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: 'On field level I get these results for the validation set:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f960b805faf7ef905d75be3cfe87e98c.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: When we look at *Doctype*, we see Donut always correctly identifies the docs
    as either a *patent* or a *datasheet*. So we can say that classification reaches
    a 100% accuracy. Also note that even though we have a class *datasheet* it doesn’t
    need this exact word to be on the document to be classifying it as such. It does
    not matter to Donut as it was finetuned to recognize it like that.
  prefs: []
  type: TYPE_NORMAL
- en: 'Other fields score quite OK as well, but it’s hard to say with this graph alone
    what goes on under the hood. I’d like to see where the model goes right and wrong
    in specific cases. So I created a routine in my notebook to generate an HTML-formatted
    report table. For every document in my validation set I have a row entry like
    this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2135bed45f368f3479c204a794a4ecb9.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: 'On the left is the recognized (inferred) data together with its ground truth.
    On the right side is the image. I also used color codes to have a quick overview:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/459bea0d48308f7f7011fe4bc18fadbe.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: So ideally, everything should be highlighted green. If you want to see the full
    report for the validation set, you can see it [here](https://neontreebot.be/data/Donut_vs_Pix2Struct/Donut_result_report.html),
    or download this [zip file](http://Donut_vs_pix2struct_validation_results.zip)
    locally.
  prefs: []
  type: TYPE_NORMAL
- en: With this information we can spot usual OCR errors like *Dczcmbci* instead of
    *December,* or *GL420* instead of *GL420* (0’s and O’s are hard to distinguish),
    that lead to false positives.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s focus now on the worst performing field: Voltage. Here are some samples
    of the inferred data, the ground truth and the actual relevant document snippet.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/38f1432cc00e4bad8d920228707e2d48.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: The problem here is that the ground truth is mostly wrong. There is no standard
    of including the unit (Volt or V) or not. Sometimes irrelevant text is taken along,
    sometimes just a (wrong!) number. I can see now why Donut had a hard time with
    this.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2c144a76bc7dd36ffe3e1019b18a256b.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: Above are some samples where Donut actually returns the best answer while the
    ground truth is incomplete or wrong.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f3d2be65fbde5bad2154dd824fdac5eb.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: Above is another good example of bad training data confusing Donut. The ‘I’
    letter in the ground truth is an artifact of the OCR reading the vertical line
    in front of the information. Sometimes it’s there, other times not. If you preprocess
    your data to be consequent in this regard, Donut will learn this and adhere to
    this structure.
  prefs: []
  type: TYPE_NORMAL
- en: Finetuning Pix2Struct
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Donut’s results are holding up, will Pix2Struct’s as well? The colab notebook
    I used for the training can be found [here](https://github.com/Toon-nooT/notebooks/blob/main/Donut_vs_pix2struct_3_Ghega_Pix2Struct.ipynb).
  prefs: []
  type: TYPE_NORMAL
- en: After training for 75 minutes I got an edit distance score of 0.197 versus 0.116
    for Donut. It is definitely slower at converging.
  prefs: []
  type: TYPE_NORMAL
- en: Another observation is that so far every value that is returned starts with
    a space. This could be an error in the ImageCaptioningDataset class, but I did
    not investigate further into the root cause. I do remove this space when generating
    the validation results though.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'I stopped the finetuning process after 2 hours because the validation metric
    went up again:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9324a3a41e08ae77d58da13021dbfe51.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: But what does that mean on field level for the validation set?
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/016b40e227339838bf67744203460909.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: That looks a lot worse than the results of Donut! If you want to see the full
    HTML report, you can see it [here](https://neontreebot.be/data/Donut_vs_Pix2Struct/Pix2Struct_result_report.html),
    or download this [zip file](http://Donut_vs_pix2struct_validation_results.zip)
    locally.
  prefs: []
  type: TYPE_NORMAL
- en: Only the classification between a *datasheet* and a *patent* seems to be quite
    OK (but not as good as Donut). The other fields are just plain bad. Can we deduct
    what’s going on?
  prefs: []
  type: TYPE_NORMAL
- en: For the *patent* docs, I see lots of orange lines which mean that Pix2Struct
    did not return those fields at all.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/19611b7234cb2f3788e67025fbb526ca.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: And even for *patents* where it returns fields, they are completely made up.
    Whereas Donut’s errors stem from picking it from another region on the document
    or having minor OCR mistakes, Pix2Struct is hallucinating here.
  prefs: []
  type: TYPE_NORMAL
- en: 'Disappointed by Pix2Struct’s performance, I tried several new training runs
    in hopes of better results:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d0505d47bb938ff2050c32530298ed09.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: 'I tried lowering gradually the accumulate_grad_batches from 8 to 1\. But then
    the learn rate is too high and overshoots. Lowering that to 1e-5 makes the model
    not converge. Other combinations lead to the model collapsing. Even if with some
    specific hyperparameters the validation metric looked quite OK, it was giving
    a lot of incorrect or unparseable lines, like:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: None of these attempts gave me substantial better results, so I left it at that.
  prefs: []
  type: TYPE_NORMAL
- en: Until I saw that a cross attention [bug](https://github.com/huggingface/transformers/pull/25200)
    was fixed in the huggingface implementation. So i decided to give it a last try.
    Trained for 2 and a half hour and stopped at a validation metric of 0.1416 .
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/75ea338e70597180b073e5d5230e3d58.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0f56d46e41ed75b07b8cd2db7e053f11.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: This looks definitely better than all previous runs. Looking at the HTML [report](https://neontreebot.be/data/Donut_vs_Pix2Struct/Pix2Struct_result_report_20230804.html),
    it now seems to hallucinate less. Overall it’s still performing worse than Donut.
  prefs: []
  type: TYPE_NORMAL
- en: As for reasons why, I have two theories. Firstly, Pix2Struct was mainly trained
    on HTML web page images (predicting what is behind masked image parts) and has
    trouble switching to another domain, namely raw text. Secondly, the dataset used
    was challenging. It contains many OCR errors and non-conformities (such as including
    units, length, minus signs). In my other experiments it really came to light that
    the quality and conformity of the dataset is more important than the quantity.
    In this dataset the data quality is really subpar. Maybe that is why I could not
    replicate the claim in the paper that Pix2Struct exceeds Donuts performance.
  prefs: []
  type: TYPE_NORMAL
- en: Inference speed
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'How do the two models compare in terms of speed? All trainings were done on
    the same T4 architecture, so the times can be readily compared. We already saw
    that Pix2Struct takes much longer to converge. But what about inference times?
    We can compare the time it took for inferring the validation set:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ffe57189a296986fe0d28e53c9caa148.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: Donut takes on average 1.3 seconds per document to extract, while Pix2Struct
    more than double.
  prefs: []
  type: TYPE_NORMAL
- en: '**Takeaways**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The clear winner for me is Donut. In terms of ease-of-use, performance, training
    stability and speed.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pix2Struct is challenging to train because it is very sensitive to training
    hyperparameters. It converges slower and doesn’t reach the results of Donut in
    this dataset. It may proof worthwhile to revisit Pix2Struct with a high(er) quality
    dataset.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Because the Ghega dataset contains too many inconsistencies, I will refrain
    from using it in further experiments.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Are there any alternative models?**'
  prefs: []
  type: TYPE_NORMAL
- en: '[Dessurt](https://arxiv.org/pdf/2203.16618v3.pdf), which seems to share a similar
    architecture with Donut should perform in the same league.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[DocParser](https://arxiv.org/pdf/2304.12484.pdf), which the paper claims to
    perform even a little better. Unfortunately there is no plan to release this model
    in the future.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[mPLUG-DocOwl](https://arxiv.org/abs/2307.02499) will soon be released which
    is yet another OCR-Free LLM for document understanding with promising benchmarks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'You may also like:'
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://toon-beerten.medium.com/hands-on-document-data-extraction-with-transformer-7130df3b6132?source=post_page-----38ce26f41951--------------------------------)
    [## Hands-on: document data extraction with 🍩 transformer'
  prefs: []
  type: TYPE_NORMAL
- en: My experience using donut transformers model to extract invoice indexes.
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: toon-beerten.medium.com](https://toon-beerten.medium.com/hands-on-document-data-extraction-with-transformer-7130df3b6132?source=post_page-----38ce26f41951--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: 'References:'
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://arxiv.org/abs/2210.03347?source=post_page-----38ce26f41951--------------------------------)
    [## Pix2Struct: Screenshot Parsing as Pretraining for Visual Language Understanding'
  prefs: []
  type: TYPE_NORMAL
- en: Visually-situated language is ubiquitous -- sources range from textbooks with
    diagrams to web pages with images and…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: arxiv.org](https://arxiv.org/abs/2210.03347?source=post_page-----38ce26f41951--------------------------------)
    [](https://arxiv.org/abs/2111.15664?source=post_page-----38ce26f41951--------------------------------)
    [## OCR-free Document Understanding Transformer
  prefs: []
  type: TYPE_NORMAL
- en: Understanding document images (e.g., invoices) is a core but challenging task
    since it requires complex functions such…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: arxiv.org](https://arxiv.org/abs/2111.15664?source=post_page-----38ce26f41951--------------------------------)
    [](https://github.com/Toon-nooT/notebooks/tree/main?source=post_page-----38ce26f41951--------------------------------)
    [## GitHub - Toon-nooT/notebooks
  prefs: []
  type: TYPE_NORMAL
- en: Contribute to Toon-nooT/notebooks development by creating an account on GitHub.
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: github.com](https://github.com/Toon-nooT/notebooks/tree/main?source=post_page-----38ce26f41951--------------------------------)
  prefs: []
  type: TYPE_NORMAL
