- en: OCR-Free Document Data Extraction with Transformers (2/2)
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ— OCRæ–‡æ¡£æ•°æ®æå–ä¸å˜æ¢å™¨ï¼ˆ2/2ï¼‰
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/ocr-free-document-data-extraction-with-transformers-2-2-38ce26f41951?source=collection_archive---------1-----------------------#2023-08-10](https://towardsdatascience.com/ocr-free-document-data-extraction-with-transformers-2-2-38ce26f41951?source=collection_archive---------1-----------------------#2023-08-10)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://towardsdatascience.com/ocr-free-document-data-extraction-with-transformers-2-2-38ce26f41951?source=collection_archive---------1-----------------------#2023-08-10](https://towardsdatascience.com/ocr-free-document-data-extraction-with-transformers-2-2-38ce26f41951?source=collection_archive---------1-----------------------#2023-08-10)
- en: Donut versus Pix2Struct on custom data
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Donutä¸Pix2Structåœ¨è‡ªå®šä¹‰æ•°æ®ä¸Šçš„å¯¹æ¯”
- en: '[](https://toon-beerten.medium.com/?source=post_page-----38ce26f41951--------------------------------)[![Toon
    Beerten](../Images/f169eaa8cefa00f17176955596972d57.png)](https://toon-beerten.medium.com/?source=post_page-----38ce26f41951--------------------------------)[](https://towardsdatascience.com/?source=post_page-----38ce26f41951--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----38ce26f41951--------------------------------)
    [Toon Beerten](https://toon-beerten.medium.com/?source=post_page-----38ce26f41951--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://toon-beerten.medium.com/?source=post_page-----38ce26f41951--------------------------------)[![Toon
    Beerten](../Images/f169eaa8cefa00f17176955596972d57.png)](https://toon-beerten.medium.com/?source=post_page-----38ce26f41951--------------------------------)[](https://towardsdatascience.com/?source=post_page-----38ce26f41951--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----38ce26f41951--------------------------------)
    [Toon Beerten](https://toon-beerten.medium.com/?source=post_page-----38ce26f41951--------------------------------)'
- en: Â·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Â·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F3aef462e13b5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Focr-free-document-data-extraction-with-transformers-2-2-38ce26f41951&user=Toon+Beerten&userId=3aef462e13b5&source=post_page-3aef462e13b5----38ce26f41951---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----38ce26f41951--------------------------------)
    Â·7 min readÂ·Aug 10, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F38ce26f41951&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Focr-free-document-data-extraction-with-transformers-2-2-38ce26f41951&user=Toon+Beerten&userId=3aef462e13b5&source=-----38ce26f41951---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[å…³æ³¨](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F3aef462e13b5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Focr-free-document-data-extraction-with-transformers-2-2-38ce26f41951&user=Toon+Beerten&userId=3aef462e13b5&source=post_page-3aef462e13b5----38ce26f41951---------------------post_header-----------)
    å‘è¡¨åœ¨ [Towards Data Science](https://towardsdatascience.com/?source=post_page-----38ce26f41951--------------------------------)
    Â·7åˆ†é’Ÿé˜…è¯»Â·2023å¹´8æœˆ10æ—¥[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F38ce26f41951&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Focr-free-document-data-extraction-with-transformers-2-2-38ce26f41951&user=Toon+Beerten&userId=3aef462e13b5&source=-----38ce26f41951---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F38ce26f41951&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Focr-free-document-data-extraction-with-transformers-2-2-38ce26f41951&source=-----38ce26f41951---------------------bookmark_footer-----------)![](../Images/45dc7196c8f321f51a04bce1054c5709.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F38ce26f41951&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Focr-free-document-data-extraction-with-transformers-2-2-38ce26f41951&source=-----38ce26f41951---------------------bookmark_footer-----------)![](../Images/45dc7196c8f321f51a04bce1054c5709.png)'
- en: Image by author ([with](https://huggingface.co/spaces/albarji/mixture-of-diffusers))
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ç‰‡ç”±ä½œè€…æä¾›ï¼ˆ[ä½¿ç”¨](https://huggingface.co/spaces/albarji/mixture-of-diffusers)ï¼‰
- en: How well do these two transformer models understand documents? In this second
    part I will show you how to train them and compare their results for the task
    of key index extraction.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸¤ç§å˜æ¢å™¨æ¨¡å‹å¯¹æ–‡æ¡£çš„ç†è§£å¦‚ä½•ï¼Ÿåœ¨ç¬¬äºŒéƒ¨åˆ†ä¸­ï¼Œæˆ‘å°†å±•ç¤ºå¦‚ä½•è®­ç»ƒå®ƒä»¬å¹¶æ¯”è¾ƒå®ƒä»¬åœ¨å…³é”®ç´¢å¼•æå–ä»»åŠ¡ä¸­çš„ç»“æœã€‚
- en: Finetuning Donut
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: è°ƒæ•´Donutæ¨¡å‹
- en: So letâ€™s pick up from [part 1](https://medium.com/towards-data-science/ocr-free-document-data-extraction-with-transformers-1-2-b5a826bc2ac3),
    where I explain how to prepare the custom data. I zipped the two folders of the
    dataset and uploaded them into a new huggingface dataset [here](https://huggingface.co/datasets/to-be/ghega_dataset_preprocessed).
    The colab notebook I used can be found [here](https://github.com/Toon-nooT/notebooks/blob/main/Donut_vs_pix2struct_2_Ghega_donut.ipynb).
    It will download the dataset, set up the environment, load the Donut model and
    train it.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥è®©æˆ‘ä»¬ä» [ç¬¬ 1 éƒ¨åˆ†](https://medium.com/towards-data-science/ocr-free-document-data-extraction-with-transformers-1-2-b5a826bc2ac3)
    å¼€å§‹ï¼Œåœ¨é‚£é‡Œæˆ‘è§£é‡Šäº†å¦‚ä½•å‡†å¤‡è‡ªå®šä¹‰æ•°æ®ã€‚æˆ‘å°†æ•°æ®é›†çš„ä¸¤ä¸ªæ–‡ä»¶å¤¹æ‰“åŒ…å¹¶ä¸Šä¼ åˆ°ä¸€ä¸ªæ–°çš„ huggingface æ•°æ®é›† [è¿™é‡Œ](https://huggingface.co/datasets/to-be/ghega_dataset_preprocessed)ã€‚æˆ‘ä½¿ç”¨çš„
    Colab ç¬”è®°æœ¬å¯ä»¥åœ¨ [è¿™é‡Œ](https://github.com/Toon-nooT/notebooks/blob/main/Donut_vs_pix2struct_2_Ghega_donut.ipynb)
    æ‰¾åˆ°ã€‚å®ƒå°†ä¸‹è½½æ•°æ®é›†ï¼Œè®¾ç½®ç¯å¢ƒï¼ŒåŠ è½½ Donut æ¨¡å‹å¹¶è¿›è¡Œè®­ç»ƒã€‚
- en: 'After finetuning for 75 minutes I stopped it when the validation metric (which
    is the edit distance) reached 0.116:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å¾®è°ƒäº† 75 åˆ†é’Ÿåï¼Œæˆ‘åœ¨éªŒè¯æŒ‡æ ‡ï¼ˆå³ç¼–è¾‘è·ç¦»ï¼‰è¾¾åˆ° 0.116 æ—¶åœæ­¢äº†ï¼š
- en: '![](../Images/f3f7633cd3098c341c9cb21ade7ca5bc.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f3f7633cd3098c341c9cb21ade7ca5bc.png)'
- en: Image by author
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: ä½œè€…æä¾›çš„å›¾åƒ
- en: 'On field level I get these results for the validation set:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å­—æ®µçº§åˆ«ï¼Œæˆ‘å¾—åˆ°è¿™äº›éªŒè¯é›†ç»“æœï¼š
- en: '![](../Images/f960b805faf7ef905d75be3cfe87e98c.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f960b805faf7ef905d75be3cfe87e98c.png)'
- en: Image by author
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: ä½œè€…æä¾›çš„å›¾åƒ
- en: When we look at *Doctype*, we see Donut always correctly identifies the docs
    as either a *patent* or a *datasheet*. So we can say that classification reaches
    a 100% accuracy. Also note that even though we have a class *datasheet* it doesnâ€™t
    need this exact word to be on the document to be classifying it as such. It does
    not matter to Donut as it was finetuned to recognize it like that.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: å½“æˆ‘ä»¬æŸ¥çœ‹*Doctype*æ—¶ï¼Œæˆ‘ä»¬å‘ç° Donut æ€»æ˜¯æ­£ç¡®åœ°å°†æ–‡æ¡£è¯†åˆ«ä¸º*ä¸“åˆ©*æˆ–*æ•°æ®è¡¨*ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¯ä»¥è¯´åˆ†ç±»è¾¾åˆ°äº† 100% çš„å‡†ç¡®ç‡ã€‚åŒæ ·éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œå³ä½¿æˆ‘ä»¬æœ‰ä¸€ä¸ªç±»åˆ«*æ•°æ®è¡¨*ï¼Œå®ƒä¹Ÿä¸éœ€è¦æ–‡æ¡£ä¸Šå‡ºç°è¿™ä¸ªç¡®åˆ‡çš„è¯æ¥è¿›è¡Œåˆ†ç±»ã€‚å¯¹äº
    Donut æ¥è¯´ï¼Œè¿™å¹¶ä¸é‡è¦ï¼Œå› ä¸ºå®ƒç»è¿‡å¾®è°ƒä»¥è¿™æ ·è¯†åˆ«ã€‚
- en: 'Other fields score quite OK as well, but itâ€™s hard to say with this graph alone
    what goes on under the hood. Iâ€™d like to see where the model goes right and wrong
    in specific cases. So I created a routine in my notebook to generate an HTML-formatted
    report table. For every document in my validation set I have a row entry like
    this:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä»–é¢†åŸŸçš„å¾—åˆ†ä¹Ÿç›¸å½“ä¸é”™ï¼Œä½†ä»…å‡­è¿™å¼ å›¾è¡¨å¾ˆéš¾äº†è§£å†…éƒ¨æƒ…å†µã€‚æˆ‘æƒ³çœ‹çœ‹æ¨¡å‹åœ¨ç‰¹å®šæƒ…å†µä¸‹çš„æ­£ç¡®ä¸é”™è¯¯ä¹‹å¤„ã€‚å› æ­¤ï¼Œæˆ‘åœ¨æˆ‘çš„ç¬”è®°æœ¬ä¸­åˆ›å»ºäº†ä¸€ä¸ªä¾‹è¡Œç¨‹åºæ¥ç”Ÿæˆ HTML
    æ ¼å¼çš„æŠ¥å‘Šè¡¨ã€‚å¯¹äºæˆ‘çš„éªŒè¯é›†ä¸­çš„æ¯ä¸ªæ–‡æ¡£ï¼Œæˆ‘éƒ½æœ‰è¿™æ ·çš„è¡Œæ¡ç›®ï¼š
- en: '![](../Images/2135bed45f368f3479c204a794a4ecb9.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2135bed45f368f3479c204a794a4ecb9.png)'
- en: Image by author
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: ä½œè€…æä¾›çš„å›¾åƒ
- en: 'On the left is the recognized (inferred) data together with its ground truth.
    On the right side is the image. I also used color codes to have a quick overview:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: å·¦ä¾§æ˜¯è¯†åˆ«ï¼ˆæ¨æ–­ï¼‰æ•°æ®åŠå…¶çœŸå®å€¼ã€‚å³ä¾§æ˜¯å›¾åƒã€‚æˆ‘è¿˜ä½¿ç”¨äº†é¢œè‰²ä»£ç ä»¥ä¾¿å¿«é€Ÿæ¦‚è§ˆï¼š
- en: '![](../Images/459bea0d48308f7f7011fe4bc18fadbe.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/459bea0d48308f7f7011fe4bc18fadbe.png)'
- en: Image by author
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: ä½œè€…æä¾›çš„å›¾åƒ
- en: So ideally, everything should be highlighted green. If you want to see the full
    report for the validation set, you can see it [here](https://neontreebot.be/data/Donut_vs_Pix2Struct/Donut_result_report.html),
    or download this [zip file](http://Donut_vs_pix2struct_validation_results.zip)
    locally.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: ç†æƒ³æƒ…å†µä¸‹ï¼Œä¸€åˆ‡éƒ½åº”è¯¥ç”¨ç»¿è‰²çªå‡ºæ˜¾ç¤ºã€‚å¦‚æœä½ æƒ³æŸ¥çœ‹éªŒè¯é›†çš„å®Œæ•´æŠ¥å‘Šï¼Œå¯ä»¥åœ¨ [è¿™é‡Œ](https://neontreebot.be/data/Donut_vs_Pix2Struct/Donut_result_report.html)
    æŸ¥çœ‹ï¼Œæˆ–è€…æœ¬åœ°ä¸‹è½½è¿™ä¸ª [zip æ–‡ä»¶](http://Donut_vs_pix2struct_validation_results.zip)ã€‚
- en: With this information we can spot usual OCR errors like *Dczcmbci* instead of
    *December,* or *GL420* instead of *GL420* (0â€™s and Oâ€™s are hard to distinguish),
    that lead to false positives.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: æœ‰äº†è¿™äº›ä¿¡æ¯ï¼Œæˆ‘ä»¬å¯ä»¥å‘ç°å¸¸è§çš„ OCR é”™è¯¯ï¼Œå¦‚*Dczcmbci*ï¼ˆåº”ä¸º*December*ï¼‰æˆ–*GL420*ï¼ˆåº”ä¸º*GL420*ï¼Œ0 å’Œ O éš¾ä»¥åŒºåˆ†ï¼‰ï¼Œè¿™äº›é”™è¯¯ä¼šå¯¼è‡´å‡é˜³æ€§ã€‚
- en: 'Letâ€™s focus now on the worst performing field: Voltage. Here are some samples
    of the inferred data, the ground truth and the actual relevant document snippet.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨è®©æˆ‘ä»¬å…³æ³¨è¡¨ç°æœ€å·®çš„å­—æ®µï¼šç”µå‹ã€‚ä»¥ä¸‹æ˜¯æ¨æ–­æ•°æ®ã€çœŸå®å€¼å’Œå®é™…ç›¸å…³æ–‡æ¡£ç‰‡æ®µçš„ä¸€äº›æ ·æœ¬ã€‚
- en: '![](../Images/38f1432cc00e4bad8d920228707e2d48.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/38f1432cc00e4bad8d920228707e2d48.png)'
- en: Image by author
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: ä½œè€…æä¾›çš„å›¾åƒ
- en: The problem here is that the ground truth is mostly wrong. There is no standard
    of including the unit (Volt or V) or not. Sometimes irrelevant text is taken along,
    sometimes just a (wrong!) number. I can see now why Donut had a hard time with
    this.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: é—®é¢˜åœ¨äºçœŸå®å€¼å¤§å¤šæ˜¯é”™è¯¯çš„ã€‚æ˜¯å¦åŒ…æ‹¬å•ä½ï¼ˆVolt æˆ– Vï¼‰æ²¡æœ‰æ ‡å‡†ã€‚æœ‰æ—¶ä¼šåŒ…å«æ— å…³æ–‡æœ¬ï¼Œæœ‰æ—¶åªæ˜¯ä¸€ä¸ªï¼ˆé”™è¯¯çš„ï¼ï¼‰æ•°å­—ã€‚æˆ‘ç°åœ¨æ˜ç™½ä¸ºä»€ä¹ˆ Donut ä¼šå¯¹æ­¤æ„Ÿåˆ°å›°éš¾ã€‚
- en: '![](../Images/2c144a76bc7dd36ffe3e1019b18a256b.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2c144a76bc7dd36ffe3e1019b18a256b.png)'
- en: Image by author
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: ä½œè€…æä¾›çš„å›¾åƒ
- en: Above are some samples where Donut actually returns the best answer while the
    ground truth is incomplete or wrong.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸Šé¢æ˜¯ä¸€äº›Donutå®é™…ä¸Šç»™å‡ºæœ€ä½³ç­”æ¡ˆçš„æ ·æœ¬ï¼Œè€Œå®é™…æƒ…å†µæ˜¯ä¸å®Œæ•´æˆ–é”™è¯¯çš„ã€‚
- en: '![](../Images/f3d2be65fbde5bad2154dd824fdac5eb.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f3d2be65fbde5bad2154dd824fdac5eb.png)'
- en: Image by author
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: ä½œè€…æä¾›çš„å›¾åƒ
- en: Above is another good example of bad training data confusing Donut. The â€˜Iâ€™
    letter in the ground truth is an artifact of the OCR reading the vertical line
    in front of the information. Sometimes itâ€™s there, other times not. If you preprocess
    your data to be consequent in this regard, Donut will learn this and adhere to
    this structure.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸Šé¢æ˜¯å¦ä¸€ä¸ªç³Ÿç³•è®­ç»ƒæ•°æ®æ··æ·†Donutçš„å¥½ä¾‹å­ã€‚åœ°é¢çœŸå®å€¼ä¸­çš„â€˜Iâ€™å­—æ¯æ˜¯OCRè¯»å–ä¿¡æ¯å‰çš„å‚ç›´çº¿çš„ä¼ªå½±ã€‚æœ‰æ—¶å®ƒå­˜åœ¨ï¼Œæœ‰æ—¶ä¸å­˜åœ¨ã€‚å¦‚æœä½ å¯¹æ•°æ®è¿›è¡Œé¢„å¤„ç†ï¼Œä½¿å…¶åœ¨è¿™æ–¹é¢ä¸€è‡´ï¼ŒDonutå°†ä¼šå­¦ä¹ å¹¶éµå¾ªè¿™ç§ç»“æ„ã€‚
- en: Finetuning Pix2Struct
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å¾®è°ƒPix2Struct
- en: Donutâ€™s results are holding up, will Pix2Structâ€™s as well? The colab notebook
    I used for the training can be found [here](https://github.com/Toon-nooT/notebooks/blob/main/Donut_vs_pix2struct_3_Ghega_Pix2Struct.ipynb).
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: Donutçš„ç»“æœä¿æŒç¨³å®šï¼ŒPix2Structçš„å‘¢ï¼Ÿæˆ‘ç”¨æ¥è®­ç»ƒçš„Colabç¬”è®°æœ¬å¯ä»¥åœ¨[è¿™é‡Œ](https://github.com/Toon-nooT/notebooks/blob/main/Donut_vs_pix2struct_3_Ghega_Pix2Struct.ipynb)æ‰¾åˆ°ã€‚
- en: After training for 75 minutes I got an edit distance score of 0.197 versus 0.116
    for Donut. It is definitely slower at converging.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: ç»è¿‡75åˆ†é’Ÿçš„è®­ç»ƒï¼Œæˆ‘å¾—åˆ°çš„ç¼–è¾‘è·ç¦»åˆ†æ•°ä¸º0.197ï¼Œè€ŒDonutçš„ä¸º0.116ã€‚è¿™æ˜¾ç„¶æ”¶æ•›é€Ÿåº¦è¾ƒæ…¢ã€‚
- en: Another observation is that so far every value that is returned starts with
    a space. This could be an error in the ImageCaptioningDataset class, but I did
    not investigate further into the root cause. I do remove this space when generating
    the validation results though.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: å¦ä¸€ä¸ªè§‚å¯Ÿç»“æœæ˜¯ï¼Œåˆ°ç›®å‰ä¸ºæ­¢ï¼Œæ¯ä¸ªè¿”å›çš„å€¼éƒ½ä»¥ä¸€ä¸ªç©ºæ ¼å¼€å¤´ã€‚è¿™å¯èƒ½æ˜¯ImageCaptioningDatasetç±»ä¸­çš„é”™è¯¯ï¼Œä½†æˆ‘æ²¡æœ‰è¿›ä¸€æ­¥è°ƒæŸ¥æ ¹æœ¬åŸå› ã€‚ä¸è¿‡ï¼Œæˆ‘åœ¨ç”ŸæˆéªŒè¯ç»“æœæ—¶ä¼šå»æ‰è¿™ä¸ªç©ºæ ¼ã€‚
- en: '[PRE0]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'I stopped the finetuning process after 2 hours because the validation metric
    went up again:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨2å°æ—¶åæˆ‘åœæ­¢äº†å¾®è°ƒè¿‡ç¨‹ï¼Œå› ä¸ºéªŒè¯æŒ‡æ ‡å†æ¬¡ä¸Šå‡ï¼š
- en: '![](../Images/9324a3a41e08ae77d58da13021dbfe51.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9324a3a41e08ae77d58da13021dbfe51.png)'
- en: Image by author
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: ä½œè€…æä¾›çš„å›¾åƒ
- en: But what does that mean on field level for the validation set?
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: é‚£ä¹ˆè¿™å¯¹éªŒè¯é›†çš„å­—æ®µçº§åˆ«æ„å‘³ç€ä»€ä¹ˆå‘¢ï¼Ÿ
- en: '![](../Images/016b40e227339838bf67744203460909.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/016b40e227339838bf67744203460909.png)'
- en: Image by author
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: ä½œè€…æä¾›çš„å›¾åƒ
- en: That looks a lot worse than the results of Donut! If you want to see the full
    HTML report, you can see it [here](https://neontreebot.be/data/Donut_vs_Pix2Struct/Pix2Struct_result_report.html),
    or download this [zip file](http://Donut_vs_pix2struct_validation_results.zip)
    locally.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™çœ‹èµ·æ¥æ¯”Donutçš„ç»“æœå·®å¾—å¤šï¼å¦‚æœä½ æƒ³æŸ¥çœ‹å®Œæ•´çš„HTMLæŠ¥å‘Šï¼Œå¯ä»¥åœ¨[è¿™é‡Œ](https://neontreebot.be/data/Donut_vs_Pix2Struct/Pix2Struct_result_report.html)æŸ¥çœ‹ï¼Œæˆ–è€…åœ¨æœ¬åœ°ä¸‹è½½[è¿™ä¸ªzipæ–‡ä»¶](http://Donut_vs_pix2struct_validation_results.zip)ã€‚
- en: Only the classification between a *datasheet* and a *patent* seems to be quite
    OK (but not as good as Donut). The other fields are just plain bad. Can we deduct
    whatâ€™s going on?
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: åªæœ‰åœ¨*æ•°æ®è¡¨*å’Œ*ä¸“åˆ©*ä¹‹é—´çš„åˆ†ç±»ä¼¼ä¹è¿˜ä¸é”™ï¼ˆä½†ä¸å¦‚Donutï¼‰ã€‚å…¶ä»–å­—æ®µåˆ™å®Œå…¨ä¸ä½³ã€‚æˆ‘ä»¬èƒ½æ¨æ–­å‘ç”Ÿäº†ä»€ä¹ˆå—ï¼Ÿ
- en: For the *patent* docs, I see lots of orange lines which mean that Pix2Struct
    did not return those fields at all.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äº*ä¸“åˆ©*æ–‡æ¡£ï¼Œæˆ‘çœ‹åˆ°å¾ˆå¤šæ©™è‰²çº¿æ¡ï¼Œè¿™æ„å‘³ç€Pix2Structæ ¹æœ¬æ²¡æœ‰è¿”å›è¿™äº›å­—æ®µã€‚
- en: '![](../Images/19611b7234cb2f3788e67025fbb526ca.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/19611b7234cb2f3788e67025fbb526ca.png)'
- en: Image by author
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: ä½œè€…æä¾›çš„å›¾åƒ
- en: And even for *patents* where it returns fields, they are completely made up.
    Whereas Donutâ€™s errors stem from picking it from another region on the document
    or having minor OCR mistakes, Pix2Struct is hallucinating here.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: å³ä½¿åœ¨*ä¸“åˆ©*ä¸­è¿”å›å­—æ®µï¼Œå®ƒä»¬ä¹Ÿå®Œå…¨æ˜¯è™šæ„çš„ã€‚è€ŒDonutçš„é”™è¯¯æºäºä»æ–‡æ¡£çš„å…¶ä»–åŒºåŸŸæå–æˆ–æœ‰è½»å¾®çš„OCRé”™è¯¯ï¼ŒPix2Structåœ¨è¿™é‡Œåˆ™æ˜¯å‡ºç°äº†å¹»è§‰ã€‚
- en: 'Disappointed by Pix2Structâ€™s performance, I tried several new training runs
    in hopes of better results:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹Pix2Structçš„è¡¨ç°æ„Ÿåˆ°å¤±æœ›ï¼Œæˆ‘å°è¯•äº†å‡ æ¬¡æ–°çš„è®­ç»ƒä»¥æœŸè·å¾—æ›´å¥½çš„ç»“æœï¼š
- en: '![](../Images/d0505d47bb938ff2050c32530298ed09.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d0505d47bb938ff2050c32530298ed09.png)'
- en: Image by author
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: ä½œè€…æä¾›çš„å›¾åƒ
- en: 'I tried lowering gradually the accumulate_grad_batches from 8 to 1\. But then
    the learn rate is too high and overshoots. Lowering that to 1e-5 makes the model
    not converge. Other combinations lead to the model collapsing. Even if with some
    specific hyperparameters the validation metric looked quite OK, it was giving
    a lot of incorrect or unparseable lines, like:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘å°è¯•å°†accumulate_grad_batchesé€æ¸ä»8é™åˆ°1ã€‚ä½†è¿™æ ·å­¦ä¹ ç‡è¿‡é«˜ï¼Œä¼šå¯¼è‡´è¶…è°ƒã€‚å°†å…¶é™ä½åˆ°1e-5ä¼šä½¿æ¨¡å‹æ— æ³•æ”¶æ•›ã€‚å…¶ä»–ç»„åˆåˆ™å¯¼è‡´æ¨¡å‹å´©æºƒã€‚å³ä½¿åœ¨ä¸€äº›ç‰¹å®šçš„è¶…å‚æ•°ä¸‹ï¼ŒéªŒè¯æŒ‡æ ‡çœ‹èµ·æ¥ç›¸å½“ä¸é”™ï¼Œä½†å®ƒç»™å‡ºäº†å¾ˆå¤šä¸æ­£ç¡®æˆ–æ— æ³•è§£æçš„è¡Œï¼Œä¾‹å¦‚ï¼š
- en: '[PRE1]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: None of these attempts gave me substantial better results, so I left it at that.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›å°è¯•éƒ½æ²¡æœ‰ç»™æˆ‘å¸¦æ¥å®è´¨æ€§çš„æ›´å¥½ç»“æœï¼Œæ‰€ä»¥æˆ‘å°±æ­¤åœæ­¢äº†ã€‚
- en: Until I saw that a cross attention [bug](https://github.com/huggingface/transformers/pull/25200)
    was fixed in the huggingface implementation. So i decided to give it a last try.
    Trained for 2 and a half hour and stopped at a validation metric of 0.1416 .
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: ç›´åˆ°æˆ‘çœ‹åˆ°huggingfaceå®ç°ä¸­çš„äº¤å‰æ³¨æ„åŠ› [bug](https://github.com/huggingface/transformers/pull/25200)è¢«ä¿®å¤ã€‚å› æ­¤ï¼Œæˆ‘å†³å®šå†è¯•ä¸€æ¬¡ã€‚è®­ç»ƒäº†ä¸¤ä¸ªåŠå°æ—¶ï¼Œåœåœ¨0.1416çš„éªŒè¯æŒ‡æ ‡ä¸Šã€‚
- en: '![](../Images/75ea338e70597180b073e5d5230e3d58.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/75ea338e70597180b073e5d5230e3d58.png)'
- en: Image by author
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: ä½œè€…æä¾›çš„å›¾ç‰‡
- en: '![](../Images/0f56d46e41ed75b07b8cd2db7e053f11.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0f56d46e41ed75b07b8cd2db7e053f11.png)'
- en: Image by author
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: ä½œè€…æä¾›çš„å›¾ç‰‡
- en: This looks definitely better than all previous runs. Looking at the HTML [report](https://neontreebot.be/data/Donut_vs_Pix2Struct/Pix2Struct_result_report_20230804.html),
    it now seems to hallucinate less. Overall itâ€™s still performing worse than Donut.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¾ç„¶æ¯”æ‰€æœ‰ä¹‹å‰çš„ç»“æœéƒ½è¦å¥½ã€‚æŸ¥çœ‹HTML [æŠ¥å‘Š](https://neontreebot.be/data/Donut_vs_Pix2Struct/Pix2Struct_result_report_20230804.html)ï¼Œç°åœ¨ä¼¼ä¹å¹»è§‰æ›´å°‘ã€‚æ€»ä½“æ¥è¯´ï¼Œå®ƒçš„è¡¨ç°ä»ä¸å¦‚Donutã€‚
- en: As for reasons why, I have two theories. Firstly, Pix2Struct was mainly trained
    on HTML web page images (predicting what is behind masked image parts) and has
    trouble switching to another domain, namely raw text. Secondly, the dataset used
    was challenging. It contains many OCR errors and non-conformities (such as including
    units, length, minus signs). In my other experiments it really came to light that
    the quality and conformity of the dataset is more important than the quantity.
    In this dataset the data quality is really subpar. Maybe that is why I could not
    replicate the claim in the paper that Pix2Struct exceeds Donuts performance.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: è‡³äºåŸå› ï¼Œæˆ‘æœ‰ä¸¤ä¸ªç†è®ºã€‚é¦–å…ˆï¼ŒPix2Structä¸»è¦åœ¨HTMLç½‘é¡µå›¾åƒä¸Šè®­ç»ƒï¼ˆé¢„æµ‹æ©ç å›¾åƒéƒ¨åˆ†åé¢çš„å†…å®¹ï¼‰ï¼Œå¹¶ä¸”åœ¨åˆ‡æ¢åˆ°å¦ä¸€ä¸ªé¢†åŸŸï¼Œå³åŸå§‹æ–‡æœ¬æ—¶ï¼Œé‡åˆ°äº†å›°éš¾ã€‚å…¶æ¬¡ï¼Œä½¿ç”¨çš„æ•°æ®é›†éå¸¸å…·æœ‰æŒ‘æˆ˜æ€§ã€‚å®ƒåŒ…å«äº†è®¸å¤šOCRé”™è¯¯å’Œä¸ä¸€è‡´ï¼ˆå¦‚åŒ…å«å•ä½ã€é•¿åº¦ã€è´Ÿå·ï¼‰ã€‚åœ¨æˆ‘çš„å…¶ä»–å®éªŒä¸­ï¼Œæˆ‘çœŸçš„å‘ç°æ•°æ®é›†çš„è´¨é‡å’Œä¸€è‡´æ€§æ¯”æ•°é‡æ›´é‡è¦ã€‚åœ¨è¿™ä¸ªæ•°æ®é›†ä¸­ï¼Œæ•°æ®è´¨é‡çœŸçš„å¾ˆå·®ã€‚ä¹Ÿè®¸è¿™å°±æ˜¯æˆ‘æ— æ³•å¤åˆ¶è®ºæ–‡ä¸­å£°ç§°Pix2Structè¶…è¶ŠDonutè¡¨ç°çš„åŸå› ã€‚
- en: Inference speed
  id: totrans-67
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ¨æ–­é€Ÿåº¦
- en: 'How do the two models compare in terms of speed? All trainings were done on
    the same T4 architecture, so the times can be readily compared. We already saw
    that Pix2Struct takes much longer to converge. But what about inference times?
    We can compare the time it took for inferring the validation set:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸¤ç§æ¨¡å‹åœ¨é€Ÿåº¦æ–¹é¢å¦‚ä½•æ¯”è¾ƒï¼Ÿæ‰€æœ‰è®­ç»ƒéƒ½åœ¨ç›¸åŒçš„T4æ¶æ„ä¸Šè¿›è¡Œï¼Œå› æ­¤æ—¶é—´å¯ä»¥ç›´æ¥æ¯”è¾ƒã€‚æˆ‘ä»¬å·²ç»çœ‹åˆ°Pix2Structæ”¶æ•›æ‰€éœ€çš„æ—¶é—´è¦é•¿å¾—å¤šã€‚é‚£ä¹ˆæ¨æ–­æ—¶é—´å‘¢ï¼Ÿæˆ‘ä»¬å¯ä»¥æ¯”è¾ƒæ¨æ–­éªŒè¯é›†æ‰€éœ€çš„æ—¶é—´ï¼š
- en: '![](../Images/ffe57189a296986fe0d28e53c9caa148.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ffe57189a296986fe0d28e53c9caa148.png)'
- en: Image by author
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: ä½œè€…æä¾›çš„å›¾ç‰‡
- en: Donut takes on average 1.3 seconds per document to extract, while Pix2Struct
    more than double.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: Donutæ¯ä¸ªæ–‡æ¡£æå–çš„å¹³å‡æ—¶é—´ä¸º1.3ç§’ï¼Œè€ŒPix2Structåˆ™è¶…è¿‡ä¸¤å€ã€‚
- en: '**Takeaways**'
  id: totrans-72
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**è¦ç‚¹**'
- en: The clear winner for me is Donut. In terms of ease-of-use, performance, training
    stability and speed.
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¯¹æˆ‘æ¥è¯´ï¼Œæœ€ç»ˆçš„èµ¢å®¶æ˜¯Donutã€‚åœ¨æ˜“ç”¨æ€§ã€æ€§èƒ½ã€è®­ç»ƒç¨³å®šæ€§å’Œé€Ÿåº¦æ–¹é¢ã€‚
- en: Pix2Struct is challenging to train because it is very sensitive to training
    hyperparameters. It converges slower and doesnâ€™t reach the results of Donut in
    this dataset. It may proof worthwhile to revisit Pix2Struct with a high(er) quality
    dataset.
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pix2Structè®­ç»ƒå…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå› ä¸ºå®ƒå¯¹è®­ç»ƒè¶…å‚æ•°éå¸¸æ•æ„Ÿã€‚å®ƒæ”¶æ•›è¾ƒæ…¢ï¼Œå¹¶ä¸”åœ¨è¿™ä¸ªæ•°æ®é›†ä¸Šæ²¡æœ‰è¾¾åˆ°Donutçš„ç»“æœã€‚å¯èƒ½å€¼å¾—é‡æ–°è€ƒè™‘ä½¿ç”¨æ›´é«˜è´¨é‡çš„æ•°æ®é›†æ¥å°è¯•Pix2Structã€‚
- en: Because the Ghega dataset contains too many inconsistencies, I will refrain
    from using it in further experiments.
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç”±äºGhegaæ•°æ®é›†åŒ…å«å¤ªå¤šä¸ä¸€è‡´æ€§ï¼Œæˆ‘å°†é¿å…åœ¨è¿›ä¸€æ­¥å®éªŒä¸­ä½¿ç”¨å®ƒã€‚
- en: '**Are there any alternative models?**'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '**æ˜¯å¦æœ‰å…¶ä»–æ›¿ä»£æ¨¡å‹ï¼Ÿ**'
- en: '[Dessurt](https://arxiv.org/pdf/2203.16618v3.pdf), which seems to share a similar
    architecture with Donut should perform in the same league.'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Dessurt](https://arxiv.org/pdf/2203.16618v3.pdf)ï¼Œä¼¼ä¹ä¸Donutæœ‰ç›¸ä¼¼çš„æ¶æ„ï¼Œåº”è¯¥è¡¨ç°ç›¸å½“ã€‚'
- en: '[DocParser](https://arxiv.org/pdf/2304.12484.pdf), which the paper claims to
    perform even a little better. Unfortunately there is no plan to release this model
    in the future.'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[DocParser](https://arxiv.org/pdf/2304.12484.pdf)ï¼Œè®ºæ–‡ç§°å…¶è¡¨ç°ç”šè‡³æ›´å¥½ã€‚ä¸å¹¸çš„æ˜¯ï¼Œç›®å‰æ²¡æœ‰è®¡åˆ’å°†è¯¥æ¨¡å‹å‘å¸ƒåˆ°æœªæ¥ã€‚'
- en: '[mPLUG-DocOwl](https://arxiv.org/abs/2307.02499) will soon be released which
    is yet another OCR-Free LLM for document understanding with promising benchmarks.'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[mPLUG-DocOwl](https://arxiv.org/abs/2307.02499)å°†å¾ˆå¿«å‘å¸ƒï¼Œè¿™æ˜¯å¦ä¸€ä¸ªæœ‰å‰æ™¯çš„æ— OCR LLMæ–‡æ¡£ç†è§£å·¥å…·ã€‚'
- en: 'You may also like:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ å¯èƒ½è¿˜ä¼šå–œæ¬¢ï¼š
- en: '[](https://toon-beerten.medium.com/hands-on-document-data-extraction-with-transformer-7130df3b6132?source=post_page-----38ce26f41951--------------------------------)
    [## Hands-on: document data extraction with ğŸ© transformer'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://toon-beerten.medium.com/hands-on-document-data-extraction-with-transformer-7130df3b6132?source=post_page-----38ce26f41951--------------------------------)
    [## å®æˆ˜ï¼šä½¿ç”¨ğŸ©å˜å‹å™¨è¿›è¡Œæ–‡æ¡£æ•°æ®æå–'
- en: My experience using donut transformers model to extract invoice indexes.
  id: totrans-82
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: æˆ‘ä½¿ç”¨ç”œç”œåœˆè½¬æ¢å™¨æ¨¡å‹æ¥æå–å‘ç¥¨ç´¢å¼•çš„ç»éªŒã€‚
- en: toon-beerten.medium.com](https://toon-beerten.medium.com/hands-on-document-data-extraction-with-transformer-7130df3b6132?source=post_page-----38ce26f41951--------------------------------)
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '[toon-beerten.medium.com](https://toon-beerten.medium.com/hands-on-document-data-extraction-with-transformer-7130df3b6132?source=post_page-----38ce26f41951--------------------------------)'
- en: 'References:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: å‚è€ƒæ–‡çŒ®ï¼š
- en: '[](https://arxiv.org/abs/2210.03347?source=post_page-----38ce26f41951--------------------------------)
    [## Pix2Struct: Screenshot Parsing as Pretraining for Visual Language Understanding'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://arxiv.org/abs/2210.03347?source=post_page-----38ce26f41951--------------------------------)
    [## Pix2Struct: æˆªå›¾è§£æä½œä¸ºè§†è§‰è¯­è¨€ç†è§£çš„é¢„è®­ç»ƒ'
- en: Visually-situated language is ubiquitous -- sources range from textbooks with
    diagrams to web pages with images andâ€¦
  id: totrans-86
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: è§†è§‰å®šä½è¯­è¨€æ— å¤„ä¸åœ¨â€”â€”æ¥æºä»å¸¦æœ‰å›¾è¡¨çš„æ•™ç§‘ä¹¦åˆ°å¸¦æœ‰å›¾åƒçš„ç½‘é¡µç­‰ã€‚
- en: arxiv.org](https://arxiv.org/abs/2210.03347?source=post_page-----38ce26f41951--------------------------------)
    [](https://arxiv.org/abs/2111.15664?source=post_page-----38ce26f41951--------------------------------)
    [## OCR-free Document Understanding Transformer
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '[arxiv.org](https://arxiv.org/abs/2210.03347?source=post_page-----38ce26f41951--------------------------------)
    [](https://arxiv.org/abs/2111.15664?source=post_page-----38ce26f41951--------------------------------)
    [## æ—  OCR æ–‡æ¡£ç†è§£è½¬æ¢å™¨'
- en: Understanding document images (e.g., invoices) is a core but challenging task
    since it requires complex functions suchâ€¦
  id: totrans-88
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ç†è§£æ–‡æ¡£å›¾åƒï¼ˆä¾‹å¦‚å‘ç¥¨ï¼‰æ˜¯ä¸€é¡¹æ ¸å¿ƒä½†å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ï¼Œå› ä¸ºå®ƒéœ€è¦å¤æ‚çš„åŠŸèƒ½ï¼Œæ¯”å¦‚â€¦â€¦
- en: arxiv.org](https://arxiv.org/abs/2111.15664?source=post_page-----38ce26f41951--------------------------------)
    [](https://github.com/Toon-nooT/notebooks/tree/main?source=post_page-----38ce26f41951--------------------------------)
    [## GitHub - Toon-nooT/notebooks
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '[arxiv.org](https://arxiv.org/abs/2111.15664?source=post_page-----38ce26f41951--------------------------------)
    [](https://github.com/Toon-nooT/notebooks/tree/main?source=post_page-----38ce26f41951--------------------------------)
    [## GitHub - Toon-nooT/notebooks'
- en: Contribute to Toon-nooT/notebooks development by creating an account on GitHub.
  id: totrans-90
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: é€šè¿‡åœ¨ GitHub ä¸Šåˆ›å»ºå¸æˆ·æ¥ä¸º Toon-nooT/notebooks çš„å¼€å‘åšè´¡çŒ®ã€‚
- en: github.com](https://github.com/Toon-nooT/notebooks/tree/main?source=post_page-----38ce26f41951--------------------------------)
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '[github.com](https://github.com/Toon-nooT/notebooks/tree/main?source=post_page-----38ce26f41951--------------------------------)'
