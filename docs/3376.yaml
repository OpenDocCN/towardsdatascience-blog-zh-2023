- en: Best Practices for Debugging Errors in Logistic Regression with Python
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Python调试逻辑回归错误的最佳实践
- en: 原文：[https://towardsdatascience.com/how-to-fix-errors-in-logistic-regression-32b8dd9fe6d7?source=collection_archive---------4-----------------------#2023-11-13](https://towardsdatascience.com/how-to-fix-errors-in-logistic-regression-32b8dd9fe6d7?source=collection_archive---------4-----------------------#2023-11-13)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/how-to-fix-errors-in-logistic-regression-32b8dd9fe6d7?source=collection_archive---------4-----------------------#2023-11-13](https://towardsdatascience.com/how-to-fix-errors-in-logistic-regression-32b8dd9fe6d7?source=collection_archive---------4-----------------------#2023-11-13)
- en: Optimizing performance using unstructured, real-world data
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 优化性能使用非结构化、真实世界数据
- en: '[](https://gabeverzino.medium.com/?source=post_page-----32b8dd9fe6d7--------------------------------)[![Gabe
    Verzino](../Images/36452afec54430c55594a26247136f6f.png)](https://gabeverzino.medium.com/?source=post_page-----32b8dd9fe6d7--------------------------------)[](https://towardsdatascience.com/?source=post_page-----32b8dd9fe6d7--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----32b8dd9fe6d7--------------------------------)
    [Gabe Verzino](https://gabeverzino.medium.com/?source=post_page-----32b8dd9fe6d7--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://gabeverzino.medium.com/?source=post_page-----32b8dd9fe6d7--------------------------------)![](../Images/36452afec54430c55594a26247136f6f.png)](https://gabeverzino.medium.com/?source=post_page-----32b8dd9fe6d7--------------------------------)[](https://towardsdatascience.com/?source=post_page-----32b8dd9fe6d7--------------------------------)![](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----32b8dd9fe6d7--------------------------------)
    [Gabe Verzino](https://gabeverzino.medium.com/?source=post_page-----32b8dd9fe6d7--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fb4abbbfdcbbb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-fix-errors-in-logistic-regression-32b8dd9fe6d7&user=Gabe+Verzino&userId=b4abbbfdcbbb&source=post_page-b4abbbfdcbbb----32b8dd9fe6d7---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----32b8dd9fe6d7--------------------------------)
    ·13 min read·Nov 13, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F32b8dd9fe6d7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-fix-errors-in-logistic-regression-32b8dd9fe6d7&user=Gabe+Verzino&userId=b4abbbfdcbbb&source=-----32b8dd9fe6d7---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fb4abbbfdcbbb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-fix-errors-in-logistic-regression-32b8dd9fe6d7&user=Gabe+Verzino&userId=b4abbbfdcbbb&source=post_page-b4abbbfdcbbb----32b8dd9fe6d7---------------------post_header-----------)
    发布在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----32b8dd9fe6d7--------------------------------)
    ·13 分钟阅读·2023 年 11 月 13 日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F32b8dd9fe6d7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-fix-errors-in-logistic-regression-32b8dd9fe6d7&user=Gabe+Verzino&userId=b4abbbfdcbbb&source=-----32b8dd9fe6d7---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F32b8dd9fe6d7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-fix-errors-in-logistic-regression-32b8dd9fe6d7&source=-----32b8dd9fe6d7---------------------bookmark_footer-----------)![](../Images/ece08cc83ca0517ebe73fcf363268779.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F32b8dd9fe6d7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-fix-errors-in-logistic-regression-32b8dd9fe6d7&source=-----32b8dd9fe6d7---------------------bookmark_footer-----------)![](../Images/ece08cc83ca0517ebe73fcf363268779.png)'
- en: Vardan Papikyan ([Unsplash](https://unsplash.com/photos/a-person-holding-two-pieces-of-a-puzzle-JzE1dHEaAew))
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: Vardan Papikyan（[Unsplash](https://unsplash.com/photos/a-person-holding-two-pieces-of-a-puzzle-JzE1dHEaAew))
- en: Much has been written about the basics of Logistic Regression (LR) — its versatility,
    time-tested performance, even the underlying math. But knowing *how* to implement
    LR successfully and debug inevitable errors is much more challenging to do. That
    information lives deep in QA websites, academic papers, or simply comes with experience.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 许多文章都详细介绍了逻辑回归（LR）的基础知识 —— 它的多功能性、经过时间验证的性能，甚至其潜在的数学原理。但是，了解*如何*成功实施LR并调试不可避免的错误要困难得多。这些信息通常隐藏在问答网站、学术论文中，或者仅仅是通过经验获得。
- en: The reality is, not every output will be as clean as the iconic iris dataset,
    swiftly classifying flower types. On large datasets (the datasets you likely use
    on the job), LR models are likely to encounter problems. Implausibly high coefficients.
    NaN standard errors. Failing to converge.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 现实是，并不是每一个输出都像标志性的鸢尾花数据集那样清晰，迅速地分类花的类型。在大数据集上（你可能在工作中使用的数据集），LR 模型很可能会遇到问题。不合理高的系数。NaN
    的标准误差。无法收敛。
- en: What’s to blame? Bad data? Your model?
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 责任归谁？是坏数据？还是你的模型？
- en: To organize the landscape a bit, I conducted some research and compiled a list
    of common LR errors, reasons, and possible solutions.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 为了对这个领域进行整理，我进行了一些研究并编制了一个常见 LR 错误、原因和可能解决方法的列表。
- en: '![](../Images/7a1c7c056d5da9f85adf5dacee78042e.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7a1c7c056d5da9f85adf5dacee78042e.png)'
- en: LR Model output problems, reasons and solutions (Gabe Verzino)
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: LR 模型输出的问题、原因和解决方法（Gabe Verzino）
- en: The above table is not exhaustive by any means, but it’s all in one place. Below
    I will use fabricated data (that’s imbalanced, sparse, categorical… but typical)
    to artificially re-create these errors and then fix them again.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 上述表格并不是详尽无遗的，但都在一个地方。下面我将使用虚构的数据（不平衡、稀疏、分类等……但典型的）来人为地再次制造这些错误，然后再次修复它们。
- en: But first.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 但首先。
- en: Quick Background
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 快速背景
- en: 'As you begin to think about LR, there are a few things unique to these models
    that you should remember:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 当你开始思考 LR 时，有一些对这些模型独特的事情你应该记住：
- en: LR is technically a probability model, not a classifier
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从技术上讲，LR 是一个概率模型，而不是分类器
- en: LR requires a linear decision boundary; it assumes linearity between features
    and target variable, which you can determine with visuals, crosstabs, or ConvexHull
    analysis from *scipy.spatial*
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: LR 需要一个线性的决策边界；它假设特征和目标变量之间的线性关系，你可以通过视觉、交叉表或 *scipy.spatial* 中的 ConvexHull
    分析来确定这一点。
- en: You cannot have missing values or major outliers
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你不能有缺失值或主要的异常值
- en: As the number of features increases, you are more likely to experience multicollinearity
    and overfitting (fixable with VIF and regularization, respectively)
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 随着特征数的增加，你更可能遇到多重共线性和过拟合问题（可以分别通过 VIF 和正则化来解决）
- en: The most popular LR packages in Python come from *sklearn* and *statsmodels*
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Python 中最流行的 LR 包来自 *sklearn* 和 *statsmodels*
- en: Now let’s get into some common problems.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们来看一些常见的问题。
- en: 'Problem 1: Convergence warning, optimization failed'
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '问题 1: 收敛警告，优化失败'
- en: 'Using the python *statsmodels* package, you may run a basic LR and get the
    warning “ConvergenceWarning: Maximum Likelihood optimization failed to converge.”'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '使用 Python 的 *statsmodels* 包，你可以运行一个基本的 LR 并得到警告：“ConvergenceWarning: Maximum
    Likelihood optimization failed to converge.”'
- en: '[PRE0]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '![](../Images/6b6a0fb8106ed7abe831c5aa339c19a3.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6b6a0fb8106ed7abe831c5aa339c19a3.png)'
- en: The coefficients and standard errors may even populate, like normal, including
    those from *LogisticRegression* in the *sklearn.linear_model* package.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 系数和标准误差可能会像平常一样出现，包括来自 *sklearn.linear_model* 包中的 *LogisticRegression*。
- en: Provided the results, you may be tempted to stop here. But don’t. You want to
    ensure your model converges to produce the best (lowest) cost function and model
    fit [1].
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 提供了结果后，你可能会停下来。但是不要停止。你希望确保你的模型收敛以产生最佳（最低）成本函数和模型拟合 [1]。
- en: You can solve this in *statsmodels* or *sklearn* by changing the solver/method
    or increasing the *maxiter* parameter. In *sklearn*, the tuning parameter *C*
    can also be lowered to apply increased L2 regularization, and can be iteratively
    tested along a logarithmic scale (100, 10, 1, 0.1, 0.01, etc.) [2].
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在 *statsmodels* 或 *sklearn* 中通过改变求解器/方法或增加 *maxiter* 参数来解决这个问题。在 *sklearn*
    中，调整参数 *C* 可以降低应用增加的 L2 正则化，并可以在对数尺度上进行迭代测试（100, 10, 1, 0.1, 0.01 等） [2]。
- en: In my particular case, I increased to *maxiter*=300000 and the model converged.
    The reason it worked is because I provided the model (e.g., solvers) more attempts
    at converging and finding the best parameters [3]. And those parameters, such
    as the coefficients and p-values, do indeed become more accurate.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在我的特定案例中，我增加到 *maxiter*=300000，模型就收敛了。它有效的原因是我提供了更多的尝试次数给模型（例如求解器），以便收敛并找到最佳参数
    [3]。而这些参数，如系数和 p 值，确实变得更加准确。
- en: '![](../Images/555d15dead43f9bcefb8fbfa4d459617.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/555d15dead43f9bcefb8fbfa4d459617.png)'
- en: method=’bfgs’, maxiter=0
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: method=’bfgs’, maxiter=0
- en: '![](../Images/1320d7f33ac6abab459908ebe51bc6f7.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1320d7f33ac6abab459908ebe51bc6f7.png)'
- en: method=’bfgs’, maxiter=30000
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: method=’bfgs’, maxiter=30000
- en: 'Problem 2: Added a feature, but LR outputs didn’t update'
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '问题 2: 添加了一个特征，但 LR 输出没有更新'
- en: This one is easy to miss, but easy to diagnose. If you’re working with many
    features or didn’t catch it in data cleaning, you may accidentally include a categorical
    feature in your LR model that is nearly constant or has only one level… bad stuff.
    The inclusion of such a feature will render coefficients and standard-errors without
    any warnings.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 这个错误很容易被忽略，但也很容易诊断。如果您正在处理许多特征或在数据清理中没有注意到它，您可能会意外地在LR模型中包含一个几乎恒定或只有一个级别的分类特征……这是个坏事。包含这样一个特征将导致系数和标准误差没有任何警告。
- en: Here’s our model and output without the bad feature.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们的模型和没有坏特征的输出。
- en: '[PRE1]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '![](../Images/d2cf4366c6e491f9009ce96fad58e785.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d2cf4366c6e491f9009ce96fad58e785.png)'
- en: Now, let’s add a one-level feature *type*, with its reference category set to
    ‘Missing’.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们添加一个一级特征*type*，其参考类别设为‘Missing’。
- en: '[PRE2]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: And here we see the impact of that superfluous feature… which was no impact
    at all. The R-squared value stays the same, highlighted above and below.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们看到了那个多余的特征的影响……实际上并没有影响。R平方值保持不变，如上下所示。
- en: '![](../Images/477a18c48288ae152b0ef4decec8cd8c.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/477a18c48288ae152b0ef4decec8cd8c.png)'
- en: Logistic Regression cannot make meaningful estimates on a feature with one level
    or constant values, and may discard it from the model. This happens at no determinant
    to the model itself, but still, best practice is to include only the features
    in the model that are having a positive impact.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑回归无法对具有一个级别或常数值的特征进行有意义的估计，并且可能会从模型中丢弃它。这对模型本身没有影响，但最佳做法仍然是仅包含对模型有正面影响的特征。
- en: A simple *value_counts()* of our feature *type* reveals it has one level, indicating
    you want to drop the feature from your model.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 对我们的特征*type*进行简单的*value_counts()*，发现它只有一个级别，表明您想从模型中删除该特征。
- en: '![](../Images/8d0043b00e630e12ae3b6ddab946aeee.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8d0043b00e630e12ae3b6ddab946aeee.png)'
- en: 'Problem 3: “Inverting Hessian failed” (and NaN standard errors)'
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题3：“反转Hessian失败”（和NaN标准误差）
- en: 'This is a big one, and happens a lot. Let’s create this new error by including
    four more features into our LR model: VA, VB, VC and VI. They’re all categorical,
    each with 3 levels (0, 1 and “Missing” as a reference).'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个大问题，而且经常发生。让我们通过将VA、VB、VC和VI这四个特征包含到我们的LR模型中来创建这个新错误。它们都是分类的，每个有3个级别（0、1和“Missing”作为参考）。
- en: '[PRE3]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'And here is our new error:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们的新错误：
- en: '![](../Images/299d000bb77ddb4dcaafb9fbb2ad3835.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/299d000bb77ddb4dcaafb9fbb2ad3835.png)'
- en: Exploring the outputs a bit, we see coefficients rendered but none of the standard
    errors or p-values. *Sklearn* provides coefficients as well.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 探索输出时，我们看到系数已经计算，但标准误差或p值没有。*Sklearn*也提供系数。
- en: '![](../Images/57210c6e56574499fa797890631e8af5.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/57210c6e56574499fa797890631e8af5.png)'
- en: But why the NaN values? And why do you need to invert Hessian, anyway?
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 但是NaN值为什么？为什么需要反转Hessian呢？
- en: Many optimization algorithms use the Hessian inverse (or an estimate of it)
    to maximize the likelihood function. So when inversion has failed, it means the
    Hessian (technically, a second derivative of the loglikelihood) is not a positive
    definite [4]. Visually, some features have huge curvatures while some have smaller
    curvatures, and the result is the model cannot produce standard errors for the
    parameters.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 许多优化算法使用Hessian逆（或其估计）来最大化似然函数。因此，当反转失败时，意味着Hessian（技术上是对数似然的二阶导数）不是正定[4]。从视觉上看，某些特征具有巨大的曲率，而其他特征具有较小的曲率，结果是模型无法为参数生成标准误差。
- en: More to the point, when a Hessian is not invertible, no computational changes
    can make it inverse because it simply does not exist [5].
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 更重要的是，当Hessian不可逆时，没有计算上的更改可以使其反转，因为它根本不存在[5]。
- en: 'What are the causes of this? There are three most common:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 造成这种情况的原因是什么？有三种最常见的：
- en: There are more feature variables than observations
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 特征变量比观察值更多
- en: The categorical feature(s) have very low frequency levels
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 分类特征的频率级别非常低
- en: Multicollinearity exists between features
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 特征之间存在多重共线性
- en: Since our dataset has many rows (~25,000) relative to features (14) we can safely
    ignore the first possibility (though solutions exist for this exact problem [6]).
    The third possibility may be at play, and we can check that with variance inflation
    factor (VIF). The second possibility is a bit easier to diagnose, so let’s start
    there.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们的数据集有许多行（~25,000）相对于特征（14），我们可以安全地忽略第一种可能性（尽管存在解决这个确切问题的解决方案[6]）。第三种可能性可能存在，并且我们可以通过方差膨胀因子（VIF）来检查。第二种可能性更容易诊断，所以让我们从这里开始。
- en: Note that features VA, VB, VC and VI are mainly comprised of 1s and 0s.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/77be00b2bb75c73c665a183fef49031b.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
- en: In particular, the VI feature has a very low (relative) frequency for the ‘Missing’
    category, actually on the order of 0.03% (9/24,874).
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/29944d309a986ad9a1426f60a9fd8dfc.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
- en: Let’s say we confirm with our business context that we can collapse 1s and “Missing”
    together. Or, at the very least, any potential consequences to refactoring data
    in this way would be far less than accepting a model with known errors (and no
    standard errors to speak of).
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/93868b80adb5151783e2976628d35f9b.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
- en: So we created VI_alt, which has 2 levels, and 0 can serve as our reference.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: Exchanging VI_alt for VI in the model,
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: There’s a slight improvement to the overall model because it converges without
    any errors, coefficients render and standard errors now appear. Again, still a
    poorly-fitted model, but it’s now a workingmodel, which is our aim here.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/de5b2d336595f759055c5b16bbf8f2aa.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
- en: Our third and final possibility for Hessian failing inversion is multi-collinearity.
    Now, multi-collinearity is a hot topic in machine learning, I think because (1)
    it plagues many popular models like LR, linear regression, KNN and Naive Bayes
    (2) the VIF heuristic for removing features turns out to be more art than science,
    and (3) ultimately, practitioners disagree about whether or not to even remove
    such features in the first place if it comes at the expense of injecting selection
    bias or losing key model information [5–9].
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: I won’t go down that rabbit hole. It’s deep.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: But I’ll show how to calculate VIFs, and maybe we can make our own conclusions.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: First, the VIF really asks “How well is one of the features jointly explained
    by all my other features?” A feature’s VIF estimate will get “inflated” when there
    is linear dependence with other features.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: Given all the features in our dataset, let’s calculate the VIFs below.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '![](../Images/0a6fb0ad0da304321f4d0e4dee906ffb.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
- en: 'Note from this partial list, features VA, VB, VD all show VIFs towards infinity,
    which far exceeds the “rule of thumb” threshold of 5\. But we need to be careful
    about heuristics like this, as VIF thresholds have two caveats:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: The 5 cut-off is relative to other features — e.g., if the large majority of
    feature VIFs fall below 7 and a smaller subset of feature VIFs are above 7, then
    7 could be a more reasonable threshold.
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Categorical features with a smaller number of cases in the reference category
    relative to other levels will produce high VIFs even if those features are not
    correlated with other variables in the model [8].
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In our case, features VA, VB, VC are all highly colinear. Crosstabs confirm
    this, and a Pearson’s correlation matrix would too if they were continuous variables.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: The general consensus for solving this is to systematically drop one feature
    at a time, review all VIFs, note possible improvements, and continue until all
    VIFs fall below your selected threshold. Careful attention should be paid to losing
    any possible explanatory power from the features, relative to both the business
    context and target variable. Confirmatory statistical tests like chi-square and
    visualization can help aid in deciding which feature to drop between two possible
    choices.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 解决这个问题的一般共识是逐个系统地去掉一个特征，检查所有的VIF，注意可能的改进，并继续，直到所有的VIF低于您选择的阈值。需要特别注意，相对于业务背景和目标变量，不要失去任何可能的解释能力。确认性统计检验如卡方检验和可视化可以帮助确定在两个可能选择之间去掉哪个特征。
- en: 'Let’s drop VB and note the VIF changes:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们去掉VB，注意VIF的变化：
- en: '[PRE6]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '![](../Images/176578df50e36401a64478974feef6ec.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/176578df50e36401a64478974feef6ec.png)'
- en: VA and VC still have VIFs at infinity. Even worse, the model is still rendering
    NaN standard errors (not shown). Let’s drop VC.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: VA和VC的VIF仍然是无穷大。更糟糕的是，模型仍然呈现NaN的标准误差（未显示）。让我们去掉VC。
- en: '[PRE7]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '![](../Images/024cb3841d6d1360ca2142f0522399e2.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/024cb3841d6d1360ca2142f0522399e2.png)'
- en: Finally, the model produces standard errors, so even though the VIFs of feature
    VA are still > 5, it’s not a problem because of the second caveat above, the reference
    category having a small number of cases relative to the other levels.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，模型产生了标准误差，因此即使特征VA的VIF仍然> 5，这并不是问题，因为以上第二个注意事项，参考类别相对于其他水平的案例数量较少。
- en: '![](../Images/e150ba20ded7a4ad0e38e6426751b90f.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e150ba20ded7a4ad0e38e6426751b90f.png)'
- en: '***Extra Credit: Let’s say you absolutely know VA, VB and VC are critical to
    explaining the target variable and need them in the model. Given the additional
    features, assume the optimization is working over a complex space, and the “Inverting
    Hessian failed” may be circumvented by choosing new solvers and starting points
    (start_params in sklearn). Training a new model that does not assume linear boundaries
    would also be an option.***'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '***额外学分：假设您绝对知道VA、VB和VC对解释目标变量至关重要，并且需要它们出现在模型中。鉴于额外的特征，假设优化正在复杂空间中进行，可能通过选择新的求解器和起始点（sklearn中的start_params）绕过“反转海森矩阵失败”。训练一个不假设线性边界的新模型也是一个选择。***'
- en: 'Problem 4: “Possibly complete quasi-separation” error (and unreasonably large
    coefficients and/or standard errors)'
  id: totrans-96
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题4：“可能完全拟合”错误（以及不合理地大的系数和/或标准误差）
- en: 'We might get excited to see large coefficients and high accuracy scores. But
    often, these estimates are implausibly large, and are caused by another common
    issue: perfect separation.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可能会因为看到大的系数和高的准确性分数而兴奋。但通常，这些估计值是不可信的大，这是由另一个常见问题引起的：完美分离。
- en: Perfect (or near-perfect) separation occurs when one or more features are strongly
    associated with the target variable. In fact, they may be almost identical to
    it.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 完美（或接近完美）分离发生在一个或多个特征与目标变量强相关时。实际上，它们可能几乎与目标变量相同。
- en: I can manufacture this error by taking the target variable *target_satisfaction*
    and creating a new feature from it called *new_target_satisfaction* that is 95%
    similar to it.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 我可以通过将目标变量*target_satisfaction*创造一个新特征*new_target_satisfaction*，它与它相似度达到95%来制造这个错误。
- en: '[PRE8]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: And put *new_target_satisfaction* into the model.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 并将*new_target_satisfaction*放入模型中。
- en: '[PRE9]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Coefficients and standard errors render, but we get this quasi-separation warning:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 系数和标准误差渲染，但我们得到了这个准分离警告：
- en: '![](../Images/30ed46310d28bd23b07ab46054b3255a.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/30ed46310d28bd23b07ab46054b3255a.png)'
- en: The feature has a ridiculously high coefficient, and an odds ratio of about
    70,000,000, which is implausible.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 这个特征的系数非常高，约为70,000,000，这是不可信的。
- en: '![](../Images/f2ed5a4d314402957da555c7f2e7d3d0.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f2ed5a4d314402957da555c7f2e7d3d0.png)'
- en: Under the hood, this is happening because features that separate “too well”
    create an inflated slope, thus a large coefficient and standard error. Possibly,
    the LR model may also not converge [10].
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在幕后，这是因为分离“太好”的特征造成了夸张的斜率，从而导致大的系数和标准误差。可能，LR模型也无法收敛[10]。
- en: '![](../Images/e347c68f2585e834a20328b5da03bb4e.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e347c68f2585e834a20328b5da03bb4e.png)'
- en: Perfect Separation (Cornell Statistical Consulting Unit) [9]
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 完美分离（康奈尔大学统计咨询单位）[9]
- en: Those two red dots, the cases misclassified removed, would have actually prevented
    perfect separation, helping the LR model converge and the standard error estimates
    to be more realistic.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 那两个红色点，被错误分类的案例移除，实际上会防止完美分离，有助于LR模型收敛和标准误差估计更加现实。
- en: The thing to remember about perfect separation in *sklearn* is that it can output
    a model that looks like near-perfect accuracy, in our case 98%, but in reality
    has a feature *new_target_satisfaction* that is a near duplicate of the target
    *target_satisfaction*.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 在*sklearn*中，关于完美分离要记住的是，它可能会输出一个看似接近完美准确度的模型，在我们的例子中是98%，但实际上有一个特征*new_target_satisfaction*几乎是目标特征*target_satisfaction*的重复。
- en: '[PRE10]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '![](../Images/ddc8a4b7b3f30161f1444ff43462c28b.png)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ddc8a4b7b3f30161f1444ff43462c28b.png)'
- en: 'The most common solution would be to simply drop the feature. But there are
    a growing number of alternative solutions:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 最常见的解决方案是简单地删除该特征。但也有越来越多的替代解决方案：
- en: Apply Firth’s correction, which maximizes a “penalized” likelihood function.
    Currently, *statsmodels* does not have this functionality available but R does
    [11]
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 应用费斯特修正，最大化一个“惩罚”似然函数。目前，*statsmodels*没有此功能，但R有[11]。
- en: Penalized regression can also work, specifically by testing combinations of
    solvers, penalties and learning rates [2]. I kept *new_target_satisfaction* in
    the model and tried various combinations, but it made little difference in this
    particular case
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 惩罚回归也可以有效，特别是通过测试求解器、惩罚项和学习率的组合[2]。我将*new_target_satisfaction*保留在模型中并尝试了各种组合，但在这个特定的情况下几乎没有变化。
- en: Some practitioners will manually swap a fewrandomly selected observations in
    the problematic feature to make it less perfectly-separated with the target, like
    adding back those red circles in the picture above [8, 10]. Running a crosstab
    on that feature with the target can help you determine what percent of cases to
    swap. While doing this you might ask yourself *Why? Why am I refactoring one feature
    like this just so the LR model can accept it?* To help you sleep better, some
    research argues perfect separation is only symptomatic of the model, not our data
    [8, 11]
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一些从业者会手动交换一些随机选择的观测值，以使其与目标的分离不那么完美，比如将上图中的红圈重新添加回去[8, 10]。对该特征和目标运行交叉表可以帮助你确定需要交换的案例百分比。在此过程中，你可能会问自己*为什么？我为什么要这样重构一个特征，以便LR模型可以接受？*
    为了让你更安心，一些研究认为完美分离只是模型的症状，而非数据[8, 11]。
- en: Finally, some contrarian practitioners actually see nothing wrong with coefficients
    so high [8]. A very high odds-ratio simply suggests it is strongly suggestive
    of an association, and is predicting almost perfectly. Caveat the findings and
    leave it at that. The basis for this argument is high coefficients are an unfortunate
    consequence of the Wald test and likelihood ratio that require an evaluation of
    info with an alternative hypothesis
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，一些反对者实际上并不认为如此高的系数有任何问题[8]。非常高的比值比仅仅表明它强烈暗示了一个关联，并且预测几乎是完美的。警告这些发现，且就此为止。这个论点的基础是高系数是不幸的结果，沃尔德检验和似然比需要评估信息与替代假设。
- en: Conclusion
  id: totrans-119
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结论
- en: Logistic Regression is most definitely versatile and powerful if you can overcome
    the challenges from real-world data sets. I hope this overview provides a good
    foundation of possible solutions. What tip did you find most interesting? What
    are some others?
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你能克服来自现实世界数据集的挑战，逻辑回归绝对是多功能且强大的。我希望这个概述为可能的解决方案提供了一个良好的基础。你觉得哪个建议最有趣？还有其他建议吗？
- en: Thanks for reading. Share your thoughts in the Comments section, and let me
    know what other problems you’ve been encountering with Logistic Regression. I’m
    also happy to connect and talk shop on [LinkedIn](https://www.linkedin.com/in/gabe-verzino-71401137/).
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 感谢阅读。在评论区分享你的想法，并告诉我你在逻辑回归中遇到的其他问题。我也很乐意在[LinkedIn](https://www.linkedin.com/in/gabe-verzino-71401137/)上联系并讨论相关问题。
- en: 'Check out some of my other articles:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 查看我其他的一些文章：
- en: '[**Using Bayesian Networks to Forecast Ancillary Service Volume in Hospitals**](https://medium.com/towards-data-science/using-bayesian-networks-to-forecast-ancillary-service-volume-in-hospitals-48968a978cb5)'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '[**使用贝叶斯网络预测医院辅助服务量**](https://medium.com/towards-data-science/using-bayesian-networks-to-forecast-ancillary-service-volume-in-hospitals-48968a978cb5)'
- en: '[**Why Balancing Classes is Over-Hyped**](https://medium.com/towards-data-science/why-balancing-classes-is-over-hyped-e382a8a410f7)'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '[**为何类平衡被过度炒作**](https://medium.com/towards-data-science/why-balancing-classes-is-over-hyped-e382a8a410f7)'
- en: '[**Feature Engineering CPT Codes**](https://medium.com/mlearning-ai/working-with-cpt-codes-5a2b04a4d183)'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '[**特征工程 CPT 代码**](https://medium.com/mlearning-ai/working-with-cpt-codes-5a2b04a4d183)'
- en: Citations
  id: totrans-126
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 引用
- en: Allison, Paul. Convergence Failures in Logistic Regression. University of Pennsylvania,
    Philadelphia, PA. [https://www.people.vcu.edu/~dbandyop/BIOS625/Convergence_Logistic.pdf](https://www.people.vcu.edu/~dbandyop/BIOS625/Convergence_Logistic.pdf)
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Allison, Paul. 逻辑回归中的收敛失败。宾夕法尼亚大学，费城，PA。 [https://www.people.vcu.edu/~dbandyop/BIOS625/Convergence_Logistic.pdf](https://www.people.vcu.edu/~dbandyop/BIOS625/Convergence_Logistic.pdf)
- en: 'Geron, Aurelien. Hands-On Machine Learning with Scikit-Learn, Keras & Tensorflow.
    Second Edition. Published by: O’Reilly, 2019.'
  id: totrans-128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Geron, Aurelien. 《动手学机器学习：使用 Scikit-Learn、Keras 和 Tensorflow》。第二版。出版单位：O’Reilly，2019年。
- en: Sckit-learn documentation, sklearn.linear_model.LogisticRegression. [https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Sckit-learn 文档，sklearn.linear_model.LogisticRegression。 [https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)
- en: 'Google Groups discussion. “MLE error: Warning: Inverting hessian failed: Maybe
    I cant use ‘matrix’ containers?” 2014\. [https://groups.google.com/g/pystatsmodels/c/aELcgNpg5f8](https://groups.google.com/g/pystatsmodels/c/aELcgNpg5f8)'
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Google Groups 讨论。“MLE 错误：警告：求逆海森矩阵失败：也许我不能使用‘matrix’容器？” 2014年。 [https://groups.google.com/g/pystatsmodels/c/aELcgNpg5f8](https://groups.google.com/g/pystatsmodels/c/aELcgNpg5f8)
- en: 'Gill, Jeff & King, Gary. What to Do When Your Hessian Is Not Invertible. Alterantives
    to Model Respecification in Nonlinear Estimation. SOCIOLOGICAL METHODS & RESEARCH,
    Vol. 33, №1, August 2004 54–87 DOI: 10.1177/0049124103262681\. [https://gking.harvard.edu/files/help.pdf](https://gking.harvard.edu/files/help.pdf)'
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Gill, Jeff & King, Gary. 《当你的海森矩阵不可逆时该怎么办：非线性估计中的模型重规格化替代方案》。社会学方法与研究，第33卷，第1期，2004年8月，第54-87页
    DOI: 10.1177/0049124103262681。 [https://gking.harvard.edu/files/help.pdf](https://gking.harvard.edu/files/help.pdf)'
- en: '“Variable Selection for a binary classification problem.” StackExchange, CrossValidated.
    Online at: [https://stats.stackexchange.com/questions/64271/variable-selection-for-a-binary-classification-problem/64637#64637](https://stats.stackexchange.com/questions/64271/variable-selection-for-a-binary-classification-problem/64637#64637)'
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '“针对二分类问题的变量选择。” StackExchange, CrossValidated。在线阅读: [https://stats.stackexchange.com/questions/64271/variable-selection-for-a-binary-classification-problem/64637#64637](https://stats.stackexchange.com/questions/64271/variable-selection-for-a-binary-classification-problem/64637#64637)'
- en: '“In supervised learning, why is it bad to have correlated features.” StackExchange,
    Data Science. Online at: [https://datascience.stackexchange.com/questions/24452/in-supervised-learning-why-is-it-bad-to-have-correlated-features](https://datascience.stackexchange.com/questions/24452/in-supervised-learning-why-is-it-bad-to-have-correlated-features)'
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '“在监督学习中，为什么特征相关性会带来不利影响。” StackExchange, 数据科学。在线阅读: [https://datascience.stackexchange.com/questions/24452/in-supervised-learning-why-is-it-bad-to-have-correlated-features](https://datascience.stackexchange.com/questions/24452/in-supervised-learning-why-is-it-bad-to-have-correlated-features)'
- en: '“How to deal with perfect separation in logistic regression”. StackExchange,
    CrossValidated. Online at: [https://stats.stackexchange.com/questions/11109/how-to-deal-with-perfect-separation-in-logistic-regression](https://stats.stackexchange.com/questions/11109/how-to-deal-with-perfect-separation-in-logistic-regression)'
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '“如何处理逻辑回归中的完美分离。” StackExchange, CrossValidated。在线阅读: [https://stats.stackexchange.com/questions/11109/how-to-deal-with-perfect-separation-in-logistic-regression](https://stats.stackexchange.com/questions/11109/how-to-deal-with-perfect-separation-in-logistic-regression)'
- en: 'Allison, Paul. “When Can You Safely Ignore Multicollinearity”. Statistical
    Horizons, September 10, 2012\. Online at: [https://statisticalhorizons.com/multicollinearity/](https://statisticalhorizons.com/multicollinearity/)'
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Allison, Paul. “何时可以安全地忽略多重共线性”。《统计前沿》，2012年9月10日。在线阅读: [https://statisticalhorizons.com/multicollinearity/](https://statisticalhorizons.com/multicollinearity/)'
- en: 'Cornell Statistical Consulting Unit. Separation and Convergence Issues in Logistic
    Regression. Statnews #82\. Published: February, 2012\. Online at: [https://cscu.cornell.edu/wp-content/uploads/82_lgsbias.pdf](https://cscu.cornell.edu/wp-content/uploads/82_lgsbias.pdf)'
  id: totrans-136
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '康奈尔统计咨询单位。逻辑回归中的分离与收敛问题。Statnews #82。出版：2012年2月。在线阅读: [https://cscu.cornell.edu/wp-content/uploads/82_lgsbias.pdf](https://cscu.cornell.edu/wp-content/uploads/82_lgsbias.pdf)'
- en: 'Logistf: Firth’s Bias-Reduced Logistic Regression. R documentation. Published
    August 18, 2023\. Online at: [https://cran.r-project.org/web/packages/logistf/index.html](https://cran.r-project.org/web/packages/logistf/index.html)'
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Logistf: Firth 的偏差减少逻辑回归。R 文档。出版日期：2023年8月18日。在线阅读: [https://cran.r-project.org/web/packages/logistf/index.html](https://cran.r-project.org/web/packages/logistf/index.html)'
- en: '*Note: All images are by the author, unless otherwise stated.*'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '*注意：所有图片均由作者提供，除非另有说明。*'
