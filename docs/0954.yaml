- en: Running a Stable Diffusion Cluster on GCP with tensorflow-serving (Part 2)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/running-a-stable-diffusion-cluster-on-gcp-with-tensorflow-serving-part-2-c421ecb7472a?source=collection_archive---------9-----------------------#2023-03-14](https://towardsdatascience.com/running-a-stable-diffusion-cluster-on-gcp-with-tensorflow-serving-part-2-c421ecb7472a?source=collection_archive---------9-----------------------#2023-03-14)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Creating the artifacts and deploying the model on the cluster
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://thushv89.medium.com/?source=post_page-----c421ecb7472a--------------------------------)[![Thushan
    Ganegedara](../Images/3fabfa37132f7d3a9e7679c3b8d7e061.png)](https://thushv89.medium.com/?source=post_page-----c421ecb7472a--------------------------------)[](https://towardsdatascience.com/?source=post_page-----c421ecb7472a--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----c421ecb7472a--------------------------------)
    [Thushan Ganegedara](https://thushv89.medium.com/?source=post_page-----c421ecb7472a--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F6f0b045d5681&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frunning-a-stable-diffusion-cluster-on-gcp-with-tensorflow-serving-part-2-c421ecb7472a&user=Thushan+Ganegedara&userId=6f0b045d5681&source=post_page-6f0b045d5681----c421ecb7472a---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----c421ecb7472a--------------------------------)
    ·14 min read·Mar 14, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fc421ecb7472a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frunning-a-stable-diffusion-cluster-on-gcp-with-tensorflow-serving-part-2-c421ecb7472a&user=Thushan+Ganegedara&userId=6f0b045d5681&source=-----c421ecb7472a---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc421ecb7472a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frunning-a-stable-diffusion-cluster-on-gcp-with-tensorflow-serving-part-2-c421ecb7472a&source=-----c421ecb7472a---------------------bookmark_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: In [part 1](/running-a-stable-diffusion-cluster-on-gcp-with-tensorflow-serving-part-1-4f7a8e2f66df),
    we learned how to use `terraform` to set up and manage our infrastructure conveniently.
    In this part, we will continue on our journey to deploy a running Stable Diffusion
    model on the provisioned cluster.
  prefs: []
  type: TYPE_NORMAL
- en: '**Note**: You can follow this tutorial end-to-end even if you’re a free user
    (as long as you have some of free tier credits left).'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: All images, unless otherwise noted, are by the author
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Github: [https://github.com/thushv89/tf-serving-gke](https://github.com/thushv89/tf-serving-gke)'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s take a look at what the final result would be.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4f57aa3e47f1793b54600176ea1a7c53.png)'
  prefs: []
  type: TYPE_IMG
- en: Some images generated by the deployed Stable Diffusion model
  prefs: []
  type: TYPE_NORMAL
- en: Preparing the model artifacts
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: What is Stable Diffusion anyway?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There are five main components that builds up the [Stable Diffusion model](https://jalammar.github.io/illustrated-stable-diffusion/):'
  prefs: []
  type: TYPE_NORMAL
- en: Tokenizer — Tokenizes a given string to a list of tokens (numerical IDs)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Text encoder — Takes in the tokenized text and produces a text embedding
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Diffusion model — Takes in the text embedding and a latent image (initially
    noise) as an input and incrementally refine the latent image to encoder more and
    more useful information (visually pleasing)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Decoder — Takes in the final latent image and produces an actual image
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Image encoder (used for the in-painting feature — we’ll be ignoring this for
    this exercise)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The principal ground-shattering idea behind stable diffusion (diffusion models)
    is,
  prefs: []
  type: TYPE_NORMAL
- en: If you add a bit of noise to an image gradually for many steps, you will end
    up with an image containing noise. By reversing the order of the process, you
    can have an input (noise) and a target (original image). Then a model is trained
    to predict the original image from noise.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: All of the above components work cohesively to achieve this idea.
  prefs: []
  type: TYPE_NORMAL
- en: Storing the Stable Diffusion model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Code: [https://github.com/thushv89/tf-serving-gke/blob/master/notebooks/savedmodel_stable_diffusion.ipynb](https://github.com/thushv89/tf-serving-gke/blob/master/notebooks/savedmodel_stable_diffusion.ipynb)'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: In order to construct a [Stable Diffusion model](https://jalammar.github.io/illustrated-stable-diffusion/),
    we’ll be using the `keras_cv` library, which includes a collection of popular
    deep learning vision models for image classification, segmentation, generative
    AI, etc. You can find a tutorial [here](https://keras.io/guides/keras_cv/generate_images_with_stable_diffusion/),
    which explains how to use the `StableDiffusion` in `keras_cv`. You can open up
    a notebook and play with the model to familiarize yourself.
  prefs: []
  type: TYPE_NORMAL
- en: Our goal here is to save the `StableDiffusion`model in the `SavedModel` format;
    the go-to standard for serializing TensorFlow models. One crucial requirement
    to do this is making sure all operations used are TensorFlow graph compatible.
    Unfortunately, this is not the case.
  prefs: []
  type: TYPE_NORMAL
- en: The current version of the model uses a TensorFlow graph incompatible tokenizer,
    so it needs to be brought out of the packaged model and used in a separate step.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The current version uses `predict_on_batch` in order to generate an image, which
    is not supported by TensorFlow graph building.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fixing the model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In order to patch up the eager mode `StableDiffusion` model, we’ll create a
    new model called `StableDiffusionNoTokenizer`. Through out this new model, we’ll
    replace all `predict_on_batch()` calls with graph compatible `__call__()` calls.
    We’ll also be decoupling the tokenization process from the model as the name suggests.
    Additionally, in the `generate_image()` function, we’ll be replacing,
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: with,
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: where,
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Two main changes I’ve done are:'
  prefs: []
  type: TYPE_NORMAL
- en: Instead of a Python `for` loop, I’ve used the `tf.while_loop` which is more
    performant in TensorFlow.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Combined the two separate calls to the `diffusion_model` to a single call and
    later split the outputs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are other changes such as replacing various operations with TensorFlow
    equivalent (e.g. `np.clip()` -> `tf.clip_by_value()`), you can compare and contrast
    the [original model](https://github.com/keras-team/keras-cv/blob/master/keras_cv/models/stable_diffusion/stable_diffusion.py#L43),
    with [this version](https://github.com/thushv89/tf-serving-gke/blob/master/notebooks/savedmodel_stable_diffusion.ipynb)
    to compare and contrast.
  prefs: []
  type: TYPE_NORMAL
- en: When working with TensorFlow in the graph execution mode, you can use `tf.print()`
    statements in order to ensure the validity of the code during execution. Please
    refer to the Appendix for more information about `tf.print()`.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Once the underlying model is fixed, we can create the following model, which
    can be executed in graph mode without a hiccup.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'This model takes in the following inputs:'
  prefs: []
  type: TYPE_NORMAL
- en: '`input_tokens`: Tokenized representation of the input string'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`negative_prompt_tokens`: Tokenized representation of the negative prompt (more
    about negative prompting: here)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_steps`: Number of steps to run the diffusion process for'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`batch_size`: Number of images to generate per image'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here’s an example usage of this model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Remember that, I’m (i.e. Free user tier) heavily restricted by quotas on this
    project.
  prefs: []
  type: TYPE_NORMAL
- en: No GPU quota at all
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Max 8 N2 CPUs (If you choose N1 CPUs, you can go up to 12)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Therefore, I cannot use any GPU instances or more than 2 `n2-standard-4`instances.
    Stable Diffusion models are quite slow therefore we’ll be challenged by latency
    using CPU instances.
  prefs: []
  type: TYPE_NORMAL
- en: Here are some details about how long it takes under different parameters. The
    tests were don on a `n2-standard-8` machine on [Vertex AI workbench](https://cloud.google.com/vertex-ai-workbench).
  prefs: []
  type: TYPE_NORMAL
- en: Image size (`num_steps = 40`)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '— 512x512 image: 474s'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '— 384x384 image: 233s'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`batch_size` and `num_steps`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '— `batch size = 1`: 21.6s (`num_steps=5`), 67.7s (`num_steps=20`) and 99.5s
    (`num_steps=30`)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: — `batch size = 2`, 55.6s (`num_steps=5`), 121.1s (`num_steps=20`) and 180.2s
    (`num_steps=30`)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: — `batch size=4`, 21.6s (`num_steps=5`), 67.7s (`num_steps=20`) and 99.5s (`num_steps=30`)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: As you can see, increasing the `image_size` , `batch_size` , `num_steps` lead
    to increased time consumption. Therefore balancing the computational cost with
    the image quality, we chose the following parameters for our deployed model.
  prefs: []
  type: TYPE_NORMAL
- en: '`image_size`: `384x384`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_steps`: `30`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`batch_size`: `1`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Once the model is created, upload the model to the created GCS bucket.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: This will be the data source we’ll be using in order to deploy our model as
    a prediction service.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s again take a moment to appreciate some images generated by the model,
    before continuing on to the next section.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f665634a061194fc8674cc778a31ab4e.png)![](../Images/ce241c77d0e034e567fa853ae515e930.png)'
  prefs: []
  type: TYPE_IMG
- en: Images generated by the deployed model
  prefs: []
  type: TYPE_NORMAL
- en: Deploying and serving up the model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Code: [https://github.com/thushv89/tf-serving-gke/tree/master/infrastrcture](https://github.com/thushv89/tf-serving-gke/tree/master/infrastrcture)'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'To deploy our model and setup a prediction service, we need 3 configurations:'
  prefs: []
  type: TYPE_NORMAL
- en: '`configmap.yaml` — This defines various variables that are required during
    the deployment. For example this would encompass the location of the SavedModel
    on GCS (i.e. accessible through the environment variable `MODEL_PATH`)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`deployment.yaml` — [Deployment](https://kubernetes.io/docs/concepts/workloads/controllers/deployment/)
    defines the pod specifications (e.g. CPU) and containers it should be running.
    In this case, we’ll be running a single container running `tensorflow-serving`
    serving the model located at `MODEL_PATH` .'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`service.yaml` — [Service](https://kubernetes.io/docs/concepts/services-networking/service/)
    is the mechanism with which we expose our `tensorflow-serving` app running in
    our pods. For example we can tell it to expose our pod(s) through a load balancer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The deployment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s first look at the spec of the `deployment` :'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'We can make few interesting observations:'
  prefs: []
  type: TYPE_NORMAL
- en: We’re only declaring a single replica in the script, scaling will be setup separately
    in and will be controlled through an autoscaling policy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We provide a `selector`, which the service will look for in a deployment to
    ensure it’s serving on the correct deployment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We expose two ports; 8501 (HTTP traffic) and 8500 (GRPC traffic)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We’ll be requesting 3 “CPU time” and 12Gi per container.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Note 1**:A node will be typically running other pods necessitated by Kubernetes
    (i.e. DNS, monitoring, etc.). Therefore such factors need to be taken into account
    when stipulating compute resources for the pod. You can see that, though we have
    a 4 CPUs in a node, we only request 3 (you could request fractional CPU resources
    as well — e.g. 3.5). You can see the allocatable CPU/Memory of each node in GCP
    (GCP Console → Clusters → Nodes → Click on a node) or using `kubectl describe
    nodes`.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: If your node is unable to fulfill the compute resources you specify Kubernetes
    will not be able to run the pods and throw an error (e.g. PodUnschedulable).
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**Note 2**: One of the most crucial arguments you need to be careful about
    is `--rest_api_timeout_in_ms=720000` . It takes about 250s to serve up a single
    request, so here, we’re setting roughly **three** times that time as the timeout,
    to account for any enqueued requests, when we send parallel requests. If you set
    this to a value that’s too small, your requests will timeout before they are complete.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Defining the service
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Here we’re defining a `LoadBalancer` type service, where we will expose the
    `stable-diffusion` app through the GCP load balancer. In this approach, you will
    be provided with the load balancer’s IP address, where the load balancer will
    route the traffic to the pods coming on to it. Users will be making requests against
    the IP address of the load balancer.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Autoscaling
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There’s an imperative topic that we’ve been putting off; scaling up our service.
    In real world, you may need to serve thousands, millions or even billions of customers.
    In order to do so, your service needs to be able to scale up/down the number of
    nodes/pods in the cluster, based on the demand. Fortunately GCP provides a variety
    of options, from fully managed autoscaling to semi/fully user managed autoscaling.
    You can learn more about these [in this video](https://www.youtube.com/watch?v=cFhch7hozRg).
  prefs: []
  type: TYPE_NORMAL
- en: Here we’ll be using a horizontal pod autoscaler (HPA). The horizontal pod autoscaler
    will scale up the number of pods, based on some threshold you provide (e.g. CPU
    or memory usage). Here’s an example.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Here we’re giving the HPA a minimum of 1, maximum of 2 pods, and asking it to
    add more pods if the average CPU across current set of pods go above 60%.
  prefs: []
  type: TYPE_NORMAL
- en: Applying the changes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We now got all the building blocks ready to start our service. Simply run the
    following commands.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Predicting from the served model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In order to predict you simply need to make a POST request to the correct URL,
    with a payload containing the input to the model.
  prefs: []
  type: TYPE_NORMAL
- en: Sequential predictions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As the first example, we show how you can make a series of requests one after
    the other.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: This took over 1600s when I ran the experiment. As you might imagine, this setup
    is quite inefficient, and is unable to leverage the cluster’s ability to scale
    up.
  prefs: []
  type: TYPE_NORMAL
- en: Parallel predictions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You can use Python’s multiprocessing library to make parallel requests, which
    is more remnant of real-world user requests.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: This ran in 900s. Therefore, we have a ~180% speed up, by scaling up the cluster
    to a maximum of 2 pods.
  prefs: []
  type: TYPE_NORMAL
- en: '**Note about setting timeouts**'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Be careful when setting up the parallel requests. If you send the parallel requests
    all at once (since this is only 6 requests), **they will likely timeout**. This
    is because it takes time to create a new node and initialize a new pod. So if
    all requests are made instantly, the load balancer might not even have time to
    see the 2nd node and end up trying to serve all requests to the single node.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Your timeout defined above is counted from the time the request is received
    (i.e. enter the `*tensorflow-serving*` queue), not from the time it starts serving
    the request. So if the request waits too long in the queue that also counts for
    the timeout.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: You can monitor the compute metrics such as the CPU usage and memory consumption
    on GCP (GCP → Kubernetes Engine → Services & Ingress → Select your service).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a93688cd809d2f3f3fb3ee6a342fc3c2.png)'
  prefs: []
  type: TYPE_IMG
- en: Usage graph for sequential requests (top) Usage for parallel requests (bottom)
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this 2 part tutorial, we,
  prefs: []
  type: TYPE_NORMAL
- en: Setup the infrastructure using terraform (an IaaS tool), which mainly consisted
    of a cluster and a node-pool (Part 1)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deployed a model and created a prediction service to serve user requests using
    a Stable Diffusion model (Part 2)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We setup this tutorial in a way that it can be run by even a free tier user.
    We setup a cluster with 2 nodes and created 1 pod per node. Then we made both
    sequential and parallel predictions and saw that parallel predictions lead to
    ~180% gains in higher throughput.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Model warmup](https://www.tensorflow.org/tfx/serving/serving_config) — `tensorflow-serving`
    offers an easy way to warm up a model. You can parse example requests and they
    will be loaded and sent to the model, prior to serving actual user requests. This
    will reduce the latency for the initial user requests.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Dynamic batching](https://www.tensorflow.org/tfx/serving/serving_config#batching_configuration)
    of requests — You can choose to dynamically batch the incoming requests. This
    will allow the model to make predictions on a batch of inputs than predicting
    on each input. Given enough memory, this will likely to provide throughput gains,
    allowing you to serve lot of requests within reasonable time bounds.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Appendix
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Debugging within pods
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When I’m trying to get this up and running, a painstaking issue I faced was
    running into the following wall of brick.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4e40e615e61954e8f711aaa3d0525c6d.png)'
  prefs: []
  type: TYPE_IMG
- en: The errors that were shown in the Workloads → Deployment section
  prefs: []
  type: TYPE_NORMAL
- en: and when I go into one of the pods in the deployment, I get a more sensible
    (still inconspicuous) error. But it was still inadequate to put a finger on what
    exactly was wrong.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d6c31012d4b7079d9bf28cfcbf84b2c5.png)'
  prefs: []
  type: TYPE_IMG
- en: Events produced by a single pod
  prefs: []
  type: TYPE_NORMAL
- en: So I had to find a way to microscopically investigate the root cause. For that
    I first logged into the pod’s container in question,
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Once I’m in, all I got to do is capitalize on the “[everything is a file](https://en.wikipedia.org/wiki/Everything_is_a_file)”
    paradigm Linux thrives on. In other words, you can simply tap into a file to see
    the output/error stream of a process. For example, in my case, `tensorflow-serving`
    process had the PID 7, therefore, `/proc/7/fd/2` gives the error stream of that
    process.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: In here, I was able to see exactly why this wasn’t kick-starting. It was because
    the container didn’t have the necessary permission to access the GCS bucket specified
    in `MODEL_PATH` .
  prefs: []
  type: TYPE_NORMAL
- en: Using `tf.print` for debugging
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As you know, TensorFlow offers two styles of execution; imperative and declarative.
    Since we use the `__call__()` to call the model (i.e. `self.model(<inputs>)`,
    these calls are executed as graph operations. You may know already that graph
    execution is notoriously difficult to debug, due to obscurities caused by the
    internal graph. One solution TensorFlow offers is the usage of `tf.print` statements.
  prefs: []
  type: TYPE_NORMAL
- en: You can place `tf.print` statements in your model calls, and those print statements
    are added as operations to the graph, so you can see values of executed tensors,
    etc. allowing you to debug the code than throwing darts and hoping for the best.
  prefs: []
  type: TYPE_NORMAL
- en: Make sure your `tf.print` statement is printing an input that appears immediately
    before the time you want it to be printed. If you add independent/dummy `tf.print`
    statements, they will not get embedded in the graph in the correct position. This
    may give you the deceptive feeling that some computation is happening very quickly,
    due to the incorrect placement of the graph.
  prefs: []
  type: TYPE_NORMAL
- en: Note about machine types
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There are two main [types of machines](https://cloud.google.com/compute/docs/machine-resource#recommendations_for_machine_types)
    you can use for this exercise; `n1` and `n2`. [N2 instances use 3rd generation
    Xeon processors](https://cloud.google.com/blog/products/compute/compute-engine-n2-vms-now-available-with-intel-ice-lake)
    that are equipped special instruction sets (`AVX-512`) to speed up operations
    such as matrix multiplication. Therefore, CPU bound TensorFlow code runs faster
    on n2 machines than on n1.
  prefs: []
  type: TYPE_NORMAL
- en: Acknowledgement
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: I’d like to acknowledge the [ML Developer Programs](https://developers.google.com/community/experts)
    and the team for the GCP credits provided to make this tutorial a success.
  prefs: []
  type: TYPE_NORMAL
