# æ„å»ºä½ çš„ç¬¬ä¸€ä¸ª LLM åº”ç”¨æ‰€éœ€çŸ¥é“çš„ä¸€åˆ‡

> åŸæ–‡ï¼š[`towardsdatascience.com/all-you-need-to-know-to-build-your-first-llm-app-eb982c78ffac`](https://towardsdatascience.com/all-you-need-to-know-to-build-your-first-llm-app-eb982c78ffac)

## ä¸€æ­¥ä¸€æ­¥çš„æ•™ç¨‹ï¼Œæ¶µç›–æ–‡æ¡£åŠ è½½å™¨ã€åµŒå…¥ã€å‘é‡å­˜å‚¨å’Œæç¤ºæ¨¡æ¿

[](https://dmnkplzr.medium.com/?source=post_page-----eb982c78ffac--------------------------------)![Dominik Polzer](https://dmnkplzr.medium.com/?source=post_page-----eb982c78ffac--------------------------------)[](https://towardsdatascience.com/?source=post_page-----eb982c78ffac--------------------------------)![Towards Data Science](https://towardsdatascience.com/?source=post_page-----eb982c78ffac--------------------------------) [Dominik Polzer](https://dmnkplzr.medium.com/?source=post_page-----eb982c78ffac--------------------------------)

Â·å‘è¡¨äº [Towards Data Science](https://towardsdatascience.com/?source=post_page-----eb982c78ffac--------------------------------) Â·é˜…è¯»æ—¶é•¿ 26 åˆ†é’ŸÂ·2023 å¹´ 6 æœˆ 22 æ—¥

--

![](img/215dfd8fde2517ee28b96285604db80a.png)

ä½¿ç”¨ä¸Šä¸‹æ–‡æ³¨å…¥æ„å»ºè‡ªå·±çš„èŠå¤©æœºå™¨äºº â€” ä½œè€…å›¾åƒ

## ç›®å½•

> å¦‚æœä½ åªæ˜¯æƒ³æ‰¾ä¸€ä¸ªç®€çŸ­çš„æ•™ç¨‹ï¼Œè¯´æ˜å¦‚ä½•æ„å»ºä¸€ä¸ªç®€å•çš„ LLM åº”ç”¨ï¼Œä½ å¯ä»¥è·³åˆ°ç¬¬ â€œ6\. åˆ›å»ºå‘é‡å­˜å‚¨â€ éƒ¨åˆ†ï¼Œåœ¨é‚£é‡Œä½ å¯ä»¥æ‰¾åˆ°æ„å»ºæœ€å°åŒ– LLM åº”ç”¨æ‰€éœ€çš„æ‰€æœ‰ä»£ç ç‰‡æ®µï¼ŒåŒ…æ‹¬å‘é‡å­˜å‚¨ã€æç¤ºæ¨¡æ¿å’Œ LLM è°ƒç”¨ã€‚

**ç®€ä»‹**

ä¸ºä»€ä¹ˆæˆ‘ä»¬éœ€è¦ LLM

å¾®è°ƒ vs. ä¸Šä¸‹æ–‡æ³¨å…¥

ä»€ä¹ˆæ˜¯ LangChainï¼Ÿ

**é€æ­¥æ•™ç¨‹**

1\. ä½¿ç”¨ LangChain åŠ è½½æ–‡æ¡£

2\. å°†æ–‡æ¡£æ‹†åˆ†æˆæ–‡æœ¬å—

3\. ä»æ–‡æœ¬å—åˆ°åµŒå…¥

4\. å®šä¹‰ä½ æƒ³ä½¿ç”¨çš„ LLM

5\. å®šä¹‰æˆ‘ä»¬çš„æç¤ºæ¨¡æ¿

6\. åˆ›å»ºä¸€ä¸ªå‘é‡å­˜å‚¨

![](img/abb5a66e45437849d1939f663604994d.png)

ç›®å½•

# ä¸ºä»€ä¹ˆæˆ‘ä»¬éœ€è¦ LLM

è¯­è¨€çš„å‘å±•ä½¿æˆ‘ä»¬äººç±»èµ°å¾—éå¸¸è¿œã€‚å®ƒä½¿æˆ‘ä»¬èƒ½å¤Ÿé«˜æ•ˆåœ°åˆ†äº«çŸ¥è¯†å¹¶ä»¥æˆ‘ä»¬ä»Šå¤©æ‰€çŸ¥é“çš„å½¢å¼è¿›è¡Œåˆä½œã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¤§éƒ¨åˆ†çš„é›†ä½“çŸ¥è¯†ä»ç„¶é€šè¿‡æ— ç»„ç»‡çš„ä¹¦é¢æ–‡æœ¬ä¿å­˜å’Œä¼ é€’ã€‚

åœ¨è¿‡å»äºŒåå¹´é‡Œï¼Œæ•°å­—åŒ–ä¿¡æ¯å’Œè¿‡ç¨‹çš„ä¸¾æªé€šå¸¸ä¸“æ³¨äºåœ¨å…³ç³»æ•°æ®åº“ä¸­ç§¯ç´¯è¶Šæ¥è¶Šå¤šçš„æ•°æ®ã€‚è¿™ç§æ–¹æ³•ä½¿ä¼ ç»Ÿçš„åˆ†ææœºå™¨å­¦ä¹ ç®—æ³•èƒ½å¤Ÿå¤„ç†å’Œç†è§£æˆ‘ä»¬çš„æ•°æ®ã€‚

å°½ç®¡æˆ‘ä»¬å¹¿æ³›åŠªåŠ›ä»¥ç»“æ„åŒ–çš„æ–¹å¼å­˜å‚¨è¶Šæ¥è¶Šå¤šçš„æ•°æ®ï¼Œä½†ä»ç„¶æ— æ³•æ•è·å’Œå¤„ç†æˆ‘ä»¬å…¨éƒ¨çš„çŸ¥è¯†ã€‚

> **å¤§çº¦ 80% çš„å…¬å¸æ•°æ®æ˜¯éç»“æ„åŒ–çš„ï¼Œä¾‹å¦‚å·¥ä½œæè¿°ã€ç®€å†ã€ç”µå­é‚®ä»¶ã€æ–‡æœ¬æ–‡ä»¶ã€PowerPoint å¹»ç¯ç‰‡ã€è¯­éŸ³å½•éŸ³ã€è§†é¢‘å’Œç¤¾äº¤åª’ä½“**

![](img/161315a8f6daffba557fdcc5d9fb0d5a.png)

ä¼ä¸šä¸­çš„æ•°æ®åˆ†å¸ƒ â€” ä½œè€…æä¾›çš„å›¾ç‰‡

GPT3.5 çš„å¼€å‘å’Œè¿›æ­¥æ ‡å¿—ç€ä¸€ä¸ªé‡è¦çš„é‡Œç¨‹ç¢‘ï¼Œå› ä¸ºå®ƒä½¿æˆ‘ä»¬èƒ½å¤Ÿæœ‰æ•ˆåœ°è§£é‡Šå’Œåˆ†æå„ç§æ•°æ®é›†ï¼Œæ— è®ºå…¶ç»“æ„å¦‚ä½•ã€‚å¦‚ä»Šï¼Œæˆ‘ä»¬æ‹¥æœ‰èƒ½å¤Ÿç†è§£å’Œç”Ÿæˆå¤šç§å†…å®¹å½¢å¼çš„æ¨¡å‹ï¼ŒåŒ…æ‹¬æ–‡æœ¬ã€å›¾åƒå’ŒéŸ³é¢‘æ–‡ä»¶ã€‚

> **é‚£ä¹ˆæˆ‘ä»¬å¦‚ä½•åˆ©ç”¨å®ƒä»¬çš„èƒ½åŠ›æ¥æ»¡è¶³æˆ‘ä»¬çš„éœ€æ±‚å’Œæ•°æ®å‘¢ï¼Ÿ**

# å¾®è°ƒä¸ä¸Šä¸‹æ–‡æ³¨å…¥

ä¸€èˆ¬æ¥è¯´ï¼Œæˆ‘ä»¬æœ‰ä¸¤ç§åŸºæœ¬çš„æ–¹æ³•æ¥ä½¿å¤§å‹è¯­è¨€æ¨¡å‹å›ç­” LLM æ— æ³•çŸ¥é“çš„é—®é¢˜ï¼š**æ¨¡å‹å¾®è°ƒ**å’Œ**ä¸Šä¸‹æ–‡æ³¨å…¥**

## **å¾®è°ƒ**

å¾®è°ƒæŒ‡çš„æ˜¯ä½¿ç”¨é¢å¤–çš„æ•°æ®å¯¹ç°æœ‰çš„è¯­è¨€æ¨¡å‹è¿›è¡Œè®­ç»ƒï¼Œä»¥ä½¿å…¶ä¼˜åŒ–ç‰¹å®šä»»åŠ¡ã€‚

ä¸åŒäºä»é›¶å¼€å§‹è®­ç»ƒè¯­è¨€æ¨¡å‹ï¼Œä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹å¦‚ BERT æˆ– LLamaï¼Œå¹¶é€šè¿‡æ·»åŠ ç‰¹å®šä»»åŠ¡çš„è®­ç»ƒæ•°æ®æ¥é€‚åº”ç‰¹å®šä»»åŠ¡çš„éœ€æ±‚ã€‚

æ–¯å¦ç¦å¤§å­¦çš„ä¸€ä¸ªå›¢é˜Ÿä½¿ç”¨äº† LLM Llamaï¼Œå¹¶é€šè¿‡ä½¿ç”¨ 50,000 ä¸ªç”¨æˆ·/æ¨¡å‹äº¤äº’çš„ç¤ºä¾‹å¯¹å…¶è¿›è¡Œäº†å¾®è°ƒã€‚ç»“æœæ˜¯ä¸€ä¸ªä¸ç”¨æˆ·äº’åŠ¨å¹¶å›ç­”æŸ¥è¯¢çš„èŠå¤©æœºå™¨äººã€‚è¿™ä¸€æ­¥å¾®è°ƒæ”¹å˜äº†æ¨¡å‹ä¸æœ€ç»ˆç”¨æˆ·çš„äº¤äº’æ–¹å¼ã€‚

**â†’ å…³äºå¾®è°ƒçš„è¯¯è§£**

PLLMsï¼ˆé¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ï¼‰çš„å¾®è°ƒæ˜¯ä¸€ç§è°ƒæ•´æ¨¡å‹ä»¥é€‚åº”ç‰¹å®šä»»åŠ¡çš„æ–¹æ³•ï¼Œä½†å®ƒå¹¶ä¸èƒ½çœŸæ­£å°†æ‚¨çš„é¢†åŸŸçŸ¥è¯†æ³¨å…¥æ¨¡å‹ã€‚è¿™æ˜¯å› ä¸ºæ¨¡å‹å·²ç»åœ¨å¤§é‡çš„é€šç”¨è¯­è¨€æ•°æ®ä¸Šè¿›è¡Œè¿‡è®­ç»ƒï¼Œè€Œæ‚¨çš„ç‰¹å®šé¢†åŸŸæ•°æ®é€šå¸¸ä¸è¶³ä»¥è¦†ç›–æ¨¡å‹å·²ç»å­¦åˆ°çš„å†…å®¹ã€‚

å› æ­¤ï¼Œå½“ä½ å¾®è°ƒæ¨¡å‹æ—¶ï¼Œå®ƒå¯èƒ½å¶å°”ä¼šæä¾›æ­£ç¡®çš„ç­”æ¡ˆï¼Œä½†å®ƒé€šå¸¸ä¼šå¤±è´¥ï¼Œå› ä¸ºå®ƒåœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šä¾èµ–äºåœ¨é¢„è®­ç»ƒæœŸé—´å­¦åˆ°çš„ä¿¡æ¯ï¼Œè¿™äº›ä¿¡æ¯å¯èƒ½ä¸å‡†ç¡®æˆ–ä¸æ‚¨çš„ç‰¹å®šä»»åŠ¡æ— å…³ã€‚æ¢å¥è¯è¯´ï¼Œå¾®è°ƒå¸®åŠ©æ¨¡å‹é€‚åº”å®ƒçš„äº¤æµæ–¹å¼ï¼Œä½†ä¸ä¸€å®šæ˜¯å®ƒäº¤æµçš„å†…å®¹ã€‚ï¼ˆä¿æ—¶æ·è‚¡ä»½å…¬å¸ï¼Œ2023ï¼‰

è¿™å°±æ˜¯ä¸Šä¸‹æ–‡æ³¨å…¥å‘æŒ¥ä½œç”¨çš„åœ°æ–¹ã€‚

## **ä¸Šä¸‹æ–‡å­¦ä¹  / ä¸Šä¸‹æ–‡æ³¨å…¥**

åœ¨ä½¿ç”¨ä¸Šä¸‹æ–‡æ³¨å…¥æ—¶ï¼Œæˆ‘ä»¬å¹¶æ²¡æœ‰ä¿®æ”¹ LLMï¼Œè€Œæ˜¯ä¸“æ³¨äºæç¤ºæœ¬èº«ï¼Œå¹¶å°†ç›¸å…³çš„ä¸Šä¸‹æ–‡æ³¨å…¥åˆ°æç¤ºä¸­ã€‚

å› æ­¤ï¼Œæˆ‘ä»¬éœ€è¦è€ƒè™‘å¦‚ä½•ä¸ºæç¤ºæä¾›æ­£ç¡®çš„ä¿¡æ¯ã€‚åœ¨ä¸‹å›¾ä¸­ï¼Œæ‚¨å¯ä»¥çœ‹åˆ°æ•´ä¸ªè¿‡ç¨‹çš„ç¤ºæ„å›¾ã€‚æˆ‘ä»¬éœ€è¦ä¸€ä¸ªèƒ½å¤Ÿè¯†åˆ«æœ€ç›¸å…³æ•°æ®çš„è¿‡ç¨‹ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬éœ€è¦ä½¿è®¡ç®—æœºèƒ½å¤Ÿæ¯”è¾ƒæ–‡æœ¬ç‰‡æ®µã€‚

![](img/6e88d7155e335397440e55e00b5b0ea6.png)

æˆ‘ä»¬éç»“æ„åŒ–æ•°æ®ä¸­çš„ç›¸ä¼¼æ€§æœç´¢ â€” ä½œè€…æä¾›çš„å›¾ç‰‡

è¿™å¯ä»¥é€šè¿‡åµŒå…¥ï¼ˆembeddingsï¼‰æ¥å®Œæˆã€‚é€šè¿‡åµŒå…¥ï¼Œæˆ‘ä»¬å°†æ–‡æœ¬è½¬æ¢ä¸ºå‘é‡ï¼Œä»è€Œå…è®¸æˆ‘ä»¬åœ¨å¤šç»´åµŒå…¥ç©ºé—´ä¸­è¡¨ç¤ºæ–‡æœ¬ã€‚åœ¨ç©ºé—´ä¸­å½¼æ­¤æ›´æ¥è¿‘çš„ç‚¹é€šå¸¸ç”¨äºç›¸åŒçš„ä¸Šä¸‹æ–‡ã€‚ä¸ºäº†é˜²æ­¢è¿™ç§ç›¸ä¼¼æ€§æœç´¢è€—æ—¶è¿‡é•¿ï¼Œæˆ‘ä»¬å°†å‘é‡å­˜å‚¨åœ¨å‘é‡æ•°æ®åº“ä¸­å¹¶å¯¹å…¶è¿›è¡Œç´¢å¼•ã€‚

> å¾®è½¯å‘æˆ‘ä»¬å±•ç¤ºäº†è¿™å¯èƒ½å¦‚ä½•åœ¨ Bing Chat ä¸­å®ç°ã€‚Bing ç»“åˆäº† LLM ç†è§£è¯­è¨€å’Œä¸Šä¸‹æ–‡çš„èƒ½åŠ›ä¸ä¼ ç»Ÿç½‘ç»œæœç´¢çš„æ•ˆç‡ã€‚

è¿™ç¯‡æ–‡ç« çš„ç›®æ ‡æ˜¯å±•ç¤ºåˆ›å»ºä¸€ä¸ªç®€å•è§£å†³æ–¹æ¡ˆçš„è¿‡ç¨‹ï¼Œä½¿æˆ‘ä»¬èƒ½å¤Ÿåˆ†æè‡ªå·±çš„æ–‡æœ¬å’Œæ–‡æ¡£ï¼Œç„¶åå°†ä»ä¸­è·å¾—çš„è§è§£èå…¥åˆ°è§£å†³æ–¹æ¡ˆè¿”å›ç»™ç”¨æˆ·çš„ç­”æ¡ˆä¸­ã€‚æˆ‘å°†æè¿°å®ç°ç«¯åˆ°ç«¯è§£å†³æ–¹æ¡ˆæ‰€éœ€çš„æ‰€æœ‰æ­¥éª¤å’Œç»„ä»¶ã€‚

é‚£ä¹ˆæˆ‘ä»¬å¦‚ä½•åˆ©ç”¨ LLM çš„èƒ½åŠ›æ¥æ»¡è¶³æˆ‘ä»¬çš„éœ€æ±‚å‘¢ï¼Ÿè®©æˆ‘ä»¬ä¸€æ­¥ä¸€æ­¥åœ°æ¥çœ‹çœ‹ã€‚

## æ­¥éª¤æ•™ç¨‹ â€” ä½ çš„ç¬¬ä¸€ä¸ª LLM åº”ç”¨

æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å¸Œæœ›åˆ©ç”¨ LLM æ¥å›åº”æœ‰å…³æˆ‘ä»¬ä¸ªäººæ•°æ®çš„è¯¢é—®ã€‚ä¸ºæ­¤ï¼Œæˆ‘å¼€å§‹å°†ä¸ªäººæ•°æ®çš„å†…å®¹è½¬ç§»åˆ°å‘é‡æ•°æ®åº“ä¸­ã€‚è¿™ä¸ªæ­¥éª¤è‡³å…³é‡è¦ï¼Œå› ä¸ºå®ƒä½¿æˆ‘ä»¬èƒ½å¤Ÿé«˜æ•ˆåœ°æœç´¢æ–‡æœ¬ä¸­çš„ç›¸å…³éƒ¨åˆ†ã€‚æˆ‘ä»¬å°†åˆ©ç”¨æ¥è‡ªæ•°æ®çš„ä¿¡æ¯å’Œ LLM çš„èƒ½åŠ›æ¥è§£é‡Šæ–‡æœ¬ï¼Œä»¥å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚

æˆ‘ä»¬è¿˜å¯ä»¥æŒ‡å¯¼èŠå¤©æœºå™¨äººä»…æ ¹æ®æˆ‘ä»¬æä¾›çš„æ•°æ®å›ç­”é—®é¢˜ã€‚è¿™æ ·ï¼Œæˆ‘ä»¬å¯ä»¥ç¡®ä¿èŠå¤©æœºå™¨äººä¸“æ³¨äºæ‰‹å¤´çš„æ•°æ®ï¼Œå¹¶æä¾›å‡†ç¡®ä¸”ç›¸å…³çš„å›åº”ã€‚

ä¸ºäº†å®ç°æˆ‘ä»¬çš„ç”¨ä¾‹ï¼Œæˆ‘ä»¬å°†å¤§é‡ä¾èµ– LangChainã€‚

# LangChain æ˜¯ä»€ä¹ˆï¼Ÿ

**â€œLangChain æ˜¯ä¸€ä¸ªç”¨äºå¼€å‘è¯­è¨€æ¨¡å‹é©±åŠ¨åº”ç”¨ç¨‹åºçš„æ¡†æ¶ã€‚â€ï¼ˆLangchain, 2023ï¼‰**

å› æ­¤ï¼ŒLangChain æ˜¯ä¸€ä¸ª Python æ¡†æ¶ï¼Œæ—¨åœ¨æ”¯æŒå„ç§ LLM åº”ç”¨ç¨‹åºçš„åˆ›å»ºï¼Œå¦‚èŠå¤©æœºå™¨äººã€æ‘˜è¦å·¥å…·ä»¥åŠåŸºæœ¬ä¸Šä»»ä½•ä½ æƒ³åˆ›å»ºä»¥åˆ©ç”¨ LLM èƒ½åŠ›çš„å·¥å…·ã€‚è¯¥åº“ç»“åˆäº†æˆ‘ä»¬æ‰€éœ€çš„å„ç§ç»„ä»¶ã€‚æˆ‘ä»¬å¯ä»¥å°†è¿™äº›ç»„ä»¶è¿æ¥æˆæ‰€è°“çš„é“¾ã€‚

Langchain æœ€é‡è¦çš„æ¨¡å—æ˜¯ï¼ˆLangchain, 2023ï¼‰ï¼š

1.  **æ¨¡å‹ï¼š** å„ç§æ¨¡å‹ç±»å‹çš„æ¥å£

1.  **æç¤ºï¼š** æç¤ºç®¡ç†ã€æç¤ºä¼˜åŒ–å’Œæç¤ºåºåˆ—åŒ–

1.  **ç´¢å¼•ï¼š** æ–‡æ¡£åŠ è½½å™¨ã€æ–‡æœ¬åˆ†å‰²å™¨ã€å‘é‡å­˜å‚¨ â€” å®ç°å¯¹æ•°æ®çš„æ›´å¿«ã€æ›´é«˜æ•ˆçš„è®¿é—®

1.  **é“¾ï¼š** é“¾è¶…è¶Šäº†å•ä¸€çš„ LLM è°ƒç”¨ï¼Œå®ƒä»¬å…è®¸æˆ‘ä»¬è®¾ç½®è°ƒç”¨çš„åºåˆ—

åœ¨ä¸‹å›¾ä¸­ï¼Œä½ å¯ä»¥çœ‹åˆ°è¿™äº›ç»„ä»¶çš„ä½œç”¨ã€‚æˆ‘ä»¬ä½¿ç”¨ç´¢å¼•æ¨¡å—ä¸­çš„æ–‡æ¡£åŠ è½½å™¨å’Œæ–‡æœ¬åˆ†å‰²å™¨æ¥åŠ è½½å’Œå¤„ç†æˆ‘ä»¬è‡ªå·±çš„éç»“æ„åŒ–æ•°æ®ã€‚æç¤ºæ¨¡å—å…è®¸æˆ‘ä»¬å°†æ‰¾åˆ°çš„å†…å®¹æ³¨å…¥åˆ°æˆ‘ä»¬çš„æç¤ºæ¨¡æ¿ä¸­ï¼Œæœ€åï¼Œæˆ‘ä»¬é€šè¿‡æ¨¡å‹æ¨¡å—å°†æç¤ºå‘é€ç»™æˆ‘ä»¬çš„æ¨¡å‹ã€‚

![](img/ef1c2c72aff354412158ff38565e9aa3.png)

ä½ ä¸º LLM åº”ç”¨æ‰€éœ€çš„ç»„ä»¶ â€” ä½œè€…æä¾›çš„å›¾åƒ

**5\. ä»£ç†ï¼š** ä»£ç†æ˜¯ä½¿ç”¨ LLM åšå‡ºå…³äºé‡‡å–å“ªäº›è¡ŒåŠ¨çš„é€‰æ‹©çš„å®ä½“ã€‚åœ¨é‡‡å–è¡ŒåŠ¨åï¼Œå®ƒä»¬è§‚å¯Ÿè¯¥è¡ŒåŠ¨çš„ç»“æœï¼Œå¹¶é‡å¤è¯¥è¿‡ç¨‹ï¼Œç›´åˆ°å®Œæˆä»»åŠ¡ã€‚

![](img/e660c81a051ece8e5fb10fce61226cfa.png)

ä»£ç†è‡ªä¸»å†³å®šå¦‚ä½•æ‰§è¡Œç‰¹å®šä»»åŠ¡ â€” ä½œè€…æä¾›çš„å›¾ç‰‡

æˆ‘ä»¬åœ¨ç¬¬ä¸€æ­¥ä¸­ä½¿ç”¨ LangChain åŠ è½½æ–‡æ¡£ï¼Œåˆ†æå®ƒä»¬å¹¶ä½¿å…¶é«˜æ•ˆå¯æœç´¢ã€‚åœ¨æˆ‘ä»¬ç´¢å¼•äº†æ–‡æœ¬ä¹‹åï¼Œè¯†åˆ«ä¸å›ç­”ç”¨æˆ·é—®é¢˜ç›¸å…³çš„æ–‡æœ¬ç‰‡æ®µåº”è¯¥å˜å¾—æ›´åŠ é«˜æ•ˆã€‚

æˆ‘ä»¬çš„ç®€å•åº”ç”¨ç¨‹åºæ‰€éœ€çš„å½“ç„¶æ˜¯ä¸€ä¸ª LLMã€‚æˆ‘ä»¬å°†é€šè¿‡ OpenAI API ä½¿ç”¨ GPT3.5ã€‚ç„¶åæˆ‘ä»¬éœ€è¦ä¸€ä¸ªå‘é‡å­˜å‚¨åº“ï¼Œä»¥ä¾¿æˆ‘ä»¬å¯ä»¥å°†è‡ªå·±çš„æ•°æ®æä¾›ç»™ LLMã€‚å¦‚æœæˆ‘ä»¬æƒ³å¯¹ä¸åŒçš„æŸ¥è¯¢æ‰§è¡Œä¸åŒçš„æ“ä½œï¼Œæˆ‘ä»¬è¿˜éœ€è¦ä¸€ä¸ªä»£ç†æ¥å†³å®šæ¯ä¸ªæŸ¥è¯¢åº”è¯¥å‘ç”Ÿä»€ä¹ˆã€‚

æˆ‘ä»¬ä»å¤´å¼€å§‹ã€‚æˆ‘ä»¬é¦–å…ˆéœ€è¦å¯¼å…¥æˆ‘ä»¬è‡ªå·±çš„æ–‡æ¡£ã€‚

ä»¥ä¸‹éƒ¨åˆ†æè¿°äº† LangChain çš„ Loader æ¨¡å—ä¸­åŒ…å«å“ªäº›æ¨¡å—ï¼Œä»¥ä»ä¸åŒæ¥æºåŠ è½½ä¸åŒç±»å‹çš„æ–‡æ¡£ã€‚

# 1\. ä½¿ç”¨ LangChain åŠ è½½æ–‡æ¡£

LangChain èƒ½å¤Ÿä»å„ç§æ¥æºåŠ è½½å¤šä¸ªæ–‡æ¡£ã€‚ä½ å¯ä»¥åœ¨ LangChain çš„[æ–‡æ¡£](https://python.langchain.com/en/latest/modules/indexes/document_loaders.html)ä¸­æ‰¾åˆ°å¯èƒ½çš„æ–‡æ¡£åŠ è½½å™¨åˆ—è¡¨ã€‚å…¶ä¸­åŒ…æ‹¬ HTML é¡µé¢ã€S3 å­˜å‚¨æ¡¶ã€PDF æ–‡ä»¶ã€Notionã€Google Drive ç­‰ç­‰çš„åŠ è½½å™¨ã€‚

å¯¹äºæˆ‘ä»¬çš„ç®€å•ç¤ºä¾‹ï¼Œæˆ‘ä»¬ä½¿ç”¨çš„æ•°æ®å¯èƒ½æœªåŒ…å«åœ¨ GPT3.5 çš„è®­ç»ƒæ•°æ®ä¸­ã€‚æˆ‘ä½¿ç”¨å…³äº GPT4 çš„ç»´åŸºç™¾ç§‘æ–‡ç« ï¼Œå› ä¸ºæˆ‘å‡è®¾ GPT3.5 å¯¹ GPT4 çš„çŸ¥è¯†æœ‰é™ã€‚

å¯¹äºè¿™ä¸ªç®€å•çš„ç¤ºä¾‹ï¼Œæˆ‘æ²¡æœ‰ä½¿ç”¨ä»»ä½• LangChain åŠ è½½å™¨ï¼Œåªæ˜¯ç›´æ¥ä»ç»´åŸºç™¾ç§‘ [è®¸å¯: CC BY-SA 3.0] æŠ“å–æ–‡æœ¬ï¼Œä½¿ç”¨äº†***BeautifulSoup.***

> **è¯·æ³¨æ„ï¼ŒæŠ“å–ç½‘ç«™å†…å®¹åº”ä»…æŒ‰ç…§ç½‘ç«™çš„ä½¿ç”¨æ¡æ¬¾ä»¥åŠä½ å¸Œæœ›ä½¿ç”¨çš„æ–‡æœ¬å’Œæ•°æ®çš„ç‰ˆæƒ/è®¸å¯çŠ¶æ€è¿›è¡Œã€‚**

```py
import requests
from bs4 import BeautifulSoup

url = "https://en.wikipedia.org/wiki/GPT-4"
response = requests.get(url)

soup = BeautifulSoup(response.content, 'html.parser')

# find all the text on the page
text = soup.get_text()

# find the content div
content_div = soup.find('div', {'class': 'mw-parser-output'})

# remove unwanted elements from div
unwanted_tags = ['sup', 'span', 'table', 'ul', 'ol']
for tag in unwanted_tags:
    for match in content_div.findAll(tag):
        match.extract()

print(content_div.get_text())
```

![](img/0ecc028506e2ab4fad286d986afcda92.png)

# 2\. å°†æ–‡æ¡£æ‹†åˆ†æˆæ–‡æœ¬ç‰‡æ®µ

æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å¿…é¡»å°†æ–‡æœ¬åˆ†æˆè¾ƒå°çš„éƒ¨åˆ†ï¼Œç§°ä¸ºæ–‡æœ¬å—ã€‚æ¯ä¸ªæ–‡æœ¬å—ä»£è¡¨åµŒå…¥ç©ºé—´ä¸­çš„ä¸€ä¸ªæ•°æ®ç‚¹ï¼Œä½¿è®¡ç®—æœºèƒ½å¤Ÿç¡®å®šè¿™äº›å—ä¹‹é—´çš„ç›¸ä¼¼æ€§ã€‚

ä»¥ä¸‹æ–‡æœ¬ç‰‡æ®µåˆ©ç”¨äº† langchain çš„æ–‡æœ¬åˆ†å‰²æ¨¡å—ã€‚åœ¨è¿™ç§ç‰¹å®šæƒ…å†µä¸‹ï¼Œæˆ‘ä»¬æŒ‡å®šäº† 100 çš„å—å¤§å°å’Œ 20 çš„å—é‡å ã€‚è™½ç„¶ä½¿ç”¨æ›´å¤§çš„æ–‡æœ¬å—å¾ˆå¸¸è§ï¼Œä½†ä½ å¯ä»¥å°è¯•ä¸€ä¸‹ä»¥æ‰¾åˆ°é€‚åˆä½ ç”¨ä¾‹çš„æœ€ä½³å¤§å°ã€‚ä½ åªéœ€è¦è®°ä½ï¼Œæ¯ä¸ª LLM éƒ½æœ‰ä¸€ä¸ªä»¤ç‰Œé™åˆ¶ï¼ˆGPT 3.5 ä¸º 4000 ä»¤ç‰Œï¼‰ã€‚ç”±äºæˆ‘ä»¬å°†æ–‡æœ¬å—æ’å…¥åˆ°æç¤ºä¸­ï¼Œæˆ‘ä»¬éœ€è¦ç¡®ä¿æ•´ä¸ªæç¤ºä¸è¶…è¿‡ 4000 ä¸ªä»¤ç‰Œã€‚

```py
from langchain.text_splitter import RecursiveCharacterTextSplitter

article_text = content_div.get_text()

text_splitter = RecursiveCharacterTextSplitter(
    # Set a really small chunk size, just to show.
    chunk_size = 100,
    chunk_overlap  = 20,
    length_function = len,
)

texts = text_splitter.create_documents([article_text])
print(texts[0])
print(texts[1])
```

![](img/5f30c3e19308d6a95e82e8dbbd2be48a.png)

è¿™å°†æˆ‘ä»¬çš„æ•´ä¸ªæ–‡æœ¬åˆ†å‰²å¦‚ä¸‹ï¼š

![](img/9883b42387e6a80ef31e153685590e63.png)

Langchain æ–‡æœ¬åˆ†å‰²å™¨ â€” ä½œè€…æä¾›çš„å›¾ç‰‡

# 3. ä»æ–‡æœ¬å—åˆ°åµŒå…¥

ç°åœ¨æˆ‘ä»¬éœ€è¦ä½¿æ–‡æœ¬ç»„ä»¶å¯¹æˆ‘ä»¬çš„ç®—æ³•å¯ç†è§£å’Œå¯æ¯”ã€‚æˆ‘ä»¬å¿…é¡»æ‰¾åˆ°ä¸€ç§å°†äººç±»è¯­è¨€è½¬æ¢ä¸ºæ•°å­—å½¢å¼ï¼ˆç”±æ¯”ç‰¹å’Œå­—èŠ‚è¡¨ç¤ºï¼‰çš„æ–¹æ³•ã€‚

è¿™å¼ å›¾ç‰‡æä¾›äº†ä¸€ä¸ªç®€å•çš„ä¾‹å­ï¼Œå¯¹å¤§å¤šæ•°äººç±»æ¥è¯´å¯èƒ½æ˜¾è€Œæ˜“è§ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬éœ€è¦æ‰¾åˆ°ä¸€ç§æ–¹æ³•ï¼Œè®©è®¡ç®—æœºç†è§£â€œCharlesâ€è¿™ä¸ªåå­—ä¸ç”·æ€§ç›¸å…³ï¼Œè€Œä¸æ˜¯å¥³æ€§ï¼Œå¹¶ä¸”å¦‚æœ Charles æ˜¯ç”·æ€§ï¼Œä»–æ˜¯å›½ç‹è€Œä¸æ˜¯å¥³ç‹ã€‚

![](img/5e983ecc380357da8438928f5f94f251.png)

ä½¿è¯­è¨€å¯¹æˆ‘ä»¬çš„è®¡ç®—æœºå¯ç†è§£ â€” ä½œè€…æä¾›çš„å›¾ç‰‡

è¿‘å¹´æ¥ï¼Œå‡ºç°äº†å¯ä»¥åšåˆ°è¿™ä¸€ç‚¹çš„æ–°æ–¹æ³•å’Œæ¨¡å‹ã€‚æˆ‘ä»¬æ‰€å¸Œæœ›çš„æ˜¯ä¸€ç§å°†å•è¯çš„å«ä¹‰è½¬æ¢ä¸º n ç»´ç©ºé—´çš„æ–¹æ³•ï¼Œä»¥ä¾¿èƒ½å¤Ÿæ¯”è¾ƒæ–‡æœ¬å—ä¹‹é—´çš„ç›¸ä¼¼æ€§ï¼Œç”šè‡³è®¡ç®—å®ƒä»¬ä¹‹é—´çš„ç›¸ä¼¼åº¦åº¦é‡ã€‚

åµŒå…¥æ¨¡å‹é€šè¿‡åˆ†æå•è¯é€šå¸¸ä½¿ç”¨çš„ä¸Šä¸‹æ–‡æ¥å°è¯•å­¦ä¹ è¿™ä¸€ç‚¹ã€‚ç”±äº teaã€coffee å’Œ breakfast ç»å¸¸åœ¨ç›¸åŒçš„ä¸Šä¸‹æ–‡ä¸­ä½¿ç”¨ï¼Œå®ƒä»¬åœ¨ n ç»´ç©ºé—´ä¸­å½¼æ­¤æ›´æ¥è¿‘ï¼Œè€Œä¸æ˜¯ï¼Œä¾‹å¦‚ï¼Œtea å’Œ peaã€‚Tea å’Œ pea å¬èµ·æ¥ç›¸ä¼¼ï¼Œä½†å¾ˆå°‘ä¸€èµ·ä½¿ç”¨ã€‚ï¼ˆAssemblyAIï¼Œ2022ï¼‰

![](img/b82fbabaf51da10acdee62bd36353824.png)

åµŒå…¥åˆ†æäº†å•è¯ä½¿ç”¨çš„ä¸Šä¸‹æ–‡ï¼Œè€Œä¸æ˜¯å•è¯æœ¬èº« â€” ä½œè€…æä¾›çš„å›¾ç‰‡

åµŒå…¥æ¨¡å‹ä¸ºåµŒå…¥ç©ºé—´ä¸­çš„æ¯ä¸ªå•è¯æä¾›äº†ä¸€ä¸ªå‘é‡ã€‚æœ€ç»ˆï¼Œé€šè¿‡ä½¿ç”¨å‘é‡è¡¨ç¤ºå®ƒä»¬ï¼Œæˆ‘ä»¬èƒ½å¤Ÿæ‰§è¡Œæ•°å­¦è®¡ç®—ï¼Œä¾‹å¦‚è®¡ç®—å•è¯ä¹‹é—´çš„ç›¸ä¼¼æ€§ï¼Œä½œä¸ºæ•°æ®ç‚¹ä¹‹é—´çš„è·ç¦»ã€‚

![](img/e52927d7c78f5f6d5235f1d06ef8a3e9.png)

éšæœºçš„è‹±æ–‡å•è¯åœ¨äºŒç»´åµŒå…¥ç©ºé—´ä¸­ â€” ä½œè€…æä¾›çš„å›¾ç‰‡

å°†æ–‡æœ¬è½¬æ¢ä¸ºåµŒå…¥æœ‰å‡ ç§æ–¹æ³•ï¼Œä¾‹å¦‚ Word2Vecã€GloVeã€fastText æˆ– ELMoã€‚

**åµŒå…¥æ¨¡å‹**

ä¸ºäº†æ•æ‰åµŒå…¥ä¸­å•è¯ä¹‹é—´çš„ç›¸ä¼¼æ€§ï¼ŒWord2Vec ä½¿ç”¨äº†ä¸€ä¸ªç®€å•çš„ç¥ç»ç½‘ç»œã€‚æˆ‘ä»¬ç”¨å¤§é‡çš„æ–‡æœ¬æ•°æ®è®­ç»ƒè¿™ä¸ªæ¨¡å‹ï¼Œå¹¶å¸Œæœ›åˆ›å»ºä¸€ä¸ªèƒ½å¤Ÿå°†æ¯ä¸ªå•è¯åˆ†é…åˆ° n ç»´åµŒå…¥ç©ºé—´ä¸­çš„ç‚¹ï¼Œå¹¶ä»¥å‘é‡çš„å½¢å¼æè¿°å…¶å«ä¹‰çš„æ¨¡å‹ã€‚

åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬å°†è¾“å…¥å±‚ä¸­çš„æ¯ä¸ªç‹¬ç‰¹å•è¯åˆ†é…ç»™ä¸€ä¸ªç¥ç»å…ƒã€‚åœ¨ä¸‹é¢çš„å›¾ç‰‡ä¸­ï¼Œä½ å¯ä»¥çœ‹åˆ°ä¸€ä¸ªç®€å•çš„ä¾‹å­ã€‚åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œéšè—å±‚åªåŒ…å«ä¸¤ä¸ªç¥ç»å…ƒã€‚ä¸¤ä¸ªç¥ç»å…ƒæ˜¯å› ä¸ºæˆ‘ä»¬å¸Œæœ›å°†å•è¯æ˜ å°„åˆ°äºŒç»´åµŒå…¥ç©ºé—´ä¸­ã€‚ï¼ˆç°æœ‰çš„æ¨¡å‹å®é™…ä¸Šè¦å¤§å¾—å¤šï¼Œå› æ­¤åœ¨æ›´é«˜ç»´ç©ºé—´ä¸­è¡¨ç¤ºå•è¯â€”â€”ä¾‹å¦‚ï¼ŒOpenAI çš„ Ada åµŒå…¥æ¨¡å‹ä½¿ç”¨çš„æ˜¯ 1536 ç»´ï¼‰åœ¨è®­ç»ƒè¿‡ç¨‹åï¼Œå•ç‹¬çš„æƒé‡æè¿°äº†åœ¨åµŒå…¥ç©ºé—´ä¸­çš„ä½ç½®ã€‚

åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œæˆ‘ä»¬çš„æ•°æ®é›†ç”±ä¸€ä¸ªå¥å­ç»„æˆï¼šâ€œGoogle is a tech company.â€ å¥å­ä¸­çš„æ¯ä¸ªè¯ä½œä¸ºç¥ç»ç½‘ç»œï¼ˆNNï¼‰çš„è¾“å…¥ã€‚å› æ­¤ï¼Œæˆ‘ä»¬çš„ç½‘ç»œæœ‰äº”ä¸ªè¾“å…¥ç¥ç»å…ƒï¼Œæ¯ä¸ªè¯ä¸€ä¸ªã€‚

åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬çš„é‡ç‚¹æ˜¯é¢„æµ‹æ¯ä¸ªè¾“å…¥è¯çš„ä¸‹ä¸€ä¸ªè¯ã€‚å½“æˆ‘ä»¬ä»å¥å­çš„å¼€å¤´å¼€å§‹æ—¶ï¼Œä¸â€œGoogleâ€ç›¸å…³çš„è¾“å…¥ç¥ç»å…ƒæ¥æ”¶åˆ°å€¼ 1ï¼Œè€Œå…¶ä½™ç¥ç»å…ƒæ¥æ”¶åˆ°å€¼ 0ã€‚æˆ‘ä»¬çš„ç›®æ ‡æ˜¯è®­ç»ƒç½‘ç»œåœ¨è¿™ç§æƒ…å†µä¸‹é¢„æµ‹å‡ºâ€œisâ€è¿™ä¸ªè¯ã€‚

![](img/207cc5bc488b08fbaa216e98e65841dd.png)

Word2Vec: å­¦ä¹ è¯åµŒå…¥ â€” å›¾ç‰‡ç”±ä½œè€…æä¾›

å®é™…ä¸Šï¼Œæœ‰å¤šç§æ–¹æ³•å¯ä»¥å­¦ä¹ åµŒå…¥æ¨¡å‹ï¼Œæ¯ç§æ–¹æ³•éƒ½æœ‰å…¶ç‹¬ç‰¹çš„é¢„æµ‹è¾“å‡ºçš„æ–¹å¼ã€‚ä¸¤ç§å¸¸ç”¨çš„æ–¹æ³•æ˜¯ CBOWï¼ˆè¿ç»­è¯è¢‹æ¨¡å‹ï¼‰å’Œ Skip-gramã€‚

åœ¨ CBOW ä¸­ï¼Œæˆ‘ä»¬å°†å‘¨å›´çš„è¯ä½œä¸ºè¾“å…¥ï¼Œç›®æ ‡æ˜¯é¢„æµ‹ä¸­é—´çš„è¯ã€‚ç›¸åï¼Œåœ¨ Skip-gram ä¸­ï¼Œæˆ‘ä»¬å°†ä¸­é—´çš„è¯ä½œä¸ºè¾“å…¥ï¼Œå¹¶å°è¯•é¢„æµ‹å…¶å·¦ä¾§å’Œå³ä¾§çš„è¯ã€‚ç„¶è€Œï¼Œæˆ‘ä¸ä¼šæ·±å…¥æ¢è®¨è¿™äº›æ–¹æ³•çš„ç»†èŠ‚ã€‚å¯ä»¥è¯´ï¼Œè¿™äº›æ–¹æ³•ä¸ºæˆ‘ä»¬æä¾›äº†åµŒå…¥ï¼Œè¿™äº›åµŒå…¥æ˜¯é€šè¿‡åˆ†æå¤§é‡æ–‡æœ¬æ•°æ®çš„ä¸Šä¸‹æ–‡æ¥æ•æ‰è¯è¯­ä¹‹é—´å…³ç³»çš„è¡¨ç¤ºã€‚

![](img/72da8315043be5325c287969edce53ab.png)

CBOW ä¸ Skip-gram â€” å›¾ç‰‡ç”±ä½œè€…æä¾›

å¦‚æœä½ æƒ³äº†è§£æ›´å¤šå…³äºåµŒå…¥çš„å†…å®¹*ï¼Œäº’è”ç½‘ä¸Šæœ‰å¤§é‡çš„ä¿¡æ¯ã€‚ç„¶è€Œï¼Œå¦‚æœä½ æ›´å–œæ¬¢è§†è§‰å’Œé€æ­¥æŒ‡å¯¼ï¼Œä½ å¯èƒ½ä¼šè§‰å¾—è§‚çœ‹ Josh* [*Starmer å…³äºè¯åµŒå…¥å’Œ Word2Vec çš„ StatQuest*](https://www.youtube.com/watch?v=viZrOnJclY0&t=204s)* *å¾ˆæœ‰å¸®åŠ©ã€‚*

**å›åˆ°åµŒå…¥æ¨¡å‹**

æˆ‘åˆšåˆšç”¨ä¸€ä¸ªç®€å•çš„äºŒç»´åµŒå…¥ç©ºé—´ç¤ºä¾‹æ¥è§£é‡Šçš„å†…å®¹ä¹Ÿé€‚ç”¨äºæ›´å¤§çš„æ¨¡å‹ã€‚ä¾‹å¦‚ï¼Œæ ‡å‡†çš„ Word2Vec å‘é‡æœ‰ 300 ç»´ï¼Œè€Œ OpenAI çš„ Ada æ¨¡å‹æœ‰ 1536 ç»´ã€‚è¿™äº›é¢„è®­ç»ƒçš„å‘é‡ä½¿æˆ‘ä»¬èƒ½å¤Ÿç²¾ç¡®åœ°æ•æ‰è¯è¯­åŠå…¶å«ä¹‰ä¹‹é—´çš„å…³ç³»ï¼Œä»¥è‡³äºæˆ‘ä»¬å¯ä»¥ç”¨å®ƒä»¬è¿›è¡Œè®¡ç®—ã€‚ä¾‹å¦‚ï¼Œä½¿ç”¨è¿™äº›å‘é‡ï¼Œæˆ‘ä»¬å¯ä»¥å‘ç°æ³•å›½ + æŸæ— â€” å¾·å›½ = å·´é»ï¼ŒåŒæ—¶ï¼Œ*å¿«é€Ÿ* + *æ¸©æš–* â€” *å¿«é€Ÿ* = *æ›´æ¸©æš–*ã€‚ (Tazzyman, n.d.)

![](img/00d72db50ce28b008b94bcdc04011b8b.png)

ä½¿ç”¨åµŒå…¥è¿›è¡Œè®¡ç®— â€” å›¾ç‰‡ç”±ä½œè€…æä¾›

åœ¨æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å¸Œæœ›ä½¿ç”¨ OpenAI APIï¼Œä¸ä»…ä½¿ç”¨ OpenAI çš„ LLMï¼Œè¿˜åˆ©ç”¨å®ƒä»¬çš„åµŒå…¥æ¨¡å‹ã€‚

*æ³¨æ„ï¼šåµŒå…¥æ¨¡å‹å’Œ LLM ä¹‹é—´çš„åŒºåˆ«åœ¨äºï¼ŒåµŒå…¥æ¨¡å‹ä¸“æ³¨äºåˆ›å»ºè¯è¯­æˆ–çŸ­è¯­çš„å‘é‡è¡¨ç¤ºï¼Œä»¥æ•æ‰å®ƒä»¬çš„å«ä¹‰å’Œå…³ç³»ï¼Œè€Œ LLM åˆ™æ˜¯å¤šåŠŸèƒ½çš„æ¨¡å‹ï¼Œç»è¿‡è®­ç»ƒå¯ä»¥æ ¹æ®æä¾›çš„æç¤ºæˆ–æŸ¥è¯¢ç”Ÿæˆè¿è´¯ä¸”ç¬¦åˆä¸Šä¸‹æ–‡çš„æ–‡æœ¬ã€‚*

**OpenAI åµŒå…¥æ¨¡å‹**

ä¸ OpenAI çš„å„ç§ LLM ç±»ä¼¼ï¼Œæ‚¨è¿˜å¯ä»¥åœ¨ Adaã€Davinciã€Curie å’Œ Babbage ç­‰å„ç§åµŒå…¥æ¨¡å‹ä¹‹é—´è¿›è¡Œé€‰æ‹©ã€‚å…¶ä¸­ï¼ŒAda-002 ç›®å‰æ˜¯æœ€å¿«å’Œæœ€å…·æˆæœ¬æ•ˆç›Šçš„æ¨¡å‹ï¼Œè€Œ Davinci é€šå¸¸æä¾›æœ€é«˜çš„å‡†ç¡®æ€§å’Œæ€§èƒ½ã€‚ç„¶è€Œï¼Œæ‚¨éœ€è¦è‡ªå·±å°è¯•ï¼Œæ‰¾åˆ°é€‚åˆæ‚¨ä½¿ç”¨æ¡ˆä¾‹çš„æœ€ä½³æ¨¡å‹ã€‚å¦‚æœæ‚¨å¯¹ OpenAI Embeddings æœ‰è¯¦ç»†äº†è§£çš„å…´è¶£ï¼Œå¯ä»¥å‚è€ƒ[OpenAI æ–‡æ¡£](https://platform.openai.com/docs/guides/embeddings/what-are-embeddings)ã€‚

æˆ‘ä»¬ä½¿ç”¨ Embedding Models çš„ç›®æ ‡æ˜¯å°†æ–‡æœ¬å—è½¬æ¢ä¸ºå‘é‡ã€‚åœ¨ç¬¬äºŒä»£ Ada çš„æƒ…å†µä¸‹ï¼Œè¿™äº›å‘é‡å…·æœ‰ 1536 ä¸ªè¾“å‡ºç»´åº¦ï¼Œè¿™æ„å‘³ç€å®ƒä»¬åœ¨ 1536 ç»´ç©ºé—´ä¸­è¡¨ç¤ºä¸€ä¸ªç‰¹å®šçš„ä½ç½®æˆ–æ–¹å‘ã€‚

OpenAI åœ¨å…¶æ–‡æ¡£ä¸­æè¿°äº†è¿™äº›åµŒå…¥å‘é‡å¦‚ä¸‹ï¼š

â€œæ•°å€¼ä¸Šç›¸ä¼¼çš„åµŒå…¥ä¹Ÿåœ¨è¯­ä¹‰ä¸Šç›¸ä¼¼ã€‚ä¾‹å¦‚ï¼Œâ€œcanine companions sayâ€çš„åµŒå…¥å‘é‡å°†æ¯”â€œmeowâ€çš„åµŒå…¥å‘é‡æ›´æ¥è¿‘â€œwoofâ€çš„åµŒå…¥å‘é‡ã€‚â€ï¼ˆOpenAIï¼Œ2022ï¼‰

è®©æˆ‘ä»¬å°è¯•ä¸€ä¸‹ã€‚æˆ‘ä»¬ä½¿ç”¨ OpenAI çš„ API å°†æ–‡æœ¬ç‰‡æ®µè½¬æ¢ä¸ºåµŒå…¥ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š

```py
import openai

print(texts[0])

embedding = openai.Embedding.create(
    input=texts[0].page_content, model="text-embedding-ada-002"
)["data"][0]["embedding"]

len(embedding)
```

![](img/0f1bafc97daecdcd048e21ad373303d3.png)

æˆ‘ä»¬å°†æ–‡æœ¬ï¼Œä¾‹å¦‚åŒ…å«â€œ2023 text-generating language modelâ€çš„ç¬¬ä¸€ä¸ªæ–‡æœ¬å—ï¼Œè½¬æ¢ä¸º 1536 ç»´çš„å‘é‡ã€‚é€šè¿‡å¯¹æ¯ä¸ªæ–‡æœ¬å—è¿›è¡Œè¿™ç§å¤„ç†ï¼Œæˆ‘ä»¬å¯ä»¥åœ¨ 1536 ç»´ç©ºé—´ä¸­è§‚å¯Ÿå“ªäº›æ–‡æœ¬å—å½¼æ­¤æ›´æ¥è¿‘ï¼Œæ›´ç›¸ä¼¼ã€‚

è®©æˆ‘ä»¬å°è¯•ä¸€ä¸‹ã€‚æˆ‘ä»¬çš„ç›®æ ‡æ˜¯é€šè¿‡ä¸ºé—®é¢˜ç”ŸæˆåµŒå…¥ï¼Œå¹¶å°†å…¶ä¸ç©ºé—´ä¸­çš„å…¶ä»–æ•°æ®ç‚¹è¿›è¡Œæ¯”è¾ƒï¼Œä»è€Œå°†ç”¨æˆ·çš„é—®é¢˜ä¸æ–‡æœ¬å—è¿›è¡Œæ¯”è¾ƒã€‚

![](img/65eb5bd6420dcbd472c3f86a5dd22b4b.png)

å“ªä¸ªæ–‡æœ¬ç‰‡æ®µåœ¨è¯­ä¹‰ä¸Šæ›´æ¥è¿‘ç”¨æˆ·çš„é—®é¢˜ï¼Ÿâ€” ä½œè€…æä¾›çš„å›¾åƒ

å½“æˆ‘ä»¬å°†æ–‡æœ¬å—å’Œç”¨æˆ·çš„é—®é¢˜è¡¨ç¤ºä¸ºå‘é‡æ—¶ï¼Œæˆ‘ä»¬èƒ½å¤Ÿæ¢ç´¢å„ç§æ•°å­¦å¯èƒ½æ€§ã€‚ä¸ºäº†ç¡®å®šä¸¤ä¸ªæ•°æ®ç‚¹ä¹‹é—´çš„ç›¸ä¼¼åº¦ï¼Œæˆ‘ä»¬éœ€è¦è®¡ç®—å®ƒä»¬åœ¨å¤šç»´ç©ºé—´ä¸­çš„æ¥è¿‘ç¨‹åº¦ï¼Œè¿™å¯ä»¥é€šè¿‡è·ç¦»åº¦é‡å®ç°ã€‚è®¡ç®—ç‚¹ä¹‹é—´è·ç¦»çš„æ–¹æ³•æœ‰å¾ˆå¤šç§ã€‚Maarten Grootendorst åœ¨ä»–çš„ Medium å¸–å­ä¸­æ€»ç»“äº†å…¶ä¸­çš„ä¹ç§ã€‚

å¸¸ç”¨çš„è·ç¦»åº¦é‡æ˜¯ä½™å¼¦ç›¸ä¼¼åº¦ã€‚å› æ­¤ï¼Œè®©æˆ‘ä»¬å°è¯•è®¡ç®—é—®é¢˜ä¸æ–‡æœ¬å—ä¹‹é—´çš„ä½™å¼¦ç›¸ä¼¼åº¦ï¼š

```py
import numpy as np
from numpy.linalg import norm
from langchain.text_splitter import RecursiveCharacterTextSplitter
import requests
from bs4 import BeautifulSoup
import pandas as pd
import openai

####################################################################
# load documents
####################################################################
# URL of the Wikipedia page to scrape
url = 'https://en.wikipedia.org/wiki/Prime_Minister_of_the_United_Kingdom'

# Send a GET request to the URL
response = requests.get(url)

# Parse the HTML content using BeautifulSoup
soup = BeautifulSoup(response.content, 'html.parser')

# Find all the text on the page
text = soup.get_text()

####################################################################
# split text
####################################################################
text_splitter = RecursiveCharacterTextSplitter(
    # Set a really small chunk size, just to show.
    chunk_size = 100,
    chunk_overlap  = 20,
    length_function = len,
)

texts = text_splitter.create_documents([text])

####################################################################
# calculate embeddings
####################################################################
# create new list with all text chunks
text_chunks=[]

for text in texts:
    text_chunks.append(text.page_content)

df = pd.DataFrame({'text_chunks': text_chunks})

####################################################################
# get embeddings from text-embedding-ada model
####################################################################
def get_embedding(text, model="text-embedding-ada-002"):
   text = text.replace("\n", " ")
   return openai.Embedding.create(input = [text], model=model)['data'][0]['embedding']

df['ada_embedding'] = df.text_chunks.apply(lambda x: get_embedding(x, model='text-embedding-ada-002'))

####################################################################
# calculate the embeddings for the user's question
####################################################################
users_question = "What is GPT-4?"

question_embedding = get_embedding(text=users_question, model="text-embedding-ada-002")

# create a list to store the calculated cosine similarity
cos_sim = []

for index, row in df.iterrows():
   A = row.ada_embedding
   B = question_embedding

   # calculate the cosine similarity
   cosine = np.dot(A,B)/(norm(A)*norm(B))

   cos_sim.append(cosine)

df["cos_sim"] = cos_sim
df.sort_values(by=["cos_sim"], ascending=False)
```

![](img/239863d9e0969e84b130659926f610c0.png)

ç°åœ¨æˆ‘ä»¬å¯ä»¥é€‰æ‹©æˆ‘ä»¬å¸Œæœ›æä¾›ç»™ LLM ä»¥å›ç­”é—®é¢˜çš„æ–‡æœ¬å—æ•°é‡ã€‚

ä¸‹ä¸€æ­¥æ˜¯ç¡®å®šæˆ‘ä»¬å¸Œæœ›ä½¿ç”¨çš„ LLMã€‚

# 4. å®šä¹‰æ‚¨è¦ä½¿ç”¨çš„æ¨¡å‹

Langchain æä¾›äº†å„ç§æ¨¡å‹å’Œé›†æˆï¼ŒåŒ…æ‹¬ OpenAI çš„ GPT å’Œ Huggingface ç­‰ã€‚å¦‚æœæˆ‘ä»¬å†³å®šä½¿ç”¨ OpenAI çš„ GPT ä½œä¸ºæˆ‘ä»¬çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œç¬¬ä¸€æ­¥æ˜¯å®šä¹‰æˆ‘ä»¬çš„ API å¯†é’¥ã€‚ç›®å‰ï¼ŒOpenAI æä¾›äº†ä¸€äº›å…è´¹çš„ä½¿ç”¨é¢åº¦ï¼Œä½†ä¸€æ—¦æˆ‘ä»¬è¶…è¿‡æ¯æœˆçš„ä»¤ç‰Œæ•°é‡ï¼Œæˆ‘ä»¬å°†éœ€è¦åˆ‡æ¢åˆ°ä»˜è´¹è´¦æˆ·ã€‚

å¦‚æœæˆ‘ä»¬åƒä½¿ç”¨ Google ä¸€æ ·ç”¨ GPT æ¥å›ç­”ç®€çŸ­çš„é—®é¢˜ï¼Œæˆæœ¬ä¼šç›¸å¯¹è¾ƒä½ã€‚ç„¶è€Œï¼Œå¦‚æœæˆ‘ä»¬ä½¿ç”¨ GPT æ¥å›ç­”éœ€è¦æä¾›å¤§é‡èƒŒæ™¯ä¿¡æ¯çš„é—®é¢˜ï¼Œä¾‹å¦‚ä¸ªäººæ•°æ®ï¼ŒæŸ¥è¯¢å¾ˆå¿«å°±ä¼šç´¯ç§¯æˆåƒä¸Šä¸‡çš„ä»¤ç‰Œã€‚è¿™ä¼šæ˜¾è‘—å¢åŠ æˆæœ¬ã€‚ä½†ä¸ç”¨æ‹…å¿ƒï¼Œä½ å¯ä»¥è®¾ç½®ä¸€ä¸ªæˆæœ¬é™åˆ¶ã€‚

**ä»€ä¹ˆæ˜¯ä»¤ç‰Œï¼Ÿ**

ç®€è€Œè¨€ä¹‹ï¼Œä»¤ç‰ŒåŸºæœ¬ä¸Šæ˜¯ä¸€ä¸ªå•è¯æˆ–ä¸€ç»„å•è¯ã€‚ç„¶è€Œï¼Œåœ¨è‹±è¯­ä¸­ï¼Œå•è¯å¯ä»¥æœ‰ä¸åŒçš„å½¢å¼ï¼Œæ¯”å¦‚åŠ¨è¯æ—¶æ€ã€å¤æ•°æˆ–å¤åˆè¯ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨å­è¯ä»¤ç‰ŒåŒ–ï¼Œå®ƒå°†ä¸€ä¸ªå•è¯æ‹†åˆ†æˆæ›´å°çš„éƒ¨åˆ†ï¼Œå¦‚è¯æ ¹ã€å‰ç¼€ã€åç¼€å’Œå…¶ä»–è¯­è¨€å­¦å…ƒç´ ã€‚ä¾‹å¦‚ï¼Œå•è¯â€œtiresomeâ€å¯ä»¥æ‹†åˆ†ä¸ºâ€œtireâ€å’Œâ€œsomeâ€ï¼Œè€Œâ€œtiredâ€å¯ä»¥åˆ†ä¸ºâ€œtireâ€å’Œâ€œdâ€ã€‚é€šè¿‡è¿™æ ·åšï¼Œæˆ‘ä»¬å¯ä»¥è¯†åˆ«å‡ºâ€œtiresomeâ€å’Œâ€œtiredâ€å…±äº«ç›¸åŒçš„è¯æ ¹ï¼Œå¹¶å…·æœ‰ç±»ä¼¼çš„è¯æºã€‚ï¼ˆWang, 2023ï¼‰

OpenAI åœ¨å…¶ç½‘ç«™ä¸Šæä¾›äº†ä¸€ä¸ªä»¤ç‰Œè®¡ç®—å™¨ï¼Œè®©ä½ äº†è§£ä»€ä¹ˆæ˜¯ä»¤ç‰Œã€‚æ ¹æ® OpenAI çš„è¯´æ³•ï¼Œä¸€ä¸ªä»¤ç‰Œé€šå¸¸å¯¹åº”äºå¤§çº¦ 4 ä¸ªå¸¸è§è‹±æ–‡å­—ç¬¦ã€‚è¿™å¤§çº¦ç›¸å½“äº Â¾ ä¸ªå•è¯ï¼ˆå› æ­¤ 100 ä¸ªä»¤ç‰Œ â‰ˆ 75 ä¸ªå•è¯ï¼‰ã€‚ä½ å¯ä»¥åœ¨ [OpenAI ç½‘ç«™ä¸Šçš„ä»¤ç‰Œè®¡ç®—å™¨](https://platform.openai.com/tokenizer) æ‰¾åˆ°ä¸€ä¸ªåº”ç”¨ï¼Œå¸®åŠ©ä½ äº†è§£ä»€ä¹ˆå®é™…ä¸Šç®—ä½œä¸€ä¸ªä»¤ç‰Œã€‚

> **è®¾ç½®ä½¿ç”¨é™åˆ¶**
> 
> å¦‚æœä½ æ‹…å¿ƒæˆæœ¬ï¼Œä½ å¯ä»¥åœ¨ OpenAI ç”¨æˆ·é—¨æˆ·ä¸­æ‰¾åˆ°ä¸€ä¸ªé€‰é¡¹æ¥é™åˆ¶æ¯æœˆè´¹ç”¨ã€‚

ä½ å¯ä»¥åœ¨ OpenAI çš„ç”¨æˆ·è´¦æˆ·ä¸­æ‰¾åˆ° API å¯†é’¥ã€‚æœ€ç®€å•çš„æ–¹æ³•æ˜¯ç”¨ Google æœç´¢â€œOpenAI API keyâ€ã€‚è¿™ä¼šç›´æ¥å¸¦ä½ åˆ°è®¾ç½®é¡µé¢ï¼Œä»¥åˆ›å»ºæ–°çš„å¯†é’¥ã€‚

è¦åœ¨ Python ä¸­ä½¿ç”¨ï¼Œä½ å¿…é¡»å°†å¯†é’¥ä¿å­˜ä¸ºåä¸º â€œOPENAI_API_KEYâ€ çš„æ–°ç¯å¢ƒå˜é‡ï¼š

```py
import os
os.environ["OPENAI_API_KEY"] = "testapikey213412"
```

å½“ä½ é€‰æ‹©è¦ä½¿ç”¨çš„è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ—¶ï¼Œå¯ä»¥é¢„è®¾ä¸€äº›å‚æ•°ã€‚ [OpenAI Playground](https://platform.openai.com/playground) è®©ä½ åœ¨å†³å®šä½¿ç”¨ä»€ä¹ˆè®¾ç½®ä¹‹å‰ï¼Œå¯ä»¥å…ˆè¯•éªŒä¸€ä¸‹ä¸åŒçš„å‚æ•°ã€‚

åœ¨ Playground WebUI çš„å³ä¾§ï¼Œä½ ä¼šæ‰¾åˆ° OpenAI æä¾›çš„å‡ ä¸ªå‚æ•°ï¼Œè¿™äº›å‚æ•°å…è®¸æˆ‘ä»¬å½±å“ LLM çš„è¾“å‡ºã€‚ä¸¤ä¸ªå€¼å¾—æ¢ç´¢çš„å‚æ•°æ˜¯æ¨¡å‹é€‰æ‹©å’Œæ¸©åº¦ã€‚

ä½ å¯ä»¥ä»å„ç§ä¸åŒçš„æ¨¡å‹ä¸­è¿›è¡Œé€‰æ‹©ã€‚ç›®å‰ï¼ŒText-davinci-003 æ¨¡å‹æ˜¯æœ€å¤§çš„ã€æœ€å¼ºå¤§çš„ã€‚å¦ä¸€æ–¹é¢ï¼Œåƒ Text-ada-001 è¿™æ ·çš„æ¨¡å‹æ›´å°ã€æ›´å¿«ã€æˆæœ¬æ›´ä½ã€‚

ä¸‹é¢ï¼Œä½ å¯ä»¥çœ‹åˆ°[OpenAI å®šä»·](https://openai.com/pricing)åˆ—è¡¨çš„æ€»ç»“ã€‚Ada çš„è´¹ç”¨ä½äºæœ€å¼ºå¤§çš„æ¨¡å‹ Davinciã€‚å› æ­¤ï¼Œå¦‚æœ Ada çš„è¡¨ç°æ»¡è¶³æˆ‘ä»¬çš„éœ€æ±‚ï¼Œæˆ‘ä»¬ä¸ä»…å¯ä»¥èŠ‚çœèµ„é‡‘ï¼Œè¿˜èƒ½å®ç°æ›´çŸ­çš„å“åº”æ—¶é—´ã€‚

ä½ å¯ä»¥é¦–å…ˆä½¿ç”¨ Davinciï¼Œç„¶åè¯„ä¼°æ˜¯å¦å¯ä»¥ä½¿ç”¨ Ada è·å¾—è¶³å¤Ÿå¥½çš„ç»“æœã€‚

æ‰€ä»¥è®©æˆ‘ä»¬åœ¨ Jupyter Notebook ä¸­è¯•è¯•å§ã€‚æˆ‘ä»¬æ­£åœ¨ä½¿ç”¨ langchain è¿æ¥åˆ° GPTã€‚

```py
from langchain.llms import OpenAI

llm = OpenAI(temperature=0.7)
```

å¦‚æœä½ æƒ³æŸ¥çœ‹åŒ…å«æ‰€æœ‰å±æ€§çš„åˆ—è¡¨ï¼Œè¯·ä½¿ç”¨ __dict__ï¼š

```py
llm.__dict__
```

![](img/a684aa79cd7cce2dbd32c1d847c4ea65.png)

å¦‚æœæˆ‘ä»¬æ²¡æœ‰æŒ‡å®šç‰¹å®šçš„æ¨¡å‹ï¼Œlangchain è¿æ¥å™¨é»˜è®¤ä½¿ç”¨â€œtext-davinci-003â€ã€‚

ç°åœ¨ï¼Œæˆ‘ä»¬å¯ä»¥ç›´æ¥åœ¨ Python ä¸­è°ƒç”¨æ¨¡å‹ã€‚åªéœ€è°ƒç”¨ llm å‡½æ•°å¹¶å°†æç¤ºä½œä¸ºè¾“å…¥æä¾›ã€‚

![](img/3b12ecf96f1efa3c6f40b112aa2b5636.png)

ç°åœ¨ä½ å¯ä»¥å‘ GPT æé—®ä»»ä½•å…³äºå¸¸è§äººç±»çŸ¥è¯†çš„é—®é¢˜ã€‚

![](img/f1e05a7a00e093c974d44fd9f31b6716.png)

GPT åªèƒ½æä¾›æœ‰é™çš„ä¿¡æ¯ï¼Œå…³äºå…¶è®­ç»ƒæ•°æ®ä¸­æœªåŒ…å«çš„ä¸»é¢˜ã€‚è¿™åŒ…æ‹¬ä¸å…¬å¼€çš„å…·ä½“ç»†èŠ‚æˆ–è®­ç»ƒæ•°æ®æœ€åæ›´æ–°åå‘ç”Ÿçš„äº‹ä»¶ã€‚

![](img/0fcfb9a7848e3a8be48b599bae9d827e.png)

**é‚£ä¹ˆï¼Œæˆ‘ä»¬å¦‚ä½•ç¡®ä¿æ¨¡å‹èƒ½å¤Ÿå›ç­”æœ‰å…³å½“å‰äº‹ä»¶çš„é—®é¢˜å‘¢ï¼Ÿ**

å¦‚å‰æ‰€è¿°ï¼Œè¿™é‡Œæœ‰ä¸€ç§æ–¹æ³•å¯ä»¥åšåˆ°è¿™ä¸€ç‚¹ã€‚æˆ‘ä»¬éœ€è¦åœ¨æç¤ºä¸­æä¾›æ¨¡å‹æ‰€éœ€çš„ä¿¡æ¯ã€‚

ä¸ºäº†å›ç­”æœ‰å…³è‹±å›½ç°ä»»é¦–ç›¸çš„é—®é¢˜ï¼Œæˆ‘ä½¿ç”¨äº†æ¥è‡ªç»´åŸºç™¾ç§‘æ–‡ç« â€œè‹±å›½é¦–ç›¸â€çš„ä¿¡æ¯ã€‚ä¸ºäº†æ€»ç»“è¿™ä¸ªè¿‡ç¨‹ï¼Œæˆ‘ä»¬æ­£åœ¨ï¼š

+   åŠ è½½æ–‡ç« 

+   å°†æ–‡æœ¬æ‹†åˆ†æˆæ–‡æœ¬å—

+   è®¡ç®—æ–‡æœ¬å—çš„åµŒå…¥

+   è®¡ç®—æ‰€æœ‰æ–‡æœ¬å—ä¸ç”¨æˆ·é—®é¢˜çš„ç›¸ä¼¼åº¦

```py
import requests
from bs4 import BeautifulSoup
from langchain.text_splitter import RecursiveCharacterTextSplitter
import numpy as np
from numpy.linalg import norm
import pandas as pd
import openai

####################################################################
# load documents
####################################################################
# URL of the Wikipedia page to scrape
url = 'https://en.wikipedia.org/wiki/Prime_Minister_of_the_United_Kingdom'

# Send a GET request to the URL
response = requests.get(url)

# Parse the HTML content using BeautifulSoup
soup = BeautifulSoup(response.content, 'html.parser')

# Find all the text on the page
text = soup.get_text()

####################################################################
# split text
####################################################################
text_splitter = RecursiveCharacterTextSplitter(
    # Set a really small chunk size, just to show.
    chunk_size = 100,
    chunk_overlap  = 20,
    length_function = len,
)

texts = text_splitter.create_documents([text])

####################################################################
# calculate embeddings
####################################################################
# create new list with all text chunks
text_chunks=[]

for text in texts:
    text_chunks.append(text.page_content)

df = pd.DataFrame({'text_chunks': text_chunks})

# get embeddings from text-embedding-ada model
def get_embedding(text, model="text-embedding-ada-002"):
   text = text.replace("\n", " ")
   return openai.Embedding.create(input = [text], model=model)['data'][0]['embedding']

df['ada_embedding'] = df.text_chunks.apply(lambda x: get_embedding(x, model='text-embedding-ada-002'))

####################################################################
# calculate similarities to the user's question
####################################################################
# calcuate the embeddings for the user's question
users_question = "Who is the current Prime Minister of the UK?"
question_embedding = get_embedding(text=users_question, model="text-embedding-ada-002")
```

ç°åœ¨æˆ‘ä»¬å°è¯•æ‰¾åˆ°ä¸ç”¨æˆ·é—®é¢˜æœ€ç›¸ä¼¼çš„æ–‡æœ¬å—ï¼š

```py
from langchain import PromptTemplate
from langchain.llms import OpenAI

# calcuate the embeddings for the user's question
users_question = "Who is the current Prime Minister of the UK?"
question_embedding = get_embedding(text=users_question, model="text-embedding-ada-002")

# create a list to store the calculated cosine similarity
cos_sim = []

for index, row in df.iterrows():
   A = row.ada_embedding
   B = question_embedding

   # calculate the cosine similiarity
   cosine = np.dot(A,B)/(norm(A)*norm(B))

   cos_sim.append(cosine)

df["cos_sim"] = cos_sim
df.sort_values(by=["cos_sim"], ascending=False)
```

![](img/ca95025e46092c6aac7cbbb536db9a81.png)

æ–‡æœ¬å—çœ‹èµ·æ¥ç›¸å½“æ··ä¹±ï¼Œä½†è®©æˆ‘ä»¬è¯•è¯•ï¼Œçœ‹ GPT æ˜¯å¦è¶³å¤Ÿèªæ˜æ¥å¤„ç†å®ƒã€‚

ç°åœ¨æˆ‘ä»¬å·²ç»è¯†åˆ«å‡ºå¯èƒ½åŒ…å«ç›¸å…³ä¿¡æ¯çš„æ–‡æœ¬æ®µè½ï¼Œæˆ‘ä»¬å¯ä»¥æµ‹è¯•æˆ‘ä»¬çš„æ¨¡å‹æ˜¯å¦èƒ½å¤Ÿå›ç­”è¿™ä¸ªé—®é¢˜ã€‚ä¸ºäº†å®ç°è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬å¿…é¡»ä»¥ä¸€ç§æ¸…æ™°åœ°ä¼ è¾¾æˆ‘ä»¬æœŸæœ›ä»»åŠ¡çš„æ–¹å¼æ„å»ºæˆ‘ä»¬çš„æç¤ºã€‚

# 5\. å®šä¹‰æˆ‘ä»¬çš„æç¤ºæ¨¡æ¿

ç°åœ¨æˆ‘ä»¬æœ‰äº†åŒ…å«æˆ‘ä»¬æ‰€å¯»æ‰¾çš„ä¿¡æ¯çš„æ–‡æœ¬ç‰‡æ®µï¼Œæˆ‘ä»¬éœ€è¦æ„å»ºä¸€ä¸ªæç¤ºã€‚åœ¨æç¤ºä¸­æˆ‘ä»¬è¿˜æŒ‡å®šæ¨¡å‹å›ç­”é—®é¢˜æ‰€éœ€çš„æ¨¡å¼ã€‚å½“æˆ‘ä»¬å®šä¹‰æ¨¡å¼æ—¶ï¼Œæˆ‘ä»¬åœ¨æŒ‡å®š LLM ç”Ÿæˆç­”æ¡ˆçš„æœŸæœ›è¡Œä¸ºé£æ ¼ã€‚

LLM å¯ä»¥ç”¨äºå„ç§ä»»åŠ¡ï¼Œä»¥ä¸‹æ˜¯ä¸€äº›å¹¿æ³›å¯èƒ½æ€§çš„ä¾‹å­ï¼š

+   **æ€»ç»“ï¼š** â€œå°†ä»¥ä¸‹æ–‡æœ¬æ€»ç»“æˆ 3 æ®µï¼Œä»¥ä¾›é«˜ç®¡å‚è€ƒï¼š[TEXT]â€

+   **çŸ¥è¯†æå–ï¼š** â€œåŸºäºè¿™ç¯‡æ–‡ç« ï¼š[TEXT]ï¼Œäººä»¬åœ¨è´­ä¹°æˆ¿å±‹ä¹‹å‰åº”è¯¥è€ƒè™‘å“ªäº›é—®é¢˜ï¼Ÿâ€

+   **æ’°å†™å†…å®¹ï¼ˆä¾‹å¦‚é‚®ä»¶ã€æ¶ˆæ¯ã€ä»£ç ï¼‰ï¼š** å†™ä¸€å°é‚®ä»¶ç»™ç®€ï¼Œè¯¢é—®æˆ‘ä»¬é¡¹ç›®æ–‡æ¡£çš„æœ€æ–°æƒ…å†µã€‚ä½¿ç”¨éæ­£å¼ã€å‹å¥½çš„è¯­æ°”ã€‚â€

+   **è¯­æ³•å’Œé£æ ¼æ”¹è¿›ï¼š** â€œå°†å…¶æ”¹ä¸ºæ ‡å‡†è‹±è¯­ï¼Œå¹¶å°†è¯­æ°”æ”¹ä¸ºæ›´å‹å¥½çš„ï¼š [TEXT]â€

+   **åˆ†ç±»ï¼š** â€œå°†æ¯æ¡æ¶ˆæ¯åˆ†ç±»ä¸ºæ”¯æŒç¥¨æ®çš„ç±»å‹ï¼š[TEXT]â€

åœ¨æˆ‘ä»¬çš„ä¾‹å­ä¸­ï¼Œæˆ‘ä»¬å¸Œæœ›å®ç°ä¸€ä¸ªä»ç»´åŸºç™¾ç§‘æå–æ•°æ®å¹¶åƒèŠå¤©æœºå™¨äººä¸€æ ·ä¸ç”¨æˆ·äº’åŠ¨çš„è§£å†³æ–¹æ¡ˆã€‚æˆ‘ä»¬å¸Œæœ›å®ƒèƒ½å¤Ÿåƒä¸€ä¸ªç§¯æã€ä¹äºåŠ©äººçš„å¸®åŠ©å°ä¸“å®¶ä¸€æ ·å›ç­”é—®é¢˜ã€‚

ä¸ºäº†å¼•å¯¼ LLM å‘æ­£ç¡®æ–¹å‘å‘å±•ï¼Œæˆ‘åœ¨æç¤ºä¸­æ·»åŠ äº†ä»¥ä¸‹æŒ‡ä»¤ï¼š

**â€œä½ æ˜¯ä¸€ä¸ªå–œæ¬¢å¸®åŠ©åˆ«äººçš„èŠå¤©æœºå™¨äººï¼ä»…ä½¿ç”¨æä¾›çš„ä¸Šä¸‹æ–‡å›ç­”ä»¥ä¸‹é—®é¢˜ã€‚å¦‚æœä½ ä¸ç¡®å®šä¸”ç­”æ¡ˆåœ¨ä¸Šä¸‹æ–‡ä¸­æ²¡æœ‰æ˜ç¡®ç»™å‡ºï¼Œè¯·è¯´â€˜å¯¹ä¸èµ·ï¼Œæˆ‘ä¸çŸ¥é“å¦‚ä½•å¸®åŠ©ä½ ã€‚â€™â€**

é€šè¿‡è¿™æ ·åšï¼Œæˆ‘è®¾å®šäº†ä¸€ä¸ªé™åˆ¶ï¼Œåªå…è®¸ GPT åˆ©ç”¨æˆ‘ä»¬æ•°æ®åº“ä¸­çš„ä¿¡æ¯ã€‚è¿™ç§é™åˆ¶ä½¿æˆ‘ä»¬èƒ½å¤Ÿæä¾›èŠå¤©æœºå™¨äººç”Ÿæˆå›åº”æ—¶æ‰€ä¾èµ–çš„æ¥æºï¼Œè¿™å¯¹è¿½è¸ªæ¥æºå’Œå»ºç«‹ä¿¡ä»»è‡³å…³é‡è¦ã€‚æ­¤å¤–ï¼Œå®ƒæœ‰åŠ©äºè§£å†³ç”Ÿæˆä¸å¯é ä¿¡æ¯çš„é—®é¢˜ï¼Œå¹¶ä½¿æˆ‘ä»¬èƒ½å¤Ÿæä¾›å¯ç”¨äºå…¬å¸å†³ç­–çš„ç­”æ¡ˆã€‚

ä½œä¸ºä¸Šä¸‹æ–‡ï¼Œæˆ‘ä»…ä½¿ç”¨ä¸é—®é¢˜æœ€ç›¸ä¼¼çš„å‰ 50 ä¸ªæ–‡æœ¬å—ã€‚æ›´å¤§çš„æ–‡æœ¬å—å¯èƒ½ä¼šæ›´å¥½ï¼Œå› ä¸ºæˆ‘ä»¬é€šå¸¸å¯ä»¥ç”¨ä¸€åˆ°ä¸¤ä¸ªæ–‡æœ¬æ®µè½å›ç­”å¤§å¤šæ•°é—®é¢˜ã€‚ä½†æˆ‘å°†æŠŠæ‰¾åˆ°æœ€ä½³å¤§å°çš„ä»»åŠ¡ç•™ç»™ä½ æ¥å®Œæˆã€‚

```py
from langchain import PromptTemplate
from langchain.llms import OpenAI
import openai
import requests
from bs4 import BeautifulSoup
from langchain.text_splitter import RecursiveCharacterTextSplitter
import numpy as np
from numpy.linalg import norm
import pandas as pd
import openai

####################################################################
# load documents
####################################################################
# URL of the Wikipedia page to scrape
url = 'https://en.wikipedia.org/wiki/Prime_Minister_of_the_United_Kingdom'

# Send a GET request to the URL
response = requests.get(url)

# Parse the HTML content using BeautifulSoup
soup = BeautifulSoup(response.content, 'html.parser')

# Find all the text on the page
text = soup.get_text()

####################################################################
# split text
####################################################################
text_splitter = RecursiveCharacterTextSplitter(
    # Set a really small chunk size, just to show.
    chunk_size = 100,
    chunk_overlap  = 20,
    length_function = len,
)

texts = text_splitter.create_documents([text])

####################################################################
# calculate embeddings
####################################################################
# create new list with all text chunks
text_chunks=[]

for text in texts:
    text_chunks.append(text.page_content)

df = pd.DataFrame({'text_chunks': text_chunks})

# get embeddings from text-embedding-ada model
def get_embedding(text, model="text-embedding-ada-002"):
   text = text.replace("\n", " ")
   return openai.Embedding.create(input = [text], model=model)['data'][0]['embedding']

df['ada_embedding'] = df.text_chunks.apply(lambda x: get_embedding(x, model='text-embedding-ada-002'))

####################################################################
# calculate similarities to the user's question
####################################################################
# calcuate the embeddings for the user's question
users_question = "Who is the current Prime Minister of the UK?"
question_embedding = get_embedding(text=users_question, model="text-embedding-ada-002")

# create a list to store the calculated cosine similarity
cos_sim = []

for index, row in df.iterrows():
   A = row.ada_embedding
   B = question_embedding

   # calculate the cosine similiarity
   cosine = np.dot(A,B)/(norm(A)*norm(B))

   cos_sim.append(cosine)

df["cos_sim"] = cos_sim
df.sort_values(by=["cos_sim"], ascending=False)

####################################################################
# build a suitable prompt and send it
####################################################################
# define the LLM you want to use
llm = OpenAI(temperature=1)

# define the context for the prompt by joining the most relevant text chunks
context = ""

for index, row in df[0:50].iterrows():
    context = context + " " + row.text_chunks

# define the prompt template
template = """
You are a chat bot who loves to help people! Given the following context sections, answer the
question using only the given context. If you are unsure and the answer is not
explicitly writting in the documentation, say "Sorry, I don't know how to help with that."

Context sections:
{context}

Question:
{users_question}

Answer:
"""

prompt = PromptTemplate(template=template, input_variables=["context", "users_question"])

# fill the prompt template
prompt_text = prompt.format(context = context, users_question = users_question)
llm(prompt_text)
```

é€šè¿‡ä½¿ç”¨é‚£ä¸ªç‰¹å®šæ¨¡æ¿ï¼Œæˆ‘å°†ä¸Šä¸‹æ–‡å’Œç”¨æˆ·çš„é—®é¢˜éƒ½çº³å…¥äº†æˆ‘ä»¬çš„æç¤ºä¸­ã€‚ç”Ÿæˆçš„å›åº”å¦‚ä¸‹ï¼š

![](img/90849344b5dd20ee37b89e46d4e84319.png)

å‡ºä¹æ„æ–™çš„æ˜¯ï¼Œå³ä½¿æ˜¯è¿™ä¸ªç®€å•çš„å®ç°ä¹Ÿä¼¼ä¹äº§ç”Ÿäº†ä¸€äº›ä»¤äººæ»¡æ„çš„ç»“æœã€‚è®©æˆ‘ä»¬ç»§ç»­å‘ç³»ç»Ÿæå‡ºæ›´å¤šå…³äºè‹±å›½é¦–ç›¸çš„é—®é¢˜ã€‚æˆ‘å°†ä¿æŒä¸€åˆ‡ä¸å˜ï¼Œåªæ›¿æ¢ç”¨æˆ·çš„é—®é¢˜ï¼š

```py
users_question = "Who was the first Prime Minister of the UK?"
```

![](img/daf31fbecc5113e76cc4f0649d162f63.png)

å®ƒä¼¼ä¹åœ¨æŸç§ç¨‹åº¦ä¸Šæ­£åœ¨è¿è¡Œã€‚ç„¶è€Œï¼Œæˆ‘ä»¬ç°åœ¨çš„ç›®æ ‡æ˜¯å°†è¿™ä¸ªç¼“æ…¢çš„è¿‡ç¨‹è½¬å˜ä¸ºä¸€ä¸ªå¼ºå¤§ä¸”é«˜æ•ˆçš„è¿‡ç¨‹ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªç´¢å¼•æ­¥éª¤ï¼Œåœ¨å‘é‡å­˜å‚¨ä¸­å­˜å‚¨æˆ‘ä»¬çš„åµŒå…¥å’Œç´¢å¼•ã€‚è¿™å°†æé«˜æ•´ä½“æ€§èƒ½å¹¶å‡å°‘å“åº”æ—¶é—´ã€‚

# 6\. åˆ›å»ºå‘é‡å­˜å‚¨ï¼ˆå‘é‡æ•°æ®åº“ï¼‰

å‘é‡å­˜å‚¨æ˜¯ä¸€ç§ä¼˜åŒ–ç”¨äºå­˜å‚¨å’Œæ£€ç´¢å¯ä»¥è¡¨ç¤ºä¸ºå‘é‡çš„å¤§é‡æ•°æ®çš„æ•°æ®å­˜å‚¨ç±»å‹ã€‚è¿™äº›ç±»å‹çš„æ•°æ®åº“å…è®¸æ ¹æ®å„ç§æ ‡å‡†ï¼ˆå¦‚ç›¸ä¼¼æ€§åº¦é‡æˆ–å…¶ä»–æ•°å­¦æ“ä½œï¼‰é«˜æ•ˆæŸ¥è¯¢å’Œæ£€ç´¢æ•°æ®çš„å­é›†ã€‚

å°†æˆ‘ä»¬çš„æ–‡æœ¬æ•°æ®è½¬æ¢ä¸ºå‘é‡æ˜¯ç¬¬ä¸€æ­¥ï¼Œä½†è¿™å¯¹äºæˆ‘ä»¬çš„éœ€æ±‚è¿˜ä¸å¤Ÿã€‚å¦‚æœæˆ‘ä»¬å°†å‘é‡å­˜å‚¨åœ¨æ•°æ®æ¡†ä¸­ï¼Œå¹¶åœ¨æ¯æ¬¡æ”¶åˆ°æŸ¥è¯¢æ—¶é€æ­¥æœç´¢å•è¯ä¹‹é—´çš„ç›¸ä¼¼æ€§ï¼Œé‚£ä¹ˆæ•´ä¸ªè¿‡ç¨‹å°†ä¼šéå¸¸ç¼“æ…¢ã€‚

ä¸ºäº†é«˜æ•ˆåœ°æœç´¢æˆ‘ä»¬çš„åµŒå…¥ï¼Œæˆ‘ä»¬éœ€è¦å¯¹å®ƒä»¬è¿›è¡Œç´¢å¼•ã€‚ç´¢å¼•æ˜¯å‘é‡æ•°æ®åº“çš„ç¬¬äºŒä¸ªé‡è¦ç»„æˆéƒ¨åˆ†ã€‚ç´¢å¼•æä¾›äº†ä¸€ç§å°†æŸ¥è¯¢æ˜ å°„åˆ°å‘é‡å­˜å‚¨åº“ä¸­æœ€ç›¸å…³çš„æ–‡æ¡£æˆ–é¡¹ç›®çš„æ–¹æ³•ï¼Œè€Œæ— éœ€è®¡ç®—æ¯ä¸ªæŸ¥è¯¢ä¸æ¯ä¸ªæ–‡æ¡£ä¹‹é—´çš„ç›¸ä¼¼æ€§ã€‚

è¿‘å¹´æ¥ï¼Œå·²ç»å‘å¸ƒäº†è®¸å¤šå‘é‡å­˜å‚¨åº“ã€‚å°¤å…¶åœ¨ LLM é¢†åŸŸï¼Œå¯¹å‘é‡å­˜å‚¨åº“çš„å…³æ³¨æ¿€å¢ï¼š

![](img/105814f00b193fc06addfb2a85f368b1.png)

è¿‘å¹´æ¥å‘å¸ƒçš„å‘é‡å­˜å‚¨åº“ â€” å›¾ç‰‡æ¥è‡ªä½œè€…

ç°åœ¨æˆ‘ä»¬æ¥é€‰æ‹©ä¸€ä¸ªå¹¶è¯•ç”¨ä¸€ä¸‹æˆ‘ä»¬çš„ç”¨ä¾‹ã€‚ç±»ä¼¼äºæˆ‘ä»¬åœ¨å‰é¢éƒ¨åˆ†æ‰€åšçš„ï¼Œæˆ‘ä»¬å†æ¬¡è®¡ç®—åµŒå…¥å¹¶å°†å…¶å­˜å‚¨åœ¨å‘é‡å­˜å‚¨åº“ä¸­ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬ä½¿ç”¨äº†æ¥è‡ª LangChain å’Œ chroma çš„åˆé€‚æ¨¡å—ä½œä¸ºå‘é‡å­˜å‚¨åº“ã€‚

1.  **æ”¶é›†æˆ‘ä»¬æƒ³è¦ç”¨æ¥å›ç­”ç”¨æˆ·é—®é¢˜çš„æ•°æ®ï¼š**

![](img/f90d9f156966a818815b8c5b4533d1a8.png)

å›¾ç‰‡æ¥è‡ªä½œè€…

```py
import requests
from bs4 import BeautifulSoup
from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.text_splitter import CharacterTextSplitter
from langchain.vectorstores import Chroma
from langchain.document_loaders import TextLoader

# URL of the Wikipedia page to scrape
url = 'https://en.wikipedia.org/wiki/Prime_Minister_of_the_United_Kingdom'

# Send a GET request to the URL
response = requests.get(url)

# Parse the HTML content using BeautifulSoup
soup = BeautifulSoup(response.content, 'html.parser')

# Find all the text on the page
text = soup.get_text()
text = text.replace('\n', '')

# Open a new file called 'output.txt' in write mode and store the file object in a variable
with open('output.txt', 'w', encoding='utf-8') as file:
    # Write the string to the file
    file.write(text)
```

**2\. åŠ è½½æ•°æ®å¹¶å®šä¹‰å¦‚ä½•å°†æ•°æ®æ‹†åˆ†ä¸ºæ–‡æœ¬å—**

![](img/d8ae68a1b459338281e1f039d3ce5ebc.png)

å›¾ç‰‡æ¥è‡ªä½œè€…

```py
from langchain.text_splitter import RecursiveCharacterTextSplitter

# load the document
with open('./output.txt', encoding='utf-8') as f:
    text = f.read()

# define the text splitter
text_splitter = RecursiveCharacterTextSplitter(    
    chunk_size = 500,
    chunk_overlap  = 100,
    length_function = len,
)

texts = text_splitter.create_documents([text])
```

**3\. å®šä¹‰è¦ç”¨æ¥è®¡ç®—æ–‡æœ¬å—åµŒå…¥çš„åµŒå…¥æ¨¡å‹ï¼Œå¹¶å°†å…¶å­˜å‚¨åœ¨å‘é‡å­˜å‚¨åº“ä¸­ï¼ˆè¿™é‡Œä½¿ç”¨ï¼šChromaï¼‰**

![](img/637ae1d82b4e950d0aeb6f96769eb065.png)

å›¾ç‰‡æ¥è‡ªä½œè€…

```py
from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.vectorstores import Chroma

# define the embeddings model
embeddings = OpenAIEmbeddings()

# use the text chunks and the embeddings model to fill our vector store
db = Chroma.from_documents(texts, embeddings)
```

**4\. è®¡ç®—ç”¨æˆ·é—®é¢˜çš„åµŒå…¥ï¼Œæ‰¾åˆ°å‘é‡å­˜å‚¨åº“ä¸­ç›¸ä¼¼çš„æ–‡æœ¬å—ï¼Œå¹¶ä½¿ç”¨å®ƒä»¬æ¥æ„å»ºæˆ‘ä»¬çš„æç¤º**

![](img/730fb905ee3a9f4c5af1ae6d7b5f89cf.png)

å›¾ç‰‡æ¥è‡ªä½œè€…

```py
from langchain.llms import OpenAI
from langchain import PromptTemplate

users_question = "Who is the current Prime Minister of the UK?"

# use our vector store to find similar text chunks
results = db.similarity_search(
    query=user_question,
    n_results=5
)

# define the prompt template
template = """
You are a chat bot who loves to help people! Given the following context sections, answer the
question using only the given context. If you are unsure and the answer is not
explicitly writting in the documentation, say "Sorry, I don't know how to help with that."

Context sections:
{context}

Question:
{users_question}

Answer:
"""

prompt = PromptTemplate(template=template, input_variables=["context", "users_question"])

# fill the prompt template
prompt_text = prompt.format(context = results, users_question = users_question)

# ask the defined LLM
llm(prompt_text)
```

![](img/b31e0fadb9e5b7644a8725ae0a7ccc30.png)

# æ¦‚è¿°

ä¸ºäº†ä½¿æˆ‘ä»¬çš„ LLM èƒ½å¤Ÿåˆ†æå’Œå›ç­”æœ‰å…³æˆ‘ä»¬æ•°æ®çš„é—®é¢˜ï¼Œæˆ‘ä»¬é€šå¸¸ä¸ä¼šå¯¹æ¨¡å‹è¿›è¡Œå¾®è°ƒã€‚ç›¸åï¼Œåœ¨å¾®è°ƒè¿‡ç¨‹ä¸­ï¼Œç›®æ ‡æ˜¯æé«˜æ¨¡å‹æœ‰æ•ˆå“åº”ç‰¹å®šä»»åŠ¡çš„èƒ½åŠ›ï¼Œè€Œä¸æ˜¯æ•™å®ƒæ–°çš„ä¿¡æ¯ã€‚

åœ¨ Alpaca 7B çš„æ¡ˆä¾‹ä¸­ï¼ŒLLMï¼ˆLLaMAï¼‰ç»è¿‡å¾®è°ƒï¼Œä»¥è¡¨ç°å’Œäº’åŠ¨åƒä¸€ä¸ªèŠå¤©æœºå™¨äººã€‚é‡ç‚¹æ˜¯å®Œå–„æ¨¡å‹çš„å›åº”ï¼Œè€Œä¸æ˜¯æ•™å®ƒå®Œå…¨æ–°çš„ä¿¡æ¯ã€‚

ä¸ºäº†èƒ½å¤Ÿå›ç­”å…³äºæˆ‘ä»¬è‡ªå·±æ•°æ®çš„é—®é¢˜ï¼Œæˆ‘ä»¬ä½¿ç”¨ä¸Šä¸‹æ–‡æ³¨å…¥æ–¹æ³•ã€‚åˆ›å»ºä¸€ä¸ªå…·æœ‰ä¸Šä¸‹æ–‡æ³¨å…¥çš„ LLM åº”ç”¨ç¨‹åºæ˜¯ä¸€ä¸ªç›¸å¯¹ç®€å•çš„è¿‡ç¨‹ã€‚ä¸»è¦æŒ‘æˆ˜åœ¨äºç»„ç»‡å’Œæ ¼å¼åŒ–è¦å­˜å‚¨åœ¨å‘é‡æ•°æ®åº“ä¸­çš„æ•°æ®ã€‚è¿™ä¸€æ­¥å¯¹äºé«˜æ•ˆæ£€ç´¢ä¸Šä¸‹æ–‡ç›¸ä¼¼ä¿¡æ¯å¹¶ç¡®ä¿å¯é ç»“æœè‡³å…³é‡è¦ã€‚

æœ¬æ–‡çš„ç›®æ ‡æ˜¯å±•ç¤ºä½¿ç”¨åµŒå…¥æ¨¡å‹ã€å‘é‡å­˜å‚¨å’Œ LLMs å¤„ç†ç”¨æˆ·æŸ¥è¯¢çš„æç®€æ–¹æ³•ã€‚å®ƒå±•ç¤ºäº†è¿™äº›æŠ€æœ¯å¦‚ä½•ååŒå·¥ä½œï¼Œå³ä½¿é¢å¯¹ä¸æ–­å˜åŒ–çš„äº‹å®ï¼Œä¹Ÿèƒ½æä¾›ç›¸å…³ä¸”å‡†ç¡®çš„ç­”æ¡ˆã€‚

> *å–œæ¬¢è¿™ä¸ªæ•…äº‹å—ï¼Ÿ*

+   [*å…è´¹è®¢é˜…*](https://dmnkplzr.medium.com/subscribe) *ä»¥ä¾¿åœ¨æˆ‘å‘å¸ƒæ–°æ•…äº‹æ—¶æ”¶åˆ°é€šçŸ¥ã€‚*

+   *æƒ³æ¯æœˆé˜…è¯»è¶…è¿‡ 3 ç¯‡å…è´¹æ•…äº‹ï¼Ÿâ€” æˆä¸º Medium ä¼šå‘˜ï¼Œæ¯æœˆ 5 ç¾å…ƒã€‚æ‚¨å¯ä»¥é€šè¿‡ä½¿ç”¨æˆ‘çš„* [*æ¨èé“¾æ¥*](https://dmnkplzr.medium.com/membership) *æ¥æ”¯æŒæˆ‘ã€‚æ‚¨ä¸ä¼šå¢åŠ é¢å¤–è´¹ç”¨ï¼Œä½†æˆ‘å°†è·å¾—ä½£é‡‘ã€‚*

> *éšæ—¶é€šè¿‡* [*LinkedIn*](https://www.linkedin.com/in/polzerdo/) *è”ç³»æˆ‘ï¼*

## å‚è€ƒæ–‡çŒ®

AssemblyAI (å¯¼æ¼”). (2022 å¹´ 1 æœˆ 5 æ—¥). å®Œæ•´çš„è¯åµŒå…¥æ¦‚è¿°. [`www.youtube.com/watch?v=5MaWmXwxFNQ`](https://www.youtube.com/watch?v=5MaWmXwxFNQ)

Grootendorst, M. (2021 å¹´ 12 æœˆ 7 æ—¥). æ•°æ®ç§‘å­¦ä¸­çš„ 9 ç§è·ç¦»åº¦é‡. Medium. `towardsdatascience.com/9-distance-measures-in-data-science-918109d069fa`

Langchain. (2023). æ¬¢è¿ä½¿ç”¨ LangChain â€” ğŸ¦œğŸ”— LangChain 0.0.189\. [`python.langchain.com/en/latest/index.html`](https://python.langchain.com/en/latest/index.html)

Nelson, P. (2023). æœç´¢ä¸éç»“æ„åŒ–æ•°æ®åˆ†æè¶‹åŠ¿ |

Accenture. æœç´¢ä¸å†…å®¹åˆ†æåšå®¢. [`www.accenture.com/us-en/blogs/search-and-content-analytics-blog/search-unstructured-data-analytics-trends`](https://www.accenture.com/us-en/blogs/search-and-content-analytics-blog/search-unstructured-data-analytics-trends)

OpenAI. (2022). ä»‹ç»æ–‡æœ¬å’Œä»£ç åµŒå…¥. [`openai.com/blog/introducing-text-and-code-embeddings`](https://openai.com/blog/introducing-text-and-code-embeddings)

OpenAI (å¯¼æ¼”). (2023 å¹´ 3 æœˆ 14 æ—¥). ä½ å¯ä»¥ç”¨ GPT-4 åšä»€ä¹ˆï¼Ÿ [`www.youtube.com/watch?v=oc6RV5c1yd0`](https://www.youtube.com/watch?v=oc6RV5c1yd0)

Porsche AG. (2023 å¹´ 5 æœˆ 17 æ—¥). ChatGPT ä¸ä¼ä¸šçŸ¥è¯†ï¼šâ€œæˆ‘å¦‚ä½•ä¸ºæˆ‘çš„ä¸šåŠ¡éƒ¨é—¨åˆ›å»ºä¸€ä¸ªèŠå¤©æœºå™¨äººï¼Ÿâ€ #NextLevelGermanEngineering. [`medium.com/next-level-german-engineering/chatgpt-enterprise-knowledge-how-can-i-create-a-chatbot-for-my-business-unit-4380f7b3d4c0`](https://medium.com/next-level-german-engineering/chatgpt-enterprise-knowledge-how-can-i-create-a-chatbot-for-my-business-unit-4380f7b3d4c0)

Tazzyman, S. (2023). ç¥ç»ç½‘ç»œæ¨¡å‹. NLP-Guidance. [`moj-analytical-services.github.io/NLP-guidance/NNmodels.html`](https://moj-analytical-services.github.io/NLP-guidance/NNmodels.html)

Wang, W. (2023 å¹´ 4 æœˆ 12 æ—¥). æ·±å…¥äº†è§£åŸºäºå˜æ¢å™¨çš„æ¨¡å‹. Medium.
