["```py\n# Load Packages\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nsns.set()\nsns.set(style=\"darkgrid\")\nfrom matplotlib import cm\nfrom matplotlib.ticker import LinearLocator, FormatStrFormatter\nfrom mpl_toolkits.mplot3d import Axes3D\n```", "```py\n# Define Function\nf = lambda x,y:  2*x**2 + 3*y**2 - 2*x*y - 1\n\n# Plot contour\nX = np.arange(-1, 1, 0.005)\nY = np.arange(-1, 1, 0.005)\nX, Y = np.meshgrid(X, Y)\nZ = f(X,Y)\nplt.figure(figsize=(12, 7))\ncmap = plt.cm.get_cmap('viridis')\nplt.contour(X,Y,Z,250, cmap=cmap)\n```", "```py\n# Define the function f(x, y)\nf = lambda x, y: 2*x**2 + 3*y**2 - 2*x*y - 1\n\n# Define the derivative of f(x, y)\ndef df(x, y):\n    return np.array([4*x - 2*y, 6*y - 2*x])\n\n# Perform gradient descent optimization\ndef grad_desc(f, df, x0, y0, t=0.1, tol=0.001):\n    x, y = [x0], [y0]  # Initialize lists to store x and y coordinates\n    num_steps = 0  # Initialize the number of steps taken\n    # Continue until the norm of the gradient is below the tolerance\n    while np.linalg.norm(df(x0, y0)) > tol:  \n        v = -df(x0, y0)  # Compute the direction of descent\n        x0 = x0 + t*v[0]  # Update x coordinate\n        y0 = y0 + t*v[1]  # Update y coordinate\n        x.append(x0)  # Append updated x coordinate to the list\n        y.append(y0)  # Append updated y coordinate to the list\n        num_steps += 1  # Increment the number of steps taken\n    return x, y, num_steps\n\n# Run the gradient descent algorithm with initial point (1, 1)\na, b, n = grad_desc(f, df, 1, 1)\n\n# Print the number of steps taken for convergence\nprint(f\"Number of Steps to Convergence: {n}\")\n```", "```py\n# Plot the contours\nX = np.arange(-1.1, 1.1, 0.005)\nY = np.arange(-1.1, 1.1, 0.005)\nX, Y = np.meshgrid(X, Y)\nZ = f(X,Y)\nplt.figure(figsize=(12, 7))\nplt.contour(X,Y,Z,250, cmap=cmap, alpha = 0.6)\nn = len(a)\nfor i in range(n - 1):\n    plt.plot([a[i]],[b[i]],marker='o',markersize=7, color ='r')\n    plt.plot([a[i + 1]],[b[i + 1]],marker='o',markersize=7, color ='r')\n    plt.arrow(a[i],b[i],a[i + 1] - a[i],b[i + 1] - b[i], \n      head_width=0, head_length=0, fc='r', ec='r', linewidth=2.0)\n```", "```py\n# Import package for 1D Optimization\nfrom scipy.optimize import minimize_scalar\n\ndef grad_desc(f, df, x0, y0, tol=0.001):\n    x, y = [x0], [y0]  # Initialize lists to store x and y coordinates\n    num_steps = 0  # Initialize the number of steps taken\n    # Continue until the norm of the gradient is below the tolerance\n    while np.linalg.norm(df(x0, y0)) > tol:  \n        v = -df(x0, y0)  # Compute the direction of descent\n        # Define optimizer function for searching t\n        g = lambda t: f(x0 + t*v[0], y0 + t*v[1]) \n        t = minimize_scalar(g).x # Minimize t\n        x0 = x0 + t*v[0]  # Update x coordinate\n        y0 = y0 + t*v[1]  # Update y coordinate\n        x.append(x0)  # Append updated x coordinate to the list\n        y.append(y0)  # Append updated y coordinate to the list\n        num_steps += 1  # Increment the number of steps taken\n    return x, y, num_steps\n\n# Run the gradient descent algorithm with initial point (1, 1)\na, b, n = grad_desc(f, df, 1, 1)\n\n# Print the number of steps taken for convergence\nprint(f\"Number of Steps to Convergence: {n}\")\n```", "```py\n# Plot the contours\nX = np.arange(-1.1, 1.1, 0.005)\nY = np.arange(-1.1, 1.1, 0.005)\nX, Y = np.meshgrid(X, Y)\nZ = f(X,Y)\nplt.figure(figsize=(12, 7))\nplt.contour(X,Y,Z,250, cmap=cmap, alpha = 0.6)\nn = len(a)\nfor i in range(n - 1):\n    plt.plot([a[i]],[b[i]],marker='o',markersize=7, color ='r')\n    plt.plot([a[i + 1]],[b[i + 1]],marker='o',markersize=7, color ='r')\n    plt.arrow(a[i],b[i],a[i + 1] - a[i],b[i + 1] - b[i], head_width=0, \n      head_length=0, fc='r', ec='r', linewidth=2.0)\n```", "```py\n# Perform gradient descent optimization\ndef grad_desc(f, df, x0, y0, tol=0.001):\n    x, y = [x0], [y0]  # Initialize lists to store x and y coordinates\n    num_steps = 0  # Initialize the number of steps taken\n    # Continue until the norm of the gradient is below the tolerance\n    while np.linalg.norm(df(x0, y0)) > tol:  \n        v = -df(x0, y0)  # Compute the direction of descent\n        # Compute the step size using Armijo line search\n        t = armijo(f, df, x0, y0, v[0], v[1]) \n        x0 = x0 + t*v[0]  # Update x coordinate\n        y0 = y0 + t*v[1]  # Update y coordinate\n        x.append(x0)  # Append updated x coordinate to the list\n        y.append(y0)  # Append updated y coordinate to the list\n        num_steps += 1  # Increment the number of steps taken\n    return x, y, num_steps\n\ndef armijo(f, df, x1, x2, v1, v2, s = 0.5, b = 0.5):\n    t = 1\n    # Perform Armijo line search until the Armijo condition is satisfied\n    while (f(x1 + t*v1, x2 + t*v2) > f(x1, x2) + \n            t*s*np.matmul(df(x1, x2).T, np.array([v1, v2]))):\n        t = t*b # Reduce the step size by a factor of b\n    return t\n\n# Run the gradient descent algorithm with initial point (1, 1)\na, b, n = grad_desc(f, df, 1, 1)\n\n# Print the number of steps taken for convergence\nprint(f\"Number of Steps to Convergence: {n}\")\n```", "```py\n# Plot the contours\nX = np.arange(-1.1, 1.1, 0.005)\nY = np.arange(-1.1, 1.1, 0.005)\nX, Y = np.meshgrid(X, Y)\nZ = f(X,Y)\nplt.figure(figsize=(12, 7))\nplt.contour(X,Y,Z,250, cmap=cmap, alpha = 0.6)\nn = len(a)\nfor i in range(n - 1):\n    plt.plot([a[i]],[b[i]],marker='o',markersize=7, color ='r')\n    plt.plot([a[i + 1]],[b[i + 1]],marker='o',markersize=7, color ='r')\n    plt.arrow(a[i],b[i],a[i + 1] - a[i],b[i + 1] - b[i], head_width=0, \n      head_length=0, fc='r', ec='r', linewidth=2.0)\n```"]