- en: How Pytorch 2.0 Accelerates Deep Learning with Operator Fusion and CPU/GPU Code-Generation
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/how-pytorch-2-0-accelerates-deep-learning-with-operator-fusion-and-cpu-gpu-code-generation-35132a85bd26?source=collection_archive---------0-----------------------#2023-04-20](https://towardsdatascience.com/how-pytorch-2-0-accelerates-deep-learning-with-operator-fusion-and-cpu-gpu-code-generation-35132a85bd26?source=collection_archive---------0-----------------------#2023-04-20)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: A primer on deep learning compiler technologies in PyTorch for graph capture,
    intermediate representations, operator fusion, and optimized C++ and GPU code
    generation
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@shashankprasanna?source=post_page-----35132a85bd26--------------------------------)[![Shashank
    Prasanna](../Images/ede96160e770a1db0bb7ab9ece9bdf4f.png)](https://medium.com/@shashankprasanna?source=post_page-----35132a85bd26--------------------------------)[](https://towardsdatascience.com/?source=post_page-----35132a85bd26--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----35132a85bd26--------------------------------)
    [Shashank Prasanna](https://medium.com/@shashankprasanna?source=post_page-----35132a85bd26--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fe0c596ca35b5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-pytorch-2-0-accelerates-deep-learning-with-operator-fusion-and-cpu-gpu-code-generation-35132a85bd26&user=Shashank+Prasanna&userId=e0c596ca35b5&source=post_page-e0c596ca35b5----35132a85bd26---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----35132a85bd26--------------------------------)
    ·17 min read·Apr 20, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F35132a85bd26&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-pytorch-2-0-accelerates-deep-learning-with-operator-fusion-and-cpu-gpu-code-generation-35132a85bd26&user=Shashank+Prasanna&userId=e0c596ca35b5&source=-----35132a85bd26---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F35132a85bd26&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-pytorch-2-0-accelerates-deep-learning-with-operator-fusion-and-cpu-gpu-code-generation-35132a85bd26&source=-----35132a85bd26---------------------bookmark_footer-----------)![](../Images/c4b5db3571e5b2492a3e7f9626723155.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: illustration by author
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: Computer programming is magical. We write code in human readable languages,
    and as though by magic, it gets translated into electric currents through silicon
    transistors making them behave like switches and allowing them to implement complex
    logic — just so we can enjoy cat videos on the internet. Between the programming
    language and hardware processors that run it, is an important piece of technology
    — the compiler. A compiler’s job is to translate and simplify our human readable
    language code into instructions that a processor understands.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 计算机编程是神奇的。我们用人类可读的语言编写代码，然后仿佛魔法一样，它被转换成通过硅晶体管的电流，使其像开关一样工作，并实现复杂的逻辑——只是为了让我们能在互联网上欣赏猫咪视频。在编程语言和运行它的硬件处理器之间，有一个重要的技术组件——编译器。编译器的工作是将我们人类可读的语言代码翻译并简化为处理器能理解的指令。
- en: Compilers play a very important role in deep learning to improve training and
    inference performance, improve energy efficiency, and target diverse AI accelerator
    hardware. In this blog post I’m going to discuss deep learning compiler technologies
    that powers PyTorch 2.0\. I’ll walk you through the different phases of the compilation
    process and discuss various underlying technologies with code examples and visualizations.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 编译器在深度学习中扮演着非常重要的角色，提升训练和推理性能，提高能源效率，并针对各种AI加速器硬件。在这篇博客文章中，我将讨论支持PyTorch 2.0的深度学习编译器技术。我将带你了解编译过程的不同阶段，并通过代码示例和可视化讨论各种基础技术。
- en: What is a deep learning compiler?
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是深度学习编译器？
- en: A deep learning compiler translates high-level code written in deep learning
    frameworks into optimized lower level hardware specific code to accelerate training
    and inference. It finds opportunities in deep learning models to optimize for
    performance by performing layer and operator fusion, better memory planning, and
    generating target specific optimized fused kernels to reduce function call overhead.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习编译器将用深度学习框架编写的高级代码转换为优化的低级硬件特定代码，以加速训练和推理。它通过执行层和操作符融合、更好的内存规划，生成目标特定的优化融合内核来减少函数调用开销，从而在深度学习模型中发现性能优化的机会。
- en: '![](../Images/da27c5876c704b425c43a40781423299.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/da27c5876c704b425c43a40781423299.png)'
- en: illustration by author
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 插图由作者提供
- en: Unlike traditional software compilers, deep learning compilers have to work
    with highly-parallelizable code often accelerated on specialized AI accelerator
    hardware (GPUs, TPUs, AWS Trainium/Inferentia, Intel Habana Gaudi etc.). To improve
    performance, a deep learning compiler has to take advantage of hardware specific
    features such as mixed precision support, performance optimized kernels and minimize
    communication between host (CPU) and AI accelerator.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 与传统软件编译器不同，深度学习编译器必须处理高度并行化的代码，这些代码通常在专用的AI加速硬件（如GPU、TPU、AWS Trainium/Inferentia、Intel
    Habana Gaudi等）上加速运行。为了提高性能，深度学习编译器必须利用硬件特定的功能，如混合精度支持、性能优化的内核，并尽量减少主机（CPU）与AI加速器之间的通信。
- en: 'While deep learning algorithms are continuing to advance at a rapid pace, hardware
    AI accelerators have also been evolving alongside to keep up with deep learning
    algorithm performance and efficiency needs. I discuss the co-evolution of algorithms
    and AI accelerators in an earlier blog post:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然深度学习算法正在以迅猛的速度不断进步，但硬件AI加速器也在不断发展，以跟上深度学习算法的性能和效率需求。我在之前的博客文章中讨论了算法与AI加速器的共同演进：
- en: '[](/ai-accelerators-machine-learning-algorithms-and-their-co-design-and-evolution-2676efd47179?source=post_page-----35132a85bd26--------------------------------)
    [## AI accelerators, machine learning algorithms and their co-design and evolution'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/ai-accelerators-machine-learning-algorithms-and-their-co-design-and-evolution-2676efd47179?source=post_page-----35132a85bd26--------------------------------)
    [## AI加速器、机器学习算法及其共同设计与演进'
- en: Efficient algorithms and methods in machine learning for AI accelerators — NVIDIA
    GPUs, Intel Habana Gaudi and AWS…
  id: totrans-18
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在AI加速器（如NVIDIA GPU、Intel Habana Gaudi和AWS等）中，机器学习的高效算法和方法——
- en: towardsdatascience.com](/ai-accelerators-machine-learning-algorithms-and-their-co-design-and-evolution-2676efd47179?source=post_page-----35132a85bd26--------------------------------)
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/ai-accelerators-machine-learning-algorithms-and-their-co-design-and-evolution-2676efd47179?source=post_page-----35132a85bd26--------------------------------)
- en: In this blog post I’ll focus on the software side of things, and particularly
    the subset of software closer to the hardware — deep learning compilers. First,
    let’s start by taking a look at different functions in a deep learning compiler.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我将专注于软件方面的内容，特别是接近硬件的软件子集 —— 深度学习编译器。首先，让我们来看看深度学习编译器中的不同功能。
- en: Deep learning compiler in PyTorch 2.0
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: PyTorch 2.0 中的深度学习编译器
- en: 'PyTorch 2.0 includes new compiler technologies to improve model performance
    and runtime efficiency and target diverse hardware backends with a simple API:
    torch.compile(). While [other blog posts](https://pytorch.org/get-started/pytorch-2.0/)
    and articles have [discussed](https://pytorch.org/blog/pytorch-2.0-release/) performance
    benefits of PyTorch 2.0 in detail, here I’m going to focus on what happens under
    the hood when you invoke the PyTorch 2.0 compiler. If you’re looking for quantified
    performance benefits, you can find a [performance dashboard](https://github.com/pytorch/pytorch/issues/93794)
    of different models from huggingface, timm and torchbench.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch 2.0 包括新的编译器技术，以提高模型性能和运行时效率，并通过简单的 API 针对不同的硬件后端进行目标化：torch.compile()。虽然[其他博客文章](https://pytorch.org/get-started/pytorch-2.0/)和文章详细讨论了PyTorch
    2.0的性能优势，但在这里，我将专注于当您调用PyTorch 2.0编译器时内部发生的情况。如果您想要量化的性能优势，可以在来自huggingface、timm和torchbench的不同模型的[性能仪表板](https://github.com/pytorch/pytorch/issues/93794)找到。
- en: 'At a high-level the default options for PyTorch 2.0 deep learning compiler
    performs the following key tasks:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在高层次上，PyTorch 2.0 深度学习编译器的默认选项执行以下关键任务：
- en: '**Graph capture**: Computational graph representation for your models and functions.
    PyTorch technologies: TorchDynamo, Torch FX, FX IR'
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**图捕获**：为您的模型和函数表示计算图。PyTorch 技术：TorchDynamo，Torch FX，FX IR'
- en: '**Automatic differentiation**: Backward graph tracing using automatic differentiation
    and lowering to primitives operators. PyTorch technologies: AOTAutograd, Aten
    IR'
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**自动微分**：使用自动微分进行后向图追踪并降低到基本运算符。PyTorch 技术：AOTAutograd，Aten IR'
- en: '**Optimizations**: Forward and backward graph-level optimizations and operator
    fusion. PyTorch technologies: TorchInductor (default) or other compilers'
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**优化**：前向和后向图级优化以及操作融合。PyTorch 技术：TorchInductor（默认）或其他编译器'
- en: '**Code generation**: Generating hardware specific C++/GPU Code. PyTorch technologies:
    TorchInductor, OpenAI Triton (default) other compilers'
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**代码生成**：生成硬件特定的 C++/GPU 代码。PyTorch 技术：TorchInductor，OpenAI Triton（默认）和其他编译器'
- en: 'Through these steps, the compiler transforms your code and generates intermediate
    representations (IRs) that are progressively “lowered”. Lowering is a term in
    the compiler lexicon that refers to mapping a broad set of operations (such as
    supported by PyTorch API) to a narrow set of operations (such as supported by
    hardware) through automatic transformation and re-writing by the compiler. The
    PyTorch 2.0 compiler flow:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这些步骤，编译器将转换您的代码并生成逐渐“降低”的中间表示（IR）。“降低”是编译器词汇中的一个术语，它指的是通过自动转换和重新编写将广泛的操作（例如由PyTorch
    API支持的操作）映射到狭窄的操作集（例如由硬件支持的操作）。PyTorch 2.0 编译器流程：
- en: '![](../Images/55cdb16777e0b865f242827c8d4130fa.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/55cdb16777e0b865f242827c8d4130fa.png)'
- en: If you are new to compiler terminology don’t let all of this scare you yet.
    I’m not a compiler engineer either. Keep reading and things will become clear
    as I’ll break the process down using a simple example and visualizations.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您对编译器术语不熟悉，请不要被这一切吓到。我也不是编译器工程师。继续阅读，随着我将使用一个简单的示例和可视化来详细解释，一切将变得清晰起来。
- en: A walk through the `torch.compile()` compiler process
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过 `torch.compile()` 编译器过程的详细步骤
- en: 'Note: This whole walkthrough is in a [Jupyter Notebook hosted here](https://github.com/shashankprasanna/pytorch-examples/blob/main/pytorch-compile-blogpost/torch-compile-under-the-hood.ipynb)'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：整个步骤详见[此处的Jupyter Notebook](https://github.com/shashankprasanna/pytorch-examples/blob/main/pytorch-compile-blogpost/torch-compile-under-the-hood.ipynb)
- en: For the sake of simplicity, I’ll define a very simple function and run it through
    the PyTorch 2.0 compiler process. You can replace this function with a deep neural
    network model or an nn.Module subclass, but this example should help you appreciate
    what’s going on under the hood much better than a complex multi-million parameter
    model.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简单起见，我将定义一个非常简单的函数并将其通过PyTorch 2.0编译器流程运行。您可以用深度神经网络模型或nn.Module子类替换此函数，但这个例子应该能帮助您更好地理解底层发生的事情，而不是复杂的多参数模型。
- en: '![](../Images/0e1df7429cacd7a8c758940431df63f8.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0e1df7429cacd7a8c758940431df63f8.png)'
- en: 'PyTorch code for that function:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 该函数的 PyTorch 代码：
- en: '[PRE0]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: If you paid attention in high-school trigonometry class, you know that the value
    of our function is always going to be 1 for all real valued x. Which means it’s
    derivative, a derivative of a constant, and must be equal to zero. This will come
    in handy to verify what the function and its derivatives are doing.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你在高中三角学课程中认真听讲，你会知道我们的函数值对于所有实值 x 始终为 1。这意味着它的导数是一个常数的导数，必须等于零。这在验证函数及其导数的作用时会很有用。
- en: Now, it’s time to call `torch.compile()` . First let’s convince ourselves that
    compiling this function doesn’t change its output. For the same 1x1000 random
    vector the mean squared error between the output of our function and a vector
    of 1s should be zero for both the compiled and the uncompiled function (under
    some error tolerance).
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，是时候调用 `torch.compile()`。首先，让我们说服自己编译这个函数不会改变其输出。对于相同的 1x1000 随机向量，函数输出与 1
    的向量之间的均方误差应在编译和未编译的函数之间为零（在某些误差容限下）。
- en: '![](../Images/e4440c36c8e45317c5e3628d1819dc59.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e4440c36c8e45317c5e3628d1819dc59.png)'
- en: screenshot by author
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 作者截图
- en: All we did was add a single line of extra code `torch.compile()` to invoke our
    compiler. Let’s now take a look at what’s happening under the hood at each stage.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 我们所做的只是添加了一行额外的代码 `torch.compile()` 来调用我们的编译器。现在让我们看看每个阶段发生了什么。
- en: 'Graph capture: Computational graph representation for your PyTorch models or
    functions'
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 图形捕获：PyTorch 模型或函数的计算图表示
- en: '**PyTorch technologies:** TorchDynamo, FX Graphs, FX IR'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '**PyTorch 技术：** TorchDynamo, FX 图, FX IR'
- en: 'The first step for the compiler is to determine what to compile. Enter TorchDynamo.
    TorchDynamo intercepts the execution of your Python code and transforms it into
    FX intermediate representation (IR), and stores it in a special data structure
    called FX Graph. What does this look like you ask? Glad you asked. Below, we’ll
    take a looks the code we use to generate this, but here is the transformation
    and output:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 编译器的第一步是确定编译内容。进入 TorchDynamo。TorchDynamo 截取你的 Python 代码的执行，并将其转换为 FX 中间表示（IR），并将其存储在一个称为
    FX 图的特殊数据结构中。你问这看起来是什么样的？很高兴你问了。下面，我们将查看生成这些的代码，但这是转换和输出的结果：
- en: '![](../Images/c6a6411faea674730ef0b417028eda20.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c6a6411faea674730ef0b417028eda20.png)'
- en: screenshot by author
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 作者截图
- en: It’s important to note that Torch FX graphs are just containers for IR and don’t
    really specify what operators it should hold. In the next section we’ll see the
    FX graph container come up again with a different set of IRs. If you compare the
    function code and FX IR there’s very little difference between the two. In fact,
    it’s the same PyTorch code you wrote, but laid out in a format that the FX graph
    data structure expects. They both will provide the same result when executed.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是要注意，Torch FX 图只是 IR 的容器，并不真正指定它应该包含哪些操作符。在下一部分，我们将看到 FX 图容器再次出现，但带有不同的 IR
    集。如果你比较函数代码和 FX IR，它们之间几乎没有差别。实际上，它是你编写的相同 PyTorch 代码，只是以 FX 图数据结构期望的格式进行布局。当执行时，它们将提供相同的结果。
- en: 'If you call torch.compile() without any arguments, it’ll use the default settings
    which runs the entire compiler stack which includes the default hardware backend
    compiler called TorchInductor. But we’d be jumping ahead if we discussed TorchInductor
    now, so let’s park that topic for now, and we’ll come back to it when we’re ready.
    First we need to discuss graph capture and we can do that by intercepting the
    calls from torch.compile(). Here’s how we’ll do that: torch.compile() allows you
    to provide your own compiler too, but because I’m not a compiler engineer, and
    I don’t have the slightest clue how to write a compiler, I’ll provide a fake compiler
    function to capture the FX graph IR that TorchDynamo generates.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你在没有任何参数的情况下调用 `torch.compile()`，它将使用默认设置，这会运行整个编译器栈，包括默认的硬件后端编译器 TorchInductor。但如果我们现在讨论
    TorchInductor 会有点超前，因此暂时搁置这个话题，等我们准备好时再回来。首先我们需要讨论图形捕获，我们可以通过截取 `torch.compile()`
    的调用来实现这一点。以下是我们将要做的：`torch.compile()` 允许你提供自己的编译器，但由于我不是编译器工程师，也不知道如何编写编译器，我将提供一个虚拟编译器函数来捕获
    TorchDynamo 生成的 FX 图 IR。
- en: 'Below is our fake compiler backend function called inspect_backend to torch.compile()
    and within that function I do two things:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是我们为 `torch.compile()` 函数编写的虚拟编译器后端函数 inspect_backend，在这个函数中我做了两件事：
- en: Print the FX IR code that was captured by TorchDynamo
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印由 TorchDynamo 捕获的 FX IR 代码
- en: Save the FX graph visualization
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 保存 FX 图形可视化
- en: The output of that above code snippet are the FX IR code and the graph diagram
    showing our function `sin^2(x)+cos^2(x)`
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4aceac24aae49fe3d4cc61ae7a54a437.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
- en: screenshot by author
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: Note that our fake compiler inspect_backend function is only invoked when we
    call the compiled function with some data i.e. when we call `compiled_model(x)`.
    In the above code snippet, we’re only evaluating the function or in deep learning
    terminology, doing a “forward-pass”. In the next section we’ll take advantage
    of the PyTorch’s automatic differentiation engine called torch.autograd to compute
    the derivative and the “backward-pass” graph.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: 'Automatic differentiation: Forward and backward computational graphs'
  id: totrans-56
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**PyTorch technologies:** AOTAutograd, Core Aten IR'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: TorchDynamo gave us the forward pass function evaluation as an FX graph, but
    what about the backward pass? For the sake of completeness, I’m going to digress
    from our primary topic and talk a bit about why we need to evaluate the gradients
    of a function with respect to its weights. If you’re already familiar with how
    mathematical optimization works skip this immediate section.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: '**What is backward pass and backward graph?**'
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The “learning” part of deep learning and machine learning is a mathematical
    optimization problem which is simply stated as: Find the value of a variable w
    that yields the lowest value of some function of w. Or more succinctly:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/89206945634aa1846e7dd6915fcf0efa.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
- en: 'In machine learning f(w) is the loss function parametrized by weights. f(w)
    can be more clearly represented as some measure of error between the training
    labels and the model’s prediction labels based on the training data:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/73c9ab5ba817313bf304d21987085e3a.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
- en: 'Turns out, if we can calculate the “rate of reduction” of loss with respect
    to weights, we can update our weights to move one step closer to a smaller and
    smaller loss f(w). In other words, we must move closer to a model that better
    fits our training dataset. We can find next values of weights by calculating the
    steepest slope of the loss f(w) at a given w and perturb w to head in that direction.
    The slope of a function with respect to the weights, is its derivative with respect
    to the weights. Since there are more than one weight values, the derivative becomes
    a vector quantity called the gradient which is a vector of partial derivatives
    with components for each weight. The weights w are perturbed at each iteration
    by some function g() of the gradients as follows:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e1d0c3208f5555a0af3b2d0396f433cf.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
- en: Where the function g(.) depends on the optimizer (e.g. sgd, sgdm, rmsprop, adam
    etc.).
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
- en: 'For SGD the weight update step becomes:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c2fba7a1140e7d8e82a8425cbf4e9515.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
- en: '**How does PyTorch 2.0 trace the backward pass graph?**'
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: First let’s calculate what we expect the backward pass graph should look like
    and then compare it with what PyTorch generates. For our simple function, the
    forward graph and the backward graph should implement the following function.
    If sin and cos bother you, you can imagine f(x) being the loss function applied
    to a neural network.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们计算我们期望反向传递图应该是什么样的，然后与 PyTorch 生成的结果进行比较。对于我们的简单函数，前向图和反向图应该实现以下函数。如果
    sin 和 cos 让你感到困扰，你可以想象 f(x) 是应用于神经网络的损失函数。
- en: '![](../Images/3f65c215f211e0ed8c73a719d0937eba.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3f65c215f211e0ed8c73a719d0937eba.png)'
- en: PyTorch uses reverse-mode [automatic differentiation](https://en.wikipedia.org/wiki/Automatic_differentiation)
    to compute the gradients, and PyTorch’s implementation of automation differentiation
    is called Autograd. PyTorch 2.0 introduces AOTAutograd which traces the forward
    and backward graph ahead of time, i.e. prior to execution, and generates a joint
    forward and backward graph. It then partitions the forward and the backward graph
    into two separate graphs. Both the forward and the backward graphs are stored
    in the FX graph data structure and can be visualized as shown below.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch 使用反向模式的 [自动微分](https://en.wikipedia.org/wiki/Automatic_differentiation)
    来计算梯度，PyTorch 的自动微分实现称为 Autograd。PyTorch 2.0 引入了 AOTAutograd，它在执行之前预先跟踪前向和反向图，并生成一个联合的前向和反向图。然后，它将前向图和反向图分成两个独立的图。前向图和反向图都存储在
    FX 图数据结构中，可以如下面所示进行可视化。
- en: '![](../Images/94722341aa44c88e98820ab72d1f3085.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/94722341aa44c88e98820ab72d1f3085.png)'
- en: screenshot by author
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 作者截图
- en: You can verify that the math checks out by working through the nodes on the
    graph. AOTAutograd generated backward pass indeed computes the derivative shown
    in the equation I shared earlier, which should equal zero since the original function
    only produces the identity.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过遍历图上的节点来验证数学计算是否正确。AOTAutograd 生成的反向传递确实计算了我之前分享的方程中的导数，该导数应该等于零，因为原始函数仅生成了单位矩阵。
- en: We’ll now run AOTAutograd by extending our fake compiler function inspect_backend
    to call AOTAutograd and generate our backward graph. The updated inspect_backed
    defines a forward (fw) and backward (bw) compiler capture function that reads
    the forward and backward graph from AOTAutograd and prints the lowered ATen IR
    and saves the FX graph for the forward and backward graphs.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将通过扩展我们伪编译器函数 `inspect_backend` 来运行 AOTAutograd，调用 AOTAutograd 并生成我们的反向图。更新后的
    `inspect_backend` 定义了一个前向 (fw) 和反向 (bw) 编译器捕获函数，该函数读取 AOTAutograd 中的前向和反向图，打印降低的
    ATen IR，并保存前向和反向图的 FX 图。
- en: This will generate the following forward AND backward graphs. Notice that the
    forward graph also looks slightly different from what we saw earlier in Figure
    x. For example torch.sin(x) in the FX graph IR and in our original code has been
    replaced by torch.ops.aten.sin.default(). What’s this funny thing called aten,
    you might ask, if you’re not already familiar with it. ATen stands for A Tensor
    library, which is a very creatively named low level library with a C++ interface
    that implements many of the fundamental operations that run on CPU and GPU.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 这将生成以下的前向和反向图。请注意，前向图也与我们在图 x 中看到的略有不同。例如，FX 图 IR 中的 `torch.sin(x)` 和我们原始代码中的已被替换为
    `torch.ops.aten.sin.default()`。你可能会问，这个叫做 aten 的奇怪东西是什么，如果你还不熟悉的话。ATen 代表一个张量库，它是一个非常富有创意命名的低级库，具有
    C++ 接口，实现了许多在 CPU 和 GPU 上运行的基本操作。
- en: 'In eager mode operation, your PyTorch operations are routed to this library
    which then calls the appropriate CPU or GPU implementation. AOTAutograd automatically
    generates code that replaces the higher level PyTorch API with ATen IR for the
    forward and backward graph which you can see in the output below:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在急切模式操作中，你的 PyTorch 操作会被路由到这个库，然后调用适当的 CPU 或 GPU 实现。AOTAutograd 自动生成代码，将较高级的
    PyTorch API 替换为前向和反向图的 ATen IR，你可以在下面的输出中看到：
- en: '![](../Images/06ff63252f4aecddd8d7f2010b65c823.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/06ff63252f4aecddd8d7f2010b65c823.png)'
- en: screenshot by author
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 作者截图
- en: You can also see that in addition to the output of the forward pass, the forward
    graph outputs some additional tensors [add, sin, cos, primals_1] . These tensors
    are saved for the backward pass for gradient calculation. You can also see this
    in the computational graphs for the forward and backward pass in the figure shared
    earlier.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 你还可以看到，除了前向传递的输出，前向图还输出了一些额外的张量 [add, sin, cos, primals_1]。这些张量被保存用于反向传递的梯度计算。你也可以在之前分享的前向和反向传递的计算图中看到这一点。
- en: What are the different types of IR in PyTorch?
  id: totrans-82
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: PyTorch 中不同类型的 IR 有哪些？
- en: 'ATen IR is a list of operators supported by the ATen library as we discussed
    in the previous section, and you can see the full list of operations implemented
    in [ATen library here](https://pytorch.org/cppdocs/api/namespace_at.html). There
    are two other IR concepts in PyTorch you should be aware of: 1/ Core Aten IR 2/
    Prims IR. Core Aten IR is a subset of the broader Aten IR and Prims IR and an
    even smaller subset of Core Aten IR. Let’s say you are designing a processor and
    want to support PyTorch code acceleration on your hardware. It’d be near impossible
    to support the full list of PyTorch API in hardware, so what you can do is build
    a compiler that only supports the smaller subset of fundamental operators defined
    in Core Aten IR or Prims IR, and let AOTAutograd decompose compound operators
    into the core operators as we’ll see in the next section.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: ATen IR 是我们在上一节讨论的 ATen 库所支持的操作符列表，你可以在 [ATen library here](https://pytorch.org/cppdocs/api/namespace_at.html)
    查看实现的完整操作列表。PyTorch 中还有两个其他的 IR 概念你应该了解：1/ Core Aten IR 2/ Prims IR。Core Aten
    IR 是更广泛的 Aten IR 和 Prims IR 的一个子集，而 Prims IR 是 Core Aten IR 的一个更小的子集。假设你正在设计一个处理器并希望在你的硬件上支持
    PyTorch 代码加速。要在硬件中支持完整的 PyTorch API 列表几乎是不可能的，因此你可以构建一个仅支持 Core Aten IR 或 Prims
    IR 中定义的较小基本操作符子集的编译器，并让 AOTAutograd 将复合操作符分解为核心操作符，正如我们将在下一节中看到的那样。
- en: What’s the difference between ATen IR, Core ATen IR, Prims IR?
  id: totrans-84
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ATen IR、Core ATen IR 和 Prims IR 之间有什么区别？
- en: '![](../Images/d93a56ae8fb97def11f795028aedf795.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d93a56ae8fb97def11f795028aedf795.png)'
- en: screenshot by author
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 作者截图
- en: '[Core Aten IR](https://pytorch.org/docs/stable/ir.html#core-aten-ir) (formerly
    canonical Aten IR) is a subset of the Aten IR that can be used to compose all
    other operators in the Aten IR. Compilers that target specific hardware accelerators
    can focus on supporting only the Core Aten IR and mapping it to their low level
    hardware API. This makes it easier to add hardware support to PyTorch since they
    don’t have to implement support for the full PyTorch API which will continue to
    grow with more and more abstractions.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '[Core Aten IR](https://pytorch.org/docs/stable/ir.html#core-aten-ir)（以前称为标准
    Aten IR）是 Aten IR 的一个子集，可用于组合 Aten IR 中的所有其他操作符。针对特定硬件加速器的编译器可以专注于支持 Core Aten
    IR 并将其映射到其低级硬件 API。这使得为 PyTorch 添加硬件支持变得更容易，因为他们不必实现对完整 PyTorch API 的支持，而 PyTorch
    API 将继续随着更多抽象的增加而增长。'
- en: '[Prims IR](https://pytorch.org/docs/stable/ir.html#prims-ir) is an even smaller
    subset of the Core Aten IR that further decomposes Core Aten IR ops into fundamental
    operations making it even easier for compilers that target specific hardware to
    support PyTorch. But decomposing operators into lower and lower operations will
    most definitely lead to performance degradation due to excess memory writes and
    function call overhead. But the expectation is that hardware compilers can take
    these operators and fuse them back together to support hardware API to get back
    performance.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '[Prims IR](https://pytorch.org/docs/stable/ir.html#prims-ir) 是 Core Aten IR
    的一个更小的子集，它将 Core Aten IR 操作进一步分解为基本操作，使得针对特定硬件的编译器更容易支持 PyTorch。但将操作符分解为更低级别的操作将不可避免地导致性能下降，因为会增加额外的内存写入和函数调用开销。预期是硬件编译器可以将这些操作符融合回去，以支持硬件
    API 并恢复性能。'
- en: While we don’t need to further decompose our function into Core Aten IR and
    Prims IR I’ll demonstrate how below.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们不需要进一步将我们的函数分解为 Core Aten IR 和 Prims IR，我将下面演示如何操作。
- en: (Optional topic) Decomposition to Core Aten IR and Prims IR
  id: totrans-90
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: （可选话题）分解为 Core Aten IR 和 Prims IR
- en: If you’re designing hardware or hardware compilers, it’d be near impossible
    to support the full list of PyTorch API in hardware, especially given the pace
    at which deep learning and AI are advancing. But the advantage for a hardware
    designer is that most deep learning functionality can be mapped into very few
    basic mathematical operations and the most computationally intensive ones are
    matrix-matrix and matrix-vector operations. Compound operators like those supported
    by PyTorch API can be decomposed into these fundamental operations using AOTAutograd
    as we’ll discuss in this section. If you don’t deal with low level hardware, you
    can skip this section.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你正在设计硬件或硬件编译器，在硬件中支持完整的 PyTorch API 列表几乎是不可能的，尤其是考虑到深度学习和 AI 发展的速度。但硬件设计师的优势在于，大多数深度学习功能可以映射到很少的基本数学操作中，而计算密集型的操作是矩阵-矩阵和矩阵-向量操作。像
    PyTorch API 支持的复合操作符可以使用 AOTAutograd 分解为这些基本操作，如我们将在本节中讨论的。如果你不涉及底层硬件，你可以跳过本节。
- en: You can update the AOTAutograd function to pass in a dictionary of decompositions
    that can lower the Aten IR into Core Aten IR and Prims IR. I’ll only share the
    relevant code snippet and output here since you can find the full notebook on
    GitHub. By default operators are not decomposed into Core Aten IR or Prims IR,
    but you can pass a dictionary of decompositions.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以更新 AOTAutograd 函数以传递一个分解字典，该字典可以将 Aten IR 降级到 Core Aten IR 和 Prims IR。我只会在这里分享相关的代码片段和输出，因为你可以在
    GitHub 上找到完整的笔记本。默认情况下，操作符不会被分解为 Core Aten IR 或 Prims IR，但你可以传递一个分解字典。
- en: In the code snippet below, I’ve converted our function f into a loss function
    f_loss by including the computation of mean squared error (MSE) into our function.
    I’m doing this to demonstrate how AOTAutograd can decompose MSE into its fundamental
    operators.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的代码片段中，我将我们的函数 f 转换为损失函数 f_loss，通过将均方误差 (MSE) 的计算包含到我们的函数中。我这样做是为了演示 AOTAutograd
    如何将 MSE 分解为其基本操作符。
- en: '![](../Images/d5b809b001a11fa4885d200666cf1ecd.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d5b809b001a11fa4885d200666cf1ecd.png)'
- en: screenshot by author
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的截图
- en: 'The output of the decomposition is that mse_loss gets decomposed into more
    fundamental operations: subtract , power(2) , mean.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 分解的输出是 mse_loss 被分解为更基本的操作：减法、幂(2)、均值。
- en: '![](../Images/7a98f1b51f979fbc1eaf146e26c068c4.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7a98f1b51f979fbc1eaf146e26c068c4.png)'
- en: screenshot by author
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的截图
- en: This is because MSE or mean square error between two vectors x and y is defined
    as the following which only needs those 3 operations subtract, where power is
    an element-wise operation. If you write a compiler for your hardware, you likely
    already support these 3 operations and by decomposition your PyTorch code would
    run without further modifications.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 这是因为两个向量 x 和 y 之间的 MSE 或均方误差定义为以下公式，只需要这 3 个操作：减法，其中幂是逐元素操作。如果你为你的硬件编写编译器，你可能已经支持这
    3 个操作，通过分解，你的 PyTorch 代码可以在不做进一步修改的情况下运行。
- en: '![](../Images/59f8eb7f7eb8b29efb52c939d8c8ca99.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/59f8eb7f7eb8b29efb52c939d8c8ca99.png)'
- en: You can also see this reflected in the FX graph visualization
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 你还可以在 FX 图形可视化中看到这一点
- en: '![](../Images/1f285f38f1e453c6c632bd556c7be40a.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1f285f38f1e453c6c632bd556c7be40a.png)'
- en: screenshot by author
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的截图
- en: Now let’s decompose it further into Prims IR which is a much smaller subset
    of ~250 operators. Again, I’ll only share the relevant code snippet and output
    here since you can find the full notebook on GitHub.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们进一步将其分解为 Prims IR，这是 ~250 个操作符的一个更小的子集。同样，我只会在这里分享相关的代码片段和输出，因为你可以在 GitHub
    上找到完整的笔记本。
- en: '![](../Images/cb738e3595a61790cd419b57d1953abc.png)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cb738e3595a61790cd419b57d1953abc.png)'
- en: screenshot by author
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的截图
- en: The output of the prim IR decomposition is below. All the aten ops in RED are
    replaced or decomposed to use prim operators in green.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: prim IR 分解的输出如下。所有标记为红色的 aten 操作都被替换或分解为使用绿色的 prim 操作符。
- en: '![](../Images/23c2cb563e551a01b80fb2b27af96c2b.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/23c2cb563e551a01b80fb2b27af96c2b.png)'
- en: screenshot by author
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的截图
- en: 'Graph optimization: Layer and operator fusion and C++/GPU code generation'
  id: totrans-110
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 图优化：层和操作符融合以及 C++/GPU 代码生成
- en: '**PyTorch technologies discussed:** TorchInductor, OpenAI Triton (default)
    other compilers'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '**讨论的 PyTorch 技术：** TorchInductor、OpenAI Triton（默认）其他编译器'
- en: 'In this final section of the blog post we’ll discuss operator fusion and automatic
    code generation for CPUs and GPUs using TorchInductor. First some basics:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 在博客的最后一部分，我们将讨论使用 TorchInductor 进行操作符融合和自动代码生成。首先是一些基础知识：
- en: '**What is a deep learning optimizing compiler?**'
  id: totrans-113
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**深度学习优化编译器是什么？**'
- en: An optimizing compiler for deep learning is good at finding performance gaps
    in code and addressing them by transforming the code to reduce code attributes
    such as memory access, kernel launches, data layout optimizations for a target
    backend. TorchInductor is the default optimizing compiler with torch.compile()
    that can generate optimized kernels for GPUs using OpenAI Triton and CPUs using
    OpenMP pragma directives.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习的优化编译器擅长发现代码中的性能差距，并通过将代码转换以减少诸如内存访问、内核启动、针对目标后端的数据布局优化等代码属性来解决这些差距。TorchInductor
    是 torch.compile() 的默认优化编译器，可以使用 OpenAI Triton 为 GPU 生成优化内核，并使用 OpenMP pragma 指令为
    CPU 生成优化内核。
- en: '**What is operator fusion in deep learning?**'
  id: totrans-115
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**深度学习中的操作符融合是什么？**'
- en: Deep learning is composed of many fundamental operations such as matrix-matrix
    and matrix-vector multiplications. In PyTorch eager mode of execution each operation
    will result in separate function calls or kernel launches on hardware. This leads
    to CPU overhead of launching kernels and results in more memory reads and writes
    between kernel launches. A deep learning optimizing compiler like TorchInductor
    can fuse multiple operations into a single compound operator in python and generate
    low-level GPU kernels or C++/OpenMP code for it. This results in faster computation
    due to fewer kernel launches and fewer memory read/writes.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/88439b04d463737c8ee388ef2abe8689.png)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
- en: screenshot by author
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: The computational graph from the output of AOTAutograd in the previous section
    is composed of many Aten operators represented in an FX graph. TorchInductor optimizations
    doesn’t change the underlying computation in the graph but merely restructures
    it with operator and layer fusion, and generates CPU or GPU code for it. Since
    TorchInductor can see the full forward and backward computational graph ahead
    of time, it can take decisions on out-of-order execution of operations that don’t
    have dependence on each other, and maximize hardware resource utilization.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: Under the hood, for GPU targets, TorchInductor uses OpenAI’s Triton to generate
    fused GPU kernels. Triton itself is a separate Python based framework and compiler
    for writing optimized low-level GPU code which is otherwise written in CUDA C/C++.
    But the only difference is that TorchInductor will generate Triton code which
    is compiled into low level PTX code.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: 'For multi-core CPU targets, TorchInductor generates C++ code and injects OpenMP
    pragma directives to generate parallel kernels. From the PyTorch user level world
    view, this is the IR transformation flow:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b798e58504966c9c4c5fb1ca8428aced.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
- en: Of course this being the high-level view, I’m omitting some details here and
    I encourage you to read the [TorchInductor forum post](https://dev-discuss.pytorch.org/t/torchinductor-a-pytorch-native-compiler-with-define-by-run-ir-and-symbolic-shapes/747)
    and [OpenAI’s triton blog post for triton](https://openai.com/research/triton).
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: We’ll now throw away our fake compiler which we used in our previous section,
    and use the full PyTorch compiler stack that uses TorchInductor.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: 'Notice that I’ve passed optional argument that enables two debug features:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: '`trace.enabled`: Generates intermediate code to inspect code generated by TorchInductor'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`trace.graph_enabled`: Generates the optimized computational graph visualization
    after operator fusion'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For our simple example TorchInductor is able to fuse all intermediate operations
    in our function into a single custom operator, and you can see below how that
    simplifies the forward and backward computational graphs.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/498c56a774d5c1681ee51dc91320325e.png)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
- en: screenshot by author
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: You must surely be wondering what this fused operator looks like in code. The
    code for the fused operator is automatically generated by TorchInductor and it’s
    in C++ or Triton based on the target device — CPU or GPU. You don’t need to explicitly
    specify to TorchInductor which device to target, it can infer it from the data
    and model device type.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 你一定好奇这个融合运算符在代码中是什么样子。融合运算符的代码是由 TorchInductor 自动根据目标设备 — CPU 或 GPU — 生成的，它不需要明确指定目标设备，可以根据数据和模型设备类型进行推断。
- en: To view the generated code, you have to enable debugging using trace.enabled=True
    and this creates a director called torch_compile_debug with debug information.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 要查看生成的代码，你必须启用调试，使用 trace.enabled=True，这将创建一个名为 torch_compile_debug 的目录，包含调试信息。
- en: 'The full path to forward and backward graph code are:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 前向和反向图代码的完整路径为：
- en: '`torch_compile_debug/run_<DATE_TIME_PID>/aot_torchinductor/model__XX_forward_XX/output_code.py`'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`torch_compile_debug/run_<DATE_TIME_PID>/aot_torchinductor/model__XX_forward_XX/output_code.py`'
- en: '`torch_compile_debug/run_<DATE_TIME_PID>/aot_torchinductor/model__XX_backward_XX/output_code.py`'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`torch_compile_debug/run_<DATE_TIME_PID>/aot_torchinductor/model__XX_backward_XX/output_code.py`'
- en: If you set device = ‘cuda’ (assuming your computer has a GPU device) then the
    generated code in the forward folder is in Open AI Triton
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你设置 device = ‘cuda’（假设你的计算机有 GPU 设备），那么在 forward 文件夹生成的代码将是 Open AI Triton。
- en: '![](../Images/0107bd207a6a3273b9627cf0b36ac5b4.png)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0107bd207a6a3273b9627cf0b36ac5b4.png)'
- en: screenshot by author
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 作者截图
- en: If you set device = ‘CPU’ then the generated code is in C++ with OpenMP pragmas
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你设置 device = ‘CPU’，那么生成的代码将是带有 OpenMP pragmas 的 C++ 代码。
- en: '![](../Images/9db2f9c42048666128a54202d4db2d0a.png)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9db2f9c42048666128a54202d4db2d0a.png)'
- en: screenshot by author
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 作者截图
- en: Recap
  id: totrans-142
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: Deep learning compilers are complex with intricate inner workings that rival
    a swiss watch. In this blog post, I hope I provided you with a gentle and easy
    to follow primer on this topic and how these technologies power PyTorch 2.0.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习编译器的内部机制复杂，有如瑞士表般精致。在本博文中，我希望给你提供一个关于这个主题的简明易懂的入门，并介绍这些技术如何驱动 PyTorch 2.0。
- en: '![](../Images/bb3bb197c0e1d2f8fe2c007bb6c5e1a0.png)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bb3bb197c0e1d2f8fe2c007bb6c5e1a0.png)'
- en: screenshot by author
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 作者截图
- en: I started with a simple PyTorch function
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我从一个简单的 PyTorch 函数开始。
- en: I showed how TorchDynamo captures the graph and represents it in FX IR
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我展示了 TorchDynamo 如何捕获图并在 FX IR 中表示它。
- en: I showed how AOTAutograd generates the backward pass graph, lowers PyTorch operators
    into Aten operators and represents it in an FX graph container.
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我展示了 AOTAutograd 如何生成反向传递图，将 PyTorch 运算符降低为 Aten 运算符，并在 FX 图容器中表示它。
- en: I discussed how Aten operators can be further decomposed into Core Aten IR and
    Prims IR that reduces the number of operators that other compilers can support
    without supporting the full PyTorch API list.
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我讨论了如何进一步将 Aten 运算符分解为 Core Aten IR 和 Prims IR，以减少其他编译器支持的运算符数量，而无需支持完整的 PyTorch
    API 列表。
- en: I showed how TorchInductor performs operator fusion and generates optimized
    code for CPU and GPU targets
  id: totrans-150
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我展示了 TorchInductor 如何执行运算符融合，并为 CPU 和 GPU 目标生成优化的代码。
- en: 'If you followed along you should be able to provide a high-level response to
    the following questions:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你跟随我，应该能够对以下问题提供高层次的回答：
- en: What is a deep learning compiler?
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 什么是深度学习编译器？
- en: What does a PyTorch 2.0 compiler do when you call torch.compile()?
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当你调用 torch.compile() 时，PyTorch 2.0 编译器会做什么？
- en: Why do we need a forward and backward pass graph ahead of time?
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为什么我们需要提前进行前向和反向传递图？
- en: What are the different intermediate representations (IR) in PyTorch
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PyTorch 中有哪些不同的中间表示（IR）？
- en: What is the difference between ATen IR, Core ATen IR, Prims IR?
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ATen IR、Core ATen IR、Prims IR 之间有何区别？
- en: What is operator fusion and why is it important?
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 什么是运算符融合，以及它为何重要？
- en: Thank you for reading all the way to the end!
  id: totrans-158
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 感谢你阅读到最后！
- en: If you found this article interesting, consider following me on medium to be
    notified when I publish new articles. Please also check out my other blog posts
    on [medium](https://medium.com/@shashankprasanna) or follow me on twitter ([@shshnkp](https://twitter.com/shshnkp)),
    [LinkedIn](https://www.linkedin.com/in/shashankprasanna/) or leave a comment below.
    Want me to write on a specific topic I’d love to hear from you!
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你觉得这篇文章有趣，请考虑在 medium 上关注我，以便在我发布新文章时收到通知。也请查看我在 [medium](https://medium.com/@shashankprasanna)
    上的其他博文，或者关注我的推特 ([@shshnkp](https://twitter.com/shshnkp))，[LinkedIn](https://www.linkedin.com/in/shashankprasanna/)
    或在下方留言。如果你希望我写关于特定主题的文章，我很乐意听取你的建议！
