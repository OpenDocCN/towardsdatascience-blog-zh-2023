- en: Graph Machine Learning @ ICML 2023
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ÂéüÊñáÔºö[https://towardsdatascience.com/graph-machine-learning-icml-2023-9b5e4306a1cc?source=collection_archive---------0-----------------------#2023-08-06](https://towardsdatascience.com/graph-machine-learning-icml-2023-9b5e4306a1cc?source=collection_archive---------0-----------------------#2023-08-06)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: What‚Äôs new in Graph ML?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Recent advancements and hot trends, August 2023 edition
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://mgalkin.medium.com/?source=post_page-----9b5e4306a1cc--------------------------------)[![Michael
    Galkin](../Images/c5eb13334712ca0462d8a5df4a268ad0.png)](https://mgalkin.medium.com/?source=post_page-----9b5e4306a1cc--------------------------------)[](https://towardsdatascience.com/?source=post_page-----9b5e4306a1cc--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----9b5e4306a1cc--------------------------------)
    [Michael Galkin](https://mgalkin.medium.com/?source=post_page-----9b5e4306a1cc--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ¬∑
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F4d4f8ddd1e68&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgraph-machine-learning-icml-2023-9b5e4306a1cc&user=Michael+Galkin&userId=4d4f8ddd1e68&source=post_page-4d4f8ddd1e68----9b5e4306a1cc---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----9b5e4306a1cc--------------------------------)
    ¬∑16 min read¬∑Aug 6, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F9b5e4306a1cc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgraph-machine-learning-icml-2023-9b5e4306a1cc&user=Michael+Galkin&userId=4d4f8ddd1e68&source=-----9b5e4306a1cc---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F9b5e4306a1cc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgraph-machine-learning-icml-2023-9b5e4306a1cc&source=-----9b5e4306a1cc---------------------bookmark_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: Magnificent beaches and tropical Hawaiian landscapes üå¥did not turn brave scientists
    away from attending the [International Conference on Machine Learning](https://icml.cc/Conferences/2023)
    in Honolulu and presenting their recent work! Let‚Äôs see what‚Äôs new in our favorite
    Graph Machine Learning area.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c984ed95b5365a9c6b59956d31ab3dac.png)'
  prefs: []
  type: TYPE_IMG
- en: Image By Author.
  prefs: []
  type: TYPE_NORMAL
- en: '*Thanks Santiago Miret for proofreading the post.*'
  prefs: []
  type: TYPE_NORMAL
- en: To make the post less boring about papers, I took some photos around Honolulu
    üì∑
  prefs: []
  type: TYPE_NORMAL
- en: 'Table of contents (clickable):'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[Graph Transformers: Sparser, Faster, and Directed](#8d41)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Theory: VC dimension of GNNs, deep dive in over-squashing](#0d40)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[New GNN architectures: delays and half-hops](#c5be)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Generative Models ‚Äî Stable Diffusion for Molecules, Discrete diffusion](#7e7c)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Geometric Learning: Geometric WL, Clifford Algebras](#b0d0)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Molecules: 2D-3D pretraining, Uncertainty Estimation in MD](#32a5)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Materials & Proteins: CLIP for proteins, Ewald Message Passing, Equivariant
    Augmentations](#1ff6)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Cool Applications: Algorithmic reasoning, Inductive KG completion, GNNs for
    mass spectra](#1891)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[The Concluding Meme Part](#5eb2)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Graph Transformers: Sparser, Faster, and Directed**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We [presented](/graphgps-navigating-graph-transformers-c2cc223a051c) **GraphGPS**
    about a year ago and it is pleasing to see many ICML papers building upon our
    framework and expanding GT capabilities even further.
  prefs: []
  type: TYPE_NORMAL
- en: '**‚û°Ô∏è Exphormer** by [Shirzad, Velingker, Venkatachalam et al](https://openreview.net/forum?id=3Ge74dgjjU)
    adds a missing piece of graph-motivated sparse attention to GTs: instead of BigBird
    or Performer (originally designed for sequences), Exphormer‚Äôs attention builds
    upon 1-hop edges, virtual nodes (connected to all nodes in a graph), and a neat
    idea of [expander edges](https://en.wikipedia.org/wiki/Expander_graph). Expander
    graphs have a constant degree and are shown to approximate fully-connected graphs.
    All components combined, attention costs *O(V+E)* instead of *O(V¬≤)*. This allows
    Exphormer to outperform GraphGPS almost everywhere and scale to really large graphs
    of up to 160k nodes. Amazing work and all chances to make Exphormer the standard
    sparse attention mechanism in GTs üëè.'
  prefs: []
  type: TYPE_NORMAL
- en: '**‚û°Ô∏è** Concurrently with graph transformers, expander graphs can already be
    used to enhance the performance of any MPNN architecture as shown in [Expander
    Graph Propagation](https://arxiv.org/abs/2210.02997) by *Deac, Lackenby, and Veliƒçkoviƒá*.'
  prefs: []
  type: TYPE_NORMAL
- en: In a similar vein, [Cai et al](https://openreview.net/forum?id=1EuHYKFPgA) show
    that MPNNs with virtual nodes can approximate linear Performer-like attention
    such that even classic GCN and GatedGCN imbued with virtual nodes show pretty
    much a SOTA performance in long-range graph tasks (we [released](/lrgb-long-range-graph-benchmark-909a6818f02c)
    the [LGRB benchmark](https://github.com/vijaydwivedi75/lrgb) last year exactly
    for measuring long-range capabilities of GNNs and GTs).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/70f4870bdd013ec69ff15c45339cc653.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Source: [Shirzad, Velingker, Venkatachalam et al](https://openreview.net/forum?id=3Ge74dgjjU)'
  prefs: []
  type: TYPE_NORMAL
- en: '**‚û°Ô∏è** A few **patch-based** subsampling approaches for GTs inspired by vision
    models: [**‚ÄúA Generalization of ViT/MLP-Mixer to Graphs‚Äù**](https://openreview.net/forum?id=l7yTbEWuOQ)
    by *He et al* split the input into several patches, encode each patch with a GNN
    into a token, and run a transformer over those tokens.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e3818ed7d6eef99eb892d922370b497d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Source: [‚ÄúA Generalization of ViT/MLP-Mixer to Graphs‚Äù](https://openreview.net/forum?id=l7yTbEWuOQ)
    by He et al'
  prefs: []
  type: TYPE_NORMAL
- en: In **GOAT** by [Kong et al](https://openreview.net/forum?id=Le2dVIoQun), node
    features are projected into a codebook of K clusters with K-Means, and a sampled
    3-hop neighborhood of each node attends to the codebook. GOAT is a 1-layer model
    and scales to graphs of millions of nodes.
  prefs: []
  type: TYPE_NORMAL
- en: '**‚û°Ô∏è Directed graphs** got some transformer love as well üíó. [**‚ÄúTransformers
    Meet Directed Graphs‚Äù**](https://openreview.net/forum?id=a7PVyayyfp) by *Geisler
    et al* introduces Magnetic Laplacian ‚Äî a generalization of a Laplacian for directed
    graphs with a non-symmetric adjacency matrix. Eigenvectors of the Magnetic Laplacian
    paired with directed random walks are strong input features for the transformer
    that enable setting a new SOTA on the [OGB Code2](https://ogb.stanford.edu/docs/leader_graphprop/#ogbg-code2)
    graph property prediction dataset by a good margin!'
  prefs: []
  type: TYPE_NORMAL
- en: üèÖ Last but not least, we have a new SOTA GT on the community standard ZINC dataset
    ‚Äî **GRIT** by [Ma, Lin, et al](https://openreview.net/forum?id=HjMdlNgybR) incorporates
    the full *d*-dimensional random walk matrix, coined as relative random walk probabilities
    (RRWP), as edge features to the attention computation (for comparison, popular
    [RWSE](https://openreview.net/forum?id=wTTjnvGphYj) features are just the diagonal
    elements of this matrix). RRWP are provably more powerful than shortest path distance
    features and set a record-low 0.059 MAE on ZINC (down from 0.070 by GraphGPS).
    GRIT often outperforms GPS in other benchmarks as well üëè. In a similar vein, [Eliasof
    et al](https://openreview.net/forum?id=1Nx2n1lk5T) propose a neat idea to combine
    random and spectral features as positional encodings that outperform RWSE but
    were not tried with GTs.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5dbb993c02b7139828ad024f7a1d9328.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author.
  prefs: []
  type: TYPE_NORMAL
- en: '**Theory: VC dimension of GNNs, deep dive into over-squashing**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**‚û°Ô∏è** [VC dimension](https://en.wikipedia.org/wiki/Vapnik%E2%80%93Chervonenkis_dimension)
    measures model capacity and expressiveness. It is studied well for classical ML
    algorithms but, surprisingly, has never been applied to study GNNs. In[**‚ÄúWL meet
    VC‚Äù**](https://openreview.net/forum?id=rZN3mc5m3C) by *Morris et al*, the connection
    between the WL test and VC dimension is finally uncovered ‚Äî turns out it the VC
    dimension can be bounded by the bitlength of GNN weights, i.e., float32 weights
    would imply the VC dimension of 32\. Furthermore, the VC dimension depends logarithmically
    on the number of unique WL colors in the given task and polynomially on the depth
    and number of layers. This is a great theoretical result and I‚Äôd encourage you
    to have a look!'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3e0e2bb4b10259d188b9ce9b748796f3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Source: [‚ÄúWL meet VC‚Äù](https://openreview.net/forum?id=rZN3mc5m3C) by *Morris
    et al*'
  prefs: []
  type: TYPE_NORMAL
- en: üçäüñêÔ∏è The over-squashing effect ‚Äî information loss when you try to stuff messages
    from too many neighboring nodes ‚Äî is another common problem of MPNNs, and we don‚Äôt
    fully understand how to properly deal with it. This year, there were 3 papers
    dedicated to this topic. Perhaps the most foundational is the work by [**Di Giovanni
    et al**](https://openreview.net/forum?id=t2tTfWwAEl) that explains how MPNNs width,
    depth, and graph topology affect over-squashing.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9b93ed516702c4448b7a0a9772541cdd.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Source: [**Di Giovanni et al**](https://openreview.net/forum?id=t2tTfWwAEl)'
  prefs: []
  type: TYPE_NORMAL
- en: Turns out that **width** might help (but with generalization issues), **depth**
    does **not** really help, and **graph topology** (characterized by the commute
    time between nodes) plays the most important role. We can reduce the commute time
    by various *graph rewiring* strategies (adding and removing edges based on spatial
    or spectral properties), and there are many of them (you might have heard about
    the [Ricci flow-based rewiring](https://openreview.net/forum?id=7UmjRGzp-A) that
    took home the Outstanding Paper award at ICLR 2022). In fact, there is a [follow-up
    work](https://arxiv.org/abs/2306.03589) to this study that goes even deeper and
    derives some impossibility statements wrt over-squashing and some MPNN properties
    ‚Äî I‚Äôd highly encourage to read it as well!
  prefs: []
  type: TYPE_NORMAL
- en: '**‚û°Ô∏è** Effective resistance is one example of spatial rewiring strategies,
    and [**Black et al**](https://openreview.net/forum?id=50SO1LwcYU)study it in great
    detail. The Ricci flow-based rewiring works with graph curvature and is studied
    further in the work by [Nguyen et al](https://openreview.net/forum?id=eWAvwKajx2).'
  prefs: []
  type: TYPE_NORMAL
- en: '**‚û°Ô∏è** Subgraph GNNs continue to be in the spotlight: two works ([**Zhang,
    Feng, Du, et al**](https://openreview.net/forum?id=2Hp7U3k5Ph) and [**Zhou, Wang,
    Zhang**](https://openreview.net/forum?id=K07XAlzh5i)) concurrently derive expressiveness
    hierarchies of the recently proposed subgraph GNNs and their relationship to the
    1- and higher-order WL tests.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ba83d721dae2076255c799aaeeb07f6b.png)'
  prefs: []
  type: TYPE_IMG
- en: Image By Author.
  prefs: []
  type: TYPE_NORMAL
- en: '**New GNN architectures: Delays and Half-hops**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'If you are tired of yet another variation of GCN or GAT, here are some fresh
    ideas that can work with any GNN of your choice:'
  prefs: []
  type: TYPE_NORMAL
- en: '‚è≥ As we know from the **Theory** section, rewiring helps combat over-squashing.
    [**Gutteridge et al**](https://openreview.net/forum?id=WEgjbJ6IDN) introduce *‚ÄúDRew:
    Dynamically Rewired Message Passing with Delay‚Äù* which gradually densifies the
    graph in later GNN layers such that long-distance nodes see the original states
    of previous nodes (the original DRew) or those skip-connections are added based
    on the *delay* ‚Äî depending on a distance between two nodes (the vDRew version).
    For example ( üñºÔ∏èüëá), in vDRew delayed message passing, a starting node from layer
    0 will show its state to 2-hop neighbors on layer 1, and will show its state to
    a 3-hop neighbor on layer 2\. **DRew** significantly improves the ability of vanilla
    GNNs to perform long-range tasks ‚Äî in fact, a DRew-enabled GCN is the current
    [SOTA](https://github.com/vijaydwivedi75/lrgb) on the Peptides-func dataset from
    the [Long Range Graph Benchmark](https://github.com/vijaydwivedi75/lrgb) üëÄ'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8089fde4f860732baf4f33dc17a967be.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Source: [**Gutteridge et al**](https://openreview.net/forum?id=WEgjbJ6IDN)'
  prefs: []
  type: TYPE_NORMAL
- en: ü¶ò Another neat idea by [**Azabou et al**](https://openreview.net/forum?id=lXczFIwQkv)
    is to slow down message passing by inserting new, *slow nodes* at each edge with
    a special connectivity pattern ‚Äî only an incoming connection from the starting
    node and a symmetric edge with the destination node. Slow nodes improve the performance
    of vanilla GNNs on heterophilic benchmarks by a large margin, and it is also possible
    to use slow nodes for self-supervised learning by creating views with different
    locations of slow nodes for the same original graph. **HalfHop** is a no-brainer-to-include
    SSL component that boosts performance and should be in a standard suite of many
    GNN libraries üëç.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/05240ff796d9ab8c9f9984a30a0e5dda.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Source: [**Azabou et al**](https://openreview.net/forum?id=lXczFIwQkv)'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d16ae29ccd4c9d38a6e3081cc4edf72c.png)'
  prefs: []
  type: TYPE_IMG
- en: Image By Author.
  prefs: []
  type: TYPE_NORMAL
- en: '**Generative Models ‚Äî Stable Diffusion for Molecules, Discrete Diffusion**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**‚û°Ô∏è** Diffusion models might work in the **feature** space (e.g., pixel space
    in image generation like the original DDPM) or in the **latent** space (like Stable
    Diffusion). In the feature space, you have to design the noising process to respect
    symmetries and equivariances of your feature space. In the latent space, you can
    just add Gaussian noise to the features produced by (pre-trained) encoder. Most
    3D molecule generation models work in the feature space (like a pioneering [EDM](https://arxiv.org/abs/2203.17003)),
    and the new **GeoLDM** model by [Xu et al](https://openreview.net/forum?id=sLfHWWrfe2)
    (authors of the prominent [GeoDiff](https://arxiv.org/abs/2203.02923)) is the
    first to define **latent** diffusion for 3D molecule generation. That is, after
    training an EGNN autoencoder, GeoLDM is trained on the denoising objective where
    noise is sampled from a standard Gaussian. GeoLDM brings significant improvements
    over EDM and other non-latent diffusion approaches üëè.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0ff52f569c070f1ff4ed28af64260f86.png)'
  prefs: []
  type: TYPE_IMG
- en: 'GeoLDM. Source: [Xu et al](https://openreview.net/forum?id=sLfHWWrfe2)'
  prefs: []
  type: TYPE_NORMAL
- en: '**‚û°Ô∏è** In the realm of non-geometric graphs (just with an adjacency and perhaps
    categorical node features), discrete graph diffusion pioneered by [DiGress](https://openreview.net/forum?id=UaAD-Nu86WX)
    (ICLR‚Äô23) seems the most applicable option. [Chen et al](https://openreview.net/forum?id=vn9O1N5ZOw)
    propose **EDGE,** a discrete diffusion model guided by the node degree distribution.
    In contrast to DiGress, the final target graph in EDGE is a disconnected graph
    without edges, a forward noising model removes edges through a Bernoulli distribution,
    and a reverse process adds edges to the most recent *active* nodes (active are
    the nodes whose degrees changed in the previous step). Thanks to the sparsity
    introduced by the degree guidance, EDGE can generate pretty large graphs up to
    4k nodes and 40k edges!'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/80698f82635bb17812b19baa5e974b13.png)'
  prefs: []
  type: TYPE_IMG
- en: Graph Generation with EDGE. Source:[Chen et al](https://openreview.net/forum?id=vn9O1N5ZOw)
  prefs: []
  type: TYPE_NORMAL
- en: '**‚û°Ô∏è** Finally, [**‚ÄúGraphically Structured Diffusion Models‚Äù**](https://openreview.net/forum?id=24wzmwrldX)
    by *Weilbach et al* bridges the gap between continuous generative models and probabilistic
    graphical models that induce a certain structure in the problem of interest ‚Äî
    often such problems have a combinatorial nature. The central idea is to encode
    the problem‚Äôs structure as an attention mask that respects permutation invariances
    and use this mask in the attention computation in the Transformer encoder (which
    by definition is equivariant to input token permutation unless you use positional
    embeddings). **GSDM** can tackle binary continuous matrix factorization, boolean
    circuits, can generate sudokus, and perform sorting. Particularly enjoyable is
    a pinch of irony the paper is written with üôÉ.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e620b5450f6622c4e491e7a640999da5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'GSDM task-to-attention-bias. Source: [**‚ÄúGraphically Structured Diffusion Models‚Äù**](https://openreview.net/forum?id=24wzmwrldX)
    by *Weilbach et al*'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7385dbac39148c3f404eed5abcd3b871.png)'
  prefs: []
  type: TYPE_IMG
- en: Image By Author
  prefs: []
  type: TYPE_NORMAL
- en: '**Geometric Learning: Geometric WL, Clifford Algebras**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Geometric Deep Learning thrives! There were so many interesting papers presented
    that would take pretty much the whole post, so I‚Äôd highlight only a few.
  prefs: []
  type: TYPE_NORMAL
- en: '**‚û°Ô∏è Geometric WL** has finally arrived in the work by [Joshi, Bodnar, et al](https://openreview.net/forum?id=6Ed3gchl9L).
    Geometric WL extends the notion of the WL test with geometric features (e.g.,
    coordinates or velocity) and derives the expressiveness hierarchy up to k-order
    GWL. Key takeaways: 1Ô∏è‚É£ **equivariant** models are more expressive than **invariant**
    (with a note that in fully connected graphs the difference disappears), 2Ô∏è‚É£ **tensor
    order** of features improves expressiveness, 3Ô∏è‚É£ **body order** of features improves
    expressiveness (see the image üëá). That is, *spherical > cartesian > scalars*,
    and *many-body interactions > just distances*. The paper also features the amazing
    learning source [Geometric GNN Dojo](https://github.com/chaitjo/geometric-gnn-dojo)
    where you can derive and implement most SOTA models from the first principles!'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1a31d31c426441c200f0f409dda8d99b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Source: [Joshi, Bodnar, et al](https://openreview.net/forum?id=6Ed3gchl9L)'
  prefs: []
  type: TYPE_NORMAL
- en: '**‚û°Ô∏è** Going beyond vectors to Clifford algebras, [Ruhe et al](https://openreview.net/forum?id=DNAJdkHPQ5)
    derive **Geometric Clifford Algebra Networks** (GCANs). Clifford algebras naturally
    support higher-order interactions by means of bivectors, trivectors, and (in general)
    multivectors. The key idea is the [Cartan-Dieudonn√© theorem](https://en.wikipedia.org/wiki/Cartan%E2%80%93Dieudonn%C3%A9_theorem)
    that every orthogonal transformation can be decomposed into *reflections* in hyperplanes,
    and geometric algebras represent data as the elements of the *Pin(p,q,r)* group.
    GCANs introduce a notion of linear layers, normalizations, non-linearities, and
    how they can be parameterized with neural networks. Experiments include modeling
    fluid dynamics and Navier-Stokes equations.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/27b33c8d8cda800ff13af41870110670.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Source: [Ruhe et al](https://openreview.net/forum?id=DNAJdkHPQ5)'
  prefs: []
  type: TYPE_NORMAL
- en: In fact, there is already a [follow-up work](https://arxiv.org/abs/2305.11141)
    introducing equivariant Clifford NNs ‚Äî you can learn more about Clifford algebras
    foundations and the most recent papers on [CliffordLayers](https://microsoft.github.io/cliffordlayers/)
    supported by Microsoft Research.
  prefs: []
  type: TYPE_NORMAL
- en: üíä [Equivariant GNN](http://proceedings.mlr.press/v139/satorras21a/satorras21a.pdf)
    (EGNN) is the Aspirin of Geometric DL that gets applied to almost every task and
    has seen quite a number of improvements. [**Eijkelboom et al**](https://openreview.net/forum?id=hF65aKF8Bf)
    marry EGNN with [Simplicial networks](https://arxiv.org/abs/2103.03212) that operate
    on higher-order structures (namely, simplicial complexes) into **EMPSN**. This
    is one of the first examples that combines geometric and topological features
    and has great improvement potential! Finally, [**Passaro and Zitnick**](https://openreview.net/forum?id=QIejMwU0r9)
    derive a neat trick to reduce SO(3) convolutions to SO(2) bringing the complexity
    down from O(L‚Å∂) to O(L¬≥) but with mathematical equivalence guarantees üëÄ. This
    finding allows to scale up geometric models on larger datasets like OpenCatalyst
    and already made it to [Equiformer V2](https://arxiv.org/abs/2306.12059) ‚Äî soon
    in many other libraries for geometric models üòâ
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/211f7d576a50de6991e79cca291a5ac4.png)'
  prefs: []
  type: TYPE_IMG
- en: Image By Author.
  prefs: []
  type: TYPE_NORMAL
- en: '**Molecules: 2D-3D pretraining, Uncertainty Estimation in MD**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**‚û°Ô∏è** [Liu, Du, et al](https://openreview.net/forum?id=mPEVwu50th) propose
    **MoleculeSDE**, a new framework for joint 2D-3D pretraining on molecular data.
    In addition to standard contrastive loss, the authors add two **generative** components:
    reconstructing 2D -> 3D and 3D -> 2D inputs through the score-based diffusion
    generation. Using standard GIN and SchNet as 2D and 3D models, MoleculeSDE is
    pre-trained on PCQM4M v2 and performs well on downstream fine-tuning tasks.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/36ea80c8fb548a5e0c318b4447aa0425.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Source: [MoleculeSDE Github repo](https://github.com/chao1224/MoleculeSDE)'
  prefs: []
  type: TYPE_NORMAL
- en: '**‚û°Ô∏è** [Wollschl√§ger et al](https://openreview.net/forum?id=DjwMRloMCO) perform
    a comprehensive study of Uncertainty Estimation in GNNs for molecular dynamics
    and force fields. Identifying key physics-informed and application-focused principles,
    the authors propose a **Localized Neural Kernel**, a Gaussian Process-based extension
    to any geometric GNN that works on invariant and equivariant quantities (tried
    on SchNet, DimeNet, and NequIP). In many cases, LNK‚Äôs estimations from one model
    are on par with or better than costly ensembling where you‚Äôd need to train several
    models.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c44e143e7fcd2be4bb3a3c5e1a0b13c8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Source: [Wollschl√§ger et al](https://openreview.net/forum?id=DjwMRloMCO)'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/72f1a4c6b25d78630777885379b512c2.png)'
  prefs: []
  type: TYPE_IMG
- en: Image By Author.
  prefs: []
  type: TYPE_NORMAL
- en: '**Materials & Proteins: CLIP for proteins, Ewald Message Passing, Equivariant
    Augmentations**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: CLIP and its descendants have become a standard staple in text-to-image models.
    Can we do the same but for text-to-protein? Yes!
  prefs: []
  type: TYPE_NORMAL
- en: '**‚û°Ô∏è** [Xu, Yuan, et al](https://openreview.net/forum?id=ZOOwHgxfR4) present
    **ProtST**, a framework for learning joint representations of text protein descriptions
    (via PubMedBERT) and protein sequences (via ESM). In addition to a contrastive
    loss, ProtST has a multimodal mask prediction objective, e.g., masking 15% of
    tokens in text and protein sequence, and predicting those jointly based on latent
    representations, and mask prediction losses based on sequences or language alone.
    Additionally, the authors design a novel **ProtDescribe** dataset with 550K aligned
    protein sequence-description pairs. **ProtST** excels across many protein modeling
    tasks in the [**PEER**](https://github.com/DeepGraphLearning/PEER_Benchmark) benchmark,
    including protein function annotation and localization, but also allows for zero-shot
    protein retrieval right from the textual description (see an example below). Looks
    like **ProtST** has a bright future of being a backbone behind many protein generative
    models üòâ'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/724581ab255403ddca3fe2aebda87a1d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Source: [Xu, Yuan, et al](https://openreview.net/forum?id=ZOOwHgxfR4)'
  prefs: []
  type: TYPE_NORMAL
- en: Actually, ICML features several protein generation works like **GENIE** by [Lin
    and AlQuraishi](https://openreview.net/forum?id=4Kw5hKY8u8) and **FrameDiff**
    by [Yim, Trippe, De Bortoli, Mathieu, et al](https://openreview.net/forum?id=m8OUBymxwv)
    ‚Äî those are not yet conditioned on textual descriptions, so incorporating ProtST
    there looks like a no-brainer performance boost üìà.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/acfc681ee5d4ad4efc9a0d11266aae01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Gif Source: [SE(3) Diffusion Github](https://github.com/jasonkyuyim/se3_diffusion)'
  prefs: []
  type: TYPE_NORMAL
- en: ‚öõÔ∏è MPNNs on molecules have a strict locality bias that inhibits modeling long-range
    interactions. [Kosmala et al](https://openreview.net/forum?id=vd5JYAml0A) derive
    **Ewald Message Passing** and apply the idea of [Ewald summation](https://en.wikipedia.org/wiki/Ewald_summation)
    that breaks down the interaction potential into short-range and long-range terms.
    Short-range interaction is modeled by any GNN while long-range interaction is
    new and is modeled with a **3D Fourier transform** and message passing over Fourier
    frequencies. Turns out this long-range term is pretty flexible and can be applied
    to any network modeling periodic and aperiodic systems (like crystals or molecules)
    like SchNet, DimeNet, or GemNet. The model was evaluated on OC20 and OE62 datasets.
    If you are interested in more details, check out the [1-hour talk by Arthur Kosmala](https://www.youtube.com/watch?v=Ip8EGde5SUQ)
    at the LOG2 Reading Group!
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5d9f656fc6565a98a0b8db30a1aca1ba.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Source: [Kosmala et al](https://openreview.net/forum?id=vd5JYAml0A)'
  prefs: []
  type: TYPE_NORMAL
- en: A similar idea of using Ewald summation for 3D crystals is used in **PotNet**
    by [Lin et al](https://openreview.net/forum?id=jxI4CulNr1) where the long-range
    connection is modeled with incomplete Bessel functions. PotNet was evaluated on
    the Materials Project dataset and JARVIS ‚Äî so reading those two papers you can
    have a good understanding of the benefits brought by Ewald summation for many
    crystal-related tasks üòâ
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2c37a782852ab0a7c1e90efe7aea17d7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Source: [Lin et al](https://openreview.net/forum?id=jxI4CulNr1)'
  prefs: []
  type: TYPE_NORMAL
- en: '**‚û°Ô∏è** Another look at imbuing *any* GNNs with equivariance for crystals and
    molecules is given by [Duval, Schmidt, et al](https://openreview.net/forum?id=HRDRZNxQXc)
    in **FAENet**. A standard way is to bake certain symmetries and equivariances
    right into GNN architectures (like in EGNN, GemNet, and Ewald Message Passing)
    ‚Äî this is a safe but computationally expensive way (especially when it comes to
    spherical harmonics and tensor products). Another option often used in vision
    ‚Äî show many augmentations of the same input and the model should eventually learn
    the same invariances in the augmentations. The authors go for the 2nd path and
    design a rigorous way to sample 2D / 3D data invariant or equivariant augmentations
    (e.g., for energy or forces, respectively) all with fancy proofs ‚úçÔ∏è. For that,
    the data augmentation pipeline includes projecting 2D / 3D inputs to a canonical
    representation (based on PCA of the covariance matrix of distances) from which
    we can uniformly sample rotations.'
  prefs: []
  type: TYPE_NORMAL
- en: The proposed FAENet is a simple model that uses only distances but shows very
    good performance with the stochastic frame averaging data augmentation while being
    6‚Äì20 times faster. Works for crystal structures as well!
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/893a76e8a13595d1073078d444759d13.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Augmentations and Stochastic Frame Averaging. Source: [Duval, Schmidt, et al](https://openreview.net/forum?id=HRDRZNxQXc)'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/45f447e910299d52cd636e7cb49cf723.png)'
  prefs: []
  type: TYPE_IMG
- en: Image By Author.
  prefs: []
  type: TYPE_NORMAL
- en: '**Cool Applications: Algorithmic Reasoning, Inductive KG Completion, GNNs for
    Mass Spectra**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A few papers in this section did not belong to any of the above but are still
    worthy of your attention.
  prefs: []
  type: TYPE_NORMAL
- en: '**‚û°Ô∏è** [**‚ÄùNeural Algorithmic Reasoning with Causal Regularisation‚Äù**](https://openreview.net/forum?id=kP2p67F4G7)
    by *Bevilacqua et al* tackles a common issue in graph learning ‚Äî OOD generalization
    to larger inputs at test time. Studying OOD generalization in algorithmic reasoning
    problems, the authors observe that there exist many different inputs that make
    identical computations at a certain step. At the same time, it means that some
    subset of inputs does not (should not) affect the prediction result. This assumption
    allows to design a self-supervised objective (termed **Hint-ReLIC**) that prefers
    a ‚Äúmeaningful‚Äù step to a bunch of steps that do not affect the prediction result.
    The new objective significantly bumps the performance on many CLRS-30 tasks to
    90+% micro-F1\. It is an interesting question whether we could leverage the same
    principle in general message passing and improve OOD transfer in other graph learning
    tasks ü§î'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/41ff0b6d703cdfba98fb986c679f46b4.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Source: [**‚ÄùNeural Algorithmic Reasoning with Causal Regularisation‚Äù**](https://openreview.net/forum?id=kP2p67F4G7)
    by *Bevilacqua et al*'
  prefs: []
  type: TYPE_NORMAL
- en: If you are further interested in neural algorithmic reasoning, check out the
    proceedings of the [Knowledge and Logical Reasoning workshop](https://klr-icml2023.github.io/papers.html)
    which has even more works on that topic.
  prefs: []
  type: TYPE_NORMAL
- en: '**‚û°Ô∏è** [**‚ÄúInGram: Inductive Knowledge Graph Embedding via Relation Graphs‚Äù**](https://openreview.net/forum?id=OoOpO0u4Xd)
    by *Lee et al* seems to be one of the very few knowledge graph papers at ICML‚Äô23
    (to the best of my search). **InGram** is one of the first approaches that can
    inductively generalize to both unseen entities and **unseen relations** at test
    time. Previously, inductive KG models needed to learn at least relation embeddings
    in some form to generalize to new nodes, and in this paradigm, new unseen relations
    are non-trivial to model. InGram builds a relation graph on top of the original
    multi-relational graph, that is, a graph of relation types, and learns representations
    of relations based on this graph by running a GAT. Entity representations are
    obtained from the random initialization and a GNN encoder. Having both entity
    and relation representations, a DistMult decoder is applied as a scoring function.
    There are good chances that InGram for unseen relations might be as influential
    as [GraIL (ICML 2020)](http://proceedings.mlr.press/v119/teru20a/teru20a.pdf)
    for unseen entities üòâ.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6d1039c15f9e6f9a04a02604d808dda2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Source: [**‚ÄúInGram: Inductive Knowledge Graph Embedding via Relation Graphs‚Äù**](https://openreview.net/forum?id=OoOpO0u4Xd)
    by *Lee et al*'
  prefs: []
  type: TYPE_NORMAL
- en: üåà [**‚ÄùEfficiently predicting high resolution mass spectra with graph neural
    networks‚Äù**](https://openreview.net/forum?id=81RIPI742h) by *Murphy et al* is
    a cool application of GNNs to a real physics problem of predicting mass spectra.
    The main finding is that most of the signal in mass spectra is explained by a
    small number of components (product ion and neutral loss *formulas*). And it is
    possible to mine a vocabulary of those *formulas* from training data. The problem
    can thus be framed as graph classification (or graph property prediction) when,
    given a molecular graph, we predict tokens from a vocabulary that correspond to
    certain mass spectra values. The approach, **GRAFF-MS**, builds molecular graph
    representation through GIN with edge features, with Laplacian features (via SignNet),
    and pooled with covariate features. Compared to the baseline CFM-ID, GRAFF-MS
    performs inference in ~19 minutes compared to 126 hours reaching much higher performance
    üëÄ.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/594717432f130e81320f52d0aae3cda5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Source: [**‚ÄùEfficiently predicting high resolution mass spectra with graph
    neural networks‚Äù**](https://openreview.net/forum?id=81RIPI742h) by *Murphy et
    al*'
  prefs: []
  type: TYPE_NORMAL
- en: The Concluding Meme Part
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/ea3760ec5d97e92fb80054a68be94867.png)'
  prefs: []
  type: TYPE_IMG
- en: Four Michaels (+ epsilon in the background) on the same photo!
  prefs: []
  type: TYPE_NORMAL
- en: The meme of 2022 has finally converged to [Michael Bronstein](https://michael-bronstein.medium.com/)!
  prefs: []
  type: TYPE_NORMAL
