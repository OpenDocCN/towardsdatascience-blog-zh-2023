- en: A Deep Dive into the Code of the Visual Transformer (ViT) Model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/a-deep-dive-into-the-code-of-the-visual-transformer-vit-model-1ce4cc05ca8d?source=collection_archive---------5-----------------------#2023-08-15](https://towardsdatascience.com/a-deep-dive-into-the-code-of-the-visual-transformer-vit-model-1ce4cc05ca8d?source=collection_archive---------5-----------------------#2023-08-15)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Breaking down the HuggingFace ViT Implementation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@alexml0123?source=post_page-----1ce4cc05ca8d--------------------------------)[![Alexey
    Kravets](../Images/3b31f9b3c73c6c7ca709f845e6f70023.png)](https://medium.com/@alexml0123?source=post_page-----1ce4cc05ca8d--------------------------------)[](https://towardsdatascience.com/?source=post_page-----1ce4cc05ca8d--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----1ce4cc05ca8d--------------------------------)
    [Alexey Kravets](https://medium.com/@alexml0123?source=post_page-----1ce4cc05ca8d--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fcf3e4a05b535&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-deep-dive-into-the-code-of-the-visual-transformer-vit-model-1ce4cc05ca8d&user=Alexey+Kravets&userId=cf3e4a05b535&source=post_page-cf3e4a05b535----1ce4cc05ca8d---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----1ce4cc05ca8d--------------------------------)
    ·14 min read·Aug 15, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F1ce4cc05ca8d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-deep-dive-into-the-code-of-the-visual-transformer-vit-model-1ce4cc05ca8d&user=Alexey+Kravets&userId=cf3e4a05b535&source=-----1ce4cc05ca8d---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F1ce4cc05ca8d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-deep-dive-into-the-code-of-the-visual-transformer-vit-model-1ce4cc05ca8d&source=-----1ce4cc05ca8d---------------------bookmark_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: Vision Transformer (ViT) stands as a remarkable milestone in the evolution of
    computer vision. ViT challenges the conventional wisdom that images are best processed
    through convolutional layers, proving that sequence-based attention mechanisms
    can effectively capture the intricate patterns, context, and semantics present
    in images. By breaking down images into manageable patches and leveraging self-attention,
    ViT captures both local and global relationships, enabling it to excel in diverse
    vision tasks, from image classification to object detection and beyond. In this
    article, we are going to break down how ViT for classification works under the
    hood.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ac2ea1bf46319ca84c69d7fe2d300fa5.png)'
  prefs: []
  type: TYPE_IMG
- en: '[https://unsplash.com/photos/aVvZJC0ynBQ](https://unsplash.com/photos/aVvZJC0ynBQ)'
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The core idea of ViT is to treat an image as a sequence of fixed-size patches,
    which are then flattened and converted into 1D vectors. These patches are subsequently
    processed by a transformer encoder, which enables the model to capture global
    context and dependencies across the entire image. By dividing the image into patches,
    ViT effectively reduces the computational complexity of handling large images
    while retaining the ability to model complex spatial interactions.
  prefs: []
  type: TYPE_NORMAL
