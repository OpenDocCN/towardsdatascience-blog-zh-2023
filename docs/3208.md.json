["```py\npip install -q -U bitsandbytes\npip install -q -U transformers\npip install -q -U peft\npip install -q -U accelerate\npip install -q -U datasets\npip install -q -U trl\n```", "```py\nimport torch\nfrom datasets import load_dataset\nfrom peft import LoraConfig, PeftModel, prepare_model_for_kbit_training\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    BitsAndBytesConfig,\n    AutoTokenizer,\n    TrainingArguments,\n)\nfrom trl import SFTTrainer\n```", "```py\nmodel_name = \"mistralai/Mistral-7B-v0.1\"\n#Tokenizer\ntokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\ntokenizer.pad_token = tokenizer.unk_token\ntokenizer.pad_token_id =  tokenizer.unk_token_id\ntokenizer.padding_side = 'left'\n```", "```py\n[ \"What percentage of the Earth's surface is covered by oceans?\", \"About 71% of the Earth's surface is covered by oceans.\", \"Wow, that's a lot of water! No wonder we call it the blue planet.\", \"Yes, it certainly is! The oceans play a vital role in regulating the Earth's climate and supporting life on our planet. And they're also a great source of food, energy, and recreation for us humans!\", \"Absolutely! I love visiting the beach and going for a swim in the ocean. It's amazing how vast and powerful the sea can be, yet also so peaceful and calming.\", \"As an AI language model, I have never gone to the beach or swam in the ocean, but based on what you said, I am sure it's a wonderful experience. The ocean is a place of great beauty and mystery, full of fascinating creatures and hidden treasures waiting to be discovered. It can be both a source of wonder and a reminder of the awesome power of nature.\" ]\n```", "```py\n### Human: What percentage of the Earth's surface is covered by oceans?### Assistant: About 71% of the Earth's surface is covered by oceans###Human: Wow, that's a lot of water! No wonder we call it the blue planet### Assistant: Yes, it certainly is! The oceans play a vital role in regulating the Earth's climate and supporting life on our planet. And they're also a great source of food, energy, and recreation for us humans!### Human: Absolutely! I love visiting the beach and going for a swim in the ocean. It's amazing how vast and powerful the sea can be, yet also so peaceful and calming### Assistant: As an AI language model, I have never gone to the beach or swam in the ocean, but based on what you said, I am sure it's a wonderful experience. The ocean is a place of great beauty and mystery, full of fascinating creatures and hidden treasures waiting to be discovered. It can be both a source of wonder and a reminder of the awesome power of nature\n```", "```py\ndataset = load_dataset(\"kaitchup/ultrachat-100k-flattened\")\n```", "```py\n#Quantization configuration\ncompute_dtype = getattr(torch, \"float16\")\nbnb_config = BitsAndBytesConfig(\n        load_in_4bit=True,\n        bnb_4bit_quant_type=\"nf4\",\n        bnb_4bit_compute_dtype=compute_dtype,\n        bnb_4bit_use_double_quant=True,\n)\n#Load the model and quantize it on the fly\nmodel = AutoModelForCausalLM.from_pretrained(\n          model_name, quantization_config=bnb_config, device_map={\"\": 0}\n)\n\n#Cast some modules of the model to fp32 \nmodel = prepare_model_for_kbit_training(model)\n\n#Configure the pad token in the model\nmodel.config.pad_token_id = tokenizer.pad_token_id\nmodel.config.use_cache = False # Gradient checkpointing is used by default but not compatible with caching\n```", "```py\npeft_config = LoraConfig(\n        lora_alpha=16,\n        lora_dropout=0.05,\n        r=16,\n        bias=\"none\",\n        task_type=\"CAUSAL_LM\",\n        target_modules= ['k_proj', 'q_proj', 'v_proj', 'o_proj', \"gate_proj\", \"down_proj\", \"up_proj\"]\n)\n```", "```py\ntraining_arguments = TrainingArguments(\n        output_dir=\"./results\",\n        #evaluation_strategy=\"steps\",\n        #do_eval=True,\n        optim=\"paged_adamw_8bit\",\n        per_device_train_batch_size=4,\n        gradient_accumulation_steps=4,\n        per_device_eval_batch_size=4,\n        log_level=\"debug\",\n        save_steps=20,\n        logging_steps=10,\n        learning_rate=4e-4,\n        #eval_steps=200,\n        #num_train_epochs=1,\n        max_steps=100,\n        warmup_steps=100,\n        lr_scheduler_type=\"linear\",\n)\n```", "```py\ntrainer = SFTTrainer(\n        model=model,\n        train_dataset=dataset['train'],\n        #eval_dataset=dataset['test'],\n        peft_config=peft_config,\n        dataset_text_field=\"text\",\n        max_seq_length=512,\n        tokenizer=tokenizer,\n        args=training_arguments,\n)\n\ntrainer.train()\n```", "```py\nfrom transformers import GenerationConfig\nmodel.config.use_cache = True\nmodel = PeftModel.from_pretrained(model, \"./results/checkpoint-100/\")\ndef generate(instruction):\n    prompt = \"### Human: \"+instruction+\"### Assistant: \"\n    inputs = tokenizer(prompt, return_tensors=\"pt\")\n    input_ids = inputs[\"input_ids\"].cuda()\n    generation_output = model.generate(\n            input_ids=input_ids,\n            generation_config=GenerationConfig(pad_token_id=tokenizer.pad_token_id, temperature=1.0, top_p=1.0, top_k=50, num_beams=1),\n            return_dict_in_generate=True,\n            output_scores=True,\n            max_new_tokens=256\n    )\n    for seq in generation_output.sequences:\n        output = tokenizer.decode(seq)\n        print(output.split(\"### Assistant: \")[1].strip())\ngenerate(\"Tell me about gravitation.\")\n```", "```py\nmodel = PeftModel.from_pretrained(model, \"./results/checkpoint-100/\")\n```", "```py\npip install -q -U bitsandbytes\npip install -q -U transformers\npip install -q -U accelerate\n```", "```py\nimport torch\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    BitsAndBytesConfig,\n    AutoTokenizer,\n)\n\nmodel_name = \"mistralai/Mistral-7B-Instruct-v0.1\"\n#Tokenizer\ntokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\ntokenizer.pad_token = tokenizer.unk_token\ntokenizer.pad_token_id =  tokenizer.unk_token_id\ntokenizer.padding_side = 'left'\n\n#Quantization configuration\ncompute_dtype = getattr(torch, \"float16\")\nbnb_config = BitsAndBytesConfig(\n        load_in_4bit=True,\n        bnb_4bit_quant_type=\"nf4\",\n        bnb_4bit_compute_dtype=compute_dtype,\n        bnb_4bit_use_double_quant=True,\n)\n#Load the model and quantize it on the fly\nmodel = AutoModelForCausalLM.from_pretrained(\n          model_name, quantization_config=bnb_config, device_map={\"\": 0}\n)\n```", "```py\nfrom transformers import GenerationConfig\n\ndef generate(instruction):\n    prompt = \"[INST] \"+instruction+\" [/INST]\\n\"\n    inputs = tokenizer(prompt, return_tensors=\"pt\")\n    input_ids = inputs[\"input_ids\"].cuda()\n    generation_output = model.generate(\n            input_ids=input_ids,\n            generation_config=GenerationConfig(pad_token_id=tokenizer.pad_token_id, temperature=1.0, top_p=1.0, top_k=50, num_beams=1),\n            return_dict_in_generate=True,\n            output_scores=True,\n            max_new_tokens=256\n    )\n    for seq in generation_output.sequences:\n        output = tokenizer.decode(seq)\n        print(output.split(\"[/INST]\")[1].strip())\ngenerate(\"Tell me about gravity.\")\n```", "```py\npip install -q -U bitsandbytes\npip install -q -U transformers\npip install -q -U accelerate\npip install -q -U optimum\npip install -q -U auto-gptq\n```", "```py\nimport torch\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    GPTQConfig,\n    AutoTokenizer\n)\n```", "```py\nmodel_name = \"mistralai/Mistral-7B-Instruct-v0.1\"\n\ntokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\ntokenizer.pad_token = tokenizer.unk_token\ntokenizer.pad_token_id =  tokenizer.unk_token_id\ntokenizer.padding_side = 'left'\n\nquantization_config = GPTQConfig(bits=4, dataset = \"c4\", tokenizer=tokenizer)\n\nmodel = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\", quantization_config=quantization_config)\n```"]