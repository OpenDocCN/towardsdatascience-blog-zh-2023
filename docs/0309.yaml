- en: A Little Pandas Hack to Handle Large Datasets with Limited Memory
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/a-little-pandas-hack-to-handle-large-datasets-with-limited-memory-6745140f473b?source=collection_archive---------3-----------------------#2023-01-19](https://towardsdatascience.com/a-little-pandas-hack-to-handle-large-datasets-with-limited-memory-6745140f473b?source=collection_archive---------3-----------------------#2023-01-19)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The Pandas defaults aren’t optimal. A tiny configuration can compress your dataframe
    to fit in your memory.
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://thuwarakesh.medium.com/?source=post_page-----6745140f473b--------------------------------)[![Thuwarakesh
    Murallie](../Images/44f1a14a899426592bbd8c7f73ce169d.png)](https://thuwarakesh.medium.com/?source=post_page-----6745140f473b--------------------------------)[](https://towardsdatascience.com/?source=post_page-----6745140f473b--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----6745140f473b--------------------------------)
    [Thuwarakesh Murallie](https://thuwarakesh.medium.com/?source=post_page-----6745140f473b--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F93ce19993bef&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-little-pandas-hack-to-handle-large-datasets-with-limited-memory-6745140f473b&user=Thuwarakesh+Murallie&userId=93ce19993bef&source=post_page-93ce19993bef----6745140f473b---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----6745140f473b--------------------------------)
    ·8 min read·Jan 19, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F6745140f473b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-little-pandas-hack-to-handle-large-datasets-with-limited-memory-6745140f473b&user=Thuwarakesh+Murallie&userId=93ce19993bef&source=-----6745140f473b---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F6745140f473b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-little-pandas-hack-to-handle-large-datasets-with-limited-memory-6745140f473b&source=-----6745140f473b---------------------bookmark_footer-----------)![](../Images/c4b1299e06f03069b94110ff87456e73.png)'
  prefs: []
  type: TYPE_NORMAL
- en: You can compress a huge Pandas dataframe without losing its properties, just
    like squeezing a burger. Save memory and work faster with this little trick. —
    Photo by [Leonardo Luz](https://www.pexels.com/photo/photo-of-a-burger-between-a-person-s-hands-14001304/)
  prefs: []
  type: TYPE_NORMAL
- en: I never thought my code needed improvement. I’ve always complained that I don’t
    have enough memory or the dataset was too big to handle.
  prefs: []
  type: TYPE_NORMAL
- en: My go-to solution was to put them on a Postgres DB and write SQL queries. After
    all, that is an acceptable way to handle large-scale datasets. I kept doing this
    whenever I got a large dataset.
  prefs: []
  type: TYPE_NORMAL
- en: But I don’t get the complete flexibility I get in Python. For this reason, I
    had to combine them and alternate between them. For instance, I load the dataset
    on a SQL database and write a Python script to run SQL queries, often using Sqlalchemy.
  prefs: []
  type: TYPE_NORMAL
- en: While it gives me the flexibility of both worlds, I have issues sharing my code
    with other team members. Other members should have the knowledge and setup for
    relational databases.
  prefs: []
  type: TYPE_NORMAL
- en: The classic solution for this problem is increasing the memory and running tasks
    in parallel. On the infrastructure part, moving to the cloud was my favorite solution.
    I can pay for the high-performance resources only for their usage. And on the
    parallel execution side, technologies like [Dask](https://www.dask.org/) cover
    me.
  prefs: []
  type: TYPE_NORMAL
