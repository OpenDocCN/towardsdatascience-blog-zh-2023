- en: 'TiDE: the ‘embarrassingly’ simple MLP that beats Transformers'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: TiDE：那个“令人尴尬”简单的MLP，击败了Transformers
- en: 原文：[https://towardsdatascience.com/tide-the-embarrassingly-simple-mlp-that-beats-transformers-7db77d588079?source=collection_archive---------1-----------------------#2023-12-08](https://towardsdatascience.com/tide-the-embarrassingly-simple-mlp-that-beats-transformers-7db77d588079?source=collection_archive---------1-----------------------#2023-12-08)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/tide-the-embarrassingly-simple-mlp-that-beats-transformers-7db77d588079?source=collection_archive---------1-----------------------#2023-12-08](https://towardsdatascience.com/tide-the-embarrassingly-simple-mlp-that-beats-transformers-7db77d588079?source=collection_archive---------1-----------------------#2023-12-08)
- en: A deep exploration of TiDE, its implementation using Darts and a real life use
    case comparison with DeepAR and TFT (a Transformer architecture)
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 对TiDE的深度探索，包括使用Darts的实现，以及与DeepAR和TFT（一个Transformer架构）的实际应用案例比较
- en: '[](https://medium.com/@rjguedes?source=post_page-----7db77d588079--------------------------------)[![Rafael
    Guedes](../Images/b3d000b3bce0113d2b2727e84db04870.png)](https://medium.com/@rjguedes?source=post_page-----7db77d588079--------------------------------)[](https://towardsdatascience.com/?source=post_page-----7db77d588079--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----7db77d588079--------------------------------)
    [Rafael Guedes](https://medium.com/@rjguedes?source=post_page-----7db77d588079--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@rjguedes?source=post_page-----7db77d588079--------------------------------)[![Rafael
    Guedes](../Images/b3d000b3bce0113d2b2727e84db04870.png)](https://medium.com/@rjguedes?source=post_page-----7db77d588079--------------------------------)[](https://towardsdatascience.com/?source=post_page-----7db77d588079--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----7db77d588079--------------------------------)
    [Rafael Guedes](https://medium.com/@rjguedes?source=post_page-----7db77d588079--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F2789d1da9c75&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftide-the-embarrassingly-simple-mlp-that-beats-transformers-7db77d588079&user=Rafael+Guedes&userId=2789d1da9c75&source=post_page-2789d1da9c75----7db77d588079---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----7db77d588079--------------------------------)
    ·9 min read·Dec 8, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F7db77d588079&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftide-the-embarrassingly-simple-mlp-that-beats-transformers-7db77d588079&user=Rafael+Guedes&userId=2789d1da9c75&source=-----7db77d588079---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[查看](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F2789d1da9c75&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftide-the-embarrassingly-simple-mlp-that-beats-transformers-7db77d588079&user=Rafael+Guedes&userId=2789d1da9c75&source=post_page-2789d1da9c75----7db77d588079---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----7db77d588079--------------------------------)
    · 9分钟阅读 · 2023年12月8日 [](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F7db77d588079&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftide-the-embarrassingly-simple-mlp-that-beats-transformers-7db77d588079&user=Rafael+Guedes&userId=2789d1da9c75&source=-----7db77d588079---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F7db77d588079&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftide-the-embarrassingly-simple-mlp-that-beats-transformers-7db77d588079&source=-----7db77d588079---------------------bookmark_footer-----------)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F7db77d588079&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftide-the-embarrassingly-simple-mlp-that-beats-transformers-7db77d588079&source=-----7db77d588079---------------------bookmark_footer-----------)'
- en: As industries continue to evolve, the importance of an accurate forecasting
    becomes a non-negotiable asset whether you work in e-commerce, healthcare, retail
    or even in agriculture. The importance of being able to foresee what comes next
    and plan accordingly to overcome future challenges is what can make you ahead
    of competition and thrive in an economy where margins are tight and the customers
    are more demanding than ever.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 随着各行业的不断发展，准确预测的重要性已经成为无可谈判的资产，无论你是从事电子商务、医疗保健、零售还是农业。能够预见未来并相应地进行规划以克服未来的挑战，这种能力可以使你在竞争中脱颖而出，并在利润紧张、客户要求比以往任何时候都高的经济环境中蓬勃发展。
- en: Transformer architectures have been the hot topic in AI for the past few years,
    specially due to their success in Natural Language Processing (NLP) being one
    of the most successful use cases the chatGPT that took the attention of everyone
    regardless if you were an AI enthusiastic or not. But NLP is not the only subject
    where Transformers have been shown to outperform the state-of-the-art solutions,
    in Computer Vision as well with Stable Diffusion and its variants.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 变压器架构在过去几年里一直是人工智能领域的热门话题，特别是由于它们在自然语言处理（NLP）中的成功，其中chatGPT作为一个引起所有人关注的成功案例，无论你是否对人工智能感兴趣。但是，NLP并不是变压器在超越最先进解决方案方面的唯一领域，计算机视觉中也有类似表现，例如Stable
    Diffusion及其变体。
- en: But can Transformers outperform state-of-the-art models in time series? Although
    many efforts have been made to develop Transformers for time series forecasting,
    it seems that for long term horizons, simple linear models can outperform several
    Transformer based approaches.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，变压器（Transformers）能否在时间序列预测中超越最先进的模型？尽管已经做出了许多努力来开发用于时间序列预测的变压器，但似乎对于长期预测，简单的线性模型可以超越几种基于变压器的方法。
