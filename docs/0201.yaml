- en: 'Why WGANs beat GANs: A journey from KL divergence to Wasserstein loss'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为什么 WGANs 超越 GANs：从 KL 散度到 Wasserstein 损失
- en: 原文：[https://towardsdatascience.com/why-wgans-beat-gans-a-journey-from-kl-divergence-to-wasserstein-loss-9ee5faf10b48?source=collection_archive---------11-----------------------#2023-01-12](https://towardsdatascience.com/why-wgans-beat-gans-a-journey-from-kl-divergence-to-wasserstein-loss-9ee5faf10b48?source=collection_archive---------11-----------------------#2023-01-12)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/why-wgans-beat-gans-a-journey-from-kl-divergence-to-wasserstein-loss-9ee5faf10b48?source=collection_archive---------11-----------------------#2023-01-12](https://towardsdatascience.com/why-wgans-beat-gans-a-journey-from-kl-divergence-to-wasserstein-loss-9ee5faf10b48?source=collection_archive---------11-----------------------#2023-01-12)
- en: Introduction
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 介绍
- en: '[](https://medium.com/@rm12?source=post_page-----9ee5faf10b48--------------------------------)[![Shashank
    Kumar](../Images/292e56fa174e7e882a94348815501370.png)](https://medium.com/@rm12?source=post_page-----9ee5faf10b48--------------------------------)[](https://towardsdatascience.com/?source=post_page-----9ee5faf10b48--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----9ee5faf10b48--------------------------------)
    [Shashank Kumar](https://medium.com/@rm12?source=post_page-----9ee5faf10b48--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@rm12?source=post_page-----9ee5faf10b48--------------------------------)[![Shashank
    Kumar](../Images/292e56fa174e7e882a94348815501370.png)](https://medium.com/@rm12?source=post_page-----9ee5faf10b48--------------------------------)[](https://towardsdatascience.com/?source=post_page-----9ee5faf10b48--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----9ee5faf10b48--------------------------------)
    [Shashank Kumar](https://medium.com/@rm12?source=post_page-----9ee5faf10b48--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fe447b689ba76&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhy-wgans-beat-gans-a-journey-from-kl-divergence-to-wasserstein-loss-9ee5faf10b48&user=Shashank+Kumar&userId=e447b689ba76&source=post_page-e447b689ba76----9ee5faf10b48---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----9ee5faf10b48--------------------------------)
    ·7 min read·Jan 12, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F9ee5faf10b48&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhy-wgans-beat-gans-a-journey-from-kl-divergence-to-wasserstein-loss-9ee5faf10b48&user=Shashank+Kumar&userId=e447b689ba76&source=-----9ee5faf10b48---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fe447b689ba76&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhy-wgans-beat-gans-a-journey-from-kl-divergence-to-wasserstein-loss-9ee5faf10b48&user=Shashank+Kumar&userId=e447b689ba76&source=post_page-e447b689ba76----9ee5faf10b48---------------------post_header-----------)
    发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----9ee5faf10b48--------------------------------)
    ·7分钟阅读·2023年1月12日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F9ee5faf10b48&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhy-wgans-beat-gans-a-journey-from-kl-divergence-to-wasserstein-loss-9ee5faf10b48&user=Shashank+Kumar&userId=e447b689ba76&source=-----9ee5faf10b48---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F9ee5faf10b48&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhy-wgans-beat-gans-a-journey-from-kl-divergence-to-wasserstein-loss-9ee5faf10b48&source=-----9ee5faf10b48---------------------bookmark_footer-----------)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F9ee5faf10b48&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhy-wgans-beat-gans-a-journey-from-kl-divergence-to-wasserstein-loss-9ee5faf10b48&source=-----9ee5faf10b48---------------------bookmark_footer-----------)'
- en: In 2014, Ian Goodfellow came up with the idea of GAN or vanilla GAN as we call
    it today. Although impressive, it was notoriously hard to train. Vanilla GAN suffered
    from an inability to converge, vanishing gradients, and mode collapse. Subsequently,
    a ton of research centred around addressing these issues. Researchers experimented
    with different model architectures, loss functions, and training methodologies.
    A particularly effective solution was Wasserstein GAN, introduced in 2017 by [Arjovsky
    et al](https://arxiv.org/pdf/1701.07875.pdf).
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在2014年，Ian Goodfellow 提出了GAN的概念，或者我们今天所称的“原始GAN”。虽然这一想法令人印象深刻，但训练起来却 notoriously
    很困难。原始GAN面临无法收敛、梯度消失以及模式崩溃等问题。随后，大量的研究集中于解决这些问题。研究人员尝试了不同的模型架构、损失函数和训练方法。一个特别有效的解决方案是
    Wasserstein GAN，它在2017年由 [Arjovsky et al](https://arxiv.org/pdf/1701.07875.pdf)
    提出。
- en: '![](../Images/ab1e1997f5b968193345c74a21c1811e.png)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ab1e1997f5b968193345c74a21c1811e.png)'
- en: Photo by [Arnold Francisca](https://unsplash.com/ja/@clark_fransa?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由 [Arnold Francisca](https://unsplash.com/ja/@clark_fransa?utm_source=medium&utm_medium=referral)
    提供，来源于 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: This post attempts to explain why Wasserstein GANs function better than vanilla
    GANs. It assumes readers have some familiarity with the math behind GANs and VAEs
    and their training process. Let’s begin with an introduction to generative networks.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 本文尝试解释为什么 Wasserstein GANs 比普通 GANs 表现更好。它假设读者对 GANs 和 VAEs 的数学原理及其训练过程有一定了解。让我们从生成网络的介绍开始。
- en: '1\. Generative Networks: A brief'
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1\. 生成网络：简要介绍
- en: Generative networks strive to spawn new samples that resemble the real data
    family. They do so by mimicking the data distribution. Popular frameworks like
    GANs and VAEs approach this by learning a mapping **G** that transforms a known/assumed
    distribution **Z** to the actual distribution space. Generators in GANs and encoders
    in VAEs handle this job. The neural weights of these networks parametrise **G**.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 生成网络努力生成与真实数据家族相似的新样本。它们通过模仿数据分布来实现这一点。像 GANs 和 VAEs 这样的流行框架通过学习一个映射 **G** 来将已知/假设的分布
    **Z** 转换为实际的分布空间。GANs 中的生成器和 VAEs 中的编码器完成了这一工作。这些网络的神经权重参数化了 **G**。
- en: 'Networks minimise the difference between the actual and generated data distributions
    to learn the mapping. Three popular measures to quantify this difference are:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 网络通过最小化实际数据分布和生成数据分布之间的差异来学习映射。量化这种差异的三种常用度量是：
- en: KL Divergence
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: KL 散度
- en: JS Divergence
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: JS 散度
- en: Earth-Mover (EM) or Wasserstein-1 distance
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Earth-Mover (EM) 或 Wasserstein-1 距离
- en: Let’s peek into them.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来窥探一下它们。
- en: 2\. Why KL and JS divergence fail?
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2\. 为什么 KL 和 JS 散度会失败？
- en: We’ll briefly introduce KL and JS divergences and understand why they fail.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将简要介绍 KL 和 JS 散度，并了解它们为何会失败。
- en: Kullback-Leibler(KL) divergence can be computed in two ways, forward or reverse,
    and is thus asymmetric. Depending on whether the distributions are continuous
    or discrete, their forward KL divergence is as follows
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Kullback-Leibler (KL) 散度可以通过正向或逆向两种方式计算，因此是非对称的。根据分布是连续的还是离散的，它们的正向 KL 散度如下：
- en: '![](../Images/46ab4d141c548248f9b44bbd517e6187.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/46ab4d141c548248f9b44bbd517e6187.png)'
- en: Forward KL Divergence
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 正向 KL 散度
- en: We can calculate the reverse KL divergence by using **Q,** instead of **P,**
    to weight the log difference of distributions. **VAEs** operate on reverse KL.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过使用 **Q** 来加权分布的对数差异，从而计算逆 KL 散度。**VAEs** 操作于逆 KL。
- en: '![](../Images/e92cea64338b4874690578f660b98be7.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e92cea64338b4874690578f660b98be7.png)'
- en: Reverse KL divergence
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 逆 KL 散度
- en: 2\. Unlike the former, Jensen-Shannon(JS) divergence is symmetric. It’s essentially
    an average of the two KL divergences. It is not conspicuous from their loss function,
    binary cross-entropy, but GANs function on JS divergence when the discriminator
    attains optimality. I urge you to read this [blog](https://lilianweng.github.io/posts/2017-08-20-gan/#:~:text=best%20value%20for,D)
    to understand why so.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 2\. 与前者不同，Jensen-Shannon (JS) 散度是对称的。它本质上是两个 KL 散度的平均值。它在损失函数二元交叉熵中并不明显，但当鉴别器达到最优时，GANs
    会基于 JS 散度。我建议你阅读这篇 [博客](https://lilianweng.github.io/posts/2017-08-20-gan/#:~:text=best%20value%20for,D)
    以了解原因。
- en: '![](../Images/b123b27d3befe15ac389ee0ae3a77b7d.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b123b27d3befe15ac389ee0ae3a77b7d.png)'
- en: JS Divergence
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: JS 散度
- en: Arjovsky et al. used a simple example to show the pitfalls of KL and JS divergence.
    Consider two probability distributions described by the two parallel lines in
    the image below. Red is the actual distribution(P), while green is its estimation(Q).
    θ is the horizontal distance between them.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: Arjovsky 等人用一个简单的例子来展示 KL 和 JS 散度的陷阱。考虑下面图像中的两条平行线所描述的两个概率分布。红色是实际分布（P），绿色是其估计（Q）。θ
    是它们之间的水平距离。
- en: '![](../Images/f7f9f833f50963e862b1d0d6df22702e.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f7f9f833f50963e862b1d0d6df22702e.png)'
- en: Image by author
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片
- en: A generative model would work to shift green closer to red, which is fixed at
    0\. Can you calculate the JS and KL divergence when θ = 1?
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 生成模型将努力将绿色靠近红色，这个红色固定在 0。当 θ = 1 时，你能计算 JS 和 KL 散度吗？
- en: '![](../Images/7239ab35f62e5489711d66c91e2ffa76.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7239ab35f62e5489711d66c91e2ffa76.png)'
- en: KL Divergences
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: KL 散度
- en: '![](../Images/3e27fe2e5473dbf62e05f20ace68b5b0.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3e27fe2e5473dbf62e05f20ace68b5b0.png)'
- en: JS Divergence
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: JS 散度
- en: Now, how would these measures vary as a function of θ? If you observe closely,
    they won’t change unless θ=0.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，这些度量会随着 θ 的变化而有所不同吗？如果你仔细观察，它们不会改变，除非 θ=0。
- en: '![](../Images/41c0c2af283c7aeab6181ec4fd94ecf4.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/41c0c2af283c7aeab6181ec4fd94ecf4.png)'
- en: I observe two main drawbacks with these measures.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我观察到这些度量有两个主要缺点。
- en: The difference between distributions at θ=0.1 and θ=1 is the same. At θ=0.1,
    the green line is much closer to the red, so the difference must be less.
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 θ=0.1 和 θ=1 之间的分布差异是相同的。在 θ=0.1 时，绿线与红线的距离更近，因此差异应该更小。
- en: θ can be loosely considered as the estimated distribution. If our generative
    model is parametrised by φ, then θ is a function f(φ). During back-propagation,
    we’ll calculate the gradient of loss function, defined by one of the above measures,
    with respect to φ to tune the parameters. The first term of the second equation
    will always be 0\. Hence, owing to zero gradients, our poor model will not learn
    anything.
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: θ 可以大致看作是估计的分布。如果我们的生成模型由 φ 参数化，那么 θ 是 f(φ) 的一个函数。在反向传播过程中，我们将计算相对于 φ 的损失函数的梯度，该损失函数由上述度量之一定义，以调整参数。第二个方程的第一个项将始终为
    0。因此，由于梯度为零，我们的模型将无法学习任何东西。
- en: '![](../Images/49a3208baa4b366140e715d2876da14c.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/49a3208baa4b366140e715d2876da14c.png)'
- en: Back-propagation using chain rule
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 使用链式法则进行反向传播
- en: From the above example we can make certain crucial inferences.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 从上述示例中，我们可以得出一些重要的推论。
- en: KL and JS divergences do not account for the horizontal difference between two
    probability distributions
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: KL 和 JS 散度未考虑两个概率分布之间的水平差异
- en: In situations where the predicted and actual distributions don’t overlap, which
    is common during training, the gradients might be 0 leading to no learning.
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在预测和实际分布不重叠的情况下，这种情况在训练过程中很常见，梯度可能为 0，从而导致无法学习。
- en: So, how does the Wasserstein distance address these issues?
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，Wasserstein 距离是如何解决这些问题的呢？
- en: '**3\. Why Wasserstein distance is a better measurement?**'
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**3\. 为什么 Wasserstein 距离是一种更好的度量？**'
- en: Frankly, the formulation of EM/Wasserstein-1 distance is horrific. So, I’ll
    refrain from its mathematical details. Instead, let’s understand it intuitively.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 坦白说，EM/Wasserstein-1 距离的公式非常糟糕。所以，我将避免其数学细节。相反，让我们直观地理解它。
- en: '![](../Images/13770808faec6388f74006f120e040c7.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/13770808faec6388f74006f120e040c7.png)'
- en: EM/Wasserstein-1
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: EM/Wasserstein-1
- en: Let’s go back to parallel lines. This time, four of them. The red lines constitute
    a probability distribution, and so do the blue ones. The numbers on top are the
    probability mass at corresponding points (x=0,1,2,3). We intend to modify the
    blue lines to align (similar distributions) them with the red family. To do so,
    we’ll shift the probability masses.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回到平行线。这次，有四条线。红线构成一个概率分布，蓝线也是如此。顶部的数字是对应点（x=0,1,2,3）的概率质量。我们打算调整蓝线，使其与红线系列对齐（相似分布）。为此，我们将移动概率质量。
- en: '![](../Images/6b5371bc183ec38b56661a87a0fc4f0b.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6b5371bc183ec38b56661a87a0fc4f0b.png)'
- en: Image by author
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：作者
- en: Shift 0.2 from x=2 to x=0
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从 x=2 移动 0.2 到 x=0
- en: Shift 0.1 from x=3 to x=0
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从 x=3 移动 0.1 到 x=0
- en: Shift remaining 0.7 from x=3 to x=1
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从 x=3 移动剩余的 0.7 到 x=1
- en: This, however, is just one amongst many other transport plans. We could also
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这只是众多运输计划中的一个。我们还可以
- en: Shift 0.1 from x=2 to x=0
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从 x=2 移动 0.1 到 x=0
- en: Shift 0.1 from x=2 to x=1
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从 x=2 移动 0.1 到 x=1
- en: Shift 0.2 from x=3 to x=0
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从 x=3 移动 0.2 到 x=0
- en: Shift 0.6 from x=3 to x=0
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从 x=3 移动 0.6 到 x=0
- en: Of the two plans, which is more optimal? To ascertain that, let’s take the analogy
    of work from physics. Here, we define work as mass multiplied by shift distance.
    So, the work done in the two transport schemes is 2.1 and 2.7.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在这两个计划中，哪个更优化？为了确定这一点，我们可以借用物理中的工作类比。在这里，我们将工作定义为质量乘以移动距离。因此，这两个运输方案中的工作量分别为
    2.1 和 2.7。
- en: '![](../Images/fdbd1e27e83b74d44cabe144ebb01339.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fdbd1e27e83b74d44cabe144ebb01339.png)'
- en: Image by author
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：作者
- en: 2.1, work involved in the optimal transport plan, is thus the EM distance. The
    same is applicable in the continuous realm also. As each point has an associated
    probability mass in continuous distributions, integration replaces summation.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 2.1，即最优运输计划中涉及的工作量，因此是 EM 距离。相同的原理也适用于连续领域。由于每个点在连续分布中都有一个相关的概率质量，积分取代了求和。
- en: In a nutshell, the above morbid equation involves computing the work required
    in all transport schemes that bring similarity between two distributions and selecting
    the minimum amongst them. As opposed to the other measures, EM distance accounts
    for the horizontal difference between two distributions while preserving their
    overall shape. Moreover, it also does away with the problem of vanishing gradients.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，上述复杂的方程涉及计算所有运输方案中所需的工作，以在两个分布之间建立相似性，并选择其中的最小值。与其他度量相比，EM 距离考虑了两个分布之间的水平差异，同时保留了它们的整体形状。此外，它还解决了梯度消失的问题。
- en: Reconsider the example in section 1\. The minimum cost in aligning the two distributions
    is θ, the horizontal distance between the lines. Hence, we get stable gradients
    to tune the parameters even when the predicted and actual distributions don’t
    overlap.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 重新考虑第 1 节中的例子。对齐两个分布的最小成本是 θ，即线条之间的水平距离。因此，即使预测和实际分布不重叠，我们也能获得稳定的梯度来调整参数。
- en: '![](../Images/7e414790a833fd4f3a41e86ede2a48d5.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7e414790a833fd4f3a41e86ede2a48d5.png)'
- en: Image by author
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图像
- en: 4\. Using python to visualize all measures
  id: totrans-72
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4\. 使用 Python 可视化所有度量
- en: Now, let’s calculate all the discussed measures in python to visualize their
    behaviour.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们在 Python 中计算所有讨论的度量，以可视化它们的行为。
- en: '[PRE0]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: We’ll define functions to calculate KL and JS divergences and thankfully, for
    EM distance, we can use the scipy library.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将定义计算 KL 和 JS 发散的函数，幸运的是，对于 EM 距离，我们可以使用 scipy 库。
- en: '[PRE1]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Next, we’ll define two normal distributions and visualize how the measures change
    as the gaussians are separated. That is, their mean difference is increased.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将定义两个正态分布，并可视化当高斯分布被分离时度量如何变化。也就是说，它们的均值差异增加。
- en: '[PRE2]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '![](../Images/2266b01d390c96758de8b5c48c01ca2c.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2266b01d390c96758de8b5c48c01ca2c.png)'
- en: Plot of all measures vs mean difference
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 所有度量与均值差异的图
- en: What do you note? As the mean difference is increased, the KL divergences explode
    and the JS divergence saturates. However, the EM distance increases linearly.
    Thus, amongst the four, EM distance seems to be the best option for maintaining
    gradient flow during training.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 你注意到什么了吗？随着均值差异的增加，KL 发散爆炸，而 JS 发散饱和。然而，EM 距离线性增加。因此，在四者中，EM 距离似乎是训练过程中保持梯度流的最佳选择。
- en: EM/Wasserstein distance also alleviates mode collapse. Roughly speaking, mode
    collapse happens when the generator learns to produce a particular mode that fools
    a discriminator stuck in local minima. As discussed in section 2, GANS — when
    the discriminator is at a minimum — operates on JS divergence leading to zero
    gradients. Consequently, the discriminator is trapped, and the generator is disincentivised
    to create varying samples.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: EM/Wasserstein 距离还缓解了模式崩溃。粗略来说，模式崩溃发生在生成器学会生成特定模式，从而欺骗陷入局部极小值的判别器。正如第 2 节所讨论的，当判别器处于最小值时，GANs
    基于 JS 发散操作，导致梯度为零。因此，判别器被困住，生成器没有动力生成不同的样本。
- en: With Wasserstein distance, discriminators can attain optimality without the
    gradients vanishing. They can escape local minima and discard generator outputs,
    impelling the generator to not overfit on a particular discriminator and produce
    multiple modes.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Wasserstein 距离，判别器可以在梯度不会消失的情况下达到最优。它们可以逃离局部最小值并丢弃生成器输出，促使生成器不会过拟合于特定判别器，从而生成多个模式。
- en: '**Conclusion**'
  id: totrans-84
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**结论**'
- en: With this, we come to the end of the post. I hope it summarised why Wasserstein
    GANs outperform vanilla GANs and VAEs. I’ve glossed over some mathematical intricacies
    surrounding the EM distance. If you’re interested, you need to read the [paper](https://arxiv.org/pdf/1701.07875.pdf).
    You must note that the discussed equation of EM distance is intractable. A mathematical
    trick is applied to approximate it. It’s unlikely that you’ll ever need it in
    practice. Still, if you’re itching to know, you can read about the approximation
    [here](https://www.alexirpan.com/2017/02/22/wasserstein-gan.html#:~:text=exactly%20is%20intractable)
    or in the [paper](https://arxiv.org/pdf/1701.07875.pdf).
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 至此，我们结束了这篇文章。希望这总结了为什么 Wasserstein GANs 优于传统的 GANs 和 VAEs。我略过了一些关于 EM 距离的数学细节。如果你感兴趣，需要阅读
    [论文](https://arxiv.org/pdf/1701.07875.pdf)。你必须注意，讨论的 EM 距离方程是不可处理的。使用了一个数学技巧来近似它。在实际中你不太可能需要它。不过，如果你迫切想知道，可以在
    [这里](https://www.alexirpan.com/2017/02/22/wasserstein-gan.html#:~:text=exactly%20is%20intractable)
    或在 [论文](https://arxiv.org/pdf/1701.07875.pdf) 中阅读有关近似的内容。
- en: References
  id: totrans-86
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[https://arxiv.org/pdf/1701.07875.pdf](https://arxiv.org/pdf/1701.07875.pdf)'
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[https://arxiv.org/pdf/1701.07875.pdf](https://arxiv.org/pdf/1701.07875.pdf)'
- en: '[https://www.alexirpan.com/2017/02/22/wasserstein-gan.html#](https://www.alexirpan.com/2017/02/22/wasserstein-gan.html#)'
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[https://www.alexirpan.com/2017/02/22/wasserstein-gan.html#](https://www.alexirpan.com/2017/02/22/wasserstein-gan.html#)'
- en: '[https://lilianweng.github.io/posts/2017-08-20-gan/#:~:text=best%20value%20for,D](https://lilianweng.github.io/posts/2017-08-20-gan/#:~:text=best%20value%20for,D)'
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[https://lilianweng.github.io/posts/2017-08-20-gan/#:~:text=best%20value%20for,D](https://lilianweng.github.io/posts/2017-08-20-gan/#:~:text=best%20value%20for,D)'
