- en: Understanding Gradient Descent for Machine Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/understanding-gradient-descent-for-machine-learning-246e324c229?source=collection_archive---------1-----------------------#2023-05-21](https://towardsdatascience.com/understanding-gradient-descent-for-machine-learning-246e324c229?source=collection_archive---------1-----------------------#2023-05-21)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: A deep dive into Batch, Stochastic, and Mini-Batch Gradient Descent algorithms
    using Python
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://idilismiguzel.medium.com/?source=post_page-----246e324c229--------------------------------)[![Idil
    Ismiguzel](../Images/6846628535770a9f3e13ebb555e82abd.png)](https://idilismiguzel.medium.com/?source=post_page-----246e324c229--------------------------------)[](https://towardsdatascience.com/?source=post_page-----246e324c229--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----246e324c229--------------------------------)
    [Idil Ismiguzel](https://idilismiguzel.medium.com/?source=post_page-----246e324c229--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F6d965c736f2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-gradient-descent-for-machine-learning-246e324c229&user=Idil+Ismiguzel&userId=6d965c736f2&source=post_page-6d965c736f2----246e324c229---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----246e324c229--------------------------------)
    ·14 min read·May 21, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F246e324c229&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-gradient-descent-for-machine-learning-246e324c229&user=Idil+Ismiguzel&userId=6d965c736f2&source=-----246e324c229---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F246e324c229&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-gradient-descent-for-machine-learning-246e324c229&source=-----246e324c229---------------------bookmark_footer-----------)![](../Images/5287b14c08ccf02afd428f2beb4f7204.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Photo by [Lucas Clara](https://unsplash.com/ko/@lux17?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: Gradient descent is a popular optimization algorithm that is used in machine
    learning and deep learning models such as linear regression, logistic regression,
    and neural networks. It uses first-order derivatives iteratively to minimize the
    cost function by updating model coefficients (for regression) and weights (for
    neural networks).
  prefs: []
  type: TYPE_NORMAL
- en: In this article, we will delve into the mathematical theory of gradient descent
    and explore how to perform calculations using Python. We will examine various
    implementations including Batch Gradient Descent, Stochastic Gradient Descent,
    and Mini-Batch Gradient Descent, and assess their effectiveness on a range of
    test cases.
  prefs: []
  type: TYPE_NORMAL
- en: While following the article, you can check out the [Jupyter Notebook](https://github.com/Idilismiguzel/Machine-Learning/blob/master/Gradient_Descent/gradient_descent_implementation.ipynb)
    on my GitHub for complete analysis and code.
  prefs: []
  type: TYPE_NORMAL
- en: Before a deep dive into gradient descent, let’s first go through the loss function.
  prefs: []
  type: TYPE_NORMAL
- en: What is Loss Function?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Loss** or **cost** are used interchangeably to describe the error in a prediction.
    A loss value indicates how different a prediction is from the actual value and
    the loss function aggregates all the loss…'
  prefs: []
  type: TYPE_NORMAL
