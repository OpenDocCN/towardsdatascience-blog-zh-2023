- en: Understanding Gradient Descent for Machine Learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解机器学习中的梯度下降
- en: 原文：[https://towardsdatascience.com/understanding-gradient-descent-for-machine-learning-246e324c229?source=collection_archive---------1-----------------------#2023-05-21](https://towardsdatascience.com/understanding-gradient-descent-for-machine-learning-246e324c229?source=collection_archive---------1-----------------------#2023-05-21)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/understanding-gradient-descent-for-machine-learning-246e324c229?source=collection_archive---------1-----------------------#2023-05-21](https://towardsdatascience.com/understanding-gradient-descent-for-machine-learning-246e324c229?source=collection_archive---------1-----------------------#2023-05-21)
- en: A deep dive into Batch, Stochastic, and Mini-Batch Gradient Descent algorithms
    using Python
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 Python 深入探讨批量梯度下降、随机梯度下降和小批量梯度下降算法
- en: '[](https://idilismiguzel.medium.com/?source=post_page-----246e324c229--------------------------------)[![Idil
    Ismiguzel](../Images/6846628535770a9f3e13ebb555e82abd.png)](https://idilismiguzel.medium.com/?source=post_page-----246e324c229--------------------------------)[](https://towardsdatascience.com/?source=post_page-----246e324c229--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----246e324c229--------------------------------)
    [Idil Ismiguzel](https://idilismiguzel.medium.com/?source=post_page-----246e324c229--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://idilismiguzel.medium.com/?source=post_page-----246e324c229--------------------------------)[![Idil
    Ismiguzel](../Images/6846628535770a9f3e13ebb555e82abd.png)](https://idilismiguzel.medium.com/?source=post_page-----246e324c229--------------------------------)[](https://towardsdatascience.com/?source=post_page-----246e324c229--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----246e324c229--------------------------------)
    [Idil Ismiguzel](https://idilismiguzel.medium.com/?source=post_page-----246e324c229--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F6d965c736f2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-gradient-descent-for-machine-learning-246e324c229&user=Idil+Ismiguzel&userId=6d965c736f2&source=post_page-6d965c736f2----246e324c229---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----246e324c229--------------------------------)
    ·14 min read·May 21, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F246e324c229&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-gradient-descent-for-machine-learning-246e324c229&user=Idil+Ismiguzel&userId=6d965c736f2&source=-----246e324c229---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F6d965c736f2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-gradient-descent-for-machine-learning-246e324c229&user=Idil+Ismiguzel&userId=6d965c736f2&source=post_page-6d965c736f2----246e324c229---------------------post_header-----------)
    发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----246e324c229--------------------------------)
    ·14 分钟阅读·2023年5月21日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F246e324c229&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-gradient-descent-for-machine-learning-246e324c229&user=Idil+Ismiguzel&userId=6d965c736f2&source=-----246e324c229---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F246e324c229&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-gradient-descent-for-machine-learning-246e324c229&source=-----246e324c229---------------------bookmark_footer-----------)![](../Images/5287b14c08ccf02afd428f2beb4f7204.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F246e324c229&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-gradient-descent-for-machine-learning-246e324c229&source=-----246e324c229---------------------bookmark_footer-----------)![](../Images/5287b14c08ccf02afd428f2beb4f7204.png)'
- en: Photo by [Lucas Clara](https://unsplash.com/ko/@lux17?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：[Lucas Clara](https://unsplash.com/ko/@lux17?utm_source=medium&utm_medium=referral)
    在 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: Gradient descent is a popular optimization algorithm that is used in machine
    learning and deep learning models such as linear regression, logistic regression,
    and neural networks. It uses first-order derivatives iteratively to minimize the
    cost function by updating model coefficients (for regression) and weights (for
    neural networks).
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度下降是一种流行的优化算法，广泛用于机器学习和深度学习模型中，如线性回归、逻辑回归和神经网络。它通过迭代地使用一阶导数来最小化成本函数，方法是更新模型系数（用于回归）和权重（用于神经网络）。
- en: In this article, we will delve into the mathematical theory of gradient descent
    and explore how to perform calculations using Python. We will examine various
    implementations including Batch Gradient Descent, Stochastic Gradient Descent,
    and Mini-Batch Gradient Descent, and assess their effectiveness on a range of
    test cases.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们将深入探讨梯度下降的数学理论，并探索如何使用Python进行计算。我们将审查包括批量梯度下降、随机梯度下降和小批量梯度下降在内的各种实现，并评估它们在不同测试案例中的效果。
- en: While following the article, you can check out the [Jupyter Notebook](https://github.com/Idilismiguzel/Machine-Learning/blob/master/Gradient_Descent/gradient_descent_implementation.ipynb)
    on my GitHub for complete analysis and code.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在阅读本文时，你可以查看我在GitHub上的[Jupyter Notebook](https://github.com/Idilismiguzel/Machine-Learning/blob/master/Gradient_Descent/gradient_descent_implementation.ipynb)以获取完整的分析和代码。
- en: Before a deep dive into gradient descent, let’s first go through the loss function.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入探讨梯度下降之前，我们首先了解一下损失函数。
- en: What is Loss Function?
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是损失函数？
- en: '**Loss** or **cost** are used interchangeably to describe the error in a prediction.
    A loss value indicates how different a prediction is from the actual value and
    the loss function aggregates all the loss…'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '**损失**或**成本**这两个词可以互换使用来描述预测中的误差。损失值表示预测与实际值之间的差异，损失函数汇总所有损失……'
