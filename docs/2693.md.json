["```py\n{\n  \"fraud\":1,\n  \"records\":[\n    {\n      \"id\":0,\n      \"totalValue\":85,\n      \"items\":2\n    },\n    {\n      \"id\":1,\n      \"totalValue\":31,\n      \"items\":4\n    },\n    {\n      \"id\":2,\n      \"totalValue\":20,\n      \"items\":9\n    }\n  ],\n  \"edges\":[\n    {\n      \"a\":1,\n      \"b\":0,\n      \"R1\":1,\n      \"R2\":1\n    },\n    {\n      \"a\":2,\n      \"b\":1,\n      \"R1\":0,\n      \"R2\":1\n    }\n  ]\n}\n```", "```py\n{\n  \"fraud\":1,\n  \"records\":[\n    {\n      \"id\":0,\n      \"totalValue\":85,\n      \"items\":5\n    }\n  ],\n  \"edges\":[\n\n  ]\n}\n```", "```py\nimport os\n\nos.environ[\"DGLBACKEND\"] = \"pytorch\"\nimport pandas as pd\nimport torch\nimport dgl\nfrom dgl.data import DGLDataset\n\nclass EntitiesDataset(DGLDataset):\n    def __init__(self, entitiesFile):\n        self.entitiesFile = entitiesFile\n        super().__init__(name=\"entities\")\n\n    def process(self):\n        entities = pd.read_json(self.entitiesFile, lines=1)\n\n        self.graphs = []\n        self.labels = []\n\n        for _, entity in entities.iterrows():\n            a = []\n            b = []\n            r1_feat = []\n            r2_feat = []\n            for edge in entity[\"edges\"]:\n                a.append(edge[\"a\"])\n                b.append(edge[\"b\"])\n                r1_feat.append(edge[\"R1\"])\n                r2_feat.append(edge[\"R2\"])\n            a = torch.LongTensor(a)\n            b = torch.LongTensor(b)\n            edge_features = torch.LongTensor([r1_feat, r2_feat]).t()\n\n            node_feat = [[node[\"totalValue\"], node[\"items\"]] for node in entity[\"records\"]]\n            node_features = torch.tensor(node_feat)\n\n            g = dgl.graph((a, b), num_nodes=len(entity[\"records\"]))\n            g.edata[\"feat\"] = edge_features\n            g.ndata[\"feat\"] = node_features\n            g = dgl.add_self_loop(g)\n\n            self.graphs.append(g)\n            self.labels.append(entity[\"fraud\"])\n\n        self.labels = torch.LongTensor(self.labels)\n\n    def __getitem__(self, i):\n        return self.graphs[i], self.labels[i]\n\n    def __len__(self):\n        return len(self.graphs)\n\ndataset = EntitiesDataset(\"./entities.jsonl\")\nprint(dataset)\nprint(dataset[0])\n```", "```py\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom dgl.nn import NNConv, SAGEConv\n\nclass EntityGraphModule(nn.Module):\n    def __init__(self, node_in_feats, edge_in_feats, h_feats, num_classes):\n        super(EntityGraphModule, self).__init__()\n        lin = nn.Linear(edge_in_feats, node_in_feats * h_feats)\n        edge_func = lambda e_feat: lin(e_feat)\n        self.conv1 = NNConv(node_in_feats, h_feats, edge_func)\n\n        self.conv2 = SAGEConv(h_feats, num_classes, \"pool\")\n\n    def forward(self, g, node_features, edge_features):\n        h = self.conv1(g, node_features, edge_features)\n        h = F.relu(h)\n        h = self.conv2(g, h)\n        g.ndata[\"h\"] = h\n        return dgl.mean_nodes(g, \"h\")\n```", "```py\nfrom torch.utils.data.sampler import SubsetRandomSampler\nfrom dgl.dataloading import GraphDataLoader\n\nnum_examples = len(dataset)\nnum_train = int(num_examples * 0.8)\n\ntrain_sampler = SubsetRandomSampler(torch.arange(num_train))\ntest_sampler = SubsetRandomSampler(torch.arange(num_train, num_examples))\n\ntrain_dataloader = GraphDataLoader(\n    dataset, sampler=train_sampler, batch_size=5, drop_last=False\n)\ntest_dataloader = GraphDataLoader(\n    dataset, sampler=test_sampler, batch_size=5, drop_last=False\n)\n```", "```py\nh_feats = 64\nlearn_iterations = 50\nlearn_rate = 0.01\n\nmodel = EntityGraphModule(\n    dataset.graphs[0].ndata[\"feat\"].shape[1],\n    dataset.graphs[0].edata[\"feat\"].shape[1],\n    h_feats,\n    dataset.labels.max().item() + 1\n)\noptimizer = torch.optim.Adam(model.parameters(), lr=learn_rate)\n\nfor _ in range(learn_iterations):\n    for batched_graph, labels in train_dataloader:\n        pred = model(batched_graph, batched_graph.ndata[\"feat\"].float(), batched_graph.edata[\"feat\"].float())\n        loss = F.cross_entropy(pred, labels)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\nnum_correct = 0\nnum_tests = 0\nfor batched_graph, labels in test_dataloader:\n    pred = model(batched_graph, batched_graph.ndata[\"feat\"].float(), batched_graph.edata[\"feat\"].float())\n    num_correct += (pred.argmax(1) == labels).sum().item()\n    num_tests += len(labels)\n\nacc = num_correct / num_tests\nprint(\"Test accuracy:\", acc)\n```"]