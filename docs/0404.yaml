- en: 'Going Under the Hood of Character-Level RNNs: A NumPy-based Implementation
    Guide'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探究字符级RNN：基于NumPy的实现指南
- en: 原文：[https://towardsdatascience.com/numpy-character-level-rnn-af1428bb10a8?source=collection_archive---------5-----------------------#2023-01-27](https://towardsdatascience.com/numpy-character-level-rnn-af1428bb10a8?source=collection_archive---------5-----------------------#2023-01-27)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/numpy-character-level-rnn-af1428bb10a8?source=collection_archive---------5-----------------------#2023-01-27](https://towardsdatascience.com/numpy-character-level-rnn-af1428bb10a8?source=collection_archive---------5-----------------------#2023-01-27)
- en: Due to the recent boom of LLMs, it is imperative to grasp the rudiments of language
    modeling
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 由于最近LLMs蓬勃发展，掌握语言建模的基础知识至关重要
- en: '[](https://sassonjoe66.medium.com/?source=post_page-----af1428bb10a8--------------------------------)[![Joe
    Sasson](../Images/f0edde425f64b6b09d3d8d4adc953d2d.png)](https://sassonjoe66.medium.com/?source=post_page-----af1428bb10a8--------------------------------)[](https://towardsdatascience.com/?source=post_page-----af1428bb10a8--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----af1428bb10a8--------------------------------)
    [Joe Sasson](https://sassonjoe66.medium.com/?source=post_page-----af1428bb10a8--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://sassonjoe66.medium.com/?source=post_page-----af1428bb10a8--------------------------------)![Joe
    Sasson](../Images/f0edde425f64b6b09d3d8d4adc953d2d.png)](https://sassonjoe66.medium.com/?source=post_page-----af1428bb10a8--------------------------------)[](https://towardsdatascience.com/?source=post_page-----af1428bb10a8--------------------------------)![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----af1428bb10a8--------------------------------)
    [Joe Sasson](https://sassonjoe66.medium.com/?source=post_page-----af1428bb10a8--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F32a49c3f499a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnumpy-character-level-rnn-af1428bb10a8&user=Joe+Sasson&userId=32a49c3f499a&source=post_page-32a49c3f499a----af1428bb10a8---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----af1428bb10a8--------------------------------)
    ·17 min read·Jan 27, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Faf1428bb10a8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnumpy-character-level-rnn-af1428bb10a8&user=Joe+Sasson&userId=32a49c3f499a&source=-----af1428bb10a8---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F32a49c3f499a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnumpy-character-level-rnn-af1428bb10a8&user=Joe+Sasson&userId=32a49c3f499a&source=post_page-32a49c3f499a----af1428bb10a8---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----af1428bb10a8--------------------------------)
    ·17 min read·Jan 27, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Faf1428bb10a8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnumpy-character-level-rnn-af1428bb10a8&user=Joe+Sasson&userId=32a49c3f499a&source=-----af1428bb10a8---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Faf1428bb10a8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnumpy-character-level-rnn-af1428bb10a8&source=-----af1428bb10a8---------------------bookmark_footer-----------)![](../Images/43b8bc23a16ed2edb617b4bdee1b1852.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Faf1428bb10a8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnumpy-character-level-rnn-af1428bb10a8&source=-----af1428bb10a8---------------------bookmark_footer-----------)![](../Images/43b8bc23a16ed2edb617b4bdee1b1852.png)'
- en: Photo by [Markus Spiske](https://unsplash.com/@markusspiske?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来自 [Markus Spiske](https://unsplash.com/@markusspiske?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: Introduction
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 简介
- en: Recurrent neural networks (RNNs) are a powerful type of neural network that
    have the ability to process sequential data, such as time series or natural language.
    In this article, we will walk through the process of building a Vanilla RNN from
    scratch using NumPy. We will begin by discussing the theory and intuition behind
    RNNs, including their architecture and the types of problems they are well-suited
    for solving. Next, we will dive into the code, explaining the various components
    of the RNN and how they interact with one another. Finally, we will demonstrate
    the effectiveness of our RNN by applying it to a real-world dataset.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 循环神经网络（RNNs）是一种强大的神经网络类型，能够处理序列数据，如时间序列或自然语言。本文将通过使用 NumPy 从零开始构建一个 Vanilla
    RNN 的过程。我们将首先讨论 RNN 的理论和直觉，包括它们的架构和适合解决的问题类型。接下来，我们将深入代码，解释 RNN 的各个组件及其相互作用。最后，我们将通过将
    RNN 应用于实际数据集来展示其有效性。
- en: Specifically, we will be implementing a many-to-many, character-level RNN that
    uses sequential, online learning. This means that the network processes the input
    sequences one character at a time and updates the parameters of the network after
    each character. This allows the network to learn on-the-fly and adapt to new patterns
    in the data as they are encountered.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，我们将实现一个多对多的字符级 RNN，使用序列化的在线学习。这意味着网络一次处理一个字符的输入序列，并在每个字符后更新网络参数。这允许网络在实时学习，并随着遇到的新模式而适应数据。
- en: A character-level RNN means that the input and output are individual characters,
    rather than words or sentences. This allows the network to learn the underlying
    patterns and dependencies between characters in a piece of text. The many-to-many
    architecture refers to the fact that the network receives an input sequence of
    characters and generates an output sequence of characters. This is different from
    a many-to-one architecture, where the network receives an input sequence and generates
    only one output, or a one-to-many architecture, where the network receives only
    one input and generates an output sequence.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 字符级 RNN 意味着输入和输出是单个字符，而不是单词或句子。这使得网络能够学习文本中字符之间的潜在模式和依赖关系。多对多架构指的是网络接收一个字符序列作为输入，并生成一个字符序列作为输出。这与多对一架构不同，在多对一架构中，网络接收一个输入序列并生成一个输出，或者与一对多架构不同，在一对多架构中，网络接收一个输入并生成一个输出序列。
- en: I used Andrej Karpathy’s code (found [here](https://gist.github.com/karpathy/d4dee566867f8291f086))
    as a foundation for my implementation, making several modifications to improve
    versatility and reliability. I expanded the code to support multiple layers, and
    also restructured it for better readability and reusability. This project builds
    on my previous work of creating a simple ANN using NumPy. The source code for
    that can be found [here](https://github.com/j0sephsasson/numpy-NN).
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 我使用了 Andrej Karpathy 的代码（见[这里](https://gist.github.com/karpathy/d4dee566867f8291f086)）作为我的实现基础，并做了几处修改以提高通用性和可靠性。我扩展了代码以支持多个层次，并重新构建了它以提高可读性和重用性。这个项目建立在我之前使用
    NumPy 创建简单 ANN 的工作基础上。相关源代码可以在[这里](https://github.com/j0sephsasson/numpy-NN)找到。
- en: Theory & Intuition
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理论与直觉
- en: RNNs can be contrasted with traditional feedforward neural networks (ANNs),
    which do not have a “memory” mechanism and process each input independently. ANNs
    are well-suited for problems where the input and output have a fixed size and
    the input does not contain sequential dependencies. In contrast, RNNs are able
    to handle variable length input sequences and maintain a “memory” of past inputs
    through a ***hidden state***.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: RNNs 可以与传统的前馈神经网络（ANNs）对比，后者没有“记忆”机制，并且独立处理每个输入。ANNs 适用于输入和输出具有固定大小且输入不包含序列依赖关系的问题。相比之下，RNNs
    能够处理变长的输入序列，并通过***隐藏状态***保持对过去输入的“记忆”。
- en: The hidden state allows RNNs to capture temporal dependencies and make predictions
    based on the entire input sequence. To summarize, the network uses information
    from previous time steps to inform its processing of current inputs. Additionally,
    more complex NLP architectures can handle long-term dependencies (GPT-3 was trained
    using a sequence length of 2048), where information from the beginning of the
    input sequence is still relevant for predicting the output at the end of the sequence.
    This ability to maintain a “memory” gives RNNs & transformers a significant advantage
    over ANNs when it comes to processing sequential data.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 隐藏状态使RNN能够捕捉时间依赖关系，并根据整个输入序列做出预测。总的来说，网络使用来自先前时间步的信息来指导其对当前输入的处理。此外，更复杂的NLP架构可以处理长期依赖关系（GPT-3使用了2048的序列长度进行训练），其中输入序列开头的信息对于预测序列末尾的输出仍然相关。这种保持“记忆”的能力使RNN和变换器在处理序列数据时相较于ANNs具有显著优势。
- en: Recently, transformer architectures such as [GPT-3](https://en.wikipedia.org/wiki/GPT-3)
    and [BERT](https://en.wikipedia.org/wiki/BERT_(language_model)) have become increasingly
    popular for a wide range of NLP tasks. These architectures are based on [self-attention](https://machinelearningmastery.com/the-transformer-attention-mechanism/)
    mechanisms that allow the network to selectively focus on different parts of the
    input sequence. This allows the network to capture long-term dependencies without
    the need for recurrence, making it more efficient and easier to train than RNNs.
    The transformer architectures have been shown to achieve state-of-the-art results
    on a wide range of NLP tasks and have been used in many real-world applications.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，像[GPT-3](https://en.wikipedia.org/wiki/GPT-3)和[BERT](https://en.wikipedia.org/wiki/BERT_(language_model))这样的变换器架构在各种NLP任务中变得越来越流行。这些架构基于[自注意力](https://machinelearningmastery.com/the-transformer-attention-mechanism/)机制，使网络能够有选择地关注输入序列的不同部分。这使得网络能够捕捉长期依赖关系，而无需递归，从而比RNN更高效且更易于训练。变换器架构已在各种NLP任务中实现了最先进的结果，并被用于许多实际应用中。
- en: Although the transformer architectures are more complex than the vanilla RNNs
    and have different characteristics, the vanilla RNNs still have an important role
    to play in the field of deep learning. They are simple to understand, easy to
    implement and debug, and can be used as a building block for other more complex
    architectures. In this article, we will focus on the vanilla RNNs and peek under
    the hood to see how they really work.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管变换器架构比普通RNN更复杂且具有不同的特性，但普通RNN在深度学习领域仍然发挥着重要作用。它们易于理解，容易实现和调试，并可以作为其他更复杂架构的构建块。在本文中，我们将重点关注普通RNN，并窥探其真正的工作原理。
- en: 'Three main types of vanilla RNNs are:'
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 普通RNN的三种主要类型是：
- en: '***one-to-many:*** input a picture of dog and output ‘picture of dog’'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***一对多：*** 输入一张狗的图片并输出‘狗的图片’'
- en: '***many-to-one:*** input a sentence and recieve a sentiment (sentiment analysis)'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***多对一：*** 输入一个句子并接收情感（情感分析）'
- en: '***many-to-many:*** input a sentence and output complete sentence (seen below)'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***多对多：*** 输入一个句子并输出完整句子（见下文）'
- en: We will be implementing the many-to-many architecture as seen below.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将实现如下所示的多对多架构。
- en: '![](../Images/34501c57a1b305a03d6a8d446315f618.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/34501c57a1b305a03d6a8d446315f618.png)'
- en: '**Source:** Kaivan Kamali, Deep Learning (Part 2) — Recurrent neural networks
    (RNN) (Galaxy Training Materials). [https://training.galaxyproject.org/training-material/topics/statistics/tutorials/RNN/tutorial.html](https://training.galaxyproject.org/training-material/topics/statistics/tutorials/RNN/tutorial.html)'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '**来源：** Kaivan Kamali，《深度学习（第二部分）——递归神经网络（RNN）》（银河培训材料）。[https://training.galaxyproject.org/training-material/topics/statistics/tutorials/RNN/tutorial.html](https://training.galaxyproject.org/training-material/topics/statistics/tutorials/RNN/tutorial.html)'
- en: From this point on, we will denote the hidden state at time step t using `h[t]`.
    In the figure this is `s[t]`.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 从这一点开始，我们将使用`h[t]`表示时间步t的隐藏状态。在图中，这被表示为`s[t]`。
- en: As you can see, the hidden state from the previous time step `h[t-1]` is combined
    with the current input `x[t]`, and this is repeated over the number of time steps.
    Inside the RNN block, we are updating the hidden state for the current time step.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，来自上一个时间步的隐藏状态`h[t-1]`与当前输入`x[t]`结合，这一过程在时间步数上重复。在RNN块内，我们正在更新当前时间步的隐藏状态。
- en: For clarification, a time step is just a character, such as ‘a’ or ‘d’. An input
    sequence contains a variable number of characters or time steps, also known as
    sequence length, which is a hyper-parameter of the network.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 为了澄清，时间步只是一个字符，如 ‘a’ 或 ‘d’。输入序列包含一个可变数量的字符或时间步，也称为序列长度，这是网络的一个超参数。
- en: The Code
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 代码
- en: '***Table of Contents***'
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '***目录***'
- en: Prepare Data
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 准备数据
- en: RNN Class
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: RNN 类
- en: Forward Pass
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 前向传播
- en: Backward Pass
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 反向传播
- en: Optimizer
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 优化器
- en: Training
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练
- en: Prepare Data
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备数据
- en: '[PRE0]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: We are reading in the data as a string, from a plain text file, and tokenizing
    the characters. Each unique character (there are 65), will be mapped to an integer
    and vice-versa.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从一个纯文本文件中读取数据作为字符串，并对字符进行标记化。每个唯一字符（共 65 个）将映射到一个整数，反之亦然。
- en: Let’s sample an input & target sequence to our RNN.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们为 RNN 采样一个输入和目标序列。
- en: '[PRE1]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The inputs are a tokenized sequence, the targets are the inputs offset by one.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 输入是一个标记化的序列，目标是输入偏移一个单位后的值。
- en: RNN Class
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: RNN 类
- en: '[PRE2]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Lets start by discussing the components of an RNN, in contrast to a basic ANN.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始讨论 RNN 的组件，与基本 ANN 相比。
- en: 'In a traditional feedforward neural network, the parameters governing the interactions
    between layers are represented by a single weight matrix, denoted as `W`. However,
    in a Recurrent Neural Network (RNN), the interactions between layers are represented
    by multiple matrices. In my code, these matrices are specifically: ***Wxh***,
    ***Whh***, and ***Why***, representing the weights between input and hidden layers,
    hidden to hidden layers, and hidden to output layers respectively.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在传统的前馈神经网络中，控制层间交互的参数由一个单一的权重矩阵表示，记作 `W`。然而，在递归神经网络（RNN）中，层间交互由多个矩阵表示。在我的代码中，这些矩阵特别是：***Wxh***、***Whh***
    和 ***Why***，分别表示输入层与隐藏层、隐藏层与隐藏层、以及隐藏层与输出层之间的权重。
- en: The `Wxh` matrix connects the input layer to the hidden layer, and is used to
    transform the input at each time step into a set of activations for the hidden
    layer. The `Whh` matrix connects the hidden layer at time step t-1 to the hidden
    layer at time step t, and is used to propagate the hidden state from one time
    step to the next. The `Why` matrix connects the hidden layer to the output layer,
    and is used to transform the hidden state into the final output of the network.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '`Wxh` 矩阵将输入层连接到隐藏层，并用于将每个时间步的输入转换为隐藏层的激活值。`Whh` 矩阵将时间步 t-1 的隐藏层连接到时间步 t 的隐藏层，并用于将隐藏状态从一个时间步传播到下一个时间步。`Why`
    矩阵将隐藏层连接到输出层，并用于将隐藏状态转换为网络的最终输出。'
- en: In summary, the main difference between the weights in an ANN and an RNN is
    that the ANN has one weight matrix, while the RNN has multiple weight matrices
    that are used to transform the input, propagate the hidden state, and produce
    the final output. These multiple weights matrices in the RNN allow it to maintain
    a memory of past inputs and move information through time.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，ANN 和 RNN 之间的主要区别在于 ANN 有一个权重矩阵，而 RNN 有多个权重矩阵，这些矩阵用于转换输入、传播隐藏状态并生成最终输出。这些多个权重矩阵使
    RNN 能够保持对过去输入的记忆，并在时间上移动信息。
- en: The Constructor
  id: totrans-49
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 构造函数
- en: '[PRE3]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Here we are defining our RNN parameters as discussed above. Something interesting
    to take note of — the parameters `Why` and `by` represent a linear layer, and
    could be abstracted even more, into a separate class such as PyTorch’s ‘nn.Linear’
    module. However, we will keep them as a part of the RNN class for this implementation.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们定义了如上所述的 RNN 参数。值得注意的是——参数 `Why` 和 `by` 表示一个线性层，甚至可以进一步抽象为一个独立的类，如 PyTorch
    的 ‘nn.Linear’ 模块。然而，在此实现中，我们将它们作为 RNN 类的一部分保留。
- en: Forward Pass
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 前向传播
- en: '[PRE4]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Let’s start at the top, and break it down. What is happening here?
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 从顶部开始，逐步解析。这儿发生了什么？
- en: '**Outside the loop, once per sequence**'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '**循环外，每个序列一次**'
- en: '***hprev*** is our initial hidden state'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***hprev*** 是我们的初始隐藏状态'
- en: We are initializing dictionaries to hold our inputs, hidden states, logits,
    and probabilities. We will need this during the backward pass.
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们正在初始化字典以保存我们的输入、隐藏状态、logits 和概率。我们将在反向传播过程中需要这些。
- en: We are initializing the loss to zero.
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将损失初始化为零。
- en: We are setting our initial hidden state `hprev` equal to `hs[-1]` (reprsenting
    time step t-1).
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将初始隐藏状态 `hprev` 设置为 `hs[-1]`（表示时间步 t-1）。
- en: '**Inside the loop, for every time step in the sequence**'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '**循环内，每个时间步**'
- en: Perform one-hot encoding on our input sequence
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对我们的输入序列进行独热编码
- en: Update the hidden state at time step ‘t’ and layer ‘l’
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在时间步‘t’和层‘l’更新隐藏状态。
- en: '![](../Images/3a81673b881f0d7431084a89ec104840.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3a81673b881f0d7431084a89ec104840.png)'
- en: Mathematical notation of our hidden state. Image by author.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 我们隐藏状态的数学表示。图片由作者提供。
- en: Also, you probably noticed there is some functionality to perform ***dropout***.
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 此外，你可能注意到有一些功能用于执行***dropout***。
- en: Dropout is a [regularization](/regularization-in-deep-learning-l1-l2-and-dropout-377e75acc036)
    technique that aims to prevent overfitting by randomly “dropping out” (setting
    to zero) certain neurons during the training process. In the code above, the dropout
    layer is applied before updating the hidden state at time step t, layer l, by
    multiplying the hidden state at t-1 with a dropout mask. The dropout mask is generated
    by creating a binary mask where each element is 1 with probability p and 0 otherwise.
    By doing this, we are randomly “dropping out” a certain number of neurons in the
    hidden state, which helps to prevent the network from becoming too dependent on
    any single neuron. This makes the network more robust and less likely to overfit
    on the training data. After applying dropout, the hidden state is scaled back
    by dividing it by (1-p) to ensure that the expected value of the hidden state
    is maintained.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: Dropout是一种[正则化](/regularization-in-deep-learning-l1-l2-and-dropout-377e75acc036)技术，旨在通过在训练过程中随机“丢弃”（置为零）某些神经元来防止过拟合。在上述代码中，dropout层在更新时间步‘t’、层‘l’的隐藏状态之前应用，通过将时间步‘t-1’的隐藏状态与dropout掩码相乘来实现。dropout掩码是通过创建一个二值掩码生成的，其中每个元素以概率p为1，否则为0。通过这样做，我们在隐藏状态中随机“丢弃”一定数量的神经元，这有助于防止网络对任何单一神经元过于依赖。这使得网络更为鲁棒，并且不容易在训练数据上过拟合。在应用dropout之后，通过将隐藏状态除以（1-p）来缩放隐藏状态，以确保隐藏状态的期望值得到保持。
- en: '`ys[t]` gives us our linear layer output for the current time step'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ys[t]`为当前时间步提供线性层的输出。'
- en: '`ps[t]` gives us our final softmax output (probabilities) for the current time
    step'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ps[t]`为当前时间步提供最终的softmax输出（概率）。'
- en: Calculations for `ys[t]` and `ps[t]` are outside the second loop because there
    is only one linear layer, as opposed to an arbitrary number of RNN layers.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 由于只有一层线性层，而不是任意数量的RNN层，因此`ys[t]`和`ps[t]`的计算位于第二个循环之外。
- en: Finally, we return the loss, and `hs[len(x)-1]` is used as `hprev` for our next
    sequence. We use the cache to fetch the gradients during the backward pass.
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，我们返回损失，并且`hs[len(x)-1]`被用作下一个序列的`hprev`。我们使用缓存来获取反向传播过程中的梯度。
- en: The choice was made to use the indexing `[t][l]` to store the hidden state for
    the l-th layer at time step t. This is because the model processes the input sequence
    one timestep at a time, and at each timestep, it updates the hidden state for
    each layer. By using the indexing `[t][l]`, we are able to keep track of the hidden
    state for each layer at each timestep, allowing us to easily perform the necessary
    computations for the forward pass.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 选择使用索引`[t][l]`来存储在时间步‘t’处第‘l’层的隐藏状态。这是因为模型一次处理一个时间步的输入序列，并且在每个时间步，它更新每一层的隐藏状态。通过使用索引`[t][l]`，我们能够跟踪每一层在每个时间步的隐藏状态，从而方便地执行前向传递所需的计算。
- en: Additionally, this indexing allows for easy access to the hidden state of the
    last timestep, which is returned as `hs[len(x)-1]`, as it is the hidden state
    of the last timestep in the sequence for each layer. This returned hidden state
    is used as the initial hidden state for the next sequence during the training
    process.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，这种索引允许轻松访问最后一个时间步的隐藏状态，该状态作为`hs[len(x)-1]`返回，因为它是每层序列中最后一个时间步的隐藏状态。这个返回的隐藏状态在训练过程中被用作下一个序列的初始隐藏状态。
- en: Let’s perform the forward pass. Remember, there is no batch dimension.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们进行前向传递。请记住，没有批量维度。
- en: '[PRE5]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Backward Pass
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 反向传播
- en: First, some intuition for the backwards pass of a RNN.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，了解RNN反向传播的一些直观感受。
- en: The key difference between backpropagation in a basic ANN and an RNN is the
    way the error is propagated through the network. While both ANNs and RNNs propagate
    the error from the output layer to the input layer, RNNs also propagate the error
    backwards through time, adjusting the weights and biases at each time step. This
    allows RNNs to process sequential data and maintain a “memory” in the form of
    its hidden state.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 基本ANN和RNN在反向传播中的关键区别在于错误在网络中的传播方式。虽然ANN和RNN都将错误从输出层传播到输入层，但RNN还会通过时间反向传播错误，在每个时间步调整权重和偏置。这使得RNN能够处理序列数据，并以隐藏状态的形式维持“记忆”。
- en: The BPTT (backpropagation through time) algorithm works by unrolling the RNN
    over time, creating a computational graph for each time step. The graph for this
    network can be seen [here](https://github.com/j0sephsasson/numpy-RNN/blob/main/README.md#dag-directed-acyclic-graph).
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: BPTT（时间反向传播）算法通过展开RNN并为每个时间步创建计算图来工作。这个网络的计算图可以在[这里](https://github.com/j0sephsasson/numpy-RNN/blob/main/README.md#dag-directed-acyclic-graph)查看。
- en: The gradients are then calculated for each time step and accumulated over the
    entire sequence.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 然后为每个时间步计算梯度，并在整个序列上累积。
- en: '[PRE6]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Same as the forward pass, let’s break it down.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 与前向传递相同，让我们分解它。
- en: The first thing this function does is initialize the gradients for the weights
    and biases to zero, similar to what happens in a feedforward ANN. This is something
    that confused me, so I am going to elaborate a bit more.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 这个函数的第一步是将权重和偏置的梯度初始化为零，这类似于前馈ANN中的情况。这是让我困惑的一点，因此我会进一步详细解释。
- en: By resetting the gradients to zero before every sequence, it ensures that the
    gradients calculated for the current sequence do not accumulate or add up with
    the gradients calculated in the previous sequences.
  id: totrans-83
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 通过在每个序列之前将梯度重置为零，确保当前序列计算的梯度不会与先前序列计算的梯度累积或叠加。
- en: This prevents the gradients from becoming too large, which can cause the optimization
    process to diverge and negatively impact model performance. Additionally, it allows
    for weight updates to be performed independently for each sequence, which can
    lead to more stable and consistent optimization.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 这可以防止梯度变得过大，从而导致优化过程发散并对模型性能产生负面影响。此外，它允许对每个序列独立执行权重更新，这可以导致更稳定和一致的优化。
- en: 'Then, it loops through the input sequence in reverse, performing the following
    computations for each time step t:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，它会反向遍历输入序列，对每个时间步t执行以下计算：
- en: Notice the comment, backprop into y, that link will explain what is happening
    perfectly. I also go into depth on this in a previous article you can check out
    [here](/coding-a-neural-network-from-scratch-in-numpy-31f04e4d605).
  id: totrans-86
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 注意注释，回传到y，这个链接将完美解释发生了什么。我也在之前的文章中深入探讨了这一点，你可以在[这里](/coding-a-neural-network-from-scratch-in-numpy-31f04e4d605)查看。
- en: Calculates the gradient of the hidden state `hs[t][l]` with respect to the loss,
    denoted by `dh`
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算隐藏状态`hs[t][l]`相对于损失的梯度，用`dh`表示。
- en: Calculates the gradient of the raw hidden state, denoted by `dhraw`
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算原始隐藏状态的梯度，用`dhraw`表示
- en: '**What is the difference between dh and dhraw?** Good question.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '**dh和dhraw的区别是什么？** 好问题。'
- en: The difference between `dh` and `dhraw` is that `dh` is the gradient of the
    hidden state `hs[t][l]` with respect to the loss, computed by backpropagating
    the gradient of the probabilities `ps[t]` of the softmax activation of the output
    layer. `dhraw` is the same gradient, but it is further backpropagated through
    the non-linear tanh activation function by element-wise multiplying the gradient
    of the hidden state `dh` with the derivative of the tanh function, which is (1
    - `hs[t][l]` * `hs[t][l]`).
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '`dh`和`dhraw`的区别在于，`dh`是相对于损失的隐藏状态`hs[t][l]`的梯度，通过反向传播输出层softmax激活的概率`ps[t]`的梯度计算得出。`dhraw`是相同的梯度，但它进一步通过非线性tanh激活函数反向传播，通过元素级相乘隐藏状态`dh`的梯度与tanh函数的导数（1
    - `hs[t][l]` * `hs[t][l]`）得到。'
- en: Calculates the gradient of the hidden bias `bh[l]`
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算隐藏偏置`bh[l]`的梯度
- en: Calculates the gradient of the input-hidden weights `Wxh[l]` with respect to
    the loss, denoted by `dWxh[l]`.
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算相对于损失的输入-隐藏权重`Wxh[l]`的梯度，用`dWxh[l]`表示。
- en: Calculates the gradient of the hidden-hidden weights `Whh[l]` with respect to
    the loss, denoted by `dWhh[l]`.
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算相对于损失的隐藏-隐藏权重`Whh[l]`的梯度，用`dWhh[l]`表示。
- en: Calculates the gradient of the next hidden state `dhnext[l]`.
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算下一个隐藏状态`dhnext[l]`的梯度。
- en: '![](../Images/e91d9fe6514462f99b70442da08f5562.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e91d9fe6514462f99b70442da08f5562.png)'
- en: Mathematical notation of backward pass computations. Image by author.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 反向传递计算的数学符号。作者提供的图像。
- en: Let’s perform the backward pass.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们进行反向传递。
- en: '[PRE7]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Finally, we return the gradients so we can update the parameters, and that is
    a segway into my next topic — the optimization.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们返回梯度，以便更新参数，这也是我下一个话题——优化的引子。
- en: Optimizer
  id: totrans-100
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 优化器
- en: Like it says in the RNN class ‘update’ method, we will be using Adagrad for
    this implementation.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 如RNN类中的‘update’方法所述，我们将使用Adagrad进行这个实现。
- en: Adagrad is an optimization algorithm that adapts the learning rate of each parameter
    in a neural network individually, based on the historical gradient information.
  id: totrans-102
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Adagrad 是一种优化算法，它根据历史梯度信息为神经网络中的每个参数单独调整学习率。
- en: It’s particularly useful for handling sparse data and is often used in natural
    language processing tasks. Adagrad makes adjustments to the learning rate at each
    iteration, ensuring that the model learns from the data as quickly and efficiently
    as possible.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 它特别适用于处理稀疏数据，通常用于自然语言处理任务。Adagrad 在每次迭代时调整学习率，确保模型尽可能快速和高效地从数据中学习。
- en: '[PRE8]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: This block of code updates the parameters of the RNN using the Adagrad optimization
    algorithm. It keeps track of the sum of squares of the gradients of the parameters
    (`mWxh`, `mWhh`, `mbh`, `mWhy` and `mby`) and divides the learning rate with the
    square root of that sum plus a small constant value of `1e-8`, to ensure numerical
    stability, effectively adjusting the learning rate for each parameter. Additionally,
    it clips the gradients to prevent [exploding gradients](https://machinelearningmastery.com/exploding-gradients-in-neural-networks/).
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码使用 Adagrad 优化算法更新 RNN 的参数。它跟踪参数的梯度平方和（`mWxh`、`mWhh`、`mbh`、`mWhy` 和 `mby`），并用这个和的平方根加上一个小常数
    `1e-8` 来除以学习率，以确保数值稳定，从而有效地调整每个参数的学习率。此外，它剪切梯度以防止 [梯度爆炸](https://machinelearningmastery.com/exploding-gradients-in-neural-networks/)。
- en: Adagrad adapts the learning rate for each parameter, performing larger updates
    for infrequent parameters and smaller updates for frequent parameters. Meaning,
    parameters that are updated infrequently, the learning rate will be larger, so
    that the model can make bigger adjustments to those parameters. On the other hand,
    for parameters that are updated frequently, the learning rate will be smaller,
    so that the model can make small adjustments to those parameters, preventing overfitting.
    This is in contrast to using a fixed learning rate, which could either under-correct
    or over-correct the parameters.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: Adagrad 为每个参数调整学习率，对不频繁更新的参数执行较大的更新，对频繁更新的参数执行较小的更新。这意味着，对于不频繁更新的参数，学习率会较大，以便模型能对这些参数做出更大的调整。另一方面，对于频繁更新的参数，学习率会较小，从而使模型对这些参数进行小幅调整，以防过拟合。这与使用固定学习率形成对比，后者可能会导致参数校正不足或过度校正。
- en: Let’s perform the parameter update.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们进行参数更新。
- en: '[PRE9]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Training
  id: totrans-109
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练
- en: The final piece is actually training the network, where the input sequences
    are fed through the network, the error is calculated and the optimizer updates
    the weights and biases.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一步实际上是训练网络，将输入序列输入网络中，计算错误，优化器更新权重和偏差。
- en: '[PRE10]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: This block of code is pretty straightforward. We are performing a forward and
    backward pass, and updating the model parameters every epoch.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码非常简单。我们执行前向和反向传播，并在每个纪元更新模型参数。
- en: Something I would like to point out —
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 我想指出的是——
- en: The loss is updated by a weighted average of the current loss and the previous
    loss.
  id: totrans-114
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 损失通过当前损失和前一个损失的加权平均来更新。
- en: The current loss is multiplied by 0.001 and added to the previous loss, which
    is multiplied by 0.999\. This means that the current loss will only have a small
    impact on the total loss, while the previous losses will have a larger impact.
    This way, the total loss will not fluctuate as much, and will be more stable over
    time.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 当前损失乘以 0.001 并加到前一个损失上，前一个损失乘以 0.999。这意味着当前损失对总损失的影响较小，而前面的损失影响较大。这样，总损失波动不会那么大，并且会随着时间更加稳定。
- en: By using an EMA (exponential moving average), it is easier to monitor the performance
    of the network and detect when it is overfitting or underfitting.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用 EMA（指数移动平均），更容易监控网络的性能，并检测它是否过拟合或欠拟合。
- en: '![](../Images/91ade5e2003cec0536e4f7e6880e1946.png)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/91ade5e2003cec0536e4f7e6880e1946.png)'
- en: Loss & text prediction at iteration zero. Image by author.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 第零次迭代的损失与文本预测。图片由作者提供。
- en: '![](../Images/6d180c19cec0dddaac9eecb78088be9e.png)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6d180c19cec0dddaac9eecb78088be9e.png)'
- en: Loss & text prediction at iteration 14,000\. Image by author.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 第 14,000 次迭代的损失与文本预测。图片由作者提供。
- en: '![](../Images/981f59809bea423f9fc013d586b3ee2d.png)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/981f59809bea423f9fc013d586b3ee2d.png)'
- en: Loss after 50,000 epochs.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 50,000 个纪元后的损失。
- en: The training process of our RNN has been successful, we can see the decrease
    in loss and the improved quality of the generated samples. However, it is important
    to note that generating original Shakespeare is a complex task, and this particular
    implementation is a simple vanilla RNN. Therefore, there is room for further improvement
    and experimentation with different architectures and techniques.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的 RNN 训练过程已经成功，我们可以看到损失的减少和生成样本质量的提高。然而，需要注意的是，生成原创莎士比亚文本是一个复杂的任务，而这一实现是一个简单的普通
    RNN。因此，仍有进一步改进和尝试不同架构和技术的空间。
- en: Conclusion
  id: totrans-124
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: In conclusion, this article has demonstrated the implementation and training
    of a character-level RNN using Numpy. The many-to-many architecture and online
    learning approach allows the network to adapt to new patterns in the data as they
    are encountered, resulting in improved sample generation. While this network is
    quasi-capable of generating original Shakespeare text, it is important to note
    that this is a simplified version and there are many other architectures and techniques
    that can be explored for much better performance.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，本文展示了如何使用 Numpy 实现和训练一个字符级 RNN。多对多架构和在线学习方法使得网络能够适应数据中的新模式，从而改进样本生成。虽然这个网络在生成原创莎士比亚文本方面具有一定能力，但需要注意的是，这只是一个简化版本，还有许多其他架构和技术可以探索，以获得更好的性能。
- en: Full code & repo [here](https://github.com/j0sephsasson/numpy-RNN).
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 完整代码和仓库 [在这里](https://github.com/j0sephsasson/numpy-RNN)。
- en: Feel free to get in touch & ask questions, or make improvements to the code.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 随时与我们联系并提出问题，或对代码进行改进。
- en: '**Thanks for reading!**'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '**感谢阅读！**'
