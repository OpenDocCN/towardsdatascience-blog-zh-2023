- en: Understanding KL Divergence
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解KL散度
- en: 原文：[https://towardsdatascience.com/understanding-kl-divergence-f3ddc8dff254?source=collection_archive---------0-----------------------#2023-02-02](https://towardsdatascience.com/understanding-kl-divergence-f3ddc8dff254?source=collection_archive---------0-----------------------#2023-02-02)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/understanding-kl-divergence-f3ddc8dff254?source=collection_archive---------0-----------------------#2023-02-02](https://towardsdatascience.com/understanding-kl-divergence-f3ddc8dff254?source=collection_archive---------0-----------------------#2023-02-02)
- en: '![](../Images/23ed4c262576e3be5227e532e0eedf31.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/23ed4c262576e3be5227e532e0eedf31.png)'
- en: Image by author
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供
- en: A guide to the math, intuition, and practical use of KL divergence — including
    how it is best used in drift monitoring
  id: totrans-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: KL散度的数学、直观理解和实际应用指南——包括如何在漂移监测中最佳使用它
- en: '[](https://aparnadhinak.medium.com/?source=post_page-----f3ddc8dff254--------------------------------)[![Aparna
    Dhinakaran](../Images/e431ee69563ecb27c86f3428ba53574c.png)](https://aparnadhinak.medium.com/?source=post_page-----f3ddc8dff254--------------------------------)[](https://towardsdatascience.com/?source=post_page-----f3ddc8dff254--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----f3ddc8dff254--------------------------------)
    [Aparna Dhinakaran](https://aparnadhinak.medium.com/?source=post_page-----f3ddc8dff254--------------------------------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://aparnadhinak.medium.com/?source=post_page-----f3ddc8dff254--------------------------------)[![Aparna
    Dhinakaran](../Images/e431ee69563ecb27c86f3428ba53574c.png)](https://aparnadhinak.medium.com/?source=post_page-----f3ddc8dff254--------------------------------)[](https://towardsdatascience.com/?source=post_page-----f3ddc8dff254--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----f3ddc8dff254--------------------------------)
    [Aparna Dhinakaran](https://aparnadhinak.medium.com/?source=post_page-----f3ddc8dff254--------------------------------)'
- en: ·
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ff32f85889f3a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-kl-divergence-f3ddc8dff254&user=Aparna+Dhinakaran&userId=f32f85889f3a&source=post_page-f32f85889f3a----f3ddc8dff254---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----f3ddc8dff254--------------------------------)
    ·7 min read·Feb 2, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Ff3ddc8dff254&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-kl-divergence-f3ddc8dff254&user=Aparna+Dhinakaran&userId=f32f85889f3a&source=-----f3ddc8dff254---------------------clap_footer-----------)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ff32f85889f3a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-kl-divergence-f3ddc8dff254&user=Aparna+Dhinakaran&userId=f32f85889f3a&source=post_page-f32f85889f3a----f3ddc8dff254---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----f3ddc8dff254--------------------------------)
    ·7分钟阅读·2023年2月2日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Ff3ddc8dff254&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-kl-divergence-f3ddc8dff254&user=Aparna+Dhinakaran&userId=f32f85889f3a&source=-----f3ddc8dff254---------------------clap_footer-----------)'
- en: --
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff3ddc8dff254&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-kl-divergence-f3ddc8dff254&source=-----f3ddc8dff254---------------------bookmark_footer-----------)'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff3ddc8dff254&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-kl-divergence-f3ddc8dff254&source=-----f3ddc8dff254---------------------bookmark_footer-----------)'
- en: '**Kullback-Leibler divergence** metric (relative entropy) is a statistical
    measurement from information theory that is commonly used to quantify the difference
    between one probability distribution from a reference probability distribution.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '**Kullback-Leibler散度**度量（相对熵）是信息理论中的一种统计测量，通常用于量化一个概率分布与参考概率分布之间的差异。'
- en: While it is popular, KL divergence is sometimes misunderstood. In practice,
    it can also sometimes be difficult to know when to use one statistical distance
    check over another.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管KL散度很受欢迎，但有时会被误解。在实践中，有时也很难知道何时应选择一种统计距离检查而非另一种。
- en: This blog covers how to use KL divergence, how it works in practice, and when
    KL divergence should and should not be used to monitor for drift.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 本博客介绍了如何使用KL散度，它在实践中的工作原理，以及何时应使用或不应使用KL散度来监测漂移。
- en: How Do You Calculate KL Divergence?
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 你如何计算KL散度？
- en: KL divergence is a non-symmetric metric that [measures the relative entropy](http://hanj.cs.illinois.edu/cs412/bk3/KL-divergence.pdf)
    or difference in information represented by two distributions. It can be thought
    of as measuring the distance between two data distributions showing how different
    the two distributions are from each other.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: KL 散度是一种非对称度量， [测量相对熵](http://hanj.cs.illinois.edu/cs412/bk3/KL-divergence.pdf)
    或两个分布所表示的信息差异。它可以被视为测量两个数据分布之间的距离，显示这两个分布彼此之间的差异。
- en: There is both a continuous form of KL divergence
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: KL 散度有连续形式
- en: '![](../Images/6ad674da352f58e57e2b6d350c36194a.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6ad674da352f58e57e2b6d350c36194a.png)'
- en: Image by author
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片
- en: 'And a discrete form of KL Divergence:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 以及离散形式的 KL 散度：
- en: '![](../Images/8ab7c7798e76224f948d0de7b8caf822.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8ab7c7798e76224f948d0de7b8caf822.png)'
- en: Image by author
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片
- en: In model monitoring, most practitioners almost exclusively use the discrete
    form of KL divergence and obtain the discrete distributions by [binning data](https://arize.com/blog-course/data-binning-production/).
    The discrete form of KL divergence and continuous forms do converge as the number
    of samples and bins limit move to infinity. There are optimal [selection approaches](https://stats.stackexchange.com/questions/510699/discrete-kl-divergence-with-decreasing-bin-width)
    to the number of bins to approach the continuous form. In practice, the number
    of bins can be far less than the above number implies — and how you create those
    bins to handle the case of 0 sample bins is more important practically speaking
    than anything else (a future post with code will address how to handle zero bins
    naturally).
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在模型监控中，大多数实践者几乎专门使用离散形式的 KL 散度，并通过 [数据分箱](https://arize.com/blog-course/data-binning-production/)
    来获得离散分布。离散形式的 KL 散度与连续形式在样本数量和箱子限制趋向于无限时会收敛。存在最佳的 [选择方法](https://stats.stackexchange.com/questions/510699/discrete-kl-divergence-with-decreasing-bin-width)
    来接近连续形式。在实践中，箱子的数量可能远少于上述数字所示的数量——如何创建这些箱子以处理零样本箱的情况在实际操作中比其他任何事情更为重要（未来的文章将用代码处理如何自然处理零箱问题）。
- en: How Is KL Divergence Used in Model Monitoring?
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: KL 散度如何用于模型监控？
- en: In model monitoring, KL divergence is used to monitor production environments,
    specifically around feature and prediction data. KL Divergence is utilized to
    ensure that input or output data in production doesn’t drastically change from
    a baseline. The baseline can be a training production window of data or a training
    or validation dataset.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在模型监控中，KL 散度用于监控生产环境，特别是围绕特征和预测数据。KL 散度被用来确保生产中的输入或输出数据不会与基准发生剧烈变化。基准可以是训练生产数据窗口，或者是训练或验证数据集。
- en: Drift monitoring can be especially useful for teams that receive delayed ground
    truth to compare against production model decisions. These teams can rely on changes
    in prediction and feature distributions as a proxy for performance.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 漂移监控对于那些接收延迟的真实数据来与生产模型决策进行比较的团队尤其有用。这些团队可以依赖预测和特征分布的变化作为性能的代理。
- en: KL divergence is typically applied to each feature independently; it is not
    designed as a covariant feature measure but rather a metric that shows how each
    feature has diverged independently from the baseline values.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: KL 散度通常应用于每个特征，独立地进行计算；它不是作为协方差特征度量的设计，而是显示每个特征如何独立于基准值发生偏离的度量。
- en: '![](../Images/52a909cc15e9bc7d6942a8712757c40b.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/52a909cc15e9bc7d6942a8712757c40b.png)'
- en: Image by author
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片
- en: The p(x) shown above in orange stripes is the reference or baseline distribution.
    The most common baselines are either a trailing window of production data or the
    training datasets. Each bin additively contributes to KL divergence. The bins
    jointly add up to the total percent distribution.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 上面橙色条纹中显示的 p(x) 是参考或基准分布。最常见的基准要么是生产数据的滞后窗口，要么是训练数据集。每个箱子会累加地贡献于 KL 散度。这些箱子共同加总到总的百分比分布中。
- en: '*✏️* ***NOTE****:* sometimes non-practitioners have a somewhat overzealous
    goal of perfecting the mathematics of catching data changes. In practice, it’s
    important to keep in mind that real data changes all the time in production and
    many models extend well to this modified data. The goal of using drift metrics
    is to have a solid, stable and strongly useful metric that enables troubleshooting.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '*✏️* ***注意***:* 有时非实践者可能有一种过于热心的目标，即完美捕捉数据变化的数学方法。在实践中，需要记住，实际数据在生产中总是不断变化的，许多模型对这种修改后的数据有很好的适应性。使用漂移度量的目标是拥有一个稳固、稳定且非常有用的度量，以便进行故障排除。'
- en: Is KL Divergence An Asymmetric Metric?
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: KL 散度是非对称度量吗？
- en: Yes. If you swap the baseline distribution p(x) and sample distribution q(x),
    you will get a different number. Being an asymmetric metric has a number of disadvantages
    as teams use KL divergence for troubleshooting data model comparisons. There are
    times where teams want to swap out a comparison baseline for a different distribution
    in a troubleshooting workflow, and having a metric where *A / B* is different
    than *B / A* can make comparing results difficult.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 是的。如果你交换基线分布 p(x) 和样本分布 q(x)，你会得到不同的数字。作为一个非对称度量，KL 散度在团队使用它进行数据模型比较时有一些缺点。有时团队希望在故障排除工作流程中用不同的分布替换比较基线，而拥有一个*A
    / B* 与*B / A* 不同的度量可能会使结果比较变得困难。
- en: '![](../Images/3cb72a873aabe21985a675c56cba853e.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3cb72a873aabe21985a675c56cba853e.png)'
- en: Image by author
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图像
- en: 'This is one reason that model monitoring tools like Arize (full disclosure:
    I co-founded Arize) defaults to [population stability index](https://arize.com/blog-course/population-stability-index-psi/)
    (PSI), a symmetric derivation of KL Divergence, as one of the main metrics to
    use for model monitoring of distributions.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 这也是模型监控工具如 Arize（完全披露：我共同创立了 Arize）将[总体稳定性指数](https://arize.com/blog-course/population-stability-index-psi/)（PSI），KL
    散度的对称衍生物，作为模型监控分布的主要指标之一的原因。
- en: Differences Between Continuous Numeric and Categorical Features
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 连续数值特征与分类特征之间的差异
- en: KL divergence can be used to measure differences between numeric distributions
    and categorical distributions.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: KL 散度可以用于测量数值分布与分类分布之间的差异。
- en: '![](../Images/521325cae5eaa261781ae9deab73eb81.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/521325cae5eaa261781ae9deab73eb81.png)'
- en: Image by author
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图像
- en: '**Numerics**'
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**数值**'
- en: In the case of numeric distributions, the data is split into bins based on cutoff
    points, bin sizes and bin widths. The binning strategies can be even bins, quintiles
    and complex mixes of strategies and does affect KL divergence in a big way.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 对于数值分布，数据根据切割点、箱体大小和箱体宽度被分成多个箱。箱体策略可以是均匀箱体、分位数或者复杂的混合策略，这会对 KL 散度产生显著影响。
- en: '**Categorical**'
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**分类**'
- en: The monitoring of KL divergence tracks large distributional shifts in the categorical
    datasets. In the case of categorical features, often there is a size where the
    cardinality gets too large for the measure to have much usefulness. The ideal
    size is around 50–100 unique values — as a distribution has higher cardinality,
    the question of how different the two distributions are and if it really matters
    gets muddied.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: KL 散度的监测跟踪分类数据集中的大规模分布变化。对于分类特征，通常有一个大小，使得基数变得太大，导致度量的实用性降低。理想的大小是约 50–100 个独特值——当分布具有更高基数时，两个分布之间的差异及其重要性的问题会变得模糊。
- en: High Cardinality
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 高基数
- en: 'In the case of high cardinality feature monitoring, out-of-the-box statistical
    distances do not generally work well — instead, we typically recommend two options:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在高基数字特征监测的情况下，现成的统计距离通常效果不好——我们通常推荐两个选项：
- en: '**Embeddings**: In some high cardinality situations, the values being used
    — such as User ID or Content ID — are already used to create embeddings internally.
    [Embedding drift monitoring](/measuring-embedding-drift-aa9b7ddb84ae) can help.'
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**嵌入**：在一些高基数的情况下，使用的值——如用户 ID 或内容 ID——已经被用于内部创建嵌入。[嵌入漂移监测](/measuring-embedding-drift-aa9b7ddb84ae)可以提供帮助。'
- en: '**Pure High Cardinality Categorical**: In other cases, where the model has
    encoded the inputs to a large space, just monitoring the top 50–100 top items
    with KL Divergence and all other values as “other” can be useful.'
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**纯高基数分类**：在其他情况下，当模型将输入编码到较大的空间时，使用 KL 散度监测前 50–100 个顶级项以及所有其他值作为“其他”可能会很有用。'
- en: Lastly, sometimes what you want to monitor is something very specific such as
    the percent of new values or bins in a period. These can be setup more specifically
    with data quality monitors.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，有时你想监测的是某个特定的内容，如在某个周期内的新值或箱体的百分比。这些可以通过数据质量监控工具进行更具体的设置。
- en: KL Divergence Example
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: KL 散度示例
- en: Here is an example of [KL divergence](https://arize.com/blog-course/kl-divergence/).
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个[KL 散度](https://arize.com/blog-course/kl-divergence/)的示例。
- en: '![](../Images/f22bd63ba1fc4b894ad6c59afb8d4bfc.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f22bd63ba1fc4b894ad6c59afb8d4bfc.png)'
- en: Image by author
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图像
- en: Imagine we have a numeric distribution of charge amounts for a model predicting
    credit card fraud. The model was built with the baseline shown in the picture
    above from training. We can see that the distribution of charges has shifted.
    There are a number of industry standards around thresholds, we actually recommend
    using a production trailing value to set an auto threshold. There are many examples
    in production where the fixed setting of <> doesn’t make sense.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下，我们有一个用于预测信用卡欺诈的模型的数值分布。该模型是基于上图所示的基准进行训练的。我们可以看到收费的分布发生了变化。行业标准对阈值有一些规定，实际上我们建议使用生产中滞后的值来设置自动阈值。在生产中，许多固定设置的<>并不合适。
- en: Intuition Behind KL Divergence
  id: totrans-53
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: KL 散度的直觉
- en: It’s important to have a bit of intuition around the metric and changes in the
    metric based on distribution changes.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 对于度量及其基于分布变化的变化，拥有一定的直觉非常重要。
- en: '![](../Images/c7e9b9e7cfb3d9d19d5af4f923ae61db.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c7e9b9e7cfb3d9d19d5af4f923ae61db.png)'
- en: Image by author
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图像
- en: The above example shows a move from one categorical bin to another. The predictions
    with “medical” as input on a feature (use of loan proceeds) increased from 2%
    to 8%, while the predictions with “vacation” decreased from 23% to 17%.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 上述例子展示了从一个类别分箱移动到另一个分箱的情况。以“医疗”作为特征（贷款用途）的预测从 2% 增加到 8%，而以“度假”为特征的预测从 23% 减少到
    17%。
- en: In this example, the component to KL divergence related to “medical” is -0.028
    and is smaller than the component for the “vacation” percentage movement of 0.070.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，与“医疗”相关的 KL 散度组件是 -0.028，且小于“度假”百分比变化的 0.070 组件。
- en: In general, movements that decrease the percentage and move it toward 0 will
    have a larger effect on this statistic relative to increases in the percentage.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，减少百分比并将其向 0 移动的变化对该统计量的影响大于百分比的增加。
- en: '![](../Images/5b4a8fcc6994cdbaa6158d4deaa6b24a.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5b4a8fcc6994cdbaa6158d4deaa6b24a.png)'
- en: Image by author
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图像
- en: About the only way to get a large movement relative to the industry standard
    number of 0.2 is to move a bin down toward 0\. In this example, moving a bin from
    9% to 0.5% moves KL divergence a large amount.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 相对于行业标准 0.2，要获得较大的变动，唯一的方法是将一个分箱向 0 移动。在这个例子中，将一个分箱从 9% 移动到 0.5% 会大幅改变 KL 散度。
- en: '[Here is a spreadsheet](https://docs.google.com/spreadsheets/d/1BNjGmJJOZpKdOhHHskVl9yT5lhhyPfNaI6KbDmb3R9Q/edit?usp=sharing)
    for those that want to play with and modify these percentages to better understand
    the intuition. It’s worth noting that this intuition is very different from PSI.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '[这里是一个电子表格](https://docs.google.com/spreadsheets/d/1BNjGmJJOZpKdOhHHskVl9yT5lhhyPfNaI6KbDmb3R9Q/edit?usp=sharing)，供那些想要操作和修改这些百分比以更好地理解直觉的人使用。值得注意的是，这种直觉与
    PSI 的直觉非常不同。'
- en: Conclusion
  id: totrans-64
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: If you are thinking about using KL divergence to measure drift, a few things
    are important to keep in mind. First, most practitioners find it easiest to use
    the discrete form of KL divergence and obtain the discrete distributions by binning
    data (more on binning challenges in a future post). Second, while it is important
    to understand the intuition and math behind KL divergence, sometimes other metrics
    like PSI — a symmetric derivation of KL divergence — or approaches may be more
    useful depending on your use case.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你考虑使用 KL 散度来衡量漂移，需要记住几件重要的事情。首先，大多数实践者发现使用 KL 散度的离散形式最为简单，并通过对数据进行分箱来获取离散分布（有关分箱挑战的更多信息将在未来的帖子中介绍）。其次，虽然理解
    KL 散度背后的直觉和数学很重要，但有时其他度量如 PSI —— KL 散度的对称推导 —— 或其他方法可能更适合你的用例。
