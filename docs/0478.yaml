- en: Understanding KL Divergence
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/understanding-kl-divergence-f3ddc8dff254?source=collection_archive---------0-----------------------#2023-02-02](https://towardsdatascience.com/understanding-kl-divergence-f3ddc8dff254?source=collection_archive---------0-----------------------#2023-02-02)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/23ed4c262576e3be5227e532e0eedf31.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: A guide to the math, intuition, and practical use of KL divergence — including
    how it is best used in drift monitoring
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://aparnadhinak.medium.com/?source=post_page-----f3ddc8dff254--------------------------------)[![Aparna
    Dhinakaran](../Images/e431ee69563ecb27c86f3428ba53574c.png)](https://aparnadhinak.medium.com/?source=post_page-----f3ddc8dff254--------------------------------)[](https://towardsdatascience.com/?source=post_page-----f3ddc8dff254--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----f3ddc8dff254--------------------------------)
    [Aparna Dhinakaran](https://aparnadhinak.medium.com/?source=post_page-----f3ddc8dff254--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ff32f85889f3a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-kl-divergence-f3ddc8dff254&user=Aparna+Dhinakaran&userId=f32f85889f3a&source=post_page-f32f85889f3a----f3ddc8dff254---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----f3ddc8dff254--------------------------------)
    ·7 min read·Feb 2, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Ff3ddc8dff254&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-kl-divergence-f3ddc8dff254&user=Aparna+Dhinakaran&userId=f32f85889f3a&source=-----f3ddc8dff254---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff3ddc8dff254&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-kl-divergence-f3ddc8dff254&source=-----f3ddc8dff254---------------------bookmark_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Kullback-Leibler divergence** metric (relative entropy) is a statistical
    measurement from information theory that is commonly used to quantify the difference
    between one probability distribution from a reference probability distribution.'
  prefs: []
  type: TYPE_NORMAL
- en: While it is popular, KL divergence is sometimes misunderstood. In practice,
    it can also sometimes be difficult to know when to use one statistical distance
    check over another.
  prefs: []
  type: TYPE_NORMAL
- en: This blog covers how to use KL divergence, how it works in practice, and when
    KL divergence should and should not be used to monitor for drift.
  prefs: []
  type: TYPE_NORMAL
- en: How Do You Calculate KL Divergence?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: KL divergence is a non-symmetric metric that [measures the relative entropy](http://hanj.cs.illinois.edu/cs412/bk3/KL-divergence.pdf)
    or difference in information represented by two distributions. It can be thought
    of as measuring the distance between two data distributions showing how different
    the two distributions are from each other.
  prefs: []
  type: TYPE_NORMAL
- en: There is both a continuous form of KL divergence
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6ad674da352f58e57e2b6d350c36194a.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: 'And a discrete form of KL Divergence:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8ab7c7798e76224f948d0de7b8caf822.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: In model monitoring, most practitioners almost exclusively use the discrete
    form of KL divergence and obtain the discrete distributions by [binning data](https://arize.com/blog-course/data-binning-production/).
    The discrete form of KL divergence and continuous forms do converge as the number
    of samples and bins limit move to infinity. There are optimal [selection approaches](https://stats.stackexchange.com/questions/510699/discrete-kl-divergence-with-decreasing-bin-width)
    to the number of bins to approach the continuous form. In practice, the number
    of bins can be far less than the above number implies — and how you create those
    bins to handle the case of 0 sample bins is more important practically speaking
    than anything else (a future post with code will address how to handle zero bins
    naturally).
  prefs: []
  type: TYPE_NORMAL
- en: How Is KL Divergence Used in Model Monitoring?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In model monitoring, KL divergence is used to monitor production environments,
    specifically around feature and prediction data. KL Divergence is utilized to
    ensure that input or output data in production doesn’t drastically change from
    a baseline. The baseline can be a training production window of data or a training
    or validation dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Drift monitoring can be especially useful for teams that receive delayed ground
    truth to compare against production model decisions. These teams can rely on changes
    in prediction and feature distributions as a proxy for performance.
  prefs: []
  type: TYPE_NORMAL
- en: KL divergence is typically applied to each feature independently; it is not
    designed as a covariant feature measure but rather a metric that shows how each
    feature has diverged independently from the baseline values.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/52a909cc15e9bc7d6942a8712757c40b.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: The p(x) shown above in orange stripes is the reference or baseline distribution.
    The most common baselines are either a trailing window of production data or the
    training datasets. Each bin additively contributes to KL divergence. The bins
    jointly add up to the total percent distribution.
  prefs: []
  type: TYPE_NORMAL
- en: '*✏️* ***NOTE****:* sometimes non-practitioners have a somewhat overzealous
    goal of perfecting the mathematics of catching data changes. In practice, it’s
    important to keep in mind that real data changes all the time in production and
    many models extend well to this modified data. The goal of using drift metrics
    is to have a solid, stable and strongly useful metric that enables troubleshooting.'
  prefs: []
  type: TYPE_NORMAL
- en: Is KL Divergence An Asymmetric Metric?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Yes. If you swap the baseline distribution p(x) and sample distribution q(x),
    you will get a different number. Being an asymmetric metric has a number of disadvantages
    as teams use KL divergence for troubleshooting data model comparisons. There are
    times where teams want to swap out a comparison baseline for a different distribution
    in a troubleshooting workflow, and having a metric where *A / B* is different
    than *B / A* can make comparing results difficult.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3cb72a873aabe21985a675c56cba853e.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: 'This is one reason that model monitoring tools like Arize (full disclosure:
    I co-founded Arize) defaults to [population stability index](https://arize.com/blog-course/population-stability-index-psi/)
    (PSI), a symmetric derivation of KL Divergence, as one of the main metrics to
    use for model monitoring of distributions.'
  prefs: []
  type: TYPE_NORMAL
- en: Differences Between Continuous Numeric and Categorical Features
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: KL divergence can be used to measure differences between numeric distributions
    and categorical distributions.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/521325cae5eaa261781ae9deab73eb81.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: '**Numerics**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the case of numeric distributions, the data is split into bins based on cutoff
    points, bin sizes and bin widths. The binning strategies can be even bins, quintiles
    and complex mixes of strategies and does affect KL divergence in a big way.
  prefs: []
  type: TYPE_NORMAL
- en: '**Categorical**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The monitoring of KL divergence tracks large distributional shifts in the categorical
    datasets. In the case of categorical features, often there is a size where the
    cardinality gets too large for the measure to have much usefulness. The ideal
    size is around 50–100 unique values — as a distribution has higher cardinality,
    the question of how different the two distributions are and if it really matters
    gets muddied.
  prefs: []
  type: TYPE_NORMAL
- en: High Cardinality
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the case of high cardinality feature monitoring, out-of-the-box statistical
    distances do not generally work well — instead, we typically recommend two options:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Embeddings**: In some high cardinality situations, the values being used
    — such as User ID or Content ID — are already used to create embeddings internally.
    [Embedding drift monitoring](/measuring-embedding-drift-aa9b7ddb84ae) can help.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Pure High Cardinality Categorical**: In other cases, where the model has
    encoded the inputs to a large space, just monitoring the top 50–100 top items
    with KL Divergence and all other values as “other” can be useful.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Lastly, sometimes what you want to monitor is something very specific such as
    the percent of new values or bins in a period. These can be setup more specifically
    with data quality monitors.
  prefs: []
  type: TYPE_NORMAL
- en: KL Divergence Example
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Here is an example of [KL divergence](https://arize.com/blog-course/kl-divergence/).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f22bd63ba1fc4b894ad6c59afb8d4bfc.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: Imagine we have a numeric distribution of charge amounts for a model predicting
    credit card fraud. The model was built with the baseline shown in the picture
    above from training. We can see that the distribution of charges has shifted.
    There are a number of industry standards around thresholds, we actually recommend
    using a production trailing value to set an auto threshold. There are many examples
    in production where the fixed setting of <> doesn’t make sense.
  prefs: []
  type: TYPE_NORMAL
- en: Intuition Behind KL Divergence
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It’s important to have a bit of intuition around the metric and changes in the
    metric based on distribution changes.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c7e9b9e7cfb3d9d19d5af4f923ae61db.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: The above example shows a move from one categorical bin to another. The predictions
    with “medical” as input on a feature (use of loan proceeds) increased from 2%
    to 8%, while the predictions with “vacation” decreased from 23% to 17%.
  prefs: []
  type: TYPE_NORMAL
- en: In this example, the component to KL divergence related to “medical” is -0.028
    and is smaller than the component for the “vacation” percentage movement of 0.070.
  prefs: []
  type: TYPE_NORMAL
- en: In general, movements that decrease the percentage and move it toward 0 will
    have a larger effect on this statistic relative to increases in the percentage.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5b4a8fcc6994cdbaa6158d4deaa6b24a.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: About the only way to get a large movement relative to the industry standard
    number of 0.2 is to move a bin down toward 0\. In this example, moving a bin from
    9% to 0.5% moves KL divergence a large amount.
  prefs: []
  type: TYPE_NORMAL
- en: '[Here is a spreadsheet](https://docs.google.com/spreadsheets/d/1BNjGmJJOZpKdOhHHskVl9yT5lhhyPfNaI6KbDmb3R9Q/edit?usp=sharing)
    for those that want to play with and modify these percentages to better understand
    the intuition. It’s worth noting that this intuition is very different from PSI.'
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you are thinking about using KL divergence to measure drift, a few things
    are important to keep in mind. First, most practitioners find it easiest to use
    the discrete form of KL divergence and obtain the discrete distributions by binning
    data (more on binning challenges in a future post). Second, while it is important
    to understand the intuition and math behind KL divergence, sometimes other metrics
    like PSI — a symmetric derivation of KL divergence — or approaches may be more
    useful depending on your use case.
  prefs: []
  type: TYPE_NORMAL
