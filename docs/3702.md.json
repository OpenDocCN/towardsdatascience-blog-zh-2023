["```py\npip install tsgm\n```", "```py\nimport numpy as np\nimport functools\nimport sklearn\nimport tensorflow as tf\nfrom tensorflow import keras\n\nimport tsgm\n\nn, n_ts, n_features = 100, 100, 20\nvae_latent_dim = 8\n\n# Load data that will be used as real\nXr, yr = tsgm.utils.gen_sine_vs_const_dataset(n, n_ts, n_features, max_value=2, const=1)\nXr = Xr.astype(np.float32)\nyr = keras.utils.to_categorical(yr).astype(np.float32)\nys = yr  # use real labels as synthetic labels\n\n# Using real data generate synthetic time series dataset\nscaler = tsgm.utils.TSFeatureWiseScaler()        \nscaled_data = scaler.fit_transform(Xr)\narchitecture = tsgm.models.zoo[\"cvae_conv5\"](n_ts, n_features, vae_latent_dim)\nencoder, decoder = architecture.encoder, architecture.decoder\nvae = tsgm.models.cvae.cBetaVAE(encoder, decoder, latent_dim=vae_latent_dim, temporal=False)\nvae.compile(optimizer=keras.optimizers.Adam())\n\n# Train VAE using historical data\nvae.fit(scaled_data, yr, epochs=1, batch_size=64)\nXs, ys = vae.generate(ys)\n\nd_real = tsgm.dataset.Dataset(Xr, yr)\nd_syn = tsgm.dataset.Dataset(Xs, ys)\n```", "```py\nstatistics = [\n    functools.partial(tsgm.metrics.statistics.axis_max_s, axis=None),\n    functools.partial(tsgm.metrics.statistics.axis_min_s, axis=None),\n    functools.partial(tsgm.metrics.statistics.axis_max_s, axis=1),\n    functools.partial(tsgm.metrics.statistics.axis_min_s, axis=1)]\n```", "```py\ndiscrepancy_func = lambda x, y: np.linalg.norm(x - y)\n```", "```py\ndist_metric = tsgm.metrics.DistanceMetric(\n    statistics=statistics, discrepancy=discrepancy_func\n)\nprint(dist_metric(d_real, d_syn))\n```", "```py\nmmd_metric = tsgm.metrics.MMDMetric()\nprint(mmd_metric(Xr, Xs))\n```", "```py\n# use LSTM classification model from TSGM zoo.\nmodel = tsgm.models.zoo[\"clf_cl_n\"](\n    seq_len=Xr.shape[1], feat_dim=Xr.shape[2], output_dim=1).model\nmodel.compile(\n    tf.keras.optimizers.Adam(),\n    tf.keras.losses.CategoricalCrossentropy(from_logits=False)\n)\n\n# use TSGM metric to measure the score\ndiscr_metric = tsgm.metrics.DiscriminativeMetric()\nprint(\n    discr_metric(\n        d_hist=Xr, d_syn=Xs, model=model,\n        test_size=0.2, random_seed=42, n_epochs=10\n    )\n)\n```", "```py\nclass EvaluatorConvLSTM():\n    '''\n    NB an oversimplified classifier, for educational purposes only.\n    '''\n\n    def __init__(self, model):\n        self._model = model\n\n    def evaluate(self, D: tsgm.dataset.Dataset, D_test: tsgm.dataset.Dataset) -> float:\n        X_train, y_train = D.Xy\n        X_test, y_test = D_test.Xy\n\n        self._model.fit(X_train, y_train)\n\n        y_pred = np.argmax(self._model.predict(X_test), 1)\n        print(self._model.predict(X_test).shape)\n        y_test = np.argmax(y_test, 1)\n        return sklearn.metrics.accuracy_score(y_pred, y_test)\n\n# Define a set of models\nseq_len, feat_dim, n_classes = *Xr.shape[1:], 2\nmodels = [tsgm.models.zoo[\"clf_cl_n\"](seq_len, feat_dim, n_classes, n_conv_lstm_blocks=i) for i in range(1, 4)]\nfor m in models:\n    m.model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\nevaluators = [EvaluatorConvLSTM(m.model) for m in models]\n\n# Utilize the set of evaluators with ConsistencyMetric from tsgm\nconsistency_metric = tsgm.metrics.ConsistencyMetric(evaluators=evaluators)\nprint(consistency_metric(d_real, d_syn, d_real))\n```", "```py\ndownstream_model = tsgm.models.zoo[\"clf_cl_n\"](seq_len, feat_dim, n_classes, n_conv_lstm_blocks=1).model\ndownstream_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n\nevaluator = EvaluatorConvLSTM(downstream_model)\n\ndownstream_perf_metric = tsgm.metrics.DownstreamPerformanceMetric(evaluator)\nprint(downstream_perf_metric(d_real, d_syn, d_real))\n```", "```py\nclass FlattenTSOneClassSVM:\n    def __init__(self, clf):\n        self._clf = clf\n\n    def fit(self, X):\n        X_fl = X.reshape(X.shape[0], -1)\n        self._clf.fit(X_fl)\n\n    def predict(self, X):\n        X_fl = X.reshape(X.shape[0], -1)\n        return self._clf.predict(X_fl)\n\nattacker = FlattenTSOneClassSVM(sklearn.svm.OneClassSVM())\nprivacy_metric = tsgm.metrics.PrivacyMembershipInferenceMetric(\n    attacker=attacker\n)\n```", "```py\nX_test, y_test = tsgm.utils.gen_sine_vs_const_dataset(10, 100, 20, max_value=2, const=1)\nd_test = tsgm.dataset.Dataset(X_test, keras.utils.to_categorical(y_test))\n\n# 1 indicates high privacy and 0 -- low privacy.\nprivacy_metric(d_real, d_syn, d_test)\n```", "```py\nspec_entropy = tsgm.metrics.EntropyMetric()\nprint(spec_entropy(Xr))\nprint(spec_entropy(Xs))\n```", "```py\ntsgm.utils.visualize_tsne_unlabeled(Xr, Xs, perplexity=10, markersize=20, alpha=0.5)\n```", "```py\n@article{\n  nikitin2023tsgm,\n  title={TSGM: A Flexible Framework for Generative Modeling of Synthetic Time Series},\n  author={Nikitin, Alexander and Iannucci, Letizia and Kaski, Samuel},\n  journal={arXiv preprint arXiv:2305.11567},\n  year={2023}\n}\n```"]