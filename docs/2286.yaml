- en: 'Practical Introduction to Transformer Models: BERT'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/practical-introduction-to-transformer-models-bert-4715ed0deede?source=collection_archive---------6-----------------------#2023-07-17](https://towardsdatascience.com/practical-introduction-to-transformer-models-bert-4715ed0deede?source=collection_archive---------6-----------------------#2023-07-17)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/15139494860ad96a134be1b20fb42fa3.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Alex Padurariu](https://unsplash.com/@alexpadurariu?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: '[Hands-on Tutorials](https://towardsdatascience.com/tagged/hands-on-tutorials)'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Hands-on tutorial on how to build your first sentiment analysis model using
    BERT
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@shashank.kapadia?source=post_page-----4715ed0deede--------------------------------)[![Shashank
    Kapadia](../Images/347e4cb92a7d27f032c5761e4526f2fa.png)](https://medium.com/@shashank.kapadia?source=post_page-----4715ed0deede--------------------------------)[](https://towardsdatascience.com/?source=post_page-----4715ed0deede--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----4715ed0deede--------------------------------)
    [Shashank Kapadia](https://medium.com/@shashank.kapadia?source=post_page-----4715ed0deede--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fcc7314ace45c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpractical-introduction-to-transformer-models-bert-4715ed0deede&user=Shashank+Kapadia&userId=cc7314ace45c&source=post_page-cc7314ace45c----4715ed0deede---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----4715ed0deede--------------------------------)
    ·7 min read·Jul 17, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F4715ed0deede&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpractical-introduction-to-transformer-models-bert-4715ed0deede&user=Shashank+Kapadia&userId=cc7314ace45c&source=-----4715ed0deede---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4715ed0deede&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpractical-introduction-to-transformer-models-bert-4715ed0deede&source=-----4715ed0deede---------------------bookmark_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: '*Preface: This article presents a summary of information about the given topic.
    It should not be considered original research. The information and code included
    in this article have may be influenced by things I have read or seen in the past
    from various online articles, research papers, books, and open-source code.*'
  prefs: []
  type: TYPE_NORMAL
- en: Table of Content
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Introduction to BERT
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pre-Training and Fine-Tuning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hands On: Using BERT for sentiment analysis'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Interpreting Results
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Closing Thoughts
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In NLP, the transformer model architecture has been a revolutionary that greatly
    enhanced the ability to understand and generate textual information.
  prefs: []
  type: TYPE_NORMAL
- en: In this tutorial, we are going to dig-deep into BERT, a well-known transformer-based
    model, and provide an hands-on example to fine-tune the base BERT model for sentiment
    analysis.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to BERT
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: BERT, introduced by researchers at Google in 2018, is a powerful language model
    that uses transformer architecture. Pushing the boundaries of earlier model architecture,
    such as LSTM and GRU, that were either unidirectional or sequentially bi-directional,
    BERT considers context from both past and future simultaneously. This is due to
    the innovative “attention mechanism,” which allows the model to weigh the importance
    of words in a sentence when generating representations.
  prefs: []
  type: TYPE_NORMAL
- en: 'The BERT model is pre-trained on the following two NLP tasks:'
  prefs: []
  type: TYPE_NORMAL
- en: Masked Language Model (MLM)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Next Sentence Prediction (NSP)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: and is generally used as the base model for various downstream NLP tasks, such
    as sentiment analysis which we will cover in this tutorial.
  prefs: []
  type: TYPE_NORMAL
- en: Pre-Training and Fine-Tuning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The power of BERT comes from its two-step process:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Pre-training** is the phase where BERT is trained on large amounts of data.
    As a result, it learns to predict masked words in a sentence (MLM task) and to
    predict if a sentence follows another one (NSP task). The output of this stage
    is a a pre-trained NLP model with a general-purpose “understanding” of the language'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Fine-tuning** is where the pre-trained BERT model is further trained on a
    specific task. The model is initialized with the pre-trained parameters, and the
    entire model is trained on a downstream task, allowing BERT to fine-tune its understanding
    of language to the specifics of the task at hand.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hands On: Using BERT for sentiment analysis'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The complete code is available as a [Jupyter Notebook on GitHub](https://github.com/kapadias/medium-articles/blob/master/natural-language-processing/transformers-series/sentiment_analysis_bert.ipynb)
  prefs: []
  type: TYPE_NORMAL
- en: 'In this hands-on exercise, we will train the sentiment analysis model on the
    IMDB movie reviews dataset [4] [(license: Apache 2.0)](https://github.com/huggingface/datasets/blob/main/LICENSE),
    which comes labeled whether a review is positive or negative. We will also load
    the model using the Hugging Face’s transformers library.'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s load all the libraries
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: First, we need to load the dataset and the model tokenizer.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Next, we’ll create a plot to see the distribution of the positive and negative
    classes.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/a751c4f02714c162279b8e2dbac1071c.png)'
  prefs: []
  type: TYPE_IMG
- en: Fig 1\. Class distribution of the training dataset
  prefs: []
  type: TYPE_NORMAL
- en: Next, we preprocess our dataset by tokenizing the texts. We use BERT’s tokenizer,
    which will convert the text into tokens that correspond to BERT’s vocabulary.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/ae0915081c37ac5286fdd5ded5e033d4.png)'
  prefs: []
  type: TYPE_IMG
- en: After that, we prepare our training and evaluation datasets. Remember, if you
    want to use all the data, you can set the `num_samples` variable to `-1`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Then, we load the pre-trained BERT model. We’ll use the `AutoModelForSequenceClassification`
    class, a BERT model designed for classification tasks.
  prefs: []
  type: TYPE_NORMAL
- en: For this tutorial, we use the ‘[bert-base-uncased](https://huggingface.co/bert-base-uncased)’
    version of BERT, which is trained on lower-case English text, is used for this
    tutorial.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Now, we’re ready to define our training arguments and create a `Trainer` instance
    to train our model.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Interpreting Results
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Having trained our model, let’s evaluate it. We’ll calculate the confusion matrix
    and the ROC curve to understand how well our model performs.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/cb104a044e899ad272b1ce2d8f888394.png)'
  prefs: []
  type: TYPE_IMG
- en: Fig 2\. Confusion Matrix
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/aa9c8ed466a7fc3bef2b259d818a359d.png)'
  prefs: []
  type: TYPE_IMG
- en: Fig 3\. ROC curve
  prefs: []
  type: TYPE_NORMAL
- en: The confusion matrix gives a detailed breakdown of how our predictions measure
    up to the actual labels, while the ROC curve shows us the trade-off between the
    true positive rate (sensitivity) and the false positive rate (1 — specificity)
    at various threshold settings.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, to see our model in action, let’s use it to infer the sentiment of
    a sample text.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/84a2556951138b36dc6bda8b93d1a4a9.png)'
  prefs: []
  type: TYPE_IMG
- en: Closing Thoughts
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: By walking through an example of sentiment analysis on IMDb movie reviews, I
    hope you’ve gained a clear understanding of how to apply BERT to real-world NLP
    problems. The Python code I’ve included here can be adjusted and extended to tackle
    different tasks and datasets, paving the way for even more sophisticated and accurate
    language models.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[1] Devlin, J., Chang, M. W., Lee, K., & Toutanova, K. (2018). BERT: Pre-training
    of Deep Bidirectional Transformers for Language Understanding. arXiv preprint
    arXiv:1810.04805'
  prefs: []
  type: TYPE_NORMAL
- en: '[2] Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez,
    A. N., … & Polosukhin, I. (2017). Attention is all you need. In Advances in neural
    information processing systems (pp. 5998–6008).'
  prefs: []
  type: TYPE_NORMAL
- en: '[3] Wolf, T., Debut, L., Sanh, V., Chaumond, J., Delangue, C., Moi, A., … &
    Rush, A. M. (2019). Huggingface’s transformers: State-of-the-art natural language
    processing. ArXiv, abs/1910.03771.'
  prefs: []
  type: TYPE_NORMAL
- en: '[4] Lhoest, Q., Villanova del Moral, A., Jernite, Y., Thakur, A., von Platen,
    P., Patil, S., Chaumond, J., Drame, M., Plu, J., Tunstall, L., Davison, J., Šaško,
    M., Chhablani, G., Malik, B., Brandeis, S., Le Scao, T., Sanh, V., Xu, C., Patry,
    N., McMillan-Major, A., Schmid, P., Gugger, S., Delangue, C., Matussière, T.,
    Debut, L., Bekman, S., Cistac, P., Goehringer, T., Mustar, V., Lagunas, F., Rush,
    A., & Wolf, T. (2021). Datasets: A Community Library for Natural Language Processing.
    In Proceedings of the 2021 Conference on Empirical Methods in Natural Language
    Processing: System Demonstrations (pp. 175–184). Online and Punta Cana, Dominican
    Republic: Association for Computational Linguistics. Retrieved from [https://aclanthology.org/2021.emnlp-demo.21](https://aclanthology.org/2021.emnlp-demo.21)'
  prefs: []
  type: TYPE_NORMAL
- en: Thanks for reading. *If you have any feedback, please feel to reach out by commenting
    on this post, messaging me on* [*LinkedIn*](https://www.linkedin.com/in/shashankkapadia/)*,
    or shooting me an email (smhkapadia[at]gmail.com)*
  prefs: []
  type: TYPE_NORMAL
- en: '*If you enjoyed this article, visit my other articles*'
  prefs: []
  type: TYPE_NORMAL
- en: '[](/domain-adaption-fine-tune-pre-trained-nlp-models-a06659ca6668?source=post_page-----4715ed0deede--------------------------------)
    [## Domain Adaption: Fine-Tune Pre-Trained NLP Models'
  prefs: []
  type: TYPE_NORMAL
- en: A step-by-step guide to fine-tuning pre-trained NLP models for any domain
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/domain-adaption-fine-tune-pre-trained-nlp-models-a06659ca6668?source=post_page-----4715ed0deede--------------------------------)
    [](https://medium.com/aimonks/the-evolution-of-natural-language-processing-56ce27916e10?source=post_page-----4715ed0deede--------------------------------)
    [## The Evolution of Natural Language Processing
  prefs: []
  type: TYPE_NORMAL
- en: A Historical Perspective on the Development of Language Models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'medium.com](https://medium.com/aimonks/the-evolution-of-natural-language-processing-56ce27916e10?source=post_page-----4715ed0deede--------------------------------)
    [](/recommendation-system-in-python-lightfm-61c85010ce17?source=post_page-----4715ed0deede--------------------------------)
    [## Recommendation System in Python: LightFM'
  prefs: []
  type: TYPE_NORMAL
- en: A Step-by-Step guide to building a recommender system in Python using LightFM
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'towardsdatascience.com](/recommendation-system-in-python-lightfm-61c85010ce17?source=post_page-----4715ed0deede--------------------------------)
    [](/evaluate-topic-model-in-python-latent-dirichlet-allocation-lda-7d57484bb5d0?source=post_page-----4715ed0deede--------------------------------)
    [## Evaluate Topic Models: Latent Dirichlet Allocation (LDA)'
  prefs: []
  type: TYPE_NORMAL
- en: A step-by-step guide to building interpretable topic models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/evaluate-topic-model-in-python-latent-dirichlet-allocation-lda-7d57484bb5d0?source=post_page-----4715ed0deede--------------------------------)
  prefs: []
  type: TYPE_NORMAL
