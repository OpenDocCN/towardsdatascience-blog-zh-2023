- en: 'Seamless Data Analytics Workflow: From Dockerized JupyterLab and MinIO to Insights
    with Spark SQL'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/seamless-data-analytics-workflow-from-dockerized-jupyterlab-and-minio-to-insights-with-spark-sql-3c5556a18ce6?source=collection_archive---------5-----------------------#2023-12-21](https://towardsdatascience.com/seamless-data-analytics-workflow-from-dockerized-jupyterlab-and-minio-to-insights-with-spark-sql-3c5556a18ce6?source=collection_archive---------5-----------------------#2023-12-21)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[](https://medium.com/@sarbahi.sarthak?source=post_page-----3c5556a18ce6--------------------------------)[![Sarthak
    Sarbahi](../Images/b2ee093e0bcb95d515f10eac906f9890.png)](https://medium.com/@sarbahi.sarthak?source=post_page-----3c5556a18ce6--------------------------------)[](https://towardsdatascience.com/?source=post_page-----3c5556a18ce6--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----3c5556a18ce6--------------------------------)
    [Sarthak Sarbahi](https://medium.com/@sarbahi.sarthak?source=post_page-----3c5556a18ce6--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F35908b3630e1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fseamless-data-analytics-workflow-from-dockerized-jupyterlab-and-minio-to-insights-with-spark-sql-3c5556a18ce6&user=Sarthak+Sarbahi&userId=35908b3630e1&source=post_page-35908b3630e1----3c5556a18ce6---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----3c5556a18ce6--------------------------------)
    ·17 min read·Dec 21, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F3c5556a18ce6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fseamless-data-analytics-workflow-from-dockerized-jupyterlab-and-minio-to-insights-with-spark-sql-3c5556a18ce6&user=Sarthak+Sarbahi&userId=35908b3630e1&source=-----3c5556a18ce6---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F3c5556a18ce6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fseamless-data-analytics-workflow-from-dockerized-jupyterlab-and-minio-to-insights-with-spark-sql-3c5556a18ce6&source=-----3c5556a18ce6---------------------bookmark_footer-----------)![](../Images/ac76545d848b73236b080b88369d5d1b.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Photo by [Ian Taylor](https://unsplash.com/@carrier_lost?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: This tutorial guides you through an analytics use case, analyzing semi-structured
    data with Spark SQL. We’ll start with the data engineering process, pulling data
    from an API and finally loading the transformed data into a data lake (represented
    by [MinIO](https://min.io/)). Plus, we'll utilise Docker to introduce a best practice
    for setting up the environment. So, let’s dive in and see how it’s all done!
  prefs: []
  type: TYPE_NORMAL
- en: Table of contents
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[Understanding the building blocks](#320a)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Setting up Docker Desktop](#fa16)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Configuring MinIO](#22ad)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Getting started with JupyterLab](#4848)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Data pipeline: The ETL process](#00d0)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Analysing semi-structured data](#a204)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Cleanup of resources](#dae2)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Understanding the building blocks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This tutorial involves a range of technologies. Before diving into the practical
    part, let’s grasp each one. We’ll use analogies to make understanding each component
    easier.
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://github.com/sarthak-sarbahi/data-analytics-minio-spark/tree/main?source=post_page-----3c5556a18ce6--------------------------------)
    [## GitHub - sarthak-sarbahi/data-analytics-minio-spark'
  prefs: []
  type: TYPE_NORMAL
- en: Contribute to sarthak-sarbahi/data-analytics-minio-spark development by creating
    an account on GitHub.
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: github.com](https://github.com/sarthak-sarbahi/data-analytics-minio-spark/tree/main?source=post_page-----3c5556a18ce6--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: Imagine you’re a captain setting sail across a vast ocean. In the world of data,
    this ocean is the endless stream of information flowing from various sources.
    Our ship? It’s the suite of tools and technologies we use to navigate these waters.
  prefs: []
  type: TYPE_NORMAL
- en: '**JupyterLab and MinIO with Docker Compose**: Just as a ship needs the right
    parts to set sail, our data journey begins with assembling our tools. Think of
    Docker Compose as our toolbox, letting us efficiently put together JupyterLab
    (our navigation chart) and MinIO (our storage deck). It’s like building a custom
    vessel that’s perfectly suited for the voyage ahead.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Fetching data with Python**: Now, it’s time to chart our course. Using Python
    is like casting a wide net into the sea to gather fish (our data). We carefully
    select our catch, pulling data through the API and storing it in JSON format —
    a way of organizing our fish so that it’s easy to access and use later.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Reading and transforming data with PySpark**: With our catch on board, we
    use PySpark, our compass, to navigate through this sea of data. PySpark helps
    us clean, organize, and make sense of our catch, transforming raw data into valuable
    insights, much like how a skilled chef would prepare a variety of dishes from
    the day’s catch.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Analytics with Spark SQL**: Finally, we dive deeper, exploring the depths
    of the ocean with Spark SQL. It’s like using a sophisticated sonar to find hidden
    treasures beneath the waves. We perform analytics to uncover insights and answers
    to questions, revealing the valuable pearls hidden within our sea of data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now that we know what lies ahead in our journey, let’s begin setting things
    up.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up Docker Desktop
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Docker is a tool that makes it easier to create, deploy, and run applications.
    Docker containers bundle up an application with everything it needs (like libraries
    and other dependencies) and ship it as one package. This means that the application
    will run the same way, no matter where the Docker container is deployed — whether
    it’s on your laptop, a colleague’s machine, or a cloud server. This solves a big
    problem: the issue of software running differently on different machines due to
    varying configurations.'
  prefs: []
  type: TYPE_NORMAL
- en: In this guide, we’re going to work with several Docker containers simultaneously.
    It’s a typical scenario in real-world applications, like a web app communicating
    with a database. **Docker Compose** facilitates this. It allows us to start multiple
    containers, with each container handling a part of the application. Docker Compose
    ensures these components can interact with each other, enabling the application
    to function as an integrated unit.
  prefs: []
  type: TYPE_NORMAL
- en: To set up Docker, we use the **Docker Desktop** application. Docker Desktop
    is free for personal and educational use. You can download it from [here](https://www.docker.com/products/docker-desktop/).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e9afbb67e0ad31f212fdd12a5accbb47.png)'
  prefs: []
  type: TYPE_IMG
- en: Docker Desktop Application (Image by author)
  prefs: []
  type: TYPE_NORMAL
- en: After installing Docker Desktop, we’ll begin with the tutorial. We’ll start
    a new project in an Integrated Development Environment (IDE). You can choose any
    IDE you prefer. I’m using *Visual Studio Code*.
  prefs: []
  type: TYPE_NORMAL
- en: For this guide, I’m using a Windows machine with **WSL 2** (Windows Subsystem
    for Linux) installed. This setup lets me run a Linux environment, specifically
    Ubuntu, on my Windows PC. If you’re using Windows too and want to enable Docker
    Desktop for WSL 2, there’s a helpful [video](https://www.youtube.com/watch?v=2ezNqqaSjq8)
    you can watch.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we’ll create a `docker-compose.yml` file in the root directory of our
    project.
  prefs: []
  type: TYPE_NORMAL
- en: Docker Compose file (Image by author)
  prefs: []
  type: TYPE_NORMAL
- en: If this is your first time encountering a file like this, don’t fret. I’ll go
    into more detail about it in the following sections. For now, just run the command
    `docker-compose up -d` in the directory where this file is located. This command
    will initially fetch the Docker images for JupyterLab and MinIO from the **Docker
    Hub**.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/67ab984418c5b6ecc0800f9d4912fb9a.png)'
  prefs: []
  type: TYPE_IMG
- en: Results of running the command (Image by author)
  prefs: []
  type: TYPE_NORMAL
- en: A **Docker image** is like a blueprint or a recipe for creating a Docker container.
    Think of it as a pre-packaged box that contains everything you need to run a specific
    software or application. This box (or image) includes the code, runtime, system
    tools, libraries, and settings — basically all the necessary parts that are required
    to run the application.
  prefs: []
  type: TYPE_NORMAL
- en: Containers are simply running instances of Docker images.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[Docker Hub](https://hub.docker.com/) is like an online library or store where
    people can find and share Docker images.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/55fbf2aaa77be9573711431faca31600.png)'
  prefs: []
  type: TYPE_IMG
- en: Required images have been downloaded (Image by author)
  prefs: []
  type: TYPE_NORMAL
- en: After the images are downloaded, it will launch a container for each image.
    This process will initiate two containers — one for JupyterLab and another for
    MinIO.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/86e5a4c4c83629fbb97519320f8bb708.png)'
  prefs: []
  type: TYPE_IMG
- en: Two containers are running (Image by author)
  prefs: []
  type: TYPE_NORMAL
- en: With the required processes now operational, let’s dive deeper into MinIO and
    how it’s configured.
  prefs: []
  type: TYPE_NORMAL
- en: Configuring MinIO
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: MinIO is an open-source object storage solution, specifically designed to handle
    large volumes and varieties of data. It’s highly compatible with Amazon S3 APIs,
    which makes it a versatile choice for cloud-native applications.
  prefs: []
  type: TYPE_NORMAL
- en: MinIO is much like using a ‘free’ version of Amazon S3 on your PC.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: We’ll utilize MinIO for storing both raw and processed data, mimicking a real-world
    scenario. Thanks to Docker, we already have MinIO up and running. Next, we need
    to learn how to use it. But first, let’s revisit the `docker-compose.yml` file.
  prefs: []
  type: TYPE_NORMAL
- en: The `services` section in the file outlines the containers we’ll run and the
    software instances they will initiate. Our focus here is on the MinIO service.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1979feec7c9e7d6e01282ea23201a5a5.png)'
  prefs: []
  type: TYPE_IMG
- en: MinIO service in **docker-compose.yml** file (Image by author)
  prefs: []
  type: TYPE_NORMAL
- en: Let’s break this down.
  prefs: []
  type: TYPE_NORMAL
- en: '`image: minio/minio` tells Docker to use the MinIO image from Docker Hub (the
    online library of Docker images).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`container_name: minio1` gives a name to this container, in this case, `minio1`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ports: - "9000:9000" - "9001:9001"` maps the ports from the container to your
    host machine. This allows you to access the MinIO service using these ports on
    your local machine.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`volumes: - /mnt/data:/data` sets up a volume, which is like a storage space,
    mapping a directory on your host machine (`/mnt/data`) to a directory in the container
    (`/data`). This means MinIO will use the `/mnt/data` directory on your machine
    to store the data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`environment:` section sets environment variables inside the container. Here,
    it''s setting the MinIO root user''s username and password.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`command: server /data --console-address ":9001"` is the command that will
    be run inside the MinIO container. It starts the MinIO server and tells it to
    use the `/data` directory.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With MinIO’s setup clear, let’s begin using it. You can access the MinIO web
    interface at `[http://localhost:9001](http://localhost:9001/)`. On your initial
    visit, you’ll need to log in with the username (`minio`)and password (`minio123`)
    specified in the docker-compose file.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/16bab217bc1b4cdccc55dd0d2c6b25cb.png)'
  prefs: []
  type: TYPE_IMG
- en: MinIO portal (Image by author)
  prefs: []
  type: TYPE_NORMAL
- en: Once logged in, go ahead and create a bucket. Click on ‘Create a Bucket’ and
    name it `**mybucket**`. After naming it, click on ‘Create Bucket.’ The default
    settings are fine for now, but feel free to read about them on the page’s right
    side.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a09022fb27cbb231673a46fef71bcb79.png)'
  prefs: []
  type: TYPE_IMG
- en: Bucket created in MinIO (Image by author)
  prefs: []
  type: TYPE_NORMAL
- en: Well done! We’re now ready to use MinIO. Let’s move on to exploring how we can
    use JupyterLab.
  prefs: []
  type: TYPE_NORMAL
- en: Getting started with JupyterLab
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: JupyterLab is an interactive web-based interface that helps us write code, perform
    analysis on notebooks and work with data. In fact, the JupyterLab image already
    includes Python and PySpark, so there’s no hassle in setting them up.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4432377abc513c0fbf507b34f842c44a.png)'
  prefs: []
  type: TYPE_IMG
- en: JupyterLab service in **docker-compose.yml** file (Image by author)
  prefs: []
  type: TYPE_NORMAL
- en: First, let’s revisit the `docker-compose.yml` file to understand the `jupyter`
    service.
  prefs: []
  type: TYPE_NORMAL
- en: '`image: jupyter/pyspark-notebook` specifies to use the JupyterLab image that
    comes with PySpark pre-installed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ports: - "8888:8888"` maps the JupyterLab port to the same port on your host
    machine, allowing you to access it through your browser.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To access its web interface, navigate to the ‘Containers’ tab in the Docker
    Desktop application. Find and click on the JupyterLab container, labeled `jupyter-1`.
    This action will display the container logs.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/02a91268fd1e6df5b57dfdcf305d9605.png)'
  prefs: []
  type: TYPE_IMG
- en: JupyterLab container logs (Image by author)
  prefs: []
  type: TYPE_NORMAL
- en: 'Within these logs, you’ll find a URL resembling this: `[http://127.0.0.1:8888/lab?token=4f1c9d4f1aeb460f1ebbf224dfa9417c88eab1691fa64b04](http://127.0.0.1:8888/lab?token=4f1c9d4f1aeb460f1ebbf224dfa9417c88eab1691fa64b04)`.
    Clicking on this URL launches the web interface.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e1accc3b8272213751be56fedcdce35a.png)'
  prefs: []
  type: TYPE_IMG
- en: JupyterLab web interface (Image by author)
  prefs: []
  type: TYPE_NORMAL
- en: Once there, select the ‘Python 3 (ipykernel)’ icon under the ‘Notebook’ section.
    This action opens a new notebook, where we’ll write code for data retrieval, transformation,
    and analysis. Before diving into coding, remember to save and name your notebook
    appropriately. And there you have it, we’re ready to start working with the data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Data pipeline: The ETL process'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Before diving into data analysis, we first need to gather the data. We’ll employ
    an **ETL** (Extract, Transform, Load) process, which involves the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Initially, we’ll extract data using a public API.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Then, we’ll load this data as a JSON file into the MinIO bucket.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: After that, we’ll use PySpark to transform the data and save it back to the
    bucket in Parquet format.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lastly, we’ll create a Hive table from this Parquet data, which we’ll use for
    running Spark SQL queries for analysis.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: First up, we need to install the `s3fs` Python package, essential for working
    with MinIO in Python.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Following that, we’ll import the necessary dependencies and modules.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: We’ll also set some environment variables that will be useful when interacting
    with MinIO.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Next, we’ll fetch data from the public API using the `requests` Python package.
    We’re using the open-source **Rest Countries Project**. It gives information about
    the different countries of the world — area, population, capital city, time zones,
    etc. Click [here](https://restcountries.com/#about-this-project) to learn more
    about it.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Once we have the data, we’ll write it as a JSON file to the `mybucket` bucket.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Great, we’ve successfully retrieved the data! Now, it’s time to initialize a
    **Spark session** for running PySpark code. If you’re new to Spark, understand
    that it’s a big data processing framework that operates on distributed computing
    principles, breaking data into chunks for parallel processing. A Spark session
    is essentially the gateway to any Spark application.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Let’s simplify this to understand it better.
  prefs: []
  type: TYPE_NORMAL
- en: '`spark.jars.packages`: Downloads the required JAR files from the [Maven repository](https://mvnrepository.com/).
    A Maven repository is a central place used for storing build artifacts like JAR
    files, libraries, and other dependencies that are used in Maven-based projects'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`spark.hadoop.fs.s3a.endpoint`: This is the endpoint URL for MinIO.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`spark.hadoop.fs.s3a.access.key` and `spark.hadoop.fs.s3a.secret.key`: This
    is the access key and secret key for MinIO. Note that it is the same as the username
    and password used to access the MinIO web interface.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`spark.hadoop.fs.s3a.path.style.access`: It is set to true to enable path-style
    access for the MinIO bucket.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`spark.hadoop.fs.s3a.impl`: This is the implementation class for S3A file system.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You might wonder how to choose the correct JAR version. It depends on compatibility
    with the PySpark and Hadoop versions we use. Here’s how to check your PySpark
    and **Hadoop** versions (Hadoop is another open-source framework for working with
    big data).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Choosing the right JAR version is crucial to avoid errors. Using the same Docker
    image, the JAR version mentioned here should work fine. If you encounter setup
    issues, feel free to leave a comment. I’ll do my best to assist you :)
  prefs: []
  type: TYPE_NORMAL
- en: Let’s start by reading the JSON data into a Spark dataframe using PySpark.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Keep in mind, our dataframe has only 250 rows. In data engineering, this is
    a very small amount. Working with millions or billions of rows is typical. However,
    for ease, we’re using a smaller dataset here.
  prefs: []
  type: TYPE_NORMAL
- en: We’ll then save this data as a **Parquet** file in the MinIO bucket. Parquet,
    a popular file format in big data, stores data column-wise to enhance query speed
    and reduce file size. It also partitions data for better query performance.
  prefs: []
  type: TYPE_NORMAL
- en: After that, we’ll read this data into a new dataframe.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: It’s good practice to store raw data separately. This way, we retain the original
    data, even after transforming and saving it differently. Before we dive into transformations,
    let’s check the MinIO bucket.
  prefs: []
  type: TYPE_NORMAL
- en: In the MinIO web interface, select ‘Object Browser’ from the left menu, then
    open the bucket.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6ecee02d2ca6d13aee6712007b6de074.png)'
  prefs: []
  type: TYPE_IMG
- en: Browsing contents of the bucket in MinIO (Image by author)
  prefs: []
  type: TYPE_NORMAL
- en: Here, you’ll find the earlier JSON file and the raw data in Parquet format.
  prefs: []
  type: TYPE_NORMAL
- en: Spark splits the Parquet data into multiple files in a folder, effectively chunking
    the data.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Now, let’s move on. We’ll transform the data as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Breaking down the code:'
  prefs: []
  type: TYPE_NORMAL
- en: We select specific columns from the raw data for analysis. To access nested
    fields in this semi-structured data, we use the dot “.” operator.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `cntry_area` column is modified so that any negative value becomes NULL.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For `border_cntry` and `capital_cities`, which are **ArrayType**, we replace
    NULL values with an array of `NA`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: After transformations, we’ll print the schema of the new dataframe.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Next, we’ll write the transformed data back as a new Parquet file.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Following that, we create an external Hive table over this Parquet file. Hive,
    a data warehouse software built on Hadoop, facilitates data querying and analysis.
    An **external table** means Hive only handles the metadata, with the actual data
    in an external location (our MinIO bucket).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Once our Hive table is ready, we can view its details with a specific command.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: You’ll see the column names, data types, database name, location, table type,
    and more.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s query the first five records from the table.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Next, we’ll read the table into a dataframe and create a temporary view on it.
    While it’s possible to query directly from the table, using temporary views on
    dataframes is common in Spark. Let’s explore that too.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Fantastic. With this setup, we’re ready to start our analysis!
  prefs: []
  type: TYPE_NORMAL
- en: Analysing semi-structured data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now comes the exciting part. Let’s dive into our data analysis by addressing
    some intriguing questions derived from our data. Here are the queries we aim to
    explore:'
  prefs: []
  type: TYPE_NORMAL
- en: Which are the 10 largest countries in terms of area? (in sq. km.)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Which country has the largest number of neighbouring countries?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Which countries have the highest number of capital cities?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How many countries lie on two or more continents?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How many landlocked countries per continent?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Which country has the highest number of time zones?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How many countries are not UN members?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Knowing what we’re looking for, let’s start crafting some Spark SQL queries
    to uncover these insights. First, we’ll set up a utility function for displaying
    results in the notebook, saving us from repetitive coding.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Let’s kick off with our initial query.
  prefs: []
  type: TYPE_NORMAL
- en: '***Q1\. Which are the 10 largest countries in terms of area? (in sq. km.)***'
  prefs: []
  type: TYPE_NORMAL
- en: For this, we’ll arrange the data by the `cntry_area` column in descending order.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: We find that Russia has the largest area, followed by Antarctica. While there’s
    debate about Antarctica’s status as a country, it’s included in this dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '***Q2\. Which country has the largest number of neighbouring countries?***'
  prefs: []
  type: TYPE_NORMAL
- en: Next, we focus on the `border_cntry` column. It’s an array type listing the
    neighbouring countries’ codes for each country. By using the `array_size` function,
    we calculate the array lengths and order the data accordingly, excluding rows
    where `border_cntry` is `NA`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: This reveals China as the country with the most neighbours — a total of 16.
  prefs: []
  type: TYPE_NORMAL
- en: '***Q3\. Which countries have the highest number of capital cities?***'
  prefs: []
  type: TYPE_NORMAL
- en: We’ll employ a similar method for the next question, applying `array_size` to
    the `capital_cities` column.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: The results highlight South Africa and Palestine as the only countries with
    multiple capital cities.
  prefs: []
  type: TYPE_NORMAL
- en: '***Q4\. How many countries lie on two or more continents?***'
  prefs: []
  type: TYPE_NORMAL
- en: We’ll again use the `array_size` function, this time on the `cntry_continent`
    column.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: It turns out Turkey, Azerbaijan, and Russia span two continents each — Europe
    and Asia.
  prefs: []
  type: TYPE_NORMAL
- en: '***Q5\. How many landlocked countries per continent?***'
  prefs: []
  type: TYPE_NORMAL
- en: We’ll build a subquery with the country name, a boolean value indicating landlocked
    status, and an explode function for each `cntry_continent` array entry. Since
    a country may span multiple continents, we’ll aggregate by continent and sum the
    boolean values for landlocked countries.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: This indicates that Europe and Africa have the highest number of landlocked
    countries, considering a country may be counted more than once if it spans multiple
    continents.
  prefs: []
  type: TYPE_NORMAL
- en: '***Q6\. Which country has the highest number of time zones?***'
  prefs: []
  type: TYPE_NORMAL
- en: Here, we use `array_size` on the `nr_timezones` column and sort the results
    in descending order.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Interestingly, France tops the list, likely due to its territories beyond mainland
    France.
  prefs: []
  type: TYPE_NORMAL
- en: '***Q7\. How many countries are not UN members?***'
  prefs: []
  type: TYPE_NORMAL
- en: Here, we count the number of countries where `is_unmember` is False.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: According to the dataset, 57 countries are not UN members, a figure that includes
    independent territories and regions classified as countries. You can find the
    complete notebook [here](https://github.com/sarthak-sarbahi/data-analytics-minio-spark/blob/main/country_analysis.ipynb).
  prefs: []
  type: TYPE_NORMAL
- en: That wraps up our analysis! But before concluding, let’s discuss how to properly
    clean up our resources.
  prefs: []
  type: TYPE_NORMAL
- en: Cleanup of resources
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: After finishing, don’t forget to save your notebook. Then, it’s time to stop
    the Docker containers. In the Docker Desktop app, just click the *stop* button.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d3a0d65b7a92d27eb8d3331ba76bd7c2.png)'
  prefs: []
  type: TYPE_IMG
- en: Stopping all the containers at once (Image by author)
  prefs: []
  type: TYPE_NORMAL
- en: This action will halt both containers simultaneously. Alternatively, you can
    stop each container one by one. You can choose to either retain these containers
    or delete them.
  prefs: []
  type: TYPE_NORMAL
- en: Be aware, deleting the JupyterLab container means losing your notebook, as a
    new JupyterLab instance starts from scratch. However, your MinIO data will stay
    intact, as it’s stored on your host machine, not in the container’s memory.
  prefs: []
  type: TYPE_NORMAL
- en: If you choose to remove the containers, you might also want to delete the Docker
    images for JupyterLab and MinIO, especially if you’re tight on storage. You can
    do this in the “Images” section of the Docker Desktop app.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this story, we explored a straightforward yet captivating data analytics
    case. We began by configuring our environment using Docker Compose. Next, we fetched
    data from an API, mimicking a real-world scenario. We then saved this data in
    a bucket similar to Amazon S3, in JSON format. Using PySpark, we enhanced this
    data and stored it persistently in Parquet format. We also learned how to create
    an external Hive table on top of this data. Finally, we used this table for our
    analysis, which involved working with complex data types like arrays.
  prefs: []
  type: TYPE_NORMAL
- en: I sincerely hope this guide was beneficial for you. Should you have any questions,
    please don’t hesitate to drop them in the comments below.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'GitHub repository: [https://github.com/sarthak-sarbahi/data-analytics-minio-spark/tree/main](https://github.com/sarthak-sarbahi/data-analytics-minio-spark/tree/main)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Docker Compose: [https://docs.docker.com/compose/](https://docs.docker.com/compose/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'MinIO: [https://min.io/docs/minio/linux/index.html](https://min.io/docs/minio/linux/index.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'PySpark: [https://spark.apache.org/docs/latest/api/python/index.html](https://spark.apache.org/docs/latest/api/python/index.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
