- en: 'Seamless Data Analytics Workflow: From Dockerized JupyterLab and MinIO to Insights
    with Spark SQL'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/seamless-data-analytics-workflow-from-dockerized-jupyterlab-and-minio-to-insights-with-spark-sql-3c5556a18ce6?source=collection_archive---------5-----------------------#2023-12-21](https://towardsdatascience.com/seamless-data-analytics-workflow-from-dockerized-jupyterlab-and-minio-to-insights-with-spark-sql-3c5556a18ce6?source=collection_archive---------5-----------------------#2023-12-21)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[](https://medium.com/@sarbahi.sarthak?source=post_page-----3c5556a18ce6--------------------------------)[![Sarthak
    Sarbahi](../Images/b2ee093e0bcb95d515f10eac906f9890.png)](https://medium.com/@sarbahi.sarthak?source=post_page-----3c5556a18ce6--------------------------------)[](https://towardsdatascience.com/?source=post_page-----3c5556a18ce6--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----3c5556a18ce6--------------------------------)
    [Sarthak Sarbahi](https://medium.com/@sarbahi.sarthak?source=post_page-----3c5556a18ce6--------------------------------)'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: ·
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F35908b3630e1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fseamless-data-analytics-workflow-from-dockerized-jupyterlab-and-minio-to-insights-with-spark-sql-3c5556a18ce6&user=Sarthak+Sarbahi&userId=35908b3630e1&source=post_page-35908b3630e1----3c5556a18ce6---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----3c5556a18ce6--------------------------------)
    ·17 min read·Dec 21, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F3c5556a18ce6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fseamless-data-analytics-workflow-from-dockerized-jupyterlab-and-minio-to-insights-with-spark-sql-3c5556a18ce6&user=Sarthak+Sarbahi&userId=35908b3630e1&source=-----3c5556a18ce6---------------------clap_footer-----------)'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F3c5556a18ce6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fseamless-data-analytics-workflow-from-dockerized-jupyterlab-and-minio-to-insights-with-spark-sql-3c5556a18ce6&source=-----3c5556a18ce6---------------------bookmark_footer-----------)![](../Images/ac76545d848b73236b080b88369d5d1b.png)'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: Photo by [Ian Taylor](https://unsplash.com/@carrier_lost?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: This tutorial guides you through an analytics use case, analyzing semi-structured
    data with Spark SQL. We’ll start with the data engineering process, pulling data
    from an API and finally loading the transformed data into a data lake (represented
    by [MinIO](https://min.io/)). Plus, we'll utilise Docker to introduce a best practice
    for setting up the environment. So, let’s dive in and see how it’s all done!
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: Table of contents
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[Understanding the building blocks](#320a)'
  id: totrans-10
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Setting up Docker Desktop](#fa16)'
  id: totrans-11
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Configuring MinIO](#22ad)'
  id: totrans-12
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Getting started with JupyterLab](#4848)'
  id: totrans-13
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Data pipeline: The ETL process](#00d0)'
  id: totrans-14
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Analysing semi-structured data](#a204)'
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Cleanup of resources](#dae2)'
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Understanding the building blocks
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This tutorial involves a range of technologies. Before diving into the practical
    part, let’s grasp each one. We’ll use analogies to make understanding each component
    easier.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://github.com/sarthak-sarbahi/data-analytics-minio-spark/tree/main?source=post_page-----3c5556a18ce6--------------------------------)
    [## GitHub - sarthak-sarbahi/data-analytics-minio-spark'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: Contribute to sarthak-sarbahi/data-analytics-minio-spark development by creating
    an account on GitHub.
  id: totrans-20
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: github.com](https://github.com/sarthak-sarbahi/data-analytics-minio-spark/tree/main?source=post_page-----3c5556a18ce6--------------------------------)
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: Imagine you’re a captain setting sail across a vast ocean. In the world of data,
    this ocean is the endless stream of information flowing from various sources.
    Our ship? It’s the suite of tools and technologies we use to navigate these waters.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: '**JupyterLab and MinIO with Docker Compose**: Just as a ship needs the right
    parts to set sail, our data journey begins with assembling our tools. Think of
    Docker Compose as our toolbox, letting us efficiently put together JupyterLab
    (our navigation chart) and MinIO (our storage deck). It’s like building a custom
    vessel that’s perfectly suited for the voyage ahead.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Fetching data with Python**: Now, it’s time to chart our course. Using Python
    is like casting a wide net into the sea to gather fish (our data). We carefully
    select our catch, pulling data through the API and storing it in JSON format —
    a way of organizing our fish so that it’s easy to access and use later.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Reading and transforming data with PySpark**: With our catch on board, we
    use PySpark, our compass, to navigate through this sea of data. PySpark helps
    us clean, organize, and make sense of our catch, transforming raw data into valuable
    insights, much like how a skilled chef would prepare a variety of dishes from
    the day’s catch.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Analytics with Spark SQL**: Finally, we dive deeper, exploring the depths
    of the ocean with Spark SQL. It’s like using a sophisticated sonar to find hidden
    treasures beneath the waves. We perform analytics to uncover insights and answers
    to questions, revealing the valuable pearls hidden within our sea of data.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now that we know what lies ahead in our journey, let’s begin setting things
    up.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: Setting up Docker Desktop
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Docker is a tool that makes it easier to create, deploy, and run applications.
    Docker containers bundle up an application with everything it needs (like libraries
    and other dependencies) and ship it as one package. This means that the application
    will run the same way, no matter where the Docker container is deployed — whether
    it’s on your laptop, a colleague’s machine, or a cloud server. This solves a big
    problem: the issue of software running differently on different machines due to
    varying configurations.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: In this guide, we’re going to work with several Docker containers simultaneously.
    It’s a typical scenario in real-world applications, like a web app communicating
    with a database. **Docker Compose** facilitates this. It allows us to start multiple
    containers, with each container handling a part of the application. Docker Compose
    ensures these components can interact with each other, enabling the application
    to function as an integrated unit.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: To set up Docker, we use the **Docker Desktop** application. Docker Desktop
    is free for personal and educational use. You can download it from [here](https://www.docker.com/products/docker-desktop/).
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e9afbb67e0ad31f212fdd12a5accbb47.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
- en: Docker Desktop Application (Image by author)
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: After installing Docker Desktop, we’ll begin with the tutorial. We’ll start
    a new project in an Integrated Development Environment (IDE). You can choose any
    IDE you prefer. I’m using *Visual Studio Code*.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: For this guide, I’m using a Windows machine with **WSL 2** (Windows Subsystem
    for Linux) installed. This setup lets me run a Linux environment, specifically
    Ubuntu, on my Windows PC. If you’re using Windows too and want to enable Docker
    Desktop for WSL 2, there’s a helpful [video](https://www.youtube.com/watch?v=2ezNqqaSjq8)
    you can watch.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: Next, we’ll create a `docker-compose.yml` file in the root directory of our
    project.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: Docker Compose file (Image by author)
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: If this is your first time encountering a file like this, don’t fret. I’ll go
    into more detail about it in the following sections. For now, just run the command
    `docker-compose up -d` in the directory where this file is located. This command
    will initially fetch the Docker images for JupyterLab and MinIO from the **Docker
    Hub**.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/67ab984418c5b6ecc0800f9d4912fb9a.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
- en: Results of running the command (Image by author)
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: A **Docker image** is like a blueprint or a recipe for creating a Docker container.
    Think of it as a pre-packaged box that contains everything you need to run a specific
    software or application. This box (or image) includes the code, runtime, system
    tools, libraries, and settings — basically all the necessary parts that are required
    to run the application.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: Containers are simply running instances of Docker images.
  id: totrans-42
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[Docker Hub](https://hub.docker.com/) is like an online library or store where
    people can find and share Docker images.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/55fbf2aaa77be9573711431faca31600.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
- en: Required images have been downloaded (Image by author)
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 所需的镜像已下载（作者提供的图片）
- en: After the images are downloaded, it will launch a container for each image.
    This process will initiate two containers — one for JupyterLab and another for
    MinIO.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 镜像下载完成后，它将为每个镜像启动一个容器。这个过程将启动两个容器——一个用于 JupyterLab，另一个用于 MinIO。
- en: '![](../Images/86e5a4c4c83629fbb97519320f8bb708.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/86e5a4c4c83629fbb97519320f8bb708.png)'
- en: Two containers are running (Image by author)
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 两个容器正在运行（作者提供的图片）
- en: With the required processes now operational, let’s dive deeper into MinIO and
    how it’s configured.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 既然所需的进程已启动，让我们深入了解 MinIO 及其配置。
- en: Configuring MinIO
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 配置 MinIO
- en: MinIO is an open-source object storage solution, specifically designed to handle
    large volumes and varieties of data. It’s highly compatible with Amazon S3 APIs,
    which makes it a versatile choice for cloud-native applications.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: MinIO 是一种开源对象存储解决方案，专门设计用于处理大量和多种类的数据。它与 Amazon S3 API 高度兼容，这使其成为云原生应用程序的多功能选择。
- en: MinIO is much like using a ‘free’ version of Amazon S3 on your PC.
  id: totrans-52
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: MinIO 就像是在你的电脑上使用‘免费的’ Amazon S3 版本。
- en: We’ll utilize MinIO for storing both raw and processed data, mimicking a real-world
    scenario. Thanks to Docker, we already have MinIO up and running. Next, we need
    to learn how to use it. But first, let’s revisit the `docker-compose.yml` file.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将利用 MinIO 存储原始数据和处理数据，模拟真实世界的场景。由于 Docker，我们已经让 MinIO 启动并运行。接下来，我们需要学习如何使用它。首先，让我们回顾一下
    `docker-compose.yml` 文件。
- en: The `services` section in the file outlines the containers we’ll run and the
    software instances they will initiate. Our focus here is on the MinIO service.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 文件中的 `services` 部分概述了我们将运行的容器以及它们将启动的软件实例。我们这里重点关注 MinIO 服务。
- en: '![](../Images/1979feec7c9e7d6e01282ea23201a5a5.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1979feec7c9e7d6e01282ea23201a5a5.png)'
- en: MinIO service in **docker-compose.yml** file (Image by author)
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '**docker-compose.yml** 文件中的 MinIO 服务（作者提供的图片）'
- en: Let’s break this down.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来详细了解一下。
- en: '`image: minio/minio` tells Docker to use the MinIO image from Docker Hub (the
    online library of Docker images).'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`image: minio/minio` 告诉 Docker 使用来自 Docker Hub（Docker 镜像的在线库）的 MinIO 镜像。'
- en: '`container_name: minio1` gives a name to this container, in this case, `minio1`.'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`container_name: minio1` 给这个容器命名，在这种情况下，命名为 `minio1`。'
- en: '`ports: - "9000:9000" - "9001:9001"` maps the ports from the container to your
    host machine. This allows you to access the MinIO service using these ports on
    your local machine.'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ports: - "9000:9000" - "9001:9001"` 将容器的端口映射到你的主机上。这允许你通过这些端口在本地机器上访问 MinIO
    服务。'
- en: '`volumes: - /mnt/data:/data` sets up a volume, which is like a storage space,
    mapping a directory on your host machine (`/mnt/data`) to a directory in the container
    (`/data`). This means MinIO will use the `/mnt/data` directory on your machine
    to store the data.'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`volumes: - /mnt/data:/data` 设置一个卷，就像一个存储空间，将你主机上的一个目录 (`/mnt/data`) 映射到容器中的一个目录
    (`/data`)。这意味着 MinIO 将使用你机器上的 `/mnt/data` 目录来存储数据。'
- en: '`environment:` section sets environment variables inside the container. Here,
    it''s setting the MinIO root user''s username and password.'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`environment:` 部分设置容器内的环境变量。在这里，它设置了 MinIO 根用户的用户名和密码。'
- en: '`command: server /data --console-address ":9001"` is the command that will
    be run inside the MinIO container. It starts the MinIO server and tells it to
    use the `/data` directory.'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`command: server /data --console-address ":9001"` 是将在 MinIO 容器内运行的命令。它启动 MinIO
    服务器并指示使用 `/data` 目录。'
- en: With MinIO’s setup clear, let’s begin using it. You can access the MinIO web
    interface at `[http://localhost:9001](http://localhost:9001/)`. On your initial
    visit, you’ll need to log in with the username (`minio`)and password (`minio123`)
    specified in the docker-compose file.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: MinIO 设置完成后，让我们开始使用它。你可以通过 `[http://localhost:9001](http://localhost:9001/)`
    访问 MinIO 的 Web 界面。在首次访问时，你需要使用 `docker-compose` 文件中指定的用户名（`minio`）和密码（`minio123`）登录。
- en: '![](../Images/16bab217bc1b4cdccc55dd0d2c6b25cb.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/16bab217bc1b4cdccc55dd0d2c6b25cb.png)'
- en: MinIO portal (Image by author)
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: MinIO 门户（作者提供的图片）
- en: Once logged in, go ahead and create a bucket. Click on ‘Create a Bucket’ and
    name it `**mybucket**`. After naming it, click on ‘Create Bucket.’ The default
    settings are fine for now, but feel free to read about them on the page’s right
    side.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 登录后，继续创建一个桶。点击“创建桶”，并将其命名为 `**mybucket**`。命名后，点击“创建桶”。默认设置现在没问题，但可以随时查看页面右侧的设置说明。
- en: '![](../Images/a09022fb27cbb231673a46fef71bcb79.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a09022fb27cbb231673a46fef71bcb79.png)'
- en: Bucket created in MinIO (Image by author)
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在 MinIO 中创建的桶（作者提供的图片）
- en: Well done! We’re now ready to use MinIO. Let’s move on to exploring how we can
    use JupyterLab.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 做得好！我们现在可以使用 MinIO 了。接下来，让我们探讨如何使用 JupyterLab。
- en: Getting started with JupyterLab
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 开始使用 JupyterLab
- en: JupyterLab is an interactive web-based interface that helps us write code, perform
    analysis on notebooks and work with data. In fact, the JupyterLab image already
    includes Python and PySpark, so there’s no hassle in setting them up.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: JupyterLab 是一个互动的基于网页的界面，帮助我们编写代码、在笔记本上进行分析和处理数据。事实上，JupyterLab 镜像已经包括了 Python
    和 PySpark，因此不需要麻烦地设置它们。
- en: '![](../Images/4432377abc513c0fbf507b34f842c44a.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4432377abc513c0fbf507b34f842c44a.png)'
- en: JupyterLab service in **docker-compose.yml** file (Image by author)
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: JupyterLab 服务在 **docker-compose.yml** 文件中的配置（作者提供的图片）
- en: First, let’s revisit the `docker-compose.yml` file to understand the `jupyter`
    service.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们回顾一下 `docker-compose.yml` 文件，以了解 `jupyter` 服务。
- en: '`image: jupyter/pyspark-notebook` specifies to use the JupyterLab image that
    comes with PySpark pre-installed.'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`image: jupyter/pyspark-notebook` 指定使用预装 PySpark 的 JupyterLab 镜像。'
- en: '`ports: - "8888:8888"` maps the JupyterLab port to the same port on your host
    machine, allowing you to access it through your browser.'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ports: - "8888:8888"` 将 JupyterLab 端口映射到主机上的同一端口，使您可以通过浏览器访问它。'
- en: To access its web interface, navigate to the ‘Containers’ tab in the Docker
    Desktop application. Find and click on the JupyterLab container, labeled `jupyter-1`.
    This action will display the container logs.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 要访问其网页界面，请在 Docker Desktop 应用程序中导航到“Containers”选项卡。找到并点击标记为 `jupyter-1` 的 JupyterLab
    容器。这将显示容器日志。
- en: '![](../Images/02a91268fd1e6df5b57dfdcf305d9605.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/02a91268fd1e6df5b57dfdcf305d9605.png)'
- en: JupyterLab container logs (Image by author)
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: JupyterLab 容器日志（作者提供的图片）
- en: 'Within these logs, you’ll find a URL resembling this: `[http://127.0.0.1:8888/lab?token=4f1c9d4f1aeb460f1ebbf224dfa9417c88eab1691fa64b04](http://127.0.0.1:8888/lab?token=4f1c9d4f1aeb460f1ebbf224dfa9417c88eab1691fa64b04)`.
    Clicking on this URL launches the web interface.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些日志中，您会找到一个类似于这样的 URL：`[http://127.0.0.1:8888/lab?token=4f1c9d4f1aeb460f1ebbf224dfa9417c88eab1691fa64b04](http://127.0.0.1:8888/lab?token=4f1c9d4f1aeb460f1ebbf224dfa9417c88eab1691fa64b04)`。点击这个
    URL 启动网页界面。
- en: '![](../Images/e1accc3b8272213751be56fedcdce35a.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e1accc3b8272213751be56fedcdce35a.png)'
- en: JupyterLab web interface (Image by author)
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: JupyterLab 网页界面（作者提供的图片）
- en: Once there, select the ‘Python 3 (ipykernel)’ icon under the ‘Notebook’ section.
    This action opens a new notebook, where we’ll write code for data retrieval, transformation,
    and analysis. Before diving into coding, remember to save and name your notebook
    appropriately. And there you have it, we’re ready to start working with the data.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 到达那里后，选择“Notebook”部分下的“Python 3 (ipykernel)”图标。此操作会打开一个新笔记本，我们将在其中编写用于数据检索、转换和分析的代码。在深入编写代码之前，请记得保存并适当地命名您的笔记本。这样，我们就可以开始处理数据了。
- en: 'Data pipeline: The ETL process'
  id: totrans-85
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据管道：ETL 过程
- en: 'Before diving into data analysis, we first need to gather the data. We’ll employ
    an **ETL** (Extract, Transform, Load) process, which involves the following steps:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入数据分析之前，我们首先需要收集数据。我们将采用 **ETL**（提取、转换、加载）过程，包括以下步骤：
- en: Initially, we’ll extract data using a public API.
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最初，我们将使用公共 API 提取数据。
- en: Then, we’ll load this data as a JSON file into the MinIO bucket.
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后，我们将把这些数据作为 JSON 文件加载到 MinIO 存储桶中。
- en: After that, we’ll use PySpark to transform the data and save it back to the
    bucket in Parquet format.
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后，我们将使用 PySpark 转换数据，并将其以 Parquet 格式保存回存储桶。
- en: Lastly, we’ll create a Hive table from this Parquet data, which we’ll use for
    running Spark SQL queries for analysis.
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，我们将从这些 Parquet 数据中创建一个 Hive 表，用于运行 Spark SQL 查询进行分析。
- en: First up, we need to install the `s3fs` Python package, essential for working
    with MinIO in Python.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要安装 `s3fs` Python 包，这对在 Python 中使用 MinIO 至关重要。
- en: '[PRE0]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Following that, we’ll import the necessary dependencies and modules.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 随后，我们将导入必要的依赖项和模块。
- en: '[PRE1]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: We’ll also set some environment variables that will be useful when interacting
    with MinIO.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将设置一些在与 MinIO 交互时有用的环境变量。
- en: '[PRE2]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Next, we’ll fetch data from the public API using the `requests` Python package.
    We’re using the open-source **Rest Countries Project**. It gives information about
    the different countries of the world — area, population, capital city, time zones,
    etc. Click [here](https://restcountries.com/#about-this-project) to learn more
    about it.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将使用 `requests` Python 包从公共 API 获取数据。我们使用的是开源的 **Rest Countries Project**。它提供有关世界各国的信息——面积、人口、首都、时区等。点击
    [这里](https://restcountries.com/#about-this-project) 了解更多信息。
- en: '[PRE3]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Once we have the data, we’ll write it as a JSON file to the `mybucket` bucket.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们拥有数据，我们将其写入 `mybucket` 桶中的 JSON 文件。
- en: '[PRE4]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Great, we’ve successfully retrieved the data! Now, it’s time to initialize a
    **Spark session** for running PySpark code. If you’re new to Spark, understand
    that it’s a big data processing framework that operates on distributed computing
    principles, breaking data into chunks for parallel processing. A Spark session
    is essentially the gateway to any Spark application.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 很好，我们已经成功检索了数据！现在，是时候初始化一个 **Spark 会话** 来运行 PySpark 代码了。如果你对 Spark 不太了解，需知道它是一个大数据处理框架，基于分布式计算原理，将数据拆分成块以进行并行处理。Spark
    会话本质上是任何 Spark 应用程序的入口。
- en: '[PRE5]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Let’s simplify this to understand it better.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们简化一下，以便更好地理解。
- en: '`spark.jars.packages`: Downloads the required JAR files from the [Maven repository](https://mvnrepository.com/).
    A Maven repository is a central place used for storing build artifacts like JAR
    files, libraries, and other dependencies that are used in Maven-based projects'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`spark.jars.packages`：从 [Maven 仓库](https://mvnrepository.com/) 下载所需的 JAR 文件。Maven
    仓库是一个用于存储构建工件（如 JAR 文件、库和其他在基于 Maven 的项目中使用的依赖项）的中央位置。'
- en: '`spark.hadoop.fs.s3a.endpoint`: This is the endpoint URL for MinIO.'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`spark.hadoop.fs.s3a.endpoint`：这是 MinIO 的端点 URL。'
- en: '`spark.hadoop.fs.s3a.access.key` and `spark.hadoop.fs.s3a.secret.key`: This
    is the access key and secret key for MinIO. Note that it is the same as the username
    and password used to access the MinIO web interface.'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`spark.hadoop.fs.s3a.access.key` 和 `spark.hadoop.fs.s3a.secret.key`：这是 MinIO
    的访问密钥和秘密密钥。注意，它们与访问 MinIO Web 界面时使用的用户名和密码相同。'
- en: '`spark.hadoop.fs.s3a.path.style.access`: It is set to true to enable path-style
    access for the MinIO bucket.'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`spark.hadoop.fs.s3a.path.style.access`：设置为 true，以启用 MinIO 桶的路径样式访问。'
- en: '`spark.hadoop.fs.s3a.impl`: This is the implementation class for S3A file system.'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`spark.hadoop.fs.s3a.impl`：这是 S3A 文件系统的实现类。'
- en: You might wonder how to choose the correct JAR version. It depends on compatibility
    with the PySpark and Hadoop versions we use. Here’s how to check your PySpark
    and **Hadoop** versions (Hadoop is another open-source framework for working with
    big data).
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会想如何选择正确的 JAR 版本。这取决于与我们使用的 PySpark 和 Hadoop 版本的兼容性。以下是检查你的 PySpark 和 **Hadoop**
    版本的方法（Hadoop 是另一个用于处理大数据的开源框架）。
- en: '[PRE6]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Choosing the right JAR version is crucial to avoid errors. Using the same Docker
    image, the JAR version mentioned here should work fine. If you encounter setup
    issues, feel free to leave a comment. I’ll do my best to assist you :)
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 选择正确的 JAR 版本对于避免错误至关重要。使用相同的 Docker 镜像，文中提到的 JAR 版本应该能正常工作。如果遇到设置问题，请随时留言。我会尽力协助你
    :)
- en: Let’s start by reading the JSON data into a Spark dataframe using PySpark.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始使用 PySpark 将 JSON 数据读入 Spark 数据框。
- en: '[PRE7]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Keep in mind, our dataframe has only 250 rows. In data engineering, this is
    a very small amount. Working with millions or billions of rows is typical. However,
    for ease, we’re using a smaller dataset here.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，我们的数据框只有 250 行。在数据工程中，这是非常少的量。处理数百万或数十亿行是很常见的。然而，为了方便，我们这里使用了一个较小的数据集。
- en: We’ll then save this data as a **Parquet** file in the MinIO bucket. Parquet,
    a popular file format in big data, stores data column-wise to enhance query speed
    and reduce file size. It also partitions data for better query performance.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将把这些数据保存为 **Parquet** 文件到 MinIO 桶中。Parquet 是一种在大数据中广泛使用的文件格式，它按列存储数据，以提高查询速度并减少文件大小。它还对数据进行分区，以提高查询性能。
- en: After that, we’ll read this data into a new dataframe.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 之后，我们将把这些数据读入一个新的数据框中。
- en: '[PRE8]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: It’s good practice to store raw data separately. This way, we retain the original
    data, even after transforming and saving it differently. Before we dive into transformations,
    let’s check the MinIO bucket.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 将原始数据分开存储是一个好的实践。这样，我们可以保留原始数据，即使在转换和保存之后也不例外。在深入转换之前，让我们检查一下 MinIO 桶。
- en: In the MinIO web interface, select ‘Object Browser’ from the left menu, then
    open the bucket.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 在 MinIO Web 界面中，从左侧菜单中选择“对象浏览器”，然后打开桶。
- en: '![](../Images/6ecee02d2ca6d13aee6712007b6de074.png)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6ecee02d2ca6d13aee6712007b6de074.png)'
- en: Browsing contents of the bucket in MinIO (Image by author)
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 在 MinIO 中浏览桶的内容（图片来自作者）
- en: Here, you’ll find the earlier JSON file and the raw data in Parquet format.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，你会找到之前的 JSON 文件以及以 Parquet 格式保存的原始数据。
- en: Spark splits the Parquet data into multiple files in a folder, effectively chunking
    the data.
  id: totrans-123
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Spark 将 Parquet 数据拆分成多个文件，存放在一个文件夹中，从而有效地将数据块化。
- en: 'Now, let’s move on. We’ll transform the data as follows:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们继续。我们将按如下方式转换数据：
- en: '[PRE9]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Breaking down the code:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 代码解析：
- en: We select specific columns from the raw data for analysis. To access nested
    fields in this semi-structured data, we use the dot “.” operator.
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们从原始数据中选择特定的列进行分析。要访问这些半结构化数据中的嵌套字段，我们使用点“.”运算符。
- en: The `cntry_area` column is modified so that any negative value becomes NULL.
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cntry_area` 列被修改，使任何负值变为 NULL。'
- en: For `border_cntry` and `capital_cities`, which are **ArrayType**, we replace
    NULL values with an array of `NA`.
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于 `border_cntry` 和 `capital_cities` 这两列，它们是**ArrayType**，我们将 NULL 值替换为 `NA`
    的数组。
- en: After transformations, we’ll print the schema of the new dataframe.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 转换后，我们将打印新数据框的模式。
- en: '[PRE10]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Next, we’ll write the transformed data back as a new Parquet file.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将转换后的数据写回为一个新的 Parquet 文件。
- en: '[PRE11]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Following that, we create an external Hive table over this Parquet file. Hive,
    a data warehouse software built on Hadoop, facilitates data querying and analysis.
    An **external table** means Hive only handles the metadata, with the actual data
    in an external location (our MinIO bucket).
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们在这个 Parquet 文件上创建一个外部 Hive 表。Hive 是基于 Hadoop 构建的数据仓库软件，便于数据查询和分析。**外部表**意味着
    Hive 只处理元数据，实际数据存储在外部位置（我们的 MinIO 存储桶）。
- en: '[PRE12]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Once our Hive table is ready, we can view its details with a specific command.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们的 Hive 表准备好后，我们可以用特定命令查看其详细信息。
- en: '[PRE13]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: You’ll see the column names, data types, database name, location, table type,
    and more.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 你将看到列名、数据类型、数据库名称、位置、表类型等信息。
- en: Now, let’s query the first five records from the table.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们查询表中的前五条记录。
- en: '[PRE14]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Next, we’ll read the table into a dataframe and create a temporary view on it.
    While it’s possible to query directly from the table, using temporary views on
    dataframes is common in Spark. Let’s explore that too.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将表读入数据框并在其上创建一个临时视图。虽然可以直接从表中查询，但在 Spark 中常用数据框的临时视图。让我们也探讨一下。
- en: '[PRE15]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Fantastic. With this setup, we’re ready to start our analysis!
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 太棒了。通过这个设置，我们准备好开始分析了！
- en: Analysing semi-structured data
  id: totrans-144
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分析半结构化数据
- en: 'Now comes the exciting part. Let’s dive into our data analysis by addressing
    some intriguing questions derived from our data. Here are the queries we aim to
    explore:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 现在进入激动人心的部分。让我们通过解决一些有趣的问题来深入我们的数据分析。以下是我们计划探讨的查询：
- en: Which are the 10 largest countries in terms of area? (in sq. km.)
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 哪些国家的面积最大？（以平方千米计）
- en: Which country has the largest number of neighbouring countries?
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 哪个国家拥有最多的邻国？
- en: Which countries have the highest number of capital cities?
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 哪些国家拥有最多的首都城市？
- en: How many countries lie on two or more continents?
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 有多少国家位于两个或更多大洲？
- en: How many landlocked countries per continent?
  id: totrans-150
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 每个大洲有多少个内陆国家？
- en: Which country has the highest number of time zones?
  id: totrans-151
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 哪个国家拥有最多的时区？
- en: How many countries are not UN members?
  id: totrans-152
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 有多少国家不是联合国成员？
- en: Knowing what we’re looking for, let’s start crafting some Spark SQL queries
    to uncover these insights. First, we’ll set up a utility function for displaying
    results in the notebook, saving us from repetitive coding.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 知道我们要寻找什么之后，让我们开始编写一些 Spark SQL 查询来揭示这些见解。首先，我们将设置一个在笔记本中显示结果的工具函数，从而避免重复编码。
- en: '[PRE16]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Let’s kick off with our initial query.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从初始查询开始。
- en: '***Q1\. Which are the 10 largest countries in terms of area? (in sq. km.)***'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '***Q1\. 哪些国家的面积最大？（以平方千米计）***'
- en: For this, we’ll arrange the data by the `cntry_area` column in descending order.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，我们将按 `cntry_area` 列的降序排列数据。
- en: '[PRE17]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: We find that Russia has the largest area, followed by Antarctica. While there’s
    debate about Antarctica’s status as a country, it’s included in this dataset.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 我们发现俄罗斯的面积最大，其次是南极洲。虽然关于南极洲是否为国家存在争议，但它被包含在这个数据集中。
- en: '***Q2\. Which country has the largest number of neighbouring countries?***'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '***Q2\. 哪个国家拥有最多的邻国？***'
- en: Next, we focus on the `border_cntry` column. It’s an array type listing the
    neighbouring countries’ codes for each country. By using the `array_size` function,
    we calculate the array lengths and order the data accordingly, excluding rows
    where `border_cntry` is `NA`.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们关注 `border_cntry` 列。它是一个数组类型，列出每个国家邻国的代码。通过使用 `array_size` 函数，我们计算数组的长度并相应地排序数据，排除
    `border_cntry` 为 `NA` 的行。
- en: '[PRE18]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: This reveals China as the country with the most neighbours — a total of 16.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 这揭示了中国是邻国最多的国家——总共有16个。
- en: '***Q3\. Which countries have the highest number of capital cities?***'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: '***Q3\. 哪些国家拥有最多的首都城市？***'
- en: We’ll employ a similar method for the next question, applying `array_size` to
    the `capital_cities` column.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将对下一个问题采用类似的方法，将 `array_size` 应用于 `capital_cities` 列。
- en: '[PRE19]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: The results highlight South Africa and Palestine as the only countries with
    multiple capital cities.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: '***Q4\. How many countries lie on two or more continents?***'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: We’ll again use the `array_size` function, this time on the `cntry_continent`
    column.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: It turns out Turkey, Azerbaijan, and Russia span two continents each — Europe
    and Asia.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: '***Q5\. How many landlocked countries per continent?***'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: We’ll build a subquery with the country name, a boolean value indicating landlocked
    status, and an explode function for each `cntry_continent` array entry. Since
    a country may span multiple continents, we’ll aggregate by continent and sum the
    boolean values for landlocked countries.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: This indicates that Europe and Africa have the highest number of landlocked
    countries, considering a country may be counted more than once if it spans multiple
    continents.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: '***Q6\. Which country has the highest number of time zones?***'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: Here, we use `array_size` on the `nr_timezones` column and sort the results
    in descending order.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Interestingly, France tops the list, likely due to its territories beyond mainland
    France.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: '***Q7\. How many countries are not UN members?***'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: Here, we count the number of countries where `is_unmember` is False.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: According to the dataset, 57 countries are not UN members, a figure that includes
    independent territories and regions classified as countries. You can find the
    complete notebook [here](https://github.com/sarthak-sarbahi/data-analytics-minio-spark/blob/main/country_analysis.ipynb).
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: That wraps up our analysis! But before concluding, let’s discuss how to properly
    clean up our resources.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: Cleanup of resources
  id: totrans-185
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: After finishing, don’t forget to save your notebook. Then, it’s time to stop
    the Docker containers. In the Docker Desktop app, just click the *stop* button.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d3a0d65b7a92d27eb8d3331ba76bd7c2.png)'
  id: totrans-187
  prefs: []
  type: TYPE_IMG
- en: Stopping all the containers at once (Image by author)
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: This action will halt both containers simultaneously. Alternatively, you can
    stop each container one by one. You can choose to either retain these containers
    or delete them.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: Be aware, deleting the JupyterLab container means losing your notebook, as a
    new JupyterLab instance starts from scratch. However, your MinIO data will stay
    intact, as it’s stored on your host machine, not in the container’s memory.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: If you choose to remove the containers, you might also want to delete the Docker
    images for JupyterLab and MinIO, especially if you’re tight on storage. You can
    do this in the “Images” section of the Docker Desktop app.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  id: totrans-192
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this story, we explored a straightforward yet captivating data analytics
    case. We began by configuring our environment using Docker Compose. Next, we fetched
    data from an API, mimicking a real-world scenario. We then saved this data in
    a bucket similar to Amazon S3, in JSON format. Using PySpark, we enhanced this
    data and stored it persistently in Parquet format. We also learned how to create
    an external Hive table on top of this data. Finally, we used this table for our
    analysis, which involved working with complex data types like arrays.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: I sincerely hope this guide was beneficial for you. Should you have any questions,
    please don’t hesitate to drop them in the comments below.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: References
  id: totrans-195
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'GitHub repository: [https://github.com/sarthak-sarbahi/data-analytics-minio-spark/tree/main](https://github.com/sarthak-sarbahi/data-analytics-minio-spark/tree/main)'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Docker Compose: [https://docs.docker.com/compose/](https://docs.docker.com/compose/)'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'MinIO: [https://min.io/docs/minio/linux/index.html](https://min.io/docs/minio/linux/index.html)'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'PySpark: [https://spark.apache.org/docs/latest/api/python/index.html](https://spark.apache.org/docs/latest/api/python/index.html)'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
