- en: How To Best Leverage OpenAI’s Evals Framework
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/how-to-best-leverage-openais-evals-framework-c38bcef0ec47?source=collection_archive---------2-----------------------#2023-05-23](https://towardsdatascience.com/how-to-best-leverage-openais-evals-framework-c38bcef0ec47?source=collection_archive---------2-----------------------#2023-05-23)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/91f397160d3039365854c9a5bc606c06.png)'
  prefs: []
  type: TYPE_IMG
- en: Image licensed by author for commercial use
  prefs: []
  type: TYPE_NORMAL
- en: Keys for evaluating LLMs using OpenAI Evals
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://aparnadhinak.medium.com/?source=post_page-----c38bcef0ec47--------------------------------)[![Aparna
    Dhinakaran](../Images/e431ee69563ecb27c86f3428ba53574c.png)](https://aparnadhinak.medium.com/?source=post_page-----c38bcef0ec47--------------------------------)[](https://towardsdatascience.com/?source=post_page-----c38bcef0ec47--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----c38bcef0ec47--------------------------------)
    [Aparna Dhinakaran](https://aparnadhinak.medium.com/?source=post_page-----c38bcef0ec47--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ff32f85889f3a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-best-leverage-openais-evals-framework-c38bcef0ec47&user=Aparna+Dhinakaran&userId=f32f85889f3a&source=post_page-f32f85889f3a----c38bcef0ec47---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----c38bcef0ec47--------------------------------)
    ·6 min read·May 23, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fc38bcef0ec47&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-best-leverage-openais-evals-framework-c38bcef0ec47&user=Aparna+Dhinakaran&userId=f32f85889f3a&source=-----c38bcef0ec47---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc38bcef0ec47&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-best-leverage-openais-evals-framework-c38bcef0ec47&source=-----c38bcef0ec47---------------------bookmark_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: '*This blog is co-authored by* [*Trevor LaViale*](https://www.linkedin.com/in/trevor-laviale/)'
  prefs: []
  type: TYPE_NORMAL
- en: According to a recent [survey](https://arize.com/blog/survey-massive-retooling-around-large-language-models-underway/),
    nearly half (43%) of data science and engineering teams are planning production
    deployments of large language models (LLMs) over the next year. The age of LLMs
    is definitely upon us; however, evaluating these models is often challenging,
    and researchers need to develop reliable methods for comparing different models’
    performance.
  prefs: []
  type: TYPE_NORMAL
- en: A few months ago, OpenAI open-sourced their framework for evaluating LLMs against
    a series of benchmarks. This framework was used internally at OpenAI to ensure
    new versions of their models were performing adequately. OpenAI’s Eval Framework
    is a tool designed to help researchers and practitioners evaluate their LLMs and
    compare them to other state-of-the-art models.
  prefs: []
  type: TYPE_NORMAL
- en: How Does the Eval Framework Work?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now at this point, you may be thinking, “Wow, this seems like a really useful
    tool for evaluating LLMs, but what is an eval and how do I use it?” Let’s dive
    into the specifics!
  prefs: []
  type: TYPE_NORMAL
- en: An “eval” refers to a specific evaluation task that is used to measure the performance
    of a language model in a particular area, such as question answering or sentiment
    analysis. These evals are typically standardized benchmarks that allow for the
    comparison of different language models. The Eval framework provides a standardized
    interface for running these evals and collecting the results.
  prefs: []
  type: TYPE_NORMAL
- en: 'At its core, an eval is a dataset and an eval class that is defined in a YAML
    file. An example of an eval is shown below (this was taken from the [Github repository
    for evals](https://github.com/openai/evals)):'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s break down what the above means:'
  prefs: []
  type: TYPE_NORMAL
- en: 'test-match: This is the name of the eval'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'id: This is the full name of the eval that *test-match* is an alias for'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'description: Description of the eval.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'metrics: The metrics for the eval.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'class: Specifies the path for the module/class for the eval.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'args: Anything you want to pass to the class constructor.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'samples_jsonl: points to the location of where the samples are, which in this
    case are in test_match/samples.jsonl'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To give you an idea of what the samples look like, these are the samples in
    the ***test_match/samples.jsonl*** file:'
  prefs: []
  type: TYPE_NORMAL
- en: Within the JSONL file (just a JSON file with a unique JSON object per line),
    we can see the samples for the eval. Each JSON object represents a task for the
    model to complete, and counts as 1 data point in the eval. For more examples of
    JSONL files, you can go to [registry/data/README.md](https://github.com/openai/evals/blob/main/evals/registry/data/README.md)
    in the Eval Github Repository.
  prefs: []
  type: TYPE_NORMAL
- en: In the section below, we’ll go over how to run the test-match eval.
  prefs: []
  type: TYPE_NORMAL
- en: How can I run an eval?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We can run the above eval with a simple command:'
  prefs: []
  type: TYPE_NORMAL
- en: Here we’re using the oaieval CLI to run this eval. We’re specifying the name
    of the completion function (gpt-3.5-turbo) and the name of the eval (test-match).
    It’s as easy as that! *We’ll dive deeper into completion functions and how to
    build your evals in the section below.* After running that command, you’ll see
    the final report of accuracy printed to the console, as well as a file path to
    a temporary file that contains the full report. This just goes to show how easy
    it is to quickly evaluate LLMs using this framework. Next, let’s learn how to
    build our own evals instead of using one already in the registry.
  prefs: []
  type: TYPE_NORMAL
- en: How can I build my own eval?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we’ll go over how to build an eval from an existing template,
    as well as explaining completion functions and how to build your own.
  prefs: []
  type: TYPE_NORMAL
- en: Building Evals
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Building Samples
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Here we’ll walk through how to build a custom eval using an [existing template](https://github.com/openai/evals/blob/main/docs/eval-templates.md)
    to speed up the work. (If you want to build a completely custom eval, [here is
    a README](https://github.com/openai/evals/blob/main/docs/custom-eval.md) from
    the Eval Github repository.)
  prefs: []
  type: TYPE_NORMAL
- en: 'The first step in building the eval is constructing the samples. The samples
    need to contain certain fields depending on the template that you choose to use.
    Each sample needs to contain an “input” field which represents the prompt, which
    is recommended to be specified in [chat format](https://platform.openai.com/docs/guides/chat/introduction).
    The other fields depend on what template you choose to use for the eval. As an
    example, let’s use the *Match* template. In this case, I’d need to specify the
    field “input” in chat format and “ideal”. This could look like the below:'
  prefs: []
  type: TYPE_NORMAL
- en: This is telling the system to complete the phrase as concisely as possible,
    and the phrase we provided is “ABC AI is a company specializing in ML ” with the
    expected answer to be “observability”.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you have the samples in a different file format from JSONL, OpenAI provides
    a CLI to convert those samples to a JSONL file. You can use the code below provided
    by the [Evals repository](https://github.com/openai/evals/blob/main/docs/build-eval.md#formatting-your-data)
    to accomplish that:'
  prefs: []
  type: TYPE_NORMAL
- en: Great, we have our samples in a JSONL file! The next step is to register our
    eval.
  prefs: []
  type: TYPE_NORMAL
- en: Registering your eval
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To register the eval, we need to add a file to *evals/registry/evals/<eval_name>.yaml*.
    The format of this file is the same format as the example *test-match* eval above.
    It needs to contain the eval name, id, optional description, metrics, class, and
    args that specify where the sample file is. Once we register the eval, we can
    go ahead and run it just like we ran the test-match eval. That’s all it takes
    to set up your own evals!
  prefs: []
  type: TYPE_NORMAL
- en: Building Completion Functions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: What is a completion function?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the *How can I run an eval?* section, we briefly mentioned that you need
    to specify a completion function to run the oaieval command. First, let’s start
    with what a completion is. A completion is a model’s output to a prompt. For example,
    if the prompt we give the model is “Tell me what the capital of California is”,
    we expect the completion to be “Sacramento”. However, some prompts may require
    access to the internet or some other operations that help the model answer the
    question accurately, and this is where completion functions come into play. Completion
    functions allow you to define these operations that the model may need to perform.
    The completion function argument in the **oaieval** command can either be CompletionFn
    URLs, or the name of a model in the OpenAI API or key in the registry. More information
    on completion functions can be found [here](https://github.com/openai/evals/blob/main/docs/completion-fns.md).
  prefs: []
  type: TYPE_NORMAL
- en: How can I build my own completion function?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we’ll go over how to build your own completion function. In
    order to make your completion function compatible with all evals, it needs to
    implement a few interfaces. These interfaces essentially just standardize the
    inputs and outputs for the eval. If you’d like to get more information on these
    interfaces, check out the [docs on the Completion Function Protocol here](https://github.com/openai/evals/blob/main/docs/completion-fn-protocol.md).
  prefs: []
  type: TYPE_NORMAL
- en: 'Once your completion functions have been implemented, you need to register
    them similarly to how we registered our eval. Registering the completion function
    allows it to become available to the **oaieval** CLI. An example registration
    taken from the [Evals repository](https://github.com/openai/evals/blob/main/evals/registry/completion_fns/cot.yaml)
    is shown below:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s break down the above:'
  prefs: []
  type: TYPE_NORMAL
- en: '**cot/gpt-3.5-turbo**: This is the full name of the completion function that
    oaieval will use'
  prefs: []
  type: TYPE_NORMAL
- en: '**class**: This is the class path to the implementation of the completion function'
  prefs: []
  type: TYPE_NORMAL
- en: '**args**: Arguments passed to your completion function when initialized'
  prefs: []
  type: TYPE_NORMAL
- en: '**-> cot_completion_fn**: This is an argument passed to the ChainOfThoughtCompletionFn
    class'
  prefs: []
  type: TYPE_NORMAL
- en: Advantages of Using the Eval Framework
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Eval Framework provides several benefits to researchers and practitioners.
  prefs: []
  type: TYPE_NORMAL
- en: '**Standardized evaluation metrics and benchmarks**: The Eval Framework provides
    a standardized set of evaluation metrics that researchers can use to compare their
    models’ performance. This allows researchers to compare their models to other
    state-of-the-art models on the same benchmarks.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Easy to use**: The Eval Framework is designed to be easy to use. You can
    use existing templates to quickly build your own evals and get up and running
    with only a few lines of code as we’ve shown above.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Flexible**: The Eval Framework is flexible and can be used to evaluate models
    on a wide range of tasks and different benchmarks.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Open-source**: The Eval Framework is open-source, which means that researchers/practitioners
    can use and modify it for their specific needs. Additionally, anyone can contribute
    to the [openai/evals Github repository](https://github.com/openai/evals), which
    will help crowdsource even more benchmarks that can be shared across the community.'
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: OpenEvals is a powerful tool for [evaluating LLMs](https://arize.com/blog-course/evals-openai-simplifying-llm-evaluation/),
    providing researchers and practitioners alike a standardized set of evaluation
    metrics and tasks to compare their models to other state-of-the-art models. Given
    its benefits, the framework will likely be a useful tool in the future for evaluating
    and comparing model performance as LLMs continue to improve. Happy evaluating!
  prefs: []
  type: TYPE_NORMAL
