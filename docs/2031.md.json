["```py\nset.seed(2)\n\nn<-2000\nbeta1<-1\nbeta2<--1.8\n\n# Model Simulation\nX<-mvrnorm(n = n, mu=c(0,0), Sigma=matrix(c(1,0.7,0.7,1), nrow=2,ncol=2))\nu<-rnorm(n=n, sd = sqrt(exp(X[,1])))\nY<- matrix(beta1*X[,1] + beta2*X[,2] + u, ncol=1)\n```", "```py\nprob_na <- 0.3\nX[, 1] <- ifelse(X[, 2] <= -0.2 & runif(n) < prob_na, NA, X[, 1]) \n```", "```py\n# Choose an x that is not too far out\nx<-matrix(c(1,1),ncol=2)\n\n# Choose alpha for CIs\nalpha<-0.05\n```", "```py\n## Fit the new DRF framework\ndrf_fit <- drfCI(X=X, Y=Y, min.node.size = 5, splitting.rule='FourierMMD', num.features=10, B=100)\n\n## predict weights\nDRF = predictdrf(drf_fit, x=x)\nweights <- DRF$weights[1,]\n```", "```py\n# Estimate the conditional expectation at x:\ncondexpest<- sum(weights*Y)\n\n# Use the distribution of weights, see below\ndistofcondexpest<-unlist(lapply(DRF$weightsb, function(wb)  sum(wb[1,]*Y)  ))\n\n# Can either use the above directly to build confidence interval, or can use the normal approximation.\n# We will use the latter\nvarest<-var(distofcondexpest-condexpest)\n\n# build 95%-CI\nlower<-condexpest - qnorm(1-alpha/2)*sqrt(varest)\nupper<-condexpest + qnorm(1-alpha/2)*sqrt(varest)\nround(c(lower, condexpest, upper),2)\n\n# without NAs: (-1.00, -0.69 -0.37)\n# with NAs: (-1.15, -0.67, -0.19)\n```", "```py\n# Estimate the conditional expectation at x:\ncondvarest<- sum(weights*Y^2) - condexpest^2\n\ndistofcondvarest<-unlist(lapply(DRF$weightsb, function(wb)  {\n  sum(wb[1,]*Y^2) - sum(wb[1,]*Y)^2\n} ))\n\n# Can either use the above directly to build confidence interval, or can use the normal approximation.\n# We will use the latter\nvarest<-var(distofcondvarest-condvarest)\n\n# build 95%-CI\nlower<-condvarest - qnorm(1-alpha/2)*sqrt(varest)\nupper<-condvarest + qnorm(1-alpha/2)*sqrt(varest)\n\nc(lower, condvarest, upper)\n\n# without NAs: (1.89, 2.65, 3.42)\n# with NAs: (1.79, 2.74, 3.69)\n```", "```py\nrequire(drf)            \n\ndrfown <-               function(X, Y,\n                              num.trees = 500,\n                              splitting.rule = \"FourierMMD\",\n                              num.features = 10,\n                              bandwidth = NULL,\n                              response.scaling = TRUE,\n                              node.scaling = FALSE,\n                              sample.weights = NULL,\n                              sample.fraction = 0.5,\n                              mtry = min(ceiling(sqrt(ncol(X)) + 20), ncol(X)),\n                              min.node.size = 15,\n                              honesty = TRUE,\n                              honesty.fraction = 0.5,\n                              honesty.prune.leaves = TRUE,\n                              alpha = 0.05,\n                              imbalance.penalty = 0,\n                              compute.oob.predictions = TRUE,\n                              num.threads = NULL,\n                              seed = stats::runif(1, 0, .Machine$integer.max),\n                              compute.variable.importance = FALSE) {\n\n  # initial checks for X and Y\n  if (is.data.frame(X)) {\n\n    if (is.null(names(X))) {\n      stop(\"the regressor should be named if provided under data.frame format.\")\n    }\n\n    if (any(apply(X, 2, class) %in% c(\"factor\", \"character\"))) {\n      any.factor.or.character <- TRUE\n      X.mat <- as.matrix(fastDummies::dummy_cols(X, remove_selected_columns = TRUE))\n    } else {\n      any.factor.or.character <- FALSE\n      X.mat <- as.matrix(X)\n    }\n\n    mat.col.names.df <- names(X)\n    mat.col.names <- colnames(X.mat)\n  } else {\n    X.mat <- X\n    mat.col.names <- NULL\n    mat.col.names.df <- NULL\n    any.factor.or.character <- FALSE\n  }\n\n  if (is.data.frame(Y)) {\n\n    if (any(apply(Y, 2, class) %in% c(\"factor\", \"character\"))) {\n      stop(\"Y should only contain numeric variables.\")\n    }\n    Y <- as.matrix(Y)\n  }\n\n  if (is.vector(Y)) {\n    Y <- matrix(Y,ncol=1)\n  }\n\n  #validate_X(X.mat)\n\n  if (inherits(X, \"Matrix\") && !(inherits(X, \"dgCMatrix\"))) {\n        stop(\"Currently only sparse data of class 'dgCMatrix' is supported.\")\n    }\n\n  drf:::validate_sample_weights(sample.weights, X.mat)\n  #Y <- validate_observations(Y, X)\n\n  # set legacy GRF parameters\n  clusters <- vector(mode = \"numeric\", length = 0)\n  samples.per.cluster <- 0\n  equalize.cluster.weights <- FALSE\n  ci.group.size <- 1\n\n  num.threads <- drf:::validate_num_threads(num.threads)\n\n  all.tunable.params <- c(\"sample.fraction\", \"mtry\", \"min.node.size\", \"honesty.fraction\",\n                          \"honesty.prune.leaves\", \"alpha\", \"imbalance.penalty\")\n\n  # should we scale or not the data\n  if (response.scaling) {\n    Y.transformed <- scale(Y)\n  } else {\n    Y.transformed <- Y\n  }\n\n  data <- drf:::create_data_matrices(X.mat, outcome = Y.transformed, sample.weights = sample.weights)\n\n  # bandwidth using median heuristic by default\n  if (is.null(bandwidth)) {\n    bandwidth <- drf:::medianHeuristic(Y.transformed)\n  }\n\n  args <- list(num.trees = num.trees,\n               clusters = clusters,\n               samples.per.cluster = samples.per.cluster,\n               sample.fraction = sample.fraction,\n               mtry = mtry,\n               min.node.size = min.node.size,\n               honesty = honesty,\n               honesty.fraction = honesty.fraction,\n               honesty.prune.leaves = honesty.prune.leaves,\n               alpha = alpha,\n               imbalance.penalty = imbalance.penalty,\n               ci.group.size = ci.group.size,\n               compute.oob.predictions = compute.oob.predictions,\n               num.threads = num.threads,\n               seed = seed,\n               num_features = num.features,\n               bandwidth = bandwidth,\n               node_scaling = ifelse(node.scaling, 1, 0))\n\n  if (splitting.rule == \"CART\") {\n    ##forest <- do.call(gini_train, c(data, args))\n    forest <- drf:::do.call.rcpp(drf:::gini_train, c(data, args))\n    ##forest <- do.call(gini_train, c(data, args))\n  } else if (splitting.rule == \"FourierMMD\") {\n    forest <- drf:::do.call.rcpp(drf:::fourier_train, c(data, args))\n  } else {\n    stop(\"splitting rule not available.\")\n  }\n\n  class(forest) <- c(\"drf\")\n  forest[[\"ci.group.size\"]] <- ci.group.size\n  forest[[\"X.orig\"]] <- X.mat\n  forest[[\"is.df.X\"]] <- is.data.frame(X)\n  forest[[\"Y.orig\"]] <- Y\n  forest[[\"sample.weights\"]] <- sample.weights\n  forest[[\"clusters\"]] <- clusters\n  forest[[\"equalize.cluster.weights\"]] <- equalize.cluster.weights\n  forest[[\"tunable.params\"]] <- args[all.tunable.params]\n  forest[[\"mat.col.names\"]] <- mat.col.names\n  forest[[\"mat.col.names.df\"]] <- mat.col.names.df\n  forest[[\"any.factor.or.character\"]] <- any.factor.or.character\n\n  if (compute.variable.importance) {\n    forest[['variable.importance']] <- variableImportance(forest, h = bandwidth)\n  }\n\n  forest\n}\n```"]