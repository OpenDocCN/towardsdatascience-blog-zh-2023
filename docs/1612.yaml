- en: A brief history of language models
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/a-brief-history-of-language-models-d9e4620e025b?source=collection_archive---------11-----------------------#2023-05-12](https://towardsdatascience.com/a-brief-history-of-language-models-d9e4620e025b?source=collection_archive---------11-----------------------#2023-05-12)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Breakthroughs on the way towards GPT — explained for non-experts
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@doriandrost?source=post_page-----d9e4620e025b--------------------------------)[![Dorian
    Drost](../Images/1795395ad0586eafd83d3e2f7b975ca8.png)](https://medium.com/@doriandrost?source=post_page-----d9e4620e025b--------------------------------)[](https://towardsdatascience.com/?source=post_page-----d9e4620e025b--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----d9e4620e025b--------------------------------)
    [Dorian Drost](https://medium.com/@doriandrost?source=post_page-----d9e4620e025b--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F1d49ea537d1c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-brief-history-of-language-models-d9e4620e025b&user=Dorian+Drost&userId=1d49ea537d1c&source=post_page-1d49ea537d1c----d9e4620e025b---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----d9e4620e025b--------------------------------)
    ·9 min read·May 12, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fd9e4620e025b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-brief-history-of-language-models-d9e4620e025b&user=Dorian+Drost&userId=1d49ea537d1c&source=-----d9e4620e025b---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fd9e4620e025b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-brief-history-of-language-models-d9e4620e025b&source=-----d9e4620e025b---------------------bookmark_footer-----------)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: The overwhelming attention large language models like GPT get in media today
    creates the impression of an ongoing revolution we all are in the middle of. However,
    even a revolution builds on the successes of its predecessors, and GPT is the
    result of decades of research.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: In this post, I want to give an overview of some of the major steps in research
    in the realm of language models, that eventually led to the large language models
    we have today. I will briefly describe what a language model is in general, before
    discussing some of the core technologies that were leading the field at different
    times, and that, by overcoming the hurdles and difficulties of their ancestors,
    paved the way for today's technologies, of which (Chat-)GPT may be the most famous
    representative.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: What is a language model?
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/02c4b9a3bb243d65d9c3ef834667b71a.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
- en: What is necessary to turn words into a language model? Photo by [Glen Carrie](https://unsplash.com/fr/@glencarrie?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: A language model is a machine learning model, that predicts the next word given
    a sequence of words. It is as simple as that!
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: The main idea is, that such a model must have some representation of the human
    language. To some extent, it models the rules our language relies on. After having
    seen millions of lines of text, the model will represent the fact, that things
    like verbs, nouns, and pronouns exist in a language and that they serve different
    functions within a sentence. It may also get some patterns that come from the
    meaning of words, like the fact, that “chocolate” often appears within a context
    of words like “sweet”, “sugar” and “fat”, but rarely together with words like
    “lawnmower” or “linear regression”.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: As mentioned, it arrives at this representation by learning to predict the next
    word given a sequence of words. This is done by analyzing large amounts of text
    to infer, which word may be next for a given context. Let’s take a look at how
    this can be achieved.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: The starters
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/12fe94508ad0570b055f733f591f30e4.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
- en: We have to start simply before we can think of more sophisticated technologies.
    Photo by [Jon Cartagena](https://unsplash.com/@cartega?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s start with a first intuitive idea: Given a large number of texts, we
    can count the frequency of each word in a given context. The context is just the
    words appearing before. That is, for example, we count how often the word “like”
    appears after the word “I”, we count, how often it appears after the word “don’t”
    and so on for all words that ever occur before the word “like”. If we divide this
    by the frequency of the word before, we easily arrive at the probability P(“like”
    | “I”), read as *the probability of the word “like” given the word “I”:*'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: P(“like” | “I”) = count(“I like”) / count(“I”)
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: P(“like” | “don’t”) = count(“don’t like”) / count(“don’t”)
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: 'We could do that for every word pair we find in the text. However, there is
    an obvious drawback: The context that is used to determine the probability is
    only a single word. That means, if we want to predict what comes after the word
    “don’t”, our model does not know what was before the “don’t” and hence can’t distinguish
    between “They don’t”, “I don’t” or “We don’t”.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: To tackle this problem, we can extend the context. So, instead of calculating
    P(“like” | “don’t”), we calculate P(“like” | “I don’t”) and P(“like” | “they don’t”)
    and P(“like” | “we don’t”) and so on. We can even extend the context to more words,
    and that we call an n-gram model, where n is the number of words to consider for
    the context. An n-gram is just a sequence of n words, so “I like chocolate”, for
    example, is a 3-gram.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 要解决这个问题，我们可以扩展上下文。所以，不再计算 P(“like” | “don’t”)，而是计算 P(“like” | “I don’t”)、P(“like”
    | “they don’t”)、P(“like” | “we don’t”) 等等。我们甚至可以将上下文扩展到更多单词，这就是我们称之为 n-gram 模型，其中
    n 表示要考虑的上下文单词数量。一个 n-gram 只是 n 个单词的序列，因此，“I like chocolate” 是一个 3-gram。
- en: The larger the n, the more context the model can take into account for predicting
    the next word. However, the larger the n, the more different probabilities we
    have to calculate because there are many more different 5-grams than are 2-grams,
    for example. The number of different n-grams grows exponentially and easily reaches
    a point where it becomes infeasible to handle them in terms of memory or calculation
    time. Therefore, n-grams only allow us a very limited context, which is not enough
    for many tasks.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: n 越大，模型在预测下一个单词时能考虑的上下文就越多。然而，n 越大，我们需要计算的不同概率就越多，因为例如 5-gram 比 2-gram 要多得多。不同的
    n-gram 数量呈指数增长，并且很容易达到一种无法处理它们的内存或计算时间的程度。因此，n-gram 只允许我们非常有限的上下文，这对于许多任务来说是不够的。
- en: Recurrent Neural Networks
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 循环神经网络
- en: '![](../Images/921cfd15d94a347c9a0366834c9368ca.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/921cfd15d94a347c9a0366834c9368ca.png)'
- en: Recurrent Neural Networks are performing the same steps over and over again.
    Photo by [Önder Örtel](https://unsplash.com/@onderortel?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 循环神经网络一遍又一遍地执行相同的步骤。照片由 [Önder Örtel](https://unsplash.com/@onderortel?utm_source=medium&utm_medium=referral)
    提供，来自 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: Recurrent Neural Networks (RNNs) introduced a way to solve the issues the n-gram
    models have with bigger contexts. In an RNN, an input sequence is processed one
    word after the other, generating a so-called *hidden representation*. The main
    idea is, that this hidden representation includes all relevant information of
    the sequence so far and can be used in the next step to predict the next word.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 循环神经网络（RNNs）引入了一种方法来解决 n-gram 模型在处理更大上下文时遇到的问题。在 RNN 中，输入序列逐个单词地被处理，生成所谓的 *hidden
    representation*。其主要思想是，这个隐藏表示包含到目前为止序列的所有相关信息，并且可以在下一步中用于预测下一个单词。
- en: 'Let’s look at an example: Say we have the sentence'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们举个例子：假设我们有这样一个句子
- en: The mouse eats the cheese
  id: totrans-30
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: The mouse eats the cheese
- en: The RNN now processes one word after the other (“mouse” first, then “eats”,…),
    creates the hidden representation, and predicts which word is coming next most
    likely. Now, if we arrive at the word “the”, the input for the model will include
    the current word (“the”) and a hidden representation vector, that includes the
    relevant information of the sentence “the mouse eats”. This information is used
    to predict the next word (e.g. “cheese”). Note that the model does not see the
    words “the”, “mouse” and “eats”; those are encoded in the hidden representation.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，RNN 逐个处理单词（首先是“mouse”，然后是“eats”，...），创建隐藏表示，并预测最可能出现的下一个单词。例如，当我们到达单词“the”时，模型的输入将包括当前单词（“the”）和一个隐藏表示向量，该向量包含了句子“the
    mouse eats”的相关信息。这些信息用于预测下一个单词（例如“cheese”）。请注意，模型看不到单词“the”、“mouse”和“eats”；它们被编码在隐藏表示中。
- en: Is that better than seeing the last n words, as the n-gram model would? Well,
    it depends. The main advantage of the hidden representation is, that it can include
    information about sequences of varying sizes without growing exponentially. In
    a 3-gram model, the model sees exactly 3 words. If that is not enough to predict
    the next word accurately, it can’t do anything about that; it doesn’t have more
    information. On the other hand, the hidden representation used in the RNNs includes
    the whole sequence. However, it somehow has to fit all information in this fixed-sized
    vector, so the information is not stored in a verbatim way. If the sequence becomes
    longer, this can become a bottleneck all relevant information has to pass through.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: 'It may help you to think of the difference like this: The n-gram model only
    sees a limited context, but this context it sees clearly (the words as they are),
    while the RNNs have a bigger and more flexible context, but they see only a blurred
    image of it (the hidden representation).'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: 'Unfortunately, there is another disadvantage to RNNs: Since they process the
    sequence one word after another, they can not be trained in parallel. To process
    the word at position t, you need the hidden representation of step t-1, for which
    you need the hidden representation of step t-2, and so on. Hence the computation
    has to be done one step after another, both during training and during inference.
    It would be much nicer if you could compute the required information for each
    word in parallel, wouldn’t it?'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: 'Attention to the rescue: Transformers'
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/3e1c6ba208a156426fa9d180a1d6f62c.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
- en: Attention is all about hitting the right spot. Photo by [Afif Ramdhasuma](https://unsplash.com/@javaistan?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: Tranformers are a family of models that tackle the drawbacks the RNNs have.
    They avoid the bottleneck problem of the hidden representation, and they allow
    to be trained in parallel. How do they do this?
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: The key component of the transformer models is the attention mechanism. Remember
    that in the RNN, there is a hidden representation that includes all information
    of the input sequence so far. To avoid the bottleneck that comes from having a
    single representation for the whole sequence, the attention mechanism constructs
    a new hidden representation in every step, that can include information from any
    of the previous words. That allows the model to decide which parts of the sequence
    are relevant for predicting the next word, so it can focus on those by assigning
    them higher relevance for calculating the probabilities of the next word. Say
    we have the sentence
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: '*When I saw Dorothy and the scarecrow the other day, I went to her and said
    “Hi*'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: and we want to predict the next word. The attention mechanism allows the model
    to focus on the words that are relevant for the continuation and ignore those
    parts, that are irrelevant. In this example, the pronoun “her” must refer to “Dorothy”
    (and not “the scarecrow”), and hence the model must decide to focus on “Dorothy”
    and ignore “the scarecrow” for predicting the next word. For this sentence, it
    is much more likely that it continues with “Hi, Dorothy” instead of “Hi, scarecrow”
    or “Hi, together”.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望预测下一个词。注意力机制使模型能够专注于与续写相关的词，并忽略那些不相关的部分。在这个例子中，代词“her”必须指代“Dorothy”（而不是“the
    scarecrow”），因此模型必须决定专注于“Dorothy”并忽略“the scarecrow”来预测下一个词。对于这个句子，更可能的续写是“Hi, Dorothy”而不是“Hi,
    scarecrow”或“Hi, together”。
- en: An RNN would just have a single hidden representation vector, that may or may
    not include the information that is required to decide whom the pronoun “her”
    refers to. In contrast, with the attention mechanism, a new hidden representation
    is created, that includes much information from the word “Dorothy”, but less from
    other words that are not relevant at the moment. For the prediction of the next
    word, a new hidden representation will be calculated again, which may look very
    different, because now the model might want to put more focus on other words,
    e.g. “scarecrow”.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: RNN只会有一个隐藏表示向量，这个向量可能包含也可能不包含决定代词“her”指代谁所需的信息。相比之下，利用注意力机制，创建了一个新的隐藏表示，其中包含来自词“Dorothy”的大量信息，但来自其他当前不相关的词的信息较少。对于下一个词的预测，将再次计算新的隐藏表示，这可能看起来非常不同，因为现在模型可能希望更多地关注其他词，例如“scarecrow”。
- en: The attention mechanism has another advantage, namely that it allows parallelization
    of the training. As mentioned before, in an RNN, you have to calculate the hidden
    representation for each word one after another. In the Transformer, you calculate
    a hidden representation at each step, that only needs the representation of the
    single words. In particular, for calculating the hidden representation of step
    t, you don’t need the hidden representation of step t-1\. Hence you can calculate
    both in parallel.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力机制还有一个优点，即它允许训练的并行化。如前所述，在RNN中，你必须一个接一个地计算每个词的隐藏表示。在Transformer中，你在每一步计算隐藏表示，仅需单个词的表示。特别是，对于计算第t步的隐藏表示，你不需要第t-1步的隐藏表示。因此，你可以同时计算两者。
- en: The increase in model sizes over the last years, which allows models to perform
    better and better each day, is only possible because it became technically feasible
    to train those models in parallel. With recurrent neural networks, we wouldn’t
    be able to train models with hundreds of billions of parameters and hence wouldn’t
    be able to use those models’ capabilities of interacting with natural language.
    The Transformer’s attention mechanism can be seen as the last component that,
    together with large amounts of training data and decent computational resources,
    was needed for developing models like GPT and its siblings and starting the ongoing
    revolution in AI and language processing.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来模型规模的增加，使得模型每天的表现越来越好，这仅仅是因为技术上实现了这些模型的并行训练。使用循环神经网络，我们无法训练具有数百亿参数的模型，因此无法利用这些模型与自然语言交互的能力。Transformer的注意力机制可以看作是最后一个组成部分，它与大量的训练数据和足够的计算资源一起，为开发像GPT及其兄弟模型并启动人工智能和语言处理的持续革命提供了必要条件。
- en: Summary
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: 'So, what have we seen in this post? My goal was to give you an overview of
    some of the major steps that were necessary to arrive at the powerful language
    models we have today. As a summary, here are the important steps in order:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，在这篇文章中我们看到了什么？我的目标是给你一个概述，介绍达到今天强大语言模型所必需的一些主要步骤。作为总结，以下是按顺序排列的重要步骤：
- en: The key aspect of language modeling is to predict the next word given a sequence
    of text.
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 语言建模的关键方面是给定一段文本序列来预测下一个词。
- en: n-gram models can represent a limited context only.
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: n-gram模型只能表示有限的上下文。
- en: Recurrent Neural Networks have a more flexible context, but their hidden representation
    can become a bottleneck, and they can’t be trained in parallel.
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 循环神经网络具有更灵活的上下文，但它们的隐藏表示可能成为瓶颈，并且无法并行训练。
- en: Transformers avoid the bottleneck by introducing the attention mechanism, that
    allows to focus on specific parts of the context in detail. Eventually, they can
    be trained in parallel, which is a requirement for training large language models.
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Transformer 通过引入注意力机制来避免瓶颈，这使得能够详细关注上下文的特定部分。最终，它们可以并行训练，这对于训练大型语言模型是必需的。
- en: Of course, there have been many more technologies that were required to arrive
    at the models we have today. This overview is just highlighting some very important
    key aspects. What would you say, which other steps were relevant on the journey
    towards large language models?
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，为了达到我们今天拥有的模型，还需要许多其他技术。这一概述只是突出了一些非常重要的关键方面。你认为在迈向大型语言模型的过程中，还有哪些步骤是相关的？
- en: Further reading
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: 'For more background and technical details you can take a look at what could
    be called the bible of language modeling:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 欲了解更多背景和技术细节，可以查看被称为语言建模圣经的书籍：
- en: Speech and Language Processing, Dan Jurafsky and James H. Martin, 3rd ed. draft.
    [https://web.stanford.edu/~jurafsky/slp3/](https://web.stanford.edu/~jurafsky/slp3/)
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 《语音与语言处理》，Dan Jurafsky 和 James H. Martin，第三版草稿。 [https://web.stanford.edu/~jurafsky/slp3/](https://web.stanford.edu/~jurafsky/slp3/)
- en: The following papers introduce some milestones on the journey toward large language
    models.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 以下论文介绍了迈向大型语言模型过程中的一些里程碑。
- en: 'Recurrent Neural Networks:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 循环神经网络：
- en: Elman, J. L. (1990). Finding structure in time. *Cognitive science*, *14*(2),
    179–211.
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Elman, J. L. (1990). Finding structure in time. *认知科学*, *14*(2), 179–211.
- en: Hochreiter, S., & Schmidhuber, J. (1997). Long short-term memory. *Neural computation*,
    *9*(8), 1735–1780.
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hochreiter, S., & Schmidhuber, J. (1997). Long short-term memory. *神经计算*, *9*(8),
    1735–1780.
- en: 'Transformers:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 'Transformer:'
- en: Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N.,
    … & Polosukhin, I. (2017). Attention is all you need. *Advances in neural information
    processing systems*, *30*.
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N.,
    … & Polosukhin, I. (2017). Attention is all you need. *神经信息处理系统进展*, *30*.
- en: '*Like this article?* [*Follow me*](https://medium.com/@doriandrost) *to be
    notified of my future posts.*'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '*喜欢这篇文章？* [*关注我*](https://medium.com/@doriandrost) *以便收到我未来的帖子通知。*'
