- en: How Enterprises Can Build Their Own Large Language Model Similar to OpenAI’s
    ChatGPT
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 企业如何构建类似于 OpenAI 的 ChatGPT 的大型语言模型
- en: 原文：[https://towardsdatascience.com/how-enterprises-can-build-their-own-large-language-model-similar-to-openais-chatgpt-23ff6696c69c?source=collection_archive---------2-----------------------#2023-07-01](https://towardsdatascience.com/how-enterprises-can-build-their-own-large-language-model-similar-to-openais-chatgpt-23ff6696c69c?source=collection_archive---------2-----------------------#2023-07-01)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/how-enterprises-can-build-their-own-large-language-model-similar-to-openais-chatgpt-23ff6696c69c?source=collection_archive---------2-----------------------#2023-07-01](https://towardsdatascience.com/how-enterprises-can-build-their-own-large-language-model-similar-to-openais-chatgpt-23ff6696c69c?source=collection_archive---------2-----------------------#2023-07-01)
- en: Want to build your own ChatGPT? Here are three ways you can do so
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 想要构建你自己的 ChatGPT 吗？这里有三种方法可以实现
- en: '[](https://pronojit.medium.com/?source=post_page-----23ff6696c69c--------------------------------)[![Pronojit
    Saha](../Images/a15a3546470293cc1bec858af63c4515.png)](https://pronojit.medium.com/?source=post_page-----23ff6696c69c--------------------------------)[](https://towardsdatascience.com/?source=post_page-----23ff6696c69c--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----23ff6696c69c--------------------------------)
    [Pronojit Saha](https://pronojit.medium.com/?source=post_page-----23ff6696c69c--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://pronojit.medium.com/?source=post_page-----23ff6696c69c--------------------------------)[![Pronojit
    Saha](../Images/a15a3546470293cc1bec858af63c4515.png)](https://pronojit.medium.com/?source=post_page-----23ff6696c69c--------------------------------)[](https://towardsdatascience.com/?source=post_page-----23ff6696c69c--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----23ff6696c69c--------------------------------)
    [Pronojit Saha](https://pronojit.medium.com/?source=post_page-----23ff6696c69c--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fa9c37f94155a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-enterprises-can-build-their-own-large-language-model-similar-to-openais-chatgpt-23ff6696c69c&user=Pronojit+Saha&userId=a9c37f94155a&source=post_page-a9c37f94155a----23ff6696c69c---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----23ff6696c69c--------------------------------)
    ·10 min read·Jul 1, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F23ff6696c69c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-enterprises-can-build-their-own-large-language-model-similar-to-openais-chatgpt-23ff6696c69c&user=Pronojit+Saha&userId=a9c37f94155a&source=-----23ff6696c69c---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fa9c37f94155a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-enterprises-can-build-their-own-large-language-model-similar-to-openais-chatgpt-23ff6696c69c&user=Pronojit+Saha&userId=a9c37f94155a&source=post_page-a9c37f94155a----23ff6696c69c---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----23ff6696c69c--------------------------------)
    · 10 分钟阅读 · 2023年7月1日'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F23ff6696c69c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-enterprises-can-build-their-own-large-language-model-similar-to-openais-chatgpt-23ff6696c69c&source=-----23ff6696c69c---------------------bookmark_footer-----------)![](../Images/57660313805e8cf4021ef4fc6a82c1a5.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F23ff6696c69c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-enterprises-can-build-their-own-large-language-model-similar-to-openais-chatgpt-23ff6696c69c&source=-----23ff6696c69c---------------------bookmark_footer-----------)![](../Images/57660313805e8cf4021ef4fc6a82c1a5.png)'
- en: 'Fig 1: Three Ways to Build Your Custom LLM (Image by Author)'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：构建自定义 LLM 的三种方式（图像来源：作者）
- en: TL;DR. Enterprises should build their own custom LLM as it offers various benefits
    like customization, control, data privacy, and transparency among others. To streamline
    the process of building own custom LLMs it is recommended to follow the three
    levels approach— L1, L2 & L3\. These levels start from low model complexity, accuracy
    & cost (L1) to high model complexity, accuracy & cost (L3). Enterprises must balance
    this tradeoff to suit their needs and extract ROI from their LLM initiatives.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: TL;DR。企业应该构建自己的定制LLM，因为它提供了定制、控制、数据隐私和透明度等各种好处。为了简化构建定制LLM的过程，建议遵循三个级别的方法——L1、L2和L3。这些级别从低模型复杂度、准确性和成本（L1）到高模型复杂度、准确性和成本（L3）。企业必须在这些折衷中取得平衡，以满足他们的需求并从LLM计划中获得投资回报。
- en: Introduction
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: Language models have gained significant attention in recent years, revolutionizing
    various fields such as natural language processing, content generation, and virtual
    assistants. One of the most prominent examples is OpenAI’s ChatGPT, a large language
    model that can generate human-like text and engage in interactive conversations.
    This has sparked the curiosity of enterprises, leading them to explore the idea
    of building their own large language models (LLMs).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 语言模型近年来获得了显著关注，彻底改变了自然语言处理、内容生成和虚拟助手等多个领域。一个最突出的例子是OpenAI的ChatGPT，这是一种大型语言模型，可以生成类人文本并进行互动对话。这引发了企业的好奇心，促使他们探索构建自己大型语言模型（LLM）的想法。
- en: However, the decision to embark on building an LLM should be reviewed carefully.
    It requires significant resources, both in terms of computational power and data
    availability. Enterprises must weigh the benefits against the costs, evaluate
    the technical expertise required, and assess whether it aligns with their long-term
    goals.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，决定是否开始构建LLM需要仔细审视。它需要大量的资源，包括计算能力和数据可用性。企业必须权衡收益与成本，评估所需的技术专长，并判断这是否符合他们的长期目标。
- en: In this article, we show you three ways of building your own LLM, similar to
    OpenAI’s ChatGPT. By the end of this article, you will have a clearer understanding
    of the challenges, requirements, and potential rewards associated with building
    your own large language model. So let’s dive in!
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们向你展示了构建自己LLM的三种方法，类似于OpenAI的ChatGPT。在文章结束时，你将对构建自己的大型语言模型相关的挑战、需求和潜在回报有更清晰的了解。让我们深入探讨吧！
- en: Should enterprises build their own LLM?
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 企业应该构建自己的LLM吗？
- en: To understand whether enterprises should build their own LLM, let’s explore
    the three primary ways they can leverage such models.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解企业是否应该构建自己的LLM，让我们探索他们可以利用这些模型的三种主要方式。
- en: '![](../Images/a0b7115bf05e1a73ab4495a8fcbe2417.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a0b7115bf05e1a73ab4495a8fcbe2417.png)'
- en: 'Fig 2: Different Ways to Leverage Large Language Models (Image by Author)'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：利用大型语言模型的不同方式（作者提供的图像）
- en: '**1\. Closed sources LLMs:** Enterprises can utilize pre-existing LLM services
    like OpenAI’s ChatGPT, Google’s Bard, or similar offerings from different providers.
    These services provide a ready-to-use solution, allowing businesses to leverage
    the power of LLMs without the need for significant infrastructure investment or
    technical expertise.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '**1\. 闭源LLM：** 企业可以利用现有的LLM服务，如OpenAI的ChatGPT、Google的Bard，或其他提供商的类似服务。这些服务提供了现成的解决方案，使企业能够利用LLM的强大功能，而无需进行大量基础设施投资或具备技术专长。'
- en: 'Pros:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 优势：
- en: Quick and easy deployment, saving time and effort.
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 快速、简便的部署，节省时间和精力。
- en: Good performance on ​​​generic text generation tasks.
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在一般文本生成任务上的良好表现。
- en: 'Cons:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 缺点：
- en: Limited control over the model’s behavior and responses
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对模型行为和响应的控制有限
- en: Less accurate on domain or enterprise-specific data
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在特定领域或企业特定数据上的准确性较低
- en: Data privacy concerns​ since data is sent to the third party hosting the service
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据隐私问题，因为数据被发送到托管服务的第三方
- en: Dependency on third-party providers and potential pricing fluctuations.
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 依赖第三方供应商和可能的价格波动。
- en: '**2\. Using domain-specific LLMs:** Another approach is to use domain-specific
    language models, such as BloombergGPT for finance, BioMedLM for biomedical applications,
    MarketingGPT for marketing applications, CommerceGPT for e-commerce applications,
    etc. These models are trained on domain-specific data, enabling more accurate
    and tailored responses in their respective fields.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '**2\. 使用领域特定LLM：** 另一种方法是使用领域特定的语言模型，例如用于金融的BloombergGPT、用于生物医学应用的BioMedLM、用于营销应用的MarketingGPT、用于电子商务应用的CommerceGPT等。这些模型在领域特定的数据上进行训练，从而在各自领域中提供更准确和量身定制的响应。'
- en: 'Pros:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 优势：
- en: Improved accuracy in specific domains due to training on relevant data.
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于在相关数据上进行训练，特定领域的准确性得到了提升。
- en: Availability of pre-trained models tailored to specific industries.
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 具有针对特定行业量身定制的预训练模型的可用性。
- en: 'Cons:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 缺点：
- en: Limited flexibility in adapting the model beyond its designated domain.
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在超出指定领域的适应性方面灵活性有限。
- en: Dependency on the provider’s updates and availability of domain-specific models.
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 依赖于供应商的更新和领域特定模型的可用性。
- en: Slightly better accuracy but still limited by not being specific to your enterprise
    data
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 稍微提高了准确性，但仍受限于非特定于您企业数据的情况
- en: Data privacy concerns​ since data is sent to the third party hosting the service
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据隐私问题，因为数据被发送到托管服务的第三方
- en: '**3\. Build and host a custom LLM:** The most comprehensive option is for enterprises
    to build and host their own LLM using their specific data. This approach provides
    the highest level of customization and privacy control over the generated content.
    It allows organizations to fine-tune the model to their unique requirements, ensuring
    domain-specific accuracy and alignment with their brand voice.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '**3\. 构建和托管自定义 LLM**：最全面的选项是企业使用其特定数据构建和托管自己的 LLM。这种方法提供了最高级别的定制和隐私控制，确保与其品牌声音的一致性和领域特定的准确性。'
- en: 'Pros:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 优点：
- en: '**Complete customization and control**: A custom model enables businesses to
    generate responses that align precisely with their brand voice, industry-specific
    terminology, and unique requirements.'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**完全定制和控制**：自定义模型使企业能够生成与其品牌声音、行业特定术语和独特要求完全一致的响应。'
- en: '**Cost-effective:** If properly setup​ (finetuning cost in the order of $100s)​'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**成本效益**：如果设置得当（微调成本在几百美元左右）'
- en: '**Transparent**: Entire data & model are known​ to the enterprise'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**透明**：整个数据和模型对企业是已知的'
- en: '**Best accuracy**: By training the model on enterprise-specific data & requirements,
    it can better understand and respond to enterprise-specific queries, resulting
    in more accurate and contextually relevant outputs.'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**最佳准确性**：通过在企业特定数据和要求上训练模型，它可以更好地理解和回应企业特定的查询，从而产生更准确和上下文相关的输出。'
- en: '**Privacy friendly:** Data & Model stay in your environment. Having a custom
    model allows enterprises to retain control over their sensitive data, minimizing
    concerns related to data privacy and security breaches.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**隐私友好**：数据和模型保留在您的环境中。拥有自定义模型可以使企业控制其敏感数据，从而减少与数据隐私和安全漏洞相关的担忧。'
- en: '**Competitive Advantage:** A custom large language model can be a significant
    differentiator in industries where personalized and accurate language processing
    plays a crucial role.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**竞争优势**：在个性化和准确语言处理起着关键作用的行业中，自定义大型语言模型可以成为显著的区分因素。'
- en: 'Cons:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 缺点：
- en: Need significant ML & LLM expertise to build a custom large language model
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 需要显著的机器学习和 LLM 专业知识来构建自定义大型语言模型
- en: It’s important to note that the approach to custom LLM depends on various factors,
    including the enterprise’s budget, time constraints, required accuracy, and the
    level of control desired. However, as you can see from above building a custom
    LLM on enterprise-specific data offers numerous benefits.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，自定义 LLM 的方法取决于各种因素，包括企业的预算、时间限制、所需准确性和期望的控制级别。然而，正如您从上述内容中看到的，基于企业特定数据构建自定义
    LLM 提供了许多好处。
- en: Custom large language models offer unparalleled customization, control, and
    accuracy for specific domains, use cases, and enterprise requirements. Thus enterprises
    should look to build their own enterprise-specific custom large language model,
    to unlock a world of possibilities tailored specifically to their needs, industry,
    and customer base.
  id: totrans-47
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 自定义大型语言模型为特定领域、使用案例和企业需求提供了无与伦比的定制、控制和准确性。因此，企业应考虑构建自己的企业特定自定义大型语言模型，以解锁量身定制的无限可能，满足其需求、行业和客户群体。
- en: Three ways to build your own custom large language model
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建自定义大型语言模型的三种方式
- en: You can build your custom LLM in three ways and these range from low complexity
    to high complexity as shown in the below image.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过三种方式构建自定义 LLM，这些方式的复杂性从低到高，如下图所示。
- en: '![](../Images/57660313805e8cf4021ef4fc6a82c1a5.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/57660313805e8cf4021ef4fc6a82c1a5.png)'
- en: 'Fig 3: Three Ways to Build Your Custom LLM (Image by Author)'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：构建自定义 LLM 的三种方式（图片来源：作者）
- en: '**L1\. Utilization Tuned LLM**'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '**L1\. 利用调整的 LLM**'
- en: One prevalent method for leveraging pre-trained LLMs involves devising effective
    prompting techniques to address diverse tasks. An example of a common prompting
    approach is [In-Context Learning (ICL)](http://ai.stanford.edu/blog/understanding-incontext/),
    which entails expressing task descriptions and/or demonstrations in natural language
    text. Furthermore, the utilization of [Chain-of-Thought (CoT)](https://ai.googleblog.com/2022/05/language-models-perform-reasoning-via.html?m=1)
    can augment in-context learning by incorporating a sequence of intermediate reasoning
    steps within prompts. To build an L1 LLM,
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 利用预训练 LLM 的一种常见方法是制定有效的提示技术以应对各种任务。一个常见的提示方法是 [上下文学习（ICL）](http://ai.stanford.edu/blog/understanding-incontext/)，其包括用自然语言文本表达任务描述和/或示例。此外，通过在提示中加入一系列中间推理步骤，[思维链（CoT）](https://ai.googleblog.com/2022/05/language-models-perform-reasoning-via.html?m=1)
    的使用可以增强上下文学习。要构建一个 L1 LLM，
- en: '![](../Images/fbe243ef4d5a45eb7ae36dc594bff9fa.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fbe243ef4d5a45eb7ae36dc594bff9fa.png)'
- en: 'Fig 4: A comparative illustration of ICL & CoT prompting. (Image from the paper
    “A survey of Large Language Models” — Reference №7)'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：ICL 和 CoT 提示的比较示意图。（图片来自论文《大型语言模型调查》——参考文献 №7）
- en: To build an L1 LLM,
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 要构建一个 L1 LLM，
- en: Begin by selecting a suitable pre-trained LLM (which can be found in the [Hugging
    Face model library](https://huggingface.co/models) or other online resources),
    ensuring its compatibility with commercial use by reviewing the license.
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先选择一个合适的预训练 LLM（可以在 [Hugging Face 模型库](https://huggingface.co/models) 或其他在线资源中找到），通过查看许可证确保其适用于商业用途。
- en: Next, identify relevant data sources for your specific domain or use case, assembling
    a diverse and comprehensive dataset that encompasses a wide range of topics and
    language variations. For L1 LLM, labeled data is not required.
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，确定与你的特定领域或用例相关的数据源，组建一个多样化且全面的数据集，涵盖广泛的主题和语言变体。对于 L1 LLM，不需要标记数据。
- en: In the customization process, the model parameters of the chosen pre-trained
    LLM remain unaltered. Instead, prompt engineering techniques are employed to tailor
    the LLM’s responses to the dataset.
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在定制过程中，所选预训练 LLM 的模型参数保持不变。相反，采用提示工程技术来调整 LLM 的响应以适应数据集。
- en: As mentioned above, In-Context Learning and Chain-of-Thought Prompting are two
    popular prompt engineering approaches. These techniques, collectively known as
    Resource Efficient Tuning (RET), offer a streamlined means of obtaining responses
    without requiring significant infrastructure resources.
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如上所述，上下文学习和思维链提示是两种流行的提示工程方法。这些技术统称为资源高效调优（RET），提供了一种简化的方式来获得回应，而无需大量基础设施资源。
- en: '**L2\. Instruction Tuned LLM**'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '**L2. 指令调优 LLM**'
- en: Instruction tuning is the approach to fine-tuning pre-trained LLMs on a collection
    of formatted instances in the form of natural language, which is highly related
    to supervised fine-tuning and multi-task prompted training. With instruction tuning,
    LLMs are enabled to follow the task instructions for new tasks without using explicit
    examples (akin to zero-shot capability), thus having an improved generalization
    ability. To build this instruction-tuned L2 LLM,
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 指令调优是对预训练 LLM 进行微调的一种方法，其形式为自然语言中的格式化实例，这与监督微调和多任务提示训练高度相关。通过指令调优，LLM 能够在没有显式示例的情况下（类似于零-shot
    能力）遵循任务指令，从而具备更好的泛化能力。要构建这种指令调优的 L2 LLM，
- en: Begin by selecting a suitable pre-trained LLM (which can be found in the [Hugging
    Face model library](https://huggingface.co/models) or other online resources),
    ensuring its compatibility with commercial use by reviewing the license.
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先选择一个合适的预训练 LLM（可以在 [Hugging Face 模型库](https://huggingface.co/models) 或其他在线资源中找到），通过查看许可证确保其适用于商业用途。
- en: Next, identify relevant data sources for your target domain or use case. A labeled
    dataset containing a variety of instructions specific to your domain or use case
    is necessary. For instance, you can refer to the [dolly-15k dataset](https://huggingface.co/datasets/databricks/databricks-dolly-15k)
    provided by Databricks, which offers instructions in different formats such as
    closed-qa, open-qa, classification, information retrieval, and more. This dataset
    can serve as a template to construct your own instruction dataset.
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，确定与你的目标领域或用例相关的数据源。需要一个包含与你的领域或用例相关的各种指令的标记数据集。例如，你可以参考Databricks提供的[dolly-15k数据集](https://huggingface.co/datasets/databricks/databricks-dolly-15k)，它提供了不同格式的指令，如闭合问答、开放问答、分类、信息检索等。这个数据集可以作为构建你自己指令数据集的模板。
- en: Moving on to the supervised fine-tuning process, we introduce new model parameters
    to the original base LLM chosen in step 1\. By adding these parameters, we can
    train the model for specific epochs to fine-tune it for the given instructions.
    The advantage of this approach is that it avoids the need to update billions of
    parameters present in the base LLM, instead focusing on a smaller number of additional
    parameters (thousands or millions) while still achieving accurate results in the
    desired task. This approach also helps reduce costs.
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 进入监督微调过程，我们向第1步中选择的原始基础LLM引入新的模型参数。通过添加这些参数，我们可以训练模型进行特定的训练轮次，以微调其对给定指令的响应。这个方法的优点在于它避免了更新基础LLM中数十亿个参数的需要，而是集中在较少数量的附加参数（数千或数百万）上，同时仍能在所需任务中取得准确的结果。这种方法还帮助减少了成本。
- en: The next step is to do the fine-tuning. Various fine-tuning techniques such
    as prefix tuning, adapters, low-rank attention, and more on these will be elaborated
    on in a future article. The process of adding new model parameters discussed in
    point 3 above is also dependent on these techniques. For more detailed information,
    please refer to the references section. These techniques fall under the category
    of Parameter Efficient Fine Tuning (PEFT), as they enable customization without
    updating all parameters of the base LLM.
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下一步是进行微调。各种微调技术，如前缀调优、适配器、低秩注意力等，将在未来的文章中详细阐述。上述第3点中讨论的添加新模型参数的过程也依赖于这些技术。有关更多详细信息，请参见参考文献部分。这些技术属于参数高效微调（PEFT）类别，因为它们能够在不更新基础LLM的所有参数的情况下进行定制。
- en: '**L3\. Alignment Tuned LLM**'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '**L3. 对齐调优的LLM**'
- en: Since LLMs are trained to capture the data characteristics of pre-training corpora
    (including both high-quality and low-quality data), they are likely to generate
    toxic, biased, or even harmful content for humans. Thus it might be necessary
    to align LLMs with human values, e.g., helpful, honest, and harmless. For this
    alignment purpose, we use the technique of reinforcement learning with human feedback
    (RLHF), an effective tuning approach that enables LLMs to follow the expected
    instructions. It incorporates humans in the training loop with elaborately designed
    labeling strategies. To build this alignment-tuned L3 LLM,
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 由于LLM（大语言模型）是通过捕捉预训练语料库的数据特征进行训练的（包括高质量和低质量的数据），它们可能会生成有毒、偏见甚至对人类有害的内容。因此，可能有必要将LLM与人类价值观对齐，例如，有帮助、诚实和无害。为了这一对齐目的，我们使用了与人类反馈的强化学习（RLHF）技术，这是一种有效的调优方法，可以使LLM遵循预期的指令。它通过精心设计的标记策略将人类纳入训练循环。要构建这个对齐调优的L3
    LLM，
- en: Begin by selecting an open-source pre-trained LLM (which can be found in the
    [Hugging Face model library](https://huggingface.co/models) or other online resources)
    or your L2 LLM as your base model.
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先选择一个开源的预训练LLM（可以在[Hugging Face模型库](https://huggingface.co/models)或其他在线资源中找到），或者选择你的L2
    LLM作为基础模型。
- en: The primary technique for building an alignment-tuned LLM is RLHF, which combines
    supervised learning and reinforcement learning. It starts with taking a fine-tuned
    LLM on a specific domain or instruction corpus (from step 1) and using it to generate
    responses. Then those responses are annotated using a human to train a supervised
    reward model (typically using another pretrained LLM as the base model). Finally,
    the LLM (from step 1) is again fine-tuned by doing reinforcement learning ([PPO](https://huggingface.co/learn/deep-rl-course/unit8/introduction))
    with the reward model to generate the final response.
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 构建对齐调整LLM的主要技术是RLHF，它结合了监督学习和强化学习。首先，从第1步的特定领域或指令语料库的微调LLM开始，使用它生成回应。然后，这些回应由人类注释，用于训练监督奖励模型（通常使用另一个预训练LLM作为基础模型）。最后，通过使用奖励模型进行强化学习（[PPO](https://huggingface.co/learn/deep-rl-course/unit8/introduction)），对LLM（来自第1步）进行再次微调，以生成最终回应。
- en: 'Thus two LLMs are trained: one for the reward model and another for fine-tuning
    the LLM for generating the final response. The base model parameters in both cases
    can be updated selectively, depending on the desired accuracy in the response.
    For example, in some RLHF methods, only the parameters in specific layers or components
    involved in reinforcement learning are updated to avoid overfitting and retain
    the general knowledge captured by the pre-trained LLM.'
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 因此，训练了两个LLM：一个用于奖励模型，另一个用于微调LLM以生成最终回应。在这两种情况下，基本模型参数可以有选择地更新，取决于所需的响应准确性。例如，在一些RLHF方法中，仅更新涉及强化学习的特定层或组件的参数，以避免过拟合，并保留预训练LLM捕获的一般知识。
- en: An interesting artifact of this process is that the successful RLHF systems
    to date have used reward language models with varying sizes relative to the text
    generation (e.g. OpenAI 175B LM, 6B reward model, Anthropic used LM and reward
    models from 10B to 52B, DeepMind uses 70B Chinchilla models for both LM and reward).
    An intuition would be that these preference models need to have a similar capacity
    to understand the text given to them as a model would need in order to generate
    said text.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 这个过程的一个有趣的现象是，到目前为止，成功的RLHF系统使用了相对于文本生成的不同大小的奖励语言模型（例如，OpenAI 175B LM，6B奖励模型，Anthropic使用从10B到52B的LM和奖励模型，DeepMind使用70B
    Chinchilla模型作为LM和奖励模型）。一种直觉是，这些偏好模型需要有类似于生成文本所需的模型的能力，以理解所给定的文本。
- en: There is also RLAIF (Reinforcement Learning with AI Feedback) which can be used
    in place of RLHF. The main difference here is instead of the human feedback an
    AI model serves as the evaluator or critic, providing feedback to the AI agent
    during the reinforcement learning process.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 还有RLAIF（带有AI反馈的强化学习），可以替代RLHF。主要区别在于，AI模型作为评估者或评论者，在强化学习过程中向AI代理提供反馈，而不是人类反馈。
- en: Conclusion
  id: totrans-74
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: Enterprises can harness the extraordinary potential of custom LLMs to achieve
    exceptional customization, control, and accuracy that align with their specific
    domains, use cases, and organizational demands. Building an enterprise-specific
    custom LLM empowers businesses to unlock a multitude of tailored opportunities,
    perfectly suited to their unique requirements, industry dynamics, and customer
    base.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 企业可以利用定制LLM的非凡潜力，实现与其特定领域、用例和组织需求相符的卓越定制、控制和准确性。建立一个特定于企业的定制LLM，使企业能够解锁大量量身定制的机会，完全符合其独特需求、行业动态和客户基础。
- en: The journey to building own custom LLM has three levels starting from low model
    complexity, accuracy & cost to high model complexity, accuracy & cost. Enterprises
    must balance this tradeoff to suit their needs to the best and extract ROI from
    their LLM initiative.
  id: totrans-76
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 建立自定义LLM的过程有三个层级，从低模型复杂性、准确性和成本到高模型复杂性、准确性和成本。企业必须在这些权衡之间找到平衡，以最适合其需求并从LLM计划中获取投资回报。
- en: '![](../Images/8ff13e199df2d9c7494e07484dc97106.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8ff13e199df2d9c7494e07484dc97106.png)'
- en: 'Fig 5: Tradeoffs among the three LLM levels (Image by Author)'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：三个LLM层级之间的权衡（图像由作者提供）
- en: '**References**'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '**参考文献**'
- en: '[What is prompt engineering](https://medium.com/r?url=https%3A%2F%2Fwww.hackerrank.com%2Fblog%2Fwhat-is-prompt-engineering%2F)?'
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[什么是提示工程](https://medium.com/r?url=https%3A%2F%2Fwww.hackerrank.com%2Fblog%2Fwhat-is-prompt-engineering%2F)?'
- en: In-Context Learning (ICL) — Q. Dong, L. Li, D. Dai, C. Zheng, Z. Wu, B. Chang,
    X. Sun, J. Xu, L. Li, and Z. Sui, “A survey for in-context learning,” CoRR, vol.
    abs/2301.00234, 2023.
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: In-Context Learning (ICL) — Q. Dong, L. Li, D. Dai, C. Zheng, Z. Wu, B. Chang,
    X. Sun, J. Xu, L. Li, 和 Z. Sui, “关于上下文学习的调查，” CoRR, 卷. abs/2301.00234, 2023。
- en: '[How does in-context learning work? A framework for understanding the differences
    from traditional supervised learning | SAIL Blog (stanford.edu)](http://ai.stanford.edu/blog/understanding-incontext/)'
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[上下文学习如何工作？理解与传统监督学习差异的框架 | SAIL博客 (stanford.edu)](http://ai.stanford.edu/blog/understanding-incontext/)'
- en: Chain of Thought Prompting — J. Wei, X. Wang, D. Schuurmans, M. Bosma, E. H.
    Chi, Q. Le, and D. Zhou, “Chain of thought prompting elicits reasoning in large
    language models,” CoRR, vol. abs/2201.11903, 2022.
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 思维链提示 — J. Wei、X. Wang、D. Schuurmans、M. Bosma、E. H. Chi、Q. Le 和 D. Zhou，“思维链提示在大型语言模型中引发推理，”CoRR，第
    abs/2201.11903 卷，2022年。
- en: '[Language Models Perform Reasoning via Chain of Thought — Google AI Blog (googleblog.com)](https://ai.googleblog.com/2022/05/language-models-perform-reasoning-via.html?m=1)'
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[语言模型通过思维链进行推理 — Google AI 博客 (googleblog.com)](https://ai.googleblog.com/2022/05/language-models-perform-reasoning-via.html?m=1)'
- en: Instruction Tuning — J. Wei, M. Bosma, V. Y. Zhao, K. Guu, A. W. Yu, B. Lester,
    N. Du, A. M. Dai, and Q. V. Le, “Fine-tuned language models are zero-shot learners,”
    in The Tenth International Conference on Learning Representations, ICLR 2022,
    Virtual Event, April 25–29, 2022\. OpenReview.net, 2022.
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 指令调整 — J. Wei、M. Bosma、V. Y. Zhao、K. Guu、A. W. Yu、B. Lester、N. Du、A. M. Dai
    和 Q. V. Le，“微调语言模型是零样本学习者，”发表于第十届国际学习表征会议，ICLR 2022，虚拟会议，2022年4月25–29日\. OpenReview.net，2022年。
- en: A survey of Large Language Models — Wayne Xin Zhao, Kun Zhou*, Junyi Li*, Tianyi
    Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican
    Dong, Yifan Du, Chen Yang, Yushuo Chen, Zhipeng Chen, Jinhao Jiang, Ruiyang Ren,
    Yifan Li, Xinyu Tang, Zikang Liu, Peiyu Liu, Jian-Yun Nie and Ji-Rong Wen, arXiv:2303.18223v4
    [cs.CL], April 12, 2023
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一项关于大型语言模型的调查 — Wayne Xin Zhao、Kun Zhou*、Junyi Li*、Tianyi Tang、Xiaolei Wang、Yupeng
    Hou、Yingqian Min、Beichen Zhang、Junjie Zhang、Zican Dong、Yifan Du、Chen Yang、Yushuo
    Chen、Zhipeng Chen、Jinhao Jiang、Ruiyang Ren、Yifan Li、Xinyu Tang、Zikang Liu、Peiyu
    Liu、Jian-Yun Nie 和 Ji-Rong Wen，arXiv:2303.18223v4 [cs.CL]，2023年4月12日
