# æƒé‡é‡åŒ–ç®€ä»‹

> åŸæ–‡ï¼š[https://towardsdatascience.com/introduction-to-weight-quantization-2494701b9c0c?source=collection_archive---------0-----------------------#2023-07-07](https://towardsdatascience.com/introduction-to-weight-quantization-2494701b9c0c?source=collection_archive---------0-----------------------#2023-07-07)

## ä½¿ç”¨8ä½é‡åŒ–æ¥å‡å°‘å¤§å‹è¯­è¨€æ¨¡å‹çš„å¤§å°

[](https://medium.com/@mlabonne?source=post_page-----2494701b9c0c--------------------------------)[![Maxime Labonne](../Images/a7efdd305e3cc77d5509bbb1076d57d8.png)](https://medium.com/@mlabonne?source=post_page-----2494701b9c0c--------------------------------)[](https://towardsdatascience.com/?source=post_page-----2494701b9c0c--------------------------------)[![Towards Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----2494701b9c0c--------------------------------) [Maxime Labonne](https://medium.com/@mlabonne?source=post_page-----2494701b9c0c--------------------------------)

Â·

[å…³æ³¨](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fdc89da634938&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintroduction-to-weight-quantization-2494701b9c0c&user=Maxime+Labonne&userId=dc89da634938&source=post_page-dc89da634938----2494701b9c0c---------------------post_header-----------) å‘è¡¨åœ¨ [Towards Data Science](https://towardsdatascience.com/?source=post_page-----2494701b9c0c--------------------------------) Â·14åˆ†é’Ÿé˜…è¯»Â·2023å¹´7æœˆ7æ—¥[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F2494701b9c0c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintroduction-to-weight-quantization-2494701b9c0c&user=Maxime+Labonne&userId=dc89da634938&source=-----2494701b9c0c---------------------clap_footer-----------)

--

[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F2494701b9c0c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintroduction-to-weight-quantization-2494701b9c0c&source=-----2494701b9c0c---------------------bookmark_footer-----------)![](../Images/cb9fa85d2c510e5126e6d30fce29eff4.png)

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä»¥å…¶å¹¿æ³›çš„è®¡ç®—éœ€æ±‚è€Œé—»åã€‚é€šå¸¸ï¼Œé€šè¿‡å°†å‚æ•°æ•°é‡ï¼ˆ**å¤§å°**ï¼‰ä¹˜ä»¥è¿™äº›å€¼çš„ç²¾åº¦ï¼ˆ**æ•°æ®ç±»å‹**ï¼‰ï¼Œå¯ä»¥è®¡ç®—æ¨¡å‹çš„å¤§å°ã€‚ç„¶è€Œï¼Œä¸ºäº†èŠ‚çœå†…å­˜ï¼Œå¯ä»¥é€šè¿‡ä¸€ç§ç§°ä¸ºé‡åŒ–çš„è¿‡ç¨‹ä½¿ç”¨è¾ƒä½ç²¾åº¦çš„æ•°æ®ç±»å‹æ¥å­˜å‚¨æƒé‡ã€‚

æˆ‘ä»¬åœ¨æ–‡çŒ®ä¸­åŒºåˆ†äº†ä¸¤å¤§ç±»æƒé‡é‡åŒ–æŠ€æœ¯ï¼š

+   **è®­ç»ƒåé‡åŒ–**ï¼ˆPTQï¼‰æ˜¯ä¸€ç§ç›´æ¥çš„æŠ€æœ¯ï¼Œå…¶ä¸­å·²è®­ç»ƒå¥½çš„æ¨¡å‹çš„æƒé‡è¢«è½¬æ¢ä¸ºè¾ƒä½çš„ç²¾åº¦ï¼Œè€Œä¸éœ€è¦è¿›è¡Œä»»ä½•é‡æ–°è®­ç»ƒã€‚å°½ç®¡å®æ–½èµ·æ¥ç®€å•ï¼Œä½†PTQå¯èƒ½ä¼šå¯¼è‡´æ€§èƒ½ä¸‹é™ã€‚

+   **é‡åŒ–æ„ŸçŸ¥è®­ç»ƒ**ï¼ˆQATï¼‰åœ¨é¢„è®­ç»ƒæˆ–å¾®è°ƒé˜¶æ®µçº³å…¥æƒé‡è½¬æ¢è¿‡ç¨‹ï¼Œä»è€Œæå‡æ¨¡å‹æ€§èƒ½ã€‚ç„¶è€Œï¼ŒQATè®¡ç®—æˆæœ¬é«˜ä¸”éœ€è¦ä»£è¡¨æ€§çš„è®­ç»ƒæ•°æ®ã€‚

æœ¬æ–‡èšç„¦äºPTQä»¥å‡å°‘å‚æ•°çš„ç²¾åº¦ã€‚ä¸ºäº†è·å¾—è‰¯å¥½çš„ç›´è§‰ï¼Œæˆ‘ä»¬å°†åº”ç”¨ç®€å•å’Œæ›´å¤æ‚çš„æŠ€æœ¯äºä¸€ä¸ªä½¿ç”¨GPT-2æ¨¡å‹çš„ç¤ºä¾‹ã€‚

å®Œæ•´ä»£ç å¯ä»¥åœ¨[Google Colab](https://colab.research.google.com/drive/1DPr4mUQ92Cc-xf4GgAaB6dFcFnWIvqYi?usp=sharing)å’Œ[GitHub](https://github.com/mlabonne/llm-course/blob/main/Introduction_to_Weight_Quantization.ipynb)ä¸Šè‡ªç”±è·å–ã€‚

# ğŸ“š æµ®ç‚¹è¡¨ç¤ºçš„èƒŒæ™¯

æ•°æ®ç±»å‹çš„é€‰æ‹©å†³å®šäº†æ‰€éœ€çš„è®¡ç®—èµ„æºé‡ï¼Œå½±å“æ¨¡å‹çš„é€Ÿåº¦å’Œæ•ˆç‡ã€‚åœ¨æ·±åº¦å­¦ä¹ åº”ç”¨ä¸­ï¼Œå¹³è¡¡ç²¾åº¦å’Œè®¡ç®—æ€§èƒ½æˆä¸ºä¸€ä¸ªè‡³å…³é‡è¦çš„ä»»åŠ¡ï¼Œå› ä¸ºæ›´é«˜çš„ç²¾åº¦é€šå¸¸æ„å‘³ç€æ›´å¤§çš„è®¡ç®—éœ€æ±‚ã€‚

åœ¨å„ç§æ•°æ®ç±»å‹ä¸­ï¼Œæµ®ç‚¹æ•°ç”±äºèƒ½å¤Ÿä»¥é«˜ç²¾åº¦è¡¨ç¤ºå¹¿æ³›çš„å€¼èŒƒå›´ï¼Œå› æ­¤åœ¨æ·±åº¦å­¦ä¹ ä¸­è¢«å¹¿æ³›ä½¿ç”¨ã€‚é€šå¸¸ï¼Œæµ®ç‚¹æ•°ä½¿ç”¨*n*ä½æ¥å­˜å‚¨æ•°å€¼ã€‚è¿™äº›*n*ä½è¿›ä¸€æ­¥è¢«åˆ’åˆ†ä¸ºä¸‰ä¸ªä¸åŒçš„éƒ¨åˆ†ï¼š

1.  **ç¬¦å·**ï¼šç¬¦å·ä½æŒ‡ç¤ºæ•°å­—çš„æ­£è´Ÿæ€§è´¨ã€‚å®ƒä½¿ç”¨ä¸€ä¸ªä½ï¼Œå…¶ä¸­0è¡¨ç¤ºæ­£æ•°ï¼Œ1è¡¨ç¤ºè´Ÿæ•°ã€‚

1.  **æŒ‡æ•°**ï¼šæŒ‡æ•°æ˜¯ä¸€æ®µä½è¡¨ç¤ºåŸºæ•°ï¼ˆåœ¨äºŒè¿›åˆ¶è¡¨ç¤ºä¸­é€šå¸¸ä¸º2ï¼‰çš„å¹‚ã€‚æŒ‡æ•°å¯ä»¥æ˜¯æ­£æ•°æˆ–è´Ÿæ•°ï¼Œä»è€Œä½¿æ•°å­—èƒ½å¤Ÿè¡¨ç¤ºéå¸¸å¤§æˆ–éå¸¸å°çš„å€¼ã€‚

1.  **å°¾æ•°/æœ‰æ•ˆæ•°å­—**ï¼šå‰©ä½™çš„ä½ç”¨äºå­˜å‚¨å°¾æ•°ï¼Œä¹Ÿç§°ä¸ºæœ‰æ•ˆæ•°å­—ã€‚å®ƒè¡¨ç¤ºæ•°å­—çš„æœ‰æ•ˆæ•°å­—ã€‚æ•°å­—çš„ç²¾åº¦ä¸¥é‡ä¾èµ–äºå°¾æ•°çš„é•¿åº¦ã€‚

è¿™ç§è®¾è®¡å…è®¸æµ®ç‚¹æ•°ä»¥ä¸åŒç²¾åº¦è¦†ç›–å¹¿æ³›çš„å€¼èŒƒå›´ã€‚ç”¨äºè¿™ç§è¡¨ç¤ºçš„å…¬å¼æ˜¯ï¼š

![](../Images/2e8ab62ec7cb0706b3d6b99f32abc5fa.png)

ä¸ºäº†æ›´å¥½åœ°ç†è§£è¿™ä¸€ç‚¹ï¼Œè®©æˆ‘ä»¬æ·±å…¥æ¢è®¨åœ¨æ·±åº¦å­¦ä¹ ä¸­æœ€å¸¸ç”¨çš„æ•°æ®ç±»å‹ï¼šfloat32ï¼ˆFP32ï¼‰ã€float16ï¼ˆFP16ï¼‰å’Œbfloat16ï¼ˆBF16ï¼‰ï¼š

+   **FP32** ä½¿ç”¨32ä½è¡¨ç¤ºä¸€ä¸ªæ•°å­—ï¼šä¸€ä¸ªä½ç”¨äºç¬¦å·ï¼Œå…«ä¸ªä½ç”¨äºæŒ‡æ•°ï¼Œå…¶ä½™23ä¸ªä½ç”¨äºå°¾æ•°ã€‚è™½ç„¶å®ƒæä¾›äº†è¾ƒé«˜çš„ç²¾åº¦ï¼Œä½†FP32çš„ç¼ºç‚¹æ˜¯å…¶è®¡ç®—å’Œå†…å­˜å¼€é”€è¾ƒå¤§ã€‚

+   **FP16** ä½¿ç”¨16ä½å­˜å‚¨ä¸€ä¸ªæ•°å­—ï¼šä¸€ä¸ªç”¨äºç¬¦å·ï¼Œäº”ä¸ªä½ç”¨äºæŒ‡æ•°ï¼Œåä¸ªä½ç”¨äºå°¾æ•°ã€‚è™½ç„¶è¿™ä½¿å…¶åœ¨å†…å­˜ä¸Šæ›´é«˜æ•ˆå¹¶åŠ é€Ÿè®¡ç®—ï¼Œä½†å‡å°‘çš„èŒƒå›´å’Œç²¾åº¦å¯èƒ½ä¼šå¼•å…¥æ•°å€¼ä¸ç¨³å®šæ€§ï¼Œå¯èƒ½å½±å“æ¨¡å‹çš„å‡†ç¡®æ€§ã€‚

+   **BF16**ä¹Ÿæ˜¯ä¸€ç§16ä½æ ¼å¼ï¼Œä½†æœ‰ä¸€ä¸ªç¬¦å·ä½ï¼Œ*å…«*ä¸ªä½ç”¨äºæŒ‡æ•°ï¼Œ*ä¸ƒ*ä¸ªä½ç”¨äºå°¾æ•°ã€‚BF16æ‰©å±•äº†è¡¨ç¤ºèŒƒå›´ï¼Œç›¸è¾ƒäºFP16ï¼Œå‡å°‘äº†ä¸‹æº¢å’Œæº¢å‡ºçš„é£é™©ã€‚å°½ç®¡ç”±äºå°¾æ•°ä½æ•°å‡å°‘è€Œç²¾åº¦é™ä½ï¼Œä½†BF16é€šå¸¸ä¸ä¼šæ˜¾è‘—å½±å“æ¨¡å‹æ€§èƒ½ï¼Œæ˜¯æ·±åº¦å­¦ä¹ ä»»åŠ¡çš„ä¸€ä¸ªæœ‰ç”¨æŠ˜ä¸­æ–¹æ¡ˆã€‚

![](../Images/7f97ce565ee75a23253ee19eeff714bd.png)

ä½œè€…æä¾›çš„å›¾ç‰‡

åœ¨æœºå™¨å­¦ä¹ æœ¯è¯­ä¸­ï¼ŒFP32é€šå¸¸è¢«ç§°ä¸ºâ€œå…¨ç²¾åº¦â€ï¼ˆ4å­—èŠ‚ï¼‰ï¼Œè€ŒBF16å’ŒFP16åˆ™è¢«ç§°ä¸ºâ€œåŠç²¾åº¦â€ï¼ˆ2å­—èŠ‚ï¼‰ã€‚ä½†æˆ‘ä»¬æ˜¯å¦å¯ä»¥æ›´è¿›ä¸€æ­¥ï¼Œç”¨ä¸€ä¸ªå­—èŠ‚æ¥å­˜å‚¨æƒé‡ï¼Ÿç­”æ¡ˆæ˜¯INT8æ•°æ®ç±»å‹ï¼Œå®ƒç”±ä¸€ä¸ª8ä½è¡¨ç¤ºç»„æˆï¼Œèƒ½å¤Ÿå­˜å‚¨2â¸ = 256ç§ä¸åŒçš„å€¼ã€‚åœ¨ä¸‹ä¸€èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†çœ‹åˆ°å¦‚ä½•å°†FP32æƒé‡è½¬æ¢ä¸ºINT8æ ¼å¼ã€‚

# ğŸ”° åˆæ­¥çš„8ä½é‡åŒ–

åœ¨è¿™ä¸€éƒ¨åˆ†ï¼Œæˆ‘ä»¬å°†å®ç°ä¸¤ç§é‡åŒ–æŠ€æœ¯ï¼šä¸€ç§æ˜¯å…·æœ‰**ç»å¯¹æœ€å¤§å€¼ï¼ˆabsmaxï¼‰é‡åŒ–**çš„å¯¹ç§°é‡åŒ–ï¼Œå¦ä¸€ç§æ˜¯å…·æœ‰**é›¶ç‚¹é‡åŒ–**çš„éå¯¹ç§°é‡åŒ–ã€‚åœ¨è¿™ä¸¤ç§æƒ…å†µä¸‹ï¼Œç›®æ ‡æ˜¯å°†FP32å¼ é‡**X**ï¼ˆåŸå§‹æƒé‡ï¼‰æ˜ å°„åˆ°INT8å¼ é‡**X_quant**ï¼ˆé‡åŒ–æƒé‡ï¼‰ã€‚

ä½¿ç”¨**absmaxé‡åŒ–**ï¼ŒåŸå§‹æ•°å€¼é™¤ä»¥å¼ é‡çš„ç»å¯¹æœ€å¤§å€¼ï¼Œå¹¶ä¹˜ä»¥ç¼©æ”¾å› å­ï¼ˆ127ï¼‰ï¼Œå°†è¾“å…¥æ˜ å°„åˆ°èŒƒå›´[-127, 127]ã€‚è¦æ£€ç´¢åŸå§‹FP16å€¼ï¼ŒINT8æ•°å€¼é™¤ä»¥é‡åŒ–å› å­ï¼Œæ‰¿è®¤ç”±äºå››èˆäº”å…¥å¯¼è‡´çš„ä¸€äº›ç²¾åº¦æŸå¤±ã€‚

![](../Images/7dfd58e4be8f1412c262188ca9eecb4f.png)

ä¾‹å¦‚ï¼Œå‡è®¾æˆ‘ä»¬æœ‰ä¸€ä¸ªç»å¯¹æœ€å¤§å€¼ä¸º3.2çš„å€¼ã€‚ä¸€ä¸ª0.1çš„æƒé‡å°†è¢«é‡åŒ–ä¸º*round(0.1 Ã— 127/3.2) = 4*ã€‚å¦‚æœæˆ‘ä»¬æƒ³å°†å…¶åé‡åŒ–ï¼Œå°†å¾—åˆ°*4 Ã— 3.2/127 = 0.1008*ï¼Œè¿™æ„å‘³ç€ä¸€ä¸ª0.008çš„è¯¯å·®ã€‚ä»¥ä¸‹æ˜¯å¯¹åº”çš„Pythonå®ç°ï¼š

```py
import torch

def absmax_quantize(X):
    # Calculate scale
    scale = 127 / torch.max(torch.abs(X))

    # Quantize
    X_quant = (scale * X).round()

    # Dequantize
    X_dequant = X_quant / scale

    return X_quant.to(torch.int8), X_dequant
```

ä½¿ç”¨**é›¶ç‚¹é‡åŒ–**ï¼Œæˆ‘ä»¬å¯ä»¥è€ƒè™‘éå¯¹ç§°è¾“å…¥åˆ†å¸ƒï¼Œè¿™åœ¨è€ƒè™‘ReLUå‡½æ•°çš„è¾“å‡ºï¼ˆä»…æ­£å€¼ï¼‰æ—¶ç‰¹åˆ«æœ‰ç”¨ã€‚è¾“å…¥å€¼é¦–å…ˆæŒ‰å€¼çš„æ€»èŒƒå›´ï¼ˆ255ï¼‰é™¤ä»¥æœ€å¤§å€¼å’Œæœ€å°å€¼ä¹‹é—´çš„å·®å¼‚è¿›è¡Œç¼©æ”¾ã€‚ç„¶åé€šè¿‡é›¶ç‚¹åç§»å°†è¿™ä¸ªåˆ†å¸ƒæ˜ å°„åˆ°èŒƒå›´[-128, 127]ï¼ˆæ³¨æ„ä¸absmaxç›¸æ¯”çš„é¢å¤–å€¼ï¼‰ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬è®¡ç®—ç¼©æ”¾å› å­å’Œé›¶ç‚¹å€¼ï¼š

![](../Images/8553578f6e6bc10ce524fcf7ec3c4088.png)

ç„¶åï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨è¿™äº›å˜é‡æ¥é‡åŒ–æˆ–åé‡åŒ–æˆ‘ä»¬çš„æƒé‡ï¼š

![](../Images/bd1fff48751ed91f96d614a7b1fdced9.png)

ä¾‹å¦‚ï¼šæˆ‘ä»¬æœ‰ä¸€ä¸ªæœ€å¤§å€¼ä¸º3.2å’Œä¸€ä¸ªæœ€å°å€¼ä¸º-3.0ã€‚æˆ‘ä»¬å¯ä»¥è®¡ç®—å‡ºç¼©æ”¾å› å­ä¸º*255/(3.2 + 3.0) = 41.13*ï¼Œé›¶ç‚¹ä¸º*-round(41.13 Ã— -3.0) - 128 = 123 -128 = -5*ï¼Œå› æ­¤æˆ‘ä»¬ä¹‹å‰çš„æƒé‡0.1å°†è¢«é‡åŒ–ä¸º*round(41.13 Ã— 0.1 -5) = -1*ã€‚è¿™ä¸ä¹‹å‰ä½¿ç”¨absmaxè·å¾—çš„å€¼ï¼ˆ4ä¸-1ï¼‰å·®å¼‚å¾ˆå¤§ã€‚

![](../Images/773e8d8dbc9ac7c43c8d7f0fbfbe5fda.png)

ä½œè€…æä¾›çš„å›¾ç‰‡

Python å®ç°éå¸¸ç®€å•ï¼š

```py
def zeropoint_quantize(X):
    # Calculate value range (denominator)
    x_range = torch.max(X) - torch.min(X)
    x_range = 1 if x_range == 0 else x_range

    # Calculate scale
    scale = 255 / x_range

    # Shift by zero-point
    zeropoint = (-scale * torch.min(X) - 128).round()

    # Scale and round the inputs
    X_quant = torch.clip((X * scale + zeropoint).round(), -128, 127)

    # Dequantize
    X_dequant = (X_quant - zeropoint) / scale

    return X_quant.to(torch.int8), X_dequant
```

æˆ‘ä»¬å¯ä»¥åˆ©ç”¨ `transformers` åº“åœ¨çœŸå®æ¨¡å‹ä¸Šä½¿ç”¨è¿™ä¸¤ä¸ªå‡½æ•°ï¼Œè€Œä¸æ˜¯ä¾èµ–å®Œæ•´çš„ç©å…·ç¤ºä¾‹ã€‚

æˆ‘ä»¬é¦–å…ˆåŠ è½½ GPT-2 çš„æ¨¡å‹å’Œåˆ†è¯å™¨ã€‚è¿™æ˜¯ä¸€ä¸ªéå¸¸å°çš„æ¨¡å‹ï¼Œæˆ‘ä»¬å¯èƒ½ä¸æƒ³å¯¹å…¶è¿›è¡Œé‡åŒ–ï¼Œä½†å®ƒå¯¹äºæœ¬æ•™ç¨‹æ¥è¯´è¶³å¤Ÿäº†ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬æƒ³è§‚å¯Ÿæ¨¡å‹çš„å¤§å°ï¼Œä»¥ä¾¿ç¨åè¿›è¡Œæ¯”è¾ƒï¼Œå¹¶è¯„ä¼°ç”±äº 8 ä½é‡åŒ–è€ŒèŠ‚çœçš„**å†…å­˜**ã€‚

```py
!pip install -q bitsandbytes>=0.39.0
!pip install -q git+https://github.com/huggingface/accelerate.git
!pip install -q git+https://github.com/huggingface/transformers.git
```

```py
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch
torch.manual_seed(0)

# Set device to CPU for now
device = 'cpu'

# Load model and tokenizer
model_id = 'gpt2'
model = AutoModelForCausalLM.from_pretrained(model_id).to(device)
tokenizer = AutoTokenizer.from_pretrained(model_id)

# Print model size
print(f"Model size: {model.get_memory_footprint():,} bytes")
```

```py
Model size: 510,342,192 bytes
```

GPT-2 æ¨¡å‹åœ¨ FP32 ä¸­çš„å¤§å°çº¦ä¸º 487MBã€‚ä¸‹ä¸€æ­¥æ˜¯ä½¿ç”¨é›¶ç‚¹å’Œ absmax é‡åŒ–æƒé‡ã€‚åœ¨ä»¥ä¸‹ç¤ºä¾‹ä¸­ï¼Œæˆ‘ä»¬å°†è¿™äº›æŠ€æœ¯åº”ç”¨äº GPT-2 çš„ç¬¬ä¸€ä¸ªæ³¨æ„åŠ›å±‚ï¼Œä»¥æŸ¥çœ‹ç»“æœã€‚

```py
# Extract weights of the first layer
weights = model.transformer.h[0].attn.c_attn.weight.data
print("Original weights:")
print(weights)

# Quantize layer using absmax quantization
weights_abs_quant, _ = absmax_quantize(weights)
print("\nAbsmax quantized weights:")
print(weights_abs_quant)

# Quantize layer using absmax quantization
weights_zp_quant, _ = zeropoint_quantize(weights)
print("\nZero-point quantized weights:")
print(weights_zp_quant)
```

```py
Original weights:
tensor([[-0.4738, -0.2614, -0.0978,  ...,  0.0513, -0.0584,  0.0250],
        [ 0.0874,  0.1473,  0.2387,  ..., -0.0525, -0.0113, -0.0156],
        [ 0.0039,  0.0695,  0.3668,  ...,  0.1143,  0.0363, -0.0318],
        ...,
        [-0.2592, -0.0164,  0.1991,  ...,  0.0095, -0.0516,  0.0319],
        [ 0.1517,  0.2170,  0.1043,  ...,  0.0293, -0.0429, -0.0475],
        [-0.4100, -0.1924, -0.2400,  ..., -0.0046,  0.0070,  0.0198]])

Absmax quantized weights:
tensor([[-21, -12,  -4,  ...,   2,  -3,   1],
        [  4,   7,  11,  ...,  -2,  -1,  -1],
        [  0,   3,  16,  ...,   5,   2,  -1],
        ...,
        [-12,  -1,   9,  ...,   0,  -2,   1],
        [  7,  10,   5,  ...,   1,  -2,  -2],
        [-18,  -9, -11,  ...,   0,   0,   1]], dtype=torch.int8)

Zero-point quantized weights:
tensor([[-20, -11,  -3,  ...,   3,  -2,   2],
        [  5,   8,  12,  ...,  -1,   0,   0],
        [  1,   4,  18,  ...,   6,   3,   0],
        ...,
        [-11,   0,  10,  ...,   1,  -1,   2],
        [  8,  11,   6,  ...,   2,  -1,  -1],
        [-18,  -8, -10,  ...,   1,   1,   2]], dtype=torch.int8)
```

åŸå§‹ï¼ˆFP32ï¼‰å€¼å’Œé‡åŒ–å€¼ï¼ˆINT8ï¼‰ä¹‹é—´çš„å·®å¼‚å¾ˆæ˜æ˜¾ï¼Œä½† absmax å’Œé›¶ç‚¹æƒé‡ä¹‹é—´çš„å·®å¼‚åˆ™æ›´å¾®å¦™ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œè¾“å…¥å€¼çœ‹èµ·æ¥åç§»äº† -1ã€‚è¿™è¡¨æ˜è¿™ä¸€å±‚çš„æƒé‡åˆ†å¸ƒç›¸å½“å¯¹ç§°ã€‚

æˆ‘ä»¬å¯ä»¥é€šè¿‡å¯¹ GPT-2 ä¸­çš„æ¯ä¸€å±‚ï¼ˆçº¿æ€§å±‚ã€æ³¨æ„åŠ›å±‚ç­‰ï¼‰è¿›è¡Œé‡åŒ–ï¼Œå¹¶åˆ›å»ºä¸¤ä¸ªæ–°æ¨¡å‹ï¼š`model_abs` å’Œ `model_zp`ï¼Œæ¥æ¯”è¾ƒè¿™äº›æŠ€æœ¯ã€‚å‡†ç¡®åœ°è¯´ï¼Œæˆ‘ä»¬å°†å®é™…ç”¨***å»***é‡åŒ–çš„æƒé‡æ›¿æ¢åŸå§‹æƒé‡ã€‚è¿™æœ‰ä¸¤ä¸ªå¥½å¤„ï¼šå®ƒä½¿æˆ‘ä»¬å¯ä»¥ 1/ æ¯”è¾ƒæƒé‡çš„åˆ†å¸ƒï¼ˆç›¸åŒçš„å°ºåº¦ï¼‰ï¼Œä»¥åŠ 2/ å®é™…è¿è¡Œè¿™äº›æ¨¡å‹ã€‚

ç¡®å®ï¼ŒPyTorch é»˜è®¤ä¸å…è®¸ INT8 çŸ©é˜µä¹˜æ³•ã€‚åœ¨å®é™…åœºæ™¯ä¸­ï¼Œæˆ‘ä»¬ä¼šå°†å…¶å»é‡åŒ–ä»¥è¿è¡Œæ¨¡å‹ï¼ˆä¾‹å¦‚ FP16ï¼‰ï¼Œä½†ä»¥ INT8 å­˜å‚¨ã€‚åœ¨ä¸‹ä¸€éƒ¨åˆ†ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ `[bitsandbytes](https://github.com/TimDettmers/bitsandbytes)` åº“æ¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚

```py
import numpy as np
from copy import deepcopy

# Store original weights
weights = [param.data.clone() for param in model.parameters()]

# Create model to quantize
model_abs = deepcopy(model)

# Quantize all model weights
weights_abs = []
for param in model_abs.parameters():
    _, dequantized = absmax_quantize(param.data)
    param.data = dequantized
    weights_abs.append(dequantized)

# Create model to quantize
model_zp = deepcopy(model)

# Quantize all model weights
weights_zp = []
for param in model_zp.parameters():
    _, dequantized = zeropoint_quantize(param.data)
    param.data = dequantized
    weights_zp.append(dequantized)
```

ç°åœ¨æˆ‘ä»¬çš„æ¨¡å‹å·²ç»è¢«é‡åŒ–ï¼Œæˆ‘ä»¬æƒ³æ£€æŸ¥è¿™ä¸€è¿‡ç¨‹çš„å½±å“ã€‚ç›´è§‚ä¸Šï¼Œæˆ‘ä»¬æƒ³ç¡®ä¿é‡åŒ–çš„æƒé‡**æ¥è¿‘åŸå§‹æƒé‡**ã€‚ä¸€ç§ç›´è§‚çš„æ–¹æ³•æ˜¯ç»˜åˆ¶å»é‡åŒ–å’ŒåŸå§‹æƒé‡çš„åˆ†å¸ƒã€‚å¦‚æœé‡åŒ–æœ‰æŸï¼Œå®ƒå°†æå¤§åœ°æ”¹å˜æƒé‡åˆ†å¸ƒã€‚

ä¸‹å›¾å±•ç¤ºäº†è¿™ç§æ¯”è¾ƒï¼Œå…¶ä¸­è“è‰²ç›´æ–¹å›¾è¡¨ç¤ºåŸå§‹ï¼ˆFP32ï¼‰æƒé‡ï¼Œçº¢è‰²ç›´æ–¹å›¾è¡¨ç¤ºå»é‡åŒ–åçš„ï¼ˆæ¥è‡ª INT8ï¼‰æƒé‡ã€‚æ³¨æ„ï¼Œæˆ‘ä»¬ä»…åœ¨ -2 å’Œ 2 ä¹‹é—´æ˜¾ç¤ºæ­¤å›¾ï¼Œå› ä¸ºæœ‰ä¸€äº›ç»å¯¹å€¼éå¸¸é«˜çš„ç¦»ç¾¤å€¼ï¼ˆç¨åä¼šè¯¦ç»†è®¨è®ºï¼‰ã€‚

![](../Images/9e93b5636f8e65de4bb386d68ad67b61.png)

ä¸¤ä¸ªå›¾è¡¨éå¸¸ç›¸ä¼¼ï¼Œéƒ½åœ¨ 0 é™„è¿‘æœ‰ä¸€ä¸ªæƒŠäººçš„å³°å€¼ã€‚è¿™ä¸ªå³°å€¼è¡¨æ˜æˆ‘ä»¬çš„é‡åŒ–ç›¸å½“æœ‰æŸï¼Œå› ä¸ºåè½¬è¿‡ç¨‹å¹¶æ²¡æœ‰è¾“å‡ºåŸå§‹å€¼ã€‚è¿™åœ¨ absmax æ¨¡å‹ä¸­å°¤ä¸ºæ˜æ˜¾ï¼Œè¯¥æ¨¡å‹åœ¨ 0 é™„è¿‘æ˜¾ç¤ºäº†ä¸€ä¸ªæ›´ä½çš„è°·å€¼å’Œä¸€ä¸ªæ›´é«˜çš„å³°å€¼ã€‚

è®©æˆ‘ä»¬æ¯”è¾ƒåŸå§‹æ¨¡å‹å’Œé‡åŒ–æ¨¡å‹çš„æ€§èƒ½ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å®šä¹‰äº†ä¸€ä¸ª `generate_text()` å‡½æ•°æ¥ç”Ÿæˆ 50 ä¸ªæ ‡è®°ï¼Œå¹¶ä½¿ç”¨ [top-k é‡‡æ ·](https://mlabonne.github.io/blog/posts/2023-06-07-Decoding_strategies.html)ã€‚

```py
def generate_text(model, input_text, max_length=50):
    input_ids = tokenizer.encode(input_text, return_tensors='pt').to(device)
    output = model.generate(inputs=input_ids,
                            max_length=max_length,
                            do_sample=True,
                            top_k=30,
                            pad_token_id=tokenizer.eos_token_id,
                            attention_mask=input_ids.new_ones(input_ids.shape))
    return tokenizer.decode(output[0], skip_special_tokens=True)

# Generate text with original and quantized models
original_text = generate_text(model, "I have a dream")
absmax_text   = generate_text(model_abs, "I have a dream")
zp_text       = generate_text(model_zp, "I have a dream")

print(f"Original model:\n{original_text}")
print("-" * 50)
print(f"Absmax model:\n{absmax_text}")
print("-" * 50)
print(f"Zeropoint model:\n{zp_text}")
```

```py
Original model:
I have a dream, and it is a dream I believe I would get to live in my future. I love my mother, and there was that one time I had been told that my family wasn't even that strong. And then I got the
--------------------------------------------------
Absmax model:
I have a dream to find out the origin of her hair. She loves it. But there's no way you could be honest about how her hair is made. She must be crazy.

We found a photo of the hairstyle posted on
--------------------------------------------------
Zeropoint model:
I have a dream of creating two full-time jobs in Americaâ€”one for people with mental health issues, and one for people who do not suffer from mental illnessâ€”or at least have an employment and family history of substance abuse, to work part
```

æˆ‘ä»¬å¯ä»¥é€šè¿‡è®¡ç®—æ¯ä¸ªè¾“å‡ºçš„**å›°æƒ‘åº¦**æ¥é‡åŒ–å®ƒï¼Œè€Œä¸æ˜¯å°è¯•åˆ¤æ–­ä¸€ä¸ªè¾“å‡ºæ˜¯å¦æ¯”å…¶ä»–è¾“å‡ºæ›´æœ‰æ„ä¹‰ã€‚è¿™æ˜¯ä¸€ä¸ªå¸¸ç”¨çš„è¯„ä¼°è¯­è¨€æ¨¡å‹çš„æŒ‡æ ‡ï¼Œç”¨äºæµ‹é‡æ¨¡å‹åœ¨é¢„æµ‹åºåˆ—ä¸­ä¸‹ä¸€ä¸ªæ ‡è®°æ—¶çš„ä¸ç¡®å®šæ€§ã€‚åœ¨è¿™ä¸ªæ¯”è¾ƒä¸­ï¼Œæˆ‘ä»¬åšäº†ä¸€ä¸ªå¸¸è§çš„å‡è®¾ï¼Œå³åˆ†æ•°è¶Šä½ï¼Œæ¨¡å‹è¶Šå¥½ã€‚å®é™…ä¸Šï¼Œé«˜å›°æƒ‘åº¦çš„å¥å­ä¹Ÿå¯èƒ½æ˜¯æ­£ç¡®çš„ã€‚

æˆ‘ä»¬ä½¿ç”¨ä¸€ä¸ªæœ€å°å‡½æ•°æ¥å®ç°å®ƒï¼Œå› ä¸ºä¸éœ€è¦è€ƒè™‘ä¸Šä¸‹æ–‡çª—å£çš„é•¿åº¦ï¼Œå› ä¸ºæˆ‘ä»¬çš„å¥å­è¾ƒçŸ­ã€‚

```py
def calculate_perplexity(model, text):
    # Encode the text
    encodings = tokenizer(text, return_tensors='pt').to(device)

    # Define input_ids and target_ids
    input_ids = encodings.input_ids
    target_ids = input_ids.clone()

    with torch.no_grad():
        outputs = model(input_ids, labels=target_ids)

    # Loss calculation
    neg_log_likelihood = outputs.loss

    # Perplexity calculation
    ppl = torch.exp(neg_log_likelihood)

    return ppl

ppl     = calculate_perplexity(model, original_text)
ppl_abs = calculate_perplexity(model_abs, absmax_text)
ppl_zp  = calculate_perplexity(model_zp, absmax_text)

print(f"Original perplexity:  {ppl.item():.2f}")
print(f"Absmax perplexity:    {ppl_abs.item():.2f}")
print(f"Zeropoint perplexity: {ppl_zp.item():.2f}")
```

```py
Original perplexity:  15.53
Absmax perplexity:    17.92
Zeropoint perplexity: 17.97
```

æˆ‘ä»¬å‘ç°åŸå§‹æ¨¡å‹çš„å›°æƒ‘åº¦**ç•¥ä½**äºå¦å¤–ä¸¤ä¸ªæ¨¡å‹ã€‚ä¸€æ¬¡å®éªŒå¹¶ä¸ååˆ†å¯é ï¼Œä½†æˆ‘ä»¬å¯ä»¥å¤šæ¬¡é‡å¤è¿™ä¸ªè¿‡ç¨‹ï¼Œä»¥æŸ¥çœ‹æ¯ä¸ªæ¨¡å‹ä¹‹é—´çš„å·®å¼‚ã€‚ä»ç†è®ºä¸Šè®²ï¼Œé›¶ç‚¹é‡åŒ–åº”ç¨å¾®ä¼˜äº absmaxï¼Œä½†è®¡ç®—æˆæœ¬ä¹Ÿæ›´é«˜ã€‚

åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œæˆ‘ä»¬å°†é‡åŒ–æŠ€æœ¯åº”ç”¨äºæ•´ä¸ªå±‚ï¼ˆæ¯å¼ é‡åŸºç¡€ï¼‰ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬å¯ä»¥åœ¨ä¸åŒçš„ç²’åº¦æ°´å¹³åº”ç”¨å®ƒï¼šä»æ•´ä¸ªæ¨¡å‹åˆ°å•ä¸ªå€¼ã€‚ä¸€æ¬¡æ€§é‡åŒ–æ•´ä¸ªæ¨¡å‹ä¼šä¸¥é‡é™ä½æ€§èƒ½ï¼Œè€Œé‡åŒ–å•ä¸ªå€¼ä¼šäº§ç”Ÿå¾ˆå¤§çš„å¼€é”€ã€‚åœ¨å®è·µä¸­ï¼Œæˆ‘ä»¬é€šå¸¸æ›´å–œæ¬¢**é€å‘é‡é‡åŒ–**ï¼Œå®ƒè€ƒè™‘äº†åŒä¸€å¼ é‡ä¸­è¡Œå’Œåˆ—çš„å€¼çš„å˜å¼‚æ€§ã€‚

ç„¶è€Œï¼Œå³ä½¿æ˜¯é€å‘é‡é‡åŒ–ä¹Ÿä¸èƒ½è§£å†³ç¦»ç¾¤ç‰¹å¾çš„é—®é¢˜ã€‚ç¦»ç¾¤ç‰¹å¾æ˜¯å‡ºç°åœ¨æ‰€æœ‰å˜æ¢å™¨å±‚ä¸­çš„æç«¯å€¼ï¼ˆè´Ÿå€¼æˆ–æ­£å€¼ï¼‰ï¼Œå½“æ¨¡å‹è¾¾åˆ°æŸä¸ªè§„æ¨¡ï¼ˆ>6.7B å‚æ•°ï¼‰æ—¶ã€‚è¿™æ˜¯ä¸€ä¸ªé—®é¢˜ï¼Œå› ä¸ºä¸€ä¸ªç¦»ç¾¤å€¼å¯ä»¥é™ä½æ‰€æœ‰å…¶ä»–å€¼çš„ç²¾åº¦ã€‚ä½†ä¸¢å¼ƒè¿™äº›ç¦»ç¾¤ç‰¹å¾å¹¶ä¸æ˜¯ä¸€ä¸ªé€‰é¡¹ï¼Œå› ä¸ºè¿™ä¼š**å¤§å¤§é™ä½**æ¨¡å‹çš„æ€§èƒ½ã€‚

# ğŸ”¢ 8 ä½é‡åŒ–ä¸ LLM.int8()

ç”±[Dettmers et al. (2022)](https://arxiv.org/abs/2208.07339)å¼•å…¥çš„LLM.int8() æ˜¯è§£å†³ç¦»ç¾¤å€¼é—®é¢˜çš„ä¸€ä¸ªæ–¹æ¡ˆã€‚å®ƒä¾èµ–äºé€å‘é‡ï¼ˆabsmaxï¼‰é‡åŒ–æ–¹æ¡ˆï¼Œå¹¶å¼•å…¥äº†æ··åˆç²¾åº¦é‡åŒ–ã€‚è¿™æ„å‘³ç€ç¦»ç¾¤ç‰¹å¾ä»¥ FP16 æ ¼å¼å¤„ç†ä»¥ä¿æŒå…¶ç²¾åº¦ï¼Œè€Œå…¶ä»–å€¼åˆ™ä»¥ INT8 æ ¼å¼å¤„ç†ã€‚ç”±äºç¦»ç¾¤å€¼çº¦å  0.1% çš„å€¼ï¼Œè¿™æœ‰æ•ˆåœ°å°† LLM çš„å†…å­˜å ç”¨å‡å°‘äº†è¿‘ 2 å€ã€‚

![](../Images/f7ce7de0c94eb22aaf2c926341e6f2ff.png)

ä½œè€…æä¾›çš„å›¾ç‰‡

LLM.int8() é€šè¿‡ä¸‰ä¸ªå…³é”®æ­¥éª¤è¿›è¡ŒçŸ©é˜µä¹˜æ³•è®¡ç®—ï¼š

1.  ä»è¾“å…¥éšè—çŠ¶æ€**X**ä¸­æå–åŒ…å«ç¦»ç¾¤ç‰¹å¾çš„åˆ—ï¼Œä½¿ç”¨è‡ªå®šä¹‰é˜ˆå€¼ã€‚

1.  ä½¿ç”¨ FP16 å¯¹ç¦»ç¾¤å€¼è¿›è¡ŒçŸ©é˜µä¹˜æ³•ï¼Œå¹¶ä½¿ç”¨ INT8 è¿›è¡Œé€å‘é‡é‡åŒ–ï¼ˆå¯¹éšè—çŠ¶æ€**X**è¿›è¡Œé€è¡Œé‡åŒ–ï¼Œå¯¹æƒé‡çŸ©é˜µ**W**è¿›è¡Œé€åˆ—é‡åŒ–ï¼‰ã€‚

1.  å¯¹éç¦»ç¾¤ç»“æœè¿›è¡Œåé‡åŒ–ï¼ˆä» INT8 åˆ° FP16ï¼‰ï¼Œå¹¶å°†å…¶ä¸ç¦»ç¾¤ç»“æœç›¸åŠ ï¼Œä»¥è·å¾—å®Œæ•´çš„ FP16 ç»“æœã€‚

![](../Images/3b4bb0a164b5ebebc1dcaca4b96f34d7.png)

ä½œè€…æä¾›çš„å›¾ç‰‡

è¿™ç§æ–¹æ³•æ˜¯å¿…è¦çš„ï¼Œå› ä¸º 8 ä½ç²¾åº¦æœ‰é™ï¼Œå½“é‡åŒ–å…·æœ‰å¤§å€¼çš„å‘é‡æ—¶å¯èƒ½å¯¼è‡´æ˜¾è‘—çš„è¯¯å·®ã€‚è¿™äº›è¯¯å·®ä¹Ÿä¼šéšç€å®ƒä»¬åœ¨å¤šä¸ªå±‚ä¹‹é—´ä¼ æ’­è€Œæ”¾å¤§ã€‚

ç”±äº `bitsandbytes` åº“å·²é›†æˆåˆ° Hugging Face ç”Ÿæ€ç³»ç»Ÿä¸­ï¼Œæˆ‘ä»¬å¯ä»¥è½»æ¾ä½¿ç”¨è¿™é¡¹æŠ€æœ¯ã€‚æˆ‘ä»¬åªéœ€åœ¨åŠ è½½æ¨¡å‹æ—¶æŒ‡å®š `load_in_8bit=True`ï¼ˆå®ƒè¿˜éœ€è¦ä¸€ä¸ª GPUï¼‰ã€‚

```py
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

model_int8 = AutoModelForCausalLM.from_pretrained(model_id,
                                             device_map='auto',
                                             load_in_8bit=True,
                                             )
print(f"Model size: {model_int8.get_memory_footprint():,} bytes")
```

```py
Model size: 176,527,896 bytes
```

é€šè¿‡è¿™é¢å¤–çš„ä¸€è¡Œä»£ç ï¼Œæ¨¡å‹ç°åœ¨å‡ ä¹ç¼©å°äº†ä¸‰å€ï¼ˆ168MB vs. 487MBï¼‰ã€‚æˆ‘ä»¬ç”šè‡³å¯ä»¥åƒä¹‹å‰é‚£æ ·æ¯”è¾ƒåŸå§‹æƒé‡å’Œé‡åŒ–æƒé‡çš„åˆ†å¸ƒï¼š

![](../Images/b46d4868b6ce8ce99e4746e100fc1525.png)

åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬çœ‹åˆ°åœ¨ -2ï¼Œ-1ï¼Œ0ï¼Œ1ï¼Œ2 ç­‰å€¼å‘¨å›´æœ‰å°–å³°ã€‚è¿™äº›å€¼å¯¹åº”äºä»¥ INT8 æ ¼å¼å­˜å‚¨çš„å‚æ•°ï¼ˆéå¼‚å¸¸å€¼ï¼‰ã€‚ä½ å¯ä»¥é€šè¿‡ä½¿ç”¨ `model_int8.parameters()` æ‰“å°æ¨¡å‹çš„æƒé‡æ¥éªŒè¯è¿™ä¸€ç‚¹ã€‚

æˆ‘ä»¬è¿˜å¯ä»¥ä½¿ç”¨è¿™ä¸ªé‡åŒ–æ¨¡å‹ç”Ÿæˆæ–‡æœ¬ï¼Œå¹¶ä¸åŸå§‹æ¨¡å‹è¿›è¡Œæ¯”è¾ƒã€‚

```py
# Generate text with quantized model
text_int8 = generate_text(model_int8, "I have a dream")

print(f"Original model:\n{original_text}")
print("-" * 50)
print(f"LLM.int8() model:\n{text_int8}")
```

```py
Original model:
I have a dream, and it is a dream I believe I would get to live in my future. I love my mother, and there was that one time I had been told that my family wasn't even that strong. And then I got the
--------------------------------------------------
LLM.int8() model:
I have a dream. I don't know what will come of it, but I am going to have to look for something that will be right. I haven't thought about it for a long time, but I have to try to get that thing
```

å†æ¬¡å¼ºè°ƒï¼Œåˆ¤æ–­æœ€ä½³è¾“å‡ºæ˜¯å›°éš¾çš„ï¼Œä½†æˆ‘ä»¬å¯ä»¥ä¾é å›°æƒ‘åº¦æŒ‡æ ‡æ¥ç»™å‡ºä¸€ä¸ªï¼ˆå¤§è‡´çš„ï¼‰ç­”æ¡ˆã€‚

```py
print(f"Perplexity (original):   {ppl.item():.2f}")

ppl = calculate_perplexity(model_int8, text_int8)
print(f"Perplexity (LLM.int8()): {ppl.item():.2f}")
```

```py
Perplexity (original):   15.53
Perplexity (LLM.int8()): 7.93
```

åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œé‡åŒ–æ¨¡å‹çš„å›°æƒ‘åº¦æ˜¯åŸå§‹æ¨¡å‹çš„ä¸¤å€ä½ã€‚ä¸€èˆ¬æ¥è¯´ï¼Œè¿™ç§æƒ…å†µå¹¶ä¸å¸¸è§ï¼Œä½†å®ƒæ˜¾ç¤ºäº†è¿™ç§é‡åŒ–æŠ€æœ¯çš„ç«äº‰åŠ›ã€‚å®é™…ä¸Šï¼ŒLLM.int8()çš„ä½œè€…å±•ç¤ºäº†æ€§èƒ½ä¸‹é™éå¸¸å°ï¼Œå¯ä»¥å¿½ç•¥ä¸è®¡ï¼ˆ<1%ï¼‰ã€‚ç„¶è€Œï¼Œå®ƒåœ¨è®¡ç®—æ–¹é¢æœ‰é¢å¤–çš„æˆæœ¬ï¼šå¯¹äºå¤§æ¨¡å‹ï¼ŒLLM.int8()å¤§çº¦æ…¢äº† 20%ã€‚

# ç»“è®º

æœ¬æ–‡æ¦‚è¿°äº†æœ€æµè¡Œçš„æƒé‡é‡åŒ–æŠ€æœ¯ã€‚æˆ‘ä»¬é¦–å…ˆäº†è§£äº†æµ®ç‚¹è¡¨ç¤ºï¼Œç„¶åä»‹ç»äº†ä¸¤ç§ 8 ä½é‡åŒ–æŠ€æœ¯ï¼š**absmax** å’Œ **é›¶ç‚¹é‡åŒ–**ã€‚ç„¶è€Œï¼Œå®ƒä»¬çš„å±€é™æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†å¼‚å¸¸å€¼æ—¶ï¼Œå¯¼è‡´äº† **LLM.int8()** æŠ€æœ¯ï¼Œè¯¥æŠ€æœ¯è¿˜ä¿ç•™äº†æ¨¡å‹çš„æ€§èƒ½ã€‚è¿™ç§æ–¹æ³•çªæ˜¾äº†æƒé‡é‡åŒ–é¢†åŸŸçš„è¿›å±•ï¼Œæ­ç¤ºäº†æ­£ç¡®å¤„ç†å¼‚å¸¸å€¼çš„é‡è¦æ€§ã€‚

å±•æœ›æœªæ¥ï¼Œæˆ‘ä»¬çš„ä¸‹ä¸€ç¯‡æ–‡ç« å°†æ·±å…¥æ¢è®¨ GPTQ æƒé‡é‡åŒ–æŠ€æœ¯ã€‚è¿™é¡¹æŠ€æœ¯ç”± [Frantar ç­‰äºº](https://arxiv.org/abs/2210.17323) æå‡ºï¼Œåªä½¿ç”¨äº† 4 ä½ï¼Œå¹¶ä¸”ä»£è¡¨äº†æƒé‡é‡åŒ–é¢†åŸŸçš„é‡å¤§è¿›å±•ã€‚æˆ‘ä»¬å°†æä¾›å¦‚ä½•ä½¿ç”¨ AutoGPTQ åº“å®ç° GPTQ çš„å…¨é¢æŒ‡å—ã€‚

å¦‚æœä½ å¯¹æ›´å¤šå…³äºLLMçš„æŠ€æœ¯å†…å®¹æ„Ÿå…´è¶£ï¼Œå¯ä»¥åœ¨Twitterä¸Šå…³æ³¨æˆ‘ [@maximelabonne](https://twitter.com/maximelabonne)ã€‚

# å‚è€ƒæ–‡çŒ®

+   T. Dettmers, M. Lewis, Y. Belkada, å’Œ L. Zettlemoyerï¼Œ[LLM.int8(): å¤§è§„æ¨¡å˜å‹å™¨çš„ 8 ä½çŸ©é˜µä¹˜æ³•](https://arxiv.org/abs/2208.07339)ã€‚2022å¹´ã€‚

+   Y. Beldaka å’Œ T. Dettmers, [8ä½çŸ©é˜µä¹˜æ³•çš„ç®€æ˜ä»‹ç»](https://huggingface.co/blog/hf-bitsandbytes-integration), Hugging Face Blog (2022)ã€‚

+   A. Gholami, S. Kim, Z. Dong, Z. Yao, M. W. Mahoney å’Œ K. Keutzer, [é«˜æ•ˆç¥ç»ç½‘ç»œæ¨ç†çš„é‡åŒ–æ–¹æ³•ç»¼è¿°](https://arxiv.org/abs/2103.13630)ã€‚2021å¹´ã€‚

+   H. Wu, P. Judd, X. Zhang, M. Isaev å’Œ P. Micikevicius, [æ·±åº¦å­¦ä¹ æ¨ç†çš„æ•´æ•°é‡åŒ–ï¼šåŸç†ä¸å®è¯è¯„ä¼°](https://arxiv.org/abs/2004.09602)ã€‚2020å¹´ã€‚

+   Lilian Weng, [å¤§å‹å˜å‹å™¨æ¨¡å‹æ¨ç†ä¼˜åŒ–](https://lilianweng.github.io/posts/2023-01-10-inference-optimization/), Lilâ€™Log (2023)ã€‚

+   Kamil Czarnogorski, [æœ¬åœ°å¤§å‹è¯­è¨€æ¨¡å‹](https://int8.io/local-large-language-models-beginners-guide/), Int8 (2023)ã€‚
