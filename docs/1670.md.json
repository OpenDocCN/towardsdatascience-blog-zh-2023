["```py\nscaler = StandardScaler()\nscaler.fit(X_train)\nscaler.transform(X_train)\nscaler.transform(X_test)\n```", "```py\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.pipeline import make_pipeline\nfrom feature_engine.encoding import OneHotEncoder\nfrom preprocfunc import OutlierTrans #self-created Python class\n\nstandardtrans = make_pipeline(OutlierTrans(2), \n                              StandardScaler()\n                             )\n\ncategoricaltrans = make_pipeline(SimpleImputer(strategy=\"most_frequent\"), \n                                 OneHotEncoder(drop_last=True)\n                                )\n\nbinarytrans = make_pipeline(SimpleImputer(strategy=\"most_frequent\")\n                           )\n\ncolumntrans = ColumnTransformer(transformers=[\n    (\"standard\", standardtrans, numerical_cols),\n    (\"categorical\", categoricaltrans, ['gender']),\n    (\"binary\", binarytrans, ['completedba'])\n])\n\nlr = LinearRegression()\npipe = make_pipeline(columntrans, KNNImputer(n_neighbors=5), lr)\n```", "```py\nfrom sklearn.model_selection import cross_validate, KFold\n\nttr = TransformedTargetRegressor(regressor=pipe, transformer=StandardScaler())\n\nkf = KFold(n_splits=5, shuffle=True, random_state=0)\nscores = cross_validate(ttr, \n                        X=X_train, \n                        y=y_train, \n                        cv=kf, \n                        scoring=('r2', 'neg_mean_absolute_error'), \n                        n_jobs=1) \n```", "```py\nimport numpy as np\nimport pandas as pd\nfrom sklearn.impute import SimpleImputer\n\ndf = pd.read_csv(\"../dataset/titanic/csv_result-phpMYEkMl.csv\")\n\n#Change column names, replace \"?\" to \"NaN\", change data types\ndef tweak_df(df):\n    features = [\"PassengerId\", \"Survived\", \"Pclass\", \"Name\", \"Sex\", \"Age\", \"SibSp\", \"Parch\", \"Ticket\", \"Fare\", \"Cabin\", \"Embarked\"]\n    return (df\n            .rename(columns={\"id\": \"PassengerId\", \"'pclass'\": \"Pclass\", \"'survived'\": \"Survived\", \"'name'\": \"Name\", \"'sex'\": \"Sex\", \"'age'\": \"Age\", \"'sibsp'\": \"SibSp\", \"'parch'\": \"Parch\", \"'ticket'\": \"Ticket\", \"'fare'\": \"Fare\", \"'cabin'\": \"Cabin\", \"'embarked'\": \"Embarked\"})\n            [features]\n            .replace('?', np.nan)\n            .astype({'Age': 'float', 'Fare': 'float16'})\n           )\n\n#Splitting dataset into train, validation, test, and unseen\nX_train_val_test, X_unseen, y_train_val_test, y_unseen = train_test_split(tweak_df(df).drop(columns=['Survived']), tweak_df(df).Survived, test_size=0.33, random_state=42)\nX_train, X_val_test, y_train, y_val_test = train_test_split(X_train_val_test, y_train_val_test, test_size=0.4, random_state=42)\nX_val, X_test, y_val, y_test = train_test_split(X_val_test, y_val_test, test_size=0.5, random_state=42)\n\n#Extensive cleanup\ndef tweak_titanic_cleaned(train_df):\n\n    impute_table = (train_df\n                     .assign(SibSp=lambda df_: np.where(df_.SibSp==0, 0, 1),\n                             Parch=lambda df_: np.where(df_.Parch==0, 0, 1))\n                     .groupby(['SibSp', 'Parch'])\n                     ['Age']\n                     .agg('mean')\n                   )\n\n    train_df_intermediary = (train_df\n                             .assign(SibSp=lambda df_: np.where(df_.SibSp==0, 0, 1),\n                                     Parch=lambda df_: np.where(df_.Parch==0, 0, 1),)\n                            )\n\n    condlist = [((train_df_intermediary.Age.isna()) & (train_df_intermediary.SibSp == 0) & (train_df_intermediary.Parch == 0)),\n                ((train_df_intermediary.Age.isna()) & (train_df_intermediary.SibSp == 0) & (train_df_intermediary.Parch == 1)),\n                ((train_df_intermediary.Age.isna()) & (train_df_intermediary.SibSp == 1) & (train_df_intermediary.Parch == 0)),\n                ((train_df_intermediary.Age.isna()) & (train_df_intermediary.SibSp == 1) & (train_df_intermediary.Parch == 1)),]\n\n    choicelist = [impute_table.iloc[0],\n                  impute_table.iloc[1],\n                  impute_table.iloc[2],\n                  impute_table.iloc[3],]\n\n    bins = [0, 12, 18, 30, 50, 100]\n    labels = ['Child', 'Teenager', 'Young Adult', 'Adult', 'Senior']\n    features = [\"Survived\", \"Pclass\",\"Sex\",\"Fare\",\"Embarked\",\"AgeGroup\",\"SibSp\",\"Parch\",\"IsAlone\",\"Title\"]\n\n    return (train_df\n             .assign(Embarked=lambda df_: SimpleImputer(strategy=\"most_frequent\").fit_transform(df_.Embarked.values.reshape(-1,1)),\n                     Age=lambda df_: np.select(condlist, choicelist, df_.Age),\n                     IsAlone=lambda df_: np.where(df_.SibSp + df_.Parch > 0, 0, 1),\n                     Title=lambda df_: df_.Name.str.extract(',(.*?)\\.'))\n             .assign(AgeGroup=lambda df_: pd.cut(df_.Age, bins=bins, labels=labels),\n                     Title=lambda df_: df_.Title.replace(['Dr', 'Rev', 'Major', 'Col', 'Capt', 'Sir', 'Lady', 'Don', 'Jonkheer', 'Countess', 'Mme', 'Ms', 'Mlle','the Countess'], \n                                                         'Other'))\n             .set_index(\"PassengerId\")\n             [features]\n            )\n```", "```py\n#Intentionally add target variable to list of features\nX_train = tweak_titanic_cleaned(pd\n .concat([X_train, pd.DataFrame(y_train)], axis=1))\n\nX_val = tweak_titanic_cleaned(pd\n .concat([X_val, pd.DataFrame(y_val)], axis=1))\n\nX_test = tweak_titanic_cleaned(pd\n .concat([X_test, pd.DataFrame(y_test)], axis=1))\n\n# Prepare the training data\nX_train = pd.get_dummies(X_train, columns=[\"Survived\", \"Pclass\", \"Sex\", \"Embarked\", \"AgeGroup\", \"IsAlone\", \"Title\"], drop_first=True)\nX_val = pd.get_dummies(X_val, columns=[\"Survived\", \"Pclass\", \"Sex\", \"Embarked\", \"AgeGroup\", \"IsAlone\", \"Title\"], drop_first=True)\n\n# Scale numerical columns\nscaler = MinMaxScaler()\nnum_cols = [\"Fare\",\"SibSp\",\"Parch\"]\nX_train[num_cols] = scaler.fit_transform(X_train[num_cols])\nX_val[num_cols] = scaler.transform(X_val[num_cols])\n\n# Fit and evaluate Logistic Regression model\nlr_model = LogisticRegression(random_state=0)\nlr_model.fit(X_train, y_train)\n\n# Make predictions on validation data\ny_pred_val = lr_model.predict(X_val)\n\n# Evaluate model on validation data\nacc_val = round(accuracy_score(y_val, y_pred_val) * 100, 2)\nprint(\"Logistic Regression Model accuracy on validation data:\", acc_val)\n```", "```py\nX_train = (pd\n           .concat([tweak_titanic_cleaned(X_train), \n                    tweak_titanic_cleaned(X_val).iloc[:150, :]])\n          )\ny_train = (pd\n           .concat([y_train, \n                    y_val.iloc[:150]])\n          )\n\nX_val = tweak_titanic_cleaned(X_val)\n\n# Prepare the training data\nX_train = pd.get_dummies(X_train, columns=[\"Pclass\", \"Sex\", \"Embarked\", \"AgeGroup\", \"IsAlone\", \"Title\"], drop_first=True)\nX_val = pd.get_dummies(X_val, columns=[\"Pclass\", \"Sex\", \"Embarked\", \"AgeGroup\", \"IsAlone\", \"Title\"], drop_first=True)\n\n# Scale numerical columns\nscaler = MinMaxScaler()\nnum_cols = [\"Fare\",\"SibSp\",\"Parch\"]\nX_train[num_cols] = scaler.fit_transform(X_train[num_cols])\nX_val[num_cols] = scaler.transform(X_val[num_cols])\n\n# Fit and evaluate Logistic Regression model\nlr_model = LogisticRegression(random_state=0)\nlr_model.fit(X_train, y_train)\n\n# Make predictions on validation data\ny_pred_val = lr_model.predict(X_val)\n\n# Evaluate model on validation data\nacc_val = round(accuracy_score(y_val, y_pred_val) * 100, 2)\nprint(\"Logistic Regression Model accuracy on validation data:\", acc_val)\n```", "```py\n# Prepare the training data\ndf_leaked = pd.get_dummies(tweak_titanic_cleaned(X_train_val_test), columns=[\"Pclass\", \"Sex\", \"Embarked\", \"AgeGroup\", \"IsAlone\", \"Title\"], drop_first=True)\n\n# Scale numerical columns\nscaler = MinMaxScaler()\nnum_cols = [\"Fare\",\"SibSp\",\"Parch\"]\ndf_leaked[num_cols] = scaler.fit_transform(df_leaked[num_cols])\n\n# Split the data into train, validation, and test sets\nX_train, X_val_test, y_train, y_val_test = train_test_split(df_leaked, y_train_val_test, test_size=0.4, random_state=42)\nX_val, X_test, y_val, y_test = train_test_split(X_val_test, y_val_test, test_size=0.5, random_state=42)\n```"]