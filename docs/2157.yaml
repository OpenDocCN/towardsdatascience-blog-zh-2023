- en: Machine Learning Made Intuitive
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习变得直观
- en: 原文：[https://towardsdatascience.com/machine-learning-made-intuitive-fabd26f97f19?source=collection_archive---------7-----------------------#2023-07-05](https://towardsdatascience.com/machine-learning-made-intuitive-fabd26f97f19?source=collection_archive---------7-----------------------#2023-07-05)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/machine-learning-made-intuitive-fabd26f97f19?source=collection_archive---------7-----------------------#2023-07-05](https://towardsdatascience.com/machine-learning-made-intuitive-fabd26f97f19?source=collection_archive---------7-----------------------#2023-07-05)
- en: 'ML: all you need to know without any overcomplicated math'
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ML：你需要知道的所有内容，没有任何复杂的数学
- en: '[](https://medium.com/@jcheigh?source=post_page-----fabd26f97f19--------------------------------)[![Justin
    Cheigh](../Images/0bafdd733fe57267074a937b4777418c.png)](https://medium.com/@jcheigh?source=post_page-----fabd26f97f19--------------------------------)[](https://towardsdatascience.com/?source=post_page-----fabd26f97f19--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----fabd26f97f19--------------------------------)
    [Justin Cheigh](https://medium.com/@jcheigh?source=post_page-----fabd26f97f19--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@jcheigh?source=post_page-----fabd26f97f19--------------------------------)[![Justin
    Cheigh](../Images/0bafdd733fe57267074a937b4777418c.png)](https://medium.com/@jcheigh?source=post_page-----fabd26f97f19--------------------------------)[](https://towardsdatascience.com/?source=post_page-----fabd26f97f19--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----fabd26f97f19--------------------------------)
    [Justin Cheigh](https://medium.com/@jcheigh?source=post_page-----fabd26f97f19--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F24cd781f1018&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learning-made-intuitive-fabd26f97f19&user=Justin+Cheigh&userId=24cd781f1018&source=post_page-24cd781f1018----fabd26f97f19---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----fabd26f97f19--------------------------------)
    ·9 min read·Jul 5, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Ffabd26f97f19&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learning-made-intuitive-fabd26f97f19&user=Justin+Cheigh&userId=24cd781f1018&source=-----fabd26f97f19---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F24cd781f1018&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learning-made-intuitive-fabd26f97f19&user=Justin+Cheigh&userId=24cd781f1018&source=post_page-24cd781f1018----fabd26f97f19---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----fabd26f97f19--------------------------------)
    ·9分钟阅读·2023年7月5日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Ffabd26f97f19&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learning-made-intuitive-fabd26f97f19&user=Justin+Cheigh&userId=24cd781f1018&source=-----fabd26f97f19---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ffabd26f97f19&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learning-made-intuitive-fabd26f97f19&source=-----fabd26f97f19---------------------bookmark_footer-----------)![](../Images/9118274351fce2976f50ba8ce8581545.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ffabd26f97f19&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learning-made-intuitive-fabd26f97f19&source=-----fabd26f97f19---------------------bookmark_footer-----------)![](../Images/9118274351fce2976f50ba8ce8581545.png)'
- en: What you may think ML is… (Photo Taken by Justin Cheigh in Billund, Denmark)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能认为的 ML…（照片由 Justin Cheigh 在丹麦比隆拍摄）
- en: What is Machine Learning?
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 什么是机器学习？
- en: Sure, the actual theory behind models like ChatGPT is admittedly very difficult,
    but the underlying intuition behind Machine Learning (ML) is, well, intuitive!
    So, what is ML?
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，像 ChatGPT 这样的模型背后的实际理论确实非常复杂，但机器学习（ML）背后的基本直觉是，嗯，很直观！那么，什么是 ML？
- en: Machine Learning allows computers to learn using data.
  id: totrans-11
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 机器学习允许计算机通过数据进行学习。
- en: But what does this mean? How do computers use data? What does it mean for a
    computer to learn? And first of all, who cares? Let’s start with the last question.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 但这意味着什么呢？计算机如何使用数据？计算机学习意味着什么？首先，谁在乎？让我们从最后一个问题开始。
- en: Nowadays, data is all around us. So it’s increasingly important to use tools
    like ML, as it can help find meaningful patterns in data without ever being explicitly
    programmed to do so! In other words, by utilizing ML we are able to apply generic
    algorithms to a wide variety of problems successfully.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，数据无处不在。因此，使用像机器学习这样的工具变得越来越重要，因为它可以在没有明确编程的情况下帮助发现数据中的有意义模式！换句话说，通过利用机器学习，我们能够将通用算法成功地应用于各种问题。
- en: There are a few main categories of Machine Learning, with some of the main types
    being supervised learning (SL), unsupervised learning (UL), and reinforcement
    learning (RL). Today I’ll just be describing supervised learning, though in subsequent
    posts I hope to elaborate more on unsupervised learning and reinforcement learning.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习有几个主要类别，其中主要类型包括有监督学习（SL）、无监督学习（UL）和强化学习（RL）。今天我将只描述有监督学习，但在后续的帖子中，我希望能更详细地讲解无监督学习和强化学习。
- en: 1 Minute SL Speedrun
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1分钟SL速跑
- en: Look, I get that you might not want to read this whole article. In this section
    I’ll teach you the very basics (which for a lot of people is all you need to know!)
    before going into more depth in the later sections.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 看，我明白你可能不想读完整篇文章。在这一部分，我将教你最基础的知识（对很多人来说，这就是你需要了解的全部内容！），然后再深入后面的部分。
- en: Supervised learning involves learning how to predict some label using different
    features.
  id: totrans-17
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 有监督学习涉及如何使用不同的特征来预测某个标签。
- en: Imagine you are trying to figure out a way to predict the price of diamonds
    using features like carat, cut, clarity, and more. Here, the goal is to learn
    a function that takes as input the features of a specific diamond and outputs
    the associated price.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 设想一下，你试图找出一种方法来预测钻石的价格，使用的特征包括克拉、切工、净度等。这里的目标是学习一个函数，该函数以特定钻石的特征作为输入，并输出相应的价格。
- en: Just as humans learn by example, in this case computers will do the same. To
    be able to learn a prediction rule, this ML agent needs “labeled examples” of
    diamonds, including both their features and their price. The supervision comes
    since you are given the label (price). In reality, it’s important to consider
    that your labeled examples are actually true, as it’s an assumption of supervised
    learning that the labeled examples are “ground truth”.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 就像人类通过例子学习一样，在这种情况下，计算机也会如此。为了能够学习预测规则，这个机器学习代理需要“标记示例”，包括钻石的特征和价格。监督的存在是因为你得到了标签（价格）。实际上，需要考虑的是你的标记示例是否真实，因为有监督学习的一个假设是标记示例是“真实情况”。
- en: Ok, now that we’ve gone over the most fundamental basics, we can get a bit more
    in depth about the whole data science/ML pipeline.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，现在我们已经了解了最基本的内容，我们可以更深入地探讨整个数据科学/机器学习流程。
- en: Problem Setup
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题设置
- en: Let’s use an extremely relatable example, which is inspired from [this textbook.](https://www.amazon.com/Understanding-Machine-Learning-Theory-Algorithms/dp/1107057132)
    Imagine you’re stranded on an island, where the only food is a rare fruit known
    as “Justin-Melon”. Even though you’ve never eaten Justin-Melon in particular,
    you’ve eaten plenty of other fruits, and you know you don’t want to eat fruit
    that has gone bad. You also know that usually you can tell if a fruit has gone
    bad by looking at the color and firmness of the fruit, so you extrapolate and
    assume this holds for Justin-Melon as well.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们用一个非常贴近的例子，这个例子来源于[这本教科书](https://www.amazon.com/Understanding-Machine-Learning-Theory-Algorithms/dp/1107057132)。假设你被困在一个只有一种稀有水果“贾斯廷蜜瓜”的岛上。尽管你从未吃过贾斯廷蜜瓜，但你吃过很多其他水果，并且你知道你不想吃已经坏掉的水果。你还知道通常通过查看水果的颜色和坚实度可以判断水果是否变坏，因此你推测这对于贾斯廷蜜瓜也适用。
- en: In ML terms, you used prior industry knowledge to determine two features (color,
    firmness) that you think will accurately predict the label (whether or not the
    Justin-Melon has gone bad).
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习术语中，你使用了之前的行业知识来确定两个特征（颜色、坚实度），你认为这将准确预测标签（贾斯廷蜜瓜是否变坏）。
- en: But how will you know what color and what firmness correspond to the fruit being
    bad? Who knows? You just need to try it out. In ML terms, we need data. More specifically,
    we need a labeled dataset consisting of real Justin-Melons and their associated
    label.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 但你怎么知道什么颜色和什么坚实度对应于水果变坏呢？谁知道呢？你只需要尝试。在机器学习术语中，我们需要数据。更具体地说，我们需要一个包含真实贾斯廷蜜瓜及其相关标签的标记数据集。
- en: Data Collection/Processing
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据收集/处理
- en: 'So you spend the next couple of days eating melons and recording the color,
    firmness, and whether or not the melon was bad. After a few painful days of constantly
    eating melons that have gone bad, you have the following labeled dataset:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 所以你花了接下来几天吃蜜瓜，并记录了颜色、硬度以及蜜瓜是否变坏。经过几天痛苦的吃坏掉的蜜瓜后，你得到了以下标注的数据集：
- en: '![](../Images/a32c05f661c1dbd504e73e5885bfc1c7.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a32c05f661c1dbd504e73e5885bfc1c7.png)'
- en: '[Code by Justin Cheigh](https://github.com/jcheigh/Medium_Articles)'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '[Justin Cheigh 的代码](https://github.com/jcheigh/Medium_Articles)'
- en: Each row is a specific melon, and each column is the value of the feature/label
    for the corresponding melon. But notice we have words, since the features are
    categorical rather than numerical.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 每一行代表一个特定的蜜瓜，每一列代表相应蜜瓜的特征/标签值。但请注意，我们有的是文字，因为特征是分类的而不是数值的。
- en: Really we need numbers for our computer to process. There are a number of techniques
    to convert categorical features to numerical features, ranging from [one hot encoding](https://machinelearningmastery.com/why-one-hot-encode-data-in-machine-learning/)
    to [embeddings](/categorical-embedding-and-transfer-learning-dd3c4af6345d) and
    beyond.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，我们需要数字以便计算机处理。有多种技术可以将分类特征转换为数值特征，从[独热编码](https://machinelearningmastery.com/why-one-hot-encode-data-in-machine-learning/)到[嵌入](
    /categorical-embedding-and-transfer-learning-dd3c4af6345d)以及其他方法。
- en: 'The simplest thing we can do is turn the column “Label” into a column “Good”,
    which is 1 if the melon is good and 0 if it’s bad. For now, assume there is some
    methodology to turn color and firmness to a scale from -10 to 10, in such a way
    that is sensible. For bonus points, think about the assumptions of putting a categorical
    feature like color on such a scale. After this preprocessing, our dataset might
    look something like this:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以做的最简单的事情是将“Label”列转换为“Good”列，如果蜜瓜好则为1，如果坏则为0。现在，假设有某种方法将颜色和硬度转换为一个从-10到10的尺度，使其合理。作为额外的挑战，考虑将像颜色这样的分类特征放到这样一个尺度上的假设。经过这种预处理后，我们的数据集可能看起来像这样：
- en: '![](../Images/ae2aff417c933affbaf146159e93f21e.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ae2aff417c933affbaf146159e93f21e.png)'
- en: '[Code by Justin Cheigh](https://github.com/jcheigh/Medium_Articles)'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '[Justin Cheigh 的代码](https://github.com/jcheigh/Medium_Articles)'
- en: We now have a labeled dataset, which means we can employ a supervised learning
    algorithm. Our algorithm needs to be a classification algorithm, as we are predicting
    a category good (1) or bad (0). Classification is in opposition to regression
    algorithms, which predict a continuous value like the price of a diamond.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在有了一个标注的数据集，这意味着我们可以使用监督学习算法。我们的算法需要是分类算法，因为我们预测的是一个类别，即好（1）或坏（0）。分类算法与回归算法相对，后者预测的是连续值，比如钻石的价格。
- en: Exploratory Data Analysis
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 探索性数据分析
- en: 'But what algorithm? There are a number of supervised classification algorithms,
    ranging in complexity from basic logistic regression to some hardcore deep learning
    algorithms. Well, let’s first take a look at our data by doing some exploratory
    data analysis (EDA):'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 但使用什么算法呢？有多种监督分类算法，从基本的逻辑回归到一些复杂的深度学习算法。首先，让我们通过进行一些探索性数据分析（EDA）来查看我们的数据：
- en: '![](../Images/bdd337c15ebafee46b3f1754fc5f6044.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bdd337c15ebafee46b3f1754fc5f6044.png)'
- en: '[Code by Justin Cheigh](https://github.com/jcheigh/Medium_Articles)'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '[Justin Cheigh 的代码](https://github.com/jcheigh/Medium_Articles)'
- en: The above image is a plot of the feature space; we have two features, and we
    are simply putting each example onto a plot with the two axes being the two features.
    Additionally, we make the point purple if the associated melon was good, and we
    make it yellow if it was bad. Clearly, with just a little bit of EDA, there’s
    an obvious answer!
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的图像是特征空间的图示；我们有两个特征，我们只是将每个例子放到一个坐标图上，两个轴分别是这两个特征。此外，如果相关的蜜瓜是好的，我们将点标记为紫色，如果是坏的，则标记为黄色。显然，通过稍微进行一些EDA，就可以找到一个明显的答案！
- en: '![](../Images/59e5e132fe6861c0b644906ea28fa91c.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/59e5e132fe6861c0b644906ea28fa91c.png)'
- en: '[Code by Justin Cheigh](https://github.com/jcheigh/Medium_Articles)'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '[Justin Cheigh 的代码](https://github.com/jcheigh/Medium_Articles)'
- en: We should probably classify all points inside the red circle as good melons,
    while ones outside of the circle should be classified in bad melons. Intuitively,
    this makes sense! For example, you don’t want a melon that’s rock solid, but you
    also don’t want it to be absurdly squishy. Rather, you want something in between,
    and the same is probably true about color as well.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可能应该将红圈内的所有点分类为优质瓜，而红圈外的点分类为劣质瓜。直观上，这样做是合理的！例如，你不想要一个坚硬如石的瓜，但也不希望它过于软绵绵。你想要的是介于两者之间的瓜，颜色的情况也可能类似。
- en: We determined we would want a decision boundary that is a circle, but this was
    just based off of preliminary data visualization. How would we systematically
    determine this? This is especially relevant in larger problems, where the answer
    is not so simple. Imagine hundreds of features. There’s no possible way to visualize
    the 100 dimensional feature space in any reasonable way.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 我们确定我们希望的决策边界是一个圆，但这只是基于初步的数据可视化。我们如何系统地确定这一点？这在更大的问题中尤为相关，答案也不那么简单。想象一下数百个特征。在任何合理的方式中，都无法可视化
    100 维特征空间。
- en: What are we learning?
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们学到了什么？
- en: The first step is to define your model. There are tons of classification models.
    Since each has their own set of assumptions, it’s important to try to make a good
    choice. To emphasize this, I’ll start by making a really bad choice.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是定义你的模型。分类模型有很多种。由于每种模型都有自己的假设，因此做出一个好的选择非常重要。为了强调这一点，我将从一个非常糟糕的选择开始。
- en: 'One intuitive idea is to make a prediction by weighing each of the factors:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 一个直观的想法是通过权衡每个因素来进行预测：
- en: Formula by Justin Cheigh using [Embed Fun](https://embed.fun/)
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: Justin Cheigh 使用 [Embed Fun](https://embed.fun/) 提供的公式
- en: For example, suppose our parameters *w1* and *w2* are 2 and 1, respectively.
    Also assume our input Justin Melon is one with Color = 4, Firmness = 6\. Then
    our prediction Good = (2 x 4) + (1 x 6) = 14.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设我们的参数 *w1* 和 *w2* 分别为 2 和 1。还假设我们的输入 Justin Melon 是 Color = 4，Firmness
    = 6 的瓜。那么我们的预测 Good = (2 x 4) + (1 x 6) = 14。
- en: 'Our classification (14) is not even one of the valid options (0 or 1). This
    is because this is actually a regression algorithm. In fact, it’s a simple case
    of the simplest regression algorithm: linear regression.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的分类（14）甚至不是有效选项（0 或 1）之一。这是因为这实际上是一个回归算法。事实上，它是最简单的回归算法的简单案例：线性回归。
- en: 'So, let’s turn this into a classification algorithm. One simple way would be
    this: use linear regression and classify as 1 if the output is higher than a bias
    term *b*. In fact, we can simplify by adding a constant term to our model in such
    a way that we classify as 1 if the output is higher than 0.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，让我们将其转化为分类算法。一个简单的方法是：使用线性回归，并在输出高于偏置项 *b* 时分类为 1。实际上，我们可以通过在模型中添加一个常数项来简化，使得当输出高于
    0 时分类为 1。
- en: 'In math, let PRED = w1 * Color + w2 * Firmness + b. Then we get:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在数学中，设 PRED = w1 * Color + w2 * Firmness + b。然后我们得到：
- en: Formula by Justin Cheigh using [Embed Fun](https://embed.fun/)
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: Justin Cheigh 使用 [Embed Fun](https://embed.fun/) 提供的公式
- en: 'This is certainly better, as we are at least performing a classification, but
    let’s make a plot of PRED on the x axis and our classification on the y axis:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 这当然更好，因为我们至少在执行分类，但让我们绘制 PRED 在 x 轴上的图和分类在 y 轴上的图：
- en: '![](../Images/6176d2582ddcd7db4900560be4db440e.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6176d2582ddcd7db4900560be4db440e.png)'
- en: '[Code by Justin Cheigh](https://github.com/jcheigh/Medium_Articles)'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '[Justin Cheigh 的代码](https://github.com/jcheigh/Medium_Articles)'
- en: 'This is a bit extreme. A slight change in PRED could change the classification
    entirely. One solution is that the output of our model represents the probability
    that the Justin-Melon is good, which we can do by smoothing out the curve:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 这有点极端。PRED 的轻微变化可能会完全改变分类。一种解决方案是让我们模型的输出表示 Justin-Melon 是好瓜的概率，我们可以通过平滑曲线来实现：
- en: '![](../Images/d8c50fa4a6166836b52c273bb2afcc7f.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d8c50fa4a6166836b52c273bb2afcc7f.png)'
- en: '[Code by Justin Cheigh](https://github.com/jcheigh/Medium_Articles)'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '[Justin Cheigh 的代码](https://github.com/jcheigh/Medium_Articles)'
- en: 'This is a sigmoid curve (or a logistic curve). So, instead of taking PRED and
    apply this piecewise activation (Good if PRED ≥ 0), we can apply this sigmoid
    activation function to get a smoothed out curve like above. Overall, our logistic
    model looks like this:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一条 sigmoid 曲线（或 logistic 曲线）。所以，与其使用 PRED 并应用这段分段激活（PRED ≥ 0 时为好），我们可以应用这个
    sigmoid 激活函数来获得如上所示的平滑曲线。总体来说，我们的 logistic 模型看起来是这样的：
- en: Formula by Justin Cheigh using [Embed Fun](https://embed.fun/)
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: Justin Cheigh 使用 [Embed Fun](https://embed.fun/) 提供的公式
- en: Here, the sigma represents the sigmoid activation function. Great, so we have
    our model, and we just need to figure out what weights and biases are best! This
    process is known as training.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，sigma 代表的是 sigmoid 激活函数。很好，所以我们有了我们的模型，我们只需要找出哪些权重和偏差是最好的！这个过程被称为训练。
- en: Training the Model
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型训练
- en: Great, so all we need to do is figure out what weights and biases are best!
    But this is much easier said than done. There are an infinite number of possibilities,
    and what does best even mean?
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 很好，所以我们需要做的就是找出哪些权重和偏差是最好的！但这说起来容易做起来难。有无数种可能性，而“最好”到底意味着什么呢？
- en: 'We begin with the latter question: what is best? Here’s one simple, yet powerful
    way: the most optimal weights are the one that get the highest accuracy on our
    training set.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从后一个问题开始：什么是最好的？这里有一种简单而强大的方法：最优的权重是那些在我们的训练集上获得最高准确率的权重。
- en: 'So, we just need to figure out an algorithm that maximizes accuracy. However,
    mathematically it’s easier to minimize something. In words, rather than defining
    a value function, where higher value is “better”, we prefer to define a loss function,
    where lower loss is better. Although people typically use something like binary
    cross entropy for (binary) classification loss, we will just use a simple example:
    minimize the number of points classified incorrectly.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们只需要找出一个能够最大化准确率的算法。然而，从数学上来说，最小化某些东西更容易。换句话说，我们更倾向于定义一个损失函数，而不是一个值函数，其中较低的损失更好。虽然人们通常使用类似二元交叉熵的东西来计算（分类）损失，但我们将使用一个简单的例子：最小化错误分类的点数。
- en: To do this, we use an algorithm known as gradient descent. At a very high level,
    gradient descent works like a nearsighted skier trying to get down a mountain.
    An important property of a good loss function (and one that our crude loss function
    actually lacks) is smoothness. If you were to plot our parameter space (parameter
    values and associated loss on the same plot), the plot would look like a mountain.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，我们使用一个被称为梯度下降的算法。从一个非常高的层次来看，梯度下降就像是一个视力不佳的滑雪者试图下山。一个好的损失函数的重要特性（而我们的粗糙损失函数实际上缺乏这种特性）是平滑性。如果你绘制我们的参数空间（参数值和相应的损失在同一图上），图像将看起来像一座山。
- en: So, we first start with random parameters, and therefore we likely start with
    bad loss. Like a skier trying to go down the mountain as fast as possible, the
    algorithm looks in every direction, trying to see the steepest way to go (i.e.
    how to change parameters in order to lower loss the most). But, the skier is nearsighted,
    so they only look a little in each direction. We iterate this process until we
    end up at the bottom (keen eyed individuals may notice we actually might end up
    at a local minima). At this point, the parameters we end up with are our trained
    parameters.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，我们首先从随机参数开始，因此我们很可能从一个糟糕的损失开始。就像一个滑雪者试图尽快下山一样，算法在各个方向上查找，试图找到最陡的路径（即如何调整参数以最大程度地降低损失）。但是，滑雪者的视力不佳，所以他们只能在每个方向上看一点。我们迭代这个过程，直到我们到达山底（敏锐的观察者可能会注意到我们实际上可能会到达局部最小值）。此时，我们得到的参数就是我们的训练参数。
- en: Once you train your logistic regression model, you realize your performance
    is still really bad, and that your accuracy is only around 60% (barely better
    than guessing!). This is because we are violating one of the model assumptions.
    Logistic regression mathematically can only output a linear decision boundary,
    but we knew from our EDA that the decision boundary should be circular!
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你训练了你的逻辑回归模型，你会发现你的性能仍然很差，准确率只有大约60%（勉强比猜测好一点！）。这是因为我们违反了模型的一个假设。逻辑回归在数学上只能输出一个线性决策边界，但从我们的
    EDA 中我们知道，决策边界应该是圆形的！
- en: With this in mind, you try different, more complex models, and you get one that
    gets 95% accuracy! You now have a fully trained classifier capable of differentiating
    between good Justin-Melons and bad Justin-Melons, and you can finally eat all
    the tasty fruit you want!
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这个理念，你尝试了不同的、更复杂的模型，结果得到一个准确率达到95%的模型！现在你有了一个完全训练好的分类器，能够区分好的 Justin-Melons
    和不好的 Justin-Melons，你终于可以吃到你想要的所有美味水果了！
- en: Conclusion
  id: totrans-70
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结论
- en: Let’s take a step back. In around 10 minutes, you learned a lot about machine
    learning, including what is essentially the whole supervised learning pipeline.
    So, what’s next?
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们退后一步。大约在10分钟内，你学到了很多关于机器学习的知识，包括基本的监督学习流程。那么，接下来是什么呢？
- en: Well, that’s for you to decide! For some, this article was enough to get a high
    level picture of what ML actually is. For others, this article may leave a lot
    of questions unanswered. That’s great! Perhaps this curiosity will allow you to
    further explore this topic.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧，那得由你来决定！对于一些人来说，这篇文章足以让他们对机器学习有一个高层次的了解。而对其他人来说，这篇文章可能会留下很多未解的问题。这很好！也许这种好奇心会促使你进一步探讨这个话题。
- en: For example, in the data collection step we assumed that you would just eat
    a ton of melons for a few days, without really taking into account any specific
    features. This makes no sense. If you ate a green mushy Justin-Melon and it made
    you violently ill, you probably would stray away from those melons. In reality,
    you would learn through experience, updating your beliefs as you go. This framework
    is more similar to reinforcement learning.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在数据收集步骤中，我们假设你会连续几天吃大量的瓜果，而没有真正考虑任何特定的特征。这是没有意义的。如果你吃了一种绿色糊状的**Justin-Melon**并且让你感到剧烈不适，你可能会避开这种瓜果。在现实中，你会通过经验来学习，不断更新你的信念。这个框架更类似于强化学习。
- en: And what if you knew that one bad Justin-Melon could kill you instantly, and
    that it was too risky to ever try one without being sure? Without these labels,
    you couldn’t perform supervised learning. But maybe there’s still a way to gain
    insight without labels. This framework is more similar to unsupervised learning.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 那么如果你知道一种不好的**Justin-Melon**可能会立即致命，并且没有确定性就尝试它太过风险呢？没有这些标签，你无法进行监督学习。但也许还有一种方法可以在没有标签的情况下获得洞察。这个框架更类似于无监督学习。
- en: In following blog posts, I hope to analogously expand on reinforcement learning
    and unsupervised learning.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的博客文章中，我希望能类比地扩展强化学习和无监督学习。
- en: Thanks for Reading!
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 感谢阅读！
