- en: Understanding NeRFs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/understanding-nerfs-2a082e13c6eb?source=collection_archive---------4-----------------------#2023-04-28](https://towardsdatascience.com/understanding-nerfs-2a082e13c6eb?source=collection_archive---------4-----------------------#2023-04-28)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: A massive breakthrough in scene representation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://wolfecameron.medium.com/?source=post_page-----2a082e13c6eb--------------------------------)[![Cameron
    R. Wolfe, Ph.D.](../Images/52bb88d7cf1105501be2fae5ccbe7a03.png)](https://wolfecameron.medium.com/?source=post_page-----2a082e13c6eb--------------------------------)[](https://towardsdatascience.com/?source=post_page-----2a082e13c6eb--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----2a082e13c6eb--------------------------------)
    [Cameron R. Wolfe, Ph.D.](https://wolfecameron.medium.com/?source=post_page-----2a082e13c6eb--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F28aa6026c553&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-nerfs-2a082e13c6eb&user=Cameron+R.+Wolfe%2C+Ph.D.&userId=28aa6026c553&source=post_page-28aa6026c553----2a082e13c6eb---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----2a082e13c6eb--------------------------------)
    ·11 min read·Apr 28, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F2a082e13c6eb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-nerfs-2a082e13c6eb&user=Cameron+R.+Wolfe%2C+Ph.D.&userId=28aa6026c553&source=-----2a082e13c6eb---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F2a082e13c6eb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-nerfs-2a082e13c6eb&source=-----2a082e13c6eb---------------------bookmark_footer-----------)![](../Images/0f3a2a80515f0f2c516a9ede3117054f.png)'
  prefs: []
  type: TYPE_NORMAL
- en: (Photo by [nuddle](https://unsplash.com/@nuddle?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    on [Unsplash](https://unsplash.com/s/photos/3d-scene?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText))
  prefs: []
  type: TYPE_NORMAL
- en: As we have seen with methods like [DeepSDF](https://cameronrwolfe.substack.com/p/3d-generative-modeling-with-deepsdf)
    [2] and [SRNs](https://cameronrwolfe.substack.com/p/scene-representation-networks)
    [4], encoding 3D objects and scenes within the weights of a feed-forward neural
    network is a memory-efficient, implicit representation of 3D data that is both
    accurate and high-resolution. However, the approaches we have seen so far are
    not quite capable of capturing realistic and complex scenes with sufficient fidelity.
    Rather, discrete representations (e.g., triangle meshes or voxel grids) produce
    a more accurate representation, assuming a sufficient allocation of memory.
  prefs: []
  type: TYPE_NORMAL
- en: This changed with the proposal of Neural Radiance Fields (NeRFs) [1], which
    use a feed-forward neural network to model a continuous representation of scenes
    and objects. The representation used by NeRFs, called a radiance field, is a bit
    different from [prior](https://cameronrwolfe.substack.com/i/94634004/signed-distance-functions)
    [proposals](https://cameronrwolfe.substack.com/i/94842305/occupancy-functions).
    In particular, NeRFs map a five-dimensional coordinate (i.e., spatial location
    and viewing direction) to a volume density and view-dependent RGB color. By accumulating
    this density and appearance information across different viewpoints and locations,
    we can render photorealist, novel views of a scene.
  prefs: []
  type: TYPE_NORMAL
- en: Like [SRNs](https://cameronrwolfe.substack.com/p/scene-representation-networks)
    [4], NeRFs can be trained using only a set of images (along with their associated
    [camera poses](https://cameronrwolfe.substack.com/i/97472888/background)) of an
    underlying scene. Compared with prior approaches, NeRF renderings are better both…
  prefs: []
  type: TYPE_NORMAL
