["```py\nclass LoRAAdapter(nn.Module):\n    def __init__(self, \n                 adaptee, # <- module to be adapted\n                 r):\n        super().__init__()\n\n        self.r = r\n        self.adaptee = adaptee\n\n        # Store a pointer to the original forward implementation \n        # of the module to be adapted.\n        # Then point its forward method to this adapter module.\n        self.orig_forward = adaptee.forward\n        adaptee.forward = self.forward\n        [..]\n```", "```py\n [..]\n        # Adding the weight matrices directly to the adaptee,\n        # which makes it more practical to report the parameters,\n        # and to remove it later.\n        adaptee.lora_A = (nn.Parameter(torch.randn(adaptee.in_features, r)/\n                          math.sqrt(adaptee.in_features)))\n        adaptee.lora_B = nn.Parameter(torch.zeros(r, adaptee.out_features))\n```", "```py\ndef forward(self, x, *args, **kwargs):\n  return (\n    self.orig_forward(x, *args, **kwargs) +\n    x @ self.adaptee.lora_A @ self.adaptee.lora_B\n  )\n```", "```py\n[..]\nroberta.encoder.layer.11.attention.output.LayerNorm.bias       0         768\nroberta.encoder.layer.11.intermediate.dense.weight             0     2359296\nroberta.encoder.layer.11.intermediate.dense.bias               0        3072\nroberta.encoder.layer.11.output.dense.weight                   0     2359296\nroberta.encoder.layer.11.output.dense.bias                     0         768\nroberta.encoder.layer.11.output.dense.lora_A                   1       12288\nroberta.encoder.layer.11.output.dense.lora_B                   1        3072\nroberta.encoder.layer.11.output.LayerNorm.weight               0         768\nroberta.encoder.layer.11.output.LayerNorm.bias                 0         768\nclassifier.dense.weight                                        1      589824\nclassifier.dense.bias                                          1         768\nclassifier.out_proj.weight                                     1        1536\nclassifier.out_proj.bias                                       1           2\n[..]\nTotal parameters: 124,978,946, thereof learnable: 923,906 (0.7392%)\n```", "```py\nfull-finetuning accuracy: 0.944\nlora-finetuning accuracy: 0.933\n```"]