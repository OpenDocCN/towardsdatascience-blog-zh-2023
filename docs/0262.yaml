- en: 'Speaking Probes: Self-Interpreting Models?'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: è¯´è¯æ¢é’ˆï¼šè‡ªè§£é‡Šæ¨¡å‹ï¼Ÿ
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/speaking-probes-self-interpreting-models-7a3dc6cb33d6?source=collection_archive---------20-----------------------#2023-01-16](https://towardsdatascience.com/speaking-probes-self-interpreting-models-7a3dc6cb33d6?source=collection_archive---------20-----------------------#2023-01-16)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://towardsdatascience.com/speaking-probes-self-interpreting-models-7a3dc6cb33d6?source=collection_archive---------20-----------------------#2023-01-16](https://towardsdatascience.com/speaking-probes-self-interpreting-models-7a3dc6cb33d6?source=collection_archive---------20-----------------------#2023-01-16)
- en: Can language models aid in their interpretation?
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è¯­è¨€æ¨¡å‹èƒ½å¸®åŠ©è§£é‡Šè‡ªå·±å—ï¼Ÿ
- en: '[](https://guydar.medium.com/?source=post_page-----7a3dc6cb33d6--------------------------------)[![Guy
    Dar](../Images/0a3b1ddd33d595e97e7a0dad551b2708.png)](https://guydar.medium.com/?source=post_page-----7a3dc6cb33d6--------------------------------)[](https://towardsdatascience.com/?source=post_page-----7a3dc6cb33d6--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----7a3dc6cb33d6--------------------------------)
    [Guy Dar](https://guydar.medium.com/?source=post_page-----7a3dc6cb33d6--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://guydar.medium.com/?source=post_page-----7a3dc6cb33d6--------------------------------)[![Guy
    Dar](../Images/0a3b1ddd33d595e97e7a0dad551b2708.png)](https://guydar.medium.com/?source=post_page-----7a3dc6cb33d6--------------------------------)[](https://towardsdatascience.com/?source=post_page-----7a3dc6cb33d6--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----7a3dc6cb33d6--------------------------------)
    [Guy Dar](https://guydar.medium.com/?source=post_page-----7a3dc6cb33d6--------------------------------)'
- en: Â·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Â·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ffab216dbde3e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fspeaking-probes-self-interpreting-models-7a3dc6cb33d6&user=Guy+Dar&userId=fab216dbde3e&source=post_page-fab216dbde3e----7a3dc6cb33d6---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----7a3dc6cb33d6--------------------------------)
    Â·16 min readÂ·Jan 16, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F7a3dc6cb33d6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fspeaking-probes-self-interpreting-models-7a3dc6cb33d6&user=Guy+Dar&userId=fab216dbde3e&source=-----7a3dc6cb33d6---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[å…³æ³¨](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ffab216dbde3e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fspeaking-probes-self-interpreting-models-7a3dc6cb33d6&user=Guy+Dar&userId=fab216dbde3e&source=post_page-fab216dbde3e----7a3dc6cb33d6---------------------post_header-----------)
    å‘è¡¨åœ¨ [Towards Data Science](https://towardsdatascience.com/?source=post_page-----7a3dc6cb33d6--------------------------------)
    Â·16åˆ†é’Ÿé˜…è¯»Â·2023å¹´1æœˆ16æ—¥[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F7a3dc6cb33d6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fspeaking-probes-self-interpreting-models-7a3dc6cb33d6&user=Guy+Dar&userId=fab216dbde3e&source=-----7a3dc6cb33d6---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F7a3dc6cb33d6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fspeaking-probes-self-interpreting-models-7a3dc6cb33d6&source=-----7a3dc6cb33d6---------------------bookmark_footer-----------)![](../Images/cd19726fc34b00c30807dcb8a268bad7.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F7a3dc6cb33d6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fspeaking-probes-self-interpreting-models-7a3dc6cb33d6&source=-----7a3dc6cb33d6---------------------bookmark_footer-----------)![](../Images/cd19726fc34b00c30807dcb8a268bad7.png)'
- en: Photo by [Kane Reinholdtsen](https://unsplash.com/@kanereinholdtsen?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: ç…§ç‰‡ç”± [Kane Reinholdtsen](https://unsplash.com/@kanereinholdtsen?utm_source=medium&utm_medium=referral)
    æä¾›ï¼Œæ¥æºäº [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: '*In this post, I experiment with the idea that language models can be coaxed
    to explain vectors coming from their parameters. It turns out to work better than
    you might expect, but still much work needs to be done.*'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '*åœ¨è¿™ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘å°è¯•äº†ä¸€ä¸ªæƒ³æ³•ï¼Œå³è¯­è¨€æ¨¡å‹å¯ä»¥è¢«å¼•å¯¼è§£é‡Šæ¥è‡ªå…¶å‚æ•°çš„å‘é‡ã€‚ç»“æœæ¯”ä½ é¢„æœŸçš„è¦å¥½ï¼Œä½†ä»ç„¶éœ€è¦åšå¾ˆå¤šå·¥ä½œã€‚*'
- en: '*As is customary in scientific papers, I use â€œweâ€ instead of â€œIâ€ (among other
    reasons, because it makes the text sound a bit less self-centered..).*'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '*ä¸ç§‘å­¦è®ºæ–‡ä¸­çš„æƒ¯ä¾‹ä¸€æ ·ï¼Œæˆ‘ä½¿ç”¨â€œæˆ‘ä»¬â€è€Œä¸æ˜¯â€œæˆ‘â€ï¼ˆéƒ¨åˆ†åŸå› æ˜¯å› ä¸ºè¿™æ ·å¯ä»¥è®©æ–‡æœ¬å¬èµ·æ¥ä¸é‚£ä¹ˆä»¥è‡ªæˆ‘ä¸ºä¸­å¿ƒ..ï¼‰ã€‚*'
- en: '*This is not really a complete work, but more like a preliminary report on
    an idea I believe might be useful, and should be shared. As a result, I only carried
    out basic experiments to sanity-check the method. I hope other researchers would
    take up the work from where I started, and see if the limitations of the methods
    I suggest are surmountable. This work is aimed at professionals, but anyone with
    decent knowledge of transformers should be comfortable reading it.*'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '*è¿™å®é™…ä¸Šä¸æ˜¯ä¸€é¡¹å®Œæ•´çš„å·¥ä½œï¼Œè€Œæ›´åƒæ˜¯ä¸€ä¸ªåˆæ­¥æŠ¥å‘Šï¼Œå…³äºæˆ‘è®¤ä¸ºå¯èƒ½æœ‰ç”¨çš„ä¸€ä¸ªæƒ³æ³•ï¼Œåº”è¯¥å…±äº«ã€‚å› æ­¤ï¼Œæˆ‘åªè¿›è¡Œäº†åŸºæœ¬å®éªŒä»¥æ£€éªŒæ–¹æ³•çš„åˆç†æ€§ã€‚æˆ‘å¸Œæœ›å…¶ä»–ç ”ç©¶äººå‘˜èƒ½ä»æˆ‘å¼€å§‹çš„åœ°æ–¹ç»§ç»­å·¥ä½œï¼Œçœ‹çœ‹æˆ‘å»ºè®®çš„æ–¹æ³•çš„å±€é™æ€§æ˜¯å¦å¯ä»¥å…‹æœã€‚è¿™é¡¹å·¥ä½œé¢å‘ä¸“ä¸šäººå£«ï¼Œä½†ä»»ä½•å…·æœ‰è‰¯å¥½å˜æ¢å™¨çŸ¥è¯†çš„äººéƒ½åº”è¯¥èƒ½è½»æ¾é˜…è¯»ã€‚*'
- en: Introduction
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ä»‹ç»
- en: In recent years, many interpretability methods have been developed in Natural
    Language Processing [*Kadar et al., 2017; Na et al., 2019; Geva et al., 2020;
    Dar et al., 2022*]. In parallel, strong language models have taken the field by
    storm. One may wonder if strong language skills allow language models to communicate
    about their inner state. This work is a brief report on our explorations of this
    conjecture. In this work, we will design natural language prompts and inject model
    parameters as virtual tokens in the input. The prompts are designed to instruct
    the model to explain words â€” but instead of a real word they are given a virtual
    token representing a model parameter. The model then generates a sequence continuing
    the prompt. We will observe this techniqueâ€™s ability to explain model parameters
    which we have existing explanations for. We call the new technique *â€œspeaking
    probesâ€*. We will also discuss on a high-level possible justifications for why
    one might expect the method to work.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: è¿‘å¹´æ¥ï¼Œè®¸å¤šå¯è§£é‡Šæ€§æ–¹æ³•åœ¨è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸå¾—åˆ°äº†å‘å±• [*Kadar et al., 2017; Na et al., 2019; Geva et al.,
    2020; Dar et al., 2022*]ã€‚ä¸æ­¤åŒæ—¶ï¼Œå¼ºå¤§çš„è¯­è¨€æ¨¡å‹åœ¨è¯¥é¢†åŸŸå¼•èµ·äº†è½°åŠ¨ã€‚æœ‰äººå¯èƒ½ä¼šæƒ³çŸ¥é“å¼ºå¤§çš„è¯­è¨€æŠ€èƒ½æ˜¯å¦ä½¿è¯­è¨€æ¨¡å‹èƒ½å¤Ÿå¯¹å…¶å†…åœ¨çŠ¶æ€è¿›è¡Œäº¤æµã€‚è¿™é¡¹å·¥ä½œæ˜¯å¯¹è¿™ä¸€çŒœæµ‹çš„ç®€è¦æŠ¥å‘Šã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å°†è®¾è®¡è‡ªç„¶è¯­è¨€æç¤ºï¼Œå¹¶å°†æ¨¡å‹å‚æ•°ä½œä¸ºè™šæ‹Ÿæ ‡è®°æ³¨å…¥è¾“å…¥ä¸­ã€‚æç¤ºçš„ç›®çš„æ˜¯æŒ‡ç¤ºæ¨¡å‹è§£é‡Šå•è¯â€”â€”ä½†ä¸æ˜¯ä¸€ä¸ªçœŸå®çš„å•è¯ï¼Œè€Œæ˜¯ä¸€ä¸ªè¡¨ç¤ºæ¨¡å‹å‚æ•°çš„è™šæ‹Ÿæ ‡è®°ã€‚ç„¶åï¼Œæ¨¡å‹ç”Ÿæˆä¸€ä¸ªç»§ç»­æç¤ºçš„åºåˆ—ã€‚æˆ‘ä»¬å°†è§‚å¯Ÿè¯¥æŠ€æœ¯è§£é‡Šæ¨¡å‹å‚æ•°çš„èƒ½åŠ›ï¼Œæˆ‘ä»¬å·²æœ‰ç°æœ‰è§£é‡Šã€‚æˆ‘ä»¬ç§°è¿™ç§æ–°æŠ€æœ¯ä¸º*â€œå‘è¨€æ¢é’ˆâ€*ã€‚æˆ‘ä»¬è¿˜å°†åœ¨é«˜å±‚æ¬¡ä¸Šè®¨è®ºä¸ºä»€ä¹ˆå¯èƒ½æœŸæœ›è¿™ç§æ–¹æ³•æœ‰æ•ˆçš„ç†ç”±ã€‚
- en: '![](../Images/fd43ad0917dede573b6130631c936a7a.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fd43ad0917dede573b6130631c936a7a.png)'
- en: 'Figure: Illustration of a speaking probes'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ç¤ºï¼šå‘è¨€æ¢é’ˆçš„æ’å›¾
- en: Interpretability researchers are encouraged to use speaking probes as a tool
    to guide their analysis. We *do* *not* suggest relying on their answers indiscriminately,
    as they are not sufficiently grounded. However, they have the important advantage
    of possessing the expressive power of natural language. Our queries are out of
    distribution for the model in the zero-shot case as it was only trained with real
    tokens. However, we hypothesize its inherent skills at manipulating its representations
    will make it easy to learn the new task.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: é¼“åŠ±å¯è§£é‡Šæ€§ç ”ç©¶äººå‘˜ä½¿ç”¨å‘è¨€æ¢é’ˆä½œä¸ºæŒ‡å¯¼åˆ†æçš„å·¥å…·ã€‚æˆ‘ä»¬*å¹¶ä¸* *å»ºè®®* ä¸åŠ åŒºåˆ†åœ°ä¾èµ–å…¶å›ç­”ï¼Œå› ä¸ºå®ƒä»¬å¹¶ä¸å®Œå…¨å¯é ã€‚ç„¶è€Œï¼Œå®ƒä»¬å…·æœ‰è‡ªç„¶è¯­è¨€çš„è¡¨è¾¾èƒ½åŠ›è¿™ä¸€é‡è¦ä¼˜åŠ¿ã€‚æˆ‘ä»¬çš„æŸ¥è¯¢åœ¨é›¶æ ·æœ¬æƒ…å†µä¸‹å¯¹æ¨¡å‹æ¥è¯´æ˜¯è¶…å‡ºåˆ†å¸ƒçš„ï¼Œå› ä¸ºå®ƒä»…ç”¨çœŸå®çš„æ ‡è®°è¿›è¡Œè®­ç»ƒã€‚ç„¶è€Œï¼Œæˆ‘ä»¬å‡è®¾å®ƒåœ¨æ“ä½œå…¶è¡¨ç¤ºæ–¹é¢çš„å›ºæœ‰æŠ€èƒ½å°†ä½¿å…¶å®¹æ˜“å­¦ä¹ æ–°ä»»åŠ¡ã€‚
- en: 'We provide the following two resources for researchers interested in exploring
    this technique for themselves:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ä¸ºæœ‰å…´è¶£è‡ªå·±æ¢ç´¢è¿™ä¸€æŠ€æœ¯çš„ç ”ç©¶äººå‘˜æä¾›ä»¥ä¸‹ä¸¤ä¸ªèµ„æºï¼š
- en: '**Code:** [https://github.com/guyd1995/speaking-probes](https://github.com/guyd1995/speaking-probes)'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ä»£ç ï¼š** [https://github.com/guyd1995/speaking-probes](https://github.com/guyd1995/speaking-probes)'
- en: '[](https://github.com/guyd1995/speaking-probes?source=post_page-----7a3dc6cb33d6--------------------------------)
    [## GitHub - guyd1995/speaking-probes'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://github.com/guyd1995/speaking-probes?source=post_page-----7a3dc6cb33d6--------------------------------)
    [## GitHub - guyd1995/speaking-probes'
- en: You can't perform that action at this time. You signed in with another tab or
    window. You signed out in another tab orâ€¦
  id: totrans-20
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ç›®å‰æ— æ³•æ‰§è¡Œè¯¥æ“ä½œã€‚æ‚¨åœ¨å…¶ä»–æ ‡ç­¾é¡µæˆ–çª—å£ä¸­ç™»å½•ã€‚æ‚¨åœ¨å…¶ä»–æ ‡ç­¾é¡µä¸­é€€å‡ºäº†ç™»å½•â€¦
- en: github.com](https://github.com/guyd1995/speaking-probes?source=post_page-----7a3dc6cb33d6--------------------------------)
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '[github.com](https://github.com/guyd1995/speaking-probes?source=post_page-----7a3dc6cb33d6--------------------------------)'
- en: '**Demo** (ğŸ¤— *HuggingFace*): [https://huggingface.co/spaces/guy-tau/speaking-probes](https://huggingface.co/spaces/guy-tau/speaking-probes)
    â€” This can be slow, so apart from basic experimentation, it is better to open
    one of the notebooks in the repository on Colab'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**æ¼”ç¤º** (ğŸ¤— *HuggingFace*): [https://huggingface.co/spaces/guy-tau/speaking-probes](https://huggingface.co/spaces/guy-tau/speaking-probes)
    â€” è¿™å¯èƒ½ä¼šå¾ˆæ…¢ï¼Œå› æ­¤é™¤äº†åŸºæœ¬å®éªŒå¤–ï¼Œæœ€å¥½åœ¨ Colab ä¸Šæ‰“å¼€å­˜å‚¨åº“ä¸­çš„ä¸€ä¸ªç¬”è®°æœ¬'
- en: 'Background: The Residual Stream'
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: èƒŒæ™¯ï¼šæ®‹å·®æµ
- en: '*This has been explained in more detail in the background section of my previous
    post:* [*Analyzing Transformers in Embedding Space â€” Explained*](/analyzing-transformers-in-embedding-space-explained-ef72130a6844)'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '*è¿™åœ¨æˆ‘ä¹‹å‰çš„å¸–å­ä¸­çš„èƒŒæ™¯éƒ¨åˆ†å·²æ›´è¯¦ç»†åœ°è§£é‡Šï¼š* [*åœ¨åµŒå…¥ç©ºé—´ä¸­åˆ†æ Transformers â€” è§£é‡Š*](/analyzing-transformers-in-embedding-space-explained-ef72130a6844)'
- en: We rely on a useful view of the transformer through its residual connections
    originally introduced in *nostalgebraist* [*2020*]. Specifically, each layer takes
    a hidden state as input and adds information to the hidden state through its residual
    connection. Under this view, the hidden state is a residual stream passed along
    the layers, from which information is read, and to which information is written
    at each layer. *Elhage et al.* [*2021*] and *Geva et al.* [*2022b*] observed that
    the residual stream is often barely updated in the last layers, and thus the final
    prediction is determined in early layers and the hidden state is mostly passed
    through the later layers. An exciting consequence of the residual stream view
    is that we can project hidden states in every layer into embedding space by multiplying
    the hidden state with the embedding matrix *E*, treating the hidden state as if
    it were the output of the last layer. *Geva et al.* [*2022a*] used this approach
    to interpret the prediction of transformer-based language models, and we follow
    a similar approach.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ä¾èµ–äºé€šè¿‡å…¶æ®‹å·®è¿æ¥å¯¹å˜æ¢å™¨çš„æœ‰ç”¨è§†å›¾ï¼Œè¿™ä¸€è§†å›¾æœ€åˆç”± *nostalgebraist* [*2020*] æå‡ºã€‚å…·ä½“è€Œè¨€ï¼Œæ¯ä¸€å±‚å°†éšè—çŠ¶æ€ä½œä¸ºè¾“å…¥ï¼Œå¹¶é€šè¿‡å…¶æ®‹å·®è¿æ¥å‘éšè—çŠ¶æ€æ·»åŠ ä¿¡æ¯ã€‚åœ¨è¿™ç§è§†å›¾ä¸‹ï¼Œéšè—çŠ¶æ€æ˜¯æ²¿å±‚ä¼ é€’çš„æ®‹å·®æµï¼Œä»ä¸­è¯»å–ä¿¡æ¯ï¼Œå¹¶åœ¨æ¯ä¸€å±‚å†™å…¥ä¿¡æ¯ã€‚*Elhage
    ç­‰äºº* [*2021*] å’Œ *Geva ç­‰äºº* [*2022b*] è§‚å¯Ÿåˆ°æ®‹å·®æµåœ¨æœ€åå‡ å±‚é€šå¸¸å‡ ä¹æ²¡æœ‰æ›´æ–°ï¼Œå› æ­¤æœ€ç»ˆé¢„æµ‹æ˜¯åœ¨æ—©æœŸå±‚ä¸­ç¡®å®šçš„ï¼Œéšè—çŠ¶æ€å¤§å¤šåœ¨åç»­å±‚ä¸­ä¼ é€’ã€‚æ®‹å·®æµè§†å›¾çš„ä¸€ä¸ªä»¤äººå…´å¥‹çš„ç»“æœæ˜¯ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡å°†éšè—çŠ¶æ€ä¸åµŒå…¥çŸ©é˜µ
    *E* ç›¸ä¹˜ï¼Œå°†æ¯å±‚çš„éšè—çŠ¶æ€æŠ•å½±åˆ°åµŒå…¥ç©ºé—´ï¼Œå°†éšè—çŠ¶æ€è§†ä¸ºæœ€åä¸€å±‚çš„è¾“å‡ºã€‚*Geva ç­‰äºº* [*2022a*] ä½¿ç”¨è¿™ç§æ–¹æ³•æ¥è§£é‡ŠåŸºäºå˜æ¢å™¨çš„è¯­è¨€æ¨¡å‹çš„é¢„æµ‹ï¼Œæˆ‘ä»¬ä¹Ÿé‡‡å–äº†ç±»ä¼¼çš„æ–¹æ³•ã€‚
- en: '![](../Images/ddefd4b7d10a39c3803b93127af7134d.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ddefd4b7d10a39c3803b93127af7134d.png)'
- en: 'Figure: the residual stream view â€” visually'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ï¼šæ®‹å·®æµè§†å›¾â€”â€”å¯è§†åŒ–
- en: Showcasing Speaking Probes
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å±•ç¤ºè¯´è¯æ¢æµ‹
- en: Overview
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ¦‚è¿°
- en: Our intuition builds on the residual stream view. In the residual stream view,
    parameters of the models are added to the hidden state on a more or less equal
    footing with token embeddings. More generally, the residual view hints that thereâ€™s
    a good case for considering parameter vectors, hidden states, and token embeddings
    to be using the same â€œlanguageâ€. â€œSyntacticallyâ€, we can use any continuous representation
    â€” be it a parameter vector or hidden state â€” as a virtual token. We will use the
    term â€œ*neuronâ€* interchangeably with â€œ*virtual tokenâ€*.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬çš„ç›´è§‰åŸºäºæ®‹å·®æµè§†å›¾ã€‚åœ¨æ®‹å·®æµè§†å›¾ä¸­ï¼Œæ¨¡å‹çš„å‚æ•°ä¸ token åµŒå…¥åœ¨éšè—çŠ¶æ€ä¸Šä»¥æˆ–å¤šæˆ–å°‘å¹³ç­‰çš„æ–¹å¼ç›¸åŠ ã€‚æ›´ä¸€èˆ¬åœ°ï¼Œæ®‹å·®è§†å›¾æš—ç¤ºè€ƒè™‘å‚æ•°å‘é‡ã€éšè—çŠ¶æ€å’Œ
    token åµŒå…¥ä½¿ç”¨ç›¸åŒâ€œè¯­è¨€â€çš„ç†ç”±ã€‚*ä»â€œå¥æ³•ä¸Šâ€*æ¥çœ‹ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ä»»ä½•è¿ç»­è¡¨ç¤ºâ€”â€”æ— è®ºæ˜¯å‚æ•°å‘é‡è¿˜æ˜¯éšè—çŠ¶æ€â€”â€”ä½œä¸ºè™šæ‹Ÿ tokenã€‚æˆ‘ä»¬å°†â€œ*ç¥ç»å…ƒ*â€ä¸â€œ*è™šæ‹Ÿ
    token*â€äº¤æ›¿ä½¿ç”¨ã€‚
- en: We will focus on parameters in this article, as hidden states seem to be more
    complicated to analyze â€” which stands to reason since they are mixtures of parameters.
    We show that parameter vectors can be used alongside token embeddings in the input
    prompt and produce meaningful responses. We hypothesize a neuron eventually collapses
    into a token that is related to the concepts it encodes.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬æ–‡å°†é‡ç‚¹è®¨è®ºå‚æ•°ï¼Œå› ä¸ºéšè—çŠ¶æ€ä¼¼ä¹æ›´å¤æ‚â€”â€”è¿™å¾ˆæœ‰é“ç†ï¼Œå› ä¸ºå®ƒä»¬æ˜¯å‚æ•°çš„æ··åˆã€‚æˆ‘ä»¬å±•ç¤ºäº†å‚æ•°å‘é‡å¯ä»¥ä¸ token åµŒå…¥ä¸€èµ·åœ¨è¾“å…¥æç¤ºä¸­ä½¿ç”¨ï¼Œå¹¶äº§ç”Ÿæœ‰æ„ä¹‰çš„å“åº”ã€‚æˆ‘ä»¬å‡è®¾ç¥ç»å…ƒæœ€ç»ˆä¼šå´©æºƒæˆä¸å…¶ç¼–ç æ¦‚å¿µç›¸å…³çš„
    tokenã€‚
- en: Our goal is to use the strong communication skills language models possess for
    expressing their latent knowledge. We suggest a few prompts in which the model
    is requested to explain a word. Instead of a word, it is given a virtual token
    representing a vector in the parameters. We represent the virtual token in the
    prompt by the label **<neuron>** (when running the model, its token embedding
    is simply replaced with the neuron we want to interpret). We then generate the
    continuation of the prompt, which is the language modelâ€™s response.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬çš„ç›®æ ‡æ˜¯åˆ©ç”¨è¯­è¨€æ¨¡å‹å…·å¤‡çš„å¼ºå¤§æ²Ÿé€šæŠ€èƒ½æ¥è¡¨è¾¾å…¶æ½œåœ¨çŸ¥è¯†ã€‚æˆ‘ä»¬å»ºè®®å‡ ä¸ªæç¤ºï¼Œè¦æ±‚æ¨¡å‹è§£é‡Šä¸€ä¸ªè¯ã€‚æ¨¡å‹ä¸ä¼šç›´æ¥æ¥æ”¶åˆ°ä¸€ä¸ªè¯ï¼Œè€Œæ˜¯ä¸€ä¸ªä»£è¡¨å‚æ•°å‘é‡çš„è™šæ‹Ÿ
    tokenã€‚æˆ‘ä»¬ç”¨æ ‡ç­¾ **<neuron>** åœ¨æç¤ºä¸­è¡¨ç¤ºè™šæ‹Ÿ tokenï¼ˆåœ¨è¿è¡Œæ¨¡å‹æ—¶ï¼Œå…¶ token åµŒå…¥ä¼šè¢«æˆ‘ä»¬æƒ³è¦è§£é‡Šçš„ç¥ç»å…ƒæ›¿æ¢ï¼‰ã€‚ç„¶åï¼Œæˆ‘ä»¬ç”Ÿæˆæç¤ºçš„ç»§ç»­éƒ¨åˆ†ï¼Œè¿™å°±æ˜¯è¯­è¨€æ¨¡å‹çš„å“åº”ã€‚
- en: Prompts
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æç¤º
- en: '[PRE0]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '[PRE1]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '[PRE2]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[PRE3]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: More examples are available in the repository under **prompts/**
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: æ›´å¤šç¤ºä¾‹å¯ä»¥åœ¨ **prompts/** ä»“åº“ä¸­æ‰¾åˆ°
- en: Method
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ–¹æ³•
- en: We feed a prompt into the model and generate the continuation of the text with
    ***<neuron>***â€™s â€œtoken embeddingâ€ being the neuron we want to interpret. To produce
    diverse outputs, we generate with sampling and not just greedy decoding. We will
    see a few examples below.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†ä¸€ä¸ªæç¤ºè¾“å…¥æ¨¡å‹ï¼Œå¹¶ç”Ÿæˆæ–‡æœ¬çš„å»¶ç»­ï¼Œå…¶ä¸­ ***<neuron>*** çš„â€œtoken embeddingâ€ æ˜¯æˆ‘ä»¬æƒ³è¦è§£é‡Šçš„ç¥ç»å…ƒã€‚ä¸ºäº†äº§ç”Ÿå¤šæ ·åŒ–çš„è¾“å‡ºï¼Œæˆ‘ä»¬ä½¿ç”¨é‡‡æ ·è€Œä¸ä»…ä»…æ˜¯è´ªå©ªè§£ç ã€‚æˆ‘ä»¬å°†çœ‹åˆ°ä¸‹é¢çš„ä¸€äº›ç¤ºä¾‹ã€‚
- en: In this work, we will focus on feedforward (FF) keys (the first layer of the
    feed-forward sublayer), as they seem somewhat easier to interpret than FF values
    (the second layer). Each layer *l* has a matrix K_*l* (do NOT confuse with *attention*
    keys) â€” each of its columns can be considered interpreted *individually*.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å°†é‡ç‚¹å…³æ³¨å‰é¦ˆï¼ˆFFï¼‰é”®ï¼ˆå‰é¦ˆå­å±‚çš„ç¬¬ä¸€å±‚ï¼‰ï¼Œå› ä¸ºå®ƒä»¬ä¼¼ä¹æ¯” FF å€¼ï¼ˆç¬¬äºŒå±‚ï¼‰æ›´å®¹æ˜“è§£é‡Šã€‚æ¯ä¸€å±‚ *l* éƒ½æœ‰ä¸€ä¸ªçŸ©é˜µ K_*l*ï¼ˆä¸è¦ä¸
    *attention* é”®æ··æ·†ï¼‰â€”â€”å®ƒçš„æ¯ä¸€åˆ—éƒ½å¯ä»¥è¢«è§†ä¸º*å•ç‹¬*è§£é‡Šçš„ã€‚
- en: 'To test our method, we use models we already have a good idea of what they
    mean in embedding space. Obviously, these are the easiest cases we can consider
    â€” so these experiments are just a sanity check. For syntactic sugar, we use ***<****param****_*i*_*j*>***
    for a neuron representing the j-th FF key in the i-th layer. All the examples
    below are from *GPT-2 medium.* The generation hyperparameters we use are:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†æµ‹è¯•æˆ‘ä»¬çš„æ–¹æ³•ï¼Œæˆ‘ä»¬ä½¿ç”¨äº†æˆ‘ä»¬å·²ç»å¯¹å…¶åœ¨åµŒå…¥ç©ºé—´ä¸­çš„æ„ä¹‰æœ‰è¾ƒå¥½äº†è§£çš„æ¨¡å‹ã€‚æ˜¾ç„¶ï¼Œè¿™äº›æ˜¯æˆ‘ä»¬å¯ä»¥è€ƒè™‘çš„æœ€ç®€å•çš„æƒ…å†µâ€”â€”æ‰€ä»¥è¿™äº›å®éªŒåªæ˜¯ä¸ºäº†åŸºæœ¬æ£€æŸ¥ã€‚ä½œä¸ºè¯­æ³•ç³–ï¼Œæˆ‘ä»¬ä½¿ç”¨
    ***<****param****_*i*_*j*>*** æ¥è¡¨ç¤º i å±‚ä¸­ç¬¬ j ä¸ª FF é”®çš„ç¥ç»å…ƒã€‚ä¸‹é¢çš„æ‰€æœ‰ç¤ºä¾‹éƒ½æ¥è‡ª *GPT-2 medium*ã€‚æˆ‘ä»¬ä½¿ç”¨çš„ç”Ÿæˆè¶…å‚æ•°æ˜¯ï¼š
- en: '[PRE4]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Examples: Comparison With Embedding Space Projection'
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹ï¼šä¸åµŒå…¥ç©ºé—´æŠ•å½±çš„æ¯”è¾ƒ
- en: '***Japanese Names***'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '***æ—¥æœ¬åå­—***'
- en: '*When projected to the embedding space <param_10_8> seems to relate to Japanese
    names and generally speaking terms associated with Japan (* as far as I can tell):*'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '*å½“æŠ•å½±åˆ°åµŒå…¥ç©ºé—´æ—¶ï¼Œ<param_10_8> ä¼¼ä¹ä¸æ—¥æœ¬åå­—ä»¥åŠé€šå¸¸ä¸æ—¥æœ¬ç›¸å…³çš„æœ¯è¯­ç›¸å…³ï¼ˆ*å°±æˆ‘æ‰€çŸ¥ï¼‰ï¼š*'
- en: '[PRE5]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Letâ€™s see what the new method gives us:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬çœ‹çœ‹æ–°æ–¹æ³•ç»™æˆ‘ä»¬å¸¦æ¥äº†ä»€ä¹ˆï¼š
- en: '[PRE6]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '**Discussion**: while it hasn''t given us a concise answer, it is not hard
    to deduce from the above samples that the term is indeed related to Japan and
    Japanese. As you can see, the model is not very truthful, even without the neuron
    being involved, spouting strange assertions like in the last sample.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '**è®¨è®º**ï¼šè™½ç„¶å®ƒæ²¡æœ‰ç»™å‡ºæ˜ç¡®çš„ç­”æ¡ˆï¼Œä½†ä»ä»¥ä¸Šç¤ºä¾‹ä¸­ä¸éš¾æ¨æµ‹ï¼Œè¯¥æœ¯è¯­ç¡®å®ä¸æ—¥æœ¬å’Œæ—¥æœ¬äººç›¸å…³ã€‚æ­£å¦‚ä½ æ‰€çœ‹åˆ°çš„ï¼Œå³ä½¿ä¸æ¶‰åŠç¥ç»å…ƒï¼Œè¯¥æ¨¡å‹ä¹Ÿä¸å¤ªçœŸå®ï¼Œåƒæœ€åä¸€ä¸ªç¤ºä¾‹é‚£æ ·æå‡ºå¥‡æ€ªçš„æ–­è¨€ã€‚'
- en: '***Progress***'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '***è¿›æ­¥***'
- en: '*<param_11_2> seems to be related to the idea of progress. If we look at it
    in the embedding space, we get:*'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '*<param_11_2> ä¼¼ä¹ä¸è¿›æ­¥çš„æ¦‚å¿µç›¸å…³ã€‚å¦‚æœæˆ‘ä»¬åœ¨åµŒå…¥ç©ºé—´ä¸­æŸ¥çœ‹å®ƒï¼Œæˆ‘ä»¬å¾—åˆ°ï¼š*'
- en: '[PRE7]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Letâ€™s see if we can get this from the new method:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬çœ‹çœ‹æ–°æ–¹æ³•æ˜¯å¦èƒ½å¾—åˆ°è¿™äº›ï¼š
- en: '[PRE8]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '**Discussion**:we can consider this one quite good. As the model was trained
    on data from the internet, some answers might replicate the format of web discussions.
    Altogether, this parameter seems easy for the speaking probe to interpret.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '**è®¨è®º**ï¼šæˆ‘ä»¬å¯ä»¥è®¤ä¸ºè¿™é¡¹ç»“æœç›¸å½“ä¸é”™ã€‚ç”±äºæ¨¡å‹æ˜¯åŸºäºäº’è”ç½‘æ•°æ®è¿›è¡Œè®­ç»ƒçš„ï¼Œå› æ­¤ä¸€äº›ç­”æ¡ˆå¯èƒ½ä¼šå¤åˆ¶ç½‘é¡µè®¨è®ºçš„æ ¼å¼ã€‚æ€»çš„æ¥è¯´ï¼Œè¿™ä¸ªå‚æ•°ä¼¼ä¹å¯¹è¯´è¯æ¢é’ˆæ˜“äºè§£é‡Šã€‚'
- en: '***Words that start with G***'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '***ä»¥ G å¼€å¤´çš„è¯æ±‡***'
- en: '*<param_0_0>* *appears to relate to words that begin with the letter â€œgâ€. In
    the embedding space, we see:*'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '*<param_0_0>* *ä¼¼ä¹ä¸ä»¥å­—æ¯â€œgâ€å¼€å¤´çš„è¯æ±‡æœ‰å…³ã€‚åœ¨åµŒå…¥ç©ºé—´ä¸­ï¼Œæˆ‘ä»¬çœ‹åˆ°ï¼š*'
- en: '[PRE9]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Letâ€™s see what happens with speaking probes:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬çœ‹çœ‹ä½¿ç”¨è¯´è¯æ¢é’ˆä¼šå‘ç”Ÿä»€ä¹ˆï¼š
- en: '[PRE10]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '[PRE11]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '**Discussion**: with the first prompt, the probe seems to be very unrelated
    to the embedding space interpretation of the parameter. The only hint we get in
    the right direction is â€œ*In the United States, there are many people with names
    that begin in G and end at gâ€¦â€* from the second sample. However, it is said in
    an indirect manner, and not as an answer to the question.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '**è®¨è®º**ï¼šä½¿ç”¨ç¬¬ä¸€ä¸ªæç¤ºæ—¶ï¼Œæ¢é’ˆä¼¼ä¹ä¸å‚æ•°çš„åµŒå…¥ç©ºé—´è§£é‡Šæ— å…³ã€‚æˆ‘ä»¬åœ¨æ­£ç¡®æ–¹å‘ä¸Šçš„å”¯ä¸€çº¿ç´¢æ˜¯æ¥è‡ªç¬¬äºŒä¸ªç¤ºä¾‹çš„â€œ*åœ¨ç¾å›½ï¼Œæœ‰è®¸å¤šäººåå­—ä»¥ G å¼€å¤´ï¼Œä»¥
    g ç»“å°¾...*â€ã€‚ç„¶è€Œï¼Œè¿™ç§è¯´æ³•æ˜¯é—´æ¥çš„ï¼Œå¹¶ä¸æ˜¯å¯¹é—®é¢˜çš„ç›´æ¥å›ç­”ã€‚'
- en: This is a good place to demonstrate the flexibility of speaking probes, as they
    let you reformulate the question and maybe get better results. One can arguably
    say that the second prompt is much better suited to the problem. While it is not
    too truthful, an overview of the responses can hint at the right direction â€” one
    sample responds â€œGâ€ and then gives a few unrelated words. Another one replies
    with â€œgg, goâ€. While itâ€™s not clear-cut, it can help to begin exploring. It is
    also generally advisable to work with more samples.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯å±•ç¤ºè¯´è¯æ¢é’ˆçµæ´»æ€§çš„å¥½åœ°æ–¹ï¼Œå› ä¸ºå®ƒä»¬å…è®¸ä½ é‡æ–°è¡¨è¿°é—®é¢˜ï¼Œå¹¶å¯èƒ½è·å¾—æ›´å¥½çš„ç»“æœã€‚å¯ä»¥è¯´ç¬¬äºŒä¸ªæç¤ºæ›´é€‚åˆé—®é¢˜ã€‚è™½ç„¶å®ƒä¸å®Œå…¨çœŸå®ï¼Œä½†å¯¹å“åº”çš„æ¦‚è¿°å¯ä»¥æš—ç¤ºæ­£ç¡®çš„æ–¹å‘â€”â€”ä¸€ä¸ªæ ·æœ¬å›åº”â€œGâ€ç„¶åç»™å‡ºå‡ ä¸ªæ— å…³çš„è¯ã€‚å¦ä¸€ä¸ªåˆ™å›å¤â€œgg,
    goâ€ã€‚è™½ç„¶ä¸æ˜ç¡®ï¼Œä½†å®ƒå¯ä»¥å¸®åŠ©å¼€å§‹æ¢ç´¢ã€‚é€šå¸¸å»ºè®®ä½¿ç”¨æ›´å¤šæ ·æœ¬ã€‚
- en: Synthetic Examples
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: åˆæˆç¤ºä¾‹
- en: It is also possible to create neurons that we know what they â€œshouldâ€ mean.
    For example, we take the average of the embeddings of two tokens and see if the
    model can reconstruct them with an appropriate prompt. This helps us debug the
    method. We can gauge its reaction to different variants of neurons and improve
    prompts based on them for the actual neurons we care about.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: ä¹Ÿå¯ä»¥åˆ›å»ºæˆ‘ä»¬çŸ¥é“å®ƒä»¬â€œåº”è¯¥â€æ„å‘³ç€ä»€ä¹ˆçš„ç¥ç»å…ƒã€‚ä¾‹å¦‚ï¼Œæˆ‘ä»¬å–ä¸¤ä¸ªä»¤ç‰ŒåµŒå…¥çš„å¹³å‡å€¼ï¼Œçœ‹çœ‹æ¨¡å‹èƒ½å¦é€šè¿‡é€‚å½“çš„æç¤ºé‡å»ºå®ƒä»¬ã€‚è¿™æœ‰åŠ©äºæˆ‘ä»¬è°ƒè¯•æ–¹æ³•ã€‚æˆ‘ä»¬å¯ä»¥æ ¹æ®ä¸åŒå˜ä½“çš„ååº”æ¥è¯„ä¼°å…¶ååº”ï¼Œå¹¶åŸºäºè¿™äº›æ”¹è¿›æˆ‘ä»¬å…³å¿ƒçš„å®é™…ç¥ç»å…ƒçš„æç¤ºã€‚
- en: In the following example, we add together the token embedding of the â€œchildrenâ€
    and â€œdogâ€. Then we apply a speaking probe to it.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä¸‹é¢çš„ç¤ºä¾‹ä¸­ï¼Œæˆ‘ä»¬å°†â€œchildrenâ€å’Œâ€œdogâ€çš„ä»¤ç‰ŒåµŒå…¥åŠ åœ¨ä¸€èµ·ã€‚ç„¶åæˆ‘ä»¬å¯¹å…¶åº”ç”¨ä¸€ä¸ªè¯´è¯æ¢é’ˆã€‚
- en: We can use this neuron as a toy example of the way the probe treats such polysemous
    neurons. Remember, however, the behavior may differ depending on the word.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥ç”¨è¿™ä¸ªç¥ç»å…ƒä½œä¸ºæ¢é’ˆå¤„ç†å¤šä¹‰ç¥ç»å…ƒçš„ç©å…·ç¤ºä¾‹ã€‚ä¸è¿‡è¦è®°ä½ï¼Œè¡Œä¸ºå¯èƒ½ä¼šæ ¹æ®è¯æ±‡çš„ä¸åŒè€Œæœ‰æ‰€ä¸åŒã€‚
- en: '[PRE12]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: The probe handles the ambiguity in the neuron moderately well. The samples seem
    to refer to one token, but then they might get confused and talk about the other.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: æ¢é’ˆå¯¹ç¥ç»å…ƒçš„æ¨¡ç³Šæ€§å¤„ç†å¾—ç›¸å½“å¥½ã€‚æ ·æœ¬ä¼¼ä¹æŒ‡å‘ä¸€ä¸ªä»¤ç‰Œï¼Œä½†ç„¶åå¯èƒ½ä¼šæ··æ·†å¹¶è®¨è®ºå¦ä¸€ä¸ªã€‚
- en: 'Letâ€™s see what happens when we combine some more challenging tokens: â€œgoogleâ€
    and â€œshowâ€. This time we use a different prompt:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬çœ‹çœ‹å½“æˆ‘ä»¬ç»“åˆä¸€äº›æ›´å…·æŒ‘æˆ˜æ€§çš„ä»¤ç‰Œæ—¶ä¼šå‘ç”Ÿä»€ä¹ˆï¼šâ€œgoogleâ€å’Œâ€œshowâ€ã€‚è¿™æ¬¡æˆ‘ä»¬ä½¿ç”¨ä¸€ä¸ªä¸åŒçš„æç¤ºï¼š
- en: '[PRE13]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'It looks like weâ€™ve run out of luck here. The word â€œshowâ€ seems to have been
    absorbed by the word â€œgoogleâ€. Letâ€™s try to see if we can mitigate this by setting
    a smaller coefficient to the token embedding of â€œgoogleâ€ â€” we multiply it by 0.9
    and get:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: çœ‹èµ·æ¥æˆ‘ä»¬åœ¨è¿™é‡Œè¿æ°”ç”¨å°½äº†ã€‚è¯è¯­â€œshowâ€ä¼¼ä¹å·²ç»è¢«â€œgoogleâ€å¸æ”¶äº†ã€‚æˆ‘ä»¬æ¥è¯•ç€é€šè¿‡å°†â€œgoogleâ€çš„ä»¤ç‰ŒåµŒå…¥ç³»æ•°è®¾ç½®ä¸ºæ›´å°çš„å€¼æ¥ç¼“è§£è¿™ä¸ªé—®é¢˜â€”â€”æˆ‘ä»¬å°†å…¶ä¹˜ä»¥0.9ï¼Œå¾—åˆ°ï¼š
- en: '[PRE14]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Seems it can recover â€œ*showâ€* and *â€œgiggleâ€* as a distorted version of *â€œgoogleâ€*.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: ä¼¼ä¹å®ƒå¯ä»¥å°†â€œ*showâ€*å’Œ*â€œgiggleâ€*æ¢å¤ä¸º*â€œgoogleâ€*çš„æ‰­æ›²ç‰ˆæœ¬ã€‚
- en: Discussion
  id: totrans-76
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: è®¨è®º
- en: Potential of the Method
  id: totrans-77
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ–¹æ³•çš„æ½œåŠ›
- en: 'The distinctive features that we want to capitalize on with this method are:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¸Œæœ›åˆ©ç”¨æ­¤æ–¹æ³•çš„ç‹¬ç‰¹ç‰¹å¾æ˜¯ï¼š
- en: '***Natural language output***: both an advantage and disadvantage, it makes
    the output harder to evaluate, but it provides much greater flexibility than other
    methods.'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***è‡ªç„¶è¯­è¨€è¾“å‡º***ï¼šæ—¢æ˜¯ä¼˜ç‚¹ä¹Ÿæ˜¯ç¼ºç‚¹ï¼Œå®ƒä½¿å¾—è¾“å‡ºæ›´éš¾è¯„ä¼°ï¼Œä½†æ¯”å…¶ä»–æ–¹æ³•æä¾›äº†æ›´å¤§çš„çµæ´»æ€§ã€‚'
- en: '***Inherent ability to manipulate latent representations***: we use the modelâ€™s
    own capabilities of manipulating its latent representations. We assume they share
    the same latent space with the model parameters due to the residual stream view.
    Other techniques need to be trained or adjusted in some other way to the modelâ€™s
    latent space in order to â€œunderstandâ€ it. The model is capable of decoding its
    own states naturally, which can be useful for interpretation.'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***æ“ä½œæ½œåœ¨è¡¨å¾çš„å›ºæœ‰èƒ½åŠ›***ï¼šæˆ‘ä»¬ä½¿ç”¨æ¨¡å‹è‡ªèº«æ“ä½œå…¶æ½œåœ¨è¡¨å¾çš„èƒ½åŠ›ã€‚æˆ‘ä»¬å‡è®¾å®ƒä»¬ä¸æ¨¡å‹å‚æ•°å…±äº«ç›¸åŒçš„æ½œåœ¨ç©ºé—´ï¼Œå› ä¸ºæ®‹å·®æµè§†å›¾ã€‚å…¶ä»–æŠ€æœ¯éœ€è¦ç»è¿‡è®­ç»ƒæˆ–ä»¥å…¶ä»–æ–¹å¼è°ƒæ•´åˆ°æ¨¡å‹çš„æ½œåœ¨ç©ºé—´ä»¥â€œç†è§£â€å®ƒã€‚æ¨¡å‹èƒ½å¤Ÿè‡ªç„¶è§£ç è‡ªèº«çŠ¶æ€ï¼Œè¿™å¯¹è§£é‡Šæ˜¯æœ‰ç”¨çš„ã€‚'
- en: In general, there is not much research on continuous vectors as first-class
    citizens in transformers. While ideas like prompt tuning [*Lester et al., 2021*],
    and exciting ideas like *Hao et al.* [*2022*] pass continuous inputs to the model,
    they require training to work and they are not used zero-shot. A central motif
    in this work is the investigation of whether some continuous vectors can be used
    like natural tokens without further training â€” under the assumption that they
    use the same â€œlanguageâ€ as the model.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€èˆ¬æ¥è¯´ï¼Œå…³äºå°†è¿ç»­å‘é‡ä½œä¸ºå˜æ¢å™¨ä¸­çš„ä¸€ç­‰å…¬æ°‘çš„ç ”ç©¶ä¸å¤šã€‚è™½ç„¶åƒæç¤ºè°ƒä¼˜ [*Lester et al., 2021*] è¿™æ ·çš„æƒ³æ³•ï¼Œä»¥åŠåƒ *Hao
    et al.* [*2022*] è¿™æ ·çš„ä»¤äººå…´å¥‹çš„æƒ³æ³•å°†è¿ç»­è¾“å…¥ä¼ é€’ç»™æ¨¡å‹ï¼Œä½†å®ƒä»¬éœ€è¦è®­ç»ƒæ‰èƒ½å·¥ä½œï¼Œä¸”ä¸èƒ½é›¶æ ·æœ¬ä½¿ç”¨ã€‚è¿™é¡¹å·¥ä½œä¸­çš„æ ¸å¿ƒä¸»é¢˜æ˜¯ç ”ç©¶æ˜¯å¦å¯ä»¥åœ¨ä¸è¿›ä¸€æ­¥è®­ç»ƒçš„æƒ…å†µä¸‹å°†æŸäº›è¿ç»­å‘é‡ç”¨ä½œè‡ªç„¶æ ‡è®°â€”â€”å‡è®¾å®ƒä»¬ä½¿ç”¨ä¸æ¨¡å‹ç›¸åŒçš„â€œè¯­è¨€â€ã€‚
- en: Another useful feature of this technique is that it uses the model more or less
    as a black box, without much technical work involved. It is easy to implement
    and understand. Casting interpretation as a generation problem, we can leverage
    literature on generation from mainstream NLP for future work. Similarly, hallucinations
    are a major concern in speaking probes, but we can hope to be able to apply mainstream
    research approaches in the future to this method.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ç§æŠ€æœ¯çš„å¦ä¸€ä¸ªæœ‰ç”¨ç‰¹æ€§æ˜¯ï¼Œå®ƒæˆ–å¤šæˆ–å°‘å°†æ¨¡å‹è§†ä¸ºé»‘ç®±ï¼Œè€Œä¸æ¶‰åŠå¤ªå¤šæŠ€æœ¯å·¥ä½œã€‚å®ƒæ˜“äºå®ç°å’Œç†è§£ã€‚å°†è§£é‡Šè§†ä¸ºç”Ÿæˆé—®é¢˜ï¼Œæˆ‘ä»¬å¯ä»¥åˆ©ç”¨ä¸»æµ NLP çš„ç”Ÿæˆæ–‡çŒ®æ¥è¿›è¡Œæœªæ¥çš„å·¥ä½œã€‚ç±»ä¼¼åœ°ï¼Œå¹»è§‰æ˜¯å‘è¨€æ¢é’ˆä¸­çš„ä¸€ä¸ªä¸»è¦é—®é¢˜ï¼Œä½†æˆ‘ä»¬å¯ä»¥å¸Œæœ›æœªæ¥å°†ä¸»æµç ”ç©¶æ–¹æ³•åº”ç”¨äºè¿™ç§æ–¹æ³•ã€‚
- en: In total, this is perhaps the *most modular* interpretability method â€” it does
    not rely on a specifically tailored algorithm, and it can adopt insights from
    other areas in NLP to improve, without losing breath. Also, it is easy to experiment
    with (even for less academically inclined practitioners) and the search space
    landscape is very different than with other methods.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: æ€»ä½“è€Œè¨€ï¼Œè¿™å¯èƒ½æ˜¯ *æœ€æ¨¡å—åŒ–* çš„å¯è§£é‡Šæ€§æ–¹æ³•â€”â€”å®ƒä¸ä¾èµ–äºç‰¹å®šçš„ç®—æ³•ï¼Œå¹¶ä¸”å¯ä»¥é‡‡ç”¨ NLP å…¶ä»–é¢†åŸŸçš„è§è§£è¿›è¡Œæ”¹è¿›ï¼Œè€Œä¸ä¼šä¸§å¤±æ•ˆç‡ã€‚æ­¤å¤–ï¼Œå®ƒæ˜“äºå®éªŒï¼ˆå³ä½¿æ˜¯å¯¹å­¦æœ¯ä¸å¤ªæ„Ÿå…´è¶£çš„ä»ä¸šè€…ï¼‰ï¼Œå¹¶ä¸”æœç´¢ç©ºé—´çš„æ ¼å±€ä¸å…¶ä»–æ–¹æ³•å¤§ç›¸å¾„åº­ã€‚
- en: Possible Future Directions
  id: totrans-84
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å¯èƒ½çš„æœªæ¥æ–¹å‘
- en: '***Eloquent. Too Eloquent***: language models are trained to produce eloquent
    explanations. Factuality is less worrisome to them. These eloquent explanations
    are not to be taken literally.'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***é›„è¾©ã€‚è¿‡äºé›„è¾©***ï¼šè¯­è¨€æ¨¡å‹è¢«è®­ç»ƒä»¥äº§ç”Ÿé›„è¾©çš„è§£é‡Šã€‚äº‹å®æ€§å¯¹å®ƒä»¬æ¥è¯´è¾ƒå°‘ä»¤äººæ‹…å¿§ã€‚è¿™äº›é›„è¾©çš„è§£é‡Šä¸åº”è¢«å­—é¢ç†è§£ã€‚'
- en: '***Layer Homogeneity***: in this article, we implicitly assume we can take
    parameters from different layers and they will react similarly to our prompts.
    It is possible that some layers are more amenable to use with speaking probes
    than others. We call this *layer homogeneity*. We need to be cautious in assuming
    that all layers can be treated the same with respect to our method.'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***å±‚åŒè´¨æ€§***ï¼šåœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬éšå«åœ°å‡è®¾æˆ‘ä»¬å¯ä»¥ä»ä¸åŒçš„å±‚è·å–å‚æ•°ï¼Œå®ƒä»¬ä¼šå¯¹æˆ‘ä»¬çš„æç¤ºåšå‡ºç±»ä¼¼çš„ååº”ã€‚å¯èƒ½æœ‰äº›å±‚æ¯”å…¶ä»–å±‚æ›´é€‚åˆä¸å‘è¨€æ¢é’ˆä¸€èµ·ä½¿ç”¨ã€‚æˆ‘ä»¬ç§°ä¹‹ä¸º
    *å±‚åŒè´¨æ€§*ã€‚æˆ‘ä»¬éœ€è¦è°¨æ…åœ°å‡è®¾æ‰€æœ‰å±‚åœ¨æˆ‘ä»¬çš„æ–¹æ³•ä¸­å¯ä»¥è¢«ç›¸åŒå¯¹å¾…ã€‚'
- en: '***Neuron Polysemy***: especially in face of word collapse, it seems that neurons
    that carry multiple unrelated interpretations will have to be sampled multiple
    times to account for all their different meanings. We would like to be able to
    extract the different meanings more faithfully and â€œin one sittingâ€™â€™.'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***ç¥ç»å…ƒå¤šä¹‰æ€§***ï¼šå°¤å…¶æ˜¯åœ¨é¢å¯¹è¯æ±‡å´©æºƒæ—¶ï¼Œä¼¼ä¹æºå¸¦å¤šä¸ªæ— å…³è§£é‡Šçš„ç¥ç»å…ƒéœ€è¦å¤šæ¬¡é‡‡æ ·ï¼Œä»¥æ¶µç›–æ‰€æœ‰ä¸åŒçš„å«ä¹‰ã€‚æˆ‘ä»¬å¸Œæœ›èƒ½å¤Ÿæ›´çœŸå®åœ°æå–è¿™äº›ä¸åŒçš„å«ä¹‰ï¼Œå¹¶â€œä¸€æ¬¡æ€§â€å®Œæˆã€‚'
- en: '***Better Prompts***: this is not the main part of our work, but many papers
    show the benefits of using carefully engineered prompts [e.g., *Liu et al., 2021*].'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***æ›´å¥½çš„æç¤º***ï¼šè¿™ä¸æ˜¯æˆ‘ä»¬å·¥ä½œçš„ä¸»è¦éƒ¨åˆ†ï¼Œä½†è®¸å¤šè®ºæ–‡å±•ç¤ºäº†ä½¿ç”¨ç²¾å¿ƒè®¾è®¡çš„æç¤ºçš„å¥½å¤„ [ä¾‹å¦‚ï¼Œ*Liu et al., 2021*]ã€‚'
- en: '***Other Types of Concepts***: we have mainly discussed neurons that represent
    a category or a concept in natural language. We know that language models can
    work with code, but we havenâ€™t considered this type of knowledge in this article.
    Also, it is interesting to use speaking probes to locate facts in model parameters.
    Facts might require a number of parameters working in unison â€” so it will be interesting
    to locate them and find prompts that will be able to extract these facts.'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***å…¶ä»–ç±»å‹çš„æ¦‚å¿µ***ï¼šæˆ‘ä»¬ä¸»è¦è®¨è®ºäº†è¡¨ç¤ºè‡ªç„¶è¯­è¨€ä¸­çš„ç±»åˆ«æˆ–æ¦‚å¿µçš„ç¥ç»å…ƒã€‚æˆ‘ä»¬çŸ¥é“è¯­è¨€æ¨¡å‹å¯ä»¥å¤„ç†ä»£ç ï¼Œä½†æˆ‘ä»¬åœ¨è¿™ç¯‡æ–‡ç« ä¸­æ²¡æœ‰è€ƒè™‘è¿™ç±»çŸ¥è¯†ã€‚æ­¤å¤–ï¼Œä½¿ç”¨å‘è¨€æ¢é’ˆæ¥å®šä½æ¨¡å‹å‚æ•°ä¸­çš„äº‹å®ä¹Ÿå¾ˆæœ‰è¶£ã€‚äº‹å®å¯èƒ½éœ€è¦å¤šä¸ªå‚æ•°ååŒå·¥ä½œâ€”â€”å› æ­¤ï¼Œå®šä½è¿™äº›äº‹å®å¹¶æ‰¾åˆ°èƒ½å¤Ÿæå–è¿™äº›äº‹å®çš„æç¤ºå°†ä¼šå¾ˆæœ‰è¶£ã€‚'
- en: 'If you do follow-up work, please cite as:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä½ è¿›è¡Œåç»­å·¥ä½œï¼Œè¯·å¼•ç”¨å¦‚ä¸‹ï¼š
- en: '[PRE15]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'You are also welcome to follow me on Twitter:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ ä¹Ÿå¯ä»¥åœ¨ Twitter ä¸Šå…³æ³¨æˆ‘ï¼š
- en: '[](https://mobile.twitter.com/guy__dar?source=post_page-----7a3dc6cb33d6--------------------------------)
    [## Guy Dar (guy__dar) / Twitter'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://mobile.twitter.com/guy__dar?source=post_page-----7a3dc6cb33d6--------------------------------)
    [## Guy Dar (guy__dar) / Twitter'
- en: Twitter](https://mobile.twitter.com/guy__dar?source=post_page-----7a3dc6cb33d6--------------------------------)
  id: totrans-94
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Twitter](https://mobile.twitter.com/guy__dar?source=post_page-----7a3dc6cb33d6--------------------------------)
- en: '*This is not a direct follow-up, but you might be also interested in my other
    blog post on a related paper I worked on with collaborators:*'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '*è¿™ä¸æ˜¯ç›´æ¥çš„åç»­ï¼Œä½†ä½ å¯èƒ½è¿˜å¯¹æˆ‘ä¸åˆä½œè€…åˆä½œçš„ç›¸å…³è®ºæ–‡çš„å¦ä¸€ç¯‡åšå®¢å¸–å­æ„Ÿå…´è¶£ï¼š*'
- en: '[](/analyzing-transformers-in-embedding-space-explained-ef72130a6844?source=post_page-----7a3dc6cb33d6--------------------------------)
    [## Analyzing Transformers in Embedding Space â€” Explained'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/analyzing-transformers-in-embedding-space-explained-ef72130a6844?source=post_page-----7a3dc6cb33d6--------------------------------)
    [## Analyzing Transformers in Embedding Space â€” Explained'
- en: In this post, I present the paper â€œAnalyzing Transformers in Embedding Spaceâ€
    (2022) by Guy Dar, Mor Geva, Ankit Guptaâ€¦
  id: totrans-97
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: åœ¨è¿™ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘ä»‹ç»äº†Guy Darã€Mor Gevaã€Ankit Guptaç­‰äººäº2022å¹´å‘è¡¨çš„è®ºæ–‡ã€Šåˆ†æåµŒå…¥ç©ºé—´ä¸­çš„è½¬æ¢å™¨ã€‹ã€‚
- en: towardsdatascience.com](/analyzing-transformers-in-embedding-space-explained-ef72130a6844?source=post_page-----7a3dc6cb33d6--------------------------------)
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/analyzing-transformers-in-embedding-space-explained-ef72130a6844?source=post_page-----7a3dc6cb33d6--------------------------------)
- en: References
  id: totrans-99
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: References
- en: D. Dai, L. Dong, Y. Hao, Z. Sui, B. Chang, and F. Wei. *Knowledge neurons in
    pretrained transformers*, 2021\. URL [https://arxiv.org/abs/2104.08696](https://arxiv.org/abs/2104.08696).
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: Dai D, Dong L, Hao Y, Sui Z, Chang B, ä»¥åŠ Wei F. *é¢„è®­ç»ƒè½¬æ¢å™¨ä¸­çš„çŸ¥è¯†ç¥ç»å…ƒ*ï¼Œ2021\. URL [https://arxiv.org/abs/2104.08696](https://arxiv.org/abs/2104.08696)ã€‚
- en: G. Dar, M. Geva, A. Gupta, and J. Berant. *Analyzing transformers in embedding
    space*, 2022\. URL [https://arxiv.org/abs/2209.02535](https://arxiv.org/abs/2209.02535).
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: Dar G, Geva M, Gupta A, ä»¥åŠ Berant J.ã€Šåœ¨åµŒå…¥ç©ºé—´ä¸­åˆ†æè½¬æ¢å™¨ã€‹ï¼Œ2022\. URL [https://arxiv.org/abs/2209.02535](https://arxiv.org/abs/2209.02535)ã€‚
- en: N. Elhage, N. Nanda, C. Olsson, T. Henighan, N. Joseph, B. Mann, A. Askell,
    Y. Bai, A. Chen, T. Conerly, N. DasSarma, D. Drain, D. Ganguli, Z. Hatfield-Dodds,
    D. Hernandez, A. Jones, J. Kernion, L. Lovitt, K. Ndousse, D. Amodei, T. Brown,
    J. Clark, J. Kaplan, S. McCandlish, and C. Olah. *A mathematical framework for
    transformer circuits*, 2021\. URL [https://transformer-circuits.pub/2021/framework/index.html](https://transformer-circuits.pub/2021/framework/index.html).
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: Elhage N, Nanda N, Olsson C, Henighan T, Joseph N, Mann B, Askell A, Bai Y,
    Chen A, Conerly T, DasSarma N, Drain D, Ganguli D, Hatfield-Dodds Z, Hernandez
    D, Jones A, Kernion J, Lovitt L, Ndousse K, Amodei D, Brown T, Clark J, Kaplan
    J, McCandlish S, ä»¥åŠ Olah C. *è½¬æ¢å™¨ç”µè·¯çš„æ•°å­¦æ¡†æ¶*ï¼Œ2021\. URL [https://transformer-circuits.pub/2021/framework/index.html](https://transformer-circuits.pub/2021/framework/index.html)ã€‚
- en: M. Geva, R. Schuster, J. Berant, and O. Levy. *Transformer feed-forward layers
    are key-value memories*, 2020\. URL [https://arxiv.org/abs/2012.14913](https://arxiv.org/abs/2012.14913).
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: Geva M, Schuster R, Berant J, ä»¥åŠ Levy O. *è½¬æ¢å™¨å‰é¦ˆå±‚æ˜¯é”®-å€¼å­˜å‚¨å™¨*ï¼Œ2020\. URL [https://arxiv.org/abs/2012.14913](https://arxiv.org/abs/2012.14913)ã€‚
- en: 'M. Geva, A. Caciularu, G. Dar, P. Roit, S. Sadde, M. Shlain, B. Tamir, and
    Y. Goldberg. *Lm-debugger: An interactive tool for inspection and intervention
    in transformer-based language models*. arXiv preprint arXiv:2204.12130, 2022a.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 'Geva M, Caciularu A, Dar G, Roit P, Sadde S, Shlain M, Tamir B, ä»¥åŠ Goldberg
    Y.ã€ŠLm-debugger: ä¸€ä¸ªäº¤äº’å¼å·¥å…·ï¼Œç”¨äºæ£€æŸ¥å’Œå¹²é¢„åŸºäºè½¬æ¢å™¨çš„è¯­è¨€æ¨¡å‹ã€‹[Lm-debugger: An interactive tool for
    inspection and intervention in transformer-based language models]. arXiv preprint
    arXiv:2204.12130, 2022aã€‚'
- en: M. Geva, A. Caciularu, K. R. Wang, and Y. Goldberg. *Transformer feed-forward
    layers build predictions by promoting concepts in the vocabulary space*, 2022b.
    URL [https://arxiv.org/abs/2203.14680](https://arxiv.org/abs/2203.14680).
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: Geva M, Caciularu A, Wang K R, ä»¥åŠ Goldberg Y.ã€Šè½¬æ¢å™¨å‰é¦ˆå±‚é€šè¿‡æ¨åŠ¨è¯æ±‡ç©ºé—´ä¸­çš„æ¦‚å¿µæ¥æ„å»ºé¢„æµ‹*ï¼Œ2022bã€‚URL
    [https://arxiv.org/abs/2203.14680](https://arxiv.org/abs/2203.14680).
- en: Y. Hao, H. Song, L. Dong, S. Huang, Z. Chi, W. Wang, S. Ma, and F. Wei. *Language
    models are general-purpose interfaces*, 2022\. URL [https://arxiv.org/abs/2206.06336](https://arxiv.org/abs/2206.06336).
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: Hao Y, Song H, Dong L, Huang S, Chi Z, Wang W, Ma S, ä»¥åŠ Wei F. *è¯­è¨€æ¨¡å‹æ˜¯é€šç”¨æ¥å£*ï¼Œ2022\.
    URL [https://arxiv.org/abs/2206.06336](https://arxiv.org/abs/2206.06336)ã€‚
- en: 'A. Kadar, G. Chrupala, and A. Alishahi. *Representation of Linguistic Form
    and Function in Recurrent Neural Networks*. Computational Linguistics, 43(4):761â€“780,
    12 2017\. ISSN 0891â€“2017\. doi: 10.1162/COLI a 00300\. URL [https://doi.org/10.1162/COLI_a_00300](https://doi.org/10.1162/COLI_a_00300).'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 'Kadar A, Chrupala G, ä»¥åŠ Alishahi A. *åœ¨é€’å½’ç¥ç»ç½‘ç»œä¸­è¡¨ç¤ºè¯­è¨€å½¢å¼å’Œå‡½æ•°*ã€‚è®¡ç®—è¯­è¨€å­¦, 43(4):761â€“780,
    2017ã€‚ISSN 0891â€“2017. doi: 10.1162/COLI a 00300. URL [https://doi.org/10.1162/COLI_a_00300](https://doi.org/10.1162/COLI_a_00300)ã€‚'
- en: B. Lester, R. Al-Rfou, and N. Constant. *The power of scale for parameter-efficient
    prompt tuning*, 2021\. URL [https://arxiv.org/abs/2104.08691](https://arxiv.org/abs/2104.08691).
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: B. Lester, R. Al-Rfou å’Œ N. Constant. *å‚æ•°é«˜æ•ˆæç¤ºè°ƒæ•´çš„è§„æ¨¡åŠ›é‡*ï¼Œ2021\. ç½‘å€ [https://arxiv.org/abs/2104.08691](https://arxiv.org/abs/2104.08691)ã€‚
- en: J. Liu, D. Shen, Y. Zhang, B. Dolan, L. Carin, and W. Chen. *What makes good
    in-context examples for gpt-3*? CoRR, abs/2101.06804, 2021\. URL [https://arxiv.org/abs/2101.06804](https://arxiv.org/abs/2101.06804).
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: J. Liu, D. Shen, Y. Zhang, B. Dolan, L. Carin å’Œ W. Chen. *ä»€ä¹ˆæ„æˆäº† GPT-3 çš„ä¼˜ç§€ä¸Šä¸‹æ–‡ç¤ºä¾‹*ï¼ŸCoRR,
    abs/2101.06804, 2021\. ç½‘å€ [https://arxiv.org/abs/2101.06804](https://arxiv.org/abs/2101.06804)ã€‚
- en: S. Na, Y. J. Choe, D.-H. Lee, and G. Kim. *Discovery of natural language concepts
    in individual units of cnns*, 2019\. URL [https://arxiv.org/abs/1902.07249](https://arxiv.org/abs/1902.07249).
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: S. Na, Y. J. Choe, D.-H. Lee å’Œ G. Kim. *å‘ç° CNN å•å…ƒä¸­çš„è‡ªç„¶è¯­è¨€æ¦‚å¿µ*ï¼Œ2019\. ç½‘å€ [https://arxiv.org/abs/1902.07249](https://arxiv.org/abs/1902.07249)ã€‚
- en: 'nostalgebraist. interpreting gpt: the logit lens, 2020\. URL [https://www.lesswrong.com/posts/AcKRB8wDpdaN6v6ru/interpreting-gpt-the-logit-lens](https://www.lesswrong.com/posts/AcKRB8wDpdaN6v6ru/interpreting-gpt-the-logit-lens).'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: nostalgebraist. è§£é‡Š GPTï¼šlogit è§†è§’ï¼Œ2020\. ç½‘å€ [https://www.lesswrong.com/posts/AcKRB8wDpdaN6v6ru/interpreting-gpt-the-logit-lens](https://www.lesswrong.com/posts/AcKRB8wDpdaN6v6ru/interpreting-gpt-the-logit-lens)ã€‚
