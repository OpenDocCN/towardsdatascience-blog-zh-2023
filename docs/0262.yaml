- en: 'Speaking Probes: Self-Interpreting Models?'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 说话探针：自解释模型？
- en: 原文：[https://towardsdatascience.com/speaking-probes-self-interpreting-models-7a3dc6cb33d6?source=collection_archive---------20-----------------------#2023-01-16](https://towardsdatascience.com/speaking-probes-self-interpreting-models-7a3dc6cb33d6?source=collection_archive---------20-----------------------#2023-01-16)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/speaking-probes-self-interpreting-models-7a3dc6cb33d6?source=collection_archive---------20-----------------------#2023-01-16](https://towardsdatascience.com/speaking-probes-self-interpreting-models-7a3dc6cb33d6?source=collection_archive---------20-----------------------#2023-01-16)
- en: Can language models aid in their interpretation?
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 语言模型能帮助解释自己吗？
- en: '[](https://guydar.medium.com/?source=post_page-----7a3dc6cb33d6--------------------------------)[![Guy
    Dar](../Images/0a3b1ddd33d595e97e7a0dad551b2708.png)](https://guydar.medium.com/?source=post_page-----7a3dc6cb33d6--------------------------------)[](https://towardsdatascience.com/?source=post_page-----7a3dc6cb33d6--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----7a3dc6cb33d6--------------------------------)
    [Guy Dar](https://guydar.medium.com/?source=post_page-----7a3dc6cb33d6--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://guydar.medium.com/?source=post_page-----7a3dc6cb33d6--------------------------------)[![Guy
    Dar](../Images/0a3b1ddd33d595e97e7a0dad551b2708.png)](https://guydar.medium.com/?source=post_page-----7a3dc6cb33d6--------------------------------)[](https://towardsdatascience.com/?source=post_page-----7a3dc6cb33d6--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----7a3dc6cb33d6--------------------------------)
    [Guy Dar](https://guydar.medium.com/?source=post_page-----7a3dc6cb33d6--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ffab216dbde3e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fspeaking-probes-self-interpreting-models-7a3dc6cb33d6&user=Guy+Dar&userId=fab216dbde3e&source=post_page-fab216dbde3e----7a3dc6cb33d6---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----7a3dc6cb33d6--------------------------------)
    ·16 min read·Jan 16, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F7a3dc6cb33d6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fspeaking-probes-self-interpreting-models-7a3dc6cb33d6&user=Guy+Dar&userId=fab216dbde3e&source=-----7a3dc6cb33d6---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ffab216dbde3e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fspeaking-probes-self-interpreting-models-7a3dc6cb33d6&user=Guy+Dar&userId=fab216dbde3e&source=post_page-fab216dbde3e----7a3dc6cb33d6---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----7a3dc6cb33d6--------------------------------)
    ·16分钟阅读·2023年1月16日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F7a3dc6cb33d6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fspeaking-probes-self-interpreting-models-7a3dc6cb33d6&user=Guy+Dar&userId=fab216dbde3e&source=-----7a3dc6cb33d6---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F7a3dc6cb33d6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fspeaking-probes-self-interpreting-models-7a3dc6cb33d6&source=-----7a3dc6cb33d6---------------------bookmark_footer-----------)![](../Images/cd19726fc34b00c30807dcb8a268bad7.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F7a3dc6cb33d6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fspeaking-probes-self-interpreting-models-7a3dc6cb33d6&source=-----7a3dc6cb33d6---------------------bookmark_footer-----------)![](../Images/cd19726fc34b00c30807dcb8a268bad7.png)'
- en: Photo by [Kane Reinholdtsen](https://unsplash.com/@kanereinholdtsen?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 照片由 [Kane Reinholdtsen](https://unsplash.com/@kanereinholdtsen?utm_source=medium&utm_medium=referral)
    提供，来源于 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: '*In this post, I experiment with the idea that language models can be coaxed
    to explain vectors coming from their parameters. It turns out to work better than
    you might expect, but still much work needs to be done.*'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '*在这篇文章中，我尝试了一个想法，即语言模型可以被引导解释来自其参数的向量。结果比你预期的要好，但仍然需要做很多工作。*'
- en: '*As is customary in scientific papers, I use “we” instead of “I” (among other
    reasons, because it makes the text sound a bit less self-centered..).*'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '*与科学论文中的惯例一样，我使用“我们”而不是“我”（部分原因是因为这样可以让文本听起来不那么以自我为中心..）。*'
- en: '*This is not really a complete work, but more like a preliminary report on
    an idea I believe might be useful, and should be shared. As a result, I only carried
    out basic experiments to sanity-check the method. I hope other researchers would
    take up the work from where I started, and see if the limitations of the methods
    I suggest are surmountable. This work is aimed at professionals, but anyone with
    decent knowledge of transformers should be comfortable reading it.*'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '*这实际上不是一项完整的工作，而更像是一个初步报告，关于我认为可能有用的一个想法，应该共享。因此，我只进行了基本实验以检验方法的合理性。我希望其他研究人员能从我开始的地方继续工作，看看我建议的方法的局限性是否可以克服。这项工作面向专业人士，但任何具有良好变换器知识的人都应该能轻松阅读。*'
- en: Introduction
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: In recent years, many interpretability methods have been developed in Natural
    Language Processing [*Kadar et al., 2017; Na et al., 2019; Geva et al., 2020;
    Dar et al., 2022*]. In parallel, strong language models have taken the field by
    storm. One may wonder if strong language skills allow language models to communicate
    about their inner state. This work is a brief report on our explorations of this
    conjecture. In this work, we will design natural language prompts and inject model
    parameters as virtual tokens in the input. The prompts are designed to instruct
    the model to explain words — but instead of a real word they are given a virtual
    token representing a model parameter. The model then generates a sequence continuing
    the prompt. We will observe this technique’s ability to explain model parameters
    which we have existing explanations for. We call the new technique *“speaking
    probes”*. We will also discuss on a high-level possible justifications for why
    one might expect the method to work.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，许多可解释性方法在自然语言处理领域得到了发展 [*Kadar et al., 2017; Na et al., 2019; Geva et al.,
    2020; Dar et al., 2022*]。与此同时，强大的语言模型在该领域引起了轰动。有人可能会想知道强大的语言技能是否使语言模型能够对其内在状态进行交流。这项工作是对这一猜测的简要报告。在这项工作中，我们将设计自然语言提示，并将模型参数作为虚拟标记注入输入中。提示的目的是指示模型解释单词——但不是一个真实的单词，而是一个表示模型参数的虚拟标记。然后，模型生成一个继续提示的序列。我们将观察该技术解释模型参数的能力，我们已有现有解释。我们称这种新技术为*“发言探针”*。我们还将在高层次上讨论为什么可能期望这种方法有效的理由。
- en: '![](../Images/fd43ad0917dede573b6130631c936a7a.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fd43ad0917dede573b6130631c936a7a.png)'
- en: 'Figure: Illustration of a speaking probes'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 图示：发言探针的插图
- en: Interpretability researchers are encouraged to use speaking probes as a tool
    to guide their analysis. We *do* *not* suggest relying on their answers indiscriminately,
    as they are not sufficiently grounded. However, they have the important advantage
    of possessing the expressive power of natural language. Our queries are out of
    distribution for the model in the zero-shot case as it was only trained with real
    tokens. However, we hypothesize its inherent skills at manipulating its representations
    will make it easy to learn the new task.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 鼓励可解释性研究人员使用发言探针作为指导分析的工具。我们*并不* *建议* 不加区分地依赖其回答，因为它们并不完全可靠。然而，它们具有自然语言的表达能力这一重要优势。我们的查询在零样本情况下对模型来说是超出分布的，因为它仅用真实的标记进行训练。然而，我们假设它在操作其表示方面的固有技能将使其容易学习新任务。
- en: 'We provide the following two resources for researchers interested in exploring
    this technique for themselves:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 我们为有兴趣自己探索这一技术的研究人员提供以下两个资源：
- en: '**Code:** [https://github.com/guyd1995/speaking-probes](https://github.com/guyd1995/speaking-probes)'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**代码：** [https://github.com/guyd1995/speaking-probes](https://github.com/guyd1995/speaking-probes)'
- en: '[](https://github.com/guyd1995/speaking-probes?source=post_page-----7a3dc6cb33d6--------------------------------)
    [## GitHub - guyd1995/speaking-probes'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://github.com/guyd1995/speaking-probes?source=post_page-----7a3dc6cb33d6--------------------------------)
    [## GitHub - guyd1995/speaking-probes'
- en: You can't perform that action at this time. You signed in with another tab or
    window. You signed out in another tab or…
  id: totrans-20
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 目前无法执行该操作。您在其他标签页或窗口中登录。您在其他标签页中退出了登录…
- en: github.com](https://github.com/guyd1995/speaking-probes?source=post_page-----7a3dc6cb33d6--------------------------------)
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '[github.com](https://github.com/guyd1995/speaking-probes?source=post_page-----7a3dc6cb33d6--------------------------------)'
- en: '**Demo** (🤗 *HuggingFace*): [https://huggingface.co/spaces/guy-tau/speaking-probes](https://huggingface.co/spaces/guy-tau/speaking-probes)
    — This can be slow, so apart from basic experimentation, it is better to open
    one of the notebooks in the repository on Colab'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**演示** (🤗 *HuggingFace*): [https://huggingface.co/spaces/guy-tau/speaking-probes](https://huggingface.co/spaces/guy-tau/speaking-probes)
    — 这可能会很慢，因此除了基本实验外，最好在 Colab 上打开存储库中的一个笔记本'
- en: 'Background: The Residual Stream'
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 背景：残差流
- en: '*This has been explained in more detail in the background section of my previous
    post:* [*Analyzing Transformers in Embedding Space — Explained*](/analyzing-transformers-in-embedding-space-explained-ef72130a6844)'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '*这在我之前的帖子中的背景部分已更详细地解释：* [*在嵌入空间中分析 Transformers — 解释*](/analyzing-transformers-in-embedding-space-explained-ef72130a6844)'
- en: We rely on a useful view of the transformer through its residual connections
    originally introduced in *nostalgebraist* [*2020*]. Specifically, each layer takes
    a hidden state as input and adds information to the hidden state through its residual
    connection. Under this view, the hidden state is a residual stream passed along
    the layers, from which information is read, and to which information is written
    at each layer. *Elhage et al.* [*2021*] and *Geva et al.* [*2022b*] observed that
    the residual stream is often barely updated in the last layers, and thus the final
    prediction is determined in early layers and the hidden state is mostly passed
    through the later layers. An exciting consequence of the residual stream view
    is that we can project hidden states in every layer into embedding space by multiplying
    the hidden state with the embedding matrix *E*, treating the hidden state as if
    it were the output of the last layer. *Geva et al.* [*2022a*] used this approach
    to interpret the prediction of transformer-based language models, and we follow
    a similar approach.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 我们依赖于通过其残差连接对变换器的有用视图，这一视图最初由 *nostalgebraist* [*2020*] 提出。具体而言，每一层将隐藏状态作为输入，并通过其残差连接向隐藏状态添加信息。在这种视图下，隐藏状态是沿层传递的残差流，从中读取信息，并在每一层写入信息。*Elhage
    等人* [*2021*] 和 *Geva 等人* [*2022b*] 观察到残差流在最后几层通常几乎没有更新，因此最终预测是在早期层中确定的，隐藏状态大多在后续层中传递。残差流视图的一个令人兴奋的结果是，我们可以通过将隐藏状态与嵌入矩阵
    *E* 相乘，将每层的隐藏状态投影到嵌入空间，将隐藏状态视为最后一层的输出。*Geva 等人* [*2022a*] 使用这种方法来解释基于变换器的语言模型的预测，我们也采取了类似的方法。
- en: '![](../Images/ddefd4b7d10a39c3803b93127af7134d.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ddefd4b7d10a39c3803b93127af7134d.png)'
- en: 'Figure: the residual stream view — visually'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 图：残差流视图——可视化
- en: Showcasing Speaking Probes
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 展示说话探测
- en: Overview
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 概述
- en: Our intuition builds on the residual stream view. In the residual stream view,
    parameters of the models are added to the hidden state on a more or less equal
    footing with token embeddings. More generally, the residual view hints that there’s
    a good case for considering parameter vectors, hidden states, and token embeddings
    to be using the same “language”. “Syntactically”, we can use any continuous representation
    — be it a parameter vector or hidden state — as a virtual token. We will use the
    term “*neuron”* interchangeably with “*virtual token”*.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的直觉基于残差流视图。在残差流视图中，模型的参数与 token 嵌入在隐藏状态上以或多或少平等的方式相加。更一般地，残差视图暗示考虑参数向量、隐藏状态和
    token 嵌入使用相同“语言”的理由。*从“句法上”*来看，我们可以使用任何连续表示——无论是参数向量还是隐藏状态——作为虚拟 token。我们将“*神经元*”与“*虚拟
    token*”交替使用。
- en: We will focus on parameters in this article, as hidden states seem to be more
    complicated to analyze — which stands to reason since they are mixtures of parameters.
    We show that parameter vectors can be used alongside token embeddings in the input
    prompt and produce meaningful responses. We hypothesize a neuron eventually collapses
    into a token that is related to the concepts it encodes.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 本文将重点讨论参数，因为隐藏状态似乎更复杂——这很有道理，因为它们是参数的混合。我们展示了参数向量可以与 token 嵌入一起在输入提示中使用，并产生有意义的响应。我们假设神经元最终会崩溃成与其编码概念相关的
    token。
- en: Our goal is to use the strong communication skills language models possess for
    expressing their latent knowledge. We suggest a few prompts in which the model
    is requested to explain a word. Instead of a word, it is given a virtual token
    representing a vector in the parameters. We represent the virtual token in the
    prompt by the label **<neuron>** (when running the model, its token embedding
    is simply replaced with the neuron we want to interpret). We then generate the
    continuation of the prompt, which is the language model’s response.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的目标是利用语言模型具备的强大沟通技能来表达其潜在知识。我们建议几个提示，要求模型解释一个词。模型不会直接接收到一个词，而是一个代表参数向量的虚拟
    token。我们用标签 **<neuron>** 在提示中表示虚拟 token（在运行模型时，其 token 嵌入会被我们想要解释的神经元替换）。然后，我们生成提示的继续部分，这就是语言模型的响应。
- en: Prompts
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 提示
- en: '[PRE0]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '[PRE1]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '[PRE2]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[PRE3]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: More examples are available in the repository under **prompts/**
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 更多示例可以在 **prompts/** 仓库中找到
- en: Method
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 方法
- en: We feed a prompt into the model and generate the continuation of the text with
    ***<neuron>***’s “token embedding” being the neuron we want to interpret. To produce
    diverse outputs, we generate with sampling and not just greedy decoding. We will
    see a few examples below.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将一个提示输入模型，并生成文本的延续，其中 ***<neuron>*** 的“token embedding” 是我们想要解释的神经元。为了产生多样化的输出，我们使用采样而不仅仅是贪婪解码。我们将看到下面的一些示例。
- en: In this work, we will focus on feedforward (FF) keys (the first layer of the
    feed-forward sublayer), as they seem somewhat easier to interpret than FF values
    (the second layer). Each layer *l* has a matrix K_*l* (do NOT confuse with *attention*
    keys) — each of its columns can be considered interpreted *individually*.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项工作中，我们将重点关注前馈（FF）键（前馈子层的第一层），因为它们似乎比 FF 值（第二层）更容易解释。每一层 *l* 都有一个矩阵 K_*l*（不要与
    *attention* 键混淆）——它的每一列都可以被视为*单独*解释的。
- en: 'To test our method, we use models we already have a good idea of what they
    mean in embedding space. Obviously, these are the easiest cases we can consider
    — so these experiments are just a sanity check. For syntactic sugar, we use ***<****param****_*i*_*j*>***
    for a neuron representing the j-th FF key in the i-th layer. All the examples
    below are from *GPT-2 medium.* The generation hyperparameters we use are:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 为了测试我们的方法，我们使用了我们已经对其在嵌入空间中的意义有较好了解的模型。显然，这些是我们可以考虑的最简单的情况——所以这些实验只是为了基本检查。作为语法糖，我们使用
    ***<****param****_*i*_*j*>*** 来表示 i 层中第 j 个 FF 键的神经元。下面的所有示例都来自 *GPT-2 medium*。我们使用的生成超参数是：
- en: '[PRE4]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Examples: Comparison With Embedding Space Projection'
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 示例：与嵌入空间投影的比较
- en: '***Japanese Names***'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '***日本名字***'
- en: '*When projected to the embedding space <param_10_8> seems to relate to Japanese
    names and generally speaking terms associated with Japan (* as far as I can tell):*'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '*当投影到嵌入空间时，<param_10_8> 似乎与日本名字以及通常与日本相关的术语相关（*就我所知）：*'
- en: '[PRE5]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Let’s see what the new method gives us:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看新方法给我们带来了什么：
- en: '[PRE6]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '**Discussion**: while it hasn''t given us a concise answer, it is not hard
    to deduce from the above samples that the term is indeed related to Japan and
    Japanese. As you can see, the model is not very truthful, even without the neuron
    being involved, spouting strange assertions like in the last sample.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '**讨论**：虽然它没有给出明确的答案，但从以上示例中不难推测，该术语确实与日本和日本人相关。正如你所看到的，即使不涉及神经元，该模型也不太真实，像最后一个示例那样提出奇怪的断言。'
- en: '***Progress***'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '***进步***'
- en: '*<param_11_2> seems to be related to the idea of progress. If we look at it
    in the embedding space, we get:*'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '*<param_11_2> 似乎与进步的概念相关。如果我们在嵌入空间中查看它，我们得到：*'
- en: '[PRE7]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Let’s see if we can get this from the new method:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看新方法是否能得到这些：
- en: '[PRE8]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '**Discussion**:we can consider this one quite good. As the model was trained
    on data from the internet, some answers might replicate the format of web discussions.
    Altogether, this parameter seems easy for the speaking probe to interpret.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '**讨论**：我们可以认为这项结果相当不错。由于模型是基于互联网数据进行训练的，因此一些答案可能会复制网页讨论的格式。总的来说，这个参数似乎对说话探针易于解释。'
- en: '***Words that start with G***'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '***以 G 开头的词汇***'
- en: '*<param_0_0>* *appears to relate to words that begin with the letter “g”. In
    the embedding space, we see:*'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '*<param_0_0>* *似乎与以字母“g”开头的词汇有关。在嵌入空间中，我们看到：*'
- en: '[PRE9]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Let’s see what happens with speaking probes:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看使用说话探针会发生什么：
- en: '[PRE10]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '[PRE11]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '**Discussion**: with the first prompt, the probe seems to be very unrelated
    to the embedding space interpretation of the parameter. The only hint we get in
    the right direction is “*In the United States, there are many people with names
    that begin in G and end at g…”* from the second sample. However, it is said in
    an indirect manner, and not as an answer to the question.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '**讨论**：使用第一个提示时，探针似乎与参数的嵌入空间解释无关。我们在正确方向上的唯一线索是来自第二个示例的“*在美国，有许多人名字以 G 开头，以
    g 结尾...*”。然而，这种说法是间接的，并不是对问题的直接回答。'
- en: This is a good place to demonstrate the flexibility of speaking probes, as they
    let you reformulate the question and maybe get better results. One can arguably
    say that the second prompt is much better suited to the problem. While it is not
    too truthful, an overview of the responses can hint at the right direction — one
    sample responds “G” and then gives a few unrelated words. Another one replies
    with “gg, go”. While it’s not clear-cut, it can help to begin exploring. It is
    also generally advisable to work with more samples.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 这是展示说话探针灵活性的好地方，因为它们允许你重新表述问题，并可能获得更好的结果。可以说第二个提示更适合问题。虽然它不完全真实，但对响应的概述可以暗示正确的方向——一个样本回应“G”然后给出几个无关的词。另一个则回复“gg,
    go”。虽然不明确，但它可以帮助开始探索。通常建议使用更多样本。
- en: Synthetic Examples
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 合成示例
- en: It is also possible to create neurons that we know what they “should” mean.
    For example, we take the average of the embeddings of two tokens and see if the
    model can reconstruct them with an appropriate prompt. This helps us debug the
    method. We can gauge its reaction to different variants of neurons and improve
    prompts based on them for the actual neurons we care about.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 也可以创建我们知道它们“应该”意味着什么的神经元。例如，我们取两个令牌嵌入的平均值，看看模型能否通过适当的提示重建它们。这有助于我们调试方法。我们可以根据不同变体的反应来评估其反应，并基于这些改进我们关心的实际神经元的提示。
- en: In the following example, we add together the token embedding of the “children”
    and “dog”. Then we apply a speaking probe to it.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的示例中，我们将“children”和“dog”的令牌嵌入加在一起。然后我们对其应用一个说话探针。
- en: We can use this neuron as a toy example of the way the probe treats such polysemous
    neurons. Remember, however, the behavior may differ depending on the word.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以用这个神经元作为探针处理多义神经元的玩具示例。不过要记住，行为可能会根据词汇的不同而有所不同。
- en: '[PRE12]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: The probe handles the ambiguity in the neuron moderately well. The samples seem
    to refer to one token, but then they might get confused and talk about the other.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 探针对神经元的模糊性处理得相当好。样本似乎指向一个令牌，但然后可能会混淆并讨论另一个。
- en: 'Let’s see what happens when we combine some more challenging tokens: “google”
    and “show”. This time we use a different prompt:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看当我们结合一些更具挑战性的令牌时会发生什么：“google”和“show”。这次我们使用一个不同的提示：
- en: '[PRE13]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'It looks like we’ve run out of luck here. The word “show” seems to have been
    absorbed by the word “google”. Let’s try to see if we can mitigate this by setting
    a smaller coefficient to the token embedding of “google” — we multiply it by 0.9
    and get:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来我们在这里运气用尽了。词语“show”似乎已经被“google”吸收了。我们来试着通过将“google”的令牌嵌入系数设置为更小的值来缓解这个问题——我们将其乘以0.9，得到：
- en: '[PRE14]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Seems it can recover “*show”* and *“giggle”* as a distorted version of *“google”*.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 似乎它可以将“*show”*和*“giggle”*恢复为*“google”*的扭曲版本。
- en: Discussion
  id: totrans-76
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 讨论
- en: Potential of the Method
  id: totrans-77
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 方法的潜力
- en: 'The distinctive features that we want to capitalize on with this method are:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望利用此方法的独特特征是：
- en: '***Natural language output***: both an advantage and disadvantage, it makes
    the output harder to evaluate, but it provides much greater flexibility than other
    methods.'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***自然语言输出***：既是优点也是缺点，它使得输出更难评估，但比其他方法提供了更大的灵活性。'
- en: '***Inherent ability to manipulate latent representations***: we use the model’s
    own capabilities of manipulating its latent representations. We assume they share
    the same latent space with the model parameters due to the residual stream view.
    Other techniques need to be trained or adjusted in some other way to the model’s
    latent space in order to “understand” it. The model is capable of decoding its
    own states naturally, which can be useful for interpretation.'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***操作潜在表征的固有能力***：我们使用模型自身操作其潜在表征的能力。我们假设它们与模型参数共享相同的潜在空间，因为残差流视图。其他技术需要经过训练或以其他方式调整到模型的潜在空间以“理解”它。模型能够自然解码自身状态，这对解释是有用的。'
- en: In general, there is not much research on continuous vectors as first-class
    citizens in transformers. While ideas like prompt tuning [*Lester et al., 2021*],
    and exciting ideas like *Hao et al.* [*2022*] pass continuous inputs to the model,
    they require training to work and they are not used zero-shot. A central motif
    in this work is the investigation of whether some continuous vectors can be used
    like natural tokens without further training — under the assumption that they
    use the same “language” as the model.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，关于将连续向量作为变换器中的一等公民的研究不多。虽然像提示调优 [*Lester et al., 2021*] 这样的想法，以及像 *Hao
    et al.* [*2022*] 这样的令人兴奋的想法将连续输入传递给模型，但它们需要训练才能工作，且不能零样本使用。这项工作中的核心主题是研究是否可以在不进一步训练的情况下将某些连续向量用作自然标记——假设它们使用与模型相同的“语言”。
- en: Another useful feature of this technique is that it uses the model more or less
    as a black box, without much technical work involved. It is easy to implement
    and understand. Casting interpretation as a generation problem, we can leverage
    literature on generation from mainstream NLP for future work. Similarly, hallucinations
    are a major concern in speaking probes, but we can hope to be able to apply mainstream
    research approaches in the future to this method.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 这种技术的另一个有用特性是，它或多或少将模型视为黑箱，而不涉及太多技术工作。它易于实现和理解。将解释视为生成问题，我们可以利用主流 NLP 的生成文献来进行未来的工作。类似地，幻觉是发言探针中的一个主要问题，但我们可以希望未来将主流研究方法应用于这种方法。
- en: In total, this is perhaps the *most modular* interpretability method — it does
    not rely on a specifically tailored algorithm, and it can adopt insights from
    other areas in NLP to improve, without losing breath. Also, it is easy to experiment
    with (even for less academically inclined practitioners) and the search space
    landscape is very different than with other methods.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，这可能是 *最模块化* 的可解释性方法——它不依赖于特定的算法，并且可以采用 NLP 其他领域的见解进行改进，而不会丧失效率。此外，它易于实验（即使是对学术不太感兴趣的从业者），并且搜索空间的格局与其他方法大相径庭。
- en: Possible Future Directions
  id: totrans-84
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 可能的未来方向
- en: '***Eloquent. Too Eloquent***: language models are trained to produce eloquent
    explanations. Factuality is less worrisome to them. These eloquent explanations
    are not to be taken literally.'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***雄辩。过于雄辩***：语言模型被训练以产生雄辩的解释。事实性对它们来说较少令人担忧。这些雄辩的解释不应被字面理解。'
- en: '***Layer Homogeneity***: in this article, we implicitly assume we can take
    parameters from different layers and they will react similarly to our prompts.
    It is possible that some layers are more amenable to use with speaking probes
    than others. We call this *layer homogeneity*. We need to be cautious in assuming
    that all layers can be treated the same with respect to our method.'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***层同质性***：在本文中，我们隐含地假设我们可以从不同的层获取参数，它们会对我们的提示做出类似的反应。可能有些层比其他层更适合与发言探针一起使用。我们称之为
    *层同质性*。我们需要谨慎地假设所有层在我们的方法中可以被相同对待。'
- en: '***Neuron Polysemy***: especially in face of word collapse, it seems that neurons
    that carry multiple unrelated interpretations will have to be sampled multiple
    times to account for all their different meanings. We would like to be able to
    extract the different meanings more faithfully and “in one sitting’’.'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***神经元多义性***：尤其是在面对词汇崩溃时，似乎携带多个无关解释的神经元需要多次采样，以涵盖所有不同的含义。我们希望能够更真实地提取这些不同的含义，并“一次性”完成。'
- en: '***Better Prompts***: this is not the main part of our work, but many papers
    show the benefits of using carefully engineered prompts [e.g., *Liu et al., 2021*].'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***更好的提示***：这不是我们工作的主要部分，但许多论文展示了使用精心设计的提示的好处 [例如，*Liu et al., 2021*]。'
- en: '***Other Types of Concepts***: we have mainly discussed neurons that represent
    a category or a concept in natural language. We know that language models can
    work with code, but we haven’t considered this type of knowledge in this article.
    Also, it is interesting to use speaking probes to locate facts in model parameters.
    Facts might require a number of parameters working in unison — so it will be interesting
    to locate them and find prompts that will be able to extract these facts.'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***其他类型的概念***：我们主要讨论了表示自然语言中的类别或概念的神经元。我们知道语言模型可以处理代码，但我们在这篇文章中没有考虑这类知识。此外，使用发言探针来定位模型参数中的事实也很有趣。事实可能需要多个参数协同工作——因此，定位这些事实并找到能够提取这些事实的提示将会很有趣。'
- en: 'If you do follow-up work, please cite as:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你进行后续工作，请引用如下：
- en: '[PRE15]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'You are also welcome to follow me on Twitter:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以在 Twitter 上关注我：
- en: '[](https://mobile.twitter.com/guy__dar?source=post_page-----7a3dc6cb33d6--------------------------------)
    [## Guy Dar (guy__dar) / Twitter'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://mobile.twitter.com/guy__dar?source=post_page-----7a3dc6cb33d6--------------------------------)
    [## Guy Dar (guy__dar) / Twitter'
- en: Twitter](https://mobile.twitter.com/guy__dar?source=post_page-----7a3dc6cb33d6--------------------------------)
  id: totrans-94
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Twitter](https://mobile.twitter.com/guy__dar?source=post_page-----7a3dc6cb33d6--------------------------------)
- en: '*This is not a direct follow-up, but you might be also interested in my other
    blog post on a related paper I worked on with collaborators:*'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '*这不是直接的后续，但你可能还对我与合作者合作的相关论文的另一篇博客帖子感兴趣：*'
- en: '[](/analyzing-transformers-in-embedding-space-explained-ef72130a6844?source=post_page-----7a3dc6cb33d6--------------------------------)
    [## Analyzing Transformers in Embedding Space — Explained'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/analyzing-transformers-in-embedding-space-explained-ef72130a6844?source=post_page-----7a3dc6cb33d6--------------------------------)
    [## Analyzing Transformers in Embedding Space — Explained'
- en: In this post, I present the paper “Analyzing Transformers in Embedding Space”
    (2022) by Guy Dar, Mor Geva, Ankit Gupta…
  id: totrans-97
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在这篇文章中，我介绍了Guy Dar、Mor Geva、Ankit Gupta等人于2022年发表的论文《分析嵌入空间中的转换器》。
- en: towardsdatascience.com](/analyzing-transformers-in-embedding-space-explained-ef72130a6844?source=post_page-----7a3dc6cb33d6--------------------------------)
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/analyzing-transformers-in-embedding-space-explained-ef72130a6844?source=post_page-----7a3dc6cb33d6--------------------------------)
- en: References
  id: totrans-99
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: References
- en: D. Dai, L. Dong, Y. Hao, Z. Sui, B. Chang, and F. Wei. *Knowledge neurons in
    pretrained transformers*, 2021\. URL [https://arxiv.org/abs/2104.08696](https://arxiv.org/abs/2104.08696).
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: Dai D, Dong L, Hao Y, Sui Z, Chang B, 以及 Wei F. *预训练转换器中的知识神经元*，2021\. URL [https://arxiv.org/abs/2104.08696](https://arxiv.org/abs/2104.08696)。
- en: G. Dar, M. Geva, A. Gupta, and J. Berant. *Analyzing transformers in embedding
    space*, 2022\. URL [https://arxiv.org/abs/2209.02535](https://arxiv.org/abs/2209.02535).
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: Dar G, Geva M, Gupta A, 以及 Berant J.《在嵌入空间中分析转换器》，2022\. URL [https://arxiv.org/abs/2209.02535](https://arxiv.org/abs/2209.02535)。
- en: N. Elhage, N. Nanda, C. Olsson, T. Henighan, N. Joseph, B. Mann, A. Askell,
    Y. Bai, A. Chen, T. Conerly, N. DasSarma, D. Drain, D. Ganguli, Z. Hatfield-Dodds,
    D. Hernandez, A. Jones, J. Kernion, L. Lovitt, K. Ndousse, D. Amodei, T. Brown,
    J. Clark, J. Kaplan, S. McCandlish, and C. Olah. *A mathematical framework for
    transformer circuits*, 2021\. URL [https://transformer-circuits.pub/2021/framework/index.html](https://transformer-circuits.pub/2021/framework/index.html).
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: Elhage N, Nanda N, Olsson C, Henighan T, Joseph N, Mann B, Askell A, Bai Y,
    Chen A, Conerly T, DasSarma N, Drain D, Ganguli D, Hatfield-Dodds Z, Hernandez
    D, Jones A, Kernion J, Lovitt L, Ndousse K, Amodei D, Brown T, Clark J, Kaplan
    J, McCandlish S, 以及 Olah C. *转换器电路的数学框架*，2021\. URL [https://transformer-circuits.pub/2021/framework/index.html](https://transformer-circuits.pub/2021/framework/index.html)。
- en: M. Geva, R. Schuster, J. Berant, and O. Levy. *Transformer feed-forward layers
    are key-value memories*, 2020\. URL [https://arxiv.org/abs/2012.14913](https://arxiv.org/abs/2012.14913).
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: Geva M, Schuster R, Berant J, 以及 Levy O. *转换器前馈层是键-值存储器*，2020\. URL [https://arxiv.org/abs/2012.14913](https://arxiv.org/abs/2012.14913)。
- en: 'M. Geva, A. Caciularu, G. Dar, P. Roit, S. Sadde, M. Shlain, B. Tamir, and
    Y. Goldberg. *Lm-debugger: An interactive tool for inspection and intervention
    in transformer-based language models*. arXiv preprint arXiv:2204.12130, 2022a.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 'Geva M, Caciularu A, Dar G, Roit P, Sadde S, Shlain M, Tamir B, 以及 Goldberg
    Y.《Lm-debugger: 一个交互式工具，用于检查和干预基于转换器的语言模型》[Lm-debugger: An interactive tool for
    inspection and intervention in transformer-based language models]. arXiv preprint
    arXiv:2204.12130, 2022a。'
- en: M. Geva, A. Caciularu, K. R. Wang, and Y. Goldberg. *Transformer feed-forward
    layers build predictions by promoting concepts in the vocabulary space*, 2022b.
    URL [https://arxiv.org/abs/2203.14680](https://arxiv.org/abs/2203.14680).
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: Geva M, Caciularu A, Wang K R, 以及 Goldberg Y.《转换器前馈层通过推动词汇空间中的概念来构建预测*，2022b。URL
    [https://arxiv.org/abs/2203.14680](https://arxiv.org/abs/2203.14680).
- en: Y. Hao, H. Song, L. Dong, S. Huang, Z. Chi, W. Wang, S. Ma, and F. Wei. *Language
    models are general-purpose interfaces*, 2022\. URL [https://arxiv.org/abs/2206.06336](https://arxiv.org/abs/2206.06336).
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: Hao Y, Song H, Dong L, Huang S, Chi Z, Wang W, Ma S, 以及 Wei F. *语言模型是通用接口*，2022\.
    URL [https://arxiv.org/abs/2206.06336](https://arxiv.org/abs/2206.06336)。
- en: 'A. Kadar, G. Chrupala, and A. Alishahi. *Representation of Linguistic Form
    and Function in Recurrent Neural Networks*. Computational Linguistics, 43(4):761–780,
    12 2017\. ISSN 0891–2017\. doi: 10.1162/COLI a 00300\. URL [https://doi.org/10.1162/COLI_a_00300](https://doi.org/10.1162/COLI_a_00300).'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 'Kadar A, Chrupala G, 以及 Alishahi A. *在递归神经网络中表示语言形式和函数*。计算语言学, 43(4):761–780,
    2017。ISSN 0891–2017. doi: 10.1162/COLI a 00300. URL [https://doi.org/10.1162/COLI_a_00300](https://doi.org/10.1162/COLI_a_00300)。'
- en: B. Lester, R. Al-Rfou, and N. Constant. *The power of scale for parameter-efficient
    prompt tuning*, 2021\. URL [https://arxiv.org/abs/2104.08691](https://arxiv.org/abs/2104.08691).
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: B. Lester, R. Al-Rfou 和 N. Constant. *参数高效提示调整的规模力量*，2021\. 网址 [https://arxiv.org/abs/2104.08691](https://arxiv.org/abs/2104.08691)。
- en: J. Liu, D. Shen, Y. Zhang, B. Dolan, L. Carin, and W. Chen. *What makes good
    in-context examples for gpt-3*? CoRR, abs/2101.06804, 2021\. URL [https://arxiv.org/abs/2101.06804](https://arxiv.org/abs/2101.06804).
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: J. Liu, D. Shen, Y. Zhang, B. Dolan, L. Carin 和 W. Chen. *什么构成了 GPT-3 的优秀上下文示例*？CoRR,
    abs/2101.06804, 2021\. 网址 [https://arxiv.org/abs/2101.06804](https://arxiv.org/abs/2101.06804)。
- en: S. Na, Y. J. Choe, D.-H. Lee, and G. Kim. *Discovery of natural language concepts
    in individual units of cnns*, 2019\. URL [https://arxiv.org/abs/1902.07249](https://arxiv.org/abs/1902.07249).
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: S. Na, Y. J. Choe, D.-H. Lee 和 G. Kim. *发现 CNN 单元中的自然语言概念*，2019\. 网址 [https://arxiv.org/abs/1902.07249](https://arxiv.org/abs/1902.07249)。
- en: 'nostalgebraist. interpreting gpt: the logit lens, 2020\. URL [https://www.lesswrong.com/posts/AcKRB8wDpdaN6v6ru/interpreting-gpt-the-logit-lens](https://www.lesswrong.com/posts/AcKRB8wDpdaN6v6ru/interpreting-gpt-the-logit-lens).'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: nostalgebraist. 解释 GPT：logit 视角，2020\. 网址 [https://www.lesswrong.com/posts/AcKRB8wDpdaN6v6ru/interpreting-gpt-the-logit-lens](https://www.lesswrong.com/posts/AcKRB8wDpdaN6v6ru/interpreting-gpt-the-logit-lens)。
