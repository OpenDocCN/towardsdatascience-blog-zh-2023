- en: What Happens When Most Content Online Becomes AI-Generated?
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/what-happens-when-most-content-online-becomes-ai-generated-684dde2a150d?source=collection_archive---------9-----------------------#2023-10-17](https://towardsdatascience.com/what-happens-when-most-content-online-becomes-ai-generated-684dde2a150d?source=collection_archive---------9-----------------------#2023-10-17)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Learn how generative models deteriorate when trained on the data they generate,
    and what to do about it
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@aichabokbot?source=post_page-----684dde2a150d--------------------------------)[![Aicha
    Bokbot](../Images/1aa9ba6ae6296d8be3350b14dba97dd2.png)](https://medium.com/@aichabokbot?source=post_page-----684dde2a150d--------------------------------)[](https://towardsdatascience.com/?source=post_page-----684dde2a150d--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----684dde2a150d--------------------------------)
    [Aicha Bokbot](https://medium.com/@aichabokbot?source=post_page-----684dde2a150d--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F50566ce7e21&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-happens-when-most-content-online-becomes-ai-generated-684dde2a150d&user=Aicha+Bokbot&userId=50566ce7e21&source=post_page-50566ce7e21----684dde2a150d---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----684dde2a150d--------------------------------)
    ·6 min read·Oct 17, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F684dde2a150d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-happens-when-most-content-online-becomes-ai-generated-684dde2a150d&user=Aicha+Bokbot&userId=50566ce7e21&source=-----684dde2a150d---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F684dde2a150d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-happens-when-most-content-online-becomes-ai-generated-684dde2a150d&source=-----684dde2a150d---------------------bookmark_footer-----------)![](../Images/0f749c7a7ddaf8046a779a545072100f.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: Spiral — Photo by [Ludde Lorentz](https://unsplash.com/@luddelorentz?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The recent breakthroughs in Generative AI have introduced publicly available
    AI models capable of producing highly realistic and complex text, image, and sound
    that revolutionize content creation.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: These models were trained on large datasets scraped from the Internet. In the
    case of text data for instance, Large Language Models (LLMs) like ChatGPT were
    mostly trained on human-generated text found online.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: Generative models have gained wide notoriety and fast adoption in society, to
    the extent that more and more AI-generated content ends up on the Internet, which
    is the major source of their training data.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: 'We see a loop forming here: generative models will inevitably be trained on
    synthetic data produced by Generative AI and not by humans. Which brings us to
    the question: if that happens, how will the models behave?'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: Generative models and degeneration
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: What will happen to GPT-{n} once LLMs contribute much of the language found
    online?
  id: totrans-15
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'That is the question asked by a team of researchers that published the paper
    [*The Curse of Recursion: Training on Generated Data Makes Models Forget*](https://browse.arxiv.org/pdf/2305.17493v2.pdf?)
    (I. Shumailov et al., 2023).'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: 'The authors experiment with making models learn from the data they generate
    over several iterations. They apply this on Gaussian Mixture Models (GMMs), Variational
    Autoencoders (VAE) and Large Language models (LLMs). For the three types of models,
    this leads to a phenomenon they call “*model collapse*”:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: a degenerative process whereby, over time, models forget the true underlying
    data distribution, even in the absence of a shift in the distribution over time.
  id: totrans-18
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*Model collapse* characterizes the behavior of models forgetting the tails
    of real distributions (i.e. the improbable, less frequent and yet important events)
    and over-representing the center of the distribution. As the process repeats,
    models converge to a distribution that has little similarity with the original
    one.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: 'In the paper [*Self-Consuming Generative Models Go MAD*](https://arxiv.org/abs/2307.01850)
    (S. Alemohammad et al., 2023), other researchers similarly describes this gradual
    divergence away from original distributions as “*Model Autophagy Disorder*” *(MAD)*:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: '[…] without enough fresh real data in each generation, future generative models
    are doomed to Model Autophagy Disorder (MAD), meaning that either their quality
    (measured in terms of precision) or their diversity (measured in terms of recall)
    will progressively degrade.'
  id: totrans-21
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'The conclusion is clear: generative models tend to deteriorate when they are
    primarily trained on the content they generate. Given that the majority of their
    training data comes from the Internet, how much of a risk does this degenerative
    process represent?'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: When human-generated content grows scarce
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If the proportion of human- vs. AI-generated content remains as it is today,
    with a clear majority of online content created by humans, the situation should
    not be worrying and the performance of Generative models should not face the degeneration
    described earlier.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: 'However, there is a legitimate concern over the prospect of an Internet dominated
    by AI-generated content. If we focus on text generation, the main argument supporting
    this projection is the convenience offered by LLMs like GPT. These tools address
    real pain points when it comes to writing: finding the right words, improving
    style, getting inspiration. Those who have tried them can attest how tempting
    and addictive it is to rely on these tools: why struggle when we can effortlessly
    prompt an LLM to generate content that is better both in style and time-efficiency?'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: So if human-generated content becomes less prevalent, what measures can be taken
    to mitigate the risk of exposing Generative models to AI-generated content? One
    possible approach is to attempt to counter the trend by encouraging content creation
    without AI tools. Another approach involves developing methods to detect AI-generated
    data and filtering it out during the model training process.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: The idea of the first approach is to find ways to incentivize creators to use
    generative models less frequently or in fewer contexts. However, the feasibility
    of this approach may be limited. Even if we could find such incentives (e.g. a
    reward or a sanction), the challenge lies in verifying whether a piece of content
    is genuinely human-generated or not. This leads us to the second approach, which
    also relies on the ability to distinguish human-generated from AI-generated data.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: How to detect AI-generated data
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In 2023, the company AI21 Labs led the largest Turing test to date: over 1.5
    million users engaged in an online chat with either a human or an AI chatbot and
    were asked to guess who they spoke to. The task proves to be non-trivial as 68%
    of people guessed correctly, showing how well generative AI can mimic humans.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: With that in mind, let’s review some available techniques to identify AI-generated
    data.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: Watermarking
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The main approach discussed in the literature to distinguish human-generated
    from AI-generated data is *watermarking*. Watermarking is the process of adding
    hidden signals in the data that are invisible to humans but detectable by algorithms.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: Watermarking would enable to easily detect AI-generated data, but it only works
    if Generative AI providers add it to their products. This is not the case today,
    and it might be too optimistic to expect that any time soon.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: In his open letter [Watermarking is a No-Go](https://www.deeplearning.ai/the-batch/watermarking-is-a-no-go/?ref=dl-staging-website.ghost.io),
    Andrew Ng is not optimistic about the adoption of watermarking. He explains how,
    in the current GenAI race, watermarking appears like a competitive disadvantage
    for AI companies.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: Using ML classifiers
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: An alternative to watermarking would be to train a Machine Learning classifier
    that learns to label whether a content is AI-generated or human-generated.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: Systems performing this task already exist, such as [GPTzero](https://gptzero.me/?ref=the-batch-deeplearning-ai).
    However they show a high error rate and fall short of offering a robust solution.
    Detecting AI-generated data proves to be a difficult task, even for machine learning
    models.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: A zero-shot approach
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Another approach discussed in the paper [DetectGPT: Zero-Shot Machine-Generated
    Text Detection using Probability Curvature](https://arxiv.org/abs/2301.11305)
    (E. Mitchell et al., 2023) does not require any training and is based on observing
    the curvature of the log probability function of LLMs.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: The criterion discovered by the authors is that AI-generated text tends to occupy
    regions with negative log probability curvature, as opposed to human-written text.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: 'This seems like a promising technique, although we may have doubts concerning
    its scalability: it is not model-agnostic (it detects whether a content was generated
    by a specific LLM, not by any given LLM) and requires to have access to the model’s
    log probabilities.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We’ve learned about the importance of training generative models on human-generated
    data to avoid the decline of their performance. This poses a significant challenge,
    particularly as the Internet is their primary source of training data, and as
    more an more content online is AI-generated.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two approaches to address this issue: reversing the trend by promoting
    human content creation, and preventing models from being trained on synthetic
    data. However both solutions raise complex, open-ended questions. How can we incentivize
    online content creators to rely less on AI? How can we establish reliable, scalable
    methods for detecting AI-generated content?'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: On a promising note, the White House made a noteworthy [announcement](https://www.whitehouse.gov/briefing-room/statements-releases/2023/07/21/fact-sheet-biden-harris-administration-secures-voluntary-commitments-from-leading-artificial-intelligence-companies-to-manage-the-risks-posed-by-ai/?utm_campaign=The+Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-_KWnjJMok4p7lxxBJgIa1f-qA07jSWCt3GXAPWGm9rOoVgsUvfcPNMlM5GrOOOH3knocHJ)
    in July 2023\. Seven major AI companies — Amazon, Anthropic, Google, Inflection,
    Meta, Microsoft, and OpenAI — agreed to respect a list of responsible-AI commitments
    which include watermarking. The future will reveal whether these commitments are
    upheld.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: To go further
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Publication of AI21 Labs](https://www.ai21.com/blog/human-or-not-results)
    where they analyze the results of their “Human or Not?” experiment and explain
    their methodology'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Sparks of Artificial General Intelligence: Early experiments with GPT-4](https://arxiv.org/abs/2303.12712),
    a thorough investigation on GPT-4 and an interesting discussion about AGI, by
    a team of researchers at Microsoft'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Watermarking is a No-Go](https://www.deeplearning.ai/the-batch/watermarking-is-a-no-go/?ref=dl-staging-website.ghost.io),
    a letter by Andrew Ng about how financial “incentives in the competitive market
    for generative AI make [watermarking] adoption challenging”'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[水印技术无望](https://www.deeplearning.ai/the-batch/watermarking-is-a-no-go/?ref=dl-staging-website.ghost.io)，这是Andrew
    Ng的一封信，讨论了金融“市场竞争中的激励措施如何使得[水印技术]的采用变得具有挑战性”。'
