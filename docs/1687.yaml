- en: Customizing Your Cloud Based Machine Learning Training Environment — Part 1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/customizing-your-cloud-based-machine-learning-training-environment-part-1-2622e10ed65a?source=collection_archive---------7-----------------------#2023-05-21](https://towardsdatascience.com/customizing-your-cloud-based-machine-learning-training-environment-part-1-2622e10ed65a?source=collection_archive---------7-----------------------#2023-05-21)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: How to leverage the power of the cloud without compromising development flexibility
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://chaimrand.medium.com/?source=post_page-----2622e10ed65a--------------------------------)[![Chaim
    Rand](../Images/c52659c389f167ad5d6dc139940e7955.png)](https://chaimrand.medium.com/?source=post_page-----2622e10ed65a--------------------------------)[](https://towardsdatascience.com/?source=post_page-----2622e10ed65a--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----2622e10ed65a--------------------------------)
    [Chaim Rand](https://chaimrand.medium.com/?source=post_page-----2622e10ed65a--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F9440b37e27fe&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcustomizing-your-cloud-based-machine-learning-training-environment-part-1-2622e10ed65a&user=Chaim+Rand&userId=9440b37e27fe&source=post_page-9440b37e27fe----2622e10ed65a---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----2622e10ed65a--------------------------------)
    ·8 min read·May 21, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F2622e10ed65a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcustomizing-your-cloud-based-machine-learning-training-environment-part-1-2622e10ed65a&user=Chaim+Rand&userId=9440b37e27fe&source=-----2622e10ed65a---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F2622e10ed65a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcustomizing-your-cloud-based-machine-learning-training-environment-part-1-2622e10ed65a&source=-----2622e10ed65a---------------------bookmark_footer-----------)![](../Images/c0807cb1ec61c0e065e0fc72f8147157.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Photo by [Jeremy Thomas](https://unsplash.com/@jeremythomasphoto?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: Cloud-based machine learning (ML) services offer a great number of conveniences
    to the AI developer, perhaps none as important as the **access** they provide
    to a wide variety of fully provisioned, fully functional, and fully maintained
    ML training environments. For example, managed training services, such as [Amazon
    SageMaker](https://aws.amazon.com/pm/sagemaker/?trk=ps_a134p000007BxdvAAC&trkCampaign=acq_paid_search_brand&sc_channel=PS&sc_campaign=acquisition_IL&sc_publisher=Google&sc_category=Machine+Learning&sc_country=IL&sc_geo=EMEA&sc_outcome=acq&sc_detail=amazon+sagemaker&sc_content=Sagemaker_e&sc_matchtype=e&sc_segment=532435490322&sc_medium=ACQ-P%7CPS-GO%7CBrand%7CDesktop%7CSU%7CMachine+Learning%7CSagemaker%7CIL%7CEN%7CText&s_kwcid=AL%214422%213%21532435490322%21e%21%21g%21%21amazon+sagemaker&ef_id=Cj0KCQiAhMOMBhDhARIsAPVml-HxIwfeABmnxXbZ9ia_5DV_TckDGpMSH2mFhSpu8jrCgntII8hcHB4aAuhfEALw_wcB%3AG%3As)
    and [Google Vertex AI](https://cloud.google.com/vertex-ai), enable users to specify
    (1) the desired instance types (e.g., with the latest available GPUs), (2) an
    ML framework and version, (3) a code source directory, and (4) a training script,
    and will automatically start up the chosen instances with the requested environment,
    run the script to train the AI model, and tear everything down upon completion.
    Among the advantages of such offerings is the potential for significant savings
    in the time and cost of building and maintaining your own training cluster. See
    [here](/6-steps-to-migrating-your-machine-learning-project-to-the-cloud-6d9b6e4f18e0)
    for more on the benefits and considerations of cloud-based training as well as
    a summary of some of the common steps required to migrate an ML workload to the
    cloud.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, coupled with the convenience of using a predefined, fully provisioned,
    fully validated, ML environment comes the potential for limitations on development
    flexibility. This is contrary to a local, “on-prem” environment that you can freely
    define to your heart’s desire. Here are a few scenarios that demonstrate the potential
    limitation:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Training dependencies**: It is easy to conceive of training flows that have
    dependencies on specific packages (e.g., Linux packages or Python packages) that
    might not be included in the predefined environments provided by your cloud service
    of choice.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Development platform independence**: You may desire (or require) a development
    environment that is independent of the underlying runtime platform. For example,
    you may want to have the ability to use the same training environment regardless
    of whether you are running on your own PC, on a local (“on-prem”) cluster, or
    in the cloud, and regardless of the cloud service provider and cloud service you
    choose. This can reduce the overhead of needing to adapt to multiple environments
    and might also facilitate debugging issues.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Personal development preferences**: Engineers (especially seasoned ones)
    can be quite particular about their development habits and development environments.
    The mere possibility of a limitation, introduced by a change to the development
    process, however much its value and importance to the team (e.g., migrating to
    cloud ML), might stir up significant resistance.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In this two-part blog post we will cover some of the options available for overcoming
    the potential development limitations of cloud-based training. Specifically, we
    will assume that the training environment is defined by a [Docker](https://www.docker.com/)
    image containing a Python environment and demonstrate a few methods for customizing
    the environment to meet our specific needs.
  prefs: []
  type: TYPE_NORMAL
- en: For the sake of demonstration, we will assume that the training service of choice
    is Amazon SageMaker. However, please note that the general methods apply equally
    to other training services as well and that this choice should not be viewed as
    an endorsement of one cloud-based service over any other. The best option for
    you will likely depend on many factors including the details of your project,
    budgetary considerations, and more.
  prefs: []
  type: TYPE_NORMAL
- en: Throughout the blog we will reference and demonstrate certain library APIs and
    behaviors. Note these are true as of the time of this writing and are subject
    to modification in future versions of the libraries. Please be sure to refer to
    the latest official documentation before trusting anything we write.
  prefs: []
  type: TYPE_NORMAL
- en: I would like to thank [Yitzhak Levi](https://www.linkedin.com/in/yitzhak-levi-49a217201/?originalSubdomain=il)
    whose experimentation formed the basis of this blog post.
  prefs: []
  type: TYPE_NORMAL
- en: Managed Training Example
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let’s begin with a simple demonstration of training in the cloud using a managed
    service. In the code block below we use [Amazon SageMaker](https://aws.amazon.com/pm/sagemaker/?trk=ps_a134p000007BxdvAAC&trkCampaign=acq_paid_search_brand&sc_channel=PS&sc_campaign=acquisition_IL&sc_publisher=Google&sc_category=Machine+Learning&sc_country=IL&sc_geo=EMEA&sc_outcome=acq&sc_detail=amazon+sagemaker&sc_content=Sagemaker_e&sc_matchtype=e&sc_segment=532435490322&sc_medium=ACQ-P%7CPS-GO%7CBrand%7CDesktop%7CSU%7CMachine+Learning%7CSagemaker%7CIL%7CEN%7CText&s_kwcid=AL%214422%213%21532435490322%21e%21%21g%21%21amazon+sagemaker&ef_id=Cj0KCQiAhMOMBhDhARIsAPVml-HxIwfeABmnxXbZ9ia_5DV_TckDGpMSH2mFhSpu8jrCgntII8hcHB4aAuhfEALw_wcB%3AG%3As)
    to run the *train.py* PyTorch (1.13.1) script from the *source_dir* folder on
    a single [*ml.g5.xlarge*](https://aws.amazon.com/ec2/instance-types/g5/) GPU instance
    type. Note that this is a rather simple example of training with Amazon SageMaker;
    for more details be sure to see the official [AWS documentation](https://aws.amazon.com/getting-started/hands-on/machine-learning-tutorial-train-a-model/).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: To get a better idea of the potential limitations on development flexibility
    when training in the cloud and how to overcome them, let’s review the steps that
    occur behind the scenes when deploying a training job using a service such as
    Amazon SageMaker.
  prefs: []
  type: TYPE_NORMAL
- en: What Happens Behind the Scenes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will focus here on the primary steps that occur; for the full details please
    see the [official documentation](https://docs.aws.amazon.com/sagemaker/index.html).
    Although we will review the actions that are taken by the SageMaker API, other
    managed training APIs exhibit similar behaviors.
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 1**: Tar and upload the code source directory to cloud storage.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 2**: Provision the requested instance type(s).'
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 3**: Pull the appropriate [pre-built Docker image](https://docs.aws.amazon.com/sagemaker/latest/dg/pre-built-containers-frameworks-deep-learning.html)
    to the instance(s). The appropriate image is determined by the properties of the
    training job. In the example above, we requested Python 3.9, PyTorch 1.13.1, and
    a GPU instance type in the *us-east-1* [AWS region](https://aws.amazon.com/about-aws/global-infrastructure/regions_az/)
    and will, accordingly, end up with the 763104351884.dkr.ecr.**us-east-1**.amazonaws.com/**pytorch**-training:**1.13.1**-**gpu**-**py39**-cu117-ubuntu20.04-sagemaker
    image.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 4**: Run the Docker image. The Docker *ENTRYPOINT* is a script (defined
    by the service) that downloads and unpacks the training code from cloud storage
    and runs the user defined training script (more details on this below).'
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 5**: When the training script has completed, stop and release the instance.'
  prefs: []
  type: TYPE_NORMAL
- en: Note that we have greatly simplified the actions taken by the service and focused
    on those that will pertain to our discussion. In practice there are many more
    “management” activities that take place behind the scenes that include [accessing
    training data](https://docs.aws.amazon.com/sagemaker/latest/dg/model-access-training-data.html),
    [orchestrating distributed training](https://docs.aws.amazon.com/sagemaker/latest/dg/distributed-training.html),
    [recovery from spot interruptions](https://docs.aws.amazon.com/sagemaker/latest/dg/model-managed-spot-training.html),
    [monitoring and analyzing training](https://docs.aws.amazon.com/sagemaker/latest/dg/training-metrics.html),
    and much more.
  prefs: []
  type: TYPE_NORMAL
- en: About the Docker Image
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The [AWS Deep Learning Containers](https://github.com/aws/deep-learning-containers)
    (DLCs) github repository includes the pre-built AWS Docker images used by the
    SageMaker service. These can be analyzed to get a better understanding of the
    environment in which the training script runs. In particular, the Dockerfile defining
    the image used in the example above is located [here](https://github.com/aws/deep-learning-containers/blob/master/pytorch/training/docker/1.13/py3/cu117/Dockerfile.gpu).
    From a cursory review of the file we can see that in addition to standard Linux
    and Python packages (e.g., OpenSSH, pandas, scikit-learn, etc.), the image contains
    several AWS specific packages, such as enhanced version of [PyTorch for AWS](https://github.com/aws/deep-learning-containers/blob/master/pytorch/training/docker/1.13/py3/cu117/Dockerfile.gpu#L385)
    and libraries for utilizing [Amazon EFA](https://aws.amazon.com/hpc/efa/). The
    AWS specific enhancements include features for managing and monitoring training
    jobs and, more importantly, optimizations for **maximizing resource utilization
    and runtime performance when running on AWS’s training infrastructures**. Furthermore,
    upon closer analysis of the Dockerfile it becomes clear that its creation required
    diligent work, occasional workarounds, and extensive testing. Barring other considerations
    (see below), our first choice will always be to use the official AWS DLCs when
    training on AWS.
  prefs: []
  type: TYPE_NORMAL
- en: The [*ENTRYPOINT* in the Dockerfile](https://github.com/aws/deep-learning-containers/blob/master/pytorch/training/docker/1.13/py3/cu117/Dockerfile.gpu#L561)
    points to a script called [*start_with_right_hostname.sh*](https://github.com/aws/sagemaker-pytorch-training-toolkit/blob/master/docker/build_artifacts/start_with_right_hostname.sh).
    This calls the [*train.py*](https://github.com/aws/sagemaker-training-toolkit/blob/1f4691ab98967bd32e2860a6afc42001d0945ec9/src/sagemaker_training/cli/train.py)
    script from the [sagemaker-training](https://pypi.org/project/sagemaker-training/)
    Python package which, in turn, calls a [function](https://github.com/aws/sagemaker-training-toolkit/blob/1f4691ab98967bd32e2860a6afc42001d0945ec9/src/sagemaker_training/trainer.py#L61)
    that parses the [*SAGEMAKER_TRAINING_MODULE*](https://github.com/aws/deep-learning-containers/blob/master/pytorch/training/docker/1.13/py3/cu117/Dockerfile.gpu#L370)environment
    variables and ultimately runs the PyTorch-specific [entry-point](https://github.com/aws/sagemaker-pytorch-training-toolkit/blob/master/src/sagemaker_pytorch_container/training.py#L152)
    from the [sagemaker-pytorch-training](https://pypi.org/project/sagemaker-pytorch-training/)
    Python package. It is this entry-point that downloads the source code and starts
    up the training as described in step 4 above. Keep in mind that the flow we just
    described and the code to which we linked are valid as of the time of this writing
    and may change as the SageMaker APIs evolve. While we have analyzed the startup
    flow for a SageMaker PyTorch 1.13 training job as configured in the example above,
    the same analysis can be performed for other types of cloud-based training jobs.
  prefs: []
  type: TYPE_NORMAL
- en: The publicly accessible AWS DLC Dockerfiles and SageMaker Python package source
    code allow us to get a sense of the cloud-based training runtime environment.
    This enables us to gain a better understanding of the capabilities and limitations
    of training in a cloud-based environment and, as we will see in the next sections,
    understand the tools at our disposal for introducing changes and customizations.
  prefs: []
  type: TYPE_NORMAL
- en: Customizing the Python Environment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The first and simplest way to customize your training environment is by adding
    Python packages. Training scripts will often depend on Python packages (or on
    specific versions of packages) that are not included in the Python environment
    of the cloud service’s default Docker image. The SageMaker APIs address this by
    allowing you to include a *requirements.txt* in the root of the *source_dir* folder
    passed into the SageMaker estimator (see the API call example above). Following
    the downloading and unpacking of the source code (in step 4 above), the SageMaker
    script will search for a *requirements.txt* file and install all of its contents
    using the [pip](https://pypi.org/project/pip/) package installer. See [here](https://sagemaker.readthedocs.io/en/stable/frameworks/pytorch/using_pytorch.html#using-third-party-libraries)
    for more details on this feature. Other cloud services include similar mechanisms
    for automating installation of package dependencies. This type of solution can
    also be easily accomplished by simply including a designated package installation
    routine at the beginning of your own training script (though this needs to be
    carefully coordinated in a case where you are running multiple processes e.g.,
    with MPI, on a single instance).
  prefs: []
  type: TYPE_NORMAL
- en: 'While this solution is simple and easy to use, it does have some limitations:'
  prefs: []
  type: TYPE_NORMAL
- en: Installation Time
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One thing to take into consideration is the overhead (in time) required to install
    the package dependencies. If we have a long list of dependencies or if any of
    the dependencies require a long installation time, customizing the environment
    in this manner might increase the overall time and cost of training to an unreasonable
    degree and we might find one of the alternative methods we will discuss to better
    suit our needs.
  prefs: []
  type: TYPE_NORMAL
- en: Access to Repository
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Another thing to keep in mind about this method is that it relies on access
    to a Python package repository. If the network of your training environment is
    configured to allow free access to the internet, then this is not a problem. However,
    if you are training in a private network environment, such as [Amazon Virtual
    Private Cloud](http://aws.amazon.com/vpc) (Amazon VPC), then this access might
    be restricted. In such cases, you will need to create a private package repository.
    In AWS this can be done using [AWS CodeArtifact](https://aws.amazon.com/codeartifact/)
    or by creating a private PyPI mirror. (Although the use case is a bit different,
    you might find the recipes described [here](https://aws.amazon.com/blogs/machine-learning/private-package-installation-in-amazon-sagemaker-running-in-internet-free-mode/)
    and [here](https://aws.amazon.com/blogs/machine-learning/hosting-a-private-pypi-server-for-amazon-sagemaker-studio-notebooks-in-a-vpc/)
    to be helpful.) In the absence of an accessible package repository, you will need
    to consider one of the alternative customization options we will present.
  prefs: []
  type: TYPE_NORMAL
- en: Conda Package Dependencies
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The customization solution we have presented is great if all of our dependencies
    are pip packages. But what if our dependency exists only as a [conda](https://docs.conda.io/en/latest/)
    package? For example, suppose we wish to use the [s5cmd](https://anaconda.org/conda-forge/s5cmd)
    for [speeding up data streaming from cloud storage](/training-from-cloud-storage-with-s5cmd-5c8fb5c06056).
    When we have full control over the training environment, we can choose to build
    our Python environment using conda and freely install conda package dependencies.
    If we use a Docker container provided by a cloud service, we don’t control the
    Python environment creation and may not enjoy the same freedoms. Point in fact,
    attempting to install [s5cmd](https://anaconda.org/conda-forge/s5cmd) via [subprocess](https://docs.python.org/3/library/subprocess.html)
    on SageMaker (*conda install -y s5cmd*) fails. Even if we were to figure out a
    way to make it work, installing a package in this manner may lead to undesired
    side effects such as overwriting other AWS-optimized packages or generally destabilizing
    the conda environment.
  prefs: []
  type: TYPE_NORMAL
- en: Linux Package Dependencies
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In theory, this method could also be extended to support Linux packages. If,
    for example, our training script depended on a specific Linux package or package
    version, we could install it using a [subprocess](https://docs.python.org/3/library/subprocess.html)
    call at the beginning of our script. In practice, many services, including Amazon
    SageMaker, limit the Linux user permissions in a manner that prevents this option.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The first solution we have presented for customizing our training environment
    is simple to use and allows us to take full advantage of the Docker images that
    were specially designed and optimized by the cloud service provider. However,
    as we have seen, it has a number of limitations. In the [second part of our post](https://chaimrand.medium.com/customizing-your-cloud-based-machine-learning-training-environment-part-2-b65a6cf91812)
    we will discuss two additional approaches that will address these limitations.
  prefs: []
  type: TYPE_NORMAL
