- en: 'The Long and Short of It: Proportion-Based Relevance to Capture Document Semantics
    End-to-End'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/the-long-and-short-of-it-proportion-based-relevance-to-capture-document-semantics-end-to-end-f5a755e5a82f?source=collection_archive---------7-----------------------#2023-11-25](https://towardsdatascience.com/the-long-and-short-of-it-proportion-based-relevance-to-capture-document-semantics-end-to-end-f5a755e5a82f?source=collection_archive---------7-----------------------#2023-11-25)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[](https://medium.com/@alcarazanthony1?source=post_page-----f5a755e5a82f--------------------------------)[![Anthony
    Alcaraz](../Images/6a71a1752677bd07c384246fb0c7f7e8.png)](https://medium.com/@alcarazanthony1?source=post_page-----f5a755e5a82f--------------------------------)[](https://towardsdatascience.com/?source=post_page-----f5a755e5a82f--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----f5a755e5a82f--------------------------------)
    [Anthony Alcaraz](https://medium.com/@alcarazanthony1?source=post_page-----f5a755e5a82f--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F30bc9ffd2f4b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-long-and-short-of-it-proportion-based-relevance-to-capture-document-semantics-end-to-end-f5a755e5a82f&user=Anthony+Alcaraz&userId=30bc9ffd2f4b&source=post_page-30bc9ffd2f4b----f5a755e5a82f---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----f5a755e5a82f--------------------------------)
    ·5 min read·Nov 25, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Ff5a755e5a82f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-long-and-short-of-it-proportion-based-relevance-to-capture-document-semantics-end-to-end-f5a755e5a82f&user=Anthony+Alcaraz&userId=30bc9ffd2f4b&source=-----f5a755e5a82f---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff5a755e5a82f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-long-and-short-of-it-proportion-based-relevance-to-capture-document-semantics-end-to-end-f5a755e5a82f&source=-----f5a755e5a82f---------------------bookmark_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: '*Artificial intelligence software was used to enhance the grammar, flow, and
    readability of this article’s text.*'
  prefs: []
  type: TYPE_NORMAL
- en: Dominant search methods today typically rely on keywords matching or vector
    space similarity to estimate relevance between a query and documents. However,
    these techniques struggle when it comes to searching corpora using entire files,
    papers or even books as search queries.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/88c8318af5222375bec03ff04d3b5364.png)'
  prefs: []
  type: TYPE_IMG
- en: Some fun with Dall-E 3
  prefs: []
  type: TYPE_NORMAL
- en: '**Keyword-based Retrieval**'
  prefs: []
  type: TYPE_NORMAL
- en: While keywords searches excel for short look up, they fail to capture semantics
    critical for long-form content. A document correctly discussing “cloud platforms”
    may be completely missed by a query seeking expertise in “AWS”. Exact term matches
    face vocabulary mismatch issues frequently in lengthy texts.
  prefs: []
  type: TYPE_NORMAL
- en: '**Vector Similarity Search**'
  prefs: []
  type: TYPE_NORMAL
- en: Modern vector embedding models like BERT condensed meaning into hundreds of
    numerical dimensions accurately estimating semantic similarity. However, transformer
    architectures with self-attention don’t scale beyond 512–1024 tokens due to exploding
    computation.
  prefs: []
  type: TYPE_NORMAL
- en: Without the capacity to fully ingest documents, the resulting “bag-of-words”
    partial embeddings lose the nuances of meaning interspersed across sections. The
    context gets lost…
  prefs: []
  type: TYPE_NORMAL
