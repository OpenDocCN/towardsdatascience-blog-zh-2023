- en: 'Elliot Activation Function: What Is It and Is It Effective?'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/elliot-activation-function-what-is-it-and-is-it-effective-59b63ec1fd8a?source=collection_archive---------5-----------------------#2023-02-04](https://towardsdatascience.com/elliot-activation-function-what-is-it-and-is-it-effective-59b63ec1fd8a?source=collection_archive---------5-----------------------#2023-02-04)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: What is the Elliot Activation Function and is it a good alternative to the other
    activation functions used in neural networks?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://ben-mccloskey20.medium.com/?source=post_page-----59b63ec1fd8a--------------------------------)[![Benjamin
    McCloskey](../Images/7118f5933f2affe2a7a4d3375452fa4c.png)](https://ben-mccloskey20.medium.com/?source=post_page-----59b63ec1fd8a--------------------------------)[](https://towardsdatascience.com/?source=post_page-----59b63ec1fd8a--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----59b63ec1fd8a--------------------------------)
    [Benjamin McCloskey](https://ben-mccloskey20.medium.com/?source=post_page-----59b63ec1fd8a--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F503796fc1483&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Felliot-activation-function-what-is-it-and-is-it-effective-59b63ec1fd8a&user=Benjamin+McCloskey&userId=503796fc1483&source=post_page-503796fc1483----59b63ec1fd8a---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----59b63ec1fd8a--------------------------------)
    ·7 min read·Feb 4, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F59b63ec1fd8a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Felliot-activation-function-what-is-it-and-is-it-effective-59b63ec1fd8a&user=Benjamin+McCloskey&userId=503796fc1483&source=-----59b63ec1fd8a---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F59b63ec1fd8a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Felliot-activation-function-what-is-it-and-is-it-effective-59b63ec1fd8a&source=-----59b63ec1fd8a---------------------bookmark_footer-----------)![](../Images/39a7ac7751ff21380e8ecc69c64edfb1.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Elliot Activation Function (Image from Author)
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Are you in the middle of creating a new machine-learning model and unsure of
    what activation function you should be using?
  prefs: []
  type: TYPE_NORMAL
- en: '**But wait, *what is an activation function?***'
  prefs: []
  type: TYPE_NORMAL
- en: Activation functions allow machine learning models to understand and solve *nonlinear*
    problems. Using an activation function in neural networks specifically helps with
    the passing of the most important information from each neuron to the next. Today,
    the ReLU Activation Function is generally used in the architecture of Neural Networks,
    however, that does not necessarily mean it is always the best choice. (Check out
    my post below on the ReLU and LReLU Activations).
  prefs: []
  type: TYPE_NORMAL
- en: '[](/leaky-relu-vs-relu-activation-functions-which-is-better-1a1533d0a89f?source=post_page-----59b63ec1fd8a--------------------------------)
    [## Leaky ReLU vs. ReLU Activation Functions: Which is Better?'
  prefs: []
  type: TYPE_NORMAL
- en: An experiment to investigate if there is a noticeable difference in a model’s
    performance when using a ReLU Activation…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/leaky-relu-vs-relu-activation-functions-which-is-better-1a1533d0a89f?source=post_page-----59b63ec1fd8a--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: I recently came across the **Elliot Activation Function** which was praised
    as being a possible alternative to various activation…
  prefs: []
  type: TYPE_NORMAL
