["```py\n#  exploring patterns in the text to assess how best to cleanse the data\npat_list = [r'\\d', '-', '\\+', ':', '!', '\\?', '\\.', '\\\\n'] # list of special characters/punctuation to search for in data\n\ndef punc_search(df, col, pat):\n    \"\"\"\n    function that counts the number of narratives\n    that contain a pre-defined list of special\n    characters and punctuation\n    \"\"\"\n    for p in pat:\n        v = df[col].str.contains(p).sum() # total n_rows that contain the pattern\n        print(f'{p} special character is present in {v} entries')\n\npunc_search(df, 'text', pat_list)\n\n# the output will look something like this:\n\n\"\"\"\n\\d special character is present in 12846 entries\n- special character is present in 3141 entries\n\\+ special character is present in 71 entries\n: special character is present in 1874 entries\n! special character is present in 117 entries\n\\? special character is present in 53 entries\n\\. special character is present in 16962 entries\n\\n special character is present in 7567 entries\n\"\"\"\n```", "```py\nlemmatizer = WordNetLemmatizer()  # initiating lemmatiser object\n\ndef text_cleanse(df, col):\n    \"\"\"\n    cleanses text by removing special\n    characters and lemmatizing each\n    word\n    \"\"\"\n    df[col] = df[col].str.lower()  # convert text to lowercase\n    df[col] = df[col].str.replace(r'-','', regex=True) # replace hyphens with '' to join hyphenated words together\n    df[col] = df[col].str.replace(r'\\d','', regex=True) # replace numbers with ''\n    df[col] = df[col].str.replace(r'\\\\n','', regex=True) # replace new line symbol with ''\n    df[col] = df[col].str.replace(r'\\W','', regex=True)  # remove special characters\n    df[col] = df[col].str.replace(r'\\s+[a-zA-Z]\\s+',' ', regex=True) # remove single characters\n    df[col] = df.apply(lambda x: nltk.word_tokenize(x[col]), axis=1) # tokenise text ready for lemmatisation\n    df[col] = df[col].apply(lambda x:[lemmatizer.lemmatize(word, 'v') for word in x]) # lemmatise words, use 'v' argument to lemmatise versbs (e.g. turns past participle of a verb to present tense)\n    df[col] = df[col].apply(lambda x : \" \".join(x)) # de-tokenise text ready for vectorisation\n```", "```py\n# create train and test data split\nX_train, X_test, y_train, y_test = train_test_split(df['text'], # features\n                                                    df['target'], # target\n                                                    test_size=0.3, # 70% train 30% test\n                                                    random_state=42, # ensures same split each time to allow repeatability\n                                                    shuffle = True, # shuffles data prior to splitting\n                                                    stratify = df['target']) # distribution of classes across train and test\n```", "```py\n#  defining models and associated parameters\nmodels = [RandomForestClassifier(n_estimators = 100, max_depth=5, random_state=42), \n          LinearSVC(random_state=42),\n          MultinomialNB(), \n          LogisticRegression(random_state=42)]\n\nkf = StratifiedKFold(n_splits=5, shuffle=True, random_state=1) # With StratifiedKFold, the folds are made by preserving the percentage of samples for each class.\n\nscoring = ['accuracy', 'f1_macro', 'recall_macro', 'precision_macro']\n\n#  iterative loop print metrics from each model\nfor model in tqdm(models):\n    model_name = model.__class__.__name__\n    result = cross_validate(model, X_train_vector, y_train, cv=kf, scoring=scoring)\n    print(\"%s: Mean Accuracy = %.2f%%; Mean F1-macro = %.2f%%; Mean recall-macro = %.2f%%; Mean precision-macro = %.2f%%\" \n          % (model_name, \n             result['test_accuracy'].mean()*100, \n             result['test_f1_macro'].mean()*100, \n             result['test_recall_macro'].mean()*100, \n             result['test_precision_macro'].mean()*100))\n```", "```py\nover_pipe = Pipeline([('RandomOverSample', RandomOverSampler(random_state=42)), \n                      ('LinearSVC', LinearSVC(random_state=42))])\n\nparams = {\"LinearSVC__C\": [0.001, 0.01, 0.1, 1, 10, 100]}\n\nsvc_oversample_cv = GridSearchCV(over_pipe, \n                                 param_grid = params, \n                                 cv=kf, \n                                 scoring='f1_macro',\n                                 return_train_score=True).fit(X_train_vector, y_train)\nsvc_oversample_cv.best_score_  # print f1 score\n```"]