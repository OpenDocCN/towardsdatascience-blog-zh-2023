# 从感知机到Adaline

> 原文：[https://towardsdatascience.com/from-the-perceptron-to-adaline-1730e33d41c5?source=collection_archive---------6-----------------------#2023-11-28](https://towardsdatascience.com/from-the-perceptron-to-adaline-1730e33d41c5?source=collection_archive---------6-----------------------#2023-11-28)

## 奠定基础

[](https://medium.com/@cretanpan?source=post_page-----1730e33d41c5--------------------------------)[![Pan Cretan](../Images/8b3fbab70c0e61f7ca516d2f54b646e5.png)](https://medium.com/@cretanpan?source=post_page-----1730e33d41c5--------------------------------)[](https://towardsdatascience.com/?source=post_page-----1730e33d41c5--------------------------------)[![Towards Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----1730e33d41c5--------------------------------) [Pan Cretan](https://medium.com/@cretanpan?source=post_page-----1730e33d41c5--------------------------------)

·

[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fff990ba57425&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffrom-the-perceptron-to-adaline-1730e33d41c5&user=Pan+Cretan&userId=ff990ba57425&source=post_page-ff990ba57425----1730e33d41c5---------------------post_header-----------) 发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----1730e33d41c5--------------------------------) ·11 min read·Nov 28, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F1730e33d41c5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffrom-the-perceptron-to-adaline-1730e33d41c5&user=Pan+Cretan&userId=ff990ba57425&source=-----1730e33d41c5---------------------clap_footer-----------)

--

[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F1730e33d41c5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffrom-the-perceptron-to-adaline-1730e33d41c5&source=-----1730e33d41c5---------------------bookmark_footer-----------)![](../Images/8ac80c2c95ab7e4f13888daeeb72fccc.png)

照片由 [Einar Storsul](https://unsplash.com/@einarstorsul?utm_source=medium&utm_medium=referral) 提供，来源于 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)

## 介绍

在上一篇文章中，我尝试解释了可能存在的最基本的二元分类器，[Rosenblatt的感知机](https://medium.com/towards-data-science/classification-with-rosenblatts-perceptron-e7f49e3af562)。理解这个算法具有教育价值，可以作为初级机器学习课程的良好介绍。这是一个可以在一个下午从零开始编写的算法，并且能够激发兴趣、成就感和深入研究更复杂主题的动力。然而，作为一个算法，它仍然有很多不足，因为只有在类是线性可分的情况下才能保证收敛，这种情况往往不会发生。

在本文中，我们将继续掌握分类概念的旅程。从Rosenblatt的感知机自然演变而来的就是**自适应** **线性** **神经**分类器，俗称adaline。从感知机到adaline的过渡并不是一个大的飞跃。我们只需将阶跃激活函数更改为线性激活函数。这一小的变化导致了一个连续的损失函数，可以被稳健地最小化。这使我们能够引入机器学习中的许多有用概念，如向量化和优化方法。

在未来的文章中，我们还将讨论激活函数和损失函数的进一步细微变化，这将使我们从adaline过渡到逻辑回归，这已经是日常实践中的有用算法。上述所有算法本质上都是单层神经网络，并且可以很容易地扩展到多层神经网络。在这方面，本文将读者带到了这次进化的下一步，并为处理更高级的概念奠定基础。

我们需要一些公式。我使用了在线的[LaTeX方程编辑器](https://latexeditor.lagrida.com/)来开发方程的LaTeX代码，然后使用Chrome插件[数学方程随处可用](https://chromewebstore.google.com/detail/math-equations-anywhere/fkioioejambaepmmpepneigdadjpfamh)将方程渲染成图片。这种方法的唯一缺点是LaTeX代码不会被存储，以备将来需要再次渲染时使用。为此，我在本文末尾提供了方程列表。如果你对LaTex不熟悉，这可能有其自身的教育价值。正确的符号表示是机器学习旅程的一部分。

## 自适应线性神经分类器（adaline）

那么adaline算法是什么呢？Adaline是一个像感知机一样的二元分类器。通过使用一组特征[x₁, .. , xₘ]的输入值来进行预测，其中m是特征的数量。输入值与权重[w₁, .. , wₘ]相乘，并加上偏置，以得到净输入z = w₁x₁ + .. + wₘxₘ + b。净输入通过线性激活函数σ(z)，然后使用阶跃函数进行预测，与感知机类似：

![](../Images/4de8a2c1883431481a490f75dae9cbc9.png)

与感知器的一个关键区别在于，线性激活函数用于学习权重，而阶跃函数仅用于最终的预测。这听起来是件小事，但它具有重要意义。线性激活函数是可微分的，而阶跃函数则不是！上面的阈值0.5并非一成不变。通过调整阈值，我们可以根据我们的使用案例调整精确度和召回率，即根据假阳性和假阴性的成本。

在 adaline 的情况下，线性激活函数就是恒等函数，即 σ(z) = z。需要在训练过程中最小化的目标函数（也称为损失函数）是

![](../Images/951717d16569b60b106264072ed64894.png)

其中 **w** 是权重。

![](../Images/950c38d1374475f49563667d652a7fab.png)

并且 b 是偏置。求和是在训练集中的所有示例上进行的。在一些实现中，损失函数还包括 1/2 系数以便于计算。这个系数在我们对损失函数关于权重和偏置的梯度进行求导时会被抵消，并且正如我们下面将看到的，它除了将学习率缩放两倍外没有其他影响。在本文中，我们不使用 1/2 系数。

对于每个示例，我们计算计算结果与真实类标签之间的平方差。

![](../Images/0428f21b171a5009335f18e58547e0d9.png)

以及真实类标签。注意，输入向量被理解为形状为 (1, m) 的矩阵，即正如我们稍后将看到的，是我们的特征矩阵**x**的一行，其形状为 (n, m)。

训练不过是一个优化问题。我们需要调整权重和偏置，使损失函数最小化。与任何最小化问题一样，我们需要计算目标函数对独立变量的梯度，在我们的情况下，这些变量将是权重和偏置。损失函数关于权重 wⱼ 的偏导数是

![](../Images/36280ce22e0f3219e9f107deaafaac5d.png)

最后一行引入了重要的矩阵符号。特征矩阵 **x** 的形状为 (n, m)，我们对其第 j 列取转置，即形状为 (1, n) 的矩阵。真实类标签 **y** 是一个形状为 (n, 1) 的矩阵。所有样本的净输出 **z** 也是一个形状为 (n, 1) 的矩阵，激活应用于其每个元素后不会改变。上述公式的最终结果是一个标量。你能猜出我们如何使用矩阵符号表示所有权重的梯度吗？

![](../Images/b8f325ce6ba59bbc67f22bce6525a850.png)

特征矩阵的转置形状为 (m, n)。此操作的最终结果是一个形状为 (m, 1) 的矩阵。这种表示法非常重要。我们将使用 [numpy](https://numpy.org/) 进行这种矩阵乘法，而不是使用循环。在神经网络和 GPU 时代，应用向量化的能力至关重要！

那么损失函数关于偏差的梯度如何呢？

![](../Images/b9670bba511407e32efb41e492f9c4e8.png)

其中横线表示向量的均值。再次强调，使用 numpy 计算均值是一个向量化操作，即求和不需要使用循环实现。

一旦我们得到梯度，就可以使用梯度下降优化方法来最小化损失。权重和偏差项会使用以下公式迭代更新

![](../Images/769ddba3bcab91dcc4cc72ee5b19191d.png)

其中 η 是合适选择的学习率。值过小可能会延迟收敛，而值过高则可能完全阻止收敛。需要进行一些实验，这通常是机器学习算法参数调整的一部分。

在上述实现中，我们假设权重和偏差是基于所有样本一起更新的。这被称为全批量梯度下降，是一种极端情况。另一种极端情况是每处理一个训练样本后更新权重和偏差，这被称为随机梯度下降（SGD）。实际上，还有一些中间情况，称为小批量梯度下降，其中权重和偏差是基于样本的子集进行更新的。通常，这种方法收敛速度更快，即我们不需要对整个训练集进行过多的迭代，同时向量化仍然（至少部分地）是可能的。如果训练集非常大（或者模型非常复杂，如现在 NLP 中的变换器），全批量梯度下降可能根本不可行。

## 替代的公式化和闭式解

在我们继续用 Python 实现 adaline 之前，我们将稍作岔开。我们可以将偏差 b 吸收到权重向量中，如下所示

![](../Images/6ff96cb3272df542dfd4908785bed347.png)

在这种情况下，训练集中所有样本的净输出变为

![](../Images/074d30b29d129705bfdffc2f75f2605e.png)

这意味着特征矩阵前面加了一列全是1，从而导致形状为 (n, m+1)。关于组合权重集的梯度变为

![](../Images/514ef3e1d6948525ce50b24bcc970cb7.png)

原则上，我们可以推导出一个闭式解，因为在最小值时所有梯度都将为零

![](../Images/04f9d0536e724cc5365846649870a4cc.png)

实际上，上述方程中的矩阵的逆可能不存在，可能是因为奇异性，或者无法足够准确地计算。因此，这种封闭形式的解决方案在实践中不被使用，无论是在机器学习中还是在数值方法中。然而，了解adeline类似于线性回归，并且具有封闭形式的解决方案仍然很有用。

## 在Python中实现adeline

我们的实现将使用小批量梯度下降。然而，该实现具有灵活性，可以使用随机梯度下降和全批量梯度下降这两个极端来优化损失函数。我们将通过改变批量大小来检查收敛行为。

我们使用一个类来实现adeline，该类在通常的[scikit-learn](https://scikit-learn.org/stable/) API样式中暴露了fit和predict函数。

在初始化时，adeline分类器设置了小批量梯度下降的批量大小。如果批量大小设置为None，则使用整个训练集（全批量梯度下降）；否则，训练集将分批使用（小批量梯度下降）。如果批量大小为1，我们实际上回到了随机梯度下降。在每次训练集遍历之前，训练集会被打乱，以避免重复周期，但这仅在使用小批量梯度下降时才会产生效果。算法的核心在于`_update_weights_bias`函数，该函数完成对训练集的完整遍历，并返回相应的损失。此函数应用了梯度下降法，使用先前章节中的导数计算的梯度。注意使用numpy的`matmul`和`dot`函数，避免了显式循环。如果批量大小设置为None，则完全没有循环，且实现完全矢量化。

## 实践中使用adeline

我们进行必要的导入，并创建一个合成数据集，如之前感知器的[文章](/classification-with-rosenblatts-perceptron-e7f49e3af562)中所示。

这产生了

![](../Images/5dee3d70e7c16dba4c0a25bf0a52e432.png)

合成数据集中两个类别的散点图。作者提供的图像。

与早期文章的唯一区别是我们调整了高斯均值和协方差，使得类别不是线性可分的，因为我们希望adeline能够克服这一点。此外，两个独立变量故意具有不同的尺度，以讨论特征缩放的重要性。

让我们尝试拟合第一个模型并可视化收敛情况。在拟合之前，我们对特征进行归一化，使其均值为零且标准差为单位。

这产生了收敛图。

![](../Images/98546345ebcf7fad86898a6205443025.png)

Adaline收敛。作者提供的图像。

Adaline缓慢收敛，但损失函数不会变为零。为了验证训练的成功，我们使用与早期文章相同的方法可视化决策边界。

产生

![](../Images/c30747fdfbb6ca87ec43952aaff19d3b.png)

拟合的Adaline模型的决策边界。图片由作者提供。

由于训练集中的两个类别不可线性分开且我们使用了线性决策边界，因此存在一些分类错误的点。尽管如此，算法还是良好地收敛了。该解决方案是确定性的。通过足够多的训练集轮次，我们可以获得数值上相等的权重和偏置，无论它们的初始值如何。

**迷你批量与全批量梯度下降**

上述数值实验使用了全批量梯度下降，这部分解释了缓慢收敛的原因。我们将使用与之前相同的数据集和随机状态，但这次我们将使用不同的批量大小来拟合Adaline分类器，范围从20到400，即我们训练集中的样本数量。

产生

![](../Images/f400fc967309960fb862c808c1c04073.png)

批量大小对收敛的影响（使用0.001学习率）。图片由作者提供。

我们可以清楚地看到，批量大小越小，收敛越快，但也会有一些震荡。这些震荡可能会在较大的学习率下使收敛不稳定。如果将学习率加倍到0.002，这一点就会变得明显。

![](../Images/44a8a9c76b5e33ccf0783f8963a9265c.png)

批量大小对收敛的影响（使用0.002学习率）。图片由作者提供。

进一步增加学习率最终会阻止较小批量大小的收敛。即使使用更大的学习率，即使是全批量梯度下降也会因为超越全局最小值而无法收敛。

## 结论

Adaline相比于感知机有了显著的改进。权重和偏置通过最小化一个连续的损失函数来获得，该函数是凸的（因此没有局部极小值）。使用足够小的学习率，即使类别不能线性分开，算法也会收敛。当使用梯度下降及其任何变体时，特征的缩放会影响收敛速度。在这篇文章中，我们使用了简单的标准化，将每个特征的均值调整为零，同时将方差调整为单位方差。通过这种方式，可以选择一个对所有权重和偏置都适用的学习率，这意味着可以在更少的轮次中获得全局最小值。

在深入研究更复杂的主题，如支持向量机和多层神经网络之前，理解如何利用向量化构建二分类器是关键。在日常实践中，可以使用 [scikit-learn](https://scikit-learn.org/stable/)，它提供了先进的分类算法，允许非线性决策边界，并支持高效和系统的超参数调整及交叉验证。然而，从零开始构建简单的二分类器能够提供深入的理解、增强自信并带来掌控感。虽然从头开始构建所有内容显然是不现实的，但深入理解简单算法能提供必要的技能和洞察力，使得从现成库中包含的更高级算法显得不那么神秘。

## 文章中使用的方程的 LaTeX 代码

文章中使用的方程可以在下面的 gist 中找到，以便你可以再次渲染它们。
