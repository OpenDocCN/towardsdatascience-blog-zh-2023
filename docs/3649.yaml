- en: Large Language Models, MirrorBERT — Transforming Models into Universal Lexical
    and Sentence Encoders
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/large-language-models-mirrorbert-transforming-models-into-universal-lexical-and-sentence-511bd592da48?source=collection_archive---------10-----------------------#2023-12-12](https://towardsdatascience.com/large-language-models-mirrorbert-transforming-models-into-universal-lexical-and-sentence-511bd592da48?source=collection_archive---------10-----------------------#2023-12-12)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Discover how mirror augmentation generates data and aces the BERT performance
    on semantic similarity tasks
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@slavahead?source=post_page-----511bd592da48--------------------------------)[![Vyacheslav
    Efimov](../Images/db4b02e75d257063e8e9d3f1f75d9d6d.png)](https://medium.com/@slavahead?source=post_page-----511bd592da48--------------------------------)[](https://towardsdatascience.com/?source=post_page-----511bd592da48--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----511bd592da48--------------------------------)
    [Vyacheslav Efimov](https://medium.com/@slavahead?source=post_page-----511bd592da48--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fc8a0ca9d85d8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flarge-language-models-mirrorbert-transforming-models-into-universal-lexical-and-sentence-511bd592da48&user=Vyacheslav+Efimov&userId=c8a0ca9d85d8&source=post_page-c8a0ca9d85d8----511bd592da48---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----511bd592da48--------------------------------)
    ·7 min read·Dec 12, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F511bd592da48&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flarge-language-models-mirrorbert-transforming-models-into-universal-lexical-and-sentence-511bd592da48&user=Vyacheslav+Efimov&userId=c8a0ca9d85d8&source=-----511bd592da48---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F511bd592da48&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flarge-language-models-mirrorbert-transforming-models-into-universal-lexical-and-sentence-511bd592da48&source=-----511bd592da48---------------------bookmark_footer-----------)![](../Images/786055ae126ec853b631f556033eba06.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It is no secret that BERT-like models play a fundamental role in modern NLP
    applications. Despite their phenomenal performance on downstream tasks, most of
    these models are not that perfect on specific problems without fine-tuning. Embedding
    construction from raw pretrained models usually leads to metrics being far from
    state-of-the-art results. At the same time, fine-tuning is a heavy procedure and
    usually requires at least thousands of annotated data samples to make the model
    better understand the domain data. In some cases, this aspect becomes problematic
    when we cannot simply collect already annotated data or it comes with a high price.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: '**MirrorBERT** was designed to overcome the aforementioned issue. Instead of
    the standard fine-tuning algorithm, MirrorBERT relies on self-supervision by smartly
    augmenting initial data without any external knowledge. This approach allows MirrorBERT
    to show comparable performance on *semantic similarity problems*. Furthermore,
    by using its innovative contrastive learning technique, MirrorBERT can transform
    pretrained models like [BERT](/bert-3d1bf880386a) or [RoBERTa](https://medium.com/towards-data-science/roberta-1ef07226c8d8)
    into universal lexical encoders in less than a minute!'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: '[](/roberta-1ef07226c8d8?source=post_page-----511bd592da48--------------------------------)
    [## Large Language Models: RoBERTa — A Robustly Optimized BERT Approach'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: Learn about key techniques used for BERT optimisation
  id: totrans-12
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/roberta-1ef07226c8d8?source=post_page-----511bd592da48--------------------------------)
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: With the help of the official [MirrorBERT paper](https://arxiv.org/pdf/2104.08027.pdf),
    we will dive into its crucial details to understand how it works under the hood.
    The obtained knowledge will be universal as the discussed techniques could then
    be used for other NLP models dealing with similarity tasks as well.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: Methodology
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To put it simply, MirrorBERT is the same BERT model except for several steps
    introduced in its learning process. Let us discuss each of them.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6246e91c630233298c6072c1d634a84f.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
- en: MirrorBERT learning process
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: 1\. Self-duplication
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As the name suggests, MirrorBERT simply duplicates the initial data.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2c9a0786ab2891c98633d61f547b057e.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
- en: Self-duplication
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: This duplicated data is then used to further construct two different embedding
    representations of the same strings.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Data augmentation
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The authors of the paper propose two intuitive techniques that slightly modify
    dataset texts. According to them, in a vast majority of cases, these text corruptions
    do not change their meaning.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: 2.1\. Input augmentation
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Given a pair of strings (xᵢ, x̄ᵢ), the algorithm randomly chooses one of them
    and applies **random span masking** consisting of a random substitution of a substring
    of a fixed length *k* in the text with the [MASK] token.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/63f14a1113cf5967175836dd7075d40d.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
- en: Input augmentation through random span masking
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: 2.2\. Feature augmentation
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Random span masking operates on sentence- / phrase-level. To make a model able
    to work well on word-level tasks as well, another augmentation mechanism is needed
    operating on shorter text fragments. Feature augmentation solves this problem
    by using dropout.
  id: totrans-31
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 随机跨度掩蔽操作在句子/短语级别。为了使模型也能在词级任务中表现良好，还需要另一种机制来处理较短的文本片段。特征增强通过使用 dropout 解决了这个问题。
- en: The **dropout** process refers to turning off a given percentage *p* of neurons
    in a certain network layer. This can be viewed as the equivalent of zeroing corresponding
    neurons in the network.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '**Dropout** 过程指的是在某一网络层中关闭一定百分比的 *p* 神经元。这可以视为将网络中对应的神经元置零的等效操作。'
- en: The authors of the paper propose using dropout for data augmentation. When a
    pair of strings (xᵢ, x̄ᵢ) is passed to the network with dropout layers, their
    output representations will be slightly different if, on each forward pass, dropout
    layers always disable different neurons.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 论文的作者建议使用 dropout 进行数据增强。当一对字符串 (xᵢ, x̄ᵢ) 传递到具有 dropout 层的网络中时，如果每次前向传递时 dropout
    层总是禁用不同的神经元，则它们的输出表示会略有不同。
- en: The great aspect of using dropout for feature augmentation is that the dropout
    layers are already included in BERT / RoBERTa architecture meaning no additional
    implementation is needed!
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 dropout 进行特征增强的一个很棒的方面是 dropout 层已经包含在 BERT / RoBERTa 架构中，这意味着无需额外的实现！
- en: While random span masking is applied only to each second object in the dataset,
    the dropout is applied to all of them.
  id: totrans-35
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 虽然随机跨度掩蔽仅应用于数据集中的每第二个对象，但 dropout 是应用于所有对象的。
- en: 3\. Constrastive learning
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3\. 对比学习
- en: '**Contrastive learning** is a machine learning technique consisting of learning
    data representations in a way that similar objects lie close to each other in
    the embedding space while dissimilar are far away from each other.'
  id: totrans-37
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**对比学习** 是一种机器学习技术，旨在学习数据表示，使得相似的对象在嵌入空间中彼此接近，而不相似的对象彼此远离。'
- en: One of the ways of constrastive learning implementation is the usage of a **constrastive
    loss function**. The one chosen for MirrorBERT is ***InfoNCELoss***. Let us understand
    how it works.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 对比学习实现的一种方法是使用 **对比损失函数**。MirrorBERT 选择的损失函数是 ***InfoNCELoss***。让我们理解它是如何工作的。
- en: '**InfoNCELoss**'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '**InfoNCELoss**'
- en: At first sight, the formula for *InfoNCELoss* might look intimidating, so let
    us gradually come to it step by step.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 初看起来，*InfoNCELoss* 的公式可能令人畏惧，所以让我们一步步逐渐理解。
- en: The cosine similarity between two vectors measures how close they align to each
    other taking values in the range from -1 to 1 with greater values indicating higher
    similarity.
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 两个向量之间的余弦相似度衡量它们彼此对齐的程度，取值范围从 -1 到 1，值越大表示相似度越高。
- en: '![](../Images/cb4165ec044813fff40e2d051d3d4715.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cb4165ec044813fff40e2d051d3d4715.png)'
- en: Cosine similarity between two vectors
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 两个向量之间的余弦相似度
- en: 2\. To better understand the next steps, it is necessary to be aware that *InfoNCELoss*
    uses softmax transformation with the temperature parameter T controlling the smoothness
    of the output softmax distribution. That is why similarities are divided by T.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 2\. 为了更好地理解下一步，必须了解 *InfoNCELoss* 使用了 softmax 转换，其中温度参数 T 控制输出 softmax 分布的平滑度。这就是为什么相似度除以
    T。
- en: For more information about softmax temperature, refer to [this article](https://medium.com/towards-data-science/distilbert-11c8810d29fc)
    explaining it in more detail.
  id: totrans-45
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 关于 softmax 温度的更多信息，请参阅 [这篇文章](https://medium.com/towards-data-science/distilbert-11c8810d29fc)
    以了解更详细的解释。
- en: '![](../Images/1041d8194b2bd4dbb761d5e7dd99c559.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1041d8194b2bd4dbb761d5e7dd99c559.png)'
- en: Cosine similarity divided by the temperature
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 余弦相似度除以温度
- en: 3\. As in the standard softmax formula, a prediction (similarity) is then transformed
    to the exponent form.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 3\. 与标准 softmax 公式一样，预测（相似度）会被转换为指数形式。
- en: '![](../Images/48220b69b90b9d528b1201a70829643e.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/48220b69b90b9d528b1201a70829643e.png)'
- en: Exponent of cosine similarity
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 余弦相似度的指数
- en: '4\. In the normal softmax formula, the numerator contains an exponent of a
    class probability whereas the denominator is the exponential sum of all distribution
    probabilities. In the case with similarities in *InfoNCELoss*, the formula analogously
    follows the logic:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 4\. 在普通的 softmax 公式中，分子包含了类别概率的指数，而分母则是所有分布概率的指数和。在 *InfoNCELoss* 中，类似度的公式也遵循类似的逻辑：
- en: The numerator contains exponential similarity of two slightly modified identical
    strings (xᵢ, x̄ᵢ) which can be thought of as a *positive example*.
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分子包含两个稍微修改的相同字符串 (xᵢ, x̄ᵢ) 的指数相似度，可以被视为 *正例*。
- en: The denominator consists of a sum of exponential similarities between xᵢ and
    all other dataset strings xⱼwhich can be seen as a set of *all negative examples*.
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/71f072adfc5040e087c104b86521453c.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
- en: Softmax formula for cosine similarity. Nᵢ denotes all dataset strings except
    for xᵢ and x̄ᵢ.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: 5\. In the ideal scenario, we want the similarity between identical strings
    (xᵢ, x̄ᵢ) to be high while the similarity of xᵢ with other strings xⱼ to be low.
    If it is true, then the numerator in the formula above will increase while the
    denominator will decrease making the whole expression larger.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: 'Loss functions work inversely: in ideal cases, they take smaller values, and,
    in worse situations, they highly penalise the model. To make the formula above
    compatible with this loss principle, let us add the *negative logarithm* before
    the whole expression.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/08b63c2c2bb05cd1d427075d0cb09d57.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
- en: Negative log of softmax similarities. This expression can be viewed as a loss
    value for a single string xᵢ.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: 6\. The expression in the previous step already corresponds to a loss value
    for a single string xᵢ. Since the dataset consists of many strings, we need to
    take all of them into account. For that, let us sum up this expression for all
    the strings.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/cebef6dbecffc5bbd9148f2d04687326.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
- en: InfoNCELoss
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: The obtained formula is exactly the *InfoNCELoss*!
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: InfoNCELoss tries to group similar objects close to each other while pushing
    away the dissimilar ones in the embedding space.
  id: totrans-64
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Triplet loss used in [SBERT](https://medium.com/towards-data-science/sbert-deb3d4aef8a4)
    is another example of contrastive learning loss.
  id: totrans-65
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[](/sbert-deb3d4aef8a4?source=post_page-----511bd592da48--------------------------------)
    [## Large Language Models: SBERT — Sentence-BERT'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
- en: Learn how siamese BERT networks accurately transform sentences into embeddings
  id: totrans-67
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/sbert-deb3d4aef8a4?source=post_page-----511bd592da48--------------------------------)
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: Training resources
  id: totrans-69
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A surprising fact about MirrorBERT is that it does not require a lot of data
    to be fine-tuned. Furthermore, this data does not have to be external as the whole
    training process is self-supervised.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: The researchers report that for fine-tuning lexical representations, they use
    only 10k most frequent words in each language. For sentence-level tasks, 10k sentences
    are used.
  id: totrans-71
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Training details
  id: totrans-72
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The details on the MirrorBERT training are listed below:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
- en: The temperature is set to *T = 0.04* in sentence-level tasks and to *T = 0.2*
    in word-level tasks.
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In random span masking, *k* is set to 5.
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dropout is set to *p = 0.1*.
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: AdamW optimizer is used with a learning rate of *2e-5*.
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The batch size is set to 200 (or 400 with duplicates).
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lexical models are trained for 2 epochs and sentence-level models are trained
    for a single epoch.
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Instead of the mean pooling of all output token representations, the [CLS] token
    representation is created.
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Single MirrorBERT training epoch needs only 10–20 seconds.
  id: totrans-81
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Evaluations
  id: totrans-82
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The authors evaluated metrics on a set of benchmarks by applying mirror fine-tuning.
    The results were reported on three types of tasks: lexical, sentence-level and
    cross-lingual. In each of them, MirrorBERT demonstrated comparable performance
    to other BERT-like fine-tuned models.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 作者通过应用镜像微调在一组基准测试上评估了指标。结果在三种任务类型上进行了报告：词汇级别、句子级别和跨语言。在每种任务中，MirrorBERT 展现了与其他
    BERT 类微调模型相当的性能。
- en: The results also showed that the range between 10k and 20k training examples
    is the most optimal for fine-tuning. The performance of the model gradually decreases
    with more training examples.
  id: totrans-84
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 结果还显示，10k 到 20k 的训练样本范围是微调的最优范围。随着训练样本数量的增加，模型的性能逐渐下降。
- en: Conclusion
  id: totrans-85
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: 'Mirror fine-tuning literally acts like a magic spell: instead of heavy fine-tuning
    procedures, the mirror framework requires much less time without the use of external
    data being on par with other fine-tuned models like BERT, SBERT or RoBERTa on
    semantic similarity tasks.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 镜像微调实际上就像一个魔法咒语：与繁重的微调程序不同，镜像框架所需的时间要少得多，而且不需要外部数据，其性能与 BERT、SBERT 或 RoBERTa
    等其他微调模型在语义相似性任务上相当。
- en: As a result, MirrorBERT can transform BERT-like pretrained model into universal
    encoders capturing linguistic knowledge with high efficiency.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，MirrorBERT 可以将类似 BERT 的预训练模型转变为通用编码器，以高效捕捉语言知识。
- en: Resources
  id: totrans-88
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 资源
- en: '[Fast, Effective, and Self-Supervised: Transforming Masked Language Models
    into Universal Lexical and Sentence Encoders](https://arxiv.org/pdf/2104.08027.pdf)'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[快速、有效且自监督：将掩蔽语言模型转化为通用词汇和句子编码器](https://arxiv.org/pdf/2104.08027.pdf)'
- en: '*All images unless noted are by the author*'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '*除非特别说明，所有图像均由作者提供*'
