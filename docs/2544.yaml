- en: Increase Llama 2's Latency and Throughput Performance by Up to 4X
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/increase-llama-2s-latency-and-throughput-performance-by-up-to-4x-23034d781b8c?source=collection_archive---------1-----------------------#2023-08-09](https://towardsdatascience.com/increase-llama-2s-latency-and-throughput-performance-by-up-to-4x-23034d781b8c?source=collection_archive---------1-----------------------#2023-08-09)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Real-world benchmarks for Llama-2 13B
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@het.trivedi05?source=post_page-----23034d781b8c--------------------------------)[![Het
    Trivedi](../Images/f6f11a66f60cacc6b553c7d1682b2fc6.png)](https://medium.com/@het.trivedi05?source=post_page-----23034d781b8c--------------------------------)[](https://towardsdatascience.com/?source=post_page-----23034d781b8c--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----23034d781b8c--------------------------------)
    [Het Trivedi](https://medium.com/@het.trivedi05?source=post_page-----23034d781b8c--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fce8ebd0c262c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fincrease-llama-2s-latency-and-throughput-performance-by-up-to-4x-23034d781b8c&user=Het+Trivedi&userId=ce8ebd0c262c&source=post_page-ce8ebd0c262c----23034d781b8c---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----23034d781b8c--------------------------------)
    ·7 min read·Aug 9, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F23034d781b8c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fincrease-llama-2s-latency-and-throughput-performance-by-up-to-4x-23034d781b8c&user=Het+Trivedi&userId=ce8ebd0c262c&source=-----23034d781b8c---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F23034d781b8c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fincrease-llama-2s-latency-and-throughput-performance-by-up-to-4x-23034d781b8c&source=-----23034d781b8c---------------------bookmark_footer-----------)![](../Images/26cdb97ee3584c0c46666f945c041218.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Image By Author — Created using Stable Diffusion
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the realm of large language models (LLMs), integrating these advanced systems
    into real-world enterprise applications is a pressing need. However, the pace
    at which generative AI is evolving is so quick that most can’t keep up with the
    advancements.
  prefs: []
  type: TYPE_NORMAL
- en: 'One solution is to use managed services like the ones provided by OpenAI. These
    managed services offer a streamlined solution, yet for those who either lack access
    to such services or prioritize factors like security and privacy, an alternative
    avenue emerges: open-source tools.'
  prefs: []
  type: TYPE_NORMAL
- en: Open-source generative AI tools are extremely popular right now and companies
    are scrambling to get their AI-powered apps out the door. While trying to build
    quickly, companies oftentimes forget that in order to truly gain value from generative
    AI they need to build “production”-ready apps, not just prototypes.
  prefs: []
  type: TYPE_NORMAL
- en: In this article, I want to show you the performance difference for Llama 2 using
    two different inference methods. The first method of inference will be a containerized
    Llama 2 model served via Fast API, a popular choice among developers for serving
    models as REST API endpoints. The second method will be the same containerized
    model served via [Text Generation Inference](https://github.com/huggingface/text-generation-inference),
    an open-source library developed by hugging face to easily deploy LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: Both methods we’re looking at are meant to work well for real-world use, like
    in businesses or apps. But it’s important to realize that they don’t scale the
    same way. We’ll dive into this comparison to see how they each perform and understand
    the differences better.
  prefs: []
  type: TYPE_NORMAL
- en: What powers LLM inference at OpenAI and Cohere
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Have you ever wondered why ChatGPT is so fast?
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Large language models require a ton of computing power and due to their sheer
    size, they oftentimes need multiple GPUs. When working with large GPU clusters,
    companies have to be very mindful of how their computing is being utilized.
  prefs: []
  type: TYPE_NORMAL
- en: LLM providers like OpenAI run large GPU clusters to power inferencing for their
    models. In order to squeeze as much performance from their GPUs they use tools
    like the [**Nvidia Triton inference server**](https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/index.html)to
    increase throughput and reduce latency.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ccdba02d89abddcc6f0400743e72c945.png)'
  prefs: []
  type: TYPE_IMG
- en: Illustration inspired by — [Triton Inference Server Architecture](https://developer.nvidia.com/triton-inference-server)
  prefs: []
  type: TYPE_NORMAL
- en: While Triton is highly performant and has many benefits, it’s quite difficult
    to use for developers. A lot of the newer models on Hugging Face are not supported
    on Triton and the process to add support for these models is not trivial.
  prefs: []
  type: TYPE_NORMAL
- en: This is where [**Text Generation Inference**](https://github.com/huggingface/text-generation-inference)
    **(TGI)** comes in. This tool offers many of the same performance improvements
    as Triton, but it’s user-friendly and works well with Hugging Face models.
  prefs: []
  type: TYPE_NORMAL
- en: LLM inference optimizations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Before we jump into the benchmarks, I want to cover a few of the optimization
    techniques used by modern inference servers such as TGI to speed up LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: '**Tensor Parallelism**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: LLMs are often too large to fit on a single GPU. Using a concept called **Model
    Parallelism**, a model can be split across multiple GPUs. **Tensor Parallelism**
    is a type of model parallelism that splits the model into multiple shards that
    are processed independently by different GPUs.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/45eec79a2e1998a5ffa7a1b793b302ed.png)'
  prefs: []
  type: TYPE_IMG
- en: Illustration inspired by [source](https://aws.amazon.com/blogs/machine-learning/deploy-large-models-at-high-performance-using-fastertransformer-on-amazon-sagemaker/)
  prefs: []
  type: TYPE_NORMAL
- en: To put it simply, imagine you’re working on a big jigsaw puzzle, but it’s so
    huge that you can’t put all the pieces on one table. So, you decide to work with
    your friend. You split the puzzle into sections, and each of you works on your
    own section at the same time. This way, you can solve the puzzle faster.
  prefs: []
  type: TYPE_NORMAL
- en: '**2\. Continuous batching**'
  prefs: []
  type: TYPE_NORMAL
- en: When you make an API call to an LLM it processes it in one go and returns the
    output. If you make 5 API calls, it will process each one sequentially. This essentially
    means we have a batch size of 1, meaning only 1 request can be processed at a
    given time. As you might guess, this design isn’t ideal because each new request
    has to wait for the one before it to complete.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/747627ad956e1a700b820f5671378f18.png)'
  prefs: []
  type: TYPE_IMG
- en: Illustration inspired by [Static Batching](https://www.anyscale.com/blog/continuous-batching-llm-inference)
    — Need to wait for all processes to finish before handling more requests
  prefs: []
  type: TYPE_NORMAL
- en: By increasing the batch size you can handle more requests in parallel. With
    a batch size of 4, you would be able to handle 4 of the 5 API calls in parallel.
    You would have to wait until all 4 requests have finished before handling the
    5th request.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7b94fc126972c1f54510bdb069b0f160.png)'
  prefs: []
  type: TYPE_IMG
- en: Illustration inspired by [Continuous Batching](https://www.anyscale.com/blog/continuous-batching-llm-inference)
    — You can handle new requests immediately without waiting for all processes to
    finish
  prefs: []
  type: TYPE_NORMAL
- en: '**Continuous batching** builds on the idea of using a bigger batch size and
    goes a step further by immediately tackling new tasks as they come in. For example,
    let’s say that your GPU has a batch size of 4 meaning it can handle 4 requests
    in parallel. If you make 5 requests, 4 of them will get processed in parallel
    and whichever process finishes first will immediately get to work on the 5th request.'
  prefs: []
  type: TYPE_NORMAL
- en: Llama 2 Benchmarks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now that we have a basic understanding of the optimizations that allow for faster
    LLM inferencing, let’s take a look at some practical benchmarks for the **Llama-2
    13B** model.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are 2 main metrics I wanted to test for this model:'
  prefs: []
  type: TYPE_NORMAL
- en: Throughput (tokens/second)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Latency (time it takes to complete one full inference)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: I wanted to compare the performance of Llama inference using two different instances.
    One instance runs via FastAPI, while the other operates through TGI. Both setups
    utilize GPUs for computation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Note: Neither of the two instances quantizes the weights of the model.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The TGI setup employs the power of two GPUs*(NVIDIA RTX A4000)* to leverage
    parallelism, whereas FastAPI relies on a single*(NVIDIA A100)*, albeit more powerful
    GPU.
  prefs: []
  type: TYPE_NORMAL
- en: Making a direct comparison between these two approaches is a bit tricky since
    FastAPI doesn’t allow the model to be distributed across two GPUs. To level the
    playing field, I opted to equip the FastAPI instance with a more powerful GPU.
  prefs: []
  type: TYPE_NORMAL
- en: Each API request made to the model had the exact **same prompt** and the generated
    output token limit was set to **128 tokens**.
  prefs: []
  type: TYPE_NORMAL
- en: '**Throughput Results:**'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8a515b42238c85ff8d70cf1b0e267765.png)![](../Images/f1b2dde2c315b7fd66019670592d9e1a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Left: Fast API vs TGI throughput | Right: Average throughput increase — Illustration
    by Author'
  prefs: []
  type: TYPE_NORMAL
- en: 'Analysis:'
  prefs: []
  type: TYPE_NORMAL
- en: In both cases, throughput decreases as the number of inference requests increases
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Batching significantly improves the throughput for an LLM, hence why TGI has
    better throughput
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Although the Fast API instance has a larger GPU memory (VRAM) available to manage
    requests as larger batches, it doesn’t handle this process efficiently
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Latency Results:**'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d91f5d18b02d21d95e6679353030e562.png)![](../Images/c2599ec06642198c04ef397ea1a30377.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Left: Fast API vs TGI latency | Right: Average latency performance increase
    — Illustration by Author'
  prefs: []
  type: TYPE_NORMAL
- en: 'Analysis:'
  prefs: []
  type: TYPE_NORMAL
- en: Tensor parallelism results in a nearly 5X reduction in latency for TGI!
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As the number of requests increases, the latency for the Fast API-based instance
    surpasses 100 seconds
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As evident from the results, optimized inference servers are highly performant
    compared to readily available API wrappers. As a final test, I wanted to evaluate
    the performance of TGI when the generated output token limit is increased to **256
    tokens** as opposed to **128 tokens**.
  prefs: []
  type: TYPE_NORMAL
- en: '**Throughput TGI 128 tokens vs 256 tokens:**'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a0dc8d9f8996937171d30a949de765c9.png)'
  prefs: []
  type: TYPE_IMG
- en: 128 token vs 256 token TGI throughput test — Illustration by Author
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, the throughput is quite similar despite doubling the number
    of generated tokens. One thing to note that’s not on this chart is that at 300
    concurrent requests, the throughput dwindled to approximately 2 tokens/sec while
    producing a 256-token output. At this throughput, the latency was over 100 seconds
    per request and there were multiple request timeouts. Due to these substantial
    performance limitations, the results from this scenario were excluded from the
    chart.
  prefs: []
  type: TYPE_NORMAL
- en: '**Latency TGI 128 tokens vs 256 tokens:**'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/13370bf7f28cb05d33a5817d47669502.png)'
  prefs: []
  type: TYPE_IMG
- en: 128 token vs 256 token TGI latency test — Illustration by Author
  prefs: []
  type: TYPE_NORMAL
- en: Unlike the throughput, the latency clearly spikes when generating longer sequences
    of text. Adding more GPUs would help decrease the latency, but it would come at
    a financial cost.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: My goal for this blog post was to compare the real-world performance of LLMs
    at scale*(hundreds of requests per second).* Often times it’s easy to deploy models
    behind an API wrapper such as Fast API, but in the case of LLMs, you’d be leaving
    a significant amount of performance on the table.
  prefs: []
  type: TYPE_NORMAL
- en: Even with the optimization techniques used by modern inference servers, the
    performance cannot match that of a managed service like ChatGPT. OpenAI surely
    runs several large GPU clusters to power inference for their models along with
    their own in-house optimization techniques.
  prefs: []
  type: TYPE_NORMAL
- en: However, for generative AI use cases enterprises will likely have to adopt inference
    servers as they are far more scalable and reliable compared to traditional model
    deployment techniques.
  prefs: []
  type: TYPE_NORMAL
- en: Thanks for reading!
  prefs: []
  type: TYPE_NORMAL
- en: Peace.
  prefs: []
  type: TYPE_NORMAL
