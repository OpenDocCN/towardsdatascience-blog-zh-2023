- en: Three Fundamental Flaws In Common Reinforcement Learning Algorithms (And How
    To Fix Them)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/three-fundamental-flaws-in-common-reinforcement-learning-algorithms-and-how-to-fix-them-951160b7a207?source=collection_archive---------9-----------------------#2023-01-30](https://towardsdatascience.com/three-fundamental-flaws-in-common-reinforcement-learning-algorithms-and-how-to-fix-them-951160b7a207?source=collection_archive---------9-----------------------#2023-01-30)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Harness yourself against these shortcomings encountered in everyday RL algorithms
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://wvheeswijk.medium.com/?source=post_page-----951160b7a207--------------------------------)[![Wouter
    van Heeswijk, PhD](../Images/9c996bccd6fdfb6d9aa8b50b93338eb9.png)](https://wvheeswijk.medium.com/?source=post_page-----951160b7a207--------------------------------)[](https://towardsdatascience.com/?source=post_page-----951160b7a207--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----951160b7a207--------------------------------)
    [Wouter van Heeswijk, PhD](https://wvheeswijk.medium.com/?source=post_page-----951160b7a207--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F33f45c9ab481&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthree-fundamental-flaws-in-common-reinforcement-learning-algorithms-and-how-to-fix-them-951160b7a207&user=Wouter+van+Heeswijk%2C+PhD&userId=33f45c9ab481&source=post_page-33f45c9ab481----951160b7a207---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----951160b7a207--------------------------------)
    ·8 min read·Jan 30, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F951160b7a207&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthree-fundamental-flaws-in-common-reinforcement-learning-algorithms-and-how-to-fix-them-951160b7a207&user=Wouter+van+Heeswijk%2C+PhD&userId=33f45c9ab481&source=-----951160b7a207---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F951160b7a207&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthree-fundamental-flaws-in-common-reinforcement-learning-algorithms-and-how-to-fix-them-951160b7a207&source=-----951160b7a207---------------------bookmark_footer-----------)![](../Images/9dda11eec29ec887f349772c00f43653.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Photo by [Varvara Grabova](https://unsplash.com/@santabarbara77?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: Reinforcement Learning algorithms such as Q-learning and REINFORCE have been
    around for decades and their textbook implementations are still widely used. Unfortunately,
    they exhibit some fundamental flaws, which greatly increase the struggle of learning
    a good policy.
  prefs: []
  type: TYPE_NORMAL
- en: This article addresses three major shortcomings of classical Reinforcement Learning
    algorithms, along with solutions to overcome them.
  prefs: []
  type: TYPE_NORMAL
- en: I. Selecting overvalued actions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Most RL algorithms use some notion of value functions to capture downstream
    rewards, many of them based on the well-known Q-learning algorithm. The mechanism
    driving Q-learning is that it **selects the action that yields the highest expected
    value**. Depending on initialization, this mechanism might get stuck at the first
    action that is tried, so we also select random actions with probability ϵ, typically
    set at 0.05 or so.
  prefs: []
  type: TYPE_NORMAL
- en: In the limit, we would explore each action infinitely often and the Q-values
    would converge to the true values. In practice, however, we work with…
  prefs: []
  type: TYPE_NORMAL
