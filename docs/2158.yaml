- en: Code understanding on your own hardware
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/code-understanding-on-your-own-hardware-dd38c4f266d6?source=collection_archive---------8-----------------------#2023-07-05](https://towardsdatascience.com/code-understanding-on-your-own-hardware-dd38c4f266d6?source=collection_archive---------8-----------------------#2023-07-05)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Setting up an LLM to talk about your code — with LangChain and local hardware
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@doriandrost?source=post_page-----dd38c4f266d6--------------------------------)[![Dorian
    Drost](../Images/1795395ad0586eafd83d3e2f7b975ca8.png)](https://medium.com/@doriandrost?source=post_page-----dd38c4f266d6--------------------------------)[](https://towardsdatascience.com/?source=post_page-----dd38c4f266d6--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----dd38c4f266d6--------------------------------)
    [Dorian Drost](https://medium.com/@doriandrost?source=post_page-----dd38c4f266d6--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F1d49ea537d1c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcode-understanding-on-your-own-hardware-dd38c4f266d6&user=Dorian+Drost&userId=1d49ea537d1c&source=post_page-1d49ea537d1c----dd38c4f266d6---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----dd38c4f266d6--------------------------------)
    ·7 min read·Jul 5, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fdd38c4f266d6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcode-understanding-on-your-own-hardware-dd38c4f266d6&user=Dorian+Drost&userId=1d49ea537d1c&source=-----dd38c4f266d6---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fdd38c4f266d6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcode-understanding-on-your-own-hardware-dd38c4f266d6&source=-----dd38c4f266d6---------------------bookmark_footer-----------)![](../Images/14ea7eeae6ca02ac579d218a93a066c4.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: I promise your code won’t leave your local hardware. Photo by [Clément Hélardot](https://unsplash.com/@clemhlrdt?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: Among the various tasks Large Language Models (LLMs) can perform today, code
    understanding may be of particular interest for you, if you work with source code
    as a software developer or a data scientist. Wouldn’t it be great to have a chatbot
    you can ask questions about your code? *Where is the data preprocessing implemented?*
    *Is there a function for verifying the user’s authentication already? What is
    the difference between the calculate_vector_dim and calculate_vector_dimension
    function?* Instead of searching for the correct file yourself, just ask the bot
    and it gives you an answer, together with a pointer to the files that contain
    the relevant code snippets. That mechanism is called semantic search, and you
    can imagine how useful it is.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在今天，大型语言模型（LLMs）能够执行的各种任务中，代码理解可能对你尤其感兴趣，如果你是一名软件开发者或数据科学家。拥有一个你可以询问代码问题的聊天机器人不是很好吗？*数据预处理在哪里实现的？*
    *是否已经有验证用户身份的函数？calculate_vector_dim 和 calculate_vector_dimension 函数之间有什么区别？*
    你不必自己搜索正确的文件，只需问机器人，它会给你答案，并指向包含相关代码片段的文件。这种机制叫做语义搜索，你可以想象它的实用性。
- en: In this tutorial, I will show you how to implement a LangChain bot that does
    exactly that. In addition, I will focus on the specific, data-privacy-related
    issue of not giving your code out of hand. The code you or your company produced
    is private property and may contain sensitive information or valuable knowledge.
    You may not want to, or your company's policies may not allow you to send it to
    an LLM hosted by another company, that may be located in a foreign country. Hence
    in this tutorial, I will show you how to set up a code understanding bot that
    runs on your local hardware, so your code never leaves your infrastructure.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个教程中，我将展示如何实现一个完全符合要求的 LangChain 机器人。此外，我将关注具体的数据隐私问题，即不把你的代码交出去。你或你的公司生产的代码是私有财产，可能包含敏感信息或宝贵的知识。你可能不希望，或者公司政策可能不允许你将其发送到另一个公司托管的
    LLM，那个公司可能位于外国。因此，在本教程中，我将展示如何设置一个运行在本地硬件上的代码理解机器人，以便你的代码不会离开你的基础设施。
- en: Let’s start already! First I will give you a brief introduction to the general
    process of semantic search before we implement a bot for code understanding.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在就开始吧！首先，我会给你简要介绍一下语义搜索的一般过程，然后我们再实现一个用于代码理解的机器人。
- en: Introduction to semantic search
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 语义搜索简介
- en: '![](../Images/d2a2495d247f2a0c813df9e0d7df3d07.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d2a2495d247f2a0c813df9e0d7df3d07.png)'
- en: In semantic search, it’s all about finding the relevant documents. Photo by
    [Markus Spiske](https://unsplash.com/@markusspiske?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在语义搜索中，关键是找到相关的文档。照片由 [Markus Spiske](https://unsplash.com/@markusspiske?utm_source=medium&utm_medium=referral)
    拍摄，来源于 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: 'First of all, let me briefly explain the general idea of semantic search. This
    approach consists of two main steps, that are the retrieval and the answer generation
    by the LLM itself. In the retrieval step, documents are selected that contain
    relevant information, and those are fed into the LLM to create a natural language
    answer. For example, if you ask a question about a function called *transform_vectors,*
    the retrieval will select those files that are relevant to answer that question.
    That may include the file where the *transform_vectors* function is implemented,
    but also files using it or parts of the documentation mentioning it. In the second
    step, those files’ content is given to the LLM in a prompt that may look somewhat
    like that:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我简要说明一下语义搜索的一般思路。这种方法包括两个主要步骤：检索和 LLM 本身生成答案。在检索步骤中，选择包含相关信息的文档，然后将这些文档输入
    LLM 以生成自然语言答案。例如，如果你问一个关于名为*transform_vectors*的函数的问题，检索步骤会选择那些与回答该问题相关的文件。这可能包括实现*transform_vectors*函数的文件，也包括使用它的文件或提及它的文档部分。在第二步，这些文件的内容会被作为提示提供给
    LLM，提示可能如下所示：
- en: '[PRE0]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The LLM creates a natural language answer to the question using information
    from the documents given to it.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: LLM 使用来自提供的文档的信息生成自然语言答案。
- en: That is the main idea of semantic search. Now let’s start implementing! First
    of all, we have to install our requirements and read in our data.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是语义搜索的主要思想。现在我们开始实现吧！首先，我们需要安装我们的要求并读取数据。
- en: Install requirements
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 安装要求
- en: 'Before we can start, make sure you have set up an environment running Python
    and install the following packages:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Read in the documents
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now we need to read in the data and convert it into a format LangChain can
    work with. For this demonstration, I will download the code of LangChain itself,
    but you can use your own code base, of course:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: We load all files and convert them to a *Document* each, i.e. each *Document*
    will contain exactly one file of the code base.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Retrieval
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/b141a8ee6f486c209b660f19068f37ec.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
- en: Which of these is relevant to answering our question? It’s the retrieval’s job
    to decide that. Photo by [Ed Robertson](https://unsplash.com/@eddrobertson?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have created our *Documents*, we need to index them to make them
    searchable. To index a *Document* means to calculate a numerical vector, that
    captures the most relevant information of the *Document*. Unlike plain text itself,
    a vector of numbers can be used to perform numerical calculations, and that means
    that we can easily calculate a similarity on it, which is then used to determine
    which *Documents* are relevant to answer a given question.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: On a technical level, this index we will create with the help of an embedding
    and store it in a *VectorStore*. There are *VectorStores* available as a service
    (e.g. [DeepLake](https://www.deeplake.ai)), which comes with some handy advantages,
    but in our scenario, we don’t want to give the code out of our hands, so we create
    a *VectorStore* locally on our machine. The easiest way to do that is using *Chroma*,
    which creates a *VectorStore* in memory and allows us to persist it.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Within the *from_documents* function, the indices are calculated and stored
    in the *Chroma* database*.* Next time, instead of calling the *from_documents*
    function again, we can load the persisted *Chroma* database itself:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: As you saw above, as an embedding I used [*krlvi/sentence-t5-base-nlpl-code-x-glue*](https://huggingface.co/krlvi/sentence-t5-base-nlpl-code_search_net),
    which is an embedding that was trained on code from open-source GitHub libraries.
    As you can imagine, it is crucial that the embedding we use has been trained on
    code (among other data), so it can make use of the data we feed it with. An embedding,
    that was trained on natural language only, will perform less well, most likely.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have our *VectorStore* and our embedding, we can create the retriever
    from the *Chroma* database directly:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: LLM
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/05fb0674b4aaeda3e15588bc43d0967e.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
- en: The LLM has to do the reasoning over the documents and come up with an answer
    to the user’s question. Photo by [Tingey Injury Law Firm](https://unsplash.com/@tingeyinjurylawfirm?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: The last component we need is an LLM. The easiest solution would be to use a
    hosted LLM, e.g. by using the OpenAI interface. However, we don’t want to send
    our code to such a hosted service. Instead, we will run an LLM on our own hardware.
    To do that we use the [HuggingFacePipeline](https://python.langchain.com/docs/modules/model_io/models/llms/integrations/huggingface_pipelines),
    which allows us to use a model from HuggingFace in the LangChain framework.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: As you see, I used the [*mosaic mpt-7b*](https://huggingface.co/mosaicml/mpt-7b)
    model, which only needs ~16GB memory on a GPU. I created an *AutoModelForCausalLM*,
    which is passed into the *transformers.pipeline*, which is eventually being transformed
    into a *HuggingFacePipeline*. The *HuggingFacePipeline* implements the same interface
    as the typical LLM objects in LangChain. That is, you can use it in the same way
    as you would use the OpenAI LLM interface, for example.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: 'If you have multiple GPUs on your machine, you have to specify which one to
    use. In this case, I want to use the GPU with index 0:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Some additional parameters I have set above can be explained as follows:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: '*trust_remote_code*: This has to be set to true to allow running a model coming
    from outside LangChain.'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*max_new_tokens*: This defines the maximum number of tokens the model may produce
    in its answer. If this value is too low, the model’s response may be cut off before
    it was able to answer the question at all.'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Connect everything together
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/a1a78263500282ac4514caf874b3f3c6.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
- en: We have all the components we need. We just have to plug it all together. Photo
    by [John Barkiple](https://unsplash.com/@barkiple?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: Now we have all the components we need and can combine them in a *ConversationalRetrievalChain*.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Eventually, we can query the chain to answer our questions. The result object
    will include a natural language answer and a list of *source_documents* that were
    consulted to arrive at that answer.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Here is the answer:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Summary
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We’re done! Well, kind of. With the code above we are now able to ask questions
    regarding the source code. However, there are some steps you may want to alter
    according to your needs
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: Use your own source code as *Documents* instead of LangChain’s code.
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Try a different embedding. If the embedding doesn’t fit, the retriever cannot
    find the right documents, and in the end, the questions cannot be answered precisely.
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Try a different model. There are bigger, more powerful models outside, but some
    may be too big to run on your hardware. You have to find the sweet spot where
    you have decent performance but are still able to run the model in a satisfying
    way.
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Try different ways of preprocessing the *Documents* to facilitate the retrieval
    step*.* A common example would be to [split them into chunks of equal length](https://python.langchain.com/docs/modules/data_connection/document_transformers/).
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: I’m sure there is much more to try out to obtain better performance. Just play
    around and adapt the bot to your needs.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: Further reading
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For more examples of code understanding with LangChain, take a look at their
    documentation here:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
- en: '[https://python.langchain.com/docs/use_cases/code/](https://python.langchain.com/docs/use_cases/code/#)'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'On HuggingFace you can find models and embeddings you can easily use in LangChain:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: '[https://huggingface.co/models](https://huggingface.co/models)'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Like this article?* [*Follow me*](/@doriandrost) *to be notified of my future
    posts.*'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
