- en: Code understanding on your own hardware
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/code-understanding-on-your-own-hardware-dd38c4f266d6?source=collection_archive---------8-----------------------#2023-07-05](https://towardsdatascience.com/code-understanding-on-your-own-hardware-dd38c4f266d6?source=collection_archive---------8-----------------------#2023-07-05)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Setting up an LLM to talk about your code — with LangChain and local hardware
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@doriandrost?source=post_page-----dd38c4f266d6--------------------------------)[![Dorian
    Drost](../Images/1795395ad0586eafd83d3e2f7b975ca8.png)](https://medium.com/@doriandrost?source=post_page-----dd38c4f266d6--------------------------------)[](https://towardsdatascience.com/?source=post_page-----dd38c4f266d6--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----dd38c4f266d6--------------------------------)
    [Dorian Drost](https://medium.com/@doriandrost?source=post_page-----dd38c4f266d6--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F1d49ea537d1c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcode-understanding-on-your-own-hardware-dd38c4f266d6&user=Dorian+Drost&userId=1d49ea537d1c&source=post_page-1d49ea537d1c----dd38c4f266d6---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----dd38c4f266d6--------------------------------)
    ·7 min read·Jul 5, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fdd38c4f266d6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcode-understanding-on-your-own-hardware-dd38c4f266d6&user=Dorian+Drost&userId=1d49ea537d1c&source=-----dd38c4f266d6---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fdd38c4f266d6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcode-understanding-on-your-own-hardware-dd38c4f266d6&source=-----dd38c4f266d6---------------------bookmark_footer-----------)![](../Images/14ea7eeae6ca02ac579d218a93a066c4.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: I promise your code won’t leave your local hardware. Photo by [Clément Hélardot](https://unsplash.com/@clemhlrdt?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: Among the various tasks Large Language Models (LLMs) can perform today, code
    understanding may be of particular interest for you, if you work with source code
    as a software developer or a data scientist. Wouldn’t it be great to have a chatbot
    you can ask questions about your code? *Where is the data preprocessing implemented?*
    *Is there a function for verifying the user’s authentication already? What is
    the difference between the calculate_vector_dim and calculate_vector_dimension
    function?* Instead of searching for the correct file yourself, just ask the bot
    and it gives you an answer, together with a pointer to the files that contain
    the relevant code snippets. That mechanism is called semantic search, and you
    can imagine how useful it is.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在今天，大型语言模型（LLMs）能够执行的各种任务中，代码理解可能对你尤其感兴趣，如果你是一名软件开发者或数据科学家。拥有一个你可以询问代码问题的聊天机器人不是很好吗？*数据预处理在哪里实现的？*
    *是否已经有验证用户身份的函数？calculate_vector_dim 和 calculate_vector_dimension 函数之间有什么区别？*
    你不必自己搜索正确的文件，只需问机器人，它会给你答案，并指向包含相关代码片段的文件。这种机制叫做语义搜索，你可以想象它的实用性。
- en: In this tutorial, I will show you how to implement a LangChain bot that does
    exactly that. In addition, I will focus on the specific, data-privacy-related
    issue of not giving your code out of hand. The code you or your company produced
    is private property and may contain sensitive information or valuable knowledge.
    You may not want to, or your company's policies may not allow you to send it to
    an LLM hosted by another company, that may be located in a foreign country. Hence
    in this tutorial, I will show you how to set up a code understanding bot that
    runs on your local hardware, so your code never leaves your infrastructure.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个教程中，我将展示如何实现一个完全符合要求的 LangChain 机器人。此外，我将关注具体的数据隐私问题，即不把你的代码交出去。你或你的公司生产的代码是私有财产，可能包含敏感信息或宝贵的知识。你可能不希望，或者公司政策可能不允许你将其发送到另一个公司托管的
    LLM，那个公司可能位于外国。因此，在本教程中，我将展示如何设置一个运行在本地硬件上的代码理解机器人，以便你的代码不会离开你的基础设施。
- en: Let’s start already! First I will give you a brief introduction to the general
    process of semantic search before we implement a bot for code understanding.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在就开始吧！首先，我会给你简要介绍一下语义搜索的一般过程，然后我们再实现一个用于代码理解的机器人。
- en: Introduction to semantic search
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 语义搜索简介
- en: '![](../Images/d2a2495d247f2a0c813df9e0d7df3d07.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d2a2495d247f2a0c813df9e0d7df3d07.png)'
- en: In semantic search, it’s all about finding the relevant documents. Photo by
    [Markus Spiske](https://unsplash.com/@markusspiske?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在语义搜索中，关键是找到相关的文档。照片由 [Markus Spiske](https://unsplash.com/@markusspiske?utm_source=medium&utm_medium=referral)
    拍摄，来源于 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: 'First of all, let me briefly explain the general idea of semantic search. This
    approach consists of two main steps, that are the retrieval and the answer generation
    by the LLM itself. In the retrieval step, documents are selected that contain
    relevant information, and those are fed into the LLM to create a natural language
    answer. For example, if you ask a question about a function called *transform_vectors,*
    the retrieval will select those files that are relevant to answer that question.
    That may include the file where the *transform_vectors* function is implemented,
    but also files using it or parts of the documentation mentioning it. In the second
    step, those files’ content is given to the LLM in a prompt that may look somewhat
    like that:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我简要说明一下语义搜索的一般思路。这种方法包括两个主要步骤：检索和 LLM 本身生成答案。在检索步骤中，选择包含相关信息的文档，然后将这些文档输入
    LLM 以生成自然语言答案。例如，如果你问一个关于名为*transform_vectors*的函数的问题，检索步骤会选择那些与回答该问题相关的文件。这可能包括实现*transform_vectors*函数的文件，也包括使用它的文件或提及它的文档部分。在第二步，这些文件的内容会被作为提示提供给
    LLM，提示可能如下所示：
- en: '[PRE0]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The LLM creates a natural language answer to the question using information
    from the documents given to it.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: LLM 使用来自提供的文档的信息生成自然语言答案。
- en: That is the main idea of semantic search. Now let’s start implementing! First
    of all, we have to install our requirements and read in our data.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是语义搜索的主要思想。现在我们开始实现吧！首先，我们需要安装我们的要求并读取数据。
- en: Install requirements
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 安装要求
- en: 'Before we can start, make sure you have set up an environment running Python
    and install the following packages:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始之前，请确保你已经设置了运行 Python 的环境，并安装了以下软件包：
- en: '[PRE1]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Read in the documents
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 读取文档
- en: 'Now we need to read in the data and convert it into a format LangChain can
    work with. For this demonstration, I will download the code of LangChain itself,
    but you can use your own code base, of course:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们需要读取数据并将其转换为 LangChain 可以使用的格式。在这个演示中，我将下载 LangChain 本身的代码，但你当然可以使用你自己的代码库：
- en: '[PRE2]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: We load all files and convert them to a *Document* each, i.e. each *Document*
    will contain exactly one file of the code base.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 我们加载所有文件并将其转换为一个 *Document*，即每个 *Document* 将包含代码库中的一个文件。
- en: '[PRE3]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Retrieval
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 检索
- en: '![](../Images/b141a8ee6f486c209b660f19068f37ec.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b141a8ee6f486c209b660f19068f37ec.png)'
- en: Which of these is relevant to answering our question? It’s the retrieval’s job
    to decide that. Photo by [Ed Robertson](https://unsplash.com/@eddrobertson?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 哪些与回答我们的提问相关？这是检索的任务来决定的。照片由 [Ed Robertson](https://unsplash.com/@eddrobertson?utm_source=medium&utm_medium=referral)
    提供，来源于 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: Now that we have created our *Documents*, we need to index them to make them
    searchable. To index a *Document* means to calculate a numerical vector, that
    captures the most relevant information of the *Document*. Unlike plain text itself,
    a vector of numbers can be used to perform numerical calculations, and that means
    that we can easily calculate a similarity on it, which is then used to determine
    which *Documents* are relevant to answer a given question.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经创建了 *Documents*，我们需要对其进行索引以使其可搜索。对 *Document* 进行索引意味着计算一个数值向量，该向量捕捉 *Document*
    中最相关的信息。与纯文本不同，数字向量可以用于进行数值计算，这意味着我们可以轻松计算相似度，然后用来确定哪些 *Documents* 对回答给定问题是相关的。
- en: On a technical level, this index we will create with the help of an embedding
    and store it in a *VectorStore*. There are *VectorStores* available as a service
    (e.g. [DeepLake](https://www.deeplake.ai)), which comes with some handy advantages,
    but in our scenario, we don’t want to give the code out of our hands, so we create
    a *VectorStore* locally on our machine. The easiest way to do that is using *Chroma*,
    which creates a *VectorStore* in memory and allows us to persist it.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 从技术层面来看，我们将利用嵌入创建的索引，并将其存储在 *VectorStore* 中。已有一些作为服务提供的 *VectorStores*（例如 [DeepLake](https://www.deeplake.ai)），这些服务有一些方便的优势，但在我们的场景中，我们不希望将代码交给他人，所以我们在本地机器上创建一个
    *VectorStore*。最简单的方法是使用 *Chroma*，它在内存中创建一个 *VectorStore* 并允许我们持久化它。
- en: '[PRE4]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Within the *from_documents* function, the indices are calculated and stored
    in the *Chroma* database*.* Next time, instead of calling the *from_documents*
    function again, we can load the persisted *Chroma* database itself:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在 *from_documents* 函数中，索引被计算并存储在 *Chroma* 数据库中。下次，我们可以加载持久化的 *Chroma* 数据库，而不是再次调用
    *from_documents* 函数：
- en: '[PRE5]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: As you saw above, as an embedding I used [*krlvi/sentence-t5-base-nlpl-code-x-glue*](https://huggingface.co/krlvi/sentence-t5-base-nlpl-code_search_net),
    which is an embedding that was trained on code from open-source GitHub libraries.
    As you can imagine, it is crucial that the embedding we use has been trained on
    code (among other data), so it can make use of the data we feed it with. An embedding,
    that was trained on natural language only, will perform less well, most likely.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你在上面看到的，作为一个嵌入，我使用了[*krlvi/sentence-t5-base-nlpl-code-x-glue*](https://huggingface.co/krlvi/sentence-t5-base-nlpl-code_search_net)，这是一个在开源
    GitHub 库的代码上训练的嵌入。可以想象，我们使用的嵌入必须在代码（以及其他数据）上进行过训练，以便能够利用我们提供的数据。仅在自然语言上训练的嵌入，表现可能会较差。
- en: 'Now that we have our *VectorStore* and our embedding, we can create the retriever
    from the *Chroma* database directly:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了 *VectorStore* 和嵌入，我们可以直接从 *Chroma* 数据库创建检索器：
- en: '[PRE6]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: LLM
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: LLM
- en: '![](../Images/05fb0674b4aaeda3e15588bc43d0967e.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/05fb0674b4aaeda3e15588bc43d0967e.png)'
- en: The LLM has to do the reasoning over the documents and come up with an answer
    to the user’s question. Photo by [Tingey Injury Law Firm](https://unsplash.com/@tingeyinjurylawfirm?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: LLM 必须对文档进行推理，并给出用户问题的答案。照片由 [Tingey Injury Law Firm](https://unsplash.com/@tingeyinjurylawfirm?utm_source=medium&utm_medium=referral)
    提供，来源于 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: The last component we need is an LLM. The easiest solution would be to use a
    hosted LLM, e.g. by using the OpenAI interface. However, we don’t want to send
    our code to such a hosted service. Instead, we will run an LLM on our own hardware.
    To do that we use the [HuggingFacePipeline](https://python.langchain.com/docs/modules/model_io/models/llms/integrations/huggingface_pipelines),
    which allows us to use a model from HuggingFace in the LangChain framework.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要的最后一个组件是一个LLM。最简单的解决方案是使用托管的LLM，例如通过使用OpenAI接口。然而，我们不想将我们的代码发送到这样的托管服务。相反，我们将在自己的硬件上运行LLM。为此，我们使用[HuggingFacePipeline](https://python.langchain.com/docs/modules/model_io/models/llms/integrations/huggingface_pipelines)，它允许我们在LangChain框架中使用来自HuggingFace的模型。
- en: '[PRE7]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: As you see, I used the [*mosaic mpt-7b*](https://huggingface.co/mosaicml/mpt-7b)
    model, which only needs ~16GB memory on a GPU. I created an *AutoModelForCausalLM*,
    which is passed into the *transformers.pipeline*, which is eventually being transformed
    into a *HuggingFacePipeline*. The *HuggingFacePipeline* implements the same interface
    as the typical LLM objects in LangChain. That is, you can use it in the same way
    as you would use the OpenAI LLM interface, for example.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所见，我使用了[*mosaic mpt-7b*](https://huggingface.co/mosaicml/mpt-7b)模型，它只需要~16GB的GPU内存。我创建了一个*AutoModelForCausalLM*，它被传递到*transformers.pipeline*中，最终被转换成一个*HuggingFacePipeline*。*HuggingFacePipeline*实现了与LangChain中典型LLM对象相同的接口。也就是说，你可以像使用OpenAI
    LLM接口一样使用它。
- en: 'If you have multiple GPUs on your machine, you have to specify which one to
    use. In this case, I want to use the GPU with index 0:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你的机器上有多个GPU，你需要指定使用哪个。在这种情况下，我想使用索引为0的GPU：
- en: '[PRE8]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Some additional parameters I have set above can be explained as follows:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 我设置的一些额外参数可以解释如下：
- en: '*trust_remote_code*: This has to be set to true to allow running a model coming
    from outside LangChain.'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*trust_remote_code*：这必须设置为true，以允许运行来自LangChain之外的模型。'
- en: '*max_new_tokens*: This defines the maximum number of tokens the model may produce
    in its answer. If this value is too low, the model’s response may be cut off before
    it was able to answer the question at all.'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*max_new_tokens*：这定义了模型在回答中可能生成的最大token数量。如果这个值太低，模型的回答可能会在完全回答问题之前被截断。'
- en: Connect everything together
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将所有内容连接起来
- en: '![](../Images/a1a78263500282ac4514caf874b3f3c6.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a1a78263500282ac4514caf874b3f3c6.png)'
- en: We have all the components we need. We just have to plug it all together. Photo
    by [John Barkiple](https://unsplash.com/@barkiple?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 我们拥有所需的所有组件。我们只需将它们连接起来。照片由[John Barkiple](https://unsplash.com/@barkiple?utm_source=medium&utm_medium=referral)拍摄，来源于[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: Now we have all the components we need and can combine them in a *ConversationalRetrievalChain*.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们拥有了所有需要的组件，并可以将它们组合成一个*ConversationalRetrievalChain*。
- en: '[PRE9]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Eventually, we can query the chain to answer our questions. The result object
    will include a natural language answer and a list of *source_documents* that were
    consulted to arrive at that answer.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，我们可以查询链以回答我们的问题。结果对象将包括一个自然语言答案和一个*source_documents*的列表，这些文档被查阅以得出答案。
- en: '[PRE10]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Here is the answer:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 这是答案：
- en: '[PRE11]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Summary
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: We’re done! Well, kind of. With the code above we are now able to ask questions
    regarding the source code. However, there are some steps you may want to alter
    according to your needs
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 完成了！嗯，差不多。通过上述代码，我们现在可以提出关于源代码的问题。然而，根据你的需求，可能需要更改一些步骤。
- en: Use your own source code as *Documents* instead of LangChain’s code.
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用你自己的源代码作为*Documents*，而不是LangChain的代码。
- en: Try a different embedding. If the embedding doesn’t fit, the retriever cannot
    find the right documents, and in the end, the questions cannot be answered precisely.
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 尝试不同的嵌入。如果嵌入不合适，检索器可能无法找到正确的文档，最终问题可能无法准确回答。
- en: Try a different model. There are bigger, more powerful models outside, but some
    may be too big to run on your hardware. You have to find the sweet spot where
    you have decent performance but are still able to run the model in a satisfying
    way.
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 尝试不同的模型。外面有更大、更强大的模型，但有些可能太大而无法在你的硬件上运行。你需要找到性能不错但仍能以令人满意的方式运行模型的最佳平衡点。
- en: Try different ways of preprocessing the *Documents* to facilitate the retrieval
    step*.* A common example would be to [split them into chunks of equal length](https://python.langchain.com/docs/modules/data_connection/document_transformers/).
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 尝试不同的*Documents*预处理方式以促进检索步骤*。*一个常见的例子是[将它们分成相等长度的块](https://python.langchain.com/docs/modules/data_connection/document_transformers/)。
- en: I’m sure there is much more to try out to obtain better performance. Just play
    around and adapt the bot to your needs.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 我相信还有更多可以尝试的，以获得更好的性能。只需动手尝试并根据你的需求调整机器人。
- en: Further reading
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: 'For more examples of code understanding with LangChain, take a look at their
    documentation here:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 有关使用 LangChain 进行代码理解的更多示例，请查看他们的文档：
- en: '[https://python.langchain.com/docs/use_cases/code/](https://python.langchain.com/docs/use_cases/code/#)'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://python.langchain.com/docs/use_cases/code/](https://python.langchain.com/docs/use_cases/code/#)'
- en: 'On HuggingFace you can find models and embeddings you can easily use in LangChain:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在 HuggingFace 上，你可以找到可以在 LangChain 中轻松使用的模型和嵌入：
- en: '[https://huggingface.co/models](https://huggingface.co/models)'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://huggingface.co/models](https://huggingface.co/models)'
- en: '*Like this article?* [*Follow me*](/@doriandrost) *to be notified of my future
    posts.*'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '*喜欢这篇文章吗？* [*关注我*](/@doriandrost) *以便获取我未来的文章更新。*'
