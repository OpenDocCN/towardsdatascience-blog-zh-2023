["```py\n# STEP 01-Read the Word document\n# remember to install  pip install python-docx\n\nimport docx\nimport pandas as pd\n\ndoc = docx.Document('D:\\....your word file on microsoft word')\n\nitems = []\nnames = []\ncomments = []\n\n# Iterate over paragraphs \nfor paragraph in doc.paragraphs:\n    text = paragraph.text.strip()\n\n    if text.endswith(':'):\n        name = text[:-1]  \n    else:\n        items.append(len(items))\n        names.append(name)\n        comments.append(text)\n\ndfsenate = pd.DataFrame({'item': items, 'name': names, 'comment': comments})\n\n# Remove rows with empty comments\ndfsenate = dfsenate[dfsenate['comment'].str.strip().astype(bool)]\n\n# Reset the index\ndfsenate.reset_index(drop=True, inplace=True)\ndfsenate['item'] = dfsenate.index + 1\nprint(dfsenate)\n```", "```py\n item name comment\n0 1 Sen. Richard Blumenthal (D-CT) Now for some introductory remarks.\n1 2 Sen. Richard Blumenthal (D-CT) “Too often we have seen what happens when technology outpaces regulation, the unbridled exploitation of personal data, the proliferation of disinformation, and the deepening of societal inequalities. We have seen how algorithmic biases can perpetuate discrimination and prejudice, and how the lack of transparency can undermine public trust. This is not the future we want.”\n2 3 Sen. Richard Blumenthal (D-CT) If you were listening from home, you might have thought that voice was mine and the words from me, but in fact, that voice was not mine. The words were not mine. And the audio was an AI voice cloning software trained on my floor speeches. The remarks were written by ChatGPT when it was asked how I would open this hearing. And you heard just now the result I asked ChatGPT, why did you pick those themes and that content? And it answered. And I’m quoting, Blumenthal has a strong record in advocating for consumer protection and civil rights. He has been vocal about issues such as data privacy and the potential for discrimination in algorithmic decision making. Therefore, the statement emphasizes these aspects.\n3 4 Sen. Richard Blumenthal (D-CT) Mr. Altman, I appreciate ChatGPT’s endorsement. In all seriousness, this apparent reasoning is pretty impressive. I am sure that we’ll look back in a decade and view ChatGPT and GPT-4 like we do the first cell phone, those big clunky things that we used to carry around. But we recognize that we are on the verge, really, of a new era. The audio and my playing, it may strike you as curious or humorous, but what reverberated in my mind was what if I had asked it? And what if it had provided an endorsement of Ukraine, surrendering or Vladimir Putin’s leadership? That would’ve been really frightening. And the prospect is more than a little scary to use the word, Mr. Altman, you have used yourself, and I think you have been very constructive in calling attention to the pitfalls as well as the promise.\n4 5 Sen. Richard Blumenthal (D-CT) And that’s the reason why we wanted you to be here today. And we thank you and our other witnesses for joining us for several months. Now, the public has been fascinated with GPT, dally and other AI tools. These examples like the homework done by ChatGPT or the articles and op-eds, that it can write feel like novelties. But the underlying advancement of this era are more than just research experiments. They are no longer fantasies of science fiction. They are real and present the promises of curing cancer or developing new understandings of physics and biology or modeling climate and weather. All very encouraging and hopeful. But we also know the potential harms and we’ve seen them already weaponized disinformation, housing discrimination, harassment of women and impersonation, fraud, voice cloning deep fakes. These are the potential risks despite the other rewards. And for me, perhaps the biggest nightmare is the looming new industrial revolution. The displacement of millions of workers, the loss of huge numbers of jobs, the need to prepare for this new industrial revolution in skill training and relocation that may be required. And already industry leaders are calling attention to those challenges.\n5 6 Sen. Richard Blumenthal (D-CT) To quote ChatGPT, this is not necessarily the future that we want. We need to maximize the good over the bad. Congress has a choice. Now. We had the same choice when we face social media. We failed to seize that moment. The result is predators on the internet, toxic content exploiting children, creating dangers for them. And Senator Blackburn and I and others like Senator Durbin on the Judiciary Committee are trying to deal with it in the Kids Online Safety Act. But Congress failed to meet the moment on social media. Now we have the obligation to do it on AI before the threats and the risks become real. Sensible safeguards are not in opposition to innovation. Accountability is not a burden far from it. They are the foundation of how we can move ahead while protecting public trust. They are how we can lead the world in technology and science, but also in promoting our democratic values.\n6 7 Sen. Richard Blumenthal (D-CT) Otherwise, in the absence of that trust, I think we may well lose both. These are sophisticated technologies, but there are basic expectations common in our law. We can start with transparency. AI companies ought to be required to test their systems, disclose known risks, and allow independent researcher access. We can establish scorecards and nutrition labels to encourage competition based on safety and trustworthiness, limitations on use. There are places where the risk of AI is so extreme that we ought to restrict or even ban their use, especially when it comes to commercial invasions of privacy for profit and decisions that affect people’s livelihoods. And of course, accountability, reliability. When AI companies and their clients cause harm, they should be held liable. We should not repeat our past mistakes, for example, Section 230, forcing companies to think ahead and be responsible for the ramifications of their business decisions can be the most powerful tool of all. Garbage in, garbage out. The principle still applies. We ought to beware of the garbage, whether it’s going into these platforms or coming out of them.\n```", "```py\n def assign_sector(name):\n    if name in ['Sam Altman', 'Christina Montgomery']:\n        return 'Private'\n    elif name == 'Gary Marcus':\n        return 'Academia'\n    else:\n        return 'Congress'\n\n# Apply function \ndfsenate['sector'] = dfsenate['name'].apply(assign_sector)\n\n# Assign organizations based on names\ndef assign_organization(name):\n    if name == 'Sam Altman':\n        return 'OpenAI'\n    elif name == 'Christina Montgomery':\n        return 'IBM'\n    elif name == 'Gary Marcus':\n        return 'Academia'\n    else:\n        return 'Congress'\n\n# Apply function\ndfsenate['Organization'] = dfsenate['name'].apply(assign_organization)\n\nprint(dfsenate)\n```", "```py\ndfsenate['WordCount'] = dfsenate['comment'].apply(lambda x: len(x.split()))\n```", "```py\n item                            name  ... Organization WordCount\n0       1  Sen. Richard Blumenthal (D-CT)  ...     Congress         5\n1       2  Sen. Richard Blumenthal (D-CT)  ...     Congress        55\n2       3  Sen. Richard Blumenthal (D-CT)  ...     Congress       125\n3       4  Sen. Richard Blumenthal (D-CT)  ...     Congress       145\n4       5  Sen. Richard Blumenthal (D-CT)  ...     Congress       197\n..    ...                             ...  ...          ...       ...\n399   400         Sen. Cory Booker (D-NJ)  ...     Congress       156\n400   401                      Sam Altman  ...       OpenAI       180\n401   402         Sen. Cory Booker (D-NJ)  ...     Congress        72\n402   403  Sen. Richard Blumenthal (D-CT)  ...     Congress       154\n403   404  Sen. Richard Blumenthal (D-CT)  ...     Congress        98\n```", "```py\n#*****************************PIE CHARTS************************************\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Pie chart - Grouping by 'Organization' Questions&Testimonies\norg_colors = {'Congress': '#6BB6FF', 'OpenAI': 'green', 'IBM': 'lightblue', 'Academia': 'lightyellow'}\norg_counts = dfsenate['Organization'].value_counts()\n\nplt.figure(figsize=(8, 6))\npatches, text, autotext = plt.pie(org_counts.values, labels=org_counts.index, \n                                  autopct=lambda p: f'{p:.1f}%\\n({int(p * sum(org_counts.values) / 100)})', \n                                  startangle=90, colors=[org_colors.get(org, 'gray') for org in org_counts.index])\nplt.title('Hearing on Oversight of AI: Questions or Testimonies')\nplt.axis('equal')\nplt.setp(text, fontsize=12)\nplt.setp(autotext, fontsize=12)\nplt.show()\n\n# Pie chart - Grouping by 'Organization' (WordCount)\norg_colors = {'Congress': '#6BB6FF', 'OpenAI': 'green', 'IBM': 'lightblue', 'Academia': 'lightyellow'}\norg_wordcount = dfsenate.groupby('Organization')['WordCount'].sum()\n\nplt.figure(figsize=(8, 6))\npatches, text, autotext = plt.pie(org_wordcount.values, labels=org_wordcount.index, \n                                  autopct=lambda p: f'{p:.1f}%\\n({int(p * sum(org_wordcount.values) / 100)})', \n                                  startangle=90, colors=[org_colors.get(org, 'gray') for org in org_wordcount.index])\n\nplt.title('Hearing on Oversight of AI: WordCount ')\nplt.axis('equal')\nplt.setp(text, fontsize=12)\nplt.setp(autotext, fontsize=12)\nplt.show()\n\n#************Engagement among the members of Congress**********************\n\n# Group by name and count the rows\nSummary_Name = dfsenate.groupby('name').agg(comment_count=('comment', 'size')).reset_index()\n\n# WordCount column for each name\nSummary_Name ['Total_Words'] = dfsenate.groupby('name')['WordCount'].sum().values\n\n# Percentage distribution for comment_count\nSummary_Name ['comment_count_%'] = Summary_Name['comment_count'] / Summary_Name['comment_count'].sum() * 100\n\n# Percentage distribution for total_word_count\nSummary_Name ['Word_count_%'] = Summary_Name['Total_Words'] / Summary_Name['Total_Words'].sum() * 100\n\nSummary_Name  = Summary_Name.sort_values('Total_Words', ascending=False)\n\nprint (Summary_Name)\n+-------+--------------------------------+---------------+-------------+-----------------+--------------+\n| index |              name              | Interventions | Total_Words | Interv_%        | Word_count_% |\n+-------+--------------------------------+---------------+-------------+-----------------+--------------+\n|     2 | Sam Altman                     |            92 |        6355 |     22.77227723 |  22.32252626 |\n|     1 | Gary Marcus                    |            47 |        5105 |     11.63366337 |  17.93178545 |\n|    15 | Sen. Richard Blumenthal (D-CT) |            58 |        3283 |     14.35643564 |  11.53184165 |\n|    10 | Sen. Josh Hawley (R-MO)        |            25 |        2283 |     6.188118812 |  8.019249008 |\n|     0 | Christina Montgomery           |            36 |        2162 |     8.910891089 |  7.594225298 |\n|     6 | Sen. Cory Booker (D-NJ)        |            20 |        1688 |      4.95049505 |  5.929256384 |\n|     7 | Sen. Dick Durbin (D-IL)        |             8 |        1143 |      1.98019802 |  4.014893393 |\n|    11 | Sen. Lindsey Graham (R-SC)     |            32 |         880 |     7.920792079 |  3.091081527 |\n|     5 | Sen. Christopher Coons (D-CT)  |             6 |         869 |     1.485148515 |  3.052443008 |\n|    12 | Sen. Marsha Blackburn (R-TN)   |            14 |         869 |     3.465346535 |  3.052443008 |\n|     4 | Sen. Amy Klobuchar (D-MN)      |            11 |         769 |     2.722772277 |  2.701183744 |\n|    13 | Sen. Mazie Hirono (D-HI)       |             7 |         755 |     1.732673267 |  2.652007447 |\n|    14 | Sen. Peter Welch (D-VT)        |            11 |         704 |     2.722772277 |  2.472865222 |\n|     3 | Sen. Alex Padilla (D-CA)       |             7 |         656 |     1.732673267 |  2.304260775 |\n+-------+--------------------------------+---------------+-------------+-----------------+--------------+\n```", "```py\n #pip install nltk\n#pip install spacy\n#pip install wordcloud\n#pip install subprocess\n#python -m spacy download en\n```", "```py\n#***************************WORD-FRECUENCY*******************************\n\nimport subprocess\nimport nltk\nimport spacy\nfrom nltk.probability import FreqDist\nfrom nltk.corpus import stopwords\n\n# Download resources\nsubprocess.run('python -m spacy download en', shell=True)\nnltk.download('punkt')\n\n# Load spaCy model and set stopwords\nnlp = spacy.load('en_core_web_sm')\nstop_words = set(stopwords.words('english'))\n\ndef preprocess_text(text):\n    words = nltk.word_tokenize(text)\n    words = [word.lower() for word in words if word.isalpha()]\n    words = [word for word in words if word not in stop_words]\n    lemmas = [token.lemma_ for token in nlp(\" \".join(words))]\n    return lemmas\n\n# Aggregate words and create Frecuency Distribution\nall_comments = ' '.join(dfsenate['comment'])\nprocessed_comments = preprocess_text(all_comments)\nfdist = FreqDist(processed_comments)\n\n#**********************HEARING TOP 30 COMMON WORDS*********************\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Most common words and their frequencies\ntop_words = fdist.most_common(30)\nwords = [word for word, freq in top_words]\nfrequencies = [freq for word, freq in top_words]\n\n# Bar plot-Hearing on Oversight of AI:Top 30 Most Common Words\nfig, ax = plt.subplots(figsize=(8, 10))\nax.barh(range(len(words)), frequencies, align='center', color='skyblue')\n\nax.invert_yaxis()\nax.set_xlabel('Frequency', fontsize=12)\nax.set_ylabel('Words', fontsize=12)\nax.set_title('Hearing on Oversight of AI:Top 30 Most Common Words', fontsize=14)\nax.set_yticks(range(len(words)))\nax.set_yticklabels(words, fontsize=10)\n\nax.spines['right'].set_visible(False)\nax.spines['top'].set_visible(False)\nax.spines['left'].set_linewidth(0.5)\nax.spines['bottom'].set_linewidth(0.5)\nax.tick_params(axis='x', labelsize=10)\nplt.subplots_adjust(left=0.3)\n\nfor i, freq in enumerate(frequencies):\n    ax.text(freq + 5, i, str(freq), va='center', fontsize=8)\n\nplt.show()\n```", "```py\nnlp = spacy.load('en_core_web_sm')\nstop_words = set(stopwords.words('english'))\n\ndef preprocess_text(text):\n    words = nltk.word_tokenize(text)\n    words = [word.lower() for word in words if word.isalpha()]\n    words = [word for word in words if word not in stop_words]\n    lemmas = [token.lemma_ for token in nlp(\" \".join(words))]\n    return lemmas\n\n# Aggregate words and create Frecuency Distribution\nall_comments = ' '.join(dfsenate['comment'])\nprocessed_comments = preprocess_text(all_comments)\nfdist = FreqDist(processed_comments)\noriginal_fdist = fdist.copy() # Save the original object\n\naggregate_words = ['regulation', 'regulate','agency', 'regulatory','legislation']\naggregate_freq = sum(fdist[word] for word in aggregate_words)\ndf_aggregatereg = pd.DataFrame({'Word': aggregate_words, 'Frequency': [fdist[word] for word in aggregate_words]})\n\n# Remove individual words and add aggregation\nfor word in aggregate_words:\n    del fdist[word]\nfdist['regulation+agency'] = aggregate_freq\n\n# Pie chart for Regulation+agency distribution\nimport matplotlib.pyplot as plt\n\nlabels = df_aggregatereg['Word']\nvalues = df_aggregatereg['Frequency']\n\nplt.figure(figsize=(8, 6))\nplt.subplots_adjust(top=0.8, bottom=0.25)  \n\npatches, text, autotext = plt.pie(values, labels=labels, \n                                  autopct=lambda p: f'{p:.1f}%\\n({int(p * sum(values) / 100)})', \n                                  startangle=90, colors=['#6BB6FF', 'green', 'lightblue', 'lightyellow', 'gray'])\n\nplt.title('Regulation+agency: Distribution', fontsize=14)\nplt.axis('equal')\nplt.setp(text, fontsize=8)  \nplt.setp(autotext, fontsize=8)  \nplt.show()\n```", "```py\n# Word cloud-Senate Hearing on Oversight of AI\nfrom wordcloud import WordCloud\nwordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(fdist)\nplt.figure(figsize=(10, 5))\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis('off')\nplt.title('Word Cloud - Senate Hearing on Oversight of AI')\nplt.show()\n```", "```py\n# Word clouds for each group of Interest\norganizations = dfsenate['Organization'].unique()\nfor organization in organizations:\n    comments = dfsenate[dfsenate['Organization'] == organization]['comment']\n    all_comments = ' '.join(comments)\n    processed_comments = preprocess_text(all_comments)\n    fdist_organization = FreqDist(processed_comments)\n\n    # Word clouds\n    wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(fdist_organization)\n    plt.figure(figsize=(10, 5))\n    plt.imshow(wordcloud, interpolation='bilinear')\n    plt.axis('off')\n    if organization == 'IBM':\n        plt.title(f'Word Cloud: {organization} - Christina Montgomery')\n    elif organization == 'OpenAI':\n        plt.title(f'Word Cloud: {organization} - Sam Altman')\n    elif organization == 'Academia':\n        plt.title(f'Word Cloud: {organization} - Gary Marcus')\n    else:\n        plt.title(f'Word Cloud: {organization}')\n    plt.show()\n```", "```py\n#************SENTIMENT ANALYSIS************\nfrom nltk.sentiment import SentimentIntensityAnalyzer\nnltk.download('vader_lexicon')\n\nsid = SentimentIntensityAnalyzer()\ndfsenate['Sentiment'] = dfsenate['comment'].apply(lambda x: sid.polarity_scores(x)['compound'])\n\n#************BOXPLOT-GROUP OF INTEREST************\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nsns.set_style('white')\nplt.figure(figsize=(12, 7))\nsns.boxplot(x='Sentiment', y='Organization', data=dfsenate, color='yellow', \n            width=0.6, showmeans=True, showfliers=True)\n\n# Customize the axis \ndef add_cosmetics(title='Sentiment Analysis Distribution by Group of Interest',\n                  xlabel='Sentiment'):\n    plt.title(title, fontsize=28)\n    plt.xlabel(xlabel, fontsize=20)\n    plt.xticks(fontsize=15)\n    plt.yticks(fontsize=15)\n    sns.despine()\n\ndef customize_labels(label):\n    if \"OpenAI\" in label:\n        return label + \"-Sam Altman\"\n    elif \"IBM\" in label:\n        return label + \"-Christina Montgomery\"\n    elif \"Academia\" in label:\n        return label + \"-Gary Marcus\"\n    else:\n        return label\n\n# Apply customized labels to y-axis\nyticks = plt.yticks()[1]\nplt.yticks(ticks=plt.yticks()[0], labels=[customize_labels(label.get_text()) \n                                          for label in yticks])\n\nadd_cosmetics()\nplt.show()\n```", "```py\n#**************************TIMELINE US SENATE AI HEARING**************************************\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom scipy.interpolate import make_interp_spline\n\n# Moving average for each organization\nwindow_size = 10  \norganizations = dfsenate['Organization'].unique()\n\n# Create the line plot\ncolor_palette = sns.color_palette('Set2', len(organizations))\n\nplt.figure(figsize=(12, 6))\nfor i, org in enumerate(organizations):\n    df_org = dfsenate[dfsenate['Organization'] == org]\n\n    # moving average\n    df_org['Sentiment'].fillna(0, inplace=True) # missing values filled with 0\n    df_org['Moving_Average'] = df_org['Sentiment'].rolling(window=window_size, min_periods=1).mean()\n\n    x = np.linspace(df_org.index.min(), df_org.index.max(), 500)\n    spl = make_interp_spline(df_org.index, df_org['Moving_Average'], k=3)\n    y = spl(x)\n    plt.plot(x, y, linewidth=2, label=f'{org} {window_size}-Point Moving Average', color=color_palette[i])\n\nplt.xlabel('Statement Number', fontsize=12)\nplt.ylabel('Sentiment Score', fontsize=12)\nplt.title('Sentiment Score Evolution during the Hearing on Oversight of AI', fontsize=16)\nplt.legend(fontsize=12)\nplt.grid(color='lightgray', linestyle='--', linewidth=0.5)\nplt.axhline(0, color='black', linewidth=0.5, alpha=0.5)\n\nfor org in organizations:\n    df_org = dfsenate[dfsenate['Organization'] == org]\n    plt.text(df_org.index[-1], df_org['Moving_Average'].iloc[-1], f'{df_org[\"Moving_Average\"].iloc[-1]:.2f}', ha='right', va='top', fontsize=12, color='black')\n\nplt.tight_layout()\nplt.show()\n```"]