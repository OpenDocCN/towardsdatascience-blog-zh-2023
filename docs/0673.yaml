- en: When Stochastic Policies Are Better Than Deterministic Ones
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/when-stochastic-policies-are-better-than-deterministic-ones-b950cd0d60f4?source=collection_archive---------5-----------------------#2023-02-18](https://towardsdatascience.com/when-stochastic-policies-are-better-than-deterministic-ones-b950cd0d60f4?source=collection_archive---------5-----------------------#2023-02-18)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Why we let randomness dictate our action selection in Reinforcement Learning
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://wvheeswijk.medium.com/?source=post_page-----b950cd0d60f4--------------------------------)[![Wouter
    van Heeswijk, PhD](../Images/9c996bccd6fdfb6d9aa8b50b93338eb9.png)](https://wvheeswijk.medium.com/?source=post_page-----b950cd0d60f4--------------------------------)[](https://towardsdatascience.com/?source=post_page-----b950cd0d60f4--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----b950cd0d60f4--------------------------------)
    [Wouter van Heeswijk, PhD](https://wvheeswijk.medium.com/?source=post_page-----b950cd0d60f4--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F33f45c9ab481&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhen-stochastic-policies-are-better-than-deterministic-ones-b950cd0d60f4&user=Wouter+van+Heeswijk%2C+PhD&userId=33f45c9ab481&source=post_page-33f45c9ab481----b950cd0d60f4---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----b950cd0d60f4--------------------------------)
    ·6 min read·Feb 18, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fb950cd0d60f4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhen-stochastic-policies-are-better-than-deterministic-ones-b950cd0d60f4&user=Wouter+van+Heeswijk%2C+PhD&userId=33f45c9ab481&source=-----b950cd0d60f4---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb950cd0d60f4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhen-stochastic-policies-are-better-than-deterministic-ones-b950cd0d60f4&source=-----b950cd0d60f4---------------------bookmark_footer-----------)![](../Images/8175bb620e2361f15241cbfd404190f9.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: Rock-paper-scissors would be a dull affair with deterministic policies [Photo
    by [Marcus Wallis](https://unsplash.com/@marcus_wallis?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)]
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: If you are used to deterministic decision-making policies (e.g., as in [Deep
    Q-learning](https://medium.com/towards-data-science/deep-q-learning-for-the-cliff-walking-problem-b54835409046)),
    the need for and use of stochastic policies might elude you. After all, **deterministic
    policies** offer a convenient state-action mapping *π:s ↦ a*, ideally even the
    optimal mapping (that is, if all the Bellman equations are learned to perfection).
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你习惯了确定性决策策略（例如，[深度 Q 学习](https://medium.com/towards-data-science/deep-q-learning-for-the-cliff-walking-problem-b54835409046)），你可能会对随机策略的需求和使用感到困惑。毕竟，**确定性策略**提供了一个便捷的状态-动作映射*π:s
    ↦ a*，理想情况下甚至是最优映射（即，如果所有的贝尔曼方程都被完美学习的话）。
- en: In contrast, **stochastic policies —** represented by a conditional probability
    distribution over the actions in a given state, *π:P(a|s)* — seem rather inconvenient
    and imprecise. Why would we allow randomness to direct our actions, why leave
    the selection of the best known decisions to chance?
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，**随机策略**——通过在给定状态下对动作的条件概率分布来表示，*π:P(a|s)*——似乎相当不便且不精确。我们为何要让随机性来指引我们的行动，为什么将选择最佳决策的权利交给运气？
- en: In reality, a huge number of Reinforcement Learning (RL) algorithms indeed deploys
    stochastic policies, judging by the sheer number of actor-critic algorithms out
    there. Evidently, there must be some benefit to this approach. This article discusses
    four cases in which stochastic policies are superior to their deterministic counterparts.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，考虑到大量的演员-评论家算法，许多强化学习（RL）算法确实使用了随机策略。显然，这种方法必定有其好处。本文讨论了四种随机策略优于其确定性对手的情况。
