- en: Linear Discriminant Analysis (LDA) Can Be So Easy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ÂéüÊñáÔºö[https://towardsdatascience.com/linear-discriminant-analysis-lda-can-be-so-easy-b3f46e32f982?source=collection_archive---------8-----------------------#2023-02-20](https://towardsdatascience.com/linear-discriminant-analysis-lda-can-be-so-easy-b3f46e32f982?source=collection_archive---------8-----------------------#2023-02-20)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: An Interactive Visualisation for You to Experiment With
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@frederikho?source=post_page-----b3f46e32f982--------------------------------)[![Frederik
    Holtel](../Images/adc1632104df83db4449ca56d0a2c2ad.png)](https://medium.com/@frederikho?source=post_page-----b3f46e32f982--------------------------------)[](https://towardsdatascience.com/?source=post_page-----b3f46e32f982--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----b3f46e32f982--------------------------------)
    [Frederik Holtel](https://medium.com/@frederikho?source=post_page-----b3f46e32f982--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ¬∑
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fde281205c742&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flinear-discriminant-analysis-lda-can-be-so-easy-b3f46e32f982&user=Frederik+Holtel&userId=de281205c742&source=post_page-de281205c742----b3f46e32f982---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----b3f46e32f982--------------------------------)
    ¬∑7 min read¬∑Feb 20, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fb3f46e32f982&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flinear-discriminant-analysis-lda-can-be-so-easy-b3f46e32f982&user=Frederik+Holtel&userId=de281205c742&source=-----b3f46e32f982---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb3f46e32f982&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flinear-discriminant-analysis-lda-can-be-so-easy-b3f46e32f982&source=-----b3f46e32f982---------------------bookmark_footer-----------)![](../Images/5f710a7c3c6c2686a4fca1620a2c6619.png)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Image created by Arus Nazaryan using Midjourney. Prompt: ‚Äú`Drone footage of
    two flocks of sheep, bright blue and deep red, divided by a fence which separates
    the flocks in the middle, clean, realistic sheep, on green grass, photorealistic`‚Äù'
  prefs: []
  type: TYPE_NORMAL
- en: Classification is a central topic in machine learning. However, it can be challenging
    to understand how the different algorithms work. In this article, we will make
    linear discriminant analysis come alive with an interactive plot that you can
    experiment with. Get ready to dive into the world of data classification!
  prefs: []
  type: TYPE_NORMAL
- en: '**Interactive plot** üëáüèΩ Click to add and remove data points, use drag to move
    them. Change the population parameters and generate new data samples.'
  prefs: []
  type: TYPE_NORMAL
- en: If you are applying or studying classification methods, you might have come
    across various methods such as Naive Bayes, K-nearest neighbours (KNN), quadratic
    discriminant analysis (QDA) and linear discriminant analysis (LDA). However, it
    is not always intuitive to understand what the different algorithms are doing.
    LDA is one of the first approaches to learn and consider and this articles demonstrates
    how the technique works.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let‚Äôs start from the beginning. All classification methods are approaches to
    answer the question: Which type of class does this observation belong to?'
  prefs: []
  type: TYPE_NORMAL
- en: In the plot above, there are two independent variables, `x_1` on the horizontal
    axis and `x_2` on the vertical axis. Think of the independent variables as scores
    in two subjects, e.g. physics and literature (ranging from 0‚Äì20). The dependent
    value `y` is the class, in the plot represented as red or blue. Think of it as
    a binary variable we want to predict, such as whether an applicant is admitted
    to university (‚Äúyes‚Äù or ‚Äúno‚Äù). A set of given observations is displayed as circles.
    Each is characterised by a `x_1` and a `x_2` value and the class.
  prefs: []
  type: TYPE_NORMAL
- en: Now, if a new data point is added, to which class shall it be assigned?
  prefs: []
  type: TYPE_NORMAL
- en: LDA allows us to draw a boundary to divide the space in two parts (or multiple
    ones, but two in this case with two classes). For example, below in figure 1 I
    marked a new data point at `(x_1 = 4, x_2 = 14)` with a cross. As it falls on
    the ‚Äúred side‚Äù of the space, this observation would be assigned to the red class.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/03f247a59ed97037b70563a516d9fdc9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: The new observation at the cross (x_1=4, x_2=14) is assigned to the
    red class.'
  prefs: []
  type: TYPE_NORMAL
- en: How it works ‚Äî the math behind it
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So given the LDA boundary, we can make classifications. But how can we draw
    the boundary line using the LDA algorithm?
  prefs: []
  type: TYPE_NORMAL
- en: The line divides the plot where the probability of the red class and the blue
    class is 50% each. Going to one side, the probability of red is higher, going
    to the opposite side, blue is more probable. We need to come up with a way to
    calculate how probable it is for the new observation to belong to each of the
    classes.
  prefs: []
  type: TYPE_NORMAL
- en: 'The probability that the new observation belongs to the red class can be written
    this way: `P(Y = red| X = (x_1 = 4, x_2 = 14))`. To make it easier to read, instead
    of `x_1 = 4, x_2 = 14`, I will in the following just write `x` which is a vector
    containing the two values.'
  prefs: []
  type: TYPE_NORMAL
- en: According to the Bayes theorem, we can express conditional probabilities as
    `P(Y = red | X = x) = P(Y = red ) * P(X = x | Y = red) / P(X = x)`. So to calculate
    the probability that the new observation is ‚Äúred‚Äù given its x values, we need
    to know three other probabilities.
  prefs: []
  type: TYPE_NORMAL
- en: Let‚Äôs go step by step. `P(Y = red)` and `P(X = x)` are called ‚Äúprior probabilities‚Äù.
    In the plot, `P(Y = red)` can be calculated simply by taking the share of all
    ‚Äúred‚Äù observations of the total number of observations (27,27% in figure 1).
  prefs: []
  type: TYPE_NORMAL
- en: Calculating the prior probability of x, `P(X = x)`, is more difficult. We don‚Äôt
    have any observation with `x_1 = 4` and `x_2 = 14`in the initial dataset, so what
    is its probability? Zero? Not quite.
  prefs: []
  type: TYPE_NORMAL
- en: '`P(X = x)` can be understood as the sum of the joint probabilities `P(Y = red,
    X = x)` and `P(Y = blue, X = x)` (see also [here](https://stats.stackexchange.com/questions/294080/proof-of-a-modified-bayes-theorem/601390#601390)):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/799fcbe610da7c6556a979336e1d23ab.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Each of the joint probabilities can be expressed with the Bayes theorem as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/24085fa6839e75ef86e639e814ace775.png)![](../Images/c151ed9d4013af48cf428953eb9cf508.png)'
  prefs: []
  type: TYPE_IMG
- en: You see the priors `P(red)` and `P(blue)` enter again, which we already know
    how to determine. What is left for us to do, is to find a way to calculate the
    conditional probabilities `P(X = x | Y = red)` and `P(X = x | Y = blue)`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Even if we did never see any red data point at position `x` in the plot in
    our original data set, we can find a probability if we regard `x` to be drawn
    from a population of a certain form. Usually when using real world data, the population
    is not directly observable, so we don‚Äôt know the true form of the population distribution.
    What LDA does is to assume that the `x` values are normally distributed on the
    `x_1` and `x_2` axis. With real world data, this assumption can be more or less
    reasonable. As we are dealing with a generated data set here though, we don‚Äôt
    have to worry about this problem. The data originate from populations which follow
    a normal (i.e. Gaussian) distribution, characterised by two parameters (per independent
    variable): `~N(mean, variance)`.'
  prefs: []
  type: TYPE_NORMAL
- en: Using the formula for multivariate normal distributions, we can describe the
    distribution of the data belonging to class ‚Äúred‚Äù as
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4b4031dc61db91365394f8ba1779263f.png)'
  prefs: []
  type: TYPE_IMG
- en: where Œ£ denotes a covariance matrix. LDA uses only one common covariance matrix
    for all classes, meaning that it assumes that all classes have the same `variance(x_1)`,
    `variance(x_2)` and `covariance(x_1, x_2)`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we‚Äôve got the formula for `P (X = x | Y = red)`, we can plug this
    in the equation above to get to `P (Y = red|X = x)`. This gives us a monstrous
    equation, which I‚Äôll skip here. Luckily, it gets simpler from here. Our goal now
    is to find the dividing line that separates the red and blue zones, i.e. we want
    to find those points where the probabilities of being class red or blue are equal:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/67b338ecddf9e092b8ae3e60bb63bd78.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Performing some transformations, it can be shown that minimising this is equivalent
    to minimising the following equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/108550b5f8c0466072a963be46d35546.png)'
  prefs: []
  type: TYPE_IMG
- en: Solving this for `x_2`, we get a line like
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/100c7df1aaca3863d287b1c2f5f0ed2a.png)'
  prefs: []
  type: TYPE_IMG
- en: where `Œ≤_0` and `Œ≤_1` are the parameters depending on the means ¬µ_red and ¬µ_blue,
    the common covariance matrix Œ£ and the prior probabilities of red and blue, P(‚Äúred‚Äù)
    and P(‚Äúblue‚Äù).
  prefs: []
  type: TYPE_NORMAL
- en: Exploring the plot
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that you know how it works, you can explore the plot using different parameters.
  prefs: []
  type: TYPE_NORMAL
- en: When you click ‚Äúcreate new sample‚Äù, the plot displays two boundaries. The Bayes
    boundary is calculated using the ‚Äúreal‚Äù population parameters. The estimated boundary
    in contrast makes estimates for the parameters based on the sample data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Some guiding questions for exploration:'
  prefs: []
  type: TYPE_NORMAL
- en: Using which parameters does the estimated boundary deviate strongly from the
    Bayes boundary? (see figure 2, left image)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How sensitive is the boundary to outliers? (see figure 2, center image)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Which parameters give results you did not expect?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/649cf2e543b218c92974395aae814f8b.png)![](../Images/a0465c6200f9b51fbca9d93cb6399d9f.png)![](../Images/bf0cdda45ba59b21e6f72ba1f0ecfcaf.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2\. Left: The estimated boundary deviates strongly from the Bayes boundary.
    Center: The boundary adapts to an outlier. Right: The boundary adapts well to
    the data point moving around.'
  prefs: []
  type: TYPE_NORMAL
- en: Final remarks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: LDA is an important tool to know in the realm of machine learning classification
    methods. However, it is one thing to understand the theory behind LDA through
    equations and formulas, and another to gain a practical understanding through
    hands-on exploration. The interactive tool presented here offers a unique opportunity
    to experiment and understand LDA in a more intuitive way.
  prefs: []
  type: TYPE_NORMAL
- en: Which other statistical methods would you like to see interactively visualised?
    Let me know in the comments!
  prefs: []
  type: TYPE_NORMAL
- en: Don‚Äôt forget to **follow me** to stay in the loop for further articles!
  prefs: []
  type: TYPE_NORMAL
- en: All images, unless otherwise noted, were created by the author using Observable.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you want to read up on LDA, have a look into the great book ‚ÄúAn Introduction
    to Statistical Learning‚Äù by James, Witten, Hastie and Tibshirani, which you can
    download for free on their website: [https://www.statlearning.com/](https://www.statlearning.com/)'
  prefs: []
  type: TYPE_NORMAL
