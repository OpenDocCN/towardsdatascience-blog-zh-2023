- en: Linear Discriminant Analysis (LDA) Can Be So Easy
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: çº¿æ€§åˆ¤åˆ«åˆ†æï¼ˆLDAï¼‰å¯ä»¥å¦‚æ­¤ç®€å•
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/linear-discriminant-analysis-lda-can-be-so-easy-b3f46e32f982?source=collection_archive---------8-----------------------#2023-02-20](https://towardsdatascience.com/linear-discriminant-analysis-lda-can-be-so-easy-b3f46e32f982?source=collection_archive---------8-----------------------#2023-02-20)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://towardsdatascience.com/linear-discriminant-analysis-lda-can-be-so-easy-b3f46e32f982?source=collection_archive---------8-----------------------#2023-02-20](https://towardsdatascience.com/linear-discriminant-analysis-lda-can-be-so-easy-b3f46e32f982?source=collection_archive---------8-----------------------#2023-02-20)
- en: An Interactive Visualisation for You to Experiment With
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä¸ºä½ å‡†å¤‡çš„äº’åŠ¨å¯è§†åŒ–å·¥å…·
- en: '[](https://medium.com/@frederikho?source=post_page-----b3f46e32f982--------------------------------)[![Frederik
    Holtel](../Images/adc1632104df83db4449ca56d0a2c2ad.png)](https://medium.com/@frederikho?source=post_page-----b3f46e32f982--------------------------------)[](https://towardsdatascience.com/?source=post_page-----b3f46e32f982--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----b3f46e32f982--------------------------------)
    [Frederik Holtel](https://medium.com/@frederikho?source=post_page-----b3f46e32f982--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@frederikho?source=post_page-----b3f46e32f982--------------------------------)[![Frederik
    Holtel](../Images/adc1632104df83db4449ca56d0a2c2ad.png)](https://medium.com/@frederikho?source=post_page-----b3f46e32f982--------------------------------)[](https://towardsdatascience.com/?source=post_page-----b3f46e32f982--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----b3f46e32f982--------------------------------)
    [Frederik Holtel](https://medium.com/@frederikho?source=post_page-----b3f46e32f982--------------------------------)'
- en: Â·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Â·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fde281205c742&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flinear-discriminant-analysis-lda-can-be-so-easy-b3f46e32f982&user=Frederik+Holtel&userId=de281205c742&source=post_page-de281205c742----b3f46e32f982---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----b3f46e32f982--------------------------------)
    Â·7 min readÂ·Feb 20, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fb3f46e32f982&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flinear-discriminant-analysis-lda-can-be-so-easy-b3f46e32f982&user=Frederik+Holtel&userId=de281205c742&source=-----b3f46e32f982---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[å…³æ³¨](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fde281205c742&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flinear-discriminant-analysis-lda-can-be-so-easy-b3f46e32f982&user=Frederik+Holtel&userId=de281205c742&source=post_page-de281205c742----b3f46e32f982---------------------post_header-----------)
    å‘è¡¨åœ¨ [Towards Data Science](https://towardsdatascience.com/?source=post_page-----b3f46e32f982--------------------------------)
    Â·7 åˆ†é’Ÿé˜…è¯»Â·2023å¹´2æœˆ20æ—¥[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fb3f46e32f982&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flinear-discriminant-analysis-lda-can-be-so-easy-b3f46e32f982&user=Frederik+Holtel&userId=de281205c742&source=-----b3f46e32f982---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb3f46e32f982&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flinear-discriminant-analysis-lda-can-be-so-easy-b3f46e32f982&source=-----b3f46e32f982---------------------bookmark_footer-----------)![](../Images/5f710a7c3c6c2686a4fca1620a2c6619.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb3f46e32f982&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flinear-discriminant-analysis-lda-can-be-so-easy-b3f46e32f982&source=-----b3f46e32f982---------------------bookmark_footer-----------)![](../Images/5f710a7c3c6c2686a4fca1620a2c6619.png)'
- en: 'Image created by Arus Nazaryan using Midjourney. Prompt: â€œ`Drone footage of
    two flocks of sheep, bright blue and deep red, divided by a fence which separates
    the flocks in the middle, clean, realistic sheep, on green grass, photorealistic`â€'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ç‰‡ç”± Arus Nazaryan ä½¿ç”¨ Midjourney åˆ›å»ºã€‚æç¤ºè¯ï¼šâ€œ`æ— äººæœºæ‹æ‘„çš„ä¸¤ç¾¤ç¾Šï¼Œæ˜äº®çš„è“è‰²å’Œæ·±çº¢è‰²ï¼Œè¢«ä¸€æ¡ä¸­é—´åˆ†éš”ç¾¤ä½“çš„å›´æ åˆ†å¼€ï¼Œå¹²å‡€ã€é€¼çœŸçš„ç¾Šï¼Œç»¿è‰åœ°ä¸Šï¼Œç…§ç‰‡çº§çœŸå®æ„Ÿ`â€
- en: Classification is a central topic in machine learning. However, it can be challenging
    to understand how the different algorithms work. In this article, we will make
    linear discriminant analysis come alive with an interactive plot that you can
    experiment with. Get ready to dive into the world of data classification!
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ†ç±»æ˜¯æœºå™¨å­¦ä¹ ä¸­çš„æ ¸å¿ƒè¯é¢˜ã€‚ç„¶è€Œï¼Œç†è§£ä¸åŒç®—æ³•çš„å·¥ä½œåŸç†å¯èƒ½ä¼šå¾ˆå…·æŒ‘æˆ˜æ€§ã€‚åœ¨è¿™ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬å°†é€šè¿‡ä¸€ä¸ªäº’åŠ¨å›¾è¡¨ä½¿çº¿æ€§åˆ¤åˆ«åˆ†æå˜å¾—ç”ŸåŠ¨ï¼Œä½ å¯ä»¥è¿›è¡Œå®éªŒã€‚å‡†å¤‡å¥½æ·±å…¥æ•°æ®åˆ†ç±»çš„ä¸–ç•Œå§ï¼
- en: '**Interactive plot** ğŸ‘‡ğŸ½ Click to add and remove data points, use drag to move
    them. Change the population parameters and generate new data samples.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '**äº¤äº’å¼å›¾** ğŸ‘‡ğŸ½ ç‚¹å‡»æ·»åŠ å’Œç§»é™¤æ•°æ®ç‚¹ï¼Œä½¿ç”¨æ‹–åŠ¨æ¥ç§»åŠ¨å®ƒä»¬ã€‚æ”¹å˜äººå£å‚æ•°å¹¶ç”Ÿæˆæ–°çš„æ•°æ®æ ·æœ¬ã€‚'
- en: If you are applying or studying classification methods, you might have come
    across various methods such as Naive Bayes, K-nearest neighbours (KNN), quadratic
    discriminant analysis (QDA) and linear discriminant analysis (LDA). However, it
    is not always intuitive to understand what the different algorithms are doing.
    LDA is one of the first approaches to learn and consider and this articles demonstrates
    how the technique works.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä½ åœ¨åº”ç”¨æˆ–ç ”ç©¶åˆ†ç±»æ–¹æ³•ï¼Œå¯èƒ½ä¼šé‡åˆ°å„ç§æ–¹æ³•ï¼Œå¦‚æœ´ç´ è´å¶æ–¯ã€Kæœ€è¿‘é‚»ï¼ˆKNNï¼‰ã€äºŒæ¬¡åˆ¤åˆ«åˆ†æï¼ˆQDAï¼‰å’Œçº¿æ€§åˆ¤åˆ«åˆ†æï¼ˆLDAï¼‰ã€‚ç„¶è€Œï¼Œç†è§£ä¸åŒç®—æ³•çš„å·¥ä½œåŸç†å¹¶ä¸æ€»æ˜¯ç›´è§‚çš„ã€‚LDAæ˜¯æœ€åˆéœ€è¦å­¦ä¹ å’Œè€ƒè™‘çš„æ–¹æ³•ä¹‹ä¸€ï¼Œæœ¬æ–‡å±•ç¤ºäº†è¯¥æŠ€æœ¯çš„å·¥ä½œåŸç†ã€‚
- en: 'Letâ€™s start from the beginning. All classification methods are approaches to
    answer the question: Which type of class does this observation belong to?'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬ä»å¤´å¼€å§‹ã€‚æ‰€æœ‰åˆ†ç±»æ–¹æ³•éƒ½æ˜¯ä¸ºäº†å›ç­”è¿™ä¸ªé—®é¢˜ï¼šè¿™ä¸ªè§‚æµ‹å€¼å±äºå“ªç§ç±»å‹çš„ç±»åˆ«ï¼Ÿ
- en: In the plot above, there are two independent variables, `x_1` on the horizontal
    axis and `x_2` on the vertical axis. Think of the independent variables as scores
    in two subjects, e.g. physics and literature (ranging from 0â€“20). The dependent
    value `y` is the class, in the plot represented as red or blue. Think of it as
    a binary variable we want to predict, such as whether an applicant is admitted
    to university (â€œyesâ€ or â€œnoâ€). A set of given observations is displayed as circles.
    Each is characterised by a `x_1` and a `x_2` value and the class.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä¸Šé¢çš„å›¾ä¸­ï¼Œæœ‰ä¸¤ä¸ªç‹¬ç«‹å˜é‡ï¼Œ`x_1` åœ¨æ°´å¹³è½´ä¸Šï¼Œ`x_2` åœ¨å‚ç›´è½´ä¸Šã€‚æŠŠç‹¬ç«‹å˜é‡çœ‹ä½œæ˜¯ä¸¤ä¸ªå­¦ç§‘çš„åˆ†æ•°ï¼Œä¾‹å¦‚ç‰©ç†å’Œæ–‡å­¦ï¼ˆèŒƒå›´ä»0åˆ°20ï¼‰ã€‚å› å˜é‡
    `y` æ˜¯ç±»åˆ«ï¼Œåœ¨å›¾ä¸­è¡¨ç¤ºä¸ºçº¢è‰²æˆ–è“è‰²ã€‚å°†å…¶è§†ä¸ºæˆ‘ä»¬æƒ³è¦é¢„æµ‹çš„äºŒå…ƒå˜é‡ï¼Œä¾‹å¦‚ç”³è¯·äººæ˜¯å¦è¢«å¤§å­¦å½•å–ï¼ˆâ€œæ˜¯â€æˆ–â€œå¦â€ï¼‰ã€‚ä¸€ç»„ç»™å®šçš„è§‚æµ‹å€¼ä»¥åœ†åœˆçš„å½¢å¼æ˜¾ç¤ºã€‚æ¯ä¸ªåœ†åœˆç”±
    `x_1` å’Œ `x_2` å€¼ä»¥åŠç±»åˆ«ç‰¹å¾ã€‚
- en: Now, if a new data point is added, to which class shall it be assigned?
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼Œå¦‚æœæ·»åŠ ä¸€ä¸ªæ–°çš„æ•°æ®ç‚¹ï¼Œå®ƒåº”è¯¥è¢«åˆ†é…åˆ°å“ªä¸ªç±»åˆ«å‘¢ï¼Ÿ
- en: LDA allows us to draw a boundary to divide the space in two parts (or multiple
    ones, but two in this case with two classes). For example, below in figure 1 I
    marked a new data point at `(x_1 = 4, x_2 = 14)` with a cross. As it falls on
    the â€œred sideâ€ of the space, this observation would be assigned to the red class.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: LDAå…è®¸æˆ‘ä»¬ç»˜åˆ¶ä¸€ä¸ªè¾¹ç•Œï¼Œå°†ç©ºé—´åˆ†æˆä¸¤éƒ¨åˆ†ï¼ˆæˆ–è€…å¤šä¸ªéƒ¨åˆ†ï¼Œä½†åœ¨è¿™ç§æƒ…å†µä¸‹æ˜¯ä¸¤ä¸ªç±»åˆ«ï¼‰ã€‚ä¾‹å¦‚ï¼Œåœ¨ä¸‹å›¾1ä¸­ï¼Œæˆ‘åœ¨ `(x_1 = 4, x_2 = 14)`
    å¤„æ ‡è®°äº†ä¸€ä¸ªæ–°çš„æ•°æ®ç‚¹ã€‚ç”±äºå®ƒè½åœ¨ç©ºé—´çš„â€œçº¢è‰²ä¾§â€ï¼Œè¿™ä¸ªè§‚æµ‹å€¼å°†è¢«åˆ†é…åˆ°çº¢è‰²ç±»åˆ«ã€‚
- en: '![](../Images/03f247a59ed97037b70563a516d9fdc9.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/03f247a59ed97037b70563a516d9fdc9.png)'
- en: 'Figure 1: The new observation at the cross (x_1=4, x_2=14) is assigned to the
    red class.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾1ï¼šäº¤å‰ç‚¹ (x_1=4, x_2=14) è¢«åˆ†é…åˆ°çº¢è‰²ç±»åˆ«ã€‚
- en: How it works â€” the math behind it
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å®ƒæ˜¯å¦‚ä½•å·¥ä½œçš„â€”â€”èƒŒåçš„æ•°å­¦
- en: So given the LDA boundary, we can make classifications. But how can we draw
    the boundary line using the LDA algorithm?
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥ï¼Œç»™å®šLDAè¾¹ç•Œï¼Œæˆ‘ä»¬å¯ä»¥è¿›è¡Œåˆ†ç±»ã€‚ä½†æ˜¯æˆ‘ä»¬å¦‚ä½•ä½¿ç”¨LDAç®—æ³•ç»˜åˆ¶è¾¹ç•Œçº¿å‘¢ï¼Ÿ
- en: The line divides the plot where the probability of the red class and the blue
    class is 50% each. Going to one side, the probability of red is higher, going
    to the opposite side, blue is more probable. We need to come up with a way to
    calculate how probable it is for the new observation to belong to each of the
    classes.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ¡çº¿å°†å›¾å½¢åˆ†æˆçº¢è‰²ç±»åˆ«å’Œè“è‰²ç±»åˆ«çš„æ¦‚ç‡å„ä¸º50%çš„åŒºåŸŸã€‚å¾€ä¸€ä¾§ç§»åŠ¨ï¼Œçº¢è‰²çš„æ¦‚ç‡æ›´é«˜ï¼›å¾€å¦ä¸€ä¾§ç§»åŠ¨ï¼Œè“è‰²çš„æ¦‚ç‡æ›´é«˜ã€‚æˆ‘ä»¬éœ€è¦æ‰¾å‡ºä¸€ç§æ–¹æ³•æ¥è®¡ç®—æ–°è§‚æµ‹å€¼å±äºæ¯ä¸ªç±»åˆ«çš„æ¦‚ç‡ã€‚
- en: 'The probability that the new observation belongs to the red class can be written
    this way: `P(Y = red| X = (x_1 = 4, x_2 = 14))`. To make it easier to read, instead
    of `x_1 = 4, x_2 = 14`, I will in the following just write `x` which is a vector
    containing the two values.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: æ–°è§‚æµ‹å€¼å±äºçº¢è‰²ç±»åˆ«çš„æ¦‚ç‡å¯ä»¥è¿™æ ·å†™ï¼š `P(Y = red| X = (x_1 = 4, x_2 = 14))`ã€‚ä¸ºäº†ä¾¿äºé˜…è¯»ï¼Œæˆ‘å°†ä»¥ä¸‹ç”¨ `x` ä»£æ›¿
    `x_1 = 4, x_2 = 14`ï¼Œå®ƒæ˜¯åŒ…å«ä¸¤ä¸ªå€¼çš„å‘é‡ã€‚
- en: According to the Bayes theorem, we can express conditional probabilities as
    `P(Y = red | X = x) = P(Y = red ) * P(X = x | Y = red) / P(X = x)`. So to calculate
    the probability that the new observation is â€œredâ€ given its x values, we need
    to know three other probabilities.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: æ ¹æ®è´å¶æ–¯å®šç†ï¼Œæˆ‘ä»¬å¯ä»¥å°†æ¡ä»¶æ¦‚ç‡è¡¨ç¤ºä¸º `P(Y = red | X = x) = P(Y = red ) * P(X = x | Y = red)
    / P(X = x)`ã€‚å› æ­¤ï¼Œä¸ºäº†è®¡ç®—ç»™å®šxå€¼çš„æ–°è§‚æµ‹å€¼æ˜¯â€œçº¢è‰²â€çš„æ¦‚ç‡ï¼Œæˆ‘ä»¬éœ€è¦çŸ¥é“å¦å¤–ä¸‰ä¸ªæ¦‚ç‡ã€‚
- en: Letâ€™s go step by step. `P(Y = red)` and `P(X = x)` are called â€œprior probabilitiesâ€.
    In the plot, `P(Y = red)` can be calculated simply by taking the share of all
    â€œredâ€ observations of the total number of observations (27,27% in figure 1).
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ä¸€æ­¥æ­¥æ¥ã€‚`P(Y = red)` å’Œ `P(X = x)` è¢«ç§°ä¸ºâ€œå…ˆéªŒæ¦‚ç‡â€ã€‚åœ¨å›¾ä¸­ï¼Œ`P(Y = red)` å¯ä»¥é€šè¿‡å°†æ‰€æœ‰â€œçº¢è‰²â€è§‚å¯Ÿå€¼å æ€»è§‚å¯Ÿå€¼æ•°é‡çš„æ¯”ä¾‹ï¼ˆå›¾1ä¸­ä¸º27,27%ï¼‰æ¥ç®€å•è®¡ç®—ã€‚
- en: Calculating the prior probability of x, `P(X = x)`, is more difficult. We donâ€™t
    have any observation with `x_1 = 4` and `x_2 = 14`in the initial dataset, so what
    is its probability? Zero? Not quite.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: è®¡ç®— `x` çš„å…ˆéªŒæ¦‚ç‡ `P(X = x)` æ›´ä¸ºå›°éš¾ã€‚æˆ‘ä»¬åœ¨åˆå§‹æ•°æ®é›†ä¸­æ²¡æœ‰ `x_1 = 4` å’Œ `x_2 = 14` çš„è§‚å¯Ÿå€¼ï¼Œé‚£ä¹ˆå®ƒçš„æ¦‚ç‡æ˜¯å¤šå°‘ï¼Ÿé›¶ï¼Ÿä¸å®Œå…¨æ˜¯ã€‚
- en: '`P(X = x)` can be understood as the sum of the joint probabilities `P(Y = red,
    X = x)` and `P(Y = blue, X = x)` (see also [here](https://stats.stackexchange.com/questions/294080/proof-of-a-modified-bayes-theorem/601390#601390)):'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '`P(X = x)` å¯ä»¥ç†è§£ä¸ºè”åˆæ¦‚ç‡ `P(Y = red, X = x)` å’Œ `P(Y = blue, X = x)` çš„æ€»å’Œï¼ˆå¦è§ [è¿™é‡Œ](https://stats.stackexchange.com/questions/294080/proof-of-a-modified-bayes-theorem/601390#601390)ï¼‰ï¼š'
- en: '![](../Images/799fcbe610da7c6556a979336e1d23ab.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/799fcbe610da7c6556a979336e1d23ab.png)'
- en: 'Each of the joint probabilities can be expressed with the Bayes theorem as:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: æ¯ä¸ªè”åˆæ¦‚ç‡å¯ä»¥ç”¨è´å¶æ–¯å®šç†è¡¨ç¤ºä¸ºï¼š
- en: '![](../Images/24085fa6839e75ef86e639e814ace775.png)![](../Images/c151ed9d4013af48cf428953eb9cf508.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/24085fa6839e75ef86e639e814ace775.png)![](../Images/c151ed9d4013af48cf428953eb9cf508.png)'
- en: You see the priors `P(red)` and `P(blue)` enter again, which we already know
    how to determine. What is left for us to do, is to find a way to calculate the
    conditional probabilities `P(X = x | Y = red)` and `P(X = x | Y = blue)`.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ ä¼šçœ‹åˆ°å…ˆéªŒ `P(red)` å’Œ `P(blue)` å†æ¬¡å‡ºç°ï¼Œæˆ‘ä»¬å·²ç»çŸ¥é“å¦‚ä½•ç¡®å®šå®ƒä»¬ã€‚å‰©ä¸‹çš„å·¥ä½œæ˜¯æ‰¾åˆ°è®¡ç®—æ¡ä»¶æ¦‚ç‡ `P(X = x | Y =
    red)` å’Œ `P(X = x | Y = blue)` çš„æ–¹æ³•ã€‚
- en: 'Even if we did never see any red data point at position `x` in the plot in
    our original data set, we can find a probability if we regard `x` to be drawn
    from a population of a certain form. Usually when using real world data, the population
    is not directly observable, so we donâ€™t know the true form of the population distribution.
    What LDA does is to assume that the `x` values are normally distributed on the
    `x_1` and `x_2` axis. With real world data, this assumption can be more or less
    reasonable. As we are dealing with a generated data set here though, we donâ€™t
    have to worry about this problem. The data originate from populations which follow
    a normal (i.e. Gaussian) distribution, characterised by two parameters (per independent
    variable): `~N(mean, variance)`.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: å³ä½¿æˆ‘ä»¬åœ¨åŸå§‹æ•°æ®é›†ä¸­ä»æœªçœ‹åˆ°ä½ç½® `x` çš„ä»»ä½•çº¢è‰²æ•°æ®ç‚¹ï¼Œæˆ‘ä»¬ä¹Ÿå¯ä»¥æ‰¾åˆ°ä¸€ä¸ªæ¦‚ç‡ï¼Œå¦‚æœæˆ‘ä»¬å°† `x` çœ‹ä½œæ˜¯ä»æŸç§å½¢å¼çš„æ€»ä½“ä¸­æŠ½å–çš„ã€‚é€šå¸¸åœ¨ä½¿ç”¨ç°å®ä¸–ç•Œæ•°æ®æ—¶ï¼Œæ€»ä½“æ˜¯ä¸å¯ç›´æ¥è§‚å¯Ÿçš„ï¼Œå› æ­¤æˆ‘ä»¬ä¸çŸ¥é“æ€»ä½“åˆ†å¸ƒçš„çœŸå®å½¢å¼ã€‚LDAçš„åšæ³•æ˜¯å‡è®¾
    `x` å€¼åœ¨ `x_1` å’Œ `x_2` è½´ä¸Šæ˜¯æ­£æ€åˆ†å¸ƒçš„ã€‚å¯¹äºç°å®ä¸–ç•Œçš„æ•°æ®ï¼Œè¿™ä¸ªå‡è®¾å¯èƒ½æˆ–å¤šæˆ–å°‘æ˜¯åˆç†çš„ã€‚ç„¶è€Œï¼Œç”±äºæˆ‘ä»¬å¤„ç†çš„æ˜¯ç”Ÿæˆçš„æ•°æ®é›†ï¼Œå› æ­¤æˆ‘ä»¬ä¸å¿…æ‹…å¿ƒè¿™ä¸ªé—®é¢˜ã€‚è¿™äº›æ•°æ®æ¥æºäºéµå¾ªæ­£æ€ï¼ˆå³é«˜æ–¯ï¼‰åˆ†å¸ƒçš„æ€»ä½“ï¼Œå…¶ç‰¹å¾ç”±ä¸¤ä¸ªå‚æ•°ï¼ˆæ¯ä¸ªç‹¬ç«‹å˜é‡ï¼‰ï¼š`~N(mean,
    variance)`ã€‚
- en: Using the formula for multivariate normal distributions, we can describe the
    distribution of the data belonging to class â€œredâ€ as
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨å¤šå…ƒæ­£æ€åˆ†å¸ƒçš„å…¬å¼ï¼Œæˆ‘ä»¬å¯ä»¥å°†å±äºâ€œçº¢è‰²â€ç±»åˆ«çš„æ•°æ®çš„åˆ†å¸ƒæè¿°ä¸º
- en: '![](../Images/4b4031dc61db91365394f8ba1779263f.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4b4031dc61db91365394f8ba1779263f.png)'
- en: where Î£ denotes a covariance matrix. LDA uses only one common covariance matrix
    for all classes, meaning that it assumes that all classes have the same `variance(x_1)`,
    `variance(x_2)` and `covariance(x_1, x_2)`.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­ Î£ è¡¨ç¤ºåæ–¹å·®çŸ©é˜µã€‚LDA ä»…ä½¿ç”¨ä¸€ä¸ªå…±åŒçš„åæ–¹å·®çŸ©é˜µï¼Œæ„å‘³ç€å®ƒå‡è®¾æ‰€æœ‰ç±»åˆ«å…·æœ‰ç›¸åŒçš„ `variance(x_1)`ã€`variance(x_2)`
    å’Œ `covariance(x_1, x_2)`ã€‚
- en: 'Now that weâ€™ve got the formula for `P (X = x | Y = red)`, we can plug this
    in the equation above to get to `P (Y = red|X = x)`. This gives us a monstrous
    equation, which Iâ€™ll skip here. Luckily, it gets simpler from here. Our goal now
    is to find the dividing line that separates the red and blue zones, i.e. we want
    to find those points where the probabilities of being class red or blue are equal:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬å¾—åˆ°äº† `P (X = x | Y = red)` çš„å…¬å¼ï¼Œæˆ‘ä»¬å¯ä»¥å°†å…¶ä»£å…¥ä¸Šé¢çš„æ–¹ç¨‹ä¸­ä»¥å¾—åˆ° `P (Y = red|X = x)`ã€‚è¿™ä¼šå¾—åˆ°ä¸€ä¸ªåºå¤§çš„æ–¹ç¨‹ï¼Œæˆ‘åœ¨è¿™é‡Œç•¥è¿‡ã€‚å¹¸è¿çš„æ˜¯ï¼Œä»è¿™é‡Œå¼€å§‹ä¼šæ›´ç®€å•ã€‚æˆ‘ä»¬çš„ç›®æ ‡æ˜¯æ‰¾åˆ°åˆ†éš”çº¢è‰²å’Œè“è‰²åŒºåŸŸçš„åˆ†ç•Œçº¿ï¼Œå³æˆ‘ä»¬è¦æ‰¾åˆ°é‚£äº›çº¢è‰²æˆ–è“è‰²çš„æ¦‚ç‡ç›¸ç­‰çš„ç‚¹ï¼š
- en: '![](../Images/67b338ecddf9e092b8ae3e60bb63bd78.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/67b338ecddf9e092b8ae3e60bb63bd78.png)'
- en: 'Performing some transformations, it can be shown that minimising this is equivalent
    to minimising the following equation:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: è¿›è¡Œä¸€äº›å˜æ¢åï¼Œå¯ä»¥è¯æ˜æœ€å°åŒ–è¿™ä¸ªç­‰ä»·äºæœ€å°åŒ–ä»¥ä¸‹æ–¹ç¨‹ï¼š
- en: '![](../Images/108550b5f8c0466072a963be46d35546.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/108550b5f8c0466072a963be46d35546.png)'
- en: Solving this for `x_2`, we get a line like
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹ `x_2` æ±‚è§£ï¼Œæˆ‘ä»¬å¾—åˆ°ä¸€æ¡ç±»ä¼¼äº
- en: '![](../Images/100c7df1aaca3863d287b1c2f5f0ed2a.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/100c7df1aaca3863d287b1c2f5f0ed2a.png)'
- en: where `Î²_0` and `Î²_1` are the parameters depending on the means Âµ_red and Âµ_blue,
    the common covariance matrix Î£ and the prior probabilities of red and blue, P(â€œredâ€)
    and P(â€œblueâ€).
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­`Î²_0`å’Œ`Î²_1`æ˜¯ä¾èµ–äºå‡å€¼Âµ_redå’ŒÂµ_blueã€å…±åŒåæ–¹å·®çŸ©é˜µÎ£ä»¥åŠçº¢è‰²å’Œè“è‰²çš„å…ˆéªŒæ¦‚ç‡Pï¼ˆâ€œredâ€ï¼‰å’ŒPï¼ˆâ€œblueâ€ï¼‰çš„å‚æ•°ã€‚
- en: Exploring the plot
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ¢ç´¢å›¾è¡¨
- en: Now that you know how it works, you can explore the plot using different parameters.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ä½ å·²ç»äº†è§£å®ƒæ˜¯å¦‚ä½•å·¥ä½œçš„ï¼Œä½ å¯ä»¥ä½¿ç”¨ä¸åŒçš„å‚æ•°æ¥æ¢ç´¢å›¾è¡¨ã€‚
- en: When you click â€œcreate new sampleâ€, the plot displays two boundaries. The Bayes
    boundary is calculated using the â€œrealâ€ population parameters. The estimated boundary
    in contrast makes estimates for the parameters based on the sample data.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: å½“ä½ ç‚¹å‡»â€œåˆ›å»ºæ–°æ ·æœ¬â€æ—¶ï¼Œå›¾è¡¨ä¼šæ˜¾ç¤ºä¸¤ä¸ªè¾¹ç•Œã€‚è´å¶æ–¯è¾¹ç•Œæ˜¯ä½¿ç”¨â€œçœŸå®â€æ€»ä½“å‚æ•°è®¡ç®—çš„ã€‚ç›¸å¯¹è€Œè¨€ï¼Œä¼°è®¡è¾¹ç•Œåˆ™åŸºäºæ ·æœ¬æ•°æ®å¯¹å‚æ•°è¿›è¡Œä¼°è®¡ã€‚
- en: 'Some guiding questions for exploration:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: æ¢ç´¢çš„ä¸€äº›æŒ‡å¯¼é—®é¢˜ï¼š
- en: Using which parameters does the estimated boundary deviate strongly from the
    Bayes boundary? (see figure 2, left image)
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä½¿ç”¨å“ªäº›å‚æ•°æ—¶ï¼Œä¼°è®¡è¾¹ç•Œä¸è´å¶æ–¯è¾¹ç•Œåå·®è¾ƒå¤§ï¼Ÿï¼ˆè§å›¾2ï¼Œå·¦ä¾§å›¾åƒï¼‰
- en: How sensitive is the boundary to outliers? (see figure 2, center image)
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è¾¹ç•Œå¯¹å¼‚å¸¸å€¼æœ‰å¤šæ•æ„Ÿï¼Ÿï¼ˆè§å›¾2ï¼Œä¸­é—´å›¾åƒï¼‰
- en: Which parameters give results you did not expect?
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å“ªäº›å‚æ•°äº§ç”Ÿäº†ä½ æ²¡æœ‰é¢„æ–™åˆ°çš„ç»“æœï¼Ÿ
- en: '![](../Images/649cf2e543b218c92974395aae814f8b.png)![](../Images/a0465c6200f9b51fbca9d93cb6399d9f.png)![](../Images/bf0cdda45ba59b21e6f72ba1f0ecfcaf.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/649cf2e543b218c92974395aae814f8b.png)![](../Images/a0465c6200f9b51fbca9d93cb6399d9f.png)![](../Images/bf0cdda45ba59b21e6f72ba1f0ecfcaf.png)'
- en: 'Figure 2\. Left: The estimated boundary deviates strongly from the Bayes boundary.
    Center: The boundary adapts to an outlier. Right: The boundary adapts well to
    the data point moving around.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾2\. å·¦ä¾§ï¼šä¼°è®¡è¾¹ç•Œä¸è´å¶æ–¯è¾¹ç•Œåå·®è¾ƒå¤§ã€‚ä¸­é—´ï¼šè¾¹ç•Œé€‚åº”å¼‚å¸¸å€¼ã€‚å³ä¾§ï¼šè¾¹ç•Œå¯¹ç§»åŠ¨çš„æ•°æ®ç‚¹é€‚åº”è‰¯å¥½ã€‚
- en: Final remarks
  id: totrans-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æœ€ç»ˆå¤‡æ³¨
- en: LDA is an important tool to know in the realm of machine learning classification
    methods. However, it is one thing to understand the theory behind LDA through
    equations and formulas, and another to gain a practical understanding through
    hands-on exploration. The interactive tool presented here offers a unique opportunity
    to experiment and understand LDA in a more intuitive way.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: LDAæ˜¯æœºå™¨å­¦ä¹ åˆ†ç±»æ–¹æ³•é¢†åŸŸä¸­çš„ä¸€ä¸ªé‡è¦å·¥å…·ã€‚ç„¶è€Œï¼Œäº†è§£LDAçš„ç†è®ºé€šè¿‡æ–¹ç¨‹å’Œå…¬å¼æ˜¯ä¸€å›äº‹ï¼Œè€Œé€šè¿‡å®è·µæ¢ç´¢è·å¾—å®é™…ç†è§£åˆ™æ˜¯å¦ä¸€å›äº‹ã€‚è¿™é‡Œæä¾›çš„äº’åŠ¨å·¥å…·æä¾›äº†ä¸€ä¸ªç‹¬ç‰¹çš„æœºä¼šï¼Œå¯ä»¥ä»¥æ›´ç›´è§‚çš„æ–¹å¼å®éªŒå’Œç†è§£LDAã€‚
- en: Which other statistical methods would you like to see interactively visualised?
    Let me know in the comments!
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ å¸Œæœ›çœ‹åˆ°å“ªäº›å…¶ä»–ç»Ÿè®¡æ–¹æ³•çš„äº’åŠ¨å¯è§†åŒ–ï¼Ÿè¯·åœ¨è¯„è®ºä¸­å‘Šè¯‰æˆ‘ï¼
- en: Donâ€™t forget to **follow me** to stay in the loop for further articles!
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ«å¿˜äº†**å…³æ³¨æˆ‘**ï¼Œä»¥ä¾¿è·å–æ›´å¤šæ–‡ç« çš„æœ€æ–°åŠ¨æ€ï¼
- en: All images, unless otherwise noted, were created by the author using Observable.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: é™¤éå¦æœ‰è¯´æ˜ï¼Œå¦åˆ™æ‰€æœ‰å›¾åƒå‡ç”±ä½œè€…ä½¿ç”¨Observableåˆ›å»ºã€‚
- en: 'If you want to read up on LDA, have a look into the great book â€œAn Introduction
    to Statistical Learningâ€ by James, Witten, Hastie and Tibshirani, which you can
    download for free on their website: [https://www.statlearning.com/](https://www.statlearning.com/)'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä½ æƒ³äº†è§£LDAï¼Œå¯ä»¥çœ‹çœ‹ç”±Jamesã€Wittenã€Hastieå’ŒTibshiraniç¼–å†™çš„ç²¾å½©ä¹¦ç±ã€Šç»Ÿè®¡å­¦ä¹ å¯¼è®ºã€‹ï¼Œä½ å¯ä»¥åœ¨ä»–ä»¬çš„ç½‘ç«™ä¸Šå…è´¹ä¸‹è½½ï¼š[https://www.statlearning.com/](https://www.statlearning.com/)
