- en: 'Unleashing the Power of GPT-3: Fine-Tuning for Superhero Descriptions'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 释放 GPT-3 的力量：超级英雄描述的微调
- en: 原文：[https://towardsdatascience.com/unleashing-the-power-of-gpt-how-to-fine-tune-your-model-da35c90766c4?source=collection_archive---------0-----------------------#2023-02-18](https://towardsdatascience.com/unleashing-the-power-of-gpt-how-to-fine-tune-your-model-da35c90766c4?source=collection_archive---------0-----------------------#2023-02-18)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/unleashing-the-power-of-gpt-how-to-fine-tune-your-model-da35c90766c4?source=collection_archive---------0-----------------------#2023-02-18](https://towardsdatascience.com/unleashing-the-power-of-gpt-how-to-fine-tune-your-model-da35c90766c4?source=collection_archive---------0-----------------------#2023-02-18)
- en: Step-by-step guide for GPT-3 fine-tuning
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: GPT-3 微调的逐步指南
- en: '[](https://medium.com/@ocaelen?source=post_page-----da35c90766c4--------------------------------)[![Olivier
    Caelen](../Images/5315295f68999af7c14b456694d19979.png)](https://medium.com/@ocaelen?source=post_page-----da35c90766c4--------------------------------)[](https://towardsdatascience.com/?source=post_page-----da35c90766c4--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----da35c90766c4--------------------------------)
    [Olivier Caelen](https://medium.com/@ocaelen?source=post_page-----da35c90766c4--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@ocaelen?source=post_page-----da35c90766c4--------------------------------)[![奥利维耶·卡伦](../Images/5315295f68999af7c14b456694d19979.png)](https://medium.com/@ocaelen?source=post_page-----da35c90766c4--------------------------------)[](https://towardsdatascience.com/?source=post_page-----da35c90766c4--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----da35c90766c4--------------------------------)
    [奥利维耶·卡伦](https://medium.com/@ocaelen?source=post_page-----da35c90766c4--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fd7268030c8a8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funleashing-the-power-of-gpt-how-to-fine-tune-your-model-da35c90766c4&user=Olivier+Caelen&userId=d7268030c8a8&source=post_page-d7268030c8a8----da35c90766c4---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----da35c90766c4--------------------------------)
    ·11 min read·Feb 18, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fda35c90766c4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funleashing-the-power-of-gpt-how-to-fine-tune-your-model-da35c90766c4&user=Olivier+Caelen&userId=d7268030c8a8&source=-----da35c90766c4---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fd7268030c8a8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funleashing-the-power-of-gpt-how-to-fine-tune-your-model-da35c90766c4&user=Olivier+Caelen&userId=d7268030c8a8&source=post_page-d7268030c8a8----da35c90766c4---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----da35c90766c4--------------------------------)
    ·11 min 阅读·2023年2月18日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fda35c90766c4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funleashing-the-power-of-gpt-how-to-fine-tune-your-model-da35c90766c4&user=Olivier+Caelen&userId=d7268030c8a8&source=-----da35c90766c4---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fda35c90766c4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funleashing-the-power-of-gpt-how-to-fine-tune-your-model-da35c90766c4&source=-----da35c90766c4---------------------bookmark_footer-----------)![](../Images/d11e6869ddaac2af15c755bc2460aae9.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fda35c90766c4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funleashing-the-power-of-gpt-how-to-fine-tune-your-model-da35c90766c4&source=-----da35c90766c4---------------------bookmark_footer-----------)![](../Images/d11e6869ddaac2af15c755bc2460aae9.png)'
- en: Photo by [h heyerlein](https://unsplash.com/@heyerlein?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '[摄影：h heyerlein](https://unsplash.com/@heyerlein?utm_source=medium&utm_medium=referral)
    于 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)'
- en: Since the end of 2022, the launch of ChatGPT by OpenAI has been considered by
    many of us to be the iPhone moment of AI. However, OpenAI’s chatbot is not the
    first generative AI text machine learning model and follows the launch of GPT-3,
    which was launched two years earlier.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 自2022年底以来，OpenAI 推出的 ChatGPT 被许多人认为是人工智能的 iPhone 时刻。然而，OpenAI 的聊天机器人并不是第一个生成式
    AI 文本机器学习模型，它跟随的是两年前推出的 GPT-3。
- en: OpenAI provides us with a ready-to-use GPT-3 trained model. Furthermore, a specific
    task can be fine-tuned on a smaller dataset. For example, suppose you want to
    create an email response generator specific to your company. First, you must collect
    a large amount of data about your particular business domain, such as customer
    email inquiries and responses. You can then use this data to fine-tune GPT-3 to
    learn your company’s specific language patterns and phrases. By fine-tuning GPT-3,
    creating a highly customized and specialized email response generator is possible,
    specifically tailored to the language patterns and words used in a particular
    business domain.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI 为我们提供了一个现成的 GPT-3 训练模型。此外，特定任务可以在较小的数据集上进行微调。例如，假设你想创建一个针对你公司特定的电子邮件回复生成器。首先，你必须收集大量有关你特定业务领域的数据，如客户电子邮件咨询和回复。然后，你可以使用这些数据来微调
    GPT-3，以学习你公司特定的语言模式和短语。通过微调 GPT-3，可以创建一个高度定制和专业化的电子邮件回复生成器，专门针对特定业务领域使用的语言模式和词汇。
- en: In this blog post, I will show you how to fine-tune GPT-3\. We will do this
    with python code and without assuming prior knowledge about GPT-3.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇博客文章中，我将向你展示如何微调 GPT-3。我们将使用 python 代码进行操作，并且不假设对 GPT-3 有任何先验知识。
- en: What is needed for fine-tuning?
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 微调需要什么？
- en: Unlike the GPT-2 model available for instance on Hugging Face (when this blog
    is written), we don’t have direct access to the GPT-3 model. Therefore, you first
    need to get an API key from OpenAI and install the Python package *openai*, which
    can be quickly done via *pip*.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 与当前在 Hugging Face 上提供的 GPT-2 模型不同（撰写本文时），我们没有直接访问 GPT-3 模型的权限。因此，你首先需要从 OpenAI
    获取一个 API 密钥，并安装 Python 包 *openai*，可以通过 *pip* 快速完成。
- en: 'For the API key from OpenAI:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 获取 OpenAI 的 API 密钥：
- en: go to [https://platform.openai.com/account/api-keys](https://platform.openai.com/account/api-keys)
    ,
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 访问 [https://platform.openai.com/account/api-keys](https://platform.openai.com/account/api-keys)
    ，
- en: create an account,
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建一个账户，
- en: click on ‘*Create new secret key’* and
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 点击‘*创建新密钥*’，然后
- en: do a copy of the key.
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 复制密钥。
- en: 'The key is a long string of characters starting with ‘sk-’. Make sure you keep
    it secret! Once you have your key, an easy way to get access to your key is to
    do the following in your terminal: (personally, for simplicity, I put it in my
    *.bachrc*):'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 关键是以‘sk-’开头的一长串字符。确保你保密！一旦你拥有了你的密钥，一个简单的获取密钥的方法是在终端中执行以下操作：（个人而言，为了简便，我将其放在我的*.bachrc*中）：
- en: '[PRE0]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Using GPT-3 models has a cost. We need credits. At the time of this writing,
    when you create a new account, you get free credits to try out the tool. I don’t
    know if this will continue…
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 GPT-3 模型是有成本的。我们需要积分。撰写本文时，当你创建一个新账户时，你会获得免费的积分来尝试这个工具。我不知道这种情况是否会持续…
- en: Now that we have our key and Python package, it is time to think about the data
    we need to fine-tune. First, we need a file of examples for fine-tuning, where
    each example is a *prompt* followed by the appropriate *completion*.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了密钥和 Python 包，是时候考虑我们需要微调的数据了。首先，我们需要一个用于微调的示例文件，其中每个示例都是一个 *prompt*，后跟相应的
    *completion*。
- en: A superhero description generation tool
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 一个超级英雄描述生成工具
- en: '![](../Images/96f93f7ff3d3b406b69c8ec1a2ffb322.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/96f93f7ff3d3b406b69c8ec1a2ffb322.png)'
- en: A superhero from DALL-E 2
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 来自 DALL-E 2 的超级英雄
- en: We will build a tool for this demo to create descriptions of imaginary superheroes.
    In the end, the tool will receive the age, gender, and power of the superhero,
    and it will automatically produce a description of our superhero.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将为这个演示构建一个工具，以创建虚构超级英雄的描述。最终，工具将接收超级英雄的年龄、性别和能力，并自动生成超级英雄的描述。
- en: In the following example, after fine-tuning the model, all we have to say is
    ’*40, woman, Healing ->*’, and we will automatically get a description from the
    model.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下示例中，经过模型微调后，我们只需输入'*40, woman, Healing ->*'，模型将自动生成一个描述。
- en: '![](../Images/6cd099cf773d7c6e17b6b7f8867d23b5.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6cd099cf773d7c6e17b6b7f8867d23b5.png)'
- en: This is what it’s all about!! 😃
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是一切的关键！！ 😃
- en: Creation of a synthetic set of data for fine-tuning
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建一个合成的数据集用于微调
- en: In some situations, you may have a data set you want to use for fine-tuning.
    But since I don’t have one, let’s see how to create a synthetic data set with
    the description of the superheroes directly from GPT-3\. The following code will
    give me a CSV file with examples of *prompts* and the corresponding *completions*.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些情况下，你可能有一个数据集想要用于微调。但由于我没有现成的数据集，我们来看看如何直接从 GPT-3 创建一个合成数据集，用于超级英雄的描述。以下代码将给我一个包含
    *prompts* 和相应 *completions* 的 CSV 文件。
- en: '[PRE1]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Let’s look at how this code works 🧐.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们深入了解这段代码是如何工作的 🧐。
- en: The variable *f_prompt* contains the following sentence where *{age}*, *{gender},*
    and *{power}* are missing.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 变量*f_prompt*包含以下句子，其中*{age}*、*{gender}*和*{power}*是缺失的。
- en: 'Imagine a complete and detailed description of a {age}-year-old {gender} fictional
    character who has the superpower of {power}. Write out the entire description
    in a maximum of 100 words in great detail:'
  id: totrans-35
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 想象一下一个详细描述的{age}岁{gender}虚构角色，他拥有{power}的超级能力。用最多100个字写出整个详细描述：
- en: In the first three *for* loops of the code, we iterate over different values
    of {age}, {gender}, and {power}. At each step of the loop, we replace the 3 missing
    variables with different values. Then we use the *openai.Completion.create* function
    to ask GPT to generate a response to our prompt.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在代码的前三个*for*循环中，我们遍历不同的{age}、{gender}和{power}值。在每一步循环中，我们用不同的值替换3个缺失的变量。然后，我们使用*openai.Completion.create*函数请求GPT生成对我们提示的响应。
- en: The most important parameters of this function are
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 这个函数的最重要参数是
- en: '*model*: The model used to generate the response. OpenAI offers four standard
    GPT-3 models (`ada`, `babbage`, `curie`, or `davinci`) that vary in size … and
    price of use. Here it is *davinci* — the biggest model.'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*model*：用于生成响应的模型。OpenAI提供了四种标准GPT-3模型（`ada`、`babbage`、`curie`或`davinci`），它们在规模和使用价格上有所不同。这里使用的是*davinci*——最大的模型。'
- en: '*prompt*: The prompt that we want to fulfill with GPT-3.'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*prompt*：我们希望GPT-3完成的提示。'
- en: '*temperature*: The temperature is a number between 0 and 1 and controls how
    much randomness is in the output. We set the temperature to the maximum to allow
    the model to be as creative as possible in creating the response.'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*temperature*：温度是一个介于0和1之间的数字，控制输出的随机性。我们将温度设置为最大值，以便模型在生成响应时尽可能具有创造力。'
- en: '*max_tokens*: Defines the maximum length of the response.'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*max_tokens*：定义响应的最大长度。'
- en: At the end of this script, we have a Pandas table stored in the file *out_openai_completion.csv.*
    The two primary columns in this table that interest us are *sub_prompt* and *response_txt.*
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个脚本的最后，我们有一个存储在文件*out_openai_completion.csv*中的Pandas表格。这个表格中我们感兴趣的两个主要列是*sub_prompt*和*response_txt*。
- en: The *sub_prompt* will be for example‘*18, man, invisibility’.* It contains the
    three values that were replaced, separated by commas.
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*sub_prompt*例如会是‘*18, man, invisibility*’。它包含了三个用逗号分隔的被替换的值。'
- en: The *response_txt* contains the output of the GPT model.
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*response_txt*包含GPT模型的输出。'
- en: Fine-tuning the GPT model
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 微调GPT模型
- en: The following code retrieves the previously created file *out_openai_completion.csv*
    and uses *openai* to fine-tune a GPT-3 model.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码检索之前创建的文件*out_openai_completion.csv*，并使用*openai*对GPT-3模型进行微调。
- en: '[PRE2]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Let’s take the time to understand this code as well 🤓!
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们花时间来理解这段代码 🤓！
- en: First, the content of the file *out_openai_completion.csv* is loaded into the
    data frame *df*. As a reminder, in our task, if the user enters ’40, female, healing’,
    we want to have a description of a 40-year-old female character with the power
    of healing. To perform fine-tuning, it is necessary to provide GPT with examples
    of what the user might type and the corresponding desired response. In the data
    frame *df*, the columns *sub_prompt* and *response_txt* contain examples of input
    with the corresponding desired response. In the code above, we first extract these
    two colons and then rename them to *prompt* and *completion*, respectively. The
    resulting data frame is stored in a new file *prepared_data.csv* containing only
    these two columns.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，文件*out_openai_completion.csv*的内容被加载到数据框*df*中。为了提醒一下，在我们的任务中，如果用户输入'40, female,
    healing'，我们希望得到一个关于40岁女性角色拥有治愈能力的描述。为了进行微调，需要向GPT提供用户可能输入的示例以及相应的期望响应。在数据框*df*中，*sub_prompt*和*response_txt*列包含了输入示例和对应的期望响应。在上面的代码中，我们首先提取这两列，然后将其分别重命名为*prompt*和*completion*。结果数据框被存储在一个新的文件*prepared_data.csv*中，仅包含这两列。
- en: 'This file *prepared_data.csv* looks like this:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 这个文件*prepared_data.csv*的内容如下：
- en: '[PRE3]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The Python *subprocess.run()* function runs a command as a subprocess. It is
    often used to execute external programs as if they were run in a terminal.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: Python的*subprocess.run()*函数将命令作为子进程运行。它通常用于执行外部程序，就像在终端中运行一样。
- en: We use *subprocess.run()* to execute ‘*openai tools fine_tunes.prepare_data’*.
    This function takes the input file *prepared_data.csv*, checks that the data are
    correct, and produces a JSONL file called *prepared_data_prepared.jsonl*. A JSONL
    file is a format that stores each JSON object on a separate line. JSONL files
    contain a sequence of JSON objects, each separated by a newline character.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用 *subprocess.run()* 执行‘*openai tools fine_tunes.prepare_data*’。这个函数接受输入文件
    *prepared_data.csv*，检查数据是否正确，并生成一个名为 *prepared_data_prepared.jsonl* 的 JSONL 文件。JSONL
    文件是一种将每个 JSON 对象存储在单独一行的格式。JSONL 文件包含一系列 JSON 对象，每个对象之间用换行符分隔。
- en: Note that we have added the option “- - quiet” to automatically accept all recommendations
    made by ‘*openai tools fine_tunes.prepare_data’. F*or example, it suggests adding
    a ‘-->’ to the end of all prompts and adding a token *END* to the end of each
    response.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们已添加选项“- - quiet”以自动接受‘*openai tools fine_tunes.prepare_data*’所做的所有建议。例如，它建议在所有提示的末尾添加‘-->’，并在每个响应的末尾添加一个*END*标记。
- en: 'The first lines of this JSONL file look like this:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 这个 JSONL 文件的前几行看起来像这样：
- en: '[PRE4]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The fine-tuning of the GPT-3 model is really achieved in the second *subprocess.run(),*
    where *openai api fine_tunes.create* is executed. In this function, we start by
    giving the name of the JSONL file created just before. You will then need to select
    the model you wish to fine-tune. OpenAI offers four main models with different
    performance levels suitable for various tasks. *Davinci* is the most powerful
    model, and Ada is the fastest. *Davinci* is also the most expensive model 😨.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-3 模型的微调实际上是在第二个 *subprocess.run()* 中实现的，其中执行了 *openai api fine_tunes.create*。在这个函数中，我们首先提供刚刚创建的
    JSONL 文件的名称。然后你需要选择要微调的模型。OpenAI 提供了四个主要模型，具有不同的性能水平，适用于各种任务。*Davinci* 是最强大的模型，而
    Ada 是最快的。*Davinci* 也是最昂贵的模型 😨。
- en: Since the purpose of my model is to create descriptions of superheroes, we give
    my new model the suffix “*Superhero*”.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我模型的目的是创建超级英雄的描述，因此我们给我的新模型添加了后缀“*超级英雄*”。
- en: And that’s it 😉 After a few minutes, you will have a fine-tuned model ready
    to use 🌟.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样😉 几分钟后，你将拥有一个可以使用的微调模型🌟。
- en: And now it’s time to test your new model
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 现在是时候测试你的新模型了。
- en: There are different ways to use a model for completion. Mainly via the Playground
    provided by OpenAI or via programming languages like Python.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 使用模型进行补全有不同的方法。主要通过 OpenAI 提供的 Playground 或者通过像 Python 这样的编程语言。
- en: The simplest way is probably to use the [playground](https://platform.openai.com/playground).
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 最简单的方法可能是使用 [playground](https://platform.openai.com/playground)。
- en: Go to [https://platform.openai.com/playground](https://platform.openai.com/playground).
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 访问 [https://platform.openai.com/playground](https://platform.openai.com/playground)。
- en: Click on ‘Model’ and search for the one with the suffix “*Superhero*”.
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 点击‘模型’并搜索带有后缀“*超级英雄*”的模型。
- en: Add in ‘Stop sequences’ the token ‘END’.
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在‘停止序列’中添加标记‘END’。
- en: '![](../Images/94c1ac78c2f2ebb1b5694840743e4dbf.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/94c1ac78c2f2ebb1b5694840743e4dbf.png)'
- en: It is now time to ask our model for a new prediction. We will ask to describe
    an 18-year-old male character who really has an unnecessary power 😉 We will ask
    to describe a character who has the power… ‘*can eat a lot*’… and see what happens…
    😆
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是时候要求我们的模型进行新的预测了。我们将要求描述一个18岁的男性角色，他真的有一个不必要的能力😉 我们将要求描述一个拥有‘*吃很多*’能力的角色……看看会发生什么……😆
- en: '![](../Images/e5746b85ae0ca808c0b93c6dc9905c3b.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e5746b85ae0ca808c0b93c6dc9905c3b.png)'
- en: Not so bad 😅
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 不错 😅
- en: Do you want to do it in Python? Simple! Click on ‘View code’ at the top right
    of the screen.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 你想用 Python 来做吗？很简单！点击屏幕右上角的‘查看代码’。
- en: '![](../Images/17fc2789ecac12e829d6a248e8dd1050.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/17fc2789ecac12e829d6a248e8dd1050.png)'
- en: 'In our case, in ‘View code’ we have this:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的案例中，在‘查看代码’中我们有：
- en: '[PRE5]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: You just have to copy and paste it 👍.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 你只需要复制并粘贴即可👍。
- en: To conclude
  id: totrans-75
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this blog, we have seen how to generate synthetic data to refine our model
    and how to do that fine-tuning. We have used a use case of creating a superhero,
    but the same method can be used for any use case you may have. The most important
    thing is to have enough quality examples with prompts and desired responses.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个博客中，我们已经了解了如何生成合成数据以优化我们的模型以及如何进行微调。我们使用了创建超级英雄的用例，但相同的方法也可以用于你可能拥有的任何用例。最重要的是要有足够的质量示例，包含提示和期望的响应。
- en: Thanks for reading!
  id: totrans-77
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 感谢阅读！
- en: Please consider following me if you wish to stay up to date with my latest publications
    and increase the visibility of this blog.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你希望保持最新的出版物并增加这个博客的可见性，请考虑关注我。
