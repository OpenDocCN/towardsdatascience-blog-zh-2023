- en: Introduction to PyTorch
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/introduction-to-pytorch-e0235570a080?source=collection_archive---------11-----------------------#2023-03-01](https://towardsdatascience.com/introduction-to-pytorch-e0235570a080?source=collection_archive---------11-----------------------#2023-03-01)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Going through the Workflow of a PyTorch Project
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@pumaline?source=post_page-----e0235570a080--------------------------------)[![Datamapu](../Images/63b0c7f9a3d160c5bb039bbebd791f7e.png)](https://medium.com/@pumaline?source=post_page-----e0235570a080--------------------------------)[](https://towardsdatascience.com/?source=post_page-----e0235570a080--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----e0235570a080--------------------------------)
    [Datamapu](https://medium.com/@pumaline?source=post_page-----e0235570a080--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ffcd72d75ae6e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintroduction-to-pytorch-e0235570a080&user=Datamapu&userId=fcd72d75ae6e&source=post_page-fcd72d75ae6e----e0235570a080---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----e0235570a080--------------------------------)
    ·13 min read·Mar 1, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fe0235570a080&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintroduction-to-pytorch-e0235570a080&user=Datamapu&userId=fcd72d75ae6e&source=-----e0235570a080---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fe0235570a080&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintroduction-to-pytorch-e0235570a080&source=-----e0235570a080---------------------bookmark_footer-----------)![](../Images/1e2d7992b69d15a3657c1c6969a38233.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Photo by Igor Lepilin on Unsplash
  prefs: []
  type: TYPE_NORMAL
- en: In this article, we will go through the lifecycle of a Deep Learning project
    using PyTorch. We assume that you are already familiar with Neural Networks and
    will not explain them in detail, but only consider the PyTorch-specific aspects.
    We will mainly follow the steps shown in the [official documentation](https://pytorch.org/tutorials/beginner/basics/intro.html)
    of PyTorch, but consider a different example. In the documentation an example
    of image classification is presented, here we will consider tabular data stored
    in a .csv file. This means that some changes will be necessary, especially in
    preparing the dataset. Having two different examples should help to understand
    the general workflow of a PyTorch project better. Additionally to this post, you
    can follow the [colab notebook](https://drive.google.com/file/d/1p31TH09BExMYyo-cm2DfcacoTdxONgwe/view?usp=sharing)
    including the entire code and workflow structure, or find the notebook on [GitHub](https://github.com/froukje/articles/blob/main/06_pytorch_introduction.ipynb).
  prefs: []
  type: TYPE_NORMAL
- en: Data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The data used is downloaded from [kaggle](https://www.kaggle.com/datasets/adityakadiwal/water-potability)
    [1] and is freely available. The data describes different features that are needed
    to determine the water quality in an urban environment. The objective is to predict
    whether the water quality is good or bad. That is we are considering a binary
    classification problem. In total 9 features, which are all numerical, and the
    label are given.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0657b63353cb716606c32939eca8dcdb.png)'
  prefs: []
  type: TYPE_IMG
- en: The features and the label of the given dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Preprocessing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As in every Data Science project, first, some preprocessing needs to be done.
    This is independent of the deep learning framework we use. Therefore we won´t
    go into detail here. For further information, please refer to the [colab notebook](https://colab.research.google.com/drive/1p31TH09BExMYyo-cm2DfcacoTdxONgwe)
    or [GitHub](https://github.com/froukje/articles/blob/main/06_pytorch_introduction.ipynb).
  prefs: []
  type: TYPE_NORMAL
- en: We are lucky and the data does not need very much preparation. All features
    are numerical and float type. There are however some missing values, which we
    imputed with the mean of the corresponding feature. Also to consider is that the
    target variable is not equally distributed, but there are much more 0s (bad water
    quality) than 1s (good water quality). For the sake of simplicity, we upsampled
    the data, such that the number of 0s is equal to the number of 1s by randomly
    drawing samples from the subset of 1s until the same number of samples is reached.
    Finally, we divided the dataset into a training (80%), validation (10%), and test
    (10%) set and scaled the data.
  prefs: []
  type: TYPE_NORMAL
- en: Create a PyTorch Dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In order to use our data in a PyTorch model, we need to bring it into a specific
    form: a *PyTorch Dataset.* The construction of this dataset is decoupled from
    the model. The dataset object stores the samples and their corresponding labels.
    At this point, this example deviates slightly from the PyTorch documentation page.
    The example data used in the documentation is [FashionMNIST](https://www.kaggle.com/datasets/zalando-research/fashionmnist).
    For this (and several other datasets) PyTorch offers pre-loaded datasets. To see
    how to load these datasets, you can check their [PyTorch tutorial](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html).
    However, if you want to use PyTorch for your own data you most likely have to
    write your own customized Dataset class.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To create a customized Dataset class, we can inherit from the Dataset class
    provided by PyTorch. We have to adjust the three following main methods:'
  prefs: []
  type: TYPE_NORMAL
- en: The **__init__** method is run once when instantiating the dataset object. In
    this simple example, only the input and labels are stored as tensors
  prefs: []
  type: TYPE_NORMAL
- en: The **__len__** method returns the number of samples in our dataset.
  prefs: []
  type: TYPE_NORMAL
- en: The **__getitem__** method loads and returns a sample from the dataset at the
    given index.
  prefs: []
  type: TYPE_NORMAL
- en: 'The dataset is also the place for transformations when working e.g. with image
    data. In our tabular data, this is not relevant and therefore not covered here.
    For the water quality problem considered here, the customized Dataset class looks
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Using this class, we define the datasets for the training, validation, and test
    data.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Define the DataLoader
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: After creating a Dataset, we use the PyTorch DataLoader to wrap an iterable
    around it that permits to easy access the data during training and validation.
    The Dataset retrieves our dataset’s features and labels one sample at a time.
    When training a model, we usually want to pass samples of batches and reshuffle
    the data at every epoch. In this example, when iterating through the DataLoader,
    each iteration returns a minibatch of 32 samples. It is possible to further configure
    the DataLoader. For all possible configurations please refer to the [documentation](https://pytorch.org/docs/stable/data.html).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Define a Model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now, the data is prepared and we are ready to define a model. We assume that
    you are familiar with the general structure of a Neural Net. In PyTorch the **torch.nn**
    namespace provides all the building blocks to create a Neural Net. The model we
    use in this example is very simple and only consists of [linear layers](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear),
    the [ReLu activation function](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html#torch.nn.ReLU),
    and a [Dropout layer](https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html).
    For an overview of all pre-defined layers in PyTorch, please refer to the [documentation](https://pytorch.org/docs/stable/nn.html).
  prefs: []
  type: TYPE_NORMAL
- en: We can build our own model by inheriting from the **nn.Module***.* A PyTorch
    model contains at least two methods. The **__init__** method, where all needed
    layers are instantiated, and the **forward** method, where the final model is
    defined. Here is an example model, that gives good enough results for our example
    data.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: This model consists of four linear layers. The number of input and output features
    are defined, which set the size of the input and output sample. Our data consists
    of 9 features, so the number of input features in the first layer is 9\. The output
    feature size can be changed but must fit the next input feature size. Since we
    finally want a 1-dimensional output (0 or 1) the last output feature size is equal
    to 1\. Note, that the final sigmoid-layer is not applied here. We will explain
    this in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Train the Model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Next, we need to train the model. Training a model in PyTorch consists of four
    main steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Apply the model
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Calculate the loss
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Backpropagation
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Update the weights
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'To train one epoch, these steps need to be done for all batches in the *train_dataloader*.
    Another loop then needs to go over the desired number of epochs. In pseudocode
    the training of one epoch looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: The optimizer and the loss function still need to be defined. We will do this
    in the next section. Below is a function that includes this training loop. Additionally,
    some metrics (accuracy, recall, and precision) are calculated. Note, that we set
    the model to training mode (`model.train()`) in contrast to the evaluation mode
    (`model.eval()`). This affects dropout or batch normalization layers, which are
    treated differently for training and validation. The inputs of this function are
  prefs: []
  type: TYPE_NORMAL
- en: 1\. The **model**. This is the above-defined model.
  prefs: []
  type: TYPE_NORMAL
- en: 2\. The **device**. This can be either GPU or CPU and will be set in the next
    section.
  prefs: []
  type: TYPE_NORMAL
- en: 3\. The **train_dataloader**. The above-defined dataloader for the training
    data.
  prefs: []
  type: TYPE_NORMAL
- en: 4\. The **optimizer**. The optimizer is used to minimize the error. We will
    also specify this in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: 5\. The **loss function (criterion)**. We will specify this in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: 6\. The **epoch**. The current epoch.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: The validation of the model is similar, but without the backpropagation and
    without updating the weights. In pseudocode, the validation for one epoch looks
    like this.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: A function for validating one epoch is given below. It is very similar to the
    previous function for training. Note, that the model is set to evaluation mode
    (`model.eval()`) and since no gradients need to be calculated during the validation
    we set `with torch.no_grad()`. This will reduce memory consumption for computations.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Putting it all Together
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To finally train a neural network using PyTorch, we need to do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Generate Datasets** from the data and wrap them into a **Dataloader**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We already did this in the previous sections, however, for the sake of completeness
    we will generate them again.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 2\. **Define the model**
  prefs: []
  type: TYPE_NORMAL
- en: We use our above-defined model `WaterNet.`
  prefs: []
  type: TYPE_NORMAL
- en: 3\. **Define the optimizer**
  prefs: []
  type: TYPE_NORMAL
- en: There are different [optimizers available in PyTorch](https://pytorch.org/docs/stable/optim.html).
    We use the Adam optimizer, which is a very common one. You can however try different
    ones. The Adam optimizer is an extension of the Stochastic Gradient Descent. Said
    in a simplified way the difference is that Stochastic Gradient Descent keeps the
    learning rate constant during training, while in Adam it is adapted. An introduction
    to the Adam optimizer can be found [here](https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 4\. **Define the loss function**
  prefs: []
  type: TYPE_NORMAL
- en: We are considering a binary classification problem with a 1-dimensional output,
    the default choice for this type of problem is the [binary cross entropy](/understanding-binary-cross-entropy-log-loss-a-visual-explanation-a3ac6025181a),
    which we will use.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note that we didn’t apply the final [sigmoid-layer](https://pytorch.org/docs/stable/generated/torch.nn.Sigmoid.html)
    in our model. This was done on purpose since PyTorch offers the `[nn.BCEWithLogitsLoss()](https://pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html)`
    method, which combines the final sigmoid layer and the binary cross entropy. We
    could also apply these two methods separately, i.e. the `[nn.Sigmoid](https://pytorch.org/docs/stable/generated/torch.nn.Sigmoid.html)`
    layer as a final step of the model and then the `[nn.BCE()](https://pytorch.org/docs/stable/generated/torch.nn.BCELoss.html)`
    loss. However, using `nn.BCEWithLogitsLoss()` is the recommended way of dealing
    with binary classification problems, as it is numerically more stable.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Before we start to train the model, we set the hyperparameters. Hyperparameters
    are adjustable parameters that let you control the model optimization process.
    Different hyperparameter values can impact model training and convergence rates.
    In our case, we have three hyperparameters, that we have to set. Note, that also
    the in- and output features of the model layers are also hyperparameters. We set
    them to fix values in the model, you can however try different values.
  prefs: []
  type: TYPE_NORMAL
- en: '`batch_size`: Training and validation batch size'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`epochs`: Number of epochs to train'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`learning_rate`: The learning rate'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We also set the variable `print_every`. This is not a hyperparameter but just
    determines how often the loss is printed during training and validation. Note,
    that we further have to manually set the device to “cuda” if a GPU is available.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Now, we are ready for training. Below is the final training loop. Additionally,
    the calculated metrics are saved for each epoch.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Evaluate the Results
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To evaluate the results we can have a look at the loss and the metrics during
    training and evaluation.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ed6db4f285385f8aa7d567e2e4d7bc7a.png)'
  prefs: []
  type: TYPE_IMG
- en: The loss for training and validation for 150 epochs.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/140d4c2d9d8d9d601fb2f02dde90be62.png)'
  prefs: []
  type: TYPE_IMG
- en: The recall for training and validation for 150 epochs.
  prefs: []
  type: TYPE_NORMAL
- en: You can find the plots for the other calculated metrics in the [notebook](https://colab.research.google.com/drive/1p31TH09BExMYyo-cm2DfcacoTdxONgwe).
  prefs: []
  type: TYPE_NORMAL
- en: Save the Model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If we later want to use our model we need to save it. We can do that by saving
    it’s `state_dict()`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: We can then load it with
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Don’t forget to set the model to evaluation mode using `model.eval()` to put
    the dropout and batch normalization layers in evaluation mode.
  prefs: []
  type: TYPE_NORMAL
- en: Apply the Model to the Test Set
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We now apply our trained model to the test data.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this article, a detailed example of how to use PyTorch for a Deep Learning
    project was shown. The individual steps in the Deep Learning workflow were discussed
    and applied to a concrete dataset. An essential step before training the model
    is to bring the data into the correct form and define a customized dataset for
    the specific application. When training the model the four main steps are (1)
    apply the model, (2) calculate the loss, (3) perform backpropagation and, (4)
    update the weights. An example training function, where all these steps are performed
    is defined and applied. Finally, to use a model it is important to know how to
    store and reload it.
  prefs: []
  type: TYPE_NORMAL
- en: Resources
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[1] Aditya Kadiwal, 2021, Water quality — Dataset for water quality classification,
    [https://www.kaggle.com/datasets/mssmartypants/water-quality](https://www.kaggle.com/datasets/mssmartypants/water-quality),
    downloaded January 2023, License: CC0: Public Domain'
  prefs: []
  type: TYPE_NORMAL
- en: '[2] PyTorch, 2022, [https://pytorch.org/tutorials/beginner/basics/intro.html](https://pytorch.org/tutorials/beginner/basics/intro.html)'
  prefs: []
  type: TYPE_NORMAL
- en: All images unless otherwise noted are by the author.
  prefs: []
  type: TYPE_NORMAL
- en: 'Find more Data Science and Machine Learning posts here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[## More'
  prefs: []
  type: TYPE_NORMAL
- en: Data Science and Machine Learning Blog
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: datamapu.com](https://datamapu.com/?source=post_page-----e0235570a080--------------------------------)
    [](https://medium.com/@pumaline/subscribe?source=post_page-----e0235570a080--------------------------------)
    [## Get an email whenever Pumaline publishes.
  prefs: []
  type: TYPE_NORMAL
- en: Get an email whenever Pumaline publishes. By signing up, you will create a Medium
    account if you don't already have…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: medium.com](https://medium.com/@pumaline/subscribe?source=post_page-----e0235570a080--------------------------------)
    [](https://www.buymeacoffee.com/pumaline?source=post_page-----e0235570a080--------------------------------)
    [## Pumaline
  prefs: []
  type: TYPE_NORMAL
- en: Hey, I like to learn and share knowledge about Data Science and Machine Learning.
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: www.buymeacoffee.com](https://www.buymeacoffee.com/pumaline?source=post_page-----e0235570a080--------------------------------)
  prefs: []
  type: TYPE_NORMAL
