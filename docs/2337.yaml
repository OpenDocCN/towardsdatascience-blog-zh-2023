- en: Deep Dive into PFI for Model Interpretability
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/deep-dive-into-pfi-for-model-interpretability-f12f0c64226c?source=collection_archive---------11-----------------------#2023-07-20](https://towardsdatascience.com/deep-dive-into-pfi-for-model-interpretability-f12f0c64226c?source=collection_archive---------11-----------------------#2023-07-20)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Another interpretability tool for your toolbox
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@tiagotoledojr?source=post_page-----f12f0c64226c--------------------------------)[![Tiago
    Toledo Jr.](../Images/577748ae15ec9eb7ead9355f94287a9d.png)](https://medium.com/@tiagotoledojr?source=post_page-----f12f0c64226c--------------------------------)[](https://towardsdatascience.com/?source=post_page-----f12f0c64226c--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----f12f0c64226c--------------------------------)
    [Tiago Toledo Jr.](https://medium.com/@tiagotoledojr?source=post_page-----f12f0c64226c--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ff4eeaf479b0c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-dive-into-pfi-for-model-interpretability-f12f0c64226c&user=Tiago+Toledo+Jr.&userId=f4eeaf479b0c&source=post_page-f4eeaf479b0c----f12f0c64226c---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----f12f0c64226c--------------------------------)
    ·6 min read·Jul 20, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Ff12f0c64226c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-dive-into-pfi-for-model-interpretability-f12f0c64226c&user=Tiago+Toledo+Jr.&userId=f4eeaf479b0c&source=-----f12f0c64226c---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff12f0c64226c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-dive-into-pfi-for-model-interpretability-f12f0c64226c&source=-----f12f0c64226c---------------------bookmark_footer-----------)![](../Images/7eb50e79f4113ff3f3d5a63159eb633f.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Photo by [fabio](https://unsplash.com/@fabioha?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: Knowing how to assess your model is essential for your work as a data scientist.
    No one will sign off on your solution if you’re not able to fully understand and
    communicate it to your stakeholders. This is why knowing interpretability methods
    is so important.
  prefs: []
  type: TYPE_NORMAL
- en: The lack of interpretability can kill a very good model. I haven’t developed
    a model where my stakeholders weren’t interested in understanding how the predictions
    were made. Therefore, knowing how to interpret a model and communicate it to the
    business is an essential ability for a data scientist.
  prefs: []
  type: TYPE_NORMAL
- en: In this post, we’re going to explore the Permutation Feature Importance (PFI),
    an model agnostic methodology that can help us identify what are the most important
    features of our model, and therefore, communicate better what the model is considering
    when doing its predictions.
  prefs: []
  type: TYPE_NORMAL
- en: What is the Permutation Feature Importance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The PFI method tries to estimate how important a feature is for model results
    based on what happens to the model when we change the feature connected to the
    target variable.
  prefs: []
  type: TYPE_NORMAL
- en: To do that, for each feature, we want to analyze the importance, we random shuffle
    it while keeping all the other features and target the same way.
  prefs: []
  type: TYPE_NORMAL
- en: This makes the feature useless to predict the target since we broke the relationship
    between them by changing their joint distribution.
  prefs: []
  type: TYPE_NORMAL
- en: Then, we can use our model to predict our shuffled dataset. The amount of performance
    reduction in our model will indicate how important that feature is.
  prefs: []
  type: TYPE_NORMAL
- en: 'The algorithm then looks something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: We train a model in a training dataset and then assess its performance on both
    the training and the testing dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For each feature, we create a new dataset where the feature is shuffled
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We then use the trained model to predict the output of the new dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The quotient of the new performance metric by the old one gives us the importance
    of the feature
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Notice that if a feature is not important, the performance of the model should
    not vary a lot. If it is, then the performance must suffer a lot.
  prefs: []
  type: TYPE_NORMAL
- en: Interpreting the PFI
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we know how to calculate the PFI, how do we interpret it?
  prefs: []
  type: TYPE_NORMAL
- en: 'It depends on which fold we are applying the PFI to. We usually have two options:
    applying it to the training or the test dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: Training Interpretation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: During training, our model learns the patterns of the data and tries to represent
    it. Of course, during training, we have no idea of how well our model generalizes
    to unseen data.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, by applying the PFI to the training dataset we are going to see which
    features were the most relevant for the learning of the representation of the
    data by the model.
  prefs: []
  type: TYPE_NORMAL
- en: In business terms, this indicates which features were the most important for
    the model construction.
  prefs: []
  type: TYPE_NORMAL
- en: Test Interpretation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now, if we apply the method to the test dataset, we are going to see the feature
    impact on the generalization of the model.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s think about it. If we see the performance of the model go down in the
    test set after we shuffled a feature, it means that that feature was important
    for the performance on that set. Since the test set is what we use to test generalization
    (if you’re doing everything right), then we can say that it is important for generalization.
  prefs: []
  type: TYPE_NORMAL
- en: The problems with PFI
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The PFI analyzes the effect of a feature in your model performance, therefore,
    it does not state anything about the raw data. If your model performance is poor,
    then any relation you find with PFI will be meaningless.
  prefs: []
  type: TYPE_NORMAL
- en: This is true for both sets, if your model is underfitting (low prediction power
    on the training set) or overfitting (low prediction power on the test set) then
    you cannot take insights from this method.
  prefs: []
  type: TYPE_NORMAL
- en: Also, when two features are highly correlated the PFI can mislead your interpretation.
    If you shuffle one feature but the required information is encoded into another
    one, then the performance may not suffer at all, which would make you think the
    feature is useless, which may not be the case.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing the PFI in Python
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To implement the PFI in Python we must first import our required libraries.
    For this, we are going to use mainly the libraries numpy, pandas, tqdm, and sklearn:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Now, we must load our dataset, which is going to be the Iris dataset. Then,
    we’re going to fit a Random Forest to the data.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'With our model fitted, let’s analyze its performance to see if we can safely
    apply the PFI to see how the features impact our model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'We can see we achieved a 99% accuracy on the training set and a 95.5% accuracy
    on the test set. Looks good for now. Let’s get the original error scores for a
    later comparison:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let’s calculate the permutation scores. For that, it is usual to run the
    shuffle for each feature several times to achieve a statistic of the feature scores
    to avoid any coincidences. In our case, let’s do 10 repetitions for each feature:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we have a dictionary with the performance for each shuffle we did. Now,
    let’s generate a table that has, for each feature in each fold, the average and
    the standard deviation of the performance when compared to the original performance
    of our model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'We will end up with something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: We can see that feature 2 seems to be the most important feature in our dataset
    for both folds, followed by feature 3\. Since we’re not fixing the random seed
    for the shuffle function from numpy we can expect this number to vary.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can then plot the importance in a graph to have a better visualization of
    the importance:'
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The PFI is a simple methodology that can help you quickly identify the most
    important features. Go ahead and try to apply it to some model you’re developing
    to see how it is behaving.
  prefs: []
  type: TYPE_NORMAL
- en: But also be aware of the limitations of the method. Not knowing where a method
    falls short will end up making you do an incorrect interpretation.
  prefs: []
  type: TYPE_NORMAL
- en: Also, notices that the PFI shows the importance of the feature but does not
    states in which direction it is influencing the model output.
  prefs: []
  type: TYPE_NORMAL
- en: So, tell me, how are you going to use this in your next models?
  prefs: []
  type: TYPE_NORMAL
- en: Stay tuned for more posts about interpretability methods that can improve your
    overall understanding of a model.
  prefs: []
  type: TYPE_NORMAL
