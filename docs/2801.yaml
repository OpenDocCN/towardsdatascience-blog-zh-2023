- en: 'Machine Learning with Expert Models: A Primer'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/machine-learning-with-expert-models-a-primer-6c74585f223f?source=collection_archive---------2-----------------------#2023-09-05](https://towardsdatascience.com/machine-learning-with-expert-models-a-primer-6c74585f223f?source=collection_archive---------2-----------------------#2023-09-05)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: How a decades-old idea enables training outrageously large neural networks today
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@samuel.flender?source=post_page-----6c74585f223f--------------------------------)[![Samuel
    Flender](../Images/390d82a673de8a8bb11cef66978269b5.png)](https://medium.com/@samuel.flender?source=post_page-----6c74585f223f--------------------------------)[](https://towardsdatascience.com/?source=post_page-----6c74585f223f--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----6c74585f223f--------------------------------)
    [Samuel Flender](https://medium.com/@samuel.flender?source=post_page-----6c74585f223f--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fce56d9dcd568&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learning-with-expert-models-a-primer-6c74585f223f&user=Samuel+Flender&userId=ce56d9dcd568&source=post_page-ce56d9dcd568----6c74585f223f---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----6c74585f223f--------------------------------)
    ·9 min read·Sep 5, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F6c74585f223f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learning-with-expert-models-a-primer-6c74585f223f&user=Samuel+Flender&userId=ce56d9dcd568&source=-----6c74585f223f---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F6c74585f223f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learning-with-expert-models-a-primer-6c74585f223f&source=-----6c74585f223f---------------------bookmark_footer-----------)![](../Images/6589b3c96e9f917a19170e43a71a9d9a.png)'
  prefs: []
  type: TYPE_NORMAL
- en: ([Pexels](https://www.pexels.com/photo/set-of-tool-wrench-162553/))
  prefs: []
  type: TYPE_NORMAL
- en: Expert models are one of the most useful inventions in Machine Learning, yet
    they hardly receive as much attention as they deserve. In fact, expert modeling
    does not only allow us to train neural networks that are “outrageously large”
    (more on that later), they also allow us to build models that learn more like
    the human brain, that is, different regions specialize in different types of input.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this article, we’ll take a tour of the key innovations in expert modeling
    which ultimately lead to recent breakthroughs such as the Switch Transformer and
    the Expert Choice Routing algorithm. But let’s go back first to the paper that
    started it all: “Mixtures of Experts”.'
  prefs: []
  type: TYPE_NORMAL
- en: Mixtures of Experts (1991)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/ee84508a45288fb4a059d3aeeee05161.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The original MoE model from 1991\. Image credit: [Jabocs et al 1991, Adaptive
    Mixtures of Local Experts](https://www.cs.toronto.edu/~hinton/absps/jjnh91.pdf).'
  prefs: []
  type: TYPE_NORMAL
- en: The idea of mixtures of experts (MoE) traces back more than 3 decades ago, to
    a 1991 [paper](https://www.cs.toronto.edu/~hinton/absps/jjnh91.pdf) co-authored
    by none other than the godfather of AI, Geoffrey Hinton. The key idea in MoE is
    to model an output “y” by combining a number of “experts” E, the weight of each
    is being controlled by a…
  prefs: []
  type: TYPE_NORMAL
