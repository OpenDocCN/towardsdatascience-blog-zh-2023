- en: 'Machine Learning with Expert Models: A Primer'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/machine-learning-with-expert-models-a-primer-6c74585f223f?source=collection_archive---------2-----------------------#2023-09-05](https://towardsdatascience.com/machine-learning-with-expert-models-a-primer-6c74585f223f?source=collection_archive---------2-----------------------#2023-09-05)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: How a decades-old idea enables training outrageously large neural networks today
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@samuel.flender?source=post_page-----6c74585f223f--------------------------------)[![Samuel
    Flender](../Images/390d82a673de8a8bb11cef66978269b5.png)](https://medium.com/@samuel.flender?source=post_page-----6c74585f223f--------------------------------)[](https://towardsdatascience.com/?source=post_page-----6c74585f223f--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----6c74585f223f--------------------------------)
    [Samuel Flender](https://medium.com/@samuel.flender?source=post_page-----6c74585f223f--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fce56d9dcd568&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learning-with-expert-models-a-primer-6c74585f223f&user=Samuel+Flender&userId=ce56d9dcd568&source=post_page-ce56d9dcd568----6c74585f223f---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----6c74585f223f--------------------------------)
    ·9 min read·Sep 5, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F6c74585f223f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learning-with-expert-models-a-primer-6c74585f223f&user=Samuel+Flender&userId=ce56d9dcd568&source=-----6c74585f223f---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F6c74585f223f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learning-with-expert-models-a-primer-6c74585f223f&source=-----6c74585f223f---------------------bookmark_footer-----------)![](../Images/6589b3c96e9f917a19170e43a71a9d9a.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: ([Pexels](https://www.pexels.com/photo/set-of-tool-wrench-162553/))
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: Expert models are one of the most useful inventions in Machine Learning, yet
    they hardly receive as much attention as they deserve. In fact, expert modeling
    does not only allow us to train neural networks that are “outrageously large”
    (more on that later), they also allow us to build models that learn more like
    the human brain, that is, different regions specialize in different types of input.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: 'In this article, we’ll take a tour of the key innovations in expert modeling
    which ultimately lead to recent breakthroughs such as the Switch Transformer and
    the Expert Choice Routing algorithm. But let’s go back first to the paper that
    started it all: “Mixtures of Experts”.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们将深入探讨专家建模中的关键创新，这些创新最终导致了最近的突破，如Switch Transformer和Expert Choice Routing算法。但首先让我们回到一切开始的论文：“专家混合”。
- en: Mixtures of Experts (1991)
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 专家混合（1991）
- en: '![](../Images/ee84508a45288fb4a059d3aeeee05161.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ee84508a45288fb4a059d3aeeee05161.png)'
- en: 'The original MoE model from 1991\. Image credit: [Jabocs et al 1991, Adaptive
    Mixtures of Local Experts](https://www.cs.toronto.edu/~hinton/absps/jjnh91.pdf).'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 1991年的原始MoE模型。图片来源：[Jabocs et al 1991, Adaptive Mixtures of Local Experts](https://www.cs.toronto.edu/~hinton/absps/jjnh91.pdf)。
- en: The idea of mixtures of experts (MoE) traces back more than 3 decades ago, to
    a 1991 [paper](https://www.cs.toronto.edu/~hinton/absps/jjnh91.pdf) co-authored
    by none other than the godfather of AI, Geoffrey Hinton. The key idea in MoE is
    to model an output “y” by combining a number of “experts” E, the weight of each
    is being controlled by a…
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 专家混合（MoE）的概念可以追溯到30多年前，源于1991年由人工智能之父**Geoffrey Hinton**共同撰写的一篇[论文](https://www.cs.toronto.edu/~hinton/absps/jjnh91.pdf)。MoE的关键思想是通过结合多个“专家”E来建模输出“y”，每个专家的权重由…
