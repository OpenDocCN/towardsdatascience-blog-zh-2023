- en: What I Learned Pushing Prompt Engineering to the Limit
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/what-i-learned-pushing-prompt-engineering-to-the-limit-c40f0740641f?source=collection_archive---------0-----------------------#2023-06-12](https://towardsdatascience.com/what-i-learned-pushing-prompt-engineering-to-the-limit-c40f0740641f?source=collection_archive---------0-----------------------#2023-06-12)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[](https://medium.com/@jacob_marks?source=post_page-----c40f0740641f--------------------------------)[![Jacob
    Marks, Ph.D.](../Images/94d9832b8706d1044e3195386613bfab.png)](https://medium.com/@jacob_marks?source=post_page-----c40f0740641f--------------------------------)[](https://towardsdatascience.com/?source=post_page-----c40f0740641f--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----c40f0740641f--------------------------------)
    [Jacob Marks, Ph.D.](https://medium.com/@jacob_marks?source=post_page-----c40f0740641f--------------------------------)'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Â·
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ff7dc0c0eae92&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-i-learned-pushing-prompt-engineering-to-the-limit-c40f0740641f&user=Jacob+Marks%2C+Ph.D.&userId=f7dc0c0eae92&source=post_page-f7dc0c0eae92----c40f0740641f---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----c40f0740641f--------------------------------)
    Â·10 min readÂ·Jun 12, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fc40f0740641f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-i-learned-pushing-prompt-engineering-to-the-limit-c40f0740641f&user=Jacob+Marks%2C+Ph.D.&userId=f7dc0c0eae92&source=-----c40f0740641f---------------------clap_footer-----------)'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc40f0740641f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-i-learned-pushing-prompt-engineering-to-the-limit-c40f0740641f&source=-----c40f0740641f---------------------bookmark_footer-----------)![](../Images/ad4b84de610367e97b341392a5b74e5e.png)'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: Satirical depiction of prompt engineering. Ironically, the DALL-E2 generated
    image was generated by the author using prompt engineering with the prompt â€œa
    mad scientist handing over a scroll to an artificially intelligent robot, generated
    in a retro styleâ€, plus a variation, plus outpainting.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: I spent the past two months building a large-language-model (LLM) powered application.
    It was an exciting, intellectually stimulating, and at times frustrating experience.
    My entire conception of prompt engineering â€” and of what is possible with LLMs
    â€” changed over the course of the project.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: Iâ€™d love to share with you some of my biggest takeaways with the goal of shedding
    light on some of the often unspoken aspects of prompt engineering. I hope that
    after reading about my trials and tribulations, you will be able to make more
    informed prompt engineering decisions. If youâ€™d already dabbled in prompt engineering,
    I hope that this helps you push forward in your own journey!
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: 'For context, here is the TL;DR on the project weâ€™ll be learning from:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: My team and I built [VoxelGPT](https://github.com/voxel51/voxelgpt/tree/main),
    an application that combines LLMs with the [FiftyOne](https://github.com/voxel51/fiftyone)
    computer vision query language to enable searching through image and video datasets
    via natural language. VoxelGPT also answers questions about FiftyOne itself.
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: VoxelGPT is open source (so is FiftyOne!). All of the code is [available on
    GitHub](https://github.com/voxel51/voxelgpt).
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can try VoxelGPT for free at gpt.fiftyone.ai.
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If youâ€™re curious how we built VoxelGPT, you can [read more about it on TDS
    here](https://medium.com/towards-data-science/how-i-turned-chatgpt-into-an-sql-like-translator-for-image-and-video-datasets-7b22b318400a).
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now, Iâ€™ve split the prompt engineering lessons into four categories:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: '[General Lessons](#5220)'
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Prompting Techniques](#0d68)'
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Examples](#383e)'
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Tooling](#9e09)'
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: General Lessons
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Science? Engineering? Black Magic?
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Prompt engineering is as much experimentation as it is engineering. There are
    an infinite number of ways to write a prompt, from the specific wording of your
    question, to the content and formatting of the context you feed in. It can be
    overwhelming. I found it easiest to start simple and build up an intuition â€” and
    then test out hypotheses.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: 'In computer vision, each dataset has its own schema, label types, and class
    names. The goal for VoxelGPT was to be able to work with *any* computer vision
    dataset, but we started with just a single dataset: MS COCO. Keeping all of the
    additional degrees of freedom fixed allowed us to nail down into the LLMâ€™s ability
    to write syntactically correct queries in the first place.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: Once youâ€™ve determined a formula that is successful in a limited context, then
    figure out how to generalize and build upon this.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: Which Model(s) to Use?
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: People say that one of the most important characteristics of large language
    models is that they are relatively interchangeable. In theory, you should be able
    to swap one LLM out for another without substantially changing the connective
    tissue.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: While it is true that changing the LLM you use is often as simple as swapping
    out an API call, there are definitely some difficulties that arise in practice.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: Some models have much shorter context lengths than others. Switching to a model
    with a shorter context can require major refactoring.
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Open source is great, but open source LLMs are not as performant (yet) as GPT
    models. Plus, if you are deploying an application with an open source LLM, you
    will need to make sure the container running the model has enough memory and storage.
    This can end up being more troublesome (and more expensive) than just using API
    endpoints.
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¼€æºæ˜¯å¾ˆæ£’çš„ï¼Œä½†å¼€æº LLM çš„æ€§èƒ½ï¼ˆç›®å‰ï¼‰è¿˜ä¸å¦‚ GPT æ¨¡å‹ã€‚è€Œä¸”ï¼Œå¦‚æœä½ è¦éƒ¨ç½²ä¸€ä¸ªä½¿ç”¨å¼€æº LLM çš„åº”ç”¨ï¼Œä½ éœ€è¦ç¡®ä¿è¿è¡Œæ¨¡å‹çš„å®¹å™¨æœ‰è¶³å¤Ÿçš„å†…å­˜å’Œå­˜å‚¨ã€‚è¿™å¯èƒ½æ¯”ç›´æ¥ä½¿ç”¨
    API ç«¯ç‚¹æ›´éº»çƒ¦ï¼ˆä¹Ÿæ›´æ˜‚è´µï¼‰ã€‚
- en: If you start using GPT-4 and then switch to GPT-3.5 because of cost, you may
    be shocked by the drop-off in performance. For complicated code generation and
    inference tasks, GPT-4 is MUCH better.
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¦‚æœä½ å¼€å§‹ä½¿ç”¨ GPT-4 ç„¶åå› ä¸ºæˆæœ¬é—®é¢˜åˆ‡æ¢åˆ° GPT-3.5ï¼Œä½ å¯èƒ½ä¼šå¯¹æ€§èƒ½çš„ä¸‹é™æ„Ÿåˆ°éœ‡æƒŠã€‚å¯¹äºå¤æ‚çš„ä»£ç ç”Ÿæˆå’Œæ¨ç†ä»»åŠ¡ï¼ŒGPT-4 è¦å¥½å¾—å¤šã€‚
- en: Where to Use LLMs?
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä½•æ—¶ä½¿ç”¨ LLMï¼Ÿ
- en: 'Large language models are powerful. But just because they may be capable of
    certain tasks doesnâ€™t mean you need to â€” or even should â€” use them for those tasks.
    The best way to think about LLMs is as *enablers*. LLMs are not the WHOLE solution:
    they are just a part of it. Donâ€™t expect large language models to do everything.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: å¤§å‹è¯­è¨€æ¨¡å‹æ˜¯å¼ºå¤§çš„ã€‚ä½†ä»…ä»…å› ä¸ºå®ƒä»¬å¯èƒ½èƒ½å¤Ÿå®ŒæˆæŸäº›ä»»åŠ¡ï¼Œå¹¶ä¸æ„å‘³ç€ä½ éœ€è¦â€”â€”ç”šè‡³åº”è¯¥â€”â€”å°†å®ƒä»¬ç”¨äºè¿™äº›ä»»åŠ¡ã€‚æ€è€ƒ LLM çš„æœ€ä½³æ–¹å¼æ˜¯å°†å…¶è§†ä¸º*èµ‹èƒ½å·¥å…·*ã€‚LLM
    ä¸æ˜¯è§£å†³æ–¹æ¡ˆçš„å…¨éƒ¨ï¼šå®ƒä»¬åªæ˜¯å…¶ä¸­çš„ä¸€éƒ¨åˆ†ã€‚ä¸è¦æŒ‡æœ›å¤§å‹è¯­è¨€æ¨¡å‹èƒ½åšæ‰€æœ‰äº‹æƒ…ã€‚
- en: As an example, it may be the case that the LLM you are using can (under ideal
    circumstances) generate properly formatted API calls. But if you know what the
    structure of the API call should look like, and you are actually interested in
    filling in sections of the API call (variable names, conditions, etc.), then just
    use the LLM to do those tasks, and use the (properly post-processed) LLM outputs
    to generate structured API calls yourself. This will be cheaper, more efficient,
    and more reliable.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸¾ä¸ªä¾‹å­ï¼Œå¯èƒ½ä½ ä½¿ç”¨çš„ LLM å¯ä»¥ï¼ˆåœ¨ç†æƒ³æƒ…å†µä¸‹ï¼‰ç”Ÿæˆæ ¼å¼æ­£ç¡®çš„ API è°ƒç”¨ã€‚ä½†å¦‚æœä½ çŸ¥é“ API è°ƒç”¨çš„ç»“æ„åº”æ˜¯ä»€ä¹ˆæ ·çš„ï¼Œå¹¶ä¸”å®é™…ä¸Šå¯¹å¡«å…… API
    è°ƒç”¨çš„éƒ¨åˆ†ï¼ˆå˜é‡åã€æ¡ä»¶ç­‰ï¼‰æ„Ÿå…´è¶£ï¼Œé‚£ä¹ˆåªéœ€ä½¿ç”¨ LLM æ¥å®Œæˆè¿™äº›ä»»åŠ¡ï¼Œç„¶åè‡ªå·±ä½¿ç”¨ï¼ˆç»è¿‡é€‚å½“åå¤„ç†çš„ï¼‰LLM è¾“å‡ºç”Ÿæˆç»“æ„åŒ–çš„ API è°ƒç”¨ã€‚è¿™å°†æ›´ä¾¿å®œã€æ›´é«˜æ•ˆï¼Œä¹Ÿæ›´å¯é ã€‚
- en: A complete system with LLMs will definitely have a lot of connective tissue
    and classical logic, plus a slew of traditional software engineering and ML engineering
    components. Find what works best for your application.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: æ‹¥æœ‰ LLM çš„å®Œæ•´ç³»ç»Ÿè‚¯å®šä¼šæœ‰å¤§é‡çš„è¿æ¥éƒ¨åˆ†å’Œç»å…¸é€»è¾‘ï¼Œä»¥åŠä¸€ç³»åˆ—ä¼ ç»Ÿçš„è½¯ä»¶å·¥ç¨‹å’Œ ML å·¥ç¨‹ç»„ä»¶ã€‚æ‰¾åˆ°æœ€é€‚åˆä½ åº”ç”¨çš„è§£å†³æ–¹æ¡ˆã€‚
- en: LLMs Are Biased
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: LLMs æœ‰åè§
- en: Language models are both inference engines and knowledge stores. Oftentimes,
    the knowledge store aspect of an LLM can be of great interest to users â€” many
    people use LLMs as search engine replacements! By now, anyone who has used an
    LLM knows that they are prone to making up fake â€œfactsâ€ â€” a phenomenon referred
    to as *hallucination*.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: è¯­è¨€æ¨¡å‹æ—¢æ˜¯æ¨ç†å¼•æ“ä¹Ÿæ˜¯çŸ¥è¯†å­˜å‚¨åº“ã€‚é€šå¸¸ï¼ŒLLM çš„çŸ¥è¯†å­˜å‚¨åº“æ–¹é¢å¯¹ç”¨æˆ·éå¸¸æœ‰å¸å¼•åŠ›â€”â€”è®¸å¤šäººå°† LLM ç”¨ä½œæœç´¢å¼•æ“çš„æ›¿ä»£å“ï¼åˆ°ç°åœ¨ï¼Œä»»ä½•ä½¿ç”¨è¿‡ LLM
    çš„äººéƒ½çŸ¥é“å®ƒä»¬å®¹æ˜“ç¼–é€ è™šå‡çš„â€œäº‹å®â€â€”â€”è¿™ç°è±¡è¢«ç§°ä¸º*å¹»è§‰*ã€‚
- en: 'Sometimes, however, LLMs suffer from the opposite problem: they are too firmly
    fixated on facts from their training data.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶è€Œï¼Œæœ‰æ—¶ LLM é‡åˆ°çš„æ˜¯ç›¸åçš„é—®é¢˜ï¼šå®ƒä»¬è¿‡äºå›ºæ‰§äºå…¶è®­ç»ƒæ•°æ®ä¸­çš„äº‹å®ã€‚
- en: In our case, we were trying to prompt GPT-3.5 to determine the appropriate ViewStages
    (pipelines of logical operations) required in converting a userâ€™s natural language
    query into a valid FiftyOne Python query. The problem was that GPT-3.5 knew about
    the `Match` and `FilterLabels` ViewStages, which have existed in FiftyOne for
    some time, but its training data did *not* include recently added functionality
    wherein a `SortBySimilarity` ViewStage can be used to find images the resemble
    a text prompt.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æˆ‘ä»¬çš„æ¡ˆä¾‹ä¸­ï¼Œæˆ‘ä»¬å°è¯•ä¿ƒä½¿ GPT-3.5 ç¡®å®šå°†ç”¨æˆ·çš„è‡ªç„¶è¯­è¨€æŸ¥è¯¢è½¬æ¢ä¸ºæœ‰æ•ˆçš„ FiftyOne Python æŸ¥è¯¢æ‰€éœ€çš„é€‚å½“ ViewStagesï¼ˆé€»è¾‘æ“ä½œçš„ç®¡é“ï¼‰ã€‚é—®é¢˜åœ¨äº
    GPT-3.5 äº†è§£ `Match` å’Œ `FilterLabels` ViewStagesï¼Œè¿™äº›åœ¨ FiftyOne ä¸­å­˜åœ¨äº†ä¸€æ®µæ—¶é—´ï¼Œä½†å…¶è®­ç»ƒæ•°æ®ä¸åŒ…æ‹¬æœ€è¿‘æ·»åŠ çš„åŠŸèƒ½ï¼Œå…¶ä¸­
    `SortBySimilarity` ViewStage å¯ä»¥ç”¨æ¥æ‰¾åˆ°ä¸æ–‡æœ¬æç¤ºç›¸ä¼¼çš„å›¾åƒã€‚
- en: We tried passing in a definition of `SortBySimilarity`, details about its usage,
    and examples. We even tried instructing GPT-3.5 that it MUST NOT use the `Match`
    or `FilterLabels` ViewStages, or else it will be penalized. No matter what we
    tried, the LLM still oriented itself towards what it *knew,* whether it was the
    right choice or not. We were fighting against the LLMâ€™s instincts!
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°è¯•ä¼ å…¥ `SortBySimilarity` çš„å®šä¹‰ã€ä½¿ç”¨ç»†èŠ‚å’Œç¤ºä¾‹ã€‚æˆ‘ä»¬ç”šè‡³å°è¯•æŒ‡ç¤º GPT-3.5 ç»å¯¹ä¸èƒ½ä½¿ç”¨ `Match` æˆ– `FilterLabels`
    ViewStagesï¼Œå¦åˆ™ä¼šå—åˆ°æƒ©ç½šã€‚ä¸ç®¡æˆ‘ä»¬å°è¯•ä»€ä¹ˆï¼ŒLLM ä»ç„¶å€¾å‘äºå…¶*å·²çŸ¥çš„*å†…å®¹ï¼Œæ— è®ºæ˜¯å¦æ˜¯æ­£ç¡®çš„é€‰æ‹©ã€‚æˆ‘ä»¬åœ¨ä¸ LLM çš„æœ¬èƒ½ä½œæ–—äº‰ï¼
- en: We ended up having to deal with this issue in post-processing.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬æœ€ç»ˆä¸å¾—ä¸åœ¨åå¤„ç†ä¸­å¤„ç†è¿™ä¸ªé—®é¢˜ã€‚
- en: Painful Post-Processing Is Inevitable
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç—›è‹¦çš„åå¤„ç†æ˜¯ä¸å¯é¿å…çš„
- en: No matter how good your examples are; no matter how strict your prompts are
    â€” large language models will invariably hallucinate, give you improperly formatted
    responses, and throw a tantrum when they donâ€™t understand input information. The
    most predictable property of LLMs is the unpredictability of their outputs.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: æ— è®ºä½ çš„ç¤ºä¾‹æœ‰å¤šå¥½ï¼›æ— è®ºä½ çš„æç¤ºæœ‰å¤šä¸¥æ ¼â€”â€”å¤§å‹è¯­è¨€æ¨¡å‹æ€»ä¼šäº§ç”Ÿå¹»è§‰ï¼Œç»™å‡ºæ ¼å¼é”™è¯¯çš„å“åº”ï¼Œå½“å®ƒä»¬æ— æ³•ç†è§£è¾“å…¥ä¿¡æ¯æ—¶ï¼Œè¿˜ä¼šå‘è„¾æ°”ã€‚LLMæœ€å¯é¢„æµ‹çš„ç‰¹æ€§å°±æ˜¯å…¶è¾“å‡ºçš„ä¸å¯é¢„æµ‹æ€§ã€‚
- en: I spent an ungodly amount of time writing routines to pattern match for and
    correct hallucinated syntax. The [post-processing file](https://github.com/voxel51/voxelgpt/blob/main/links/dataset_view_generator.py)
    ended up containing almost 1600 lines of Python code!
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘èŠ±è´¹äº†å¤§é‡çš„æ—¶é—´ç¼–å†™ä¾‹ç¨‹æ¥åŒ¹é…æ¨¡å¼å¹¶çº æ­£å¹»è§‰è¯­æ³•ã€‚æœ€ç»ˆï¼Œ[åå¤„ç†æ–‡ä»¶](https://github.com/voxel51/voxelgpt/blob/main/links/dataset_view_generator.py)åŒ…å«äº†è¿‘1600è¡ŒPythonä»£ç ï¼
- en: Some of these subroutines were as straightforward as adding parenthesis, or
    changing â€œandâ€ and â€œorâ€ to â€œ&â€ and â€œ|â€ in logical expressions. Some subroutines
    were far more involved, like validating the names of the entities in the LLMâ€™s
    responses, converting one ViewStage to another if certain conditions were met,
    ensuring that the numbers and types of arguments to methods were valid.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›å­ä¾‹ç¨‹ä¸­æœ‰äº›éå¸¸ç®€å•ï¼Œæ¯”å¦‚æ·»åŠ æ‹¬å·ï¼Œæˆ–åœ¨é€»è¾‘è¡¨è¾¾å¼ä¸­å°†â€œandâ€å’Œâ€œorâ€æ›´æ”¹ä¸ºâ€œ&â€å’Œâ€œ|â€ã€‚æœ‰äº›å­ä¾‹ç¨‹åˆ™å¤æ‚å¾—å¤šï¼Œæ¯”å¦‚éªŒè¯LLMå“åº”ä¸­çš„å®ä½“åç§°ï¼Œå¦‚æœæ»¡è¶³æŸäº›æ¡ä»¶ï¼Œå°†ä¸€ä¸ªViewStageè½¬æ¢ä¸ºå¦ä¸€ä¸ªï¼Œç¡®ä¿æ–¹æ³•çš„å‚æ•°æ•°é‡å’Œç±»å‹æœ‰æ•ˆã€‚
- en: 'If you are using prompt engineering in a somewhat confined code generation
    context, Iâ€™d recommend the following approach:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä½ åœ¨ä¸€ä¸ªç›¸å¯¹å°é—­çš„ä»£ç ç”Ÿæˆç¯å¢ƒä¸­ä½¿ç”¨æç¤ºå·¥ç¨‹ï¼Œæˆ‘æ¨èä»¥ä¸‹æ–¹æ³•ï¼š
- en: Write your own custom error parser using Abstract Syntax Trees (Pythonâ€™s [ast](https://docs.python.org/3/library/ast.html)
    module).
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ä½¿ç”¨æŠ½è±¡è¯­æ³•æ ‘ï¼ˆPythonçš„[ast](https://docs.python.org/3/library/ast.html)æ¨¡å—ï¼‰ç¼–å†™ä½ è‡ªå·±çš„è‡ªå®šä¹‰é”™è¯¯è§£æå™¨ã€‚
- en: If the results are syntactically invalid, feed the generated error message into
    your LLM and have it try again.
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å¦‚æœç»“æœåœ¨è¯­æ³•ä¸Šæ— æ•ˆï¼Œå°†ç”Ÿæˆçš„é”™è¯¯æ¶ˆæ¯è¾“å…¥åˆ°LLMä¸­ï¼Œè®©å®ƒå†è¯•ä¸€æ¬¡ã€‚
- en: This approach fails to address the more insidious case where syntax is valid
    but the results are not right. If anyone has a good suggestion for this (beyond
    AutoGPT and â€œshow your workâ€ style approaches), please let me know!
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ç§æ–¹æ³•æœªèƒ½è§£å†³æ›´éšè”½çš„æƒ…å†µï¼Œå³è¯­æ³•æœ‰æ•ˆä½†ç»“æœä¸æ­£ç¡®ã€‚å¦‚æœæœ‰äººå¯¹è¿™ä¸ªé—®é¢˜æœ‰å¥½çš„å»ºè®®ï¼ˆè¶…å‡ºAutoGPTå’Œâ€œå±•ç¤ºä½ çš„å·¥ä½œâ€é£æ ¼çš„æ–¹æ³•ï¼‰ï¼Œè¯·å‘Šè¯‰æˆ‘ï¼
- en: Prompting Techniques
  id: totrans-49
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æç¤ºæŠ€æœ¯
- en: The More the Merrier
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: äººè¶Šå¤šè¶Šå¥½
- en: 'To build VoxelGPT, I used what seemed like every prompting technique under
    the sun:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†æ„å»ºVoxelGPTï¼Œæˆ‘ä½¿ç”¨äº†å‡ ä¹æ‰€æœ‰çš„æç¤ºæŠ€æœ¯ï¼š
- en: â€œYou are an expertâ€
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€œä½ æ˜¯ä¸€ä¸ªä¸“å®¶â€
- en: â€œYour task isâ€
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€œä½ çš„ä»»åŠ¡æ˜¯â€
- en: â€œYou MUSTâ€
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€œä½ å¿…é¡»â€
- en: â€œYou will be penalizedâ€
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€œä½ å°†ä¼šå—åˆ°æƒ©ç½šâ€
- en: â€œHere are the rulesâ€
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€œä»¥ä¸‹æ˜¯è§„åˆ™â€
- en: No combination of such phrases will *ensure* a certain type of behavior. Clever
    prompting just isnâ€™t enough.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: ä»»ä½•è¿™ç§çŸ­è¯­çš„ç»„åˆéƒ½æ— æ³•*ç¡®ä¿*æŸç§è¡Œä¸ºã€‚å·§å¦™çš„æç¤ºæ˜¯ä¸å¤Ÿçš„ã€‚
- en: That being said, the more of these techniques you employ in a prompt, the more
    you nudge the LLM in the right direction!
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: è¯è™½å¦‚æ­¤ï¼Œä½ åœ¨æç¤ºä¸­ä½¿ç”¨çš„è¿™äº›æŠ€æœ¯è¶Šå¤šï¼Œå°±è¶Šèƒ½å°†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¼•å¯¼åˆ°æ­£ç¡®çš„æ–¹å‘ï¼
- en: Examples > Documentation
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹ > æ–‡æ¡£
- en: It is common knowledge by now (and common sense!) that both examples and other
    contextual information like documentation can help elicit better responses from
    a large language model. I found this to be the case for VoxelGPT.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨å·²ç»æ˜¯å¸¸è¯†ï¼ˆä¹Ÿæ˜¯å¸¸è¯†ï¼ï¼‰ï¼Œç¤ºä¾‹å’Œå…¶ä»–ä¸Šä¸‹æ–‡ä¿¡æ¯å¦‚æ–‡æ¡£å¯ä»¥å¸®åŠ©å¼•å¯¼å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆæ›´å¥½çš„å“åº”ã€‚æˆ‘å‘ç°VoxelGPTç¡®å®æ˜¯è¿™æ ·ã€‚
- en: Once you add all of the directly pertinent examples and documentation though,
    what should you do if you have extra room in the context window? In my experience,
    I found that tangentially related examples mattered more than tangentially related
    documentation.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€æ—¦ä½ æ·»åŠ äº†æ‰€æœ‰ç›´æ¥ç›¸å…³çš„ç¤ºä¾‹å’Œæ–‡æ¡£ï¼Œå¦‚æœä¸Šä¸‹æ–‡çª—å£ä¸­è¿˜æœ‰é¢å¤–çš„ç©ºé—´ï¼Œä½ åº”è¯¥æ€ä¹ˆåšï¼Ÿæ ¹æ®æˆ‘çš„ç»éªŒï¼Œæˆ‘å‘ç°é—´æ¥ç›¸å…³çš„ç¤ºä¾‹æ¯”é—´æ¥ç›¸å…³çš„æ–‡æ¡£æ›´é‡è¦ã€‚
- en: Modularity >> Monolith
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ¨¡å—åŒ– >> å¤§ä¸€ç»Ÿ
- en: The more you can break down an overarching problem into smaller subproblems,
    the better. Rather than feeding the dataset schema and a list of end-to-end examples,
    it is much more effective to identify individual selection and inference steps
    (selection-inference prompting), and feed in only the relevant information at
    each step.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ å°†ä¸€ä¸ªæ€»ä½“é—®é¢˜æ‹†è§£æˆæ›´å°çš„å­é—®é¢˜çš„ç¨‹åº¦è¶Šé«˜ï¼Œæ•ˆæœè¶Šå¥½ã€‚ä¸å…¶æä¾›æ•°æ®é›†æ¨¡å¼å’Œç«¯åˆ°ç«¯ç¤ºä¾‹åˆ—è¡¨ï¼Œä¸å¦‚è¯†åˆ«å•ç‹¬çš„é€‰æ‹©å’Œæ¨ç†æ­¥éª¤ï¼ˆé€‰æ‹©-æ¨ç†æç¤ºï¼‰ï¼Œå¹¶åœ¨æ¯ä¸ªæ­¥éª¤ä¸­ä»…æä¾›ç›¸å…³ä¿¡æ¯ã€‚
- en: 'This is preferable for three reasons:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æœ‰ä¸‰ä¸ªä¼˜ç‚¹ï¼š
- en: LLMs are better at doing one task at a time than multiple tasks at once.
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: LLM åœ¨ä¸€æ¬¡å¤„ç†ä¸€ä¸ªä»»åŠ¡æ—¶è¡¨ç°å¾—æ¯”åŒæ—¶å¤„ç†å¤šä¸ªä»»åŠ¡æ›´å¥½ã€‚
- en: The smaller the steps, the easier to sanitize inputs and outputs.
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æ­¥éª¤è¶Šå°ï¼Œæ¸…ç†è¾“å…¥å’Œè¾“å‡ºå°±è¶Šå®¹æ˜“ã€‚
- en: It is an important exercise for you as the engineer to understand the logic
    of your application. The point of LLMs isnâ€™t to make the world a black box. It
    is to enable new workflows.
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å¯¹ä½ ä½œä¸ºå·¥ç¨‹å¸ˆæ¥è¯´ï¼Œç†è§£åº”ç”¨é€»è¾‘æ˜¯ä¸€ä¸ªé‡è¦çš„ç»ƒä¹ ã€‚LLM çš„ç›®çš„ä¸æ˜¯è®©ä¸–ç•Œå˜æˆé»‘åŒ£å­ï¼Œè€Œæ˜¯å¯ç”¨æ–°çš„å·¥ä½œæµç¨‹ã€‚
- en: Examples
  id: totrans-68
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ä¾‹å­
- en: How Many Do I Need?
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æˆ‘éœ€è¦å¤šå°‘ä¸ªä¾‹å­ï¼Ÿ
- en: A big part of prompt engineering is figuring out how many examples you need
    for a given task. This is *highly* problem specific.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: æç¤ºå·¥ç¨‹çš„ä¸€ä¸ªé‡è¦éƒ¨åˆ†æ˜¯ç¡®å®šä½ éœ€è¦å¤šå°‘ä¸ªä¾‹å­æ¥å®Œæˆç‰¹å®šä»»åŠ¡ã€‚è¿™æ˜¯*é«˜åº¦*ä¾èµ–äºé—®é¢˜çš„ã€‚
- en: For some tasks ([effective query generation](https://github.com/voxel51/voxelgpt/blob/main/prompts/effective_prompt_generator_prefix.txt)
    and [answering questions based on the FiftyOne documentation](https://github.com/voxel51/voxelgpt/blob/main/prompts/docs_qa_template.txt)),
    we were able to get away without *any* examples. For others ([tag selection](https://github.com/voxel51/voxelgpt/blob/main/examples/tag_selection_examples.csv),
    [whether or not chat history is relevant](https://github.com/voxel51/voxelgpt/blob/main/examples/history_relevance_examples.csv),
    and [named entity recognition for label classes](https://github.com/voxel51/voxelgpt/blob/main/examples/label_class_examples.csv))
    we just needed a few examples to get the job done. Our main inference task, however,
    has almost 400 examples (and that is still the limiting factor in overall performance),
    so we only pass in the most relevant examples at inference time.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºä¸€äº›ä»»åŠ¡ï¼ˆ[æœ‰æ•ˆæŸ¥è¯¢ç”Ÿæˆ](https://github.com/voxel51/voxelgpt/blob/main/prompts/effective_prompt_generator_prefix.txt)
    å’Œ [æ ¹æ® FiftyOne æ–‡æ¡£å›ç­”é—®é¢˜](https://github.com/voxel51/voxelgpt/blob/main/prompts/docs_qa_template.txt)ï¼‰ï¼Œæˆ‘ä»¬èƒ½å¤Ÿåšåˆ°æ²¡æœ‰*ä»»ä½•*ä¾‹å­ã€‚è€Œå¯¹äºå…¶ä»–ä»»åŠ¡ï¼ˆ[æ ‡ç­¾é€‰æ‹©](https://github.com/voxel51/voxelgpt/blob/main/examples/tag_selection_examples.csv)ï¼Œ[èŠå¤©è®°å½•æ˜¯å¦ç›¸å…³](https://github.com/voxel51/voxelgpt/blob/main/examples/history_relevance_examples.csv)å’Œ
    [æ ‡ç­¾ç±»å‘½åå®ä½“è¯†åˆ«](https://github.com/voxel51/voxelgpt/blob/main/examples/label_class_examples.csv)ï¼‰ï¼Œæˆ‘ä»¬åªéœ€è¦å‡ ä¸ªä¾‹å­å°±èƒ½å®Œæˆå·¥ä½œã€‚ç„¶è€Œï¼Œæˆ‘ä»¬çš„ä¸»è¦æ¨ç†ä»»åŠ¡å‡ ä¹æœ‰400ä¸ªä¾‹å­ï¼ˆè¿™ä»ç„¶æ˜¯æ•´ä½“æ€§èƒ½çš„é™åˆ¶å› ç´ ï¼‰ï¼Œå› æ­¤æˆ‘ä»¬ä»…åœ¨æ¨ç†æ—¶ä¼ å…¥æœ€ç›¸å…³çš„ä¾‹å­ã€‚
- en: 'When you are generating examples, try to follow two guidelines:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: å½“ä½ ç”Ÿæˆä¾‹å­æ—¶ï¼Œå°è¯•éµå¾ªä¸¤ä¸ªæŒ‡å—ï¼š
- en: Be as comprehensive as possible. If you have a finite space of possibilities,
    then try to give the LLM at least one example for each case. For VoxelGPT, we
    tried to have at the very least one example for each syntactically correct way
    of using each and every ViewStage â€” and typically a few examples for each, so
    the LLM can do pattern matching.
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å°½å¯èƒ½å…¨é¢ã€‚å¦‚æœä½ æœ‰ä¸€ä¸ªæœ‰é™çš„å¯èƒ½æ€§ç©ºé—´ï¼Œé‚£ä¹ˆå°è¯•ä¸ºæ¯ç§æƒ…å†µæä¾›è‡³å°‘ä¸€ä¸ªä¾‹å­ã€‚å¯¹äº VoxelGPTï¼Œæˆ‘ä»¬è‡³å°‘å°è¯•ä¸ºæ¯ç§è¯­æ³•æ­£ç¡®çš„ ViewStage
    ä½¿ç”¨æ–¹å¼æä¾›ä¸€ä¸ªä¾‹å­â€”â€”é€šå¸¸æ¯ç§æ–¹å¼ä¼šæœ‰å‡ ä¸ªä¾‹å­ï¼Œä»¥ä¾¿ LLM è¿›è¡Œæ¨¡å¼åŒ¹é…ã€‚
- en: Be as consistent as possible. If you are breaking the task down into multiple
    subtasks, make sure the examples are consistent from one task to the next. You
    can reuse examples!
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å°½å¯èƒ½ä¿æŒä¸€è‡´ã€‚å¦‚æœä½ å°†ä»»åŠ¡æ‹†åˆ†ä¸ºå¤šä¸ªå­ä»»åŠ¡ï¼Œç¡®ä¿ä¾‹å­åœ¨ä¸€ä¸ªä»»åŠ¡ä¸ä¸‹ä¸€ä¸ªä»»åŠ¡ä¹‹é—´ä¿æŒä¸€è‡´ã€‚ä½ å¯ä»¥é‡ç”¨ä¾‹å­ï¼
- en: Synthetic Examples
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: åˆæˆä¾‹å­
- en: Generating examples is a laborious process, and handcrafted examples can only
    take you so far. Itâ€™s just not possible to think of every possible scenario ahead
    of time. When you deploy your application, you can log user queries and use these
    to improve your example set.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: ç”Ÿæˆä¾‹å­æ˜¯ä¸€ä¸ªç¹ççš„è¿‡ç¨‹ï¼Œæ‰‹å·¥åˆ¶ä½œçš„ä¾‹å­åªèƒ½åšåˆ°æœ‰é™ç¨‹åº¦ã€‚äº‹å…ˆæƒ³åˆ°æ¯ç§å¯èƒ½çš„æƒ…å¢ƒå‡ ä¹æ˜¯ä¸å¯èƒ½çš„ã€‚å½“ä½ éƒ¨ç½²åº”ç”¨æ—¶ï¼Œå¯ä»¥è®°å½•ç”¨æˆ·æŸ¥è¯¢ï¼Œå¹¶åˆ©ç”¨è¿™äº›æŸ¥è¯¢æ¥æ”¹è¿›ä½ çš„ä¾‹å­é›†ã€‚
- en: Prior to deployment, however, your best bet might be to generate *synthetic*
    examples.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶è€Œï¼Œåœ¨éƒ¨ç½²ä¹‹å‰ï¼Œä½ æœ€å¥½çš„é€‰æ‹©å¯èƒ½æ˜¯ç”Ÿæˆ*åˆæˆ*ä¾‹å­ã€‚
- en: 'Here are two approaches to generating synthetic examples that you might find
    helpful:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œæœ‰ä¸¤ç§ç”Ÿæˆåˆæˆä¾‹å­çš„æ–¹å¼ï¼Œä½ å¯èƒ½ä¼šå‘ç°æœ‰ç”¨ï¼š
- en: Use an LLM to generate examples. You can ask the LLM to vary its language, or
    even imitate the style of potential users! This didnâ€™t work for us, but Iâ€™m convinced
    it could work for many applications.
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ LLM ç”Ÿæˆä¾‹å­ã€‚ä½ å¯ä»¥è®© LLM å˜æ¢è¯­è¨€ï¼Œç”šè‡³æ¨¡ä»¿æ½œåœ¨ç”¨æˆ·çš„é£æ ¼ï¼è™½ç„¶è¿™å¯¹æˆ‘ä»¬æ²¡ç”¨ï¼Œä½†æˆ‘ç›¸ä¿¡å®ƒå¯¹è®¸å¤šåº”ç”¨å¯èƒ½æœ‰æ•ˆã€‚
- en: Programmatically generate examples â€” potentially with randomness â€” based on
    elements in the input query itself. For VoxelGPT, this means generating examples
    based on the fields in the userâ€™s dataset. We are in the process of incorporating
    this into our pipeline, and the results weâ€™ve seen so far have been promising.
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: åŸºäºè¾“å…¥æŸ¥è¯¢ä¸­çš„å…ƒç´ ç¨‹åºæ€§åœ°ç”Ÿæˆç¤ºä¾‹â€”â€”å¯èƒ½å¸¦æœ‰éšæœºæ€§ã€‚å¯¹äº VoxelGPTï¼Œè¿™æ„å‘³ç€åŸºäºç”¨æˆ·æ•°æ®é›†ä¸­çš„å­—æ®µç”Ÿæˆç¤ºä¾‹ã€‚æˆ‘ä»¬æ­£åœ¨å°†å…¶çº³å…¥æˆ‘ä»¬çš„ç®¡é“ä¸­ï¼Œç›®å‰ä¸ºæ­¢çœ‹åˆ°çš„ç»“æœå¾ˆæœ‰å¸Œæœ›ã€‚
- en: Tooling
  id: totrans-81
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å·¥å…·
- en: LangChain
  id: totrans-82
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: LangChain
- en: 'LangChain is popular for a reason: the library makes it easy to connect LLM
    inputs and outputs in complex ways, abstracting away the gory details. The Models
    and Prompts modules especially are top notch.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: LangChain ä¹‹æ‰€ä»¥å—æ¬¢è¿æ˜¯æœ‰åŸå› çš„ï¼šè¿™ä¸ªåº“ä½¿å¾—ä»¥å¤æ‚çš„æ–¹å¼è¿æ¥ LLM çš„è¾“å…¥å’Œè¾“å‡ºå˜å¾—ç®€å•ï¼ŒæŠ½è±¡æ‰äº†ç¹ççš„ç»†èŠ‚ã€‚ç‰¹åˆ«æ˜¯æ¨¡å‹å’Œæç¤ºæ¨¡å—è¡¨ç°å‡ºè‰²ã€‚
- en: 'That being said, LangChain is definitely a work in progress: their Memories,
    Indexes, and Chains modules all have significant limitations. Here are just a
    few of the issues I encountered when trying to use LangChain:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: è¯è™½å¦‚æ­¤ï¼ŒLangChain ç»å¯¹æ˜¯ä¸€ä¸ªä¸æ–­è¿›æ­¥çš„é¡¹ç›®ï¼šä»–ä»¬çš„è®°å¿†ã€ç´¢å¼•å’Œé“¾æ¨¡å—éƒ½æœ‰æ˜¾è‘—çš„é™åˆ¶ã€‚è¿™åªæ˜¯æˆ‘åœ¨å°è¯•ä½¿ç”¨ LangChain æ—¶é‡åˆ°çš„ä¸€äº›é—®é¢˜ã€‚
- en: 'Document Loaders and Text Splitters: In LangChain, [Document Loaders](https://python.langchain.com/en/latest/modules/indexes/document_loaders.html)
    are supposed to transform data from different file formats into text, and [Text
    Splitters](https://python.langchain.com/en/latest/modules/indexes/text_splitters.html)
    are supposed to split text into semantically meaningful chunks. VoxelGPT answers
    questions about the FiftyOne documentation by retrieving the most relevant chunks
    of the docs and piping them into a prompt. In order to generate meaningful answers
    to questions about the FiftyOne docs, I had to effectively build custom loaders
    and splitters, because LangChain didnâ€™t provide the appropriate flexibility.'
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æ–‡æ¡£åŠ è½½å™¨å’Œæ–‡æœ¬æ‹†åˆ†å™¨ï¼šåœ¨ LangChain ä¸­ï¼Œ[æ–‡æ¡£åŠ è½½å™¨](https://python.langchain.com/en/latest/modules/indexes/document_loaders.html)
    åº”å°†ä¸åŒæ–‡ä»¶æ ¼å¼çš„æ•°æ®è½¬æ¢ä¸ºæ–‡æœ¬ï¼Œè€Œ [æ–‡æœ¬æ‹†åˆ†å™¨](https://python.langchain.com/en/latest/modules/indexes/text_splitters.html)
    åˆ™åº”å°†æ–‡æœ¬æ‹†åˆ†æˆè¯­ä¹‰ä¸Šæœ‰æ„ä¹‰çš„å—ã€‚VoxelGPT é€šè¿‡æ£€ç´¢æ–‡æ¡£ä¸­æœ€ç›¸å…³çš„å—å¹¶å°†å…¶ä¼ å…¥æç¤ºä¸­æ¥å›ç­”æœ‰å…³ FiftyOne æ–‡æ¡£çš„é—®é¢˜ã€‚ä¸ºäº†ç”Ÿæˆæœ‰æ„ä¹‰çš„ç­”æ¡ˆï¼Œæˆ‘ä¸å¾—ä¸æœ‰æ•ˆåœ°æ„å»ºè‡ªå®šä¹‰åŠ è½½å™¨å’Œæ‹†åˆ†å™¨ï¼Œå› ä¸º
    LangChain æ²¡æœ‰æä¾›é€‚å½“çš„çµæ´»æ€§ã€‚
- en: 'Vectorstores: LangChain offers [Vectorstore](https://python.langchain.com/en/latest/modules/indexes/vectorstores.html)
    integrations and Vectorstore-based [Retrievers](https://python.langchain.com/en/latest/modules/indexes/retrievers.html)
    to help find relevant information to incorporate into LLM prompts. This is great
    in theory, but the implementations are lacking in flexibility. I had to write
    a custom implementation with ChromaDB in order to pass embedding vectors ahead
    of time and not have them recomputed every time I ran the application. I also
    had to write a custom retriever to implement the custom pre-filtering I needed.'
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å‘é‡å­˜å‚¨ï¼šLangChain æä¾›äº† [Vectorstore](https://python.langchain.com/en/latest/modules/indexes/vectorstores.html)
    é›†æˆå’ŒåŸºäº Vectorstore çš„ [æ£€ç´¢å™¨](https://python.langchain.com/en/latest/modules/indexes/retrievers.html)ï¼Œä»¥å¸®åŠ©æ‰¾åˆ°ç›¸å…³ä¿¡æ¯å¹¶çº³å…¥
    LLM æç¤ºä¸­ã€‚è¿™åœ¨ç†è®ºä¸Šå¾ˆæ£’ï¼Œä½†å®ç°ç¼ºä¹çµæ´»æ€§ã€‚æˆ‘ä¸å¾—ä¸ä½¿ç”¨ ChromaDB ç¼–å†™è‡ªå®šä¹‰å®ç°ï¼Œä»¥ä¾¿é¢„å…ˆä¼ é€’åµŒå…¥å‘é‡ï¼Œè€Œä¸æ˜¯æ¯æ¬¡è¿è¡Œåº”ç”¨ç¨‹åºæ—¶éƒ½é‡æ–°è®¡ç®—å®ƒä»¬ã€‚æˆ‘è¿˜ä¸å¾—ä¸ç¼–å†™è‡ªå®šä¹‰æ£€ç´¢å™¨ï¼Œä»¥å®ç°æ‰€éœ€çš„è‡ªå®šä¹‰é¢„ç­›é€‰ã€‚
- en: 'Question Answering with Sources: When building out question answering over
    the FiftyOne docs, I arrived at a reasonable solution utilizing LangChainâ€™s `RetrievalQA`
    Chain. When I wanted to add sources in, I thought it would be as straightforward
    as swapping out that chain for LangChainâ€™s `RetrievalQAWithSourcesChain`. However,
    bad prompting techniques meant that this chain exhibited some unfortunate behavior,
    such as [hallucinating about Michael Jackson](https://github.com/hwchase17/langchain/issues/2510).
    Once again, I had to [take matters into my own hands](https://github.com/voxel51/voxelgpt/blob/main/links/utils.py).'
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å¸¦æ¥æºçš„é—®ç­”ï¼šåœ¨æ„å»ºå¯¹ FiftyOne æ–‡æ¡£çš„é—®ç­”æ—¶ï¼Œæˆ‘æ‰¾åˆ°äº†ä¸€ç§åˆç†çš„è§£å†³æ–¹æ¡ˆï¼Œåˆ©ç”¨äº† LangChain çš„ `RetrievalQA` Chainã€‚å½“æˆ‘æƒ³æ·»åŠ æ¥æºæ—¶ï¼Œæˆ‘ä»¥ä¸ºåªéœ€å°†è¯¥é“¾æ›¿æ¢ä¸º
    LangChain çš„ `RetrievalQAWithSourcesChain` å°±å¯ä»¥äº†ã€‚ç„¶è€Œï¼Œä¸è‰¯çš„æç¤ºæŠ€æœ¯å¯¼è‡´è¯¥é“¾è¡¨ç°å‡ºä¸€äº›ä¸å¹¸çš„è¡Œä¸ºï¼Œä¾‹å¦‚ [å…³äºè¿ˆå…‹å°”Â·æ°å…‹é€Šçš„è™šå‡ä¿¡æ¯](https://github.com/hwchase17/langchain/issues/2510)ã€‚æˆ‘å†æ¬¡ä¸å¾—ä¸
    [è‡ªå·±åŠ¨æ‰‹](https://github.com/voxel51/voxelgpt/blob/main/links/utils.py)ã€‚
- en: What does all of this mean? It may be easier to just build the components yourself!
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ„å‘³ç€ä»€ä¹ˆå‘¢ï¼Ÿä¹Ÿè®¸è‡ªå·±æ„å»ºè¿™äº›ç»„ä»¶æ›´ç®€å•ï¼
- en: Vector Databases
  id: totrans-89
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å‘é‡æ•°æ®åº“
- en: Vector search may be on ğŸ”¥ğŸ”¥ğŸ”¥, but that doesnâ€™t mean you NEED it for your project.
    I initially implemented our similar example retrieval routine using ChromaDB,
    but because we only had hundreds of examples, I ended up switching to an exact
    nearest neighbor search. I did need to deal with all of the metadata filtering
    myself, but the result was a faster routine with fewer dependencies.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: TikToken
  id: totrans-91
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Adding [TikToken](https://github.com/openai/tiktoken) into the equation was
    incredibly easy. In total, TikToken added <10 lines of code to the project, but
    allowed us to be much more precise when counting tokens and trying to fit as much
    information as possible into the context length. This is the only true no-brainer
    when it comes to tooling.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  id: totrans-93
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There are tons of LLMs to choose from, lots of shiny new tools, and a bunch
    of â€œprompt engineeringâ€ techniques. All of this can be both exciting and overwhelming.
    The key to building an application with prompt engineering is to:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: Break the problem down; build the solution up
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Treat LLMs as *enablers*, not as end-to-end solutions
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Only use tools when they make your life easier
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Embrace experimentation!
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Go build something cool!
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
