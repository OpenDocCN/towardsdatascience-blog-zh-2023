- en: What I Learned Pushing Prompt Engineering to the Limit
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/what-i-learned-pushing-prompt-engineering-to-the-limit-c40f0740641f?source=collection_archive---------0-----------------------#2023-06-12](https://towardsdatascience.com/what-i-learned-pushing-prompt-engineering-to-the-limit-c40f0740641f?source=collection_archive---------0-----------------------#2023-06-12)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[](https://medium.com/@jacob_marks?source=post_page-----c40f0740641f--------------------------------)[![Jacob
    Marks, Ph.D.](../Images/94d9832b8706d1044e3195386613bfab.png)](https://medium.com/@jacob_marks?source=post_page-----c40f0740641f--------------------------------)[](https://towardsdatascience.com/?source=post_page-----c40f0740641f--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----c40f0740641f--------------------------------)
    [Jacob Marks, Ph.D.](https://medium.com/@jacob_marks?source=post_page-----c40f0740641f--------------------------------)'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: ·
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ff7dc0c0eae92&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-i-learned-pushing-prompt-engineering-to-the-limit-c40f0740641f&user=Jacob+Marks%2C+Ph.D.&userId=f7dc0c0eae92&source=post_page-f7dc0c0eae92----c40f0740641f---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----c40f0740641f--------------------------------)
    ·10 min read·Jun 12, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fc40f0740641f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-i-learned-pushing-prompt-engineering-to-the-limit-c40f0740641f&user=Jacob+Marks%2C+Ph.D.&userId=f7dc0c0eae92&source=-----c40f0740641f---------------------clap_footer-----------)'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc40f0740641f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-i-learned-pushing-prompt-engineering-to-the-limit-c40f0740641f&source=-----c40f0740641f---------------------bookmark_footer-----------)![](../Images/ad4b84de610367e97b341392a5b74e5e.png)'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: Satirical depiction of prompt engineering. Ironically, the DALL-E2 generated
    image was generated by the author using prompt engineering with the prompt “a
    mad scientist handing over a scroll to an artificially intelligent robot, generated
    in a retro style”, plus a variation, plus outpainting.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: I spent the past two months building a large-language-model (LLM) powered application.
    It was an exciting, intellectually stimulating, and at times frustrating experience.
    My entire conception of prompt engineering — and of what is possible with LLMs
    — changed over the course of the project.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: I’d love to share with you some of my biggest takeaways with the goal of shedding
    light on some of the often unspoken aspects of prompt engineering. I hope that
    after reading about my trials and tribulations, you will be able to make more
    informed prompt engineering decisions. If you’d already dabbled in prompt engineering,
    I hope that this helps you push forward in your own journey!
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: 'For context, here is the TL;DR on the project we’ll be learning from:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: My team and I built [VoxelGPT](https://github.com/voxel51/voxelgpt/tree/main),
    an application that combines LLMs with the [FiftyOne](https://github.com/voxel51/fiftyone)
    computer vision query language to enable searching through image and video datasets
    via natural language. VoxelGPT also answers questions about FiftyOne itself.
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: VoxelGPT is open source (so is FiftyOne!). All of the code is [available on
    GitHub](https://github.com/voxel51/voxelgpt).
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can try VoxelGPT for free at gpt.fiftyone.ai.
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you’re curious how we built VoxelGPT, you can [read more about it on TDS
    here](https://medium.com/towards-data-science/how-i-turned-chatgpt-into-an-sql-like-translator-for-image-and-video-datasets-7b22b318400a).
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now, I’ve split the prompt engineering lessons into four categories:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: '[General Lessons](#5220)'
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Prompting Techniques](#0d68)'
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Examples](#383e)'
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Tooling](#9e09)'
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: General Lessons
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Science? Engineering? Black Magic?
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Prompt engineering is as much experimentation as it is engineering. There are
    an infinite number of ways to write a prompt, from the specific wording of your
    question, to the content and formatting of the context you feed in. It can be
    overwhelming. I found it easiest to start simple and build up an intuition — and
    then test out hypotheses.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: 'In computer vision, each dataset has its own schema, label types, and class
    names. The goal for VoxelGPT was to be able to work with *any* computer vision
    dataset, but we started with just a single dataset: MS COCO. Keeping all of the
    additional degrees of freedom fixed allowed us to nail down into the LLM’s ability
    to write syntactically correct queries in the first place.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: Once you’ve determined a formula that is successful in a limited context, then
    figure out how to generalize and build upon this.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: Which Model(s) to Use?
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: People say that one of the most important characteristics of large language
    models is that they are relatively interchangeable. In theory, you should be able
    to swap one LLM out for another without substantially changing the connective
    tissue.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: While it is true that changing the LLM you use is often as simple as swapping
    out an API call, there are definitely some difficulties that arise in practice.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: Some models have much shorter context lengths than others. Switching to a model
    with a shorter context can require major refactoring.
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Open source is great, but open source LLMs are not as performant (yet) as GPT
    models. Plus, if you are deploying an application with an open source LLM, you
    will need to make sure the container running the model has enough memory and storage.
    This can end up being more troublesome (and more expensive) than just using API
    endpoints.
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 开源是很棒的，但开源 LLM 的性能（目前）还不如 GPT 模型。而且，如果你要部署一个使用开源 LLM 的应用，你需要确保运行模型的容器有足够的内存和存储。这可能比直接使用
    API 端点更麻烦（也更昂贵）。
- en: If you start using GPT-4 and then switch to GPT-3.5 because of cost, you may
    be shocked by the drop-off in performance. For complicated code generation and
    inference tasks, GPT-4 is MUCH better.
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你开始使用 GPT-4 然后因为成本问题切换到 GPT-3.5，你可能会对性能的下降感到震惊。对于复杂的代码生成和推理任务，GPT-4 要好得多。
- en: Where to Use LLMs?
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 何时使用 LLM？
- en: 'Large language models are powerful. But just because they may be capable of
    certain tasks doesn’t mean you need to — or even should — use them for those tasks.
    The best way to think about LLMs is as *enablers*. LLMs are not the WHOLE solution:
    they are just a part of it. Don’t expect large language models to do everything.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型是强大的。但仅仅因为它们可能能够完成某些任务，并不意味着你需要——甚至应该——将它们用于这些任务。思考 LLM 的最佳方式是将其视为*赋能工具*。LLM
    不是解决方案的全部：它们只是其中的一部分。不要指望大型语言模型能做所有事情。
- en: As an example, it may be the case that the LLM you are using can (under ideal
    circumstances) generate properly formatted API calls. But if you know what the
    structure of the API call should look like, and you are actually interested in
    filling in sections of the API call (variable names, conditions, etc.), then just
    use the LLM to do those tasks, and use the (properly post-processed) LLM outputs
    to generate structured API calls yourself. This will be cheaper, more efficient,
    and more reliable.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 举个例子，可能你使用的 LLM 可以（在理想情况下）生成格式正确的 API 调用。但如果你知道 API 调用的结构应是什么样的，并且实际上对填充 API
    调用的部分（变量名、条件等）感兴趣，那么只需使用 LLM 来完成这些任务，然后自己使用（经过适当后处理的）LLM 输出生成结构化的 API 调用。这将更便宜、更高效，也更可靠。
- en: A complete system with LLMs will definitely have a lot of connective tissue
    and classical logic, plus a slew of traditional software engineering and ML engineering
    components. Find what works best for your application.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 拥有 LLM 的完整系统肯定会有大量的连接部分和经典逻辑，以及一系列传统的软件工程和 ML 工程组件。找到最适合你应用的解决方案。
- en: LLMs Are Biased
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: LLMs 有偏见
- en: Language models are both inference engines and knowledge stores. Oftentimes,
    the knowledge store aspect of an LLM can be of great interest to users — many
    people use LLMs as search engine replacements! By now, anyone who has used an
    LLM knows that they are prone to making up fake “facts” — a phenomenon referred
    to as *hallucination*.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 语言模型既是推理引擎也是知识存储库。通常，LLM 的知识存储库方面对用户非常有吸引力——许多人将 LLM 用作搜索引擎的替代品！到现在，任何使用过 LLM
    的人都知道它们容易编造虚假的“事实”——这现象被称为*幻觉*。
- en: 'Sometimes, however, LLMs suffer from the opposite problem: they are too firmly
    fixated on facts from their training data.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，有时 LLM 遇到的是相反的问题：它们过于固执于其训练数据中的事实。
- en: In our case, we were trying to prompt GPT-3.5 to determine the appropriate ViewStages
    (pipelines of logical operations) required in converting a user’s natural language
    query into a valid FiftyOne Python query. The problem was that GPT-3.5 knew about
    the `Match` and `FilterLabels` ViewStages, which have existed in FiftyOne for
    some time, but its training data did *not* include recently added functionality
    wherein a `SortBySimilarity` ViewStage can be used to find images the resemble
    a text prompt.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的案例中，我们尝试促使 GPT-3.5 确定将用户的自然语言查询转换为有效的 FiftyOne Python 查询所需的适当 ViewStages（逻辑操作的管道）。问题在于
    GPT-3.5 了解 `Match` 和 `FilterLabels` ViewStages，这些在 FiftyOne 中存在了一段时间，但其训练数据不包括最近添加的功能，其中
    `SortBySimilarity` ViewStage 可以用来找到与文本提示相似的图像。
- en: We tried passing in a definition of `SortBySimilarity`, details about its usage,
    and examples. We even tried instructing GPT-3.5 that it MUST NOT use the `Match`
    or `FilterLabels` ViewStages, or else it will be penalized. No matter what we
    tried, the LLM still oriented itself towards what it *knew,* whether it was the
    right choice or not. We were fighting against the LLM’s instincts!
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 我们尝试传入 `SortBySimilarity` 的定义、使用细节和示例。我们甚至尝试指示 GPT-3.5 绝对不能使用 `Match` 或 `FilterLabels`
    ViewStages，否则会受到惩罚。不管我们尝试什么，LLM 仍然倾向于其*已知的*内容，无论是否是正确的选择。我们在与 LLM 的本能作斗争！
- en: We ended up having to deal with this issue in post-processing.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我们最终不得不在后处理中处理这个问题。
- en: Painful Post-Processing Is Inevitable
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 痛苦的后处理是不可避免的
- en: No matter how good your examples are; no matter how strict your prompts are
    — large language models will invariably hallucinate, give you improperly formatted
    responses, and throw a tantrum when they don’t understand input information. The
    most predictable property of LLMs is the unpredictability of their outputs.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 无论你的示例有多好；无论你的提示有多严格——大型语言模型总会产生幻觉，给出格式错误的响应，当它们无法理解输入信息时，还会发脾气。LLM最可预测的特性就是其输出的不可预测性。
- en: I spent an ungodly amount of time writing routines to pattern match for and
    correct hallucinated syntax. The [post-processing file](https://github.com/voxel51/voxelgpt/blob/main/links/dataset_view_generator.py)
    ended up containing almost 1600 lines of Python code!
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 我花费了大量的时间编写例程来匹配模式并纠正幻觉语法。最终，[后处理文件](https://github.com/voxel51/voxelgpt/blob/main/links/dataset_view_generator.py)包含了近1600行Python代码！
- en: Some of these subroutines were as straightforward as adding parenthesis, or
    changing “and” and “or” to “&” and “|” in logical expressions. Some subroutines
    were far more involved, like validating the names of the entities in the LLM’s
    responses, converting one ViewStage to another if certain conditions were met,
    ensuring that the numbers and types of arguments to methods were valid.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 这些子例程中有些非常简单，比如添加括号，或在逻辑表达式中将“and”和“or”更改为“&”和“|”。有些子例程则复杂得多，比如验证LLM响应中的实体名称，如果满足某些条件，将一个ViewStage转换为另一个，确保方法的参数数量和类型有效。
- en: 'If you are using prompt engineering in a somewhat confined code generation
    context, I’d recommend the following approach:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你在一个相对封闭的代码生成环境中使用提示工程，我推荐以下方法：
- en: Write your own custom error parser using Abstract Syntax Trees (Python’s [ast](https://docs.python.org/3/library/ast.html)
    module).
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用抽象语法树（Python的[ast](https://docs.python.org/3/library/ast.html)模块）编写你自己的自定义错误解析器。
- en: If the results are syntactically invalid, feed the generated error message into
    your LLM and have it try again.
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果结果在语法上无效，将生成的错误消息输入到LLM中，让它再试一次。
- en: This approach fails to address the more insidious case where syntax is valid
    but the results are not right. If anyone has a good suggestion for this (beyond
    AutoGPT and “show your work” style approaches), please let me know!
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法未能解决更隐蔽的情况，即语法有效但结果不正确。如果有人对这个问题有好的建议（超出AutoGPT和“展示你的工作”风格的方法），请告诉我！
- en: Prompting Techniques
  id: totrans-49
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 提示技术
- en: The More the Merrier
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 人越多越好
- en: 'To build VoxelGPT, I used what seemed like every prompting technique under
    the sun:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 为了构建VoxelGPT，我使用了几乎所有的提示技术：
- en: “You are an expert”
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “你是一个专家”
- en: “Your task is”
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “你的任务是”
- en: “You MUST”
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “你必须”
- en: “You will be penalized”
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “你将会受到惩罚”
- en: “Here are the rules”
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “以下是规则”
- en: No combination of such phrases will *ensure* a certain type of behavior. Clever
    prompting just isn’t enough.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 任何这种短语的组合都无法*确保*某种行为。巧妙的提示是不够的。
- en: That being said, the more of these techniques you employ in a prompt, the more
    you nudge the LLM in the right direction!
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 话虽如此，你在提示中使用的这些技术越多，就越能将大型语言模型（LLM）引导到正确的方向！
- en: Examples > Documentation
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 示例 > 文档
- en: It is common knowledge by now (and common sense!) that both examples and other
    contextual information like documentation can help elicit better responses from
    a large language model. I found this to be the case for VoxelGPT.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 现在已经是常识（也是常识！），示例和其他上下文信息如文档可以帮助引导大型语言模型生成更好的响应。我发现VoxelGPT确实是这样。
- en: Once you add all of the directly pertinent examples and documentation though,
    what should you do if you have extra room in the context window? In my experience,
    I found that tangentially related examples mattered more than tangentially related
    documentation.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你添加了所有直接相关的示例和文档，如果上下文窗口中还有额外的空间，你应该怎么做？根据我的经验，我发现间接相关的示例比间接相关的文档更重要。
- en: Modularity >> Monolith
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模块化 >> 大一统
- en: The more you can break down an overarching problem into smaller subproblems,
    the better. Rather than feeding the dataset schema and a list of end-to-end examples,
    it is much more effective to identify individual selection and inference steps
    (selection-inference prompting), and feed in only the relevant information at
    each step.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 你将一个总体问题拆解成更小的子问题的程度越高，效果越好。与其提供数据集模式和端到端示例列表，不如识别单独的选择和推理步骤（选择-推理提示），并在每个步骤中仅提供相关信息。
- en: 'This is preferable for three reasons:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 这有三个优点：
- en: LLMs are better at doing one task at a time than multiple tasks at once.
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: LLM 在一次处理一个任务时表现得比同时处理多个任务更好。
- en: The smaller the steps, the easier to sanitize inputs and outputs.
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 步骤越小，清理输入和输出就越容易。
- en: It is an important exercise for you as the engineer to understand the logic
    of your application. The point of LLMs isn’t to make the world a black box. It
    is to enable new workflows.
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对你作为工程师来说，理解应用逻辑是一个重要的练习。LLM 的目的不是让世界变成黑匣子，而是启用新的工作流程。
- en: Examples
  id: totrans-68
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 例子
- en: How Many Do I Need?
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我需要多少个例子？
- en: A big part of prompt engineering is figuring out how many examples you need
    for a given task. This is *highly* problem specific.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 提示工程的一个重要部分是确定你需要多少个例子来完成特定任务。这是*高度*依赖于问题的。
- en: For some tasks ([effective query generation](https://github.com/voxel51/voxelgpt/blob/main/prompts/effective_prompt_generator_prefix.txt)
    and [answering questions based on the FiftyOne documentation](https://github.com/voxel51/voxelgpt/blob/main/prompts/docs_qa_template.txt)),
    we were able to get away without *any* examples. For others ([tag selection](https://github.com/voxel51/voxelgpt/blob/main/examples/tag_selection_examples.csv),
    [whether or not chat history is relevant](https://github.com/voxel51/voxelgpt/blob/main/examples/history_relevance_examples.csv),
    and [named entity recognition for label classes](https://github.com/voxel51/voxelgpt/blob/main/examples/label_class_examples.csv))
    we just needed a few examples to get the job done. Our main inference task, however,
    has almost 400 examples (and that is still the limiting factor in overall performance),
    so we only pass in the most relevant examples at inference time.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 对于一些任务（[有效查询生成](https://github.com/voxel51/voxelgpt/blob/main/prompts/effective_prompt_generator_prefix.txt)
    和 [根据 FiftyOne 文档回答问题](https://github.com/voxel51/voxelgpt/blob/main/prompts/docs_qa_template.txt)），我们能够做到没有*任何*例子。而对于其他任务（[标签选择](https://github.com/voxel51/voxelgpt/blob/main/examples/tag_selection_examples.csv)，[聊天记录是否相关](https://github.com/voxel51/voxelgpt/blob/main/examples/history_relevance_examples.csv)和
    [标签类命名实体识别](https://github.com/voxel51/voxelgpt/blob/main/examples/label_class_examples.csv)），我们只需要几个例子就能完成工作。然而，我们的主要推理任务几乎有400个例子（这仍然是整体性能的限制因素），因此我们仅在推理时传入最相关的例子。
- en: 'When you are generating examples, try to follow two guidelines:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 当你生成例子时，尝试遵循两个指南：
- en: Be as comprehensive as possible. If you have a finite space of possibilities,
    then try to give the LLM at least one example for each case. For VoxelGPT, we
    tried to have at the very least one example for each syntactically correct way
    of using each and every ViewStage — and typically a few examples for each, so
    the LLM can do pattern matching.
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 尽可能全面。如果你有一个有限的可能性空间，那么尝试为每种情况提供至少一个例子。对于 VoxelGPT，我们至少尝试为每种语法正确的 ViewStage
    使用方式提供一个例子——通常每种方式会有几个例子，以便 LLM 进行模式匹配。
- en: Be as consistent as possible. If you are breaking the task down into multiple
    subtasks, make sure the examples are consistent from one task to the next. You
    can reuse examples!
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 尽可能保持一致。如果你将任务拆分为多个子任务，确保例子在一个任务与下一个任务之间保持一致。你可以重用例子！
- en: Synthetic Examples
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 合成例子
- en: Generating examples is a laborious process, and handcrafted examples can only
    take you so far. It’s just not possible to think of every possible scenario ahead
    of time. When you deploy your application, you can log user queries and use these
    to improve your example set.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 生成例子是一个繁琐的过程，手工制作的例子只能做到有限程度。事先想到每种可能的情境几乎是不可能的。当你部署应用时，可以记录用户查询，并利用这些查询来改进你的例子集。
- en: Prior to deployment, however, your best bet might be to generate *synthetic*
    examples.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在部署之前，你最好的选择可能是生成*合成*例子。
- en: 'Here are two approaches to generating synthetic examples that you might find
    helpful:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有两种生成合成例子的方式，你可能会发现有用：
- en: Use an LLM to generate examples. You can ask the LLM to vary its language, or
    even imitate the style of potential users! This didn’t work for us, but I’m convinced
    it could work for many applications.
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 LLM 生成例子。你可以让 LLM 变换语言，甚至模仿潜在用户的风格！虽然这对我们没用，但我相信它对许多应用可能有效。
- en: Programmatically generate examples — potentially with randomness — based on
    elements in the input query itself. For VoxelGPT, this means generating examples
    based on the fields in the user’s dataset. We are in the process of incorporating
    this into our pipeline, and the results we’ve seen so far have been promising.
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 基于输入查询中的元素程序性地生成示例——可能带有随机性。对于 VoxelGPT，这意味着基于用户数据集中的字段生成示例。我们正在将其纳入我们的管道中，目前为止看到的结果很有希望。
- en: Tooling
  id: totrans-81
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 工具
- en: LangChain
  id: totrans-82
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: LangChain
- en: 'LangChain is popular for a reason: the library makes it easy to connect LLM
    inputs and outputs in complex ways, abstracting away the gory details. The Models
    and Prompts modules especially are top notch.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: LangChain 之所以受欢迎是有原因的：这个库使得以复杂的方式连接 LLM 的输入和输出变得简单，抽象掉了繁琐的细节。特别是模型和提示模块表现出色。
- en: 'That being said, LangChain is definitely a work in progress: their Memories,
    Indexes, and Chains modules all have significant limitations. Here are just a
    few of the issues I encountered when trying to use LangChain:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 话虽如此，LangChain 绝对是一个不断进步的项目：他们的记忆、索引和链模块都有显著的限制。这只是我在尝试使用 LangChain 时遇到的一些问题。
- en: 'Document Loaders and Text Splitters: In LangChain, [Document Loaders](https://python.langchain.com/en/latest/modules/indexes/document_loaders.html)
    are supposed to transform data from different file formats into text, and [Text
    Splitters](https://python.langchain.com/en/latest/modules/indexes/text_splitters.html)
    are supposed to split text into semantically meaningful chunks. VoxelGPT answers
    questions about the FiftyOne documentation by retrieving the most relevant chunks
    of the docs and piping them into a prompt. In order to generate meaningful answers
    to questions about the FiftyOne docs, I had to effectively build custom loaders
    and splitters, because LangChain didn’t provide the appropriate flexibility.'
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 文档加载器和文本拆分器：在 LangChain 中，[文档加载器](https://python.langchain.com/en/latest/modules/indexes/document_loaders.html)
    应将不同文件格式的数据转换为文本，而 [文本拆分器](https://python.langchain.com/en/latest/modules/indexes/text_splitters.html)
    则应将文本拆分成语义上有意义的块。VoxelGPT 通过检索文档中最相关的块并将其传入提示中来回答有关 FiftyOne 文档的问题。为了生成有意义的答案，我不得不有效地构建自定义加载器和拆分器，因为
    LangChain 没有提供适当的灵活性。
- en: 'Vectorstores: LangChain offers [Vectorstore](https://python.langchain.com/en/latest/modules/indexes/vectorstores.html)
    integrations and Vectorstore-based [Retrievers](https://python.langchain.com/en/latest/modules/indexes/retrievers.html)
    to help find relevant information to incorporate into LLM prompts. This is great
    in theory, but the implementations are lacking in flexibility. I had to write
    a custom implementation with ChromaDB in order to pass embedding vectors ahead
    of time and not have them recomputed every time I ran the application. I also
    had to write a custom retriever to implement the custom pre-filtering I needed.'
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 向量存储：LangChain 提供了 [Vectorstore](https://python.langchain.com/en/latest/modules/indexes/vectorstores.html)
    集成和基于 Vectorstore 的 [检索器](https://python.langchain.com/en/latest/modules/indexes/retrievers.html)，以帮助找到相关信息并纳入
    LLM 提示中。这在理论上很棒，但实现缺乏灵活性。我不得不使用 ChromaDB 编写自定义实现，以便预先传递嵌入向量，而不是每次运行应用程序时都重新计算它们。我还不得不编写自定义检索器，以实现所需的自定义预筛选。
- en: 'Question Answering with Sources: When building out question answering over
    the FiftyOne docs, I arrived at a reasonable solution utilizing LangChain’s `RetrievalQA`
    Chain. When I wanted to add sources in, I thought it would be as straightforward
    as swapping out that chain for LangChain’s `RetrievalQAWithSourcesChain`. However,
    bad prompting techniques meant that this chain exhibited some unfortunate behavior,
    such as [hallucinating about Michael Jackson](https://github.com/hwchase17/langchain/issues/2510).
    Once again, I had to [take matters into my own hands](https://github.com/voxel51/voxelgpt/blob/main/links/utils.py).'
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 带来源的问答：在构建对 FiftyOne 文档的问答时，我找到了一种合理的解决方案，利用了 LangChain 的 `RetrievalQA` Chain。当我想添加来源时，我以为只需将该链替换为
    LangChain 的 `RetrievalQAWithSourcesChain` 就可以了。然而，不良的提示技术导致该链表现出一些不幸的行为，例如 [关于迈克尔·杰克逊的虚假信息](https://github.com/hwchase17/langchain/issues/2510)。我再次不得不
    [自己动手](https://github.com/voxel51/voxelgpt/blob/main/links/utils.py)。
- en: What does all of this mean? It may be easier to just build the components yourself!
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着什么呢？也许自己构建这些组件更简单！
- en: Vector Databases
  id: totrans-89
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 向量数据库
- en: Vector search may be on 🔥🔥🔥, but that doesn’t mean you NEED it for your project.
    I initially implemented our similar example retrieval routine using ChromaDB,
    but because we only had hundreds of examples, I ended up switching to an exact
    nearest neighbor search. I did need to deal with all of the metadata filtering
    myself, but the result was a faster routine with fewer dependencies.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: TikToken
  id: totrans-91
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Adding [TikToken](https://github.com/openai/tiktoken) into the equation was
    incredibly easy. In total, TikToken added <10 lines of code to the project, but
    allowed us to be much more precise when counting tokens and trying to fit as much
    information as possible into the context length. This is the only true no-brainer
    when it comes to tooling.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  id: totrans-93
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There are tons of LLMs to choose from, lots of shiny new tools, and a bunch
    of “prompt engineering” techniques. All of this can be both exciting and overwhelming.
    The key to building an application with prompt engineering is to:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: Break the problem down; build the solution up
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Treat LLMs as *enablers*, not as end-to-end solutions
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Only use tools when they make your life easier
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Embrace experimentation!
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Go build something cool!
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
