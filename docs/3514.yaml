- en: LMQL — SQL for Language Models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/lmql-sql-for-language-models-d7486d88c541?source=collection_archive---------0-----------------------#2023-11-27](https://towardsdatascience.com/lmql-sql-for-language-models-d7486d88c541?source=collection_archive---------0-----------------------#2023-11-27)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Yet another tool that could help you with LLM applications
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://miptgirl.medium.com/?source=post_page-----d7486d88c541--------------------------------)[![Mariya
    Mansurova](../Images/b1dd377b0a1887db900cc5108bca8ea8.png)](https://miptgirl.medium.com/?source=post_page-----d7486d88c541--------------------------------)[](https://towardsdatascience.com/?source=post_page-----d7486d88c541--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----d7486d88c541--------------------------------)
    [Mariya Mansurova](https://miptgirl.medium.com/?source=post_page-----d7486d88c541--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F15a29a4fc6ad&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flmql-sql-for-language-models-d7486d88c541&user=Mariya+Mansurova&userId=15a29a4fc6ad&source=post_page-15a29a4fc6ad----d7486d88c541---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----d7486d88c541--------------------------------)
    ·17 min read·Nov 27, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fd7486d88c541&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flmql-sql-for-language-models-d7486d88c541&user=Mariya+Mansurova&userId=15a29a4fc6ad&source=-----d7486d88c541---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fd7486d88c541&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flmql-sql-for-language-models-d7486d88c541&source=-----d7486d88c541---------------------bookmark_footer-----------)![](../Images/a3ba043a5a4b6836f2488f4c4b6759e7.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Image by DALL-E 3
  prefs: []
  type: TYPE_NORMAL
- en: I’m sure you’ve heard about SQL or even have mastered it. **SQL (Structured
    Query Language)** is a declarative language widely used to work with database
    data.
  prefs: []
  type: TYPE_NORMAL
- en: According to the annual [StackOverflow survey](https://survey.stackoverflow.co/2023/#technology-most-popular-technologies),
    SQL is still one of the most popular languages in the world. For professional
    developers, SQL is in the top-3 languages (after Javascript and HTML/CSS). More
    than a half of professionals use it. Surprisingly, SQL is even more popular than
    Python.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/72a34ba2e51238a06a33f6289c2d0bef.png)'
  prefs: []
  type: TYPE_IMG
- en: Graph by author, data from [StackOverflow survey](https://survey.stackoverflow.co/2023/#technology-most-popular-technologies)
  prefs: []
  type: TYPE_NORMAL
- en: SQL is a common way to talk to your data in a database. So, it is no surprise
    that there are attempts to use a similar approach for LLMs. In this article, I
    would like to tell you about one such approach called LMQL.
  prefs: []
  type: TYPE_NORMAL
- en: What is LMQL?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[**LMQL**](https://lmql.ai/) **(Language Model Query Language)** is an open-source
    programming language for language models. LMQL is released under [Apache 2.0 license](https://github.com/eth-sri/lmql/blob/main/LICENSE),
    which allows you to use it commercially.'
  prefs: []
  type: TYPE_NORMAL
- en: 'LMQL was developed by ETH Zurich researchers. They proposed a novel idea of
    LMP (Language Model Programming). LMP combines natural and programming languages:
    text prompt and scripting instructions.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In [the original paper](https://arxiv.org/abs/2212.06094), *“Prompting Is Programming:
    A Query Language for Large Language Models” by Luca Beurer-Kellner, Marc Fischer
    and Martin Vechev*, the authors flagged the following challenges of the current
    LLM usage:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Interaction.** For example, we could use meta prompting, asking LM to expand
    the initial prompt. As a practical case, we could first ask the model to define
    the language of the initial question and then respond in that language. For such
    a task, we will need to send the first prompt, extract language from the output,
    add it to the second prompt template and make another call to the LM. There’s
    quite a lot of interactions we need to manage. With LMQL, you can define multiple
    input and output variables within one prompt. More than that, LMQL will optimise
    overall likelihood across numerous calls, which might yield better results.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Constraint & token representation.** The current LMs don’t provide the functionality
    to constrain output, which is crucial if we use LMs in production. Imagine building
    a sentiment analysis in production to mark negative reviews in our interface for
    CS agents. Our program would expect to receive from the LLM “positive”, “negative”,
    or “neutral”. However, quite often, you could get something like “The sentiment
    for provided customer review is positive” from the LLM, which is not so easy to
    process in your API. That’s why constraints would be pretty helpful. LMQL allows
    you to control output using human-understandable words (not tokens that LMs operate
    with).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Efficiency and cost.** LLMs are large networks, so they are pretty expensive,
    regardless of whether you use them via API or in your local environment. LMQL
    can leverage predefined behaviour and the constraint of the search space (introduced
    by constraints) to reduce the number of LM invoke calls.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As you can see, LMQL can address these challenges. It allows you to combine
    multiple calls in one prompt, control your output and even reduce cost.
  prefs: []
  type: TYPE_NORMAL
- en: The impact on cost and efficiency could be pretty substantial. The limitations
    to the search space can significantly reduce costs for LLMs. For example, in the
    cases from [the LMQL paper](https://arxiv.org/abs/2212.06094), there were 75–85%
    fewer billable tokens with LMQL compared to standard decoding, which means it
    will significantly reduce your cost.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4767e98e5ab6f4ff82b8566c9e8f8a4f.png)'
  prefs: []
  type: TYPE_IMG
- en: Image from [the paper by Beurer-Kellner et al. (2023)](https://arxiv.org/abs/2212.06094)
  prefs: []
  type: TYPE_NORMAL
- en: 'I believe the most crucial benefit of LMQL is the complete control of your
    output. However, with such an approach, you will also have another layer of abstraction
    over LLM (similar to LangChain, which we discussed earlier). It will allow you
    to switch from one backend to another easily if you need to. LMQL can work with
    different backends: OpenAI, HuggingFace Transformers or `llama.cpp`.'
  prefs: []
  type: TYPE_NORMAL
- en: You can install LMQL locally or use a web-based [Playground](https://lmql.ai/playground/)
    online. Playground can be pretty handy for debugging, but you can only use the
    OpenAI backend here. For all other use cases, you will have to use local installation.
  prefs: []
  type: TYPE_NORMAL
- en: 'As usual, there are some limitations to this approach:'
  prefs: []
  type: TYPE_NORMAL
- en: This library is not very popular yet, so the community is pretty small, and
    few external materials are available.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In some cases, documentation might not be very detailed.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The most popular and best-performing OpenAI models have [some limitations](https://lmql.ai/docs/models/openai.html#openai-api-limitations),
    so you can’t use the full power of LMQL with ChatGPT.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: I wouldn’t use LMQL in production since I can’t say that it’s a mature project.
    For example, distribution over tokens provides pretty poor accuracy.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Somewhat close alternative to LMQL is [Guidance](https://github.com/guidance-ai/guidance).
    It also allows you to constrain generation and control the LM’s output.
  prefs: []
  type: TYPE_NORMAL
- en: Despite all the limitations, I like the concept of Language Model Programming,
    and that’s why I’ve decided to discuss it in this article.
  prefs: []
  type: TYPE_NORMAL
- en: If you’re interested to learn more about LMQL from its authors, check [this
    video](https://www.youtube.com/watch?v=4StBzmb6OH0).
  prefs: []
  type: TYPE_NORMAL
- en: LMQL syntax
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now, we know a bit what LMQL is. Let’s look at the example of an LMQL query
    to get acquainted with its syntax.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: I hope you can guess its meaning. But let’s discuss it in detail.
  prefs: []
  type: TYPE_NORMAL
- en: Here’s a scheme for a LMQL query
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/adc58c40f3c91b3e4b8fe02965607dfa.png)'
  prefs: []
  type: TYPE_IMG
- en: Image from [paper by Beurer-Kellner et al. (2023)](https://arxiv.org/abs/2212.06094)
  prefs: []
  type: TYPE_NORMAL
- en: 'Any LMQL program consists of 5 parts:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Decoder` defines the decoding procedure used. In simple words, it describes
    the algorithm to pick up the next token. LMQL has three different types of decoders:
    argmax, beam and sample. You can learn about them in more detail from [the paper.](https://arxiv.org/pdf/2212.06094.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Actual query is similar to the classic prompt but in Python syntax, which means
    that you could use such structures as loops or if-statements.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In `from` clause, we specified the model to use (`openai/text-davinci-003` in
    our example).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Where` clause defines constraints.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Distribution` is used when you want to see probabilities for tokens in the
    return. We haven’t used distribution in this query, but we will use it to get
    class probabilities for the sentiment analysis later.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Also, you might have noticed special variables in our query `{name}` and `[RESPONSE]`.
    Let’s discuss how they work:'
  prefs: []
  type: TYPE_NORMAL
- en: '`{name}` is an input parameter. It could be any variable from your scope. Such
    parameters help you create handy functions that could be easily re-used for different
    inputs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`[RESPONSE]` is a phrase that LM will generate. It can also be called a hole
    or placeholder. All the text before `[RESPONSE]` is sent to LM, and then the model’s
    output is assigned to the variable. It’s handy that you could easily re-use this
    output later in the prompt, referring to it as `{RESPONSE}`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We’ve briefly covered the main concepts. Let’s try it ourselves. Practice makes
    perfect.
  prefs: []
  type: TYPE_NORMAL
- en: Getting started
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Setting up environment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: First of all, we need to set up our environment. To use LMQL in Python, we need
    to install a package first. No surprises, we can just use pip. You need an environment
    with Python ≥ 3.10.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: If you want to use LMQL with local GPU, follow the instructions in [the documentation](https://lmql.ai/docs/installation.html).
  prefs: []
  type: TYPE_NORMAL
- en: To use OpenAI models, you need to set up APIKey to access OpenAI. The easiest
    way is to specify the `OPENAI_API_KEY` environment variable.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: However, OpenAI models have many [limitations](https://lmql.ai/docs/models/openai.html#openai-api-limitations)
    (for example, you won’t be able to get distributions with more than five classes).
    So, we will use Llama.cpp to test LMQL with local models.
  prefs: []
  type: TYPE_NORMAL
- en: First, you need to install Python binding for Llama.cpp in the same environment
    as LMQL.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: If you want to use local GPU, specify the following parameters.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Then, we need to load model weights as `.gguf` files. You can find models on
    [HuggingFace Models Hub](https://huggingface.co/models).
  prefs: []
  type: TYPE_NORMAL
- en: 'We will be using two models:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Llama-2-7B` ([link](https://huggingface.co/TheBloke/Llama-2-7B-GGUF))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`zephyr-7B-beta` ([link](https://huggingface.co/TheBloke/zephyr-7B-beta-GGUF))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Llama-2–7B is the smallest version of fine-tuned generative text models by Meta.
    It’s a pretty basic model, so we shouldn’t expect outstanding performance from
    it.
  prefs: []
  type: TYPE_NORMAL
- en: Zephyr is a fine-tuned version of the [Mistral](https://huggingface.co/mistralai/Mistral-7B-v0.1)
    model with decent performance. It performs better in some aspects than a 10x larger
    open-source model Llama-2–70b. However, there’s still some gap between Zephyr
    and proprietary models like ChatGPT or Claude.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ad8a00d37526988daaa9ea0e0f63cb1d.png)'
  prefs: []
  type: TYPE_IMG
- en: Image from the paper by [Tunstall et al. (2023)](https://arxiv.org/abs/2310.16944)
  prefs: []
  type: TYPE_NORMAL
- en: According to the [LMSYS ChatBot Arena leaderboard](https://huggingface.co/spaces/lmsys/chatbot-arena-leaderboard),
    Zephyr is the best-performing model with 7B parameters. It’s on par with much
    bigger models.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/879e0f123b15262e8fdd22139726d698.png)'
  prefs: []
  type: TYPE_IMG
- en: Screenshot of leaderboard | [source](https://huggingface.co/spaces/lmsys/chatbot-arena-leaderboard)
  prefs: []
  type: TYPE_NORMAL
- en: Let’s load `.gguf` files for our models.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: We need to download a few GBs so that it might take some time (10–15 minutes
    for each model). Luckily, you need to do it only once.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'You can interact with the local models in two different ways ([documentation](https://lmql.ai/docs/models/hf.html)):'
  prefs: []
  type: TYPE_NORMAL
- en: Two-process architecture when you have a separate long-running process with
    your model and short-running inference calls. This approach is more suitable for
    production.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For ad-hoc tasks, we could use in-process model loading, specifying `local:`
    before the model name. We will be using this approach to work with the local models.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now, we’ve set up the environment, and it’s time to discuss how to use LMQL
    from Python.
  prefs: []
  type: TYPE_NORMAL
- en: Python functions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s briefly discuss how to use LMQL in Python. Playground can be handy for
    debugging, but if you want to use LM in production, you need an API.
  prefs: []
  type: TYPE_NORMAL
- en: 'LMQL provides four main approaches to its functionality: `lmql.F` , `lmql.run`
    , `@lmql.query` decorator and [Generations API](https://lmql.ai/docs/lib/generations.html).'
  prefs: []
  type: TYPE_NORMAL
- en: '[Generations API](https://lmql.ai/docs/lib/generations.html) has been recently
    added. It’s a simple Python API that helps to do inference without writing LMQL
    yourself. Since I am more interested in the LMP concept, we won’t cover this API
    in this article.'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s discuss the other three approaches in detail and try to use them.
  prefs: []
  type: TYPE_NORMAL
- en: First, you could use `lmql.F`. It’s a lightweight functionality similar to lambda
    functions in Python that could allow you to execute part of LMQL code. `lmql.F`
    can have only one placeholder variable that will be returned from the lambda function.
  prefs: []
  type: TYPE_NORMAL
- en: We could specify both prompt and constraint for the function. The constraint
    will be equivalent to the `where` clause in the LMQL query.
  prefs: []
  type: TYPE_NORMAL
- en: Since we haven’t specified any model, the OpenAI `text-davinci` will be used.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: If you’re using Jupyter Notebooks, you might encounter some problems since Notebooks
    environments are asynchronous. You could enable nested event loops in your notebook
    to avoid such issues.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: The second approach allows you to define more complex queries. You can use `lmql.run`
    to execute an LMQL query without creating a function. Let’s make our query a bit
    more complicated and use the answer from the model in the following question.
  prefs: []
  type: TYPE_NORMAL
- en: In this case, we’ve defined constraints in the `where` clause of the query string
    itself.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Also, I’ve used `run_sync` instead of `run` to get a result synchronously.
  prefs: []
  type: TYPE_NORMAL
- en: 'As a result, we got an `LMQLResult` object with a set of fields:'
  prefs: []
  type: TYPE_NORMAL
- en: '`prompt` — include the whole prompt with the parameters and the model’s answers.
    We could see that the model answer was used for the second question.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`variables` — dictionary with all the variables we defined: `ANSWER` and `CAPITAL`
    .'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`distribution_variable` and `distribution_values` are `None` since we haven’t
    used this functionality.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/071065b1358f774d779849cc09cbc095.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: The third way to use Python API is the `[@lmql](http://twitter.com/lmql).query`
    decorator, which allows you to define a Python function that will be handy to
    use in the future. It’s more convenient if you plan to call this prompt several
    times.
  prefs: []
  type: TYPE_NORMAL
- en: We could create a function for our previous query and get only the final answer
    instead of returning the whole `LMQLResult` object.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Also, you could use LMQL in combination with LangChain:'
  prefs: []
  type: TYPE_NORMAL
- en: LMQL queries are Prompt Templates on steroids and could be part of LangChain
    chains.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You could leverage LangChain components from LMQL (for example, retrieval).
    You can find examples in [the documentation](https://lmql.ai/docs/lib/integrations/langchain.html).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now, we know all the basics of LMQL syntax, and we are ready to move on to our
    task — to define sentiment for customer comments.
  prefs: []
  type: TYPE_NORMAL
- en: Sentiment Analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To see how LMQL is performing, we will use labelled Yelp reviews from the [UCI
    Machine Learning Repository](https://archive.ics.uci.edu/dataset/331/sentiment+labelled+sentences)
    and try to predict sentiment. All reviews in the dataset are positive or negative,
    but we will keep neutral as one of the possible options for classification.
  prefs: []
  type: TYPE_NORMAL
- en: For this task, let’s use local models — `Zephyr` and `Llama-2`. To use them
    in LMQL, we need to specify the model and tokeniser when we are calling LMQL.
    For Llama-family models, we can use the default tokeniser.
  prefs: []
  type: TYPE_NORMAL
- en: First attempts
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s pick one customer review `The food was very good.` and try to define its
    sentiment. We will use `lmql.run` for debugging since it’s convenient for such
    ad-hoc calls.
  prefs: []
  type: TYPE_NORMAL
- en: I’ve started with a very naive approach.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]The food was very good.[PRE11]'
  prefs: []
  type: TYPE_NORMAL
- en: If your local model works exceptionally slowly, check whether your computer
    uses swap memory. Restart could be an excellent option to solve it.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The code looks absolutely straightforward. Surprisingly, however, it doesn’t
    work and returns the following error.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: From the message, we can guess that the output doesn’t fit the context size.
    Our prompt is about 20 tokens. So, it’s a bit weird that we’ve hit the threshold
    on the context size. Let’s try to constrain the number of tokens for `SENTIMENT`
    and see the output.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]The food was very good.[PRE14]The service was terrible.[PRE15]The hotel
    was amazing, the staff were friendly and the location was perfect.[PRE16]The product
    was a complete disappointment.[PRE17]The flight was delayed for 3 hours, the food
    was cold and the entertainment system didn''t work.[PRE18]The restaurant was packed,
    but the waiter was efficient and the food was delicious.[PRE19]'
  prefs: []
  type: TYPE_NORMAL
- en: Now, we could see the root cause of the problem — the model was stuck in a cycle,
    repeating the question variations and answers again and again. I haven’t seen
    such issues with OpenAI models (suppose they might control it), but they are pretty
    standard to open-source local models. We could use the `STOPS_AT` constraint to
    stop generation if we see `Q:` or a new line in the model response to avoid such
    cycles.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]The food was very good.[PRE21]'
  prefs: []
  type: TYPE_NORMAL
- en: 'Excellent, we’ve solved the issue and got the result. But since we will do
    classification, we would like the model to return one of the three outputs (class
    labels): `negative`, `neutral` or `positive`. We could add such a filter to the
    LMQL query to constrain the output.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]The food was very good.[PRE23]'
  prefs: []
  type: TYPE_NORMAL
- en: We don’t need filters with stopping criteria since we are already limiting output
    to just three possible options, and LMQL doesn’t look at any other possibilities.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s try to use the chain of thoughts reasoning approach. Giving the model
    some time to think usually improves the results. Using LMQL syntax, we could quickly
    implement this approach.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]The food was very good.[PRE25]'
  prefs: []
  type: TYPE_NORMAL
- en: The output from the Zephyr model is pretty decent.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/cebf97ad58f3d16d2b466e85c7e8171d.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: We can try the same prompt with Llama 2.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]The food was very good.[PRE27]'
  prefs: []
  type: TYPE_NORMAL
- en: The reasoning doesn’t make much sense. We’ve already seen on the Leaderboard
    that the Zephyr model is much better than Llama-2–7b.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6bc197c1f46bfd75046211535bc79a28.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: In classical Machine Learning, we usually get not only class labels but also
    their probability. We could get the same data using `distribution` in LMQL. We
    just need to specify the variable and possible values — `distribution SENTIMENT
    in [‘positive’, ‘negative’, ‘neutral’]`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]The food was very good.[PRE29]'
  prefs: []
  type: TYPE_NORMAL
- en: Now, we got probabilities in the output, and we could see that the model is
    quite confident in the positive sentiment.
  prefs: []
  type: TYPE_NORMAL
- en: Probabilities could be helpful in practice if you want to use only decisions
    when the model is confident.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7b8932e91d9ac23440ea5bd4066ac404.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s create a function to use our sentiment analysis for various inputs.
    It would be interesting to compare results with and without distribution, so we
    need two functions.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]{review}[PRE31]{review}[PRE32]'
  prefs: []
  type: TYPE_NORMAL
- en: Then, we could use this function for the new review.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: The model decided that it was neutral.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/60dead54440461f0e49fc8a134f63882.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: There’s a rationale behind this conclusion, but I would say this review is negative.
    Let’s see whether we could use other decoders and get better results.
  prefs: []
  type: TYPE_NORMAL
- en: 'By default, the `argmax` decoder is used. It’s the most straightforward approach:
    at each step, the model selects the token with the highest probability. We could
    try to play with other options.'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s try to use [the beam search](https://en.wikipedia.org/wiki/Beam_search)
    approach with `n = 3` and a pretty high `tempreture = 0.8`. As a result, we would
    get three sequences sorted by likelihood, so we could just get the first one (with
    the highest likelihood).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: Now, the model was able to spot the negative sentiment in this review.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/bab33e0feb447f962c2c333260c8b447.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: 'It’s worth saying that there’s a cost for beam search decoding. Since we are
    working on three sequences (beams), getting an LLM result takes 3 times more time
    on average: 39.55 secs vs 13.15 secs.'
  prefs: []
  type: TYPE_NORMAL
- en: Now, we have our functions and can test them with our real data.
  prefs: []
  type: TYPE_NORMAL
- en: Results on real-life data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'I’ve run all the functions on a 10% sample of the 1K dataset of Yelp reviews
    with different parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '**models**: Llama 2 or Zephyr,'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**approach**: using distribution or just constrained prompt,'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**decoders**: argmax or beam search.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: First, let’s compare accuracy — share of reviews with correct sentiment. We
    can see that Zephyr performs much better than the Llama 2 model. Also, for some
    reason, we get significantly poorer quality with distributions.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/20556862454d7b6648fa4bfcba8909ec.png)'
  prefs: []
  type: TYPE_IMG
- en: Graph by author
  prefs: []
  type: TYPE_NORMAL
- en: 'If we look a bit deeper, we could notice:'
  prefs: []
  type: TYPE_NORMAL
- en: For positive reviews, accuracy is usually higher.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The most common error is marking the review as neutral,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For Llama 2 with prompt, we could see a high rate of critical issues (positive
    comments that were labelled as negatives).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In many cases, I suppose the model uses a similar rationale, scoring negative
    comments as neutral as we’ve seen earlier with the “dirty room” example. The model
    is unsure whether “dirty room” has a negative or neutral sentiment since we don’t
    know whether the customer expected a clean room.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/61f60d4457987a2e4cc6f88f0d0bddbe.png)'
  prefs: []
  type: TYPE_IMG
- en: Graph by author
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/236e21eb76dca04e4f8ff40a58ef5972.png)'
  prefs: []
  type: TYPE_IMG
- en: Graph by author
  prefs: []
  type: TYPE_NORMAL
- en: 'It’s also interesting to look at actual probabilities:'
  prefs: []
  type: TYPE_NORMAL
- en: 75% percentile of positive labels for positive comments is above 0.85 for the
    Zephyr model, while it is way lower for Llama 2.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All models show poor performance for negative comments, where the 75% percentile
    for negative labels for negative comments is way below even 0.5.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/9fda378f7b359e789f197d5b33309843.png)'
  prefs: []
  type: TYPE_IMG
- en: Graph by author
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ae745cfa031e4a2d4f0d4fafa41694f7.png)'
  prefs: []
  type: TYPE_IMG
- en: Graph by author
  prefs: []
  type: TYPE_NORMAL
- en: Our quick research shows that a vanilla prompt with a Zephyr model and `argmax`
    decoder would be the best option for sentiment analysis. However, it’s worth checking
    different approaches for your use case. Also, you could often achieve better results
    by tweaking prompts.
  prefs: []
  type: TYPE_NORMAL
- en: You can find the full code on [GitHub](https://github.com/miptgirl/miptgirl_medium/blob/main/lmql_intro/lmql_intro.ipynb).
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Today, we’ve discussed a concept of LMP (Language Model Programming) that allows
    you to mix prompts in natural language and scripting instructions. We’ve tried
    using it for sentiment analysis tasks and got decent results using local open-source
    models.
  prefs: []
  type: TYPE_NORMAL
- en: Even though LMQL is not widespread yet, this approach might be handy and gain
    popularity in the future since it combines natural and programming languages into
    a powerful tool for LMs.
  prefs: []
  type: TYPE_NORMAL
- en: Thank you a lot for reading this article. I hope it was insightful to you. If
    you have any follow-up questions or comments, please leave them in the comments
    section.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Kotzias,Dimitrios. (2015). Sentiment Labelled Sentences. UCI Machine Learning
    Repository (CC BY 4.0 license).* [*https://doi.org/10.24432/C57604*](https://doi.org/10.24432/C57604.)'
  prefs: []
  type: TYPE_NORMAL
