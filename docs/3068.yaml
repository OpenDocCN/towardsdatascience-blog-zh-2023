- en: Mastering data integration from SAP Systems with prompt engineering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/mastering-sap-data-integration-tasks-with-prompt-engineering-4cb03a57463a?source=collection_archive---------13-----------------------#2023-10-07](https://towardsdatascience.com/mastering-sap-data-integration-tasks-with-prompt-engineering-4cb03a57463a?source=collection_archive---------13-----------------------#2023-10-07)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[](https://medium.com/@markus.stadi?source=post_page-----4cb03a57463a--------------------------------)[![Markus
    Stadi](../Images/c3b1c3b040370d4e24e406b172a830d5.png)](https://medium.com/@markus.stadi?source=post_page-----4cb03a57463a--------------------------------)[](https://towardsdatascience.com/?source=post_page-----4cb03a57463a--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----4cb03a57463a--------------------------------)
    [Markus Stadi](https://medium.com/@markus.stadi?source=post_page-----4cb03a57463a--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F88759409b50c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmastering-sap-data-integration-tasks-with-prompt-engineering-4cb03a57463a&user=Markus+Stadi&userId=88759409b50c&source=post_page-88759409b50c----4cb03a57463a---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----4cb03a57463a--------------------------------)
    ·8 min read·Oct 7, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F4cb03a57463a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmastering-sap-data-integration-tasks-with-prompt-engineering-4cb03a57463a&user=Markus+Stadi&userId=88759409b50c&source=-----4cb03a57463a---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4cb03a57463a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmastering-sap-data-integration-tasks-with-prompt-engineering-4cb03a57463a&source=-----4cb03a57463a---------------------bookmark_footer-----------)![](../Images/21e67a7c882aeffe929ead60333342e1.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Construction engineer investigating his work — Stable diffusion
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In our previous publication, [From Data Engineering to Prompt Engineering](https://medium.com/towards-data-science/from-data-engineering-to-prompt-engineering-5debd1c636e0),
    we demonstrated how to utilize ChatGPT to solve data preparation tasks. Apart
    from the good feedback we have received, one critical point has been raised: Prompt
    engineering may help with simple tasks, but is it really useful in a more challenging
    environment? This is a fair point. In recent decades, data architectures have
    grown increasingly diverse and complex. As a result of this complexity, data engineers
    more and more have to integrate a variety of data sources they are not necessarily
    familiar with. Can prompt engineering help in this context?'
  prefs: []
  type: TYPE_NORMAL
- en: This article examines the question based on a real use case from human resources
    management. We apply few shot learning to introduce an SAP HCM data model to ChatGPT
    and analyze the collected information with Apache Spark. This way, we illustrate
    how prompt engineering can deliver value even in advanced data engineering settings.
  prefs: []
  type: TYPE_NORMAL
- en: About the business case
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A common task every medium to large company has to accomplish, is to determine
    the number of its employees and their organizational assignment for any given
    point in time. The associated data in our scenario is stored in a SAP HCM system
    which is one of the leading applications for human resource management in enterprise
    environments.
  prefs: []
  type: TYPE_NORMAL
- en: To solve this kind of objective, every data engineer needs to build up a lot
    of business related knowledge which is strongly interdependent to the underlying
    data model.
  prefs: []
  type: TYPE_NORMAL
- en: This article will provide a step by step guide to solve the described business
    problem by creating PySpark code that can be used to build the data model and
    consequently the basis for any reporting solution.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e474925308bd9e8f426d6c767fef7069.png)'
  prefs: []
  type: TYPE_IMG
- en: PowerBi Example report showing personnel headcount
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 1: Determine which information is needed'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'One of the main challenges in data science is to select the necessary information
    according to the business use case and to determine its origin in the source systems.
    To solve this we have to bring in some business knowledge to chatgpt. For this
    purpose we teach chatgpt some information on SAP HCM basic tables which can be
    found in SAP reference manual: [Human Resources | SAP Help Portal](https://help.sap.com/docs/HR_RENEWAL/4946a4f5c2d7427c96d89242e1ff2d9a/48d5d5537c004308e10000000a174cb4.html?locale=en-US)
    combining it with a csv-Sample record for each table.'
  prefs: []
  type: TYPE_NORMAL
- en: In this first scenario, our intention is to report all active Employees at a
    specific point in time. The result should also include the employees personal
    number, name, status and organizational assignment.
  prefs: []
  type: TYPE_NORMAL
- en: To gather all the necessary information we need to infere a Database Schema
    to ChatGPT including example datasets and field descriptions by using few-shot
    prompting. We will start out propagating the Database Schema and some example
    data to ChatGPT.
  prefs: []
  type: TYPE_NORMAL
- en: Everyone who knows SAP HCMs Data Model should be familiar with the concept of
    infotypes and transparent tables. The infotype contains all the transactional
    information whereas the transparent tables contain the business information (masterdata)
    of each entity.
  prefs: []
  type: TYPE_NORMAL
- en: For the following scenario we will be using *OpenAIs GPT-4* to create the code
    we need. Lets start by providing the basic table information to ChatGPT.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Step 2:Join the necessary base tables and filter active employees only
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now lets create the code to join the base tables:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'This will produce pretty decent and well formatted PySpark code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Notice GPT-4 recognized the join criteria for both tables which is based on
    Column STAT2 of Table PA0000 and column STATV of table T529U which is the corresponding
    transparent table. Over that the created code contains the business descriptions
    as column aliases to improve its readability.
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 3: Build a Timeline to reflect the companies employees history'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now we will create a DataFrame that contains date values for the period starting
    from 2020–01–01 until 2024–01–01 and join all valid employees according to their
    entry date (BEGDA) and possible exit date (ENDDA) we need to create an artificial
    timeline to join the employees to.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Step 4: Dissolving a parent-child relationship table by determining the highest
    level organizational object'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this last step we want to build a DataFrame that represents the organizational
    structure of the company and determine each objects organizational assignment.
    Specifically we want to determine which highest level organizational unit (e.g.
    area or division) each child object is assigned to.
  prefs: []
  type: TYPE_NORMAL
- en: 'The organizational structure can be used to join the employees timeline and
    get detailed information on each employees organizational assignment at a later
    step. We need to utilize SAPs HRP1001 table to achieve this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The Language model produces a recursive function that is searching for the
    highest level organizational unit (‘O’) for each object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Even when the created code is not very well optimized it contains a recursive
    function to dissolve the hierarchy. Users that prefer common table expressions
    (CTEs) should give the hint (using a common table expression) in the input prompt
    to create a more readable and understandable PySpark statement.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Both code versions will create the dataframe for the flattened hierarchical
    organization structure which can be used for further data integration steps by
    simply joining it to the previously generated DataFrame.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0325bb79846b04476b5c5a967469cd3b.png)'
  prefs: []
  type: TYPE_IMG
- en: Dataset containing hierarchical information
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We have shown how to tackle more advanced data engineering tasks in a practical
    use case by extracting and integrating data from SAP Systems using ChatGPT to
    generate PySpark code. Large Language models might not yet be perfect but everyone
    can perhaps already imagine how powerful these techniques can become for data
    engineers. There are several key takeaways:'
  prefs: []
  type: TYPE_NORMAL
- en: ChatGPT is capable of understanding the fundamental principles of data models.
    You can refine its understanding utilizing prompting techniques to supply more
    in depth knowledge.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Even if the approach won´t produce perfect code at the first try, we can easily
    adjust the created code to fit our individual scenarios.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Due to the wide availability of open reference documents and SAP knowledge bases,
    the approach can be expanded to an Retrieval-Augmented Generation (RAG) solution.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When it comes to prompt engineering best practices, try to be as precise as
    possible and provide errorcodes returned by your Spark environment to leverage
    the LLMs capabilities to refactor the generated code. Multiple tries might be
    necessary to refine the code, nevertheless adding keywords like “precise” to your
    prompt might help ChatGPT to produce better results. Ask for detailed explanation
    for the solution approach as this will force ChatGPTs transformer model to dig
    deeper.
  prefs: []
  type: TYPE_NORMAL
- en: 'Note: The prompts containing the csv example datasets had to be cut off due
    to length constraints of this article.'
  prefs: []
  type: TYPE_NORMAL
- en: About the authors
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Markus Stadi* is a Senior Cloud Data Engineer at Dehn SE working in the field
    of Data Engineering, Data Science and Data Analytics for many years.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Christian Koch* is an Enterprise Architect at BWI GmbH and Lecturer at the
    Nuremberg Institute of Technology Georg Simon Ohm.'
  prefs: []
  type: TYPE_NORMAL
