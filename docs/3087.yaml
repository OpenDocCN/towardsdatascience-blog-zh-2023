- en: Scene Graph Generation and its Application in Robotics
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 场景图生成及其在机器人学中的应用
- en: 原文：[https://towardsdatascience.com/scene-graph-generation-and-its-application-in-robotics-f9ba864aa572?source=collection_archive---------6-----------------------#2023-10-10](https://towardsdatascience.com/scene-graph-generation-and-its-application-in-robotics-f9ba864aa572?source=collection_archive---------6-----------------------#2023-10-10)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/scene-graph-generation-and-its-application-in-robotics-f9ba864aa572?source=collection_archive---------6-----------------------#2023-10-10](https://towardsdatascience.com/scene-graph-generation-and-its-application-in-robotics-f9ba864aa572?source=collection_archive---------6-----------------------#2023-10-10)
- en: Let’s have a *small* talk about visualizing images with interactive graphical
    representation!
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 让我们来进行一次关于用交互式图形表示法可视化图像的*简短*讨论吧！
- en: '[](https://medium.com/@ritanshiagarwal?source=post_page-----f9ba864aa572--------------------------------)[![Ritanshi
    Agarwal](../Images/68c99a95cade2a64988dc1f84f008076.png)](https://medium.com/@ritanshiagarwal?source=post_page-----f9ba864aa572--------------------------------)[](https://towardsdatascience.com/?source=post_page-----f9ba864aa572--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----f9ba864aa572--------------------------------)
    [Ritanshi Agarwal](https://medium.com/@ritanshiagarwal?source=post_page-----f9ba864aa572--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@ritanshiagarwal?source=post_page-----f9ba864aa572--------------------------------)[![Ritanshi
    Agarwal](../Images/68c99a95cade2a64988dc1f84f008076.png)](https://medium.com/@ritanshiagarwal?source=post_page-----f9ba864aa572--------------------------------)[](https://towardsdatascience.com/?source=post_page-----f9ba864aa572--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----f9ba864aa572--------------------------------)
    [Ritanshi Agarwal](https://medium.com/@ritanshiagarwal?source=post_page-----f9ba864aa572--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fb2a8c7a68d54&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fscene-graph-generation-and-its-application-in-robotics-f9ba864aa572&user=Ritanshi+Agarwal&userId=b2a8c7a68d54&source=post_page-b2a8c7a68d54----f9ba864aa572---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----f9ba864aa572--------------------------------)
    ·12 min read·Oct 10, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Ff9ba864aa572&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fscene-graph-generation-and-its-application-in-robotics-f9ba864aa572&user=Ritanshi+Agarwal&userId=b2a8c7a68d54&source=-----f9ba864aa572---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fb2a8c7a68d54&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fscene-graph-generation-and-its-application-in-robotics-f9ba864aa572&user=Ritanshi+Agarwal&userId=b2a8c7a68d54&source=post_page-b2a8c7a68d54----f9ba864aa572---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----f9ba864aa572--------------------------------)
    · 12分钟阅读 · 2023年10月10日'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff9ba864aa572&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fscene-graph-generation-and-its-application-in-robotics-f9ba864aa572&source=-----f9ba864aa572---------------------bookmark_footer-----------)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff9ba864aa572&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fscene-graph-generation-and-its-application-in-robotics-f9ba864aa572&source=-----f9ba864aa572---------------------bookmark_footer-----------)'
- en: 'Scene graph generation is the process of generating scene graphs and a scene
    graph contains the visual understanding of an image in the form of a graph. It
    has nodes and edges representing the objects and their relationships, respectively.
    Contextual information about the scenes can help in semantic scene understanding.
    Although there are certain challenges such as uncertainty of real-world scenarios
    or unavailability of a standard dataset, researchers are trying to apply the scene
    graphs in the field of robotics. This write-up contains two major parts: one is
    about how scene graphs can be obtained by having an in-depth discussion about
    two state-of-the-art approaches for scene graph generation based on region-based
    convolutional neural networks, and the other part deals with the application of
    scene graphs in robot planning. In its former part, this article further explains
    that only having a numerical metric for model evaluation is insufficient for proper
    performance analysis. The latter part includes a thorough explanation of a method
    that uses the concept of scene graphs for visual context-aware robot planning.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 场景图生成是生成场景图的过程，场景图以图形的形式包含图像的视觉理解。它具有表示对象和它们关系的节点和边。关于场景的上下文信息可以帮助语义场景理解。尽管存在如现实世界场景的不确定性或缺乏标准数据集等挑战，研究人员正尝试将场景图应用于机器人领域。本文包含两个主要部分：一部分讨论了如何通过对基于区域的卷积神经网络的两种最先进方法的深入讨论来获取场景图，另一部分涉及场景图在机器人规划中的应用。在前一部分中，文章进一步解释了仅有数值度量来评估模型是不够的，无法进行适当的性能分析。后一部分包括对一种利用场景图概念进行视觉上下文感知机器人规划的方法的详细解释。
- en: '![](../Images/bd1ee322371527cfd647250bb3bdcf12.png)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bd1ee322371527cfd647250bb3bdcf12.png)'
- en: 'Figure 1a: Nodes and Edges depiction to create scene graphs (Background Image
    by [TruckRun](https://unsplash.com/@truckrun_ebike_systems?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)
    on [Unsplash](https://unsplash.com/photos/a-couple-of-people-riding-bikes-through-a-forest-g1YAFw6_lHo?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash))'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 图1a：创建场景图的节点和边的描绘（背景图片来自[TruckRun](https://unsplash.com/@truckrun_ebike_systems?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)于[Unsplash](https://unsplash.com/photos/a-couple-of-people-riding-bikes-through-a-forest-g1YAFw6_lHo?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)）
- en: 1\. Introduction
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 1\. 介绍
- en: When considering the description of image elements in words, well-known active
    techniques such as image segmentation, object classification, activity identification,
    etc. play a significant role in the process. Scene Graph Generation (SGG) is a
    method to generate scene graphs (SGs), by depicting objects (e.g., humans, animals,
    etc.) and their attributes (e.g., color, clothes, vehicles, etc.) as nodes, and
    the relationships among objects as the edges. Fig.1a and 1b shows how a scene
    graph generated from an RGB image usually looks like. It can be seen that the
    distinct colored nodes depict different major objects in the image (a male, a
    female, and a vehicle), the information about these objects is shown by the nodes
    having the variant of the same node color, and the edges are labeled with verbs
    showing a connection, or we can say relation among the nodes. The research has
    been going on for past years to make the SGs possible even for small details in
    the image.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在考虑用文字描述图像元素时，图像分割、对象分类、活动识别等知名的活跃技术在过程中发挥了重要作用。场景图生成（SGG）是一种生成场景图（SGs）的方法，通过将对象（如人类、动物等）及其属性（如颜色、衣物、车辆等）描绘为节点，并将对象之间的关系描绘为边。图1a和1b展示了从RGB图像生成的场景图通常是什么样的。可以看到，不同颜色的节点描绘了图像中的主要对象（一个男性，一个女性和一辆车辆），关于这些对象的信息由具有相同节点颜色变体的节点显示，边缘则用动词标注，显示连接，或者说节点之间的关系。研究人员在过去几年中一直致力于使SGs即使对图像中的小细节也能实现。
- en: '![](../Images/4c2ab148202efa281bd3da401c6fb11e.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4c2ab148202efa281bd3da401c6fb11e.png)'
- en: 'Figure 1b: Process and Major Challenges in Scene Graph Generation'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 图1b：场景图生成中的过程和主要挑战
- en: This article extensively discusses two approaches in the upcoming sections for
    a better understanding of generating scene graphs. The write-up structure comprises
    six sections. The brief start of SGG and its prior applications including image
    and video captioning is explained in section 2\. Section 3 describes the state-of-the-art
    methods with their model description. The experimental analysis of these methods
    is discussed and compared in section 4\. Section 5 discusses some possible applications
    of SGs in the field of robotics with a main focus on robot planning, and also
    how having accurate SGs will make applications like telepresence robots, human-robot
    collaboration, etc. much easier to implement. Finally, section 6 concludes the
    entire literature.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 本文将在接下来的章节中广泛讨论生成场景图的两种方法，以便更好地理解。写作结构包括六个部分。第2节简要介绍了SGG及其先前的应用，包括图像和视频字幕。第3节描述了最先进的方法及其模型描述。第4节讨论并比较了这些方法的实验分析。第5节讨论了SGs在机器人领域的一些可能应用，主要集中在机器人规划方面，还讨论了准确的SGs如何使远程机器人、人机协作等应用更易于实现。最后，第6节总结了整个文献。
- en: 2\. Related Work
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 2\. 相关工作
- en: The SGs have a history of distinct applications, for instance, image and video
    captioning, visual question answering, 3D scene understanding, many applications
    in robotics, etc. [10] [3]. Most of the recent approaches include machine learning
    methods like Region-based Convolutional Neural Networks (R-CNN) [8], faster R-CNN
    [9], Recurrent Neural Networks (RNN) [7], etc. [7] proposed a structure that takes
    the image as input, then passes messages containing contextual information, and
    refines the prediction using RNN. Another method [6] uses the energy model of
    the image to generate comprehensive scene graphs. The datasets for SGG are also
    obtained using different real-world scenarios, images, and videos. The dataset
    mentioned here is a subset [7] of Visual Genome [4] containing 108,073 human-annotated
    images.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: SGs有着不同的应用历史，例如图像和视频字幕、视觉问答、3D场景理解、机器人领域的许多应用等。[10] [3]。最近的大多数方法包括机器学习方法，如基于区域的卷积神经网络（R-CNN）[8]、更快的R-CNN
    [9]、递归神经网络（RNN）[7]等。[7]提出了一种结构，将图像作为输入，然后传递包含上下文信息的消息，并使用RNN进行预测的细化。另一种方法[6]使用图像的能量模型生成综合的场景图。SGG的数据集也是通过不同的真实世界场景、图像和视频获得的。这里提到的数据集是Visual
    Genome [4]的一个子集[7]，包含108,073张人类注释的图像。
- en: 3\. State-of-the-Art Approaches
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 3\. 最先进的方法
- en: 3.1 Graph R-CNN Approach
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.1 图形 R-CNN 方法
- en: J. Yang et. al. [8] proposes a model that first considers all the objects connected
    by an edge, relationship, then nullifies the unlikely relationship using a parameter
    called *relatedness*. For instance, it is more likely to have a relation between
    *car* and *wheel*, than a *building* and a *wheel*.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: J. Yang等[8]提出了一种模型，首先考虑所有由边连接的对象和关系，然后使用称为*相关性*的参数来排除不太可能的关系。例如，*车*和*轮子*之间的关系比*建筑物*和*轮子*之间的关系更有可能存在。
- en: The paper shows a novel framework, namely Graph R-CNN with the introduction
    of a new evaluation metric, SGGen+. The model consists of three blocks, first
    to extract object nodes, second to remove unlikely edges, and finally to propagate
    graph context throughout the remaining graph so that the final scene graph can
    be produced. The novelty resides in the second and third block of the model, having
    a new relation proposal network (RePN) which computes *relatedness* that further
    helps in removing unlikely relationships, also called relation edge pruning and
    an attentional graph convolutional network (aGCN) that constitutes the propagation
    of higher-order context that leads to an update in scene graph giving it a final
    touch, respectively. Basically, the final block labeled the generated scene graph.
    The probability distribution of the whole idea can be depicted by the equation
    below, where for image *I*, *O* is the set of objects, *R* is the relation among
    objects, and *Rl* and *Ol* are relation and object labels respectively.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 论文展示了一个新颖的框架，即Graph R-CNN，引入了一种新的评估指标SGGen+。该模型由三个模块组成：首先提取对象节点，其次移除不太可能的边，最后在剩余图中传播图上下文，以便生成最终的场景图。创新点在于模型的第二个和第三个模块，具有一个新的关系提议网络（RePN），该网络计算*相关性*，进一步帮助移除不太可能的关系，也称为关系边修剪，以及一个注意力图卷积网络（aGCN），它构成了高阶上下文的传播，从而更新场景图，赋予其最终修饰。基本上，最终的模块标记生成的场景图。整个概念的概率分布可以通过下面的方程表示，其中对图像
    *I*，*O* 是对象的集合，*R* 是对象之间的关系，*Rl* 和 *Ol* 分别是关系和对象标签。
- en: Pr(G|I) = Pr(O|I) * Pr(R|O, I) * Pr(Rl, Ol|O, R, I)
  id: totrans-22
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Pr(G|I) = Pr(O|I) * Pr(R|O, I) * Pr(Rl, Ol|O, R, I)
- en: 'Fig. 2 shows the entire algorithm in pictorial form, and how all three stages
    work consecutively. The pipeline can be described as: using faster R-CNN for object
    detection, then generating an initial graph with all defined relationships, then
    pruning unlikely edges based on *relatedness* score, and then finally using aGCN
    for refining.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2 展示了整个算法的图示形式，以及所有三个阶段如何依次工作。该流程可以描述为：使用 Faster R-CNN 进行对象检测，然后生成一个包含所有定义关系的初始图，然后基于
    *相关性* 分数修剪不太可能的边缘，最后使用 aGCN 进行精炼。
- en: '![](../Images/e95853623c01823b7da7f73796aa1bb6.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e95853623c01823b7da7f73796aa1bb6.png)'
- en: 'Figure 2: Graph R-CNN [8] Working Explained'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '图 2: 图形 R-CNN [8] 工作说明'
- en: 3.2 Stacked Motif Network (MotifNet)
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.2 堆叠 Motif 网络（MotifNet）
- en: R. Zellers et. al. [9] proposes a network that performs bounding box collection,
    object identification, and relation identification consecutively. The method shows
    the intense use of faster R-CNN and Long Short-term Memory (LSTM) networks. In
    contrast with Graph R-CNN, this method is based on detecting the relationships
    among objects and adding the relationship edges.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: R. Zellers 等人[9] 提出了一个网络，该网络依次执行边界框收集、对象识别和关系识别。这种方法展示了对 Faster R-CNN 和长短期记忆（LSTM）网络的密集使用。与图形
    R-CNN 相比，该方法基于检测对象之间的关系并添加关系边。
- en: The idea here can be seen as a conditional probability equation, as shown in
    the equation below, which states the probability to find graph *G* of image *I*
    is given as the product of the probability of finding bounding box array *B* given
    *I*, probability of obtaining object *O* given *B* and *I*, and the probability
    of getting relations *R* among *O* given *B*, *O* and *I*.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的想法可以看作是一个条件概率方程，如下方程所示，表示在给定 *I* 的情况下找到图 *G* 的概率为找到边界框数组 *B* 的概率、在给定 *B*
    和 *I* 的情况下获得对象 *O* 的概率，以及在给定 *B*、*O* 和 *I* 的情况下获得关系 *R* 的概率的乘积。
- en: Pr(G|I) = Pr(B|I) * Pr(O|B,I) * Pr(R|B,O,I)
  id: totrans-29
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Pr(G|I) = Pr(B|I) * Pr(O|B,I) * Pr(R|B,O,I)
- en: Firstly, to find the set of the bounding box, they also use faster R-CNN, as
    in [8], shown on the left side of Fig. 3\. It uses the VGG backbone structure
    for object identification while determining bounding boxes in *I*. Then, the model
    proceeds further with bidirectional LSTM layers depicted by the object context
    layer in Fig. 3\. This block of layers generates the object labels for each bounding
    box region, *bi* ∈ *B*. Another bidirectional LSTM layers block is again used
    for determining relations as edge context, portrayed in the same figure. The output
    is computed as determining the edge labels which clearly depict the outer product
    with objects and relations among them. The detailed model structure can be learned
    in [9].
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，为了找到边界框的集合，他们也使用 Faster R-CNN，如 [8] 所示，见图 3 的左侧。它使用 VGG 主干结构进行对象识别，同时确定 *I*
    中的边界框。然后，模型继续使用图 3 中的对象上下文层所示的双向 LSTM 层。该层块生成每个边界框区域的对象标签，*bi* ∈ *B*。另一个双向 LSTM
    层块用于确定作为边缘上下文的关系，如同一图中所示。输出结果计算为确定边缘标签，这些标签清晰地描述了对象及其之间的关系的外积。详细的模型结构可以在 [9] 中学习。
- en: '![](../Images/418fe06b2b6453556f0f2ede61b2c4d7.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/418fe06b2b6453556f0f2ede61b2c4d7.png)'
- en: 'Figure 3: MotifNet [9] Structure Flow Diagram Explained (Dog Image by [Justin
    Veenema](https://unsplash.com/@justinveenema?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)
    on [Unsplash](https://unsplash.com/photos/white-and-black-american-pit-bull-terrier-at-daytime-NH1d0xX6Ldk?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash))'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '图 3: MotifNet [9] 结构流程图说明（狗的图片由 [Justin Veenema](https://unsplash.com/@justinveenema?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)
    提供，来源于 [Unsplash](https://unsplash.com/photos/white-and-black-american-pit-bull-terrier-at-daytime-NH1d0xX6Ldk?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)）'
- en: 4\. Experiments in State-of-the-Art Approaches
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 4\. 最先进方法中的实验
- en: 4.1 Experimental Setup
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.1 实验设置
- en: 'The setup for both methods is described below:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 两种方法的设置如下所述：
- en: '**Graph R-CNN:** The use of faster R-CNN with VGG16 backbone to ensure object
    detection is implemented via PyTorch. For the RePN implementation, a multi-layer
    perceptron structure is used to analyze the relatedness score using two projection
    functions, each for subject and object relation. Two aGCN layers are used, one
    for the feature level, the result of which is sent to the other one at the semantic
    level. The training is done in two stages, first only the object detector is trained,
    and then the whole model is trained jointly.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '**图形 R-CNN：** 使用带有 VGG16 主干的更快 R-CNN 来确保对象检测，通过 PyTorch 实现。对于 RePN 实现，使用多层感知机结构通过两个投影函数（分别针对主题和对象关系）分析相关性得分。使用两个
    aGCN 层，一个用于特征级别，其结果传递给另一个语义级别的层。训练分两个阶段进行，首先训练对象检测器，然后联合训练整个模型。'
- en: '**MotifNet:** The images that are fed into the bounding box detector are made
    to be of size 592x592, by using the zero-padding method. All the LSTM layers undergo
    highway connections. Two and four alternating highway LSTM layers are used for
    object and edge context respectively. The ordering of the bounding box regions
    can be done in several ways using central x-coordinate, maximum non-background
    prediction, size of the bounding box, or just random shuffling.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '**MotifNet：** 通过使用零填充方法，将输入到边界框检测器的图像调整为 592x592 的大小。所有 LSTM 层都经过高速公路连接。分别为对象和边缘上下文使用了两个和四个交替的高速公路
    LSTM 层。边界框区域的排序可以通过中央 x 坐标、最大非背景预测、边界框大小或随机洗牌来完成。'
- en: The main challenge is to analyze the model with a common dataset framework,
    as different approaches use different data preprocessing, split, and evaluation.
    However, the discussed approaches, Graph R-CNN and MotifNet uses the publicly
    available data processing scheme and split from [7]. There are 150 object classes
    and 50 classes for relations in this Visual Genome dataset [4].
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 主要挑战是使用共同的数据集框架分析模型，因为不同的方法使用不同的数据预处理、拆分和评估。然而，讨论的方法，Graph R-CNN 和 MotifNet
    使用了[7]中公开的数据处理方案和拆分。这个 Visual Genome 数据集中有 150 个对象类别和 50 个关系类别[4]。
- en: '**Visual Genome Dataset [4] in a nutshell:**'
  id: totrans-39
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**Visual Genome 数据集 [4] 总结：**'
- en: ''
  id: totrans-40
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Human Annotated Images
  id: totrans-41
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 人工标注的图像
- en: ''
  id: totrans-42
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: More than 100,000 images
  id: totrans-43
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 超过 100,000 张图像
- en: ''
  id: totrans-44
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 150 Object Classes
  id: totrans-45
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 150 个对象类别
- en: ''
  id: totrans-46
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 50 Relation Classes
  id: totrans-47
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 50 个关系类别
- en: ''
  id: totrans-48
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Each image has around 11.5 objects and 6.2 relationships in scene graph
  id: totrans-49
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 每张图像在场景图中大约有 11.5 个对象和 6.2 个关系
- en: 4.2 Experimental Results
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.2 实验结果
- en: '![](../Images/045f9e42cf75bc160863454ce696921b.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/045f9e42cf75bc160863454ce696921b.png)'
- en: 'Table 1: Performance Comparison'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1：性能比较
- en: '**Quantitative Comparison:** Both methods evaluated their model using the recall
    metric. Table 1 shows the comparison of both methods via different quantitative
    indicators. (1) Predicate Classification (PredCls) denotes the performance to
    recognize the relation between objects, (2) Phrase Classification (PhrCls) or
    scene graph classification in [9] depicts the ability to observe the categories
    of both objects and relations, (3) Scene Graph Generation (SGGen) or scene graph
    detection in [9] represents the performance to combine the objects with detected
    relations among them. In [8], they enhance the latter metric with a comprehensive
    SGGen (SGGen+) that includes the possibility of having a certain scenario like
    detecting a *man* as *boy*, technically it is a failed detection, but qualitatively
    if all the relations to this object is detected successfully then it should be
    considered as a successful result, hence increasing the SGGen metric value.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '**定量比较：** 两种方法都使用召回率指标评估了它们的模型。表 1 通过不同的定量指标展示了这两种方法的比较。(1) 谓词分类（PredCls）表示识别对象之间关系的性能，(2)
    短语分类（PhrCls）或[9]中的场景图分类表示观察对象和关系类别的能力，(3) 场景图生成（SGGen）或[9]中的场景图检测表示将对象与检测到的关系结合的性能。在[8]中，他们通过全面的SGGen（SGGen+）来提升后者的指标，该指标包括了识别*男人*为*男孩*的可能性，从技术上讲，这是一个失败的检测，但如果对该对象的所有关系都被成功检测到，那么应视为成功结果，从而提高SGGen指标值。'
- en: According to table 1, MotifNet [9] performs comparatively better when analyzing
    objects, edges, and relation labels separately. However, the generation of the
    entire graph of a given image is more accurate using the second approach, Graph
    R-CNN [8]. It also shows that having the comprehensive output metric shows a better
    analysis of the scene graph model.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 根据表1，MotifNet [9] 在分别分析对象、边缘和关系标签时表现相对更好。然而，使用第二种方法，即 Graph R-CNN [8]，生成给定图像的整个图形更为准确。它还表明，拥有全面的输出指标可以更好地分析场景图模型。
- en: '**Qualitative Comparison:** In neural motifs structure [9], they consider the
    qualitative results separately. For instance, the detection of relation edge *wearing*
    as *wears* falls under the category of failed detection. It shows that the model
    [9] performs better than what the output metric number shows. On the other hand,
    [8] includes this understanding of result in their comprehensive SGGen (SGGen+)
    metric which already takes possible not-so-failed detections into consideration.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '**定性比较：** 在神经结构 [9] 中，他们分别考虑定性结果。例如，关系边缘的检测 *wearing* 作为 *wears* 被归为失败检测类别。这表明该模型
    [9] 的表现优于输出指标所显示的效果。另一方面，[8] 将这种结果理解纳入他们的全面SGGen (SGGen+) 指标中，该指标已考虑了可能不是完全失败的检测。'
- en: 5\. Applications in Robotics
  id: totrans-56
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 5\. 机器人学中的应用
- en: This article will overview the understanding of scenes that helps in robotic
    applications, especially robot planning. The wide area of robotics including automatic
    indoor mapping, teleoperation robots, human-robot control for purposes like telemedicine,
    and many more results in a very deep area of SGs applications.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 本文将概述有助于机器人应用的场景理解，特别是机器人规划。机器人学的广泛领域包括自动室内制图、遥控机器人、用于远程医疗等目的的人机控制，以及更多应用，形成了一个非常深奥的场景图应用领域。
- en: When both humans and robots collaborated in a similar workspace to complete
    a specified task, then it is called Human-Robot Collaboration (HRC), and the collaborative
    robots are called *cobots*. Having semantic information along with object detection
    makes the tasks even easier to do. [5] shows a method that uses SGG for safety
    analysis in HRC. Another application involves teleoperation robots that include
    social interaction skills like a meeting chat room on a mobile stick controlled
    by a human from a remote location. The applications include elderly care, attending
    events even if a person is disabled, etc. [2] discusses a method where scene understanding
    helps the user to analyze and control remote environment clearly.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 当人类和机器人在类似的工作空间中合作完成指定任务时，这称为人机协作（HRC），而协作机器人被称为*cobots*。拥有语义信息以及对象检测可以使任务变得更容易。
    [5] 展示了一种使用SGG进行HRC安全分析的方法。另一个应用涉及遥控机器人，包括社会互动技能，例如由远程位置的人操控的移动杆上的会议聊天室。应用领域包括老年护理、即使在残疾情况下也能参加活动等。[2]
    讨论了一种方法，其中场景理解帮助用户清晰地分析和控制远程环境。
- en: 5.1 Robot Planning
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.1 机器人规划
- en: Assigning certain motion tasks to a robot, such as moving one object from one
    place to another, and expecting smooth service from a machine requires a highly
    accurate level of planning and development. SGG plays an important role in producing
    service robots as it gives the robot an in-depth abstracted picture of the scene
    which further helps it to locate objects precisely. [1] uses local scene graphs
    to perceive the global view to reach its target. It describes robot planning for
    a common task that includes searching for an object, such as a fruit, in an indoor
    environment.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 将特定的运动任务分配给机器人，如将一个物体从一个地方移动到另一个地方，并期望机器提供平稳服务，需要高度准确的规划和开发。SGG 在生产服务机器人中发挥重要作用，因为它为机器人提供了场景的深入抽象图像，这进一步帮助机器人精确定位物体。[1]
    使用局部场景图来感知全球视图以达到目标。它描述了一个包括在室内环境中寻找物体（如水果）的常见任务的机器人规划。
- en: '**Scene analysis for robot planning (SARP)** [1] is an algorithm designed for
    robots to use visual contextual information to complete the planned task. This
    method takes advantage of scene graphs to model the global scene understanding
    using MotifNet [9], the algorithm discussed in section 3.2\. The block diagram
    of the model can be accessed in [1]. The model uses MotifNet to generate local
    scene graphs, then the contextual information to analyze uncertainty between observations
    and actions, and update the robot’s belief. The latter process is done using the
    Partially Observable Markov Decision Process (PO-MDP) framework. The MDPs are
    known for sequential decision-making. In the block diagram, the offline trained
    scene graph network generates the global scene graphs and feeds the result into
    the Markov process.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '**机器人规划的场景分析 (SARP)** [1] 是一种算法，旨在使机器人利用视觉上下文信息来完成计划任务。该方法利用场景图通过MotifNet [9]
    建模全局场景理解，MotifNet算法在第3.2节中讨论。模型的框图可以在[1]中查看。该模型使用MotifNet生成本地场景图，然后利用上下文信息来分析观察和行动之间的不确定性，并更新机器人的信念。后续过程使用部分可观察马尔可夫决策过程（PO-MDP）框架完成。MDP以顺序决策著称。在框图中，离线训练的场景图网络生成全局场景图，并将结果输入到马尔可夫过程。'
- en: '***Multiple Local SGs — — — — — →Global Context SGs — — — — →Target Search***'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '***多个本地SGs — — — — — →全局上下文SGs — — — — →目标搜索***'
- en: The method is experimented for locating objects scattered through the area with
    precision and in minimal time. The robot is fed with a scene graph dataset, an
    already trained scene graph network, and a domain map for directive mobility.
    As a result, the robot navigates through the area for target search, and to do
    so, it creates scene graphs for contextual information every step of the way,
    as clearly shown in figure 2 and 3 of the [published IEEE article](https://ieeexplore.ieee.org/abstract/document/9730031)
    [1]. It is the demonstration where the robot is assigned the task of locating
    a *banana*. It increments the global scene graph at every time instant except
    at T=4 because there are no new objects’ instances in that frame. It can be seen
    that having local scene graphs helped the robot to determine the global context
    for the target search. According to the performance comparison in [1], SARP outperforms
    the baseline methods both in terms of success rate and action cost. However, the
    model can be further expanded with facial recognition of humans, and also analyzed
    when changing or adding objects during the test process.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 该方法在精准且最小时间内定位散布在区域内的物体。机器人被提供了一个场景图数据集、一个已经训练好的场景图网络和一个用于指令移动的领域地图。结果是，机器人在区域内进行目标搜索，为此，它在每一步都创建场景图以获取上下文信息，如[发表的IEEE文章](https://ieeexplore.ieee.org/abstract/document/9730031)
    [1]中的图2和图3所示。这是一个演示，展示了机器人被分配任务以定位*香蕉*。它在每个时间点都更新全局场景图，除了T=4，因为在那个帧中没有新的物体实例。可以看出，拥有本地场景图帮助机器人确定目标搜索的全局上下文。根据[1]中的性能比较，SARP在成功率和行动成本方面都优于基线方法。然而，该模型可以进一步扩展，加入人脸识别功能，并且在测试过程中更换或添加物体时也需要进行分析。
- en: 6\. Conclusion
  id: totrans-64
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 6\. 结论
- en: The article discusses the different methods for SGG, the generation of graphs
    having semantic information about scenes, and some of its robotic applications.
    The object and relation contextual information gives us a very descriptive understanding
    of a scene. MotifNet detects and adds the relation among objects as edge context
    using the LSTM network, while Graph R-CNN forms the scene graphs by eliminating
    the unlikely relation edges using *relatedness* score. There are several versions
    of the Visual Genome dataset used in recent approaches, however, MotifNet and
    Graph R-CNN evaluate their model on the same dataset model, which makes their
    quantitative comparison reasonable. Overall, both the quantitative and qualitative
    measurements are analyzed and it can be seen that numbers don’t tell the whole
    story, the not-so-failed scenarios, such as *man* instead of *boy*, should also
    be considered as successful outcome, if all of its relation edges are detected
    correctly, while evaluating a method for SGG. The applications in robotics is
    growing rapidly over the past decades and having an in-depth semantic description
    of the image, or scene will help in several image and video-related applications.
    One method is extensively discussed for the robot planning application that uses
    scene graphs for better visual scene understanding so that the robot can perform
    tasks (e.g., locating objects in indoor environment) in the long run.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 这篇文章讨论了 SGG 的不同方法，即生成包含场景语义信息的图形，以及其一些机器人应用。对象和关系的上下文信息让我们对场景有了非常详细的理解。MotifNet
    使用 LSTM 网络检测并添加对象之间的关系作为边缘上下文，而 Graph R-CNN 通过使用*相关性*分数来消除不太可能的关系边缘，从而形成场景图。虽然最近的方法中使用了几种版本的
    Visual Genome 数据集，但 MotifNet 和 Graph R-CNN 在相同的数据集模型上评估了它们的模型，这使得它们的定量比较是合理的。总体而言，对定量和定性测量进行了分析，可以看出，数字并不能完全说明问题，像*男人*而不是*男孩*这样的并非失败的场景，如果其所有关系边缘都正确检测到，也应被视为成功结果。在评估
    SGG 方法时，考虑图像或场景的深入语义描述将有助于图像和视频相关应用。一个方法在机器人规划应用中得到了广泛讨论，该方法使用场景图来更好地理解视觉场景，以便机器人在长远中执行任务（例如，在室内环境中定位物体）。
- en: Well, this is so much for a *small* research talk about converting images into
    interactive graphical texts. I hope you guys had fun (maybe a little!). I’ll be
    back with another research article summing up recent cool researches (probably
    on any topic from communication networking, Network Simulator (NS3), or maybe
    Wi-Fi 7😉).
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 好了，这就是关于将图像转换为互动图形文本的*简短*研究讲座的全部内容。我希望大家玩得愉快（可能有一点点！）。我会带着另一篇研究文章回来，总结近期的一些有趣研究（可能涉及通信网络、网络模拟器（NS3），或者Wi-Fi
    7😉）。
- en: Have fun researching!!
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 祝你们研究愉快！！
- en: Regards,
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 此致，
- en: Ritanshi
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: Ritanshi
- en: REFERENCES
  id: totrans-70
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: S. Amiri, K. Chandan, and S. Zhang. Reasoning with scene graphs for robot planning
    under partial observability. IEEE Robotics and Automation Letters, 7(2):5560–5567,
    2022.
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: S. Amiri, K. Chandan 和 S. Zhang。《Reasoning with scene graphs for robot planning
    under partial observability》。IEEE Robotics and Automation Letters，7(2):5560–5567，2022年。
- en: 'F. Amodeo, F. Caballero, N. Díaz-Rodríguez, and L. Merino. Og-sgg: Ontology-guided
    scene graph generation. a case study in transfer learning for telepresence robotics.
    IEEE Access, pages 1–1, 2022.'
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'F. Amodeo, F. Caballero, N. Díaz-Rodríguez 和 L. Merino。《Og-sgg: Ontology-guided
    scene graph generation. A case study in transfer learning for telepresence robotics》。IEEE
    Access，页码 1–1，2022年。'
- en: 'X. Chang, P. Ren, P. Xu, Z. Li, X. Chen, and A. Hauptmann. A comprehensive
    survey of scene graphs: Generation and application. IEEE Transactions on Pattern
    Analysis and Machine Intelligence, 45(1):1–26, 2023.'
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'X. Chang, P. Ren, P. Xu, Z. Li, X. Chen 和 A. Hauptmann。《A comprehensive survey
    of scene graphs: Generation and application》。IEEE Transactions on Pattern Analysis
    and Machine Intelligence，45(1):1–26，2023年。'
- en: 'R. Krishna, Y. Zhu, O. Groth, J. Johnson, K. Hata, J. Kravitz, S. Chen, Y.
    Kalantidis, L.-J. Li, D. A. Shamma, et al. Visual genome: Connecting language
    and vision using crowdsourced dense image annotations. International journal of
    computer vision, 123(1):32–73, 2017.'
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'R. Krishna, Y. Zhu, O. Groth, J. Johnson, K. Hata, J. Kravitz, S. Chen, Y.
    Kalantidis, L.-J. Li, D. A. Shamma 等。《Visual genome: Connecting language and vision
    using crowdsourced dense image annotations》。International Journal of Computer
    Vision，123(1):32–73，2017年。'
- en: H. Riaz, A. Terra, K. Raizer, R. Inam, and A. Hata. Scene understanding for
    safety analysis in human-robot collaborative operations. In 2020 6th International
    Conference on Control, Automation and Robotics (ICCAR), pages 722–731, 2020.
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: H. Riaz, A. Terra, K. Raizer, R. Inam 和 A. Hata。《Scene understanding for safety
    analysis in human-robot collaborative operations》。在 2020年第六届控制、自动化与机器人国际会议（ICCAR），页码
    722–731，2020年。
- en: M. Suhail, A. Mittal, B. Siddiquie, C. Broaddus, J. Eledath, G. Medioni, and
    L. Sigal. Energy-based learning for scene graph generation. In 2021 IEEE/CVF Conference
    on Computer Vision and Pattern Recognition (CVPR), pages 13931–13940, 2021.
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: M. Suhail, A. Mittal, B. Siddiquie, C. Broaddus, J. Eledath, G. Medioni, 和 L.
    Sigal。基于能量的场景图生成学习。发表于2021年IEEE/CVF计算机视觉与模式识别会议（CVPR），第13931–13940页，2021年。
- en: D. Xu, Y. Zhu, C. B. Choy, and L. Fei-Fei. Scene graph generation by iterative
    message passing. In Proceedings of the IEEE conference on computer vision and
    pattern recognition, pages 5410–5419, 2017.
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: D. Xu, Y. Zhu, C. B. Choy, 和 L. Fei-Fei。通过迭代消息传递生成场景图。发表于IEEE计算机视觉与模式识别会议论文集，第5410–5419页，2017年。
- en: J. Yang, J. Lu, S. Lee, D. Batra, and D. Parikh. Graph r-cnn for scene graph
    generation. In Proceedings of the European conference on computer vision (ECCV),
    pages 670–685, 2018.
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: J. Yang, J. Lu, S. Lee, D. Batra, 和 D. Parikh。用于场景图生成的图形r-cnn。发表于欧洲计算机视觉会议（ECCV）论文集，第670–685页，2018年。
- en: 'R. Zellers, M. Yatskar, S. Thomson, and Y. Choi. Neural motifs: Scene graph
    parsing with global context. In 2018 IEEE/CVF Conference on Computer Vision and
    Pattern Recognition, pages 5831–5840, 2018.'
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: R. Zellers, M. Yatskar, S. Thomson, 和 Y. Choi。神经模式：具有全局上下文的场景图解析。发表于2018年IEEE/CVF计算机视觉与模式识别会议，第5831–5840页，2018年。
- en: 'G. Zhu, L. Zhang, Y. Jiang, Y. Dang, H. Hou, P. Shen, M. Feng, X. Zhao, Q.
    Miao, S. A. A. Shah, et al. Scene graph generation: A comprehensive survey. arXiv
    preprint arXiv:2201.00443, 2022.'
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: G. Zhu, L. Zhang, Y. Jiang, Y. Dang, H. Hou, P. Shen, M. Feng, X. Zhao, Q. Miao,
    S. A. A. Shah等。场景图生成：一项综合调查。arXiv预印本arXiv:2201.00443，2022年。
