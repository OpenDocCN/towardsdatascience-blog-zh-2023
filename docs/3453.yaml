- en: Semantic Search with PostgreSQL and OpenAI Embeddings
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/semantic-search-with-postgresql-and-openai-embeddings-4d327236f41f?source=collection_archive---------3-----------------------#2023-11-21](https://towardsdatascience.com/semantic-search-with-postgresql-and-openai-embeddings-4d327236f41f?source=collection_archive---------3-----------------------#2023-11-21)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[](https://curiousdima.medium.com/?source=post_page-----4d327236f41f--------------------------------)[![Dima
    Timofeev](../Images/a11f81c67f04923c0ca454347f1fe423.png)](https://curiousdima.medium.com/?source=post_page-----4d327236f41f--------------------------------)[](https://towardsdatascience.com/?source=post_page-----4d327236f41f--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----4d327236f41f--------------------------------)
    [Dima Timofeev](https://curiousdima.medium.com/?source=post_page-----4d327236f41f--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F1df0a90be7e9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsemantic-search-with-postgresql-and-openai-embeddings-4d327236f41f&user=Dima+Timofeev&userId=1df0a90be7e9&source=post_page-1df0a90be7e9----4d327236f41f---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----4d327236f41f--------------------------------)
    ·4 min read·Nov 21, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F4d327236f41f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsemantic-search-with-postgresql-and-openai-embeddings-4d327236f41f&user=Dima+Timofeev&userId=1df0a90be7e9&source=-----4d327236f41f---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4d327236f41f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsemantic-search-with-postgresql-and-openai-embeddings-4d327236f41f&source=-----4d327236f41f---------------------bookmark_footer-----------)![](../Images/2c0f934e64c66aa85851890637f67bfa.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Image by [Igor Omilaev](https://unsplash.com/@omilaev) on [Unsplash](https://unsplash.com/photos/a-blue-background-with-a-bunch-of-cookies-and-a-red-object-Z2PahC-Fi08)
  prefs: []
  type: TYPE_NORMAL
- en: Implementing semantic search within corporate databases can be challenging and
    requires significant effort. However, does it have to be this way? In this article,
    I demonstrate how you can utilize PostgreSQL along with OpenAI Embeddings to implement
    semantic search on your data. If you prefer not to use OpenAI Embeddings API,
    I will provide you with links to free embedding models.
  prefs: []
  type: TYPE_NORMAL
- en: On a very high level, vector databases with LLMs allow to do semantic search
    on available data (stored in databases, documents, etc.) Thank to the “[Efficient
    Estimation of Word Representations in Vector Space](https://arxiv.org/pdf/1301.3781.pdf)”
    paper (also known as “Word2Vec Paper”) co-authored by legendary [Jeff Dean](https://en.wikipedia.org/wiki/Jeff_Dean),
    we know how to represent words as real-valued vectors. Word embeddings are dense
    vector representations of words in a vector space where words with similar meanings
    are closer to each other. Word embeddings capture semantic relationships between
    words and there are more than one technique to create them.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/615281e0ab21b19949fcf1800e50fe06.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by the author
  prefs: []
  type: TYPE_NORMAL
- en: Let’s practice and use OpenAI’s [*text-embedding-ada*](https://openai.com/blog/new-and-improved-embedding-model)
    model! The choice of distance function typically doesn’t matter much. OpenAI recommends
    cosine similarity. If you don’t want to use OpenAI embeddings and prefer running
    a different model locally instead of making API calls, I suggest considering one
    of the [SentenceTransformers pretrained models](https://www.sbert.net/docs/pretrained_models.html).
    Choose your model wisely.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Now that we have developed an understanding of what an embedding is, let’s utilize
    it to sort some reviews.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: While the absolute difference may appear small, consider a sorting function
    with thousands and thousands of reviews. In such cases, we can prioritize highlighting
    only the positive ones at the top.
  prefs: []
  type: TYPE_NORMAL
- en: Once a word or a document has been transformed into an embedding, it can be
    stored in a database. This action, however, does not automatically classify the
    database as a vector database. It’s only when the database begins to support fast
    operations on the vector that we can rightfully label it as a vector database.
  prefs: []
  type: TYPE_NORMAL
- en: There are numerous commercial and open-source vector databases, making it a
    highly discussed topic. I will demonstrate the functioning of vector databases
    using a [pgvector](https://www.postgresql.org/about/news/pgvector-050-released-2700/),
    an open-source PostgreSQL extension that enables vector similarity search functionalities
    for arguably the most popular database.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s run the PostgreSQL container with pgvector:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s start [pgcli](https://github.com/dbcli/pgcli) to connect to the database
    (*pgcli postgres://postgres:postgres@localhost:5432*) and create a table, insert
    the embeddings we computed above, and then select similar items:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: We are prepared to search for similar documents now. I have shortened the embedding
    for “good ride” again because printing 1536 dimensions is excessive.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Completed! As you can observe, we have computed embeddings for multiple documents,
    stored them in the database, and conducted vector similarity searches. The potential
    applications are vast, ranging from corporate searches to features in medical
    record systems for identifying patients with similar symptoms. Furthermore, this
    method is not restricted to texts; similarity can also be calculated for other
    types of data such as sound, video, and images.
  prefs: []
  type: TYPE_NORMAL
- en: Enjoy!
  prefs: []
  type: TYPE_NORMAL
