- en: Implementing a sales & support agent with LangChain
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/implementing-a-sales-support-agent-with-langchain-63c4761193e7?source=collection_archive---------0-----------------------#2023-04-19](https://towardsdatascience.com/implementing-a-sales-support-agent-with-langchain-63c4761193e7?source=collection_archive---------0-----------------------#2023-04-19)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Learn how to develop a chatbot that can answer questions based on the information
    provided in your company’s documentation
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://bratanic-tomaz.medium.com/?source=post_page-----63c4761193e7--------------------------------)[![Tomaz
    Bratanic](../Images/d5821aa70918fcb3fc1ff0013497b3d5.png)](https://bratanic-tomaz.medium.com/?source=post_page-----63c4761193e7--------------------------------)[](https://towardsdatascience.com/?source=post_page-----63c4761193e7--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----63c4761193e7--------------------------------)
    [Tomaz Bratanic](https://bratanic-tomaz.medium.com/?source=post_page-----63c4761193e7--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F57f13c0ea39a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimplementing-a-sales-support-agent-with-langchain-63c4761193e7&user=Tomaz+Bratanic&userId=57f13c0ea39a&source=post_page-57f13c0ea39a----63c4761193e7---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----63c4761193e7--------------------------------)
    ·10 min read·Apr 19, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F63c4761193e7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimplementing-a-sales-support-agent-with-langchain-63c4761193e7&user=Tomaz+Bratanic&userId=57f13c0ea39a&source=-----63c4761193e7---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F63c4761193e7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimplementing-a-sales-support-agent-with-langchain-63c4761193e7&source=-----63c4761193e7---------------------bookmark_footer-----------)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: Recently, I have been fascinated by the power of ChatGPT and its ability to
    construct various types of chatbots. I have tried and written about multiple approaches
    to implementing a chatbot that can access external information to improve its
    answers. I joined a few Discord channels during my chatbot coding sessions, hoping
    to get some help as the libraries are relatively new, and not much documentation
    is available yet. To my amazement, I found custom bots that could answer most
    of the questions for the given library.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b6412e36fd86059a0c293b5f63d8d361.png)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
- en: Example of a discord support bot. Image by the author.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: The idea is to provide the chatbot the ability to dig through various resources
    like company documentation, code, or other content in order to allow it to answer
    company support questions. Since I already have some experience with chatbots,
    I decided to test how hard it is to implement a custom bot with access to the
    company’s resources.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: In this blog post, I will walk you through how I used OpenAI’s models to implement
    a sales & support agent with in the [LangChain library](https://python.langchain.com/en/latest/index.html)
    that can be used to answer information about applications with a [graph database
    Neo4j](https://neo4j.com/). The agent can also help you debug or produce any Cypher
    statement you are struggling with. Such an agent could then be deployed to serve
    users on Discord or other platforms.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9ec9e7b7feaa9f81a3bee35083bf94a1.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
- en: Agent design. Image by the author.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: We will be using the [LangChain library](https://python.langchain.com/en/latest/index.html)
    to implement the support bot. The library is easy to use and provides an excellent
    integration of LLM prompts and Python code, allowing us to develop chatbots in
    only a few minutes. In addition, the library supports a range of LLMs, text embedding
    models, and vector databases, along with utility functions that help us load and
    embed frequent types of files we might come across, like text, PowerPoint, images,
    HTML, PDF, and more.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: The code for this blog post is available on GitHub.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://github.com/tomasonjo/blogs/blob/master/neo4jdocs/neo4j_support_bot.ipynb?source=post_page-----63c4761193e7--------------------------------)
    [## blogs/neo4j_support_bot.ipynb at master · tomasonjo/blogs'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: You can't perform that action at this time. You signed in with another tab or
    window. You signed out in another tab or…
  id: totrans-18
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: github.com](https://github.com/tomasonjo/blogs/blob/master/neo4jdocs/neo4j_support_bot.ipynb?source=post_page-----63c4761193e7--------------------------------)
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: LangChain document loaders
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: First, we must preprocess the company’s resources and store them in a vector
    database. Luckily, LangChain can help us load external data, calculate text embeddings,
    and store the documents in a vector database of our choice.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: First, we have to load the text into documents. LangChain offers a variety of
    [helper functions that can take various formats and types of data and produce
    a document output](https://python.langchain.com/en/latest/modules/indexes/document_loaders.html?highlight=document+loaders).
    The helper functions are called Document loaders.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: Neo4j has a lot of its documentation available in GitHub repositories. Conveniently,
    LangChain provides a document loader that takes a repository URL as input and
    produces a document for each file in the repository. Additionally, we can use
    the filter function to ignore files during the loading process if needed.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: We will begin by loading the AsciiDoc files from the [Neo4j’s knowledge base
    repository](https://github.com/neo4j-documentation/knowledge-base).
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Wasn’t that easy as a pie? The `GitLoader` function clones the repository and
    load relevant files as documents. In this example, we specified that the file
    must end with `.adoc` suffix and be a part of the `articles` folder. In total,
    309 articles were loaded. We also have to be mindful of the size of the documents.
    For example, GPT-3.5-turbo has a token limit of 4000, while GPT-4 allows 8000
    tokens in a single request. While number of words is not exactly identical to
    the number of tokens, it is still a good estimator.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will load the documentation of the [Graph Data Science repository](https://github.com/neo4j/graph-data-science).
    Here, we will use a text splitter to make sure none of the documents exceed 2000
    words. Again, I know that number of words is not equal to the number of tokens,
    but it is a good approximation. Defining the threshold number of tokens can significantly
    affect how the database is found and retrieved. I found a [great article by Pinecone
    that can help you understand the basics of various chunking strategies](https://www.pinecone.io/learn/chunking-strategies/).
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: We could load other Neo4j repositories that contain documentation. However,
    the idea is to show various data loading methods and not explore all of Neo4j’s
    repositories containing documentation. Therefore, we will move on and look at
    how we can load documents from a Pandas Dataframe.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: For example, say that we want to load a YouTube video as a document source for
    our chatbot. Neo4j has its own YouTube channel and, even I appear in a video or
    two. Two years ago I presented how to implement an information extraction pipeline.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: With LangChain, we can use the captions of the video and load it as documents
    with only three lines of code.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: It couldn’t get any easier than this. Next, we will look at loading documents
    from a Pandas dataframe. A month ago, I retrieved information from Neo4j medium
    publication for a separate blog post. Since we want to bring external information
    about Neo4j to the bot, we can also use the content of the medium articles.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Here, we used Pandas to load a CSV file from GitHub, renamed one column, and
    used the `DataFrameLoader`function to load the articles as documents. Since medium
    posts could exceed 4000 tokens, we used the text splitter to split the articles
    into multiple chunks.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: The last source we will use is the Stack Overflow API. Stack Overflow is a web
    platform where users help others solve coding problems. Their API does not require
    any authorization. Therefore, we can use the API to retrieve questions with accepted
    answers that are tagged with the Neo4j tag.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Each approved answer and the original question are used to construct a single
    document. Since most Stack overflow questions and answers do not exceed 4000 tokens,
    we skipped the text-splitting step.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have loaded the documentation resources as documents, we can move
    on to the next step.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经将文档资源加载为文档，可以进入下一步。
- en: Storing documents in a vector database
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将文档存储在向量数据库中
- en: A chatbot finds relevant information by comparing the vector embedding of questions
    with document embeddings. A text embedding is a machine-readable representation
    of text in the form of a vector or, more plainly, a list of floats. In this example,
    we will use the **ada-002** model provided by OpenAI to embed documents.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 聊天机器人通过将问题的向量嵌入与文档嵌入进行比较来找到相关信息。文本嵌入是机器可读的文本表示形式，通常是一个向量或更简单地说，是一个浮点数列表。在这个例子中，我们将使用
    OpenAI 提供的**ada-002**模型来嵌入文档。
- en: The whole idea behind vector databases is the ability to store vectors and provide
    fast similarity searches. The vectors are usually compared using cosine similarity.
    LangChain includes [integration with a variety of vector databases](https://python.langchain.com/en/latest/modules/indexes/vectorstores.html).
    To keep things simple, we will use the [Chroma vector database](https://www.trychroma.com/),
    which can be used as a local in-memory. For a more serious chatbot application,
    we want to use a persistent database that doesn’t lose data once the script or
    notebook is closed.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 向量数据库的核心理念是存储向量并提供快速的相似性搜索。向量通常使用余弦相似性进行比较。LangChain 包括与各种向量数据库的[集成](https://python.langchain.com/en/latest/modules/indexes/vectorstores.html)。为了简单起见，我们将使用[Chroma
    向量数据库](https://www.trychroma.com/)，它可以用作本地内存数据库。对于更严肃的聊天机器人应用程序，我们希望使用一个持久的数据库，以避免在脚本或笔记本关闭后丢失数据。
- en: We will create two collections of documents. The first will be more sales and
    marketing oriented, containing documents from Medium and YouTube. The second collection
    focuses more on support use cases and consists of documentation and Stack Overflow
    documents.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将创建两个文档集合。第一个将更多关注销售和营销，包含来自 Medium 和 YouTube 的文档。第二个集合更多关注支持用例，包括文档和 Stack
    Overflow 文档。
- en: '[PRE5]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: This script runs each document through OpenAI’s text embedding API and inserts
    the resulting embedding along with text in the Chroma database. The process of
    text embedding costs 0.80$, which is a reasonable price.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 这个脚本通过 OpenAI 的文本嵌入 API 处理每个文档，并将生成的嵌入以及文本插入 Chroma 数据库。文本嵌入的费用为 0.80$，这是一个合理的价格。
- en: Question answering using external context
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用外部上下文进行问答
- en: The last thing to do is to implement two separate question-answering flow. The
    first will handle the sales & marketing requests, while the other will handle
    support. The LangChain library uses LLMs for reasoning and providing answers to
    the user. Therefore, we start by defining the LLM. Here, we will be using the
    **GPT-3.5-turbo** model from OpenAI.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 最后要做的是实现两个独立的问答流程。第一个将处理销售和营销请求，另一个将处理支持请求。LangChain 库使用 LLMs 进行推理并向用户提供答案。因此，我们首先定义
    LLM。在这里，我们将使用来自 OpenAI 的**GPT-3.5-turbo**模型。
- en: '[PRE6]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Implementing a question-answering flow is about as easy as it gets with LangChain.
    We only need to provide the LLM to be used along with the retriever that is used
    to fetch relevant documents. Additionally, we have the option to customize the
    LLM prompt used to answer questions.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 实现问答流程在 LangChain 中非常简单。我们只需提供要使用的 LLM 以及用于获取相关文档的检索器。此外，我们还可以自定义用于回答问题的 LLM
    提示。
- en: '[PRE7]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The most important part of the sales prompt is to prohibit the LLM from basing
    its responses without relying on official company resources. Remember, LLMs can
    act very assertively while providing invalid information. However, we would like
    to avoid that scenario and avoid getting into problems where the bot promised
    or sold non-existing features. We can test the sales question answering flow by
    asking the following question:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 销售提示中最重要的部分是禁止 LLM 在没有依赖官方公司资源的情况下进行回答。记住，LLMs 在提供无效信息时可能表现得非常自信。然而，我们希望避免这种情况，并避免出现机器人承诺或销售不存在的功能的问题。我们可以通过询问以下问题来测试销售问答流程：
- en: '![](../Images/51fd80bad01f9fae580506c01f7a2ca2.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/51fd80bad01f9fae580506c01f7a2ca2.png)'
- en: Sales question-answering. Image by the author.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 销售问答。作者图片。
- en: The response to the question seems relevant and accurate. Remember, the information
    to construct this response came from Medium articles.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 对于问题的回答似乎是相关且准确的。记住，构建此回答的信息来自 Medium 文章。
- en: Next, we will implement the support question-answering flow. Here, we will allow
    the LLM model to use its knowledge of Cypher and Neo4j to help solve the user’s
    problem if the context doesn’t provide enough information.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将实现支持问答流程。在这里，我们将允许LLM模型利用其对Cypher和Neo4j的知识来帮助解决用户的问题，前提是上下文信息不足。
- en: '[PRE8]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: And again, we can test the support question-answering abilities. I took a random
    question from Neo4j’s discord server.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，我们可以测试支持问答能力。我从Neo4j的Discord服务器上随机挑了一个问题。
- en: '![](../Images/7376d8dd9759b3e0de2dbb97c731112d.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7376d8dd9759b3e0de2dbb97c731112d.png)'
- en: Support question-answering. Image by the author
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 支持问答。图片由作者提供。
- en: The response is quite to the point. Remember, we retrieved the Graph Data Science
    documentation and are using it as context to form the chatbot questions.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 响应非常切题。请记住，我们检索了图数据科学文档，并将其作为上下文来形成聊天机器人问题。
- en: Agent implementation
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 代理实现
- en: We now have two separate instructions and stores for **sales** and **support**
    responses. If we had to put a human in the loop to distinguish between the two,
    the whole point of the chatbot would be lost. Luckily, we can use a LangChain
    agent to decide which tool to use based on the user input. First, we need to define
    the available tools of an agent along with instructions on when and how to use
    them.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在有两个独立的指令和存储，用于**销售**和**支持**回应。如果我们必须让人来区分这两者，那么聊天机器人的全部意义就会丧失。幸运的是，我们可以使用LangChain代理根据用户输入决定使用哪个工具。首先，我们需要定义代理的可用工具以及使用它们的时机和方式。
- en: '[PRE9]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The description of a tool is used by an agent to identify when and how to use
    a tool. For example, the support tool should be used to optimize or debug a Cypher
    statement and the input to the tool should be a fully formed question.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 工具的描述用于帮助代理识别何时以及如何使用工具。例如，支持工具应当用于优化或调试Cypher语句，而工具的输入应为一个完整的问题。
- en: The last thing we need to do is to initialize the agent.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要做的最后一件事是初始化代理。
- en: '[PRE10]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Now we can go ahead and test the agent on a couple of questions.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以继续测试代理在几个问题上的表现。
- en: '![](../Images/c41124f57d1518b2aa058f12e21ff37a.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c41124f57d1518b2aa058f12e21ff37a.png)'
- en: Sales agent example. Image by the author.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 销售代理示例。图片由作者提供。
- en: '![](../Images/6b5bda832a1d884337d9dd5258180510.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6b5bda832a1d884337d9dd5258180510.png)'
- en: Support agent example. Image by the author.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 支持代理示例。图片由作者提供。
- en: Remember, the main difference between the two QAs beside the context sources
    is that we allow the support QA to form answers that can’t be found in the provided
    context. On the other hand, we prohibit the sales QA from doing that to avoid
    any overpromising statements.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，除了上下文来源之外，这两种问答的主要区别在于，我们允许支持问答形成在提供的上下文中找不到的答案。另一方面，我们禁止销售问答这样做，以避免任何过度承诺的陈述。
- en: Summary
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 总结
- en: In the era of LLMs, you can develop a chatbot that uses company’s resources
    to answer questions in a single day thanks to LangChain library as it offers various
    document loaders as well as integration with popular LLM models. Therefore, the
    only thing you need to do is to collect company’s resources, import them into
    a vector database, and you are good to go. Just note that the implementation is
    not deterministic, which means you can get slightly different results on identical
    prompts. GPT-4 model is much better for more accurate and consistent responses.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在LLM时代，得益于LangChain库，您可以在一天内开发一个利用公司资源回答问题的聊天机器人，因为它提供了各种文档加载器以及与流行LLM模型的集成。因此，您只需收集公司的资源，将其导入到向量数据库中即可开始使用。请注意，实施并非确定性的，这意味着在相同提示下您可能会获得略微不同的结果。GPT-4模型在提供更准确和一致的回应方面要好得多。
- en: Let me know if you have any ideas or feedback regarding the chatbot implementation.
    As always, the code is available on [GitHub](https://github.com/tomasonjo/blogs/blob/master/neo4jdocs/neo4j_support_bot.ipynb).
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您对聊天机器人实现有任何想法或反馈，请告诉我。代码总是可以在[GitHub](https://github.com/tomasonjo/blogs/blob/master/neo4jdocs/neo4j_support_bot.ipynb)上找到。
