- en: Classification With Rosenblatt’s Perceptron
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用罗斯恩布拉特感知器进行分类
- en: 原文：[https://towardsdatascience.com/classification-with-rosenblatts-perceptron-e7f49e3af562?source=collection_archive---------3-----------------------#2023-09-09](https://towardsdatascience.com/classification-with-rosenblatts-perceptron-e7f49e3af562?source=collection_archive---------3-----------------------#2023-09-09)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/classification-with-rosenblatts-perceptron-e7f49e3af562?source=collection_archive---------3-----------------------#2023-09-09](https://towardsdatascience.com/classification-with-rosenblatts-perceptron-e7f49e3af562?source=collection_archive---------3-----------------------#2023-09-09)
- en: The “hello-world” of machine learning
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 机器学习的“hello-world”
- en: '[](https://medium.com/@cretanpan?source=post_page-----e7f49e3af562--------------------------------)[![Pan
    Cretan](../Images/8b3fbab70c0e61f7ca516d2f54b646e5.png)](https://medium.com/@cretanpan?source=post_page-----e7f49e3af562--------------------------------)[](https://towardsdatascience.com/?source=post_page-----e7f49e3af562--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----e7f49e3af562--------------------------------)
    [Pan Cretan](https://medium.com/@cretanpan?source=post_page-----e7f49e3af562--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@cretanpan?source=post_page-----e7f49e3af562--------------------------------)[![Pan
    Cretan](../Images/8b3fbab70c0e61f7ca516d2f54b646e5.png)](https://medium.com/@cretanpan?source=post_page-----e7f49e3af562--------------------------------)[](https://towardsdatascience.com/?source=post_page-----e7f49e3af562--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----e7f49e3af562--------------------------------)
    [潘·克里坦](https://medium.com/@cretanpan?source=post_page-----e7f49e3af562--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fff990ba57425&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fclassification-with-rosenblatts-perceptron-e7f49e3af562&user=Pan+Cretan&userId=ff990ba57425&source=post_page-ff990ba57425----e7f49e3af562---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----e7f49e3af562--------------------------------)
    ·8 min read·Sep 9, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fe7f49e3af562&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fclassification-with-rosenblatts-perceptron-e7f49e3af562&user=Pan+Cretan&userId=ff990ba57425&source=-----e7f49e3af562---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fff990ba57425&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fclassification-with-rosenblatts-perceptron-e7f49e3af562&user=Pan+Cretan&userId=ff990ba57425&source=post_page-ff990ba57425----e7f49e3af562---------------------post_header-----------)
    发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----e7f49e3af562--------------------------------)
    ·8分钟阅读·2023年9月9日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fe7f49e3af562&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fclassification-with-rosenblatts-perceptron-e7f49e3af562&user=Pan+Cretan&userId=ff990ba57425&source=-----e7f49e3af562---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fe7f49e3af562&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fclassification-with-rosenblatts-perceptron-e7f49e3af562&source=-----e7f49e3af562---------------------bookmark_footer-----------)![](../Images/34b5a03da018249e350885074adae59b.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fe7f49e3af562&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fclassification-with-rosenblatts-perceptron-e7f49e3af562&source=-----e7f49e3af562---------------------bookmark_footer-----------)![](../Images/34b5a03da018249e350885074adae59b.png)'
- en: Photo by [Lucie Morel](https://unsplash.com/@kaklahad?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 照片由 [Lucie Morel](https://unsplash.com/@kaklahad?utm_source=medium&utm_medium=referral)
    提供，刊登在 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: 'Recently I was pondering what could be the most basic introduction to machine
    learning. I was after a simple task, such as binary classification, and of an
    algorithm that is sufficiently simple that it could be built from scratch and
    explained within a short article. If the algorithm had some history even better.
    It did not take much time to find the candidate: the perceptron. The perceptron
    takes us to the beginning of machine learning. It was [introduced](https://blogs.umass.edu/brain-wars/files/2016/03/rosenblatt-1957.pdf)
    by Frank Rosenblatt more than 60 years ago. Like a neuron, the perceptron rule
    takes multiple input features and fits weights that when multiplied with the input
    feature vector allow taking a decision of whether the neuron outputs a signal
    or not, or in a machine learning classification context whether the output is
    0 or 1\. The perceptron is likely the simplest binary classifier and I am not
    aware of any practical machine learning use cases that could be tackled with it
    these days. However, it has significant educational and historical value as it
    paved the way to neural networks.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 最近我在思考什么是机器学习最基本的介绍。我寻找一个简单的任务，比如二分类，并且一个足够简单的算法，以便可以从头开始构建并在短文中解释。如果这个算法有一定的历史背景，那就更好了。很快我找到了候选者：感知机。感知机将我们带回机器学习的起点。它由Frank
    Rosenblatt在60多年前[引入](https://blogs.umass.edu/brain-wars/files/2016/03/rosenblatt-1957.pdf)。像神经元一样，感知机规则接受多个输入特征，并拟合权重，这些权重与输入特征向量相乘后决定神经元是否输出信号，或在机器学习分类上下文中是否输出0或1。感知机可能是最简单的二分类器，我不知道现在是否有任何实际的机器学习应用可以用它来解决。然而，它具有重要的教育和历史价值，因为它为神经网络铺平了道路。
- en: The purpose of this article is to introduce the perceptron and use it in a simple
    binary classification task. The perceptron has been [implemented](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Perceptron.html)
    in scikit-learn but instead of relying on this we will build it from scratch.
    We will also create a set of visualisations to understand how the algorithm sets
    its decision boundary and peak into its convergence. The perceptron is a linear
    model comprising weights and a bias term that are simultaneously and iteratively
    adjusted during the fitting process. However, it does not have a continuous loss
    function like its perhaps immediate successor in the history of machine learning,
    the adaptive linear neuron (Adaline) algorithm that is also a single-layer neural
    network. Fitting the perceptron simply relies on detecting misclassified samples,
    and the weights and bias are updated immediately as soon as a misclassified sample
    occurs and not once per epoch (epoch being a full pass through the training set).
    Hence, the algorithm does not even need an optimiser. I would dare to say that
    is so simple and elegant that it becomes beautiful. If you are curious to see
    how it works stay tuned!
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 本文的目的是介绍感知机并在一个简单的二分类任务中使用它。感知机已经在[scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Perceptron.html)中实现，但我们将从零开始构建它。我们还将创建一组可视化图表，以理解算法如何设定其决策边界并探究其收敛性。感知机是一个线性模型，包括权重和在拟合过程中同时和迭代调整的偏置项。然而，它不像机器学习历史上可能的直接继承者——自适应线性神经元（Adaline）算法那样具有连续的损失函数，后者也是一个单层神经网络。拟合感知机完全依赖于检测错误分类的样本，权重和偏置在每次错误分类样本出现时都会立即更新，而不是每个时期（时期为完整的训练集通过一次）。因此，该算法甚至不需要优化器。我敢说，它如此简单和优雅，以至于它变得美丽。如果你对它的工作原理感到好奇，请继续关注！
- en: Perceptron theory
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 感知机理论
- en: The perceptron, like other linear models, uses a set of weights, one for each
    of the features, and to generate a prediction it computes the dot product of the
    weights and the feature values and adds a bias
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 感知机，像其他线性模型一样，使用一组权重，每个特征一个权重，并通过计算权重与特征值的点积并添加偏置来生成预测。
- en: '![](../Images/aa0e10de077a11492d1dbdd3340d9a80.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/aa0e10de077a11492d1dbdd3340d9a80.png)'
- en: The result of this linear function, that is also known as net input, is fed
    into an activation function f(z) that in the case of the perceptron is a simple
    step function, i.e. f(z) takes the value of 1 if z≥0 and 0 otherwise. The role
    of the activation function is to map the net input to two values, namely 0 and
    1\. Essentially what we have done is nothing more that defining a hyperplane.
    Points that are at the same side of the hyperplane belong to the same class. The
    weights define the vector vertical to the hyperplane, i.e. the orientation of
    the hyperplane, and the bias the distance of the hyperplane from the origin. When
    the fitting process starts we have a randomly orientated hyperplane at a random
    distance from the origin. Whenever we encounter a misclassified sample we nudge
    the hyperplane a little bit and change its orientation and position so that in
    the next epoch the sample is at the right side of the hyperplane. We can decide
    how much to nudge the hyperplane, i.e. what the learning rate should be.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 这个线性函数的结果，也称为净输入，会被输入到激活函数 f(z) 中，在感知器的情况下，这个函数是一个简单的阶跃函数，即 f(z) 如果 z≥0 则取值
    1，否则取值 0。激活函数的作用是将净输入映射到两个值，即 0 和 1。实际上，我们所做的不过是定义了一个超平面。处于超平面同一侧的点属于同一类。权重定义了垂直于超平面的向量，即超平面的方向，而偏差则是超平面距离原点的距离。当拟合过程开始时，我们有一个随机定向的超平面，距离原点也是随机的。每当遇到一个错误分类的样本时，我们会稍微调整超平面，改变其方向和位置，以便在下一个周期中样本位于超平面的正确侧。我们可以决定调整超平面的幅度，即学习率应该是多少。
- en: 'Usually we need to pass through all samples a few items (epochs) until no point
    is misclassified, or to be more precise until no more progress can be made. In
    every epoch we loop over all samples i = 1,.., nₛₐₘₚₗₑₛ in the training set and
    using the current weights and bias we examine if the model misclassifies and if
    it does we update all weights j=1,.., nfₑₐₜᵤᵣₑₛ using a learning rate η:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 通常我们需要通过所有样本几个项目（周期），直到没有点被错误分类，或者更准确地说，直到没有更多的进展。每个周期，我们遍历训练集中所有样本 i = 1,..,
    nₛₐₘₚₗₑₛ，并使用当前的权重和偏差检查模型是否错误分类，如果是，我们将更新所有权重 j=1,.., nfₑₒₜᵤᵣₑₛ，使用学习率 η：
- en: '![](../Images/bd3c0d6e28353db089c1b01ab781b87d.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bd3c0d6e28353db089c1b01ab781b87d.png)'
- en: where
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 其中
- en: '![](../Images/c3e63aa4b7aabb1364403883b8b41c06.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c3e63aa4b7aabb1364403883b8b41c06.png)'
- en: with hat indicating the prediction output. We also update the bias with
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 带有帽子的符号表示预测输出。我们也会更新偏差
- en: '![](../Images/7177fe0e109bd0c1b63a8b7a9d1f2690.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7177fe0e109bd0c1b63a8b7a9d1f2690.png)'
- en: where
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 其中
- en: '![](../Images/a7d40e97070398c6a31fbec6a10e1b7d.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a7d40e97070398c6a31fbec6a10e1b7d.png)'
- en: It is conceptually easy to see why we do these operations. Assume that the model
    predicts class 0, whilst the correct one is 1\. If xⱼ is positive, then the weight
    will be increased so that the net input increases. If xⱼ is negative, then the
    weight will be reduced so that the net input once more increases (regardless of
    the sign of the weight). Similarly the bias will be increased leading to further
    increases of the net input. With these changes it is more likely to predict the
    correct class for the misclassified sample in the next epoch. The logic is similar
    when the model predicts class 1, whilst the correct one is 0, with the only difference
    that all signs are inverted.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 理解这些操作的概念上是简单的。假设模型预测了类 0，而正确的应该是 1。如果 xⱼ 是正数，那么权重将增加，从而使净输入增加。如果 xⱼ 是负数，那么权重将减少，以使净输入再次增加（无论权重的符号如何）。类似地，偏差将增加，从而进一步增加净输入。通过这些改变，更有可能在下一个周期中为错误分类的样本预测正确的类别。当模型预测类
    1，而正确的应该是 0 时，逻辑类似，唯一的区别是所有符号都会被反转。
- en: If you notice carefully, the weights and bias may be updated multiple times
    within the same epoch, once for every misclassified sample. Every misclassification
    reorients and repositions the decision boundary hyperplane so the sample is correctly
    predicted in the next epoch.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你仔细观察，权重和偏差可能会在同一个周期内更新多次，每次针对一个被错误分类的样本。每次错误分类都会重新调整和定位决策边界超平面，以便在下一个周期中正确预测样本。
- en: Data preparation
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据准备
- en: We will be using a synthetic data set comprising two Gaussian distributions.
    The perceptron can be used with features of any dimension but for the purposes
    of this article we will limit ourselves to two dimensions to facilitate visualisation.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用一个合成数据集，其中包含两个高斯分布。感知器可以与任何维度的特征一起使用，但为了本文的目的，我们将限制在两个维度，以便于可视化。
- en: that produces the following figure
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 产生如下图形
- en: '![](../Images/89c5745307f6e118107ca9db2371e8b1.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/89c5745307f6e118107ca9db2371e8b1.png)'
- en: Scatterplot of the two classes in the synthetic dataset. Image by the Author.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 合成数据集中两类的散点图。图片由作者提供。
- en: The two Gaussian distributions are elongated and put further apart on purpose
    by choosing appropriate means and covariances. We will come back to this later.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 两个高斯分布被故意拉长并进一步分开，通过选择适当的均值和协方差。我们稍后会再回到这个话题。
- en: Perceptron implementation and use
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 感知机实现及使用
- en: The implementation of the Perceptron can be found below. We use the scikit-learn
    style to initiate the model, fit it and finally run a prediction
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 感知机的实现见下文。我们使用scikit-learn风格来初始化模型、拟合它并最终进行预测。
- en: The initialisation method sets the learning rate, the maximum number of iterations
    and the random number generator seed for reproducibility purposes. The fit method
    creates a random number generator that is then used to set the weights to some
    small numbers sampled from a uniform distribution, whilst the bias is initiated
    to zero. We then iterate for a maximum number of epochs. For every epoch we count
    the number of misclassifications so that we can monitor convergence and terminate
    early if possible. For every sample that is misclassified we update the weights
    and bias as described in the previous section. If the number of misclassifications
    is zero then no further improvements can be made and hence there is no need to
    continue with the next epoch. The predict method is simply computing the dot product
    of the weights and feature values, adds the bias and applies the step function.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 初始化方法设置学习率、最大迭代次数和随机数生成器种子以确保可重复性。`fit`方法创建一个随机数生成器，然后用于将权重设置为从均匀分布中采样的一些小数，同时偏置初始化为零。然后我们迭代最多若干个时期。每个时期我们计算错误分类的数量，以便监控收敛情况，并在可能的情况下提前终止。对于每个错误分类的样本，我们更新权重和偏置，如前述部分所述。如果错误分类的数量为零，则无法进一步改进，因此无需继续下一个时期。`predict`方法只是计算权重和特征值的点积，添加偏置并应用步进函数。
- en: If we use the above Perceptron class with the synthetic data set
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们使用上述感知机类与合成数据集
- en: we can see that convergence was achieved in 24 epochs, i.e. it was not necessary
    to exhaust the maximum number of epochs specified
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到在24个时期内达到了收敛，即无需耗尽指定的最大时期数。
- en: '![](../Images/09ad39ab9b494840472363b5c97b0d97.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/09ad39ab9b494840472363b5c97b0d97.png)'
- en: Perceptron convergence. Image by the Author.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 感知机收敛情况。图片由作者提供。
- en: The decision boundary can be readily visualised using the decision boundary
    utility [function](https://scikit-learn.org/stable/modules/generated/sklearn.inspection.DecisionBoundaryDisplay.html)
    from scikit-learn. In order to use this function we generate a grid of 200x200
    points spanning the range of feature values in the training set. Essentially we
    construct a contour plot with the predicted class and overlay the samples as a
    scatterplot coloured using the true labels. This way of plotting the decision
    boundary is quite generic and can be useful with any two dimensional classifier.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用scikit-learn中的决策边界实用工具[函数](https://scikit-learn.org/stable/modules/generated/sklearn.inspection.DecisionBoundaryDisplay.html)来直观地展示决策边界。为了使用此函数，我们生成一个200x200的点网格，覆盖训练集中特征值的范围。本质上，我们构建了一个轮廓图，显示预测的类别，并将样本作为散点图叠加，使用真实标签进行着色。这种绘制决策边界的方式非常通用，可以适用于任何二维分类器。
- en: The two synthetically constructed Gaussian distributions have been perfectly
    separated using a model that could be coded from scratch in few lines of code.
    The simplicity and elegance of this method makes it a brilliant introductory and
    motivational example for machine learning.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个合成的高斯分布已经通过一个可以用几行代码从零开始编码的模型完美分开。这种方法的简洁性和优雅使它成为机器学习的一个出色的入门和激励示例。
- en: '![](../Images/8a1cb2fb933cc2387731f07c2b035f93.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8a1cb2fb933cc2387731f07c2b035f93.png)'
- en: Decision boundary of the fitted perceptron model. Image by the Author.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 拟合的感知机模型的决策边界。图片由作者提供。
- en: We can also visualise the evolution of the decision boundary in the different
    epochs by stopping the model fitting process prematurely. This can be practically
    done by fitting the model using an increasing cutoff for the maximum number of
    epochs. For every attempt we use the weights and bias of the fitted (possibly
    non converged) model and plot the decision boundary as a line. The lines are annotated
    using the epoch number. This could have been implemented more elegantly using
    a warm start, but fitting the model is very quick and hence it is not worthwhile
    the additional complexity.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以通过提前停止模型拟合过程来可视化不同时代决策边界的演变。实际上，可以通过使用递增的最大时代数来拟合模型来实现。对于每一次尝试，我们使用拟合（可能未收敛）的模型的权重和偏置，并将决策边界绘制为一条线。这些线条用时代编号进行注释。这本可以通过温启动实现得更优雅，但拟合模型非常快速，因此额外的复杂性不值得。
- en: The decision boundary evolution for the various epochs is shown in the figure
    below. Initially, a small number of class 0 samples is misclassified, which causes
    gradual changes to the slope and intercept of the decision boundary line. We can
    see that convergence was achieved in 24 epochs, that is consistent with the convergence
    figure above. The fitting stops once the decision boundary achieves perfect separation
    of the classes, regardless of how close the boundary is to the samples in its
    vicinity.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 各个时代的决策边界演变如下面的图所示。最初，一小部分类别0的样本被误分类，这导致决策边界线的斜率和截距逐渐变化。我们可以看到在24个时代内达到了收敛，这与上面的收敛图一致。一旦决策边界实现了对类别的完美分隔，拟合过程就会停止，无论边界距离其周围的样本有多近。
- en: '![](../Images/30816be78521827cdf3a59daa0e286a4.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/30816be78521827cdf3a59daa0e286a4.png)'
- en: A few notes of caution. Perceptron convergence cannot be taken for granted and
    for this reason it is important to set a maximum number of iterations. In fact,
    it can be [proven](https://web.mit.edu/course/other/i2course/www/vision_and_learning/perceptron_notes.pdf)
    mathematically that convergence is guaranteed for linearly separable classes.
    If the classes are not linearly separable then the weights and bias will keep
    being updated until the maximum number of iterations is reached. This is why the
    two Gaussians were moved further apart in the synthetic dataset.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 几点注意事项。感知器的收敛不能被视为理所当然，因此重要的是设置最大迭代次数。实际上，可以通过数学证明，对于线性可分的类别，收敛是有保证的。如果类别不是线性可分的，那么权重和偏置将不断更新，直到达到最大迭代次数。这就是为什么在合成数据集中两个高斯分布被进一步分开的原因。
- en: Another important note is that the perceptron has no unique solution. Typically
    there will be an infinite number of hyperplanes that can separate linearly separable
    classes and the model will randomly converge to one of them. This also means that
    measuring the distance from the decision boundary is not deterministic and hence
    not so useful. Support-vector machines address this limitation.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个重要的注意点是感知器没有唯一的解决方案。通常，会有无数个超平面可以分隔线性可分的类别，模型将随机收敛到其中一个。这也意味着测量决策边界的距离不是确定性的，因此不是特别有用。支持向量机解决了这个限制。
- en: The perceptron is essentially a single layer neural network. Understanding how
    it works is helpful before jumping to multi-layered neural networks and the backpropagation
    algorithm that can be used for non-linear problems.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 感知器本质上是一个单层神经网络。在跳到多层神经网络和可以用于非线性问题的反向传播算法之前，理解它的工作原理是有帮助的。
