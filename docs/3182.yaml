- en: How to Measure the Success of Your RAG-based LLM System
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何衡量您的基于 RAG 的 LLM 系统的成功
- en: 原文：[https://towardsdatascience.com/how-to-measure-the-success-of-your-rag-based-llm-system-874a232b27eb?source=collection_archive---------0-----------------------#2023-10-23](https://towardsdatascience.com/how-to-measure-the-success-of-your-rag-based-llm-system-874a232b27eb?source=collection_archive---------0-----------------------#2023-10-23)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/how-to-measure-the-success-of-your-rag-based-llm-system-874a232b27eb?source=collection_archive---------0-----------------------#2023-10-23](https://towardsdatascience.com/how-to-measure-the-success-of-your-rag-based-llm-system-874a232b27eb?source=collection_archive---------0-----------------------#2023-10-23)
- en: Use the machines to grade the machines
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用机器来评估机器
- en: Including a new novel method for judging answers with a qualitative score and
    detailed explanation.
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 包括一种新的定性评分方法和详细解释。
- en: '[](https://medium.com/@ccrngd1?source=post_page-----874a232b27eb--------------------------------)[![Nicholaus
    Lawson](../Images/5c19c9bea340514d6839fdd9f5a485aa.png)](https://medium.com/@ccrngd1?source=post_page-----874a232b27eb--------------------------------)[](https://towardsdatascience.com/?source=post_page-----874a232b27eb--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----874a232b27eb--------------------------------)
    [Nicholaus Lawson](https://medium.com/@ccrngd1?source=post_page-----874a232b27eb--------------------------------)'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@ccrngd1?source=post_page-----874a232b27eb--------------------------------)[![Nicholaus
    Lawson](../Images/5c19c9bea340514d6839fdd9f5a485aa.png)](https://medium.com/@ccrngd1?source=post_page-----874a232b27eb--------------------------------)[](https://towardsdatascience.com/?source=post_page-----874a232b27eb--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----874a232b27eb--------------------------------)
    [Nicholaus Lawson](https://medium.com/@ccrngd1?source=post_page-----874a232b27eb--------------------------------)'
- en: ·
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F87247d6fe280&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-measure-the-success-of-your-rag-based-llm-system-874a232b27eb&user=Nicholaus+Lawson&userId=87247d6fe280&source=post_page-87247d6fe280----874a232b27eb---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----874a232b27eb--------------------------------)
    ·11 min read·Oct 23, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F874a232b27eb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-measure-the-success-of-your-rag-based-llm-system-874a232b27eb&user=Nicholaus+Lawson&userId=87247d6fe280&source=-----874a232b27eb---------------------clap_footer-----------)'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F87247d6fe280&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-measure-the-success-of-your-rag-based-llm-system-874a232b27eb&user=Nicholaus+Lawson&userId=87247d6fe280&source=post_page-87247d6fe280----874a232b27eb---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----874a232b27eb--------------------------------)
    ·11分钟阅读·2023年10月23日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F874a232b27eb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-measure-the-success-of-your-rag-based-llm-system-874a232b27eb&user=Nicholaus+Lawson&userId=87247d6fe280&source=-----874a232b27eb---------------------clap_footer-----------)'
- en: --
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F874a232b27eb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-measure-the-success-of-your-rag-based-llm-system-874a232b27eb&source=-----874a232b27eb---------------------bookmark_footer-----------)![](../Images/3bb2a6412f41e639ce0cc4167f345371.png)'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F874a232b27eb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-measure-the-success-of-your-rag-based-llm-system-874a232b27eb&source=-----874a232b27eb---------------------bookmark_footer-----------)![](../Images/3bb2a6412f41e639ce0cc4167f345371.png)'
- en: Image generated by Stable Diffusion XL
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 由 Stable Diffusion XL 生成的图像
- en: Research Augmented Generation, or RAG, is easily the most common use case for
    Large Language Models (LLMs) that have emerged this year. While text summarization
    and generation are often the focus of individual users, businesses have realized
    that they need the ability to use their data to leverage this technology. Reflecting
    on how I still use LLMs, text generation is high on the list. I want to ask questions
    to Bard and have it search the web; I want Claude to rewrite emails or blog posts
    to punch up my content. But the most exciting use I have encountered is piping
    my own data into the LLM. I want to search my notes, emails, calendar, and Slack
    messages and have Llama function as another me (but a me that can remember details
    of things that happened before today).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 增强生成研究，或RAG，是今年出现的大型语言模型（LLMs）最常见的使用案例。尽管文本摘要和生成通常是个别用户的重点，但企业已经意识到他们需要利用自己的数据来利用这项技术。回顾我仍然使用LLM的情况，文本生成位居高位。我想问Bard问题并让它搜索网络；我希望Claude重写电子邮件或博客文章，以增强我的内容。但我遇到的最令人兴奋的使用是将我自己的数据输入LLM。我想搜索我的笔记、电子邮件、日历和Slack消息，并让Llama充当另一个我（但这个我能记住今天之前发生的细节）。
- en: This isn’t a post about how to build a RAG (there are many of those out there
    already…and I am working on that post for another day). What we will explore today
    is how to evaluate RAG systems.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 这不是关于如何构建RAG的帖子（已经有很多这样的帖子了……我还在为其他时间写这篇帖子）。今天我们将探讨的是如何评估RAG系统。
- en: How do we get data out of a RAG?
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 我们如何从RAG中获取数据？
- en: Let’s level set before we get into the weeds. When we talk about a RAG, there
    are two parts of the system we mean.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入细节之前，我们先设定一个基础。当我们谈论RAG时，我们指的是系统的两个部分。
- en: The knowledge source
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 知识源
- en: A knowledge source can be a vector database, a search engine, a few text files
    loaded into memory, SQL data, or anything where our data is stored.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 知识源可以是向量数据库、搜索引擎、加载到内存中的几个文本文件、SQL 数据，或任何存储我们数据的地方。
- en: The LLM
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: LLM
- en: Once we have our data, we pipe that into the LLM. This is done through the context
    window. So, ultimately, we search, get some text, stuff that found text into a
    prompt, and pass our question to the LLM. The model then takes everything from
    that context window and provides an answer.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们有了数据，我们将其输入到LLM中。这是通过上下文窗口完成的。因此，*最终*，我们进行搜索，得到一些文本，将找到的文本放入提示中，然后将问题传递给LLM。模型随后从该上下文窗口中获取所有内容并提供答案。
- en: Why does this matter?
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为什么这很重要？
- en: When we talk about evaluating an RAG system, we have to know what we will evaluate
    before defining how to evaluate it. We can see now that two pieces need to be
    examined. The initial data retrieval is the most critical piece here. LLMs, generally,
    are great about summarizing/question answering with data supplied in the context.
    What might be lacking is the search functionality itself.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们谈论评估RAG系统时，我们必须知道在定义如何评估之前我们要评估什么。我们现在可以看到需要检查两个部分。初始数据检索是最关键的部分。LLM通常在总结/回答问题方面表现出色，前提是提供了上下文数据。可能缺乏的部分是搜索功能本身。
- en: These knowledge sources have some built-in limitations. For instance, when using
    vector databases to store large text files, you have to ‘chunk’ your data going
    in. What does this mean? Let’s say you have a 100-page document, but the database
    can only handle saving 1 page at a time. Once you load your documents up and go
    to search, the database can only examine a single page at a time (ok, this is
    a little reductionist, but bear with me; it’s close enough for government work).
    When we find data that matches our search, there is a genuine possibility that
    the entire answer to our question doesn’t live on that single page. Too bad! We
    only get a single page back! This is a good illustration of why there is a need
    to examine this part of the system before worrying about the output from the LLM.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 这些知识源有一些内建的限制。例如，当使用向量数据库存储大型文本文件时，你必须对数据进行‘切块’处理。这是什么意思？假设你有一个100页的文档，但数据库一次只能处理1页。加载文档并进行搜索时，数据库只能逐页检查（好吧，这有点简化，但请耐心一点；这对于实际应用已经足够接近了）。当我们找到匹配的搜索结果时，确实有可能整个问题的答案不在这一页上。真遗憾！我们只能得到一页！这很好地说明了在担心LLM的输出之前需要检查系统这一部分的原因。
- en: What do we need to evaluate?
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 我们需要评估什么？
- en: Evaluating the initial search
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 评估初始搜索
- en: This isn’t going to be the answer that most technologists want to hear. Some
    level of human evaluation will be required to assess the results out of your knowledge
    source.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能不是大多数技术专家想听到的答案。评估结果需要一定程度的人为评估。
- en: Why? Well, if a business is using its data and it is private, it will be hard
    to automate tests to verify the search results are wholly accurate. Don’t fret,
    it doesn’t have to be 100% manual; we can automate parts of it. Let’s dig a little
    deeper.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么？如果一个企业使用的是私有数据，自动化测试来验证搜索结果是否完全准确将很困难。不要担心，这不必完全手动；我们可以自动化其中的一部分。让我们深入了解一下。
- en: There are two implementations I see for this initial validation and evaluation.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 我看到有两种实现方式用于初步验证和评估。
- en: The first option is to have a set of common and expected questions for the data
    set and have a human QA team verify the search results. For example, if your team
    is tasked with building a customer service Q&A bot for a bank, some common questions
    might be, ‘What is the minimum amount I am required to keep in my account?’, ‘How
    do I make a payment on my loan?’, ‘What time is my branch open?’. It’s ideal if
    you’re QAs can supply both the questions and the expected answers in something
    like a CSV file that can be read programmatically; then, we can use some of our
    automated tests that we will cover a bit further down in this post.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个选项是为数据集设置一组常见的预期问题，并让人工QA团队验证搜索结果。例如，如果你的团队负责为银行构建一个客服问答机器人，一些常见问题可能是：“我必须在账户中保持的最低金额是多少？”，“我怎么还款？”，“我的分行几点开门？”。如果你的QA能够提供问题和预期答案，并以类似CSV文件的形式提供，这样可以程序化读取，那就最好不过了；然后，我们可以使用一些自动化测试，稍后会在本文中进一步介绍。
- en: If the time or resources are not available for this, the second method has a
    QA team search and review in real time. This is an option for early POCs and prototypes,
    but beware, this won’t scale for actual production workloads.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 如果没有时间或资源，第二种方法是QA团队实时搜索和审查。这是早期POC和原型的一个选项，但请注意，这不能扩展到实际生产工作负载中。
- en: Evaluating the LLM responses
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 评估LLM响应
- en: Once we have a level of comfort that the data from our knowledge source is reliable,
    we must ensure that the final answers are accurate. RAG systems are great for
    reducing the possibility of hallucinations, and this can be extended by tweaking
    the underlying prompt. However, it may leave out information, misunderstand the
    data fed to it, or try to bring in apriori knowledge from its training.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们对知识来源的数据的可靠性感到放心，我们必须确保最终答案的准确性。RAG系统在减少幻觉的可能性方面表现出色，这可以通过调整底层提示来进一步延伸。然而，它可能会遗漏信息、误解输入的数据，或试图引入训练中的先验知识。
- en: Evaluating this step is similar to assessing the search before it. If QA teams
    can provide questions and expected answers, we can attempt to gauge the answers
    programmatically.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 评估这一步骤类似于评估之前的搜索。如果QA团队可以提供问题和预期答案，我们可以尝试程序化地评估答案。
- en: Let’s look at some of those options now.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看看其中的一些选项。
- en: Evaluation frameworks
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评估框架
- en: It’s essential to remember that LLMs and RAGs are very early in their maturity
    cycle. It has only been a year since ChatGPT debuted, and every day brings more
    advancements, models, frameworks, and research in this field. That being said,
    a handful of metrics are becoming the standard way to measure the performance
    of these systems.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 需要记住的是，LLM和RAG在其成熟周期中还处于早期阶段。自ChatGPT首次亮相以来仅过去了一年，每天都有更多的进展、模型、框架和研究。尽管如此，一些指标正成为衡量这些系统性能的标准方式。
- en: We won’t cover ways to evaluate the base LLM. There are things like ARC, MMLU,
    HellaSwag, etc, that all target the underlying language model. There isn’t a need
    to run these measures yourself; you can check sites such as
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不会涵盖评估基础LLM的方法。像ARC、MMLU、HellaSwag等都针对底层语言模型。你不需要自己运行这些指标；可以查看像这样的站点
- en: '[https://llm-leaderboard.streamlit.app/](https://llm-leaderboard.streamlit.app/)
    and [https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard)'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://llm-leaderboard.streamlit.app/](https://llm-leaderboard.streamlit.app/)
    和 [https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard)'
- en: to see how different models fare. We are only interested in measuring the results
    we get out of the RAG systems.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 查看不同模型的表现。我们只对从RAG系统中得到的结果感兴趣。
- en: That leads us to look at algorithms like ROUGE, BLEU, BLUERT, and METEOR. Let’s
    take a closer look at each. I’ll also include a small code snippet for how you
    call each metric and what the output score looks like. I import the eval framework
    to get started and include the reference and answer I want to score.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 这引导我们关注像 ROUGE、BLEU、BLEURT 和 METEOR 这样的算法。让我们逐一详细了解每个指标。我还会附上一小段代码，展示如何调用每个指标以及输出分数的样子。我引入评估框架以便开始，并包括了我想要评分的参考和答案。
- en: '[PRE0]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '[ROUGE](https://huggingface.co/spaces/evaluate-metric/rouge) (Recall-Oriented
    Understudy for Gisting Evaluation)'
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '[ROUGE](https://huggingface.co/spaces/evaluate-metric/rouge)（召回导向的摘要评估指标）'
- en: ROUGE is a set of metrics for evaluating automatic summarization and machine-translation
    output. It is based on the count of overlapping n-grams between the system output
    and the reference summaries.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: ROUGE 是一组用于评估自动摘要和机器翻译输出的指标。它基于系统输出与参考摘要之间的重叠 n-gram 的计数。
- en: '[PRE1]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '[PRE2]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[BLEU](https://huggingface.co/spaces/evaluate-metric/bleu) (Bilingual Evaluation
    Understudy)'
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '[BLEU](https://huggingface.co/spaces/evaluate-metric/bleu)（双语评估指标）'
- en: BLEU is a metric for the automatic evaluation of machine-translation output.
    It is based on the n-gram precision of the candidate translation against a set
    of reference translations.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: BLEU 是一种用于自动评估机器翻译输出的指标。它基于候选翻译与一组参考翻译的 n-gram 精度。
- en: '[PRE3]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[PRE4]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[BLEURT](https://huggingface.co/spaces/evaluate-metric/bleurt) (BLEU Regression
    with Transformers)'
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '[BLEURT](https://huggingface.co/spaces/evaluate-metric/bleurt)（基于变换器的 BLEU
    回归）'
- en: BLEURT is an evaluation metric for Natural Language Generation (NLG). It is
    based on the BERT, which allows BLEURT to learn the statistical relationships
    between words and phrases and to identify patterns in NLG output.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: BLEURT 是一种自然语言生成（NLG）的评估指标。它基于 BERT，使得 BLEURT 能够学习词汇和短语之间的统计关系，并识别 NLG 输出中的模式。
- en: BLEURT has been shown to outperform other NLG evaluation metrics, such as BLEU
    and ROUGE, on a variety of tasks, including machine translation, summarization,
    and question-answering.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: BLEURT 已被证明在各种任务上优于其他自然语言生成（NLG）评估指标，如 BLEU 和 ROUGE，包括机器翻译、文本摘要和问答系统。
- en: '[PRE5]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '[PRE6]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '[METEOR](https://huggingface.co/spaces/evaluate-metric/meteor) (Metric for
    Evaluation of Translation with Explicit ORdering)'
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '[METEOR](https://huggingface.co/spaces/evaluate-metric/meteor)（显式排序翻译评估指标）'
- en: METEOR is an automatic evaluation metric for machine-translation output. It
    also has features not found in other metrics, such as stemming, synonymy matching,
    and the standard exact word matching. The metric was designed to fix some of the
    problems encountered in the more popular BLEU metric and also produce a good correlation
    with human judgment at the sentence or segment level.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: METEOR 是一种用于机器翻译输出的自动评估指标。它还具有其他指标所没有的特性，如词干提取、同义词匹配和标准的精确词汇匹配。该指标旨在解决更流行的 BLEU
    指标遇到的一些问题，并在句子或片段级别上与人工判断产生良好的相关性。
- en: '[PRE7]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '[PRE8]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: I was promised something new!
  id: totrans-56
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 我被承诺会有新东西！
- en: While I have your attention, though, I want to introduce a new idea. While those
    four algorithms will give you a quantifiable score that allows your QA team to
    quickly determine whether an answer/summary is similar, there are some shortcomings.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我已经引起了你的注意，但我想介绍一个新的想法。虽然这四种算法会给出一个量化的分数，让你的 QA 团队能够快速确定答案/摘要是否相似，但仍存在一些不足之处。
- en: First, the reference sentences and result may be similar enough to answer the
    question from users, but it can still receive a poor score. It is essential to
    run a known set of questions and answers to establish a good baseline and compare
    future answers against this baseline.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，参考句子和结果可能足够相似以回答用户的问题，但它仍然可能获得较低的分数。运行一组已知的问题和答案以建立良好的基准并将未来的答案与该基准进行比较是至关重要的。
- en: Second, it doesn’t tell you why the score suffers. Is it because there is a
    penalty for repeating words? Is it because some words are missing? Did the summary
    altogether leave out an essential piece of the answer? There isn’t a way to tell.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，它无法告诉你分数为何受损。是因为重复词汇会受到惩罚吗？还是因为缺少了一些词汇？摘要是否完全遗漏了答案中的重要部分？没有办法得知。
- en: Lastly, just because a response receives a low score doesn’t necessarily mean
    a human would view the answer as insufficient or incorrect. The baseline can be
    helpful here to establish what acceptable scores may look like, but it’s important
    to have some skepticism when using these for judging RAG answers.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，仅仅因为一个回答获得低分并不一定意味着人类会认为这个回答不足或错误。基准在这里可能有帮助，用于确定什么是可接受的评分，但在使用这些评分来判断RAG答案时，保持一些怀疑态度是重要的。
- en: LLMs grading LLMs
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: LLMs评分LLMs
- en: BLEURT has introduced us to the idea that we can use LLMs in some way to gauge
    answers from an RAG system. What if we leverage this directly ourselves? We instruct
    an LLM to give a qualitative score for our answer and provide both bulleted reasons
    and a narrative explanation of the score it assigned. This gives us the best of
    both worlds. We can extract a numerical score to report to users and QA in a report;
    we can also supply more detail about why an answer scored badly.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: BLEURT向我们介绍了我们可以以某种方式使用LLMs来评估RAG系统的答案。如果我们直接利用这一点呢？我们指示LLM给我们的答案一个定性评分，并提供列点的理由和评分的叙述解释。这让我们兼得两全其美的效果。我们可以提取一个数值评分来报告给用户和QA；我们也可以提供更多细节来解释为什么答案得分低。
- en: Here is a sample prompt template that can be used for ClaudeV2\. We pass instructions
    on how we want the model to score our answer, pass in the reference data, and
    pass in the answer we received from our RAG system.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是一个可以用于ClaudeV2的示例提示模板。我们传递关于如何希望模型评分我们答案的指令，传递参考数据，并传递我们从RAG系统收到的答案。
- en: '[PRE9]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '[PRE10]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: There we are. If teams can provide expected answers, we can feed the RAG answers
    back into an LLM to have them graded. The benefits are that we don’t have to rely
    on the LLM's apriori knowledge since we are still piping in the relevant data.
    We can use a different LLM than the one used in the RAG system, meaning we can
    even ask multiple models to grade our output to ensure we have a balanced assessment.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样。如果团队能提供预期的答案，我们可以将RAG答案反馈到LLM中进行评分。好处是我们不必依赖LLM的先验知识，因为我们仍在传输相关数据。我们可以使用与RAG系统不同的LLM，这意味着我们甚至可以让多个模型来评分我们的输出，以确保评估的平衡性。
- en: This method also gives us an excellent explanation of what was wrong. In this
    example, I had a question about what kinds of dragons existed in the DND universe.
    The judging LLM correctly identified that it didn’t mention chromatic dragons.
    However, it also dinged the answer for not including the ages of Dragons, the
    DND Monster Manual, or the expansion adventure. Those omissions weren’t important
    to the question I asked, but this allows QA teams to decide for themselves once.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法还给我们提供了出错的优秀解释。在这个例子中，我问了关于DND宇宙中存在哪些龙的问题。评判LLM正确识别出它没有提到色彩龙。然而，它还因为没有包括龙的年代、DND怪物手册或扩展冒险而扣分。这些遗漏对我提出的问题并不重要，但这允许QA团队自行决定一次。
- en: Where do we go now?
  id: totrans-68
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 接下来我们怎么做？
- en: RAG-based systems and the frameworks used to create them are advancing every
    day. New ways and mechanisms for grading them will continue to advance as well.
    There are even tools from giants like LangChain that can aid in this task, such
    as [LangSmith](https://www.langchain.com/langsmith) .
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 基于RAG的系统及其创建框架每天都在进步。评分它们的新方法和机制也会不断进步。甚至有来自像LangChain这样的巨头的工具可以帮助完成这个任务，例如[LangSmith](https://www.langchain.com/langsmith)。
- en: While we wait for more advancements, using a combination of some manual validation
    data and either the HuggingFace metrics library or LLMs themselves gives us a
    great way to begin trusting these systems.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们等待更多进展的同时，结合一些手动验证数据和HuggingFace度量库或LLMs本身，给我们提供了一个很好的开始信任这些系统的方法。
- en: Remember, once you have confidence and are ready to deploy your new RAG into
    production, the evaluation of the answers doesn’t stop! As part of routine monitoring
    and auditing efforts, you must continue storing questions and answers and plan
    for a human-in-the-loop effort to grade and flag answers supplied to end users.
    That, however, is a topic for another day.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，一旦你对你的新RAG充满信心并准备投入生产，答案的评估不会停止！作为常规监控和审计工作的一部分，你必须继续存储问题和答案，并计划进行人工评估工作，以评分和标记提供给最终用户的答案。然而，这个话题另说。
