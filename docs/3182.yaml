- en: How to Measure the Success of Your RAG-based LLM System
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/how-to-measure-the-success-of-your-rag-based-llm-system-874a232b27eb?source=collection_archive---------0-----------------------#2023-10-23](https://towardsdatascience.com/how-to-measure-the-success-of-your-rag-based-llm-system-874a232b27eb?source=collection_archive---------0-----------------------#2023-10-23)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Use the machines to grade the machines
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Including a new novel method for judging answers with a qualitative score and
    detailed explanation.
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@ccrngd1?source=post_page-----874a232b27eb--------------------------------)[![Nicholaus
    Lawson](../Images/5c19c9bea340514d6839fdd9f5a485aa.png)](https://medium.com/@ccrngd1?source=post_page-----874a232b27eb--------------------------------)[](https://towardsdatascience.com/?source=post_page-----874a232b27eb--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----874a232b27eb--------------------------------)
    [Nicholaus Lawson](https://medium.com/@ccrngd1?source=post_page-----874a232b27eb--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F87247d6fe280&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-measure-the-success-of-your-rag-based-llm-system-874a232b27eb&user=Nicholaus+Lawson&userId=87247d6fe280&source=post_page-87247d6fe280----874a232b27eb---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----874a232b27eb--------------------------------)
    ·11 min read·Oct 23, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F874a232b27eb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-measure-the-success-of-your-rag-based-llm-system-874a232b27eb&user=Nicholaus+Lawson&userId=87247d6fe280&source=-----874a232b27eb---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F874a232b27eb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-measure-the-success-of-your-rag-based-llm-system-874a232b27eb&source=-----874a232b27eb---------------------bookmark_footer-----------)![](../Images/3bb2a6412f41e639ce0cc4167f345371.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Image generated by Stable Diffusion XL
  prefs: []
  type: TYPE_NORMAL
- en: Research Augmented Generation, or RAG, is easily the most common use case for
    Large Language Models (LLMs) that have emerged this year. While text summarization
    and generation are often the focus of individual users, businesses have realized
    that they need the ability to use their data to leverage this technology. Reflecting
    on how I still use LLMs, text generation is high on the list. I want to ask questions
    to Bard and have it search the web; I want Claude to rewrite emails or blog posts
    to punch up my content. But the most exciting use I have encountered is piping
    my own data into the LLM. I want to search my notes, emails, calendar, and Slack
    messages and have Llama function as another me (but a me that can remember details
    of things that happened before today).
  prefs: []
  type: TYPE_NORMAL
- en: This isn’t a post about how to build a RAG (there are many of those out there
    already…and I am working on that post for another day). What we will explore today
    is how to evaluate RAG systems.
  prefs: []
  type: TYPE_NORMAL
- en: How do we get data out of a RAG?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let’s level set before we get into the weeds. When we talk about a RAG, there
    are two parts of the system we mean.
  prefs: []
  type: TYPE_NORMAL
- en: The knowledge source
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A knowledge source can be a vector database, a search engine, a few text files
    loaded into memory, SQL data, or anything where our data is stored.
  prefs: []
  type: TYPE_NORMAL
- en: The LLM
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Once we have our data, we pipe that into the LLM. This is done through the context
    window. So, ultimately, we search, get some text, stuff that found text into a
    prompt, and pass our question to the LLM. The model then takes everything from
    that context window and provides an answer.
  prefs: []
  type: TYPE_NORMAL
- en: Why does this matter?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When we talk about evaluating an RAG system, we have to know what we will evaluate
    before defining how to evaluate it. We can see now that two pieces need to be
    examined. The initial data retrieval is the most critical piece here. LLMs, generally,
    are great about summarizing/question answering with data supplied in the context.
    What might be lacking is the search functionality itself.
  prefs: []
  type: TYPE_NORMAL
- en: These knowledge sources have some built-in limitations. For instance, when using
    vector databases to store large text files, you have to ‘chunk’ your data going
    in. What does this mean? Let’s say you have a 100-page document, but the database
    can only handle saving 1 page at a time. Once you load your documents up and go
    to search, the database can only examine a single page at a time (ok, this is
    a little reductionist, but bear with me; it’s close enough for government work).
    When we find data that matches our search, there is a genuine possibility that
    the entire answer to our question doesn’t live on that single page. Too bad! We
    only get a single page back! This is a good illustration of why there is a need
    to examine this part of the system before worrying about the output from the LLM.
  prefs: []
  type: TYPE_NORMAL
- en: What do we need to evaluate?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Evaluating the initial search
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This isn’t going to be the answer that most technologists want to hear. Some
    level of human evaluation will be required to assess the results out of your knowledge
    source.
  prefs: []
  type: TYPE_NORMAL
- en: Why? Well, if a business is using its data and it is private, it will be hard
    to automate tests to verify the search results are wholly accurate. Don’t fret,
    it doesn’t have to be 100% manual; we can automate parts of it. Let’s dig a little
    deeper.
  prefs: []
  type: TYPE_NORMAL
- en: There are two implementations I see for this initial validation and evaluation.
  prefs: []
  type: TYPE_NORMAL
- en: The first option is to have a set of common and expected questions for the data
    set and have a human QA team verify the search results. For example, if your team
    is tasked with building a customer service Q&A bot for a bank, some common questions
    might be, ‘What is the minimum amount I am required to keep in my account?’, ‘How
    do I make a payment on my loan?’, ‘What time is my branch open?’. It’s ideal if
    you’re QAs can supply both the questions and the expected answers in something
    like a CSV file that can be read programmatically; then, we can use some of our
    automated tests that we will cover a bit further down in this post.
  prefs: []
  type: TYPE_NORMAL
- en: If the time or resources are not available for this, the second method has a
    QA team search and review in real time. This is an option for early POCs and prototypes,
    but beware, this won’t scale for actual production workloads.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating the LLM responses
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Once we have a level of comfort that the data from our knowledge source is reliable,
    we must ensure that the final answers are accurate. RAG systems are great for
    reducing the possibility of hallucinations, and this can be extended by tweaking
    the underlying prompt. However, it may leave out information, misunderstand the
    data fed to it, or try to bring in apriori knowledge from its training.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating this step is similar to assessing the search before it. If QA teams
    can provide questions and expected answers, we can attempt to gauge the answers
    programmatically.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s look at some of those options now.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluation frameworks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It’s essential to remember that LLMs and RAGs are very early in their maturity
    cycle. It has only been a year since ChatGPT debuted, and every day brings more
    advancements, models, frameworks, and research in this field. That being said,
    a handful of metrics are becoming the standard way to measure the performance
    of these systems.
  prefs: []
  type: TYPE_NORMAL
- en: We won’t cover ways to evaluate the base LLM. There are things like ARC, MMLU,
    HellaSwag, etc, that all target the underlying language model. There isn’t a need
    to run these measures yourself; you can check sites such as
  prefs: []
  type: TYPE_NORMAL
- en: '[https://llm-leaderboard.streamlit.app/](https://llm-leaderboard.streamlit.app/)
    and [https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard)'
  prefs: []
  type: TYPE_NORMAL
- en: to see how different models fare. We are only interested in measuring the results
    we get out of the RAG systems.
  prefs: []
  type: TYPE_NORMAL
- en: That leads us to look at algorithms like ROUGE, BLEU, BLUERT, and METEOR. Let’s
    take a closer look at each. I’ll also include a small code snippet for how you
    call each metric and what the output score looks like. I import the eval framework
    to get started and include the reference and answer I want to score.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '[ROUGE](https://huggingface.co/spaces/evaluate-metric/rouge) (Recall-Oriented
    Understudy for Gisting Evaluation)'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: ROUGE is a set of metrics for evaluating automatic summarization and machine-translation
    output. It is based on the count of overlapping n-grams between the system output
    and the reference summaries.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '[BLEU](https://huggingface.co/spaces/evaluate-metric/bleu) (Bilingual Evaluation
    Understudy)'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: BLEU is a metric for the automatic evaluation of machine-translation output.
    It is based on the n-gram precision of the candidate translation against a set
    of reference translations.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '[BLEURT](https://huggingface.co/spaces/evaluate-metric/bleurt) (BLEU Regression
    with Transformers)'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: BLEURT is an evaluation metric for Natural Language Generation (NLG). It is
    based on the BERT, which allows BLEURT to learn the statistical relationships
    between words and phrases and to identify patterns in NLG output.
  prefs: []
  type: TYPE_NORMAL
- en: BLEURT has been shown to outperform other NLG evaluation metrics, such as BLEU
    and ROUGE, on a variety of tasks, including machine translation, summarization,
    and question-answering.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '[METEOR](https://huggingface.co/spaces/evaluate-metric/meteor) (Metric for
    Evaluation of Translation with Explicit ORdering)'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: METEOR is an automatic evaluation metric for machine-translation output. It
    also has features not found in other metrics, such as stemming, synonymy matching,
    and the standard exact word matching. The metric was designed to fix some of the
    problems encountered in the more popular BLEU metric and also produce a good correlation
    with human judgment at the sentence or segment level.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: I was promised something new!
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While I have your attention, though, I want to introduce a new idea. While those
    four algorithms will give you a quantifiable score that allows your QA team to
    quickly determine whether an answer/summary is similar, there are some shortcomings.
  prefs: []
  type: TYPE_NORMAL
- en: First, the reference sentences and result may be similar enough to answer the
    question from users, but it can still receive a poor score. It is essential to
    run a known set of questions and answers to establish a good baseline and compare
    future answers against this baseline.
  prefs: []
  type: TYPE_NORMAL
- en: Second, it doesn’t tell you why the score suffers. Is it because there is a
    penalty for repeating words? Is it because some words are missing? Did the summary
    altogether leave out an essential piece of the answer? There isn’t a way to tell.
  prefs: []
  type: TYPE_NORMAL
- en: Lastly, just because a response receives a low score doesn’t necessarily mean
    a human would view the answer as insufficient or incorrect. The baseline can be
    helpful here to establish what acceptable scores may look like, but it’s important
    to have some skepticism when using these for judging RAG answers.
  prefs: []
  type: TYPE_NORMAL
- en: LLMs grading LLMs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: BLEURT has introduced us to the idea that we can use LLMs in some way to gauge
    answers from an RAG system. What if we leverage this directly ourselves? We instruct
    an LLM to give a qualitative score for our answer and provide both bulleted reasons
    and a narrative explanation of the score it assigned. This gives us the best of
    both worlds. We can extract a numerical score to report to users and QA in a report;
    we can also supply more detail about why an answer scored badly.
  prefs: []
  type: TYPE_NORMAL
- en: Here is a sample prompt template that can be used for ClaudeV2\. We pass instructions
    on how we want the model to score our answer, pass in the reference data, and
    pass in the answer we received from our RAG system.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: There we are. If teams can provide expected answers, we can feed the RAG answers
    back into an LLM to have them graded. The benefits are that we don’t have to rely
    on the LLM's apriori knowledge since we are still piping in the relevant data.
    We can use a different LLM than the one used in the RAG system, meaning we can
    even ask multiple models to grade our output to ensure we have a balanced assessment.
  prefs: []
  type: TYPE_NORMAL
- en: This method also gives us an excellent explanation of what was wrong. In this
    example, I had a question about what kinds of dragons existed in the DND universe.
    The judging LLM correctly identified that it didn’t mention chromatic dragons.
    However, it also dinged the answer for not including the ages of Dragons, the
    DND Monster Manual, or the expansion adventure. Those omissions weren’t important
    to the question I asked, but this allows QA teams to decide for themselves once.
  prefs: []
  type: TYPE_NORMAL
- en: Where do we go now?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: RAG-based systems and the frameworks used to create them are advancing every
    day. New ways and mechanisms for grading them will continue to advance as well.
    There are even tools from giants like LangChain that can aid in this task, such
    as [LangSmith](https://www.langchain.com/langsmith) .
  prefs: []
  type: TYPE_NORMAL
- en: While we wait for more advancements, using a combination of some manual validation
    data and either the HuggingFace metrics library or LLMs themselves gives us a
    great way to begin trusting these systems.
  prefs: []
  type: TYPE_NORMAL
- en: Remember, once you have confidence and are ready to deploy your new RAG into
    production, the evaluation of the answers doesn’t stop! As part of routine monitoring
    and auditing efforts, you must continue storing questions and answers and plan
    for a human-in-the-loop effort to grade and flag answers supplied to end users.
    That, however, is a topic for another day.
  prefs: []
  type: TYPE_NORMAL
