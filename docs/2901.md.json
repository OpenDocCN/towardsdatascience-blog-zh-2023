["```py\nfrom plip.plip import PLIP\nimport numpy as np\n\nplip = PLIP('vinid/plip')\n\n# we create image embeddings and text embeddings\nimage_embeddings = plip.encode_images(images, batch_size=32)\ntext_embeddings = plip.encode_text(texts, batch_size=32)\n\n# we normalize the embeddings to unit norm (so that we can use dot product instead of cosine similarity to do comparisons)\nimage_embeddings = image_embeddings/np.linalg.norm(image_embeddings, ord=2, axis=-1, keepdims=True)\ntext_embeddings = text_embeddings/np.linalg.norm(text_embeddings, ord=2, axis=-1, keepdims=True)\n```", "```py\nfrom PIL import Image\nfrom transformers import CLIPProcessor, CLIPModel\n\nmodel = CLIPModel.from_pretrained(\"vinid/plip\")\nprocessor = CLIPProcessor.from_pretrained(\"vinid/plip\")\n\nimage = Image.open(\"images/image1.jpg\")\n\ninputs = processor(text=[\"a photo of label 1\", \"a photo of label 2\"],\n                   images=image, return_tensors=\"pt\", padding=True)\n\noutputs = model(**inputs)\nlogits_per_image = outputs.logits_per_image \nprobs = logits_per_image.softmax(dim=1) \n```"]