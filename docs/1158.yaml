- en: Multi-Layer Perceptrons Explained and Illustrated
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/multi-layer-perceptrons-8d76972afa2b?source=collection_archive---------0-----------------------#2023-04-02](https://towardsdatascience.com/multi-layer-perceptrons-8d76972afa2b?source=collection_archive---------0-----------------------#2023-04-02)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Understand the first fully-functional model of neural networks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@roiyeho?source=post_page-----8d76972afa2b--------------------------------)[![Dr.
    Roi Yehoshua](../Images/905a512ffc8879069403a87dbcbeb4db.png)](https://medium.com/@roiyeho?source=post_page-----8d76972afa2b--------------------------------)[](https://towardsdatascience.com/?source=post_page-----8d76972afa2b--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----8d76972afa2b--------------------------------)
    [Dr. Roi Yehoshua](https://medium.com/@roiyeho?source=post_page-----8d76972afa2b--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F3886620c5cf9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmulti-layer-perceptrons-8d76972afa2b&user=Dr.+Roi+Yehoshua&userId=3886620c5cf9&source=post_page-3886620c5cf9----8d76972afa2b---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----8d76972afa2b--------------------------------)
    ·13 min read·Apr 2, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F8d76972afa2b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmulti-layer-perceptrons-8d76972afa2b&user=Dr.+Roi+Yehoshua&userId=3886620c5cf9&source=-----8d76972afa2b---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F8d76972afa2b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmulti-layer-perceptrons-8d76972afa2b&source=-----8d76972afa2b---------------------bookmark_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: In [the previous article](https://medium.com/@roiyeho/perceptrons-the-first-neural-network-model-8b3ee4513757)
    we talked about perceptrons as one of the earliest models of neural networks.
    As we have seen, single perceptrons are limited in their computational power since
    they can solve only linearly separable problems.
  prefs: []
  type: TYPE_NORMAL
- en: In this article we will discuss multi-layer perceptrons (MLPs), which are networks
    consisting of multiple layers of perceptrons and are much more powerful than single-layer
    perceptrons. We will see how these networks operate and how to use them to solve
    complex tasks such as image classification.
  prefs: []
  type: TYPE_NORMAL
- en: Definitions and Notations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A **multi-layer perceptron** (MLP) is a neural network that has at least three
    layers: an input layer, an hidden layer and an output layer. Each layer operates
    on the outputs of its preceding layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a1eb1fbb0ed55fbf175252ae54f14c61.png)'
  prefs: []
  type: TYPE_IMG
- en: The MLP architecture
  prefs: []
  type: TYPE_NORMAL
- en: 'We will use the following notations:'
  prefs: []
  type: TYPE_NORMAL
- en: '*aᵢˡ* is the activation (output) of neuron *i* in layer *l*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*wᵢⱼˡ* is the weight of the connection from neuron *j* in layer *l*-1 to neuron
    *i* in layer *l*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*bᵢˡ* is the bias term of neuron *i* in layer *l*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The intermediate layers between the input and the output are called **hidden**…
  prefs: []
  type: TYPE_NORMAL
