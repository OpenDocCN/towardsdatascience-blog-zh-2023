- en: 'Hybrid Search 2.0: The Pursuit of Better Search'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/hybrid-search-2-0-the-pursuit-of-better-search-ce44d6f20c08?source=collection_archive---------1-----------------------#2023-09-30](https://towardsdatascience.com/hybrid-search-2-0-the-pursuit-of-better-search-ce44d6f20c08?source=collection_archive---------1-----------------------#2023-09-30)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: A Journey of Learning, Improvement, and the Quest for the Ultimate Hybrid Search
    System
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@noamschwartz1?source=post_page-----ce44d6f20c08--------------------------------)[![Noam
    Schwartz](../Images/c5bf11b1267a95242290ca6105eb0b16.png)](https://medium.com/@noamschwartz1?source=post_page-----ce44d6f20c08--------------------------------)[](https://towardsdatascience.com/?source=post_page-----ce44d6f20c08--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----ce44d6f20c08--------------------------------)
    [Noam Schwartz](https://medium.com/@noamschwartz1?source=post_page-----ce44d6f20c08--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F77ffd6350db9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhybrid-search-2-0-the-pursuit-of-better-search-ce44d6f20c08&user=Noam+Schwartz&userId=77ffd6350db9&source=post_page-77ffd6350db9----ce44d6f20c08---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----ce44d6f20c08--------------------------------)
    ·7 min read·Sep 30, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fce44d6f20c08&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhybrid-search-2-0-the-pursuit-of-better-search-ce44d6f20c08&user=Noam+Schwartz&userId=77ffd6350db9&source=-----ce44d6f20c08---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fce44d6f20c08&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhybrid-search-2-0-the-pursuit-of-better-search-ce44d6f20c08&source=-----ce44d6f20c08---------------------bookmark_footer-----------)![](../Images/717615f9c83f0f65f5f7ad7eb46d0770.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Photo by [bert b](https://unsplash.com/@bertsz?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: The idea of combining the strength of text and vector search has gained momentum
    in the field of search systems as a way to improve search relevancy and accuracy.
    I discussed using OpenSearch to build a hybrid search engine in [a recent blog
    post](/text-search-vs-vector-search-better-together-3bd48eb6132a). By integrating
    text-based lexical search with vector-based semantic search, we were able to enhance
    both the latency and accuracy of our search results.
  prefs: []
  type: TYPE_NORMAL
- en: I’ve recently been contemplating the drawbacks of the hybrid system and possible
    improvements. In this article, I’ll examine three weak areas in the prior system
    and suggest improvements that will strengthen it overall and deliver better results.
  prefs: []
  type: TYPE_NORMAL
- en: Please read my [prior blog post](/text-search-vs-vector-search-better-together-3bd48eb6132a)
    before continuing as I refer to the steps described there.
  prefs: []
  type: TYPE_NORMAL
- en: '**The normalization function was slightly biased; it weighed text search higher
    and gave it more significance in the final results.**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Distance-based algorithms such as K-Nearest Neighbors (KNN) calculate distances
    between data points, whereas BM25 is based on the frequencies of occurances of
    keywords. Both return scores that are completely on different scales. This can
    lead to biased results and inaccurate rankings. Our normilization procedure always
    produced a perfect score (1) for at least one document in the lexical search result
    set, hence in our situation the results were biased in favor of the lexical search.
  prefs: []
  type: TYPE_NORMAL
- en: 'To tackle this problem, let’s explore two commonly used functions: Min-Max
    normalization and Z-Score normalization. The Z-Score method scales the data to
    achieve a zero mean and unit variance, while Min-Max normalization rescales the
    data to fit within a specific range.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The key idea is that I can gain a basic understanding of how scores are distributed
    for each search type with similar queries if I compute the parameters used in
    these normilizing functions beforehand and apply them during the normlizing stage.
    The two functions formulas are:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f64fcfc9f52a09e8a0058ab1c13d5a3e.png)'
  prefs: []
  type: TYPE_IMG
- en: Considering your index’s structure can help you decide which one to choose since
    each has advantages of its own. If your documents are more similar to one another
    and the top-k results of a typical query return documents that are very similar
    to one another and clustered together within the index, as seen in the graph below,
    Min-Max may be a better option.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ee9a48204865c0c9d2e8c34fe6f2d950.png)'
  prefs: []
  type: TYPE_IMG
- en: Diagram by the Author
  prefs: []
  type: TYPE_NORMAL
- en: However, Z-Score is more suited if the results are more evenly distributed and
    have some characteristics of a normal distribution, as shown in the example below.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9cfce46b7bbcfbe25b75a7bada603465.png)'
  prefs: []
  type: TYPE_IMG
- en: Diagram by the Author
  prefs: []
  type: TYPE_NORMAL
- en: Both approaches call for certain parameters to be determined; we must compute
    the mean, standard deviation, minimum score, and maximum score. We must determine
    these values separately for each search type because vector and semantic results
    have different scoring systems. Let’s run 1000 random queries, a vector search,
    and a semantic search to do this. If you don’t have queries, you can use OpenSearch’s
    [scroll API](https://opensearch.org/docs/latest/api-reference/scroll/) to extract
    text fields to be used as queries from different parts of your index. Set k to
    a significant value, for example k=1000, to better understand the ratios within
    our score range. However, be careful not to set k too high since this may have
    an impact on the Min-Max function. Simply compute the necessary parameters after
    collecting all of these scores.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The process is shown in the diagram below:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c82df63283940d632423cb8b97f15d7b.png)'
  prefs: []
  type: TYPE_IMG
- en: Diagram by the Author
  prefs: []
  type: TYPE_NORMAL
- en: 'Set aside the parameters you’ve extracted for the lexical and vector results.
    Each index will need to have this done separately once. Finally, in the normalization
    step, we’ll use these parameters in the following way:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/76bd47b321c5ed31082c8b92208696bc.png)'
  prefs: []
  type: TYPE_IMG
- en: Normlizing using the Z-Score function (Diagram by the Author)
  prefs: []
  type: TYPE_NORMAL
- en: '**2\. Scores that didn’t appear in either set were “unfairly” handled and weren’t
    given adequate consideration as potential matches.**'
  prefs: []
  type: TYPE_NORMAL
- en: Previously, any result specific to one set received an arbitrary low score,
    which significantly affected its final ranking. Let’s attempt an alternative strategy
    to solve this issue. We will instead award the lowest score from the missing result
    set to documents that only show up in one type of result set. For instance, we
    assign the document *c5a44d-fa8d-4bba* (colored in yellow) the lowest semantic
    search score among the top k results because it only showed up in my lexical (keyword)
    search results. In this approach, we guarantee that these results are still legitimate
    by providing a score that is within the range of other scores.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2c1ec6904da1eabe44612376cb244e3f.png)'
  prefs: []
  type: TYPE_IMG
- en: Diagram by the Author
  prefs: []
  type: TYPE_NORMAL
- en: '**3\. The old method of results evaluation lacked a strong data-driven or scientific
    basis. As a result, we lacked a way to select tuning parameters like boost based
    on data.**'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s tackle both of these problems at once. The task at hand involves comparing
    two sets of promising results — lexical and semantic results — and claiming that
    hybrid results outperforms them. To accomplish this, we’ll experiment on the [MS
    MARCO](https://microsoft.github.io/msmarco/). MS MARCO is a dataset curated by
    Microsoft Research which comprises an extensive collection of 3.2 million web
    documents extracted from web pages and over 350,000 queries sourced from real
    Bing web search queries. Initially designed for benchmarking question-and-answer
    (Q&A) systems, this dataset contains queries that resemble real-world questions.
    Given the Q&A nature of the dataset, the relevance labels are straightforward,
    with each result assigned just one “relevant” label (1).
  prefs: []
  type: TYPE_NORMAL
- en: In our scenario, the MS MARCO document ranking challenge uses the [mean reciprocal
    rank](https://en.wikipedia.org/wiki/Mean_reciprocal_rank) (MRR) for the top 100
    results (MRR@100) as the relevance metric for its ranking challenge. It calculates
    the reciprocal rank *(1/rank)* of the first relevant document and averages this
    across all queries.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/99309604d32e5228acd8aedd7082cd9a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Source: Wikipedia'
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ll go on by performing three different kinds of searches: lexical, semantic,
    and hybrid. We’ll experiment with various boost levels, changing them by 0.1 each
    time. We want to assess the Mean Reciprocal Rank at 100 (MRR@100) for all our
    search experiments therefore we set our top-k to 100\. For each query, we will
    compare the results with the “correct” labeled document ID and determine its rank.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Finally, we will add together each rank to determine the Mean Reciprocal Rank
    (MRR).
  prefs: []
  type: TYPE_NORMAL
- en: 'The MRR@100 scores for each boosting ratio between 0 and 1 (in increments/subtractions
    of 0.1) are shown in the tables below:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/efcdad8c89784672e89824c2e7d84530.png)'
  prefs: []
  type: TYPE_IMG
- en: MRR@100 using the Z-Score function (Screenshot by the Author)
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/bf1864379376ea7ace9ede982661d7cf.png)'
  prefs: []
  type: TYPE_IMG
- en: MRR@100 using the Min-Max function (Screenshot by the Author)
  prefs: []
  type: TYPE_NORMAL
- en: The findings presented above clearly show that the Min-Max approach performed
    best when boosting somewhat in the direction of lexical search. Nevertheless,
    combining it with semantic results enhanced its accuracy. On the other hand, the
    Z-Score function, which evenly distributes the boost while keeping a 50–50 split
    between lexical and semantic search, produced the best overall results, demonstrating
    that the hybrid search approach is the ideal choice.
  prefs: []
  type: TYPE_NORMAL
- en: 'In our quest for a more effective hybrid search system, we’ve faced challenges
    head-on and made significant improvements. We’ve taken steps to address inherent
    challenges, and the results are clear: the combination of lexical and semantic
    search techniques holds immense promise. This isn’t the final destination; it’s
    a stepping stone towards a more efficient, user-centric search experience. With
    data-backed methodologies at our disposal, I plan to keep improving the search
    system and embrace the ever-evolving challenges of information retrieval.'
  prefs: []
  type: TYPE_NORMAL
- en: Many thanks to [Yaniv Vaknin](https://medium.com/@yaniv.vaknin) and especially
    [Daphna Idelson](https://www.linkedin.com/in/daphnaidelson/) for all of their
    help putting this together!
  prefs: []
  type: TYPE_NORMAL
