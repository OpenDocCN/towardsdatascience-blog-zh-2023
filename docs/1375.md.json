["```py\n# qs: [N, H, D], ks: [L, H, D], vs: [L, H, D]\n\nattn = torch.einsum(\"nhd,lhd->nlh\", qs, ks)  # [N, L, H]\nattn = torch.softmax(attn, dim=1) # [N, L, H]\nz_next = torch.einsum(\"nlh,lhd->nhd\", attn, vs)  # [N, H, D]\n```", "```py\n# qs: [N, H, D], ks: [L, H, D], vs: [L, H, D]\n\nqs = softmax_kernel(qs) # [N, H, M]\nks = softmax_kernel(ks) # [L, H, M]\n\n# numerator\nkvs = torch.einsum(\"lhm,lhd->hmd\", ks, vs)\nattn_num = torch.einsum(\"nhm,hmd->nhd\", qs, kvs) # [N, H, D]\n\n# denominator\nall_ones = torch.ones([ks.shape[0]])\nks_sum = torch.einsum(\"lhm,l->hm\", ks, all_ones)\nattn_den = torch.einsum(\"nhm,hm->nh\", qs, ks_sum)  # [N, H]\n\n# attentive aggregated results\nz_next = attn_num / attn_den # [N, H, D]\n```", "```py\n# qs: [N, H, D], ks: [L, H, D], vs: [L, H, D]\n\nqs = qs / torch.norm(qs, p=2) # [N, H, D]\nks = ks / torch.norm(ks, p=2) # [L, H, D]\nN = qs.shape[0]\n\n# numerator\nkvs = torch.einsum(\"lhm,lhd->hmd\", ks, vs)\nattn_num = torch.einsum(\"nhm,hmd->nhd\", qs, kvs) # [N, H, D]\nall_ones = torch.ones([vs.shape[0]])\nvs_sum = torch.einsum(\"l,lhd->hd\", all_ones, vs) # [H, D]\nattn_num += vs_sum.unsqueeze(0).repeat(vs.shape[0], 1, 1) # [N, H, D]\n\n# denominator\nall_ones = torch.ones([ks.shape[0]])\nks_sum = torch.einsum(\"lhm,l->hm\", ks, all_ones)\nattn_den = torch.einsum(\"nhm,hm->nh\", qs, ks_sum)  # [N, H]\n\n# attentive aggregated results\nattn_den = torch.unsqueeze(attn_den, len(attn_den.shape))  # [N, H, 1]\nattn_den += torch.ones_like(attn_den) * N\nz_next = attn_num / attn_den # [N, H, D]\n```"]