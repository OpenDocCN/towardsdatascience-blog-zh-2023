- en: 'Making Sense of A/B Testing: Understand Better with Hard Questions'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 了解A/B测试的意义：通过困难问题更好地理解
- en: 原文：[https://towardsdatascience.com/making-sense-of-a-b-testing-understand-better-with-hard-questions-cfc98c2937e2?source=collection_archive---------2-----------------------#2023-07-04](https://towardsdatascience.com/making-sense-of-a-b-testing-understand-better-with-hard-questions-cfc98c2937e2?source=collection_archive---------2-----------------------#2023-07-04)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/making-sense-of-a-b_testing-understand-better-with-hard-questions-cfc98c2937e2?source=collection_archive---------2-----------------------#2023-07-04](https://towardsdatascience.com/making-sense-of-a-b_testing_understand-better-with-hard-questions-cfc98c2937e2?source=collection_archive---------2-----------------------#2023-07-04)
- en: Uncover the counterintuitive aspects of A/B testing through challenging questions,
    improve your understanding, and steer clear of mistakes
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 通过具有挑战性的问题揭示A/B测试中反直觉的方面，提升你的理解，并避免错误
- en: '[](https://medium.com/@aliaksandrkazlou?source=post_page-----cfc98c2937e2--------------------------------)[![Aliaksandr
    Kazlou](../Images/e50023954c125cdd1dbf85834b84f1d8.png)](https://medium.com/@aliaksandrkazlou?source=post_page-----cfc98c2937e2--------------------------------)[](https://towardsdatascience.com/?source=post_page-----cfc98c2937e2--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----cfc98c2937e2--------------------------------)
    [Aliaksandr Kazlou](https://medium.com/@aliaksandrkazlou?source=post_page-----cfc98c2937e2--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@aliaksandrkazlou?source=post_page-----cfc98c2937e2--------------------------------)[![Aliaksandr
    Kazlou](../Images/e50023954c125cdd1dbf85834b84f1d8.png)](https://medium.com/@aliaksandrkazlou?source=post_page-----cfc98c2937e2--------------------------------)[](https://towardsdatascience.com/?source=post_page-----cfc98c2937e2--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----cfc98c2937e2--------------------------------)
    [Aliaksandr Kazlou](https://medium.com/@aliaksandrkazlou?source=post_page-----cfc98c2937e2--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fa3b0b8410b61&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmaking-sense-of-a-b-testing-understand-better-with-hard-questions-cfc98c2937e2&user=Aliaksandr+Kazlou&userId=a3b0b8410b61&source=post_page-a3b0b8410b61----cfc98c2937e2---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----cfc98c2937e2--------------------------------)
    ·6 min read·Jul 4, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fcfc98c2937e2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmaking-sense-of-a-b-testing-understand-better-with-hard-questions-cfc98c2937e2&user=Aliaksandr+Kazlou&userId=a3b0b8410b61&source=-----cfc98c2937e2---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fa3b0b8410b61&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmaking-sense-of-a-b_testing-understand-better-with-hard-questions-cfc98c2937e2&user=Aliaksandr+Kazlou&userId=a3b0b8410b61&source=post_page-a3b0b8410b61----cfc98c2937e2---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----cfc98c2937e2--------------------------------)
    · 6分钟阅读 · 2023年7月4日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fcfc98c2937e2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmaking-sense-of-a-b_testing-understand-better-with-hard-questions-cfc98c2937e2&user=Aliaksandr+Kazlou&userId=a3b0b8410b61&source=-----cfc98c2937e2---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fcfc98c2937e2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmaking-sense-of-a-b-testing-understand-better-with-hard-questions-cfc98c2937e2&source=-----cfc98c2937e2---------------------bookmark_footer-----------)![](../Images/c91ff8d84b406d2088289d5b649693f8.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fcfc98c2937e2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmaking-sense-of-a-b_testing-understand-better-with-hard-questions-cfc98c2937e2&source=-----cfc98c2937e2---------------------bookmark_footer-----------)![](../Images/c91ff8d84b406d2088289d5b649693f8.png)'
- en: Photo by [ALAN DE LA CRUZ](https://unsplash.com/es/@alandelacruz4?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 照片由 [ALAN DE LA CRUZ](https://unsplash.com/es/@alandelacruz4?utm_source=medium&utm_medium=referral)
    提供，来源于 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: This article highlights common statistical errors in the context of experiments.
    It’s set up as five questions with answers that many find counterintuitive. It’s
    tailored for those who are already familiar with A/B tests but are aiming to expand
    their understanding. This can help you prevent common errors in your daily work
    or ace a job interview.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 这篇文章强调了实验中常见的统计错误。它以五个问题和答案的形式展开，这些答案常常让人感到直觉上不对劲。它专为那些已经熟悉A/B测试但希望扩展理解的人群量身定制。这可以帮助你在日常工作中避免常见错误或在面试中表现出色。
- en: '**Question 1: You’ve conducted an A/B test (α = 0.05, β = 0.2), which yields
    a statistically significant result. In this scenario, what is the likelihood that
    it’s a true positive?**'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题 1：你进行了一个A/B测试（α = 0.05, β = 0.2），结果具有统计学意义。在这种情况下，它是真阳性的可能性有多大？**'
- en: Imagine if you were to measure only working hypotheses. Then, 100% of successful
    A/B tests would be true positives. When none of your hypotheses work, 100% of
    successful A/B tests would be false positives.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下，如果你只测量有效的假设。那么，100%的成功A/B测试将是真阳性。当你的所有假设都无效时，100%的成功A/B测试将是假阳性。
- en: These two extremes are meant to demonstrate that it’s impossible to answer this
    question without an extra step — an assumption about the distribution of hypotheses.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个极端的例子旨在说明，没有额外的步骤——对假设分布的假设——是不可能回答这个问题的。
- en: Let’s try one more time and assume that 10% of the hypotheses we test are effective.
    Then, observing a statistically significant result from an A/B test implies there’s
    a 64% (by [Bayes’ theorem](https://en.wikipedia.org/wiki/Bayes%27_theorem), (1–0.2)*0.1
    / ((1–0.2)*0.1 + 0.05*(1–0.1))) chance that it’s a true positive.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 再试一次，假设我们测试的10%的假设是有效的。那么，从A/B测试中观察到统计显著的结果意味着有64%（根据[贝叶斯定理](https://en.wikipedia.org/wiki/Bayes%27_theorem)，(1–0.2)*0.1
    / ((1–0.2)*0.1 + 0.05*(1–0.1)))的机会它是真阳性。
- en: '![](../Images/1ff5e68d42664d80321c6a6ec84eb47b.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1ff5e68d42664d80321c6a6ec84eb47b.png)'
- en: Image by author
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图像
- en: '**Question 2: Suppose the null hypothesis is true. Under this circumstance,
    would a higher or lower p-value be more likely?**'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题 2：假设原假设成立。在这种情况下，更高还是更低的p值更可能出现？**'
- en: 'Many think it’s the former. This seems intuitive: when there’s no effect, the
    result is more likely to be far from statistical significance, hence a higher
    p-value.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 很多人认为是前者。这似乎是直观的：当没有效应时，结果更可能远离统计显著性，因此p值更高。
- en: However, the answer is neither. When the null hypothesis is true, p-values are
    distributed uniformly.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，答案是否定的。当原假设成立时，p值是均匀分布的。
- en: The confusion arises because people often visualise these concepts in terms
    of z-scores, or sample means, or differences in sample means. All of these are
    normally distributed. It might be hard to grasp, then, the uniformity of p-values.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 混淆的原因在于，人们通常通过z分数、样本均值或样本均值的差异来可视化这些概念。所有这些都是正态分布的。因此，可能很难理解p值的一致性。
- en: Let’s illustrate this with a simulation. Assume that both the treatment and
    control groups are drawn from the same normal distribution (μ = 0, σ = 1), meaning
    the null hypothesis is true. We’ll then compare their means, calculate p-values,
    and repeat this process multiple times. For simplicity, let’s only look at cases
    where the mean of the treatment group was larger. And then, let’s look at cases
    with p-values from 0.9 to 0.8 and from 0.2 to 0.1.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过一个模拟来说明这一点。假设治疗组和对照组均来自同一个正态分布（μ = 0, σ = 1），这意味着原假设成立。我们将比较它们的均值，计算p值，并多次重复这个过程。为了简化起见，我们只关注治疗组均值较大的情况。然后，我们再看看p值在0.9到0.8和0.2到0.1之间的情况。
- en: When we map these p-value intervals onto the distribution we simulated, the
    picture becomes clearer. Although the peak of the distribution near zero is higher,
    the interval’s width here is narrower. Conversely, as we move away from zero,
    the peak shrinks but the width of the intervals increases. This is because p-values
    are computed in such a way that intervals of equal length encompass the same area
    under the curve.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们将这些p值区间映射到我们模拟的分布上时，图像变得更加清晰。尽管接近零的分布峰值较高，但此处区间的宽度较窄。相反，当我们远离零时，峰值缩小，但区间的宽度增加。这是因为p值的计算方式使得等长区间包含曲线下相同的面积。
- en: '![](../Images/8690a40606b009dae1d35fc1a5d7a024.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8690a40606b009dae1d35fc1a5d7a024.png)'
- en: Image by author
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图像
- en: '**Question 3: Due to some technical or business constraints, you’ve run an
    A/B test with a smaller-than-usual sample size. The result is barely significant.
    However, the effect size is large, larger than what you typically see in similar
    A/B tests. Should the larger effect size bolster your confidence in the result?**'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题3：由于一些技术或业务限制，你进行了一个样本量较小的A/B测试。结果勉强显著。然而，效应值很大，比你在类似A/B测试中通常看到的更大。效应值的增大是否能增强你对结果的信心？**'
- en: Not really. In order for an effect to be classified as significant, it must
    be either plus or minus 2 standard errors away from zero (when α = 0.05). As the
    sample size shrinks, standard errors generally rise. This implies that statistically
    significant effects observed in smaller samples tend to be larger.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 不一定。为了将效应归类为显著，它必须与零的距离为正负2个标准误差（当α = 0.05）。随着样本量的缩小，标准误差通常会增加。这意味着在小样本中观察到的统计显著效应往往更大。
- en: 'The simulation below demonstrates that: these are absolute effect sizes of
    significant A/B tests when both groups (N=1000) are sampled from the same normal
    distribution (μ = 0, σ = 1).'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 以下模拟演示了：这些是当两个组（N=1000）都从相同的正态分布（μ = 0，σ = 1）中抽样时的显著A/B测试的绝对效应值。
- en: '![](../Images/93df01c68694d9286f170085e0a7454e.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/93df01c68694d9286f170085e0a7454e.png)'
- en: Image by author
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图像
- en: '**Question 4: Let’s build on the understanding gained from the previous question**.
    **Is it possible to detect a true effect that is smaller than 2 standard errors?**'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题4：让我们基于之前问题中获得的理解继续探讨**。**是否可以检测到小于2个标准误差的真实效应？**'
- en: Yes, although semantics here is muddy**.** The true effect size could be significantly
    smaller than 2 standard errors. Even then, you would anticipate a certain fraction
    of A/B tests to exhibit statistical significance.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 是的，尽管这里的语义比较模糊**。** 真实效应值可能显著小于2个标准误差。即便如此，你仍然会预期一定比例的A/B测试显示统计显著性。
- en: However, under these conditions your detected effect size is always exaggerated.
    Imagine that the true effect is 0.4, but you’ve detected an effect of 0.5 with
    a p-value of 0.05\. Would you consider this a true positive? What if the true
    effect size is only 0.1, yet you again detect an effect of 0.5? Is it still a
    true positive if the true effect is a mere 0.01?
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在这些条件下，你检测到的效应值总是被夸大的。假设真正的效应是0.4，但你检测到的效应是0.5，且p值为0.05。你会认为这是真阳性吗？如果真正的效应值只有0.1，但你又检测到效应为0.5呢？如果真正的效应仅为0.01，那这仍然是一个真阳性吗？
- en: Let’s visualise this scenario. Control groups (N=100) are sampled from a normal
    distribution (μ = 0, σ = 2), while treatment groups (N=100) are sampled from the
    same distribution but with μ varying from 0.1 to 1\. Regardless of the true effect
    size, a successful A/B test generates an estimated effect size of at least 0.5\.
    When the true effects are smaller than this, the resulting estimate is clearly
    inflated.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们可视化这个场景。对照组（N=100）从正态分布（μ = 0，σ = 2）中抽样，而处理组（N=100）则从相同的分布中抽样，但μ从0.1到1变化。无论真实效应大小如何，成功的A/B测试生成的估计效应值至少为0.5。当真实效应小于这个值时，结果估计显然被夸大了。
- en: '![](../Images/b1c9d35cac60a6a66dee7ce516a5e20d.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b1c9d35cac60a6a66dee7ce516a5e20d.png)'
- en: Image by author
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图像
- en: This is why some statisticians avoid dividing outcomes into binary categories
    like ‘true positives’ or ‘false positives’. Instead, they treat them in a more
    continuous manner [1].
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是为什么一些统计学家避免将结果划分为“真阳性”或“假阳性”这样的二元类别。他们更倾向于以更连续的方式处理这些结果[1]。
- en: '**Question 5: You’ve conducted an A/B test that produces a significant result,
    with a p-value of 0.04\. However, your boss remains unconvinced and asks for a
    second test. This subsequent test doesn’t yield a significant result, presenting
    a p-value of 0.25\. Does this mean that the original effect wasn’t real, and the
    initial result was a false positive?**'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题5：你进行了A/B测试，结果显著，p值为0.04。然而，你的老板仍然不相信，并要求再做一次测试。这个后续测试没有产生显著结果，p值为0.25。这是否意味着原始效应不真实，初始结果是一个假阳性？**'
- en: There’s always a risk in interpreting p-values as a binary, lexicographic decision
    rule. Let’s remind ourselves what a p-value actually is. It’s a measure of surprise.
    And it’s random and it’s continuous. And it’s only one piece of evidence.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 将p值解释为二元的、词典式的决策规则总是存在风险。让我们回顾一下p值实际上是什么。它是惊讶的度量。它是随机的，是连续的。它只是证据中的一部分。
- en: Imagine the first experiment (p=0.04) was run on 1.000 users. The second one
    (p=0.25) — on 10.000 users. Apart from the noticeable differences in quality,
    the second A/B test, as we discussed in Questions 3 and 4, probably had a much
    smaller estimated effect size that might not be practically significant anymore.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 设想第一个实验（p=0.04）是在 1,000 人中进行的。第二个实验（p=0.25）则是在 10,000 人中进行的。除了质量上的明显差异外，正如我们在问题
    3 和 4 中讨论的，第二个 A/B 测试可能估计的效应大小要小得多，可能在实际中不再具有显著意义。
- en: 'Let’s reverse this scenario: the first one (p=0.04) was run on 10.000, and
    the second one (p=0.25) — on 1.000 users. Here we are much more confident that
    the effect ‘exists’.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们倒过来看这个场景：第一个（p=0.04）是在 10,000 人中进行的，第二个（p=0.25）则是在 1,000 人中进行的。在这种情况下，我们对效果‘存在’的信心更大。
- en: Now, imagine both A/B tests were identical. In this situation, you’ve observed
    two fairly similar, somewhat surprising results, neither are too consistent with
    the null hypothesis. The fact that they fall on opposite sides of .05 is not terribly
    important. What’s important is that observing two small p-values consecutively
    when the null is true is unlikely.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，假设两个 A/B 测试是相同的。在这种情况下，你观察到了两个相当相似的、稍微令人惊讶的结果，它们都与零假设不太一致。它们落在 0.05 的两侧并不是特别重要。重要的是，在零假设为真时，连续观察到两个小
    p 值是不太可能的。
- en: One question we might consider is whether this difference is statistically significant
    itself. Categorising p-values in a binary way skews our intuition, making us believe
    there’s a vast, even ontological, difference between p-values on different sides
    of the cutoff. However, the p-value is a fairly continuous function, and it might
    be possible that two A/B tests, despite different p-values, present very similar
    evidence against the null [2].
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可能需要考虑的一个问题是，这个差异本身是否具有统计显著性。以二元方式分类 p 值会扭曲我们的直觉，让我们相信不同侧的 p 值之间存在巨大甚至本体上的差异。然而，p
    值是一个相当连续的函数，尽管两个 A/B 测试的 p 值不同，但它们可能对零假设提供了非常相似的证据[2]。
- en: 'Another way to look at this is to combine the evidence. Assuming the null hypothesis
    is true for both tests, the combined p-value stands at 0.05, according to [Fisher’s
    method](https://en.wikipedia.org/wiki/Fisher%27s_method). There are other methods
    to combine p-values, but the general logic remains the same: a sharp null isn’t
    a realistic hypothesis in most settings. Therefore, enough ‘surprising’ outcomes,
    even if none of them are statistically significant individually, might be sufficient
    to reject the null.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种看待这个问题的方法是结合证据。假设两个测试的零假设都为真，根据[Fisher 方法](https://en.wikipedia.org/wiki/Fisher%27s_method)，组合后的
    p 值为 0.05。还有其他方法可以结合 p 值，但基本逻辑相同：在大多数情况下，尖锐的零假设并不现实。因此，即使这些结果在统计上没有单独显著，足够多的‘令人惊讶’的结果也可能足以拒绝零假设。
- en: '![](../Images/ca549bdd0f58d7ea341632905c7e5070.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ca549bdd0f58d7ea341632905c7e5070.png)'
- en: Fusing two p-values by using Fisher’s method. Image by Chen-Pan Liao, from [Wikipedia](https://en.wikipedia.org/wiki/Fisher%27s_method)
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用 Fisher 方法融合两个 p 值。图片来源：Chen-Pan Liao，来自[维基百科](https://en.wikipedia.org/wiki/Fisher%27s_method)
- en: '**Conclusion**'
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**结论**'
- en: The Null Hypothesis Testing Framework, which we commonly use to analyze A/B
    tests, isn’t especially intuitive. Without regular mental practice, we often revert
    to an ‘intuitive’ understanding, which can be misleading. We may also develop
    routines to ease this cognitive burden. Unfortunately, these routines often become
    somewhat ritualistic, with the adherence to formal procedures overshadowing the
    actual objective of inference.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 我们常用的零假设检验框架，在分析 A/B 测试时并不是特别直观。没有经常的心理训练，我们常常会退回到‘直观’的理解，这可能会误导我们。我们也可能会形成一些例行程序以减轻这种认知负担。不幸的是，这些例行程序往往变得有些仪式化，遵守正式程序的过程掩盖了实际推断的目标。
- en: References
  id: totrans-47
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: McShane, B. B., Gal, D., Gelman, A., Robert, C., & Tackett, J. L. (2019). Abandon
    statistical significance. *The American Statistician*, *73*(sup1), 235–245.
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: McShane, B. B., Gal, D., Gelman, A., Robert, C., & Tackett, J. L. (2019). 放弃统计显著性。*美国统计学家*,
    *73*(sup1), 235–245.
- en: Gelman, A., & Stern, H. (2006). The difference between “significant” and “not
    significant” is not itself statistically significant. *The American Statistician*,
    *60*(4), 328–331.
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Gelman, A., & Stern, H. (2006). “显著”和“非显著”之间的差异本身并不具有统计显著性。*美国统计学家*, *60*(4),
    328–331.
