- en: 'Making Sense of A/B Testing: Understand Better with Hard Questions'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/making-sense-of-a-b-testing-understand-better-with-hard-questions-cfc98c2937e2?source=collection_archive---------2-----------------------#2023-07-04](https://towardsdatascience.com/making-sense-of-a-b-testing-understand-better-with-hard-questions-cfc98c2937e2?source=collection_archive---------2-----------------------#2023-07-04)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Uncover the counterintuitive aspects of A/B testing through challenging questions,
    improve your understanding, and steer clear of mistakes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@aliaksandrkazlou?source=post_page-----cfc98c2937e2--------------------------------)[![Aliaksandr
    Kazlou](../Images/e50023954c125cdd1dbf85834b84f1d8.png)](https://medium.com/@aliaksandrkazlou?source=post_page-----cfc98c2937e2--------------------------------)[](https://towardsdatascience.com/?source=post_page-----cfc98c2937e2--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----cfc98c2937e2--------------------------------)
    [Aliaksandr Kazlou](https://medium.com/@aliaksandrkazlou?source=post_page-----cfc98c2937e2--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fa3b0b8410b61&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmaking-sense-of-a-b-testing-understand-better-with-hard-questions-cfc98c2937e2&user=Aliaksandr+Kazlou&userId=a3b0b8410b61&source=post_page-a3b0b8410b61----cfc98c2937e2---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----cfc98c2937e2--------------------------------)
    ·6 min read·Jul 4, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fcfc98c2937e2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmaking-sense-of-a-b-testing-understand-better-with-hard-questions-cfc98c2937e2&user=Aliaksandr+Kazlou&userId=a3b0b8410b61&source=-----cfc98c2937e2---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fcfc98c2937e2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmaking-sense-of-a-b-testing-understand-better-with-hard-questions-cfc98c2937e2&source=-----cfc98c2937e2---------------------bookmark_footer-----------)![](../Images/c91ff8d84b406d2088289d5b649693f8.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Photo by [ALAN DE LA CRUZ](https://unsplash.com/es/@alandelacruz4?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: This article highlights common statistical errors in the context of experiments.
    It’s set up as five questions with answers that many find counterintuitive. It’s
    tailored for those who are already familiar with A/B tests but are aiming to expand
    their understanding. This can help you prevent common errors in your daily work
    or ace a job interview.
  prefs: []
  type: TYPE_NORMAL
- en: '**Question 1: You’ve conducted an A/B test (α = 0.05, β = 0.2), which yields
    a statistically significant result. In this scenario, what is the likelihood that
    it’s a true positive?**'
  prefs: []
  type: TYPE_NORMAL
- en: Imagine if you were to measure only working hypotheses. Then, 100% of successful
    A/B tests would be true positives. When none of your hypotheses work, 100% of
    successful A/B tests would be false positives.
  prefs: []
  type: TYPE_NORMAL
- en: These two extremes are meant to demonstrate that it’s impossible to answer this
    question without an extra step — an assumption about the distribution of hypotheses.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s try one more time and assume that 10% of the hypotheses we test are effective.
    Then, observing a statistically significant result from an A/B test implies there’s
    a 64% (by [Bayes’ theorem](https://en.wikipedia.org/wiki/Bayes%27_theorem), (1–0.2)*0.1
    / ((1–0.2)*0.1 + 0.05*(1–0.1))) chance that it’s a true positive.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1ff5e68d42664d80321c6a6ec84eb47b.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: '**Question 2: Suppose the null hypothesis is true. Under this circumstance,
    would a higher or lower p-value be more likely?**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Many think it’s the former. This seems intuitive: when there’s no effect, the
    result is more likely to be far from statistical significance, hence a higher
    p-value.'
  prefs: []
  type: TYPE_NORMAL
- en: However, the answer is neither. When the null hypothesis is true, p-values are
    distributed uniformly.
  prefs: []
  type: TYPE_NORMAL
- en: The confusion arises because people often visualise these concepts in terms
    of z-scores, or sample means, or differences in sample means. All of these are
    normally distributed. It might be hard to grasp, then, the uniformity of p-values.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s illustrate this with a simulation. Assume that both the treatment and
    control groups are drawn from the same normal distribution (μ = 0, σ = 1), meaning
    the null hypothesis is true. We’ll then compare their means, calculate p-values,
    and repeat this process multiple times. For simplicity, let’s only look at cases
    where the mean of the treatment group was larger. And then, let’s look at cases
    with p-values from 0.9 to 0.8 and from 0.2 to 0.1.
  prefs: []
  type: TYPE_NORMAL
- en: When we map these p-value intervals onto the distribution we simulated, the
    picture becomes clearer. Although the peak of the distribution near zero is higher,
    the interval’s width here is narrower. Conversely, as we move away from zero,
    the peak shrinks but the width of the intervals increases. This is because p-values
    are computed in such a way that intervals of equal length encompass the same area
    under the curve.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8690a40606b009dae1d35fc1a5d7a024.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: '**Question 3: Due to some technical or business constraints, you’ve run an
    A/B test with a smaller-than-usual sample size. The result is barely significant.
    However, the effect size is large, larger than what you typically see in similar
    A/B tests. Should the larger effect size bolster your confidence in the result?**'
  prefs: []
  type: TYPE_NORMAL
- en: Not really. In order for an effect to be classified as significant, it must
    be either plus or minus 2 standard errors away from zero (when α = 0.05). As the
    sample size shrinks, standard errors generally rise. This implies that statistically
    significant effects observed in smaller samples tend to be larger.
  prefs: []
  type: TYPE_NORMAL
- en: 'The simulation below demonstrates that: these are absolute effect sizes of
    significant A/B tests when both groups (N=1000) are sampled from the same normal
    distribution (μ = 0, σ = 1).'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/93df01c68694d9286f170085e0a7454e.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: '**Question 4: Let’s build on the understanding gained from the previous question**.
    **Is it possible to detect a true effect that is smaller than 2 standard errors?**'
  prefs: []
  type: TYPE_NORMAL
- en: Yes, although semantics here is muddy**.** The true effect size could be significantly
    smaller than 2 standard errors. Even then, you would anticipate a certain fraction
    of A/B tests to exhibit statistical significance.
  prefs: []
  type: TYPE_NORMAL
- en: However, under these conditions your detected effect size is always exaggerated.
    Imagine that the true effect is 0.4, but you’ve detected an effect of 0.5 with
    a p-value of 0.05\. Would you consider this a true positive? What if the true
    effect size is only 0.1, yet you again detect an effect of 0.5? Is it still a
    true positive if the true effect is a mere 0.01?
  prefs: []
  type: TYPE_NORMAL
- en: Let’s visualise this scenario. Control groups (N=100) are sampled from a normal
    distribution (μ = 0, σ = 2), while treatment groups (N=100) are sampled from the
    same distribution but with μ varying from 0.1 to 1\. Regardless of the true effect
    size, a successful A/B test generates an estimated effect size of at least 0.5\.
    When the true effects are smaller than this, the resulting estimate is clearly
    inflated.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b1c9d35cac60a6a66dee7ce516a5e20d.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: This is why some statisticians avoid dividing outcomes into binary categories
    like ‘true positives’ or ‘false positives’. Instead, they treat them in a more
    continuous manner [1].
  prefs: []
  type: TYPE_NORMAL
- en: '**Question 5: You’ve conducted an A/B test that produces a significant result,
    with a p-value of 0.04\. However, your boss remains unconvinced and asks for a
    second test. This subsequent test doesn’t yield a significant result, presenting
    a p-value of 0.25\. Does this mean that the original effect wasn’t real, and the
    initial result was a false positive?**'
  prefs: []
  type: TYPE_NORMAL
- en: There’s always a risk in interpreting p-values as a binary, lexicographic decision
    rule. Let’s remind ourselves what a p-value actually is. It’s a measure of surprise.
    And it’s random and it’s continuous. And it’s only one piece of evidence.
  prefs: []
  type: TYPE_NORMAL
- en: Imagine the first experiment (p=0.04) was run on 1.000 users. The second one
    (p=0.25) — on 10.000 users. Apart from the noticeable differences in quality,
    the second A/B test, as we discussed in Questions 3 and 4, probably had a much
    smaller estimated effect size that might not be practically significant anymore.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s reverse this scenario: the first one (p=0.04) was run on 10.000, and
    the second one (p=0.25) — on 1.000 users. Here we are much more confident that
    the effect ‘exists’.'
  prefs: []
  type: TYPE_NORMAL
- en: Now, imagine both A/B tests were identical. In this situation, you’ve observed
    two fairly similar, somewhat surprising results, neither are too consistent with
    the null hypothesis. The fact that they fall on opposite sides of .05 is not terribly
    important. What’s important is that observing two small p-values consecutively
    when the null is true is unlikely.
  prefs: []
  type: TYPE_NORMAL
- en: One question we might consider is whether this difference is statistically significant
    itself. Categorising p-values in a binary way skews our intuition, making us believe
    there’s a vast, even ontological, difference between p-values on different sides
    of the cutoff. However, the p-value is a fairly continuous function, and it might
    be possible that two A/B tests, despite different p-values, present very similar
    evidence against the null [2].
  prefs: []
  type: TYPE_NORMAL
- en: 'Another way to look at this is to combine the evidence. Assuming the null hypothesis
    is true for both tests, the combined p-value stands at 0.05, according to [Fisher’s
    method](https://en.wikipedia.org/wiki/Fisher%27s_method). There are other methods
    to combine p-values, but the general logic remains the same: a sharp null isn’t
    a realistic hypothesis in most settings. Therefore, enough ‘surprising’ outcomes,
    even if none of them are statistically significant individually, might be sufficient
    to reject the null.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ca549bdd0f58d7ea341632905c7e5070.png)'
  prefs: []
  type: TYPE_IMG
- en: Fusing two p-values by using Fisher’s method. Image by Chen-Pan Liao, from [Wikipedia](https://en.wikipedia.org/wiki/Fisher%27s_method)
  prefs: []
  type: TYPE_NORMAL
- en: '**Conclusion**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Null Hypothesis Testing Framework, which we commonly use to analyze A/B
    tests, isn’t especially intuitive. Without regular mental practice, we often revert
    to an ‘intuitive’ understanding, which can be misleading. We may also develop
    routines to ease this cognitive burden. Unfortunately, these routines often become
    somewhat ritualistic, with the adherence to formal procedures overshadowing the
    actual objective of inference.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: McShane, B. B., Gal, D., Gelman, A., Robert, C., & Tackett, J. L. (2019). Abandon
    statistical significance. *The American Statistician*, *73*(sup1), 235–245.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Gelman, A., & Stern, H. (2006). The difference between “significant” and “not
    significant” is not itself statistically significant. *The American Statistician*,
    *60*(4), 328–331.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
