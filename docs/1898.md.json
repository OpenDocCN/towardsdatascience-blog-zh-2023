["```py\nversion: '3'\nservices:\n  zookeeper:\n    image: wurstmeister/zookeeper:3.4.6\n    ports:\n     - \"2181:2181\"\n  kafka:\n    depends_on: \n      - zookeeper\n    image: wurstmeister/kafka\n    ports:\n     - \"9092:9092\"\n    expose:\n     - \"9093\"\n    environment:\n      KAFKA_ADVERTISED_LISTENERS: INSIDE://kafka:9093,OUTSIDE://localhost:9092\n      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: INSIDE:PLAINTEXT,OUTSIDE:PLAINTEXT\n      KAFKA_LISTENERS: INSIDE://0.0.0.0:9093,OUTSIDE://0.0.0.0:9092\n      KAFKA_INTER_BROKER_LISTENER_NAME: INSIDE\n      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181\n      KAFKA_CREATE_TOPICS: \"ml_training_data:1:1\"\n    volumes:\n     - /var/run/docker.sock:/var/run/docker.sock\n```", "```py\ndocker-compose up\n```", "```py\npython -m pip install kafka-python river \n```", "```py\nfrom time import sleep\nfrom json import dumps\nimport random\n\nfrom river import datasets\nfrom kafka import KafkaProducer\n\n# create a kafka product that connects to Kafka on port 9092\nproducer = KafkaProducer(\n    bootstrap_servers=[\"localhost:9092\"],\n    value_serializer=lambda x: dumps(x).encode(\"utf-8\"),\n)\n\n# Initialize the River phishing dataset.\n# This dataset contains features from web pages \n# that are classified as phishing or not.\ndataset = datasets.Phishing()\n\n# Send observations to the Kafka topic one-at-a-time with a random sleep\nfor x, y in dataset:\n    print(f\"Sending: {x, y}\")\n    data = {\"x\": x, \"y\": y}\n    producer.send(\"ml_training_data\", value=data)\n    sleep(random.random())\n```", "```py\n [({'empty_server_form_handler': 0.0,\n   'popup_window': 0.0,\n   'https': 0.0,\n   'request_from_other_domain': 0.0,\n   'anchor_from_other_domain': 0.0,\n   'is_popular': 0.5,\n   'long_url': 1.0,\n   'age_of_domain': 1,\n   'ip_in_url': 1},\n  True),\n ({'empty_server_form_handler': 1.0,\n   'popup_window': 0.0,\n   'https': 0.5,\n   'request_from_other_domain': 0.5,\n   'anchor_from_other_domain': 0.0,\n   'is_popular': 0.5,\n   'long_url': 0.0,\n   'age_of_domain': 1,\n   'ip_in_url': 0},\n  True)]\n```", "```py\npython producer.py\n```", "```py\n Sending: ({'empty_server_form_handler': 1.0, 'popup_window': 0.5, 'https': 1.0, 'request_from_other_domain': 1.0, 'anchor_from_other_domain': 0.5, 'is_popular': 0.5, 'long_url': 0.0, 'age_of_domain': 1, 'ip_in_url': 1}, False)\nSending: ({'empty_server_form_handler': 0.0, 'popup_window': 0.5, 'https': 0.0, 'request_from_other_domain': 0.0, 'anchor_from_other_domain': 0.0, 'is_popular': 0.5, 'long_url': 0.0, 'age_of_domain': 1, 'ip_in_url': 0}, True)\nSending: ({'empty_server_form_handler': 1.0, 'popup_window': 1.0, 'https': 1.0, 'request_from_other_domain': 0.0, 'anchor_from_other_domain': 1.0, 'is_popular': 0.0, 'long_url': 0.5, 'age_of_domain': 1, 'ip_in_url': 0}, False)\nSending: ({'empty_server_form_handler': 0.5, 'popup_window': 0.0, 'https': 0.0, 'request_from_other_domain': 0.5, 'anchor_from_other_domain': 1.0, 'is_popular': 0.5, 'long_url': 1.0, 'age_of_domain': 0, 'ip_in_url': 0}, True)\nSending: ({'empty_server_form_handler': 0.0, 'popup_window': 0.0, 'https': 1.0, 'request_from_other_domain': 1.0, 'anchor_from_other_domain': 0.0, 'is_popular': 1.0, 'long_url': 0.0, 'age_of_domain': 0, 'ip_in_url': 0}, True)\nSending: ({'empty_server_form_handler': 1.0, 'popup_window': 1.0, 'https': 1.0, 'request_from_other_domain': 0.5, 'anchor_from_other_domain': 0.0, 'is_popular': 1.0, 'long_url': 1.0, 'age_of_domain': 0, 'ip_in_url': 0}, False)\n```", "```py\n from json import loads\nfrom time import sleep\n\nfrom kafka import KafkaConsumer\n\nfrom river import linear_model\nfrom river import compose\nfrom river import preprocessing\nfrom river import metrics\n\n# use rocauc as the metric for evaluation\nmetric = metrics.ROCAUC()\n\n# create a simple LR model with a scaler\nmodel = compose.Pipeline(\n    preprocessing.StandardScaler(), linear_model.LogisticRegression()\n)\n\n# create our Kafka consumer\nconsumer = KafkaConsumer(\n    \"ml_training_data\",\n    bootstrap_servers=[\"localhost:9092\"],\n    auto_offset_reset=\"earliest\",\n    enable_auto_commit=True,\n    group_id=\"my-group-id\",\n    value_deserializer=lambda x: loads(x.decode(\"utf-8\")),\n)\n\n# use each event to update our model and print the metrics\nfor event in consumer:\n    event_data = event.value\n    try:\n        x = event_data[\"x\"]\n        y = event_data[\"y\"]\n        y_pred = model.predict_proba_one(x)\n        model.learn_one(x, y)\n        metric.update(y, y_pred)\n        print(metric)\n    except:\n        print(\"Processing bad data...\")\n```", "```py\npython consumer.py\n```", "```py\nROCAUC: 87.12%\nROCAUC: 87.29%\nROCAUC: 87.42%\nROCAUC: 87.29%\nROCAUC: 87.42%\n```"]