- en: 'The History of Open-Source LLMs: Early Days (Part One)'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 开源 LLM 的历史：早期阶段（第一部分）
- en: 原文：[https://towardsdatascience.com/the-history-of-open-source-llms-early-days-part-one-d782bcd8f7e8?source=collection_archive---------8-----------------------#2023-11-07](https://towardsdatascience.com/the-history-of-open-source-llms-early-days-part-one-d782bcd8f7e8?source=collection_archive---------8-----------------------#2023-11-07)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/the-history-of-open-source-llms-early-days-part-one-d782bcd8f7e8?source=collection_archive---------8-----------------------#2023-11-07](https://towardsdatascience.com/the-history-of-open-source-llms-early-days-part-one-d782bcd8f7e8?source=collection_archive---------8-----------------------#2023-11-07)
- en: Understanding GPT-Neo, GPT-J, GLM, OPT, BLOOM, and more…
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 了解 GPT-Neo、GPT-J、GLM、OPT、BLOOM 等…
- en: '[](https://wolfecameron.medium.com/?source=post_page-----d782bcd8f7e8--------------------------------)[![Cameron
    R. Wolfe, Ph.D.](../Images/52bb88d7cf1105501be2fae5ccbe7a03.png)](https://wolfecameron.medium.com/?source=post_page-----d782bcd8f7e8--------------------------------)[](https://towardsdatascience.com/?source=post_page-----d782bcd8f7e8--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----d782bcd8f7e8--------------------------------)
    [Cameron R. Wolfe, Ph.D.](https://wolfecameron.medium.com/?source=post_page-----d782bcd8f7e8--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://wolfecameron.medium.com/?source=post_page-----d782bcd8f7e8--------------------------------)[![Cameron
    R. Wolfe, Ph.D.](../Images/52bb88d7cf1105501be2fae5ccbe7a03.png)](https://wolfecameron.medium.com/?source=post_page-----d782bcd8f7e8--------------------------------)[](https://towardsdatascience.com/?source=post_page-----d782bcd8f7e8--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----d782bcd8f7e8--------------------------------)
    [Cameron R. Wolfe, Ph.D.](https://wolfecameron.medium.com/?source=post_page-----d782bcd8f7e8--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F28aa6026c553&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-history-of-open-source-llms-early-days-part-one-d782bcd8f7e8&user=Cameron+R.+Wolfe%2C+Ph.D.&userId=28aa6026c553&source=post_page-28aa6026c553----d782bcd8f7e8---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----d782bcd8f7e8--------------------------------)
    ·20 min read·Nov 7, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fd782bcd8f7e8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-history-of-open-source-llms-early-days-part-one-d782bcd8f7e8&user=Cameron+R.+Wolfe%2C+Ph.D.&userId=28aa6026c553&source=-----d782bcd8f7e8---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F28aa6026c553&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-history-of-open-source-llms-early-days-part-one-d782bcd8f7e8&user=Cameron+R.+Wolfe%2C+Ph.D.&userId=28aa6026c553&source=post_page-28aa6026c553----d782bcd8f7e8---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----d782bcd8f7e8--------------------------------)
    · 20 min 阅读 · 2023年11月7日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fd782bcd8f7e8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-history-of-open-source-llms-early-days-part-one-d782bcd8f7e8&user=Cameron+R.+Wolfe%2C+Ph.D.&userId=28aa6026c553&source=-----d782bcd8f7e8---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fd782bcd8f7e8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-history-of-open-source-llms-early-days-part-one-d782bcd8f7e8&source=-----d782bcd8f7e8---------------------bookmark_footer-----------)![](../Images/05d30e1fd65a03fd2cb858285b0d7f58.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fd782bcd8f7e8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-history-of-open-source-llms-early-days-part-one-d782bcd8f7e8&source=-----d782bcd8f7e8---------------------bookmark_footer-----------)![](../Images/05d30e1fd65a03fd2cb858285b0d7f58.png)'
- en: (Photo by [Chris Lawton](https://unsplash.com/@chrislawton?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)
    on [Unsplash](https://unsplash.com/photos/stack-of-six-brown-hardbound-books-9T346Ij4kGk?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash))
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: （图片由 [Chris Lawton](https://unsplash.com/@chrislawton?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)
    提供，来自 [Unsplash](https://unsplash.com/photos/stack-of-six-brown-hardbound-books-9T346Ij4kGk?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)）
- en: Research on language modeling has a long history that dates back to models like
    GTP and GPT-2 or even RNN-based techniques (e.g., [ULMFit](https://arxiv.org/abs/1801.06146))
    that predate modern, transformer-based language models. Despite this long history,
    however, language models have only become popular relatively recently. The first
    rise in popularity came with the proposal of GPT-3 [1], which showed that impressive
    few-shot learning performance could be achieved across many tasks via a combination
    of self-supervised pre-training and in-context learning; see below.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 语言建模的研究有着悠久的历史，早期的模型如 GTP 和 GPT-2，甚至是基于 RNN 的技术（例如，[ULMFit](https://arxiv.org/abs/1801.06146)）都早于现代的基于变换器的语言模型。然而，尽管有着如此悠久的历史，语言模型的受欢迎程度却相对较晚。第一次受欢迎是由于
    GPT-3 [1] 的提出，它展示了通过自监督预训练和上下文学习的结合可以在许多任务中实现令人印象深刻的少量学习表现；详见下文。
- en: '![](../Images/3052b7fe6011e83507c05a91e32c5cda.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3052b7fe6011e83507c05a91e32c5cda.png)'
- en: (from [1])
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: （来自 [1]）
- en: After this, the recognition garnered by GPT-3 led to the proposal of a swath
    of large language models (LLMs). Shortly after, research on language model alignment
    led to the creation of even more impressive models like InstructGPT [19] and,
    most notably, its sister model ChatGPT. The impressive performance of these models
    led to a flood of interest in language modeling and generative AI.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在这之后，GPT-3 获得的认可促使了大量大语言模型（LLM）的提出。随后，语言模型对齐的研究导致了更令人印象深刻的模型的创建，如 InstructGPT
    [19] 和最著名的姊妹模型 ChatGPT。这些模型的卓越表现引发了对语言建模和生成性 AI 的极大兴趣。
- en: Despite being incredibly powerful, many early developments in LLM research have
    one common property — *they are closed source*…
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管极其强大，许多早期 LLM 研究的发展都有一个共同的特点 — *它们是闭源的*…
