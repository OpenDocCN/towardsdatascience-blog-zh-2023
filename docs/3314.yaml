- en: 'The History of Open-Source LLMs: Early Days (Part One)'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/the-history-of-open-source-llms-early-days-part-one-d782bcd8f7e8?source=collection_archive---------8-----------------------#2023-11-07](https://towardsdatascience.com/the-history-of-open-source-llms-early-days-part-one-d782bcd8f7e8?source=collection_archive---------8-----------------------#2023-11-07)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Understanding GPT-Neo, GPT-J, GLM, OPT, BLOOM, and more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://wolfecameron.medium.com/?source=post_page-----d782bcd8f7e8--------------------------------)[![Cameron
    R. Wolfe, Ph.D.](../Images/52bb88d7cf1105501be2fae5ccbe7a03.png)](https://wolfecameron.medium.com/?source=post_page-----d782bcd8f7e8--------------------------------)[](https://towardsdatascience.com/?source=post_page-----d782bcd8f7e8--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----d782bcd8f7e8--------------------------------)
    [Cameron R. Wolfe, Ph.D.](https://wolfecameron.medium.com/?source=post_page-----d782bcd8f7e8--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F28aa6026c553&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-history-of-open-source-llms-early-days-part-one-d782bcd8f7e8&user=Cameron+R.+Wolfe%2C+Ph.D.&userId=28aa6026c553&source=post_page-28aa6026c553----d782bcd8f7e8---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----d782bcd8f7e8--------------------------------)
    ·20 min read·Nov 7, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fd782bcd8f7e8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-history-of-open-source-llms-early-days-part-one-d782bcd8f7e8&user=Cameron+R.+Wolfe%2C+Ph.D.&userId=28aa6026c553&source=-----d782bcd8f7e8---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fd782bcd8f7e8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-history-of-open-source-llms-early-days-part-one-d782bcd8f7e8&source=-----d782bcd8f7e8---------------------bookmark_footer-----------)![](../Images/05d30e1fd65a03fd2cb858285b0d7f58.png)'
  prefs: []
  type: TYPE_NORMAL
- en: (Photo by [Chris Lawton](https://unsplash.com/@chrislawton?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)
    on [Unsplash](https://unsplash.com/photos/stack-of-six-brown-hardbound-books-9T346Ij4kGk?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash))
  prefs: []
  type: TYPE_NORMAL
- en: Research on language modeling has a long history that dates back to models like
    GTP and GPT-2 or even RNN-based techniques (e.g., [ULMFit](https://arxiv.org/abs/1801.06146))
    that predate modern, transformer-based language models. Despite this long history,
    however, language models have only become popular relatively recently. The first
    rise in popularity came with the proposal of GPT-3 [1], which showed that impressive
    few-shot learning performance could be achieved across many tasks via a combination
    of self-supervised pre-training and in-context learning; see below.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3052b7fe6011e83507c05a91e32c5cda.png)'
  prefs: []
  type: TYPE_IMG
- en: (from [1])
  prefs: []
  type: TYPE_NORMAL
- en: After this, the recognition garnered by GPT-3 led to the proposal of a swath
    of large language models (LLMs). Shortly after, research on language model alignment
    led to the creation of even more impressive models like InstructGPT [19] and,
    most notably, its sister model ChatGPT. The impressive performance of these models
    led to a flood of interest in language modeling and generative AI.
  prefs: []
  type: TYPE_NORMAL
- en: Despite being incredibly powerful, many early developments in LLM research have
    one common property — *they are closed source*…
  prefs: []
  type: TYPE_NORMAL
