- en: Introducing a Dataset to Detect GPT-Generated Text
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/introducing-a-dataset-to-detect-gpt-generated-text-96bb76dd2ed2?source=collection_archive---------7-----------------------#2023-02-08](https://towardsdatascience.com/introducing-a-dataset-to-detect-gpt-generated-text-96bb76dd2ed2?source=collection_archive---------7-----------------------#2023-02-08)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: How to create datasets for ChatGPT detection models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@aadityaubhat?source=post_page-----96bb76dd2ed2--------------------------------)[![Aaditya
    Bhat](../Images/4ae4a03d798d4a3fbec02d81c9c87146.png)](https://medium.com/@aadityaubhat?source=post_page-----96bb76dd2ed2--------------------------------)[](https://towardsdatascience.com/?source=post_page-----96bb76dd2ed2--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----96bb76dd2ed2--------------------------------)
    [Aaditya Bhat](https://medium.com/@aadityaubhat?source=post_page-----96bb76dd2ed2--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Feff870d7210e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintroducing-a-dataset-to-detect-gpt-generated-text-96bb76dd2ed2&user=Aaditya+Bhat&userId=eff870d7210e&source=post_page-eff870d7210e----96bb76dd2ed2---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----96bb76dd2ed2--------------------------------)
    ·4 min read·Feb 8, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F96bb76dd2ed2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintroducing-a-dataset-to-detect-gpt-generated-text-96bb76dd2ed2&user=Aaditya+Bhat&userId=eff870d7210e&source=-----96bb76dd2ed2---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F96bb76dd2ed2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintroducing-a-dataset-to-detect-gpt-generated-text-96bb76dd2ed2&source=-----96bb76dd2ed2---------------------bookmark_footer-----------)![](../Images/f19e3823d8d934d82e0773690e2b7474.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Photo by [Markus Spiske](https://unsplash.com/@markusspiske?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    on [Unsplash](https://unsplash.com/photos/iar-afB0QQw?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
  prefs: []
  type: TYPE_NORMAL
- en: With the breakthrough success of large language models like ChatGPT, people
    are finding innovative ways to use these models in their daily lives. However,
    this has also led to unintended consequences such as cheating on homework and
    tests by students, the use of ChatGPT to publish research papers, and even scammers
    using these models to trick people. To address these issues, there is a growing
    need for models that can detect text generated by GPT models. One of the crucial
    requirements for building robust models for detecting GPT-generated text is access
    to a large dataset of human-written and GPT-generated responses. This article
    introduces such a dataset, consisting of 150k human-written and GPT-generated
    responses to Wikipedia topics and outlines a framework for generating similar
    datasets in the future.
  prefs: []
  type: TYPE_NORMAL
- en: Dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[GPT-wiki-intro Dataset](https://huggingface.co/datasets/aadityaubhat/GPT-wiki-intro)
    is available on Hugging Face. This dataset has human written and GPT (Curie) generated
    introductions for 150,000 Wikipedia topics. The prompt used to generate GPT response
    is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Where the `title` is the title of Wikipedia page, and `starter_text` is the
    first 7 words from the introduction paragraph. The dataset also has useful metadata
    such as title_len, wiki_intro_len, generated_intro_len, prompt_tokens, generated_text_tokens,
    etc. The schema of the dataset is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: This dataset is shared under Creative Commons license and can be used to distribute,
    remix, adapt, and build upon. The code to generate this dataset can be found [here](https://github.com/aadityaubhat/wiki_gpt).
  prefs: []
  type: TYPE_NORMAL
- en: Framework
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This dataset is a good starting point for a general use-case of detecting GPT
    generated text. But if you have a specific use-case, for example, detecting ChatGPT
    generated answers to test/homework questions, detecting whether a message is sent
    by a human or chatbot, or if you need a larger dataset in a specific domain you
    can use the framework to generate your own dataset. The dataset generation process
    involves three main steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Get the anchor dataset
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Clean the anchor dataset
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Augment the dataset with human written/ GPT generated data
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Get the anchor dataset**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this step, we acquire the anchor dataset. This would be the existing data
    which is readily available for the specific use-case. For [GPT-wiki-intro dataset](https://huggingface.co/datasets/aadityaubhat/GPT-wiki-intro),
    the anchor dataset was [wikipedia dataset](https://huggingface.co/datasets/wikipedia#licensing-information),
    which contains cleaned articles of all languages. For detecting cheating on tests
    and homework, the anchor dataset could be the question answer pairs submitted
    by previous students. If you don’t have well defined anchor dataset, you can explore
    various open source datasets on Hugging Face and Kaggle which align well with
    the use-case. The anchor dataset doesn’t have to be human written, we can also
    use GPT generated data as the anchor data. For example, we can use data from [ChatGPT
    prompt response library](https://www.emergentmind.com/).
  prefs: []
  type: TYPE_NORMAL
- en: Clean the anchor dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Once we have the anchor dataset, we need to clean the data to keep the most
    relevant information. ChatGPT detection models are sensitive to the length of
    text. These models perform poorly on smaller texts. We can set a threshold and
    filter out any responses shorter than the threshold. For example, in GPT-wiki-intro
    dataset, we filter out all rows where length of introduction is less than 150
    words or greater than 350 words. We also filter out all rows where title is more
    than three words. At this step we also need to decide the total size of the dataset.
    As augmenting the data with either human written or GPT generated responses is
    going to be expensive, we need to figure out what’s the minimum required size
    of dataset for our use-case.
  prefs: []
  type: TYPE_NORMAL
- en: Augment the dataset with human written/ GPT generated data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This is the final step of dataset generation. At this step, we augment the anchor
    dataset with human written or GPT generated data. The most important part in this
    step is coming up with the **prompt** used for generating the response from GPT
    or **question** that will be answered by humans. For finalizing the prompt we
    can leverage [OpenAI Playground](https://platform.openai.com/playground) to test
    out different prompts with different models, temperature, frequency penalty and
    presence penalty. To increase the diversity of the dataset, we can finalize n
    prompts and use them uniformly to get the responses. In case of human responses,
    we would want to finalize the question by giving different variations of the questions
    to small survey population and then checking the results to finalize the n questions.
    Once the prompts or the questions are finalized, we can use the OpenAI API to
    generate the GPT generated responses or use service such as Mechanical Turk to
    get the human written responses.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In conclusion, with the widespread use of large language models like ChatGPT,
    there is a growing need for models that can detect text generated by these models.
    This article introduced [GPT-wiki-intro dataset](https://huggingface.co/datasets/aadityaubhat/GPT-wiki-intro)
    and outlined a framework for generating similar datasets. The availability of
    such datasets will play a critical role in developing robust models for detecting
    GPT-generated text and address the unintended consequences of the use of these
    models.
  prefs: []
  type: TYPE_NORMAL
- en: Citation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'If you find this work helpful, please consider citing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
