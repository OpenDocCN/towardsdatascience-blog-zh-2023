["```py\nfeatures = { \n  'rule3_selection_atexec' -> false, \n  'rule2_selection' -> true \n}\n```", "```py\n cause = (\n        spark.readStream\n        .format(\"rate\")\n        .option(\"rowsPerSecond\", rate)\n        .load()\n        .withWatermark(\"timestamp\", \"0 seconds\")\n        .withColumn(\"name\", F.lit(name))\n    )\n```", "```py\n cause = cause.select('cause_timestamp', 'cause_key', 'cause_load')\n\neffect = effect.select(\n  'effect_timestamp', \n  'effect_key', \n  'effect_load', \n  'host_id', \n  'id', \n  'name', \n  'value')\n\njoindf = (\n    effect.join(\n        cause,\n        F.expr(f\"\"\"\n            effect_key = cause_key\n            and effect_timestamp >= cause_timestamp\n            and effect_timestamp < (cause_timestamp + interval {window} seconds)\n        \"\"\"),\n        \"left\"\n    )\n\njoindf\n    .writeStream\n    .format(\"iceberg\")\n    .outputMode(\"append\")\n    .trigger(processingTime=\"1 minutes\")\n    .queryName(name)\n    .option(\"checkpointLocation\", checkpoint)\n    .toTable(output_table_name)\n```", "```py\n.config(\"spark.sql.streaming.stateStore.providerClass\", \n  \"org.apache.spark.sql.execution.streaming.state.RocksDBStateStoreProvider\")\n```"]