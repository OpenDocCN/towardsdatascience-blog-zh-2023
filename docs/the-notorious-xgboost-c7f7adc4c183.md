# 恶名昭著的XGBoost

> 原文：[https://towardsdatascience.com/the-notorious-xgboost-c7f7adc4c183?source=collection_archive---------3-----------------------#2023-05-20](https://towardsdatascience.com/the-notorious-xgboost-c7f7adc4c183?source=collection_archive---------3-----------------------#2023-05-20)

## 回顾最受奖项青睐的机器学习算法之一

[](https://crodriguez1a.medium.com/?source=post_page-----c7f7adc4c183--------------------------------)[![卡洛斯·罗德里格斯 (他/他)](../Images/f93397a05d50935e2f7eb83e79dbddc6.png)](https://crodriguez1a.medium.com/?source=post_page-----c7f7adc4c183--------------------------------)[](https://towardsdatascience.com/?source=post_page-----c7f7adc4c183--------------------------------)[![Towards Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----c7f7adc4c183--------------------------------) [卡洛斯·罗德里格斯 (他/他)](https://crodriguez1a.medium.com/?source=post_page-----c7f7adc4c183--------------------------------)

·

[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F8b0823c53807&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-notorious-xgboost-c7f7adc4c183&user=Carlos+Rodriguez+%28he%2Fhim%29&userId=8b0823c53807&source=post_page-8b0823c53807----c7f7adc4c183---------------------post_header-----------) 发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----c7f7adc4c183--------------------------------) ·8分钟阅读·2023年5月20日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fc7f7adc4c183&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-notorious-xgboost-c7f7adc4c183&user=Carlos+Rodriguez+%28he%2Fhim%29&userId=8b0823c53807&source=-----c7f7adc4c183---------------------clap_footer-----------)

--

[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc7f7adc4c183&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-notorious-xgboost-c7f7adc4c183&source=-----c7f7adc4c183---------------------bookmark_footer-----------)![](../Images/ffb7cc6b025bf7a5d1087565bfee4993.png)

XGBoost 的化身 — 作者 + OpenJourney

如果你是一名从事监督学习问题的数据科学家，你可能已经注意到XGBoost经常在你的排行榜上名列前茅。它的成功很大程度上归功于该算法在捕捉难以捉摸的非线性关系方面的卓越准确性。尽管现在还为时已晚，尚无法判断XGBoost是否会在各种最先进的通用模型涌现后幸存，但XGBoost的一贯可靠性促使我们回顾其背后的优雅数学。

XGBoost（极端梯度提升）是一种强大的基于树的集成技术，特别擅长完成分类和回归任务。它基于梯度提升机（GBM）算法，通过结合多个弱模型（在这种情况下是决策树）来形成更强的模型（Friedman, 2001）。XGBoost 的学习过程可以分解如下：

# 分解

## 目标函数

XGBoost 的基本原理是最小化目标函数。目标函数，表示为 Objective(T)，结合了训练损失（对训练数据的评估）和正则化项（防止过拟合）。

***Objective(T) = Loss + Regularization***

XGBoost 的目标函数如下所示：

***Objective(T) = ∑l(yi, y_pred,i) + ∑Ω(f)***

其中：

- ***T*** 代表决策树的集成

- ***l(y, y_pred)*** 是一个可微分的凸损失函数，衡量真实输出 *(y)* 和预测输出 *(y_pred)* 之间的差异

- ***yi*** 是真实输出，例如 *i*

- ***y_pred,i*** 是实例 *i* 的预测输出

- ***Ω(f)*** 是对集成中的每棵树 *(f)* 应用的正则化项 *(T)*

## 加法训练

XGBoost 以加法方式学习目标函数，创建一个迭代的决策树集成（弱学习器），逐步最小化目标函数。在每次迭代中，向集成中添加一棵新树，并优化目标函数。

为了形式化这一点，我们考虑以下内容：

***Fm(x) = Fm-1(x) + fm(x)***

其中：

- ***Fm(x)*** 是添加了 m 棵树后的预测

- ***Fm-1(x)*** 是预测值直到 *m-1* 棵树

- ***fm(x)*** 是在 *m-th* 迭代中添加的新树

## 梯度和Hessian计算

为了最小化目标函数，XGBoost 使用梯度下降。在每次迭代中，计算损失函数相对于预测输出 *(y_pred)* 的一阶和二阶导数（梯度和Hessian）。

***Gradient(g): ∇l(y, y_pred) = d(l) / dy_pred***

***Hessian(h): ∇²l(y, y_pred) = d²(l) / dy_pred²***

对于数据中的每个实例 *(i)*，计算导数，得到梯度和Hessian值的向量 *g* 和 *h*。

## 树的构造

在 *m-th* 迭代中，使用计算得到的梯度和Hessian，添加最小化目标函数的最佳树 *(fm)*。XGBoost 从空树开始，然后依次分裂每个叶子以最小化以下方程：

***Gain = 1/2 * [Gl² / (Hl + λ) + Gr² / (Hr + λ) - G² / (H + λ)] - γ***

其中，

- ***Gl*** 和 ***Gr*** 是分裂后左侧和右侧区域的梯度之和

- ***Hl*** 和 ***Hr*** 是分裂后左侧和右侧区域的Hessian之和

- ***G = Gl + Gr***，整个节点的梯度之和

- ***H = Hl + Hr***，整个节点的Hessian矩阵之和

- ***λ***，L2 正则化项

- ***γ***，进行分裂所需的最小损失减少（另一个正则化项）

增益方程结合了损失减少和正则化项，这有助于防止过拟合并在复杂度和预测能力之间做出最佳权衡。

## 超参数

***learning_rate***: (eta) 控制每次迭代时的更新步长，缩小每棵树的效果以防止过拟合。它是新添加到集成中的树的权重因子。

***max_depth***: 树的最大允许深度。随着深度的增加，模型变得更加复杂，可能会过拟合数据。

***min_child_weight***: (最小实例权重和 H) 进行树的分裂所需的最小 Hessian 值和。增加此值可以通过使树更为保守来防止过拟合。

***lambda***: 权重的 L2 正则化项，作为增益计算的一部分应用。帮助控制模型复杂性。

***gamma***: (min_split_loss) 进行进一步分裂所需的最小损失减少。控制树的生长和复杂性。

***subsample***: 每次提升轮次中从训练集中采样的比例。随机选择数据子集通过引入随机性来减少过拟合的可能性，从而提高集成过程的鲁棒性。

***colsample_bytree***: 每次提升轮次中选择的特征比例。随机选择列（特征）来构建相关性较小的树，防止过拟合。

实际上，这些超参数影响树的构建和增益计算（如 *λ* 和 *γ*），或者每次迭代中选择数据和特征的过程（如 subsample 和 colsample_bytree）。调整这些超参数有助于平衡模型复杂度和预测能力，提高性能，同时减轻过拟合。

简而言之，XGBoost 通过迭代地构建决策树的集成来学习，最小化由训练损失和正则化项组成的目标函数。它利用梯度下降来找到最佳树，采用损失函数的一阶和二阶导数。XGBoost 利用超参数，如最大深度、正则化参数和特征及实例的子采样策略，以防止过拟合并提高计算效率。值得注意的是，特别是子采样，通过在每次迭代中处理较少的数据点，引入了随机性和多样性，从而减少过拟合的机会并加快训练过程。

# 独特特征

陈天奇和古斯特林强调了几个使 XGBoost 与其他提升算法不同的独特特征，并提升了其性能（陈天奇和古斯特林，2016）。这些特征包括：

## 稀疏感知

XGBoost旨在有效处理稀疏数据，这在包含缺失值或零值的现实世界数据集中很常见。XGBoost使用一种考虑稀疏性的算法来为缺失值的数据点找到最佳分裂，从而提高了对稀疏数据的性能。

具体来说，XGBoost在树构建过程中采用了默认（缺失值）方向。当特征值缺失时，算法会自动选择产生最高增益的方向，而不会对缺失值进行显式分裂。这种考虑稀疏性的方式使XGBoost高效，并减少了树构建所需的信息量。

## 正则化提升：

如前所述，XGBoost在树构建过程中引入了正则化项（L1和L2），这有助于控制模型的复杂性并减少过拟合。这与缺乏正则化组件的传统GBM有一个关键区别。

## 列块（缓存感知）和并行学习：

XGBoost支持树构建过程中的并行处理，使其能够利用多个处理器核心以加快学习速度。该算法按列排序数据，并以压缩形式存储在列块中。XGBoost通过使用缓存感知算法预取列块，确保树构建过程中的高效内存访问，使其适合处理大数据集。

# 假设、优势和缺点

## 假设

简而言之，XGBoost假设弱学习器（决策树）可以组合成一个更强大、更稳健的模型。它还假设目标函数是连续的、可微的和凸的。

## 优势

**高性能**：XGBoost在分类和回归任务中始终取得最先进的结果。

**可扩展性**：XGBoost高效利用内存和计算资源，使其适合大规模问题。

**正则化**：内置正则化项有助于防止过拟合。

**稀疏意识**：XGBoost被设计为有效处理稀疏数据。

**并行性**：支持并行和分布式计算以加快学习速度。

## 缺点

**计算复杂性**：尽管XGBoost高效，但对于大数据集或大规模集成方法，它仍可能计算开销较大。

**可解释性**：尽管线性回归或单一决策树等参数化方法具有固有的可解释性，但像XGBoost这样的非参数集成方法需要额外的解释方法，如Tree SHAP，以解释结果。

**对超参数的敏感性**：XGBoost的性能受超参数的影响，通常需要微调以获得最佳结果。

# 对社会偏见、可信度和安全性的敏感性

像其他机器学习算法一样，XGBoost的性能高度依赖于输入数据的质量。虽然算法本身可能不会表现出与算法偏见、可信度或安全漏洞相关的弱点，但这些问题可能由于偏见采样、不当应用或不公平解释模型结果而出现。

## 社会偏见

XGBoost可能会无意中传播或甚至放大数据中存在的社会偏见。当训练数据反映出代表性不足、歧视或延续的刻板印象时，XGBoost模型将不可避免地学习这些模式，可能导致有害的结果。确保数据中的代表性并解决社会偏见对于减少相关风险至关重要。

## 可信度

XGBoost是一个决策树的集成，可能会导致复杂的模型，难以解释。这种缺乏透明度可能使利益相关者难以信任模型并理解其决策过程。像Shapley加法解释（SHAP）这样的方法已帮助减少“黑箱”问题，但解释性仍然是一个关注点（Lundberg和Lee 2017；Rudin 2019）。

## 安全性

机器学习模型，包括XGBoost，可能会受到对抗性攻击、数据中毒或逆向工程的影响，这些可能暴露敏感信息（即去匿名化）或影响模型性能。确保数据集的安全性和保护模型免受恶意攻击对于维护系统的完整性和稳健性至关重要。此外，篡改或改变输入数据的来源可能导致误导性或不正确的预测，这引发了关于模型可信度的问题。

# 结论

XGBoost是一个强大且多功能的机器学习算法，由于其卓越的性能、可扩展性和效率，它在排行榜上占据了主导地位。通过利用集成学习、梯度下降和正则化技术，XGBoost克服了许多传统提升方法的局限，同时适应处理稀疏数据并优化计算资源。

然而，必须承认，任何机器学习模型，包括XGBoost，相关的潜在风险依赖于算法的负责任使用。具体而言，通过仔细的数据预处理、通过解释性技术增强透明度和实施稳健的安全措施，可以帮助应对这些挑战，并确保XGBoost模型在实践和伦理上都符合要求。

采纳伦理原则和最佳实践可以让我们继续利用XGBoost和其他机器学习技术的力量，同时推动这些技术在未来带来公平和有益的成果。

# 参考文献

1\. Chen T, Guestrin C. XGBoost：一种可扩展的树提升系统。发表于：第22届ACM SIGKDD国际知识发现与数据挖掘会议（KDD ‘16）；2016年8月13–17日；加州旧金山。纽约：计算机协会；2016年。第785–794页。DOI: [https://doi.org/10.1145/2939672.2939785](https://doi.org/10.1145/2939672.2939785)。

2\. Friedman JH. 贪婪的函数近似：一种梯度提升机。统计年鉴。2001年；29(5)：1189–1232。

3\. Lundberg SM, Lee SI. 统一的模型预测解释方法。发表于：神经信息处理系统进展（NIPS 2017）；2017年12月4–9日；加州长滩。2017年。

4\. Rudin C. 请停止解释用于高风险决策的黑箱模型。arXiv:1811.10154。2019年。
